Logging output to training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/train-log_2017-08-15_19-04-07.txt
Using pretrained model training/imagenet_jacintonet11v2_iter_320000.caffemodel
training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/initial
training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/l1reg
training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/sparse
training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/test
training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/test_quantize
I0815 19:04:17.830703 20809 caffe.cpp:608] This is NVCaffe 0.16.3 started at Tue Aug 15 19:04:17 2017
I0815 19:04:17.830837 20809 caffe.cpp:611] CuDNN version: 6021
I0815 19:04:17.830839 20809 caffe.cpp:612] CuBLAS version: 8000
I0815 19:04:17.830842 20809 caffe.cpp:613] CUDA version: 8000
I0815 19:04:17.830843 20809 caffe.cpp:614] CUDA driver version: 8000
I0815 19:04:18.104919 20809 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0815 19:04:18.105489 20809 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I0815 19:04:18.106029 20809 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[1]: total=8508145664 free=8379236352
I0815 19:04:18.106546 20809 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[2]: total=8508145664 free=8379236352
I0815 19:04:18.106554 20809 caffe.cpp:208] Using GPUs 0, 1, 2
I0815 19:04:18.106875 20809 caffe.cpp:213] GPU 0: GeForce GTX 1080
I0815 19:04:18.107197 20809 caffe.cpp:213] GPU 1: GeForce GTX 1080
I0815 19:04:18.107518 20809 caffe.cpp:213] GPU 2: GeForce GTX 1080
I0815 19:04:18.112586 20809 solver.cpp:42] Solver data type: FLOAT
I0815 19:04:18.112623 20809 solver.cpp:45] Initializing solver from parameters: 
train_net: "training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/initial/train.prototxt"
test_net: "training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/initial/test.prototxt"
test_iter: 125
test_interval: 2000
base_lr: 0.0001
display: 100
max_iter: 32000
lr_policy: "multistep"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 0.0001
snapshot: 10000
snapshot_prefix: "training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/initial/cityscapes5_jsegnet21v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
test_initialization: false
stepvalue: 24000
iter_size: 1
type: "Adam"
I0815 19:04:18.119510 20809 solver.cpp:77] Creating training net from train_net file: training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/initial/train.prototxt
I0815 19:04:18.120100 20809 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0815 19:04:18.120107 20809 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
W0815 19:04:18.120146 20809 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0815 19:04:18.120394 20809 net.cpp:72] Initializing net from parameters: 
name: "jsegnet21v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/train-image-lmdb"
    label_list_path: "data/train-label-lmdb"
    batch_size: 6
    shuffle: false
    threads: 1
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
I0815 19:04:18.120579 20809 net.cpp:104] Using FLOAT as default forward math type
I0815 19:04:18.120585 20809 net.cpp:110] Using FLOAT as default backward math type
I0815 19:04:18.120589 20809 layer_factory.hpp:136] Creating layer 'data' of type 'ImageLabelData'
I0815 19:04:18.120594 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.125507 20809 net.cpp:184] Created Layer data (0)
I0815 19:04:18.125516 20809 net.cpp:530] data -> data
I0815 19:04:18.125532 20809 net.cpp:530] data -> label
I0815 19:04:18.125617 20809 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0815 19:04:18.125632 20809 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 19:04:18.175300 20830 db_lmdb.cpp:24] Opened lmdb data/train-image-lmdb
I0815 19:04:18.246284 20809 data_layer.cpp:185] [0] ReshapePrefetch 6, 3, 640, 640
I0815 19:04:18.246367 20809 data_layer.cpp:209] [0] Output data size: 6, 3, 640, 640
I0815 19:04:18.246382 20809 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 19:04:18.246474 20809 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0815 19:04:18.246501 20809 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 19:04:18.247313 20840 data_layer.cpp:97] [0] Parser threads: 1
I0815 19:04:18.247323 20840 data_layer.cpp:99] [0] Transformer threads: 1
I0815 19:04:18.257385 20840 blocking_queue.cpp:40] Waiting for datum
I0815 19:04:18.283761 20841 db_lmdb.cpp:24] Opened lmdb data/train-label-lmdb
I0815 19:04:18.429167 20809 data_layer.cpp:185] [0] ReshapePrefetch 6, 1, 640, 640
I0815 19:04:18.429215 20809 data_layer.cpp:209] [0] Output data size: 6, 1, 640, 640
I0815 19:04:18.429220 20809 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 19:04:18.429265 20809 net.cpp:245] Setting up data
I0815 19:04:18.429276 20809 net.cpp:252] TRAIN Top shape for layer 0 'data' 6 3 640 640 (7372800)
I0815 19:04:18.429288 20809 net.cpp:252] TRAIN Top shape for layer 0 'data' 6 1 640 640 (2457600)
I0815 19:04:18.429297 20809 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0815 19:04:18.429303 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.429324 20809 net.cpp:184] Created Layer data/bias (1)
I0815 19:04:18.429330 20809 net.cpp:561] data/bias <- data
I0815 19:04:18.429343 20809 net.cpp:530] data/bias -> data/bias
I0815 19:04:18.430322 20865 data_layer.cpp:97] [0] Parser threads: 1
I0815 19:04:18.430333 20865 data_layer.cpp:99] [0] Transformer threads: 1
I0815 19:04:18.433727 20809 net.cpp:245] Setting up data/bias
I0815 19:04:18.433750 20809 net.cpp:252] TRAIN Top shape for layer 1 'data/bias' 6 3 640 640 (7372800)
I0815 19:04:18.433765 20809 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0815 19:04:18.433773 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.433802 20809 net.cpp:184] Created Layer conv1a (2)
I0815 19:04:18.433807 20809 net.cpp:561] conv1a <- data/bias
I0815 19:04:18.433812 20809 net.cpp:530] conv1a -> conv1a
I0815 19:04:18.795943 20809 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 7.91G, req 0G)
I0815 19:04:18.795996 20809 net.cpp:245] Setting up conv1a
I0815 19:04:18.796005 20809 net.cpp:252] TRAIN Top shape for layer 2 'conv1a' 6 32 320 320 (19660800)
I0815 19:04:18.796021 20809 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0815 19:04:18.796027 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.796041 20809 net.cpp:184] Created Layer conv1a/bn (3)
I0815 19:04:18.796046 20809 net.cpp:561] conv1a/bn <- conv1a
I0815 19:04:18.796051 20809 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0815 19:04:18.796741 20809 net.cpp:245] Setting up conv1a/bn
I0815 19:04:18.796751 20809 net.cpp:252] TRAIN Top shape for layer 3 'conv1a/bn' 6 32 320 320 (19660800)
I0815 19:04:18.796759 20809 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0815 19:04:18.796764 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.796772 20809 net.cpp:184] Created Layer conv1a/relu (4)
I0815 19:04:18.796775 20809 net.cpp:561] conv1a/relu <- conv1a
I0815 19:04:18.796780 20809 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0815 19:04:18.796794 20809 net.cpp:245] Setting up conv1a/relu
I0815 19:04:18.796799 20809 net.cpp:252] TRAIN Top shape for layer 4 'conv1a/relu' 6 32 320 320 (19660800)
I0815 19:04:18.796803 20809 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0815 19:04:18.796808 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.796818 20809 net.cpp:184] Created Layer conv1b (5)
I0815 19:04:18.796823 20809 net.cpp:561] conv1b <- conv1a
I0815 19:04:18.796826 20809 net.cpp:530] conv1b -> conv1b
I0815 19:04:18.842250 20809 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.74G, req 0G)
I0815 19:04:18.842273 20809 net.cpp:245] Setting up conv1b
I0815 19:04:18.842280 20809 net.cpp:252] TRAIN Top shape for layer 5 'conv1b' 6 32 320 320 (19660800)
I0815 19:04:18.842293 20809 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0815 19:04:18.842298 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.842309 20809 net.cpp:184] Created Layer conv1b/bn (6)
I0815 19:04:18.842314 20809 net.cpp:561] conv1b/bn <- conv1b
I0815 19:04:18.842319 20809 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0815 19:04:18.843003 20809 net.cpp:245] Setting up conv1b/bn
I0815 19:04:18.843013 20809 net.cpp:252] TRAIN Top shape for layer 6 'conv1b/bn' 6 32 320 320 (19660800)
I0815 19:04:18.843020 20809 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0815 19:04:18.843024 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.843031 20809 net.cpp:184] Created Layer conv1b/relu (7)
I0815 19:04:18.843034 20809 net.cpp:561] conv1b/relu <- conv1b
I0815 19:04:18.843039 20809 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0815 19:04:18.843045 20809 net.cpp:245] Setting up conv1b/relu
I0815 19:04:18.843050 20809 net.cpp:252] TRAIN Top shape for layer 7 'conv1b/relu' 6 32 320 320 (19660800)
I0815 19:04:18.843053 20809 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0815 19:04:18.843058 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.843067 20809 net.cpp:184] Created Layer pool1 (8)
I0815 19:04:18.843071 20809 net.cpp:561] pool1 <- conv1b
I0815 19:04:18.843075 20809 net.cpp:530] pool1 -> pool1
I0815 19:04:18.843152 20809 net.cpp:245] Setting up pool1
I0815 19:04:18.843158 20809 net.cpp:252] TRAIN Top shape for layer 8 'pool1' 6 32 160 160 (4915200)
I0815 19:04:18.843171 20809 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0815 19:04:18.843176 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.843188 20809 net.cpp:184] Created Layer res2a_branch2a (9)
I0815 19:04:18.843190 20809 net.cpp:561] res2a_branch2a <- pool1
I0815 19:04:18.843195 20809 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0815 19:04:18.881155 20809 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.61G, req 0G)
I0815 19:04:18.881175 20809 net.cpp:245] Setting up res2a_branch2a
I0815 19:04:18.881182 20809 net.cpp:252] TRAIN Top shape for layer 9 'res2a_branch2a' 6 64 160 160 (9830400)
I0815 19:04:18.881194 20809 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0815 19:04:18.881199 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.881209 20809 net.cpp:184] Created Layer res2a_branch2a/bn (10)
I0815 19:04:18.881214 20809 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0815 19:04:18.881220 20809 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0815 19:04:18.882313 20809 net.cpp:245] Setting up res2a_branch2a/bn
I0815 19:04:18.882323 20809 net.cpp:252] TRAIN Top shape for layer 10 'res2a_branch2a/bn' 6 64 160 160 (9830400)
I0815 19:04:18.882331 20809 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0815 19:04:18.882335 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.882340 20809 net.cpp:184] Created Layer res2a_branch2a/relu (11)
I0815 19:04:18.882344 20809 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0815 19:04:18.882349 20809 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0815 19:04:18.882355 20809 net.cpp:245] Setting up res2a_branch2a/relu
I0815 19:04:18.882359 20809 net.cpp:252] TRAIN Top shape for layer 11 'res2a_branch2a/relu' 6 64 160 160 (9830400)
I0815 19:04:18.882364 20809 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0815 19:04:18.882367 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.882377 20809 net.cpp:184] Created Layer res2a_branch2b (12)
I0815 19:04:18.882380 20809 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0815 19:04:18.882385 20809 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0815 19:04:18.902343 20809 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.52G, req 0G)
I0815 19:04:18.902362 20809 net.cpp:245] Setting up res2a_branch2b
I0815 19:04:18.902369 20809 net.cpp:252] TRAIN Top shape for layer 12 'res2a_branch2b' 6 64 160 160 (9830400)
I0815 19:04:18.902379 20809 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0815 19:04:18.902384 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.902395 20809 net.cpp:184] Created Layer res2a_branch2b/bn (13)
I0815 19:04:18.902398 20809 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0815 19:04:18.902405 20809 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0815 19:04:18.903079 20809 net.cpp:245] Setting up res2a_branch2b/bn
I0815 19:04:18.903089 20809 net.cpp:252] TRAIN Top shape for layer 13 'res2a_branch2b/bn' 6 64 160 160 (9830400)
I0815 19:04:18.903097 20809 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0815 19:04:18.903101 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.903107 20809 net.cpp:184] Created Layer res2a_branch2b/relu (14)
I0815 19:04:18.903111 20809 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0815 19:04:18.903115 20809 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0815 19:04:18.903121 20809 net.cpp:245] Setting up res2a_branch2b/relu
I0815 19:04:18.903126 20809 net.cpp:252] TRAIN Top shape for layer 14 'res2a_branch2b/relu' 6 64 160 160 (9830400)
I0815 19:04:18.903138 20809 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0815 19:04:18.903143 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.903151 20809 net.cpp:184] Created Layer pool2 (15)
I0815 19:04:18.903154 20809 net.cpp:561] pool2 <- res2a_branch2b
I0815 19:04:18.903158 20809 net.cpp:530] pool2 -> pool2
I0815 19:04:18.903221 20809 net.cpp:245] Setting up pool2
I0815 19:04:18.903228 20809 net.cpp:252] TRAIN Top shape for layer 15 'pool2' 6 64 80 80 (2457600)
I0815 19:04:18.903231 20809 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0815 19:04:18.903236 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.903245 20809 net.cpp:184] Created Layer res3a_branch2a (16)
I0815 19:04:18.903249 20809 net.cpp:561] res3a_branch2a <- pool2
I0815 19:04:18.903254 20809 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0815 19:04:18.922541 20809 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 7.46G, req 0G)
I0815 19:04:18.922561 20809 net.cpp:245] Setting up res3a_branch2a
I0815 19:04:18.922569 20809 net.cpp:252] TRAIN Top shape for layer 16 'res3a_branch2a' 6 128 80 80 (4915200)
I0815 19:04:18.922580 20809 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0815 19:04:18.922585 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.922595 20809 net.cpp:184] Created Layer res3a_branch2a/bn (17)
I0815 19:04:18.922598 20809 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0815 19:04:18.922605 20809 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0815 19:04:18.923295 20809 net.cpp:245] Setting up res3a_branch2a/bn
I0815 19:04:18.923303 20809 net.cpp:252] TRAIN Top shape for layer 17 'res3a_branch2a/bn' 6 128 80 80 (4915200)
I0815 19:04:18.923316 20809 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0815 19:04:18.923321 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.923327 20809 net.cpp:184] Created Layer res3a_branch2a/relu (18)
I0815 19:04:18.923331 20809 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0815 19:04:18.923334 20809 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0815 19:04:18.923341 20809 net.cpp:245] Setting up res3a_branch2a/relu
I0815 19:04:18.923346 20809 net.cpp:252] TRAIN Top shape for layer 18 'res3a_branch2a/relu' 6 128 80 80 (4915200)
I0815 19:04:18.923349 20809 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0815 19:04:18.923353 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.923363 20809 net.cpp:184] Created Layer res3a_branch2b (19)
I0815 19:04:18.923367 20809 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0815 19:04:18.923370 20809 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0815 19:04:18.933624 20809 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.42G, req 0G)
I0815 19:04:18.933639 20809 net.cpp:245] Setting up res3a_branch2b
I0815 19:04:18.933645 20809 net.cpp:252] TRAIN Top shape for layer 19 'res3a_branch2b' 6 128 80 80 (4915200)
I0815 19:04:18.933652 20809 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0815 19:04:18.933656 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.933665 20809 net.cpp:184] Created Layer res3a_branch2b/bn (20)
I0815 19:04:18.933670 20809 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0815 19:04:18.933675 20809 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0815 19:04:18.934293 20809 net.cpp:245] Setting up res3a_branch2b/bn
I0815 19:04:18.934301 20809 net.cpp:252] TRAIN Top shape for layer 20 'res3a_branch2b/bn' 6 128 80 80 (4915200)
I0815 19:04:18.934317 20809 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0815 19:04:18.934321 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.934327 20809 net.cpp:184] Created Layer res3a_branch2b/relu (21)
I0815 19:04:18.934330 20809 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0815 19:04:18.934335 20809 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0815 19:04:18.934341 20809 net.cpp:245] Setting up res3a_branch2b/relu
I0815 19:04:18.934345 20809 net.cpp:252] TRAIN Top shape for layer 21 'res3a_branch2b/relu' 6 128 80 80 (4915200)
I0815 19:04:18.934350 20809 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0815 19:04:18.934355 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.934360 20809 net.cpp:184] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (22)
I0815 19:04:18.934363 20809 net.cpp:561] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0815 19:04:18.934367 20809 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0815 19:04:18.934372 20809 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0815 19:04:18.934417 20809 net.cpp:245] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0815 19:04:18.934422 20809 net.cpp:252] TRAIN Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 6 128 80 80 (4915200)
I0815 19:04:18.934427 20809 net.cpp:252] TRAIN Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 6 128 80 80 (4915200)
I0815 19:04:18.934432 20809 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0815 19:04:18.934435 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.934442 20809 net.cpp:184] Created Layer pool3 (23)
I0815 19:04:18.934445 20809 net.cpp:561] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0815 19:04:18.934449 20809 net.cpp:530] pool3 -> pool3
I0815 19:04:18.934514 20809 net.cpp:245] Setting up pool3
I0815 19:04:18.934518 20809 net.cpp:252] TRAIN Top shape for layer 23 'pool3' 6 128 40 40 (1228800)
I0815 19:04:18.934523 20809 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0815 19:04:18.934527 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.934536 20809 net.cpp:184] Created Layer res4a_branch2a (24)
I0815 19:04:18.934540 20809 net.cpp:561] res4a_branch2a <- pool3
I0815 19:04:18.934545 20809 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0815 19:04:18.958847 20809 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.38G, req 0G)
I0815 19:04:18.958868 20809 net.cpp:245] Setting up res4a_branch2a
I0815 19:04:18.958875 20809 net.cpp:252] TRAIN Top shape for layer 24 'res4a_branch2a' 6 256 40 40 (2457600)
I0815 19:04:18.958884 20809 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0815 19:04:18.958889 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.958899 20809 net.cpp:184] Created Layer res4a_branch2a/bn (25)
I0815 19:04:18.958904 20809 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0815 19:04:18.958909 20809 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0815 19:04:18.959615 20809 net.cpp:245] Setting up res4a_branch2a/bn
I0815 19:04:18.959623 20809 net.cpp:252] TRAIN Top shape for layer 25 'res4a_branch2a/bn' 6 256 40 40 (2457600)
I0815 19:04:18.959632 20809 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0815 19:04:18.959636 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.959642 20809 net.cpp:184] Created Layer res4a_branch2a/relu (26)
I0815 19:04:18.959646 20809 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0815 19:04:18.959659 20809 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0815 19:04:18.959666 20809 net.cpp:245] Setting up res4a_branch2a/relu
I0815 19:04:18.959671 20809 net.cpp:252] TRAIN Top shape for layer 26 'res4a_branch2a/relu' 6 256 40 40 (2457600)
I0815 19:04:18.959674 20809 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0815 19:04:18.959679 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.959692 20809 net.cpp:184] Created Layer res4a_branch2b (27)
I0815 19:04:18.959694 20809 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0815 19:04:18.959698 20809 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0815 19:04:18.968643 20809 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.36G, req 0G)
I0815 19:04:18.968657 20809 net.cpp:245] Setting up res4a_branch2b
I0815 19:04:18.968664 20809 net.cpp:252] TRAIN Top shape for layer 27 'res4a_branch2b' 6 256 40 40 (2457600)
I0815 19:04:18.968672 20809 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0815 19:04:18.968677 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.968683 20809 net.cpp:184] Created Layer res4a_branch2b/bn (28)
I0815 19:04:18.968688 20809 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0815 19:04:18.968693 20809 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0815 19:04:18.969334 20809 net.cpp:245] Setting up res4a_branch2b/bn
I0815 19:04:18.969343 20809 net.cpp:252] TRAIN Top shape for layer 28 'res4a_branch2b/bn' 6 256 40 40 (2457600)
I0815 19:04:18.969352 20809 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0815 19:04:18.969355 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.969360 20809 net.cpp:184] Created Layer res4a_branch2b/relu (29)
I0815 19:04:18.969364 20809 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0815 19:04:18.969368 20809 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0815 19:04:18.969375 20809 net.cpp:245] Setting up res4a_branch2b/relu
I0815 19:04:18.969379 20809 net.cpp:252] TRAIN Top shape for layer 29 'res4a_branch2b/relu' 6 256 40 40 (2457600)
I0815 19:04:18.969383 20809 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0815 19:04:18.969388 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.969393 20809 net.cpp:184] Created Layer pool4 (30)
I0815 19:04:18.969398 20809 net.cpp:561] pool4 <- res4a_branch2b
I0815 19:04:18.969403 20809 net.cpp:530] pool4 -> pool4
I0815 19:04:18.969471 20809 net.cpp:245] Setting up pool4
I0815 19:04:18.969476 20809 net.cpp:252] TRAIN Top shape for layer 30 'pool4' 6 256 40 40 (2457600)
I0815 19:04:18.969481 20809 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0815 19:04:18.969485 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.969497 20809 net.cpp:184] Created Layer res5a_branch2a (31)
I0815 19:04:18.969501 20809 net.cpp:561] res5a_branch2a <- pool4
I0815 19:04:18.969506 20809 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0815 19:04:18.995772 20809 net.cpp:245] Setting up res5a_branch2a
I0815 19:04:18.995793 20809 net.cpp:252] TRAIN Top shape for layer 31 'res5a_branch2a' 6 512 40 40 (4915200)
I0815 19:04:18.995803 20809 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0815 19:04:18.995810 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.995828 20809 net.cpp:184] Created Layer res5a_branch2a/bn (32)
I0815 19:04:18.995833 20809 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0815 19:04:18.995838 20809 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0815 19:04:18.996480 20809 net.cpp:245] Setting up res5a_branch2a/bn
I0815 19:04:18.996490 20809 net.cpp:252] TRAIN Top shape for layer 32 'res5a_branch2a/bn' 6 512 40 40 (4915200)
I0815 19:04:18.996507 20809 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0815 19:04:18.996512 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.996517 20809 net.cpp:184] Created Layer res5a_branch2a/relu (33)
I0815 19:04:18.996521 20809 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0815 19:04:18.996526 20809 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0815 19:04:18.996532 20809 net.cpp:245] Setting up res5a_branch2a/relu
I0815 19:04:18.996536 20809 net.cpp:252] TRAIN Top shape for layer 33 'res5a_branch2a/relu' 6 512 40 40 (4915200)
I0815 19:04:18.996541 20809 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0815 19:04:18.996546 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.996554 20809 net.cpp:184] Created Layer res5a_branch2b (34)
I0815 19:04:18.996558 20809 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0815 19:04:18.996562 20809 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0815 19:04:19.010489 20809 net.cpp:245] Setting up res5a_branch2b
I0815 19:04:19.010516 20809 net.cpp:252] TRAIN Top shape for layer 34 'res5a_branch2b' 6 512 40 40 (4915200)
I0815 19:04:19.010532 20809 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0815 19:04:19.010537 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.010547 20809 net.cpp:184] Created Layer res5a_branch2b/bn (35)
I0815 19:04:19.010551 20809 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0815 19:04:19.010556 20809 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0815 19:04:19.011183 20809 net.cpp:245] Setting up res5a_branch2b/bn
I0815 19:04:19.011191 20809 net.cpp:252] TRAIN Top shape for layer 35 'res5a_branch2b/bn' 6 512 40 40 (4915200)
I0815 19:04:19.011200 20809 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0815 19:04:19.011204 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.011209 20809 net.cpp:184] Created Layer res5a_branch2b/relu (36)
I0815 19:04:19.011222 20809 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0815 19:04:19.011227 20809 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0815 19:04:19.011234 20809 net.cpp:245] Setting up res5a_branch2b/relu
I0815 19:04:19.011240 20809 net.cpp:252] TRAIN Top shape for layer 36 'res5a_branch2b/relu' 6 512 40 40 (4915200)
I0815 19:04:19.011243 20809 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I0815 19:04:19.011248 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.011260 20809 net.cpp:184] Created Layer out5a (37)
I0815 19:04:19.011263 20809 net.cpp:561] out5a <- res5a_branch2b
I0815 19:04:19.011266 20809 net.cpp:530] out5a -> out5a
I0815 19:04:19.015564 20809 net.cpp:245] Setting up out5a
I0815 19:04:19.015585 20809 net.cpp:252] TRAIN Top shape for layer 37 'out5a' 6 64 40 40 (614400)
I0815 19:04:19.015594 20809 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I0815 19:04:19.015607 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.015616 20809 net.cpp:184] Created Layer out5a/bn (38)
I0815 19:04:19.015620 20809 net.cpp:561] out5a/bn <- out5a
I0815 19:04:19.015625 20809 net.cpp:513] out5a/bn -> out5a (in-place)
I0815 19:04:19.016278 20809 net.cpp:245] Setting up out5a/bn
I0815 19:04:19.016288 20809 net.cpp:252] TRAIN Top shape for layer 38 'out5a/bn' 6 64 40 40 (614400)
I0815 19:04:19.016295 20809 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I0815 19:04:19.016299 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.016305 20809 net.cpp:184] Created Layer out5a/relu (39)
I0815 19:04:19.016309 20809 net.cpp:561] out5a/relu <- out5a
I0815 19:04:19.016321 20809 net.cpp:513] out5a/relu -> out5a (in-place)
I0815 19:04:19.016329 20809 net.cpp:245] Setting up out5a/relu
I0815 19:04:19.016332 20809 net.cpp:252] TRAIN Top shape for layer 39 'out5a/relu' 6 64 40 40 (614400)
I0815 19:04:19.016337 20809 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I0815 19:04:19.016341 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.060957 20809 net.cpp:184] Created Layer out5a_up2 (40)
I0815 19:04:19.060968 20809 net.cpp:561] out5a_up2 <- out5a
I0815 19:04:19.060971 20809 net.cpp:530] out5a_up2 -> out5a_up2
I0815 19:04:19.061307 20809 net.cpp:245] Setting up out5a_up2
I0815 19:04:19.061316 20809 net.cpp:252] TRAIN Top shape for layer 40 'out5a_up2' 6 64 80 80 (2457600)
I0815 19:04:19.061319 20809 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I0815 19:04:19.061321 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.061329 20809 net.cpp:184] Created Layer out3a (41)
I0815 19:04:19.061331 20809 net.cpp:561] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0815 19:04:19.061334 20809 net.cpp:530] out3a -> out3a
I0815 19:04:19.072268 20809 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 1 3  (limit 7.3G, req 0G)
I0815 19:04:19.072280 20809 net.cpp:245] Setting up out3a
I0815 19:04:19.072284 20809 net.cpp:252] TRAIN Top shape for layer 41 'out3a' 6 64 80 80 (2457600)
I0815 19:04:19.072288 20809 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I0815 19:04:19.072291 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.072296 20809 net.cpp:184] Created Layer out3a/bn (42)
I0815 19:04:19.072299 20809 net.cpp:561] out3a/bn <- out3a
I0815 19:04:19.072301 20809 net.cpp:513] out3a/bn -> out3a (in-place)
I0815 19:04:19.072959 20809 net.cpp:245] Setting up out3a/bn
I0815 19:04:19.072966 20809 net.cpp:252] TRAIN Top shape for layer 42 'out3a/bn' 6 64 80 80 (2457600)
I0815 19:04:19.072973 20809 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I0815 19:04:19.072974 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.072978 20809 net.cpp:184] Created Layer out3a/relu (43)
I0815 19:04:19.072980 20809 net.cpp:561] out3a/relu <- out3a
I0815 19:04:19.072983 20809 net.cpp:513] out3a/relu -> out3a (in-place)
I0815 19:04:19.072986 20809 net.cpp:245] Setting up out3a/relu
I0815 19:04:19.072988 20809 net.cpp:252] TRAIN Top shape for layer 43 'out3a/relu' 6 64 80 80 (2457600)
I0815 19:04:19.072990 20809 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I0815 19:04:19.072993 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.142196 20809 net.cpp:184] Created Layer out3_out5_combined (44)
I0815 19:04:19.142206 20809 net.cpp:561] out3_out5_combined <- out5a_up2
I0815 19:04:19.142210 20809 net.cpp:561] out3_out5_combined <- out3a
I0815 19:04:19.142213 20809 net.cpp:530] out3_out5_combined -> out3_out5_combined
I0815 19:04:19.177711 20809 net.cpp:245] Setting up out3_out5_combined
I0815 19:04:19.177721 20809 net.cpp:252] TRAIN Top shape for layer 44 'out3_out5_combined' 6 64 80 80 (2457600)
I0815 19:04:19.177726 20809 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I0815 19:04:19.177728 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.177742 20809 net.cpp:184] Created Layer ctx_conv1 (45)
I0815 19:04:19.177745 20809 net.cpp:561] ctx_conv1 <- out3_out5_combined
I0815 19:04:19.177747 20809 net.cpp:530] ctx_conv1 -> ctx_conv1
I0815 19:04:19.190714 20809 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 7.24G, req 0G)
I0815 19:04:19.190733 20809 net.cpp:245] Setting up ctx_conv1
I0815 19:04:19.190739 20809 net.cpp:252] TRAIN Top shape for layer 45 'ctx_conv1' 6 64 80 80 (2457600)
I0815 19:04:19.190759 20809 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I0815 19:04:19.190764 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.190773 20809 net.cpp:184] Created Layer ctx_conv1/bn (46)
I0815 19:04:19.190778 20809 net.cpp:561] ctx_conv1/bn <- ctx_conv1
I0815 19:04:19.190783 20809 net.cpp:513] ctx_conv1/bn -> ctx_conv1 (in-place)
I0815 19:04:19.191746 20809 net.cpp:245] Setting up ctx_conv1/bn
I0815 19:04:19.191756 20809 net.cpp:252] TRAIN Top shape for layer 46 'ctx_conv1/bn' 6 64 80 80 (2457600)
I0815 19:04:19.191764 20809 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I0815 19:04:19.191769 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.191774 20809 net.cpp:184] Created Layer ctx_conv1/relu (47)
I0815 19:04:19.191777 20809 net.cpp:561] ctx_conv1/relu <- ctx_conv1
I0815 19:04:19.191781 20809 net.cpp:513] ctx_conv1/relu -> ctx_conv1 (in-place)
I0815 19:04:19.191787 20809 net.cpp:245] Setting up ctx_conv1/relu
I0815 19:04:19.191792 20809 net.cpp:252] TRAIN Top shape for layer 47 'ctx_conv1/relu' 6 64 80 80 (2457600)
I0815 19:04:19.191794 20809 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I0815 19:04:19.191798 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.191807 20809 net.cpp:184] Created Layer ctx_conv2 (48)
I0815 19:04:19.191810 20809 net.cpp:561] ctx_conv2 <- ctx_conv1
I0815 19:04:19.191814 20809 net.cpp:530] ctx_conv2 -> ctx_conv2
I0815 19:04:19.193331 20809 net.cpp:245] Setting up ctx_conv2
I0815 19:04:19.193341 20809 net.cpp:252] TRAIN Top shape for layer 48 'ctx_conv2' 6 64 80 80 (2457600)
I0815 19:04:19.193346 20809 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I0815 19:04:19.193351 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.193356 20809 net.cpp:184] Created Layer ctx_conv2/bn (49)
I0815 19:04:19.193361 20809 net.cpp:561] ctx_conv2/bn <- ctx_conv2
I0815 19:04:19.193364 20809 net.cpp:513] ctx_conv2/bn -> ctx_conv2 (in-place)
I0815 19:04:19.194257 20809 net.cpp:245] Setting up ctx_conv2/bn
I0815 19:04:19.194265 20809 net.cpp:252] TRAIN Top shape for layer 49 'ctx_conv2/bn' 6 64 80 80 (2457600)
I0815 19:04:19.194274 20809 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I0815 19:04:19.194278 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.194283 20809 net.cpp:184] Created Layer ctx_conv2/relu (50)
I0815 19:04:19.194286 20809 net.cpp:561] ctx_conv2/relu <- ctx_conv2
I0815 19:04:19.194290 20809 net.cpp:513] ctx_conv2/relu -> ctx_conv2 (in-place)
I0815 19:04:19.194295 20809 net.cpp:245] Setting up ctx_conv2/relu
I0815 19:04:19.194299 20809 net.cpp:252] TRAIN Top shape for layer 50 'ctx_conv2/relu' 6 64 80 80 (2457600)
I0815 19:04:19.194303 20809 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I0815 19:04:19.194308 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.194314 20809 net.cpp:184] Created Layer ctx_conv3 (51)
I0815 19:04:19.194317 20809 net.cpp:561] ctx_conv3 <- ctx_conv2
I0815 19:04:19.194321 20809 net.cpp:530] ctx_conv3 -> ctx_conv3
I0815 19:04:19.195812 20809 net.cpp:245] Setting up ctx_conv3
I0815 19:04:19.195822 20809 net.cpp:252] TRAIN Top shape for layer 51 'ctx_conv3' 6 64 80 80 (2457600)
I0815 19:04:19.195827 20809 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I0815 19:04:19.195832 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.195837 20809 net.cpp:184] Created Layer ctx_conv3/bn (52)
I0815 19:04:19.195840 20809 net.cpp:561] ctx_conv3/bn <- ctx_conv3
I0815 19:04:19.195843 20809 net.cpp:513] ctx_conv3/bn -> ctx_conv3 (in-place)
I0815 19:04:19.196735 20809 net.cpp:245] Setting up ctx_conv3/bn
I0815 19:04:19.196745 20809 net.cpp:252] TRAIN Top shape for layer 52 'ctx_conv3/bn' 6 64 80 80 (2457600)
I0815 19:04:19.196753 20809 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I0815 19:04:19.196756 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.196761 20809 net.cpp:184] Created Layer ctx_conv3/relu (53)
I0815 19:04:19.196764 20809 net.cpp:561] ctx_conv3/relu <- ctx_conv3
I0815 19:04:19.196768 20809 net.cpp:513] ctx_conv3/relu -> ctx_conv3 (in-place)
I0815 19:04:19.196772 20809 net.cpp:245] Setting up ctx_conv3/relu
I0815 19:04:19.196777 20809 net.cpp:252] TRAIN Top shape for layer 53 'ctx_conv3/relu' 6 64 80 80 (2457600)
I0815 19:04:19.196781 20809 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I0815 19:04:19.196784 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.196791 20809 net.cpp:184] Created Layer ctx_conv4 (54)
I0815 19:04:19.196795 20809 net.cpp:561] ctx_conv4 <- ctx_conv3
I0815 19:04:19.196799 20809 net.cpp:530] ctx_conv4 -> ctx_conv4
I0815 19:04:19.198465 20809 net.cpp:245] Setting up ctx_conv4
I0815 19:04:19.198478 20809 net.cpp:252] TRAIN Top shape for layer 54 'ctx_conv4' 6 64 80 80 (2457600)
I0815 19:04:19.198487 20809 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I0815 19:04:19.198492 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.198511 20809 net.cpp:184] Created Layer ctx_conv4/bn (55)
I0815 19:04:19.198518 20809 net.cpp:561] ctx_conv4/bn <- ctx_conv4
I0815 19:04:19.198523 20809 net.cpp:513] ctx_conv4/bn -> ctx_conv4 (in-place)
I0815 19:04:19.199522 20809 net.cpp:245] Setting up ctx_conv4/bn
I0815 19:04:19.199532 20809 net.cpp:252] TRAIN Top shape for layer 55 'ctx_conv4/bn' 6 64 80 80 (2457600)
I0815 19:04:19.199540 20809 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I0815 19:04:19.199543 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.199548 20809 net.cpp:184] Created Layer ctx_conv4/relu (56)
I0815 19:04:19.199553 20809 net.cpp:561] ctx_conv4/relu <- ctx_conv4
I0815 19:04:19.199556 20809 net.cpp:513] ctx_conv4/relu -> ctx_conv4 (in-place)
I0815 19:04:19.199561 20809 net.cpp:245] Setting up ctx_conv4/relu
I0815 19:04:19.199566 20809 net.cpp:252] TRAIN Top shape for layer 56 'ctx_conv4/relu' 6 64 80 80 (2457600)
I0815 19:04:19.199569 20809 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I0815 19:04:19.199573 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.199581 20809 net.cpp:184] Created Layer ctx_final (57)
I0815 19:04:19.199585 20809 net.cpp:561] ctx_final <- ctx_conv4
I0815 19:04:19.199590 20809 net.cpp:530] ctx_final -> ctx_final
I0815 19:04:19.212684 20809 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 7.22G, req 0G)
I0815 19:04:19.212702 20809 net.cpp:245] Setting up ctx_final
I0815 19:04:19.212709 20809 net.cpp:252] TRAIN Top shape for layer 57 'ctx_final' 6 8 80 80 (307200)
I0815 19:04:19.212716 20809 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I0815 19:04:19.212721 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.212728 20809 net.cpp:184] Created Layer ctx_final/relu (58)
I0815 19:04:19.212733 20809 net.cpp:561] ctx_final/relu <- ctx_final
I0815 19:04:19.212738 20809 net.cpp:513] ctx_final/relu -> ctx_final (in-place)
I0815 19:04:19.212743 20809 net.cpp:245] Setting up ctx_final/relu
I0815 19:04:19.212748 20809 net.cpp:252] TRAIN Top shape for layer 58 'ctx_final/relu' 6 8 80 80 (307200)
I0815 19:04:19.212749 20809 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I0815 19:04:19.212751 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.212769 20809 net.cpp:184] Created Layer out_deconv_final_up2 (59)
I0815 19:04:19.212772 20809 net.cpp:561] out_deconv_final_up2 <- ctx_final
I0815 19:04:19.212774 20809 net.cpp:530] out_deconv_final_up2 -> out_deconv_final_up2
I0815 19:04:19.213148 20809 net.cpp:245] Setting up out_deconv_final_up2
I0815 19:04:19.213155 20809 net.cpp:252] TRAIN Top shape for layer 59 'out_deconv_final_up2' 6 8 160 160 (1228800)
I0815 19:04:19.213160 20809 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I0815 19:04:19.213162 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.356034 20809 net.cpp:184] Created Layer out_deconv_final_up4 (60)
I0815 19:04:19.356055 20809 net.cpp:561] out_deconv_final_up4 <- out_deconv_final_up2
I0815 19:04:19.356061 20809 net.cpp:530] out_deconv_final_up4 -> out_deconv_final_up4
I0815 19:04:19.356451 20809 net.cpp:245] Setting up out_deconv_final_up4
I0815 19:04:19.356461 20809 net.cpp:252] TRAIN Top shape for layer 60 'out_deconv_final_up4' 6 8 320 320 (4915200)
I0815 19:04:19.356465 20809 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I0815 19:04:19.356468 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.356475 20809 net.cpp:184] Created Layer out_deconv_final_up8 (61)
I0815 19:04:19.356478 20809 net.cpp:561] out_deconv_final_up8 <- out_deconv_final_up4
I0815 19:04:19.356480 20809 net.cpp:530] out_deconv_final_up8 -> out_deconv_final_up8
I0815 19:04:19.356745 20809 net.cpp:245] Setting up out_deconv_final_up8
I0815 19:04:19.356750 20809 net.cpp:252] TRAIN Top shape for layer 61 'out_deconv_final_up8' 6 8 640 640 (19660800)
I0815 19:04:19.356753 20809 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0815 19:04:19.356756 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.356765 20809 net.cpp:184] Created Layer loss (62)
I0815 19:04:19.356768 20809 net.cpp:561] loss <- out_deconv_final_up8
I0815 19:04:19.356770 20809 net.cpp:561] loss <- label
I0815 19:04:19.356775 20809 net.cpp:530] loss -> loss
I0815 19:04:19.358168 20809 net.cpp:245] Setting up loss
I0815 19:04:19.358177 20809 net.cpp:252] TRAIN Top shape for layer 62 'loss' (1)
I0815 19:04:19.358180 20809 net.cpp:256]     with loss weight 1
I0815 19:04:19.358186 20809 net.cpp:323] loss needs backward computation.
I0815 19:04:19.358187 20809 net.cpp:323] out_deconv_final_up8 needs backward computation.
I0815 19:04:19.358189 20809 net.cpp:323] out_deconv_final_up4 needs backward computation.
I0815 19:04:19.358191 20809 net.cpp:323] out_deconv_final_up2 needs backward computation.
I0815 19:04:19.358193 20809 net.cpp:323] ctx_final/relu needs backward computation.
I0815 19:04:19.358196 20809 net.cpp:323] ctx_final needs backward computation.
I0815 19:04:19.358196 20809 net.cpp:323] ctx_conv4/relu needs backward computation.
I0815 19:04:19.358198 20809 net.cpp:323] ctx_conv4/bn needs backward computation.
I0815 19:04:19.358201 20809 net.cpp:323] ctx_conv4 needs backward computation.
I0815 19:04:19.358203 20809 net.cpp:323] ctx_conv3/relu needs backward computation.
I0815 19:04:19.358204 20809 net.cpp:323] ctx_conv3/bn needs backward computation.
I0815 19:04:19.358206 20809 net.cpp:323] ctx_conv3 needs backward computation.
I0815 19:04:19.358208 20809 net.cpp:323] ctx_conv2/relu needs backward computation.
I0815 19:04:19.358211 20809 net.cpp:323] ctx_conv2/bn needs backward computation.
I0815 19:04:19.358213 20809 net.cpp:323] ctx_conv2 needs backward computation.
I0815 19:04:19.358216 20809 net.cpp:323] ctx_conv1/relu needs backward computation.
I0815 19:04:19.358218 20809 net.cpp:323] ctx_conv1/bn needs backward computation.
I0815 19:04:19.358219 20809 net.cpp:323] ctx_conv1 needs backward computation.
I0815 19:04:19.358222 20809 net.cpp:323] out3_out5_combined needs backward computation.
I0815 19:04:19.358224 20809 net.cpp:323] out3a/relu needs backward computation.
I0815 19:04:19.358234 20809 net.cpp:323] out3a/bn needs backward computation.
I0815 19:04:19.358237 20809 net.cpp:323] out3a needs backward computation.
I0815 19:04:19.358238 20809 net.cpp:323] out5a_up2 needs backward computation.
I0815 19:04:19.358240 20809 net.cpp:323] out5a/relu needs backward computation.
I0815 19:04:19.358242 20809 net.cpp:323] out5a/bn needs backward computation.
I0815 19:04:19.358244 20809 net.cpp:323] out5a needs backward computation.
I0815 19:04:19.358247 20809 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0815 19:04:19.358248 20809 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0815 19:04:19.358249 20809 net.cpp:323] res5a_branch2b needs backward computation.
I0815 19:04:19.358252 20809 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0815 19:04:19.358253 20809 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0815 19:04:19.358254 20809 net.cpp:323] res5a_branch2a needs backward computation.
I0815 19:04:19.358256 20809 net.cpp:323] pool4 needs backward computation.
I0815 19:04:19.358258 20809 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0815 19:04:19.358260 20809 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0815 19:04:19.358263 20809 net.cpp:323] res4a_branch2b needs backward computation.
I0815 19:04:19.358264 20809 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0815 19:04:19.358266 20809 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0815 19:04:19.358268 20809 net.cpp:323] res4a_branch2a needs backward computation.
I0815 19:04:19.358269 20809 net.cpp:323] pool3 needs backward computation.
I0815 19:04:19.358271 20809 net.cpp:323] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I0815 19:04:19.358273 20809 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0815 19:04:19.358275 20809 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0815 19:04:19.358278 20809 net.cpp:323] res3a_branch2b needs backward computation.
I0815 19:04:19.358279 20809 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0815 19:04:19.358281 20809 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0815 19:04:19.358284 20809 net.cpp:323] res3a_branch2a needs backward computation.
I0815 19:04:19.358286 20809 net.cpp:323] pool2 needs backward computation.
I0815 19:04:19.358289 20809 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0815 19:04:19.358290 20809 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0815 19:04:19.358292 20809 net.cpp:323] res2a_branch2b needs backward computation.
I0815 19:04:19.358295 20809 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0815 19:04:19.358297 20809 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0815 19:04:19.358300 20809 net.cpp:323] res2a_branch2a needs backward computation.
I0815 19:04:19.358302 20809 net.cpp:323] pool1 needs backward computation.
I0815 19:04:19.358305 20809 net.cpp:323] conv1b/relu needs backward computation.
I0815 19:04:19.358309 20809 net.cpp:323] conv1b/bn needs backward computation.
I0815 19:04:19.358310 20809 net.cpp:323] conv1b needs backward computation.
I0815 19:04:19.358312 20809 net.cpp:323] conv1a/relu needs backward computation.
I0815 19:04:19.358314 20809 net.cpp:323] conv1a/bn needs backward computation.
I0815 19:04:19.358317 20809 net.cpp:323] conv1a needs backward computation.
I0815 19:04:19.358319 20809 net.cpp:325] data/bias does not need backward computation.
I0815 19:04:19.358324 20809 net.cpp:325] data does not need backward computation.
I0815 19:04:19.358325 20809 net.cpp:367] This network produces output loss
I0815 19:04:19.358376 20809 net.cpp:389] Top memory (TRAIN) required for data: 956006400 diff: 946176008
I0815 19:04:19.358379 20809 net.cpp:392] Bottom memory (TRAIN) required for data: 956006400 diff: 956006400
I0815 19:04:19.358381 20809 net.cpp:395] Shared (in-place) memory (TRAIN) by data: 630374400 diff: 630374400
I0815 19:04:19.358383 20809 net.cpp:398] Parameters memory (TRAIN) required for data: 2692608 diff: 2692608
I0815 19:04:19.358388 20809 net.cpp:401] Parameters shared memory (TRAIN) by data: 0 diff: 0
I0815 19:04:19.358392 20809 net.cpp:407] Network initialization done.
I0815 19:04:19.358944 20809 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/initial/test.prototxt
W0815 19:04:19.359001 20809 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0815 19:04:19.359176 20809 net.cpp:72] Initializing net from parameters: 
name: "jsegnet21v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/val-image-lmdb"
    label_list_path: "data/val-label-lmdb"
    batch_size: 2
    threads: 1
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
  accuracy_param {
    ignore_label: 255
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
    ignore_label: 255
  }
}
I0815 19:04:19.359328 20809 net.cpp:104] Using FLOAT as default forward math type
I0815 19:04:19.359333 20809 net.cpp:110] Using FLOAT as default backward math type
I0815 19:04:19.359335 20809 layer_factory.hpp:136] Creating layer 'data' of type 'ImageLabelData'
I0815 19:04:19.359342 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.359347 20809 net.cpp:184] Created Layer data (0)
I0815 19:04:19.359350 20809 net.cpp:530] data -> data
I0815 19:04:19.359354 20809 net.cpp:530] data -> label
I0815 19:04:19.359371 20809 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0815 19:04:19.359377 20809 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 19:04:19.440485 20869 db_lmdb.cpp:24] Opened lmdb data/val-image-lmdb
I0815 19:04:19.564152 20809 data_layer.cpp:185] (0) ReshapePrefetch 2, 3, 640, 640
I0815 19:04:19.564262 20809 data_layer.cpp:209] (0) Output data size: 2, 3, 640, 640
I0815 19:04:19.564270 20809 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 19:04:19.564309 20809 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0815 19:04:19.564321 20809 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 19:04:19.565315 20884 data_layer.cpp:97] (0) Parser threads: 1
I0815 19:04:19.565331 20884 data_layer.cpp:99] (0) Transformer threads: 1
I0815 19:04:19.623591 20885 db_lmdb.cpp:24] Opened lmdb data/val-label-lmdb
I0815 19:04:19.694633 20809 data_layer.cpp:185] (0) ReshapePrefetch 2, 1, 640, 640
I0815 19:04:19.694720 20809 data_layer.cpp:209] (0) Output data size: 2, 1, 640, 640
I0815 19:04:19.694728 20809 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 19:04:19.694775 20809 net.cpp:245] Setting up data
I0815 19:04:19.694785 20809 net.cpp:252] TEST Top shape for layer 0 'data' 2 3 640 640 (2457600)
I0815 19:04:19.694794 20809 net.cpp:252] TEST Top shape for layer 0 'data' 2 1 640 640 (819200)
I0815 19:04:19.694802 20809 layer_factory.hpp:136] Creating layer 'label_data_1_split' of type 'Split'
I0815 19:04:19.694809 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.694821 20809 net.cpp:184] Created Layer label_data_1_split (1)
I0815 19:04:19.694825 20809 net.cpp:561] label_data_1_split <- label
I0815 19:04:19.694831 20809 net.cpp:530] label_data_1_split -> label_data_1_split_0
I0815 19:04:19.694838 20809 net.cpp:530] label_data_1_split -> label_data_1_split_1
I0815 19:04:19.694842 20809 net.cpp:530] label_data_1_split -> label_data_1_split_2
I0815 19:04:19.695014 20809 net.cpp:245] Setting up label_data_1_split
I0815 19:04:19.695024 20809 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0815 19:04:19.695029 20809 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0815 19:04:19.695034 20809 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0815 19:04:19.695039 20809 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0815 19:04:19.695044 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.695052 20809 net.cpp:184] Created Layer data/bias (2)
I0815 19:04:19.695055 20809 net.cpp:561] data/bias <- data
I0815 19:04:19.695060 20809 net.cpp:530] data/bias -> data/bias
I0815 19:04:19.695915 20886 data_layer.cpp:97] (0) Parser threads: 1
I0815 19:04:19.695925 20886 data_layer.cpp:99] (0) Transformer threads: 1
I0815 19:04:19.697540 20809 net.cpp:245] Setting up data/bias
I0815 19:04:19.697552 20809 net.cpp:252] TEST Top shape for layer 2 'data/bias' 2 3 640 640 (2457600)
I0815 19:04:19.697561 20809 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0815 19:04:19.697567 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.697582 20809 net.cpp:184] Created Layer conv1a (3)
I0815 19:04:19.697585 20809 net.cpp:561] conv1a <- data/bias
I0815 19:04:19.697590 20809 net.cpp:530] conv1a -> conv1a
I0815 19:04:19.701495 20809 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.1G, req 0G)
I0815 19:04:19.701508 20809 net.cpp:245] Setting up conv1a
I0815 19:04:19.701514 20809 net.cpp:252] TEST Top shape for layer 3 'conv1a' 2 32 320 320 (6553600)
I0815 19:04:19.701531 20809 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0815 19:04:19.701536 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.701545 20809 net.cpp:184] Created Layer conv1a/bn (4)
I0815 19:04:19.701550 20809 net.cpp:561] conv1a/bn <- conv1a
I0815 19:04:19.701553 20809 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0815 19:04:19.702247 20809 net.cpp:245] Setting up conv1a/bn
I0815 19:04:19.702255 20809 net.cpp:252] TEST Top shape for layer 4 'conv1a/bn' 2 32 320 320 (6553600)
I0815 19:04:19.702265 20809 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0815 19:04:19.702270 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.702275 20809 net.cpp:184] Created Layer conv1a/relu (5)
I0815 19:04:19.702280 20809 net.cpp:561] conv1a/relu <- conv1a
I0815 19:04:19.702282 20809 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0815 19:04:19.702289 20809 net.cpp:245] Setting up conv1a/relu
I0815 19:04:19.702296 20809 net.cpp:252] TEST Top shape for layer 5 'conv1a/relu' 2 32 320 320 (6553600)
I0815 19:04:19.702298 20809 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0815 19:04:19.702301 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.702308 20809 net.cpp:184] Created Layer conv1b (6)
I0815 19:04:19.702312 20809 net.cpp:561] conv1b <- conv1a
I0815 19:04:19.702316 20809 net.cpp:530] conv1b -> conv1b
I0815 19:04:19.715555 20809 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.07G, req 0G)
I0815 19:04:19.715567 20809 net.cpp:245] Setting up conv1b
I0815 19:04:19.715571 20809 net.cpp:252] TEST Top shape for layer 6 'conv1b' 2 32 320 320 (6553600)
I0815 19:04:19.715577 20809 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0815 19:04:19.715580 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.715585 20809 net.cpp:184] Created Layer conv1b/bn (7)
I0815 19:04:19.715589 20809 net.cpp:561] conv1b/bn <- conv1b
I0815 19:04:19.715590 20809 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0815 19:04:19.716389 20809 net.cpp:245] Setting up conv1b/bn
I0815 19:04:19.716398 20809 net.cpp:252] TEST Top shape for layer 7 'conv1b/bn' 2 32 320 320 (6553600)
I0815 19:04:19.716403 20809 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0815 19:04:19.716408 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.716415 20809 net.cpp:184] Created Layer conv1b/relu (8)
I0815 19:04:19.716418 20809 net.cpp:561] conv1b/relu <- conv1b
I0815 19:04:19.716421 20809 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0815 19:04:19.716425 20809 net.cpp:245] Setting up conv1b/relu
I0815 19:04:19.716428 20809 net.cpp:252] TEST Top shape for layer 8 'conv1b/relu' 2 32 320 320 (6553600)
I0815 19:04:19.716430 20809 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0815 19:04:19.716434 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.716439 20809 net.cpp:184] Created Layer pool1 (9)
I0815 19:04:19.716444 20809 net.cpp:561] pool1 <- conv1b
I0815 19:04:19.716447 20809 net.cpp:530] pool1 -> pool1
I0815 19:04:19.716526 20809 net.cpp:245] Setting up pool1
I0815 19:04:19.716531 20809 net.cpp:252] TEST Top shape for layer 9 'pool1' 2 32 160 160 (1638400)
I0815 19:04:19.716533 20809 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0815 19:04:19.716537 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.716547 20809 net.cpp:184] Created Layer res2a_branch2a (10)
I0815 19:04:19.716550 20809 net.cpp:561] res2a_branch2a <- pool1
I0815 19:04:19.716553 20809 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0815 19:04:19.724673 20809 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.05G, req 0G)
I0815 19:04:19.724692 20809 net.cpp:245] Setting up res2a_branch2a
I0815 19:04:19.724697 20809 net.cpp:252] TEST Top shape for layer 10 'res2a_branch2a' 2 64 160 160 (3276800)
I0815 19:04:19.724705 20809 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0815 19:04:19.724710 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.724715 20809 net.cpp:184] Created Layer res2a_branch2a/bn (11)
I0815 19:04:19.724720 20809 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0815 19:04:19.724723 20809 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0815 19:04:19.725432 20809 net.cpp:245] Setting up res2a_branch2a/bn
I0815 19:04:19.725440 20809 net.cpp:252] TEST Top shape for layer 11 'res2a_branch2a/bn' 2 64 160 160 (3276800)
I0815 19:04:19.725445 20809 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0815 19:04:19.725448 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.725451 20809 net.cpp:184] Created Layer res2a_branch2a/relu (12)
I0815 19:04:19.725455 20809 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0815 19:04:19.725456 20809 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0815 19:04:19.725461 20809 net.cpp:245] Setting up res2a_branch2a/relu
I0815 19:04:19.725463 20809 net.cpp:252] TEST Top shape for layer 12 'res2a_branch2a/relu' 2 64 160 160 (3276800)
I0815 19:04:19.725466 20809 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0815 19:04:19.725468 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.725474 20809 net.cpp:184] Created Layer res2a_branch2b (13)
I0815 19:04:19.725479 20809 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0815 19:04:19.725482 20809 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0815 19:04:19.731919 20809 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.03G, req 0G)
I0815 19:04:19.731935 20809 net.cpp:245] Setting up res2a_branch2b
I0815 19:04:19.731940 20809 net.cpp:252] TEST Top shape for layer 13 'res2a_branch2b' 2 64 160 160 (3276800)
I0815 19:04:19.731945 20809 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0815 19:04:19.731952 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.731972 20809 net.cpp:184] Created Layer res2a_branch2b/bn (14)
I0815 19:04:19.731978 20809 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0815 19:04:19.731983 20809 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0815 19:04:19.732772 20809 net.cpp:245] Setting up res2a_branch2b/bn
I0815 19:04:19.732782 20809 net.cpp:252] TEST Top shape for layer 14 'res2a_branch2b/bn' 2 64 160 160 (3276800)
I0815 19:04:19.732789 20809 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0815 19:04:19.732794 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.732797 20809 net.cpp:184] Created Layer res2a_branch2b/relu (15)
I0815 19:04:19.732801 20809 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0815 19:04:19.732805 20809 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0815 19:04:19.732811 20809 net.cpp:245] Setting up res2a_branch2b/relu
I0815 19:04:19.732816 20809 net.cpp:252] TEST Top shape for layer 15 'res2a_branch2b/relu' 2 64 160 160 (3276800)
I0815 19:04:19.732827 20809 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0815 19:04:19.732832 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.732841 20809 net.cpp:184] Created Layer pool2 (16)
I0815 19:04:19.732846 20809 net.cpp:561] pool2 <- res2a_branch2b
I0815 19:04:19.732849 20809 net.cpp:530] pool2 -> pool2
I0815 19:04:19.732930 20809 net.cpp:245] Setting up pool2
I0815 19:04:19.732937 20809 net.cpp:252] TEST Top shape for layer 16 'pool2' 2 64 80 80 (819200)
I0815 19:04:19.732954 20809 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0815 19:04:19.732959 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.732969 20809 net.cpp:184] Created Layer res3a_branch2a (17)
I0815 19:04:19.732972 20809 net.cpp:561] res3a_branch2a <- pool2
I0815 19:04:19.732977 20809 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0815 19:04:19.738621 20809 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.02G, req 0G)
I0815 19:04:19.738634 20809 net.cpp:245] Setting up res3a_branch2a
I0815 19:04:19.738641 20809 net.cpp:252] TEST Top shape for layer 17 'res3a_branch2a' 2 128 80 80 (1638400)
I0815 19:04:19.738648 20809 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0815 19:04:19.738652 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.738661 20809 net.cpp:184] Created Layer res3a_branch2a/bn (18)
I0815 19:04:19.738664 20809 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0815 19:04:19.738668 20809 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0815 19:04:19.739379 20809 net.cpp:245] Setting up res3a_branch2a/bn
I0815 19:04:19.739388 20809 net.cpp:252] TEST Top shape for layer 18 'res3a_branch2a/bn' 2 128 80 80 (1638400)
I0815 19:04:19.739398 20809 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0815 19:04:19.739403 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.739408 20809 net.cpp:184] Created Layer res3a_branch2a/relu (19)
I0815 19:04:19.739411 20809 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0815 19:04:19.739415 20809 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0815 19:04:19.739421 20809 net.cpp:245] Setting up res3a_branch2a/relu
I0815 19:04:19.739426 20809 net.cpp:252] TEST Top shape for layer 19 'res3a_branch2a/relu' 2 128 80 80 (1638400)
I0815 19:04:19.739430 20809 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0815 19:04:19.739434 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.739454 20809 net.cpp:184] Created Layer res3a_branch2b (20)
I0815 19:04:19.739457 20809 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0815 19:04:19.739461 20809 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0815 19:04:19.745003 20809 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7G, req 0G)
I0815 19:04:19.745015 20809 net.cpp:245] Setting up res3a_branch2b
I0815 19:04:19.745021 20809 net.cpp:252] TEST Top shape for layer 20 'res3a_branch2b' 2 128 80 80 (1638400)
I0815 19:04:19.745028 20809 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0815 19:04:19.745033 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.745040 20809 net.cpp:184] Created Layer res3a_branch2b/bn (21)
I0815 19:04:19.745045 20809 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0815 19:04:19.745049 20809 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0815 19:04:19.745736 20809 net.cpp:245] Setting up res3a_branch2b/bn
I0815 19:04:19.745744 20809 net.cpp:252] TEST Top shape for layer 21 'res3a_branch2b/bn' 2 128 80 80 (1638400)
I0815 19:04:19.745754 20809 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0815 19:04:19.745757 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.745761 20809 net.cpp:184] Created Layer res3a_branch2b/relu (22)
I0815 19:04:19.745765 20809 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0815 19:04:19.745769 20809 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0815 19:04:19.745775 20809 net.cpp:245] Setting up res3a_branch2b/relu
I0815 19:04:19.745780 20809 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b/relu' 2 128 80 80 (1638400)
I0815 19:04:19.745790 20809 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0815 19:04:19.745795 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.745800 20809 net.cpp:184] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (23)
I0815 19:04:19.745805 20809 net.cpp:561] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0815 19:04:19.745810 20809 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0815 19:04:19.745815 20809 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0815 19:04:19.745862 20809 net.cpp:245] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0815 19:04:19.745868 20809 net.cpp:252] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 2 128 80 80 (1638400)
I0815 19:04:19.745873 20809 net.cpp:252] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 2 128 80 80 (1638400)
I0815 19:04:19.745877 20809 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0815 19:04:19.745882 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.745888 20809 net.cpp:184] Created Layer pool3 (24)
I0815 19:04:19.745893 20809 net.cpp:561] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0815 19:04:19.745896 20809 net.cpp:530] pool3 -> pool3
I0815 19:04:19.745968 20809 net.cpp:245] Setting up pool3
I0815 19:04:19.745975 20809 net.cpp:252] TEST Top shape for layer 24 'pool3' 2 128 40 40 (409600)
I0815 19:04:19.745978 20809 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0815 19:04:19.745982 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.745995 20809 net.cpp:184] Created Layer res4a_branch2a (25)
I0815 19:04:19.745999 20809 net.cpp:561] res4a_branch2a <- pool3
I0815 19:04:19.746003 20809 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0815 19:04:19.757181 20809 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.99G, req 0G)
I0815 19:04:19.757199 20809 net.cpp:245] Setting up res4a_branch2a
I0815 19:04:19.757205 20809 net.cpp:252] TEST Top shape for layer 25 'res4a_branch2a' 2 256 40 40 (819200)
I0815 19:04:19.757212 20809 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0815 19:04:19.757218 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.757290 20809 net.cpp:184] Created Layer res4a_branch2a/bn (26)
I0815 19:04:19.757297 20809 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0815 19:04:19.757302 20809 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0815 19:04:19.758059 20809 net.cpp:245] Setting up res4a_branch2a/bn
I0815 19:04:19.758067 20809 net.cpp:252] TEST Top shape for layer 26 'res4a_branch2a/bn' 2 256 40 40 (819200)
I0815 19:04:19.758076 20809 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0815 19:04:19.758081 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.758086 20809 net.cpp:184] Created Layer res4a_branch2a/relu (27)
I0815 19:04:19.758090 20809 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0815 19:04:19.758095 20809 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0815 19:04:19.758101 20809 net.cpp:245] Setting up res4a_branch2a/relu
I0815 19:04:19.758106 20809 net.cpp:252] TEST Top shape for layer 27 'res4a_branch2a/relu' 2 256 40 40 (819200)
I0815 19:04:19.758111 20809 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0815 19:04:19.758114 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.758124 20809 net.cpp:184] Created Layer res4a_branch2b (28)
I0815 19:04:19.758136 20809 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0815 19:04:19.758141 20809 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0815 19:04:19.764926 20809 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.99G, req 0G)
I0815 19:04:19.764940 20809 net.cpp:245] Setting up res4a_branch2b
I0815 19:04:19.764946 20809 net.cpp:252] TEST Top shape for layer 28 'res4a_branch2b' 2 256 40 40 (819200)
I0815 19:04:19.764953 20809 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0815 19:04:19.764960 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.764971 20809 net.cpp:184] Created Layer res4a_branch2b/bn (29)
I0815 19:04:19.764976 20809 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0815 19:04:19.764981 20809 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0815 19:04:19.765697 20809 net.cpp:245] Setting up res4a_branch2b/bn
I0815 19:04:19.765705 20809 net.cpp:252] TEST Top shape for layer 29 'res4a_branch2b/bn' 2 256 40 40 (819200)
I0815 19:04:19.765714 20809 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0815 19:04:19.765718 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.765723 20809 net.cpp:184] Created Layer res4a_branch2b/relu (30)
I0815 19:04:19.765727 20809 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0815 19:04:19.765732 20809 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0815 19:04:19.765738 20809 net.cpp:245] Setting up res4a_branch2b/relu
I0815 19:04:19.765743 20809 net.cpp:252] TEST Top shape for layer 30 'res4a_branch2b/relu' 2 256 40 40 (819200)
I0815 19:04:19.765746 20809 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0815 19:04:19.765751 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.765758 20809 net.cpp:184] Created Layer pool4 (31)
I0815 19:04:19.765761 20809 net.cpp:561] pool4 <- res4a_branch2b
I0815 19:04:19.765765 20809 net.cpp:530] pool4 -> pool4
I0815 19:04:19.765837 20809 net.cpp:245] Setting up pool4
I0815 19:04:19.765843 20809 net.cpp:252] TEST Top shape for layer 31 'pool4' 2 256 40 40 (819200)
I0815 19:04:19.765847 20809 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0815 19:04:19.765852 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.765867 20809 net.cpp:184] Created Layer res5a_branch2a (32)
I0815 19:04:19.765872 20809 net.cpp:561] res5a_branch2a <- pool4
I0815 19:04:19.765875 20809 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0815 19:04:19.791191 20809 net.cpp:245] Setting up res5a_branch2a
I0815 19:04:19.791216 20809 net.cpp:252] TEST Top shape for layer 32 'res5a_branch2a' 2 512 40 40 (1638400)
I0815 19:04:19.791225 20809 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0815 19:04:19.791230 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.791241 20809 net.cpp:184] Created Layer res5a_branch2a/bn (33)
I0815 19:04:19.791246 20809 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0815 19:04:19.791251 20809 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0815 19:04:19.791942 20809 net.cpp:245] Setting up res5a_branch2a/bn
I0815 19:04:19.791950 20809 net.cpp:252] TEST Top shape for layer 33 'res5a_branch2a/bn' 2 512 40 40 (1638400)
I0815 19:04:19.791960 20809 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0815 19:04:19.791965 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.791970 20809 net.cpp:184] Created Layer res5a_branch2a/relu (34)
I0815 19:04:19.791973 20809 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0815 19:04:19.791977 20809 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0815 19:04:19.791985 20809 net.cpp:245] Setting up res5a_branch2a/relu
I0815 19:04:19.791998 20809 net.cpp:252] TEST Top shape for layer 34 'res5a_branch2a/relu' 2 512 40 40 (1638400)
I0815 19:04:19.792002 20809 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0815 19:04:19.792006 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.792016 20809 net.cpp:184] Created Layer res5a_branch2b (35)
I0815 19:04:19.792019 20809 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0815 19:04:19.792023 20809 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0815 19:04:19.805243 20809 net.cpp:245] Setting up res5a_branch2b
I0815 19:04:19.805258 20809 net.cpp:252] TEST Top shape for layer 35 'res5a_branch2b' 2 512 40 40 (1638400)
I0815 19:04:19.805269 20809 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0815 19:04:19.805275 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.805282 20809 net.cpp:184] Created Layer res5a_branch2b/bn (36)
I0815 19:04:19.805287 20809 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0815 19:04:19.805291 20809 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0815 19:04:19.805970 20809 net.cpp:245] Setting up res5a_branch2b/bn
I0815 19:04:19.805979 20809 net.cpp:252] TEST Top shape for layer 36 'res5a_branch2b/bn' 2 512 40 40 (1638400)
I0815 19:04:19.805987 20809 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0815 19:04:19.805991 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.805997 20809 net.cpp:184] Created Layer res5a_branch2b/relu (37)
I0815 19:04:19.806001 20809 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0815 19:04:19.806005 20809 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0815 19:04:19.806011 20809 net.cpp:245] Setting up res5a_branch2b/relu
I0815 19:04:19.806016 20809 net.cpp:252] TEST Top shape for layer 37 'res5a_branch2b/relu' 2 512 40 40 (1638400)
I0815 19:04:19.806020 20809 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I0815 19:04:19.806025 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.806037 20809 net.cpp:184] Created Layer out5a (38)
I0815 19:04:19.806041 20809 net.cpp:561] out5a <- res5a_branch2b
I0815 19:04:19.806046 20809 net.cpp:530] out5a -> out5a
I0815 19:04:19.809345 20809 net.cpp:245] Setting up out5a
I0815 19:04:19.809353 20809 net.cpp:252] TEST Top shape for layer 38 'out5a' 2 64 40 40 (204800)
I0815 19:04:19.809360 20809 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I0815 19:04:19.809363 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.809370 20809 net.cpp:184] Created Layer out5a/bn (39)
I0815 19:04:19.809375 20809 net.cpp:561] out5a/bn <- out5a
I0815 19:04:19.809378 20809 net.cpp:513] out5a/bn -> out5a (in-place)
I0815 19:04:19.810066 20809 net.cpp:245] Setting up out5a/bn
I0815 19:04:19.810075 20809 net.cpp:252] TEST Top shape for layer 39 'out5a/bn' 2 64 40 40 (204800)
I0815 19:04:19.810082 20809 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I0815 19:04:19.810086 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.810091 20809 net.cpp:184] Created Layer out5a/relu (40)
I0815 19:04:19.810094 20809 net.cpp:561] out5a/relu <- out5a
I0815 19:04:19.810098 20809 net.cpp:513] out5a/relu -> out5a (in-place)
I0815 19:04:19.810103 20809 net.cpp:245] Setting up out5a/relu
I0815 19:04:19.810108 20809 net.cpp:252] TEST Top shape for layer 40 'out5a/relu' 2 64 40 40 (204800)
I0815 19:04:19.810112 20809 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I0815 19:04:19.810117 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.810128 20809 net.cpp:184] Created Layer out5a_up2 (41)
I0815 19:04:19.810132 20809 net.cpp:561] out5a_up2 <- out5a
I0815 19:04:19.810143 20809 net.cpp:530] out5a_up2 -> out5a_up2
I0815 19:04:19.810441 20809 net.cpp:245] Setting up out5a_up2
I0815 19:04:19.810446 20809 net.cpp:252] TEST Top shape for layer 41 'out5a_up2' 2 64 80 80 (819200)
I0815 19:04:19.810452 20809 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I0815 19:04:19.810456 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.810466 20809 net.cpp:184] Created Layer out3a (42)
I0815 19:04:19.810468 20809 net.cpp:561] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0815 19:04:19.810473 20809 net.cpp:530] out3a -> out3a
I0815 19:04:19.814461 20809 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.97G, req 0G)
I0815 19:04:19.814471 20809 net.cpp:245] Setting up out3a
I0815 19:04:19.814477 20809 net.cpp:252] TEST Top shape for layer 42 'out3a' 2 64 80 80 (819200)
I0815 19:04:19.814484 20809 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I0815 19:04:19.814488 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.814496 20809 net.cpp:184] Created Layer out3a/bn (43)
I0815 19:04:19.814499 20809 net.cpp:561] out3a/bn <- out3a
I0815 19:04:19.814504 20809 net.cpp:513] out3a/bn -> out3a (in-place)
I0815 19:04:19.815438 20809 net.cpp:245] Setting up out3a/bn
I0815 19:04:19.815448 20809 net.cpp:252] TEST Top shape for layer 43 'out3a/bn' 2 64 80 80 (819200)
I0815 19:04:19.815457 20809 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I0815 19:04:19.815461 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.815465 20809 net.cpp:184] Created Layer out3a/relu (44)
I0815 19:04:19.815469 20809 net.cpp:561] out3a/relu <- out3a
I0815 19:04:19.815472 20809 net.cpp:513] out3a/relu -> out3a (in-place)
I0815 19:04:19.815477 20809 net.cpp:245] Setting up out3a/relu
I0815 19:04:19.815481 20809 net.cpp:252] TEST Top shape for layer 44 'out3a/relu' 2 64 80 80 (819200)
I0815 19:04:19.815486 20809 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I0815 19:04:19.815490 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.815495 20809 net.cpp:184] Created Layer out3_out5_combined (45)
I0815 19:04:19.815497 20809 net.cpp:561] out3_out5_combined <- out5a_up2
I0815 19:04:19.815500 20809 net.cpp:561] out3_out5_combined <- out3a
I0815 19:04:19.815503 20809 net.cpp:530] out3_out5_combined -> out3_out5_combined
I0815 19:04:19.816606 20809 net.cpp:245] Setting up out3_out5_combined
I0815 19:04:19.816617 20809 net.cpp:252] TEST Top shape for layer 45 'out3_out5_combined' 2 64 80 80 (819200)
I0815 19:04:19.816622 20809 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I0815 19:04:19.816627 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.816637 20809 net.cpp:184] Created Layer ctx_conv1 (46)
I0815 19:04:19.816640 20809 net.cpp:561] ctx_conv1 <- out3_out5_combined
I0815 19:04:19.816645 20809 net.cpp:530] ctx_conv1 -> ctx_conv1
I0815 19:04:19.820621 20809 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.96G, req 0G)
I0815 19:04:19.820636 20809 net.cpp:245] Setting up ctx_conv1
I0815 19:04:19.820642 20809 net.cpp:252] TEST Top shape for layer 46 'ctx_conv1' 2 64 80 80 (819200)
I0815 19:04:19.820648 20809 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I0815 19:04:19.820653 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.820659 20809 net.cpp:184] Created Layer ctx_conv1/bn (47)
I0815 19:04:19.820664 20809 net.cpp:561] ctx_conv1/bn <- ctx_conv1
I0815 19:04:19.820669 20809 net.cpp:513] ctx_conv1/bn -> ctx_conv1 (in-place)
I0815 19:04:19.821651 20809 net.cpp:245] Setting up ctx_conv1/bn
I0815 19:04:19.821661 20809 net.cpp:252] TEST Top shape for layer 47 'ctx_conv1/bn' 2 64 80 80 (819200)
I0815 19:04:19.821681 20809 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I0815 19:04:19.821684 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.821689 20809 net.cpp:184] Created Layer ctx_conv1/relu (48)
I0815 19:04:19.821693 20809 net.cpp:561] ctx_conv1/relu <- ctx_conv1
I0815 19:04:19.821698 20809 net.cpp:513] ctx_conv1/relu -> ctx_conv1 (in-place)
I0815 19:04:19.821707 20809 net.cpp:245] Setting up ctx_conv1/relu
I0815 19:04:19.821712 20809 net.cpp:252] TEST Top shape for layer 48 'ctx_conv1/relu' 2 64 80 80 (819200)
I0815 19:04:19.821715 20809 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I0815 19:04:19.821720 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.821735 20809 net.cpp:184] Created Layer ctx_conv2 (49)
I0815 19:04:19.821739 20809 net.cpp:561] ctx_conv2 <- ctx_conv1
I0815 19:04:19.821744 20809 net.cpp:530] ctx_conv2 -> ctx_conv2
I0815 19:04:19.823282 20809 net.cpp:245] Setting up ctx_conv2
I0815 19:04:19.823292 20809 net.cpp:252] TEST Top shape for layer 49 'ctx_conv2' 2 64 80 80 (819200)
I0815 19:04:19.823299 20809 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I0815 19:04:19.823303 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.823309 20809 net.cpp:184] Created Layer ctx_conv2/bn (50)
I0815 19:04:19.823313 20809 net.cpp:561] ctx_conv2/bn <- ctx_conv2
I0815 19:04:19.823318 20809 net.cpp:513] ctx_conv2/bn -> ctx_conv2 (in-place)
I0815 19:04:19.824270 20809 net.cpp:245] Setting up ctx_conv2/bn
I0815 19:04:19.824280 20809 net.cpp:252] TEST Top shape for layer 50 'ctx_conv2/bn' 2 64 80 80 (819200)
I0815 19:04:19.824291 20809 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I0815 19:04:19.824295 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.824301 20809 net.cpp:184] Created Layer ctx_conv2/relu (51)
I0815 19:04:19.824304 20809 net.cpp:561] ctx_conv2/relu <- ctx_conv2
I0815 19:04:19.824307 20809 net.cpp:513] ctx_conv2/relu -> ctx_conv2 (in-place)
I0815 19:04:19.824313 20809 net.cpp:245] Setting up ctx_conv2/relu
I0815 19:04:19.824317 20809 net.cpp:252] TEST Top shape for layer 51 'ctx_conv2/relu' 2 64 80 80 (819200)
I0815 19:04:19.824321 20809 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I0815 19:04:19.824326 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.824334 20809 net.cpp:184] Created Layer ctx_conv3 (52)
I0815 19:04:19.824338 20809 net.cpp:561] ctx_conv3 <- ctx_conv2
I0815 19:04:19.824342 20809 net.cpp:530] ctx_conv3 -> ctx_conv3
I0815 19:04:19.825978 20809 net.cpp:245] Setting up ctx_conv3
I0815 19:04:19.825987 20809 net.cpp:252] TEST Top shape for layer 52 'ctx_conv3' 2 64 80 80 (819200)
I0815 19:04:19.825992 20809 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I0815 19:04:19.825995 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.825999 20809 net.cpp:184] Created Layer ctx_conv3/bn (53)
I0815 19:04:19.826002 20809 net.cpp:561] ctx_conv3/bn <- ctx_conv3
I0815 19:04:19.826004 20809 net.cpp:513] ctx_conv3/bn -> ctx_conv3 (in-place)
I0815 19:04:19.826690 20809 net.cpp:245] Setting up ctx_conv3/bn
I0815 19:04:19.826697 20809 net.cpp:252] TEST Top shape for layer 53 'ctx_conv3/bn' 2 64 80 80 (819200)
I0815 19:04:19.826704 20809 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I0815 19:04:19.826705 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.826709 20809 net.cpp:184] Created Layer ctx_conv3/relu (54)
I0815 19:04:19.826711 20809 net.cpp:561] ctx_conv3/relu <- ctx_conv3
I0815 19:04:19.826714 20809 net.cpp:513] ctx_conv3/relu -> ctx_conv3 (in-place)
I0815 19:04:19.826717 20809 net.cpp:245] Setting up ctx_conv3/relu
I0815 19:04:19.826727 20809 net.cpp:252] TEST Top shape for layer 54 'ctx_conv3/relu' 2 64 80 80 (819200)
I0815 19:04:19.826730 20809 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I0815 19:04:19.826732 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.826737 20809 net.cpp:184] Created Layer ctx_conv4 (55)
I0815 19:04:19.826740 20809 net.cpp:561] ctx_conv4 <- ctx_conv3
I0815 19:04:19.826742 20809 net.cpp:530] ctx_conv4 -> ctx_conv4
I0815 19:04:19.827847 20809 net.cpp:245] Setting up ctx_conv4
I0815 19:04:19.827853 20809 net.cpp:252] TEST Top shape for layer 55 'ctx_conv4' 2 64 80 80 (819200)
I0815 19:04:19.827858 20809 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I0815 19:04:19.827862 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.827864 20809 net.cpp:184] Created Layer ctx_conv4/bn (56)
I0815 19:04:19.827867 20809 net.cpp:561] ctx_conv4/bn <- ctx_conv4
I0815 19:04:19.827869 20809 net.cpp:513] ctx_conv4/bn -> ctx_conv4 (in-place)
I0815 19:04:19.828569 20809 net.cpp:245] Setting up ctx_conv4/bn
I0815 19:04:19.828575 20809 net.cpp:252] TEST Top shape for layer 56 'ctx_conv4/bn' 2 64 80 80 (819200)
I0815 19:04:19.828582 20809 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I0815 19:04:19.828584 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.828588 20809 net.cpp:184] Created Layer ctx_conv4/relu (57)
I0815 19:04:19.828589 20809 net.cpp:561] ctx_conv4/relu <- ctx_conv4
I0815 19:04:19.828593 20809 net.cpp:513] ctx_conv4/relu -> ctx_conv4 (in-place)
I0815 19:04:19.828595 20809 net.cpp:245] Setting up ctx_conv4/relu
I0815 19:04:19.828598 20809 net.cpp:252] TEST Top shape for layer 57 'ctx_conv4/relu' 2 64 80 80 (819200)
I0815 19:04:19.828601 20809 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I0815 19:04:19.828603 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.828608 20809 net.cpp:184] Created Layer ctx_final (58)
I0815 19:04:19.828611 20809 net.cpp:561] ctx_final <- ctx_conv4
I0815 19:04:19.828613 20809 net.cpp:530] ctx_final -> ctx_final
I0815 19:04:19.833222 20809 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.96G, req 0G)
I0815 19:04:19.833235 20809 net.cpp:245] Setting up ctx_final
I0815 19:04:19.833240 20809 net.cpp:252] TEST Top shape for layer 58 'ctx_final' 2 8 80 80 (102400)
I0815 19:04:19.833248 20809 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I0815 19:04:19.833252 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.833258 20809 net.cpp:184] Created Layer ctx_final/relu (59)
I0815 19:04:19.833263 20809 net.cpp:561] ctx_final/relu <- ctx_final
I0815 19:04:19.833267 20809 net.cpp:513] ctx_final/relu -> ctx_final (in-place)
I0815 19:04:19.833282 20809 net.cpp:245] Setting up ctx_final/relu
I0815 19:04:19.833287 20809 net.cpp:252] TEST Top shape for layer 59 'ctx_final/relu' 2 8 80 80 (102400)
I0815 19:04:19.833292 20809 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I0815 19:04:19.833297 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.833307 20809 net.cpp:184] Created Layer out_deconv_final_up2 (60)
I0815 19:04:19.833312 20809 net.cpp:561] out_deconv_final_up2 <- ctx_final
I0815 19:04:19.833315 20809 net.cpp:530] out_deconv_final_up2 -> out_deconv_final_up2
I0815 19:04:19.833632 20809 net.cpp:245] Setting up out_deconv_final_up2
I0815 19:04:19.833640 20809 net.cpp:252] TEST Top shape for layer 60 'out_deconv_final_up2' 2 8 160 160 (409600)
I0815 19:04:19.833645 20809 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I0815 19:04:19.833650 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.833664 20809 net.cpp:184] Created Layer out_deconv_final_up4 (61)
I0815 19:04:19.833668 20809 net.cpp:561] out_deconv_final_up4 <- out_deconv_final_up2
I0815 19:04:19.833673 20809 net.cpp:530] out_deconv_final_up4 -> out_deconv_final_up4
I0815 19:04:19.833966 20809 net.cpp:245] Setting up out_deconv_final_up4
I0815 19:04:19.833972 20809 net.cpp:252] TEST Top shape for layer 61 'out_deconv_final_up4' 2 8 320 320 (1638400)
I0815 19:04:19.833978 20809 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I0815 19:04:19.833982 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.833989 20809 net.cpp:184] Created Layer out_deconv_final_up8 (62)
I0815 19:04:19.833992 20809 net.cpp:561] out_deconv_final_up8 <- out_deconv_final_up4
I0815 19:04:19.833997 20809 net.cpp:530] out_deconv_final_up8 -> out_deconv_final_up8
I0815 19:04:19.834290 20809 net.cpp:245] Setting up out_deconv_final_up8
I0815 19:04:19.834296 20809 net.cpp:252] TEST Top shape for layer 62 'out_deconv_final_up8' 2 8 640 640 (6553600)
I0815 19:04:19.834302 20809 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8_out_deconv_final_up8_0_split' of type 'Split'
I0815 19:04:19.834307 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.834312 20809 net.cpp:184] Created Layer out_deconv_final_up8_out_deconv_final_up8_0_split (63)
I0815 19:04:19.834316 20809 net.cpp:561] out_deconv_final_up8_out_deconv_final_up8_0_split <- out_deconv_final_up8
I0815 19:04:19.834321 20809 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0815 19:04:19.834326 20809 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0815 19:04:19.834331 20809 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0815 19:04:19.834403 20809 net.cpp:245] Setting up out_deconv_final_up8_out_deconv_final_up8_0_split
I0815 19:04:19.834408 20809 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 8 640 640 (6553600)
I0815 19:04:19.834414 20809 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 8 640 640 (6553600)
I0815 19:04:19.834419 20809 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 8 640 640 (6553600)
I0815 19:04:19.834424 20809 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0815 19:04:19.834427 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.834435 20809 net.cpp:184] Created Layer loss (64)
I0815 19:04:19.834439 20809 net.cpp:561] loss <- out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0815 19:04:19.834444 20809 net.cpp:561] loss <- label_data_1_split_0
I0815 19:04:19.834447 20809 net.cpp:530] loss -> loss
I0815 19:04:19.835253 20809 net.cpp:245] Setting up loss
I0815 19:04:19.835263 20809 net.cpp:252] TEST Top shape for layer 64 'loss' (1)
I0815 19:04:19.835266 20809 net.cpp:256]     with loss weight 1
I0815 19:04:19.835273 20809 layer_factory.hpp:136] Creating layer 'accuracy/top1' of type 'Accuracy'
I0815 19:04:19.835278 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.835286 20809 net.cpp:184] Created Layer accuracy/top1 (65)
I0815 19:04:19.835290 20809 net.cpp:561] accuracy/top1 <- out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0815 19:04:19.835295 20809 net.cpp:561] accuracy/top1 <- label_data_1_split_1
I0815 19:04:19.835300 20809 net.cpp:530] accuracy/top1 -> accuracy/top1
I0815 19:04:19.835307 20809 net.cpp:245] Setting up accuracy/top1
I0815 19:04:19.835311 20809 net.cpp:252] TEST Top shape for layer 65 'accuracy/top1' (1)
I0815 19:04:19.835316 20809 layer_factory.hpp:136] Creating layer 'accuracy/top5' of type 'Accuracy'
I0815 19:04:19.835325 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.835330 20809 net.cpp:184] Created Layer accuracy/top5 (66)
I0815 19:04:19.835335 20809 net.cpp:561] accuracy/top5 <- out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0815 19:04:19.835340 20809 net.cpp:561] accuracy/top5 <- label_data_1_split_2
I0815 19:04:19.835345 20809 net.cpp:530] accuracy/top5 -> accuracy/top5
I0815 19:04:19.835351 20809 net.cpp:245] Setting up accuracy/top5
I0815 19:04:19.835355 20809 net.cpp:252] TEST Top shape for layer 66 'accuracy/top5' (1)
I0815 19:04:19.835360 20809 net.cpp:325] accuracy/top5 does not need backward computation.
I0815 19:04:19.835364 20809 net.cpp:325] accuracy/top1 does not need backward computation.
I0815 19:04:19.835368 20809 net.cpp:323] loss needs backward computation.
I0815 19:04:19.835372 20809 net.cpp:323] out_deconv_final_up8_out_deconv_final_up8_0_split needs backward computation.
I0815 19:04:19.835376 20809 net.cpp:323] out_deconv_final_up8 needs backward computation.
I0815 19:04:19.835381 20809 net.cpp:323] out_deconv_final_up4 needs backward computation.
I0815 19:04:19.835384 20809 net.cpp:323] out_deconv_final_up2 needs backward computation.
I0815 19:04:19.835388 20809 net.cpp:323] ctx_final/relu needs backward computation.
I0815 19:04:19.835392 20809 net.cpp:323] ctx_final needs backward computation.
I0815 19:04:19.835397 20809 net.cpp:323] ctx_conv4/relu needs backward computation.
I0815 19:04:19.835399 20809 net.cpp:323] ctx_conv4/bn needs backward computation.
I0815 19:04:19.835403 20809 net.cpp:323] ctx_conv4 needs backward computation.
I0815 19:04:19.835407 20809 net.cpp:323] ctx_conv3/relu needs backward computation.
I0815 19:04:19.835410 20809 net.cpp:323] ctx_conv3/bn needs backward computation.
I0815 19:04:19.835414 20809 net.cpp:323] ctx_conv3 needs backward computation.
I0815 19:04:19.835417 20809 net.cpp:323] ctx_conv2/relu needs backward computation.
I0815 19:04:19.835422 20809 net.cpp:323] ctx_conv2/bn needs backward computation.
I0815 19:04:19.835425 20809 net.cpp:323] ctx_conv2 needs backward computation.
I0815 19:04:19.835428 20809 net.cpp:323] ctx_conv1/relu needs backward computation.
I0815 19:04:19.835433 20809 net.cpp:323] ctx_conv1/bn needs backward computation.
I0815 19:04:19.835436 20809 net.cpp:323] ctx_conv1 needs backward computation.
I0815 19:04:19.835440 20809 net.cpp:323] out3_out5_combined needs backward computation.
I0815 19:04:19.835444 20809 net.cpp:323] out3a/relu needs backward computation.
I0815 19:04:19.835448 20809 net.cpp:323] out3a/bn needs backward computation.
I0815 19:04:19.835451 20809 net.cpp:323] out3a needs backward computation.
I0815 19:04:19.835455 20809 net.cpp:323] out5a_up2 needs backward computation.
I0815 19:04:19.835460 20809 net.cpp:323] out5a/relu needs backward computation.
I0815 19:04:19.835464 20809 net.cpp:323] out5a/bn needs backward computation.
I0815 19:04:19.835467 20809 net.cpp:323] out5a needs backward computation.
I0815 19:04:19.835471 20809 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0815 19:04:19.835475 20809 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0815 19:04:19.835479 20809 net.cpp:323] res5a_branch2b needs backward computation.
I0815 19:04:19.835484 20809 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0815 19:04:19.835487 20809 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0815 19:04:19.835491 20809 net.cpp:323] res5a_branch2a needs backward computation.
I0815 19:04:19.835494 20809 net.cpp:323] pool4 needs backward computation.
I0815 19:04:19.835500 20809 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0815 19:04:19.835502 20809 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0815 19:04:19.835506 20809 net.cpp:323] res4a_branch2b needs backward computation.
I0815 19:04:19.835510 20809 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0815 19:04:19.835515 20809 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0815 19:04:19.835518 20809 net.cpp:323] res4a_branch2a needs backward computation.
I0815 19:04:19.835526 20809 net.cpp:323] pool3 needs backward computation.
I0815 19:04:19.835530 20809 net.cpp:323] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I0815 19:04:19.835536 20809 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0815 19:04:19.835539 20809 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0815 19:04:19.835542 20809 net.cpp:323] res3a_branch2b needs backward computation.
I0815 19:04:19.835547 20809 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0815 19:04:19.835551 20809 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0815 19:04:19.835554 20809 net.cpp:323] res3a_branch2a needs backward computation.
I0815 19:04:19.835558 20809 net.cpp:323] pool2 needs backward computation.
I0815 19:04:19.835562 20809 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0815 19:04:19.835566 20809 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0815 19:04:19.835571 20809 net.cpp:323] res2a_branch2b needs backward computation.
I0815 19:04:19.835574 20809 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0815 19:04:19.835578 20809 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0815 19:04:19.835582 20809 net.cpp:323] res2a_branch2a needs backward computation.
I0815 19:04:19.835587 20809 net.cpp:323] pool1 needs backward computation.
I0815 19:04:19.835590 20809 net.cpp:323] conv1b/relu needs backward computation.
I0815 19:04:19.835594 20809 net.cpp:323] conv1b/bn needs backward computation.
I0815 19:04:19.835598 20809 net.cpp:323] conv1b needs backward computation.
I0815 19:04:19.835602 20809 net.cpp:323] conv1a/relu needs backward computation.
I0815 19:04:19.835605 20809 net.cpp:323] conv1a/bn needs backward computation.
I0815 19:04:19.835609 20809 net.cpp:323] conv1a needs backward computation.
I0815 19:04:19.835613 20809 net.cpp:325] data/bias does not need backward computation.
I0815 19:04:19.835618 20809 net.cpp:325] label_data_1_split does not need backward computation.
I0815 19:04:19.835623 20809 net.cpp:325] data does not need backward computation.
I0815 19:04:19.835626 20809 net.cpp:367] This network produces output accuracy/top1
I0815 19:04:19.835630 20809 net.cpp:367] This network produces output accuracy/top5
I0815 19:04:19.835633 20809 net.cpp:367] This network produces output loss
I0815 19:04:19.835677 20809 net.cpp:389] Top memory (TEST) required for data: 318668800 diff: 8
I0815 19:04:19.835681 20809 net.cpp:392] Bottom memory (TEST) required for data: 318668800 diff: 318668800
I0815 19:04:19.835685 20809 net.cpp:395] Shared (in-place) memory (TEST) by data: 210124800 diff: 210124800
I0815 19:04:19.835688 20809 net.cpp:398] Parameters memory (TEST) required for data: 2692608 diff: 2692608
I0815 19:04:19.835691 20809 net.cpp:401] Parameters shared memory (TEST) by data: 0 diff: 0
I0815 19:04:19.835695 20809 net.cpp:407] Network initialization done.
I0815 19:04:19.835768 20809 solver.cpp:56] Solver scaffolding done.
I0815 19:04:19.845239 20809 caffe.cpp:137] Finetuning from training/imagenet_jacintonet11v2_iter_320000.caffemodel
I0815 19:04:20.107897 20809 net.cpp:1095] Copying source layer data Type:Data #blobs=0
I0815 19:04:20.107915 20809 net.cpp:1095] Copying source layer data/bias Type:Bias #blobs=1
I0815 19:04:20.107923 20809 net.cpp:1095] Copying source layer conv1a Type:Convolution #blobs=2
I0815 19:04:20.107960 20809 net.cpp:1095] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0815 19:04:20.108508 20809 net.cpp:1108] BN legacy DIGITS format detected ... 
I0815 19:04:20.108516 20809 net.cpp:1114] BN Transforming to new format completed.
I0815 19:04:20.108518 20809 net.cpp:1095] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0815 19:04:20.108520 20809 net.cpp:1095] Copying source layer conv1b Type:Convolution #blobs=2
I0815 19:04:20.108559 20809 net.cpp:1095] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0815 19:04:20.108923 20809 net.cpp:1108] BN legacy DIGITS format detected ... 
I0815 19:04:20.108929 20809 net.cpp:1114] BN Transforming to new format completed.
I0815 19:04:20.108939 20809 net.cpp:1095] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0815 19:04:20.108942 20809 net.cpp:1095] Copying source layer pool1 Type:Pooling #blobs=0
I0815 19:04:20.108943 20809 net.cpp:1095] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0815 19:04:20.109194 20809 net.cpp:1095] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0815 19:04:20.109568 20809 net.cpp:1108] BN legacy DIGITS format detected ... 
I0815 19:04:20.109575 20809 net.cpp:1114] BN Transforming to new format completed.
I0815 19:04:20.109576 20809 net.cpp:1095] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0815 19:04:20.109578 20809 net.cpp:1095] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0815 19:04:20.109706 20809 net.cpp:1095] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0815 19:04:20.110069 20809 net.cpp:1108] BN legacy DIGITS format detected ... 
I0815 19:04:20.110074 20809 net.cpp:1114] BN Transforming to new format completed.
I0815 19:04:20.110075 20809 net.cpp:1095] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0815 19:04:20.110077 20809 net.cpp:1095] Copying source layer pool2 Type:Pooling #blobs=0
I0815 19:04:20.110080 20809 net.cpp:1095] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0815 19:04:20.111053 20809 net.cpp:1095] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0815 19:04:20.111390 20809 net.cpp:1108] BN legacy DIGITS format detected ... 
I0815 19:04:20.111395 20809 net.cpp:1114] BN Transforming to new format completed.
I0815 19:04:20.111398 20809 net.cpp:1095] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0815 19:04:20.111400 20809 net.cpp:1095] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0815 19:04:20.111891 20809 net.cpp:1095] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0815 19:04:20.112227 20809 net.cpp:1108] BN legacy DIGITS format detected ... 
I0815 19:04:20.112233 20809 net.cpp:1114] BN Transforming to new format completed.
I0815 19:04:20.112236 20809 net.cpp:1095] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0815 19:04:20.112239 20809 net.cpp:1095] Copying source layer pool3 Type:Pooling #blobs=0
I0815 19:04:20.112243 20809 net.cpp:1095] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0815 19:04:20.116119 20809 net.cpp:1095] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0815 19:04:20.116466 20809 net.cpp:1108] BN legacy DIGITS format detected ... 
I0815 19:04:20.116472 20809 net.cpp:1114] BN Transforming to new format completed.
I0815 19:04:20.116473 20809 net.cpp:1095] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0815 19:04:20.116475 20809 net.cpp:1095] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0815 19:04:20.118419 20809 net.cpp:1095] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0815 19:04:20.118758 20809 net.cpp:1108] BN legacy DIGITS format detected ... 
I0815 19:04:20.118763 20809 net.cpp:1114] BN Transforming to new format completed.
I0815 19:04:20.118765 20809 net.cpp:1095] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0815 19:04:20.118768 20809 net.cpp:1095] Copying source layer pool4 Type:Pooling #blobs=0
I0815 19:04:20.118772 20809 net.cpp:1095] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0815 19:04:20.134233 20809 net.cpp:1095] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0815 19:04:20.134603 20809 net.cpp:1108] BN legacy DIGITS format detected ... 
I0815 19:04:20.134608 20809 net.cpp:1114] BN Transforming to new format completed.
I0815 19:04:20.134610 20809 net.cpp:1095] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0815 19:04:20.134613 20809 net.cpp:1095] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0815 19:04:20.142344 20809 net.cpp:1095] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0815 19:04:20.142698 20809 net.cpp:1108] BN legacy DIGITS format detected ... 
I0815 19:04:20.142709 20809 net.cpp:1114] BN Transforming to new format completed.
I0815 19:04:20.142712 20809 net.cpp:1095] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0815 19:04:20.142714 20809 net.cpp:1079] Ignoring source layer pool5
I0815 19:04:20.142716 20809 net.cpp:1079] Ignoring source layer fc1000
I0815 19:04:20.142719 20809 net.cpp:1095] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0815 19:04:20.151964 20809 net.cpp:1095] Copying source layer data Type:Data #blobs=0
I0815 19:04:20.151983 20809 net.cpp:1095] Copying source layer data/bias Type:Bias #blobs=1
I0815 19:04:20.151988 20809 net.cpp:1095] Copying source layer conv1a Type:Convolution #blobs=2
I0815 19:04:20.152024 20809 net.cpp:1095] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0815 19:04:20.152531 20809 net.cpp:1108] BN legacy DIGITS format detected ... 
I0815 19:04:20.152539 20809 net.cpp:1114] BN Transforming to new format completed.
I0815 19:04:20.152540 20809 net.cpp:1095] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0815 19:04:20.152542 20809 net.cpp:1095] Copying source layer conv1b Type:Convolution #blobs=2
I0815 19:04:20.152580 20809 net.cpp:1095] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0815 19:04:20.152945 20809 net.cpp:1108] BN legacy DIGITS format detected ... 
I0815 19:04:20.152951 20809 net.cpp:1114] BN Transforming to new format completed.
I0815 19:04:20.152953 20809 net.cpp:1095] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0815 19:04:20.152958 20809 net.cpp:1095] Copying source layer pool1 Type:Pooling #blobs=0
I0815 19:04:20.152962 20809 net.cpp:1095] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0815 19:04:20.153211 20809 net.cpp:1095] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0815 19:04:20.153587 20809 net.cpp:1108] BN legacy DIGITS format detected ... 
I0815 19:04:20.153594 20809 net.cpp:1114] BN Transforming to new format completed.
I0815 19:04:20.153595 20809 net.cpp:1095] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0815 19:04:20.153597 20809 net.cpp:1095] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0815 19:04:20.153725 20809 net.cpp:1095] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0815 19:04:20.154091 20809 net.cpp:1108] BN legacy DIGITS format detected ... 
I0815 19:04:20.154096 20809 net.cpp:1114] BN Transforming to new format completed.
I0815 19:04:20.154099 20809 net.cpp:1095] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0815 19:04:20.154101 20809 net.cpp:1095] Copying source layer pool2 Type:Pooling #blobs=0
I0815 19:04:20.154103 20809 net.cpp:1095] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0815 19:04:20.155076 20809 net.cpp:1095] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0815 19:04:20.155422 20809 net.cpp:1108] BN legacy DIGITS format detected ... 
I0815 19:04:20.155427 20809 net.cpp:1114] BN Transforming to new format completed.
I0815 19:04:20.155429 20809 net.cpp:1095] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0815 19:04:20.155431 20809 net.cpp:1095] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0815 19:04:20.155925 20809 net.cpp:1095] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0815 19:04:20.156266 20809 net.cpp:1108] BN legacy DIGITS format detected ... 
I0815 19:04:20.156271 20809 net.cpp:1114] BN Transforming to new format completed.
I0815 19:04:20.156273 20809 net.cpp:1095] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0815 19:04:20.156275 20809 net.cpp:1095] Copying source layer pool3 Type:Pooling #blobs=0
I0815 19:04:20.156277 20809 net.cpp:1095] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0815 19:04:20.160150 20809 net.cpp:1095] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0815 19:04:20.160502 20809 net.cpp:1108] BN legacy DIGITS format detected ... 
I0815 19:04:20.160507 20809 net.cpp:1114] BN Transforming to new format completed.
I0815 19:04:20.160509 20809 net.cpp:1095] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0815 19:04:20.160519 20809 net.cpp:1095] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0815 19:04:20.162474 20809 net.cpp:1095] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0815 19:04:20.162837 20809 net.cpp:1108] BN legacy DIGITS format detected ... 
I0815 19:04:20.162842 20809 net.cpp:1114] BN Transforming to new format completed.
I0815 19:04:20.162844 20809 net.cpp:1095] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0815 19:04:20.162847 20809 net.cpp:1095] Copying source layer pool4 Type:Pooling #blobs=0
I0815 19:04:20.162848 20809 net.cpp:1095] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0815 19:04:20.178328 20809 net.cpp:1095] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0815 19:04:20.178694 20809 net.cpp:1108] BN legacy DIGITS format detected ... 
I0815 19:04:20.178699 20809 net.cpp:1114] BN Transforming to new format completed.
I0815 19:04:20.178701 20809 net.cpp:1095] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0815 19:04:20.178704 20809 net.cpp:1095] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0815 19:04:20.186466 20809 net.cpp:1095] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0815 19:04:20.186894 20809 net.cpp:1108] BN legacy DIGITS format detected ... 
I0815 19:04:20.186900 20809 net.cpp:1114] BN Transforming to new format completed.
I0815 19:04:20.186903 20809 net.cpp:1095] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0815 19:04:20.186905 20809 net.cpp:1079] Ignoring source layer pool5
I0815 19:04:20.186908 20809 net.cpp:1079] Ignoring source layer fc1000
I0815 19:04:20.186909 20809 net.cpp:1095] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0815 19:04:20.187000 20809 parallel.cpp:106] [0 - 0] P2pSync adding callback
I0815 19:04:20.187005 20809 parallel.cpp:106] [1 - 1] P2pSync adding callback
I0815 19:04:20.187006 20809 parallel.cpp:106] [2 - 2] P2pSync adding callback
I0815 19:04:20.187010 20809 parallel.cpp:59] Starting Optimization
I0815 19:04:20.187013 20809 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 19:04:20.187047 20809 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 19:04:20.187072 20809 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 19:04:20.187785 20887 device_alternate.hpp:116] NVML initialized on thread 136059126073088
I0815 19:04:20.216938 20887 common.cpp:583] NVML succeeded to set CPU affinity on device 0
I0815 19:04:20.216989 20888 device_alternate.hpp:116] NVML initialized on thread 136059117680384
I0815 19:04:20.217978 20888 common.cpp:583] NVML succeeded to set CPU affinity on device 1
I0815 19:04:20.218019 20889 device_alternate.hpp:116] NVML initialized on thread 136059109287680
I0815 19:04:20.218729 20889 common.cpp:583] NVML succeeded to set CPU affinity on device 2
I0815 19:04:20.222512 20888 solver.cpp:42] Solver data type: FLOAT
W0815 19:04:20.223088 20888 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0815 19:04:20.223220 20888 net.cpp:104] Using FLOAT as default forward math type
I0815 19:04:20.223227 20888 net.cpp:110] Using FLOAT as default backward math type
I0815 19:04:20.223263 20888 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0815 19:04:20.223273 20888 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 19:04:20.226574 20889 solver.cpp:42] Solver data type: FLOAT
W0815 19:04:20.227164 20889 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0815 19:04:20.227274 20890 db_lmdb.cpp:24] Opened lmdb data/train-image-lmdb
I0815 19:04:20.227398 20889 net.cpp:104] Using FLOAT as default forward math type
I0815 19:04:20.227404 20889 net.cpp:110] Using FLOAT as default backward math type
I0815 19:04:20.227450 20889 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0815 19:04:20.227473 20889 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 19:04:20.230345 20891 db_lmdb.cpp:24] Opened lmdb data/train-image-lmdb
I0815 19:04:20.230478 20888 data_layer.cpp:185] [1] ReshapePrefetch 6, 3, 640, 640
I0815 19:04:20.230572 20888 data_layer.cpp:209] [1] Output data size: 6, 3, 640, 640
I0815 19:04:20.230587 20888 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 19:04:20.230648 20888 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0815 19:04:20.230664 20888 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 19:04:20.231775 20892 data_layer.cpp:97] [1] Parser threads: 1
I0815 19:04:20.231808 20892 data_layer.cpp:99] [1] Transformer threads: 1
I0815 19:04:20.237700 20893 db_lmdb.cpp:24] Opened lmdb data/train-label-lmdb
I0815 19:04:20.241207 20888 data_layer.cpp:185] [1] ReshapePrefetch 6, 1, 640, 640
I0815 19:04:20.241683 20889 data_layer.cpp:185] [2] ReshapePrefetch 6, 3, 640, 640
I0815 19:04:20.241838 20889 data_layer.cpp:209] [2] Output data size: 6, 3, 640, 640
I0815 19:04:20.241852 20889 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 19:04:20.241943 20889 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0815 19:04:20.241962 20889 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 19:04:20.242064 20888 data_layer.cpp:209] [1] Output data size: 6, 1, 640, 640
I0815 19:04:20.242074 20888 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 19:04:20.243764 20894 data_layer.cpp:97] [2] Parser threads: 1
I0815 19:04:20.243855 20894 data_layer.cpp:99] [2] Transformer threads: 1
I0815 19:04:20.249794 20895 db_lmdb.cpp:24] Opened lmdb data/train-label-lmdb
I0815 19:04:20.251224 20896 data_layer.cpp:97] [1] Parser threads: 1
I0815 19:04:20.251327 20896 data_layer.cpp:99] [1] Transformer threads: 1
I0815 19:04:20.259923 20889 data_layer.cpp:185] [2] ReshapePrefetch 6, 1, 640, 640
I0815 19:04:20.260023 20889 data_layer.cpp:209] [2] Output data size: 6, 1, 640, 640
I0815 19:04:20.260035 20889 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 19:04:20.268067 20897 data_layer.cpp:97] [2] Parser threads: 1
I0815 19:04:20.268101 20897 data_layer.cpp:99] [2] Transformer threads: 1
I0815 19:04:20.790302 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 7.99G, req 0G)
I0815 19:04:20.836558 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 0  (limit 7.99G, req 0G)
I0815 19:04:20.840016 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.82G, req 0G)
I0815 19:04:20.882266 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.7G, req 0G)
I0815 19:04:20.887094 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.83G, req 0G)
I0815 19:04:20.904455 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.62G, req 0G)
I0815 19:04:20.925640 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 7.56G, req 0G)
I0815 19:04:20.929401 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.7G, req 0G)
I0815 19:04:20.938050 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.51G, req 0G)
I0815 19:04:20.952937 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.62G, req 0G)
I0815 19:04:20.965083 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.48G, req 0G)
I0815 19:04:20.977811 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.46G, req 0G)
I0815 19:04:20.978339 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 7.56G, req 0G)
I0815 19:04:20.991750 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.52G, req 0G)
I0815 19:04:21.018362 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.48G, req 0G)
I0815 19:04:21.028920 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.46G, req 0G)
I0815 19:04:21.039433 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 7.39G, req 0G)
I0815 19:04:21.055289 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 7.34G, req 0G)
I0815 19:04:21.077791 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 7.31G, req 0G)
I0815 19:04:21.081231 20888 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/initial/test.prototxt
W0815 19:04:21.081298 20888 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0815 19:04:21.081432 20888 net.cpp:104] Using FLOAT as default forward math type
I0815 19:04:21.081439 20888 net.cpp:110] Using FLOAT as default backward math type
I0815 19:04:21.081467 20888 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0815 19:04:21.081473 20888 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 19:04:21.082231 20933 db_lmdb.cpp:24] Opened lmdb data/val-image-lmdb
I0815 19:04:21.083804 20888 data_layer.cpp:185] (1) ReshapePrefetch 2, 3, 640, 640
I0815 19:04:21.083900 20888 data_layer.cpp:209] (1) Output data size: 2, 3, 640, 640
I0815 19:04:21.083907 20888 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 19:04:21.083950 20888 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0815 19:04:21.083961 20888 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 19:04:21.084702 20934 data_layer.cpp:97] (1) Parser threads: 1
I0815 19:04:21.084715 20934 data_layer.cpp:99] (1) Transformer threads: 1
I0815 19:04:21.087347 20935 db_lmdb.cpp:24] Opened lmdb data/val-label-lmdb
I0815 19:04:21.088629 20888 data_layer.cpp:185] (1) ReshapePrefetch 2, 1, 640, 640
I0815 19:04:21.088796 20888 data_layer.cpp:209] (1) Output data size: 2, 1, 640, 640
I0815 19:04:21.088806 20888 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 19:04:21.089855 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 7.39G, req 0G)
I0815 19:04:21.090257 20936 data_layer.cpp:97] (1) Parser threads: 1
I0815 19:04:21.090266 20936 data_layer.cpp:99] (1) Transformer threads: 1
I0815 19:04:21.100538 20888 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.18G, req 0G)
I0815 19:04:21.112747 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 7.34G, req 0G)
I0815 19:04:21.116387 20888 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.15G, req 0G)
I0815 19:04:21.127382 20888 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.13G, req 0G)
I0815 19:04:21.139648 20888 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.11G, req 0G)
I0815 19:04:21.139892 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 7.32G, req 0G)
I0815 19:04:21.144456 20889 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/initial/test.prototxt
W0815 19:04:21.144558 20889 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0815 19:04:21.144740 20889 net.cpp:104] Using FLOAT as default forward math type
I0815 19:04:21.144755 20889 net.cpp:110] Using FLOAT as default backward math type
I0815 19:04:21.144788 20889 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0815 19:04:21.144805 20889 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 19:04:21.145902 20937 db_lmdb.cpp:24] Opened lmdb data/val-image-lmdb
I0815 19:04:21.147521 20889 data_layer.cpp:185] (2) ReshapePrefetch 2, 3, 640, 640
I0815 19:04:21.147680 20889 data_layer.cpp:209] (2) Output data size: 2, 3, 640, 640
I0815 19:04:21.147688 20889 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 19:04:21.147758 20889 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0815 19:04:21.147789 20889 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 19:04:21.148803 20888 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.1G, req 0G)
I0815 19:04:21.149058 20938 data_layer.cpp:97] (2) Parser threads: 1
I0815 19:04:21.149085 20938 data_layer.cpp:99] (2) Transformer threads: 1
I0815 19:04:21.152585 20939 db_lmdb.cpp:24] Opened lmdb data/val-label-lmdb
I0815 19:04:21.156201 20889 data_layer.cpp:185] (2) ReshapePrefetch 2, 1, 640, 640
I0815 19:04:21.156272 20889 data_layer.cpp:209] (2) Output data size: 2, 1, 640, 640
I0815 19:04:21.156278 20889 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 19:04:21.157889 20940 data_layer.cpp:97] (2) Parser threads: 1
I0815 19:04:21.157901 20940 data_layer.cpp:99] (2) Transformer threads: 1
I0815 19:04:21.164400 20888 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.09G, req 0G)
I0815 19:04:21.168901 20889 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.18G, req 0G)
I0815 19:04:21.178737 20888 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.08G, req 0G)
I0815 19:04:21.185384 20889 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.15G, req 0G)
I0815 19:04:21.188057 20888 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.08G, req 0G)
I0815 19:04:21.196280 20889 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.13G, req 0G)
I0815 19:04:21.204373 20889 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.12G, req 0G)
I0815 19:04:21.211268 20889 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.1G, req 0G)
I0815 19:04:21.217571 20889 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.09G, req 0G)
I0815 19:04:21.230587 20889 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.09G, req 0G)
I0815 19:04:21.238672 20889 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.08G, req 0G)
I0815 19:04:21.248504 20888 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 7.07G, req 0G)
I0815 19:04:21.256043 20888 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 7.05G, req 0G)
I0815 19:04:21.267993 20888 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 7.05G, req 0G)
I0815 19:04:21.270311 20888 solver.cpp:56] Solver scaffolding done.
I0815 19:04:21.291203 20889 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 7.07G, req 0G)
I0815 19:04:21.297050 20889 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 7.06G, req 0G)
I0815 19:04:21.308682 20889 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 7.05G, req 0G)
I0815 19:04:21.311141 20889 solver.cpp:56] Solver scaffolding done.
I0815 19:04:21.373580 20887 parallel.cpp:161] [0 - 0] P2pSync adding callback
I0815 19:04:21.373605 20888 parallel.cpp:161] [1 - 1] P2pSync adding callback
I0815 19:04:21.373606 20889 parallel.cpp:161] [2 - 2] P2pSync adding callback
I0815 19:04:21.563798 20888 solver.cpp:438] Solving jsegnet21v2_train
I0815 19:04:21.563798 20887 solver.cpp:438] Solving jsegnet21v2_train
I0815 19:04:21.563798 20889 solver.cpp:438] Solving jsegnet21v2_train
I0815 19:04:21.563817 20888 solver.cpp:439] Learning Rate Policy: multistep
I0815 19:04:21.563824 20889 solver.cpp:439] Learning Rate Policy: multistep
I0815 19:04:21.563822 20887 solver.cpp:439] Learning Rate Policy: multistep
I0815 19:04:21.577165 20888 solver.cpp:227] Starting Optimization on GPU 1
I0815 19:04:21.577172 20889 solver.cpp:227] Starting Optimization on GPU 2
I0815 19:04:21.577167 20887 solver.cpp:227] Starting Optimization on GPU 0
I0815 19:04:21.577354 20887 solver.cpp:509] Iteration 0, Testing net (#0)
I0815 19:04:21.577358 20955 device_alternate.hpp:116] NVML initialized on thread 128058949420800
I0815 19:04:21.577379 20955 common.cpp:583] NVML succeeded to set CPU affinity on device 2
I0815 19:04:21.577394 20956 device_alternate.hpp:116] NVML initialized on thread 128058957813504
I0815 19:04:21.577409 20956 common.cpp:583] NVML succeeded to set CPU affinity on device 1
I0815 19:04:21.577419 20957 device_alternate.hpp:116] NVML initialized on thread 128058941028096
I0815 19:04:21.577425 20957 common.cpp:583] NVML succeeded to set CPU affinity on device 0
I0815 19:04:21.592221 20888 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 6.95G, req 0G)
I0815 19:04:21.593780 20889 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 6.96G, req 0G)
I0815 19:04:21.608796 20887 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.01G/1 1  (limit 6.88G, req 0G)
I0815 19:04:21.612206 20888 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 6.9G, req 0G)
I0815 19:04:21.613306 20889 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 6.9G, req 0G)
I0815 19:04:21.625893 20888 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 6.84G, req 0G)
I0815 19:04:21.626796 20887 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 6.82G, req 0G)
I0815 19:04:21.627864 20889 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 6.84G, req 0G)
I0815 19:04:21.682943 20888 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 6.81G, req 0G)
I0815 19:04:21.683262 20889 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 6.81G, req 0G)
I0815 19:04:21.686522 20887 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 6.75G, req 0G)
I0815 19:04:21.691627 20888 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 6.77G, req 0G)
I0815 19:04:21.692080 20889 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 6.78G, req 0G)
I0815 19:04:21.696161 20887 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 6.72G, req 0G)
I0815 19:04:21.698230 20888 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 6.76G, req 0G)
I0815 19:04:21.698768 20889 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 6.76G, req 0G)
I0815 19:04:21.703778 20887 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 6.69G, req 0G)
I0815 19:04:21.706073 20888 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.74G, req 0G)
I0815 19:04:21.707621 20889 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.74G, req 0G)
I0815 19:04:21.709588 20887 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 6.67G, req 0G)
I0815 19:04:21.710105 20888 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.73G, req 0G)
I0815 19:04:21.712693 20889 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.74G, req 0G)
I0815 19:04:21.717785 20887 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.65G, req 0G)
I0815 19:04:21.722050 20887 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.65G, req 0G)
I0815 19:04:21.739228 20888 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.58G, req 0G)
I0815 19:04:21.744658 20888 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.57G, req 0G)
I0815 19:04:21.745573 20889 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.58G, req 0G)
I0815 19:04:21.747895 20887 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.49G, req 0G)
I0815 19:04:21.752460 20889 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.57G, req 0G)
I0815 19:04:21.753594 20887 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.48G, req 0G)
I0815 19:04:21.769798 20888 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.48G, req 0G)
I0815 19:04:21.775382 20889 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.48G, req 0G)
I0815 19:04:21.777070 20887 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.39G, req 0G)
I0815 19:04:21.925346 20887 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.0484509
I0815 19:04:21.925366 20887 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.590535
I0815 19:04:21.925372 20887 solver.cpp:594]     Test net output #2: loss = 83.105 (* 1 = 83.105 loss)
I0815 19:04:21.925377 20887 solver.cpp:254] [MultiGPU] Initial Test completed
I0815 19:04:21.998857 20887 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 6.19G, req 0G)
I0815 19:04:22.002629 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 6.27G, req 0G)
I0815 19:04:22.003283 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 6.28G, req 0G)
I0815 19:04:22.048925 20887 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 1  (limit 6.03G, req 0G)
I0815 19:04:22.054224 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 6.11G, req 0G)
I0815 19:04:22.054594 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 6.12G, req 0G)
I0815 19:04:22.090960 20887 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 1 4 3  (limit 5.85G, req 0G)
I0815 19:04:22.098762 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 5.93G, req 0G)
I0815 19:04:22.099243 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 5.94G, req 0G)
I0815 19:04:22.112012 20887 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 5.77G, req 0G)
I0815 19:04:22.122488 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 5.86G, req 0G)
I0815 19:04:22.123044 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 5.85G, req 0G)
I0815 19:04:22.133066 20887 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 5.68G, req 0G)
I0815 19:04:22.143565 20887 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 5.64G, req 0G)
I0815 19:04:22.147939 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 5.76G, req 0G)
I0815 19:04:22.148274 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 5.77G, req 0G)
I0815 19:04:22.161126 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 5.72G, req 0G)
I0815 19:04:22.161907 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 5.72G, req 0G)
I0815 19:04:22.166162 20887 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 5.61G, req 0G)
I0815 19:04:22.173475 20887 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 5.59G, req 0G)
I0815 19:04:22.180930 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 5.69G, req 0G)
I0815 19:04:22.182677 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 5.7G, req 0G)
I0815 19:04:22.189007 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 5.67G, req 0G)
I0815 19:04:22.191591 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 5.68G, req 0G)
I0815 19:04:22.217931 20887 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 5.32G, req 0G)
I0815 19:04:22.232102 20887 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 5.3G, req 0G)
I0815 19:04:22.239286 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 5.4G, req 0G)
I0815 19:04:22.241324 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 5.41G, req 0G)
I0815 19:04:22.255272 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 5.38G, req 0G)
I0815 19:04:22.256893 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 5.39G, req 0G)
I0815 19:04:22.259907 20887 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 5.14G, req 0G)
I0815 19:04:22.289665 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 5.23G, req 0G)
I0815 19:04:22.290809 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 5.23G, req 0G)
I0815 19:04:22.488346 20887 solver.cpp:317] Iteration 0 (0.562928 s), loss = 2.16281
I0815 19:04:22.488373 20887 solver.cpp:334]     Train net output #0: loss = 2.16281 (* 1 = 2.16281 loss)
I0815 19:04:22.488407 20887 sgd_solver.cpp:136] Iteration 0, lr = 0.0001, m = 0.9
I0815 19:04:22.695040 20887 solver.cpp:317] Iteration 1 (0.206681 s), loss = 2.1386
I0815 19:04:22.695050 20888 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 19:04:22.695065 20887 solver.cpp:334]     Train net output #0: loss = 2.1386 (* 1 = 2.1386 loss)
I0815 19:04:22.862552 20887 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1a' with space 1.29G/1 1 0 3  (limit 2.99G, req 0G)
I0815 19:04:22.867040 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1a' with space 1.29G/1 1 0 3  (limit 3.08G, req 0G)
I0815 19:04:22.868839 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1a' with space 1.29G/1 1 0 3  (limit 3.08G, req 0G)
I0815 19:04:22.920820 20887 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1b' with space 2.57G/2 6 4 3  (limit 1.7G, req 0G)
I0815 19:04:22.929148 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0G)
I0815 19:04:22.929606 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0G)
I0815 19:04:23.046139 20887 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 2.57G/1 6 4 3  (limit 1.7G, req 0G)
I0815 19:04:23.056579 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 2.57G/1 6 4 3  (limit 1.79G, req 0G)
I0815 19:04:23.056988 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 2.57G/1 6 4 3  (limit 1.79G, req 0G)
I0815 19:04:23.084736 20887 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 2.57G/2 6 4 3  (limit 1.7G, req 0G)
I0815 19:04:23.095661 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0G)
I0815 19:04:23.096238 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0G)
I0815 19:04:23.177140 20887 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 2.57G/1 6 4 5  (limit 1.7G, req 0.07G)
I0815 19:04:23.189581 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 2.57G/1 6 4 5  (limit 1.79G, req 0.07G)
I0815 19:04:23.190361 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 2.57G/1 6 4 5  (limit 1.79G, req 0.07G)
I0815 19:04:23.198058 20887 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 2.57G/2 6 4 3  (limit 1.7G, req 0.07G)
I0815 19:04:23.211036 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0.07G)
I0815 19:04:23.211714 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0.07G)
I0815 19:04:23.262054 20887 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 2.57G/1 6 4 5  (limit 1.7G, req 0.07G)
I0815 19:04:23.274524 20887 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 2.57G/2 6 4 3  (limit 1.7G, req 0.07G)
I0815 19:04:23.278427 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 2.57G/1 6 4 5  (limit 1.79G, req 0.07G)
I0815 19:04:23.278817 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 2.57G/1 6 4 5  (limit 1.79G, req 0.07G)
I0815 19:04:23.290850 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0.07G)
I0815 19:04:23.291402 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0.07G)
I0815 19:04:23.324597 20887 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'out3a' with space 2.57G/2 6 4 3  (limit 1.7G, req 0.07G)
I0815 19:04:23.344372 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'out3a' with space 2.57G/2 6 4 3  (limit 1.79G, req 0.07G)
I0815 19:04:23.344472 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'out3a' with space 2.57G/2 6 4 3  (limit 1.79G, req 0.07G)
I0815 19:04:23.376847 20887 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 2.57G/1 6 4 3  (limit 1.7G, req 0.07G)
I0815 19:04:23.398620 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 2.57G/1 6 4 3  (limit 1.79G, req 0.07G)
I0815 19:04:23.407742 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 2.57G/1 6 4 3  (limit 1.79G, req 0.07G)
I0815 19:04:23.408517 20887 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 2.57G/1 6 1 5  (limit 1.7G, req 0.07G)
I0815 19:04:23.426144 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 2.57G/1 6 1 5  (limit 1.79G, req 0.07G)
I0815 19:04:23.434356 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 2.57G/1 6 1 5  (limit 1.79G, req 0.07G)
I0815 19:04:23.559273 20887 solver.cpp:317] Iteration 2 (0.864201 s), loss = 2.13893
I0815 19:04:23.559298 20887 solver.cpp:334]     Train net output #0: loss = 2.13893 (* 1 = 2.13893 loss)
I0815 19:04:23.559908 20889 cudnn_conv_layer.cpp:292] [2] Layer 'conv1a' reallocating workspace: 2.57G -> 0.14G
I0815 19:04:23.559934 20888 cudnn_conv_layer.cpp:292] [1] Layer 'conv1a' reallocating workspace: 2.57G -> 0.14G
I0815 19:04:23.559922 20887 cudnn_conv_layer.cpp:292] [0] Layer 'conv1a' reallocating workspace: 2.57G -> 0.14G
I0815 19:05:29.841306 20887 solver.cpp:312] Iteration 100 (1.47857 iter/s, 66.2802s/98 iter), loss = 0.568677
I0815 19:05:29.841363 20887 solver.cpp:334]     Train net output #0: loss = 0.568677 (* 1 = 0.568677 loss)
I0815 19:05:29.841368 20887 sgd_solver.cpp:136] Iteration 100, lr = 0.0001, m = 0.9
I0815 19:06:10.397675 20893 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 19:06:28.492179 20887 solver.cpp:312] Iteration 200 (1.70505 iter/s, 58.6492s/100 iter), loss = 0.508882
I0815 19:06:28.492208 20887 solver.cpp:334]     Train net output #0: loss = 0.508882 (* 1 = 0.508882 loss)
I0815 19:06:28.492214 20887 sgd_solver.cpp:136] Iteration 200, lr = 0.0001, m = 0.9
I0815 19:07:19.524049 20887 solver.cpp:312] Iteration 300 (1.95961 iter/s, 51.0304s/100 iter), loss = 0.342625
I0815 19:07:19.524093 20887 solver.cpp:334]     Train net output #0: loss = 0.342625 (* 1 = 0.342625 loss)
I0815 19:07:19.524099 20887 sgd_solver.cpp:136] Iteration 300, lr = 0.0001, m = 0.9
I0815 19:08:09.968456 20887 solver.cpp:312] Iteration 400 (1.98244 iter/s, 50.443s/100 iter), loss = 0.366578
I0815 19:08:09.968531 20887 solver.cpp:334]     Train net output #0: loss = 0.366578 (* 1 = 0.366578 loss)
I0815 19:08:09.968536 20887 sgd_solver.cpp:136] Iteration 400, lr = 0.0001, m = 0.9
I0815 19:08:58.183528 20895 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 19:09:00.517235 20887 solver.cpp:312] Iteration 500 (1.97834 iter/s, 50.5474s/100 iter), loss = 0.238404
I0815 19:09:00.517261 20887 solver.cpp:334]     Train net output #0: loss = 0.238404 (* 1 = 0.238404 loss)
I0815 19:09:00.517266 20887 sgd_solver.cpp:136] Iteration 500, lr = 0.0001, m = 0.9
I0815 19:09:13.128109 20894 blocking_queue.cpp:40] Waiting for datum
I0815 19:09:20.462153 20887 solver.cpp:312] Iteration 600 (5.01395 iter/s, 19.9444s/100 iter), loss = 0.256399
I0815 19:09:20.462184 20887 solver.cpp:334]     Train net output #0: loss = 0.256399 (* 1 = 0.256399 loss)
I0815 19:09:20.462190 20887 sgd_solver.cpp:136] Iteration 600, lr = 0.0001, m = 0.9
I0815 19:09:39.838171 20887 solver.cpp:312] Iteration 700 (5.16116 iter/s, 19.3755s/100 iter), loss = 0.334159
I0815 19:09:39.838220 20887 solver.cpp:334]     Train net output #0: loss = 0.334158 (* 1 = 0.334158 loss)
I0815 19:09:39.838225 20887 sgd_solver.cpp:136] Iteration 700, lr = 0.0001, m = 0.9
I0815 19:09:58.889179 20887 solver.cpp:312] Iteration 800 (5.24921 iter/s, 19.0505s/100 iter), loss = 0.282593
I0815 19:09:58.889200 20887 solver.cpp:334]     Train net output #0: loss = 0.282593 (* 1 = 0.282593 loss)
I0815 19:09:58.889206 20887 sgd_solver.cpp:136] Iteration 800, lr = 0.0001, m = 0.9
I0815 19:10:03.421311 20841 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 19:10:18.161305 20887 solver.cpp:312] Iteration 900 (5.18899 iter/s, 19.2716s/100 iter), loss = 0.213218
I0815 19:10:18.161360 20887 solver.cpp:334]     Train net output #0: loss = 0.213218 (* 1 = 0.213218 loss)
I0815 19:10:18.161366 20887 sgd_solver.cpp:136] Iteration 900, lr = 0.0001, m = 0.9
I0815 19:10:35.373358 20891 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 19:10:37.614287 20887 solver.cpp:312] Iteration 1000 (5.14074 iter/s, 19.4524s/100 iter), loss = 0.235387
I0815 19:10:37.614311 20887 solver.cpp:334]     Train net output #0: loss = 0.235387 (* 1 = 0.235387 loss)
I0815 19:10:37.614315 20887 sgd_solver.cpp:136] Iteration 1000, lr = 0.0001, m = 0.9
I0815 19:10:56.937587 20887 solver.cpp:312] Iteration 1100 (5.17524 iter/s, 19.3228s/100 iter), loss = 0.191233
I0815 19:10:56.937638 20887 solver.cpp:334]     Train net output #0: loss = 0.191233 (* 1 = 0.191233 loss)
I0815 19:10:56.937644 20887 sgd_solver.cpp:136] Iteration 1100, lr = 0.0001, m = 0.9
I0815 19:11:16.374416 20887 solver.cpp:312] Iteration 1200 (5.14501 iter/s, 19.4363s/100 iter), loss = 0.24677
I0815 19:11:16.374439 20887 solver.cpp:334]     Train net output #0: loss = 0.24677 (* 1 = 0.24677 loss)
I0815 19:11:16.374444 20887 sgd_solver.cpp:136] Iteration 1200, lr = 0.0001, m = 0.9
I0815 19:11:36.008695 20887 solver.cpp:312] Iteration 1300 (5.09327 iter/s, 19.6337s/100 iter), loss = 0.330945
I0815 19:11:36.008750 20887 solver.cpp:334]     Train net output #0: loss = 0.330945 (* 1 = 0.330945 loss)
I0815 19:11:36.008757 20887 sgd_solver.cpp:136] Iteration 1300, lr = 0.0001, m = 0.9
I0815 19:11:39.849097 20895 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 19:11:55.392154 20887 solver.cpp:312] Iteration 1400 (5.15918 iter/s, 19.3829s/100 iter), loss = 0.326585
I0815 19:11:55.392177 20887 solver.cpp:334]     Train net output #0: loss = 0.326585 (* 1 = 0.326585 loss)
I0815 19:11:55.392182 20887 sgd_solver.cpp:136] Iteration 1400, lr = 0.0001, m = 0.9
I0815 19:12:14.789826 20887 solver.cpp:312] Iteration 1500 (5.1554 iter/s, 19.3971s/100 iter), loss = 0.390338
I0815 19:12:14.789883 20887 solver.cpp:334]     Train net output #0: loss = 0.390337 (* 1 = 0.390337 loss)
I0815 19:12:14.789891 20887 sgd_solver.cpp:136] Iteration 1500, lr = 0.0001, m = 0.9
I0815 19:12:34.164957 20887 solver.cpp:312] Iteration 1600 (5.1614 iter/s, 19.3746s/100 iter), loss = 0.197186
I0815 19:12:34.164985 20887 solver.cpp:334]     Train net output #0: loss = 0.197185 (* 1 = 0.197185 loss)
I0815 19:12:34.164993 20887 sgd_solver.cpp:136] Iteration 1600, lr = 0.0001, m = 0.9
I0815 19:12:43.715723 20893 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 19:12:53.682375 20887 solver.cpp:312] Iteration 1700 (5.12377 iter/s, 19.5169s/100 iter), loss = 0.276403
I0815 19:12:53.682440 20887 solver.cpp:334]     Train net output #0: loss = 0.276403 (* 1 = 0.276403 loss)
I0815 19:12:53.682449 20887 sgd_solver.cpp:136] Iteration 1700, lr = 0.0001, m = 0.9
I0815 19:13:13.275924 20887 solver.cpp:312] Iteration 1800 (5.10386 iter/s, 19.593s/100 iter), loss = 0.169287
I0815 19:13:13.275949 20887 solver.cpp:334]     Train net output #0: loss = 0.169287 (* 1 = 0.169287 loss)
I0815 19:13:13.275955 20887 sgd_solver.cpp:136] Iteration 1800, lr = 0.0001, m = 0.9
I0815 19:13:16.211633 20830 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 19:13:32.503075 20887 solver.cpp:312] Iteration 1900 (5.20112 iter/s, 19.2266s/100 iter), loss = 0.139058
I0815 19:13:32.503129 20887 solver.cpp:334]     Train net output #0: loss = 0.139058 (* 1 = 0.139058 loss)
I0815 19:13:32.503135 20887 sgd_solver.cpp:136] Iteration 1900, lr = 0.0001, m = 0.9
I0815 19:13:51.872505 20887 solver.cpp:509] Iteration 2000, Testing net (#0)
I0815 19:14:09.870875 20885 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 19:14:17.948968 20887 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.930587
I0815 19:14:17.948993 20887 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999582
I0815 19:14:17.948998 20887 solver.cpp:594]     Test net output #2: loss = 0.20762 (* 1 = 0.20762 loss)
I0815 19:14:17.949105 20887 solver.cpp:264] [MultiGPU] Tests completed in 26.0759s
I0815 19:14:18.164482 20887 solver.cpp:312] Iteration 2000 (2.19009 iter/s, 45.6602s/100 iter), loss = 0.249104
I0815 19:14:18.164510 20887 solver.cpp:334]     Train net output #0: loss = 0.249104 (* 1 = 0.249104 loss)
I0815 19:14:18.164517 20887 sgd_solver.cpp:136] Iteration 2000, lr = 0.0001, m = 0.9
I0815 19:14:37.512840 20887 solver.cpp:312] Iteration 2100 (5.16854 iter/s, 19.3478s/100 iter), loss = 0.13502
I0815 19:14:37.512863 20887 solver.cpp:334]     Train net output #0: loss = 0.135019 (* 1 = 0.135019 loss)
I0815 19:14:37.512867 20887 sgd_solver.cpp:136] Iteration 2100, lr = 0.0001, m = 0.9
I0815 19:14:56.988699 20887 solver.cpp:312] Iteration 2200 (5.1347 iter/s, 19.4753s/100 iter), loss = 0.330887
I0815 19:14:56.988752 20887 solver.cpp:334]     Train net output #0: loss = 0.330887 (* 1 = 0.330887 loss)
I0815 19:14:56.988757 20887 sgd_solver.cpp:136] Iteration 2200, lr = 0.0001, m = 0.9
I0815 19:15:16.329325 20887 solver.cpp:312] Iteration 2300 (5.17061 iter/s, 19.3401s/100 iter), loss = 0.155582
I0815 19:15:16.329350 20887 solver.cpp:334]     Train net output #0: loss = 0.155582 (* 1 = 0.155582 loss)
I0815 19:15:16.329357 20887 sgd_solver.cpp:136] Iteration 2300, lr = 0.0001, m = 0.9
I0815 19:15:18.363301 20893 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 19:15:35.822829 20887 solver.cpp:312] Iteration 2400 (5.13006 iter/s, 19.493s/100 iter), loss = 0.138271
I0815 19:15:35.822903 20887 solver.cpp:334]     Train net output #0: loss = 0.138271 (* 1 = 0.138271 loss)
I0815 19:15:35.822911 20887 sgd_solver.cpp:136] Iteration 2400, lr = 0.0001, m = 0.9
I0815 19:15:50.778798 20890 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 19:15:55.319775 20887 solver.cpp:312] Iteration 2500 (5.12915 iter/s, 19.4964s/100 iter), loss = 0.189999
I0815 19:15:55.319798 20887 solver.cpp:334]     Train net output #0: loss = 0.189999 (* 1 = 0.189999 loss)
I0815 19:15:55.319805 20887 sgd_solver.cpp:136] Iteration 2500, lr = 0.0001, m = 0.9
I0815 19:16:14.678107 20887 solver.cpp:312] Iteration 2600 (5.16588 iter/s, 19.3578s/100 iter), loss = 0.162291
I0815 19:16:14.678156 20887 solver.cpp:334]     Train net output #0: loss = 0.162291 (* 1 = 0.162291 loss)
I0815 19:16:14.678162 20887 sgd_solver.cpp:136] Iteration 2600, lr = 0.0001, m = 0.9
I0815 19:16:33.979534 20887 solver.cpp:312] Iteration 2700 (5.18111 iter/s, 19.3009s/100 iter), loss = 0.168306
I0815 19:16:33.979558 20887 solver.cpp:334]     Train net output #0: loss = 0.168306 (* 1 = 0.168306 loss)
I0815 19:16:33.979562 20887 sgd_solver.cpp:136] Iteration 2700, lr = 0.0001, m = 0.9
I0815 19:16:53.367676 20887 solver.cpp:312] Iteration 2800 (5.15793 iter/s, 19.3876s/100 iter), loss = 0.185736
I0815 19:16:53.367758 20887 solver.cpp:334]     Train net output #0: loss = 0.185736 (* 1 = 0.185736 loss)
I0815 19:16:53.367766 20887 sgd_solver.cpp:136] Iteration 2800, lr = 0.0001, m = 0.9
I0815 19:16:54.527053 20830 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 19:17:12.623780 20887 solver.cpp:312] Iteration 2900 (5.1933 iter/s, 19.2556s/100 iter), loss = 0.174731
I0815 19:17:12.623805 20887 solver.cpp:334]     Train net output #0: loss = 0.174731 (* 1 = 0.174731 loss)
I0815 19:17:12.623808 20887 sgd_solver.cpp:136] Iteration 2900, lr = 0.0001, m = 0.9
I0815 19:17:31.827792 20887 solver.cpp:312] Iteration 3000 (5.20739 iter/s, 19.2035s/100 iter), loss = 0.176327
I0815 19:17:31.827846 20887 solver.cpp:334]     Train net output #0: loss = 0.176327 (* 1 = 0.176327 loss)
I0815 19:17:31.827853 20887 sgd_solver.cpp:136] Iteration 3000, lr = 0.0001, m = 0.9
I0815 19:17:51.295557 20887 solver.cpp:312] Iteration 3100 (5.13684 iter/s, 19.4672s/100 iter), loss = 0.0998377
I0815 19:17:51.295585 20887 solver.cpp:334]     Train net output #0: loss = 0.0998376 (* 1 = 0.0998376 loss)
I0815 19:17:51.295591 20887 sgd_solver.cpp:136] Iteration 3100, lr = 0.0001, m = 0.9
I0815 19:17:58.466398 20895 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 19:18:10.561971 20887 solver.cpp:312] Iteration 3200 (5.19052 iter/s, 19.2659s/100 iter), loss = 0.336115
I0815 19:18:10.562028 20887 solver.cpp:334]     Train net output #0: loss = 0.336115 (* 1 = 0.336115 loss)
I0815 19:18:10.562036 20887 sgd_solver.cpp:136] Iteration 3200, lr = 0.0001, m = 0.9
I0815 19:18:29.885367 20887 solver.cpp:312] Iteration 3300 (5.17522 iter/s, 19.3229s/100 iter), loss = 0.136027
I0815 19:18:29.885430 20887 solver.cpp:334]     Train net output #0: loss = 0.136027 (* 1 = 0.136027 loss)
I0815 19:18:29.885447 20887 sgd_solver.cpp:136] Iteration 3300, lr = 0.0001, m = 0.9
I0815 19:18:30.323766 20891 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 19:18:49.301692 20887 solver.cpp:312] Iteration 3400 (5.15045 iter/s, 19.4158s/100 iter), loss = 0.102566
I0815 19:18:49.301744 20887 solver.cpp:334]     Train net output #0: loss = 0.102566 (* 1 = 0.102566 loss)
I0815 19:18:49.301751 20887 sgd_solver.cpp:136] Iteration 3400, lr = 0.0001, m = 0.9
I0815 19:19:08.884848 20887 solver.cpp:312] Iteration 3500 (5.10657 iter/s, 19.5826s/100 iter), loss = 0.266654
I0815 19:19:08.884873 20887 solver.cpp:334]     Train net output #0: loss = 0.266654 (* 1 = 0.266654 loss)
I0815 19:19:08.884879 20887 sgd_solver.cpp:136] Iteration 3500, lr = 0.0001, m = 0.9
I0815 19:19:28.393863 20887 solver.cpp:312] Iteration 3600 (5.12598 iter/s, 19.5085s/100 iter), loss = 0.181055
I0815 19:19:28.393939 20887 solver.cpp:334]     Train net output #0: loss = 0.181055 (* 1 = 0.181055 loss)
I0815 19:19:28.393944 20887 sgd_solver.cpp:136] Iteration 3600, lr = 0.0001, m = 0.9
I0815 19:19:34.802449 20890 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 19:19:47.861315 20887 solver.cpp:312] Iteration 3700 (5.13692 iter/s, 19.4669s/100 iter), loss = 0.160068
I0815 19:19:47.861337 20887 solver.cpp:334]     Train net output #0: loss = 0.160068 (* 1 = 0.160068 loss)
I0815 19:19:47.861343 20887 sgd_solver.cpp:136] Iteration 3700, lr = 0.0001, m = 0.9
I0815 19:20:07.360322 20887 solver.cpp:312] Iteration 3800 (5.12861 iter/s, 19.4985s/100 iter), loss = 0.246731
I0815 19:20:07.360376 20887 solver.cpp:334]     Train net output #0: loss = 0.246731 (* 1 = 0.246731 loss)
I0815 19:20:07.360383 20887 sgd_solver.cpp:136] Iteration 3800, lr = 0.0001, m = 0.9
I0815 19:20:26.714295 20887 solver.cpp:312] Iteration 3900 (5.16704 iter/s, 19.3534s/100 iter), loss = 0.159033
I0815 19:20:26.714316 20887 solver.cpp:334]     Train net output #0: loss = 0.159033 (* 1 = 0.159033 loss)
I0815 19:20:26.714321 20887 sgd_solver.cpp:136] Iteration 3900, lr = 0.0001, m = 0.9
I0815 19:20:39.035265 20841 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 19:20:45.890650 20887 solver.cpp:509] Iteration 4000, Testing net (#0)
I0815 19:21:06.824028 20935 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 19:21:06.824028 20939 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 19:21:06.824028 20885 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 19:21:07.353682 20887 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.936879
I0815 19:21:07.353703 20887 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999938
I0815 19:21:07.353709 20887 solver.cpp:594]     Test net output #2: loss = 0.176712 (* 1 = 0.176712 loss)
I0815 19:21:07.353735 20887 solver.cpp:264] [MultiGPU] Tests completed in 21.4625s
I0815 19:21:07.596345 20887 solver.cpp:312] Iteration 4000 (2.44613 iter/s, 40.8809s/100 iter), loss = 0.100487
I0815 19:21:07.596375 20887 solver.cpp:334]     Train net output #0: loss = 0.100487 (* 1 = 0.100487 loss)
I0815 19:21:07.596381 20887 sgd_solver.cpp:136] Iteration 4000, lr = 0.0001, m = 0.9
I0815 19:21:27.010953 20887 solver.cpp:312] Iteration 4100 (5.15091 iter/s, 19.4141s/100 iter), loss = 0.173701
I0815 19:21:27.011056 20887 solver.cpp:334]     Train net output #0: loss = 0.173701 (* 1 = 0.173701 loss)
I0815 19:21:27.011065 20887 sgd_solver.cpp:136] Iteration 4100, lr = 0.0001, m = 0.9
I0815 19:21:46.538709 20887 solver.cpp:312] Iteration 4200 (5.12106 iter/s, 19.5272s/100 iter), loss = 0.189477
I0815 19:21:46.538733 20887 solver.cpp:334]     Train net output #0: loss = 0.189478 (* 1 = 0.189478 loss)
I0815 19:21:46.538739 20887 sgd_solver.cpp:136] Iteration 4200, lr = 0.0001, m = 0.9
I0815 19:22:05.175688 20893 data_reader.cpp:288] Starting prefetch of epoch 4
I0815 19:22:06.334301 20887 solver.cpp:312] Iteration 4300 (5.05177 iter/s, 19.795s/100 iter), loss = 0.185414
I0815 19:22:06.334324 20887 solver.cpp:334]     Train net output #0: loss = 0.185414 (* 1 = 0.185414 loss)
I0815 19:22:06.334328 20887 sgd_solver.cpp:136] Iteration 4300, lr = 0.0001, m = 0.9
I0815 19:22:25.539458 20887 solver.cpp:312] Iteration 4400 (5.20708 iter/s, 19.2046s/100 iter), loss = 0.116149
I0815 19:22:25.539481 20887 solver.cpp:334]     Train net output #0: loss = 0.116149 (* 1 = 0.116149 loss)
I0815 19:22:25.539487 20887 sgd_solver.cpp:136] Iteration 4400, lr = 0.0001, m = 0.9
I0815 19:22:36.911303 20891 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 19:22:44.745389 20887 solver.cpp:312] Iteration 4500 (5.20687 iter/s, 19.2054s/100 iter), loss = 0.0911353
I0815 19:22:44.745435 20887 solver.cpp:334]     Train net output #0: loss = 0.0911354 (* 1 = 0.0911354 loss)
I0815 19:22:44.745455 20887 sgd_solver.cpp:136] Iteration 4500, lr = 0.0001, m = 0.9
I0815 19:23:04.374128 20887 solver.cpp:312] Iteration 4600 (5.09471 iter/s, 19.6282s/100 iter), loss = 0.180316
I0815 19:23:04.374155 20887 solver.cpp:334]     Train net output #0: loss = 0.180316 (* 1 = 0.180316 loss)
I0815 19:23:04.374161 20887 sgd_solver.cpp:136] Iteration 4600, lr = 0.0001, m = 0.9
I0815 19:23:23.783146 20887 solver.cpp:312] Iteration 4700 (5.15239 iter/s, 19.4085s/100 iter), loss = 0.156884
I0815 19:23:23.784695 20887 solver.cpp:334]     Train net output #0: loss = 0.156884 (* 1 = 0.156884 loss)
I0815 19:23:23.784715 20887 sgd_solver.cpp:136] Iteration 4700, lr = 0.0001, m = 0.9
I0815 19:23:41.463778 20890 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 19:23:43.381151 20887 solver.cpp:312] Iteration 4800 (5.1027 iter/s, 19.5975s/100 iter), loss = 0.232845
I0815 19:23:43.381178 20887 solver.cpp:334]     Train net output #0: loss = 0.232845 (* 1 = 0.232845 loss)
I0815 19:23:43.381184 20887 sgd_solver.cpp:136] Iteration 4800, lr = 0.0001, m = 0.9
I0815 19:24:02.906836 20887 solver.cpp:312] Iteration 4900 (5.1216 iter/s, 19.5251s/100 iter), loss = 0.142292
I0815 19:24:02.906911 20887 solver.cpp:334]     Train net output #0: loss = 0.142292 (* 1 = 0.142292 loss)
I0815 19:24:02.906919 20887 sgd_solver.cpp:136] Iteration 4900, lr = 0.0001, m = 0.9
I0815 19:24:22.280521 20887 solver.cpp:312] Iteration 5000 (5.16178 iter/s, 19.3731s/100 iter), loss = 0.159069
I0815 19:24:22.280550 20887 solver.cpp:334]     Train net output #0: loss = 0.159069 (* 1 = 0.159069 loss)
I0815 19:24:22.280556 20887 sgd_solver.cpp:136] Iteration 5000, lr = 0.0001, m = 0.9
I0815 19:24:41.897524 20887 solver.cpp:312] Iteration 5100 (5.09776 iter/s, 19.6165s/100 iter), loss = 0.212673
I0815 19:24:41.897581 20887 solver.cpp:334]     Train net output #0: loss = 0.212673 (* 1 = 0.212673 loss)
I0815 19:24:41.897588 20887 sgd_solver.cpp:136] Iteration 5100, lr = 0.0001, m = 0.9
I0815 19:24:45.880569 20895 data_reader.cpp:288] Starting prefetch of epoch 4
I0815 19:25:01.499992 20887 solver.cpp:312] Iteration 5200 (5.10154 iter/s, 19.6019s/100 iter), loss = 0.202232
I0815 19:25:01.500015 20887 solver.cpp:334]     Train net output #0: loss = 0.202232 (* 1 = 0.202232 loss)
I0815 19:25:01.500020 20887 sgd_solver.cpp:136] Iteration 5200, lr = 0.0001, m = 0.9
I0815 19:25:18.153836 20891 data_reader.cpp:288] Starting prefetch of epoch 4
I0815 19:25:21.033697 20887 solver.cpp:312] Iteration 5300 (5.1195 iter/s, 19.5332s/100 iter), loss = 0.102887
I0815 19:25:21.033720 20887 solver.cpp:334]     Train net output #0: loss = 0.102887 (* 1 = 0.102887 loss)
I0815 19:25:21.033725 20887 sgd_solver.cpp:136] Iteration 5300, lr = 0.0001, m = 0.9
I0815 19:25:40.777055 20887 solver.cpp:312] Iteration 5400 (5.06513 iter/s, 19.7428s/100 iter), loss = 0.101848
I0815 19:25:40.777081 20887 solver.cpp:334]     Train net output #0: loss = 0.101848 (* 1 = 0.101848 loss)
I0815 19:25:40.777086 20887 sgd_solver.cpp:136] Iteration 5400, lr = 0.0001, m = 0.9
I0815 19:26:00.193652 20887 solver.cpp:312] Iteration 5500 (5.15037 iter/s, 19.4161s/100 iter), loss = 0.232078
I0815 19:26:00.193701 20887 solver.cpp:334]     Train net output #0: loss = 0.232078 (* 1 = 0.232078 loss)
I0815 19:26:00.193708 20887 sgd_solver.cpp:136] Iteration 5500, lr = 0.0001, m = 0.9
I0815 19:26:19.473047 20887 solver.cpp:312] Iteration 5600 (5.18703 iter/s, 19.2789s/100 iter), loss = 0.0812484
I0815 19:26:19.473073 20887 solver.cpp:334]     Train net output #0: loss = 0.0812484 (* 1 = 0.0812484 loss)
I0815 19:26:19.473076 20887 sgd_solver.cpp:136] Iteration 5600, lr = 0.0001, m = 0.9
I0815 19:26:22.597847 20893 data_reader.cpp:288] Starting prefetch of epoch 5
I0815 19:26:38.969365 20887 solver.cpp:312] Iteration 5700 (5.12931 iter/s, 19.4958s/100 iter), loss = 0.380505
I0815 19:26:38.969432 20887 solver.cpp:334]     Train net output #0: loss = 0.380505 (* 1 = 0.380505 loss)
I0815 19:26:38.969437 20887 sgd_solver.cpp:136] Iteration 5700, lr = 0.0001, m = 0.9
I0815 19:26:58.460011 20887 solver.cpp:312] Iteration 5800 (5.13081 iter/s, 19.4901s/100 iter), loss = 0.117095
I0815 19:26:58.460039 20887 solver.cpp:334]     Train net output #0: loss = 0.117095 (* 1 = 0.117095 loss)
I0815 19:26:58.460045 20887 sgd_solver.cpp:136] Iteration 5800, lr = 0.0001, m = 0.9
I0815 19:27:17.771387 20887 solver.cpp:312] Iteration 5900 (5.17844 iter/s, 19.3108s/100 iter), loss = 0.236365
I0815 19:27:17.771737 20887 solver.cpp:334]     Train net output #0: loss = 0.236365 (* 1 = 0.236365 loss)
I0815 19:27:17.771744 20887 sgd_solver.cpp:136] Iteration 5900, lr = 0.0001, m = 0.9
I0815 19:27:26.728127 20893 data_reader.cpp:288] Starting prefetch of epoch 6
I0815 19:27:37.020706 20887 solver.cpp:509] Iteration 6000, Testing net (#0)
I0815 19:27:44.551409 20935 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 19:27:48.944089 20887 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.938328
I0815 19:27:48.944151 20887 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999991
I0815 19:27:48.944159 20887 solver.cpp:594]     Test net output #2: loss = 0.175991 (* 1 = 0.175991 loss)
I0815 19:27:48.944178 20887 solver.cpp:264] [MultiGPU] Tests completed in 11.9231s
I0815 19:27:49.134445 20887 solver.cpp:312] Iteration 6000 (3.18855 iter/s, 31.3622s/100 iter), loss = 0.248493
I0815 19:27:49.134470 20887 solver.cpp:334]     Train net output #0: loss = 0.248493 (* 1 = 0.248493 loss)
I0815 19:27:49.134476 20887 sgd_solver.cpp:136] Iteration 6000, lr = 0.0001, m = 0.9
I0815 19:28:08.641875 20887 solver.cpp:312] Iteration 6100 (5.12639 iter/s, 19.5069s/100 iter), loss = 0.136336
I0815 19:28:08.641898 20887 solver.cpp:334]     Train net output #0: loss = 0.136336 (* 1 = 0.136336 loss)
I0815 19:28:08.641902 20887 sgd_solver.cpp:136] Iteration 6100, lr = 0.0001, m = 0.9
I0815 19:28:27.934284 20887 solver.cpp:312] Iteration 6200 (5.18353 iter/s, 19.2919s/100 iter), loss = 0.160164
I0815 19:28:27.934340 20887 solver.cpp:334]     Train net output #0: loss = 0.160164 (* 1 = 0.160164 loss)
I0815 19:28:27.934345 20887 sgd_solver.cpp:136] Iteration 6200, lr = 0.0001, m = 0.9
I0815 19:28:43.127050 20893 data_reader.cpp:288] Starting prefetch of epoch 7
I0815 19:28:47.550725 20887 solver.cpp:312] Iteration 6300 (5.09791 iter/s, 19.6159s/100 iter), loss = 0.136442
I0815 19:28:47.550750 20887 solver.cpp:334]     Train net output #0: loss = 0.136442 (* 1 = 0.136442 loss)
I0815 19:28:47.550753 20887 sgd_solver.cpp:136] Iteration 6300, lr = 0.0001, m = 0.9
I0815 19:29:07.030766 20887 solver.cpp:312] Iteration 6400 (5.1336 iter/s, 19.4795s/100 iter), loss = 0.16143
I0815 19:29:07.030813 20887 solver.cpp:334]     Train net output #0: loss = 0.16143 (* 1 = 0.16143 loss)
I0815 19:29:07.030818 20887 sgd_solver.cpp:136] Iteration 6400, lr = 0.0001, m = 0.9
I0815 19:29:26.554488 20887 solver.cpp:312] Iteration 6500 (5.12211 iter/s, 19.5232s/100 iter), loss = 0.142198
I0815 19:29:26.554512 20887 solver.cpp:334]     Train net output #0: loss = 0.142198 (* 1 = 0.142198 loss)
I0815 19:29:26.554515 20887 sgd_solver.cpp:136] Iteration 6500, lr = 0.0001, m = 0.9
I0815 19:29:46.035254 20887 solver.cpp:312] Iteration 6600 (5.13341 iter/s, 19.4802s/100 iter), loss = 0.224642
I0815 19:29:46.035298 20887 solver.cpp:334]     Train net output #0: loss = 0.224642 (* 1 = 0.224642 loss)
I0815 19:29:46.035305 20887 sgd_solver.cpp:136] Iteration 6600, lr = 0.0001, m = 0.9
I0815 19:29:47.598014 20841 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 19:30:05.281642 20887 solver.cpp:312] Iteration 6700 (5.19592 iter/s, 19.2459s/100 iter), loss = 0.182586
I0815 19:30:05.281666 20887 solver.cpp:334]     Train net output #0: loss = 0.182586 (* 1 = 0.182586 loss)
I0815 19:30:05.281672 20887 sgd_solver.cpp:136] Iteration 6700, lr = 0.0001, m = 0.9
I0815 19:30:19.666806 20890 data_reader.cpp:288] Starting prefetch of epoch 4
I0815 19:30:24.849052 20887 solver.cpp:312] Iteration 6800 (5.11068 iter/s, 19.5669s/100 iter), loss = 0.0855129
I0815 19:30:24.849082 20887 solver.cpp:334]     Train net output #0: loss = 0.0855128 (* 1 = 0.0855128 loss)
I0815 19:30:24.849089 20887 sgd_solver.cpp:136] Iteration 6800, lr = 0.0001, m = 0.9
I0815 19:30:44.217967 20887 solver.cpp:312] Iteration 6900 (5.16305 iter/s, 19.3684s/100 iter), loss = 0.168279
I0815 19:30:44.217991 20887 solver.cpp:334]     Train net output #0: loss = 0.168279 (* 1 = 0.168279 loss)
I0815 19:30:44.217996 20887 sgd_solver.cpp:136] Iteration 6900, lr = 0.0001, m = 0.9
I0815 19:31:03.787535 20887 solver.cpp:312] Iteration 7000 (5.11012 iter/s, 19.569s/100 iter), loss = 0.233495
I0815 19:31:03.787606 20887 solver.cpp:334]     Train net output #0: loss = 0.233495 (* 1 = 0.233495 loss)
I0815 19:31:03.787613 20887 sgd_solver.cpp:136] Iteration 7000, lr = 0.0001, m = 0.9
I0815 19:31:23.515705 20887 solver.cpp:312] Iteration 7100 (5.06903 iter/s, 19.7276s/100 iter), loss = 0.13819
I0815 19:31:23.515725 20887 solver.cpp:334]     Train net output #0: loss = 0.13819 (* 1 = 0.13819 loss)
I0815 19:31:23.515729 20887 sgd_solver.cpp:136] Iteration 7100, lr = 0.0001, m = 0.9
I0815 19:31:24.130620 20841 data_reader.cpp:288] Starting prefetch of epoch 4
I0815 19:31:42.798174 20887 solver.cpp:312] Iteration 7200 (5.1862 iter/s, 19.2819s/100 iter), loss = 0.142292
I0815 19:31:42.798228 20887 solver.cpp:334]     Train net output #0: loss = 0.142292 (* 1 = 0.142292 loss)
I0815 19:31:42.798234 20887 sgd_solver.cpp:136] Iteration 7200, lr = 0.0001, m = 0.9
I0815 19:32:02.208838 20887 solver.cpp:312] Iteration 7300 (5.15195 iter/s, 19.4101s/100 iter), loss = 0.110638
I0815 19:32:02.208864 20887 solver.cpp:334]     Train net output #0: loss = 0.110638 (* 1 = 0.110638 loss)
I0815 19:32:02.208870 20887 sgd_solver.cpp:136] Iteration 7300, lr = 0.0001, m = 0.9
I0815 19:32:21.843300 20887 solver.cpp:312] Iteration 7400 (5.09323 iter/s, 19.6339s/100 iter), loss = 0.211895
I0815 19:32:21.843353 20887 solver.cpp:334]     Train net output #0: loss = 0.211895 (* 1 = 0.211895 loss)
I0815 19:32:21.843358 20887 sgd_solver.cpp:136] Iteration 7400, lr = 0.0001, m = 0.9
I0815 19:32:28.391145 20841 data_reader.cpp:288] Starting prefetch of epoch 5
I0815 19:32:41.259456 20887 solver.cpp:312] Iteration 7500 (5.15049 iter/s, 19.4156s/100 iter), loss = 0.151964
I0815 19:32:41.259485 20887 solver.cpp:334]     Train net output #0: loss = 0.151964 (* 1 = 0.151964 loss)
I0815 19:32:41.259490 20887 sgd_solver.cpp:136] Iteration 7500, lr = 0.0001, m = 0.9
I0815 19:33:00.865417 20891 data_reader.cpp:288] Starting prefetch of epoch 5
I0815 19:33:01.016575 20887 solver.cpp:312] Iteration 7600 (5.0616 iter/s, 19.7566s/100 iter), loss = 0.0955082
I0815 19:33:01.016598 20887 solver.cpp:334]     Train net output #0: loss = 0.0955081 (* 1 = 0.0955081 loss)
I0815 19:33:01.016604 20887 sgd_solver.cpp:136] Iteration 7600, lr = 0.0001, m = 0.9
I0815 19:33:20.556568 20887 solver.cpp:312] Iteration 7700 (5.11785 iter/s, 19.5395s/100 iter), loss = 0.0835602
I0815 19:33:20.556587 20887 solver.cpp:334]     Train net output #0: loss = 0.0835601 (* 1 = 0.0835601 loss)
I0815 19:33:20.556591 20887 sgd_solver.cpp:136] Iteration 7700, lr = 0.0001, m = 0.9
I0815 19:33:40.040750 20887 solver.cpp:312] Iteration 7800 (5.13251 iter/s, 19.4837s/100 iter), loss = 0.243024
I0815 19:33:40.040801 20887 solver.cpp:334]     Train net output #0: loss = 0.243023 (* 1 = 0.243023 loss)
I0815 19:33:40.040807 20887 sgd_solver.cpp:136] Iteration 7800, lr = 0.0001, m = 0.9
I0815 19:33:59.611721 20887 solver.cpp:312] Iteration 7900 (5.10975 iter/s, 19.5704s/100 iter), loss = 0.117616
I0815 19:33:59.611743 20887 solver.cpp:334]     Train net output #0: loss = 0.117616 (* 1 = 0.117616 loss)
I0815 19:33:59.611750 20887 sgd_solver.cpp:136] Iteration 7900, lr = 0.0001, m = 0.9
I0815 19:34:05.489470 20830 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 19:34:18.691819 20887 solver.cpp:509] Iteration 8000, Testing net (#0)
I0815 19:34:29.672966 20869 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 19:34:30.631613 20887 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.941444
I0815 19:34:30.631631 20887 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999995
I0815 19:34:30.631636 20887 solver.cpp:594]     Test net output #2: loss = 0.159701 (* 1 = 0.159701 loss)
I0815 19:34:30.631673 20887 solver.cpp:264] [MultiGPU] Tests completed in 11.9395s
I0815 19:34:30.830366 20887 solver.cpp:312] Iteration 8000 (3.2033 iter/s, 31.2178s/100 iter), loss = 0.124213
I0815 19:34:30.830389 20887 solver.cpp:334]     Train net output #0: loss = 0.124213 (* 1 = 0.124213 loss)
I0815 19:34:30.830394 20887 sgd_solver.cpp:136] Iteration 8000, lr = 0.0001, m = 0.9
I0815 19:34:49.296908 20830 data_reader.cpp:288] Starting prefetch of epoch 4
I0815 19:34:50.246264 20887 solver.cpp:312] Iteration 8100 (5.15056 iter/s, 19.4154s/100 iter), loss = 0.135762
I0815 19:34:50.246289 20887 solver.cpp:334]     Train net output #0: loss = 0.135762 (* 1 = 0.135762 loss)
I0815 19:34:50.246295 20887 sgd_solver.cpp:136] Iteration 8100, lr = 0.0001, m = 0.9
I0815 19:35:09.830482 20887 solver.cpp:312] Iteration 8200 (5.10629 iter/s, 19.5837s/100 iter), loss = 0.098685
I0815 19:35:09.830510 20887 solver.cpp:334]     Train net output #0: loss = 0.0986848 (* 1 = 0.0986848 loss)
I0815 19:35:09.830514 20887 sgd_solver.cpp:136] Iteration 8200, lr = 0.0001, m = 0.9
I0815 19:35:29.041316 20887 solver.cpp:312] Iteration 8300 (5.20554 iter/s, 19.2103s/100 iter), loss = 0.29135
I0815 19:35:29.041363 20887 solver.cpp:334]     Train net output #0: loss = 0.29135 (* 1 = 0.29135 loss)
I0815 19:35:29.041368 20887 sgd_solver.cpp:136] Iteration 8300, lr = 0.0001, m = 0.9
I0815 19:35:48.271138 20887 solver.cpp:312] Iteration 8400 (5.2004 iter/s, 19.2293s/100 iter), loss = 0.117632
I0815 19:35:48.271165 20887 solver.cpp:334]     Train net output #0: loss = 0.117632 (* 1 = 0.117632 loss)
I0815 19:35:48.271172 20887 sgd_solver.cpp:136] Iteration 8400, lr = 0.0001, m = 0.9
I0815 19:35:53.328879 20830 data_reader.cpp:288] Starting prefetch of epoch 5
I0815 19:36:08.002846 20887 solver.cpp:312] Iteration 8500 (5.06812 iter/s, 19.7312s/100 iter), loss = 0.103444
I0815 19:36:08.002931 20887 solver.cpp:334]     Train net output #0: loss = 0.103444 (* 1 = 0.103444 loss)
I0815 19:36:08.002938 20887 sgd_solver.cpp:136] Iteration 8500, lr = 0.0001, m = 0.9
I0815 19:36:27.610750 20887 solver.cpp:312] Iteration 8600 (5.10012 iter/s, 19.6074s/100 iter), loss = 0.168835
I0815 19:36:27.610780 20887 solver.cpp:334]     Train net output #0: loss = 0.168835 (* 1 = 0.168835 loss)
I0815 19:36:27.610786 20887 sgd_solver.cpp:136] Iteration 8600, lr = 0.0001, m = 0.9
I0815 19:36:47.076860 20887 solver.cpp:312] Iteration 8700 (5.13728 iter/s, 19.4656s/100 iter), loss = 0.101042
I0815 19:36:47.076912 20887 solver.cpp:334]     Train net output #0: loss = 0.101042 (* 1 = 0.101042 loss)
I0815 19:36:47.076920 20887 sgd_solver.cpp:136] Iteration 8700, lr = 0.0001, m = 0.9
I0815 19:36:57.846571 20841 data_reader.cpp:288] Starting prefetch of epoch 6
I0815 19:37:06.374675 20887 solver.cpp:312] Iteration 8800 (5.18208 iter/s, 19.2973s/100 iter), loss = 0.0972986
I0815 19:37:06.374704 20887 solver.cpp:334]     Train net output #0: loss = 0.0972985 (* 1 = 0.0972985 loss)
I0815 19:37:06.374711 20887 sgd_solver.cpp:136] Iteration 8800, lr = 0.0001, m = 0.9
I0815 19:37:25.843616 20887 solver.cpp:312] Iteration 8900 (5.13653 iter/s, 19.4684s/100 iter), loss = 0.139747
I0815 19:37:25.843660 20887 solver.cpp:334]     Train net output #0: loss = 0.139747 (* 1 = 0.139747 loss)
I0815 19:37:25.843667 20887 sgd_solver.cpp:136] Iteration 8900, lr = 0.0001, m = 0.9
I0815 19:37:30.011557 20890 data_reader.cpp:288] Starting prefetch of epoch 5
I0815 19:37:45.297019 20887 solver.cpp:312] Iteration 9000 (5.14063 iter/s, 19.4529s/100 iter), loss = 0.0983153
I0815 19:37:45.297044 20887 solver.cpp:334]     Train net output #0: loss = 0.0983151 (* 1 = 0.0983151 loss)
I0815 19:37:45.297050 20887 sgd_solver.cpp:136] Iteration 9000, lr = 0.0001, m = 0.9
I0815 19:38:04.594187 20887 solver.cpp:312] Iteration 9100 (5.18225 iter/s, 19.2966s/100 iter), loss = 0.190395
I0815 19:38:04.594288 20887 solver.cpp:334]     Train net output #0: loss = 0.190395 (* 1 = 0.190395 loss)
I0815 19:38:04.594295 20887 sgd_solver.cpp:136] Iteration 9100, lr = 0.0001, m = 0.9
I0815 19:38:24.299403 20887 solver.cpp:312] Iteration 9200 (5.07494 iter/s, 19.7047s/100 iter), loss = 0.11624
I0815 19:38:24.299432 20887 solver.cpp:334]     Train net output #0: loss = 0.11624 (* 1 = 0.11624 loss)
I0815 19:38:24.299438 20887 sgd_solver.cpp:136] Iteration 9200, lr = 0.0001, m = 0.9
I0815 19:38:34.330744 20830 data_reader.cpp:288] Starting prefetch of epoch 6
I0815 19:38:43.509220 20887 solver.cpp:312] Iteration 9300 (5.20582 iter/s, 19.2093s/100 iter), loss = 0.126366
I0815 19:38:43.509275 20887 solver.cpp:334]     Train net output #0: loss = 0.126366 (* 1 = 0.126366 loss)
I0815 19:38:43.509280 20887 sgd_solver.cpp:136] Iteration 9300, lr = 0.0001, m = 0.9
I0815 19:39:02.916123 20887 solver.cpp:312] Iteration 9400 (5.15295 iter/s, 19.4064s/100 iter), loss = 0.144463
I0815 19:39:02.916157 20887 solver.cpp:334]     Train net output #0: loss = 0.144463 (* 1 = 0.144463 loss)
I0815 19:39:02.916164 20887 sgd_solver.cpp:136] Iteration 9400, lr = 0.0001, m = 0.9
I0815 19:39:22.391883 20887 solver.cpp:312] Iteration 9500 (5.13473 iter/s, 19.4752s/100 iter), loss = 0.115688
I0815 19:39:22.391937 20887 solver.cpp:334]     Train net output #0: loss = 0.115687 (* 1 = 0.115687 loss)
I0815 19:39:22.391943 20887 sgd_solver.cpp:136] Iteration 9500, lr = 0.0001, m = 0.9
I0815 19:39:38.502830 20841 data_reader.cpp:288] Starting prefetch of epoch 7
I0815 19:39:41.792395 20887 solver.cpp:312] Iteration 9600 (5.15464 iter/s, 19.4s/100 iter), loss = 0.114717
I0815 19:39:41.792423 20887 solver.cpp:334]     Train net output #0: loss = 0.114716 (* 1 = 0.114716 loss)
I0815 19:39:41.792429 20887 sgd_solver.cpp:136] Iteration 9600, lr = 0.0001, m = 0.9
I0815 19:40:01.307190 20887 solver.cpp:312] Iteration 9700 (5.12446 iter/s, 19.5143s/100 iter), loss = 0.104907
I0815 19:40:01.307272 20887 solver.cpp:334]     Train net output #0: loss = 0.104907 (* 1 = 0.104907 loss)
I0815 19:40:01.307277 20887 sgd_solver.cpp:136] Iteration 9700, lr = 0.0001, m = 0.9
I0815 19:40:10.660459 20890 data_reader.cpp:288] Starting prefetch of epoch 6
I0815 19:40:20.864078 20887 solver.cpp:312] Iteration 9800 (5.11343 iter/s, 19.5564s/100 iter), loss = 0.108421
I0815 19:40:20.864099 20887 solver.cpp:334]     Train net output #0: loss = 0.108421 (* 1 = 0.108421 loss)
I0815 19:40:20.864104 20887 sgd_solver.cpp:136] Iteration 9800, lr = 0.0001, m = 0.9
I0815 19:40:40.437625 20887 solver.cpp:312] Iteration 9900 (5.10908 iter/s, 19.573s/100 iter), loss = 0.183411
I0815 19:40:40.437707 20887 solver.cpp:334]     Train net output #0: loss = 0.183411 (* 1 = 0.183411 loss)
I0815 19:40:40.437714 20887 sgd_solver.cpp:136] Iteration 9900, lr = 0.0001, m = 0.9
I0815 19:40:59.863323 20887 solver.cpp:639] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/initial/cityscapes5_jsegnet21v2_iter_10000.caffemodel
I0815 19:41:00.159395 20887 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/initial/cityscapes5_jsegnet21v2_iter_10000.solverstate
I0815 19:41:00.167876 20887 solver.cpp:509] Iteration 10000, Testing net (#0)
I0815 19:41:18.551153 20937 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 19:41:27.401728 20887 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.946446
I0815 19:41:27.401753 20887 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999994
I0815 19:41:27.401760 20887 solver.cpp:594]     Test net output #2: loss = 0.155604 (* 1 = 0.155604 loss)
I0815 19:41:27.401788 20887 solver.cpp:264] [MultiGPU] Tests completed in 27.2331s
I0815 19:41:27.599385 20887 solver.cpp:312] Iteration 10000 (2.12042 iter/s, 47.1605s/100 iter), loss = 0.0919557
I0815 19:41:27.599413 20887 solver.cpp:334]     Train net output #0: loss = 0.0919555 (* 1 = 0.0919555 loss)
I0815 19:41:27.599419 20887 sgd_solver.cpp:136] Iteration 10000, lr = 0.0001, m = 0.9
I0815 19:41:46.959487 20887 solver.cpp:312] Iteration 10100 (5.16541 iter/s, 19.3596s/100 iter), loss = 0.0956914
I0815 19:41:46.959514 20887 solver.cpp:334]     Train net output #0: loss = 0.0956911 (* 1 = 0.0956911 loss)
I0815 19:41:46.959519 20887 sgd_solver.cpp:136] Iteration 10100, lr = 0.0001, m = 0.9
I0815 19:42:06.174270 20887 solver.cpp:312] Iteration 10200 (5.20447 iter/s, 19.2142s/100 iter), loss = 0.323771
I0815 19:42:06.174342 20887 solver.cpp:334]     Train net output #0: loss = 0.32377 (* 1 = 0.32377 loss)
I0815 19:42:06.174347 20887 sgd_solver.cpp:136] Iteration 10200, lr = 0.0001, m = 0.9
I0815 19:42:14.739534 20893 data_reader.cpp:288] Starting prefetch of epoch 8
I0815 19:42:25.655863 20887 solver.cpp:312] Iteration 10300 (5.13319 iter/s, 19.4811s/100 iter), loss = 0.0961498
I0815 19:42:25.655890 20887 solver.cpp:334]     Train net output #0: loss = 0.0961496 (* 1 = 0.0961496 loss)
I0815 19:42:25.655897 20887 sgd_solver.cpp:136] Iteration 10300, lr = 0.0001, m = 0.9
I0815 19:42:45.040076 20887 solver.cpp:312] Iteration 10400 (5.15898 iter/s, 19.3837s/100 iter), loss = 0.0783144
I0815 19:42:45.040120 20887 solver.cpp:334]     Train net output #0: loss = 0.0783141 (* 1 = 0.0783141 loss)
I0815 19:42:45.040127 20887 sgd_solver.cpp:136] Iteration 10400, lr = 0.0001, m = 0.9
I0815 19:43:04.531802 20887 solver.cpp:312] Iteration 10500 (5.13052 iter/s, 19.4912s/100 iter), loss = 0.0840846
I0815 19:43:04.531828 20887 solver.cpp:334]     Train net output #0: loss = 0.0840844 (* 1 = 0.0840844 loss)
I0815 19:43:04.531834 20887 sgd_solver.cpp:136] Iteration 10500, lr = 0.0001, m = 0.9
I0815 19:43:18.949287 20893 data_reader.cpp:288] Starting prefetch of epoch 9
I0815 19:43:24.022712 20887 solver.cpp:312] Iteration 10600 (5.13074 iter/s, 19.4904s/100 iter), loss = 0.14687
I0815 19:43:24.022734 20887 solver.cpp:334]     Train net output #0: loss = 0.146869 (* 1 = 0.146869 loss)
I0815 19:43:24.022740 20887 sgd_solver.cpp:136] Iteration 10600, lr = 0.0001, m = 0.9
I0815 19:43:43.455555 20887 solver.cpp:312] Iteration 10700 (5.14607 iter/s, 19.4323s/100 iter), loss = 0.088799
I0815 19:43:43.455580 20887 solver.cpp:334]     Train net output #0: loss = 0.0887987 (* 1 = 0.0887987 loss)
I0815 19:43:43.455585 20887 sgd_solver.cpp:136] Iteration 10700, lr = 0.0001, m = 0.9
I0815 19:43:51.192931 20841 data_reader.cpp:288] Starting prefetch of epoch 8
I0815 19:44:02.871054 20887 solver.cpp:312] Iteration 10800 (5.15067 iter/s, 19.415s/100 iter), loss = 0.166933
I0815 19:44:02.871083 20887 solver.cpp:334]     Train net output #0: loss = 0.166933 (* 1 = 0.166933 loss)
I0815 19:44:02.871089 20887 sgd_solver.cpp:136] Iteration 10800, lr = 0.0001, m = 0.9
I0815 19:44:22.268507 20887 solver.cpp:312] Iteration 10900 (5.15546 iter/s, 19.3969s/100 iter), loss = 0.0660203
I0815 19:44:22.268555 20887 solver.cpp:334]     Train net output #0: loss = 0.0660201 (* 1 = 0.0660201 loss)
I0815 19:44:22.268561 20887 sgd_solver.cpp:136] Iteration 10900, lr = 0.0001, m = 0.9
I0815 19:44:41.583034 20887 solver.cpp:312] Iteration 11000 (5.17759 iter/s, 19.314s/100 iter), loss = 0.107541
I0815 19:44:41.583060 20887 solver.cpp:334]     Train net output #0: loss = 0.107541 (* 1 = 0.107541 loss)
I0815 19:44:41.583063 20887 sgd_solver.cpp:136] Iteration 11000, lr = 0.0001, m = 0.9
I0815 19:44:55.176383 20890 data_reader.cpp:288] Starting prefetch of epoch 7
I0815 19:45:00.983646 20887 solver.cpp:312] Iteration 11100 (5.15462 iter/s, 19.4001s/100 iter), loss = 0.122738
I0815 19:45:00.983670 20887 solver.cpp:334]     Train net output #0: loss = 0.122738 (* 1 = 0.122738 loss)
I0815 19:45:00.983675 20887 sgd_solver.cpp:136] Iteration 11100, lr = 0.0001, m = 0.9
I0815 19:45:20.423815 20887 solver.cpp:312] Iteration 11200 (5.14413 iter/s, 19.4396s/100 iter), loss = 0.0945036
I0815 19:45:20.423836 20887 solver.cpp:334]     Train net output #0: loss = 0.0945033 (* 1 = 0.0945033 loss)
I0815 19:45:20.423841 20887 sgd_solver.cpp:136] Iteration 11200, lr = 0.0001, m = 0.9
I0815 19:45:39.773124 20887 solver.cpp:312] Iteration 11300 (5.16829 iter/s, 19.3488s/100 iter), loss = 0.108577
I0815 19:45:39.773196 20887 solver.cpp:334]     Train net output #0: loss = 0.108577 (* 1 = 0.108577 loss)
I0815 19:45:39.773203 20887 sgd_solver.cpp:136] Iteration 11300, lr = 0.0001, m = 0.9
I0815 19:45:59.489234 20887 solver.cpp:312] Iteration 11400 (5.07213 iter/s, 19.7156s/100 iter), loss = 0.0807952
I0815 19:45:59.489254 20887 solver.cpp:334]     Train net output #0: loss = 0.0807949 (* 1 = 0.0807949 loss)
I0815 19:45:59.489259 20887 sgd_solver.cpp:136] Iteration 11400, lr = 0.0001, m = 0.9
I0815 19:45:59.691185 20895 data_reader.cpp:288] Starting prefetch of epoch 5
I0815 19:46:18.877540 20887 solver.cpp:312] Iteration 11500 (5.15789 iter/s, 19.3878s/100 iter), loss = 0.141415
I0815 19:46:18.877622 20887 solver.cpp:334]     Train net output #0: loss = 0.141415 (* 1 = 0.141415 loss)
I0815 19:46:18.877629 20887 sgd_solver.cpp:136] Iteration 11500, lr = 0.0001, m = 0.9
I0815 19:46:31.891383 20830 data_reader.cpp:288] Starting prefetch of epoch 7
I0815 19:46:38.269542 20887 solver.cpp:312] Iteration 11600 (5.15691 iter/s, 19.3915s/100 iter), loss = 0.147734
I0815 19:46:38.269572 20887 solver.cpp:334]     Train net output #0: loss = 0.147733 (* 1 = 0.147733 loss)
I0815 19:46:38.269578 20887 sgd_solver.cpp:136] Iteration 11600, lr = 0.0001, m = 0.9
I0815 19:46:57.755529 20887 solver.cpp:312] Iteration 11700 (5.13203 iter/s, 19.4855s/100 iter), loss = 0.0574526
I0815 19:46:57.755609 20887 solver.cpp:334]     Train net output #0: loss = 0.0574523 (* 1 = 0.0574523 loss)
I0815 19:46:57.755622 20887 sgd_solver.cpp:136] Iteration 11700, lr = 0.0001, m = 0.9
I0815 19:47:17.143787 20887 solver.cpp:312] Iteration 11800 (5.1579 iter/s, 19.3877s/100 iter), loss = 0.136417
I0815 19:47:17.143813 20887 solver.cpp:334]     Train net output #0: loss = 0.136417 (* 1 = 0.136417 loss)
I0815 19:47:17.143821 20887 sgd_solver.cpp:136] Iteration 11800, lr = 0.0001, m = 0.9
I0815 19:47:35.829191 20891 data_reader.cpp:288] Starting prefetch of epoch 6
I0815 19:47:36.561664 20887 solver.cpp:312] Iteration 11900 (5.15004 iter/s, 19.4173s/100 iter), loss = 0.109749
I0815 19:47:36.561684 20887 solver.cpp:334]     Train net output #0: loss = 0.109748 (* 1 = 0.109748 loss)
I0815 19:47:36.561691 20887 sgd_solver.cpp:136] Iteration 11900, lr = 0.0001, m = 0.9
I0815 19:47:55.680375 20887 solver.cpp:509] Iteration 12000, Testing net (#0)
I0815 19:48:07.510062 20869 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 19:48:08.007380 20887 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.943812
I0815 19:48:08.007405 20887 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999996
I0815 19:48:08.007410 20887 solver.cpp:594]     Test net output #2: loss = 0.154984 (* 1 = 0.154984 loss)
I0815 19:48:08.007501 20887 solver.cpp:264] [MultiGPU] Tests completed in 12.3268s
I0815 19:48:08.208499 20887 solver.cpp:312] Iteration 12000 (3.15996 iter/s, 31.646s/100 iter), loss = 0.0608578
I0815 19:48:08.208526 20887 solver.cpp:334]     Train net output #0: loss = 0.0608576 (* 1 = 0.0608576 loss)
I0815 19:48:08.208533 20887 sgd_solver.cpp:136] Iteration 12000, lr = 0.0001, m = 0.9
I0815 19:48:20.365422 20830 data_reader.cpp:288] Starting prefetch of epoch 8
I0815 19:48:27.622198 20887 solver.cpp:312] Iteration 12100 (5.15114 iter/s, 19.4132s/100 iter), loss = 0.140818
I0815 19:48:27.622218 20887 solver.cpp:334]     Train net output #0: loss = 0.140818 (* 1 = 0.140818 loss)
I0815 19:48:27.622222 20887 sgd_solver.cpp:136] Iteration 12100, lr = 0.0001, m = 0.9
I0815 19:48:46.895294 20887 solver.cpp:312] Iteration 12200 (5.18872 iter/s, 19.2726s/100 iter), loss = 0.135253
I0815 19:48:46.895395 20887 solver.cpp:334]     Train net output #0: loss = 0.135253 (* 1 = 0.135253 loss)
I0815 19:48:46.895416 20887 sgd_solver.cpp:136] Iteration 12200, lr = 0.0001, m = 0.9
I0815 19:49:06.376462 20887 solver.cpp:312] Iteration 12300 (5.1333 iter/s, 19.4806s/100 iter), loss = 0.0997504
I0815 19:49:06.376695 20887 solver.cpp:334]     Train net output #0: loss = 0.0997502 (* 1 = 0.0997502 loss)
I0815 19:49:06.376824 20887 sgd_solver.cpp:136] Iteration 12300, lr = 0.0001, m = 0.9
I0815 19:49:24.417132 20890 data_reader.cpp:288] Starting prefetch of epoch 8
I0815 19:49:25.851806 20887 solver.cpp:312] Iteration 12400 (5.13484 iter/s, 19.4748s/100 iter), loss = 0.0787297
I0815 19:49:25.851830 20887 solver.cpp:334]     Train net output #0: loss = 0.0787295 (* 1 = 0.0787295 loss)
I0815 19:49:25.851833 20887 sgd_solver.cpp:136] Iteration 12400, lr = 0.0001, m = 0.9
I0815 19:49:45.183172 20887 solver.cpp:312] Iteration 12500 (5.17308 iter/s, 19.3308s/100 iter), loss = 0.0938057
I0815 19:49:45.183199 20887 solver.cpp:334]     Train net output #0: loss = 0.0938055 (* 1 = 0.0938055 loss)
I0815 19:49:45.183205 20887 sgd_solver.cpp:136] Iteration 12500, lr = 0.0001, m = 0.9
I0815 19:50:04.847785 20887 solver.cpp:312] Iteration 12600 (5.08542 iter/s, 19.6641s/100 iter), loss = 0.111992
I0815 19:50:04.847832 20887 solver.cpp:334]     Train net output #0: loss = 0.111992 (* 1 = 0.111992 loss)
I0815 19:50:04.847839 20887 sgd_solver.cpp:136] Iteration 12600, lr = 0.0001, m = 0.9
I0815 19:50:24.561409 20887 solver.cpp:312] Iteration 12700 (5.07277 iter/s, 19.7131s/100 iter), loss = 0.252726
I0815 19:50:24.561434 20887 solver.cpp:334]     Train net output #0: loss = 0.252726 (* 1 = 0.252726 loss)
I0815 19:50:24.561439 20887 sgd_solver.cpp:136] Iteration 12700, lr = 0.0001, m = 0.9
I0815 19:50:28.994598 20893 data_reader.cpp:288] Starting prefetch of epoch 10
I0815 19:50:44.092450 20887 solver.cpp:312] Iteration 12800 (5.1202 iter/s, 19.5305s/100 iter), loss = 0.0799723
I0815 19:50:44.092501 20887 solver.cpp:334]     Train net output #0: loss = 0.079972 (* 1 = 0.079972 loss)
I0815 19:50:44.092509 20887 sgd_solver.cpp:136] Iteration 12800, lr = 0.0001, m = 0.9
I0815 19:51:01.407833 20830 data_reader.cpp:288] Starting prefetch of epoch 9
I0815 19:51:03.696179 20887 solver.cpp:312] Iteration 12900 (5.10121 iter/s, 19.6032s/100 iter), loss = 0.10406
I0815 19:51:03.696200 20887 solver.cpp:334]     Train net output #0: loss = 0.10406 (* 1 = 0.10406 loss)
I0815 19:51:03.696207 20887 sgd_solver.cpp:136] Iteration 12900, lr = 0.0001, m = 0.9
I0815 19:51:23.191540 20887 solver.cpp:312] Iteration 13000 (5.12957 iter/s, 19.4948s/100 iter), loss = 0.0810247
I0815 19:51:23.191596 20887 solver.cpp:334]     Train net output #0: loss = 0.0810244 (* 1 = 0.0810244 loss)
I0815 19:51:23.191601 20887 sgd_solver.cpp:136] Iteration 13000, lr = 0.0001, m = 0.9
I0815 19:51:42.322966 20887 solver.cpp:312] Iteration 13100 (5.22714 iter/s, 19.1309s/100 iter), loss = 0.18332
I0815 19:51:42.322988 20887 solver.cpp:334]     Train net output #0: loss = 0.183319 (* 1 = 0.183319 loss)
I0815 19:51:42.322993 20887 sgd_solver.cpp:136] Iteration 13100, lr = 0.0001, m = 0.9
I0815 19:52:01.582149 20887 solver.cpp:312] Iteration 13200 (5.19247 iter/s, 19.2587s/100 iter), loss = 0.0812014
I0815 19:52:01.582201 20887 solver.cpp:334]     Train net output #0: loss = 0.0812012 (* 1 = 0.0812012 loss)
I0815 19:52:01.582206 20887 sgd_solver.cpp:136] Iteration 13200, lr = 0.0001, m = 0.9
I0815 19:52:05.272518 20891 data_reader.cpp:288] Starting prefetch of epoch 7
I0815 19:52:20.898071 20887 solver.cpp:312] Iteration 13300 (5.17722 iter/s, 19.3154s/100 iter), loss = 0.0869071
I0815 19:52:20.898092 20887 solver.cpp:334]     Train net output #0: loss = 0.0869069 (* 1 = 0.0869069 loss)
I0815 19:52:20.898095 20887 sgd_solver.cpp:136] Iteration 13300, lr = 0.0001, m = 0.9
I0815 19:52:40.443697 20887 solver.cpp:312] Iteration 13400 (5.11637 iter/s, 19.5451s/100 iter), loss = 0.0825462
I0815 19:52:40.443755 20887 solver.cpp:334]     Train net output #0: loss = 0.082546 (* 1 = 0.082546 loss)
I0815 19:52:40.443763 20887 sgd_solver.cpp:136] Iteration 13400, lr = 0.0001, m = 0.9
I0815 19:53:00.014734 20887 solver.cpp:312] Iteration 13500 (5.10973 iter/s, 19.5705s/100 iter), loss = 0.136757
I0815 19:53:00.014758 20887 solver.cpp:334]     Train net output #0: loss = 0.136757 (* 1 = 0.136757 loss)
I0815 19:53:00.014765 20887 sgd_solver.cpp:136] Iteration 13500, lr = 0.0001, m = 0.9
I0815 19:53:09.531096 20893 data_reader.cpp:288] Starting prefetch of epoch 11
I0815 19:53:19.399896 20887 solver.cpp:312] Iteration 13600 (5.15873 iter/s, 19.3846s/100 iter), loss = 0.112086
I0815 19:53:19.399973 20887 solver.cpp:334]     Train net output #0: loss = 0.112086 (* 1 = 0.112086 loss)
I0815 19:53:19.399981 20887 sgd_solver.cpp:136] Iteration 13600, lr = 0.0001, m = 0.9
I0815 19:53:38.700660 20887 solver.cpp:312] Iteration 13700 (5.18128 iter/s, 19.3002s/100 iter), loss = 0.0837837
I0815 19:53:38.700685 20887 solver.cpp:334]     Train net output #0: loss = 0.0837835 (* 1 = 0.0837835 loss)
I0815 19:53:38.700692 20887 sgd_solver.cpp:136] Iteration 13700, lr = 0.0001, m = 0.9
I0815 19:53:41.647369 20890 data_reader.cpp:288] Starting prefetch of epoch 9
I0815 19:53:57.985699 20887 solver.cpp:312] Iteration 13800 (5.18551 iter/s, 19.2845s/100 iter), loss = 0.104896
I0815 19:53:57.985746 20887 solver.cpp:334]     Train net output #0: loss = 0.104896 (* 1 = 0.104896 loss)
I0815 19:53:57.985752 20887 sgd_solver.cpp:136] Iteration 13800, lr = 0.0001, m = 0.9
I0815 19:54:17.465381 20887 solver.cpp:312] Iteration 13900 (5.13369 iter/s, 19.4791s/100 iter), loss = 0.110906
I0815 19:54:17.465407 20887 solver.cpp:334]     Train net output #0: loss = 0.110906 (* 1 = 0.110906 loss)
I0815 19:54:17.465414 20887 sgd_solver.cpp:136] Iteration 13900, lr = 0.0001, m = 0.9
I0815 19:54:36.827085 20887 solver.cpp:509] Iteration 14000, Testing net (#0)
I0815 19:54:44.533480 20933 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 19:54:48.995455 20887 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.947668
I0815 19:54:48.995477 20887 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999995
I0815 19:54:48.995483 20887 solver.cpp:594]     Test net output #2: loss = 0.155081 (* 1 = 0.155081 loss)
I0815 19:54:48.995676 20887 solver.cpp:264] [MultiGPU] Tests completed in 12.1683s
I0815 19:54:49.206209 20887 solver.cpp:312] Iteration 14000 (3.1506 iter/s, 31.74s/100 iter), loss = 0.131852
I0815 19:54:49.206236 20887 solver.cpp:334]     Train net output #0: loss = 0.131851 (* 1 = 0.131851 loss)
I0815 19:54:49.206243 20887 sgd_solver.cpp:136] Iteration 14000, lr = 0.0001, m = 0.9
I0815 19:55:08.798252 20887 solver.cpp:312] Iteration 14100 (5.10425 iter/s, 19.5915s/100 iter), loss = 0.145497
I0815 19:55:08.798303 20887 solver.cpp:334]     Train net output #0: loss = 0.145497 (* 1 = 0.145497 loss)
I0815 19:55:08.798308 20887 sgd_solver.cpp:136] Iteration 14100, lr = 0.0001, m = 0.9
I0815 19:55:28.305280 20887 solver.cpp:312] Iteration 14200 (5.1265 iter/s, 19.5065s/100 iter), loss = 0.100638
I0815 19:55:28.305305 20887 solver.cpp:334]     Train net output #0: loss = 0.100637 (* 1 = 0.100637 loss)
I0815 19:55:28.305311 20887 sgd_solver.cpp:136] Iteration 14200, lr = 0.0001, m = 0.9
I0815 19:55:30.319392 20893 data_reader.cpp:288] Starting prefetch of epoch 12
I0815 19:55:48.038604 20887 solver.cpp:312] Iteration 14300 (5.06771 iter/s, 19.7328s/100 iter), loss = 0.162185
I0815 19:55:48.038657 20887 solver.cpp:334]     Train net output #0: loss = 0.162184 (* 1 = 0.162184 loss)
I0815 19:55:48.038663 20887 sgd_solver.cpp:136] Iteration 14300, lr = 0.0001, m = 0.9
I0815 19:56:02.806126 20891 data_reader.cpp:288] Starting prefetch of epoch 8
I0815 19:56:07.332414 20887 solver.cpp:312] Iteration 14400 (5.18315 iter/s, 19.2933s/100 iter), loss = 0.101411
I0815 19:56:07.332439 20887 solver.cpp:334]     Train net output #0: loss = 0.101411 (* 1 = 0.101411 loss)
I0815 19:56:07.332444 20887 sgd_solver.cpp:136] Iteration 14400, lr = 0.0001, m = 0.9
I0815 19:56:26.857587 20887 solver.cpp:312] Iteration 14500 (5.12174 iter/s, 19.5246s/100 iter), loss = 0.105859
I0815 19:56:26.857640 20887 solver.cpp:334]     Train net output #0: loss = 0.105858 (* 1 = 0.105858 loss)
I0815 19:56:26.857646 20887 sgd_solver.cpp:136] Iteration 14500, lr = 0.0001, m = 0.9
I0815 19:56:46.193773 20887 solver.cpp:312] Iteration 14600 (5.17179 iter/s, 19.3356s/100 iter), loss = 0.0974646
I0815 19:56:46.193794 20887 solver.cpp:334]     Train net output #0: loss = 0.0974644 (* 1 = 0.0974644 loss)
I0815 19:56:46.193800 20887 sgd_solver.cpp:136] Iteration 14600, lr = 0.0001, m = 0.9
I0815 19:57:05.698410 20887 solver.cpp:312] Iteration 14700 (5.12713 iter/s, 19.5041s/100 iter), loss = 0.175828
I0815 19:57:05.698475 20887 solver.cpp:334]     Train net output #0: loss = 0.175828 (* 1 = 0.175828 loss)
I0815 19:57:05.698482 20887 sgd_solver.cpp:136] Iteration 14700, lr = 0.0001, m = 0.9
I0815 19:57:06.852655 20895 data_reader.cpp:288] Starting prefetch of epoch 6
I0815 19:57:24.907562 20887 solver.cpp:312] Iteration 14800 (5.20599 iter/s, 19.2086s/100 iter), loss = 0.0992714
I0815 19:57:24.907586 20887 solver.cpp:334]     Train net output #0: loss = 0.0992712 (* 1 = 0.0992712 loss)
I0815 19:57:24.907593 20887 sgd_solver.cpp:136] Iteration 14800, lr = 0.0001, m = 0.9
I0815 19:57:44.334313 20887 solver.cpp:312] Iteration 14900 (5.14768 iter/s, 19.4262s/100 iter), loss = 0.178289
I0815 19:57:44.334365 20887 solver.cpp:334]     Train net output #0: loss = 0.178289 (* 1 = 0.178289 loss)
I0815 19:57:44.334374 20887 sgd_solver.cpp:136] Iteration 14900, lr = 0.0001, m = 0.9
I0815 19:58:04.063727 20887 solver.cpp:312] Iteration 15000 (5.06871 iter/s, 19.7289s/100 iter), loss = 0.0919131
I0815 19:58:04.063751 20887 solver.cpp:334]     Train net output #0: loss = 0.0919129 (* 1 = 0.0919129 loss)
I0815 19:58:04.063758 20887 sgd_solver.cpp:136] Iteration 15000, lr = 0.0001, m = 0.9
I0815 19:58:11.147490 20841 data_reader.cpp:288] Starting prefetch of epoch 9
I0815 19:58:23.408269 20887 solver.cpp:312] Iteration 15100 (5.16956 iter/s, 19.344s/100 iter), loss = 0.193135
I0815 19:58:23.408315 20887 solver.cpp:334]     Train net output #0: loss = 0.193134 (* 1 = 0.193134 loss)
I0815 19:58:23.408323 20887 sgd_solver.cpp:136] Iteration 15100, lr = 0.0001, m = 0.9
I0815 19:58:42.645319 20887 solver.cpp:312] Iteration 15200 (5.19845 iter/s, 19.2365s/100 iter), loss = 0.105367
I0815 19:58:42.645344 20887 solver.cpp:334]     Train net output #0: loss = 0.105367 (* 1 = 0.105367 loss)
I0815 19:58:42.645349 20887 sgd_solver.cpp:136] Iteration 15200, lr = 0.0001, m = 0.9
I0815 19:59:02.347156 20887 solver.cpp:312] Iteration 15300 (5.07581 iter/s, 19.7013s/100 iter), loss = 0.0494174
I0815 19:59:02.347208 20887 solver.cpp:334]     Train net output #0: loss = 0.0494172 (* 1 = 0.0494172 loss)
I0815 19:59:02.347216 20887 sgd_solver.cpp:136] Iteration 15300, lr = 0.0001, m = 0.9
I0815 19:59:15.591271 20893 data_reader.cpp:288] Starting prefetch of epoch 13
I0815 19:59:21.964874 20887 solver.cpp:312] Iteration 15400 (5.09757 iter/s, 19.6172s/100 iter), loss = 0.107777
I0815 19:59:21.964901 20887 solver.cpp:334]     Train net output #0: loss = 0.107777 (* 1 = 0.107777 loss)
I0815 19:59:21.964907 20887 sgd_solver.cpp:136] Iteration 15400, lr = 0.0001, m = 0.9
I0815 19:59:41.566017 20887 solver.cpp:312] Iteration 15500 (5.10188 iter/s, 19.6006s/100 iter), loss = 0.0936631
I0815 19:59:41.566061 20887 solver.cpp:334]     Train net output #0: loss = 0.093663 (* 1 = 0.093663 loss)
I0815 19:59:41.566068 20887 sgd_solver.cpp:136] Iteration 15500, lr = 0.0001, m = 0.9
I0815 19:59:48.088066 20830 data_reader.cpp:288] Starting prefetch of epoch 10
I0815 20:00:01.018204 20887 solver.cpp:312] Iteration 15600 (5.14095 iter/s, 19.4517s/100 iter), loss = 0.0937205
I0815 20:00:01.018225 20887 solver.cpp:334]     Train net output #0: loss = 0.0937203 (* 1 = 0.0937203 loss)
I0815 20:00:01.018230 20887 sgd_solver.cpp:136] Iteration 15600, lr = 0.0001, m = 0.9
I0815 20:00:20.601639 20887 solver.cpp:312] Iteration 15700 (5.1065 iter/s, 19.5829s/100 iter), loss = 0.119206
I0815 20:00:20.601686 20887 solver.cpp:334]     Train net output #0: loss = 0.119206 (* 1 = 0.119206 loss)
I0815 20:00:20.601693 20887 sgd_solver.cpp:136] Iteration 15700, lr = 0.0001, m = 0.9
I0815 20:00:40.243232 20887 solver.cpp:312] Iteration 15800 (5.09138 iter/s, 19.641s/100 iter), loss = 0.113153
I0815 20:00:40.243261 20887 solver.cpp:334]     Train net output #0: loss = 0.113152 (* 1 = 0.113152 loss)
I0815 20:00:40.243268 20887 sgd_solver.cpp:136] Iteration 15800, lr = 0.0001, m = 0.9
I0815 20:00:52.459815 20841 data_reader.cpp:288] Starting prefetch of epoch 10
I0815 20:00:59.581045 20887 solver.cpp:312] Iteration 15900 (5.17136 iter/s, 19.3373s/100 iter), loss = 0.0680901
I0815 20:00:59.581069 20887 solver.cpp:334]     Train net output #0: loss = 0.0680899 (* 1 = 0.0680899 loss)
I0815 20:00:59.581076 20887 sgd_solver.cpp:136] Iteration 15900, lr = 0.0001, m = 0.9
I0815 20:01:18.623725 20887 solver.cpp:509] Iteration 16000, Testing net (#0)
I0815 20:01:29.363675 20885 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 20:01:30.078336 20887 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.948013
I0815 20:01:30.078356 20887 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999998
I0815 20:01:30.078363 20887 solver.cpp:594]     Test net output #2: loss = 0.1515 (* 1 = 0.1515 loss)
I0815 20:01:30.078444 20887 solver.cpp:264] [MultiGPU] Tests completed in 11.4544s
I0815 20:01:30.280939 20887 solver.cpp:312] Iteration 16000 (3.25743 iter/s, 30.6991s/100 iter), loss = 0.112008
I0815 20:01:30.280964 20887 solver.cpp:334]     Train net output #0: loss = 0.112008 (* 1 = 0.112008 loss)
I0815 20:01:30.280971 20887 sgd_solver.cpp:136] Iteration 16000, lr = 0.0001, m = 0.9
I0815 20:01:35.665576 20830 data_reader.cpp:288] Starting prefetch of epoch 11
I0815 20:01:49.707916 20887 solver.cpp:312] Iteration 16100 (5.14762 iter/s, 19.4264s/100 iter), loss = 0.111454
I0815 20:01:49.707942 20887 solver.cpp:334]     Train net output #0: loss = 0.111453 (* 1 = 0.111453 loss)
I0815 20:01:49.707948 20887 sgd_solver.cpp:136] Iteration 16100, lr = 0.0001, m = 0.9
I0815 20:02:09.144367 20887 solver.cpp:312] Iteration 16200 (5.14511 iter/s, 19.4359s/100 iter), loss = 0.1521
I0815 20:02:09.144413 20887 solver.cpp:334]     Train net output #0: loss = 0.1521 (* 1 = 0.1521 loss)
I0815 20:02:09.144419 20887 sgd_solver.cpp:136] Iteration 16200, lr = 0.0001, m = 0.9
I0815 20:02:28.706775 20887 solver.cpp:312] Iteration 16300 (5.11199 iter/s, 19.5619s/100 iter), loss = 0.097242
I0815 20:02:28.706799 20887 solver.cpp:334]     Train net output #0: loss = 0.0972418 (* 1 = 0.0972418 loss)
I0815 20:02:28.706804 20887 sgd_solver.cpp:136] Iteration 16300, lr = 0.0001, m = 0.9
I0815 20:02:40.159482 20841 data_reader.cpp:288] Starting prefetch of epoch 11
I0815 20:02:48.101294 20887 solver.cpp:312] Iteration 16400 (5.15624 iter/s, 19.394s/100 iter), loss = 0.0689483
I0815 20:02:48.101315 20887 solver.cpp:334]     Train net output #0: loss = 0.0689482 (* 1 = 0.0689482 loss)
I0815 20:02:48.101320 20887 sgd_solver.cpp:136] Iteration 16400, lr = 0.0001, m = 0.9
I0815 20:03:07.524394 20887 solver.cpp:312] Iteration 16500 (5.14865 iter/s, 19.4226s/100 iter), loss = 0.0893815
I0815 20:03:07.524423 20887 solver.cpp:334]     Train net output #0: loss = 0.0893814 (* 1 = 0.0893814 loss)
I0815 20:03:07.524430 20887 sgd_solver.cpp:136] Iteration 16500, lr = 0.0001, m = 0.9
I0815 20:03:27.080693 20887 solver.cpp:312] Iteration 16600 (5.11358 iter/s, 19.5558s/100 iter), loss = 0.0923838
I0815 20:03:27.080737 20887 solver.cpp:334]     Train net output #0: loss = 0.0923837 (* 1 = 0.0923837 loss)
I0815 20:03:27.080744 20887 sgd_solver.cpp:136] Iteration 16600, lr = 0.0001, m = 0.9
I0815 20:03:44.686127 20893 data_reader.cpp:288] Starting prefetch of epoch 14
I0815 20:03:46.641327 20887 solver.cpp:312] Iteration 16700 (5.11245 iter/s, 19.5601s/100 iter), loss = 0.278854
I0815 20:03:46.641358 20887 solver.cpp:334]     Train net output #0: loss = 0.278854 (* 1 = 0.278854 loss)
I0815 20:03:46.641364 20887 sgd_solver.cpp:136] Iteration 16700, lr = 0.0001, m = 0.9
I0815 20:04:06.117625 20887 solver.cpp:312] Iteration 16800 (5.13459 iter/s, 19.4758s/100 iter), loss = 0.11335
I0815 20:04:06.117671 20887 solver.cpp:334]     Train net output #0: loss = 0.113349 (* 1 = 0.113349 loss)
I0815 20:04:06.117677 20887 sgd_solver.cpp:136] Iteration 16800, lr = 0.0001, m = 0.9
I0815 20:04:16.839162 20891 data_reader.cpp:288] Starting prefetch of epoch 9
I0815 20:04:25.353484 20887 solver.cpp:312] Iteration 16900 (5.19877 iter/s, 19.2353s/100 iter), loss = 0.097402
I0815 20:04:25.353513 20887 solver.cpp:334]     Train net output #0: loss = 0.0974018 (* 1 = 0.0974018 loss)
I0815 20:04:25.353518 20887 sgd_solver.cpp:136] Iteration 16900, lr = 0.0001, m = 0.9
I0815 20:04:44.864217 20887 solver.cpp:312] Iteration 17000 (5.12552 iter/s, 19.5102s/100 iter), loss = 0.148773
I0815 20:04:44.864281 20887 solver.cpp:334]     Train net output #0: loss = 0.148772 (* 1 = 0.148772 loss)
I0815 20:04:44.864286 20887 sgd_solver.cpp:136] Iteration 17000, lr = 0.0001, m = 0.9
I0815 20:05:04.353549 20887 solver.cpp:312] Iteration 17100 (5.13115 iter/s, 19.4888s/100 iter), loss = 0.121164
I0815 20:05:04.353572 20887 solver.cpp:334]     Train net output #0: loss = 0.121164 (* 1 = 0.121164 loss)
I0815 20:05:04.353579 20887 sgd_solver.cpp:136] Iteration 17100, lr = 0.0001, m = 0.9
I0815 20:05:21.132386 20890 data_reader.cpp:288] Starting prefetch of epoch 10
I0815 20:05:24.020932 20887 solver.cpp:312] Iteration 17200 (5.0847 iter/s, 19.6668s/100 iter), loss = 0.12643
I0815 20:05:24.020959 20887 solver.cpp:334]     Train net output #0: loss = 0.12643 (* 1 = 0.12643 loss)
I0815 20:05:24.020967 20887 sgd_solver.cpp:136] Iteration 17200, lr = 0.0001, m = 0.9
I0815 20:05:43.629972 20887 solver.cpp:312] Iteration 17300 (5.09983 iter/s, 19.6085s/100 iter), loss = 0.0779089
I0815 20:05:43.630002 20887 solver.cpp:334]     Train net output #0: loss = 0.0779087 (* 1 = 0.0779087 loss)
I0815 20:05:43.630007 20887 sgd_solver.cpp:136] Iteration 17300, lr = 0.0001, m = 0.9
I0815 20:06:03.305611 20887 solver.cpp:312] Iteration 17400 (5.08257 iter/s, 19.6751s/100 iter), loss = 0.0663688
I0815 20:06:03.305665 20887 solver.cpp:334]     Train net output #0: loss = 0.0663686 (* 1 = 0.0663686 loss)
I0815 20:06:03.305670 20887 sgd_solver.cpp:136] Iteration 17400, lr = 0.0001, m = 0.9
I0815 20:06:22.790623 20887 solver.cpp:312] Iteration 17500 (5.13229 iter/s, 19.4845s/100 iter), loss = 0.0645475
I0815 20:06:22.790645 20887 solver.cpp:334]     Train net output #0: loss = 0.0645473 (* 1 = 0.0645473 loss)
I0815 20:06:22.790649 20887 sgd_solver.cpp:136] Iteration 17500, lr = 0.0001, m = 0.9
I0815 20:06:25.947602 20841 data_reader.cpp:288] Starting prefetch of epoch 12
I0815 20:06:42.135861 20887 solver.cpp:312] Iteration 17600 (5.16937 iter/s, 19.3447s/100 iter), loss = 0.188118
I0815 20:06:42.135911 20887 solver.cpp:334]     Train net output #0: loss = 0.188117 (* 1 = 0.188117 loss)
I0815 20:06:42.135916 20887 sgd_solver.cpp:136] Iteration 17600, lr = 0.0001, m = 0.9
I0815 20:06:57.822511 20830 data_reader.cpp:288] Starting prefetch of epoch 12
I0815 20:07:01.445495 20887 solver.cpp:312] Iteration 17700 (5.1789 iter/s, 19.3091s/100 iter), loss = 0.0839225
I0815 20:07:01.445516 20887 solver.cpp:334]     Train net output #0: loss = 0.0839223 (* 1 = 0.0839223 loss)
I0815 20:07:01.445520 20887 sgd_solver.cpp:136] Iteration 17700, lr = 0.0001, m = 0.9
I0815 20:07:20.699311 20887 solver.cpp:312] Iteration 17800 (5.19392 iter/s, 19.2533s/100 iter), loss = 0.0822247
I0815 20:07:20.699369 20887 solver.cpp:334]     Train net output #0: loss = 0.0822245 (* 1 = 0.0822245 loss)
I0815 20:07:20.699378 20887 sgd_solver.cpp:136] Iteration 17800, lr = 0.0001, m = 0.9
I0815 20:07:40.284270 20887 solver.cpp:312] Iteration 17900 (5.1061 iter/s, 19.5844s/100 iter), loss = 0.120198
I0815 20:07:40.284299 20887 solver.cpp:334]     Train net output #0: loss = 0.120198 (* 1 = 0.120198 loss)
I0815 20:07:40.284303 20887 sgd_solver.cpp:136] Iteration 17900, lr = 0.0001, m = 0.9
I0815 20:07:59.368239 20887 solver.cpp:509] Iteration 18000, Testing net (#0)
I0815 20:08:06.848153 20937 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 20:08:11.124701 20887 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.945952
I0815 20:08:11.124724 20887 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999882
I0815 20:08:11.124730 20887 solver.cpp:594]     Test net output #2: loss = 0.174745 (* 1 = 0.174745 loss)
I0815 20:08:11.124822 20887 solver.cpp:264] [MultiGPU] Tests completed in 11.7563s
I0815 20:08:11.335078 20887 solver.cpp:312] Iteration 18000 (3.22062 iter/s, 31.05s/100 iter), loss = 0.0976373
I0815 20:08:11.335105 20887 solver.cpp:334]     Train net output #0: loss = 0.0976371 (* 1 = 0.0976371 loss)
I0815 20:08:11.335111 20887 sgd_solver.cpp:136] Iteration 18000, lr = 0.0001, m = 0.9
I0815 20:08:30.528764 20887 solver.cpp:312] Iteration 18100 (5.21019 iter/s, 19.1932s/100 iter), loss = 0.109155
I0815 20:08:30.530510 20887 solver.cpp:334]     Train net output #0: loss = 0.109155 (* 1 = 0.109155 loss)
I0815 20:08:30.530539 20887 sgd_solver.cpp:136] Iteration 18100, lr = 0.0001, m = 0.9
I0815 20:08:45.702811 20841 data_reader.cpp:288] Starting prefetch of epoch 13
I0815 20:08:50.216902 20887 solver.cpp:312] Iteration 18200 (5.07934 iter/s, 19.6876s/100 iter), loss = 0.121216
I0815 20:08:50.216925 20887 solver.cpp:334]     Train net output #0: loss = 0.121216 (* 1 = 0.121216 loss)
I0815 20:08:50.216930 20887 sgd_solver.cpp:136] Iteration 18200, lr = 0.0001, m = 0.9
I0815 20:09:09.819298 20887 solver.cpp:312] Iteration 18300 (5.10156 iter/s, 19.6019s/100 iter), loss = 0.142155
I0815 20:09:09.819372 20887 solver.cpp:334]     Train net output #0: loss = 0.142155 (* 1 = 0.142155 loss)
I0815 20:09:09.819380 20887 sgd_solver.cpp:136] Iteration 18300, lr = 0.0001, m = 0.9
I0815 20:09:18.141571 20891 data_reader.cpp:288] Starting prefetch of epoch 10
I0815 20:09:29.401952 20887 solver.cpp:312] Iteration 18400 (5.1067 iter/s, 19.5821s/100 iter), loss = 0.0906389
I0815 20:09:29.401976 20887 solver.cpp:334]     Train net output #0: loss = 0.0906388 (* 1 = 0.0906388 loss)
I0815 20:09:29.401980 20887 sgd_solver.cpp:136] Iteration 18400, lr = 0.0001, m = 0.9
I0815 20:09:49.022663 20887 solver.cpp:312] Iteration 18500 (5.0968 iter/s, 19.6202s/100 iter), loss = 0.167884
I0815 20:09:49.023308 20887 solver.cpp:334]     Train net output #0: loss = 0.167884 (* 1 = 0.167884 loss)
I0815 20:09:49.023327 20887 sgd_solver.cpp:136] Iteration 18500, lr = 0.0001, m = 0.9
I0815 20:10:08.511674 20887 solver.cpp:312] Iteration 18600 (5.13124 iter/s, 19.4885s/100 iter), loss = 0.119794
I0815 20:10:08.511703 20887 solver.cpp:334]     Train net output #0: loss = 0.119793 (* 1 = 0.119793 loss)
I0815 20:10:08.511706 20887 sgd_solver.cpp:136] Iteration 18600, lr = 0.0001, m = 0.9
I0815 20:10:22.866700 20890 data_reader.cpp:288] Starting prefetch of epoch 11
I0815 20:10:28.082607 20887 solver.cpp:312] Iteration 18700 (5.10976 iter/s, 19.5704s/100 iter), loss = 0.0585748
I0815 20:10:28.082628 20887 solver.cpp:334]     Train net output #0: loss = 0.0585746 (* 1 = 0.0585746 loss)
I0815 20:10:28.082633 20887 sgd_solver.cpp:136] Iteration 18700, lr = 0.0001, m = 0.9
I0815 20:10:47.436853 20887 solver.cpp:312] Iteration 18800 (5.16697 iter/s, 19.3537s/100 iter), loss = 0.25228
I0815 20:10:47.436878 20887 solver.cpp:334]     Train net output #0: loss = 0.252279 (* 1 = 0.252279 loss)
I0815 20:10:47.436884 20887 sgd_solver.cpp:136] Iteration 18800, lr = 0.0001, m = 0.9
I0815 20:11:06.876497 20887 solver.cpp:312] Iteration 18900 (5.14427 iter/s, 19.4391s/100 iter), loss = 0.117324
I0815 20:11:06.876577 20887 solver.cpp:334]     Train net output #0: loss = 0.117324 (* 1 = 0.117324 loss)
I0815 20:11:06.876585 20887 sgd_solver.cpp:136] Iteration 18900, lr = 0.0001, m = 0.9
I0815 20:11:26.258443 20887 solver.cpp:312] Iteration 19000 (5.15958 iter/s, 19.3814s/100 iter), loss = 0.214046
I0815 20:11:26.258462 20887 solver.cpp:334]     Train net output #0: loss = 0.214046 (* 1 = 0.214046 loss)
I0815 20:11:26.258467 20887 sgd_solver.cpp:136] Iteration 19000, lr = 0.0001, m = 0.9
I0815 20:11:26.861572 20890 data_reader.cpp:288] Starting prefetch of epoch 12
I0815 20:11:45.644176 20887 solver.cpp:312] Iteration 19100 (5.15858 iter/s, 19.3852s/100 iter), loss = 0.0713238
I0815 20:11:45.644225 20887 solver.cpp:334]     Train net output #0: loss = 0.0713236 (* 1 = 0.0713236 loss)
I0815 20:11:45.644230 20887 sgd_solver.cpp:136] Iteration 19100, lr = 0.0001, m = 0.9
I0815 20:11:59.076792 20891 data_reader.cpp:288] Starting prefetch of epoch 11
I0815 20:12:05.027456 20887 solver.cpp:312] Iteration 19200 (5.15923 iter/s, 19.3827s/100 iter), loss = 0.123743
I0815 20:12:05.027479 20887 solver.cpp:334]     Train net output #0: loss = 0.123743 (* 1 = 0.123743 loss)
I0815 20:12:05.027485 20887 sgd_solver.cpp:136] Iteration 19200, lr = 0.0001, m = 0.9
I0815 20:12:24.204676 20887 solver.cpp:312] Iteration 19300 (5.21466 iter/s, 19.1767s/100 iter), loss = 0.125956
I0815 20:12:24.204746 20887 solver.cpp:334]     Train net output #0: loss = 0.125955 (* 1 = 0.125955 loss)
I0815 20:12:24.204751 20887 sgd_solver.cpp:136] Iteration 19300, lr = 0.0001, m = 0.9
I0815 20:12:43.657680 20887 solver.cpp:312] Iteration 19400 (5.14074 iter/s, 19.4525s/100 iter), loss = 0.121584
I0815 20:12:43.657709 20887 solver.cpp:334]     Train net output #0: loss = 0.121584 (* 1 = 0.121584 loss)
I0815 20:12:43.657716 20887 sgd_solver.cpp:136] Iteration 19400, lr = 0.0001, m = 0.9
I0815 20:13:02.750362 20890 data_reader.cpp:288] Starting prefetch of epoch 13
I0815 20:13:02.914042 20887 solver.cpp:312] Iteration 19500 (5.19323 iter/s, 19.2558s/100 iter), loss = 0.0895744
I0815 20:13:02.914067 20887 solver.cpp:334]     Train net output #0: loss = 0.0895742 (* 1 = 0.0895742 loss)
I0815 20:13:02.914072 20887 sgd_solver.cpp:136] Iteration 19500, lr = 0.0001, m = 0.9
I0815 20:13:22.229951 20887 solver.cpp:312] Iteration 19600 (5.17722 iter/s, 19.3154s/100 iter), loss = 0.0977333
I0815 20:13:22.229979 20887 solver.cpp:334]     Train net output #0: loss = 0.0977331 (* 1 = 0.0977331 loss)
I0815 20:13:22.229986 20887 sgd_solver.cpp:136] Iteration 19600, lr = 0.0001, m = 0.9
I0815 20:13:41.852233 20887 solver.cpp:312] Iteration 19700 (5.09639 iter/s, 19.6217s/100 iter), loss = 0.816596
I0815 20:13:41.852298 20887 solver.cpp:334]     Train net output #0: loss = 0.816596 (* 1 = 0.816596 loss)
I0815 20:13:41.852304 20887 sgd_solver.cpp:136] Iteration 19700, lr = 0.0001, m = 0.9
I0815 20:14:01.161156 20887 solver.cpp:312] Iteration 19800 (5.1791 iter/s, 19.3084s/100 iter), loss = 0.0724904
I0815 20:14:01.161187 20887 solver.cpp:334]     Train net output #0: loss = 0.0724903 (* 1 = 0.0724903 loss)
I0815 20:14:01.161195 20887 sgd_solver.cpp:136] Iteration 19800, lr = 0.0001, m = 0.9
I0815 20:14:06.941290 20841 data_reader.cpp:288] Starting prefetch of epoch 14
I0815 20:14:20.620399 20887 solver.cpp:312] Iteration 19900 (5.13909 iter/s, 19.4587s/100 iter), loss = 0.0907687
I0815 20:14:20.620451 20887 solver.cpp:334]     Train net output #0: loss = 0.0907686 (* 1 = 0.0907686 loss)
I0815 20:14:20.620458 20887 sgd_solver.cpp:136] Iteration 19900, lr = 0.0001, m = 0.9
I0815 20:14:39.062800 20891 data_reader.cpp:288] Starting prefetch of epoch 12
I0815 20:14:39.822628 20887 solver.cpp:639] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/initial/cityscapes5_jsegnet21v2_iter_20000.caffemodel
I0815 20:14:39.914824 20887 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/initial/cityscapes5_jsegnet21v2_iter_20000.solverstate
I0815 20:14:39.922950 20887 solver.cpp:509] Iteration 20000, Testing net (#0)
I0815 20:14:51.099223 20933 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 20:14:51.579118 20887 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.948568
I0815 20:14:51.579141 20887 solver.cpp:594]     Test net output #1: accuracy/top5 = 1
I0815 20:14:51.579147 20887 solver.cpp:594]     Test net output #2: loss = 0.149101 (* 1 = 0.149101 loss)
I0815 20:14:51.579273 20887 solver.cpp:264] [MultiGPU] Tests completed in 11.656s
I0815 20:14:51.791076 20887 solver.cpp:312] Iteration 20000 (3.20823 iter/s, 31.1698s/100 iter), loss = 0.0764047
I0815 20:14:51.791103 20887 solver.cpp:334]     Train net output #0: loss = 0.0764045 (* 1 = 0.0764045 loss)
I0815 20:14:51.791107 20887 sgd_solver.cpp:136] Iteration 20000, lr = 0.0001, m = 0.9
I0815 20:15:11.250434 20887 solver.cpp:312] Iteration 20100 (5.13906 iter/s, 19.4588s/100 iter), loss = 0.0507235
I0815 20:15:11.250463 20887 solver.cpp:334]     Train net output #0: loss = 0.0507233 (* 1 = 0.0507233 loss)
I0815 20:15:11.250468 20887 sgd_solver.cpp:136] Iteration 20100, lr = 0.0001, m = 0.9
I0815 20:15:30.863703 20887 solver.cpp:312] Iteration 20200 (5.09873 iter/s, 19.6127s/100 iter), loss = 0.113502
I0815 20:15:30.863780 20887 solver.cpp:334]     Train net output #0: loss = 0.113502 (* 1 = 0.113502 loss)
I0815 20:15:30.863786 20887 sgd_solver.cpp:136] Iteration 20200, lr = 0.0001, m = 0.9
I0815 20:15:50.117161 20887 solver.cpp:312] Iteration 20300 (5.19402 iter/s, 19.2529s/100 iter), loss = 0.0818793
I0815 20:15:50.117185 20887 solver.cpp:334]     Train net output #0: loss = 0.0818792 (* 1 = 0.0818792 loss)
I0815 20:15:50.117192 20887 sgd_solver.cpp:136] Iteration 20300, lr = 0.0001, m = 0.9
I0815 20:15:55.258770 20890 data_reader.cpp:288] Starting prefetch of epoch 14
I0815 20:16:09.679267 20887 solver.cpp:312] Iteration 20400 (5.11207 iter/s, 19.5616s/100 iter), loss = 0.115193
I0815 20:16:09.679324 20887 solver.cpp:334]     Train net output #0: loss = 0.115193 (* 1 = 0.115193 loss)
I0815 20:16:09.679332 20887 sgd_solver.cpp:136] Iteration 20400, lr = 0.0001, m = 0.9
I0815 20:16:29.137717 20887 solver.cpp:312] Iteration 20500 (5.1393 iter/s, 19.4579s/100 iter), loss = 0.076128
I0815 20:16:29.137766 20887 solver.cpp:334]     Train net output #0: loss = 0.0761278 (* 1 = 0.0761278 loss)
I0815 20:16:29.137779 20887 sgd_solver.cpp:136] Iteration 20500, lr = 0.0001, m = 0.9
I0815 20:16:48.739567 20887 solver.cpp:312] Iteration 20600 (5.1017 iter/s, 19.6013s/100 iter), loss = 0.0848693
I0815 20:16:48.739657 20887 solver.cpp:334]     Train net output #0: loss = 0.0848692 (* 1 = 0.0848692 loss)
I0815 20:16:48.739678 20887 sgd_solver.cpp:136] Iteration 20600, lr = 0.0001, m = 0.9
I0815 20:16:59.641152 20895 data_reader.cpp:288] Starting prefetch of epoch 7
I0815 20:17:08.253819 20887 solver.cpp:312] Iteration 20700 (5.1246 iter/s, 19.5137s/100 iter), loss = 0.071831
I0815 20:17:08.253893 20887 solver.cpp:334]     Train net output #0: loss = 0.0718309 (* 1 = 0.0718309 loss)
I0815 20:17:08.253917 20887 sgd_solver.cpp:136] Iteration 20700, lr = 0.0001, m = 0.9
I0815 20:17:27.823114 20887 solver.cpp:312] Iteration 20800 (5.11019 iter/s, 19.5688s/100 iter), loss = 0.0770694
I0815 20:17:27.823160 20887 solver.cpp:334]     Train net output #0: loss = 0.0770692 (* 1 = 0.0770692 loss)
I0815 20:17:27.823168 20887 sgd_solver.cpp:136] Iteration 20800, lr = 0.0001, m = 0.9
I0815 20:17:31.977843 20830 data_reader.cpp:288] Starting prefetch of epoch 13
I0815 20:17:47.322957 20887 solver.cpp:312] Iteration 20900 (5.12839 iter/s, 19.4993s/100 iter), loss = 0.0606319
I0815 20:17:47.322980 20887 solver.cpp:334]     Train net output #0: loss = 0.0606317 (* 1 = 0.0606317 loss)
I0815 20:17:47.322984 20887 sgd_solver.cpp:136] Iteration 20900, lr = 0.0001, m = 0.9
I0815 20:18:06.822839 20887 solver.cpp:312] Iteration 21000 (5.12838 iter/s, 19.4993s/100 iter), loss = 0.154579
I0815 20:18:06.822888 20887 solver.cpp:334]     Train net output #0: loss = 0.154579 (* 1 = 0.154579 loss)
I0815 20:18:06.822896 20887 sgd_solver.cpp:136] Iteration 21000, lr = 0.0001, m = 0.9
I0815 20:18:26.223424 20887 solver.cpp:312] Iteration 21100 (5.15463 iter/s, 19.4s/100 iter), loss = 0.0927383
I0815 20:18:26.223453 20887 solver.cpp:334]     Train net output #0: loss = 0.0927381 (* 1 = 0.0927381 loss)
I0815 20:18:26.223459 20887 sgd_solver.cpp:136] Iteration 21100, lr = 0.0001, m = 0.9
I0815 20:18:36.209447 20841 data_reader.cpp:288] Starting prefetch of epoch 15
I0815 20:18:45.529616 20887 solver.cpp:312] Iteration 21200 (5.17983 iter/s, 19.3057s/100 iter), loss = 0.111547
I0815 20:18:45.529669 20887 solver.cpp:334]     Train net output #0: loss = 0.111546 (* 1 = 0.111546 loss)
I0815 20:18:45.529675 20887 sgd_solver.cpp:136] Iteration 21200, lr = 0.0001, m = 0.9
I0815 20:19:04.810256 20887 solver.cpp:312] Iteration 21300 (5.18669 iter/s, 19.2801s/100 iter), loss = 0.100896
I0815 20:19:04.810276 20887 solver.cpp:334]     Train net output #0: loss = 0.100896 (* 1 = 0.100896 loss)
I0815 20:19:04.810279 20887 sgd_solver.cpp:136] Iteration 21300, lr = 0.0001, m = 0.9
I0815 20:19:24.137325 20887 solver.cpp:312] Iteration 21400 (5.17423 iter/s, 19.3265s/100 iter), loss = 0.0804823
I0815 20:19:24.138638 20887 solver.cpp:334]     Train net output #0: loss = 0.0804822 (* 1 = 0.0804822 loss)
I0815 20:19:24.138646 20887 sgd_solver.cpp:136] Iteration 21400, lr = 0.0001, m = 0.9
I0815 20:19:40.171567 20895 data_reader.cpp:288] Starting prefetch of epoch 8
I0815 20:19:43.488165 20887 solver.cpp:312] Iteration 21500 (5.16788 iter/s, 19.3503s/100 iter), loss = 0.0966118
I0815 20:19:43.488191 20887 solver.cpp:334]     Train net output #0: loss = 0.0966117 (* 1 = 0.0966117 loss)
I0815 20:19:43.488195 20887 sgd_solver.cpp:136] Iteration 21500, lr = 0.0001, m = 0.9
I0815 20:20:02.958060 20887 solver.cpp:312] Iteration 21600 (5.13628 iter/s, 19.4694s/100 iter), loss = 0.117883
I0815 20:20:02.958106 20887 solver.cpp:334]     Train net output #0: loss = 0.117883 (* 1 = 0.117883 loss)
I0815 20:20:02.958111 20887 sgd_solver.cpp:136] Iteration 21600, lr = 0.0001, m = 0.9
I0815 20:20:12.291733 20830 data_reader.cpp:288] Starting prefetch of epoch 14
I0815 20:20:22.295228 20887 solver.cpp:312] Iteration 21700 (5.17153 iter/s, 19.3366s/100 iter), loss = 0.0889876
I0815 20:20:22.295256 20887 solver.cpp:334]     Train net output #0: loss = 0.0889875 (* 1 = 0.0889875 loss)
I0815 20:20:22.295262 20887 sgd_solver.cpp:136] Iteration 21700, lr = 0.0001, m = 0.9
I0815 20:20:41.622429 20887 solver.cpp:312] Iteration 21800 (5.1742 iter/s, 19.3267s/100 iter), loss = 0.0983334
I0815 20:20:41.622483 20887 solver.cpp:334]     Train net output #0: loss = 0.0983332 (* 1 = 0.0983332 loss)
I0815 20:20:41.622488 20887 sgd_solver.cpp:136] Iteration 21800, lr = 0.0001, m = 0.9
I0815 20:21:01.323627 20887 solver.cpp:312] Iteration 21900 (5.07597 iter/s, 19.7007s/100 iter), loss = 0.0905187
I0815 20:21:01.323652 20887 solver.cpp:334]     Train net output #0: loss = 0.0905185 (* 1 = 0.0905185 loss)
I0815 20:21:01.323657 20887 sgd_solver.cpp:136] Iteration 21900, lr = 0.0001, m = 0.9
I0815 20:21:16.550317 20891 data_reader.cpp:288] Starting prefetch of epoch 13
I0815 20:21:20.577914 20887 solver.cpp:509] Iteration 22000, Testing net (#0)
I0815 20:21:32.542434 20887 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.949965
I0815 20:21:32.542459 20887 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999914
I0815 20:21:32.542464 20887 solver.cpp:594]     Test net output #2: loss = 0.159075 (* 1 = 0.159075 loss)
I0815 20:21:32.542558 20887 solver.cpp:264] [MultiGPU] Tests completed in 11.9643s
I0815 20:21:32.761438 20887 solver.cpp:312] Iteration 22000 (3.18097 iter/s, 31.437s/100 iter), loss = 0.122632
I0815 20:21:32.761463 20887 solver.cpp:334]     Train net output #0: loss = 0.122632 (* 1 = 0.122632 loss)
I0815 20:21:32.761469 20887 sgd_solver.cpp:136] Iteration 22000, lr = 0.0001, m = 0.9
I0815 20:21:52.199527 20887 solver.cpp:312] Iteration 22100 (5.14468 iter/s, 19.4375s/100 iter), loss = 0.159587
I0815 20:21:52.199580 20887 solver.cpp:334]     Train net output #0: loss = 0.159587 (* 1 = 0.159587 loss)
I0815 20:21:52.199587 20887 sgd_solver.cpp:136] Iteration 22100, lr = 0.0001, m = 0.9
I0815 20:22:00.837425 20893 data_reader.cpp:288] Starting prefetch of epoch 15
I0815 20:22:11.575182 20887 solver.cpp:312] Iteration 22200 (5.16126 iter/s, 19.3751s/100 iter), loss = 0.120574
I0815 20:22:11.575242 20887 solver.cpp:334]     Train net output #0: loss = 0.120574 (* 1 = 0.120574 loss)
I0815 20:22:11.575263 20887 sgd_solver.cpp:136] Iteration 22200, lr = 0.0001, m = 0.9
I0815 20:22:31.077555 20887 solver.cpp:312] Iteration 22300 (5.12772 iter/s, 19.5018s/100 iter), loss = 0.0654961
I0815 20:22:31.077656 20887 solver.cpp:334]     Train net output #0: loss = 0.065496 (* 1 = 0.065496 loss)
I0815 20:22:31.077673 20887 sgd_solver.cpp:136] Iteration 22300, lr = 0.0001, m = 0.9
I0815 20:22:32.787011 20890 data_reader.cpp:288] Starting prefetch of epoch 15
I0815 20:22:50.249833 20887 solver.cpp:312] Iteration 22400 (5.21601 iter/s, 19.1717s/100 iter), loss = 0.0960178
I0815 20:22:50.249856 20887 solver.cpp:334]     Train net output #0: loss = 0.0960176 (* 1 = 0.0960176 loss)
I0815 20:22:50.249861 20887 sgd_solver.cpp:136] Iteration 22400, lr = 0.0001, m = 0.9
I0815 20:23:09.643633 20887 solver.cpp:312] Iteration 22500 (5.15643 iter/s, 19.3933s/100 iter), loss = 0.120441
I0815 20:23:09.643702 20887 solver.cpp:334]     Train net output #0: loss = 0.120441 (* 1 = 0.120441 loss)
I0815 20:23:09.643707 20887 sgd_solver.cpp:136] Iteration 22500, lr = 0.0001, m = 0.9
I0815 20:23:28.875548 20887 solver.cpp:312] Iteration 22600 (5.19983 iter/s, 19.2314s/100 iter), loss = 0.0817216
I0815 20:23:28.875572 20887 solver.cpp:334]     Train net output #0: loss = 0.0817215 (* 1 = 0.0817215 loss)
I0815 20:23:28.875576 20887 sgd_solver.cpp:136] Iteration 22600, lr = 0.0001, m = 0.9
I0815 20:23:36.673696 20893 data_reader.cpp:288] Starting prefetch of epoch 16
I0815 20:23:48.490190 20887 solver.cpp:312] Iteration 22700 (5.09837 iter/s, 19.6141s/100 iter), loss = 0.0928479
I0815 20:23:48.490275 20887 solver.cpp:334]     Train net output #0: loss = 0.0928478 (* 1 = 0.0928478 loss)
I0815 20:23:48.490283 20887 sgd_solver.cpp:136] Iteration 22700, lr = 0.0001, m = 0.9
I0815 20:24:07.895961 20887 solver.cpp:312] Iteration 22800 (5.15325 iter/s, 19.4052s/100 iter), loss = 0.0514077
I0815 20:24:07.895989 20887 solver.cpp:334]     Train net output #0: loss = 0.0514076 (* 1 = 0.0514076 loss)
I0815 20:24:07.895995 20887 sgd_solver.cpp:136] Iteration 22800, lr = 0.0001, m = 0.9
I0815 20:24:27.221199 20887 solver.cpp:312] Iteration 22900 (5.17472 iter/s, 19.3247s/100 iter), loss = 0.0990283
I0815 20:24:27.221299 20887 solver.cpp:334]     Train net output #0: loss = 0.0990282 (* 1 = 0.0990282 loss)
I0815 20:24:27.221307 20887 sgd_solver.cpp:136] Iteration 22900, lr = 0.0001, m = 0.9
I0815 20:24:40.971360 20893 data_reader.cpp:288] Starting prefetch of epoch 17
I0815 20:24:46.717125 20887 solver.cpp:312] Iteration 23000 (5.12942 iter/s, 19.4954s/100 iter), loss = 0.108347
I0815 20:24:46.717149 20887 solver.cpp:334]     Train net output #0: loss = 0.108347 (* 1 = 0.108347 loss)
I0815 20:24:46.717152 20887 sgd_solver.cpp:136] Iteration 23000, lr = 0.0001, m = 0.9
I0815 20:25:06.212220 20887 solver.cpp:312] Iteration 23100 (5.12964 iter/s, 19.4946s/100 iter), loss = 0.0831834
I0815 20:25:06.212267 20887 solver.cpp:334]     Train net output #0: loss = 0.0831833 (* 1 = 0.0831833 loss)
I0815 20:25:06.212272 20887 sgd_solver.cpp:136] Iteration 23100, lr = 0.0001, m = 0.9
I0815 20:25:13.096894 20891 data_reader.cpp:288] Starting prefetch of epoch 14
I0815 20:25:25.797545 20887 solver.cpp:312] Iteration 23200 (5.106 iter/s, 19.5848s/100 iter), loss = 0.0873131
I0815 20:25:25.797569 20887 solver.cpp:334]     Train net output #0: loss = 0.087313 (* 1 = 0.087313 loss)
I0815 20:25:25.797575 20887 sgd_solver.cpp:136] Iteration 23200, lr = 0.0001, m = 0.9
I0815 20:25:45.161249 20887 solver.cpp:312] Iteration 23300 (5.16444 iter/s, 19.3632s/100 iter), loss = 0.0548812
I0815 20:25:45.161301 20887 solver.cpp:334]     Train net output #0: loss = 0.0548812 (* 1 = 0.0548812 loss)
I0815 20:25:45.161306 20887 sgd_solver.cpp:136] Iteration 23300, lr = 0.0001, m = 0.9
I0815 20:26:04.677160 20887 solver.cpp:312] Iteration 23400 (5.12417 iter/s, 19.5154s/100 iter), loss = 0.0814971
I0815 20:26:04.677184 20887 solver.cpp:334]     Train net output #0: loss = 0.081497 (* 1 = 0.081497 loss)
I0815 20:26:04.677188 20887 sgd_solver.cpp:136] Iteration 23400, lr = 0.0001, m = 0.9
I0815 20:26:17.604169 20841 data_reader.cpp:288] Starting prefetch of epoch 16
I0815 20:26:24.107429 20887 solver.cpp:312] Iteration 23500 (5.14675 iter/s, 19.4297s/100 iter), loss = 0.279218
I0815 20:26:24.107455 20887 solver.cpp:334]     Train net output #0: loss = 0.279218 (* 1 = 0.279218 loss)
I0815 20:26:24.107460 20887 sgd_solver.cpp:136] Iteration 23500, lr = 0.0001, m = 0.9
I0815 20:26:43.346532 20887 solver.cpp:312] Iteration 23600 (5.19789 iter/s, 19.2386s/100 iter), loss = 0.0660515
I0815 20:26:43.346556 20887 solver.cpp:334]     Train net output #0: loss = 0.0660514 (* 1 = 0.0660514 loss)
I0815 20:26:43.346563 20887 sgd_solver.cpp:136] Iteration 23600, lr = 0.0001, m = 0.9
I0815 20:27:02.486438 20887 solver.cpp:312] Iteration 23700 (5.22483 iter/s, 19.1394s/100 iter), loss = 0.0793521
I0815 20:27:02.486508 20887 solver.cpp:334]     Train net output #0: loss = 0.079352 (* 1 = 0.079352 loss)
I0815 20:27:02.486513 20887 sgd_solver.cpp:136] Iteration 23700, lr = 0.0001, m = 0.9
I0815 20:27:21.002897 20893 data_reader.cpp:288] Starting prefetch of epoch 18
I0815 20:27:21.768245 20887 solver.cpp:312] Iteration 23800 (5.18638 iter/s, 19.2813s/100 iter), loss = 0.156531
I0815 20:27:21.768265 20887 solver.cpp:334]     Train net output #0: loss = 0.15653 (* 1 = 0.15653 loss)
I0815 20:27:21.768268 20887 sgd_solver.cpp:136] Iteration 23800, lr = 0.0001, m = 0.9
I0815 20:27:41.036820 20887 solver.cpp:312] Iteration 23900 (5.18994 iter/s, 19.2681s/100 iter), loss = 0.0700453
I0815 20:27:41.036866 20887 solver.cpp:334]     Train net output #0: loss = 0.0700452 (* 1 = 0.0700452 loss)
I0815 20:27:41.036873 20887 sgd_solver.cpp:136] Iteration 23900, lr = 0.0001, m = 0.9
I0815 20:27:53.175218 20891 data_reader.cpp:288] Starting prefetch of epoch 15
I0815 20:28:00.429504 20887 solver.cpp:509] Iteration 24000, Testing net (#0)
I0815 20:28:12.191305 20885 data_reader.cpp:288] Starting prefetch of epoch 4
I0815 20:28:12.731998 20887 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.947983
I0815 20:28:12.732018 20887 solver.cpp:594]     Test net output #1: accuracy/top5 = 1
I0815 20:28:12.732023 20887 solver.cpp:594]     Test net output #2: loss = 0.162361 (* 1 = 0.162361 loss)
I0815 20:28:12.732043 20887 solver.cpp:264] [MultiGPU] Tests completed in 12.3022s
I0815 20:28:12.833636 20956 sgd_solver.cpp:48] MultiStep Status: Iteration 24000, step = 1
I0815 20:28:12.833747 20955 sgd_solver.cpp:48] MultiStep Status: Iteration 24000, step = 1
I0815 20:28:12.833642 20957 sgd_solver.cpp:48] MultiStep Status: Iteration 24000, step = 1
I0815 20:28:12.936708 20887 solver.cpp:312] Iteration 24000 (3.13489 iter/s, 31.899s/100 iter), loss = 0.0949077
I0815 20:28:12.936738 20887 solver.cpp:334]     Train net output #0: loss = 0.0949076 (* 1 = 0.0949076 loss)
I0815 20:28:12.936745 20887 sgd_solver.cpp:136] Iteration 24000, lr = 1e-05, m = 0.9
I0815 20:28:32.367503 20887 solver.cpp:312] Iteration 24100 (5.14661 iter/s, 19.4303s/100 iter), loss = 0.108398
I0815 20:28:32.367527 20887 solver.cpp:334]     Train net output #0: loss = 0.108398 (* 1 = 0.108398 loss)
I0815 20:28:32.367532 20887 sgd_solver.cpp:136] Iteration 24100, lr = 1e-05, m = 0.9
I0815 20:28:51.744683 20887 solver.cpp:312] Iteration 24200 (5.16085 iter/s, 19.3766s/100 iter), loss = 0.194035
I0815 20:28:51.744730 20887 solver.cpp:334]     Train net output #0: loss = 0.194034 (* 1 = 0.194034 loss)
I0815 20:28:51.744735 20887 sgd_solver.cpp:136] Iteration 24200, lr = 1e-05, m = 0.9
I0815 20:29:09.580590 20895 data_reader.cpp:288] Starting prefetch of epoch 9
I0815 20:29:11.095831 20887 solver.cpp:312] Iteration 24300 (5.16779 iter/s, 19.3506s/100 iter), loss = 0.063287
I0815 20:29:11.095852 20887 solver.cpp:334]     Train net output #0: loss = 0.0632869 (* 1 = 0.0632869 loss)
I0815 20:29:11.095856 20887 sgd_solver.cpp:136] Iteration 24300, lr = 1e-05, m = 0.9
I0815 20:29:30.837043 20887 solver.cpp:312] Iteration 24400 (5.06569 iter/s, 19.7407s/100 iter), loss = 0.0860034
I0815 20:29:30.837137 20887 solver.cpp:334]     Train net output #0: loss = 0.0860033 (* 1 = 0.0860033 loss)
I0815 20:29:30.837155 20887 sgd_solver.cpp:136] Iteration 24400, lr = 1e-05, m = 0.9
I0815 20:29:42.047065 20891 data_reader.cpp:288] Starting prefetch of epoch 16
I0815 20:29:50.251183 20887 solver.cpp:312] Iteration 24500 (5.15103 iter/s, 19.4136s/100 iter), loss = 0.0670915
I0815 20:29:50.251206 20887 solver.cpp:334]     Train net output #0: loss = 0.0670914 (* 1 = 0.0670914 loss)
I0815 20:29:50.251210 20887 sgd_solver.cpp:136] Iteration 24500, lr = 1e-05, m = 0.9
I0815 20:30:09.855731 20887 solver.cpp:312] Iteration 24600 (5.101 iter/s, 19.604s/100 iter), loss = 0.109931
I0815 20:30:09.855782 20887 solver.cpp:334]     Train net output #0: loss = 0.109931 (* 1 = 0.109931 loss)
I0815 20:30:09.855788 20887 sgd_solver.cpp:136] Iteration 24600, lr = 1e-05, m = 0.9
I0815 20:30:29.281791 20887 solver.cpp:312] Iteration 24700 (5.14787 iter/s, 19.4255s/100 iter), loss = 0.0690158
I0815 20:30:29.281817 20887 solver.cpp:334]     Train net output #0: loss = 0.0690158 (* 1 = 0.0690158 loss)
I0815 20:30:29.281821 20887 sgd_solver.cpp:136] Iteration 24700, lr = 1e-05, m = 0.9
I0815 20:30:46.300076 20893 data_reader.cpp:288] Starting prefetch of epoch 19
I0815 20:30:48.592631 20887 solver.cpp:312] Iteration 24800 (5.17858 iter/s, 19.3103s/100 iter), loss = 0.111094
I0815 20:30:48.592658 20887 solver.cpp:334]     Train net output #0: loss = 0.111094 (* 1 = 0.111094 loss)
I0815 20:30:48.592665 20887 sgd_solver.cpp:136] Iteration 24800, lr = 1e-05, m = 0.9
I0815 20:31:07.895364 20887 solver.cpp:312] Iteration 24900 (5.18076 iter/s, 19.3022s/100 iter), loss = 0.0687769
I0815 20:31:07.895388 20887 solver.cpp:334]     Train net output #0: loss = 0.0687768 (* 1 = 0.0687768 loss)
I0815 20:31:07.895392 20887 sgd_solver.cpp:136] Iteration 24900, lr = 1e-05, m = 0.9
I0815 20:31:27.372643 20887 solver.cpp:312] Iteration 25000 (5.13433 iter/s, 19.4767s/100 iter), loss = 0.0898818
I0815 20:31:27.372692 20887 solver.cpp:334]     Train net output #0: loss = 0.0898817 (* 1 = 0.0898817 loss)
I0815 20:31:27.372699 20887 sgd_solver.cpp:136] Iteration 25000, lr = 1e-05, m = 0.9
I0815 20:31:46.785009 20887 solver.cpp:312] Iteration 25100 (5.1515 iter/s, 19.4118s/100 iter), loss = 0.082423
I0815 20:31:46.785033 20887 solver.cpp:334]     Train net output #0: loss = 0.0824229 (* 1 = 0.0824229 loss)
I0815 20:31:46.785037 20887 sgd_solver.cpp:136] Iteration 25100, lr = 1e-05, m = 0.9
I0815 20:31:50.517025 20830 data_reader.cpp:288] Starting prefetch of epoch 15
I0815 20:32:06.284994 20887 solver.cpp:312] Iteration 25200 (5.12835 iter/s, 19.4995s/100 iter), loss = 0.099597
I0815 20:32:06.285050 20887 solver.cpp:334]     Train net output #0: loss = 0.0995969 (* 1 = 0.0995969 loss)
I0815 20:32:06.285058 20887 sgd_solver.cpp:136] Iteration 25200, lr = 1e-05, m = 0.9
I0815 20:32:22.708191 20890 data_reader.cpp:288] Starting prefetch of epoch 16
I0815 20:32:25.755148 20887 solver.cpp:312] Iteration 25300 (5.13621 iter/s, 19.4696s/100 iter), loss = 0.0890651
I0815 20:32:25.755169 20887 solver.cpp:334]     Train net output #0: loss = 0.089065 (* 1 = 0.089065 loss)
I0815 20:32:25.755173 20887 sgd_solver.cpp:136] Iteration 25300, lr = 1e-05, m = 0.9
I0815 20:32:45.065402 20887 solver.cpp:312] Iteration 25400 (5.17874 iter/s, 19.3097s/100 iter), loss = 0.0951909
I0815 20:32:45.065452 20887 solver.cpp:334]     Train net output #0: loss = 0.0951909 (* 1 = 0.0951909 loss)
I0815 20:32:45.065457 20887 sgd_solver.cpp:136] Iteration 25400, lr = 1e-05, m = 0.9
I0815 20:33:04.481497 20887 solver.cpp:312] Iteration 25500 (5.15051 iter/s, 19.4156s/100 iter), loss = 0.0593201
I0815 20:33:04.481523 20887 solver.cpp:334]     Train net output #0: loss = 0.0593201 (* 1 = 0.0593201 loss)
I0815 20:33:04.481526 20887 sgd_solver.cpp:136] Iteration 25500, lr = 1e-05, m = 0.9
I0815 20:33:23.952814 20887 solver.cpp:312] Iteration 25600 (5.1359 iter/s, 19.4708s/100 iter), loss = 0.0830585
I0815 20:33:23.952865 20887 solver.cpp:334]     Train net output #0: loss = 0.0830585 (* 1 = 0.0830585 loss)
I0815 20:33:23.952869 20887 sgd_solver.cpp:136] Iteration 25600, lr = 1e-05, m = 0.9
I0815 20:33:26.963819 20890 data_reader.cpp:288] Starting prefetch of epoch 17
I0815 20:33:43.391345 20887 solver.cpp:312] Iteration 25700 (5.14456 iter/s, 19.438s/100 iter), loss = 0.0500879
I0815 20:33:43.391366 20887 solver.cpp:334]     Train net output #0: loss = 0.0500879 (* 1 = 0.0500879 loss)
I0815 20:33:43.391372 20887 sgd_solver.cpp:136] Iteration 25700, lr = 1e-05, m = 0.9
I0815 20:34:02.929395 20887 solver.cpp:312] Iteration 25800 (5.11836 iter/s, 19.5375s/100 iter), loss = 0.0783117
I0815 20:34:02.929440 20887 solver.cpp:334]     Train net output #0: loss = 0.0783116 (* 1 = 0.0783116 loss)
I0815 20:34:02.929446 20887 sgd_solver.cpp:136] Iteration 25800, lr = 1e-05, m = 0.9
I0815 20:34:22.479843 20887 solver.cpp:312] Iteration 25900 (5.11511 iter/s, 19.5499s/100 iter), loss = 0.0652774
I0815 20:34:22.479871 20887 solver.cpp:334]     Train net output #0: loss = 0.0652773 (* 1 = 0.0652773 loss)
I0815 20:34:22.479878 20887 sgd_solver.cpp:136] Iteration 25900, lr = 1e-05, m = 0.9
I0815 20:34:31.242030 20895 data_reader.cpp:288] Starting prefetch of epoch 10
I0815 20:34:41.662044 20887 solver.cpp:509] Iteration 26000, Testing net (#0)
I0815 20:34:49.173477 20939 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 20:34:53.435894 20887 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.951068
I0815 20:34:53.435914 20887 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999756
I0815 20:34:53.435921 20887 solver.cpp:594]     Test net output #2: loss = 0.163875 (* 1 = 0.163875 loss)
I0815 20:34:53.435945 20887 solver.cpp:264] [MultiGPU] Tests completed in 11.7736s
I0815 20:34:53.643824 20887 solver.cpp:312] Iteration 26000 (3.20892 iter/s, 31.1631s/100 iter), loss = 0.0971869
I0815 20:34:53.643848 20887 solver.cpp:334]     Train net output #0: loss = 0.0971867 (* 1 = 0.0971867 loss)
I0815 20:34:53.643854 20887 sgd_solver.cpp:136] Iteration 26000, lr = 1e-05, m = 0.9
I0815 20:35:13.167471 20887 solver.cpp:312] Iteration 26100 (5.12214 iter/s, 19.5231s/100 iter), loss = 0.0523584
I0815 20:35:13.167520 20887 solver.cpp:334]     Train net output #0: loss = 0.0523583 (* 1 = 0.0523583 loss)
I0815 20:35:13.167527 20887 sgd_solver.cpp:136] Iteration 26100, lr = 1e-05, m = 0.9
I0815 20:35:32.511937 20887 solver.cpp:312] Iteration 26200 (5.16958 iter/s, 19.3439s/100 iter), loss = 0.111994
I0815 20:35:32.511963 20887 solver.cpp:334]     Train net output #0: loss = 0.111994 (* 1 = 0.111994 loss)
I0815 20:35:32.511970 20887 sgd_solver.cpp:136] Iteration 26200, lr = 1e-05, m = 0.9
I0815 20:35:47.310266 20893 data_reader.cpp:288] Starting prefetch of epoch 20
I0815 20:35:51.954131 20887 solver.cpp:312] Iteration 26300 (5.14359 iter/s, 19.4417s/100 iter), loss = 0.090723
I0815 20:35:51.954152 20887 solver.cpp:334]     Train net output #0: loss = 0.0907229 (* 1 = 0.0907229 loss)
I0815 20:35:51.954157 20887 sgd_solver.cpp:136] Iteration 26300, lr = 1e-05, m = 0.9
I0815 20:36:11.518757 20887 solver.cpp:312] Iteration 26400 (5.11141 iter/s, 19.5641s/100 iter), loss = 0.136346
I0815 20:36:11.518781 20887 solver.cpp:334]     Train net output #0: loss = 0.136346 (* 1 = 0.136346 loss)
I0815 20:36:11.518785 20887 sgd_solver.cpp:136] Iteration 26400, lr = 1e-05, m = 0.9
I0815 20:36:30.849249 20887 solver.cpp:312] Iteration 26500 (5.17332 iter/s, 19.33s/100 iter), loss = 0.0550273
I0815 20:36:30.849326 20887 solver.cpp:334]     Train net output #0: loss = 0.0550272 (* 1 = 0.0550272 loss)
I0815 20:36:30.849334 20887 sgd_solver.cpp:136] Iteration 26500, lr = 1e-05, m = 0.9
I0815 20:36:50.182215 20887 solver.cpp:312] Iteration 26600 (5.17265 iter/s, 19.3324s/100 iter), loss = 0.0842122
I0815 20:36:50.182238 20887 solver.cpp:334]     Train net output #0: loss = 0.0842121 (* 1 = 0.0842121 loss)
I0815 20:36:50.182242 20887 sgd_solver.cpp:136] Iteration 26600, lr = 1e-05, m = 0.9
I0815 20:36:51.320736 20895 data_reader.cpp:288] Starting prefetch of epoch 11
I0815 20:37:09.532579 20887 solver.cpp:312] Iteration 26700 (5.168 iter/s, 19.3498s/100 iter), loss = 0.0756016
I0815 20:37:09.532629 20887 solver.cpp:334]     Train net output #0: loss = 0.0756015 (* 1 = 0.0756015 loss)
I0815 20:37:09.532634 20887 sgd_solver.cpp:136] Iteration 26700, lr = 1e-05, m = 0.9
I0815 20:37:23.363185 20830 data_reader.cpp:288] Starting prefetch of epoch 16
I0815 20:37:28.818567 20887 solver.cpp:312] Iteration 26800 (5.18525 iter/s, 19.2855s/100 iter), loss = 0.0964386
I0815 20:37:28.818595 20887 solver.cpp:334]     Train net output #0: loss = 0.0964386 (* 1 = 0.0964386 loss)
I0815 20:37:28.818603 20887 sgd_solver.cpp:136] Iteration 26800, lr = 1e-05, m = 0.9
I0815 20:37:48.216562 20887 solver.cpp:312] Iteration 26900 (5.15531 iter/s, 19.3975s/100 iter), loss = 0.0595216
I0815 20:37:48.216619 20887 solver.cpp:334]     Train net output #0: loss = 0.0595215 (* 1 = 0.0595215 loss)
I0815 20:37:48.216625 20887 sgd_solver.cpp:136] Iteration 26900, lr = 1e-05, m = 0.9
I0815 20:38:07.594070 20887 solver.cpp:312] Iteration 27000 (5.16076 iter/s, 19.377s/100 iter), loss = 0.191666
I0815 20:38:07.594099 20887 solver.cpp:334]     Train net output #0: loss = 0.191666 (* 1 = 0.191666 loss)
I0815 20:38:07.594105 20887 sgd_solver.cpp:136] Iteration 27000, lr = 1e-05, m = 0.9
I0815 20:38:27.079282 20887 solver.cpp:312] Iteration 27100 (5.13224 iter/s, 19.4847s/100 iter), loss = 0.0757632
I0815 20:38:27.079813 20887 solver.cpp:334]     Train net output #0: loss = 0.0757631 (* 1 = 0.0757631 loss)
I0815 20:38:27.079834 20887 sgd_solver.cpp:136] Iteration 27100, lr = 1e-05, m = 0.9
I0815 20:38:27.495735 20895 data_reader.cpp:288] Starting prefetch of epoch 12
I0815 20:38:46.565253 20887 solver.cpp:312] Iteration 27200 (5.13204 iter/s, 19.4854s/100 iter), loss = 0.0459424
I0815 20:38:46.565275 20887 solver.cpp:334]     Train net output #0: loss = 0.0459424 (* 1 = 0.0459424 loss)
I0815 20:38:46.565279 20887 sgd_solver.cpp:136] Iteration 27200, lr = 1e-05, m = 0.9
I0815 20:39:06.064127 20887 solver.cpp:312] Iteration 27300 (5.12864 iter/s, 19.4983s/100 iter), loss = 0.0799937
I0815 20:39:06.064209 20887 solver.cpp:334]     Train net output #0: loss = 0.0799936 (* 1 = 0.0799936 loss)
I0815 20:39:06.064216 20887 sgd_solver.cpp:136] Iteration 27300, lr = 1e-05, m = 0.9
I0815 20:39:25.598397 20887 solver.cpp:312] Iteration 27400 (5.11935 iter/s, 19.5337s/100 iter), loss = 0.145281
I0815 20:39:25.598422 20887 solver.cpp:334]     Train net output #0: loss = 0.145281 (* 1 = 0.145281 loss)
I0815 20:39:25.598425 20887 sgd_solver.cpp:136] Iteration 27400, lr = 1e-05, m = 0.9
I0815 20:39:32.151238 20895 data_reader.cpp:288] Starting prefetch of epoch 13
I0815 20:39:45.149108 20887 solver.cpp:312] Iteration 27500 (5.11504 iter/s, 19.5502s/100 iter), loss = 0.0707827
I0815 20:39:45.149161 20887 solver.cpp:334]     Train net output #0: loss = 0.0707826 (* 1 = 0.0707826 loss)
I0815 20:39:45.149168 20887 sgd_solver.cpp:136] Iteration 27500, lr = 1e-05, m = 0.9
I0815 20:40:04.493883 20890 data_reader.cpp:288] Starting prefetch of epoch 18
I0815 20:40:04.855621 20887 solver.cpp:312] Iteration 27600 (5.0746 iter/s, 19.706s/100 iter), loss = 0.0942213
I0815 20:40:04.855643 20887 solver.cpp:334]     Train net output #0: loss = 0.0942212 (* 1 = 0.0942212 loss)
I0815 20:40:04.855646 20887 sgd_solver.cpp:136] Iteration 27600, lr = 1e-05, m = 0.9
I0815 20:40:24.264500 20887 solver.cpp:312] Iteration 27700 (5.15242 iter/s, 19.4083s/100 iter), loss = 0.0923788
I0815 20:40:24.264546 20887 solver.cpp:334]     Train net output #0: loss = 0.0923788 (* 1 = 0.0923788 loss)
I0815 20:40:24.264552 20887 sgd_solver.cpp:136] Iteration 27700, lr = 1e-05, m = 0.9
I0815 20:40:43.623736 20887 solver.cpp:312] Iteration 27800 (5.16563 iter/s, 19.3587s/100 iter), loss = 0.0489933
I0815 20:40:43.623759 20887 solver.cpp:334]     Train net output #0: loss = 0.0489932 (* 1 = 0.0489932 loss)
I0815 20:40:43.623764 20887 sgd_solver.cpp:136] Iteration 27800, lr = 1e-05, m = 0.9
I0815 20:41:03.234752 20887 solver.cpp:312] Iteration 27900 (5.09931 iter/s, 19.6105s/100 iter), loss = 0.0691467
I0815 20:41:03.234805 20887 solver.cpp:334]     Train net output #0: loss = 0.0691467 (* 1 = 0.0691467 loss)
I0815 20:41:03.234812 20887 sgd_solver.cpp:136] Iteration 27900, lr = 1e-05, m = 0.9
I0815 20:41:08.680934 20841 data_reader.cpp:288] Starting prefetch of epoch 17
I0815 20:41:22.383684 20887 solver.cpp:509] Iteration 28000, Testing net (#0)
I0815 20:41:33.205883 20885 data_reader.cpp:288] Starting prefetch of epoch 5
I0815 20:41:34.087718 20887 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.949315
I0815 20:41:34.087811 20887 solver.cpp:594]     Test net output #1: accuracy/top5 = 1
I0815 20:41:34.087821 20887 solver.cpp:594]     Test net output #2: loss = 0.152676 (* 1 = 0.152676 loss)
I0815 20:41:34.087841 20887 solver.cpp:264] [MultiGPU] Tests completed in 11.7038s
I0815 20:41:34.276509 20887 solver.cpp:312] Iteration 28000 (3.22156 iter/s, 31.0409s/100 iter), loss = 0.102328
I0815 20:41:34.276533 20887 solver.cpp:334]     Train net output #0: loss = 0.102328 (* 1 = 0.102328 loss)
I0815 20:41:34.276540 20887 sgd_solver.cpp:136] Iteration 28000, lr = 1e-05, m = 0.9
I0815 20:41:52.671653 20830 data_reader.cpp:288] Starting prefetch of epoch 17
I0815 20:41:53.763195 20887 solver.cpp:312] Iteration 28100 (5.13185 iter/s, 19.4861s/100 iter), loss = 0.0809231
I0815 20:41:53.763219 20887 solver.cpp:334]     Train net output #0: loss = 0.0809231 (* 1 = 0.0809231 loss)
I0815 20:41:53.763226 20887 sgd_solver.cpp:136] Iteration 28100, lr = 1e-05, m = 0.9
I0815 20:42:13.230830 20887 solver.cpp:312] Iteration 28200 (5.13687 iter/s, 19.4671s/100 iter), loss = 0.0834406
I0815 20:42:13.230907 20887 solver.cpp:334]     Train net output #0: loss = 0.0834405 (* 1 = 0.0834405 loss)
I0815 20:42:13.230914 20887 sgd_solver.cpp:136] Iteration 28200, lr = 1e-05, m = 0.9
I0815 20:42:32.578362 20887 solver.cpp:312] Iteration 28300 (5.16876 iter/s, 19.347s/100 iter), loss = 0.0536742
I0815 20:42:32.578388 20887 solver.cpp:334]     Train net output #0: loss = 0.0536741 (* 1 = 0.0536741 loss)
I0815 20:42:32.578395 20887 sgd_solver.cpp:136] Iteration 28300, lr = 1e-05, m = 0.9
I0815 20:42:52.168970 20887 solver.cpp:312] Iteration 28400 (5.10463 iter/s, 19.5901s/100 iter), loss = 0.0969053
I0815 20:42:52.169010 20887 solver.cpp:334]     Train net output #0: loss = 0.0969053 (* 1 = 0.0969053 loss)
I0815 20:42:52.169015 20887 sgd_solver.cpp:136] Iteration 28400, lr = 1e-05, m = 0.9
I0815 20:42:56.881093 20841 data_reader.cpp:288] Starting prefetch of epoch 18
I0815 20:43:11.747241 20887 solver.cpp:312] Iteration 28500 (5.10784 iter/s, 19.5777s/100 iter), loss = 0.0900104
I0815 20:43:11.747267 20887 solver.cpp:334]     Train net output #0: loss = 0.0900104 (* 1 = 0.0900104 loss)
I0815 20:43:11.747272 20887 sgd_solver.cpp:136] Iteration 28500, lr = 1e-05, m = 0.9
I0815 20:43:31.243201 20887 solver.cpp:312] Iteration 28600 (5.12941 iter/s, 19.4954s/100 iter), loss = 0.121283
I0815 20:43:31.243254 20887 solver.cpp:334]     Train net output #0: loss = 0.121283 (* 1 = 0.121283 loss)
I0815 20:43:31.243259 20887 sgd_solver.cpp:136] Iteration 28600, lr = 1e-05, m = 0.9
I0815 20:43:50.789892 20887 solver.cpp:312] Iteration 28700 (5.1161 iter/s, 19.5462s/100 iter), loss = 0.0993443
I0815 20:43:50.789916 20887 solver.cpp:334]     Train net output #0: loss = 0.0993442 (* 1 = 0.0993442 loss)
I0815 20:43:50.789922 20887 sgd_solver.cpp:136] Iteration 28700, lr = 1e-05, m = 0.9
I0815 20:44:01.392946 20895 data_reader.cpp:288] Starting prefetch of epoch 14
I0815 20:44:10.024425 20887 solver.cpp:312] Iteration 28800 (5.19913 iter/s, 19.234s/100 iter), loss = 0.0735802
I0815 20:44:10.024444 20887 solver.cpp:334]     Train net output #0: loss = 0.0735802 (* 1 = 0.0735802 loss)
I0815 20:44:10.024448 20887 sgd_solver.cpp:136] Iteration 28800, lr = 1e-05, m = 0.9
I0815 20:44:29.585264 20887 solver.cpp:312] Iteration 28900 (5.1124 iter/s, 19.5603s/100 iter), loss = 0.159104
I0815 20:44:29.585294 20887 solver.cpp:334]     Train net output #0: loss = 0.159104 (* 1 = 0.159104 loss)
I0815 20:44:29.585300 20887 sgd_solver.cpp:136] Iteration 28900, lr = 1e-05, m = 0.9
I0815 20:44:33.394704 20890 data_reader.cpp:288] Starting prefetch of epoch 19
I0815 20:44:48.766034 20887 solver.cpp:312] Iteration 29000 (5.2137 iter/s, 19.1802s/100 iter), loss = 0.0952514
I0815 20:44:48.766058 20887 solver.cpp:334]     Train net output #0: loss = 0.0952514 (* 1 = 0.0952514 loss)
I0815 20:44:48.766062 20887 sgd_solver.cpp:136] Iteration 29000, lr = 1e-05, m = 0.9
I0815 20:45:08.174700 20887 solver.cpp:312] Iteration 29100 (5.15248 iter/s, 19.4081s/100 iter), loss = 0.0913881
I0815 20:45:08.174751 20887 solver.cpp:334]     Train net output #0: loss = 0.091388 (* 1 = 0.091388 loss)
I0815 20:45:08.174758 20887 sgd_solver.cpp:136] Iteration 29100, lr = 1e-05, m = 0.9
I0815 20:45:27.375522 20887 solver.cpp:312] Iteration 29200 (5.20825 iter/s, 19.2003s/100 iter), loss = 0.0621565
I0815 20:45:27.375547 20887 solver.cpp:334]     Train net output #0: loss = 0.0621565 (* 1 = 0.0621565 loss)
I0815 20:45:27.375553 20887 sgd_solver.cpp:136] Iteration 29200, lr = 1e-05, m = 0.9
I0815 20:45:37.461752 20895 data_reader.cpp:288] Starting prefetch of epoch 15
I0815 20:45:47.107094 20887 solver.cpp:312] Iteration 29300 (5.06816 iter/s, 19.731s/100 iter), loss = 0.0605243
I0815 20:45:47.107162 20887 solver.cpp:334]     Train net output #0: loss = 0.0605243 (* 1 = 0.0605243 loss)
I0815 20:45:47.107167 20887 sgd_solver.cpp:136] Iteration 29300, lr = 1e-05, m = 0.9
I0815 20:46:06.560631 20887 solver.cpp:312] Iteration 29400 (5.14059 iter/s, 19.453s/100 iter), loss = 0.0602441
I0815 20:46:06.560657 20887 solver.cpp:334]     Train net output #0: loss = 0.0602441 (* 1 = 0.0602441 loss)
I0815 20:46:06.560662 20887 sgd_solver.cpp:136] Iteration 29400, lr = 1e-05, m = 0.9
I0815 20:46:26.061928 20887 solver.cpp:312] Iteration 29500 (5.128 iter/s, 19.5008s/100 iter), loss = 0.140698
I0815 20:46:26.062006 20887 solver.cpp:334]     Train net output #0: loss = 0.140698 (* 1 = 0.140698 loss)
I0815 20:46:26.062012 20887 sgd_solver.cpp:136] Iteration 29500, lr = 1e-05, m = 0.9
I0815 20:46:41.747416 20895 data_reader.cpp:288] Starting prefetch of epoch 16
I0815 20:46:45.438400 20887 solver.cpp:312] Iteration 29600 (5.16104 iter/s, 19.3759s/100 iter), loss = 0.0846207
I0815 20:46:45.438429 20887 solver.cpp:334]     Train net output #0: loss = 0.0846207 (* 1 = 0.0846207 loss)
I0815 20:46:45.438436 20887 sgd_solver.cpp:136] Iteration 29600, lr = 1e-05, m = 0.9
I0815 20:47:04.765357 20887 solver.cpp:312] Iteration 29700 (5.17426 iter/s, 19.3264s/100 iter), loss = 0.0693928
I0815 20:47:04.765406 20887 solver.cpp:334]     Train net output #0: loss = 0.0693928 (* 1 = 0.0693928 loss)
I0815 20:47:04.765411 20887 sgd_solver.cpp:136] Iteration 29700, lr = 1e-05, m = 0.9
I0815 20:47:13.730834 20890 data_reader.cpp:288] Starting prefetch of epoch 20
I0815 20:47:24.174249 20887 solver.cpp:312] Iteration 29800 (5.15242 iter/s, 19.4084s/100 iter), loss = 0.149759
I0815 20:47:24.174275 20887 solver.cpp:334]     Train net output #0: loss = 0.149759 (* 1 = 0.149759 loss)
I0815 20:47:24.174281 20887 sgd_solver.cpp:136] Iteration 29800, lr = 1e-05, m = 0.9
I0815 20:47:43.756636 20887 solver.cpp:312] Iteration 29900 (5.10677 iter/s, 19.5819s/100 iter), loss = 0.078146
I0815 20:47:43.772194 20887 solver.cpp:334]     Train net output #0: loss = 0.078146 (* 1 = 0.078146 loss)
I0815 20:47:43.772222 20887 sgd_solver.cpp:136] Iteration 29900, lr = 1e-05, m = 0.9
I0815 20:48:03.170014 20887 solver.cpp:639] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/initial/cityscapes5_jsegnet21v2_iter_30000.caffemodel
I0815 20:48:03.220566 20887 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/initial/cityscapes5_jsegnet21v2_iter_30000.solverstate
I0815 20:48:03.231389 20887 solver.cpp:509] Iteration 30000, Testing net (#0)
I0815 20:48:10.719717 20935 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 20:48:15.033571 20887 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.950035
I0815 20:48:15.033612 20887 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999546
I0815 20:48:15.033618 20887 solver.cpp:594]     Test net output #2: loss = 0.184462 (* 1 = 0.184462 loss)
I0815 20:48:15.033644 20887 solver.cpp:264] [MultiGPU] Tests completed in 11.8019s
I0815 20:48:15.241544 20887 solver.cpp:312] Iteration 30000 (3.17621 iter/s, 31.484s/100 iter), loss = 0.139326
I0815 20:48:15.241574 20887 solver.cpp:334]     Train net output #0: loss = 0.139326 (* 1 = 0.139326 loss)
I0815 20:48:15.241580 20887 sgd_solver.cpp:136] Iteration 30000, lr = 1e-05, m = 0.9
I0815 20:48:34.578555 20887 solver.cpp:312] Iteration 30100 (5.17157 iter/s, 19.3365s/100 iter), loss = 0.0796032
I0815 20:48:34.578584 20887 solver.cpp:334]     Train net output #0: loss = 0.0796032 (* 1 = 0.0796032 loss)
I0815 20:48:34.578590 20887 sgd_solver.cpp:136] Iteration 30100, lr = 1e-05, m = 0.9
I0815 20:48:54.076216 20887 solver.cpp:312] Iteration 30200 (5.12896 iter/s, 19.4971s/100 iter), loss = 0.0695413
I0815 20:48:54.076288 20887 solver.cpp:334]     Train net output #0: loss = 0.0695414 (* 1 = 0.0695414 loss)
I0815 20:48:54.076295 20887 sgd_solver.cpp:136] Iteration 30200, lr = 1e-05, m = 0.9
I0815 20:49:02.210515 20895 data_reader.cpp:288] Starting prefetch of epoch 17
I0815 20:49:13.357017 20887 solver.cpp:312] Iteration 30300 (5.18665 iter/s, 19.2803s/100 iter), loss = 0.0937344
I0815 20:49:13.357043 20887 solver.cpp:334]     Train net output #0: loss = 0.0937345 (* 1 = 0.0937345 loss)
I0815 20:49:13.357050 20887 sgd_solver.cpp:136] Iteration 30300, lr = 1e-05, m = 0.9
I0815 20:49:32.971503 20887 solver.cpp:312] Iteration 30400 (5.09841 iter/s, 19.6139s/100 iter), loss = 0.0831745
I0815 20:49:32.971550 20887 solver.cpp:334]     Train net output #0: loss = 0.0831745 (* 1 = 0.0831745 loss)
I0815 20:49:32.971556 20887 sgd_solver.cpp:136] Iteration 30400, lr = 1e-05, m = 0.9
I0815 20:49:34.565201 20895 data_reader.cpp:288] Starting prefetch of epoch 18
I0815 20:49:52.302301 20887 solver.cpp:312] Iteration 30500 (5.17324 iter/s, 19.3303s/100 iter), loss = 0.0899855
I0815 20:49:52.302325 20887 solver.cpp:334]     Train net output #0: loss = 0.0899855 (* 1 = 0.0899855 loss)
I0815 20:49:52.302330 20887 sgd_solver.cpp:136] Iteration 30500, lr = 1e-05, m = 0.9
I0815 20:50:11.622195 20887 solver.cpp:312] Iteration 30600 (5.17616 iter/s, 19.3194s/100 iter), loss = 0.0647404
I0815 20:50:11.622274 20887 solver.cpp:334]     Train net output #0: loss = 0.0647405 (* 1 = 0.0647405 loss)
I0815 20:50:11.622282 20887 sgd_solver.cpp:136] Iteration 30600, lr = 1e-05, m = 0.9
I0815 20:50:31.135249 20887 solver.cpp:312] Iteration 30700 (5.12491 iter/s, 19.5125s/100 iter), loss = 0.177439
I0815 20:50:31.135272 20887 solver.cpp:334]     Train net output #0: loss = 0.177439 (* 1 = 0.177439 loss)
I0815 20:50:31.135275 20887 sgd_solver.cpp:136] Iteration 30700, lr = 1e-05, m = 0.9
I0815 20:50:38.589594 20895 data_reader.cpp:288] Starting prefetch of epoch 19
I0815 20:50:50.498916 20887 solver.cpp:312] Iteration 30800 (5.16445 iter/s, 19.3631s/100 iter), loss = 0.0889117
I0815 20:50:50.499006 20887 solver.cpp:334]     Train net output #0: loss = 0.0889118 (* 1 = 0.0889118 loss)
I0815 20:50:50.499014 20887 sgd_solver.cpp:136] Iteration 30800, lr = 1e-05, m = 0.9
I0815 20:51:09.835805 20887 solver.cpp:312] Iteration 30900 (5.17161 iter/s, 19.3364s/100 iter), loss = 0.0795591
I0815 20:51:09.835835 20887 solver.cpp:334]     Train net output #0: loss = 0.0795591 (* 1 = 0.0795591 loss)
I0815 20:51:09.835842 20887 sgd_solver.cpp:136] Iteration 30900, lr = 1e-05, m = 0.9
I0815 20:51:29.254753 20887 solver.cpp:312] Iteration 31000 (5.14975 iter/s, 19.4184s/100 iter), loss = 0.0708222
I0815 20:51:29.268203 20887 solver.cpp:334]     Train net output #0: loss = 0.0708222 (* 1 = 0.0708222 loss)
I0815 20:51:29.268235 20887 sgd_solver.cpp:136] Iteration 31000, lr = 1e-05, m = 0.9
I0815 20:51:42.679879 20841 data_reader.cpp:288] Starting prefetch of epoch 19
I0815 20:51:48.782821 20887 solver.cpp:312] Iteration 31100 (5.12098 iter/s, 19.5275s/100 iter), loss = 0.0839641
I0815 20:51:48.782850 20887 solver.cpp:334]     Train net output #0: loss = 0.0839642 (* 1 = 0.0839642 loss)
I0815 20:51:48.782855 20887 sgd_solver.cpp:136] Iteration 31100, lr = 1e-05, m = 0.9
I0815 20:52:08.278045 20887 solver.cpp:312] Iteration 31200 (5.1296 iter/s, 19.4947s/100 iter), loss = 0.160095
I0815 20:52:08.278097 20887 solver.cpp:334]     Train net output #0: loss = 0.160095 (* 1 = 0.160095 loss)
I0815 20:52:08.278105 20887 sgd_solver.cpp:136] Iteration 31200, lr = 1e-05, m = 0.9
I0815 20:52:14.917326 20890 data_reader.cpp:288] Starting prefetch of epoch 21
I0815 20:52:27.736165 20887 solver.cpp:312] Iteration 31300 (5.13938 iter/s, 19.4576s/100 iter), loss = 0.110656
I0815 20:52:27.736186 20887 solver.cpp:334]     Train net output #0: loss = 0.110656 (* 1 = 0.110656 loss)
I0815 20:52:27.736191 20887 sgd_solver.cpp:136] Iteration 31300, lr = 1e-05, m = 0.9
I0815 20:52:47.278334 20887 solver.cpp:312] Iteration 31400 (5.11728 iter/s, 19.5416s/100 iter), loss = 0.0663427
I0815 20:52:47.278383 20887 solver.cpp:334]     Train net output #0: loss = 0.0663427 (* 1 = 0.0663427 loss)
I0815 20:52:47.278388 20887 sgd_solver.cpp:136] Iteration 31400, lr = 1e-05, m = 0.9
I0815 20:53:06.781877 20887 solver.cpp:312] Iteration 31500 (5.12741 iter/s, 19.503s/100 iter), loss = 0.0735966
I0815 20:53:06.781900 20887 solver.cpp:334]     Train net output #0: loss = 0.0735966 (* 1 = 0.0735966 loss)
I0815 20:53:06.781905 20887 sgd_solver.cpp:136] Iteration 31500, lr = 1e-05, m = 0.9
I0815 20:53:19.493676 20895 data_reader.cpp:288] Starting prefetch of epoch 20
I0815 20:53:26.275195 20887 solver.cpp:312] Iteration 31600 (5.1301 iter/s, 19.4928s/100 iter), loss = 0.205428
I0815 20:53:26.275219 20887 solver.cpp:334]     Train net output #0: loss = 0.205428 (* 1 = 0.205428 loss)
I0815 20:53:26.275224 20887 sgd_solver.cpp:136] Iteration 31600, lr = 1e-05, m = 0.9
I0815 20:53:45.822414 20887 solver.cpp:312] Iteration 31700 (5.11596 iter/s, 19.5467s/100 iter), loss = 0.0931227
I0815 20:53:45.822439 20887 solver.cpp:334]     Train net output #0: loss = 0.0931228 (* 1 = 0.0931228 loss)
I0815 20:53:45.822444 20887 sgd_solver.cpp:136] Iteration 31700, lr = 1e-05, m = 0.9
I0815 20:54:05.198626 20887 solver.cpp:312] Iteration 31800 (5.16111 iter/s, 19.3757s/100 iter), loss = 0.0812021
I0815 20:54:05.198683 20887 solver.cpp:334]     Train net output #0: loss = 0.0812021 (* 1 = 0.0812021 loss)
I0815 20:54:05.198690 20887 sgd_solver.cpp:136] Iteration 31800, lr = 1e-05, m = 0.9
I0815 20:54:23.522258 20893 data_reader.cpp:288] Starting prefetch of epoch 21
I0815 20:54:23.522258 20841 data_reader.cpp:288] Starting prefetch of epoch 20
I0815 20:54:24.464838 20887 solver.cpp:312] Iteration 31900 (5.19058 iter/s, 19.2657s/100 iter), loss = 0.0506355
I0815 20:54:24.464862 20887 solver.cpp:334]     Train net output #0: loss = 0.0506355 (* 1 = 0.0506355 loss)
I0815 20:54:24.464869 20887 sgd_solver.cpp:136] Iteration 31900, lr = 1e-05, m = 0.9
I0815 20:54:43.869547 20887 solver.cpp:312] Iteration 31999 (5.10199 iter/s, 19.4042s/99 iter), loss = 0.0637584
I0815 20:54:43.869639 20887 solver.cpp:334]     Train net output #0: loss = 0.0637585 (* 1 = 0.0637585 loss)
I0815 20:54:43.926149 20887 solver.cpp:639] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/initial/cityscapes5_jsegnet21v2_iter_32000.caffemodel
I0815 20:54:43.971223 20887 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/initial/cityscapes5_jsegnet21v2_iter_32000.solverstate
I0815 20:54:44.040051 20887 solver.cpp:486] Iteration 32000, loss = 0.0706673
I0815 20:54:44.040077 20887 solver.cpp:509] Iteration 32000, Testing net (#0)
I0815 20:54:47.664839 20933 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 20:54:55.930138 20887 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.949219
I0815 20:54:55.930162 20887 solver.cpp:594]     Test net output #1: accuracy/top5 = 1
I0815 20:54:55.930169 20887 solver.cpp:594]     Test net output #2: loss = 0.154581 (* 1 = 0.154581 loss)
I0815 20:54:55.989426 20809 parallel.cpp:71] Root Solver performance on device 0: 4.833 * 6 = 29 img/sec (32000 itr in 6620 sec)
I0815 20:54:55.989445 20809 parallel.cpp:76]      Solver performance on device 1: 4.833 * 6 = 29 img/sec (32000 itr in 6620 sec)
I0815 20:54:55.989450 20809 parallel.cpp:76]      Solver performance on device 2: 4.833 * 6 = 29 img/sec (32000 itr in 6620 sec)
I0815 20:54:55.989451 20809 parallel.cpp:79] Overall multi-GPU performance: 87.0007 img/sec
I0815 20:54:57.321442 20809 caffe.cpp:247] Optimization Done in 1h 50m 40s
I0815 20:55:04.030964 20241 caffe.cpp:608] This is NVCaffe 0.16.3 started at Tue Aug 15 20:55:03 2017
I0815 20:55:04.032111 20241 caffe.cpp:611] CuDNN version: 6021
I0815 20:55:04.032115 20241 caffe.cpp:612] CuBLAS version: 8000
I0815 20:55:04.032117 20241 caffe.cpp:613] CUDA version: 8000
I0815 20:55:04.032119 20241 caffe.cpp:614] CUDA driver version: 8000
I0815 20:55:04.321586 20241 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0815 20:55:04.322156 20241 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I0815 20:55:04.322679 20241 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[1]: total=8508145664 free=8379236352
I0815 20:55:04.323196 20241 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[2]: total=8508145664 free=8379236352
I0815 20:55:04.323205 20241 caffe.cpp:208] Using GPUs 0, 1, 2
I0815 20:55:04.323529 20241 caffe.cpp:213] GPU 0: GeForce GTX 1080
I0815 20:55:04.323855 20241 caffe.cpp:213] GPU 1: GeForce GTX 1080
I0815 20:55:04.324185 20241 caffe.cpp:213] GPU 2: GeForce GTX 1080
I0815 20:55:04.331199 20241 solver.cpp:42] Solver data type: FLOAT
I0815 20:55:04.331264 20241 solver.cpp:45] Initializing solver from parameters: 
train_net: "training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/l1reg/train.prototxt"
test_net: "training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/l1reg/test.prototxt"
test_iter: 125
test_interval: 2000
base_lr: 1e-05
display: 100
max_iter: 32000
lr_policy: "multistep"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 1e-05
snapshot: 10000
snapshot_prefix: "training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/l1reg/cityscapes5_jsegnet21v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
regularization_type: "L1"
test_initialization: false
stepvalue: 24000
iter_size: 1
type: "Adam"
I0815 20:55:04.347368 20241 solver.cpp:77] Creating training net from train_net file: training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/l1reg/train.prototxt
I0815 20:55:04.358194 20241 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0815 20:55:04.358220 20241 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
W0815 20:55:04.358280 20241 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0815 20:55:04.359346 20241 net.cpp:72] Initializing net from parameters: 
name: "jsegnet21v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/train-image-lmdb"
    label_list_path: "data/train-label-lmdb"
    batch_size: 6
    shuffle: false
    threads: 1
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
I0815 20:55:04.359563 20241 net.cpp:104] Using FLOAT as default forward math type
I0815 20:55:04.359570 20241 net.cpp:110] Using FLOAT as default backward math type
I0815 20:55:04.359573 20241 layer_factory.hpp:136] Creating layer 'data' of type 'ImageLabelData'
I0815 20:55:04.359578 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:04.359592 20241 net.cpp:184] Created Layer data (0)
I0815 20:55:04.359597 20241 net.cpp:530] data -> data
I0815 20:55:04.359611 20241 net.cpp:530] data -> label
I0815 20:55:04.359923 20241 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0815 20:55:04.359943 20241 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 20:55:04.393926 20314 db_lmdb.cpp:24] Opened lmdb data/train-image-lmdb
I0815 20:55:04.397159 20241 data_layer.cpp:185] [0] ReshapePrefetch 6, 3, 640, 640
I0815 20:55:04.397229 20241 data_layer.cpp:209] [0] Output data size: 6, 3, 640, 640
I0815 20:55:04.397239 20241 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 20:55:04.397348 20241 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0815 20:55:04.397359 20241 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 20:55:04.398102 20315 data_layer.cpp:97] [0] Parser threads: 1
I0815 20:55:04.398111 20315 data_layer.cpp:99] [0] Transformer threads: 1
I0815 20:55:04.403543 20316 db_lmdb.cpp:24] Opened lmdb data/train-label-lmdb
I0815 20:55:04.404803 20241 data_layer.cpp:185] [0] ReshapePrefetch 6, 1, 640, 640
I0815 20:55:04.404897 20241 data_layer.cpp:209] [0] Output data size: 6, 1, 640, 640
I0815 20:55:04.404907 20241 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 20:55:04.404999 20241 net.cpp:245] Setting up data
I0815 20:55:04.405017 20241 net.cpp:252] TRAIN Top shape for layer 0 'data' 6 3 640 640 (7372800)
I0815 20:55:04.405026 20241 net.cpp:252] TRAIN Top shape for layer 0 'data' 6 1 640 640 (2457600)
I0815 20:55:04.405037 20241 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0815 20:55:04.405046 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:04.405081 20241 net.cpp:184] Created Layer data/bias (1)
I0815 20:55:04.405087 20241 net.cpp:561] data/bias <- data
I0815 20:55:04.405100 20241 net.cpp:530] data/bias -> data/bias
I0815 20:55:04.406790 20317 data_layer.cpp:97] [0] Parser threads: 1
I0815 20:55:04.406826 20317 data_layer.cpp:99] [0] Transformer threads: 1
I0815 20:55:04.411610 20241 net.cpp:245] Setting up data/bias
I0815 20:55:04.411761 20241 net.cpp:252] TRAIN Top shape for layer 1 'data/bias' 6 3 640 640 (7372800)
I0815 20:55:04.411801 20241 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0815 20:55:04.411835 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:04.411950 20241 net.cpp:184] Created Layer conv1a (2)
I0815 20:55:04.411967 20241 net.cpp:561] conv1a <- data/bias
I0815 20:55:04.411984 20241 net.cpp:530] conv1a -> conv1a
I0815 20:55:05.081058 20241 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 0  (limit 7.9G, req 0G)
I0815 20:55:05.081075 20241 net.cpp:245] Setting up conv1a
I0815 20:55:05.081081 20241 net.cpp:252] TRAIN Top shape for layer 2 'conv1a' 6 32 320 320 (19660800)
I0815 20:55:05.081090 20241 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0815 20:55:05.081094 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.081104 20241 net.cpp:184] Created Layer conv1a/bn (3)
I0815 20:55:05.081109 20241 net.cpp:561] conv1a/bn <- conv1a
I0815 20:55:05.081112 20241 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0815 20:55:05.081791 20241 net.cpp:245] Setting up conv1a/bn
I0815 20:55:05.081800 20241 net.cpp:252] TRAIN Top shape for layer 3 'conv1a/bn' 6 32 320 320 (19660800)
I0815 20:55:05.081809 20241 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0815 20:55:05.081811 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.081816 20241 net.cpp:184] Created Layer conv1a/relu (4)
I0815 20:55:05.081820 20241 net.cpp:561] conv1a/relu <- conv1a
I0815 20:55:05.081822 20241 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0815 20:55:05.081835 20241 net.cpp:245] Setting up conv1a/relu
I0815 20:55:05.081840 20241 net.cpp:252] TRAIN Top shape for layer 4 'conv1a/relu' 6 32 320 320 (19660800)
I0815 20:55:05.081841 20241 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0815 20:55:05.081843 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.081854 20241 net.cpp:184] Created Layer conv1b (5)
I0815 20:55:05.081857 20241 net.cpp:561] conv1b <- conv1a
I0815 20:55:05.081861 20241 net.cpp:530] conv1b -> conv1b
I0815 20:55:05.127974 20241 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.73G, req 0G)
I0815 20:55:05.127986 20241 net.cpp:245] Setting up conv1b
I0815 20:55:05.127990 20241 net.cpp:252] TRAIN Top shape for layer 5 'conv1b' 6 32 320 320 (19660800)
I0815 20:55:05.127996 20241 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0815 20:55:05.128000 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.128005 20241 net.cpp:184] Created Layer conv1b/bn (6)
I0815 20:55:05.128007 20241 net.cpp:561] conv1b/bn <- conv1b
I0815 20:55:05.128010 20241 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0815 20:55:05.128664 20241 net.cpp:245] Setting up conv1b/bn
I0815 20:55:05.128672 20241 net.cpp:252] TRAIN Top shape for layer 6 'conv1b/bn' 6 32 320 320 (19660800)
I0815 20:55:05.128679 20241 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0815 20:55:05.128681 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.128689 20241 net.cpp:184] Created Layer conv1b/relu (7)
I0815 20:55:05.128691 20241 net.cpp:561] conv1b/relu <- conv1b
I0815 20:55:05.128693 20241 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0815 20:55:05.128697 20241 net.cpp:245] Setting up conv1b/relu
I0815 20:55:05.128700 20241 net.cpp:252] TRAIN Top shape for layer 7 'conv1b/relu' 6 32 320 320 (19660800)
I0815 20:55:05.128702 20241 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0815 20:55:05.128706 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.128711 20241 net.cpp:184] Created Layer pool1 (8)
I0815 20:55:05.128715 20241 net.cpp:561] pool1 <- conv1b
I0815 20:55:05.128716 20241 net.cpp:530] pool1 -> pool1
I0815 20:55:05.128785 20241 net.cpp:245] Setting up pool1
I0815 20:55:05.128790 20241 net.cpp:252] TRAIN Top shape for layer 8 'pool1' 6 32 160 160 (4915200)
I0815 20:55:05.128793 20241 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0815 20:55:05.128804 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.128811 20241 net.cpp:184] Created Layer res2a_branch2a (9)
I0815 20:55:05.128814 20241 net.cpp:561] res2a_branch2a <- pool1
I0815 20:55:05.128818 20241 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0815 20:55:05.168486 20241 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.61G, req 0G)
I0815 20:55:05.168499 20241 net.cpp:245] Setting up res2a_branch2a
I0815 20:55:05.168504 20241 net.cpp:252] TRAIN Top shape for layer 9 'res2a_branch2a' 6 64 160 160 (9830400)
I0815 20:55:05.168510 20241 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0815 20:55:05.168514 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.168519 20241 net.cpp:184] Created Layer res2a_branch2a/bn (10)
I0815 20:55:05.168522 20241 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0815 20:55:05.168525 20241 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0815 20:55:05.169811 20241 net.cpp:245] Setting up res2a_branch2a/bn
I0815 20:55:05.169819 20241 net.cpp:252] TRAIN Top shape for layer 10 'res2a_branch2a/bn' 6 64 160 160 (9830400)
I0815 20:55:05.169826 20241 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0815 20:55:05.169829 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.169833 20241 net.cpp:184] Created Layer res2a_branch2a/relu (11)
I0815 20:55:05.169836 20241 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0815 20:55:05.169838 20241 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0815 20:55:05.169842 20241 net.cpp:245] Setting up res2a_branch2a/relu
I0815 20:55:05.169845 20241 net.cpp:252] TRAIN Top shape for layer 11 'res2a_branch2a/relu' 6 64 160 160 (9830400)
I0815 20:55:05.169848 20241 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0815 20:55:05.169850 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.169857 20241 net.cpp:184] Created Layer res2a_branch2b (12)
I0815 20:55:05.169859 20241 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0815 20:55:05.169862 20241 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0815 20:55:05.190790 20241 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.52G, req 0G)
I0815 20:55:05.190814 20241 net.cpp:245] Setting up res2a_branch2b
I0815 20:55:05.190819 20241 net.cpp:252] TRAIN Top shape for layer 12 'res2a_branch2b' 6 64 160 160 (9830400)
I0815 20:55:05.190827 20241 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0815 20:55:05.190832 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.190842 20241 net.cpp:184] Created Layer res2a_branch2b/bn (13)
I0815 20:55:05.190846 20241 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0815 20:55:05.190850 20241 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0815 20:55:05.191596 20241 net.cpp:245] Setting up res2a_branch2b/bn
I0815 20:55:05.191604 20241 net.cpp:252] TRAIN Top shape for layer 13 'res2a_branch2b/bn' 6 64 160 160 (9830400)
I0815 20:55:05.191612 20241 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0815 20:55:05.191615 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.191619 20241 net.cpp:184] Created Layer res2a_branch2b/relu (14)
I0815 20:55:05.191622 20241 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0815 20:55:05.191624 20241 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0815 20:55:05.191629 20241 net.cpp:245] Setting up res2a_branch2b/relu
I0815 20:55:05.191632 20241 net.cpp:252] TRAIN Top shape for layer 14 'res2a_branch2b/relu' 6 64 160 160 (9830400)
I0815 20:55:05.191646 20241 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0815 20:55:05.191650 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.191656 20241 net.cpp:184] Created Layer pool2 (15)
I0815 20:55:05.191658 20241 net.cpp:561] pool2 <- res2a_branch2b
I0815 20:55:05.191661 20241 net.cpp:530] pool2 -> pool2
I0815 20:55:05.191725 20241 net.cpp:245] Setting up pool2
I0815 20:55:05.191730 20241 net.cpp:252] TRAIN Top shape for layer 15 'pool2' 6 64 80 80 (2457600)
I0815 20:55:05.191732 20241 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0815 20:55:05.191735 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.191743 20241 net.cpp:184] Created Layer res3a_branch2a (16)
I0815 20:55:05.191746 20241 net.cpp:561] res3a_branch2a <- pool2
I0815 20:55:05.191748 20241 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0815 20:55:05.212394 20241 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 7.46G, req 0G)
I0815 20:55:05.212412 20241 net.cpp:245] Setting up res3a_branch2a
I0815 20:55:05.212419 20241 net.cpp:252] TRAIN Top shape for layer 16 'res3a_branch2a' 6 128 80 80 (4915200)
I0815 20:55:05.212426 20241 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0815 20:55:05.212431 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.212440 20241 net.cpp:184] Created Layer res3a_branch2a/bn (17)
I0815 20:55:05.212443 20241 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0815 20:55:05.212446 20241 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0815 20:55:05.213122 20241 net.cpp:245] Setting up res3a_branch2a/bn
I0815 20:55:05.213130 20241 net.cpp:252] TRAIN Top shape for layer 17 'res3a_branch2a/bn' 6 128 80 80 (4915200)
I0815 20:55:05.213140 20241 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0815 20:55:05.213143 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.213147 20241 net.cpp:184] Created Layer res3a_branch2a/relu (18)
I0815 20:55:05.213150 20241 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0815 20:55:05.213152 20241 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0815 20:55:05.213156 20241 net.cpp:245] Setting up res3a_branch2a/relu
I0815 20:55:05.213160 20241 net.cpp:252] TRAIN Top shape for layer 18 'res3a_branch2a/relu' 6 128 80 80 (4915200)
I0815 20:55:05.213161 20241 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0815 20:55:05.213165 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.213171 20241 net.cpp:184] Created Layer res3a_branch2b (19)
I0815 20:55:05.213174 20241 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0815 20:55:05.213177 20241 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0815 20:55:05.227406 20241 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.42G, req 0G)
I0815 20:55:05.227429 20241 net.cpp:245] Setting up res3a_branch2b
I0815 20:55:05.227435 20241 net.cpp:252] TRAIN Top shape for layer 19 'res3a_branch2b' 6 128 80 80 (4915200)
I0815 20:55:05.227442 20241 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0815 20:55:05.227447 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.227458 20241 net.cpp:184] Created Layer res3a_branch2b/bn (20)
I0815 20:55:05.227460 20241 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0815 20:55:05.227464 20241 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0815 20:55:05.228173 20241 net.cpp:245] Setting up res3a_branch2b/bn
I0815 20:55:05.228181 20241 net.cpp:252] TRAIN Top shape for layer 20 'res3a_branch2b/bn' 6 128 80 80 (4915200)
I0815 20:55:05.228189 20241 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0815 20:55:05.228202 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.228206 20241 net.cpp:184] Created Layer res3a_branch2b/relu (21)
I0815 20:55:05.228209 20241 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0815 20:55:05.228212 20241 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0815 20:55:05.228217 20241 net.cpp:245] Setting up res3a_branch2b/relu
I0815 20:55:05.228220 20241 net.cpp:252] TRAIN Top shape for layer 21 'res3a_branch2b/relu' 6 128 80 80 (4915200)
I0815 20:55:05.228222 20241 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0815 20:55:05.228225 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.228231 20241 net.cpp:184] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (22)
I0815 20:55:05.228235 20241 net.cpp:561] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0815 20:55:05.228236 20241 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0815 20:55:05.228240 20241 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0815 20:55:05.228281 20241 net.cpp:245] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0815 20:55:05.228284 20241 net.cpp:252] TRAIN Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 6 128 80 80 (4915200)
I0815 20:55:05.228287 20241 net.cpp:252] TRAIN Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 6 128 80 80 (4915200)
I0815 20:55:05.228291 20241 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0815 20:55:05.228292 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.228297 20241 net.cpp:184] Created Layer pool3 (23)
I0815 20:55:05.228301 20241 net.cpp:561] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0815 20:55:05.228302 20241 net.cpp:530] pool3 -> pool3
I0815 20:55:05.228364 20241 net.cpp:245] Setting up pool3
I0815 20:55:05.228368 20241 net.cpp:252] TRAIN Top shape for layer 23 'pool3' 6 128 40 40 (1228800)
I0815 20:55:05.228371 20241 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0815 20:55:05.228374 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.228380 20241 net.cpp:184] Created Layer res4a_branch2a (24)
I0815 20:55:05.228384 20241 net.cpp:561] res4a_branch2a <- pool3
I0815 20:55:05.228386 20241 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0815 20:55:05.253123 20241 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.38G, req 0G)
I0815 20:55:05.253139 20241 net.cpp:245] Setting up res4a_branch2a
I0815 20:55:05.253144 20241 net.cpp:252] TRAIN Top shape for layer 24 'res4a_branch2a' 6 256 40 40 (2457600)
I0815 20:55:05.253150 20241 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0815 20:55:05.253154 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.253163 20241 net.cpp:184] Created Layer res4a_branch2a/bn (25)
I0815 20:55:05.253167 20241 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0815 20:55:05.253170 20241 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0815 20:55:05.253814 20241 net.cpp:245] Setting up res4a_branch2a/bn
I0815 20:55:05.253823 20241 net.cpp:252] TRAIN Top shape for layer 25 'res4a_branch2a/bn' 6 256 40 40 (2457600)
I0815 20:55:05.253828 20241 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0815 20:55:05.253831 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.253834 20241 net.cpp:184] Created Layer res4a_branch2a/relu (26)
I0815 20:55:05.253837 20241 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0815 20:55:05.253840 20241 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0815 20:55:05.253854 20241 net.cpp:245] Setting up res4a_branch2a/relu
I0815 20:55:05.253857 20241 net.cpp:252] TRAIN Top shape for layer 26 'res4a_branch2a/relu' 6 256 40 40 (2457600)
I0815 20:55:05.253859 20241 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0815 20:55:05.253862 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.253870 20241 net.cpp:184] Created Layer res4a_branch2b (27)
I0815 20:55:05.253873 20241 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0815 20:55:05.253876 20241 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0815 20:55:05.262922 20241 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.36G, req 0G)
I0815 20:55:05.262936 20241 net.cpp:245] Setting up res4a_branch2b
I0815 20:55:05.262943 20241 net.cpp:252] TRAIN Top shape for layer 27 'res4a_branch2b' 6 256 40 40 (2457600)
I0815 20:55:05.262948 20241 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0815 20:55:05.262953 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.262959 20241 net.cpp:184] Created Layer res4a_branch2b/bn (28)
I0815 20:55:05.262962 20241 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0815 20:55:05.262965 20241 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0815 20:55:05.263644 20241 net.cpp:245] Setting up res4a_branch2b/bn
I0815 20:55:05.263653 20241 net.cpp:252] TRAIN Top shape for layer 28 'res4a_branch2b/bn' 6 256 40 40 (2457600)
I0815 20:55:05.263659 20241 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0815 20:55:05.263664 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.263669 20241 net.cpp:184] Created Layer res4a_branch2b/relu (29)
I0815 20:55:05.263672 20241 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0815 20:55:05.263676 20241 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0815 20:55:05.263684 20241 net.cpp:245] Setting up res4a_branch2b/relu
I0815 20:55:05.263689 20241 net.cpp:252] TRAIN Top shape for layer 29 'res4a_branch2b/relu' 6 256 40 40 (2457600)
I0815 20:55:05.263692 20241 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0815 20:55:05.263696 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.263703 20241 net.cpp:184] Created Layer pool4 (30)
I0815 20:55:05.263706 20241 net.cpp:561] pool4 <- res4a_branch2b
I0815 20:55:05.263710 20241 net.cpp:530] pool4 -> pool4
I0815 20:55:05.263780 20241 net.cpp:245] Setting up pool4
I0815 20:55:05.263787 20241 net.cpp:252] TRAIN Top shape for layer 30 'pool4' 6 256 40 40 (2457600)
I0815 20:55:05.263792 20241 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0815 20:55:05.263795 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.263808 20241 net.cpp:184] Created Layer res5a_branch2a (31)
I0815 20:55:05.263811 20241 net.cpp:561] res5a_branch2a <- pool4
I0815 20:55:05.263815 20241 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0815 20:55:05.291147 20241 net.cpp:245] Setting up res5a_branch2a
I0815 20:55:05.291167 20241 net.cpp:252] TRAIN Top shape for layer 31 'res5a_branch2a' 6 512 40 40 (4915200)
I0815 20:55:05.291175 20241 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0815 20:55:05.291182 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.291194 20241 net.cpp:184] Created Layer res5a_branch2a/bn (32)
I0815 20:55:05.291198 20241 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0815 20:55:05.291203 20241 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0815 20:55:05.291891 20241 net.cpp:245] Setting up res5a_branch2a/bn
I0815 20:55:05.291904 20241 net.cpp:252] TRAIN Top shape for layer 32 'res5a_branch2a/bn' 6 512 40 40 (4915200)
I0815 20:55:05.291924 20241 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0815 20:55:05.291929 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.291934 20241 net.cpp:184] Created Layer res5a_branch2a/relu (33)
I0815 20:55:05.291937 20241 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0815 20:55:05.291942 20241 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0815 20:55:05.291949 20241 net.cpp:245] Setting up res5a_branch2a/relu
I0815 20:55:05.291954 20241 net.cpp:252] TRAIN Top shape for layer 33 'res5a_branch2a/relu' 6 512 40 40 (4915200)
I0815 20:55:05.291957 20241 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0815 20:55:05.291961 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.291970 20241 net.cpp:184] Created Layer res5a_branch2b (34)
I0815 20:55:05.291975 20241 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0815 20:55:05.291978 20241 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0815 20:55:05.305256 20241 net.cpp:245] Setting up res5a_branch2b
I0815 20:55:05.305279 20241 net.cpp:252] TRAIN Top shape for layer 34 'res5a_branch2b' 6 512 40 40 (4915200)
I0815 20:55:05.305289 20241 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0815 20:55:05.305294 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.305300 20241 net.cpp:184] Created Layer res5a_branch2b/bn (35)
I0815 20:55:05.305304 20241 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0815 20:55:05.305306 20241 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0815 20:55:05.305930 20241 net.cpp:245] Setting up res5a_branch2b/bn
I0815 20:55:05.305938 20241 net.cpp:252] TRAIN Top shape for layer 35 'res5a_branch2b/bn' 6 512 40 40 (4915200)
I0815 20:55:05.305943 20241 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0815 20:55:05.305946 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.305949 20241 net.cpp:184] Created Layer res5a_branch2b/relu (36)
I0815 20:55:05.305951 20241 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0815 20:55:05.305954 20241 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0815 20:55:05.305958 20241 net.cpp:245] Setting up res5a_branch2b/relu
I0815 20:55:05.305960 20241 net.cpp:252] TRAIN Top shape for layer 36 'res5a_branch2b/relu' 6 512 40 40 (4915200)
I0815 20:55:05.305963 20241 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I0815 20:55:05.305964 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.305972 20241 net.cpp:184] Created Layer out5a (37)
I0815 20:55:05.305975 20241 net.cpp:561] out5a <- res5a_branch2b
I0815 20:55:05.305977 20241 net.cpp:530] out5a -> out5a
I0815 20:55:05.310181 20241 net.cpp:245] Setting up out5a
I0815 20:55:05.310191 20241 net.cpp:252] TRAIN Top shape for layer 37 'out5a' 6 64 40 40 (614400)
I0815 20:55:05.310196 20241 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I0815 20:55:05.310199 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.310204 20241 net.cpp:184] Created Layer out5a/bn (38)
I0815 20:55:05.310205 20241 net.cpp:561] out5a/bn <- out5a
I0815 20:55:05.310209 20241 net.cpp:513] out5a/bn -> out5a (in-place)
I0815 20:55:05.310834 20241 net.cpp:245] Setting up out5a/bn
I0815 20:55:05.310842 20241 net.cpp:252] TRAIN Top shape for layer 38 'out5a/bn' 6 64 40 40 (614400)
I0815 20:55:05.310847 20241 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I0815 20:55:05.310849 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.310853 20241 net.cpp:184] Created Layer out5a/relu (39)
I0815 20:55:05.310855 20241 net.cpp:561] out5a/relu <- out5a
I0815 20:55:05.310858 20241 net.cpp:513] out5a/relu -> out5a (in-place)
I0815 20:55:05.310870 20241 net.cpp:245] Setting up out5a/relu
I0815 20:55:05.310873 20241 net.cpp:252] TRAIN Top shape for layer 39 'out5a/relu' 6 64 40 40 (614400)
I0815 20:55:05.310875 20241 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I0815 20:55:05.310878 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.310889 20241 net.cpp:184] Created Layer out5a_up2 (40)
I0815 20:55:05.310892 20241 net.cpp:561] out5a_up2 <- out5a
I0815 20:55:05.310894 20241 net.cpp:530] out5a_up2 -> out5a_up2
I0815 20:55:05.311173 20241 net.cpp:245] Setting up out5a_up2
I0815 20:55:05.311178 20241 net.cpp:252] TRAIN Top shape for layer 40 'out5a_up2' 6 64 80 80 (2457600)
I0815 20:55:05.311182 20241 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I0815 20:55:05.311183 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.311192 20241 net.cpp:184] Created Layer out3a (41)
I0815 20:55:05.311195 20241 net.cpp:561] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0815 20:55:05.311197 20241 net.cpp:530] out3a -> out3a
I0815 20:55:05.322540 20241 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 7.3G, req 0G)
I0815 20:55:05.322556 20241 net.cpp:245] Setting up out3a
I0815 20:55:05.322561 20241 net.cpp:252] TRAIN Top shape for layer 41 'out3a' 6 64 80 80 (2457600)
I0815 20:55:05.322567 20241 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I0815 20:55:05.322571 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.322577 20241 net.cpp:184] Created Layer out3a/bn (42)
I0815 20:55:05.322580 20241 net.cpp:561] out3a/bn <- out3a
I0815 20:55:05.322585 20241 net.cpp:513] out3a/bn -> out3a (in-place)
I0815 20:55:05.323366 20241 net.cpp:245] Setting up out3a/bn
I0815 20:55:05.323374 20241 net.cpp:252] TRAIN Top shape for layer 42 'out3a/bn' 6 64 80 80 (2457600)
I0815 20:55:05.323381 20241 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I0815 20:55:05.323384 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.323387 20241 net.cpp:184] Created Layer out3a/relu (43)
I0815 20:55:05.323390 20241 net.cpp:561] out3a/relu <- out3a
I0815 20:55:05.323391 20241 net.cpp:513] out3a/relu -> out3a (in-place)
I0815 20:55:05.323395 20241 net.cpp:245] Setting up out3a/relu
I0815 20:55:05.323397 20241 net.cpp:252] TRAIN Top shape for layer 43 'out3a/relu' 6 64 80 80 (2457600)
I0815 20:55:05.323400 20241 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I0815 20:55:05.323402 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.323858 20241 net.cpp:184] Created Layer out3_out5_combined (44)
I0815 20:55:05.323863 20241 net.cpp:561] out3_out5_combined <- out5a_up2
I0815 20:55:05.323865 20241 net.cpp:561] out3_out5_combined <- out3a
I0815 20:55:05.323868 20241 net.cpp:530] out3_out5_combined -> out3_out5_combined
I0815 20:55:05.324877 20241 net.cpp:245] Setting up out3_out5_combined
I0815 20:55:05.324885 20241 net.cpp:252] TRAIN Top shape for layer 44 'out3_out5_combined' 6 64 80 80 (2457600)
I0815 20:55:05.324888 20241 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I0815 20:55:05.324892 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.324899 20241 net.cpp:184] Created Layer ctx_conv1 (45)
I0815 20:55:05.324903 20241 net.cpp:561] ctx_conv1 <- out3_out5_combined
I0815 20:55:05.324904 20241 net.cpp:530] ctx_conv1 -> ctx_conv1
I0815 20:55:05.338371 20241 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 7.25G, req 0G)
I0815 20:55:05.338384 20241 net.cpp:245] Setting up ctx_conv1
I0815 20:55:05.338388 20241 net.cpp:252] TRAIN Top shape for layer 45 'ctx_conv1' 6 64 80 80 (2457600)
I0815 20:55:05.338403 20241 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I0815 20:55:05.338407 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.338413 20241 net.cpp:184] Created Layer ctx_conv1/bn (46)
I0815 20:55:05.338415 20241 net.cpp:561] ctx_conv1/bn <- ctx_conv1
I0815 20:55:05.338418 20241 net.cpp:513] ctx_conv1/bn -> ctx_conv1 (in-place)
I0815 20:55:05.339083 20241 net.cpp:245] Setting up ctx_conv1/bn
I0815 20:55:05.339092 20241 net.cpp:252] TRAIN Top shape for layer 46 'ctx_conv1/bn' 6 64 80 80 (2457600)
I0815 20:55:05.339097 20241 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I0815 20:55:05.339099 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.339102 20241 net.cpp:184] Created Layer ctx_conv1/relu (47)
I0815 20:55:05.339104 20241 net.cpp:561] ctx_conv1/relu <- ctx_conv1
I0815 20:55:05.339107 20241 net.cpp:513] ctx_conv1/relu -> ctx_conv1 (in-place)
I0815 20:55:05.339110 20241 net.cpp:245] Setting up ctx_conv1/relu
I0815 20:55:05.339113 20241 net.cpp:252] TRAIN Top shape for layer 47 'ctx_conv1/relu' 6 64 80 80 (2457600)
I0815 20:55:05.339115 20241 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I0815 20:55:05.339118 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.339123 20241 net.cpp:184] Created Layer ctx_conv2 (48)
I0815 20:55:05.339124 20241 net.cpp:561] ctx_conv2 <- ctx_conv1
I0815 20:55:05.339128 20241 net.cpp:530] ctx_conv2 -> ctx_conv2
I0815 20:55:05.340204 20241 net.cpp:245] Setting up ctx_conv2
I0815 20:55:05.340212 20241 net.cpp:252] TRAIN Top shape for layer 48 'ctx_conv2' 6 64 80 80 (2457600)
I0815 20:55:05.340216 20241 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I0815 20:55:05.340219 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.340222 20241 net.cpp:184] Created Layer ctx_conv2/bn (49)
I0815 20:55:05.340224 20241 net.cpp:561] ctx_conv2/bn <- ctx_conv2
I0815 20:55:05.340226 20241 net.cpp:513] ctx_conv2/bn -> ctx_conv2 (in-place)
I0815 20:55:05.340843 20241 net.cpp:245] Setting up ctx_conv2/bn
I0815 20:55:05.340849 20241 net.cpp:252] TRAIN Top shape for layer 49 'ctx_conv2/bn' 6 64 80 80 (2457600)
I0815 20:55:05.340854 20241 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I0815 20:55:05.340857 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.340860 20241 net.cpp:184] Created Layer ctx_conv2/relu (50)
I0815 20:55:05.340862 20241 net.cpp:561] ctx_conv2/relu <- ctx_conv2
I0815 20:55:05.340864 20241 net.cpp:513] ctx_conv2/relu -> ctx_conv2 (in-place)
I0815 20:55:05.340867 20241 net.cpp:245] Setting up ctx_conv2/relu
I0815 20:55:05.340869 20241 net.cpp:252] TRAIN Top shape for layer 50 'ctx_conv2/relu' 6 64 80 80 (2457600)
I0815 20:55:05.340872 20241 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I0815 20:55:05.340873 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.340878 20241 net.cpp:184] Created Layer ctx_conv3 (51)
I0815 20:55:05.340880 20241 net.cpp:561] ctx_conv3 <- ctx_conv2
I0815 20:55:05.340883 20241 net.cpp:530] ctx_conv3 -> ctx_conv3
I0815 20:55:05.341936 20241 net.cpp:245] Setting up ctx_conv3
I0815 20:55:05.341943 20241 net.cpp:252] TRAIN Top shape for layer 51 'ctx_conv3' 6 64 80 80 (2457600)
I0815 20:55:05.341948 20241 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I0815 20:55:05.341949 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.341953 20241 net.cpp:184] Created Layer ctx_conv3/bn (52)
I0815 20:55:05.341955 20241 net.cpp:561] ctx_conv3/bn <- ctx_conv3
I0815 20:55:05.341958 20241 net.cpp:513] ctx_conv3/bn -> ctx_conv3 (in-place)
I0815 20:55:05.342569 20241 net.cpp:245] Setting up ctx_conv3/bn
I0815 20:55:05.342582 20241 net.cpp:252] TRAIN Top shape for layer 52 'ctx_conv3/bn' 6 64 80 80 (2457600)
I0815 20:55:05.342587 20241 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I0815 20:55:05.342589 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.342592 20241 net.cpp:184] Created Layer ctx_conv3/relu (53)
I0815 20:55:05.342594 20241 net.cpp:561] ctx_conv3/relu <- ctx_conv3
I0815 20:55:05.342597 20241 net.cpp:513] ctx_conv3/relu -> ctx_conv3 (in-place)
I0815 20:55:05.342599 20241 net.cpp:245] Setting up ctx_conv3/relu
I0815 20:55:05.342602 20241 net.cpp:252] TRAIN Top shape for layer 53 'ctx_conv3/relu' 6 64 80 80 (2457600)
I0815 20:55:05.342604 20241 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I0815 20:55:05.342607 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.342612 20241 net.cpp:184] Created Layer ctx_conv4 (54)
I0815 20:55:05.342613 20241 net.cpp:561] ctx_conv4 <- ctx_conv3
I0815 20:55:05.342615 20241 net.cpp:530] ctx_conv4 -> ctx_conv4
I0815 20:55:05.343672 20241 net.cpp:245] Setting up ctx_conv4
I0815 20:55:05.343677 20241 net.cpp:252] TRAIN Top shape for layer 54 'ctx_conv4' 6 64 80 80 (2457600)
I0815 20:55:05.343682 20241 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I0815 20:55:05.343683 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.343689 20241 net.cpp:184] Created Layer ctx_conv4/bn (55)
I0815 20:55:05.343691 20241 net.cpp:561] ctx_conv4/bn <- ctx_conv4
I0815 20:55:05.343693 20241 net.cpp:513] ctx_conv4/bn -> ctx_conv4 (in-place)
I0815 20:55:05.344313 20241 net.cpp:245] Setting up ctx_conv4/bn
I0815 20:55:05.344321 20241 net.cpp:252] TRAIN Top shape for layer 55 'ctx_conv4/bn' 6 64 80 80 (2457600)
I0815 20:55:05.344326 20241 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I0815 20:55:05.344327 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.344331 20241 net.cpp:184] Created Layer ctx_conv4/relu (56)
I0815 20:55:05.344332 20241 net.cpp:561] ctx_conv4/relu <- ctx_conv4
I0815 20:55:05.344334 20241 net.cpp:513] ctx_conv4/relu -> ctx_conv4 (in-place)
I0815 20:55:05.344338 20241 net.cpp:245] Setting up ctx_conv4/relu
I0815 20:55:05.344341 20241 net.cpp:252] TRAIN Top shape for layer 56 'ctx_conv4/relu' 6 64 80 80 (2457600)
I0815 20:55:05.344342 20241 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I0815 20:55:05.344344 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.344349 20241 net.cpp:184] Created Layer ctx_final (57)
I0815 20:55:05.344352 20241 net.cpp:561] ctx_final <- ctx_conv4
I0815 20:55:05.344353 20241 net.cpp:530] ctx_final -> ctx_final
I0815 20:55:05.356672 20241 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 7.22G, req 0G)
I0815 20:55:05.356683 20241 net.cpp:245] Setting up ctx_final
I0815 20:55:05.356688 20241 net.cpp:252] TRAIN Top shape for layer 57 'ctx_final' 6 8 80 80 (307200)
I0815 20:55:05.356693 20241 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I0815 20:55:05.356696 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.356700 20241 net.cpp:184] Created Layer ctx_final/relu (58)
I0815 20:55:05.356703 20241 net.cpp:561] ctx_final/relu <- ctx_final
I0815 20:55:05.356705 20241 net.cpp:513] ctx_final/relu -> ctx_final (in-place)
I0815 20:55:05.356709 20241 net.cpp:245] Setting up ctx_final/relu
I0815 20:55:05.356714 20241 net.cpp:252] TRAIN Top shape for layer 58 'ctx_final/relu' 6 8 80 80 (307200)
I0815 20:55:05.356715 20241 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I0815 20:55:05.356717 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.356730 20241 net.cpp:184] Created Layer out_deconv_final_up2 (59)
I0815 20:55:05.356734 20241 net.cpp:561] out_deconv_final_up2 <- ctx_final
I0815 20:55:05.356736 20241 net.cpp:530] out_deconv_final_up2 -> out_deconv_final_up2
I0815 20:55:05.357017 20241 net.cpp:245] Setting up out_deconv_final_up2
I0815 20:55:05.357023 20241 net.cpp:252] TRAIN Top shape for layer 59 'out_deconv_final_up2' 6 8 160 160 (1228800)
I0815 20:55:05.357025 20241 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I0815 20:55:05.357028 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.357033 20241 net.cpp:184] Created Layer out_deconv_final_up4 (60)
I0815 20:55:05.357035 20241 net.cpp:561] out_deconv_final_up4 <- out_deconv_final_up2
I0815 20:55:05.357038 20241 net.cpp:530] out_deconv_final_up4 -> out_deconv_final_up4
I0815 20:55:05.357290 20241 net.cpp:245] Setting up out_deconv_final_up4
I0815 20:55:05.357295 20241 net.cpp:252] TRAIN Top shape for layer 60 'out_deconv_final_up4' 6 8 320 320 (4915200)
I0815 20:55:05.357298 20241 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I0815 20:55:05.357300 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.357306 20241 net.cpp:184] Created Layer out_deconv_final_up8 (61)
I0815 20:55:05.357308 20241 net.cpp:561] out_deconv_final_up8 <- out_deconv_final_up4
I0815 20:55:05.357311 20241 net.cpp:530] out_deconv_final_up8 -> out_deconv_final_up8
I0815 20:55:05.357563 20241 net.cpp:245] Setting up out_deconv_final_up8
I0815 20:55:05.357568 20241 net.cpp:252] TRAIN Top shape for layer 61 'out_deconv_final_up8' 6 8 640 640 (19660800)
I0815 20:55:05.357570 20241 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0815 20:55:05.357573 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.357583 20241 net.cpp:184] Created Layer loss (62)
I0815 20:55:05.357586 20241 net.cpp:561] loss <- out_deconv_final_up8
I0815 20:55:05.357589 20241 net.cpp:561] loss <- label
I0815 20:55:05.357592 20241 net.cpp:530] loss -> loss
I0815 20:55:05.358876 20241 net.cpp:245] Setting up loss
I0815 20:55:05.358886 20241 net.cpp:252] TRAIN Top shape for layer 62 'loss' (1)
I0815 20:55:05.358888 20241 net.cpp:256]     with loss weight 1
I0815 20:55:05.358892 20241 net.cpp:323] loss needs backward computation.
I0815 20:55:05.358896 20241 net.cpp:323] out_deconv_final_up8 needs backward computation.
I0815 20:55:05.358897 20241 net.cpp:323] out_deconv_final_up4 needs backward computation.
I0815 20:55:05.358901 20241 net.cpp:323] out_deconv_final_up2 needs backward computation.
I0815 20:55:05.358902 20241 net.cpp:323] ctx_final/relu needs backward computation.
I0815 20:55:05.358906 20241 net.cpp:323] ctx_final needs backward computation.
I0815 20:55:05.358907 20241 net.cpp:323] ctx_conv4/relu needs backward computation.
I0815 20:55:05.358909 20241 net.cpp:323] ctx_conv4/bn needs backward computation.
I0815 20:55:05.358911 20241 net.cpp:323] ctx_conv4 needs backward computation.
I0815 20:55:05.358913 20241 net.cpp:323] ctx_conv3/relu needs backward computation.
I0815 20:55:05.358916 20241 net.cpp:323] ctx_conv3/bn needs backward computation.
I0815 20:55:05.358918 20241 net.cpp:323] ctx_conv3 needs backward computation.
I0815 20:55:05.358919 20241 net.cpp:323] ctx_conv2/relu needs backward computation.
I0815 20:55:05.358922 20241 net.cpp:323] ctx_conv2/bn needs backward computation.
I0815 20:55:05.358924 20241 net.cpp:323] ctx_conv2 needs backward computation.
I0815 20:55:05.358927 20241 net.cpp:323] ctx_conv1/relu needs backward computation.
I0815 20:55:05.358928 20241 net.cpp:323] ctx_conv1/bn needs backward computation.
I0815 20:55:05.358930 20241 net.cpp:323] ctx_conv1 needs backward computation.
I0815 20:55:05.358932 20241 net.cpp:323] out3_out5_combined needs backward computation.
I0815 20:55:05.358934 20241 net.cpp:323] out3a/relu needs backward computation.
I0815 20:55:05.358942 20241 net.cpp:323] out3a/bn needs backward computation.
I0815 20:55:05.358944 20241 net.cpp:323] out3a needs backward computation.
I0815 20:55:05.358947 20241 net.cpp:323] out5a_up2 needs backward computation.
I0815 20:55:05.358949 20241 net.cpp:323] out5a/relu needs backward computation.
I0815 20:55:05.358952 20241 net.cpp:323] out5a/bn needs backward computation.
I0815 20:55:05.358954 20241 net.cpp:323] out5a needs backward computation.
I0815 20:55:05.358956 20241 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0815 20:55:05.358959 20241 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0815 20:55:05.358961 20241 net.cpp:323] res5a_branch2b needs backward computation.
I0815 20:55:05.358963 20241 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0815 20:55:05.358965 20241 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0815 20:55:05.358968 20241 net.cpp:323] res5a_branch2a needs backward computation.
I0815 20:55:05.358969 20241 net.cpp:323] pool4 needs backward computation.
I0815 20:55:05.358973 20241 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0815 20:55:05.358973 20241 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0815 20:55:05.358975 20241 net.cpp:323] res4a_branch2b needs backward computation.
I0815 20:55:05.358978 20241 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0815 20:55:05.358980 20241 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0815 20:55:05.358981 20241 net.cpp:323] res4a_branch2a needs backward computation.
I0815 20:55:05.358984 20241 net.cpp:323] pool3 needs backward computation.
I0815 20:55:05.358986 20241 net.cpp:323] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I0815 20:55:05.358989 20241 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0815 20:55:05.358991 20241 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0815 20:55:05.358994 20241 net.cpp:323] res3a_branch2b needs backward computation.
I0815 20:55:05.358996 20241 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0815 20:55:05.358999 20241 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0815 20:55:05.359000 20241 net.cpp:323] res3a_branch2a needs backward computation.
I0815 20:55:05.359002 20241 net.cpp:323] pool2 needs backward computation.
I0815 20:55:05.359005 20241 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0815 20:55:05.359007 20241 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0815 20:55:05.359009 20241 net.cpp:323] res2a_branch2b needs backward computation.
I0815 20:55:05.359012 20241 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0815 20:55:05.359014 20241 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0815 20:55:05.359016 20241 net.cpp:323] res2a_branch2a needs backward computation.
I0815 20:55:05.359019 20241 net.cpp:323] pool1 needs backward computation.
I0815 20:55:05.359021 20241 net.cpp:323] conv1b/relu needs backward computation.
I0815 20:55:05.359024 20241 net.cpp:323] conv1b/bn needs backward computation.
I0815 20:55:05.359025 20241 net.cpp:323] conv1b needs backward computation.
I0815 20:55:05.359028 20241 net.cpp:323] conv1a/relu needs backward computation.
I0815 20:55:05.359030 20241 net.cpp:323] conv1a/bn needs backward computation.
I0815 20:55:05.359031 20241 net.cpp:323] conv1a needs backward computation.
I0815 20:55:05.359035 20241 net.cpp:325] data/bias does not need backward computation.
I0815 20:55:05.359038 20241 net.cpp:325] data does not need backward computation.
I0815 20:55:05.359040 20241 net.cpp:367] This network produces output loss
I0815 20:55:05.359083 20241 net.cpp:389] Top memory (TRAIN) required for data: 956006400 diff: 946176008
I0815 20:55:05.359086 20241 net.cpp:392] Bottom memory (TRAIN) required for data: 956006400 diff: 956006400
I0815 20:55:05.359088 20241 net.cpp:395] Shared (in-place) memory (TRAIN) by data: 630374400 diff: 630374400
I0815 20:55:05.359091 20241 net.cpp:398] Parameters memory (TRAIN) required for data: 2692608 diff: 2692608
I0815 20:55:05.359097 20241 net.cpp:401] Parameters shared memory (TRAIN) by data: 0 diff: 0
I0815 20:55:05.359099 20241 net.cpp:407] Network initialization done.
I0815 20:55:05.359787 20241 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/l1reg/test.prototxt
W0815 20:55:05.359844 20241 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0815 20:55:05.360016 20241 net.cpp:72] Initializing net from parameters: 
name: "jsegnet21v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/val-image-lmdb"
    label_list_path: "data/val-label-lmdb"
    batch_size: 2
    threads: 1
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
  accuracy_param {
    ignore_label: 255
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
    ignore_label: 255
  }
}
I0815 20:55:05.360146 20241 net.cpp:104] Using FLOAT as default forward math type
I0815 20:55:05.360152 20241 net.cpp:110] Using FLOAT as default backward math type
I0815 20:55:05.360154 20241 layer_factory.hpp:136] Creating layer 'data' of type 'ImageLabelData'
I0815 20:55:05.360158 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.360172 20241 net.cpp:184] Created Layer data (0)
I0815 20:55:05.360175 20241 net.cpp:530] data -> data
I0815 20:55:05.360180 20241 net.cpp:530] data -> label
I0815 20:55:05.360200 20241 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0815 20:55:05.360206 20241 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 20:55:05.360929 20336 db_lmdb.cpp:24] Opened lmdb data/val-image-lmdb
I0815 20:55:05.362357 20241 data_layer.cpp:185] (0) ReshapePrefetch 2, 3, 640, 640
I0815 20:55:05.362448 20241 data_layer.cpp:209] (0) Output data size: 2, 3, 640, 640
I0815 20:55:05.362454 20241 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 20:55:05.362573 20241 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0815 20:55:05.362586 20241 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 20:55:05.363306 20337 data_layer.cpp:97] (0) Parser threads: 1
I0815 20:55:05.363315 20337 data_layer.cpp:99] (0) Transformer threads: 1
I0815 20:55:05.365850 20338 db_lmdb.cpp:24] Opened lmdb data/val-label-lmdb
I0815 20:55:05.367210 20241 data_layer.cpp:185] (0) ReshapePrefetch 2, 1, 640, 640
I0815 20:55:05.367383 20241 data_layer.cpp:209] (0) Output data size: 2, 1, 640, 640
I0815 20:55:05.367394 20241 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 20:55:05.367462 20241 net.cpp:245] Setting up data
I0815 20:55:05.367476 20241 net.cpp:252] TEST Top shape for layer 0 'data' 2 3 640 640 (2457600)
I0815 20:55:05.367491 20241 net.cpp:252] TEST Top shape for layer 0 'data' 2 1 640 640 (819200)
I0815 20:55:05.367504 20241 layer_factory.hpp:136] Creating layer 'label_data_1_split' of type 'Split'
I0815 20:55:05.367517 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.367537 20241 net.cpp:184] Created Layer label_data_1_split (1)
I0815 20:55:05.367544 20241 net.cpp:561] label_data_1_split <- label
I0815 20:55:05.367558 20241 net.cpp:530] label_data_1_split -> label_data_1_split_0
I0815 20:55:05.367571 20241 net.cpp:530] label_data_1_split -> label_data_1_split_1
I0815 20:55:05.367579 20241 net.cpp:530] label_data_1_split -> label_data_1_split_2
I0815 20:55:05.367727 20241 net.cpp:245] Setting up label_data_1_split
I0815 20:55:05.367734 20241 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0815 20:55:05.367741 20241 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0815 20:55:05.367748 20241 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0815 20:55:05.367753 20241 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0815 20:55:05.367759 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.367771 20241 net.cpp:184] Created Layer data/bias (2)
I0815 20:55:05.367775 20241 net.cpp:561] data/bias <- data
I0815 20:55:05.367782 20241 net.cpp:530] data/bias -> data/bias
I0815 20:55:05.368913 20339 data_layer.cpp:97] (0) Parser threads: 1
I0815 20:55:05.368922 20339 data_layer.cpp:99] (0) Transformer threads: 1
I0815 20:55:05.370781 20241 net.cpp:245] Setting up data/bias
I0815 20:55:05.370807 20241 net.cpp:252] TEST Top shape for layer 2 'data/bias' 2 3 640 640 (2457600)
I0815 20:55:05.370831 20241 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0815 20:55:05.370842 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.370869 20241 net.cpp:184] Created Layer conv1a (3)
I0815 20:55:05.370877 20241 net.cpp:561] conv1a <- data/bias
I0815 20:55:05.370885 20241 net.cpp:530] conv1a -> conv1a
I0815 20:55:05.377547 20241 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.09G, req 0G)
I0815 20:55:05.377563 20241 net.cpp:245] Setting up conv1a
I0815 20:55:05.377568 20241 net.cpp:252] TEST Top shape for layer 3 'conv1a' 2 32 320 320 (6553600)
I0815 20:55:05.377589 20241 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0815 20:55:05.377593 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.377602 20241 net.cpp:184] Created Layer conv1a/bn (4)
I0815 20:55:05.377607 20241 net.cpp:561] conv1a/bn <- conv1a
I0815 20:55:05.377611 20241 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0815 20:55:05.378350 20241 net.cpp:245] Setting up conv1a/bn
I0815 20:55:05.378356 20241 net.cpp:252] TEST Top shape for layer 4 'conv1a/bn' 2 32 320 320 (6553600)
I0815 20:55:05.378363 20241 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0815 20:55:05.378366 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.378370 20241 net.cpp:184] Created Layer conv1a/relu (5)
I0815 20:55:05.378371 20241 net.cpp:561] conv1a/relu <- conv1a
I0815 20:55:05.378374 20241 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0815 20:55:05.378377 20241 net.cpp:245] Setting up conv1a/relu
I0815 20:55:05.378381 20241 net.cpp:252] TEST Top shape for layer 5 'conv1a/relu' 2 32 320 320 (6553600)
I0815 20:55:05.378382 20241 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0815 20:55:05.378384 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.378391 20241 net.cpp:184] Created Layer conv1b (6)
I0815 20:55:05.378396 20241 net.cpp:561] conv1b <- conv1a
I0815 20:55:05.378399 20241 net.cpp:530] conv1b -> conv1b
I0815 20:55:05.392042 20241 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.06G, req 0G)
I0815 20:55:05.392055 20241 net.cpp:245] Setting up conv1b
I0815 20:55:05.392060 20241 net.cpp:252] TEST Top shape for layer 6 'conv1b' 2 32 320 320 (6553600)
I0815 20:55:05.392066 20241 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0815 20:55:05.392069 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.392076 20241 net.cpp:184] Created Layer conv1b/bn (7)
I0815 20:55:05.392077 20241 net.cpp:561] conv1b/bn <- conv1b
I0815 20:55:05.392081 20241 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0815 20:55:05.392802 20241 net.cpp:245] Setting up conv1b/bn
I0815 20:55:05.392810 20241 net.cpp:252] TEST Top shape for layer 7 'conv1b/bn' 2 32 320 320 (6553600)
I0815 20:55:05.392817 20241 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0815 20:55:05.392819 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.392823 20241 net.cpp:184] Created Layer conv1b/relu (8)
I0815 20:55:05.392825 20241 net.cpp:561] conv1b/relu <- conv1b
I0815 20:55:05.392828 20241 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0815 20:55:05.392832 20241 net.cpp:245] Setting up conv1b/relu
I0815 20:55:05.392834 20241 net.cpp:252] TEST Top shape for layer 8 'conv1b/relu' 2 32 320 320 (6553600)
I0815 20:55:05.392837 20241 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0815 20:55:05.392838 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.392843 20241 net.cpp:184] Created Layer pool1 (9)
I0815 20:55:05.392844 20241 net.cpp:561] pool1 <- conv1b
I0815 20:55:05.392848 20241 net.cpp:530] pool1 -> pool1
I0815 20:55:05.392917 20241 net.cpp:245] Setting up pool1
I0815 20:55:05.392922 20241 net.cpp:252] TEST Top shape for layer 9 'pool1' 2 32 160 160 (1638400)
I0815 20:55:05.392925 20241 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0815 20:55:05.392927 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.392933 20241 net.cpp:184] Created Layer res2a_branch2a (10)
I0815 20:55:05.392936 20241 net.cpp:561] res2a_branch2a <- pool1
I0815 20:55:05.392938 20241 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0815 20:55:05.401135 20241 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.03G, req 0G)
I0815 20:55:05.401154 20241 net.cpp:245] Setting up res2a_branch2a
I0815 20:55:05.401160 20241 net.cpp:252] TEST Top shape for layer 10 'res2a_branch2a' 2 64 160 160 (3276800)
I0815 20:55:05.401165 20241 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0815 20:55:05.401168 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.401173 20241 net.cpp:184] Created Layer res2a_branch2a/bn (11)
I0815 20:55:05.401176 20241 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0815 20:55:05.401180 20241 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0815 20:55:05.401872 20241 net.cpp:245] Setting up res2a_branch2a/bn
I0815 20:55:05.401880 20241 net.cpp:252] TEST Top shape for layer 11 'res2a_branch2a/bn' 2 64 160 160 (3276800)
I0815 20:55:05.401885 20241 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0815 20:55:05.401887 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.401890 20241 net.cpp:184] Created Layer res2a_branch2a/relu (12)
I0815 20:55:05.401892 20241 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0815 20:55:05.401895 20241 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0815 20:55:05.401898 20241 net.cpp:245] Setting up res2a_branch2a/relu
I0815 20:55:05.401901 20241 net.cpp:252] TEST Top shape for layer 12 'res2a_branch2a/relu' 2 64 160 160 (3276800)
I0815 20:55:05.401902 20241 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0815 20:55:05.401904 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.401911 20241 net.cpp:184] Created Layer res2a_branch2b (13)
I0815 20:55:05.401913 20241 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0815 20:55:05.401916 20241 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0815 20:55:05.408524 20241 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.02G, req 0G)
I0815 20:55:05.408534 20241 net.cpp:245] Setting up res2a_branch2b
I0815 20:55:05.408537 20241 net.cpp:252] TEST Top shape for layer 13 'res2a_branch2b' 2 64 160 160 (3276800)
I0815 20:55:05.408542 20241 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0815 20:55:05.408545 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.408548 20241 net.cpp:184] Created Layer res2a_branch2b/bn (14)
I0815 20:55:05.408551 20241 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0815 20:55:05.408553 20241 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0815 20:55:05.409238 20241 net.cpp:245] Setting up res2a_branch2b/bn
I0815 20:55:05.409246 20241 net.cpp:252] TEST Top shape for layer 14 'res2a_branch2b/bn' 2 64 160 160 (3276800)
I0815 20:55:05.409251 20241 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0815 20:55:05.409255 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.409257 20241 net.cpp:184] Created Layer res2a_branch2b/relu (15)
I0815 20:55:05.409260 20241 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0815 20:55:05.409261 20241 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0815 20:55:05.409265 20241 net.cpp:245] Setting up res2a_branch2b/relu
I0815 20:55:05.409267 20241 net.cpp:252] TEST Top shape for layer 15 'res2a_branch2b/relu' 2 64 160 160 (3276800)
I0815 20:55:05.409270 20241 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0815 20:55:05.409271 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.409274 20241 net.cpp:184] Created Layer pool2 (16)
I0815 20:55:05.409277 20241 net.cpp:561] pool2 <- res2a_branch2b
I0815 20:55:05.409281 20241 net.cpp:530] pool2 -> pool2
I0815 20:55:05.409349 20241 net.cpp:245] Setting up pool2
I0815 20:55:05.409354 20241 net.cpp:252] TEST Top shape for layer 16 'pool2' 2 64 80 80 (819200)
I0815 20:55:05.409364 20241 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0815 20:55:05.409368 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.409374 20241 net.cpp:184] Created Layer res3a_branch2a (17)
I0815 20:55:05.409377 20241 net.cpp:561] res3a_branch2a <- pool2
I0815 20:55:05.409380 20241 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0815 20:55:05.414916 20241 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.01G, req 0G)
I0815 20:55:05.414928 20241 net.cpp:245] Setting up res3a_branch2a
I0815 20:55:05.414932 20241 net.cpp:252] TEST Top shape for layer 17 'res3a_branch2a' 2 128 80 80 (1638400)
I0815 20:55:05.414937 20241 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0815 20:55:05.414940 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.414947 20241 net.cpp:184] Created Layer res3a_branch2a/bn (18)
I0815 20:55:05.414948 20241 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0815 20:55:05.414952 20241 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0815 20:55:05.415666 20241 net.cpp:245] Setting up res3a_branch2a/bn
I0815 20:55:05.415673 20241 net.cpp:252] TEST Top shape for layer 18 'res3a_branch2a/bn' 2 128 80 80 (1638400)
I0815 20:55:05.415680 20241 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0815 20:55:05.415683 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.415686 20241 net.cpp:184] Created Layer res3a_branch2a/relu (19)
I0815 20:55:05.415688 20241 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0815 20:55:05.415691 20241 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0815 20:55:05.415695 20241 net.cpp:245] Setting up res3a_branch2a/relu
I0815 20:55:05.415697 20241 net.cpp:252] TEST Top shape for layer 19 'res3a_branch2a/relu' 2 128 80 80 (1638400)
I0815 20:55:05.415699 20241 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0815 20:55:05.415701 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.415710 20241 net.cpp:184] Created Layer res3a_branch2b (20)
I0815 20:55:05.415715 20241 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0815 20:55:05.415717 20241 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0815 20:55:05.420872 20241 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7G, req 0G)
I0815 20:55:05.420884 20241 net.cpp:245] Setting up res3a_branch2b
I0815 20:55:05.420888 20241 net.cpp:252] TEST Top shape for layer 20 'res3a_branch2b' 2 128 80 80 (1638400)
I0815 20:55:05.420893 20241 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0815 20:55:05.420897 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.420902 20241 net.cpp:184] Created Layer res3a_branch2b/bn (21)
I0815 20:55:05.420903 20241 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0815 20:55:05.420907 20241 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0815 20:55:05.421656 20241 net.cpp:245] Setting up res3a_branch2b/bn
I0815 20:55:05.421665 20241 net.cpp:252] TEST Top shape for layer 21 'res3a_branch2b/bn' 2 128 80 80 (1638400)
I0815 20:55:05.421671 20241 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0815 20:55:05.421674 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.421679 20241 net.cpp:184] Created Layer res3a_branch2b/relu (22)
I0815 20:55:05.421680 20241 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0815 20:55:05.421684 20241 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0815 20:55:05.421687 20241 net.cpp:245] Setting up res3a_branch2b/relu
I0815 20:55:05.421689 20241 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b/relu' 2 128 80 80 (1638400)
I0815 20:55:05.421708 20241 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0815 20:55:05.421713 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.421716 20241 net.cpp:184] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (23)
I0815 20:55:05.421720 20241 net.cpp:561] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0815 20:55:05.421721 20241 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0815 20:55:05.421725 20241 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0815 20:55:05.421772 20241 net.cpp:245] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0815 20:55:05.421777 20241 net.cpp:252] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 2 128 80 80 (1638400)
I0815 20:55:05.421779 20241 net.cpp:252] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 2 128 80 80 (1638400)
I0815 20:55:05.421782 20241 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0815 20:55:05.421784 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.421788 20241 net.cpp:184] Created Layer pool3 (24)
I0815 20:55:05.421790 20241 net.cpp:561] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0815 20:55:05.421793 20241 net.cpp:530] pool3 -> pool3
I0815 20:55:05.421865 20241 net.cpp:245] Setting up pool3
I0815 20:55:05.421871 20241 net.cpp:252] TEST Top shape for layer 24 'pool3' 2 128 40 40 (409600)
I0815 20:55:05.421874 20241 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0815 20:55:05.421875 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.421885 20241 net.cpp:184] Created Layer res4a_branch2a (25)
I0815 20:55:05.421888 20241 net.cpp:561] res4a_branch2a <- pool3
I0815 20:55:05.421890 20241 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0815 20:55:05.433149 20241 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.99G, req 0G)
I0815 20:55:05.433161 20241 net.cpp:245] Setting up res4a_branch2a
I0815 20:55:05.433166 20241 net.cpp:252] TEST Top shape for layer 25 'res4a_branch2a' 2 256 40 40 (819200)
I0815 20:55:05.433169 20241 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0815 20:55:05.433172 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.433183 20241 net.cpp:184] Created Layer res4a_branch2a/bn (26)
I0815 20:55:05.433187 20241 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0815 20:55:05.433188 20241 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0815 20:55:05.433881 20241 net.cpp:245] Setting up res4a_branch2a/bn
I0815 20:55:05.433888 20241 net.cpp:252] TEST Top shape for layer 26 'res4a_branch2a/bn' 2 256 40 40 (819200)
I0815 20:55:05.433893 20241 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0815 20:55:05.433897 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.433899 20241 net.cpp:184] Created Layer res4a_branch2a/relu (27)
I0815 20:55:05.433902 20241 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0815 20:55:05.433903 20241 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0815 20:55:05.433907 20241 net.cpp:245] Setting up res4a_branch2a/relu
I0815 20:55:05.433909 20241 net.cpp:252] TEST Top shape for layer 27 'res4a_branch2a/relu' 2 256 40 40 (819200)
I0815 20:55:05.433912 20241 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0815 20:55:05.433914 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.433920 20241 net.cpp:184] Created Layer res4a_branch2b (28)
I0815 20:55:05.433923 20241 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0815 20:55:05.433938 20241 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0815 20:55:05.440789 20241 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.98G, req 0G)
I0815 20:55:05.440804 20241 net.cpp:245] Setting up res4a_branch2b
I0815 20:55:05.440809 20241 net.cpp:252] TEST Top shape for layer 28 'res4a_branch2b' 2 256 40 40 (819200)
I0815 20:55:05.440815 20241 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0815 20:55:05.440819 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.440830 20241 net.cpp:184] Created Layer res4a_branch2b/bn (29)
I0815 20:55:05.440834 20241 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0815 20:55:05.440841 20241 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0815 20:55:05.441576 20241 net.cpp:245] Setting up res4a_branch2b/bn
I0815 20:55:05.441586 20241 net.cpp:252] TEST Top shape for layer 29 'res4a_branch2b/bn' 2 256 40 40 (819200)
I0815 20:55:05.441591 20241 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0815 20:55:05.441594 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.441599 20241 net.cpp:184] Created Layer res4a_branch2b/relu (30)
I0815 20:55:05.441602 20241 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0815 20:55:05.441604 20241 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0815 20:55:05.441610 20241 net.cpp:245] Setting up res4a_branch2b/relu
I0815 20:55:05.441613 20241 net.cpp:252] TEST Top shape for layer 30 'res4a_branch2b/relu' 2 256 40 40 (819200)
I0815 20:55:05.441617 20241 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0815 20:55:05.441618 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.441623 20241 net.cpp:184] Created Layer pool4 (31)
I0815 20:55:05.441625 20241 net.cpp:561] pool4 <- res4a_branch2b
I0815 20:55:05.441628 20241 net.cpp:530] pool4 -> pool4
I0815 20:55:05.441695 20241 net.cpp:245] Setting up pool4
I0815 20:55:05.441700 20241 net.cpp:252] TEST Top shape for layer 31 'pool4' 2 256 40 40 (819200)
I0815 20:55:05.441704 20241 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0815 20:55:05.441707 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.441721 20241 net.cpp:184] Created Layer res5a_branch2a (32)
I0815 20:55:05.441725 20241 net.cpp:561] res5a_branch2a <- pool4
I0815 20:55:05.441727 20241 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0815 20:55:05.466642 20241 net.cpp:245] Setting up res5a_branch2a
I0815 20:55:05.466667 20241 net.cpp:252] TEST Top shape for layer 32 'res5a_branch2a' 2 512 40 40 (1638400)
I0815 20:55:05.466675 20241 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0815 20:55:05.466678 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.466684 20241 net.cpp:184] Created Layer res5a_branch2a/bn (33)
I0815 20:55:05.466688 20241 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0815 20:55:05.466691 20241 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0815 20:55:05.467378 20241 net.cpp:245] Setting up res5a_branch2a/bn
I0815 20:55:05.467386 20241 net.cpp:252] TEST Top shape for layer 33 'res5a_branch2a/bn' 2 512 40 40 (1638400)
I0815 20:55:05.467392 20241 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0815 20:55:05.467394 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.467397 20241 net.cpp:184] Created Layer res5a_branch2a/relu (34)
I0815 20:55:05.467401 20241 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0815 20:55:05.467402 20241 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0815 20:55:05.467406 20241 net.cpp:245] Setting up res5a_branch2a/relu
I0815 20:55:05.467408 20241 net.cpp:252] TEST Top shape for layer 34 'res5a_branch2a/relu' 2 512 40 40 (1638400)
I0815 20:55:05.467424 20241 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0815 20:55:05.467428 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.467434 20241 net.cpp:184] Created Layer res5a_branch2b (35)
I0815 20:55:05.467437 20241 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0815 20:55:05.467439 20241 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0815 20:55:05.480543 20241 net.cpp:245] Setting up res5a_branch2b
I0815 20:55:05.480569 20241 net.cpp:252] TEST Top shape for layer 35 'res5a_branch2b' 2 512 40 40 (1638400)
I0815 20:55:05.480578 20241 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0815 20:55:05.480582 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.480589 20241 net.cpp:184] Created Layer res5a_branch2b/bn (36)
I0815 20:55:05.480592 20241 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0815 20:55:05.480595 20241 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0815 20:55:05.481307 20241 net.cpp:245] Setting up res5a_branch2b/bn
I0815 20:55:05.481315 20241 net.cpp:252] TEST Top shape for layer 36 'res5a_branch2b/bn' 2 512 40 40 (1638400)
I0815 20:55:05.481320 20241 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0815 20:55:05.481323 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.481328 20241 net.cpp:184] Created Layer res5a_branch2b/relu (37)
I0815 20:55:05.481329 20241 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0815 20:55:05.481331 20241 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0815 20:55:05.481335 20241 net.cpp:245] Setting up res5a_branch2b/relu
I0815 20:55:05.481338 20241 net.cpp:252] TEST Top shape for layer 37 'res5a_branch2b/relu' 2 512 40 40 (1638400)
I0815 20:55:05.481339 20241 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I0815 20:55:05.481341 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.481351 20241 net.cpp:184] Created Layer out5a (38)
I0815 20:55:05.481355 20241 net.cpp:561] out5a <- res5a_branch2b
I0815 20:55:05.481361 20241 net.cpp:530] out5a -> out5a
I0815 20:55:05.484740 20241 net.cpp:245] Setting up out5a
I0815 20:55:05.484747 20241 net.cpp:252] TEST Top shape for layer 38 'out5a' 2 64 40 40 (204800)
I0815 20:55:05.484751 20241 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I0815 20:55:05.484755 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.484758 20241 net.cpp:184] Created Layer out5a/bn (39)
I0815 20:55:05.484761 20241 net.cpp:561] out5a/bn <- out5a
I0815 20:55:05.484763 20241 net.cpp:513] out5a/bn -> out5a (in-place)
I0815 20:55:05.485461 20241 net.cpp:245] Setting up out5a/bn
I0815 20:55:05.485467 20241 net.cpp:252] TEST Top shape for layer 39 'out5a/bn' 2 64 40 40 (204800)
I0815 20:55:05.485473 20241 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I0815 20:55:05.485476 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.485478 20241 net.cpp:184] Created Layer out5a/relu (40)
I0815 20:55:05.485481 20241 net.cpp:561] out5a/relu <- out5a
I0815 20:55:05.485482 20241 net.cpp:513] out5a/relu -> out5a (in-place)
I0815 20:55:05.485486 20241 net.cpp:245] Setting up out5a/relu
I0815 20:55:05.485488 20241 net.cpp:252] TEST Top shape for layer 40 'out5a/relu' 2 64 40 40 (204800)
I0815 20:55:05.485491 20241 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I0815 20:55:05.485492 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.485497 20241 net.cpp:184] Created Layer out5a_up2 (41)
I0815 20:55:05.485499 20241 net.cpp:561] out5a_up2 <- out5a
I0815 20:55:05.485502 20241 net.cpp:530] out5a_up2 -> out5a_up2
I0815 20:55:05.485818 20241 net.cpp:245] Setting up out5a_up2
I0815 20:55:05.485824 20241 net.cpp:252] TEST Top shape for layer 41 'out5a_up2' 2 64 80 80 (819200)
I0815 20:55:05.485827 20241 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I0815 20:55:05.485829 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.485834 20241 net.cpp:184] Created Layer out3a (42)
I0815 20:55:05.485836 20241 net.cpp:561] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0815 20:55:05.485839 20241 net.cpp:530] out3a -> out3a
I0815 20:55:05.489984 20241 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.97G, req 0G)
I0815 20:55:05.489995 20241 net.cpp:245] Setting up out3a
I0815 20:55:05.490000 20241 net.cpp:252] TEST Top shape for layer 42 'out3a' 2 64 80 80 (819200)
I0815 20:55:05.490003 20241 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I0815 20:55:05.490006 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.490011 20241 net.cpp:184] Created Layer out3a/bn (43)
I0815 20:55:05.490013 20241 net.cpp:561] out3a/bn <- out3a
I0815 20:55:05.490016 20241 net.cpp:513] out3a/bn -> out3a (in-place)
I0815 20:55:05.490748 20241 net.cpp:245] Setting up out3a/bn
I0815 20:55:05.490756 20241 net.cpp:252] TEST Top shape for layer 43 'out3a/bn' 2 64 80 80 (819200)
I0815 20:55:05.490761 20241 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I0815 20:55:05.490763 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.490767 20241 net.cpp:184] Created Layer out3a/relu (44)
I0815 20:55:05.490768 20241 net.cpp:561] out3a/relu <- out3a
I0815 20:55:05.490772 20241 net.cpp:513] out3a/relu -> out3a (in-place)
I0815 20:55:05.490774 20241 net.cpp:245] Setting up out3a/relu
I0815 20:55:05.490777 20241 net.cpp:252] TEST Top shape for layer 44 'out3a/relu' 2 64 80 80 (819200)
I0815 20:55:05.490778 20241 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I0815 20:55:05.490782 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.490785 20241 net.cpp:184] Created Layer out3_out5_combined (45)
I0815 20:55:05.490787 20241 net.cpp:561] out3_out5_combined <- out5a_up2
I0815 20:55:05.490789 20241 net.cpp:561] out3_out5_combined <- out3a
I0815 20:55:05.490792 20241 net.cpp:530] out3_out5_combined -> out3_out5_combined
I0815 20:55:05.491699 20241 net.cpp:245] Setting up out3_out5_combined
I0815 20:55:05.491708 20241 net.cpp:252] TEST Top shape for layer 45 'out3_out5_combined' 2 64 80 80 (819200)
I0815 20:55:05.491710 20241 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I0815 20:55:05.491714 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.491719 20241 net.cpp:184] Created Layer ctx_conv1 (46)
I0815 20:55:05.491721 20241 net.cpp:561] ctx_conv1 <- out3_out5_combined
I0815 20:55:05.491724 20241 net.cpp:530] ctx_conv1 -> ctx_conv1
I0815 20:55:05.495625 20241 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.96G, req 0G)
I0815 20:55:05.495635 20241 net.cpp:245] Setting up ctx_conv1
I0815 20:55:05.495640 20241 net.cpp:252] TEST Top shape for layer 46 'ctx_conv1' 2 64 80 80 (819200)
I0815 20:55:05.495643 20241 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I0815 20:55:05.495645 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.495651 20241 net.cpp:184] Created Layer ctx_conv1/bn (47)
I0815 20:55:05.495653 20241 net.cpp:561] ctx_conv1/bn <- ctx_conv1
I0815 20:55:05.495656 20241 net.cpp:513] ctx_conv1/bn -> ctx_conv1 (in-place)
I0815 20:55:05.496384 20241 net.cpp:245] Setting up ctx_conv1/bn
I0815 20:55:05.496392 20241 net.cpp:252] TEST Top shape for layer 47 'ctx_conv1/bn' 2 64 80 80 (819200)
I0815 20:55:05.496398 20241 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I0815 20:55:05.496407 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.496412 20241 net.cpp:184] Created Layer ctx_conv1/relu (48)
I0815 20:55:05.496413 20241 net.cpp:561] ctx_conv1/relu <- ctx_conv1
I0815 20:55:05.496417 20241 net.cpp:513] ctx_conv1/relu -> ctx_conv1 (in-place)
I0815 20:55:05.496419 20241 net.cpp:245] Setting up ctx_conv1/relu
I0815 20:55:05.496423 20241 net.cpp:252] TEST Top shape for layer 48 'ctx_conv1/relu' 2 64 80 80 (819200)
I0815 20:55:05.496424 20241 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I0815 20:55:05.496426 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.496434 20241 net.cpp:184] Created Layer ctx_conv2 (49)
I0815 20:55:05.496438 20241 net.cpp:561] ctx_conv2 <- ctx_conv1
I0815 20:55:05.496440 20241 net.cpp:530] ctx_conv2 -> ctx_conv2
I0815 20:55:05.497553 20241 net.cpp:245] Setting up ctx_conv2
I0815 20:55:05.497560 20241 net.cpp:252] TEST Top shape for layer 49 'ctx_conv2' 2 64 80 80 (819200)
I0815 20:55:05.497565 20241 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I0815 20:55:05.497566 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.497570 20241 net.cpp:184] Created Layer ctx_conv2/bn (50)
I0815 20:55:05.497572 20241 net.cpp:561] ctx_conv2/bn <- ctx_conv2
I0815 20:55:05.497575 20241 net.cpp:513] ctx_conv2/bn -> ctx_conv2 (in-place)
I0815 20:55:05.498284 20241 net.cpp:245] Setting up ctx_conv2/bn
I0815 20:55:05.498291 20241 net.cpp:252] TEST Top shape for layer 50 'ctx_conv2/bn' 2 64 80 80 (819200)
I0815 20:55:05.498296 20241 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I0815 20:55:05.498299 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.498301 20241 net.cpp:184] Created Layer ctx_conv2/relu (51)
I0815 20:55:05.498304 20241 net.cpp:561] ctx_conv2/relu <- ctx_conv2
I0815 20:55:05.498306 20241 net.cpp:513] ctx_conv2/relu -> ctx_conv2 (in-place)
I0815 20:55:05.498309 20241 net.cpp:245] Setting up ctx_conv2/relu
I0815 20:55:05.498311 20241 net.cpp:252] TEST Top shape for layer 51 'ctx_conv2/relu' 2 64 80 80 (819200)
I0815 20:55:05.498313 20241 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I0815 20:55:05.498316 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.498320 20241 net.cpp:184] Created Layer ctx_conv3 (52)
I0815 20:55:05.498322 20241 net.cpp:561] ctx_conv3 <- ctx_conv2
I0815 20:55:05.498324 20241 net.cpp:530] ctx_conv3 -> ctx_conv3
I0815 20:55:05.499433 20241 net.cpp:245] Setting up ctx_conv3
I0815 20:55:05.499439 20241 net.cpp:252] TEST Top shape for layer 52 'ctx_conv3' 2 64 80 80 (819200)
I0815 20:55:05.499444 20241 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I0815 20:55:05.499445 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.499449 20241 net.cpp:184] Created Layer ctx_conv3/bn (53)
I0815 20:55:05.499451 20241 net.cpp:561] ctx_conv3/bn <- ctx_conv3
I0815 20:55:05.499454 20241 net.cpp:513] ctx_conv3/bn -> ctx_conv3 (in-place)
I0815 20:55:05.500483 20241 net.cpp:245] Setting up ctx_conv3/bn
I0815 20:55:05.500495 20241 net.cpp:252] TEST Top shape for layer 53 'ctx_conv3/bn' 2 64 80 80 (819200)
I0815 20:55:05.500504 20241 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I0815 20:55:05.500509 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.500514 20241 net.cpp:184] Created Layer ctx_conv3/relu (54)
I0815 20:55:05.500516 20241 net.cpp:561] ctx_conv3/relu <- ctx_conv3
I0815 20:55:05.500520 20241 net.cpp:513] ctx_conv3/relu -> ctx_conv3 (in-place)
I0815 20:55:05.500525 20241 net.cpp:245] Setting up ctx_conv3/relu
I0815 20:55:05.500530 20241 net.cpp:252] TEST Top shape for layer 54 'ctx_conv3/relu' 2 64 80 80 (819200)
I0815 20:55:05.500541 20241 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I0815 20:55:05.500545 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.500552 20241 net.cpp:184] Created Layer ctx_conv4 (55)
I0815 20:55:05.500556 20241 net.cpp:561] ctx_conv4 <- ctx_conv3
I0815 20:55:05.500560 20241 net.cpp:530] ctx_conv4 -> ctx_conv4
I0815 20:55:05.502034 20241 net.cpp:245] Setting up ctx_conv4
I0815 20:55:05.502044 20241 net.cpp:252] TEST Top shape for layer 55 'ctx_conv4' 2 64 80 80 (819200)
I0815 20:55:05.502049 20241 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I0815 20:55:05.502053 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.502058 20241 net.cpp:184] Created Layer ctx_conv4/bn (56)
I0815 20:55:05.502063 20241 net.cpp:561] ctx_conv4/bn <- ctx_conv4
I0815 20:55:05.502065 20241 net.cpp:513] ctx_conv4/bn -> ctx_conv4 (in-place)
I0815 20:55:05.502971 20241 net.cpp:245] Setting up ctx_conv4/bn
I0815 20:55:05.502979 20241 net.cpp:252] TEST Top shape for layer 56 'ctx_conv4/bn' 2 64 80 80 (819200)
I0815 20:55:05.502987 20241 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I0815 20:55:05.502991 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.502995 20241 net.cpp:184] Created Layer ctx_conv4/relu (57)
I0815 20:55:05.502998 20241 net.cpp:561] ctx_conv4/relu <- ctx_conv4
I0815 20:55:05.503002 20241 net.cpp:513] ctx_conv4/relu -> ctx_conv4 (in-place)
I0815 20:55:05.503006 20241 net.cpp:245] Setting up ctx_conv4/relu
I0815 20:55:05.503011 20241 net.cpp:252] TEST Top shape for layer 57 'ctx_conv4/relu' 2 64 80 80 (819200)
I0815 20:55:05.503015 20241 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I0815 20:55:05.503018 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.503026 20241 net.cpp:184] Created Layer ctx_final (58)
I0815 20:55:05.503031 20241 net.cpp:561] ctx_final <- ctx_conv4
I0815 20:55:05.503034 20241 net.cpp:530] ctx_final -> ctx_final
I0815 20:55:05.508097 20241 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.96G, req 0G)
I0815 20:55:05.508108 20241 net.cpp:245] Setting up ctx_final
I0815 20:55:05.508114 20241 net.cpp:252] TEST Top shape for layer 58 'ctx_final' 2 8 80 80 (102400)
I0815 20:55:05.508121 20241 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I0815 20:55:05.508123 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.508136 20241 net.cpp:184] Created Layer ctx_final/relu (59)
I0815 20:55:05.508139 20241 net.cpp:561] ctx_final/relu <- ctx_final
I0815 20:55:05.508143 20241 net.cpp:513] ctx_final/relu -> ctx_final (in-place)
I0815 20:55:05.508158 20241 net.cpp:245] Setting up ctx_final/relu
I0815 20:55:05.508163 20241 net.cpp:252] TEST Top shape for layer 59 'ctx_final/relu' 2 8 80 80 (102400)
I0815 20:55:05.508167 20241 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I0815 20:55:05.508170 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.508182 20241 net.cpp:184] Created Layer out_deconv_final_up2 (60)
I0815 20:55:05.508184 20241 net.cpp:561] out_deconv_final_up2 <- ctx_final
I0815 20:55:05.508188 20241 net.cpp:530] out_deconv_final_up2 -> out_deconv_final_up2
I0815 20:55:05.508597 20241 net.cpp:245] Setting up out_deconv_final_up2
I0815 20:55:05.508605 20241 net.cpp:252] TEST Top shape for layer 60 'out_deconv_final_up2' 2 8 160 160 (409600)
I0815 20:55:05.508610 20241 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I0815 20:55:05.508613 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.508620 20241 net.cpp:184] Created Layer out_deconv_final_up4 (61)
I0815 20:55:05.508631 20241 net.cpp:561] out_deconv_final_up4 <- out_deconv_final_up2
I0815 20:55:05.508635 20241 net.cpp:530] out_deconv_final_up4 -> out_deconv_final_up4
I0815 20:55:05.509016 20241 net.cpp:245] Setting up out_deconv_final_up4
I0815 20:55:05.509022 20241 net.cpp:252] TEST Top shape for layer 61 'out_deconv_final_up4' 2 8 320 320 (1638400)
I0815 20:55:05.509027 20241 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I0815 20:55:05.509029 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.509037 20241 net.cpp:184] Created Layer out_deconv_final_up8 (62)
I0815 20:55:05.509040 20241 net.cpp:561] out_deconv_final_up8 <- out_deconv_final_up4
I0815 20:55:05.509043 20241 net.cpp:530] out_deconv_final_up8 -> out_deconv_final_up8
I0815 20:55:05.509419 20241 net.cpp:245] Setting up out_deconv_final_up8
I0815 20:55:05.509425 20241 net.cpp:252] TEST Top shape for layer 62 'out_deconv_final_up8' 2 8 640 640 (6553600)
I0815 20:55:05.509430 20241 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8_out_deconv_final_up8_0_split' of type 'Split'
I0815 20:55:05.509433 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.509438 20241 net.cpp:184] Created Layer out_deconv_final_up8_out_deconv_final_up8_0_split (63)
I0815 20:55:05.509441 20241 net.cpp:561] out_deconv_final_up8_out_deconv_final_up8_0_split <- out_deconv_final_up8
I0815 20:55:05.509444 20241 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0815 20:55:05.509449 20241 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0815 20:55:05.509452 20241 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0815 20:55:05.509541 20241 net.cpp:245] Setting up out_deconv_final_up8_out_deconv_final_up8_0_split
I0815 20:55:05.509548 20241 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 8 640 640 (6553600)
I0815 20:55:05.509552 20241 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 8 640 640 (6553600)
I0815 20:55:05.509557 20241 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 8 640 640 (6553600)
I0815 20:55:05.509562 20241 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0815 20:55:05.509567 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.509573 20241 net.cpp:184] Created Layer loss (64)
I0815 20:55:05.509577 20241 net.cpp:561] loss <- out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0815 20:55:05.509582 20241 net.cpp:561] loss <- label_data_1_split_0
I0815 20:55:05.509587 20241 net.cpp:530] loss -> loss
I0815 20:55:05.510603 20241 net.cpp:245] Setting up loss
I0815 20:55:05.510613 20241 net.cpp:252] TEST Top shape for layer 64 'loss' (1)
I0815 20:55:05.510617 20241 net.cpp:256]     with loss weight 1
I0815 20:55:05.510623 20241 layer_factory.hpp:136] Creating layer 'accuracy/top1' of type 'Accuracy'
I0815 20:55:05.510627 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.510640 20241 net.cpp:184] Created Layer accuracy/top1 (65)
I0815 20:55:05.510645 20241 net.cpp:561] accuracy/top1 <- out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0815 20:55:05.510649 20241 net.cpp:561] accuracy/top1 <- label_data_1_split_1
I0815 20:55:05.510654 20241 net.cpp:530] accuracy/top1 -> accuracy/top1
I0815 20:55:05.510661 20241 net.cpp:245] Setting up accuracy/top1
I0815 20:55:05.510665 20241 net.cpp:252] TEST Top shape for layer 65 'accuracy/top1' (1)
I0815 20:55:05.510669 20241 layer_factory.hpp:136] Creating layer 'accuracy/top5' of type 'Accuracy'
I0815 20:55:05.510673 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.510684 20241 net.cpp:184] Created Layer accuracy/top5 (66)
I0815 20:55:05.510689 20241 net.cpp:561] accuracy/top5 <- out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0815 20:55:05.510694 20241 net.cpp:561] accuracy/top5 <- label_data_1_split_2
I0815 20:55:05.510697 20241 net.cpp:530] accuracy/top5 -> accuracy/top5
I0815 20:55:05.510704 20241 net.cpp:245] Setting up accuracy/top5
I0815 20:55:05.510709 20241 net.cpp:252] TEST Top shape for layer 66 'accuracy/top5' (1)
I0815 20:55:05.510712 20241 net.cpp:325] accuracy/top5 does not need backward computation.
I0815 20:55:05.510716 20241 net.cpp:325] accuracy/top1 does not need backward computation.
I0815 20:55:05.510720 20241 net.cpp:323] loss needs backward computation.
I0815 20:55:05.510725 20241 net.cpp:323] out_deconv_final_up8_out_deconv_final_up8_0_split needs backward computation.
I0815 20:55:05.510728 20241 net.cpp:323] out_deconv_final_up8 needs backward computation.
I0815 20:55:05.510732 20241 net.cpp:323] out_deconv_final_up4 needs backward computation.
I0815 20:55:05.510736 20241 net.cpp:323] out_deconv_final_up2 needs backward computation.
I0815 20:55:05.510740 20241 net.cpp:323] ctx_final/relu needs backward computation.
I0815 20:55:05.510743 20241 net.cpp:323] ctx_final needs backward computation.
I0815 20:55:05.510747 20241 net.cpp:323] ctx_conv4/relu needs backward computation.
I0815 20:55:05.510751 20241 net.cpp:323] ctx_conv4/bn needs backward computation.
I0815 20:55:05.510754 20241 net.cpp:323] ctx_conv4 needs backward computation.
I0815 20:55:05.510758 20241 net.cpp:323] ctx_conv3/relu needs backward computation.
I0815 20:55:05.510762 20241 net.cpp:323] ctx_conv3/bn needs backward computation.
I0815 20:55:05.510766 20241 net.cpp:323] ctx_conv3 needs backward computation.
I0815 20:55:05.510769 20241 net.cpp:323] ctx_conv2/relu needs backward computation.
I0815 20:55:05.510772 20241 net.cpp:323] ctx_conv2/bn needs backward computation.
I0815 20:55:05.510776 20241 net.cpp:323] ctx_conv2 needs backward computation.
I0815 20:55:05.510779 20241 net.cpp:323] ctx_conv1/relu needs backward computation.
I0815 20:55:05.510783 20241 net.cpp:323] ctx_conv1/bn needs backward computation.
I0815 20:55:05.510787 20241 net.cpp:323] ctx_conv1 needs backward computation.
I0815 20:55:05.510792 20241 net.cpp:323] out3_out5_combined needs backward computation.
I0815 20:55:05.510795 20241 net.cpp:323] out3a/relu needs backward computation.
I0815 20:55:05.510798 20241 net.cpp:323] out3a/bn needs backward computation.
I0815 20:55:05.510802 20241 net.cpp:323] out3a needs backward computation.
I0815 20:55:05.510807 20241 net.cpp:323] out5a_up2 needs backward computation.
I0815 20:55:05.510810 20241 net.cpp:323] out5a/relu needs backward computation.
I0815 20:55:05.510814 20241 net.cpp:323] out5a/bn needs backward computation.
I0815 20:55:05.510818 20241 net.cpp:323] out5a needs backward computation.
I0815 20:55:05.510821 20241 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0815 20:55:05.510825 20241 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0815 20:55:05.510829 20241 net.cpp:323] res5a_branch2b needs backward computation.
I0815 20:55:05.510833 20241 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0815 20:55:05.510836 20241 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0815 20:55:05.510840 20241 net.cpp:323] res5a_branch2a needs backward computation.
I0815 20:55:05.510844 20241 net.cpp:323] pool4 needs backward computation.
I0815 20:55:05.510848 20241 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0815 20:55:05.510851 20241 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0815 20:55:05.510855 20241 net.cpp:323] res4a_branch2b needs backward computation.
I0815 20:55:05.510859 20241 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0815 20:55:05.510864 20241 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0815 20:55:05.510866 20241 net.cpp:323] res4a_branch2a needs backward computation.
I0815 20:55:05.510874 20241 net.cpp:323] pool3 needs backward computation.
I0815 20:55:05.510879 20241 net.cpp:323] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I0815 20:55:05.510884 20241 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0815 20:55:05.510886 20241 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0815 20:55:05.510890 20241 net.cpp:323] res3a_branch2b needs backward computation.
I0815 20:55:05.510895 20241 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0815 20:55:05.510898 20241 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0815 20:55:05.510901 20241 net.cpp:323] res3a_branch2a needs backward computation.
I0815 20:55:05.510905 20241 net.cpp:323] pool2 needs backward computation.
I0815 20:55:05.510910 20241 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0815 20:55:05.510913 20241 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0815 20:55:05.510916 20241 net.cpp:323] res2a_branch2b needs backward computation.
I0815 20:55:05.510921 20241 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0815 20:55:05.510924 20241 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0815 20:55:05.510928 20241 net.cpp:323] res2a_branch2a needs backward computation.
I0815 20:55:05.510932 20241 net.cpp:323] pool1 needs backward computation.
I0815 20:55:05.510936 20241 net.cpp:323] conv1b/relu needs backward computation.
I0815 20:55:05.510941 20241 net.cpp:323] conv1b/bn needs backward computation.
I0815 20:55:05.510943 20241 net.cpp:323] conv1b needs backward computation.
I0815 20:55:05.510947 20241 net.cpp:323] conv1a/relu needs backward computation.
I0815 20:55:05.510951 20241 net.cpp:323] conv1a/bn needs backward computation.
I0815 20:55:05.510956 20241 net.cpp:323] conv1a needs backward computation.
I0815 20:55:05.510959 20241 net.cpp:325] data/bias does not need backward computation.
I0815 20:55:05.510963 20241 net.cpp:325] label_data_1_split does not need backward computation.
I0815 20:55:05.510968 20241 net.cpp:325] data does not need backward computation.
I0815 20:55:05.510972 20241 net.cpp:367] This network produces output accuracy/top1
I0815 20:55:05.510975 20241 net.cpp:367] This network produces output accuracy/top5
I0815 20:55:05.510979 20241 net.cpp:367] This network produces output loss
I0815 20:55:05.511036 20241 net.cpp:389] Top memory (TEST) required for data: 318668800 diff: 8
I0815 20:55:05.511040 20241 net.cpp:392] Bottom memory (TEST) required for data: 318668800 diff: 318668800
I0815 20:55:05.511044 20241 net.cpp:395] Shared (in-place) memory (TEST) by data: 210124800 diff: 210124800
I0815 20:55:05.511047 20241 net.cpp:398] Parameters memory (TEST) required for data: 2692608 diff: 2692608
I0815 20:55:05.511051 20241 net.cpp:401] Parameters shared memory (TEST) by data: 0 diff: 0
I0815 20:55:05.511055 20241 net.cpp:407] Network initialization done.
I0815 20:55:05.511155 20241 solver.cpp:56] Solver scaffolding done.
I0815 20:55:05.523864 20241 caffe.cpp:137] Finetuning from training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/initial/cityscapes5_jsegnet21v2_iter_32000.caffemodel
I0815 20:55:05.530268 20241 net.cpp:1095] Copying source layer data Type:ImageLabelData #blobs=0
I0815 20:55:05.530294 20241 net.cpp:1095] Copying source layer data/bias Type:Bias #blobs=1
I0815 20:55:05.530328 20241 net.cpp:1095] Copying source layer conv1a Type:Convolution #blobs=2
I0815 20:55:05.530346 20241 net.cpp:1095] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0815 20:55:05.531006 20241 net.cpp:1095] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0815 20:55:05.531014 20241 net.cpp:1095] Copying source layer conv1b Type:Convolution #blobs=2
I0815 20:55:05.531026 20241 net.cpp:1095] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0815 20:55:05.531504 20241 net.cpp:1095] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0815 20:55:05.531512 20241 net.cpp:1095] Copying source layer pool1 Type:Pooling #blobs=0
I0815 20:55:05.531514 20241 net.cpp:1095] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0815 20:55:05.531545 20241 net.cpp:1095] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0815 20:55:05.532037 20241 net.cpp:1095] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0815 20:55:05.532044 20241 net.cpp:1095] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0815 20:55:05.532059 20241 net.cpp:1095] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0815 20:55:05.532543 20241 net.cpp:1095] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0815 20:55:05.532551 20241 net.cpp:1095] Copying source layer pool2 Type:Pooling #blobs=0
I0815 20:55:05.532554 20241 net.cpp:1095] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0815 20:55:05.532596 20241 net.cpp:1095] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0815 20:55:05.533051 20241 net.cpp:1095] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0815 20:55:05.533058 20241 net.cpp:1095] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0815 20:55:05.533083 20241 net.cpp:1095] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0815 20:55:05.533527 20241 net.cpp:1095] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0815 20:55:05.533535 20241 net.cpp:1095] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I0815 20:55:05.533538 20241 net.cpp:1095] Copying source layer pool3 Type:Pooling #blobs=0
I0815 20:55:05.533541 20241 net.cpp:1095] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0815 20:55:05.533743 20241 net.cpp:1095] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0815 20:55:05.534224 20241 net.cpp:1095] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0815 20:55:05.534231 20241 net.cpp:1095] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0815 20:55:05.534296 20241 net.cpp:1095] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0815 20:55:05.534739 20241 net.cpp:1095] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0815 20:55:05.534745 20241 net.cpp:1095] Copying source layer pool4 Type:Pooling #blobs=0
I0815 20:55:05.534749 20241 net.cpp:1095] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0815 20:55:05.535133 20241 net.cpp:1095] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0815 20:55:05.535569 20241 net.cpp:1095] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0815 20:55:05.535575 20241 net.cpp:1095] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0815 20:55:05.535773 20241 net.cpp:1095] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0815 20:55:05.536238 20241 net.cpp:1095] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0815 20:55:05.536248 20241 net.cpp:1095] Copying source layer out5a Type:Convolution #blobs=2
I0815 20:55:05.536306 20241 net.cpp:1095] Copying source layer out5a/bn Type:BatchNorm #blobs=5
I0815 20:55:05.536478 20241 net.cpp:1095] Copying source layer out5a/relu Type:ReLU #blobs=0
I0815 20:55:05.536484 20241 net.cpp:1095] Copying source layer out5a_up2 Type:Deconvolution #blobs=1
I0815 20:55:05.536490 20241 net.cpp:1095] Copying source layer out3a Type:Convolution #blobs=2
I0815 20:55:05.536511 20241 net.cpp:1095] Copying source layer out3a/bn Type:BatchNorm #blobs=5
I0815 20:55:05.536660 20241 net.cpp:1095] Copying source layer out3a/relu Type:ReLU #blobs=0
I0815 20:55:05.536665 20241 net.cpp:1095] Copying source layer out3_out5_combined Type:Eltwise #blobs=0
I0815 20:55:05.536667 20241 net.cpp:1095] Copying source layer ctx_conv1 Type:Convolution #blobs=2
I0815 20:55:05.536687 20241 net.cpp:1095] Copying source layer ctx_conv1/bn Type:BatchNorm #blobs=5
I0815 20:55:05.536839 20241 net.cpp:1095] Copying source layer ctx_conv1/relu Type:ReLU #blobs=0
I0815 20:55:05.536844 20241 net.cpp:1095] Copying source layer ctx_conv2 Type:Convolution #blobs=2
I0815 20:55:05.536860 20241 net.cpp:1095] Copying source layer ctx_conv2/bn Type:BatchNorm #blobs=5
I0815 20:55:05.537010 20241 net.cpp:1095] Copying source layer ctx_conv2/relu Type:ReLU #blobs=0
I0815 20:55:05.537022 20241 net.cpp:1095] Copying source layer ctx_conv3 Type:Convolution #blobs=2
I0815 20:55:05.537042 20241 net.cpp:1095] Copying source layer ctx_conv3/bn Type:BatchNorm #blobs=5
I0815 20:55:05.537194 20241 net.cpp:1095] Copying source layer ctx_conv3/relu Type:ReLU #blobs=0
I0815 20:55:05.537199 20241 net.cpp:1095] Copying source layer ctx_conv4 Type:Convolution #blobs=2
I0815 20:55:05.537216 20241 net.cpp:1095] Copying source layer ctx_conv4/bn Type:BatchNorm #blobs=5
I0815 20:55:05.537364 20241 net.cpp:1095] Copying source layer ctx_conv4/relu Type:ReLU #blobs=0
I0815 20:55:05.537369 20241 net.cpp:1095] Copying source layer ctx_final Type:Convolution #blobs=2
I0815 20:55:05.537379 20241 net.cpp:1095] Copying source layer ctx_final/relu Type:ReLU #blobs=0
I0815 20:55:05.537380 20241 net.cpp:1095] Copying source layer out_deconv_final_up2 Type:Deconvolution #blobs=1
I0815 20:55:05.537385 20241 net.cpp:1095] Copying source layer out_deconv_final_up4 Type:Deconvolution #blobs=1
I0815 20:55:05.537390 20241 net.cpp:1095] Copying source layer out_deconv_final_up8 Type:Deconvolution #blobs=1
I0815 20:55:05.537395 20241 net.cpp:1095] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0815 20:55:05.540998 20241 net.cpp:1095] Copying source layer data Type:ImageLabelData #blobs=0
I0815 20:55:05.541015 20241 net.cpp:1095] Copying source layer data/bias Type:Bias #blobs=1
I0815 20:55:05.541039 20241 net.cpp:1095] Copying source layer conv1a Type:Convolution #blobs=2
I0815 20:55:05.541051 20241 net.cpp:1095] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0815 20:55:05.541577 20241 net.cpp:1095] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0815 20:55:05.541584 20241 net.cpp:1095] Copying source layer conv1b Type:Convolution #blobs=2
I0815 20:55:05.541594 20241 net.cpp:1095] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0815 20:55:05.541976 20241 net.cpp:1095] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0815 20:55:05.541982 20241 net.cpp:1095] Copying source layer pool1 Type:Pooling #blobs=0
I0815 20:55:05.541985 20241 net.cpp:1095] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0815 20:55:05.542001 20241 net.cpp:1095] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0815 20:55:05.542387 20241 net.cpp:1095] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0815 20:55:05.542393 20241 net.cpp:1095] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0815 20:55:05.542405 20241 net.cpp:1095] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0815 20:55:05.542784 20241 net.cpp:1095] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0815 20:55:05.542790 20241 net.cpp:1095] Copying source layer pool2 Type:Pooling #blobs=0
I0815 20:55:05.542793 20241 net.cpp:1095] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0815 20:55:05.542831 20241 net.cpp:1095] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0815 20:55:05.543202 20241 net.cpp:1095] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0815 20:55:05.543208 20241 net.cpp:1095] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0815 20:55:05.543231 20241 net.cpp:1095] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0815 20:55:05.543588 20241 net.cpp:1095] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0815 20:55:05.543594 20241 net.cpp:1095] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I0815 20:55:05.543596 20241 net.cpp:1095] Copying source layer pool3 Type:Pooling #blobs=0
I0815 20:55:05.543599 20241 net.cpp:1095] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0815 20:55:05.543710 20241 net.cpp:1095] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0815 20:55:05.544073 20241 net.cpp:1095] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0815 20:55:05.544080 20241 net.cpp:1095] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0815 20:55:05.544159 20241 net.cpp:1095] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0815 20:55:05.544523 20241 net.cpp:1095] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0815 20:55:05.544528 20241 net.cpp:1095] Copying source layer pool4 Type:Pooling #blobs=0
I0815 20:55:05.544530 20241 net.cpp:1095] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0815 20:55:05.544893 20241 net.cpp:1095] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0815 20:55:05.545245 20241 net.cpp:1095] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0815 20:55:05.545251 20241 net.cpp:1095] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0815 20:55:05.545429 20241 net.cpp:1095] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0815 20:55:05.545795 20241 net.cpp:1095] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0815 20:55:05.545801 20241 net.cpp:1095] Copying source layer out5a Type:Convolution #blobs=2
I0815 20:55:05.545850 20241 net.cpp:1095] Copying source layer out5a/bn Type:BatchNorm #blobs=5
I0815 20:55:05.546017 20241 net.cpp:1095] Copying source layer out5a/relu Type:ReLU #blobs=0
I0815 20:55:05.546022 20241 net.cpp:1095] Copying source layer out5a_up2 Type:Deconvolution #blobs=1
I0815 20:55:05.546028 20241 net.cpp:1095] Copying source layer out3a Type:Convolution #blobs=2
I0815 20:55:05.546047 20241 net.cpp:1095] Copying source layer out3a/bn Type:BatchNorm #blobs=5
I0815 20:55:05.546198 20241 net.cpp:1095] Copying source layer out3a/relu Type:ReLU #blobs=0
I0815 20:55:05.546205 20241 net.cpp:1095] Copying source layer out3_out5_combined Type:Eltwise #blobs=0
I0815 20:55:05.546207 20241 net.cpp:1095] Copying source layer ctx_conv1 Type:Convolution #blobs=2
I0815 20:55:05.546232 20241 net.cpp:1095] Copying source layer ctx_conv1/bn Type:BatchNorm #blobs=5
I0815 20:55:05.546383 20241 net.cpp:1095] Copying source layer ctx_conv1/relu Type:ReLU #blobs=0
I0815 20:55:05.546388 20241 net.cpp:1095] Copying source layer ctx_conv2 Type:Convolution #blobs=2
I0815 20:55:05.546413 20241 net.cpp:1095] Copying source layer ctx_conv2/bn Type:BatchNorm #blobs=5
I0815 20:55:05.546589 20241 net.cpp:1095] Copying source layer ctx_conv2/relu Type:ReLU #blobs=0
I0815 20:55:05.546596 20241 net.cpp:1095] Copying source layer ctx_conv3 Type:Convolution #blobs=2
I0815 20:55:05.546617 20241 net.cpp:1095] Copying source layer ctx_conv3/bn Type:BatchNorm #blobs=5
I0815 20:55:05.546768 20241 net.cpp:1095] Copying source layer ctx_conv3/relu Type:ReLU #blobs=0
I0815 20:55:05.546774 20241 net.cpp:1095] Copying source layer ctx_conv4 Type:Convolution #blobs=2
I0815 20:55:05.546797 20241 net.cpp:1095] Copying source layer ctx_conv4/bn Type:BatchNorm #blobs=5
I0815 20:55:05.546964 20241 net.cpp:1095] Copying source layer ctx_conv4/relu Type:ReLU #blobs=0
I0815 20:55:05.546972 20241 net.cpp:1095] Copying source layer ctx_final Type:Convolution #blobs=2
I0815 20:55:05.546980 20241 net.cpp:1095] Copying source layer ctx_final/relu Type:ReLU #blobs=0
I0815 20:55:05.546984 20241 net.cpp:1095] Copying source layer out_deconv_final_up2 Type:Deconvolution #blobs=1
I0815 20:55:05.546988 20241 net.cpp:1095] Copying source layer out_deconv_final_up4 Type:Deconvolution #blobs=1
I0815 20:55:05.546994 20241 net.cpp:1095] Copying source layer out_deconv_final_up8 Type:Deconvolution #blobs=1
I0815 20:55:05.547000 20241 net.cpp:1095] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0815 20:55:05.547106 20241 parallel.cpp:106] [0 - 0] P2pSync adding callback
I0815 20:55:05.547112 20241 parallel.cpp:106] [1 - 1] P2pSync adding callback
I0815 20:55:05.547114 20241 parallel.cpp:106] [2 - 2] P2pSync adding callback
I0815 20:55:05.547116 20241 parallel.cpp:59] Starting Optimization
I0815 20:55:05.547118 20241 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 20:55:05.547147 20241 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 20:55:05.547159 20241 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 20:55:05.547842 20340 device_alternate.hpp:116] NVML initialized on thread 135814615820032
I0815 20:55:05.568855 20340 common.cpp:583] NVML succeeded to set CPU affinity on device 0
I0815 20:55:05.568903 20341 device_alternate.hpp:116] NVML initialized on thread 135814607427328
I0815 20:55:05.569682 20341 common.cpp:583] NVML succeeded to set CPU affinity on device 1
I0815 20:55:05.569723 20342 device_alternate.hpp:116] NVML initialized on thread 135814599034624
I0815 20:55:05.570462 20342 common.cpp:583] NVML succeeded to set CPU affinity on device 2
I0815 20:55:05.574446 20341 solver.cpp:42] Solver data type: FLOAT
W0815 20:55:05.575291 20341 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0815 20:55:05.575400 20341 net.cpp:104] Using FLOAT as default forward math type
I0815 20:55:05.575405 20341 net.cpp:110] Using FLOAT as default backward math type
I0815 20:55:05.575435 20341 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0815 20:55:05.575443 20341 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 20:55:05.578485 20342 solver.cpp:42] Solver data type: FLOAT
W0815 20:55:05.579005 20342 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0815 20:55:05.579107 20342 net.cpp:104] Using FLOAT as default forward math type
I0815 20:55:05.579113 20342 net.cpp:110] Using FLOAT as default backward math type
I0815 20:55:05.579138 20342 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0815 20:55:05.579144 20342 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 20:55:05.579170 20343 db_lmdb.cpp:24] Opened lmdb data/train-image-lmdb
I0815 20:55:05.579887 20344 db_lmdb.cpp:24] Opened lmdb data/train-image-lmdb
I0815 20:55:05.583672 20341 data_layer.cpp:185] [1] ReshapePrefetch 6, 3, 640, 640
I0815 20:55:05.584003 20342 data_layer.cpp:185] [2] ReshapePrefetch 6, 3, 640, 640
I0815 20:55:05.584041 20341 data_layer.cpp:209] [1] Output data size: 6, 3, 640, 640
I0815 20:55:05.584051 20341 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 20:55:05.584065 20342 data_layer.cpp:209] [2] Output data size: 6, 3, 640, 640
I0815 20:55:05.584071 20342 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 20:55:05.584262 20342 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0815 20:55:05.584262 20341 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0815 20:55:05.584278 20341 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 20:55:05.584280 20342 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 20:55:05.585237 20346 data_layer.cpp:97] [1] Parser threads: 1
I0815 20:55:05.585268 20346 data_layer.cpp:99] [1] Transformer threads: 1
I0815 20:55:05.590941 20345 data_layer.cpp:97] [2] Parser threads: 1
I0815 20:55:05.591079 20345 data_layer.cpp:99] [2] Transformer threads: 1
I0815 20:55:05.597431 20347 db_lmdb.cpp:24] Opened lmdb data/train-label-lmdb
I0815 20:55:05.599140 20348 db_lmdb.cpp:24] Opened lmdb data/train-label-lmdb
I0815 20:55:05.599731 20341 data_layer.cpp:185] [1] ReshapePrefetch 6, 1, 640, 640
I0815 20:55:05.599917 20341 data_layer.cpp:209] [1] Output data size: 6, 1, 640, 640
I0815 20:55:05.599934 20341 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 20:55:05.603819 20349 data_layer.cpp:97] [1] Parser threads: 1
I0815 20:55:05.603853 20349 data_layer.cpp:99] [1] Transformer threads: 1
I0815 20:55:05.607861 20342 data_layer.cpp:185] [2] ReshapePrefetch 6, 1, 640, 640
I0815 20:55:05.608286 20342 data_layer.cpp:209] [2] Output data size: 6, 1, 640, 640
I0815 20:55:05.608410 20342 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 20:55:05.610231 20346 blocking_queue.cpp:40] Waiting for datum
I0815 20:55:05.620275 20350 data_layer.cpp:97] [2] Parser threads: 1
I0815 20:55:05.620321 20350 data_layer.cpp:99] [2] Transformer threads: 1
I0815 20:55:06.116806 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 7.99G, req 0G)
I0815 20:55:06.166353 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.82G, req 0G)
I0815 20:55:06.176913 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 7.99G, req 0G)
I0815 20:55:06.208263 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.7G, req 0G)
I0815 20:55:06.226871 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.83G, req 0G)
I0815 20:55:06.232194 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.62G, req 0G)
I0815 20:55:06.255568 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 7.56G, req 0G)
I0815 20:55:06.267984 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.51G, req 0G)
I0815 20:55:06.270162 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.7G, req 0G)
I0815 20:55:06.295128 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.62G, req 0G)
I0815 20:55:06.299576 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.48G, req 0G)
I0815 20:55:06.311136 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.46G, req 0G)
I0815 20:55:06.319959 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 7.56G, req 0G)
I0815 20:55:06.332059 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.52G, req 0G)
I0815 20:55:06.360409 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.48G, req 0G)
I0815 20:55:06.371050 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 7.39G, req 0G)
I0815 20:55:06.372742 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.46G, req 0G)
I0815 20:55:06.389614 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 7.34G, req 0G)
I0815 20:55:06.410089 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 7.31G, req 0G)
I0815 20:55:06.413079 20341 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/l1reg/test.prototxt
W0815 20:55:06.413146 20341 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0815 20:55:06.413257 20341 net.cpp:104] Using FLOAT as default forward math type
I0815 20:55:06.413261 20341 net.cpp:110] Using FLOAT as default backward math type
I0815 20:55:06.413282 20341 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0815 20:55:06.413287 20341 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 20:55:06.413985 20385 db_lmdb.cpp:24] Opened lmdb data/val-image-lmdb
I0815 20:55:06.416313 20341 data_layer.cpp:185] (1) ReshapePrefetch 2, 3, 640, 640
I0815 20:55:06.416709 20341 data_layer.cpp:209] (1) Output data size: 2, 3, 640, 640
I0815 20:55:06.416723 20341 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 20:55:06.416842 20341 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0815 20:55:06.416857 20341 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 20:55:06.417670 20386 data_layer.cpp:97] (1) Parser threads: 1
I0815 20:55:06.417685 20386 data_layer.cpp:99] (1) Transformer threads: 1
I0815 20:55:06.419996 20387 db_lmdb.cpp:24] Opened lmdb data/val-label-lmdb
I0815 20:55:06.421015 20341 data_layer.cpp:185] (1) ReshapePrefetch 2, 1, 640, 640
I0815 20:55:06.421188 20341 data_layer.cpp:209] (1) Output data size: 2, 1, 640, 640
I0815 20:55:06.421223 20341 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 20:55:06.422780 20388 data_layer.cpp:97] (1) Parser threads: 1
I0815 20:55:06.422796 20388 data_layer.cpp:99] (1) Transformer threads: 1
I0815 20:55:06.433030 20341 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.18G, req 0G)
I0815 20:55:06.441244 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 7.39G, req 0G)
I0815 20:55:06.450827 20341 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.15G, req 0G)
I0815 20:55:06.458376 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 7.34G, req 0G)
I0815 20:55:06.461164 20341 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.13G, req 0G)
I0815 20:55:06.470839 20341 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.11G, req 0G)
I0815 20:55:06.478215 20341 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.1G, req 0G)
I0815 20:55:06.483044 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 7.32G, req 0G)
I0815 20:55:06.484740 20341 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.09G, req 0G)
I0815 20:55:06.487237 20342 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/l1reg/test.prototxt
W0815 20:55:06.487371 20342 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0815 20:55:06.487560 20342 net.cpp:104] Using FLOAT as default forward math type
I0815 20:55:06.487565 20342 net.cpp:110] Using FLOAT as default backward math type
I0815 20:55:06.487604 20342 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0815 20:55:06.487615 20342 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 20:55:06.488385 20389 db_lmdb.cpp:24] Opened lmdb data/val-image-lmdb
I0815 20:55:06.489799 20342 data_layer.cpp:185] (2) ReshapePrefetch 2, 3, 640, 640
I0815 20:55:06.489888 20342 data_layer.cpp:209] (2) Output data size: 2, 3, 640, 640
I0815 20:55:06.489895 20342 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 20:55:06.489936 20342 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0815 20:55:06.489945 20342 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 20:55:06.490726 20390 data_layer.cpp:97] (2) Parser threads: 1
I0815 20:55:06.490741 20390 data_layer.cpp:99] (2) Transformer threads: 1
I0815 20:55:06.493162 20391 db_lmdb.cpp:24] Opened lmdb data/val-label-lmdb
I0815 20:55:06.494156 20342 data_layer.cpp:185] (2) ReshapePrefetch 2, 1, 640, 640
I0815 20:55:06.494273 20342 data_layer.cpp:209] (2) Output data size: 2, 1, 640, 640
I0815 20:55:06.494282 20342 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 20:55:06.495847 20392 data_layer.cpp:97] (2) Parser threads: 1
I0815 20:55:06.495859 20392 data_layer.cpp:99] (2) Transformer threads: 1
I0815 20:55:06.502408 20341 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.08G, req 0G)
I0815 20:55:06.506176 20342 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.18G, req 0G)
I0815 20:55:06.512889 20341 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.08G, req 0G)
I0815 20:55:06.523638 20342 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.15G, req 0G)
I0815 20:55:06.534602 20342 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.13G, req 0G)
I0815 20:55:06.543901 20342 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.12G, req 0G)
I0815 20:55:06.553189 20342 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.1G, req 0G)
I0815 20:55:06.560873 20342 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.09G, req 0G)
I0815 20:55:06.568480 20341 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 7.07G, req 0G)
I0815 20:55:06.577419 20341 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 7.05G, req 0G)
I0815 20:55:06.577932 20342 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.09G, req 0G)
I0815 20:55:06.588953 20342 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.08G, req 0G)
I0815 20:55:06.593104 20341 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 7.05G, req 0G)
I0815 20:55:06.595505 20341 solver.cpp:56] Solver scaffolding done.
I0815 20:55:06.654067 20342 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 7.07G, req 0G)
I0815 20:55:06.661461 20342 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 7.06G, req 0G)
I0815 20:55:06.676239 20342 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 7.05G, req 0G)
I0815 20:55:06.679085 20342 solver.cpp:56] Solver scaffolding done.
I0815 20:55:06.745313 20341 parallel.cpp:161] [1 - 1] P2pSync adding callback
I0815 20:55:06.745339 20340 parallel.cpp:161] [0 - 0] P2pSync adding callback
I0815 20:55:06.745362 20342 parallel.cpp:161] [2 - 2] P2pSync adding callback
I0815 20:55:06.927949 20342 solver.cpp:438] Solving jsegnet21v2_train
I0815 20:55:06.927966 20342 solver.cpp:439] Learning Rate Policy: multistep
I0815 20:55:06.927975 20340 solver.cpp:438] Solving jsegnet21v2_train
I0815 20:55:06.927975 20341 solver.cpp:438] Solving jsegnet21v2_train
I0815 20:55:06.927985 20340 solver.cpp:439] Learning Rate Policy: multistep
I0815 20:55:06.928000 20341 solver.cpp:439] Learning Rate Policy: multistep
I0815 20:55:06.941558 20341 solver.cpp:227] Starting Optimization on GPU 1
I0815 20:55:06.941561 20342 solver.cpp:227] Starting Optimization on GPU 2
I0815 20:55:06.941587 20340 solver.cpp:227] Starting Optimization on GPU 0
I0815 20:55:06.941751 20340 solver.cpp:509] Iteration 0, Testing net (#0)
I0815 20:55:06.941779 20393 device_alternate.hpp:116] NVML initialized on thread 127814439167744
I0815 20:55:06.941794 20393 common.cpp:583] NVML succeeded to set CPU affinity on device 2
I0815 20:55:06.941803 20394 device_alternate.hpp:116] NVML initialized on thread 127814447560448
I0815 20:55:06.941814 20394 common.cpp:583] NVML succeeded to set CPU affinity on device 1
I0815 20:55:06.941889 20395 device_alternate.hpp:116] NVML initialized on thread 127814430775040
I0815 20:55:06.941905 20395 common.cpp:583] NVML succeeded to set CPU affinity on device 0
I0815 20:55:06.954310 20342 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 6.96G, req 0G)
I0815 20:55:06.959652 20341 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 6.95G, req 0G)
I0815 20:55:06.990298 20342 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 6.9G, req 0G)
I0815 20:55:07.001977 20341 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 6.9G, req 0G)
I0815 20:55:07.012328 20342 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 6.84G, req 0G)
I0815 20:55:07.017201 20341 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 6.84G, req 0G)
I0815 20:55:07.026374 20342 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 6.81G, req 0G)
I0815 20:55:07.027462 20341 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 6.81G, req 0G)
I0815 20:55:07.031258 20340 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.01G/1 1  (limit 6.88G, req 0G)
I0815 20:55:07.034688 20342 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 6.78G, req 0G)
I0815 20:55:07.037163 20341 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 6.77G, req 0G)
I0815 20:55:07.041093 20342 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 6.76G, req 0G)
I0815 20:55:07.043346 20341 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 6.76G, req 0G)
I0815 20:55:07.051816 20340 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 6.82G, req 0G)
I0815 20:55:07.054707 20342 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.74G, req 0G)
I0815 20:55:07.056615 20341 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.74G, req 0G)
I0815 20:55:07.059545 20342 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.74G, req 0G)
I0815 20:55:07.060940 20341 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.73G, req 0G)
I0815 20:55:07.065738 20340 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 6.75G, req 0G)
I0815 20:55:07.074611 20340 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 6.73G, req 0G)
I0815 20:55:07.084028 20340 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 6.69G, req 0G)
I0815 20:55:07.086625 20342 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.58G, req 0G)
I0815 20:55:07.088049 20341 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.58G, req 0G)
I0815 20:55:07.090801 20340 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 6.67G, req 0G)
I0815 20:55:07.092324 20342 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.57G, req 0G)
I0815 20:55:07.093493 20341 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.57G, req 0G)
I0815 20:55:07.099001 20340 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.66G, req 0G)
I0815 20:55:07.104575 20340 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.65G, req 0G)
I0815 20:55:07.115103 20342 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.48G, req 0G)
I0815 20:55:07.116997 20341 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.48G, req 0G)
I0815 20:55:07.130798 20340 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.5G, req 0G)
I0815 20:55:07.139092 20340 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.49G, req 0G)
I0815 20:55:07.158143 20340 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.39G, req 0G)
I0815 20:55:07.265287 20340 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.912905
I0815 20:55:07.265306 20340 solver.cpp:594]     Test net output #1: accuracy/top5 = 1
I0815 20:55:07.265313 20340 solver.cpp:594]     Test net output #2: loss = 0.207345 (* 1 = 0.207345 loss)
I0815 20:55:07.265321 20340 solver.cpp:254] [MultiGPU] Initial Test completed
I0815 20:55:07.375529 20340 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 6.19G, req 0G)
I0815 20:55:07.382421 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 6.27G, req 0G)
I0815 20:55:07.382601 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 6.28G, req 0G)
I0815 20:55:07.427453 20340 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 6.03G, req 0G)
I0815 20:55:07.437292 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 6.12G, req 0G)
I0815 20:55:07.437455 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 6.11G, req 0G)
I0815 20:55:07.470415 20340 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 1 4 3  (limit 5.85G, req 0G)
I0815 20:55:07.485597 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 5.93G, req 0G)
I0815 20:55:07.486256 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 5.94G, req 0G)
I0815 20:55:07.492489 20340 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 5.77G, req 0G)
I0815 20:55:07.509840 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 5.85G, req 0G)
I0815 20:55:07.510592 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 5.86G, req 0G)
I0815 20:55:07.514379 20340 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 5.68G, req 0G)
I0815 20:55:07.526824 20340 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 5.64G, req 0G)
I0815 20:55:07.535346 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 1  (limit 5.76G, req 0G)
I0815 20:55:07.537411 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 5.77G, req 0G)
I0815 20:55:07.547621 20340 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 5.61G, req 0G)
I0815 20:55:07.548118 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 5.72G, req 0G)
I0815 20:55:07.552491 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 5.72G, req 0G)
I0815 20:55:07.555820 20340 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 5.59G, req 0G)
I0815 20:55:07.571069 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 5.69G, req 0G)
I0815 20:55:07.575003 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 5.7G, req 0G)
I0815 20:55:07.579576 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 5.67G, req 0G)
I0815 20:55:07.584277 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 5.68G, req 0G)
I0815 20:55:07.602763 20340 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 5.32G, req 0G)
I0815 20:55:07.616556 20340 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 5.3G, req 0G)
I0815 20:55:07.629063 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 5.4G, req 0G)
I0815 20:55:07.634802 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 5.41G, req 0G)
I0815 20:55:07.646373 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 5.38G, req 0G)
I0815 20:55:07.646746 20340 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 5.14G, req 0G)
I0815 20:55:07.649763 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 5.39G, req 0G)
I0815 20:55:07.691057 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 5.23G, req 0G)
I0815 20:55:07.693806 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 5.23G, req 0G)
I0815 20:55:07.879531 20340 solver.cpp:317] Iteration 0 (0.614164 s), loss = 0.0996983
I0815 20:55:07.879551 20340 solver.cpp:334]     Train net output #0: loss = 0.0996983 (* 1 = 0.0996983 loss)
I0815 20:55:07.879557 20340 sgd_solver.cpp:136] Iteration 0, lr = 1e-05, m = 0.9
I0815 20:55:08.085065 20340 solver.cpp:317] Iteration 1 (0.205519 s), loss = 0.114394
I0815 20:55:08.085094 20340 solver.cpp:334]     Train net output #0: loss = 0.114394 (* 1 = 0.114394 loss)
I0815 20:55:08.173406 20340 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1a' with space 1.29G/1 1 0 3  (limit 2.99G, req 0G)
I0815 20:55:08.182677 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1a' with space 1.29G/1 1 0 0  (limit 3.08G, req 0G)
I0815 20:55:08.183941 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1a' with space 1.29G/1 1 0 3  (limit 3.08G, req 0G)
I0815 20:55:08.234228 20340 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1b' with space 2.57G/2 6 4 3  (limit 1.71G, req 0G)
I0815 20:55:08.246400 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0G)
I0815 20:55:08.249250 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0G)
I0815 20:55:08.359125 20340 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 2.57G/1 6 4 3  (limit 1.71G, req 0G)
I0815 20:55:08.379487 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 2.57G/1 6 4 3  (limit 1.79G, req 0G)
I0815 20:55:08.383771 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 2.57G/1 6 4 3  (limit 1.79G, req 0G)
I0815 20:55:08.397979 20340 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 2.57G/2 6 4 3  (limit 1.71G, req 0G)
I0815 20:55:08.422083 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0G)
I0815 20:55:08.426616 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0G)
I0815 20:55:08.491181 20340 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 2.57G/1 6 4 5  (limit 1.71G, req 0.07G)
I0815 20:55:08.512953 20340 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 2.57G/2 6 4 3  (limit 1.71G, req 0.07G)
I0815 20:55:08.521044 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 2.57G/1 6 4 5  (limit 1.79G, req 0.07G)
I0815 20:55:08.526093 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 2.57G/1 6 4 5  (limit 1.79G, req 0.07G)
I0815 20:55:08.544591 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0.07G)
I0815 20:55:08.550048 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0.07G)
I0815 20:55:08.575569 20340 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 2.57G/1 6 4 5  (limit 1.71G, req 0.07G)
I0815 20:55:08.588523 20340 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 2.57G/2 6 4 3  (limit 1.71G, req 0.07G)
I0815 20:55:08.614038 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 2.57G/1 6 4 5  (limit 1.79G, req 0.07G)
I0815 20:55:08.619843 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 2.57G/1 6 4 5  (limit 1.79G, req 0.07G)
I0815 20:55:08.627851 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0.07G)
I0815 20:55:08.633585 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0.07G)
I0815 20:55:08.638731 20340 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'out3a' with space 2.57G/2 6 4 3  (limit 1.71G, req 0.07G)
I0815 20:55:08.681958 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'out3a' with space 2.57G/2 6 4 3  (limit 1.79G, req 0.07G)
I0815 20:55:08.689226 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'out3a' with space 2.57G/2 6 4 3  (limit 1.79G, req 0.07G)
I0815 20:55:08.691432 20340 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 2.57G/1 6 4 3  (limit 1.71G, req 0.07G)
I0815 20:55:08.717326 20340 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 2.57G/1 6 1 3  (limit 1.71G, req 0.07G)
I0815 20:55:08.738029 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 2.57G/1 6 4 3  (limit 1.79G, req 0.07G)
I0815 20:55:08.745391 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 2.57G/1 6 4 3  (limit 1.79G, req 0.07G)
I0815 20:55:08.765918 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 2.57G/1 6 1 5  (limit 1.79G, req 0.07G)
I0815 20:55:08.772995 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 2.57G/1 6 1 5  (limit 1.79G, req 0.07G)
I0815 20:55:08.903450 20340 solver.cpp:317] Iteration 2 (0.818358 s), loss = 0.0715012
I0815 20:55:08.903475 20340 solver.cpp:334]     Train net output #0: loss = 0.0715012 (* 1 = 0.0715012 loss)
I0815 20:55:08.904083 20342 cudnn_conv_layer.cpp:292] [2] Layer 'conv1a' reallocating workspace: 2.57G -> 0.14G
I0815 20:55:08.911759 20341 cudnn_conv_layer.cpp:292] [1] Layer 'conv1a' reallocating workspace: 2.57G -> 0.14G
I0815 20:55:08.914017 20340 cudnn_conv_layer.cpp:292] [0] Layer 'conv1a' reallocating workspace: 2.57G -> 0.14G
I0815 20:55:27.785809 20340 solver.cpp:312] Iteration 100 (5.19017 iter/s, 18.8818s/98 iter), loss = 0.0744315
I0815 20:55:27.785831 20340 solver.cpp:334]     Train net output #0: loss = 0.0744315 (* 1 = 0.0744315 loss)
I0815 20:55:27.785837 20340 sgd_solver.cpp:136] Iteration 100, lr = 1e-05, m = 0.9
I0815 20:55:39.810969 20316 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 20:55:47.184540 20340 solver.cpp:312] Iteration 200 (5.15512 iter/s, 19.3982s/100 iter), loss = 0.114652
I0815 20:55:47.184564 20340 solver.cpp:334]     Train net output #0: loss = 0.114652 (* 1 = 0.114652 loss)
I0815 20:55:47.184571 20340 sgd_solver.cpp:136] Iteration 200, lr = 1e-05, m = 0.9
I0815 20:56:06.807045 20340 solver.cpp:312] Iteration 300 (5.09633 iter/s, 19.622s/100 iter), loss = 0.0806429
I0815 20:56:06.807075 20340 solver.cpp:334]     Train net output #0: loss = 0.0806429 (* 1 = 0.0806429 loss)
I0815 20:56:06.807082 20340 sgd_solver.cpp:136] Iteration 300, lr = 1e-05, m = 0.9
I0815 20:56:12.025400 20343 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 20:56:26.173642 20340 solver.cpp:312] Iteration 400 (5.16367 iter/s, 19.3661s/100 iter), loss = 0.131283
I0815 20:56:26.173665 20340 solver.cpp:334]     Train net output #0: loss = 0.131283 (* 1 = 0.131283 loss)
I0815 20:56:26.173669 20340 sgd_solver.cpp:136] Iteration 400, lr = 1e-05, m = 0.9
I0815 20:56:45.718008 20340 solver.cpp:312] Iteration 500 (5.1167 iter/s, 19.5438s/100 iter), loss = 0.0862497
I0815 20:56:45.718065 20340 solver.cpp:334]     Train net output #0: loss = 0.0862497 (* 1 = 0.0862497 loss)
I0815 20:56:45.718071 20340 sgd_solver.cpp:136] Iteration 500, lr = 1e-05, m = 0.9
I0815 20:57:04.919220 20340 solver.cpp:312] Iteration 600 (5.20815 iter/s, 19.2007s/100 iter), loss = 0.0724129
I0815 20:57:04.919242 20340 solver.cpp:334]     Train net output #0: loss = 0.0724129 (* 1 = 0.0724129 loss)
I0815 20:57:04.919247 20340 sgd_solver.cpp:136] Iteration 600, lr = 1e-05, m = 0.9
I0815 20:57:16.190793 20348 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 20:57:24.244139 20340 solver.cpp:312] Iteration 700 (5.17481 iter/s, 19.3244s/100 iter), loss = 0.0915192
I0815 20:57:24.244165 20340 solver.cpp:334]     Train net output #0: loss = 0.0915191 (* 1 = 0.0915191 loss)
I0815 20:57:24.244171 20340 sgd_solver.cpp:136] Iteration 700, lr = 1e-05, m = 0.9
I0815 20:57:43.718833 20340 solver.cpp:312] Iteration 800 (5.13501 iter/s, 19.4742s/100 iter), loss = 0.0979365
I0815 20:57:43.718859 20340 solver.cpp:334]     Train net output #0: loss = 0.0979365 (* 1 = 0.0979365 loss)
I0815 20:57:43.718864 20340 sgd_solver.cpp:136] Iteration 800, lr = 1e-05, m = 0.9
I0815 20:58:03.339042 20340 solver.cpp:312] Iteration 900 (5.09693 iter/s, 19.6197s/100 iter), loss = 0.0786026
I0815 20:58:03.339159 20340 solver.cpp:334]     Train net output #0: loss = 0.0786026 (* 1 = 0.0786026 loss)
I0815 20:58:03.339167 20340 sgd_solver.cpp:136] Iteration 900, lr = 1e-05, m = 0.9
I0815 20:58:20.497809 20316 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 20:58:22.843394 20340 solver.cpp:312] Iteration 1000 (5.1272 iter/s, 19.5038s/100 iter), loss = 0.0951657
I0815 20:58:22.843417 20340 solver.cpp:334]     Train net output #0: loss = 0.0951657 (* 1 = 0.0951657 loss)
I0815 20:58:22.843423 20340 sgd_solver.cpp:136] Iteration 1000, lr = 1e-05, m = 0.9
I0815 20:58:42.616354 20340 solver.cpp:312] Iteration 1100 (5.05755 iter/s, 19.7724s/100 iter), loss = 0.0658986
I0815 20:58:42.616405 20340 solver.cpp:334]     Train net output #0: loss = 0.0658986 (* 1 = 0.0658986 loss)
I0815 20:58:42.616412 20340 sgd_solver.cpp:136] Iteration 1100, lr = 1e-05, m = 0.9
I0815 20:58:52.853037 20344 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 20:59:01.904433 20340 solver.cpp:312] Iteration 1200 (5.18469 iter/s, 19.2875s/100 iter), loss = 0.0840935
I0815 20:59:01.904458 20340 solver.cpp:334]     Train net output #0: loss = 0.0840935 (* 1 = 0.0840935 loss)
I0815 20:59:01.904462 20340 sgd_solver.cpp:136] Iteration 1200, lr = 1e-05, m = 0.9
I0815 20:59:21.213376 20340 solver.cpp:312] Iteration 1300 (5.17909 iter/s, 19.3084s/100 iter), loss = 0.094995
I0815 20:59:21.213435 20340 solver.cpp:334]     Train net output #0: loss = 0.0949949 (* 1 = 0.0949949 loss)
I0815 20:59:21.213443 20340 sgd_solver.cpp:136] Iteration 1300, lr = 1e-05, m = 0.9
I0815 20:59:40.597743 20340 solver.cpp:312] Iteration 1400 (5.15894 iter/s, 19.3838s/100 iter), loss = 0.117895
I0815 20:59:40.597774 20340 solver.cpp:334]     Train net output #0: loss = 0.117895 (* 1 = 0.117895 loss)
I0815 20:59:40.597780 20340 sgd_solver.cpp:136] Iteration 1400, lr = 1e-05, m = 0.9
I0815 20:59:56.967591 20343 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 21:00:00.062680 20340 solver.cpp:312] Iteration 1500 (5.13758 iter/s, 19.4644s/100 iter), loss = 0.103425
I0815 21:00:00.062703 20340 solver.cpp:334]     Train net output #0: loss = 0.103425 (* 1 = 0.103425 loss)
I0815 21:00:00.062711 20340 sgd_solver.cpp:136] Iteration 1500, lr = 1e-05, m = 0.9
I0815 21:00:19.572815 20340 solver.cpp:312] Iteration 1600 (5.12568 iter/s, 19.5096s/100 iter), loss = 0.0738339
I0815 21:00:19.572840 20340 solver.cpp:334]     Train net output #0: loss = 0.0738338 (* 1 = 0.0738338 loss)
I0815 21:00:19.572844 20340 sgd_solver.cpp:136] Iteration 1600, lr = 1e-05, m = 0.9
I0815 21:00:39.184069 20340 solver.cpp:312] Iteration 1700 (5.09925 iter/s, 19.6107s/100 iter), loss = 0.101573
I0815 21:00:39.184123 20340 solver.cpp:334]     Train net output #0: loss = 0.101573 (* 1 = 0.101573 loss)
I0815 21:00:39.184135 20340 sgd_solver.cpp:136] Iteration 1700, lr = 1e-05, m = 0.9
I0815 21:00:58.712644 20340 solver.cpp:312] Iteration 1800 (5.12084 iter/s, 19.528s/100 iter), loss = 0.0650994
I0815 21:00:58.712673 20340 solver.cpp:334]     Train net output #0: loss = 0.0650993 (* 1 = 0.0650993 loss)
I0815 21:00:58.712679 20340 sgd_solver.cpp:136] Iteration 1800, lr = 1e-05, m = 0.9
I0815 21:01:01.710003 20347 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 21:01:18.367207 20340 solver.cpp:312] Iteration 1900 (5.08802 iter/s, 19.654s/100 iter), loss = 0.057876
I0815 21:01:18.367271 20340 solver.cpp:334]     Train net output #0: loss = 0.0578759 (* 1 = 0.0578759 loss)
I0815 21:01:18.367280 20340 sgd_solver.cpp:136] Iteration 1900, lr = 1e-05, m = 0.9
I0815 21:01:33.820943 20314 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 21:01:37.350908 20340 solver.cpp:509] Iteration 2000, Testing net (#0)
I0815 21:01:49.513990 20340 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.951425
I0815 21:01:49.514056 20340 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999695
I0815 21:01:49.514065 20340 solver.cpp:594]     Test net output #2: loss = 0.172523 (* 1 = 0.172523 loss)
I0815 21:01:49.514094 20340 solver.cpp:264] [MultiGPU] Tests completed in 12.1629s
I0815 21:01:49.706238 20340 solver.cpp:312] Iteration 2000 (3.191 iter/s, 31.3382s/100 iter), loss = 0.0721825
I0815 21:01:49.706262 20340 solver.cpp:334]     Train net output #0: loss = 0.0721824 (* 1 = 0.0721824 loss)
I0815 21:01:49.706269 20340 sgd_solver.cpp:136] Iteration 2000, lr = 1e-05, m = 0.9
I0815 21:02:09.114236 20340 solver.cpp:312] Iteration 2100 (5.15266 iter/s, 19.4075s/100 iter), loss = 0.0743474
I0815 21:02:09.114256 20340 solver.cpp:334]     Train net output #0: loss = 0.0743472 (* 1 = 0.0743472 loss)
I0815 21:02:09.114261 20340 sgd_solver.cpp:136] Iteration 2100, lr = 1e-05, m = 0.9
I0815 21:02:17.851065 20344 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 21:02:28.476832 20340 solver.cpp:312] Iteration 2200 (5.16474 iter/s, 19.3621s/100 iter), loss = 0.141381
I0815 21:02:28.476912 20340 solver.cpp:334]     Train net output #0: loss = 0.141381 (* 1 = 0.141381 loss)
I0815 21:02:28.476918 20340 sgd_solver.cpp:136] Iteration 2200, lr = 1e-05, m = 0.9
I0815 21:02:48.023267 20340 solver.cpp:312] Iteration 2300 (5.11616 iter/s, 19.5459s/100 iter), loss = 0.0734314
I0815 21:02:48.023288 20340 solver.cpp:334]     Train net output #0: loss = 0.0734312 (* 1 = 0.0734312 loss)
I0815 21:02:48.023291 20340 sgd_solver.cpp:136] Iteration 2300, lr = 1e-05, m = 0.9
I0815 21:03:07.396823 20340 solver.cpp:312] Iteration 2400 (5.16182 iter/s, 19.373s/100 iter), loss = 0.0668185
I0815 21:03:07.396906 20340 solver.cpp:334]     Train net output #0: loss = 0.0668184 (* 1 = 0.0668184 loss)
I0815 21:03:07.396914 20340 sgd_solver.cpp:136] Iteration 2400, lr = 1e-05, m = 0.9
I0815 21:03:22.203213 20344 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 21:03:26.845836 20340 solver.cpp:312] Iteration 2500 (5.14179 iter/s, 19.4485s/100 iter), loss = 0.0924095
I0815 21:03:26.845865 20340 solver.cpp:334]     Train net output #0: loss = 0.0924094 (* 1 = 0.0924094 loss)
I0815 21:03:26.845871 20340 sgd_solver.cpp:136] Iteration 2500, lr = 1e-05, m = 0.9
I0815 21:03:46.345978 20340 solver.cpp:312] Iteration 2600 (5.12831 iter/s, 19.4996s/100 iter), loss = 0.0842264
I0815 21:03:46.346060 20340 solver.cpp:334]     Train net output #0: loss = 0.0842263 (* 1 = 0.0842263 loss)
I0815 21:03:46.346073 20340 sgd_solver.cpp:136] Iteration 2600, lr = 1e-05, m = 0.9
I0815 21:03:54.447190 20344 data_reader.cpp:288] Starting prefetch of epoch 4
I0815 21:04:05.783952 20340 solver.cpp:312] Iteration 2700 (5.14471 iter/s, 19.4374s/100 iter), loss = 0.0704088
I0815 21:04:05.783977 20340 solver.cpp:334]     Train net output #0: loss = 0.0704086 (* 1 = 0.0704086 loss)
I0815 21:04:05.783983 20340 sgd_solver.cpp:136] Iteration 2700, lr = 1e-05, m = 0.9
I0815 21:04:24.779595 20340 solver.cpp:312] Iteration 2800 (5.26451 iter/s, 18.9951s/100 iter), loss = 0.0638916
I0815 21:04:24.779646 20340 solver.cpp:334]     Train net output #0: loss = 0.0638915 (* 1 = 0.0638915 loss)
I0815 21:04:24.779654 20340 sgd_solver.cpp:136] Iteration 2800, lr = 1e-05, m = 0.9
I0815 21:04:44.095306 20340 solver.cpp:312] Iteration 2900 (5.17728 iter/s, 19.3152s/100 iter), loss = 0.0920668
I0815 21:04:44.095330 20340 solver.cpp:334]     Train net output #0: loss = 0.0920667 (* 1 = 0.0920667 loss)
I0815 21:04:44.095338 20340 sgd_solver.cpp:136] Iteration 2900, lr = 1e-05, m = 0.9
I0815 21:04:57.798789 20314 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 21:05:03.358461 20340 solver.cpp:312] Iteration 3000 (5.1914 iter/s, 19.2626s/100 iter), loss = 0.104681
I0815 21:05:03.358489 20340 solver.cpp:334]     Train net output #0: loss = 0.104681 (* 1 = 0.104681 loss)
I0815 21:05:03.358496 20340 sgd_solver.cpp:136] Iteration 3000, lr = 1e-05, m = 0.9
I0815 21:05:22.978044 20340 solver.cpp:312] Iteration 3100 (5.09709 iter/s, 19.619s/100 iter), loss = 0.0455306
I0815 21:05:22.978065 20340 solver.cpp:334]     Train net output #0: loss = 0.0455305 (* 1 = 0.0455305 loss)
I0815 21:05:22.978070 20340 sgd_solver.cpp:136] Iteration 3100, lr = 1e-05, m = 0.9
I0815 21:05:42.476125 20340 solver.cpp:312] Iteration 3200 (5.12885 iter/s, 19.4975s/100 iter), loss = 0.169136
I0815 21:05:42.476187 20340 solver.cpp:334]     Train net output #0: loss = 0.169135 (* 1 = 0.169135 loss)
I0815 21:05:42.476192 20340 sgd_solver.cpp:136] Iteration 3200, lr = 1e-05, m = 0.9
I0815 21:06:01.859913 20340 solver.cpp:312] Iteration 3300 (5.15909 iter/s, 19.3833s/100 iter), loss = 0.0666914
I0815 21:06:01.859941 20340 solver.cpp:334]     Train net output #0: loss = 0.0666912 (* 1 = 0.0666912 loss)
I0815 21:06:01.859946 20340 sgd_solver.cpp:136] Iteration 3300, lr = 1e-05, m = 0.9
I0815 21:06:02.288660 20344 data_reader.cpp:288] Starting prefetch of epoch 5
I0815 21:06:21.409353 20340 solver.cpp:312] Iteration 3400 (5.11538 iter/s, 19.5489s/100 iter), loss = 0.0469009
I0815 21:06:21.409415 20340 solver.cpp:334]     Train net output #0: loss = 0.0469008 (* 1 = 0.0469008 loss)
I0815 21:06:21.409421 20340 sgd_solver.cpp:136] Iteration 3400, lr = 1e-05, m = 0.9
I0815 21:06:40.949355 20340 solver.cpp:312] Iteration 3500 (5.11785 iter/s, 19.5395s/100 iter), loss = 0.082622
I0815 21:06:40.949380 20340 solver.cpp:334]     Train net output #0: loss = 0.0826219 (* 1 = 0.0826219 loss)
I0815 21:06:40.949386 20340 sgd_solver.cpp:136] Iteration 3500, lr = 1e-05, m = 0.9
I0815 21:07:00.364210 20340 solver.cpp:312] Iteration 3600 (5.15084 iter/s, 19.4143s/100 iter), loss = 0.0767218
I0815 21:07:00.364293 20340 solver.cpp:334]     Train net output #0: loss = 0.0767217 (* 1 = 0.0767217 loss)
I0815 21:07:00.364300 20340 sgd_solver.cpp:136] Iteration 3600, lr = 1e-05, m = 0.9
I0815 21:07:06.820622 20344 data_reader.cpp:288] Starting prefetch of epoch 6
I0815 21:07:19.870383 20340 solver.cpp:312] Iteration 3700 (5.12672 iter/s, 19.5056s/100 iter), loss = 0.0821178
I0815 21:07:19.870405 20340 solver.cpp:334]     Train net output #0: loss = 0.0821177 (* 1 = 0.0821177 loss)
I0815 21:07:19.870409 20340 sgd_solver.cpp:136] Iteration 3700, lr = 1e-05, m = 0.9
I0815 21:07:38.910761 20343 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 21:07:39.269497 20340 solver.cpp:312] Iteration 3800 (5.15502 iter/s, 19.3986s/100 iter), loss = 0.0910058
I0815 21:07:39.269520 20340 solver.cpp:334]     Train net output #0: loss = 0.0910057 (* 1 = 0.0910057 loss)
I0815 21:07:39.269527 20340 sgd_solver.cpp:136] Iteration 3800, lr = 1e-05, m = 0.9
I0815 21:07:58.771150 20340 solver.cpp:312] Iteration 3900 (5.12791 iter/s, 19.5011s/100 iter), loss = 0.0697615
I0815 21:07:58.771175 20340 solver.cpp:334]     Train net output #0: loss = 0.0697614 (* 1 = 0.0697614 loss)
I0815 21:07:58.771181 20340 sgd_solver.cpp:136] Iteration 3900, lr = 1e-05, m = 0.9
I0815 21:08:17.858283 20340 solver.cpp:509] Iteration 4000, Testing net (#0)
I0815 21:08:21.569129 20387 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 21:08:30.276677 20340 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.950551
I0815 21:08:30.276697 20340 solver.cpp:594]     Test net output #1: accuracy/top5 = 1
I0815 21:08:30.276702 20340 solver.cpp:594]     Test net output #2: loss = 0.154649 (* 1 = 0.154649 loss)
I0815 21:08:30.276726 20340 solver.cpp:264] [MultiGPU] Tests completed in 12.4181s
I0815 21:08:30.473649 20340 solver.cpp:312] Iteration 4000 (3.15441 iter/s, 31.7016s/100 iter), loss = 0.0599244
I0815 21:08:30.473675 20340 solver.cpp:334]     Train net output #0: loss = 0.0599243 (* 1 = 0.0599243 loss)
I0815 21:08:30.473682 20340 sgd_solver.cpp:136] Iteration 4000, lr = 1e-05, m = 0.9
I0815 21:08:50.115857 20340 solver.cpp:312] Iteration 4100 (5.09122 iter/s, 19.6417s/100 iter), loss = 0.0955899
I0815 21:08:50.115926 20340 solver.cpp:334]     Train net output #0: loss = 0.0955898 (* 1 = 0.0955898 loss)
I0815 21:08:50.115933 20340 sgd_solver.cpp:136] Iteration 4100, lr = 1e-05, m = 0.9
I0815 21:08:55.483667 20316 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 21:09:09.510843 20340 solver.cpp:312] Iteration 4200 (5.15612 iter/s, 19.3944s/100 iter), loss = 0.0782703
I0815 21:09:09.510867 20340 solver.cpp:334]     Train net output #0: loss = 0.0782702 (* 1 = 0.0782702 loss)
I0815 21:09:09.510874 20340 sgd_solver.cpp:136] Iteration 4200, lr = 1e-05, m = 0.9
I0815 21:09:27.665464 20344 data_reader.cpp:288] Starting prefetch of epoch 7
I0815 21:09:28.819283 20340 solver.cpp:312] Iteration 4300 (5.17922 iter/s, 19.3079s/100 iter), loss = 0.0994151
I0815 21:09:28.819305 20340 solver.cpp:334]     Train net output #0: loss = 0.099415 (* 1 = 0.099415 loss)
I0815 21:09:28.819310 20340 sgd_solver.cpp:136] Iteration 4300, lr = 1e-05, m = 0.9
I0815 21:09:48.089741 20340 solver.cpp:312] Iteration 4400 (5.18943 iter/s, 19.2699s/100 iter), loss = 0.0585641
I0815 21:09:48.089768 20340 solver.cpp:334]     Train net output #0: loss = 0.058564 (* 1 = 0.058564 loss)
I0815 21:09:48.089776 20340 sgd_solver.cpp:136] Iteration 4400, lr = 1e-05, m = 0.9
I0815 21:10:07.548296 20340 solver.cpp:312] Iteration 4500 (5.13927 iter/s, 19.458s/100 iter), loss = 0.0499164
I0815 21:10:07.548344 20340 solver.cpp:334]     Train net output #0: loss = 0.0499162 (* 1 = 0.0499162 loss)
I0815 21:10:07.548352 20340 sgd_solver.cpp:136] Iteration 4500, lr = 1e-05, m = 0.9
I0815 21:10:26.849522 20340 solver.cpp:312] Iteration 4600 (5.18116 iter/s, 19.3007s/100 iter), loss = 0.111439
I0815 21:10:26.849547 20340 solver.cpp:334]     Train net output #0: loss = 0.111439 (* 1 = 0.111439 loss)
I0815 21:10:26.849551 20340 sgd_solver.cpp:136] Iteration 4600, lr = 1e-05, m = 0.9
I0815 21:10:31.547060 20316 data_reader.cpp:288] Starting prefetch of epoch 4
I0815 21:10:46.309079 20340 solver.cpp:312] Iteration 4700 (5.139 iter/s, 19.459s/100 iter), loss = 0.0744868
I0815 21:10:46.309159 20340 solver.cpp:334]     Train net output #0: loss = 0.0744867 (* 1 = 0.0744867 loss)
I0815 21:10:46.309166 20340 sgd_solver.cpp:136] Iteration 4700, lr = 1e-05, m = 0.9
I0815 21:11:05.603715 20340 solver.cpp:312] Iteration 4800 (5.18293 iter/s, 19.2941s/100 iter), loss = 0.0873141
I0815 21:11:05.603739 20340 solver.cpp:334]     Train net output #0: loss = 0.087314 (* 1 = 0.087314 loss)
I0815 21:11:05.603744 20340 sgd_solver.cpp:136] Iteration 4800, lr = 1e-05, m = 0.9
I0815 21:11:24.956303 20340 solver.cpp:312] Iteration 4900 (5.16741 iter/s, 19.3521s/100 iter), loss = 0.0928402
I0815 21:11:24.956357 20340 solver.cpp:334]     Train net output #0: loss = 0.0928401 (* 1 = 0.0928401 loss)
I0815 21:11:24.956362 20340 sgd_solver.cpp:136] Iteration 4900, lr = 1e-05, m = 0.9
I0815 21:11:35.660697 20347 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 21:11:44.431219 20340 solver.cpp:312] Iteration 5000 (5.13495 iter/s, 19.4744s/100 iter), loss = 0.0894408
I0815 21:11:44.431247 20340 solver.cpp:334]     Train net output #0: loss = 0.0894406 (* 1 = 0.0894406 loss)
I0815 21:11:44.431254 20340 sgd_solver.cpp:136] Iteration 5000, lr = 1e-05, m = 0.9
I0815 21:12:03.778939 20340 solver.cpp:312] Iteration 5100 (5.16871 iter/s, 19.3472s/100 iter), loss = 0.131269
I0815 21:12:03.778987 20340 solver.cpp:334]     Train net output #0: loss = 0.131268 (* 1 = 0.131268 loss)
I0815 21:12:03.778992 20340 sgd_solver.cpp:136] Iteration 5100, lr = 1e-05, m = 0.9
I0815 21:12:07.662277 20343 data_reader.cpp:288] Starting prefetch of epoch 4
I0815 21:12:23.216048 20340 solver.cpp:312] Iteration 5200 (5.14494 iter/s, 19.4366s/100 iter), loss = 0.0921541
I0815 21:12:23.216073 20340 solver.cpp:334]     Train net output #0: loss = 0.092154 (* 1 = 0.092154 loss)
I0815 21:12:23.216078 20340 sgd_solver.cpp:136] Iteration 5200, lr = 1e-05, m = 0.9
I0815 21:12:42.789566 20340 solver.cpp:312] Iteration 5300 (5.10908 iter/s, 19.573s/100 iter), loss = 0.055003
I0815 21:12:42.789635 20340 solver.cpp:334]     Train net output #0: loss = 0.0550028 (* 1 = 0.0550028 loss)
I0815 21:12:42.789643 20340 sgd_solver.cpp:136] Iteration 5300, lr = 1e-05, m = 0.9
I0815 21:13:02.127333 20340 solver.cpp:312] Iteration 5400 (5.17137 iter/s, 19.3372s/100 iter), loss = 0.0596323
I0815 21:13:02.127357 20340 solver.cpp:334]     Train net output #0: loss = 0.0596321 (* 1 = 0.0596321 loss)
I0815 21:13:02.127360 20340 sgd_solver.cpp:136] Iteration 5400, lr = 1e-05, m = 0.9
I0815 21:13:12.056370 20344 data_reader.cpp:288] Starting prefetch of epoch 8
I0815 21:13:21.520181 20340 solver.cpp:312] Iteration 5500 (5.15668 iter/s, 19.3923s/100 iter), loss = 0.0748437
I0815 21:13:21.520236 20340 solver.cpp:334]     Train net output #0: loss = 0.0748436 (* 1 = 0.0748436 loss)
I0815 21:13:21.520243 20340 sgd_solver.cpp:136] Iteration 5500, lr = 1e-05, m = 0.9
I0815 21:13:41.093893 20340 solver.cpp:312] Iteration 5600 (5.10903 iter/s, 19.5732s/100 iter), loss = 0.0395246
I0815 21:13:41.093919 20340 solver.cpp:334]     Train net output #0: loss = 0.0395244 (* 1 = 0.0395244 loss)
I0815 21:13:41.093925 20340 sgd_solver.cpp:136] Iteration 5600, lr = 1e-05, m = 0.9
I0815 21:14:00.504632 20340 solver.cpp:312] Iteration 5700 (5.15193 iter/s, 19.4102s/100 iter), loss = 0.126977
I0815 21:14:00.504684 20340 solver.cpp:334]     Train net output #0: loss = 0.126977 (* 1 = 0.126977 loss)
I0815 21:14:00.504689 20340 sgd_solver.cpp:136] Iteration 5700, lr = 1e-05, m = 0.9
I0815 21:14:16.295114 20347 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 21:14:20.037794 20340 solver.cpp:312] Iteration 5800 (5.11964 iter/s, 19.5326s/100 iter), loss = 0.0756217
I0815 21:14:20.037817 20340 solver.cpp:334]     Train net output #0: loss = 0.0756215 (* 1 = 0.0756215 loss)
I0815 21:14:20.037822 20340 sgd_solver.cpp:136] Iteration 5800, lr = 1e-05, m = 0.9
I0815 21:14:39.578085 20340 solver.cpp:312] Iteration 5900 (5.11777 iter/s, 19.5398s/100 iter), loss = 0.0763264
I0815 21:14:39.578136 20340 solver.cpp:334]     Train net output #0: loss = 0.0763262 (* 1 = 0.0763262 loss)
I0815 21:14:39.578142 20340 sgd_solver.cpp:136] Iteration 5900, lr = 1e-05, m = 0.9
I0815 21:14:48.560659 20314 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 21:14:58.900734 20340 solver.cpp:509] Iteration 6000, Testing net (#0)
I0815 21:15:10.856832 20340 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.950058
I0815 21:15:10.856887 20340 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.99956
I0815 21:15:10.856895 20340 solver.cpp:594]     Test net output #2: loss = 0.187184 (* 1 = 0.187184 loss)
I0815 21:15:10.856925 20340 solver.cpp:264] [MultiGPU] Tests completed in 11.9559s
I0815 21:15:11.070072 20340 solver.cpp:312] Iteration 6000 (3.1755 iter/s, 31.4911s/100 iter), loss = 0.106352
I0815 21:15:11.070096 20340 solver.cpp:334]     Train net output #0: loss = 0.106352 (* 1 = 0.106352 loss)
I0815 21:15:11.070102 20340 sgd_solver.cpp:136] Iteration 6000, lr = 1e-05, m = 0.9
I0815 21:15:30.490276 20340 solver.cpp:312] Iteration 6100 (5.14942 iter/s, 19.4197s/100 iter), loss = 0.0766106
I0815 21:15:30.490303 20340 solver.cpp:334]     Train net output #0: loss = 0.0766105 (* 1 = 0.0766105 loss)
I0815 21:15:30.490309 20340 sgd_solver.cpp:136] Iteration 6100, lr = 1e-05, m = 0.9
I0815 21:15:32.816047 20348 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 21:15:50.018755 20340 solver.cpp:312] Iteration 6200 (5.12087 iter/s, 19.5279s/100 iter), loss = 0.0864399
I0815 21:15:50.018805 20340 solver.cpp:334]     Train net output #0: loss = 0.0864398 (* 1 = 0.0864398 loss)
I0815 21:15:50.018810 20340 sgd_solver.cpp:136] Iteration 6200, lr = 1e-05, m = 0.9
I0815 21:16:09.358423 20340 solver.cpp:312] Iteration 6300 (5.17086 iter/s, 19.3391s/100 iter), loss = 0.0808005
I0815 21:16:09.358445 20340 solver.cpp:334]     Train net output #0: loss = 0.0808003 (* 1 = 0.0808003 loss)
I0815 21:16:09.358449 20340 sgd_solver.cpp:136] Iteration 6300, lr = 1e-05, m = 0.9
I0815 21:16:28.783330 20340 solver.cpp:312] Iteration 6400 (5.14817 iter/s, 19.4244s/100 iter), loss = 0.0938407
I0815 21:16:28.783404 20340 solver.cpp:334]     Train net output #0: loss = 0.0938406 (* 1 = 0.0938406 loss)
I0815 21:16:28.783411 20340 sgd_solver.cpp:136] Iteration 6400, lr = 1e-05, m = 0.9
I0815 21:16:36.943466 20316 data_reader.cpp:288] Starting prefetch of epoch 5
I0815 21:16:48.052723 20340 solver.cpp:312] Iteration 6500 (5.18972 iter/s, 19.2689s/100 iter), loss = 0.0890595
I0815 21:16:48.052744 20340 solver.cpp:334]     Train net output #0: loss = 0.0890594 (* 1 = 0.0890594 loss)
I0815 21:16:48.052750 20340 sgd_solver.cpp:136] Iteration 6500, lr = 1e-05, m = 0.9
I0815 21:17:07.677852 20340 solver.cpp:312] Iteration 6600 (5.09565 iter/s, 19.6246s/100 iter), loss = 0.087179
I0815 21:17:07.677906 20340 solver.cpp:334]     Train net output #0: loss = 0.0871789 (* 1 = 0.0871789 loss)
I0815 21:17:07.677911 20340 sgd_solver.cpp:136] Iteration 6600, lr = 1e-05, m = 0.9
I0815 21:17:09.248491 20343 data_reader.cpp:288] Starting prefetch of epoch 5
I0815 21:17:27.323156 20340 solver.cpp:312] Iteration 6700 (5.09042 iter/s, 19.6448s/100 iter), loss = 0.0934402
I0815 21:17:27.323179 20340 solver.cpp:334]     Train net output #0: loss = 0.0934401 (* 1 = 0.0934401 loss)
I0815 21:17:27.323184 20340 sgd_solver.cpp:136] Iteration 6700, lr = 1e-05, m = 0.9
I0815 21:17:46.874676 20340 solver.cpp:312] Iteration 6800 (5.11483 iter/s, 19.551s/100 iter), loss = 0.0503814
I0815 21:17:46.874758 20340 solver.cpp:334]     Train net output #0: loss = 0.0503813 (* 1 = 0.0503813 loss)
I0815 21:17:46.874765 20340 sgd_solver.cpp:136] Iteration 6800, lr = 1e-05, m = 0.9
I0815 21:18:06.299929 20340 solver.cpp:312] Iteration 6900 (5.14808 iter/s, 19.4247s/100 iter), loss = 0.100402
I0815 21:18:06.299948 20340 solver.cpp:334]     Train net output #0: loss = 0.100401 (* 1 = 0.100401 loss)
I0815 21:18:06.299953 20340 sgd_solver.cpp:136] Iteration 6900, lr = 1e-05, m = 0.9
I0815 21:18:13.806743 20348 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 21:18:26.096057 20340 solver.cpp:312] Iteration 7000 (5.05163 iter/s, 19.7956s/100 iter), loss = 0.106453
I0815 21:18:26.096103 20340 solver.cpp:334]     Train net output #0: loss = 0.106453 (* 1 = 0.106453 loss)
I0815 21:18:26.096110 20340 sgd_solver.cpp:136] Iteration 7000, lr = 1e-05, m = 0.9
I0815 21:18:45.513262 20340 solver.cpp:312] Iteration 7100 (5.15021 iter/s, 19.4167s/100 iter), loss = 0.0812609
I0815 21:18:45.513288 20340 solver.cpp:334]     Train net output #0: loss = 0.0812607 (* 1 = 0.0812607 loss)
I0815 21:18:45.513293 20340 sgd_solver.cpp:136] Iteration 7100, lr = 1e-05, m = 0.9
I0815 21:19:05.088687 20340 solver.cpp:312] Iteration 7200 (5.10859 iter/s, 19.5749s/100 iter), loss = 0.0669766
I0815 21:19:05.088737 20340 solver.cpp:334]     Train net output #0: loss = 0.0669765 (* 1 = 0.0669765 loss)
I0815 21:19:05.088744 20340 sgd_solver.cpp:136] Iteration 7200, lr = 1e-05, m = 0.9
I0815 21:19:18.349217 20344 data_reader.cpp:288] Starting prefetch of epoch 9
I0815 21:19:24.240247 20340 solver.cpp:312] Iteration 7300 (5.22165 iter/s, 19.151s/100 iter), loss = 0.0655575
I0815 21:19:24.240270 20340 solver.cpp:334]     Train net output #0: loss = 0.0655574 (* 1 = 0.0655574 loss)
I0815 21:19:24.240274 20340 sgd_solver.cpp:136] Iteration 7300, lr = 1e-05, m = 0.9
I0815 21:19:43.500478 20340 solver.cpp:312] Iteration 7400 (5.19219 iter/s, 19.2597s/100 iter), loss = 0.112614
I0815 21:19:43.500564 20340 solver.cpp:334]     Train net output #0: loss = 0.112614 (* 1 = 0.112614 loss)
I0815 21:19:43.500571 20340 sgd_solver.cpp:136] Iteration 7400, lr = 1e-05, m = 0.9
I0815 21:19:50.244405 20344 data_reader.cpp:288] Starting prefetch of epoch 10
I0815 21:20:03.213490 20340 solver.cpp:312] Iteration 7500 (5.07293 iter/s, 19.7125s/100 iter), loss = 0.106126
I0815 21:20:03.213511 20340 solver.cpp:334]     Train net output #0: loss = 0.106126 (* 1 = 0.106126 loss)
I0815 21:20:03.213516 20340 sgd_solver.cpp:136] Iteration 7500, lr = 1e-05, m = 0.9
I0815 21:20:22.714553 20340 solver.cpp:312] Iteration 7600 (5.12807 iter/s, 19.5005s/100 iter), loss = 0.0640639
I0815 21:20:22.714617 20340 solver.cpp:334]     Train net output #0: loss = 0.0640638 (* 1 = 0.0640638 loss)
I0815 21:20:22.714624 20340 sgd_solver.cpp:136] Iteration 7600, lr = 1e-05, m = 0.9
I0815 21:20:42.082278 20340 solver.cpp:312] Iteration 7700 (5.16337 iter/s, 19.3672s/100 iter), loss = 0.0706793
I0815 21:20:42.082345 20340 solver.cpp:334]     Train net output #0: loss = 0.0706792 (* 1 = 0.0706792 loss)
I0815 21:20:42.082365 20340 sgd_solver.cpp:136] Iteration 7700, lr = 1e-05, m = 0.9
I0815 21:20:54.809898 20314 data_reader.cpp:288] Starting prefetch of epoch 4
I0815 21:21:01.686642 20340 solver.cpp:312] Iteration 7800 (5.10105 iter/s, 19.6038s/100 iter), loss = 0.0994551
I0815 21:21:01.686667 20340 solver.cpp:334]     Train net output #0: loss = 0.099455 (* 1 = 0.099455 loss)
I0815 21:21:01.686672 20340 sgd_solver.cpp:136] Iteration 7800, lr = 1e-05, m = 0.9
I0815 21:21:21.376860 20340 solver.cpp:312] Iteration 7900 (5.0788 iter/s, 19.6897s/100 iter), loss = 0.0576541
I0815 21:21:21.376883 20340 solver.cpp:334]     Train net output #0: loss = 0.057654 (* 1 = 0.057654 loss)
I0815 21:21:21.376888 20340 sgd_solver.cpp:136] Iteration 7900, lr = 1e-05, m = 0.9
I0815 21:21:40.562413 20340 solver.cpp:509] Iteration 8000, Testing net (#0)
I0815 21:21:44.077246 20338 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 21:21:48.921495 20340 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 21:21:52.093289 20387 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 21:21:52.444936 20340 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.951278
I0815 21:21:52.444958 20340 solver.cpp:594]     Test net output #1: accuracy/top5 = 1
I0815 21:21:52.444963 20340 solver.cpp:594]     Test net output #2: loss = 0.149912 (* 1 = 0.149912 loss)
I0815 21:21:52.444990 20340 solver.cpp:264] [MultiGPU] Tests completed in 11.8823s
I0815 21:21:52.668609 20340 solver.cpp:312] Iteration 8000 (3.19582 iter/s, 31.2909s/100 iter), loss = 0.102859
I0815 21:21:52.668637 20340 solver.cpp:334]     Train net output #0: loss = 0.102859 (* 1 = 0.102859 loss)
I0815 21:21:52.668644 20340 sgd_solver.cpp:136] Iteration 8000, lr = 1e-05, m = 0.9
I0815 21:22:12.094183 20340 solver.cpp:312] Iteration 8100 (5.14799 iter/s, 19.425s/100 iter), loss = 0.072124
I0815 21:22:12.094230 20340 solver.cpp:334]     Train net output #0: loss = 0.0721239 (* 1 = 0.0721239 loss)
I0815 21:22:12.094235 20340 sgd_solver.cpp:136] Iteration 8100, lr = 1e-05, m = 0.9
I0815 21:22:31.926692 20340 solver.cpp:312] Iteration 8200 (5.04237 iter/s, 19.832s/100 iter), loss = 0.0641626
I0815 21:22:31.926715 20340 solver.cpp:334]     Train net output #0: loss = 0.0641624 (* 1 = 0.0641624 loss)
I0815 21:22:31.926720 20340 sgd_solver.cpp:136] Iteration 8200, lr = 1e-05, m = 0.9
I0815 21:22:43.628002 20347 data_reader.cpp:288] Starting prefetch of epoch 4
I0815 21:22:51.826402 20340 solver.cpp:312] Iteration 8300 (5.02534 iter/s, 19.8992s/100 iter), loss = 0.140638
I0815 21:22:51.826427 20340 solver.cpp:334]     Train net output #0: loss = 0.140637 (* 1 = 0.140637 loss)
I0815 21:22:51.826433 20340 sgd_solver.cpp:136] Iteration 8300, lr = 1e-05, m = 0.9
I0815 21:23:11.368651 20340 solver.cpp:312] Iteration 8400 (5.11726 iter/s, 19.5417s/100 iter), loss = 0.0682535
I0815 21:23:11.368672 20340 solver.cpp:334]     Train net output #0: loss = 0.0682534 (* 1 = 0.0682534 loss)
I0815 21:23:11.368676 20340 sgd_solver.cpp:136] Iteration 8400, lr = 1e-05, m = 0.9
I0815 21:23:30.678675 20340 solver.cpp:312] Iteration 8500 (5.1788 iter/s, 19.3095s/100 iter), loss = 0.0569647
I0815 21:23:30.678731 20340 solver.cpp:334]     Train net output #0: loss = 0.0569645 (* 1 = 0.0569645 loss)
I0815 21:23:30.678740 20340 sgd_solver.cpp:136] Iteration 8500, lr = 1e-05, m = 0.9
I0815 21:23:48.194772 20347 data_reader.cpp:288] Starting prefetch of epoch 5
I0815 21:23:49.884723 20340 solver.cpp:312] Iteration 8600 (5.20684 iter/s, 19.2055s/100 iter), loss = 0.113253
I0815 21:23:49.884745 20340 solver.cpp:334]     Train net output #0: loss = 0.113253 (* 1 = 0.113253 loss)
I0815 21:23:49.884749 20340 sgd_solver.cpp:136] Iteration 8600, lr = 1e-05, m = 0.9
I0815 21:24:09.404042 20340 solver.cpp:312] Iteration 8700 (5.12327 iter/s, 19.5188s/100 iter), loss = 0.0774181
I0815 21:24:09.404109 20340 solver.cpp:334]     Train net output #0: loss = 0.077418 (* 1 = 0.077418 loss)
I0815 21:24:09.404114 20340 sgd_solver.cpp:136] Iteration 8700, lr = 1e-05, m = 0.9
I0815 21:24:20.264587 20344 data_reader.cpp:288] Starting prefetch of epoch 11
I0815 21:24:28.818883 20340 solver.cpp:312] Iteration 8800 (5.15084 iter/s, 19.4143s/100 iter), loss = 0.073973
I0815 21:24:28.818910 20340 solver.cpp:334]     Train net output #0: loss = 0.0739729 (* 1 = 0.0739729 loss)
I0815 21:24:28.818917 20340 sgd_solver.cpp:136] Iteration 8800, lr = 1e-05, m = 0.9
I0815 21:24:48.274982 20340 solver.cpp:312] Iteration 8900 (5.13992 iter/s, 19.4556s/100 iter), loss = 0.0746262
I0815 21:24:48.275037 20340 solver.cpp:334]     Train net output #0: loss = 0.074626 (* 1 = 0.074626 loss)
I0815 21:24:48.275041 20340 sgd_solver.cpp:136] Iteration 8900, lr = 1e-05, m = 0.9
I0815 21:25:07.681324 20340 solver.cpp:312] Iteration 9000 (5.1531 iter/s, 19.4058s/100 iter), loss = 0.0669641
I0815 21:25:07.681351 20340 solver.cpp:334]     Train net output #0: loss = 0.0669639 (* 1 = 0.0669639 loss)
I0815 21:25:07.681358 20340 sgd_solver.cpp:136] Iteration 9000, lr = 1e-05, m = 0.9
I0815 21:25:24.756458 20348 data_reader.cpp:288] Starting prefetch of epoch 4
I0815 21:25:27.216897 20340 solver.cpp:312] Iteration 9100 (5.11901 iter/s, 19.535s/100 iter), loss = 0.0870504
I0815 21:25:27.216925 20340 solver.cpp:334]     Train net output #0: loss = 0.0870502 (* 1 = 0.0870502 loss)
I0815 21:25:27.216931 20340 sgd_solver.cpp:136] Iteration 9100, lr = 1e-05, m = 0.9
I0815 21:25:46.638818 20340 solver.cpp:312] Iteration 9200 (5.14896 iter/s, 19.4214s/100 iter), loss = 0.0709906
I0815 21:25:46.638844 20340 solver.cpp:334]     Train net output #0: loss = 0.0709905 (* 1 = 0.0709905 loss)
I0815 21:25:46.638849 20340 sgd_solver.cpp:136] Iteration 9200, lr = 1e-05, m = 0.9
I0815 21:26:06.162968 20340 solver.cpp:312] Iteration 9300 (5.122 iter/s, 19.5236s/100 iter), loss = 0.0750414
I0815 21:26:06.163015 20340 solver.cpp:334]     Train net output #0: loss = 0.0750413 (* 1 = 0.0750413 loss)
I0815 21:26:06.163022 20340 sgd_solver.cpp:136] Iteration 9300, lr = 1e-05, m = 0.9
I0815 21:26:25.707492 20340 solver.cpp:312] Iteration 9400 (5.11666 iter/s, 19.544s/100 iter), loss = 0.102015
I0815 21:26:25.707515 20340 solver.cpp:334]     Train net output #0: loss = 0.102015 (* 1 = 0.102015 loss)
I0815 21:26:25.707521 20340 sgd_solver.cpp:136] Iteration 9400, lr = 1e-05, m = 0.9
I0815 21:26:29.031188 20348 data_reader.cpp:288] Starting prefetch of epoch 5
I0815 21:26:45.041190 20340 solver.cpp:312] Iteration 9500 (5.17246 iter/s, 19.3332s/100 iter), loss = 0.0655118
I0815 21:26:45.041266 20340 solver.cpp:334]     Train net output #0: loss = 0.0655118 (* 1 = 0.0655118 loss)
I0815 21:26:45.041275 20340 sgd_solver.cpp:136] Iteration 9500, lr = 1e-05, m = 0.9
I0815 21:27:01.196401 20344 data_reader.cpp:288] Starting prefetch of epoch 12
I0815 21:27:04.524194 20340 solver.cpp:312] Iteration 9600 (5.13282 iter/s, 19.4825s/100 iter), loss = 0.0680127
I0815 21:27:04.524224 20340 solver.cpp:334]     Train net output #0: loss = 0.0680126 (* 1 = 0.0680126 loss)
I0815 21:27:04.524230 20340 sgd_solver.cpp:136] Iteration 9600, lr = 1e-05, m = 0.9
I0815 21:27:23.985097 20340 solver.cpp:312] Iteration 9700 (5.13865 iter/s, 19.4604s/100 iter), loss = 0.0685929
I0815 21:27:23.996358 20340 solver.cpp:334]     Train net output #0: loss = 0.0685928 (* 1 = 0.0685928 loss)
I0815 21:27:23.996397 20340 sgd_solver.cpp:136] Iteration 9700, lr = 1e-05, m = 0.9
I0815 21:27:43.376627 20340 solver.cpp:312] Iteration 9800 (5.15703 iter/s, 19.391s/100 iter), loss = 0.0669615
I0815 21:27:43.376652 20340 solver.cpp:334]     Train net output #0: loss = 0.0669614 (* 1 = 0.0669614 loss)
I0815 21:27:43.376658 20340 sgd_solver.cpp:136] Iteration 9800, lr = 1e-05, m = 0.9
I0815 21:28:02.715863 20340 solver.cpp:312] Iteration 9900 (5.17098 iter/s, 19.3387s/100 iter), loss = 0.0750452
I0815 21:28:02.715935 20340 solver.cpp:334]     Train net output #0: loss = 0.075045 (* 1 = 0.075045 loss)
I0815 21:28:02.715942 20340 sgd_solver.cpp:136] Iteration 9900, lr = 1e-05, m = 0.9
I0815 21:28:05.250067 20316 data_reader.cpp:288] Starting prefetch of epoch 6
I0815 21:28:22.046216 20340 solver.cpp:639] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/l1reg/cityscapes5_jsegnet21v2_iter_10000.caffemodel
I0815 21:28:22.154637 20340 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/l1reg/cityscapes5_jsegnet21v2_iter_10000.solverstate
I0815 21:28:22.166014 20340 solver.cpp:509] Iteration 10000, Testing net (#0)
I0815 21:28:50.910784 20340 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.951444
I0815 21:28:50.910898 20340 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999501
I0815 21:28:50.910908 20340 solver.cpp:594]     Test net output #2: loss = 0.183997 (* 1 = 0.183997 loss)
I0815 21:28:50.910936 20340 solver.cpp:264] [MultiGPU] Tests completed in 28.7441s
I0815 21:28:51.113185 20340 solver.cpp:312] Iteration 10000 (2.06629 iter/s, 48.396s/100 iter), loss = 0.0750551
I0815 21:28:51.113209 20340 solver.cpp:334]     Train net output #0: loss = 0.075055 (* 1 = 0.075055 loss)
I0815 21:28:51.113215 20340 sgd_solver.cpp:136] Iteration 10000, lr = 1e-05, m = 0.9
I0815 21:29:06.092814 20316 data_reader.cpp:288] Starting prefetch of epoch 7
I0815 21:29:10.357316 20340 solver.cpp:312] Iteration 10100 (5.19654 iter/s, 19.2436s/100 iter), loss = 0.0738182
I0815 21:29:10.357342 20340 solver.cpp:334]     Train net output #0: loss = 0.0738181 (* 1 = 0.0738181 loss)
I0815 21:29:10.357347 20340 sgd_solver.cpp:136] Iteration 10100, lr = 1e-05, m = 0.9
I0815 21:29:29.968673 20340 solver.cpp:312] Iteration 10200 (5.09923 iter/s, 19.6108s/100 iter), loss = 0.0972948
I0815 21:29:29.968758 20340 solver.cpp:334]     Train net output #0: loss = 0.0972947 (* 1 = 0.0972947 loss)
I0815 21:29:29.968765 20340 sgd_solver.cpp:136] Iteration 10200, lr = 1e-05, m = 0.9
I0815 21:29:49.602535 20340 solver.cpp:312] Iteration 10300 (5.09338 iter/s, 19.6333s/100 iter), loss = 0.0636069
I0815 21:29:49.602560 20340 solver.cpp:334]     Train net output #0: loss = 0.0636068 (* 1 = 0.0636068 loss)
I0815 21:29:49.602563 20340 sgd_solver.cpp:136] Iteration 10300, lr = 1e-05, m = 0.9
I0815 21:30:09.052209 20340 solver.cpp:312] Iteration 10400 (5.14162 iter/s, 19.4491s/100 iter), loss = 0.0551724
I0815 21:30:09.052258 20340 solver.cpp:334]     Train net output #0: loss = 0.0551723 (* 1 = 0.0551723 loss)
I0815 21:30:09.052264 20340 sgd_solver.cpp:136] Iteration 10400, lr = 1e-05, m = 0.9
I0815 21:30:10.830199 20316 data_reader.cpp:288] Starting prefetch of epoch 8
I0815 21:30:28.516858 20340 solver.cpp:312] Iteration 10500 (5.13766 iter/s, 19.4641s/100 iter), loss = 0.0607413
I0815 21:30:28.516885 20340 solver.cpp:334]     Train net output #0: loss = 0.0607413 (* 1 = 0.0607413 loss)
I0815 21:30:28.516891 20340 sgd_solver.cpp:136] Iteration 10500, lr = 1e-05, m = 0.9
I0815 21:30:42.921372 20314 data_reader.cpp:288] Starting prefetch of epoch 5
I0815 21:30:47.973812 20340 solver.cpp:312] Iteration 10600 (5.13969 iter/s, 19.4564s/100 iter), loss = 0.0873066
I0815 21:30:47.973834 20340 solver.cpp:334]     Train net output #0: loss = 0.0873065 (* 1 = 0.0873065 loss)
I0815 21:30:47.973837 20340 sgd_solver.cpp:136] Iteration 10600, lr = 1e-05, m = 0.9
I0815 21:31:07.500941 20340 solver.cpp:312] Iteration 10700 (5.12122 iter/s, 19.5266s/100 iter), loss = 0.0450957
I0815 21:31:07.500967 20340 solver.cpp:334]     Train net output #0: loss = 0.0450956 (* 1 = 0.0450956 loss)
I0815 21:31:07.500972 20340 sgd_solver.cpp:136] Iteration 10700, lr = 1e-05, m = 0.9
I0815 21:31:26.876904 20340 solver.cpp:312] Iteration 10800 (5.16118 iter/s, 19.3754s/100 iter), loss = 0.0815366
I0815 21:31:26.876977 20340 solver.cpp:334]     Train net output #0: loss = 0.0815365 (* 1 = 0.0815365 loss)
I0815 21:31:26.876984 20340 sgd_solver.cpp:136] Iteration 10800, lr = 1e-05, m = 0.9
I0815 21:31:46.415328 20340 solver.cpp:312] Iteration 10900 (5.11826 iter/s, 19.5379s/100 iter), loss = 0.0497804
I0815 21:31:46.415351 20340 solver.cpp:334]     Train net output #0: loss = 0.0497803 (* 1 = 0.0497803 loss)
I0815 21:31:46.415355 20340 sgd_solver.cpp:136] Iteration 10900, lr = 1e-05, m = 0.9
I0815 21:31:47.389114 20347 data_reader.cpp:288] Starting prefetch of epoch 6
I0815 21:32:05.850765 20340 solver.cpp:312] Iteration 11000 (5.14538 iter/s, 19.4349s/100 iter), loss = 0.0671847
I0815 21:32:05.850816 20340 solver.cpp:334]     Train net output #0: loss = 0.0671846 (* 1 = 0.0671846 loss)
I0815 21:32:05.850821 20340 sgd_solver.cpp:136] Iteration 11000, lr = 1e-05, m = 0.9
I0815 21:32:25.149065 20340 solver.cpp:312] Iteration 11100 (5.18195 iter/s, 19.2978s/100 iter), loss = 0.0813203
I0815 21:32:25.149088 20340 solver.cpp:334]     Train net output #0: loss = 0.0813202 (* 1 = 0.0813202 loss)
I0815 21:32:25.149093 20340 sgd_solver.cpp:136] Iteration 11100, lr = 1e-05, m = 0.9
I0815 21:32:44.631494 20340 solver.cpp:312] Iteration 11200 (5.13297 iter/s, 19.4819s/100 iter), loss = 0.0828284
I0815 21:32:44.631544 20340 solver.cpp:334]     Train net output #0: loss = 0.0828283 (* 1 = 0.0828283 loss)
I0815 21:32:44.631549 20340 sgd_solver.cpp:136] Iteration 11200, lr = 1e-05, m = 0.9
I0815 21:32:51.223830 20348 data_reader.cpp:288] Starting prefetch of epoch 6
I0815 21:33:03.655544 20340 solver.cpp:312] Iteration 11300 (5.25665 iter/s, 19.0235s/100 iter), loss = 0.0640767
I0815 21:33:03.655571 20340 solver.cpp:334]     Train net output #0: loss = 0.0640766 (* 1 = 0.0640766 loss)
I0815 21:33:03.655578 20340 sgd_solver.cpp:136] Iteration 11300, lr = 1e-05, m = 0.9
I0815 21:33:23.188393 20340 solver.cpp:312] Iteration 11400 (5.11972 iter/s, 19.5323s/100 iter), loss = 0.0671259
I0815 21:33:23.188450 20340 solver.cpp:334]     Train net output #0: loss = 0.0671258 (* 1 = 0.0671258 loss)
I0815 21:33:23.188459 20340 sgd_solver.cpp:136] Iteration 11400, lr = 1e-05, m = 0.9
I0815 21:33:23.416414 20348 data_reader.cpp:288] Starting prefetch of epoch 7
I0815 21:33:42.527842 20340 solver.cpp:312] Iteration 11500 (5.17092 iter/s, 19.3389s/100 iter), loss = 0.0854167
I0815 21:33:42.527866 20340 solver.cpp:334]     Train net output #0: loss = 0.0854166 (* 1 = 0.0854166 loss)
I0815 21:33:42.527871 20340 sgd_solver.cpp:136] Iteration 11500, lr = 1e-05, m = 0.9
I0815 21:34:01.886310 20340 solver.cpp:312] Iteration 11600 (5.16584 iter/s, 19.3579s/100 iter), loss = 0.114739
I0815 21:34:01.886394 20340 solver.cpp:334]     Train net output #0: loss = 0.114739 (* 1 = 0.114739 loss)
I0815 21:34:01.886400 20340 sgd_solver.cpp:136] Iteration 11600, lr = 1e-05, m = 0.9
I0815 21:34:21.521194 20340 solver.cpp:312] Iteration 11700 (5.09312 iter/s, 19.6343s/100 iter), loss = 0.0433572
I0815 21:34:21.521224 20340 solver.cpp:334]     Train net output #0: loss = 0.0433571 (* 1 = 0.0433571 loss)
I0815 21:34:21.521230 20340 sgd_solver.cpp:136] Iteration 11700, lr = 1e-05, m = 0.9
I0815 21:34:27.497308 20348 data_reader.cpp:288] Starting prefetch of epoch 8
I0815 21:34:40.781050 20340 solver.cpp:312] Iteration 11800 (5.19229 iter/s, 19.2593s/100 iter), loss = 0.0669257
I0815 21:34:40.781101 20340 solver.cpp:334]     Train net output #0: loss = 0.0669256 (* 1 = 0.0669256 loss)
I0815 21:34:40.781108 20340 sgd_solver.cpp:136] Iteration 11800, lr = 1e-05, m = 0.9
I0815 21:35:00.349812 20340 solver.cpp:312] Iteration 11900 (5.11033 iter/s, 19.5682s/100 iter), loss = 0.0672635
I0815 21:35:00.349839 20340 solver.cpp:334]     Train net output #0: loss = 0.0672634 (* 1 = 0.0672634 loss)
I0815 21:35:00.349844 20340 sgd_solver.cpp:136] Iteration 11900, lr = 1e-05, m = 0.9
I0815 21:35:19.667872 20340 solver.cpp:509] Iteration 12000, Testing net (#0)
I0815 21:35:23.378932 20387 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 21:35:31.584599 20391 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 21:35:31.938318 20340 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.950933
I0815 21:35:31.938345 20340 solver.cpp:594]     Test net output #1: accuracy/top5 = 1
I0815 21:35:31.938350 20340 solver.cpp:594]     Test net output #2: loss = 0.153057 (* 1 = 0.153057 loss)
I0815 21:35:31.938459 20340 solver.cpp:264] [MultiGPU] Tests completed in 12.2703s
I0815 21:35:32.153844 20340 solver.cpp:312] Iteration 12000 (3.14434 iter/s, 31.8032s/100 iter), loss = 0.0411355
I0815 21:35:32.153867 20340 solver.cpp:334]     Train net output #0: loss = 0.0411354 (* 1 = 0.0411354 loss)
I0815 21:35:32.153870 20340 sgd_solver.cpp:136] Iteration 12000, lr = 1e-05, m = 0.9
I0815 21:35:51.588374 20340 solver.cpp:312] Iteration 12100 (5.14562 iter/s, 19.434s/100 iter), loss = 0.0967906
I0815 21:35:51.588459 20340 solver.cpp:334]     Train net output #0: loss = 0.0967905 (* 1 = 0.0967905 loss)
I0815 21:35:51.588466 20340 sgd_solver.cpp:136] Iteration 12100, lr = 1e-05, m = 0.9
I0815 21:36:10.954427 20340 solver.cpp:312] Iteration 12200 (5.16382 iter/s, 19.3655s/100 iter), loss = 0.061021
I0815 21:36:10.954455 20340 solver.cpp:334]     Train net output #0: loss = 0.0610209 (* 1 = 0.0610209 loss)
I0815 21:36:10.954463 20340 sgd_solver.cpp:136] Iteration 12200, lr = 1e-05, m = 0.9
I0815 21:36:16.121666 20348 data_reader.cpp:288] Starting prefetch of epoch 9
I0815 21:36:30.213063 20340 solver.cpp:312] Iteration 12300 (5.19262 iter/s, 19.2581s/100 iter), loss = 0.055112
I0815 21:36:30.213163 20340 solver.cpp:334]     Train net output #0: loss = 0.0551119 (* 1 = 0.0551119 loss)
I0815 21:36:30.213183 20340 sgd_solver.cpp:136] Iteration 12300, lr = 1e-05, m = 0.9
I0815 21:36:49.746166 20340 solver.cpp:312] Iteration 12400 (5.11966 iter/s, 19.5326s/100 iter), loss = 0.0558946
I0815 21:36:49.746191 20340 solver.cpp:334]     Train net output #0: loss = 0.0558945 (* 1 = 0.0558945 loss)
I0815 21:36:49.746197 20340 sgd_solver.cpp:136] Iteration 12400, lr = 1e-05, m = 0.9
I0815 21:37:08.761080 20340 solver.cpp:312] Iteration 12500 (5.25917 iter/s, 19.0144s/100 iter), loss = 0.0756989
I0815 21:37:08.761186 20340 solver.cpp:334]     Train net output #0: loss = 0.0756987 (* 1 = 0.0756987 loss)
I0815 21:37:08.761193 20340 sgd_solver.cpp:136] Iteration 12500, lr = 1e-05, m = 0.9
I0815 21:37:20.091063 20316 data_reader.cpp:288] Starting prefetch of epoch 9
I0815 21:37:28.347288 20340 solver.cpp:312] Iteration 12600 (5.10577 iter/s, 19.5857s/100 iter), loss = 0.0488623
I0815 21:37:28.347311 20340 solver.cpp:334]     Train net output #0: loss = 0.0488622 (* 1 = 0.0488622 loss)
I0815 21:37:28.347316 20340 sgd_solver.cpp:136] Iteration 12600, lr = 1e-05, m = 0.9
I0815 21:37:48.862893 20340 solver.cpp:312] Iteration 12700 (4.87447 iter/s, 20.515s/100 iter), loss = 0.119463
I0815 21:37:48.862977 20340 solver.cpp:334]     Train net output #0: loss = 0.119463 (* 1 = 0.119463 loss)
I0815 21:37:48.862983 20340 sgd_solver.cpp:136] Iteration 12700, lr = 1e-05, m = 0.9
I0815 21:37:53.365496 20314 data_reader.cpp:288] Starting prefetch of epoch 6
I0815 21:38:08.358737 20340 solver.cpp:312] Iteration 12800 (5.12944 iter/s, 19.4953s/100 iter), loss = 0.0505349
I0815 21:38:08.358758 20340 solver.cpp:334]     Train net output #0: loss = 0.0505348 (* 1 = 0.0505348 loss)
I0815 21:38:08.358762 20340 sgd_solver.cpp:136] Iteration 12800, lr = 1e-05, m = 0.9
I0815 21:38:27.596653 20340 solver.cpp:312] Iteration 12900 (5.19821 iter/s, 19.2374s/100 iter), loss = 0.0779058
I0815 21:38:27.596704 20340 solver.cpp:334]     Train net output #0: loss = 0.0779057 (* 1 = 0.0779057 loss)
I0815 21:38:27.596709 20340 sgd_solver.cpp:136] Iteration 12900, lr = 1e-05, m = 0.9
I0815 21:38:47.156220 20340 solver.cpp:312] Iteration 13000 (5.11273 iter/s, 19.559s/100 iter), loss = 0.0587579
I0815 21:38:47.156241 20340 solver.cpp:334]     Train net output #0: loss = 0.0587578 (* 1 = 0.0587578 loss)
I0815 21:38:47.156245 20340 sgd_solver.cpp:136] Iteration 13000, lr = 1e-05, m = 0.9
I0815 21:38:57.729100 20316 data_reader.cpp:288] Starting prefetch of epoch 10
I0815 21:39:06.808943 20340 solver.cpp:312] Iteration 13100 (5.08849 iter/s, 19.6522s/100 iter), loss = 0.0896918
I0815 21:39:06.808967 20340 solver.cpp:334]     Train net output #0: loss = 0.0896917 (* 1 = 0.0896917 loss)
I0815 21:39:06.808972 20340 sgd_solver.cpp:136] Iteration 13100, lr = 1e-05, m = 0.9
I0815 21:39:26.133759 20340 solver.cpp:312] Iteration 13200 (5.17484 iter/s, 19.3243s/100 iter), loss = 0.055454
I0815 21:39:26.133780 20340 solver.cpp:334]     Train net output #0: loss = 0.0554539 (* 1 = 0.0554539 loss)
I0815 21:39:26.133785 20340 sgd_solver.cpp:136] Iteration 13200, lr = 1e-05, m = 0.9
I0815 21:39:45.594086 20340 solver.cpp:312] Iteration 13300 (5.1388 iter/s, 19.4598s/100 iter), loss = 0.0706577
I0815 21:39:45.594136 20340 solver.cpp:334]     Train net output #0: loss = 0.0706575 (* 1 = 0.0706575 loss)
I0815 21:39:45.594143 20340 sgd_solver.cpp:136] Iteration 13300, lr = 1e-05, m = 0.9
I0815 21:40:01.897001 20316 data_reader.cpp:288] Starting prefetch of epoch 11
I0815 21:40:05.032793 20340 solver.cpp:312] Iteration 13400 (5.14452 iter/s, 19.4382s/100 iter), loss = 0.053825
I0815 21:40:05.032812 20340 solver.cpp:334]     Train net output #0: loss = 0.0538249 (* 1 = 0.0538249 loss)
I0815 21:40:05.032819 20340 sgd_solver.cpp:136] Iteration 13400, lr = 1e-05, m = 0.9
I0815 21:40:24.496829 20340 solver.cpp:312] Iteration 13500 (5.13782 iter/s, 19.4635s/100 iter), loss = 0.0782919
I0815 21:40:24.509099 20340 solver.cpp:334]     Train net output #0: loss = 0.0782918 (* 1 = 0.0782918 loss)
I0815 21:40:24.509133 20340 sgd_solver.cpp:136] Iteration 13500, lr = 1e-05, m = 0.9
I0815 21:40:34.236438 20343 data_reader.cpp:288] Starting prefetch of epoch 6
I0815 21:40:44.061805 20340 solver.cpp:312] Iteration 13600 (5.11132 iter/s, 19.5644s/100 iter), loss = 0.0852463
I0815 21:40:44.061827 20340 solver.cpp:334]     Train net output #0: loss = 0.0852462 (* 1 = 0.0852462 loss)
I0815 21:40:44.061833 20340 sgd_solver.cpp:136] Iteration 13600, lr = 1e-05, m = 0.9
I0815 21:41:03.400585 20340 solver.cpp:312] Iteration 13700 (5.1711 iter/s, 19.3383s/100 iter), loss = 0.0578729
I0815 21:41:03.400645 20340 solver.cpp:334]     Train net output #0: loss = 0.0578727 (* 1 = 0.0578727 loss)
I0815 21:41:03.400652 20340 sgd_solver.cpp:136] Iteration 13700, lr = 1e-05, m = 0.9
I0815 21:41:22.930440 20340 solver.cpp:312] Iteration 13800 (5.12051 iter/s, 19.5293s/100 iter), loss = 0.0616944
I0815 21:41:22.930474 20340 solver.cpp:334]     Train net output #0: loss = 0.0616942 (* 1 = 0.0616942 loss)
I0815 21:41:22.930480 20340 sgd_solver.cpp:136] Iteration 13800, lr = 1e-05, m = 0.9
I0815 21:41:38.458358 20314 data_reader.cpp:288] Starting prefetch of epoch 7
I0815 21:41:42.311100 20340 solver.cpp:312] Iteration 13900 (5.15992 iter/s, 19.3801s/100 iter), loss = 0.078685
I0815 21:41:42.311125 20340 solver.cpp:334]     Train net output #0: loss = 0.0786849 (* 1 = 0.0786849 loss)
I0815 21:41:42.311131 20340 sgd_solver.cpp:136] Iteration 13900, lr = 1e-05, m = 0.9
I0815 21:42:01.532940 20340 solver.cpp:509] Iteration 14000, Testing net (#0)
I0815 21:42:14.400473 20340 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.951847
I0815 21:42:14.400575 20340 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999406
I0815 21:42:14.400584 20340 solver.cpp:594]     Test net output #2: loss = 0.183419 (* 1 = 0.183419 loss)
I0815 21:42:14.400610 20340 solver.cpp:264] [MultiGPU] Tests completed in 12.8673s
I0815 21:42:14.606631 20340 solver.cpp:312] Iteration 14000 (3.09649 iter/s, 32.2947s/100 iter), loss = 0.0910856
I0815 21:42:14.606654 20340 solver.cpp:334]     Train net output #0: loss = 0.0910855 (* 1 = 0.0910855 loss)
I0815 21:42:14.606660 20340 sgd_solver.cpp:136] Iteration 14000, lr = 1e-05, m = 0.9
I0815 21:42:23.459728 20343 data_reader.cpp:288] Starting prefetch of epoch 7
I0815 21:42:34.143970 20340 solver.cpp:312] Iteration 14100 (5.11855 iter/s, 19.5368s/100 iter), loss = 0.0938018
I0815 21:42:34.143990 20340 solver.cpp:334]     Train net output #0: loss = 0.0938017 (* 1 = 0.0938017 loss)
I0815 21:42:34.143996 20340 sgd_solver.cpp:136] Iteration 14100, lr = 1e-05, m = 0.9
I0815 21:42:53.881539 20340 solver.cpp:312] Iteration 14200 (5.06662 iter/s, 19.737s/100 iter), loss = 0.0654386
I0815 21:42:53.881602 20340 solver.cpp:334]     Train net output #0: loss = 0.0654384 (* 1 = 0.0654384 loss)
I0815 21:42:53.881606 20340 sgd_solver.cpp:136] Iteration 14200, lr = 1e-05, m = 0.9
I0815 21:43:13.472937 20340 solver.cpp:312] Iteration 14300 (5.10442 iter/s, 19.5909s/100 iter), loss = 0.119391
I0815 21:43:13.472966 20340 solver.cpp:334]     Train net output #0: loss = 0.11939 (* 1 = 0.11939 loss)
I0815 21:43:13.472973 20340 sgd_solver.cpp:136] Iteration 14300, lr = 1e-05, m = 0.9
I0815 21:43:28.209612 20347 data_reader.cpp:288] Starting prefetch of epoch 7
I0815 21:43:32.912039 20340 solver.cpp:312] Iteration 14400 (5.14441 iter/s, 19.4386s/100 iter), loss = 0.0835157
I0815 21:43:32.912190 20340 solver.cpp:334]     Train net output #0: loss = 0.0835155 (* 1 = 0.0835155 loss)
I0815 21:43:32.912209 20340 sgd_solver.cpp:136] Iteration 14400, lr = 1e-05, m = 0.9
I0815 21:43:52.604413 20340 solver.cpp:312] Iteration 14500 (5.07825 iter/s, 19.6918s/100 iter), loss = 0.0809736
I0815 21:43:52.604441 20340 solver.cpp:334]     Train net output #0: loss = 0.0809734 (* 1 = 0.0809734 loss)
I0815 21:43:52.604449 20340 sgd_solver.cpp:136] Iteration 14500, lr = 1e-05, m = 0.9
I0815 21:44:00.605223 20314 data_reader.cpp:288] Starting prefetch of epoch 8
I0815 21:44:12.004859 20340 solver.cpp:312] Iteration 14600 (5.15466 iter/s, 19.3999s/100 iter), loss = 0.0698945
I0815 21:44:12.004884 20340 solver.cpp:334]     Train net output #0: loss = 0.0698943 (* 1 = 0.0698943 loss)
I0815 21:44:12.004890 20340 sgd_solver.cpp:136] Iteration 14600, lr = 1e-05, m = 0.9
I0815 21:44:31.482828 20340 solver.cpp:312] Iteration 14700 (5.13415 iter/s, 19.4774s/100 iter), loss = 0.0675951
I0815 21:44:31.482888 20340 solver.cpp:334]     Train net output #0: loss = 0.0675949 (* 1 = 0.0675949 loss)
I0815 21:44:31.482897 20340 sgd_solver.cpp:136] Iteration 14700, lr = 1e-05, m = 0.9
I0815 21:44:50.836637 20340 solver.cpp:312] Iteration 14800 (5.16708 iter/s, 19.3533s/100 iter), loss = 0.0724857
I0815 21:44:50.836663 20340 solver.cpp:334]     Train net output #0: loss = 0.0724856 (* 1 = 0.0724856 loss)
I0815 21:44:50.836669 20340 sgd_solver.cpp:136] Iteration 14800, lr = 1e-05, m = 0.9
I0815 21:45:04.788949 20348 data_reader.cpp:288] Starting prefetch of epoch 10
I0815 21:45:10.517057 20340 solver.cpp:312] Iteration 14900 (5.08133 iter/s, 19.6799s/100 iter), loss = 0.0845488
I0815 21:45:10.517083 20340 solver.cpp:334]     Train net output #0: loss = 0.0845486 (* 1 = 0.0845486 loss)
I0815 21:45:10.517089 20340 sgd_solver.cpp:136] Iteration 14900, lr = 1e-05, m = 0.9
I0815 21:45:30.248164 20340 solver.cpp:312] Iteration 15000 (5.06828 iter/s, 19.7306s/100 iter), loss = 0.063867
I0815 21:45:30.248263 20340 solver.cpp:334]     Train net output #0: loss = 0.0638669 (* 1 = 0.0638669 loss)
I0815 21:45:30.248277 20340 sgd_solver.cpp:136] Iteration 15000, lr = 1e-05, m = 0.9
I0815 21:45:49.713414 20340 solver.cpp:312] Iteration 15100 (5.1375 iter/s, 19.4647s/100 iter), loss = 0.101518
I0815 21:45:49.713472 20340 solver.cpp:334]     Train net output #0: loss = 0.101518 (* 1 = 0.101518 loss)
I0815 21:45:49.713479 20340 sgd_solver.cpp:136] Iteration 15100, lr = 1e-05, m = 0.9
I0815 21:46:09.016561 20340 solver.cpp:312] Iteration 15200 (5.18064 iter/s, 19.3026s/100 iter), loss = 0.0832158
I0815 21:46:09.016580 20340 solver.cpp:334]     Train net output #0: loss = 0.0832156 (* 1 = 0.0832156 loss)
I0815 21:46:09.016584 20340 sgd_solver.cpp:136] Iteration 15200, lr = 1e-05, m = 0.9
I0815 21:46:09.397882 20347 data_reader.cpp:288] Starting prefetch of epoch 8
I0815 21:46:28.385699 20340 solver.cpp:312] Iteration 15300 (5.16299 iter/s, 19.3686s/100 iter), loss = 0.0371951
I0815 21:46:28.385751 20340 solver.cpp:334]     Train net output #0: loss = 0.0371949 (* 1 = 0.0371949 loss)
I0815 21:46:28.385759 20340 sgd_solver.cpp:136] Iteration 15300, lr = 1e-05, m = 0.9
I0815 21:46:41.551926 20343 data_reader.cpp:288] Starting prefetch of epoch 8
I0815 21:46:47.900740 20340 solver.cpp:312] Iteration 15400 (5.12439 iter/s, 19.5145s/100 iter), loss = 0.0757225
I0815 21:46:47.900764 20340 solver.cpp:334]     Train net output #0: loss = 0.0757224 (* 1 = 0.0757224 loss)
I0815 21:46:47.900770 20340 sgd_solver.cpp:136] Iteration 15400, lr = 1e-05, m = 0.9
I0815 21:47:07.385529 20340 solver.cpp:312] Iteration 15500 (5.13235 iter/s, 19.4843s/100 iter), loss = 0.0756475
I0815 21:47:07.385628 20340 solver.cpp:334]     Train net output #0: loss = 0.0756473 (* 1 = 0.0756473 loss)
I0815 21:47:07.385634 20340 sgd_solver.cpp:136] Iteration 15500, lr = 1e-05, m = 0.9
I0815 21:47:26.757582 20340 solver.cpp:312] Iteration 15600 (5.16222 iter/s, 19.3715s/100 iter), loss = 0.0672814
I0815 21:47:26.757606 20340 solver.cpp:334]     Train net output #0: loss = 0.0672812 (* 1 = 0.0672812 loss)
I0815 21:47:26.757611 20340 sgd_solver.cpp:136] Iteration 15600, lr = 1e-05, m = 0.9
I0815 21:47:45.696593 20314 data_reader.cpp:288] Starting prefetch of epoch 9
I0815 21:47:46.029026 20340 solver.cpp:312] Iteration 15700 (5.18917 iter/s, 19.2709s/100 iter), loss = 0.0821114
I0815 21:47:46.029047 20340 solver.cpp:334]     Train net output #0: loss = 0.0821112 (* 1 = 0.0821112 loss)
I0815 21:47:46.029052 20340 sgd_solver.cpp:136] Iteration 15700, lr = 1e-05, m = 0.9
I0815 21:48:05.356068 20340 solver.cpp:312] Iteration 15800 (5.17424 iter/s, 19.3265s/100 iter), loss = 0.0730028
I0815 21:48:05.356091 20340 solver.cpp:334]     Train net output #0: loss = 0.0730027 (* 1 = 0.0730027 loss)
I0815 21:48:05.356096 20340 sgd_solver.cpp:136] Iteration 15800, lr = 1e-05, m = 0.9
I0815 21:48:24.805800 20340 solver.cpp:312] Iteration 15900 (5.1416 iter/s, 19.4492s/100 iter), loss = 0.053871
I0815 21:48:24.805850 20340 solver.cpp:334]     Train net output #0: loss = 0.0538708 (* 1 = 0.0538708 loss)
I0815 21:48:24.805856 20340 sgd_solver.cpp:136] Iteration 15900, lr = 1e-05, m = 0.9
I0815 21:48:43.894271 20340 solver.cpp:509] Iteration 16000, Testing net (#0)
I0815 21:48:47.276491 20336 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 21:48:55.873617 20385 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 21:48:56.218099 20340 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.95205
I0815 21:48:56.218122 20340 solver.cpp:594]     Test net output #1: accuracy/top5 = 1
I0815 21:48:56.218127 20340 solver.cpp:594]     Test net output #2: loss = 0.152022 (* 1 = 0.152022 loss)
I0815 21:48:56.218154 20340 solver.cpp:264] [MultiGPU] Tests completed in 12.3235s
I0815 21:48:56.436861 20340 solver.cpp:312] Iteration 16000 (3.16154 iter/s, 31.6302s/100 iter), loss = 0.0829468
I0815 21:48:56.436885 20340 solver.cpp:334]     Train net output #0: loss = 0.0829466 (* 1 = 0.0829466 loss)
I0815 21:48:56.436892 20340 sgd_solver.cpp:136] Iteration 16000, lr = 1e-05, m = 0.9
I0815 21:49:15.877687 20340 solver.cpp:312] Iteration 16100 (5.14396 iter/s, 19.4403s/100 iter), loss = 0.0783559
I0815 21:49:15.877715 20340 solver.cpp:334]     Train net output #0: loss = 0.0783557 (* 1 = 0.0783557 loss)
I0815 21:49:15.877722 20340 sgd_solver.cpp:136] Iteration 16100, lr = 1e-05, m = 0.9
I0815 21:49:34.186939 20344 data_reader.cpp:288] Starting prefetch of epoch 13
I0815 21:49:35.333593 20340 solver.cpp:312] Iteration 16200 (5.13997 iter/s, 19.4554s/100 iter), loss = 0.0824674
I0815 21:49:35.333616 20340 solver.cpp:334]     Train net output #0: loss = 0.0824672 (* 1 = 0.0824672 loss)
I0815 21:49:35.333621 20340 sgd_solver.cpp:136] Iteration 16200, lr = 1e-05, m = 0.9
I0815 21:49:54.739296 20340 solver.cpp:312] Iteration 16300 (5.15327 iter/s, 19.4052s/100 iter), loss = 0.072472
I0815 21:49:54.739326 20340 solver.cpp:334]     Train net output #0: loss = 0.0724718 (* 1 = 0.0724718 loss)
I0815 21:49:54.739331 20340 sgd_solver.cpp:136] Iteration 16300, lr = 1e-05, m = 0.9
I0815 21:50:14.124903 20340 solver.cpp:312] Iteration 16400 (5.15861 iter/s, 19.3851s/100 iter), loss = 0.0451146
I0815 21:50:14.124951 20340 solver.cpp:334]     Train net output #0: loss = 0.0451145 (* 1 = 0.0451145 loss)
I0815 21:50:14.124958 20340 sgd_solver.cpp:136] Iteration 16400, lr = 1e-05, m = 0.9
I0815 21:50:33.656622 20340 solver.cpp:312] Iteration 16500 (5.12002 iter/s, 19.5312s/100 iter), loss = 0.0696338
I0815 21:50:33.656646 20340 solver.cpp:334]     Train net output #0: loss = 0.0696336 (* 1 = 0.0696336 loss)
I0815 21:50:33.656649 20340 sgd_solver.cpp:136] Iteration 16500, lr = 1e-05, m = 0.9
I0815 21:50:38.265662 20348 data_reader.cpp:288] Starting prefetch of epoch 11
I0815 21:50:53.009505 20340 solver.cpp:312] Iteration 16600 (5.16733 iter/s, 19.3524s/100 iter), loss = 0.0729952
I0815 21:50:53.009627 20340 solver.cpp:334]     Train net output #0: loss = 0.072995 (* 1 = 0.072995 loss)
I0815 21:50:53.009634 20340 sgd_solver.cpp:136] Iteration 16600, lr = 1e-05, m = 0.9
I0815 21:51:10.513000 20314 data_reader.cpp:288] Starting prefetch of epoch 10
I0815 21:51:12.455721 20340 solver.cpp:312] Iteration 16700 (5.14253 iter/s, 19.4457s/100 iter), loss = 0.0970855
I0815 21:51:12.455746 20340 solver.cpp:334]     Train net output #0: loss = 0.0970853 (* 1 = 0.0970853 loss)
I0815 21:51:12.455754 20340 sgd_solver.cpp:136] Iteration 16700, lr = 1e-05, m = 0.9
I0815 21:51:31.845571 20340 solver.cpp:312] Iteration 16800 (5.15748 iter/s, 19.3893s/100 iter), loss = 0.0821061
I0815 21:51:31.845633 20340 solver.cpp:334]     Train net output #0: loss = 0.082106 (* 1 = 0.082106 loss)
I0815 21:51:31.845640 20340 sgd_solver.cpp:136] Iteration 16800, lr = 1e-05, m = 0.9
I0815 21:51:51.234591 20340 solver.cpp:312] Iteration 16900 (5.1577 iter/s, 19.3885s/100 iter), loss = 0.0667725
I0815 21:51:51.234613 20340 solver.cpp:334]     Train net output #0: loss = 0.0667724 (* 1 = 0.0667724 loss)
I0815 21:51:51.234617 20340 sgd_solver.cpp:136] Iteration 16900, lr = 1e-05, m = 0.9
I0815 21:52:10.647358 20340 solver.cpp:312] Iteration 17000 (5.15139 iter/s, 19.4122s/100 iter), loss = 0.106915
I0815 21:52:10.647406 20340 solver.cpp:334]     Train net output #0: loss = 0.106914 (* 1 = 0.106914 loss)
I0815 21:52:10.647413 20340 sgd_solver.cpp:136] Iteration 17000, lr = 1e-05, m = 0.9
I0815 21:52:14.568336 20314 data_reader.cpp:288] Starting prefetch of epoch 11
I0815 21:52:30.222038 20340 solver.cpp:312] Iteration 17100 (5.10878 iter/s, 19.5741s/100 iter), loss = 0.0752588
I0815 21:52:30.222065 20340 solver.cpp:334]     Train net output #0: loss = 0.0752586 (* 1 = 0.0752586 loss)
I0815 21:52:30.222072 20340 sgd_solver.cpp:136] Iteration 17100, lr = 1e-05, m = 0.9
I0815 21:52:50.064724 20340 solver.cpp:312] Iteration 17200 (5.03978 iter/s, 19.8421s/100 iter), loss = 0.0900699
I0815 21:52:50.064775 20340 solver.cpp:334]     Train net output #0: loss = 0.0900698 (* 1 = 0.0900698 loss)
I0815 21:52:50.064782 20340 sgd_solver.cpp:136] Iteration 17200, lr = 1e-05, m = 0.9
I0815 21:53:09.546895 20340 solver.cpp:312] Iteration 17300 (5.13304 iter/s, 19.4816s/100 iter), loss = 0.0545636
I0815 21:53:09.546914 20340 solver.cpp:334]     Train net output #0: loss = 0.0545634 (* 1 = 0.0545634 loss)
I0815 21:53:09.546918 20340 sgd_solver.cpp:136] Iteration 17300, lr = 1e-05, m = 0.9
I0815 21:53:19.455206 20347 data_reader.cpp:288] Starting prefetch of epoch 9
I0815 21:53:29.137043 20340 solver.cpp:312] Iteration 17400 (5.10475 iter/s, 19.5896s/100 iter), loss = 0.056833
I0815 21:53:29.137092 20340 solver.cpp:334]     Train net output #0: loss = 0.0568329 (* 1 = 0.0568329 loss)
I0815 21:53:29.137099 20340 sgd_solver.cpp:136] Iteration 17400, lr = 1e-05, m = 0.9
I0815 21:53:48.687974 20340 solver.cpp:312] Iteration 17500 (5.11499 iter/s, 19.5504s/100 iter), loss = 0.0504352
I0815 21:53:48.687996 20340 solver.cpp:334]     Train net output #0: loss = 0.0504351 (* 1 = 0.0504351 loss)
I0815 21:53:48.688000 20340 sgd_solver.cpp:136] Iteration 17500, lr = 1e-05, m = 0.9
I0815 21:53:51.860638 20343 data_reader.cpp:288] Starting prefetch of epoch 9
I0815 21:54:08.071648 20340 solver.cpp:312] Iteration 17600 (5.15912 iter/s, 19.3831s/100 iter), loss = 0.0820081
I0815 21:54:08.088259 20340 solver.cpp:334]     Train net output #0: loss = 0.082008 (* 1 = 0.082008 loss)
I0815 21:54:08.088304 20340 sgd_solver.cpp:136] Iteration 17600, lr = 1e-05, m = 0.9
I0815 21:54:27.696913 20340 solver.cpp:312] Iteration 17700 (5.09561 iter/s, 19.6247s/100 iter), loss = 0.0646257
I0815 21:54:27.696938 20340 solver.cpp:334]     Train net output #0: loss = 0.0646256 (* 1 = 0.0646256 loss)
I0815 21:54:27.696944 20340 sgd_solver.cpp:136] Iteration 17700, lr = 1e-05, m = 0.9
I0815 21:54:47.068032 20340 solver.cpp:312] Iteration 17800 (5.16246 iter/s, 19.3706s/100 iter), loss = 0.0627478
I0815 21:54:47.068096 20340 solver.cpp:334]     Train net output #0: loss = 0.0627476 (* 1 = 0.0627476 loss)
I0815 21:54:47.068104 20340 sgd_solver.cpp:136] Iteration 17800, lr = 1e-05, m = 0.9
I0815 21:54:56.051527 20314 data_reader.cpp:288] Starting prefetch of epoch 12
I0815 21:55:06.488045 20340 solver.cpp:312] Iteration 17900 (5.14947 iter/s, 19.4195s/100 iter), loss = 0.0881428
I0815 21:55:06.488075 20340 solver.cpp:334]     Train net output #0: loss = 0.0881427 (* 1 = 0.0881427 loss)
I0815 21:55:06.488081 20340 sgd_solver.cpp:136] Iteration 17900, lr = 1e-05, m = 0.9
I0815 21:55:25.607326 20340 solver.cpp:509] Iteration 18000, Testing net (#0)
I0815 21:55:37.405570 20340 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.952128
I0815 21:55:37.405596 20340 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999398
I0815 21:55:37.405601 20340 solver.cpp:594]     Test net output #2: loss = 0.185021 (* 1 = 0.185021 loss)
I0815 21:55:37.405632 20340 solver.cpp:264] [MultiGPU] Tests completed in 11.798s
I0815 21:55:37.619376 20340 solver.cpp:312] Iteration 18000 (3.21229 iter/s, 31.1305s/100 iter), loss = 0.0493416
I0815 21:55:37.619421 20340 solver.cpp:334]     Train net output #0: loss = 0.0493414 (* 1 = 0.0493414 loss)
I0815 21:55:37.619431 20340 sgd_solver.cpp:136] Iteration 18000, lr = 1e-05, m = 0.9
I0815 21:55:40.080613 20348 data_reader.cpp:288] Starting prefetch of epoch 12
I0815 21:55:57.452165 20340 solver.cpp:312] Iteration 18100 (5.04229 iter/s, 19.8322s/100 iter), loss = 0.0614519
I0815 21:55:57.452217 20340 solver.cpp:334]     Train net output #0: loss = 0.0614518 (* 1 = 0.0614518 loss)
I0815 21:55:57.452224 20340 sgd_solver.cpp:136] Iteration 18100, lr = 1e-05, m = 0.9
I0815 21:56:13.913151 20343 data_reader.cpp:288] Starting prefetch of epoch 10
I0815 21:56:18.293324 20340 solver.cpp:312] Iteration 18200 (4.79833 iter/s, 20.8406s/100 iter), loss = 0.0687756
I0815 21:56:18.293349 20340 solver.cpp:334]     Train net output #0: loss = 0.0687755 (* 1 = 0.0687755 loss)
I0815 21:56:18.293354 20340 sgd_solver.cpp:136] Iteration 18200, lr = 1e-05, m = 0.9
I0815 21:56:37.594873 20340 solver.cpp:312] Iteration 18300 (5.18107 iter/s, 19.301s/100 iter), loss = 0.0756623
I0815 21:56:37.594929 20340 solver.cpp:334]     Train net output #0: loss = 0.0756622 (* 1 = 0.0756622 loss)
I0815 21:56:37.594936 20340 sgd_solver.cpp:136] Iteration 18300, lr = 1e-05, m = 0.9
I0815 21:56:57.143744 20340 solver.cpp:312] Iteration 18400 (5.11553 iter/s, 19.5483s/100 iter), loss = 0.0706558
I0815 21:56:57.143765 20340 solver.cpp:334]     Train net output #0: loss = 0.0706557 (* 1 = 0.0706557 loss)
I0815 21:56:57.143771 20340 sgd_solver.cpp:136] Iteration 18400, lr = 1e-05, m = 0.9
I0815 21:57:16.402951 20340 solver.cpp:312] Iteration 18500 (5.19247 iter/s, 19.2587s/100 iter), loss = 0.0791771
I0815 21:57:16.403005 20340 solver.cpp:334]     Train net output #0: loss = 0.079177 (* 1 = 0.079177 loss)
I0815 21:57:16.403012 20340 sgd_solver.cpp:136] Iteration 18500, lr = 1e-05, m = 0.9
I0815 21:57:17.984721 20344 data_reader.cpp:288] Starting prefetch of epoch 14
I0815 21:57:35.825989 20340 solver.cpp:312] Iteration 18600 (5.14867 iter/s, 19.4225s/100 iter), loss = 0.0654734
I0815 21:57:35.826010 20340 solver.cpp:334]     Train net output #0: loss = 0.0654733 (* 1 = 0.0654733 loss)
I0815 21:57:35.826015 20340 sgd_solver.cpp:136] Iteration 18600, lr = 1e-05, m = 0.9
I0815 21:57:55.172663 20340 solver.cpp:312] Iteration 18700 (5.16899 iter/s, 19.3461s/100 iter), loss = 0.0472058
I0815 21:57:55.172711 20340 solver.cpp:334]     Train net output #0: loss = 0.0472057 (* 1 = 0.0472057 loss)
I0815 21:57:55.172716 20340 sgd_solver.cpp:136] Iteration 18700, lr = 1e-05, m = 0.9
I0815 21:58:14.705561 20340 solver.cpp:312] Iteration 18800 (5.11971 iter/s, 19.5324s/100 iter), loss = 0.138831
I0815 21:58:14.705590 20340 solver.cpp:334]     Train net output #0: loss = 0.138831 (* 1 = 0.138831 loss)
I0815 21:58:14.705596 20340 sgd_solver.cpp:136] Iteration 18800, lr = 1e-05, m = 0.9
I0815 21:58:22.031708 20347 data_reader.cpp:288] Starting prefetch of epoch 10
I0815 21:58:34.004164 20340 solver.cpp:312] Iteration 18900 (5.18186 iter/s, 19.2981s/100 iter), loss = 0.072826
I0815 21:58:34.004231 20340 solver.cpp:334]     Train net output #0: loss = 0.0728259 (* 1 = 0.0728259 loss)
I0815 21:58:34.004236 20340 sgd_solver.cpp:136] Iteration 18900, lr = 1e-05, m = 0.9
I0815 21:58:53.449409 20340 solver.cpp:312] Iteration 19000 (5.14279 iter/s, 19.4447s/100 iter), loss = 0.110535
I0815 21:58:53.449437 20340 solver.cpp:334]     Train net output #0: loss = 0.110534 (* 1 = 0.110534 loss)
I0815 21:58:53.449443 20340 sgd_solver.cpp:136] Iteration 19000, lr = 1e-05, m = 0.9
I0815 21:59:12.824458 20340 solver.cpp:312] Iteration 19100 (5.16142 iter/s, 19.3745s/100 iter), loss = 0.0570659
I0815 21:59:12.833165 20340 solver.cpp:334]     Train net output #0: loss = 0.0570658 (* 1 = 0.0570658 loss)
I0815 21:59:12.833200 20340 sgd_solver.cpp:136] Iteration 19100, lr = 1e-05, m = 0.9
I0815 21:59:26.347614 20347 data_reader.cpp:288] Starting prefetch of epoch 11
I0815 21:59:32.306257 20340 solver.cpp:312] Iteration 19200 (5.13314 iter/s, 19.4813s/100 iter), loss = 0.0899752
I0815 21:59:32.306284 20340 solver.cpp:334]     Train net output #0: loss = 0.0899751 (* 1 = 0.0899751 loss)
I0815 21:59:32.306290 20340 sgd_solver.cpp:136] Iteration 19200, lr = 1e-05, m = 0.9
I0815 21:59:51.909133 20340 solver.cpp:312] Iteration 19300 (5.10143 iter/s, 19.6023s/100 iter), loss = 0.0984874
I0815 21:59:51.909204 20340 solver.cpp:334]     Train net output #0: loss = 0.0984873 (* 1 = 0.0984873 loss)
I0815 21:59:51.909211 20340 sgd_solver.cpp:136] Iteration 19300, lr = 1e-05, m = 0.9
I0815 21:59:58.405031 20314 data_reader.cpp:288] Starting prefetch of epoch 13
I0815 22:00:11.192304 20340 solver.cpp:312] Iteration 19400 (5.18601 iter/s, 19.2826s/100 iter), loss = 0.0957336
I0815 22:00:11.192328 20340 solver.cpp:334]     Train net output #0: loss = 0.0957335 (* 1 = 0.0957335 loss)
I0815 22:00:11.192333 20340 sgd_solver.cpp:136] Iteration 19400, lr = 1e-05, m = 0.9
I0815 22:00:30.523510 20340 solver.cpp:312] Iteration 19500 (5.17313 iter/s, 19.3307s/100 iter), loss = 0.0758252
I0815 22:00:30.523603 20340 solver.cpp:334]     Train net output #0: loss = 0.0758251 (* 1 = 0.0758251 loss)
I0815 22:00:30.523612 20340 sgd_solver.cpp:136] Iteration 19500, lr = 1e-05, m = 0.9
I0815 22:00:49.775473 20340 solver.cpp:312] Iteration 19600 (5.19442 iter/s, 19.2514s/100 iter), loss = 0.0855744
I0815 22:00:49.775497 20340 solver.cpp:334]     Train net output #0: loss = 0.0855743 (* 1 = 0.0855743 loss)
I0815 22:00:49.775503 20340 sgd_solver.cpp:136] Iteration 19600, lr = 1e-05, m = 0.9
I0815 22:01:02.425889 20316 data_reader.cpp:288] Starting prefetch of epoch 12
I0815 22:01:09.278808 20340 solver.cpp:312] Iteration 19700 (5.12747 iter/s, 19.5028s/100 iter), loss = 0.413458
I0815 22:01:09.278834 20340 solver.cpp:334]     Train net output #0: loss = 0.413458 (* 1 = 0.413458 loss)
I0815 22:01:09.278841 20340 sgd_solver.cpp:136] Iteration 19700, lr = 1e-05, m = 0.9
I0815 22:01:28.862474 20340 solver.cpp:312] Iteration 19800 (5.10644 iter/s, 19.5831s/100 iter), loss = 0.0701182
I0815 22:01:28.862498 20340 solver.cpp:334]     Train net output #0: loss = 0.0701181 (* 1 = 0.0701181 loss)
I0815 22:01:28.862504 20340 sgd_solver.cpp:136] Iteration 19800, lr = 1e-05, m = 0.9
I0815 22:01:49.119892 20340 solver.cpp:312] Iteration 19900 (4.9366 iter/s, 20.2569s/100 iter), loss = 0.0664639
I0815 22:01:49.119946 20340 solver.cpp:334]     Train net output #0: loss = 0.0664638 (* 1 = 0.0664638 loss)
I0815 22:01:49.119951 20340 sgd_solver.cpp:136] Iteration 19900, lr = 1e-05, m = 0.9
I0815 22:02:07.906409 20348 data_reader.cpp:288] Starting prefetch of epoch 13
I0815 22:02:08.712378 20340 solver.cpp:639] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/l1reg/cityscapes5_jsegnet21v2_iter_20000.caffemodel
I0815 22:02:08.762676 20340 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/l1reg/cityscapes5_jsegnet21v2_iter_20000.solverstate
I0815 22:02:08.772105 20340 solver.cpp:509] Iteration 20000, Testing net (#0)
I0815 22:02:12.329313 20389 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 22:02:20.591318 20340 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.950655
I0815 22:02:20.591392 20340 solver.cpp:594]     Test net output #1: accuracy/top5 = 1
I0815 22:02:20.591401 20340 solver.cpp:594]     Test net output #2: loss = 0.155207 (* 1 = 0.155207 loss)
I0815 22:02:20.591426 20340 solver.cpp:264] [MultiGPU] Tests completed in 11.819s
I0815 22:02:20.793279 20340 solver.cpp:312] Iteration 20000 (3.15731 iter/s, 31.6725s/100 iter), loss = 0.0496259
I0815 22:02:20.793304 20340 solver.cpp:334]     Train net output #0: loss = 0.0496258 (* 1 = 0.0496258 loss)
I0815 22:02:20.793308 20340 sgd_solver.cpp:136] Iteration 20000, lr = 1e-05, m = 0.9
I0815 22:02:40.202034 20340 solver.cpp:312] Iteration 20100 (5.15246 iter/s, 19.4082s/100 iter), loss = 0.0427253
I0815 22:02:40.202056 20340 solver.cpp:334]     Train net output #0: loss = 0.0427252 (* 1 = 0.0427252 loss)
I0815 22:02:40.202062 20340 sgd_solver.cpp:136] Iteration 20100, lr = 1e-05, m = 0.9
I0815 22:02:51.756261 20347 data_reader.cpp:288] Starting prefetch of epoch 12
I0815 22:02:59.521330 20340 solver.cpp:312] Iteration 20200 (5.17632 iter/s, 19.3188s/100 iter), loss = 0.0955568
I0815 22:02:59.521376 20340 solver.cpp:334]     Train net output #0: loss = 0.0955567 (* 1 = 0.0955567 loss)
I0815 22:02:59.521390 20340 sgd_solver.cpp:136] Iteration 20200, lr = 1e-05, m = 0.9
I0815 22:03:18.780706 20340 solver.cpp:312] Iteration 20300 (5.19242 iter/s, 19.2588s/100 iter), loss = 0.0656151
I0815 22:03:18.780730 20340 solver.cpp:334]     Train net output #0: loss = 0.0656149 (* 1 = 0.0656149 loss)
I0815 22:03:18.780733 20340 sgd_solver.cpp:136] Iteration 20300, lr = 1e-05, m = 0.9
I0815 22:03:38.576925 20340 solver.cpp:312] Iteration 20400 (5.05161 iter/s, 19.7957s/100 iter), loss = 0.0735535
I0815 22:03:38.577006 20340 solver.cpp:334]     Train net output #0: loss = 0.0735534 (* 1 = 0.0735534 loss)
I0815 22:03:38.577011 20340 sgd_solver.cpp:136] Iteration 20400, lr = 1e-05, m = 0.9
I0815 22:03:56.354912 20347 data_reader.cpp:288] Starting prefetch of epoch 13
I0815 22:03:58.111500 20340 solver.cpp:312] Iteration 20500 (5.11927 iter/s, 19.534s/100 iter), loss = 0.0594301
I0815 22:03:58.111522 20340 solver.cpp:334]     Train net output #0: loss = 0.0594299 (* 1 = 0.0594299 loss)
I0815 22:03:58.111528 20340 sgd_solver.cpp:136] Iteration 20500, lr = 1e-05, m = 0.9
I0815 22:04:17.643035 20340 solver.cpp:312] Iteration 20600 (5.12007 iter/s, 19.531s/100 iter), loss = 0.0590846
I0815 22:04:17.643086 20340 solver.cpp:334]     Train net output #0: loss = 0.0590845 (* 1 = 0.0590845 loss)
I0815 22:04:17.643091 20340 sgd_solver.cpp:136] Iteration 20600, lr = 1e-05, m = 0.9
I0815 22:04:28.562645 20343 data_reader.cpp:288] Starting prefetch of epoch 11
I0815 22:04:37.012163 20340 solver.cpp:312] Iteration 20700 (5.163 iter/s, 19.3686s/100 iter), loss = 0.0488809
I0815 22:04:37.012188 20340 solver.cpp:334]     Train net output #0: loss = 0.0488807 (* 1 = 0.0488807 loss)
I0815 22:04:37.012194 20340 sgd_solver.cpp:136] Iteration 20700, lr = 1e-05, m = 0.9
I0815 22:04:56.424635 20340 solver.cpp:312] Iteration 20800 (5.15147 iter/s, 19.4119s/100 iter), loss = 0.0438402
I0815 22:04:56.424688 20340 solver.cpp:334]     Train net output #0: loss = 0.04384 (* 1 = 0.04384 loss)
I0815 22:04:56.424695 20340 sgd_solver.cpp:136] Iteration 20800, lr = 1e-05, m = 0.9
I0815 22:05:15.932956 20340 solver.cpp:312] Iteration 20900 (5.12616 iter/s, 19.5078s/100 iter), loss = 0.0521751
I0815 22:05:15.932986 20340 solver.cpp:334]     Train net output #0: loss = 0.0521749 (* 1 = 0.0521749 loss)
I0815 22:05:15.932992 20340 sgd_solver.cpp:136] Iteration 20900, lr = 1e-05, m = 0.9
I0815 22:05:32.497756 20347 data_reader.cpp:288] Starting prefetch of epoch 14
I0815 22:05:34.959592 20340 solver.cpp:312] Iteration 21000 (5.25593 iter/s, 19.0261s/100 iter), loss = 0.0762028
I0815 22:05:34.959619 20340 solver.cpp:334]     Train net output #0: loss = 0.0762027 (* 1 = 0.0762027 loss)
I0815 22:05:34.959626 20340 sgd_solver.cpp:136] Iteration 21000, lr = 1e-05, m = 0.9
I0815 22:05:54.157135 20340 solver.cpp:312] Iteration 21100 (5.20914 iter/s, 19.197s/100 iter), loss = 0.0607682
I0815 22:05:54.157158 20340 solver.cpp:334]     Train net output #0: loss = 0.060768 (* 1 = 0.060768 loss)
I0815 22:05:54.157166 20340 sgd_solver.cpp:136] Iteration 21100, lr = 1e-05, m = 0.9
I0815 22:06:13.660311 20340 solver.cpp:312] Iteration 21200 (5.12751 iter/s, 19.5026s/100 iter), loss = 0.06693
I0815 22:06:13.660362 20340 solver.cpp:334]     Train net output #0: loss = 0.0669299 (* 1 = 0.0669299 loss)
I0815 22:06:13.660368 20340 sgd_solver.cpp:136] Iteration 21200, lr = 1e-05, m = 0.9
I0815 22:06:33.177750 20340 solver.cpp:312] Iteration 21300 (5.12376 iter/s, 19.5169s/100 iter), loss = 0.0820231
I0815 22:06:33.177778 20340 solver.cpp:334]     Train net output #0: loss = 0.082023 (* 1 = 0.082023 loss)
I0815 22:06:33.177784 20340 sgd_solver.cpp:136] Iteration 21300, lr = 1e-05, m = 0.9
I0815 22:06:36.597865 20316 data_reader.cpp:288] Starting prefetch of epoch 13
I0815 22:06:52.449029 20340 solver.cpp:312] Iteration 21400 (5.18921 iter/s, 19.2708s/100 iter), loss = 0.060342
I0815 22:06:52.449108 20340 solver.cpp:334]     Train net output #0: loss = 0.0603419 (* 1 = 0.0603419 loss)
I0815 22:06:52.449116 20340 sgd_solver.cpp:136] Iteration 21400, lr = 1e-05, m = 0.9
I0815 22:07:08.778952 20344 data_reader.cpp:288] Starting prefetch of epoch 15
I0815 22:07:12.021885 20340 solver.cpp:312] Iteration 21500 (5.10926 iter/s, 19.5723s/100 iter), loss = 0.0496411
I0815 22:07:12.021909 20340 solver.cpp:334]     Train net output #0: loss = 0.049641 (* 1 = 0.049641 loss)
I0815 22:07:12.021916 20340 sgd_solver.cpp:136] Iteration 21500, lr = 1e-05, m = 0.9
I0815 22:07:31.756603 20340 solver.cpp:312] Iteration 21600 (5.06735 iter/s, 19.7342s/100 iter), loss = 0.0783045
I0815 22:07:31.756654 20340 solver.cpp:334]     Train net output #0: loss = 0.0783044 (* 1 = 0.0783044 loss)
I0815 22:07:31.756660 20340 sgd_solver.cpp:136] Iteration 21600, lr = 1e-05, m = 0.9
I0815 22:07:51.060309 20340 solver.cpp:312] Iteration 21700 (5.18049 iter/s, 19.3032s/100 iter), loss = 0.0596376
I0815 22:07:51.060331 20340 solver.cpp:334]     Train net output #0: loss = 0.0596375 (* 1 = 0.0596375 loss)
I0815 22:07:51.060336 20340 sgd_solver.cpp:136] Iteration 21700, lr = 1e-05, m = 0.9
I0815 22:08:10.644134 20340 solver.cpp:312] Iteration 21800 (5.1064 iter/s, 19.5833s/100 iter), loss = 0.0668501
I0815 22:08:10.644179 20340 solver.cpp:334]     Train net output #0: loss = 0.06685 (* 1 = 0.06685 loss)
I0815 22:08:10.644186 20340 sgd_solver.cpp:136] Iteration 21800, lr = 1e-05, m = 0.9
I0815 22:08:13.171921 20314 data_reader.cpp:288] Starting prefetch of epoch 14
I0815 22:08:30.026271 20340 solver.cpp:312] Iteration 21900 (5.15953 iter/s, 19.3816s/100 iter), loss = 0.0672619
I0815 22:08:30.026296 20340 solver.cpp:334]     Train net output #0: loss = 0.0672618 (* 1 = 0.0672618 loss)
I0815 22:08:30.026304 20340 sgd_solver.cpp:136] Iteration 21900, lr = 1e-05, m = 0.9
I0815 22:08:49.162571 20340 solver.cpp:509] Iteration 22000, Testing net (#0)
I0815 22:08:56.525483 20389 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 22:09:01.167242 20340 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.952361
I0815 22:09:01.167268 20340 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999535
I0815 22:09:01.167277 20340 solver.cpp:594]     Test net output #2: loss = 0.183424 (* 1 = 0.183424 loss)
I0815 22:09:01.167332 20340 solver.cpp:264] [MultiGPU] Tests completed in 12.0044s
I0815 22:09:01.384243 20340 solver.cpp:312] Iteration 22000 (3.18907 iter/s, 31.3571s/100 iter), loss = 0.0870393
I0815 22:09:01.384268 20340 solver.cpp:334]     Train net output #0: loss = 0.0870392 (* 1 = 0.0870392 loss)
I0815 22:09:01.384274 20340 sgd_solver.cpp:136] Iteration 22000, lr = 1e-05, m = 0.9
I0815 22:09:20.811606 20340 solver.cpp:312] Iteration 22100 (5.14752 iter/s, 19.4268s/100 iter), loss = 0.0597997
I0815 22:09:20.811668 20340 solver.cpp:334]     Train net output #0: loss = 0.0597996 (* 1 = 0.0597996 loss)
I0815 22:09:20.811674 20340 sgd_solver.cpp:136] Iteration 22100, lr = 1e-05, m = 0.9
I0815 22:09:29.444658 20343 data_reader.cpp:288] Starting prefetch of epoch 12
I0815 22:09:40.206831 20340 solver.cpp:312] Iteration 22200 (5.15605 iter/s, 19.3947s/100 iter), loss = 0.0774289
I0815 22:09:40.206883 20340 solver.cpp:334]     Train net output #0: loss = 0.0774288 (* 1 = 0.0774288 loss)
I0815 22:09:40.206897 20340 sgd_solver.cpp:136] Iteration 22200, lr = 1e-05, m = 0.9
I0815 22:09:59.623039 20340 solver.cpp:312] Iteration 22300 (5.15048 iter/s, 19.4157s/100 iter), loss = 0.0496672
I0815 22:09:59.623087 20340 solver.cpp:334]     Train net output #0: loss = 0.0496671 (* 1 = 0.0496671 loss)
I0815 22:09:59.623095 20340 sgd_solver.cpp:136] Iteration 22300, lr = 1e-05, m = 0.9
I0815 22:10:19.056504 20340 solver.cpp:312] Iteration 22400 (5.1459 iter/s, 19.4329s/100 iter), loss = 0.0538586
I0815 22:10:19.056526 20340 solver.cpp:334]     Train net output #0: loss = 0.0538585 (* 1 = 0.0538585 loss)
I0815 22:10:19.056531 20340 sgd_solver.cpp:136] Iteration 22400, lr = 1e-05, m = 0.9
I0815 22:10:33.685928 20347 data_reader.cpp:288] Starting prefetch of epoch 15
I0815 22:10:38.718520 20340 solver.cpp:312] Iteration 22500 (5.08609 iter/s, 19.6615s/100 iter), loss = 0.0693155
I0815 22:10:38.718544 20340 solver.cpp:334]     Train net output #0: loss = 0.0693154 (* 1 = 0.0693154 loss)
I0815 22:10:38.718549 20340 sgd_solver.cpp:136] Iteration 22500, lr = 1e-05, m = 0.9
I0815 22:10:58.059425 20340 solver.cpp:312] Iteration 22600 (5.17053 iter/s, 19.3404s/100 iter), loss = 0.0467074
I0815 22:10:58.059447 20340 solver.cpp:334]     Train net output #0: loss = 0.0467073 (* 1 = 0.0467073 loss)
I0815 22:10:58.059453 20340 sgd_solver.cpp:136] Iteration 22600, lr = 1e-05, m = 0.9
I0815 22:11:17.471494 20340 solver.cpp:312] Iteration 22700 (5.15158 iter/s, 19.4115s/100 iter), loss = 0.0760816
I0815 22:11:17.471549 20340 solver.cpp:334]     Train net output #0: loss = 0.0760815 (* 1 = 0.0760815 loss)
I0815 22:11:17.471555 20340 sgd_solver.cpp:136] Iteration 22700, lr = 1e-05, m = 0.9
I0815 22:11:36.916137 20340 solver.cpp:312] Iteration 22800 (5.14295 iter/s, 19.4441s/100 iter), loss = 0.0443761
I0815 22:11:36.916162 20340 solver.cpp:334]     Train net output #0: loss = 0.0443761 (* 1 = 0.0443761 loss)
I0815 22:11:36.916168 20340 sgd_solver.cpp:136] Iteration 22800, lr = 1e-05, m = 0.9
I0815 22:11:37.899402 20316 data_reader.cpp:288] Starting prefetch of epoch 14
I0815 22:11:56.505901 20340 solver.cpp:312] Iteration 22900 (5.10485 iter/s, 19.5892s/100 iter), loss = 0.0744037
I0815 22:11:56.505964 20340 solver.cpp:334]     Train net output #0: loss = 0.0744036 (* 1 = 0.0744036 loss)
I0815 22:11:56.505970 20340 sgd_solver.cpp:136] Iteration 22900, lr = 1e-05, m = 0.9
I0815 22:12:10.373841 20314 data_reader.cpp:288] Starting prefetch of epoch 15
I0815 22:12:16.110270 20340 solver.cpp:312] Iteration 23000 (5.10104 iter/s, 19.6038s/100 iter), loss = 0.0867242
I0815 22:12:16.110294 20340 solver.cpp:334]     Train net output #0: loss = 0.0867241 (* 1 = 0.0867241 loss)
I0815 22:12:16.110301 20340 sgd_solver.cpp:136] Iteration 23000, lr = 1e-05, m = 0.9
I0815 22:12:35.688491 20340 solver.cpp:312] Iteration 23100 (5.10786 iter/s, 19.5777s/100 iter), loss = 0.0581464
I0815 22:12:35.688575 20340 solver.cpp:334]     Train net output #0: loss = 0.0581464 (* 1 = 0.0581464 loss)
I0815 22:12:35.688582 20340 sgd_solver.cpp:136] Iteration 23100, lr = 1e-05, m = 0.9
I0815 22:12:55.341956 20340 solver.cpp:312] Iteration 23200 (5.0883 iter/s, 19.6529s/100 iter), loss = 0.0512201
I0815 22:12:55.341981 20340 solver.cpp:334]     Train net output #0: loss = 0.05122 (* 1 = 0.05122 loss)
I0815 22:12:55.341985 20340 sgd_solver.cpp:136] Iteration 23200, lr = 1e-05, m = 0.9
I0815 22:13:14.760393 20340 solver.cpp:312] Iteration 23300 (5.14989 iter/s, 19.4179s/100 iter), loss = 0.0418441
I0815 22:13:14.760465 20340 solver.cpp:334]     Train net output #0: loss = 0.0418441 (* 1 = 0.0418441 loss)
I0815 22:13:14.760473 20340 sgd_solver.cpp:136] Iteration 23300, lr = 1e-05, m = 0.9
I0815 22:13:14.978732 20316 data_reader.cpp:288] Starting prefetch of epoch 15
I0815 22:13:34.263622 20340 solver.cpp:312] Iteration 23400 (5.1275 iter/s, 19.5027s/100 iter), loss = 0.0540716
I0815 22:13:34.263645 20340 solver.cpp:334]     Train net output #0: loss = 0.0540715 (* 1 = 0.0540715 loss)
I0815 22:13:34.263649 20340 sgd_solver.cpp:136] Iteration 23400, lr = 1e-05, m = 0.9
I0815 22:13:53.613840 20340 solver.cpp:312] Iteration 23500 (5.16804 iter/s, 19.3497s/100 iter), loss = 0.205649
I0815 22:13:53.613914 20340 solver.cpp:334]     Train net output #0: loss = 0.205649 (* 1 = 0.205649 loss)
I0815 22:13:53.613919 20340 sgd_solver.cpp:136] Iteration 23500, lr = 1e-05, m = 0.9
I0815 22:14:13.048552 20340 solver.cpp:312] Iteration 23600 (5.14557 iter/s, 19.4342s/100 iter), loss = 0.0442324
I0815 22:14:13.048573 20340 solver.cpp:334]     Train net output #0: loss = 0.0442323 (* 1 = 0.0442323 loss)
I0815 22:14:13.048579 20340 sgd_solver.cpp:136] Iteration 23600, lr = 1e-05, m = 0.9
I0815 22:14:19.050833 20347 data_reader.cpp:288] Starting prefetch of epoch 16
I0815 22:14:32.408994 20340 solver.cpp:312] Iteration 23700 (5.16531 iter/s, 19.3599s/100 iter), loss = 0.0640697
I0815 22:14:32.409071 20340 solver.cpp:334]     Train net output #0: loss = 0.0640696 (* 1 = 0.0640696 loss)
I0815 22:14:32.409080 20340 sgd_solver.cpp:136] Iteration 23700, lr = 1e-05, m = 0.9
I0815 22:14:51.282626 20344 data_reader.cpp:288] Starting prefetch of epoch 16
I0815 22:14:51.995898 20340 solver.cpp:312] Iteration 23800 (5.10559 iter/s, 19.5864s/100 iter), loss = 0.094189
I0815 22:14:51.995921 20340 solver.cpp:334]     Train net output #0: loss = 0.0941889 (* 1 = 0.0941889 loss)
I0815 22:14:51.995929 20340 sgd_solver.cpp:136] Iteration 23800, lr = 1e-05, m = 0.9
I0815 22:15:11.188769 20340 solver.cpp:312] Iteration 23900 (5.21041 iter/s, 19.1924s/100 iter), loss = 0.0615151
I0815 22:15:11.188818 20340 solver.cpp:334]     Train net output #0: loss = 0.061515 (* 1 = 0.061515 loss)
I0815 22:15:11.188825 20340 sgd_solver.cpp:136] Iteration 23900, lr = 1e-05, m = 0.9
I0815 22:15:30.388159 20340 solver.cpp:509] Iteration 24000, Testing net (#0)
I0815 22:15:33.902541 20389 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 22:15:42.710198 20340 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.951106
I0815 22:15:42.710252 20340 solver.cpp:594]     Test net output #1: accuracy/top5 = 1
I0815 22:15:42.710258 20340 solver.cpp:594]     Test net output #2: loss = 0.155748 (* 1 = 0.155748 loss)
I0815 22:15:42.710285 20340 solver.cpp:264] [MultiGPU] Tests completed in 12.3218s
I0815 22:15:42.799706 20394 sgd_solver.cpp:48] MultiStep Status: Iteration 24000, step = 1
I0815 22:15:42.799706 20393 sgd_solver.cpp:48] MultiStep Status: Iteration 24000, step = 1
I0815 22:15:42.799706 20395 sgd_solver.cpp:48] MultiStep Status: Iteration 24000, step = 1
I0815 22:15:42.910908 20340 solver.cpp:312] Iteration 24000 (3.15246 iter/s, 31.7213s/100 iter), loss = 0.0837923
I0815 22:15:42.910936 20340 solver.cpp:334]     Train net output #0: loss = 0.0837922 (* 1 = 0.0837922 loss)
I0815 22:15:42.910940 20340 sgd_solver.cpp:136] Iteration 24000, lr = 1e-06, m = 0.9
I0815 22:16:02.377009 20340 solver.cpp:312] Iteration 24100 (5.13728 iter/s, 19.4656s/100 iter), loss = 0.0733487
I0815 22:16:02.377030 20340 solver.cpp:334]     Train net output #0: loss = 0.0733486 (* 1 = 0.0733486 loss)
I0815 22:16:02.377034 20340 sgd_solver.cpp:136] Iteration 24100, lr = 1e-06, m = 0.9
I0815 22:16:07.545810 20348 data_reader.cpp:288] Starting prefetch of epoch 14
I0815 22:16:21.720396 20340 solver.cpp:312] Iteration 24200 (5.16987 iter/s, 19.3429s/100 iter), loss = 0.0984448
I0815 22:16:21.720453 20340 solver.cpp:334]     Train net output #0: loss = 0.0984447 (* 1 = 0.0984447 loss)
I0815 22:16:21.720460 20340 sgd_solver.cpp:136] Iteration 24200, lr = 1e-06, m = 0.9
I0815 22:16:39.794972 20344 data_reader.cpp:288] Starting prefetch of epoch 17
I0815 22:16:41.339844 20340 solver.cpp:312] Iteration 24300 (5.09712 iter/s, 19.6189s/100 iter), loss = 0.0563466
I0815 22:16:41.339867 20340 solver.cpp:334]     Train net output #0: loss = 0.0563465 (* 1 = 0.0563465 loss)
I0815 22:16:41.339872 20340 sgd_solver.cpp:136] Iteration 24300, lr = 1e-06, m = 0.9
I0815 22:17:00.812769 20340 solver.cpp:312] Iteration 24400 (5.13548 iter/s, 19.4724s/100 iter), loss = 0.0737933
I0815 22:17:00.812855 20340 solver.cpp:334]     Train net output #0: loss = 0.0737932 (* 1 = 0.0737932 loss)
I0815 22:17:00.812862 20340 sgd_solver.cpp:136] Iteration 24400, lr = 1e-06, m = 0.9
I0815 22:17:20.270941 20340 solver.cpp:312] Iteration 24500 (5.13937 iter/s, 19.4576s/100 iter), loss = 0.0552021
I0815 22:17:20.270964 20340 solver.cpp:334]     Train net output #0: loss = 0.055202 (* 1 = 0.055202 loss)
I0815 22:17:20.270972 20340 sgd_solver.cpp:136] Iteration 24500, lr = 1e-06, m = 0.9
I0815 22:17:39.834749 20340 solver.cpp:312] Iteration 24600 (5.11162 iter/s, 19.5633s/100 iter), loss = 0.095897
I0815 22:17:39.834805 20340 solver.cpp:334]     Train net output #0: loss = 0.0958969 (* 1 = 0.0958969 loss)
I0815 22:17:39.834811 20340 sgd_solver.cpp:136] Iteration 24600, lr = 1e-06, m = 0.9
I0815 22:17:44.395795 20343 data_reader.cpp:288] Starting prefetch of epoch 13
I0815 22:17:59.419193 20340 solver.cpp:312] Iteration 24700 (5.10623 iter/s, 19.5839s/100 iter), loss = 0.0697245
I0815 22:17:59.419217 20340 solver.cpp:334]     Train net output #0: loss = 0.0697244 (* 1 = 0.0697244 loss)
I0815 22:17:59.419221 20340 sgd_solver.cpp:136] Iteration 24700, lr = 1e-06, m = 0.9
I0815 22:18:18.745676 20340 solver.cpp:312] Iteration 24800 (5.17439 iter/s, 19.326s/100 iter), loss = 0.095143
I0815 22:18:18.745751 20340 solver.cpp:334]     Train net output #0: loss = 0.0951429 (* 1 = 0.0951429 loss)
I0815 22:18:18.745759 20340 sgd_solver.cpp:136] Iteration 24800, lr = 1e-06, m = 0.9
I0815 22:18:38.808868 20340 solver.cpp:312] Iteration 24900 (4.98439 iter/s, 20.0626s/100 iter), loss = 0.0641684
I0815 22:18:38.808892 20340 solver.cpp:334]     Train net output #0: loss = 0.0641683 (* 1 = 0.0641683 loss)
I0815 22:18:38.808897 20340 sgd_solver.cpp:136] Iteration 24900, lr = 1e-06, m = 0.9
I0815 22:18:49.493021 20316 data_reader.cpp:288] Starting prefetch of epoch 16
I0815 22:18:58.674554 20340 solver.cpp:312] Iteration 25000 (5.03394 iter/s, 19.8651s/100 iter), loss = 0.0744963
I0815 22:18:58.674578 20340 solver.cpp:334]     Train net output #0: loss = 0.0744962 (* 1 = 0.0744962 loss)
I0815 22:18:58.674584 20340 sgd_solver.cpp:136] Iteration 25000, lr = 1e-06, m = 0.9
I0815 22:19:17.933603 20340 solver.cpp:312] Iteration 25100 (5.19251 iter/s, 19.2585s/100 iter), loss = 0.0591091
I0815 22:19:17.933631 20340 solver.cpp:334]     Train net output #0: loss = 0.059109 (* 1 = 0.059109 loss)
I0815 22:19:17.933639 20340 sgd_solver.cpp:136] Iteration 25100, lr = 1e-06, m = 0.9
I0815 22:19:21.731133 20344 data_reader.cpp:288] Starting prefetch of epoch 18
I0815 22:19:37.511253 20340 solver.cpp:312] Iteration 25200 (5.108 iter/s, 19.5771s/100 iter), loss = 0.0868782
I0815 22:19:37.511281 20340 solver.cpp:334]     Train net output #0: loss = 0.0868781 (* 1 = 0.0868781 loss)
I0815 22:19:37.511287 20340 sgd_solver.cpp:136] Iteration 25200, lr = 1e-06, m = 0.9
I0815 22:19:56.976060 20340 solver.cpp:312] Iteration 25300 (5.13762 iter/s, 19.4643s/100 iter), loss = 0.0825749
I0815 22:19:56.976111 20340 solver.cpp:334]     Train net output #0: loss = 0.0825748 (* 1 = 0.0825748 loss)
I0815 22:19:56.976119 20340 sgd_solver.cpp:136] Iteration 25300, lr = 1e-06, m = 0.9
I0815 22:20:16.314599 20340 solver.cpp:312] Iteration 25400 (5.17116 iter/s, 19.338s/100 iter), loss = 0.0750396
I0815 22:20:16.314625 20340 solver.cpp:334]     Train net output #0: loss = 0.0750395 (* 1 = 0.0750395 loss)
I0815 22:20:16.314632 20340 sgd_solver.cpp:136] Iteration 25400, lr = 1e-06, m = 0.9
I0815 22:20:25.908221 20347 data_reader.cpp:288] Starting prefetch of epoch 17
I0815 22:20:35.801107 20340 solver.cpp:312] Iteration 25500 (5.1319 iter/s, 19.486s/100 iter), loss = 0.0534591
I0815 22:20:35.801214 20340 solver.cpp:334]     Train net output #0: loss = 0.053459 (* 1 = 0.053459 loss)
I0815 22:20:35.801223 20340 sgd_solver.cpp:136] Iteration 25500, lr = 1e-06, m = 0.9
I0815 22:20:55.368726 20340 solver.cpp:312] Iteration 25600 (5.11062 iter/s, 19.5671s/100 iter), loss = 0.0635905
I0815 22:20:55.368747 20340 solver.cpp:334]     Train net output #0: loss = 0.0635904 (* 1 = 0.0635904 loss)
I0815 22:20:55.368753 20340 sgd_solver.cpp:136] Iteration 25600, lr = 1e-06, m = 0.9
I0815 22:21:14.512449 20340 solver.cpp:312] Iteration 25700 (5.22379 iter/s, 19.1432s/100 iter), loss = 0.0463915
I0815 22:21:14.512500 20340 solver.cpp:334]     Train net output #0: loss = 0.0463914 (* 1 = 0.0463914 loss)
I0815 22:21:14.512504 20340 sgd_solver.cpp:136] Iteration 25700, lr = 1e-06, m = 0.9
I0815 22:21:29.986714 20347 data_reader.cpp:288] Starting prefetch of epoch 18
I0815 22:21:33.836220 20340 solver.cpp:312] Iteration 25800 (5.17512 iter/s, 19.3232s/100 iter), loss = 0.0699915
I0815 22:21:33.836246 20340 solver.cpp:334]     Train net output #0: loss = 0.0699914 (* 1 = 0.0699914 loss)
I0815 22:21:33.836251 20340 sgd_solver.cpp:136] Iteration 25800, lr = 1e-06, m = 0.9
I0815 22:21:53.215855 20340 solver.cpp:312] Iteration 25900 (5.1602 iter/s, 19.3791s/100 iter), loss = 0.0557967
I0815 22:21:53.215935 20340 solver.cpp:334]     Train net output #0: loss = 0.0557966 (* 1 = 0.0557966 loss)
I0815 22:21:53.215950 20340 sgd_solver.cpp:136] Iteration 25900, lr = 1e-06, m = 0.9
I0815 22:22:02.106600 20343 data_reader.cpp:288] Starting prefetch of epoch 14
I0815 22:22:12.651248 20340 solver.cpp:509] Iteration 26000, Testing net (#0)
I0815 22:22:24.555025 20340 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.952398
I0815 22:22:24.555124 20340 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999381
I0815 22:22:24.555136 20340 solver.cpp:594]     Test net output #2: loss = 0.18951 (* 1 = 0.18951 loss)
I0815 22:22:24.555199 20340 solver.cpp:264] [MultiGPU] Tests completed in 11.9036s
I0815 22:22:24.750391 20340 solver.cpp:312] Iteration 26000 (3.17121 iter/s, 31.5337s/100 iter), loss = 0.0852029
I0815 22:22:24.750414 20340 solver.cpp:334]     Train net output #0: loss = 0.0852028 (* 1 = 0.0852028 loss)
I0815 22:22:24.750421 20340 sgd_solver.cpp:136] Iteration 26000, lr = 1e-06, m = 0.9
I0815 22:22:44.310104 20340 solver.cpp:312] Iteration 26100 (5.11269 iter/s, 19.5592s/100 iter), loss = 0.0439342
I0815 22:22:44.310130 20340 solver.cpp:334]     Train net output #0: loss = 0.0439341 (* 1 = 0.0439341 loss)
I0815 22:22:44.310137 20340 sgd_solver.cpp:136] Iteration 26100, lr = 1e-06, m = 0.9
I0815 22:22:46.268014 20343 data_reader.cpp:288] Starting prefetch of epoch 15
I0815 22:23:03.708930 20340 solver.cpp:312] Iteration 26200 (5.15509 iter/s, 19.3983s/100 iter), loss = 0.0908666
I0815 22:23:03.709002 20340 solver.cpp:334]     Train net output #0: loss = 0.0908665 (* 1 = 0.0908665 loss)
I0815 22:23:03.709007 20340 sgd_solver.cpp:136] Iteration 26200, lr = 1e-06, m = 0.9
I0815 22:23:23.202327 20340 solver.cpp:312] Iteration 26300 (5.13008 iter/s, 19.4929s/100 iter), loss = 0.0796469
I0815 22:23:23.202352 20340 solver.cpp:334]     Train net output #0: loss = 0.0796468 (* 1 = 0.0796468 loss)
I0815 22:23:23.202356 20340 sgd_solver.cpp:136] Iteration 26300, lr = 1e-06, m = 0.9
I0815 22:23:42.695250 20340 solver.cpp:312] Iteration 26400 (5.13021 iter/s, 19.4924s/100 iter), loss = 0.0893417
I0815 22:23:42.695294 20340 solver.cpp:334]     Train net output #0: loss = 0.0893415 (* 1 = 0.0893415 loss)
I0815 22:23:42.695302 20340 sgd_solver.cpp:136] Iteration 26400, lr = 1e-06, m = 0.9
I0815 22:23:50.590639 20348 data_reader.cpp:288] Starting prefetch of epoch 15
I0815 22:24:02.215854 20340 solver.cpp:312] Iteration 26500 (5.12293 iter/s, 19.5201s/100 iter), loss = 0.0461657
I0815 22:24:02.215876 20340 solver.cpp:334]     Train net output #0: loss = 0.0461656 (* 1 = 0.0461656 loss)
I0815 22:24:02.215883 20340 sgd_solver.cpp:136] Iteration 26500, lr = 1e-06, m = 0.9
I0815 22:24:21.706744 20340 solver.cpp:312] Iteration 26600 (5.13074 iter/s, 19.4904s/100 iter), loss = 0.0786646
I0815 22:24:21.706810 20340 solver.cpp:334]     Train net output #0: loss = 0.0786645 (* 1 = 0.0786645 loss)
I0815 22:24:21.706815 20340 sgd_solver.cpp:136] Iteration 26600, lr = 1e-06, m = 0.9
I0815 22:24:22.933387 20343 data_reader.cpp:288] Starting prefetch of epoch 16
I0815 22:24:41.157234 20340 solver.cpp:312] Iteration 26700 (5.1414 iter/s, 19.45s/100 iter), loss = 0.0674228
I0815 22:24:41.157259 20340 solver.cpp:334]     Train net output #0: loss = 0.0674227 (* 1 = 0.0674227 loss)
I0815 22:24:41.157265 20340 sgd_solver.cpp:136] Iteration 26700, lr = 1e-06, m = 0.9
I0815 22:25:00.837994 20340 solver.cpp:312] Iteration 26800 (5.08124 iter/s, 19.6802s/100 iter), loss = 0.0851516
I0815 22:25:00.838078 20340 solver.cpp:334]     Train net output #0: loss = 0.0851515 (* 1 = 0.0851515 loss)
I0815 22:25:00.838086 20340 sgd_solver.cpp:136] Iteration 26800, lr = 1e-06, m = 0.9
I0815 22:25:20.232151 20340 solver.cpp:312] Iteration 26900 (5.15633 iter/s, 19.3936s/100 iter), loss = 0.0548329
I0815 22:25:20.232175 20340 solver.cpp:334]     Train net output #0: loss = 0.0548328 (* 1 = 0.0548328 loss)
I0815 22:25:20.232182 20340 sgd_solver.cpp:136] Iteration 26900, lr = 1e-06, m = 0.9
I0815 22:25:27.507388 20343 data_reader.cpp:288] Starting prefetch of epoch 17
I0815 22:25:39.795857 20340 solver.cpp:312] Iteration 27000 (5.11165 iter/s, 19.5632s/100 iter), loss = 0.107937
I0815 22:25:39.795904 20340 solver.cpp:334]     Train net output #0: loss = 0.107937 (* 1 = 0.107937 loss)
I0815 22:25:39.795909 20340 sgd_solver.cpp:136] Iteration 27000, lr = 1e-06, m = 0.9
I0815 22:25:59.325578 20340 solver.cpp:312] Iteration 27100 (5.12054 iter/s, 19.5292s/100 iter), loss = 0.0667986
I0815 22:25:59.325604 20340 solver.cpp:334]     Train net output #0: loss = 0.0667985 (* 1 = 0.0667985 loss)
I0815 22:25:59.325610 20340 sgd_solver.cpp:136] Iteration 27100, lr = 1e-06, m = 0.9
I0815 22:26:19.656445 20340 solver.cpp:312] Iteration 27200 (4.91876 iter/s, 20.3303s/100 iter), loss = 0.0433044
I0815 22:26:19.656502 20340 solver.cpp:334]     Train net output #0: loss = 0.0433042 (* 1 = 0.0433042 loss)
I0815 22:26:19.656507 20340 sgd_solver.cpp:136] Iteration 27200, lr = 1e-06, m = 0.9
I0815 22:26:32.593287 20348 data_reader.cpp:288] Starting prefetch of epoch 16
I0815 22:26:39.027997 20340 solver.cpp:312] Iteration 27300 (5.16235 iter/s, 19.371s/100 iter), loss = 0.0662793
I0815 22:26:39.028019 20340 solver.cpp:334]     Train net output #0: loss = 0.0662792 (* 1 = 0.0662792 loss)
I0815 22:26:39.028023 20340 sgd_solver.cpp:136] Iteration 27300, lr = 1e-06, m = 0.9
I0815 22:26:58.145345 20340 solver.cpp:312] Iteration 27400 (5.23099 iter/s, 19.1168s/100 iter), loss = 0.103427
I0815 22:26:58.145395 20340 solver.cpp:334]     Train net output #0: loss = 0.103427 (* 1 = 0.103427 loss)
I0815 22:26:58.145401 20340 sgd_solver.cpp:136] Iteration 27400, lr = 1e-06, m = 0.9
I0815 22:27:04.695806 20344 data_reader.cpp:288] Starting prefetch of epoch 19
I0815 22:27:17.730026 20340 solver.cpp:312] Iteration 27500 (5.10617 iter/s, 19.5841s/100 iter), loss = 0.0615918
I0815 22:27:17.730054 20340 solver.cpp:334]     Train net output #0: loss = 0.0615917 (* 1 = 0.0615917 loss)
I0815 22:27:17.730062 20340 sgd_solver.cpp:136] Iteration 27500, lr = 1e-06, m = 0.9
I0815 22:27:37.054550 20340 solver.cpp:312] Iteration 27600 (5.17491 iter/s, 19.324s/100 iter), loss = 0.0811987
I0815 22:27:37.054606 20340 solver.cpp:334]     Train net output #0: loss = 0.0811986 (* 1 = 0.0811986 loss)
I0815 22:27:37.054613 20340 sgd_solver.cpp:136] Iteration 27600, lr = 1e-06, m = 0.9
I0815 22:27:56.253489 20340 solver.cpp:312] Iteration 27700 (5.20876 iter/s, 19.1984s/100 iter), loss = 0.0820305
I0815 22:27:56.253513 20340 solver.cpp:334]     Train net output #0: loss = 0.0820304 (* 1 = 0.0820304 loss)
I0815 22:27:56.253520 20340 sgd_solver.cpp:136] Iteration 27700, lr = 1e-06, m = 0.9
I0815 22:28:08.465520 20343 data_reader.cpp:288] Starting prefetch of epoch 18
I0815 22:28:15.674504 20340 solver.cpp:312] Iteration 27800 (5.1492 iter/s, 19.4205s/100 iter), loss = 0.043198
I0815 22:28:15.674530 20340 solver.cpp:334]     Train net output #0: loss = 0.0431979 (* 1 = 0.0431979 loss)
I0815 22:28:15.674535 20340 sgd_solver.cpp:136] Iteration 27800, lr = 1e-06, m = 0.9
I0815 22:28:35.175464 20340 solver.cpp:312] Iteration 27900 (5.12809 iter/s, 19.5004s/100 iter), loss = 0.0571961
I0815 22:28:35.175487 20340 solver.cpp:334]     Train net output #0: loss = 0.057196 (* 1 = 0.057196 loss)
I0815 22:28:35.175490 20340 sgd_solver.cpp:136] Iteration 27900, lr = 1e-06, m = 0.9
I0815 22:28:54.466284 20340 solver.cpp:509] Iteration 28000, Testing net (#0)
I0815 22:28:58.036579 20391 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 22:29:06.398869 20340 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.951358
I0815 22:29:06.398893 20340 solver.cpp:594]     Test net output #1: accuracy/top5 = 1
I0815 22:29:06.398900 20340 solver.cpp:594]     Test net output #2: loss = 0.155065 (* 1 = 0.155065 loss)
I0815 22:29:06.399416 20340 solver.cpp:264] [MultiGPU] Tests completed in 11.9328s
I0815 22:29:06.589465 20340 solver.cpp:312] Iteration 28000 (3.18338 iter/s, 31.4131s/100 iter), loss = 0.0782925
I0815 22:29:06.589489 20340 solver.cpp:334]     Train net output #0: loss = 0.0782924 (* 1 = 0.0782924 loss)
I0815 22:29:06.589493 20340 sgd_solver.cpp:136] Iteration 28000, lr = 1e-06, m = 0.9
I0815 22:29:24.824710 20316 data_reader.cpp:288] Starting prefetch of epoch 17
I0815 22:29:26.020182 20340 solver.cpp:312] Iteration 28100 (5.14663 iter/s, 19.4302s/100 iter), loss = 0.075721
I0815 22:29:26.020205 20340 solver.cpp:334]     Train net output #0: loss = 0.0757209 (* 1 = 0.0757209 loss)
I0815 22:29:26.020210 20340 sgd_solver.cpp:136] Iteration 28100, lr = 1e-06, m = 0.9
I0815 22:29:45.540186 20340 solver.cpp:312] Iteration 28200 (5.12309 iter/s, 19.5195s/100 iter), loss = 0.0751694
I0815 22:29:45.540213 20340 solver.cpp:334]     Train net output #0: loss = 0.0751693 (* 1 = 0.0751693 loss)
I0815 22:29:45.540220 20340 sgd_solver.cpp:136] Iteration 28200, lr = 1e-06, m = 0.9
I0815 22:29:56.930387 20344 data_reader.cpp:288] Starting prefetch of epoch 20
I0815 22:30:04.770489 20340 solver.cpp:312] Iteration 28300 (5.20027 iter/s, 19.2298s/100 iter), loss = 0.0474279
I0815 22:30:04.770514 20340 solver.cpp:334]     Train net output #0: loss = 0.0474277 (* 1 = 0.0474277 loss)
I0815 22:30:04.770519 20340 sgd_solver.cpp:136] Iteration 28300, lr = 1e-06, m = 0.9
I0815 22:30:24.491544 20340 solver.cpp:312] Iteration 28400 (5.07086 iter/s, 19.7205s/100 iter), loss = 0.0843493
I0815 22:30:24.491570 20340 solver.cpp:334]     Train net output #0: loss = 0.0843492 (* 1 = 0.0843492 loss)
I0815 22:30:24.491576 20340 sgd_solver.cpp:136] Iteration 28400, lr = 1e-06, m = 0.9
I0815 22:30:44.186442 20340 solver.cpp:312] Iteration 28500 (5.0776 iter/s, 19.6944s/100 iter), loss = 0.075355
I0815 22:30:44.186491 20340 solver.cpp:334]     Train net output #0: loss = 0.0753548 (* 1 = 0.0753548 loss)
I0815 22:30:44.186496 20340 sgd_solver.cpp:136] Iteration 28500, lr = 1e-06, m = 0.9
I0815 22:31:01.707175 20314 data_reader.cpp:288] Starting prefetch of epoch 16
I0815 22:31:03.627086 20340 solver.cpp:312] Iteration 28600 (5.144 iter/s, 19.4401s/100 iter), loss = 0.0869939
I0815 22:31:03.627110 20340 solver.cpp:334]     Train net output #0: loss = 0.0869938 (* 1 = 0.0869938 loss)
I0815 22:31:03.627115 20340 sgd_solver.cpp:136] Iteration 28600, lr = 1e-06, m = 0.9
I0815 22:31:23.021142 20340 solver.cpp:312] Iteration 28700 (5.15636 iter/s, 19.3935s/100 iter), loss = 0.0861619
I0815 22:31:23.021212 20340 solver.cpp:334]     Train net output #0: loss = 0.0861618 (* 1 = 0.0861618 loss)
I0815 22:31:23.021219 20340 sgd_solver.cpp:136] Iteration 28700, lr = 1e-06, m = 0.9
I0815 22:31:42.343809 20340 solver.cpp:312] Iteration 28800 (5.17541 iter/s, 19.3221s/100 iter), loss = 0.0557022
I0815 22:31:42.343832 20340 solver.cpp:334]     Train net output #0: loss = 0.0557021 (* 1 = 0.0557021 loss)
I0815 22:31:42.343837 20340 sgd_solver.cpp:136] Iteration 28800, lr = 1e-06, m = 0.9
I0815 22:32:01.904758 20340 solver.cpp:312] Iteration 28900 (5.11237 iter/s, 19.5604s/100 iter), loss = 0.134713
I0815 22:32:01.904827 20340 solver.cpp:334]     Train net output #0: loss = 0.134713 (* 1 = 0.134713 loss)
I0815 22:32:01.904834 20340 sgd_solver.cpp:136] Iteration 28900, lr = 1e-06, m = 0.9
I0815 22:32:05.814517 20348 data_reader.cpp:288] Starting prefetch of epoch 17
I0815 22:32:21.546865 20340 solver.cpp:312] Iteration 29000 (5.09124 iter/s, 19.6416s/100 iter), loss = 0.0807742
I0815 22:32:21.546891 20340 solver.cpp:334]     Train net output #0: loss = 0.0807741 (* 1 = 0.0807741 loss)
I0815 22:32:21.546896 20340 sgd_solver.cpp:136] Iteration 29000, lr = 1e-06, m = 0.9
I0815 22:32:38.172719 20344 data_reader.cpp:288] Starting prefetch of epoch 21
I0815 22:32:41.088184 20340 solver.cpp:312] Iteration 29100 (5.1175 iter/s, 19.5408s/100 iter), loss = 0.076932
I0815 22:32:41.088205 20340 solver.cpp:334]     Train net output #0: loss = 0.0769319 (* 1 = 0.0769319 loss)
I0815 22:32:41.088209 20340 sgd_solver.cpp:136] Iteration 29100, lr = 1e-06, m = 0.9
I0815 22:33:00.861202 20340 solver.cpp:312] Iteration 29200 (5.05754 iter/s, 19.7725s/100 iter), loss = 0.0558579
I0815 22:33:00.861227 20340 solver.cpp:334]     Train net output #0: loss = 0.0558578 (* 1 = 0.0558578 loss)
I0815 22:33:00.861232 20340 sgd_solver.cpp:136] Iteration 29200, lr = 1e-06, m = 0.9
I0815 22:33:20.546221 20340 solver.cpp:312] Iteration 29300 (5.08014 iter/s, 19.6845s/100 iter), loss = 0.0560971
I0815 22:33:20.546273 20340 solver.cpp:334]     Train net output #0: loss = 0.056097 (* 1 = 0.056097 loss)
I0815 22:33:20.546278 20340 sgd_solver.cpp:136] Iteration 29300, lr = 1e-06, m = 0.9
I0815 22:33:39.778661 20340 solver.cpp:312] Iteration 29400 (5.19969 iter/s, 19.2319s/100 iter), loss = 0.054485
I0815 22:33:39.778687 20340 solver.cpp:334]     Train net output #0: loss = 0.0544849 (* 1 = 0.0544849 loss)
I0815 22:33:39.778692 20340 sgd_solver.cpp:136] Iteration 29400, lr = 1e-06, m = 0.9
I0815 22:33:42.919169 20348 data_reader.cpp:288] Starting prefetch of epoch 18
I0815 22:33:59.421006 20340 solver.cpp:312] Iteration 29500 (5.09118 iter/s, 19.6418s/100 iter), loss = 0.0801201
I0815 22:33:59.421061 20340 solver.cpp:334]     Train net output #0: loss = 0.08012 (* 1 = 0.08012 loss)
I0815 22:33:59.421066 20340 sgd_solver.cpp:136] Iteration 29500, lr = 1e-06, m = 0.9
I0815 22:34:19.268265 20340 solver.cpp:312] Iteration 29600 (5.03862 iter/s, 19.8467s/100 iter), loss = 0.0782107
I0815 22:34:19.268288 20340 solver.cpp:334]     Train net output #0: loss = 0.0782106 (* 1 = 0.0782106 loss)
I0815 22:34:19.268295 20340 sgd_solver.cpp:136] Iteration 29600, lr = 1e-06, m = 0.9
I0815 22:34:38.937443 20340 solver.cpp:312] Iteration 29700 (5.08424 iter/s, 19.6686s/100 iter), loss = 0.0615155
I0815 22:34:38.937495 20340 solver.cpp:334]     Train net output #0: loss = 0.0615154 (* 1 = 0.0615154 loss)
I0815 22:34:38.937500 20340 sgd_solver.cpp:136] Iteration 29700, lr = 1e-06, m = 0.9
I0815 22:34:47.871922 20348 data_reader.cpp:288] Starting prefetch of epoch 19
I0815 22:34:58.276587 20340 solver.cpp:312] Iteration 29800 (5.171 iter/s, 19.3386s/100 iter), loss = 0.131275
I0815 22:34:58.276610 20340 solver.cpp:334]     Train net output #0: loss = 0.131275 (* 1 = 0.131275 loss)
I0815 22:34:58.276614 20340 sgd_solver.cpp:136] Iteration 29800, lr = 1e-06, m = 0.9
I0815 22:35:17.858021 20340 solver.cpp:312] Iteration 29900 (5.10702 iter/s, 19.5809s/100 iter), loss = 0.0739719
I0815 22:35:17.858114 20340 solver.cpp:334]     Train net output #0: loss = 0.0739718 (* 1 = 0.0739718 loss)
I0815 22:35:17.858120 20340 sgd_solver.cpp:136] Iteration 29900, lr = 1e-06, m = 0.9
I0815 22:35:20.264994 20344 data_reader.cpp:288] Starting prefetch of epoch 22
I0815 22:35:37.316536 20340 solver.cpp:639] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/l1reg/cityscapes5_jsegnet21v2_iter_30000.caffemodel
I0815 22:35:37.501479 20340 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/l1reg/cityscapes5_jsegnet21v2_iter_30000.solverstate
I0815 22:35:37.516894 20340 solver.cpp:509] Iteration 30000, Testing net (#0)
I0815 22:35:49.276396 20340 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.952278
I0815 22:35:49.276526 20340 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999355
I0815 22:35:49.276536 20340 solver.cpp:594]     Test net output #2: loss = 0.189736 (* 1 = 0.189736 loss)
I0815 22:35:49.276563 20340 solver.cpp:264] [MultiGPU] Tests completed in 11.7593s
I0815 22:35:49.507056 20340 solver.cpp:312] Iteration 30000 (3.15974 iter/s, 31.6482s/100 iter), loss = 0.0795342
I0815 22:35:49.507083 20340 solver.cpp:334]     Train net output #0: loss = 0.0795341 (* 1 = 0.0795341 loss)
I0815 22:35:49.507091 20340 sgd_solver.cpp:136] Iteration 30000, lr = 1e-06, m = 0.9
I0815 22:36:04.757462 20344 data_reader.cpp:288] Starting prefetch of epoch 23
I0815 22:36:09.182349 20340 solver.cpp:312] Iteration 30100 (5.08266 iter/s, 19.6747s/100 iter), loss = 0.0667023
I0815 22:36:09.182376 20340 solver.cpp:334]     Train net output #0: loss = 0.0667022 (* 1 = 0.0667022 loss)
I0815 22:36:09.182381 20340 sgd_solver.cpp:136] Iteration 30100, lr = 1e-06, m = 0.9
I0815 22:36:28.444442 20340 solver.cpp:312] Iteration 30200 (5.19169 iter/s, 19.2616s/100 iter), loss = 0.0621481
I0815 22:36:28.444497 20340 solver.cpp:334]     Train net output #0: loss = 0.062148 (* 1 = 0.062148 loss)
I0815 22:36:28.444505 20340 sgd_solver.cpp:136] Iteration 30200, lr = 1e-06, m = 0.9
I0815 22:36:48.019304 20340 solver.cpp:312] Iteration 30300 (5.10873 iter/s, 19.5743s/100 iter), loss = 0.0854767
I0815 22:36:48.019326 20340 solver.cpp:334]     Train net output #0: loss = 0.0854766 (* 1 = 0.0854766 loss)
I0815 22:36:48.019331 20340 sgd_solver.cpp:136] Iteration 30300, lr = 1e-06, m = 0.9
I0815 22:37:07.434895 20340 solver.cpp:312] Iteration 30400 (5.15064 iter/s, 19.4151s/100 iter), loss = 0.074081
I0815 22:37:07.434954 20340 solver.cpp:334]     Train net output #0: loss = 0.0740809 (* 1 = 0.0740809 loss)
I0815 22:37:07.434962 20340 sgd_solver.cpp:136] Iteration 30400, lr = 1e-06, m = 0.9
I0815 22:37:08.951863 20316 data_reader.cpp:288] Starting prefetch of epoch 18
I0815 22:37:26.741588 20340 solver.cpp:312] Iteration 30500 (5.17969 iter/s, 19.3062s/100 iter), loss = 0.0791669
I0815 22:37:26.741614 20340 solver.cpp:334]     Train net output #0: loss = 0.0791669 (* 1 = 0.0791669 loss)
I0815 22:37:26.741621 20340 sgd_solver.cpp:136] Iteration 30500, lr = 1e-06, m = 0.9
I0815 22:37:40.870120 20344 data_reader.cpp:288] Starting prefetch of epoch 24
I0815 22:37:46.065138 20340 solver.cpp:312] Iteration 30600 (5.17518 iter/s, 19.323s/100 iter), loss = 0.0582716
I0815 22:37:46.065165 20340 solver.cpp:334]     Train net output #0: loss = 0.0582715 (* 1 = 0.0582715 loss)
I0815 22:37:46.065172 20340 sgd_solver.cpp:136] Iteration 30600, lr = 1e-06, m = 0.9
I0815 22:38:05.399670 20340 solver.cpp:312] Iteration 30700 (5.17224 iter/s, 19.334s/100 iter), loss = 0.147601
I0815 22:38:05.399694 20340 solver.cpp:334]     Train net output #0: loss = 0.1476 (* 1 = 0.1476 loss)
I0815 22:38:05.399698 20340 sgd_solver.cpp:136] Iteration 30700, lr = 1e-06, m = 0.9
I0815 22:38:24.763720 20340 solver.cpp:312] Iteration 30800 (5.16435 iter/s, 19.3635s/100 iter), loss = 0.0727277
I0815 22:38:24.763762 20340 solver.cpp:334]     Train net output #0: loss = 0.0727276 (* 1 = 0.0727276 loss)
I0815 22:38:24.763767 20340 sgd_solver.cpp:136] Iteration 30800, lr = 1e-06, m = 0.9
I0815 22:38:44.141718 20340 solver.cpp:312] Iteration 30900 (5.16063 iter/s, 19.3775s/100 iter), loss = 0.0703209
I0815 22:38:44.141741 20340 solver.cpp:334]     Train net output #0: loss = 0.0703208 (* 1 = 0.0703208 loss)
I0815 22:38:44.141744 20340 sgd_solver.cpp:136] Iteration 30900, lr = 1e-06, m = 0.9
I0815 22:38:44.732020 20344 data_reader.cpp:288] Starting prefetch of epoch 25
I0815 22:39:03.562834 20340 solver.cpp:312] Iteration 31000 (5.14918 iter/s, 19.4206s/100 iter), loss = 0.0646855
I0815 22:39:03.562904 20340 solver.cpp:334]     Train net output #0: loss = 0.0646854 (* 1 = 0.0646854 loss)
I0815 22:39:03.562911 20340 sgd_solver.cpp:136] Iteration 31000, lr = 1e-06, m = 0.9
I0815 22:39:23.030905 20340 solver.cpp:312] Iteration 31100 (5.13676 iter/s, 19.4675s/100 iter), loss = 0.0592646
I0815 22:39:23.030926 20340 solver.cpp:334]     Train net output #0: loss = 0.0592645 (* 1 = 0.0592645 loss)
I0815 22:39:23.030931 20340 sgd_solver.cpp:136] Iteration 31100, lr = 1e-06, m = 0.9
I0815 22:39:42.582192 20340 solver.cpp:312] Iteration 31200 (5.11489 iter/s, 19.5508s/100 iter), loss = 0.107137
I0815 22:39:42.582937 20340 solver.cpp:334]     Train net output #0: loss = 0.107137 (* 1 = 0.107137 loss)
I0815 22:39:42.582959 20340 sgd_solver.cpp:136] Iteration 31200, lr = 1e-06, m = 0.9
I0815 22:39:49.234690 20316 data_reader.cpp:288] Starting prefetch of epoch 19
I0815 22:40:01.998394 20340 solver.cpp:312] Iteration 31300 (5.15048 iter/s, 19.4157s/100 iter), loss = 0.0967286
I0815 22:40:01.998421 20340 solver.cpp:334]     Train net output #0: loss = 0.0967285 (* 1 = 0.0967285 loss)
I0815 22:40:01.998428 20340 sgd_solver.cpp:136] Iteration 31300, lr = 1e-06, m = 0.9
I0815 22:40:21.310854 20314 data_reader.cpp:288] Starting prefetch of epoch 17
I0815 22:40:21.450029 20340 solver.cpp:312] Iteration 31400 (5.1411 iter/s, 19.4511s/100 iter), loss = 0.0616355
I0815 22:40:21.450053 20340 solver.cpp:334]     Train net output #0: loss = 0.0616354 (* 1 = 0.0616354 loss)
I0815 22:40:21.450057 20340 sgd_solver.cpp:136] Iteration 31400, lr = 1e-06, m = 0.9
I0815 22:40:40.877178 20340 solver.cpp:312] Iteration 31500 (5.14758 iter/s, 19.4266s/100 iter), loss = 0.0671232
I0815 22:40:40.877197 20340 solver.cpp:334]     Train net output #0: loss = 0.0671231 (* 1 = 0.0671231 loss)
I0815 22:40:40.877202 20340 sgd_solver.cpp:136] Iteration 31500, lr = 1e-06, m = 0.9
I0815 22:41:00.141348 20340 solver.cpp:312] Iteration 31600 (5.19113 iter/s, 19.2636s/100 iter), loss = 0.139039
I0815 22:41:00.141402 20340 solver.cpp:334]     Train net output #0: loss = 0.139039 (* 1 = 0.139039 loss)
I0815 22:41:00.141407 20340 sgd_solver.cpp:136] Iteration 31600, lr = 1e-06, m = 0.9
I0815 22:41:19.675767 20340 solver.cpp:312] Iteration 31700 (5.11931 iter/s, 19.5339s/100 iter), loss = 0.0763931
I0815 22:41:19.675793 20340 solver.cpp:334]     Train net output #0: loss = 0.076393 (* 1 = 0.076393 loss)
I0815 22:41:19.675799 20340 sgd_solver.cpp:136] Iteration 31700, lr = 1e-06, m = 0.9
I0815 22:41:25.555701 20344 data_reader.cpp:288] Starting prefetch of epoch 26
I0815 22:41:39.007856 20340 solver.cpp:312] Iteration 31800 (5.17289 iter/s, 19.3316s/100 iter), loss = 0.0745503
I0815 22:41:39.007930 20340 solver.cpp:334]     Train net output #0: loss = 0.0745502 (* 1 = 0.0745502 loss)
I0815 22:41:39.007937 20340 sgd_solver.cpp:136] Iteration 31800, lr = 1e-06, m = 0.9
I0815 22:41:58.246103 20340 solver.cpp:312] Iteration 31900 (5.19812 iter/s, 19.2377s/100 iter), loss = 0.0444371
I0815 22:41:58.246129 20340 solver.cpp:334]     Train net output #0: loss = 0.044437 (* 1 = 0.044437 loss)
I0815 22:41:58.246134 20340 sgd_solver.cpp:136] Iteration 31900, lr = 1e-06, m = 0.9
I0815 22:42:17.742462 20340 solver.cpp:312] Iteration 31999 (5.07801 iter/s, 19.4958s/99 iter), loss = 0.0614077
I0815 22:42:17.742566 20340 solver.cpp:334]     Train net output #0: loss = 0.0614076 (* 1 = 0.0614076 loss)
I0815 22:42:17.784222 20340 solver.cpp:639] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/l1reg/cityscapes5_jsegnet21v2_iter_32000.caffemodel
I0815 22:42:17.829357 20340 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/l1reg/cityscapes5_jsegnet21v2_iter_32000.solverstate
I0815 22:42:17.908219 20340 solver.cpp:486] Iteration 32000, loss = 0.0621046
I0815 22:42:17.908239 20340 solver.cpp:509] Iteration 32000, Testing net (#0)
I0815 22:42:21.377460 20336 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 22:42:29.276087 20385 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 22:42:29.621076 20340 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.951159
I0815 22:42:29.621105 20340 solver.cpp:594]     Test net output #1: accuracy/top5 = 1
I0815 22:42:29.621110 20340 solver.cpp:594]     Test net output #2: loss = 0.156164 (* 1 = 0.156164 loss)
I0815 22:42:29.665659 20241 parallel.cpp:71] Root Solver performance on device 0: 4.977 * 6 = 29.86 img/sec (32000 itr in 6429 sec)
I0815 22:42:29.665681 20241 parallel.cpp:76]      Solver performance on device 1: 4.977 * 6 = 29.86 img/sec (32000 itr in 6429 sec)
I0815 22:42:29.665688 20241 parallel.cpp:76]      Solver performance on device 2: 4.977 * 6 = 29.86 img/sec (32000 itr in 6429 sec)
I0815 22:42:29.665690 20241 parallel.cpp:79] Overall multi-GPU performance: 89.5918 img/sec
I0815 22:42:30.982499 20241 caffe.cpp:247] Optimization Done in 1h 47m 27s
I0815 22:42:37.046985 14815 caffe.cpp:608] This is NVCaffe 0.16.3 started at Tue Aug 15 22:42:36 2017
I0815 22:42:37.048161 14815 caffe.cpp:611] CuDNN version: 6021
I0815 22:42:37.048166 14815 caffe.cpp:612] CuBLAS version: 8000
I0815 22:42:37.048169 14815 caffe.cpp:613] CUDA version: 8000
I0815 22:42:37.048172 14815 caffe.cpp:614] CUDA driver version: 8000
I0815 22:42:37.366562 14815 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0815 22:42:37.367226 14815 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I0815 22:42:37.367828 14815 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[1]: total=8508145664 free=8379236352
I0815 22:42:37.368413 14815 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[2]: total=8508145664 free=8379236352
I0815 22:42:37.368424 14815 caffe.cpp:208] Using GPUs 0, 1, 2
I0815 22:42:37.368785 14815 caffe.cpp:213] GPU 0: GeForce GTX 1080
I0815 22:42:37.369148 14815 caffe.cpp:213] GPU 1: GeForce GTX 1080
I0815 22:42:37.369508 14815 caffe.cpp:213] GPU 2: GeForce GTX 1080
I0815 22:42:37.369551 14815 solver.cpp:42] Solver data type: FLOAT
I0815 22:42:37.369596 14815 solver.cpp:45] Initializing solver from parameters: 
train_net: "training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/sparse/train.prototxt"
test_net: "training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/sparse/test.prototxt"
test_iter: 125
test_interval: 2000
base_lr: 1e-05
display: 100
max_iter: 32000
lr_policy: "multistep"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 1e-05
snapshot: 10000
snapshot_prefix: "training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/sparse/cityscapes5_jsegnet21v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
regularization_type: "L1"
test_initialization: false
stepvalue: 24000
iter_size: 1
type: "Adam"
display_sparsity: 1000
sparse_mode: SPARSE_UPDATE
sparsity_target: 0.8
sparsity_step_factor: 0.01
sparsity_step_iter: 1000
sparsity_start_iter: 0
sparsity_start_factor: 0.8
I0815 22:42:37.385466 14815 solver.cpp:77] Creating training net from train_net file: training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/sparse/train.prototxt
I0815 22:42:37.386652 14815 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0815 22:42:37.386662 14815 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
W0815 22:42:37.386710 14815 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0815 22:42:37.387645 14815 net.cpp:72] Initializing net from parameters: 
name: "jsegnet21v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/train-image-lmdb"
    label_list_path: "data/train-label-lmdb"
    batch_size: 6
    shuffle: false
    threads: 1
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
I0815 22:42:37.387857 14815 net.cpp:104] Using FLOAT as default forward math type
I0815 22:42:37.387863 14815 net.cpp:110] Using FLOAT as default backward math type
I0815 22:42:37.387867 14815 layer_factory.hpp:136] Creating layer 'data' of type 'ImageLabelData'
I0815 22:42:37.387873 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:37.387887 14815 net.cpp:184] Created Layer data (0)
I0815 22:42:37.387892 14815 net.cpp:530] data -> data
I0815 22:42:37.387917 14815 net.cpp:530] data -> label
I0815 22:42:37.398145 14815 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0815 22:42:37.398164 14815 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 22:42:37.428581 14870 db_lmdb.cpp:24] Opened lmdb data/train-image-lmdb
I0815 22:42:37.431659 14815 data_layer.cpp:185] [0] ReshapePrefetch 6, 3, 640, 640
I0815 22:42:37.431738 14815 data_layer.cpp:209] [0] Output data size: 6, 3, 640, 640
I0815 22:42:37.431749 14815 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 22:42:37.431829 14815 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0815 22:42:37.431841 14815 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 22:42:37.432718 14871 data_layer.cpp:97] [0] Parser threads: 1
I0815 22:42:37.432730 14871 data_layer.cpp:99] [0] Transformer threads: 1
I0815 22:42:37.438060 14872 db_lmdb.cpp:24] Opened lmdb data/train-label-lmdb
I0815 22:42:37.439265 14815 data_layer.cpp:185] [0] ReshapePrefetch 6, 1, 640, 640
I0815 22:42:37.439353 14815 data_layer.cpp:209] [0] Output data size: 6, 1, 640, 640
I0815 22:42:37.439363 14815 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 22:42:37.439466 14815 net.cpp:245] Setting up data
I0815 22:42:37.439486 14815 net.cpp:252] TRAIN Top shape for layer 0 'data' 6 3 640 640 (7372800)
I0815 22:42:37.439496 14815 net.cpp:252] TRAIN Top shape for layer 0 'data' 6 1 640 640 (2457600)
I0815 22:42:37.439530 14815 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0815 22:42:37.439540 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:37.439569 14815 net.cpp:184] Created Layer data/bias (1)
I0815 22:42:37.439575 14815 net.cpp:561] data/bias <- data
I0815 22:42:37.439592 14815 net.cpp:530] data/bias -> data/bias
I0815 22:42:37.444911 14873 data_layer.cpp:97] [0] Parser threads: 1
I0815 22:42:37.444939 14873 data_layer.cpp:99] [0] Transformer threads: 1
I0815 22:42:37.447520 14815 net.cpp:245] Setting up data/bias
I0815 22:42:37.447597 14815 net.cpp:252] TRAIN Top shape for layer 1 'data/bias' 6 3 640 640 (7372800)
I0815 22:42:37.447621 14815 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0815 22:42:37.447652 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:37.447692 14815 net.cpp:184] Created Layer conv1a (2)
I0815 22:42:37.447697 14815 net.cpp:561] conv1a <- data/bias
I0815 22:42:37.447703 14815 net.cpp:530] conv1a -> conv1a
I0815 22:42:38.114511 14815 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 7.9G, req 0G)
I0815 22:42:38.114533 14815 net.cpp:245] Setting up conv1a
I0815 22:42:38.114540 14815 net.cpp:252] TRAIN Top shape for layer 2 'conv1a' 6 32 320 320 (19660800)
I0815 22:42:38.114552 14815 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0815 22:42:38.114557 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.114570 14815 net.cpp:184] Created Layer conv1a/bn (3)
I0815 22:42:38.114574 14815 net.cpp:561] conv1a/bn <- conv1a
I0815 22:42:38.114581 14815 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0815 22:42:38.115439 14815 net.cpp:245] Setting up conv1a/bn
I0815 22:42:38.115449 14815 net.cpp:252] TRAIN Top shape for layer 3 'conv1a/bn' 6 32 320 320 (19660800)
I0815 22:42:38.115459 14815 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0815 22:42:38.115463 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.115469 14815 net.cpp:184] Created Layer conv1a/relu (4)
I0815 22:42:38.115473 14815 net.cpp:561] conv1a/relu <- conv1a
I0815 22:42:38.115476 14815 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0815 22:42:38.115489 14815 net.cpp:245] Setting up conv1a/relu
I0815 22:42:38.115494 14815 net.cpp:252] TRAIN Top shape for layer 4 'conv1a/relu' 6 32 320 320 (19660800)
I0815 22:42:38.115499 14815 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0815 22:42:38.115501 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.115515 14815 net.cpp:184] Created Layer conv1b (5)
I0815 22:42:38.115520 14815 net.cpp:561] conv1b <- conv1a
I0815 22:42:38.115525 14815 net.cpp:530] conv1b -> conv1b
I0815 22:42:38.162139 14815 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.73G, req 0G)
I0815 22:42:38.162151 14815 net.cpp:245] Setting up conv1b
I0815 22:42:38.162158 14815 net.cpp:252] TRAIN Top shape for layer 5 'conv1b' 6 32 320 320 (19660800)
I0815 22:42:38.162165 14815 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0815 22:42:38.162169 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.162175 14815 net.cpp:184] Created Layer conv1b/bn (6)
I0815 22:42:38.162178 14815 net.cpp:561] conv1b/bn <- conv1b
I0815 22:42:38.162183 14815 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0815 22:42:38.162966 14815 net.cpp:245] Setting up conv1b/bn
I0815 22:42:38.162974 14815 net.cpp:252] TRAIN Top shape for layer 6 'conv1b/bn' 6 32 320 320 (19660800)
I0815 22:42:38.162983 14815 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0815 22:42:38.162986 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.162992 14815 net.cpp:184] Created Layer conv1b/relu (7)
I0815 22:42:38.162995 14815 net.cpp:561] conv1b/relu <- conv1b
I0815 22:42:38.162998 14815 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0815 22:42:38.163003 14815 net.cpp:245] Setting up conv1b/relu
I0815 22:42:38.163008 14815 net.cpp:252] TRAIN Top shape for layer 7 'conv1b/relu' 6 32 320 320 (19660800)
I0815 22:42:38.163012 14815 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0815 22:42:38.163017 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.163023 14815 net.cpp:184] Created Layer pool1 (8)
I0815 22:42:38.163027 14815 net.cpp:561] pool1 <- conv1b
I0815 22:42:38.163030 14815 net.cpp:530] pool1 -> pool1
I0815 22:42:38.163132 14815 net.cpp:245] Setting up pool1
I0815 22:42:38.163138 14815 net.cpp:252] TRAIN Top shape for layer 8 'pool1' 6 32 160 160 (4915200)
I0815 22:42:38.163142 14815 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0815 22:42:38.163146 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.163154 14815 net.cpp:184] Created Layer res2a_branch2a (9)
I0815 22:42:38.163157 14815 net.cpp:561] res2a_branch2a <- pool1
I0815 22:42:38.163161 14815 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0815 22:42:38.204340 14815 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.61G, req 0G)
I0815 22:42:38.204365 14815 net.cpp:245] Setting up res2a_branch2a
I0815 22:42:38.204373 14815 net.cpp:252] TRAIN Top shape for layer 9 'res2a_branch2a' 6 64 160 160 (9830400)
I0815 22:42:38.204385 14815 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0815 22:42:38.204391 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.204402 14815 net.cpp:184] Created Layer res2a_branch2a/bn (10)
I0815 22:42:38.204407 14815 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0815 22:42:38.204412 14815 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0815 22:42:38.205930 14815 net.cpp:245] Setting up res2a_branch2a/bn
I0815 22:42:38.205941 14815 net.cpp:252] TRAIN Top shape for layer 10 'res2a_branch2a/bn' 6 64 160 160 (9830400)
I0815 22:42:38.205951 14815 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0815 22:42:38.205955 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.205960 14815 net.cpp:184] Created Layer res2a_branch2a/relu (11)
I0815 22:42:38.205965 14815 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0815 22:42:38.205968 14815 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0815 22:42:38.205974 14815 net.cpp:245] Setting up res2a_branch2a/relu
I0815 22:42:38.205979 14815 net.cpp:252] TRAIN Top shape for layer 11 'res2a_branch2a/relu' 6 64 160 160 (9830400)
I0815 22:42:38.205982 14815 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0815 22:42:38.205986 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.205996 14815 net.cpp:184] Created Layer res2a_branch2b (12)
I0815 22:42:38.206001 14815 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0815 22:42:38.206003 14815 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0815 22:42:38.227565 14815 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.52G, req 0G)
I0815 22:42:38.227578 14815 net.cpp:245] Setting up res2a_branch2b
I0815 22:42:38.227583 14815 net.cpp:252] TRAIN Top shape for layer 12 'res2a_branch2b' 6 64 160 160 (9830400)
I0815 22:42:38.227589 14815 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0815 22:42:38.227593 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.227600 14815 net.cpp:184] Created Layer res2a_branch2b/bn (13)
I0815 22:42:38.227603 14815 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0815 22:42:38.227607 14815 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0815 22:42:38.228422 14815 net.cpp:245] Setting up res2a_branch2b/bn
I0815 22:42:38.228431 14815 net.cpp:252] TRAIN Top shape for layer 13 'res2a_branch2b/bn' 6 64 160 160 (9830400)
I0815 22:42:38.228440 14815 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0815 22:42:38.228444 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.228448 14815 net.cpp:184] Created Layer res2a_branch2b/relu (14)
I0815 22:42:38.228451 14815 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0815 22:42:38.228456 14815 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0815 22:42:38.228469 14815 net.cpp:245] Setting up res2a_branch2b/relu
I0815 22:42:38.228474 14815 net.cpp:252] TRAIN Top shape for layer 14 'res2a_branch2b/relu' 6 64 160 160 (9830400)
I0815 22:42:38.228478 14815 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0815 22:42:38.228482 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.228489 14815 net.cpp:184] Created Layer pool2 (15)
I0815 22:42:38.228492 14815 net.cpp:561] pool2 <- res2a_branch2b
I0815 22:42:38.228497 14815 net.cpp:530] pool2 -> pool2
I0815 22:42:38.228574 14815 net.cpp:245] Setting up pool2
I0815 22:42:38.228579 14815 net.cpp:252] TRAIN Top shape for layer 15 'pool2' 6 64 80 80 (2457600)
I0815 22:42:38.228584 14815 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0815 22:42:38.228586 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.228595 14815 net.cpp:184] Created Layer res3a_branch2a (16)
I0815 22:42:38.228598 14815 net.cpp:561] res3a_branch2a <- pool2
I0815 22:42:38.228601 14815 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0815 22:42:38.250130 14815 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 7.46G, req 0G)
I0815 22:42:38.250151 14815 net.cpp:245] Setting up res3a_branch2a
I0815 22:42:38.250159 14815 net.cpp:252] TRAIN Top shape for layer 16 'res3a_branch2a' 6 128 80 80 (4915200)
I0815 22:42:38.250167 14815 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0815 22:42:38.250172 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.250181 14815 net.cpp:184] Created Layer res3a_branch2a/bn (17)
I0815 22:42:38.250186 14815 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0815 22:42:38.250190 14815 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0815 22:42:38.251045 14815 net.cpp:245] Setting up res3a_branch2a/bn
I0815 22:42:38.251055 14815 net.cpp:252] TRAIN Top shape for layer 17 'res3a_branch2a/bn' 6 128 80 80 (4915200)
I0815 22:42:38.251066 14815 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0815 22:42:38.251070 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.251075 14815 net.cpp:184] Created Layer res3a_branch2a/relu (18)
I0815 22:42:38.251078 14815 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0815 22:42:38.251081 14815 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0815 22:42:38.251086 14815 net.cpp:245] Setting up res3a_branch2a/relu
I0815 22:42:38.251091 14815 net.cpp:252] TRAIN Top shape for layer 18 'res3a_branch2a/relu' 6 128 80 80 (4915200)
I0815 22:42:38.251096 14815 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0815 22:42:38.251099 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.251107 14815 net.cpp:184] Created Layer res3a_branch2b (19)
I0815 22:42:38.251111 14815 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0815 22:42:38.251114 14815 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0815 22:42:38.262527 14815 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.42G, req 0G)
I0815 22:42:38.262540 14815 net.cpp:245] Setting up res3a_branch2b
I0815 22:42:38.262545 14815 net.cpp:252] TRAIN Top shape for layer 19 'res3a_branch2b' 6 128 80 80 (4915200)
I0815 22:42:38.262552 14815 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0815 22:42:38.262555 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.262562 14815 net.cpp:184] Created Layer res3a_branch2b/bn (20)
I0815 22:42:38.262565 14815 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0815 22:42:38.262569 14815 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0815 22:42:38.263373 14815 net.cpp:245] Setting up res3a_branch2b/bn
I0815 22:42:38.263393 14815 net.cpp:252] TRAIN Top shape for layer 20 'res3a_branch2b/bn' 6 128 80 80 (4915200)
I0815 22:42:38.263402 14815 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0815 22:42:38.263406 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.263411 14815 net.cpp:184] Created Layer res3a_branch2b/relu (21)
I0815 22:42:38.263414 14815 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0815 22:42:38.263418 14815 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0815 22:42:38.263423 14815 net.cpp:245] Setting up res3a_branch2b/relu
I0815 22:42:38.263428 14815 net.cpp:252] TRAIN Top shape for layer 21 'res3a_branch2b/relu' 6 128 80 80 (4915200)
I0815 22:42:38.263432 14815 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0815 22:42:38.263435 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.263443 14815 net.cpp:184] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (22)
I0815 22:42:38.263447 14815 net.cpp:561] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0815 22:42:38.263449 14815 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0815 22:42:38.263453 14815 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0815 22:42:38.263506 14815 net.cpp:245] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0815 22:42:38.263512 14815 net.cpp:252] TRAIN Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 6 128 80 80 (4915200)
I0815 22:42:38.263517 14815 net.cpp:252] TRAIN Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 6 128 80 80 (4915200)
I0815 22:42:38.263520 14815 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0815 22:42:38.263525 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.263530 14815 net.cpp:184] Created Layer pool3 (23)
I0815 22:42:38.263533 14815 net.cpp:561] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0815 22:42:38.263537 14815 net.cpp:530] pool3 -> pool3
I0815 22:42:38.263623 14815 net.cpp:245] Setting up pool3
I0815 22:42:38.263629 14815 net.cpp:252] TRAIN Top shape for layer 23 'pool3' 6 128 40 40 (1228800)
I0815 22:42:38.263633 14815 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0815 22:42:38.263636 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.263646 14815 net.cpp:184] Created Layer res4a_branch2a (24)
I0815 22:42:38.263669 14815 net.cpp:561] res4a_branch2a <- pool3
I0815 22:42:38.263674 14815 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0815 22:42:38.293735 14815 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.38G, req 0G)
I0815 22:42:38.293761 14815 net.cpp:245] Setting up res4a_branch2a
I0815 22:42:38.293767 14815 net.cpp:252] TRAIN Top shape for layer 24 'res4a_branch2a' 6 256 40 40 (2457600)
I0815 22:42:38.293776 14815 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0815 22:42:38.293781 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.293794 14815 net.cpp:184] Created Layer res4a_branch2a/bn (25)
I0815 22:42:38.293799 14815 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0815 22:42:38.293804 14815 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0815 22:42:38.294672 14815 net.cpp:245] Setting up res4a_branch2a/bn
I0815 22:42:38.294682 14815 net.cpp:252] TRAIN Top shape for layer 25 'res4a_branch2a/bn' 6 256 40 40 (2457600)
I0815 22:42:38.294692 14815 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0815 22:42:38.294695 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.294700 14815 net.cpp:184] Created Layer res4a_branch2a/relu (26)
I0815 22:42:38.294713 14815 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0815 22:42:38.294718 14815 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0815 22:42:38.294724 14815 net.cpp:245] Setting up res4a_branch2a/relu
I0815 22:42:38.294728 14815 net.cpp:252] TRAIN Top shape for layer 26 'res4a_branch2a/relu' 6 256 40 40 (2457600)
I0815 22:42:38.294733 14815 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0815 22:42:38.294736 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.294749 14815 net.cpp:184] Created Layer res4a_branch2b (27)
I0815 22:42:38.294752 14815 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0815 22:42:38.294756 14815 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0815 22:42:38.304922 14815 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.36G, req 0G)
I0815 22:42:38.304939 14815 net.cpp:245] Setting up res4a_branch2b
I0815 22:42:38.304945 14815 net.cpp:252] TRAIN Top shape for layer 27 'res4a_branch2b' 6 256 40 40 (2457600)
I0815 22:42:38.304953 14815 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0815 22:42:38.304958 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.304967 14815 net.cpp:184] Created Layer res4a_branch2b/bn (28)
I0815 22:42:38.304971 14815 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0815 22:42:38.304976 14815 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0815 22:42:38.306119 14815 net.cpp:245] Setting up res4a_branch2b/bn
I0815 22:42:38.306129 14815 net.cpp:252] TRAIN Top shape for layer 28 'res4a_branch2b/bn' 6 256 40 40 (2457600)
I0815 22:42:38.306138 14815 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0815 22:42:38.306143 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.306149 14815 net.cpp:184] Created Layer res4a_branch2b/relu (29)
I0815 22:42:38.306152 14815 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0815 22:42:38.306156 14815 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0815 22:42:38.306162 14815 net.cpp:245] Setting up res4a_branch2b/relu
I0815 22:42:38.306167 14815 net.cpp:252] TRAIN Top shape for layer 29 'res4a_branch2b/relu' 6 256 40 40 (2457600)
I0815 22:42:38.306170 14815 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0815 22:42:38.306174 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.306182 14815 net.cpp:184] Created Layer pool4 (30)
I0815 22:42:38.306186 14815 net.cpp:561] pool4 <- res4a_branch2b
I0815 22:42:38.306190 14815 net.cpp:530] pool4 -> pool4
I0815 22:42:38.306270 14815 net.cpp:245] Setting up pool4
I0815 22:42:38.306277 14815 net.cpp:252] TRAIN Top shape for layer 30 'pool4' 6 256 40 40 (2457600)
I0815 22:42:38.306282 14815 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0815 22:42:38.306285 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.306296 14815 net.cpp:184] Created Layer res5a_branch2a (31)
I0815 22:42:38.306300 14815 net.cpp:561] res5a_branch2a <- pool4
I0815 22:42:38.306304 14815 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0815 22:42:38.340586 14815 net.cpp:245] Setting up res5a_branch2a
I0815 22:42:38.340611 14815 net.cpp:252] TRAIN Top shape for layer 31 'res5a_branch2a' 6 512 40 40 (4915200)
I0815 22:42:38.340620 14815 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0815 22:42:38.340626 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.340636 14815 net.cpp:184] Created Layer res5a_branch2a/bn (32)
I0815 22:42:38.340641 14815 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0815 22:42:38.340646 14815 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0815 22:42:38.341481 14815 net.cpp:245] Setting up res5a_branch2a/bn
I0815 22:42:38.341495 14815 net.cpp:252] TRAIN Top shape for layer 32 'res5a_branch2a/bn' 6 512 40 40 (4915200)
I0815 22:42:38.341505 14815 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0815 22:42:38.341508 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.341513 14815 net.cpp:184] Created Layer res5a_branch2a/relu (33)
I0815 22:42:38.341516 14815 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0815 22:42:38.341521 14815 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0815 22:42:38.341526 14815 net.cpp:245] Setting up res5a_branch2a/relu
I0815 22:42:38.341531 14815 net.cpp:252] TRAIN Top shape for layer 33 'res5a_branch2a/relu' 6 512 40 40 (4915200)
I0815 22:42:38.341534 14815 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0815 22:42:38.341538 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.341547 14815 net.cpp:184] Created Layer res5a_branch2b (34)
I0815 22:42:38.341552 14815 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0815 22:42:38.341554 14815 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0815 22:42:38.358665 14815 net.cpp:245] Setting up res5a_branch2b
I0815 22:42:38.358691 14815 net.cpp:252] TRAIN Top shape for layer 34 'res5a_branch2b' 6 512 40 40 (4915200)
I0815 22:42:38.358707 14815 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0815 22:42:38.358713 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.358722 14815 net.cpp:184] Created Layer res5a_branch2b/bn (35)
I0815 22:42:38.358727 14815 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0815 22:42:38.358732 14815 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0815 22:42:38.359558 14815 net.cpp:245] Setting up res5a_branch2b/bn
I0815 22:42:38.359567 14815 net.cpp:252] TRAIN Top shape for layer 35 'res5a_branch2b/bn' 6 512 40 40 (4915200)
I0815 22:42:38.359575 14815 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0815 22:42:38.359580 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.359585 14815 net.cpp:184] Created Layer res5a_branch2b/relu (36)
I0815 22:42:38.359587 14815 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0815 22:42:38.359591 14815 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0815 22:42:38.359596 14815 net.cpp:245] Setting up res5a_branch2b/relu
I0815 22:42:38.359601 14815 net.cpp:252] TRAIN Top shape for layer 36 'res5a_branch2b/relu' 6 512 40 40 (4915200)
I0815 22:42:38.359604 14815 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I0815 22:42:38.359607 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.359622 14815 net.cpp:184] Created Layer out5a (37)
I0815 22:42:38.359625 14815 net.cpp:561] out5a <- res5a_branch2b
I0815 22:42:38.359629 14815 net.cpp:530] out5a -> out5a
I0815 22:42:38.365226 14815 net.cpp:245] Setting up out5a
I0815 22:42:38.365238 14815 net.cpp:252] TRAIN Top shape for layer 37 'out5a' 6 64 40 40 (614400)
I0815 22:42:38.365245 14815 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I0815 22:42:38.365249 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.365257 14815 net.cpp:184] Created Layer out5a/bn (38)
I0815 22:42:38.365259 14815 net.cpp:561] out5a/bn <- out5a
I0815 22:42:38.365264 14815 net.cpp:513] out5a/bn -> out5a (in-place)
I0815 22:42:38.366106 14815 net.cpp:245] Setting up out5a/bn
I0815 22:42:38.366113 14815 net.cpp:252] TRAIN Top shape for layer 38 'out5a/bn' 6 64 40 40 (614400)
I0815 22:42:38.366122 14815 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I0815 22:42:38.366125 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.366139 14815 net.cpp:184] Created Layer out5a/relu (39)
I0815 22:42:38.366142 14815 net.cpp:561] out5a/relu <- out5a
I0815 22:42:38.366147 14815 net.cpp:513] out5a/relu -> out5a (in-place)
I0815 22:42:38.366152 14815 net.cpp:245] Setting up out5a/relu
I0815 22:42:38.366156 14815 net.cpp:252] TRAIN Top shape for layer 39 'out5a/relu' 6 64 40 40 (614400)
I0815 22:42:38.366159 14815 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I0815 22:42:38.366163 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.366179 14815 net.cpp:184] Created Layer out5a_up2 (40)
I0815 22:42:38.366183 14815 net.cpp:561] out5a_up2 <- out5a
I0815 22:42:38.366186 14815 net.cpp:530] out5a_up2 -> out5a_up2
I0815 22:42:38.366602 14815 net.cpp:245] Setting up out5a_up2
I0815 22:42:38.366611 14815 net.cpp:252] TRAIN Top shape for layer 40 'out5a_up2' 6 64 80 80 (2457600)
I0815 22:42:38.366616 14815 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I0815 22:42:38.366619 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.366636 14815 net.cpp:184] Created Layer out3a (41)
I0815 22:42:38.366638 14815 net.cpp:561] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0815 22:42:38.366643 14815 net.cpp:530] out3a -> out3a
I0815 22:42:38.378798 14815 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 7.3G, req 0G)
I0815 22:42:38.378809 14815 net.cpp:245] Setting up out3a
I0815 22:42:38.378815 14815 net.cpp:252] TRAIN Top shape for layer 41 'out3a' 6 64 80 80 (2457600)
I0815 22:42:38.378821 14815 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I0815 22:42:38.378824 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.378830 14815 net.cpp:184] Created Layer out3a/bn (42)
I0815 22:42:38.378834 14815 net.cpp:561] out3a/bn <- out3a
I0815 22:42:38.378839 14815 net.cpp:513] out3a/bn -> out3a (in-place)
I0815 22:42:38.379775 14815 net.cpp:245] Setting up out3a/bn
I0815 22:42:38.379784 14815 net.cpp:252] TRAIN Top shape for layer 42 'out3a/bn' 6 64 80 80 (2457600)
I0815 22:42:38.379792 14815 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I0815 22:42:38.379796 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.379801 14815 net.cpp:184] Created Layer out3a/relu (43)
I0815 22:42:38.379804 14815 net.cpp:561] out3a/relu <- out3a
I0815 22:42:38.379807 14815 net.cpp:513] out3a/relu -> out3a (in-place)
I0815 22:42:38.379813 14815 net.cpp:245] Setting up out3a/relu
I0815 22:42:38.379817 14815 net.cpp:252] TRAIN Top shape for layer 43 'out3a/relu' 6 64 80 80 (2457600)
I0815 22:42:38.379822 14815 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I0815 22:42:38.379825 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.395547 14815 net.cpp:184] Created Layer out3_out5_combined (44)
I0815 22:42:38.395566 14815 net.cpp:561] out3_out5_combined <- out5a_up2
I0815 22:42:38.395572 14815 net.cpp:561] out3_out5_combined <- out3a
I0815 22:42:38.395576 14815 net.cpp:530] out3_out5_combined -> out3_out5_combined
I0815 22:42:38.397056 14815 net.cpp:245] Setting up out3_out5_combined
I0815 22:42:38.397069 14815 net.cpp:252] TRAIN Top shape for layer 44 'out3_out5_combined' 6 64 80 80 (2457600)
I0815 22:42:38.397075 14815 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I0815 22:42:38.397080 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.397099 14815 net.cpp:184] Created Layer ctx_conv1 (45)
I0815 22:42:38.397104 14815 net.cpp:561] ctx_conv1 <- out3_out5_combined
I0815 22:42:38.397109 14815 net.cpp:530] ctx_conv1 -> ctx_conv1
I0815 22:42:38.411305 14815 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 7.25G, req 0G)
I0815 22:42:38.411327 14815 net.cpp:245] Setting up ctx_conv1
I0815 22:42:38.411332 14815 net.cpp:252] TRAIN Top shape for layer 45 'ctx_conv1' 6 64 80 80 (2457600)
I0815 22:42:38.411339 14815 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I0815 22:42:38.411343 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.411350 14815 net.cpp:184] Created Layer ctx_conv1/bn (46)
I0815 22:42:38.411353 14815 net.cpp:561] ctx_conv1/bn <- ctx_conv1
I0815 22:42:38.411357 14815 net.cpp:513] ctx_conv1/bn -> ctx_conv1 (in-place)
I0815 22:42:38.412258 14815 net.cpp:245] Setting up ctx_conv1/bn
I0815 22:42:38.412267 14815 net.cpp:252] TRAIN Top shape for layer 46 'ctx_conv1/bn' 6 64 80 80 (2457600)
I0815 22:42:38.412276 14815 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I0815 22:42:38.412279 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.412284 14815 net.cpp:184] Created Layer ctx_conv1/relu (47)
I0815 22:42:38.412287 14815 net.cpp:561] ctx_conv1/relu <- ctx_conv1
I0815 22:42:38.412292 14815 net.cpp:513] ctx_conv1/relu -> ctx_conv1 (in-place)
I0815 22:42:38.412297 14815 net.cpp:245] Setting up ctx_conv1/relu
I0815 22:42:38.412300 14815 net.cpp:252] TRAIN Top shape for layer 47 'ctx_conv1/relu' 6 64 80 80 (2457600)
I0815 22:42:38.412305 14815 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I0815 22:42:38.412309 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.412317 14815 net.cpp:184] Created Layer ctx_conv2 (48)
I0815 22:42:38.412322 14815 net.cpp:561] ctx_conv2 <- ctx_conv1
I0815 22:42:38.412324 14815 net.cpp:530] ctx_conv2 -> ctx_conv2
I0815 22:42:38.413818 14815 net.cpp:245] Setting up ctx_conv2
I0815 22:42:38.413827 14815 net.cpp:252] TRAIN Top shape for layer 48 'ctx_conv2' 6 64 80 80 (2457600)
I0815 22:42:38.413833 14815 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I0815 22:42:38.413836 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.413841 14815 net.cpp:184] Created Layer ctx_conv2/bn (49)
I0815 22:42:38.413846 14815 net.cpp:561] ctx_conv2/bn <- ctx_conv2
I0815 22:42:38.413848 14815 net.cpp:513] ctx_conv2/bn -> ctx_conv2 (in-place)
I0815 22:42:38.414758 14815 net.cpp:245] Setting up ctx_conv2/bn
I0815 22:42:38.414767 14815 net.cpp:252] TRAIN Top shape for layer 49 'ctx_conv2/bn' 6 64 80 80 (2457600)
I0815 22:42:38.414774 14815 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I0815 22:42:38.414778 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.414783 14815 net.cpp:184] Created Layer ctx_conv2/relu (50)
I0815 22:42:38.414786 14815 net.cpp:561] ctx_conv2/relu <- ctx_conv2
I0815 22:42:38.414789 14815 net.cpp:513] ctx_conv2/relu -> ctx_conv2 (in-place)
I0815 22:42:38.414794 14815 net.cpp:245] Setting up ctx_conv2/relu
I0815 22:42:38.414798 14815 net.cpp:252] TRAIN Top shape for layer 50 'ctx_conv2/relu' 6 64 80 80 (2457600)
I0815 22:42:38.414803 14815 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I0815 22:42:38.414806 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.414813 14815 net.cpp:184] Created Layer ctx_conv3 (51)
I0815 22:42:38.414816 14815 net.cpp:561] ctx_conv3 <- ctx_conv2
I0815 22:42:38.414819 14815 net.cpp:530] ctx_conv3 -> ctx_conv3
I0815 22:42:38.416352 14815 net.cpp:245] Setting up ctx_conv3
I0815 22:42:38.416368 14815 net.cpp:252] TRAIN Top shape for layer 51 'ctx_conv3' 6 64 80 80 (2457600)
I0815 22:42:38.416375 14815 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I0815 22:42:38.416380 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.416388 14815 net.cpp:184] Created Layer ctx_conv3/bn (52)
I0815 22:42:38.416407 14815 net.cpp:561] ctx_conv3/bn <- ctx_conv3
I0815 22:42:38.416414 14815 net.cpp:513] ctx_conv3/bn -> ctx_conv3 (in-place)
I0815 22:42:38.417325 14815 net.cpp:245] Setting up ctx_conv3/bn
I0815 22:42:38.417333 14815 net.cpp:252] TRAIN Top shape for layer 52 'ctx_conv3/bn' 6 64 80 80 (2457600)
I0815 22:42:38.417342 14815 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I0815 22:42:38.417346 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.417351 14815 net.cpp:184] Created Layer ctx_conv3/relu (53)
I0815 22:42:38.417356 14815 net.cpp:561] ctx_conv3/relu <- ctx_conv3
I0815 22:42:38.417359 14815 net.cpp:513] ctx_conv3/relu -> ctx_conv3 (in-place)
I0815 22:42:38.417366 14815 net.cpp:245] Setting up ctx_conv3/relu
I0815 22:42:38.417371 14815 net.cpp:252] TRAIN Top shape for layer 53 'ctx_conv3/relu' 6 64 80 80 (2457600)
I0815 22:42:38.417373 14815 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I0815 22:42:38.417378 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.417392 14815 net.cpp:184] Created Layer ctx_conv4 (54)
I0815 22:42:38.417397 14815 net.cpp:561] ctx_conv4 <- ctx_conv3
I0815 22:42:38.417402 14815 net.cpp:530] ctx_conv4 -> ctx_conv4
I0815 22:42:38.418908 14815 net.cpp:245] Setting up ctx_conv4
I0815 22:42:38.418917 14815 net.cpp:252] TRAIN Top shape for layer 54 'ctx_conv4' 6 64 80 80 (2457600)
I0815 22:42:38.418923 14815 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I0815 22:42:38.418928 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.418944 14815 net.cpp:184] Created Layer ctx_conv4/bn (55)
I0815 22:42:38.418949 14815 net.cpp:561] ctx_conv4/bn <- ctx_conv4
I0815 22:42:38.418953 14815 net.cpp:513] ctx_conv4/bn -> ctx_conv4 (in-place)
I0815 22:42:38.419853 14815 net.cpp:245] Setting up ctx_conv4/bn
I0815 22:42:38.419862 14815 net.cpp:252] TRAIN Top shape for layer 55 'ctx_conv4/bn' 6 64 80 80 (2457600)
I0815 22:42:38.419870 14815 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I0815 22:42:38.419874 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.419885 14815 net.cpp:184] Created Layer ctx_conv4/relu (56)
I0815 22:42:38.419889 14815 net.cpp:561] ctx_conv4/relu <- ctx_conv4
I0815 22:42:38.419893 14815 net.cpp:513] ctx_conv4/relu -> ctx_conv4 (in-place)
I0815 22:42:38.419899 14815 net.cpp:245] Setting up ctx_conv4/relu
I0815 22:42:38.419903 14815 net.cpp:252] TRAIN Top shape for layer 56 'ctx_conv4/relu' 6 64 80 80 (2457600)
I0815 22:42:38.419908 14815 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I0815 22:42:38.419911 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.419919 14815 net.cpp:184] Created Layer ctx_final (57)
I0815 22:42:38.419924 14815 net.cpp:561] ctx_final <- ctx_conv4
I0815 22:42:38.419927 14815 net.cpp:530] ctx_final -> ctx_final
I0815 22:42:38.433146 14815 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 7.22G, req 0G)
I0815 22:42:38.433161 14815 net.cpp:245] Setting up ctx_final
I0815 22:42:38.433166 14815 net.cpp:252] TRAIN Top shape for layer 57 'ctx_final' 6 8 80 80 (307200)
I0815 22:42:38.433171 14815 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I0815 22:42:38.433174 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.433178 14815 net.cpp:184] Created Layer ctx_final/relu (58)
I0815 22:42:38.433182 14815 net.cpp:561] ctx_final/relu <- ctx_final
I0815 22:42:38.433183 14815 net.cpp:513] ctx_final/relu -> ctx_final (in-place)
I0815 22:42:38.433188 14815 net.cpp:245] Setting up ctx_final/relu
I0815 22:42:38.433192 14815 net.cpp:252] TRAIN Top shape for layer 58 'ctx_final/relu' 6 8 80 80 (307200)
I0815 22:42:38.433194 14815 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I0815 22:42:38.433209 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.433215 14815 net.cpp:184] Created Layer out_deconv_final_up2 (59)
I0815 22:42:38.433218 14815 net.cpp:561] out_deconv_final_up2 <- ctx_final
I0815 22:42:38.433220 14815 net.cpp:530] out_deconv_final_up2 -> out_deconv_final_up2
I0815 22:42:38.433555 14815 net.cpp:245] Setting up out_deconv_final_up2
I0815 22:42:38.433562 14815 net.cpp:252] TRAIN Top shape for layer 59 'out_deconv_final_up2' 6 8 160 160 (1228800)
I0815 22:42:38.433567 14815 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I0815 22:42:38.433568 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.433576 14815 net.cpp:184] Created Layer out_deconv_final_up4 (60)
I0815 22:42:38.433578 14815 net.cpp:561] out_deconv_final_up4 <- out_deconv_final_up2
I0815 22:42:38.433581 14815 net.cpp:530] out_deconv_final_up4 -> out_deconv_final_up4
I0815 22:42:38.433857 14815 net.cpp:245] Setting up out_deconv_final_up4
I0815 22:42:38.433863 14815 net.cpp:252] TRAIN Top shape for layer 60 'out_deconv_final_up4' 6 8 320 320 (4915200)
I0815 22:42:38.433866 14815 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I0815 22:42:38.433868 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.433873 14815 net.cpp:184] Created Layer out_deconv_final_up8 (61)
I0815 22:42:38.433876 14815 net.cpp:561] out_deconv_final_up8 <- out_deconv_final_up4
I0815 22:42:38.433878 14815 net.cpp:530] out_deconv_final_up8 -> out_deconv_final_up8
I0815 22:42:38.434152 14815 net.cpp:245] Setting up out_deconv_final_up8
I0815 22:42:38.434157 14815 net.cpp:252] TRAIN Top shape for layer 61 'out_deconv_final_up8' 6 8 640 640 (19660800)
I0815 22:42:38.434160 14815 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0815 22:42:38.434164 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.434175 14815 net.cpp:184] Created Layer loss (62)
I0815 22:42:38.434177 14815 net.cpp:561] loss <- out_deconv_final_up8
I0815 22:42:38.434180 14815 net.cpp:561] loss <- label
I0815 22:42:38.434182 14815 net.cpp:530] loss -> loss
I0815 22:42:38.435469 14815 net.cpp:245] Setting up loss
I0815 22:42:38.435477 14815 net.cpp:252] TRAIN Top shape for layer 62 'loss' (1)
I0815 22:42:38.435479 14815 net.cpp:256]     with loss weight 1
I0815 22:42:38.435483 14815 net.cpp:323] loss needs backward computation.
I0815 22:42:38.435487 14815 net.cpp:323] out_deconv_final_up8 needs backward computation.
I0815 22:42:38.435488 14815 net.cpp:323] out_deconv_final_up4 needs backward computation.
I0815 22:42:38.435489 14815 net.cpp:323] out_deconv_final_up2 needs backward computation.
I0815 22:42:38.435492 14815 net.cpp:323] ctx_final/relu needs backward computation.
I0815 22:42:38.435493 14815 net.cpp:323] ctx_final needs backward computation.
I0815 22:42:38.435495 14815 net.cpp:323] ctx_conv4/relu needs backward computation.
I0815 22:42:38.435497 14815 net.cpp:323] ctx_conv4/bn needs backward computation.
I0815 22:42:38.435498 14815 net.cpp:323] ctx_conv4 needs backward computation.
I0815 22:42:38.435500 14815 net.cpp:323] ctx_conv3/relu needs backward computation.
I0815 22:42:38.435503 14815 net.cpp:323] ctx_conv3/bn needs backward computation.
I0815 22:42:38.435504 14815 net.cpp:323] ctx_conv3 needs backward computation.
I0815 22:42:38.435506 14815 net.cpp:323] ctx_conv2/relu needs backward computation.
I0815 22:42:38.435508 14815 net.cpp:323] ctx_conv2/bn needs backward computation.
I0815 22:42:38.435510 14815 net.cpp:323] ctx_conv2 needs backward computation.
I0815 22:42:38.435513 14815 net.cpp:323] ctx_conv1/relu needs backward computation.
I0815 22:42:38.435514 14815 net.cpp:323] ctx_conv1/bn needs backward computation.
I0815 22:42:38.435518 14815 net.cpp:323] ctx_conv1 needs backward computation.
I0815 22:42:38.435525 14815 net.cpp:323] out3_out5_combined needs backward computation.
I0815 22:42:38.435528 14815 net.cpp:323] out3a/relu needs backward computation.
I0815 22:42:38.435529 14815 net.cpp:323] out3a/bn needs backward computation.
I0815 22:42:38.435531 14815 net.cpp:323] out3a needs backward computation.
I0815 22:42:38.435534 14815 net.cpp:323] out5a_up2 needs backward computation.
I0815 22:42:38.435536 14815 net.cpp:323] out5a/relu needs backward computation.
I0815 22:42:38.435539 14815 net.cpp:323] out5a/bn needs backward computation.
I0815 22:42:38.435540 14815 net.cpp:323] out5a needs backward computation.
I0815 22:42:38.435544 14815 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0815 22:42:38.435546 14815 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0815 22:42:38.435549 14815 net.cpp:323] res5a_branch2b needs backward computation.
I0815 22:42:38.435552 14815 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0815 22:42:38.435555 14815 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0815 22:42:38.435559 14815 net.cpp:323] res5a_branch2a needs backward computation.
I0815 22:42:38.435564 14815 net.cpp:323] pool4 needs backward computation.
I0815 22:42:38.435565 14815 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0815 22:42:38.435567 14815 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0815 22:42:38.435570 14815 net.cpp:323] res4a_branch2b needs backward computation.
I0815 22:42:38.435572 14815 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0815 22:42:38.435573 14815 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0815 22:42:38.435575 14815 net.cpp:323] res4a_branch2a needs backward computation.
I0815 22:42:38.435577 14815 net.cpp:323] pool3 needs backward computation.
I0815 22:42:38.435580 14815 net.cpp:323] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I0815 22:42:38.435582 14815 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0815 22:42:38.435585 14815 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0815 22:42:38.435586 14815 net.cpp:323] res3a_branch2b needs backward computation.
I0815 22:42:38.435588 14815 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0815 22:42:38.435591 14815 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0815 22:42:38.435593 14815 net.cpp:323] res3a_branch2a needs backward computation.
I0815 22:42:38.435595 14815 net.cpp:323] pool2 needs backward computation.
I0815 22:42:38.435597 14815 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0815 22:42:38.435600 14815 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0815 22:42:38.435602 14815 net.cpp:323] res2a_branch2b needs backward computation.
I0815 22:42:38.435603 14815 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0815 22:42:38.435606 14815 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0815 22:42:38.435607 14815 net.cpp:323] res2a_branch2a needs backward computation.
I0815 22:42:38.435609 14815 net.cpp:323] pool1 needs backward computation.
I0815 22:42:38.435612 14815 net.cpp:323] conv1b/relu needs backward computation.
I0815 22:42:38.435614 14815 net.cpp:323] conv1b/bn needs backward computation.
I0815 22:42:38.435616 14815 net.cpp:323] conv1b needs backward computation.
I0815 22:42:38.435618 14815 net.cpp:323] conv1a/relu needs backward computation.
I0815 22:42:38.435621 14815 net.cpp:323] conv1a/bn needs backward computation.
I0815 22:42:38.435623 14815 net.cpp:323] conv1a needs backward computation.
I0815 22:42:38.435626 14815 net.cpp:325] data/bias does not need backward computation.
I0815 22:42:38.435628 14815 net.cpp:325] data does not need backward computation.
I0815 22:42:38.435631 14815 net.cpp:367] This network produces output loss
I0815 22:42:38.435678 14815 net.cpp:389] Top memory (TRAIN) required for data: 956006400 diff: 946176008
I0815 22:42:38.435681 14815 net.cpp:392] Bottom memory (TRAIN) required for data: 956006400 diff: 956006400
I0815 22:42:38.435683 14815 net.cpp:395] Shared (in-place) memory (TRAIN) by data: 630374400 diff: 630374400
I0815 22:42:38.435689 14815 net.cpp:398] Parameters memory (TRAIN) required for data: 2692608 diff: 2692608
I0815 22:42:38.435691 14815 net.cpp:401] Parameters shared memory (TRAIN) by data: 0 diff: 0
I0815 22:42:38.435693 14815 net.cpp:407] Network initialization done.
I0815 22:42:38.448155 14815 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/sparse/test.prototxt
W0815 22:42:38.448247 14815 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0815 22:42:38.448547 14815 net.cpp:72] Initializing net from parameters: 
name: "jsegnet21v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/val-image-lmdb"
    label_list_path: "data/val-label-lmdb"
    batch_size: 2
    threads: 1
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
  accuracy_param {
    ignore_label: 255
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
    ignore_label: 255
  }
}
I0815 22:42:38.448760 14815 net.cpp:104] Using FLOAT as default forward math type
I0815 22:42:38.448766 14815 net.cpp:110] Using FLOAT as default backward math type
I0815 22:42:38.448776 14815 layer_factory.hpp:136] Creating layer 'data' of type 'ImageLabelData'
I0815 22:42:38.448779 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.448786 14815 net.cpp:184] Created Layer data (0)
I0815 22:42:38.448789 14815 net.cpp:530] data -> data
I0815 22:42:38.448793 14815 net.cpp:530] data -> label
I0815 22:42:38.448817 14815 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0815 22:42:38.448823 14815 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 22:42:38.450192 14888 db_lmdb.cpp:24] Opened lmdb data/val-image-lmdb
I0815 22:42:38.451623 14815 data_layer.cpp:185] (0) ReshapePrefetch 2, 3, 640, 640
I0815 22:42:38.451696 14815 data_layer.cpp:209] (0) Output data size: 2, 3, 640, 640
I0815 22:42:38.451704 14815 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 22:42:38.451758 14815 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0815 22:42:38.451771 14815 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 22:42:38.452522 14889 data_layer.cpp:97] (0) Parser threads: 1
I0815 22:42:38.452536 14889 data_layer.cpp:99] (0) Transformer threads: 1
I0815 22:42:38.454927 14890 db_lmdb.cpp:24] Opened lmdb data/val-label-lmdb
I0815 22:42:38.456228 14815 data_layer.cpp:185] (0) ReshapePrefetch 2, 1, 640, 640
I0815 22:42:38.456341 14815 data_layer.cpp:209] (0) Output data size: 2, 1, 640, 640
I0815 22:42:38.456348 14815 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 22:42:38.456400 14815 net.cpp:245] Setting up data
I0815 22:42:38.456411 14815 net.cpp:252] TEST Top shape for layer 0 'data' 2 3 640 640 (2457600)
I0815 22:42:38.456418 14815 net.cpp:252] TEST Top shape for layer 0 'data' 2 1 640 640 (819200)
I0815 22:42:38.456425 14815 layer_factory.hpp:136] Creating layer 'label_data_1_split' of type 'Split'
I0815 22:42:38.456446 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.456459 14815 net.cpp:184] Created Layer label_data_1_split (1)
I0815 22:42:38.456463 14815 net.cpp:561] label_data_1_split <- label
I0815 22:42:38.456470 14815 net.cpp:530] label_data_1_split -> label_data_1_split_0
I0815 22:42:38.456478 14815 net.cpp:530] label_data_1_split -> label_data_1_split_1
I0815 22:42:38.456483 14815 net.cpp:530] label_data_1_split -> label_data_1_split_2
I0815 22:42:38.456605 14815 net.cpp:245] Setting up label_data_1_split
I0815 22:42:38.456612 14815 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0815 22:42:38.456617 14815 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0815 22:42:38.456622 14815 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0815 22:42:38.456627 14815 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0815 22:42:38.456638 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.456646 14815 net.cpp:184] Created Layer data/bias (2)
I0815 22:42:38.456660 14815 net.cpp:561] data/bias <- data
I0815 22:42:38.456670 14815 net.cpp:530] data/bias -> data/bias
I0815 22:42:38.457923 14891 data_layer.cpp:97] (0) Parser threads: 1
I0815 22:42:38.457934 14891 data_layer.cpp:99] (0) Transformer threads: 1
I0815 22:42:38.459810 14815 net.cpp:245] Setting up data/bias
I0815 22:42:38.459831 14815 net.cpp:252] TEST Top shape for layer 2 'data/bias' 2 3 640 640 (2457600)
I0815 22:42:38.459846 14815 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0815 22:42:38.459852 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.459877 14815 net.cpp:184] Created Layer conv1a (3)
I0815 22:42:38.459893 14815 net.cpp:561] conv1a <- data/bias
I0815 22:42:38.459899 14815 net.cpp:530] conv1a -> conv1a
I0815 22:42:38.466639 14815 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.09G, req 0G)
I0815 22:42:38.466678 14815 net.cpp:245] Setting up conv1a
I0815 22:42:38.466686 14815 net.cpp:252] TEST Top shape for layer 3 'conv1a' 2 32 320 320 (6553600)
I0815 22:42:38.466698 14815 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0815 22:42:38.466706 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.466722 14815 net.cpp:184] Created Layer conv1a/bn (4)
I0815 22:42:38.466732 14815 net.cpp:561] conv1a/bn <- conv1a
I0815 22:42:38.466737 14815 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0815 22:42:38.467732 14815 net.cpp:245] Setting up conv1a/bn
I0815 22:42:38.467742 14815 net.cpp:252] TEST Top shape for layer 4 'conv1a/bn' 2 32 320 320 (6553600)
I0815 22:42:38.467752 14815 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0815 22:42:38.467756 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.467761 14815 net.cpp:184] Created Layer conv1a/relu (5)
I0815 22:42:38.467763 14815 net.cpp:561] conv1a/relu <- conv1a
I0815 22:42:38.467767 14815 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0815 22:42:38.467773 14815 net.cpp:245] Setting up conv1a/relu
I0815 22:42:38.467777 14815 net.cpp:252] TEST Top shape for layer 5 'conv1a/relu' 2 32 320 320 (6553600)
I0815 22:42:38.467782 14815 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0815 22:42:38.467784 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.467794 14815 net.cpp:184] Created Layer conv1b (6)
I0815 22:42:38.467798 14815 net.cpp:561] conv1b <- conv1a
I0815 22:42:38.467803 14815 net.cpp:530] conv1b -> conv1b
I0815 22:42:38.481931 14815 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.06G, req 0G)
I0815 22:42:38.481945 14815 net.cpp:245] Setting up conv1b
I0815 22:42:38.481951 14815 net.cpp:252] TEST Top shape for layer 6 'conv1b' 2 32 320 320 (6553600)
I0815 22:42:38.481959 14815 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0815 22:42:38.481964 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.481971 14815 net.cpp:184] Created Layer conv1b/bn (7)
I0815 22:42:38.481976 14815 net.cpp:561] conv1b/bn <- conv1b
I0815 22:42:38.481979 14815 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0815 22:42:38.482906 14815 net.cpp:245] Setting up conv1b/bn
I0815 22:42:38.482915 14815 net.cpp:252] TEST Top shape for layer 7 'conv1b/bn' 2 32 320 320 (6553600)
I0815 22:42:38.482924 14815 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0815 22:42:38.482928 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.482937 14815 net.cpp:184] Created Layer conv1b/relu (8)
I0815 22:42:38.482941 14815 net.cpp:561] conv1b/relu <- conv1b
I0815 22:42:38.482945 14815 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0815 22:42:38.482950 14815 net.cpp:245] Setting up conv1b/relu
I0815 22:42:38.482955 14815 net.cpp:252] TEST Top shape for layer 8 'conv1b/relu' 2 32 320 320 (6553600)
I0815 22:42:38.482959 14815 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0815 22:42:38.482962 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.482969 14815 net.cpp:184] Created Layer pool1 (9)
I0815 22:42:38.482973 14815 net.cpp:561] pool1 <- conv1b
I0815 22:42:38.482976 14815 net.cpp:530] pool1 -> pool1
I0815 22:42:38.483062 14815 net.cpp:245] Setting up pool1
I0815 22:42:38.483068 14815 net.cpp:252] TEST Top shape for layer 9 'pool1' 2 32 160 160 (1638400)
I0815 22:42:38.483072 14815 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0815 22:42:38.483077 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.483085 14815 net.cpp:184] Created Layer res2a_branch2a (10)
I0815 22:42:38.483090 14815 net.cpp:561] res2a_branch2a <- pool1
I0815 22:42:38.483103 14815 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0815 22:42:38.491924 14815 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.03G, req 0G)
I0815 22:42:38.491935 14815 net.cpp:245] Setting up res2a_branch2a
I0815 22:42:38.491940 14815 net.cpp:252] TEST Top shape for layer 10 'res2a_branch2a' 2 64 160 160 (3276800)
I0815 22:42:38.491948 14815 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0815 22:42:38.491952 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.491958 14815 net.cpp:184] Created Layer res2a_branch2a/bn (11)
I0815 22:42:38.491962 14815 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0815 22:42:38.491966 14815 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0815 22:42:38.492897 14815 net.cpp:245] Setting up res2a_branch2a/bn
I0815 22:42:38.492907 14815 net.cpp:252] TEST Top shape for layer 11 'res2a_branch2a/bn' 2 64 160 160 (3276800)
I0815 22:42:38.492914 14815 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0815 22:42:38.492918 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.492923 14815 net.cpp:184] Created Layer res2a_branch2a/relu (12)
I0815 22:42:38.492926 14815 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0815 22:42:38.492930 14815 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0815 22:42:38.492935 14815 net.cpp:245] Setting up res2a_branch2a/relu
I0815 22:42:38.492939 14815 net.cpp:252] TEST Top shape for layer 12 'res2a_branch2a/relu' 2 64 160 160 (3276800)
I0815 22:42:38.492944 14815 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0815 22:42:38.492946 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.492954 14815 net.cpp:184] Created Layer res2a_branch2b (13)
I0815 22:42:38.492959 14815 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0815 22:42:38.492961 14815 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0815 22:42:38.499900 14815 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.02G, req 0G)
I0815 22:42:38.499910 14815 net.cpp:245] Setting up res2a_branch2b
I0815 22:42:38.499917 14815 net.cpp:252] TEST Top shape for layer 13 'res2a_branch2b' 2 64 160 160 (3276800)
I0815 22:42:38.499922 14815 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0815 22:42:38.499927 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.499933 14815 net.cpp:184] Created Layer res2a_branch2b/bn (14)
I0815 22:42:38.499938 14815 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0815 22:42:38.499941 14815 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0815 22:42:38.500844 14815 net.cpp:245] Setting up res2a_branch2b/bn
I0815 22:42:38.500852 14815 net.cpp:252] TEST Top shape for layer 14 'res2a_branch2b/bn' 2 64 160 160 (3276800)
I0815 22:42:38.500861 14815 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0815 22:42:38.500865 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.500870 14815 net.cpp:184] Created Layer res2a_branch2b/relu (15)
I0815 22:42:38.500874 14815 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0815 22:42:38.500879 14815 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0815 22:42:38.500885 14815 net.cpp:245] Setting up res2a_branch2b/relu
I0815 22:42:38.500888 14815 net.cpp:252] TEST Top shape for layer 15 'res2a_branch2b/relu' 2 64 160 160 (3276800)
I0815 22:42:38.500892 14815 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0815 22:42:38.500896 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.500902 14815 net.cpp:184] Created Layer pool2 (16)
I0815 22:42:38.500906 14815 net.cpp:561] pool2 <- res2a_branch2b
I0815 22:42:38.500918 14815 net.cpp:530] pool2 -> pool2
I0815 22:42:38.501010 14815 net.cpp:245] Setting up pool2
I0815 22:42:38.501016 14815 net.cpp:252] TEST Top shape for layer 16 'pool2' 2 64 80 80 (819200)
I0815 22:42:38.501021 14815 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0815 22:42:38.501025 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.501034 14815 net.cpp:184] Created Layer res3a_branch2a (17)
I0815 22:42:38.501039 14815 net.cpp:561] res3a_branch2a <- pool2
I0815 22:42:38.501042 14815 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0815 22:42:38.507601 14815 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.01G, req 0G)
I0815 22:42:38.507614 14815 net.cpp:245] Setting up res3a_branch2a
I0815 22:42:38.507621 14815 net.cpp:252] TEST Top shape for layer 17 'res3a_branch2a' 2 128 80 80 (1638400)
I0815 22:42:38.507627 14815 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0815 22:42:38.507632 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.507638 14815 net.cpp:184] Created Layer res3a_branch2a/bn (18)
I0815 22:42:38.507642 14815 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0815 22:42:38.507647 14815 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0815 22:42:38.508569 14815 net.cpp:245] Setting up res3a_branch2a/bn
I0815 22:42:38.508579 14815 net.cpp:252] TEST Top shape for layer 18 'res3a_branch2a/bn' 2 128 80 80 (1638400)
I0815 22:42:38.508591 14815 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0815 22:42:38.508595 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.508600 14815 net.cpp:184] Created Layer res3a_branch2a/relu (19)
I0815 22:42:38.508604 14815 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0815 22:42:38.508607 14815 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0815 22:42:38.508612 14815 net.cpp:245] Setting up res3a_branch2a/relu
I0815 22:42:38.508617 14815 net.cpp:252] TEST Top shape for layer 19 'res3a_branch2a/relu' 2 128 80 80 (1638400)
I0815 22:42:38.508621 14815 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0815 22:42:38.508625 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.508638 14815 net.cpp:184] Created Layer res3a_branch2b (20)
I0815 22:42:38.508642 14815 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0815 22:42:38.508646 14815 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0815 22:42:38.514272 14815 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7G, req 0G)
I0815 22:42:38.514287 14815 net.cpp:245] Setting up res3a_branch2b
I0815 22:42:38.514293 14815 net.cpp:252] TEST Top shape for layer 20 'res3a_branch2b' 2 128 80 80 (1638400)
I0815 22:42:38.514300 14815 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0815 22:42:38.514304 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.514312 14815 net.cpp:184] Created Layer res3a_branch2b/bn (21)
I0815 22:42:38.514317 14815 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0815 22:42:38.514320 14815 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0815 22:42:38.515285 14815 net.cpp:245] Setting up res3a_branch2b/bn
I0815 22:42:38.515296 14815 net.cpp:252] TEST Top shape for layer 21 'res3a_branch2b/bn' 2 128 80 80 (1638400)
I0815 22:42:38.515305 14815 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0815 22:42:38.515310 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.515314 14815 net.cpp:184] Created Layer res3a_branch2b/relu (22)
I0815 22:42:38.515318 14815 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0815 22:42:38.515322 14815 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0815 22:42:38.515338 14815 net.cpp:245] Setting up res3a_branch2b/relu
I0815 22:42:38.515343 14815 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b/relu' 2 128 80 80 (1638400)
I0815 22:42:38.515347 14815 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0815 22:42:38.515349 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.515354 14815 net.cpp:184] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (23)
I0815 22:42:38.515357 14815 net.cpp:561] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0815 22:42:38.515360 14815 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0815 22:42:38.515367 14815 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0815 22:42:38.515430 14815 net.cpp:245] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0815 22:42:38.515436 14815 net.cpp:252] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 2 128 80 80 (1638400)
I0815 22:42:38.515441 14815 net.cpp:252] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 2 128 80 80 (1638400)
I0815 22:42:38.515450 14815 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0815 22:42:38.515455 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.515461 14815 net.cpp:184] Created Layer pool3 (24)
I0815 22:42:38.515465 14815 net.cpp:561] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0815 22:42:38.515470 14815 net.cpp:530] pool3 -> pool3
I0815 22:42:38.515555 14815 net.cpp:245] Setting up pool3
I0815 22:42:38.515561 14815 net.cpp:252] TEST Top shape for layer 24 'pool3' 2 128 40 40 (409600)
I0815 22:42:38.515566 14815 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0815 22:42:38.515570 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.515662 14815 net.cpp:184] Created Layer res4a_branch2a (25)
I0815 22:42:38.515667 14815 net.cpp:561] res4a_branch2a <- pool3
I0815 22:42:38.515672 14815 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0815 22:42:38.529551 14815 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.99G, req 0G)
I0815 22:42:38.529563 14815 net.cpp:245] Setting up res4a_branch2a
I0815 22:42:38.529568 14815 net.cpp:252] TEST Top shape for layer 25 'res4a_branch2a' 2 256 40 40 (819200)
I0815 22:42:38.529575 14815 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0815 22:42:38.529580 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.529593 14815 net.cpp:184] Created Layer res4a_branch2a/bn (26)
I0815 22:42:38.529597 14815 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0815 22:42:38.529602 14815 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0815 22:42:38.530496 14815 net.cpp:245] Setting up res4a_branch2a/bn
I0815 22:42:38.530505 14815 net.cpp:252] TEST Top shape for layer 26 'res4a_branch2a/bn' 2 256 40 40 (819200)
I0815 22:42:38.530514 14815 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0815 22:42:38.530517 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.530521 14815 net.cpp:184] Created Layer res4a_branch2a/relu (27)
I0815 22:42:38.530525 14815 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0815 22:42:38.530529 14815 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0815 22:42:38.530534 14815 net.cpp:245] Setting up res4a_branch2a/relu
I0815 22:42:38.530539 14815 net.cpp:252] TEST Top shape for layer 27 'res4a_branch2a/relu' 2 256 40 40 (819200)
I0815 22:42:38.530542 14815 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0815 22:42:38.530546 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.530565 14815 net.cpp:184] Created Layer res4a_branch2b (28)
I0815 22:42:38.530568 14815 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0815 22:42:38.530571 14815 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0815 22:42:38.538686 14815 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.98G, req 0G)
I0815 22:42:38.538699 14815 net.cpp:245] Setting up res4a_branch2b
I0815 22:42:38.538705 14815 net.cpp:252] TEST Top shape for layer 28 'res4a_branch2b' 2 256 40 40 (819200)
I0815 22:42:38.538712 14815 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0815 22:42:38.538717 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.538730 14815 net.cpp:184] Created Layer res4a_branch2b/bn (29)
I0815 22:42:38.538734 14815 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0815 22:42:38.538748 14815 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0815 22:42:38.539665 14815 net.cpp:245] Setting up res4a_branch2b/bn
I0815 22:42:38.539674 14815 net.cpp:252] TEST Top shape for layer 29 'res4a_branch2b/bn' 2 256 40 40 (819200)
I0815 22:42:38.539682 14815 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0815 22:42:38.539686 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.539691 14815 net.cpp:184] Created Layer res4a_branch2b/relu (30)
I0815 22:42:38.539695 14815 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0815 22:42:38.539698 14815 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0815 22:42:38.539703 14815 net.cpp:245] Setting up res4a_branch2b/relu
I0815 22:42:38.539708 14815 net.cpp:252] TEST Top shape for layer 30 'res4a_branch2b/relu' 2 256 40 40 (819200)
I0815 22:42:38.539712 14815 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0815 22:42:38.539717 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.539723 14815 net.cpp:184] Created Layer pool4 (31)
I0815 22:42:38.539726 14815 net.cpp:561] pool4 <- res4a_branch2b
I0815 22:42:38.539731 14815 net.cpp:530] pool4 -> pool4
I0815 22:42:38.539818 14815 net.cpp:245] Setting up pool4
I0815 22:42:38.539824 14815 net.cpp:252] TEST Top shape for layer 31 'pool4' 2 256 40 40 (819200)
I0815 22:42:38.539827 14815 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0815 22:42:38.539831 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.539846 14815 net.cpp:184] Created Layer res5a_branch2a (32)
I0815 22:42:38.539851 14815 net.cpp:561] res5a_branch2a <- pool4
I0815 22:42:38.539855 14815 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0815 22:42:38.573050 14815 net.cpp:245] Setting up res5a_branch2a
I0815 22:42:38.573079 14815 net.cpp:252] TEST Top shape for layer 32 'res5a_branch2a' 2 512 40 40 (1638400)
I0815 22:42:38.573088 14815 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0815 22:42:38.573093 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.573103 14815 net.cpp:184] Created Layer res5a_branch2a/bn (33)
I0815 22:42:38.573108 14815 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0815 22:42:38.573112 14815 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0815 22:42:38.573984 14815 net.cpp:245] Setting up res5a_branch2a/bn
I0815 22:42:38.573994 14815 net.cpp:252] TEST Top shape for layer 33 'res5a_branch2a/bn' 2 512 40 40 (1638400)
I0815 22:42:38.574002 14815 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0815 22:42:38.574007 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.574010 14815 net.cpp:184] Created Layer res5a_branch2a/relu (34)
I0815 22:42:38.574014 14815 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0815 22:42:38.574018 14815 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0815 22:42:38.574033 14815 net.cpp:245] Setting up res5a_branch2a/relu
I0815 22:42:38.574038 14815 net.cpp:252] TEST Top shape for layer 34 'res5a_branch2a/relu' 2 512 40 40 (1638400)
I0815 22:42:38.574043 14815 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0815 22:42:38.574046 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.574056 14815 net.cpp:184] Created Layer res5a_branch2b (35)
I0815 22:42:38.574060 14815 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0815 22:42:38.574064 14815 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0815 22:42:38.591336 14815 net.cpp:245] Setting up res5a_branch2b
I0815 22:42:38.591358 14815 net.cpp:252] TEST Top shape for layer 35 'res5a_branch2b' 2 512 40 40 (1638400)
I0815 22:42:38.591370 14815 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0815 22:42:38.591375 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.591385 14815 net.cpp:184] Created Layer res5a_branch2b/bn (36)
I0815 22:42:38.591389 14815 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0815 22:42:38.591394 14815 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0815 22:42:38.592288 14815 net.cpp:245] Setting up res5a_branch2b/bn
I0815 22:42:38.592296 14815 net.cpp:252] TEST Top shape for layer 36 'res5a_branch2b/bn' 2 512 40 40 (1638400)
I0815 22:42:38.592305 14815 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0815 22:42:38.592308 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.592314 14815 net.cpp:184] Created Layer res5a_branch2b/relu (37)
I0815 22:42:38.592316 14815 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0815 22:42:38.592319 14815 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0815 22:42:38.592325 14815 net.cpp:245] Setting up res5a_branch2b/relu
I0815 22:42:38.592329 14815 net.cpp:252] TEST Top shape for layer 37 'res5a_branch2b/relu' 2 512 40 40 (1638400)
I0815 22:42:38.592334 14815 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I0815 22:42:38.592339 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.592352 14815 net.cpp:184] Created Layer out5a (38)
I0815 22:42:38.592356 14815 net.cpp:561] out5a <- res5a_branch2b
I0815 22:42:38.592360 14815 net.cpp:530] out5a -> out5a
I0815 22:42:38.596719 14815 net.cpp:245] Setting up out5a
I0815 22:42:38.596729 14815 net.cpp:252] TEST Top shape for layer 38 'out5a' 2 64 40 40 (204800)
I0815 22:42:38.596735 14815 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I0815 22:42:38.596738 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.596745 14815 net.cpp:184] Created Layer out5a/bn (39)
I0815 22:42:38.596747 14815 net.cpp:561] out5a/bn <- out5a
I0815 22:42:38.596751 14815 net.cpp:513] out5a/bn -> out5a (in-place)
I0815 22:42:38.597973 14815 net.cpp:245] Setting up out5a/bn
I0815 22:42:38.597983 14815 net.cpp:252] TEST Top shape for layer 39 'out5a/bn' 2 64 40 40 (204800)
I0815 22:42:38.597991 14815 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I0815 22:42:38.597995 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.598001 14815 net.cpp:184] Created Layer out5a/relu (40)
I0815 22:42:38.598003 14815 net.cpp:561] out5a/relu <- out5a
I0815 22:42:38.598007 14815 net.cpp:513] out5a/relu -> out5a (in-place)
I0815 22:42:38.598012 14815 net.cpp:245] Setting up out5a/relu
I0815 22:42:38.598016 14815 net.cpp:252] TEST Top shape for layer 40 'out5a/relu' 2 64 40 40 (204800)
I0815 22:42:38.598021 14815 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I0815 22:42:38.598024 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.598042 14815 net.cpp:184] Created Layer out5a_up2 (41)
I0815 22:42:38.598045 14815 net.cpp:561] out5a_up2 <- out5a
I0815 22:42:38.598048 14815 net.cpp:530] out5a_up2 -> out5a_up2
I0815 22:42:38.598459 14815 net.cpp:245] Setting up out5a_up2
I0815 22:42:38.598467 14815 net.cpp:252] TEST Top shape for layer 41 'out5a_up2' 2 64 80 80 (819200)
I0815 22:42:38.598471 14815 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I0815 22:42:38.598475 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.598489 14815 net.cpp:184] Created Layer out3a (42)
I0815 22:42:38.598492 14815 net.cpp:561] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0815 22:42:38.598496 14815 net.cpp:530] out3a -> out3a
I0815 22:42:38.603363 14815 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.97G, req 0G)
I0815 22:42:38.603377 14815 net.cpp:245] Setting up out3a
I0815 22:42:38.603384 14815 net.cpp:252] TEST Top shape for layer 42 'out3a' 2 64 80 80 (819200)
I0815 22:42:38.603390 14815 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I0815 22:42:38.603395 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.603404 14815 net.cpp:184] Created Layer out3a/bn (43)
I0815 22:42:38.603408 14815 net.cpp:561] out3a/bn <- out3a
I0815 22:42:38.603412 14815 net.cpp:513] out3a/bn -> out3a (in-place)
I0815 22:42:38.604383 14815 net.cpp:245] Setting up out3a/bn
I0815 22:42:38.604393 14815 net.cpp:252] TEST Top shape for layer 43 'out3a/bn' 2 64 80 80 (819200)
I0815 22:42:38.604401 14815 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I0815 22:42:38.604405 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.604409 14815 net.cpp:184] Created Layer out3a/relu (44)
I0815 22:42:38.604413 14815 net.cpp:561] out3a/relu <- out3a
I0815 22:42:38.604416 14815 net.cpp:513] out3a/relu -> out3a (in-place)
I0815 22:42:38.604423 14815 net.cpp:245] Setting up out3a/relu
I0815 22:42:38.604426 14815 net.cpp:252] TEST Top shape for layer 44 'out3a/relu' 2 64 80 80 (819200)
I0815 22:42:38.604430 14815 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I0815 22:42:38.604434 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.604439 14815 net.cpp:184] Created Layer out3_out5_combined (45)
I0815 22:42:38.604444 14815 net.cpp:561] out3_out5_combined <- out5a_up2
I0815 22:42:38.604447 14815 net.cpp:561] out3_out5_combined <- out3a
I0815 22:42:38.604451 14815 net.cpp:530] out3_out5_combined -> out3_out5_combined
I0815 22:42:38.605695 14815 net.cpp:245] Setting up out3_out5_combined
I0815 22:42:38.605705 14815 net.cpp:252] TEST Top shape for layer 45 'out3_out5_combined' 2 64 80 80 (819200)
I0815 22:42:38.605708 14815 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I0815 22:42:38.605711 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.605721 14815 net.cpp:184] Created Layer ctx_conv1 (46)
I0815 22:42:38.605725 14815 net.cpp:561] ctx_conv1 <- out3_out5_combined
I0815 22:42:38.605728 14815 net.cpp:530] ctx_conv1 -> ctx_conv1
I0815 22:42:38.610385 14815 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.96G, req 0G)
I0815 22:42:38.610404 14815 net.cpp:245] Setting up ctx_conv1
I0815 22:42:38.610409 14815 net.cpp:252] TEST Top shape for layer 46 'ctx_conv1' 2 64 80 80 (819200)
I0815 22:42:38.610417 14815 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I0815 22:42:38.610422 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.610431 14815 net.cpp:184] Created Layer ctx_conv1/bn (47)
I0815 22:42:38.610435 14815 net.cpp:561] ctx_conv1/bn <- ctx_conv1
I0815 22:42:38.610440 14815 net.cpp:513] ctx_conv1/bn -> ctx_conv1 (in-place)
I0815 22:42:38.611415 14815 net.cpp:245] Setting up ctx_conv1/bn
I0815 22:42:38.611438 14815 net.cpp:252] TEST Top shape for layer 47 'ctx_conv1/bn' 2 64 80 80 (819200)
I0815 22:42:38.611448 14815 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I0815 22:42:38.611451 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.611456 14815 net.cpp:184] Created Layer ctx_conv1/relu (48)
I0815 22:42:38.611459 14815 net.cpp:561] ctx_conv1/relu <- ctx_conv1
I0815 22:42:38.611464 14815 net.cpp:513] ctx_conv1/relu -> ctx_conv1 (in-place)
I0815 22:42:38.611469 14815 net.cpp:245] Setting up ctx_conv1/relu
I0815 22:42:38.611474 14815 net.cpp:252] TEST Top shape for layer 48 'ctx_conv1/relu' 2 64 80 80 (819200)
I0815 22:42:38.611477 14815 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I0815 22:42:38.611481 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.611495 14815 net.cpp:184] Created Layer ctx_conv2 (49)
I0815 22:42:38.611498 14815 net.cpp:561] ctx_conv2 <- ctx_conv1
I0815 22:42:38.611502 14815 net.cpp:530] ctx_conv2 -> ctx_conv2
I0815 22:42:38.613015 14815 net.cpp:245] Setting up ctx_conv2
I0815 22:42:38.613025 14815 net.cpp:252] TEST Top shape for layer 49 'ctx_conv2' 2 64 80 80 (819200)
I0815 22:42:38.613030 14815 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I0815 22:42:38.613034 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.613040 14815 net.cpp:184] Created Layer ctx_conv2/bn (50)
I0815 22:42:38.613044 14815 net.cpp:561] ctx_conv2/bn <- ctx_conv2
I0815 22:42:38.613047 14815 net.cpp:513] ctx_conv2/bn -> ctx_conv2 (in-place)
I0815 22:42:38.613965 14815 net.cpp:245] Setting up ctx_conv2/bn
I0815 22:42:38.613975 14815 net.cpp:252] TEST Top shape for layer 50 'ctx_conv2/bn' 2 64 80 80 (819200)
I0815 22:42:38.613982 14815 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I0815 22:42:38.613986 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.613992 14815 net.cpp:184] Created Layer ctx_conv2/relu (51)
I0815 22:42:38.613996 14815 net.cpp:561] ctx_conv2/relu <- ctx_conv2
I0815 22:42:38.613999 14815 net.cpp:513] ctx_conv2/relu -> ctx_conv2 (in-place)
I0815 22:42:38.614004 14815 net.cpp:245] Setting up ctx_conv2/relu
I0815 22:42:38.614009 14815 net.cpp:252] TEST Top shape for layer 51 'ctx_conv2/relu' 2 64 80 80 (819200)
I0815 22:42:38.614015 14815 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I0815 22:42:38.614019 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.614027 14815 net.cpp:184] Created Layer ctx_conv3 (52)
I0815 22:42:38.614030 14815 net.cpp:561] ctx_conv3 <- ctx_conv2
I0815 22:42:38.614033 14815 net.cpp:530] ctx_conv3 -> ctx_conv3
I0815 22:42:38.615587 14815 net.cpp:245] Setting up ctx_conv3
I0815 22:42:38.615604 14815 net.cpp:252] TEST Top shape for layer 52 'ctx_conv3' 2 64 80 80 (819200)
I0815 22:42:38.615612 14815 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I0815 22:42:38.615617 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.615624 14815 net.cpp:184] Created Layer ctx_conv3/bn (53)
I0815 22:42:38.615628 14815 net.cpp:561] ctx_conv3/bn <- ctx_conv3
I0815 22:42:38.615633 14815 net.cpp:513] ctx_conv3/bn -> ctx_conv3 (in-place)
I0815 22:42:38.616565 14815 net.cpp:245] Setting up ctx_conv3/bn
I0815 22:42:38.616575 14815 net.cpp:252] TEST Top shape for layer 53 'ctx_conv3/bn' 2 64 80 80 (819200)
I0815 22:42:38.616582 14815 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I0815 22:42:38.616585 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.616590 14815 net.cpp:184] Created Layer ctx_conv3/relu (54)
I0815 22:42:38.616593 14815 net.cpp:561] ctx_conv3/relu <- ctx_conv3
I0815 22:42:38.616605 14815 net.cpp:513] ctx_conv3/relu -> ctx_conv3 (in-place)
I0815 22:42:38.616611 14815 net.cpp:245] Setting up ctx_conv3/relu
I0815 22:42:38.616616 14815 net.cpp:252] TEST Top shape for layer 54 'ctx_conv3/relu' 2 64 80 80 (819200)
I0815 22:42:38.616621 14815 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I0815 22:42:38.616623 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.616631 14815 net.cpp:184] Created Layer ctx_conv4 (55)
I0815 22:42:38.616636 14815 net.cpp:561] ctx_conv4 <- ctx_conv3
I0815 22:42:38.616639 14815 net.cpp:530] ctx_conv4 -> ctx_conv4
I0815 22:42:38.618109 14815 net.cpp:245] Setting up ctx_conv4
I0815 22:42:38.618118 14815 net.cpp:252] TEST Top shape for layer 55 'ctx_conv4' 2 64 80 80 (819200)
I0815 22:42:38.618124 14815 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I0815 22:42:38.618126 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.618132 14815 net.cpp:184] Created Layer ctx_conv4/bn (56)
I0815 22:42:38.618135 14815 net.cpp:561] ctx_conv4/bn <- ctx_conv4
I0815 22:42:38.618139 14815 net.cpp:513] ctx_conv4/bn -> ctx_conv4 (in-place)
I0815 22:42:38.619069 14815 net.cpp:245] Setting up ctx_conv4/bn
I0815 22:42:38.619077 14815 net.cpp:252] TEST Top shape for layer 56 'ctx_conv4/bn' 2 64 80 80 (819200)
I0815 22:42:38.619086 14815 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I0815 22:42:38.619089 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.619093 14815 net.cpp:184] Created Layer ctx_conv4/relu (57)
I0815 22:42:38.619096 14815 net.cpp:561] ctx_conv4/relu <- ctx_conv4
I0815 22:42:38.619101 14815 net.cpp:513] ctx_conv4/relu -> ctx_conv4 (in-place)
I0815 22:42:38.619104 14815 net.cpp:245] Setting up ctx_conv4/relu
I0815 22:42:38.619108 14815 net.cpp:252] TEST Top shape for layer 57 'ctx_conv4/relu' 2 64 80 80 (819200)
I0815 22:42:38.619112 14815 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I0815 22:42:38.619117 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.619123 14815 net.cpp:184] Created Layer ctx_final (58)
I0815 22:42:38.619127 14815 net.cpp:561] ctx_final <- ctx_conv4
I0815 22:42:38.619130 14815 net.cpp:530] ctx_final -> ctx_final
I0815 22:42:38.624081 14815 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.96G, req 0G)
I0815 22:42:38.624094 14815 net.cpp:245] Setting up ctx_final
I0815 22:42:38.624099 14815 net.cpp:252] TEST Top shape for layer 58 'ctx_final' 2 8 80 80 (102400)
I0815 22:42:38.624105 14815 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I0815 22:42:38.624109 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.624114 14815 net.cpp:184] Created Layer ctx_final/relu (59)
I0815 22:42:38.624117 14815 net.cpp:561] ctx_final/relu <- ctx_final
I0815 22:42:38.624121 14815 net.cpp:513] ctx_final/relu -> ctx_final (in-place)
I0815 22:42:38.624143 14815 net.cpp:245] Setting up ctx_final/relu
I0815 22:42:38.624150 14815 net.cpp:252] TEST Top shape for layer 59 'ctx_final/relu' 2 8 80 80 (102400)
I0815 22:42:38.624152 14815 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I0815 22:42:38.624156 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.624168 14815 net.cpp:184] Created Layer out_deconv_final_up2 (60)
I0815 22:42:38.624172 14815 net.cpp:561] out_deconv_final_up2 <- ctx_final
I0815 22:42:38.624176 14815 net.cpp:530] out_deconv_final_up2 -> out_deconv_final_up2
I0815 22:42:38.624590 14815 net.cpp:245] Setting up out_deconv_final_up2
I0815 22:42:38.624598 14815 net.cpp:252] TEST Top shape for layer 60 'out_deconv_final_up2' 2 8 160 160 (409600)
I0815 22:42:38.624603 14815 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I0815 22:42:38.624614 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.624620 14815 net.cpp:184] Created Layer out_deconv_final_up4 (61)
I0815 22:42:38.624624 14815 net.cpp:561] out_deconv_final_up4 <- out_deconv_final_up2
I0815 22:42:38.624629 14815 net.cpp:530] out_deconv_final_up4 -> out_deconv_final_up4
I0815 22:42:38.625008 14815 net.cpp:245] Setting up out_deconv_final_up4
I0815 22:42:38.625015 14815 net.cpp:252] TEST Top shape for layer 61 'out_deconv_final_up4' 2 8 320 320 (1638400)
I0815 22:42:38.625020 14815 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I0815 22:42:38.625023 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.625028 14815 net.cpp:184] Created Layer out_deconv_final_up8 (62)
I0815 22:42:38.625031 14815 net.cpp:561] out_deconv_final_up8 <- out_deconv_final_up4
I0815 22:42:38.625036 14815 net.cpp:530] out_deconv_final_up8 -> out_deconv_final_up8
I0815 22:42:38.625412 14815 net.cpp:245] Setting up out_deconv_final_up8
I0815 22:42:38.625419 14815 net.cpp:252] TEST Top shape for layer 62 'out_deconv_final_up8' 2 8 640 640 (6553600)
I0815 22:42:38.625424 14815 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8_out_deconv_final_up8_0_split' of type 'Split'
I0815 22:42:38.625427 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.625432 14815 net.cpp:184] Created Layer out_deconv_final_up8_out_deconv_final_up8_0_split (63)
I0815 22:42:38.625435 14815 net.cpp:561] out_deconv_final_up8_out_deconv_final_up8_0_split <- out_deconv_final_up8
I0815 22:42:38.625438 14815 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0815 22:42:38.625442 14815 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0815 22:42:38.625447 14815 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0815 22:42:38.625535 14815 net.cpp:245] Setting up out_deconv_final_up8_out_deconv_final_up8_0_split
I0815 22:42:38.625542 14815 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 8 640 640 (6553600)
I0815 22:42:38.625546 14815 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 8 640 640 (6553600)
I0815 22:42:38.625550 14815 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 8 640 640 (6553600)
I0815 22:42:38.625555 14815 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0815 22:42:38.625560 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.625566 14815 net.cpp:184] Created Layer loss (64)
I0815 22:42:38.625569 14815 net.cpp:561] loss <- out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0815 22:42:38.625573 14815 net.cpp:561] loss <- label_data_1_split_0
I0815 22:42:38.625578 14815 net.cpp:530] loss -> loss
I0815 22:42:38.626617 14815 net.cpp:245] Setting up loss
I0815 22:42:38.626627 14815 net.cpp:252] TEST Top shape for layer 64 'loss' (1)
I0815 22:42:38.626631 14815 net.cpp:256]     with loss weight 1
I0815 22:42:38.626636 14815 layer_factory.hpp:136] Creating layer 'accuracy/top1' of type 'Accuracy'
I0815 22:42:38.626639 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.626648 14815 net.cpp:184] Created Layer accuracy/top1 (65)
I0815 22:42:38.626652 14815 net.cpp:561] accuracy/top1 <- out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0815 22:42:38.626655 14815 net.cpp:561] accuracy/top1 <- label_data_1_split_1
I0815 22:42:38.626659 14815 net.cpp:530] accuracy/top1 -> accuracy/top1
I0815 22:42:38.626667 14815 net.cpp:245] Setting up accuracy/top1
I0815 22:42:38.626672 14815 net.cpp:252] TEST Top shape for layer 65 'accuracy/top1' (1)
I0815 22:42:38.626682 14815 layer_factory.hpp:136] Creating layer 'accuracy/top5' of type 'Accuracy'
I0815 22:42:38.626685 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.626691 14815 net.cpp:184] Created Layer accuracy/top5 (66)
I0815 22:42:38.626695 14815 net.cpp:561] accuracy/top5 <- out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0815 22:42:38.626700 14815 net.cpp:561] accuracy/top5 <- label_data_1_split_2
I0815 22:42:38.626704 14815 net.cpp:530] accuracy/top5 -> accuracy/top5
I0815 22:42:38.626710 14815 net.cpp:245] Setting up accuracy/top5
I0815 22:42:38.626715 14815 net.cpp:252] TEST Top shape for layer 66 'accuracy/top5' (1)
I0815 22:42:38.626719 14815 net.cpp:325] accuracy/top5 does not need backward computation.
I0815 22:42:38.626724 14815 net.cpp:325] accuracy/top1 does not need backward computation.
I0815 22:42:38.626727 14815 net.cpp:323] loss needs backward computation.
I0815 22:42:38.626732 14815 net.cpp:323] out_deconv_final_up8_out_deconv_final_up8_0_split needs backward computation.
I0815 22:42:38.626736 14815 net.cpp:323] out_deconv_final_up8 needs backward computation.
I0815 22:42:38.626740 14815 net.cpp:323] out_deconv_final_up4 needs backward computation.
I0815 22:42:38.626744 14815 net.cpp:323] out_deconv_final_up2 needs backward computation.
I0815 22:42:38.626747 14815 net.cpp:323] ctx_final/relu needs backward computation.
I0815 22:42:38.626750 14815 net.cpp:323] ctx_final needs backward computation.
I0815 22:42:38.626754 14815 net.cpp:323] ctx_conv4/relu needs backward computation.
I0815 22:42:38.626757 14815 net.cpp:323] ctx_conv4/bn needs backward computation.
I0815 22:42:38.626761 14815 net.cpp:323] ctx_conv4 needs backward computation.
I0815 22:42:38.626765 14815 net.cpp:323] ctx_conv3/relu needs backward computation.
I0815 22:42:38.626768 14815 net.cpp:323] ctx_conv3/bn needs backward computation.
I0815 22:42:38.626771 14815 net.cpp:323] ctx_conv3 needs backward computation.
I0815 22:42:38.626775 14815 net.cpp:323] ctx_conv2/relu needs backward computation.
I0815 22:42:38.626778 14815 net.cpp:323] ctx_conv2/bn needs backward computation.
I0815 22:42:38.626781 14815 net.cpp:323] ctx_conv2 needs backward computation.
I0815 22:42:38.626785 14815 net.cpp:323] ctx_conv1/relu needs backward computation.
I0815 22:42:38.626788 14815 net.cpp:323] ctx_conv1/bn needs backward computation.
I0815 22:42:38.626792 14815 net.cpp:323] ctx_conv1 needs backward computation.
I0815 22:42:38.626796 14815 net.cpp:323] out3_out5_combined needs backward computation.
I0815 22:42:38.626799 14815 net.cpp:323] out3a/relu needs backward computation.
I0815 22:42:38.626803 14815 net.cpp:323] out3a/bn needs backward computation.
I0815 22:42:38.626807 14815 net.cpp:323] out3a needs backward computation.
I0815 22:42:38.626811 14815 net.cpp:323] out5a_up2 needs backward computation.
I0815 22:42:38.626814 14815 net.cpp:323] out5a/relu needs backward computation.
I0815 22:42:38.626817 14815 net.cpp:323] out5a/bn needs backward computation.
I0815 22:42:38.626821 14815 net.cpp:323] out5a needs backward computation.
I0815 22:42:38.626826 14815 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0815 22:42:38.626829 14815 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0815 22:42:38.626833 14815 net.cpp:323] res5a_branch2b needs backward computation.
I0815 22:42:38.626837 14815 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0815 22:42:38.626840 14815 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0815 22:42:38.626843 14815 net.cpp:323] res5a_branch2a needs backward computation.
I0815 22:42:38.626847 14815 net.cpp:323] pool4 needs backward computation.
I0815 22:42:38.626850 14815 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0815 22:42:38.626854 14815 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0815 22:42:38.626857 14815 net.cpp:323] res4a_branch2b needs backward computation.
I0815 22:42:38.626862 14815 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0815 22:42:38.626869 14815 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0815 22:42:38.626873 14815 net.cpp:323] res4a_branch2a needs backward computation.
I0815 22:42:38.626878 14815 net.cpp:323] pool3 needs backward computation.
I0815 22:42:38.626881 14815 net.cpp:323] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I0815 22:42:38.626885 14815 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0815 22:42:38.626889 14815 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0815 22:42:38.626893 14815 net.cpp:323] res3a_branch2b needs backward computation.
I0815 22:42:38.626896 14815 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0815 22:42:38.626900 14815 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0815 22:42:38.626904 14815 net.cpp:323] res3a_branch2a needs backward computation.
I0815 22:42:38.626907 14815 net.cpp:323] pool2 needs backward computation.
I0815 22:42:38.626911 14815 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0815 22:42:38.626915 14815 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0815 22:42:38.626919 14815 net.cpp:323] res2a_branch2b needs backward computation.
I0815 22:42:38.626922 14815 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0815 22:42:38.626925 14815 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0815 22:42:38.626929 14815 net.cpp:323] res2a_branch2a needs backward computation.
I0815 22:42:38.626932 14815 net.cpp:323] pool1 needs backward computation.
I0815 22:42:38.626936 14815 net.cpp:323] conv1b/relu needs backward computation.
I0815 22:42:38.626940 14815 net.cpp:323] conv1b/bn needs backward computation.
I0815 22:42:38.626945 14815 net.cpp:323] conv1b needs backward computation.
I0815 22:42:38.626948 14815 net.cpp:323] conv1a/relu needs backward computation.
I0815 22:42:38.626952 14815 net.cpp:323] conv1a/bn needs backward computation.
I0815 22:42:38.626956 14815 net.cpp:323] conv1a needs backward computation.
I0815 22:42:38.626960 14815 net.cpp:325] data/bias does not need backward computation.
I0815 22:42:38.626965 14815 net.cpp:325] label_data_1_split does not need backward computation.
I0815 22:42:38.626968 14815 net.cpp:325] data does not need backward computation.
I0815 22:42:38.626971 14815 net.cpp:367] This network produces output accuracy/top1
I0815 22:42:38.626976 14815 net.cpp:367] This network produces output accuracy/top5
I0815 22:42:38.626979 14815 net.cpp:367] This network produces output loss
I0815 22:42:38.627041 14815 net.cpp:389] Top memory (TEST) required for data: 318668800 diff: 8
I0815 22:42:38.627046 14815 net.cpp:392] Bottom memory (TEST) required for data: 318668800 diff: 318668800
I0815 22:42:38.627049 14815 net.cpp:395] Shared (in-place) memory (TEST) by data: 210124800 diff: 210124800
I0815 22:42:38.627053 14815 net.cpp:398] Parameters memory (TEST) required for data: 2692608 diff: 2692608
I0815 22:42:38.627058 14815 net.cpp:401] Parameters shared memory (TEST) by data: 0 diff: 0
I0815 22:42:38.627060 14815 net.cpp:407] Network initialization done.
I0815 22:42:38.627182 14815 solver.cpp:56] Solver scaffolding done.
I0815 22:42:38.639786 14815 caffe.cpp:137] Finetuning from training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/l1reg/cityscapes5_jsegnet21v2_iter_32000.caffemodel
I0815 22:42:38.646469 14815 net.cpp:1095] Copying source layer data Type:ImageLabelData #blobs=0
I0815 22:42:38.646492 14815 net.cpp:1095] Copying source layer data/bias Type:Bias #blobs=1
I0815 22:42:38.646533 14815 net.cpp:1095] Copying source layer conv1a Type:Convolution #blobs=2
I0815 22:42:38.646554 14815 net.cpp:1095] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0815 22:42:38.647317 14815 net.cpp:1095] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0815 22:42:38.647328 14815 net.cpp:1095] Copying source layer conv1b Type:Convolution #blobs=2
I0815 22:42:38.647346 14815 net.cpp:1095] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0815 22:42:38.647919 14815 net.cpp:1095] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0815 22:42:38.647940 14815 net.cpp:1095] Copying source layer pool1 Type:Pooling #blobs=0
I0815 22:42:38.647945 14815 net.cpp:1095] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0815 22:42:38.647967 14815 net.cpp:1095] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0815 22:42:38.648558 14815 net.cpp:1095] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0815 22:42:38.648568 14815 net.cpp:1095] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0815 22:42:38.648587 14815 net.cpp:1095] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0815 22:42:38.649158 14815 net.cpp:1095] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0815 22:42:38.649168 14815 net.cpp:1095] Copying source layer pool2 Type:Pooling #blobs=0
I0815 22:42:38.649173 14815 net.cpp:1095] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0815 22:42:38.649219 14815 net.cpp:1095] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0815 22:42:38.649749 14815 net.cpp:1095] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0815 22:42:38.649757 14815 net.cpp:1095] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0815 22:42:38.649788 14815 net.cpp:1095] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0815 22:42:38.650321 14815 net.cpp:1095] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0815 22:42:38.650331 14815 net.cpp:1095] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I0815 22:42:38.650336 14815 net.cpp:1095] Copying source layer pool3 Type:Pooling #blobs=0
I0815 22:42:38.650341 14815 net.cpp:1095] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0815 22:42:38.650607 14815 net.cpp:1095] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0815 22:42:38.651162 14815 net.cpp:1095] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0815 22:42:38.651172 14815 net.cpp:1095] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0815 22:42:38.651253 14815 net.cpp:1095] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0815 22:42:38.651777 14815 net.cpp:1095] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0815 22:42:38.651787 14815 net.cpp:1095] Copying source layer pool4 Type:Pooling #blobs=0
I0815 22:42:38.651793 14815 net.cpp:1095] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0815 22:42:38.652236 14815 net.cpp:1095] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0815 22:42:38.652770 14815 net.cpp:1095] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0815 22:42:38.652781 14815 net.cpp:1095] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0815 22:42:38.653012 14815 net.cpp:1095] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0815 22:42:38.653545 14815 net.cpp:1095] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0815 22:42:38.653555 14815 net.cpp:1095] Copying source layer out5a Type:Convolution #blobs=2
I0815 22:42:38.653630 14815 net.cpp:1095] Copying source layer out5a/bn Type:BatchNorm #blobs=5
I0815 22:42:38.653870 14815 net.cpp:1095] Copying source layer out5a/relu Type:ReLU #blobs=0
I0815 22:42:38.653879 14815 net.cpp:1095] Copying source layer out5a_up2 Type:Deconvolution #blobs=1
I0815 22:42:38.653890 14815 net.cpp:1095] Copying source layer out3a Type:Convolution #blobs=2
I0815 22:42:38.653920 14815 net.cpp:1095] Copying source layer out3a/bn Type:BatchNorm #blobs=5
I0815 22:42:38.654145 14815 net.cpp:1095] Copying source layer out3a/relu Type:ReLU #blobs=0
I0815 22:42:38.654155 14815 net.cpp:1095] Copying source layer out3_out5_combined Type:Eltwise #blobs=0
I0815 22:42:38.654162 14815 net.cpp:1095] Copying source layer ctx_conv1 Type:Convolution #blobs=2
I0815 22:42:38.654191 14815 net.cpp:1095] Copying source layer ctx_conv1/bn Type:BatchNorm #blobs=5
I0815 22:42:38.654417 14815 net.cpp:1095] Copying source layer ctx_conv1/relu Type:ReLU #blobs=0
I0815 22:42:38.654428 14815 net.cpp:1095] Copying source layer ctx_conv2 Type:Convolution #blobs=2
I0815 22:42:38.654469 14815 net.cpp:1095] Copying source layer ctx_conv2/bn Type:BatchNorm #blobs=5
I0815 22:42:38.654692 14815 net.cpp:1095] Copying source layer ctx_conv2/relu Type:ReLU #blobs=0
I0815 22:42:38.654701 14815 net.cpp:1095] Copying source layer ctx_conv3 Type:Convolution #blobs=2
I0815 22:42:38.654732 14815 net.cpp:1095] Copying source layer ctx_conv3/bn Type:BatchNorm #blobs=5
I0815 22:42:38.654953 14815 net.cpp:1095] Copying source layer ctx_conv3/relu Type:ReLU #blobs=0
I0815 22:42:38.654963 14815 net.cpp:1095] Copying source layer ctx_conv4 Type:Convolution #blobs=2
I0815 22:42:38.654991 14815 net.cpp:1095] Copying source layer ctx_conv4/bn Type:BatchNorm #blobs=5
I0815 22:42:38.655210 14815 net.cpp:1095] Copying source layer ctx_conv4/relu Type:ReLU #blobs=0
I0815 22:42:38.655218 14815 net.cpp:1095] Copying source layer ctx_final Type:Convolution #blobs=2
I0815 22:42:38.655234 14815 net.cpp:1095] Copying source layer ctx_final/relu Type:ReLU #blobs=0
I0815 22:42:38.655239 14815 net.cpp:1095] Copying source layer out_deconv_final_up2 Type:Deconvolution #blobs=1
I0815 22:42:38.655248 14815 net.cpp:1095] Copying source layer out_deconv_final_up4 Type:Deconvolution #blobs=1
I0815 22:42:38.655257 14815 net.cpp:1095] Copying source layer out_deconv_final_up8 Type:Deconvolution #blobs=1
I0815 22:42:38.655267 14815 net.cpp:1095] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0815 22:42:38.660143 14815 net.cpp:1095] Copying source layer data Type:ImageLabelData #blobs=0
I0815 22:42:38.660166 14815 net.cpp:1095] Copying source layer data/bias Type:Bias #blobs=1
I0815 22:42:38.660197 14815 net.cpp:1095] Copying source layer conv1a Type:Convolution #blobs=2
I0815 22:42:38.660208 14815 net.cpp:1095] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0815 22:42:38.660787 14815 net.cpp:1095] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0815 22:42:38.660797 14815 net.cpp:1095] Copying source layer conv1b Type:Convolution #blobs=2
I0815 22:42:38.660810 14815 net.cpp:1095] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0815 22:42:38.661371 14815 net.cpp:1095] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0815 22:42:38.661382 14815 net.cpp:1095] Copying source layer pool1 Type:Pooling #blobs=0
I0815 22:42:38.661388 14815 net.cpp:1095] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0815 22:42:38.661409 14815 net.cpp:1095] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0815 22:42:38.661998 14815 net.cpp:1095] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0815 22:42:38.662008 14815 net.cpp:1095] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0815 22:42:38.662027 14815 net.cpp:1095] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0815 22:42:38.662607 14815 net.cpp:1095] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0815 22:42:38.662617 14815 net.cpp:1095] Copying source layer pool2 Type:Pooling #blobs=0
I0815 22:42:38.662623 14815 net.cpp:1095] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0815 22:42:38.662669 14815 net.cpp:1095] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0815 22:42:38.663223 14815 net.cpp:1095] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0815 22:42:38.663233 14815 net.cpp:1095] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0815 22:42:38.663262 14815 net.cpp:1095] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0815 22:42:38.663801 14815 net.cpp:1095] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0815 22:42:38.663812 14815 net.cpp:1095] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I0815 22:42:38.663817 14815 net.cpp:1095] Copying source layer pool3 Type:Pooling #blobs=0
I0815 22:42:38.663821 14815 net.cpp:1095] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0815 22:42:38.663945 14815 net.cpp:1095] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0815 22:42:38.664515 14815 net.cpp:1095] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0815 22:42:38.664525 14815 net.cpp:1095] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0815 22:42:38.664593 14815 net.cpp:1095] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0815 22:42:38.665138 14815 net.cpp:1095] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0815 22:42:38.665148 14815 net.cpp:1095] Copying source layer pool4 Type:Pooling #blobs=0
I0815 22:42:38.665153 14815 net.cpp:1095] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0815 22:42:38.665550 14815 net.cpp:1095] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0815 22:42:38.666081 14815 net.cpp:1095] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0815 22:42:38.666091 14815 net.cpp:1095] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0815 22:42:38.666307 14815 net.cpp:1095] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0815 22:42:38.666838 14815 net.cpp:1095] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0815 22:42:38.666848 14815 net.cpp:1095] Copying source layer out5a Type:Convolution #blobs=2
I0815 22:42:38.666916 14815 net.cpp:1095] Copying source layer out5a/bn Type:BatchNorm #blobs=5
I0815 22:42:38.667157 14815 net.cpp:1095] Copying source layer out5a/relu Type:ReLU #blobs=0
I0815 22:42:38.667166 14815 net.cpp:1095] Copying source layer out5a_up2 Type:Deconvolution #blobs=1
I0815 22:42:38.667176 14815 net.cpp:1095] Copying source layer out3a Type:Convolution #blobs=2
I0815 22:42:38.667202 14815 net.cpp:1095] Copying source layer out3a/bn Type:BatchNorm #blobs=5
I0815 22:42:38.667429 14815 net.cpp:1095] Copying source layer out3a/relu Type:ReLU #blobs=0
I0815 22:42:38.667438 14815 net.cpp:1095] Copying source layer out3_out5_combined Type:Eltwise #blobs=0
I0815 22:42:38.667443 14815 net.cpp:1095] Copying source layer ctx_conv1 Type:Convolution #blobs=2
I0815 22:42:38.667469 14815 net.cpp:1095] Copying source layer ctx_conv1/bn Type:BatchNorm #blobs=5
I0815 22:42:38.667695 14815 net.cpp:1095] Copying source layer ctx_conv1/relu Type:ReLU #blobs=0
I0815 22:42:38.667704 14815 net.cpp:1095] Copying source layer ctx_conv2 Type:Convolution #blobs=2
I0815 22:42:38.667732 14815 net.cpp:1095] Copying source layer ctx_conv2/bn Type:BatchNorm #blobs=5
I0815 22:42:38.667955 14815 net.cpp:1095] Copying source layer ctx_conv2/relu Type:ReLU #blobs=0
I0815 22:42:38.667964 14815 net.cpp:1095] Copying source layer ctx_conv3 Type:Convolution #blobs=2
I0815 22:42:38.667994 14815 net.cpp:1095] Copying source layer ctx_conv3/bn Type:BatchNorm #blobs=5
I0815 22:42:38.668223 14815 net.cpp:1095] Copying source layer ctx_conv3/relu Type:ReLU #blobs=0
I0815 22:42:38.668232 14815 net.cpp:1095] Copying source layer ctx_conv4 Type:Convolution #blobs=2
I0815 22:42:38.668261 14815 net.cpp:1095] Copying source layer ctx_conv4/bn Type:BatchNorm #blobs=5
I0815 22:42:38.668486 14815 net.cpp:1095] Copying source layer ctx_conv4/relu Type:ReLU #blobs=0
I0815 22:42:38.668496 14815 net.cpp:1095] Copying source layer ctx_final Type:Convolution #blobs=2
I0815 22:42:38.668512 14815 net.cpp:1095] Copying source layer ctx_final/relu Type:ReLU #blobs=0
I0815 22:42:38.668517 14815 net.cpp:1095] Copying source layer out_deconv_final_up2 Type:Deconvolution #blobs=1
I0815 22:42:38.668525 14815 net.cpp:1095] Copying source layer out_deconv_final_up4 Type:Deconvolution #blobs=1
I0815 22:42:38.668535 14815 net.cpp:1095] Copying source layer out_deconv_final_up8 Type:Deconvolution #blobs=1
I0815 22:42:38.668545 14815 net.cpp:1095] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0815 22:42:38.668668 14815 parallel.cpp:106] [0 - 0] P2pSync adding callback
I0815 22:42:38.668678 14815 parallel.cpp:106] [1 - 1] P2pSync adding callback
I0815 22:42:38.668682 14815 parallel.cpp:106] [2 - 2] P2pSync adding callback
I0815 22:42:38.668686 14815 parallel.cpp:59] Starting Optimization
I0815 22:42:38.668690 14815 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 22:42:38.668727 14815 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 22:42:38.668773 14815 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 22:42:38.671696 14907 device_alternate.hpp:116] NVML initialized on thread 136480662079232
I0815 22:42:38.703514 14907 common.cpp:583] NVML succeeded to set CPU affinity on device 0
I0815 22:42:38.703567 14908 device_alternate.hpp:116] NVML initialized on thread 136480653686528
I0815 22:42:38.704160 14908 common.cpp:583] NVML succeeded to set CPU affinity on device 1
I0815 22:42:38.704179 14909 device_alternate.hpp:116] NVML initialized on thread 136480645293824
I0815 22:42:38.704754 14909 common.cpp:583] NVML succeeded to set CPU affinity on device 2
I0815 22:42:38.709017 14908 solver.cpp:42] Solver data type: FLOAT
W0815 22:42:38.709815 14908 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0815 22:42:38.709969 14908 net.cpp:104] Using FLOAT as default forward math type
I0815 22:42:38.709976 14908 net.cpp:110] Using FLOAT as default backward math type
I0815 22:42:38.710016 14908 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0815 22:42:38.710026 14908 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 22:42:38.713348 14909 solver.cpp:42] Solver data type: FLOAT
W0815 22:42:38.713912 14909 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0815 22:42:38.714030 14909 net.cpp:104] Using FLOAT as default forward math type
I0815 22:42:38.714035 14909 net.cpp:110] Using FLOAT as default backward math type
I0815 22:42:38.714038 14915 db_lmdb.cpp:24] Opened lmdb data/train-image-lmdb
I0815 22:42:38.714128 14909 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0815 22:42:38.714134 14909 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 22:42:38.715008 14916 db_lmdb.cpp:24] Opened lmdb data/train-image-lmdb
I0815 22:42:38.717324 14908 data_layer.cpp:185] [1] ReshapePrefetch 6, 3, 640, 640
I0815 22:42:38.717434 14908 data_layer.cpp:209] [1] Output data size: 6, 3, 640, 640
I0815 22:42:38.717443 14908 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 22:42:38.717511 14908 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0815 22:42:38.717523 14908 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 22:42:38.718389 14917 data_layer.cpp:97] [1] Parser threads: 1
I0815 22:42:38.718399 14917 data_layer.cpp:99] [1] Transformer threads: 1
I0815 22:42:38.724099 14909 data_layer.cpp:185] [2] ReshapePrefetch 6, 3, 640, 640
I0815 22:42:38.724102 14918 db_lmdb.cpp:24] Opened lmdb data/train-label-lmdb
I0815 22:42:38.725055 14909 data_layer.cpp:209] [2] Output data size: 6, 3, 640, 640
I0815 22:42:38.725114 14909 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 22:42:38.725373 14909 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0815 22:42:38.725450 14909 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 22:42:38.726379 14908 data_layer.cpp:185] [1] ReshapePrefetch 6, 1, 640, 640
I0815 22:42:38.726521 14908 data_layer.cpp:209] [1] Output data size: 6, 1, 640, 640
I0815 22:42:38.726531 14908 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 22:42:38.726971 14919 data_layer.cpp:97] [2] Parser threads: 1
I0815 22:42:38.727007 14919 data_layer.cpp:99] [2] Transformer threads: 1
I0815 22:42:38.734621 14920 db_lmdb.cpp:24] Opened lmdb data/train-label-lmdb
I0815 22:42:38.737197 14909 data_layer.cpp:185] [2] ReshapePrefetch 6, 1, 640, 640
I0815 22:42:38.737510 14909 data_layer.cpp:209] [2] Output data size: 6, 1, 640, 640
I0815 22:42:38.737546 14909 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 22:42:38.740530 14921 data_layer.cpp:97] [1] Parser threads: 1
I0815 22:42:38.740593 14921 data_layer.cpp:99] [1] Transformer threads: 1
I0815 22:42:38.742661 14917 blocking_queue.cpp:40] Waiting for datum
I0815 22:42:38.743232 14922 data_layer.cpp:97] [2] Parser threads: 1
I0815 22:42:38.743288 14922 data_layer.cpp:99] [2] Transformer threads: 1
I0815 22:42:39.279404 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 7.99G, req 0G)
I0815 22:42:39.293010 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 7.99G, req 0G)
I0815 22:42:39.328670 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.83G, req 0G)
I0815 22:42:39.342290 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.83G, req 0G)
I0815 22:42:39.371150 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.7G, req 0G)
I0815 22:42:39.384855 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.7G, req 0G)
I0815 22:42:39.394398 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.62G, req 0G)
I0815 22:42:39.408059 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.62G, req 0G)
I0815 22:42:39.417698 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 7.56G, req 0G)
I0815 22:42:39.430300 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.52G, req 0G)
I0815 22:42:39.432096 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 7.56G, req 0G)
I0815 22:42:39.445282 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.52G, req 0G)
I0815 22:42:39.459321 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.48G, req 0G)
I0815 22:42:39.470358 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.46G, req 0G)
I0815 22:42:39.473407 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.48G, req 0G)
I0815 22:42:39.485296 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.46G, req 0G)
I0815 22:42:39.545521 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 7.39G, req 0G)
I0815 22:42:39.559749 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 7.39G, req 0G)
I0815 22:42:39.564873 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 7.34G, req 0G)
I0815 22:42:39.577150 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 7.34G, req 0G)
I0815 22:42:39.587128 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 4 3  (limit 7.32G, req 0G)
I0815 22:42:39.590557 14908 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/sparse/test.prototxt
W0815 22:42:39.590631 14908 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0815 22:42:39.590767 14908 net.cpp:104] Using FLOAT as default forward math type
I0815 22:42:39.590772 14908 net.cpp:110] Using FLOAT as default backward math type
I0815 22:42:39.590801 14908 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0815 22:42:39.590808 14908 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 22:42:39.591625 14938 db_lmdb.cpp:24] Opened lmdb data/val-image-lmdb
I0815 22:42:39.593129 14908 data_layer.cpp:185] (1) ReshapePrefetch 2, 3, 640, 640
I0815 22:42:39.593200 14908 data_layer.cpp:209] (1) Output data size: 2, 3, 640, 640
I0815 22:42:39.593206 14908 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 22:42:39.593255 14908 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0815 22:42:39.593264 14908 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 22:42:39.594053 14939 data_layer.cpp:97] (1) Parser threads: 1
I0815 22:42:39.594069 14939 data_layer.cpp:99] (1) Transformer threads: 1
I0815 22:42:39.596352 14940 db_lmdb.cpp:24] Opened lmdb data/val-label-lmdb
I0815 22:42:39.597757 14908 data_layer.cpp:185] (1) ReshapePrefetch 2, 1, 640, 640
I0815 22:42:39.597883 14908 data_layer.cpp:209] (1) Output data size: 2, 1, 640, 640
I0815 22:42:39.597890 14908 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 22:42:39.598996 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 7.32G, req 0G)
I0815 22:42:39.599375 14941 data_layer.cpp:97] (1) Parser threads: 1
I0815 22:42:39.599385 14941 data_layer.cpp:99] (1) Transformer threads: 1
I0815 22:42:39.608963 14909 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/sparse/test.prototxt
W0815 22:42:39.609086 14909 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0815 22:42:39.609267 14909 net.cpp:104] Using FLOAT as default forward math type
I0815 22:42:39.609274 14909 net.cpp:110] Using FLOAT as default backward math type
I0815 22:42:39.609303 14909 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0815 22:42:39.609325 14909 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 22:42:39.610144 14942 db_lmdb.cpp:24] Opened lmdb data/val-image-lmdb
I0815 22:42:39.612097 14909 data_layer.cpp:185] (2) ReshapePrefetch 2, 3, 640, 640
I0815 22:42:39.612236 14909 data_layer.cpp:209] (2) Output data size: 2, 3, 640, 640
I0815 22:42:39.612246 14909 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 22:42:39.612285 14909 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0815 22:42:39.612293 14909 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 22:42:39.612387 14908 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.18G, req 0G)
I0815 22:42:39.613189 14943 data_layer.cpp:97] (2) Parser threads: 1
I0815 22:42:39.613203 14943 data_layer.cpp:99] (2) Transformer threads: 1
I0815 22:42:39.615862 14944 db_lmdb.cpp:24] Opened lmdb data/val-label-lmdb
I0815 22:42:39.617158 14909 data_layer.cpp:185] (2) ReshapePrefetch 2, 1, 640, 640
I0815 22:42:39.617266 14909 data_layer.cpp:209] (2) Output data size: 2, 1, 640, 640
I0815 22:42:39.617274 14909 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 22:42:39.618893 14945 data_layer.cpp:97] (2) Parser threads: 1
I0815 22:42:39.618906 14945 data_layer.cpp:99] (2) Transformer threads: 1
I0815 22:42:39.628881 14909 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.18G, req 0G)
I0815 22:42:39.637356 14908 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.15G, req 0G)
I0815 22:42:39.646371 14909 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.15G, req 0G)
I0815 22:42:39.647997 14908 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.13G, req 0G)
I0815 22:42:39.656929 14909 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.13G, req 0G)
I0815 22:42:39.657852 14908 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.12G, req 0G)
I0815 22:42:39.666260 14909 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.12G, req 0G)
I0815 22:42:39.666448 14908 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.1G, req 0G)
I0815 22:42:39.674228 14908 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.09G, req 0G)
I0815 22:42:39.676362 14909 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.1G, req 0G)
I0815 22:42:39.683136 14909 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.09G, req 0G)
I0815 22:42:39.687414 14908 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.09G, req 0G)
I0815 22:42:39.695914 14908 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.08G, req 0G)
I0815 22:42:39.696897 14909 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.09G, req 0G)
I0815 22:42:39.708438 14909 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.08G, req 0G)
I0815 22:42:39.746639 14908 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 7.07G, req 0G)
I0815 22:42:39.753126 14908 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 7.06G, req 0G)
I0815 22:42:39.766201 14909 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 7.07G, req 0G)
I0815 22:42:39.767442 14908 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 7.05G, req 0G)
I0815 22:42:39.771782 14908 solver.cpp:56] Solver scaffolding done.
I0815 22:42:39.774127 14909 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 7.06G, req 0G)
I0815 22:42:39.791589 14909 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 7.05G, req 0G)
I0815 22:42:39.794790 14909 solver.cpp:56] Solver scaffolding done.
I0815 22:42:39.855839 14908 parallel.cpp:161] [1 - 1] P2pSync adding callback
I0815 22:42:39.855839 14909 parallel.cpp:161] [2 - 2] P2pSync adding callback
I0815 22:42:39.855839 14907 parallel.cpp:161] [0 - 0] P2pSync adding callback
I0815 22:42:40.044247 14907 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 22:42:40.063479 14908 solver.cpp:438] Solving jsegnet21v2_train
I0815 22:42:40.063495 14908 solver.cpp:439] Learning Rate Policy: multistep
I0815 22:42:40.063848 14907 solver.cpp:438] Solving jsegnet21v2_train
I0815 22:42:40.063856 14907 solver.cpp:439] Learning Rate Policy: multistep
I0815 22:42:40.064180 14909 solver.cpp:438] Solving jsegnet21v2_train
I0815 22:42:40.064188 14909 solver.cpp:439] Learning Rate Policy: multistep
I0815 22:42:40.079131 14908 solver.cpp:227] Starting Optimization on GPU 1
I0815 22:42:40.079133 14909 solver.cpp:227] Starting Optimization on GPU 2
I0815 22:42:40.079133 14907 solver.cpp:227] Starting Optimization on GPU 0
I0815 22:42:40.079304 14907 solver.cpp:509] Iteration 0, Testing net (#0)
I0815 22:42:40.079308 14960 device_alternate.hpp:116] NVML initialized on thread 128480493819648
I0815 22:42:40.079347 14960 common.cpp:583] NVML succeeded to set CPU affinity on device 1
I0815 22:42:40.079357 14961 device_alternate.hpp:116] NVML initialized on thread 128480485426944
I0815 22:42:40.079372 14961 common.cpp:583] NVML succeeded to set CPU affinity on device 2
I0815 22:42:40.079382 14962 device_alternate.hpp:116] NVML initialized on thread 128480477034240
I0815 22:42:40.079396 14962 common.cpp:583] NVML succeeded to set CPU affinity on device 0
I0815 22:42:40.090656 14909 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 6.95G, req 0G)
I0815 22:42:40.098800 14908 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 6.95G, req 0G)
I0815 22:42:40.113490 14909 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 6.9G, req 0G)
I0815 22:42:40.117839 14908 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 6.9G, req 0G)
I0815 22:42:40.128267 14909 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 6.83G, req 0G)
I0815 22:42:40.132247 14908 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 6.83G, req 0G)
I0815 22:42:40.139905 14909 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 6.8G, req 0G)
I0815 22:42:40.140966 14907 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.01G/1 1  (limit 6.88G, req 0G)
I0815 22:42:40.143402 14908 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 1  (limit 6.8G, req 0G)
I0815 22:42:40.148938 14909 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 6.77G, req 0G)
I0815 22:42:40.152717 14908 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 6.77G, req 0G)
I0815 22:42:40.154927 14909 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 6.75G, req 0G)
I0815 22:42:40.158126 14908 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 6.75G, req 0G)
I0815 22:42:40.162106 14907 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 6.82G, req 0G)
I0815 22:42:40.165175 14909 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.74G, req 0G)
I0815 22:42:40.166488 14908 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.74G, req 0G)
I0815 22:42:40.170428 14909 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.73G, req 0G)
I0815 22:42:40.170855 14908 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.73G, req 0G)
I0815 22:42:40.176751 14907 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 6.75G, req 0G)
I0815 22:42:40.185240 14907 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 6.73G, req 0G)
I0815 22:42:40.192723 14907 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 6.69G, req 0G)
I0815 22:42:40.198454 14907 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 6.67G, req 0G)
I0815 22:42:40.198678 14908 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.58G, req 0G)
I0815 22:42:40.198902 14909 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.58G, req 0G)
I0815 22:42:40.205545 14908 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.57G, req 0G)
I0815 22:42:40.205971 14909 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.57G, req 0G)
I0815 22:42:40.206626 14907 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.66G, req 0G)
I0815 22:42:40.213539 14907 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.65G, req 0G)
I0815 22:42:40.234558 14908 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.47G, req 0G)
I0815 22:42:40.235227 14909 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.47G, req 0G)
I0815 22:42:40.246263 14907 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.5G, req 0G)
I0815 22:42:40.252600 14907 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.49G, req 0G)
I0815 22:42:40.276795 14907 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.39G, req 0G)
I0815 22:42:40.379930 14907 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.896719
I0815 22:42:40.379973 14907 solver.cpp:594]     Test net output #1: accuracy/top5 = 1
I0815 22:42:40.379984 14907 solver.cpp:594]     Test net output #2: loss = 0.299862 (* 1 = 0.299862 loss)
I0815 22:42:40.379992 14907 solver.cpp:254] [MultiGPU] Initial Test completed
I0815 22:42:40.478572 14907 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 6.19G, req 0G)
I0815 22:42:40.485781 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 6.27G, req 0G)
I0815 22:42:40.486399 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 6.27G, req 0G)
I0815 22:42:40.526414 14907 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 6.03G, req 0G)
I0815 22:42:40.540319 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 6.11G, req 0G)
I0815 22:42:40.540871 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 6.11G, req 0G)
I0815 22:42:40.569708 14907 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 5.85G, req 0G)
I0815 22:42:40.593789 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 5.93G, req 0G)
I0815 22:42:40.595212 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 5.93G, req 0G)
I0815 22:42:40.595358 14907 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 5.77G, req 0G)
I0815 22:42:40.621081 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 5.85G, req 0G)
I0815 22:42:40.621309 14907 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 5.68G, req 0G)
I0815 22:42:40.621474 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 5.85G, req 0G)
I0815 22:42:40.634471 14907 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 5.64G, req 0G)
I0815 22:42:40.646179 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 5.76G, req 0G)
I0815 22:42:40.646410 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 5.76G, req 0G)
I0815 22:42:40.654546 14907 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 5.61G, req 0G)
I0815 22:42:40.659328 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 5.72G, req 0G)
I0815 22:42:40.659647 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 5.72G, req 0G)
I0815 22:42:40.661916 14907 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 5.59G, req 0G)
I0815 22:42:40.682024 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 5.69G, req 0G)
I0815 22:42:40.683169 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 5.69G, req 0G)
I0815 22:42:40.694177 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 5.67G, req 0G)
I0815 22:42:40.696897 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 5.67G, req 0G)
I0815 22:42:40.727841 14907 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 5.32G, req 0G)
I0815 22:42:40.745527 14907 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 5.3G, req 0G)
I0815 22:42:40.774261 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 5.4G, req 0G)
I0815 22:42:40.776185 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 5.4G, req 0G)
I0815 22:42:40.780616 14907 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 5.14G, req 0G)
I0815 22:42:40.788974 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 5.38G, req 0G)
I0815 22:42:40.790817 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 5.38G, req 0G)
I0815 22:42:40.824991 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 5.23G, req 0G)
I0815 22:42:40.825572 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 5.23G, req 0G)
I0815 22:42:41.056231 14907 solver.cpp:317] Iteration 0 (0.676165 s), loss = 0.0815038
I0815 22:42:41.056294 14907 solver.cpp:334]     Train net output #0: loss = 0.0815038 (* 1 = 0.0815038 loss)
I0815 22:42:41.056313 14907 sgd_solver.cpp:136] Iteration 0, lr = 1e-05, m = 0.9
I0815 22:42:41.265873 14907 solver.cpp:317] Iteration 1 (0.209639 s), loss = 0.106995
I0815 22:42:41.265903 14907 solver.cpp:334]     Train net output #0: loss = 0.106995 (* 1 = 0.106995 loss)
I0815 22:42:41.369432 14907 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1a' with space 1.29G/1 1 0 0  (limit 2.98G, req 0G)
I0815 22:42:41.370842 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1a' with space 1.29G/1 1 0 3  (limit 3.06G, req 0G)
I0815 22:42:41.379101 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1a' with space 1.29G/1 1 0 3  (limit 3.06G, req 0G)
I0815 22:42:41.429605 14907 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1b' with space 2.57G/2 6 4 3  (limit 1.69G, req 0G)
I0815 22:42:41.438313 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1b' with space 2.57G/2 6 4 3  (limit 1.78G, req 0G)
I0815 22:42:41.453783 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1b' with space 2.57G/2 6 4 3  (limit 1.78G, req 0G)
I0815 22:42:41.556071 14907 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 2.57G/1 6 4 3  (limit 1.69G, req 0G)
I0815 22:42:41.571681 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 2.57G/1 6 4 3  (limit 1.78G, req 0G)
I0815 22:42:41.587641 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 2.57G/1 6 4 3  (limit 1.78G, req 0G)
I0815 22:42:41.596122 14907 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 2.57G/2 6 4 3  (limit 1.69G, req 0G)
I0815 22:42:41.614511 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 2.57G/2 6 4 3  (limit 1.78G, req 0G)
I0815 22:42:41.630661 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 2.57G/2 6 4 3  (limit 1.78G, req 0G)
I0815 22:42:41.689532 14907 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 2.57G/1 6 4 5  (limit 1.69G, req 0.07G)
I0815 22:42:41.712231 14907 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 2.57G/2 6 4 3  (limit 1.69G, req 0.07G)
I0815 22:42:41.713217 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 2.57G/1 6 4 5  (limit 1.78G, req 0.07G)
I0815 22:42:41.730388 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 2.57G/1 6 4 5  (limit 1.78G, req 0.07G)
I0815 22:42:41.736629 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 2.57G/2 6 4 3  (limit 1.78G, req 0.07G)
I0815 22:42:41.754385 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 2.57G/2 6 4 3  (limit 1.78G, req 0.07G)
I0815 22:42:41.775269 14907 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 2.57G/1 6 4 5  (limit 1.69G, req 0.07G)
I0815 22:42:41.788725 14907 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 2.57G/2 6 4 3  (limit 1.69G, req 0.07G)
I0815 22:42:41.806126 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 2.57G/1 6 4 5  (limit 1.78G, req 0.07G)
I0815 22:42:41.820251 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 2.57G/2 6 4 3  (limit 1.78G, req 0.07G)
I0815 22:42:41.823521 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 2.57G/1 6 4 5  (limit 1.78G, req 0.07G)
I0815 22:42:41.836963 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 2.57G/2 6 4 3  (limit 1.78G, req 0.07G)
I0815 22:42:41.841683 14907 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'out3a' with space 2.57G/2 6 4 3  (limit 1.69G, req 0.07G)
I0815 22:42:41.874892 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'out3a' with space 2.57G/2 6 4 3  (limit 1.78G, req 0.07G)
I0815 22:42:41.892287 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'out3a' with space 2.57G/2 6 4 3  (limit 1.78G, req 0.07G)
I0815 22:42:41.895052 14907 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 2.57G/1 6 4 3  (limit 1.69G, req 0.07G)
I0815 22:42:41.922404 14907 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 2.57G/1 6 1 5  (limit 1.69G, req 0.07G)
I0815 22:42:41.931397 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 2.57G/1 6 4 3  (limit 1.78G, req 0.07G)
I0815 22:42:41.948889 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 2.57G/1 6 4 3  (limit 1.78G, req 0.07G)
I0815 22:42:41.959044 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 2.57G/1 6 1 5  (limit 1.78G, req 0.07G)
I0815 22:42:41.976095 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 2.57G/1 6 1 5  (limit 1.78G, req 0.07G)
I0815 22:42:42.106993 14907 solver.cpp:317] Iteration 2 (0.841092 s), loss = 0.066865
I0815 22:42:42.107014 14907 solver.cpp:334]     Train net output #0: loss = 0.066865 (* 1 = 0.066865 loss)
I0815 22:42:42.107635 14909 cudnn_conv_layer.cpp:292] [2] Layer 'conv1a' reallocating workspace: 2.57G -> 0.14G
I0815 22:42:42.107638 14908 cudnn_conv_layer.cpp:292] [1] Layer 'conv1a' reallocating workspace: 2.57G -> 0.14G
I0815 22:42:42.125629 14907 cudnn_conv_layer.cpp:292] [0] Layer 'conv1a' reallocating workspace: 2.57G -> 0.14G
I0815 22:43:01.341945 14907 solver.cpp:312] Iteration 100 (5.09503 iter/s, 19.2344s/98 iter), loss = 0.0686244
I0815 22:43:01.341969 14907 solver.cpp:334]     Train net output #0: loss = 0.0686244 (* 1 = 0.0686244 loss)
I0815 22:43:01.341974 14907 sgd_solver.cpp:136] Iteration 100, lr = 1e-05, m = 0.9
I0815 22:43:13.654039 14872 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 22:43:21.066071 14907 solver.cpp:312] Iteration 200 (5.07007 iter/s, 19.7236s/100 iter), loss = 0.0976446
I0815 22:43:21.066092 14907 solver.cpp:334]     Train net output #0: loss = 0.0976446 (* 1 = 0.0976446 loss)
I0815 22:43:21.066097 14907 sgd_solver.cpp:136] Iteration 200, lr = 1e-05, m = 0.9
I0815 22:43:40.758436 14907 solver.cpp:312] Iteration 300 (5.07825 iter/s, 19.6918s/100 iter), loss = 0.0637274
I0815 22:43:40.758461 14907 solver.cpp:334]     Train net output #0: loss = 0.0637274 (* 1 = 0.0637274 loss)
I0815 22:43:40.758466 14907 sgd_solver.cpp:136] Iteration 300, lr = 1e-05, m = 0.9
I0815 22:43:46.044625 14870 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 22:44:00.336427 14907 solver.cpp:312] Iteration 400 (5.10792 iter/s, 19.5774s/100 iter), loss = 0.0752905
I0815 22:44:00.336449 14907 solver.cpp:334]     Train net output #0: loss = 0.0752905 (* 1 = 0.0752905 loss)
I0815 22:44:00.336454 14907 sgd_solver.cpp:136] Iteration 400, lr = 1e-05, m = 0.9
I0815 22:44:19.959313 14907 solver.cpp:312] Iteration 500 (5.09623 iter/s, 19.6223s/100 iter), loss = 0.0865745
I0815 22:44:19.959381 14907 solver.cpp:334]     Train net output #0: loss = 0.0865745 (* 1 = 0.0865745 loss)
I0815 22:44:19.959388 14907 sgd_solver.cpp:136] Iteration 500, lr = 1e-05, m = 0.9
I0815 22:44:39.328218 14907 solver.cpp:312] Iteration 600 (5.16306 iter/s, 19.3684s/100 iter), loss = 0.0658122
I0815 22:44:39.328243 14907 solver.cpp:334]     Train net output #0: loss = 0.0658122 (* 1 = 0.0658122 loss)
I0815 22:44:39.328248 14907 sgd_solver.cpp:136] Iteration 600, lr = 1e-05, m = 0.9
I0815 22:44:50.650686 14916 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 22:44:58.887686 14907 solver.cpp:312] Iteration 700 (5.11275 iter/s, 19.5589s/100 iter), loss = 0.0645232
I0815 22:44:58.887711 14907 solver.cpp:334]     Train net output #0: loss = 0.0645232 (* 1 = 0.0645232 loss)
I0815 22:44:58.887714 14907 sgd_solver.cpp:136] Iteration 700, lr = 1e-05, m = 0.9
I0815 22:45:18.239030 14907 solver.cpp:312] Iteration 800 (5.16774 iter/s, 19.3508s/100 iter), loss = 0.0804667
I0815 22:45:18.239053 14907 solver.cpp:334]     Train net output #0: loss = 0.0804667 (* 1 = 0.0804667 loss)
I0815 22:45:18.239058 14907 sgd_solver.cpp:136] Iteration 800, lr = 1e-05, m = 0.9
I0815 22:45:37.573274 14907 solver.cpp:312] Iteration 900 (5.17231 iter/s, 19.3337s/100 iter), loss = 0.0687623
I0815 22:45:37.573351 14907 solver.cpp:334]     Train net output #0: loss = 0.0687622 (* 1 = 0.0687622 loss)
I0815 22:45:37.573359 14907 sgd_solver.cpp:136] Iteration 900, lr = 1e-05, m = 0.9
I0815 22:45:54.648846 14918 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 22:45:56.796686 14907 solver.cpp:363] Sparsity after update:
I0815 22:45:56.823575 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0815 22:45:56.823614 14907 net.cpp:2192] conv1a_param_0(0) 
I0815 22:45:56.823637 14907 net.cpp:2192] conv1b_param_0(0) 
I0815 22:45:56.823662 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0815 22:45:56.823675 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0815 22:45:56.823688 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0815 22:45:56.823699 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0815 22:45:56.823710 14907 net.cpp:2192] ctx_final_param_0(0) 
I0815 22:45:56.823722 14907 net.cpp:2192] out3a_param_0(0) 
I0815 22:45:56.823735 14907 net.cpp:2192] out5a_param_0(0) 
I0815 22:45:56.823747 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 22:45:56.823760 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 22:45:56.823770 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 22:45:56.823782 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 22:45:56.823793 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 22:45:56.823803 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 22:45:56.823815 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 22:45:56.823825 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 22:45:56.823837 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0815 22:45:57.017827 14907 solver.cpp:312] Iteration 1000 (5.14297 iter/s, 19.444s/100 iter), loss = 0.0936237
I0815 22:45:57.017853 14907 solver.cpp:334]     Train net output #0: loss = 0.0936236 (* 1 = 0.0936236 loss)
I0815 22:45:57.017856 14907 sgd_solver.cpp:136] Iteration 1000, lr = 1e-05, m = 0.9
I0815 22:46:16.472687 14907 solver.cpp:312] Iteration 1100 (5.14025 iter/s, 19.4543s/100 iter), loss = 0.0615981
I0815 22:46:16.472740 14907 solver.cpp:334]     Train net output #0: loss = 0.061598 (* 1 = 0.061598 loss)
I0815 22:46:16.472748 14907 sgd_solver.cpp:136] Iteration 1100, lr = 1e-05, m = 0.9
I0815 22:46:26.689368 14916 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 22:46:35.830919 14907 solver.cpp:312] Iteration 1200 (5.1659 iter/s, 19.3577s/100 iter), loss = 0.0758057
I0815 22:46:35.830948 14907 solver.cpp:334]     Train net output #0: loss = 0.0758057 (* 1 = 0.0758057 loss)
I0815 22:46:35.830952 14907 sgd_solver.cpp:136] Iteration 1200, lr = 1e-05, m = 0.9
I0815 22:46:55.302230 14907 solver.cpp:312] Iteration 1300 (5.1359 iter/s, 19.4708s/100 iter), loss = 0.0853191
I0815 22:46:55.302299 14907 solver.cpp:334]     Train net output #0: loss = 0.085319 (* 1 = 0.085319 loss)
I0815 22:46:55.302305 14907 sgd_solver.cpp:136] Iteration 1300, lr = 1e-05, m = 0.9
I0815 22:47:14.772589 14907 solver.cpp:312] Iteration 1400 (5.13615 iter/s, 19.4698s/100 iter), loss = 0.10685
I0815 22:47:14.772617 14907 solver.cpp:334]     Train net output #0: loss = 0.106849 (* 1 = 0.106849 loss)
I0815 22:47:14.772624 14907 sgd_solver.cpp:136] Iteration 1400, lr = 1e-05, m = 0.9
I0815 22:47:31.254215 14870 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 22:47:34.286036 14907 solver.cpp:312] Iteration 1500 (5.12481 iter/s, 19.5129s/100 iter), loss = 0.0866053
I0815 22:47:34.286056 14907 solver.cpp:334]     Train net output #0: loss = 0.0866053 (* 1 = 0.0866053 loss)
I0815 22:47:34.286061 14907 sgd_solver.cpp:136] Iteration 1500, lr = 1e-05, m = 0.9
I0815 22:47:53.923362 14907 solver.cpp:312] Iteration 1600 (5.09248 iter/s, 19.6368s/100 iter), loss = 0.0663926
I0815 22:47:53.923384 14907 solver.cpp:334]     Train net output #0: loss = 0.0663925 (* 1 = 0.0663925 loss)
I0815 22:47:53.923388 14907 sgd_solver.cpp:136] Iteration 1600, lr = 1e-05, m = 0.9
I0815 22:48:13.516434 14907 solver.cpp:312] Iteration 1700 (5.10399 iter/s, 19.5925s/100 iter), loss = 0.0921687
I0815 22:48:13.516485 14907 solver.cpp:334]     Train net output #0: loss = 0.0921686 (* 1 = 0.0921686 loss)
I0815 22:48:13.516492 14907 sgd_solver.cpp:136] Iteration 1700, lr = 1e-05, m = 0.9
I0815 22:48:32.992612 14907 solver.cpp:312] Iteration 1800 (5.13462 iter/s, 19.4756s/100 iter), loss = 0.0580605
I0815 22:48:32.992637 14907 solver.cpp:334]     Train net output #0: loss = 0.0580604 (* 1 = 0.0580604 loss)
I0815 22:48:32.992641 14907 sgd_solver.cpp:136] Iteration 1800, lr = 1e-05, m = 0.9
I0815 22:48:35.903399 14918 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 22:48:52.329174 14907 solver.cpp:312] Iteration 1900 (5.17169 iter/s, 19.336s/100 iter), loss = 0.0500722
I0815 22:48:52.336280 14907 solver.cpp:334]     Train net output #0: loss = 0.0500722 (* 1 = 0.0500722 loss)
I0815 22:48:52.336331 14907 sgd_solver.cpp:136] Iteration 1900, lr = 1e-05, m = 0.9
I0815 22:49:08.012506 14870 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 22:49:11.682070 14907 solver.cpp:363] Sparsity after update:
I0815 22:49:11.691974 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0815 22:49:11.692030 14907 net.cpp:2192] conv1a_param_0(0) 
I0815 22:49:11.692059 14907 net.cpp:2192] conv1b_param_0(0) 
I0815 22:49:11.692066 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0815 22:49:11.692076 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0815 22:49:11.692085 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0815 22:49:11.692090 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0815 22:49:11.692096 14907 net.cpp:2192] ctx_final_param_0(0) 
I0815 22:49:11.692101 14907 net.cpp:2192] out3a_param_0(0) 
I0815 22:49:11.692106 14907 net.cpp:2192] out5a_param_0(0) 
I0815 22:49:11.692113 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 22:49:11.692121 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 22:49:11.692143 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 22:49:11.692152 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 22:49:11.692160 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 22:49:11.692167 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 22:49:11.692173 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 22:49:11.692181 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 22:49:11.692188 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0815 22:49:11.692205 14907 solver.cpp:509] Iteration 2000, Testing net (#0)
I0815 22:49:24.000157 14907 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.953107
I0815 22:49:24.000216 14907 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999531
I0815 22:49:24.000222 14907 solver.cpp:594]     Test net output #2: loss = 0.181906 (* 1 = 0.181906 loss)
I0815 22:49:24.000244 14907 solver.cpp:264] [MultiGPU] Tests completed in 12.3077s
I0815 22:49:24.216711 14907 solver.cpp:312] Iteration 2000 (3.13611 iter/s, 31.8867s/100 iter), loss = 0.0583783
I0815 22:49:24.216740 14907 solver.cpp:334]     Train net output #0: loss = 0.0583783 (* 1 = 0.0583783 loss)
I0815 22:49:24.216745 14907 sgd_solver.cpp:136] Iteration 2000, lr = 1e-05, m = 0.9
I0815 22:49:43.916226 14907 solver.cpp:312] Iteration 2100 (5.07641 iter/s, 19.699s/100 iter), loss = 0.0676623
I0815 22:49:43.916357 14907 solver.cpp:334]     Train net output #0: loss = 0.0676623 (* 1 = 0.0676623 loss)
I0815 22:49:43.916374 14907 sgd_solver.cpp:136] Iteration 2100, lr = 1e-05, m = 0.9
I0815 22:49:52.864326 14915 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 22:50:03.459097 14907 solver.cpp:312] Iteration 2200 (5.11709 iter/s, 19.5423s/100 iter), loss = 0.120557
I0815 22:50:03.459161 14907 solver.cpp:334]     Train net output #0: loss = 0.120557 (* 1 = 0.120557 loss)
I0815 22:50:03.459166 14907 sgd_solver.cpp:136] Iteration 2200, lr = 1e-05, m = 0.9
I0815 22:50:23.080025 14907 solver.cpp:312] Iteration 2300 (5.09674 iter/s, 19.6204s/100 iter), loss = 0.0684635
I0815 22:50:23.080047 14907 solver.cpp:334]     Train net output #0: loss = 0.0684635 (* 1 = 0.0684635 loss)
I0815 22:50:23.080051 14907 sgd_solver.cpp:136] Iteration 2300, lr = 1e-05, m = 0.9
I0815 22:50:42.554600 14907 solver.cpp:312] Iteration 2400 (5.13504 iter/s, 19.474s/100 iter), loss = 0.0598489
I0815 22:50:42.554675 14907 solver.cpp:334]     Train net output #0: loss = 0.0598489 (* 1 = 0.0598489 loss)
I0815 22:50:42.554683 14907 sgd_solver.cpp:136] Iteration 2400, lr = 1e-05, m = 0.9
I0815 22:50:57.440361 14872 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 22:51:02.194344 14907 solver.cpp:312] Iteration 2500 (5.09186 iter/s, 19.6392s/100 iter), loss = 0.0856105
I0815 22:51:02.194367 14907 solver.cpp:334]     Train net output #0: loss = 0.0856104 (* 1 = 0.0856104 loss)
I0815 22:51:02.194373 14907 sgd_solver.cpp:136] Iteration 2500, lr = 1e-05, m = 0.9
I0815 22:51:21.817829 14907 solver.cpp:312] Iteration 2600 (5.09608 iter/s, 19.6229s/100 iter), loss = 0.0766372
I0815 22:51:21.817910 14907 solver.cpp:334]     Train net output #0: loss = 0.0766372 (* 1 = 0.0766372 loss)
I0815 22:51:21.817919 14907 sgd_solver.cpp:136] Iteration 2600, lr = 1e-05, m = 0.9
I0815 22:51:29.945611 14870 data_reader.cpp:288] Starting prefetch of epoch 4
I0815 22:51:41.358024 14907 solver.cpp:312] Iteration 2700 (5.1178 iter/s, 19.5397s/100 iter), loss = 0.0653204
I0815 22:51:41.358049 14907 solver.cpp:334]     Train net output #0: loss = 0.0653204 (* 1 = 0.0653204 loss)
I0815 22:51:41.358055 14907 sgd_solver.cpp:136] Iteration 2700, lr = 1e-05, m = 0.9
I0815 22:52:00.715930 14907 solver.cpp:312] Iteration 2800 (5.16599 iter/s, 19.3574s/100 iter), loss = 0.0598765
I0815 22:52:00.715978 14907 solver.cpp:334]     Train net output #0: loss = 0.0598765 (* 1 = 0.0598765 loss)
I0815 22:52:00.715983 14907 sgd_solver.cpp:136] Iteration 2800, lr = 1e-05, m = 0.9
I0815 22:52:20.248504 14907 solver.cpp:312] Iteration 2900 (5.11979 iter/s, 19.532s/100 iter), loss = 0.0834499
I0815 22:52:20.248531 14907 solver.cpp:334]     Train net output #0: loss = 0.0834498 (* 1 = 0.0834498 loss)
I0815 22:52:20.248538 14907 sgd_solver.cpp:136] Iteration 2900, lr = 1e-05, m = 0.9
I0815 22:52:34.267211 14870 data_reader.cpp:288] Starting prefetch of epoch 5
I0815 22:52:39.787719 14907 solver.cpp:363] Sparsity after update:
I0815 22:52:39.805766 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0815 22:52:39.806150 14907 net.cpp:2192] conv1a_param_0(0) 
I0815 22:52:39.806267 14907 net.cpp:2192] conv1b_param_0(0) 
I0815 22:52:39.806354 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0815 22:52:39.806435 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0815 22:52:39.806517 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0815 22:52:39.806599 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0815 22:52:39.806681 14907 net.cpp:2192] ctx_final_param_0(0) 
I0815 22:52:39.806761 14907 net.cpp:2192] out3a_param_0(0) 
I0815 22:52:39.806839 14907 net.cpp:2192] out5a_param_0(0) 
I0815 22:52:39.806918 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 22:52:39.806999 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 22:52:39.807080 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 22:52:39.807160 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 22:52:39.807240 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 22:52:39.807322 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 22:52:39.807409 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 22:52:39.807492 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 22:52:39.807581 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0815 22:52:40.004289 14907 solver.cpp:312] Iteration 3000 (5.06195 iter/s, 19.7552s/100 iter), loss = 0.0853676
I0815 22:52:40.004313 14907 solver.cpp:334]     Train net output #0: loss = 0.0853676 (* 1 = 0.0853676 loss)
I0815 22:52:40.004318 14907 sgd_solver.cpp:136] Iteration 3000, lr = 1e-05, m = 0.9
I0815 22:52:59.686667 14907 solver.cpp:312] Iteration 3100 (5.08083 iter/s, 19.6818s/100 iter), loss = 0.0427826
I0815 22:52:59.686697 14907 solver.cpp:334]     Train net output #0: loss = 0.0427826 (* 1 = 0.0427826 loss)
I0815 22:52:59.686703 14907 sgd_solver.cpp:136] Iteration 3100, lr = 1e-05, m = 0.9
I0815 22:53:19.285200 14907 solver.cpp:312] Iteration 3200 (5.10256 iter/s, 19.598s/100 iter), loss = 0.124939
I0815 22:53:19.285266 14907 solver.cpp:334]     Train net output #0: loss = 0.124939 (* 1 = 0.124939 loss)
I0815 22:53:19.285272 14907 sgd_solver.cpp:136] Iteration 3200, lr = 1e-05, m = 0.9
I0815 22:53:38.839561 14907 solver.cpp:312] Iteration 3300 (5.11409 iter/s, 19.5538s/100 iter), loss = 0.0616888
I0815 22:53:38.839587 14907 solver.cpp:334]     Train net output #0: loss = 0.0616888 (* 1 = 0.0616888 loss)
I0815 22:53:38.839593 14907 sgd_solver.cpp:136] Iteration 3300, lr = 1e-05, m = 0.9
I0815 22:53:39.250320 14920 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 22:53:58.335009 14907 solver.cpp:312] Iteration 3400 (5.12954 iter/s, 19.4949s/100 iter), loss = 0.0444929
I0815 22:53:58.340167 14907 solver.cpp:334]     Train net output #0: loss = 0.0444929 (* 1 = 0.0444929 loss)
I0815 22:53:58.340184 14907 sgd_solver.cpp:136] Iteration 3400, lr = 1e-05, m = 0.9
I0815 22:54:17.797096 14907 solver.cpp:312] Iteration 3500 (5.13834 iter/s, 19.4616s/100 iter), loss = 0.0751494
I0815 22:54:17.797118 14907 solver.cpp:334]     Train net output #0: loss = 0.0751494 (* 1 = 0.0751494 loss)
I0815 22:54:17.797124 14907 sgd_solver.cpp:136] Iteration 3500, lr = 1e-05, m = 0.9
I0815 22:54:37.255991 14907 solver.cpp:312] Iteration 3600 (5.13918 iter/s, 19.4584s/100 iter), loss = 0.0727355
I0815 22:54:37.256044 14907 solver.cpp:334]     Train net output #0: loss = 0.0727355 (* 1 = 0.0727355 loss)
I0815 22:54:37.256049 14907 sgd_solver.cpp:136] Iteration 3600, lr = 1e-05, m = 0.9
I0815 22:54:43.644078 14920 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 22:54:56.902101 14907 solver.cpp:312] Iteration 3700 (5.09021 iter/s, 19.6456s/100 iter), loss = 0.0723288
I0815 22:54:56.902124 14907 solver.cpp:334]     Train net output #0: loss = 0.0723288 (* 1 = 0.0723288 loss)
I0815 22:54:56.902129 14907 sgd_solver.cpp:136] Iteration 3700, lr = 1e-05, m = 0.9
I0815 22:55:16.268573 14870 data_reader.cpp:288] Starting prefetch of epoch 6
I0815 22:55:16.622700 14907 solver.cpp:312] Iteration 3800 (5.07098 iter/s, 19.7201s/100 iter), loss = 0.0794883
I0815 22:55:16.622725 14907 solver.cpp:334]     Train net output #0: loss = 0.0794883 (* 1 = 0.0794883 loss)
I0815 22:55:16.622730 14907 sgd_solver.cpp:136] Iteration 3800, lr = 1e-05, m = 0.9
I0815 22:55:35.985396 14907 solver.cpp:312] Iteration 3900 (5.16471 iter/s, 19.3622s/100 iter), loss = 0.0614188
I0815 22:55:35.985415 14907 solver.cpp:334]     Train net output #0: loss = 0.0614188 (* 1 = 0.0614188 loss)
I0815 22:55:35.985419 14907 sgd_solver.cpp:136] Iteration 3900, lr = 1e-05, m = 0.9
I0815 22:55:55.248708 14907 solver.cpp:363] Sparsity after update:
I0815 22:55:55.263154 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0815 22:55:55.263172 14907 net.cpp:2192] conv1a_param_0(0) 
I0815 22:55:55.263181 14907 net.cpp:2192] conv1b_param_0(0) 
I0815 22:55:55.263185 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0815 22:55:55.263187 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0815 22:55:55.263190 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0815 22:55:55.263195 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0815 22:55:55.263198 14907 net.cpp:2192] ctx_final_param_0(0) 
I0815 22:55:55.263200 14907 net.cpp:2192] out3a_param_0(0) 
I0815 22:55:55.263203 14907 net.cpp:2192] out5a_param_0(0) 
I0815 22:55:55.263206 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 22:55:55.263209 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 22:55:55.263212 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 22:55:55.263216 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 22:55:55.263218 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 22:55:55.263221 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 22:55:55.263226 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 22:55:55.263229 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 22:55:55.263232 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0815 22:55:55.263245 14907 solver.cpp:509] Iteration 4000, Testing net (#0)
I0815 22:55:58.885042 14890 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 22:56:07.345016 14907 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.951925
I0815 22:56:07.345036 14907 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999899
I0815 22:56:07.345041 14907 solver.cpp:594]     Test net output #2: loss = 0.161988 (* 1 = 0.161988 loss)
I0815 22:56:07.345067 14907 solver.cpp:264] [MultiGPU] Tests completed in 12.0815s
I0815 22:56:07.552745 14907 solver.cpp:312] Iteration 4000 (3.16792 iter/s, 31.5665s/100 iter), loss = 0.0564036
I0815 22:56:07.552769 14907 solver.cpp:334]     Train net output #0: loss = 0.0564036 (* 1 = 0.0564036 loss)
I0815 22:56:07.552773 14907 sgd_solver.cpp:136] Iteration 4000, lr = 1e-05, m = 0.9
I0815 22:56:26.949587 14907 solver.cpp:312] Iteration 4100 (5.15562 iter/s, 19.3963s/100 iter), loss = 0.0878937
I0815 22:56:26.949657 14907 solver.cpp:334]     Train net output #0: loss = 0.0878937 (* 1 = 0.0878937 loss)
I0815 22:56:26.949666 14907 sgd_solver.cpp:136] Iteration 4100, lr = 1e-05, m = 0.9
I0815 22:56:32.450440 14920 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 22:56:46.377604 14907 solver.cpp:312] Iteration 4200 (5.14735 iter/s, 19.4275s/100 iter), loss = 0.0718417
I0815 22:56:46.377634 14907 solver.cpp:334]     Train net output #0: loss = 0.0718417 (* 1 = 0.0718417 loss)
I0815 22:56:46.377641 14907 sgd_solver.cpp:136] Iteration 4200, lr = 1e-05, m = 0.9
I0815 22:57:05.022186 14915 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 22:57:06.193083 14907 solver.cpp:312] Iteration 4300 (5.0467 iter/s, 19.8149s/100 iter), loss = 0.0941952
I0815 22:57:06.193109 14907 solver.cpp:334]     Train net output #0: loss = 0.0941952 (* 1 = 0.0941952 loss)
I0815 22:57:06.193114 14907 sgd_solver.cpp:136] Iteration 4300, lr = 1e-05, m = 0.9
I0815 22:57:25.773568 14907 solver.cpp:312] Iteration 4400 (5.10727 iter/s, 19.5799s/100 iter), loss = 0.0549483
I0815 22:57:25.773591 14907 solver.cpp:334]     Train net output #0: loss = 0.0549483 (* 1 = 0.0549483 loss)
I0815 22:57:25.773596 14907 sgd_solver.cpp:136] Iteration 4400, lr = 1e-05, m = 0.9
I0815 22:57:45.270709 14907 solver.cpp:312] Iteration 4500 (5.1291 iter/s, 19.4966s/100 iter), loss = 0.0456376
I0815 22:57:45.270756 14907 solver.cpp:334]     Train net output #0: loss = 0.0456377 (* 1 = 0.0456377 loss)
I0815 22:57:45.270761 14907 sgd_solver.cpp:136] Iteration 4500, lr = 1e-05, m = 0.9
I0815 22:58:04.690323 14907 solver.cpp:312] Iteration 4600 (5.14957 iter/s, 19.4191s/100 iter), loss = 0.104379
I0815 22:58:04.690354 14907 solver.cpp:334]     Train net output #0: loss = 0.104379 (* 1 = 0.104379 loss)
I0815 22:58:04.690361 14907 sgd_solver.cpp:136] Iteration 4600, lr = 1e-05, m = 0.9
I0815 22:58:09.471403 14916 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 22:58:24.092232 14907 solver.cpp:312] Iteration 4700 (5.15427 iter/s, 19.4014s/100 iter), loss = 0.0703168
I0815 22:58:24.092284 14907 solver.cpp:334]     Train net output #0: loss = 0.0703169 (* 1 = 0.0703169 loss)
I0815 22:58:24.092290 14907 sgd_solver.cpp:136] Iteration 4700, lr = 1e-05, m = 0.9
I0815 22:58:43.965000 14907 solver.cpp:312] Iteration 4800 (5.03215 iter/s, 19.8722s/100 iter), loss = 0.0764964
I0815 22:58:43.965034 14907 solver.cpp:334]     Train net output #0: loss = 0.0764965 (* 1 = 0.0764965 loss)
I0815 22:58:43.965041 14907 sgd_solver.cpp:136] Iteration 4800, lr = 1e-05, m = 0.9
I0815 22:59:03.653935 14907 solver.cpp:312] Iteration 4900 (5.07913 iter/s, 19.6884s/100 iter), loss = 0.0702351
I0815 22:59:03.656162 14907 solver.cpp:334]     Train net output #0: loss = 0.0702352 (* 1 = 0.0702352 loss)
I0815 22:59:03.656172 14907 sgd_solver.cpp:136] Iteration 4900, lr = 1e-05, m = 0.9
I0815 22:59:14.468473 14918 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 22:59:23.072685 14907 solver.cpp:363] Sparsity after update:
I0815 22:59:23.074623 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0815 22:59:23.074650 14907 net.cpp:2192] conv1a_param_0(0) 
I0815 22:59:23.074661 14907 net.cpp:2192] conv1b_param_0(0) 
I0815 22:59:23.074663 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0815 22:59:23.074666 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0815 22:59:23.074669 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0815 22:59:23.074684 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0815 22:59:23.074695 14907 net.cpp:2192] ctx_final_param_0(0) 
I0815 22:59:23.074703 14907 net.cpp:2192] out3a_param_0(0) 
I0815 22:59:23.074712 14907 net.cpp:2192] out5a_param_0(0) 
I0815 22:59:23.074720 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 22:59:23.074730 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 22:59:23.074738 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 22:59:23.074748 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 22:59:23.074754 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 22:59:23.074764 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 22:59:23.074771 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 22:59:23.074779 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 22:59:23.074789 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0815 22:59:23.267906 14907 solver.cpp:312] Iteration 5000 (5.09855 iter/s, 19.6134s/100 iter), loss = 0.0795705
I0815 22:59:23.267930 14907 solver.cpp:334]     Train net output #0: loss = 0.0795706 (* 1 = 0.0795706 loss)
I0815 22:59:23.267935 14907 sgd_solver.cpp:136] Iteration 5000, lr = 1e-05, m = 0.9
I0815 22:59:42.678761 14907 solver.cpp:312] Iteration 5100 (5.1519 iter/s, 19.4103s/100 iter), loss = 0.110932
I0815 22:59:42.679008 14907 solver.cpp:334]     Train net output #0: loss = 0.110932 (* 1 = 0.110932 loss)
I0815 22:59:42.679016 14907 sgd_solver.cpp:136] Iteration 5100, lr = 1e-05, m = 0.9
I0815 22:59:46.756242 14916 data_reader.cpp:288] Starting prefetch of epoch 4
I0815 23:00:02.266089 14907 solver.cpp:312] Iteration 5200 (5.10548 iter/s, 19.5868s/100 iter), loss = 0.0796664
I0815 23:00:02.266111 14907 solver.cpp:334]     Train net output #0: loss = 0.0796664 (* 1 = 0.0796664 loss)
I0815 23:00:02.266115 14907 sgd_solver.cpp:136] Iteration 5200, lr = 1e-05, m = 0.9
I0815 23:00:21.637356 14907 solver.cpp:312] Iteration 5300 (5.16243 iter/s, 19.3707s/100 iter), loss = 0.0495893
I0815 23:00:21.637430 14907 solver.cpp:334]     Train net output #0: loss = 0.0495893 (* 1 = 0.0495893 loss)
I0815 23:00:21.637437 14907 sgd_solver.cpp:136] Iteration 5300, lr = 1e-05, m = 0.9
I0815 23:00:41.357671 14907 solver.cpp:312] Iteration 5400 (5.07105 iter/s, 19.7198s/100 iter), loss = 0.0557451
I0815 23:00:41.357693 14907 solver.cpp:334]     Train net output #0: loss = 0.0557451 (* 1 = 0.0557451 loss)
I0815 23:00:41.357698 14907 sgd_solver.cpp:136] Iteration 5400, lr = 1e-05, m = 0.9
I0815 23:00:51.293861 14920 data_reader.cpp:288] Starting prefetch of epoch 4
I0815 23:01:00.758060 14907 solver.cpp:312] Iteration 5500 (5.15468 iter/s, 19.3999s/100 iter), loss = 0.0740712
I0815 23:01:00.758111 14907 solver.cpp:334]     Train net output #0: loss = 0.0740712 (* 1 = 0.0740712 loss)
I0815 23:01:00.758116 14907 sgd_solver.cpp:136] Iteration 5500, lr = 1e-05, m = 0.9
I0815 23:01:20.217659 14907 solver.cpp:312] Iteration 5600 (5.13899 iter/s, 19.4591s/100 iter), loss = 0.0366421
I0815 23:01:20.217730 14907 solver.cpp:334]     Train net output #0: loss = 0.0366422 (* 1 = 0.0366422 loss)
I0815 23:01:20.217747 14907 sgd_solver.cpp:136] Iteration 5600, lr = 1e-05, m = 0.9
I0815 23:01:39.560231 14907 solver.cpp:312] Iteration 5700 (5.17008 iter/s, 19.342s/100 iter), loss = 0.0773739
I0815 23:01:39.560300 14907 solver.cpp:334]     Train net output #0: loss = 0.0773739 (* 1 = 0.0773739 loss)
I0815 23:01:39.560307 14907 sgd_solver.cpp:136] Iteration 5700, lr = 1e-05, m = 0.9
I0815 23:01:55.246150 14920 data_reader.cpp:288] Starting prefetch of epoch 5
I0815 23:01:59.031536 14907 solver.cpp:312] Iteration 5800 (5.1359 iter/s, 19.4708s/100 iter), loss = 0.0719006
I0815 23:01:59.031558 14907 solver.cpp:334]     Train net output #0: loss = 0.0719006 (* 1 = 0.0719006 loss)
I0815 23:01:59.031565 14907 sgd_solver.cpp:136] Iteration 5800, lr = 1e-05, m = 0.9
I0815 23:02:18.630785 14907 solver.cpp:312] Iteration 5900 (5.10238 iter/s, 19.5987s/100 iter), loss = 0.0605701
I0815 23:02:18.630864 14907 solver.cpp:334]     Train net output #0: loss = 0.0605701 (* 1 = 0.0605701 loss)
I0815 23:02:18.630872 14907 sgd_solver.cpp:136] Iteration 5900, lr = 1e-05, m = 0.9
I0815 23:02:27.648569 14915 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 23:02:37.941296 14907 solver.cpp:363] Sparsity after update:
I0815 23:02:37.946640 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0815 23:02:37.946665 14907 net.cpp:2192] conv1a_param_0(0) 
I0815 23:02:37.946674 14907 net.cpp:2192] conv1b_param_0(0) 
I0815 23:02:37.946677 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0815 23:02:37.946681 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0815 23:02:37.946682 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0815 23:02:37.946686 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0815 23:02:37.946688 14907 net.cpp:2192] ctx_final_param_0(0) 
I0815 23:02:37.946707 14907 net.cpp:2192] out3a_param_0(0) 
I0815 23:02:37.946715 14907 net.cpp:2192] out5a_param_0(0) 
I0815 23:02:37.946722 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 23:02:37.946730 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 23:02:37.946738 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 23:02:37.946745 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 23:02:37.946753 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 23:02:37.946760 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 23:02:37.946768 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 23:02:37.946775 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 23:02:37.946782 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0815 23:02:37.946799 14907 solver.cpp:509] Iteration 6000, Testing net (#0)
I0815 23:02:48.891825 14907 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 23:02:49.942041 14907 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.950637
I0815 23:02:49.942067 14907 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999224
I0815 23:02:49.942073 14907 solver.cpp:594]     Test net output #2: loss = 0.207568 (* 1 = 0.207568 loss)
I0815 23:02:49.942129 14907 solver.cpp:264] [MultiGPU] Tests completed in 11.995s
I0815 23:02:50.149562 14907 solver.cpp:312] Iteration 6000 (3.1728 iter/s, 31.5179s/100 iter), loss = 0.0930146
I0815 23:02:50.149585 14907 solver.cpp:334]     Train net output #0: loss = 0.0930146 (* 1 = 0.0930146 loss)
I0815 23:02:50.149591 14907 sgd_solver.cpp:136] Iteration 6000, lr = 1e-05, m = 0.9
I0815 23:03:09.902242 14907 solver.cpp:312] Iteration 6100 (5.06275 iter/s, 19.7521s/100 iter), loss = 0.072984
I0815 23:03:09.902271 14907 solver.cpp:334]     Train net output #0: loss = 0.072984 (* 1 = 0.072984 loss)
I0815 23:03:09.902277 14907 sgd_solver.cpp:136] Iteration 6100, lr = 1e-05, m = 0.9
I0815 23:03:12.309451 14920 data_reader.cpp:288] Starting prefetch of epoch 6
I0815 23:03:29.164685 14907 solver.cpp:312] Iteration 6200 (5.19159 iter/s, 19.2619s/100 iter), loss = 0.0693349
I0815 23:03:29.164753 14907 solver.cpp:334]     Train net output #0: loss = 0.0693349 (* 1 = 0.0693349 loss)
I0815 23:03:29.164760 14907 sgd_solver.cpp:136] Iteration 6200, lr = 1e-05, m = 0.9
I0815 23:03:48.949785 14907 solver.cpp:312] Iteration 6300 (5.05445 iter/s, 19.7846s/100 iter), loss = 0.0692507
I0815 23:03:48.949810 14907 solver.cpp:334]     Train net output #0: loss = 0.0692507 (* 1 = 0.0692507 loss)
I0815 23:03:48.949816 14907 sgd_solver.cpp:136] Iteration 6300, lr = 1e-05, m = 0.9
I0815 23:04:08.870555 14907 solver.cpp:312] Iteration 6400 (5.02002 iter/s, 19.9202s/100 iter), loss = 0.0821214
I0815 23:04:08.870617 14907 solver.cpp:334]     Train net output #0: loss = 0.0821214 (* 1 = 0.0821214 loss)
I0815 23:04:08.870625 14907 sgd_solver.cpp:136] Iteration 6400, lr = 1e-05, m = 0.9
I0815 23:04:17.101155 14872 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 23:04:28.438601 14907 solver.cpp:312] Iteration 6500 (5.11051 iter/s, 19.5675s/100 iter), loss = 0.0804752
I0815 23:04:28.438629 14907 solver.cpp:334]     Train net output #0: loss = 0.0804751 (* 1 = 0.0804751 loss)
I0815 23:04:28.438658 14907 sgd_solver.cpp:136] Iteration 6500, lr = 1e-05, m = 0.9
I0815 23:04:48.086793 14907 solver.cpp:312] Iteration 6600 (5.08967 iter/s, 19.6477s/100 iter), loss = 0.0809348
I0815 23:04:48.087064 14907 solver.cpp:334]     Train net output #0: loss = 0.0809348 (* 1 = 0.0809348 loss)
I0815 23:04:48.087071 14907 sgd_solver.cpp:136] Iteration 6600, lr = 1e-05, m = 0.9
I0815 23:04:49.699412 14915 data_reader.cpp:288] Starting prefetch of epoch 4
I0815 23:05:07.585875 14907 solver.cpp:312] Iteration 6700 (5.12859 iter/s, 19.4985s/100 iter), loss = 0.0904878
I0815 23:05:07.585896 14907 solver.cpp:334]     Train net output #0: loss = 0.0904878 (* 1 = 0.0904878 loss)
I0815 23:05:07.585902 14907 sgd_solver.cpp:136] Iteration 6700, lr = 1e-05, m = 0.9
I0815 23:05:27.089349 14907 solver.cpp:312] Iteration 6800 (5.12743 iter/s, 19.5029s/100 iter), loss = 0.0484946
I0815 23:05:27.089398 14907 solver.cpp:334]     Train net output #0: loss = 0.0484946 (* 1 = 0.0484946 loss)
I0815 23:05:27.089403 14907 sgd_solver.cpp:136] Iteration 6800, lr = 1e-05, m = 0.9
I0815 23:05:46.704663 14907 solver.cpp:312] Iteration 6900 (5.0982 iter/s, 19.6148s/100 iter), loss = 0.0911915
I0815 23:05:46.704684 14907 solver.cpp:334]     Train net output #0: loss = 0.0911915 (* 1 = 0.0911915 loss)
I0815 23:05:46.704689 14907 sgd_solver.cpp:136] Iteration 6900, lr = 1e-05, m = 0.9
I0815 23:05:53.983722 14918 data_reader.cpp:288] Starting prefetch of epoch 4
I0815 23:06:05.844784 14907 solver.cpp:363] Sparsity after update:
I0815 23:06:05.850344 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0815 23:06:05.850404 14907 net.cpp:2192] conv1a_param_0(0) 
I0815 23:06:05.850421 14907 net.cpp:2192] conv1b_param_0(0) 
I0815 23:06:05.850425 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0815 23:06:05.850427 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0815 23:06:05.850430 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0815 23:06:05.850432 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0815 23:06:05.850435 14907 net.cpp:2192] ctx_final_param_0(0) 
I0815 23:06:05.850438 14907 net.cpp:2192] out3a_param_0(0) 
I0815 23:06:05.850445 14907 net.cpp:2192] out5a_param_0(0) 
I0815 23:06:05.850448 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 23:06:05.850451 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 23:06:05.850455 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 23:06:05.850457 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 23:06:05.850461 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 23:06:05.850463 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 23:06:05.850466 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 23:06:05.850469 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 23:06:05.850471 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0815 23:06:06.045689 14907 solver.cpp:312] Iteration 7000 (5.1705 iter/s, 19.3405s/100 iter), loss = 0.0982227
I0815 23:06:06.045713 14907 solver.cpp:334]     Train net output #0: loss = 0.0982227 (* 1 = 0.0982227 loss)
I0815 23:06:06.045718 14907 sgd_solver.cpp:136] Iteration 7000, lr = 1e-05, m = 0.9
I0815 23:06:25.702143 14907 solver.cpp:312] Iteration 7100 (5.08753 iter/s, 19.6559s/100 iter), loss = 0.0707592
I0815 23:06:25.702165 14907 solver.cpp:334]     Train net output #0: loss = 0.0707592 (* 1 = 0.0707592 loss)
I0815 23:06:25.702172 14907 sgd_solver.cpp:136] Iteration 7100, lr = 1e-05, m = 0.9
I0815 23:06:45.145412 14907 solver.cpp:312] Iteration 7200 (5.14331 iter/s, 19.4427s/100 iter), loss = 0.0644682
I0815 23:06:45.145512 14907 solver.cpp:334]     Train net output #0: loss = 0.0644682 (* 1 = 0.0644682 loss)
I0815 23:06:45.145519 14907 sgd_solver.cpp:136] Iteration 7200, lr = 1e-05, m = 0.9
I0815 23:06:58.612114 14872 data_reader.cpp:288] Starting prefetch of epoch 4
I0815 23:07:04.671530 14907 solver.cpp:312] Iteration 7300 (5.12149 iter/s, 19.5256s/100 iter), loss = 0.0602461
I0815 23:07:04.671553 14907 solver.cpp:334]     Train net output #0: loss = 0.0602461 (* 1 = 0.0602461 loss)
I0815 23:07:04.671557 14907 sgd_solver.cpp:136] Iteration 7300, lr = 1e-05, m = 0.9
I0815 23:07:24.400293 14907 solver.cpp:312] Iteration 7400 (5.06888 iter/s, 19.7282s/100 iter), loss = 0.0916521
I0815 23:07:24.400377 14907 solver.cpp:334]     Train net output #0: loss = 0.091652 (* 1 = 0.091652 loss)
I0815 23:07:24.400384 14907 sgd_solver.cpp:136] Iteration 7400, lr = 1e-05, m = 0.9
I0815 23:07:31.144485 14916 data_reader.cpp:288] Starting prefetch of epoch 5
I0815 23:07:43.904687 14907 solver.cpp:312] Iteration 7500 (5.12719 iter/s, 19.5039s/100 iter), loss = 0.0932663
I0815 23:07:43.904716 14907 solver.cpp:334]     Train net output #0: loss = 0.0932663 (* 1 = 0.0932663 loss)
I0815 23:07:43.904722 14907 sgd_solver.cpp:136] Iteration 7500, lr = 1e-05, m = 0.9
I0815 23:08:03.419869 14907 solver.cpp:312] Iteration 7600 (5.12436 iter/s, 19.5146s/100 iter), loss = 0.0603292
I0815 23:08:03.420286 14907 solver.cpp:334]     Train net output #0: loss = 0.0603292 (* 1 = 0.0603292 loss)
I0815 23:08:03.420295 14907 sgd_solver.cpp:136] Iteration 7600, lr = 1e-05, m = 0.9
I0815 23:08:23.002099 14907 solver.cpp:312] Iteration 7700 (5.10681 iter/s, 19.5817s/100 iter), loss = 0.0677599
I0815 23:08:23.002126 14907 solver.cpp:334]     Train net output #0: loss = 0.0677599 (* 1 = 0.0677599 loss)
I0815 23:08:23.002132 14907 sgd_solver.cpp:136] Iteration 7700, lr = 1e-05, m = 0.9
I0815 23:08:35.965867 14870 data_reader.cpp:288] Starting prefetch of epoch 7
I0815 23:08:43.790622 14907 solver.cpp:312] Iteration 7800 (4.81048 iter/s, 20.788s/100 iter), loss = 0.0733587
I0815 23:08:43.790648 14907 solver.cpp:334]     Train net output #0: loss = 0.0733587 (* 1 = 0.0733587 loss)
I0815 23:08:43.790654 14907 sgd_solver.cpp:136] Iteration 7800, lr = 1e-05, m = 0.9
I0815 23:09:06.545853 14907 solver.cpp:312] Iteration 7900 (4.39471 iter/s, 22.7546s/100 iter), loss = 0.0530147
I0815 23:09:06.545907 14907 solver.cpp:334]     Train net output #0: loss = 0.0530147 (* 1 = 0.0530147 loss)
I0815 23:09:06.545914 14907 sgd_solver.cpp:136] Iteration 7900, lr = 1e-05, m = 0.9
I0815 23:09:25.824373 14907 solver.cpp:363] Sparsity after update:
I0815 23:09:25.840075 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0815 23:09:25.840095 14907 net.cpp:2192] conv1a_param_0(0) 
I0815 23:09:25.840103 14907 net.cpp:2192] conv1b_param_0(0) 
I0815 23:09:25.840106 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0815 23:09:25.840111 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0815 23:09:25.840113 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0815 23:09:25.840117 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0815 23:09:25.840121 14907 net.cpp:2192] ctx_final_param_0(0) 
I0815 23:09:25.840126 14907 net.cpp:2192] out3a_param_0(0) 
I0815 23:09:25.840137 14907 net.cpp:2192] out5a_param_0(0) 
I0815 23:09:25.840140 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 23:09:25.840143 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 23:09:25.840147 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 23:09:25.840152 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 23:09:25.840157 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 23:09:25.840160 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 23:09:25.840163 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 23:09:25.840167 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 23:09:25.840170 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0815 23:09:25.840181 14907 solver.cpp:509] Iteration 8000, Testing net (#0)
I0815 23:09:29.461092 14940 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 23:09:37.753823 14944 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 23:09:38.163487 14907 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.9517
I0815 23:09:38.163516 14907 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999938
I0815 23:09:38.163523 14907 solver.cpp:594]     Test net output #2: loss = 0.160049 (* 1 = 0.160049 loss)
I0815 23:09:38.163549 14907 solver.cpp:264] [MultiGPU] Tests completed in 12.323s
I0815 23:09:38.362529 14907 solver.cpp:312] Iteration 8000 (3.14309 iter/s, 31.8158s/100 iter), loss = 0.0947565
I0815 23:09:38.362551 14907 solver.cpp:334]     Train net output #0: loss = 0.0947564 (* 1 = 0.0947564 loss)
I0815 23:09:38.362555 14907 sgd_solver.cpp:136] Iteration 8000, lr = 1e-05, m = 0.9
I0815 23:09:57.799269 14907 solver.cpp:312] Iteration 8100 (5.14504 iter/s, 19.4362s/100 iter), loss = 0.060784
I0815 23:09:57.799299 14907 solver.cpp:334]     Train net output #0: loss = 0.060784 (* 1 = 0.060784 loss)
I0815 23:09:57.799306 14907 sgd_solver.cpp:136] Iteration 8100, lr = 1e-05, m = 0.9
I0815 23:10:17.763748 14907 solver.cpp:312] Iteration 8200 (5.00903 iter/s, 19.9639s/100 iter), loss = 0.0574155
I0815 23:10:17.763801 14907 solver.cpp:334]     Train net output #0: loss = 0.0574155 (* 1 = 0.0574155 loss)
I0815 23:10:17.763808 14907 sgd_solver.cpp:136] Iteration 8200, lr = 1e-05, m = 0.9
I0815 23:10:29.616750 14872 data_reader.cpp:288] Starting prefetch of epoch 5
I0815 23:10:37.472920 14907 solver.cpp:312] Iteration 8300 (5.07392 iter/s, 19.7086s/100 iter), loss = 0.115171
I0815 23:10:37.472945 14907 solver.cpp:334]     Train net output #0: loss = 0.115171 (* 1 = 0.115171 loss)
I0815 23:10:37.472951 14907 sgd_solver.cpp:136] Iteration 8300, lr = 1e-05, m = 0.9
I0815 23:10:57.266960 14907 solver.cpp:312] Iteration 8400 (5.05217 iter/s, 19.7935s/100 iter), loss = 0.0623324
I0815 23:10:57.267045 14907 solver.cpp:334]     Train net output #0: loss = 0.0623324 (* 1 = 0.0623324 loss)
I0815 23:10:57.267052 14907 sgd_solver.cpp:136] Iteration 8400, lr = 1e-05, m = 0.9
I0815 23:11:16.855150 14907 solver.cpp:312] Iteration 8500 (5.10526 iter/s, 19.5877s/100 iter), loss = 0.0530169
I0815 23:11:16.855181 14907 solver.cpp:334]     Train net output #0: loss = 0.0530169 (* 1 = 0.0530169 loss)
I0815 23:11:16.855188 14907 sgd_solver.cpp:136] Iteration 8500, lr = 1e-05, m = 0.9
I0815 23:11:34.769525 14915 data_reader.cpp:288] Starting prefetch of epoch 5
I0815 23:11:36.481268 14907 solver.cpp:312] Iteration 8600 (5.09539 iter/s, 19.6256s/100 iter), loss = 0.0936117
I0815 23:11:36.481292 14907 solver.cpp:334]     Train net output #0: loss = 0.0936117 (* 1 = 0.0936117 loss)
I0815 23:11:36.481297 14907 sgd_solver.cpp:136] Iteration 8600, lr = 1e-05, m = 0.9
I0815 23:11:56.138574 14907 solver.cpp:312] Iteration 8700 (5.08731 iter/s, 19.6568s/100 iter), loss = 0.0720299
I0815 23:11:56.138600 14907 solver.cpp:334]     Train net output #0: loss = 0.0720299 (* 1 = 0.0720299 loss)
I0815 23:11:56.138607 14907 sgd_solver.cpp:136] Iteration 8700, lr = 1e-05, m = 0.9
I0815 23:12:15.777654 14907 solver.cpp:312] Iteration 8800 (5.09203 iter/s, 19.6385s/100 iter), loss = 0.0713198
I0815 23:12:15.777705 14907 solver.cpp:334]     Train net output #0: loss = 0.0713198 (* 1 = 0.0713198 loss)
I0815 23:12:15.777712 14907 sgd_solver.cpp:136] Iteration 8800, lr = 1e-05, m = 0.9
I0815 23:12:35.427063 14907 solver.cpp:312] Iteration 8900 (5.08935 iter/s, 19.6489s/100 iter), loss = 0.0714195
I0815 23:12:35.427091 14907 solver.cpp:334]     Train net output #0: loss = 0.0714195 (* 1 = 0.0714195 loss)
I0815 23:12:35.427098 14907 sgd_solver.cpp:136] Iteration 8900, lr = 1e-05, m = 0.9
I0815 23:12:39.612243 14872 data_reader.cpp:288] Starting prefetch of epoch 6
I0815 23:12:55.103649 14907 solver.cpp:363] Sparsity after update:
I0815 23:12:55.105824 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0815 23:12:55.105837 14907 net.cpp:2192] conv1a_param_0(0) 
I0815 23:12:55.105845 14907 net.cpp:2192] conv1b_param_0(0) 
I0815 23:12:55.105847 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0815 23:12:55.105849 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0815 23:12:55.105851 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0815 23:12:55.105854 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0815 23:12:55.105855 14907 net.cpp:2192] ctx_final_param_0(0) 
I0815 23:12:55.105857 14907 net.cpp:2192] out3a_param_0(0) 
I0815 23:12:55.105859 14907 net.cpp:2192] out5a_param_0(0) 
I0815 23:12:55.105861 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 23:12:55.105865 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 23:12:55.105866 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 23:12:55.105868 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 23:12:55.105871 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 23:12:55.105875 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 23:12:55.105878 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 23:12:55.105880 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 23:12:55.105883 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0815 23:12:55.295634 14907 solver.cpp:312] Iteration 9000 (5.03321 iter/s, 19.868s/100 iter), loss = 0.0637469
I0815 23:12:55.295660 14907 solver.cpp:334]     Train net output #0: loss = 0.0637469 (* 1 = 0.0637469 loss)
I0815 23:12:55.295665 14907 sgd_solver.cpp:136] Iteration 9000, lr = 1e-05, m = 0.9
I0815 23:13:14.874986 14907 solver.cpp:312] Iteration 9100 (5.10756 iter/s, 19.5788s/100 iter), loss = 0.0791558
I0815 23:13:14.875011 14907 solver.cpp:334]     Train net output #0: loss = 0.0791558 (* 1 = 0.0791558 loss)
I0815 23:13:14.875015 14907 sgd_solver.cpp:136] Iteration 9100, lr = 1e-05, m = 0.9
I0815 23:13:34.505937 14907 solver.cpp:312] Iteration 9200 (5.09414 iter/s, 19.6304s/100 iter), loss = 0.0625818
I0815 23:13:34.505993 14907 solver.cpp:334]     Train net output #0: loss = 0.0625818 (* 1 = 0.0625818 loss)
I0815 23:13:34.506000 14907 sgd_solver.cpp:136] Iteration 9200, lr = 1e-05, m = 0.9
I0815 23:13:44.718806 14918 data_reader.cpp:288] Starting prefetch of epoch 5
I0815 23:13:54.027995 14907 solver.cpp:312] Iteration 9300 (5.12255 iter/s, 19.5215s/100 iter), loss = 0.0689121
I0815 23:13:54.028020 14907 solver.cpp:334]     Train net output #0: loss = 0.0689121 (* 1 = 0.0689121 loss)
I0815 23:13:54.028028 14907 sgd_solver.cpp:136] Iteration 9300, lr = 1e-05, m = 0.9
I0815 23:14:13.596858 14907 solver.cpp:312] Iteration 9400 (5.1103 iter/s, 19.5683s/100 iter), loss = 0.0893542
I0815 23:14:13.596909 14907 solver.cpp:334]     Train net output #0: loss = 0.0893542 (* 1 = 0.0893542 loss)
I0815 23:14:13.596915 14907 sgd_solver.cpp:136] Iteration 9400, lr = 1e-05, m = 0.9
I0815 23:14:16.953148 14915 data_reader.cpp:288] Starting prefetch of epoch 6
I0815 23:14:33.092782 14907 solver.cpp:312] Iteration 9500 (5.12942 iter/s, 19.4954s/100 iter), loss = 0.0610399
I0815 23:14:33.092806 14907 solver.cpp:334]     Train net output #0: loss = 0.0610399 (* 1 = 0.0610399 loss)
I0815 23:14:33.092813 14907 sgd_solver.cpp:136] Iteration 9500, lr = 1e-05, m = 0.9
I0815 23:14:52.540289 14907 solver.cpp:312] Iteration 9600 (5.14219 iter/s, 19.447s/100 iter), loss = 0.0645963
I0815 23:14:52.540338 14907 solver.cpp:334]     Train net output #0: loss = 0.0645963 (* 1 = 0.0645963 loss)
I0815 23:14:52.540343 14907 sgd_solver.cpp:136] Iteration 9600, lr = 1e-05, m = 0.9
I0815 23:15:11.983840 14907 solver.cpp:312] Iteration 9700 (5.14323 iter/s, 19.443s/100 iter), loss = 0.0634353
I0815 23:15:11.983870 14907 solver.cpp:334]     Train net output #0: loss = 0.0634353 (* 1 = 0.0634353 loss)
I0815 23:15:11.983875 14907 sgd_solver.cpp:136] Iteration 9700, lr = 1e-05, m = 0.9
I0815 23:15:21.364152 14872 data_reader.cpp:288] Starting prefetch of epoch 7
I0815 23:15:31.541091 14907 solver.cpp:312] Iteration 9800 (5.11334 iter/s, 19.5567s/100 iter), loss = 0.0613361
I0815 23:15:31.541277 14907 solver.cpp:334]     Train net output #0: loss = 0.0613361 (* 1 = 0.0613361 loss)
I0815 23:15:31.541304 14907 sgd_solver.cpp:136] Iteration 9800, lr = 1e-05, m = 0.9
I0815 23:15:51.274864 14907 solver.cpp:312] Iteration 9900 (5.06759 iter/s, 19.7332s/100 iter), loss = 0.0665242
I0815 23:15:51.274888 14907 solver.cpp:334]     Train net output #0: loss = 0.0665242 (* 1 = 0.0665242 loss)
I0815 23:15:51.274891 14907 sgd_solver.cpp:136] Iteration 9900, lr = 1e-05, m = 0.9
I0815 23:16:10.775241 14907 solver.cpp:639] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/sparse/cityscapes5_jsegnet21v2_iter_10000.caffemodel
I0815 23:16:10.878458 14907 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/sparse/cityscapes5_jsegnet21v2_iter_10000.solverstate
I0815 23:16:10.886278 14907 solver.cpp:363] Sparsity after update:
I0815 23:16:10.896919 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0815 23:16:10.896934 14907 net.cpp:2192] conv1a_param_0(0) 
I0815 23:16:10.896944 14907 net.cpp:2192] conv1b_param_0(0) 
I0815 23:16:10.896946 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0815 23:16:10.896950 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0815 23:16:10.896955 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0815 23:16:10.896957 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0815 23:16:10.896960 14907 net.cpp:2192] ctx_final_param_0(0) 
I0815 23:16:10.896962 14907 net.cpp:2192] out3a_param_0(0) 
I0815 23:16:10.896966 14907 net.cpp:2192] out5a_param_0(0) 
I0815 23:16:10.896970 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 23:16:10.896973 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 23:16:10.896977 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 23:16:10.896980 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 23:16:10.896983 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 23:16:10.896986 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 23:16:10.896991 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 23:16:10.896993 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 23:16:10.896996 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0815 23:16:10.897009 14907 solver.cpp:509] Iteration 10000, Testing net (#0)
I0815 23:16:19.003911 14944 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 23:16:24.619354 14907 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.951972
I0815 23:16:24.619380 14907 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999155
I0815 23:16:24.619386 14907 solver.cpp:594]     Test net output #2: loss = 0.200229 (* 1 = 0.200229 loss)
I0815 23:16:24.619468 14907 solver.cpp:264] [MultiGPU] Tests completed in 13.7221s
I0815 23:16:24.828840 14907 solver.cpp:312] Iteration 10000 (2.98035 iter/s, 33.5531s/100 iter), loss = 0.0718695
I0815 23:16:24.828868 14907 solver.cpp:334]     Train net output #0: loss = 0.0718694 (* 1 = 0.0718694 loss)
I0815 23:16:24.828873 14907 sgd_solver.cpp:136] Iteration 10000, lr = 1e-05, m = 0.9
I0815 23:16:44.600798 14907 solver.cpp:312] Iteration 10100 (5.05781 iter/s, 19.7714s/100 iter), loss = 0.067439
I0815 23:16:44.600857 14907 solver.cpp:334]     Train net output #0: loss = 0.067439 (* 1 = 0.067439 loss)
I0815 23:16:44.600864 14907 sgd_solver.cpp:136] Iteration 10100, lr = 1e-05, m = 0.9
I0815 23:17:04.055550 14907 solver.cpp:312] Iteration 10200 (5.14028 iter/s, 19.4542s/100 iter), loss = 0.0815444
I0815 23:17:04.055584 14907 solver.cpp:334]     Train net output #0: loss = 0.0815444 (* 1 = 0.0815444 loss)
I0815 23:17:04.055589 14907 sgd_solver.cpp:136] Iteration 10200, lr = 1e-05, m = 0.9
I0815 23:17:12.745636 14872 data_reader.cpp:288] Starting prefetch of epoch 8
I0815 23:17:23.714076 14907 solver.cpp:312] Iteration 10300 (5.08699 iter/s, 19.658s/100 iter), loss = 0.0557319
I0815 23:17:23.714174 14907 solver.cpp:334]     Train net output #0: loss = 0.0557318 (* 1 = 0.0557318 loss)
I0815 23:17:23.714197 14907 sgd_solver.cpp:136] Iteration 10300, lr = 1e-05, m = 0.9
I0815 23:17:43.242667 14907 solver.cpp:312] Iteration 10400 (5.12084 iter/s, 19.5281s/100 iter), loss = 0.0518824
I0815 23:17:43.242693 14907 solver.cpp:334]     Train net output #0: loss = 0.0518824 (* 1 = 0.0518824 loss)
I0815 23:17:43.242699 14907 sgd_solver.cpp:136] Iteration 10400, lr = 1e-05, m = 0.9
I0815 23:17:45.106705 14916 data_reader.cpp:288] Starting prefetch of epoch 6
I0815 23:18:02.945128 14907 solver.cpp:312] Iteration 10500 (5.07565 iter/s, 19.7019s/100 iter), loss = 0.0599832
I0815 23:18:02.945185 14907 solver.cpp:334]     Train net output #0: loss = 0.0599832 (* 1 = 0.0599832 loss)
I0815 23:18:02.945191 14907 sgd_solver.cpp:136] Iteration 10500, lr = 1e-05, m = 0.9
I0815 23:18:22.617050 14907 solver.cpp:312] Iteration 10600 (5.08353 iter/s, 19.6714s/100 iter), loss = 0.081204
I0815 23:18:22.617079 14907 solver.cpp:334]     Train net output #0: loss = 0.081204 (* 1 = 0.081204 loss)
I0815 23:18:22.617086 14907 sgd_solver.cpp:136] Iteration 10600, lr = 1e-05, m = 0.9
I0815 23:18:42.104506 14907 solver.cpp:312] Iteration 10700 (5.13165 iter/s, 19.4869s/100 iter), loss = 0.0398074
I0815 23:18:42.104586 14907 solver.cpp:334]     Train net output #0: loss = 0.0398074 (* 1 = 0.0398074 loss)
I0815 23:18:42.104604 14907 sgd_solver.cpp:136] Iteration 10700, lr = 1e-05, m = 0.9
I0815 23:18:49.957062 14918 data_reader.cpp:288] Starting prefetch of epoch 6
I0815 23:19:01.762235 14907 solver.cpp:312] Iteration 10800 (5.0872 iter/s, 19.6572s/100 iter), loss = 0.0742281
I0815 23:19:01.762259 14907 solver.cpp:334]     Train net output #0: loss = 0.0742281 (* 1 = 0.0742281 loss)
I0815 23:19:01.762264 14907 sgd_solver.cpp:136] Iteration 10800, lr = 1e-05, m = 0.9
I0815 23:19:21.296891 14907 solver.cpp:312] Iteration 10900 (5.11925 iter/s, 19.5341s/100 iter), loss = 0.0459606
I0815 23:19:21.296942 14907 solver.cpp:334]     Train net output #0: loss = 0.0459606 (* 1 = 0.0459606 loss)
I0815 23:19:21.296952 14907 sgd_solver.cpp:136] Iteration 10900, lr = 1e-05, m = 0.9
I0815 23:19:40.590901 14907 solver.cpp:363] Sparsity after update:
I0815 23:19:40.609973 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0815 23:19:40.610100 14907 net.cpp:2192] conv1a_param_0(0) 
I0815 23:19:40.610123 14907 net.cpp:2192] conv1b_param_0(0) 
I0815 23:19:40.610131 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0815 23:19:40.610139 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0815 23:19:40.610147 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0815 23:19:40.610157 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0815 23:19:40.610163 14907 net.cpp:2192] ctx_final_param_0(0) 
I0815 23:19:40.610170 14907 net.cpp:2192] out3a_param_0(0) 
I0815 23:19:40.610179 14907 net.cpp:2192] out5a_param_0(0) 
I0815 23:19:40.610188 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 23:19:40.610195 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 23:19:40.610204 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 23:19:40.610211 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 23:19:40.610219 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 23:19:40.610226 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 23:19:40.610234 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 23:19:40.610240 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 23:19:40.610249 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0815 23:19:40.789026 14907 solver.cpp:312] Iteration 11000 (5.13042 iter/s, 19.4916s/100 iter), loss = 0.0596991
I0815 23:19:40.789053 14907 solver.cpp:334]     Train net output #0: loss = 0.0596991 (* 1 = 0.0596991 loss)
I0815 23:19:40.789059 14907 sgd_solver.cpp:136] Iteration 11000, lr = 1e-05, m = 0.9
I0815 23:19:54.446727 14872 data_reader.cpp:288] Starting prefetch of epoch 9
I0815 23:20:00.388139 14907 solver.cpp:312] Iteration 11100 (5.10241 iter/s, 19.5986s/100 iter), loss = 0.0762926
I0815 23:20:00.388170 14907 solver.cpp:334]     Train net output #0: loss = 0.0762926 (* 1 = 0.0762926 loss)
I0815 23:20:00.388175 14907 sgd_solver.cpp:136] Iteration 11100, lr = 1e-05, m = 0.9
I0815 23:20:20.132601 14907 solver.cpp:312] Iteration 11200 (5.06485 iter/s, 19.7439s/100 iter), loss = 0.0720218
I0815 23:20:20.132628 14907 solver.cpp:334]     Train net output #0: loss = 0.0720218 (* 1 = 0.0720218 loss)
I0815 23:20:20.132637 14907 sgd_solver.cpp:136] Iteration 11200, lr = 1e-05, m = 0.9
I0815 23:20:27.071640 14916 data_reader.cpp:288] Starting prefetch of epoch 7
I0815 23:20:39.629201 14907 solver.cpp:312] Iteration 11300 (5.12924 iter/s, 19.4961s/100 iter), loss = 0.0555065
I0815 23:20:39.629222 14907 solver.cpp:334]     Train net output #0: loss = 0.0555065 (* 1 = 0.0555065 loss)
I0815 23:20:39.629228 14907 sgd_solver.cpp:136] Iteration 11300, lr = 1e-05, m = 0.9
I0815 23:20:59.403218 14907 solver.cpp:312] Iteration 11400 (5.05728 iter/s, 19.7735s/100 iter), loss = 0.0621033
I0815 23:20:59.403271 14907 solver.cpp:334]     Train net output #0: loss = 0.0621033 (* 1 = 0.0621033 loss)
I0815 23:20:59.403278 14907 sgd_solver.cpp:136] Iteration 11400, lr = 1e-05, m = 0.9
I0815 23:21:18.855098 14907 solver.cpp:312] Iteration 11500 (5.14103 iter/s, 19.4513s/100 iter), loss = 0.080125
I0815 23:21:18.855123 14907 solver.cpp:334]     Train net output #0: loss = 0.080125 (* 1 = 0.080125 loss)
I0815 23:21:18.855128 14907 sgd_solver.cpp:136] Iteration 11500, lr = 1e-05, m = 0.9
I0815 23:21:31.731226 14872 data_reader.cpp:288] Starting prefetch of epoch 10
I0815 23:21:38.337175 14907 solver.cpp:312] Iteration 11600 (5.13306 iter/s, 19.4815s/100 iter), loss = 0.105912
I0815 23:21:38.337201 14907 solver.cpp:334]     Train net output #0: loss = 0.105912 (* 1 = 0.105912 loss)
I0815 23:21:38.337208 14907 sgd_solver.cpp:136] Iteration 11600, lr = 1e-05, m = 0.9
I0815 23:21:57.986567 14907 solver.cpp:312] Iteration 11700 (5.08936 iter/s, 19.6489s/100 iter), loss = 0.0412071
I0815 23:21:57.986593 14907 solver.cpp:334]     Train net output #0: loss = 0.041207 (* 1 = 0.041207 loss)
I0815 23:21:57.986598 14907 sgd_solver.cpp:136] Iteration 11700, lr = 1e-05, m = 0.9
I0815 23:22:17.524937 14907 solver.cpp:312] Iteration 11800 (5.11827 iter/s, 19.5378s/100 iter), loss = 0.0606172
I0815 23:22:17.524996 14907 solver.cpp:334]     Train net output #0: loss = 0.0606172 (* 1 = 0.0606172 loss)
I0815 23:22:17.525004 14907 sgd_solver.cpp:136] Iteration 11800, lr = 1e-05, m = 0.9
I0815 23:22:36.297773 14920 data_reader.cpp:288] Starting prefetch of epoch 7
I0815 23:22:37.050910 14907 solver.cpp:312] Iteration 11900 (5.12152 iter/s, 19.5254s/100 iter), loss = 0.0632305
I0815 23:22:37.050937 14907 solver.cpp:334]     Train net output #0: loss = 0.0632305 (* 1 = 0.0632305 loss)
I0815 23:22:37.050943 14907 sgd_solver.cpp:136] Iteration 11900, lr = 1e-05, m = 0.9
I0815 23:22:56.733485 14907 solver.cpp:363] Sparsity after update:
I0815 23:22:56.749357 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0815 23:22:56.749399 14907 net.cpp:2192] conv1a_param_0(0) 
I0815 23:22:56.749414 14907 net.cpp:2192] conv1b_param_0(0) 
I0815 23:22:56.749424 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0815 23:22:56.749433 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0815 23:22:56.749442 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0815 23:22:56.749451 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0815 23:22:56.749460 14907 net.cpp:2192] ctx_final_param_0(0) 
I0815 23:22:56.749469 14907 net.cpp:2192] out3a_param_0(0) 
I0815 23:22:56.749478 14907 net.cpp:2192] out5a_param_0(0) 
I0815 23:22:56.749487 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 23:22:56.749496 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 23:22:56.749505 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 23:22:56.749514 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 23:22:56.749523 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 23:22:56.749531 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 23:22:56.749541 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 23:22:56.749550 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 23:22:56.749558 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0815 23:22:56.749583 14907 solver.cpp:509] Iteration 12000, Testing net (#0)
I0815 23:23:01.146621 14940 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 23:23:10.263531 14907 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.951132
I0815 23:23:10.263552 14907 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999868
I0815 23:23:10.263557 14907 solver.cpp:594]     Test net output #2: loss = 0.165016 (* 1 = 0.165016 loss)
I0815 23:23:10.263588 14907 solver.cpp:264] [MultiGPU] Tests completed in 13.5136s
I0815 23:23:10.475903 14907 solver.cpp:312] Iteration 12000 (2.99185 iter/s, 33.4241s/100 iter), loss = 0.0378173
I0815 23:23:10.475929 14907 solver.cpp:334]     Train net output #0: loss = 0.0378173 (* 1 = 0.0378173 loss)
I0815 23:23:10.475934 14907 sgd_solver.cpp:136] Iteration 12000, lr = 1e-05, m = 0.9
I0815 23:23:22.549295 14920 data_reader.cpp:288] Starting prefetch of epoch 8
I0815 23:23:29.967221 14907 solver.cpp:312] Iteration 12100 (5.13063 iter/s, 19.4908s/100 iter), loss = 0.0885455
I0815 23:23:29.967285 14907 solver.cpp:334]     Train net output #0: loss = 0.0885455 (* 1 = 0.0885455 loss)
I0815 23:23:29.967290 14907 sgd_solver.cpp:136] Iteration 12100, lr = 1e-05, m = 0.9
I0815 23:23:49.566907 14907 solver.cpp:312] Iteration 12200 (5.10226 iter/s, 19.5991s/100 iter), loss = 0.0548304
I0815 23:23:49.566933 14907 solver.cpp:334]     Train net output #0: loss = 0.0548304 (* 1 = 0.0548304 loss)
I0815 23:23:49.566937 14907 sgd_solver.cpp:136] Iteration 12200, lr = 1e-05, m = 0.9
I0815 23:24:08.966152 14907 solver.cpp:312] Iteration 12300 (5.15498 iter/s, 19.3987s/100 iter), loss = 0.0497524
I0815 23:24:08.966197 14907 solver.cpp:334]     Train net output #0: loss = 0.0497523 (* 1 = 0.0497523 loss)
I0815 23:24:08.966202 14907 sgd_solver.cpp:136] Iteration 12300, lr = 1e-05, m = 0.9
I0815 23:24:26.863000 14872 data_reader.cpp:288] Starting prefetch of epoch 11
I0815 23:24:28.459403 14907 solver.cpp:312] Iteration 12400 (5.13012 iter/s, 19.4927s/100 iter), loss = 0.0532798
I0815 23:24:28.459429 14907 solver.cpp:334]     Train net output #0: loss = 0.0532798 (* 1 = 0.0532798 loss)
I0815 23:24:28.459435 14907 sgd_solver.cpp:136] Iteration 12400, lr = 1e-05, m = 0.9
I0815 23:24:48.184952 14907 solver.cpp:312] Iteration 12500 (5.06971 iter/s, 19.725s/100 iter), loss = 0.0715397
I0815 23:24:48.184999 14907 solver.cpp:334]     Train net output #0: loss = 0.0715396 (* 1 = 0.0715396 loss)
I0815 23:24:48.185004 14907 sgd_solver.cpp:136] Iteration 12500, lr = 1e-05, m = 0.9
I0815 23:24:59.573421 14870 data_reader.cpp:288] Starting prefetch of epoch 8
I0815 23:25:07.756774 14907 solver.cpp:312] Iteration 12600 (5.10953 iter/s, 19.5713s/100 iter), loss = 0.0459288
I0815 23:25:07.756799 14907 solver.cpp:334]     Train net output #0: loss = 0.0459288 (* 1 = 0.0459288 loss)
I0815 23:25:07.756803 14907 sgd_solver.cpp:136] Iteration 12600, lr = 1e-05, m = 0.9
I0815 23:25:27.724364 14907 solver.cpp:312] Iteration 12700 (5.00825 iter/s, 19.967s/100 iter), loss = 0.115026
I0815 23:25:27.724422 14907 solver.cpp:334]     Train net output #0: loss = 0.115026 (* 1 = 0.115026 loss)
I0815 23:25:27.724431 14907 sgd_solver.cpp:136] Iteration 12700, lr = 1e-05, m = 0.9
I0815 23:25:47.231850 14907 solver.cpp:312] Iteration 12800 (5.12638 iter/s, 19.507s/100 iter), loss = 0.0464258
I0815 23:25:47.231874 14907 solver.cpp:334]     Train net output #0: loss = 0.0464257 (* 1 = 0.0464257 loss)
I0815 23:25:47.231878 14907 sgd_solver.cpp:136] Iteration 12800, lr = 1e-05, m = 0.9
I0815 23:26:04.440820 14918 data_reader.cpp:288] Starting prefetch of epoch 7
I0815 23:26:06.746907 14907 solver.cpp:312] Iteration 12900 (5.12439 iter/s, 19.5145s/100 iter), loss = 0.0737448
I0815 23:26:06.746932 14907 solver.cpp:334]     Train net output #0: loss = 0.0737448 (* 1 = 0.0737448 loss)
I0815 23:26:06.746937 14907 sgd_solver.cpp:136] Iteration 12900, lr = 1e-05, m = 0.9
I0815 23:26:25.933656 14907 solver.cpp:363] Sparsity after update:
I0815 23:26:25.940088 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0815 23:26:25.940165 14907 net.cpp:2192] conv1a_param_0(0) 
I0815 23:26:25.940183 14907 net.cpp:2192] conv1b_param_0(0) 
I0815 23:26:25.940186 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0815 23:26:25.940189 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0815 23:26:25.940192 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0815 23:26:25.940196 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0815 23:26:25.940198 14907 net.cpp:2192] ctx_final_param_0(0) 
I0815 23:26:25.940201 14907 net.cpp:2192] out3a_param_0(0) 
I0815 23:26:25.940203 14907 net.cpp:2192] out5a_param_0(0) 
I0815 23:26:25.940207 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 23:26:25.940209 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 23:26:25.940212 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 23:26:25.940214 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 23:26:25.940217 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 23:26:25.940220 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 23:26:25.940223 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 23:26:25.940227 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 23:26:25.940229 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0815 23:26:26.138111 14907 solver.cpp:312] Iteration 13000 (5.15712 iter/s, 19.3907s/100 iter), loss = 0.0568267
I0815 23:26:26.138144 14907 solver.cpp:334]     Train net output #0: loss = 0.0568267 (* 1 = 0.0568267 loss)
I0815 23:26:26.138151 14907 sgd_solver.cpp:136] Iteration 13000, lr = 1e-05, m = 0.9
I0815 23:26:45.741760 14907 solver.cpp:312] Iteration 13100 (5.10123 iter/s, 19.6031s/100 iter), loss = 0.0793199
I0815 23:26:45.741833 14907 solver.cpp:334]     Train net output #0: loss = 0.0793198 (* 1 = 0.0793198 loss)
I0815 23:26:45.741840 14907 sgd_solver.cpp:136] Iteration 13100, lr = 1e-05, m = 0.9
I0815 23:27:05.313426 14907 solver.cpp:312] Iteration 13200 (5.10957 iter/s, 19.5711s/100 iter), loss = 0.0529609
I0815 23:27:05.313450 14907 solver.cpp:334]     Train net output #0: loss = 0.0529609 (* 1 = 0.0529609 loss)
I0815 23:27:05.313457 14907 sgd_solver.cpp:136] Iteration 13200, lr = 1e-05, m = 0.9
I0815 23:27:08.935531 14920 data_reader.cpp:288] Starting prefetch of epoch 9
I0815 23:27:24.666550 14907 solver.cpp:312] Iteration 13300 (5.16727 iter/s, 19.3526s/100 iter), loss = 0.0630468
I0815 23:27:24.666604 14907 solver.cpp:334]     Train net output #0: loss = 0.0630467 (* 1 = 0.0630467 loss)
I0815 23:27:24.666611 14907 sgd_solver.cpp:136] Iteration 13300, lr = 1e-05, m = 0.9
I0815 23:27:41.114593 14870 data_reader.cpp:288] Starting prefetch of epoch 9
I0815 23:27:44.169436 14907 solver.cpp:312] Iteration 13400 (5.12759 iter/s, 19.5024s/100 iter), loss = 0.0508843
I0815 23:27:44.169457 14907 solver.cpp:334]     Train net output #0: loss = 0.0508842 (* 1 = 0.0508842 loss)
I0815 23:27:44.169463 14907 sgd_solver.cpp:136] Iteration 13400, lr = 1e-05, m = 0.9
I0815 23:28:03.563326 14907 solver.cpp:312] Iteration 13500 (5.1564 iter/s, 19.3934s/100 iter), loss = 0.0685883
I0815 23:28:03.563386 14907 solver.cpp:334]     Train net output #0: loss = 0.0685882 (* 1 = 0.0685882 loss)
I0815 23:28:03.563393 14907 sgd_solver.cpp:136] Iteration 13500, lr = 1e-05, m = 0.9
I0815 23:28:22.974452 14907 solver.cpp:312] Iteration 13600 (5.15183 iter/s, 19.4106s/100 iter), loss = 0.0735402
I0815 23:28:22.974486 14907 solver.cpp:334]     Train net output #0: loss = 0.0735401 (* 1 = 0.0735401 loss)
I0815 23:28:22.974493 14907 sgd_solver.cpp:136] Iteration 13600, lr = 1e-05, m = 0.9
I0815 23:28:42.704449 14907 solver.cpp:312] Iteration 13700 (5.06856 iter/s, 19.7295s/100 iter), loss = 0.0544922
I0815 23:28:42.704937 14907 solver.cpp:334]     Train net output #0: loss = 0.0544921 (* 1 = 0.0544921 loss)
I0815 23:28:42.704944 14907 sgd_solver.cpp:136] Iteration 13700, lr = 1e-05, m = 0.9
I0815 23:28:45.623405 14920 data_reader.cpp:288] Starting prefetch of epoch 10
I0815 23:29:02.291692 14907 solver.cpp:312] Iteration 13800 (5.1055 iter/s, 19.5867s/100 iter), loss = 0.0542544
I0815 23:29:02.291716 14907 solver.cpp:334]     Train net output #0: loss = 0.0542543 (* 1 = 0.0542543 loss)
I0815 23:29:02.291720 14907 sgd_solver.cpp:136] Iteration 13800, lr = 1e-05, m = 0.9
I0815 23:29:21.718762 14907 solver.cpp:312] Iteration 13900 (5.1476 iter/s, 19.4265s/100 iter), loss = 0.0673829
I0815 23:29:21.718863 14907 solver.cpp:334]     Train net output #0: loss = 0.0673829 (* 1 = 0.0673829 loss)
I0815 23:29:21.718869 14907 sgd_solver.cpp:136] Iteration 13900, lr = 1e-05, m = 0.9
I0815 23:29:40.923331 14907 solver.cpp:363] Sparsity after update:
I0815 23:29:40.933768 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0815 23:29:40.933790 14907 net.cpp:2192] conv1a_param_0(0) 
I0815 23:29:40.933800 14907 net.cpp:2192] conv1b_param_0(0) 
I0815 23:29:40.933804 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0815 23:29:40.933809 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0815 23:29:40.933814 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0815 23:29:40.933817 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0815 23:29:40.933820 14907 net.cpp:2192] ctx_final_param_0(0) 
I0815 23:29:40.933825 14907 net.cpp:2192] out3a_param_0(0) 
I0815 23:29:40.933828 14907 net.cpp:2192] out5a_param_0(0) 
I0815 23:29:40.933832 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 23:29:40.933835 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 23:29:40.933840 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 23:29:40.933843 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 23:29:40.933847 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 23:29:40.933851 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 23:29:40.933853 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 23:29:40.933857 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 23:29:40.933861 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0815 23:29:40.933871 14907 solver.cpp:509] Iteration 14000, Testing net (#0)
I0815 23:29:48.329011 14888 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 23:29:52.869362 14907 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.952773
I0815 23:29:52.869467 14907 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.99917
I0815 23:29:52.869475 14907 solver.cpp:594]     Test net output #2: loss = 0.196637 (* 1 = 0.196637 loss)
I0815 23:29:52.869499 14907 solver.cpp:264] [MultiGPU] Tests completed in 11.9353s
I0815 23:29:53.075677 14907 solver.cpp:312] Iteration 14000 (3.18918 iter/s, 31.3561s/100 iter), loss = 0.0848091
I0815 23:29:53.075700 14907 solver.cpp:334]     Train net output #0: loss = 0.084809 (* 1 = 0.084809 loss)
I0815 23:29:53.075706 14907 sgd_solver.cpp:136] Iteration 14000, lr = 1e-05, m = 0.9
I0815 23:30:02.083956 14872 data_reader.cpp:288] Starting prefetch of epoch 12
I0815 23:30:12.612432 14907 solver.cpp:312] Iteration 14100 (5.1187 iter/s, 19.5362s/100 iter), loss = 0.0855483
I0815 23:30:12.612455 14907 solver.cpp:334]     Train net output #0: loss = 0.0855483 (* 1 = 0.0855483 loss)
I0815 23:30:12.612460 14907 sgd_solver.cpp:136] Iteration 14100, lr = 1e-05, m = 0.9
I0815 23:30:32.567929 14907 solver.cpp:312] Iteration 14200 (5.01129 iter/s, 19.9549s/100 iter), loss = 0.0530053
I0815 23:30:32.567980 14907 solver.cpp:334]     Train net output #0: loss = 0.0530052 (* 1 = 0.0530052 loss)
I0815 23:30:32.567986 14907 sgd_solver.cpp:136] Iteration 14200, lr = 1e-05, m = 0.9
I0815 23:30:52.216013 14907 solver.cpp:312] Iteration 14300 (5.0897 iter/s, 19.6475s/100 iter), loss = 0.107677
I0815 23:30:52.216039 14907 solver.cpp:334]     Train net output #0: loss = 0.107677 (* 1 = 0.107677 loss)
I0815 23:30:52.216044 14907 sgd_solver.cpp:136] Iteration 14300, lr = 1e-05, m = 0.9
I0815 23:31:07.105198 14920 data_reader.cpp:288] Starting prefetch of epoch 11
I0815 23:31:11.843602 14907 solver.cpp:312] Iteration 14400 (5.09501 iter/s, 19.627s/100 iter), loss = 0.0781074
I0815 23:31:11.843631 14907 solver.cpp:334]     Train net output #0: loss = 0.0781073 (* 1 = 0.0781073 loss)
I0815 23:31:11.843637 14907 sgd_solver.cpp:136] Iteration 14400, lr = 1e-05, m = 0.9
I0815 23:31:31.295197 14907 solver.cpp:312] Iteration 14500 (5.14111 iter/s, 19.4511s/100 iter), loss = 0.0770872
I0815 23:31:31.295220 14907 solver.cpp:334]     Train net output #0: loss = 0.0770872 (* 1 = 0.0770872 loss)
I0815 23:31:31.295225 14907 sgd_solver.cpp:136] Iteration 14500, lr = 1e-05, m = 0.9
I0815 23:31:50.810127 14907 solver.cpp:312] Iteration 14600 (5.12442 iter/s, 19.5144s/100 iter), loss = 0.0659908
I0815 23:31:50.810225 14907 solver.cpp:334]     Train net output #0: loss = 0.0659907 (* 1 = 0.0659907 loss)
I0815 23:31:50.810231 14907 sgd_solver.cpp:136] Iteration 14600, lr = 1e-05, m = 0.9
I0815 23:32:10.276232 14907 solver.cpp:312] Iteration 14700 (5.13728 iter/s, 19.4656s/100 iter), loss = 0.0613687
I0815 23:32:10.276257 14907 solver.cpp:334]     Train net output #0: loss = 0.0613687 (* 1 = 0.0613687 loss)
I0815 23:32:10.276262 14907 sgd_solver.cpp:136] Iteration 14700, lr = 1e-05, m = 0.9
I0815 23:32:11.462498 14915 data_reader.cpp:288] Starting prefetch of epoch 7
I0815 23:32:29.663789 14907 solver.cpp:312] Iteration 14800 (5.15809 iter/s, 19.387s/100 iter), loss = 0.0648248
I0815 23:32:29.663835 14907 solver.cpp:334]     Train net output #0: loss = 0.0648248 (* 1 = 0.0648248 loss)
I0815 23:32:29.663839 14907 sgd_solver.cpp:136] Iteration 14800, lr = 1e-05, m = 0.9
I0815 23:32:49.253484 14907 solver.cpp:312] Iteration 14900 (5.10487 iter/s, 19.5891s/100 iter), loss = 0.0774729
I0815 23:32:49.253517 14907 solver.cpp:334]     Train net output #0: loss = 0.0774729 (* 1 = 0.0774729 loss)
I0815 23:32:49.253522 14907 sgd_solver.cpp:136] Iteration 14900, lr = 1e-05, m = 0.9
I0815 23:33:08.669517 14907 solver.cpp:363] Sparsity after update:
I0815 23:33:08.695283 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0815 23:33:08.695385 14907 net.cpp:2192] conv1a_param_0(0) 
I0815 23:33:08.695411 14907 net.cpp:2192] conv1b_param_0(0) 
I0815 23:33:08.695425 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0815 23:33:08.695441 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0815 23:33:08.695454 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0815 23:33:08.695468 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0815 23:33:08.695487 14907 net.cpp:2192] ctx_final_param_0(0) 
I0815 23:33:08.695502 14907 net.cpp:2192] out3a_param_0(0) 
I0815 23:33:08.695515 14907 net.cpp:2192] out5a_param_0(0) 
I0815 23:33:08.695529 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 23:33:08.695544 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 23:33:08.695557 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 23:33:08.695570 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 23:33:08.695585 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 23:33:08.695598 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 23:33:08.695612 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 23:33:08.695626 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 23:33:08.695639 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0815 23:33:08.871237 14907 solver.cpp:312] Iteration 15000 (5.09756 iter/s, 19.6172s/100 iter), loss = 0.0576991
I0815 23:33:08.871268 14907 solver.cpp:334]     Train net output #0: loss = 0.0576991 (* 1 = 0.0576991 loss)
I0815 23:33:08.871274 14907 sgd_solver.cpp:136] Iteration 15000, lr = 1e-05, m = 0.9
I0815 23:33:16.192296 14872 data_reader.cpp:288] Starting prefetch of epoch 13
I0815 23:33:28.537195 14907 solver.cpp:312] Iteration 15100 (5.08507 iter/s, 19.6654s/100 iter), loss = 0.0907492
I0815 23:33:28.537219 14907 solver.cpp:334]     Train net output #0: loss = 0.0907491 (* 1 = 0.0907491 loss)
I0815 23:33:28.537223 14907 sgd_solver.cpp:136] Iteration 15100, lr = 1e-05, m = 0.9
I0815 23:33:48.138231 14907 solver.cpp:312] Iteration 15200 (5.10191 iter/s, 19.6005s/100 iter), loss = 0.0765603
I0815 23:33:48.138283 14907 solver.cpp:334]     Train net output #0: loss = 0.0765602 (* 1 = 0.0765602 loss)
I0815 23:33:48.138289 14907 sgd_solver.cpp:136] Iteration 15200, lr = 1e-05, m = 0.9
I0815 23:34:07.600464 14907 solver.cpp:312] Iteration 15300 (5.1383 iter/s, 19.4617s/100 iter), loss = 0.035469
I0815 23:34:07.600487 14907 solver.cpp:334]     Train net output #0: loss = 0.0354689 (* 1 = 0.0354689 loss)
I0815 23:34:07.600493 14907 sgd_solver.cpp:136] Iteration 15300, lr = 1e-05, m = 0.9
I0815 23:34:20.689438 14918 data_reader.cpp:288] Starting prefetch of epoch 8
I0815 23:34:27.110848 14907 solver.cpp:312] Iteration 15400 (5.12562 iter/s, 19.5099s/100 iter), loss = 0.0717578
I0815 23:34:27.110874 14907 solver.cpp:334]     Train net output #0: loss = 0.0717577 (* 1 = 0.0717577 loss)
I0815 23:34:27.110880 14907 sgd_solver.cpp:136] Iteration 15400, lr = 1e-05, m = 0.9
I0815 23:34:46.839047 14907 solver.cpp:312] Iteration 15500 (5.06903 iter/s, 19.7277s/100 iter), loss = 0.0702776
I0815 23:34:46.839073 14907 solver.cpp:334]     Train net output #0: loss = 0.0702775 (* 1 = 0.0702775 loss)
I0815 23:34:46.839079 14907 sgd_solver.cpp:136] Iteration 15500, lr = 1e-05, m = 0.9
I0815 23:34:53.325502 14916 data_reader.cpp:288] Starting prefetch of epoch 8
I0815 23:35:06.459787 14907 solver.cpp:312] Iteration 15600 (5.09679 iter/s, 19.6202s/100 iter), loss = 0.0613694
I0815 23:35:06.459811 14907 solver.cpp:334]     Train net output #0: loss = 0.0613693 (* 1 = 0.0613693 loss)
I0815 23:35:06.459816 14907 sgd_solver.cpp:136] Iteration 15600, lr = 1e-05, m = 0.9
I0815 23:35:25.958520 14907 solver.cpp:312] Iteration 15700 (5.12868 iter/s, 19.4982s/100 iter), loss = 0.0745149
I0815 23:35:25.958573 14907 solver.cpp:334]     Train net output #0: loss = 0.0745148 (* 1 = 0.0745148 loss)
I0815 23:35:25.958578 14907 sgd_solver.cpp:136] Iteration 15700, lr = 1e-05, m = 0.9
I0815 23:35:45.473044 14907 solver.cpp:312] Iteration 15800 (5.12453 iter/s, 19.514s/100 iter), loss = 0.0713325
I0815 23:35:45.473067 14907 solver.cpp:334]     Train net output #0: loss = 0.0713324 (* 1 = 0.0713324 loss)
I0815 23:35:45.473071 14907 sgd_solver.cpp:136] Iteration 15800, lr = 1e-05, m = 0.9
I0815 23:35:57.994663 14920 data_reader.cpp:288] Starting prefetch of epoch 12
I0815 23:36:05.059864 14907 solver.cpp:312] Iteration 15900 (5.10561 iter/s, 19.5863s/100 iter), loss = 0.0515843
I0815 23:36:05.059885 14907 solver.cpp:334]     Train net output #0: loss = 0.0515842 (* 1 = 0.0515842 loss)
I0815 23:36:05.059891 14907 sgd_solver.cpp:136] Iteration 15900, lr = 1e-05, m = 0.9
I0815 23:36:24.281293 14907 solver.cpp:363] Sparsity after update:
I0815 23:36:24.292484 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0815 23:36:24.292501 14907 net.cpp:2192] conv1a_param_0(0) 
I0815 23:36:24.292508 14907 net.cpp:2192] conv1b_param_0(0) 
I0815 23:36:24.292510 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0815 23:36:24.292512 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0815 23:36:24.292513 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0815 23:36:24.292515 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0815 23:36:24.292517 14907 net.cpp:2192] ctx_final_param_0(0) 
I0815 23:36:24.292520 14907 net.cpp:2192] out3a_param_0(0) 
I0815 23:36:24.292521 14907 net.cpp:2192] out5a_param_0(0) 
I0815 23:36:24.292523 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 23:36:24.292526 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 23:36:24.292527 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 23:36:24.292529 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 23:36:24.292531 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 23:36:24.292533 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 23:36:24.292538 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 23:36:24.292541 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 23:36:24.292544 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0815 23:36:24.292562 14907 solver.cpp:509] Iteration 16000, Testing net (#0)
I0815 23:36:35.296324 14890 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 23:36:36.688540 14907 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.95209
I0815 23:36:36.688561 14907 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999961
I0815 23:36:36.688570 14907 solver.cpp:594]     Test net output #2: loss = 0.160567 (* 1 = 0.160567 loss)
I0815 23:36:36.688604 14907 solver.cpp:264] [MultiGPU] Tests completed in 12.3957s
I0815 23:36:36.885869 14907 solver.cpp:312] Iteration 16000 (3.14217 iter/s, 31.8251s/100 iter), loss = 0.0762705
I0815 23:36:36.885893 14907 solver.cpp:334]     Train net output #0: loss = 0.0762704 (* 1 = 0.0762704 loss)
I0815 23:36:36.885898 14907 sgd_solver.cpp:136] Iteration 16000, lr = 1e-05, m = 0.9
I0815 23:36:42.358113 14870 data_reader.cpp:288] Starting prefetch of epoch 10
I0815 23:36:56.437661 14907 solver.cpp:312] Iteration 16100 (5.11476 iter/s, 19.5513s/100 iter), loss = 0.0716365
I0815 23:36:56.437687 14907 solver.cpp:334]     Train net output #0: loss = 0.0716364 (* 1 = 0.0716364 loss)
I0815 23:36:56.437693 14907 sgd_solver.cpp:136] Iteration 16100, lr = 1e-05, m = 0.9
I0815 23:37:15.732719 14907 solver.cpp:312] Iteration 16200 (5.18282 iter/s, 19.2945s/100 iter), loss = 0.0784553
I0815 23:37:15.732784 14907 solver.cpp:334]     Train net output #0: loss = 0.0784552 (* 1 = 0.0784552 loss)
I0815 23:37:15.732789 14907 sgd_solver.cpp:136] Iteration 16200, lr = 1e-05, m = 0.9
I0815 23:37:35.077229 14907 solver.cpp:312] Iteration 16300 (5.16957 iter/s, 19.344s/100 iter), loss = 0.0690488
I0815 23:37:35.077253 14907 solver.cpp:334]     Train net output #0: loss = 0.0690487 (* 1 = 0.0690487 loss)
I0815 23:37:35.077261 14907 sgd_solver.cpp:136] Iteration 16300, lr = 1e-05, m = 0.9
I0815 23:37:46.493077 14918 data_reader.cpp:288] Starting prefetch of epoch 9
I0815 23:37:54.441293 14907 solver.cpp:312] Iteration 16400 (5.16435 iter/s, 19.3635s/100 iter), loss = 0.0419087
I0815 23:37:54.441361 14907 solver.cpp:334]     Train net output #0: loss = 0.0419086 (* 1 = 0.0419086 loss)
I0815 23:37:54.441383 14907 sgd_solver.cpp:136] Iteration 16400, lr = 1e-05, m = 0.9
I0815 23:38:13.945461 14907 solver.cpp:312] Iteration 16500 (5.12725 iter/s, 19.5036s/100 iter), loss = 0.0660976
I0815 23:38:13.945482 14907 solver.cpp:334]     Train net output #0: loss = 0.0660975 (* 1 = 0.0660975 loss)
I0815 23:38:13.945487 14907 sgd_solver.cpp:136] Iteration 16500, lr = 1e-05, m = 0.9
I0815 23:38:33.219328 14907 solver.cpp:312] Iteration 16600 (5.18852 iter/s, 19.2733s/100 iter), loss = 0.0680654
I0815 23:38:33.219383 14907 solver.cpp:334]     Train net output #0: loss = 0.0680653 (* 1 = 0.0680653 loss)
I0815 23:38:33.219388 14907 sgd_solver.cpp:136] Iteration 16600, lr = 1e-05, m = 0.9
I0815 23:38:50.789521 14872 data_reader.cpp:288] Starting prefetch of epoch 14
I0815 23:38:52.739527 14907 solver.cpp:312] Iteration 16700 (5.12304 iter/s, 19.5197s/100 iter), loss = 0.0746607
I0815 23:38:52.739555 14907 solver.cpp:334]     Train net output #0: loss = 0.0746606 (* 1 = 0.0746606 loss)
I0815 23:38:52.739560 14907 sgd_solver.cpp:136] Iteration 16700, lr = 1e-05, m = 0.9
I0815 23:39:12.203140 14907 solver.cpp:312] Iteration 16800 (5.13793 iter/s, 19.4631s/100 iter), loss = 0.0784965
I0815 23:39:12.203222 14907 solver.cpp:334]     Train net output #0: loss = 0.0784964 (* 1 = 0.0784964 loss)
I0815 23:39:12.203229 14907 sgd_solver.cpp:136] Iteration 16800, lr = 1e-05, m = 0.9
I0815 23:39:23.201671 14916 data_reader.cpp:288] Starting prefetch of epoch 9
I0815 23:39:32.230306 14907 solver.cpp:312] Iteration 16900 (4.99336 iter/s, 20.0266s/100 iter), loss = 0.0630666
I0815 23:39:32.230334 14907 solver.cpp:334]     Train net output #0: loss = 0.0630665 (* 1 = 0.0630665 loss)
I0815 23:39:32.230340 14907 sgd_solver.cpp:136] Iteration 16900, lr = 1e-05, m = 0.9
I0815 23:39:51.578822 14907 solver.cpp:363] Sparsity after update:
I0815 23:39:51.582172 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0815 23:39:51.582209 14907 net.cpp:2192] conv1a_param_0(0) 
I0815 23:39:51.582226 14907 net.cpp:2192] conv1b_param_0(0) 
I0815 23:39:51.582228 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0815 23:39:51.582231 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0815 23:39:51.582234 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0815 23:39:51.582237 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0815 23:39:51.582240 14907 net.cpp:2192] ctx_final_param_0(0) 
I0815 23:39:51.582243 14907 net.cpp:2192] out3a_param_0(0) 
I0815 23:39:51.582247 14907 net.cpp:2192] out5a_param_0(0) 
I0815 23:39:51.582249 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 23:39:51.582252 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 23:39:51.582255 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 23:39:51.582258 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 23:39:51.582260 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 23:39:51.582264 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 23:39:51.582271 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 23:39:51.582274 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 23:39:51.582278 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0815 23:39:51.778899 14907 solver.cpp:312] Iteration 17000 (5.1156 iter/s, 19.5481s/100 iter), loss = 0.0951022
I0815 23:39:51.778925 14907 solver.cpp:334]     Train net output #0: loss = 0.0951021 (* 1 = 0.0951021 loss)
I0815 23:39:51.778930 14907 sgd_solver.cpp:136] Iteration 17000, lr = 1e-05, m = 0.9
I0815 23:40:11.133471 14907 solver.cpp:312] Iteration 17100 (5.16688 iter/s, 19.354s/100 iter), loss = 0.0676104
I0815 23:40:11.133492 14907 solver.cpp:334]     Train net output #0: loss = 0.0676103 (* 1 = 0.0676103 loss)
I0815 23:40:11.133496 14907 sgd_solver.cpp:136] Iteration 17100, lr = 1e-05, m = 0.9
I0815 23:40:27.751163 14920 data_reader.cpp:288] Starting prefetch of epoch 13
I0815 23:40:30.661355 14907 solver.cpp:312] Iteration 17200 (5.12102 iter/s, 19.5273s/100 iter), loss = 0.0815321
I0815 23:40:30.661381 14907 solver.cpp:334]     Train net output #0: loss = 0.081532 (* 1 = 0.081532 loss)
I0815 23:40:30.661386 14907 sgd_solver.cpp:136] Iteration 17200, lr = 1e-05, m = 0.9
I0815 23:40:50.269695 14907 solver.cpp:312] Iteration 17300 (5.10001 iter/s, 19.6078s/100 iter), loss = 0.051093
I0815 23:40:50.269722 14907 solver.cpp:334]     Train net output #0: loss = 0.0510929 (* 1 = 0.0510929 loss)
I0815 23:40:50.269727 14907 sgd_solver.cpp:136] Iteration 17300, lr = 1e-05, m = 0.9
I0815 23:41:09.890775 14907 solver.cpp:312] Iteration 17400 (5.0967 iter/s, 19.6205s/100 iter), loss = 0.0536433
I0815 23:41:09.890857 14907 solver.cpp:334]     Train net output #0: loss = 0.0536432 (* 1 = 0.0536432 loss)
I0815 23:41:09.890863 14907 sgd_solver.cpp:136] Iteration 17400, lr = 1e-05, m = 0.9
I0815 23:41:29.464340 14907 solver.cpp:312] Iteration 17500 (5.10907 iter/s, 19.573s/100 iter), loss = 0.0465695
I0815 23:41:29.464368 14907 solver.cpp:334]     Train net output #0: loss = 0.0465693 (* 1 = 0.0465693 loss)
I0815 23:41:29.464375 14907 sgd_solver.cpp:136] Iteration 17500, lr = 1e-05, m = 0.9
I0815 23:41:32.670158 14872 data_reader.cpp:288] Starting prefetch of epoch 15
I0815 23:41:49.121115 14907 solver.cpp:312] Iteration 17600 (5.08744 iter/s, 19.6562s/100 iter), loss = 0.0751181
I0815 23:41:49.121163 14907 solver.cpp:334]     Train net output #0: loss = 0.075118 (* 1 = 0.075118 loss)
I0815 23:41:49.121168 14907 sgd_solver.cpp:136] Iteration 17600, lr = 1e-05, m = 0.9
I0815 23:42:04.887600 14870 data_reader.cpp:288] Starting prefetch of epoch 11
I0815 23:42:08.574489 14907 solver.cpp:312] Iteration 17700 (5.14064 iter/s, 19.4528s/100 iter), loss = 0.0622319
I0815 23:42:08.574518 14907 solver.cpp:334]     Train net output #0: loss = 0.0622318 (* 1 = 0.0622318 loss)
I0815 23:42:08.574524 14907 sgd_solver.cpp:136] Iteration 17700, lr = 1e-05, m = 0.9
I0815 23:42:28.150519 14907 solver.cpp:312] Iteration 17800 (5.10843 iter/s, 19.5755s/100 iter), loss = 0.0590967
I0815 23:42:28.150576 14907 solver.cpp:334]     Train net output #0: loss = 0.0590966 (* 1 = 0.0590966 loss)
I0815 23:42:28.150581 14907 sgd_solver.cpp:136] Iteration 17800, lr = 1e-05, m = 0.9
I0815 23:42:47.611865 14907 solver.cpp:312] Iteration 17900 (5.13853 iter/s, 19.4608s/100 iter), loss = 0.0812782
I0815 23:42:47.611889 14907 solver.cpp:334]     Train net output #0: loss = 0.081278 (* 1 = 0.081278 loss)
I0815 23:42:47.611893 14907 sgd_solver.cpp:136] Iteration 17900, lr = 1e-05, m = 0.9
I0815 23:43:06.874465 14907 solver.cpp:363] Sparsity after update:
I0815 23:43:06.888511 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0815 23:43:06.888530 14907 net.cpp:2192] conv1a_param_0(0) 
I0815 23:43:06.888540 14907 net.cpp:2192] conv1b_param_0(0) 
I0815 23:43:06.888542 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0815 23:43:06.888545 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0815 23:43:06.888548 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0815 23:43:06.888555 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0815 23:43:06.888557 14907 net.cpp:2192] ctx_final_param_0(0) 
I0815 23:43:06.888561 14907 net.cpp:2192] out3a_param_0(0) 
I0815 23:43:06.888563 14907 net.cpp:2192] out5a_param_0(0) 
I0815 23:43:06.888566 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 23:43:06.888569 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 23:43:06.888574 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 23:43:06.888577 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 23:43:06.888581 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 23:43:06.888586 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 23:43:06.888589 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 23:43:06.888593 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 23:43:06.888597 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0815 23:43:06.888610 14907 solver.cpp:509] Iteration 18000, Testing net (#0)
I0815 23:43:15.176524 14890 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 23:43:19.871060 14907 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.952486
I0815 23:43:19.871081 14907 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999226
I0815 23:43:19.871088 14907 solver.cpp:594]     Test net output #2: loss = 0.202347 (* 1 = 0.202347 loss)
I0815 23:43:19.871116 14907 solver.cpp:264] [MultiGPU] Tests completed in 12.9821s
I0815 23:43:20.073647 14907 solver.cpp:312] Iteration 18000 (3.08063 iter/s, 32.4609s/100 iter), loss = 0.0480344
I0815 23:43:20.073668 14907 solver.cpp:334]     Train net output #0: loss = 0.0480343 (* 1 = 0.0480343 loss)
I0815 23:43:20.073674 14907 sgd_solver.cpp:136] Iteration 18000, lr = 1e-05, m = 0.9
I0815 23:43:39.949877 14907 solver.cpp:312] Iteration 18100 (5.03127 iter/s, 19.8757s/100 iter), loss = 0.0564225
I0815 23:43:39.949967 14907 solver.cpp:334]     Train net output #0: loss = 0.0564223 (* 1 = 0.0564223 loss)
I0815 23:43:39.949975 14907 sgd_solver.cpp:136] Iteration 18100, lr = 1e-05, m = 0.9
I0815 23:43:54.870635 14872 data_reader.cpp:288] Starting prefetch of epoch 16
I0815 23:43:59.437064 14907 solver.cpp:312] Iteration 18200 (5.13172 iter/s, 19.4866s/100 iter), loss = 0.0640341
I0815 23:43:59.437093 14907 solver.cpp:334]     Train net output #0: loss = 0.0640339 (* 1 = 0.0640339 loss)
I0815 23:43:59.437098 14907 sgd_solver.cpp:136] Iteration 18200, lr = 1e-05, m = 0.9
I0815 23:44:19.167204 14907 solver.cpp:312] Iteration 18300 (5.06853 iter/s, 19.7296s/100 iter), loss = 0.070324
I0815 23:44:19.167260 14907 solver.cpp:334]     Train net output #0: loss = 0.0703239 (* 1 = 0.0703239 loss)
I0815 23:44:19.167265 14907 sgd_solver.cpp:136] Iteration 18300, lr = 1e-05, m = 0.9
I0815 23:44:27.441990 14915 data_reader.cpp:288] Starting prefetch of epoch 8
I0815 23:44:38.818123 14907 solver.cpp:312] Iteration 18400 (5.08896 iter/s, 19.6504s/100 iter), loss = 0.0655307
I0815 23:44:38.818146 14907 solver.cpp:334]     Train net output #0: loss = 0.0655305 (* 1 = 0.0655305 loss)
I0815 23:44:38.818150 14907 sgd_solver.cpp:136] Iteration 18400, lr = 1e-05, m = 0.9
I0815 23:44:58.352896 14907 solver.cpp:312] Iteration 18500 (5.11922 iter/s, 19.5342s/100 iter), loss = 0.0746135
I0815 23:44:58.352946 14907 solver.cpp:334]     Train net output #0: loss = 0.0746134 (* 1 = 0.0746134 loss)
I0815 23:44:58.352952 14907 sgd_solver.cpp:136] Iteration 18500, lr = 1e-05, m = 0.9
I0815 23:45:17.941558 14907 solver.cpp:312] Iteration 18600 (5.10513 iter/s, 19.5881s/100 iter), loss = 0.0604071
I0815 23:45:17.941584 14907 solver.cpp:334]     Train net output #0: loss = 0.0604069 (* 1 = 0.0604069 loss)
I0815 23:45:17.941589 14907 sgd_solver.cpp:136] Iteration 18600, lr = 1e-05, m = 0.9
I0815 23:45:32.064332 14920 data_reader.cpp:288] Starting prefetch of epoch 14
I0815 23:45:37.276218 14907 solver.cpp:312] Iteration 18700 (5.1722 iter/s, 19.3341s/100 iter), loss = 0.0455412
I0815 23:45:37.276247 14907 solver.cpp:334]     Train net output #0: loss = 0.045541 (* 1 = 0.045541 loss)
I0815 23:45:37.276252 14907 sgd_solver.cpp:136] Iteration 18700, lr = 1e-05, m = 0.9
I0815 23:45:56.866417 14907 solver.cpp:312] Iteration 18800 (5.10473 iter/s, 19.5897s/100 iter), loss = 0.112264
I0815 23:45:56.866442 14907 solver.cpp:334]     Train net output #0: loss = 0.112263 (* 1 = 0.112263 loss)
I0815 23:45:56.866446 14907 sgd_solver.cpp:136] Iteration 18800, lr = 1e-05, m = 0.9
I0815 23:46:16.550223 14907 solver.cpp:312] Iteration 18900 (5.08046 iter/s, 19.6833s/100 iter), loss = 0.0652813
I0815 23:46:16.550343 14907 solver.cpp:334]     Train net output #0: loss = 0.0652812 (* 1 = 0.0652812 loss)
I0815 23:46:16.550350 14907 sgd_solver.cpp:136] Iteration 18900, lr = 1e-05, m = 0.9
I0815 23:46:35.724416 14907 solver.cpp:363] Sparsity after update:
I0815 23:46:35.744755 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0815 23:46:35.744786 14907 net.cpp:2192] conv1a_param_0(0) 
I0815 23:46:35.744797 14907 net.cpp:2192] conv1b_param_0(0) 
I0815 23:46:35.744801 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0815 23:46:35.744802 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0815 23:46:35.744805 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0815 23:46:35.744808 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0815 23:46:35.744810 14907 net.cpp:2192] ctx_final_param_0(0) 
I0815 23:46:35.744813 14907 net.cpp:2192] out3a_param_0(0) 
I0815 23:46:35.744817 14907 net.cpp:2192] out5a_param_0(0) 
I0815 23:46:35.744822 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 23:46:35.744825 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 23:46:35.744827 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 23:46:35.744830 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 23:46:35.744833 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 23:46:35.744837 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 23:46:35.744838 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 23:46:35.744843 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 23:46:35.744845 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0815 23:46:35.920790 14907 solver.cpp:312] Iteration 19000 (5.16261 iter/s, 19.37s/100 iter), loss = 0.0802453
I0815 23:46:35.920820 14907 solver.cpp:334]     Train net output #0: loss = 0.0802452 (* 1 = 0.0802452 loss)
I0815 23:46:35.920826 14907 sgd_solver.cpp:136] Iteration 19000, lr = 1e-05, m = 0.9
I0815 23:46:36.508731 14918 data_reader.cpp:288] Starting prefetch of epoch 10
I0815 23:46:55.322057 14907 solver.cpp:312] Iteration 19100 (5.15444 iter/s, 19.4007s/100 iter), loss = 0.0533587
I0815 23:46:55.322110 14907 solver.cpp:334]     Train net output #0: loss = 0.0533586 (* 1 = 0.0533586 loss)
I0815 23:46:55.322115 14907 sgd_solver.cpp:136] Iteration 19100, lr = 1e-05, m = 0.9
I0815 23:47:08.772284 14916 data_reader.cpp:288] Starting prefetch of epoch 10
I0815 23:47:14.774088 14907 solver.cpp:312] Iteration 19200 (5.14099 iter/s, 19.4515s/100 iter), loss = 0.0815216
I0815 23:47:14.774113 14907 solver.cpp:334]     Train net output #0: loss = 0.0815215 (* 1 = 0.0815215 loss)
I0815 23:47:14.774119 14907 sgd_solver.cpp:136] Iteration 19200, lr = 1e-05, m = 0.9
I0815 23:47:34.348359 14907 solver.cpp:312] Iteration 19300 (5.10889 iter/s, 19.5737s/100 iter), loss = 0.0875267
I0815 23:47:34.348414 14907 solver.cpp:334]     Train net output #0: loss = 0.0875266 (* 1 = 0.0875266 loss)
I0815 23:47:34.348422 14907 sgd_solver.cpp:136] Iteration 19300, lr = 1e-05, m = 0.9
I0815 23:47:56.267796 14907 solver.cpp:312] Iteration 19400 (4.56228 iter/s, 21.9188s/100 iter), loss = 0.0917167
I0815 23:47:56.267818 14907 solver.cpp:334]     Train net output #0: loss = 0.0917166 (* 1 = 0.0917166 loss)
I0815 23:47:56.267822 14907 sgd_solver.cpp:136] Iteration 19400, lr = 1e-05, m = 0.9
I0815 23:48:16.987520 14872 data_reader.cpp:288] Starting prefetch of epoch 17
I0815 23:48:17.136850 14907 solver.cpp:312] Iteration 19500 (4.79192 iter/s, 20.8685s/100 iter), loss = 0.071186
I0815 23:48:17.136878 14907 solver.cpp:334]     Train net output #0: loss = 0.0711858 (* 1 = 0.0711858 loss)
I0815 23:48:17.136883 14907 sgd_solver.cpp:136] Iteration 19500, lr = 1e-05, m = 0.9
I0815 23:48:37.187561 14907 solver.cpp:312] Iteration 19600 (4.98749 iter/s, 20.0502s/100 iter), loss = 0.0805104
I0815 23:48:37.187593 14907 solver.cpp:334]     Train net output #0: loss = 0.0805103 (* 1 = 0.0805103 loss)
I0815 23:48:37.187600 14907 sgd_solver.cpp:136] Iteration 19600, lr = 1e-05, m = 0.9
I0815 23:48:56.949928 14907 solver.cpp:312] Iteration 19700 (5.06026 iter/s, 19.7618s/100 iter), loss = 0.373376
I0815 23:48:56.949988 14907 solver.cpp:334]     Train net output #0: loss = 0.373376 (* 1 = 0.373376 loss)
I0815 23:48:56.949995 14907 sgd_solver.cpp:136] Iteration 19700, lr = 1e-05, m = 0.9
I0815 23:49:16.817250 14907 solver.cpp:312] Iteration 19800 (5.03353 iter/s, 19.8668s/100 iter), loss = 0.0672699
I0815 23:49:16.817275 14907 solver.cpp:334]     Train net output #0: loss = 0.0672698 (* 1 = 0.0672698 loss)
I0815 23:49:16.817278 14907 sgd_solver.cpp:136] Iteration 19800, lr = 1e-05, m = 0.9
I0815 23:49:22.731447 14920 data_reader.cpp:288] Starting prefetch of epoch 15
I0815 23:49:36.493877 14907 solver.cpp:312] Iteration 19900 (5.08231 iter/s, 19.6761s/100 iter), loss = 0.0644941
I0815 23:49:36.493929 14907 solver.cpp:334]     Train net output #0: loss = 0.0644939 (* 1 = 0.0644939 loss)
I0815 23:49:36.493934 14907 sgd_solver.cpp:136] Iteration 19900, lr = 1e-05, m = 0.9
I0815 23:49:54.862450 14916 data_reader.cpp:288] Starting prefetch of epoch 11
I0815 23:49:55.561434 14907 solver.cpp:639] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/sparse/cityscapes5_jsegnet21v2_iter_20000.caffemodel
I0815 23:49:55.582094 14907 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/sparse/cityscapes5_jsegnet21v2_iter_20000.solverstate
I0815 23:49:55.590448 14907 solver.cpp:363] Sparsity after update:
I0815 23:49:55.601428 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0815 23:49:55.601474 14907 net.cpp:2192] conv1a_param_0(0) 
I0815 23:49:55.601485 14907 net.cpp:2192] conv1b_param_0(0) 
I0815 23:49:55.601493 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0815 23:49:55.601500 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0815 23:49:55.601507 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0815 23:49:55.601514 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0815 23:49:55.601521 14907 net.cpp:2192] ctx_final_param_0(0) 
I0815 23:49:55.601528 14907 net.cpp:2192] out3a_param_0(0) 
I0815 23:49:55.601536 14907 net.cpp:2192] out5a_param_0(0) 
I0815 23:49:55.601542 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 23:49:55.601549 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 23:49:55.601557 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 23:49:55.601563 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 23:49:55.601570 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 23:49:55.601577 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 23:49:55.601584 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 23:49:55.601593 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 23:49:55.601601 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0815 23:49:55.601620 14907 solver.cpp:509] Iteration 20000, Testing net (#0)
I0815 23:50:07.146203 14942 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 23:50:07.605367 14907 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.951085
I0815 23:50:07.605387 14907 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999951
I0815 23:50:07.605394 14907 solver.cpp:594]     Test net output #2: loss = 0.167501 (* 1 = 0.167501 loss)
I0815 23:50:07.605428 14907 solver.cpp:264] [MultiGPU] Tests completed in 12.0035s
I0815 23:50:07.817020 14907 solver.cpp:312] Iteration 20000 (3.19261 iter/s, 31.3223s/100 iter), loss = 0.0470906
I0815 23:50:07.817044 14907 solver.cpp:334]     Train net output #0: loss = 0.0470905 (* 1 = 0.0470905 loss)
I0815 23:50:07.817047 14907 sgd_solver.cpp:136] Iteration 20000, lr = 1e-05, m = 0.9
I0815 23:50:27.355743 14907 solver.cpp:312] Iteration 20100 (5.11818 iter/s, 19.5382s/100 iter), loss = 0.0418307
I0815 23:50:27.355770 14907 solver.cpp:334]     Train net output #0: loss = 0.0418306 (* 1 = 0.0418306 loss)
I0815 23:50:27.355774 14907 sgd_solver.cpp:136] Iteration 20100, lr = 1e-05, m = 0.9
I0815 23:50:46.760087 14907 solver.cpp:312] Iteration 20200 (5.15363 iter/s, 19.4038s/100 iter), loss = 0.0870871
I0815 23:50:46.760154 14907 solver.cpp:334]     Train net output #0: loss = 0.087087 (* 1 = 0.087087 loss)
I0815 23:50:46.760160 14907 sgd_solver.cpp:136] Iteration 20200, lr = 1e-05, m = 0.9
I0815 23:51:06.246397 14907 solver.cpp:312] Iteration 20300 (5.13195 iter/s, 19.4858s/100 iter), loss = 0.0619633
I0815 23:51:06.246423 14907 solver.cpp:334]     Train net output #0: loss = 0.0619632 (* 1 = 0.0619632 loss)
I0815 23:51:06.246429 14907 sgd_solver.cpp:136] Iteration 20300, lr = 1e-05, m = 0.9
I0815 23:51:11.357270 14915 data_reader.cpp:288] Starting prefetch of epoch 9
I0815 23:51:25.894989 14907 solver.cpp:312] Iteration 20400 (5.08956 iter/s, 19.648s/100 iter), loss = 0.0683923
I0815 23:51:25.895035 14907 solver.cpp:334]     Train net output #0: loss = 0.0683922 (* 1 = 0.0683922 loss)
I0815 23:51:25.895651 14907 sgd_solver.cpp:136] Iteration 20400, lr = 1e-05, m = 0.9
I0815 23:51:43.816572 14870 data_reader.cpp:288] Starting prefetch of epoch 12
I0815 23:51:45.542307 14907 solver.cpp:312] Iteration 20500 (5.08989 iter/s, 19.6468s/100 iter), loss = 0.0573736
I0815 23:51:45.542340 14907 solver.cpp:334]     Train net output #0: loss = 0.0573735 (* 1 = 0.0573735 loss)
I0815 23:51:45.542369 14907 sgd_solver.cpp:136] Iteration 20500, lr = 1e-05, m = 0.9
I0815 23:52:05.147907 14907 solver.cpp:312] Iteration 20600 (5.10072 iter/s, 19.6051s/100 iter), loss = 0.0546356
I0815 23:52:05.147956 14907 solver.cpp:334]     Train net output #0: loss = 0.0546355 (* 1 = 0.0546355 loss)
I0815 23:52:05.147961 14907 sgd_solver.cpp:136] Iteration 20600, lr = 1e-05, m = 0.9
I0815 23:52:24.700646 14907 solver.cpp:312] Iteration 20700 (5.11451 iter/s, 19.5522s/100 iter), loss = 0.0457091
I0815 23:52:24.700670 14907 solver.cpp:334]     Train net output #0: loss = 0.045709 (* 1 = 0.045709 loss)
I0815 23:52:24.700673 14907 sgd_solver.cpp:136] Iteration 20700, lr = 1e-05, m = 0.9
I0815 23:52:44.200448 14907 solver.cpp:312] Iteration 20800 (5.1284 iter/s, 19.4993s/100 iter), loss = 0.0406756
I0815 23:52:44.200505 14907 solver.cpp:334]     Train net output #0: loss = 0.0406755 (* 1 = 0.0406755 loss)
I0815 23:52:44.200510 14907 sgd_solver.cpp:136] Iteration 20800, lr = 1e-05, m = 0.9
I0815 23:52:48.308429 14916 data_reader.cpp:288] Starting prefetch of epoch 12
I0815 23:53:03.657646 14907 solver.cpp:312] Iteration 20900 (5.13963 iter/s, 19.4567s/100 iter), loss = 0.0486119
I0815 23:53:03.657675 14907 solver.cpp:334]     Train net output #0: loss = 0.0486118 (* 1 = 0.0486118 loss)
I0815 23:53:03.657680 14907 sgd_solver.cpp:136] Iteration 20900, lr = 1e-05, m = 0.9
I0815 23:53:22.973522 14907 solver.cpp:363] Sparsity after update:
I0815 23:53:22.978873 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0815 23:53:22.978914 14907 net.cpp:2192] conv1a_param_0(0) 
I0815 23:53:22.978929 14907 net.cpp:2192] conv1b_param_0(0) 
I0815 23:53:22.978931 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0815 23:53:22.978935 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0815 23:53:22.978936 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0815 23:53:22.978940 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0815 23:53:22.978942 14907 net.cpp:2192] ctx_final_param_0(0) 
I0815 23:53:22.978945 14907 net.cpp:2192] out3a_param_0(0) 
I0815 23:53:22.978947 14907 net.cpp:2192] out5a_param_0(0) 
I0815 23:53:22.978950 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 23:53:22.978953 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 23:53:22.978955 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 23:53:22.978958 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 23:53:22.978961 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 23:53:22.978965 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 23:53:22.978966 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 23:53:22.978970 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 23:53:22.978972 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0815 23:53:23.186875 14907 solver.cpp:312] Iteration 21000 (5.12067 iter/s, 19.5287s/100 iter), loss = 0.073563
I0815 23:53:23.186902 14907 solver.cpp:334]     Train net output #0: loss = 0.0735629 (* 1 = 0.0735629 loss)
I0815 23:53:23.186908 14907 sgd_solver.cpp:136] Iteration 21000, lr = 1e-05, m = 0.9
I0815 23:53:42.880239 14907 solver.cpp:312] Iteration 21100 (5.07799 iter/s, 19.6928s/100 iter), loss = 0.0546189
I0815 23:53:42.880262 14907 solver.cpp:334]     Train net output #0: loss = 0.0546187 (* 1 = 0.0546187 loss)
I0815 23:53:42.880269 14907 sgd_solver.cpp:136] Iteration 21100, lr = 1e-05, m = 0.9
I0815 23:53:53.167446 14920 data_reader.cpp:288] Starting prefetch of epoch 16
I0815 23:54:02.527252 14907 solver.cpp:312] Iteration 21200 (5.08997 iter/s, 19.6465s/100 iter), loss = 0.0613557
I0815 23:54:02.527279 14907 solver.cpp:334]     Train net output #0: loss = 0.0613556 (* 1 = 0.0613556 loss)
I0815 23:54:02.527287 14907 sgd_solver.cpp:136] Iteration 21200, lr = 1e-05, m = 0.9
I0815 23:54:22.275456 14907 solver.cpp:312] Iteration 21300 (5.06389 iter/s, 19.7477s/100 iter), loss = 0.0770313
I0815 23:54:22.275481 14907 solver.cpp:334]     Train net output #0: loss = 0.0770311 (* 1 = 0.0770311 loss)
I0815 23:54:22.275487 14907 sgd_solver.cpp:136] Iteration 21300, lr = 1e-05, m = 0.9
I0815 23:54:25.668941 14916 data_reader.cpp:288] Starting prefetch of epoch 13
I0815 23:54:41.872234 14907 solver.cpp:312] Iteration 21400 (5.10302 iter/s, 19.5962s/100 iter), loss = 0.0566219
I0815 23:54:41.872263 14907 solver.cpp:334]     Train net output #0: loss = 0.0566218 (* 1 = 0.0566218 loss)
I0815 23:54:41.872270 14907 sgd_solver.cpp:136] Iteration 21400, lr = 1e-05, m = 0.9
I0815 23:55:01.509882 14907 solver.cpp:312] Iteration 21500 (5.0924 iter/s, 19.6371s/100 iter), loss = 0.0473402
I0815 23:55:01.509979 14907 solver.cpp:334]     Train net output #0: loss = 0.0473401 (* 1 = 0.0473401 loss)
I0815 23:55:01.509986 14907 sgd_solver.cpp:136] Iteration 21500, lr = 1e-05, m = 0.9
I0815 23:55:21.145774 14907 solver.cpp:312] Iteration 21600 (5.09285 iter/s, 19.6354s/100 iter), loss = 0.0743124
I0815 23:55:21.145800 14907 solver.cpp:334]     Train net output #0: loss = 0.0743122 (* 1 = 0.0743122 loss)
I0815 23:55:21.145807 14907 sgd_solver.cpp:136] Iteration 21600, lr = 1e-05, m = 0.9
I0815 23:55:30.473955 14870 data_reader.cpp:288] Starting prefetch of epoch 13
I0815 23:55:40.459172 14907 solver.cpp:312] Iteration 21700 (5.1779 iter/s, 19.3129s/100 iter), loss = 0.0564789
I0815 23:55:40.459254 14907 solver.cpp:334]     Train net output #0: loss = 0.0564788 (* 1 = 0.0564788 loss)
I0815 23:55:40.459261 14907 sgd_solver.cpp:136] Iteration 21700, lr = 1e-05, m = 0.9
I0815 23:56:00.035748 14907 solver.cpp:312] Iteration 21800 (5.10829 iter/s, 19.576s/100 iter), loss = 0.0597349
I0815 23:56:00.035810 14907 solver.cpp:334]     Train net output #0: loss = 0.0597348 (* 1 = 0.0597348 loss)
I0815 23:56:00.035827 14907 sgd_solver.cpp:136] Iteration 21800, lr = 1e-05, m = 0.9
I0815 23:56:19.737989 14907 solver.cpp:312] Iteration 21900 (5.0757 iter/s, 19.7017s/100 iter), loss = 0.0650271
I0815 23:56:19.738087 14907 solver.cpp:334]     Train net output #0: loss = 0.0650269 (* 1 = 0.0650269 loss)
I0815 23:56:19.738095 14907 sgd_solver.cpp:136] Iteration 21900, lr = 1e-05, m = 0.9
I0815 23:56:35.157990 14872 data_reader.cpp:288] Starting prefetch of epoch 18
I0815 23:56:39.328775 14907 solver.cpp:363] Sparsity after update:
I0815 23:56:39.331538 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0815 23:56:39.331576 14907 net.cpp:2192] conv1a_param_0(0) 
I0815 23:56:39.331588 14907 net.cpp:2192] conv1b_param_0(0) 
I0815 23:56:39.331591 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0815 23:56:39.331594 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0815 23:56:39.331598 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0815 23:56:39.331600 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0815 23:56:39.331603 14907 net.cpp:2192] ctx_final_param_0(0) 
I0815 23:56:39.331605 14907 net.cpp:2192] out3a_param_0(0) 
I0815 23:56:39.331609 14907 net.cpp:2192] out5a_param_0(0) 
I0815 23:56:39.331610 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 23:56:39.331614 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 23:56:39.331616 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 23:56:39.331619 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 23:56:39.331621 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 23:56:39.331624 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 23:56:39.331626 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 23:56:39.331629 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 23:56:39.331632 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0815 23:56:39.331645 14907 solver.cpp:509] Iteration 22000, Testing net (#0)
I0815 23:56:47.201822 14942 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 23:56:51.513540 14907 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.953033
I0815 23:56:51.513661 14907 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999359
I0815 23:56:51.513674 14907 solver.cpp:594]     Test net output #2: loss = 0.195634 (* 1 = 0.195634 loss)
I0815 23:56:51.513698 14907 solver.cpp:264] [MultiGPU] Tests completed in 12.1817s
I0815 23:56:51.733094 14907 solver.cpp:312] Iteration 22000 (3.12556 iter/s, 31.9942s/100 iter), loss = 0.0793808
I0815 23:56:51.733119 14907 solver.cpp:334]     Train net output #0: loss = 0.0793807 (* 1 = 0.0793807 loss)
I0815 23:56:51.733124 14907 sgd_solver.cpp:136] Iteration 22000, lr = 1e-05, m = 0.9
I0815 23:57:11.341295 14907 solver.cpp:312] Iteration 22100 (5.10005 iter/s, 19.6077s/100 iter), loss = 0.0533418
I0815 23:57:11.341321 14907 solver.cpp:334]     Train net output #0: loss = 0.0533417 (* 1 = 0.0533417 loss)
I0815 23:57:11.341326 14907 sgd_solver.cpp:136] Iteration 22100, lr = 1e-05, m = 0.9
I0815 23:57:30.695421 14907 solver.cpp:312] Iteration 22200 (5.167 iter/s, 19.3536s/100 iter), loss = 0.0714366
I0815 23:57:30.695472 14907 solver.cpp:334]     Train net output #0: loss = 0.0714365 (* 1 = 0.0714365 loss)
I0815 23:57:30.695477 14907 sgd_solver.cpp:136] Iteration 22200, lr = 1e-05, m = 0.9
I0815 23:57:50.161085 14907 solver.cpp:312] Iteration 22300 (5.1374 iter/s, 19.4651s/100 iter), loss = 0.0450926
I0815 23:57:50.161145 14907 solver.cpp:334]     Train net output #0: loss = 0.0450924 (* 1 = 0.0450924 loss)
I0815 23:57:50.161159 14907 sgd_solver.cpp:136] Iteration 22300, lr = 1e-05, m = 0.9
I0815 23:57:51.924901 14870 data_reader.cpp:288] Starting prefetch of epoch 14
I0815 23:58:09.702455 14907 solver.cpp:312] Iteration 22400 (5.11749 iter/s, 19.5408s/100 iter), loss = 0.0477054
I0815 23:58:09.702538 14907 solver.cpp:334]     Train net output #0: loss = 0.0477053 (* 1 = 0.0477053 loss)
I0815 23:58:09.702545 14907 sgd_solver.cpp:136] Iteration 22400, lr = 1e-05, m = 0.9
I0815 23:58:29.210021 14907 solver.cpp:312] Iteration 22500 (5.12636 iter/s, 19.507s/100 iter), loss = 0.0658046
I0815 23:58:29.210047 14907 solver.cpp:334]     Train net output #0: loss = 0.0658045 (* 1 = 0.0658045 loss)
I0815 23:58:29.210052 14907 sgd_solver.cpp:136] Iteration 22500, lr = 1e-05, m = 0.9
I0815 23:58:49.048873 14907 solver.cpp:312] Iteration 22600 (5.04075 iter/s, 19.8383s/100 iter), loss = 0.0439003
I0815 23:58:49.048945 14907 solver.cpp:334]     Train net output #0: loss = 0.0439002 (* 1 = 0.0439002 loss)
I0815 23:58:49.048950 14907 sgd_solver.cpp:136] Iteration 22600, lr = 1e-05, m = 0.9
I0815 23:58:56.923501 14872 data_reader.cpp:288] Starting prefetch of epoch 19
I0815 23:59:08.686321 14907 solver.cpp:312] Iteration 22700 (5.09245 iter/s, 19.6369s/100 iter), loss = 0.0712108
I0815 23:59:08.686349 14907 solver.cpp:334]     Train net output #0: loss = 0.0712107 (* 1 = 0.0712107 loss)
I0815 23:59:08.686357 14907 sgd_solver.cpp:136] Iteration 22700, lr = 1e-05, m = 0.9
I0815 23:59:28.223240 14907 solver.cpp:312] Iteration 22800 (5.11866 iter/s, 19.5364s/100 iter), loss = 0.0414534
I0815 23:59:28.223364 14907 solver.cpp:334]     Train net output #0: loss = 0.0414533 (* 1 = 0.0414533 loss)
I0815 23:59:28.223371 14907 sgd_solver.cpp:136] Iteration 22800, lr = 1e-05, m = 0.9
I0815 23:59:29.269711 14915 data_reader.cpp:288] Starting prefetch of epoch 10
I0815 23:59:47.747740 14907 solver.cpp:312] Iteration 22900 (5.12191 iter/s, 19.524s/100 iter), loss = 0.0685426
I0815 23:59:47.747794 14907 solver.cpp:334]     Train net output #0: loss = 0.0685425 (* 1 = 0.0685425 loss)
I0815 23:59:47.747809 14907 sgd_solver.cpp:136] Iteration 22900, lr = 1e-05, m = 0.9
I0816 00:00:07.189261 14907 solver.cpp:363] Sparsity after update:
I0816 00:00:07.214704 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0816 00:00:07.214764 14907 net.cpp:2192] conv1a_param_0(0) 
I0816 00:00:07.214778 14907 net.cpp:2192] conv1b_param_0(0) 
I0816 00:00:07.214781 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0816 00:00:07.214784 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0816 00:00:07.214787 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0816 00:00:07.214789 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0816 00:00:07.214792 14907 net.cpp:2192] ctx_final_param_0(0) 
I0816 00:00:07.214795 14907 net.cpp:2192] out3a_param_0(0) 
I0816 00:00:07.214798 14907 net.cpp:2192] out5a_param_0(0) 
I0816 00:00:07.214804 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0816 00:00:07.214807 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0816 00:00:07.214810 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0816 00:00:07.214813 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0816 00:00:07.214818 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0816 00:00:07.214819 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0816 00:00:07.214823 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0816 00:00:07.214825 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0816 00:00:07.214828 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0816 00:00:07.401695 14907 solver.cpp:312] Iteration 23000 (5.08817 iter/s, 19.6534s/100 iter), loss = 0.0848114
I0816 00:00:07.401717 14907 solver.cpp:334]     Train net output #0: loss = 0.0848113 (* 1 = 0.0848113 loss)
I0816 00:00:07.401722 14907 sgd_solver.cpp:136] Iteration 23000, lr = 1e-05, m = 0.9
I0816 00:00:27.302656 14907 solver.cpp:312] Iteration 23100 (5.02502 iter/s, 19.9004s/100 iter), loss = 0.0521976
I0816 00:00:27.302681 14907 solver.cpp:334]     Train net output #0: loss = 0.0521975 (* 1 = 0.0521975 loss)
I0816 00:00:27.302685 14907 sgd_solver.cpp:136] Iteration 23100, lr = 1e-05, m = 0.9
I0816 00:00:34.416801 14918 data_reader.cpp:288] Starting prefetch of epoch 11
I0816 00:00:47.030138 14907 solver.cpp:312] Iteration 23200 (5.06921 iter/s, 19.7269s/100 iter), loss = 0.0476554
I0816 00:00:47.030211 14907 solver.cpp:334]     Train net output #0: loss = 0.0476553 (* 1 = 0.0476553 loss)
I0816 00:00:47.030217 14907 sgd_solver.cpp:136] Iteration 23200, lr = 1e-05, m = 0.9
I0816 00:01:06.613134 14907 solver.cpp:312] Iteration 23300 (5.10661 iter/s, 19.5825s/100 iter), loss = 0.0394216
I0816 00:01:06.613188 14907 solver.cpp:334]     Train net output #0: loss = 0.0394215 (* 1 = 0.0394215 loss)
I0816 00:01:06.613200 14907 sgd_solver.cpp:136] Iteration 23300, lr = 1e-05, m = 0.9
I0816 00:01:26.184684 14907 solver.cpp:312] Iteration 23400 (5.1096 iter/s, 19.571s/100 iter), loss = 0.0516496
I0816 00:01:26.184743 14907 solver.cpp:334]     Train net output #0: loss = 0.0516495 (* 1 = 0.0516495 loss)
I0816 00:01:26.184749 14907 sgd_solver.cpp:136] Iteration 23400, lr = 1e-05, m = 0.9
I0816 00:01:39.124276 14918 data_reader.cpp:288] Starting prefetch of epoch 12
I0816 00:01:45.708436 14907 solver.cpp:312] Iteration 23500 (5.12211 iter/s, 19.5232s/100 iter), loss = 0.192318
I0816 00:01:45.708461 14907 solver.cpp:334]     Train net output #0: loss = 0.192318 (* 1 = 0.192318 loss)
I0816 00:01:45.708465 14907 sgd_solver.cpp:136] Iteration 23500, lr = 1e-05, m = 0.9
I0816 00:02:05.438953 14907 solver.cpp:312] Iteration 23600 (5.06843 iter/s, 19.73s/100 iter), loss = 0.0414409
I0816 00:02:05.439055 14907 solver.cpp:334]     Train net output #0: loss = 0.0414408 (* 1 = 0.0414408 loss)
I0816 00:02:05.439069 14907 sgd_solver.cpp:136] Iteration 23600, lr = 1e-05, m = 0.9
I0816 00:02:11.577553 14916 data_reader.cpp:288] Starting prefetch of epoch 14
I0816 00:02:25.074669 14907 solver.cpp:312] Iteration 23700 (5.0929 iter/s, 19.6352s/100 iter), loss = 0.0620911
I0816 00:02:25.074697 14907 solver.cpp:334]     Train net output #0: loss = 0.062091 (* 1 = 0.062091 loss)
I0816 00:02:25.074702 14907 sgd_solver.cpp:136] Iteration 23700, lr = 1e-05, m = 0.9
I0816 00:02:44.692782 14907 solver.cpp:312] Iteration 23800 (5.09747 iter/s, 19.6176s/100 iter), loss = 0.0884648
I0816 00:02:44.692881 14907 solver.cpp:334]     Train net output #0: loss = 0.0884647 (* 1 = 0.0884647 loss)
I0816 00:02:44.692901 14907 sgd_solver.cpp:136] Iteration 23800, lr = 1e-05, m = 0.9
I0816 00:03:04.754312 14907 solver.cpp:312] Iteration 23900 (4.9848 iter/s, 20.061s/100 iter), loss = 0.0586731
I0816 00:03:04.754340 14907 solver.cpp:334]     Train net output #0: loss = 0.058673 (* 1 = 0.058673 loss)
I0816 00:03:04.754346 14907 sgd_solver.cpp:136] Iteration 23900, lr = 1e-05, m = 0.9
I0816 00:03:16.898092 14870 data_reader.cpp:288] Starting prefetch of epoch 15
I0816 00:03:24.141130 14907 solver.cpp:363] Sparsity after update:
I0816 00:03:24.151890 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0816 00:03:24.151908 14907 net.cpp:2192] conv1a_param_0(0) 
I0816 00:03:24.151916 14907 net.cpp:2192] conv1b_param_0(0) 
I0816 00:03:24.151919 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0816 00:03:24.151922 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0816 00:03:24.151924 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0816 00:03:24.151927 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0816 00:03:24.151932 14907 net.cpp:2192] ctx_final_param_0(0) 
I0816 00:03:24.151937 14907 net.cpp:2192] out3a_param_0(0) 
I0816 00:03:24.151940 14907 net.cpp:2192] out5a_param_0(0) 
I0816 00:03:24.151943 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0816 00:03:24.151947 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0816 00:03:24.151952 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0816 00:03:24.151957 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0816 00:03:24.151959 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0816 00:03:24.151963 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0816 00:03:24.151968 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0816 00:03:24.151970 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0816 00:03:24.151974 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0816 00:03:24.151988 14907 solver.cpp:509] Iteration 24000, Testing net (#0)
I0816 00:03:35.324319 14888 data_reader.cpp:288] Starting prefetch of epoch 2
I0816 00:03:36.222295 14907 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.95118
I0816 00:03:36.222319 14907 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999949
I0816 00:03:36.222326 14907 solver.cpp:594]     Test net output #2: loss = 0.165574 (* 1 = 0.165574 loss)
I0816 00:03:36.222352 14907 solver.cpp:264] [MultiGPU] Tests completed in 12.07s
I0816 00:03:36.319121 14962 sgd_solver.cpp:48] MultiStep Status: Iteration 24000, step = 1
I0816 00:03:36.319182 14961 sgd_solver.cpp:48] MultiStep Status: Iteration 24000, step = 1
I0816 00:03:36.319206 14960 sgd_solver.cpp:48] MultiStep Status: Iteration 24000, step = 1
I0816 00:03:36.424865 14907 solver.cpp:312] Iteration 24000 (3.15759 iter/s, 31.6697s/100 iter), loss = 0.0784022
I0816 00:03:36.424890 14907 solver.cpp:334]     Train net output #0: loss = 0.0784021 (* 1 = 0.0784021 loss)
I0816 00:03:36.424896 14907 sgd_solver.cpp:136] Iteration 24000, lr = 1e-06, m = 0.9
I0816 00:03:55.882346 14907 solver.cpp:312] Iteration 24100 (5.13955 iter/s, 19.4569s/100 iter), loss = 0.0666187
I0816 00:03:55.882412 14907 solver.cpp:334]     Train net output #0: loss = 0.0666185 (* 1 = 0.0666185 loss)
I0816 00:03:55.882417 14907 sgd_solver.cpp:136] Iteration 24100, lr = 1e-06, m = 0.9
I0816 00:04:01.147586 14916 data_reader.cpp:288] Starting prefetch of epoch 15
I0816 00:04:15.434377 14907 solver.cpp:312] Iteration 24200 (5.1147 iter/s, 19.5515s/100 iter), loss = 0.0800891
I0816 00:04:15.434403 14907 solver.cpp:334]     Train net output #0: loss = 0.080089 (* 1 = 0.080089 loss)
I0816 00:04:15.434407 14907 sgd_solver.cpp:136] Iteration 24200, lr = 1e-06, m = 0.9
I0816 00:04:35.086583 14907 solver.cpp:312] Iteration 24300 (5.08863 iter/s, 19.6517s/100 iter), loss = 0.0538179
I0816 00:04:35.086848 14907 solver.cpp:334]     Train net output #0: loss = 0.0538178 (* 1 = 0.0538178 loss)
I0816 00:04:35.086854 14907 sgd_solver.cpp:136] Iteration 24300, lr = 1e-06, m = 0.9
I0816 00:04:54.695070 14907 solver.cpp:312] Iteration 24400 (5.09997 iter/s, 19.6079s/100 iter), loss = 0.0710455
I0816 00:04:54.695092 14907 solver.cpp:334]     Train net output #0: loss = 0.0710454 (* 1 = 0.0710454 loss)
I0816 00:04:54.695098 14907 sgd_solver.cpp:136] Iteration 24400, lr = 1e-06, m = 0.9
I0816 00:05:05.814119 14870 data_reader.cpp:288] Starting prefetch of epoch 16
I0816 00:05:13.902207 14907 solver.cpp:312] Iteration 24500 (5.20654 iter/s, 19.2066s/100 iter), loss = 0.0540409
I0816 00:05:13.902232 14907 solver.cpp:334]     Train net output #0: loss = 0.0540408 (* 1 = 0.0540408 loss)
I0816 00:05:13.902236 14907 sgd_solver.cpp:136] Iteration 24500, lr = 1e-06, m = 0.9
I0816 00:05:33.435394 14907 solver.cpp:312] Iteration 24600 (5.11963 iter/s, 19.5327s/100 iter), loss = 0.0912744
I0816 00:05:33.435417 14907 solver.cpp:334]     Train net output #0: loss = 0.0912743 (* 1 = 0.0912743 loss)
I0816 00:05:33.435422 14907 sgd_solver.cpp:136] Iteration 24600, lr = 1e-06, m = 0.9
I0816 00:05:53.158977 14907 solver.cpp:312] Iteration 24700 (5.07021 iter/s, 19.723s/100 iter), loss = 0.0676383
I0816 00:05:53.159029 14907 solver.cpp:334]     Train net output #0: loss = 0.0676382 (* 1 = 0.0676382 loss)
I0816 00:05:53.159034 14907 sgd_solver.cpp:136] Iteration 24700, lr = 1e-06, m = 0.9
I0816 00:06:10.501845 14918 data_reader.cpp:288] Starting prefetch of epoch 13
I0816 00:06:12.822973 14907 solver.cpp:312] Iteration 24800 (5.08558 iter/s, 19.6635s/100 iter), loss = 0.0885494
I0816 00:06:12.822996 14907 solver.cpp:334]     Train net output #0: loss = 0.0885493 (* 1 = 0.0885493 loss)
I0816 00:06:12.823001 14907 sgd_solver.cpp:136] Iteration 24800, lr = 1e-06, m = 0.9
I0816 00:06:32.218603 14907 solver.cpp:312] Iteration 24900 (5.15594 iter/s, 19.3951s/100 iter), loss = 0.0614068
I0816 00:06:32.218655 14907 solver.cpp:334]     Train net output #0: loss = 0.0614067 (* 1 = 0.0614067 loss)
I0816 00:06:32.218662 14907 sgd_solver.cpp:136] Iteration 24900, lr = 1e-06, m = 0.9
I0816 00:06:42.681551 14916 data_reader.cpp:288] Starting prefetch of epoch 16
I0816 00:06:51.545001 14907 solver.cpp:363] Sparsity after update:
I0816 00:06:51.579069 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0816 00:06:51.579090 14907 net.cpp:2192] conv1a_param_0(0) 
I0816 00:06:51.579097 14907 net.cpp:2192] conv1b_param_0(0) 
I0816 00:06:51.579099 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0816 00:06:51.579102 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0816 00:06:51.579102 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0816 00:06:51.579104 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0816 00:06:51.579107 14907 net.cpp:2192] ctx_final_param_0(0) 
I0816 00:06:51.579108 14907 net.cpp:2192] out3a_param_0(0) 
I0816 00:06:51.579110 14907 net.cpp:2192] out5a_param_0(0) 
I0816 00:06:51.579113 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0816 00:06:51.579113 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0816 00:06:51.579115 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0816 00:06:51.579118 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0816 00:06:51.579123 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0816 00:06:51.579126 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0816 00:06:51.579129 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0816 00:06:51.579133 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0816 00:06:51.579135 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0816 00:06:51.752192 14907 solver.cpp:312] Iteration 25000 (5.11953 iter/s, 19.5331s/100 iter), loss = 0.0695945
I0816 00:06:51.752218 14907 solver.cpp:334]     Train net output #0: loss = 0.0695944 (* 1 = 0.0695944 loss)
I0816 00:06:51.752224 14907 sgd_solver.cpp:136] Iteration 25000, lr = 1e-06, m = 0.9
I0816 00:07:11.468677 14907 solver.cpp:312] Iteration 25100 (5.07204 iter/s, 19.7159s/100 iter), loss = 0.0561501
I0816 00:07:11.468768 14907 solver.cpp:334]     Train net output #0: loss = 0.05615 (* 1 = 0.05615 loss)
I0816 00:07:11.468776 14907 sgd_solver.cpp:136] Iteration 25100, lr = 1e-06, m = 0.9
I0816 00:07:30.966171 14907 solver.cpp:312] Iteration 25200 (5.129 iter/s, 19.497s/100 iter), loss = 0.0804161
I0816 00:07:30.966198 14907 solver.cpp:334]     Train net output #0: loss = 0.080416 (* 1 = 0.080416 loss)
I0816 00:07:30.966204 14907 sgd_solver.cpp:136] Iteration 25200, lr = 1e-06, m = 0.9
I0816 00:07:47.416268 14915 data_reader.cpp:288] Starting prefetch of epoch 11
I0816 00:07:50.545424 14907 solver.cpp:312] Iteration 25300 (5.10759 iter/s, 19.5787s/100 iter), loss = 0.075048
I0816 00:07:50.545450 14907 solver.cpp:334]     Train net output #0: loss = 0.0750479 (* 1 = 0.0750479 loss)
I0816 00:07:50.545455 14907 sgd_solver.cpp:136] Iteration 25300, lr = 1e-06, m = 0.9
I0816 00:08:10.232336 14907 solver.cpp:312] Iteration 25400 (5.07966 iter/s, 19.6864s/100 iter), loss = 0.0658098
I0816 00:08:10.232362 14907 solver.cpp:334]     Train net output #0: loss = 0.0658097 (* 1 = 0.0658097 loss)
I0816 00:08:10.232367 14907 sgd_solver.cpp:136] Iteration 25400, lr = 1e-06, m = 0.9
I0816 00:08:29.864226 14907 solver.cpp:312] Iteration 25500 (5.09389 iter/s, 19.6314s/100 iter), loss = 0.0508077
I0816 00:08:29.864282 14907 solver.cpp:334]     Train net output #0: loss = 0.0508076 (* 1 = 0.0508076 loss)
I0816 00:08:29.864289 14907 sgd_solver.cpp:136] Iteration 25500, lr = 1e-06, m = 0.9
I0816 00:08:49.316244 14907 solver.cpp:312] Iteration 25600 (5.141 iter/s, 19.4515s/100 iter), loss = 0.060869
I0816 00:08:49.316272 14907 solver.cpp:334]     Train net output #0: loss = 0.0608689 (* 1 = 0.0608689 loss)
I0816 00:08:49.316275 14907 sgd_solver.cpp:136] Iteration 25600, lr = 1e-06, m = 0.9
I0816 00:08:52.249717 14918 data_reader.cpp:288] Starting prefetch of epoch 14
I0816 00:09:09.117440 14907 solver.cpp:312] Iteration 25700 (5.05034 iter/s, 19.8007s/100 iter), loss = 0.0438456
I0816 00:09:09.117496 14907 solver.cpp:334]     Train net output #0: loss = 0.0438455 (* 1 = 0.0438455 loss)
I0816 00:09:09.117503 14907 sgd_solver.cpp:136] Iteration 25700, lr = 1e-06, m = 0.9
I0816 00:09:28.561662 14907 solver.cpp:312] Iteration 25800 (5.14306 iter/s, 19.4437s/100 iter), loss = 0.0661617
I0816 00:09:28.561687 14907 solver.cpp:334]     Train net output #0: loss = 0.0661616 (* 1 = 0.0661616 loss)
I0816 00:09:28.561691 14907 sgd_solver.cpp:136] Iteration 25800, lr = 1e-06, m = 0.9
I0816 00:09:48.081126 14907 solver.cpp:312] Iteration 25900 (5.12323 iter/s, 19.5189s/100 iter), loss = 0.0532603
I0816 00:09:48.081174 14907 solver.cpp:334]     Train net output #0: loss = 0.0532602 (* 1 = 0.0532602 loss)
I0816 00:09:48.081179 14907 sgd_solver.cpp:136] Iteration 25900, lr = 1e-06, m = 0.9
I0816 00:09:56.882515 14918 data_reader.cpp:288] Starting prefetch of epoch 15
I0816 00:10:07.287201 14907 solver.cpp:363] Sparsity after update:
I0816 00:10:07.300873 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0816 00:10:07.300896 14907 net.cpp:2192] conv1a_param_0(0) 
I0816 00:10:07.300904 14907 net.cpp:2192] conv1b_param_0(0) 
I0816 00:10:07.300907 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0816 00:10:07.300910 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0816 00:10:07.300914 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0816 00:10:07.300916 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0816 00:10:07.300920 14907 net.cpp:2192] ctx_final_param_0(0) 
I0816 00:10:07.300923 14907 net.cpp:2192] out3a_param_0(0) 
I0816 00:10:07.300926 14907 net.cpp:2192] out5a_param_0(0) 
I0816 00:10:07.300930 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0816 00:10:07.300932 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0816 00:10:07.300935 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0816 00:10:07.300938 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0816 00:10:07.300941 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0816 00:10:07.300945 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0816 00:10:07.300948 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0816 00:10:07.300951 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0816 00:10:07.300954 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0816 00:10:07.300971 14907 solver.cpp:509] Iteration 26000, Testing net (#0)
I0816 00:10:15.144742 14944 data_reader.cpp:288] Starting prefetch of epoch 3
I0816 00:10:19.369318 14907 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.952633
I0816 00:10:19.369441 14907 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999248
I0816 00:10:19.369451 14907 solver.cpp:594]     Test net output #2: loss = 0.203218 (* 1 = 0.203218 loss)
I0816 00:10:19.369477 14907 solver.cpp:264] [MultiGPU] Tests completed in 12.0682s
I0816 00:10:19.592017 14907 solver.cpp:312] Iteration 26000 (3.17359 iter/s, 31.51s/100 iter), loss = 0.0821776
I0816 00:10:19.592046 14907 solver.cpp:334]     Train net output #0: loss = 0.0821775 (* 1 = 0.0821775 loss)
I0816 00:10:19.592052 14907 sgd_solver.cpp:136] Iteration 26000, lr = 1e-06, m = 0.9
I0816 00:10:38.996999 14907 solver.cpp:312] Iteration 26100 (5.15346 iter/s, 19.4044s/100 iter), loss = 0.0409351
I0816 00:10:38.997022 14907 solver.cpp:334]     Train net output #0: loss = 0.040935 (* 1 = 0.040935 loss)
I0816 00:10:38.997027 14907 sgd_solver.cpp:136] Iteration 26100, lr = 1e-06, m = 0.9
I0816 00:10:58.694727 14907 solver.cpp:312] Iteration 26200 (5.07687 iter/s, 19.6972s/100 iter), loss = 0.0884714
I0816 00:10:58.694782 14907 solver.cpp:334]     Train net output #0: loss = 0.0884713 (* 1 = 0.0884713 loss)
I0816 00:10:58.694789 14907 sgd_solver.cpp:136] Iteration 26200, lr = 1e-06, m = 0.9
I0816 00:11:13.464109 14918 data_reader.cpp:288] Starting prefetch of epoch 16
I0816 00:11:18.124089 14907 solver.cpp:312] Iteration 26300 (5.14699 iter/s, 19.4288s/100 iter), loss = 0.0742401
I0816 00:11:18.124109 14907 solver.cpp:334]     Train net output #0: loss = 0.07424 (* 1 = 0.07424 loss)
I0816 00:11:18.124114 14907 sgd_solver.cpp:136] Iteration 26300, lr = 1e-06, m = 0.9
I0816 00:11:37.469102 14907 solver.cpp:312] Iteration 26400 (5.16943 iter/s, 19.3445s/100 iter), loss = 0.0862997
I0816 00:11:37.469183 14907 solver.cpp:334]     Train net output #0: loss = 0.0862996 (* 1 = 0.0862996 loss)
I0816 00:11:37.469190 14907 sgd_solver.cpp:136] Iteration 26400, lr = 1e-06, m = 0.9
I0816 00:11:57.026494 14907 solver.cpp:312] Iteration 26500 (5.1133 iter/s, 19.5569s/100 iter), loss = 0.044197
I0816 00:11:57.026515 14907 solver.cpp:334]     Train net output #0: loss = 0.0441969 (* 1 = 0.0441969 loss)
I0816 00:11:57.026518 14907 sgd_solver.cpp:136] Iteration 26500, lr = 1e-06, m = 0.9
I0816 00:12:16.701149 14907 solver.cpp:312] Iteration 26600 (5.08282 iter/s, 19.6741s/100 iter), loss = 0.0747174
I0816 00:12:16.701196 14907 solver.cpp:334]     Train net output #0: loss = 0.0747173 (* 1 = 0.0747173 loss)
I0816 00:12:16.701201 14907 sgd_solver.cpp:136] Iteration 26600, lr = 1e-06, m = 0.9
I0816 00:12:17.879559 14920 data_reader.cpp:288] Starting prefetch of epoch 17
I0816 00:12:35.962353 14907 solver.cpp:312] Iteration 26700 (5.19193 iter/s, 19.2607s/100 iter), loss = 0.0631532
I0816 00:12:35.962378 14907 solver.cpp:334]     Train net output #0: loss = 0.0631531 (* 1 = 0.0631531 loss)
I0816 00:12:35.962383 14907 sgd_solver.cpp:136] Iteration 26700, lr = 1e-06, m = 0.9
I0816 00:12:50.081393 14915 data_reader.cpp:288] Starting prefetch of epoch 12
I0816 00:12:55.803917 14907 solver.cpp:312] Iteration 26800 (5.04006 iter/s, 19.841s/100 iter), loss = 0.0761298
I0816 00:12:55.803941 14907 solver.cpp:334]     Train net output #0: loss = 0.0761297 (* 1 = 0.0761297 loss)
I0816 00:12:55.803946 14907 sgd_solver.cpp:136] Iteration 26800, lr = 1e-06, m = 0.9
I0816 00:13:15.540285 14907 solver.cpp:312] Iteration 26900 (5.06693 iter/s, 19.7358s/100 iter), loss = 0.0533744
I0816 00:13:15.540309 14907 solver.cpp:334]     Train net output #0: loss = 0.0533743 (* 1 = 0.0533743 loss)
I0816 00:13:15.540313 14907 sgd_solver.cpp:136] Iteration 26900, lr = 1e-06, m = 0.9
I0816 00:13:35.301862 14907 solver.cpp:363] Sparsity after update:
I0816 00:13:35.321022 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0816 00:13:35.321069 14907 net.cpp:2192] conv1a_param_0(0) 
I0816 00:13:35.321084 14907 net.cpp:2192] conv1b_param_0(0) 
I0816 00:13:35.321086 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0816 00:13:35.321089 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0816 00:13:35.321091 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0816 00:13:35.321094 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0816 00:13:35.321097 14907 net.cpp:2192] ctx_final_param_0(0) 
I0816 00:13:35.321102 14907 net.cpp:2192] out3a_param_0(0) 
I0816 00:13:35.321105 14907 net.cpp:2192] out5a_param_0(0) 
I0816 00:13:35.321108 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0816 00:13:35.321111 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0816 00:13:35.321113 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0816 00:13:35.321116 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0816 00:13:35.321120 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0816 00:13:35.321121 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0816 00:13:35.321125 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0816 00:13:35.321127 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0816 00:13:35.321130 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0816 00:13:35.504619 14907 solver.cpp:312] Iteration 27000 (5.00907 iter/s, 19.9638s/100 iter), loss = 0.0966118
I0816 00:13:35.504645 14907 solver.cpp:334]     Train net output #0: loss = 0.0966117 (* 1 = 0.0966117 loss)
I0816 00:13:35.504649 14907 sgd_solver.cpp:136] Iteration 27000, lr = 1e-06, m = 0.9
I0816 00:13:54.948097 14907 solver.cpp:312] Iteration 27100 (5.14325 iter/s, 19.4429s/100 iter), loss = 0.0652169
I0816 00:13:54.948122 14907 solver.cpp:334]     Train net output #0: loss = 0.0652168 (* 1 = 0.0652168 loss)
I0816 00:13:54.948125 14907 sgd_solver.cpp:136] Iteration 27100, lr = 1e-06, m = 0.9
I0816 00:13:55.387607 14918 data_reader.cpp:288] Starting prefetch of epoch 17
I0816 00:14:14.626161 14907 solver.cpp:312] Iteration 27200 (5.08194 iter/s, 19.6775s/100 iter), loss = 0.0413523
I0816 00:14:14.626241 14907 solver.cpp:334]     Train net output #0: loss = 0.0413522 (* 1 = 0.0413522 loss)
I0816 00:14:14.626248 14907 sgd_solver.cpp:136] Iteration 27200, lr = 1e-06, m = 0.9
I0816 00:14:34.288139 14907 solver.cpp:312] Iteration 27300 (5.0861 iter/s, 19.6614s/100 iter), loss = 0.062718
I0816 00:14:34.288193 14907 solver.cpp:334]     Train net output #0: loss = 0.0627179 (* 1 = 0.0627179 loss)
I0816 00:14:34.288211 14907 sgd_solver.cpp:136] Iteration 27300, lr = 1e-06, m = 0.9
I0816 00:14:54.368080 14907 solver.cpp:312] Iteration 27400 (4.98023 iter/s, 20.0794s/100 iter), loss = 0.0911269
I0816 00:14:54.368192 14907 solver.cpp:334]     Train net output #0: loss = 0.0911268 (* 1 = 0.0911268 loss)
I0816 00:14:54.368207 14907 sgd_solver.cpp:136] Iteration 27400, lr = 1e-06, m = 0.9
I0816 00:15:00.749447 14872 data_reader.cpp:288] Starting prefetch of epoch 20
I0816 00:15:13.785629 14907 solver.cpp:312] Iteration 27500 (5.15012 iter/s, 19.417s/100 iter), loss = 0.058027
I0816 00:15:13.785681 14907 solver.cpp:334]     Train net output #0: loss = 0.058027 (* 1 = 0.058027 loss)
I0816 00:15:13.785693 14907 sgd_solver.cpp:136] Iteration 27500, lr = 1e-06, m = 0.9
I0816 00:15:32.828485 14915 data_reader.cpp:288] Starting prefetch of epoch 13
I0816 00:15:33.180001 14907 solver.cpp:312] Iteration 27600 (5.15628 iter/s, 19.3938s/100 iter), loss = 0.0757859
I0816 00:15:33.180029 14907 solver.cpp:334]     Train net output #0: loss = 0.0757858 (* 1 = 0.0757858 loss)
I0816 00:15:33.180035 14907 sgd_solver.cpp:136] Iteration 27600, lr = 1e-06, m = 0.9
I0816 00:15:52.794209 14907 solver.cpp:312] Iteration 27700 (5.09849 iter/s, 19.6137s/100 iter), loss = 0.0796964
I0816 00:15:52.794234 14907 solver.cpp:334]     Train net output #0: loss = 0.0796963 (* 1 = 0.0796963 loss)
I0816 00:15:52.794239 14907 sgd_solver.cpp:136] Iteration 27700, lr = 1e-06, m = 0.9
I0816 00:16:12.554177 14907 solver.cpp:312] Iteration 27800 (5.06088 iter/s, 19.7594s/100 iter), loss = 0.0413635
I0816 00:16:12.554255 14907 solver.cpp:334]     Train net output #0: loss = 0.0413634 (* 1 = 0.0413634 loss)
I0816 00:16:12.554261 14907 sgd_solver.cpp:136] Iteration 27800, lr = 1e-06, m = 0.9
I0816 00:16:32.258548 14907 solver.cpp:312] Iteration 27900 (5.07516 iter/s, 19.7038s/100 iter), loss = 0.054623
I0816 00:16:32.258576 14907 solver.cpp:334]     Train net output #0: loss = 0.0546229 (* 1 = 0.0546229 loss)
I0816 00:16:32.258582 14907 sgd_solver.cpp:136] Iteration 27900, lr = 1e-06, m = 0.9
I0816 00:16:37.719362 14872 data_reader.cpp:288] Starting prefetch of epoch 21
I0816 00:16:51.734525 14907 solver.cpp:363] Sparsity after update:
I0816 00:16:51.739277 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0816 00:16:51.739307 14907 net.cpp:2192] conv1a_param_0(0) 
I0816 00:16:51.739320 14907 net.cpp:2192] conv1b_param_0(0) 
I0816 00:16:51.739323 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0816 00:16:51.739326 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0816 00:16:51.739329 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0816 00:16:51.739332 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0816 00:16:51.739336 14907 net.cpp:2192] ctx_final_param_0(0) 
I0816 00:16:51.739337 14907 net.cpp:2192] out3a_param_0(0) 
I0816 00:16:51.739341 14907 net.cpp:2192] out5a_param_0(0) 
I0816 00:16:51.739343 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0816 00:16:51.739361 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0816 00:16:51.739369 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0816 00:16:51.739377 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0816 00:16:51.739393 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0816 00:16:51.739399 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0816 00:16:51.739403 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0816 00:16:51.739406 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0816 00:16:51.739414 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0816 00:16:51.739430 14907 solver.cpp:509] Iteration 28000, Testing net (#0)
I0816 00:17:03.025506 14890 data_reader.cpp:288] Starting prefetch of epoch 4
I0816 00:17:04.050485 14907 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.951441
I0816 00:17:04.050508 14907 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999908
I0816 00:17:04.050514 14907 solver.cpp:594]     Test net output #2: loss = 0.165095 (* 1 = 0.165095 loss)
I0816 00:17:04.050544 14907 solver.cpp:264] [MultiGPU] Tests completed in 12.3108s
I0816 00:17:04.261812 14907 solver.cpp:312] Iteration 28000 (3.12477 iter/s, 32.0024s/100 iter), loss = 0.0744268
I0816 00:17:04.261842 14907 solver.cpp:334]     Train net output #0: loss = 0.0744268 (* 1 = 0.0744268 loss)
I0816 00:17:04.261848 14907 sgd_solver.cpp:136] Iteration 28000, lr = 1e-06, m = 0.9
I0816 00:17:22.592229 14872 data_reader.cpp:288] Starting prefetch of epoch 22
I0816 00:17:23.692824 14907 solver.cpp:312] Iteration 28100 (5.14655 iter/s, 19.4305s/100 iter), loss = 0.0730648
I0816 00:17:23.692849 14907 solver.cpp:334]     Train net output #0: loss = 0.0730647 (* 1 = 0.0730647 loss)
I0816 00:17:23.692855 14907 sgd_solver.cpp:136] Iteration 28100, lr = 1e-06, m = 0.9
I0816 00:17:43.084321 14907 solver.cpp:312] Iteration 28200 (5.15704 iter/s, 19.391s/100 iter), loss = 0.0709689
I0816 00:17:43.084344 14907 solver.cpp:334]     Train net output #0: loss = 0.0709688 (* 1 = 0.0709688 loss)
I0816 00:17:43.084350 14907 sgd_solver.cpp:136] Iteration 28200, lr = 1e-06, m = 0.9
I0816 00:18:02.720535 14907 solver.cpp:312] Iteration 28300 (5.09277 iter/s, 19.6357s/100 iter), loss = 0.0454328
I0816 00:18:02.720633 14907 solver.cpp:334]     Train net output #0: loss = 0.0454327 (* 1 = 0.0454327 loss)
I0816 00:18:02.720639 14907 sgd_solver.cpp:136] Iteration 28300, lr = 1e-06, m = 0.9
I0816 00:18:22.312999 14907 solver.cpp:312] Iteration 28400 (5.10414 iter/s, 19.5919s/100 iter), loss = 0.0788459
I0816 00:18:22.313025 14907 solver.cpp:334]     Train net output #0: loss = 0.0788458 (* 1 = 0.0788458 loss)
I0816 00:18:22.313030 14907 sgd_solver.cpp:136] Iteration 28400, lr = 1e-06, m = 0.9
I0816 00:18:27.019403 14920 data_reader.cpp:288] Starting prefetch of epoch 18
I0816 00:18:41.993448 14907 solver.cpp:312] Iteration 28500 (5.08132 iter/s, 19.6799s/100 iter), loss = 0.0710122
I0816 00:18:41.993530 14907 solver.cpp:334]     Train net output #0: loss = 0.0710121 (* 1 = 0.0710121 loss)
I0816 00:18:41.993537 14907 sgd_solver.cpp:136] Iteration 28500, lr = 1e-06, m = 0.9
I0816 00:19:01.797464 14907 solver.cpp:312] Iteration 28600 (5.04962 iter/s, 19.8035s/100 iter), loss = 0.0836511
I0816 00:19:01.797487 14907 solver.cpp:334]     Train net output #0: loss = 0.083651 (* 1 = 0.083651 loss)
I0816 00:19:01.797492 14907 sgd_solver.cpp:136] Iteration 28600, lr = 1e-06, m = 0.9
I0816 00:19:21.281172 14907 solver.cpp:312] Iteration 28700 (5.13263 iter/s, 19.4832s/100 iter), loss = 0.0810614
I0816 00:19:21.281227 14907 solver.cpp:334]     Train net output #0: loss = 0.0810613 (* 1 = 0.0810613 loss)
I0816 00:19:21.281234 14907 sgd_solver.cpp:136] Iteration 28700, lr = 1e-06, m = 0.9
I0816 00:19:32.105643 14872 data_reader.cpp:288] Starting prefetch of epoch 23
I0816 00:19:41.096441 14907 solver.cpp:312] Iteration 28800 (5.04675 iter/s, 19.8147s/100 iter), loss = 0.053383
I0816 00:19:41.096463 14907 solver.cpp:334]     Train net output #0: loss = 0.0533829 (* 1 = 0.0533829 loss)
I0816 00:19:41.096467 14907 sgd_solver.cpp:136] Iteration 28800, lr = 1e-06, m = 0.9
I0816 00:20:00.601825 14907 solver.cpp:312] Iteration 28900 (5.12693 iter/s, 19.5048s/100 iter), loss = 0.118967
I0816 00:20:00.601909 14907 solver.cpp:334]     Train net output #0: loss = 0.118967 (* 1 = 0.118967 loss)
I0816 00:20:00.601917 14907 sgd_solver.cpp:136] Iteration 28900, lr = 1e-06, m = 0.9
I0816 00:20:04.456065 14870 data_reader.cpp:288] Starting prefetch of epoch 17
I0816 00:20:19.794955 14907 solver.cpp:363] Sparsity after update:
I0816 00:20:19.810505 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0816 00:20:19.810631 14907 net.cpp:2192] conv1a_param_0(0) 
I0816 00:20:19.810678 14907 net.cpp:2192] conv1b_param_0(0) 
I0816 00:20:19.810691 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0816 00:20:19.810704 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0816 00:20:19.810716 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0816 00:20:19.810729 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0816 00:20:19.810740 14907 net.cpp:2192] ctx_final_param_0(0) 
I0816 00:20:19.810752 14907 net.cpp:2192] out3a_param_0(0) 
I0816 00:20:19.810765 14907 net.cpp:2192] out5a_param_0(0) 
I0816 00:20:19.810777 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0816 00:20:19.810788 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0816 00:20:19.810801 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0816 00:20:19.810812 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0816 00:20:19.810822 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0816 00:20:19.810833 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0816 00:20:19.810845 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0816 00:20:19.810858 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0816 00:20:19.810868 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0816 00:20:20.005827 14907 solver.cpp:312] Iteration 29000 (5.15372 iter/s, 19.4035s/100 iter), loss = 0.0755216
I0816 00:20:20.005853 14907 solver.cpp:334]     Train net output #0: loss = 0.0755215 (* 1 = 0.0755215 loss)
I0816 00:20:20.005859 14907 sgd_solver.cpp:136] Iteration 29000, lr = 1e-06, m = 0.9
I0816 00:20:39.737541 14907 solver.cpp:312] Iteration 29100 (5.06812 iter/s, 19.7312s/100 iter), loss = 0.0723439
I0816 00:20:39.737642 14907 solver.cpp:334]     Train net output #0: loss = 0.0723438 (* 1 = 0.0723438 loss)
I0816 00:20:39.737649 14907 sgd_solver.cpp:136] Iteration 29100, lr = 1e-06, m = 0.9
I0816 00:20:59.180016 14907 solver.cpp:312] Iteration 29200 (5.14352 iter/s, 19.4419s/100 iter), loss = 0.0525972
I0816 00:20:59.180042 14907 solver.cpp:334]     Train net output #0: loss = 0.0525971 (* 1 = 0.0525971 loss)
I0816 00:20:59.180048 14907 sgd_solver.cpp:136] Iteration 29200, lr = 1e-06, m = 0.9
I0816 00:21:09.261040 14918 data_reader.cpp:288] Starting prefetch of epoch 18
I0816 00:21:18.783612 14907 solver.cpp:312] Iteration 29300 (5.10124 iter/s, 19.6031s/100 iter), loss = 0.0531815
I0816 00:21:18.783677 14907 solver.cpp:334]     Train net output #0: loss = 0.0531814 (* 1 = 0.0531814 loss)
I0816 00:21:18.783685 14907 sgd_solver.cpp:136] Iteration 29300, lr = 1e-06, m = 0.9
I0816 00:21:38.293993 14907 solver.cpp:312] Iteration 29400 (5.12562 iter/s, 19.5098s/100 iter), loss = 0.0505311
I0816 00:21:38.294018 14907 solver.cpp:334]     Train net output #0: loss = 0.050531 (* 1 = 0.050531 loss)
I0816 00:21:38.294023 14907 sgd_solver.cpp:136] Iteration 29400, lr = 1e-06, m = 0.9
I0816 00:21:57.717576 14907 solver.cpp:312] Iteration 29500 (5.14852 iter/s, 19.423s/100 iter), loss = 0.0613772
I0816 00:21:57.717664 14907 solver.cpp:334]     Train net output #0: loss = 0.0613771 (* 1 = 0.0613771 loss)
I0816 00:21:57.717671 14907 sgd_solver.cpp:136] Iteration 29500, lr = 1e-06, m = 0.9
I0816 00:22:13.609127 14872 data_reader.cpp:288] Starting prefetch of epoch 24
I0816 00:22:17.401620 14907 solver.cpp:312] Iteration 29600 (5.0804 iter/s, 19.6835s/100 iter), loss = 0.0723489
I0816 00:22:17.401644 14907 solver.cpp:334]     Train net output #0: loss = 0.0723488 (* 1 = 0.0723488 loss)
I0816 00:22:17.401649 14907 sgd_solver.cpp:136] Iteration 29600, lr = 1e-06, m = 0.9
I0816 00:22:36.916424 14907 solver.cpp:312] Iteration 29700 (5.12446 iter/s, 19.5143s/100 iter), loss = 0.0579867
I0816 00:22:36.916479 14907 solver.cpp:334]     Train net output #0: loss = 0.0579866 (* 1 = 0.0579866 loss)
I0816 00:22:36.916486 14907 sgd_solver.cpp:136] Iteration 29700, lr = 1e-06, m = 0.9
I0816 00:22:45.967703 14915 data_reader.cpp:288] Starting prefetch of epoch 14
I0816 00:22:56.467890 14907 solver.cpp:312] Iteration 29800 (5.11485 iter/s, 19.5509s/100 iter), loss = 0.132043
I0816 00:22:56.467922 14907 solver.cpp:334]     Train net output #0: loss = 0.132043 (* 1 = 0.132043 loss)
I0816 00:22:56.467929 14907 sgd_solver.cpp:136] Iteration 29800, lr = 1e-06, m = 0.9
I0816 00:23:15.872185 14907 solver.cpp:312] Iteration 29900 (5.15364 iter/s, 19.4038s/100 iter), loss = 0.0700822
I0816 00:23:15.872231 14907 solver.cpp:334]     Train net output #0: loss = 0.0700821 (* 1 = 0.0700821 loss)
I0816 00:23:15.872238 14907 sgd_solver.cpp:136] Iteration 29900, lr = 1e-06, m = 0.9
I0816 00:23:35.135635 14907 solver.cpp:639] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/sparse/cityscapes5_jsegnet21v2_iter_30000.caffemodel
I0816 00:23:35.226835 14907 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/sparse/cityscapes5_jsegnet21v2_iter_30000.solverstate
I0816 00:23:35.239183 14907 solver.cpp:363] Sparsity after update:
I0816 00:23:35.252444 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0816 00:23:35.252508 14907 net.cpp:2192] conv1a_param_0(0) 
I0816 00:23:35.252530 14907 net.cpp:2192] conv1b_param_0(0) 
I0816 00:23:35.252545 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0816 00:23:35.252560 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0816 00:23:35.252574 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0816 00:23:35.252588 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0816 00:23:35.252602 14907 net.cpp:2192] ctx_final_param_0(0) 
I0816 00:23:35.252624 14907 net.cpp:2192] out3a_param_0(0) 
I0816 00:23:35.252638 14907 net.cpp:2192] out5a_param_0(0) 
I0816 00:23:35.252652 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0816 00:23:35.252666 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0816 00:23:35.252679 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0816 00:23:35.252692 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0816 00:23:35.252707 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0816 00:23:35.252719 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0816 00:23:35.252734 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0816 00:23:35.252748 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0816 00:23:35.252761 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0816 00:23:35.252789 14907 solver.cpp:509] Iteration 30000, Testing net (#0)
I0816 00:23:42.889487 14890 data_reader.cpp:288] Starting prefetch of epoch 5
I0816 00:23:47.226872 14907 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.952419
I0816 00:23:47.226999 14907 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999238
I0816 00:23:47.227010 14907 solver.cpp:594]     Test net output #2: loss = 0.202727 (* 1 = 0.202727 loss)
I0816 00:23:47.227036 14907 solver.cpp:264] [MultiGPU] Tests completed in 11.9739s
I0816 00:23:47.442519 14907 solver.cpp:312] Iteration 30000 (3.16762 iter/s, 31.5695s/100 iter), loss = 0.0717813
I0816 00:23:47.442550 14907 solver.cpp:334]     Train net output #0: loss = 0.0717812 (* 1 = 0.0717812 loss)
I0816 00:23:47.442556 14907 sgd_solver.cpp:136] Iteration 30000, lr = 1e-06, m = 0.9
I0816 00:24:07.017861 14907 solver.cpp:312] Iteration 30100 (5.10861 iter/s, 19.5748s/100 iter), loss = 0.0595228
I0816 00:24:07.017889 14907 solver.cpp:334]     Train net output #0: loss = 0.0595227 (* 1 = 0.0595227 loss)
I0816 00:24:07.017925 14907 sgd_solver.cpp:136] Iteration 30100, lr = 1e-06, m = 0.9
I0816 00:24:26.720572 14907 solver.cpp:312] Iteration 30200 (5.07558 iter/s, 19.7022s/100 iter), loss = 0.0602127
I0816 00:24:26.720620 14907 solver.cpp:334]     Train net output #0: loss = 0.0602126 (* 1 = 0.0602126 loss)
I0816 00:24:26.720625 14907 sgd_solver.cpp:136] Iteration 30200, lr = 1e-06, m = 0.9
I0816 00:24:34.985824 14918 data_reader.cpp:288] Starting prefetch of epoch 19
I0816 00:24:46.272783 14907 solver.cpp:312] Iteration 30300 (5.11465 iter/s, 19.5517s/100 iter), loss = 0.0801672
I0816 00:24:46.272805 14907 solver.cpp:334]     Train net output #0: loss = 0.0801672 (* 1 = 0.0801672 loss)
I0816 00:24:46.272809 14907 sgd_solver.cpp:136] Iteration 30300, lr = 1e-06, m = 0.9
I0816 00:25:05.830607 14907 solver.cpp:312] Iteration 30400 (5.11318 iter/s, 19.5573s/100 iter), loss = 0.07121
I0816 00:25:05.830653 14907 solver.cpp:334]     Train net output #0: loss = 0.07121 (* 1 = 0.07121 loss)
I0816 00:25:05.830660 14907 sgd_solver.cpp:136] Iteration 30400, lr = 1e-06, m = 0.9
I0816 00:25:07.434259 14920 data_reader.cpp:288] Starting prefetch of epoch 19
I0816 00:25:25.524360 14907 solver.cpp:312] Iteration 30500 (5.07789 iter/s, 19.6932s/100 iter), loss = 0.0745704
I0816 00:25:25.524391 14907 solver.cpp:334]     Train net output #0: loss = 0.0745703 (* 1 = 0.0745703 loss)
I0816 00:25:25.524397 14907 sgd_solver.cpp:136] Iteration 30500, lr = 1e-06, m = 0.9
I0816 00:25:44.957881 14907 solver.cpp:312] Iteration 30600 (5.14589 iter/s, 19.433s/100 iter), loss = 0.0562695
I0816 00:25:44.957931 14907 solver.cpp:334]     Train net output #0: loss = 0.0562694 (* 1 = 0.0562694 loss)
I0816 00:25:44.957938 14907 sgd_solver.cpp:136] Iteration 30600, lr = 1e-06, m = 0.9
I0816 00:26:04.498692 14907 solver.cpp:312] Iteration 30700 (5.11764 iter/s, 19.5403s/100 iter), loss = 0.112671
I0816 00:26:04.498718 14907 solver.cpp:334]     Train net output #0: loss = 0.112671 (* 1 = 0.112671 loss)
I0816 00:26:04.498723 14907 sgd_solver.cpp:136] Iteration 30700, lr = 1e-06, m = 0.9
I0816 00:26:11.892210 14920 data_reader.cpp:288] Starting prefetch of epoch 20
I0816 00:26:23.875689 14907 solver.cpp:312] Iteration 30800 (5.1609 iter/s, 19.3765s/100 iter), loss = 0.0672035
I0816 00:26:23.875762 14907 solver.cpp:334]     Train net output #0: loss = 0.0672034 (* 1 = 0.0672034 loss)
I0816 00:26:23.875767 14907 sgd_solver.cpp:136] Iteration 30800, lr = 1e-06, m = 0.9
I0816 00:26:43.227187 14907 solver.cpp:312] Iteration 30900 (5.1677 iter/s, 19.351s/100 iter), loss = 0.0649902
I0816 00:26:43.227210 14907 solver.cpp:334]     Train net output #0: loss = 0.0649901 (* 1 = 0.0649901 loss)
I0816 00:26:43.227216 14907 sgd_solver.cpp:136] Iteration 30900, lr = 1e-06, m = 0.9
I0816 00:27:02.686569 14907 solver.cpp:363] Sparsity after update:
I0816 00:27:02.706657 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0816 00:27:02.706727 14907 net.cpp:2192] conv1a_param_0(0) 
I0816 00:27:02.706743 14907 net.cpp:2192] conv1b_param_0(0) 
I0816 00:27:02.706753 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0816 00:27:02.706761 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0816 00:27:02.706770 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0816 00:27:02.706779 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0816 00:27:02.706789 14907 net.cpp:2192] ctx_final_param_0(0) 
I0816 00:27:02.706797 14907 net.cpp:2192] out3a_param_0(0) 
I0816 00:27:02.706806 14907 net.cpp:2192] out5a_param_0(0) 
I0816 00:27:02.706815 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0816 00:27:02.706825 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0816 00:27:02.706833 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0816 00:27:02.706842 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0816 00:27:02.706851 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0816 00:27:02.706861 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0816 00:27:02.706869 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0816 00:27:02.706878 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0816 00:27:02.706887 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0816 00:27:02.889801 14907 solver.cpp:312] Iteration 31000 (5.08593 iter/s, 19.6621s/100 iter), loss = 0.0610258
I0816 00:27:02.889829 14907 solver.cpp:334]     Train net output #0: loss = 0.0610258 (* 1 = 0.0610258 loss)
I0816 00:27:02.889837 14907 sgd_solver.cpp:136] Iteration 31000, lr = 1e-06, m = 0.9
I0816 00:27:16.303589 14920 data_reader.cpp:288] Starting prefetch of epoch 21
I0816 00:27:22.298532 14907 solver.cpp:312] Iteration 31100 (5.15246 iter/s, 19.4082s/100 iter), loss = 0.0546895
I0816 00:27:22.298553 14907 solver.cpp:334]     Train net output #0: loss = 0.0546894 (* 1 = 0.0546894 loss)
I0816 00:27:22.298559 14907 sgd_solver.cpp:136] Iteration 31100, lr = 1e-06, m = 0.9
I0816 00:27:42.172782 14907 solver.cpp:312] Iteration 31200 (5.03178 iter/s, 19.8737s/100 iter), loss = 0.0896979
I0816 00:27:42.172835 14907 solver.cpp:334]     Train net output #0: loss = 0.0896978 (* 1 = 0.0896978 loss)
I0816 00:27:42.172842 14907 sgd_solver.cpp:136] Iteration 31200, lr = 1e-06, m = 0.9
I0816 00:27:48.820549 14916 data_reader.cpp:288] Starting prefetch of epoch 17
I0816 00:28:01.617902 14907 solver.cpp:312] Iteration 31300 (5.14282 iter/s, 19.4446s/100 iter), loss = 0.0925379
I0816 00:28:01.617929 14907 solver.cpp:334]     Train net output #0: loss = 0.0925378 (* 1 = 0.0925378 loss)
I0816 00:28:01.617936 14907 sgd_solver.cpp:136] Iteration 31300, lr = 1e-06, m = 0.9
I0816 00:28:20.948731 14907 solver.cpp:312] Iteration 31400 (5.17323 iter/s, 19.3303s/100 iter), loss = 0.0583722
I0816 00:28:20.948786 14907 solver.cpp:334]     Train net output #0: loss = 0.0583722 (* 1 = 0.0583722 loss)
I0816 00:28:20.948792 14907 sgd_solver.cpp:136] Iteration 31400, lr = 1e-06, m = 0.9
I0816 00:28:44.077503 14907 solver.cpp:312] Iteration 31500 (4.32374 iter/s, 23.1281s/100 iter), loss = 0.0629486
I0816 00:28:44.077553 14907 solver.cpp:334]     Train net output #0: loss = 0.0629485 (* 1 = 0.0629485 loss)
I0816 00:28:44.077569 14907 sgd_solver.cpp:136] Iteration 31500, lr = 1e-06, m = 0.9
I0816 00:28:58.417264 14870 data_reader.cpp:288] Starting prefetch of epoch 18
I0816 00:29:05.642498 14907 solver.cpp:312] Iteration 31600 (4.63727 iter/s, 21.5644s/100 iter), loss = 0.113864
I0816 00:29:05.642525 14907 solver.cpp:334]     Train net output #0: loss = 0.113864 (* 1 = 0.113864 loss)
I0816 00:29:05.642531 14907 sgd_solver.cpp:136] Iteration 31600, lr = 1e-06, m = 0.9
I0816 00:29:25.678930 14907 solver.cpp:312] Iteration 31700 (4.99105 iter/s, 20.0359s/100 iter), loss = 0.0639655
I0816 00:29:25.678973 14907 solver.cpp:334]     Train net output #0: loss = 0.0639654 (* 1 = 0.0639654 loss)
I0816 00:29:25.678983 14907 sgd_solver.cpp:136] Iteration 31700, lr = 1e-06, m = 0.9
I0816 00:29:45.209295 14907 solver.cpp:312] Iteration 31800 (5.12037 iter/s, 19.5298s/100 iter), loss = 0.071197
I0816 00:29:45.209352 14907 solver.cpp:334]     Train net output #0: loss = 0.0711969 (* 1 = 0.0711969 loss)
I0816 00:29:45.209357 14907 sgd_solver.cpp:136] Iteration 31800, lr = 1e-06, m = 0.9
I0816 00:30:03.730185 14920 data_reader.cpp:288] Starting prefetch of epoch 22
I0816 00:30:04.740299 14907 solver.cpp:312] Iteration 31900 (5.1202 iter/s, 19.5305s/100 iter), loss = 0.0422064
I0816 00:30:04.740325 14907 solver.cpp:334]     Train net output #0: loss = 0.0422063 (* 1 = 0.0422063 loss)
I0816 00:30:04.740329 14907 sgd_solver.cpp:136] Iteration 31900, lr = 1e-06, m = 0.9
I0816 00:30:23.986390 14907 solver.cpp:312] Iteration 31999 (5.14404 iter/s, 19.2456s/99 iter), loss = 0.0607484
I0816 00:30:23.986444 14907 solver.cpp:334]     Train net output #0: loss = 0.0607483 (* 1 = 0.0607483 loss)
I0816 00:30:23.986449 14907 solver.cpp:363] Sparsity after update:
I0816 00:30:23.988098 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0816 00:30:23.988106 14907 net.cpp:2192] conv1a_param_0(0) 
I0816 00:30:23.988109 14907 net.cpp:2192] conv1b_param_0(0) 
I0816 00:30:23.988111 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0816 00:30:23.988113 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0816 00:30:23.988116 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0816 00:30:23.988121 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0816 00:30:23.988123 14907 net.cpp:2192] ctx_final_param_0(0) 
I0816 00:30:23.988127 14907 net.cpp:2192] out3a_param_0(0) 
I0816 00:30:23.988137 14907 net.cpp:2192] out5a_param_0(0) 
I0816 00:30:23.988140 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0816 00:30:23.988144 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0816 00:30:23.988148 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0816 00:30:23.988152 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0816 00:30:23.988155 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0816 00:30:23.988158 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0816 00:30:23.988162 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0816 00:30:23.988165 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0816 00:30:23.988168 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0816 00:30:24.041965 14907 solver.cpp:639] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/sparse/cityscapes5_jsegnet21v2_iter_32000.caffemodel
I0816 00:30:24.111923 14907 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/sparse/cityscapes5_jsegnet21v2_iter_32000.solverstate
I0816 00:30:24.188591 14907 solver.cpp:486] Iteration 32000, loss = 0.0566738
I0816 00:30:24.188621 14907 solver.cpp:509] Iteration 32000, Testing net (#0)
I0816 00:30:36.700569 14888 data_reader.cpp:288] Starting prefetch of epoch 3
I0816 00:30:37.308720 14907 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.951346
I0816 00:30:37.308745 14907 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999917
I0816 00:30:37.308751 14907 solver.cpp:594]     Test net output #2: loss = 0.166435 (* 1 = 0.166435 loss)
I0816 00:30:37.360635 14815 parallel.cpp:71] Root Solver performance on device 0: 4.952 * 6 = 29.71 img/sec (32000 itr in 6462 sec)
I0816 00:30:37.360656 14815 parallel.cpp:76]      Solver performance on device 1: 4.952 * 6 = 29.71 img/sec (32000 itr in 6462 sec)
I0816 00:30:37.360662 14815 parallel.cpp:76]      Solver performance on device 2: 4.952 * 6 = 29.71 img/sec (32000 itr in 6462 sec)
I0816 00:30:37.360664 14815 parallel.cpp:79] Overall multi-GPU performance: 89.1337 img/sec
I0816 00:30:38.622130 14815 caffe.cpp:247] Optimization Done in 1h 48m 2s
I0816 00:30:44.308667  9762 caffe.cpp:608] This is NVCaffe 0.16.3 started at Wed Aug 16 00:30:43 2017
I0816 00:30:44.309717  9762 caffe.cpp:611] CuDNN version: 6021
I0816 00:30:44.309721  9762 caffe.cpp:612] CuBLAS version: 8000
I0816 00:30:44.309723  9762 caffe.cpp:613] CUDA version: 8000
I0816 00:30:44.309725  9762 caffe.cpp:614] CUDA driver version: 8000
I0816 00:30:44.309731  9762 caffe.cpp:263] Not using GPU #2 for single-GPU function
I0816 00:30:44.309733  9762 caffe.cpp:263] Not using GPU #1 for single-GPU function
I0816 00:30:44.310281  9762 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0816 00:30:44.310819  9762 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I0816 00:30:44.310824  9762 caffe.cpp:275] Use GPU with device ID 0
I0816 00:30:44.311146  9762 caffe.cpp:279] GPU device name: GeForce GTX 1080
I0816 00:30:44.328624  9762 net.cpp:72] Initializing net from parameters: 
name: "jsegnet21v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/val-image-lmdb"
    label_list_path: "data/val-label-lmdb"
    batch_size: 4
    threads: 1
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
  accuracy_param {
    ignore_label: 255
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
    ignore_label: 255
  }
}
I0816 00:30:44.328812  9762 net.cpp:104] Using FLOAT as default forward math type
I0816 00:30:44.328829  9762 net.cpp:110] Using FLOAT as default backward math type
I0816 00:30:44.328835  9762 layer_factory.hpp:136] Creating layer 'data' of type 'ImageLabelData'
I0816 00:30:44.328840  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.328858  9762 net.cpp:184] Created Layer data (0)
I0816 00:30:44.328869  9762 net.cpp:530] data -> data
I0816 00:30:44.328886  9762 net.cpp:530] data -> label
I0816 00:30:44.341277  9762 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 4
I0816 00:30:44.341292  9762 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0816 00:30:44.357152  9811 db_lmdb.cpp:24] Opened lmdb data/val-image-lmdb
I0816 00:30:44.359326  9762 data_layer.cpp:185] (0) ReshapePrefetch 4, 3, 640, 640
I0816 00:30:44.359377  9762 data_layer.cpp:209] (0) Output data size: 4, 3, 640, 640
I0816 00:30:44.359383  9762 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0816 00:30:44.359457  9762 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 4
I0816 00:30:44.359468  9762 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0816 00:30:44.360188  9812 data_layer.cpp:97] (0) Parser threads: 1
I0816 00:30:44.360198  9812 data_layer.cpp:99] (0) Transformer threads: 1
I0816 00:30:44.364596  9813 db_lmdb.cpp:24] Opened lmdb data/val-label-lmdb
I0816 00:30:44.365926  9762 data_layer.cpp:185] (0) ReshapePrefetch 4, 1, 640, 640
I0816 00:30:44.365962  9762 data_layer.cpp:209] (0) Output data size: 4, 1, 640, 640
I0816 00:30:44.365970  9762 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0816 00:30:44.366199  9762 net.cpp:245] Setting up data
I0816 00:30:44.366221  9762 net.cpp:252] TEST Top shape for layer 0 'data' 4 3 640 640 (4915200)
I0816 00:30:44.366237  9762 net.cpp:252] TEST Top shape for layer 0 'data' 4 1 640 640 (1638400)
I0816 00:30:44.366250  9762 layer_factory.hpp:136] Creating layer 'label_data_1_split' of type 'Split'
I0816 00:30:44.366263  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.366291  9762 net.cpp:184] Created Layer label_data_1_split (1)
I0816 00:30:44.366302  9762 net.cpp:561] label_data_1_split <- label
I0816 00:30:44.366322  9762 net.cpp:530] label_data_1_split -> label_data_1_split_0
I0816 00:30:44.366333  9762 net.cpp:530] label_data_1_split -> label_data_1_split_1
I0816 00:30:44.366338  9762 net.cpp:530] label_data_1_split -> label_data_1_split_2
I0816 00:30:44.366399  9762 net.cpp:245] Setting up label_data_1_split
I0816 00:30:44.366405  9762 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 4 1 640 640 (1638400)
I0816 00:30:44.366410  9762 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 4 1 640 640 (1638400)
I0816 00:30:44.366415  9762 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 4 1 640 640 (1638400)
I0816 00:30:44.366420  9762 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0816 00:30:44.366425  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.366446  9762 net.cpp:184] Created Layer data/bias (2)
I0816 00:30:44.366451  9762 net.cpp:561] data/bias <- data
I0816 00:30:44.366456  9762 net.cpp:530] data/bias -> data/bias
I0816 00:30:44.367733  9814 data_layer.cpp:97] (0) Parser threads: 1
I0816 00:30:44.367755  9814 data_layer.cpp:99] (0) Transformer threads: 1
I0816 00:30:44.370901  9762 net.cpp:245] Setting up data/bias
I0816 00:30:44.370976  9762 net.cpp:252] TEST Top shape for layer 2 'data/bias' 4 3 640 640 (4915200)
I0816 00:30:44.371001  9762 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0816 00:30:44.371014  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.371049  9762 net.cpp:184] Created Layer conv1a (3)
I0816 00:30:44.371068  9762 net.cpp:561] conv1a <- data/bias
I0816 00:30:44.371076  9762 net.cpp:530] conv1a -> conv1a
I0816 00:30:44.941376  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.01G/1 1  (limit 8.06G, req 0G)
I0816 00:30:44.941398  9762 net.cpp:245] Setting up conv1a
I0816 00:30:44.941406  9762 net.cpp:252] TEST Top shape for layer 3 'conv1a' 4 32 320 320 (13107200)
I0816 00:30:44.941417  9762 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0816 00:30:44.941423  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.941437  9762 net.cpp:184] Created Layer conv1a/bn (4)
I0816 00:30:44.941442  9762 net.cpp:561] conv1a/bn <- conv1a
I0816 00:30:44.941447  9762 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0816 00:30:44.942003  9762 net.cpp:245] Setting up conv1a/bn
I0816 00:30:44.942013  9762 net.cpp:252] TEST Top shape for layer 4 'conv1a/bn' 4 32 320 320 (13107200)
I0816 00:30:44.942023  9762 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0816 00:30:44.942028  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.942039  9762 net.cpp:184] Created Layer conv1a/relu (5)
I0816 00:30:44.942042  9762 net.cpp:561] conv1a/relu <- conv1a
I0816 00:30:44.942046  9762 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0816 00:30:44.942059  9762 net.cpp:245] Setting up conv1a/relu
I0816 00:30:44.942065  9762 net.cpp:252] TEST Top shape for layer 5 'conv1a/relu' 4 32 320 320 (13107200)
I0816 00:30:44.942068  9762 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0816 00:30:44.942072  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.942082  9762 net.cpp:184] Created Layer conv1b (6)
I0816 00:30:44.942086  9762 net.cpp:561] conv1b <- conv1a
I0816 00:30:44.942091  9762 net.cpp:530] conv1b -> conv1b
I0816 00:30:44.959354  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 8G, req 0G)
I0816 00:30:44.959365  9762 net.cpp:245] Setting up conv1b
I0816 00:30:44.959370  9762 net.cpp:252] TEST Top shape for layer 6 'conv1b' 4 32 320 320 (13107200)
I0816 00:30:44.959378  9762 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0816 00:30:44.959383  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.959389  9762 net.cpp:184] Created Layer conv1b/bn (7)
I0816 00:30:44.959393  9762 net.cpp:561] conv1b/bn <- conv1b
I0816 00:30:44.959398  9762 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0816 00:30:44.959930  9762 net.cpp:245] Setting up conv1b/bn
I0816 00:30:44.959939  9762 net.cpp:252] TEST Top shape for layer 7 'conv1b/bn' 4 32 320 320 (13107200)
I0816 00:30:44.959949  9762 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0816 00:30:44.959951  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.959956  9762 net.cpp:184] Created Layer conv1b/relu (8)
I0816 00:30:44.959959  9762 net.cpp:561] conv1b/relu <- conv1b
I0816 00:30:44.959964  9762 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0816 00:30:44.959969  9762 net.cpp:245] Setting up conv1b/relu
I0816 00:30:44.959972  9762 net.cpp:252] TEST Top shape for layer 8 'conv1b/relu' 4 32 320 320 (13107200)
I0816 00:30:44.959976  9762 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0816 00:30:44.959980  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.959986  9762 net.cpp:184] Created Layer pool1 (9)
I0816 00:30:44.959990  9762 net.cpp:561] pool1 <- conv1b
I0816 00:30:44.959995  9762 net.cpp:530] pool1 -> pool1
I0816 00:30:44.960045  9762 net.cpp:245] Setting up pool1
I0816 00:30:44.960052  9762 net.cpp:252] TEST Top shape for layer 9 'pool1' 4 32 160 160 (3276800)
I0816 00:30:44.960055  9762 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0816 00:30:44.960060  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.960080  9762 net.cpp:184] Created Layer res2a_branch2a (10)
I0816 00:30:44.960085  9762 net.cpp:561] res2a_branch2a <- pool1
I0816 00:30:44.960090  9762 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0816 00:30:44.972374  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.95G, req 0G)
I0816 00:30:44.972386  9762 net.cpp:245] Setting up res2a_branch2a
I0816 00:30:44.972391  9762 net.cpp:252] TEST Top shape for layer 10 'res2a_branch2a' 4 64 160 160 (6553600)
I0816 00:30:44.972399  9762 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0816 00:30:44.972404  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.972410  9762 net.cpp:184] Created Layer res2a_branch2a/bn (11)
I0816 00:30:44.972414  9762 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0816 00:30:44.972419  9762 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0816 00:30:44.972949  9762 net.cpp:245] Setting up res2a_branch2a/bn
I0816 00:30:44.972957  9762 net.cpp:252] TEST Top shape for layer 11 'res2a_branch2a/bn' 4 64 160 160 (6553600)
I0816 00:30:44.972965  9762 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0816 00:30:44.972970  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.972973  9762 net.cpp:184] Created Layer res2a_branch2a/relu (12)
I0816 00:30:44.972977  9762 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0816 00:30:44.972980  9762 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0816 00:30:44.972986  9762 net.cpp:245] Setting up res2a_branch2a/relu
I0816 00:30:44.972990  9762 net.cpp:252] TEST Top shape for layer 12 'res2a_branch2a/relu' 4 64 160 160 (6553600)
I0816 00:30:44.972995  9762 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0816 00:30:44.972997  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.973006  9762 net.cpp:184] Created Layer res2a_branch2b (13)
I0816 00:30:44.973011  9762 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0816 00:30:44.973014  9762 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0816 00:30:44.981643  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.92G, req 0G)
I0816 00:30:44.981662  9762 net.cpp:245] Setting up res2a_branch2b
I0816 00:30:44.981667  9762 net.cpp:252] TEST Top shape for layer 13 'res2a_branch2b' 4 64 160 160 (6553600)
I0816 00:30:44.981674  9762 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0816 00:30:44.981679  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.981688  9762 net.cpp:184] Created Layer res2a_branch2b/bn (14)
I0816 00:30:44.981693  9762 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0816 00:30:44.981696  9762 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0816 00:30:44.982255  9762 net.cpp:245] Setting up res2a_branch2b/bn
I0816 00:30:44.982264  9762 net.cpp:252] TEST Top shape for layer 14 'res2a_branch2b/bn' 4 64 160 160 (6553600)
I0816 00:30:44.982273  9762 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0816 00:30:44.982277  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.982282  9762 net.cpp:184] Created Layer res2a_branch2b/relu (15)
I0816 00:30:44.982285  9762 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0816 00:30:44.982290  9762 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0816 00:30:44.982295  9762 net.cpp:245] Setting up res2a_branch2b/relu
I0816 00:30:44.982300  9762 net.cpp:252] TEST Top shape for layer 15 'res2a_branch2b/relu' 4 64 160 160 (6553600)
I0816 00:30:44.982303  9762 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0816 00:30:44.982306  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.982326  9762 net.cpp:184] Created Layer pool2 (16)
I0816 00:30:44.982329  9762 net.cpp:561] pool2 <- res2a_branch2b
I0816 00:30:44.982332  9762 net.cpp:530] pool2 -> pool2
I0816 00:30:44.982374  9762 net.cpp:245] Setting up pool2
I0816 00:30:44.982380  9762 net.cpp:252] TEST Top shape for layer 16 'pool2' 4 64 80 80 (1638400)
I0816 00:30:44.982384  9762 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0816 00:30:44.982388  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.982396  9762 net.cpp:184] Created Layer res3a_branch2a (17)
I0816 00:30:44.982399  9762 net.cpp:561] res3a_branch2a <- pool2
I0816 00:30:44.982403  9762 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0816 00:30:44.989929  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.9G, req 0G)
I0816 00:30:44.989940  9762 net.cpp:245] Setting up res3a_branch2a
I0816 00:30:44.989946  9762 net.cpp:252] TEST Top shape for layer 17 'res3a_branch2a' 4 128 80 80 (3276800)
I0816 00:30:44.989953  9762 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0816 00:30:44.989956  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.989962  9762 net.cpp:184] Created Layer res3a_branch2a/bn (18)
I0816 00:30:44.989965  9762 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0816 00:30:44.989969  9762 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0816 00:30:44.990906  9762 net.cpp:245] Setting up res3a_branch2a/bn
I0816 00:30:44.990916  9762 net.cpp:252] TEST Top shape for layer 18 'res3a_branch2a/bn' 4 128 80 80 (3276800)
I0816 00:30:44.990926  9762 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0816 00:30:44.990929  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.990934  9762 net.cpp:184] Created Layer res3a_branch2a/relu (19)
I0816 00:30:44.990937  9762 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0816 00:30:44.990942  9762 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0816 00:30:44.990948  9762 net.cpp:245] Setting up res3a_branch2a/relu
I0816 00:30:44.990952  9762 net.cpp:252] TEST Top shape for layer 19 'res3a_branch2a/relu' 4 128 80 80 (3276800)
I0816 00:30:44.990957  9762 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0816 00:30:44.990960  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.990968  9762 net.cpp:184] Created Layer res3a_branch2b (20)
I0816 00:30:44.990972  9762 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0816 00:30:44.990977  9762 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0816 00:30:44.996207  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.88G, req 0G)
I0816 00:30:44.996218  9762 net.cpp:245] Setting up res3a_branch2b
I0816 00:30:44.996225  9762 net.cpp:252] TEST Top shape for layer 20 'res3a_branch2b' 4 128 80 80 (3276800)
I0816 00:30:44.996232  9762 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0816 00:30:44.996237  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.996243  9762 net.cpp:184] Created Layer res3a_branch2b/bn (21)
I0816 00:30:44.996248  9762 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0816 00:30:44.996253  9762 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0816 00:30:44.996757  9762 net.cpp:245] Setting up res3a_branch2b/bn
I0816 00:30:44.996765  9762 net.cpp:252] TEST Top shape for layer 21 'res3a_branch2b/bn' 4 128 80 80 (3276800)
I0816 00:30:44.996773  9762 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0816 00:30:44.996778  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.996783  9762 net.cpp:184] Created Layer res3a_branch2b/relu (22)
I0816 00:30:44.996794  9762 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0816 00:30:44.996800  9762 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0816 00:30:44.996805  9762 net.cpp:245] Setting up res3a_branch2b/relu
I0816 00:30:44.996809  9762 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b/relu' 4 128 80 80 (3276800)
I0816 00:30:44.996824  9762 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0816 00:30:44.996829  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.996834  9762 net.cpp:184] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (23)
I0816 00:30:44.996837  9762 net.cpp:561] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0816 00:30:44.996842  9762 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0816 00:30:44.996847  9762 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0816 00:30:44.996877  9762 net.cpp:245] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0816 00:30:44.996884  9762 net.cpp:252] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 4 128 80 80 (3276800)
I0816 00:30:44.996889  9762 net.cpp:252] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 4 128 80 80 (3276800)
I0816 00:30:44.996893  9762 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0816 00:30:44.996897  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.996902  9762 net.cpp:184] Created Layer pool3 (24)
I0816 00:30:44.996906  9762 net.cpp:561] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0816 00:30:44.996912  9762 net.cpp:530] pool3 -> pool3
I0816 00:30:44.996960  9762 net.cpp:245] Setting up pool3
I0816 00:30:44.996968  9762 net.cpp:252] TEST Top shape for layer 24 'pool3' 4 128 40 40 (819200)
I0816 00:30:44.996971  9762 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0816 00:30:44.996975  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.996984  9762 net.cpp:184] Created Layer res4a_branch2a (25)
I0816 00:30:44.996987  9762 net.cpp:561] res4a_branch2a <- pool3
I0816 00:30:44.996992  9762 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0816 00:30:45.011598  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.87G, req 0G)
I0816 00:30:45.011616  9762 net.cpp:245] Setting up res4a_branch2a
I0816 00:30:45.011623  9762 net.cpp:252] TEST Top shape for layer 25 'res4a_branch2a' 4 256 40 40 (1638400)
I0816 00:30:45.011631  9762 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0816 00:30:45.011636  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.011646  9762 net.cpp:184] Created Layer res4a_branch2a/bn (26)
I0816 00:30:45.011651  9762 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0816 00:30:45.011656  9762 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0816 00:30:45.012204  9762 net.cpp:245] Setting up res4a_branch2a/bn
I0816 00:30:45.012213  9762 net.cpp:252] TEST Top shape for layer 26 'res4a_branch2a/bn' 4 256 40 40 (1638400)
I0816 00:30:45.012223  9762 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0816 00:30:45.012226  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.012230  9762 net.cpp:184] Created Layer res4a_branch2a/relu (27)
I0816 00:30:45.012234  9762 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0816 00:30:45.012238  9762 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0816 00:30:45.012243  9762 net.cpp:245] Setting up res4a_branch2a/relu
I0816 00:30:45.012248  9762 net.cpp:252] TEST Top shape for layer 27 'res4a_branch2a/relu' 4 256 40 40 (1638400)
I0816 00:30:45.012253  9762 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0816 00:30:45.012266  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.012275  9762 net.cpp:184] Created Layer res4a_branch2b (28)
I0816 00:30:45.012279  9762 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0816 00:30:45.012284  9762 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0816 00:30:45.019093  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.86G, req 0G)
I0816 00:30:45.019104  9762 net.cpp:245] Setting up res4a_branch2b
I0816 00:30:45.019109  9762 net.cpp:252] TEST Top shape for layer 28 'res4a_branch2b' 4 256 40 40 (1638400)
I0816 00:30:45.019114  9762 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0816 00:30:45.019117  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.019124  9762 net.cpp:184] Created Layer res4a_branch2b/bn (29)
I0816 00:30:45.019127  9762 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0816 00:30:45.019131  9762 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0816 00:30:45.019640  9762 net.cpp:245] Setting up res4a_branch2b/bn
I0816 00:30:45.019649  9762 net.cpp:252] TEST Top shape for layer 29 'res4a_branch2b/bn' 4 256 40 40 (1638400)
I0816 00:30:45.019656  9762 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0816 00:30:45.019660  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.019665  9762 net.cpp:184] Created Layer res4a_branch2b/relu (30)
I0816 00:30:45.019667  9762 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0816 00:30:45.019671  9762 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0816 00:30:45.019676  9762 net.cpp:245] Setting up res4a_branch2b/relu
I0816 00:30:45.019680  9762 net.cpp:252] TEST Top shape for layer 30 'res4a_branch2b/relu' 4 256 40 40 (1638400)
I0816 00:30:45.019685  9762 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0816 00:30:45.019688  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.019697  9762 net.cpp:184] Created Layer pool4 (31)
I0816 00:30:45.019701  9762 net.cpp:561] pool4 <- res4a_branch2b
I0816 00:30:45.019706  9762 net.cpp:530] pool4 -> pool4
I0816 00:30:45.019745  9762 net.cpp:245] Setting up pool4
I0816 00:30:45.019752  9762 net.cpp:252] TEST Top shape for layer 31 'pool4' 4 256 40 40 (1638400)
I0816 00:30:45.019755  9762 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0816 00:30:45.019759  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.019773  9762 net.cpp:184] Created Layer res5a_branch2a (32)
I0816 00:30:45.019776  9762 net.cpp:561] res5a_branch2a <- pool4
I0816 00:30:45.019779  9762 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0816 00:30:45.052844  9762 net.cpp:245] Setting up res5a_branch2a
I0816 00:30:45.052865  9762 net.cpp:252] TEST Top shape for layer 32 'res5a_branch2a' 4 512 40 40 (3276800)
I0816 00:30:45.052873  9762 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0816 00:30:45.052878  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.052887  9762 net.cpp:184] Created Layer res5a_branch2a/bn (33)
I0816 00:30:45.052891  9762 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0816 00:30:45.052896  9762 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0816 00:30:45.053966  9762 net.cpp:245] Setting up res5a_branch2a/bn
I0816 00:30:45.053975  9762 net.cpp:252] TEST Top shape for layer 33 'res5a_branch2a/bn' 4 512 40 40 (3276800)
I0816 00:30:45.053984  9762 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0816 00:30:45.053988  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.053993  9762 net.cpp:184] Created Layer res5a_branch2a/relu (34)
I0816 00:30:45.054008  9762 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0816 00:30:45.054011  9762 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0816 00:30:45.054018  9762 net.cpp:245] Setting up res5a_branch2a/relu
I0816 00:30:45.054023  9762 net.cpp:252] TEST Top shape for layer 34 'res5a_branch2a/relu' 4 512 40 40 (3276800)
I0816 00:30:45.054026  9762 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0816 00:30:45.054030  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.054039  9762 net.cpp:184] Created Layer res5a_branch2b (35)
I0816 00:30:45.054044  9762 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0816 00:30:45.054046  9762 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0816 00:30:45.070345  9762 net.cpp:245] Setting up res5a_branch2b
I0816 00:30:45.070358  9762 net.cpp:252] TEST Top shape for layer 35 'res5a_branch2b' 4 512 40 40 (3276800)
I0816 00:30:45.070376  9762 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0816 00:30:45.070381  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.070387  9762 net.cpp:184] Created Layer res5a_branch2b/bn (36)
I0816 00:30:45.070390  9762 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0816 00:30:45.070395  9762 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0816 00:30:45.070911  9762 net.cpp:245] Setting up res5a_branch2b/bn
I0816 00:30:45.070920  9762 net.cpp:252] TEST Top shape for layer 36 'res5a_branch2b/bn' 4 512 40 40 (3276800)
I0816 00:30:45.070929  9762 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0816 00:30:45.070933  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.070938  9762 net.cpp:184] Created Layer res5a_branch2b/relu (37)
I0816 00:30:45.070942  9762 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0816 00:30:45.070947  9762 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0816 00:30:45.070952  9762 net.cpp:245] Setting up res5a_branch2b/relu
I0816 00:30:45.070957  9762 net.cpp:252] TEST Top shape for layer 37 'res5a_branch2b/relu' 4 512 40 40 (3276800)
I0816 00:30:45.070961  9762 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I0816 00:30:45.070966  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.070974  9762 net.cpp:184] Created Layer out5a (38)
I0816 00:30:45.070978  9762 net.cpp:561] out5a <- res5a_branch2b
I0816 00:30:45.070983  9762 net.cpp:530] out5a -> out5a
I0816 00:30:45.075768  9762 net.cpp:245] Setting up out5a
I0816 00:30:45.075779  9762 net.cpp:252] TEST Top shape for layer 38 'out5a' 4 64 40 40 (409600)
I0816 00:30:45.075786  9762 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I0816 00:30:45.075790  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.075798  9762 net.cpp:184] Created Layer out5a/bn (39)
I0816 00:30:45.075801  9762 net.cpp:561] out5a/bn <- out5a
I0816 00:30:45.075806  9762 net.cpp:513] out5a/bn -> out5a (in-place)
I0816 00:30:45.076351  9762 net.cpp:245] Setting up out5a/bn
I0816 00:30:45.076360  9762 net.cpp:252] TEST Top shape for layer 39 'out5a/bn' 4 64 40 40 (409600)
I0816 00:30:45.076370  9762 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I0816 00:30:45.076373  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.076380  9762 net.cpp:184] Created Layer out5a/relu (40)
I0816 00:30:45.076383  9762 net.cpp:561] out5a/relu <- out5a
I0816 00:30:45.076387  9762 net.cpp:513] out5a/relu -> out5a (in-place)
I0816 00:30:45.076392  9762 net.cpp:245] Setting up out5a/relu
I0816 00:30:45.076397  9762 net.cpp:252] TEST Top shape for layer 40 'out5a/relu' 4 64 40 40 (409600)
I0816 00:30:45.076401  9762 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I0816 00:30:45.076414  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.076428  9762 net.cpp:184] Created Layer out5a_up2 (41)
I0816 00:30:45.076432  9762 net.cpp:561] out5a_up2 <- out5a
I0816 00:30:45.076436  9762 net.cpp:530] out5a_up2 -> out5a_up2
I0816 00:30:45.076643  9762 net.cpp:245] Setting up out5a_up2
I0816 00:30:45.076650  9762 net.cpp:252] TEST Top shape for layer 41 'out5a_up2' 4 64 80 80 (1638400)
I0816 00:30:45.076656  9762 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I0816 00:30:45.076660  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.076673  9762 net.cpp:184] Created Layer out3a (42)
I0816 00:30:45.076678  9762 net.cpp:561] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0816 00:30:45.076683  9762 net.cpp:530] out3a -> out3a
I0816 00:30:45.081637  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 7.84G, req 0G)
I0816 00:30:45.081648  9762 net.cpp:245] Setting up out3a
I0816 00:30:45.081655  9762 net.cpp:252] TEST Top shape for layer 42 'out3a' 4 64 80 80 (1638400)
I0816 00:30:45.081660  9762 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I0816 00:30:45.081665  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.081670  9762 net.cpp:184] Created Layer out3a/bn (43)
I0816 00:30:45.081673  9762 net.cpp:561] out3a/bn <- out3a
I0816 00:30:45.081677  9762 net.cpp:513] out3a/bn -> out3a (in-place)
I0816 00:30:45.082237  9762 net.cpp:245] Setting up out3a/bn
I0816 00:30:45.082247  9762 net.cpp:252] TEST Top shape for layer 43 'out3a/bn' 4 64 80 80 (1638400)
I0816 00:30:45.082254  9762 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I0816 00:30:45.082258  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.082262  9762 net.cpp:184] Created Layer out3a/relu (44)
I0816 00:30:45.082265  9762 net.cpp:561] out3a/relu <- out3a
I0816 00:30:45.082269  9762 net.cpp:513] out3a/relu -> out3a (in-place)
I0816 00:30:45.082274  9762 net.cpp:245] Setting up out3a/relu
I0816 00:30:45.082278  9762 net.cpp:252] TEST Top shape for layer 44 'out3a/relu' 4 64 80 80 (1638400)
I0816 00:30:45.082283  9762 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I0816 00:30:45.082286  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.082720  9762 net.cpp:184] Created Layer out3_out5_combined (45)
I0816 00:30:45.082726  9762 net.cpp:561] out3_out5_combined <- out5a_up2
I0816 00:30:45.082729  9762 net.cpp:561] out3_out5_combined <- out3a
I0816 00:30:45.082732  9762 net.cpp:530] out3_out5_combined -> out3_out5_combined
I0816 00:30:45.083721  9762 net.cpp:245] Setting up out3_out5_combined
I0816 00:30:45.083731  9762 net.cpp:252] TEST Top shape for layer 45 'out3_out5_combined' 4 64 80 80 (1638400)
I0816 00:30:45.083735  9762 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I0816 00:30:45.083739  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.083747  9762 net.cpp:184] Created Layer ctx_conv1 (46)
I0816 00:30:45.083751  9762 net.cpp:561] ctx_conv1 <- out3_out5_combined
I0816 00:30:45.083755  9762 net.cpp:530] ctx_conv1 -> ctx_conv1
I0816 00:30:45.088973  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 7.81G, req 0G)
I0816 00:30:45.088984  9762 net.cpp:245] Setting up ctx_conv1
I0816 00:30:45.088989  9762 net.cpp:252] TEST Top shape for layer 46 'ctx_conv1' 4 64 80 80 (1638400)
I0816 00:30:45.088995  9762 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I0816 00:30:45.088999  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.089013  9762 net.cpp:184] Created Layer ctx_conv1/bn (47)
I0816 00:30:45.089025  9762 net.cpp:561] ctx_conv1/bn <- ctx_conv1
I0816 00:30:45.089030  9762 net.cpp:513] ctx_conv1/bn -> ctx_conv1 (in-place)
I0816 00:30:45.089584  9762 net.cpp:245] Setting up ctx_conv1/bn
I0816 00:30:45.089593  9762 net.cpp:252] TEST Top shape for layer 47 'ctx_conv1/bn' 4 64 80 80 (1638400)
I0816 00:30:45.089601  9762 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I0816 00:30:45.089604  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.089608  9762 net.cpp:184] Created Layer ctx_conv1/relu (48)
I0816 00:30:45.089612  9762 net.cpp:561] ctx_conv1/relu <- ctx_conv1
I0816 00:30:45.089615  9762 net.cpp:513] ctx_conv1/relu -> ctx_conv1 (in-place)
I0816 00:30:45.089620  9762 net.cpp:245] Setting up ctx_conv1/relu
I0816 00:30:45.089624  9762 net.cpp:252] TEST Top shape for layer 48 'ctx_conv1/relu' 4 64 80 80 (1638400)
I0816 00:30:45.089628  9762 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I0816 00:30:45.089632  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.089645  9762 net.cpp:184] Created Layer ctx_conv2 (49)
I0816 00:30:45.089649  9762 net.cpp:561] ctx_conv2 <- ctx_conv1
I0816 00:30:45.089653  9762 net.cpp:530] ctx_conv2 -> ctx_conv2
I0816 00:30:45.090852  9762 net.cpp:245] Setting up ctx_conv2
I0816 00:30:45.090860  9762 net.cpp:252] TEST Top shape for layer 49 'ctx_conv2' 4 64 80 80 (1638400)
I0816 00:30:45.090867  9762 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I0816 00:30:45.090870  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.090875  9762 net.cpp:184] Created Layer ctx_conv2/bn (50)
I0816 00:30:45.090879  9762 net.cpp:561] ctx_conv2/bn <- ctx_conv2
I0816 00:30:45.090883  9762 net.cpp:513] ctx_conv2/bn -> ctx_conv2 (in-place)
I0816 00:30:45.091421  9762 net.cpp:245] Setting up ctx_conv2/bn
I0816 00:30:45.091429  9762 net.cpp:252] TEST Top shape for layer 50 'ctx_conv2/bn' 4 64 80 80 (1638400)
I0816 00:30:45.091444  9762 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I0816 00:30:45.091447  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.091451  9762 net.cpp:184] Created Layer ctx_conv2/relu (51)
I0816 00:30:45.091455  9762 net.cpp:561] ctx_conv2/relu <- ctx_conv2
I0816 00:30:45.091459  9762 net.cpp:513] ctx_conv2/relu -> ctx_conv2 (in-place)
I0816 00:30:45.091464  9762 net.cpp:245] Setting up ctx_conv2/relu
I0816 00:30:45.091469  9762 net.cpp:252] TEST Top shape for layer 51 'ctx_conv2/relu' 4 64 80 80 (1638400)
I0816 00:30:45.091472  9762 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I0816 00:30:45.091476  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.091483  9762 net.cpp:184] Created Layer ctx_conv3 (52)
I0816 00:30:45.091487  9762 net.cpp:561] ctx_conv3 <- ctx_conv2
I0816 00:30:45.091492  9762 net.cpp:530] ctx_conv3 -> ctx_conv3
I0816 00:30:45.092700  9762 net.cpp:245] Setting up ctx_conv3
I0816 00:30:45.092710  9762 net.cpp:252] TEST Top shape for layer 52 'ctx_conv3' 4 64 80 80 (1638400)
I0816 00:30:45.092716  9762 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I0816 00:30:45.092720  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.092726  9762 net.cpp:184] Created Layer ctx_conv3/bn (53)
I0816 00:30:45.092730  9762 net.cpp:561] ctx_conv3/bn <- ctx_conv3
I0816 00:30:45.092734  9762 net.cpp:513] ctx_conv3/bn -> ctx_conv3 (in-place)
I0816 00:30:45.093274  9762 net.cpp:245] Setting up ctx_conv3/bn
I0816 00:30:45.093282  9762 net.cpp:252] TEST Top shape for layer 53 'ctx_conv3/bn' 4 64 80 80 (1638400)
I0816 00:30:45.093291  9762 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I0816 00:30:45.093296  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.093307  9762 net.cpp:184] Created Layer ctx_conv3/relu (54)
I0816 00:30:45.093312  9762 net.cpp:561] ctx_conv3/relu <- ctx_conv3
I0816 00:30:45.093315  9762 net.cpp:513] ctx_conv3/relu -> ctx_conv3 (in-place)
I0816 00:30:45.093320  9762 net.cpp:245] Setting up ctx_conv3/relu
I0816 00:30:45.093325  9762 net.cpp:252] TEST Top shape for layer 54 'ctx_conv3/relu' 4 64 80 80 (1638400)
I0816 00:30:45.093329  9762 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I0816 00:30:45.093333  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.093346  9762 net.cpp:184] Created Layer ctx_conv4 (55)
I0816 00:30:45.093350  9762 net.cpp:561] ctx_conv4 <- ctx_conv3
I0816 00:30:45.093354  9762 net.cpp:530] ctx_conv4 -> ctx_conv4
I0816 00:30:45.094545  9762 net.cpp:245] Setting up ctx_conv4
I0816 00:30:45.094553  9762 net.cpp:252] TEST Top shape for layer 55 'ctx_conv4' 4 64 80 80 (1638400)
I0816 00:30:45.094559  9762 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I0816 00:30:45.094564  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.094570  9762 net.cpp:184] Created Layer ctx_conv4/bn (56)
I0816 00:30:45.094574  9762 net.cpp:561] ctx_conv4/bn <- ctx_conv4
I0816 00:30:45.094578  9762 net.cpp:513] ctx_conv4/bn -> ctx_conv4 (in-place)
I0816 00:30:45.095108  9762 net.cpp:245] Setting up ctx_conv4/bn
I0816 00:30:45.095115  9762 net.cpp:252] TEST Top shape for layer 56 'ctx_conv4/bn' 4 64 80 80 (1638400)
I0816 00:30:45.095124  9762 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I0816 00:30:45.095127  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.095132  9762 net.cpp:184] Created Layer ctx_conv4/relu (57)
I0816 00:30:45.095136  9762 net.cpp:561] ctx_conv4/relu <- ctx_conv4
I0816 00:30:45.095141  9762 net.cpp:513] ctx_conv4/relu -> ctx_conv4 (in-place)
I0816 00:30:45.095146  9762 net.cpp:245] Setting up ctx_conv4/relu
I0816 00:30:45.095151  9762 net.cpp:252] TEST Top shape for layer 57 'ctx_conv4/relu' 4 64 80 80 (1638400)
I0816 00:30:45.095155  9762 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I0816 00:30:45.095158  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.095172  9762 net.cpp:184] Created Layer ctx_final (58)
I0816 00:30:45.095176  9762 net.cpp:561] ctx_final <- ctx_conv4
I0816 00:30:45.095181  9762 net.cpp:530] ctx_final -> ctx_final
I0816 00:30:45.100643  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 7.8G, req 0G)
I0816 00:30:45.100654  9762 net.cpp:245] Setting up ctx_final
I0816 00:30:45.100659  9762 net.cpp:252] TEST Top shape for layer 58 'ctx_final' 4 8 80 80 (204800)
I0816 00:30:45.100666  9762 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I0816 00:30:45.100669  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.100673  9762 net.cpp:184] Created Layer ctx_final/relu (59)
I0816 00:30:45.100677  9762 net.cpp:561] ctx_final/relu <- ctx_final
I0816 00:30:45.100682  9762 net.cpp:513] ctx_final/relu -> ctx_final (in-place)
I0816 00:30:45.100687  9762 net.cpp:245] Setting up ctx_final/relu
I0816 00:30:45.100692  9762 net.cpp:252] TEST Top shape for layer 59 'ctx_final/relu' 4 8 80 80 (204800)
I0816 00:30:45.100694  9762 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I0816 00:30:45.100698  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.100705  9762 net.cpp:184] Created Layer out_deconv_final_up2 (60)
I0816 00:30:45.100709  9762 net.cpp:561] out_deconv_final_up2 <- ctx_final
I0816 00:30:45.100713  9762 net.cpp:530] out_deconv_final_up2 -> out_deconv_final_up2
I0816 00:30:45.100898  9762 net.cpp:245] Setting up out_deconv_final_up2
I0816 00:30:45.100905  9762 net.cpp:252] TEST Top shape for layer 60 'out_deconv_final_up2' 4 8 160 160 (819200)
I0816 00:30:45.100916  9762 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I0816 00:30:45.100920  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.100926  9762 net.cpp:184] Created Layer out_deconv_final_up4 (61)
I0816 00:30:45.100929  9762 net.cpp:561] out_deconv_final_up4 <- out_deconv_final_up2
I0816 00:30:45.100934  9762 net.cpp:530] out_deconv_final_up4 -> out_deconv_final_up4
I0816 00:30:45.101096  9762 net.cpp:245] Setting up out_deconv_final_up4
I0816 00:30:45.101104  9762 net.cpp:252] TEST Top shape for layer 61 'out_deconv_final_up4' 4 8 320 320 (3276800)
I0816 00:30:45.101109  9762 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I0816 00:30:45.101111  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.101125  9762 net.cpp:184] Created Layer out_deconv_final_up8 (62)
I0816 00:30:45.101130  9762 net.cpp:561] out_deconv_final_up8 <- out_deconv_final_up4
I0816 00:30:45.101132  9762 net.cpp:530] out_deconv_final_up8 -> out_deconv_final_up8
I0816 00:30:45.101296  9762 net.cpp:245] Setting up out_deconv_final_up8
I0816 00:30:45.101302  9762 net.cpp:252] TEST Top shape for layer 62 'out_deconv_final_up8' 4 8 640 640 (13107200)
I0816 00:30:45.101307  9762 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8_out_deconv_final_up8_0_split' of type 'Split'
I0816 00:30:45.101310  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.101315  9762 net.cpp:184] Created Layer out_deconv_final_up8_out_deconv_final_up8_0_split (63)
I0816 00:30:45.101318  9762 net.cpp:561] out_deconv_final_up8_out_deconv_final_up8_0_split <- out_deconv_final_up8
I0816 00:30:45.101322  9762 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0816 00:30:45.101326  9762 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0816 00:30:45.101330  9762 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0816 00:30:45.101379  9762 net.cpp:245] Setting up out_deconv_final_up8_out_deconv_final_up8_0_split
I0816 00:30:45.101385  9762 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 4 8 640 640 (13107200)
I0816 00:30:45.101389  9762 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 4 8 640 640 (13107200)
I0816 00:30:45.101393  9762 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 4 8 640 640 (13107200)
I0816 00:30:45.101397  9762 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0816 00:30:45.101400  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.101414  9762 net.cpp:184] Created Layer loss (64)
I0816 00:30:45.101418  9762 net.cpp:561] loss <- out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0816 00:30:45.101421  9762 net.cpp:561] loss <- label_data_1_split_0
I0816 00:30:45.101426  9762 net.cpp:530] loss -> loss
I0816 00:30:45.102613  9762 net.cpp:245] Setting up loss
I0816 00:30:45.102623  9762 net.cpp:252] TEST Top shape for layer 64 'loss' (1)
I0816 00:30:45.102627  9762 net.cpp:256]     with loss weight 1
I0816 00:30:45.102632  9762 layer_factory.hpp:136] Creating layer 'accuracy/top1' of type 'Accuracy'
I0816 00:30:45.102636  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.102643  9762 net.cpp:184] Created Layer accuracy/top1 (65)
I0816 00:30:45.102648  9762 net.cpp:561] accuracy/top1 <- out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0816 00:30:45.102651  9762 net.cpp:561] accuracy/top1 <- label_data_1_split_1
I0816 00:30:45.102656  9762 net.cpp:530] accuracy/top1 -> accuracy/top1
I0816 00:30:45.102675  9762 net.cpp:245] Setting up accuracy/top1
I0816 00:30:45.102680  9762 net.cpp:252] TEST Top shape for layer 65 'accuracy/top1' (1)
I0816 00:30:45.102684  9762 layer_factory.hpp:136] Creating layer 'accuracy/top5' of type 'Accuracy'
I0816 00:30:45.102686  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.102691  9762 net.cpp:184] Created Layer accuracy/top5 (66)
I0816 00:30:45.102694  9762 net.cpp:561] accuracy/top5 <- out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0816 00:30:45.102699  9762 net.cpp:561] accuracy/top5 <- label_data_1_split_2
I0816 00:30:45.102701  9762 net.cpp:530] accuracy/top5 -> accuracy/top5
I0816 00:30:45.102706  9762 net.cpp:245] Setting up accuracy/top5
I0816 00:30:45.102710  9762 net.cpp:252] TEST Top shape for layer 66 'accuracy/top5' (1)
I0816 00:30:45.102715  9762 net.cpp:325] accuracy/top5 does not need backward computation.
I0816 00:30:45.102718  9762 net.cpp:325] accuracy/top1 does not need backward computation.
I0816 00:30:45.102721  9762 net.cpp:323] loss needs backward computation.
I0816 00:30:45.102726  9762 net.cpp:323] out_deconv_final_up8_out_deconv_final_up8_0_split needs backward computation.
I0816 00:30:45.102730  9762 net.cpp:323] out_deconv_final_up8 needs backward computation.
I0816 00:30:45.102732  9762 net.cpp:323] out_deconv_final_up4 needs backward computation.
I0816 00:30:45.102736  9762 net.cpp:323] out_deconv_final_up2 needs backward computation.
I0816 00:30:45.102741  9762 net.cpp:323] ctx_final/relu needs backward computation.
I0816 00:30:45.102743  9762 net.cpp:323] ctx_final needs backward computation.
I0816 00:30:45.102747  9762 net.cpp:323] ctx_conv4/relu needs backward computation.
I0816 00:30:45.102751  9762 net.cpp:323] ctx_conv4/bn needs backward computation.
I0816 00:30:45.102753  9762 net.cpp:323] ctx_conv4 needs backward computation.
I0816 00:30:45.102757  9762 net.cpp:323] ctx_conv3/relu needs backward computation.
I0816 00:30:45.102761  9762 net.cpp:323] ctx_conv3/bn needs backward computation.
I0816 00:30:45.102764  9762 net.cpp:323] ctx_conv3 needs backward computation.
I0816 00:30:45.102767  9762 net.cpp:323] ctx_conv2/relu needs backward computation.
I0816 00:30:45.102771  9762 net.cpp:323] ctx_conv2/bn needs backward computation.
I0816 00:30:45.102776  9762 net.cpp:323] ctx_conv2 needs backward computation.
I0816 00:30:45.102778  9762 net.cpp:323] ctx_conv1/relu needs backward computation.
I0816 00:30:45.102782  9762 net.cpp:323] ctx_conv1/bn needs backward computation.
I0816 00:30:45.102785  9762 net.cpp:323] ctx_conv1 needs backward computation.
I0816 00:30:45.102789  9762 net.cpp:323] out3_out5_combined needs backward computation.
I0816 00:30:45.102793  9762 net.cpp:323] out3a/relu needs backward computation.
I0816 00:30:45.102797  9762 net.cpp:323] out3a/bn needs backward computation.
I0816 00:30:45.102802  9762 net.cpp:323] out3a needs backward computation.
I0816 00:30:45.102805  9762 net.cpp:323] out5a_up2 needs backward computation.
I0816 00:30:45.102808  9762 net.cpp:323] out5a/relu needs backward computation.
I0816 00:30:45.102813  9762 net.cpp:323] out5a/bn needs backward computation.
I0816 00:30:45.102816  9762 net.cpp:323] out5a needs backward computation.
I0816 00:30:45.102825  9762 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0816 00:30:45.102829  9762 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0816 00:30:45.102833  9762 net.cpp:323] res5a_branch2b needs backward computation.
I0816 00:30:45.102836  9762 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0816 00:30:45.102840  9762 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0816 00:30:45.102844  9762 net.cpp:323] res5a_branch2a needs backward computation.
I0816 00:30:45.102847  9762 net.cpp:323] pool4 needs backward computation.
I0816 00:30:45.102852  9762 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0816 00:30:45.102855  9762 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0816 00:30:45.102865  9762 net.cpp:323] res4a_branch2b needs backward computation.
I0816 00:30:45.102869  9762 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0816 00:30:45.102874  9762 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0816 00:30:45.102877  9762 net.cpp:323] res4a_branch2a needs backward computation.
I0816 00:30:45.102881  9762 net.cpp:323] pool3 needs backward computation.
I0816 00:30:45.102885  9762 net.cpp:323] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I0816 00:30:45.102890  9762 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0816 00:30:45.102893  9762 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0816 00:30:45.102897  9762 net.cpp:323] res3a_branch2b needs backward computation.
I0816 00:30:45.102900  9762 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0816 00:30:45.102905  9762 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0816 00:30:45.102908  9762 net.cpp:323] res3a_branch2a needs backward computation.
I0816 00:30:45.102912  9762 net.cpp:323] pool2 needs backward computation.
I0816 00:30:45.102916  9762 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0816 00:30:45.102921  9762 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0816 00:30:45.102923  9762 net.cpp:323] res2a_branch2b needs backward computation.
I0816 00:30:45.102927  9762 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0816 00:30:45.102931  9762 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0816 00:30:45.102936  9762 net.cpp:323] res2a_branch2a needs backward computation.
I0816 00:30:45.102939  9762 net.cpp:323] pool1 needs backward computation.
I0816 00:30:45.102942  9762 net.cpp:323] conv1b/relu needs backward computation.
I0816 00:30:45.102946  9762 net.cpp:323] conv1b/bn needs backward computation.
I0816 00:30:45.102951  9762 net.cpp:323] conv1b needs backward computation.
I0816 00:30:45.102954  9762 net.cpp:323] conv1a/relu needs backward computation.
I0816 00:30:45.102958  9762 net.cpp:323] conv1a/bn needs backward computation.
I0816 00:30:45.102962  9762 net.cpp:323] conv1a needs backward computation.
I0816 00:30:45.102967  9762 net.cpp:325] data/bias does not need backward computation.
I0816 00:30:45.102977  9762 net.cpp:325] label_data_1_split does not need backward computation.
I0816 00:30:45.102987  9762 net.cpp:325] data does not need backward computation.
I0816 00:30:45.102989  9762 net.cpp:367] This network produces output accuracy/top1
I0816 00:30:45.102993  9762 net.cpp:367] This network produces output accuracy/top5
I0816 00:30:45.102998  9762 net.cpp:367] This network produces output loss
I0816 00:30:45.103054  9762 net.cpp:389] Top memory (TEST) required for data: 637337600 diff: 8
I0816 00:30:45.103058  9762 net.cpp:392] Bottom memory (TEST) required for data: 637337600 diff: 637337600
I0816 00:30:45.103062  9762 net.cpp:395] Shared (in-place) memory (TEST) by data: 420249600 diff: 420249600
I0816 00:30:45.103065  9762 net.cpp:398] Parameters memory (TEST) required for data: 2692608 diff: 2692608
I0816 00:30:45.103070  9762 net.cpp:401] Parameters shared memory (TEST) by data: 0 diff: 0
I0816 00:30:45.103073  9762 net.cpp:407] Network initialization done.
I0816 00:30:45.108525  9762 net.cpp:1095] Copying source layer data Type:ImageLabelData #blobs=0
I0816 00:30:45.108547  9762 net.cpp:1095] Copying source layer data/bias Type:Bias #blobs=1
I0816 00:30:45.108584  9762 net.cpp:1095] Copying source layer conv1a Type:Convolution #blobs=2
I0816 00:30:45.108599  9762 net.cpp:1095] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0816 00:30:45.108907  9762 net.cpp:1095] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0816 00:30:45.108914  9762 net.cpp:1095] Copying source layer conv1b Type:Convolution #blobs=2
I0816 00:30:45.108927  9762 net.cpp:1095] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0816 00:30:45.109166  9762 net.cpp:1095] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0816 00:30:45.109174  9762 net.cpp:1095] Copying source layer pool1 Type:Pooling #blobs=0
I0816 00:30:45.109189  9762 net.cpp:1095] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0816 00:30:45.109210  9762 net.cpp:1095] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0816 00:30:45.109447  9762 net.cpp:1095] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0816 00:30:45.109452  9762 net.cpp:1095] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0816 00:30:45.109467  9762 net.cpp:1095] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0816 00:30:45.109697  9762 net.cpp:1095] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0816 00:30:45.109704  9762 net.cpp:1095] Copying source layer pool2 Type:Pooling #blobs=0
I0816 00:30:45.109707  9762 net.cpp:1095] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0816 00:30:45.109747  9762 net.cpp:1095] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0816 00:30:45.109961  9762 net.cpp:1095] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0816 00:30:45.109968  9762 net.cpp:1095] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0816 00:30:45.109997  9762 net.cpp:1095] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0816 00:30:45.110224  9762 net.cpp:1095] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0816 00:30:45.110230  9762 net.cpp:1095] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I0816 00:30:45.110234  9762 net.cpp:1095] Copying source layer pool3 Type:Pooling #blobs=0
I0816 00:30:45.110237  9762 net.cpp:1095] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0816 00:30:45.110360  9762 net.cpp:1095] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0816 00:30:45.110571  9762 net.cpp:1095] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0816 00:30:45.110579  9762 net.cpp:1095] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0816 00:30:45.110643  9762 net.cpp:1095] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0816 00:30:45.110837  9762 net.cpp:1095] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0816 00:30:45.110843  9762 net.cpp:1095] Copying source layer pool4 Type:Pooling #blobs=0
I0816 00:30:45.110847  9762 net.cpp:1095] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0816 00:30:45.111238  9762 net.cpp:1095] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0816 00:30:45.111448  9762 net.cpp:1095] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0816 00:30:45.111454  9762 net.cpp:1095] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0816 00:30:45.111620  9762 net.cpp:1095] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0816 00:30:45.111825  9762 net.cpp:1095] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0816 00:30:45.111832  9762 net.cpp:1095] Copying source layer out5a Type:Convolution #blobs=2
I0816 00:30:45.111886  9762 net.cpp:1095] Copying source layer out5a/bn Type:BatchNorm #blobs=5
I0816 00:30:45.111995  9762 net.cpp:1095] Copying source layer out5a/relu Type:ReLU #blobs=0
I0816 00:30:45.112001  9762 net.cpp:1095] Copying source layer out5a_up2 Type:Deconvolution #blobs=1
I0816 00:30:45.112009  9762 net.cpp:1095] Copying source layer out3a Type:Convolution #blobs=2
I0816 00:30:45.112028  9762 net.cpp:1095] Copying source layer out3a/bn Type:BatchNorm #blobs=5
I0816 00:30:45.112150  9762 net.cpp:1095] Copying source layer out3a/relu Type:ReLU #blobs=0
I0816 00:30:45.112157  9762 net.cpp:1095] Copying source layer out3_out5_combined Type:Eltwise #blobs=0
I0816 00:30:45.112161  9762 net.cpp:1095] Copying source layer ctx_conv1 Type:Convolution #blobs=2
I0816 00:30:45.112195  9762 net.cpp:1095] Copying source layer ctx_conv1/bn Type:BatchNorm #blobs=5
I0816 00:30:45.112306  9762 net.cpp:1095] Copying source layer ctx_conv1/relu Type:ReLU #blobs=0
I0816 00:30:45.112313  9762 net.cpp:1095] Copying source layer ctx_conv2 Type:Convolution #blobs=2
I0816 00:30:45.112340  9762 net.cpp:1095] Copying source layer ctx_conv2/bn Type:BatchNorm #blobs=5
I0816 00:30:45.112455  9762 net.cpp:1095] Copying source layer ctx_conv2/relu Type:ReLU #blobs=0
I0816 00:30:45.112462  9762 net.cpp:1095] Copying source layer ctx_conv3 Type:Convolution #blobs=2
I0816 00:30:45.112484  9762 net.cpp:1095] Copying source layer ctx_conv3/bn Type:BatchNorm #blobs=5
I0816 00:30:45.112601  9762 net.cpp:1095] Copying source layer ctx_conv3/relu Type:ReLU #blobs=0
I0816 00:30:45.112607  9762 net.cpp:1095] Copying source layer ctx_conv4 Type:Convolution #blobs=2
I0816 00:30:45.112627  9762 net.cpp:1095] Copying source layer ctx_conv4/bn Type:BatchNorm #blobs=5
I0816 00:30:45.112736  9762 net.cpp:1095] Copying source layer ctx_conv4/relu Type:ReLU #blobs=0
I0816 00:30:45.112742  9762 net.cpp:1095] Copying source layer ctx_final Type:Convolution #blobs=2
I0816 00:30:45.112754  9762 net.cpp:1095] Copying source layer ctx_final/relu Type:ReLU #blobs=0
I0816 00:30:45.112758  9762 net.cpp:1095] Copying source layer out_deconv_final_up2 Type:Deconvolution #blobs=1
I0816 00:30:45.112772  9762 net.cpp:1095] Copying source layer out_deconv_final_up4 Type:Deconvolution #blobs=1
I0816 00:30:45.112779  9762 net.cpp:1095] Copying source layer out_deconv_final_up8 Type:Deconvolution #blobs=1
I0816 00:30:45.112787  9762 net.cpp:1095] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0816 00:30:45.112874  9762 caffe.cpp:290] Running for 50 iterations.
I0816 00:30:45.118573  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.72G, req 0G)
I0816 00:30:45.138870  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.62G, req 0G)
I0816 00:30:45.155766  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.5G, req 0G)
I0816 00:30:45.169842  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.44G, req 0G)
I0816 00:30:45.177834  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.38G, req 0G)
I0816 00:30:45.183218  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.35G, req 0G)
I0816 00:30:45.190201  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.33G, req 0G)
I0816 00:30:45.193990  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.32G, req 0G)
I0816 00:30:45.218603  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 7.09G, req 0G)
I0816 00:30:45.223830  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 7.07G, req 0G)
I0816 00:30:45.240011  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.94G, req 0G)
I0816 00:30:45.396050  9762 caffe.cpp:313] Batch 0, accuracy/top1 = 0.930066
I0816 00:30:45.396071  9762 caffe.cpp:313] Batch 0, accuracy/top5 = 1
I0816 00:30:45.396075  9762 caffe.cpp:313] Batch 0, loss = 0.220756
I0816 00:30:45.402719  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 1.22G/1 1  (limit 5.49G, req 0G)
I0816 00:30:45.426745  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 2.44G/2 6  (limit 4.27G, req 0G)
I0816 00:30:45.472440  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 2.44G/1 6  (limit 4.27G, req 0G)
I0816 00:30:45.488504  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 2.44G/2 6  (limit 4.27G, req 0G)
I0816 00:30:45.524411  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 2.44G/1 6  (limit 4.27G, req 0G)
I0816 00:30:45.533339  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 2.44G/2 6  (limit 4.27G, req 0G)
I0816 00:30:45.554831  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 2.44G/1 6  (limit 4.27G, req 0G)
I0816 00:30:45.560751  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 2.44G/2 6  (limit 4.27G, req 0G)
I0816 00:30:45.585239  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'out3a' with space 2.44G/2 6  (limit 4.27G, req 0G)
I0816 00:30:45.605265  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_conv1' with space 2.44G/1 6  (limit 4.27G, req 0G)
I0816 00:30:45.617367  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_final' with space 2.44G/1 6  (limit 4.27G, req 0G)
I0816 00:30:45.751566  9762 caffe.cpp:313] Batch 1, accuracy/top1 = 0.955384
I0816 00:30:45.751590  9762 caffe.cpp:313] Batch 1, accuracy/top5 = 1
I0816 00:30:45.751592  9762 caffe.cpp:313] Batch 1, loss = 0.141199
I0816 00:30:45.915011  9762 caffe.cpp:313] Batch 2, accuracy/top1 = 0.959618
I0816 00:30:45.915033  9762 caffe.cpp:313] Batch 2, accuracy/top5 = 1
I0816 00:30:45.915036  9762 caffe.cpp:313] Batch 2, loss = 0.112921
I0816 00:30:46.080250  9762 caffe.cpp:313] Batch 3, accuracy/top1 = 0.972609
I0816 00:30:46.080269  9762 caffe.cpp:313] Batch 3, accuracy/top5 = 0.999996
I0816 00:30:46.080272  9762 caffe.cpp:313] Batch 3, loss = 0.0760811
I0816 00:30:46.243175  9762 caffe.cpp:313] Batch 4, accuracy/top1 = 0.962518
I0816 00:30:46.243196  9762 caffe.cpp:313] Batch 4, accuracy/top5 = 0.999993
I0816 00:30:46.243199  9762 caffe.cpp:313] Batch 4, loss = 0.120001
I0816 00:30:46.407047  9762 caffe.cpp:313] Batch 5, accuracy/top1 = 0.813128
I0816 00:30:46.407068  9762 caffe.cpp:313] Batch 5, accuracy/top5 = 1
I0816 00:30:46.407071  9762 caffe.cpp:313] Batch 5, loss = 1.02897
I0816 00:30:46.572259  9762 caffe.cpp:313] Batch 6, accuracy/top1 = 0.962231
I0816 00:30:46.572278  9762 caffe.cpp:313] Batch 6, accuracy/top5 = 1
I0816 00:30:46.572280  9762 caffe.cpp:313] Batch 6, loss = 0.0979562
I0816 00:30:46.738240  9762 caffe.cpp:313] Batch 7, accuracy/top1 = 0.962217
I0816 00:30:46.738261  9762 caffe.cpp:313] Batch 7, accuracy/top5 = 1
I0816 00:30:46.738265  9762 caffe.cpp:313] Batch 7, loss = 0.0962906
I0816 00:30:46.900099  9762 caffe.cpp:313] Batch 8, accuracy/top1 = 0.974926
I0816 00:30:46.900121  9762 caffe.cpp:313] Batch 8, accuracy/top5 = 1
I0816 00:30:46.900125  9762 caffe.cpp:313] Batch 8, loss = 0.0665642
I0816 00:30:47.065760  9762 caffe.cpp:313] Batch 9, accuracy/top1 = 0.982213
I0816 00:30:47.065778  9762 caffe.cpp:313] Batch 9, accuracy/top5 = 1
I0816 00:30:47.065781  9762 caffe.cpp:313] Batch 9, loss = 0.0482233
I0816 00:30:47.231524  9762 caffe.cpp:313] Batch 10, accuracy/top1 = 0.895045
I0816 00:30:47.231549  9762 caffe.cpp:313] Batch 10, accuracy/top5 = 1
I0816 00:30:47.231552  9762 caffe.cpp:313] Batch 10, loss = 0.286693
I0816 00:30:47.399586  9762 caffe.cpp:313] Batch 11, accuracy/top1 = 0.97725
I0816 00:30:47.399605  9762 caffe.cpp:313] Batch 11, accuracy/top5 = 1
I0816 00:30:47.399608  9762 caffe.cpp:313] Batch 11, loss = 0.0629691
I0816 00:30:47.565316  9762 caffe.cpp:313] Batch 12, accuracy/top1 = 0.965007
I0816 00:30:47.565335  9762 caffe.cpp:313] Batch 12, accuracy/top5 = 1
I0816 00:30:47.565337  9762 caffe.cpp:313] Batch 12, loss = 0.0938991
I0816 00:30:47.729823  9762 caffe.cpp:313] Batch 13, accuracy/top1 = 0.979467
I0816 00:30:47.729843  9762 caffe.cpp:313] Batch 13, accuracy/top5 = 1
I0816 00:30:47.729846  9762 caffe.cpp:313] Batch 13, loss = 0.0551973
I0816 00:30:47.894198  9762 caffe.cpp:313] Batch 14, accuracy/top1 = 0.977653
I0816 00:30:47.894222  9762 caffe.cpp:313] Batch 14, accuracy/top5 = 1
I0816 00:30:47.894224  9762 caffe.cpp:313] Batch 14, loss = 0.0572141
I0816 00:30:48.059252  9762 caffe.cpp:313] Batch 15, accuracy/top1 = 0.962654
I0816 00:30:48.059270  9762 caffe.cpp:313] Batch 15, accuracy/top5 = 1
I0816 00:30:48.059274  9762 caffe.cpp:313] Batch 15, loss = 0.10314
I0816 00:30:48.223881  9762 caffe.cpp:313] Batch 16, accuracy/top1 = 0.899707
I0816 00:30:48.223902  9762 caffe.cpp:313] Batch 16, accuracy/top5 = 1
I0816 00:30:48.223906  9762 caffe.cpp:313] Batch 16, loss = 0.388575
I0816 00:30:48.387871  9762 caffe.cpp:313] Batch 17, accuracy/top1 = 0.87246
I0816 00:30:48.387892  9762 caffe.cpp:313] Batch 17, accuracy/top5 = 1
I0816 00:30:48.387907  9762 caffe.cpp:313] Batch 17, loss = 0.601822
I0816 00:30:48.554101  9762 caffe.cpp:313] Batch 18, accuracy/top1 = 0.982779
I0816 00:30:48.554119  9762 caffe.cpp:313] Batch 18, accuracy/top5 = 0.99999
I0816 00:30:48.554122  9762 caffe.cpp:313] Batch 18, loss = 0.0439325
I0816 00:30:48.719720  9762 caffe.cpp:313] Batch 19, accuracy/top1 = 0.982094
I0816 00:30:48.719743  9762 caffe.cpp:313] Batch 19, accuracy/top5 = 1
I0816 00:30:48.719745  9762 caffe.cpp:313] Batch 19, loss = 0.051014
I0816 00:30:48.885210  9762 caffe.cpp:313] Batch 20, accuracy/top1 = 0.975827
I0816 00:30:48.885231  9762 caffe.cpp:313] Batch 20, accuracy/top5 = 1
I0816 00:30:48.885233  9762 caffe.cpp:313] Batch 20, loss = 0.0692319
I0816 00:30:49.048382  9762 caffe.cpp:313] Batch 21, accuracy/top1 = 0.891715
I0816 00:30:49.048399  9762 caffe.cpp:313] Batch 21, accuracy/top5 = 0.9999
I0816 00:30:49.048403  9762 caffe.cpp:313] Batch 21, loss = 0.604256
I0816 00:30:49.213877  9762 caffe.cpp:313] Batch 22, accuracy/top1 = 0.967383
I0816 00:30:49.213901  9762 caffe.cpp:313] Batch 22, accuracy/top5 = 1
I0816 00:30:49.213904  9762 caffe.cpp:313] Batch 22, loss = 0.088235
I0816 00:30:49.379048  9762 caffe.cpp:313] Batch 23, accuracy/top1 = 0.978001
I0816 00:30:49.379065  9762 caffe.cpp:313] Batch 23, accuracy/top5 = 1
I0816 00:30:49.379068  9762 caffe.cpp:313] Batch 23, loss = 0.0590779
I0816 00:30:49.545467  9762 caffe.cpp:313] Batch 24, accuracy/top1 = 0.950794
I0816 00:30:49.545485  9762 caffe.cpp:313] Batch 24, accuracy/top5 = 1
I0816 00:30:49.545488  9762 caffe.cpp:313] Batch 24, loss = 0.128335
I0816 00:30:49.713721  9762 caffe.cpp:313] Batch 25, accuracy/top1 = 0.97273
I0816 00:30:49.713744  9762 caffe.cpp:313] Batch 25, accuracy/top5 = 1
I0816 00:30:49.713747  9762 caffe.cpp:313] Batch 25, loss = 0.074232
I0816 00:30:49.876585  9762 caffe.cpp:313] Batch 26, accuracy/top1 = 0.952759
I0816 00:30:49.876607  9762 caffe.cpp:313] Batch 26, accuracy/top5 = 1
I0816 00:30:49.876610  9762 caffe.cpp:313] Batch 26, loss = 0.118792
I0816 00:30:50.041847  9762 caffe.cpp:313] Batch 27, accuracy/top1 = 0.966694
I0816 00:30:50.041867  9762 caffe.cpp:313] Batch 27, accuracy/top5 = 1
I0816 00:30:50.041869  9762 caffe.cpp:313] Batch 27, loss = 0.0944889
I0816 00:30:50.206145  9762 caffe.cpp:313] Batch 28, accuracy/top1 = 0.953026
I0816 00:30:50.206167  9762 caffe.cpp:313] Batch 28, accuracy/top5 = 1
I0816 00:30:50.206171  9762 caffe.cpp:313] Batch 28, loss = 0.125202
I0816 00:30:50.371675  9762 caffe.cpp:313] Batch 29, accuracy/top1 = 0.965533
I0816 00:30:50.371698  9762 caffe.cpp:313] Batch 29, accuracy/top5 = 1
I0816 00:30:50.371701  9762 caffe.cpp:313] Batch 29, loss = 0.104905
I0816 00:30:50.536644  9762 caffe.cpp:313] Batch 30, accuracy/top1 = 0.861784
I0816 00:30:50.536665  9762 caffe.cpp:313] Batch 30, accuracy/top5 = 1
I0816 00:30:50.536669  9762 caffe.cpp:313] Batch 30, loss = 0.644569
I0816 00:30:50.700757  9762 caffe.cpp:313] Batch 31, accuracy/top1 = 0.968184
I0816 00:30:50.700778  9762 caffe.cpp:313] Batch 31, accuracy/top5 = 1
I0816 00:30:50.700781  9762 caffe.cpp:313] Batch 31, loss = 0.0884397
I0816 00:30:50.864923  9762 caffe.cpp:313] Batch 32, accuracy/top1 = 0.950449
I0816 00:30:50.864944  9762 caffe.cpp:313] Batch 32, accuracy/top5 = 1
I0816 00:30:50.864948  9762 caffe.cpp:313] Batch 32, loss = 0.134578
I0816 00:30:51.030355  9762 caffe.cpp:313] Batch 33, accuracy/top1 = 0.967974
I0816 00:30:51.030374  9762 caffe.cpp:313] Batch 33, accuracy/top5 = 1
I0816 00:30:51.030376  9762 caffe.cpp:313] Batch 33, loss = 0.0850275
I0816 00:30:51.197466  9762 caffe.cpp:313] Batch 34, accuracy/top1 = 0.97725
I0816 00:30:51.197491  9762 caffe.cpp:313] Batch 34, accuracy/top5 = 1
I0816 00:30:51.197494  9762 caffe.cpp:313] Batch 34, loss = 0.0645503
I0816 00:30:51.360913  9762 caffe.cpp:313] Batch 35, accuracy/top1 = 0.975526
I0816 00:30:51.360935  9762 caffe.cpp:313] Batch 35, accuracy/top5 = 1
I0816 00:30:51.360939  9762 caffe.cpp:313] Batch 35, loss = 0.0668076
I0816 00:30:51.525363  9762 caffe.cpp:313] Batch 36, accuracy/top1 = 0.963138
I0816 00:30:51.525393  9762 caffe.cpp:313] Batch 36, accuracy/top5 = 1
I0816 00:30:51.525398  9762 caffe.cpp:313] Batch 36, loss = 0.104859
I0816 00:30:51.688918  9762 caffe.cpp:313] Batch 37, accuracy/top1 = 0.962077
I0816 00:30:51.688940  9762 caffe.cpp:313] Batch 37, accuracy/top5 = 1
I0816 00:30:51.688942  9762 caffe.cpp:313] Batch 37, loss = 0.113325
I0816 00:30:51.853919  9762 caffe.cpp:313] Batch 38, accuracy/top1 = 0.941712
I0816 00:30:51.853942  9762 caffe.cpp:313] Batch 38, accuracy/top5 = 1
I0816 00:30:51.853945  9762 caffe.cpp:313] Batch 38, loss = 0.174112
I0816 00:30:52.015498  9762 caffe.cpp:313] Batch 39, accuracy/top1 = 0.916295
I0816 00:30:52.015521  9762 caffe.cpp:313] Batch 39, accuracy/top5 = 1
I0816 00:30:52.015523  9762 caffe.cpp:313] Batch 39, loss = 0.230812
I0816 00:30:52.182013  9762 caffe.cpp:313] Batch 40, accuracy/top1 = 0.980853
I0816 00:30:52.182041  9762 caffe.cpp:313] Batch 40, accuracy/top5 = 1
I0816 00:30:52.182045  9762 caffe.cpp:313] Batch 40, loss = 0.0588717
I0816 00:30:52.347806  9762 caffe.cpp:313] Batch 41, accuracy/top1 = 0.976998
I0816 00:30:52.347829  9762 caffe.cpp:313] Batch 41, accuracy/top5 = 1
I0816 00:30:52.347832  9762 caffe.cpp:313] Batch 41, loss = 0.0659184
I0816 00:30:52.514281  9762 caffe.cpp:313] Batch 42, accuracy/top1 = 0.972377
I0816 00:30:52.514299  9762 caffe.cpp:313] Batch 42, accuracy/top5 = 1
I0816 00:30:52.514302  9762 caffe.cpp:313] Batch 42, loss = 0.0748789
I0816 00:30:52.680268  9762 caffe.cpp:313] Batch 43, accuracy/top1 = 0.977115
I0816 00:30:52.680289  9762 caffe.cpp:313] Batch 43, accuracy/top5 = 1
I0816 00:30:52.680291  9762 caffe.cpp:313] Batch 43, loss = 0.0663094
I0816 00:30:52.846160  9762 caffe.cpp:313] Batch 44, accuracy/top1 = 0.958332
I0816 00:30:52.846184  9762 caffe.cpp:313] Batch 44, accuracy/top5 = 1
I0816 00:30:52.846186  9762 caffe.cpp:313] Batch 44, loss = 0.11734
I0816 00:30:53.011114  9762 caffe.cpp:313] Batch 45, accuracy/top1 = 0.976915
I0816 00:30:53.011137  9762 caffe.cpp:313] Batch 45, accuracy/top5 = 1
I0816 00:30:53.011139  9762 caffe.cpp:313] Batch 45, loss = 0.0723509
I0816 00:30:53.177412  9762 caffe.cpp:313] Batch 46, accuracy/top1 = 0.972087
I0816 00:30:53.177431  9762 caffe.cpp:313] Batch 46, accuracy/top5 = 1
I0816 00:30:53.177434  9762 caffe.cpp:313] Batch 46, loss = 0.0759381
I0816 00:30:53.344467  9762 caffe.cpp:313] Batch 47, accuracy/top1 = 0.967969
I0816 00:30:53.344491  9762 caffe.cpp:313] Batch 47, accuracy/top5 = 1
I0816 00:30:53.344493  9762 caffe.cpp:313] Batch 47, loss = 0.118059
I0816 00:30:53.508762  9762 caffe.cpp:313] Batch 48, accuracy/top1 = 0.876619
I0816 00:30:53.508780  9762 caffe.cpp:313] Batch 48, accuracy/top5 = 1
I0816 00:30:53.508782  9762 caffe.cpp:313] Batch 48, loss = 0.465836
I0816 00:30:53.670256  9762 caffe.cpp:313] Batch 49, accuracy/top1 = 0.950034
I0816 00:30:53.670277  9762 caffe.cpp:313] Batch 49, accuracy/top5 = 1
I0816 00:30:53.670280  9762 caffe.cpp:313] Batch 49, loss = 0.135004
I0816 00:30:53.670284  9762 caffe.cpp:318] Loss: 0.163555
I0816 00:30:53.670290  9762 caffe.cpp:330] accuracy/top1 = 0.952743
I0816 00:30:53.670294  9762 caffe.cpp:330] accuracy/top5 = 0.999998
I0816 00:30:53.670298  9762 caffe.cpp:330] loss = 0.163555 (* 1 = 0.163555 loss)
I0816 15:04:49.231748 21700 caffe.cpp:608] This is NVCaffe 0.16.3 started at Wed Aug 16 15:04:49 2017
I0816 15:04:49.231879 21700 caffe.cpp:611] CuDNN version: 6021
I0816 15:04:49.231884 21700 caffe.cpp:612] CuBLAS version: 8000
I0816 15:04:49.231884 21700 caffe.cpp:613] CUDA version: 8000
I0816 15:04:49.231886 21700 caffe.cpp:614] CUDA driver version: 8000
I0816 15:04:49.231892 21700 caffe.cpp:263] Not using GPU #2 for single-GPU function
I0816 15:04:49.231894 21700 caffe.cpp:263] Not using GPU #1 for single-GPU function
I0816 15:04:49.232491 21700 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0816 15:04:49.233098 21700 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I0816 15:04:49.233103 21700 caffe.cpp:275] Use GPU with device ID 0
I0816 15:04:49.233486 21700 caffe.cpp:279] GPU device name: GeForce GTX 1080
I0816 15:04:49.234958 21700 net.cpp:72] Initializing net from parameters: 
name: "jsegnet21v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/val-image-lmdb"
    label_list_path: "data/val-label-lmdb"
    batch_size: 4
    threads: 1
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
  accuracy_param {
    ignore_label: 255
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
    ignore_label: 255
  }
}
quantize: true
I0816 15:04:49.235100 21700 net.cpp:104] Using FLOAT as default forward math type
I0816 15:04:49.235105 21700 net.cpp:110] Using FLOAT as default backward math type
I0816 15:04:49.235107 21700 layer_factory.hpp:136] Creating layer 'data' of type 'ImageLabelData'
I0816 15:04:49.235110 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.235121 21700 net.cpp:184] Created Layer data (0)
I0816 15:04:49.235126 21700 net.cpp:530] data -> data
I0816 15:04:49.235136 21700 net.cpp:530] data -> label
I0816 15:04:49.235471 21700 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 4
I0816 15:04:49.235486 21700 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0816 15:04:49.242686 21722 db_lmdb.cpp:24] Opened lmdb data/val-image-lmdb
I0816 15:04:49.244652 21700 data_layer.cpp:185] (0) ReshapePrefetch 4, 3, 640, 640
I0816 15:04:49.244701 21700 data_layer.cpp:209] (0) Output data size: 4, 3, 640, 640
I0816 15:04:49.244709 21700 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0816 15:04:49.244772 21700 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 4
I0816 15:04:49.244786 21700 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0816 15:04:49.245525 21723 data_layer.cpp:97] (0) Parser threads: 1
I0816 15:04:49.245535 21723 data_layer.cpp:99] (0) Transformer threads: 1
I0816 15:04:49.249060 21724 db_lmdb.cpp:24] Opened lmdb data/val-label-lmdb
I0816 15:04:49.250963 21700 data_layer.cpp:185] (0) ReshapePrefetch 4, 1, 640, 640
I0816 15:04:49.251037 21700 data_layer.cpp:209] (0) Output data size: 4, 1, 640, 640
I0816 15:04:49.251047 21700 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0816 15:04:49.251224 21700 net.cpp:245] Setting up data
I0816 15:04:49.251266 21700 net.cpp:252] TEST Top shape for layer 0 'data' 4 3 640 640 (4915200)
I0816 15:04:49.251304 21700 net.cpp:252] TEST Top shape for layer 0 'data' 4 1 640 640 (1638400)
I0816 15:04:49.251317 21700 layer_factory.hpp:136] Creating layer 'label_data_1_split' of type 'Split'
I0816 15:04:49.251325 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.251361 21700 net.cpp:184] Created Layer label_data_1_split (1)
I0816 15:04:49.251372 21700 net.cpp:561] label_data_1_split <- label
I0816 15:04:49.251389 21700 net.cpp:530] label_data_1_split -> label_data_1_split_0
I0816 15:04:49.251397 21700 net.cpp:530] label_data_1_split -> label_data_1_split_1
I0816 15:04:49.251404 21700 net.cpp:530] label_data_1_split -> label_data_1_split_2
I0816 15:04:49.251489 21700 net.cpp:245] Setting up label_data_1_split
I0816 15:04:49.251498 21700 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 4 1 640 640 (1638400)
I0816 15:04:49.251503 21700 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 4 1 640 640 (1638400)
I0816 15:04:49.251508 21700 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 4 1 640 640 (1638400)
I0816 15:04:49.251512 21700 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0816 15:04:49.251516 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.251530 21700 net.cpp:184] Created Layer data/bias (2)
I0816 15:04:49.251534 21700 net.cpp:561] data/bias <- data
I0816 15:04:49.251538 21700 net.cpp:530] data/bias -> data/bias
I0816 15:04:49.253116 21725 data_layer.cpp:97] (0) Parser threads: 1
I0816 15:04:49.253208 21725 data_layer.cpp:99] (0) Transformer threads: 1
I0816 15:04:49.255545 21700 net.cpp:245] Setting up data/bias
I0816 15:04:49.255611 21700 net.cpp:252] TEST Top shape for layer 2 'data/bias' 4 3 640 640 (4915200)
I0816 15:04:49.255643 21700 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0816 15:04:49.255653 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.255697 21700 net.cpp:184] Created Layer conv1a (3)
I0816 15:04:49.255703 21700 net.cpp:561] conv1a <- data/bias
I0816 15:04:49.255712 21700 net.cpp:530] conv1a -> conv1a
I0816 15:04:49.547052 21700 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.01G/1 1  (limit 8.06G, req 0G)
I0816 15:04:49.547078 21700 net.cpp:245] Setting up conv1a
I0816 15:04:49.547086 21700 net.cpp:252] TEST Top shape for layer 3 'conv1a' 4 32 320 320 (13107200)
I0816 15:04:49.547099 21700 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0816 15:04:49.547106 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.547121 21700 net.cpp:184] Created Layer conv1a/bn (4)
I0816 15:04:49.547128 21700 net.cpp:561] conv1a/bn <- conv1a
I0816 15:04:49.547134 21700 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0816 15:04:49.547791 21700 net.cpp:245] Setting up conv1a/bn
I0816 15:04:49.547802 21700 net.cpp:252] TEST Top shape for layer 4 'conv1a/bn' 4 32 320 320 (13107200)
I0816 15:04:49.547814 21700 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0816 15:04:49.547819 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.547825 21700 net.cpp:184] Created Layer conv1a/relu (5)
I0816 15:04:49.547829 21700 net.cpp:561] conv1a/relu <- conv1a
I0816 15:04:49.547833 21700 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0816 15:04:49.547848 21700 net.cpp:245] Setting up conv1a/relu
I0816 15:04:49.547855 21700 net.cpp:252] TEST Top shape for layer 5 'conv1a/relu' 4 32 320 320 (13107200)
I0816 15:04:49.547859 21700 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0816 15:04:49.547863 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.547874 21700 net.cpp:184] Created Layer conv1b (6)
I0816 15:04:49.547879 21700 net.cpp:561] conv1b <- conv1a
I0816 15:04:49.547883 21700 net.cpp:530] conv1b -> conv1b
I0816 15:04:49.563792 21700 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 8G, req 0G)
I0816 15:04:49.563817 21700 net.cpp:245] Setting up conv1b
I0816 15:04:49.563825 21700 net.cpp:252] TEST Top shape for layer 6 'conv1b' 4 32 320 320 (13107200)
I0816 15:04:49.563838 21700 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0816 15:04:49.563844 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.563855 21700 net.cpp:184] Created Layer conv1b/bn (7)
I0816 15:04:49.563861 21700 net.cpp:561] conv1b/bn <- conv1b
I0816 15:04:49.563866 21700 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0816 15:04:49.564527 21700 net.cpp:245] Setting up conv1b/bn
I0816 15:04:49.564539 21700 net.cpp:252] TEST Top shape for layer 7 'conv1b/bn' 4 32 320 320 (13107200)
I0816 15:04:49.564550 21700 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0816 15:04:49.564555 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.564561 21700 net.cpp:184] Created Layer conv1b/relu (8)
I0816 15:04:49.564565 21700 net.cpp:561] conv1b/relu <- conv1b
I0816 15:04:49.564569 21700 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0816 15:04:49.564576 21700 net.cpp:245] Setting up conv1b/relu
I0816 15:04:49.564581 21700 net.cpp:252] TEST Top shape for layer 8 'conv1b/relu' 4 32 320 320 (13107200)
I0816 15:04:49.564585 21700 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0816 15:04:49.564589 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.564597 21700 net.cpp:184] Created Layer pool1 (9)
I0816 15:04:49.564601 21700 net.cpp:561] pool1 <- conv1b
I0816 15:04:49.564605 21700 net.cpp:530] pool1 -> pool1
I0816 15:04:49.564673 21700 net.cpp:245] Setting up pool1
I0816 15:04:49.564682 21700 net.cpp:252] TEST Top shape for layer 9 'pool1' 4 32 160 160 (3276800)
I0816 15:04:49.564687 21700 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0816 15:04:49.564690 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.564715 21700 net.cpp:184] Created Layer res2a_branch2a (10)
I0816 15:04:49.564723 21700 net.cpp:561] res2a_branch2a <- pool1
I0816 15:04:49.564726 21700 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0816 15:04:49.576880 21700 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 1  (limit 7.95G, req 0G)
I0816 15:04:49.576908 21700 net.cpp:245] Setting up res2a_branch2a
I0816 15:04:49.576917 21700 net.cpp:252] TEST Top shape for layer 10 'res2a_branch2a' 4 64 160 160 (6553600)
I0816 15:04:49.576932 21700 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0816 15:04:49.576938 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.576953 21700 net.cpp:184] Created Layer res2a_branch2a/bn (11)
I0816 15:04:49.576961 21700 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0816 15:04:49.576967 21700 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0816 15:04:49.577625 21700 net.cpp:245] Setting up res2a_branch2a/bn
I0816 15:04:49.577636 21700 net.cpp:252] TEST Top shape for layer 11 'res2a_branch2a/bn' 4 64 160 160 (6553600)
I0816 15:04:49.577646 21700 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0816 15:04:49.577651 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.577657 21700 net.cpp:184] Created Layer res2a_branch2a/relu (12)
I0816 15:04:49.577661 21700 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0816 15:04:49.577667 21700 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0816 15:04:49.577674 21700 net.cpp:245] Setting up res2a_branch2a/relu
I0816 15:04:49.577680 21700 net.cpp:252] TEST Top shape for layer 12 'res2a_branch2a/relu' 4 64 160 160 (6553600)
I0816 15:04:49.577683 21700 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0816 15:04:49.577687 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.577700 21700 net.cpp:184] Created Layer res2a_branch2b (13)
I0816 15:04:49.577705 21700 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0816 15:04:49.577709 21700 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0816 15:04:49.586165 21700 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.92G, req 0G)
I0816 15:04:49.586187 21700 net.cpp:245] Setting up res2a_branch2b
I0816 15:04:49.586195 21700 net.cpp:252] TEST Top shape for layer 13 'res2a_branch2b' 4 64 160 160 (6553600)
I0816 15:04:49.586205 21700 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0816 15:04:49.586211 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.586225 21700 net.cpp:184] Created Layer res2a_branch2b/bn (14)
I0816 15:04:49.586230 21700 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0816 15:04:49.586236 21700 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0816 15:04:49.586856 21700 net.cpp:245] Setting up res2a_branch2b/bn
I0816 15:04:49.586869 21700 net.cpp:252] TEST Top shape for layer 14 'res2a_branch2b/bn' 4 64 160 160 (6553600)
I0816 15:04:49.586877 21700 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0816 15:04:49.586881 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.586886 21700 net.cpp:184] Created Layer res2a_branch2b/relu (15)
I0816 15:04:49.586891 21700 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0816 15:04:49.586895 21700 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0816 15:04:49.586901 21700 net.cpp:245] Setting up res2a_branch2b/relu
I0816 15:04:49.586906 21700 net.cpp:252] TEST Top shape for layer 15 'res2a_branch2b/relu' 4 64 160 160 (6553600)
I0816 15:04:49.586910 21700 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0816 15:04:49.586915 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.586946 21700 net.cpp:184] Created Layer pool2 (16)
I0816 15:04:49.586952 21700 net.cpp:561] pool2 <- res2a_branch2b
I0816 15:04:49.586956 21700 net.cpp:530] pool2 -> pool2
I0816 15:04:49.586992 21700 net.cpp:245] Setting up pool2
I0816 15:04:49.586998 21700 net.cpp:252] TEST Top shape for layer 16 'pool2' 4 64 80 80 (1638400)
I0816 15:04:49.587002 21700 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0816 15:04:49.587007 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.587018 21700 net.cpp:184] Created Layer res3a_branch2a (17)
I0816 15:04:49.587020 21700 net.cpp:561] res3a_branch2a <- pool2
I0816 15:04:49.587024 21700 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0816 15:04:49.593653 21700 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.9G, req 0G)
I0816 15:04:49.593668 21700 net.cpp:245] Setting up res3a_branch2a
I0816 15:04:49.593674 21700 net.cpp:252] TEST Top shape for layer 17 'res3a_branch2a' 4 128 80 80 (3276800)
I0816 15:04:49.593683 21700 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0816 15:04:49.593688 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.593696 21700 net.cpp:184] Created Layer res3a_branch2a/bn (18)
I0816 15:04:49.593700 21700 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0816 15:04:49.593704 21700 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0816 15:04:49.594483 21700 net.cpp:245] Setting up res3a_branch2a/bn
I0816 15:04:49.594492 21700 net.cpp:252] TEST Top shape for layer 18 'res3a_branch2a/bn' 4 128 80 80 (3276800)
I0816 15:04:49.594504 21700 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0816 15:04:49.594508 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.594513 21700 net.cpp:184] Created Layer res3a_branch2a/relu (19)
I0816 15:04:49.594517 21700 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0816 15:04:49.594521 21700 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0816 15:04:49.594527 21700 net.cpp:245] Setting up res3a_branch2a/relu
I0816 15:04:49.594533 21700 net.cpp:252] TEST Top shape for layer 19 'res3a_branch2a/relu' 4 128 80 80 (3276800)
I0816 15:04:49.594537 21700 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0816 15:04:49.594540 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.594550 21700 net.cpp:184] Created Layer res3a_branch2b (20)
I0816 15:04:49.594554 21700 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0816 15:04:49.594558 21700 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0816 15:04:49.599122 21700 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.88G, req 0G)
I0816 15:04:49.599133 21700 net.cpp:245] Setting up res3a_branch2b
I0816 15:04:49.599136 21700 net.cpp:252] TEST Top shape for layer 20 'res3a_branch2b' 4 128 80 80 (3276800)
I0816 15:04:49.599141 21700 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0816 15:04:49.599144 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.599149 21700 net.cpp:184] Created Layer res3a_branch2b/bn (21)
I0816 15:04:49.599153 21700 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0816 15:04:49.599154 21700 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0816 15:04:49.599562 21700 net.cpp:245] Setting up res3a_branch2b/bn
I0816 15:04:49.599570 21700 net.cpp:252] TEST Top shape for layer 21 'res3a_branch2b/bn' 4 128 80 80 (3276800)
I0816 15:04:49.599575 21700 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0816 15:04:49.599578 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.599581 21700 net.cpp:184] Created Layer res3a_branch2b/relu (22)
I0816 15:04:49.599594 21700 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0816 15:04:49.599598 21700 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0816 15:04:49.599603 21700 net.cpp:245] Setting up res3a_branch2b/relu
I0816 15:04:49.599607 21700 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b/relu' 4 128 80 80 (3276800)
I0816 15:04:49.599609 21700 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0816 15:04:49.599612 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.599617 21700 net.cpp:184] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (23)
I0816 15:04:49.599620 21700 net.cpp:561] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0816 15:04:49.599622 21700 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0816 15:04:49.599625 21700 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0816 15:04:49.599647 21700 net.cpp:245] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0816 15:04:49.599651 21700 net.cpp:252] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 4 128 80 80 (3276800)
I0816 15:04:49.599654 21700 net.cpp:252] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 4 128 80 80 (3276800)
I0816 15:04:49.599656 21700 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0816 15:04:49.599658 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.599663 21700 net.cpp:184] Created Layer pool3 (24)
I0816 15:04:49.599664 21700 net.cpp:561] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0816 15:04:49.599668 21700 net.cpp:530] pool3 -> pool3
I0816 15:04:49.599696 21700 net.cpp:245] Setting up pool3
I0816 15:04:49.599701 21700 net.cpp:252] TEST Top shape for layer 24 'pool3' 4 128 40 40 (819200)
I0816 15:04:49.599704 21700 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0816 15:04:49.599705 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.599711 21700 net.cpp:184] Created Layer res4a_branch2a (25)
I0816 15:04:49.599714 21700 net.cpp:561] res4a_branch2a <- pool3
I0816 15:04:49.599715 21700 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0816 15:04:49.611789 21700 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.87G, req 0G)
I0816 15:04:49.611814 21700 net.cpp:245] Setting up res4a_branch2a
I0816 15:04:49.611821 21700 net.cpp:252] TEST Top shape for layer 25 'res4a_branch2a' 4 256 40 40 (1638400)
I0816 15:04:49.611832 21700 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0816 15:04:49.611845 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.611858 21700 net.cpp:184] Created Layer res4a_branch2a/bn (26)
I0816 15:04:49.611865 21700 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0816 15:04:49.611870 21700 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0816 15:04:49.612469 21700 net.cpp:245] Setting up res4a_branch2a/bn
I0816 15:04:49.612480 21700 net.cpp:252] TEST Top shape for layer 26 'res4a_branch2a/bn' 4 256 40 40 (1638400)
I0816 15:04:49.612489 21700 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0816 15:04:49.612495 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.612500 21700 net.cpp:184] Created Layer res4a_branch2a/relu (27)
I0816 15:04:49.612504 21700 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0816 15:04:49.612509 21700 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0816 15:04:49.612514 21700 net.cpp:245] Setting up res4a_branch2a/relu
I0816 15:04:49.612519 21700 net.cpp:252] TEST Top shape for layer 27 'res4a_branch2a/relu' 4 256 40 40 (1638400)
I0816 15:04:49.612522 21700 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0816 15:04:49.612537 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.612548 21700 net.cpp:184] Created Layer res4a_branch2b (28)
I0816 15:04:49.612552 21700 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0816 15:04:49.612556 21700 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0816 15:04:49.619669 21700 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.86G, req 0G)
I0816 15:04:49.619686 21700 net.cpp:245] Setting up res4a_branch2b
I0816 15:04:49.619693 21700 net.cpp:252] TEST Top shape for layer 28 'res4a_branch2b' 4 256 40 40 (1638400)
I0816 15:04:49.619699 21700 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0816 15:04:49.619704 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.619717 21700 net.cpp:184] Created Layer res4a_branch2b/bn (29)
I0816 15:04:49.619721 21700 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0816 15:04:49.619724 21700 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0816 15:04:49.620165 21700 net.cpp:245] Setting up res4a_branch2b/bn
I0816 15:04:49.620173 21700 net.cpp:252] TEST Top shape for layer 29 'res4a_branch2b/bn' 4 256 40 40 (1638400)
I0816 15:04:49.620178 21700 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0816 15:04:49.620182 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.620185 21700 net.cpp:184] Created Layer res4a_branch2b/relu (30)
I0816 15:04:49.620187 21700 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0816 15:04:49.620189 21700 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0816 15:04:49.620193 21700 net.cpp:245] Setting up res4a_branch2b/relu
I0816 15:04:49.620198 21700 net.cpp:252] TEST Top shape for layer 30 'res4a_branch2b/relu' 4 256 40 40 (1638400)
I0816 15:04:49.620199 21700 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0816 15:04:49.620201 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.620208 21700 net.cpp:184] Created Layer pool4 (31)
I0816 15:04:49.620215 21700 net.cpp:561] pool4 <- res4a_branch2b
I0816 15:04:49.620218 21700 net.cpp:530] pool4 -> pool4
I0816 15:04:49.620249 21700 net.cpp:245] Setting up pool4
I0816 15:04:49.620252 21700 net.cpp:252] TEST Top shape for layer 31 'pool4' 4 256 40 40 (1638400)
I0816 15:04:49.620254 21700 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0816 15:04:49.620257 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.620268 21700 net.cpp:184] Created Layer res5a_branch2a (32)
I0816 15:04:49.620271 21700 net.cpp:561] res5a_branch2a <- pool4
I0816 15:04:49.620273 21700 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0816 15:04:49.648272 21700 net.cpp:245] Setting up res5a_branch2a
I0816 15:04:49.648311 21700 net.cpp:252] TEST Top shape for layer 32 'res5a_branch2a' 4 512 40 40 (3276800)
I0816 15:04:49.648324 21700 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0816 15:04:49.648330 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.648346 21700 net.cpp:184] Created Layer res5a_branch2a/bn (33)
I0816 15:04:49.648353 21700 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0816 15:04:49.648360 21700 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0816 15:04:49.649056 21700 net.cpp:245] Setting up res5a_branch2a/bn
I0816 15:04:49.649067 21700 net.cpp:252] TEST Top shape for layer 33 'res5a_branch2a/bn' 4 512 40 40 (3276800)
I0816 15:04:49.649077 21700 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0816 15:04:49.649083 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.649091 21700 net.cpp:184] Created Layer res5a_branch2a/relu (34)
I0816 15:04:49.649108 21700 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0816 15:04:49.649113 21700 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0816 15:04:49.649119 21700 net.cpp:245] Setting up res5a_branch2a/relu
I0816 15:04:49.649125 21700 net.cpp:252] TEST Top shape for layer 34 'res5a_branch2a/relu' 4 512 40 40 (3276800)
I0816 15:04:49.649128 21700 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0816 15:04:49.649132 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.649142 21700 net.cpp:184] Created Layer res5a_branch2b (35)
I0816 15:04:49.649147 21700 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0816 15:04:49.649150 21700 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0816 15:04:49.661739 21700 net.cpp:245] Setting up res5a_branch2b
I0816 15:04:49.661761 21700 net.cpp:252] TEST Top shape for layer 35 'res5a_branch2b' 4 512 40 40 (3276800)
I0816 15:04:49.661772 21700 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0816 15:04:49.661775 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.661782 21700 net.cpp:184] Created Layer res5a_branch2b/bn (36)
I0816 15:04:49.661785 21700 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0816 15:04:49.661788 21700 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0816 15:04:49.662191 21700 net.cpp:245] Setting up res5a_branch2b/bn
I0816 15:04:49.662199 21700 net.cpp:252] TEST Top shape for layer 36 'res5a_branch2b/bn' 4 512 40 40 (3276800)
I0816 15:04:49.662204 21700 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0816 15:04:49.662207 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.662210 21700 net.cpp:184] Created Layer res5a_branch2b/relu (37)
I0816 15:04:49.662212 21700 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0816 15:04:49.662214 21700 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0816 15:04:49.662219 21700 net.cpp:245] Setting up res5a_branch2b/relu
I0816 15:04:49.662221 21700 net.cpp:252] TEST Top shape for layer 37 'res5a_branch2b/relu' 4 512 40 40 (3276800)
I0816 15:04:49.662223 21700 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I0816 15:04:49.662225 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.662231 21700 net.cpp:184] Created Layer out5a (38)
I0816 15:04:49.662233 21700 net.cpp:561] out5a <- res5a_branch2b
I0816 15:04:49.662235 21700 net.cpp:530] out5a -> out5a
I0816 15:04:49.665904 21700 net.cpp:245] Setting up out5a
I0816 15:04:49.665916 21700 net.cpp:252] TEST Top shape for layer 38 'out5a' 4 64 40 40 (409600)
I0816 15:04:49.665921 21700 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I0816 15:04:49.665925 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.665928 21700 net.cpp:184] Created Layer out5a/bn (39)
I0816 15:04:49.665931 21700 net.cpp:561] out5a/bn <- out5a
I0816 15:04:49.665935 21700 net.cpp:513] out5a/bn -> out5a (in-place)
I0816 15:04:49.666342 21700 net.cpp:245] Setting up out5a/bn
I0816 15:04:49.666348 21700 net.cpp:252] TEST Top shape for layer 39 'out5a/bn' 4 64 40 40 (409600)
I0816 15:04:49.666354 21700 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I0816 15:04:49.666357 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.666360 21700 net.cpp:184] Created Layer out5a/relu (40)
I0816 15:04:49.666363 21700 net.cpp:561] out5a/relu <- out5a
I0816 15:04:49.666364 21700 net.cpp:513] out5a/relu -> out5a (in-place)
I0816 15:04:49.666368 21700 net.cpp:245] Setting up out5a/relu
I0816 15:04:49.666370 21700 net.cpp:252] TEST Top shape for layer 40 'out5a/relu' 4 64 40 40 (409600)
I0816 15:04:49.666373 21700 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I0816 15:04:49.666384 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.666395 21700 net.cpp:184] Created Layer out5a_up2 (41)
I0816 15:04:49.666399 21700 net.cpp:561] out5a_up2 <- out5a
I0816 15:04:49.666401 21700 net.cpp:530] out5a_up2 -> out5a_up2
I0816 15:04:49.666538 21700 net.cpp:245] Setting up out5a_up2
I0816 15:04:49.666543 21700 net.cpp:252] TEST Top shape for layer 41 'out5a_up2' 4 64 80 80 (1638400)
I0816 15:04:49.666546 21700 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I0816 15:04:49.666548 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.666555 21700 net.cpp:184] Created Layer out3a (42)
I0816 15:04:49.666558 21700 net.cpp:561] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0816 15:04:49.666561 21700 net.cpp:530] out3a -> out3a
I0816 15:04:49.671036 21700 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 7.84G, req 0G)
I0816 15:04:49.671053 21700 net.cpp:245] Setting up out3a
I0816 15:04:49.671061 21700 net.cpp:252] TEST Top shape for layer 42 'out3a' 4 64 80 80 (1638400)
I0816 15:04:49.671069 21700 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I0816 15:04:49.671074 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.671084 21700 net.cpp:184] Created Layer out3a/bn (43)
I0816 15:04:49.671089 21700 net.cpp:561] out3a/bn <- out3a
I0816 15:04:49.671094 21700 net.cpp:513] out3a/bn -> out3a (in-place)
I0816 15:04:49.671697 21700 net.cpp:245] Setting up out3a/bn
I0816 15:04:49.671706 21700 net.cpp:252] TEST Top shape for layer 43 'out3a/bn' 4 64 80 80 (1638400)
I0816 15:04:49.671715 21700 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I0816 15:04:49.671718 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.671723 21700 net.cpp:184] Created Layer out3a/relu (44)
I0816 15:04:49.671727 21700 net.cpp:561] out3a/relu <- out3a
I0816 15:04:49.671730 21700 net.cpp:513] out3a/relu -> out3a (in-place)
I0816 15:04:49.671736 21700 net.cpp:245] Setting up out3a/relu
I0816 15:04:49.671741 21700 net.cpp:252] TEST Top shape for layer 44 'out3a/relu' 4 64 80 80 (1638400)
I0816 15:04:49.671743 21700 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I0816 15:04:49.671746 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.671761 21700 net.cpp:184] Created Layer out3_out5_combined (45)
I0816 15:04:49.671764 21700 net.cpp:561] out3_out5_combined <- out5a_up2
I0816 15:04:49.671768 21700 net.cpp:561] out3_out5_combined <- out3a
I0816 15:04:49.671773 21700 net.cpp:530] out3_out5_combined -> out3_out5_combined
I0816 15:04:49.672833 21700 net.cpp:245] Setting up out3_out5_combined
I0816 15:04:49.672845 21700 net.cpp:252] TEST Top shape for layer 45 'out3_out5_combined' 4 64 80 80 (1638400)
I0816 15:04:49.672849 21700 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I0816 15:04:49.672853 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.672863 21700 net.cpp:184] Created Layer ctx_conv1 (46)
I0816 15:04:49.672868 21700 net.cpp:561] ctx_conv1 <- out3_out5_combined
I0816 15:04:49.672871 21700 net.cpp:530] ctx_conv1 -> ctx_conv1
I0816 15:04:49.678011 21700 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 7.81G, req 0G)
I0816 15:04:49.678031 21700 net.cpp:245] Setting up ctx_conv1
I0816 15:04:49.678040 21700 net.cpp:252] TEST Top shape for layer 46 'ctx_conv1' 4 64 80 80 (1638400)
I0816 15:04:49.678048 21700 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I0816 15:04:49.678053 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.678067 21700 net.cpp:184] Created Layer ctx_conv1/bn (47)
I0816 15:04:49.678082 21700 net.cpp:561] ctx_conv1/bn <- ctx_conv1
I0816 15:04:49.678088 21700 net.cpp:513] ctx_conv1/bn -> ctx_conv1 (in-place)
I0816 15:04:49.678593 21700 net.cpp:245] Setting up ctx_conv1/bn
I0816 15:04:49.678601 21700 net.cpp:252] TEST Top shape for layer 47 'ctx_conv1/bn' 4 64 80 80 (1638400)
I0816 15:04:49.678608 21700 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I0816 15:04:49.678611 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.678614 21700 net.cpp:184] Created Layer ctx_conv1/relu (48)
I0816 15:04:49.678617 21700 net.cpp:561] ctx_conv1/relu <- ctx_conv1
I0816 15:04:49.678619 21700 net.cpp:513] ctx_conv1/relu -> ctx_conv1 (in-place)
I0816 15:04:49.678623 21700 net.cpp:245] Setting up ctx_conv1/relu
I0816 15:04:49.678625 21700 net.cpp:252] TEST Top shape for layer 48 'ctx_conv1/relu' 4 64 80 80 (1638400)
I0816 15:04:49.678627 21700 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I0816 15:04:49.678629 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.678638 21700 net.cpp:184] Created Layer ctx_conv2 (49)
I0816 15:04:49.678640 21700 net.cpp:561] ctx_conv2 <- ctx_conv1
I0816 15:04:49.678643 21700 net.cpp:530] ctx_conv2 -> ctx_conv2
I0816 15:04:49.679543 21700 net.cpp:245] Setting up ctx_conv2
I0816 15:04:49.679550 21700 net.cpp:252] TEST Top shape for layer 49 'ctx_conv2' 4 64 80 80 (1638400)
I0816 15:04:49.679553 21700 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I0816 15:04:49.679556 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.679561 21700 net.cpp:184] Created Layer ctx_conv2/bn (50)
I0816 15:04:49.679563 21700 net.cpp:561] ctx_conv2/bn <- ctx_conv2
I0816 15:04:49.679566 21700 net.cpp:513] ctx_conv2/bn -> ctx_conv2 (in-place)
I0816 15:04:49.679962 21700 net.cpp:245] Setting up ctx_conv2/bn
I0816 15:04:49.679968 21700 net.cpp:252] TEST Top shape for layer 50 'ctx_conv2/bn' 4 64 80 80 (1638400)
I0816 15:04:49.679975 21700 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I0816 15:04:49.679978 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.679982 21700 net.cpp:184] Created Layer ctx_conv2/relu (51)
I0816 15:04:49.679985 21700 net.cpp:561] ctx_conv2/relu <- ctx_conv2
I0816 15:04:49.679987 21700 net.cpp:513] ctx_conv2/relu -> ctx_conv2 (in-place)
I0816 15:04:49.679991 21700 net.cpp:245] Setting up ctx_conv2/relu
I0816 15:04:49.679993 21700 net.cpp:252] TEST Top shape for layer 51 'ctx_conv2/relu' 4 64 80 80 (1638400)
I0816 15:04:49.679996 21700 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I0816 15:04:49.679997 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.680002 21700 net.cpp:184] Created Layer ctx_conv3 (52)
I0816 15:04:49.680006 21700 net.cpp:561] ctx_conv3 <- ctx_conv2
I0816 15:04:49.680007 21700 net.cpp:530] ctx_conv3 -> ctx_conv3
I0816 15:04:49.680915 21700 net.cpp:245] Setting up ctx_conv3
I0816 15:04:49.680922 21700 net.cpp:252] TEST Top shape for layer 52 'ctx_conv3' 4 64 80 80 (1638400)
I0816 15:04:49.680927 21700 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I0816 15:04:49.680929 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.680934 21700 net.cpp:184] Created Layer ctx_conv3/bn (53)
I0816 15:04:49.680938 21700 net.cpp:561] ctx_conv3/bn <- ctx_conv3
I0816 15:04:49.680940 21700 net.cpp:513] ctx_conv3/bn -> ctx_conv3 (in-place)
I0816 15:04:49.681334 21700 net.cpp:245] Setting up ctx_conv3/bn
I0816 15:04:49.681340 21700 net.cpp:252] TEST Top shape for layer 53 'ctx_conv3/bn' 4 64 80 80 (1638400)
I0816 15:04:49.681346 21700 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I0816 15:04:49.681349 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.681358 21700 net.cpp:184] Created Layer ctx_conv3/relu (54)
I0816 15:04:49.681361 21700 net.cpp:561] ctx_conv3/relu <- ctx_conv3
I0816 15:04:49.681365 21700 net.cpp:513] ctx_conv3/relu -> ctx_conv3 (in-place)
I0816 15:04:49.681367 21700 net.cpp:245] Setting up ctx_conv3/relu
I0816 15:04:49.681370 21700 net.cpp:252] TEST Top shape for layer 54 'ctx_conv3/relu' 4 64 80 80 (1638400)
I0816 15:04:49.681372 21700 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I0816 15:04:49.681375 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.681381 21700 net.cpp:184] Created Layer ctx_conv4 (55)
I0816 15:04:49.681385 21700 net.cpp:561] ctx_conv4 <- ctx_conv3
I0816 15:04:49.681386 21700 net.cpp:530] ctx_conv4 -> ctx_conv4
I0816 15:04:49.682271 21700 net.cpp:245] Setting up ctx_conv4
I0816 15:04:49.682277 21700 net.cpp:252] TEST Top shape for layer 55 'ctx_conv4' 4 64 80 80 (1638400)
I0816 15:04:49.682282 21700 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I0816 15:04:49.682286 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.682289 21700 net.cpp:184] Created Layer ctx_conv4/bn (56)
I0816 15:04:49.682292 21700 net.cpp:561] ctx_conv4/bn <- ctx_conv4
I0816 15:04:49.682294 21700 net.cpp:513] ctx_conv4/bn -> ctx_conv4 (in-place)
I0816 15:04:49.682680 21700 net.cpp:245] Setting up ctx_conv4/bn
I0816 15:04:49.682687 21700 net.cpp:252] TEST Top shape for layer 56 'ctx_conv4/bn' 4 64 80 80 (1638400)
I0816 15:04:49.682693 21700 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I0816 15:04:49.682695 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.682698 21700 net.cpp:184] Created Layer ctx_conv4/relu (57)
I0816 15:04:49.682700 21700 net.cpp:561] ctx_conv4/relu <- ctx_conv4
I0816 15:04:49.682703 21700 net.cpp:513] ctx_conv4/relu -> ctx_conv4 (in-place)
I0816 15:04:49.682705 21700 net.cpp:245] Setting up ctx_conv4/relu
I0816 15:04:49.682708 21700 net.cpp:252] TEST Top shape for layer 57 'ctx_conv4/relu' 4 64 80 80 (1638400)
I0816 15:04:49.682709 21700 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I0816 15:04:49.682711 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.682718 21700 net.cpp:184] Created Layer ctx_final (58)
I0816 15:04:49.682720 21700 net.cpp:561] ctx_final <- ctx_conv4
I0816 15:04:49.682723 21700 net.cpp:530] ctx_final -> ctx_final
I0816 15:04:49.687533 21700 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 7.8G, req 0G)
I0816 15:04:49.687544 21700 net.cpp:245] Setting up ctx_final
I0816 15:04:49.687548 21700 net.cpp:252] TEST Top shape for layer 58 'ctx_final' 4 8 80 80 (204800)
I0816 15:04:49.687552 21700 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I0816 15:04:49.687556 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.687558 21700 net.cpp:184] Created Layer ctx_final/relu (59)
I0816 15:04:49.687561 21700 net.cpp:561] ctx_final/relu <- ctx_final
I0816 15:04:49.687563 21700 net.cpp:513] ctx_final/relu -> ctx_final (in-place)
I0816 15:04:49.687567 21700 net.cpp:245] Setting up ctx_final/relu
I0816 15:04:49.687569 21700 net.cpp:252] TEST Top shape for layer 59 'ctx_final/relu' 4 8 80 80 (204800)
I0816 15:04:49.687572 21700 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I0816 15:04:49.687573 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.687579 21700 net.cpp:184] Created Layer out_deconv_final_up2 (60)
I0816 15:04:49.687582 21700 net.cpp:561] out_deconv_final_up2 <- ctx_final
I0816 15:04:49.687583 21700 net.cpp:530] out_deconv_final_up2 -> out_deconv_final_up2
I0816 15:04:49.687710 21700 net.cpp:245] Setting up out_deconv_final_up2
I0816 15:04:49.687721 21700 net.cpp:252] TEST Top shape for layer 60 'out_deconv_final_up2' 4 8 160 160 (819200)
I0816 15:04:49.687726 21700 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I0816 15:04:49.687727 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.687733 21700 net.cpp:184] Created Layer out_deconv_final_up4 (61)
I0816 15:04:49.687736 21700 net.cpp:561] out_deconv_final_up4 <- out_deconv_final_up2
I0816 15:04:49.687739 21700 net.cpp:530] out_deconv_final_up4 -> out_deconv_final_up4
I0816 15:04:49.687849 21700 net.cpp:245] Setting up out_deconv_final_up4
I0816 15:04:49.687855 21700 net.cpp:252] TEST Top shape for layer 61 'out_deconv_final_up4' 4 8 320 320 (3276800)
I0816 15:04:49.687857 21700 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I0816 15:04:49.687860 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.687866 21700 net.cpp:184] Created Layer out_deconv_final_up8 (62)
I0816 15:04:49.687870 21700 net.cpp:561] out_deconv_final_up8 <- out_deconv_final_up4
I0816 15:04:49.687871 21700 net.cpp:530] out_deconv_final_up8 -> out_deconv_final_up8
I0816 15:04:49.687978 21700 net.cpp:245] Setting up out_deconv_final_up8
I0816 15:04:49.687983 21700 net.cpp:252] TEST Top shape for layer 62 'out_deconv_final_up8' 4 8 640 640 (13107200)
I0816 15:04:49.687985 21700 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8_out_deconv_final_up8_0_split' of type 'Split'
I0816 15:04:49.687988 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.687990 21700 net.cpp:184] Created Layer out_deconv_final_up8_out_deconv_final_up8_0_split (63)
I0816 15:04:49.687993 21700 net.cpp:561] out_deconv_final_up8_out_deconv_final_up8_0_split <- out_deconv_final_up8
I0816 15:04:49.687995 21700 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0816 15:04:49.687997 21700 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0816 15:04:49.688000 21700 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0816 15:04:49.688026 21700 net.cpp:245] Setting up out_deconv_final_up8_out_deconv_final_up8_0_split
I0816 15:04:49.688030 21700 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 4 8 640 640 (13107200)
I0816 15:04:49.688032 21700 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 4 8 640 640 (13107200)
I0816 15:04:49.688035 21700 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 4 8 640 640 (13107200)
I0816 15:04:49.688037 21700 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0816 15:04:49.688040 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.688050 21700 net.cpp:184] Created Layer loss (64)
I0816 15:04:49.688053 21700 net.cpp:561] loss <- out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0816 15:04:49.688055 21700 net.cpp:561] loss <- label_data_1_split_0
I0816 15:04:49.688058 21700 net.cpp:530] loss -> loss
I0816 15:04:49.688988 21700 net.cpp:245] Setting up loss
I0816 15:04:49.688997 21700 net.cpp:252] TEST Top shape for layer 64 'loss' (1)
I0816 15:04:49.688999 21700 net.cpp:256]     with loss weight 1
I0816 15:04:49.689003 21700 layer_factory.hpp:136] Creating layer 'accuracy/top1' of type 'Accuracy'
I0816 15:04:49.689005 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.689012 21700 net.cpp:184] Created Layer accuracy/top1 (65)
I0816 15:04:49.689013 21700 net.cpp:561] accuracy/top1 <- out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0816 15:04:49.689016 21700 net.cpp:561] accuracy/top1 <- label_data_1_split_1
I0816 15:04:49.689025 21700 net.cpp:530] accuracy/top1 -> accuracy/top1
I0816 15:04:49.689033 21700 net.cpp:245] Setting up accuracy/top1
I0816 15:04:49.689035 21700 net.cpp:252] TEST Top shape for layer 65 'accuracy/top1' (1)
I0816 15:04:49.689038 21700 layer_factory.hpp:136] Creating layer 'accuracy/top5' of type 'Accuracy'
I0816 15:04:49.689039 21700 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 15:04:49.689043 21700 net.cpp:184] Created Layer accuracy/top5 (66)
I0816 15:04:49.689044 21700 net.cpp:561] accuracy/top5 <- out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0816 15:04:49.689047 21700 net.cpp:561] accuracy/top5 <- label_data_1_split_2
I0816 15:04:49.689049 21700 net.cpp:530] accuracy/top5 -> accuracy/top5
I0816 15:04:49.689052 21700 net.cpp:245] Setting up accuracy/top5
I0816 15:04:49.689057 21700 net.cpp:252] TEST Top shape for layer 66 'accuracy/top5' (1)
I0816 15:04:49.689059 21700 net.cpp:325] accuracy/top5 does not need backward computation.
I0816 15:04:49.689061 21700 net.cpp:325] accuracy/top1 does not need backward computation.
I0816 15:04:49.689064 21700 net.cpp:323] loss needs backward computation.
I0816 15:04:49.689066 21700 net.cpp:323] out_deconv_final_up8_out_deconv_final_up8_0_split needs backward computation.
I0816 15:04:49.689069 21700 net.cpp:323] out_deconv_final_up8 needs backward computation.
I0816 15:04:49.689070 21700 net.cpp:323] out_deconv_final_up4 needs backward computation.
I0816 15:04:49.689072 21700 net.cpp:323] out_deconv_final_up2 needs backward computation.
I0816 15:04:49.689074 21700 net.cpp:323] ctx_final/relu needs backward computation.
I0816 15:04:49.689075 21700 net.cpp:323] ctx_final needs backward computation.
I0816 15:04:49.689077 21700 net.cpp:323] ctx_conv4/relu needs backward computation.
I0816 15:04:49.689079 21700 net.cpp:323] ctx_conv4/bn needs backward computation.
I0816 15:04:49.689081 21700 net.cpp:323] ctx_conv4 needs backward computation.
I0816 15:04:49.689083 21700 net.cpp:323] ctx_conv3/relu needs backward computation.
I0816 15:04:49.689085 21700 net.cpp:323] ctx_conv3/bn needs backward computation.
I0816 15:04:49.689086 21700 net.cpp:323] ctx_conv3 needs backward computation.
I0816 15:04:49.689088 21700 net.cpp:323] ctx_conv2/relu needs backward computation.
I0816 15:04:49.689090 21700 net.cpp:323] ctx_conv2/bn needs backward computation.
I0816 15:04:49.689091 21700 net.cpp:323] ctx_conv2 needs backward computation.
I0816 15:04:49.689093 21700 net.cpp:323] ctx_conv1/relu needs backward computation.
I0816 15:04:49.689095 21700 net.cpp:323] ctx_conv1/bn needs backward computation.
I0816 15:04:49.689097 21700 net.cpp:323] ctx_conv1 needs backward computation.
I0816 15:04:49.689100 21700 net.cpp:323] out3_out5_combined needs backward computation.
I0816 15:04:49.689101 21700 net.cpp:323] out3a/relu needs backward computation.
I0816 15:04:49.689103 21700 net.cpp:323] out3a/bn needs backward computation.
I0816 15:04:49.689105 21700 net.cpp:323] out3a needs backward computation.
I0816 15:04:49.689107 21700 net.cpp:323] out5a_up2 needs backward computation.
I0816 15:04:49.689110 21700 net.cpp:323] out5a/relu needs backward computation.
I0816 15:04:49.689111 21700 net.cpp:323] out5a/bn needs backward computation.
I0816 15:04:49.689113 21700 net.cpp:323] out5a needs backward computation.
I0816 15:04:49.689117 21700 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0816 15:04:49.689119 21700 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0816 15:04:49.689121 21700 net.cpp:323] res5a_branch2b needs backward computation.
I0816 15:04:49.689123 21700 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0816 15:04:49.689126 21700 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0816 15:04:49.689127 21700 net.cpp:323] res5a_branch2a needs backward computation.
I0816 15:04:49.689129 21700 net.cpp:323] pool4 needs backward computation.
I0816 15:04:49.689131 21700 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0816 15:04:49.689133 21700 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0816 15:04:49.689138 21700 net.cpp:323] res4a_branch2b needs backward computation.
I0816 15:04:49.689141 21700 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0816 15:04:49.689142 21700 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0816 15:04:49.689144 21700 net.cpp:323] res4a_branch2a needs backward computation.
I0816 15:04:49.689146 21700 net.cpp:323] pool3 needs backward computation.
I0816 15:04:49.689148 21700 net.cpp:323] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I0816 15:04:49.689151 21700 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0816 15:04:49.689152 21700 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0816 15:04:49.689154 21700 net.cpp:323] res3a_branch2b needs backward computation.
I0816 15:04:49.689157 21700 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0816 15:04:49.689158 21700 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0816 15:04:49.689160 21700 net.cpp:323] res3a_branch2a needs backward computation.
I0816 15:04:49.689162 21700 net.cpp:323] pool2 needs backward computation.
I0816 15:04:49.689164 21700 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0816 15:04:49.689167 21700 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0816 15:04:49.689168 21700 net.cpp:323] res2a_branch2b needs backward computation.
I0816 15:04:49.689170 21700 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0816 15:04:49.689172 21700 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0816 15:04:49.689173 21700 net.cpp:323] res2a_branch2a needs backward computation.
I0816 15:04:49.689175 21700 net.cpp:323] pool1 needs backward computation.
I0816 15:04:49.689177 21700 net.cpp:323] conv1b/relu needs backward computation.
I0816 15:04:49.689179 21700 net.cpp:323] conv1b/bn needs backward computation.
I0816 15:04:49.689182 21700 net.cpp:323] conv1b needs backward computation.
I0816 15:04:49.689183 21700 net.cpp:323] conv1a/relu needs backward computation.
I0816 15:04:49.689185 21700 net.cpp:323] conv1a/bn needs backward computation.
I0816 15:04:49.689188 21700 net.cpp:323] conv1a needs backward computation.
I0816 15:04:49.689190 21700 net.cpp:325] data/bias does not need backward computation.
I0816 15:04:49.689193 21700 net.cpp:325] label_data_1_split does not need backward computation.
I0816 15:04:49.689195 21700 net.cpp:325] data does not need backward computation.
I0816 15:04:49.689198 21700 net.cpp:367] This network produces output accuracy/top1
I0816 15:04:49.689199 21700 net.cpp:367] This network produces output accuracy/top5
I0816 15:04:49.689201 21700 net.cpp:367] This network produces output loss
I0816 15:04:49.689241 21700 net.cpp:389] Top memory (TEST) required for data: 637337600 diff: 8
I0816 15:04:49.689244 21700 net.cpp:392] Bottom memory (TEST) required for data: 637337600 diff: 637337600
I0816 15:04:49.689246 21700 net.cpp:395] Shared (in-place) memory (TEST) by data: 420249600 diff: 420249600
I0816 15:04:49.689249 21700 net.cpp:398] Parameters memory (TEST) required for data: 2692608 diff: 2692608
I0816 15:04:49.689250 21700 net.cpp:401] Parameters shared memory (TEST) by data: 0 diff: 0
I0816 15:04:49.689252 21700 net.cpp:407] Network initialization done.
I0816 15:04:49.693928 21700 net.cpp:1095] Copying source layer data Type:ImageLabelData #blobs=0
I0816 15:04:49.693946 21700 net.cpp:1095] Copying source layer data/bias Type:Bias #blobs=1
I0816 15:04:49.693976 21700 net.cpp:1095] Copying source layer conv1a Type:Convolution #blobs=2
I0816 15:04:49.693989 21700 net.cpp:1095] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0816 15:04:49.694236 21700 net.cpp:1095] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0816 15:04:49.694241 21700 net.cpp:1095] Copying source layer conv1b Type:Convolution #blobs=2
I0816 15:04:49.694249 21700 net.cpp:1095] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0816 15:04:49.694416 21700 net.cpp:1095] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0816 15:04:49.694422 21700 net.cpp:1095] Copying source layer pool1 Type:Pooling #blobs=0
I0816 15:04:49.694433 21700 net.cpp:1095] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0816 15:04:49.694448 21700 net.cpp:1095] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0816 15:04:49.694617 21700 net.cpp:1095] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0816 15:04:49.694622 21700 net.cpp:1095] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0816 15:04:49.694633 21700 net.cpp:1095] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0816 15:04:49.694795 21700 net.cpp:1095] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0816 15:04:49.694799 21700 net.cpp:1095] Copying source layer pool2 Type:Pooling #blobs=0
I0816 15:04:49.694802 21700 net.cpp:1095] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0816 15:04:49.694839 21700 net.cpp:1095] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0816 15:04:49.695003 21700 net.cpp:1095] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0816 15:04:49.695008 21700 net.cpp:1095] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0816 15:04:49.695030 21700 net.cpp:1095] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0816 15:04:49.695181 21700 net.cpp:1095] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0816 15:04:49.695186 21700 net.cpp:1095] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I0816 15:04:49.695188 21700 net.cpp:1095] Copying source layer pool3 Type:Pooling #blobs=0
I0816 15:04:49.695190 21700 net.cpp:1095] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0816 15:04:49.695299 21700 net.cpp:1095] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0816 15:04:49.695456 21700 net.cpp:1095] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0816 15:04:49.695459 21700 net.cpp:1095] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0816 15:04:49.695519 21700 net.cpp:1095] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0816 15:04:49.695677 21700 net.cpp:1095] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0816 15:04:49.695680 21700 net.cpp:1095] Copying source layer pool4 Type:Pooling #blobs=0
I0816 15:04:49.695683 21700 net.cpp:1095] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0816 15:04:49.696058 21700 net.cpp:1095] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0816 15:04:49.696223 21700 net.cpp:1095] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0816 15:04:49.696228 21700 net.cpp:1095] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0816 15:04:49.696367 21700 net.cpp:1095] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0816 15:04:49.696519 21700 net.cpp:1095] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0816 15:04:49.696524 21700 net.cpp:1095] Copying source layer out5a Type:Convolution #blobs=2
I0816 15:04:49.696570 21700 net.cpp:1095] Copying source layer out5a/bn Type:BatchNorm #blobs=5
I0816 15:04:49.696658 21700 net.cpp:1095] Copying source layer out5a/relu Type:ReLU #blobs=0
I0816 15:04:49.696662 21700 net.cpp:1095] Copying source layer out5a_up2 Type:Deconvolution #blobs=1
I0816 15:04:49.696667 21700 net.cpp:1095] Copying source layer out3a Type:Convolution #blobs=2
I0816 15:04:49.696684 21700 net.cpp:1095] Copying source layer out3a/bn Type:BatchNorm #blobs=5
I0816 15:04:49.696770 21700 net.cpp:1095] Copying source layer out3a/relu Type:ReLU #blobs=0
I0816 15:04:49.696774 21700 net.cpp:1095] Copying source layer out3_out5_combined Type:Eltwise #blobs=0
I0816 15:04:49.696776 21700 net.cpp:1095] Copying source layer ctx_conv1 Type:Convolution #blobs=2
I0816 15:04:49.696791 21700 net.cpp:1095] Copying source layer ctx_conv1/bn Type:BatchNorm #blobs=5
I0816 15:04:49.696874 21700 net.cpp:1095] Copying source layer ctx_conv1/relu Type:ReLU #blobs=0
I0816 15:04:49.696878 21700 net.cpp:1095] Copying source layer ctx_conv2 Type:Convolution #blobs=2
I0816 15:04:49.696902 21700 net.cpp:1095] Copying source layer ctx_conv2/bn Type:BatchNorm #blobs=5
I0816 15:04:49.696983 21700 net.cpp:1095] Copying source layer ctx_conv2/relu Type:ReLU #blobs=0
I0816 15:04:49.696987 21700 net.cpp:1095] Copying source layer ctx_conv3 Type:Convolution #blobs=2
I0816 15:04:49.697007 21700 net.cpp:1095] Copying source layer ctx_conv3/bn Type:BatchNorm #blobs=5
I0816 15:04:49.697095 21700 net.cpp:1095] Copying source layer ctx_conv3/relu Type:ReLU #blobs=0
I0816 15:04:49.697099 21700 net.cpp:1095] Copying source layer ctx_conv4 Type:Convolution #blobs=2
I0816 15:04:49.697116 21700 net.cpp:1095] Copying source layer ctx_conv4/bn Type:BatchNorm #blobs=5
I0816 15:04:49.697201 21700 net.cpp:1095] Copying source layer ctx_conv4/relu Type:ReLU #blobs=0
I0816 15:04:49.697206 21700 net.cpp:1095] Copying source layer ctx_final Type:Convolution #blobs=2
I0816 15:04:49.697213 21700 net.cpp:1095] Copying source layer ctx_final/relu Type:ReLU #blobs=0
I0816 15:04:49.697216 21700 net.cpp:1095] Copying source layer out_deconv_final_up2 Type:Deconvolution #blobs=1
I0816 15:04:49.697221 21700 net.cpp:1095] Copying source layer out_deconv_final_up4 Type:Deconvolution #blobs=1
I0816 15:04:49.697227 21700 net.cpp:1095] Copying source layer out_deconv_final_up8 Type:Deconvolution #blobs=1
I0816 15:04:49.697232 21700 net.cpp:1095] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0816 15:04:49.697311 21700 caffe.cpp:290] Running for 50 iterations.
I0816 15:04:49.703053 21700 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.72G, req 0G)
I0816 15:04:49.721576 21700 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.62G, req 0G)
I0816 15:04:49.736719 21700 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.5G, req 0G)
I0816 15:04:49.745637 21700 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.44G, req 0G)
I0816 15:04:49.753090 21700 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.38G, req 0G)
I0816 15:04:49.758136 21700 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.35G, req 0G)
I0816 15:04:49.765090 21700 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.33G, req 0G)
I0816 15:04:49.768901 21700 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.32G, req 0G)
I0816 15:04:49.791436 21700 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 7.09G, req 0G)
I0816 15:04:49.796676 21700 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 7.07G, req 0G)
I0816 15:04:49.811082 21700 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.94G, req 0G)
I0816 15:04:50.002291 21700 caffe.cpp:313] Batch 0, accuracy/top1 = 0.930066
I0816 15:04:50.002312 21700 caffe.cpp:313] Batch 0, accuracy/top5 = 1
I0816 15:04:50.002316 21700 caffe.cpp:313] Batch 0, loss = 0.220756
I0816 15:04:50.002320 21700 net.cpp:1620] Adding quantization params at infer/iter index: 1
I0816 15:04:50.008620 21700 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 1.22G/1 1  (limit 5.47G, req 0G)
I0816 15:04:50.031585 21700 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 2.44G/2 6  (limit 4.25G, req 0G)
I0816 15:04:50.076397 21700 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 2.44G/1 6  (limit 4.25G, req 0G)
I0816 15:04:50.092042 21700 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 2.44G/2 6  (limit 4.25G, req 0G)
I0816 15:04:50.127205 21700 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 2.44G/1 6  (limit 4.25G, req 0G)
I0816 15:04:50.136332 21700 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 2.44G/2 6  (limit 4.25G, req 0G)
I0816 15:04:50.157816 21700 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 2.44G/1 6  (limit 4.25G, req 0G)
I0816 15:04:50.163877 21700 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 2.44G/2 6  (limit 4.25G, req 0G)
I0816 15:04:50.188581 21700 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'out3a' with space 2.44G/2 6  (limit 4.25G, req 0G)
I0816 15:04:50.208703 21700 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_conv1' with space 2.44G/1 6  (limit 4.25G, req 0G)
I0816 15:04:50.221102 21700 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_final' with space 2.44G/1 6  (limit 4.25G, req 0G)
I0816 15:04:50.397675 21700 caffe.cpp:313] Batch 1, accuracy/top1 = 0.954935
I0816 15:04:50.397697 21700 caffe.cpp:313] Batch 1, accuracy/top5 = 1
I0816 15:04:50.397701 21700 caffe.cpp:313] Batch 1, loss = 0.141016
I0816 15:04:50.606240 21700 caffe.cpp:313] Batch 2, accuracy/top1 = 0.960384
I0816 15:04:50.606263 21700 caffe.cpp:313] Batch 2, accuracy/top5 = 1
I0816 15:04:50.606267 21700 caffe.cpp:313] Batch 2, loss = 0.113166
I0816 15:04:50.815106 21700 caffe.cpp:313] Batch 3, accuracy/top1 = 0.971681
I0816 15:04:50.815130 21700 caffe.cpp:313] Batch 3, accuracy/top5 = 0.999996
I0816 15:04:50.815134 21700 caffe.cpp:313] Batch 3, loss = 0.079467
I0816 15:04:51.021821 21700 caffe.cpp:313] Batch 4, accuracy/top1 = 0.960429
I0816 15:04:51.021844 21700 caffe.cpp:313] Batch 4, accuracy/top5 = 0.99989
I0816 15:04:51.021850 21700 caffe.cpp:313] Batch 4, loss = 0.13521
I0816 15:04:51.230494 21700 caffe.cpp:313] Batch 5, accuracy/top1 = 0.804983
I0816 15:04:51.230517 21700 caffe.cpp:313] Batch 5, accuracy/top5 = 1
I0816 15:04:51.230520 21700 caffe.cpp:313] Batch 5, loss = 0.974268
I0816 15:04:51.440524 21700 caffe.cpp:313] Batch 6, accuracy/top1 = 0.959824
I0816 15:04:51.440559 21700 caffe.cpp:313] Batch 6, accuracy/top5 = 1
I0816 15:04:51.440564 21700 caffe.cpp:313] Batch 6, loss = 0.107834
I0816 15:04:51.649116 21700 caffe.cpp:313] Batch 7, accuracy/top1 = 0.963959
I0816 15:04:51.649140 21700 caffe.cpp:313] Batch 7, accuracy/top5 = 1
I0816 15:04:51.649144 21700 caffe.cpp:313] Batch 7, loss = 0.0867746
I0816 15:04:51.854667 21700 caffe.cpp:313] Batch 8, accuracy/top1 = 0.973871
I0816 15:04:51.854689 21700 caffe.cpp:313] Batch 8, accuracy/top5 = 1
I0816 15:04:51.854693 21700 caffe.cpp:313] Batch 8, loss = 0.0694783
I0816 15:04:52.064885 21700 caffe.cpp:313] Batch 9, accuracy/top1 = 0.982211
I0816 15:04:52.064906 21700 caffe.cpp:313] Batch 9, accuracy/top5 = 1
I0816 15:04:52.064910 21700 caffe.cpp:313] Batch 9, loss = 0.0488484
I0816 15:04:52.273712 21700 caffe.cpp:313] Batch 10, accuracy/top1 = 0.907169
I0816 15:04:52.273736 21700 caffe.cpp:313] Batch 10, accuracy/top5 = 1
I0816 15:04:52.273739 21700 caffe.cpp:313] Batch 10, loss = 0.240765
I0816 15:04:52.483723 21700 caffe.cpp:313] Batch 11, accuracy/top1 = 0.976151
I0816 15:04:52.483741 21700 caffe.cpp:313] Batch 11, accuracy/top5 = 1
I0816 15:04:52.483745 21700 caffe.cpp:313] Batch 11, loss = 0.0669316
I0816 15:04:52.691623 21700 caffe.cpp:313] Batch 12, accuracy/top1 = 0.965527
I0816 15:04:52.691648 21700 caffe.cpp:313] Batch 12, accuracy/top5 = 1
I0816 15:04:52.691651 21700 caffe.cpp:313] Batch 12, loss = 0.0932255
I0816 15:04:52.899977 21700 caffe.cpp:313] Batch 13, accuracy/top1 = 0.980014
I0816 15:04:52.900001 21700 caffe.cpp:313] Batch 13, accuracy/top5 = 1
I0816 15:04:52.900004 21700 caffe.cpp:313] Batch 13, loss = 0.0543337
I0816 15:04:53.108444 21700 caffe.cpp:313] Batch 14, accuracy/top1 = 0.978123
I0816 15:04:53.108469 21700 caffe.cpp:313] Batch 14, accuracy/top5 = 1
I0816 15:04:53.108474 21700 caffe.cpp:313] Batch 14, loss = 0.0568038
I0816 15:04:53.315795 21700 caffe.cpp:313] Batch 15, accuracy/top1 = 0.963361
I0816 15:04:53.315819 21700 caffe.cpp:313] Batch 15, accuracy/top5 = 1
I0816 15:04:53.315824 21700 caffe.cpp:313] Batch 15, loss = 0.10092
I0816 15:04:53.524670 21700 caffe.cpp:313] Batch 16, accuracy/top1 = 0.897402
I0816 15:04:53.524693 21700 caffe.cpp:313] Batch 16, accuracy/top5 = 1
I0816 15:04:53.524698 21700 caffe.cpp:313] Batch 16, loss = 0.410094
I0816 15:04:53.732290 21700 caffe.cpp:313] Batch 17, accuracy/top1 = 0.872404
I0816 15:04:53.732314 21700 caffe.cpp:313] Batch 17, accuracy/top5 = 1
I0816 15:04:53.732318 21700 caffe.cpp:313] Batch 17, loss = 0.60418
I0816 15:04:53.941908 21700 caffe.cpp:313] Batch 18, accuracy/top1 = 0.982859
I0816 15:04:53.941929 21700 caffe.cpp:313] Batch 18, accuracy/top5 = 0.99999
I0816 15:04:53.941933 21700 caffe.cpp:313] Batch 18, loss = 0.0441799
I0816 15:04:54.150547 21700 caffe.cpp:313] Batch 19, accuracy/top1 = 0.982297
I0816 15:04:54.150569 21700 caffe.cpp:313] Batch 19, accuracy/top5 = 1
I0816 15:04:54.150574 21700 caffe.cpp:313] Batch 19, loss = 0.0502538
I0816 15:04:54.358497 21700 caffe.cpp:313] Batch 20, accuracy/top1 = 0.97564
I0816 15:04:54.358520 21700 caffe.cpp:313] Batch 20, accuracy/top5 = 1
I0816 15:04:54.358525 21700 caffe.cpp:313] Batch 20, loss = 0.0701572
I0816 15:04:54.565249 21700 caffe.cpp:313] Batch 21, accuracy/top1 = 0.89128
I0816 15:04:54.565270 21700 caffe.cpp:313] Batch 21, accuracy/top5 = 0.9999
I0816 15:04:54.565274 21700 caffe.cpp:313] Batch 21, loss = 0.601206
I0816 15:04:54.773988 21700 caffe.cpp:313] Batch 22, accuracy/top1 = 0.967232
I0816 15:04:54.774010 21700 caffe.cpp:313] Batch 22, accuracy/top5 = 1
I0816 15:04:54.774014 21700 caffe.cpp:313] Batch 22, loss = 0.0885853
I0816 15:04:54.981973 21700 caffe.cpp:313] Batch 23, accuracy/top1 = 0.977987
I0816 15:04:54.981997 21700 caffe.cpp:313] Batch 23, accuracy/top5 = 1
I0816 15:04:54.982002 21700 caffe.cpp:313] Batch 23, loss = 0.0591069
I0816 15:04:55.191258 21700 caffe.cpp:313] Batch 24, accuracy/top1 = 0.950704
I0816 15:04:55.191282 21700 caffe.cpp:313] Batch 24, accuracy/top5 = 1
I0816 15:04:55.191285 21700 caffe.cpp:313] Batch 24, loss = 0.127114
I0816 15:04:55.401715 21700 caffe.cpp:313] Batch 25, accuracy/top1 = 0.972822
I0816 15:04:55.401737 21700 caffe.cpp:313] Batch 25, accuracy/top5 = 1
I0816 15:04:55.401741 21700 caffe.cpp:313] Batch 25, loss = 0.0745312
I0816 15:04:55.607954 21700 caffe.cpp:313] Batch 26, accuracy/top1 = 0.952134
I0816 15:04:55.607978 21700 caffe.cpp:313] Batch 26, accuracy/top5 = 1
I0816 15:04:55.607982 21700 caffe.cpp:313] Batch 26, loss = 0.120336
I0816 15:04:55.815650 21700 caffe.cpp:313] Batch 27, accuracy/top1 = 0.96663
I0816 15:04:55.815672 21700 caffe.cpp:313] Batch 27, accuracy/top5 = 1
I0816 15:04:55.815677 21700 caffe.cpp:313] Batch 27, loss = 0.0966206
I0816 15:04:56.023260 21700 caffe.cpp:313] Batch 28, accuracy/top1 = 0.952988
I0816 15:04:56.023280 21700 caffe.cpp:313] Batch 28, accuracy/top5 = 1
I0816 15:04:56.023284 21700 caffe.cpp:313] Batch 28, loss = 0.125972
I0816 15:04:56.232661 21700 caffe.cpp:313] Batch 29, accuracy/top1 = 0.965673
I0816 15:04:56.232683 21700 caffe.cpp:313] Batch 29, accuracy/top5 = 1
I0816 15:04:56.232687 21700 caffe.cpp:313] Batch 29, loss = 0.105083
I0816 15:04:56.440552 21700 caffe.cpp:313] Batch 30, accuracy/top1 = 0.857988
I0816 15:04:56.440575 21700 caffe.cpp:313] Batch 30, accuracy/top5 = 1
I0816 15:04:56.440579 21700 caffe.cpp:313] Batch 30, loss = 0.685862
I0816 15:04:56.648664 21700 caffe.cpp:313] Batch 31, accuracy/top1 = 0.96817
I0816 15:04:56.648686 21700 caffe.cpp:313] Batch 31, accuracy/top5 = 1
I0816 15:04:56.648690 21700 caffe.cpp:313] Batch 31, loss = 0.0881977
I0816 15:04:56.857123 21700 caffe.cpp:313] Batch 32, accuracy/top1 = 0.950037
I0816 15:04:56.857147 21700 caffe.cpp:313] Batch 32, accuracy/top5 = 1
I0816 15:04:56.857151 21700 caffe.cpp:313] Batch 32, loss = 0.135017
I0816 15:04:57.066273 21700 caffe.cpp:313] Batch 33, accuracy/top1 = 0.968106
I0816 15:04:57.066293 21700 caffe.cpp:313] Batch 33, accuracy/top5 = 1
I0816 15:04:57.066298 21700 caffe.cpp:313] Batch 33, loss = 0.0850388
I0816 15:04:57.277372 21700 caffe.cpp:313] Batch 34, accuracy/top1 = 0.97743
I0816 15:04:57.277395 21700 caffe.cpp:313] Batch 34, accuracy/top5 = 1
I0816 15:04:57.277400 21700 caffe.cpp:313] Batch 34, loss = 0.0647
I0816 15:04:57.485177 21700 caffe.cpp:313] Batch 35, accuracy/top1 = 0.974825
I0816 15:04:57.485193 21700 caffe.cpp:313] Batch 35, accuracy/top5 = 1
I0816 15:04:57.485210 21700 caffe.cpp:313] Batch 35, loss = 0.0681908
I0816 15:04:57.690901 21700 caffe.cpp:313] Batch 36, accuracy/top1 = 0.963178
I0816 15:04:57.690923 21700 caffe.cpp:313] Batch 36, accuracy/top5 = 1
I0816 15:04:57.690927 21700 caffe.cpp:313] Batch 36, loss = 0.10449
I0816 15:04:57.896879 21700 caffe.cpp:313] Batch 37, accuracy/top1 = 0.961598
I0816 15:04:57.896903 21700 caffe.cpp:313] Batch 37, accuracy/top5 = 1
I0816 15:04:57.896905 21700 caffe.cpp:313] Batch 37, loss = 0.114748
I0816 15:04:58.104727 21700 caffe.cpp:313] Batch 38, accuracy/top1 = 0.941042
I0816 15:04:58.104745 21700 caffe.cpp:313] Batch 38, accuracy/top5 = 1
I0816 15:04:58.104748 21700 caffe.cpp:313] Batch 38, loss = 0.173815
I0816 15:04:58.309476 21700 caffe.cpp:313] Batch 39, accuracy/top1 = 0.916838
I0816 15:04:58.309499 21700 caffe.cpp:313] Batch 39, accuracy/top5 = 1
I0816 15:04:58.309502 21700 caffe.cpp:313] Batch 39, loss = 0.23062
I0816 15:04:58.518257 21700 caffe.cpp:313] Batch 40, accuracy/top1 = 0.980851
I0816 15:04:58.518278 21700 caffe.cpp:313] Batch 40, accuracy/top5 = 1
I0816 15:04:58.518281 21700 caffe.cpp:313] Batch 40, loss = 0.0585243
I0816 15:04:58.727596 21700 caffe.cpp:313] Batch 41, accuracy/top1 = 0.976921
I0816 15:04:58.727618 21700 caffe.cpp:313] Batch 41, accuracy/top5 = 1
I0816 15:04:58.727622 21700 caffe.cpp:313] Batch 41, loss = 0.0659568
I0816 15:04:58.937592 21700 caffe.cpp:313] Batch 42, accuracy/top1 = 0.972545
I0816 15:04:58.937614 21700 caffe.cpp:313] Batch 42, accuracy/top5 = 1
I0816 15:04:58.937618 21700 caffe.cpp:313] Batch 42, loss = 0.0757445
I0816 15:04:59.146862 21700 caffe.cpp:313] Batch 43, accuracy/top1 = 0.977328
I0816 15:04:59.146884 21700 caffe.cpp:313] Batch 43, accuracy/top5 = 1
I0816 15:04:59.146888 21700 caffe.cpp:313] Batch 43, loss = 0.065797
I0816 15:04:59.357535 21700 caffe.cpp:313] Batch 44, accuracy/top1 = 0.958067
I0816 15:04:59.357558 21700 caffe.cpp:313] Batch 44, accuracy/top5 = 1
I0816 15:04:59.357560 21700 caffe.cpp:313] Batch 44, loss = 0.117783
I0816 15:04:59.564972 21700 caffe.cpp:313] Batch 45, accuracy/top1 = 0.976776
I0816 15:04:59.564995 21700 caffe.cpp:313] Batch 45, accuracy/top5 = 1
I0816 15:04:59.564997 21700 caffe.cpp:313] Batch 45, loss = 0.0725977
I0816 15:04:59.775602 21700 caffe.cpp:313] Batch 46, accuracy/top1 = 0.971757
I0816 15:04:59.775625 21700 caffe.cpp:313] Batch 46, accuracy/top5 = 1
I0816 15:04:59.775629 21700 caffe.cpp:313] Batch 46, loss = 0.0761611
I0816 15:04:59.986062 21700 caffe.cpp:313] Batch 47, accuracy/top1 = 0.967722
I0816 15:04:59.986083 21700 caffe.cpp:313] Batch 47, accuracy/top5 = 1
I0816 15:04:59.986086 21700 caffe.cpp:313] Batch 47, loss = 0.120214
I0816 15:05:00.192793 21700 caffe.cpp:313] Batch 48, accuracy/top1 = 0.876008
I0816 15:05:00.192813 21700 caffe.cpp:313] Batch 48, accuracy/top5 = 1
I0816 15:05:00.192816 21700 caffe.cpp:313] Batch 48, loss = 0.461427
I0816 15:05:00.398716 21700 caffe.cpp:313] Batch 49, accuracy/top1 = 0.949904
I0816 15:05:00.398739 21700 caffe.cpp:313] Batch 49, accuracy/top5 = 1
I0816 15:05:00.398742 21700 caffe.cpp:313] Batch 49, loss = 0.134805
I0816 15:05:00.398744 21700 caffe.cpp:318] Loss: 0.163338
I0816 15:05:00.398751 21700 caffe.cpp:330] accuracy/top1 = 0.952557
I0816 15:05:00.398756 21700 caffe.cpp:330] accuracy/top5 = 0.999996
I0816 15:05:00.398761 21700 caffe.cpp:330] loss = 0.163338 (* 1 = 0.163338 loss)

