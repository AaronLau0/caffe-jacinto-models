Logging output to training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/train-log_2017-07-31_18-11-04.txt
Using pretrained model training/imagenet_jacintonet11v2_iter_320000.caffemodel
training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/initial
I0731 18:11:05.542618 16855 caffe.cpp:608] This is NVCaffe 0.16.3 started at Mon Jul 31 18:11:05 2017
I0731 18:11:05.542742 16855 caffe.cpp:611] CuDNN version: 6021
I0731 18:11:05.542745 16855 caffe.cpp:612] CuBLAS version: 8000
I0731 18:11:05.542747 16855 caffe.cpp:613] CUDA version: 8000
I0731 18:11:05.542749 16855 caffe.cpp:614] CUDA driver version: 8000
I0731 18:11:05.832962 16855 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0731 18:11:05.833528 16855 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I0731 18:11:05.834048 16855 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[1]: total=8508145664 free=8379236352
I0731 18:11:05.834560 16855 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[2]: total=8508145664 free=8379236352
I0731 18:11:05.834568 16855 caffe.cpp:208] Using GPUs 0, 1, 2
I0731 18:11:05.834890 16855 caffe.cpp:213] GPU 0: GeForce GTX 1080
I0731 18:11:05.835211 16855 caffe.cpp:213] GPU 1: GeForce GTX 1080
I0731 18:11:05.835532 16855 caffe.cpp:213] GPU 2: GeForce GTX 1080
I0731 18:11:05.835566 16855 solver.cpp:42] Solver data type: FLOAT
I0731 18:11:05.835595 16855 solver.cpp:45] Initializing solver from parameters: 
train_net: "training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/initial/train.prototxt"
test_net: "training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/initial/test.prototxt"
test_iter: 125
test_interval: 2000
base_lr: 0.0001
display: 100
max_iter: 32000
lr_policy: "multistep"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 0.0001
snapshot: 10000
snapshot_prefix: "training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/initial/cityscapes5_jsegnet21v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
test_initialization: false
stepvalue: 24000
iter_size: 1
type: "Adam"
I0731 18:11:05.841791 16855 solver.cpp:77] Creating training net from train_net file: training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/initial/train.prototxt
I0731 18:11:05.842391 16855 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0731 18:11:05.842398 16855 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
W0731 18:11:05.842432 16855 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0731 18:11:05.842669 16855 net.cpp:72] Initializing net from parameters: 
name: "jsegnet21v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/train-image-lmdb"
    label_list_path: "data/train-label-lmdb"
    batch_size: 6
    shuffle: false
    threads: 1
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
I0731 18:11:05.842805 16855 net.cpp:104] Using FLOAT as default forward math type
I0731 18:11:05.842810 16855 net.cpp:110] Using FLOAT as default backward math type
I0731 18:11:05.842813 16855 layer_factory.hpp:136] Creating layer 'data' of type 'ImageLabelData'
I0731 18:11:05.842816 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:05.842828 16855 net.cpp:184] Created Layer data (0)
I0731 18:11:05.842831 16855 net.cpp:530] data -> data
I0731 18:11:05.842842 16855 net.cpp:530] data -> label
I0731 18:11:05.842906 16855 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0731 18:11:05.842918 16855 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 18:11:05.844923 16874 db_lmdb.cpp:35] Opened lmdb data/train-image-lmdb
I0731 18:11:05.847651 16855 data_layer.cpp:184] [0] ReshapePrefetch 6, 3, 640, 640
I0731 18:11:05.847714 16855 data_layer.cpp:208] [0] Output data size: 6, 3, 640, 640
I0731 18:11:05.847721 16855 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 18:11:05.847877 16855 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0731 18:11:05.847887 16855 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 18:11:05.848757 16876 db_lmdb.cpp:35] Opened lmdb data/train-label-lmdb
I0731 18:11:05.849496 16875 data_layer.cpp:97] [0] Parser threads: 1
I0731 18:11:05.849509 16875 data_layer.cpp:99] [0] Transformer threads: 1
I0731 18:11:05.855829 16855 data_layer.cpp:184] [0] ReshapePrefetch 6, 1, 640, 640
I0731 18:11:05.856003 16855 data_layer.cpp:208] [0] Output data size: 6, 1, 640, 640
I0731 18:11:05.856030 16855 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 18:11:05.856277 16855 net.cpp:245] Setting up data
I0731 18:11:05.856313 16855 net.cpp:252] TRAIN Top shape for layer 0 'data' 6 3 640 640 (7372800)
I0731 18:11:05.856333 16855 net.cpp:252] TRAIN Top shape for layer 0 'data' 6 1 640 640 (2457600)
I0731 18:11:05.856350 16855 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0731 18:11:05.856365 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:05.856407 16855 net.cpp:184] Created Layer data/bias (1)
I0731 18:11:05.856421 16855 net.cpp:561] data/bias <- data
I0731 18:11:05.856442 16855 net.cpp:530] data/bias -> data/bias
I0731 18:11:05.858108 16877 data_layer.cpp:97] [0] Parser threads: 1
I0731 18:11:05.858134 16877 data_layer.cpp:99] [0] Transformer threads: 1
I0731 18:11:05.862846 16855 net.cpp:245] Setting up data/bias
I0731 18:11:05.862987 16855 net.cpp:252] TRAIN Top shape for layer 1 'data/bias' 6 3 640 640 (7372800)
I0731 18:11:05.863034 16855 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0731 18:11:05.863051 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:05.863116 16855 net.cpp:184] Created Layer conv1a (2)
I0731 18:11:05.863123 16855 net.cpp:561] conv1a <- data/bias
I0731 18:11:05.863131 16855 net.cpp:530] conv1a -> conv1a
I0731 18:11:06.239864 16855 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 7.89G, req 0G)
I0731 18:11:06.239900 16855 net.cpp:245] Setting up conv1a
I0731 18:11:06.239914 16855 net.cpp:252] TRAIN Top shape for layer 2 'conv1a' 6 32 320 320 (19660800)
I0731 18:11:06.239936 16855 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0731 18:11:06.239946 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.239975 16855 net.cpp:184] Created Layer conv1a/bn (3)
I0731 18:11:06.239984 16855 net.cpp:561] conv1a/bn <- conv1a
I0731 18:11:06.239994 16855 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0731 18:11:06.241240 16855 net.cpp:245] Setting up conv1a/bn
I0731 18:11:06.241258 16855 net.cpp:252] TRAIN Top shape for layer 3 'conv1a/bn' 6 32 320 320 (19660800)
I0731 18:11:06.241279 16855 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0731 18:11:06.241287 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.241300 16855 net.cpp:184] Created Layer conv1a/relu (4)
I0731 18:11:06.241308 16855 net.cpp:561] conv1a/relu <- conv1a
I0731 18:11:06.241315 16855 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0731 18:11:06.241343 16855 net.cpp:245] Setting up conv1a/relu
I0731 18:11:06.241353 16855 net.cpp:252] TRAIN Top shape for layer 4 'conv1a/relu' 6 32 320 320 (19660800)
I0731 18:11:06.241361 16855 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0731 18:11:06.241370 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.241396 16855 net.cpp:184] Created Layer conv1b (5)
I0731 18:11:06.241405 16855 net.cpp:561] conv1b <- conv1a
I0731 18:11:06.241411 16855 net.cpp:530] conv1b -> conv1b
I0731 18:11:06.294466 16855 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.73G, req 0G)
I0731 18:11:06.294549 16855 net.cpp:245] Setting up conv1b
I0731 18:11:06.294585 16855 net.cpp:252] TRAIN Top shape for layer 5 'conv1b' 6 32 320 320 (19660800)
I0731 18:11:06.294636 16855 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0731 18:11:06.294661 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.294700 16855 net.cpp:184] Created Layer conv1b/bn (6)
I0731 18:11:06.294719 16855 net.cpp:561] conv1b/bn <- conv1b
I0731 18:11:06.294742 16855 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0731 18:11:06.297818 16855 net.cpp:245] Setting up conv1b/bn
I0731 18:11:06.297864 16855 net.cpp:252] TRAIN Top shape for layer 6 'conv1b/bn' 6 32 320 320 (19660800)
I0731 18:11:06.297914 16855 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0731 18:11:06.297935 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.297979 16855 net.cpp:184] Created Layer conv1b/relu (7)
I0731 18:11:06.297999 16855 net.cpp:561] conv1b/relu <- conv1b
I0731 18:11:06.298019 16855 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0731 18:11:06.298054 16855 net.cpp:245] Setting up conv1b/relu
I0731 18:11:06.298079 16855 net.cpp:252] TRAIN Top shape for layer 7 'conv1b/relu' 6 32 320 320 (19660800)
I0731 18:11:06.298101 16855 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0731 18:11:06.298125 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.298163 16855 net.cpp:184] Created Layer pool1 (8)
I0731 18:11:06.298182 16855 net.cpp:561] pool1 <- conv1b
I0731 18:11:06.298202 16855 net.cpp:530] pool1 -> pool1
I0731 18:11:06.298560 16855 net.cpp:245] Setting up pool1
I0731 18:11:06.298593 16855 net.cpp:252] TRAIN Top shape for layer 8 'pool1' 6 32 160 160 (4915200)
I0731 18:11:06.298616 16855 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0731 18:11:06.298671 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.298729 16855 net.cpp:184] Created Layer res2a_branch2a (9)
I0731 18:11:06.298749 16855 net.cpp:561] res2a_branch2a <- pool1
I0731 18:11:06.298770 16855 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0731 18:11:06.344637 16855 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.6G, req 0G)
I0731 18:11:06.344661 16855 net.cpp:245] Setting up res2a_branch2a
I0731 18:11:06.344669 16855 net.cpp:252] TRAIN Top shape for layer 9 'res2a_branch2a' 6 64 160 160 (9830400)
I0731 18:11:06.344681 16855 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0731 18:11:06.344687 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.344699 16855 net.cpp:184] Created Layer res2a_branch2a/bn (10)
I0731 18:11:06.344704 16855 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0731 18:11:06.344719 16855 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0731 18:11:06.345924 16855 net.cpp:245] Setting up res2a_branch2a/bn
I0731 18:11:06.345934 16855 net.cpp:252] TRAIN Top shape for layer 10 'res2a_branch2a/bn' 6 64 160 160 (9830400)
I0731 18:11:06.345945 16855 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0731 18:11:06.345950 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.345957 16855 net.cpp:184] Created Layer res2a_branch2a/relu (11)
I0731 18:11:06.345960 16855 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0731 18:11:06.345964 16855 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0731 18:11:06.345971 16855 net.cpp:245] Setting up res2a_branch2a/relu
I0731 18:11:06.345975 16855 net.cpp:252] TRAIN Top shape for layer 11 'res2a_branch2a/relu' 6 64 160 160 (9830400)
I0731 18:11:06.345980 16855 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0731 18:11:06.345984 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.345995 16855 net.cpp:184] Created Layer res2a_branch2b (12)
I0731 18:11:06.345999 16855 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0731 18:11:06.346002 16855 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0731 18:11:06.367775 16855 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.52G, req 0G)
I0731 18:11:06.367799 16855 net.cpp:245] Setting up res2a_branch2b
I0731 18:11:06.367805 16855 net.cpp:252] TRAIN Top shape for layer 12 'res2a_branch2b' 6 64 160 160 (9830400)
I0731 18:11:06.367813 16855 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0731 18:11:06.367820 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.367830 16855 net.cpp:184] Created Layer res2a_branch2b/bn (13)
I0731 18:11:06.367833 16855 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0731 18:11:06.367837 16855 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0731 18:11:06.368733 16855 net.cpp:245] Setting up res2a_branch2b/bn
I0731 18:11:06.368744 16855 net.cpp:252] TRAIN Top shape for layer 13 'res2a_branch2b/bn' 6 64 160 160 (9830400)
I0731 18:11:06.368752 16855 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0731 18:11:06.368757 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.368762 16855 net.cpp:184] Created Layer res2a_branch2b/relu (14)
I0731 18:11:06.368765 16855 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0731 18:11:06.368768 16855 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0731 18:11:06.368773 16855 net.cpp:245] Setting up res2a_branch2b/relu
I0731 18:11:06.368777 16855 net.cpp:252] TRAIN Top shape for layer 14 'res2a_branch2b/relu' 6 64 160 160 (9830400)
I0731 18:11:06.368796 16855 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0731 18:11:06.368803 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.368813 16855 net.cpp:184] Created Layer pool2 (15)
I0731 18:11:06.368834 16855 net.cpp:561] pool2 <- res2a_branch2b
I0731 18:11:06.368839 16855 net.cpp:530] pool2 -> pool2
I0731 18:11:06.368943 16855 net.cpp:245] Setting up pool2
I0731 18:11:06.368952 16855 net.cpp:252] TRAIN Top shape for layer 15 'pool2' 6 64 80 80 (2457600)
I0731 18:11:06.368957 16855 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0731 18:11:06.368963 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.368975 16855 net.cpp:184] Created Layer res3a_branch2a (16)
I0731 18:11:06.368980 16855 net.cpp:561] res3a_branch2a <- pool2
I0731 18:11:06.368986 16855 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0731 18:11:06.391413 16855 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 7.46G, req 0G)
I0731 18:11:06.391433 16855 net.cpp:245] Setting up res3a_branch2a
I0731 18:11:06.391441 16855 net.cpp:252] TRAIN Top shape for layer 16 'res3a_branch2a' 6 128 80 80 (4915200)
I0731 18:11:06.391454 16855 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0731 18:11:06.391461 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.391472 16855 net.cpp:184] Created Layer res3a_branch2a/bn (17)
I0731 18:11:06.391479 16855 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0731 18:11:06.391484 16855 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0731 18:11:06.392383 16855 net.cpp:245] Setting up res3a_branch2a/bn
I0731 18:11:06.392395 16855 net.cpp:252] TRAIN Top shape for layer 17 'res3a_branch2a/bn' 6 128 80 80 (4915200)
I0731 18:11:06.392410 16855 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0731 18:11:06.392416 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.392423 16855 net.cpp:184] Created Layer res3a_branch2a/relu (18)
I0731 18:11:06.392429 16855 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0731 18:11:06.392434 16855 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0731 18:11:06.392443 16855 net.cpp:245] Setting up res3a_branch2a/relu
I0731 18:11:06.392451 16855 net.cpp:252] TRAIN Top shape for layer 18 'res3a_branch2a/relu' 6 128 80 80 (4915200)
I0731 18:11:06.392457 16855 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0731 18:11:06.392462 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.392477 16855 net.cpp:184] Created Layer res3a_branch2b (19)
I0731 18:11:06.392482 16855 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0731 18:11:06.392488 16855 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0731 18:11:06.404358 16855 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.42G, req 0G)
I0731 18:11:06.404381 16855 net.cpp:245] Setting up res3a_branch2b
I0731 18:11:06.404391 16855 net.cpp:252] TRAIN Top shape for layer 19 'res3a_branch2b' 6 128 80 80 (4915200)
I0731 18:11:06.404403 16855 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0731 18:11:06.404410 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.404420 16855 net.cpp:184] Created Layer res3a_branch2b/bn (20)
I0731 18:11:06.404426 16855 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0731 18:11:06.404433 16855 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0731 18:11:06.405292 16855 net.cpp:245] Setting up res3a_branch2b/bn
I0731 18:11:06.405304 16855 net.cpp:252] TRAIN Top shape for layer 20 'res3a_branch2b/bn' 6 128 80 80 (4915200)
I0731 18:11:06.405316 16855 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0731 18:11:06.405334 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.405342 16855 net.cpp:184] Created Layer res3a_branch2b/relu (21)
I0731 18:11:06.405349 16855 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0731 18:11:06.405354 16855 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0731 18:11:06.405364 16855 net.cpp:245] Setting up res3a_branch2b/relu
I0731 18:11:06.405369 16855 net.cpp:252] TRAIN Top shape for layer 21 'res3a_branch2b/relu' 6 128 80 80 (4915200)
I0731 18:11:06.405375 16855 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0731 18:11:06.405381 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.405391 16855 net.cpp:184] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (22)
I0731 18:11:06.405396 16855 net.cpp:561] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0731 18:11:06.405401 16855 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0731 18:11:06.405408 16855 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0731 18:11:06.405472 16855 net.cpp:245] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0731 18:11:06.405479 16855 net.cpp:252] TRAIN Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 6 128 80 80 (4915200)
I0731 18:11:06.405488 16855 net.cpp:252] TRAIN Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 6 128 80 80 (4915200)
I0731 18:11:06.405493 16855 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0731 18:11:06.405499 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.405508 16855 net.cpp:184] Created Layer pool3 (23)
I0731 18:11:06.405514 16855 net.cpp:561] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0731 18:11:06.405520 16855 net.cpp:530] pool3 -> pool3
I0731 18:11:06.405604 16855 net.cpp:245] Setting up pool3
I0731 18:11:06.405611 16855 net.cpp:252] TRAIN Top shape for layer 23 'pool3' 6 128 40 40 (1228800)
I0731 18:11:06.405617 16855 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0731 18:11:06.405623 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.405637 16855 net.cpp:184] Created Layer res4a_branch2a (24)
I0731 18:11:06.405642 16855 net.cpp:561] res4a_branch2a <- pool3
I0731 18:11:06.405647 16855 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0731 18:11:06.433912 16855 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.38G, req 0G)
I0731 18:11:06.433936 16855 net.cpp:245] Setting up res4a_branch2a
I0731 18:11:06.433945 16855 net.cpp:252] TRAIN Top shape for layer 24 'res4a_branch2a' 6 256 40 40 (2457600)
I0731 18:11:06.433959 16855 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0731 18:11:06.433965 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.433982 16855 net.cpp:184] Created Layer res4a_branch2a/bn (25)
I0731 18:11:06.433989 16855 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0731 18:11:06.433996 16855 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0731 18:11:06.434900 16855 net.cpp:245] Setting up res4a_branch2a/bn
I0731 18:11:06.434914 16855 net.cpp:252] TRAIN Top shape for layer 25 'res4a_branch2a/bn' 6 256 40 40 (2457600)
I0731 18:11:06.434928 16855 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0731 18:11:06.434934 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.434942 16855 net.cpp:184] Created Layer res4a_branch2a/relu (26)
I0731 18:11:06.434947 16855 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0731 18:11:06.434952 16855 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0731 18:11:06.434973 16855 net.cpp:245] Setting up res4a_branch2a/relu
I0731 18:11:06.434980 16855 net.cpp:252] TRAIN Top shape for layer 26 'res4a_branch2a/relu' 6 256 40 40 (2457600)
I0731 18:11:06.434985 16855 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0731 18:11:06.434993 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.435005 16855 net.cpp:184] Created Layer res4a_branch2b (27)
I0731 18:11:06.435010 16855 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0731 18:11:06.435016 16855 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0731 18:11:06.447881 16855 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.36G, req 0G)
I0731 18:11:06.447901 16855 net.cpp:245] Setting up res4a_branch2b
I0731 18:11:06.447911 16855 net.cpp:252] TRAIN Top shape for layer 27 'res4a_branch2b' 6 256 40 40 (2457600)
I0731 18:11:06.447921 16855 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0731 18:11:06.447928 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.447937 16855 net.cpp:184] Created Layer res4a_branch2b/bn (28)
I0731 18:11:06.447943 16855 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0731 18:11:06.447948 16855 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0731 18:11:06.448832 16855 net.cpp:245] Setting up res4a_branch2b/bn
I0731 18:11:06.448844 16855 net.cpp:252] TRAIN Top shape for layer 28 'res4a_branch2b/bn' 6 256 40 40 (2457600)
I0731 18:11:06.448856 16855 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0731 18:11:06.448863 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.448870 16855 net.cpp:184] Created Layer res4a_branch2b/relu (29)
I0731 18:11:06.448876 16855 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0731 18:11:06.448881 16855 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0731 18:11:06.448890 16855 net.cpp:245] Setting up res4a_branch2b/relu
I0731 18:11:06.448897 16855 net.cpp:252] TRAIN Top shape for layer 29 'res4a_branch2b/relu' 6 256 40 40 (2457600)
I0731 18:11:06.448904 16855 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0731 18:11:06.448909 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.448918 16855 net.cpp:184] Created Layer pool4 (30)
I0731 18:11:06.448923 16855 net.cpp:561] pool4 <- res4a_branch2b
I0731 18:11:06.448930 16855 net.cpp:530] pool4 -> pool4
I0731 18:11:06.449025 16855 net.cpp:245] Setting up pool4
I0731 18:11:06.449033 16855 net.cpp:252] TRAIN Top shape for layer 30 'pool4' 6 256 40 40 (2457600)
I0731 18:11:06.449039 16855 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0731 18:11:06.449046 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.449065 16855 net.cpp:184] Created Layer res5a_branch2a (31)
I0731 18:11:06.449071 16855 net.cpp:561] res5a_branch2a <- pool4
I0731 18:11:06.449077 16855 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0731 18:11:06.479456 16855 net.cpp:245] Setting up res5a_branch2a
I0731 18:11:06.479477 16855 net.cpp:252] TRAIN Top shape for layer 31 'res5a_branch2a' 6 512 40 40 (4915200)
I0731 18:11:06.479486 16855 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0731 18:11:06.479491 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.479503 16855 net.cpp:184] Created Layer res5a_branch2a/bn (32)
I0731 18:11:06.479507 16855 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0731 18:11:06.479512 16855 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0731 18:11:06.480149 16855 net.cpp:245] Setting up res5a_branch2a/bn
I0731 18:11:06.480157 16855 net.cpp:252] TRAIN Top shape for layer 32 'res5a_branch2a/bn' 6 512 40 40 (4915200)
I0731 18:11:06.480175 16855 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0731 18:11:06.480180 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.480186 16855 net.cpp:184] Created Layer res5a_branch2a/relu (33)
I0731 18:11:06.480190 16855 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0731 18:11:06.480195 16855 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0731 18:11:06.480201 16855 net.cpp:245] Setting up res5a_branch2a/relu
I0731 18:11:06.480206 16855 net.cpp:252] TRAIN Top shape for layer 33 'res5a_branch2a/relu' 6 512 40 40 (4915200)
I0731 18:11:06.480211 16855 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0731 18:11:06.480214 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.480224 16855 net.cpp:184] Created Layer res5a_branch2b (34)
I0731 18:11:06.480227 16855 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0731 18:11:06.480232 16855 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0731 18:11:06.493242 16855 net.cpp:245] Setting up res5a_branch2b
I0731 18:11:06.493266 16855 net.cpp:252] TRAIN Top shape for layer 34 'res5a_branch2b' 6 512 40 40 (4915200)
I0731 18:11:06.493278 16855 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0731 18:11:06.493285 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.493294 16855 net.cpp:184] Created Layer res5a_branch2b/bn (35)
I0731 18:11:06.493299 16855 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0731 18:11:06.493305 16855 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0731 18:11:06.493952 16855 net.cpp:245] Setting up res5a_branch2b/bn
I0731 18:11:06.493959 16855 net.cpp:252] TRAIN Top shape for layer 35 'res5a_branch2b/bn' 6 512 40 40 (4915200)
I0731 18:11:06.493968 16855 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0731 18:11:06.493973 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.493978 16855 net.cpp:184] Created Layer res5a_branch2b/relu (36)
I0731 18:11:06.493981 16855 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0731 18:11:06.493986 16855 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0731 18:11:06.493993 16855 net.cpp:245] Setting up res5a_branch2b/relu
I0731 18:11:06.493998 16855 net.cpp:252] TRAIN Top shape for layer 36 'res5a_branch2b/relu' 6 512 40 40 (4915200)
I0731 18:11:06.494001 16855 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I0731 18:11:06.494006 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.494022 16855 net.cpp:184] Created Layer out5a (37)
I0731 18:11:06.494025 16855 net.cpp:561] out5a <- res5a_branch2b
I0731 18:11:06.494030 16855 net.cpp:530] out5a -> out5a
I0731 18:11:06.498297 16855 net.cpp:245] Setting up out5a
I0731 18:11:06.498307 16855 net.cpp:252] TRAIN Top shape for layer 37 'out5a' 6 64 40 40 (614400)
I0731 18:11:06.498314 16855 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I0731 18:11:06.498319 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.498330 16855 net.cpp:184] Created Layer out5a/bn (38)
I0731 18:11:06.498335 16855 net.cpp:561] out5a/bn <- out5a
I0731 18:11:06.498339 16855 net.cpp:513] out5a/bn -> out5a (in-place)
I0731 18:11:06.498994 16855 net.cpp:245] Setting up out5a/bn
I0731 18:11:06.499002 16855 net.cpp:252] TRAIN Top shape for layer 38 'out5a/bn' 6 64 40 40 (614400)
I0731 18:11:06.499011 16855 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I0731 18:11:06.499016 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.499020 16855 net.cpp:184] Created Layer out5a/relu (39)
I0731 18:11:06.499024 16855 net.cpp:561] out5a/relu <- out5a
I0731 18:11:06.499028 16855 net.cpp:513] out5a/relu -> out5a (in-place)
I0731 18:11:06.499042 16855 net.cpp:245] Setting up out5a/relu
I0731 18:11:06.499048 16855 net.cpp:252] TRAIN Top shape for layer 39 'out5a/relu' 6 64 40 40 (614400)
I0731 18:11:06.499053 16855 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I0731 18:11:06.499056 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.499071 16855 net.cpp:184] Created Layer out5a_up2 (40)
I0731 18:11:06.499075 16855 net.cpp:561] out5a_up2 <- out5a
I0731 18:11:06.499079 16855 net.cpp:530] out5a_up2 -> out5a_up2
I0731 18:11:06.499369 16855 net.cpp:245] Setting up out5a_up2
I0731 18:11:06.499377 16855 net.cpp:252] TRAIN Top shape for layer 40 'out5a_up2' 6 64 80 80 (2457600)
I0731 18:11:06.499382 16855 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I0731 18:11:06.499387 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.499402 16855 net.cpp:184] Created Layer out3a (41)
I0731 18:11:06.499406 16855 net.cpp:561] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0731 18:11:06.499411 16855 net.cpp:530] out3a -> out3a
I0731 18:11:06.512928 16855 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 7.3G, req 0G)
I0731 18:11:06.512941 16855 net.cpp:245] Setting up out3a
I0731 18:11:06.512949 16855 net.cpp:252] TRAIN Top shape for layer 41 'out3a' 6 64 80 80 (2457600)
I0731 18:11:06.512958 16855 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I0731 18:11:06.512961 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.512969 16855 net.cpp:184] Created Layer out3a/bn (42)
I0731 18:11:06.512974 16855 net.cpp:561] out3a/bn <- out3a
I0731 18:11:06.512977 16855 net.cpp:513] out3a/bn -> out3a (in-place)
I0731 18:11:06.513710 16855 net.cpp:245] Setting up out3a/bn
I0731 18:11:06.513720 16855 net.cpp:252] TRAIN Top shape for layer 42 'out3a/bn' 6 64 80 80 (2457600)
I0731 18:11:06.513728 16855 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I0731 18:11:06.513732 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.513737 16855 net.cpp:184] Created Layer out3a/relu (43)
I0731 18:11:06.513741 16855 net.cpp:561] out3a/relu <- out3a
I0731 18:11:06.513746 16855 net.cpp:513] out3a/relu -> out3a (in-place)
I0731 18:11:06.513751 16855 net.cpp:245] Setting up out3a/relu
I0731 18:11:06.513756 16855 net.cpp:252] TRAIN Top shape for layer 43 'out3a/relu' 6 64 80 80 (2457600)
I0731 18:11:06.513761 16855 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I0731 18:11:06.513764 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.513777 16855 net.cpp:184] Created Layer out3_out5_combined (44)
I0731 18:11:06.513782 16855 net.cpp:561] out3_out5_combined <- out5a_up2
I0731 18:11:06.513785 16855 net.cpp:561] out3_out5_combined <- out3a
I0731 18:11:06.513789 16855 net.cpp:530] out3_out5_combined -> out3_out5_combined
I0731 18:11:06.514809 16855 net.cpp:245] Setting up out3_out5_combined
I0731 18:11:06.514822 16855 net.cpp:252] TRAIN Top shape for layer 44 'out3_out5_combined' 6 64 80 80 (2457600)
I0731 18:11:06.514827 16855 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I0731 18:11:06.514832 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.514844 16855 net.cpp:184] Created Layer ctx_conv1 (45)
I0731 18:11:06.514848 16855 net.cpp:561] ctx_conv1 <- out3_out5_combined
I0731 18:11:06.514853 16855 net.cpp:530] ctx_conv1 -> ctx_conv1
I0731 18:11:06.529580 16855 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 7.24G, req 0G)
I0731 18:11:06.529599 16855 net.cpp:245] Setting up ctx_conv1
I0731 18:11:06.529606 16855 net.cpp:252] TRAIN Top shape for layer 45 'ctx_conv1' 6 64 80 80 (2457600)
I0731 18:11:06.529624 16855 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I0731 18:11:06.529629 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.529645 16855 net.cpp:184] Created Layer ctx_conv1/bn (46)
I0731 18:11:06.529649 16855 net.cpp:561] ctx_conv1/bn <- ctx_conv1
I0731 18:11:06.529654 16855 net.cpp:513] ctx_conv1/bn -> ctx_conv1 (in-place)
I0731 18:11:06.530382 16855 net.cpp:245] Setting up ctx_conv1/bn
I0731 18:11:06.530391 16855 net.cpp:252] TRAIN Top shape for layer 46 'ctx_conv1/bn' 6 64 80 80 (2457600)
I0731 18:11:06.530400 16855 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I0731 18:11:06.530405 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.530411 16855 net.cpp:184] Created Layer ctx_conv1/relu (47)
I0731 18:11:06.530414 16855 net.cpp:561] ctx_conv1/relu <- ctx_conv1
I0731 18:11:06.530418 16855 net.cpp:513] ctx_conv1/relu -> ctx_conv1 (in-place)
I0731 18:11:06.530424 16855 net.cpp:245] Setting up ctx_conv1/relu
I0731 18:11:06.530429 16855 net.cpp:252] TRAIN Top shape for layer 47 'ctx_conv1/relu' 6 64 80 80 (2457600)
I0731 18:11:06.530434 16855 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I0731 18:11:06.530438 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.530449 16855 net.cpp:184] Created Layer ctx_conv2 (48)
I0731 18:11:06.530452 16855 net.cpp:561] ctx_conv2 <- ctx_conv1
I0731 18:11:06.530457 16855 net.cpp:530] ctx_conv2 -> ctx_conv2
I0731 18:11:06.531564 16855 net.cpp:245] Setting up ctx_conv2
I0731 18:11:06.531572 16855 net.cpp:252] TRAIN Top shape for layer 48 'ctx_conv2' 6 64 80 80 (2457600)
I0731 18:11:06.531579 16855 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I0731 18:11:06.531584 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.531589 16855 net.cpp:184] Created Layer ctx_conv2/bn (49)
I0731 18:11:06.531594 16855 net.cpp:561] ctx_conv2/bn <- ctx_conv2
I0731 18:11:06.531597 16855 net.cpp:513] ctx_conv2/bn -> ctx_conv2 (in-place)
I0731 18:11:06.532261 16855 net.cpp:245] Setting up ctx_conv2/bn
I0731 18:11:06.532269 16855 net.cpp:252] TRAIN Top shape for layer 49 'ctx_conv2/bn' 6 64 80 80 (2457600)
I0731 18:11:06.532276 16855 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I0731 18:11:06.532280 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.532286 16855 net.cpp:184] Created Layer ctx_conv2/relu (50)
I0731 18:11:06.532290 16855 net.cpp:561] ctx_conv2/relu <- ctx_conv2
I0731 18:11:06.532294 16855 net.cpp:513] ctx_conv2/relu -> ctx_conv2 (in-place)
I0731 18:11:06.532300 16855 net.cpp:245] Setting up ctx_conv2/relu
I0731 18:11:06.532305 16855 net.cpp:252] TRAIN Top shape for layer 50 'ctx_conv2/relu' 6 64 80 80 (2457600)
I0731 18:11:06.532310 16855 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I0731 18:11:06.532313 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.532326 16855 net.cpp:184] Created Layer ctx_conv3 (51)
I0731 18:11:06.532330 16855 net.cpp:561] ctx_conv3 <- ctx_conv2
I0731 18:11:06.532335 16855 net.cpp:530] ctx_conv3 -> ctx_conv3
I0731 18:11:06.533432 16855 net.cpp:245] Setting up ctx_conv3
I0731 18:11:06.533440 16855 net.cpp:252] TRAIN Top shape for layer 51 'ctx_conv3' 6 64 80 80 (2457600)
I0731 18:11:06.533447 16855 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I0731 18:11:06.533450 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.533457 16855 net.cpp:184] Created Layer ctx_conv3/bn (52)
I0731 18:11:06.533462 16855 net.cpp:561] ctx_conv3/bn <- ctx_conv3
I0731 18:11:06.533465 16855 net.cpp:513] ctx_conv3/bn -> ctx_conv3 (in-place)
I0731 18:11:06.534131 16855 net.cpp:245] Setting up ctx_conv3/bn
I0731 18:11:06.534143 16855 net.cpp:252] TRAIN Top shape for layer 52 'ctx_conv3/bn' 6 64 80 80 (2457600)
I0731 18:11:06.534152 16855 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I0731 18:11:06.534157 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.534162 16855 net.cpp:184] Created Layer ctx_conv3/relu (53)
I0731 18:11:06.534166 16855 net.cpp:561] ctx_conv3/relu <- ctx_conv3
I0731 18:11:06.534170 16855 net.cpp:513] ctx_conv3/relu -> ctx_conv3 (in-place)
I0731 18:11:06.534176 16855 net.cpp:245] Setting up ctx_conv3/relu
I0731 18:11:06.534181 16855 net.cpp:252] TRAIN Top shape for layer 53 'ctx_conv3/relu' 6 64 80 80 (2457600)
I0731 18:11:06.534185 16855 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I0731 18:11:06.534189 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.534198 16855 net.cpp:184] Created Layer ctx_conv4 (54)
I0731 18:11:06.534201 16855 net.cpp:561] ctx_conv4 <- ctx_conv3
I0731 18:11:06.534205 16855 net.cpp:530] ctx_conv4 -> ctx_conv4
I0731 18:11:06.535293 16855 net.cpp:245] Setting up ctx_conv4
I0731 18:11:06.535301 16855 net.cpp:252] TRAIN Top shape for layer 54 'ctx_conv4' 6 64 80 80 (2457600)
I0731 18:11:06.535307 16855 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I0731 18:11:06.535311 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.535318 16855 net.cpp:184] Created Layer ctx_conv4/bn (55)
I0731 18:11:06.535321 16855 net.cpp:561] ctx_conv4/bn <- ctx_conv4
I0731 18:11:06.535326 16855 net.cpp:513] ctx_conv4/bn -> ctx_conv4 (in-place)
I0731 18:11:06.535982 16855 net.cpp:245] Setting up ctx_conv4/bn
I0731 18:11:06.535990 16855 net.cpp:252] TRAIN Top shape for layer 55 'ctx_conv4/bn' 6 64 80 80 (2457600)
I0731 18:11:06.535998 16855 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I0731 18:11:06.536002 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.536007 16855 net.cpp:184] Created Layer ctx_conv4/relu (56)
I0731 18:11:06.536011 16855 net.cpp:561] ctx_conv4/relu <- ctx_conv4
I0731 18:11:06.536015 16855 net.cpp:513] ctx_conv4/relu -> ctx_conv4 (in-place)
I0731 18:11:06.536021 16855 net.cpp:245] Setting up ctx_conv4/relu
I0731 18:11:06.536026 16855 net.cpp:252] TRAIN Top shape for layer 56 'ctx_conv4/relu' 6 64 80 80 (2457600)
I0731 18:11:06.536031 16855 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I0731 18:11:06.536034 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.536047 16855 net.cpp:184] Created Layer ctx_final (57)
I0731 18:11:06.536051 16855 net.cpp:561] ctx_final <- ctx_conv4
I0731 18:11:06.536054 16855 net.cpp:530] ctx_final -> ctx_final
I0731 18:11:06.549239 16855 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 7.22G, req 0G)
I0731 18:11:06.549259 16855 net.cpp:245] Setting up ctx_final
I0731 18:11:06.549268 16855 net.cpp:252] TRAIN Top shape for layer 57 'ctx_final' 6 8 80 80 (307200)
I0731 18:11:06.549278 16855 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I0731 18:11:06.549283 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.549290 16855 net.cpp:184] Created Layer ctx_final/relu (58)
I0731 18:11:06.549295 16855 net.cpp:561] ctx_final/relu <- ctx_final
I0731 18:11:06.549301 16855 net.cpp:513] ctx_final/relu -> ctx_final (in-place)
I0731 18:11:06.549310 16855 net.cpp:245] Setting up ctx_final/relu
I0731 18:11:06.549315 16855 net.cpp:252] TRAIN Top shape for layer 58 'ctx_final/relu' 6 8 80 80 (307200)
I0731 18:11:06.549319 16855 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I0731 18:11:06.549324 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.549345 16855 net.cpp:184] Created Layer out_deconv_final_up2 (59)
I0731 18:11:06.549348 16855 net.cpp:561] out_deconv_final_up2 <- ctx_final
I0731 18:11:06.549353 16855 net.cpp:530] out_deconv_final_up2 -> out_deconv_final_up2
I0731 18:11:06.549775 16855 net.cpp:245] Setting up out_deconv_final_up2
I0731 18:11:06.549783 16855 net.cpp:252] TRAIN Top shape for layer 59 'out_deconv_final_up2' 6 8 160 160 (1228800)
I0731 18:11:06.549789 16855 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I0731 18:11:06.549794 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.549801 16855 net.cpp:184] Created Layer out_deconv_final_up4 (60)
I0731 18:11:06.549805 16855 net.cpp:561] out_deconv_final_up4 <- out_deconv_final_up2
I0731 18:11:06.549809 16855 net.cpp:530] out_deconv_final_up4 -> out_deconv_final_up4
I0731 18:11:06.550091 16855 net.cpp:245] Setting up out_deconv_final_up4
I0731 18:11:06.550097 16855 net.cpp:252] TRAIN Top shape for layer 60 'out_deconv_final_up4' 6 8 320 320 (4915200)
I0731 18:11:06.550103 16855 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I0731 18:11:06.550107 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.550117 16855 net.cpp:184] Created Layer out_deconv_final_up8 (61)
I0731 18:11:06.550119 16855 net.cpp:561] out_deconv_final_up8 <- out_deconv_final_up4
I0731 18:11:06.550124 16855 net.cpp:530] out_deconv_final_up8 -> out_deconv_final_up8
I0731 18:11:06.550398 16855 net.cpp:245] Setting up out_deconv_final_up8
I0731 18:11:06.550405 16855 net.cpp:252] TRAIN Top shape for layer 61 'out_deconv_final_up8' 6 8 640 640 (19660800)
I0731 18:11:06.550411 16855 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0731 18:11:06.550415 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.550431 16855 net.cpp:184] Created Layer loss (62)
I0731 18:11:06.550434 16855 net.cpp:561] loss <- out_deconv_final_up8
I0731 18:11:06.550439 16855 net.cpp:561] loss <- label
I0731 18:11:06.550446 16855 net.cpp:530] loss -> loss
I0731 18:11:06.551834 16855 net.cpp:245] Setting up loss
I0731 18:11:06.551844 16855 net.cpp:252] TRAIN Top shape for layer 62 'loss' (1)
I0731 18:11:06.551848 16855 net.cpp:256]     with loss weight 1
I0731 18:11:06.551854 16855 net.cpp:323] loss needs backward computation.
I0731 18:11:06.551859 16855 net.cpp:323] out_deconv_final_up8 needs backward computation.
I0731 18:11:06.551863 16855 net.cpp:323] out_deconv_final_up4 needs backward computation.
I0731 18:11:06.551868 16855 net.cpp:323] out_deconv_final_up2 needs backward computation.
I0731 18:11:06.551872 16855 net.cpp:323] ctx_final/relu needs backward computation.
I0731 18:11:06.551877 16855 net.cpp:323] ctx_final needs backward computation.
I0731 18:11:06.551880 16855 net.cpp:323] ctx_conv4/relu needs backward computation.
I0731 18:11:06.551884 16855 net.cpp:323] ctx_conv4/bn needs backward computation.
I0731 18:11:06.551888 16855 net.cpp:323] ctx_conv4 needs backward computation.
I0731 18:11:06.551892 16855 net.cpp:323] ctx_conv3/relu needs backward computation.
I0731 18:11:06.551897 16855 net.cpp:323] ctx_conv3/bn needs backward computation.
I0731 18:11:06.551900 16855 net.cpp:323] ctx_conv3 needs backward computation.
I0731 18:11:06.551904 16855 net.cpp:323] ctx_conv2/relu needs backward computation.
I0731 18:11:06.551908 16855 net.cpp:323] ctx_conv2/bn needs backward computation.
I0731 18:11:06.551911 16855 net.cpp:323] ctx_conv2 needs backward computation.
I0731 18:11:06.551915 16855 net.cpp:323] ctx_conv1/relu needs backward computation.
I0731 18:11:06.551919 16855 net.cpp:323] ctx_conv1/bn needs backward computation.
I0731 18:11:06.551923 16855 net.cpp:323] ctx_conv1 needs backward computation.
I0731 18:11:06.551928 16855 net.cpp:323] out3_out5_combined needs backward computation.
I0731 18:11:06.551933 16855 net.cpp:323] out3a/relu needs backward computation.
I0731 18:11:06.551942 16855 net.cpp:323] out3a/bn needs backward computation.
I0731 18:11:06.551947 16855 net.cpp:323] out3a needs backward computation.
I0731 18:11:06.551951 16855 net.cpp:323] out5a_up2 needs backward computation.
I0731 18:11:06.551955 16855 net.cpp:323] out5a/relu needs backward computation.
I0731 18:11:06.551959 16855 net.cpp:323] out5a/bn needs backward computation.
I0731 18:11:06.551964 16855 net.cpp:323] out5a needs backward computation.
I0731 18:11:06.551967 16855 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0731 18:11:06.551971 16855 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0731 18:11:06.551975 16855 net.cpp:323] res5a_branch2b needs backward computation.
I0731 18:11:06.551980 16855 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0731 18:11:06.551983 16855 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0731 18:11:06.551987 16855 net.cpp:323] res5a_branch2a needs backward computation.
I0731 18:11:06.551991 16855 net.cpp:323] pool4 needs backward computation.
I0731 18:11:06.551996 16855 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0731 18:11:06.552000 16855 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0731 18:11:06.552004 16855 net.cpp:323] res4a_branch2b needs backward computation.
I0731 18:11:06.552008 16855 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0731 18:11:06.552012 16855 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0731 18:11:06.552016 16855 net.cpp:323] res4a_branch2a needs backward computation.
I0731 18:11:06.552021 16855 net.cpp:323] pool3 needs backward computation.
I0731 18:11:06.552026 16855 net.cpp:323] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I0731 18:11:06.552029 16855 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0731 18:11:06.552033 16855 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0731 18:11:06.552037 16855 net.cpp:323] res3a_branch2b needs backward computation.
I0731 18:11:06.552042 16855 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0731 18:11:06.552047 16855 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0731 18:11:06.552050 16855 net.cpp:323] res3a_branch2a needs backward computation.
I0731 18:11:06.552054 16855 net.cpp:323] pool2 needs backward computation.
I0731 18:11:06.552058 16855 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0731 18:11:06.552062 16855 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0731 18:11:06.552067 16855 net.cpp:323] res2a_branch2b needs backward computation.
I0731 18:11:06.552070 16855 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0731 18:11:06.552074 16855 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0731 18:11:06.552079 16855 net.cpp:323] res2a_branch2a needs backward computation.
I0731 18:11:06.552083 16855 net.cpp:323] pool1 needs backward computation.
I0731 18:11:06.552088 16855 net.cpp:323] conv1b/relu needs backward computation.
I0731 18:11:06.552091 16855 net.cpp:323] conv1b/bn needs backward computation.
I0731 18:11:06.552095 16855 net.cpp:323] conv1b needs backward computation.
I0731 18:11:06.552100 16855 net.cpp:323] conv1a/relu needs backward computation.
I0731 18:11:06.552103 16855 net.cpp:323] conv1a/bn needs backward computation.
I0731 18:11:06.552108 16855 net.cpp:323] conv1a needs backward computation.
I0731 18:11:06.552112 16855 net.cpp:325] data/bias does not need backward computation.
I0731 18:11:06.552117 16855 net.cpp:325] data does not need backward computation.
I0731 18:11:06.552121 16855 net.cpp:367] This network produces output loss
I0731 18:11:06.552170 16855 net.cpp:389] Top memory (TRAIN) required for data: 956006400 diff: 946176008
I0731 18:11:06.552175 16855 net.cpp:392] Bottom memory (TRAIN) required for data: 956006400 diff: 956006400
I0731 18:11:06.552178 16855 net.cpp:395] Shared (in-place) memory (TRAIN) by data: 630374400 diff: 630374400
I0731 18:11:06.552181 16855 net.cpp:398] Parameters memory (TRAIN) required for data: 2692608 diff: 2692608
I0731 18:11:06.552189 16855 net.cpp:401] Parameters shared memory (TRAIN) by data: 0 diff: 0
I0731 18:11:06.552193 16855 net.cpp:407] Network initialization done.
I0731 18:11:06.552717 16855 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/initial/test.prototxt
W0731 18:11:06.552783 16855 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0731 18:11:06.552968 16855 net.cpp:72] Initializing net from parameters: 
name: "jsegnet21v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/val-image-lmdb"
    label_list_path: "data/val-label-lmdb"
    batch_size: 2
    threads: 1
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
  accuracy_param {
    ignore_label: 255
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
    ignore_label: 255
  }
}
I0731 18:11:06.553118 16855 net.cpp:104] Using FLOAT as default forward math type
I0731 18:11:06.553123 16855 net.cpp:110] Using FLOAT as default backward math type
I0731 18:11:06.553127 16855 layer_factory.hpp:136] Creating layer 'data' of type 'ImageLabelData'
I0731 18:11:06.553130 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.553140 16855 net.cpp:184] Created Layer data (0)
I0731 18:11:06.553145 16855 net.cpp:530] data -> data
I0731 18:11:06.553150 16855 net.cpp:530] data -> label
I0731 18:11:06.553169 16855 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0731 18:11:06.553176 16855 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 18:11:06.554091 16891 db_lmdb.cpp:35] Opened lmdb data/val-image-lmdb
I0731 18:11:06.555550 16855 data_layer.cpp:184] (0) ReshapePrefetch 2, 3, 640, 640
I0731 18:11:06.555608 16855 data_layer.cpp:208] (0) Output data size: 2, 3, 640, 640
I0731 18:11:06.555614 16855 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 18:11:06.555655 16855 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0731 18:11:06.555662 16855 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 18:11:06.556402 16892 data_layer.cpp:97] (0) Parser threads: 1
I0731 18:11:06.556417 16892 data_layer.cpp:99] (0) Transformer threads: 1
I0731 18:11:06.559315 16893 db_lmdb.cpp:35] Opened lmdb data/val-label-lmdb
I0731 18:11:06.560286 16855 data_layer.cpp:184] (0) ReshapePrefetch 2, 1, 640, 640
I0731 18:11:06.560333 16855 data_layer.cpp:208] (0) Output data size: 2, 1, 640, 640
I0731 18:11:06.560340 16855 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 18:11:06.560379 16855 net.cpp:245] Setting up data
I0731 18:11:06.560387 16855 net.cpp:252] TEST Top shape for layer 0 'data' 2 3 640 640 (2457600)
I0731 18:11:06.560394 16855 net.cpp:252] TEST Top shape for layer 0 'data' 2 1 640 640 (819200)
I0731 18:11:06.560400 16855 layer_factory.hpp:136] Creating layer 'label_data_1_split' of type 'Split'
I0731 18:11:06.560406 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.560416 16855 net.cpp:184] Created Layer label_data_1_split (1)
I0731 18:11:06.560420 16855 net.cpp:561] label_data_1_split <- label
I0731 18:11:06.560425 16855 net.cpp:530] label_data_1_split -> label_data_1_split_0
I0731 18:11:06.560432 16855 net.cpp:530] label_data_1_split -> label_data_1_split_1
I0731 18:11:06.560436 16855 net.cpp:530] label_data_1_split -> label_data_1_split_2
I0731 18:11:06.560509 16855 net.cpp:245] Setting up label_data_1_split
I0731 18:11:06.560515 16855 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0731 18:11:06.560521 16855 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0731 18:11:06.560526 16855 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0731 18:11:06.560530 16855 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0731 18:11:06.560535 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.560544 16855 net.cpp:184] Created Layer data/bias (2)
I0731 18:11:06.560547 16855 net.cpp:561] data/bias <- data
I0731 18:11:06.560551 16855 net.cpp:530] data/bias -> data/bias
I0731 18:11:06.561625 16894 data_layer.cpp:97] (0) Parser threads: 1
I0731 18:11:06.561640 16894 data_layer.cpp:99] (0) Transformer threads: 1
I0731 18:11:06.563438 16855 net.cpp:245] Setting up data/bias
I0731 18:11:06.563455 16855 net.cpp:252] TEST Top shape for layer 2 'data/bias' 2 3 640 640 (2457600)
I0731 18:11:06.563469 16855 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0731 18:11:06.563477 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.563499 16855 net.cpp:184] Created Layer conv1a (3)
I0731 18:11:06.563504 16855 net.cpp:561] conv1a <- data/bias
I0731 18:11:06.563511 16855 net.cpp:530] conv1a -> conv1a
I0731 18:11:06.569077 16855 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.08G, req 0G)
I0731 18:11:06.569093 16855 net.cpp:245] Setting up conv1a
I0731 18:11:06.569100 16855 net.cpp:252] TEST Top shape for layer 3 'conv1a' 2 32 320 320 (6553600)
I0731 18:11:06.569113 16855 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0731 18:11:06.569125 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.569138 16855 net.cpp:184] Created Layer conv1a/bn (4)
I0731 18:11:06.569141 16855 net.cpp:561] conv1a/bn <- conv1a
I0731 18:11:06.569146 16855 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0731 18:11:06.569897 16855 net.cpp:245] Setting up conv1a/bn
I0731 18:11:06.569906 16855 net.cpp:252] TEST Top shape for layer 4 'conv1a/bn' 2 32 320 320 (6553600)
I0731 18:11:06.569916 16855 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0731 18:11:06.569921 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.569926 16855 net.cpp:184] Created Layer conv1a/relu (5)
I0731 18:11:06.569931 16855 net.cpp:561] conv1a/relu <- conv1a
I0731 18:11:06.569934 16855 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0731 18:11:06.569941 16855 net.cpp:245] Setting up conv1a/relu
I0731 18:11:06.569946 16855 net.cpp:252] TEST Top shape for layer 5 'conv1a/relu' 2 32 320 320 (6553600)
I0731 18:11:06.569950 16855 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0731 18:11:06.569954 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.569964 16855 net.cpp:184] Created Layer conv1b (6)
I0731 18:11:06.569967 16855 net.cpp:561] conv1b <- conv1a
I0731 18:11:06.569972 16855 net.cpp:530] conv1b -> conv1b
I0731 18:11:06.583525 16855 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 7.06G, req 0G)
I0731 18:11:06.583547 16855 net.cpp:245] Setting up conv1b
I0731 18:11:06.583555 16855 net.cpp:252] TEST Top shape for layer 6 'conv1b' 2 32 320 320 (6553600)
I0731 18:11:06.583569 16855 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0731 18:11:06.583573 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.583586 16855 net.cpp:184] Created Layer conv1b/bn (7)
I0731 18:11:06.583591 16855 net.cpp:561] conv1b/bn <- conv1b
I0731 18:11:06.583596 16855 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0731 18:11:06.584501 16855 net.cpp:245] Setting up conv1b/bn
I0731 18:11:06.584511 16855 net.cpp:252] TEST Top shape for layer 7 'conv1b/bn' 2 32 320 320 (6553600)
I0731 18:11:06.584520 16855 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0731 18:11:06.584524 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.584535 16855 net.cpp:184] Created Layer conv1b/relu (8)
I0731 18:11:06.584539 16855 net.cpp:561] conv1b/relu <- conv1b
I0731 18:11:06.584543 16855 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0731 18:11:06.584550 16855 net.cpp:245] Setting up conv1b/relu
I0731 18:11:06.584555 16855 net.cpp:252] TEST Top shape for layer 8 'conv1b/relu' 2 32 320 320 (6553600)
I0731 18:11:06.584560 16855 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0731 18:11:06.584564 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.584571 16855 net.cpp:184] Created Layer pool1 (9)
I0731 18:11:06.584574 16855 net.cpp:561] pool1 <- conv1b
I0731 18:11:06.584579 16855 net.cpp:530] pool1 -> pool1
I0731 18:11:06.584655 16855 net.cpp:245] Setting up pool1
I0731 18:11:06.584661 16855 net.cpp:252] TEST Top shape for layer 9 'pool1' 2 32 160 160 (1638400)
I0731 18:11:06.584666 16855 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0731 18:11:06.584671 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.584681 16855 net.cpp:184] Created Layer res2a_branch2a (10)
I0731 18:11:06.584686 16855 net.cpp:561] res2a_branch2a <- pool1
I0731 18:11:06.584689 16855 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0731 18:11:06.594310 16855 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 1  (limit 7.03G, req 0G)
I0731 18:11:06.594408 16855 net.cpp:245] Setting up res2a_branch2a
I0731 18:11:06.594441 16855 net.cpp:252] TEST Top shape for layer 10 'res2a_branch2a' 2 64 160 160 (3276800)
I0731 18:11:06.594487 16855 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0731 18:11:06.594512 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.594571 16855 net.cpp:184] Created Layer res2a_branch2a/bn (11)
I0731 18:11:06.594594 16855 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0731 18:11:06.594619 16855 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0731 18:11:06.598037 16855 net.cpp:245] Setting up res2a_branch2a/bn
I0731 18:11:06.598085 16855 net.cpp:252] TEST Top shape for layer 11 'res2a_branch2a/bn' 2 64 160 160 (3276800)
I0731 18:11:06.598131 16855 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0731 18:11:06.598152 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.598178 16855 net.cpp:184] Created Layer res2a_branch2a/relu (12)
I0731 18:11:06.598201 16855 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0731 18:11:06.598225 16855 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0731 18:11:06.598258 16855 net.cpp:245] Setting up res2a_branch2a/relu
I0731 18:11:06.598284 16855 net.cpp:252] TEST Top shape for layer 12 'res2a_branch2a/relu' 2 64 160 160 (3276800)
I0731 18:11:06.598305 16855 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0731 18:11:06.598330 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.598372 16855 net.cpp:184] Created Layer res2a_branch2b (13)
I0731 18:11:06.598392 16855 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0731 18:11:06.598412 16855 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0731 18:11:06.611882 16855 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 1  (limit 7.02G, req 0G)
I0731 18:11:06.611953 16855 net.cpp:245] Setting up res2a_branch2b
I0731 18:11:06.611992 16855 net.cpp:252] TEST Top shape for layer 13 'res2a_branch2b' 2 64 160 160 (3276800)
I0731 18:11:06.612032 16855 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0731 18:11:06.612061 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.612097 16855 net.cpp:184] Created Layer res2a_branch2b/bn (14)
I0731 18:11:06.612118 16855 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0731 18:11:06.612141 16855 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0731 18:11:06.615384 16855 net.cpp:245] Setting up res2a_branch2b/bn
I0731 18:11:06.615417 16855 net.cpp:252] TEST Top shape for layer 14 'res2a_branch2b/bn' 2 64 160 160 (3276800)
I0731 18:11:06.615447 16855 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0731 18:11:06.615463 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.615479 16855 net.cpp:184] Created Layer res2a_branch2b/relu (15)
I0731 18:11:06.615495 16855 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0731 18:11:06.615509 16855 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0731 18:11:06.615530 16855 net.cpp:245] Setting up res2a_branch2b/relu
I0731 18:11:06.615547 16855 net.cpp:252] TEST Top shape for layer 15 'res2a_branch2b/relu' 2 64 160 160 (3276800)
I0731 18:11:06.615562 16855 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0731 18:11:06.615576 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.615602 16855 net.cpp:184] Created Layer pool2 (16)
I0731 18:11:06.615614 16855 net.cpp:561] pool2 <- res2a_branch2b
I0731 18:11:06.615628 16855 net.cpp:530] pool2 -> pool2
I0731 18:11:06.615861 16855 net.cpp:245] Setting up pool2
I0731 18:11:06.615881 16855 net.cpp:252] TEST Top shape for layer 16 'pool2' 2 64 80 80 (819200)
I0731 18:11:06.615919 16855 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0731 18:11:06.615936 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.615985 16855 net.cpp:184] Created Layer res3a_branch2a (17)
I0731 18:11:06.615999 16855 net.cpp:561] res3a_branch2a <- pool2
I0731 18:11:06.616014 16855 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0731 18:11:06.627661 16855 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.01G, req 0G)
I0731 18:11:06.627687 16855 net.cpp:245] Setting up res3a_branch2a
I0731 18:11:06.627701 16855 net.cpp:252] TEST Top shape for layer 17 'res3a_branch2a' 2 128 80 80 (1638400)
I0731 18:11:06.627717 16855 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0731 18:11:06.627727 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.627741 16855 net.cpp:184] Created Layer res3a_branch2a/bn (18)
I0731 18:11:06.627749 16855 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0731 18:11:06.627758 16855 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0731 18:11:06.629189 16855 net.cpp:245] Setting up res3a_branch2a/bn
I0731 18:11:06.629209 16855 net.cpp:252] TEST Top shape for layer 18 'res3a_branch2a/bn' 2 128 80 80 (1638400)
I0731 18:11:06.629230 16855 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0731 18:11:06.629240 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.629251 16855 net.cpp:184] Created Layer res3a_branch2a/relu (19)
I0731 18:11:06.629259 16855 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0731 18:11:06.629267 16855 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0731 18:11:06.629281 16855 net.cpp:245] Setting up res3a_branch2a/relu
I0731 18:11:06.629292 16855 net.cpp:252] TEST Top shape for layer 19 'res3a_branch2a/relu' 2 128 80 80 (1638400)
I0731 18:11:06.629299 16855 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0731 18:11:06.629308 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.629328 16855 net.cpp:184] Created Layer res3a_branch2b (20)
I0731 18:11:06.629335 16855 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0731 18:11:06.629343 16855 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0731 18:11:06.636102 16855 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 1  (limit 6.99G, req 0G)
I0731 18:11:06.636128 16855 net.cpp:245] Setting up res3a_branch2b
I0731 18:11:06.636138 16855 net.cpp:252] TEST Top shape for layer 20 'res3a_branch2b' 2 128 80 80 (1638400)
I0731 18:11:06.636152 16855 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0731 18:11:06.636159 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.636173 16855 net.cpp:184] Created Layer res3a_branch2b/bn (21)
I0731 18:11:06.636179 16855 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0731 18:11:06.636188 16855 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0731 18:11:06.637305 16855 net.cpp:245] Setting up res3a_branch2b/bn
I0731 18:11:06.637320 16855 net.cpp:252] TEST Top shape for layer 21 'res3a_branch2b/bn' 2 128 80 80 (1638400)
I0731 18:11:06.637332 16855 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0731 18:11:06.637339 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.637348 16855 net.cpp:184] Created Layer res3a_branch2b/relu (22)
I0731 18:11:06.637354 16855 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0731 18:11:06.637362 16855 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0731 18:11:06.637372 16855 net.cpp:245] Setting up res3a_branch2b/relu
I0731 18:11:06.637378 16855 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b/relu' 2 128 80 80 (1638400)
I0731 18:11:06.637397 16855 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0731 18:11:06.637403 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.637411 16855 net.cpp:184] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (23)
I0731 18:11:06.637418 16855 net.cpp:561] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0731 18:11:06.637424 16855 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0731 18:11:06.637432 16855 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0731 18:11:06.637504 16855 net.cpp:245] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0731 18:11:06.637512 16855 net.cpp:252] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 2 128 80 80 (1638400)
I0731 18:11:06.637521 16855 net.cpp:252] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 2 128 80 80 (1638400)
I0731 18:11:06.637526 16855 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0731 18:11:06.637533 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.637554 16855 net.cpp:184] Created Layer pool3 (24)
I0731 18:11:06.637560 16855 net.cpp:561] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0731 18:11:06.637567 16855 net.cpp:530] pool3 -> pool3
I0731 18:11:06.637673 16855 net.cpp:245] Setting up pool3
I0731 18:11:06.637682 16855 net.cpp:252] TEST Top shape for layer 24 'pool3' 2 128 40 40 (409600)
I0731 18:11:06.637689 16855 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0731 18:11:06.637696 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.637711 16855 net.cpp:184] Created Layer res4a_branch2a (25)
I0731 18:11:06.637717 16855 net.cpp:561] res4a_branch2a <- pool3
I0731 18:11:06.637722 16855 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0731 18:11:06.651975 16855 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.99G, req 0G)
I0731 18:11:06.651989 16855 net.cpp:245] Setting up res4a_branch2a
I0731 18:11:06.651995 16855 net.cpp:252] TEST Top shape for layer 25 'res4a_branch2a' 2 256 40 40 (819200)
I0731 18:11:06.652005 16855 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0731 18:11:06.652010 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.652025 16855 net.cpp:184] Created Layer res4a_branch2a/bn (26)
I0731 18:11:06.652030 16855 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0731 18:11:06.652035 16855 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0731 18:11:06.652765 16855 net.cpp:245] Setting up res4a_branch2a/bn
I0731 18:11:06.652775 16855 net.cpp:252] TEST Top shape for layer 26 'res4a_branch2a/bn' 2 256 40 40 (819200)
I0731 18:11:06.652783 16855 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0731 18:11:06.652787 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.652793 16855 net.cpp:184] Created Layer res4a_branch2a/relu (27)
I0731 18:11:06.652797 16855 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0731 18:11:06.652801 16855 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0731 18:11:06.652822 16855 net.cpp:245] Setting up res4a_branch2a/relu
I0731 18:11:06.652828 16855 net.cpp:252] TEST Top shape for layer 27 'res4a_branch2a/relu' 2 256 40 40 (819200)
I0731 18:11:06.652833 16855 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0731 18:11:06.652838 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.652851 16855 net.cpp:184] Created Layer res4a_branch2b (28)
I0731 18:11:06.652855 16855 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0731 18:11:06.652869 16855 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0731 18:11:06.660167 16855 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.98G, req 0G)
I0731 18:11:06.660178 16855 net.cpp:245] Setting up res4a_branch2b
I0731 18:11:06.660184 16855 net.cpp:252] TEST Top shape for layer 28 'res4a_branch2b' 2 256 40 40 (819200)
I0731 18:11:06.660192 16855 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0731 18:11:06.660197 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.660204 16855 net.cpp:184] Created Layer res4a_branch2b/bn (29)
I0731 18:11:06.660209 16855 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0731 18:11:06.660213 16855 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0731 18:11:06.660923 16855 net.cpp:245] Setting up res4a_branch2b/bn
I0731 18:11:06.660930 16855 net.cpp:252] TEST Top shape for layer 29 'res4a_branch2b/bn' 2 256 40 40 (819200)
I0731 18:11:06.660939 16855 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0731 18:11:06.660943 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.660948 16855 net.cpp:184] Created Layer res4a_branch2b/relu (30)
I0731 18:11:06.660953 16855 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0731 18:11:06.660956 16855 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0731 18:11:06.660962 16855 net.cpp:245] Setting up res4a_branch2b/relu
I0731 18:11:06.660967 16855 net.cpp:252] TEST Top shape for layer 30 'res4a_branch2b/relu' 2 256 40 40 (819200)
I0731 18:11:06.660971 16855 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0731 18:11:06.660976 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.660982 16855 net.cpp:184] Created Layer pool4 (31)
I0731 18:11:06.660986 16855 net.cpp:561] pool4 <- res4a_branch2b
I0731 18:11:06.660991 16855 net.cpp:530] pool4 -> pool4
I0731 18:11:06.661069 16855 net.cpp:245] Setting up pool4
I0731 18:11:06.661075 16855 net.cpp:252] TEST Top shape for layer 31 'pool4' 2 256 40 40 (819200)
I0731 18:11:06.661080 16855 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0731 18:11:06.661084 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.661099 16855 net.cpp:184] Created Layer res5a_branch2a (32)
I0731 18:11:06.661103 16855 net.cpp:561] res5a_branch2a <- pool4
I0731 18:11:06.661108 16855 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0731 18:11:06.686245 16855 net.cpp:245] Setting up res5a_branch2a
I0731 18:11:06.686269 16855 net.cpp:252] TEST Top shape for layer 32 'res5a_branch2a' 2 512 40 40 (1638400)
I0731 18:11:06.686278 16855 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0731 18:11:06.686283 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.686295 16855 net.cpp:184] Created Layer res5a_branch2a/bn (33)
I0731 18:11:06.686300 16855 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0731 18:11:06.686305 16855 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0731 18:11:06.687000 16855 net.cpp:245] Setting up res5a_branch2a/bn
I0731 18:11:06.687007 16855 net.cpp:252] TEST Top shape for layer 33 'res5a_branch2a/bn' 2 512 40 40 (1638400)
I0731 18:11:06.687017 16855 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0731 18:11:06.687022 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.687028 16855 net.cpp:184] Created Layer res5a_branch2a/relu (34)
I0731 18:11:06.687032 16855 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0731 18:11:06.687036 16855 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0731 18:11:06.687043 16855 net.cpp:245] Setting up res5a_branch2a/relu
I0731 18:11:06.687048 16855 net.cpp:252] TEST Top shape for layer 34 'res5a_branch2a/relu' 2 512 40 40 (1638400)
I0731 18:11:06.687060 16855 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0731 18:11:06.687065 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.687074 16855 net.cpp:184] Created Layer res5a_branch2b (35)
I0731 18:11:06.687078 16855 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0731 18:11:06.687083 16855 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0731 18:11:06.700268 16855 net.cpp:245] Setting up res5a_branch2b
I0731 18:11:06.700292 16855 net.cpp:252] TEST Top shape for layer 35 'res5a_branch2b' 2 512 40 40 (1638400)
I0731 18:11:06.700314 16855 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0731 18:11:06.700320 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.700330 16855 net.cpp:184] Created Layer res5a_branch2b/bn (36)
I0731 18:11:06.700335 16855 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0731 18:11:06.700340 16855 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0731 18:11:06.701050 16855 net.cpp:245] Setting up res5a_branch2b/bn
I0731 18:11:06.701057 16855 net.cpp:252] TEST Top shape for layer 36 'res5a_branch2b/bn' 2 512 40 40 (1638400)
I0731 18:11:06.701066 16855 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0731 18:11:06.701071 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.701076 16855 net.cpp:184] Created Layer res5a_branch2b/relu (37)
I0731 18:11:06.701081 16855 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0731 18:11:06.701086 16855 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0731 18:11:06.701092 16855 net.cpp:245] Setting up res5a_branch2b/relu
I0731 18:11:06.701097 16855 net.cpp:252] TEST Top shape for layer 37 'res5a_branch2b/relu' 2 512 40 40 (1638400)
I0731 18:11:06.701100 16855 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I0731 18:11:06.701105 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.701118 16855 net.cpp:184] Created Layer out5a (38)
I0731 18:11:06.701122 16855 net.cpp:561] out5a <- res5a_branch2b
I0731 18:11:06.701128 16855 net.cpp:530] out5a -> out5a
I0731 18:11:06.704458 16855 net.cpp:245] Setting up out5a
I0731 18:11:06.704468 16855 net.cpp:252] TEST Top shape for layer 38 'out5a' 2 64 40 40 (204800)
I0731 18:11:06.704474 16855 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I0731 18:11:06.704479 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.704486 16855 net.cpp:184] Created Layer out5a/bn (39)
I0731 18:11:06.704490 16855 net.cpp:561] out5a/bn <- out5a
I0731 18:11:06.704494 16855 net.cpp:513] out5a/bn -> out5a (in-place)
I0731 18:11:06.705200 16855 net.cpp:245] Setting up out5a/bn
I0731 18:11:06.705209 16855 net.cpp:252] TEST Top shape for layer 39 'out5a/bn' 2 64 40 40 (204800)
I0731 18:11:06.705217 16855 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I0731 18:11:06.705221 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.705226 16855 net.cpp:184] Created Layer out5a/relu (40)
I0731 18:11:06.705231 16855 net.cpp:561] out5a/relu <- out5a
I0731 18:11:06.705235 16855 net.cpp:513] out5a/relu -> out5a (in-place)
I0731 18:11:06.705242 16855 net.cpp:245] Setting up out5a/relu
I0731 18:11:06.705246 16855 net.cpp:252] TEST Top shape for layer 40 'out5a/relu' 2 64 40 40 (204800)
I0731 18:11:06.705251 16855 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I0731 18:11:06.705255 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.705263 16855 net.cpp:184] Created Layer out5a_up2 (41)
I0731 18:11:06.705266 16855 net.cpp:561] out5a_up2 <- out5a
I0731 18:11:06.705271 16855 net.cpp:530] out5a_up2 -> out5a_up2
I0731 18:11:06.705657 16855 net.cpp:245] Setting up out5a_up2
I0731 18:11:06.705665 16855 net.cpp:252] TEST Top shape for layer 41 'out5a_up2' 2 64 80 80 (819200)
I0731 18:11:06.705672 16855 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I0731 18:11:06.705675 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.705683 16855 net.cpp:184] Created Layer out3a (42)
I0731 18:11:06.705688 16855 net.cpp:561] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0731 18:11:06.705693 16855 net.cpp:530] out3a -> out3a
I0731 18:11:06.710281 16855 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.97G, req 0G)
I0731 18:11:06.710294 16855 net.cpp:245] Setting up out3a
I0731 18:11:06.710300 16855 net.cpp:252] TEST Top shape for layer 42 'out3a' 2 64 80 80 (819200)
I0731 18:11:06.710309 16855 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I0731 18:11:06.710314 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.710320 16855 net.cpp:184] Created Layer out3a/bn (43)
I0731 18:11:06.710325 16855 net.cpp:561] out3a/bn <- out3a
I0731 18:11:06.710330 16855 net.cpp:513] out3a/bn -> out3a (in-place)
I0731 18:11:06.711061 16855 net.cpp:245] Setting up out3a/bn
I0731 18:11:06.711069 16855 net.cpp:252] TEST Top shape for layer 43 'out3a/bn' 2 64 80 80 (819200)
I0731 18:11:06.711078 16855 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I0731 18:11:06.711082 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.711087 16855 net.cpp:184] Created Layer out3a/relu (44)
I0731 18:11:06.711091 16855 net.cpp:561] out3a/relu <- out3a
I0731 18:11:06.711096 16855 net.cpp:513] out3a/relu -> out3a (in-place)
I0731 18:11:06.711102 16855 net.cpp:245] Setting up out3a/relu
I0731 18:11:06.711105 16855 net.cpp:252] TEST Top shape for layer 44 'out3a/relu' 2 64 80 80 (819200)
I0731 18:11:06.711110 16855 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I0731 18:11:06.711113 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.711119 16855 net.cpp:184] Created Layer out3_out5_combined (45)
I0731 18:11:06.711123 16855 net.cpp:561] out3_out5_combined <- out5a_up2
I0731 18:11:06.711127 16855 net.cpp:561] out3_out5_combined <- out3a
I0731 18:11:06.711133 16855 net.cpp:530] out3_out5_combined -> out3_out5_combined
I0731 18:11:06.712039 16855 net.cpp:245] Setting up out3_out5_combined
I0731 18:11:06.712049 16855 net.cpp:252] TEST Top shape for layer 45 'out3_out5_combined' 2 64 80 80 (819200)
I0731 18:11:06.712054 16855 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I0731 18:11:06.712059 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.712066 16855 net.cpp:184] Created Layer ctx_conv1 (46)
I0731 18:11:06.712070 16855 net.cpp:561] ctx_conv1 <- out3_out5_combined
I0731 18:11:06.712076 16855 net.cpp:530] ctx_conv1 -> ctx_conv1
I0731 18:11:06.716709 16855 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.96G, req 0G)
I0731 18:11:06.716722 16855 net.cpp:245] Setting up ctx_conv1
I0731 18:11:06.716727 16855 net.cpp:252] TEST Top shape for layer 46 'ctx_conv1' 2 64 80 80 (819200)
I0731 18:11:06.716734 16855 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I0731 18:11:06.716739 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.716751 16855 net.cpp:184] Created Layer ctx_conv1/bn (47)
I0731 18:11:06.716756 16855 net.cpp:561] ctx_conv1/bn <- ctx_conv1
I0731 18:11:06.716760 16855 net.cpp:513] ctx_conv1/bn -> ctx_conv1 (in-place)
I0731 18:11:06.717494 16855 net.cpp:245] Setting up ctx_conv1/bn
I0731 18:11:06.717502 16855 net.cpp:252] TEST Top shape for layer 47 'ctx_conv1/bn' 2 64 80 80 (819200)
I0731 18:11:06.717511 16855 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I0731 18:11:06.717523 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.717530 16855 net.cpp:184] Created Layer ctx_conv1/relu (48)
I0731 18:11:06.717535 16855 net.cpp:561] ctx_conv1/relu <- ctx_conv1
I0731 18:11:06.717540 16855 net.cpp:513] ctx_conv1/relu -> ctx_conv1 (in-place)
I0731 18:11:06.717545 16855 net.cpp:245] Setting up ctx_conv1/relu
I0731 18:11:06.717550 16855 net.cpp:252] TEST Top shape for layer 48 'ctx_conv1/relu' 2 64 80 80 (819200)
I0731 18:11:06.717555 16855 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I0731 18:11:06.717559 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.717567 16855 net.cpp:184] Created Layer ctx_conv2 (49)
I0731 18:11:06.717571 16855 net.cpp:561] ctx_conv2 <- ctx_conv1
I0731 18:11:06.717576 16855 net.cpp:530] ctx_conv2 -> ctx_conv2
I0731 18:11:06.718694 16855 net.cpp:245] Setting up ctx_conv2
I0731 18:11:06.718703 16855 net.cpp:252] TEST Top shape for layer 49 'ctx_conv2' 2 64 80 80 (819200)
I0731 18:11:06.718710 16855 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I0731 18:11:06.718714 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.718720 16855 net.cpp:184] Created Layer ctx_conv2/bn (50)
I0731 18:11:06.718724 16855 net.cpp:561] ctx_conv2/bn <- ctx_conv2
I0731 18:11:06.718729 16855 net.cpp:513] ctx_conv2/bn -> ctx_conv2 (in-place)
I0731 18:11:06.719440 16855 net.cpp:245] Setting up ctx_conv2/bn
I0731 18:11:06.719449 16855 net.cpp:252] TEST Top shape for layer 50 'ctx_conv2/bn' 2 64 80 80 (819200)
I0731 18:11:06.719458 16855 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I0731 18:11:06.719462 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.719467 16855 net.cpp:184] Created Layer ctx_conv2/relu (51)
I0731 18:11:06.719471 16855 net.cpp:561] ctx_conv2/relu <- ctx_conv2
I0731 18:11:06.719475 16855 net.cpp:513] ctx_conv2/relu -> ctx_conv2 (in-place)
I0731 18:11:06.719481 16855 net.cpp:245] Setting up ctx_conv2/relu
I0731 18:11:06.719486 16855 net.cpp:252] TEST Top shape for layer 51 'ctx_conv2/relu' 2 64 80 80 (819200)
I0731 18:11:06.719491 16855 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I0731 18:11:06.719494 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.719502 16855 net.cpp:184] Created Layer ctx_conv3 (52)
I0731 18:11:06.719506 16855 net.cpp:561] ctx_conv3 <- ctx_conv2
I0731 18:11:06.719511 16855 net.cpp:530] ctx_conv3 -> ctx_conv3
I0731 18:11:06.720630 16855 net.cpp:245] Setting up ctx_conv3
I0731 18:11:06.720638 16855 net.cpp:252] TEST Top shape for layer 52 'ctx_conv3' 2 64 80 80 (819200)
I0731 18:11:06.720644 16855 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I0731 18:11:06.720649 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.720654 16855 net.cpp:184] Created Layer ctx_conv3/bn (53)
I0731 18:11:06.720659 16855 net.cpp:561] ctx_conv3/bn <- ctx_conv3
I0731 18:11:06.720664 16855 net.cpp:513] ctx_conv3/bn -> ctx_conv3 (in-place)
I0731 18:11:06.721369 16855 net.cpp:245] Setting up ctx_conv3/bn
I0731 18:11:06.721376 16855 net.cpp:252] TEST Top shape for layer 53 'ctx_conv3/bn' 2 64 80 80 (819200)
I0731 18:11:06.721385 16855 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I0731 18:11:06.721388 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.721395 16855 net.cpp:184] Created Layer ctx_conv3/relu (54)
I0731 18:11:06.721398 16855 net.cpp:561] ctx_conv3/relu <- ctx_conv3
I0731 18:11:06.721402 16855 net.cpp:513] ctx_conv3/relu -> ctx_conv3 (in-place)
I0731 18:11:06.721408 16855 net.cpp:245] Setting up ctx_conv3/relu
I0731 18:11:06.721415 16855 net.cpp:252] TEST Top shape for layer 54 'ctx_conv3/relu' 2 64 80 80 (819200)
I0731 18:11:06.721423 16855 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I0731 18:11:06.721428 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.721441 16855 net.cpp:184] Created Layer ctx_conv4 (55)
I0731 18:11:06.721444 16855 net.cpp:561] ctx_conv4 <- ctx_conv3
I0731 18:11:06.721448 16855 net.cpp:530] ctx_conv4 -> ctx_conv4
I0731 18:11:06.722558 16855 net.cpp:245] Setting up ctx_conv4
I0731 18:11:06.722565 16855 net.cpp:252] TEST Top shape for layer 55 'ctx_conv4' 2 64 80 80 (819200)
I0731 18:11:06.722573 16855 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I0731 18:11:06.722578 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.722584 16855 net.cpp:184] Created Layer ctx_conv4/bn (56)
I0731 18:11:06.722589 16855 net.cpp:561] ctx_conv4/bn <- ctx_conv4
I0731 18:11:06.722592 16855 net.cpp:513] ctx_conv4/bn -> ctx_conv4 (in-place)
I0731 18:11:06.723363 16855 net.cpp:245] Setting up ctx_conv4/bn
I0731 18:11:06.723372 16855 net.cpp:252] TEST Top shape for layer 56 'ctx_conv4/bn' 2 64 80 80 (819200)
I0731 18:11:06.723381 16855 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I0731 18:11:06.723384 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.723389 16855 net.cpp:184] Created Layer ctx_conv4/relu (57)
I0731 18:11:06.723393 16855 net.cpp:561] ctx_conv4/relu <- ctx_conv4
I0731 18:11:06.723398 16855 net.cpp:513] ctx_conv4/relu -> ctx_conv4 (in-place)
I0731 18:11:06.723404 16855 net.cpp:245] Setting up ctx_conv4/relu
I0731 18:11:06.723409 16855 net.cpp:252] TEST Top shape for layer 57 'ctx_conv4/relu' 2 64 80 80 (819200)
I0731 18:11:06.723413 16855 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I0731 18:11:06.723417 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.723426 16855 net.cpp:184] Created Layer ctx_final (58)
I0731 18:11:06.723430 16855 net.cpp:561] ctx_final <- ctx_conv4
I0731 18:11:06.723434 16855 net.cpp:530] ctx_final -> ctx_final
I0731 18:11:06.727973 16855 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.95G, req 0G)
I0731 18:11:06.727993 16855 net.cpp:245] Setting up ctx_final
I0731 18:11:06.728001 16855 net.cpp:252] TEST Top shape for layer 58 'ctx_final' 2 8 80 80 (102400)
I0731 18:11:06.728011 16855 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I0731 18:11:06.728016 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.728025 16855 net.cpp:184] Created Layer ctx_final/relu (59)
I0731 18:11:06.728030 16855 net.cpp:561] ctx_final/relu <- ctx_final
I0731 18:11:06.728039 16855 net.cpp:513] ctx_final/relu -> ctx_final (in-place)
I0731 18:11:06.728057 16855 net.cpp:245] Setting up ctx_final/relu
I0731 18:11:06.728062 16855 net.cpp:252] TEST Top shape for layer 59 'ctx_final/relu' 2 8 80 80 (102400)
I0731 18:11:06.728066 16855 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I0731 18:11:06.728070 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.728080 16855 net.cpp:184] Created Layer out_deconv_final_up2 (60)
I0731 18:11:06.728085 16855 net.cpp:561] out_deconv_final_up2 <- ctx_final
I0731 18:11:06.728090 16855 net.cpp:530] out_deconv_final_up2 -> out_deconv_final_up2
I0731 18:11:06.728484 16855 net.cpp:245] Setting up out_deconv_final_up2
I0731 18:11:06.728492 16855 net.cpp:252] TEST Top shape for layer 60 'out_deconv_final_up2' 2 8 160 160 (409600)
I0731 18:11:06.728498 16855 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I0731 18:11:06.728502 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.728510 16855 net.cpp:184] Created Layer out_deconv_final_up4 (61)
I0731 18:11:06.728523 16855 net.cpp:561] out_deconv_final_up4 <- out_deconv_final_up2
I0731 18:11:06.728528 16855 net.cpp:530] out_deconv_final_up4 -> out_deconv_final_up4
I0731 18:11:06.728849 16855 net.cpp:245] Setting up out_deconv_final_up4
I0731 18:11:06.728857 16855 net.cpp:252] TEST Top shape for layer 61 'out_deconv_final_up4' 2 8 320 320 (1638400)
I0731 18:11:06.728863 16855 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I0731 18:11:06.728868 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.728881 16855 net.cpp:184] Created Layer out_deconv_final_up8 (62)
I0731 18:11:06.728885 16855 net.cpp:561] out_deconv_final_up8 <- out_deconv_final_up4
I0731 18:11:06.728890 16855 net.cpp:530] out_deconv_final_up8 -> out_deconv_final_up8
I0731 18:11:06.729189 16855 net.cpp:245] Setting up out_deconv_final_up8
I0731 18:11:06.729197 16855 net.cpp:252] TEST Top shape for layer 62 'out_deconv_final_up8' 2 8 640 640 (6553600)
I0731 18:11:06.729202 16855 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8_out_deconv_final_up8_0_split' of type 'Split'
I0731 18:11:06.729207 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.729212 16855 net.cpp:184] Created Layer out_deconv_final_up8_out_deconv_final_up8_0_split (63)
I0731 18:11:06.729216 16855 net.cpp:561] out_deconv_final_up8_out_deconv_final_up8_0_split <- out_deconv_final_up8
I0731 18:11:06.729220 16855 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0731 18:11:06.729226 16855 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0731 18:11:06.729231 16855 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0731 18:11:06.729298 16855 net.cpp:245] Setting up out_deconv_final_up8_out_deconv_final_up8_0_split
I0731 18:11:06.729303 16855 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 8 640 640 (6553600)
I0731 18:11:06.729310 16855 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 8 640 640 (6553600)
I0731 18:11:06.729315 16855 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 8 640 640 (6553600)
I0731 18:11:06.729318 16855 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0731 18:11:06.729322 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.729331 16855 net.cpp:184] Created Layer loss (64)
I0731 18:11:06.729336 16855 net.cpp:561] loss <- out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0731 18:11:06.729339 16855 net.cpp:561] loss <- label_data_1_split_0
I0731 18:11:06.729346 16855 net.cpp:530] loss -> loss
I0731 18:11:06.730235 16855 net.cpp:245] Setting up loss
I0731 18:11:06.730244 16855 net.cpp:252] TEST Top shape for layer 64 'loss' (1)
I0731 18:11:06.730248 16855 net.cpp:256]     with loss weight 1
I0731 18:11:06.730255 16855 layer_factory.hpp:136] Creating layer 'accuracy/top1' of type 'Accuracy'
I0731 18:11:06.730260 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.730270 16855 net.cpp:184] Created Layer accuracy/top1 (65)
I0731 18:11:06.730274 16855 net.cpp:561] accuracy/top1 <- out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0731 18:11:06.730279 16855 net.cpp:561] accuracy/top1 <- label_data_1_split_1
I0731 18:11:06.730284 16855 net.cpp:530] accuracy/top1 -> accuracy/top1
I0731 18:11:06.730293 16855 net.cpp:245] Setting up accuracy/top1
I0731 18:11:06.730296 16855 net.cpp:252] TEST Top shape for layer 65 'accuracy/top1' (1)
I0731 18:11:06.730301 16855 layer_factory.hpp:136] Creating layer 'accuracy/top5' of type 'Accuracy'
I0731 18:11:06.730305 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.730317 16855 net.cpp:184] Created Layer accuracy/top5 (66)
I0731 18:11:06.730321 16855 net.cpp:561] accuracy/top5 <- out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0731 18:11:06.730326 16855 net.cpp:561] accuracy/top5 <- label_data_1_split_2
I0731 18:11:06.730331 16855 net.cpp:530] accuracy/top5 -> accuracy/top5
I0731 18:11:06.730339 16855 net.cpp:245] Setting up accuracy/top5
I0731 18:11:06.730342 16855 net.cpp:252] TEST Top shape for layer 66 'accuracy/top5' (1)
I0731 18:11:06.730347 16855 net.cpp:325] accuracy/top5 does not need backward computation.
I0731 18:11:06.730351 16855 net.cpp:325] accuracy/top1 does not need backward computation.
I0731 18:11:06.730355 16855 net.cpp:323] loss needs backward computation.
I0731 18:11:06.730360 16855 net.cpp:323] out_deconv_final_up8_out_deconv_final_up8_0_split needs backward computation.
I0731 18:11:06.730365 16855 net.cpp:323] out_deconv_final_up8 needs backward computation.
I0731 18:11:06.730368 16855 net.cpp:323] out_deconv_final_up4 needs backward computation.
I0731 18:11:06.730372 16855 net.cpp:323] out_deconv_final_up2 needs backward computation.
I0731 18:11:06.730377 16855 net.cpp:323] ctx_final/relu needs backward computation.
I0731 18:11:06.730381 16855 net.cpp:323] ctx_final needs backward computation.
I0731 18:11:06.730386 16855 net.cpp:323] ctx_conv4/relu needs backward computation.
I0731 18:11:06.730389 16855 net.cpp:323] ctx_conv4/bn needs backward computation.
I0731 18:11:06.730393 16855 net.cpp:323] ctx_conv4 needs backward computation.
I0731 18:11:06.730397 16855 net.cpp:323] ctx_conv3/relu needs backward computation.
I0731 18:11:06.730401 16855 net.cpp:323] ctx_conv3/bn needs backward computation.
I0731 18:11:06.730406 16855 net.cpp:323] ctx_conv3 needs backward computation.
I0731 18:11:06.730409 16855 net.cpp:323] ctx_conv2/relu needs backward computation.
I0731 18:11:06.730412 16855 net.cpp:323] ctx_conv2/bn needs backward computation.
I0731 18:11:06.730417 16855 net.cpp:323] ctx_conv2 needs backward computation.
I0731 18:11:06.730420 16855 net.cpp:323] ctx_conv1/relu needs backward computation.
I0731 18:11:06.730425 16855 net.cpp:323] ctx_conv1/bn needs backward computation.
I0731 18:11:06.730429 16855 net.cpp:323] ctx_conv1 needs backward computation.
I0731 18:11:06.730433 16855 net.cpp:323] out3_out5_combined needs backward computation.
I0731 18:11:06.730438 16855 net.cpp:323] out3a/relu needs backward computation.
I0731 18:11:06.730443 16855 net.cpp:323] out3a/bn needs backward computation.
I0731 18:11:06.730446 16855 net.cpp:323] out3a needs backward computation.
I0731 18:11:06.730450 16855 net.cpp:323] out5a_up2 needs backward computation.
I0731 18:11:06.730454 16855 net.cpp:323] out5a/relu needs backward computation.
I0731 18:11:06.730458 16855 net.cpp:323] out5a/bn needs backward computation.
I0731 18:11:06.730463 16855 net.cpp:323] out5a needs backward computation.
I0731 18:11:06.730468 16855 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0731 18:11:06.730471 16855 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0731 18:11:06.730474 16855 net.cpp:323] res5a_branch2b needs backward computation.
I0731 18:11:06.730479 16855 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0731 18:11:06.730482 16855 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0731 18:11:06.730486 16855 net.cpp:323] res5a_branch2a needs backward computation.
I0731 18:11:06.730490 16855 net.cpp:323] pool4 needs backward computation.
I0731 18:11:06.730495 16855 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0731 18:11:06.730499 16855 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0731 18:11:06.730502 16855 net.cpp:323] res4a_branch2b needs backward computation.
I0731 18:11:06.730506 16855 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0731 18:11:06.730510 16855 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0731 18:11:06.730515 16855 net.cpp:323] res4a_branch2a needs backward computation.
I0731 18:11:06.730518 16855 net.cpp:323] pool3 needs backward computation.
I0731 18:11:06.730527 16855 net.cpp:323] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I0731 18:11:06.730531 16855 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0731 18:11:06.730536 16855 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0731 18:11:06.730540 16855 net.cpp:323] res3a_branch2b needs backward computation.
I0731 18:11:06.730545 16855 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0731 18:11:06.730548 16855 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0731 18:11:06.730552 16855 net.cpp:323] res3a_branch2a needs backward computation.
I0731 18:11:06.730556 16855 net.cpp:323] pool2 needs backward computation.
I0731 18:11:06.730561 16855 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0731 18:11:06.730566 16855 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0731 18:11:06.730569 16855 net.cpp:323] res2a_branch2b needs backward computation.
I0731 18:11:06.730573 16855 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0731 18:11:06.730577 16855 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0731 18:11:06.730581 16855 net.cpp:323] res2a_branch2a needs backward computation.
I0731 18:11:06.730587 16855 net.cpp:323] pool1 needs backward computation.
I0731 18:11:06.730590 16855 net.cpp:323] conv1b/relu needs backward computation.
I0731 18:11:06.730594 16855 net.cpp:323] conv1b/bn needs backward computation.
I0731 18:11:06.730598 16855 net.cpp:323] conv1b needs backward computation.
I0731 18:11:06.730602 16855 net.cpp:323] conv1a/relu needs backward computation.
I0731 18:11:06.730607 16855 net.cpp:323] conv1a/bn needs backward computation.
I0731 18:11:06.730612 16855 net.cpp:323] conv1a needs backward computation.
I0731 18:11:06.730615 16855 net.cpp:325] data/bias does not need backward computation.
I0731 18:11:06.730620 16855 net.cpp:325] label_data_1_split does not need backward computation.
I0731 18:11:06.730625 16855 net.cpp:325] data does not need backward computation.
I0731 18:11:06.730629 16855 net.cpp:367] This network produces output accuracy/top1
I0731 18:11:06.730633 16855 net.cpp:367] This network produces output accuracy/top5
I0731 18:11:06.730638 16855 net.cpp:367] This network produces output loss
I0731 18:11:06.730685 16855 net.cpp:389] Top memory (TEST) required for data: 318668800 diff: 8
I0731 18:11:06.730690 16855 net.cpp:392] Bottom memory (TEST) required for data: 318668800 diff: 318668800
I0731 18:11:06.730692 16855 net.cpp:395] Shared (in-place) memory (TEST) by data: 210124800 diff: 210124800
I0731 18:11:06.730696 16855 net.cpp:398] Parameters memory (TEST) required for data: 2692608 diff: 2692608
I0731 18:11:06.730700 16855 net.cpp:401] Parameters shared memory (TEST) by data: 0 diff: 0
I0731 18:11:06.730705 16855 net.cpp:407] Network initialization done.
I0731 18:11:06.730787 16855 solver.cpp:56] Solver scaffolding done.
I0731 18:11:06.740223 16855 caffe.cpp:137] Finetuning from training/imagenet_jacintonet11v2_iter_320000.caffemodel
I0731 18:11:06.751122 16855 net.cpp:1089] Copying source layer data Type:Data #blobs=0
I0731 18:11:06.751142 16855 net.cpp:1089] Copying source layer data/bias Type:Bias #blobs=1
I0731 18:11:06.751150 16855 net.cpp:1089] Copying source layer conv1a Type:Convolution #blobs=2
I0731 18:11:06.751191 16855 net.cpp:1089] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0731 18:11:06.751718 16855 net.cpp:1102] BN legacy DIGITS format detected ... 
I0731 18:11:06.751725 16855 net.cpp:1108] BN Transforming to new format completed.
I0731 18:11:06.751729 16855 net.cpp:1089] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0731 18:11:06.751734 16855 net.cpp:1089] Copying source layer conv1b Type:Convolution #blobs=2
I0731 18:11:06.751773 16855 net.cpp:1089] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0731 18:11:06.752144 16855 net.cpp:1102] BN legacy DIGITS format detected ... 
I0731 18:11:06.752151 16855 net.cpp:1108] BN Transforming to new format completed.
I0731 18:11:06.752164 16855 net.cpp:1089] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0731 18:11:06.752169 16855 net.cpp:1089] Copying source layer pool1 Type:Pooling #blobs=0
I0731 18:11:06.752173 16855 net.cpp:1089] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0731 18:11:06.752426 16855 net.cpp:1089] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0731 18:11:06.752805 16855 net.cpp:1102] BN legacy DIGITS format detected ... 
I0731 18:11:06.752811 16855 net.cpp:1108] BN Transforming to new format completed.
I0731 18:11:06.752820 16855 net.cpp:1089] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0731 18:11:06.752822 16855 net.cpp:1089] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0731 18:11:06.752954 16855 net.cpp:1089] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0731 18:11:06.753329 16855 net.cpp:1102] BN legacy DIGITS format detected ... 
I0731 18:11:06.753335 16855 net.cpp:1108] BN Transforming to new format completed.
I0731 18:11:06.753338 16855 net.cpp:1089] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0731 18:11:06.753342 16855 net.cpp:1089] Copying source layer pool2 Type:Pooling #blobs=0
I0731 18:11:06.753346 16855 net.cpp:1089] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0731 18:11:06.754335 16855 net.cpp:1089] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0731 18:11:06.754683 16855 net.cpp:1102] BN legacy DIGITS format detected ... 
I0731 18:11:06.754688 16855 net.cpp:1108] BN Transforming to new format completed.
I0731 18:11:06.754693 16855 net.cpp:1089] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0731 18:11:06.754695 16855 net.cpp:1089] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0731 18:11:06.755192 16855 net.cpp:1089] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0731 18:11:06.755524 16855 net.cpp:1102] BN legacy DIGITS format detected ... 
I0731 18:11:06.755530 16855 net.cpp:1108] BN Transforming to new format completed.
I0731 18:11:06.755533 16855 net.cpp:1089] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0731 18:11:06.755537 16855 net.cpp:1089] Copying source layer pool3 Type:Pooling #blobs=0
I0731 18:11:06.755542 16855 net.cpp:1089] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0731 18:11:06.759474 16855 net.cpp:1089] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0731 18:11:06.759918 16855 net.cpp:1102] BN legacy DIGITS format detected ... 
I0731 18:11:06.759925 16855 net.cpp:1108] BN Transforming to new format completed.
I0731 18:11:06.759929 16855 net.cpp:1089] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0731 18:11:06.759933 16855 net.cpp:1089] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0731 18:11:06.761888 16855 net.cpp:1089] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0731 18:11:06.762243 16855 net.cpp:1102] BN legacy DIGITS format detected ... 
I0731 18:11:06.762249 16855 net.cpp:1108] BN Transforming to new format completed.
I0731 18:11:06.762253 16855 net.cpp:1089] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0731 18:11:06.762257 16855 net.cpp:1089] Copying source layer pool4 Type:Pooling #blobs=0
I0731 18:11:06.762262 16855 net.cpp:1089] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0731 18:11:06.777854 16855 net.cpp:1089] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0731 18:11:06.778228 16855 net.cpp:1102] BN legacy DIGITS format detected ... 
I0731 18:11:06.778234 16855 net.cpp:1108] BN Transforming to new format completed.
I0731 18:11:06.778237 16855 net.cpp:1089] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0731 18:11:06.778241 16855 net.cpp:1089] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0731 18:11:06.786031 16855 net.cpp:1089] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0731 18:11:06.786388 16855 net.cpp:1102] BN legacy DIGITS format detected ... 
I0731 18:11:06.786393 16855 net.cpp:1108] BN Transforming to new format completed.
I0731 18:11:06.786408 16855 net.cpp:1089] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0731 18:11:06.786413 16855 net.cpp:1073] Ignoring source layer pool5
I0731 18:11:06.786417 16855 net.cpp:1073] Ignoring source layer fc1000
I0731 18:11:06.786422 16855 net.cpp:1089] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0731 18:11:06.796341 16855 net.cpp:1089] Copying source layer data Type:Data #blobs=0
I0731 18:11:06.796360 16855 net.cpp:1089] Copying source layer data/bias Type:Bias #blobs=1
I0731 18:11:06.796367 16855 net.cpp:1089] Copying source layer conv1a Type:Convolution #blobs=2
I0731 18:11:06.796407 16855 net.cpp:1089] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0731 18:11:06.796942 16855 net.cpp:1102] BN legacy DIGITS format detected ... 
I0731 18:11:06.796949 16855 net.cpp:1108] BN Transforming to new format completed.
I0731 18:11:06.796952 16855 net.cpp:1089] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0731 18:11:06.796957 16855 net.cpp:1089] Copying source layer conv1b Type:Convolution #blobs=2
I0731 18:11:06.796994 16855 net.cpp:1089] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0731 18:11:06.797376 16855 net.cpp:1102] BN legacy DIGITS format detected ... 
I0731 18:11:06.797382 16855 net.cpp:1108] BN Transforming to new format completed.
I0731 18:11:06.797385 16855 net.cpp:1089] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0731 18:11:06.797389 16855 net.cpp:1089] Copying source layer pool1 Type:Pooling #blobs=0
I0731 18:11:06.797394 16855 net.cpp:1089] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0731 18:11:06.797646 16855 net.cpp:1089] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0731 18:11:06.798027 16855 net.cpp:1102] BN legacy DIGITS format detected ... 
I0731 18:11:06.798032 16855 net.cpp:1108] BN Transforming to new format completed.
I0731 18:11:06.798036 16855 net.cpp:1089] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0731 18:11:06.798039 16855 net.cpp:1089] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0731 18:11:06.798171 16855 net.cpp:1089] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0731 18:11:06.798545 16855 net.cpp:1102] BN legacy DIGITS format detected ... 
I0731 18:11:06.798552 16855 net.cpp:1108] BN Transforming to new format completed.
I0731 18:11:06.798555 16855 net.cpp:1089] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0731 18:11:06.798558 16855 net.cpp:1089] Copying source layer pool2 Type:Pooling #blobs=0
I0731 18:11:06.798563 16855 net.cpp:1089] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0731 18:11:06.799551 16855 net.cpp:1089] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0731 18:11:06.799901 16855 net.cpp:1102] BN legacy DIGITS format detected ... 
I0731 18:11:06.799907 16855 net.cpp:1108] BN Transforming to new format completed.
I0731 18:11:06.799911 16855 net.cpp:1089] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0731 18:11:06.799914 16855 net.cpp:1089] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0731 18:11:06.800415 16855 net.cpp:1089] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0731 18:11:06.800766 16855 net.cpp:1102] BN legacy DIGITS format detected ... 
I0731 18:11:06.800771 16855 net.cpp:1108] BN Transforming to new format completed.
I0731 18:11:06.800776 16855 net.cpp:1089] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0731 18:11:06.800779 16855 net.cpp:1089] Copying source layer pool3 Type:Pooling #blobs=0
I0731 18:11:06.800783 16855 net.cpp:1089] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0731 18:11:06.804702 16855 net.cpp:1089] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0731 18:11:06.805066 16855 net.cpp:1102] BN legacy DIGITS format detected ... 
I0731 18:11:06.805073 16855 net.cpp:1108] BN Transforming to new format completed.
I0731 18:11:06.805075 16855 net.cpp:1089] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0731 18:11:06.805088 16855 net.cpp:1089] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0731 18:11:06.807029 16855 net.cpp:1089] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0731 18:11:06.807382 16855 net.cpp:1102] BN legacy DIGITS format detected ... 
I0731 18:11:06.807387 16855 net.cpp:1108] BN Transforming to new format completed.
I0731 18:11:06.807391 16855 net.cpp:1089] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0731 18:11:06.807395 16855 net.cpp:1089] Copying source layer pool4 Type:Pooling #blobs=0
I0731 18:11:06.807399 16855 net.cpp:1089] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0731 18:11:06.823009 16855 net.cpp:1089] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0731 18:11:06.823470 16855 net.cpp:1102] BN legacy DIGITS format detected ... 
I0731 18:11:06.823477 16855 net.cpp:1108] BN Transforming to new format completed.
I0731 18:11:06.823482 16855 net.cpp:1089] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0731 18:11:06.823485 16855 net.cpp:1089] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0731 18:11:06.831284 16855 net.cpp:1089] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0731 18:11:06.831652 16855 net.cpp:1102] BN legacy DIGITS format detected ... 
I0731 18:11:06.831657 16855 net.cpp:1108] BN Transforming to new format completed.
I0731 18:11:06.831661 16855 net.cpp:1089] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0731 18:11:06.831665 16855 net.cpp:1073] Ignoring source layer pool5
I0731 18:11:06.831670 16855 net.cpp:1073] Ignoring source layer fc1000
I0731 18:11:06.831672 16855 net.cpp:1089] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0731 18:11:06.831761 16855 parallel.cpp:108] [0 - 0] P2pSync adding callback
I0731 18:11:06.831768 16855 parallel.cpp:108] [1 - 1] P2pSync adding callback
I0731 18:11:06.831773 16855 parallel.cpp:108] [2 - 2] P2pSync adding callback
I0731 18:11:06.831775 16855 parallel.cpp:61] Starting Optimization
I0731 18:11:06.831779 16855 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 18:11:06.831809 16855 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 18:11:06.831822 16855 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 18:11:06.832262 16895 device_alternate.hpp:116] NVML initialized on thread 135869095761664
I0731 18:11:06.844985 16895 common.cpp:583] NVML succeeded to set CPU affinity on device 0
I0731 18:11:06.845038 16897 device_alternate.hpp:116] NVML initialized on thread 135869078976256
I0731 18:11:06.846217 16897 common.cpp:583] NVML succeeded to set CPU affinity on device 2
I0731 18:11:06.846230 16896 device_alternate.hpp:116] NVML initialized on thread 135869087368960
I0731 18:11:06.846776 16896 common.cpp:583] NVML succeeded to set CPU affinity on device 1
I0731 18:11:06.851002 16897 solver.cpp:42] Solver data type: FLOAT
W0731 18:11:06.851804 16897 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0731 18:11:06.851963 16897 net.cpp:104] Using FLOAT as default forward math type
I0731 18:11:06.851971 16897 net.cpp:110] Using FLOAT as default backward math type
I0731 18:11:06.852011 16897 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0731 18:11:06.852021 16897 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 18:11:06.855191 16896 solver.cpp:42] Solver data type: FLOAT
W0731 18:11:06.855737 16896 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0731 18:11:06.855850 16896 net.cpp:104] Using FLOAT as default forward math type
I0731 18:11:06.855855 16896 net.cpp:110] Using FLOAT as default backward math type
I0731 18:11:06.855893 16896 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0731 18:11:06.855901 16896 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 18:11:06.855937 16898 db_lmdb.cpp:35] Opened lmdb data/train-image-lmdb
I0731 18:11:06.856840 16899 db_lmdb.cpp:35] Opened lmdb data/train-image-lmdb
I0731 18:11:06.859509 16897 data_layer.cpp:184] [2] ReshapePrefetch 6, 3, 640, 640
I0731 18:11:06.860684 16897 data_layer.cpp:208] [2] Output data size: 6, 3, 640, 640
I0731 18:11:06.860720 16897 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 18:11:06.860795 16897 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0731 18:11:06.860808 16897 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 18:11:06.861140 16896 data_layer.cpp:184] [1] ReshapePrefetch 6, 3, 640, 640
I0731 18:11:06.861230 16896 data_layer.cpp:208] [1] Output data size: 6, 3, 640, 640
I0731 18:11:06.861237 16896 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 18:11:06.861263 16896 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0731 18:11:06.861269 16896 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 18:11:06.861763 16900 data_layer.cpp:97] [2] Parser threads: 1
I0731 18:11:06.861768 16900 data_layer.cpp:99] [2] Transformer threads: 1
I0731 18:11:06.867401 16901 db_lmdb.cpp:35] Opened lmdb data/train-label-lmdb
I0731 18:11:06.868541 16902 data_layer.cpp:97] [1] Parser threads: 1
I0731 18:11:06.868577 16902 data_layer.cpp:99] [1] Transformer threads: 1
I0731 18:11:06.874500 16903 db_lmdb.cpp:35] Opened lmdb data/train-label-lmdb
I0731 18:11:06.874506 16897 data_layer.cpp:184] [2] ReshapePrefetch 6, 1, 640, 640
I0731 18:11:06.875553 16897 data_layer.cpp:208] [2] Output data size: 6, 1, 640, 640
I0731 18:11:06.875592 16897 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 18:11:06.877156 16896 data_layer.cpp:184] [1] ReshapePrefetch 6, 1, 640, 640
I0731 18:11:06.886016 16896 data_layer.cpp:208] [1] Output data size: 6, 1, 640, 640
I0731 18:11:06.886054 16896 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 18:11:06.886148 16904 data_layer.cpp:97] [2] Parser threads: 1
I0731 18:11:06.886157 16904 data_layer.cpp:99] [2] Transformer threads: 1
I0731 18:11:06.886183 16900 blocking_queue.cpp:40] Waiting for datum
I0731 18:11:06.894809 16905 data_layer.cpp:97] [1] Parser threads: 1
I0731 18:11:06.894873 16905 data_layer.cpp:99] [1] Transformer threads: 1
I0731 18:11:07.434801 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 7.99G, req 0G)
I0731 18:11:07.459107 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 7.99G, req 0G)
I0731 18:11:07.486738 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.83G, req 0G)
I0731 18:11:07.515368 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.83G, req 0G)
I0731 18:11:07.537058 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.7G, req 0G)
I0731 18:11:07.566838 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.7G, req 0G)
I0731 18:11:07.567260 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.62G, req 0G)
I0731 18:11:07.595706 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.62G, req 0G)
I0731 18:11:07.599392 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 7.56G, req 0G)
I0731 18:11:07.615047 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.52G, req 0G)
I0731 18:11:07.622081 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 7.56G, req 0G)
I0731 18:11:07.636879 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.52G, req 0G)
I0731 18:11:07.647147 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.48G, req 0G)
I0731 18:11:07.662684 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.46G, req 0G)
I0731 18:11:07.667522 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.48G, req 0G)
I0731 18:11:07.678479 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.46G, req 0G)
I0731 18:11:07.727372 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 7.39G, req 0G)
I0731 18:11:07.740222 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 7.39G, req 0G)
I0731 18:11:07.745641 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 7.34G, req 0G)
I0731 18:11:07.760699 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 7.34G, req 0G)
I0731 18:11:07.768312 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 7.32G, req 0G)
I0731 18:11:07.772894 16897 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/initial/test.prototxt
W0731 18:11:07.772974 16897 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0731 18:11:07.773118 16897 net.cpp:104] Using FLOAT as default forward math type
I0731 18:11:07.773123 16897 net.cpp:110] Using FLOAT as default backward math type
I0731 18:11:07.773149 16897 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0731 18:11:07.773154 16897 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 18:11:07.773926 16934 db_lmdb.cpp:35] Opened lmdb data/val-image-lmdb
I0731 18:11:07.775635 16897 data_layer.cpp:184] (2) ReshapePrefetch 2, 3, 640, 640
I0731 18:11:07.775709 16897 data_layer.cpp:208] (2) Output data size: 2, 3, 640, 640
I0731 18:11:07.775715 16897 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 18:11:07.775750 16897 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0731 18:11:07.775758 16897 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 18:11:07.777453 16936 db_lmdb.cpp:35] Opened lmdb data/val-label-lmdb
I0731 18:11:07.778421 16935 data_layer.cpp:97] (2) Parser threads: 1
I0731 18:11:07.778448 16935 data_layer.cpp:99] (2) Transformer threads: 1
I0731 18:11:07.781810 16897 data_layer.cpp:184] (2) ReshapePrefetch 2, 1, 640, 640
I0731 18:11:07.781919 16897 data_layer.cpp:208] (2) Output data size: 2, 1, 640, 640
I0731 18:11:07.781927 16897 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 18:11:07.783246 16937 data_layer.cpp:97] (2) Parser threads: 1
I0731 18:11:07.783255 16937 data_layer.cpp:99] (2) Transformer threads: 1
I0731 18:11:07.783886 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 7.32G, req 0G)
I0731 18:11:07.790647 16896 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/initial/test.prototxt
W0731 18:11:07.790725 16896 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0731 18:11:07.790853 16896 net.cpp:104] Using FLOAT as default forward math type
I0731 18:11:07.790858 16896 net.cpp:110] Using FLOAT as default backward math type
I0731 18:11:07.790880 16896 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0731 18:11:07.790885 16896 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 18:11:07.791796 16938 db_lmdb.cpp:35] Opened lmdb data/val-image-lmdb
I0731 18:11:07.793246 16896 data_layer.cpp:184] (1) ReshapePrefetch 2, 3, 640, 640
I0731 18:11:07.793371 16896 data_layer.cpp:208] (1) Output data size: 2, 3, 640, 640
I0731 18:11:07.793387 16896 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 18:11:07.793548 16896 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0731 18:11:07.793561 16896 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 18:11:07.794535 16897 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.18G, req 0G)
I0731 18:11:07.794718 16939 data_layer.cpp:97] (1) Parser threads: 1
I0731 18:11:07.794729 16939 data_layer.cpp:99] (1) Transformer threads: 1
I0731 18:11:07.797039 16940 db_lmdb.cpp:35] Opened lmdb data/val-label-lmdb
I0731 18:11:07.797806 16896 data_layer.cpp:184] (1) ReshapePrefetch 2, 1, 640, 640
I0731 18:11:07.797951 16896 data_layer.cpp:208] (1) Output data size: 2, 1, 640, 640
I0731 18:11:07.797961 16896 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 18:11:07.799612 16941 data_layer.cpp:97] (1) Parser threads: 1
I0731 18:11:07.799625 16941 data_layer.cpp:99] (1) Transformer threads: 1
I0731 18:11:07.810817 16896 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.18G, req 0G)
I0731 18:11:07.823050 16897 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.15G, req 0G)
I0731 18:11:07.827843 16896 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.15G, req 0G)
I0731 18:11:07.832900 16897 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.13G, req 0G)
I0731 18:11:07.837729 16896 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.13G, req 0G)
I0731 18:11:07.841496 16897 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.12G, req 0G)
I0731 18:11:07.846345 16896 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.12G, req 0G)
I0731 18:11:07.848808 16897 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.1G, req 0G)
I0731 18:11:07.854193 16896 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.1G, req 0G)
I0731 18:11:07.857270 16897 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.09G, req 0G)
I0731 18:11:07.860867 16896 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.09G, req 0G)
I0731 18:11:07.871522 16897 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.09G, req 0G)
I0731 18:11:07.874418 16896 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.09G, req 0G)
I0731 18:11:07.879724 16897 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.08G, req 0G)
I0731 18:11:07.882202 16896 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.08G, req 0G)
I0731 18:11:07.930487 16897 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 7.07G, req 0G)
I0731 18:11:07.934023 16896 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 7.07G, req 0G)
I0731 18:11:07.939203 16897 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 7.06G, req 0G)
I0731 18:11:07.941087 16896 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 7.06G, req 0G)
I0731 18:11:07.953794 16897 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 7.05G, req 0G)
I0731 18:11:07.956670 16896 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 7.05G, req 0G)
I0731 18:11:07.956921 16897 solver.cpp:56] Solver scaffolding done.
I0731 18:11:07.964550 16896 solver.cpp:56] Solver scaffolding done.
I0731 18:11:08.033459 16896 parallel.cpp:164] [1 - 1] P2pSync adding callback
I0731 18:11:08.033459 16895 parallel.cpp:164] [0 - 0] P2pSync adding callback
I0731 18:11:08.033459 16897 parallel.cpp:164] [2 - 2] P2pSync adding callback
I0731 18:11:08.254377 16897 solver.cpp:479] Solving jsegnet21v2_train
I0731 18:11:08.254395 16897 solver.cpp:480] Learning Rate Policy: multistep
I0731 18:11:08.254406 16895 solver.cpp:479] Solving jsegnet21v2_train
I0731 18:11:08.254413 16895 solver.cpp:480] Learning Rate Policy: multistep
I0731 18:11:08.254406 16896 solver.cpp:479] Solving jsegnet21v2_train
I0731 18:11:08.254422 16896 solver.cpp:480] Learning Rate Policy: multistep
I0731 18:11:08.268198 16896 solver.cpp:268] Starting Optimization on GPU 1
I0731 18:11:08.268199 16895 solver.cpp:268] Starting Optimization on GPU 0
I0731 18:11:08.268199 16897 solver.cpp:268] Starting Optimization on GPU 2
I0731 18:11:08.268409 16895 solver.cpp:550] Iteration 0, Testing net (#0)
I0731 18:11:08.268424 16958 device_alternate.hpp:116] NVML initialized on thread 127868910716672
I0731 18:11:08.268450 16958 common.cpp:583] NVML succeeded to set CPU affinity on device 2
I0731 18:11:08.269170 16959 device_alternate.hpp:116] NVML initialized on thread 127868919109376
I0731 18:11:08.269181 16959 common.cpp:583] NVML succeeded to set CPU affinity on device 0
I0731 18:11:08.269201 16960 device_alternate.hpp:116] NVML initialized on thread 127868927502080
I0731 18:11:08.269214 16960 common.cpp:583] NVML succeeded to set CPU affinity on device 1
I0731 18:11:08.282621 16897 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 6.96G, req 0G)
I0731 18:11:08.288322 16896 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 6.96G, req 0G)
I0731 18:11:08.304783 16897 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 6.9G, req 0G)
I0731 18:11:08.306720 16896 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 6.9G, req 0G)
I0731 18:11:08.320374 16897 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 6.84G, req 0G)
I0731 18:11:08.321465 16895 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1a' with space 0.01G/1 1  (limit 6.88G, req 0G)
I0731 18:11:08.325698 16896 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 6.84G, req 0G)
I0731 18:11:08.331784 16897 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 6.81G, req 0G)
I0731 18:11:08.339139 16896 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 6.81G, req 0G)
I0731 18:11:08.339395 16897 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 6.78G, req 0G)
I0731 18:11:08.344094 16895 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 6.82G, req 0G)
I0731 18:11:08.345793 16897 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 6.76G, req 0G)
I0731 18:11:08.347674 16896 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 6.78G, req 0G)
I0731 18:11:08.354610 16897 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.74G, req 0G)
I0731 18:11:08.354832 16896 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 6.76G, req 0G)
I0731 18:11:08.358342 16895 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 6.75G, req 0G)
I0731 18:11:08.359365 16897 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.74G, req 0G)
I0731 18:11:08.363245 16896 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.74G, req 0G)
I0731 18:11:08.367462 16896 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.74G, req 0G)
I0731 18:11:08.369386 16895 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 6.72G, req 0G)
I0731 18:11:08.378510 16895 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 6.69G, req 0G)
I0731 18:11:08.384770 16895 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 6.67G, req 0G)
I0731 18:11:08.387061 16897 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.58G, req 0G)
I0731 18:11:08.392993 16895 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.65G, req 0G)
I0731 18:11:08.393155 16897 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.57G, req 0G)
I0731 18:11:08.397418 16896 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.58G, req 0G)
I0731 18:11:08.397995 16895 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.65G, req 0G)
I0731 18:11:08.404460 16896 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.57G, req 0G)
I0731 18:11:08.421082 16897 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.48G, req 0G)
I0731 18:11:08.426198 16895 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.49G, req 0G)
I0731 18:11:08.429194 16896 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.48G, req 0G)
I0731 18:11:08.431536 16895 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.48G, req 0G)
I0731 18:11:08.460011 16895 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.39G, req 0G)
I0731 18:11:08.546785 16895 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.0484509
I0731 18:11:08.546823 16895 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.590535
I0731 18:11:08.546829 16895 solver.cpp:635]     Test net output #2: loss = 83.105 (* 1 = 83.105 loss)
I0731 18:11:08.546833 16895 solver.cpp:295] [MultiGPU] Initial Test completed
I0731 18:11:08.631512 16895 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 6.19G, req 0G)
I0731 18:11:08.634982 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 6.28G, req 0G)
I0731 18:11:08.642751 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 6.28G, req 0G)
I0731 18:11:08.683637 16895 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 6.03G, req 0G)
I0731 18:11:08.691367 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 6.12G, req 0G)
I0731 18:11:08.697335 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 6.12G, req 0G)
I0731 18:11:08.728037 16895 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 1 4 3  (limit 5.85G, req 0G)
I0731 18:11:08.741925 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 5.94G, req 0G)
I0731 18:11:08.745398 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 5.94G, req 0G)
I0731 18:11:08.750954 16895 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 5.77G, req 0G)
I0731 18:11:08.767264 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 5.86G, req 0G)
I0731 18:11:08.770375 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 5.86G, req 0G)
I0731 18:11:08.773150 16895 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 5.68G, req 0G)
I0731 18:11:08.786556 16895 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 5.64G, req 0G)
I0731 18:11:08.791337 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 5.77G, req 0G)
I0731 18:11:08.796172 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 5.77G, req 0G)
I0731 18:11:08.806862 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 5.72G, req 0G)
I0731 18:11:08.810760 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 5.72G, req 0G)
I0731 18:11:08.810948 16895 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 5.61G, req 0G)
I0731 18:11:08.819629 16895 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 1 4 3  (limit 5.59G, req 0G)
I0731 18:11:08.828474 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 5.7G, req 0G)
I0731 18:11:08.833760 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 5.7G, req 0G)
I0731 18:11:08.837771 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 5.68G, req 0G)
I0731 18:11:08.843185 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 1 3  (limit 5.68G, req 0G)
I0731 18:11:08.864863 16895 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 5.32G, req 0G)
I0731 18:11:08.879400 16895 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 5.3G, req 0G)
I0731 18:11:08.886741 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 5.41G, req 0G)
I0731 18:11:08.889319 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 5.41G, req 0G)
I0731 18:11:08.903465 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 5.39G, req 0G)
I0731 18:11:08.906509 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 5.39G, req 0G)
I0731 18:11:08.907308 16895 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 5.14G, req 0G)
I0731 18:11:08.937857 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 5.23G, req 0G)
I0731 18:11:08.941301 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 5.23G, req 0G)
I0731 18:11:09.123849 16895 solver.cpp:358] Iteration 0 (0.576975 s), loss = 2.16281
I0731 18:11:09.123868 16895 solver.cpp:375]     Train net output #0: loss = 2.16281 (* 1 = 2.16281 loss)
I0731 18:11:09.123873 16895 sgd_solver.cpp:136] Iteration 0, lr = 0.0001, m = 0.9
I0731 18:11:09.340369 16895 solver.cpp:358] Iteration 1 (0.216504 s), loss = 2.1386
I0731 18:11:09.340394 16895 solver.cpp:375]     Train net output #0: loss = 2.1386 (* 1 = 2.1386 loss)
I0731 18:11:09.443681 16895 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1a' with space 1.29G/1 1 0 3  (limit 2.99G, req 0G)
I0731 18:11:09.451325 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1a' with space 1.29G/1 1 0 3  (limit 3.08G, req 0G)
I0731 18:11:09.452286 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1a' with space 1.29G/1 1 0 3  (limit 3.08G, req 0G)
I0731 18:11:09.505254 16895 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1b' with space 2.57G/2 6 4 3  (limit 1.7G, req 0G)
I0731 18:11:09.531277 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0G)
I0731 18:11:09.535879 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0G)
I0731 18:11:09.634876 16895 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 2.57G/1 6 4 3  (limit 1.7G, req 0G)
I0731 18:11:09.668767 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 2.57G/1 6 4 3  (limit 1.79G, req 0G)
I0731 18:11:09.675148 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 2.57G/1 6 4 3  (limit 1.79G, req 0G)
I0731 18:11:09.676532 16895 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 2.57G/2 6 4 3  (limit 1.7G, req 0G)
I0731 18:11:09.713724 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0G)
I0731 18:11:09.719440 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0G)
I0731 18:11:09.772716 16895 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 2.57G/1 6 4 5  (limit 1.7G, req 0.07G)
I0731 18:11:09.798176 16895 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 2.57G/2 6 1 3  (limit 1.7G, req 0.07G)
I0731 18:11:09.815923 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 2.57G/1 6 4 5  (limit 1.79G, req 0.07G)
I0731 18:11:09.822451 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 2.57G/1 6 4 5  (limit 1.79G, req 0.07G)
I0731 18:11:09.844193 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 2.57G/2 6 4 5  (limit 1.79G, req 0.07G)
I0731 18:11:09.848942 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0.07G)
I0731 18:11:09.864670 16895 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 2.57G/1 6 4 5  (limit 1.7G, req 0.07G)
I0731 18:11:09.878459 16895 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 2.57G/2 6 4 3  (limit 1.7G, req 0.07G)
I0731 18:11:09.915311 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 2.57G/1 6 4 5  (limit 1.79G, req 0.07G)
I0731 18:11:09.920207 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 2.57G/1 6 4 5  (limit 1.79G, req 0.07G)
I0731 18:11:09.929852 16895 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'out3a' with space 2.57G/2 6 4 3  (limit 1.7G, req 0.07G)
I0731 18:11:09.930867 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0.07G)
I0731 18:11:09.934341 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0.07G)
I0731 18:11:09.983330 16895 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 2.57G/1 6 4 3  (limit 1.7G, req 0.07G)
I0731 18:11:09.987082 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'out3a' with space 2.57G/2 6 4 3  (limit 1.79G, req 0.07G)
I0731 18:11:09.990953 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'out3a' with space 2.57G/2 6 4 3  (limit 1.79G, req 0.07G)
I0731 18:11:10.011384 16895 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 2.57G/1 6 1 3  (limit 1.7G, req 0.07G)
I0731 18:11:10.043587 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 2.57G/1 6 4 3  (limit 1.79G, req 0.07G)
I0731 18:11:10.048048 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 2.57G/1 6 4 3  (limit 1.79G, req 0.07G)
I0731 18:11:10.071342 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 2.57G/1 6 1 5  (limit 1.79G, req 0.07G)
I0731 18:11:10.075688 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 2.57G/1 6 1 5  (limit 1.79G, req 0.07G)
I0731 18:11:10.204319 16895 solver.cpp:358] Iteration 2 (0.86392 s), loss = 2.13893
I0731 18:11:10.204342 16895 solver.cpp:375]     Train net output #0: loss = 2.13893 (* 1 = 2.13893 loss)
I0731 18:11:10.204941 16896 cudnn_conv_layer.cpp:292] [1] Layer 'conv1a' reallocating workspace: 2.57G -> 0.14G
I0731 18:11:10.204953 16897 cudnn_conv_layer.cpp:292] [2] Layer 'conv1a' reallocating workspace: 2.57G -> 0.14G
I0731 18:11:10.225599 16895 cudnn_conv_layer.cpp:292] [0] Layer 'conv1a' reallocating workspace: 2.57G -> 0.14G
I0731 18:11:28.474076 16895 solver.cpp:353] Iteration 100 (5.36421 iter/s, 18.2692s/98 iter), loss = 0.568728
I0731 18:11:28.474098 16895 solver.cpp:375]     Train net output #0: loss = 0.568728 (* 1 = 0.568728 loss)
I0731 18:11:28.474102 16895 sgd_solver.cpp:136] Iteration 100, lr = 0.0001, m = 0.9
I0731 18:11:39.958631 16903 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 18:11:46.961035 16895 solver.cpp:353] Iteration 200 (5.40937 iter/s, 18.4864s/100 iter), loss = 0.509982
I0731 18:11:46.961060 16895 solver.cpp:375]     Train net output #0: loss = 0.509982 (* 1 = 0.509982 loss)
I0731 18:11:46.961063 16895 sgd_solver.cpp:136] Iteration 200, lr = 0.0001, m = 0.9
I0731 18:12:05.369566 16895 solver.cpp:353] Iteration 300 (5.43242 iter/s, 18.408s/100 iter), loss = 0.342063
I0731 18:12:05.369595 16895 solver.cpp:375]     Train net output #0: loss = 0.342063 (* 1 = 0.342063 loss)
I0731 18:12:05.369601 16895 sgd_solver.cpp:136] Iteration 300, lr = 0.0001, m = 0.9
I0731 18:12:10.424635 16874 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 18:12:23.813911 16895 solver.cpp:353] Iteration 400 (5.42187 iter/s, 18.4438s/100 iter), loss = 0.366329
I0731 18:12:23.813932 16895 solver.cpp:375]     Train net output #0: loss = 0.366329 (* 1 = 0.366329 loss)
I0731 18:12:23.813937 16895 sgd_solver.cpp:136] Iteration 400, lr = 0.0001, m = 0.9
I0731 18:12:42.496090 16895 solver.cpp:353] Iteration 500 (5.35284 iter/s, 18.6817s/100 iter), loss = 0.23861
I0731 18:12:42.496150 16895 solver.cpp:375]     Train net output #0: loss = 0.238609 (* 1 = 0.238609 loss)
I0731 18:12:42.496157 16895 sgd_solver.cpp:136] Iteration 500, lr = 0.0001, m = 0.9
I0731 18:13:01.049793 16895 solver.cpp:353] Iteration 600 (5.38991 iter/s, 18.5532s/100 iter), loss = 0.256315
I0731 18:13:01.049818 16895 solver.cpp:375]     Train net output #0: loss = 0.256315 (* 1 = 0.256315 loss)
I0731 18:13:01.049823 16895 sgd_solver.cpp:136] Iteration 600, lr = 0.0001, m = 0.9
I0731 18:13:11.765561 16876 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 18:13:19.544248 16895 solver.cpp:353] Iteration 700 (5.40718 iter/s, 18.4939s/100 iter), loss = 0.334405
I0731 18:13:19.544299 16895 solver.cpp:375]     Train net output #0: loss = 0.334405 (* 1 = 0.334405 loss)
I0731 18:13:19.544304 16895 sgd_solver.cpp:136] Iteration 700, lr = 0.0001, m = 0.9
I0731 18:13:38.354162 16895 solver.cpp:353] Iteration 800 (5.31649 iter/s, 18.8094s/100 iter), loss = 0.282974
I0731 18:13:38.354190 16895 solver.cpp:375]     Train net output #0: loss = 0.282974 (* 1 = 0.282974 loss)
I0731 18:13:38.354197 16895 sgd_solver.cpp:136] Iteration 800, lr = 0.0001, m = 0.9
I0731 18:13:56.983738 16895 solver.cpp:353] Iteration 900 (5.36796 iter/s, 18.6291s/100 iter), loss = 0.213485
I0731 18:13:56.983816 16895 solver.cpp:375]     Train net output #0: loss = 0.213485 (* 1 = 0.213485 loss)
I0731 18:13:56.983821 16895 sgd_solver.cpp:136] Iteration 900, lr = 0.0001, m = 0.9
I0731 18:14:13.357496 16903 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 18:14:15.605506 16895 solver.cpp:353] Iteration 1000 (5.37021 iter/s, 18.6213s/100 iter), loss = 0.235646
I0731 18:14:15.605533 16895 solver.cpp:375]     Train net output #0: loss = 0.235645 (* 1 = 0.235645 loss)
I0731 18:14:15.605540 16895 sgd_solver.cpp:136] Iteration 1000, lr = 0.0001, m = 0.9
I0731 18:14:34.155038 16895 solver.cpp:353] Iteration 1100 (5.39112 iter/s, 18.549s/100 iter), loss = 0.191682
I0731 18:14:34.155097 16895 solver.cpp:375]     Train net output #0: loss = 0.191682 (* 1 = 0.191682 loss)
I0731 18:14:34.155104 16895 sgd_solver.cpp:136] Iteration 1100, lr = 0.0001, m = 0.9
I0731 18:14:43.980026 16899 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 18:14:52.629736 16895 solver.cpp:353] Iteration 1200 (5.41296 iter/s, 18.4742s/100 iter), loss = 0.246225
I0731 18:14:52.629762 16895 solver.cpp:375]     Train net output #0: loss = 0.246225 (* 1 = 0.246225 loss)
I0731 18:14:52.629766 16895 sgd_solver.cpp:136] Iteration 1200, lr = 0.0001, m = 0.9
I0731 18:15:11.125047 16895 solver.cpp:353] Iteration 1300 (5.40693 iter/s, 18.4948s/100 iter), loss = 0.329271
I0731 18:15:11.125321 16895 solver.cpp:375]     Train net output #0: loss = 0.329271 (* 1 = 0.329271 loss)
I0731 18:15:11.125329 16895 sgd_solver.cpp:136] Iteration 1300, lr = 0.0001, m = 0.9
I0731 18:15:29.649334 16895 solver.cpp:353] Iteration 1400 (5.39847 iter/s, 18.5238s/100 iter), loss = 0.325848
I0731 18:15:29.649358 16895 solver.cpp:375]     Train net output #0: loss = 0.325848 (* 1 = 0.325848 loss)
I0731 18:15:29.649361 16895 sgd_solver.cpp:136] Iteration 1400, lr = 0.0001, m = 0.9
I0731 18:15:45.260100 16901 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 18:15:48.259616 16895 solver.cpp:353] Iteration 1500 (5.37352 iter/s, 18.6098s/100 iter), loss = 0.389745
I0731 18:15:48.259644 16895 solver.cpp:375]     Train net output #0: loss = 0.389745 (* 1 = 0.389745 loss)
I0731 18:15:48.259649 16895 sgd_solver.cpp:136] Iteration 1500, lr = 0.0001, m = 0.9
I0731 18:16:06.875597 16895 solver.cpp:353] Iteration 1600 (5.37188 iter/s, 18.6155s/100 iter), loss = 0.196608
I0731 18:16:06.875622 16895 solver.cpp:375]     Train net output #0: loss = 0.196608 (* 1 = 0.196608 loss)
I0731 18:16:06.875627 16895 sgd_solver.cpp:136] Iteration 1600, lr = 0.0001, m = 0.9
I0731 18:16:25.395726 16895 solver.cpp:353] Iteration 1700 (5.39968 iter/s, 18.5196s/100 iter), loss = 0.277557
I0731 18:16:25.395776 16895 solver.cpp:375]     Train net output #0: loss = 0.277557 (* 1 = 0.277557 loss)
I0731 18:16:25.395781 16895 sgd_solver.cpp:136] Iteration 1700, lr = 0.0001, m = 0.9
I0731 18:16:44.026406 16895 solver.cpp:353] Iteration 1800 (5.36764 iter/s, 18.6302s/100 iter), loss = 0.168652
I0731 18:16:44.026433 16895 solver.cpp:375]     Train net output #0: loss = 0.168652 (* 1 = 0.168652 loss)
I0731 18:16:44.026439 16895 sgd_solver.cpp:136] Iteration 1800, lr = 0.0001, m = 0.9
I0731 18:16:46.825479 16903 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 18:17:02.564273 16895 solver.cpp:353] Iteration 1900 (5.39451 iter/s, 18.5374s/100 iter), loss = 0.13945
I0731 18:17:02.564388 16895 solver.cpp:375]     Train net output #0: loss = 0.13945 (* 1 = 0.13945 loss)
I0731 18:17:02.564394 16895 sgd_solver.cpp:136] Iteration 1900, lr = 0.0001, m = 0.9
I0731 18:17:17.443951 16898 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 18:17:20.914850 16895 solver.cpp:550] Iteration 2000, Testing net (#0)
I0731 18:17:21.184744 16895 blocking_queue.cpp:40] Data layer prefetch queue empty
I0731 18:17:53.604980 16895 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.930324
I0731 18:17:53.605078 16895 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999572
I0731 18:17:53.605087 16895 solver.cpp:635]     Test net output #2: loss = 0.208021 (* 1 = 0.208021 loss)
I0731 18:17:53.605118 16895 solver.cpp:305] [MultiGPU] Tests completed in 32.6894s
I0731 18:17:53.816975 16895 solver.cpp:353] Iteration 2000 (1.95117 iter/s, 51.2513s/100 iter), loss = 0.24678
I0731 18:17:53.817005 16895 solver.cpp:375]     Train net output #0: loss = 0.24678 (* 1 = 0.24678 loss)
I0731 18:17:53.817013 16895 sgd_solver.cpp:136] Iteration 2000, lr = 0.0001, m = 0.9
I0731 18:18:12.268326 16895 solver.cpp:353] Iteration 2100 (5.41981 iter/s, 18.4508s/100 iter), loss = 0.135249
I0731 18:18:12.268354 16895 solver.cpp:375]     Train net output #0: loss = 0.135249 (* 1 = 0.135249 loss)
I0731 18:18:12.268362 16895 sgd_solver.cpp:136] Iteration 2100, lr = 0.0001, m = 0.9
I0731 18:18:20.714885 16903 data_reader.cpp:264] Starting prefetch of epoch 4
I0731 18:18:30.821285 16895 solver.cpp:353] Iteration 2200 (5.39013 iter/s, 18.5524s/100 iter), loss = 0.332999
I0731 18:18:30.821336 16895 solver.cpp:375]     Train net output #0: loss = 0.332999 (* 1 = 0.332999 loss)
I0731 18:18:30.821341 16895 sgd_solver.cpp:136] Iteration 2200, lr = 0.0001, m = 0.9
I0731 18:18:49.256325 16895 solver.cpp:353] Iteration 2300 (5.4246 iter/s, 18.4345s/100 iter), loss = 0.157549
I0731 18:18:49.256347 16895 solver.cpp:375]     Train net output #0: loss = 0.157549 (* 1 = 0.157549 loss)
I0731 18:18:49.256352 16895 sgd_solver.cpp:136] Iteration 2300, lr = 0.0001, m = 0.9
I0731 18:19:07.777689 16895 solver.cpp:353] Iteration 2400 (5.39932 iter/s, 18.5208s/100 iter), loss = 0.136544
I0731 18:19:07.777806 16895 solver.cpp:375]     Train net output #0: loss = 0.136544 (* 1 = 0.136544 loss)
I0731 18:19:07.777812 16895 sgd_solver.cpp:136] Iteration 2400, lr = 0.0001, m = 0.9
I0731 18:19:21.819599 16876 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 18:19:26.283193 16895 solver.cpp:353] Iteration 2500 (5.40395 iter/s, 18.505s/100 iter), loss = 0.189543
I0731 18:19:26.283221 16895 solver.cpp:375]     Train net output #0: loss = 0.189543 (* 1 = 0.189543 loss)
I0731 18:19:26.283224 16895 sgd_solver.cpp:136] Iteration 2500, lr = 0.0001, m = 0.9
I0731 18:19:44.738615 16895 solver.cpp:353] Iteration 2600 (5.41861 iter/s, 18.4549s/100 iter), loss = 0.161889
I0731 18:19:44.738690 16895 solver.cpp:375]     Train net output #0: loss = 0.161889 (* 1 = 0.161889 loss)
I0731 18:19:44.738695 16895 sgd_solver.cpp:136] Iteration 2600, lr = 0.0001, m = 0.9
I0731 18:19:52.439607 16898 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 18:20:03.364154 16895 solver.cpp:353] Iteration 2700 (5.36912 iter/s, 18.625s/100 iter), loss = 0.170089
I0731 18:20:03.364181 16895 solver.cpp:375]     Train net output #0: loss = 0.170089 (* 1 = 0.170089 loss)
I0731 18:20:03.364184 16895 sgd_solver.cpp:136] Iteration 2700, lr = 0.0001, m = 0.9
I0731 18:20:21.886448 16895 solver.cpp:353] Iteration 2800 (5.39905 iter/s, 18.5218s/100 iter), loss = 0.18703
I0731 18:20:21.887114 16895 solver.cpp:375]     Train net output #0: loss = 0.18703 (* 1 = 0.18703 loss)
I0731 18:20:21.887133 16895 sgd_solver.cpp:136] Iteration 2800, lr = 0.0001, m = 0.9
I0731 18:20:40.329900 16895 solver.cpp:353] Iteration 2900 (5.42213 iter/s, 18.4429s/100 iter), loss = 0.178707
I0731 18:20:40.329924 16895 solver.cpp:375]     Train net output #0: loss = 0.178707 (* 1 = 0.178707 loss)
I0731 18:20:40.329928 16895 sgd_solver.cpp:136] Iteration 2900, lr = 0.0001, m = 0.9
I0731 18:20:53.570683 16899 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 18:20:58.882877 16895 solver.cpp:353] Iteration 3000 (5.39012 iter/s, 18.5525s/100 iter), loss = 0.176111
I0731 18:20:58.882899 16895 solver.cpp:375]     Train net output #0: loss = 0.176111 (* 1 = 0.176111 loss)
I0731 18:20:58.882903 16895 sgd_solver.cpp:136] Iteration 3000, lr = 0.0001, m = 0.9
I0731 18:21:17.448598 16895 solver.cpp:353] Iteration 3100 (5.38642 iter/s, 18.5652s/100 iter), loss = 0.0989209
I0731 18:21:17.448626 16895 solver.cpp:375]     Train net output #0: loss = 0.0989209 (* 1 = 0.0989209 loss)
I0731 18:21:17.448629 16895 sgd_solver.cpp:136] Iteration 3100, lr = 0.0001, m = 0.9
I0731 18:21:35.854506 16895 solver.cpp:353] Iteration 3200 (5.43319 iter/s, 18.4054s/100 iter), loss = 0.338542
I0731 18:21:35.854554 16895 solver.cpp:375]     Train net output #0: loss = 0.338542 (* 1 = 0.338542 loss)
I0731 18:21:35.854562 16895 sgd_solver.cpp:136] Iteration 3200, lr = 0.0001, m = 0.9
I0731 18:21:54.229046 16895 solver.cpp:353] Iteration 3300 (5.44246 iter/s, 18.374s/100 iter), loss = 0.134473
I0731 18:21:54.229068 16895 solver.cpp:375]     Train net output #0: loss = 0.134473 (* 1 = 0.134473 loss)
I0731 18:21:54.229073 16895 sgd_solver.cpp:136] Iteration 3300, lr = 0.0001, m = 0.9
I0731 18:21:54.629715 16876 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 18:22:12.843266 16895 solver.cpp:353] Iteration 3400 (5.37238 iter/s, 18.6137s/100 iter), loss = 0.102924
I0731 18:22:12.843497 16895 solver.cpp:375]     Train net output #0: loss = 0.102924 (* 1 = 0.102924 loss)
I0731 18:22:12.843504 16895 sgd_solver.cpp:136] Iteration 3400, lr = 0.0001, m = 0.9
I0731 18:22:25.323323 16899 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 18:22:31.383546 16895 solver.cpp:353] Iteration 3500 (5.39381 iter/s, 18.5398s/100 iter), loss = 0.264272
I0731 18:22:31.383570 16895 solver.cpp:375]     Train net output #0: loss = 0.264272 (* 1 = 0.264272 loss)
I0731 18:22:31.383574 16895 sgd_solver.cpp:136] Iteration 3500, lr = 0.0001, m = 0.9
I0731 18:22:49.967012 16895 solver.cpp:353] Iteration 3600 (5.38128 iter/s, 18.583s/100 iter), loss = 0.181135
I0731 18:22:49.967083 16895 solver.cpp:375]     Train net output #0: loss = 0.181134 (* 1 = 0.181134 loss)
I0731 18:22:49.967088 16895 sgd_solver.cpp:136] Iteration 3600, lr = 0.0001, m = 0.9
I0731 18:23:08.772717 16895 solver.cpp:353] Iteration 3700 (5.31768 iter/s, 18.8052s/100 iter), loss = 0.162281
I0731 18:23:08.772742 16895 solver.cpp:375]     Train net output #0: loss = 0.16228 (* 1 = 0.16228 loss)
I0731 18:23:08.772745 16895 sgd_solver.cpp:136] Iteration 3700, lr = 0.0001, m = 0.9
I0731 18:23:27.115873 16898 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 18:23:27.452574 16895 solver.cpp:353] Iteration 3800 (5.35351 iter/s, 18.6793s/100 iter), loss = 0.248596
I0731 18:23:27.452597 16895 solver.cpp:375]     Train net output #0: loss = 0.248596 (* 1 = 0.248596 loss)
I0731 18:23:27.452601 16895 sgd_solver.cpp:136] Iteration 3800, lr = 0.0001, m = 0.9
I0731 18:23:46.073104 16895 solver.cpp:353] Iteration 3900 (5.37056 iter/s, 18.62s/100 iter), loss = 0.158688
I0731 18:23:46.073127 16895 solver.cpp:375]     Train net output #0: loss = 0.158688 (* 1 = 0.158688 loss)
I0731 18:23:46.073132 16895 sgd_solver.cpp:136] Iteration 3900, lr = 0.0001, m = 0.9
I0731 18:24:04.558969 16895 solver.cpp:550] Iteration 4000, Testing net (#0)
I0731 18:24:08.317531 16891 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 18:24:17.219688 16938 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 18:24:17.564672 16895 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.93666
I0731 18:24:17.564700 16895 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999939
I0731 18:24:17.564707 16895 solver.cpp:635]     Test net output #2: loss = 0.176968 (* 1 = 0.176968 loss)
I0731 18:24:17.564788 16895 solver.cpp:305] [MultiGPU] Tests completed in 13.0055s
I0731 18:24:17.752861 16895 solver.cpp:353] Iteration 4000 (3.15668 iter/s, 31.6789s/100 iter), loss = 0.100464
I0731 18:24:17.752887 16895 solver.cpp:375]     Train net output #0: loss = 0.100464 (* 1 = 0.100464 loss)
I0731 18:24:17.752892 16895 sgd_solver.cpp:136] Iteration 4000, lr = 0.0001, m = 0.9
I0731 18:24:36.393286 16895 solver.cpp:353] Iteration 4100 (5.36483 iter/s, 18.6399s/100 iter), loss = 0.177553
I0731 18:24:36.393337 16895 solver.cpp:375]     Train net output #0: loss = 0.177552 (* 1 = 0.177552 loss)
I0731 18:24:36.393342 16895 sgd_solver.cpp:136] Iteration 4100, lr = 0.0001, m = 0.9
I0731 18:24:54.841101 16895 solver.cpp:353] Iteration 4200 (5.42085 iter/s, 18.4473s/100 iter), loss = 0.191574
I0731 18:24:54.841125 16895 solver.cpp:375]     Train net output #0: loss = 0.191574 (* 1 = 0.191574 loss)
I0731 18:24:54.841130 16895 sgd_solver.cpp:136] Iteration 4200, lr = 0.0001, m = 0.9
I0731 18:25:12.312049 16874 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 18:25:13.391573 16895 solver.cpp:353] Iteration 4300 (5.39085 iter/s, 18.55s/100 iter), loss = 0.186605
I0731 18:25:13.391598 16895 solver.cpp:375]     Train net output #0: loss = 0.186605 (* 1 = 0.186605 loss)
I0731 18:25:13.391604 16895 sgd_solver.cpp:136] Iteration 4300, lr = 0.0001, m = 0.9
I0731 18:25:31.915668 16895 solver.cpp:353] Iteration 4400 (5.39852 iter/s, 18.5236s/100 iter), loss = 0.114894
I0731 18:25:31.915693 16895 solver.cpp:375]     Train net output #0: loss = 0.114894 (* 1 = 0.114894 loss)
I0731 18:25:31.915697 16895 sgd_solver.cpp:136] Iteration 4400, lr = 0.0001, m = 0.9
I0731 18:25:50.386718 16895 solver.cpp:353] Iteration 4500 (5.41403 iter/s, 18.4705s/100 iter), loss = 0.090743
I0731 18:25:50.386828 16895 solver.cpp:375]     Train net output #0: loss = 0.0907429 (* 1 = 0.0907429 loss)
I0731 18:25:50.386835 16895 sgd_solver.cpp:136] Iteration 4500, lr = 0.0001, m = 0.9
I0731 18:26:08.817718 16895 solver.cpp:353] Iteration 4600 (5.42579 iter/s, 18.4305s/100 iter), loss = 0.179321
I0731 18:26:08.817742 16895 solver.cpp:375]     Train net output #0: loss = 0.179321 (* 1 = 0.179321 loss)
I0731 18:26:08.817746 16895 sgd_solver.cpp:136] Iteration 4600, lr = 0.0001, m = 0.9
I0731 18:26:13.298305 16876 data_reader.cpp:264] Starting prefetch of epoch 4
I0731 18:26:27.423954 16895 solver.cpp:353] Iteration 4700 (5.37469 iter/s, 18.6057s/100 iter), loss = 0.158534
I0731 18:26:27.424036 16895 solver.cpp:375]     Train net output #0: loss = 0.158534 (* 1 = 0.158534 loss)
I0731 18:26:27.424041 16895 sgd_solver.cpp:136] Iteration 4700, lr = 0.0001, m = 0.9
I0731 18:26:44.173993 16874 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 18:26:45.998708 16895 solver.cpp:353] Iteration 4800 (5.3838 iter/s, 18.5742s/100 iter), loss = 0.230809
I0731 18:26:45.998733 16895 solver.cpp:375]     Train net output #0: loss = 0.230809 (* 1 = 0.230809 loss)
I0731 18:26:45.998739 16895 sgd_solver.cpp:136] Iteration 4800, lr = 0.0001, m = 0.9
I0731 18:27:04.479656 16895 solver.cpp:353] Iteration 4900 (5.41113 iter/s, 18.4804s/100 iter), loss = 0.144173
I0731 18:27:04.479712 16895 solver.cpp:375]     Train net output #0: loss = 0.144173 (* 1 = 0.144173 loss)
I0731 18:27:04.479717 16895 sgd_solver.cpp:136] Iteration 4900, lr = 0.0001, m = 0.9
I0731 18:27:23.092249 16895 solver.cpp:353] Iteration 5000 (5.37285 iter/s, 18.6121s/100 iter), loss = 0.162114
I0731 18:27:23.092273 16895 solver.cpp:375]     Train net output #0: loss = 0.162114 (* 1 = 0.162114 loss)
I0731 18:27:23.092278 16895 sgd_solver.cpp:136] Iteration 5000, lr = 0.0001, m = 0.9
I0731 18:27:41.685920 16895 solver.cpp:353] Iteration 5100 (5.37832 iter/s, 18.5932s/100 iter), loss = 0.213413
I0731 18:27:41.694779 16895 solver.cpp:375]     Train net output #0: loss = 0.213413 (* 1 = 0.213413 loss)
I0731 18:27:41.694824 16895 sgd_solver.cpp:136] Iteration 5100, lr = 0.0001, m = 0.9
I0731 18:27:45.415237 16901 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 18:28:00.181833 16895 solver.cpp:353] Iteration 5200 (5.40675 iter/s, 18.4954s/100 iter), loss = 0.210293
I0731 18:28:00.181856 16895 solver.cpp:375]     Train net output #0: loss = 0.210293 (* 1 = 0.210293 loss)
I0731 18:28:00.181861 16895 sgd_solver.cpp:136] Iteration 5200, lr = 0.0001, m = 0.9
I0731 18:28:18.618372 16895 solver.cpp:353] Iteration 5300 (5.42416 iter/s, 18.436s/100 iter), loss = 0.102583
I0731 18:28:18.618424 16895 solver.cpp:375]     Train net output #0: loss = 0.102583 (* 1 = 0.102583 loss)
I0731 18:28:18.618429 16895 sgd_solver.cpp:136] Iteration 5300, lr = 0.0001, m = 0.9
I0731 18:28:37.144887 16895 solver.cpp:353] Iteration 5400 (5.39782 iter/s, 18.526s/100 iter), loss = 0.100171
I0731 18:28:37.144913 16895 solver.cpp:375]     Train net output #0: loss = 0.100171 (* 1 = 0.100171 loss)
I0731 18:28:37.144917 16895 sgd_solver.cpp:136] Iteration 5400, lr = 0.0001, m = 0.9
I0731 18:28:46.657218 16876 data_reader.cpp:264] Starting prefetch of epoch 5
I0731 18:28:55.770293 16895 solver.cpp:353] Iteration 5500 (5.36916 iter/s, 18.6249s/100 iter), loss = 0.236338
I0731 18:28:55.770344 16895 solver.cpp:375]     Train net output #0: loss = 0.236338 (* 1 = 0.236338 loss)
I0731 18:28:55.770349 16895 sgd_solver.cpp:136] Iteration 5500, lr = 0.0001, m = 0.9
I0731 18:29:14.314664 16895 solver.cpp:353] Iteration 5600 (5.39262 iter/s, 18.5439s/100 iter), loss = 0.0807359
I0731 18:29:14.314687 16895 solver.cpp:375]     Train net output #0: loss = 0.0807359 (* 1 = 0.0807359 loss)
I0731 18:29:14.314693 16895 sgd_solver.cpp:136] Iteration 5600, lr = 0.0001, m = 0.9
I0731 18:29:17.329725 16874 data_reader.cpp:264] Starting prefetch of epoch 4
I0731 18:29:33.040515 16895 solver.cpp:353] Iteration 5700 (5.34036 iter/s, 18.7253s/100 iter), loss = 0.374645
I0731 18:29:33.040612 16895 solver.cpp:375]     Train net output #0: loss = 0.374645 (* 1 = 0.374645 loss)
I0731 18:29:33.040619 16895 sgd_solver.cpp:136] Iteration 5700, lr = 0.0001, m = 0.9
I0731 18:29:51.729663 16895 solver.cpp:353] Iteration 5800 (5.35085 iter/s, 18.6886s/100 iter), loss = 0.117933
I0731 18:29:51.729691 16895 solver.cpp:375]     Train net output #0: loss = 0.117933 (* 1 = 0.117933 loss)
I0731 18:29:51.729698 16895 sgd_solver.cpp:136] Iteration 5800, lr = 0.0001, m = 0.9
I0731 18:30:10.318073 16895 solver.cpp:353] Iteration 5900 (5.37984 iter/s, 18.5879s/100 iter), loss = 0.234558
I0731 18:30:10.318172 16895 solver.cpp:375]     Train net output #0: loss = 0.234558 (* 1 = 0.234558 loss)
I0731 18:30:10.318179 16895 sgd_solver.cpp:136] Iteration 5900, lr = 0.0001, m = 0.9
I0731 18:30:19.006623 16899 data_reader.cpp:264] Starting prefetch of epoch 4
I0731 18:30:28.762199 16895 solver.cpp:550] Iteration 6000, Testing net (#0)
I0731 18:30:40.010843 16895 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.938279
I0731 18:30:40.010866 16895 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999993
I0731 18:30:40.010871 16895 solver.cpp:635]     Test net output #2: loss = 0.175419 (* 1 = 0.175419 loss)
I0731 18:30:40.010948 16895 solver.cpp:305] [MultiGPU] Tests completed in 11.2484s
I0731 18:30:40.204170 16895 solver.cpp:353] Iteration 6000 (3.34613 iter/s, 29.8853s/100 iter), loss = 0.244488
I0731 18:30:40.204198 16895 solver.cpp:375]     Train net output #0: loss = 0.244488 (* 1 = 0.244488 loss)
I0731 18:30:40.204205 16895 sgd_solver.cpp:136] Iteration 6000, lr = 0.0001, m = 0.9
I0731 18:30:58.992117 16895 solver.cpp:353] Iteration 6100 (5.32271 iter/s, 18.7874s/100 iter), loss = 0.131227
I0731 18:30:58.992236 16895 solver.cpp:375]     Train net output #0: loss = 0.131227 (* 1 = 0.131227 loss)
I0731 18:30:58.992244 16895 sgd_solver.cpp:136] Iteration 6100, lr = 0.0001, m = 0.9
I0731 18:31:01.280764 16903 data_reader.cpp:264] Starting prefetch of epoch 5
I0731 18:31:17.636564 16895 solver.cpp:353] Iteration 6200 (5.36367 iter/s, 18.6439s/100 iter), loss = 0.15832
I0731 18:31:17.636590 16895 solver.cpp:375]     Train net output #0: loss = 0.15832 (* 1 = 0.15832 loss)
I0731 18:31:17.636593 16895 sgd_solver.cpp:136] Iteration 6200, lr = 0.0001, m = 0.9
I0731 18:31:31.922065 16899 data_reader.cpp:264] Starting prefetch of epoch 5
I0731 18:31:36.112583 16895 solver.cpp:353] Iteration 6300 (5.41257 iter/s, 18.4755s/100 iter), loss = 0.135247
I0731 18:31:36.112607 16895 solver.cpp:375]     Train net output #0: loss = 0.135247 (* 1 = 0.135247 loss)
I0731 18:31:36.112612 16895 sgd_solver.cpp:136] Iteration 6300, lr = 0.0001, m = 0.9
I0731 18:31:54.748971 16895 solver.cpp:353] Iteration 6400 (5.36599 iter/s, 18.6359s/100 iter), loss = 0.165852
I0731 18:31:54.748996 16895 solver.cpp:375]     Train net output #0: loss = 0.165852 (* 1 = 0.165852 loss)
I0731 18:31:54.749001 16895 sgd_solver.cpp:136] Iteration 6400, lr = 0.0001, m = 0.9
I0731 18:32:13.334138 16895 solver.cpp:353] Iteration 6500 (5.38078 iter/s, 18.5847s/100 iter), loss = 0.14043
I0731 18:32:13.334192 16895 solver.cpp:375]     Train net output #0: loss = 0.14043 (* 1 = 0.14043 loss)
I0731 18:32:13.334197 16895 sgd_solver.cpp:136] Iteration 6500, lr = 0.0001, m = 0.9
I0731 18:32:31.812597 16895 solver.cpp:353] Iteration 6600 (5.41186 iter/s, 18.4779s/100 iter), loss = 0.229537
I0731 18:32:31.812624 16895 solver.cpp:375]     Train net output #0: loss = 0.229537 (* 1 = 0.229537 loss)
I0731 18:32:31.812628 16895 sgd_solver.cpp:136] Iteration 6600, lr = 0.0001, m = 0.9
I0731 18:32:33.302883 16876 data_reader.cpp:264] Starting prefetch of epoch 6
I0731 18:32:50.325315 16895 solver.cpp:353] Iteration 6700 (5.40184 iter/s, 18.5122s/100 iter), loss = 0.186273
I0731 18:32:50.325366 16895 solver.cpp:375]     Train net output #0: loss = 0.186273 (* 1 = 0.186273 loss)
I0731 18:32:50.325371 16895 sgd_solver.cpp:136] Iteration 6700, lr = 0.0001, m = 0.9
I0731 18:33:08.941462 16895 solver.cpp:353] Iteration 6800 (5.37183 iter/s, 18.6156s/100 iter), loss = 0.0856368
I0731 18:33:08.941484 16895 solver.cpp:375]     Train net output #0: loss = 0.0856368 (* 1 = 0.0856368 loss)
I0731 18:33:08.941489 16895 sgd_solver.cpp:136] Iteration 6800, lr = 0.0001, m = 0.9
I0731 18:33:27.711562 16895 solver.cpp:353] Iteration 6900 (5.32777 iter/s, 18.7696s/100 iter), loss = 0.166159
I0731 18:33:27.711625 16895 solver.cpp:375]     Train net output #0: loss = 0.166159 (* 1 = 0.166159 loss)
I0731 18:33:27.711632 16895 sgd_solver.cpp:136] Iteration 6900, lr = 0.0001, m = 0.9
I0731 18:33:34.753293 16876 data_reader.cpp:264] Starting prefetch of epoch 7
I0731 18:33:46.238759 16895 solver.cpp:353] Iteration 7000 (5.39762 iter/s, 18.5267s/100 iter), loss = 0.233967
I0731 18:33:46.238783 16895 solver.cpp:375]     Train net output #0: loss = 0.233967 (* 1 = 0.233967 loss)
I0731 18:33:46.238790 16895 sgd_solver.cpp:136] Iteration 7000, lr = 0.0001, m = 0.9
I0731 18:34:04.842321 16895 solver.cpp:353] Iteration 7100 (5.37546 iter/s, 18.6031s/100 iter), loss = 0.137743
I0731 18:34:04.842429 16895 solver.cpp:375]     Train net output #0: loss = 0.137743 (* 1 = 0.137743 loss)
I0731 18:34:04.842437 16895 sgd_solver.cpp:136] Iteration 7100, lr = 0.0001, m = 0.9
I0731 18:34:05.446573 16899 data_reader.cpp:264] Starting prefetch of epoch 6
I0731 18:34:23.344197 16895 solver.cpp:353] Iteration 7200 (5.40501 iter/s, 18.5014s/100 iter), loss = 0.144289
I0731 18:34:23.344223 16895 solver.cpp:375]     Train net output #0: loss = 0.144289 (* 1 = 0.144289 loss)
I0731 18:34:23.344226 16895 sgd_solver.cpp:136] Iteration 7200, lr = 0.0001, m = 0.9
I0731 18:34:41.824532 16895 solver.cpp:353] Iteration 7300 (5.41131 iter/s, 18.4798s/100 iter), loss = 0.113652
I0731 18:34:41.824582 16895 solver.cpp:375]     Train net output #0: loss = 0.113651 (* 1 = 0.113651 loss)
I0731 18:34:41.824589 16895 sgd_solver.cpp:136] Iteration 7300, lr = 0.0001, m = 0.9
I0731 18:35:00.477684 16895 solver.cpp:353] Iteration 7400 (5.36117 iter/s, 18.6526s/100 iter), loss = 0.211173
I0731 18:35:00.477710 16895 solver.cpp:375]     Train net output #0: loss = 0.211173 (* 1 = 0.211173 loss)
I0731 18:35:00.477713 16895 sgd_solver.cpp:136] Iteration 7400, lr = 0.0001, m = 0.9
I0731 18:35:06.754748 16874 data_reader.cpp:264] Starting prefetch of epoch 5
I0731 18:35:19.013409 16895 solver.cpp:353] Iteration 7500 (5.39513 iter/s, 18.5352s/100 iter), loss = 0.151271
I0731 18:35:19.013460 16895 solver.cpp:375]     Train net output #0: loss = 0.15127 (* 1 = 0.15127 loss)
I0731 18:35:19.013465 16895 sgd_solver.cpp:136] Iteration 7500, lr = 0.0001, m = 0.9
I0731 18:35:37.643838 16895 solver.cpp:353] Iteration 7600 (5.36771 iter/s, 18.6299s/100 iter), loss = 0.096497
I0731 18:35:37.643863 16895 solver.cpp:375]     Train net output #0: loss = 0.0964969 (* 1 = 0.0964969 loss)
I0731 18:35:37.643867 16895 sgd_solver.cpp:136] Iteration 7600, lr = 0.0001, m = 0.9
I0731 18:35:56.369447 16895 solver.cpp:353] Iteration 7700 (5.34043 iter/s, 18.7251s/100 iter), loss = 0.0841814
I0731 18:35:56.369500 16895 solver.cpp:375]     Train net output #0: loss = 0.0841812 (* 1 = 0.0841812 loss)
I0731 18:35:56.369505 16895 sgd_solver.cpp:136] Iteration 7700, lr = 0.0001, m = 0.9
I0731 18:36:08.421048 16903 data_reader.cpp:264] Starting prefetch of epoch 6
I0731 18:36:15.065824 16895 solver.cpp:353] Iteration 7800 (5.34878 iter/s, 18.6959s/100 iter), loss = 0.2411
I0731 18:36:15.065847 16895 solver.cpp:375]     Train net output #0: loss = 0.2411 (* 1 = 0.2411 loss)
I0731 18:36:15.065851 16895 sgd_solver.cpp:136] Iteration 7800, lr = 0.0001, m = 0.9
I0731 18:36:33.627349 16895 solver.cpp:353] Iteration 7900 (5.38764 iter/s, 18.561s/100 iter), loss = 0.12066
I0731 18:36:33.627434 16895 solver.cpp:375]     Train net output #0: loss = 0.12066 (* 1 = 0.12066 loss)
I0731 18:36:33.627440 16895 sgd_solver.cpp:136] Iteration 7900, lr = 0.0001, m = 0.9
I0731 18:36:39.258896 16874 data_reader.cpp:264] Starting prefetch of epoch 6
I0731 18:36:52.108695 16895 solver.cpp:550] Iteration 8000, Testing net (#0)
I0731 18:37:18.723600 16934 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 18:37:19.112588 16895 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.941687
I0731 18:37:19.112609 16895 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999995
I0731 18:37:19.112615 16895 solver.cpp:635]     Test net output #2: loss = 0.158836 (* 1 = 0.158836 loss)
I0731 18:37:19.112639 16895 solver.cpp:305] [MultiGPU] Tests completed in 27.0032s
I0731 18:37:19.314896 16895 solver.cpp:353] Iteration 8000 (2.18884 iter/s, 45.6863s/100 iter), loss = 0.124591
I0731 18:37:19.314924 16895 solver.cpp:375]     Train net output #0: loss = 0.124591 (* 1 = 0.124591 loss)
I0731 18:37:19.314930 16895 sgd_solver.cpp:136] Iteration 8000, lr = 0.0001, m = 0.9
I0731 18:37:37.936120 16895 solver.cpp:353] Iteration 8100 (5.37037 iter/s, 18.6207s/100 iter), loss = 0.135524
I0731 18:37:37.936141 16895 solver.cpp:375]     Train net output #0: loss = 0.135524 (* 1 = 0.135524 loss)
I0731 18:37:37.936146 16895 sgd_solver.cpp:136] Iteration 8100, lr = 0.0001, m = 0.9
I0731 18:37:56.505499 16895 solver.cpp:353] Iteration 8200 (5.38536 iter/s, 18.5689s/100 iter), loss = 0.0984445
I0731 18:37:56.505626 16895 solver.cpp:375]     Train net output #0: loss = 0.0984443 (* 1 = 0.0984443 loss)
I0731 18:37:56.505632 16895 sgd_solver.cpp:136] Iteration 8200, lr = 0.0001, m = 0.9
I0731 18:38:07.680297 16901 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 18:38:15.044185 16895 solver.cpp:353] Iteration 8300 (5.39428 iter/s, 18.5382s/100 iter), loss = 0.289801
I0731 18:38:15.044209 16895 solver.cpp:375]     Train net output #0: loss = 0.289801 (* 1 = 0.289801 loss)
I0731 18:38:15.044212 16895 sgd_solver.cpp:136] Iteration 8300, lr = 0.0001, m = 0.9
I0731 18:38:33.636068 16895 solver.cpp:353] Iteration 8400 (5.37884 iter/s, 18.5914s/100 iter), loss = 0.115228
I0731 18:38:33.636126 16895 solver.cpp:375]     Train net output #0: loss = 0.115227 (* 1 = 0.115227 loss)
I0731 18:38:33.636132 16895 sgd_solver.cpp:136] Iteration 8400, lr = 0.0001, m = 0.9
I0731 18:38:52.243659 16895 solver.cpp:353] Iteration 8500 (5.3743 iter/s, 18.6071s/100 iter), loss = 0.101134
I0731 18:38:52.243685 16895 solver.cpp:375]     Train net output #0: loss = 0.101134 (* 1 = 0.101134 loss)
I0731 18:38:52.243690 16895 sgd_solver.cpp:136] Iteration 8500, lr = 0.0001, m = 0.9
I0731 18:39:09.168807 16903 data_reader.cpp:264] Starting prefetch of epoch 7
I0731 18:39:10.830551 16895 solver.cpp:353] Iteration 8600 (5.38028 iter/s, 18.5864s/100 iter), loss = 0.170582
I0731 18:39:10.830576 16895 solver.cpp:375]     Train net output #0: loss = 0.170581 (* 1 = 0.170581 loss)
I0731 18:39:10.830580 16895 sgd_solver.cpp:136] Iteration 8600, lr = 0.0001, m = 0.9
I0731 18:39:29.348841 16895 solver.cpp:353] Iteration 8700 (5.40022 iter/s, 18.5178s/100 iter), loss = 0.101873
I0731 18:39:29.348865 16895 solver.cpp:375]     Train net output #0: loss = 0.101873 (* 1 = 0.101873 loss)
I0731 18:39:29.348870 16895 sgd_solver.cpp:136] Iteration 8700, lr = 0.0001, m = 0.9
I0731 18:39:39.760704 16874 data_reader.cpp:264] Starting prefetch of epoch 7
I0731 18:39:47.833278 16895 solver.cpp:353] Iteration 8800 (5.41011 iter/s, 18.4839s/100 iter), loss = 0.0978431
I0731 18:39:47.833304 16895 solver.cpp:375]     Train net output #0: loss = 0.0978429 (* 1 = 0.0978429 loss)
I0731 18:39:47.833308 16895 sgd_solver.cpp:136] Iteration 8800, lr = 0.0001, m = 0.9
I0731 18:40:06.491861 16895 solver.cpp:353] Iteration 8900 (5.35961 iter/s, 18.6581s/100 iter), loss = 0.134101
I0731 18:40:06.491884 16895 solver.cpp:375]     Train net output #0: loss = 0.134101 (* 1 = 0.134101 loss)
I0731 18:40:06.491888 16895 sgd_solver.cpp:136] Iteration 8900, lr = 0.0001, m = 0.9
I0731 18:40:25.197238 16895 solver.cpp:353] Iteration 9000 (5.3462 iter/s, 18.7049s/100 iter), loss = 0.0990635
I0731 18:40:25.197285 16895 solver.cpp:375]     Train net output #0: loss = 0.0990633 (* 1 = 0.0990633 loss)
I0731 18:40:25.197293 16895 sgd_solver.cpp:136] Iteration 9000, lr = 0.0001, m = 0.9
I0731 18:40:41.305459 16903 data_reader.cpp:264] Starting prefetch of epoch 8
I0731 18:40:43.687042 16895 solver.cpp:353] Iteration 9100 (5.40854 iter/s, 18.4893s/100 iter), loss = 0.191978
I0731 18:40:43.687068 16895 solver.cpp:375]     Train net output #0: loss = 0.191978 (* 1 = 0.191978 loss)
I0731 18:40:43.687074 16895 sgd_solver.cpp:136] Iteration 9100, lr = 0.0001, m = 0.9
I0731 18:41:02.346854 16895 solver.cpp:353] Iteration 9200 (5.35926 iter/s, 18.6593s/100 iter), loss = 0.115555
I0731 18:41:02.346930 16895 solver.cpp:375]     Train net output #0: loss = 0.115555 (* 1 = 0.115555 loss)
I0731 18:41:02.346935 16895 sgd_solver.cpp:136] Iteration 9200, lr = 0.0001, m = 0.9
I0731 18:41:20.873234 16895 solver.cpp:353] Iteration 9300 (5.39786 iter/s, 18.5259s/100 iter), loss = 0.127822
I0731 18:41:20.873260 16895 solver.cpp:375]     Train net output #0: loss = 0.127822 (* 1 = 0.127822 loss)
I0731 18:41:20.873265 16895 sgd_solver.cpp:136] Iteration 9300, lr = 0.0001, m = 0.9
I0731 18:41:39.542906 16895 solver.cpp:353] Iteration 9400 (5.35643 iter/s, 18.6692s/100 iter), loss = 0.142559
I0731 18:41:39.543009 16895 solver.cpp:375]     Train net output #0: loss = 0.142559 (* 1 = 0.142559 loss)
I0731 18:41:39.543015 16895 sgd_solver.cpp:136] Iteration 9400, lr = 0.0001, m = 0.9
I0731 18:41:42.746332 16901 data_reader.cpp:264] Starting prefetch of epoch 4
I0731 18:41:58.168244 16895 solver.cpp:353] Iteration 9500 (5.36918 iter/s, 18.6248s/100 iter), loss = 0.118447
I0731 18:41:58.168274 16895 solver.cpp:375]     Train net output #0: loss = 0.118447 (* 1 = 0.118447 loss)
I0731 18:41:58.168279 16895 sgd_solver.cpp:136] Iteration 9500, lr = 0.0001, m = 0.9
I0731 18:42:13.759500 16899 data_reader.cpp:264] Starting prefetch of epoch 7
I0731 18:42:16.864542 16895 solver.cpp:353] Iteration 9600 (5.3488 iter/s, 18.6958s/100 iter), loss = 0.111096
I0731 18:42:16.864567 16895 solver.cpp:375]     Train net output #0: loss = 0.111095 (* 1 = 0.111095 loss)
I0731 18:42:16.864573 16895 sgd_solver.cpp:136] Iteration 9600, lr = 0.0001, m = 0.9
I0731 18:42:35.438446 16895 solver.cpp:353] Iteration 9700 (5.38405 iter/s, 18.5734s/100 iter), loss = 0.106962
I0731 18:42:35.438472 16895 solver.cpp:375]     Train net output #0: loss = 0.106962 (* 1 = 0.106962 loss)
I0731 18:42:35.438477 16895 sgd_solver.cpp:136] Iteration 9700, lr = 0.0001, m = 0.9
I0731 18:42:53.994302 16895 solver.cpp:353] Iteration 9800 (5.38928 iter/s, 18.5553s/100 iter), loss = 0.109069
I0731 18:42:53.994359 16895 solver.cpp:375]     Train net output #0: loss = 0.109069 (* 1 = 0.109069 loss)
I0731 18:42:53.994364 16895 sgd_solver.cpp:136] Iteration 9800, lr = 0.0001, m = 0.9
I0731 18:43:12.514705 16895 solver.cpp:353] Iteration 9900 (5.3996 iter/s, 18.5199s/100 iter), loss = 0.199178
I0731 18:43:12.514729 16895 solver.cpp:375]     Train net output #0: loss = 0.199178 (* 1 = 0.199178 loss)
I0731 18:43:12.514734 16895 sgd_solver.cpp:136] Iteration 9900, lr = 0.0001, m = 0.9
I0731 18:43:15.013370 16874 data_reader.cpp:264] Starting prefetch of epoch 8
I0731 18:43:30.939188 16895 solver.cpp:680] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/initial/cityscapes5_jsegnet21v2_iter_10000.caffemodel
I0731 18:43:31.016254 16895 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/initial/cityscapes5_jsegnet21v2_iter_10000.solverstate
I0731 18:43:31.024690 16895 solver.cpp:550] Iteration 10000, Testing net (#0)
I0731 18:43:43.513289 16895 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.946047
I0731 18:43:43.513309 16895 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999994
I0731 18:43:43.513315 16895 solver.cpp:635]     Test net output #2: loss = 0.155756 (* 1 = 0.155756 loss)
I0731 18:43:43.513396 16895 solver.cpp:305] [MultiGPU] Tests completed in 12.4884s
I0731 18:43:43.717808 16895 solver.cpp:353] Iteration 10000 (3.2049 iter/s, 31.2022s/100 iter), loss = 0.0913581
I0731 18:43:43.717834 16895 solver.cpp:375]     Train net output #0: loss = 0.0913579 (* 1 = 0.0913579 loss)
I0731 18:43:43.717839 16895 sgd_solver.cpp:136] Iteration 10000, lr = 0.0001, m = 0.9
I0731 18:43:58.303889 16876 data_reader.cpp:264] Starting prefetch of epoch 8
I0731 18:44:02.418900 16895 solver.cpp:353] Iteration 10100 (5.34743 iter/s, 18.7006s/100 iter), loss = 0.0956916
I0731 18:44:02.418948 16895 solver.cpp:375]     Train net output #0: loss = 0.0956914 (* 1 = 0.0956914 loss)
I0731 18:44:02.418953 16895 sgd_solver.cpp:136] Iteration 10100, lr = 0.0001, m = 0.9
I0731 18:44:20.936651 16895 solver.cpp:353] Iteration 10200 (5.40037 iter/s, 18.5172s/100 iter), loss = 0.31475
I0731 18:44:20.936676 16895 solver.cpp:375]     Train net output #0: loss = 0.31475 (* 1 = 0.31475 loss)
I0731 18:44:20.936681 16895 sgd_solver.cpp:136] Iteration 10200, lr = 0.0001, m = 0.9
I0731 18:44:29.105712 16899 data_reader.cpp:264] Starting prefetch of epoch 8
I0731 18:44:39.578400 16895 solver.cpp:353] Iteration 10300 (5.36445 iter/s, 18.6412s/100 iter), loss = 0.0960819
I0731 18:44:39.578521 16895 solver.cpp:375]     Train net output #0: loss = 0.0960817 (* 1 = 0.0960817 loss)
I0731 18:44:39.578527 16895 sgd_solver.cpp:136] Iteration 10300, lr = 0.0001, m = 0.9
I0731 18:44:58.212108 16895 solver.cpp:353] Iteration 10400 (5.36677 iter/s, 18.6332s/100 iter), loss = 0.0793995
I0731 18:44:58.212132 16895 solver.cpp:375]     Train net output #0: loss = 0.0793993 (* 1 = 0.0793993 loss)
I0731 18:44:58.212136 16895 sgd_solver.cpp:136] Iteration 10400, lr = 0.0001, m = 0.9
I0731 18:45:17.003475 16895 solver.cpp:353] Iteration 10500 (5.32174 iter/s, 18.7908s/100 iter), loss = 0.0850744
I0731 18:45:17.003559 16895 solver.cpp:375]     Train net output #0: loss = 0.0850742 (* 1 = 0.0850742 loss)
I0731 18:45:17.003567 16895 sgd_solver.cpp:136] Iteration 10500, lr = 0.0001, m = 0.9
I0731 18:45:30.776718 16874 data_reader.cpp:264] Starting prefetch of epoch 9
I0731 18:45:35.592051 16895 solver.cpp:353] Iteration 10600 (5.3798 iter/s, 18.5881s/100 iter), loss = 0.148572
I0731 18:45:35.592073 16895 solver.cpp:375]     Train net output #0: loss = 0.148572 (* 1 = 0.148572 loss)
I0731 18:45:35.592077 16895 sgd_solver.cpp:136] Iteration 10600, lr = 0.0001, m = 0.9
I0731 18:45:54.076863 16895 solver.cpp:353] Iteration 10700 (5.41 iter/s, 18.4843s/100 iter), loss = 0.0883492
I0731 18:45:54.076911 16895 solver.cpp:375]     Train net output #0: loss = 0.088349 (* 1 = 0.088349 loss)
I0731 18:45:54.076916 16895 sgd_solver.cpp:136] Iteration 10700, lr = 0.0001, m = 0.9
I0731 18:46:12.548568 16895 solver.cpp:353] Iteration 10800 (5.41383 iter/s, 18.4712s/100 iter), loss = 0.165894
I0731 18:46:12.548591 16895 solver.cpp:375]     Train net output #0: loss = 0.165894 (* 1 = 0.165894 loss)
I0731 18:46:12.548595 16895 sgd_solver.cpp:136] Iteration 10800, lr = 0.0001, m = 0.9
I0731 18:46:31.179129 16895 solver.cpp:353] Iteration 10900 (5.36767 iter/s, 18.63s/100 iter), loss = 0.0659118
I0731 18:46:31.179553 16895 solver.cpp:375]     Train net output #0: loss = 0.0659116 (* 1 = 0.0659116 loss)
I0731 18:46:31.179561 16895 sgd_solver.cpp:136] Iteration 10900, lr = 0.0001, m = 0.9
I0731 18:46:32.119209 16903 data_reader.cpp:264] Starting prefetch of epoch 9
I0731 18:46:49.799329 16895 solver.cpp:353] Iteration 11000 (5.37066 iter/s, 18.6197s/100 iter), loss = 0.109232
I0731 18:46:49.799353 16895 solver.cpp:375]     Train net output #0: loss = 0.109232 (* 1 = 0.109232 loss)
I0731 18:46:49.799358 16895 sgd_solver.cpp:136] Iteration 11000, lr = 0.0001, m = 0.9
I0731 18:47:02.854945 16899 data_reader.cpp:264] Starting prefetch of epoch 9
I0731 18:47:08.358635 16895 solver.cpp:353] Iteration 11100 (5.38828 iter/s, 18.5588s/100 iter), loss = 0.122437
I0731 18:47:08.358659 16895 solver.cpp:375]     Train net output #0: loss = 0.122437 (* 1 = 0.122437 loss)
I0731 18:47:08.358664 16895 sgd_solver.cpp:136] Iteration 11100, lr = 0.0001, m = 0.9
I0731 18:47:27.013348 16895 solver.cpp:353] Iteration 11200 (5.36072 iter/s, 18.6542s/100 iter), loss = 0.0968579
I0731 18:47:27.013373 16895 solver.cpp:375]     Train net output #0: loss = 0.0968577 (* 1 = 0.0968577 loss)
I0731 18:47:27.013380 16895 sgd_solver.cpp:136] Iteration 11200, lr = 0.0001, m = 0.9
I0731 18:47:45.667250 16895 solver.cpp:353] Iteration 11300 (5.36096 iter/s, 18.6534s/100 iter), loss = 0.10304
I0731 18:47:45.667302 16895 solver.cpp:375]     Train net output #0: loss = 0.10304 (* 1 = 0.10304 loss)
I0731 18:47:45.667309 16895 sgd_solver.cpp:136] Iteration 11300, lr = 0.0001, m = 0.9
I0731 18:48:04.179816 16895 solver.cpp:353] Iteration 11400 (5.40189 iter/s, 18.5121s/100 iter), loss = 0.0817959
I0731 18:48:04.179841 16895 solver.cpp:375]     Train net output #0: loss = 0.0817956 (* 1 = 0.0817956 loss)
I0731 18:48:04.179847 16895 sgd_solver.cpp:136] Iteration 11400, lr = 0.0001, m = 0.9
I0731 18:48:04.391109 16898 data_reader.cpp:264] Starting prefetch of epoch 4
I0731 18:48:22.842214 16895 solver.cpp:353] Iteration 11500 (5.35852 iter/s, 18.6619s/100 iter), loss = 0.139584
I0731 18:48:22.842308 16895 solver.cpp:375]     Train net output #0: loss = 0.139584 (* 1 = 0.139584 loss)
I0731 18:48:22.842315 16895 sgd_solver.cpp:136] Iteration 11500, lr = 0.0001, m = 0.9
I0731 18:48:41.477908 16895 solver.cpp:353] Iteration 11600 (5.36619 iter/s, 18.6352s/100 iter), loss = 0.145391
I0731 18:48:41.477931 16895 solver.cpp:375]     Train net output #0: loss = 0.145391 (* 1 = 0.145391 loss)
I0731 18:48:41.477936 16895 sgd_solver.cpp:136] Iteration 11600, lr = 0.0001, m = 0.9
I0731 18:48:59.908741 16895 solver.cpp:353] Iteration 11700 (5.42584 iter/s, 18.4303s/100 iter), loss = 0.0575542
I0731 18:48:59.908849 16895 solver.cpp:375]     Train net output #0: loss = 0.0575541 (* 1 = 0.0575541 loss)
I0731 18:48:59.908856 16895 sgd_solver.cpp:136] Iteration 11700, lr = 0.0001, m = 0.9
I0731 18:49:05.654397 16876 data_reader.cpp:264] Starting prefetch of epoch 9
I0731 18:49:18.379835 16895 solver.cpp:353] Iteration 11800 (5.41401 iter/s, 18.4706s/100 iter), loss = 0.133147
I0731 18:49:18.379859 16895 solver.cpp:375]     Train net output #0: loss = 0.133147 (* 1 = 0.133147 loss)
I0731 18:49:18.379863 16895 sgd_solver.cpp:136] Iteration 11800, lr = 0.0001, m = 0.9
I0731 18:49:37.030122 16895 solver.cpp:353] Iteration 11900 (5.362 iter/s, 18.6498s/100 iter), loss = 0.108828
I0731 18:49:37.030227 16895 solver.cpp:375]     Train net output #0: loss = 0.108828 (* 1 = 0.108828 loss)
I0731 18:49:37.030234 16895 sgd_solver.cpp:136] Iteration 11900, lr = 0.0001, m = 0.9
I0731 18:49:55.382186 16895 solver.cpp:550] Iteration 12000, Testing net (#0)
I0731 18:49:59.247328 16893 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 18:50:11.079319 16934 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 18:50:11.079319 16938 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 18:50:11.559463 16895 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.94416
I0731 18:50:11.559484 16895 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999996
I0731 18:50:11.559490 16895 solver.cpp:635]     Test net output #2: loss = 0.153105 (* 1 = 0.153105 loss)
I0731 18:50:11.559520 16895 solver.cpp:305] [MultiGPU] Tests completed in 16.1769s
I0731 18:50:11.754133 16895 solver.cpp:353] Iteration 12000 (2.87993 iter/s, 34.723s/100 iter), loss = 0.0631996
I0731 18:50:11.754179 16895 solver.cpp:375]     Train net output #0: loss = 0.0631995 (* 1 = 0.0631995 loss)
I0731 18:50:11.754189 16895 sgd_solver.cpp:136] Iteration 12000, lr = 0.0001, m = 0.9
I0731 18:50:30.423099 16895 solver.cpp:353] Iteration 12100 (5.35663 iter/s, 18.6684s/100 iter), loss = 0.139045
I0731 18:50:30.423121 16895 solver.cpp:375]     Train net output #0: loss = 0.139045 (* 1 = 0.139045 loss)
I0731 18:50:30.423125 16895 sgd_solver.cpp:136] Iteration 12100, lr = 0.0001, m = 0.9
I0731 18:50:48.974345 16895 solver.cpp:353] Iteration 12200 (5.39062 iter/s, 18.5507s/100 iter), loss = 0.132657
I0731 18:50:48.974390 16895 solver.cpp:375]     Train net output #0: loss = 0.132657 (* 1 = 0.132657 loss)
I0731 18:50:48.974397 16895 sgd_solver.cpp:136] Iteration 12200, lr = 0.0001, m = 0.9
I0731 18:50:53.991716 16898 data_reader.cpp:264] Starting prefetch of epoch 5
I0731 18:51:07.577587 16895 solver.cpp:353] Iteration 12300 (5.37556 iter/s, 18.6027s/100 iter), loss = 0.09952
I0731 18:51:07.577613 16895 solver.cpp:375]     Train net output #0: loss = 0.0995199 (* 1 = 0.0995199 loss)
I0731 18:51:07.577618 16895 sgd_solver.cpp:136] Iteration 12300, lr = 0.0001, m = 0.9
I0731 18:51:26.004676 16895 solver.cpp:353] Iteration 12400 (5.42694 iter/s, 18.4266s/100 iter), loss = 0.0791331
I0731 18:51:26.004776 16895 solver.cpp:375]     Train net output #0: loss = 0.079133 (* 1 = 0.079133 loss)
I0731 18:51:26.004783 16895 sgd_solver.cpp:136] Iteration 12400, lr = 0.0001, m = 0.9
I0731 18:51:44.428256 16895 solver.cpp:353] Iteration 12500 (5.42798 iter/s, 18.4231s/100 iter), loss = 0.093474
I0731 18:51:44.428282 16895 solver.cpp:375]     Train net output #0: loss = 0.0934739 (* 1 = 0.0934739 loss)
I0731 18:51:44.428285 16895 sgd_solver.cpp:136] Iteration 12500, lr = 0.0001, m = 0.9
I0731 18:51:55.243103 16901 data_reader.cpp:264] Starting prefetch of epoch 5
I0731 18:52:03.042153 16895 solver.cpp:353] Iteration 12600 (5.37248 iter/s, 18.6134s/100 iter), loss = 0.119121
I0731 18:52:03.042242 16895 solver.cpp:375]     Train net output #0: loss = 0.119121 (* 1 = 0.119121 loss)
I0731 18:52:03.042250 16895 sgd_solver.cpp:136] Iteration 12600, lr = 0.0001, m = 0.9
I0731 18:52:21.614773 16895 solver.cpp:353] Iteration 12700 (5.38442 iter/s, 18.5721s/100 iter), loss = 0.247917
I0731 18:52:21.614796 16895 solver.cpp:375]     Train net output #0: loss = 0.247917 (* 1 = 0.247917 loss)
I0731 18:52:21.614800 16895 sgd_solver.cpp:136] Iteration 12700, lr = 0.0001, m = 0.9
I0731 18:52:25.965541 16898 data_reader.cpp:264] Starting prefetch of epoch 6
I0731 18:52:40.149981 16895 solver.cpp:353] Iteration 12800 (5.39529 iter/s, 18.5347s/100 iter), loss = 0.0774385
I0731 18:52:40.150027 16895 solver.cpp:375]     Train net output #0: loss = 0.0774383 (* 1 = 0.0774383 loss)
I0731 18:52:40.150033 16895 sgd_solver.cpp:136] Iteration 12800, lr = 0.0001, m = 0.9
I0731 18:52:58.714998 16895 solver.cpp:353] Iteration 12900 (5.38662 iter/s, 18.5645s/100 iter), loss = 0.103083
I0731 18:52:58.715023 16895 solver.cpp:375]     Train net output #0: loss = 0.103083 (* 1 = 0.103083 loss)
I0731 18:52:58.715026 16895 sgd_solver.cpp:136] Iteration 12900, lr = 0.0001, m = 0.9
I0731 18:53:17.233166 16895 solver.cpp:353] Iteration 13000 (5.40025 iter/s, 18.5177s/100 iter), loss = 0.0824777
I0731 18:53:17.233268 16895 solver.cpp:375]     Train net output #0: loss = 0.0824775 (* 1 = 0.0824775 loss)
I0731 18:53:17.233274 16895 sgd_solver.cpp:136] Iteration 13000, lr = 0.0001, m = 0.9
I0731 18:53:27.132706 16898 data_reader.cpp:264] Starting prefetch of epoch 7
I0731 18:53:35.848454 16895 solver.cpp:353] Iteration 13100 (5.37208 iter/s, 18.6148s/100 iter), loss = 0.185881
I0731 18:53:35.848479 16895 solver.cpp:375]     Train net output #0: loss = 0.18588 (* 1 = 0.18588 loss)
I0731 18:53:35.848484 16895 sgd_solver.cpp:136] Iteration 13100, lr = 0.0001, m = 0.9
I0731 18:53:54.376802 16895 solver.cpp:353] Iteration 13200 (5.39728 iter/s, 18.5278s/100 iter), loss = 0.0818811
I0731 18:53:54.376857 16895 solver.cpp:375]     Train net output #0: loss = 0.0818809 (* 1 = 0.0818809 loss)
I0731 18:53:54.376864 16895 sgd_solver.cpp:136] Iteration 13200, lr = 0.0001, m = 0.9
I0731 18:54:12.920202 16895 solver.cpp:353] Iteration 13300 (5.3929 iter/s, 18.5429s/100 iter), loss = 0.0860399
I0731 18:54:12.920228 16895 solver.cpp:375]     Train net output #0: loss = 0.0860397 (* 1 = 0.0860397 loss)
I0731 18:54:12.920231 16895 sgd_solver.cpp:136] Iteration 13300, lr = 0.0001, m = 0.9
I0731 18:54:28.543704 16901 data_reader.cpp:264] Starting prefetch of epoch 6
I0731 18:54:31.544867 16895 solver.cpp:353] Iteration 13400 (5.36937 iter/s, 18.6242s/100 iter), loss = 0.083717
I0731 18:54:31.544895 16895 solver.cpp:375]     Train net output #0: loss = 0.0837168 (* 1 = 0.0837168 loss)
I0731 18:54:31.544903 16895 sgd_solver.cpp:136] Iteration 13400, lr = 0.0001, m = 0.9
I0731 18:54:50.086377 16895 solver.cpp:353] Iteration 13500 (5.39345 iter/s, 18.541s/100 iter), loss = 0.133579
I0731 18:54:50.086428 16895 solver.cpp:375]     Train net output #0: loss = 0.133579 (* 1 = 0.133579 loss)
I0731 18:54:50.086441 16895 sgd_solver.cpp:136] Iteration 13500, lr = 0.0001, m = 0.9
I0731 18:54:59.167177 16898 data_reader.cpp:264] Starting prefetch of epoch 8
I0731 18:55:08.550746 16895 solver.cpp:353] Iteration 13600 (5.41598 iter/s, 18.4639s/100 iter), loss = 0.113525
I0731 18:55:08.550770 16895 solver.cpp:375]     Train net output #0: loss = 0.113525 (* 1 = 0.113525 loss)
I0731 18:55:08.550773 16895 sgd_solver.cpp:136] Iteration 13600, lr = 0.0001, m = 0.9
I0731 18:55:27.154137 16895 solver.cpp:353] Iteration 13700 (5.37551 iter/s, 18.6029s/100 iter), loss = 0.0830864
I0731 18:55:27.154161 16895 solver.cpp:375]     Train net output #0: loss = 0.0830862 (* 1 = 0.0830862 loss)
I0731 18:55:27.154166 16895 sgd_solver.cpp:136] Iteration 13700, lr = 0.0001, m = 0.9
I0731 18:55:45.673655 16895 solver.cpp:353] Iteration 13800 (5.39986 iter/s, 18.519s/100 iter), loss = 0.103585
I0731 18:55:45.673771 16895 solver.cpp:375]     Train net output #0: loss = 0.103585 (* 1 = 0.103585 loss)
I0731 18:55:45.673779 16895 sgd_solver.cpp:136] Iteration 13800, lr = 0.0001, m = 0.9
I0731 18:56:00.570564 16874 data_reader.cpp:264] Starting prefetch of epoch 10
I0731 18:56:04.204515 16895 solver.cpp:353] Iteration 13900 (5.39655 iter/s, 18.5304s/100 iter), loss = 0.109163
I0731 18:56:04.204540 16895 solver.cpp:375]     Train net output #0: loss = 0.109163 (* 1 = 0.109163 loss)
I0731 18:56:04.204545 16895 sgd_solver.cpp:136] Iteration 13900, lr = 0.0001, m = 0.9
I0731 18:56:22.544787 16895 solver.cpp:550] Iteration 14000, Testing net (#0)
I0731 18:56:33.837448 16895 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.947747
I0731 18:56:33.837472 16895 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999986
I0731 18:56:33.837477 16895 solver.cpp:635]     Test net output #2: loss = 0.155834 (* 1 = 0.155834 loss)
I0731 18:56:33.837581 16895 solver.cpp:305] [MultiGPU] Tests completed in 11.2925s
I0731 18:56:34.027556 16895 solver.cpp:353] Iteration 14000 (3.3532 iter/s, 29.8222s/100 iter), loss = 0.134754
I0731 18:56:34.027582 16895 solver.cpp:375]     Train net output #0: loss = 0.134754 (* 1 = 0.134754 loss)
I0731 18:56:34.027590 16895 sgd_solver.cpp:136] Iteration 14000, lr = 0.0001, m = 0.9
I0731 18:56:42.423758 16898 data_reader.cpp:264] Starting prefetch of epoch 9
I0731 18:56:52.683917 16895 solver.cpp:353] Iteration 14100 (5.36025 iter/s, 18.6558s/100 iter), loss = 0.146534
I0731 18:56:52.683969 16895 solver.cpp:375]     Train net output #0: loss = 0.146534 (* 1 = 0.146534 loss)
I0731 18:56:52.683975 16895 sgd_solver.cpp:136] Iteration 14100, lr = 0.0001, m = 0.9
I0731 18:57:11.207749 16895 solver.cpp:353] Iteration 14200 (5.3986 iter/s, 18.5233s/100 iter), loss = 0.0979127
I0731 18:57:11.207778 16895 solver.cpp:375]     Train net output #0: loss = 0.0979126 (* 1 = 0.0979126 loss)
I0731 18:57:11.207785 16895 sgd_solver.cpp:136] Iteration 14200, lr = 0.0001, m = 0.9
I0731 18:57:29.884889 16895 solver.cpp:353] Iteration 14300 (5.35429 iter/s, 18.6766s/100 iter), loss = 0.162341
I0731 18:57:29.884999 16895 solver.cpp:375]     Train net output #0: loss = 0.162341 (* 1 = 0.162341 loss)
I0731 18:57:29.885006 16895 sgd_solver.cpp:136] Iteration 14300, lr = 0.0001, m = 0.9
I0731 18:57:43.944159 16901 data_reader.cpp:264] Starting prefetch of epoch 7
I0731 18:57:48.456812 16895 solver.cpp:353] Iteration 14400 (5.38462 iter/s, 18.5714s/100 iter), loss = 0.103589
I0731 18:57:48.456843 16895 solver.cpp:375]     Train net output #0: loss = 0.103589 (* 1 = 0.103589 loss)
I0731 18:57:48.456847 16895 sgd_solver.cpp:136] Iteration 14400, lr = 0.0001, m = 0.9
I0731 18:58:07.050645 16895 solver.cpp:353] Iteration 14500 (5.37828 iter/s, 18.5933s/100 iter), loss = 0.106696
I0731 18:58:07.050693 16895 solver.cpp:375]     Train net output #0: loss = 0.106696 (* 1 = 0.106696 loss)
I0731 18:58:07.050698 16895 sgd_solver.cpp:136] Iteration 14500, lr = 0.0001, m = 0.9
I0731 18:58:25.612597 16895 solver.cpp:353] Iteration 14600 (5.38751 iter/s, 18.5614s/100 iter), loss = 0.0996438
I0731 18:58:25.612622 16895 solver.cpp:375]     Train net output #0: loss = 0.0996437 (* 1 = 0.0996437 loss)
I0731 18:58:25.612627 16895 sgd_solver.cpp:136] Iteration 14600, lr = 0.0001, m = 0.9
I0731 18:58:44.105226 16895 solver.cpp:353] Iteration 14700 (5.40771 iter/s, 18.4921s/100 iter), loss = 0.180351
I0731 18:58:44.105332 16895 solver.cpp:375]     Train net output #0: loss = 0.180351 (* 1 = 0.180351 loss)
I0731 18:58:44.105340 16895 sgd_solver.cpp:136] Iteration 14700, lr = 0.0001, m = 0.9
I0731 18:58:45.227049 16876 data_reader.cpp:264] Starting prefetch of epoch 10
I0731 18:59:02.767776 16895 solver.cpp:353] Iteration 14800 (5.35847 iter/s, 18.662s/100 iter), loss = 0.0995206
I0731 18:59:02.767801 16895 solver.cpp:375]     Train net output #0: loss = 0.0995205 (* 1 = 0.0995205 loss)
I0731 18:59:02.767805 16895 sgd_solver.cpp:136] Iteration 14800, lr = 0.0001, m = 0.9
I0731 18:59:15.868837 16874 data_reader.cpp:264] Starting prefetch of epoch 11
I0731 18:59:21.177620 16895 solver.cpp:353] Iteration 14900 (5.43203 iter/s, 18.4093s/100 iter), loss = 0.177936
I0731 18:59:21.177649 16895 solver.cpp:375]     Train net output #0: loss = 0.177936 (* 1 = 0.177936 loss)
I0731 18:59:21.177655 16895 sgd_solver.cpp:136] Iteration 14900, lr = 0.0001, m = 0.9
I0731 18:59:39.720043 16895 solver.cpp:353] Iteration 15000 (5.39319 iter/s, 18.5419s/100 iter), loss = 0.0922562
I0731 18:59:39.720072 16895 solver.cpp:375]     Train net output #0: loss = 0.0922561 (* 1 = 0.0922561 loss)
I0731 18:59:39.720078 16895 sgd_solver.cpp:136] Iteration 15000, lr = 0.0001, m = 0.9
I0731 18:59:58.491348 16895 solver.cpp:353] Iteration 15100 (5.32743 iter/s, 18.7708s/100 iter), loss = 0.191082
I0731 18:59:58.491427 16895 solver.cpp:375]     Train net output #0: loss = 0.191082 (* 1 = 0.191082 loss)
I0731 18:59:58.491435 16895 sgd_solver.cpp:136] Iteration 15100, lr = 0.0001, m = 0.9
I0731 19:00:17.120600 16895 solver.cpp:353] Iteration 15200 (5.36805 iter/s, 18.6287s/100 iter), loss = 0.106029
I0731 19:00:17.120628 16895 solver.cpp:375]     Train net output #0: loss = 0.106029 (* 1 = 0.106029 loss)
I0731 19:00:17.120633 16895 sgd_solver.cpp:136] Iteration 15200, lr = 0.0001, m = 0.9
I0731 19:00:17.545142 16901 data_reader.cpp:264] Starting prefetch of epoch 8
I0731 19:00:35.737596 16895 solver.cpp:353] Iteration 15300 (5.37158 iter/s, 18.6165s/100 iter), loss = 0.0486253
I0731 19:00:35.737645 16895 solver.cpp:375]     Train net output #0: loss = 0.0486251 (* 1 = 0.0486251 loss)
I0731 19:00:35.737650 16895 sgd_solver.cpp:136] Iteration 15300, lr = 0.0001, m = 0.9
I0731 19:00:54.333246 16895 solver.cpp:353] Iteration 15400 (5.37775 iter/s, 18.5951s/100 iter), loss = 0.108729
I0731 19:00:54.333272 16895 solver.cpp:375]     Train net output #0: loss = 0.108728 (* 1 = 0.108728 loss)
I0731 19:00:54.333276 16895 sgd_solver.cpp:136] Iteration 15400, lr = 0.0001, m = 0.9
I0731 19:01:12.878336 16895 solver.cpp:353] Iteration 15500 (5.39241 iter/s, 18.5446s/100 iter), loss = 0.0940228
I0731 19:01:12.878388 16895 solver.cpp:375]     Train net output #0: loss = 0.0940227 (* 1 = 0.0940227 loss)
I0731 19:01:12.878393 16895 sgd_solver.cpp:136] Iteration 15500, lr = 0.0001, m = 0.9
I0731 19:01:19.000459 16901 data_reader.cpp:264] Starting prefetch of epoch 9
I0731 19:01:31.452477 16895 solver.cpp:353] Iteration 15600 (5.38398 iter/s, 18.5736s/100 iter), loss = 0.0937304
I0731 19:01:31.452500 16895 solver.cpp:375]     Train net output #0: loss = 0.0937303 (* 1 = 0.0937303 loss)
I0731 19:01:31.452504 16895 sgd_solver.cpp:136] Iteration 15600, lr = 0.0001, m = 0.9
I0731 19:01:49.662520 16898 data_reader.cpp:264] Starting prefetch of epoch 10
I0731 19:01:49.983714 16895 solver.cpp:353] Iteration 15700 (5.39644 iter/s, 18.5307s/100 iter), loss = 0.120774
I0731 19:01:49.983737 16895 solver.cpp:375]     Train net output #0: loss = 0.120773 (* 1 = 0.120773 loss)
I0731 19:01:49.983741 16895 sgd_solver.cpp:136] Iteration 15700, lr = 0.0001, m = 0.9
I0731 19:02:08.564663 16895 solver.cpp:353] Iteration 15800 (5.382 iter/s, 18.5804s/100 iter), loss = 0.110289
I0731 19:02:08.564687 16895 solver.cpp:375]     Train net output #0: loss = 0.110289 (* 1 = 0.110289 loss)
I0731 19:02:08.564692 16895 sgd_solver.cpp:136] Iteration 15800, lr = 0.0001, m = 0.9
I0731 19:02:27.104354 16895 solver.cpp:353] Iteration 15900 (5.39398 iter/s, 18.5392s/100 iter), loss = 0.0698556
I0731 19:02:27.104406 16895 solver.cpp:375]     Train net output #0: loss = 0.0698554 (* 1 = 0.0698554 loss)
I0731 19:02:27.104413 16895 sgd_solver.cpp:136] Iteration 15900, lr = 0.0001, m = 0.9
I0731 19:02:45.498348 16895 solver.cpp:550] Iteration 16000, Testing net (#0)
I0731 19:02:48.844746 16891 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 19:02:56.564635 16895 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.948936
I0731 19:02:56.564658 16895 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999998
I0731 19:02:56.564663 16895 solver.cpp:635]     Test net output #2: loss = 0.149419 (* 1 = 0.149419 loss)
I0731 19:02:56.564744 16895 solver.cpp:305] [MultiGPU] Tests completed in 11.0661s
I0731 19:02:56.767482 16895 solver.cpp:353] Iteration 16000 (3.37128 iter/s, 29.6623s/100 iter), loss = 0.112194
I0731 19:02:56.767504 16895 solver.cpp:375]     Train net output #0: loss = 0.112193 (* 1 = 0.112193 loss)
I0731 19:02:56.767508 16895 sgd_solver.cpp:136] Iteration 16000, lr = 0.0001, m = 0.9
I0731 19:03:01.974112 16903 data_reader.cpp:264] Starting prefetch of epoch 10
I0731 19:03:15.306840 16895 solver.cpp:353] Iteration 16100 (5.39408 iter/s, 18.5388s/100 iter), loss = 0.112619
I0731 19:03:15.306864 16895 solver.cpp:375]     Train net output #0: loss = 0.112619 (* 1 = 0.112619 loss)
I0731 19:03:15.306869 16895 sgd_solver.cpp:136] Iteration 16100, lr = 0.0001, m = 0.9
I0731 19:03:32.799865 16899 data_reader.cpp:264] Starting prefetch of epoch 10
I0731 19:03:33.883098 16895 solver.cpp:353] Iteration 16200 (5.38336 iter/s, 18.5757s/100 iter), loss = 0.155786
I0731 19:03:33.883121 16895 solver.cpp:375]     Train net output #0: loss = 0.155786 (* 1 = 0.155786 loss)
I0731 19:03:33.883124 16895 sgd_solver.cpp:136] Iteration 16200, lr = 0.0001, m = 0.9
I0731 19:03:52.466433 16895 solver.cpp:353] Iteration 16300 (5.38132 iter/s, 18.5828s/100 iter), loss = 0.0932663
I0731 19:03:52.466456 16895 solver.cpp:375]     Train net output #0: loss = 0.0932661 (* 1 = 0.0932661 loss)
I0731 19:03:52.466464 16895 sgd_solver.cpp:136] Iteration 16300, lr = 0.0001, m = 0.9
I0731 19:04:11.143896 16895 solver.cpp:353] Iteration 16400 (5.35419 iter/s, 18.6769s/100 iter), loss = 0.068053
I0731 19:04:11.143946 16895 solver.cpp:375]     Train net output #0: loss = 0.0680528 (* 1 = 0.0680528 loss)
I0731 19:04:11.143954 16895 sgd_solver.cpp:136] Iteration 16400, lr = 0.0001, m = 0.9
I0731 19:04:29.609537 16895 solver.cpp:353] Iteration 16500 (5.41561 iter/s, 18.4651s/100 iter), loss = 0.0902472
I0731 19:04:29.609562 16895 solver.cpp:375]     Train net output #0: loss = 0.090247 (* 1 = 0.090247 loss)
I0731 19:04:29.609568 16895 sgd_solver.cpp:136] Iteration 16500, lr = 0.0001, m = 0.9
I0731 19:04:34.118156 16898 data_reader.cpp:264] Starting prefetch of epoch 11
I0731 19:04:48.239200 16895 solver.cpp:353] Iteration 16600 (5.36793 iter/s, 18.6291s/100 iter), loss = 0.0927396
I0731 19:04:48.239253 16895 solver.cpp:375]     Train net output #0: loss = 0.0927394 (* 1 = 0.0927394 loss)
I0731 19:04:48.239259 16895 sgd_solver.cpp:136] Iteration 16600, lr = 0.0001, m = 0.9
I0731 19:05:06.816862 16895 solver.cpp:353] Iteration 16700 (5.38296 iter/s, 18.5771s/100 iter), loss = 0.282002
I0731 19:05:06.816889 16895 solver.cpp:375]     Train net output #0: loss = 0.282001 (* 1 = 0.282001 loss)
I0731 19:05:06.816892 16895 sgd_solver.cpp:136] Iteration 16700, lr = 0.0001, m = 0.9
I0731 19:05:25.363725 16895 solver.cpp:353] Iteration 16800 (5.3919 iter/s, 18.5464s/100 iter), loss = 0.114452
I0731 19:05:25.363782 16895 solver.cpp:375]     Train net output #0: loss = 0.114451 (* 1 = 0.114451 loss)
I0731 19:05:25.363787 16895 sgd_solver.cpp:136] Iteration 16800, lr = 0.0001, m = 0.9
I0731 19:05:35.601124 16903 data_reader.cpp:264] Starting prefetch of epoch 11
I0731 19:05:43.998019 16895 solver.cpp:353] Iteration 16900 (5.3666 iter/s, 18.6338s/100 iter), loss = 0.097389
I0731 19:05:43.998042 16895 solver.cpp:375]     Train net output #0: loss = 0.0973887 (* 1 = 0.0973887 loss)
I0731 19:05:43.998046 16895 sgd_solver.cpp:136] Iteration 16900, lr = 0.0001, m = 0.9
I0731 19:06:02.658252 16895 solver.cpp:353] Iteration 17000 (5.35914 iter/s, 18.6597s/100 iter), loss = 0.149229
I0731 19:06:02.658316 16895 solver.cpp:375]     Train net output #0: loss = 0.149229 (* 1 = 0.149229 loss)
I0731 19:06:02.658323 16895 sgd_solver.cpp:136] Iteration 17000, lr = 0.0001, m = 0.9
I0731 19:06:06.378216 16898 data_reader.cpp:264] Starting prefetch of epoch 12
I0731 19:06:21.097846 16895 solver.cpp:353] Iteration 17100 (5.42326 iter/s, 18.4391s/100 iter), loss = 0.124452
I0731 19:06:21.097868 16895 solver.cpp:375]     Train net output #0: loss = 0.124452 (* 1 = 0.124452 loss)
I0731 19:06:21.097872 16895 sgd_solver.cpp:136] Iteration 17100, lr = 0.0001, m = 0.9
I0731 19:06:39.578052 16895 solver.cpp:353] Iteration 17200 (5.41135 iter/s, 18.4797s/100 iter), loss = 0.128592
I0731 19:06:39.578105 16895 solver.cpp:375]     Train net output #0: loss = 0.128592 (* 1 = 0.128592 loss)
I0731 19:06:39.578110 16895 sgd_solver.cpp:136] Iteration 17200, lr = 0.0001, m = 0.9
I0731 19:06:58.159934 16895 solver.cpp:353] Iteration 17300 (5.38174 iter/s, 18.5814s/100 iter), loss = 0.0762832
I0731 19:06:58.159958 16895 solver.cpp:375]     Train net output #0: loss = 0.0762829 (* 1 = 0.0762829 loss)
I0731 19:06:58.159965 16895 sgd_solver.cpp:136] Iteration 17300, lr = 0.0001, m = 0.9
I0731 19:07:07.704155 16899 data_reader.cpp:264] Starting prefetch of epoch 11
I0731 19:07:16.661224 16895 solver.cpp:353] Iteration 17400 (5.40518 iter/s, 18.5008s/100 iter), loss = 0.0661483
I0731 19:07:16.661301 16895 solver.cpp:375]     Train net output #0: loss = 0.066148 (* 1 = 0.066148 loss)
I0731 19:07:16.661306 16895 sgd_solver.cpp:136] Iteration 17400, lr = 0.0001, m = 0.9
I0731 19:07:35.313269 16895 solver.cpp:353] Iteration 17500 (5.36149 iter/s, 18.6515s/100 iter), loss = 0.0654078
I0731 19:07:35.313293 16895 solver.cpp:375]     Train net output #0: loss = 0.0654076 (* 1 = 0.0654076 loss)
I0731 19:07:35.313299 16895 sgd_solver.cpp:136] Iteration 17500, lr = 0.0001, m = 0.9
I0731 19:07:53.883342 16895 solver.cpp:353] Iteration 17600 (5.38516 iter/s, 18.5696s/100 iter), loss = 0.184322
I0731 19:07:53.883385 16895 solver.cpp:375]     Train net output #0: loss = 0.184322 (* 1 = 0.184322 loss)
I0731 19:07:53.883390 16895 sgd_solver.cpp:136] Iteration 17600, lr = 0.0001, m = 0.9
I0731 19:08:08.911833 16876 data_reader.cpp:264] Starting prefetch of epoch 11
I0731 19:08:12.406455 16895 solver.cpp:353] Iteration 17700 (5.39881 iter/s, 18.5226s/100 iter), loss = 0.0843731
I0731 19:08:12.406481 16895 solver.cpp:375]     Train net output #0: loss = 0.0843729 (* 1 = 0.0843729 loss)
I0731 19:08:12.406484 16895 sgd_solver.cpp:136] Iteration 17700, lr = 0.0001, m = 0.9
I0731 19:08:30.867286 16895 solver.cpp:353] Iteration 17800 (5.41702 iter/s, 18.4603s/100 iter), loss = 0.082174
I0731 19:08:30.867341 16895 solver.cpp:375]     Train net output #0: loss = 0.0821738 (* 1 = 0.0821738 loss)
I0731 19:08:30.867347 16895 sgd_solver.cpp:136] Iteration 17800, lr = 0.0001, m = 0.9
I0731 19:08:39.359382 16899 data_reader.cpp:264] Starting prefetch of epoch 12
I0731 19:08:49.358778 16895 solver.cpp:353] Iteration 17900 (5.40804 iter/s, 18.491s/100 iter), loss = 0.117314
I0731 19:08:49.358805 16895 solver.cpp:375]     Train net output #0: loss = 0.117314 (* 1 = 0.117314 loss)
I0731 19:08:49.358809 16895 sgd_solver.cpp:136] Iteration 17900, lr = 0.0001, m = 0.9
I0731 19:09:07.735949 16895 solver.cpp:550] Iteration 18000, Testing net (#0)
I0731 19:09:18.811198 16895 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.945321
I0731 19:09:18.811223 16895 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999869
I0731 19:09:18.811228 16895 solver.cpp:635]     Test net output #2: loss = 0.177661 (* 1 = 0.177661 loss)
I0731 19:09:18.811327 16895 solver.cpp:305] [MultiGPU] Tests completed in 11.0751s
I0731 19:09:19.001227 16895 solver.cpp:353] Iteration 18000 (3.37363 iter/s, 29.6416s/100 iter), loss = 0.104226
I0731 19:09:19.001251 16895 solver.cpp:375]     Train net output #0: loss = 0.104225 (* 1 = 0.104225 loss)
I0731 19:09:19.001255 16895 sgd_solver.cpp:136] Iteration 18000, lr = 0.0001, m = 0.9
I0731 19:09:21.276152 16876 data_reader.cpp:264] Starting prefetch of epoch 12
I0731 19:09:37.554942 16895 solver.cpp:353] Iteration 18100 (5.38991 iter/s, 18.5532s/100 iter), loss = 0.108922
I0731 19:09:37.554971 16895 solver.cpp:375]     Train net output #0: loss = 0.108922 (* 1 = 0.108922 loss)
I0731 19:09:37.554978 16895 sgd_solver.cpp:136] Iteration 18100, lr = 0.0001, m = 0.9
I0731 19:09:56.261030 16895 solver.cpp:353] Iteration 18200 (5.346 iter/s, 18.7056s/100 iter), loss = 0.116848
I0731 19:09:56.261096 16895 solver.cpp:375]     Train net output #0: loss = 0.116847 (* 1 = 0.116847 loss)
I0731 19:09:56.261102 16895 sgd_solver.cpp:136] Iteration 18200, lr = 0.0001, m = 0.9
I0731 19:10:14.871625 16895 solver.cpp:353] Iteration 18300 (5.37343 iter/s, 18.6101s/100 iter), loss = 0.137911
I0731 19:10:14.871649 16895 solver.cpp:375]     Train net output #0: loss = 0.137911 (* 1 = 0.137911 loss)
I0731 19:10:14.871652 16895 sgd_solver.cpp:136] Iteration 18300, lr = 0.0001, m = 0.9
I0731 19:10:22.715400 16876 data_reader.cpp:264] Starting prefetch of epoch 13
I0731 19:10:33.448885 16895 solver.cpp:353] Iteration 18400 (5.38307 iter/s, 18.5767s/100 iter), loss = 0.092573
I0731 19:10:33.448938 16895 solver.cpp:375]     Train net output #0: loss = 0.0925729 (* 1 = 0.0925729 loss)
I0731 19:10:33.448943 16895 sgd_solver.cpp:136] Iteration 18400, lr = 0.0001, m = 0.9
I0731 19:10:52.052911 16895 solver.cpp:353] Iteration 18500 (5.37533 iter/s, 18.6035s/100 iter), loss = 0.155035
I0731 19:10:52.052937 16895 solver.cpp:375]     Train net output #0: loss = 0.155035 (* 1 = 0.155035 loss)
I0731 19:10:52.052943 16895 sgd_solver.cpp:136] Iteration 18500, lr = 0.0001, m = 0.9
I0731 19:10:53.543772 16898 data_reader.cpp:264] Starting prefetch of epoch 13
I0731 19:11:10.615413 16895 solver.cpp:353] Iteration 18600 (5.38735 iter/s, 18.562s/100 iter), loss = 0.122815
I0731 19:11:10.615469 16895 solver.cpp:375]     Train net output #0: loss = 0.122815 (* 1 = 0.122815 loss)
I0731 19:11:10.615476 16895 sgd_solver.cpp:136] Iteration 18600, lr = 0.0001, m = 0.9
I0731 19:11:29.223745 16895 solver.cpp:353] Iteration 18700 (5.37408 iter/s, 18.6078s/100 iter), loss = 0.0587228
I0731 19:11:29.223770 16895 solver.cpp:375]     Train net output #0: loss = 0.0587227 (* 1 = 0.0587227 loss)
I0731 19:11:29.223774 16895 sgd_solver.cpp:136] Iteration 18700, lr = 0.0001, m = 0.9
I0731 19:11:47.965447 16895 solver.cpp:353] Iteration 18800 (5.33584 iter/s, 18.7412s/100 iter), loss = 0.245342
I0731 19:11:47.965531 16895 solver.cpp:375]     Train net output #0: loss = 0.245342 (* 1 = 0.245342 loss)
I0731 19:11:47.965538 16895 sgd_solver.cpp:136] Iteration 18800, lr = 0.0001, m = 0.9
I0731 19:11:55.067176 16874 data_reader.cpp:264] Starting prefetch of epoch 12
I0731 19:12:06.501562 16895 solver.cpp:353] Iteration 18900 (5.39502 iter/s, 18.5356s/100 iter), loss = 0.121225
I0731 19:12:06.501585 16895 solver.cpp:375]     Train net output #0: loss = 0.121225 (* 1 = 0.121225 loss)
I0731 19:12:06.501590 16895 sgd_solver.cpp:136] Iteration 18900, lr = 0.0001, m = 0.9
I0731 19:12:25.206038 16895 solver.cpp:353] Iteration 19000 (5.34646 iter/s, 18.704s/100 iter), loss = 0.211816
I0731 19:12:25.206090 16895 solver.cpp:375]     Train net output #0: loss = 0.211816 (* 1 = 0.211816 loss)
I0731 19:12:25.206096 16895 sgd_solver.cpp:136] Iteration 19000, lr = 0.0001, m = 0.9
I0731 19:12:43.848235 16895 solver.cpp:353] Iteration 19100 (5.36432 iter/s, 18.6417s/100 iter), loss = 0.0720573
I0731 19:12:43.848263 16895 solver.cpp:375]     Train net output #0: loss = 0.0720571 (* 1 = 0.0720571 loss)
I0731 19:12:43.848266 16895 sgd_solver.cpp:136] Iteration 19100, lr = 0.0001, m = 0.9
I0731 19:12:56.723909 16876 data_reader.cpp:264] Starting prefetch of epoch 14
I0731 19:13:02.511258 16895 solver.cpp:353] Iteration 19200 (5.35834 iter/s, 18.6625s/100 iter), loss = 0.123291
I0731 19:13:02.511281 16895 solver.cpp:375]     Train net output #0: loss = 0.123291 (* 1 = 0.123291 loss)
I0731 19:13:02.511286 16895 sgd_solver.cpp:136] Iteration 19200, lr = 0.0001, m = 0.9
I0731 19:13:21.123739 16895 solver.cpp:353] Iteration 19300 (5.37289 iter/s, 18.612s/100 iter), loss = 0.127959
I0731 19:13:21.123759 16895 solver.cpp:375]     Train net output #0: loss = 0.127959 (* 1 = 0.127959 loss)
I0731 19:13:21.123764 16895 sgd_solver.cpp:136] Iteration 19300, lr = 0.0001, m = 0.9
I0731 19:13:27.517976 16899 data_reader.cpp:264] Starting prefetch of epoch 13
I0731 19:13:39.634241 16895 solver.cpp:353] Iteration 19400 (5.40249 iter/s, 18.51s/100 iter), loss = 0.122667
I0731 19:13:39.634265 16895 solver.cpp:375]     Train net output #0: loss = 0.122667 (* 1 = 0.122667 loss)
I0731 19:13:39.634269 16895 sgd_solver.cpp:136] Iteration 19400, lr = 0.0001, m = 0.9
I0731 19:13:58.218338 16895 solver.cpp:353] Iteration 19500 (5.38109 iter/s, 18.5836s/100 iter), loss = 0.0873203
I0731 19:13:58.218390 16895 solver.cpp:375]     Train net output #0: loss = 0.0873201 (* 1 = 0.0873201 loss)
I0731 19:13:58.218395 16895 sgd_solver.cpp:136] Iteration 19500, lr = 0.0001, m = 0.9
I0731 19:14:16.805137 16895 solver.cpp:353] Iteration 19600 (5.38031 iter/s, 18.5863s/100 iter), loss = 0.0963952
I0731 19:14:16.805169 16895 solver.cpp:375]     Train net output #0: loss = 0.0963949 (* 1 = 0.0963949 loss)
I0731 19:14:16.805173 16895 sgd_solver.cpp:136] Iteration 19600, lr = 0.0001, m = 0.9
I0731 19:14:28.937301 16899 data_reader.cpp:264] Starting prefetch of epoch 14
I0731 19:14:35.381180 16895 solver.cpp:353] Iteration 19700 (5.38342 iter/s, 18.5755s/100 iter), loss = 0.810466
I0731 19:14:35.381204 16895 solver.cpp:375]     Train net output #0: loss = 0.810466 (* 1 = 0.810466 loss)
I0731 19:14:35.381208 16895 sgd_solver.cpp:136] Iteration 19700, lr = 0.0001, m = 0.9
I0731 19:14:53.953583 16895 solver.cpp:353] Iteration 19800 (5.38448 iter/s, 18.5719s/100 iter), loss = 0.0721171
I0731 19:14:53.953608 16895 solver.cpp:375]     Train net output #0: loss = 0.0721169 (* 1 = 0.0721169 loss)
I0731 19:14:53.953613 16895 sgd_solver.cpp:136] Iteration 19800, lr = 0.0001, m = 0.9
I0731 19:15:12.437906 16895 solver.cpp:353] Iteration 19900 (5.41014 iter/s, 18.4838s/100 iter), loss = 0.0925922
I0731 19:15:12.438002 16895 solver.cpp:375]     Train net output #0: loss = 0.092592 (* 1 = 0.092592 loss)
I0731 19:15:12.438009 16895 sgd_solver.cpp:136] Iteration 19900, lr = 0.0001, m = 0.9
I0731 19:15:30.096657 16899 data_reader.cpp:264] Starting prefetch of epoch 15
I0731 19:15:30.801620 16895 solver.cpp:680] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/initial/cityscapes5_jsegnet21v2_iter_20000.caffemodel
I0731 19:15:30.852090 16895 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/initial/cityscapes5_jsegnet21v2_iter_20000.solverstate
I0731 19:15:30.860527 16895 solver.cpp:550] Iteration 20000, Testing net (#0)
I0731 19:15:41.490460 16893 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 19:15:42.031971 16895 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.949154
I0731 19:15:42.031994 16895 solver.cpp:635]     Test net output #1: accuracy/top5 = 1
I0731 19:15:42.032001 16895 solver.cpp:635]     Test net output #2: loss = 0.146284 (* 1 = 0.146284 loss)
I0731 19:15:42.032104 16895 solver.cpp:305] [MultiGPU] Tests completed in 11.1713s
I0731 19:15:42.235386 16895 solver.cpp:353] Iteration 20000 (3.35608 iter/s, 29.7967s/100 iter), loss = 0.0742942
I0731 19:15:42.235409 16895 solver.cpp:375]     Train net output #0: loss = 0.074294 (* 1 = 0.074294 loss)
I0731 19:15:42.235415 16895 sgd_solver.cpp:136] Iteration 20000, lr = 0.0001, m = 0.9
I0731 19:16:00.898090 16895 solver.cpp:353] Iteration 20100 (5.35843 iter/s, 18.6622s/100 iter), loss = 0.051696
I0731 19:16:00.898201 16895 solver.cpp:375]     Train net output #0: loss = 0.0516958 (* 1 = 0.0516958 loss)
I0731 19:16:00.898208 16895 sgd_solver.cpp:136] Iteration 20100, lr = 0.0001, m = 0.9
I0731 19:16:12.017257 16899 data_reader.cpp:264] Starting prefetch of epoch 16
I0731 19:16:19.427353 16895 solver.cpp:353] Iteration 20200 (5.39702 iter/s, 18.5287s/100 iter), loss = 0.113389
I0731 19:16:19.427376 16895 solver.cpp:375]     Train net output #0: loss = 0.113388 (* 1 = 0.113388 loss)
I0731 19:16:19.427381 16895 sgd_solver.cpp:136] Iteration 20200, lr = 0.0001, m = 0.9
I0731 19:16:37.983608 16895 solver.cpp:353] Iteration 20300 (5.38917 iter/s, 18.5557s/100 iter), loss = 0.0819916
I0731 19:16:37.983680 16895 solver.cpp:375]     Train net output #0: loss = 0.0819914 (* 1 = 0.0819914 loss)
I0731 19:16:37.983685 16895 sgd_solver.cpp:136] Iteration 20300, lr = 0.0001, m = 0.9
I0731 19:16:56.493852 16895 solver.cpp:353] Iteration 20400 (5.40256 iter/s, 18.5097s/100 iter), loss = 0.11921
I0731 19:16:56.493875 16895 solver.cpp:375]     Train net output #0: loss = 0.11921 (* 1 = 0.11921 loss)
I0731 19:16:56.493880 16895 sgd_solver.cpp:136] Iteration 20400, lr = 0.0001, m = 0.9
I0731 19:17:13.386955 16898 data_reader.cpp:264] Starting prefetch of epoch 14
I0731 19:17:15.112869 16895 solver.cpp:353] Iteration 20500 (5.37101 iter/s, 18.6185s/100 iter), loss = 0.0770927
I0731 19:17:15.112895 16895 solver.cpp:375]     Train net output #0: loss = 0.0770925 (* 1 = 0.0770925 loss)
I0731 19:17:15.112898 16895 sgd_solver.cpp:136] Iteration 20500, lr = 0.0001, m = 0.9
I0731 19:17:33.608599 16895 solver.cpp:353] Iteration 20600 (5.4068 iter/s, 18.4952s/100 iter), loss = 0.0820835
I0731 19:17:33.608626 16895 solver.cpp:375]     Train net output #0: loss = 0.0820832 (* 1 = 0.0820832 loss)
I0731 19:17:33.608631 16895 sgd_solver.cpp:136] Iteration 20600, lr = 0.0001, m = 0.9
I0731 19:17:52.128307 16895 solver.cpp:353] Iteration 20700 (5.3998 iter/s, 18.5192s/100 iter), loss = 0.0700263
I0731 19:17:52.128362 16895 solver.cpp:375]     Train net output #0: loss = 0.0700261 (* 1 = 0.0700261 loss)
I0731 19:17:52.128370 16895 sgd_solver.cpp:136] Iteration 20700, lr = 0.0001, m = 0.9
I0731 19:18:10.810611 16895 solver.cpp:353] Iteration 20800 (5.35281 iter/s, 18.6818s/100 iter), loss = 0.0713599
I0731 19:18:10.810633 16895 solver.cpp:375]     Train net output #0: loss = 0.0713597 (* 1 = 0.0713597 loss)
I0731 19:18:10.810638 16895 sgd_solver.cpp:136] Iteration 20800, lr = 0.0001, m = 0.9
I0731 19:18:14.749864 16903 data_reader.cpp:264] Starting prefetch of epoch 12
I0731 19:18:29.365914 16895 solver.cpp:353] Iteration 20900 (5.38944 iter/s, 18.5548s/100 iter), loss = 0.0615994
I0731 19:18:29.365988 16895 solver.cpp:375]     Train net output #0: loss = 0.0615992 (* 1 = 0.0615992 loss)
I0731 19:18:29.366003 16895 sgd_solver.cpp:136] Iteration 20900, lr = 0.0001, m = 0.9
I0731 19:18:47.961951 16895 solver.cpp:353] Iteration 21000 (5.37764 iter/s, 18.5955s/100 iter), loss = 0.149327
I0731 19:18:47.961975 16895 solver.cpp:375]     Train net output #0: loss = 0.149327 (* 1 = 0.149327 loss)
I0731 19:18:47.961982 16895 sgd_solver.cpp:136] Iteration 21000, lr = 0.0001, m = 0.9
I0731 19:19:06.446781 16895 solver.cpp:353] Iteration 21100 (5.40999 iter/s, 18.4843s/100 iter), loss = 0.0814454
I0731 19:19:06.446832 16895 solver.cpp:375]     Train net output #0: loss = 0.0814451 (* 1 = 0.0814451 loss)
I0731 19:19:06.446838 16895 sgd_solver.cpp:136] Iteration 21100, lr = 0.0001, m = 0.9
I0731 19:19:16.067337 16903 data_reader.cpp:264] Starting prefetch of epoch 13
I0731 19:19:25.053400 16895 solver.cpp:353] Iteration 21200 (5.37458 iter/s, 18.6061s/100 iter), loss = 0.115749
I0731 19:19:25.053423 16895 solver.cpp:375]     Train net output #0: loss = 0.115748 (* 1 = 0.115748 loss)
I0731 19:19:25.053427 16895 sgd_solver.cpp:136] Iteration 21200, lr = 0.0001, m = 0.9
I0731 19:19:43.518749 16895 solver.cpp:353] Iteration 21300 (5.4157 iter/s, 18.4648s/100 iter), loss = 0.109178
I0731 19:19:43.518856 16895 solver.cpp:375]     Train net output #0: loss = 0.109178 (* 1 = 0.109178 loss)
I0731 19:19:43.518864 16895 sgd_solver.cpp:136] Iteration 21300, lr = 0.0001, m = 0.9
I0731 19:19:46.760347 16874 data_reader.cpp:264] Starting prefetch of epoch 13
I0731 19:20:01.990077 16895 solver.cpp:353] Iteration 21400 (5.41395 iter/s, 18.4708s/100 iter), loss = 0.0761149
I0731 19:20:01.990098 16895 solver.cpp:375]     Train net output #0: loss = 0.0761147 (* 1 = 0.0761147 loss)
I0731 19:20:01.990103 16895 sgd_solver.cpp:136] Iteration 21400, lr = 0.0001, m = 0.9
I0731 19:20:20.671473 16895 solver.cpp:353] Iteration 21500 (5.35307 iter/s, 18.6809s/100 iter), loss = 0.102652
I0731 19:20:20.671582 16895 solver.cpp:375]     Train net output #0: loss = 0.102652 (* 1 = 0.102652 loss)
I0731 19:20:20.671589 16895 sgd_solver.cpp:136] Iteration 21500, lr = 0.0001, m = 0.9
I0731 19:20:39.238338 16895 solver.cpp:353] Iteration 21600 (5.38609 iter/s, 18.5663s/100 iter), loss = 0.117655
I0731 19:20:39.238363 16895 solver.cpp:375]     Train net output #0: loss = 0.117655 (* 1 = 0.117655 loss)
I0731 19:20:39.238366 16895 sgd_solver.cpp:136] Iteration 21600, lr = 0.0001, m = 0.9
I0731 19:20:48.227697 16901 data_reader.cpp:264] Starting prefetch of epoch 10
I0731 19:20:57.911337 16895 solver.cpp:353] Iteration 21700 (5.35547 iter/s, 18.6725s/100 iter), loss = 0.0830412
I0731 19:20:57.911442 16895 solver.cpp:375]     Train net output #0: loss = 0.083041 (* 1 = 0.083041 loss)
I0731 19:20:57.911448 16895 sgd_solver.cpp:136] Iteration 21700, lr = 0.0001, m = 0.9
I0731 19:21:16.528400 16895 solver.cpp:353] Iteration 21800 (5.37157 iter/s, 18.6165s/100 iter), loss = 0.0948835
I0731 19:21:16.528424 16895 solver.cpp:375]     Train net output #0: loss = 0.0948832 (* 1 = 0.0948832 loss)
I0731 19:21:16.528429 16895 sgd_solver.cpp:136] Iteration 21800, lr = 0.0001, m = 0.9
I0731 19:21:35.201375 16895 solver.cpp:353] Iteration 21900 (5.35548 iter/s, 18.6725s/100 iter), loss = 0.0923789
I0731 19:21:35.201448 16895 solver.cpp:375]     Train net output #0: loss = 0.0923787 (* 1 = 0.0923787 loss)
I0731 19:21:35.201454 16895 sgd_solver.cpp:136] Iteration 21900, lr = 0.0001, m = 0.9
I0731 19:21:49.576385 16876 data_reader.cpp:264] Starting prefetch of epoch 15
I0731 19:21:53.458272 16895 solver.cpp:550] Iteration 22000, Testing net (#0)
I0731 19:22:00.664882 16938 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 19:22:04.663736 16895 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.949959
I0731 19:22:04.663753 16895 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999922
I0731 19:22:04.663759 16895 solver.cpp:635]     Test net output #2: loss = 0.163894 (* 1 = 0.163894 loss)
I0731 19:22:04.663789 16895 solver.cpp:305] [MultiGPU] Tests completed in 11.2052s
I0731 19:22:04.859923 16895 solver.cpp:353] Iteration 22000 (3.3718 iter/s, 29.6577s/100 iter), loss = 0.121644
I0731 19:22:04.859952 16895 solver.cpp:375]     Train net output #0: loss = 0.121643 (* 1 = 0.121643 loss)
I0731 19:22:04.859956 16895 sgd_solver.cpp:136] Iteration 22000, lr = 0.0001, m = 0.9
I0731 19:22:23.461740 16895 solver.cpp:353] Iteration 22100 (5.37597 iter/s, 18.6013s/100 iter), loss = 0.141431
I0731 19:22:23.461853 16895 solver.cpp:375]     Train net output #0: loss = 0.141431 (* 1 = 0.141431 loss)
I0731 19:22:23.461860 16895 sgd_solver.cpp:136] Iteration 22100, lr = 0.0001, m = 0.9
I0731 19:22:41.928019 16895 solver.cpp:353] Iteration 22200 (5.41543 iter/s, 18.4658s/100 iter), loss = 0.122802
I0731 19:22:41.928043 16895 solver.cpp:375]     Train net output #0: loss = 0.122801 (* 1 = 0.122801 loss)
I0731 19:22:41.928047 16895 sgd_solver.cpp:136] Iteration 22200, lr = 0.0001, m = 0.9
I0731 19:23:00.421383 16895 solver.cpp:353] Iteration 22300 (5.4075 iter/s, 18.4928s/100 iter), loss = 0.0625515
I0731 19:23:00.421438 16895 solver.cpp:375]     Train net output #0: loss = 0.0625512 (* 1 = 0.0625512 loss)
I0731 19:23:00.421444 16895 sgd_solver.cpp:136] Iteration 22300, lr = 0.0001, m = 0.9
I0731 19:23:02.133085 16899 data_reader.cpp:264] Starting prefetch of epoch 17
I0731 19:23:18.916250 16895 solver.cpp:353] Iteration 22400 (5.40706 iter/s, 18.4944s/100 iter), loss = 0.097383
I0731 19:23:18.916275 16895 solver.cpp:375]     Train net output #0: loss = 0.0973828 (* 1 = 0.0973828 loss)
I0731 19:23:18.916280 16895 sgd_solver.cpp:136] Iteration 22400, lr = 0.0001, m = 0.9
I0731 19:23:37.433881 16895 solver.cpp:353] Iteration 22500 (5.40041 iter/s, 18.5171s/100 iter), loss = 0.126856
I0731 19:23:37.433959 16895 solver.cpp:375]     Train net output #0: loss = 0.126856 (* 1 = 0.126856 loss)
I0731 19:23:37.433964 16895 sgd_solver.cpp:136] Iteration 22500, lr = 0.0001, m = 0.9
I0731 19:23:56.018836 16895 solver.cpp:353] Iteration 22600 (5.38085 iter/s, 18.5844s/100 iter), loss = 0.0870659
I0731 19:23:56.018862 16895 solver.cpp:375]     Train net output #0: loss = 0.0870657 (* 1 = 0.0870657 loss)
I0731 19:23:56.018865 16895 sgd_solver.cpp:136] Iteration 22600, lr = 0.0001, m = 0.9
I0731 19:24:03.382838 16901 data_reader.cpp:264] Starting prefetch of epoch 11
I0731 19:24:14.542289 16895 solver.cpp:353] Iteration 22700 (5.39871 iter/s, 18.5229s/100 iter), loss = 0.0922053
I0731 19:24:14.542414 16895 solver.cpp:375]     Train net output #0: loss = 0.0922051 (* 1 = 0.0922051 loss)
I0731 19:24:14.542421 16895 sgd_solver.cpp:136] Iteration 22700, lr = 0.0001, m = 0.9
I0731 19:24:33.061033 16895 solver.cpp:353] Iteration 22800 (5.40008 iter/s, 18.5182s/100 iter), loss = 0.0500346
I0731 19:24:33.061055 16895 solver.cpp:375]     Train net output #0: loss = 0.0500344 (* 1 = 0.0500344 loss)
I0731 19:24:33.061059 16895 sgd_solver.cpp:136] Iteration 22800, lr = 0.0001, m = 0.9
I0731 19:24:51.683943 16895 solver.cpp:353] Iteration 22900 (5.36988 iter/s, 18.6224s/100 iter), loss = 0.10342
I0731 19:24:51.684026 16895 solver.cpp:375]     Train net output #0: loss = 0.10342 (* 1 = 0.10342 loss)
I0731 19:24:51.684031 16895 sgd_solver.cpp:136] Iteration 22900, lr = 0.0001, m = 0.9
I0731 19:25:04.737663 16901 data_reader.cpp:264] Starting prefetch of epoch 12
I0731 19:25:10.378610 16895 solver.cpp:353] Iteration 23000 (5.34927 iter/s, 18.6941s/100 iter), loss = 0.109147
I0731 19:25:10.378633 16895 solver.cpp:375]     Train net output #0: loss = 0.109146 (* 1 = 0.109146 loss)
I0731 19:25:10.378638 16895 sgd_solver.cpp:136] Iteration 23000, lr = 0.0001, m = 0.9
I0731 19:25:28.958138 16895 solver.cpp:353] Iteration 23100 (5.38242 iter/s, 18.579s/100 iter), loss = 0.0843774
I0731 19:25:28.958191 16895 solver.cpp:375]     Train net output #0: loss = 0.0843772 (* 1 = 0.0843772 loss)
I0731 19:25:28.958197 16895 sgd_solver.cpp:136] Iteration 23100, lr = 0.0001, m = 0.9
I0731 19:25:35.529309 16874 data_reader.cpp:264] Starting prefetch of epoch 14
I0731 19:25:47.502320 16895 solver.cpp:353] Iteration 23200 (5.39268 iter/s, 18.5437s/100 iter), loss = 0.0863819
I0731 19:25:47.502346 16895 solver.cpp:375]     Train net output #0: loss = 0.0863818 (* 1 = 0.0863818 loss)
I0731 19:25:47.502353 16895 sgd_solver.cpp:136] Iteration 23200, lr = 0.0001, m = 0.9
I0731 19:26:06.026197 16895 solver.cpp:353] Iteration 23300 (5.39859 iter/s, 18.5234s/100 iter), loss = 0.0535975
I0731 19:26:06.026284 16895 solver.cpp:375]     Train net output #0: loss = 0.0535973 (* 1 = 0.0535973 loss)
I0731 19:26:06.026293 16895 sgd_solver.cpp:136] Iteration 23300, lr = 0.0001, m = 0.9
I0731 19:26:24.660686 16895 solver.cpp:353] Iteration 23400 (5.36654 iter/s, 18.634s/100 iter), loss = 0.0841558
I0731 19:26:24.660711 16895 solver.cpp:375]     Train net output #0: loss = 0.0841557 (* 1 = 0.0841557 loss)
I0731 19:26:24.660714 16895 sgd_solver.cpp:136] Iteration 23400, lr = 0.0001, m = 0.9
I0731 19:26:36.949760 16903 data_reader.cpp:264] Starting prefetch of epoch 14
I0731 19:26:43.317206 16895 solver.cpp:353] Iteration 23500 (5.36021 iter/s, 18.656s/100 iter), loss = 0.282119
I0731 19:26:43.317235 16895 solver.cpp:375]     Train net output #0: loss = 0.282119 (* 1 = 0.282119 loss)
I0731 19:26:43.317242 16895 sgd_solver.cpp:136] Iteration 23500, lr = 0.0001, m = 0.9
I0731 19:27:01.847048 16895 solver.cpp:353] Iteration 23600 (5.39685 iter/s, 18.5293s/100 iter), loss = 0.0693958
I0731 19:27:01.847074 16895 solver.cpp:375]     Train net output #0: loss = 0.0693957 (* 1 = 0.0693957 loss)
I0731 19:27:01.847079 16895 sgd_solver.cpp:136] Iteration 23600, lr = 0.0001, m = 0.9
I0731 19:27:20.367046 16895 solver.cpp:353] Iteration 23700 (5.39972 iter/s, 18.5195s/100 iter), loss = 0.0777276
I0731 19:27:20.367147 16895 solver.cpp:375]     Train net output #0: loss = 0.0777275 (* 1 = 0.0777275 loss)
I0731 19:27:20.367156 16895 sgd_solver.cpp:136] Iteration 23700, lr = 0.0001, m = 0.9
I0731 19:27:38.187031 16903 data_reader.cpp:264] Starting prefetch of epoch 15
I0731 19:27:38.922068 16895 solver.cpp:353] Iteration 23800 (5.38953 iter/s, 18.5545s/100 iter), loss = 0.158977
I0731 19:27:38.922094 16895 solver.cpp:375]     Train net output #0: loss = 0.158976 (* 1 = 0.158976 loss)
I0731 19:27:38.922099 16895 sgd_solver.cpp:136] Iteration 23800, lr = 0.0001, m = 0.9
I0731 19:27:57.457180 16895 solver.cpp:353] Iteration 23900 (5.39531 iter/s, 18.5346s/100 iter), loss = 0.0708941
I0731 19:27:57.457278 16895 solver.cpp:375]     Train net output #0: loss = 0.070894 (* 1 = 0.070894 loss)
I0731 19:27:57.457285 16895 sgd_solver.cpp:136] Iteration 23900, lr = 0.0001, m = 0.9
I0731 19:28:08.908278 16874 data_reader.cpp:264] Starting prefetch of epoch 15
I0731 19:28:15.776485 16895 solver.cpp:550] Iteration 24000, Testing net (#0)
I0731 19:28:26.588626 16940 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 19:28:26.940583 16895 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.947568
I0731 19:28:26.940608 16895 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999995
I0731 19:28:26.940613 16895 solver.cpp:635]     Test net output #2: loss = 0.162854 (* 1 = 0.162854 loss)
I0731 19:28:26.940685 16895 solver.cpp:305] [MultiGPU] Tests completed in 11.1639s
I0731 19:28:27.032938 16958 sgd_solver.cpp:48] MultiStep Status: Iteration 24000, step = 1
I0731 19:28:27.032938 16960 sgd_solver.cpp:48] MultiStep Status: Iteration 24000, step = 1
I0731 19:28:27.032938 16959 sgd_solver.cpp:48] MultiStep Status: Iteration 24000, step = 1
I0731 19:28:27.133074 16895 solver.cpp:353] Iteration 24000 (3.36983 iter/s, 29.6751s/100 iter), loss = 0.0909775
I0731 19:28:27.133102 16895 solver.cpp:375]     Train net output #0: loss = 0.0909775 (* 1 = 0.0909775 loss)
I0731 19:28:27.133110 16895 sgd_solver.cpp:136] Iteration 24000, lr = 1e-05, m = 0.9
I0731 19:28:45.595423 16895 solver.cpp:353] Iteration 24100 (5.41658 iter/s, 18.4618s/100 iter), loss = 0.113347
I0731 19:28:45.595475 16895 solver.cpp:375]     Train net output #0: loss = 0.113346 (* 1 = 0.113346 loss)
I0731 19:28:45.595481 16895 sgd_solver.cpp:136] Iteration 24100, lr = 1e-05, m = 0.9
I0731 19:29:04.142102 16895 solver.cpp:353] Iteration 24200 (5.39195 iter/s, 18.5462s/100 iter), loss = 0.20701
I0731 19:29:04.142124 16895 solver.cpp:375]     Train net output #0: loss = 0.20701 (* 1 = 0.20701 loss)
I0731 19:29:04.142129 16895 sgd_solver.cpp:136] Iteration 24200, lr = 1e-05, m = 0.9
I0731 19:29:21.317585 16901 data_reader.cpp:264] Starting prefetch of epoch 13
I0731 19:29:22.816807 16895 solver.cpp:353] Iteration 24300 (5.35499 iter/s, 18.6742s/100 iter), loss = 0.0644075
I0731 19:29:22.816836 16895 solver.cpp:375]     Train net output #0: loss = 0.0644074 (* 1 = 0.0644074 loss)
I0731 19:29:22.816840 16895 sgd_solver.cpp:136] Iteration 24300, lr = 1e-05, m = 0.9
I0731 19:29:41.463171 16895 solver.cpp:353] Iteration 24400 (5.36312 iter/s, 18.6458s/100 iter), loss = 0.084307
I0731 19:29:41.463196 16895 solver.cpp:375]     Train net output #0: loss = 0.0843069 (* 1 = 0.0843069 loss)
I0731 19:29:41.463199 16895 sgd_solver.cpp:136] Iteration 24400, lr = 1e-05, m = 0.9
I0731 19:29:52.250021 16899 data_reader.cpp:264] Starting prefetch of epoch 18
I0731 19:29:59.952898 16895 solver.cpp:353] Iteration 24500 (5.40856 iter/s, 18.4892s/100 iter), loss = 0.0630685
I0731 19:29:59.952929 16895 solver.cpp:375]     Train net output #0: loss = 0.0630684 (* 1 = 0.0630684 loss)
I0731 19:29:59.952935 16895 sgd_solver.cpp:136] Iteration 24500, lr = 1e-05, m = 0.9
I0731 19:30:18.477191 16895 solver.cpp:353] Iteration 24600 (5.39847 iter/s, 18.5238s/100 iter), loss = 0.105378
I0731 19:30:18.477214 16895 solver.cpp:375]     Train net output #0: loss = 0.105378 (* 1 = 0.105378 loss)
I0731 19:30:18.477219 16895 sgd_solver.cpp:136] Iteration 24600, lr = 1e-05, m = 0.9
I0731 19:30:37.189606 16895 solver.cpp:353] Iteration 24700 (5.34419 iter/s, 18.7119s/100 iter), loss = 0.0685053
I0731 19:30:37.189658 16895 solver.cpp:375]     Train net output #0: loss = 0.0685051 (* 1 = 0.0685051 loss)
I0731 19:30:37.189663 16895 sgd_solver.cpp:136] Iteration 24700, lr = 1e-05, m = 0.9
I0731 19:30:53.540956 16903 data_reader.cpp:264] Starting prefetch of epoch 16
I0731 19:30:55.737457 16895 solver.cpp:353] Iteration 24800 (5.39161 iter/s, 18.5473s/100 iter), loss = 0.114455
I0731 19:30:55.737478 16895 solver.cpp:375]     Train net output #0: loss = 0.114455 (* 1 = 0.114455 loss)
I0731 19:30:55.737483 16895 sgd_solver.cpp:136] Iteration 24800, lr = 1e-05, m = 0.9
I0731 19:31:14.241164 16895 solver.cpp:353] Iteration 24900 (5.40447 iter/s, 18.5032s/100 iter), loss = 0.0686629
I0731 19:31:14.241261 16895 solver.cpp:375]     Train net output #0: loss = 0.0686627 (* 1 = 0.0686627 loss)
I0731 19:31:14.241266 16895 sgd_solver.cpp:136] Iteration 24900, lr = 1e-05, m = 0.9
I0731 19:31:32.854246 16895 solver.cpp:353] Iteration 25000 (5.37271 iter/s, 18.6126s/100 iter), loss = 0.0883391
I0731 19:31:32.854274 16895 solver.cpp:375]     Train net output #0: loss = 0.088339 (* 1 = 0.088339 loss)
I0731 19:31:32.854281 16895 sgd_solver.cpp:136] Iteration 25000, lr = 1e-05, m = 0.9
I0731 19:31:51.441371 16895 solver.cpp:353] Iteration 25100 (5.38022 iter/s, 18.5866s/100 iter), loss = 0.0869434
I0731 19:31:51.441440 16895 solver.cpp:375]     Train net output #0: loss = 0.0869432 (* 1 = 0.0869432 loss)
I0731 19:31:51.441447 16895 sgd_solver.cpp:136] Iteration 25100, lr = 1e-05, m = 0.9
I0731 19:31:54.953179 16874 data_reader.cpp:264] Starting prefetch of epoch 16
I0731 19:32:10.040729 16895 solver.cpp:353] Iteration 25200 (5.37668 iter/s, 18.5988s/100 iter), loss = 0.0992967
I0731 19:32:10.040755 16895 solver.cpp:375]     Train net output #0: loss = 0.0992966 (* 1 = 0.0992966 loss)
I0731 19:32:10.040758 16895 sgd_solver.cpp:136] Iteration 25200, lr = 1e-05, m = 0.9
I0731 19:32:28.667088 16895 solver.cpp:353] Iteration 25300 (5.36888 iter/s, 18.6258s/100 iter), loss = 0.0911182
I0731 19:32:28.667137 16895 solver.cpp:375]     Train net output #0: loss = 0.0911181 (* 1 = 0.0911181 loss)
I0731 19:32:28.667142 16895 sgd_solver.cpp:136] Iteration 25300, lr = 1e-05, m = 0.9
I0731 19:32:47.484758 16895 solver.cpp:353] Iteration 25400 (5.3143 iter/s, 18.8172s/100 iter), loss = 0.105268
I0731 19:32:47.484782 16895 solver.cpp:375]     Train net output #0: loss = 0.105268 (* 1 = 0.105268 loss)
I0731 19:32:47.484786 16895 sgd_solver.cpp:136] Iteration 25400, lr = 1e-05, m = 0.9
I0731 19:32:56.613703 16876 data_reader.cpp:264] Starting prefetch of epoch 16
I0731 19:33:06.068445 16895 solver.cpp:353] Iteration 25500 (5.38121 iter/s, 18.5832s/100 iter), loss = 0.0595335
I0731 19:33:06.068500 16895 solver.cpp:375]     Train net output #0: loss = 0.0595334 (* 1 = 0.0595334 loss)
I0731 19:33:06.068506 16895 sgd_solver.cpp:136] Iteration 25500, lr = 1e-05, m = 0.9
I0731 19:33:24.685256 16895 solver.cpp:353] Iteration 25600 (5.37164 iter/s, 18.6163s/100 iter), loss = 0.0798673
I0731 19:33:24.685286 16895 solver.cpp:375]     Train net output #0: loss = 0.0798671 (* 1 = 0.0798671 loss)
I0731 19:33:24.685292 16895 sgd_solver.cpp:136] Iteration 25600, lr = 1e-05, m = 0.9
I0731 19:33:27.457157 16899 data_reader.cpp:264] Starting prefetch of epoch 19
I0731 19:33:43.145362 16895 solver.cpp:353] Iteration 25700 (5.41724 iter/s, 18.4596s/100 iter), loss = 0.0510433
I0731 19:33:43.145416 16895 solver.cpp:375]     Train net output #0: loss = 0.0510431 (* 1 = 0.0510431 loss)
I0731 19:33:43.145421 16895 sgd_solver.cpp:136] Iteration 25700, lr = 1e-05, m = 0.9
I0731 19:34:01.641012 16895 solver.cpp:353] Iteration 25800 (5.40683 iter/s, 18.4951s/100 iter), loss = 0.0773511
I0731 19:34:01.641037 16895 solver.cpp:375]     Train net output #0: loss = 0.077351 (* 1 = 0.077351 loss)
I0731 19:34:01.641041 16895 sgd_solver.cpp:136] Iteration 25800, lr = 1e-05, m = 0.9
I0731 19:34:20.276801 16895 solver.cpp:353] Iteration 25900 (5.36617 iter/s, 18.6353s/100 iter), loss = 0.0652455
I0731 19:34:20.276882 16895 solver.cpp:375]     Train net output #0: loss = 0.0652454 (* 1 = 0.0652454 loss)
I0731 19:34:20.276890 16895 sgd_solver.cpp:136] Iteration 25900, lr = 1e-05, m = 0.9
I0731 19:34:28.694089 16874 data_reader.cpp:264] Starting prefetch of epoch 17
I0731 19:34:38.785473 16895 solver.cpp:550] Iteration 26000, Testing net (#0)
I0731 19:34:49.753564 16895 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.950595
I0731 19:34:49.753585 16895 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999689
I0731 19:34:49.753590 16895 solver.cpp:635]     Test net output #2: loss = 0.169406 (* 1 = 0.169406 loss)
I0731 19:34:49.753664 16895 solver.cpp:305] [MultiGPU] Tests completed in 10.9679s
I0731 19:34:49.948613 16895 solver.cpp:353] Iteration 26000 (3.3703 iter/s, 29.671s/100 iter), loss = 0.0948464
I0731 19:34:49.948639 16895 solver.cpp:375]     Train net output #0: loss = 0.0948463 (* 1 = 0.0948463 loss)
I0731 19:34:49.948645 16895 sgd_solver.cpp:136] Iteration 26000, lr = 1e-05, m = 0.9
I0731 19:35:08.526060 16895 solver.cpp:353] Iteration 26100 (5.38302 iter/s, 18.5769s/100 iter), loss = 0.0518993
I0731 19:35:08.526134 16895 solver.cpp:375]     Train net output #0: loss = 0.0518991 (* 1 = 0.0518991 loss)
I0731 19:35:08.526141 16895 sgd_solver.cpp:136] Iteration 26100, lr = 1e-05, m = 0.9
I0731 19:35:10.410540 16901 data_reader.cpp:264] Starting prefetch of epoch 14
I0731 19:35:27.081132 16895 solver.cpp:353] Iteration 26200 (5.38951 iter/s, 18.5546s/100 iter), loss = 0.112135
I0731 19:35:27.081154 16895 solver.cpp:375]     Train net output #0: loss = 0.112135 (* 1 = 0.112135 loss)
I0731 19:35:27.081158 16895 sgd_solver.cpp:136] Iteration 26200, lr = 1e-05, m = 0.9
I0731 19:35:45.583425 16895 solver.cpp:353] Iteration 26300 (5.40489 iter/s, 18.5018s/100 iter), loss = 0.0901977
I0731 19:35:45.583556 16895 solver.cpp:375]     Train net output #0: loss = 0.0901976 (* 1 = 0.0901976 loss)
I0731 19:35:45.583564 16895 sgd_solver.cpp:136] Iteration 26300, lr = 1e-05, m = 0.9
I0731 19:36:04.103682 16895 solver.cpp:353] Iteration 26400 (5.39964 iter/s, 18.5197s/100 iter), loss = 0.134318
I0731 19:36:04.103703 16895 solver.cpp:375]     Train net output #0: loss = 0.134318 (* 1 = 0.134318 loss)
I0731 19:36:04.103708 16895 sgd_solver.cpp:136] Iteration 26400, lr = 1e-05, m = 0.9
I0731 19:36:11.675438 16903 data_reader.cpp:264] Starting prefetch of epoch 17
I0731 19:36:22.605406 16895 solver.cpp:353] Iteration 26500 (5.40505 iter/s, 18.5012s/100 iter), loss = 0.0546557
I0731 19:36:22.605458 16895 solver.cpp:375]     Train net output #0: loss = 0.0546555 (* 1 = 0.0546555 loss)
I0731 19:36:22.605463 16895 sgd_solver.cpp:136] Iteration 26500, lr = 1e-05, m = 0.9
I0731 19:36:41.139250 16895 solver.cpp:353] Iteration 26600 (5.39568 iter/s, 18.5333s/100 iter), loss = 0.0870072
I0731 19:36:41.139272 16895 solver.cpp:375]     Train net output #0: loss = 0.087007 (* 1 = 0.087007 loss)
I0731 19:36:41.139278 16895 sgd_solver.cpp:136] Iteration 26600, lr = 1e-05, m = 0.9
I0731 19:36:42.279579 16899 data_reader.cpp:264] Starting prefetch of epoch 20
I0731 19:36:59.708160 16895 solver.cpp:353] Iteration 26700 (5.3855 iter/s, 18.5684s/100 iter), loss = 0.0764507
I0731 19:36:59.708269 16895 solver.cpp:375]     Train net output #0: loss = 0.0764506 (* 1 = 0.0764506 loss)
I0731 19:36:59.708276 16895 sgd_solver.cpp:136] Iteration 26700, lr = 1e-05, m = 0.9
I0731 19:37:18.320086 16895 solver.cpp:353] Iteration 26800 (5.37305 iter/s, 18.6114s/100 iter), loss = 0.0949733
I0731 19:37:18.320116 16895 solver.cpp:375]     Train net output #0: loss = 0.0949731 (* 1 = 0.0949731 loss)
I0731 19:37:18.320123 16895 sgd_solver.cpp:136] Iteration 26800, lr = 1e-05, m = 0.9
I0731 19:37:37.005497 16895 solver.cpp:353] Iteration 26900 (5.35192 iter/s, 18.6849s/100 iter), loss = 0.0591203
I0731 19:37:37.005549 16895 solver.cpp:375]     Train net output #0: loss = 0.0591202 (* 1 = 0.0591202 loss)
I0731 19:37:37.005554 16895 sgd_solver.cpp:136] Iteration 26900, lr = 1e-05, m = 0.9
I0731 19:37:43.954077 16901 data_reader.cpp:264] Starting prefetch of epoch 15
I0731 19:37:55.597563 16895 solver.cpp:353] Iteration 27000 (5.37879 iter/s, 18.5915s/100 iter), loss = 0.190562
I0731 19:37:55.597589 16895 solver.cpp:375]     Train net output #0: loss = 0.190562 (* 1 = 0.190562 loss)
I0731 19:37:55.597595 16895 sgd_solver.cpp:136] Iteration 27000, lr = 1e-05, m = 0.9
I0731 19:38:14.193681 16895 solver.cpp:353] Iteration 27100 (5.37761 iter/s, 18.5956s/100 iter), loss = 0.0775418
I0731 19:38:14.193768 16895 solver.cpp:375]     Train net output #0: loss = 0.0775416 (* 1 = 0.0775416 loss)
I0731 19:38:14.193775 16895 sgd_solver.cpp:136] Iteration 27100, lr = 1e-05, m = 0.9
I0731 19:38:32.673146 16895 solver.cpp:353] Iteration 27200 (5.41156 iter/s, 18.4789s/100 iter), loss = 0.0479062
I0731 19:38:32.673168 16895 solver.cpp:375]     Train net output #0: loss = 0.047906 (* 1 = 0.047906 loss)
I0731 19:38:32.673172 16895 sgd_solver.cpp:136] Iteration 27200, lr = 1e-05, m = 0.9
I0731 19:38:45.206573 16901 data_reader.cpp:264] Starting prefetch of epoch 16
I0731 19:38:51.344334 16895 solver.cpp:353] Iteration 27300 (5.35599 iter/s, 18.6707s/100 iter), loss = 0.0772896
I0731 19:38:51.344357 16895 solver.cpp:375]     Train net output #0: loss = 0.0772894 (* 1 = 0.0772894 loss)
I0731 19:38:51.344360 16895 sgd_solver.cpp:136] Iteration 27300, lr = 1e-05, m = 0.9
I0731 19:39:09.809181 16895 solver.cpp:353] Iteration 27400 (5.41585 iter/s, 18.4643s/100 iter), loss = 0.143495
I0731 19:39:09.809206 16895 solver.cpp:375]     Train net output #0: loss = 0.143495 (* 1 = 0.143495 loss)
I0731 19:39:09.809211 16895 sgd_solver.cpp:136] Iteration 27400, lr = 1e-05, m = 0.9
I0731 19:39:15.958884 16898 data_reader.cpp:264] Starting prefetch of epoch 15
I0731 19:39:28.399848 16895 solver.cpp:353] Iteration 27500 (5.37919 iter/s, 18.5902s/100 iter), loss = 0.0687978
I0731 19:39:28.399871 16895 solver.cpp:375]     Train net output #0: loss = 0.0687976 (* 1 = 0.0687976 loss)
I0731 19:39:28.399876 16895 sgd_solver.cpp:136] Iteration 27500, lr = 1e-05, m = 0.9
I0731 19:39:47.042378 16895 solver.cpp:353] Iteration 27600 (5.36423 iter/s, 18.642s/100 iter), loss = 0.0962823
I0731 19:39:47.042434 16895 solver.cpp:375]     Train net output #0: loss = 0.0962821 (* 1 = 0.0962821 loss)
I0731 19:39:47.042441 16895 sgd_solver.cpp:136] Iteration 27600, lr = 1e-05, m = 0.9
I0731 19:40:05.533861 16895 solver.cpp:353] Iteration 27700 (5.40804 iter/s, 18.491s/100 iter), loss = 0.0939745
I0731 19:40:05.533885 16895 solver.cpp:375]     Train net output #0: loss = 0.0939743 (* 1 = 0.0939743 loss)
I0731 19:40:05.533888 16895 sgd_solver.cpp:136] Iteration 27700, lr = 1e-05, m = 0.9
I0731 19:40:17.175045 16903 data_reader.cpp:264] Starting prefetch of epoch 18
I0731 19:40:23.991966 16895 solver.cpp:353] Iteration 27800 (5.41782 iter/s, 18.4576s/100 iter), loss = 0.0479142
I0731 19:40:23.991989 16895 solver.cpp:375]     Train net output #0: loss = 0.0479141 (* 1 = 0.0479141 loss)
I0731 19:40:23.991993 16895 sgd_solver.cpp:136] Iteration 27800, lr = 1e-05, m = 0.9
I0731 19:40:42.596488 16895 solver.cpp:353] Iteration 27900 (5.37519 iter/s, 18.604s/100 iter), loss = 0.0700293
I0731 19:40:42.596520 16895 solver.cpp:375]     Train net output #0: loss = 0.0700291 (* 1 = 0.0700291 loss)
I0731 19:40:42.596527 16895 sgd_solver.cpp:136] Iteration 27900, lr = 1e-05, m = 0.9
I0731 19:41:01.146373 16895 solver.cpp:550] Iteration 28000, Testing net (#0)
I0731 19:41:04.412425 16893 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 19:41:12.301131 16936 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 19:41:12.639309 16895 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.94887
I0731 19:41:12.639333 16895 solver.cpp:635]     Test net output #1: accuracy/top5 = 1
I0731 19:41:12.639338 16895 solver.cpp:635]     Test net output #2: loss = 0.158042 (* 1 = 0.158042 loss)
I0731 19:41:12.639415 16895 solver.cpp:305] [MultiGPU] Tests completed in 11.4927s
I0731 19:41:12.852612 16895 solver.cpp:353] Iteration 28000 (3.30521 iter/s, 30.2553s/100 iter), loss = 0.103962
I0731 19:41:12.852639 16895 solver.cpp:375]     Train net output #0: loss = 0.103962 (* 1 = 0.103962 loss)
I0731 19:41:12.852643 16895 sgd_solver.cpp:136] Iteration 28000, lr = 1e-05, m = 0.9
I0731 19:41:31.387063 16895 solver.cpp:353] Iteration 28100 (5.39551 iter/s, 18.5339s/100 iter), loss = 0.0813759
I0731 19:41:31.387131 16895 solver.cpp:375]     Train net output #0: loss = 0.0813757 (* 1 = 0.0813757 loss)
I0731 19:41:31.387138 16895 sgd_solver.cpp:136] Iteration 28100, lr = 1e-05, m = 0.9
I0731 19:41:50.028465 16895 solver.cpp:353] Iteration 28200 (5.36455 iter/s, 18.6409s/100 iter), loss = 0.0835746
I0731 19:41:50.028487 16895 solver.cpp:375]     Train net output #0: loss = 0.0835745 (* 1 = 0.0835745 loss)
I0731 19:41:50.028491 16895 sgd_solver.cpp:136] Iteration 28200, lr = 1e-05, m = 0.9
I0731 19:42:00.970288 16903 data_reader.cpp:264] Starting prefetch of epoch 19
I0731 19:42:08.583876 16895 solver.cpp:353] Iteration 28300 (5.38941 iter/s, 18.5549s/100 iter), loss = 0.055764
I0731 19:42:08.583927 16895 solver.cpp:375]     Train net output #0: loss = 0.0557639 (* 1 = 0.0557639 loss)
I0731 19:42:08.583932 16895 sgd_solver.cpp:136] Iteration 28300, lr = 1e-05, m = 0.9
I0731 19:42:27.114148 16895 solver.cpp:353] Iteration 28400 (5.39672 iter/s, 18.5298s/100 iter), loss = 0.0938832
I0731 19:42:27.114172 16895 solver.cpp:375]     Train net output #0: loss = 0.093883 (* 1 = 0.093883 loss)
I0731 19:42:27.114177 16895 sgd_solver.cpp:136] Iteration 28400, lr = 1e-05, m = 0.9
I0731 19:42:45.688261 16895 solver.cpp:353] Iteration 28500 (5.38399 iter/s, 18.5736s/100 iter), loss = 0.0914015
I0731 19:42:45.688315 16895 solver.cpp:375]     Train net output #0: loss = 0.0914013 (* 1 = 0.0914013 loss)
I0731 19:42:45.688321 16895 sgd_solver.cpp:136] Iteration 28500, lr = 1e-05, m = 0.9
I0731 19:43:02.320116 16901 data_reader.cpp:264] Starting prefetch of epoch 17
I0731 19:43:04.161609 16895 solver.cpp:353] Iteration 28600 (5.41335 iter/s, 18.4728s/100 iter), loss = 0.133157
I0731 19:43:04.161633 16895 solver.cpp:375]     Train net output #0: loss = 0.133157 (* 1 = 0.133157 loss)
I0731 19:43:04.161639 16895 sgd_solver.cpp:136] Iteration 28600, lr = 1e-05, m = 0.9
I0731 19:43:22.646903 16895 solver.cpp:353] Iteration 28700 (5.40986 iter/s, 18.4848s/100 iter), loss = 0.0990574
I0731 19:43:22.646955 16895 solver.cpp:375]     Train net output #0: loss = 0.0990573 (* 1 = 0.0990573 loss)
I0731 19:43:22.646958 16895 sgd_solver.cpp:136] Iteration 28700, lr = 1e-05, m = 0.9
I0731 19:43:32.911159 16874 data_reader.cpp:264] Starting prefetch of epoch 18
I0731 19:43:41.213423 16895 solver.cpp:353] Iteration 28800 (5.38619 iter/s, 18.566s/100 iter), loss = 0.0723923
I0731 19:43:41.213449 16895 solver.cpp:375]     Train net output #0: loss = 0.0723922 (* 1 = 0.0723922 loss)
I0731 19:43:41.213452 16895 sgd_solver.cpp:136] Iteration 28800, lr = 1e-05, m = 0.9
I0731 19:43:59.824240 16895 solver.cpp:353] Iteration 28900 (5.37337 iter/s, 18.6103s/100 iter), loss = 0.157444
I0731 19:43:59.824295 16895 solver.cpp:375]     Train net output #0: loss = 0.157444 (* 1 = 0.157444 loss)
I0731 19:43:59.824301 16895 sgd_solver.cpp:136] Iteration 28900, lr = 1e-05, m = 0.9
I0731 19:44:18.581553 16895 solver.cpp:353] Iteration 29000 (5.3314 iter/s, 18.7568s/100 iter), loss = 0.0946085
I0731 19:44:18.581578 16895 solver.cpp:375]     Train net output #0: loss = 0.0946084 (* 1 = 0.0946084 loss)
I0731 19:44:18.581583 16895 sgd_solver.cpp:136] Iteration 29000, lr = 1e-05, m = 0.9
I0731 19:44:34.451192 16876 data_reader.cpp:264] Starting prefetch of epoch 17
I0731 19:44:37.203186 16895 solver.cpp:353] Iteration 29100 (5.37025 iter/s, 18.6211s/100 iter), loss = 0.0919976
I0731 19:44:37.203208 16895 solver.cpp:375]     Train net output #0: loss = 0.0919975 (* 1 = 0.0919975 loss)
I0731 19:44:37.203212 16895 sgd_solver.cpp:136] Iteration 29100, lr = 1e-05, m = 0.9
I0731 19:44:55.611901 16895 solver.cpp:353] Iteration 29200 (5.43236 iter/s, 18.4082s/100 iter), loss = 0.0620114
I0731 19:44:55.611923 16895 solver.cpp:375]     Train net output #0: loss = 0.0620113 (* 1 = 0.0620113 loss)
I0731 19:44:55.611927 16895 sgd_solver.cpp:136] Iteration 29200, lr = 1e-05, m = 0.9
I0731 19:45:14.333528 16895 solver.cpp:353] Iteration 29300 (5.34156 iter/s, 18.7211s/100 iter), loss = 0.0598422
I0731 19:45:14.333657 16895 solver.cpp:375]     Train net output #0: loss = 0.0598421 (* 1 = 0.0598421 loss)
I0731 19:45:14.333663 16895 sgd_solver.cpp:136] Iteration 29300, lr = 1e-05, m = 0.9
I0731 19:45:32.897390 16895 solver.cpp:353] Iteration 29400 (5.38696 iter/s, 18.5634s/100 iter), loss = 0.0585891
I0731 19:45:32.897415 16895 solver.cpp:375]     Train net output #0: loss = 0.058589 (* 1 = 0.058589 loss)
I0731 19:45:32.897419 16895 sgd_solver.cpp:136] Iteration 29400, lr = 1e-05, m = 0.9
I0731 19:45:35.868558 16903 data_reader.cpp:264] Starting prefetch of epoch 20
I0731 19:45:51.468192 16895 solver.cpp:353] Iteration 29500 (5.38494 iter/s, 18.5703s/100 iter), loss = 0.138378
I0731 19:45:51.468245 16895 solver.cpp:375]     Train net output #0: loss = 0.138377 (* 1 = 0.138377 loss)
I0731 19:45:51.468251 16895 sgd_solver.cpp:136] Iteration 29500, lr = 1e-05, m = 0.9
I0731 19:46:06.484594 16874 data_reader.cpp:264] Starting prefetch of epoch 19
I0731 19:46:09.973763 16895 solver.cpp:353] Iteration 29600 (5.40392 iter/s, 18.5051s/100 iter), loss = 0.0859054
I0731 19:46:09.973788 16895 solver.cpp:375]     Train net output #0: loss = 0.0859052 (* 1 = 0.0859052 loss)
I0731 19:46:09.973791 16895 sgd_solver.cpp:136] Iteration 29600, lr = 1e-05, m = 0.9
I0731 19:46:28.535109 16895 solver.cpp:353] Iteration 29700 (5.38769 iter/s, 18.5608s/100 iter), loss = 0.0705883
I0731 19:46:28.535166 16895 solver.cpp:375]     Train net output #0: loss = 0.0705882 (* 1 = 0.0705882 loss)
I0731 19:46:28.535171 16895 sgd_solver.cpp:136] Iteration 29700, lr = 1e-05, m = 0.9
I0731 19:46:47.202584 16895 solver.cpp:353] Iteration 29800 (5.35706 iter/s, 18.667s/100 iter), loss = 0.148605
I0731 19:46:47.202610 16895 solver.cpp:375]     Train net output #0: loss = 0.148605 (* 1 = 0.148605 loss)
I0731 19:46:47.202615 16895 sgd_solver.cpp:136] Iteration 29800, lr = 1e-05, m = 0.9
I0731 19:47:05.637884 16895 solver.cpp:353] Iteration 29900 (5.42452 iter/s, 18.4348s/100 iter), loss = 0.0793571
I0731 19:47:05.637990 16895 solver.cpp:375]     Train net output #0: loss = 0.079357 (* 1 = 0.079357 loss)
I0731 19:47:05.637996 16895 sgd_solver.cpp:136] Iteration 29900, lr = 1e-05, m = 0.9
I0731 19:47:07.900349 16903 data_reader.cpp:264] Starting prefetch of epoch 21
I0731 19:47:24.050166 16895 solver.cpp:680] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/initial/cityscapes5_jsegnet21v2_iter_30000.caffemodel
I0731 19:47:24.159864 16895 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/initial/cityscapes5_jsegnet21v2_iter_30000.solverstate
I0731 19:47:24.170681 16895 solver.cpp:550] Iteration 30000, Testing net (#0)
I0731 19:47:35.102744 16895 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.949544
I0731 19:47:35.102764 16895 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999469
I0731 19:47:35.102771 16895 solver.cpp:635]     Test net output #2: loss = 0.191064 (* 1 = 0.191064 loss)
I0731 19:47:35.102787 16895 solver.cpp:305] [MultiGPU] Tests completed in 10.9318s
I0731 19:47:35.300060 16895 solver.cpp:353] Iteration 30000 (3.37139 iter/s, 29.6614s/100 iter), loss = 0.146011
I0731 19:47:35.300084 16895 solver.cpp:375]     Train net output #0: loss = 0.146011 (* 1 = 0.146011 loss)
I0731 19:47:35.300088 16895 sgd_solver.cpp:136] Iteration 30000, lr = 1e-05, m = 0.9
I0731 19:47:49.560660 16901 data_reader.cpp:264] Starting prefetch of epoch 18
I0731 19:47:53.792579 16895 solver.cpp:353] Iteration 30100 (5.40774 iter/s, 18.492s/100 iter), loss = 0.0781085
I0731 19:47:53.792604 16895 solver.cpp:375]     Train net output #0: loss = 0.0781084 (* 1 = 0.0781084 loss)
I0731 19:47:53.792608 16895 sgd_solver.cpp:136] Iteration 30100, lr = 1e-05, m = 0.9
I0731 19:48:12.269429 16895 solver.cpp:353] Iteration 30200 (5.41233 iter/s, 18.4763s/100 iter), loss = 0.0717178
I0731 19:48:12.269454 16895 solver.cpp:375]     Train net output #0: loss = 0.0717177 (* 1 = 0.0717177 loss)
I0731 19:48:12.269459 16895 sgd_solver.cpp:136] Iteration 30200, lr = 1e-05, m = 0.9
I0731 19:48:20.105785 16899 data_reader.cpp:264] Starting prefetch of epoch 21
I0731 19:48:30.881665 16895 solver.cpp:353] Iteration 30300 (5.37296 iter/s, 18.6117s/100 iter), loss = 0.0937092
I0731 19:48:30.881690 16895 solver.cpp:375]     Train net output #0: loss = 0.0937091 (* 1 = 0.0937091 loss)
I0731 19:48:30.881695 16895 sgd_solver.cpp:136] Iteration 30300, lr = 1e-05, m = 0.9
I0731 19:48:49.435701 16895 solver.cpp:353] Iteration 30400 (5.38981 iter/s, 18.5535s/100 iter), loss = 0.0825513
I0731 19:48:49.435724 16895 solver.cpp:375]     Train net output #0: loss = 0.0825512 (* 1 = 0.0825512 loss)
I0731 19:48:49.435727 16895 sgd_solver.cpp:136] Iteration 30400, lr = 1e-05, m = 0.9
I0731 19:49:08.125344 16895 solver.cpp:353] Iteration 30500 (5.3507 iter/s, 18.6891s/100 iter), loss = 0.0909085
I0731 19:49:08.125432 16895 solver.cpp:375]     Train net output #0: loss = 0.0909084 (* 1 = 0.0909084 loss)
I0731 19:49:08.125439 16895 sgd_solver.cpp:136] Iteration 30500, lr = 1e-05, m = 0.9
I0731 19:49:21.570225 16876 data_reader.cpp:264] Starting prefetch of epoch 18
I0731 19:49:26.601089 16895 solver.cpp:353] Iteration 30600 (5.41265 iter/s, 18.4752s/100 iter), loss = 0.0649177
I0731 19:49:26.601111 16895 solver.cpp:375]     Train net output #0: loss = 0.0649176 (* 1 = 0.0649176 loss)
I0731 19:49:26.601115 16895 sgd_solver.cpp:136] Iteration 30600, lr = 1e-05, m = 0.9
I0731 19:49:45.104059 16895 solver.cpp:353] Iteration 30700 (5.40469 iter/s, 18.5025s/100 iter), loss = 0.175896
I0731 19:49:45.104109 16895 solver.cpp:375]     Train net output #0: loss = 0.175896 (* 1 = 0.175896 loss)
I0731 19:49:45.104115 16895 sgd_solver.cpp:136] Iteration 30700, lr = 1e-05, m = 0.9
I0731 19:50:03.672487 16895 solver.cpp:353] Iteration 30800 (5.38563 iter/s, 18.5679s/100 iter), loss = 0.087321
I0731 19:50:03.672511 16895 solver.cpp:375]     Train net output #0: loss = 0.0873209 (* 1 = 0.0873209 loss)
I0731 19:50:03.672515 16895 sgd_solver.cpp:136] Iteration 30800, lr = 1e-05, m = 0.9
I0731 19:50:22.309486 16895 solver.cpp:353] Iteration 30900 (5.36582 iter/s, 18.6365s/100 iter), loss = 0.0820568
I0731 19:50:22.309535 16895 solver.cpp:375]     Train net output #0: loss = 0.0820567 (* 1 = 0.0820567 loss)
I0731 19:50:22.309540 16895 sgd_solver.cpp:136] Iteration 30900, lr = 1e-05, m = 0.9
I0731 19:50:22.870481 16901 data_reader.cpp:264] Starting prefetch of epoch 19
I0731 19:50:40.807093 16895 solver.cpp:353] Iteration 31000 (5.40626 iter/s, 18.4971s/100 iter), loss = 0.0724859
I0731 19:50:40.807121 16895 solver.cpp:375]     Train net output #0: loss = 0.0724858 (* 1 = 0.0724858 loss)
I0731 19:50:40.807126 16895 sgd_solver.cpp:136] Iteration 31000, lr = 1e-05, m = 0.9
I0731 19:50:53.653362 16898 data_reader.cpp:264] Starting prefetch of epoch 16
I0731 19:50:59.428081 16895 solver.cpp:353] Iteration 31100 (5.37043 iter/s, 18.6205s/100 iter), loss = 0.081388
I0731 19:50:59.428104 16895 solver.cpp:375]     Train net output #0: loss = 0.0813879 (* 1 = 0.0813879 loss)
I0731 19:50:59.428109 16895 sgd_solver.cpp:136] Iteration 31100, lr = 1e-05, m = 0.9
I0731 19:51:17.879245 16895 solver.cpp:353] Iteration 31200 (5.41986 iter/s, 18.4507s/100 iter), loss = 0.162828
I0731 19:51:17.879271 16895 solver.cpp:375]     Train net output #0: loss = 0.162828 (* 1 = 0.162828 loss)
I0731 19:51:17.879276 16895 sgd_solver.cpp:136] Iteration 31200, lr = 1e-05, m = 0.9
I0731 19:51:36.347036 16895 solver.cpp:353] Iteration 31300 (5.41498 iter/s, 18.4673s/100 iter), loss = 0.110033
I0731 19:51:36.347098 16895 solver.cpp:375]     Train net output #0: loss = 0.110033 (* 1 = 0.110033 loss)
I0731 19:51:36.347105 16895 sgd_solver.cpp:136] Iteration 31300, lr = 1e-05, m = 0.9
I0731 19:51:54.666635 16901 data_reader.cpp:264] Starting prefetch of epoch 20
I0731 19:51:54.847101 16895 solver.cpp:353] Iteration 31400 (5.40554 iter/s, 18.4996s/100 iter), loss = 0.0658627
I0731 19:51:54.847127 16895 solver.cpp:375]     Train net output #0: loss = 0.0658627 (* 1 = 0.0658627 loss)
I0731 19:51:54.847131 16895 sgd_solver.cpp:136] Iteration 31400, lr = 1e-05, m = 0.9
I0731 19:52:13.387361 16895 solver.cpp:353] Iteration 31500 (5.39382 iter/s, 18.5397s/100 iter), loss = 0.0709707
I0731 19:52:13.387467 16895 solver.cpp:375]     Train net output #0: loss = 0.0709706 (* 1 = 0.0709706 loss)
I0731 19:52:13.387475 16895 sgd_solver.cpp:136] Iteration 31500, lr = 1e-05, m = 0.9
I0731 19:52:31.985975 16895 solver.cpp:353] Iteration 31600 (5.37689 iter/s, 18.5981s/100 iter), loss = 0.196909
I0731 19:52:31.985999 16895 solver.cpp:375]     Train net output #0: loss = 0.196909 (* 1 = 0.196909 loss)
I0731 19:52:31.986003 16895 sgd_solver.cpp:136] Iteration 31600, lr = 1e-05, m = 0.9
I0731 19:52:50.521822 16895 solver.cpp:353] Iteration 31700 (5.3951 iter/s, 18.5353s/100 iter), loss = 0.0991279
I0731 19:52:50.521874 16895 solver.cpp:375]     Train net output #0: loss = 0.0991279 (* 1 = 0.0991279 loss)
I0731 19:52:50.521879 16895 sgd_solver.cpp:136] Iteration 31700, lr = 1e-05, m = 0.9
I0731 19:52:56.089323 16876 data_reader.cpp:264] Starting prefetch of epoch 19
I0731 19:53:08.965854 16895 solver.cpp:353] Iteration 31800 (5.42196 iter/s, 18.4435s/100 iter), loss = 0.0821226
I0731 19:53:08.965878 16895 solver.cpp:375]     Train net output #0: loss = 0.0821225 (* 1 = 0.0821225 loss)
I0731 19:53:08.965883 16895 sgd_solver.cpp:136] Iteration 31800, lr = 1e-05, m = 0.9
I0731 19:53:26.679898 16899 data_reader.cpp:264] Starting prefetch of epoch 22
I0731 19:53:27.554625 16895 solver.cpp:353] Iteration 31900 (5.37974 iter/s, 18.5883s/100 iter), loss = 0.051225
I0731 19:53:27.554648 16895 solver.cpp:375]     Train net output #0: loss = 0.051225 (* 1 = 0.051225 loss)
I0731 19:53:27.554653 16895 sgd_solver.cpp:136] Iteration 31900, lr = 1e-05, m = 0.9
I0731 19:53:45.906549 16895 solver.cpp:353] Iteration 31999 (5.39468 iter/s, 18.3514s/99 iter), loss = 0.0638738
I0731 19:53:45.906575 16895 solver.cpp:375]     Train net output #0: loss = 0.0638738 (* 1 = 0.0638738 loss)
I0731 19:53:45.941938 16895 solver.cpp:680] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/initial/cityscapes5_jsegnet21v2_iter_32000.caffemodel
I0731 19:53:45.990514 16895 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/initial/cityscapes5_jsegnet21v2_iter_32000.solverstate
I0731 19:53:46.062991 16895 solver.cpp:527] Iteration 32000, loss = 0.06889
I0731 19:53:46.063015 16895 solver.cpp:550] Iteration 32000, Testing net (#0)
I0731 19:53:56.963451 16893 data_reader.cpp:264] Starting prefetch of epoch 4
I0731 19:53:57.428248 16895 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.949015
I0731 19:53:57.428269 16895 solver.cpp:635]     Test net output #1: accuracy/top5 = 1
I0731 19:53:57.428277 16895 solver.cpp:635]     Test net output #2: loss = 0.159448 (* 1 = 0.159448 loss)
I0731 19:53:57.465728 16855 parallel.cpp:73] Root Solver performance on device 0: 5.198 * 6 = 31.19 img/sec (32000 itr in 6156 sec)
I0731 19:53:57.465749 16855 parallel.cpp:78]      Solver performance on device 1: 5.198 * 6 = 31.19 img/sec (32000 itr in 6156 sec)
I0731 19:53:57.465754 16855 parallel.cpp:78]      Solver performance on device 2: 5.198 * 6 = 31.19 img/sec (32000 itr in 6156 sec)
I0731 19:53:57.465755 16855 parallel.cpp:81] Overall multi-GPU performance: 93.5672 img/sec
I0731 19:53:58.702842 16855 caffe.cpp:247] Optimization Done in 1h 42m 53s
training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/l1reg
I0731 19:54:14.041030   830 caffe.cpp:608] This is NVCaffe 0.16.3 started at Mon Jul 31 19:54:13 2017
I0731 19:54:14.043110   830 caffe.cpp:611] CuDNN version: 6021
I0731 19:54:14.043138   830 caffe.cpp:612] CuBLAS version: 8000
I0731 19:54:14.043148   830 caffe.cpp:613] CUDA version: 8000
I0731 19:54:14.043159   830 caffe.cpp:614] CUDA driver version: 8000
I0731 19:54:14.369033   830 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0731 19:54:14.369599   830 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I0731 19:54:14.370120   830 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[1]: total=8508145664 free=8379236352
I0731 19:54:14.370630   830 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[2]: total=8508145664 free=8379236352
I0731 19:54:14.370638   830 caffe.cpp:208] Using GPUs 0, 1, 2
I0731 19:54:14.370959   830 caffe.cpp:213] GPU 0: GeForce GTX 1080
I0731 19:54:14.371280   830 caffe.cpp:213] GPU 1: GeForce GTX 1080
I0731 19:54:14.371605   830 caffe.cpp:213] GPU 2: GeForce GTX 1080
I0731 19:54:14.372409   830 solver.cpp:42] Solver data type: FLOAT
I0731 19:54:14.372448   830 solver.cpp:45] Initializing solver from parameters: 
train_net: "training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/l1reg/train.prototxt"
test_net: "training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/l1reg/test.prototxt"
test_iter: 125
test_interval: 2000
base_lr: 1e-05
display: 100
max_iter: 32000
lr_policy: "multistep"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 1e-05
snapshot: 10000
snapshot_prefix: "training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/l1reg/cityscapes5_jsegnet21v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
regularization_type: "L1"
test_initialization: false
stepvalue: 24000
iter_size: 1
type: "Adam"
I0731 19:54:14.395118   830 solver.cpp:77] Creating training net from train_net file: training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/l1reg/train.prototxt
I0731 19:54:14.398308   830 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0731 19:54:14.398347   830 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
W0731 19:54:14.398500   830 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0731 19:54:14.399991   830 net.cpp:72] Initializing net from parameters: 
name: "jsegnet21v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/train-image-lmdb"
    label_list_path: "data/train-label-lmdb"
    batch_size: 6
    shuffle: false
    threads: 1
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
I0731 19:54:14.400849   830 net.cpp:104] Using FLOAT as default forward math type
I0731 19:54:14.400892   830 net.cpp:110] Using FLOAT as default backward math type
I0731 19:54:14.400914   830 layer_factory.hpp:136] Creating layer 'data' of type 'ImageLabelData'
I0731 19:54:14.400941   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:14.401002   830 net.cpp:184] Created Layer data (0)
I0731 19:54:14.401028   830 net.cpp:530] data -> data
I0731 19:54:14.401197   830 net.cpp:530] data -> label
I0731 19:54:14.402330   830 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0731 19:54:14.402423   830 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 19:54:14.437793   892 db_lmdb.cpp:35] Opened lmdb data/train-image-lmdb
I0731 19:54:14.447075   830 data_layer.cpp:184] [0] ReshapePrefetch 6, 3, 640, 640
I0731 19:54:14.447247   830 data_layer.cpp:208] [0] Output data size: 6, 3, 640, 640
I0731 19:54:14.447273   830 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 19:54:14.447398   830 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0731 19:54:14.447438   830 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 19:54:14.449883   893 data_layer.cpp:97] [0] Parser threads: 1
I0731 19:54:14.449940   893 data_layer.cpp:99] [0] Transformer threads: 1
I0731 19:54:14.485596   894 db_lmdb.cpp:35] Opened lmdb data/train-label-lmdb
I0731 19:54:14.486945   830 data_layer.cpp:184] [0] ReshapePrefetch 6, 1, 640, 640
I0731 19:54:14.487015   830 data_layer.cpp:208] [0] Output data size: 6, 1, 640, 640
I0731 19:54:14.487025   830 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 19:54:14.487088   830 net.cpp:245] Setting up data
I0731 19:54:14.487104   830 net.cpp:252] TRAIN Top shape for layer 0 'data' 6 3 640 640 (7372800)
I0731 19:54:14.487123   830 net.cpp:252] TRAIN Top shape for layer 0 'data' 6 1 640 640 (2457600)
I0731 19:54:14.487133   830 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0731 19:54:14.487143   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:14.487165   830 net.cpp:184] Created Layer data/bias (1)
I0731 19:54:14.487174   830 net.cpp:561] data/bias <- data
I0731 19:54:14.487187   830 net.cpp:530] data/bias -> data/bias
I0731 19:54:14.488534   895 data_layer.cpp:97] [0] Parser threads: 1
I0731 19:54:14.488556   895 data_layer.cpp:99] [0] Transformer threads: 1
I0731 19:54:14.493466   830 net.cpp:245] Setting up data/bias
I0731 19:54:14.493525   830 net.cpp:252] TRAIN Top shape for layer 1 'data/bias' 6 3 640 640 (7372800)
I0731 19:54:14.493557   830 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0731 19:54:14.493567   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:14.493638   830 net.cpp:184] Created Layer conv1a (2)
I0731 19:54:14.493645   830 net.cpp:561] conv1a <- data/bias
I0731 19:54:14.493655   830 net.cpp:530] conv1a -> conv1a
I0731 19:54:15.228180   830 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 7.9G, req 0G)
I0731 19:54:15.228217   830 net.cpp:245] Setting up conv1a
I0731 19:54:15.228229   830 net.cpp:252] TRAIN Top shape for layer 2 'conv1a' 6 32 320 320 (19660800)
I0731 19:54:15.228245   830 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0731 19:54:15.228253   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.228269   830 net.cpp:184] Created Layer conv1a/bn (3)
I0731 19:54:15.228278   830 net.cpp:561] conv1a/bn <- conv1a
I0731 19:54:15.228286   830 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0731 19:54:15.229715   830 net.cpp:245] Setting up conv1a/bn
I0731 19:54:15.229735   830 net.cpp:252] TRAIN Top shape for layer 3 'conv1a/bn' 6 32 320 320 (19660800)
I0731 19:54:15.229751   830 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0731 19:54:15.229758   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.229768   830 net.cpp:184] Created Layer conv1a/relu (4)
I0731 19:54:15.229774   830 net.cpp:561] conv1a/relu <- conv1a
I0731 19:54:15.229784   830 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0731 19:54:15.229807   830 net.cpp:245] Setting up conv1a/relu
I0731 19:54:15.229815   830 net.cpp:252] TRAIN Top shape for layer 4 'conv1a/relu' 6 32 320 320 (19660800)
I0731 19:54:15.229820   830 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0731 19:54:15.229826   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.229851   830 net.cpp:184] Created Layer conv1b (5)
I0731 19:54:15.229857   830 net.cpp:561] conv1b <- conv1a
I0731 19:54:15.229863   830 net.cpp:530] conv1b -> conv1b
I0731 19:54:15.289197   830 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.73G, req 0G)
I0731 19:54:15.289222   830 net.cpp:245] Setting up conv1b
I0731 19:54:15.289229   830 net.cpp:252] TRAIN Top shape for layer 5 'conv1b' 6 32 320 320 (19660800)
I0731 19:54:15.289242   830 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0731 19:54:15.289247   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.289258   830 net.cpp:184] Created Layer conv1b/bn (6)
I0731 19:54:15.289263   830 net.cpp:561] conv1b/bn <- conv1b
I0731 19:54:15.289266   830 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0731 19:54:15.290153   830 net.cpp:245] Setting up conv1b/bn
I0731 19:54:15.290165   830 net.cpp:252] TRAIN Top shape for layer 6 'conv1b/bn' 6 32 320 320 (19660800)
I0731 19:54:15.290175   830 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0731 19:54:15.290180   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.290185   830 net.cpp:184] Created Layer conv1b/relu (7)
I0731 19:54:15.290189   830 net.cpp:561] conv1b/relu <- conv1b
I0731 19:54:15.290194   830 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0731 19:54:15.290199   830 net.cpp:245] Setting up conv1b/relu
I0731 19:54:15.290205   830 net.cpp:252] TRAIN Top shape for layer 7 'conv1b/relu' 6 32 320 320 (19660800)
I0731 19:54:15.290207   830 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0731 19:54:15.290211   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.290221   830 net.cpp:184] Created Layer pool1 (8)
I0731 19:54:15.290226   830 net.cpp:561] pool1 <- conv1b
I0731 19:54:15.290230   830 net.cpp:530] pool1 -> pool1
I0731 19:54:15.290822   830 net.cpp:245] Setting up pool1
I0731 19:54:15.290834   830 net.cpp:252] TRAIN Top shape for layer 8 'pool1' 6 32 160 160 (4915200)
I0731 19:54:15.290839   830 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0731 19:54:15.290855   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.290870   830 net.cpp:184] Created Layer res2a_branch2a (9)
I0731 19:54:15.290875   830 net.cpp:561] res2a_branch2a <- pool1
I0731 19:54:15.290880   830 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0731 19:54:15.331277   830 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.61G, req 0G)
I0731 19:54:15.331349   830 net.cpp:245] Setting up res2a_branch2a
I0731 19:54:15.331373   830 net.cpp:252] TRAIN Top shape for layer 9 'res2a_branch2a' 6 64 160 160 (9830400)
I0731 19:54:15.331406   830 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0731 19:54:15.331424   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.331451   830 net.cpp:184] Created Layer res2a_branch2a/bn (10)
I0731 19:54:15.331470   830 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0731 19:54:15.331508   830 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0731 19:54:15.336916   830 net.cpp:245] Setting up res2a_branch2a/bn
I0731 19:54:15.336966   830 net.cpp:252] TRAIN Top shape for layer 10 'res2a_branch2a/bn' 6 64 160 160 (9830400)
I0731 19:54:15.337000   830 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0731 19:54:15.337014   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.337033   830 net.cpp:184] Created Layer res2a_branch2a/relu (11)
I0731 19:54:15.337050   830 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0731 19:54:15.337065   830 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0731 19:54:15.337090   830 net.cpp:245] Setting up res2a_branch2a/relu
I0731 19:54:15.337113   830 net.cpp:252] TRAIN Top shape for layer 11 'res2a_branch2a/relu' 6 64 160 160 (9830400)
I0731 19:54:15.337141   830 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0731 19:54:15.337167   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.337215   830 net.cpp:184] Created Layer res2a_branch2b (12)
I0731 19:54:15.337239   830 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0731 19:54:15.337263   830 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0731 19:54:15.370810   830 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.52G, req 0G)
I0731 19:54:15.370877   830 net.cpp:245] Setting up res2a_branch2b
I0731 19:54:15.370898   830 net.cpp:252] TRAIN Top shape for layer 12 'res2a_branch2b' 6 64 160 160 (9830400)
I0731 19:54:15.370924   830 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0731 19:54:15.370939   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.370965   830 net.cpp:184] Created Layer res2a_branch2b/bn (13)
I0731 19:54:15.370981   830 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0731 19:54:15.370996   830 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0731 19:54:15.374055   830 net.cpp:245] Setting up res2a_branch2b/bn
I0731 19:54:15.374095   830 net.cpp:252] TRAIN Top shape for layer 13 'res2a_branch2b/bn' 6 64 160 160 (9830400)
I0731 19:54:15.374125   830 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0731 19:54:15.374138   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.374155   830 net.cpp:184] Created Layer res2a_branch2b/relu (14)
I0731 19:54:15.374167   830 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0731 19:54:15.374179   830 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0731 19:54:15.374204   830 net.cpp:245] Setting up res2a_branch2b/relu
I0731 19:54:15.374222   830 net.cpp:252] TRAIN Top shape for layer 14 'res2a_branch2b/relu' 6 64 160 160 (9830400)
I0731 19:54:15.374274   830 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0731 19:54:15.374289   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.374310   830 net.cpp:184] Created Layer pool2 (15)
I0731 19:54:15.374325   830 net.cpp:561] pool2 <- res2a_branch2b
I0731 19:54:15.374336   830 net.cpp:530] pool2 -> pool2
I0731 19:54:15.374637   830 net.cpp:245] Setting up pool2
I0731 19:54:15.374662   830 net.cpp:252] TRAIN Top shape for layer 15 'pool2' 6 64 80 80 (2457600)
I0731 19:54:15.374675   830 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0731 19:54:15.374687   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.374718   830 net.cpp:184] Created Layer res3a_branch2a (16)
I0731 19:54:15.374732   830 net.cpp:561] res3a_branch2a <- pool2
I0731 19:54:15.374744   830 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0731 19:54:15.405437   830 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 7.46G, req 0G)
I0731 19:54:15.405463   830 net.cpp:245] Setting up res3a_branch2a
I0731 19:54:15.405472   830 net.cpp:252] TRAIN Top shape for layer 16 'res3a_branch2a' 6 128 80 80 (4915200)
I0731 19:54:15.405481   830 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0731 19:54:15.405488   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.405498   830 net.cpp:184] Created Layer res3a_branch2a/bn (17)
I0731 19:54:15.405503   830 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0731 19:54:15.405508   830 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0731 19:54:15.406613   830 net.cpp:245] Setting up res3a_branch2a/bn
I0731 19:54:15.406626   830 net.cpp:252] TRAIN Top shape for layer 17 'res3a_branch2a/bn' 6 128 80 80 (4915200)
I0731 19:54:15.406639   830 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0731 19:54:15.406644   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.406651   830 net.cpp:184] Created Layer res3a_branch2a/relu (18)
I0731 19:54:15.406654   830 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0731 19:54:15.406658   830 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0731 19:54:15.406666   830 net.cpp:245] Setting up res3a_branch2a/relu
I0731 19:54:15.406671   830 net.cpp:252] TRAIN Top shape for layer 18 'res3a_branch2a/relu' 6 128 80 80 (4915200)
I0731 19:54:15.406677   830 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0731 19:54:15.406680   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.406692   830 net.cpp:184] Created Layer res3a_branch2b (19)
I0731 19:54:15.406697   830 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0731 19:54:15.406700   830 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0731 19:54:15.419273   830 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.42G, req 0G)
I0731 19:54:15.419293   830 net.cpp:245] Setting up res3a_branch2b
I0731 19:54:15.419301   830 net.cpp:252] TRAIN Top shape for layer 19 'res3a_branch2b' 6 128 80 80 (4915200)
I0731 19:54:15.419309   830 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0731 19:54:15.419314   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.419322   830 net.cpp:184] Created Layer res3a_branch2b/bn (20)
I0731 19:54:15.419327   830 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0731 19:54:15.419332   830 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0731 19:54:15.420379   830 net.cpp:245] Setting up res3a_branch2b/bn
I0731 19:54:15.420392   830 net.cpp:252] TRAIN Top shape for layer 20 'res3a_branch2b/bn' 6 128 80 80 (4915200)
I0731 19:54:15.420403   830 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0731 19:54:15.420426   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.420434   830 net.cpp:184] Created Layer res3a_branch2b/relu (21)
I0731 19:54:15.420441   830 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0731 19:54:15.420445   830 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0731 19:54:15.420454   830 net.cpp:245] Setting up res3a_branch2b/relu
I0731 19:54:15.420459   830 net.cpp:252] TRAIN Top shape for layer 21 'res3a_branch2b/relu' 6 128 80 80 (4915200)
I0731 19:54:15.420462   830 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0731 19:54:15.420466   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.420473   830 net.cpp:184] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (22)
I0731 19:54:15.420477   830 net.cpp:561] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0731 19:54:15.420481   830 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0731 19:54:15.420487   830 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0731 19:54:15.420570   830 net.cpp:245] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0731 19:54:15.420578   830 net.cpp:252] TRAIN Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 6 128 80 80 (4915200)
I0731 19:54:15.420583   830 net.cpp:252] TRAIN Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 6 128 80 80 (4915200)
I0731 19:54:15.420588   830 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0731 19:54:15.420591   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.420598   830 net.cpp:184] Created Layer pool3 (23)
I0731 19:54:15.420603   830 net.cpp:561] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0731 19:54:15.420606   830 net.cpp:530] pool3 -> pool3
I0731 19:54:15.420711   830 net.cpp:245] Setting up pool3
I0731 19:54:15.420718   830 net.cpp:252] TRAIN Top shape for layer 23 'pool3' 6 128 40 40 (1228800)
I0731 19:54:15.420722   830 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0731 19:54:15.420727   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.420737   830 net.cpp:184] Created Layer res4a_branch2a (24)
I0731 19:54:15.420742   830 net.cpp:561] res4a_branch2a <- pool3
I0731 19:54:15.420745   830 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0731 19:54:15.454989   830 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.38G, req 0G)
I0731 19:54:15.455009   830 net.cpp:245] Setting up res4a_branch2a
I0731 19:54:15.455016   830 net.cpp:252] TRAIN Top shape for layer 24 'res4a_branch2a' 6 256 40 40 (2457600)
I0731 19:54:15.455024   830 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0731 19:54:15.455029   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.455044   830 net.cpp:184] Created Layer res4a_branch2a/bn (25)
I0731 19:54:15.455049   830 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0731 19:54:15.455052   830 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0731 19:54:15.455919   830 net.cpp:245] Setting up res4a_branch2a/bn
I0731 19:54:15.455929   830 net.cpp:252] TRAIN Top shape for layer 25 'res4a_branch2a/bn' 6 256 40 40 (2457600)
I0731 19:54:15.455936   830 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0731 19:54:15.455940   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.455945   830 net.cpp:184] Created Layer res4a_branch2a/relu (26)
I0731 19:54:15.455948   830 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0731 19:54:15.455951   830 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0731 19:54:15.455970   830 net.cpp:245] Setting up res4a_branch2a/relu
I0731 19:54:15.455979   830 net.cpp:252] TRAIN Top shape for layer 26 'res4a_branch2a/relu' 6 256 40 40 (2457600)
I0731 19:54:15.455983   830 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0731 19:54:15.455986   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.455996   830 net.cpp:184] Created Layer res4a_branch2b (27)
I0731 19:54:15.455998   830 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0731 19:54:15.456002   830 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0731 19:54:15.466856   830 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.36G, req 0G)
I0731 19:54:15.466873   830 net.cpp:245] Setting up res4a_branch2b
I0731 19:54:15.466879   830 net.cpp:252] TRAIN Top shape for layer 27 'res4a_branch2b' 6 256 40 40 (2457600)
I0731 19:54:15.466886   830 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0731 19:54:15.466889   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.466897   830 net.cpp:184] Created Layer res4a_branch2b/bn (28)
I0731 19:54:15.466900   830 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0731 19:54:15.466904   830 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0731 19:54:15.467813   830 net.cpp:245] Setting up res4a_branch2b/bn
I0731 19:54:15.467823   830 net.cpp:252] TRAIN Top shape for layer 28 'res4a_branch2b/bn' 6 256 40 40 (2457600)
I0731 19:54:15.467831   830 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0731 19:54:15.467834   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.467839   830 net.cpp:184] Created Layer res4a_branch2b/relu (29)
I0731 19:54:15.467842   830 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0731 19:54:15.467845   830 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0731 19:54:15.467849   830 net.cpp:245] Setting up res4a_branch2b/relu
I0731 19:54:15.467854   830 net.cpp:252] TRAIN Top shape for layer 29 'res4a_branch2b/relu' 6 256 40 40 (2457600)
I0731 19:54:15.467856   830 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0731 19:54:15.467859   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.467866   830 net.cpp:184] Created Layer pool4 (30)
I0731 19:54:15.467869   830 net.cpp:561] pool4 <- res4a_branch2b
I0731 19:54:15.467874   830 net.cpp:530] pool4 -> pool4
I0731 19:54:15.467958   830 net.cpp:245] Setting up pool4
I0731 19:54:15.467964   830 net.cpp:252] TRAIN Top shape for layer 30 'pool4' 6 256 40 40 (2457600)
I0731 19:54:15.467968   830 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0731 19:54:15.467972   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.467985   830 net.cpp:184] Created Layer res5a_branch2a (31)
I0731 19:54:15.467989   830 net.cpp:561] res5a_branch2a <- pool4
I0731 19:54:15.467993   830 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0731 19:54:15.496901   830 net.cpp:245] Setting up res5a_branch2a
I0731 19:54:15.496920   830 net.cpp:252] TRAIN Top shape for layer 31 'res5a_branch2a' 6 512 40 40 (4915200)
I0731 19:54:15.496927   830 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0731 19:54:15.496930   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.496938   830 net.cpp:184] Created Layer res5a_branch2a/bn (32)
I0731 19:54:15.496942   830 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0731 19:54:15.496944   830 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0731 19:54:15.497577   830 net.cpp:245] Setting up res5a_branch2a/bn
I0731 19:54:15.497586   830 net.cpp:252] TRAIN Top shape for layer 32 'res5a_branch2a/bn' 6 512 40 40 (4915200)
I0731 19:54:15.497601   830 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0731 19:54:15.497604   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.497607   830 net.cpp:184] Created Layer res5a_branch2a/relu (33)
I0731 19:54:15.497609   830 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0731 19:54:15.497612   830 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0731 19:54:15.497617   830 net.cpp:245] Setting up res5a_branch2a/relu
I0731 19:54:15.497618   830 net.cpp:252] TRAIN Top shape for layer 33 'res5a_branch2a/relu' 6 512 40 40 (4915200)
I0731 19:54:15.497620   830 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0731 19:54:15.497623   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.497629   830 net.cpp:184] Created Layer res5a_branch2b (34)
I0731 19:54:15.497632   830 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0731 19:54:15.497634   830 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0731 19:54:15.510490   830 net.cpp:245] Setting up res5a_branch2b
I0731 19:54:15.510500   830 net.cpp:252] TRAIN Top shape for layer 34 'res5a_branch2b' 6 512 40 40 (4915200)
I0731 19:54:15.510507   830 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0731 19:54:15.510510   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.510514   830 net.cpp:184] Created Layer res5a_branch2b/bn (35)
I0731 19:54:15.510517   830 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0731 19:54:15.510519   830 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0731 19:54:15.511140   830 net.cpp:245] Setting up res5a_branch2b/bn
I0731 19:54:15.511147   830 net.cpp:252] TRAIN Top shape for layer 35 'res5a_branch2b/bn' 6 512 40 40 (4915200)
I0731 19:54:15.511152   830 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0731 19:54:15.511155   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.511158   830 net.cpp:184] Created Layer res5a_branch2b/relu (36)
I0731 19:54:15.511160   830 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0731 19:54:15.511162   830 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0731 19:54:15.511165   830 net.cpp:245] Setting up res5a_branch2b/relu
I0731 19:54:15.511168   830 net.cpp:252] TRAIN Top shape for layer 36 'res5a_branch2b/relu' 6 512 40 40 (4915200)
I0731 19:54:15.511170   830 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I0731 19:54:15.511173   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.511183   830 net.cpp:184] Created Layer out5a (37)
I0731 19:54:15.511185   830 net.cpp:561] out5a <- res5a_branch2b
I0731 19:54:15.511188   830 net.cpp:530] out5a -> out5a
I0731 19:54:15.515357   830 net.cpp:245] Setting up out5a
I0731 19:54:15.515367   830 net.cpp:252] TRAIN Top shape for layer 37 'out5a' 6 64 40 40 (614400)
I0731 19:54:15.515372   830 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I0731 19:54:15.515374   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.515384   830 net.cpp:184] Created Layer out5a/bn (38)
I0731 19:54:15.515388   830 net.cpp:561] out5a/bn <- out5a
I0731 19:54:15.515389   830 net.cpp:513] out5a/bn -> out5a (in-place)
I0731 19:54:15.516047   830 net.cpp:245] Setting up out5a/bn
I0731 19:54:15.516053   830 net.cpp:252] TRAIN Top shape for layer 38 'out5a/bn' 6 64 40 40 (614400)
I0731 19:54:15.516059   830 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I0731 19:54:15.516062   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.516064   830 net.cpp:184] Created Layer out5a/relu (39)
I0731 19:54:15.516067   830 net.cpp:561] out5a/relu <- out5a
I0731 19:54:15.516068   830 net.cpp:513] out5a/relu -> out5a (in-place)
I0731 19:54:15.516080   830 net.cpp:245] Setting up out5a/relu
I0731 19:54:15.516083   830 net.cpp:252] TRAIN Top shape for layer 39 'out5a/relu' 6 64 40 40 (614400)
I0731 19:54:15.516085   830 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I0731 19:54:15.516088   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.516099   830 net.cpp:184] Created Layer out5a_up2 (40)
I0731 19:54:15.516103   830 net.cpp:561] out5a_up2 <- out5a
I0731 19:54:15.516104   830 net.cpp:530] out5a_up2 -> out5a_up2
I0731 19:54:15.516396   830 net.cpp:245] Setting up out5a_up2
I0731 19:54:15.516402   830 net.cpp:252] TRAIN Top shape for layer 40 'out5a_up2' 6 64 80 80 (2457600)
I0731 19:54:15.516405   830 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I0731 19:54:15.516407   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.516423   830 net.cpp:184] Created Layer out3a (41)
I0731 19:54:15.516427   830 net.cpp:561] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0731 19:54:15.516429   830 net.cpp:530] out3a -> out3a
I0731 19:54:15.529728   830 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 7.3G, req 0G)
I0731 19:54:15.529739   830 net.cpp:245] Setting up out3a
I0731 19:54:15.529744   830 net.cpp:252] TRAIN Top shape for layer 41 'out3a' 6 64 80 80 (2457600)
I0731 19:54:15.529748   830 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I0731 19:54:15.529750   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.529755   830 net.cpp:184] Created Layer out3a/bn (42)
I0731 19:54:15.529757   830 net.cpp:561] out3a/bn <- out3a
I0731 19:54:15.529760   830 net.cpp:513] out3a/bn -> out3a (in-place)
I0731 19:54:15.530438   830 net.cpp:245] Setting up out3a/bn
I0731 19:54:15.530447   830 net.cpp:252] TRAIN Top shape for layer 42 'out3a/bn' 6 64 80 80 (2457600)
I0731 19:54:15.530452   830 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I0731 19:54:15.530454   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.530457   830 net.cpp:184] Created Layer out3a/relu (43)
I0731 19:54:15.530460   830 net.cpp:561] out3a/relu <- out3a
I0731 19:54:15.530462   830 net.cpp:513] out3a/relu -> out3a (in-place)
I0731 19:54:15.530465   830 net.cpp:245] Setting up out3a/relu
I0731 19:54:15.530468   830 net.cpp:252] TRAIN Top shape for layer 43 'out3a/relu' 6 64 80 80 (2457600)
I0731 19:54:15.530470   830 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I0731 19:54:15.530472   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.530933   830 net.cpp:184] Created Layer out3_out5_combined (44)
I0731 19:54:15.530942   830 net.cpp:561] out3_out5_combined <- out5a_up2
I0731 19:54:15.530946   830 net.cpp:561] out3_out5_combined <- out3a
I0731 19:54:15.530948   830 net.cpp:530] out3_out5_combined -> out3_out5_combined
I0731 19:54:15.531929   830 net.cpp:245] Setting up out3_out5_combined
I0731 19:54:15.531937   830 net.cpp:252] TRAIN Top shape for layer 44 'out3_out5_combined' 6 64 80 80 (2457600)
I0731 19:54:15.531940   830 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I0731 19:54:15.531944   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.531949   830 net.cpp:184] Created Layer ctx_conv1 (45)
I0731 19:54:15.531951   830 net.cpp:561] ctx_conv1 <- out3_out5_combined
I0731 19:54:15.531955   830 net.cpp:530] ctx_conv1 -> ctx_conv1
I0731 19:54:15.545760   830 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 7.25G, req 0G)
I0731 19:54:15.545786   830 net.cpp:245] Setting up ctx_conv1
I0731 19:54:15.545794   830 net.cpp:252] TRAIN Top shape for layer 45 'ctx_conv1' 6 64 80 80 (2457600)
I0731 19:54:15.545826   830 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I0731 19:54:15.545833   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.545845   830 net.cpp:184] Created Layer ctx_conv1/bn (46)
I0731 19:54:15.545850   830 net.cpp:561] ctx_conv1/bn <- ctx_conv1
I0731 19:54:15.545855   830 net.cpp:513] ctx_conv1/bn -> ctx_conv1 (in-place)
I0731 19:54:15.546903   830 net.cpp:245] Setting up ctx_conv1/bn
I0731 19:54:15.546914   830 net.cpp:252] TRAIN Top shape for layer 46 'ctx_conv1/bn' 6 64 80 80 (2457600)
I0731 19:54:15.546924   830 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I0731 19:54:15.546929   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.546936   830 net.cpp:184] Created Layer ctx_conv1/relu (47)
I0731 19:54:15.546939   830 net.cpp:561] ctx_conv1/relu <- ctx_conv1
I0731 19:54:15.546943   830 net.cpp:513] ctx_conv1/relu -> ctx_conv1 (in-place)
I0731 19:54:15.546949   830 net.cpp:245] Setting up ctx_conv1/relu
I0731 19:54:15.546955   830 net.cpp:252] TRAIN Top shape for layer 47 'ctx_conv1/relu' 6 64 80 80 (2457600)
I0731 19:54:15.546959   830 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I0731 19:54:15.546962   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.546973   830 net.cpp:184] Created Layer ctx_conv2 (48)
I0731 19:54:15.546977   830 net.cpp:561] ctx_conv2 <- ctx_conv1
I0731 19:54:15.546980   830 net.cpp:530] ctx_conv2 -> ctx_conv2
I0731 19:54:15.548498   830 net.cpp:245] Setting up ctx_conv2
I0731 19:54:15.548508   830 net.cpp:252] TRAIN Top shape for layer 48 'ctx_conv2' 6 64 80 80 (2457600)
I0731 19:54:15.548514   830 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I0731 19:54:15.548518   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.548524   830 net.cpp:184] Created Layer ctx_conv2/bn (49)
I0731 19:54:15.548527   830 net.cpp:561] ctx_conv2/bn <- ctx_conv2
I0731 19:54:15.548532   830 net.cpp:513] ctx_conv2/bn -> ctx_conv2 (in-place)
I0731 19:54:15.549443   830 net.cpp:245] Setting up ctx_conv2/bn
I0731 19:54:15.549453   830 net.cpp:252] TRAIN Top shape for layer 49 'ctx_conv2/bn' 6 64 80 80 (2457600)
I0731 19:54:15.549460   830 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I0731 19:54:15.549464   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.549469   830 net.cpp:184] Created Layer ctx_conv2/relu (50)
I0731 19:54:15.549473   830 net.cpp:561] ctx_conv2/relu <- ctx_conv2
I0731 19:54:15.549476   830 net.cpp:513] ctx_conv2/relu -> ctx_conv2 (in-place)
I0731 19:54:15.549481   830 net.cpp:245] Setting up ctx_conv2/relu
I0731 19:54:15.549484   830 net.cpp:252] TRAIN Top shape for layer 50 'ctx_conv2/relu' 6 64 80 80 (2457600)
I0731 19:54:15.549489   830 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I0731 19:54:15.549494   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.549506   830 net.cpp:184] Created Layer ctx_conv3 (51)
I0731 19:54:15.549510   830 net.cpp:561] ctx_conv3 <- ctx_conv2
I0731 19:54:15.549515   830 net.cpp:530] ctx_conv3 -> ctx_conv3
I0731 19:54:15.551018   830 net.cpp:245] Setting up ctx_conv3
I0731 19:54:15.551028   830 net.cpp:252] TRAIN Top shape for layer 51 'ctx_conv3' 6 64 80 80 (2457600)
I0731 19:54:15.551034   830 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I0731 19:54:15.551039   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.551045   830 net.cpp:184] Created Layer ctx_conv3/bn (52)
I0731 19:54:15.551050   830 net.cpp:561] ctx_conv3/bn <- ctx_conv3
I0731 19:54:15.551054   830 net.cpp:513] ctx_conv3/bn -> ctx_conv3 (in-place)
I0731 19:54:15.551951   830 net.cpp:245] Setting up ctx_conv3/bn
I0731 19:54:15.551968   830 net.cpp:252] TRAIN Top shape for layer 52 'ctx_conv3/bn' 6 64 80 80 (2457600)
I0731 19:54:15.551977   830 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I0731 19:54:15.551981   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.551986   830 net.cpp:184] Created Layer ctx_conv3/relu (53)
I0731 19:54:15.551990   830 net.cpp:561] ctx_conv3/relu <- ctx_conv3
I0731 19:54:15.551993   830 net.cpp:513] ctx_conv3/relu -> ctx_conv3 (in-place)
I0731 19:54:15.551998   830 net.cpp:245] Setting up ctx_conv3/relu
I0731 19:54:15.552003   830 net.cpp:252] TRAIN Top shape for layer 53 'ctx_conv3/relu' 6 64 80 80 (2457600)
I0731 19:54:15.552007   830 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I0731 19:54:15.552011   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.552019   830 net.cpp:184] Created Layer ctx_conv4 (54)
I0731 19:54:15.552023   830 net.cpp:561] ctx_conv4 <- ctx_conv3
I0731 19:54:15.552027   830 net.cpp:530] ctx_conv4 -> ctx_conv4
I0731 19:54:15.553649   830 net.cpp:245] Setting up ctx_conv4
I0731 19:54:15.553663   830 net.cpp:252] TRAIN Top shape for layer 54 'ctx_conv4' 6 64 80 80 (2457600)
I0731 19:54:15.553673   830 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I0731 19:54:15.553678   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.553694   830 net.cpp:184] Created Layer ctx_conv4/bn (55)
I0731 19:54:15.553701   830 net.cpp:561] ctx_conv4/bn <- ctx_conv4
I0731 19:54:15.553706   830 net.cpp:513] ctx_conv4/bn -> ctx_conv4 (in-place)
I0731 19:54:15.554651   830 net.cpp:245] Setting up ctx_conv4/bn
I0731 19:54:15.554659   830 net.cpp:252] TRAIN Top shape for layer 55 'ctx_conv4/bn' 6 64 80 80 (2457600)
I0731 19:54:15.554668   830 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I0731 19:54:15.554673   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.554678   830 net.cpp:184] Created Layer ctx_conv4/relu (56)
I0731 19:54:15.554682   830 net.cpp:561] ctx_conv4/relu <- ctx_conv4
I0731 19:54:15.554687   830 net.cpp:513] ctx_conv4/relu -> ctx_conv4 (in-place)
I0731 19:54:15.554692   830 net.cpp:245] Setting up ctx_conv4/relu
I0731 19:54:15.554697   830 net.cpp:252] TRAIN Top shape for layer 56 'ctx_conv4/relu' 6 64 80 80 (2457600)
I0731 19:54:15.554699   830 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I0731 19:54:15.554702   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.554715   830 net.cpp:184] Created Layer ctx_final (57)
I0731 19:54:15.554718   830 net.cpp:561] ctx_final <- ctx_conv4
I0731 19:54:15.554723   830 net.cpp:530] ctx_final -> ctx_final
I0731 19:54:15.570250   830 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 7.22G, req 0G)
I0731 19:54:15.570263   830 net.cpp:245] Setting up ctx_final
I0731 19:54:15.570267   830 net.cpp:252] TRAIN Top shape for layer 57 'ctx_final' 6 8 80 80 (307200)
I0731 19:54:15.570272   830 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I0731 19:54:15.570276   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.570281   830 net.cpp:184] Created Layer ctx_final/relu (58)
I0731 19:54:15.570283   830 net.cpp:561] ctx_final/relu <- ctx_final
I0731 19:54:15.570286   830 net.cpp:513] ctx_final/relu -> ctx_final (in-place)
I0731 19:54:15.570291   830 net.cpp:245] Setting up ctx_final/relu
I0731 19:54:15.570294   830 net.cpp:252] TRAIN Top shape for layer 58 'ctx_final/relu' 6 8 80 80 (307200)
I0731 19:54:15.570297   830 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I0731 19:54:15.570300   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.570314   830 net.cpp:184] Created Layer out_deconv_final_up2 (59)
I0731 19:54:15.570317   830 net.cpp:561] out_deconv_final_up2 <- ctx_final
I0731 19:54:15.570320   830 net.cpp:530] out_deconv_final_up2 -> out_deconv_final_up2
I0731 19:54:15.570632   830 net.cpp:245] Setting up out_deconv_final_up2
I0731 19:54:15.570641   830 net.cpp:252] TRAIN Top shape for layer 59 'out_deconv_final_up2' 6 8 160 160 (1228800)
I0731 19:54:15.570644   830 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I0731 19:54:15.570647   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.570652   830 net.cpp:184] Created Layer out_deconv_final_up4 (60)
I0731 19:54:15.570655   830 net.cpp:561] out_deconv_final_up4 <- out_deconv_final_up2
I0731 19:54:15.570658   830 net.cpp:530] out_deconv_final_up4 -> out_deconv_final_up4
I0731 19:54:15.570945   830 net.cpp:245] Setting up out_deconv_final_up4
I0731 19:54:15.570952   830 net.cpp:252] TRAIN Top shape for layer 60 'out_deconv_final_up4' 6 8 320 320 (4915200)
I0731 19:54:15.570956   830 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I0731 19:54:15.570960   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.570966   830 net.cpp:184] Created Layer out_deconv_final_up8 (61)
I0731 19:54:15.570968   830 net.cpp:561] out_deconv_final_up8 <- out_deconv_final_up4
I0731 19:54:15.570971   830 net.cpp:530] out_deconv_final_up8 -> out_deconv_final_up8
I0731 19:54:15.571333   830 net.cpp:245] Setting up out_deconv_final_up8
I0731 19:54:15.571341   830 net.cpp:252] TRAIN Top shape for layer 61 'out_deconv_final_up8' 6 8 640 640 (19660800)
I0731 19:54:15.571344   830 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0731 19:54:15.571347   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.571360   830 net.cpp:184] Created Layer loss (62)
I0731 19:54:15.571363   830 net.cpp:561] loss <- out_deconv_final_up8
I0731 19:54:15.571367   830 net.cpp:561] loss <- label
I0731 19:54:15.571370   830 net.cpp:530] loss -> loss
I0731 19:54:15.572674   830 net.cpp:245] Setting up loss
I0731 19:54:15.572684   830 net.cpp:252] TRAIN Top shape for layer 62 'loss' (1)
I0731 19:54:15.572686   830 net.cpp:256]     with loss weight 1
I0731 19:54:15.572690   830 net.cpp:323] loss needs backward computation.
I0731 19:54:15.572695   830 net.cpp:323] out_deconv_final_up8 needs backward computation.
I0731 19:54:15.572696   830 net.cpp:323] out_deconv_final_up4 needs backward computation.
I0731 19:54:15.572700   830 net.cpp:323] out_deconv_final_up2 needs backward computation.
I0731 19:54:15.572701   830 net.cpp:323] ctx_final/relu needs backward computation.
I0731 19:54:15.572705   830 net.cpp:323] ctx_final needs backward computation.
I0731 19:54:15.572707   830 net.cpp:323] ctx_conv4/relu needs backward computation.
I0731 19:54:15.572710   830 net.cpp:323] ctx_conv4/bn needs backward computation.
I0731 19:54:15.572713   830 net.cpp:323] ctx_conv4 needs backward computation.
I0731 19:54:15.572717   830 net.cpp:323] ctx_conv3/relu needs backward computation.
I0731 19:54:15.572721   830 net.cpp:323] ctx_conv3/bn needs backward computation.
I0731 19:54:15.572726   830 net.cpp:323] ctx_conv3 needs backward computation.
I0731 19:54:15.572728   830 net.cpp:323] ctx_conv2/relu needs backward computation.
I0731 19:54:15.572731   830 net.cpp:323] ctx_conv2/bn needs backward computation.
I0731 19:54:15.572732   830 net.cpp:323] ctx_conv2 needs backward computation.
I0731 19:54:15.572736   830 net.cpp:323] ctx_conv1/relu needs backward computation.
I0731 19:54:15.572737   830 net.cpp:323] ctx_conv1/bn needs backward computation.
I0731 19:54:15.572739   830 net.cpp:323] ctx_conv1 needs backward computation.
I0731 19:54:15.572742   830 net.cpp:323] out3_out5_combined needs backward computation.
I0731 19:54:15.572744   830 net.cpp:323] out3a/relu needs backward computation.
I0731 19:54:15.572753   830 net.cpp:323] out3a/bn needs backward computation.
I0731 19:54:15.572757   830 net.cpp:323] out3a needs backward computation.
I0731 19:54:15.572759   830 net.cpp:323] out5a_up2 needs backward computation.
I0731 19:54:15.572762   830 net.cpp:323] out5a/relu needs backward computation.
I0731 19:54:15.572764   830 net.cpp:323] out5a/bn needs backward computation.
I0731 19:54:15.572767   830 net.cpp:323] out5a needs backward computation.
I0731 19:54:15.572769   830 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0731 19:54:15.572772   830 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0731 19:54:15.572774   830 net.cpp:323] res5a_branch2b needs backward computation.
I0731 19:54:15.572777   830 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0731 19:54:15.572779   830 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0731 19:54:15.572782   830 net.cpp:323] res5a_branch2a needs backward computation.
I0731 19:54:15.572783   830 net.cpp:323] pool4 needs backward computation.
I0731 19:54:15.572787   830 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0731 19:54:15.572788   830 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0731 19:54:15.572790   830 net.cpp:323] res4a_branch2b needs backward computation.
I0731 19:54:15.572793   830 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0731 19:54:15.572795   830 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0731 19:54:15.572798   830 net.cpp:323] res4a_branch2a needs backward computation.
I0731 19:54:15.572800   830 net.cpp:323] pool3 needs backward computation.
I0731 19:54:15.572803   830 net.cpp:323] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I0731 19:54:15.572805   830 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0731 19:54:15.572808   830 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0731 19:54:15.572810   830 net.cpp:323] res3a_branch2b needs backward computation.
I0731 19:54:15.572821   830 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0731 19:54:15.572826   830 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0731 19:54:15.572830   830 net.cpp:323] res3a_branch2a needs backward computation.
I0731 19:54:15.572834   830 net.cpp:323] pool2 needs backward computation.
I0731 19:54:15.572839   830 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0731 19:54:15.572842   830 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0731 19:54:15.572845   830 net.cpp:323] res2a_branch2b needs backward computation.
I0731 19:54:15.572849   830 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0731 19:54:15.572854   830 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0731 19:54:15.572856   830 net.cpp:323] res2a_branch2a needs backward computation.
I0731 19:54:15.572860   830 net.cpp:323] pool1 needs backward computation.
I0731 19:54:15.572865   830 net.cpp:323] conv1b/relu needs backward computation.
I0731 19:54:15.572867   830 net.cpp:323] conv1b/bn needs backward computation.
I0731 19:54:15.572871   830 net.cpp:323] conv1b needs backward computation.
I0731 19:54:15.572875   830 net.cpp:323] conv1a/relu needs backward computation.
I0731 19:54:15.572877   830 net.cpp:323] conv1a/bn needs backward computation.
I0731 19:54:15.572880   830 net.cpp:323] conv1a needs backward computation.
I0731 19:54:15.572882   830 net.cpp:325] data/bias does not need backward computation.
I0731 19:54:15.572885   830 net.cpp:325] data does not need backward computation.
I0731 19:54:15.572887   830 net.cpp:367] This network produces output loss
I0731 19:54:15.572933   830 net.cpp:389] Top memory (TRAIN) required for data: 956006400 diff: 946176008
I0731 19:54:15.572937   830 net.cpp:392] Bottom memory (TRAIN) required for data: 956006400 diff: 956006400
I0731 19:54:15.572939   830 net.cpp:395] Shared (in-place) memory (TRAIN) by data: 630374400 diff: 630374400
I0731 19:54:15.572942   830 net.cpp:398] Parameters memory (TRAIN) required for data: 2692608 diff: 2692608
I0731 19:54:15.572948   830 net.cpp:401] Parameters shared memory (TRAIN) by data: 0 diff: 0
I0731 19:54:15.572952   830 net.cpp:407] Network initialization done.
I0731 19:54:15.573524   830 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/l1reg/test.prototxt
W0731 19:54:15.573621   830 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0731 19:54:15.573918   830 net.cpp:72] Initializing net from parameters: 
name: "jsegnet21v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/val-image-lmdb"
    label_list_path: "data/val-label-lmdb"
    batch_size: 2
    threads: 1
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
  accuracy_param {
    ignore_label: 255
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
    ignore_label: 255
  }
}
I0731 19:54:15.574112   830 net.cpp:104] Using FLOAT as default forward math type
I0731 19:54:15.574120   830 net.cpp:110] Using FLOAT as default backward math type
I0731 19:54:15.574121   830 layer_factory.hpp:136] Creating layer 'data' of type 'ImageLabelData'
I0731 19:54:15.574124   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.574133   830 net.cpp:184] Created Layer data (0)
I0731 19:54:15.574136   830 net.cpp:530] data -> data
I0731 19:54:15.574139   830 net.cpp:530] data -> label
I0731 19:54:15.574156   830 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0731 19:54:15.574162   830 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 19:54:15.592660   934 db_lmdb.cpp:35] Opened lmdb data/val-image-lmdb
I0731 19:54:15.594195   830 data_layer.cpp:184] (0) ReshapePrefetch 2, 3, 640, 640
I0731 19:54:15.594256   830 data_layer.cpp:208] (0) Output data size: 2, 3, 640, 640
I0731 19:54:15.594262   830 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 19:54:15.594383   830 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0731 19:54:15.594390   830 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 19:54:15.595160   935 data_layer.cpp:97] (0) Parser threads: 1
I0731 19:54:15.595171   935 data_layer.cpp:99] (0) Transformer threads: 1
I0731 19:54:15.615459   936 db_lmdb.cpp:35] Opened lmdb data/val-label-lmdb
I0731 19:54:15.616344   830 data_layer.cpp:184] (0) ReshapePrefetch 2, 1, 640, 640
I0731 19:54:15.616402   830 data_layer.cpp:208] (0) Output data size: 2, 1, 640, 640
I0731 19:54:15.616408   830 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 19:54:15.616446   830 net.cpp:245] Setting up data
I0731 19:54:15.616453   830 net.cpp:252] TEST Top shape for layer 0 'data' 2 3 640 640 (2457600)
I0731 19:54:15.616464   830 net.cpp:252] TEST Top shape for layer 0 'data' 2 1 640 640 (819200)
I0731 19:54:15.616469   830 layer_factory.hpp:136] Creating layer 'label_data_1_split' of type 'Split'
I0731 19:54:15.616474   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.616482   830 net.cpp:184] Created Layer label_data_1_split (1)
I0731 19:54:15.616485   830 net.cpp:561] label_data_1_split <- label
I0731 19:54:15.616489   830 net.cpp:530] label_data_1_split -> label_data_1_split_0
I0731 19:54:15.616494   830 net.cpp:530] label_data_1_split -> label_data_1_split_1
I0731 19:54:15.616497   830 net.cpp:530] label_data_1_split -> label_data_1_split_2
I0731 19:54:15.616580   830 net.cpp:245] Setting up label_data_1_split
I0731 19:54:15.616583   830 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0731 19:54:15.616586   830 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0731 19:54:15.616590   830 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0731 19:54:15.616592   830 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0731 19:54:15.616595   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.616601   830 net.cpp:184] Created Layer data/bias (2)
I0731 19:54:15.616605   830 net.cpp:561] data/bias <- data
I0731 19:54:15.616606   830 net.cpp:530] data/bias -> data/bias
I0731 19:54:15.617771   938 data_layer.cpp:97] (0) Parser threads: 1
I0731 19:54:15.617786   938 data_layer.cpp:99] (0) Transformer threads: 1
I0731 19:54:15.619474   830 net.cpp:245] Setting up data/bias
I0731 19:54:15.619488   830 net.cpp:252] TEST Top shape for layer 2 'data/bias' 2 3 640 640 (2457600)
I0731 19:54:15.619498   830 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0731 19:54:15.619503   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.619515   830 net.cpp:184] Created Layer conv1a (3)
I0731 19:54:15.619519   830 net.cpp:561] conv1a <- data/bias
I0731 19:54:15.619523   830 net.cpp:530] conv1a -> conv1a
I0731 19:54:15.624616   830 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.09G, req 0G)
I0731 19:54:15.624631   830 net.cpp:245] Setting up conv1a
I0731 19:54:15.624636   830 net.cpp:252] TEST Top shape for layer 3 'conv1a' 2 32 320 320 (6553600)
I0731 19:54:15.624653   830 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0731 19:54:15.624658   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.624665   830 net.cpp:184] Created Layer conv1a/bn (4)
I0731 19:54:15.624668   830 net.cpp:561] conv1a/bn <- conv1a
I0731 19:54:15.624672   830 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0731 19:54:15.625429   830 net.cpp:245] Setting up conv1a/bn
I0731 19:54:15.625437   830 net.cpp:252] TEST Top shape for layer 4 'conv1a/bn' 2 32 320 320 (6553600)
I0731 19:54:15.625444   830 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0731 19:54:15.625447   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.625455   830 net.cpp:184] Created Layer conv1a/relu (5)
I0731 19:54:15.625458   830 net.cpp:561] conv1a/relu <- conv1a
I0731 19:54:15.625460   830 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0731 19:54:15.625464   830 net.cpp:245] Setting up conv1a/relu
I0731 19:54:15.625468   830 net.cpp:252] TEST Top shape for layer 5 'conv1a/relu' 2 32 320 320 (6553600)
I0731 19:54:15.625469   830 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0731 19:54:15.625473   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.625480   830 net.cpp:184] Created Layer conv1b (6)
I0731 19:54:15.625483   830 net.cpp:561] conv1b <- conv1a
I0731 19:54:15.625485   830 net.cpp:530] conv1b -> conv1b
I0731 19:54:15.639390   830 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.06G, req 0G)
I0731 19:54:15.639415   830 net.cpp:245] Setting up conv1b
I0731 19:54:15.639420   830 net.cpp:252] TEST Top shape for layer 6 'conv1b' 2 32 320 320 (6553600)
I0731 19:54:15.639431   830 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0731 19:54:15.639436   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.639446   830 net.cpp:184] Created Layer conv1b/bn (7)
I0731 19:54:15.639451   830 net.cpp:561] conv1b/bn <- conv1b
I0731 19:54:15.639456   830 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0731 19:54:15.640745   830 net.cpp:245] Setting up conv1b/bn
I0731 19:54:15.640763   830 net.cpp:252] TEST Top shape for layer 7 'conv1b/bn' 2 32 320 320 (6553600)
I0731 19:54:15.640774   830 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0731 19:54:15.640779   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.640785   830 net.cpp:184] Created Layer conv1b/relu (8)
I0731 19:54:15.640789   830 net.cpp:561] conv1b/relu <- conv1b
I0731 19:54:15.640794   830 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0731 19:54:15.640801   830 net.cpp:245] Setting up conv1b/relu
I0731 19:54:15.640806   830 net.cpp:252] TEST Top shape for layer 8 'conv1b/relu' 2 32 320 320 (6553600)
I0731 19:54:15.640810   830 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0731 19:54:15.640828   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.640835   830 net.cpp:184] Created Layer pool1 (9)
I0731 19:54:15.640838   830 net.cpp:561] pool1 <- conv1b
I0731 19:54:15.640843   830 net.cpp:530] pool1 -> pool1
I0731 19:54:15.640950   830 net.cpp:245] Setting up pool1
I0731 19:54:15.640959   830 net.cpp:252] TEST Top shape for layer 9 'pool1' 2 32 160 160 (1638400)
I0731 19:54:15.640964   830 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0731 19:54:15.640967   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.640978   830 net.cpp:184] Created Layer res2a_branch2a (10)
I0731 19:54:15.640982   830 net.cpp:561] res2a_branch2a <- pool1
I0731 19:54:15.640986   830 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0731 19:54:15.650307   830 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 1  (limit 7.03G, req 0G)
I0731 19:54:15.650331   830 net.cpp:245] Setting up res2a_branch2a
I0731 19:54:15.650336   830 net.cpp:252] TEST Top shape for layer 10 'res2a_branch2a' 2 64 160 160 (3276800)
I0731 19:54:15.650342   830 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0731 19:54:15.650346   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.650357   830 net.cpp:184] Created Layer res2a_branch2a/bn (11)
I0731 19:54:15.650360   830 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0731 19:54:15.650364   830 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0731 19:54:15.651113   830 net.cpp:245] Setting up res2a_branch2a/bn
I0731 19:54:15.651121   830 net.cpp:252] TEST Top shape for layer 11 'res2a_branch2a/bn' 2 64 160 160 (3276800)
I0731 19:54:15.651126   830 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0731 19:54:15.651129   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.651134   830 net.cpp:184] Created Layer res2a_branch2a/relu (12)
I0731 19:54:15.651135   830 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0731 19:54:15.651139   830 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0731 19:54:15.651141   830 net.cpp:245] Setting up res2a_branch2a/relu
I0731 19:54:15.651144   830 net.cpp:252] TEST Top shape for layer 12 'res2a_branch2a/relu' 2 64 160 160 (3276800)
I0731 19:54:15.651146   830 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0731 19:54:15.651149   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.651154   830 net.cpp:184] Created Layer res2a_branch2b (13)
I0731 19:54:15.651157   830 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0731 19:54:15.651160   830 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0731 19:54:15.658759   830 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.02G, req 0G)
I0731 19:54:15.658771   830 net.cpp:245] Setting up res2a_branch2b
I0731 19:54:15.658774   830 net.cpp:252] TEST Top shape for layer 13 'res2a_branch2b' 2 64 160 160 (3276800)
I0731 19:54:15.658778   830 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0731 19:54:15.658782   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.658785   830 net.cpp:184] Created Layer res2a_branch2b/bn (14)
I0731 19:54:15.658788   830 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0731 19:54:15.658790   830 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0731 19:54:15.659502   830 net.cpp:245] Setting up res2a_branch2b/bn
I0731 19:54:15.659510   830 net.cpp:252] TEST Top shape for layer 14 'res2a_branch2b/bn' 2 64 160 160 (3276800)
I0731 19:54:15.659515   830 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0731 19:54:15.659518   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.659521   830 net.cpp:184] Created Layer res2a_branch2b/relu (15)
I0731 19:54:15.659523   830 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0731 19:54:15.659525   830 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0731 19:54:15.659529   830 net.cpp:245] Setting up res2a_branch2b/relu
I0731 19:54:15.659531   830 net.cpp:252] TEST Top shape for layer 15 'res2a_branch2b/relu' 2 64 160 160 (3276800)
I0731 19:54:15.659533   830 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0731 19:54:15.659536   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.659540   830 net.cpp:184] Created Layer pool2 (16)
I0731 19:54:15.659543   830 net.cpp:561] pool2 <- res2a_branch2b
I0731 19:54:15.659544   830 net.cpp:530] pool2 -> pool2
I0731 19:54:15.659611   830 net.cpp:245] Setting up pool2
I0731 19:54:15.659616   830 net.cpp:252] TEST Top shape for layer 16 'pool2' 2 64 80 80 (819200)
I0731 19:54:15.659626   830 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0731 19:54:15.659628   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.659639   830 net.cpp:184] Created Layer res3a_branch2a (17)
I0731 19:54:15.659641   830 net.cpp:561] res3a_branch2a <- pool2
I0731 19:54:15.659644   830 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0731 19:54:15.666054   830 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.01G, req 0G)
I0731 19:54:15.666065   830 net.cpp:245] Setting up res3a_branch2a
I0731 19:54:15.666069   830 net.cpp:252] TEST Top shape for layer 17 'res3a_branch2a' 2 128 80 80 (1638400)
I0731 19:54:15.666074   830 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0731 19:54:15.666077   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.666081   830 net.cpp:184] Created Layer res3a_branch2a/bn (18)
I0731 19:54:15.666085   830 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0731 19:54:15.666086   830 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0731 19:54:15.666787   830 net.cpp:245] Setting up res3a_branch2a/bn
I0731 19:54:15.666795   830 net.cpp:252] TEST Top shape for layer 18 'res3a_branch2a/bn' 2 128 80 80 (1638400)
I0731 19:54:15.666802   830 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0731 19:54:15.666805   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.666808   830 net.cpp:184] Created Layer res3a_branch2a/relu (19)
I0731 19:54:15.666810   830 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0731 19:54:15.666813   830 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0731 19:54:15.666816   830 net.cpp:245] Setting up res3a_branch2a/relu
I0731 19:54:15.666820   830 net.cpp:252] TEST Top shape for layer 19 'res3a_branch2a/relu' 2 128 80 80 (1638400)
I0731 19:54:15.666821   830 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0731 19:54:15.666823   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.666828   830 net.cpp:184] Created Layer res3a_branch2b (20)
I0731 19:54:15.666831   830 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0731 19:54:15.666834   830 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0731 19:54:15.671862   830 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7G, req 0G)
I0731 19:54:15.671874   830 net.cpp:245] Setting up res3a_branch2b
I0731 19:54:15.671877   830 net.cpp:252] TEST Top shape for layer 20 'res3a_branch2b' 2 128 80 80 (1638400)
I0731 19:54:15.671881   830 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0731 19:54:15.671885   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.671890   830 net.cpp:184] Created Layer res3a_branch2b/bn (21)
I0731 19:54:15.671891   830 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0731 19:54:15.671895   830 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0731 19:54:15.672601   830 net.cpp:245] Setting up res3a_branch2b/bn
I0731 19:54:15.672610   830 net.cpp:252] TEST Top shape for layer 21 'res3a_branch2b/bn' 2 128 80 80 (1638400)
I0731 19:54:15.672616   830 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0731 19:54:15.672617   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.672621   830 net.cpp:184] Created Layer res3a_branch2b/relu (22)
I0731 19:54:15.672623   830 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0731 19:54:15.672626   830 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0731 19:54:15.672629   830 net.cpp:245] Setting up res3a_branch2b/relu
I0731 19:54:15.672631   830 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b/relu' 2 128 80 80 (1638400)
I0731 19:54:15.672641   830 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0731 19:54:15.672644   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.672647   830 net.cpp:184] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (23)
I0731 19:54:15.672649   830 net.cpp:561] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0731 19:54:15.672652   830 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0731 19:54:15.672655   830 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0731 19:54:15.672701   830 net.cpp:245] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0731 19:54:15.672708   830 net.cpp:252] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 2 128 80 80 (1638400)
I0731 19:54:15.672709   830 net.cpp:252] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 2 128 80 80 (1638400)
I0731 19:54:15.672711   830 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0731 19:54:15.672714   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.672724   830 net.cpp:184] Created Layer pool3 (24)
I0731 19:54:15.672729   830 net.cpp:561] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0731 19:54:15.672732   830 net.cpp:530] pool3 -> pool3
I0731 19:54:15.672801   830 net.cpp:245] Setting up pool3
I0731 19:54:15.672806   830 net.cpp:252] TEST Top shape for layer 24 'pool3' 2 128 40 40 (409600)
I0731 19:54:15.672808   830 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0731 19:54:15.672811   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.672823   830 net.cpp:184] Created Layer res4a_branch2a (25)
I0731 19:54:15.672827   830 net.cpp:561] res4a_branch2a <- pool3
I0731 19:54:15.672830   830 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0731 19:54:15.685107   830 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.99G, req 0G)
I0731 19:54:15.685118   830 net.cpp:245] Setting up res4a_branch2a
I0731 19:54:15.685122   830 net.cpp:252] TEST Top shape for layer 25 'res4a_branch2a' 2 256 40 40 (819200)
I0731 19:54:15.685127   830 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0731 19:54:15.685128   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.685138   830 net.cpp:184] Created Layer res4a_branch2a/bn (26)
I0731 19:54:15.685142   830 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0731 19:54:15.685144   830 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0731 19:54:15.685891   830 net.cpp:245] Setting up res4a_branch2a/bn
I0731 19:54:15.685899   830 net.cpp:252] TEST Top shape for layer 26 'res4a_branch2a/bn' 2 256 40 40 (819200)
I0731 19:54:15.685904   830 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0731 19:54:15.685907   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.685910   830 net.cpp:184] Created Layer res4a_branch2a/relu (27)
I0731 19:54:15.685914   830 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0731 19:54:15.685921   830 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0731 19:54:15.685925   830 net.cpp:245] Setting up res4a_branch2a/relu
I0731 19:54:15.685927   830 net.cpp:252] TEST Top shape for layer 27 'res4a_branch2a/relu' 2 256 40 40 (819200)
I0731 19:54:15.685930   830 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0731 19:54:15.685933   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.685943   830 net.cpp:184] Created Layer res4a_branch2b (28)
I0731 19:54:15.685946   830 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0731 19:54:15.685956   830 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0731 19:54:15.694806   830 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.98G, req 0G)
I0731 19:54:15.694825   830 net.cpp:245] Setting up res4a_branch2b
I0731 19:54:15.694833   830 net.cpp:252] TEST Top shape for layer 28 'res4a_branch2b' 2 256 40 40 (819200)
I0731 19:54:15.694841   830 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0731 19:54:15.694846   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.694855   830 net.cpp:184] Created Layer res4a_branch2b/bn (29)
I0731 19:54:15.694860   830 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0731 19:54:15.694864   830 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0731 19:54:15.695830   830 net.cpp:245] Setting up res4a_branch2b/bn
I0731 19:54:15.695840   830 net.cpp:252] TEST Top shape for layer 29 'res4a_branch2b/bn' 2 256 40 40 (819200)
I0731 19:54:15.695849   830 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0731 19:54:15.695853   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.695858   830 net.cpp:184] Created Layer res4a_branch2b/relu (30)
I0731 19:54:15.695863   830 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0731 19:54:15.695865   830 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0731 19:54:15.695871   830 net.cpp:245] Setting up res4a_branch2b/relu
I0731 19:54:15.695876   830 net.cpp:252] TEST Top shape for layer 30 'res4a_branch2b/relu' 2 256 40 40 (819200)
I0731 19:54:15.695880   830 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0731 19:54:15.695884   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.695890   830 net.cpp:184] Created Layer pool4 (31)
I0731 19:54:15.695894   830 net.cpp:561] pool4 <- res4a_branch2b
I0731 19:54:15.695899   830 net.cpp:530] pool4 -> pool4
I0731 19:54:15.696002   830 net.cpp:245] Setting up pool4
I0731 19:54:15.696008   830 net.cpp:252] TEST Top shape for layer 31 'pool4' 2 256 40 40 (819200)
I0731 19:54:15.696013   830 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0731 19:54:15.696017   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.696028   830 net.cpp:184] Created Layer res5a_branch2a (32)
I0731 19:54:15.696033   830 net.cpp:561] res5a_branch2a <- pool4
I0731 19:54:15.696038   830 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0731 19:54:15.724181   830 net.cpp:245] Setting up res5a_branch2a
I0731 19:54:15.724205   830 net.cpp:252] TEST Top shape for layer 32 'res5a_branch2a' 2 512 40 40 (1638400)
I0731 19:54:15.724211   830 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0731 19:54:15.724215   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.724221   830 net.cpp:184] Created Layer res5a_branch2a/bn (33)
I0731 19:54:15.724225   830 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0731 19:54:15.724230   830 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0731 19:54:15.724921   830 net.cpp:245] Setting up res5a_branch2a/bn
I0731 19:54:15.724934   830 net.cpp:252] TEST Top shape for layer 33 'res5a_branch2a/bn' 2 512 40 40 (1638400)
I0731 19:54:15.724941   830 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0731 19:54:15.724943   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.724946   830 net.cpp:184] Created Layer res5a_branch2a/relu (34)
I0731 19:54:15.724949   830 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0731 19:54:15.724951   830 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0731 19:54:15.724956   830 net.cpp:245] Setting up res5a_branch2a/relu
I0731 19:54:15.724958   830 net.cpp:252] TEST Top shape for layer 34 'res5a_branch2a/relu' 2 512 40 40 (1638400)
I0731 19:54:15.724970   830 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0731 19:54:15.724974   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.724980   830 net.cpp:184] Created Layer res5a_branch2b (35)
I0731 19:54:15.724983   830 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0731 19:54:15.724985   830 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0731 19:54:15.738055   830 net.cpp:245] Setting up res5a_branch2b
I0731 19:54:15.738067   830 net.cpp:252] TEST Top shape for layer 35 'res5a_branch2b' 2 512 40 40 (1638400)
I0731 19:54:15.738085   830 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0731 19:54:15.738087   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.738097   830 net.cpp:184] Created Layer res5a_branch2b/bn (36)
I0731 19:54:15.738101   830 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0731 19:54:15.738103   830 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0731 19:54:15.738788   830 net.cpp:245] Setting up res5a_branch2b/bn
I0731 19:54:15.738795   830 net.cpp:252] TEST Top shape for layer 36 'res5a_branch2b/bn' 2 512 40 40 (1638400)
I0731 19:54:15.738801   830 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0731 19:54:15.738803   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.738806   830 net.cpp:184] Created Layer res5a_branch2b/relu (37)
I0731 19:54:15.738808   830 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0731 19:54:15.738811   830 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0731 19:54:15.738814   830 net.cpp:245] Setting up res5a_branch2b/relu
I0731 19:54:15.738817   830 net.cpp:252] TEST Top shape for layer 37 'res5a_branch2b/relu' 2 512 40 40 (1638400)
I0731 19:54:15.738819   830 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I0731 19:54:15.738822   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.738831   830 net.cpp:184] Created Layer out5a (38)
I0731 19:54:15.738834   830 net.cpp:561] out5a <- res5a_branch2b
I0731 19:54:15.738837   830 net.cpp:530] out5a -> out5a
I0731 19:54:15.742130   830 net.cpp:245] Setting up out5a
I0731 19:54:15.742137   830 net.cpp:252] TEST Top shape for layer 38 'out5a' 2 64 40 40 (204800)
I0731 19:54:15.742141   830 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I0731 19:54:15.742144   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.742147   830 net.cpp:184] Created Layer out5a/bn (39)
I0731 19:54:15.742151   830 net.cpp:561] out5a/bn <- out5a
I0731 19:54:15.742152   830 net.cpp:513] out5a/bn -> out5a (in-place)
I0731 19:54:15.742842   830 net.cpp:245] Setting up out5a/bn
I0731 19:54:15.742849   830 net.cpp:252] TEST Top shape for layer 39 'out5a/bn' 2 64 40 40 (204800)
I0731 19:54:15.742854   830 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I0731 19:54:15.742856   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.742859   830 net.cpp:184] Created Layer out5a/relu (40)
I0731 19:54:15.742861   830 net.cpp:561] out5a/relu <- out5a
I0731 19:54:15.742864   830 net.cpp:513] out5a/relu -> out5a (in-place)
I0731 19:54:15.742867   830 net.cpp:245] Setting up out5a/relu
I0731 19:54:15.742871   830 net.cpp:252] TEST Top shape for layer 40 'out5a/relu' 2 64 40 40 (204800)
I0731 19:54:15.742872   830 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I0731 19:54:15.742874   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.742878   830 net.cpp:184] Created Layer out5a_up2 (41)
I0731 19:54:15.742880   830 net.cpp:561] out5a_up2 <- out5a
I0731 19:54:15.742882   830 net.cpp:530] out5a_up2 -> out5a_up2
I0731 19:54:15.743196   830 net.cpp:245] Setting up out5a_up2
I0731 19:54:15.743202   830 net.cpp:252] TEST Top shape for layer 41 'out5a_up2' 2 64 80 80 (819200)
I0731 19:54:15.743206   830 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I0731 19:54:15.743207   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.743213   830 net.cpp:184] Created Layer out3a (42)
I0731 19:54:15.743216   830 net.cpp:561] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0731 19:54:15.743219   830 net.cpp:530] out3a -> out3a
I0731 19:54:15.747632   830 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.97G, req 0G)
I0731 19:54:15.747648   830 net.cpp:245] Setting up out3a
I0731 19:54:15.747654   830 net.cpp:252] TEST Top shape for layer 42 'out3a' 2 64 80 80 (819200)
I0731 19:54:15.747661   830 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I0731 19:54:15.747668   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.747676   830 net.cpp:184] Created Layer out3a/bn (43)
I0731 19:54:15.747681   830 net.cpp:561] out3a/bn <- out3a
I0731 19:54:15.747686   830 net.cpp:513] out3a/bn -> out3a (in-place)
I0731 19:54:15.748723   830 net.cpp:245] Setting up out3a/bn
I0731 19:54:15.748733   830 net.cpp:252] TEST Top shape for layer 43 'out3a/bn' 2 64 80 80 (819200)
I0731 19:54:15.748742   830 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I0731 19:54:15.748745   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.748750   830 net.cpp:184] Created Layer out3a/relu (44)
I0731 19:54:15.748754   830 net.cpp:561] out3a/relu <- out3a
I0731 19:54:15.748759   830 net.cpp:513] out3a/relu -> out3a (in-place)
I0731 19:54:15.748764   830 net.cpp:245] Setting up out3a/relu
I0731 19:54:15.748769   830 net.cpp:252] TEST Top shape for layer 44 'out3a/relu' 2 64 80 80 (819200)
I0731 19:54:15.748772   830 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I0731 19:54:15.748776   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.748780   830 net.cpp:184] Created Layer out3_out5_combined (45)
I0731 19:54:15.748785   830 net.cpp:561] out3_out5_combined <- out5a_up2
I0731 19:54:15.748790   830 net.cpp:561] out3_out5_combined <- out3a
I0731 19:54:15.748793   830 net.cpp:530] out3_out5_combined -> out3_out5_combined
I0731 19:54:15.750098   830 net.cpp:245] Setting up out3_out5_combined
I0731 19:54:15.750109   830 net.cpp:252] TEST Top shape for layer 45 'out3_out5_combined' 2 64 80 80 (819200)
I0731 19:54:15.750115   830 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I0731 19:54:15.750120   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.750131   830 net.cpp:184] Created Layer ctx_conv1 (46)
I0731 19:54:15.750136   830 net.cpp:561] ctx_conv1 <- out3_out5_combined
I0731 19:54:15.750143   830 net.cpp:530] ctx_conv1 -> ctx_conv1
I0731 19:54:15.754809   830 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.96G, req 0G)
I0731 19:54:15.754822   830 net.cpp:245] Setting up ctx_conv1
I0731 19:54:15.754828   830 net.cpp:252] TEST Top shape for layer 46 'ctx_conv1' 2 64 80 80 (819200)
I0731 19:54:15.754835   830 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I0731 19:54:15.754839   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.754851   830 net.cpp:184] Created Layer ctx_conv1/bn (47)
I0731 19:54:15.754856   830 net.cpp:561] ctx_conv1/bn <- ctx_conv1
I0731 19:54:15.754860   830 net.cpp:513] ctx_conv1/bn -> ctx_conv1 (in-place)
I0731 19:54:15.755856   830 net.cpp:245] Setting up ctx_conv1/bn
I0731 19:54:15.755864   830 net.cpp:252] TEST Top shape for layer 47 'ctx_conv1/bn' 2 64 80 80 (819200)
I0731 19:54:15.755873   830 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I0731 19:54:15.755889   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.755895   830 net.cpp:184] Created Layer ctx_conv1/relu (48)
I0731 19:54:15.755899   830 net.cpp:561] ctx_conv1/relu <- ctx_conv1
I0731 19:54:15.755903   830 net.cpp:513] ctx_conv1/relu -> ctx_conv1 (in-place)
I0731 19:54:15.755909   830 net.cpp:245] Setting up ctx_conv1/relu
I0731 19:54:15.755914   830 net.cpp:252] TEST Top shape for layer 48 'ctx_conv1/relu' 2 64 80 80 (819200)
I0731 19:54:15.755918   830 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I0731 19:54:15.755921   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.755931   830 net.cpp:184] Created Layer ctx_conv2 (49)
I0731 19:54:15.755935   830 net.cpp:561] ctx_conv2 <- ctx_conv1
I0731 19:54:15.755939   830 net.cpp:530] ctx_conv2 -> ctx_conv2
I0731 19:54:15.757103   830 net.cpp:245] Setting up ctx_conv2
I0731 19:54:15.757112   830 net.cpp:252] TEST Top shape for layer 49 'ctx_conv2' 2 64 80 80 (819200)
I0731 19:54:15.757117   830 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I0731 19:54:15.757119   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.757123   830 net.cpp:184] Created Layer ctx_conv2/bn (50)
I0731 19:54:15.757127   830 net.cpp:561] ctx_conv2/bn <- ctx_conv2
I0731 19:54:15.757128   830 net.cpp:513] ctx_conv2/bn -> ctx_conv2 (in-place)
I0731 19:54:15.757840   830 net.cpp:245] Setting up ctx_conv2/bn
I0731 19:54:15.757846   830 net.cpp:252] TEST Top shape for layer 50 'ctx_conv2/bn' 2 64 80 80 (819200)
I0731 19:54:15.757853   830 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I0731 19:54:15.757855   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.757859   830 net.cpp:184] Created Layer ctx_conv2/relu (51)
I0731 19:54:15.757861   830 net.cpp:561] ctx_conv2/relu <- ctx_conv2
I0731 19:54:15.757864   830 net.cpp:513] ctx_conv2/relu -> ctx_conv2 (in-place)
I0731 19:54:15.757869   830 net.cpp:245] Setting up ctx_conv2/relu
I0731 19:54:15.757871   830 net.cpp:252] TEST Top shape for layer 51 'ctx_conv2/relu' 2 64 80 80 (819200)
I0731 19:54:15.757874   830 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I0731 19:54:15.757876   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.757881   830 net.cpp:184] Created Layer ctx_conv3 (52)
I0731 19:54:15.757884   830 net.cpp:561] ctx_conv3 <- ctx_conv2
I0731 19:54:15.757886   830 net.cpp:530] ctx_conv3 -> ctx_conv3
I0731 19:54:15.759006   830 net.cpp:245] Setting up ctx_conv3
I0731 19:54:15.759013   830 net.cpp:252] TEST Top shape for layer 52 'ctx_conv3' 2 64 80 80 (819200)
I0731 19:54:15.759018   830 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I0731 19:54:15.759021   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.759026   830 net.cpp:184] Created Layer ctx_conv3/bn (53)
I0731 19:54:15.759028   830 net.cpp:561] ctx_conv3/bn <- ctx_conv3
I0731 19:54:15.759032   830 net.cpp:513] ctx_conv3/bn -> ctx_conv3 (in-place)
I0731 19:54:15.759735   830 net.cpp:245] Setting up ctx_conv3/bn
I0731 19:54:15.759742   830 net.cpp:252] TEST Top shape for layer 53 'ctx_conv3/bn' 2 64 80 80 (819200)
I0731 19:54:15.759748   830 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I0731 19:54:15.759752   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.759754   830 net.cpp:184] Created Layer ctx_conv3/relu (54)
I0731 19:54:15.759757   830 net.cpp:561] ctx_conv3/relu <- ctx_conv3
I0731 19:54:15.759760   830 net.cpp:513] ctx_conv3/relu -> ctx_conv3 (in-place)
I0731 19:54:15.759763   830 net.cpp:245] Setting up ctx_conv3/relu
I0731 19:54:15.759766   830 net.cpp:252] TEST Top shape for layer 54 'ctx_conv3/relu' 2 64 80 80 (819200)
I0731 19:54:15.759775   830 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I0731 19:54:15.759778   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.759788   830 net.cpp:184] Created Layer ctx_conv4 (55)
I0731 19:54:15.759791   830 net.cpp:561] ctx_conv4 <- ctx_conv3
I0731 19:54:15.759793   830 net.cpp:530] ctx_conv4 -> ctx_conv4
I0731 19:54:15.760910   830 net.cpp:245] Setting up ctx_conv4
I0731 19:54:15.760918   830 net.cpp:252] TEST Top shape for layer 55 'ctx_conv4' 2 64 80 80 (819200)
I0731 19:54:15.760922   830 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I0731 19:54:15.760926   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.760931   830 net.cpp:184] Created Layer ctx_conv4/bn (56)
I0731 19:54:15.760933   830 net.cpp:561] ctx_conv4/bn <- ctx_conv4
I0731 19:54:15.760936   830 net.cpp:513] ctx_conv4/bn -> ctx_conv4 (in-place)
I0731 19:54:15.761652   830 net.cpp:245] Setting up ctx_conv4/bn
I0731 19:54:15.761658   830 net.cpp:252] TEST Top shape for layer 56 'ctx_conv4/bn' 2 64 80 80 (819200)
I0731 19:54:15.761664   830 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I0731 19:54:15.761667   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.761670   830 net.cpp:184] Created Layer ctx_conv4/relu (57)
I0731 19:54:15.761673   830 net.cpp:561] ctx_conv4/relu <- ctx_conv4
I0731 19:54:15.761677   830 net.cpp:513] ctx_conv4/relu -> ctx_conv4 (in-place)
I0731 19:54:15.761679   830 net.cpp:245] Setting up ctx_conv4/relu
I0731 19:54:15.761682   830 net.cpp:252] TEST Top shape for layer 57 'ctx_conv4/relu' 2 64 80 80 (819200)
I0731 19:54:15.761685   830 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I0731 19:54:15.761687   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.761693   830 net.cpp:184] Created Layer ctx_final (58)
I0731 19:54:15.761696   830 net.cpp:561] ctx_final <- ctx_conv4
I0731 19:54:15.761698   830 net.cpp:530] ctx_final -> ctx_final
I0731 19:54:15.766535   830 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'ctx_final' with space 0.02G/1 1  (limit 6.96G, req 0G)
I0731 19:54:15.766548   830 net.cpp:245] Setting up ctx_final
I0731 19:54:15.766553   830 net.cpp:252] TEST Top shape for layer 58 'ctx_final' 2 8 80 80 (102400)
I0731 19:54:15.766558   830 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I0731 19:54:15.766561   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.766564   830 net.cpp:184] Created Layer ctx_final/relu (59)
I0731 19:54:15.766567   830 net.cpp:561] ctx_final/relu <- ctx_final
I0731 19:54:15.766571   830 net.cpp:513] ctx_final/relu -> ctx_final (in-place)
I0731 19:54:15.766579   830 net.cpp:245] Setting up ctx_final/relu
I0731 19:54:15.766583   830 net.cpp:252] TEST Top shape for layer 59 'ctx_final/relu' 2 8 80 80 (102400)
I0731 19:54:15.766587   830 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I0731 19:54:15.766588   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.766593   830 net.cpp:184] Created Layer out_deconv_final_up2 (60)
I0731 19:54:15.766597   830 net.cpp:561] out_deconv_final_up2 <- ctx_final
I0731 19:54:15.766599   830 net.cpp:530] out_deconv_final_up2 -> out_deconv_final_up2
I0731 19:54:15.766942   830 net.cpp:245] Setting up out_deconv_final_up2
I0731 19:54:15.766949   830 net.cpp:252] TEST Top shape for layer 60 'out_deconv_final_up2' 2 8 160 160 (409600)
I0731 19:54:15.766953   830 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I0731 19:54:15.766955   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.766960   830 net.cpp:184] Created Layer out_deconv_final_up4 (61)
I0731 19:54:15.766971   830 net.cpp:561] out_deconv_final_up4 <- out_deconv_final_up2
I0731 19:54:15.766974   830 net.cpp:530] out_deconv_final_up4 -> out_deconv_final_up4
I0731 19:54:15.767274   830 net.cpp:245] Setting up out_deconv_final_up4
I0731 19:54:15.767279   830 net.cpp:252] TEST Top shape for layer 61 'out_deconv_final_up4' 2 8 320 320 (1638400)
I0731 19:54:15.767283   830 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I0731 19:54:15.767285   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.767292   830 net.cpp:184] Created Layer out_deconv_final_up8 (62)
I0731 19:54:15.767295   830 net.cpp:561] out_deconv_final_up8 <- out_deconv_final_up4
I0731 19:54:15.767297   830 net.cpp:530] out_deconv_final_up8 -> out_deconv_final_up8
I0731 19:54:15.767593   830 net.cpp:245] Setting up out_deconv_final_up8
I0731 19:54:15.767598   830 net.cpp:252] TEST Top shape for layer 62 'out_deconv_final_up8' 2 8 640 640 (6553600)
I0731 19:54:15.767601   830 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8_out_deconv_final_up8_0_split' of type 'Split'
I0731 19:54:15.767603   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.767606   830 net.cpp:184] Created Layer out_deconv_final_up8_out_deconv_final_up8_0_split (63)
I0731 19:54:15.767608   830 net.cpp:561] out_deconv_final_up8_out_deconv_final_up8_0_split <- out_deconv_final_up8
I0731 19:54:15.767611   830 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0731 19:54:15.767614   830 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0731 19:54:15.767616   830 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0731 19:54:15.767680   830 net.cpp:245] Setting up out_deconv_final_up8_out_deconv_final_up8_0_split
I0731 19:54:15.767685   830 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 8 640 640 (6553600)
I0731 19:54:15.767688   830 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 8 640 640 (6553600)
I0731 19:54:15.767690   830 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 8 640 640 (6553600)
I0731 19:54:15.767693   830 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0731 19:54:15.767694   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.767699   830 net.cpp:184] Created Layer loss (64)
I0731 19:54:15.767702   830 net.cpp:561] loss <- out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0731 19:54:15.767704   830 net.cpp:561] loss <- label_data_1_split_0
I0731 19:54:15.767707   830 net.cpp:530] loss -> loss
I0731 19:54:15.768566   830 net.cpp:245] Setting up loss
I0731 19:54:15.768575   830 net.cpp:252] TEST Top shape for layer 64 'loss' (1)
I0731 19:54:15.768579   830 net.cpp:256]     with loss weight 1
I0731 19:54:15.768582   830 layer_factory.hpp:136] Creating layer 'accuracy/top1' of type 'Accuracy'
I0731 19:54:15.768585   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.768591   830 net.cpp:184] Created Layer accuracy/top1 (65)
I0731 19:54:15.768594   830 net.cpp:561] accuracy/top1 <- out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0731 19:54:15.768596   830 net.cpp:561] accuracy/top1 <- label_data_1_split_1
I0731 19:54:15.768599   830 net.cpp:530] accuracy/top1 -> accuracy/top1
I0731 19:54:15.768604   830 net.cpp:245] Setting up accuracy/top1
I0731 19:54:15.768606   830 net.cpp:252] TEST Top shape for layer 65 'accuracy/top1' (1)
I0731 19:54:15.768610   830 layer_factory.hpp:136] Creating layer 'accuracy/top5' of type 'Accuracy'
I0731 19:54:15.768611   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.768620   830 net.cpp:184] Created Layer accuracy/top5 (66)
I0731 19:54:15.768623   830 net.cpp:561] accuracy/top5 <- out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0731 19:54:15.768626   830 net.cpp:561] accuracy/top5 <- label_data_1_split_2
I0731 19:54:15.768630   830 net.cpp:530] accuracy/top5 -> accuracy/top5
I0731 19:54:15.768633   830 net.cpp:245] Setting up accuracy/top5
I0731 19:54:15.768637   830 net.cpp:252] TEST Top shape for layer 66 'accuracy/top5' (1)
I0731 19:54:15.768640   830 net.cpp:325] accuracy/top5 does not need backward computation.
I0731 19:54:15.768645   830 net.cpp:325] accuracy/top1 does not need backward computation.
I0731 19:54:15.768648   830 net.cpp:323] loss needs backward computation.
I0731 19:54:15.768652   830 net.cpp:323] out_deconv_final_up8_out_deconv_final_up8_0_split needs backward computation.
I0731 19:54:15.768656   830 net.cpp:323] out_deconv_final_up8 needs backward computation.
I0731 19:54:15.768659   830 net.cpp:323] out_deconv_final_up4 needs backward computation.
I0731 19:54:15.768663   830 net.cpp:323] out_deconv_final_up2 needs backward computation.
I0731 19:54:15.768667   830 net.cpp:323] ctx_final/relu needs backward computation.
I0731 19:54:15.768671   830 net.cpp:323] ctx_final needs backward computation.
I0731 19:54:15.768674   830 net.cpp:323] ctx_conv4/relu needs backward computation.
I0731 19:54:15.768678   830 net.cpp:323] ctx_conv4/bn needs backward computation.
I0731 19:54:15.768682   830 net.cpp:323] ctx_conv4 needs backward computation.
I0731 19:54:15.768685   830 net.cpp:323] ctx_conv3/relu needs backward computation.
I0731 19:54:15.768689   830 net.cpp:323] ctx_conv3/bn needs backward computation.
I0731 19:54:15.768692   830 net.cpp:323] ctx_conv3 needs backward computation.
I0731 19:54:15.768697   830 net.cpp:323] ctx_conv2/relu needs backward computation.
I0731 19:54:15.768700   830 net.cpp:323] ctx_conv2/bn needs backward computation.
I0731 19:54:15.768704   830 net.cpp:323] ctx_conv2 needs backward computation.
I0731 19:54:15.768708   830 net.cpp:323] ctx_conv1/relu needs backward computation.
I0731 19:54:15.768712   830 net.cpp:323] ctx_conv1/bn needs backward computation.
I0731 19:54:15.768715   830 net.cpp:323] ctx_conv1 needs backward computation.
I0731 19:54:15.768719   830 net.cpp:323] out3_out5_combined needs backward computation.
I0731 19:54:15.768723   830 net.cpp:323] out3a/relu needs backward computation.
I0731 19:54:15.768728   830 net.cpp:323] out3a/bn needs backward computation.
I0731 19:54:15.768731   830 net.cpp:323] out3a needs backward computation.
I0731 19:54:15.768736   830 net.cpp:323] out5a_up2 needs backward computation.
I0731 19:54:15.768740   830 net.cpp:323] out5a/relu needs backward computation.
I0731 19:54:15.768744   830 net.cpp:323] out5a/bn needs backward computation.
I0731 19:54:15.768748   830 net.cpp:323] out5a needs backward computation.
I0731 19:54:15.768752   830 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0731 19:54:15.768755   830 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0731 19:54:15.768759   830 net.cpp:323] res5a_branch2b needs backward computation.
I0731 19:54:15.768764   830 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0731 19:54:15.768767   830 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0731 19:54:15.768771   830 net.cpp:323] res5a_branch2a needs backward computation.
I0731 19:54:15.768775   830 net.cpp:323] pool4 needs backward computation.
I0731 19:54:15.768779   830 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0731 19:54:15.768784   830 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0731 19:54:15.768788   830 net.cpp:323] res4a_branch2b needs backward computation.
I0731 19:54:15.768792   830 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0731 19:54:15.768796   830 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0731 19:54:15.768800   830 net.cpp:323] res4a_branch2a needs backward computation.
I0731 19:54:15.768808   830 net.cpp:323] pool3 needs backward computation.
I0731 19:54:15.768813   830 net.cpp:323] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I0731 19:54:15.768826   830 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0731 19:54:15.768831   830 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0731 19:54:15.768834   830 net.cpp:323] res3a_branch2b needs backward computation.
I0731 19:54:15.768838   830 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0731 19:54:15.768842   830 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0731 19:54:15.768846   830 net.cpp:323] res3a_branch2a needs backward computation.
I0731 19:54:15.768851   830 net.cpp:323] pool2 needs backward computation.
I0731 19:54:15.768854   830 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0731 19:54:15.768858   830 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0731 19:54:15.768862   830 net.cpp:323] res2a_branch2b needs backward computation.
I0731 19:54:15.768867   830 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0731 19:54:15.768870   830 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0731 19:54:15.768874   830 net.cpp:323] res2a_branch2a needs backward computation.
I0731 19:54:15.768879   830 net.cpp:323] pool1 needs backward computation.
I0731 19:54:15.768883   830 net.cpp:323] conv1b/relu needs backward computation.
I0731 19:54:15.768887   830 net.cpp:323] conv1b/bn needs backward computation.
I0731 19:54:15.768892   830 net.cpp:323] conv1b needs backward computation.
I0731 19:54:15.768895   830 net.cpp:323] conv1a/relu needs backward computation.
I0731 19:54:15.768899   830 net.cpp:323] conv1a/bn needs backward computation.
I0731 19:54:15.768903   830 net.cpp:323] conv1a needs backward computation.
I0731 19:54:15.768908   830 net.cpp:325] data/bias does not need backward computation.
I0731 19:54:15.768913   830 net.cpp:325] label_data_1_split does not need backward computation.
I0731 19:54:15.768918   830 net.cpp:325] data does not need backward computation.
I0731 19:54:15.768921   830 net.cpp:367] This network produces output accuracy/top1
I0731 19:54:15.768925   830 net.cpp:367] This network produces output accuracy/top5
I0731 19:54:15.768929   830 net.cpp:367] This network produces output loss
I0731 19:54:15.768991   830 net.cpp:389] Top memory (TEST) required for data: 318668800 diff: 8
I0731 19:54:15.768996   830 net.cpp:392] Bottom memory (TEST) required for data: 318668800 diff: 318668800
I0731 19:54:15.769001   830 net.cpp:395] Shared (in-place) memory (TEST) by data: 210124800 diff: 210124800
I0731 19:54:15.769004   830 net.cpp:398] Parameters memory (TEST) required for data: 2692608 diff: 2692608
I0731 19:54:15.769008   830 net.cpp:401] Parameters shared memory (TEST) by data: 0 diff: 0
I0731 19:54:15.769012   830 net.cpp:407] Network initialization done.
I0731 19:54:15.769125   830 solver.cpp:56] Solver scaffolding done.
I0731 19:54:15.779228   830 caffe.cpp:137] Finetuning from training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/initial/cityscapes5_jsegnet21v2_iter_32000.caffemodel
I0731 19:54:15.784060   830 net.cpp:1089] Copying source layer data Type:ImageLabelData #blobs=0
I0731 19:54:15.784081   830 net.cpp:1089] Copying source layer data/bias Type:Bias #blobs=1
I0731 19:54:15.784112   830 net.cpp:1089] Copying source layer conv1a Type:Convolution #blobs=2
I0731 19:54:15.784126   830 net.cpp:1089] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0731 19:54:15.784687   830 net.cpp:1089] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0731 19:54:15.784694   830 net.cpp:1089] Copying source layer conv1b Type:Convolution #blobs=2
I0731 19:54:15.784703   830 net.cpp:1089] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0731 19:54:15.785145   830 net.cpp:1089] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0731 19:54:15.785152   830 net.cpp:1089] Copying source layer pool1 Type:Pooling #blobs=0
I0731 19:54:15.785156   830 net.cpp:1089] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0731 19:54:15.785182   830 net.cpp:1089] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0731 19:54:15.785617   830 net.cpp:1089] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0731 19:54:15.785624   830 net.cpp:1089] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0731 19:54:15.785636   830 net.cpp:1089] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0731 19:54:15.786051   830 net.cpp:1089] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0731 19:54:15.786057   830 net.cpp:1089] Copying source layer pool2 Type:Pooling #blobs=0
I0731 19:54:15.786061   830 net.cpp:1089] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0731 19:54:15.786098   830 net.cpp:1089] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0731 19:54:15.786536   830 net.cpp:1089] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0731 19:54:15.786548   830 net.cpp:1089] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0731 19:54:15.786602   830 net.cpp:1089] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0731 19:54:15.787401   830 net.cpp:1089] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0731 19:54:15.787410   830 net.cpp:1089] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I0731 19:54:15.787415   830 net.cpp:1089] Copying source layer pool3 Type:Pooling #blobs=0
I0731 19:54:15.787420   830 net.cpp:1089] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0731 19:54:15.787683   830 net.cpp:1089] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0731 19:54:15.788213   830 net.cpp:1089] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0731 19:54:15.788221   830 net.cpp:1089] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0731 19:54:15.788300   830 net.cpp:1089] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0731 19:54:15.788776   830 net.cpp:1089] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0731 19:54:15.788784   830 net.cpp:1089] Copying source layer pool4 Type:Pooling #blobs=0
I0731 19:54:15.788789   830 net.cpp:1089] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0731 19:54:15.789212   830 net.cpp:1089] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0731 19:54:15.789686   830 net.cpp:1089] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0731 19:54:15.789695   830 net.cpp:1089] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0731 19:54:15.789901   830 net.cpp:1089] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0731 19:54:15.790376   830 net.cpp:1089] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0731 19:54:15.790385   830 net.cpp:1089] Copying source layer out5a Type:Convolution #blobs=2
I0731 19:54:15.790449   830 net.cpp:1089] Copying source layer out5a/bn Type:BatchNorm #blobs=5
I0731 19:54:15.790673   830 net.cpp:1089] Copying source layer out5a/relu Type:ReLU #blobs=0
I0731 19:54:15.790679   830 net.cpp:1089] Copying source layer out5a_up2 Type:Deconvolution #blobs=1
I0731 19:54:15.790688   830 net.cpp:1089] Copying source layer out3a Type:Convolution #blobs=2
I0731 19:54:15.790712   830 net.cpp:1089] Copying source layer out3a/bn Type:BatchNorm #blobs=5
I0731 19:54:15.790916   830 net.cpp:1089] Copying source layer out3a/relu Type:ReLU #blobs=0
I0731 19:54:15.790922   830 net.cpp:1089] Copying source layer out3_out5_combined Type:Eltwise #blobs=0
I0731 19:54:15.790926   830 net.cpp:1089] Copying source layer ctx_conv1 Type:Convolution #blobs=2
I0731 19:54:15.790951   830 net.cpp:1089] Copying source layer ctx_conv1/bn Type:BatchNorm #blobs=5
I0731 19:54:15.791152   830 net.cpp:1089] Copying source layer ctx_conv1/relu Type:ReLU #blobs=0
I0731 19:54:15.791158   830 net.cpp:1089] Copying source layer ctx_conv2 Type:Convolution #blobs=2
I0731 19:54:15.791189   830 net.cpp:1089] Copying source layer ctx_conv2/bn Type:BatchNorm #blobs=5
I0731 19:54:15.791388   830 net.cpp:1089] Copying source layer ctx_conv2/relu Type:ReLU #blobs=0
I0731 19:54:15.791405   830 net.cpp:1089] Copying source layer ctx_conv3 Type:Convolution #blobs=2
I0731 19:54:15.791432   830 net.cpp:1089] Copying source layer ctx_conv3/bn Type:BatchNorm #blobs=5
I0731 19:54:15.791637   830 net.cpp:1089] Copying source layer ctx_conv3/relu Type:ReLU #blobs=0
I0731 19:54:15.791643   830 net.cpp:1089] Copying source layer ctx_conv4 Type:Convolution #blobs=2
I0731 19:54:15.791668   830 net.cpp:1089] Copying source layer ctx_conv4/bn Type:BatchNorm #blobs=5
I0731 19:54:15.791868   830 net.cpp:1089] Copying source layer ctx_conv4/relu Type:ReLU #blobs=0
I0731 19:54:15.791875   830 net.cpp:1089] Copying source layer ctx_final Type:Convolution #blobs=2
I0731 19:54:15.791889   830 net.cpp:1089] Copying source layer ctx_final/relu Type:ReLU #blobs=0
I0731 19:54:15.791893   830 net.cpp:1089] Copying source layer out_deconv_final_up2 Type:Deconvolution #blobs=1
I0731 19:54:15.791901   830 net.cpp:1089] Copying source layer out_deconv_final_up4 Type:Deconvolution #blobs=1
I0731 19:54:15.791910   830 net.cpp:1089] Copying source layer out_deconv_final_up8 Type:Deconvolution #blobs=1
I0731 19:54:15.791921   830 net.cpp:1089] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0731 19:54:15.796180   830 net.cpp:1089] Copying source layer data Type:ImageLabelData #blobs=0
I0731 19:54:15.796203   830 net.cpp:1089] Copying source layer data/bias Type:Bias #blobs=1
I0731 19:54:15.796234   830 net.cpp:1089] Copying source layer conv1a Type:Convolution #blobs=2
I0731 19:54:15.796252   830 net.cpp:1089] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0731 19:54:15.796957   830 net.cpp:1089] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0731 19:54:15.796965   830 net.cpp:1089] Copying source layer conv1b Type:Convolution #blobs=2
I0731 19:54:15.796977   830 net.cpp:1089] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0731 19:54:15.797461   830 net.cpp:1089] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0731 19:54:15.797468   830 net.cpp:1089] Copying source layer pool1 Type:Pooling #blobs=0
I0731 19:54:15.797472   830 net.cpp:1089] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0731 19:54:15.797488   830 net.cpp:1089] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0731 19:54:15.797904   830 net.cpp:1089] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0731 19:54:15.797911   830 net.cpp:1089] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0731 19:54:15.797924   830 net.cpp:1089] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0731 19:54:15.798310   830 net.cpp:1089] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0731 19:54:15.798316   830 net.cpp:1089] Copying source layer pool2 Type:Pooling #blobs=0
I0731 19:54:15.798318   830 net.cpp:1089] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0731 19:54:15.798354   830 net.cpp:1089] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0731 19:54:15.798717   830 net.cpp:1089] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0731 19:54:15.798722   830 net.cpp:1089] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0731 19:54:15.798743   830 net.cpp:1089] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0731 19:54:15.799094   830 net.cpp:1089] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0731 19:54:15.799099   830 net.cpp:1089] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I0731 19:54:15.799103   830 net.cpp:1089] Copying source layer pool3 Type:Pooling #blobs=0
I0731 19:54:15.799104   830 net.cpp:1089] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0731 19:54:15.799214   830 net.cpp:1089] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0731 19:54:15.799567   830 net.cpp:1089] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0731 19:54:15.799572   830 net.cpp:1089] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0731 19:54:15.799644   830 net.cpp:1089] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0731 19:54:15.800000   830 net.cpp:1089] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0731 19:54:15.800005   830 net.cpp:1089] Copying source layer pool4 Type:Pooling #blobs=0
I0731 19:54:15.800009   830 net.cpp:1089] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0731 19:54:15.800374   830 net.cpp:1089] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0731 19:54:15.800734   830 net.cpp:1089] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0731 19:54:15.800740   830 net.cpp:1089] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0731 19:54:15.800927   830 net.cpp:1089] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0731 19:54:15.801281   830 net.cpp:1089] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0731 19:54:15.801286   830 net.cpp:1089] Copying source layer out5a Type:Convolution #blobs=2
I0731 19:54:15.801338   830 net.cpp:1089] Copying source layer out5a/bn Type:BatchNorm #blobs=5
I0731 19:54:15.801499   830 net.cpp:1089] Copying source layer out5a/relu Type:ReLU #blobs=0
I0731 19:54:15.801504   830 net.cpp:1089] Copying source layer out5a_up2 Type:Deconvolution #blobs=1
I0731 19:54:15.801509   830 net.cpp:1089] Copying source layer out3a Type:Convolution #blobs=2
I0731 19:54:15.801528   830 net.cpp:1089] Copying source layer out3a/bn Type:BatchNorm #blobs=5
I0731 19:54:15.801674   830 net.cpp:1089] Copying source layer out3a/relu Type:ReLU #blobs=0
I0731 19:54:15.801679   830 net.cpp:1089] Copying source layer out3_out5_combined Type:Eltwise #blobs=0
I0731 19:54:15.801681   830 net.cpp:1089] Copying source layer ctx_conv1 Type:Convolution #blobs=2
I0731 19:54:15.801702   830 net.cpp:1089] Copying source layer ctx_conv1/bn Type:BatchNorm #blobs=5
I0731 19:54:15.801846   830 net.cpp:1089] Copying source layer ctx_conv1/relu Type:ReLU #blobs=0
I0731 19:54:15.801851   830 net.cpp:1089] Copying source layer ctx_conv2 Type:Convolution #blobs=2
I0731 19:54:15.801870   830 net.cpp:1089] Copying source layer ctx_conv2/bn Type:BatchNorm #blobs=5
I0731 19:54:15.802016   830 net.cpp:1089] Copying source layer ctx_conv2/relu Type:ReLU #blobs=0
I0731 19:54:15.802021   830 net.cpp:1089] Copying source layer ctx_conv3 Type:Convolution #blobs=2
I0731 19:54:15.802040   830 net.cpp:1089] Copying source layer ctx_conv3/bn Type:BatchNorm #blobs=5
I0731 19:54:15.802186   830 net.cpp:1089] Copying source layer ctx_conv3/relu Type:ReLU #blobs=0
I0731 19:54:15.802191   830 net.cpp:1089] Copying source layer ctx_conv4 Type:Convolution #blobs=2
I0731 19:54:15.802211   830 net.cpp:1089] Copying source layer ctx_conv4/bn Type:BatchNorm #blobs=5
I0731 19:54:15.802356   830 net.cpp:1089] Copying source layer ctx_conv4/relu Type:ReLU #blobs=0
I0731 19:54:15.802361   830 net.cpp:1089] Copying source layer ctx_final Type:Convolution #blobs=2
I0731 19:54:15.802369   830 net.cpp:1089] Copying source layer ctx_final/relu Type:ReLU #blobs=0
I0731 19:54:15.802371   830 net.cpp:1089] Copying source layer out_deconv_final_up2 Type:Deconvolution #blobs=1
I0731 19:54:15.802376   830 net.cpp:1089] Copying source layer out_deconv_final_up4 Type:Deconvolution #blobs=1
I0731 19:54:15.802382   830 net.cpp:1089] Copying source layer out_deconv_final_up8 Type:Deconvolution #blobs=1
I0731 19:54:15.802387   830 net.cpp:1089] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0731 19:54:15.802477   830 parallel.cpp:108] [0 - 0] P2pSync adding callback
I0731 19:54:15.802482   830 parallel.cpp:108] [1 - 1] P2pSync adding callback
I0731 19:54:15.802484   830 parallel.cpp:108] [2 - 2] P2pSync adding callback
I0731 19:54:15.802486   830 parallel.cpp:61] Starting Optimization
I0731 19:54:15.802489   830 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 19:54:15.802515   830 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 19:54:15.802525   830 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 19:54:15.803225   939 device_alternate.hpp:116] NVML initialized on thread 136111348954880
I0731 19:54:15.821187   939 common.cpp:583] NVML succeeded to set CPU affinity on device 0
I0731 19:54:15.821238   940 device_alternate.hpp:116] NVML initialized on thread 136111340562176
I0731 19:54:15.822722   940 common.cpp:583] NVML succeeded to set CPU affinity on device 1
I0731 19:54:15.822763   941 device_alternate.hpp:116] NVML initialized on thread 136111332169472
I0731 19:54:15.823607   941 common.cpp:583] NVML succeeded to set CPU affinity on device 2
I0731 19:54:15.827250   940 solver.cpp:42] Solver data type: FLOAT
W0731 19:54:15.827801   940 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0731 19:54:15.827908   940 net.cpp:104] Using FLOAT as default forward math type
I0731 19:54:15.827913   940 net.cpp:110] Using FLOAT as default backward math type
I0731 19:54:15.827941   940 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0731 19:54:15.827949   940 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 19:54:15.831079   941 solver.cpp:42] Solver data type: FLOAT
W0731 19:54:15.831598   941 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0731 19:54:15.831703   941 net.cpp:104] Using FLOAT as default forward math type
I0731 19:54:15.831708   941 net.cpp:110] Using FLOAT as default backward math type
I0731 19:54:15.831733   941 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0731 19:54:15.831738   941 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 19:54:15.831769   942 db_lmdb.cpp:35] Opened lmdb data/train-image-lmdb
I0731 19:54:15.832594   943 db_lmdb.cpp:35] Opened lmdb data/train-image-lmdb
I0731 19:54:15.836418   940 data_layer.cpp:184] [1] ReshapePrefetch 6, 3, 640, 640
I0731 19:54:15.836861   941 data_layer.cpp:184] [2] ReshapePrefetch 6, 3, 640, 640
I0731 19:54:15.836912   940 data_layer.cpp:208] [1] Output data size: 6, 3, 640, 640
I0731 19:54:15.836925   940 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 19:54:15.836941   941 data_layer.cpp:208] [2] Output data size: 6, 3, 640, 640
I0731 19:54:15.836946   941 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 19:54:15.837121   941 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0731 19:54:15.837131   941 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 19:54:15.837170   940 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0731 19:54:15.837182   940 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 19:54:15.837942   944 data_layer.cpp:97] [2] Parser threads: 1
I0731 19:54:15.837954   944 data_layer.cpp:99] [2] Transformer threads: 1
I0731 19:54:15.843523   945 data_layer.cpp:97] [1] Parser threads: 1
I0731 19:54:15.843556   945 data_layer.cpp:99] [1] Transformer threads: 1
I0731 19:54:15.849803   946 db_lmdb.cpp:35] Opened lmdb data/train-label-lmdb
I0731 19:54:15.851642   947 db_lmdb.cpp:35] Opened lmdb data/train-label-lmdb
I0731 19:54:15.851671   941 data_layer.cpp:184] [2] ReshapePrefetch 6, 1, 640, 640
I0731 19:54:15.852005   941 data_layer.cpp:208] [2] Output data size: 6, 1, 640, 640
I0731 19:54:15.852027   941 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 19:54:15.854154   948 data_layer.cpp:97] [2] Parser threads: 1
I0731 19:54:15.854248   948 data_layer.cpp:99] [2] Transformer threads: 1
I0731 19:54:15.859083   940 data_layer.cpp:184] [1] ReshapePrefetch 6, 1, 640, 640
I0731 19:54:15.859390   944 blocking_queue.cpp:40] Waiting for datum
I0731 19:54:15.860092   940 data_layer.cpp:208] [1] Output data size: 6, 1, 640, 640
I0731 19:54:15.860122   940 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 19:54:15.865092   949 data_layer.cpp:97] [1] Parser threads: 1
I0731 19:54:15.865133   949 data_layer.cpp:99] [1] Transformer threads: 1
I0731 19:54:16.398991   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 7.99G, req 0G)
I0731 19:54:16.420138   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 0  (limit 7.99G, req 0G)
I0731 19:54:16.449851   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.83G, req 0G)
I0731 19:54:16.474705   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.83G, req 0G)
I0731 19:54:16.499472   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.7G, req 0G)
I0731 19:54:16.525719   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.7G, req 0G)
I0731 19:54:16.527765   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.62G, req 0G)
I0731 19:54:16.556030   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.62G, req 0G)
I0731 19:54:16.559341   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 7.56G, req 0G)
I0731 19:54:16.573261   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.52G, req 0G)
I0731 19:54:16.583153   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 7.56G, req 0G)
I0731 19:54:16.599076   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.52G, req 0G)
I0731 19:54:16.604113   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.48G, req 0G)
I0731 19:54:16.615919   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.46G, req 0G)
I0731 19:54:16.632206   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.48G, req 0G)
I0731 19:54:16.643980   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.46G, req 0G)
I0731 19:54:16.675894   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 7.39G, req 0G)
I0731 19:54:16.694242   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 7.34G, req 0G)
I0731 19:54:16.712069   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 7.39G, req 0G)
I0731 19:54:16.716853   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 7.32G, req 0G)
I0731 19:54:16.719987   940 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/l1reg/test.prototxt
W0731 19:54:16.720055   940 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0731 19:54:16.720193   940 net.cpp:104] Using FLOAT as default forward math type
I0731 19:54:16.720198   940 net.cpp:110] Using FLOAT as default backward math type
I0731 19:54:16.720227   940 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0731 19:54:16.720232   940 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 19:54:16.721117   964 db_lmdb.cpp:35] Opened lmdb data/val-image-lmdb
I0731 19:54:16.722640   940 data_layer.cpp:184] (1) ReshapePrefetch 2, 3, 640, 640
I0731 19:54:16.722724   940 data_layer.cpp:208] (1) Output data size: 2, 3, 640, 640
I0731 19:54:16.722743   940 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 19:54:16.722790   940 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0731 19:54:16.722811   940 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 19:54:16.723523   965 data_layer.cpp:97] (1) Parser threads: 1
I0731 19:54:16.723552   965 data_layer.cpp:99] (1) Transformer threads: 1
I0731 19:54:16.726212   966 db_lmdb.cpp:35] Opened lmdb data/val-label-lmdb
I0731 19:54:16.727339   940 data_layer.cpp:184] (1) ReshapePrefetch 2, 1, 640, 640
I0731 19:54:16.727438   940 data_layer.cpp:208] (1) Output data size: 2, 1, 640, 640
I0731 19:54:16.727445   940 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 19:54:16.728826   967 data_layer.cpp:97] (1) Parser threads: 1
I0731 19:54:16.728840   967 data_layer.cpp:99] (1) Transformer threads: 1
I0731 19:54:16.731699   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 7.34G, req 0G)
I0731 19:54:16.738030   940 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.18G, req 0G)
I0731 19:54:16.754645   940 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.15G, req 0G)
I0731 19:54:16.758494   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 7.32G, req 0G)
I0731 19:54:16.761687   941 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/l1reg/test.prototxt
W0731 19:54:16.761760   941 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0731 19:54:16.761901   941 net.cpp:104] Using FLOAT as default forward math type
I0731 19:54:16.761906   941 net.cpp:110] Using FLOAT as default backward math type
I0731 19:54:16.761925   941 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0731 19:54:16.761930   941 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 19:54:16.762670   968 db_lmdb.cpp:35] Opened lmdb data/val-image-lmdb
I0731 19:54:16.764781   940 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 1  (limit 7.13G, req 0G)
I0731 19:54:16.764847   941 data_layer.cpp:184] (2) ReshapePrefetch 2, 3, 640, 640
I0731 19:54:16.764971   941 data_layer.cpp:208] (2) Output data size: 2, 3, 640, 640
I0731 19:54:16.764979   941 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 19:54:16.765024   941 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0731 19:54:16.765033   941 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 19:54:16.765763   970 data_layer.cpp:97] (2) Parser threads: 1
I0731 19:54:16.765780   970 data_layer.cpp:99] (2) Transformer threads: 1
I0731 19:54:16.768791   971 db_lmdb.cpp:35] Opened lmdb data/val-label-lmdb
I0731 19:54:16.770108   941 data_layer.cpp:184] (2) ReshapePrefetch 2, 1, 640, 640
I0731 19:54:16.770187   941 data_layer.cpp:208] (2) Output data size: 2, 1, 640, 640
I0731 19:54:16.770193   941 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 19:54:16.771611   972 data_layer.cpp:97] (2) Parser threads: 1
I0731 19:54:16.771627   972 data_layer.cpp:99] (2) Transformer threads: 1
I0731 19:54:16.778089   940 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.12G, req 0G)
I0731 19:54:16.781675   941 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.18G, req 0G)
I0731 19:54:16.786357   940 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.1G, req 0G)
I0731 19:54:16.792389   940 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.09G, req 0G)
I0731 19:54:16.799931   941 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.15G, req 0G)
I0731 19:54:16.805920   940 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.09G, req 0G)
I0731 19:54:16.812507   941 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.13G, req 0G)
I0731 19:54:16.814327   940 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.08G, req 0G)
I0731 19:54:16.821131   941 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.12G, req 0G)
I0731 19:54:16.828341   941 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.1G, req 0G)
I0731 19:54:16.834671   941 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.09G, req 0G)
I0731 19:54:16.848481   941 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.09G, req 0G)
I0731 19:54:16.856329   941 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.08G, req 0G)
I0731 19:54:16.866878   940 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 7.07G, req 0G)
I0731 19:54:16.872887   940 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 7.06G, req 0G)
I0731 19:54:16.884866   940 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 7.05G, req 0G)
I0731 19:54:16.887861   940 solver.cpp:56] Solver scaffolding done.
I0731 19:54:16.908162   941 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 7.07G, req 0G)
I0731 19:54:16.914868   941 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 7.06G, req 0G)
I0731 19:54:16.927439   941 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 7.05G, req 0G)
I0731 19:54:16.929726   941 solver.cpp:56] Solver scaffolding done.
I0731 19:54:16.989797   941 parallel.cpp:164] [2 - 2] P2pSync adding callback
I0731 19:54:16.989797   940 parallel.cpp:164] [1 - 1] P2pSync adding callback
I0731 19:54:16.989822   939 parallel.cpp:164] [0 - 0] P2pSync adding callback
I0731 19:54:17.201231   939 solver.cpp:479] Solving jsegnet21v2_train
I0731 19:54:17.201248   939 solver.cpp:480] Learning Rate Policy: multistep
I0731 19:54:17.201257   941 solver.cpp:479] Solving jsegnet21v2_train
I0731 19:54:17.201266   941 solver.cpp:480] Learning Rate Policy: multistep
I0731 19:54:17.201321   940 solver.cpp:479] Solving jsegnet21v2_train
I0731 19:54:17.201330   940 solver.cpp:480] Learning Rate Policy: multistep
I0731 19:54:17.215128   940 solver.cpp:268] Starting Optimization on GPU 1
I0731 19:54:17.215134   939 solver.cpp:268] Starting Optimization on GPU 0
I0731 19:54:17.215129   941 solver.cpp:268] Starting Optimization on GPU 2
I0731 19:54:17.215330   993 device_alternate.hpp:116] NVML initialized on thread 128111172302592
I0731 19:54:17.215340   939 solver.cpp:550] Iteration 0, Testing net (#0)
I0731 19:54:17.215353   993 common.cpp:583] NVML succeeded to set CPU affinity on device 2
I0731 19:54:17.215363   994 device_alternate.hpp:116] NVML initialized on thread 128111180695296
I0731 19:54:17.215376   994 common.cpp:583] NVML succeeded to set CPU affinity on device 1
I0731 19:54:17.215386   995 device_alternate.hpp:116] NVML initialized on thread 128111163909888
I0731 19:54:17.215396   995 common.cpp:583] NVML succeeded to set CPU affinity on device 0
I0731 19:54:17.231560   941 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 6.96G, req 0G)
I0731 19:54:17.233786   940 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 6.96G, req 0G)
I0731 19:54:17.252002   941 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 6.9G, req 0G)
I0731 19:54:17.252372   940 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 6.9G, req 0G)
I0731 19:54:17.266181   939 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1a' with space 0.01G/1 1  (limit 6.88G, req 0G)
I0731 19:54:17.268043   940 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 6.84G, req 0G)
I0731 19:54:17.269210   941 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 6.84G, req 0G)
I0731 19:54:17.278429   940 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 6.81G, req 0G)
I0731 19:54:17.280594   941 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 6.81G, req 0G)
I0731 19:54:17.285269   939 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 6.82G, req 0G)
I0731 19:54:17.286295   940 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 6.78G, req 0G)
I0731 19:54:17.287683   941 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 6.78G, req 0G)
I0731 19:54:17.293311   940 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 6.76G, req 0G)
I0731 19:54:17.295183   941 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 6.76G, req 0G)
I0731 19:54:17.302363   939 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 6.75G, req 0G)
I0731 19:54:17.309957   940 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.74G, req 0G)
I0731 19:54:17.325206   941 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.74G, req 0G)
I0731 19:54:17.339277   939 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 6.73G, req 0G)
I0731 19:54:17.339521   940 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.74G, req 0G)
I0731 19:54:17.340183   941 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.74G, req 0G)
I0731 19:54:17.348664   939 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 6.69G, req 0G)
I0731 19:54:17.353886   939 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 6.67G, req 0G)
I0731 19:54:17.362522   939 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.66G, req 0G)
I0731 19:54:17.365991   939 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.65G, req 0G)
I0731 19:54:17.369945   940 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.58G, req 0G)
I0731 19:54:17.371572   941 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.58G, req 0G)
I0731 19:54:17.377269   940 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.57G, req 0G)
I0731 19:54:17.377672   941 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.57G, req 0G)
I0731 19:54:17.394341   939 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.5G, req 0G)
I0731 19:54:17.401204   939 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.49G, req 0G)
I0731 19:54:17.403079   940 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.48G, req 0G)
I0731 19:54:17.403528   941 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.48G, req 0G)
I0731 19:54:17.422211   939 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.39G, req 0G)
I0731 19:54:17.518098   939 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.913131
I0731 19:54:17.518123   939 solver.cpp:635]     Test net output #1: accuracy/top5 = 1
I0731 19:54:17.518128   939 solver.cpp:635]     Test net output #2: loss = 0.203904 (* 1 = 0.203904 loss)
I0731 19:54:17.518187   939 solver.cpp:295] [MultiGPU] Initial Test completed
I0731 19:54:17.600782   939 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 6.19G, req 0G)
I0731 19:54:17.603092   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 6.28G, req 0G)
I0731 19:54:17.605846   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 0  (limit 6.28G, req 0G)
I0731 19:54:17.666666   939 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 6.03G, req 0G)
I0731 19:54:17.673681   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 6.12G, req 0G)
I0731 19:54:17.683203   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 6.12G, req 0G)
I0731 19:54:17.730412   939 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 5.85G, req 0G)
I0731 19:54:17.740702   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 1 4 3  (limit 5.94G, req 0G)
I0731 19:54:17.746624   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 5.94G, req 0G)
I0731 19:54:17.762652   939 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 5.77G, req 0G)
I0731 19:54:17.774299   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 5.86G, req 0G)
I0731 19:54:17.779783   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 5.86G, req 0G)
I0731 19:54:17.787881   939 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 5.68G, req 0G)
I0731 19:54:17.799230   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 5.77G, req 0G)
I0731 19:54:17.800715   939 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 5.64G, req 0G)
I0731 19:54:17.806252   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 5.77G, req 0G)
I0731 19:54:17.815181   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 5.72G, req 0G)
I0731 19:54:17.820674   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 5.72G, req 0G)
I0731 19:54:17.824702   939 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 5.61G, req 0G)
I0731 19:54:17.832846   939 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 5.59G, req 0G)
I0731 19:54:17.835887   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 5.7G, req 0G)
I0731 19:54:17.843008   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 5.7G, req 0G)
I0731 19:54:17.843982   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 5.68G, req 0G)
I0731 19:54:17.852718   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 5.68G, req 0G)
I0731 19:54:17.879532   939 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 5.32G, req 0G)
I0731 19:54:17.891922   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 5.41G, req 0G)
I0731 19:54:17.892807   939 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 5.3G, req 0G)
I0731 19:54:17.900441   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 5.41G, req 0G)
I0731 19:54:17.906700   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 5.39G, req 0G)
I0731 19:54:17.917001   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 5.39G, req 0G)
I0731 19:54:17.921819   939 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 5.14G, req 0G)
I0731 19:54:17.938982   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 5.23G, req 0G)
I0731 19:54:17.947180   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 5.23G, req 0G)
I0731 19:54:18.120656   939 solver.cpp:358] Iteration 0 (0.602424 s), loss = 0.100357
I0731 19:54:18.120676   939 solver.cpp:375]     Train net output #0: loss = 0.100357 (* 1 = 0.100357 loss)
I0731 19:54:18.120682   939 sgd_solver.cpp:136] Iteration 0, lr = 1e-05, m = 0.9
I0731 19:54:18.340522   939 solver.cpp:358] Iteration 1 (0.219851 s), loss = 0.115906
I0731 19:54:18.340543   939 solver.cpp:375]     Train net output #0: loss = 0.115906 (* 1 = 0.115906 loss)
I0731 19:54:18.412173   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1a' with space 1.29G/1 1 0 3  (limit 3.08G, req 0G)
I0731 19:54:18.428428   939 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1a' with space 1.29G/1 1 0 3  (limit 2.99G, req 0G)
I0731 19:54:18.430943   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1a' with space 1.29G/1 1 0 3  (limit 3.08G, req 0G)
I0731 19:54:18.488687   939 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1b' with space 2.57G/2 6 4 3  (limit 1.71G, req 0G)
I0731 19:54:18.490432   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0G)
I0731 19:54:18.495631   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0G)
I0731 19:54:18.618576   939 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 2.57G/1 1 4 3  (limit 1.71G, req 0G)
I0731 19:54:18.627189   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 2.57G/1 6 4 3  (limit 1.79G, req 0G)
I0731 19:54:18.634335   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 2.57G/1 6 4 3  (limit 1.79G, req 0G)
I0731 19:54:18.665750   939 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 2.57G/2 6 4 3  (limit 1.71G, req 0G)
I0731 19:54:18.681157   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0G)
I0731 19:54:18.682981   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0G)
I0731 19:54:18.765100   939 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 2.57G/1 6 4 5  (limit 1.71G, req 0.07G)
I0731 19:54:18.781916   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 2.57G/1 6 4 5  (limit 1.79G, req 0.07G)
I0731 19:54:18.783591   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 2.57G/1 6 4 5  (limit 1.79G, req 0.07G)
I0731 19:54:18.789046   939 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 2.57G/2 6 4 3  (limit 1.71G, req 0.07G)
I0731 19:54:18.806897   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0.07G)
I0731 19:54:18.810487   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0.07G)
I0731 19:54:18.853585   939 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 2.57G/1 6 4 5  (limit 1.71G, req 0.07G)
I0731 19:54:18.867079   939 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 2.57G/2 6 4 5  (limit 1.71G, req 0.07G)
I0731 19:54:18.877096   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 2.57G/1 6 4 5  (limit 1.79G, req 0.07G)
I0731 19:54:18.881006   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 2.57G/1 6 4 5  (limit 1.79G, req 0.07G)
I0731 19:54:18.891497   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 2.57G/2 6 4 5  (limit 1.79G, req 0.07G)
I0731 19:54:18.895179   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0.07G)
I0731 19:54:18.917647   939 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'out3a' with space 2.57G/2 6 4 3  (limit 1.71G, req 0.07G)
I0731 19:54:18.947290   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'out3a' with space 2.57G/2 6 4 3  (limit 1.79G, req 0.07G)
I0731 19:54:18.952214   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'out3a' with space 2.57G/2 6 4 3  (limit 1.79G, req 0.07G)
I0731 19:54:18.972178   939 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 2.57G/1 6 4 3  (limit 1.71G, req 0.07G)
I0731 19:54:18.998374   939 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 2.57G/1 6 1 5  (limit 1.71G, req 0.07G)
I0731 19:54:19.003435   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 2.57G/1 6 4 3  (limit 1.79G, req 0.07G)
I0731 19:54:19.009099   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 2.57G/1 6 4 3  (limit 1.79G, req 0.07G)
I0731 19:54:19.029953   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 2.57G/1 6 1 5  (limit 1.79G, req 0.07G)
I0731 19:54:19.036820   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 2.57G/1 6 1 5  (limit 1.79G, req 0.07G)
I0731 19:54:19.166751   939 solver.cpp:358] Iteration 2 (0.826196 s), loss = 0.0694385
I0731 19:54:19.166779   939 solver.cpp:375]     Train net output #0: loss = 0.0694385 (* 1 = 0.0694385 loss)
I0731 19:54:19.167405   940 cudnn_conv_layer.cpp:292] [1] Layer 'conv1a' reallocating workspace: 2.57G -> 0.14G
I0731 19:54:19.167407   941 cudnn_conv_layer.cpp:292] [2] Layer 'conv1a' reallocating workspace: 2.57G -> 0.14G
I0731 19:54:19.170024   939 cudnn_conv_layer.cpp:292] [0] Layer 'conv1a' reallocating workspace: 2.57G -> 0.14G
I0731 19:54:37.237655   939 solver.cpp:353] Iteration 100 (5.42323 iter/s, 18.0704s/98 iter), loss = 0.0749862
I0731 19:54:37.237681   939 solver.cpp:375]     Train net output #0: loss = 0.0749862 (* 1 = 0.0749862 loss)
I0731 19:54:37.237685   939 sgd_solver.cpp:136] Iteration 100, lr = 1e-05, m = 0.9
I0731 19:54:48.729470   894 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 19:54:55.686940   939 solver.cpp:353] Iteration 200 (5.42041 iter/s, 18.4488s/100 iter), loss = 0.116966
I0731 19:54:55.686966   939 solver.cpp:375]     Train net output #0: loss = 0.116966 (* 1 = 0.116966 loss)
I0731 19:54:55.686971   939 sgd_solver.cpp:136] Iteration 200, lr = 1e-05, m = 0.9
I0731 19:55:14.210649   939 solver.cpp:353] Iteration 300 (5.39864 iter/s, 18.5232s/100 iter), loss = 0.0817837
I0731 19:55:14.210675   939 solver.cpp:375]     Train net output #0: loss = 0.0817837 (* 1 = 0.0817837 loss)
I0731 19:55:14.210680   939 sgd_solver.cpp:136] Iteration 300, lr = 1e-05, m = 0.9
I0731 19:55:19.226295   943 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 19:55:32.647809   939 solver.cpp:353] Iteration 400 (5.42398 iter/s, 18.4366s/100 iter), loss = 0.138529
I0731 19:55:32.647831   939 solver.cpp:375]     Train net output #0: loss = 0.138529 (* 1 = 0.138529 loss)
I0731 19:55:32.647837   939 sgd_solver.cpp:136] Iteration 400, lr = 1e-05, m = 0.9
I0731 19:55:51.185993   939 solver.cpp:353] Iteration 500 (5.39442 iter/s, 18.5377s/100 iter), loss = 0.0804452
I0731 19:55:51.186046   939 solver.cpp:375]     Train net output #0: loss = 0.0804452 (* 1 = 0.0804452 loss)
I0731 19:55:51.186053   939 sgd_solver.cpp:136] Iteration 500, lr = 1e-05, m = 0.9
I0731 19:56:09.673460   939 solver.cpp:353] Iteration 600 (5.40922 iter/s, 18.487s/100 iter), loss = 0.0726721
I0731 19:56:09.673482   939 solver.cpp:375]     Train net output #0: loss = 0.0726721 (* 1 = 0.0726721 loss)
I0731 19:56:09.673486   939 sgd_solver.cpp:136] Iteration 600, lr = 1e-05, m = 0.9
I0731 19:56:20.427151   946 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 19:56:28.175742   939 solver.cpp:353] Iteration 700 (5.40489 iter/s, 18.5018s/100 iter), loss = 0.0959757
I0731 19:56:28.175873   939 solver.cpp:375]     Train net output #0: loss = 0.0959756 (* 1 = 0.0959756 loss)
I0731 19:56:28.175879   939 sgd_solver.cpp:136] Iteration 700, lr = 1e-05, m = 0.9
I0731 19:56:46.673211   939 solver.cpp:353] Iteration 800 (5.40629 iter/s, 18.497s/100 iter), loss = 0.0967733
I0731 19:56:46.673235   939 solver.cpp:375]     Train net output #0: loss = 0.0967733 (* 1 = 0.0967733 loss)
I0731 19:56:46.673240   939 sgd_solver.cpp:136] Iteration 800, lr = 1e-05, m = 0.9
I0731 19:57:05.260005   939 solver.cpp:353] Iteration 900 (5.38031 iter/s, 18.5863s/100 iter), loss = 0.0776402
I0731 19:57:05.260056   939 solver.cpp:375]     Train net output #0: loss = 0.0776401 (* 1 = 0.0776401 loss)
I0731 19:57:05.260061   939 sgd_solver.cpp:136] Iteration 900, lr = 1e-05, m = 0.9
I0731 19:57:21.630036   946 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 19:57:23.920120   939 solver.cpp:353] Iteration 1000 (5.35917 iter/s, 18.6596s/100 iter), loss = 0.0954359
I0731 19:57:23.920143   939 solver.cpp:375]     Train net output #0: loss = 0.0954358 (* 1 = 0.0954358 loss)
I0731 19:57:23.920148   939 sgd_solver.cpp:136] Iteration 1000, lr = 1e-05, m = 0.9
I0731 19:57:42.503890   939 solver.cpp:353] Iteration 1100 (5.38119 iter/s, 18.5833s/100 iter), loss = 0.0649557
I0731 19:57:42.503949   939 solver.cpp:375]     Train net output #0: loss = 0.0649557 (* 1 = 0.0649557 loss)
I0731 19:57:42.503957   939 sgd_solver.cpp:136] Iteration 1100, lr = 1e-05, m = 0.9
I0731 19:57:52.270619   942 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 19:58:00.923698   939 solver.cpp:353] Iteration 1200 (5.42909 iter/s, 18.4193s/100 iter), loss = 0.0829385
I0731 19:58:00.923722   939 solver.cpp:375]     Train net output #0: loss = 0.0829385 (* 1 = 0.0829385 loss)
I0731 19:58:00.923727   939 sgd_solver.cpp:136] Iteration 1200, lr = 1e-05, m = 0.9
I0731 19:58:19.426764   939 solver.cpp:353] Iteration 1300 (5.40466 iter/s, 18.5026s/100 iter), loss = 0.0973721
I0731 19:58:19.426895   939 solver.cpp:375]     Train net output #0: loss = 0.0973721 (* 1 = 0.0973721 loss)
I0731 19:58:19.426903   939 sgd_solver.cpp:136] Iteration 1300, lr = 1e-05, m = 0.9
I0731 19:58:37.950187   939 solver.cpp:353] Iteration 1400 (5.39872 iter/s, 18.5229s/100 iter), loss = 0.117299
I0731 19:58:37.950211   939 solver.cpp:375]     Train net output #0: loss = 0.117299 (* 1 = 0.117299 loss)
I0731 19:58:37.950217   939 sgd_solver.cpp:136] Iteration 1400, lr = 1e-05, m = 0.9
I0731 19:58:53.856534   947 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 19:58:56.796560   939 solver.cpp:353] Iteration 1500 (5.30621 iter/s, 18.8458s/100 iter), loss = 0.0997248
I0731 19:58:56.796584   939 solver.cpp:375]     Train net output #0: loss = 0.0997248 (* 1 = 0.0997248 loss)
I0731 19:58:56.796591   939 sgd_solver.cpp:136] Iteration 1500, lr = 1e-05, m = 0.9
I0731 19:59:15.566650   939 solver.cpp:353] Iteration 1600 (5.32777 iter/s, 18.7696s/100 iter), loss = 0.0740124
I0731 19:59:15.566674   939 solver.cpp:375]     Train net output #0: loss = 0.0740123 (* 1 = 0.0740123 loss)
I0731 19:59:15.566679   939 sgd_solver.cpp:136] Iteration 1600, lr = 1e-05, m = 0.9
I0731 19:59:34.040174   939 solver.cpp:353] Iteration 1700 (5.4133 iter/s, 18.473s/100 iter), loss = 0.101538
I0731 19:59:34.040230   939 solver.cpp:375]     Train net output #0: loss = 0.101538 (* 1 = 0.101538 loss)
I0731 19:59:34.040236   939 sgd_solver.cpp:136] Iteration 1700, lr = 1e-05, m = 0.9
I0731 19:59:52.632609   939 solver.cpp:353] Iteration 1800 (5.37868 iter/s, 18.5919s/100 iter), loss = 0.0664867
I0731 19:59:52.632634   939 solver.cpp:375]     Train net output #0: loss = 0.0664867 (* 1 = 0.0664867 loss)
I0731 19:59:52.632639   939 sgd_solver.cpp:136] Iteration 1800, lr = 1e-05, m = 0.9
I0731 19:59:55.419172   947 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 20:00:11.236517   939 solver.cpp:353] Iteration 1900 (5.37536 iter/s, 18.6034s/100 iter), loss = 0.0580823
I0731 20:00:11.236594   939 solver.cpp:375]     Train net output #0: loss = 0.0580823 (* 1 = 0.0580823 loss)
I0731 20:00:11.236600   939 sgd_solver.cpp:136] Iteration 1900, lr = 1e-05, m = 0.9
I0731 20:00:26.083421   892 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 20:00:29.596447   939 solver.cpp:550] Iteration 2000, Testing net (#0)
I0731 20:00:29.772799   941 blocking_queue.cpp:40] Data layer prefetch queue empty
I0731 20:00:46.653820   939 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.950703
I0731 20:00:46.653937   939 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999653
I0731 20:00:46.653946   939 solver.cpp:635]     Test net output #2: loss = 0.179264 (* 1 = 0.179264 loss)
I0731 20:00:46.653972   939 solver.cpp:305] [MultiGPU] Tests completed in 17.0571s
I0731 20:00:46.861771   939 solver.cpp:353] Iteration 2000 (2.80707 iter/s, 35.6243s/100 iter), loss = 0.070615
I0731 20:00:46.861795   939 solver.cpp:375]     Train net output #0: loss = 0.0706149 (* 1 = 0.0706149 loss)
I0731 20:00:46.861799   939 sgd_solver.cpp:136] Iteration 2000, lr = 1e-05, m = 0.9
I0731 20:01:05.610003   939 solver.cpp:353] Iteration 2100 (5.33398 iter/s, 18.7477s/100 iter), loss = 0.0744172
I0731 20:01:05.610038   939 solver.cpp:375]     Train net output #0: loss = 0.0744171 (* 1 = 0.0744171 loss)
I0731 20:01:05.610043   939 sgd_solver.cpp:136] Iteration 2100, lr = 1e-05, m = 0.9
I0731 20:01:13.919586   894 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 20:01:23.970795   939 solver.cpp:353] Iteration 2200 (5.44654 iter/s, 18.3603s/100 iter), loss = 0.143696
I0731 20:01:23.970846   939 solver.cpp:375]     Train net output #0: loss = 0.143696 (* 1 = 0.143696 loss)
I0731 20:01:23.970854   939 sgd_solver.cpp:136] Iteration 2200, lr = 1e-05, m = 0.9
I0731 20:01:42.425518   939 solver.cpp:353] Iteration 2300 (5.41882 iter/s, 18.4542s/100 iter), loss = 0.0730889
I0731 20:01:42.425547   939 solver.cpp:375]     Train net output #0: loss = 0.0730888 (* 1 = 0.0730888 loss)
I0731 20:01:42.425554   939 sgd_solver.cpp:136] Iteration 2300, lr = 1e-05, m = 0.9
I0731 20:02:00.903017   939 solver.cpp:353] Iteration 2400 (5.41214 iter/s, 18.477s/100 iter), loss = 0.0677369
I0731 20:02:00.914434   939 solver.cpp:375]     Train net output #0: loss = 0.0677368 (* 1 = 0.0677368 loss)
I0731 20:02:00.914484   939 sgd_solver.cpp:136] Iteration 2400, lr = 1e-05, m = 0.9
I0731 20:02:14.999907   947 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 20:02:19.421376   939 solver.cpp:353] Iteration 2500 (5.40019 iter/s, 18.5179s/100 iter), loss = 0.0919619
I0731 20:02:19.421397   939 solver.cpp:375]     Train net output #0: loss = 0.0919618 (* 1 = 0.0919618 loss)
I0731 20:02:19.421401   939 sgd_solver.cpp:136] Iteration 2500, lr = 1e-05, m = 0.9
I0731 20:02:37.976142   939 solver.cpp:353] Iteration 2600 (5.3896 iter/s, 18.5543s/100 iter), loss = 0.0849303
I0731 20:02:37.976192   939 solver.cpp:375]     Train net output #0: loss = 0.0849302 (* 1 = 0.0849302 loss)
I0731 20:02:37.976197   939 sgd_solver.cpp:136] Iteration 2600, lr = 1e-05, m = 0.9
I0731 20:02:45.599664   892 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 20:02:56.503707   939 solver.cpp:353] Iteration 2700 (5.39751 iter/s, 18.5271s/100 iter), loss = 0.0709556
I0731 20:02:56.503731   939 solver.cpp:375]     Train net output #0: loss = 0.0709555 (* 1 = 0.0709555 loss)
I0731 20:02:56.503736   939 sgd_solver.cpp:136] Iteration 2700, lr = 1e-05, m = 0.9
I0731 20:03:15.121366   939 solver.cpp:353] Iteration 2800 (5.37139 iter/s, 18.6171s/100 iter), loss = 0.0639438
I0731 20:03:15.121423   939 solver.cpp:375]     Train net output #0: loss = 0.0639438 (* 1 = 0.0639438 loss)
I0731 20:03:15.121428   939 sgd_solver.cpp:136] Iteration 2800, lr = 1e-05, m = 0.9
I0731 20:03:33.567334   939 solver.cpp:353] Iteration 2900 (5.42139 iter/s, 18.4455s/100 iter), loss = 0.0912324
I0731 20:03:33.567358   939 solver.cpp:375]     Train net output #0: loss = 0.0912323 (* 1 = 0.0912323 loss)
I0731 20:03:33.567363   939 sgd_solver.cpp:136] Iteration 2900, lr = 1e-05, m = 0.9
I0731 20:03:46.884724   947 data_reader.cpp:264] Starting prefetch of epoch 4
I0731 20:03:52.304348   939 solver.cpp:353] Iteration 3000 (5.33718 iter/s, 18.7365s/100 iter), loss = 0.105364
I0731 20:03:52.304371   939 solver.cpp:375]     Train net output #0: loss = 0.105364 (* 1 = 0.105364 loss)
I0731 20:03:52.304376   939 sgd_solver.cpp:136] Iteration 3000, lr = 1e-05, m = 0.9
I0731 20:04:10.861722   939 solver.cpp:353] Iteration 3100 (5.38884 iter/s, 18.5569s/100 iter), loss = 0.0453951
I0731 20:04:10.861747   939 solver.cpp:375]     Train net output #0: loss = 0.0453951 (* 1 = 0.0453951 loss)
I0731 20:04:10.861750   939 sgd_solver.cpp:136] Iteration 3100, lr = 1e-05, m = 0.9
I0731 20:04:29.356855   939 solver.cpp:353] Iteration 3200 (5.40698 iter/s, 18.4946s/100 iter), loss = 0.162707
I0731 20:04:29.356936   939 solver.cpp:375]     Train net output #0: loss = 0.162707 (* 1 = 0.162707 loss)
I0731 20:04:29.356941   939 sgd_solver.cpp:136] Iteration 3200, lr = 1e-05, m = 0.9
I0731 20:04:47.825956   939 solver.cpp:353] Iteration 3300 (5.4146 iter/s, 18.4686s/100 iter), loss = 0.0656438
I0731 20:04:47.825981   939 solver.cpp:375]     Train net output #0: loss = 0.0656438 (* 1 = 0.0656438 loss)
I0731 20:04:47.825985   939 sgd_solver.cpp:136] Iteration 3300, lr = 1e-05, m = 0.9
I0731 20:04:48.220846   894 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 20:05:06.317966   939 solver.cpp:353] Iteration 3400 (5.40789 iter/s, 18.4915s/100 iter), loss = 0.0466187
I0731 20:05:06.318048   939 solver.cpp:375]     Train net output #0: loss = 0.0466186 (* 1 = 0.0466186 loss)
I0731 20:05:06.318054   939 sgd_solver.cpp:136] Iteration 3400, lr = 1e-05, m = 0.9
I0731 20:05:18.655464   942 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 20:05:24.686235   939 solver.cpp:353] Iteration 3500 (5.44432 iter/s, 18.3678s/100 iter), loss = 0.0843866
I0731 20:05:24.686259   939 solver.cpp:375]     Train net output #0: loss = 0.0843865 (* 1 = 0.0843865 loss)
I0731 20:05:24.686264   939 sgd_solver.cpp:136] Iteration 3500, lr = 1e-05, m = 0.9
I0731 20:05:43.515466   939 solver.cpp:353] Iteration 3600 (5.31104 iter/s, 18.8287s/100 iter), loss = 0.0760281
I0731 20:05:43.515545   939 solver.cpp:375]     Train net output #0: loss = 0.076028 (* 1 = 0.076028 loss)
I0731 20:05:43.515550   939 sgd_solver.cpp:136] Iteration 3600, lr = 1e-05, m = 0.9
I0731 20:06:02.265388   939 solver.cpp:353] Iteration 3700 (5.3335 iter/s, 18.7494s/100 iter), loss = 0.0852726
I0731 20:06:02.265410   939 solver.cpp:375]     Train net output #0: loss = 0.0852725 (* 1 = 0.0852725 loss)
I0731 20:06:02.265414   939 sgd_solver.cpp:136] Iteration 3700, lr = 1e-05, m = 0.9
I0731 20:06:20.483669   942 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 20:06:20.812911   939 solver.cpp:353] Iteration 3800 (5.3917 iter/s, 18.547s/100 iter), loss = 0.0879581
I0731 20:06:20.812933   939 solver.cpp:375]     Train net output #0: loss = 0.087958 (* 1 = 0.087958 loss)
I0731 20:06:20.812940   939 sgd_solver.cpp:136] Iteration 3800, lr = 1e-05, m = 0.9
I0731 20:06:39.275703   939 solver.cpp:353] Iteration 3900 (5.41645 iter/s, 18.4623s/100 iter), loss = 0.073381
I0731 20:06:39.275727   939 solver.cpp:375]     Train net output #0: loss = 0.0733809 (* 1 = 0.0733809 loss)
I0731 20:06:39.275732   939 sgd_solver.cpp:136] Iteration 3900, lr = 1e-05, m = 0.9
I0731 20:06:57.611049   939 solver.cpp:550] Iteration 4000, Testing net (#0)
I0731 20:07:01.454674   936 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 20:07:17.577641   934 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 20:07:18.010118   939 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.950099
I0731 20:07:18.010136   939 solver.cpp:635]     Test net output #1: accuracy/top5 = 1
I0731 20:07:18.010143   939 solver.cpp:635]     Test net output #2: loss = 0.159117 (* 1 = 0.159117 loss)
I0731 20:07:18.010174   939 solver.cpp:305] [MultiGPU] Tests completed in 20.3986s
I0731 20:07:18.222491   939 solver.cpp:353] Iteration 4000 (2.56768 iter/s, 38.9457s/100 iter), loss = 0.0602668
I0731 20:07:18.222533   939 solver.cpp:375]     Train net output #0: loss = 0.0602667 (* 1 = 0.0602667 loss)
I0731 20:07:18.222539   939 sgd_solver.cpp:136] Iteration 4000, lr = 1e-05, m = 0.9
I0731 20:07:37.197731   939 solver.cpp:353] Iteration 4100 (5.27017 iter/s, 18.9747s/100 iter), loss = 0.0962674
I0731 20:07:37.197803   939 solver.cpp:375]     Train net output #0: loss = 0.0962673 (* 1 = 0.0962673 loss)
I0731 20:07:37.197809   939 sgd_solver.cpp:136] Iteration 4100, lr = 1e-05, m = 0.9
I0731 20:07:55.800204   939 solver.cpp:353] Iteration 4200 (5.37578 iter/s, 18.602s/100 iter), loss = 0.0778708
I0731 20:07:55.800228   939 solver.cpp:375]     Train net output #0: loss = 0.0778707 (* 1 = 0.0778707 loss)
I0731 20:07:55.800232   939 sgd_solver.cpp:136] Iteration 4200, lr = 1e-05, m = 0.9
I0731 20:08:13.188974   942 data_reader.cpp:264] Starting prefetch of epoch 4
I0731 20:08:14.262632   939 solver.cpp:353] Iteration 4300 (5.41655 iter/s, 18.4619s/100 iter), loss = 0.100781
I0731 20:08:14.262655   939 solver.cpp:375]     Train net output #0: loss = 0.100781 (* 1 = 0.100781 loss)
I0731 20:08:14.262660   939 sgd_solver.cpp:136] Iteration 4300, lr = 1e-05, m = 0.9
I0731 20:08:32.717310   939 solver.cpp:353] Iteration 4400 (5.41883 iter/s, 18.4542s/100 iter), loss = 0.0572364
I0731 20:08:32.717336   939 solver.cpp:375]     Train net output #0: loss = 0.0572363 (* 1 = 0.0572363 loss)
I0731 20:08:32.717341   939 sgd_solver.cpp:136] Iteration 4400, lr = 1e-05, m = 0.9
I0731 20:08:51.187919   939 solver.cpp:353] Iteration 4500 (5.41416 iter/s, 18.4701s/100 iter), loss = 0.0502841
I0731 20:08:51.187973   939 solver.cpp:375]     Train net output #0: loss = 0.050284 (* 1 = 0.050284 loss)
I0731 20:08:51.187978   939 sgd_solver.cpp:136] Iteration 4500, lr = 1e-05, m = 0.9
I0731 20:09:09.735152   939 solver.cpp:353] Iteration 4600 (5.39179 iter/s, 18.5467s/100 iter), loss = 0.113455
I0731 20:09:09.735177   939 solver.cpp:375]     Train net output #0: loss = 0.113455 (* 1 = 0.113455 loss)
I0731 20:09:09.735182   939 sgd_solver.cpp:136] Iteration 4600, lr = 1e-05, m = 0.9
I0731 20:09:14.204813   947 data_reader.cpp:264] Starting prefetch of epoch 5
I0731 20:09:28.158460   939 solver.cpp:353] Iteration 4700 (5.42806 iter/s, 18.4228s/100 iter), loss = 0.0757026
I0731 20:09:28.158540   939 solver.cpp:375]     Train net output #0: loss = 0.0757025 (* 1 = 0.0757025 loss)
I0731 20:09:28.158547   939 sgd_solver.cpp:136] Iteration 4700, lr = 1e-05, m = 0.9
I0731 20:09:44.784705   942 data_reader.cpp:264] Starting prefetch of epoch 5
I0731 20:09:46.580257   939 solver.cpp:353] Iteration 4800 (5.4285 iter/s, 18.4213s/100 iter), loss = 0.0857558
I0731 20:09:46.580279   939 solver.cpp:375]     Train net output #0: loss = 0.0857557 (* 1 = 0.0857557 loss)
I0731 20:09:46.580284   939 sgd_solver.cpp:136] Iteration 4800, lr = 1e-05, m = 0.9
I0731 20:10:05.165266   939 solver.cpp:353] Iteration 4900 (5.38083 iter/s, 18.5845s/100 iter), loss = 0.0899845
I0731 20:10:05.165323   939 solver.cpp:375]     Train net output #0: loss = 0.0899843 (* 1 = 0.0899843 loss)
I0731 20:10:05.165330   939 sgd_solver.cpp:136] Iteration 4900, lr = 1e-05, m = 0.9
I0731 20:10:23.715363   939 solver.cpp:353] Iteration 5000 (5.39096 iter/s, 18.5496s/100 iter), loss = 0.087764
I0731 20:10:23.715385   939 solver.cpp:375]     Train net output #0: loss = 0.0877639 (* 1 = 0.0877639 loss)
I0731 20:10:23.715390   939 sgd_solver.cpp:136] Iteration 5000, lr = 1e-05, m = 0.9
I0731 20:10:42.339678   939 solver.cpp:353] Iteration 5100 (5.36947 iter/s, 18.6238s/100 iter), loss = 0.133236
I0731 20:10:42.339782   939 solver.cpp:375]     Train net output #0: loss = 0.133236 (* 1 = 0.133236 loss)
I0731 20:10:42.339788   939 sgd_solver.cpp:136] Iteration 5100, lr = 1e-05, m = 0.9
I0731 20:10:46.068114   892 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 20:11:00.891099   939 solver.cpp:353] Iteration 5200 (5.39057 iter/s, 18.5509s/100 iter), loss = 0.0919368
I0731 20:11:00.891126   939 solver.cpp:375]     Train net output #0: loss = 0.0919367 (* 1 = 0.0919367 loss)
I0731 20:11:00.891130   939 sgd_solver.cpp:136] Iteration 5200, lr = 1e-05, m = 0.9
I0731 20:11:19.412878   939 solver.cpp:353] Iteration 5300 (5.3992 iter/s, 18.5213s/100 iter), loss = 0.0541902
I0731 20:11:19.412950   939 solver.cpp:375]     Train net output #0: loss = 0.0541901 (* 1 = 0.0541901 loss)
I0731 20:11:19.412955   939 sgd_solver.cpp:136] Iteration 5300, lr = 1e-05, m = 0.9
I0731 20:11:37.959484   939 solver.cpp:353] Iteration 5400 (5.39197 iter/s, 18.5461s/100 iter), loss = 0.0600524
I0731 20:11:37.959517   939 solver.cpp:375]     Train net output #0: loss = 0.0600523 (* 1 = 0.0600523 loss)
I0731 20:11:37.959522   939 sgd_solver.cpp:136] Iteration 5400, lr = 1e-05, m = 0.9
I0731 20:11:47.429708   894 data_reader.cpp:264] Starting prefetch of epoch 4
I0731 20:11:56.482910   939 solver.cpp:353] Iteration 5500 (5.39872 iter/s, 18.5229s/100 iter), loss = 0.0745925
I0731 20:11:56.482960   939 solver.cpp:375]     Train net output #0: loss = 0.0745924 (* 1 = 0.0745924 loss)
I0731 20:11:56.482966   939 sgd_solver.cpp:136] Iteration 5500, lr = 1e-05, m = 0.9
I0731 20:12:14.933490   939 solver.cpp:353] Iteration 5600 (5.42003 iter/s, 18.4501s/100 iter), loss = 0.0400955
I0731 20:12:14.933512   939 solver.cpp:375]     Train net output #0: loss = 0.0400954 (* 1 = 0.0400954 loss)
I0731 20:12:14.933518   939 sgd_solver.cpp:136] Iteration 5600, lr = 1e-05, m = 0.9
I0731 20:12:17.917033   943 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 20:12:33.374260   939 solver.cpp:353] Iteration 5700 (5.42291 iter/s, 18.4403s/100 iter), loss = 0.125437
I0731 20:12:33.374341   939 solver.cpp:375]     Train net output #0: loss = 0.125437 (* 1 = 0.125437 loss)
I0731 20:12:33.374348   939 sgd_solver.cpp:136] Iteration 5700, lr = 1e-05, m = 0.9
I0731 20:12:51.947352   939 solver.cpp:353] Iteration 5800 (5.38428 iter/s, 18.5726s/100 iter), loss = 0.0759582
I0731 20:12:51.947376   939 solver.cpp:375]     Train net output #0: loss = 0.0759582 (* 1 = 0.0759582 loss)
I0731 20:12:51.947381   939 sgd_solver.cpp:136] Iteration 5800, lr = 1e-05, m = 0.9
I0731 20:13:10.675426   939 solver.cpp:353] Iteration 5900 (5.33973 iter/s, 18.7276s/100 iter), loss = 0.080704
I0731 20:13:10.675496   939 solver.cpp:375]     Train net output #0: loss = 0.0807039 (* 1 = 0.0807039 loss)
I0731 20:13:10.675503   939 sgd_solver.cpp:136] Iteration 5900, lr = 1e-05, m = 0.9
I0731 20:13:19.266425   942 data_reader.cpp:264] Starting prefetch of epoch 6
I0731 20:13:28.994153   939 solver.cpp:550] Iteration 6000, Testing net (#0)
I0731 20:13:42.002018   939 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.949194
I0731 20:13:42.002070   939 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999431
I0731 20:13:42.002080   939 solver.cpp:635]     Test net output #2: loss = 0.196036 (* 1 = 0.196036 loss)
I0731 20:13:42.002110   939 solver.cpp:305] [MultiGPU] Tests completed in 13.0076s
I0731 20:13:42.192071   939 solver.cpp:353] Iteration 6000 (3.17301 iter/s, 31.5158s/100 iter), loss = 0.104554
I0731 20:13:42.192097   939 solver.cpp:375]     Train net output #0: loss = 0.104554 (* 1 = 0.104554 loss)
I0731 20:13:42.192101   939 sgd_solver.cpp:136] Iteration 6000, lr = 1e-05, m = 0.9
I0731 20:14:00.649153   939 solver.cpp:353] Iteration 6100 (5.41812 iter/s, 18.4566s/100 iter), loss = 0.0782115
I0731 20:14:00.649178   939 solver.cpp:375]     Train net output #0: loss = 0.0782115 (* 1 = 0.0782115 loss)
I0731 20:14:00.649181   939 sgd_solver.cpp:136] Iteration 6100, lr = 1e-05, m = 0.9
I0731 20:14:02.889444   947 data_reader.cpp:264] Starting prefetch of epoch 6
I0731 20:14:19.263712   939 solver.cpp:353] Iteration 6200 (5.37229 iter/s, 18.614s/100 iter), loss = 0.0862987
I0731 20:14:19.263813   939 solver.cpp:375]     Train net output #0: loss = 0.0862987 (* 1 = 0.0862987 loss)
I0731 20:14:19.263819   939 sgd_solver.cpp:136] Iteration 6200, lr = 1e-05, m = 0.9
I0731 20:14:33.519976   942 data_reader.cpp:264] Starting prefetch of epoch 7
I0731 20:14:37.700903   939 solver.cpp:353] Iteration 6300 (5.42397 iter/s, 18.4367s/100 iter), loss = 0.0810728
I0731 20:14:37.700930   939 solver.cpp:375]     Train net output #0: loss = 0.0810728 (* 1 = 0.0810728 loss)
I0731 20:14:37.700937   939 sgd_solver.cpp:136] Iteration 6300, lr = 1e-05, m = 0.9
I0731 20:14:56.202558   939 solver.cpp:353] Iteration 6400 (5.40507 iter/s, 18.5011s/100 iter), loss = 0.0908732
I0731 20:14:56.202623   939 solver.cpp:375]     Train net output #0: loss = 0.0908731 (* 1 = 0.0908731 loss)
I0731 20:14:56.202630   939 sgd_solver.cpp:136] Iteration 6400, lr = 1e-05, m = 0.9
I0731 20:15:14.788594   939 solver.cpp:353] Iteration 6500 (5.38053 iter/s, 18.5855s/100 iter), loss = 0.0888236
I0731 20:15:14.788619   939 solver.cpp:375]     Train net output #0: loss = 0.0888236 (* 1 = 0.0888236 loss)
I0731 20:15:14.788622   939 sgd_solver.cpp:136] Iteration 6500, lr = 1e-05, m = 0.9
I0731 20:15:33.447649   939 solver.cpp:353] Iteration 6600 (5.35948 iter/s, 18.6585s/100 iter), loss = 0.0854795
I0731 20:15:33.447724   939 solver.cpp:375]     Train net output #0: loss = 0.0854794 (* 1 = 0.0854794 loss)
I0731 20:15:33.447729   939 sgd_solver.cpp:136] Iteration 6600, lr = 1e-05, m = 0.9
I0731 20:15:34.940228   943 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 20:15:51.948253   939 solver.cpp:353] Iteration 6700 (5.40538 iter/s, 18.5001s/100 iter), loss = 0.0935336
I0731 20:15:51.948276   939 solver.cpp:375]     Train net output #0: loss = 0.0935336 (* 1 = 0.0935336 loss)
I0731 20:15:51.948279   939 sgd_solver.cpp:136] Iteration 6700, lr = 1e-05, m = 0.9
I0731 20:16:10.536573   939 solver.cpp:353] Iteration 6800 (5.37987 iter/s, 18.5878s/100 iter), loss = 0.0498113
I0731 20:16:10.536628   939 solver.cpp:375]     Train net output #0: loss = 0.0498113 (* 1 = 0.0498113 loss)
I0731 20:16:10.536633   939 sgd_solver.cpp:136] Iteration 6800, lr = 1e-05, m = 0.9
I0731 20:16:29.048372   939 solver.cpp:353] Iteration 6900 (5.40211 iter/s, 18.5113s/100 iter), loss = 0.100908
I0731 20:16:29.048393   939 solver.cpp:375]     Train net output #0: loss = 0.100908 (* 1 = 0.100908 loss)
I0731 20:16:29.048398   939 sgd_solver.cpp:136] Iteration 6900, lr = 1e-05, m = 0.9
I0731 20:16:36.121834   894 data_reader.cpp:264] Starting prefetch of epoch 5
I0731 20:16:47.675128   939 solver.cpp:353] Iteration 7000 (5.36877 iter/s, 18.6262s/100 iter), loss = 0.105827
I0731 20:16:47.675184   939 solver.cpp:375]     Train net output #0: loss = 0.105827 (* 1 = 0.105827 loss)
I0731 20:16:47.675191   939 sgd_solver.cpp:136] Iteration 7000, lr = 1e-05, m = 0.9
I0731 20:17:06.195106   939 solver.cpp:353] Iteration 7100 (5.39972 iter/s, 18.5195s/100 iter), loss = 0.0794259
I0731 20:17:06.195133   939 solver.cpp:375]     Train net output #0: loss = 0.0794259 (* 1 = 0.0794259 loss)
I0731 20:17:06.195139   939 sgd_solver.cpp:136] Iteration 7100, lr = 1e-05, m = 0.9
I0731 20:17:06.798491   943 data_reader.cpp:264] Starting prefetch of epoch 4
I0731 20:17:24.712141   939 solver.cpp:353] Iteration 7200 (5.40058 iter/s, 18.5165s/100 iter), loss = 0.0679529
I0731 20:17:24.712244   939 solver.cpp:375]     Train net output #0: loss = 0.0679528 (* 1 = 0.0679528 loss)
I0731 20:17:24.712252   939 sgd_solver.cpp:136] Iteration 7200, lr = 1e-05, m = 0.9
I0731 20:17:43.216641   939 solver.cpp:353] Iteration 7300 (5.40424 iter/s, 18.504s/100 iter), loss = 0.0660755
I0731 20:17:43.216665   939 solver.cpp:375]     Train net output #0: loss = 0.0660754 (* 1 = 0.0660754 loss)
I0731 20:17:43.216668   939 sgd_solver.cpp:136] Iteration 7300, lr = 1e-05, m = 0.9
I0731 20:18:01.707326   939 solver.cpp:353] Iteration 7400 (5.40828 iter/s, 18.4902s/100 iter), loss = 0.110046
I0731 20:18:01.707406   939 solver.cpp:375]     Train net output #0: loss = 0.110046 (* 1 = 0.110046 loss)
I0731 20:18:01.707412   939 sgd_solver.cpp:136] Iteration 7400, lr = 1e-05, m = 0.9
I0731 20:18:08.054198   942 data_reader.cpp:264] Starting prefetch of epoch 8
I0731 20:18:20.241487   939 solver.cpp:353] Iteration 7500 (5.39559 iter/s, 18.5336s/100 iter), loss = 0.104816
I0731 20:18:20.241510   939 solver.cpp:375]     Train net output #0: loss = 0.104816 (* 1 = 0.104816 loss)
I0731 20:18:20.241515   939 sgd_solver.cpp:136] Iteration 7500, lr = 1e-05, m = 0.9
I0731 20:18:38.722410   939 solver.cpp:353] Iteration 7600 (5.41113 iter/s, 18.4804s/100 iter), loss = 0.0641242
I0731 20:18:38.722473   939 solver.cpp:375]     Train net output #0: loss = 0.0641241 (* 1 = 0.0641241 loss)
I0731 20:18:38.722478   939 sgd_solver.cpp:136] Iteration 7600, lr = 1e-05, m = 0.9
I0731 20:18:57.205937   939 solver.cpp:353] Iteration 7700 (5.41037 iter/s, 18.483s/100 iter), loss = 0.0723037
I0731 20:18:57.205960   939 solver.cpp:375]     Train net output #0: loss = 0.0723037 (* 1 = 0.0723037 loss)
I0731 20:18:57.205963   939 sgd_solver.cpp:136] Iteration 7700, lr = 1e-05, m = 0.9
I0731 20:19:09.222554   894 data_reader.cpp:264] Starting prefetch of epoch 6
I0731 20:19:15.661316   939 solver.cpp:353] Iteration 7800 (5.41862 iter/s, 18.4549s/100 iter), loss = 0.0975931
I0731 20:19:15.661342   939 solver.cpp:375]     Train net output #0: loss = 0.0975931 (* 1 = 0.0975931 loss)
I0731 20:19:15.661346   939 sgd_solver.cpp:136] Iteration 7800, lr = 1e-05, m = 0.9
I0731 20:19:34.246060   939 solver.cpp:353] Iteration 7900 (5.3809 iter/s, 18.5842s/100 iter), loss = 0.056816
I0731 20:19:34.246083   939 solver.cpp:375]     Train net output #0: loss = 0.056816 (* 1 = 0.056816 loss)
I0731 20:19:34.246086   939 sgd_solver.cpp:136] Iteration 7900, lr = 1e-05, m = 0.9
I0731 20:19:39.832788   892 data_reader.cpp:264] Starting prefetch of epoch 4
I0731 20:19:52.522413   939 solver.cpp:550] Iteration 8000, Testing net (#0)
I0731 20:20:03.785984   968 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 20:20:04.253296   939 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.951444
I0731 20:20:04.253343   939 solver.cpp:635]     Test net output #1: accuracy/top5 = 1
I0731 20:20:04.253361   939 solver.cpp:635]     Test net output #2: loss = 0.154188 (* 1 = 0.154188 loss)
I0731 20:20:04.253410   939 solver.cpp:305] [MultiGPU] Tests completed in 11.7307s
I0731 20:20:04.479298   939 solver.cpp:353] Iteration 8000 (3.30771 iter/s, 30.2324s/100 iter), loss = 0.101574
I0731 20:20:04.479349   939 solver.cpp:375]     Train net output #0: loss = 0.101574 (* 1 = 0.101574 loss)
I0731 20:20:04.479360   939 sgd_solver.cpp:136] Iteration 8000, lr = 1e-05, m = 0.9
I0731 20:20:23.634474   939 solver.cpp:353] Iteration 8100 (5.22066 iter/s, 19.1546s/100 iter), loss = 0.0697335
I0731 20:20:23.634526   939 solver.cpp:375]     Train net output #0: loss = 0.0697334 (* 1 = 0.0697334 loss)
I0731 20:20:23.634531   939 sgd_solver.cpp:136] Iteration 8100, lr = 1e-05, m = 0.9
I0731 20:20:42.084396   939 solver.cpp:353] Iteration 8200 (5.42023 iter/s, 18.4494s/100 iter), loss = 0.0632941
I0731 20:20:42.084419   939 solver.cpp:375]     Train net output #0: loss = 0.063294 (* 1 = 0.063294 loss)
I0731 20:20:42.084424   939 sgd_solver.cpp:136] Iteration 8200, lr = 1e-05, m = 0.9
I0731 20:20:53.156318   894 data_reader.cpp:264] Starting prefetch of epoch 7
I0731 20:21:00.578996   939 solver.cpp:353] Iteration 8300 (5.40713 iter/s, 18.4941s/100 iter), loss = 0.135063
I0731 20:21:00.579049   939 solver.cpp:375]     Train net output #0: loss = 0.135063 (* 1 = 0.135063 loss)
I0731 20:21:00.579054   939 sgd_solver.cpp:136] Iteration 8300, lr = 1e-05, m = 0.9
I0731 20:21:19.183478   939 solver.cpp:353] Iteration 8400 (5.3752 iter/s, 18.604s/100 iter), loss = 0.0690997
I0731 20:21:19.183502   939 solver.cpp:375]     Train net output #0: loss = 0.0690996 (* 1 = 0.0690996 loss)
I0731 20:21:19.183506   939 sgd_solver.cpp:136] Iteration 8400, lr = 1e-05, m = 0.9
I0731 20:21:24.070121   942 data_reader.cpp:264] Starting prefetch of epoch 9
I0731 20:21:37.676601   939 solver.cpp:353] Iteration 8500 (5.40756 iter/s, 18.4926s/100 iter), loss = 0.0575547
I0731 20:21:37.676651   939 solver.cpp:375]     Train net output #0: loss = 0.0575546 (* 1 = 0.0575546 loss)
I0731 20:21:37.676657   939 sgd_solver.cpp:136] Iteration 8500, lr = 1e-05, m = 0.9
I0731 20:21:56.312517   939 solver.cpp:353] Iteration 8600 (5.36613 iter/s, 18.6354s/100 iter), loss = 0.110705
I0731 20:21:56.312546   939 solver.cpp:375]     Train net output #0: loss = 0.110705 (* 1 = 0.110705 loss)
I0731 20:21:56.312553   939 sgd_solver.cpp:136] Iteration 8600, lr = 1e-05, m = 0.9
I0731 20:22:14.772946   939 solver.cpp:353] Iteration 8700 (5.41714 iter/s, 18.4599s/100 iter), loss = 0.077508
I0731 20:22:14.773025   939 solver.cpp:375]     Train net output #0: loss = 0.077508 (* 1 = 0.077508 loss)
I0731 20:22:14.773030   939 sgd_solver.cpp:136] Iteration 8700, lr = 1e-05, m = 0.9
I0731 20:22:25.286682   943 data_reader.cpp:264] Starting prefetch of epoch 5
I0731 20:22:33.379261   939 solver.cpp:353] Iteration 8800 (5.37467 iter/s, 18.6058s/100 iter), loss = 0.0759625
I0731 20:22:33.379282   939 solver.cpp:375]     Train net output #0: loss = 0.0759625 (* 1 = 0.0759625 loss)
I0731 20:22:33.379287   939 sgd_solver.cpp:136] Iteration 8800, lr = 1e-05, m = 0.9
I0731 20:22:52.004930   939 solver.cpp:353] Iteration 8900 (5.36908 iter/s, 18.6252s/100 iter), loss = 0.0751425
I0731 20:22:52.004981   939 solver.cpp:375]     Train net output #0: loss = 0.0751425 (* 1 = 0.0751425 loss)
I0731 20:22:52.004987   939 sgd_solver.cpp:136] Iteration 8900, lr = 1e-05, m = 0.9
I0731 20:23:10.669811   939 solver.cpp:353] Iteration 9000 (5.3578 iter/s, 18.6644s/100 iter), loss = 0.0667758
I0731 20:23:10.669837   939 solver.cpp:375]     Train net output #0: loss = 0.0667757 (* 1 = 0.0667757 loss)
I0731 20:23:10.669842   939 sgd_solver.cpp:136] Iteration 9000, lr = 1e-05, m = 0.9
I0731 20:23:26.746210   946 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 20:23:29.140506   939 solver.cpp:353] Iteration 9100 (5.41413 iter/s, 18.4702s/100 iter), loss = 0.086671
I0731 20:23:29.140530   939 solver.cpp:375]     Train net output #0: loss = 0.0866709 (* 1 = 0.0866709 loss)
I0731 20:23:29.140534   939 sgd_solver.cpp:136] Iteration 9100, lr = 1e-05, m = 0.9
I0731 20:23:47.650341   939 solver.cpp:353] Iteration 9200 (5.40268 iter/s, 18.5093s/100 iter), loss = 0.0697889
I0731 20:23:47.650365   939 solver.cpp:375]     Train net output #0: loss = 0.0697889 (* 1 = 0.0697889 loss)
I0731 20:23:47.650370   939 sgd_solver.cpp:136] Iteration 9200, lr = 1e-05, m = 0.9
I0731 20:23:57.240258   942 data_reader.cpp:264] Starting prefetch of epoch 10
I0731 20:24:06.115327   939 solver.cpp:353] Iteration 9300 (5.4158 iter/s, 18.4645s/100 iter), loss = 0.0786486
I0731 20:24:06.115352   939 solver.cpp:375]     Train net output #0: loss = 0.0786485 (* 1 = 0.0786485 loss)
I0731 20:24:06.115356   939 sgd_solver.cpp:136] Iteration 9300, lr = 1e-05, m = 0.9
I0731 20:24:24.633124   939 solver.cpp:353] Iteration 9400 (5.40036 iter/s, 18.5173s/100 iter), loss = 0.102437
I0731 20:24:24.633147   939 solver.cpp:375]     Train net output #0: loss = 0.102437 (* 1 = 0.102437 loss)
I0731 20:24:24.633152   939 sgd_solver.cpp:136] Iteration 9400, lr = 1e-05, m = 0.9
I0731 20:24:43.233778   939 solver.cpp:353] Iteration 9500 (5.3763 iter/s, 18.6001s/100 iter), loss = 0.0656029
I0731 20:24:43.233834   939 solver.cpp:375]     Train net output #0: loss = 0.0656028 (* 1 = 0.0656028 loss)
I0731 20:24:43.233839   939 sgd_solver.cpp:136] Iteration 9500, lr = 1e-05, m = 0.9
I0731 20:24:58.675091   894 data_reader.cpp:264] Starting prefetch of epoch 8
I0731 20:25:01.840579   939 solver.cpp:353] Iteration 9600 (5.37453 iter/s, 18.6063s/100 iter), loss = 0.0687463
I0731 20:25:01.840600   939 solver.cpp:375]     Train net output #0: loss = 0.0687463 (* 1 = 0.0687463 loss)
I0731 20:25:01.840605   939 sgd_solver.cpp:136] Iteration 9600, lr = 1e-05, m = 0.9
I0731 20:25:20.366802   939 solver.cpp:353] Iteration 9700 (5.3979 iter/s, 18.5257s/100 iter), loss = 0.0698831
I0731 20:25:20.366858   939 solver.cpp:375]     Train net output #0: loss = 0.0698831 (* 1 = 0.0698831 loss)
I0731 20:25:20.366863   939 sgd_solver.cpp:136] Iteration 9700, lr = 1e-05, m = 0.9
I0731 20:25:38.909775   939 solver.cpp:353] Iteration 9800 (5.39303 iter/s, 18.5425s/100 iter), loss = 0.0663345
I0731 20:25:38.909797   939 solver.cpp:375]     Train net output #0: loss = 0.0663344 (* 1 = 0.0663344 loss)
I0731 20:25:38.909802   939 sgd_solver.cpp:136] Iteration 9800, lr = 1e-05, m = 0.9
I0731 20:25:57.401998   939 solver.cpp:353] Iteration 9900 (5.40783 iter/s, 18.4917s/100 iter), loss = 0.0775769
I0731 20:25:57.402070   939 solver.cpp:375]     Train net output #0: loss = 0.0775769 (* 1 = 0.0775769 loss)
I0731 20:25:57.402074   939 sgd_solver.cpp:136] Iteration 9900, lr = 1e-05, m = 0.9
I0731 20:25:59.809309   947 data_reader.cpp:264] Starting prefetch of epoch 7
I0731 20:26:15.696808   939 solver.cpp:680] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/l1reg/cityscapes5_jsegnet21v2_iter_10000.caffemodel
I0731 20:26:15.813272   939 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/l1reg/cityscapes5_jsegnet21v2_iter_10000.solverstate
I0731 20:26:15.823809   939 solver.cpp:550] Iteration 10000, Testing net (#0)
I0731 20:26:23.907428   964 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 20:26:29.066617   939 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.950826
I0731 20:26:29.067124   939 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999442
I0731 20:26:29.067134   939 solver.cpp:635]     Test net output #2: loss = 0.191844 (* 1 = 0.191844 loss)
I0731 20:26:29.067159   939 solver.cpp:305] [MultiGPU] Tests completed in 13.243s
I0731 20:26:29.275454   939 solver.cpp:353] Iteration 10000 (3.13749 iter/s, 31.8726s/100 iter), loss = 0.0737782
I0731 20:26:29.275481   939 solver.cpp:375]     Train net output #0: loss = 0.0737782 (* 1 = 0.0737782 loss)
I0731 20:26:29.275488   939 sgd_solver.cpp:136] Iteration 10000, lr = 1e-05, m = 0.9
I0731 20:26:48.895581   939 solver.cpp:353] Iteration 10100 (5.09695 iter/s, 19.6196s/100 iter), loss = 0.0733768
I0731 20:26:48.895627   939 solver.cpp:375]     Train net output #0: loss = 0.0733768 (* 1 = 0.0733768 loss)
I0731 20:26:48.895634   939 sgd_solver.cpp:136] Iteration 10100, lr = 1e-05, m = 0.9
I0731 20:27:08.037338   939 solver.cpp:353] Iteration 10200 (5.22432 iter/s, 19.1412s/100 iter), loss = 0.0894128
I0731 20:27:08.037395   939 solver.cpp:375]     Train net output #0: loss = 0.0894128 (* 1 = 0.0894128 loss)
I0731 20:27:08.037400   939 sgd_solver.cpp:136] Iteration 10200, lr = 1e-05, m = 0.9
I0731 20:27:16.135689   943 data_reader.cpp:264] Starting prefetch of epoch 6
I0731 20:27:26.419374   939 solver.cpp:353] Iteration 10300 (5.44024 iter/s, 18.3815s/100 iter), loss = 0.0611011
I0731 20:27:26.419404   939 solver.cpp:375]     Train net output #0: loss = 0.0611011 (* 1 = 0.0611011 loss)
I0731 20:27:26.419409   939 sgd_solver.cpp:136] Iteration 10300, lr = 1e-05, m = 0.9
I0731 20:27:44.832520   939 solver.cpp:353] Iteration 10400 (5.43105 iter/s, 18.4126s/100 iter), loss = 0.0542672
I0731 20:27:44.832561   939 solver.cpp:375]     Train net output #0: loss = 0.0542671 (* 1 = 0.0542671 loss)
I0731 20:27:44.832566   939 sgd_solver.cpp:136] Iteration 10400, lr = 1e-05, m = 0.9
I0731 20:28:03.323395   939 solver.cpp:353] Iteration 10500 (5.40822 iter/s, 18.4904s/100 iter), loss = 0.0591627
I0731 20:28:03.323423   939 solver.cpp:375]     Train net output #0: loss = 0.0591627 (* 1 = 0.0591627 loss)
I0731 20:28:03.323431   939 sgd_solver.cpp:136] Iteration 10500, lr = 1e-05, m = 0.9
I0731 20:28:17.060617   946 data_reader.cpp:264] Starting prefetch of epoch 4
I0731 20:28:21.916247   939 solver.cpp:353] Iteration 10600 (5.37856 iter/s, 18.5923s/100 iter), loss = 0.0853715
I0731 20:28:21.916270   939 solver.cpp:375]     Train net output #0: loss = 0.0853715 (* 1 = 0.0853715 loss)
I0731 20:28:21.916273   939 sgd_solver.cpp:136] Iteration 10600, lr = 1e-05, m = 0.9
I0731 20:28:40.443987   939 solver.cpp:353] Iteration 10700 (5.39746 iter/s, 18.5272s/100 iter), loss = 0.0453151
I0731 20:28:40.444010   939 solver.cpp:375]     Train net output #0: loss = 0.0453151 (* 1 = 0.0453151 loss)
I0731 20:28:40.444015   939 sgd_solver.cpp:136] Iteration 10700, lr = 1e-05, m = 0.9
I0731 20:28:47.846539   943 data_reader.cpp:264] Starting prefetch of epoch 7
I0731 20:28:58.862195   939 solver.cpp:353] Iteration 10800 (5.42956 iter/s, 18.4177s/100 iter), loss = 0.0818395
I0731 20:28:58.862217   939 solver.cpp:375]     Train net output #0: loss = 0.0818394 (* 1 = 0.0818394 loss)
I0731 20:28:58.862222   939 sgd_solver.cpp:136] Iteration 10800, lr = 1e-05, m = 0.9
I0731 20:29:17.341419   939 solver.cpp:353] Iteration 10900 (5.41163 iter/s, 18.4787s/100 iter), loss = 0.0490002
I0731 20:29:17.341442   939 solver.cpp:375]     Train net output #0: loss = 0.0490001 (* 1 = 0.0490001 loss)
I0731 20:29:17.341447   939 sgd_solver.cpp:136] Iteration 10900, lr = 1e-05, m = 0.9
I0731 20:29:35.814685   939 solver.cpp:353] Iteration 11000 (5.41338 iter/s, 18.4728s/100 iter), loss = 0.0660167
I0731 20:29:35.814743   939 solver.cpp:375]     Train net output #0: loss = 0.0660167 (* 1 = 0.0660167 loss)
I0731 20:29:35.814749   939 sgd_solver.cpp:136] Iteration 11000, lr = 1e-05, m = 0.9
I0731 20:29:48.860930   942 data_reader.cpp:264] Starting prefetch of epoch 11
I0731 20:29:54.318528   939 solver.cpp:353] Iteration 11100 (5.40443 iter/s, 18.5033s/100 iter), loss = 0.0813252
I0731 20:29:54.318552   939 solver.cpp:375]     Train net output #0: loss = 0.0813252 (* 1 = 0.0813252 loss)
I0731 20:29:54.318557   939 sgd_solver.cpp:136] Iteration 11100, lr = 1e-05, m = 0.9
I0731 20:30:12.852248   939 solver.cpp:353] Iteration 11200 (5.39572 iter/s, 18.5332s/100 iter), loss = 0.0831627
I0731 20:30:12.852304   939 solver.cpp:375]     Train net output #0: loss = 0.0831627 (* 1 = 0.0831627 loss)
I0731 20:30:12.852311   939 sgd_solver.cpp:136] Iteration 11200, lr = 1e-05, m = 0.9
I0731 20:30:31.360620   939 solver.cpp:353] Iteration 11300 (5.40311 iter/s, 18.5079s/100 iter), loss = 0.0618568
I0731 20:30:31.360643   939 solver.cpp:375]     Train net output #0: loss = 0.0618568 (* 1 = 0.0618568 loss)
I0731 20:30:31.360647   939 sgd_solver.cpp:136] Iteration 11300, lr = 1e-05, m = 0.9
I0731 20:30:49.963982   939 solver.cpp:353] Iteration 11400 (5.37552 iter/s, 18.6029s/100 iter), loss = 0.0708982
I0731 20:30:49.964035   939 solver.cpp:375]     Train net output #0: loss = 0.0708982 (* 1 = 0.0708982 loss)
I0731 20:30:49.964040   939 sgd_solver.cpp:136] Iteration 11400, lr = 1e-05, m = 0.9
I0731 20:30:50.160792   946 data_reader.cpp:264] Starting prefetch of epoch 5
I0731 20:31:08.436131   939 solver.cpp:353] Iteration 11500 (5.4137 iter/s, 18.4716s/100 iter), loss = 0.0850143
I0731 20:31:08.436153   939 solver.cpp:375]     Train net output #0: loss = 0.0850143 (* 1 = 0.0850143 loss)
I0731 20:31:08.436158   939 sgd_solver.cpp:136] Iteration 11500, lr = 1e-05, m = 0.9
I0731 20:31:20.769757   943 data_reader.cpp:264] Starting prefetch of epoch 8
I0731 20:31:27.042646   939 solver.cpp:353] Iteration 11600 (5.37461 iter/s, 18.606s/100 iter), loss = 0.111212
I0731 20:31:27.042670   939 solver.cpp:375]     Train net output #0: loss = 0.111212 (* 1 = 0.111212 loss)
I0731 20:31:27.042673   939 sgd_solver.cpp:136] Iteration 11600, lr = 1e-05, m = 0.9
I0731 20:31:45.735303   939 solver.cpp:353] Iteration 11700 (5.34984 iter/s, 18.6921s/100 iter), loss = 0.0431036
I0731 20:31:45.735333   939 solver.cpp:375]     Train net output #0: loss = 0.0431036 (* 1 = 0.0431036 loss)
I0731 20:31:45.735340   939 sgd_solver.cpp:136] Iteration 11700, lr = 1e-05, m = 0.9
I0731 20:32:04.242347   939 solver.cpp:353] Iteration 11800 (5.4035 iter/s, 18.5065s/100 iter), loss = 0.0665061
I0731 20:32:04.242413   939 solver.cpp:375]     Train net output #0: loss = 0.0665061 (* 1 = 0.0665061 loss)
I0731 20:32:04.242420   939 sgd_solver.cpp:136] Iteration 11800, lr = 1e-05, m = 0.9
I0731 20:32:22.039140   943 data_reader.cpp:264] Starting prefetch of epoch 9
I0731 20:32:22.745910   939 solver.cpp:353] Iteration 11900 (5.40451 iter/s, 18.5031s/100 iter), loss = 0.0672774
I0731 20:32:22.745939   939 solver.cpp:375]     Train net output #0: loss = 0.0672773 (* 1 = 0.0672773 loss)
I0731 20:32:22.745944   939 sgd_solver.cpp:136] Iteration 11900, lr = 1e-05, m = 0.9
I0731 20:32:41.004896   939 solver.cpp:550] Iteration 12000, Testing net (#0)
I0731 20:32:51.620846   934 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 20:32:52.335296   939 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.950892
I0731 20:32:52.335321   939 solver.cpp:635]     Test net output #1: accuracy/top5 = 1
I0731 20:32:52.335326   939 solver.cpp:635]     Test net output #2: loss = 0.155945 (* 1 = 0.155945 loss)
I0731 20:32:52.335405   939 solver.cpp:305] [MultiGPU] Tests completed in 11.3302s
I0731 20:32:52.545044   939 solver.cpp:353] Iteration 12000 (3.35589 iter/s, 29.7983s/100 iter), loss = 0.0421853
I0731 20:32:52.545071   939 solver.cpp:375]     Train net output #0: loss = 0.0421853 (* 1 = 0.0421853 loss)
I0731 20:32:52.545078   939 sgd_solver.cpp:136] Iteration 12000, lr = 1e-05, m = 0.9
I0731 20:33:04.062234   942 data_reader.cpp:264] Starting prefetch of epoch 12
I0731 20:33:10.969849   939 solver.cpp:353] Iteration 12100 (5.42761 iter/s, 18.4243s/100 iter), loss = 0.0975126
I0731 20:33:10.969873   939 solver.cpp:375]     Train net output #0: loss = 0.0975126 (* 1 = 0.0975126 loss)
I0731 20:33:10.969877   939 sgd_solver.cpp:136] Iteration 12100, lr = 1e-05, m = 0.9
I0731 20:33:29.420258   939 solver.cpp:353] Iteration 12200 (5.42008 iter/s, 18.4499s/100 iter), loss = 0.062694
I0731 20:33:29.420339   939 solver.cpp:375]     Train net output #0: loss = 0.062694 (* 1 = 0.062694 loss)
I0731 20:33:29.420344   939 sgd_solver.cpp:136] Iteration 12200, lr = 1e-05, m = 0.9
I0731 20:33:47.866955   939 solver.cpp:353] Iteration 12300 (5.42117 iter/s, 18.4462s/100 iter), loss = 0.055874
I0731 20:33:47.866981   939 solver.cpp:375]     Train net output #0: loss = 0.055874 (* 1 = 0.055874 loss)
I0731 20:33:47.866984   939 sgd_solver.cpp:136] Iteration 12300, lr = 1e-05, m = 0.9
I0731 20:34:05.065181   946 data_reader.cpp:264] Starting prefetch of epoch 6
I0731 20:34:06.514569   939 solver.cpp:353] Iteration 12400 (5.36276 iter/s, 18.6471s/100 iter), loss = 0.0554459
I0731 20:34:06.514592   939 solver.cpp:375]     Train net output #0: loss = 0.0554459 (* 1 = 0.0554459 loss)
I0731 20:34:06.514597   939 sgd_solver.cpp:136] Iteration 12400, lr = 1e-05, m = 0.9
I0731 20:34:25.117856   939 solver.cpp:353] Iteration 12500 (5.37554 iter/s, 18.6028s/100 iter), loss = 0.0760807
I0731 20:34:25.117882   939 solver.cpp:375]     Train net output #0: loss = 0.0760807 (* 1 = 0.0760807 loss)
I0731 20:34:25.117885   939 sgd_solver.cpp:136] Iteration 12500, lr = 1e-05, m = 0.9
I0731 20:34:43.727246   939 solver.cpp:353] Iteration 12600 (5.37378 iter/s, 18.6089s/100 iter), loss = 0.0486876
I0731 20:34:43.727299   939 solver.cpp:375]     Train net output #0: loss = 0.0486875 (* 1 = 0.0486875 loss)
I0731 20:34:43.727304   939 sgd_solver.cpp:136] Iteration 12600, lr = 1e-05, m = 0.9
I0731 20:35:02.533051   939 solver.cpp:353] Iteration 12700 (5.31765 iter/s, 18.8053s/100 iter), loss = 0.118348
I0731 20:35:02.533078   939 solver.cpp:375]     Train net output #0: loss = 0.118348 (* 1 = 0.118348 loss)
I0731 20:35:02.533085   939 sgd_solver.cpp:136] Iteration 12700, lr = 1e-05, m = 0.9
I0731 20:35:06.848325   894 data_reader.cpp:264] Starting prefetch of epoch 9
I0731 20:35:21.010911   939 solver.cpp:353] Iteration 12800 (5.41203 iter/s, 18.4774s/100 iter), loss = 0.0518142
I0731 20:35:21.011021   939 solver.cpp:375]     Train net output #0: loss = 0.0518142 (* 1 = 0.0518142 loss)
I0731 20:35:21.011029   939 sgd_solver.cpp:136] Iteration 12800, lr = 1e-05, m = 0.9
I0731 20:35:37.404165   892 data_reader.cpp:264] Starting prefetch of epoch 5
I0731 20:35:39.564046   939 solver.cpp:353] Iteration 12900 (5.39007 iter/s, 18.5526s/100 iter), loss = 0.080278
I0731 20:35:39.564070   939 solver.cpp:375]     Train net output #0: loss = 0.0802779 (* 1 = 0.0802779 loss)
I0731 20:35:39.564074   939 sgd_solver.cpp:136] Iteration 12900, lr = 1e-05, m = 0.9
I0731 20:35:58.186383   939 solver.cpp:353] Iteration 13000 (5.37004 iter/s, 18.6218s/100 iter), loss = 0.0585534
I0731 20:35:58.186439   939 solver.cpp:375]     Train net output #0: loss = 0.0585534 (* 1 = 0.0585534 loss)
I0731 20:35:58.186445   939 sgd_solver.cpp:136] Iteration 13000, lr = 1e-05, m = 0.9
I0731 20:36:16.702384   939 solver.cpp:353] Iteration 13100 (5.40088 iter/s, 18.5155s/100 iter), loss = 0.0921866
I0731 20:36:16.702409   939 solver.cpp:375]     Train net output #0: loss = 0.0921866 (* 1 = 0.0921866 loss)
I0731 20:36:16.702412   939 sgd_solver.cpp:136] Iteration 13100, lr = 1e-05, m = 0.9
I0731 20:36:35.172974   939 solver.cpp:353] Iteration 13200 (5.41416 iter/s, 18.4701s/100 iter), loss = 0.0562943
I0731 20:36:35.173049   939 solver.cpp:375]     Train net output #0: loss = 0.0562943 (* 1 = 0.0562943 loss)
I0731 20:36:35.173055   939 sgd_solver.cpp:136] Iteration 13200, lr = 1e-05, m = 0.9
I0731 20:36:38.740443   892 data_reader.cpp:264] Starting prefetch of epoch 6
I0731 20:36:53.725924   939 solver.cpp:353] Iteration 13300 (5.39013 iter/s, 18.5524s/100 iter), loss = 0.0706098
I0731 20:36:53.725949   939 solver.cpp:375]     Train net output #0: loss = 0.0706097 (* 1 = 0.0706097 loss)
I0731 20:36:53.725952   939 sgd_solver.cpp:136] Iteration 13300, lr = 1e-05, m = 0.9
I0731 20:37:12.324774   939 solver.cpp:353] Iteration 13400 (5.37682 iter/s, 18.5983s/100 iter), loss = 0.0531397
I0731 20:37:12.324834   939 solver.cpp:375]     Train net output #0: loss = 0.0531397 (* 1 = 0.0531397 loss)
I0731 20:37:12.324841   939 sgd_solver.cpp:136] Iteration 13400, lr = 1e-05, m = 0.9
I0731 20:37:30.853324   939 solver.cpp:353] Iteration 13500 (5.39722 iter/s, 18.528s/100 iter), loss = 0.077297
I0731 20:37:30.853353   939 solver.cpp:375]     Train net output #0: loss = 0.0772969 (* 1 = 0.0772969 loss)
I0731 20:37:30.853359   939 sgd_solver.cpp:136] Iteration 13500, lr = 1e-05, m = 0.9
I0731 20:37:39.922035   946 data_reader.cpp:264] Starting prefetch of epoch 7
I0731 20:37:49.421243   939 solver.cpp:353] Iteration 13600 (5.38578 iter/s, 18.5674s/100 iter), loss = 0.0852952
I0731 20:37:49.422781   939 solver.cpp:375]     Train net output #0: loss = 0.0852952 (* 1 = 0.0852952 loss)
I0731 20:37:49.422816   939 sgd_solver.cpp:136] Iteration 13600, lr = 1e-05, m = 0.9
I0731 20:38:07.858697   939 solver.cpp:353] Iteration 13700 (5.42389 iter/s, 18.4369s/100 iter), loss = 0.0586072
I0731 20:38:07.858723   939 solver.cpp:375]     Train net output #0: loss = 0.0586072 (* 1 = 0.0586072 loss)
I0731 20:38:07.858727   939 sgd_solver.cpp:136] Iteration 13700, lr = 1e-05, m = 0.9
I0731 20:38:10.704278   946 data_reader.cpp:264] Starting prefetch of epoch 8
I0731 20:38:26.472170   939 solver.cpp:353] Iteration 13800 (5.3726 iter/s, 18.613s/100 iter), loss = 0.0635349
I0731 20:38:26.472223   939 solver.cpp:375]     Train net output #0: loss = 0.0635349 (* 1 = 0.0635349 loss)
I0731 20:38:26.472229   939 sgd_solver.cpp:136] Iteration 13800, lr = 1e-05, m = 0.9
I0731 20:38:45.645328   939 solver.cpp:353] Iteration 13900 (5.21577 iter/s, 19.1726s/100 iter), loss = 0.0795874
I0731 20:38:45.645350   939 solver.cpp:375]     Train net output #0: loss = 0.0795874 (* 1 = 0.0795874 loss)
I0731 20:38:45.645355   939 sgd_solver.cpp:136] Iteration 13900, lr = 1e-05, m = 0.9
I0731 20:39:04.079917   939 solver.cpp:550] Iteration 14000, Testing net (#0)
I0731 20:39:11.365726   964 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 20:39:15.424510   939 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.951185
I0731 20:39:15.424532   939 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999377
I0731 20:39:15.424538   939 solver.cpp:635]     Test net output #2: loss = 0.189781 (* 1 = 0.189781 loss)
I0731 20:39:15.424615   939 solver.cpp:305] [MultiGPU] Tests completed in 11.3444s
I0731 20:39:15.628978   939 solver.cpp:353] Iteration 14000 (3.33524 iter/s, 29.9828s/100 iter), loss = 0.0917251
I0731 20:39:15.629000   939 solver.cpp:375]     Train net output #0: loss = 0.0917251 (* 1 = 0.0917251 loss)
I0731 20:39:15.629005   939 sgd_solver.cpp:136] Iteration 14000, lr = 1e-05, m = 0.9
I0731 20:39:34.119830   939 solver.cpp:353] Iteration 14100 (5.40823 iter/s, 18.4903s/100 iter), loss = 0.0957247
I0731 20:39:34.119877   939 solver.cpp:375]     Train net output #0: loss = 0.0957247 (* 1 = 0.0957247 loss)
I0731 20:39:34.119882   939 sgd_solver.cpp:136] Iteration 14100, lr = 1e-05, m = 0.9
I0731 20:39:52.626406   939 solver.cpp:353] Iteration 14200 (5.40363 iter/s, 18.5061s/100 iter), loss = 0.0651215
I0731 20:39:52.626428   939 solver.cpp:375]     Train net output #0: loss = 0.0651215 (* 1 = 0.0651215 loss)
I0731 20:39:52.626433   939 sgd_solver.cpp:136] Iteration 14200, lr = 1e-05, m = 0.9
I0731 20:39:54.541522   894 data_reader.cpp:264] Starting prefetch of epoch 10
I0731 20:40:11.305819   939 solver.cpp:353] Iteration 14300 (5.35364 iter/s, 18.6789s/100 iter), loss = 0.11724
I0731 20:40:11.305904   939 solver.cpp:375]     Train net output #0: loss = 0.11724 (* 1 = 0.11724 loss)
I0731 20:40:11.305909   939 sgd_solver.cpp:136] Iteration 14300, lr = 1e-05, m = 0.9
I0731 20:40:25.386140   942 data_reader.cpp:264] Starting prefetch of epoch 13
I0731 20:40:29.807031   939 solver.cpp:353] Iteration 14400 (5.4052 iter/s, 18.5007s/100 iter), loss = 0.083212
I0731 20:40:29.807054   939 solver.cpp:375]     Train net output #0: loss = 0.083212 (* 1 = 0.083212 loss)
I0731 20:40:29.807059   939 sgd_solver.cpp:136] Iteration 14400, lr = 1e-05, m = 0.9
I0731 20:40:48.395483   939 solver.cpp:353] Iteration 14500 (5.37983 iter/s, 18.5879s/100 iter), loss = 0.082013
I0731 20:40:48.395540   939 solver.cpp:375]     Train net output #0: loss = 0.0820129 (* 1 = 0.0820129 loss)
I0731 20:40:48.395545   939 sgd_solver.cpp:136] Iteration 14500, lr = 1e-05, m = 0.9
I0731 20:41:06.939455   939 solver.cpp:353] Iteration 14600 (5.39274 iter/s, 18.5435s/100 iter), loss = 0.0690233
I0731 20:41:06.939479   939 solver.cpp:375]     Train net output #0: loss = 0.0690233 (* 1 = 0.0690233 loss)
I0731 20:41:06.939483   939 sgd_solver.cpp:136] Iteration 14600, lr = 1e-05, m = 0.9
I0731 20:41:25.420676   939 solver.cpp:353] Iteration 14700 (5.41105 iter/s, 18.4807s/100 iter), loss = 0.0692289
I0731 20:41:25.420729   939 solver.cpp:375]     Train net output #0: loss = 0.0692289 (* 1 = 0.0692289 loss)
I0731 20:41:25.420735   939 sgd_solver.cpp:136] Iteration 14700, lr = 1e-05, m = 0.9
I0731 20:41:26.572103   943 data_reader.cpp:264] Starting prefetch of epoch 10
I0731 20:41:44.034658   939 solver.cpp:353] Iteration 14800 (5.37245 iter/s, 18.6135s/100 iter), loss = 0.0715818
I0731 20:41:44.034682   939 solver.cpp:375]     Train net output #0: loss = 0.0715818 (* 1 = 0.0715818 loss)
I0731 20:41:44.034685   939 sgd_solver.cpp:136] Iteration 14800, lr = 1e-05, m = 0.9
I0731 20:42:02.501672   939 solver.cpp:353] Iteration 14900 (5.41521 iter/s, 18.4665s/100 iter), loss = 0.0848472
I0731 20:42:02.501782   939 solver.cpp:375]     Train net output #0: loss = 0.0848472 (* 1 = 0.0848472 loss)
I0731 20:42:02.501790   939 sgd_solver.cpp:136] Iteration 14900, lr = 1e-05, m = 0.9
I0731 20:42:21.115388   939 solver.cpp:353] Iteration 15000 (5.37253 iter/s, 18.6132s/100 iter), loss = 0.0622132
I0731 20:42:21.115417   939 solver.cpp:375]     Train net output #0: loss = 0.0622132 (* 1 = 0.0622132 loss)
I0731 20:42:21.115422   939 sgd_solver.cpp:136] Iteration 15000, lr = 1e-05, m = 0.9
I0731 20:42:28.015661   946 data_reader.cpp:264] Starting prefetch of epoch 9
I0731 20:42:39.626898   939 solver.cpp:353] Iteration 15100 (5.40219 iter/s, 18.511s/100 iter), loss = 0.0991368
I0731 20:42:39.626947   939 solver.cpp:375]     Train net output #0: loss = 0.0991368 (* 1 = 0.0991368 loss)
I0731 20:42:39.626952   939 sgd_solver.cpp:136] Iteration 15100, lr = 1e-05, m = 0.9
I0731 20:42:58.179714   939 solver.cpp:353] Iteration 15200 (5.39017 iter/s, 18.5523s/100 iter), loss = 0.0838387
I0731 20:42:58.179738   939 solver.cpp:375]     Train net output #0: loss = 0.0838387 (* 1 = 0.0838387 loss)
I0731 20:42:58.179742   939 sgd_solver.cpp:136] Iteration 15200, lr = 1e-05, m = 0.9
I0731 20:43:16.757056   939 solver.cpp:353] Iteration 15300 (5.38305 iter/s, 18.5768s/100 iter), loss = 0.0382647
I0731 20:43:16.757130   939 solver.cpp:375]     Train net output #0: loss = 0.0382647 (* 1 = 0.0382647 loss)
I0731 20:43:16.757135   939 sgd_solver.cpp:136] Iteration 15300, lr = 1e-05, m = 0.9
I0731 20:43:29.233465   894 data_reader.cpp:264] Starting prefetch of epoch 11
I0731 20:43:29.233465   947 data_reader.cpp:264] Starting prefetch of epoch 8
I0731 20:43:35.380658   939 solver.cpp:353] Iteration 15400 (5.36968 iter/s, 18.6231s/100 iter), loss = 0.0742465
I0731 20:43:35.380713   939 solver.cpp:375]     Train net output #0: loss = 0.0742465 (* 1 = 0.0742465 loss)
I0731 20:43:35.380728   939 sgd_solver.cpp:136] Iteration 15400, lr = 1e-05, m = 0.9
I0731 20:43:54.797899   939 solver.cpp:353] Iteration 15500 (5.1502 iter/s, 19.4167s/100 iter), loss = 0.0759017
I0731 20:43:54.797973   939 solver.cpp:375]     Train net output #0: loss = 0.0759017 (* 1 = 0.0759017 loss)
I0731 20:43:54.797979   939 sgd_solver.cpp:136] Iteration 15500, lr = 1e-05, m = 0.9
I0731 20:44:00.971457   946 data_reader.cpp:264] Starting prefetch of epoch 10
I0731 20:44:13.351469   939 solver.cpp:353] Iteration 15600 (5.38995 iter/s, 18.5531s/100 iter), loss = 0.0672114
I0731 20:44:13.351493   939 solver.cpp:375]     Train net output #0: loss = 0.0672114 (* 1 = 0.0672114 loss)
I0731 20:44:13.351498   939 sgd_solver.cpp:136] Iteration 15600, lr = 1e-05, m = 0.9
I0731 20:44:31.897680   939 solver.cpp:353] Iteration 15700 (5.39209 iter/s, 18.5457s/100 iter), loss = 0.0813442
I0731 20:44:31.897733   939 solver.cpp:375]     Train net output #0: loss = 0.0813442 (* 1 = 0.0813442 loss)
I0731 20:44:31.897740   939 sgd_solver.cpp:136] Iteration 15700, lr = 1e-05, m = 0.9
I0731 20:44:50.403766   939 solver.cpp:353] Iteration 15800 (5.40378 iter/s, 18.5056s/100 iter), loss = 0.0727335
I0731 20:44:50.403794   939 solver.cpp:375]     Train net output #0: loss = 0.0727336 (* 1 = 0.0727336 loss)
I0731 20:44:50.403800   939 sgd_solver.cpp:136] Iteration 15800, lr = 1e-05, m = 0.9
I0731 20:45:02.088675   892 data_reader.cpp:264] Starting prefetch of epoch 7
I0731 20:45:08.874780   939 solver.cpp:353] Iteration 15900 (5.41404 iter/s, 18.4705s/100 iter), loss = 0.053651
I0731 20:45:08.874804   939 solver.cpp:375]     Train net output #0: loss = 0.0536511 (* 1 = 0.0536511 loss)
I0731 20:45:08.874809   939 sgd_solver.cpp:136] Iteration 15900, lr = 1e-05, m = 0.9
I0731 20:45:27.160521   939 solver.cpp:550] Iteration 16000, Testing net (#0)
I0731 20:45:38.169436   936 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 20:45:38.579989   939 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.952044
I0731 20:45:38.580005   939 solver.cpp:635]     Test net output #1: accuracy/top5 = 1
I0731 20:45:38.580011   939 solver.cpp:635]     Test net output #2: loss = 0.155514 (* 1 = 0.155514 loss)
I0731 20:45:38.580075   939 solver.cpp:305] [MultiGPU] Tests completed in 11.4192s
I0731 20:45:38.793963   939 solver.cpp:353] Iteration 16000 (3.34243 iter/s, 29.9184s/100 iter), loss = 0.0828593
I0731 20:45:38.793987   939 solver.cpp:375]     Train net output #0: loss = 0.0828593 (* 1 = 0.0828593 loss)
I0731 20:45:38.793992   939 sgd_solver.cpp:136] Iteration 16000, lr = 1e-05, m = 0.9
I0731 20:45:57.273690   939 solver.cpp:353] Iteration 16100 (5.41148 iter/s, 18.4792s/100 iter), loss = 0.0791324
I0731 20:45:57.273715   939 solver.cpp:375]     Train net output #0: loss = 0.0791325 (* 1 = 0.0791325 loss)
I0731 20:45:57.273720   939 sgd_solver.cpp:136] Iteration 16100, lr = 1e-05, m = 0.9
I0731 20:46:14.680006   894 data_reader.cpp:264] Starting prefetch of epoch 12
I0731 20:46:15.771966   939 solver.cpp:353] Iteration 16200 (5.40606 iter/s, 18.4978s/100 iter), loss = 0.0830605
I0731 20:46:15.771991   939 solver.cpp:375]     Train net output #0: loss = 0.0830605 (* 1 = 0.0830605 loss)
I0731 20:46:15.771996   939 sgd_solver.cpp:136] Iteration 16200, lr = 1e-05, m = 0.9
I0731 20:46:34.286165   939 solver.cpp:353] Iteration 16300 (5.40141 iter/s, 18.5137s/100 iter), loss = 0.0732633
I0731 20:46:34.286190   939 solver.cpp:375]     Train net output #0: loss = 0.0732634 (* 1 = 0.0732634 loss)
I0731 20:46:34.286195   939 sgd_solver.cpp:136] Iteration 16300, lr = 1e-05, m = 0.9
I0731 20:46:45.221496   942 data_reader.cpp:264] Starting prefetch of epoch 14
I0731 20:46:52.802165   939 solver.cpp:353] Iteration 16400 (5.40088 iter/s, 18.5155s/100 iter), loss = 0.0457898
I0731 20:46:52.802192   939 solver.cpp:375]     Train net output #0: loss = 0.0457899 (* 1 = 0.0457899 loss)
I0731 20:46:52.802197   939 sgd_solver.cpp:136] Iteration 16400, lr = 1e-05, m = 0.9
I0731 20:47:11.395292   939 solver.cpp:353] Iteration 16500 (5.37848 iter/s, 18.5926s/100 iter), loss = 0.0700195
I0731 20:47:11.395316   939 solver.cpp:375]     Train net output #0: loss = 0.0700195 (* 1 = 0.0700195 loss)
I0731 20:47:11.395321   939 sgd_solver.cpp:136] Iteration 16500, lr = 1e-05, m = 0.9
I0731 20:47:29.756744   939 solver.cpp:353] Iteration 16600 (5.44634 iter/s, 18.3609s/100 iter), loss = 0.07245
I0731 20:47:29.756826   939 solver.cpp:375]     Train net output #0: loss = 0.0724501 (* 1 = 0.0724501 loss)
I0731 20:47:29.756832   939 sgd_solver.cpp:136] Iteration 16600, lr = 1e-05, m = 0.9
I0731 20:47:46.397233   892 data_reader.cpp:264] Starting prefetch of epoch 8
I0731 20:47:48.268729   939 solver.cpp:353] Iteration 16700 (5.40205 iter/s, 18.5115s/100 iter), loss = 0.0987817
I0731 20:47:48.268752   939 solver.cpp:375]     Train net output #0: loss = 0.0987818 (* 1 = 0.0987818 loss)
I0731 20:47:48.268757   939 sgd_solver.cpp:136] Iteration 16700, lr = 1e-05, m = 0.9
I0731 20:48:07.185649   939 solver.cpp:353] Iteration 16800 (5.28642 iter/s, 18.9164s/100 iter), loss = 0.0810787
I0731 20:48:07.185806   939 solver.cpp:375]     Train net output #0: loss = 0.0810788 (* 1 = 0.0810788 loss)
I0731 20:48:07.185828   939 sgd_solver.cpp:136] Iteration 16800, lr = 1e-05, m = 0.9
I0731 20:48:26.308109   939 solver.cpp:353] Iteration 16900 (5.22959 iter/s, 19.1219s/100 iter), loss = 0.0680191
I0731 20:48:26.308132   939 solver.cpp:375]     Train net output #0: loss = 0.0680192 (* 1 = 0.0680192 loss)
I0731 20:48:26.308137   939 sgd_solver.cpp:136] Iteration 16900, lr = 1e-05, m = 0.9
I0731 20:48:44.817047   939 solver.cpp:353] Iteration 17000 (5.40294 iter/s, 18.5084s/100 iter), loss = 0.109556
I0731 20:48:44.817103   939 solver.cpp:375]     Train net output #0: loss = 0.109556 (* 1 = 0.109556 loss)
I0731 20:48:44.817108   939 sgd_solver.cpp:136] Iteration 17000, lr = 1e-05, m = 0.9
I0731 20:48:48.538607   947 data_reader.cpp:264] Starting prefetch of epoch 9
I0731 20:49:03.393391   939 solver.cpp:353] Iteration 17100 (5.38334 iter/s, 18.5758s/100 iter), loss = 0.0767635
I0731 20:49:03.393414   939 solver.cpp:375]     Train net output #0: loss = 0.0767636 (* 1 = 0.0767636 loss)
I0731 20:49:03.393419   939 sgd_solver.cpp:136] Iteration 17100, lr = 1e-05, m = 0.9
I0731 20:49:19.157716   892 data_reader.cpp:264] Starting prefetch of epoch 9
I0731 20:49:21.889629   939 solver.cpp:353] Iteration 17200 (5.40665 iter/s, 18.4957s/100 iter), loss = 0.0895705
I0731 20:49:21.889654   939 solver.cpp:375]     Train net output #0: loss = 0.0895706 (* 1 = 0.0895706 loss)
I0731 20:49:21.889659   939 sgd_solver.cpp:136] Iteration 17200, lr = 1e-05, m = 0.9
I0731 20:49:40.475559   939 solver.cpp:353] Iteration 17300 (5.38056 iter/s, 18.5854s/100 iter), loss = 0.0540904
I0731 20:49:40.475585   939 solver.cpp:375]     Train net output #0: loss = 0.0540904 (* 1 = 0.0540904 loss)
I0731 20:49:40.475589   939 sgd_solver.cpp:136] Iteration 17300, lr = 1e-05, m = 0.9
I0731 20:49:59.110576   939 solver.cpp:353] Iteration 17400 (5.36639 iter/s, 18.6345s/100 iter), loss = 0.0569095
I0731 20:49:59.110679   939 solver.cpp:375]     Train net output #0: loss = 0.0569096 (* 1 = 0.0569096 loss)
I0731 20:49:59.110687   939 sgd_solver.cpp:136] Iteration 17400, lr = 1e-05, m = 0.9
I0731 20:50:17.710886   939 solver.cpp:353] Iteration 17500 (5.3764 iter/s, 18.5998s/100 iter), loss = 0.0497751
I0731 20:50:17.710909   939 solver.cpp:375]     Train net output #0: loss = 0.0497752 (* 1 = 0.0497752 loss)
I0731 20:50:17.710913   939 sgd_solver.cpp:136] Iteration 17500, lr = 1e-05, m = 0.9
I0731 20:50:20.737745   892 data_reader.cpp:264] Starting prefetch of epoch 10
I0731 20:50:36.311942   939 solver.cpp:353] Iteration 17600 (5.37619 iter/s, 18.6005s/100 iter), loss = 0.0828262
I0731 20:50:36.320150   939 solver.cpp:375]     Train net output #0: loss = 0.0828262 (* 1 = 0.0828262 loss)
I0731 20:50:36.320201   939 sgd_solver.cpp:136] Iteration 17600, lr = 1e-05, m = 0.9
I0731 20:50:54.850461   939 solver.cpp:353] Iteration 17700 (5.39432 iter/s, 18.538s/100 iter), loss = 0.0636286
I0731 20:50:54.850482   939 solver.cpp:375]     Train net output #0: loss = 0.0636286 (* 1 = 0.0636286 loss)
I0731 20:50:54.850486   939 sgd_solver.cpp:136] Iteration 17700, lr = 1e-05, m = 0.9
I0731 20:51:13.427798   939 solver.cpp:353] Iteration 17800 (5.38305 iter/s, 18.5768s/100 iter), loss = 0.0625521
I0731 20:51:13.427850   939 solver.cpp:375]     Train net output #0: loss = 0.0625522 (* 1 = 0.0625522 loss)
I0731 20:51:13.427855   939 sgd_solver.cpp:136] Iteration 17800, lr = 1e-05, m = 0.9
I0731 20:51:21.991533   947 data_reader.cpp:264] Starting prefetch of epoch 10
I0731 20:51:32.029822   939 solver.cpp:353] Iteration 17900 (5.37591 iter/s, 18.6015s/100 iter), loss = 0.0881784
I0731 20:51:32.029845   939 solver.cpp:375]     Train net output #0: loss = 0.0881785 (* 1 = 0.0881785 loss)
I0731 20:51:32.029850   939 sgd_solver.cpp:136] Iteration 17900, lr = 1e-05, m = 0.9
I0731 20:51:50.482477   939 solver.cpp:550] Iteration 18000, Testing net (#0)
I0731 20:51:57.703033   964 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 20:52:01.706744   939 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.951585
I0731 20:52:01.706771   939 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999445
I0731 20:52:01.706778   939 solver.cpp:635]     Test net output #2: loss = 0.189355 (* 1 = 0.189355 loss)
I0731 20:52:01.706799   939 solver.cpp:305] [MultiGPU] Tests completed in 11.224s
I0731 20:52:01.904304   939 solver.cpp:353] Iteration 18000 (3.34743 iter/s, 29.8737s/100 iter), loss = 0.0490804
I0731 20:52:01.904327   939 solver.cpp:375]     Train net output #0: loss = 0.0490804 (* 1 = 0.0490804 loss)
I0731 20:52:01.904332   939 sgd_solver.cpp:136] Iteration 18000, lr = 1e-05, m = 0.9
I0731 20:52:20.324607   939 solver.cpp:353] Iteration 18100 (5.42894 iter/s, 18.4198s/100 iter), loss = 0.058745
I0731 20:52:20.324630   939 solver.cpp:375]     Train net output #0: loss = 0.058745 (* 1 = 0.058745 loss)
I0731 20:52:20.324635   939 sgd_solver.cpp:136] Iteration 18100, lr = 1e-05, m = 0.9
I0731 20:52:34.636981   946 data_reader.cpp:264] Starting prefetch of epoch 11
I0731 20:52:38.852813   939 solver.cpp:353] Iteration 18200 (5.39733 iter/s, 18.5277s/100 iter), loss = 0.0679373
I0731 20:52:38.852843   939 solver.cpp:375]     Train net output #0: loss = 0.0679373 (* 1 = 0.0679373 loss)
I0731 20:52:38.852849   939 sgd_solver.cpp:136] Iteration 18200, lr = 1e-05, m = 0.9
I0731 20:52:57.369592   939 solver.cpp:353] Iteration 18300 (5.40066 iter/s, 18.5163s/100 iter), loss = 0.0761983
I0731 20:52:57.369618   939 solver.cpp:375]     Train net output #0: loss = 0.0761984 (* 1 = 0.0761984 loss)
I0731 20:52:57.369622   939 sgd_solver.cpp:136] Iteration 18300, lr = 1e-05, m = 0.9
I0731 20:53:15.846679   939 solver.cpp:353] Iteration 18400 (5.41226 iter/s, 18.4766s/100 iter), loss = 0.0698998
I0731 20:53:15.846736   939 solver.cpp:375]     Train net output #0: loss = 0.0698999 (* 1 = 0.0698999 loss)
I0731 20:53:15.846741   939 sgd_solver.cpp:136] Iteration 18400, lr = 1e-05, m = 0.9
I0731 20:53:34.331626   939 solver.cpp:353] Iteration 18500 (5.40996 iter/s, 18.4844s/100 iter), loss = 0.0821422
I0731 20:53:34.331647   939 solver.cpp:375]     Train net output #0: loss = 0.0821422 (* 1 = 0.0821422 loss)
I0731 20:53:34.331651   939 sgd_solver.cpp:136] Iteration 18500, lr = 1e-05, m = 0.9
I0731 20:53:35.803184   946 data_reader.cpp:264] Starting prefetch of epoch 12
I0731 20:53:52.934165   939 solver.cpp:353] Iteration 18600 (5.37576 iter/s, 18.602s/100 iter), loss = 0.066207
I0731 20:53:52.934267   939 solver.cpp:375]     Train net output #0: loss = 0.0662071 (* 1 = 0.0662071 loss)
I0731 20:53:52.934274   939 sgd_solver.cpp:136] Iteration 18600, lr = 1e-05, m = 0.9
I0731 20:54:11.327471   939 solver.cpp:353] Iteration 18700 (5.43691 iter/s, 18.3928s/100 iter), loss = 0.0471658
I0731 20:54:11.327497   939 solver.cpp:375]     Train net output #0: loss = 0.0471658 (* 1 = 0.0471658 loss)
I0731 20:54:11.327503   939 sgd_solver.cpp:136] Iteration 18700, lr = 1e-05, m = 0.9
I0731 20:54:29.884342   939 solver.cpp:353] Iteration 18800 (5.38899 iter/s, 18.5564s/100 iter), loss = 0.139459
I0731 20:54:29.884467   939 solver.cpp:375]     Train net output #0: loss = 0.139459 (* 1 = 0.139459 loss)
I0731 20:54:29.884474   939 sgd_solver.cpp:136] Iteration 18800, lr = 1e-05, m = 0.9
I0731 20:54:36.961314   946 data_reader.cpp:264] Starting prefetch of epoch 13
I0731 20:54:48.527128   939 solver.cpp:353] Iteration 18900 (5.36415 iter/s, 18.6423s/100 iter), loss = 0.0740938
I0731 20:54:48.527164   939 solver.cpp:375]     Train net output #0: loss = 0.0740939 (* 1 = 0.0740939 loss)
I0731 20:54:48.527171   939 sgd_solver.cpp:136] Iteration 18900, lr = 1e-05, m = 0.9
I0731 20:55:07.043957   939 solver.cpp:353] Iteration 19000 (5.40064 iter/s, 18.5163s/100 iter), loss = 0.117273
I0731 20:55:07.044014   939 solver.cpp:375]     Train net output #0: loss = 0.117273 (* 1 = 0.117273 loss)
I0731 20:55:07.044019   939 sgd_solver.cpp:136] Iteration 19000, lr = 1e-05, m = 0.9
I0731 20:55:07.634112   892 data_reader.cpp:264] Starting prefetch of epoch 11
I0731 20:55:25.515283   939 solver.cpp:353] Iteration 19100 (5.41395 iter/s, 18.4708s/100 iter), loss = 0.0563159
I0731 20:55:25.515306   939 solver.cpp:375]     Train net output #0: loss = 0.056316 (* 1 = 0.056316 loss)
I0731 20:55:25.515311   939 sgd_solver.cpp:136] Iteration 19100, lr = 1e-05, m = 0.9
I0731 20:55:43.966562   939 solver.cpp:353] Iteration 19200 (5.41983 iter/s, 18.4508s/100 iter), loss = 0.0902543
I0731 20:55:43.966614   939 solver.cpp:375]     Train net output #0: loss = 0.0902544 (* 1 = 0.0902544 loss)
I0731 20:55:43.966619   939 sgd_solver.cpp:136] Iteration 19200, lr = 1e-05, m = 0.9
I0731 20:56:02.476471   939 solver.cpp:353] Iteration 19300 (5.40266 iter/s, 18.5094s/100 iter), loss = 0.09839
I0731 20:56:02.476495   939 solver.cpp:375]     Train net output #0: loss = 0.0983901 (* 1 = 0.0983901 loss)
I0731 20:56:02.476501   939 sgd_solver.cpp:136] Iteration 19300, lr = 1e-05, m = 0.9
I0731 20:56:08.778620   943 data_reader.cpp:264] Starting prefetch of epoch 11
I0731 20:56:21.025800   939 solver.cpp:353] Iteration 19400 (5.39118 iter/s, 18.5488s/100 iter), loss = 0.0960012
I0731 20:56:21.025853   939 solver.cpp:375]     Train net output #0: loss = 0.0960013 (* 1 = 0.0960013 loss)
I0731 20:56:21.025859   939 sgd_solver.cpp:136] Iteration 19400, lr = 1e-05, m = 0.9
I0731 20:56:39.417237   939 solver.cpp:353] Iteration 19500 (5.43746 iter/s, 18.3909s/100 iter), loss = 0.0759387
I0731 20:56:39.417261   939 solver.cpp:375]     Train net output #0: loss = 0.0759388 (* 1 = 0.0759388 loss)
I0731 20:56:39.417265   939 sgd_solver.cpp:136] Iteration 19500, lr = 1e-05, m = 0.9
I0731 20:56:57.874951   939 solver.cpp:353] Iteration 19600 (5.41794 iter/s, 18.4572s/100 iter), loss = 0.0866476
I0731 20:56:57.875003   939 solver.cpp:375]     Train net output #0: loss = 0.0866476 (* 1 = 0.0866476 loss)
I0731 20:56:57.875010   939 sgd_solver.cpp:136] Iteration 19600, lr = 1e-05, m = 0.9
I0731 20:57:09.954059   947 data_reader.cpp:264] Starting prefetch of epoch 11
I0731 20:57:16.490694   939 solver.cpp:353] Iteration 19700 (5.37194 iter/s, 18.6152s/100 iter), loss = 0.393477
I0731 20:57:16.490718   939 solver.cpp:375]     Train net output #0: loss = 0.393477 (* 1 = 0.393477 loss)
I0731 20:57:16.490725   939 sgd_solver.cpp:136] Iteration 19700, lr = 1e-05, m = 0.9
I0731 20:57:34.917773   939 solver.cpp:353] Iteration 19800 (5.42695 iter/s, 18.4266s/100 iter), loss = 0.0709639
I0731 20:57:34.917835   939 solver.cpp:375]     Train net output #0: loss = 0.070964 (* 1 = 0.070964 loss)
I0731 20:57:34.917845   939 sgd_solver.cpp:136] Iteration 19800, lr = 1e-05, m = 0.9
I0731 20:57:40.489784   892 data_reader.cpp:264] Starting prefetch of epoch 12
I0731 20:57:53.635242   939 solver.cpp:353] Iteration 19900 (5.34275 iter/s, 18.717s/100 iter), loss = 0.0659377
I0731 20:57:53.635267   939 solver.cpp:375]     Train net output #0: loss = 0.0659378 (* 1 = 0.0659378 loss)
I0731 20:57:53.635272   939 sgd_solver.cpp:136] Iteration 19900, lr = 1e-05, m = 0.9
I0731 20:58:12.020771   939 solver.cpp:680] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/l1reg/cityscapes5_jsegnet21v2_iter_20000.caffemodel
I0731 20:58:12.081610   939 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/l1reg/cityscapes5_jsegnet21v2_iter_20000.solverstate
I0731 20:58:12.092514   939 solver.cpp:550] Iteration 20000, Testing net (#0)
I0731 20:58:15.439329   968 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 20:58:23.383131   939 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.950996
I0731 20:58:23.383153   939 solver.cpp:635]     Test net output #1: accuracy/top5 = 1
I0731 20:58:23.383158   939 solver.cpp:635]     Test net output #2: loss = 0.158346 (* 1 = 0.158346 loss)
I0731 20:58:23.383235   939 solver.cpp:305] [MultiGPU] Tests completed in 11.2904s
I0731 20:58:23.574241   939 solver.cpp:353] Iteration 20000 (3.34022 iter/s, 29.9382s/100 iter), loss = 0.0512986
I0731 20:58:23.574265   939 solver.cpp:375]     Train net output #0: loss = 0.0512986 (* 1 = 0.0512986 loss)
I0731 20:58:23.574268   939 sgd_solver.cpp:136] Iteration 20000, lr = 1e-05, m = 0.9
I0731 20:58:42.067914   939 solver.cpp:353] Iteration 20100 (5.40741 iter/s, 18.4932s/100 iter), loss = 0.0434395
I0731 20:58:42.067966   939 solver.cpp:375]     Train net output #0: loss = 0.0434396 (* 1 = 0.0434396 loss)
I0731 20:58:42.067971   939 sgd_solver.cpp:136] Iteration 20100, lr = 1e-05, m = 0.9
I0731 20:58:53.194483   947 data_reader.cpp:264] Starting prefetch of epoch 12
I0731 20:59:00.596474   939 solver.cpp:353] Iteration 20200 (5.39722 iter/s, 18.528s/100 iter), loss = 0.0937129
I0731 20:59:00.596498   939 solver.cpp:375]     Train net output #0: loss = 0.0937129 (* 1 = 0.0937129 loss)
I0731 20:59:00.596503   939 sgd_solver.cpp:136] Iteration 20200, lr = 1e-05, m = 0.9
I0731 20:59:19.105165   939 solver.cpp:353] Iteration 20300 (5.40302 iter/s, 18.5082s/100 iter), loss = 0.0662116
I0731 20:59:19.105216   939 solver.cpp:375]     Train net output #0: loss = 0.0662116 (* 1 = 0.0662116 loss)
I0731 20:59:19.105221   939 sgd_solver.cpp:136] Iteration 20300, lr = 1e-05, m = 0.9
I0731 20:59:23.969076   942 data_reader.cpp:264] Starting prefetch of epoch 15
I0731 20:59:37.544450   939 solver.cpp:353] Iteration 20400 (5.42335 iter/s, 18.4388s/100 iter), loss = 0.0727837
I0731 20:59:37.544473   939 solver.cpp:375]     Train net output #0: loss = 0.0727838 (* 1 = 0.0727838 loss)
I0731 20:59:37.544477   939 sgd_solver.cpp:136] Iteration 20400, lr = 1e-05, m = 0.9
I0731 20:59:56.009604   939 solver.cpp:353] Iteration 20500 (5.41575 iter/s, 18.4646s/100 iter), loss = 0.0600298
I0731 20:59:56.009654   939 solver.cpp:375]     Train net output #0: loss = 0.0600299 (* 1 = 0.0600299 loss)
I0731 20:59:56.009660   939 sgd_solver.cpp:136] Iteration 20500, lr = 1e-05, m = 0.9
I0731 21:00:14.679563   939 solver.cpp:353] Iteration 20600 (5.35635 iter/s, 18.6694s/100 iter), loss = 0.0593681
I0731 21:00:14.679584   939 solver.cpp:375]     Train net output #0: loss = 0.0593682 (* 1 = 0.0593682 loss)
I0731 21:00:14.679589   939 sgd_solver.cpp:136] Iteration 20600, lr = 1e-05, m = 0.9
I0731 21:00:25.087661   943 data_reader.cpp:264] Starting prefetch of epoch 12
I0731 21:00:33.129458   939 solver.cpp:353] Iteration 20700 (5.42023 iter/s, 18.4494s/100 iter), loss = 0.0493263
I0731 21:00:33.129513   939 solver.cpp:375]     Train net output #0: loss = 0.0493263 (* 1 = 0.0493263 loss)
I0731 21:00:33.129518   939 sgd_solver.cpp:136] Iteration 20700, lr = 1e-05, m = 0.9
I0731 21:00:51.621429   939 solver.cpp:353] Iteration 20800 (5.4079 iter/s, 18.4915s/100 iter), loss = 0.0438492
I0731 21:00:51.621451   939 solver.cpp:375]     Train net output #0: loss = 0.0438492 (* 1 = 0.0438492 loss)
I0731 21:00:51.621455   939 sgd_solver.cpp:136] Iteration 20800, lr = 1e-05, m = 0.9
I0731 21:01:10.072343   939 solver.cpp:353] Iteration 20900 (5.41993 iter/s, 18.4504s/100 iter), loss = 0.0516629
I0731 21:01:10.072438   939 solver.cpp:375]     Train net output #0: loss = 0.051663 (* 1 = 0.051663 loss)
I0731 21:01:10.072445   939 sgd_solver.cpp:136] Iteration 20900, lr = 1e-05, m = 0.9
I0731 21:01:26.192036   946 data_reader.cpp:264] Starting prefetch of epoch 14
I0731 21:01:28.562803   939 solver.cpp:353] Iteration 21000 (5.40834 iter/s, 18.49s/100 iter), loss = 0.0778184
I0731 21:01:28.562829   939 solver.cpp:375]     Train net output #0: loss = 0.0778185 (* 1 = 0.0778185 loss)
I0731 21:01:28.562834   939 sgd_solver.cpp:136] Iteration 21000, lr = 1e-05, m = 0.9
I0731 21:01:47.017199   939 solver.cpp:353] Iteration 21100 (5.41891 iter/s, 18.4539s/100 iter), loss = 0.0584773
I0731 21:01:47.017248   939 solver.cpp:375]     Train net output #0: loss = 0.0584773 (* 1 = 0.0584773 loss)
I0731 21:01:47.017254   939 sgd_solver.cpp:136] Iteration 21100, lr = 1e-05, m = 0.9
I0731 21:01:56.673998   942 data_reader.cpp:264] Starting prefetch of epoch 16
I0731 21:02:05.480778   939 solver.cpp:353] Iteration 21200 (5.41622 iter/s, 18.4631s/100 iter), loss = 0.0687939
I0731 21:02:05.480803   939 solver.cpp:375]     Train net output #0: loss = 0.0687939 (* 1 = 0.0687939 loss)
I0731 21:02:05.480808   939 sgd_solver.cpp:136] Iteration 21200, lr = 1e-05, m = 0.9
I0731 21:02:24.125124   939 solver.cpp:353] Iteration 21300 (5.3637 iter/s, 18.6438s/100 iter), loss = 0.0833647
I0731 21:02:24.125205   939 solver.cpp:375]     Train net output #0: loss = 0.0833647 (* 1 = 0.0833647 loss)
I0731 21:02:24.125210   939 sgd_solver.cpp:136] Iteration 21300, lr = 1e-05, m = 0.9
I0731 21:02:42.688063   939 solver.cpp:353] Iteration 21400 (5.38723 iter/s, 18.5624s/100 iter), loss = 0.0602999
I0731 21:02:42.688087   939 solver.cpp:375]     Train net output #0: loss = 0.0603 (* 1 = 0.0603 loss)
I0731 21:02:42.688092   939 sgd_solver.cpp:136] Iteration 21400, lr = 1e-05, m = 0.9
I0731 21:02:58.139914   892 data_reader.cpp:264] Starting prefetch of epoch 13
I0731 21:03:01.240658   939 solver.cpp:353] Iteration 21500 (5.39023 iter/s, 18.5521s/100 iter), loss = 0.0496321
I0731 21:03:01.240679   939 solver.cpp:375]     Train net output #0: loss = 0.0496322 (* 1 = 0.0496322 loss)
I0731 21:03:01.240684   939 sgd_solver.cpp:136] Iteration 21500, lr = 1e-05, m = 0.9
I0731 21:03:19.712658   939 solver.cpp:353] Iteration 21600 (5.41375 iter/s, 18.4715s/100 iter), loss = 0.0773465
I0731 21:03:19.712683   939 solver.cpp:375]     Train net output #0: loss = 0.0773465 (* 1 = 0.0773465 loss)
I0731 21:03:19.712687   939 sgd_solver.cpp:136] Iteration 21600, lr = 1e-05, m = 0.9
I0731 21:03:38.317005   939 solver.cpp:353] Iteration 21700 (5.37524 iter/s, 18.6038s/100 iter), loss = 0.0566411
I0731 21:03:38.317059   939 solver.cpp:375]     Train net output #0: loss = 0.0566412 (* 1 = 0.0566412 loss)
I0731 21:03:38.317065   939 sgd_solver.cpp:136] Iteration 21700, lr = 1e-05, m = 0.9
I0731 21:03:56.924098   939 solver.cpp:353] Iteration 21800 (5.37444 iter/s, 18.6066s/100 iter), loss = 0.0675101
I0731 21:03:56.924123   939 solver.cpp:375]     Train net output #0: loss = 0.0675102 (* 1 = 0.0675102 loss)
I0731 21:03:56.924126   939 sgd_solver.cpp:136] Iteration 21800, lr = 1e-05, m = 0.9
I0731 21:03:59.365932   942 data_reader.cpp:264] Starting prefetch of epoch 17
I0731 21:04:15.356528   939 solver.cpp:353] Iteration 21900 (5.42537 iter/s, 18.4319s/100 iter), loss = 0.0664937
I0731 21:04:15.356578   939 solver.cpp:375]     Train net output #0: loss = 0.0664937 (* 1 = 0.0664937 loss)
I0731 21:04:15.356585   939 sgd_solver.cpp:136] Iteration 21900, lr = 1e-05, m = 0.9
I0731 21:04:33.742339   939 solver.cpp:550] Iteration 22000, Testing net (#0)
I0731 21:04:44.735383   968 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 21:04:50.559931   939 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.952044
I0731 21:04:50.560056   939 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999556
I0731 21:04:50.560066   939 solver.cpp:635]     Test net output #2: loss = 0.187762 (* 1 = 0.187762 loss)
I0731 21:04:50.560092   939 solver.cpp:305] [MultiGPU] Tests completed in 16.8173s
I0731 21:04:50.769841   939 solver.cpp:353] Iteration 22000 (2.82387 iter/s, 35.4123s/100 iter), loss = 0.0866886
I0731 21:04:50.769865   939 solver.cpp:375]     Train net output #0: loss = 0.0866887 (* 1 = 0.0866887 loss)
I0731 21:04:50.769870   939 sgd_solver.cpp:136] Iteration 22000, lr = 1e-05, m = 0.9
I0731 21:05:10.767659   939 solver.cpp:353] Iteration 22100 (5.00068 iter/s, 19.9973s/100 iter), loss = 0.0588401
I0731 21:05:10.767688   939 solver.cpp:375]     Train net output #0: loss = 0.0588401 (* 1 = 0.0588401 loss)
I0731 21:05:10.767691   939 sgd_solver.cpp:136] Iteration 22100, lr = 1e-05, m = 0.9
I0731 21:05:19.094565   943 data_reader.cpp:264] Starting prefetch of epoch 13
I0731 21:05:29.424554   939 solver.cpp:353] Iteration 22200 (5.3601 iter/s, 18.6564s/100 iter), loss = 0.0782181
I0731 21:05:29.424628   939 solver.cpp:375]     Train net output #0: loss = 0.0782181 (* 1 = 0.0782181 loss)
I0731 21:05:29.424633   939 sgd_solver.cpp:136] Iteration 22200, lr = 1e-05, m = 0.9
I0731 21:05:47.859737   939 solver.cpp:353] Iteration 22300 (5.42456 iter/s, 18.4347s/100 iter), loss = 0.0477387
I0731 21:05:47.859763   939 solver.cpp:375]     Train net output #0: loss = 0.0477387 (* 1 = 0.0477387 loss)
I0731 21:05:47.859767   939 sgd_solver.cpp:136] Iteration 22300, lr = 1e-05, m = 0.9
I0731 21:06:06.467952   939 solver.cpp:353] Iteration 22400 (5.37412 iter/s, 18.6077s/100 iter), loss = 0.0560148
I0731 21:06:06.468052   939 solver.cpp:375]     Train net output #0: loss = 0.0560149 (* 1 = 0.0560149 loss)
I0731 21:06:06.468060   939 sgd_solver.cpp:136] Iteration 22400, lr = 1e-05, m = 0.9
I0731 21:06:20.336583   943 data_reader.cpp:264] Starting prefetch of epoch 14
I0731 21:06:25.116791   939 solver.cpp:353] Iteration 22500 (5.36241 iter/s, 18.6483s/100 iter), loss = 0.0693913
I0731 21:06:25.116813   939 solver.cpp:375]     Train net output #0: loss = 0.0693914 (* 1 = 0.0693914 loss)
I0731 21:06:25.116822   939 sgd_solver.cpp:136] Iteration 22500, lr = 1e-05, m = 0.9
I0731 21:06:43.553745   939 solver.cpp:353] Iteration 22600 (5.42404 iter/s, 18.4364s/100 iter), loss = 0.0473462
I0731 21:06:43.553802   939 solver.cpp:375]     Train net output #0: loss = 0.0473463 (* 1 = 0.0473463 loss)
I0731 21:06:43.553807   939 sgd_solver.cpp:136] Iteration 22600, lr = 1e-05, m = 0.9
I0731 21:07:02.062702   939 solver.cpp:353] Iteration 22700 (5.40294 iter/s, 18.5084s/100 iter), loss = 0.0760019
I0731 21:07:02.062729   939 solver.cpp:375]     Train net output #0: loss = 0.0760019 (* 1 = 0.0760019 loss)
I0731 21:07:02.062734   939 sgd_solver.cpp:136] Iteration 22700, lr = 1e-05, m = 0.9
I0731 21:07:20.495075   939 solver.cpp:353] Iteration 22800 (5.42539 iter/s, 18.4319s/100 iter), loss = 0.0444027
I0731 21:07:20.495131   939 solver.cpp:375]     Train net output #0: loss = 0.0444027 (* 1 = 0.0444027 loss)
I0731 21:07:20.495138   939 sgd_solver.cpp:136] Iteration 22800, lr = 1e-05, m = 0.9
I0731 21:07:21.483717   947 data_reader.cpp:264] Starting prefetch of epoch 13
I0731 21:07:39.116727   939 solver.cpp:353] Iteration 22900 (5.37024 iter/s, 18.6211s/100 iter), loss = 0.0751983
I0731 21:07:39.116750   939 solver.cpp:375]     Train net output #0: loss = 0.0751984 (* 1 = 0.0751984 loss)
I0731 21:07:39.116755   939 sgd_solver.cpp:136] Iteration 22900, lr = 1e-05, m = 0.9
I0731 21:07:57.673069   939 solver.cpp:353] Iteration 23000 (5.38914 iter/s, 18.5558s/100 iter), loss = 0.086472
I0731 21:07:57.673117   939 solver.cpp:375]     Train net output #0: loss = 0.0864721 (* 1 = 0.0864721 loss)
I0731 21:07:57.673123   939 sgd_solver.cpp:136] Iteration 23000, lr = 1e-05, m = 0.9
I0731 21:08:16.235271   939 solver.cpp:353] Iteration 23100 (5.38744 iter/s, 18.5617s/100 iter), loss = 0.0576055
I0731 21:08:16.235296   939 solver.cpp:375]     Train net output #0: loss = 0.0576056 (* 1 = 0.0576056 loss)
I0731 21:08:16.235299   939 sgd_solver.cpp:136] Iteration 23100, lr = 1e-05, m = 0.9
I0731 21:08:22.733002   946 data_reader.cpp:264] Starting prefetch of epoch 15
I0731 21:08:34.806177   939 solver.cpp:353] Iteration 23200 (5.38491 iter/s, 18.5704s/100 iter), loss = 0.0519157
I0731 21:08:34.806251   939 solver.cpp:375]     Train net output #0: loss = 0.0519157 (* 1 = 0.0519157 loss)
I0731 21:08:34.806257   939 sgd_solver.cpp:136] Iteration 23200, lr = 1e-05, m = 0.9
I0731 21:08:53.147899   939 solver.cpp:353] Iteration 23300 (5.4522 iter/s, 18.3412s/100 iter), loss = 0.0422721
I0731 21:08:53.147923   939 solver.cpp:375]     Train net output #0: loss = 0.0422721 (* 1 = 0.0422721 loss)
I0731 21:08:53.147927   939 sgd_solver.cpp:136] Iteration 23300, lr = 1e-05, m = 0.9
I0731 21:08:53.363098   942 data_reader.cpp:264] Starting prefetch of epoch 18
I0731 21:09:11.718912   939 solver.cpp:353] Iteration 23400 (5.38488 iter/s, 18.5705s/100 iter), loss = 0.0534979
I0731 21:09:11.718986   939 solver.cpp:375]     Train net output #0: loss = 0.0534979 (* 1 = 0.0534979 loss)
I0731 21:09:11.718991   939 sgd_solver.cpp:136] Iteration 23400, lr = 1e-05, m = 0.9
I0731 21:09:30.239451   939 solver.cpp:353] Iteration 23500 (5.39956 iter/s, 18.52s/100 iter), loss = 0.200214
I0731 21:09:30.239476   939 solver.cpp:375]     Train net output #0: loss = 0.200214 (* 1 = 0.200214 loss)
I0731 21:09:30.239480   939 sgd_solver.cpp:136] Iteration 23500, lr = 1e-05, m = 0.9
I0731 21:09:48.747464   939 solver.cpp:353] Iteration 23600 (5.40321 iter/s, 18.5075s/100 iter), loss = 0.0441283
I0731 21:09:48.747514   939 solver.cpp:375]     Train net output #0: loss = 0.0441284 (* 1 = 0.0441284 loss)
I0731 21:09:48.747520   939 sgd_solver.cpp:136] Iteration 23600, lr = 1e-05, m = 0.9
I0731 21:09:54.480120   892 data_reader.cpp:264] Starting prefetch of epoch 14
I0731 21:10:07.384369   939 solver.cpp:353] Iteration 23700 (5.36585 iter/s, 18.6364s/100 iter), loss = 0.064901
I0731 21:10:07.384394   939 solver.cpp:375]     Train net output #0: loss = 0.0649011 (* 1 = 0.0649011 loss)
I0731 21:10:07.384399   939 sgd_solver.cpp:136] Iteration 23700, lr = 1e-05, m = 0.9
I0731 21:10:25.946593   939 solver.cpp:353] Iteration 23800 (5.38743 iter/s, 18.5617s/100 iter), loss = 0.0935597
I0731 21:10:25.946650   939 solver.cpp:375]     Train net output #0: loss = 0.0935598 (* 1 = 0.0935598 loss)
I0731 21:10:25.946655   939 sgd_solver.cpp:136] Iteration 23800, lr = 1e-05, m = 0.9
I0731 21:10:44.486723   939 solver.cpp:353] Iteration 23900 (5.39385 iter/s, 18.5396s/100 iter), loss = 0.0616524
I0731 21:10:44.486752   939 solver.cpp:375]     Train net output #0: loss = 0.0616525 (* 1 = 0.0616525 loss)
I0731 21:10:44.486758   939 sgd_solver.cpp:136] Iteration 23900, lr = 1e-05, m = 0.9
I0731 21:10:56.213419   946 data_reader.cpp:264] Starting prefetch of epoch 16
I0731 21:11:03.049525   939 solver.cpp:550] Iteration 24000, Testing net (#0)
I0731 21:11:06.443019   964 data_reader.cpp:264] Starting prefetch of epoch 4
I0731 21:11:14.191648   939 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.950746
I0731 21:11:14.191673   939 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999997
I0731 21:11:14.191679   939 solver.cpp:635]     Test net output #2: loss = 0.160371 (* 1 = 0.160371 loss)
I0731 21:11:14.191701   939 solver.cpp:305] [MultiGPU] Tests completed in 11.1419s
I0731 21:11:14.292558   995 sgd_solver.cpp:48] MultiStep Status: Iteration 24000, step = 1
I0731 21:11:14.292558   994 sgd_solver.cpp:48] MultiStep Status: Iteration 24000, step = 1
I0731 21:11:14.292558   993 sgd_solver.cpp:48] MultiStep Status: Iteration 24000, step = 1
I0731 21:11:14.399313   939 solver.cpp:353] Iteration 24000 (3.34316 iter/s, 29.9118s/100 iter), loss = 0.084097
I0731 21:11:14.399336   939 solver.cpp:375]     Train net output #0: loss = 0.0840971 (* 1 = 0.0840971 loss)
I0731 21:11:14.399341   939 sgd_solver.cpp:136] Iteration 24000, lr = 1e-06, m = 0.9
I0731 21:11:32.837975   939 solver.cpp:353] Iteration 24100 (5.42354 iter/s, 18.4382s/100 iter), loss = 0.0736763
I0731 21:11:32.845600   939 solver.cpp:375]     Train net output #0: loss = 0.0736764 (* 1 = 0.0736764 loss)
I0731 21:11:32.845657   939 sgd_solver.cpp:136] Iteration 24100, lr = 1e-06, m = 0.9
I0731 21:11:37.857512   892 data_reader.cpp:264] Starting prefetch of epoch 15
I0731 21:11:51.422911   939 solver.cpp:353] Iteration 24200 (5.38085 iter/s, 18.5844s/100 iter), loss = 0.0923543
I0731 21:11:51.422938   939 solver.cpp:375]     Train net output #0: loss = 0.0923543 (* 1 = 0.0923543 loss)
I0731 21:11:51.422945   939 sgd_solver.cpp:136] Iteration 24200, lr = 1e-06, m = 0.9
I0731 21:12:09.868685   939 solver.cpp:353] Iteration 24300 (5.42145 iter/s, 18.4453s/100 iter), loss = 0.0561257
I0731 21:12:09.868760   939 solver.cpp:375]     Train net output #0: loss = 0.0561258 (* 1 = 0.0561258 loss)
I0731 21:12:09.868767   939 sgd_solver.cpp:136] Iteration 24300, lr = 1e-06, m = 0.9
I0731 21:12:28.413172   939 solver.cpp:353] Iteration 24400 (5.39259 iter/s, 18.544s/100 iter), loss = 0.072673
I0731 21:12:28.413197   939 solver.cpp:375]     Train net output #0: loss = 0.0726731 (* 1 = 0.0726731 loss)
I0731 21:12:28.413203   939 sgd_solver.cpp:136] Iteration 24400, lr = 1e-06, m = 0.9
I0731 21:12:39.173022   947 data_reader.cpp:264] Starting prefetch of epoch 14
I0731 21:12:46.928208   939 solver.cpp:353] Iteration 24500 (5.40116 iter/s, 18.5145s/100 iter), loss = 0.0502452
I0731 21:12:46.928256   939 solver.cpp:375]     Train net output #0: loss = 0.0502452 (* 1 = 0.0502452 loss)
I0731 21:12:46.928261   939 sgd_solver.cpp:136] Iteration 24500, lr = 1e-06, m = 0.9
I0731 21:13:05.487732   939 solver.cpp:353] Iteration 24600 (5.38822 iter/s, 18.559s/100 iter), loss = 0.098129
I0731 21:13:05.487756   939 solver.cpp:375]     Train net output #0: loss = 0.0981291 (* 1 = 0.0981291 loss)
I0731 21:13:05.487761   939 sgd_solver.cpp:136] Iteration 24600, lr = 1e-06, m = 0.9
I0731 21:13:23.963021   939 solver.cpp:353] Iteration 24700 (5.41278 iter/s, 18.4748s/100 iter), loss = 0.0681087
I0731 21:13:23.963121   939 solver.cpp:375]     Train net output #0: loss = 0.0681087 (* 1 = 0.0681087 loss)
I0731 21:13:23.963129   939 sgd_solver.cpp:136] Iteration 24700, lr = 1e-06, m = 0.9
I0731 21:13:40.206693   947 data_reader.cpp:264] Starting prefetch of epoch 15
I0731 21:13:42.409831   939 solver.cpp:353] Iteration 24800 (5.42114 iter/s, 18.4463s/100 iter), loss = 0.101672
I0731 21:13:42.409857   939 solver.cpp:375]     Train net output #0: loss = 0.101673 (* 1 = 0.101673 loss)
I0731 21:13:42.409862   939 sgd_solver.cpp:136] Iteration 24800, lr = 1e-06, m = 0.9
I0731 21:14:00.982034   939 solver.cpp:353] Iteration 24900 (5.38454 iter/s, 18.5717s/100 iter), loss = 0.063454
I0731 21:14:00.982142   939 solver.cpp:375]     Train net output #0: loss = 0.0634541 (* 1 = 0.0634541 loss)
I0731 21:14:00.982149   939 sgd_solver.cpp:136] Iteration 24900, lr = 1e-06, m = 0.9
I0731 21:14:11.018896   943 data_reader.cpp:264] Starting prefetch of epoch 15
I0731 21:14:19.797041   939 solver.cpp:353] Iteration 25000 (5.31505 iter/s, 18.8145s/100 iter), loss = 0.0726288
I0731 21:14:19.797067   939 solver.cpp:375]     Train net output #0: loss = 0.0726288 (* 1 = 0.0726288 loss)
I0731 21:14:19.797072   939 sgd_solver.cpp:136] Iteration 25000, lr = 1e-06, m = 0.9
I0731 21:14:39.881033   939 solver.cpp:353] Iteration 25100 (4.97923 iter/s, 20.0834s/100 iter), loss = 0.0593327
I0731 21:14:39.881150   939 solver.cpp:375]     Train net output #0: loss = 0.0593327 (* 1 = 0.0593327 loss)
I0731 21:14:39.881156   939 sgd_solver.cpp:136] Iteration 25100, lr = 1e-06, m = 0.9
I0731 21:14:59.970604   939 solver.cpp:353] Iteration 25200 (4.97784 iter/s, 20.089s/100 iter), loss = 0.0865271
I0731 21:14:59.970628   939 solver.cpp:375]     Train net output #0: loss = 0.0865271 (* 1 = 0.0865271 loss)
I0731 21:14:59.970633   939 sgd_solver.cpp:136] Iteration 25200, lr = 1e-06, m = 0.9
I0731 21:15:15.667268   947 data_reader.cpp:264] Starting prefetch of epoch 16
I0731 21:15:18.563088   939 solver.cpp:353] Iteration 25300 (5.37867 iter/s, 18.592s/100 iter), loss = 0.0835865
I0731 21:15:18.563112   939 solver.cpp:375]     Train net output #0: loss = 0.0835865 (* 1 = 0.0835865 loss)
I0731 21:15:18.563117   939 sgd_solver.cpp:136] Iteration 25300, lr = 1e-06, m = 0.9
I0731 21:15:37.133354   939 solver.cpp:353] Iteration 25400 (5.3851 iter/s, 18.5698s/100 iter), loss = 0.0782822
I0731 21:15:37.133378   939 solver.cpp:375]     Train net output #0: loss = 0.0782822 (* 1 = 0.0782822 loss)
I0731 21:15:37.133383   939 sgd_solver.cpp:136] Iteration 25400, lr = 1e-06, m = 0.9
I0731 21:15:55.650367   939 solver.cpp:353] Iteration 25500 (5.40059 iter/s, 18.5165s/100 iter), loss = 0.0535741
I0731 21:15:55.650496   939 solver.cpp:375]     Train net output #0: loss = 0.0535741 (* 1 = 0.0535741 loss)
I0731 21:15:55.650503   939 sgd_solver.cpp:136] Iteration 25500, lr = 1e-06, m = 0.9
I0731 21:16:14.129472   939 solver.cpp:353] Iteration 25600 (5.41167 iter/s, 18.4786s/100 iter), loss = 0.0638608
I0731 21:16:14.129501   939 solver.cpp:375]     Train net output #0: loss = 0.0638609 (* 1 = 0.0638609 loss)
I0731 21:16:14.129508   939 sgd_solver.cpp:136] Iteration 25600, lr = 1e-06, m = 0.9
I0731 21:16:16.948148   894 data_reader.cpp:264] Starting prefetch of epoch 13
I0731 21:16:32.617113   939 solver.cpp:353] Iteration 25700 (5.40917 iter/s, 18.4871s/100 iter), loss = 0.0458184
I0731 21:16:32.617167   939 solver.cpp:375]     Train net output #0: loss = 0.0458185 (* 1 = 0.0458185 loss)
I0731 21:16:32.617173   939 sgd_solver.cpp:136] Iteration 25700, lr = 1e-06, m = 0.9
I0731 21:16:47.459816   892 data_reader.cpp:264] Starting prefetch of epoch 16
I0731 21:16:51.093902   939 solver.cpp:353] Iteration 25800 (5.41234 iter/s, 18.4763s/100 iter), loss = 0.0698513
I0731 21:16:51.093927   939 solver.cpp:375]     Train net output #0: loss = 0.0698514 (* 1 = 0.0698514 loss)
I0731 21:16:51.093933   939 sgd_solver.cpp:136] Iteration 25800, lr = 1e-06, m = 0.9
I0731 21:17:09.550305   939 solver.cpp:353] Iteration 25900 (5.41832 iter/s, 18.4559s/100 iter), loss = 0.0549739
I0731 21:17:09.550355   939 solver.cpp:375]     Train net output #0: loss = 0.054974 (* 1 = 0.054974 loss)
I0731 21:17:09.550361   939 sgd_solver.cpp:136] Iteration 25900, lr = 1e-06, m = 0.9
I0731 21:17:27.833823   939 solver.cpp:550] Iteration 26000, Testing net (#0)
I0731 21:17:34.967453   971 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 21:17:39.045761   939 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.952092
I0731 21:17:39.045781   939 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999373
I0731 21:17:39.045788   939 solver.cpp:635]     Test net output #2: loss = 0.194979 (* 1 = 0.194979 loss)
I0731 21:17:39.045878   939 solver.cpp:305] [MultiGPU] Tests completed in 11.2117s
I0731 21:17:39.248468   939 solver.cpp:353] Iteration 26000 (3.3673 iter/s, 29.6974s/100 iter), loss = 0.0848278
I0731 21:17:39.248492   939 solver.cpp:375]     Train net output #0: loss = 0.0848278 (* 1 = 0.0848278 loss)
I0731 21:17:39.248495   939 sgd_solver.cpp:136] Iteration 26000, lr = 1e-06, m = 0.9
I0731 21:17:57.709962   939 solver.cpp:353] Iteration 26100 (5.41683 iter/s, 18.461s/100 iter), loss = 0.0440621
I0731 21:17:57.710072   939 solver.cpp:375]     Train net output #0: loss = 0.0440622 (* 1 = 0.0440622 loss)
I0731 21:17:57.710079   939 sgd_solver.cpp:136] Iteration 26100, lr = 1e-06, m = 0.9
I0731 21:18:16.163537   939 solver.cpp:353] Iteration 26200 (5.41915 iter/s, 18.4531s/100 iter), loss = 0.0912828
I0731 21:18:16.163560   939 solver.cpp:375]     Train net output #0: loss = 0.0912829 (* 1 = 0.0912829 loss)
I0731 21:18:16.163564   939 sgd_solver.cpp:136] Iteration 26200, lr = 1e-06, m = 0.9
I0731 21:18:30.247458   946 data_reader.cpp:264] Starting prefetch of epoch 17
I0731 21:18:34.655086   939 solver.cpp:353] Iteration 26300 (5.40803 iter/s, 18.491s/100 iter), loss = 0.0777734
I0731 21:18:34.655108   939 solver.cpp:375]     Train net output #0: loss = 0.0777734 (* 1 = 0.0777734 loss)
I0731 21:18:34.655113   939 sgd_solver.cpp:136] Iteration 26300, lr = 1e-06, m = 0.9
I0731 21:18:53.174218   939 solver.cpp:353] Iteration 26400 (5.39997 iter/s, 18.5186s/100 iter), loss = 0.0900482
I0731 21:18:53.174242   939 solver.cpp:375]     Train net output #0: loss = 0.0900483 (* 1 = 0.0900483 loss)
I0731 21:18:53.174245   939 sgd_solver.cpp:136] Iteration 26400, lr = 1e-06, m = 0.9
I0731 21:19:00.780333   942 data_reader.cpp:264] Starting prefetch of epoch 19
I0731 21:19:11.697208   939 solver.cpp:353] Iteration 26500 (5.39884 iter/s, 18.5225s/100 iter), loss = 0.0465642
I0731 21:19:11.697234   939 solver.cpp:375]     Train net output #0: loss = 0.0465643 (* 1 = 0.0465643 loss)
I0731 21:19:11.697239   939 sgd_solver.cpp:136] Iteration 26500, lr = 1e-06, m = 0.9
I0731 21:19:30.300449   939 solver.cpp:353] Iteration 26600 (5.37556 iter/s, 18.6027s/100 iter), loss = 0.0779839
I0731 21:19:30.300478   939 solver.cpp:375]     Train net output #0: loss = 0.077984 (* 1 = 0.077984 loss)
I0731 21:19:30.300485   939 sgd_solver.cpp:136] Iteration 26600, lr = 1e-06, m = 0.9
I0731 21:19:48.860736   939 solver.cpp:353] Iteration 26700 (5.38799 iter/s, 18.5598s/100 iter), loss = 0.0678253
I0731 21:19:48.860795   939 solver.cpp:375]     Train net output #0: loss = 0.0678253 (* 1 = 0.0678253 loss)
I0731 21:19:48.860801   939 sgd_solver.cpp:136] Iteration 26700, lr = 1e-06, m = 0.9
I0731 21:20:02.187693   894 data_reader.cpp:264] Starting prefetch of epoch 14
I0731 21:20:07.479032   939 solver.cpp:353] Iteration 26800 (5.37121 iter/s, 18.6178s/100 iter), loss = 0.0836179
I0731 21:20:07.479053   939 solver.cpp:375]     Train net output #0: loss = 0.083618 (* 1 = 0.083618 loss)
I0731 21:20:07.479058   939 sgd_solver.cpp:136] Iteration 26800, lr = 1e-06, m = 0.9
I0731 21:20:26.180583   939 solver.cpp:353] Iteration 26900 (5.3473 iter/s, 18.701s/100 iter), loss = 0.0544827
I0731 21:20:26.180693   939 solver.cpp:375]     Train net output #0: loss = 0.0544828 (* 1 = 0.0544828 loss)
I0731 21:20:26.180701   939 sgd_solver.cpp:136] Iteration 26900, lr = 1e-06, m = 0.9
I0731 21:20:44.663292   939 solver.cpp:353] Iteration 27000 (5.41061 iter/s, 18.4822s/100 iter), loss = 0.108636
I0731 21:20:44.663319   939 solver.cpp:375]     Train net output #0: loss = 0.108637 (* 1 = 0.108637 loss)
I0731 21:20:44.663326   939 sgd_solver.cpp:136] Iteration 27000, lr = 1e-06, m = 0.9
I0731 21:21:03.217550   939 solver.cpp:353] Iteration 27100 (5.38975 iter/s, 18.5538s/100 iter), loss = 0.0650538
I0731 21:21:03.217602   939 solver.cpp:375]     Train net output #0: loss = 0.0650539 (* 1 = 0.0650539 loss)
I0731 21:21:03.217607   939 sgd_solver.cpp:136] Iteration 27100, lr = 1e-06, m = 0.9
I0731 21:21:03.594910   947 data_reader.cpp:264] Starting prefetch of epoch 17
I0731 21:21:21.725080   939 solver.cpp:353] Iteration 27200 (5.40335 iter/s, 18.507s/100 iter), loss = 0.0440861
I0731 21:21:21.725105   939 solver.cpp:375]     Train net output #0: loss = 0.0440861 (* 1 = 0.0440861 loss)
I0731 21:21:21.725108   939 sgd_solver.cpp:136] Iteration 27200, lr = 1e-06, m = 0.9
I0731 21:21:34.155483   942 data_reader.cpp:264] Starting prefetch of epoch 20
I0731 21:21:40.129714   939 solver.cpp:353] Iteration 27300 (5.43356 iter/s, 18.4041s/100 iter), loss = 0.065299
I0731 21:21:40.129737   939 solver.cpp:375]     Train net output #0: loss = 0.065299 (* 1 = 0.065299 loss)
I0731 21:21:40.129742   939 sgd_solver.cpp:136] Iteration 27300, lr = 1e-06, m = 0.9
I0731 21:21:58.777196   939 solver.cpp:353] Iteration 27400 (5.3628 iter/s, 18.647s/100 iter), loss = 0.105187
I0731 21:21:58.777225   939 solver.cpp:375]     Train net output #0: loss = 0.105188 (* 1 = 0.105188 loss)
I0731 21:21:58.777232   939 sgd_solver.cpp:136] Iteration 27400, lr = 1e-06, m = 0.9
I0731 21:22:17.395074   939 solver.cpp:353] Iteration 27500 (5.37133 iter/s, 18.6174s/100 iter), loss = 0.0597829
I0731 21:22:17.395170   939 solver.cpp:375]     Train net output #0: loss = 0.059783 (* 1 = 0.059783 loss)
I0731 21:22:17.395176   939 sgd_solver.cpp:136] Iteration 27500, lr = 1e-06, m = 0.9
I0731 21:22:35.555429   946 data_reader.cpp:264] Starting prefetch of epoch 18
I0731 21:22:35.898489   939 solver.cpp:353] Iteration 27600 (5.40456 iter/s, 18.5029s/100 iter), loss = 0.0826088
I0731 21:22:35.898514   939 solver.cpp:375]     Train net output #0: loss = 0.0826088 (* 1 = 0.0826088 loss)
I0731 21:22:35.898519   939 sgd_solver.cpp:136] Iteration 27600, lr = 1e-06, m = 0.9
I0731 21:22:54.458612   939 solver.cpp:353] Iteration 27700 (5.38804 iter/s, 18.5596s/100 iter), loss = 0.0817283
I0731 21:22:54.458691   939 solver.cpp:375]     Train net output #0: loss = 0.0817284 (* 1 = 0.0817284 loss)
I0731 21:22:54.458698   939 sgd_solver.cpp:136] Iteration 27700, lr = 1e-06, m = 0.9
I0731 21:23:13.043653   939 solver.cpp:353] Iteration 27800 (5.38082 iter/s, 18.5845s/100 iter), loss = 0.0427709
I0731 21:23:13.043680   939 solver.cpp:375]     Train net output #0: loss = 0.0427709 (* 1 = 0.0427709 loss)
I0731 21:23:13.043687   939 sgd_solver.cpp:136] Iteration 27800, lr = 1e-06, m = 0.9
I0731 21:23:31.618603   939 solver.cpp:353] Iteration 27900 (5.38374 iter/s, 18.5744s/100 iter), loss = 0.0579976
I0731 21:23:31.618711   939 solver.cpp:375]     Train net output #0: loss = 0.0579976 (* 1 = 0.0579976 loss)
I0731 21:23:31.618718   939 sgd_solver.cpp:136] Iteration 27900, lr = 1e-06, m = 0.9
I0731 21:23:36.867759   947 data_reader.cpp:264] Starting prefetch of epoch 18
I0731 21:23:50.100430   939 solver.cpp:550] Iteration 28000, Testing net (#0)
I0731 21:23:53.488512   971 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 21:24:01.198395   939 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.950973
I0731 21:24:01.198417   939 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999952
I0731 21:24:01.198422   939 solver.cpp:635]     Test net output #2: loss = 0.160489 (* 1 = 0.160489 loss)
I0731 21:24:01.198447   939 solver.cpp:305] [MultiGPU] Tests completed in 11.0977s
I0731 21:24:01.405356   939 solver.cpp:353] Iteration 28000 (3.35729 iter/s, 29.7859s/100 iter), loss = 0.0789846
I0731 21:24:01.405380   939 solver.cpp:375]     Train net output #0: loss = 0.0789846 (* 1 = 0.0789846 loss)
I0731 21:24:01.405385   939 sgd_solver.cpp:136] Iteration 28000, lr = 1e-06, m = 0.9
I0731 21:24:18.812129   946 data_reader.cpp:264] Starting prefetch of epoch 19
I0731 21:24:19.916771   939 solver.cpp:353] Iteration 28100 (5.40222 iter/s, 18.5109s/100 iter), loss = 0.0770678
I0731 21:24:19.916798   939 solver.cpp:375]     Train net output #0: loss = 0.0770678 (* 1 = 0.0770678 loss)
I0731 21:24:19.916803   939 sgd_solver.cpp:136] Iteration 28100, lr = 1e-06, m = 0.9
I0731 21:24:38.463917   939 solver.cpp:353] Iteration 28200 (5.39181 iter/s, 18.5466s/100 iter), loss = 0.0738573
I0731 21:24:38.463939   939 solver.cpp:375]     Train net output #0: loss = 0.0738574 (* 1 = 0.0738574 loss)
I0731 21:24:38.463943   939 sgd_solver.cpp:136] Iteration 28200, lr = 1e-06, m = 0.9
I0731 21:24:58.198614   939 solver.cpp:353] Iteration 28300 (5.06736 iter/s, 19.7341s/100 iter), loss = 0.0477171
I0731 21:24:58.198700   939 solver.cpp:375]     Train net output #0: loss = 0.0477172 (* 1 = 0.0477172 loss)
I0731 21:24:58.198712   939 sgd_solver.cpp:136] Iteration 28300, lr = 1e-06, m = 0.9
I0731 21:25:18.225245   939 solver.cpp:353] Iteration 28400 (4.99349 iter/s, 20.0261s/100 iter), loss = 0.084747
I0731 21:25:18.225270   939 solver.cpp:375]     Train net output #0: loss = 0.084747 (* 1 = 0.084747 loss)
I0731 21:25:18.225275   939 sgd_solver.cpp:136] Iteration 28400, lr = 1e-06, m = 0.9
I0731 21:25:22.903429   947 data_reader.cpp:264] Starting prefetch of epoch 19
I0731 21:25:37.993486   939 solver.cpp:353] Iteration 28500 (5.05876 iter/s, 19.7677s/100 iter), loss = 0.0768826
I0731 21:25:37.993551   939 solver.cpp:375]     Train net output #0: loss = 0.0768827 (* 1 = 0.0768827 loss)
I0731 21:25:37.993558   939 sgd_solver.cpp:136] Iteration 28500, lr = 1e-06, m = 0.9
I0731 21:25:55.370597   942 data_reader.cpp:264] Starting prefetch of epoch 21
I0731 21:25:57.393331   939 solver.cpp:353] Iteration 28600 (5.15482 iter/s, 19.3993s/100 iter), loss = 0.0897997
I0731 21:25:57.393386   939 solver.cpp:375]     Train net output #0: loss = 0.0897998 (* 1 = 0.0897998 loss)
I0731 21:25:57.393400   939 sgd_solver.cpp:136] Iteration 28600, lr = 1e-06, m = 0.9
I0731 21:26:17.163039   939 solver.cpp:353] Iteration 28700 (5.05838 iter/s, 19.7692s/100 iter), loss = 0.0854878
I0731 21:26:17.163107   939 solver.cpp:375]     Train net output #0: loss = 0.0854879 (* 1 = 0.0854879 loss)
I0731 21:26:17.163113   939 sgd_solver.cpp:136] Iteration 28700, lr = 1e-06, m = 0.9
I0731 21:26:37.184747   939 solver.cpp:353] Iteration 28800 (4.99472 iter/s, 20.0212s/100 iter), loss = 0.0554338
I0731 21:26:37.184772   939 solver.cpp:375]     Train net output #0: loss = 0.0554338 (* 1 = 0.0554338 loss)
I0731 21:26:37.184777   939 sgd_solver.cpp:136] Iteration 28800, lr = 1e-06, m = 0.9
I0731 21:26:56.661568   939 solver.cpp:353] Iteration 28900 (5.13445 iter/s, 19.4763s/100 iter), loss = 0.130536
I0731 21:26:56.661615   939 solver.cpp:375]     Train net output #0: loss = 0.130536 (* 1 = 0.130536 loss)
I0731 21:26:56.661620   939 sgd_solver.cpp:136] Iteration 28900, lr = 1e-06, m = 0.9
I0731 21:27:00.746701   947 data_reader.cpp:264] Starting prefetch of epoch 20
I0731 21:27:16.213573   939 solver.cpp:353] Iteration 29000 (5.11471 iter/s, 19.5515s/100 iter), loss = 0.0780841
I0731 21:27:16.213596   939 solver.cpp:375]     Train net output #0: loss = 0.0780842 (* 1 = 0.0780842 loss)
I0731 21:27:16.213603   939 sgd_solver.cpp:136] Iteration 29000, lr = 1e-06, m = 0.9
I0731 21:27:36.240200   939 solver.cpp:353] Iteration 29100 (4.99349 iter/s, 20.0261s/100 iter), loss = 0.0772361
I0731 21:27:36.240257   939 solver.cpp:375]     Train net output #0: loss = 0.0772362 (* 1 = 0.0772362 loss)
I0731 21:27:36.240262   939 sgd_solver.cpp:136] Iteration 29100, lr = 1e-06, m = 0.9
I0731 21:27:56.177099   939 solver.cpp:353] Iteration 29200 (5.01596 iter/s, 19.9363s/100 iter), loss = 0.0566756
I0731 21:27:56.177331   939 solver.cpp:375]     Train net output #0: loss = 0.0566757 (* 1 = 0.0566757 loss)
I0731 21:27:56.177429   939 sgd_solver.cpp:136] Iteration 29200, lr = 1e-06, m = 0.9
I0731 21:28:05.859292   894 data_reader.cpp:264] Starting prefetch of epoch 15
I0731 21:28:15.283607   939 solver.cpp:353] Iteration 29300 (5.23396 iter/s, 19.106s/100 iter), loss = 0.0556381
I0731 21:28:15.283694   939 solver.cpp:375]     Train net output #0: loss = 0.0556382 (* 1 = 0.0556382 loss)
I0731 21:28:15.283717   939 sgd_solver.cpp:136] Iteration 29300, lr = 1e-06, m = 0.9
I0731 21:28:35.038472   939 solver.cpp:353] Iteration 29400 (5.06218 iter/s, 19.7543s/100 iter), loss = 0.0528643
I0731 21:28:35.038528   939 solver.cpp:375]     Train net output #0: loss = 0.0528644 (* 1 = 0.0528644 loss)
I0731 21:28:35.038555   939 sgd_solver.cpp:136] Iteration 29400, lr = 1e-06, m = 0.9
I0731 21:28:38.132155   892 data_reader.cpp:264] Starting prefetch of epoch 17
I0731 21:28:53.990371   939 solver.cpp:353] Iteration 29500 (5.27666 iter/s, 18.9514s/100 iter), loss = 0.0770322
I0731 21:28:53.990427   939 solver.cpp:375]     Train net output #0: loss = 0.0770323 (* 1 = 0.0770323 loss)
I0731 21:28:53.990432   939 sgd_solver.cpp:136] Iteration 29500, lr = 1e-06, m = 0.9
I0731 21:29:12.433998   939 solver.cpp:353] Iteration 29600 (5.42208 iter/s, 18.4431s/100 iter), loss = 0.0793818
I0731 21:29:12.434023   939 solver.cpp:375]     Train net output #0: loss = 0.079382 (* 1 = 0.079382 loss)
I0731 21:29:12.434027   939 sgd_solver.cpp:136] Iteration 29600, lr = 1e-06, m = 0.9
I0731 21:29:30.965664   939 solver.cpp:353] Iteration 29700 (5.39632 iter/s, 18.5312s/100 iter), loss = 0.0617822
I0731 21:29:30.965718   939 solver.cpp:375]     Train net output #0: loss = 0.0617823 (* 1 = 0.0617823 loss)
I0731 21:29:30.965724   939 sgd_solver.cpp:136] Iteration 29700, lr = 1e-06, m = 0.9
I0731 21:29:39.493511   942 data_reader.cpp:264] Starting prefetch of epoch 22
I0731 21:29:49.460578   939 solver.cpp:353] Iteration 29800 (5.40704 iter/s, 18.4944s/100 iter), loss = 0.131377
I0731 21:29:49.460602   939 solver.cpp:375]     Train net output #0: loss = 0.131377 (* 1 = 0.131377 loss)
I0731 21:29:49.460606   939 sgd_solver.cpp:136] Iteration 29800, lr = 1e-06, m = 0.9
I0731 21:30:07.837761   939 solver.cpp:353] Iteration 29900 (5.44168 iter/s, 18.3767s/100 iter), loss = 0.074149
I0731 21:30:07.837823   939 solver.cpp:375]     Train net output #0: loss = 0.0741491 (* 1 = 0.0741491 loss)
I0731 21:30:07.837831   939 sgd_solver.cpp:136] Iteration 29900, lr = 1e-06, m = 0.9
I0731 21:30:26.306776   939 solver.cpp:680] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/l1reg/cityscapes5_jsegnet21v2_iter_30000.caffemodel
I0731 21:30:26.366845   939 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/l1reg/cityscapes5_jsegnet21v2_iter_30000.solverstate
I0731 21:30:26.377286   939 solver.cpp:550] Iteration 30000, Testing net (#0)
I0731 21:30:33.613379   936 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 21:30:37.803828   939 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.951811
I0731 21:30:37.803851   939 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999342
I0731 21:30:37.803858   939 solver.cpp:635]     Test net output #2: loss = 0.196373 (* 1 = 0.196373 loss)
I0731 21:30:37.803887   939 solver.cpp:305] [MultiGPU] Tests completed in 11.4263s
I0731 21:30:38.016870   939 solver.cpp:353] Iteration 30000 (3.31364 iter/s, 30.1783s/100 iter), loss = 0.0826424
I0731 21:30:38.017313   939 solver.cpp:375]     Train net output #0: loss = 0.0826425 (* 1 = 0.0826425 loss)
I0731 21:30:38.017328   939 sgd_solver.cpp:136] Iteration 30000, lr = 1e-06, m = 0.9
I0731 21:30:56.555650   939 solver.cpp:353] Iteration 30100 (5.39425 iter/s, 18.5383s/100 iter), loss = 0.0634634
I0731 21:30:56.555675   939 solver.cpp:375]     Train net output #0: loss = 0.0634635 (* 1 = 0.0634635 loss)
I0731 21:30:56.555678   939 sgd_solver.cpp:136] Iteration 30100, lr = 1e-06, m = 0.9
I0731 21:31:15.066757   939 solver.cpp:353] Iteration 30200 (5.40231 iter/s, 18.5106s/100 iter), loss = 0.0636802
I0731 21:31:15.066809   939 solver.cpp:375]     Train net output #0: loss = 0.0636803 (* 1 = 0.0636803 loss)
I0731 21:31:15.066814   939 sgd_solver.cpp:136] Iteration 30200, lr = 1e-06, m = 0.9
I0731 21:31:22.925766   947 data_reader.cpp:264] Starting prefetch of epoch 21
I0731 21:31:33.685940   939 solver.cpp:353] Iteration 30300 (5.37095 iter/s, 18.6187s/100 iter), loss = 0.0849341
I0731 21:31:33.685968   939 solver.cpp:375]     Train net output #0: loss = 0.0849342 (* 1 = 0.0849342 loss)
I0731 21:31:33.685976   939 sgd_solver.cpp:136] Iteration 30300, lr = 1e-06, m = 0.9
I0731 21:31:52.189103   939 solver.cpp:353] Iteration 30400 (5.40463 iter/s, 18.5027s/100 iter), loss = 0.073473
I0731 21:31:52.189302   939 solver.cpp:375]     Train net output #0: loss = 0.0734731 (* 1 = 0.0734731 loss)
I0731 21:31:52.189311   939 sgd_solver.cpp:136] Iteration 30400, lr = 1e-06, m = 0.9
I0731 21:31:53.682375   892 data_reader.cpp:264] Starting prefetch of epoch 18
I0731 21:32:10.763114   939 solver.cpp:353] Iteration 30500 (5.38402 iter/s, 18.5735s/100 iter), loss = 0.0796906
I0731 21:32:10.763139   939 solver.cpp:375]     Train net output #0: loss = 0.0796907 (* 1 = 0.0796907 loss)
I0731 21:32:10.763144   939 sgd_solver.cpp:136] Iteration 30500, lr = 1e-06, m = 0.9
I0731 21:32:29.367702   939 solver.cpp:353] Iteration 30600 (5.37517 iter/s, 18.6041s/100 iter), loss = 0.0587438
I0731 21:32:29.367748   939 solver.cpp:375]     Train net output #0: loss = 0.058744 (* 1 = 0.058744 loss)
I0731 21:32:29.367753   939 sgd_solver.cpp:136] Iteration 30600, lr = 1e-06, m = 0.9
I0731 21:32:47.972957   939 solver.cpp:353] Iteration 30700 (5.37497 iter/s, 18.6047s/100 iter), loss = 0.13812
I0731 21:32:47.972982   939 solver.cpp:375]     Train net output #0: loss = 0.13812 (* 1 = 0.13812 loss)
I0731 21:32:47.972987   939 sgd_solver.cpp:136] Iteration 30700, lr = 1e-06, m = 0.9
I0731 21:32:55.137670   946 data_reader.cpp:264] Starting prefetch of epoch 20
I0731 21:33:06.721060   939 solver.cpp:353] Iteration 30800 (5.33402 iter/s, 18.7476s/100 iter), loss = 0.0703356
I0731 21:33:06.721108   939 solver.cpp:375]     Train net output #0: loss = 0.0703357 (* 1 = 0.0703357 loss)
I0731 21:33:06.721113   939 sgd_solver.cpp:136] Iteration 30800, lr = 1e-06, m = 0.9
I0731 21:33:25.082556   939 solver.cpp:353] Iteration 30900 (5.44633 iter/s, 18.361s/100 iter), loss = 0.0714601
I0731 21:33:25.082581   939 solver.cpp:375]     Train net output #0: loss = 0.0714602 (* 1 = 0.0714602 loss)
I0731 21:33:25.082586   939 sgd_solver.cpp:136] Iteration 30900, lr = 1e-06, m = 0.9
I0731 21:33:43.613636   939 solver.cpp:353] Iteration 31000 (5.39649 iter/s, 18.5306s/100 iter), loss = 0.0650416
I0731 21:33:43.613725   939 solver.cpp:375]     Train net output #0: loss = 0.0650417 (* 1 = 0.0650417 loss)
I0731 21:33:43.613732   939 sgd_solver.cpp:136] Iteration 31000, lr = 1e-06, m = 0.9
I0731 21:33:56.446012   946 data_reader.cpp:264] Starting prefetch of epoch 21
I0731 21:34:02.164999   939 solver.cpp:353] Iteration 31100 (5.39059 iter/s, 18.5509s/100 iter), loss = 0.0602407
I0731 21:34:02.165026   939 solver.cpp:375]     Train net output #0: loss = 0.0602408 (* 1 = 0.0602408 loss)
I0731 21:34:02.165032   939 sgd_solver.cpp:136] Iteration 31100, lr = 1e-06, m = 0.9
I0731 21:34:20.862089   939 solver.cpp:353] Iteration 31200 (5.34857 iter/s, 18.6966s/100 iter), loss = 0.113415
I0731 21:34:20.862138   939 solver.cpp:375]     Train net output #0: loss = 0.113415 (* 1 = 0.113415 loss)
I0731 21:34:20.862144   939 sgd_solver.cpp:136] Iteration 31200, lr = 1e-06, m = 0.9
I0731 21:34:39.345957   939 solver.cpp:353] Iteration 31300 (5.41027 iter/s, 18.4834s/100 iter), loss = 0.0949569
I0731 21:34:39.345981   939 solver.cpp:375]     Train net output #0: loss = 0.094957 (* 1 = 0.094957 loss)
I0731 21:34:39.345986   939 sgd_solver.cpp:136] Iteration 31300, lr = 1e-06, m = 0.9
I0731 21:34:57.799237   894 data_reader.cpp:264] Starting prefetch of epoch 16
I0731 21:34:57.967116   939 solver.cpp:353] Iteration 31400 (5.37038 iter/s, 18.6206s/100 iter), loss = 0.0605311
I0731 21:34:57.967141   939 solver.cpp:375]     Train net output #0: loss = 0.0605312 (* 1 = 0.0605312 loss)
I0731 21:34:57.967145   939 sgd_solver.cpp:136] Iteration 31400, lr = 1e-06, m = 0.9
I0731 21:35:16.556733   939 solver.cpp:353] Iteration 31500 (5.3795 iter/s, 18.5891s/100 iter), loss = 0.0635378
I0731 21:35:16.556757   939 solver.cpp:375]     Train net output #0: loss = 0.0635379 (* 1 = 0.0635379 loss)
I0731 21:35:16.556761   939 sgd_solver.cpp:136] Iteration 31500, lr = 1e-06, m = 0.9
I0731 21:35:28.561239   946 data_reader.cpp:264] Starting prefetch of epoch 22
I0731 21:35:35.036737   939 solver.cpp:353] Iteration 31600 (5.4114 iter/s, 18.4795s/100 iter), loss = 0.136079
I0731 21:35:35.036761   939 solver.cpp:375]     Train net output #0: loss = 0.136079 (* 1 = 0.136079 loss)
I0731 21:35:35.036765   939 sgd_solver.cpp:136] Iteration 31600, lr = 1e-06, m = 0.9
I0731 21:35:53.457433   939 solver.cpp:353] Iteration 31700 (5.42883 iter/s, 18.4202s/100 iter), loss = 0.0829734
I0731 21:35:53.457463   939 solver.cpp:375]     Train net output #0: loss = 0.0829735 (* 1 = 0.0829735 loss)
I0731 21:35:53.457468   939 sgd_solver.cpp:136] Iteration 31700, lr = 1e-06, m = 0.9
I0731 21:36:12.067144   939 solver.cpp:353] Iteration 31800 (5.37369 iter/s, 18.6092s/100 iter), loss = 0.0745872
I0731 21:36:12.067198   939 solver.cpp:375]     Train net output #0: loss = 0.0745873 (* 1 = 0.0745873 loss)
I0731 21:36:12.067204   939 sgd_solver.cpp:136] Iteration 31800, lr = 1e-06, m = 0.9
I0731 21:36:29.661536   943 data_reader.cpp:264] Starting prefetch of epoch 16
I0731 21:36:30.541697   939 solver.cpp:353] Iteration 31900 (5.413 iter/s, 18.474s/100 iter), loss = 0.0456669
I0731 21:36:30.541723   939 solver.cpp:375]     Train net output #0: loss = 0.045667 (* 1 = 0.045667 loss)
I0731 21:36:30.541730   939 sgd_solver.cpp:136] Iteration 31900, lr = 1e-06, m = 0.9
I0731 21:36:49.033942   939 solver.cpp:353] Iteration 31999 (5.35374 iter/s, 18.4917s/99 iter), loss = 0.0634366
I0731 21:36:49.033993   939 solver.cpp:375]     Train net output #0: loss = 0.0634367 (* 1 = 0.0634367 loss)
I0731 21:36:49.109697   939 solver.cpp:680] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/l1reg/cityscapes5_jsegnet21v2_iter_32000.caffemodel
I0731 21:36:49.179056   939 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/l1reg/cityscapes5_jsegnet21v2_iter_32000.solverstate
I0731 21:36:49.276705   939 solver.cpp:527] Iteration 32000, loss = 0.0613403
I0731 21:36:49.276733   939 solver.cpp:550] Iteration 32000, Testing net (#0)
I0731 21:36:59.885489   971 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 21:37:00.562300   939 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.950704
I0731 21:37:00.562320   939 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999972
I0731 21:37:00.562325   939 solver.cpp:635]     Test net output #2: loss = 0.161054 (* 1 = 0.161054 loss)
I0731 21:37:00.633184   830 parallel.cpp:73] Root Solver performance on device 0: 5.203 * 6 = 31.22 img/sec (32000 itr in 6150 sec)
I0731 21:37:00.633242   830 parallel.cpp:78]      Solver performance on device 1: 5.203 * 6 = 31.22 img/sec (32000 itr in 6150 sec)
I0731 21:37:00.633262   830 parallel.cpp:78]      Solver performance on device 2: 5.203 * 6 = 31.22 img/sec (32000 itr in 6150 sec)
I0731 21:37:00.633270   830 parallel.cpp:81] Overall multi-GPU performance: 93.656 img/sec
I0731 21:37:01.845003   830 caffe.cpp:247] Optimization Done in 1h 42m 48s
training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/sparse
I0731 21:37:16.223815 20312 caffe.cpp:608] This is NVCaffe 0.16.3 started at Mon Jul 31 21:37:15 2017
I0731 21:37:16.224097 20312 caffe.cpp:611] CuDNN version: 6021
I0731 21:37:16.224100 20312 caffe.cpp:612] CuBLAS version: 8000
I0731 21:37:16.224103 20312 caffe.cpp:613] CUDA version: 8000
I0731 21:37:16.224105 20312 caffe.cpp:614] CUDA driver version: 8000
I0731 21:37:16.516619 20312 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0731 21:37:16.517204 20312 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I0731 21:37:16.517742 20312 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[1]: total=8508145664 free=8379236352
I0731 21:37:16.518259 20312 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[2]: total=8508145664 free=8379236352
I0731 21:37:16.518266 20312 caffe.cpp:208] Using GPUs 0, 1, 2
I0731 21:37:16.518587 20312 caffe.cpp:213] GPU 0: GeForce GTX 1080
I0731 21:37:16.518909 20312 caffe.cpp:213] GPU 1: GeForce GTX 1080
I0731 21:37:16.519229 20312 caffe.cpp:213] GPU 2: GeForce GTX 1080
I0731 21:37:16.519265 20312 solver.cpp:42] Solver data type: FLOAT
I0731 21:37:16.519327 20312 solver.cpp:45] Initializing solver from parameters: 
train_net: "training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/sparse/train.prototxt"
test_net: "training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/sparse/test.prototxt"
test_iter: 125
test_interval: 2000
base_lr: 1e-05
display: 100
max_iter: 32000
lr_policy: "multistep"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 1e-05
snapshot: 10000
snapshot_prefix: "training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/sparse/cityscapes5_jsegnet21v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
regularization_type: "L1"
test_initialization: false
stepvalue: 24000
iter_size: 1
type: "Adam"
display_sparsity: 1000
sparse_mode: SPARSE_UPDATE
sparsity_target: 0.8
sparsity_step_factor: 0.01
sparsity_step_iter: 1000
sparsity_start_iter: 0
sparsity_start_factor: 0.8
I0731 21:37:16.536775 20312 solver.cpp:77] Creating training net from train_net file: training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/sparse/train.prototxt
I0731 21:37:16.537390 20312 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0731 21:37:16.537398 20312 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
W0731 21:37:16.537431 20312 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0731 21:37:16.537672 20312 net.cpp:72] Initializing net from parameters: 
name: "jsegnet21v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/train-image-lmdb"
    label_list_path: "data/train-label-lmdb"
    batch_size: 6
    shuffle: false
    threads: 1
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
I0731 21:37:16.537825 20312 net.cpp:104] Using FLOAT as default forward math type
I0731 21:37:16.537832 20312 net.cpp:110] Using FLOAT as default backward math type
I0731 21:37:16.537834 20312 layer_factory.hpp:136] Creating layer 'data' of type 'ImageLabelData'
I0731 21:37:16.537837 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:16.537852 20312 net.cpp:184] Created Layer data (0)
I0731 21:37:16.537856 20312 net.cpp:530] data -> data
I0731 21:37:16.546272 20312 net.cpp:530] data -> label
I0731 21:37:16.554443 20312 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0731 21:37:16.554462 20312 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 21:37:16.572679 20346 db_lmdb.cpp:35] Opened lmdb data/train-image-lmdb
I0731 21:37:16.575791 20312 data_layer.cpp:184] [0] ReshapePrefetch 6, 3, 640, 640
I0731 21:37:16.575894 20312 data_layer.cpp:208] [0] Output data size: 6, 3, 640, 640
I0731 21:37:16.575903 20312 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 21:37:16.575968 20312 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0731 21:37:16.575979 20312 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 21:37:16.576671 20363 data_layer.cpp:97] [0] Parser threads: 1
I0731 21:37:16.576678 20363 data_layer.cpp:99] [0] Transformer threads: 1
I0731 21:37:16.600420 20364 db_lmdb.cpp:35] Opened lmdb data/train-label-lmdb
I0731 21:37:16.602892 20312 data_layer.cpp:184] [0] ReshapePrefetch 6, 1, 640, 640
I0731 21:37:16.602931 20312 data_layer.cpp:208] [0] Output data size: 6, 1, 640, 640
I0731 21:37:16.602936 20312 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 21:37:16.602984 20312 net.cpp:245] Setting up data
I0731 21:37:16.602995 20312 net.cpp:252] TRAIN Top shape for layer 0 'data' 6 3 640 640 (7372800)
I0731 21:37:16.603003 20312 net.cpp:252] TRAIN Top shape for layer 0 'data' 6 1 640 640 (2457600)
I0731 21:37:16.603010 20312 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0731 21:37:16.603016 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:16.603036 20312 net.cpp:184] Created Layer data/bias (1)
I0731 21:37:16.603044 20312 net.cpp:561] data/bias <- data
I0731 21:37:16.603055 20312 net.cpp:530] data/bias -> data/bias
I0731 21:37:16.605535 20369 data_layer.cpp:97] [0] Parser threads: 1
I0731 21:37:16.605566 20369 data_layer.cpp:99] [0] Transformer threads: 1
I0731 21:37:16.609308 20312 net.cpp:245] Setting up data/bias
I0731 21:37:16.609344 20312 net.cpp:252] TRAIN Top shape for layer 1 'data/bias' 6 3 640 640 (7372800)
I0731 21:37:16.609365 20312 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0731 21:37:16.609419 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:16.609463 20312 net.cpp:184] Created Layer conv1a (2)
I0731 21:37:16.609470 20312 net.cpp:561] conv1a <- data/bias
I0731 21:37:16.609477 20312 net.cpp:530] conv1a -> conv1a
I0731 21:37:17.346477 20312 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 7.9G, req 0G)
I0731 21:37:17.346510 20312 net.cpp:245] Setting up conv1a
I0731 21:37:17.346520 20312 net.cpp:252] TRAIN Top shape for layer 2 'conv1a' 6 32 320 320 (19660800)
I0731 21:37:17.346536 20312 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0731 21:37:17.346544 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.346565 20312 net.cpp:184] Created Layer conv1a/bn (3)
I0731 21:37:17.346571 20312 net.cpp:561] conv1a/bn <- conv1a
I0731 21:37:17.346578 20312 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0731 21:37:17.347837 20312 net.cpp:245] Setting up conv1a/bn
I0731 21:37:17.347854 20312 net.cpp:252] TRAIN Top shape for layer 3 'conv1a/bn' 6 32 320 320 (19660800)
I0731 21:37:17.347868 20312 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0731 21:37:17.347874 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.347892 20312 net.cpp:184] Created Layer conv1a/relu (4)
I0731 21:37:17.347898 20312 net.cpp:561] conv1a/relu <- conv1a
I0731 21:37:17.347903 20312 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0731 21:37:17.347923 20312 net.cpp:245] Setting up conv1a/relu
I0731 21:37:17.347929 20312 net.cpp:252] TRAIN Top shape for layer 4 'conv1a/relu' 6 32 320 320 (19660800)
I0731 21:37:17.347934 20312 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0731 21:37:17.347939 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.347961 20312 net.cpp:184] Created Layer conv1b (5)
I0731 21:37:17.347966 20312 net.cpp:561] conv1b <- conv1a
I0731 21:37:17.347971 20312 net.cpp:530] conv1b -> conv1b
I0731 21:37:17.401226 20312 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.73G, req 0G)
I0731 21:37:17.401298 20312 net.cpp:245] Setting up conv1b
I0731 21:37:17.401324 20312 net.cpp:252] TRAIN Top shape for layer 5 'conv1b' 6 32 320 320 (19660800)
I0731 21:37:17.401356 20312 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0731 21:37:17.401376 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.401406 20312 net.cpp:184] Created Layer conv1b/bn (6)
I0731 21:37:17.401420 20312 net.cpp:561] conv1b/bn <- conv1b
I0731 21:37:17.401435 20312 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0731 21:37:17.404876 20312 net.cpp:245] Setting up conv1b/bn
I0731 21:37:17.404906 20312 net.cpp:252] TRAIN Top shape for layer 6 'conv1b/bn' 6 32 320 320 (19660800)
I0731 21:37:17.404927 20312 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0731 21:37:17.404937 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.404948 20312 net.cpp:184] Created Layer conv1b/relu (7)
I0731 21:37:17.404956 20312 net.cpp:561] conv1b/relu <- conv1b
I0731 21:37:17.404969 20312 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0731 21:37:17.404983 20312 net.cpp:245] Setting up conv1b/relu
I0731 21:37:17.404992 20312 net.cpp:252] TRAIN Top shape for layer 7 'conv1b/relu' 6 32 320 320 (19660800)
I0731 21:37:17.405001 20312 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0731 21:37:17.405014 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.405059 20312 net.cpp:184] Created Layer pool1 (8)
I0731 21:37:17.405076 20312 net.cpp:561] pool1 <- conv1b
I0731 21:37:17.405089 20312 net.cpp:530] pool1 -> pool1
I0731 21:37:17.405946 20312 net.cpp:245] Setting up pool1
I0731 21:37:17.405977 20312 net.cpp:252] TRAIN Top shape for layer 8 'pool1' 6 32 160 160 (4915200)
I0731 21:37:17.405992 20312 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0731 21:37:17.406008 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.406040 20312 net.cpp:184] Created Layer res2a_branch2a (9)
I0731 21:37:17.406057 20312 net.cpp:561] res2a_branch2a <- pool1
I0731 21:37:17.406071 20312 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0731 21:37:17.454162 20312 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.61G, req 0G)
I0731 21:37:17.454186 20312 net.cpp:245] Setting up res2a_branch2a
I0731 21:37:17.454191 20312 net.cpp:252] TRAIN Top shape for layer 9 'res2a_branch2a' 6 64 160 160 (9830400)
I0731 21:37:17.454202 20312 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0731 21:37:17.454206 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.454215 20312 net.cpp:184] Created Layer res2a_branch2a/bn (10)
I0731 21:37:17.454217 20312 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0731 21:37:17.454229 20312 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0731 21:37:17.455521 20312 net.cpp:245] Setting up res2a_branch2a/bn
I0731 21:37:17.455531 20312 net.cpp:252] TRAIN Top shape for layer 10 'res2a_branch2a/bn' 6 64 160 160 (9830400)
I0731 21:37:17.455538 20312 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0731 21:37:17.455540 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.455544 20312 net.cpp:184] Created Layer res2a_branch2a/relu (11)
I0731 21:37:17.455548 20312 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0731 21:37:17.455549 20312 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0731 21:37:17.455554 20312 net.cpp:245] Setting up res2a_branch2a/relu
I0731 21:37:17.455556 20312 net.cpp:252] TRAIN Top shape for layer 11 'res2a_branch2a/relu' 6 64 160 160 (9830400)
I0731 21:37:17.455559 20312 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0731 21:37:17.455560 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.455567 20312 net.cpp:184] Created Layer res2a_branch2b (12)
I0731 21:37:17.455570 20312 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0731 21:37:17.455572 20312 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0731 21:37:17.478261 20312 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.52G, req 0G)
I0731 21:37:17.478274 20312 net.cpp:245] Setting up res2a_branch2b
I0731 21:37:17.478279 20312 net.cpp:252] TRAIN Top shape for layer 12 'res2a_branch2b' 6 64 160 160 (9830400)
I0731 21:37:17.478284 20312 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0731 21:37:17.478287 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.478292 20312 net.cpp:184] Created Layer res2a_branch2b/bn (13)
I0731 21:37:17.478296 20312 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0731 21:37:17.478299 20312 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0731 21:37:17.479029 20312 net.cpp:245] Setting up res2a_branch2b/bn
I0731 21:37:17.479038 20312 net.cpp:252] TRAIN Top shape for layer 13 'res2a_branch2b/bn' 6 64 160 160 (9830400)
I0731 21:37:17.479044 20312 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0731 21:37:17.479048 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.479050 20312 net.cpp:184] Created Layer res2a_branch2b/relu (14)
I0731 21:37:17.479053 20312 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0731 21:37:17.479055 20312 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0731 21:37:17.479068 20312 net.cpp:245] Setting up res2a_branch2b/relu
I0731 21:37:17.479071 20312 net.cpp:252] TRAIN Top shape for layer 14 'res2a_branch2b/relu' 6 64 160 160 (9830400)
I0731 21:37:17.479074 20312 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0731 21:37:17.479077 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.479081 20312 net.cpp:184] Created Layer pool2 (15)
I0731 21:37:17.479084 20312 net.cpp:561] pool2 <- res2a_branch2b
I0731 21:37:17.479086 20312 net.cpp:530] pool2 -> pool2
I0731 21:37:17.479154 20312 net.cpp:245] Setting up pool2
I0731 21:37:17.479163 20312 net.cpp:252] TRAIN Top shape for layer 15 'pool2' 6 64 80 80 (2457600)
I0731 21:37:17.479167 20312 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0731 21:37:17.479171 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.479179 20312 net.cpp:184] Created Layer res3a_branch2a (16)
I0731 21:37:17.479183 20312 net.cpp:561] res3a_branch2a <- pool2
I0731 21:37:17.479187 20312 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0731 21:37:17.501444 20312 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 7.46G, req 0G)
I0731 21:37:17.501456 20312 net.cpp:245] Setting up res3a_branch2a
I0731 21:37:17.501461 20312 net.cpp:252] TRAIN Top shape for layer 16 'res3a_branch2a' 6 128 80 80 (4915200)
I0731 21:37:17.501466 20312 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0731 21:37:17.501467 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.501472 20312 net.cpp:184] Created Layer res3a_branch2a/bn (17)
I0731 21:37:17.501474 20312 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0731 21:37:17.501477 20312 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0731 21:37:17.502207 20312 net.cpp:245] Setting up res3a_branch2a/bn
I0731 21:37:17.502216 20312 net.cpp:252] TRAIN Top shape for layer 17 'res3a_branch2a/bn' 6 128 80 80 (4915200)
I0731 21:37:17.502223 20312 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0731 21:37:17.502226 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.502229 20312 net.cpp:184] Created Layer res3a_branch2a/relu (18)
I0731 21:37:17.502231 20312 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0731 21:37:17.502234 20312 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0731 21:37:17.502238 20312 net.cpp:245] Setting up res3a_branch2a/relu
I0731 21:37:17.502240 20312 net.cpp:252] TRAIN Top shape for layer 18 'res3a_branch2a/relu' 6 128 80 80 (4915200)
I0731 21:37:17.502243 20312 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0731 21:37:17.502244 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.502250 20312 net.cpp:184] Created Layer res3a_branch2b (19)
I0731 21:37:17.502252 20312 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0731 21:37:17.502255 20312 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0731 21:37:17.516034 20312 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.42G, req 0G)
I0731 21:37:17.516047 20312 net.cpp:245] Setting up res3a_branch2b
I0731 21:37:17.516050 20312 net.cpp:252] TRAIN Top shape for layer 19 'res3a_branch2b' 6 128 80 80 (4915200)
I0731 21:37:17.516055 20312 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0731 21:37:17.516057 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.516062 20312 net.cpp:184] Created Layer res3a_branch2b/bn (20)
I0731 21:37:17.516064 20312 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0731 21:37:17.516067 20312 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0731 21:37:17.516767 20312 net.cpp:245] Setting up res3a_branch2b/bn
I0731 21:37:17.516783 20312 net.cpp:252] TRAIN Top shape for layer 20 'res3a_branch2b/bn' 6 128 80 80 (4915200)
I0731 21:37:17.516789 20312 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0731 21:37:17.516793 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.516795 20312 net.cpp:184] Created Layer res3a_branch2b/relu (21)
I0731 21:37:17.516798 20312 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0731 21:37:17.516800 20312 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0731 21:37:17.516803 20312 net.cpp:245] Setting up res3a_branch2b/relu
I0731 21:37:17.516806 20312 net.cpp:252] TRAIN Top shape for layer 21 'res3a_branch2b/relu' 6 128 80 80 (4915200)
I0731 21:37:17.516808 20312 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0731 21:37:17.516810 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.516819 20312 net.cpp:184] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (22)
I0731 21:37:17.516822 20312 net.cpp:561] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0731 21:37:17.516824 20312 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0731 21:37:17.516829 20312 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0731 21:37:17.516880 20312 net.cpp:245] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0731 21:37:17.516886 20312 net.cpp:252] TRAIN Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 6 128 80 80 (4915200)
I0731 21:37:17.516891 20312 net.cpp:252] TRAIN Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 6 128 80 80 (4915200)
I0731 21:37:17.516896 20312 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0731 21:37:17.516901 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.516906 20312 net.cpp:184] Created Layer pool3 (23)
I0731 21:37:17.516911 20312 net.cpp:561] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0731 21:37:17.516914 20312 net.cpp:530] pool3 -> pool3
I0731 21:37:17.516993 20312 net.cpp:245] Setting up pool3
I0731 21:37:17.516999 20312 net.cpp:252] TRAIN Top shape for layer 23 'pool3' 6 128 40 40 (1228800)
I0731 21:37:17.517004 20312 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0731 21:37:17.517007 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.517015 20312 net.cpp:184] Created Layer res4a_branch2a (24)
I0731 21:37:17.517019 20312 net.cpp:561] res4a_branch2a <- pool3
I0731 21:37:17.517024 20312 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0731 21:37:17.546128 20312 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.38G, req 0G)
I0731 21:37:17.546147 20312 net.cpp:245] Setting up res4a_branch2a
I0731 21:37:17.546154 20312 net.cpp:252] TRAIN Top shape for layer 24 'res4a_branch2a' 6 256 40 40 (2457600)
I0731 21:37:17.546161 20312 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0731 21:37:17.546165 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.546180 20312 net.cpp:184] Created Layer res4a_branch2a/bn (25)
I0731 21:37:17.546183 20312 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0731 21:37:17.546186 20312 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0731 21:37:17.546965 20312 net.cpp:245] Setting up res4a_branch2a/bn
I0731 21:37:17.546973 20312 net.cpp:252] TRAIN Top shape for layer 25 'res4a_branch2a/bn' 6 256 40 40 (2457600)
I0731 21:37:17.546980 20312 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0731 21:37:17.546983 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.546986 20312 net.cpp:184] Created Layer res4a_branch2a/relu (26)
I0731 21:37:17.546998 20312 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0731 21:37:17.547000 20312 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0731 21:37:17.547004 20312 net.cpp:245] Setting up res4a_branch2a/relu
I0731 21:37:17.547011 20312 net.cpp:252] TRAIN Top shape for layer 26 'res4a_branch2a/relu' 6 256 40 40 (2457600)
I0731 21:37:17.547014 20312 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0731 21:37:17.547018 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.547024 20312 net.cpp:184] Created Layer res4a_branch2b (27)
I0731 21:37:17.547026 20312 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0731 21:37:17.547029 20312 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0731 21:37:17.557807 20312 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.36G, req 0G)
I0731 21:37:17.557821 20312 net.cpp:245] Setting up res4a_branch2b
I0731 21:37:17.557826 20312 net.cpp:252] TRAIN Top shape for layer 27 'res4a_branch2b' 6 256 40 40 (2457600)
I0731 21:37:17.557829 20312 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0731 21:37:17.557832 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.557837 20312 net.cpp:184] Created Layer res4a_branch2b/bn (28)
I0731 21:37:17.557839 20312 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0731 21:37:17.557842 20312 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0731 21:37:17.558540 20312 net.cpp:245] Setting up res4a_branch2b/bn
I0731 21:37:17.558564 20312 net.cpp:252] TRAIN Top shape for layer 28 'res4a_branch2b/bn' 6 256 40 40 (2457600)
I0731 21:37:17.558583 20312 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0731 21:37:17.558594 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.558605 20312 net.cpp:184] Created Layer res4a_branch2b/relu (29)
I0731 21:37:17.558615 20312 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0731 21:37:17.558624 20312 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0731 21:37:17.558634 20312 net.cpp:245] Setting up res4a_branch2b/relu
I0731 21:37:17.558645 20312 net.cpp:252] TRAIN Top shape for layer 29 'res4a_branch2b/relu' 6 256 40 40 (2457600)
I0731 21:37:17.558652 20312 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0731 21:37:17.558661 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.558673 20312 net.cpp:184] Created Layer pool4 (30)
I0731 21:37:17.558681 20312 net.cpp:561] pool4 <- res4a_branch2b
I0731 21:37:17.558691 20312 net.cpp:530] pool4 -> pool4
I0731 21:37:17.558801 20312 net.cpp:245] Setting up pool4
I0731 21:37:17.558815 20312 net.cpp:252] TRAIN Top shape for layer 30 'pool4' 6 256 40 40 (2457600)
I0731 21:37:17.558825 20312 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0731 21:37:17.558835 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.558857 20312 net.cpp:184] Created Layer res5a_branch2a (31)
I0731 21:37:17.558867 20312 net.cpp:561] res5a_branch2a <- pool4
I0731 21:37:17.558876 20312 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0731 21:37:17.586984 20312 net.cpp:245] Setting up res5a_branch2a
I0731 21:37:17.587003 20312 net.cpp:252] TRAIN Top shape for layer 31 'res5a_branch2a' 6 512 40 40 (4915200)
I0731 21:37:17.587010 20312 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0731 21:37:17.587014 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.587023 20312 net.cpp:184] Created Layer res5a_branch2a/bn (32)
I0731 21:37:17.587025 20312 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0731 21:37:17.587028 20312 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0731 21:37:17.587679 20312 net.cpp:245] Setting up res5a_branch2a/bn
I0731 21:37:17.587687 20312 net.cpp:252] TRAIN Top shape for layer 32 'res5a_branch2a/bn' 6 512 40 40 (4915200)
I0731 21:37:17.587692 20312 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0731 21:37:17.587695 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.587698 20312 net.cpp:184] Created Layer res5a_branch2a/relu (33)
I0731 21:37:17.587700 20312 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0731 21:37:17.587702 20312 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0731 21:37:17.587707 20312 net.cpp:245] Setting up res5a_branch2a/relu
I0731 21:37:17.587709 20312 net.cpp:252] TRAIN Top shape for layer 33 'res5a_branch2a/relu' 6 512 40 40 (4915200)
I0731 21:37:17.587712 20312 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0731 21:37:17.587713 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.587719 20312 net.cpp:184] Created Layer res5a_branch2b (34)
I0731 21:37:17.587723 20312 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0731 21:37:17.587724 20312 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0731 21:37:17.601045 20312 net.cpp:245] Setting up res5a_branch2b
I0731 21:37:17.601068 20312 net.cpp:252] TRAIN Top shape for layer 34 'res5a_branch2b' 6 512 40 40 (4915200)
I0731 21:37:17.601079 20312 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0731 21:37:17.601083 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.601091 20312 net.cpp:184] Created Layer res5a_branch2b/bn (35)
I0731 21:37:17.601094 20312 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0731 21:37:17.601097 20312 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0731 21:37:17.601738 20312 net.cpp:245] Setting up res5a_branch2b/bn
I0731 21:37:17.601745 20312 net.cpp:252] TRAIN Top shape for layer 35 'res5a_branch2b/bn' 6 512 40 40 (4915200)
I0731 21:37:17.601752 20312 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0731 21:37:17.601753 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.601757 20312 net.cpp:184] Created Layer res5a_branch2b/relu (36)
I0731 21:37:17.601759 20312 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0731 21:37:17.601761 20312 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0731 21:37:17.601765 20312 net.cpp:245] Setting up res5a_branch2b/relu
I0731 21:37:17.601768 20312 net.cpp:252] TRAIN Top shape for layer 36 'res5a_branch2b/relu' 6 512 40 40 (4915200)
I0731 21:37:17.601769 20312 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I0731 21:37:17.601773 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.601783 20312 net.cpp:184] Created Layer out5a (37)
I0731 21:37:17.601785 20312 net.cpp:561] out5a <- res5a_branch2b
I0731 21:37:17.601788 20312 net.cpp:530] out5a -> out5a
I0731 21:37:17.606081 20312 net.cpp:245] Setting up out5a
I0731 21:37:17.606091 20312 net.cpp:252] TRAIN Top shape for layer 37 'out5a' 6 64 40 40 (614400)
I0731 21:37:17.606096 20312 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I0731 21:37:17.606098 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.606107 20312 net.cpp:184] Created Layer out5a/bn (38)
I0731 21:37:17.606111 20312 net.cpp:561] out5a/bn <- out5a
I0731 21:37:17.606113 20312 net.cpp:513] out5a/bn -> out5a (in-place)
I0731 21:37:17.606830 20312 net.cpp:245] Setting up out5a/bn
I0731 21:37:17.606838 20312 net.cpp:252] TRAIN Top shape for layer 38 'out5a/bn' 6 64 40 40 (614400)
I0731 21:37:17.606844 20312 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I0731 21:37:17.606848 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.606858 20312 net.cpp:184] Created Layer out5a/relu (39)
I0731 21:37:17.606861 20312 net.cpp:561] out5a/relu <- out5a
I0731 21:37:17.606863 20312 net.cpp:513] out5a/relu -> out5a (in-place)
I0731 21:37:17.606868 20312 net.cpp:245] Setting up out5a/relu
I0731 21:37:17.606870 20312 net.cpp:252] TRAIN Top shape for layer 39 'out5a/relu' 6 64 40 40 (614400)
I0731 21:37:17.606873 20312 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I0731 21:37:17.606874 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.606885 20312 net.cpp:184] Created Layer out5a_up2 (40)
I0731 21:37:17.606889 20312 net.cpp:561] out5a_up2 <- out5a
I0731 21:37:17.606890 20312 net.cpp:530] out5a_up2 -> out5a_up2
I0731 21:37:17.607254 20312 net.cpp:245] Setting up out5a_up2
I0731 21:37:17.607262 20312 net.cpp:252] TRAIN Top shape for layer 40 'out5a_up2' 6 64 80 80 (2457600)
I0731 21:37:17.607267 20312 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I0731 21:37:17.607271 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.607290 20312 net.cpp:184] Created Layer out3a (41)
I0731 21:37:17.607295 20312 net.cpp:561] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0731 21:37:17.607301 20312 net.cpp:530] out3a -> out3a
I0731 21:37:17.621115 20312 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 7.3G, req 0G)
I0731 21:37:17.621131 20312 net.cpp:245] Setting up out3a
I0731 21:37:17.621139 20312 net.cpp:252] TRAIN Top shape for layer 41 'out3a' 6 64 80 80 (2457600)
I0731 21:37:17.621145 20312 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I0731 21:37:17.621150 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.621156 20312 net.cpp:184] Created Layer out3a/bn (42)
I0731 21:37:17.621160 20312 net.cpp:561] out3a/bn <- out3a
I0731 21:37:17.621165 20312 net.cpp:513] out3a/bn -> out3a (in-place)
I0731 21:37:17.621956 20312 net.cpp:245] Setting up out3a/bn
I0731 21:37:17.621965 20312 net.cpp:252] TRAIN Top shape for layer 42 'out3a/bn' 6 64 80 80 (2457600)
I0731 21:37:17.621971 20312 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I0731 21:37:17.621974 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.621978 20312 net.cpp:184] Created Layer out3a/relu (43)
I0731 21:37:17.621980 20312 net.cpp:561] out3a/relu <- out3a
I0731 21:37:17.621982 20312 net.cpp:513] out3a/relu -> out3a (in-place)
I0731 21:37:17.621986 20312 net.cpp:245] Setting up out3a/relu
I0731 21:37:17.621989 20312 net.cpp:252] TRAIN Top shape for layer 43 'out3a/relu' 6 64 80 80 (2457600)
I0731 21:37:17.621990 20312 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I0731 21:37:17.621992 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.622429 20312 net.cpp:184] Created Layer out3_out5_combined (44)
I0731 21:37:17.622436 20312 net.cpp:561] out3_out5_combined <- out5a_up2
I0731 21:37:17.622438 20312 net.cpp:561] out3_out5_combined <- out3a
I0731 21:37:17.622440 20312 net.cpp:530] out3_out5_combined -> out3_out5_combined
I0731 21:37:17.623430 20312 net.cpp:245] Setting up out3_out5_combined
I0731 21:37:17.623438 20312 net.cpp:252] TRAIN Top shape for layer 44 'out3_out5_combined' 6 64 80 80 (2457600)
I0731 21:37:17.623441 20312 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I0731 21:37:17.623445 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.623450 20312 net.cpp:184] Created Layer ctx_conv1 (45)
I0731 21:37:17.623452 20312 net.cpp:561] ctx_conv1 <- out3_out5_combined
I0731 21:37:17.623456 20312 net.cpp:530] ctx_conv1 -> ctx_conv1
I0731 21:37:17.639556 20312 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 7.25G, req 0G)
I0731 21:37:17.639576 20312 net.cpp:245] Setting up ctx_conv1
I0731 21:37:17.639581 20312 net.cpp:252] TRAIN Top shape for layer 45 'ctx_conv1' 6 64 80 80 (2457600)
I0731 21:37:17.639585 20312 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I0731 21:37:17.639588 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.639598 20312 net.cpp:184] Created Layer ctx_conv1/bn (46)
I0731 21:37:17.639601 20312 net.cpp:561] ctx_conv1/bn <- ctx_conv1
I0731 21:37:17.639603 20312 net.cpp:513] ctx_conv1/bn -> ctx_conv1 (in-place)
I0731 21:37:17.640291 20312 net.cpp:245] Setting up ctx_conv1/bn
I0731 21:37:17.640298 20312 net.cpp:252] TRAIN Top shape for layer 46 'ctx_conv1/bn' 6 64 80 80 (2457600)
I0731 21:37:17.640305 20312 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I0731 21:37:17.640306 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.640310 20312 net.cpp:184] Created Layer ctx_conv1/relu (47)
I0731 21:37:17.640312 20312 net.cpp:561] ctx_conv1/relu <- ctx_conv1
I0731 21:37:17.640314 20312 net.cpp:513] ctx_conv1/relu -> ctx_conv1 (in-place)
I0731 21:37:17.640317 20312 net.cpp:245] Setting up ctx_conv1/relu
I0731 21:37:17.640321 20312 net.cpp:252] TRAIN Top shape for layer 47 'ctx_conv1/relu' 6 64 80 80 (2457600)
I0731 21:37:17.640322 20312 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I0731 21:37:17.640324 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.640329 20312 net.cpp:184] Created Layer ctx_conv2 (48)
I0731 21:37:17.640332 20312 net.cpp:561] ctx_conv2 <- ctx_conv1
I0731 21:37:17.640334 20312 net.cpp:530] ctx_conv2 -> ctx_conv2
I0731 21:37:17.641469 20312 net.cpp:245] Setting up ctx_conv2
I0731 21:37:17.641480 20312 net.cpp:252] TRAIN Top shape for layer 48 'ctx_conv2' 6 64 80 80 (2457600)
I0731 21:37:17.641485 20312 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I0731 21:37:17.641489 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.641492 20312 net.cpp:184] Created Layer ctx_conv2/bn (49)
I0731 21:37:17.641495 20312 net.cpp:561] ctx_conv2/bn <- ctx_conv2
I0731 21:37:17.641499 20312 net.cpp:513] ctx_conv2/bn -> ctx_conv2 (in-place)
I0731 21:37:17.642170 20312 net.cpp:245] Setting up ctx_conv2/bn
I0731 21:37:17.642177 20312 net.cpp:252] TRAIN Top shape for layer 49 'ctx_conv2/bn' 6 64 80 80 (2457600)
I0731 21:37:17.642184 20312 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I0731 21:37:17.642185 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.642189 20312 net.cpp:184] Created Layer ctx_conv2/relu (50)
I0731 21:37:17.642190 20312 net.cpp:561] ctx_conv2/relu <- ctx_conv2
I0731 21:37:17.642194 20312 net.cpp:513] ctx_conv2/relu -> ctx_conv2 (in-place)
I0731 21:37:17.642196 20312 net.cpp:245] Setting up ctx_conv2/relu
I0731 21:37:17.642199 20312 net.cpp:252] TRAIN Top shape for layer 50 'ctx_conv2/relu' 6 64 80 80 (2457600)
I0731 21:37:17.642200 20312 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I0731 21:37:17.642204 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.642212 20312 net.cpp:184] Created Layer ctx_conv3 (51)
I0731 21:37:17.642215 20312 net.cpp:561] ctx_conv3 <- ctx_conv2
I0731 21:37:17.642218 20312 net.cpp:530] ctx_conv3 -> ctx_conv3
I0731 21:37:17.643343 20312 net.cpp:245] Setting up ctx_conv3
I0731 21:37:17.643350 20312 net.cpp:252] TRAIN Top shape for layer 51 'ctx_conv3' 6 64 80 80 (2457600)
I0731 21:37:17.643355 20312 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I0731 21:37:17.643357 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.643362 20312 net.cpp:184] Created Layer ctx_conv3/bn (52)
I0731 21:37:17.643371 20312 net.cpp:561] ctx_conv3/bn <- ctx_conv3
I0731 21:37:17.643374 20312 net.cpp:513] ctx_conv3/bn -> ctx_conv3 (in-place)
I0731 21:37:17.644084 20312 net.cpp:245] Setting up ctx_conv3/bn
I0731 21:37:17.644093 20312 net.cpp:252] TRAIN Top shape for layer 52 'ctx_conv3/bn' 6 64 80 80 (2457600)
I0731 21:37:17.644098 20312 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I0731 21:37:17.644100 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.644104 20312 net.cpp:184] Created Layer ctx_conv3/relu (53)
I0731 21:37:17.644105 20312 net.cpp:561] ctx_conv3/relu <- ctx_conv3
I0731 21:37:17.644107 20312 net.cpp:513] ctx_conv3/relu -> ctx_conv3 (in-place)
I0731 21:37:17.644110 20312 net.cpp:245] Setting up ctx_conv3/relu
I0731 21:37:17.644114 20312 net.cpp:252] TRAIN Top shape for layer 53 'ctx_conv3/relu' 6 64 80 80 (2457600)
I0731 21:37:17.644114 20312 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I0731 21:37:17.644117 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.644122 20312 net.cpp:184] Created Layer ctx_conv4 (54)
I0731 21:37:17.644125 20312 net.cpp:561] ctx_conv4 <- ctx_conv3
I0731 21:37:17.644130 20312 net.cpp:530] ctx_conv4 -> ctx_conv4
I0731 21:37:17.645510 20312 net.cpp:245] Setting up ctx_conv4
I0731 21:37:17.645520 20312 net.cpp:252] TRAIN Top shape for layer 54 'ctx_conv4' 6 64 80 80 (2457600)
I0731 21:37:17.645526 20312 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I0731 21:37:17.645530 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.645541 20312 net.cpp:184] Created Layer ctx_conv4/bn (55)
I0731 21:37:17.645545 20312 net.cpp:561] ctx_conv4/bn <- ctx_conv4
I0731 21:37:17.645548 20312 net.cpp:513] ctx_conv4/bn -> ctx_conv4 (in-place)
I0731 21:37:17.646446 20312 net.cpp:245] Setting up ctx_conv4/bn
I0731 21:37:17.646456 20312 net.cpp:252] TRAIN Top shape for layer 55 'ctx_conv4/bn' 6 64 80 80 (2457600)
I0731 21:37:17.646466 20312 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I0731 21:37:17.646469 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.646474 20312 net.cpp:184] Created Layer ctx_conv4/relu (56)
I0731 21:37:17.646477 20312 net.cpp:561] ctx_conv4/relu <- ctx_conv4
I0731 21:37:17.646481 20312 net.cpp:513] ctx_conv4/relu -> ctx_conv4 (in-place)
I0731 21:37:17.646486 20312 net.cpp:245] Setting up ctx_conv4/relu
I0731 21:37:17.646492 20312 net.cpp:252] TRAIN Top shape for layer 56 'ctx_conv4/relu' 6 64 80 80 (2457600)
I0731 21:37:17.646495 20312 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I0731 21:37:17.646498 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.646512 20312 net.cpp:184] Created Layer ctx_final (57)
I0731 21:37:17.646515 20312 net.cpp:561] ctx_final <- ctx_conv4
I0731 21:37:17.646519 20312 net.cpp:530] ctx_final -> ctx_final
I0731 21:37:17.661491 20312 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 7.22G, req 0G)
I0731 21:37:17.661514 20312 net.cpp:245] Setting up ctx_final
I0731 21:37:17.661520 20312 net.cpp:252] TRAIN Top shape for layer 57 'ctx_final' 6 8 80 80 (307200)
I0731 21:37:17.661530 20312 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I0731 21:37:17.661533 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.661540 20312 net.cpp:184] Created Layer ctx_final/relu (58)
I0731 21:37:17.661545 20312 net.cpp:561] ctx_final/relu <- ctx_final
I0731 21:37:17.661550 20312 net.cpp:513] ctx_final/relu -> ctx_final (in-place)
I0731 21:37:17.661556 20312 net.cpp:245] Setting up ctx_final/relu
I0731 21:37:17.661558 20312 net.cpp:252] TRAIN Top shape for layer 58 'ctx_final/relu' 6 8 80 80 (307200)
I0731 21:37:17.661561 20312 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I0731 21:37:17.661576 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.661586 20312 net.cpp:184] Created Layer out_deconv_final_up2 (59)
I0731 21:37:17.661588 20312 net.cpp:561] out_deconv_final_up2 <- ctx_final
I0731 21:37:17.661592 20312 net.cpp:530] out_deconv_final_up2 -> out_deconv_final_up2
I0731 21:37:17.661972 20312 net.cpp:245] Setting up out_deconv_final_up2
I0731 21:37:17.661981 20312 net.cpp:252] TRAIN Top shape for layer 59 'out_deconv_final_up2' 6 8 160 160 (1228800)
I0731 21:37:17.661986 20312 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I0731 21:37:17.661990 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.661998 20312 net.cpp:184] Created Layer out_deconv_final_up4 (60)
I0731 21:37:17.662001 20312 net.cpp:561] out_deconv_final_up4 <- out_deconv_final_up2
I0731 21:37:17.662005 20312 net.cpp:530] out_deconv_final_up4 -> out_deconv_final_up4
I0731 21:37:17.662395 20312 net.cpp:245] Setting up out_deconv_final_up4
I0731 21:37:17.662405 20312 net.cpp:252] TRAIN Top shape for layer 60 'out_deconv_final_up4' 6 8 320 320 (4915200)
I0731 21:37:17.662410 20312 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I0731 21:37:17.662413 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.662421 20312 net.cpp:184] Created Layer out_deconv_final_up8 (61)
I0731 21:37:17.662425 20312 net.cpp:561] out_deconv_final_up8 <- out_deconv_final_up4
I0731 21:37:17.662430 20312 net.cpp:530] out_deconv_final_up8 -> out_deconv_final_up8
I0731 21:37:17.662784 20312 net.cpp:245] Setting up out_deconv_final_up8
I0731 21:37:17.662793 20312 net.cpp:252] TRAIN Top shape for layer 61 'out_deconv_final_up8' 6 8 640 640 (19660800)
I0731 21:37:17.662799 20312 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0731 21:37:17.662803 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.662819 20312 net.cpp:184] Created Layer loss (62)
I0731 21:37:17.662822 20312 net.cpp:561] loss <- out_deconv_final_up8
I0731 21:37:17.662825 20312 net.cpp:561] loss <- label
I0731 21:37:17.662829 20312 net.cpp:530] loss -> loss
I0731 21:37:17.664244 20312 net.cpp:245] Setting up loss
I0731 21:37:17.664252 20312 net.cpp:252] TRAIN Top shape for layer 62 'loss' (1)
I0731 21:37:17.664255 20312 net.cpp:256]     with loss weight 1
I0731 21:37:17.664259 20312 net.cpp:323] loss needs backward computation.
I0731 21:37:17.664261 20312 net.cpp:323] out_deconv_final_up8 needs backward computation.
I0731 21:37:17.664263 20312 net.cpp:323] out_deconv_final_up4 needs backward computation.
I0731 21:37:17.664265 20312 net.cpp:323] out_deconv_final_up2 needs backward computation.
I0731 21:37:17.664268 20312 net.cpp:323] ctx_final/relu needs backward computation.
I0731 21:37:17.664269 20312 net.cpp:323] ctx_final needs backward computation.
I0731 21:37:17.664271 20312 net.cpp:323] ctx_conv4/relu needs backward computation.
I0731 21:37:17.664273 20312 net.cpp:323] ctx_conv4/bn needs backward computation.
I0731 21:37:17.664275 20312 net.cpp:323] ctx_conv4 needs backward computation.
I0731 21:37:17.664278 20312 net.cpp:323] ctx_conv3/relu needs backward computation.
I0731 21:37:17.664279 20312 net.cpp:323] ctx_conv3/bn needs backward computation.
I0731 21:37:17.664280 20312 net.cpp:323] ctx_conv3 needs backward computation.
I0731 21:37:17.664283 20312 net.cpp:323] ctx_conv2/relu needs backward computation.
I0731 21:37:17.664284 20312 net.cpp:323] ctx_conv2/bn needs backward computation.
I0731 21:37:17.664288 20312 net.cpp:323] ctx_conv2 needs backward computation.
I0731 21:37:17.664289 20312 net.cpp:323] ctx_conv1/relu needs backward computation.
I0731 21:37:17.664291 20312 net.cpp:323] ctx_conv1/bn needs backward computation.
I0731 21:37:17.664294 20312 net.cpp:323] ctx_conv1 needs backward computation.
I0731 21:37:17.664302 20312 net.cpp:323] out3_out5_combined needs backward computation.
I0731 21:37:17.664304 20312 net.cpp:323] out3a/relu needs backward computation.
I0731 21:37:17.664306 20312 net.cpp:323] out3a/bn needs backward computation.
I0731 21:37:17.664309 20312 net.cpp:323] out3a needs backward computation.
I0731 21:37:17.664311 20312 net.cpp:323] out5a_up2 needs backward computation.
I0731 21:37:17.664314 20312 net.cpp:323] out5a/relu needs backward computation.
I0731 21:37:17.664319 20312 net.cpp:323] out5a/bn needs backward computation.
I0731 21:37:17.664321 20312 net.cpp:323] out5a needs backward computation.
I0731 21:37:17.664325 20312 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0731 21:37:17.664330 20312 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0731 21:37:17.664333 20312 net.cpp:323] res5a_branch2b needs backward computation.
I0731 21:37:17.664336 20312 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0731 21:37:17.664340 20312 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0731 21:37:17.664343 20312 net.cpp:323] res5a_branch2a needs backward computation.
I0731 21:37:17.664347 20312 net.cpp:323] pool4 needs backward computation.
I0731 21:37:17.664351 20312 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0731 21:37:17.664355 20312 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0731 21:37:17.664358 20312 net.cpp:323] res4a_branch2b needs backward computation.
I0731 21:37:17.664363 20312 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0731 21:37:17.664366 20312 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0731 21:37:17.664371 20312 net.cpp:323] res4a_branch2a needs backward computation.
I0731 21:37:17.664376 20312 net.cpp:323] pool3 needs backward computation.
I0731 21:37:17.664379 20312 net.cpp:323] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I0731 21:37:17.664383 20312 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0731 21:37:17.664387 20312 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0731 21:37:17.664391 20312 net.cpp:323] res3a_branch2b needs backward computation.
I0731 21:37:17.664396 20312 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0731 21:37:17.664399 20312 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0731 21:37:17.664403 20312 net.cpp:323] res3a_branch2a needs backward computation.
I0731 21:37:17.664407 20312 net.cpp:323] pool2 needs backward computation.
I0731 21:37:17.664412 20312 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0731 21:37:17.664415 20312 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0731 21:37:17.664419 20312 net.cpp:323] res2a_branch2b needs backward computation.
I0731 21:37:17.664423 20312 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0731 21:37:17.664427 20312 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0731 21:37:17.664432 20312 net.cpp:323] res2a_branch2a needs backward computation.
I0731 21:37:17.664435 20312 net.cpp:323] pool1 needs backward computation.
I0731 21:37:17.664440 20312 net.cpp:323] conv1b/relu needs backward computation.
I0731 21:37:17.664444 20312 net.cpp:323] conv1b/bn needs backward computation.
I0731 21:37:17.664448 20312 net.cpp:323] conv1b needs backward computation.
I0731 21:37:17.664453 20312 net.cpp:323] conv1a/relu needs backward computation.
I0731 21:37:17.664455 20312 net.cpp:323] conv1a/bn needs backward computation.
I0731 21:37:17.664459 20312 net.cpp:323] conv1a needs backward computation.
I0731 21:37:17.664464 20312 net.cpp:325] data/bias does not need backward computation.
I0731 21:37:17.664469 20312 net.cpp:325] data does not need backward computation.
I0731 21:37:17.664472 20312 net.cpp:367] This network produces output loss
I0731 21:37:17.664542 20312 net.cpp:389] Top memory (TRAIN) required for data: 956006400 diff: 946176008
I0731 21:37:17.664548 20312 net.cpp:392] Bottom memory (TRAIN) required for data: 956006400 diff: 956006400
I0731 21:37:17.664552 20312 net.cpp:395] Shared (in-place) memory (TRAIN) by data: 630374400 diff: 630374400
I0731 21:37:17.664561 20312 net.cpp:398] Parameters memory (TRAIN) required for data: 2692608 diff: 2692608
I0731 21:37:17.664564 20312 net.cpp:401] Parameters shared memory (TRAIN) by data: 0 diff: 0
I0731 21:37:17.664569 20312 net.cpp:407] Network initialization done.
I0731 21:37:17.665467 20312 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/sparse/test.prototxt
W0731 21:37:17.665568 20312 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0731 21:37:17.665882 20312 net.cpp:72] Initializing net from parameters: 
name: "jsegnet21v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/val-image-lmdb"
    label_list_path: "data/val-label-lmdb"
    batch_size: 2
    threads: 1
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
  accuracy_param {
    ignore_label: 255
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
    ignore_label: 255
  }
}
I0731 21:37:17.666095 20312 net.cpp:104] Using FLOAT as default forward math type
I0731 21:37:17.666101 20312 net.cpp:110] Using FLOAT as default backward math type
I0731 21:37:17.666110 20312 layer_factory.hpp:136] Creating layer 'data' of type 'ImageLabelData'
I0731 21:37:17.666115 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.666122 20312 net.cpp:184] Created Layer data (0)
I0731 21:37:17.666126 20312 net.cpp:530] data -> data
I0731 21:37:17.666132 20312 net.cpp:530] data -> label
I0731 21:37:17.666154 20312 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0731 21:37:17.666162 20312 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 21:37:17.707537 20402 db_lmdb.cpp:35] Opened lmdb data/val-image-lmdb
I0731 21:37:17.709429 20312 data_layer.cpp:184] (0) ReshapePrefetch 2, 3, 640, 640
I0731 21:37:17.709525 20312 data_layer.cpp:208] (0) Output data size: 2, 3, 640, 640
I0731 21:37:17.709535 20312 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 21:37:17.709595 20312 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0731 21:37:17.709607 20312 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 21:37:17.710530 20403 data_layer.cpp:97] (0) Parser threads: 1
I0731 21:37:17.710544 20403 data_layer.cpp:99] (0) Transformer threads: 1
I0731 21:37:17.740299 20404 db_lmdb.cpp:35] Opened lmdb data/val-label-lmdb
I0731 21:37:17.741274 20312 data_layer.cpp:184] (0) ReshapePrefetch 2, 1, 640, 640
I0731 21:37:17.741356 20312 data_layer.cpp:208] (0) Output data size: 2, 1, 640, 640
I0731 21:37:17.741365 20312 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 21:37:17.741411 20312 net.cpp:245] Setting up data
I0731 21:37:17.741422 20312 net.cpp:252] TEST Top shape for layer 0 'data' 2 3 640 640 (2457600)
I0731 21:37:17.741431 20312 net.cpp:252] TEST Top shape for layer 0 'data' 2 1 640 640 (819200)
I0731 21:37:17.741438 20312 layer_factory.hpp:136] Creating layer 'label_data_1_split' of type 'Split'
I0731 21:37:17.741446 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.741458 20312 net.cpp:184] Created Layer label_data_1_split (1)
I0731 21:37:17.741464 20312 net.cpp:561] label_data_1_split <- label
I0731 21:37:17.741472 20312 net.cpp:530] label_data_1_split -> label_data_1_split_0
I0731 21:37:17.741480 20312 net.cpp:530] label_data_1_split -> label_data_1_split_1
I0731 21:37:17.741487 20312 net.cpp:530] label_data_1_split -> label_data_1_split_2
I0731 21:37:17.741613 20312 net.cpp:245] Setting up label_data_1_split
I0731 21:37:17.741623 20312 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0731 21:37:17.741631 20312 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0731 21:37:17.741636 20312 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0731 21:37:17.741641 20312 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0731 21:37:17.741648 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.741659 20312 net.cpp:184] Created Layer data/bias (2)
I0731 21:37:17.741675 20312 net.cpp:561] data/bias <- data
I0731 21:37:17.741681 20312 net.cpp:530] data/bias -> data/bias
I0731 21:37:17.743319 20405 data_layer.cpp:97] (0) Parser threads: 1
I0731 21:37:17.743335 20405 data_layer.cpp:99] (0) Transformer threads: 1
I0731 21:37:17.745365 20312 net.cpp:245] Setting up data/bias
I0731 21:37:17.745386 20312 net.cpp:252] TEST Top shape for layer 2 'data/bias' 2 3 640 640 (2457600)
I0731 21:37:17.745398 20312 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0731 21:37:17.745404 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.745416 20312 net.cpp:184] Created Layer conv1a (3)
I0731 21:37:17.745421 20312 net.cpp:561] conv1a <- data/bias
I0731 21:37:17.745425 20312 net.cpp:530] conv1a -> conv1a
I0731 21:37:17.751524 20312 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.09G, req 0G)
I0731 21:37:17.751560 20312 net.cpp:245] Setting up conv1a
I0731 21:37:17.751567 20312 net.cpp:252] TEST Top shape for layer 3 'conv1a' 2 32 320 320 (6553600)
I0731 21:37:17.751579 20312 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0731 21:37:17.751585 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.751593 20312 net.cpp:184] Created Layer conv1a/bn (4)
I0731 21:37:17.751597 20312 net.cpp:561] conv1a/bn <- conv1a
I0731 21:37:17.751601 20312 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0731 21:37:17.752620 20312 net.cpp:245] Setting up conv1a/bn
I0731 21:37:17.752630 20312 net.cpp:252] TEST Top shape for layer 4 'conv1a/bn' 2 32 320 320 (6553600)
I0731 21:37:17.752640 20312 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0731 21:37:17.752643 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.752712 20312 net.cpp:184] Created Layer conv1a/relu (5)
I0731 21:37:17.752718 20312 net.cpp:561] conv1a/relu <- conv1a
I0731 21:37:17.752722 20312 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0731 21:37:17.752727 20312 net.cpp:245] Setting up conv1a/relu
I0731 21:37:17.752732 20312 net.cpp:252] TEST Top shape for layer 5 'conv1a/relu' 2 32 320 320 (6553600)
I0731 21:37:17.752735 20312 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0731 21:37:17.752739 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.752748 20312 net.cpp:184] Created Layer conv1b (6)
I0731 21:37:17.752753 20312 net.cpp:561] conv1b <- conv1a
I0731 21:37:17.752755 20312 net.cpp:530] conv1b -> conv1b
I0731 21:37:17.767802 20312 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.06G, req 0G)
I0731 21:37:17.767817 20312 net.cpp:245] Setting up conv1b
I0731 21:37:17.767822 20312 net.cpp:252] TEST Top shape for layer 6 'conv1b' 2 32 320 320 (6553600)
I0731 21:37:17.767830 20312 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0731 21:37:17.767834 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.767840 20312 net.cpp:184] Created Layer conv1b/bn (7)
I0731 21:37:17.767844 20312 net.cpp:561] conv1b/bn <- conv1b
I0731 21:37:17.767848 20312 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0731 21:37:17.768808 20312 net.cpp:245] Setting up conv1b/bn
I0731 21:37:17.768826 20312 net.cpp:252] TEST Top shape for layer 7 'conv1b/bn' 2 32 320 320 (6553600)
I0731 21:37:17.768836 20312 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0731 21:37:17.768839 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.768843 20312 net.cpp:184] Created Layer conv1b/relu (8)
I0731 21:37:17.768847 20312 net.cpp:561] conv1b/relu <- conv1b
I0731 21:37:17.768851 20312 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0731 21:37:17.768854 20312 net.cpp:245] Setting up conv1b/relu
I0731 21:37:17.768859 20312 net.cpp:252] TEST Top shape for layer 8 'conv1b/relu' 2 32 320 320 (6553600)
I0731 21:37:17.768863 20312 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0731 21:37:17.768867 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.768872 20312 net.cpp:184] Created Layer pool1 (9)
I0731 21:37:17.768877 20312 net.cpp:561] pool1 <- conv1b
I0731 21:37:17.768880 20312 net.cpp:530] pool1 -> pool1
I0731 21:37:17.768981 20312 net.cpp:245] Setting up pool1
I0731 21:37:17.768988 20312 net.cpp:252] TEST Top shape for layer 9 'pool1' 2 32 160 160 (1638400)
I0731 21:37:17.768991 20312 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0731 21:37:17.768996 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.769003 20312 net.cpp:184] Created Layer res2a_branch2a (10)
I0731 21:37:17.769007 20312 net.cpp:561] res2a_branch2a <- pool1
I0731 21:37:17.769021 20312 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0731 21:37:17.778324 20312 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.03G, req 0G)
I0731 21:37:17.778347 20312 net.cpp:245] Setting up res2a_branch2a
I0731 21:37:17.778353 20312 net.cpp:252] TEST Top shape for layer 10 'res2a_branch2a' 2 64 160 160 (3276800)
I0731 21:37:17.778367 20312 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0731 21:37:17.778372 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.778390 20312 net.cpp:184] Created Layer res2a_branch2a/bn (11)
I0731 21:37:17.778395 20312 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0731 21:37:17.778399 20312 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0731 21:37:17.779481 20312 net.cpp:245] Setting up res2a_branch2a/bn
I0731 21:37:17.779492 20312 net.cpp:252] TEST Top shape for layer 11 'res2a_branch2a/bn' 2 64 160 160 (3276800)
I0731 21:37:17.779500 20312 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0731 21:37:17.779505 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.779510 20312 net.cpp:184] Created Layer res2a_branch2a/relu (12)
I0731 21:37:17.779512 20312 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0731 21:37:17.779516 20312 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0731 21:37:17.779521 20312 net.cpp:245] Setting up res2a_branch2a/relu
I0731 21:37:17.779525 20312 net.cpp:252] TEST Top shape for layer 12 'res2a_branch2a/relu' 2 64 160 160 (3276800)
I0731 21:37:17.779527 20312 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0731 21:37:17.779532 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.779541 20312 net.cpp:184] Created Layer res2a_branch2b (13)
I0731 21:37:17.779546 20312 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0731 21:37:17.779548 20312 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0731 21:37:17.787411 20312 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.02G, req 0G)
I0731 21:37:17.787426 20312 net.cpp:245] Setting up res2a_branch2b
I0731 21:37:17.787432 20312 net.cpp:252] TEST Top shape for layer 13 'res2a_branch2b' 2 64 160 160 (3276800)
I0731 21:37:17.787438 20312 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0731 21:37:17.787442 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.787448 20312 net.cpp:184] Created Layer res2a_branch2b/bn (14)
I0731 21:37:17.787452 20312 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0731 21:37:17.787456 20312 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0731 21:37:17.788439 20312 net.cpp:245] Setting up res2a_branch2b/bn
I0731 21:37:17.788450 20312 net.cpp:252] TEST Top shape for layer 14 'res2a_branch2b/bn' 2 64 160 160 (3276800)
I0731 21:37:17.788457 20312 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0731 21:37:17.788461 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.788466 20312 net.cpp:184] Created Layer res2a_branch2b/relu (15)
I0731 21:37:17.788470 20312 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0731 21:37:17.788472 20312 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0731 21:37:17.788477 20312 net.cpp:245] Setting up res2a_branch2b/relu
I0731 21:37:17.788481 20312 net.cpp:252] TEST Top shape for layer 15 'res2a_branch2b/relu' 2 64 160 160 (3276800)
I0731 21:37:17.788483 20312 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0731 21:37:17.788488 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.788494 20312 net.cpp:184] Created Layer pool2 (16)
I0731 21:37:17.788498 20312 net.cpp:561] pool2 <- res2a_branch2b
I0731 21:37:17.788513 20312 net.cpp:530] pool2 -> pool2
I0731 21:37:17.788606 20312 net.cpp:245] Setting up pool2
I0731 21:37:17.788612 20312 net.cpp:252] TEST Top shape for layer 16 'pool2' 2 64 80 80 (819200)
I0731 21:37:17.788616 20312 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0731 21:37:17.788620 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.788638 20312 net.cpp:184] Created Layer res3a_branch2a (17)
I0731 21:37:17.788642 20312 net.cpp:561] res3a_branch2a <- pool2
I0731 21:37:17.788646 20312 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0731 21:37:17.795866 20312 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.01G, req 0G)
I0731 21:37:17.795879 20312 net.cpp:245] Setting up res3a_branch2a
I0731 21:37:17.795884 20312 net.cpp:252] TEST Top shape for layer 17 'res3a_branch2a' 2 128 80 80 (1638400)
I0731 21:37:17.795892 20312 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0731 21:37:17.795898 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.795907 20312 net.cpp:184] Created Layer res3a_branch2a/bn (18)
I0731 21:37:17.795912 20312 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0731 21:37:17.795914 20312 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0731 21:37:17.796939 20312 net.cpp:245] Setting up res3a_branch2a/bn
I0731 21:37:17.796952 20312 net.cpp:252] TEST Top shape for layer 18 'res3a_branch2a/bn' 2 128 80 80 (1638400)
I0731 21:37:17.796962 20312 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0731 21:37:17.796965 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.796969 20312 net.cpp:184] Created Layer res3a_branch2a/relu (19)
I0731 21:37:17.796972 20312 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0731 21:37:17.796977 20312 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0731 21:37:17.796980 20312 net.cpp:245] Setting up res3a_branch2a/relu
I0731 21:37:17.796984 20312 net.cpp:252] TEST Top shape for layer 19 'res3a_branch2a/relu' 2 128 80 80 (1638400)
I0731 21:37:17.796988 20312 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0731 21:37:17.796993 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.797000 20312 net.cpp:184] Created Layer res3a_branch2b (20)
I0731 21:37:17.797003 20312 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0731 21:37:17.797008 20312 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0731 21:37:17.803287 20312 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7G, req 0G)
I0731 21:37:17.803304 20312 net.cpp:245] Setting up res3a_branch2b
I0731 21:37:17.803313 20312 net.cpp:252] TEST Top shape for layer 20 'res3a_branch2b' 2 128 80 80 (1638400)
I0731 21:37:17.803323 20312 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0731 21:37:17.803328 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.803336 20312 net.cpp:184] Created Layer res3a_branch2b/bn (21)
I0731 21:37:17.803344 20312 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0731 21:37:17.803350 20312 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0731 21:37:17.805080 20312 net.cpp:245] Setting up res3a_branch2b/bn
I0731 21:37:17.805100 20312 net.cpp:252] TEST Top shape for layer 21 'res3a_branch2b/bn' 2 128 80 80 (1638400)
I0731 21:37:17.805115 20312 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0731 21:37:17.805120 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.805129 20312 net.cpp:184] Created Layer res3a_branch2b/relu (22)
I0731 21:37:17.805133 20312 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0731 21:37:17.805141 20312 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0731 21:37:17.805163 20312 net.cpp:245] Setting up res3a_branch2b/relu
I0731 21:37:17.805169 20312 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b/relu' 2 128 80 80 (1638400)
I0731 21:37:17.805174 20312 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0731 21:37:17.805179 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.805184 20312 net.cpp:184] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (23)
I0731 21:37:17.805189 20312 net.cpp:561] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0731 21:37:17.805193 20312 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0731 21:37:17.805199 20312 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0731 21:37:17.805292 20312 net.cpp:245] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0731 21:37:17.805302 20312 net.cpp:252] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 2 128 80 80 (1638400)
I0731 21:37:17.805308 20312 net.cpp:252] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 2 128 80 80 (1638400)
I0731 21:37:17.805313 20312 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0731 21:37:17.805318 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.805335 20312 net.cpp:184] Created Layer pool3 (24)
I0731 21:37:17.805341 20312 net.cpp:561] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0731 21:37:17.805348 20312 net.cpp:530] pool3 -> pool3
I0731 21:37:17.805477 20312 net.cpp:245] Setting up pool3
I0731 21:37:17.805497 20312 net.cpp:252] TEST Top shape for layer 24 'pool3' 2 128 40 40 (409600)
I0731 21:37:17.805503 20312 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0731 21:37:17.805508 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.805526 20312 net.cpp:184] Created Layer res4a_branch2a (25)
I0731 21:37:17.805531 20312 net.cpp:561] res4a_branch2a <- pool3
I0731 21:37:17.805536 20312 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0731 21:37:17.822706 20312 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.99G, req 0G)
I0731 21:37:17.822720 20312 net.cpp:245] Setting up res4a_branch2a
I0731 21:37:17.822723 20312 net.cpp:252] TEST Top shape for layer 25 'res4a_branch2a' 2 256 40 40 (819200)
I0731 21:37:17.822728 20312 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0731 21:37:17.822731 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.822742 20312 net.cpp:184] Created Layer res4a_branch2a/bn (26)
I0731 21:37:17.822746 20312 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0731 21:37:17.822748 20312 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0731 21:37:17.823456 20312 net.cpp:245] Setting up res4a_branch2a/bn
I0731 21:37:17.823462 20312 net.cpp:252] TEST Top shape for layer 26 'res4a_branch2a/bn' 2 256 40 40 (819200)
I0731 21:37:17.823468 20312 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0731 21:37:17.823472 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.823474 20312 net.cpp:184] Created Layer res4a_branch2a/relu (27)
I0731 21:37:17.823477 20312 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0731 21:37:17.823485 20312 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0731 21:37:17.823489 20312 net.cpp:245] Setting up res4a_branch2a/relu
I0731 21:37:17.823492 20312 net.cpp:252] TEST Top shape for layer 27 'res4a_branch2a/relu' 2 256 40 40 (819200)
I0731 21:37:17.823494 20312 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0731 21:37:17.823498 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.823521 20312 net.cpp:184] Created Layer res4a_branch2b (28)
I0731 21:37:17.823524 20312 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0731 21:37:17.823526 20312 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0731 21:37:17.830296 20312 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.98G, req 0G)
I0731 21:37:17.830307 20312 net.cpp:245] Setting up res4a_branch2b
I0731 21:37:17.830312 20312 net.cpp:252] TEST Top shape for layer 28 'res4a_branch2b' 2 256 40 40 (819200)
I0731 21:37:17.830317 20312 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0731 21:37:17.830319 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.830324 20312 net.cpp:184] Created Layer res4a_branch2b/bn (29)
I0731 21:37:17.830327 20312 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0731 21:37:17.830330 20312 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0731 21:37:17.831032 20312 net.cpp:245] Setting up res4a_branch2b/bn
I0731 21:37:17.831039 20312 net.cpp:252] TEST Top shape for layer 29 'res4a_branch2b/bn' 2 256 40 40 (819200)
I0731 21:37:17.831045 20312 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0731 21:37:17.831048 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.831053 20312 net.cpp:184] Created Layer res4a_branch2b/relu (30)
I0731 21:37:17.831055 20312 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0731 21:37:17.831058 20312 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0731 21:37:17.831063 20312 net.cpp:245] Setting up res4a_branch2b/relu
I0731 21:37:17.831065 20312 net.cpp:252] TEST Top shape for layer 30 'res4a_branch2b/relu' 2 256 40 40 (819200)
I0731 21:37:17.831068 20312 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0731 21:37:17.831070 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.831074 20312 net.cpp:184] Created Layer pool4 (31)
I0731 21:37:17.831077 20312 net.cpp:561] pool4 <- res4a_branch2b
I0731 21:37:17.831080 20312 net.cpp:530] pool4 -> pool4
I0731 21:37:17.831156 20312 net.cpp:245] Setting up pool4
I0731 21:37:17.831161 20312 net.cpp:252] TEST Top shape for layer 31 'pool4' 2 256 40 40 (819200)
I0731 21:37:17.831164 20312 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0731 21:37:17.831167 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.831178 20312 net.cpp:184] Created Layer res5a_branch2a (32)
I0731 21:37:17.831182 20312 net.cpp:561] res5a_branch2a <- pool4
I0731 21:37:17.831184 20312 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0731 21:37:17.856552 20312 net.cpp:245] Setting up res5a_branch2a
I0731 21:37:17.856576 20312 net.cpp:252] TEST Top shape for layer 32 'res5a_branch2a' 2 512 40 40 (1638400)
I0731 21:37:17.856583 20312 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0731 21:37:17.856587 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.856593 20312 net.cpp:184] Created Layer res5a_branch2a/bn (33)
I0731 21:37:17.856597 20312 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0731 21:37:17.856601 20312 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0731 21:37:17.857312 20312 net.cpp:245] Setting up res5a_branch2a/bn
I0731 21:37:17.857321 20312 net.cpp:252] TEST Top shape for layer 33 'res5a_branch2a/bn' 2 512 40 40 (1638400)
I0731 21:37:17.857326 20312 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0731 21:37:17.857329 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.857332 20312 net.cpp:184] Created Layer res5a_branch2a/relu (34)
I0731 21:37:17.857336 20312 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0731 21:37:17.857337 20312 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0731 21:37:17.857350 20312 net.cpp:245] Setting up res5a_branch2a/relu
I0731 21:37:17.857353 20312 net.cpp:252] TEST Top shape for layer 34 'res5a_branch2a/relu' 2 512 40 40 (1638400)
I0731 21:37:17.857355 20312 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0731 21:37:17.857358 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.857365 20312 net.cpp:184] Created Layer res5a_branch2b (35)
I0731 21:37:17.857368 20312 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0731 21:37:17.857370 20312 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0731 21:37:17.870718 20312 net.cpp:245] Setting up res5a_branch2b
I0731 21:37:17.870740 20312 net.cpp:252] TEST Top shape for layer 35 'res5a_branch2b' 2 512 40 40 (1638400)
I0731 21:37:17.870759 20312 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0731 21:37:17.870764 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.870774 20312 net.cpp:184] Created Layer res5a_branch2b/bn (36)
I0731 21:37:17.870777 20312 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0731 21:37:17.870780 20312 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0731 21:37:17.871495 20312 net.cpp:245] Setting up res5a_branch2b/bn
I0731 21:37:17.871503 20312 net.cpp:252] TEST Top shape for layer 36 'res5a_branch2b/bn' 2 512 40 40 (1638400)
I0731 21:37:17.871510 20312 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0731 21:37:17.871512 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.871515 20312 net.cpp:184] Created Layer res5a_branch2b/relu (37)
I0731 21:37:17.871517 20312 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0731 21:37:17.871520 20312 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0731 21:37:17.871523 20312 net.cpp:245] Setting up res5a_branch2b/relu
I0731 21:37:17.871526 20312 net.cpp:252] TEST Top shape for layer 37 'res5a_branch2b/relu' 2 512 40 40 (1638400)
I0731 21:37:17.871528 20312 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I0731 21:37:17.871531 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.871541 20312 net.cpp:184] Created Layer out5a (38)
I0731 21:37:17.871543 20312 net.cpp:561] out5a <- res5a_branch2b
I0731 21:37:17.871546 20312 net.cpp:530] out5a -> out5a
I0731 21:37:17.874876 20312 net.cpp:245] Setting up out5a
I0731 21:37:17.874883 20312 net.cpp:252] TEST Top shape for layer 38 'out5a' 2 64 40 40 (204800)
I0731 21:37:17.874888 20312 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I0731 21:37:17.874891 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.874894 20312 net.cpp:184] Created Layer out5a/bn (39)
I0731 21:37:17.874897 20312 net.cpp:561] out5a/bn <- out5a
I0731 21:37:17.874899 20312 net.cpp:513] out5a/bn -> out5a (in-place)
I0731 21:37:17.875659 20312 net.cpp:245] Setting up out5a/bn
I0731 21:37:17.875666 20312 net.cpp:252] TEST Top shape for layer 39 'out5a/bn' 2 64 40 40 (204800)
I0731 21:37:17.875672 20312 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I0731 21:37:17.875674 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.875677 20312 net.cpp:184] Created Layer out5a/relu (40)
I0731 21:37:17.875680 20312 net.cpp:561] out5a/relu <- out5a
I0731 21:37:17.875682 20312 net.cpp:513] out5a/relu -> out5a (in-place)
I0731 21:37:17.875685 20312 net.cpp:245] Setting up out5a/relu
I0731 21:37:17.875687 20312 net.cpp:252] TEST Top shape for layer 40 'out5a/relu' 2 64 40 40 (204800)
I0731 21:37:17.875689 20312 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I0731 21:37:17.875692 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.875708 20312 net.cpp:184] Created Layer out5a_up2 (41)
I0731 21:37:17.875711 20312 net.cpp:561] out5a_up2 <- out5a
I0731 21:37:17.875713 20312 net.cpp:530] out5a_up2 -> out5a_up2
I0731 21:37:17.876020 20312 net.cpp:245] Setting up out5a_up2
I0731 21:37:17.876025 20312 net.cpp:252] TEST Top shape for layer 41 'out5a_up2' 2 64 80 80 (819200)
I0731 21:37:17.876029 20312 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I0731 21:37:17.876031 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.876036 20312 net.cpp:184] Created Layer out3a (42)
I0731 21:37:17.876039 20312 net.cpp:561] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0731 21:37:17.876041 20312 net.cpp:530] out3a -> out3a
I0731 21:37:17.880188 20312 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.97G, req 0G)
I0731 21:37:17.880198 20312 net.cpp:245] Setting up out3a
I0731 21:37:17.880201 20312 net.cpp:252] TEST Top shape for layer 42 'out3a' 2 64 80 80 (819200)
I0731 21:37:17.880205 20312 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I0731 21:37:17.880208 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.880213 20312 net.cpp:184] Created Layer out3a/bn (43)
I0731 21:37:17.880215 20312 net.cpp:561] out3a/bn <- out3a
I0731 21:37:17.880218 20312 net.cpp:513] out3a/bn -> out3a (in-place)
I0731 21:37:17.880962 20312 net.cpp:245] Setting up out3a/bn
I0731 21:37:17.880970 20312 net.cpp:252] TEST Top shape for layer 43 'out3a/bn' 2 64 80 80 (819200)
I0731 21:37:17.880975 20312 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I0731 21:37:17.880978 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.880981 20312 net.cpp:184] Created Layer out3a/relu (44)
I0731 21:37:17.880983 20312 net.cpp:561] out3a/relu <- out3a
I0731 21:37:17.880985 20312 net.cpp:513] out3a/relu -> out3a (in-place)
I0731 21:37:17.880990 20312 net.cpp:245] Setting up out3a/relu
I0731 21:37:17.880991 20312 net.cpp:252] TEST Top shape for layer 44 'out3a/relu' 2 64 80 80 (819200)
I0731 21:37:17.880993 20312 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I0731 21:37:17.880995 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.880998 20312 net.cpp:184] Created Layer out3_out5_combined (45)
I0731 21:37:17.881000 20312 net.cpp:561] out3_out5_combined <- out5a_up2
I0731 21:37:17.881003 20312 net.cpp:561] out3_out5_combined <- out3a
I0731 21:37:17.881006 20312 net.cpp:530] out3_out5_combined -> out3_out5_combined
I0731 21:37:17.881903 20312 net.cpp:245] Setting up out3_out5_combined
I0731 21:37:17.881912 20312 net.cpp:252] TEST Top shape for layer 45 'out3_out5_combined' 2 64 80 80 (819200)
I0731 21:37:17.881916 20312 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I0731 21:37:17.881917 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.881923 20312 net.cpp:184] Created Layer ctx_conv1 (46)
I0731 21:37:17.881925 20312 net.cpp:561] ctx_conv1 <- out3_out5_combined
I0731 21:37:17.881928 20312 net.cpp:530] ctx_conv1 -> ctx_conv1
I0731 21:37:17.886366 20312 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.96G, req 0G)
I0731 21:37:17.886375 20312 net.cpp:245] Setting up ctx_conv1
I0731 21:37:17.886379 20312 net.cpp:252] TEST Top shape for layer 46 'ctx_conv1' 2 64 80 80 (819200)
I0731 21:37:17.886384 20312 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I0731 21:37:17.886385 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.886394 20312 net.cpp:184] Created Layer ctx_conv1/bn (47)
I0731 21:37:17.886396 20312 net.cpp:561] ctx_conv1/bn <- ctx_conv1
I0731 21:37:17.886399 20312 net.cpp:513] ctx_conv1/bn -> ctx_conv1 (in-place)
I0731 21:37:17.887125 20312 net.cpp:245] Setting up ctx_conv1/bn
I0731 21:37:17.887138 20312 net.cpp:252] TEST Top shape for layer 47 'ctx_conv1/bn' 2 64 80 80 (819200)
I0731 21:37:17.887145 20312 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I0731 21:37:17.887147 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.887151 20312 net.cpp:184] Created Layer ctx_conv1/relu (48)
I0731 21:37:17.887153 20312 net.cpp:561] ctx_conv1/relu <- ctx_conv1
I0731 21:37:17.887156 20312 net.cpp:513] ctx_conv1/relu -> ctx_conv1 (in-place)
I0731 21:37:17.887159 20312 net.cpp:245] Setting up ctx_conv1/relu
I0731 21:37:17.887161 20312 net.cpp:252] TEST Top shape for layer 48 'ctx_conv1/relu' 2 64 80 80 (819200)
I0731 21:37:17.887163 20312 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I0731 21:37:17.887166 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.887171 20312 net.cpp:184] Created Layer ctx_conv2 (49)
I0731 21:37:17.887173 20312 net.cpp:561] ctx_conv2 <- ctx_conv1
I0731 21:37:17.887176 20312 net.cpp:530] ctx_conv2 -> ctx_conv2
I0731 21:37:17.888293 20312 net.cpp:245] Setting up ctx_conv2
I0731 21:37:17.888299 20312 net.cpp:252] TEST Top shape for layer 49 'ctx_conv2' 2 64 80 80 (819200)
I0731 21:37:17.888303 20312 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I0731 21:37:17.888305 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.888309 20312 net.cpp:184] Created Layer ctx_conv2/bn (50)
I0731 21:37:17.888311 20312 net.cpp:561] ctx_conv2/bn <- ctx_conv2
I0731 21:37:17.888314 20312 net.cpp:513] ctx_conv2/bn -> ctx_conv2 (in-place)
I0731 21:37:17.889037 20312 net.cpp:245] Setting up ctx_conv2/bn
I0731 21:37:17.889045 20312 net.cpp:252] TEST Top shape for layer 50 'ctx_conv2/bn' 2 64 80 80 (819200)
I0731 21:37:17.889050 20312 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I0731 21:37:17.889053 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.889055 20312 net.cpp:184] Created Layer ctx_conv2/relu (51)
I0731 21:37:17.889057 20312 net.cpp:561] ctx_conv2/relu <- ctx_conv2
I0731 21:37:17.889060 20312 net.cpp:513] ctx_conv2/relu -> ctx_conv2 (in-place)
I0731 21:37:17.889063 20312 net.cpp:245] Setting up ctx_conv2/relu
I0731 21:37:17.889065 20312 net.cpp:252] TEST Top shape for layer 51 'ctx_conv2/relu' 2 64 80 80 (819200)
I0731 21:37:17.889067 20312 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I0731 21:37:17.889070 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.889075 20312 net.cpp:184] Created Layer ctx_conv3 (52)
I0731 21:37:17.889076 20312 net.cpp:561] ctx_conv3 <- ctx_conv2
I0731 21:37:17.889078 20312 net.cpp:530] ctx_conv3 -> ctx_conv3
I0731 21:37:17.890188 20312 net.cpp:245] Setting up ctx_conv3
I0731 21:37:17.890195 20312 net.cpp:252] TEST Top shape for layer 52 'ctx_conv3' 2 64 80 80 (819200)
I0731 21:37:17.890199 20312 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I0731 21:37:17.890202 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.890205 20312 net.cpp:184] Created Layer ctx_conv3/bn (53)
I0731 21:37:17.890208 20312 net.cpp:561] ctx_conv3/bn <- ctx_conv3
I0731 21:37:17.890210 20312 net.cpp:513] ctx_conv3/bn -> ctx_conv3 (in-place)
I0731 21:37:17.890960 20312 net.cpp:245] Setting up ctx_conv3/bn
I0731 21:37:17.890969 20312 net.cpp:252] TEST Top shape for layer 53 'ctx_conv3/bn' 2 64 80 80 (819200)
I0731 21:37:17.890974 20312 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I0731 21:37:17.890977 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.890980 20312 net.cpp:184] Created Layer ctx_conv3/relu (54)
I0731 21:37:17.890982 20312 net.cpp:561] ctx_conv3/relu <- ctx_conv3
I0731 21:37:17.890990 20312 net.cpp:513] ctx_conv3/relu -> ctx_conv3 (in-place)
I0731 21:37:17.890995 20312 net.cpp:245] Setting up ctx_conv3/relu
I0731 21:37:17.890997 20312 net.cpp:252] TEST Top shape for layer 54 'ctx_conv3/relu' 2 64 80 80 (819200)
I0731 21:37:17.891000 20312 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I0731 21:37:17.891001 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.891010 20312 net.cpp:184] Created Layer ctx_conv4 (55)
I0731 21:37:17.891013 20312 net.cpp:561] ctx_conv4 <- ctx_conv3
I0731 21:37:17.891016 20312 net.cpp:530] ctx_conv4 -> ctx_conv4
I0731 21:37:17.892292 20312 net.cpp:245] Setting up ctx_conv4
I0731 21:37:17.892300 20312 net.cpp:252] TEST Top shape for layer 55 'ctx_conv4' 2 64 80 80 (819200)
I0731 21:37:17.892304 20312 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I0731 21:37:17.892307 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.892310 20312 net.cpp:184] Created Layer ctx_conv4/bn (56)
I0731 21:37:17.892313 20312 net.cpp:561] ctx_conv4/bn <- ctx_conv4
I0731 21:37:17.892315 20312 net.cpp:513] ctx_conv4/bn -> ctx_conv4 (in-place)
I0731 21:37:17.893079 20312 net.cpp:245] Setting up ctx_conv4/bn
I0731 21:37:17.893086 20312 net.cpp:252] TEST Top shape for layer 56 'ctx_conv4/bn' 2 64 80 80 (819200)
I0731 21:37:17.893092 20312 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I0731 21:37:17.893095 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.893097 20312 net.cpp:184] Created Layer ctx_conv4/relu (57)
I0731 21:37:17.893100 20312 net.cpp:561] ctx_conv4/relu <- ctx_conv4
I0731 21:37:17.893102 20312 net.cpp:513] ctx_conv4/relu -> ctx_conv4 (in-place)
I0731 21:37:17.893105 20312 net.cpp:245] Setting up ctx_conv4/relu
I0731 21:37:17.893108 20312 net.cpp:252] TEST Top shape for layer 57 'ctx_conv4/relu' 2 64 80 80 (819200)
I0731 21:37:17.893110 20312 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I0731 21:37:17.893111 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.893116 20312 net.cpp:184] Created Layer ctx_final (58)
I0731 21:37:17.893118 20312 net.cpp:561] ctx_final <- ctx_conv4
I0731 21:37:17.893121 20312 net.cpp:530] ctx_final -> ctx_final
I0731 21:37:17.897956 20312 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.96G, req 0G)
I0731 21:37:17.897966 20312 net.cpp:245] Setting up ctx_final
I0731 21:37:17.897971 20312 net.cpp:252] TEST Top shape for layer 58 'ctx_final' 2 8 80 80 (102400)
I0731 21:37:17.897975 20312 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I0731 21:37:17.897979 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.897981 20312 net.cpp:184] Created Layer ctx_final/relu (59)
I0731 21:37:17.897984 20312 net.cpp:561] ctx_final/relu <- ctx_final
I0731 21:37:17.897986 20312 net.cpp:513] ctx_final/relu -> ctx_final (in-place)
I0731 21:37:17.897996 20312 net.cpp:245] Setting up ctx_final/relu
I0731 21:37:17.898000 20312 net.cpp:252] TEST Top shape for layer 59 'ctx_final/relu' 2 8 80 80 (102400)
I0731 21:37:17.898001 20312 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I0731 21:37:17.898003 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.898010 20312 net.cpp:184] Created Layer out_deconv_final_up2 (60)
I0731 21:37:17.898012 20312 net.cpp:561] out_deconv_final_up2 <- ctx_final
I0731 21:37:17.898015 20312 net.cpp:530] out_deconv_final_up2 -> out_deconv_final_up2
I0731 21:37:17.898360 20312 net.cpp:245] Setting up out_deconv_final_up2
I0731 21:37:17.898368 20312 net.cpp:252] TEST Top shape for layer 60 'out_deconv_final_up2' 2 8 160 160 (409600)
I0731 21:37:17.898373 20312 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I0731 21:37:17.898386 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.898393 20312 net.cpp:184] Created Layer out_deconv_final_up4 (61)
I0731 21:37:17.898397 20312 net.cpp:561] out_deconv_final_up4 <- out_deconv_final_up2
I0731 21:37:17.898401 20312 net.cpp:530] out_deconv_final_up4 -> out_deconv_final_up4
I0731 21:37:17.898810 20312 net.cpp:245] Setting up out_deconv_final_up4
I0731 21:37:17.898818 20312 net.cpp:252] TEST Top shape for layer 61 'out_deconv_final_up4' 2 8 320 320 (1638400)
I0731 21:37:17.898821 20312 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I0731 21:37:17.898823 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.898831 20312 net.cpp:184] Created Layer out_deconv_final_up8 (62)
I0731 21:37:17.898834 20312 net.cpp:561] out_deconv_final_up8 <- out_deconv_final_up4
I0731 21:37:17.898838 20312 net.cpp:530] out_deconv_final_up8 -> out_deconv_final_up8
I0731 21:37:17.899199 20312 net.cpp:245] Setting up out_deconv_final_up8
I0731 21:37:17.899204 20312 net.cpp:252] TEST Top shape for layer 62 'out_deconv_final_up8' 2 8 640 640 (6553600)
I0731 21:37:17.899209 20312 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8_out_deconv_final_up8_0_split' of type 'Split'
I0731 21:37:17.899210 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.899214 20312 net.cpp:184] Created Layer out_deconv_final_up8_out_deconv_final_up8_0_split (63)
I0731 21:37:17.899216 20312 net.cpp:561] out_deconv_final_up8_out_deconv_final_up8_0_split <- out_deconv_final_up8
I0731 21:37:17.899219 20312 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0731 21:37:17.899221 20312 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0731 21:37:17.899224 20312 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0731 21:37:17.899296 20312 net.cpp:245] Setting up out_deconv_final_up8_out_deconv_final_up8_0_split
I0731 21:37:17.899303 20312 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 8 640 640 (6553600)
I0731 21:37:17.899307 20312 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 8 640 640 (6553600)
I0731 21:37:17.899312 20312 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 8 640 640 (6553600)
I0731 21:37:17.899315 20312 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0731 21:37:17.899319 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.899327 20312 net.cpp:184] Created Layer loss (64)
I0731 21:37:17.899330 20312 net.cpp:561] loss <- out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0731 21:37:17.899335 20312 net.cpp:561] loss <- label_data_1_split_0
I0731 21:37:17.899340 20312 net.cpp:530] loss -> loss
I0731 21:37:17.900352 20312 net.cpp:245] Setting up loss
I0731 21:37:17.900362 20312 net.cpp:252] TEST Top shape for layer 64 'loss' (1)
I0731 21:37:17.900363 20312 net.cpp:256]     with loss weight 1
I0731 21:37:17.900367 20312 layer_factory.hpp:136] Creating layer 'accuracy/top1' of type 'Accuracy'
I0731 21:37:17.900370 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.900377 20312 net.cpp:184] Created Layer accuracy/top1 (65)
I0731 21:37:17.900379 20312 net.cpp:561] accuracy/top1 <- out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0731 21:37:17.900382 20312 net.cpp:561] accuracy/top1 <- label_data_1_split_1
I0731 21:37:17.900385 20312 net.cpp:530] accuracy/top1 -> accuracy/top1
I0731 21:37:17.900390 20312 net.cpp:245] Setting up accuracy/top1
I0731 21:37:17.900393 20312 net.cpp:252] TEST Top shape for layer 65 'accuracy/top1' (1)
I0731 21:37:17.900401 20312 layer_factory.hpp:136] Creating layer 'accuracy/top5' of type 'Accuracy'
I0731 21:37:17.900404 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.900408 20312 net.cpp:184] Created Layer accuracy/top5 (66)
I0731 21:37:17.900410 20312 net.cpp:561] accuracy/top5 <- out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0731 21:37:17.900413 20312 net.cpp:561] accuracy/top5 <- label_data_1_split_2
I0731 21:37:17.900415 20312 net.cpp:530] accuracy/top5 -> accuracy/top5
I0731 21:37:17.900419 20312 net.cpp:245] Setting up accuracy/top5
I0731 21:37:17.900423 20312 net.cpp:252] TEST Top shape for layer 66 'accuracy/top5' (1)
I0731 21:37:17.900425 20312 net.cpp:325] accuracy/top5 does not need backward computation.
I0731 21:37:17.900429 20312 net.cpp:325] accuracy/top1 does not need backward computation.
I0731 21:37:17.900434 20312 net.cpp:323] loss needs backward computation.
I0731 21:37:17.900437 20312 net.cpp:323] out_deconv_final_up8_out_deconv_final_up8_0_split needs backward computation.
I0731 21:37:17.900441 20312 net.cpp:323] out_deconv_final_up8 needs backward computation.
I0731 21:37:17.900445 20312 net.cpp:323] out_deconv_final_up4 needs backward computation.
I0731 21:37:17.900449 20312 net.cpp:323] out_deconv_final_up2 needs backward computation.
I0731 21:37:17.900452 20312 net.cpp:323] ctx_final/relu needs backward computation.
I0731 21:37:17.900455 20312 net.cpp:323] ctx_final needs backward computation.
I0731 21:37:17.900460 20312 net.cpp:323] ctx_conv4/relu needs backward computation.
I0731 21:37:17.900463 20312 net.cpp:323] ctx_conv4/bn needs backward computation.
I0731 21:37:17.900466 20312 net.cpp:323] ctx_conv4 needs backward computation.
I0731 21:37:17.900470 20312 net.cpp:323] ctx_conv3/relu needs backward computation.
I0731 21:37:17.900475 20312 net.cpp:323] ctx_conv3/bn needs backward computation.
I0731 21:37:17.900478 20312 net.cpp:323] ctx_conv3 needs backward computation.
I0731 21:37:17.900482 20312 net.cpp:323] ctx_conv2/relu needs backward computation.
I0731 21:37:17.900486 20312 net.cpp:323] ctx_conv2/bn needs backward computation.
I0731 21:37:17.900490 20312 net.cpp:323] ctx_conv2 needs backward computation.
I0731 21:37:17.900493 20312 net.cpp:323] ctx_conv1/relu needs backward computation.
I0731 21:37:17.900497 20312 net.cpp:323] ctx_conv1/bn needs backward computation.
I0731 21:37:17.900501 20312 net.cpp:323] ctx_conv1 needs backward computation.
I0731 21:37:17.900506 20312 net.cpp:323] out3_out5_combined needs backward computation.
I0731 21:37:17.900509 20312 net.cpp:323] out3a/relu needs backward computation.
I0731 21:37:17.900513 20312 net.cpp:323] out3a/bn needs backward computation.
I0731 21:37:17.900517 20312 net.cpp:323] out3a needs backward computation.
I0731 21:37:17.900522 20312 net.cpp:323] out5a_up2 needs backward computation.
I0731 21:37:17.900527 20312 net.cpp:323] out5a/relu needs backward computation.
I0731 21:37:17.900530 20312 net.cpp:323] out5a/bn needs backward computation.
I0731 21:37:17.900533 20312 net.cpp:323] out5a needs backward computation.
I0731 21:37:17.900538 20312 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0731 21:37:17.900542 20312 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0731 21:37:17.900545 20312 net.cpp:323] res5a_branch2b needs backward computation.
I0731 21:37:17.900549 20312 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0731 21:37:17.900553 20312 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0731 21:37:17.900557 20312 net.cpp:323] res5a_branch2a needs backward computation.
I0731 21:37:17.900562 20312 net.cpp:323] pool4 needs backward computation.
I0731 21:37:17.900565 20312 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0731 21:37:17.900569 20312 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0731 21:37:17.900573 20312 net.cpp:323] res4a_branch2b needs backward computation.
I0731 21:37:17.900578 20312 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0731 21:37:17.900586 20312 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0731 21:37:17.900590 20312 net.cpp:323] res4a_branch2a needs backward computation.
I0731 21:37:17.900594 20312 net.cpp:323] pool3 needs backward computation.
I0731 21:37:17.900599 20312 net.cpp:323] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I0731 21:37:17.900602 20312 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0731 21:37:17.900606 20312 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0731 21:37:17.900610 20312 net.cpp:323] res3a_branch2b needs backward computation.
I0731 21:37:17.900615 20312 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0731 21:37:17.900619 20312 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0731 21:37:17.900624 20312 net.cpp:323] res3a_branch2a needs backward computation.
I0731 21:37:17.900627 20312 net.cpp:323] pool2 needs backward computation.
I0731 21:37:17.900631 20312 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0731 21:37:17.900635 20312 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0731 21:37:17.900640 20312 net.cpp:323] res2a_branch2b needs backward computation.
I0731 21:37:17.900643 20312 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0731 21:37:17.900647 20312 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0731 21:37:17.900651 20312 net.cpp:323] res2a_branch2a needs backward computation.
I0731 21:37:17.900655 20312 net.cpp:323] pool1 needs backward computation.
I0731 21:37:17.900660 20312 net.cpp:323] conv1b/relu needs backward computation.
I0731 21:37:17.900663 20312 net.cpp:323] conv1b/bn needs backward computation.
I0731 21:37:17.900667 20312 net.cpp:323] conv1b needs backward computation.
I0731 21:37:17.900671 20312 net.cpp:323] conv1a/relu needs backward computation.
I0731 21:37:17.900676 20312 net.cpp:323] conv1a/bn needs backward computation.
I0731 21:37:17.900679 20312 net.cpp:323] conv1a needs backward computation.
I0731 21:37:17.900684 20312 net.cpp:325] data/bias does not need backward computation.
I0731 21:37:17.900689 20312 net.cpp:325] label_data_1_split does not need backward computation.
I0731 21:37:17.900693 20312 net.cpp:325] data does not need backward computation.
I0731 21:37:17.900697 20312 net.cpp:367] This network produces output accuracy/top1
I0731 21:37:17.900702 20312 net.cpp:367] This network produces output accuracy/top5
I0731 21:37:17.900707 20312 net.cpp:367] This network produces output loss
I0731 21:37:17.900765 20312 net.cpp:389] Top memory (TEST) required for data: 318668800 diff: 8
I0731 21:37:17.900770 20312 net.cpp:392] Bottom memory (TEST) required for data: 318668800 diff: 318668800
I0731 21:37:17.900774 20312 net.cpp:395] Shared (in-place) memory (TEST) by data: 210124800 diff: 210124800
I0731 21:37:17.900779 20312 net.cpp:398] Parameters memory (TEST) required for data: 2692608 diff: 2692608
I0731 21:37:17.900781 20312 net.cpp:401] Parameters shared memory (TEST) by data: 0 diff: 0
I0731 21:37:17.900785 20312 net.cpp:407] Network initialization done.
I0731 21:37:17.900904 20312 solver.cpp:56] Solver scaffolding done.
I0731 21:37:17.910991 20312 caffe.cpp:137] Finetuning from training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/l1reg/cityscapes5_jsegnet21v2_iter_32000.caffemodel
I0731 21:37:17.916599 20312 net.cpp:1089] Copying source layer data Type:ImageLabelData #blobs=0
I0731 21:37:17.916632 20312 net.cpp:1089] Copying source layer data/bias Type:Bias #blobs=1
I0731 21:37:17.916683 20312 net.cpp:1089] Copying source layer conv1a Type:Convolution #blobs=2
I0731 21:37:17.916703 20312 net.cpp:1089] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0731 21:37:17.917369 20312 net.cpp:1089] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0731 21:37:17.917377 20312 net.cpp:1089] Copying source layer conv1b Type:Convolution #blobs=2
I0731 21:37:17.917387 20312 net.cpp:1089] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0731 21:37:17.917804 20312 net.cpp:1089] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0731 21:37:17.917821 20312 net.cpp:1089] Copying source layer pool1 Type:Pooling #blobs=0
I0731 21:37:17.917824 20312 net.cpp:1089] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0731 21:37:17.917840 20312 net.cpp:1089] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0731 21:37:17.918282 20312 net.cpp:1089] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0731 21:37:17.918289 20312 net.cpp:1089] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0731 21:37:17.918303 20312 net.cpp:1089] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0731 21:37:17.918730 20312 net.cpp:1089] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0731 21:37:17.918736 20312 net.cpp:1089] Copying source layer pool2 Type:Pooling #blobs=0
I0731 21:37:17.918738 20312 net.cpp:1089] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0731 21:37:17.918776 20312 net.cpp:1089] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0731 21:37:17.919179 20312 net.cpp:1089] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0731 21:37:17.919185 20312 net.cpp:1089] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0731 21:37:17.919209 20312 net.cpp:1089] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0731 21:37:17.919603 20312 net.cpp:1089] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0731 21:37:17.919610 20312 net.cpp:1089] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I0731 21:37:17.919612 20312 net.cpp:1089] Copying source layer pool3 Type:Pooling #blobs=0
I0731 21:37:17.919615 20312 net.cpp:1089] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0731 21:37:17.919734 20312 net.cpp:1089] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0731 21:37:17.920122 20312 net.cpp:1089] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0731 21:37:17.920128 20312 net.cpp:1089] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0731 21:37:17.920200 20312 net.cpp:1089] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0731 21:37:17.920585 20312 net.cpp:1089] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0731 21:37:17.920593 20312 net.cpp:1089] Copying source layer pool4 Type:Pooling #blobs=0
I0731 21:37:17.920594 20312 net.cpp:1089] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0731 21:37:17.921011 20312 net.cpp:1089] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0731 21:37:17.921386 20312 net.cpp:1089] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0731 21:37:17.921392 20312 net.cpp:1089] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0731 21:37:17.921599 20312 net.cpp:1089] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0731 21:37:17.921967 20312 net.cpp:1089] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0731 21:37:17.921974 20312 net.cpp:1089] Copying source layer out5a Type:Convolution #blobs=2
I0731 21:37:17.922036 20312 net.cpp:1089] Copying source layer out5a/bn Type:BatchNorm #blobs=5
I0731 21:37:17.922232 20312 net.cpp:1089] Copying source layer out5a/relu Type:ReLU #blobs=0
I0731 21:37:17.922238 20312 net.cpp:1089] Copying source layer out5a_up2 Type:Deconvolution #blobs=1
I0731 21:37:17.922245 20312 net.cpp:1089] Copying source layer out3a Type:Convolution #blobs=2
I0731 21:37:17.922266 20312 net.cpp:1089] Copying source layer out3a/bn Type:BatchNorm #blobs=5
I0731 21:37:17.922430 20312 net.cpp:1089] Copying source layer out3a/relu Type:ReLU #blobs=0
I0731 21:37:17.922437 20312 net.cpp:1089] Copying source layer out3_out5_combined Type:Eltwise #blobs=0
I0731 21:37:17.922441 20312 net.cpp:1089] Copying source layer ctx_conv1 Type:Convolution #blobs=2
I0731 21:37:17.922466 20312 net.cpp:1089] Copying source layer ctx_conv1/bn Type:BatchNorm #blobs=5
I0731 21:37:17.922647 20312 net.cpp:1089] Copying source layer ctx_conv1/relu Type:ReLU #blobs=0
I0731 21:37:17.922653 20312 net.cpp:1089] Copying source layer ctx_conv2 Type:Convolution #blobs=2
I0731 21:37:17.922684 20312 net.cpp:1089] Copying source layer ctx_conv2/bn Type:BatchNorm #blobs=5
I0731 21:37:17.922888 20312 net.cpp:1089] Copying source layer ctx_conv2/relu Type:ReLU #blobs=0
I0731 21:37:17.922894 20312 net.cpp:1089] Copying source layer ctx_conv3 Type:Convolution #blobs=2
I0731 21:37:17.922914 20312 net.cpp:1089] Copying source layer ctx_conv3/bn Type:BatchNorm #blobs=5
I0731 21:37:17.923091 20312 net.cpp:1089] Copying source layer ctx_conv3/relu Type:ReLU #blobs=0
I0731 21:37:17.923096 20312 net.cpp:1089] Copying source layer ctx_conv4 Type:Convolution #blobs=2
I0731 21:37:17.923117 20312 net.cpp:1089] Copying source layer ctx_conv4/bn Type:BatchNorm #blobs=5
I0731 21:37:17.923295 20312 net.cpp:1089] Copying source layer ctx_conv4/relu Type:ReLU #blobs=0
I0731 21:37:17.923300 20312 net.cpp:1089] Copying source layer ctx_final Type:Convolution #blobs=2
I0731 21:37:17.923310 20312 net.cpp:1089] Copying source layer ctx_final/relu Type:ReLU #blobs=0
I0731 21:37:17.923313 20312 net.cpp:1089] Copying source layer out_deconv_final_up2 Type:Deconvolution #blobs=1
I0731 21:37:17.923318 20312 net.cpp:1089] Copying source layer out_deconv_final_up4 Type:Deconvolution #blobs=1
I0731 21:37:17.923323 20312 net.cpp:1089] Copying source layer out_deconv_final_up8 Type:Deconvolution #blobs=1
I0731 21:37:17.923329 20312 net.cpp:1089] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0731 21:37:17.927064 20312 net.cpp:1089] Copying source layer data Type:ImageLabelData #blobs=0
I0731 21:37:17.927083 20312 net.cpp:1089] Copying source layer data/bias Type:Bias #blobs=1
I0731 21:37:17.927111 20312 net.cpp:1089] Copying source layer conv1a Type:Convolution #blobs=2
I0731 21:37:17.927124 20312 net.cpp:1089] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0731 21:37:17.927645 20312 net.cpp:1089] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0731 21:37:17.927650 20312 net.cpp:1089] Copying source layer conv1b Type:Convolution #blobs=2
I0731 21:37:17.927659 20312 net.cpp:1089] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0731 21:37:17.928040 20312 net.cpp:1089] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0731 21:37:17.928045 20312 net.cpp:1089] Copying source layer pool1 Type:Pooling #blobs=0
I0731 21:37:17.928047 20312 net.cpp:1089] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0731 21:37:17.928063 20312 net.cpp:1089] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0731 21:37:17.928444 20312 net.cpp:1089] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0731 21:37:17.928449 20312 net.cpp:1089] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0731 21:37:17.928460 20312 net.cpp:1089] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0731 21:37:17.928845 20312 net.cpp:1089] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0731 21:37:17.928851 20312 net.cpp:1089] Copying source layer pool2 Type:Pooling #blobs=0
I0731 21:37:17.928853 20312 net.cpp:1089] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0731 21:37:17.928891 20312 net.cpp:1089] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0731 21:37:17.929246 20312 net.cpp:1089] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0731 21:37:17.929251 20312 net.cpp:1089] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0731 21:37:17.929273 20312 net.cpp:1089] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0731 21:37:17.929630 20312 net.cpp:1089] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0731 21:37:17.929636 20312 net.cpp:1089] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I0731 21:37:17.929637 20312 net.cpp:1089] Copying source layer pool3 Type:Pooling #blobs=0
I0731 21:37:17.929639 20312 net.cpp:1089] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0731 21:37:17.929745 20312 net.cpp:1089] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0731 21:37:17.930109 20312 net.cpp:1089] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0731 21:37:17.930114 20312 net.cpp:1089] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0731 21:37:17.930172 20312 net.cpp:1089] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0731 21:37:17.930526 20312 net.cpp:1089] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0731 21:37:17.930531 20312 net.cpp:1089] Copying source layer pool4 Type:Pooling #blobs=0
I0731 21:37:17.930533 20312 net.cpp:1089] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0731 21:37:17.930899 20312 net.cpp:1089] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0731 21:37:17.931255 20312 net.cpp:1089] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0731 21:37:17.931260 20312 net.cpp:1089] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0731 21:37:17.931437 20312 net.cpp:1089] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0731 21:37:17.931790 20312 net.cpp:1089] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0731 21:37:17.931797 20312 net.cpp:1089] Copying source layer out5a Type:Convolution #blobs=2
I0731 21:37:17.931843 20312 net.cpp:1089] Copying source layer out5a/bn Type:BatchNorm #blobs=5
I0731 21:37:17.932006 20312 net.cpp:1089] Copying source layer out5a/relu Type:ReLU #blobs=0
I0731 21:37:17.932010 20312 net.cpp:1089] Copying source layer out5a_up2 Type:Deconvolution #blobs=1
I0731 21:37:17.932016 20312 net.cpp:1089] Copying source layer out3a Type:Convolution #blobs=2
I0731 21:37:17.932034 20312 net.cpp:1089] Copying source layer out3a/bn Type:BatchNorm #blobs=5
I0731 21:37:17.932181 20312 net.cpp:1089] Copying source layer out3a/relu Type:ReLU #blobs=0
I0731 21:37:17.932186 20312 net.cpp:1089] Copying source layer out3_out5_combined Type:Eltwise #blobs=0
I0731 21:37:17.932188 20312 net.cpp:1089] Copying source layer ctx_conv1 Type:Convolution #blobs=2
I0731 21:37:17.932207 20312 net.cpp:1089] Copying source layer ctx_conv1/bn Type:BatchNorm #blobs=5
I0731 21:37:17.932401 20312 net.cpp:1089] Copying source layer ctx_conv1/relu Type:ReLU #blobs=0
I0731 21:37:17.932410 20312 net.cpp:1089] Copying source layer ctx_conv2 Type:Convolution #blobs=2
I0731 21:37:17.932433 20312 net.cpp:1089] Copying source layer ctx_conv2/bn Type:BatchNorm #blobs=5
I0731 21:37:17.932621 20312 net.cpp:1089] Copying source layer ctx_conv2/relu Type:ReLU #blobs=0
I0731 21:37:17.932626 20312 net.cpp:1089] Copying source layer ctx_conv3 Type:Convolution #blobs=2
I0731 21:37:17.932646 20312 net.cpp:1089] Copying source layer ctx_conv3/bn Type:BatchNorm #blobs=5
I0731 21:37:17.932835 20312 net.cpp:1089] Copying source layer ctx_conv3/relu Type:ReLU #blobs=0
I0731 21:37:17.932842 20312 net.cpp:1089] Copying source layer ctx_conv4 Type:Convolution #blobs=2
I0731 21:37:17.932860 20312 net.cpp:1089] Copying source layer ctx_conv4/bn Type:BatchNorm #blobs=5
I0731 21:37:17.933039 20312 net.cpp:1089] Copying source layer ctx_conv4/relu Type:ReLU #blobs=0
I0731 21:37:17.933045 20312 net.cpp:1089] Copying source layer ctx_final Type:Convolution #blobs=2
I0731 21:37:17.933054 20312 net.cpp:1089] Copying source layer ctx_final/relu Type:ReLU #blobs=0
I0731 21:37:17.933058 20312 net.cpp:1089] Copying source layer out_deconv_final_up2 Type:Deconvolution #blobs=1
I0731 21:37:17.933063 20312 net.cpp:1089] Copying source layer out_deconv_final_up4 Type:Deconvolution #blobs=1
I0731 21:37:17.933068 20312 net.cpp:1089] Copying source layer out_deconv_final_up8 Type:Deconvolution #blobs=1
I0731 21:37:17.933073 20312 net.cpp:1089] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0731 21:37:17.933193 20312 parallel.cpp:108] [0 - 0] P2pSync adding callback
I0731 21:37:17.933200 20312 parallel.cpp:108] [1 - 1] P2pSync adding callback
I0731 21:37:17.933204 20312 parallel.cpp:108] [2 - 2] P2pSync adding callback
I0731 21:37:17.933208 20312 parallel.cpp:61] Starting Optimization
I0731 21:37:17.933212 20312 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 21:37:17.933251 20312 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 21:37:17.933269 20312 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 21:37:17.933954 20406 device_alternate.hpp:116] NVML initialized on thread 136079995094784
I0731 21:37:17.948607 20406 common.cpp:583] NVML succeeded to set CPU affinity on device 0
I0731 21:37:17.948662 20407 device_alternate.hpp:116] NVML initialized on thread 136079986702080
I0731 21:37:17.949589 20407 common.cpp:583] NVML succeeded to set CPU affinity on device 1
I0731 21:37:17.949641 20408 device_alternate.hpp:116] NVML initialized on thread 136079978309376
I0731 21:37:17.950461 20408 common.cpp:583] NVML succeeded to set CPU affinity on device 2
I0731 21:37:17.953948 20407 solver.cpp:42] Solver data type: FLOAT
W0731 21:37:17.954510 20407 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0731 21:37:17.954638 20407 net.cpp:104] Using FLOAT as default forward math type
I0731 21:37:17.954643 20407 net.cpp:110] Using FLOAT as default backward math type
I0731 21:37:17.954674 20407 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0731 21:37:17.954679 20407 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 21:37:17.958235 20408 solver.cpp:42] Solver data type: FLOAT
W0731 21:37:17.958777 20408 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0731 21:37:17.958886 20408 net.cpp:104] Using FLOAT as default forward math type
I0731 21:37:17.958892 20408 net.cpp:110] Using FLOAT as default backward math type
I0731 21:37:17.958921 20408 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0731 21:37:17.958928 20408 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 21:37:17.958966 20409 db_lmdb.cpp:35] Opened lmdb data/train-image-lmdb
I0731 21:37:17.961843 20407 data_layer.cpp:184] [1] ReshapePrefetch 6, 3, 640, 640
I0731 21:37:17.961854 20410 db_lmdb.cpp:35] Opened lmdb data/train-image-lmdb
I0731 21:37:17.961968 20407 data_layer.cpp:208] [1] Output data size: 6, 3, 640, 640
I0731 21:37:17.961977 20407 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 21:37:17.962038 20407 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0731 21:37:17.962050 20407 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 21:37:17.962954 20411 data_layer.cpp:97] [1] Parser threads: 1
I0731 21:37:17.962990 20411 data_layer.cpp:99] [1] Transformer threads: 1
I0731 21:37:17.969084 20412 db_lmdb.cpp:35] Opened lmdb data/train-label-lmdb
I0731 21:37:17.971240 20408 data_layer.cpp:184] [2] ReshapePrefetch 6, 3, 640, 640
I0731 21:37:17.971310 20407 data_layer.cpp:184] [1] ReshapePrefetch 6, 1, 640, 640
I0731 21:37:17.971328 20408 data_layer.cpp:208] [2] Output data size: 6, 3, 640, 640
I0731 21:37:17.971339 20408 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 21:37:17.971369 20407 data_layer.cpp:208] [1] Output data size: 6, 1, 640, 640
I0731 21:37:17.971375 20407 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 21:37:17.971698 20408 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0731 21:37:17.971715 20408 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 21:37:17.972748 20413 data_layer.cpp:97] [1] Parser threads: 1
I0731 21:37:17.972779 20413 data_layer.cpp:99] [1] Transformer threads: 1
I0731 21:37:17.975131 20414 data_layer.cpp:97] [2] Parser threads: 1
I0731 21:37:17.975175 20414 data_layer.cpp:99] [2] Transformer threads: 1
I0731 21:37:17.982218 20415 db_lmdb.cpp:35] Opened lmdb data/train-label-lmdb
I0731 21:37:17.986227 20408 data_layer.cpp:184] [2] ReshapePrefetch 6, 1, 640, 640
I0731 21:37:17.986925 20408 data_layer.cpp:208] [2] Output data size: 6, 1, 640, 640
I0731 21:37:17.986946 20408 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 21:37:17.992627 20413 blocking_queue.cpp:40] Waiting for datum
I0731 21:37:17.993294 20416 data_layer.cpp:97] [2] Parser threads: 1
I0731 21:37:17.993372 20416 data_layer.cpp:99] [2] Transformer threads: 1
I0731 21:37:18.519807 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 7.99G, req 0G)
I0731 21:37:18.535709 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 7.99G, req 0G)
I0731 21:37:18.572796 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.83G, req 0G)
I0731 21:37:18.590581 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.83G, req 0G)
I0731 21:37:18.618458 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.7G, req 0G)
I0731 21:37:18.640271 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.7G, req 0G)
I0731 21:37:18.655947 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.62G, req 0G)
I0731 21:37:18.675751 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.62G, req 0G)
I0731 21:37:18.681535 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 7.56G, req 0G)
I0731 21:37:18.696717 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.52G, req 0G)
I0731 21:37:18.700321 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 7.56G, req 0G)
I0731 21:37:18.713639 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.52G, req 0G)
I0731 21:37:18.726083 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.48G, req 0G)
I0731 21:37:18.738119 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.46G, req 0G)
I0731 21:37:18.742321 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.48G, req 0G)
I0731 21:37:18.753047 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.46G, req 0G)
I0731 21:37:18.803251 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 7.39G, req 0G)
I0731 21:37:18.814246 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 7.39G, req 0G)
I0731 21:37:18.824282 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 7.34G, req 0G)
I0731 21:37:18.833729 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 7.34G, req 0G)
I0731 21:37:18.845731 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 7.32G, req 0G)
I0731 21:37:18.848871 20407 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/sparse/test.prototxt
W0731 21:37:18.848958 20407 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0731 21:37:18.849095 20407 net.cpp:104] Using FLOAT as default forward math type
I0731 21:37:18.849100 20407 net.cpp:110] Using FLOAT as default backward math type
I0731 21:37:18.849128 20407 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0731 21:37:18.849135 20407 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 21:37:18.849853 20434 db_lmdb.cpp:35] Opened lmdb data/val-image-lmdb
I0731 21:37:18.851527 20407 data_layer.cpp:184] (1) ReshapePrefetch 2, 3, 640, 640
I0731 21:37:18.851593 20407 data_layer.cpp:208] (1) Output data size: 2, 3, 640, 640
I0731 21:37:18.851600 20407 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 21:37:18.851647 20407 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0731 21:37:18.851655 20407 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 21:37:18.852494 20435 data_layer.cpp:97] (1) Parser threads: 1
I0731 21:37:18.852506 20435 data_layer.cpp:99] (1) Transformer threads: 1
I0731 21:37:18.854698 20436 db_lmdb.cpp:35] Opened lmdb data/val-label-lmdb
I0731 21:37:18.856030 20407 data_layer.cpp:184] (1) ReshapePrefetch 2, 1, 640, 640
I0731 21:37:18.856045 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 7.32G, req 0G)
I0731 21:37:18.856202 20407 data_layer.cpp:208] (1) Output data size: 2, 1, 640, 640
I0731 21:37:18.856211 20407 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 21:37:18.857125 20437 data_layer.cpp:97] (1) Parser threads: 1
I0731 21:37:18.857136 20437 data_layer.cpp:99] (1) Transformer threads: 1
I0731 21:37:18.866287 20408 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/sparse/test.prototxt
W0731 21:37:18.866366 20408 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0731 21:37:18.866492 20408 net.cpp:104] Using FLOAT as default forward math type
I0731 21:37:18.866497 20408 net.cpp:110] Using FLOAT as default backward math type
I0731 21:37:18.866523 20408 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0731 21:37:18.866528 20408 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 21:37:18.867341 20438 db_lmdb.cpp:35] Opened lmdb data/val-image-lmdb
I0731 21:37:18.868343 20407 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.18G, req 0G)
I0731 21:37:18.868980 20408 data_layer.cpp:184] (2) ReshapePrefetch 2, 3, 640, 640
I0731 21:37:18.869122 20408 data_layer.cpp:208] (2) Output data size: 2, 3, 640, 640
I0731 21:37:18.869135 20408 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 21:37:18.869187 20408 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0731 21:37:18.869201 20408 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 21:37:18.869972 20439 data_layer.cpp:97] (2) Parser threads: 1
I0731 21:37:18.869983 20439 data_layer.cpp:99] (2) Transformer threads: 1
I0731 21:37:18.872686 20440 db_lmdb.cpp:35] Opened lmdb data/val-label-lmdb
I0731 21:37:18.874047 20408 data_layer.cpp:184] (2) ReshapePrefetch 2, 1, 640, 640
I0731 21:37:18.874261 20408 data_layer.cpp:208] (2) Output data size: 2, 1, 640, 640
I0731 21:37:18.874272 20408 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 21:37:18.875777 20441 data_layer.cpp:97] (2) Parser threads: 1
I0731 21:37:18.875792 20441 data_layer.cpp:99] (2) Transformer threads: 1
I0731 21:37:18.886585 20408 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.18G, req 0G)
I0731 21:37:18.893762 20407 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.15G, req 0G)
I0731 21:37:18.904338 20408 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.15G, req 0G)
I0731 21:37:18.904808 20407 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.13G, req 0G)
I0731 21:37:18.914539 20407 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 1  (limit 7.12G, req 0G)
I0731 21:37:18.916216 20408 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.13G, req 0G)
I0731 21:37:18.922758 20407 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.1G, req 0G)
I0731 21:37:18.925755 20408 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.12G, req 0G)
I0731 21:37:18.929806 20407 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.09G, req 0G)
I0731 21:37:18.935590 20408 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.1G, req 0G)
I0731 21:37:18.942962 20408 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.09G, req 0G)
I0731 21:37:18.944257 20407 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.09G, req 0G)
I0731 21:37:18.952741 20407 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.08G, req 0G)
I0731 21:37:18.957046 20408 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.09G, req 0G)
I0731 21:37:18.967164 20408 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.08G, req 0G)
I0731 21:37:19.004966 20407 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 7.07G, req 0G)
I0731 21:37:19.011307 20407 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 7.06G, req 0G)
I0731 21:37:19.025051 20408 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 7.07G, req 0G)
I0731 21:37:19.025792 20407 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 7.05G, req 0G)
I0731 21:37:19.029878 20407 solver.cpp:56] Solver scaffolding done.
I0731 21:37:19.032649 20408 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 7.06G, req 0G)
I0731 21:37:19.049891 20408 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 7.05G, req 0G)
I0731 21:37:19.052213 20408 solver.cpp:56] Solver scaffolding done.
I0731 21:37:19.112440 20406 parallel.cpp:164] [0 - 0] P2pSync adding callback
I0731 21:37:19.112440 20407 parallel.cpp:164] [1 - 1] P2pSync adding callback
I0731 21:37:19.112440 20408 parallel.cpp:164] [2 - 2] P2pSync adding callback
I0731 21:37:19.320559 20406 net.cpp:2244] All zero weights of convolution layers are frozen
I0731 21:37:19.336573 20408 solver.cpp:479] Solving jsegnet21v2_train
I0731 21:37:19.336591 20408 solver.cpp:480] Learning Rate Policy: multistep
I0731 21:37:19.338251 20407 solver.cpp:479] Solving jsegnet21v2_train
I0731 21:37:19.338260 20407 solver.cpp:480] Learning Rate Policy: multistep
I0731 21:37:19.338660 20406 solver.cpp:479] Solving jsegnet21v2_train
I0731 21:37:19.338670 20406 solver.cpp:480] Learning Rate Policy: multistep
I0731 21:37:19.352705 20406 solver.cpp:268] Starting Optimization on GPU 0
I0731 21:37:19.352705 20408 solver.cpp:268] Starting Optimization on GPU 2
I0731 21:37:19.352705 20407 solver.cpp:268] Starting Optimization on GPU 1
I0731 21:37:19.353006 20406 solver.cpp:550] Iteration 0, Testing net (#0)
I0731 21:37:19.353034 20460 device_alternate.hpp:116] NVML initialized on thread 128079826835200
I0731 21:37:19.353067 20460 common.cpp:583] NVML succeeded to set CPU affinity on device 2
I0731 21:37:19.353080 20459 device_alternate.hpp:116] NVML initialized on thread 128079810049792
I0731 21:37:19.353091 20459 common.cpp:583] NVML succeeded to set CPU affinity on device 1
I0731 21:37:19.353822 20461 device_alternate.hpp:116] NVML initialized on thread 128079818442496
I0731 21:37:19.353834 20461 common.cpp:583] NVML succeeded to set CPU affinity on device 0
I0731 21:37:19.369244 20408 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 6.95G, req 0G)
I0731 21:37:19.372304 20407 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 6.95G, req 0G)
I0731 21:37:19.390118 20408 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 6.9G, req 0G)
I0731 21:37:19.390686 20407 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 6.9G, req 0G)
I0731 21:37:19.406363 20406 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1a' with space 0.01G/1 1  (limit 6.88G, req 0G)
I0731 21:37:19.408455 20408 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 6.83G, req 0G)
I0731 21:37:19.408644 20407 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 6.83G, req 0G)
I0731 21:37:19.419778 20408 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 6.8G, req 0G)
I0731 21:37:19.420095 20407 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 6.8G, req 0G)
I0731 21:37:19.427031 20406 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 6.82G, req 0G)
I0731 21:37:19.429046 20408 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 6.77G, req 0G)
I0731 21:37:19.430095 20407 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 6.77G, req 0G)
I0731 21:37:19.435477 20408 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 6.75G, req 0G)
I0731 21:37:19.436337 20407 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 6.75G, req 0G)
I0731 21:37:19.442193 20406 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 6.75G, req 0G)
I0731 21:37:19.444633 20408 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.74G, req 0G)
I0731 21:37:19.446466 20407 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.74G, req 0G)
I0731 21:37:19.449489 20408 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.73G, req 0G)
I0731 21:37:19.450136 20407 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.73G, req 0G)
I0731 21:37:19.452047 20406 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 6.73G, req 0G)
I0731 21:37:19.462432 20406 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 6.69G, req 0G)
I0731 21:37:19.470166 20406 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 6.67G, req 0G)
I0731 21:37:19.477594 20406 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.66G, req 0G)
I0731 21:37:19.479645 20407 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.58G, req 0G)
I0731 21:37:19.480876 20408 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.58G, req 0G)
I0731 21:37:19.483157 20406 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.65G, req 0G)
I0731 21:37:19.485607 20407 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.57G, req 0G)
I0731 21:37:19.486870 20408 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.57G, req 0G)
I0731 21:37:19.513156 20407 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.47G, req 0G)
I0731 21:37:19.513636 20406 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.5G, req 0G)
I0731 21:37:19.514708 20408 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.47G, req 0G)
I0731 21:37:19.520865 20406 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.49G, req 0G)
I0731 21:37:19.550586 20406 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.39G, req 0G)
I0731 21:37:19.635999 20406 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.900861
I0731 21:37:19.636036 20406 solver.cpp:635]     Test net output #1: accuracy/top5 = 1
I0731 21:37:19.636042 20406 solver.cpp:635]     Test net output #2: loss = 0.266714 (* 1 = 0.266714 loss)
I0731 21:37:19.636046 20406 solver.cpp:295] [MultiGPU] Initial Test completed
I0731 21:37:19.730851 20406 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 6.19G, req 0G)
I0731 21:37:19.735430 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 6.27G, req 0G)
I0731 21:37:19.736721 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 6.27G, req 0G)
I0731 21:37:19.784109 20406 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 6.03G, req 0G)
I0731 21:37:19.789625 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 6.11G, req 0G)
I0731 21:37:19.796519 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 6.11G, req 0G)
I0731 21:37:19.836803 20406 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 5.85G, req 0G)
I0731 21:37:19.843951 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 5.93G, req 0G)
I0731 21:37:19.847156 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 5.93G, req 0G)
I0731 21:37:19.861817 20406 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 5.77G, req 0G)
I0731 21:37:19.870528 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 5.85G, req 0G)
I0731 21:37:19.872687 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 5.85G, req 0G)
I0731 21:37:19.885849 20406 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 5.68G, req 0G)
I0731 21:37:19.896317 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 5.76G, req 0G)
I0731 21:37:19.899260 20406 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 5.64G, req 0G)
I0731 21:37:19.900467 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 5.76G, req 0G)
I0731 21:37:19.911617 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 5.72G, req 0G)
I0731 21:37:19.917472 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 5.72G, req 0G)
I0731 21:37:19.923041 20406 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 5.61G, req 0G)
I0731 21:37:19.931219 20406 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 5.59G, req 0G)
I0731 21:37:19.935911 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 5.69G, req 0G)
I0731 21:37:19.949374 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 5.69G, req 0G)
I0731 21:37:19.952692 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 5.67G, req 0G)
I0731 21:37:19.959426 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 5.67G, req 0G)
I0731 21:37:19.980346 20406 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 5.32G, req 0G)
I0731 21:37:19.994748 20406 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 5.3G, req 0G)
I0731 21:37:19.999917 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 5.4G, req 0G)
I0731 21:37:20.005347 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 5.4G, req 0G)
I0731 21:37:20.015234 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 5.38G, req 0G)
I0731 21:37:20.020908 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 5.38G, req 0G)
I0731 21:37:20.023792 20406 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 5.14G, req 0G)
I0731 21:37:20.049309 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 5.23G, req 0G)
I0731 21:37:20.055439 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 5.23G, req 0G)
I0731 21:37:20.268901 20406 solver.cpp:358] Iteration 0 (0.6328 s), loss = 0.0844328
I0731 21:37:20.268951 20406 solver.cpp:375]     Train net output #0: loss = 0.0844328 (* 1 = 0.0844328 loss)
I0731 21:37:20.268963 20406 sgd_solver.cpp:136] Iteration 0, lr = 1e-05, m = 0.9
I0731 21:37:20.457412 20406 solver.cpp:358] Iteration 1 (0.188507 s), loss = 0.105994
I0731 21:37:20.457438 20406 solver.cpp:375]     Train net output #0: loss = 0.105994 (* 1 = 0.105994 loss)
I0731 21:37:20.547955 20406 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1a' with space 1.29G/1 1 0 3  (limit 2.98G, req 0G)
I0731 21:37:20.563175 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1a' with space 1.29G/1 1 0 3  (limit 3.06G, req 0G)
I0731 21:37:20.563724 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1a' with space 1.29G/1 1 0 3  (limit 3.06G, req 0G)
I0731 21:37:20.613157 20406 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1b' with space 2.57G/2 6 4 3  (limit 1.69G, req 0G)
I0731 21:37:20.638814 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1b' with space 2.57G/2 6 4 3  (limit 1.78G, req 0G)
I0731 21:37:20.641772 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1b' with space 2.57G/2 6 4 3  (limit 1.78G, req 0G)
I0731 21:37:20.745617 20406 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 2.57G/1 1 4 3  (limit 1.69G, req 0G)
I0731 21:37:20.783813 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 2.57G/1 6 4 3  (limit 1.78G, req 0G)
I0731 21:37:20.784180 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 2.57G/1 6 4 3  (limit 1.78G, req 0G)
I0731 21:37:20.794327 20406 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 2.57G/2 6 4 3  (limit 1.69G, req 0G)
I0731 21:37:20.835959 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 2.57G/2 6 4 3  (limit 1.78G, req 0G)
I0731 21:37:20.838064 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 2.57G/2 6 4 3  (limit 1.78G, req 0G)
I0731 21:37:20.890305 20406 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 2.57G/1 6 4 5  (limit 1.69G, req 0.07G)
I0731 21:37:20.913028 20406 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 2.57G/2 6 4 3  (limit 1.69G, req 0.07G)
I0731 21:37:20.936583 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 2.57G/1 6 4 5  (limit 1.78G, req 0.07G)
I0731 21:37:20.940023 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 2.57G/1 6 4 5  (limit 1.78G, req 0.07G)
I0731 21:37:20.960263 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 2.57G/2 6 4 3  (limit 1.78G, req 0.07G)
I0731 21:37:20.963724 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 2.57G/2 6 4 3  (limit 1.78G, req 0.07G)
I0731 21:37:20.978600 20406 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 2.57G/1 6 4 5  (limit 1.69G, req 0.07G)
I0731 21:37:20.996453 20406 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 2.57G/2 1 4 5  (limit 1.69G, req 0.07G)
I0731 21:37:21.032729 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 2.57G/1 6 4 5  (limit 1.78G, req 0.07G)
I0731 21:37:21.036015 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 2.57G/1 6 4 5  (limit 1.78G, req 0.07G)
I0731 21:37:21.048055 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 2.57G/2 6 4 3  (limit 1.78G, req 0.07G)
I0731 21:37:21.051266 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 2.57G/2 6 4 3  (limit 1.78G, req 0.07G)
I0731 21:37:21.053794 20406 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'out3a' with space 2.57G/2 6 4 3  (limit 1.69G, req 0.07G)
I0731 21:37:21.103407 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'out3a' with space 2.57G/2 6 4 3  (limit 1.78G, req 0.07G)
I0731 21:37:21.106115 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'out3a' with space 2.57G/2 6 4 3  (limit 1.78G, req 0.07G)
I0731 21:37:21.107122 20406 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 2.57G/1 6 4 3  (limit 1.69G, req 0.07G)
I0731 21:37:21.133007 20406 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 2.57G/1 6 1 5  (limit 1.69G, req 0.07G)
I0731 21:37:21.160446 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 2.57G/1 6 4 3  (limit 1.78G, req 0.07G)
I0731 21:37:21.162037 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 2.57G/1 6 4 3  (limit 1.78G, req 0.07G)
I0731 21:37:21.188170 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 2.57G/1 6 1 5  (limit 1.78G, req 0.07G)
I0731 21:37:21.190227 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 2.57G/1 6 1 5  (limit 1.78G, req 0.07G)
I0731 21:37:21.319000 20406 solver.cpp:358] Iteration 2 (0.861559 s), loss = 0.0655569
I0731 21:37:21.319022 20406 solver.cpp:375]     Train net output #0: loss = 0.0655569 (* 1 = 0.0655569 loss)
I0731 21:37:21.319627 20407 cudnn_conv_layer.cpp:292] [1] Layer 'conv1a' reallocating workspace: 2.57G -> 0.14G
I0731 21:37:21.319633 20408 cudnn_conv_layer.cpp:292] [2] Layer 'conv1a' reallocating workspace: 2.57G -> 0.14G
I0731 21:37:21.319653 20406 cudnn_conv_layer.cpp:292] [0] Layer 'conv1a' reallocating workspace: 2.57G -> 0.14G
I0731 21:37:39.563769 20406 solver.cpp:353] Iteration 100 (5.37155 iter/s, 18.2443s/98 iter), loss = 0.0701501
I0731 21:37:39.563796 20406 solver.cpp:375]     Train net output #0: loss = 0.0701501 (* 1 = 0.0701501 loss)
I0731 21:37:39.563802 20406 sgd_solver.cpp:136] Iteration 100, lr = 1e-05, m = 0.9
I0731 21:37:51.142010 20412 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 21:37:58.234972 20406 solver.cpp:353] Iteration 200 (5.35599 iter/s, 18.6707s/100 iter), loss = 0.0988953
I0731 21:37:58.234998 20406 solver.cpp:375]     Train net output #0: loss = 0.0988953 (* 1 = 0.0988953 loss)
I0731 21:37:58.235003 20406 sgd_solver.cpp:136] Iteration 200, lr = 1e-05, m = 0.9
I0731 21:38:16.678007 20406 solver.cpp:353] Iteration 300 (5.42225 iter/s, 18.4425s/100 iter), loss = 0.0645709
I0731 21:38:16.678032 20406 solver.cpp:375]     Train net output #0: loss = 0.0645709 (* 1 = 0.0645709 loss)
I0731 21:38:16.678037 20406 sgd_solver.cpp:136] Iteration 300, lr = 1e-05, m = 0.9
I0731 21:38:21.708425 20346 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 21:38:35.196321 20406 solver.cpp:353] Iteration 400 (5.40021 iter/s, 18.5178s/100 iter), loss = 0.0785404
I0731 21:38:35.196346 20406 solver.cpp:375]     Train net output #0: loss = 0.0785404 (* 1 = 0.0785404 loss)
I0731 21:38:35.196350 20406 sgd_solver.cpp:136] Iteration 400, lr = 1e-05, m = 0.9
I0731 21:38:53.810977 20406 solver.cpp:353] Iteration 500 (5.37226 iter/s, 18.6141s/100 iter), loss = 0.0787777
I0731 21:38:53.811064 20406 solver.cpp:375]     Train net output #0: loss = 0.0787776 (* 1 = 0.0787776 loss)
I0731 21:38:53.811072 20406 sgd_solver.cpp:136] Iteration 500, lr = 1e-05, m = 0.9
I0731 21:39:12.550768 20406 solver.cpp:353] Iteration 600 (5.33639 iter/s, 18.7393s/100 iter), loss = 0.0653573
I0731 21:39:12.550791 20406 solver.cpp:375]     Train net output #0: loss = 0.0653572 (* 1 = 0.0653572 loss)
I0731 21:39:12.550796 20406 sgd_solver.cpp:136] Iteration 600, lr = 1e-05, m = 0.9
I0731 21:39:23.241492 20415 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 21:39:31.063150 20406 solver.cpp:353] Iteration 700 (5.40194 iter/s, 18.5119s/100 iter), loss = 0.0634848
I0731 21:39:31.063230 20406 solver.cpp:375]     Train net output #0: loss = 0.0634848 (* 1 = 0.0634848 loss)
I0731 21:39:31.063237 20406 sgd_solver.cpp:136] Iteration 700, lr = 1e-05, m = 0.9
I0731 21:39:49.444386 20406 solver.cpp:353] Iteration 800 (5.44048 iter/s, 18.3807s/100 iter), loss = 0.0783888
I0731 21:39:49.444411 20406 solver.cpp:375]     Train net output #0: loss = 0.0783888 (* 1 = 0.0783888 loss)
I0731 21:39:49.444416 20406 sgd_solver.cpp:136] Iteration 800, lr = 1e-05, m = 0.9
I0731 21:40:08.108201 20406 solver.cpp:353] Iteration 900 (5.35811 iter/s, 18.6633s/100 iter), loss = 0.0674016
I0731 21:40:08.108286 20406 solver.cpp:375]     Train net output #0: loss = 0.0674016 (* 1 = 0.0674016 loss)
I0731 21:40:08.108294 20406 sgd_solver.cpp:136] Iteration 900, lr = 1e-05, m = 0.9
I0731 21:40:24.653353 20412 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 21:40:26.651243 20406 solver.cpp:404] Sparsity after update:
I0731 21:40:26.679087 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 21:40:26.679116 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 21:40:26.679136 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 21:40:26.679139 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 21:40:26.679142 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 21:40:26.679144 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 21:40:26.679149 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 21:40:26.679162 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 21:40:26.679164 20406 net.cpp:2270] out3a_param_0(0) 
I0731 21:40:26.679167 20406 net.cpp:2270] out5a_param_0(0) 
I0731 21:40:26.679169 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 21:40:26.679173 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 21:40:26.679178 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 21:40:26.679184 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 21:40:26.679190 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 21:40:26.679194 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 21:40:26.679198 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 21:40:26.679201 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 21:40:26.679205 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 21:40:26.848171 20406 solver.cpp:353] Iteration 1000 (5.33634 iter/s, 18.7395s/100 iter), loss = 0.0937093
I0731 21:40:26.848196 20406 solver.cpp:375]     Train net output #0: loss = 0.0937093 (* 1 = 0.0937093 loss)
I0731 21:40:26.848202 20406 sgd_solver.cpp:136] Iteration 1000, lr = 1e-05, m = 0.9
I0731 21:40:45.409927 20406 solver.cpp:353] Iteration 1100 (5.38757 iter/s, 18.5612s/100 iter), loss = 0.0610806
I0731 21:40:45.409979 20406 solver.cpp:375]     Train net output #0: loss = 0.0610805 (* 1 = 0.0610805 loss)
I0731 21:40:45.409984 20406 sgd_solver.cpp:136] Iteration 1100, lr = 1e-05, m = 0.9
I0731 21:40:55.259168 20346 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 21:41:03.978263 20406 solver.cpp:353] Iteration 1200 (5.38566 iter/s, 18.5678s/100 iter), loss = 0.0726554
I0731 21:41:03.978288 20406 solver.cpp:375]     Train net output #0: loss = 0.0726554 (* 1 = 0.0726554 loss)
I0731 21:41:03.978292 20406 sgd_solver.cpp:136] Iteration 1200, lr = 1e-05, m = 0.9
I0731 21:41:22.482134 20406 solver.cpp:353] Iteration 1300 (5.40442 iter/s, 18.5034s/100 iter), loss = 0.0864815
I0731 21:41:22.482218 20406 solver.cpp:375]     Train net output #0: loss = 0.0864814 (* 1 = 0.0864814 loss)
I0731 21:41:22.482226 20406 sgd_solver.cpp:136] Iteration 1300, lr = 1e-05, m = 0.9
I0731 21:41:41.052461 20406 solver.cpp:353] Iteration 1400 (5.38508 iter/s, 18.5698s/100 iter), loss = 0.106898
I0731 21:41:41.052484 20406 solver.cpp:375]     Train net output #0: loss = 0.106898 (* 1 = 0.106898 loss)
I0731 21:41:41.052489 20406 sgd_solver.cpp:136] Iteration 1400, lr = 1e-05, m = 0.9
I0731 21:41:56.657925 20409 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 21:41:59.542181 20406 solver.cpp:353] Iteration 1500 (5.40856 iter/s, 18.4892s/100 iter), loss = 0.0855621
I0731 21:41:59.542207 20406 solver.cpp:375]     Train net output #0: loss = 0.0855621 (* 1 = 0.0855621 loss)
I0731 21:41:59.542212 20406 sgd_solver.cpp:136] Iteration 1500, lr = 1e-05, m = 0.9
I0731 21:42:18.021134 20406 solver.cpp:353] Iteration 1600 (5.41171 iter/s, 18.4784s/100 iter), loss = 0.0664962
I0731 21:42:18.021159 20406 solver.cpp:375]     Train net output #0: loss = 0.0664961 (* 1 = 0.0664961 loss)
I0731 21:42:18.021163 20406 sgd_solver.cpp:136] Iteration 1600, lr = 1e-05, m = 0.9
I0731 21:42:36.526711 20406 solver.cpp:353] Iteration 1700 (5.40392 iter/s, 18.5051s/100 iter), loss = 0.0906552
I0731 21:42:36.526770 20406 solver.cpp:375]     Train net output #0: loss = 0.0906552 (* 1 = 0.0906552 loss)
I0731 21:42:36.526777 20406 sgd_solver.cpp:136] Iteration 1700, lr = 1e-05, m = 0.9
I0731 21:42:55.183761 20406 solver.cpp:353] Iteration 1800 (5.36005 iter/s, 18.6565s/100 iter), loss = 0.0591703
I0731 21:42:55.183789 20406 solver.cpp:375]     Train net output #0: loss = 0.0591702 (* 1 = 0.0591702 loss)
I0731 21:42:55.183795 20406 sgd_solver.cpp:136] Iteration 1800, lr = 1e-05, m = 0.9
I0731 21:42:57.993700 20364 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 21:43:13.772389 20406 solver.cpp:353] Iteration 1900 (5.37978 iter/s, 18.5881s/100 iter), loss = 0.0506905
I0731 21:43:13.772470 20406 solver.cpp:375]     Train net output #0: loss = 0.0506904 (* 1 = 0.0506904 loss)
I0731 21:43:13.772475 20406 sgd_solver.cpp:136] Iteration 1900, lr = 1e-05, m = 0.9
I0731 21:43:32.105382 20406 solver.cpp:404] Sparsity after update:
I0731 21:43:32.120806 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 21:43:32.120838 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 21:43:32.120847 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 21:43:32.120851 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 21:43:32.120856 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 21:43:32.120859 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 21:43:32.120862 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 21:43:32.120867 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 21:43:32.120869 20406 net.cpp:2270] out3a_param_0(0) 
I0731 21:43:32.120873 20406 net.cpp:2270] out5a_param_0(0) 
I0731 21:43:32.120882 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 21:43:32.120885 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 21:43:32.120888 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 21:43:32.120893 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 21:43:32.120895 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 21:43:32.120899 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 21:43:32.120903 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 21:43:32.120906 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 21:43:32.120909 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 21:43:32.120920 20406 solver.cpp:550] Iteration 2000, Testing net (#0)
I0731 21:43:33.387984 20406 blocking_queue.cpp:40] Data layer prefetch queue empty
I0731 21:43:39.108537 20404 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 21:43:43.264416 20406 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.952441
I0731 21:43:43.264441 20406 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999476
I0731 21:43:43.264446 20406 solver.cpp:635]     Test net output #2: loss = 0.189906 (* 1 = 0.189906 loss)
I0731 21:43:43.264533 20406 solver.cpp:305] [MultiGPU] Tests completed in 11.1433s
I0731 21:43:43.461644 20406 solver.cpp:353] Iteration 2000 (3.36831 iter/s, 29.6884s/100 iter), loss = 0.0578856
I0731 21:43:43.461671 20406 solver.cpp:375]     Train net output #0: loss = 0.0578856 (* 1 = 0.0578856 loss)
I0731 21:43:43.461679 20406 sgd_solver.cpp:136] Iteration 2000, lr = 1e-05, m = 0.9
I0731 21:44:02.015105 20406 solver.cpp:353] Iteration 2100 (5.38998 iter/s, 18.5529s/100 iter), loss = 0.067486
I0731 21:44:02.015177 20406 solver.cpp:375]     Train net output #0: loss = 0.067486 (* 1 = 0.067486 loss)
I0731 21:44:02.015182 20406 sgd_solver.cpp:136] Iteration 2100, lr = 1e-05, m = 0.9
I0731 21:44:10.427899 20409 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 21:44:20.628748 20406 solver.cpp:353] Iteration 2200 (5.37255 iter/s, 18.6131s/100 iter), loss = 0.120577
I0731 21:44:20.628773 20406 solver.cpp:375]     Train net output #0: loss = 0.120577 (* 1 = 0.120577 loss)
I0731 21:44:20.628777 20406 sgd_solver.cpp:136] Iteration 2200, lr = 1e-05, m = 0.9
I0731 21:44:39.139827 20406 solver.cpp:353] Iteration 2300 (5.40232 iter/s, 18.5106s/100 iter), loss = 0.0680242
I0731 21:44:39.148630 20406 solver.cpp:375]     Train net output #0: loss = 0.0680242 (* 1 = 0.0680242 loss)
I0731 21:44:39.148811 20406 sgd_solver.cpp:136] Iteration 2300, lr = 1e-05, m = 0.9
I0731 21:44:57.724230 20406 solver.cpp:353] Iteration 2400 (5.381 iter/s, 18.5839s/100 iter), loss = 0.0602255
I0731 21:44:57.724263 20406 solver.cpp:375]     Train net output #0: loss = 0.0602255 (* 1 = 0.0602255 loss)
I0731 21:44:57.724268 20406 sgd_solver.cpp:136] Iteration 2400, lr = 1e-05, m = 0.9
I0731 21:45:11.780216 20409 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 21:45:16.209568 20406 solver.cpp:353] Iteration 2500 (5.40984 iter/s, 18.4848s/100 iter), loss = 0.0849564
I0731 21:45:16.209592 20406 solver.cpp:375]     Train net output #0: loss = 0.0849564 (* 1 = 0.0849564 loss)
I0731 21:45:16.209596 20406 sgd_solver.cpp:136] Iteration 2500, lr = 1e-05, m = 0.9
I0731 21:45:34.908954 20406 solver.cpp:353] Iteration 2600 (5.34792 iter/s, 18.6989s/100 iter), loss = 0.0769499
I0731 21:45:34.908983 20406 solver.cpp:375]     Train net output #0: loss = 0.0769499 (* 1 = 0.0769499 loss)
I0731 21:45:34.908987 20406 sgd_solver.cpp:136] Iteration 2600, lr = 1e-05, m = 0.9
I0731 21:45:53.592036 20406 solver.cpp:353] Iteration 2700 (5.35258 iter/s, 18.6826s/100 iter), loss = 0.0661474
I0731 21:45:53.592090 20406 solver.cpp:375]     Train net output #0: loss = 0.0661474 (* 1 = 0.0661474 loss)
I0731 21:45:53.592097 20406 sgd_solver.cpp:136] Iteration 2700, lr = 1e-05, m = 0.9
I0731 21:46:12.182863 20406 solver.cpp:353] Iteration 2800 (5.37915 iter/s, 18.5903s/100 iter), loss = 0.0604884
I0731 21:46:12.182886 20406 solver.cpp:375]     Train net output #0: loss = 0.0604884 (* 1 = 0.0604884 loss)
I0731 21:46:12.182890 20406 sgd_solver.cpp:136] Iteration 2800, lr = 1e-05, m = 0.9
I0731 21:46:13.286644 20364 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 21:46:30.717608 20406 solver.cpp:353] Iteration 2900 (5.39542 iter/s, 18.5342s/100 iter), loss = 0.0828159
I0731 21:46:30.717663 20406 solver.cpp:375]     Train net output #0: loss = 0.0828159 (* 1 = 0.0828159 loss)
I0731 21:46:30.717667 20406 sgd_solver.cpp:136] Iteration 2900, lr = 1e-05, m = 0.9
I0731 21:46:43.870417 20346 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 21:46:48.969892 20406 solver.cpp:404] Sparsity after update:
I0731 21:46:48.986460 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 21:46:48.986508 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 21:46:48.986526 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 21:46:48.986529 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 21:46:48.986532 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 21:46:48.986536 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 21:46:48.986538 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 21:46:48.986541 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 21:46:48.986544 20406 net.cpp:2270] out3a_param_0(0) 
I0731 21:46:48.986546 20406 net.cpp:2270] out5a_param_0(0) 
I0731 21:46:48.986549 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 21:46:48.986552 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 21:46:48.986554 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 21:46:48.986558 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 21:46:48.986596 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 21:46:48.986608 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 21:46:48.986618 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 21:46:48.986625 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 21:46:48.986634 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 21:46:49.162711 20406 solver.cpp:353] Iteration 3000 (5.42164 iter/s, 18.4446s/100 iter), loss = 0.0867729
I0731 21:46:49.162739 20406 solver.cpp:375]     Train net output #0: loss = 0.0867729 (* 1 = 0.0867729 loss)
I0731 21:46:49.162746 20406 sgd_solver.cpp:136] Iteration 3000, lr = 1e-05, m = 0.9
I0731 21:47:07.838109 20406 solver.cpp:353] Iteration 3100 (5.35479 iter/s, 18.6749s/100 iter), loss = 0.042687
I0731 21:47:07.838197 20406 solver.cpp:375]     Train net output #0: loss = 0.042687 (* 1 = 0.042687 loss)
I0731 21:47:07.838207 20406 sgd_solver.cpp:136] Iteration 3100, lr = 1e-05, m = 0.9
I0731 21:47:26.493017 20406 solver.cpp:353] Iteration 3200 (5.36067 iter/s, 18.6544s/100 iter), loss = 0.122004
I0731 21:47:26.493044 20406 solver.cpp:375]     Train net output #0: loss = 0.122004 (* 1 = 0.122004 loss)
I0731 21:47:26.493050 20406 sgd_solver.cpp:136] Iteration 3200, lr = 1e-05, m = 0.9
I0731 21:47:45.015565 20406 solver.cpp:353] Iteration 3300 (5.39897 iter/s, 18.522s/100 iter), loss = 0.0614537
I0731 21:47:45.015625 20406 solver.cpp:375]     Train net output #0: loss = 0.0614537 (* 1 = 0.0614537 loss)
I0731 21:47:45.015630 20406 sgd_solver.cpp:136] Iteration 3300, lr = 1e-05, m = 0.9
I0731 21:47:45.416330 20412 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 21:48:03.574316 20406 solver.cpp:353] Iteration 3400 (5.38844 iter/s, 18.5582s/100 iter), loss = 0.0442674
I0731 21:48:03.574339 20406 solver.cpp:375]     Train net output #0: loss = 0.0442674 (* 1 = 0.0442674 loss)
I0731 21:48:03.574344 20406 sgd_solver.cpp:136] Iteration 3400, lr = 1e-05, m = 0.9
I0731 21:48:22.069855 20406 solver.cpp:353] Iteration 3500 (5.40686 iter/s, 18.495s/100 iter), loss = 0.0757211
I0731 21:48:22.069907 20406 solver.cpp:375]     Train net output #0: loss = 0.0757211 (* 1 = 0.0757211 loss)
I0731 21:48:22.069912 20406 sgd_solver.cpp:136] Iteration 3500, lr = 1e-05, m = 0.9
I0731 21:48:40.623047 20406 solver.cpp:353] Iteration 3600 (5.39006 iter/s, 18.5527s/100 iter), loss = 0.0711497
I0731 21:48:40.623073 20406 solver.cpp:375]     Train net output #0: loss = 0.0711497 (* 1 = 0.0711497 loss)
I0731 21:48:40.623080 20406 sgd_solver.cpp:136] Iteration 3600, lr = 1e-05, m = 0.9
I0731 21:48:46.736836 20412 data_reader.cpp:264] Starting prefetch of epoch 4
I0731 21:48:59.188287 20406 solver.cpp:353] Iteration 3700 (5.38656 iter/s, 18.5647s/100 iter), loss = 0.0752344
I0731 21:48:59.188344 20406 solver.cpp:375]     Train net output #0: loss = 0.0752344 (* 1 = 0.0752344 loss)
I0731 21:48:59.188349 20406 sgd_solver.cpp:136] Iteration 3700, lr = 1e-05, m = 0.9
I0731 21:49:17.256353 20410 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 21:49:17.594110 20406 solver.cpp:353] Iteration 3800 (5.43321 iter/s, 18.4053s/100 iter), loss = 0.078594
I0731 21:49:17.594136 20406 solver.cpp:375]     Train net output #0: loss = 0.078594 (* 1 = 0.078594 loss)
I0731 21:49:17.594141 20406 sgd_solver.cpp:136] Iteration 3800, lr = 1e-05, m = 0.9
I0731 21:49:37.080976 20406 solver.cpp:353] Iteration 3900 (5.1318 iter/s, 19.4863s/100 iter), loss = 0.0646251
I0731 21:49:37.081033 20406 solver.cpp:375]     Train net output #0: loss = 0.0646251 (* 1 = 0.0646251 loss)
I0731 21:49:37.081038 20406 sgd_solver.cpp:136] Iteration 3900, lr = 1e-05, m = 0.9
I0731 21:49:55.430253 20406 solver.cpp:404] Sparsity after update:
I0731 21:49:55.441603 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 21:49:55.441622 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 21:49:55.441627 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 21:49:55.441629 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 21:49:55.441632 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 21:49:55.441633 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 21:49:55.441635 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 21:49:55.441637 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 21:49:55.441639 20406 net.cpp:2270] out3a_param_0(0) 
I0731 21:49:55.441642 20406 net.cpp:2270] out5a_param_0(0) 
I0731 21:49:55.441642 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 21:49:55.441644 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 21:49:55.441646 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 21:49:55.441648 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 21:49:55.441651 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 21:49:55.441653 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 21:49:55.441656 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 21:49:55.441658 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 21:49:55.441660 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 21:49:55.441669 20406 solver.cpp:550] Iteration 4000, Testing net (#0)
I0731 21:49:58.852716 20436 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 21:50:06.984479 20406 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.951996
I0731 21:50:06.984498 20406 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999815
I0731 21:50:06.984504 20406 solver.cpp:635]     Test net output #2: loss = 0.165038 (* 1 = 0.165038 loss)
I0731 21:50:06.984532 20406 solver.cpp:305] [MultiGPU] Tests completed in 11.5425s
I0731 21:50:07.194308 20406 solver.cpp:353] Iteration 4000 (3.32088 iter/s, 30.1125s/100 iter), loss = 0.0569631
I0731 21:50:07.194388 20406 solver.cpp:375]     Train net output #0: loss = 0.0569631 (* 1 = 0.0569631 loss)
I0731 21:50:07.194396 20406 sgd_solver.cpp:136] Iteration 4000, lr = 1e-05, m = 0.9
I0731 21:50:25.932782 20406 solver.cpp:353] Iteration 4100 (5.33676 iter/s, 18.738s/100 iter), loss = 0.0895072
I0731 21:50:25.932811 20406 solver.cpp:375]     Train net output #0: loss = 0.0895072 (* 1 = 0.0895072 loss)
I0731 21:50:25.932821 20406 sgd_solver.cpp:136] Iteration 4100, lr = 1e-05, m = 0.9
I0731 21:50:31.149884 20412 data_reader.cpp:264] Starting prefetch of epoch 5
I0731 21:50:44.486593 20406 solver.cpp:353] Iteration 4200 (5.38988 iter/s, 18.5533s/100 iter), loss = 0.0702297
I0731 21:50:44.486649 20406 solver.cpp:375]     Train net output #0: loss = 0.0702297 (* 1 = 0.0702297 loss)
I0731 21:50:44.486654 20406 sgd_solver.cpp:136] Iteration 4200, lr = 1e-05, m = 0.9
I0731 21:51:01.828342 20409 data_reader.cpp:264] Starting prefetch of epoch 4
I0731 21:51:02.913709 20406 solver.cpp:353] Iteration 4300 (5.42694 iter/s, 18.4266s/100 iter), loss = 0.0939048
I0731 21:51:02.913735 20406 solver.cpp:375]     Train net output #0: loss = 0.0939048 (* 1 = 0.0939048 loss)
I0731 21:51:02.913739 20406 sgd_solver.cpp:136] Iteration 4300, lr = 1e-05, m = 0.9
I0731 21:51:21.705718 20406 solver.cpp:353] Iteration 4400 (5.32156 iter/s, 18.7915s/100 iter), loss = 0.0542645
I0731 21:51:21.705768 20406 solver.cpp:375]     Train net output #0: loss = 0.0542645 (* 1 = 0.0542645 loss)
I0731 21:51:21.705773 20406 sgd_solver.cpp:136] Iteration 4400, lr = 1e-05, m = 0.9
I0731 21:51:40.295115 20406 solver.cpp:353] Iteration 4500 (5.37956 iter/s, 18.5889s/100 iter), loss = 0.0463979
I0731 21:51:40.295140 20406 solver.cpp:375]     Train net output #0: loss = 0.0463979 (* 1 = 0.0463979 loss)
I0731 21:51:40.295145 20406 sgd_solver.cpp:136] Iteration 4500, lr = 1e-05, m = 0.9
I0731 21:51:58.650795 20406 solver.cpp:353] Iteration 4600 (5.44806 iter/s, 18.3552s/100 iter), loss = 0.106224
I0731 21:51:58.650846 20406 solver.cpp:375]     Train net output #0: loss = 0.106224 (* 1 = 0.106224 loss)
I0731 21:51:58.650851 20406 sgd_solver.cpp:136] Iteration 4600, lr = 1e-05, m = 0.9
I0731 21:52:03.071838 20415 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 21:52:17.075870 20406 solver.cpp:353] Iteration 4700 (5.42754 iter/s, 18.4246s/100 iter), loss = 0.0710359
I0731 21:52:17.075897 20406 solver.cpp:375]     Train net output #0: loss = 0.0710359 (* 1 = 0.0710359 loss)
I0731 21:52:17.075902 20406 sgd_solver.cpp:136] Iteration 4700, lr = 1e-05, m = 0.9
I0731 21:52:35.619675 20406 solver.cpp:353] Iteration 4800 (5.39279 iter/s, 18.5433s/100 iter), loss = 0.0751191
I0731 21:52:35.619797 20406 solver.cpp:375]     Train net output #0: loss = 0.0751191 (* 1 = 0.0751191 loss)
I0731 21:52:35.619803 20406 sgd_solver.cpp:136] Iteration 4800, lr = 1e-05, m = 0.9
I0731 21:52:54.262728 20406 solver.cpp:353] Iteration 4900 (5.36408 iter/s, 18.6425s/100 iter), loss = 0.0706066
I0731 21:52:54.262753 20406 solver.cpp:375]     Train net output #0: loss = 0.0706066 (* 1 = 0.0706066 loss)
I0731 21:52:54.262758 20406 sgd_solver.cpp:136] Iteration 4900, lr = 1e-05, m = 0.9
I0731 21:53:04.551415 20364 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 21:53:13.067399 20406 solver.cpp:404] Sparsity after update:
I0731 21:53:13.101663 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 21:53:13.101707 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 21:53:13.101722 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 21:53:13.101725 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 21:53:13.101728 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 21:53:13.101733 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 21:53:13.101737 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 21:53:13.101739 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 21:53:13.101742 20406 net.cpp:2270] out3a_param_0(0) 
I0731 21:53:13.101745 20406 net.cpp:2270] out5a_param_0(0) 
I0731 21:53:13.101747 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 21:53:13.101752 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 21:53:13.101754 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 21:53:13.101758 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 21:53:13.101761 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 21:53:13.101764 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 21:53:13.101768 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 21:53:13.101771 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 21:53:13.101775 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 21:53:13.272439 20406 solver.cpp:353] Iteration 5000 (5.26062 iter/s, 19.0092s/100 iter), loss = 0.0778511
I0731 21:53:13.272461 20406 solver.cpp:375]     Train net output #0: loss = 0.0778511 (* 1 = 0.0778511 loss)
I0731 21:53:13.272465 20406 sgd_solver.cpp:136] Iteration 5000, lr = 1e-05, m = 0.9
I0731 21:53:32.827229 20406 solver.cpp:353] Iteration 5100 (5.11398 iter/s, 19.5543s/100 iter), loss = 0.111352
I0731 21:53:32.827256 20406 solver.cpp:375]     Train net output #0: loss = 0.111352 (* 1 = 0.111352 loss)
I0731 21:53:32.827260 20406 sgd_solver.cpp:136] Iteration 5100, lr = 1e-05, m = 0.9
I0731 21:53:36.776298 20346 data_reader.cpp:264] Starting prefetch of epoch 4
I0731 21:53:52.396360 20406 solver.cpp:353] Iteration 5200 (5.11023 iter/s, 19.5686s/100 iter), loss = 0.078714
I0731 21:53:52.396423 20406 solver.cpp:375]     Train net output #0: loss = 0.078714 (* 1 = 0.078714 loss)
I0731 21:53:52.396430 20406 sgd_solver.cpp:136] Iteration 5200, lr = 1e-05, m = 0.9
I0731 21:54:11.641379 20406 solver.cpp:353] Iteration 5300 (5.19629 iter/s, 19.2445s/100 iter), loss = 0.0496281
I0731 21:54:11.641402 20406 solver.cpp:375]     Train net output #0: loss = 0.0496281 (* 1 = 0.0496281 loss)
I0731 21:54:11.641407 20406 sgd_solver.cpp:136] Iteration 5300, lr = 1e-05, m = 0.9
I0731 21:54:30.878854 20406 solver.cpp:353] Iteration 5400 (5.19833 iter/s, 19.2369s/100 iter), loss = 0.0563819
I0731 21:54:30.878914 20406 solver.cpp:375]     Train net output #0: loss = 0.0563819 (* 1 = 0.0563819 loss)
I0731 21:54:30.878919 20406 sgd_solver.cpp:136] Iteration 5400, lr = 1e-05, m = 0.9
I0731 21:54:40.893900 20364 data_reader.cpp:264] Starting prefetch of epoch 4
I0731 21:54:50.300691 20406 solver.cpp:353] Iteration 5500 (5.14899 iter/s, 19.4213s/100 iter), loss = 0.0743792
I0731 21:54:50.300720 20406 solver.cpp:375]     Train net output #0: loss = 0.0743792 (* 1 = 0.0743792 loss)
I0731 21:54:50.300727 20406 sgd_solver.cpp:136] Iteration 5500, lr = 1e-05, m = 0.9
I0731 21:55:09.272109 20406 solver.cpp:353] Iteration 5600 (5.27123 iter/s, 18.9709s/100 iter), loss = 0.0368734
I0731 21:55:09.272195 20406 solver.cpp:375]     Train net output #0: loss = 0.0368734 (* 1 = 0.0368734 loss)
I0731 21:55:09.272203 20406 sgd_solver.cpp:136] Iteration 5600, lr = 1e-05, m = 0.9
I0731 21:55:28.394176 20406 solver.cpp:353] Iteration 5700 (5.2297 iter/s, 19.1215s/100 iter), loss = 0.080994
I0731 21:55:28.394206 20406 solver.cpp:375]     Train net output #0: loss = 0.0809939 (* 1 = 0.0809939 loss)
I0731 21:55:28.394209 20406 sgd_solver.cpp:136] Iteration 5700, lr = 1e-05, m = 0.9
I0731 21:55:43.991722 20412 data_reader.cpp:264] Starting prefetch of epoch 6
I0731 21:55:47.614729 20406 solver.cpp:353] Iteration 5800 (5.20291 iter/s, 19.22s/100 iter), loss = 0.0724273
I0731 21:55:47.614754 20406 solver.cpp:375]     Train net output #0: loss = 0.0724272 (* 1 = 0.0724272 loss)
I0731 21:55:47.614759 20406 sgd_solver.cpp:136] Iteration 5800, lr = 1e-05, m = 0.9
I0731 21:56:07.083497 20406 solver.cpp:353] Iteration 5900 (5.13658 iter/s, 19.4682s/100 iter), loss = 0.0636804
I0731 21:56:07.083539 20406 solver.cpp:375]     Train net output #0: loss = 0.0636804 (* 1 = 0.0636804 loss)
I0731 21:56:07.083547 20406 sgd_solver.cpp:136] Iteration 5900, lr = 1e-05, m = 0.9
I0731 21:56:16.203260 20346 data_reader.cpp:264] Starting prefetch of epoch 5
I0731 21:56:26.232123 20406 solver.cpp:404] Sparsity after update:
I0731 21:56:26.237597 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 21:56:26.237615 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 21:56:26.237625 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 21:56:26.237628 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 21:56:26.237632 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 21:56:26.237634 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 21:56:26.237637 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 21:56:26.237645 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 21:56:26.237651 20406 net.cpp:2270] out3a_param_0(0) 
I0731 21:56:26.237656 20406 net.cpp:2270] out5a_param_0(0) 
I0731 21:56:26.237661 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 21:56:26.237666 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 21:56:26.237670 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 21:56:26.237674 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 21:56:26.237679 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 21:56:26.237682 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 21:56:26.237686 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 21:56:26.237690 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 21:56:26.237692 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 21:56:26.237704 20406 solver.cpp:550] Iteration 6000, Testing net (#0)
I0731 21:56:38.814949 20406 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.95024
I0731 21:56:38.814970 20406 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999182
I0731 21:56:38.814976 20406 solver.cpp:635]     Test net output #2: loss = 0.213933 (* 1 = 0.213933 loss)
I0731 21:56:38.815003 20406 solver.cpp:305] [MultiGPU] Tests completed in 12.577s
I0731 21:56:39.010752 20406 solver.cpp:353] Iteration 6000 (3.13221 iter/s, 31.9263s/100 iter), loss = 0.0937672
I0731 21:56:39.010852 20406 solver.cpp:375]     Train net output #0: loss = 0.0937671 (* 1 = 0.0937671 loss)
I0731 21:56:39.010874 20406 sgd_solver.cpp:136] Iteration 6000, lr = 1e-05, m = 0.9
I0731 21:56:58.428895 20406 solver.cpp:353] Iteration 6100 (5.14996 iter/s, 19.4176s/100 iter), loss = 0.0725928
I0731 21:56:58.428978 20406 solver.cpp:375]     Train net output #0: loss = 0.0725928 (* 1 = 0.0725928 loss)
I0731 21:56:58.428983 20406 sgd_solver.cpp:136] Iteration 6100, lr = 1e-05, m = 0.9
I0731 21:57:00.778717 20346 data_reader.cpp:264] Starting prefetch of epoch 6
I0731 21:57:17.461510 20406 solver.cpp:353] Iteration 6200 (5.25428 iter/s, 19.0321s/100 iter), loss = 0.070183
I0731 21:57:17.461539 20406 solver.cpp:375]     Train net output #0: loss = 0.070183 (* 1 = 0.070183 loss)
I0731 21:57:17.461544 20406 sgd_solver.cpp:136] Iteration 6200, lr = 1e-05, m = 0.9
I0731 21:57:36.800494 20406 solver.cpp:353] Iteration 6300 (5.17105 iter/s, 19.3384s/100 iter), loss = 0.0700748
I0731 21:57:36.800623 20406 solver.cpp:375]     Train net output #0: loss = 0.0700748 (* 1 = 0.0700748 loss)
I0731 21:57:36.800631 20406 sgd_solver.cpp:136] Iteration 6300, lr = 1e-05, m = 0.9
I0731 21:57:55.336874 20406 solver.cpp:353] Iteration 6400 (5.39495 iter/s, 18.5359s/100 iter), loss = 0.0789097
I0731 21:57:55.336905 20406 solver.cpp:375]     Train net output #0: loss = 0.0789096 (* 1 = 0.0789096 loss)
I0731 21:57:55.336908 20406 sgd_solver.cpp:136] Iteration 6400, lr = 1e-05, m = 0.9
I0731 21:58:03.121816 20364 data_reader.cpp:264] Starting prefetch of epoch 5
I0731 21:58:13.852633 20406 solver.cpp:353] Iteration 6500 (5.40095 iter/s, 18.5152s/100 iter), loss = 0.0808699
I0731 21:58:13.852694 20406 solver.cpp:375]     Train net output #0: loss = 0.0808699 (* 1 = 0.0808699 loss)
I0731 21:58:13.852699 20406 sgd_solver.cpp:136] Iteration 6500, lr = 1e-05, m = 0.9
I0731 21:58:32.525014 20406 solver.cpp:353] Iteration 6600 (5.35565 iter/s, 18.6719s/100 iter), loss = 0.0788466
I0731 21:58:32.525054 20406 solver.cpp:375]     Train net output #0: loss = 0.0788466 (* 1 = 0.0788466 loss)
I0731 21:58:32.525063 20406 sgd_solver.cpp:136] Iteration 6600, lr = 1e-05, m = 0.9
I0731 21:58:34.031359 20409 data_reader.cpp:264] Starting prefetch of epoch 5
I0731 21:58:50.992234 20406 solver.cpp:353] Iteration 6700 (5.41515 iter/s, 18.4667s/100 iter), loss = 0.088701
I0731 21:58:50.992295 20406 solver.cpp:375]     Train net output #0: loss = 0.088701 (* 1 = 0.088701 loss)
I0731 21:58:50.992300 20406 sgd_solver.cpp:136] Iteration 6700, lr = 1e-05, m = 0.9
I0731 21:59:09.476020 20406 solver.cpp:353] Iteration 6800 (5.4103 iter/s, 18.4833s/100 iter), loss = 0.0480586
I0731 21:59:09.476045 20406 solver.cpp:375]     Train net output #0: loss = 0.0480586 (* 1 = 0.0480586 loss)
I0731 21:59:09.476050 20406 sgd_solver.cpp:136] Iteration 6800, lr = 1e-05, m = 0.9
I0731 21:59:28.011332 20406 solver.cpp:353] Iteration 6900 (5.39526 iter/s, 18.5348s/100 iter), loss = 0.0902742
I0731 21:59:28.011389 20406 solver.cpp:375]     Train net output #0: loss = 0.0902741 (* 1 = 0.0902741 loss)
I0731 21:59:28.011395 20406 sgd_solver.cpp:136] Iteration 6900, lr = 1e-05, m = 0.9
I0731 21:59:35.055663 20346 data_reader.cpp:264] Starting prefetch of epoch 7
I0731 21:59:46.304666 20406 solver.cpp:404] Sparsity after update:
I0731 21:59:46.323163 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 21:59:46.323220 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 21:59:46.323236 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 21:59:46.323240 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 21:59:46.323242 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 21:59:46.323246 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 21:59:46.323249 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 21:59:46.323252 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 21:59:46.323256 20406 net.cpp:2270] out3a_param_0(0) 
I0731 21:59:46.323258 20406 net.cpp:2270] out5a_param_0(0) 
I0731 21:59:46.323261 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 21:59:46.323264 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 21:59:46.323267 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 21:59:46.323288 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 21:59:46.323298 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 21:59:46.323305 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 21:59:46.323314 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 21:59:46.323323 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 21:59:46.323333 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 21:59:46.495177 20406 solver.cpp:353] Iteration 7000 (5.41028 iter/s, 18.4833s/100 iter), loss = 0.0949538
I0731 21:59:46.495203 20406 solver.cpp:375]     Train net output #0: loss = 0.0949538 (* 1 = 0.0949538 loss)
I0731 21:59:46.495208 20406 sgd_solver.cpp:136] Iteration 7000, lr = 1e-05, m = 0.9
I0731 22:00:04.934856 20406 solver.cpp:353] Iteration 7100 (5.42324 iter/s, 18.4392s/100 iter), loss = 0.0711184
I0731 22:00:04.934988 20406 solver.cpp:375]     Train net output #0: loss = 0.0711184 (* 1 = 0.0711184 loss)
I0731 22:00:04.934995 20406 sgd_solver.cpp:136] Iteration 7100, lr = 1e-05, m = 0.9
I0731 22:00:23.411299 20406 solver.cpp:353] Iteration 7200 (5.41245 iter/s, 18.4759s/100 iter), loss = 0.0660203
I0731 22:00:23.411329 20406 solver.cpp:375]     Train net output #0: loss = 0.0660202 (* 1 = 0.0660202 loss)
I0731 22:00:23.411332 20406 sgd_solver.cpp:136] Iteration 7200, lr = 1e-05, m = 0.9
I0731 22:00:36.178922 20412 data_reader.cpp:264] Starting prefetch of epoch 7
I0731 22:00:41.896205 20406 solver.cpp:353] Iteration 7300 (5.40997 iter/s, 18.4844s/100 iter), loss = 0.0607179
I0731 22:00:41.896235 20406 solver.cpp:375]     Train net output #0: loss = 0.0607179 (* 1 = 0.0607179 loss)
I0731 22:00:41.896244 20406 sgd_solver.cpp:136] Iteration 7300, lr = 1e-05, m = 0.9
I0731 22:01:00.498553 20406 solver.cpp:353] Iteration 7400 (5.37581 iter/s, 18.6018s/100 iter), loss = 0.0909598
I0731 22:01:00.498576 20406 solver.cpp:375]     Train net output #0: loss = 0.0909598 (* 1 = 0.0909598 loss)
I0731 22:01:00.498581 20406 sgd_solver.cpp:136] Iteration 7400, lr = 1e-05, m = 0.9
I0731 22:01:06.771844 20346 data_reader.cpp:264] Starting prefetch of epoch 8
I0731 22:01:18.948786 20406 solver.cpp:353] Iteration 7500 (5.42014 iter/s, 18.4497s/100 iter), loss = 0.0923757
I0731 22:01:18.948808 20406 solver.cpp:375]     Train net output #0: loss = 0.0923757 (* 1 = 0.0923757 loss)
I0731 22:01:18.948824 20406 sgd_solver.cpp:136] Iteration 7500, lr = 1e-05, m = 0.9
I0731 22:01:37.425094 20406 solver.cpp:353] Iteration 7600 (5.41249 iter/s, 18.4758s/100 iter), loss = 0.0605909
I0731 22:01:37.425143 20406 solver.cpp:375]     Train net output #0: loss = 0.0605908 (* 1 = 0.0605908 loss)
I0731 22:01:37.425148 20406 sgd_solver.cpp:136] Iteration 7600, lr = 1e-05, m = 0.9
I0731 22:01:56.038457 20406 solver.cpp:353] Iteration 7700 (5.37263 iter/s, 18.6129s/100 iter), loss = 0.069707
I0731 22:01:56.038483 20406 solver.cpp:375]     Train net output #0: loss = 0.069707 (* 1 = 0.069707 loss)
I0731 22:01:56.038487 20406 sgd_solver.cpp:136] Iteration 7700, lr = 1e-05, m = 0.9
I0731 22:02:08.065588 20364 data_reader.cpp:264] Starting prefetch of epoch 6
I0731 22:02:14.501930 20406 solver.cpp:353] Iteration 7800 (5.41625 iter/s, 18.463s/100 iter), loss = 0.074102
I0731 22:02:14.501957 20406 solver.cpp:375]     Train net output #0: loss = 0.0741019 (* 1 = 0.0741019 loss)
I0731 22:02:14.501962 20406 sgd_solver.cpp:136] Iteration 7800, lr = 1e-05, m = 0.9
I0731 22:02:33.304854 20406 solver.cpp:353] Iteration 7900 (5.31847 iter/s, 18.8024s/100 iter), loss = 0.0530646
I0731 22:02:33.304888 20406 solver.cpp:375]     Train net output #0: loss = 0.0530645 (* 1 = 0.0530645 loss)
I0731 22:02:33.304895 20406 sgd_solver.cpp:136] Iteration 7900, lr = 1e-05, m = 0.9
I0731 22:02:51.651887 20406 solver.cpp:404] Sparsity after update:
I0731 22:02:51.662370 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 22:02:51.662390 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 22:02:51.662398 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 22:02:51.662400 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 22:02:51.662402 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 22:02:51.662405 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 22:02:51.662406 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 22:02:51.662408 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 22:02:51.662410 20406 net.cpp:2270] out3a_param_0(0) 
I0731 22:02:51.662411 20406 net.cpp:2270] out5a_param_0(0) 
I0731 22:02:51.662413 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 22:02:51.662415 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 22:02:51.664041 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 22:02:51.664049 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 22:02:51.664054 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 22:02:51.664059 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 22:02:51.664062 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 22:02:51.664065 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 22:02:51.664068 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 22:02:51.664083 20406 solver.cpp:550] Iteration 8000, Testing net (#0)
I0731 22:02:54.894074 20402 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 22:03:02.504953 20438 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 22:03:02.848737 20406 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.95212
I0731 22:03:02.848762 20406 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999833
I0731 22:03:02.848768 20406 solver.cpp:635]     Test net output #2: loss = 0.163685 (* 1 = 0.163685 loss)
I0731 22:03:02.848848 20406 solver.cpp:305] [MultiGPU] Tests completed in 11.1845s
I0731 22:03:03.047128 20406 solver.cpp:353] Iteration 8000 (3.36231 iter/s, 29.7415s/100 iter), loss = 0.092974
I0731 22:03:03.047155 20406 solver.cpp:375]     Train net output #0: loss = 0.0929739 (* 1 = 0.0929739 loss)
I0731 22:03:03.047159 20406 sgd_solver.cpp:136] Iteration 8000, lr = 1e-05, m = 0.9
I0731 22:03:21.552589 20406 solver.cpp:353] Iteration 8100 (5.40396 iter/s, 18.505s/100 iter), loss = 0.0602612
I0731 22:03:21.552613 20406 solver.cpp:375]     Train net output #0: loss = 0.0602611 (* 1 = 0.0602611 loss)
I0731 22:03:21.552618 20406 sgd_solver.cpp:136] Iteration 8100, lr = 1e-05, m = 0.9
I0731 22:03:40.151628 20406 solver.cpp:353] Iteration 8200 (5.37677 iter/s, 18.5985s/100 iter), loss = 0.0572736
I0731 22:03:40.151701 20406 solver.cpp:375]     Train net output #0: loss = 0.0572735 (* 1 = 0.0572735 loss)
I0731 22:03:40.151710 20406 sgd_solver.cpp:136] Iteration 8200, lr = 1e-05, m = 0.9
I0731 22:03:51.375408 20409 data_reader.cpp:264] Starting prefetch of epoch 6
I0731 22:03:58.743124 20406 solver.cpp:353] Iteration 8300 (5.37895 iter/s, 18.591s/100 iter), loss = 0.110949
I0731 22:03:58.743149 20406 solver.cpp:375]     Train net output #0: loss = 0.110948 (* 1 = 0.110948 loss)
I0731 22:03:58.743152 20406 sgd_solver.cpp:136] Iteration 8300, lr = 1e-05, m = 0.9
I0731 22:04:17.272485 20406 solver.cpp:353] Iteration 8400 (5.39699 iter/s, 18.5288s/100 iter), loss = 0.063459
I0731 22:04:17.272541 20406 solver.cpp:375]     Train net output #0: loss = 0.0634589 (* 1 = 0.0634589 loss)
I0731 22:04:17.272547 20406 sgd_solver.cpp:136] Iteration 8400, lr = 1e-05, m = 0.9
I0731 22:04:35.865823 20406 solver.cpp:353] Iteration 8500 (5.37842 iter/s, 18.5928s/100 iter), loss = 0.0531844
I0731 22:04:35.865851 20406 solver.cpp:375]     Train net output #0: loss = 0.0531844 (* 1 = 0.0531844 loss)
I0731 22:04:35.865855 20406 sgd_solver.cpp:136] Iteration 8500, lr = 1e-05, m = 0.9
I0731 22:04:52.984211 20410 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 22:04:54.609482 20406 solver.cpp:353] Iteration 8600 (5.33528 iter/s, 18.7431s/100 iter), loss = 0.0927126
I0731 22:04:54.609506 20406 solver.cpp:375]     Train net output #0: loss = 0.0927126 (* 1 = 0.0927126 loss)
I0731 22:04:54.609511 20406 sgd_solver.cpp:136] Iteration 8600, lr = 1e-05, m = 0.9
I0731 22:05:13.299134 20406 solver.cpp:353] Iteration 8700 (5.3507 iter/s, 18.6891s/100 iter), loss = 0.0728258
I0731 22:05:13.299161 20406 solver.cpp:375]     Train net output #0: loss = 0.0728257 (* 1 = 0.0728257 loss)
I0731 22:05:13.299165 20406 sgd_solver.cpp:136] Iteration 8700, lr = 1e-05, m = 0.9
I0731 22:05:31.820360 20406 solver.cpp:353] Iteration 8800 (5.39936 iter/s, 18.5207s/100 iter), loss = 0.0725976
I0731 22:05:31.820416 20406 solver.cpp:375]     Train net output #0: loss = 0.0725975 (* 1 = 0.0725975 loss)
I0731 22:05:31.820420 20406 sgd_solver.cpp:136] Iteration 8800, lr = 1e-05, m = 0.9
I0731 22:05:50.536602 20406 solver.cpp:353] Iteration 8900 (5.3431 iter/s, 18.7157s/100 iter), loss = 0.070402
I0731 22:05:50.536631 20406 solver.cpp:375]     Train net output #0: loss = 0.070402 (* 1 = 0.070402 loss)
I0731 22:05:50.536636 20406 sgd_solver.cpp:136] Iteration 8900, lr = 1e-05, m = 0.9
I0731 22:05:54.485249 20364 data_reader.cpp:264] Starting prefetch of epoch 7
I0731 22:06:08.925165 20406 solver.cpp:404] Sparsity after update:
I0731 22:06:08.952764 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 22:06:08.952790 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 22:06:08.952797 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 22:06:08.952800 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 22:06:08.952801 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 22:06:08.952803 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 22:06:08.952805 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 22:06:08.952807 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 22:06:08.952808 20406 net.cpp:2270] out3a_param_0(0) 
I0731 22:06:08.952811 20406 net.cpp:2270] out5a_param_0(0) 
I0731 22:06:08.952812 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 22:06:08.952826 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 22:06:08.952828 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 22:06:08.952831 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 22:06:08.952832 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 22:06:08.952834 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 22:06:08.952836 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 22:06:08.952838 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 22:06:08.952841 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 22:06:09.122265 20406 solver.cpp:353] Iteration 9000 (5.38064 iter/s, 18.5851s/100 iter), loss = 0.0637801
I0731 22:06:09.122344 20406 solver.cpp:375]     Train net output #0: loss = 0.06378 (* 1 = 0.06378 loss)
I0731 22:06:09.122364 20406 sgd_solver.cpp:136] Iteration 9000, lr = 1e-05, m = 0.9
I0731 22:06:27.687109 20406 solver.cpp:353] Iteration 9100 (5.38667 iter/s, 18.5643s/100 iter), loss = 0.0805036
I0731 22:06:27.687132 20406 solver.cpp:375]     Train net output #0: loss = 0.0805036 (* 1 = 0.0805036 loss)
I0731 22:06:27.687136 20406 sgd_solver.cpp:136] Iteration 9100, lr = 1e-05, m = 0.9
I0731 22:06:46.344805 20406 solver.cpp:353] Iteration 9200 (5.35987 iter/s, 18.6572s/100 iter), loss = 0.061881
I0731 22:06:46.344892 20406 solver.cpp:375]     Train net output #0: loss = 0.0618809 (* 1 = 0.0618809 loss)
I0731 22:06:46.344900 20406 sgd_solver.cpp:136] Iteration 9200, lr = 1e-05, m = 0.9
I0731 22:06:56.098182 20364 data_reader.cpp:264] Starting prefetch of epoch 8
I0731 22:07:05.091732 20406 solver.cpp:353] Iteration 9300 (5.33435 iter/s, 18.7464s/100 iter), loss = 0.0722495
I0731 22:07:05.091758 20406 solver.cpp:375]     Train net output #0: loss = 0.0722494 (* 1 = 0.0722494 loss)
I0731 22:07:05.091765 20406 sgd_solver.cpp:136] Iteration 9300, lr = 1e-05, m = 0.9
I0731 22:07:23.675930 20406 solver.cpp:353] Iteration 9400 (5.38106 iter/s, 18.5837s/100 iter), loss = 0.0891466
I0731 22:07:23.675984 20406 solver.cpp:375]     Train net output #0: loss = 0.0891465 (* 1 = 0.0891465 loss)
I0731 22:07:23.675989 20406 sgd_solver.cpp:136] Iteration 9400, lr = 1e-05, m = 0.9
I0731 22:07:26.819880 20409 data_reader.cpp:264] Starting prefetch of epoch 7
I0731 22:07:42.135563 20406 solver.cpp:353] Iteration 9500 (5.41738 iter/s, 18.4591s/100 iter), loss = 0.0603324
I0731 22:07:42.135589 20406 solver.cpp:375]     Train net output #0: loss = 0.0603323 (* 1 = 0.0603323 loss)
I0731 22:07:42.135593 20406 sgd_solver.cpp:136] Iteration 9500, lr = 1e-05, m = 0.9
I0731 22:08:00.589334 20406 solver.cpp:353] Iteration 9600 (5.4191 iter/s, 18.4533s/100 iter), loss = 0.0653687
I0731 22:08:00.589395 20406 solver.cpp:375]     Train net output #0: loss = 0.0653686 (* 1 = 0.0653686 loss)
I0731 22:08:00.589401 20406 sgd_solver.cpp:136] Iteration 9600, lr = 1e-05, m = 0.9
I0731 22:08:19.111280 20406 solver.cpp:353] Iteration 9700 (5.39915 iter/s, 18.5214s/100 iter), loss = 0.0654426
I0731 22:08:19.111307 20406 solver.cpp:375]     Train net output #0: loss = 0.0654425 (* 1 = 0.0654425 loss)
I0731 22:08:19.111313 20406 sgd_solver.cpp:136] Iteration 9700, lr = 1e-05, m = 0.9
I0731 22:08:28.053586 20415 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 22:08:37.682633 20406 solver.cpp:353] Iteration 9800 (5.38479 iter/s, 18.5708s/100 iter), loss = 0.0611385
I0731 22:08:37.682705 20406 solver.cpp:375]     Train net output #0: loss = 0.0611384 (* 1 = 0.0611384 loss)
I0731 22:08:37.682711 20406 sgd_solver.cpp:136] Iteration 9800, lr = 1e-05, m = 0.9
I0731 22:08:56.794173 20406 solver.cpp:353] Iteration 9900 (5.23258 iter/s, 19.111s/100 iter), loss = 0.0677815
I0731 22:08:56.794196 20406 solver.cpp:375]     Train net output #0: loss = 0.0677814 (* 1 = 0.0677814 loss)
I0731 22:08:56.794201 20406 sgd_solver.cpp:136] Iteration 9900, lr = 1e-05, m = 0.9
I0731 22:09:15.276203 20406 solver.cpp:680] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/sparse/cityscapes5_jsegnet21v2_iter_10000.caffemodel
I0731 22:09:15.356381 20406 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/sparse/cityscapes5_jsegnet21v2_iter_10000.solverstate
I0731 22:09:15.364439 20406 solver.cpp:404] Sparsity after update:
I0731 22:09:15.370259 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 22:09:15.370326 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 22:09:15.370344 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 22:09:15.370348 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 22:09:15.370352 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 22:09:15.370357 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 22:09:15.370360 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 22:09:15.370364 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 22:09:15.370370 20406 net.cpp:2270] out3a_param_0(0) 
I0731 22:09:15.370374 20406 net.cpp:2270] out5a_param_0(0) 
I0731 22:09:15.370379 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 22:09:15.370383 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 22:09:15.370388 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 22:09:15.370391 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 22:09:15.370395 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 22:09:15.370399 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 22:09:15.370402 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 22:09:15.370406 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 22:09:15.370410 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 22:09:15.370426 20406 solver.cpp:550] Iteration 10000, Testing net (#0)
I0731 22:09:22.336493 20434 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 22:09:26.336403 20406 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.951388
I0731 22:09:26.336429 20406 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999208
I0731 22:09:26.336436 20406 solver.cpp:635]     Test net output #2: loss = 0.207556 (* 1 = 0.207556 loss)
I0731 22:09:26.336457 20406 solver.cpp:305] [MultiGPU] Tests completed in 10.9657s
I0731 22:09:26.532377 20406 solver.cpp:353] Iteration 10000 (3.36277 iter/s, 29.7374s/100 iter), loss = 0.069129
I0731 22:09:26.532402 20406 solver.cpp:375]     Train net output #0: loss = 0.069129 (* 1 = 0.069129 loss)
I0731 22:09:26.532405 20406 sgd_solver.cpp:136] Iteration 10000, lr = 1e-05, m = 0.9
I0731 22:09:41.032611 20409 data_reader.cpp:264] Starting prefetch of epoch 8
I0731 22:09:45.117739 20406 solver.cpp:353] Iteration 10100 (5.38073 iter/s, 18.5849s/100 iter), loss = 0.0670187
I0731 22:09:45.117763 20406 solver.cpp:375]     Train net output #0: loss = 0.0670186 (* 1 = 0.0670186 loss)
I0731 22:09:45.117769 20406 sgd_solver.cpp:136] Iteration 10100, lr = 1e-05, m = 0.9
I0731 22:10:03.456010 20406 solver.cpp:353] Iteration 10200 (5.45323 iter/s, 18.3378s/100 iter), loss = 0.0747474
I0731 22:10:03.456060 20406 solver.cpp:375]     Train net output #0: loss = 0.0747473 (* 1 = 0.0747473 loss)
I0731 22:10:03.456068 20406 sgd_solver.cpp:136] Iteration 10200, lr = 1e-05, m = 0.9
I0731 22:10:21.852144 20406 solver.cpp:353] Iteration 10300 (5.43608 iter/s, 18.3956s/100 iter), loss = 0.0542392
I0731 22:10:21.852174 20406 solver.cpp:375]     Train net output #0: loss = 0.0542391 (* 1 = 0.0542391 loss)
I0731 22:10:21.852177 20406 sgd_solver.cpp:136] Iteration 10300, lr = 1e-05, m = 0.9
I0731 22:10:40.361291 20406 solver.cpp:353] Iteration 10400 (5.40288 iter/s, 18.5086s/100 iter), loss = 0.0519043
I0731 22:10:40.361392 20406 solver.cpp:375]     Train net output #0: loss = 0.0519042 (* 1 = 0.0519042 loss)
I0731 22:10:40.361398 20406 sgd_solver.cpp:136] Iteration 10400, lr = 1e-05, m = 0.9
I0731 22:10:42.073048 20346 data_reader.cpp:264] Starting prefetch of epoch 9
I0731 22:10:59.120299 20406 solver.cpp:353] Iteration 10500 (5.33092 iter/s, 18.7585s/100 iter), loss = 0.0571081
I0731 22:10:59.120323 20406 solver.cpp:375]     Train net output #0: loss = 0.057108 (* 1 = 0.057108 loss)
I0731 22:10:59.120329 20406 sgd_solver.cpp:136] Iteration 10500, lr = 1e-05, m = 0.9
I0731 22:11:17.674078 20406 solver.cpp:353] Iteration 10600 (5.38989 iter/s, 18.5533s/100 iter), loss = 0.0796069
I0731 22:11:17.674154 20406 solver.cpp:375]     Train net output #0: loss = 0.0796068 (* 1 = 0.0796068 loss)
I0731 22:11:17.674159 20406 sgd_solver.cpp:136] Iteration 10600, lr = 1e-05, m = 0.9
I0731 22:11:36.133182 20406 solver.cpp:353] Iteration 10700 (5.41753 iter/s, 18.4586s/100 iter), loss = 0.0400956
I0731 22:11:36.133206 20406 solver.cpp:375]     Train net output #0: loss = 0.0400955 (* 1 = 0.0400955 loss)
I0731 22:11:36.133213 20406 sgd_solver.cpp:136] Iteration 10700, lr = 1e-05, m = 0.9
I0731 22:11:43.606480 20346 data_reader.cpp:264] Starting prefetch of epoch 10
I0731 22:11:54.689424 20406 solver.cpp:353] Iteration 10800 (5.38917 iter/s, 18.5557s/100 iter), loss = 0.0753683
I0731 22:11:54.689504 20406 solver.cpp:375]     Train net output #0: loss = 0.0753682 (* 1 = 0.0753682 loss)
I0731 22:11:54.689512 20406 sgd_solver.cpp:136] Iteration 10800, lr = 1e-05, m = 0.9
I0731 22:12:13.199458 20406 solver.cpp:353] Iteration 10900 (5.40262 iter/s, 18.5095s/100 iter), loss = 0.0453636
I0731 22:12:13.199484 20406 solver.cpp:375]     Train net output #0: loss = 0.0453635 (* 1 = 0.0453635 loss)
I0731 22:12:13.199488 20406 sgd_solver.cpp:136] Iteration 10900, lr = 1e-05, m = 0.9
I0731 22:12:31.853662 20406 solver.cpp:404] Sparsity after update:
I0731 22:12:31.869884 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 22:12:31.869912 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 22:12:31.869925 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 22:12:31.869930 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 22:12:31.869931 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 22:12:31.869935 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 22:12:31.869937 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 22:12:31.869940 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 22:12:31.869943 20406 net.cpp:2270] out3a_param_0(0) 
I0731 22:12:31.869956 20406 net.cpp:2270] out5a_param_0(0) 
I0731 22:12:31.869961 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 22:12:31.869966 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 22:12:31.869969 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 22:12:31.869974 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 22:12:31.869978 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 22:12:31.869982 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 22:12:31.869987 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 22:12:31.869990 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 22:12:31.869994 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 22:12:32.046416 20406 solver.cpp:353] Iteration 11000 (5.30604 iter/s, 18.8464s/100 iter), loss = 0.0590418
I0731 22:12:32.046445 20406 solver.cpp:375]     Train net output #0: loss = 0.0590418 (* 1 = 0.0590418 loss)
I0731 22:12:32.046452 20406 sgd_solver.cpp:136] Iteration 11000, lr = 1e-05, m = 0.9
I0731 22:12:45.105818 20364 data_reader.cpp:264] Starting prefetch of epoch 9
I0731 22:12:50.689982 20406 solver.cpp:353] Iteration 11100 (5.36393 iter/s, 18.6431s/100 iter), loss = 0.0758334
I0731 22:12:50.690006 20406 solver.cpp:375]     Train net output #0: loss = 0.0758334 (* 1 = 0.0758334 loss)
I0731 22:12:50.690011 20406 sgd_solver.cpp:136] Iteration 11100, lr = 1e-05, m = 0.9
I0731 22:13:09.305512 20406 solver.cpp:353] Iteration 11200 (5.37201 iter/s, 18.615s/100 iter), loss = 0.0704695
I0731 22:13:09.305584 20406 solver.cpp:375]     Train net output #0: loss = 0.0704695 (* 1 = 0.0704695 loss)
I0731 22:13:09.305589 20406 sgd_solver.cpp:136] Iteration 11200, lr = 1e-05, m = 0.9
I0731 22:13:15.813253 20346 data_reader.cpp:264] Starting prefetch of epoch 11
I0731 22:13:27.798015 20406 solver.cpp:353] Iteration 11300 (5.40775 iter/s, 18.492s/100 iter), loss = 0.0537021
I0731 22:13:27.798040 20406 solver.cpp:375]     Train net output #0: loss = 0.053702 (* 1 = 0.053702 loss)
I0731 22:13:27.798045 20406 sgd_solver.cpp:136] Iteration 11300, lr = 1e-05, m = 0.9
I0731 22:13:46.346863 20406 solver.cpp:353] Iteration 11400 (5.39132 iter/s, 18.5483s/100 iter), loss = 0.0618375
I0731 22:13:46.346941 20406 solver.cpp:375]     Train net output #0: loss = 0.0618374 (* 1 = 0.0618374 loss)
I0731 22:13:46.346946 20406 sgd_solver.cpp:136] Iteration 11400, lr = 1e-05, m = 0.9
I0731 22:14:04.862558 20406 solver.cpp:353] Iteration 11500 (5.40097 iter/s, 18.5152s/100 iter), loss = 0.0782867
I0731 22:14:04.862581 20406 solver.cpp:375]     Train net output #0: loss = 0.0782866 (* 1 = 0.0782866 loss)
I0731 22:14:04.862586 20406 sgd_solver.cpp:136] Iteration 11500, lr = 1e-05, m = 0.9
I0731 22:14:17.274116 20415 data_reader.cpp:264] Starting prefetch of epoch 4
I0731 22:14:23.541743 20406 solver.cpp:353] Iteration 11600 (5.3537 iter/s, 18.6787s/100 iter), loss = 0.103513
I0731 22:14:23.541766 20406 solver.cpp:375]     Train net output #0: loss = 0.103513 (* 1 = 0.103513 loss)
I0731 22:14:23.541770 20406 sgd_solver.cpp:136] Iteration 11600, lr = 1e-05, m = 0.9
I0731 22:14:42.136912 20406 solver.cpp:353] Iteration 11700 (5.37789 iter/s, 18.5947s/100 iter), loss = 0.0416332
I0731 22:14:42.136939 20406 solver.cpp:375]     Train net output #0: loss = 0.0416331 (* 1 = 0.0416331 loss)
I0731 22:14:42.136943 20406 sgd_solver.cpp:136] Iteration 11700, lr = 1e-05, m = 0.9
I0731 22:15:00.604954 20406 solver.cpp:353] Iteration 11800 (5.41491 iter/s, 18.4675s/100 iter), loss = 0.0590762
I0731 22:15:00.605005 20406 solver.cpp:375]     Train net output #0: loss = 0.0590761 (* 1 = 0.0590761 loss)
I0731 22:15:00.605010 20406 sgd_solver.cpp:136] Iteration 11800, lr = 1e-05, m = 0.9
I0731 22:15:18.402215 20415 data_reader.cpp:264] Starting prefetch of epoch 5
I0731 22:15:19.158397 20406 solver.cpp:353] Iteration 11900 (5.38998 iter/s, 18.5529s/100 iter), loss = 0.063318
I0731 22:15:19.158421 20406 solver.cpp:375]     Train net output #0: loss = 0.0633179 (* 1 = 0.0633179 loss)
I0731 22:15:19.158426 20406 sgd_solver.cpp:136] Iteration 11900, lr = 1e-05, m = 0.9
I0731 22:15:37.448199 20406 solver.cpp:404] Sparsity after update:
I0731 22:15:37.457264 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 22:15:37.457304 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 22:15:37.457320 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 22:15:37.457325 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 22:15:37.457330 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 22:15:37.457334 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 22:15:37.457340 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 22:15:37.457343 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 22:15:37.457347 20406 net.cpp:2270] out3a_param_0(0) 
I0731 22:15:37.457351 20406 net.cpp:2270] out5a_param_0(0) 
I0731 22:15:37.457357 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 22:15:37.457362 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 22:15:37.457367 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 22:15:37.457372 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 22:15:37.457377 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 22:15:37.457383 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 22:15:37.457388 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 22:15:37.457392 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 22:15:37.457398 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 22:15:37.457417 20406 solver.cpp:550] Iteration 12000, Testing net (#0)
I0731 22:15:40.930235 20436 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 22:15:48.670322 20406 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.951711
I0731 22:15:48.670347 20406 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999813
I0731 22:15:48.670353 20406 solver.cpp:635]     Test net output #2: loss = 0.169579 (* 1 = 0.169579 loss)
I0731 22:15:48.670428 20406 solver.cpp:305] [MultiGPU] Tests completed in 11.2127s
I0731 22:15:48.879675 20406 solver.cpp:353] Iteration 12000 (3.36469 iter/s, 29.7205s/100 iter), loss = 0.0383862
I0731 22:15:48.879705 20406 solver.cpp:375]     Train net output #0: loss = 0.0383861 (* 1 = 0.0383861 loss)
I0731 22:15:48.879712 20406 sgd_solver.cpp:136] Iteration 12000, lr = 1e-05, m = 0.9
I0731 22:16:00.438796 20412 data_reader.cpp:264] Starting prefetch of epoch 8
I0731 22:16:07.409868 20406 solver.cpp:353] Iteration 12100 (5.39675 iter/s, 18.5297s/100 iter), loss = 0.088828
I0731 22:16:07.409893 20406 solver.cpp:375]     Train net output #0: loss = 0.0888279 (* 1 = 0.0888279 loss)
I0731 22:16:07.409898 20406 sgd_solver.cpp:136] Iteration 12100, lr = 1e-05, m = 0.9
I0731 22:16:26.030901 20406 solver.cpp:353] Iteration 12200 (5.37042 iter/s, 18.6205s/100 iter), loss = 0.0564426
I0731 22:16:26.030973 20406 solver.cpp:375]     Train net output #0: loss = 0.0564426 (* 1 = 0.0564426 loss)
I0731 22:16:26.030978 20406 sgd_solver.cpp:136] Iteration 12200, lr = 1e-05, m = 0.9
I0731 22:16:44.446223 20406 solver.cpp:353] Iteration 12300 (5.43041 iter/s, 18.4148s/100 iter), loss = 0.0506161
I0731 22:16:44.446246 20406 solver.cpp:375]     Train net output #0: loss = 0.050616 (* 1 = 0.050616 loss)
I0731 22:16:44.446251 20406 sgd_solver.cpp:136] Iteration 12300, lr = 1e-05, m = 0.9
I0731 22:17:01.710795 20412 data_reader.cpp:264] Starting prefetch of epoch 9
I0731 22:17:03.160410 20406 solver.cpp:353] Iteration 12400 (5.34369 iter/s, 18.7137s/100 iter), loss = 0.0525824
I0731 22:17:03.160435 20406 solver.cpp:375]     Train net output #0: loss = 0.0525823 (* 1 = 0.0525823 loss)
I0731 22:17:03.160440 20406 sgd_solver.cpp:136] Iteration 12400, lr = 1e-05, m = 0.9
I0731 22:17:21.767268 20406 solver.cpp:353] Iteration 12500 (5.37451 iter/s, 18.6063s/100 iter), loss = 0.0724006
I0731 22:17:21.767294 20406 solver.cpp:375]     Train net output #0: loss = 0.0724006 (* 1 = 0.0724006 loss)
I0731 22:17:21.767298 20406 sgd_solver.cpp:136] Iteration 12500, lr = 1e-05, m = 0.9
I0731 22:17:32.524216 20410 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 22:17:40.332470 20406 solver.cpp:353] Iteration 12600 (5.38657 iter/s, 18.5647s/100 iter), loss = 0.0455852
I0731 22:17:40.332495 20406 solver.cpp:375]     Train net output #0: loss = 0.0455851 (* 1 = 0.0455851 loss)
I0731 22:17:40.332499 20406 sgd_solver.cpp:136] Iteration 12600, lr = 1e-05, m = 0.9
I0731 22:17:58.981226 20406 solver.cpp:353] Iteration 12700 (5.36244 iter/s, 18.6482s/100 iter), loss = 0.116025
I0731 22:17:58.981250 20406 solver.cpp:375]     Train net output #0: loss = 0.116025 (* 1 = 0.116025 loss)
I0731 22:17:58.981253 20406 sgd_solver.cpp:136] Iteration 12700, lr = 1e-05, m = 0.9
I0731 22:18:18.461277 20406 solver.cpp:353] Iteration 12800 (5.1336 iter/s, 19.4795s/100 iter), loss = 0.046678
I0731 22:18:18.469159 20406 solver.cpp:375]     Train net output #0: loss = 0.0466779 (* 1 = 0.0466779 loss)
I0731 22:18:18.469208 20406 sgd_solver.cpp:136] Iteration 12800, lr = 1e-05, m = 0.9
I0731 22:18:35.177151 20415 data_reader.cpp:264] Starting prefetch of epoch 6
I0731 22:18:37.395769 20406 solver.cpp:353] Iteration 12900 (5.28151 iter/s, 18.934s/100 iter), loss = 0.0751797
I0731 22:18:37.395794 20406 solver.cpp:375]     Train net output #0: loss = 0.0751796 (* 1 = 0.0751796 loss)
I0731 22:18:37.395798 20406 sgd_solver.cpp:136] Iteration 12900, lr = 1e-05, m = 0.9
I0731 22:18:55.926450 20406 solver.cpp:404] Sparsity after update:
I0731 22:18:55.941293 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 22:18:55.941332 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 22:18:55.941346 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 22:18:55.941349 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 22:18:55.941352 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 22:18:55.941355 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 22:18:55.941357 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 22:18:55.941360 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 22:18:55.941362 20406 net.cpp:2270] out3a_param_0(0) 
I0731 22:18:55.941365 20406 net.cpp:2270] out5a_param_0(0) 
I0731 22:18:55.941368 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 22:18:55.941371 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 22:18:55.941375 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 22:18:55.941377 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 22:18:55.941380 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 22:18:55.941382 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 22:18:55.941385 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 22:18:55.941387 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 22:18:55.941390 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 22:18:56.121577 20406 solver.cpp:353] Iteration 13000 (5.34037 iter/s, 18.7253s/100 iter), loss = 0.057671
I0731 22:18:56.121608 20406 solver.cpp:375]     Train net output #0: loss = 0.0576709 (* 1 = 0.0576709 loss)
I0731 22:18:56.121614 20406 sgd_solver.cpp:136] Iteration 13000, lr = 1e-05, m = 0.9
I0731 22:19:14.957764 20406 solver.cpp:353] Iteration 13100 (5.30908 iter/s, 18.8357s/100 iter), loss = 0.0787123
I0731 22:19:14.957789 20406 solver.cpp:375]     Train net output #0: loss = 0.0787122 (* 1 = 0.0787122 loss)
I0731 22:19:14.957793 20406 sgd_solver.cpp:136] Iteration 13100, lr = 1e-05, m = 0.9
I0731 22:19:33.436477 20406 solver.cpp:353] Iteration 13200 (5.41178 iter/s, 18.4782s/100 iter), loss = 0.0535732
I0731 22:19:33.436558 20406 solver.cpp:375]     Train net output #0: loss = 0.0535731 (* 1 = 0.0535731 loss)
I0731 22:19:33.436563 20406 sgd_solver.cpp:136] Iteration 13200, lr = 1e-05, m = 0.9
I0731 22:19:36.981868 20412 data_reader.cpp:264] Starting prefetch of epoch 10
I0731 22:19:52.126524 20406 solver.cpp:353] Iteration 13300 (5.35059 iter/s, 18.6895s/100 iter), loss = 0.0631193
I0731 22:19:52.126549 20406 solver.cpp:375]     Train net output #0: loss = 0.0631192 (* 1 = 0.0631192 loss)
I0731 22:19:52.126554 20406 sgd_solver.cpp:136] Iteration 13300, lr = 1e-05, m = 0.9
I0731 22:20:07.789438 20409 data_reader.cpp:264] Starting prefetch of epoch 9
I0731 22:20:10.730262 20406 solver.cpp:353] Iteration 13400 (5.37541 iter/s, 18.6032s/100 iter), loss = 0.0496634
I0731 22:20:10.730286 20406 solver.cpp:375]     Train net output #0: loss = 0.0496633 (* 1 = 0.0496633 loss)
I0731 22:20:10.730290 20406 sgd_solver.cpp:136] Iteration 13400, lr = 1e-05, m = 0.9
I0731 22:20:29.294134 20406 solver.cpp:353] Iteration 13500 (5.38696 iter/s, 18.5634s/100 iter), loss = 0.0689355
I0731 22:20:29.294160 20406 solver.cpp:375]     Train net output #0: loss = 0.0689354 (* 1 = 0.0689354 loss)
I0731 22:20:29.294164 20406 sgd_solver.cpp:136] Iteration 13500, lr = 1e-05, m = 0.9
I0731 22:20:47.988234 20406 solver.cpp:353] Iteration 13600 (5.34943 iter/s, 18.6936s/100 iter), loss = 0.0741932
I0731 22:20:47.988289 20406 solver.cpp:375]     Train net output #0: loss = 0.0741931 (* 1 = 0.0741931 loss)
I0731 22:20:47.988296 20406 sgd_solver.cpp:136] Iteration 13600, lr = 1e-05, m = 0.9
I0731 22:21:06.499389 20406 solver.cpp:353] Iteration 13700 (5.4023 iter/s, 18.5106s/100 iter), loss = 0.0543656
I0731 22:21:06.499414 20406 solver.cpp:375]     Train net output #0: loss = 0.0543655 (* 1 = 0.0543655 loss)
I0731 22:21:06.499419 20406 sgd_solver.cpp:136] Iteration 13700, lr = 1e-05, m = 0.9
I0731 22:21:09.286660 20409 data_reader.cpp:264] Starting prefetch of epoch 10
I0731 22:21:25.009263 20406 solver.cpp:353] Iteration 13800 (5.40267 iter/s, 18.5094s/100 iter), loss = 0.0559696
I0731 22:21:25.009338 20406 solver.cpp:375]     Train net output #0: loss = 0.0559695 (* 1 = 0.0559695 loss)
I0731 22:21:25.009346 20406 sgd_solver.cpp:136] Iteration 13800, lr = 1e-05, m = 0.9
I0731 22:21:43.667062 20406 solver.cpp:353] Iteration 13900 (5.35984 iter/s, 18.6573s/100 iter), loss = 0.0668986
I0731 22:21:43.667088 20406 solver.cpp:375]     Train net output #0: loss = 0.0668985 (* 1 = 0.0668985 loss)
I0731 22:21:43.667094 20406 sgd_solver.cpp:136] Iteration 13900, lr = 1e-05, m = 0.9
I0731 22:22:02.058190 20406 solver.cpp:404] Sparsity after update:
I0731 22:22:02.068070 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 22:22:02.068145 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 22:22:02.068167 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 22:22:02.068178 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 22:22:02.068188 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 22:22:02.068197 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 22:22:02.068202 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 22:22:02.068205 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 22:22:02.068208 20406 net.cpp:2270] out3a_param_0(0) 
I0731 22:22:02.068212 20406 net.cpp:2270] out5a_param_0(0) 
I0731 22:22:02.068217 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 22:22:02.068222 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 22:22:02.068225 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 22:22:02.068228 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 22:22:02.068233 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 22:22:02.068236 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 22:22:02.068240 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 22:22:02.068244 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 22:22:02.068248 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 22:22:02.068261 20406 solver.cpp:550] Iteration 14000, Testing net (#0)
I0731 22:22:09.104611 20440 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 22:22:13.314414 20406 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.952114
I0731 22:22:13.314438 20406 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999226
I0731 22:22:13.314445 20406 solver.cpp:635]     Test net output #2: loss = 0.20303 (* 1 = 0.20303 loss)
I0731 22:22:13.314509 20406 solver.cpp:305] [MultiGPU] Tests completed in 11.2459s
I0731 22:22:13.526702 20406 solver.cpp:353] Iteration 14000 (3.34909 iter/s, 29.8588s/100 iter), loss = 0.0864668
I0731 22:22:13.526726 20406 solver.cpp:375]     Train net output #0: loss = 0.0864667 (* 1 = 0.0864667 loss)
I0731 22:22:13.526729 20406 sgd_solver.cpp:136] Iteration 14000, lr = 1e-05, m = 0.9
I0731 22:22:21.886937 20409 data_reader.cpp:264] Starting prefetch of epoch 11
I0731 22:22:32.118086 20406 solver.cpp:353] Iteration 14100 (5.37898 iter/s, 18.5909s/100 iter), loss = 0.0871592
I0731 22:22:32.118140 20406 solver.cpp:375]     Train net output #0: loss = 0.0871591 (* 1 = 0.0871591 loss)
I0731 22:22:32.118144 20406 sgd_solver.cpp:136] Iteration 14100, lr = 1e-05, m = 0.9
I0731 22:22:50.655429 20406 solver.cpp:353] Iteration 14200 (5.39467 iter/s, 18.5368s/100 iter), loss = 0.0522425
I0731 22:22:50.655454 20406 solver.cpp:375]     Train net output #0: loss = 0.0522424 (* 1 = 0.0522424 loss)
I0731 22:22:50.655459 20406 sgd_solver.cpp:136] Iteration 14200, lr = 1e-05, m = 0.9
I0731 22:23:09.223960 20406 solver.cpp:353] Iteration 14300 (5.3856 iter/s, 18.568s/100 iter), loss = 0.105552
I0731 22:23:09.224014 20406 solver.cpp:375]     Train net output #0: loss = 0.105551 (* 1 = 0.105551 loss)
I0731 22:23:09.224020 20406 sgd_solver.cpp:136] Iteration 14300, lr = 1e-05, m = 0.9
I0731 22:23:23.309267 20409 data_reader.cpp:264] Starting prefetch of epoch 12
I0731 22:23:27.684556 20406 solver.cpp:353] Iteration 14400 (5.41709 iter/s, 18.4601s/100 iter), loss = 0.078282
I0731 22:23:27.684582 20406 solver.cpp:375]     Train net output #0: loss = 0.0782819 (* 1 = 0.0782819 loss)
I0731 22:23:27.684587 20406 sgd_solver.cpp:136] Iteration 14400, lr = 1e-05, m = 0.9
I0731 22:23:46.245316 20406 solver.cpp:353] Iteration 14500 (5.38786 iter/s, 18.5602s/100 iter), loss = 0.0778428
I0731 22:23:46.245605 20406 solver.cpp:375]     Train net output #0: loss = 0.0778427 (* 1 = 0.0778427 loss)
I0731 22:23:46.245611 20406 sgd_solver.cpp:136] Iteration 14500, lr = 1e-05, m = 0.9
I0731 22:24:04.788801 20406 solver.cpp:353] Iteration 14600 (5.39288 iter/s, 18.543s/100 iter), loss = 0.0653354
I0731 22:24:04.788830 20406 solver.cpp:375]     Train net output #0: loss = 0.0653353 (* 1 = 0.0653353 loss)
I0731 22:24:04.788835 20406 sgd_solver.cpp:136] Iteration 14600, lr = 1e-05, m = 0.9
I0731 22:24:23.291146 20406 solver.cpp:353] Iteration 14700 (5.40487 iter/s, 18.5018s/100 iter), loss = 0.0624376
I0731 22:24:23.291195 20406 solver.cpp:375]     Train net output #0: loss = 0.0624375 (* 1 = 0.0624375 loss)
I0731 22:24:23.291200 20406 sgd_solver.cpp:136] Iteration 14700, lr = 1e-05, m = 0.9
I0731 22:24:24.397909 20415 data_reader.cpp:264] Starting prefetch of epoch 7
I0731 22:24:24.397909 20412 data_reader.cpp:264] Starting prefetch of epoch 11
I0731 22:24:41.829017 20406 solver.cpp:353] Iteration 14800 (5.39451 iter/s, 18.5374s/100 iter), loss = 0.0656156
I0731 22:24:41.829041 20406 solver.cpp:375]     Train net output #0: loss = 0.0656155 (* 1 = 0.0656155 loss)
I0731 22:24:41.829046 20406 sgd_solver.cpp:136] Iteration 14800, lr = 1e-05, m = 0.9
I0731 22:25:00.368556 20406 solver.cpp:353] Iteration 14900 (5.39403 iter/s, 18.539s/100 iter), loss = 0.0778251
I0731 22:25:00.368618 20406 solver.cpp:375]     Train net output #0: loss = 0.077825 (* 1 = 0.077825 loss)
I0731 22:25:00.368624 20406 sgd_solver.cpp:136] Iteration 14900, lr = 1e-05, m = 0.9
I0731 22:25:18.686828 20406 solver.cpp:404] Sparsity after update:
I0731 22:25:18.714058 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 22:25:18.714087 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 22:25:18.714097 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 22:25:18.714098 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 22:25:18.714100 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 22:25:18.714102 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 22:25:18.714104 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 22:25:18.714105 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 22:25:18.714107 20406 net.cpp:2270] out3a_param_0(0) 
I0731 22:25:18.714109 20406 net.cpp:2270] out5a_param_0(0) 
I0731 22:25:18.714112 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 22:25:18.714113 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 22:25:18.714115 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 22:25:18.714118 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 22:25:18.714119 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 22:25:18.714128 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 22:25:18.714130 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 22:25:18.714133 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 22:25:18.714134 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 22:25:18.883098 20406 solver.cpp:353] Iteration 15000 (5.40131 iter/s, 18.514s/100 iter), loss = 0.0579527
I0731 22:25:18.883126 20406 solver.cpp:375]     Train net output #0: loss = 0.0579526 (* 1 = 0.0579526 loss)
I0731 22:25:18.883129 20406 sgd_solver.cpp:136] Iteration 15000, lr = 1e-05, m = 0.9
I0731 22:25:25.755148 20412 data_reader.cpp:264] Starting prefetch of epoch 12
I0731 22:25:37.395508 20406 solver.cpp:353] Iteration 15100 (5.40193 iter/s, 18.5119s/100 iter), loss = 0.0888706
I0731 22:25:37.396010 20406 solver.cpp:375]     Train net output #0: loss = 0.0888705 (* 1 = 0.0888705 loss)
I0731 22:25:37.396018 20406 sgd_solver.cpp:136] Iteration 15100, lr = 1e-05, m = 0.9
I0731 22:25:55.950124 20406 solver.cpp:353] Iteration 15200 (5.38964 iter/s, 18.5541s/100 iter), loss = 0.077178
I0731 22:25:55.950152 20406 solver.cpp:375]     Train net output #0: loss = 0.0771779 (* 1 = 0.0771779 loss)
I0731 22:25:55.950156 20406 sgd_solver.cpp:136] Iteration 15200, lr = 1e-05, m = 0.9
I0731 22:25:56.355211 20410 data_reader.cpp:264] Starting prefetch of epoch 4
I0731 22:26:14.499464 20406 solver.cpp:353] Iteration 15300 (5.39118 iter/s, 18.5488s/100 iter), loss = 0.0363722
I0731 22:26:14.499532 20406 solver.cpp:375]     Train net output #0: loss = 0.0363721 (* 1 = 0.0363721 loss)
I0731 22:26:14.499538 20406 sgd_solver.cpp:136] Iteration 15300, lr = 1e-05, m = 0.9
I0731 22:26:32.837043 20406 solver.cpp:353] Iteration 15400 (5.45343 iter/s, 18.3371s/100 iter), loss = 0.0712054
I0731 22:26:32.837064 20406 solver.cpp:375]     Train net output #0: loss = 0.0712053 (* 1 = 0.0712053 loss)
I0731 22:26:32.837067 20406 sgd_solver.cpp:136] Iteration 15400, lr = 1e-05, m = 0.9
I0731 22:26:51.374306 20406 solver.cpp:353] Iteration 15500 (5.39469 iter/s, 18.5368s/100 iter), loss = 0.0720965
I0731 22:26:51.374366 20406 solver.cpp:375]     Train net output #0: loss = 0.0720964 (* 1 = 0.0720964 loss)
I0731 22:26:51.374373 20406 sgd_solver.cpp:136] Iteration 15500, lr = 1e-05, m = 0.9
I0731 22:26:57.546051 20364 data_reader.cpp:264] Starting prefetch of epoch 10
I0731 22:27:09.956176 20406 solver.cpp:353] Iteration 15600 (5.38174 iter/s, 18.5814s/100 iter), loss = 0.0608944
I0731 22:27:09.956202 20406 solver.cpp:375]     Train net output #0: loss = 0.0608943 (* 1 = 0.0608943 loss)
I0731 22:27:09.956207 20406 sgd_solver.cpp:136] Iteration 15600, lr = 1e-05, m = 0.9
I0731 22:27:28.480444 20406 solver.cpp:353] Iteration 15700 (5.39847 iter/s, 18.5238s/100 iter), loss = 0.0735068
I0731 22:27:28.480525 20406 solver.cpp:375]     Train net output #0: loss = 0.0735067 (* 1 = 0.0735067 loss)
I0731 22:27:28.480530 20406 sgd_solver.cpp:136] Iteration 15700, lr = 1e-05, m = 0.9
I0731 22:27:46.976012 20406 solver.cpp:353] Iteration 15800 (5.40685 iter/s, 18.4951s/100 iter), loss = 0.0705278
I0731 22:27:46.976042 20406 solver.cpp:375]     Train net output #0: loss = 0.0705277 (* 1 = 0.0705277 loss)
I0731 22:27:46.976045 20406 sgd_solver.cpp:136] Iteration 15800, lr = 1e-05, m = 0.9
I0731 22:27:58.698298 20364 data_reader.cpp:264] Starting prefetch of epoch 11
I0731 22:28:05.492270 20406 solver.cpp:353] Iteration 15900 (5.40081 iter/s, 18.5157s/100 iter), loss = 0.0512759
I0731 22:28:05.492295 20406 solver.cpp:375]     Train net output #0: loss = 0.0512758 (* 1 = 0.0512758 loss)
I0731 22:28:05.492300 20406 sgd_solver.cpp:136] Iteration 15900, lr = 1e-05, m = 0.9
I0731 22:28:24.011626 20406 solver.cpp:404] Sparsity after update:
I0731 22:28:24.019908 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 22:28:24.019965 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 22:28:24.019981 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 22:28:24.020000 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 22:28:24.020011 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 22:28:24.020022 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 22:28:24.020031 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 22:28:24.020041 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 22:28:24.020051 20406 net.cpp:2270] out3a_param_0(0) 
I0731 22:28:24.020059 20406 net.cpp:2270] out5a_param_0(0) 
I0731 22:28:24.020066 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 22:28:24.020074 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 22:28:24.020083 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 22:28:24.020090 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 22:28:24.020099 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 22:28:24.020107 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 22:28:24.020114 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 22:28:24.020122 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 22:28:24.020129 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 22:28:24.020146 20406 solver.cpp:550] Iteration 16000, Testing net (#0)
I0731 22:28:27.421789 20440 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 22:28:35.163816 20406 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.952327
I0731 22:28:35.163935 20406 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999847
I0731 22:28:35.163944 20406 solver.cpp:635]     Test net output #2: loss = 0.168726 (* 1 = 0.168726 loss)
I0731 22:28:35.163975 20406 solver.cpp:305] [MultiGPU] Tests completed in 11.1435s
I0731 22:28:35.349607 20406 solver.cpp:353] Iteration 16000 (3.34935 iter/s, 29.8565s/100 iter), loss = 0.0764273
I0731 22:28:35.349629 20406 solver.cpp:375]     Train net output #0: loss = 0.0764272 (* 1 = 0.0764272 loss)
I0731 22:28:35.349634 20406 sgd_solver.cpp:136] Iteration 16000, lr = 1e-05, m = 0.9
I0731 22:28:40.547849 20364 data_reader.cpp:264] Starting prefetch of epoch 12
I0731 22:28:53.809666 20406 solver.cpp:353] Iteration 16100 (5.41725 iter/s, 18.4595s/100 iter), loss = 0.0727638
I0731 22:28:53.809693 20406 solver.cpp:375]     Train net output #0: loss = 0.0727637 (* 1 = 0.0727637 loss)
I0731 22:28:53.809698 20406 sgd_solver.cpp:136] Iteration 16100, lr = 1e-05, m = 0.9
I0731 22:29:12.283141 20406 solver.cpp:353] Iteration 16200 (5.41332 iter/s, 18.473s/100 iter), loss = 0.078281
I0731 22:29:12.283195 20406 solver.cpp:375]     Train net output #0: loss = 0.0782809 (* 1 = 0.0782809 loss)
I0731 22:29:12.283201 20406 sgd_solver.cpp:136] Iteration 16200, lr = 1e-05, m = 0.9
I0731 22:29:30.821733 20406 solver.cpp:353] Iteration 16300 (5.3943 iter/s, 18.5381s/100 iter), loss = 0.0705002
I0731 22:29:30.821761 20406 solver.cpp:375]     Train net output #0: loss = 0.0705001 (* 1 = 0.0705001 loss)
I0731 22:29:30.821765 20406 sgd_solver.cpp:136] Iteration 16300, lr = 1e-05, m = 0.9
I0731 22:29:41.699110 20364 data_reader.cpp:264] Starting prefetch of epoch 13
I0731 22:29:49.242982 20406 solver.cpp:353] Iteration 16400 (5.42866 iter/s, 18.4207s/100 iter), loss = 0.0426735
I0731 22:29:49.243039 20406 solver.cpp:375]     Train net output #0: loss = 0.0426734 (* 1 = 0.0426734 loss)
I0731 22:29:49.243044 20406 sgd_solver.cpp:136] Iteration 16400, lr = 1e-05, m = 0.9
I0731 22:30:08.469020 20406 solver.cpp:353] Iteration 16500 (5.20142 iter/s, 19.2255s/100 iter), loss = 0.0657932
I0731 22:30:08.469044 20406 solver.cpp:375]     Train net output #0: loss = 0.0657931 (* 1 = 0.0657931 loss)
I0731 22:30:08.469050 20406 sgd_solver.cpp:136] Iteration 16500, lr = 1e-05, m = 0.9
I0731 22:30:12.943063 20409 data_reader.cpp:264] Starting prefetch of epoch 13
I0731 22:30:26.922237 20406 solver.cpp:353] Iteration 16600 (5.41926 iter/s, 18.4527s/100 iter), loss = 0.0681183
I0731 22:30:26.922289 20406 solver.cpp:375]     Train net output #0: loss = 0.0681182 (* 1 = 0.0681182 loss)
I0731 22:30:26.922294 20406 sgd_solver.cpp:136] Iteration 16600, lr = 1e-05, m = 0.9
I0731 22:30:45.409776 20406 solver.cpp:353] Iteration 16700 (5.4092 iter/s, 18.487s/100 iter), loss = 0.0768582
I0731 22:30:45.409801 20406 solver.cpp:375]     Train net output #0: loss = 0.076858 (* 1 = 0.076858 loss)
I0731 22:30:45.409807 20406 sgd_solver.cpp:136] Iteration 16700, lr = 1e-05, m = 0.9
I0731 22:31:03.858206 20406 solver.cpp:353] Iteration 16800 (5.42067 iter/s, 18.4479s/100 iter), loss = 0.077025
I0731 22:31:03.858264 20406 solver.cpp:375]     Train net output #0: loss = 0.0770249 (* 1 = 0.0770249 loss)
I0731 22:31:03.858269 20406 sgd_solver.cpp:136] Iteration 16800, lr = 1e-05, m = 0.9
I0731 22:31:14.055524 20364 data_reader.cpp:264] Starting prefetch of epoch 14
I0731 22:31:22.365020 20406 solver.cpp:353] Iteration 16900 (5.40356 iter/s, 18.5063s/100 iter), loss = 0.064495
I0731 22:31:22.365048 20406 solver.cpp:375]     Train net output #0: loss = 0.0644949 (* 1 = 0.0644949 loss)
I0731 22:31:22.365052 20406 sgd_solver.cpp:136] Iteration 16900, lr = 1e-05, m = 0.9
I0731 22:31:40.773885 20406 solver.cpp:404] Sparsity after update:
I0731 22:31:40.802433 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 22:31:40.802450 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 22:31:40.802458 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 22:31:40.802459 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 22:31:40.802461 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 22:31:40.802464 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 22:31:40.802465 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 22:31:40.802467 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 22:31:40.802469 20406 net.cpp:2270] out3a_param_0(0) 
I0731 22:31:40.802471 20406 net.cpp:2270] out5a_param_0(0) 
I0731 22:31:40.802474 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 22:31:40.802474 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 22:31:40.802476 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 22:31:40.802479 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 22:31:40.802480 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 22:31:40.802482 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 22:31:40.802484 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 22:31:40.802489 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 22:31:40.802490 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 22:31:40.971263 20406 solver.cpp:353] Iteration 17000 (5.37469 iter/s, 18.6057s/100 iter), loss = 0.0967323
I0731 22:31:40.971289 20406 solver.cpp:375]     Train net output #0: loss = 0.0967322 (* 1 = 0.0967322 loss)
I0731 22:31:40.971294 20406 sgd_solver.cpp:136] Iteration 17000, lr = 1e-05, m = 0.9
I0731 22:31:59.504187 20406 solver.cpp:353] Iteration 17100 (5.39595 iter/s, 18.5324s/100 iter), loss = 0.0692452
I0731 22:31:59.504211 20406 solver.cpp:375]     Train net output #0: loss = 0.0692451 (* 1 = 0.0692451 loss)
I0731 22:31:59.504216 20406 sgd_solver.cpp:136] Iteration 17100, lr = 1e-05, m = 0.9
I0731 22:32:15.242388 20412 data_reader.cpp:264] Starting prefetch of epoch 13
I0731 22:32:18.009256 20406 solver.cpp:353] Iteration 17200 (5.40407 iter/s, 18.5046s/100 iter), loss = 0.0813413
I0731 22:32:18.009282 20406 solver.cpp:375]     Train net output #0: loss = 0.0813412 (* 1 = 0.0813412 loss)
I0731 22:32:18.009289 20406 sgd_solver.cpp:136] Iteration 17200, lr = 1e-05, m = 0.9
I0731 22:32:36.450795 20406 solver.cpp:353] Iteration 17300 (5.42269 iter/s, 18.441s/100 iter), loss = 0.0510494
I0731 22:32:36.450826 20406 solver.cpp:375]     Train net output #0: loss = 0.0510493 (* 1 = 0.0510493 loss)
I0731 22:32:36.450834 20406 sgd_solver.cpp:136] Iteration 17300, lr = 1e-05, m = 0.9
I0731 22:32:45.972179 20346 data_reader.cpp:264] Starting prefetch of epoch 12
I0731 22:32:55.035815 20406 solver.cpp:353] Iteration 17400 (5.38083 iter/s, 18.5845s/100 iter), loss = 0.0539015
I0731 22:32:55.035838 20406 solver.cpp:375]     Train net output #0: loss = 0.0539014 (* 1 = 0.0539014 loss)
I0731 22:32:55.035842 20406 sgd_solver.cpp:136] Iteration 17400, lr = 1e-05, m = 0.9
I0731 22:33:13.565027 20406 solver.cpp:353] Iteration 17500 (5.39703 iter/s, 18.5287s/100 iter), loss = 0.045859
I0731 22:33:13.565049 20406 solver.cpp:375]     Train net output #0: loss = 0.0458589 (* 1 = 0.0458589 loss)
I0731 22:33:13.565053 20406 sgd_solver.cpp:136] Iteration 17500, lr = 1e-05, m = 0.9
I0731 22:33:32.675581 20406 solver.cpp:353] Iteration 17600 (5.23285 iter/s, 19.11s/100 iter), loss = 0.0769661
I0731 22:33:32.675688 20406 solver.cpp:375]     Train net output #0: loss = 0.076966 (* 1 = 0.076966 loss)
I0731 22:33:32.675695 20406 sgd_solver.cpp:136] Iteration 17600, lr = 1e-05, m = 0.9
I0731 22:33:47.759150 20412 data_reader.cpp:264] Starting prefetch of epoch 14
I0731 22:33:51.355846 20406 solver.cpp:353] Iteration 17700 (5.35339 iter/s, 18.6798s/100 iter), loss = 0.0616923
I0731 22:33:51.355871 20406 solver.cpp:375]     Train net output #0: loss = 0.0616922 (* 1 = 0.0616922 loss)
I0731 22:33:51.355876 20406 sgd_solver.cpp:136] Iteration 17700, lr = 1e-05, m = 0.9
I0731 22:34:09.889245 20406 solver.cpp:353] Iteration 17800 (5.39581 iter/s, 18.5329s/100 iter), loss = 0.0590552
I0731 22:34:09.889364 20406 solver.cpp:375]     Train net output #0: loss = 0.0590551 (* 1 = 0.0590551 loss)
I0731 22:34:09.889369 20406 sgd_solver.cpp:136] Iteration 17800, lr = 1e-05, m = 0.9
I0731 22:34:28.399505 20406 solver.cpp:353] Iteration 17900 (5.40256 iter/s, 18.5098s/100 iter), loss = 0.0815268
I0731 22:34:28.399531 20406 solver.cpp:375]     Train net output #0: loss = 0.0815267 (* 1 = 0.0815267 loss)
I0731 22:34:28.399535 20406 sgd_solver.cpp:136] Iteration 17900, lr = 1e-05, m = 0.9
I0731 22:34:46.826324 20406 solver.cpp:404] Sparsity after update:
I0731 22:34:46.836097 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 22:34:46.836115 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 22:34:46.836122 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 22:34:46.836124 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 22:34:46.836127 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 22:34:46.836128 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 22:34:46.836130 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 22:34:46.836133 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 22:34:46.836134 20406 net.cpp:2270] out3a_param_0(0) 
I0731 22:34:46.836136 20406 net.cpp:2270] out5a_param_0(0) 
I0731 22:34:46.836138 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 22:34:46.836139 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 22:34:46.836143 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 22:34:46.836145 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 22:34:46.836146 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 22:34:46.836148 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 22:34:46.836150 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 22:34:46.836153 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 22:34:46.836154 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 22:34:46.836163 20406 solver.cpp:550] Iteration 18000, Testing net (#0)
I0731 22:34:53.971143 20436 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 22:34:58.095144 20406 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.952041
I0731 22:34:58.095162 20406 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999296
I0731 22:34:58.095168 20406 solver.cpp:635]     Test net output #2: loss = 0.20741 (* 1 = 0.20741 loss)
I0731 22:34:58.095253 20406 solver.cpp:305] [MultiGPU] Tests completed in 11.2588s
I0731 22:34:58.284867 20406 solver.cpp:353] Iteration 18000 (3.34621 iter/s, 29.8845s/100 iter), loss = 0.0482773
I0731 22:34:58.284895 20406 solver.cpp:375]     Train net output #0: loss = 0.0482773 (* 1 = 0.0482773 loss)
I0731 22:34:58.284903 20406 sgd_solver.cpp:136] Iteration 18000, lr = 1e-05, m = 0.9
I0731 22:35:00.619935 20410 data_reader.cpp:264] Starting prefetch of epoch 5
I0731 22:35:16.883488 20406 solver.cpp:353] Iteration 18100 (5.37689 iter/s, 18.5981s/100 iter), loss = 0.0539076
I0731 22:35:16.883543 20406 solver.cpp:375]     Train net output #0: loss = 0.0539076 (* 1 = 0.0539076 loss)
I0731 22:35:16.883548 20406 sgd_solver.cpp:136] Iteration 18100, lr = 1e-05, m = 0.9
I0731 22:35:35.474287 20406 solver.cpp:353] Iteration 18200 (5.37915 iter/s, 18.5903s/100 iter), loss = 0.0636319
I0731 22:35:35.474311 20406 solver.cpp:375]     Train net output #0: loss = 0.0636318 (* 1 = 0.0636318 loss)
I0731 22:35:35.474315 20406 sgd_solver.cpp:136] Iteration 18200, lr = 1e-05, m = 0.9
I0731 22:35:54.081684 20406 solver.cpp:353] Iteration 18300 (5.37436 iter/s, 18.6069s/100 iter), loss = 0.0722368
I0731 22:35:54.081742 20406 solver.cpp:375]     Train net output #0: loss = 0.0722367 (* 1 = 0.0722367 loss)
I0731 22:35:54.081748 20406 sgd_solver.cpp:136] Iteration 18300, lr = 1e-05, m = 0.9
I0731 22:36:01.996889 20412 data_reader.cpp:264] Starting prefetch of epoch 15
I0731 22:36:12.764571 20406 solver.cpp:353] Iteration 18400 (5.35264 iter/s, 18.6824s/100 iter), loss = 0.0652431
I0731 22:36:12.764600 20406 solver.cpp:375]     Train net output #0: loss = 0.0652431 (* 1 = 0.0652431 loss)
I0731 22:36:12.764605 20406 sgd_solver.cpp:136] Iteration 18400, lr = 1e-05, m = 0.9
I0731 22:36:31.474684 20406 solver.cpp:353] Iteration 18500 (5.34485 iter/s, 18.7096s/100 iter), loss = 0.0768533
I0731 22:36:31.474768 20406 solver.cpp:375]     Train net output #0: loss = 0.0768532 (* 1 = 0.0768532 loss)
I0731 22:36:31.474776 20406 sgd_solver.cpp:136] Iteration 18500, lr = 1e-05, m = 0.9
I0731 22:36:50.226991 20406 solver.cpp:353] Iteration 18600 (5.33282 iter/s, 18.7518s/100 iter), loss = 0.0613751
I0731 22:36:50.227020 20406 solver.cpp:375]     Train net output #0: loss = 0.061375 (* 1 = 0.061375 loss)
I0731 22:36:50.227023 20406 sgd_solver.cpp:136] Iteration 18600, lr = 1e-05, m = 0.9
I0731 22:37:03.747638 20364 data_reader.cpp:264] Starting prefetch of epoch 15
I0731 22:37:08.714085 20406 solver.cpp:353] Iteration 18700 (5.40933 iter/s, 18.4866s/100 iter), loss = 0.0454252
I0731 22:37:08.714112 20406 solver.cpp:375]     Train net output #0: loss = 0.0454251 (* 1 = 0.0454251 loss)
I0731 22:37:08.714115 20406 sgd_solver.cpp:136] Iteration 18700, lr = 1e-05, m = 0.9
I0731 22:37:27.639310 20406 solver.cpp:353] Iteration 18800 (5.2841 iter/s, 18.9247s/100 iter), loss = 0.107652
I0731 22:37:27.639340 20406 solver.cpp:375]     Train net output #0: loss = 0.107651 (* 1 = 0.107651 loss)
I0731 22:37:27.639346 20406 sgd_solver.cpp:136] Iteration 18800, lr = 1e-05, m = 0.9
I0731 22:37:34.692612 20410 data_reader.cpp:264] Starting prefetch of epoch 6
I0731 22:37:46.188537 20406 solver.cpp:353] Iteration 18900 (5.39121 iter/s, 18.5487s/100 iter), loss = 0.0654242
I0731 22:37:46.188565 20406 solver.cpp:375]     Train net output #0: loss = 0.0654241 (* 1 = 0.0654241 loss)
I0731 22:37:46.188570 20406 sgd_solver.cpp:136] Iteration 18900, lr = 1e-05, m = 0.9
I0731 22:38:04.664296 20406 solver.cpp:404] Sparsity after update:
I0731 22:38:04.680946 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 22:38:04.681031 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 22:38:04.681059 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 22:38:04.681066 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 22:38:04.681071 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 22:38:04.681074 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 22:38:04.681078 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 22:38:04.681082 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 22:38:04.681087 20406 net.cpp:2270] out3a_param_0(0) 
I0731 22:38:04.681092 20406 net.cpp:2270] out5a_param_0(0) 
I0731 22:38:04.681100 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 22:38:04.681105 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 22:38:04.681109 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 22:38:04.681113 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 22:38:04.681118 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 22:38:04.681123 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 22:38:04.681126 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 22:38:04.681130 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 22:38:04.681135 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 22:38:04.867835 20406 solver.cpp:353] Iteration 19000 (5.35367 iter/s, 18.6788s/100 iter), loss = 0.08694
I0731 22:38:04.867888 20406 solver.cpp:375]     Train net output #0: loss = 0.0869399 (* 1 = 0.0869399 loss)
I0731 22:38:04.867895 20406 sgd_solver.cpp:136] Iteration 19000, lr = 1e-05, m = 0.9
I0731 22:38:23.613698 20406 solver.cpp:353] Iteration 19100 (5.33466 iter/s, 18.7453s/100 iter), loss = 0.0527304
I0731 22:38:23.613718 20406 solver.cpp:375]     Train net output #0: loss = 0.0527303 (* 1 = 0.0527303 loss)
I0731 22:38:23.613723 20406 sgd_solver.cpp:136] Iteration 19100, lr = 1e-05, m = 0.9
I0731 22:38:36.460886 20415 data_reader.cpp:264] Starting prefetch of epoch 8
I0731 22:38:42.182375 20406 solver.cpp:353] Iteration 19200 (5.38556 iter/s, 18.5682s/100 iter), loss = 0.0822689
I0731 22:38:42.182399 20406 solver.cpp:375]     Train net output #0: loss = 0.0822688 (* 1 = 0.0822688 loss)
I0731 22:38:42.182402 20406 sgd_solver.cpp:136] Iteration 19200, lr = 1e-05, m = 0.9
I0731 22:39:00.806689 20406 solver.cpp:353] Iteration 19300 (5.36948 iter/s, 18.6238s/100 iter), loss = 0.0880464
I0731 22:39:00.806715 20406 solver.cpp:375]     Train net output #0: loss = 0.0880463 (* 1 = 0.0880463 loss)
I0731 22:39:00.806722 20406 sgd_solver.cpp:136] Iteration 19300, lr = 1e-05, m = 0.9
I0731 22:39:19.339581 20406 solver.cpp:353] Iteration 19400 (5.39596 iter/s, 18.5324s/100 iter), loss = 0.0917561
I0731 22:39:19.339658 20406 solver.cpp:375]     Train net output #0: loss = 0.091756 (* 1 = 0.091756 loss)
I0731 22:39:19.339663 20406 sgd_solver.cpp:136] Iteration 19400, lr = 1e-05, m = 0.9
I0731 22:39:37.639943 20415 data_reader.cpp:264] Starting prefetch of epoch 9
I0731 22:39:37.813586 20406 solver.cpp:353] Iteration 19500 (5.41316 iter/s, 18.4735s/100 iter), loss = 0.0708254
I0731 22:39:37.813611 20406 solver.cpp:375]     Train net output #0: loss = 0.0708253 (* 1 = 0.0708253 loss)
I0731 22:39:37.813616 20406 sgd_solver.cpp:136] Iteration 19500, lr = 1e-05, m = 0.9
I0731 22:39:56.331344 20406 solver.cpp:353] Iteration 19600 (5.40037 iter/s, 18.5172s/100 iter), loss = 0.0813626
I0731 22:39:56.331430 20406 solver.cpp:375]     Train net output #0: loss = 0.0813625 (* 1 = 0.0813625 loss)
I0731 22:39:56.331437 20406 sgd_solver.cpp:136] Iteration 19600, lr = 1e-05, m = 0.9
I0731 22:40:08.487987 20410 data_reader.cpp:264] Starting prefetch of epoch 7
I0731 22:40:14.975004 20406 solver.cpp:353] Iteration 19700 (5.3639 iter/s, 18.6431s/100 iter), loss = 0.369294
I0731 22:40:14.975029 20406 solver.cpp:375]     Train net output #0: loss = 0.369294 (* 1 = 0.369294 loss)
I0731 22:40:14.975033 20406 sgd_solver.cpp:136] Iteration 19700, lr = 1e-05, m = 0.9
I0731 22:40:33.464076 20406 solver.cpp:353] Iteration 19800 (5.40875 iter/s, 18.4886s/100 iter), loss = 0.0685171
I0731 22:40:33.464128 20406 solver.cpp:375]     Train net output #0: loss = 0.068517 (* 1 = 0.068517 loss)
I0731 22:40:33.464133 20406 sgd_solver.cpp:136] Iteration 19800, lr = 1e-05, m = 0.9
I0731 22:40:53.869384 20406 solver.cpp:353] Iteration 19900 (4.90082 iter/s, 20.4047s/100 iter), loss = 0.0647307
I0731 22:40:53.869426 20406 solver.cpp:375]     Train net output #0: loss = 0.0647306 (* 1 = 0.0647306 loss)
I0731 22:40:53.869433 20406 sgd_solver.cpp:136] Iteration 19900, lr = 1e-05, m = 0.9
I0731 22:41:13.531352 20415 data_reader.cpp:264] Starting prefetch of epoch 10
I0731 22:41:14.272343 20406 solver.cpp:680] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/sparse/cityscapes5_jsegnet21v2_iter_20000.caffemodel
I0731 22:41:14.435808 20406 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/sparse/cityscapes5_jsegnet21v2_iter_20000.solverstate
I0731 22:41:14.447885 20406 solver.cpp:404] Sparsity after update:
I0731 22:41:14.452733 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 22:41:14.452751 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 22:41:14.452760 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 22:41:14.452764 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 22:41:14.452766 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 22:41:14.452770 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 22:41:14.452774 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 22:41:14.452776 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 22:41:14.452780 20406 net.cpp:2270] out3a_param_0(0) 
I0731 22:41:14.452782 20406 net.cpp:2270] out5a_param_0(0) 
I0731 22:41:14.452785 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 22:41:14.452788 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 22:41:14.452792 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 22:41:14.452795 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 22:41:14.452806 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 22:41:14.452811 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 22:41:14.452821 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 22:41:14.452823 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 22:41:14.452826 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 22:41:14.452841 20406 solver.cpp:550] Iteration 20000, Testing net (#0)
I0731 22:41:27.334867 20404 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 22:41:27.933938 20406 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.951425
I0731 22:41:27.933964 20406 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999856
I0731 22:41:27.933969 20406 solver.cpp:635]     Test net output #2: loss = 0.17009 (* 1 = 0.17009 loss)
I0731 22:41:27.933997 20406 solver.cpp:305] [MultiGPU] Tests completed in 13.4808s
I0731 22:41:28.136939 20406 solver.cpp:353] Iteration 20000 (2.91829 iter/s, 34.2666s/100 iter), loss = 0.0482651
I0731 22:41:28.136967 20406 solver.cpp:375]     Train net output #0: loss = 0.048265 (* 1 = 0.048265 loss)
I0731 22:41:28.136975 20406 sgd_solver.cpp:136] Iteration 20000, lr = 1e-05, m = 0.9
I0731 22:41:46.601910 20406 solver.cpp:353] Iteration 20100 (5.41581 iter/s, 18.4645s/100 iter), loss = 0.0427278
I0731 22:41:46.601992 20406 solver.cpp:375]     Train net output #0: loss = 0.0427277 (* 1 = 0.0427277 loss)
I0731 22:41:46.602000 20406 sgd_solver.cpp:136] Iteration 20100, lr = 1e-05, m = 0.9
I0731 22:41:57.773510 20410 data_reader.cpp:264] Starting prefetch of epoch 8
I0731 22:42:05.101105 20406 solver.cpp:353] Iteration 20200 (5.40579 iter/s, 18.4987s/100 iter), loss = 0.0872486
I0731 22:42:05.101130 20406 solver.cpp:375]     Train net output #0: loss = 0.0872486 (* 1 = 0.0872486 loss)
I0731 22:42:05.101133 20406 sgd_solver.cpp:136] Iteration 20200, lr = 1e-05, m = 0.9
I0731 22:42:23.637199 20406 solver.cpp:353] Iteration 20300 (5.39503 iter/s, 18.5356s/100 iter), loss = 0.0626739
I0731 22:42:23.637256 20406 solver.cpp:375]     Train net output #0: loss = 0.0626738 (* 1 = 0.0626738 loss)
I0731 22:42:23.637261 20406 sgd_solver.cpp:136] Iteration 20300, lr = 1e-05, m = 0.9
I0731 22:42:42.181447 20406 solver.cpp:353] Iteration 20400 (5.39266 iter/s, 18.5437s/100 iter), loss = 0.0671409
I0731 22:42:42.181478 20406 solver.cpp:375]     Train net output #0: loss = 0.0671408 (* 1 = 0.0671408 loss)
I0731 22:42:42.181483 20406 sgd_solver.cpp:136] Iteration 20400, lr = 1e-05, m = 0.9
I0731 22:42:59.160432 20412 data_reader.cpp:264] Starting prefetch of epoch 16
I0731 22:43:00.822048 20406 solver.cpp:353] Iteration 20500 (5.36478 iter/s, 18.6401s/100 iter), loss = 0.0578302
I0731 22:43:00.822073 20406 solver.cpp:375]     Train net output #0: loss = 0.0578301 (* 1 = 0.0578301 loss)
I0731 22:43:00.822078 20406 sgd_solver.cpp:136] Iteration 20500, lr = 1e-05, m = 0.9
I0731 22:43:19.306592 20406 solver.cpp:353] Iteration 20600 (5.41008 iter/s, 18.484s/100 iter), loss = 0.0548095
I0731 22:43:19.306614 20406 solver.cpp:375]     Train net output #0: loss = 0.0548094 (* 1 = 0.0548094 loss)
I0731 22:43:19.306619 20406 sgd_solver.cpp:136] Iteration 20600, lr = 1e-05, m = 0.9
I0731 22:43:37.853991 20406 solver.cpp:353] Iteration 20700 (5.39174 iter/s, 18.5469s/100 iter), loss = 0.0465325
I0731 22:43:37.854049 20406 solver.cpp:375]     Train net output #0: loss = 0.0465325 (* 1 = 0.0465325 loss)
I0731 22:43:37.854054 20406 sgd_solver.cpp:136] Iteration 20700, lr = 1e-05, m = 0.9
I0731 22:43:56.429612 20406 solver.cpp:353] Iteration 20800 (5.38355 iter/s, 18.5751s/100 iter), loss = 0.0410007
I0731 22:43:56.429636 20406 solver.cpp:375]     Train net output #0: loss = 0.0410006 (* 1 = 0.0410006 loss)
I0731 22:43:56.429641 20406 sgd_solver.cpp:136] Iteration 20800, lr = 1e-05, m = 0.9
I0731 22:44:00.290374 20364 data_reader.cpp:264] Starting prefetch of epoch 16
I0731 22:44:14.771960 20406 solver.cpp:353] Iteration 20900 (5.45201 iter/s, 18.3418s/100 iter), loss = 0.0490235
I0731 22:44:14.772075 20406 solver.cpp:375]     Train net output #0: loss = 0.0490234 (* 1 = 0.0490234 loss)
I0731 22:44:14.772083 20406 sgd_solver.cpp:136] Iteration 20900, lr = 1e-05, m = 0.9
I0731 22:44:30.925614 20346 data_reader.cpp:264] Starting prefetch of epoch 13
I0731 22:44:33.206984 20406 solver.cpp:404] Sparsity after update:
I0731 22:44:33.228220 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 22:44:33.228322 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 22:44:33.228350 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 22:44:33.228368 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 22:44:33.228384 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 22:44:33.228399 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 22:44:33.228415 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 22:44:33.228428 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 22:44:33.228440 20406 net.cpp:2270] out3a_param_0(0) 
I0731 22:44:33.228461 20406 net.cpp:2270] out5a_param_0(0) 
I0731 22:44:33.228474 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 22:44:33.228488 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 22:44:33.228502 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 22:44:33.228515 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 22:44:33.228529 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 22:44:33.228544 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 22:44:33.228559 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 22:44:33.228572 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 22:44:33.228585 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 22:44:33.401351 20406 solver.cpp:353] Iteration 21000 (5.36801 iter/s, 18.6289s/100 iter), loss = 0.0754625
I0731 22:44:33.401377 20406 solver.cpp:375]     Train net output #0: loss = 0.0754624 (* 1 = 0.0754624 loss)
I0731 22:44:33.401381 20406 sgd_solver.cpp:136] Iteration 21000, lr = 1e-05, m = 0.9
I0731 22:44:51.931226 20406 solver.cpp:353] Iteration 21100 (5.39684 iter/s, 18.5294s/100 iter), loss = 0.053391
I0731 22:44:51.931298 20406 solver.cpp:375]     Train net output #0: loss = 0.0533909 (* 1 = 0.0533909 loss)
I0731 22:44:51.931304 20406 sgd_solver.cpp:136] Iteration 21100, lr = 1e-05, m = 0.9
I0731 22:45:10.558574 20406 solver.cpp:353] Iteration 21200 (5.3686 iter/s, 18.6268s/100 iter), loss = 0.0628085
I0731 22:45:10.558598 20406 solver.cpp:375]     Train net output #0: loss = 0.0628085 (* 1 = 0.0628085 loss)
I0731 22:45:10.558603 20406 sgd_solver.cpp:136] Iteration 21200, lr = 1e-05, m = 0.9
I0731 22:45:29.837936 20406 solver.cpp:353] Iteration 21300 (5.18704 iter/s, 19.2788s/100 iter), loss = 0.0781118
I0731 22:45:29.838060 20406 solver.cpp:375]     Train net output #0: loss = 0.0781117 (* 1 = 0.0781117 loss)
I0731 22:45:29.838075 20406 sgd_solver.cpp:136] Iteration 21300, lr = 1e-05, m = 0.9
I0731 22:45:33.198043 20410 data_reader.cpp:264] Starting prefetch of epoch 9
I0731 22:45:49.820979 20406 solver.cpp:353] Iteration 21400 (5.00438 iter/s, 19.9825s/100 iter), loss = 0.0560582
I0731 22:45:49.821043 20406 solver.cpp:375]     Train net output #0: loss = 0.0560582 (* 1 = 0.0560582 loss)
I0731 22:45:49.821053 20406 sgd_solver.cpp:136] Iteration 21400, lr = 1e-05, m = 0.9
I0731 22:46:09.979542 20406 solver.cpp:353] Iteration 21500 (4.96081 iter/s, 20.158s/100 iter), loss = 0.0467375
I0731 22:46:09.979588 20406 solver.cpp:375]     Train net output #0: loss = 0.0467374 (* 1 = 0.0467374 loss)
I0731 22:46:09.979595 20406 sgd_solver.cpp:136] Iteration 21500, lr = 1e-05, m = 0.9
I0731 22:46:30.000568 20406 solver.cpp:353] Iteration 21600 (4.99489 iter/s, 20.0205s/100 iter), loss = 0.0725782
I0731 22:46:30.000622 20406 solver.cpp:375]     Train net output #0: loss = 0.0725781 (* 1 = 0.0725781 loss)
I0731 22:46:30.000636 20406 sgd_solver.cpp:136] Iteration 21600, lr = 1e-05, m = 0.9
I0731 22:46:39.206610 20412 data_reader.cpp:264] Starting prefetch of epoch 17
I0731 22:46:49.520536 20406 solver.cpp:353] Iteration 21700 (5.1231 iter/s, 19.5194s/100 iter), loss = 0.0536072
I0731 22:46:49.520612 20406 solver.cpp:375]     Train net output #0: loss = 0.0536071 (* 1 = 0.0536071 loss)
I0731 22:46:49.520617 20406 sgd_solver.cpp:136] Iteration 21700, lr = 1e-05, m = 0.9
I0731 22:47:09.719588 20406 solver.cpp:353] Iteration 21800 (4.95087 iter/s, 20.1985s/100 iter), loss = 0.0601684
I0731 22:47:09.719632 20406 solver.cpp:375]     Train net output #0: loss = 0.0601683 (* 1 = 0.0601683 loss)
I0731 22:47:09.719642 20406 sgd_solver.cpp:136] Iteration 21800, lr = 1e-05, m = 0.9
I0731 22:47:12.323323 20410 data_reader.cpp:264] Starting prefetch of epoch 10
I0731 22:47:29.547864 20406 solver.cpp:353] Iteration 21900 (5.04344 iter/s, 19.8277s/100 iter), loss = 0.0651862
I0731 22:47:29.547924 20406 solver.cpp:375]     Train net output #0: loss = 0.0651861 (* 1 = 0.0651861 loss)
I0731 22:47:29.547931 20406 sgd_solver.cpp:136] Iteration 21900, lr = 1e-05, m = 0.9
I0731 22:47:48.024570 20406 solver.cpp:404] Sparsity after update:
I0731 22:47:48.034515 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 22:47:48.034550 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 22:47:48.034559 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 22:47:48.034561 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 22:47:48.034564 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 22:47:48.034569 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 22:47:48.034571 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 22:47:48.034574 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 22:47:48.034577 20406 net.cpp:2270] out3a_param_0(0) 
I0731 22:47:48.034580 20406 net.cpp:2270] out5a_param_0(0) 
I0731 22:47:48.034584 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 22:47:48.034586 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 22:47:48.034590 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 22:47:48.034593 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 22:47:48.034597 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 22:47:48.034600 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 22:47:48.034616 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 22:47:48.034622 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 22:47:48.034626 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 22:47:48.034642 20406 solver.cpp:550] Iteration 22000, Testing net (#0)
I0731 22:47:55.072757 20434 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 22:47:59.249797 20406 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.952781
I0731 22:47:59.249821 20406 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999381
I0731 22:47:59.249827 20406 solver.cpp:635]     Test net output #2: loss = 0.20424 (* 1 = 0.20424 loss)
I0731 22:47:59.249900 20406 solver.cpp:305] [MultiGPU] Tests completed in 11.2149s
I0731 22:47:59.460201 20406 solver.cpp:353] Iteration 22000 (3.34319 iter/s, 29.9115s/100 iter), loss = 0.0811862
I0731 22:47:59.460227 20406 solver.cpp:375]     Train net output #0: loss = 0.0811861 (* 1 = 0.0811861 loss)
I0731 22:47:59.460232 20406 sgd_solver.cpp:136] Iteration 22000, lr = 1e-05, m = 0.9
I0731 22:48:18.025182 20406 solver.cpp:353] Iteration 22100 (5.38664 iter/s, 18.5645s/100 iter), loss = 0.0532882
I0731 22:48:18.025312 20406 solver.cpp:375]     Train net output #0: loss = 0.0532881 (* 1 = 0.0532881 loss)
I0731 22:48:18.025319 20406 sgd_solver.cpp:136] Iteration 22100, lr = 1e-05, m = 0.9
I0731 22:48:36.638259 20406 solver.cpp:353] Iteration 22200 (5.37272 iter/s, 18.6126s/100 iter), loss = 0.0716118
I0731 22:48:36.638283 20406 solver.cpp:375]     Train net output #0: loss = 0.0716117 (* 1 = 0.0716117 loss)
I0731 22:48:36.638288 20406 sgd_solver.cpp:136] Iteration 22200, lr = 1e-05, m = 0.9
I0731 22:48:55.112478 20406 solver.cpp:353] Iteration 22300 (5.4131 iter/s, 18.4737s/100 iter), loss = 0.0447723
I0731 22:48:55.112567 20406 solver.cpp:375]     Train net output #0: loss = 0.0447722 (* 1 = 0.0447722 loss)
I0731 22:48:55.112574 20406 sgd_solver.cpp:136] Iteration 22300, lr = 1e-05, m = 0.9
I0731 22:48:56.806146 20415 data_reader.cpp:264] Starting prefetch of epoch 11
I0731 22:49:13.789340 20406 solver.cpp:353] Iteration 22400 (5.35437 iter/s, 18.6763s/100 iter), loss = 0.0478767
I0731 22:49:13.789366 20406 solver.cpp:375]     Train net output #0: loss = 0.0478766 (* 1 = 0.0478766 loss)
I0731 22:49:13.789369 20406 sgd_solver.cpp:136] Iteration 22400, lr = 1e-05, m = 0.9
I0731 22:49:27.554797 20346 data_reader.cpp:264] Starting prefetch of epoch 14
I0731 22:49:32.329690 20406 solver.cpp:353] Iteration 22500 (5.39379 iter/s, 18.5398s/100 iter), loss = 0.0677726
I0731 22:49:32.329720 20406 solver.cpp:375]     Train net output #0: loss = 0.0677725 (* 1 = 0.0677725 loss)
I0731 22:49:32.329725 20406 sgd_solver.cpp:136] Iteration 22500, lr = 1e-05, m = 0.9
I0731 22:49:50.977135 20406 solver.cpp:353] Iteration 22600 (5.36281 iter/s, 18.6469s/100 iter), loss = 0.0441089
I0731 22:49:50.977160 20406 solver.cpp:375]     Train net output #0: loss = 0.0441088 (* 1 = 0.0441088 loss)
I0731 22:49:50.977165 20406 sgd_solver.cpp:136] Iteration 22600, lr = 1e-05, m = 0.9
I0731 22:50:09.575695 20406 solver.cpp:353] Iteration 22700 (5.37691 iter/s, 18.598s/100 iter), loss = 0.0715134
I0731 22:50:09.575793 20406 solver.cpp:375]     Train net output #0: loss = 0.0715133 (* 1 = 0.0715133 loss)
I0731 22:50:09.575798 20406 sgd_solver.cpp:136] Iteration 22700, lr = 1e-05, m = 0.9
I0731 22:50:28.180476 20406 solver.cpp:353] Iteration 22800 (5.37511 iter/s, 18.6043s/100 iter), loss = 0.041768
I0731 22:50:28.180500 20406 solver.cpp:375]     Train net output #0: loss = 0.0417679 (* 1 = 0.0417679 loss)
I0731 22:50:28.180505 20406 sgd_solver.cpp:136] Iteration 22800, lr = 1e-05, m = 0.9
I0731 22:50:29.157249 20346 data_reader.cpp:264] Starting prefetch of epoch 15
I0731 22:50:46.715844 20406 solver.cpp:353] Iteration 22900 (5.39524 iter/s, 18.5349s/100 iter), loss = 0.069175
I0731 22:50:46.715899 20406 solver.cpp:375]     Train net output #0: loss = 0.0691749 (* 1 = 0.0691749 loss)
I0731 22:50:46.715904 20406 sgd_solver.cpp:136] Iteration 22900, lr = 1e-05, m = 0.9
I0731 22:51:05.092069 20406 solver.cpp:404] Sparsity after update:
I0731 22:51:05.121847 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 22:51:05.121865 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 22:51:05.121871 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 22:51:05.121873 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 22:51:05.121876 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 22:51:05.121877 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 22:51:05.121879 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 22:51:05.121881 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 22:51:05.121883 20406 net.cpp:2270] out3a_param_0(0) 
I0731 22:51:05.121886 20406 net.cpp:2270] out5a_param_0(0) 
I0731 22:51:05.121887 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 22:51:05.121889 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 22:51:05.121891 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 22:51:05.121893 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 22:51:05.121896 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 22:51:05.121897 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 22:51:05.121899 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 22:51:05.121901 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 22:51:05.121902 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 22:51:05.290372 20406 solver.cpp:353] Iteration 23000 (5.38387 iter/s, 18.574s/100 iter), loss = 0.0840957
I0731 22:51:05.290400 20406 solver.cpp:375]     Train net output #0: loss = 0.0840956 (* 1 = 0.0840956 loss)
I0731 22:51:05.290405 20406 sgd_solver.cpp:136] Iteration 23000, lr = 1e-05, m = 0.9
I0731 22:51:23.827229 20406 solver.cpp:353] Iteration 23100 (5.39481 iter/s, 18.5363s/100 iter), loss = 0.0524736
I0731 22:51:23.827309 20406 solver.cpp:375]     Train net output #0: loss = 0.0524735 (* 1 = 0.0524735 loss)
I0731 22:51:23.827317 20406 sgd_solver.cpp:136] Iteration 23100, lr = 1e-05, m = 0.9
I0731 22:51:30.367539 20412 data_reader.cpp:264] Starting prefetch of epoch 18
I0731 22:51:42.403483 20406 solver.cpp:353] Iteration 23200 (5.38336 iter/s, 18.5757s/100 iter), loss = 0.0484776
I0731 22:51:42.403508 20406 solver.cpp:375]     Train net output #0: loss = 0.0484775 (* 1 = 0.0484775 loss)
I0731 22:51:42.403513 20406 sgd_solver.cpp:136] Iteration 23200, lr = 1e-05, m = 0.9
I0731 22:52:01.056681 20406 solver.cpp:353] Iteration 23300 (5.36116 iter/s, 18.6527s/100 iter), loss = 0.0394119
I0731 22:52:01.056736 20406 solver.cpp:375]     Train net output #0: loss = 0.0394118 (* 1 = 0.0394118 loss)
I0731 22:52:01.056741 20406 sgd_solver.cpp:136] Iteration 23300, lr = 1e-05, m = 0.9
I0731 22:52:01.272927 20410 data_reader.cpp:264] Starting prefetch of epoch 11
I0731 22:52:19.559386 20406 solver.cpp:353] Iteration 23400 (5.40476 iter/s, 18.5022s/100 iter), loss = 0.0518391
I0731 22:52:19.559412 20406 solver.cpp:375]     Train net output #0: loss = 0.051839 (* 1 = 0.051839 loss)
I0731 22:52:19.559417 20406 sgd_solver.cpp:136] Iteration 23400, lr = 1e-05, m = 0.9
I0731 22:52:38.250367 20406 solver.cpp:353] Iteration 23500 (5.35032 iter/s, 18.6905s/100 iter), loss = 0.18031
I0731 22:52:38.250493 20406 solver.cpp:375]     Train net output #0: loss = 0.18031 (* 1 = 0.18031 loss)
I0731 22:52:38.250499 20406 sgd_solver.cpp:136] Iteration 23500, lr = 1e-05, m = 0.9
I0731 22:52:56.872737 20406 solver.cpp:353] Iteration 23600 (5.37003 iter/s, 18.6219s/100 iter), loss = 0.0412659
I0731 22:52:56.872766 20406 solver.cpp:375]     Train net output #0: loss = 0.0412658 (* 1 = 0.0412658 loss)
I0731 22:52:56.872771 20406 sgd_solver.cpp:136] Iteration 23600, lr = 1e-05, m = 0.9
I0731 22:53:02.645288 20410 data_reader.cpp:264] Starting prefetch of epoch 12
I0731 22:53:15.428835 20406 solver.cpp:353] Iteration 23700 (5.38922 iter/s, 18.5556s/100 iter), loss = 0.063004
I0731 22:53:15.428905 20406 solver.cpp:375]     Train net output #0: loss = 0.063004 (* 1 = 0.063004 loss)
I0731 22:53:15.428911 20406 sgd_solver.cpp:136] Iteration 23700, lr = 1e-05, m = 0.9
I0731 22:53:33.985683 20406 solver.cpp:353] Iteration 23800 (5.38899 iter/s, 18.5564s/100 iter), loss = 0.0878488
I0731 22:53:33.985707 20406 solver.cpp:375]     Train net output #0: loss = 0.0878487 (* 1 = 0.0878487 loss)
I0731 22:53:33.985711 20406 sgd_solver.cpp:136] Iteration 23800, lr = 1e-05, m = 0.9
I0731 22:53:52.686878 20406 solver.cpp:353] Iteration 23900 (5.3474 iter/s, 18.7007s/100 iter), loss = 0.0586963
I0731 22:53:52.686924 20406 solver.cpp:375]     Train net output #0: loss = 0.0586963 (* 1 = 0.0586963 loss)
I0731 22:53:52.686929 20406 sgd_solver.cpp:136] Iteration 23900, lr = 1e-05, m = 0.9
I0731 22:54:04.263317 20364 data_reader.cpp:264] Starting prefetch of epoch 17
I0731 22:54:11.075690 20406 solver.cpp:404] Sparsity after update:
I0731 22:54:11.086776 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 22:54:11.086792 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 22:54:11.086799 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 22:54:11.086802 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 22:54:11.086804 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 22:54:11.086805 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 22:54:11.086807 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 22:54:11.086809 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 22:54:11.086812 20406 net.cpp:2270] out3a_param_0(0) 
I0731 22:54:11.086813 20406 net.cpp:2270] out5a_param_0(0) 
I0731 22:54:11.086815 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 22:54:11.086817 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 22:54:11.086820 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 22:54:11.086822 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 22:54:11.086824 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 22:54:11.086827 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 22:54:11.086828 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 22:54:11.086829 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 22:54:11.086832 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 22:54:11.086840 20406 solver.cpp:550] Iteration 24000, Testing net (#0)
I0731 22:54:14.640410 20402 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 22:54:22.551545 20406 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.951041
I0731 22:54:22.551563 20406 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999844
I0731 22:54:22.551568 20406 solver.cpp:635]     Test net output #2: loss = 0.169531 (* 1 = 0.169531 loss)
I0731 22:54:22.551590 20406 solver.cpp:305] [MultiGPU] Tests completed in 11.4644s
I0731 22:54:22.637689 20461 sgd_solver.cpp:48] MultiStep Status: Iteration 24000, step = 1
I0731 22:54:22.637689 20460 sgd_solver.cpp:48] MultiStep Status: Iteration 24000, step = 1
I0731 22:54:22.637689 20459 sgd_solver.cpp:48] MultiStep Status: Iteration 24000, step = 1
I0731 22:54:22.747530 20406 solver.cpp:353] Iteration 24000 (3.3267 iter/s, 30.0598s/100 iter), loss = 0.0782841
I0731 22:54:22.747660 20406 solver.cpp:375]     Train net output #0: loss = 0.0782841 (* 1 = 0.0782841 loss)
I0731 22:54:22.747670 20406 sgd_solver.cpp:136] Iteration 24000, lr = 1e-06, m = 0.9
I0731 22:54:41.289474 20406 solver.cpp:353] Iteration 24100 (5.39333 iter/s, 18.5414s/100 iter), loss = 0.06716
I0731 22:54:41.289496 20406 solver.cpp:375]     Train net output #0: loss = 0.0671599 (* 1 = 0.0671599 loss)
I0731 22:54:41.289501 20406 sgd_solver.cpp:136] Iteration 24100, lr = 1e-06, m = 0.9
I0731 22:54:46.338245 20346 data_reader.cpp:264] Starting prefetch of epoch 16
I0731 22:54:59.978909 20406 solver.cpp:353] Iteration 24200 (5.35077 iter/s, 18.6889s/100 iter), loss = 0.0816253
I0731 22:54:59.978961 20406 solver.cpp:375]     Train net output #0: loss = 0.0816252 (* 1 = 0.0816252 loss)
I0731 22:54:59.978966 20406 sgd_solver.cpp:136] Iteration 24200, lr = 1e-06, m = 0.9
I0731 22:55:18.596159 20406 solver.cpp:353] Iteration 24300 (5.37151 iter/s, 18.6167s/100 iter), loss = 0.0532994
I0731 22:55:18.596184 20406 solver.cpp:375]     Train net output #0: loss = 0.0532994 (* 1 = 0.0532994 loss)
I0731 22:55:18.596190 20406 sgd_solver.cpp:136] Iteration 24300, lr = 1e-06, m = 0.9
I0731 22:55:37.129906 20406 solver.cpp:353] Iteration 24400 (5.39571 iter/s, 18.5332s/100 iter), loss = 0.0701097
I0731 22:55:37.129986 20406 solver.cpp:375]     Train net output #0: loss = 0.0701096 (* 1 = 0.0701096 loss)
I0731 22:55:37.129993 20406 sgd_solver.cpp:136] Iteration 24400, lr = 1e-06, m = 0.9
I0731 22:55:47.934613 20364 data_reader.cpp:264] Starting prefetch of epoch 18
I0731 22:55:55.701972 20406 solver.cpp:353] Iteration 24500 (5.38458 iter/s, 18.5716s/100 iter), loss = 0.0492014
I0731 22:55:55.702000 20406 solver.cpp:375]     Train net output #0: loss = 0.0492013 (* 1 = 0.0492013 loss)
I0731 22:55:55.702004 20406 sgd_solver.cpp:136] Iteration 24500, lr = 1e-06, m = 0.9
I0731 22:56:14.341500 20406 solver.cpp:353] Iteration 24600 (5.36509 iter/s, 18.639s/100 iter), loss = 0.092836
I0731 22:56:14.341547 20406 solver.cpp:375]     Train net output #0: loss = 0.0928359 (* 1 = 0.0928359 loss)
I0731 22:56:14.341552 20406 sgd_solver.cpp:136] Iteration 24600, lr = 1e-06, m = 0.9
I0731 22:56:18.583927 20410 data_reader.cpp:264] Starting prefetch of epoch 13
I0731 22:56:32.943590 20406 solver.cpp:353] Iteration 24700 (5.37589 iter/s, 18.6016s/100 iter), loss = 0.0660087
I0731 22:56:32.943615 20406 solver.cpp:375]     Train net output #0: loss = 0.0660086 (* 1 = 0.0660086 loss)
I0731 22:56:32.943619 20406 sgd_solver.cpp:136] Iteration 24700, lr = 1e-06, m = 0.9
I0731 22:56:51.510804 20406 solver.cpp:353] Iteration 24800 (5.38599 iter/s, 18.5667s/100 iter), loss = 0.0936572
I0731 22:56:51.510892 20406 solver.cpp:375]     Train net output #0: loss = 0.0936571 (* 1 = 0.0936571 loss)
I0731 22:56:51.510900 20406 sgd_solver.cpp:136] Iteration 24800, lr = 1e-06, m = 0.9
I0731 22:57:10.346500 20406 solver.cpp:353] Iteration 24900 (5.30922 iter/s, 18.8352s/100 iter), loss = 0.0609188
I0731 22:57:10.346542 20406 solver.cpp:375]     Train net output #0: loss = 0.0609187 (* 1 = 0.0609187 loss)
I0731 22:57:10.346555 20406 sgd_solver.cpp:136] Iteration 24900, lr = 1e-06, m = 0.9
I0731 22:57:21.180764 20409 data_reader.cpp:264] Starting prefetch of epoch 14
I0731 22:57:30.351199 20406 solver.cpp:404] Sparsity after update:
I0731 22:57:30.370857 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 22:57:30.370964 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 22:57:30.370990 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 22:57:30.371001 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 22:57:30.371008 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 22:57:30.371016 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 22:57:30.371026 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 22:57:30.371034 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 22:57:30.371043 20406 net.cpp:2270] out3a_param_0(0) 
I0731 22:57:30.371053 20406 net.cpp:2270] out5a_param_0(0) 
I0731 22:57:30.371062 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 22:57:30.371071 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 22:57:30.371080 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 22:57:30.371099 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 22:57:30.371109 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 22:57:30.371117 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 22:57:30.371126 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 22:57:30.371135 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 22:57:30.371145 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 22:57:30.545156 20406 solver.cpp:353] Iteration 25000 (4.95096 iter/s, 20.1981s/100 iter), loss = 0.0676049
I0731 22:57:30.545184 20406 solver.cpp:375]     Train net output #0: loss = 0.0676048 (* 1 = 0.0676048 loss)
I0731 22:57:30.545192 20406 sgd_solver.cpp:136] Iteration 25000, lr = 1e-06, m = 0.9
I0731 22:57:51.021121 20406 solver.cpp:353] Iteration 25100 (4.88391 iter/s, 20.4754s/100 iter), loss = 0.055798
I0731 22:57:51.021186 20406 solver.cpp:375]     Train net output #0: loss = 0.0557979 (* 1 = 0.0557979 loss)
I0731 22:57:51.021204 20406 sgd_solver.cpp:136] Iteration 25100, lr = 1e-06, m = 0.9
I0731 22:58:11.256671 20406 solver.cpp:353] Iteration 25200 (4.94193 iter/s, 20.235s/100 iter), loss = 0.0796873
I0731 22:58:11.256744 20406 solver.cpp:375]     Train net output #0: loss = 0.0796872 (* 1 = 0.0796872 loss)
I0731 22:58:11.256749 20406 sgd_solver.cpp:136] Iteration 25200, lr = 1e-06, m = 0.9
I0731 22:58:27.825883 20415 data_reader.cpp:264] Starting prefetch of epoch 12
I0731 22:58:31.032615 20406 solver.cpp:353] Iteration 25300 (5.05679 iter/s, 19.7754s/100 iter), loss = 0.0755685
I0731 22:58:31.032658 20406 solver.cpp:375]     Train net output #0: loss = 0.0755684 (* 1 = 0.0755684 loss)
I0731 22:58:31.032666 20406 sgd_solver.cpp:136] Iteration 25300, lr = 1e-06, m = 0.9
I0731 22:58:51.596232 20406 solver.cpp:353] Iteration 25400 (4.86309 iter/s, 20.5631s/100 iter), loss = 0.0677014
I0731 22:58:51.596313 20406 solver.cpp:375]     Train net output #0: loss = 0.0677013 (* 1 = 0.0677013 loss)
I0731 22:58:51.596320 20406 sgd_solver.cpp:136] Iteration 25400, lr = 1e-06, m = 0.9
I0731 22:59:01.249126 20409 data_reader.cpp:264] Starting prefetch of epoch 15
I0731 22:59:11.721197 20406 solver.cpp:353] Iteration 25500 (4.96909 iter/s, 20.1244s/100 iter), loss = 0.0512099
I0731 22:59:11.721256 20406 solver.cpp:375]     Train net output #0: loss = 0.0512098 (* 1 = 0.0512098 loss)
I0731 22:59:11.721284 20406 sgd_solver.cpp:136] Iteration 25500, lr = 1e-06, m = 0.9
I0731 22:59:31.261665 20406 solver.cpp:353] Iteration 25600 (5.11773 iter/s, 19.5399s/100 iter), loss = 0.0609539
I0731 22:59:31.261724 20406 solver.cpp:375]     Train net output #0: loss = 0.0609538 (* 1 = 0.0609538 loss)
I0731 22:59:31.261729 20406 sgd_solver.cpp:136] Iteration 25600, lr = 1e-06, m = 0.9
I0731 22:59:51.431061 20406 solver.cpp:353] Iteration 25700 (4.95814 iter/s, 20.1688s/100 iter), loss = 0.0438565
I0731 22:59:51.431087 20406 solver.cpp:375]     Train net output #0: loss = 0.0438564 (* 1 = 0.0438564 loss)
I0731 22:59:51.431092 20406 sgd_solver.cpp:136] Iteration 25700, lr = 1e-06, m = 0.9
I0731 23:00:07.728149 20410 data_reader.cpp:264] Starting prefetch of epoch 14
I0731 23:00:11.785522 20406 solver.cpp:353] Iteration 25800 (4.91307 iter/s, 20.3539s/100 iter), loss = 0.0662195
I0731 23:00:11.785570 20406 solver.cpp:375]     Train net output #0: loss = 0.0662193 (* 1 = 0.0662193 loss)
I0731 23:00:11.785594 20406 sgd_solver.cpp:136] Iteration 25800, lr = 1e-06, m = 0.9
I0731 23:00:31.578505 20406 solver.cpp:353] Iteration 25900 (5.05244 iter/s, 19.7924s/100 iter), loss = 0.0523201
I0731 23:00:31.578547 20406 solver.cpp:375]     Train net output #0: loss = 0.05232 (* 1 = 0.05232 loss)
I0731 23:00:31.578553 20406 sgd_solver.cpp:136] Iteration 25900, lr = 1e-06, m = 0.9
I0731 23:00:51.283259 20406 solver.cpp:404] Sparsity after update:
I0731 23:00:51.293623 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 23:00:51.293644 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 23:00:51.293651 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 23:00:51.293653 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 23:00:51.293655 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 23:00:51.293658 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 23:00:51.293659 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 23:00:51.293661 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 23:00:51.293663 20406 net.cpp:2270] out3a_param_0(0) 
I0731 23:00:51.293664 20406 net.cpp:2270] out5a_param_0(0) 
I0731 23:00:51.293666 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 23:00:51.293668 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 23:00:51.293670 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 23:00:51.293673 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 23:00:51.293675 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 23:00:51.293678 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 23:00:51.293679 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 23:00:51.293681 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 23:00:51.293684 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 23:00:51.293691 20406 solver.cpp:550] Iteration 26000, Testing net (#0)
I0731 23:00:59.216686 20402 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 23:01:03.524132 20406 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.952684
I0731 23:01:03.524160 20406 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999238
I0731 23:01:03.524165 20406 solver.cpp:635]     Test net output #2: loss = 0.208827 (* 1 = 0.208827 loss)
I0731 23:01:03.524250 20406 solver.cpp:305] [MultiGPU] Tests completed in 12.2302s
I0731 23:01:03.719925 20406 solver.cpp:353] Iteration 26000 (3.11134 iter/s, 32.1405s/100 iter), loss = 0.0822551
I0731 23:01:03.719951 20406 solver.cpp:375]     Train net output #0: loss = 0.082255 (* 1 = 0.082255 loss)
I0731 23:01:03.719956 20406 sgd_solver.cpp:136] Iteration 26000, lr = 1e-06, m = 0.9
I0731 23:01:22.442853 20406 solver.cpp:353] Iteration 26100 (5.34119 iter/s, 18.7224s/100 iter), loss = 0.0404537
I0731 23:01:22.442924 20406 solver.cpp:375]     Train net output #0: loss = 0.0404536 (* 1 = 0.0404536 loss)
I0731 23:01:22.442929 20406 sgd_solver.cpp:136] Iteration 26100, lr = 1e-06, m = 0.9
I0731 23:01:24.340495 20346 data_reader.cpp:264] Starting prefetch of epoch 17
I0731 23:01:40.903657 20406 solver.cpp:353] Iteration 26200 (5.41703 iter/s, 18.4603s/100 iter), loss = 0.0875606
I0731 23:01:40.903692 20406 solver.cpp:375]     Train net output #0: loss = 0.0875604 (* 1 = 0.0875604 loss)
I0731 23:01:40.903695 20406 sgd_solver.cpp:136] Iteration 26200, lr = 1e-06, m = 0.9
I0731 23:01:59.413755 20406 solver.cpp:353] Iteration 26300 (5.40261 iter/s, 18.5096s/100 iter), loss = 0.0730365
I0731 23:01:59.413869 20406 solver.cpp:375]     Train net output #0: loss = 0.0730364 (* 1 = 0.0730364 loss)
I0731 23:01:59.413877 20406 sgd_solver.cpp:136] Iteration 26300, lr = 1e-06, m = 0.9
I0731 23:02:17.949764 20406 solver.cpp:353] Iteration 26400 (5.39506 iter/s, 18.5355s/100 iter), loss = 0.0877131
I0731 23:02:17.949795 20406 solver.cpp:375]     Train net output #0: loss = 0.087713 (* 1 = 0.087713 loss)
I0731 23:02:17.949800 20406 sgd_solver.cpp:136] Iteration 26400, lr = 1e-06, m = 0.9
I0731 23:02:25.607731 20346 data_reader.cpp:264] Starting prefetch of epoch 18
I0731 23:02:36.422958 20406 solver.cpp:353] Iteration 26500 (5.4134 iter/s, 18.4727s/100 iter), loss = 0.0443398
I0731 23:02:36.423017 20406 solver.cpp:375]     Train net output #0: loss = 0.0443396 (* 1 = 0.0443396 loss)
I0731 23:02:36.423022 20406 sgd_solver.cpp:136] Iteration 26500, lr = 1e-06, m = 0.9
I0731 23:02:54.962013 20406 solver.cpp:353] Iteration 26600 (5.39417 iter/s, 18.5385s/100 iter), loss = 0.0731329
I0731 23:02:54.962040 20406 solver.cpp:375]     Train net output #0: loss = 0.0731328 (* 1 = 0.0731328 loss)
I0731 23:02:54.962044 20406 sgd_solver.cpp:136] Iteration 26600, lr = 1e-06, m = 0.9
I0731 23:03:13.516842 20406 solver.cpp:353] Iteration 26700 (5.38958 iter/s, 18.5543s/100 iter), loss = 0.0635741
I0731 23:03:13.516912 20406 solver.cpp:375]     Train net output #0: loss = 0.063574 (* 1 = 0.063574 loss)
I0731 23:03:13.516918 20406 sgd_solver.cpp:136] Iteration 26700, lr = 1e-06, m = 0.9
I0731 23:03:26.660473 20415 data_reader.cpp:264] Starting prefetch of epoch 13
I0731 23:03:32.080206 20406 solver.cpp:353] Iteration 26800 (5.3871 iter/s, 18.5628s/100 iter), loss = 0.0744798
I0731 23:03:32.080230 20406 solver.cpp:375]     Train net output #0: loss = 0.0744797 (* 1 = 0.0744797 loss)
I0731 23:03:32.080235 20406 sgd_solver.cpp:136] Iteration 26800, lr = 1e-06, m = 0.9
I0731 23:03:50.683495 20406 solver.cpp:353] Iteration 26900 (5.37554 iter/s, 18.6028s/100 iter), loss = 0.0530878
I0731 23:03:50.683595 20406 solver.cpp:375]     Train net output #0: loss = 0.0530877 (* 1 = 0.0530877 loss)
I0731 23:03:50.683603 20406 sgd_solver.cpp:136] Iteration 26900, lr = 1e-06, m = 0.9
I0731 23:04:08.978792 20406 solver.cpp:404] Sparsity after update:
I0731 23:04:08.997300 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 23:04:08.997370 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 23:04:08.997398 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 23:04:08.997404 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 23:04:08.997409 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 23:04:08.997413 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 23:04:08.997417 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 23:04:08.997427 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 23:04:08.997433 20406 net.cpp:2270] out3a_param_0(0) 
I0731 23:04:08.997438 20406 net.cpp:2270] out5a_param_0(0) 
I0731 23:04:08.997443 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 23:04:08.997448 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 23:04:08.997453 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 23:04:08.997457 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 23:04:08.997462 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 23:04:08.997467 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 23:04:08.997470 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 23:04:08.997474 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 23:04:08.997479 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 23:04:09.169281 20406 solver.cpp:353] Iteration 27000 (5.40971 iter/s, 18.4853s/100 iter), loss = 0.0954375
I0731 23:04:09.169303 20406 solver.cpp:375]     Train net output #0: loss = 0.0954373 (* 1 = 0.0954373 loss)
I0731 23:04:09.169307 20406 sgd_solver.cpp:136] Iteration 27000, lr = 1e-06, m = 0.9
I0731 23:04:29.191629 20406 solver.cpp:353] Iteration 27100 (4.99456 iter/s, 20.0218s/100 iter), loss = 0.0622642
I0731 23:04:29.191685 20406 solver.cpp:375]     Train net output #0: loss = 0.0622641 (* 1 = 0.0622641 loss)
I0731 23:04:29.191692 20406 sgd_solver.cpp:136] Iteration 27100, lr = 1e-06, m = 0.9
I0731 23:04:29.579996 20364 data_reader.cpp:264] Starting prefetch of epoch 19
I0731 23:04:47.817909 20406 solver.cpp:353] Iteration 27200 (5.36891 iter/s, 18.6258s/100 iter), loss = 0.0424939
I0731 23:04:47.817934 20406 solver.cpp:375]     Train net output #0: loss = 0.0424938 (* 1 = 0.0424938 loss)
I0731 23:04:47.817937 20406 sgd_solver.cpp:136] Iteration 27200, lr = 1e-06, m = 0.9
I0731 23:05:00.224440 20409 data_reader.cpp:264] Starting prefetch of epoch 16
I0731 23:05:06.329279 20406 solver.cpp:353] Iteration 27300 (5.40223 iter/s, 18.5109s/100 iter), loss = 0.0615048
I0731 23:05:06.329306 20406 solver.cpp:375]     Train net output #0: loss = 0.0615047 (* 1 = 0.0615047 loss)
I0731 23:05:06.329313 20406 sgd_solver.cpp:136] Iteration 27300, lr = 1e-06, m = 0.9
I0731 23:05:24.843576 20406 solver.cpp:353] Iteration 27400 (5.40138 iter/s, 18.5138s/100 iter), loss = 0.0924028
I0731 23:05:24.843601 20406 solver.cpp:375]     Train net output #0: loss = 0.0924027 (* 1 = 0.0924027 loss)
I0731 23:05:24.843606 20406 sgd_solver.cpp:136] Iteration 27400, lr = 1e-06, m = 0.9
I0731 23:05:43.510236 20406 solver.cpp:353] Iteration 27500 (5.35729 iter/s, 18.6661s/100 iter), loss = 0.0568531
I0731 23:05:43.510306 20406 solver.cpp:375]     Train net output #0: loss = 0.056853 (* 1 = 0.056853 loss)
I0731 23:05:43.510313 20406 sgd_solver.cpp:136] Iteration 27500, lr = 1e-06, m = 0.9
I0731 23:06:01.826253 20415 data_reader.cpp:264] Starting prefetch of epoch 14
I0731 23:06:02.175640 20406 solver.cpp:353] Iteration 27600 (5.35765 iter/s, 18.6649s/100 iter), loss = 0.0778439
I0731 23:06:02.175664 20406 solver.cpp:375]     Train net output #0: loss = 0.0778438 (* 1 = 0.0778438 loss)
I0731 23:06:02.175670 20406 sgd_solver.cpp:136] Iteration 27600, lr = 1e-06, m = 0.9
I0731 23:06:20.855353 20406 solver.cpp:353] Iteration 27700 (5.35355 iter/s, 18.6792s/100 iter), loss = 0.0780241
I0731 23:06:20.855456 20406 solver.cpp:375]     Train net output #0: loss = 0.0780239 (* 1 = 0.0780239 loss)
I0731 23:06:20.855463 20406 sgd_solver.cpp:136] Iteration 27700, lr = 1e-06, m = 0.9
I0731 23:06:39.440183 20406 solver.cpp:353] Iteration 27800 (5.38088 iter/s, 18.5843s/100 iter), loss = 0.041343
I0731 23:06:39.440208 20406 solver.cpp:375]     Train net output #0: loss = 0.0413428 (* 1 = 0.0413428 loss)
I0731 23:06:39.440212 20406 sgd_solver.cpp:136] Iteration 27800, lr = 1e-06, m = 0.9
I0731 23:06:58.042965 20406 solver.cpp:353] Iteration 27900 (5.37569 iter/s, 18.6023s/100 iter), loss = 0.0553453
I0731 23:06:58.043047 20406 solver.cpp:375]     Train net output #0: loss = 0.0553452 (* 1 = 0.0553452 loss)
I0731 23:06:58.043052 20406 sgd_solver.cpp:136] Iteration 27900, lr = 1e-06, m = 0.9
I0731 23:07:03.275290 20412 data_reader.cpp:264] Starting prefetch of epoch 19
I0731 23:07:16.524415 20406 solver.cpp:404] Sparsity after update:
I0731 23:07:16.533387 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 23:07:16.533439 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 23:07:16.533457 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 23:07:16.533463 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 23:07:16.533466 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 23:07:16.533470 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 23:07:16.533494 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 23:07:16.533504 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 23:07:16.533516 20406 net.cpp:2270] out3a_param_0(0) 
I0731 23:07:16.533529 20406 net.cpp:2270] out5a_param_0(0) 
I0731 23:07:16.533541 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 23:07:16.533555 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 23:07:16.533566 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 23:07:16.533579 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 23:07:16.533592 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 23:07:16.533605 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 23:07:16.533618 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 23:07:16.533632 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 23:07:16.533643 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 23:07:16.533671 20406 solver.cpp:550] Iteration 28000, Testing net (#0)
I0731 23:07:20.025143 20440 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 23:07:27.895768 20406 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.951344
I0731 23:07:27.895789 20406 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999745
I0731 23:07:27.895794 20406 solver.cpp:635]     Test net output #2: loss = 0.170113 (* 1 = 0.170113 loss)
I0731 23:07:27.895824 20406 solver.cpp:305] [MultiGPU] Tests completed in 11.3618s
I0731 23:07:28.114089 20406 solver.cpp:353] Iteration 28000 (3.32554 iter/s, 30.0703s/100 iter), loss = 0.0751927
I0731 23:07:28.114142 20406 solver.cpp:375]     Train net output #0: loss = 0.0751926 (* 1 = 0.0751926 loss)
I0731 23:07:28.114147 20406 sgd_solver.cpp:136] Iteration 28000, lr = 1e-06, m = 0.9
I0731 23:07:45.727375 20364 data_reader.cpp:264] Starting prefetch of epoch 20
I0731 23:07:46.813369 20406 solver.cpp:353] Iteration 28100 (5.34795 iter/s, 18.6988s/100 iter), loss = 0.0741945
I0731 23:07:46.813400 20406 solver.cpp:375]     Train net output #0: loss = 0.0741943 (* 1 = 0.0741943 loss)
I0731 23:07:46.813407 20406 sgd_solver.cpp:136] Iteration 28100, lr = 1e-06, m = 0.9
I0731 23:08:06.419543 20406 solver.cpp:353] Iteration 28200 (5.10058 iter/s, 19.6056s/100 iter), loss = 0.0701177
I0731 23:08:06.419767 20406 solver.cpp:375]     Train net output #0: loss = 0.0701175 (* 1 = 0.0701175 loss)
I0731 23:08:06.419790 20406 sgd_solver.cpp:136] Iteration 28200, lr = 1e-06, m = 0.9
I0731 23:08:25.981039 20406 solver.cpp:353] Iteration 28300 (5.11222 iter/s, 19.561s/100 iter), loss = 0.0454937
I0731 23:08:25.981067 20406 solver.cpp:375]     Train net output #0: loss = 0.0454936 (* 1 = 0.0454936 loss)
I0731 23:08:25.981073 20406 sgd_solver.cpp:136] Iteration 28300, lr = 1e-06, m = 0.9
I0731 23:08:45.471046 20406 solver.cpp:353] Iteration 28400 (5.13098 iter/s, 19.4895s/100 iter), loss = 0.0797288
I0731 23:08:45.471140 20406 solver.cpp:375]     Train net output #0: loss = 0.0797287 (* 1 = 0.0797287 loss)
I0731 23:08:45.471154 20406 sgd_solver.cpp:136] Iteration 28400, lr = 1e-06, m = 0.9
I0731 23:08:50.069488 20415 data_reader.cpp:264] Starting prefetch of epoch 15
I0731 23:09:04.742830 20406 solver.cpp:353] Iteration 28500 (5.18908 iter/s, 19.2712s/100 iter), loss = 0.0719309
I0731 23:09:04.742877 20406 solver.cpp:375]     Train net output #0: loss = 0.0719308 (* 1 = 0.0719308 loss)
I0731 23:09:04.742887 20406 sgd_solver.cpp:136] Iteration 28500, lr = 1e-06, m = 0.9
I0731 23:09:23.116390 20410 data_reader.cpp:264] Starting prefetch of epoch 15
I0731 23:09:25.056561 20406 solver.cpp:353] Iteration 28600 (4.92292 iter/s, 20.3132s/100 iter), loss = 0.0835511
I0731 23:09:25.056651 20406 solver.cpp:375]     Train net output #0: loss = 0.083551 (* 1 = 0.083551 loss)
I0731 23:09:25.056689 20406 sgd_solver.cpp:136] Iteration 28600, lr = 1e-06, m = 0.9
I0731 23:09:45.516777 20406 solver.cpp:353] Iteration 28700 (4.88767 iter/s, 20.4596s/100 iter), loss = 0.079277
I0731 23:09:45.516829 20406 solver.cpp:375]     Train net output #0: loss = 0.0792769 (* 1 = 0.0792769 loss)
I0731 23:09:45.516836 20406 sgd_solver.cpp:136] Iteration 28700, lr = 1e-06, m = 0.9
I0731 23:10:04.848291 20406 solver.cpp:353] Iteration 28800 (5.17304 iter/s, 19.331s/100 iter), loss = 0.0535298
I0731 23:10:04.848924 20406 solver.cpp:375]     Train net output #0: loss = 0.0535297 (* 1 = 0.0535297 loss)
I0731 23:10:04.848940 20406 sgd_solver.cpp:136] Iteration 28800, lr = 1e-06, m = 0.9
I0731 23:10:25.244675 20406 solver.cpp:353] Iteration 28900 (4.90297 iter/s, 20.3958s/100 iter), loss = 0.116229
I0731 23:10:25.244705 20406 solver.cpp:375]     Train net output #0: loss = 0.116228 (* 1 = 0.116228 loss)
I0731 23:10:25.244709 20406 sgd_solver.cpp:136] Iteration 28900, lr = 1e-06, m = 0.9
I0731 23:10:29.442172 20412 data_reader.cpp:264] Starting prefetch of epoch 20
I0731 23:10:45.686597 20406 solver.cpp:404] Sparsity after update:
I0731 23:10:45.741345 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 23:10:45.741401 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 23:10:45.741423 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 23:10:45.741432 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 23:10:45.741438 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 23:10:45.741444 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 23:10:45.741451 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 23:10:45.741458 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 23:10:45.741466 20406 net.cpp:2270] out3a_param_0(0) 
I0731 23:10:45.741473 20406 net.cpp:2270] out5a_param_0(0) 
I0731 23:10:45.741480 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 23:10:45.741488 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 23:10:45.741495 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 23:10:45.741502 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 23:10:45.741509 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 23:10:45.741518 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 23:10:45.741523 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 23:10:45.741530 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 23:10:45.741539 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 23:10:45.939795 20406 solver.cpp:353] Iteration 29000 (4.83219 iter/s, 20.6945s/100 iter), loss = 0.0716947
I0731 23:10:45.939849 20406 solver.cpp:375]     Train net output #0: loss = 0.0716946 (* 1 = 0.0716946 loss)
I0731 23:10:45.939865 20406 sgd_solver.cpp:136] Iteration 29000, lr = 1e-06, m = 0.9
I0731 23:11:06.273299 20406 solver.cpp:353] Iteration 29100 (4.91813 iter/s, 20.3329s/100 iter), loss = 0.0714806
I0731 23:11:06.273469 20406 solver.cpp:375]     Train net output #0: loss = 0.0714805 (* 1 = 0.0714805 loss)
I0731 23:11:06.273494 20406 sgd_solver.cpp:136] Iteration 29100, lr = 1e-06, m = 0.9
I0731 23:11:25.517832 20406 solver.cpp:353] Iteration 29200 (5.19642 iter/s, 19.244s/100 iter), loss = 0.0533539
I0731 23:11:25.517906 20406 solver.cpp:375]     Train net output #0: loss = 0.0533538 (* 1 = 0.0533538 loss)
I0731 23:11:25.517912 20406 sgd_solver.cpp:136] Iteration 29200, lr = 1e-06, m = 0.9
I0731 23:11:34.966653 20364 data_reader.cpp:264] Starting prefetch of epoch 21
I0731 23:11:43.927615 20406 solver.cpp:353] Iteration 29300 (5.43204 iter/s, 18.4093s/100 iter), loss = 0.0527052
I0731 23:11:43.927644 20406 solver.cpp:375]     Train net output #0: loss = 0.052705 (* 1 = 0.052705 loss)
I0731 23:11:43.927647 20406 sgd_solver.cpp:136] Iteration 29300, lr = 1e-06, m = 0.9
I0731 23:12:02.363013 20406 solver.cpp:353] Iteration 29400 (5.4245 iter/s, 18.4349s/100 iter), loss = 0.0487238
I0731 23:12:02.363070 20406 solver.cpp:375]     Train net output #0: loss = 0.0487237 (* 1 = 0.0487237 loss)
I0731 23:12:02.363077 20406 sgd_solver.cpp:136] Iteration 29400, lr = 1e-06, m = 0.9
I0731 23:12:05.391907 20410 data_reader.cpp:264] Starting prefetch of epoch 16
I0731 23:12:21.147363 20406 solver.cpp:353] Iteration 29500 (5.32373 iter/s, 18.7838s/100 iter), loss = 0.0572922
I0731 23:12:21.147387 20406 solver.cpp:375]     Train net output #0: loss = 0.0572921 (* 1 = 0.0572921 loss)
I0731 23:12:21.147390 20406 sgd_solver.cpp:136] Iteration 29500, lr = 1e-06, m = 0.9
I0731 23:12:39.685219 20406 solver.cpp:353] Iteration 29600 (5.39452 iter/s, 18.5373s/100 iter), loss = 0.0737677
I0731 23:12:39.685322 20406 solver.cpp:375]     Train net output #0: loss = 0.0737676 (* 1 = 0.0737676 loss)
I0731 23:12:39.685329 20406 sgd_solver.cpp:136] Iteration 29600, lr = 1e-06, m = 0.9
I0731 23:12:58.215837 20406 solver.cpp:353] Iteration 29700 (5.39662 iter/s, 18.5301s/100 iter), loss = 0.0589148
I0731 23:12:58.215862 20406 solver.cpp:375]     Train net output #0: loss = 0.0589147 (* 1 = 0.0589147 loss)
I0731 23:12:58.215867 20406 sgd_solver.cpp:136] Iteration 29700, lr = 1e-06, m = 0.9
I0731 23:13:06.802382 20409 data_reader.cpp:264] Starting prefetch of epoch 17
I0731 23:13:16.820776 20406 solver.cpp:353] Iteration 29800 (5.37507 iter/s, 18.6044s/100 iter), loss = 0.133701
I0731 23:13:16.820832 20406 solver.cpp:375]     Train net output #0: loss = 0.133701 (* 1 = 0.133701 loss)
I0731 23:13:16.820839 20406 sgd_solver.cpp:136] Iteration 29800, lr = 1e-06, m = 0.9
I0731 23:13:35.330078 20406 solver.cpp:353] Iteration 29900 (5.40284 iter/s, 18.5088s/100 iter), loss = 0.0688835
I0731 23:13:35.330104 20406 solver.cpp:375]     Train net output #0: loss = 0.0688834 (* 1 = 0.0688834 loss)
I0731 23:13:35.330111 20406 sgd_solver.cpp:136] Iteration 29900, lr = 1e-06, m = 0.9
I0731 23:13:53.562196 20406 solver.cpp:680] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/sparse/cityscapes5_jsegnet21v2_iter_30000.caffemodel
I0731 23:13:53.591804 20406 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/sparse/cityscapes5_jsegnet21v2_iter_30000.solverstate
I0731 23:13:53.600965 20406 solver.cpp:404] Sparsity after update:
I0731 23:13:53.602792 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 23:13:53.602802 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 23:13:53.602809 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 23:13:53.602811 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 23:13:53.602813 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 23:13:53.602815 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 23:13:53.602818 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 23:13:53.602819 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 23:13:53.602821 20406 net.cpp:2270] out3a_param_0(0) 
I0731 23:13:53.602823 20406 net.cpp:2270] out5a_param_0(0) 
I0731 23:13:53.602825 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 23:13:53.602828 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 23:13:53.602829 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 23:13:53.602831 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 23:13:53.602833 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 23:13:53.602835 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 23:13:53.602838 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 23:13:53.602838 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 23:13:53.602840 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 23:13:53.602852 20406 solver.cpp:550] Iteration 30000, Testing net (#0)
I0731 23:14:00.413100 20402 data_reader.cpp:264] Starting prefetch of epoch 4
I0731 23:14:04.840283 20406 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.952376
I0731 23:14:04.840312 20406 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999237
I0731 23:14:04.840318 20406 solver.cpp:635]     Test net output #2: loss = 0.209476 (* 1 = 0.209476 loss)
I0731 23:14:04.840343 20406 solver.cpp:305] [MultiGPU] Tests completed in 11.2372s
I0731 23:14:05.032958 20406 solver.cpp:353] Iteration 30000 (3.36677 iter/s, 29.7021s/100 iter), loss = 0.0730025
I0731 23:14:05.032981 20406 solver.cpp:375]     Train net output #0: loss = 0.0730024 (* 1 = 0.0730024 loss)
I0731 23:14:05.032985 20406 sgd_solver.cpp:136] Iteration 30000, lr = 1e-06, m = 0.9
I0731 23:14:19.475546 20346 data_reader.cpp:264] Starting prefetch of epoch 19
I0731 23:14:23.651366 20406 solver.cpp:353] Iteration 30100 (5.37118 iter/s, 18.6179s/100 iter), loss = 0.0565624
I0731 23:14:23.651448 20406 solver.cpp:375]     Train net output #0: loss = 0.0565623 (* 1 = 0.0565623 loss)
I0731 23:14:23.651455 20406 sgd_solver.cpp:136] Iteration 30100, lr = 1e-06, m = 0.9
I0731 23:14:42.362090 20406 solver.cpp:353] Iteration 30200 (5.34468 iter/s, 18.7102s/100 iter), loss = 0.0623254
I0731 23:14:42.362119 20406 solver.cpp:375]     Train net output #0: loss = 0.0623253 (* 1 = 0.0623253 loss)
I0731 23:14:42.362124 20406 sgd_solver.cpp:136] Iteration 30200, lr = 1e-06, m = 0.9
I0731 23:15:00.955533 20406 solver.cpp:353] Iteration 30300 (5.37839 iter/s, 18.5929s/100 iter), loss = 0.0791104
I0731 23:15:00.955593 20406 solver.cpp:375]     Train net output #0: loss = 0.0791102 (* 1 = 0.0791102 loss)
I0731 23:15:00.955598 20406 sgd_solver.cpp:136] Iteration 30300, lr = 1e-06, m = 0.9
I0731 23:15:19.531883 20406 solver.cpp:353] Iteration 30400 (5.38334 iter/s, 18.5758s/100 iter), loss = 0.0707658
I0731 23:15:19.531908 20406 solver.cpp:375]     Train net output #0: loss = 0.0707657 (* 1 = 0.0707657 loss)
I0731 23:15:19.531913 20406 sgd_solver.cpp:136] Iteration 30400, lr = 1e-06, m = 0.9
I0731 23:15:21.015668 20410 data_reader.cpp:264] Starting prefetch of epoch 17
I0731 23:15:38.030635 20406 solver.cpp:353] Iteration 30500 (5.40592 iter/s, 18.4982s/100 iter), loss = 0.0740908
I0731 23:15:38.030694 20406 solver.cpp:375]     Train net output #0: loss = 0.0740907 (* 1 = 0.0740907 loss)
I0731 23:15:38.030699 20406 sgd_solver.cpp:136] Iteration 30500, lr = 1e-06, m = 0.9
I0731 23:15:56.720571 20406 solver.cpp:353] Iteration 30600 (5.35062 iter/s, 18.6894s/100 iter), loss = 0.0565714
I0731 23:15:56.720612 20406 solver.cpp:375]     Train net output #0: loss = 0.0565713 (* 1 = 0.0565713 loss)
I0731 23:15:56.720615 20406 sgd_solver.cpp:136] Iteration 30600, lr = 1e-06, m = 0.9
I0731 23:16:15.262223 20406 solver.cpp:353] Iteration 30700 (5.39341 iter/s, 18.5411s/100 iter), loss = 0.104596
I0731 23:16:15.262334 20406 solver.cpp:375]     Train net output #0: loss = 0.104596 (* 1 = 0.104596 loss)
I0731 23:16:15.262341 20406 sgd_solver.cpp:136] Iteration 30700, lr = 1e-06, m = 0.9
I0731 23:16:22.381150 20364 data_reader.cpp:264] Starting prefetch of epoch 22
I0731 23:16:34.003877 20406 solver.cpp:353] Iteration 30800 (5.33586 iter/s, 18.7411s/100 iter), loss = 0.0648787
I0731 23:16:34.003903 20406 solver.cpp:375]     Train net output #0: loss = 0.0648786 (* 1 = 0.0648786 loss)
I0731 23:16:34.003907 20406 sgd_solver.cpp:136] Iteration 30800, lr = 1e-06, m = 0.9
I0731 23:16:52.765228 20406 solver.cpp:353] Iteration 30900 (5.33025 iter/s, 18.7608s/100 iter), loss = 0.0653854
I0731 23:16:52.765303 20406 solver.cpp:375]     Train net output #0: loss = 0.0653853 (* 1 = 0.0653853 loss)
I0731 23:16:52.765308 20406 sgd_solver.cpp:136] Iteration 30900, lr = 1e-06, m = 0.9
I0731 23:16:53.354302 20346 data_reader.cpp:264] Starting prefetch of epoch 20
I0731 23:17:11.238997 20406 solver.cpp:404] Sparsity after update:
I0731 23:17:11.264940 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 23:17:11.265017 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 23:17:11.265038 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 23:17:11.265048 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 23:17:11.265055 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 23:17:11.265063 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 23:17:11.265070 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 23:17:11.265079 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 23:17:11.265085 20406 net.cpp:2270] out3a_param_0(0) 
I0731 23:17:11.265094 20406 net.cpp:2270] out5a_param_0(0) 
I0731 23:17:11.265100 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 23:17:11.265108 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 23:17:11.265115 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 23:17:11.265122 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 23:17:11.265130 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 23:17:11.265137 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 23:17:11.265146 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 23:17:11.265152 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 23:17:11.265159 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 23:17:11.436771 20406 solver.cpp:353] Iteration 31000 (5.35589 iter/s, 18.671s/100 iter), loss = 0.0622772
I0731 23:17:11.436794 20406 solver.cpp:375]     Train net output #0: loss = 0.0622771 (* 1 = 0.0622771 loss)
I0731 23:17:11.436800 20406 sgd_solver.cpp:136] Iteration 31000, lr = 1e-06, m = 0.9
I0731 23:17:30.196768 20406 solver.cpp:353] Iteration 31100 (5.33064 iter/s, 18.7595s/100 iter), loss = 0.0559277
I0731 23:17:30.196823 20406 solver.cpp:375]     Train net output #0: loss = 0.0559276 (* 1 = 0.0559276 loss)
I0731 23:17:30.196828 20406 sgd_solver.cpp:136] Iteration 31100, lr = 1e-06, m = 0.9
I0731 23:17:48.791985 20406 solver.cpp:353] Iteration 31200 (5.37788 iter/s, 18.5947s/100 iter), loss = 0.0943663
I0731 23:17:48.792007 20406 solver.cpp:375]     Train net output #0: loss = 0.0943662 (* 1 = 0.0943662 loss)
I0731 23:17:48.792011 20406 sgd_solver.cpp:136] Iteration 31200, lr = 1e-06, m = 0.9
I0731 23:17:55.137423 20409 data_reader.cpp:264] Starting prefetch of epoch 18
I0731 23:18:07.416762 20406 solver.cpp:353] Iteration 31300 (5.36934 iter/s, 18.6243s/100 iter), loss = 0.0890799
I0731 23:18:07.417325 20406 solver.cpp:375]     Train net output #0: loss = 0.0890798 (* 1 = 0.0890798 loss)
I0731 23:18:07.417335 20406 sgd_solver.cpp:136] Iteration 31300, lr = 1e-06, m = 0.9
I0731 23:18:26.008054 20406 solver.cpp:353] Iteration 31400 (5.37901 iter/s, 18.5908s/100 iter), loss = 0.0572344
I0731 23:18:26.008080 20406 solver.cpp:375]     Train net output #0: loss = 0.0572344 (* 1 = 0.0572344 loss)
I0731 23:18:26.008083 20406 sgd_solver.cpp:136] Iteration 31400, lr = 1e-06, m = 0.9
I0731 23:18:44.463618 20406 solver.cpp:353] Iteration 31500 (5.41857 iter/s, 18.4551s/100 iter), loss = 0.0598888
I0731 23:18:44.463672 20406 solver.cpp:375]     Train net output #0: loss = 0.0598887 (* 1 = 0.0598887 loss)
I0731 23:18:44.463680 20406 sgd_solver.cpp:136] Iteration 31500, lr = 1e-06, m = 0.9
I0731 23:18:56.596601 20412 data_reader.cpp:264] Starting prefetch of epoch 21
I0731 23:19:03.133800 20406 solver.cpp:353] Iteration 31600 (5.35628 iter/s, 18.6697s/100 iter), loss = 0.112467
I0731 23:19:03.133822 20406 solver.cpp:375]     Train net output #0: loss = 0.112467 (* 1 = 0.112467 loss)
I0731 23:19:03.133826 20406 sgd_solver.cpp:136] Iteration 31600, lr = 1e-06, m = 0.9
I0731 23:19:21.717061 20406 solver.cpp:353] Iteration 31700 (5.38134 iter/s, 18.5827s/100 iter), loss = 0.0669986
I0731 23:19:21.717125 20406 solver.cpp:375]     Train net output #0: loss = 0.0669985 (* 1 = 0.0669985 loss)
I0731 23:19:21.717131 20406 sgd_solver.cpp:136] Iteration 31700, lr = 1e-06, m = 0.9
I0731 23:19:40.221477 20406 solver.cpp:353] Iteration 31800 (5.40426 iter/s, 18.5039s/100 iter), loss = 0.0710424
I0731 23:19:40.221501 20406 solver.cpp:375]     Train net output #0: loss = 0.0710423 (* 1 = 0.0710423 loss)
I0731 23:19:40.221505 20406 sgd_solver.cpp:136] Iteration 31800, lr = 1e-06, m = 0.9
I0731 23:19:57.896847 20415 data_reader.cpp:264] Starting prefetch of epoch 16
I0731 23:19:58.785298 20406 solver.cpp:353] Iteration 31900 (5.38697 iter/s, 18.5633s/100 iter), loss = 0.0436161
I0731 23:19:58.785326 20406 solver.cpp:375]     Train net output #0: loss = 0.043616 (* 1 = 0.043616 loss)
I0731 23:19:58.785332 20406 sgd_solver.cpp:136] Iteration 31900, lr = 1e-06, m = 0.9
I0731 23:20:17.252892 20406 solver.cpp:353] Iteration 31999 (5.36089 iter/s, 18.4671s/99 iter), loss = 0.0635586
I0731 23:20:17.252918 20406 solver.cpp:375]     Train net output #0: loss = 0.0635585 (* 1 = 0.0635585 loss)
I0731 23:20:17.252923 20406 solver.cpp:404] Sparsity after update:
I0731 23:20:17.254719 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 23:20:17.254727 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 23:20:17.254730 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 23:20:17.254736 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 23:20:17.254739 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 23:20:17.254741 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 23:20:17.254743 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 23:20:17.254746 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 23:20:17.254748 20406 net.cpp:2270] out3a_param_0(0) 
I0731 23:20:17.254750 20406 net.cpp:2270] out5a_param_0(0) 
I0731 23:20:17.254753 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 23:20:17.254756 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 23:20:17.254758 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 23:20:17.254760 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 23:20:17.254763 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 23:20:17.254765 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 23:20:17.254767 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 23:20:17.254770 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 23:20:17.254772 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 23:20:17.313537 20406 solver.cpp:680] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/sparse/cityscapes5_jsegnet21v2_iter_32000.caffemodel
I0731 23:20:17.425184 20406 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/sparse/cityscapes5_jsegnet21v2_iter_32000.solverstate
I0731 23:20:17.498436 20406 solver.cpp:527] Iteration 32000, loss = 0.0560478
I0731 23:20:17.498464 20406 solver.cpp:550] Iteration 32000, Testing net (#0)
I0731 23:20:20.769330 20402 data_reader.cpp:264] Starting prefetch of epoch 5
I0731 23:20:28.547997 20406 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.95112
I0731 23:20:28.548146 20406 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999798
I0731 23:20:28.548156 20406 solver.cpp:635]     Test net output #2: loss = 0.171002 (* 1 = 0.171002 loss)
I0731 23:20:28.577919 20312 parallel.cpp:73] Root Solver performance on device 0: 5.181 * 6 = 31.09 img/sec (32000 itr in 6176 sec)
I0731 23:20:28.577985 20312 parallel.cpp:78]      Solver performance on device 1: 5.181 * 6 = 31.09 img/sec (32000 itr in 6176 sec)
I0731 23:20:28.578007 20312 parallel.cpp:78]      Solver performance on device 2: 5.181 * 6 = 31.09 img/sec (32000 itr in 6176 sec)
I0731 23:20:28.578018 20312 parallel.cpp:81] Overall multi-GPU performance: 93.2607 img/sec
I0731 23:20:29.846812 20312 caffe.cpp:247] Optimization Done in 1h 43m 14s
training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/test
I0731 23:20:43.839834  7360 caffe.cpp:608] This is NVCaffe 0.16.3 started at Mon Jul 31 23:20:42 2017
I0731 23:20:43.842034  7360 caffe.cpp:611] CuDNN version: 6021
I0731 23:20:43.842039  7360 caffe.cpp:612] CuBLAS version: 8000
I0731 23:20:43.842041  7360 caffe.cpp:613] CUDA version: 8000
I0731 23:20:43.842042  7360 caffe.cpp:614] CUDA driver version: 8000
I0731 23:20:43.842049  7360 caffe.cpp:263] Not using GPU #2 for single-GPU function
I0731 23:20:43.842051  7360 caffe.cpp:263] Not using GPU #1 for single-GPU function
I0731 23:20:43.842633  7360 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0731 23:20:43.843189  7360 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I0731 23:20:43.843194  7360 caffe.cpp:275] Use GPU with device ID 0
I0731 23:20:43.843533  7360 caffe.cpp:279] GPU device name: GeForce GTX 1080
I0731 23:20:43.845191  7360 net.cpp:72] Initializing net from parameters: 
name: "jsegnet21v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/val-image-lmdb"
    label_list_path: "data/val-label-lmdb"
    batch_size: 4
    threads: 1
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
  accuracy_param {
    ignore_label: 255
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
    ignore_label: 255
  }
}
I0731 23:20:43.845357  7360 net.cpp:104] Using FLOAT as default forward math type
I0731 23:20:43.845365  7360 net.cpp:110] Using FLOAT as default backward math type
I0731 23:20:43.845369  7360 layer_factory.hpp:136] Creating layer 'data' of type 'ImageLabelData'
I0731 23:20:43.845376  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:43.845388  7360 net.cpp:184] Created Layer data (0)
I0731 23:20:43.845408  7360 net.cpp:530] data -> data
I0731 23:20:43.864593  7360 net.cpp:530] data -> label
I0731 23:20:43.886404  7360 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 4
I0731 23:20:43.886433  7360 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 23:20:43.926553  7417 db_lmdb.cpp:35] Opened lmdb data/val-image-lmdb
I0731 23:20:43.930811  7360 data_layer.cpp:184] (0) ReshapePrefetch 4, 3, 640, 640
I0731 23:20:43.930891  7360 data_layer.cpp:208] (0) Output data size: 4, 3, 640, 640
I0731 23:20:43.930910  7360 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 23:20:43.931349  7360 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 4
I0731 23:20:43.931380  7360 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 23:20:43.932533  7418 data_layer.cpp:97] (0) Parser threads: 1
I0731 23:20:43.932557  7418 data_layer.cpp:99] (0) Transformer threads: 1
I0731 23:20:43.969446  7419 db_lmdb.cpp:35] Opened lmdb data/val-label-lmdb
I0731 23:20:43.970182  7360 data_layer.cpp:184] (0) ReshapePrefetch 4, 1, 640, 640
I0731 23:20:43.970203  7360 data_layer.cpp:208] (0) Output data size: 4, 1, 640, 640
I0731 23:20:43.970208  7360 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 23:20:43.970252  7360 net.cpp:245] Setting up data
I0731 23:20:43.970263  7360 net.cpp:252] TEST Top shape for layer 0 'data' 4 3 640 640 (4915200)
I0731 23:20:43.970271  7360 net.cpp:252] TEST Top shape for layer 0 'data' 4 1 640 640 (1638400)
I0731 23:20:43.970278  7360 layer_factory.hpp:136] Creating layer 'label_data_1_split' of type 'Split'
I0731 23:20:43.970285  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:43.970302  7360 net.cpp:184] Created Layer label_data_1_split (1)
I0731 23:20:43.970309  7360 net.cpp:561] label_data_1_split <- label
I0731 23:20:43.970317  7360 net.cpp:530] label_data_1_split -> label_data_1_split_0
I0731 23:20:43.970324  7360 net.cpp:530] label_data_1_split -> label_data_1_split_1
I0731 23:20:43.970329  7360 net.cpp:530] label_data_1_split -> label_data_1_split_2
I0731 23:20:43.970357  7360 net.cpp:245] Setting up label_data_1_split
I0731 23:20:43.970363  7360 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 4 1 640 640 (1638400)
I0731 23:20:43.970368  7360 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 4 1 640 640 (1638400)
I0731 23:20:43.970373  7360 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 4 1 640 640 (1638400)
I0731 23:20:43.970377  7360 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0731 23:20:43.970382  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:43.970393  7360 net.cpp:184] Created Layer data/bias (2)
I0731 23:20:43.970397  7360 net.cpp:561] data/bias <- data
I0731 23:20:43.970402  7360 net.cpp:530] data/bias -> data/bias
I0731 23:20:43.971617  7420 data_layer.cpp:97] (0) Parser threads: 1
I0731 23:20:43.971631  7420 data_layer.cpp:99] (0) Transformer threads: 1
I0731 23:20:43.973013  7360 net.cpp:245] Setting up data/bias
I0731 23:20:43.973042  7360 net.cpp:252] TEST Top shape for layer 2 'data/bias' 4 3 640 640 (4915200)
I0731 23:20:43.973062  7360 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0731 23:20:43.973071  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:43.973098  7360 net.cpp:184] Created Layer conv1a (3)
I0731 23:20:43.973103  7360 net.cpp:561] conv1a <- data/bias
I0731 23:20:43.973109  7360 net.cpp:530] conv1a -> conv1a
I0731 23:20:44.679003  7360 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1a' with space 0.01G/1 1  (limit 8.06G, req 0G)
I0731 23:20:44.679024  7360 net.cpp:245] Setting up conv1a
I0731 23:20:44.679030  7360 net.cpp:252] TEST Top shape for layer 3 'conv1a' 4 32 320 320 (13107200)
I0731 23:20:44.679039  7360 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0731 23:20:44.679044  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.679054  7360 net.cpp:184] Created Layer conv1a/bn (4)
I0731 23:20:44.679059  7360 net.cpp:561] conv1a/bn <- conv1a
I0731 23:20:44.679061  7360 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0731 23:20:44.679574  7360 net.cpp:245] Setting up conv1a/bn
I0731 23:20:44.679584  7360 net.cpp:252] TEST Top shape for layer 4 'conv1a/bn' 4 32 320 320 (13107200)
I0731 23:20:44.679591  7360 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0731 23:20:44.679594  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.679600  7360 net.cpp:184] Created Layer conv1a/relu (5)
I0731 23:20:44.679601  7360 net.cpp:561] conv1a/relu <- conv1a
I0731 23:20:44.679605  7360 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0731 23:20:44.679615  7360 net.cpp:245] Setting up conv1a/relu
I0731 23:20:44.679620  7360 net.cpp:252] TEST Top shape for layer 5 'conv1a/relu' 4 32 320 320 (13107200)
I0731 23:20:44.679621  7360 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0731 23:20:44.679625  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.679633  7360 net.cpp:184] Created Layer conv1b (6)
I0731 23:20:44.679636  7360 net.cpp:561] conv1b <- conv1a
I0731 23:20:44.679639  7360 net.cpp:530] conv1b -> conv1b
I0731 23:20:44.697739  7360 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 8G, req 0G)
I0731 23:20:44.697751  7360 net.cpp:245] Setting up conv1b
I0731 23:20:44.697753  7360 net.cpp:252] TEST Top shape for layer 6 'conv1b' 4 32 320 320 (13107200)
I0731 23:20:44.697759  7360 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0731 23:20:44.697762  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.697775  7360 net.cpp:184] Created Layer conv1b/bn (7)
I0731 23:20:44.697778  7360 net.cpp:561] conv1b/bn <- conv1b
I0731 23:20:44.697782  7360 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0731 23:20:44.698266  7360 net.cpp:245] Setting up conv1b/bn
I0731 23:20:44.698274  7360 net.cpp:252] TEST Top shape for layer 7 'conv1b/bn' 4 32 320 320 (13107200)
I0731 23:20:44.698281  7360 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0731 23:20:44.698283  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.698287  7360 net.cpp:184] Created Layer conv1b/relu (8)
I0731 23:20:44.698289  7360 net.cpp:561] conv1b/relu <- conv1b
I0731 23:20:44.698292  7360 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0731 23:20:44.698294  7360 net.cpp:245] Setting up conv1b/relu
I0731 23:20:44.698297  7360 net.cpp:252] TEST Top shape for layer 8 'conv1b/relu' 4 32 320 320 (13107200)
I0731 23:20:44.698299  7360 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0731 23:20:44.698302  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.698307  7360 net.cpp:184] Created Layer pool1 (9)
I0731 23:20:44.698308  7360 net.cpp:561] pool1 <- conv1b
I0731 23:20:44.698312  7360 net.cpp:530] pool1 -> pool1
I0731 23:20:44.698874  7360 net.cpp:245] Setting up pool1
I0731 23:20:44.698884  7360 net.cpp:252] TEST Top shape for layer 9 'pool1' 4 32 160 160 (3276800)
I0731 23:20:44.698889  7360 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0731 23:20:44.698891  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.698914  7360 net.cpp:184] Created Layer res2a_branch2a (10)
I0731 23:20:44.698916  7360 net.cpp:561] res2a_branch2a <- pool1
I0731 23:20:44.698920  7360 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0731 23:20:44.711679  7360 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.95G, req 0G)
I0731 23:20:44.711694  7360 net.cpp:245] Setting up res2a_branch2a
I0731 23:20:44.711699  7360 net.cpp:252] TEST Top shape for layer 10 'res2a_branch2a' 4 64 160 160 (6553600)
I0731 23:20:44.711709  7360 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0731 23:20:44.711711  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.711719  7360 net.cpp:184] Created Layer res2a_branch2a/bn (11)
I0731 23:20:44.711722  7360 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0731 23:20:44.711725  7360 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0731 23:20:44.712252  7360 net.cpp:245] Setting up res2a_branch2a/bn
I0731 23:20:44.712260  7360 net.cpp:252] TEST Top shape for layer 11 'res2a_branch2a/bn' 4 64 160 160 (6553600)
I0731 23:20:44.712266  7360 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0731 23:20:44.712270  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.712273  7360 net.cpp:184] Created Layer res2a_branch2a/relu (12)
I0731 23:20:44.712277  7360 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0731 23:20:44.712280  7360 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0731 23:20:44.712285  7360 net.cpp:245] Setting up res2a_branch2a/relu
I0731 23:20:44.712287  7360 net.cpp:252] TEST Top shape for layer 12 'res2a_branch2a/relu' 4 64 160 160 (6553600)
I0731 23:20:44.712290  7360 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0731 23:20:44.712291  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.712301  7360 net.cpp:184] Created Layer res2a_branch2b (13)
I0731 23:20:44.712303  7360 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0731 23:20:44.712306  7360 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0731 23:20:44.721138  7360 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.93G, req 0G)
I0731 23:20:44.721151  7360 net.cpp:245] Setting up res2a_branch2b
I0731 23:20:44.721156  7360 net.cpp:252] TEST Top shape for layer 13 'res2a_branch2b' 4 64 160 160 (6553600)
I0731 23:20:44.721161  7360 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0731 23:20:44.721165  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.721171  7360 net.cpp:184] Created Layer res2a_branch2b/bn (14)
I0731 23:20:44.721174  7360 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0731 23:20:44.721176  7360 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0731 23:20:44.721683  7360 net.cpp:245] Setting up res2a_branch2b/bn
I0731 23:20:44.721691  7360 net.cpp:252] TEST Top shape for layer 14 'res2a_branch2b/bn' 4 64 160 160 (6553600)
I0731 23:20:44.721698  7360 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0731 23:20:44.721701  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.721704  7360 net.cpp:184] Created Layer res2a_branch2b/relu (15)
I0731 23:20:44.721707  7360 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0731 23:20:44.721709  7360 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0731 23:20:44.721714  7360 net.cpp:245] Setting up res2a_branch2b/relu
I0731 23:20:44.721715  7360 net.cpp:252] TEST Top shape for layer 15 'res2a_branch2b/relu' 4 64 160 160 (6553600)
I0731 23:20:44.721717  7360 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0731 23:20:44.721720  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.721735  7360 net.cpp:184] Created Layer pool2 (16)
I0731 23:20:44.721738  7360 net.cpp:561] pool2 <- res2a_branch2b
I0731 23:20:44.721740  7360 net.cpp:530] pool2 -> pool2
I0731 23:20:44.721771  7360 net.cpp:245] Setting up pool2
I0731 23:20:44.721776  7360 net.cpp:252] TEST Top shape for layer 16 'pool2' 4 64 80 80 (1638400)
I0731 23:20:44.721778  7360 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0731 23:20:44.721781  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.721789  7360 net.cpp:184] Created Layer res3a_branch2a (17)
I0731 23:20:44.721793  7360 net.cpp:561] res3a_branch2a <- pool2
I0731 23:20:44.721797  7360 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0731 23:20:44.729562  7360 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.9G, req 0G)
I0731 23:20:44.729573  7360 net.cpp:245] Setting up res3a_branch2a
I0731 23:20:44.729576  7360 net.cpp:252] TEST Top shape for layer 17 'res3a_branch2a' 4 128 80 80 (3276800)
I0731 23:20:44.729580  7360 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0731 23:20:44.729583  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.729588  7360 net.cpp:184] Created Layer res3a_branch2a/bn (18)
I0731 23:20:44.729589  7360 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0731 23:20:44.729593  7360 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0731 23:20:44.730422  7360 net.cpp:245] Setting up res3a_branch2a/bn
I0731 23:20:44.730432  7360 net.cpp:252] TEST Top shape for layer 18 'res3a_branch2a/bn' 4 128 80 80 (3276800)
I0731 23:20:44.730438  7360 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0731 23:20:44.730442  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.730444  7360 net.cpp:184] Created Layer res3a_branch2a/relu (19)
I0731 23:20:44.730448  7360 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0731 23:20:44.730449  7360 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0731 23:20:44.730453  7360 net.cpp:245] Setting up res3a_branch2a/relu
I0731 23:20:44.730455  7360 net.cpp:252] TEST Top shape for layer 19 'res3a_branch2a/relu' 4 128 80 80 (3276800)
I0731 23:20:44.730458  7360 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0731 23:20:44.730460  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.730465  7360 net.cpp:184] Created Layer res3a_branch2b (20)
I0731 23:20:44.730468  7360 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0731 23:20:44.730471  7360 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0731 23:20:44.736443  7360 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.89G, req 0G)
I0731 23:20:44.736452  7360 net.cpp:245] Setting up res3a_branch2b
I0731 23:20:44.736457  7360 net.cpp:252] TEST Top shape for layer 20 'res3a_branch2b' 4 128 80 80 (3276800)
I0731 23:20:44.736461  7360 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0731 23:20:44.736464  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.736469  7360 net.cpp:184] Created Layer res3a_branch2b/bn (21)
I0731 23:20:44.736470  7360 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0731 23:20:44.736474  7360 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0731 23:20:44.736956  7360 net.cpp:245] Setting up res3a_branch2b/bn
I0731 23:20:44.736964  7360 net.cpp:252] TEST Top shape for layer 21 'res3a_branch2b/bn' 4 128 80 80 (3276800)
I0731 23:20:44.736970  7360 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0731 23:20:44.736974  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.736976  7360 net.cpp:184] Created Layer res3a_branch2b/relu (22)
I0731 23:20:44.736985  7360 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0731 23:20:44.736989  7360 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0731 23:20:44.736992  7360 net.cpp:245] Setting up res3a_branch2b/relu
I0731 23:20:44.736994  7360 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b/relu' 4 128 80 80 (3276800)
I0731 23:20:44.736996  7360 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0731 23:20:44.736999  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.737001  7360 net.cpp:184] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (23)
I0731 23:20:44.737004  7360 net.cpp:561] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0731 23:20:44.737006  7360 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0731 23:20:44.737010  7360 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0731 23:20:44.737031  7360 net.cpp:245] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0731 23:20:44.737035  7360 net.cpp:252] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 4 128 80 80 (3276800)
I0731 23:20:44.737038  7360 net.cpp:252] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 4 128 80 80 (3276800)
I0731 23:20:44.737040  7360 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0731 23:20:44.737042  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.737046  7360 net.cpp:184] Created Layer pool3 (24)
I0731 23:20:44.737048  7360 net.cpp:561] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0731 23:20:44.737051  7360 net.cpp:530] pool3 -> pool3
I0731 23:20:44.737085  7360 net.cpp:245] Setting up pool3
I0731 23:20:44.737093  7360 net.cpp:252] TEST Top shape for layer 24 'pool3' 4 128 40 40 (819200)
I0731 23:20:44.737097  7360 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0731 23:20:44.737100  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.737108  7360 net.cpp:184] Created Layer res4a_branch2a (25)
I0731 23:20:44.737112  7360 net.cpp:561] res4a_branch2a <- pool3
I0731 23:20:44.737115  7360 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0731 23:20:44.751196  7360 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.87G, req 0G)
I0731 23:20:44.751212  7360 net.cpp:245] Setting up res4a_branch2a
I0731 23:20:44.751219  7360 net.cpp:252] TEST Top shape for layer 25 'res4a_branch2a' 4 256 40 40 (1638400)
I0731 23:20:44.751224  7360 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0731 23:20:44.751229  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.751235  7360 net.cpp:184] Created Layer res4a_branch2a/bn (26)
I0731 23:20:44.751237  7360 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0731 23:20:44.751241  7360 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0731 23:20:44.751745  7360 net.cpp:245] Setting up res4a_branch2a/bn
I0731 23:20:44.751754  7360 net.cpp:252] TEST Top shape for layer 26 'res4a_branch2a/bn' 4 256 40 40 (1638400)
I0731 23:20:44.751760  7360 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0731 23:20:44.751763  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.751766  7360 net.cpp:184] Created Layer res4a_branch2a/relu (27)
I0731 23:20:44.751768  7360 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0731 23:20:44.751771  7360 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0731 23:20:44.751775  7360 net.cpp:245] Setting up res4a_branch2a/relu
I0731 23:20:44.751777  7360 net.cpp:252] TEST Top shape for layer 27 'res4a_branch2a/relu' 4 256 40 40 (1638400)
I0731 23:20:44.751780  7360 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0731 23:20:44.751792  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.751801  7360 net.cpp:184] Created Layer res4a_branch2b (28)
I0731 23:20:44.751803  7360 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0731 23:20:44.751806  7360 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0731 23:20:44.758215  7360 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.86G, req 0G)
I0731 23:20:44.758225  7360 net.cpp:245] Setting up res4a_branch2b
I0731 23:20:44.758229  7360 net.cpp:252] TEST Top shape for layer 28 'res4a_branch2b' 4 256 40 40 (1638400)
I0731 23:20:44.758234  7360 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0731 23:20:44.758236  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.758241  7360 net.cpp:184] Created Layer res4a_branch2b/bn (29)
I0731 23:20:44.758244  7360 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0731 23:20:44.758246  7360 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0731 23:20:44.758726  7360 net.cpp:245] Setting up res4a_branch2b/bn
I0731 23:20:44.758734  7360 net.cpp:252] TEST Top shape for layer 29 'res4a_branch2b/bn' 4 256 40 40 (1638400)
I0731 23:20:44.758740  7360 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0731 23:20:44.758744  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.758746  7360 net.cpp:184] Created Layer res4a_branch2b/relu (30)
I0731 23:20:44.758749  7360 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0731 23:20:44.758751  7360 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0731 23:20:44.758754  7360 net.cpp:245] Setting up res4a_branch2b/relu
I0731 23:20:44.758757  7360 net.cpp:252] TEST Top shape for layer 30 'res4a_branch2b/relu' 4 256 40 40 (1638400)
I0731 23:20:44.758759  7360 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0731 23:20:44.758761  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.758767  7360 net.cpp:184] Created Layer pool4 (31)
I0731 23:20:44.758769  7360 net.cpp:561] pool4 <- res4a_branch2b
I0731 23:20:44.758774  7360 net.cpp:530] pool4 -> pool4
I0731 23:20:44.758805  7360 net.cpp:245] Setting up pool4
I0731 23:20:44.758810  7360 net.cpp:252] TEST Top shape for layer 31 'pool4' 4 256 40 40 (1638400)
I0731 23:20:44.758811  7360 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0731 23:20:44.758815  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.758821  7360 net.cpp:184] Created Layer res5a_branch2a (32)
I0731 23:20:44.758823  7360 net.cpp:561] res5a_branch2a <- pool4
I0731 23:20:44.758826  7360 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0731 23:20:44.784338  7360 net.cpp:245] Setting up res5a_branch2a
I0731 23:20:44.784359  7360 net.cpp:252] TEST Top shape for layer 32 'res5a_branch2a' 4 512 40 40 (3276800)
I0731 23:20:44.784366  7360 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0731 23:20:44.784370  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.784377  7360 net.cpp:184] Created Layer res5a_branch2a/bn (33)
I0731 23:20:44.784380  7360 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0731 23:20:44.784384  7360 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0731 23:20:44.784868  7360 net.cpp:245] Setting up res5a_branch2a/bn
I0731 23:20:44.784876  7360 net.cpp:252] TEST Top shape for layer 33 'res5a_branch2a/bn' 4 512 40 40 (3276800)
I0731 23:20:44.784883  7360 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0731 23:20:44.784885  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.784888  7360 net.cpp:184] Created Layer res5a_branch2a/relu (34)
I0731 23:20:44.784900  7360 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0731 23:20:44.784904  7360 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0731 23:20:44.784907  7360 net.cpp:245] Setting up res5a_branch2a/relu
I0731 23:20:44.784910  7360 net.cpp:252] TEST Top shape for layer 34 'res5a_branch2a/relu' 4 512 40 40 (3276800)
I0731 23:20:44.784912  7360 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0731 23:20:44.784914  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.784921  7360 net.cpp:184] Created Layer res5a_branch2b (35)
I0731 23:20:44.784924  7360 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0731 23:20:44.784926  7360 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0731 23:20:44.797497  7360 net.cpp:245] Setting up res5a_branch2b
I0731 23:20:44.797505  7360 net.cpp:252] TEST Top shape for layer 35 'res5a_branch2b' 4 512 40 40 (3276800)
I0731 23:20:44.797513  7360 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0731 23:20:44.797516  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.797521  7360 net.cpp:184] Created Layer res5a_branch2b/bn (36)
I0731 23:20:44.797523  7360 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0731 23:20:44.797526  7360 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0731 23:20:44.797976  7360 net.cpp:245] Setting up res5a_branch2b/bn
I0731 23:20:44.797984  7360 net.cpp:252] TEST Top shape for layer 36 'res5a_branch2b/bn' 4 512 40 40 (3276800)
I0731 23:20:44.797991  7360 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0731 23:20:44.797992  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.797996  7360 net.cpp:184] Created Layer res5a_branch2b/relu (37)
I0731 23:20:44.797998  7360 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0731 23:20:44.798002  7360 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0731 23:20:44.798005  7360 net.cpp:245] Setting up res5a_branch2b/relu
I0731 23:20:44.798007  7360 net.cpp:252] TEST Top shape for layer 37 'res5a_branch2b/relu' 4 512 40 40 (3276800)
I0731 23:20:44.798009  7360 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I0731 23:20:44.798012  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.798017  7360 net.cpp:184] Created Layer out5a (38)
I0731 23:20:44.798019  7360 net.cpp:561] out5a <- res5a_branch2b
I0731 23:20:44.798022  7360 net.cpp:530] out5a -> out5a
I0731 23:20:44.802116  7360 net.cpp:245] Setting up out5a
I0731 23:20:44.802142  7360 net.cpp:252] TEST Top shape for layer 38 'out5a' 4 64 40 40 (409600)
I0731 23:20:44.802150  7360 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I0731 23:20:44.802153  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.802160  7360 net.cpp:184] Created Layer out5a/bn (39)
I0731 23:20:44.802165  7360 net.cpp:561] out5a/bn <- out5a
I0731 23:20:44.802167  7360 net.cpp:513] out5a/bn -> out5a (in-place)
I0731 23:20:44.802637  7360 net.cpp:245] Setting up out5a/bn
I0731 23:20:44.802645  7360 net.cpp:252] TEST Top shape for layer 39 'out5a/bn' 4 64 40 40 (409600)
I0731 23:20:44.802651  7360 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I0731 23:20:44.802655  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.802659  7360 net.cpp:184] Created Layer out5a/relu (40)
I0731 23:20:44.802661  7360 net.cpp:561] out5a/relu <- out5a
I0731 23:20:44.802664  7360 net.cpp:513] out5a/relu -> out5a (in-place)
I0731 23:20:44.802670  7360 net.cpp:245] Setting up out5a/relu
I0731 23:20:44.802676  7360 net.cpp:252] TEST Top shape for layer 40 'out5a/relu' 4 64 40 40 (409600)
I0731 23:20:44.802680  7360 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I0731 23:20:44.802693  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.802706  7360 net.cpp:184] Created Layer out5a_up2 (41)
I0731 23:20:44.802709  7360 net.cpp:561] out5a_up2 <- out5a
I0731 23:20:44.802712  7360 net.cpp:530] out5a_up2 -> out5a_up2
I0731 23:20:44.802872  7360 net.cpp:245] Setting up out5a_up2
I0731 23:20:44.802878  7360 net.cpp:252] TEST Top shape for layer 41 'out5a_up2' 4 64 80 80 (1638400)
I0731 23:20:44.802881  7360 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I0731 23:20:44.802884  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.802891  7360 net.cpp:184] Created Layer out3a (42)
I0731 23:20:44.802893  7360 net.cpp:561] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0731 23:20:44.802896  7360 net.cpp:530] out3a -> out3a
I0731 23:20:44.807472  7360 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 7.84G, req 0G)
I0731 23:20:44.807483  7360 net.cpp:245] Setting up out3a
I0731 23:20:44.807487  7360 net.cpp:252] TEST Top shape for layer 42 'out3a' 4 64 80 80 (1638400)
I0731 23:20:44.807492  7360 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I0731 23:20:44.807497  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.807507  7360 net.cpp:184] Created Layer out3a/bn (43)
I0731 23:20:44.807510  7360 net.cpp:561] out3a/bn <- out3a
I0731 23:20:44.807513  7360 net.cpp:513] out3a/bn -> out3a (in-place)
I0731 23:20:44.807996  7360 net.cpp:245] Setting up out3a/bn
I0731 23:20:44.808003  7360 net.cpp:252] TEST Top shape for layer 43 'out3a/bn' 4 64 80 80 (1638400)
I0731 23:20:44.808009  7360 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I0731 23:20:44.808012  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.808017  7360 net.cpp:184] Created Layer out3a/relu (44)
I0731 23:20:44.808019  7360 net.cpp:561] out3a/relu <- out3a
I0731 23:20:44.808023  7360 net.cpp:513] out3a/relu -> out3a (in-place)
I0731 23:20:44.808027  7360 net.cpp:245] Setting up out3a/relu
I0731 23:20:44.808030  7360 net.cpp:252] TEST Top shape for layer 44 'out3a/relu' 4 64 80 80 (1638400)
I0731 23:20:44.808033  7360 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I0731 23:20:44.808035  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.808496  7360 net.cpp:184] Created Layer out3_out5_combined (45)
I0731 23:20:44.808506  7360 net.cpp:561] out3_out5_combined <- out5a_up2
I0731 23:20:44.808511  7360 net.cpp:561] out3_out5_combined <- out3a
I0731 23:20:44.808516  7360 net.cpp:530] out3_out5_combined -> out3_out5_combined
I0731 23:20:44.809581  7360 net.cpp:245] Setting up out3_out5_combined
I0731 23:20:44.809590  7360 net.cpp:252] TEST Top shape for layer 45 'out3_out5_combined' 4 64 80 80 (1638400)
I0731 23:20:44.809594  7360 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I0731 23:20:44.809598  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.809612  7360 net.cpp:184] Created Layer ctx_conv1 (46)
I0731 23:20:44.809617  7360 net.cpp:561] ctx_conv1 <- out3_out5_combined
I0731 23:20:44.809620  7360 net.cpp:530] ctx_conv1 -> ctx_conv1
I0731 23:20:44.815171  7360 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 7.81G, req 0G)
I0731 23:20:44.815182  7360 net.cpp:245] Setting up ctx_conv1
I0731 23:20:44.815186  7360 net.cpp:252] TEST Top shape for layer 46 'ctx_conv1' 4 64 80 80 (1638400)
I0731 23:20:44.815191  7360 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I0731 23:20:44.815193  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.815203  7360 net.cpp:184] Created Layer ctx_conv1/bn (47)
I0731 23:20:44.815214  7360 net.cpp:561] ctx_conv1/bn <- ctx_conv1
I0731 23:20:44.815217  7360 net.cpp:513] ctx_conv1/bn -> ctx_conv1 (in-place)
I0731 23:20:44.815732  7360 net.cpp:245] Setting up ctx_conv1/bn
I0731 23:20:44.815740  7360 net.cpp:252] TEST Top shape for layer 47 'ctx_conv1/bn' 4 64 80 80 (1638400)
I0731 23:20:44.815747  7360 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I0731 23:20:44.815749  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.815753  7360 net.cpp:184] Created Layer ctx_conv1/relu (48)
I0731 23:20:44.815755  7360 net.cpp:561] ctx_conv1/relu <- ctx_conv1
I0731 23:20:44.815757  7360 net.cpp:513] ctx_conv1/relu -> ctx_conv1 (in-place)
I0731 23:20:44.815762  7360 net.cpp:245] Setting up ctx_conv1/relu
I0731 23:20:44.815763  7360 net.cpp:252] TEST Top shape for layer 48 'ctx_conv1/relu' 4 64 80 80 (1638400)
I0731 23:20:44.815765  7360 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I0731 23:20:44.815768  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.815780  7360 net.cpp:184] Created Layer ctx_conv2 (49)
I0731 23:20:44.815783  7360 net.cpp:561] ctx_conv2 <- ctx_conv1
I0731 23:20:44.815785  7360 net.cpp:530] ctx_conv2 -> ctx_conv2
I0731 23:20:44.816787  7360 net.cpp:245] Setting up ctx_conv2
I0731 23:20:44.816794  7360 net.cpp:252] TEST Top shape for layer 49 'ctx_conv2' 4 64 80 80 (1638400)
I0731 23:20:44.816798  7360 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I0731 23:20:44.816802  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.816805  7360 net.cpp:184] Created Layer ctx_conv2/bn (50)
I0731 23:20:44.816807  7360 net.cpp:561] ctx_conv2/bn <- ctx_conv2
I0731 23:20:44.816809  7360 net.cpp:513] ctx_conv2/bn -> ctx_conv2 (in-place)
I0731 23:20:44.817307  7360 net.cpp:245] Setting up ctx_conv2/bn
I0731 23:20:44.817315  7360 net.cpp:252] TEST Top shape for layer 50 'ctx_conv2/bn' 4 64 80 80 (1638400)
I0731 23:20:44.817320  7360 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I0731 23:20:44.817323  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.817327  7360 net.cpp:184] Created Layer ctx_conv2/relu (51)
I0731 23:20:44.817328  7360 net.cpp:561] ctx_conv2/relu <- ctx_conv2
I0731 23:20:44.817330  7360 net.cpp:513] ctx_conv2/relu -> ctx_conv2 (in-place)
I0731 23:20:44.817334  7360 net.cpp:245] Setting up ctx_conv2/relu
I0731 23:20:44.817337  7360 net.cpp:252] TEST Top shape for layer 51 'ctx_conv2/relu' 4 64 80 80 (1638400)
I0731 23:20:44.817338  7360 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I0731 23:20:44.817340  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.817345  7360 net.cpp:184] Created Layer ctx_conv3 (52)
I0731 23:20:44.817348  7360 net.cpp:561] ctx_conv3 <- ctx_conv2
I0731 23:20:44.817351  7360 net.cpp:530] ctx_conv3 -> ctx_conv3
I0731 23:20:44.818343  7360 net.cpp:245] Setting up ctx_conv3
I0731 23:20:44.818351  7360 net.cpp:252] TEST Top shape for layer 52 'ctx_conv3' 4 64 80 80 (1638400)
I0731 23:20:44.818354  7360 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I0731 23:20:44.818357  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.818361  7360 net.cpp:184] Created Layer ctx_conv3/bn (53)
I0731 23:20:44.818363  7360 net.cpp:561] ctx_conv3/bn <- ctx_conv3
I0731 23:20:44.818366  7360 net.cpp:513] ctx_conv3/bn -> ctx_conv3 (in-place)
I0731 23:20:44.818819  7360 net.cpp:245] Setting up ctx_conv3/bn
I0731 23:20:44.818826  7360 net.cpp:252] TEST Top shape for layer 53 'ctx_conv3/bn' 4 64 80 80 (1638400)
I0731 23:20:44.818832  7360 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I0731 23:20:44.818835  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.818843  7360 net.cpp:184] Created Layer ctx_conv3/relu (54)
I0731 23:20:44.818845  7360 net.cpp:561] ctx_conv3/relu <- ctx_conv3
I0731 23:20:44.818848  7360 net.cpp:513] ctx_conv3/relu -> ctx_conv3 (in-place)
I0731 23:20:44.818851  7360 net.cpp:245] Setting up ctx_conv3/relu
I0731 23:20:44.818855  7360 net.cpp:252] TEST Top shape for layer 54 'ctx_conv3/relu' 4 64 80 80 (1638400)
I0731 23:20:44.818856  7360 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I0731 23:20:44.818858  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.818867  7360 net.cpp:184] Created Layer ctx_conv4 (55)
I0731 23:20:44.818871  7360 net.cpp:561] ctx_conv4 <- ctx_conv3
I0731 23:20:44.818872  7360 net.cpp:530] ctx_conv4 -> ctx_conv4
I0731 23:20:44.819814  7360 net.cpp:245] Setting up ctx_conv4
I0731 23:20:44.819823  7360 net.cpp:252] TEST Top shape for layer 55 'ctx_conv4' 4 64 80 80 (1638400)
I0731 23:20:44.819826  7360 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I0731 23:20:44.819828  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.819831  7360 net.cpp:184] Created Layer ctx_conv4/bn (56)
I0731 23:20:44.819834  7360 net.cpp:561] ctx_conv4/bn <- ctx_conv4
I0731 23:20:44.819836  7360 net.cpp:513] ctx_conv4/bn -> ctx_conv4 (in-place)
I0731 23:20:44.820261  7360 net.cpp:245] Setting up ctx_conv4/bn
I0731 23:20:44.820268  7360 net.cpp:252] TEST Top shape for layer 56 'ctx_conv4/bn' 4 64 80 80 (1638400)
I0731 23:20:44.820273  7360 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I0731 23:20:44.820276  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.820278  7360 net.cpp:184] Created Layer ctx_conv4/relu (57)
I0731 23:20:44.820281  7360 net.cpp:561] ctx_conv4/relu <- ctx_conv4
I0731 23:20:44.820282  7360 net.cpp:513] ctx_conv4/relu -> ctx_conv4 (in-place)
I0731 23:20:44.820286  7360 net.cpp:245] Setting up ctx_conv4/relu
I0731 23:20:44.820288  7360 net.cpp:252] TEST Top shape for layer 57 'ctx_conv4/relu' 4 64 80 80 (1638400)
I0731 23:20:44.820291  7360 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I0731 23:20:44.820292  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.820299  7360 net.cpp:184] Created Layer ctx_final (58)
I0731 23:20:44.820303  7360 net.cpp:561] ctx_final <- ctx_conv4
I0731 23:20:44.820305  7360 net.cpp:530] ctx_final -> ctx_final
I0731 23:20:44.825536  7360 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 7.81G, req 0G)
I0731 23:20:44.825563  7360 net.cpp:245] Setting up ctx_final
I0731 23:20:44.825572  7360 net.cpp:252] TEST Top shape for layer 58 'ctx_final' 4 8 80 80 (204800)
I0731 23:20:44.825583  7360 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I0731 23:20:44.825599  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.825613  7360 net.cpp:184] Created Layer ctx_final/relu (59)
I0731 23:20:44.825618  7360 net.cpp:561] ctx_final/relu <- ctx_final
I0731 23:20:44.825625  7360 net.cpp:513] ctx_final/relu -> ctx_final (in-place)
I0731 23:20:44.825636  7360 net.cpp:245] Setting up ctx_final/relu
I0731 23:20:44.825641  7360 net.cpp:252] TEST Top shape for layer 59 'ctx_final/relu' 4 8 80 80 (204800)
I0731 23:20:44.825645  7360 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I0731 23:20:44.825650  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.825666  7360 net.cpp:184] Created Layer out_deconv_final_up2 (60)
I0731 23:20:44.825670  7360 net.cpp:561] out_deconv_final_up2 <- ctx_final
I0731 23:20:44.825675  7360 net.cpp:530] out_deconv_final_up2 -> out_deconv_final_up2
I0731 23:20:44.825914  7360 net.cpp:245] Setting up out_deconv_final_up2
I0731 23:20:44.825924  7360 net.cpp:252] TEST Top shape for layer 60 'out_deconv_final_up2' 4 8 160 160 (819200)
I0731 23:20:44.825947  7360 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I0731 23:20:44.825951  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.825959  7360 net.cpp:184] Created Layer out_deconv_final_up4 (61)
I0731 23:20:44.825964  7360 net.cpp:561] out_deconv_final_up4 <- out_deconv_final_up2
I0731 23:20:44.825969  7360 net.cpp:530] out_deconv_final_up4 -> out_deconv_final_up4
I0731 23:20:44.826149  7360 net.cpp:245] Setting up out_deconv_final_up4
I0731 23:20:44.826159  7360 net.cpp:252] TEST Top shape for layer 61 'out_deconv_final_up4' 4 8 320 320 (3276800)
I0731 23:20:44.826165  7360 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I0731 23:20:44.826170  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.826179  7360 net.cpp:184] Created Layer out_deconv_final_up8 (62)
I0731 23:20:44.826184  7360 net.cpp:561] out_deconv_final_up8 <- out_deconv_final_up4
I0731 23:20:44.826189  7360 net.cpp:530] out_deconv_final_up8 -> out_deconv_final_up8
I0731 23:20:44.826375  7360 net.cpp:245] Setting up out_deconv_final_up8
I0731 23:20:44.826382  7360 net.cpp:252] TEST Top shape for layer 62 'out_deconv_final_up8' 4 8 640 640 (13107200)
I0731 23:20:44.826388  7360 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8_out_deconv_final_up8_0_split' of type 'Split'
I0731 23:20:44.826393  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.826400  7360 net.cpp:184] Created Layer out_deconv_final_up8_out_deconv_final_up8_0_split (63)
I0731 23:20:44.826405  7360 net.cpp:561] out_deconv_final_up8_out_deconv_final_up8_0_split <- out_deconv_final_up8
I0731 23:20:44.826409  7360 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0731 23:20:44.826414  7360 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0731 23:20:44.826421  7360 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0731 23:20:44.826460  7360 net.cpp:245] Setting up out_deconv_final_up8_out_deconv_final_up8_0_split
I0731 23:20:44.826467  7360 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 4 8 640 640 (13107200)
I0731 23:20:44.826472  7360 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 4 8 640 640 (13107200)
I0731 23:20:44.826478  7360 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 4 8 640 640 (13107200)
I0731 23:20:44.826481  7360 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0731 23:20:44.826485  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.826505  7360 net.cpp:184] Created Layer loss (64)
I0731 23:20:44.826510  7360 net.cpp:561] loss <- out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0731 23:20:44.826514  7360 net.cpp:561] loss <- label_data_1_split_0
I0731 23:20:44.826519  7360 net.cpp:530] loss -> loss
I0731 23:20:44.827612  7360 net.cpp:245] Setting up loss
I0731 23:20:44.827622  7360 net.cpp:252] TEST Top shape for layer 64 'loss' (1)
I0731 23:20:44.827625  7360 net.cpp:256]     with loss weight 1
I0731 23:20:44.827633  7360 layer_factory.hpp:136] Creating layer 'accuracy/top1' of type 'Accuracy'
I0731 23:20:44.827636  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.827643  7360 net.cpp:184] Created Layer accuracy/top1 (65)
I0731 23:20:44.827646  7360 net.cpp:561] accuracy/top1 <- out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0731 23:20:44.827649  7360 net.cpp:561] accuracy/top1 <- label_data_1_split_1
I0731 23:20:44.827652  7360 net.cpp:530] accuracy/top1 -> accuracy/top1
I0731 23:20:44.827666  7360 net.cpp:245] Setting up accuracy/top1
I0731 23:20:44.827668  7360 net.cpp:252] TEST Top shape for layer 65 'accuracy/top1' (1)
I0731 23:20:44.827672  7360 layer_factory.hpp:136] Creating layer 'accuracy/top5' of type 'Accuracy'
I0731 23:20:44.827673  7360 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 23:20:44.827677  7360 net.cpp:184] Created Layer accuracy/top5 (66)
I0731 23:20:44.827679  7360 net.cpp:561] accuracy/top5 <- out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0731 23:20:44.827682  7360 net.cpp:561] accuracy/top5 <- label_data_1_split_2
I0731 23:20:44.827685  7360 net.cpp:530] accuracy/top5 -> accuracy/top5
I0731 23:20:44.827689  7360 net.cpp:245] Setting up accuracy/top5
I0731 23:20:44.827692  7360 net.cpp:252] TEST Top shape for layer 66 'accuracy/top5' (1)
I0731 23:20:44.827695  7360 net.cpp:325] accuracy/top5 does not need backward computation.
I0731 23:20:44.827697  7360 net.cpp:325] accuracy/top1 does not need backward computation.
I0731 23:20:44.827700  7360 net.cpp:323] loss needs backward computation.
I0731 23:20:44.827702  7360 net.cpp:323] out_deconv_final_up8_out_deconv_final_up8_0_split needs backward computation.
I0731 23:20:44.827705  7360 net.cpp:323] out_deconv_final_up8 needs backward computation.
I0731 23:20:44.827708  7360 net.cpp:323] out_deconv_final_up4 needs backward computation.
I0731 23:20:44.827709  7360 net.cpp:323] out_deconv_final_up2 needs backward computation.
I0731 23:20:44.827713  7360 net.cpp:323] ctx_final/relu needs backward computation.
I0731 23:20:44.827714  7360 net.cpp:323] ctx_final needs backward computation.
I0731 23:20:44.827718  7360 net.cpp:323] ctx_conv4/relu needs backward computation.
I0731 23:20:44.827719  7360 net.cpp:323] ctx_conv4/bn needs backward computation.
I0731 23:20:44.827723  7360 net.cpp:323] ctx_conv4 needs backward computation.
I0731 23:20:44.827724  7360 net.cpp:323] ctx_conv3/relu needs backward computation.
I0731 23:20:44.827726  7360 net.cpp:323] ctx_conv3/bn needs backward computation.
I0731 23:20:44.827728  7360 net.cpp:323] ctx_conv3 needs backward computation.
I0731 23:20:44.827731  7360 net.cpp:323] ctx_conv2/relu needs backward computation.
I0731 23:20:44.827733  7360 net.cpp:323] ctx_conv2/bn needs backward computation.
I0731 23:20:44.827735  7360 net.cpp:323] ctx_conv2 needs backward computation.
I0731 23:20:44.827738  7360 net.cpp:323] ctx_conv1/relu needs backward computation.
I0731 23:20:44.827740  7360 net.cpp:323] ctx_conv1/bn needs backward computation.
I0731 23:20:44.827742  7360 net.cpp:323] ctx_conv1 needs backward computation.
I0731 23:20:44.827744  7360 net.cpp:323] out3_out5_combined needs backward computation.
I0731 23:20:44.827747  7360 net.cpp:323] out3a/relu needs backward computation.
I0731 23:20:44.827749  7360 net.cpp:323] out3a/bn needs backward computation.
I0731 23:20:44.827752  7360 net.cpp:323] out3a needs backward computation.
I0731 23:20:44.827754  7360 net.cpp:323] out5a_up2 needs backward computation.
I0731 23:20:44.827756  7360 net.cpp:323] out5a/relu needs backward computation.
I0731 23:20:44.827759  7360 net.cpp:323] out5a/bn needs backward computation.
I0731 23:20:44.827762  7360 net.cpp:323] out5a needs backward computation.
I0731 23:20:44.827764  7360 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0731 23:20:44.827766  7360 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0731 23:20:44.827769  7360 net.cpp:323] res5a_branch2b needs backward computation.
I0731 23:20:44.827771  7360 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0731 23:20:44.827774  7360 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0731 23:20:44.827775  7360 net.cpp:323] res5a_branch2a needs backward computation.
I0731 23:20:44.827777  7360 net.cpp:323] pool4 needs backward computation.
I0731 23:20:44.827780  7360 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0731 23:20:44.827782  7360 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0731 23:20:44.827788  7360 net.cpp:323] res4a_branch2b needs backward computation.
I0731 23:20:44.827790  7360 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0731 23:20:44.827793  7360 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0731 23:20:44.827795  7360 net.cpp:323] res4a_branch2a needs backward computation.
I0731 23:20:44.827797  7360 net.cpp:323] pool3 needs backward computation.
I0731 23:20:44.827800  7360 net.cpp:323] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I0731 23:20:44.827802  7360 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0731 23:20:44.827805  7360 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0731 23:20:44.827807  7360 net.cpp:323] res3a_branch2b needs backward computation.
I0731 23:20:44.827810  7360 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0731 23:20:44.827811  7360 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0731 23:20:44.827814  7360 net.cpp:323] res3a_branch2a needs backward computation.
I0731 23:20:44.827816  7360 net.cpp:323] pool2 needs backward computation.
I0731 23:20:44.827818  7360 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0731 23:20:44.827821  7360 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0731 23:20:44.827823  7360 net.cpp:323] res2a_branch2b needs backward computation.
I0731 23:20:44.827826  7360 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0731 23:20:44.827827  7360 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0731 23:20:44.827831  7360 net.cpp:323] res2a_branch2a needs backward computation.
I0731 23:20:44.827832  7360 net.cpp:323] pool1 needs backward computation.
I0731 23:20:44.827836  7360 net.cpp:323] conv1b/relu needs backward computation.
I0731 23:20:44.827837  7360 net.cpp:323] conv1b/bn needs backward computation.
I0731 23:20:44.827839  7360 net.cpp:323] conv1b needs backward computation.
I0731 23:20:44.827842  7360 net.cpp:323] conv1a/relu needs backward computation.
I0731 23:20:44.827844  7360 net.cpp:323] conv1a/bn needs backward computation.
I0731 23:20:44.827847  7360 net.cpp:323] conv1a needs backward computation.
I0731 23:20:44.827849  7360 net.cpp:325] data/bias does not need backward computation.
I0731 23:20:44.827852  7360 net.cpp:325] label_data_1_split does not need backward computation.
I0731 23:20:44.827855  7360 net.cpp:325] data does not need backward computation.
I0731 23:20:44.827857  7360 net.cpp:367] This network produces output accuracy/top1
I0731 23:20:44.827859  7360 net.cpp:367] This network produces output accuracy/top5
I0731 23:20:44.827862  7360 net.cpp:367] This network produces output loss
I0731 23:20:44.827908  7360 net.cpp:389] Top memory (TEST) required for data: 637337600 diff: 8
I0731 23:20:44.827910  7360 net.cpp:392] Bottom memory (TEST) required for data: 637337600 diff: 637337600
I0731 23:20:44.827913  7360 net.cpp:395] Shared (in-place) memory (TEST) by data: 420249600 diff: 420249600
I0731 23:20:44.827915  7360 net.cpp:398] Parameters memory (TEST) required for data: 2692608 diff: 2692608
I0731 23:20:44.827917  7360 net.cpp:401] Parameters shared memory (TEST) by data: 0 diff: 0
I0731 23:20:44.827919  7360 net.cpp:407] Network initialization done.
I0731 23:20:44.833978  7360 net.cpp:1089] Copying source layer data Type:ImageLabelData #blobs=0
I0731 23:20:44.833999  7360 net.cpp:1089] Copying source layer data/bias Type:Bias #blobs=1
I0731 23:20:44.834043  7360 net.cpp:1089] Copying source layer conv1a Type:Convolution #blobs=2
I0731 23:20:44.834061  7360 net.cpp:1089] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0731 23:20:44.834364  7360 net.cpp:1089] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0731 23:20:44.834373  7360 net.cpp:1089] Copying source layer conv1b Type:Convolution #blobs=2
I0731 23:20:44.834384  7360 net.cpp:1089] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0731 23:20:44.834594  7360 net.cpp:1089] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0731 23:20:44.834601  7360 net.cpp:1089] Copying source layer pool1 Type:Pooling #blobs=0
I0731 23:20:44.834615  7360 net.cpp:1089] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0731 23:20:44.834635  7360 net.cpp:1089] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0731 23:20:44.834854  7360 net.cpp:1089] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0731 23:20:44.834862  7360 net.cpp:1089] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0731 23:20:44.834878  7360 net.cpp:1089] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0731 23:20:44.835103  7360 net.cpp:1089] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0731 23:20:44.835113  7360 net.cpp:1089] Copying source layer pool2 Type:Pooling #blobs=0
I0731 23:20:44.835119  7360 net.cpp:1089] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0731 23:20:44.835198  7360 net.cpp:1089] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0731 23:20:44.835448  7360 net.cpp:1089] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0731 23:20:44.835458  7360 net.cpp:1089] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0731 23:20:44.835486  7360 net.cpp:1089] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0731 23:20:44.835711  7360 net.cpp:1089] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0731 23:20:44.835719  7360 net.cpp:1089] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I0731 23:20:44.835723  7360 net.cpp:1089] Copying source layer pool3 Type:Pooling #blobs=0
I0731 23:20:44.835726  7360 net.cpp:1089] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0731 23:20:44.835847  7360 net.cpp:1089] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0731 23:20:44.836045  7360 net.cpp:1089] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0731 23:20:44.836051  7360 net.cpp:1089] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0731 23:20:44.836122  7360 net.cpp:1089] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0731 23:20:44.836318  7360 net.cpp:1089] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0731 23:20:44.836323  7360 net.cpp:1089] Copying source layer pool4 Type:Pooling #blobs=0
I0731 23:20:44.836326  7360 net.cpp:1089] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0731 23:20:44.836720  7360 net.cpp:1089] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0731 23:20:44.836918  7360 net.cpp:1089] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0731 23:20:44.836925  7360 net.cpp:1089] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0731 23:20:44.837131  7360 net.cpp:1089] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0731 23:20:44.837313  7360 net.cpp:1089] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0731 23:20:44.837318  7360 net.cpp:1089] Copying source layer out5a Type:Convolution #blobs=2
I0731 23:20:44.837376  7360 net.cpp:1089] Copying source layer out5a/bn Type:BatchNorm #blobs=5
I0731 23:20:44.837502  7360 net.cpp:1089] Copying source layer out5a/relu Type:ReLU #blobs=0
I0731 23:20:44.837507  7360 net.cpp:1089] Copying source layer out5a_up2 Type:Deconvolution #blobs=1
I0731 23:20:44.837512  7360 net.cpp:1089] Copying source layer out3a Type:Convolution #blobs=2
I0731 23:20:44.837533  7360 net.cpp:1089] Copying source layer out3a/bn Type:BatchNorm #blobs=5
I0731 23:20:44.837641  7360 net.cpp:1089] Copying source layer out3a/relu Type:ReLU #blobs=0
I0731 23:20:44.837646  7360 net.cpp:1089] Copying source layer out3_out5_combined Type:Eltwise #blobs=0
I0731 23:20:44.837651  7360 net.cpp:1089] Copying source layer ctx_conv1 Type:Convolution #blobs=2
I0731 23:20:44.837674  7360 net.cpp:1089] Copying source layer ctx_conv1/bn Type:BatchNorm #blobs=5
I0731 23:20:44.837793  7360 net.cpp:1089] Copying source layer ctx_conv1/relu Type:ReLU #blobs=0
I0731 23:20:44.837800  7360 net.cpp:1089] Copying source layer ctx_conv2 Type:Convolution #blobs=2
I0731 23:20:44.837838  7360 net.cpp:1089] Copying source layer ctx_conv2/bn Type:BatchNorm #blobs=5
I0731 23:20:44.837954  7360 net.cpp:1089] Copying source layer ctx_conv2/relu Type:ReLU #blobs=0
I0731 23:20:44.837959  7360 net.cpp:1089] Copying source layer ctx_conv3 Type:Convolution #blobs=2
I0731 23:20:44.837982  7360 net.cpp:1089] Copying source layer ctx_conv3/bn Type:BatchNorm #blobs=5
I0731 23:20:44.838093  7360 net.cpp:1089] Copying source layer ctx_conv3/relu Type:ReLU #blobs=0
I0731 23:20:44.838099  7360 net.cpp:1089] Copying source layer ctx_conv4 Type:Convolution #blobs=2
I0731 23:20:44.838124  7360 net.cpp:1089] Copying source layer ctx_conv4/bn Type:BatchNorm #blobs=5
I0731 23:20:44.838237  7360 net.cpp:1089] Copying source layer ctx_conv4/relu Type:ReLU #blobs=0
I0731 23:20:44.838243  7360 net.cpp:1089] Copying source layer ctx_final Type:Convolution #blobs=2
I0731 23:20:44.838254  7360 net.cpp:1089] Copying source layer ctx_final/relu Type:ReLU #blobs=0
I0731 23:20:44.838259  7360 net.cpp:1089] Copying source layer out_deconv_final_up2 Type:Deconvolution #blobs=1
I0731 23:20:44.838268  7360 net.cpp:1089] Copying source layer out_deconv_final_up4 Type:Deconvolution #blobs=1
I0731 23:20:44.838276  7360 net.cpp:1089] Copying source layer out_deconv_final_up8 Type:Deconvolution #blobs=1
I0731 23:20:44.838285  7360 net.cpp:1089] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0731 23:20:44.838392  7360 caffe.cpp:290] Running for 50 iterations.
I0731 23:20:44.844139  7360 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.72G, req 0G)
I0731 23:20:44.866653  7360 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.62G, req 0G)
I0731 23:20:44.883682  7360 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.5G, req 0G)
I0731 23:20:44.893678  7360 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.45G, req 0G)
I0731 23:20:44.901648  7360 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.38G, req 0G)
I0731 23:20:44.907209  7360 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.35G, req 0G)
I0731 23:20:44.914204  7360 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.33G, req 0G)
I0731 23:20:44.918042  7360 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.32G, req 0G)
I0731 23:20:44.941808  7360 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 7.09G, req 0G)
I0731 23:20:44.947019  7360 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 7.07G, req 0G)
I0731 23:20:44.961433  7360 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.94G, req 0G)
I0731 23:20:45.113870  7360 caffe.cpp:313] Batch 0, accuracy/top1 = 0.932327
I0731 23:20:45.113889  7360 caffe.cpp:313] Batch 0, accuracy/top5 = 1
I0731 23:20:45.113893  7360 caffe.cpp:313] Batch 0, loss = 0.20303
I0731 23:20:45.120537  7360 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1a' with space 1.22G/1 1  (limit 5.49G, req 0G)
I0731 23:20:45.144253  7360 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1b' with space 2.44G/2 6  (limit 4.27G, req 0G)
I0731 23:20:45.189424  7360 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2a' with space 2.44G/1 6  (limit 4.27G, req 0G)
I0731 23:20:45.205667  7360 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2b' with space 2.44G/2 6  (limit 4.27G, req 0G)
I0731 23:20:45.240761  7360 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2a' with space 2.44G/1 6  (limit 4.27G, req 0G)
I0731 23:20:45.251348  7360 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2b' with space 2.44G/2 1  (limit 4.27G, req 0G)
I0731 23:20:45.277031  7360 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2a' with space 2.44G/1 6  (limit 4.27G, req 0G)
I0731 23:20:45.287036  7360 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2b' with space 2.44G/2 6  (limit 4.27G, req 0G)
I0731 23:20:45.314224  7360 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'out3a' with space 2.44G/2 6  (limit 4.27G, req 0G)
I0731 23:20:45.334336  7360 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'ctx_conv1' with space 2.44G/1 6  (limit 4.27G, req 0G)
I0731 23:20:45.347347  7360 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'ctx_final' with space 2.44G/1 6  (limit 4.27G, req 0G)
I0731 23:20:45.490356  7360 caffe.cpp:313] Batch 1, accuracy/top1 = 0.953817
I0731 23:20:45.490378  7360 caffe.cpp:313] Batch 1, accuracy/top5 = 1
I0731 23:20:45.490382  7360 caffe.cpp:313] Batch 1, loss = 0.147382
I0731 23:20:45.652664  7360 caffe.cpp:313] Batch 2, accuracy/top1 = 0.960969
I0731 23:20:45.652688  7360 caffe.cpp:313] Batch 2, accuracy/top5 = 1
I0731 23:20:45.652691  7360 caffe.cpp:313] Batch 2, loss = 0.107341
I0731 23:20:45.816349  7360 caffe.cpp:313] Batch 3, accuracy/top1 = 0.973338
I0731 23:20:45.816375  7360 caffe.cpp:313] Batch 3, accuracy/top5 = 0.999996
I0731 23:20:45.816377  7360 caffe.cpp:313] Batch 3, loss = 0.0749496
I0731 23:20:45.977665  7360 caffe.cpp:313] Batch 4, accuracy/top1 = 0.963353
I0731 23:20:45.977689  7360 caffe.cpp:313] Batch 4, accuracy/top5 = 1
I0731 23:20:45.977691  7360 caffe.cpp:313] Batch 4, loss = 0.109943
I0731 23:20:46.141820  7360 caffe.cpp:313] Batch 5, accuracy/top1 = 0.810928
I0731 23:20:46.141844  7360 caffe.cpp:313] Batch 5, accuracy/top5 = 1
I0731 23:20:46.141849  7360 caffe.cpp:313] Batch 5, loss = 1.18355
I0731 23:20:46.305583  7360 caffe.cpp:313] Batch 6, accuracy/top1 = 0.961721
I0731 23:20:46.305608  7360 caffe.cpp:313] Batch 6, accuracy/top5 = 1
I0731 23:20:46.305610  7360 caffe.cpp:313] Batch 6, loss = 0.100892
I0731 23:20:46.469992  7360 caffe.cpp:313] Batch 7, accuracy/top1 = 0.962388
I0731 23:20:46.470015  7360 caffe.cpp:313] Batch 7, accuracy/top5 = 1
I0731 23:20:46.470017  7360 caffe.cpp:313] Batch 7, loss = 0.098759
I0731 23:20:46.631201  7360 caffe.cpp:313] Batch 8, accuracy/top1 = 0.974959
I0731 23:20:46.631225  7360 caffe.cpp:313] Batch 8, accuracy/top5 = 1
I0731 23:20:46.631229  7360 caffe.cpp:313] Batch 8, loss = 0.0658388
I0731 23:20:46.795917  7360 caffe.cpp:313] Batch 9, accuracy/top1 = 0.982891
I0731 23:20:46.795940  7360 caffe.cpp:313] Batch 9, accuracy/top5 = 1
I0731 23:20:46.795943  7360 caffe.cpp:313] Batch 9, loss = 0.0473068
I0731 23:20:46.960966  7360 caffe.cpp:313] Batch 10, accuracy/top1 = 0.933828
I0731 23:20:46.960988  7360 caffe.cpp:313] Batch 10, accuracy/top5 = 1
I0731 23:20:46.960991  7360 caffe.cpp:313] Batch 10, loss = 0.167292
I0731 23:20:47.125730  7360 caffe.cpp:313] Batch 11, accuracy/top1 = 0.976976
I0731 23:20:47.125751  7360 caffe.cpp:313] Batch 11, accuracy/top5 = 1
I0731 23:20:47.125753  7360 caffe.cpp:313] Batch 11, loss = 0.0638392
I0731 23:20:47.289132  7360 caffe.cpp:313] Batch 12, accuracy/top1 = 0.965253
I0731 23:20:47.289155  7360 caffe.cpp:313] Batch 12, accuracy/top5 = 1
I0731 23:20:47.289158  7360 caffe.cpp:313] Batch 12, loss = 0.0949737
I0731 23:20:47.452775  7360 caffe.cpp:313] Batch 13, accuracy/top1 = 0.978989
I0731 23:20:47.452798  7360 caffe.cpp:313] Batch 13, accuracy/top5 = 1
I0731 23:20:47.452801  7360 caffe.cpp:313] Batch 13, loss = 0.0566592
I0731 23:20:47.619086  7360 caffe.cpp:313] Batch 14, accuracy/top1 = 0.98566
I0731 23:20:47.619110  7360 caffe.cpp:313] Batch 14, accuracy/top5 = 1
I0731 23:20:47.619113  7360 caffe.cpp:313] Batch 14, loss = 0.0407282
I0731 23:20:47.781807  7360 caffe.cpp:313] Batch 15, accuracy/top1 = 0.96152
I0731 23:20:47.781831  7360 caffe.cpp:313] Batch 15, accuracy/top5 = 1
I0731 23:20:47.781834  7360 caffe.cpp:313] Batch 15, loss = 0.102921
I0731 23:20:47.945437  7360 caffe.cpp:313] Batch 16, accuracy/top1 = 0.890217
I0731 23:20:47.945458  7360 caffe.cpp:313] Batch 16, accuracy/top5 = 1
I0731 23:20:47.945462  7360 caffe.cpp:313] Batch 16, loss = 0.510901
I0731 23:20:48.107318  7360 caffe.cpp:313] Batch 17, accuracy/top1 = 0.86498
I0731 23:20:48.107342  7360 caffe.cpp:313] Batch 17, accuracy/top5 = 1
I0731 23:20:48.107360  7360 caffe.cpp:313] Batch 17, loss = 0.738334
I0731 23:20:48.271764  7360 caffe.cpp:313] Batch 18, accuracy/top1 = 0.983278
I0731 23:20:48.271786  7360 caffe.cpp:313] Batch 18, accuracy/top5 = 1
I0731 23:20:48.271790  7360 caffe.cpp:313] Batch 18, loss = 0.0435361
I0731 23:20:48.434854  7360 caffe.cpp:313] Batch 19, accuracy/top1 = 0.982001
I0731 23:20:48.434877  7360 caffe.cpp:313] Batch 19, accuracy/top5 = 1
I0731 23:20:48.434881  7360 caffe.cpp:313] Batch 19, loss = 0.051371
I0731 23:20:48.597856  7360 caffe.cpp:313] Batch 20, accuracy/top1 = 0.975053
I0731 23:20:48.597879  7360 caffe.cpp:313] Batch 20, accuracy/top5 = 1
I0731 23:20:48.597883  7360 caffe.cpp:313] Batch 20, loss = 0.0703251
I0731 23:20:48.759050  7360 caffe.cpp:313] Batch 21, accuracy/top1 = 0.890217
I0731 23:20:48.759073  7360 caffe.cpp:313] Batch 21, accuracy/top5 = 0.996345
I0731 23:20:48.759076  7360 caffe.cpp:313] Batch 21, loss = 0.656284
I0731 23:20:48.923267  7360 caffe.cpp:313] Batch 22, accuracy/top1 = 0.967746
I0731 23:20:48.923286  7360 caffe.cpp:313] Batch 22, accuracy/top5 = 1
I0731 23:20:48.923290  7360 caffe.cpp:313] Batch 22, loss = 0.0859041
I0731 23:20:49.085857  7360 caffe.cpp:313] Batch 23, accuracy/top1 = 0.978284
I0731 23:20:49.085880  7360 caffe.cpp:313] Batch 23, accuracy/top5 = 1
I0731 23:20:49.085883  7360 caffe.cpp:313] Batch 23, loss = 0.0582804
I0731 23:20:49.252012  7360 caffe.cpp:313] Batch 24, accuracy/top1 = 0.951931
I0731 23:20:49.252033  7360 caffe.cpp:313] Batch 24, accuracy/top5 = 1
I0731 23:20:49.252038  7360 caffe.cpp:313] Batch 24, loss = 0.124054
I0731 23:20:49.417794  7360 caffe.cpp:313] Batch 25, accuracy/top1 = 0.973421
I0731 23:20:49.417816  7360 caffe.cpp:313] Batch 25, accuracy/top5 = 1
I0731 23:20:49.417820  7360 caffe.cpp:313] Batch 25, loss = 0.07303
I0731 23:20:49.578279  7360 caffe.cpp:313] Batch 26, accuracy/top1 = 0.950378
I0731 23:20:49.578301  7360 caffe.cpp:313] Batch 26, accuracy/top5 = 1
I0731 23:20:49.578305  7360 caffe.cpp:313] Batch 26, loss = 0.125152
I0731 23:20:49.741639  7360 caffe.cpp:313] Batch 27, accuracy/top1 = 0.966843
I0731 23:20:49.741662  7360 caffe.cpp:313] Batch 27, accuracy/top5 = 1
I0731 23:20:49.741664  7360 caffe.cpp:313] Batch 27, loss = 0.0959835
I0731 23:20:49.904541  7360 caffe.cpp:313] Batch 28, accuracy/top1 = 0.952878
I0731 23:20:49.904561  7360 caffe.cpp:313] Batch 28, accuracy/top5 = 1
I0731 23:20:49.904564  7360 caffe.cpp:313] Batch 28, loss = 0.126782
I0731 23:20:50.069597  7360 caffe.cpp:313] Batch 29, accuracy/top1 = 0.965811
I0731 23:20:50.069619  7360 caffe.cpp:313] Batch 29, accuracy/top5 = 1
I0731 23:20:50.069622  7360 caffe.cpp:313] Batch 29, loss = 0.104058
I0731 23:20:50.231479  7360 caffe.cpp:313] Batch 30, accuracy/top1 = 0.846383
I0731 23:20:50.231504  7360 caffe.cpp:313] Batch 30, accuracy/top5 = 1
I0731 23:20:50.231508  7360 caffe.cpp:313] Batch 30, loss = 0.76434
I0731 23:20:50.394577  7360 caffe.cpp:313] Batch 31, accuracy/top1 = 0.967345
I0731 23:20:50.394599  7360 caffe.cpp:313] Batch 31, accuracy/top5 = 1
I0731 23:20:50.394603  7360 caffe.cpp:313] Batch 31, loss = 0.0923419
I0731 23:20:50.557618  7360 caffe.cpp:313] Batch 32, accuracy/top1 = 0.957156
I0731 23:20:50.557641  7360 caffe.cpp:313] Batch 32, accuracy/top5 = 1
I0731 23:20:50.557643  7360 caffe.cpp:313] Batch 32, loss = 0.121207
I0731 23:20:50.721660  7360 caffe.cpp:313] Batch 33, accuracy/top1 = 0.966964
I0731 23:20:50.721683  7360 caffe.cpp:313] Batch 33, accuracy/top5 = 1
I0731 23:20:50.721685  7360 caffe.cpp:313] Batch 33, loss = 0.0866359
I0731 23:20:50.887557  7360 caffe.cpp:313] Batch 34, accuracy/top1 = 0.977094
I0731 23:20:50.887576  7360 caffe.cpp:313] Batch 34, accuracy/top5 = 1
I0731 23:20:50.887580  7360 caffe.cpp:313] Batch 34, loss = 0.0650478
I0731 23:20:51.053338  7360 caffe.cpp:313] Batch 35, accuracy/top1 = 0.977346
I0731 23:20:51.053360  7360 caffe.cpp:313] Batch 35, accuracy/top5 = 1
I0731 23:20:51.053364  7360 caffe.cpp:313] Batch 35, loss = 0.0616944
I0731 23:20:51.214915  7360 caffe.cpp:313] Batch 36, accuracy/top1 = 0.96439
I0731 23:20:51.214946  7360 caffe.cpp:313] Batch 36, accuracy/top5 = 1
I0731 23:20:51.214949  7360 caffe.cpp:313] Batch 36, loss = 0.100727
I0731 23:20:51.375638  7360 caffe.cpp:313] Batch 37, accuracy/top1 = 0.962577
I0731 23:20:51.375658  7360 caffe.cpp:313] Batch 37, accuracy/top5 = 1
I0731 23:20:51.375660  7360 caffe.cpp:313] Batch 37, loss = 0.1073
I0731 23:20:51.538584  7360 caffe.cpp:313] Batch 38, accuracy/top1 = 0.94022
I0731 23:20:51.538606  7360 caffe.cpp:313] Batch 38, accuracy/top5 = 1
I0731 23:20:51.538609  7360 caffe.cpp:313] Batch 38, loss = 0.188994
I0731 23:20:51.697533  7360 caffe.cpp:313] Batch 39, accuracy/top1 = 0.919991
I0731 23:20:51.697556  7360 caffe.cpp:313] Batch 39, accuracy/top5 = 1
I0731 23:20:51.697558  7360 caffe.cpp:313] Batch 39, loss = 0.212029
I0731 23:20:51.861172  7360 caffe.cpp:313] Batch 40, accuracy/top1 = 0.980825
I0731 23:20:51.861191  7360 caffe.cpp:313] Batch 40, accuracy/top5 = 1
I0731 23:20:51.861196  7360 caffe.cpp:313] Batch 40, loss = 0.0583811
I0731 23:20:52.026008  7360 caffe.cpp:313] Batch 41, accuracy/top1 = 0.976935
I0731 23:20:52.026031  7360 caffe.cpp:313] Batch 41, accuracy/top5 = 1
I0731 23:20:52.026034  7360 caffe.cpp:313] Batch 41, loss = 0.0665227
I0731 23:20:52.190517  7360 caffe.cpp:313] Batch 42, accuracy/top1 = 0.975914
I0731 23:20:52.190541  7360 caffe.cpp:313] Batch 42, accuracy/top5 = 1
I0731 23:20:52.190543  7360 caffe.cpp:313] Batch 42, loss = 0.0661847
I0731 23:20:52.354277  7360 caffe.cpp:313] Batch 43, accuracy/top1 = 0.976473
I0731 23:20:52.354300  7360 caffe.cpp:313] Batch 43, accuracy/top5 = 1
I0731 23:20:52.354303  7360 caffe.cpp:313] Batch 43, loss = 0.0675297
I0731 23:20:52.518242  7360 caffe.cpp:313] Batch 44, accuracy/top1 = 0.959264
I0731 23:20:52.518265  7360 caffe.cpp:313] Batch 44, accuracy/top5 = 1
I0731 23:20:52.518268  7360 caffe.cpp:313] Batch 44, loss = 0.116423
I0731 23:20:52.680609  7360 caffe.cpp:313] Batch 45, accuracy/top1 = 0.976664
I0731 23:20:52.680632  7360 caffe.cpp:313] Batch 45, accuracy/top5 = 1
I0731 23:20:52.680635  7360 caffe.cpp:313] Batch 45, loss = 0.0723218
I0731 23:20:52.845873  7360 caffe.cpp:313] Batch 46, accuracy/top1 = 0.971935
I0731 23:20:52.845892  7360 caffe.cpp:313] Batch 46, accuracy/top5 = 1
I0731 23:20:52.845896  7360 caffe.cpp:313] Batch 46, loss = 0.0762741
I0731 23:20:53.011525  7360 caffe.cpp:313] Batch 47, accuracy/top1 = 0.967182
I0731 23:20:53.011548  7360 caffe.cpp:313] Batch 47, accuracy/top5 = 1
I0731 23:20:53.011551  7360 caffe.cpp:313] Batch 47, loss = 0.122423
I0731 23:20:53.172299  7360 caffe.cpp:313] Batch 48, accuracy/top1 = 0.872947
I0731 23:20:53.172323  7360 caffe.cpp:313] Batch 48, accuracy/top5 = 1
I0731 23:20:53.172327  7360 caffe.cpp:313] Batch 48, loss = 0.569015
I0731 23:20:53.331670  7360 caffe.cpp:313] Batch 49, accuracy/top1 = 0.949764
I0731 23:20:53.331691  7360 caffe.cpp:313] Batch 49, accuracy/top5 = 1
I0731 23:20:53.331694  7360 caffe.cpp:313] Batch 49, loss = 0.134507
I0731 23:20:53.331697  7360 caffe.cpp:318] Loss: 0.173586
I0731 23:20:53.331703  7360 caffe.cpp:330] accuracy/top1 = 0.953186
I0731 23:20:53.331707  7360 caffe.cpp:330] accuracy/top5 = 0.999927
I0731 23:20:53.331712  7360 caffe.cpp:330] loss = 0.173586 (* 1 = 0.173586 loss)
