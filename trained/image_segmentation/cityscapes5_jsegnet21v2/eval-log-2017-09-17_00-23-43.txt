Logging output to training/eval-log-2017-09-17_00-23-43.txt
I0917 00:23:44.367601 17626 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0917 00:23:44.368304 17626 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I0917 00:23:44.368903 17626 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0917 00:23:44.369474 17626 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I0917 00:23:44.371137 17626 upgrade_proto.cpp:66] Attempting to upgrade input file specified using deprecated input fields: ../trained/image_segmentation/cityscapes5_jsegnet21v2/initial/deploy.prototxt
I0917 00:23:44.371155 17626 upgrade_proto.cpp:69] Successfully upgraded file specified using deprecated input fields.
W0917 00:23:44.371160 17626 upgrade_proto.cpp:71] Note that future Caffe releases will only support input layers and not input fields.
I0917 00:23:44.371434 17626 net.cpp:72] Initializing net from parameters: 
name: "jsegnet21v2_deploy"
state {
  phase: TEST
}
layer {
  name: "input"
  type: "Input"
  top: "data"
  input_param {
    shape {
      dim: 1
      dim: 3
      dim: 1024
      dim: 2048
    }
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "argMaxOut"
  type: "ArgMax"
  bottom: "out_deconv_final_up8"
  top: "argMaxOut"
  argmax_param {
    axis: 1
  }
}
I0917 00:23:44.371582 17626 net.cpp:104] Using FLOAT as default forward math type
I0917 00:23:44.371587 17626 net.cpp:110] Using FLOAT as default backward math type
I0917 00:23:44.371590 17626 layer_factory.hpp:136] Creating layer 'input' of type 'Input'
I0917 00:23:44.371595 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.371603 17626 net.cpp:184] Created Layer input (0)
I0917 00:23:44.371608 17626 net.cpp:530] input -> data
I0917 00:23:44.372313 17626 net.cpp:245] Setting up input
I0917 00:23:44.372323 17626 net.cpp:252] TEST Top shape for layer 0 'input' 1 3 1024 2048 (6291456)
I0917 00:23:44.372328 17626 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0917 00:23:44.372339 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.372351 17626 net.cpp:184] Created Layer data/bias (1)
I0917 00:23:44.372355 17626 net.cpp:561] data/bias <- data
I0917 00:23:44.372359 17626 net.cpp:530] data/bias -> data/bias
I0917 00:23:44.376664 17626 net.cpp:245] Setting up data/bias
I0917 00:23:44.376684 17626 net.cpp:252] TEST Top shape for layer 1 'data/bias' 1 3 1024 2048 (6291456)
I0917 00:23:44.376694 17626 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0917 00:23:44.376703 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.376724 17626 net.cpp:184] Created Layer conv1a (2)
I0917 00:23:44.376729 17626 net.cpp:561] conv1a <- data/bias
I0917 00:23:44.376734 17626 net.cpp:530] conv1a -> conv1a
I0917 00:23:44.668006 17626 net.cpp:245] Setting up conv1a
I0917 00:23:44.668031 17626 net.cpp:252] TEST Top shape for layer 2 'conv1a' 1 32 512 1024 (16777216)
I0917 00:23:44.668045 17626 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0917 00:23:44.668051 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.668061 17626 net.cpp:184] Created Layer conv1a/bn (3)
I0917 00:23:44.668066 17626 net.cpp:561] conv1a/bn <- conv1a
I0917 00:23:44.668071 17626 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0917 00:23:44.668905 17626 net.cpp:245] Setting up conv1a/bn
I0917 00:23:44.668915 17626 net.cpp:252] TEST Top shape for layer 3 'conv1a/bn' 1 32 512 1024 (16777216)
I0917 00:23:44.668926 17626 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0917 00:23:44.668931 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.668936 17626 net.cpp:184] Created Layer conv1a/relu (4)
I0917 00:23:44.668941 17626 net.cpp:561] conv1a/relu <- conv1a
I0917 00:23:44.668944 17626 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0917 00:23:44.668957 17626 net.cpp:245] Setting up conv1a/relu
I0917 00:23:44.668962 17626 net.cpp:252] TEST Top shape for layer 4 'conv1a/relu' 1 32 512 1024 (16777216)
I0917 00:23:44.668965 17626 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0917 00:23:44.668969 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.668987 17626 net.cpp:184] Created Layer conv1b (5)
I0917 00:23:44.668990 17626 net.cpp:561] conv1b <- conv1a
I0917 00:23:44.668993 17626 net.cpp:530] conv1b -> conv1b
I0917 00:23:44.670506 17626 net.cpp:245] Setting up conv1b
I0917 00:23:44.670516 17626 net.cpp:252] TEST Top shape for layer 5 'conv1b' 1 32 512 1024 (16777216)
I0917 00:23:44.670523 17626 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0917 00:23:44.670528 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.670534 17626 net.cpp:184] Created Layer conv1b/bn (6)
I0917 00:23:44.670539 17626 net.cpp:561] conv1b/bn <- conv1b
I0917 00:23:44.670543 17626 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0917 00:23:44.671339 17626 net.cpp:245] Setting up conv1b/bn
I0917 00:23:44.671347 17626 net.cpp:252] TEST Top shape for layer 6 'conv1b/bn' 1 32 512 1024 (16777216)
I0917 00:23:44.671356 17626 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0917 00:23:44.671361 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.671366 17626 net.cpp:184] Created Layer conv1b/relu (7)
I0917 00:23:44.671370 17626 net.cpp:561] conv1b/relu <- conv1b
I0917 00:23:44.671373 17626 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0917 00:23:44.671380 17626 net.cpp:245] Setting up conv1b/relu
I0917 00:23:44.671385 17626 net.cpp:252] TEST Top shape for layer 7 'conv1b/relu' 1 32 512 1024 (16777216)
I0917 00:23:44.671389 17626 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0917 00:23:44.671402 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.671411 17626 net.cpp:184] Created Layer pool1 (8)
I0917 00:23:44.671414 17626 net.cpp:561] pool1 <- conv1b
I0917 00:23:44.671417 17626 net.cpp:530] pool1 -> pool1
I0917 00:23:44.671458 17626 net.cpp:245] Setting up pool1
I0917 00:23:44.671464 17626 net.cpp:252] TEST Top shape for layer 8 'pool1' 1 32 256 512 (4194304)
I0917 00:23:44.671468 17626 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0917 00:23:44.671473 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.671485 17626 net.cpp:184] Created Layer res2a_branch2a (9)
I0917 00:23:44.671489 17626 net.cpp:561] res2a_branch2a <- pool1
I0917 00:23:44.671492 17626 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0917 00:23:44.672607 17626 net.cpp:245] Setting up res2a_branch2a
I0917 00:23:44.672617 17626 net.cpp:252] TEST Top shape for layer 9 'res2a_branch2a' 1 64 256 512 (8388608)
I0917 00:23:44.672626 17626 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0917 00:23:44.672631 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.672637 17626 net.cpp:184] Created Layer res2a_branch2a/bn (10)
I0917 00:23:44.672641 17626 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0917 00:23:44.672646 17626 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0917 00:23:44.673065 17626 net.cpp:245] Setting up res2a_branch2a/bn
I0917 00:23:44.673074 17626 net.cpp:252] TEST Top shape for layer 10 'res2a_branch2a/bn' 1 64 256 512 (8388608)
I0917 00:23:44.673081 17626 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0917 00:23:44.673085 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.673090 17626 net.cpp:184] Created Layer res2a_branch2a/relu (11)
I0917 00:23:44.673094 17626 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0917 00:23:44.673099 17626 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0917 00:23:44.673104 17626 net.cpp:245] Setting up res2a_branch2a/relu
I0917 00:23:44.673108 17626 net.cpp:252] TEST Top shape for layer 11 'res2a_branch2a/relu' 1 64 256 512 (8388608)
I0917 00:23:44.673113 17626 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0917 00:23:44.673117 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.673126 17626 net.cpp:184] Created Layer res2a_branch2b (12)
I0917 00:23:44.673130 17626 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0917 00:23:44.673135 17626 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0917 00:23:44.674046 17626 net.cpp:245] Setting up res2a_branch2b
I0917 00:23:44.674057 17626 net.cpp:252] TEST Top shape for layer 12 'res2a_branch2b' 1 64 256 512 (8388608)
I0917 00:23:44.674062 17626 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0917 00:23:44.674067 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.674077 17626 net.cpp:184] Created Layer res2a_branch2b/bn (13)
I0917 00:23:44.674082 17626 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0917 00:23:44.674085 17626 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0917 00:23:44.674877 17626 net.cpp:245] Setting up res2a_branch2b/bn
I0917 00:23:44.674886 17626 net.cpp:252] TEST Top shape for layer 13 'res2a_branch2b/bn' 1 64 256 512 (8388608)
I0917 00:23:44.674896 17626 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0917 00:23:44.674901 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.674906 17626 net.cpp:184] Created Layer res2a_branch2b/relu (14)
I0917 00:23:44.674909 17626 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0917 00:23:44.674913 17626 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0917 00:23:44.674926 17626 net.cpp:245] Setting up res2a_branch2b/relu
I0917 00:23:44.674932 17626 net.cpp:252] TEST Top shape for layer 14 'res2a_branch2b/relu' 1 64 256 512 (8388608)
I0917 00:23:44.674937 17626 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0917 00:23:44.674940 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.674947 17626 net.cpp:184] Created Layer pool2 (15)
I0917 00:23:44.674952 17626 net.cpp:561] pool2 <- res2a_branch2b
I0917 00:23:44.674954 17626 net.cpp:530] pool2 -> pool2
I0917 00:23:44.674988 17626 net.cpp:245] Setting up pool2
I0917 00:23:44.674994 17626 net.cpp:252] TEST Top shape for layer 15 'pool2' 1 64 128 256 (2097152)
I0917 00:23:44.674998 17626 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0917 00:23:44.675002 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.675011 17626 net.cpp:184] Created Layer res3a_branch2a (16)
I0917 00:23:44.675015 17626 net.cpp:561] res3a_branch2a <- pool2
I0917 00:23:44.675019 17626 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0917 00:23:44.677539 17626 net.cpp:245] Setting up res3a_branch2a
I0917 00:23:44.677549 17626 net.cpp:252] TEST Top shape for layer 16 'res3a_branch2a' 1 128 128 256 (4194304)
I0917 00:23:44.677556 17626 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0917 00:23:44.677561 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.677568 17626 net.cpp:184] Created Layer res3a_branch2a/bn (17)
I0917 00:23:44.677572 17626 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0917 00:23:44.677577 17626 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0917 00:23:44.677994 17626 net.cpp:245] Setting up res3a_branch2a/bn
I0917 00:23:44.678002 17626 net.cpp:252] TEST Top shape for layer 17 'res3a_branch2a/bn' 1 128 128 256 (4194304)
I0917 00:23:44.678014 17626 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0917 00:23:44.678016 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.678021 17626 net.cpp:184] Created Layer res3a_branch2a/relu (18)
I0917 00:23:44.678025 17626 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0917 00:23:44.678030 17626 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0917 00:23:44.678035 17626 net.cpp:245] Setting up res3a_branch2a/relu
I0917 00:23:44.678040 17626 net.cpp:252] TEST Top shape for layer 18 'res3a_branch2a/relu' 1 128 128 256 (4194304)
I0917 00:23:44.678045 17626 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0917 00:23:44.678048 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.678056 17626 net.cpp:184] Created Layer res3a_branch2b (19)
I0917 00:23:44.678059 17626 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0917 00:23:44.678063 17626 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0917 00:23:44.678964 17626 net.cpp:245] Setting up res3a_branch2b
I0917 00:23:44.678973 17626 net.cpp:252] TEST Top shape for layer 19 'res3a_branch2b' 1 128 128 256 (4194304)
I0917 00:23:44.678979 17626 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0917 00:23:44.678983 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.679000 17626 net.cpp:184] Created Layer res3a_branch2b/bn (20)
I0917 00:23:44.679004 17626 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0917 00:23:44.679008 17626 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0917 00:23:44.679409 17626 net.cpp:245] Setting up res3a_branch2b/bn
I0917 00:23:44.679427 17626 net.cpp:252] TEST Top shape for layer 20 'res3a_branch2b/bn' 1 128 128 256 (4194304)
I0917 00:23:44.679437 17626 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0917 00:23:44.679441 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.679453 17626 net.cpp:184] Created Layer res3a_branch2b/relu (21)
I0917 00:23:44.679458 17626 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0917 00:23:44.679462 17626 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0917 00:23:44.679469 17626 net.cpp:245] Setting up res3a_branch2b/relu
I0917 00:23:44.679474 17626 net.cpp:252] TEST Top shape for layer 21 'res3a_branch2b/relu' 1 128 128 256 (4194304)
I0917 00:23:44.679478 17626 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0917 00:23:44.679483 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.679489 17626 net.cpp:184] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (22)
I0917 00:23:44.679493 17626 net.cpp:561] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0917 00:23:44.679497 17626 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0917 00:23:44.679502 17626 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0917 00:23:44.679531 17626 net.cpp:245] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0917 00:23:44.679538 17626 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 1 128 128 256 (4194304)
I0917 00:23:44.679541 17626 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 1 128 128 256 (4194304)
I0917 00:23:44.679546 17626 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0917 00:23:44.679549 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.679556 17626 net.cpp:184] Created Layer pool3 (23)
I0917 00:23:44.679560 17626 net.cpp:561] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0917 00:23:44.679564 17626 net.cpp:530] pool3 -> pool3
I0917 00:23:44.679597 17626 net.cpp:245] Setting up pool3
I0917 00:23:44.679602 17626 net.cpp:252] TEST Top shape for layer 23 'pool3' 1 128 64 128 (1048576)
I0917 00:23:44.679606 17626 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0917 00:23:44.679611 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.679620 17626 net.cpp:184] Created Layer res4a_branch2a (24)
I0917 00:23:44.679622 17626 net.cpp:561] res4a_branch2a <- pool3
I0917 00:23:44.679626 17626 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0917 00:23:44.685621 17626 net.cpp:245] Setting up res4a_branch2a
I0917 00:23:44.685631 17626 net.cpp:252] TEST Top shape for layer 24 'res4a_branch2a' 1 256 64 128 (2097152)
I0917 00:23:44.685637 17626 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0917 00:23:44.685642 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.685650 17626 net.cpp:184] Created Layer res4a_branch2a/bn (25)
I0917 00:23:44.685654 17626 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0917 00:23:44.685658 17626 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0917 00:23:44.686051 17626 net.cpp:245] Setting up res4a_branch2a/bn
I0917 00:23:44.686059 17626 net.cpp:252] TEST Top shape for layer 25 'res4a_branch2a/bn' 1 256 64 128 (2097152)
I0917 00:23:44.686067 17626 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0917 00:23:44.686071 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.686079 17626 net.cpp:184] Created Layer res4a_branch2a/relu (26)
I0917 00:23:44.686081 17626 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0917 00:23:44.686085 17626 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0917 00:23:44.686090 17626 net.cpp:245] Setting up res4a_branch2a/relu
I0917 00:23:44.686095 17626 net.cpp:252] TEST Top shape for layer 26 'res4a_branch2a/relu' 1 256 64 128 (2097152)
I0917 00:23:44.686098 17626 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0917 00:23:44.686108 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.686118 17626 net.cpp:184] Created Layer res4a_branch2b (27)
I0917 00:23:44.686121 17626 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0917 00:23:44.686125 17626 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0917 00:23:44.689196 17626 net.cpp:245] Setting up res4a_branch2b
I0917 00:23:44.689205 17626 net.cpp:252] TEST Top shape for layer 27 'res4a_branch2b' 1 256 64 128 (2097152)
I0917 00:23:44.689211 17626 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0917 00:23:44.689216 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.689223 17626 net.cpp:184] Created Layer res4a_branch2b/bn (28)
I0917 00:23:44.689226 17626 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0917 00:23:44.689230 17626 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0917 00:23:44.689618 17626 net.cpp:245] Setting up res4a_branch2b/bn
I0917 00:23:44.689626 17626 net.cpp:252] TEST Top shape for layer 28 'res4a_branch2b/bn' 1 256 64 128 (2097152)
I0917 00:23:44.689635 17626 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0917 00:23:44.689640 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.689643 17626 net.cpp:184] Created Layer res4a_branch2b/relu (29)
I0917 00:23:44.689647 17626 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0917 00:23:44.689651 17626 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0917 00:23:44.689657 17626 net.cpp:245] Setting up res4a_branch2b/relu
I0917 00:23:44.689661 17626 net.cpp:252] TEST Top shape for layer 29 'res4a_branch2b/relu' 1 256 64 128 (2097152)
I0917 00:23:44.689666 17626 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0917 00:23:44.689671 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.689677 17626 net.cpp:184] Created Layer pool4 (30)
I0917 00:23:44.689682 17626 net.cpp:561] pool4 <- res4a_branch2b
I0917 00:23:44.689684 17626 net.cpp:530] pool4 -> pool4
I0917 00:23:44.689720 17626 net.cpp:245] Setting up pool4
I0917 00:23:44.689725 17626 net.cpp:252] TEST Top shape for layer 30 'pool4' 1 256 64 128 (2097152)
I0917 00:23:44.689729 17626 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0917 00:23:44.689734 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.689743 17626 net.cpp:184] Created Layer res5a_branch2a (31)
I0917 00:23:44.689746 17626 net.cpp:561] res5a_branch2a <- pool4
I0917 00:23:44.689750 17626 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0917 00:23:44.714833 17626 net.cpp:245] Setting up res5a_branch2a
I0917 00:23:44.714855 17626 net.cpp:252] TEST Top shape for layer 31 'res5a_branch2a' 1 512 64 128 (4194304)
I0917 00:23:44.714864 17626 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0917 00:23:44.714869 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.714881 17626 net.cpp:184] Created Layer res5a_branch2a/bn (32)
I0917 00:23:44.714886 17626 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0917 00:23:44.714891 17626 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0917 00:23:44.715663 17626 net.cpp:245] Setting up res5a_branch2a/bn
I0917 00:23:44.715673 17626 net.cpp:252] TEST Top shape for layer 32 'res5a_branch2a/bn' 1 512 64 128 (4194304)
I0917 00:23:44.715682 17626 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0917 00:23:44.715687 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.715692 17626 net.cpp:184] Created Layer res5a_branch2a/relu (33)
I0917 00:23:44.715697 17626 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0917 00:23:44.715708 17626 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0917 00:23:44.715715 17626 net.cpp:245] Setting up res5a_branch2a/relu
I0917 00:23:44.715720 17626 net.cpp:252] TEST Top shape for layer 33 'res5a_branch2a/relu' 1 512 64 128 (4194304)
I0917 00:23:44.715724 17626 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0917 00:23:44.715728 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.715739 17626 net.cpp:184] Created Layer res5a_branch2b (34)
I0917 00:23:44.715741 17626 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0917 00:23:44.715745 17626 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0917 00:23:44.728075 17626 net.cpp:245] Setting up res5a_branch2b
I0917 00:23:44.728085 17626 net.cpp:252] TEST Top shape for layer 34 'res5a_branch2b' 1 512 64 128 (4194304)
I0917 00:23:44.728096 17626 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0917 00:23:44.728101 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.728107 17626 net.cpp:184] Created Layer res5a_branch2b/bn (35)
I0917 00:23:44.728111 17626 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0917 00:23:44.728116 17626 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0917 00:23:44.728497 17626 net.cpp:245] Setting up res5a_branch2b/bn
I0917 00:23:44.728504 17626 net.cpp:252] TEST Top shape for layer 35 'res5a_branch2b/bn' 1 512 64 128 (4194304)
I0917 00:23:44.728513 17626 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0917 00:23:44.728518 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.728521 17626 net.cpp:184] Created Layer res5a_branch2b/relu (36)
I0917 00:23:44.728525 17626 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0917 00:23:44.728530 17626 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0917 00:23:44.728538 17626 net.cpp:245] Setting up res5a_branch2b/relu
I0917 00:23:44.728543 17626 net.cpp:252] TEST Top shape for layer 36 'res5a_branch2b/relu' 1 512 64 128 (4194304)
I0917 00:23:44.728546 17626 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I0917 00:23:44.728550 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.728559 17626 net.cpp:184] Created Layer out5a (37)
I0917 00:23:44.728564 17626 net.cpp:561] out5a <- res5a_branch2b
I0917 00:23:44.728566 17626 net.cpp:530] out5a -> out5a
I0917 00:23:44.732316 17626 net.cpp:245] Setting up out5a
I0917 00:23:44.732336 17626 net.cpp:252] TEST Top shape for layer 37 'out5a' 1 64 64 128 (524288)
I0917 00:23:44.732344 17626 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I0917 00:23:44.732350 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.732360 17626 net.cpp:184] Created Layer out5a/bn (38)
I0917 00:23:44.732364 17626 net.cpp:561] out5a/bn <- out5a
I0917 00:23:44.732370 17626 net.cpp:513] out5a/bn -> out5a (in-place)
I0917 00:23:44.732795 17626 net.cpp:245] Setting up out5a/bn
I0917 00:23:44.732802 17626 net.cpp:252] TEST Top shape for layer 38 'out5a/bn' 1 64 64 128 (524288)
I0917 00:23:44.732810 17626 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I0917 00:23:44.732815 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.732820 17626 net.cpp:184] Created Layer out5a/relu (39)
I0917 00:23:44.732825 17626 net.cpp:561] out5a/relu <- out5a
I0917 00:23:44.732828 17626 net.cpp:513] out5a/relu -> out5a (in-place)
I0917 00:23:44.732834 17626 net.cpp:245] Setting up out5a/relu
I0917 00:23:44.732839 17626 net.cpp:252] TEST Top shape for layer 39 'out5a/relu' 1 64 64 128 (524288)
I0917 00:23:44.732843 17626 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I0917 00:23:44.732847 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.732869 17626 net.cpp:184] Created Layer out5a_up2 (40)
I0917 00:23:44.732873 17626 net.cpp:561] out5a_up2 <- out5a
I0917 00:23:44.732877 17626 net.cpp:530] out5a_up2 -> out5a_up2
I0917 00:23:44.733017 17626 net.cpp:245] Setting up out5a_up2
I0917 00:23:44.733023 17626 net.cpp:252] TEST Top shape for layer 40 'out5a_up2' 1 64 128 256 (2097152)
I0917 00:23:44.733028 17626 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I0917 00:23:44.733033 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.733043 17626 net.cpp:184] Created Layer out3a (41)
I0917 00:23:44.733047 17626 net.cpp:561] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0917 00:23:44.733052 17626 net.cpp:530] out3a -> out3a
I0917 00:23:44.733966 17626 net.cpp:245] Setting up out3a
I0917 00:23:44.733974 17626 net.cpp:252] TEST Top shape for layer 41 'out3a' 1 64 128 256 (2097152)
I0917 00:23:44.733981 17626 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I0917 00:23:44.733985 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.733992 17626 net.cpp:184] Created Layer out3a/bn (42)
I0917 00:23:44.733996 17626 net.cpp:561] out3a/bn <- out3a
I0917 00:23:44.734000 17626 net.cpp:513] out3a/bn -> out3a (in-place)
I0917 00:23:44.734410 17626 net.cpp:245] Setting up out3a/bn
I0917 00:23:44.734417 17626 net.cpp:252] TEST Top shape for layer 42 'out3a/bn' 1 64 128 256 (2097152)
I0917 00:23:44.734426 17626 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I0917 00:23:44.734429 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.734434 17626 net.cpp:184] Created Layer out3a/relu (43)
I0917 00:23:44.734438 17626 net.cpp:561] out3a/relu <- out3a
I0917 00:23:44.734442 17626 net.cpp:513] out3a/relu -> out3a (in-place)
I0917 00:23:44.734448 17626 net.cpp:245] Setting up out3a/relu
I0917 00:23:44.734453 17626 net.cpp:252] TEST Top shape for layer 43 'out3a/relu' 1 64 128 256 (2097152)
I0917 00:23:44.734457 17626 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I0917 00:23:44.734462 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.734472 17626 net.cpp:184] Created Layer out3_out5_combined (44)
I0917 00:23:44.734477 17626 net.cpp:561] out3_out5_combined <- out5a_up2
I0917 00:23:44.734480 17626 net.cpp:561] out3_out5_combined <- out3a
I0917 00:23:44.734484 17626 net.cpp:530] out3_out5_combined -> out3_out5_combined
I0917 00:23:44.734503 17626 net.cpp:245] Setting up out3_out5_combined
I0917 00:23:44.734508 17626 net.cpp:252] TEST Top shape for layer 44 'out3_out5_combined' 1 64 128 256 (2097152)
I0917 00:23:44.734513 17626 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I0917 00:23:44.734519 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.734527 17626 net.cpp:184] Created Layer ctx_conv1 (45)
I0917 00:23:44.734530 17626 net.cpp:561] ctx_conv1 <- out3_out5_combined
I0917 00:23:44.734534 17626 net.cpp:530] ctx_conv1 -> ctx_conv1
I0917 00:23:44.735446 17626 net.cpp:245] Setting up ctx_conv1
I0917 00:23:44.735453 17626 net.cpp:252] TEST Top shape for layer 45 'ctx_conv1' 1 64 128 256 (2097152)
I0917 00:23:44.735460 17626 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I0917 00:23:44.735465 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.735471 17626 net.cpp:184] Created Layer ctx_conv1/bn (46)
I0917 00:23:44.735474 17626 net.cpp:561] ctx_conv1/bn <- ctx_conv1
I0917 00:23:44.735478 17626 net.cpp:513] ctx_conv1/bn -> ctx_conv1 (in-place)
I0917 00:23:44.735875 17626 net.cpp:245] Setting up ctx_conv1/bn
I0917 00:23:44.735882 17626 net.cpp:252] TEST Top shape for layer 46 'ctx_conv1/bn' 1 64 128 256 (2097152)
I0917 00:23:44.735890 17626 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I0917 00:23:44.735900 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.735905 17626 net.cpp:184] Created Layer ctx_conv1/relu (47)
I0917 00:23:44.735910 17626 net.cpp:561] ctx_conv1/relu <- ctx_conv1
I0917 00:23:44.735913 17626 net.cpp:513] ctx_conv1/relu -> ctx_conv1 (in-place)
I0917 00:23:44.735919 17626 net.cpp:245] Setting up ctx_conv1/relu
I0917 00:23:44.735924 17626 net.cpp:252] TEST Top shape for layer 47 'ctx_conv1/relu' 1 64 128 256 (2097152)
I0917 00:23:44.735929 17626 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I0917 00:23:44.735934 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.735942 17626 net.cpp:184] Created Layer ctx_conv2 (48)
I0917 00:23:44.735945 17626 net.cpp:561] ctx_conv2 <- ctx_conv1
I0917 00:23:44.735949 17626 net.cpp:530] ctx_conv2 -> ctx_conv2
I0917 00:23:44.736840 17626 net.cpp:245] Setting up ctx_conv2
I0917 00:23:44.736847 17626 net.cpp:252] TEST Top shape for layer 48 'ctx_conv2' 1 64 128 256 (2097152)
I0917 00:23:44.736853 17626 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I0917 00:23:44.736858 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.736863 17626 net.cpp:184] Created Layer ctx_conv2/bn (49)
I0917 00:23:44.736867 17626 net.cpp:561] ctx_conv2/bn <- ctx_conv2
I0917 00:23:44.736871 17626 net.cpp:513] ctx_conv2/bn -> ctx_conv2 (in-place)
I0917 00:23:44.737263 17626 net.cpp:245] Setting up ctx_conv2/bn
I0917 00:23:44.737272 17626 net.cpp:252] TEST Top shape for layer 49 'ctx_conv2/bn' 1 64 128 256 (2097152)
I0917 00:23:44.737279 17626 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I0917 00:23:44.737283 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.737287 17626 net.cpp:184] Created Layer ctx_conv2/relu (50)
I0917 00:23:44.737291 17626 net.cpp:561] ctx_conv2/relu <- ctx_conv2
I0917 00:23:44.737295 17626 net.cpp:513] ctx_conv2/relu -> ctx_conv2 (in-place)
I0917 00:23:44.737300 17626 net.cpp:245] Setting up ctx_conv2/relu
I0917 00:23:44.737305 17626 net.cpp:252] TEST Top shape for layer 50 'ctx_conv2/relu' 1 64 128 256 (2097152)
I0917 00:23:44.737309 17626 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I0917 00:23:44.737313 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.737320 17626 net.cpp:184] Created Layer ctx_conv3 (51)
I0917 00:23:44.737324 17626 net.cpp:561] ctx_conv3 <- ctx_conv2
I0917 00:23:44.737327 17626 net.cpp:530] ctx_conv3 -> ctx_conv3
I0917 00:23:44.738219 17626 net.cpp:245] Setting up ctx_conv3
I0917 00:23:44.738225 17626 net.cpp:252] TEST Top shape for layer 51 'ctx_conv3' 1 64 128 256 (2097152)
I0917 00:23:44.738231 17626 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I0917 00:23:44.738235 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.738243 17626 net.cpp:184] Created Layer ctx_conv3/bn (52)
I0917 00:23:44.738247 17626 net.cpp:561] ctx_conv3/bn <- ctx_conv3
I0917 00:23:44.738251 17626 net.cpp:513] ctx_conv3/bn -> ctx_conv3 (in-place)
I0917 00:23:44.738651 17626 net.cpp:245] Setting up ctx_conv3/bn
I0917 00:23:44.738658 17626 net.cpp:252] TEST Top shape for layer 52 'ctx_conv3/bn' 1 64 128 256 (2097152)
I0917 00:23:44.738667 17626 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I0917 00:23:44.738672 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.738675 17626 net.cpp:184] Created Layer ctx_conv3/relu (53)
I0917 00:23:44.738679 17626 net.cpp:561] ctx_conv3/relu <- ctx_conv3
I0917 00:23:44.738683 17626 net.cpp:513] ctx_conv3/relu -> ctx_conv3 (in-place)
I0917 00:23:44.738688 17626 net.cpp:245] Setting up ctx_conv3/relu
I0917 00:23:44.738693 17626 net.cpp:252] TEST Top shape for layer 53 'ctx_conv3/relu' 1 64 128 256 (2097152)
I0917 00:23:44.738701 17626 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I0917 00:23:44.738706 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.738713 17626 net.cpp:184] Created Layer ctx_conv4 (54)
I0917 00:23:44.738718 17626 net.cpp:561] ctx_conv4 <- ctx_conv3
I0917 00:23:44.738721 17626 net.cpp:530] ctx_conv4 -> ctx_conv4
I0917 00:23:44.739609 17626 net.cpp:245] Setting up ctx_conv4
I0917 00:23:44.739616 17626 net.cpp:252] TEST Top shape for layer 54 'ctx_conv4' 1 64 128 256 (2097152)
I0917 00:23:44.739622 17626 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I0917 00:23:44.739627 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.739634 17626 net.cpp:184] Created Layer ctx_conv4/bn (55)
I0917 00:23:44.739636 17626 net.cpp:561] ctx_conv4/bn <- ctx_conv4
I0917 00:23:44.739640 17626 net.cpp:513] ctx_conv4/bn -> ctx_conv4 (in-place)
I0917 00:23:44.740036 17626 net.cpp:245] Setting up ctx_conv4/bn
I0917 00:23:44.740043 17626 net.cpp:252] TEST Top shape for layer 55 'ctx_conv4/bn' 1 64 128 256 (2097152)
I0917 00:23:44.740051 17626 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I0917 00:23:44.740056 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.740061 17626 net.cpp:184] Created Layer ctx_conv4/relu (56)
I0917 00:23:44.740064 17626 net.cpp:561] ctx_conv4/relu <- ctx_conv4
I0917 00:23:44.740068 17626 net.cpp:513] ctx_conv4/relu -> ctx_conv4 (in-place)
I0917 00:23:44.740074 17626 net.cpp:245] Setting up ctx_conv4/relu
I0917 00:23:44.740079 17626 net.cpp:252] TEST Top shape for layer 56 'ctx_conv4/relu' 1 64 128 256 (2097152)
I0917 00:23:44.740083 17626 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I0917 00:23:44.740087 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.740095 17626 net.cpp:184] Created Layer ctx_final (57)
I0917 00:23:44.740099 17626 net.cpp:561] ctx_final <- ctx_conv4
I0917 00:23:44.740103 17626 net.cpp:530] ctx_final -> ctx_final
I0917 00:23:44.740365 17626 net.cpp:245] Setting up ctx_final
I0917 00:23:44.740372 17626 net.cpp:252] TEST Top shape for layer 57 'ctx_final' 1 8 128 256 (262144)
I0917 00:23:44.740378 17626 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I0917 00:23:44.740382 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.740386 17626 net.cpp:184] Created Layer ctx_final/relu (58)
I0917 00:23:44.740391 17626 net.cpp:561] ctx_final/relu <- ctx_final
I0917 00:23:44.740394 17626 net.cpp:513] ctx_final/relu -> ctx_final (in-place)
I0917 00:23:44.740401 17626 net.cpp:245] Setting up ctx_final/relu
I0917 00:23:44.740406 17626 net.cpp:252] TEST Top shape for layer 58 'ctx_final/relu' 1 8 128 256 (262144)
I0917 00:23:44.740409 17626 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I0917 00:23:44.740414 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.740423 17626 net.cpp:184] Created Layer out_deconv_final_up2 (59)
I0917 00:23:44.740427 17626 net.cpp:561] out_deconv_final_up2 <- ctx_final
I0917 00:23:44.740430 17626 net.cpp:530] out_deconv_final_up2 -> out_deconv_final_up2
I0917 00:23:44.740543 17626 net.cpp:245] Setting up out_deconv_final_up2
I0917 00:23:44.740550 17626 net.cpp:252] TEST Top shape for layer 59 'out_deconv_final_up2' 1 8 256 512 (1048576)
I0917 00:23:44.740555 17626 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I0917 00:23:44.740558 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.740564 17626 net.cpp:184] Created Layer out_deconv_final_up4 (60)
I0917 00:23:44.740569 17626 net.cpp:561] out_deconv_final_up4 <- out_deconv_final_up2
I0917 00:23:44.740577 17626 net.cpp:530] out_deconv_final_up4 -> out_deconv_final_up4
I0917 00:23:44.740691 17626 net.cpp:245] Setting up out_deconv_final_up4
I0917 00:23:44.740697 17626 net.cpp:252] TEST Top shape for layer 60 'out_deconv_final_up4' 1 8 512 1024 (4194304)
I0917 00:23:44.740702 17626 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I0917 00:23:44.740705 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.740715 17626 net.cpp:184] Created Layer out_deconv_final_up8 (61)
I0917 00:23:44.740717 17626 net.cpp:561] out_deconv_final_up8 <- out_deconv_final_up4
I0917 00:23:44.740721 17626 net.cpp:530] out_deconv_final_up8 -> out_deconv_final_up8
I0917 00:23:44.740833 17626 net.cpp:245] Setting up out_deconv_final_up8
I0917 00:23:44.740839 17626 net.cpp:252] TEST Top shape for layer 61 'out_deconv_final_up8' 1 8 1024 2048 (16777216)
I0917 00:23:44.740844 17626 layer_factory.hpp:136] Creating layer 'argMaxOut' of type 'ArgMax'
I0917 00:23:44.740849 17626 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:23:44.740854 17626 net.cpp:184] Created Layer argMaxOut (62)
I0917 00:23:44.740857 17626 net.cpp:561] argMaxOut <- out_deconv_final_up8
I0917 00:23:44.740860 17626 net.cpp:530] argMaxOut -> argMaxOut
I0917 00:23:44.740877 17626 net.cpp:245] Setting up argMaxOut
I0917 00:23:44.740882 17626 net.cpp:252] TEST Top shape for layer 62 'argMaxOut' 1 1 1024 2048 (2097152)
I0917 00:23:44.740886 17626 net.cpp:325] argMaxOut does not need backward computation.
I0917 00:23:44.740890 17626 net.cpp:325] out_deconv_final_up8 does not need backward computation.
I0917 00:23:44.740895 17626 net.cpp:325] out_deconv_final_up4 does not need backward computation.
I0917 00:23:44.740897 17626 net.cpp:325] out_deconv_final_up2 does not need backward computation.
I0917 00:23:44.740901 17626 net.cpp:325] ctx_final/relu does not need backward computation.
I0917 00:23:44.740905 17626 net.cpp:325] ctx_final does not need backward computation.
I0917 00:23:44.740907 17626 net.cpp:325] ctx_conv4/relu does not need backward computation.
I0917 00:23:44.740911 17626 net.cpp:325] ctx_conv4/bn does not need backward computation.
I0917 00:23:44.740914 17626 net.cpp:325] ctx_conv4 does not need backward computation.
I0917 00:23:44.740917 17626 net.cpp:325] ctx_conv3/relu does not need backward computation.
I0917 00:23:44.740921 17626 net.cpp:325] ctx_conv3/bn does not need backward computation.
I0917 00:23:44.740924 17626 net.cpp:325] ctx_conv3 does not need backward computation.
I0917 00:23:44.740927 17626 net.cpp:325] ctx_conv2/relu does not need backward computation.
I0917 00:23:44.740931 17626 net.cpp:325] ctx_conv2/bn does not need backward computation.
I0917 00:23:44.740934 17626 net.cpp:325] ctx_conv2 does not need backward computation.
I0917 00:23:44.740937 17626 net.cpp:325] ctx_conv1/relu does not need backward computation.
I0917 00:23:44.740942 17626 net.cpp:325] ctx_conv1/bn does not need backward computation.
I0917 00:23:44.740944 17626 net.cpp:325] ctx_conv1 does not need backward computation.
I0917 00:23:44.740947 17626 net.cpp:325] out3_out5_combined does not need backward computation.
I0917 00:23:44.740952 17626 net.cpp:325] out3a/relu does not need backward computation.
I0917 00:23:44.740957 17626 net.cpp:325] out3a/bn does not need backward computation.
I0917 00:23:44.740960 17626 net.cpp:325] out3a does not need backward computation.
I0917 00:23:44.740963 17626 net.cpp:325] out5a_up2 does not need backward computation.
I0917 00:23:44.740967 17626 net.cpp:325] out5a/relu does not need backward computation.
I0917 00:23:44.740970 17626 net.cpp:325] out5a/bn does not need backward computation.
I0917 00:23:44.740974 17626 net.cpp:325] out5a does not need backward computation.
I0917 00:23:44.740978 17626 net.cpp:325] res5a_branch2b/relu does not need backward computation.
I0917 00:23:44.740981 17626 net.cpp:325] res5a_branch2b/bn does not need backward computation.
I0917 00:23:44.740989 17626 net.cpp:325] res5a_branch2b does not need backward computation.
I0917 00:23:44.740993 17626 net.cpp:325] res5a_branch2a/relu does not need backward computation.
I0917 00:23:44.740996 17626 net.cpp:325] res5a_branch2a/bn does not need backward computation.
I0917 00:23:44.741000 17626 net.cpp:325] res5a_branch2a does not need backward computation.
I0917 00:23:44.741004 17626 net.cpp:325] pool4 does not need backward computation.
I0917 00:23:44.741008 17626 net.cpp:325] res4a_branch2b/relu does not need backward computation.
I0917 00:23:44.741011 17626 net.cpp:325] res4a_branch2b/bn does not need backward computation.
I0917 00:23:44.741015 17626 net.cpp:325] res4a_branch2b does not need backward computation.
I0917 00:23:44.741019 17626 net.cpp:325] res4a_branch2a/relu does not need backward computation.
I0917 00:23:44.741022 17626 net.cpp:325] res4a_branch2a/bn does not need backward computation.
I0917 00:23:44.741026 17626 net.cpp:325] res4a_branch2a does not need backward computation.
I0917 00:23:44.741030 17626 net.cpp:325] pool3 does not need backward computation.
I0917 00:23:44.741034 17626 net.cpp:325] res3a_branch2b_res3a_branch2b/relu_0_split does not need backward computation.
I0917 00:23:44.741039 17626 net.cpp:325] res3a_branch2b/relu does not need backward computation.
I0917 00:23:44.741042 17626 net.cpp:325] res3a_branch2b/bn does not need backward computation.
I0917 00:23:44.741046 17626 net.cpp:325] res3a_branch2b does not need backward computation.
I0917 00:23:44.741050 17626 net.cpp:325] res3a_branch2a/relu does not need backward computation.
I0917 00:23:44.741055 17626 net.cpp:325] res3a_branch2a/bn does not need backward computation.
I0917 00:23:44.741057 17626 net.cpp:325] res3a_branch2a does not need backward computation.
I0917 00:23:44.741061 17626 net.cpp:325] pool2 does not need backward computation.
I0917 00:23:44.741065 17626 net.cpp:325] res2a_branch2b/relu does not need backward computation.
I0917 00:23:44.741070 17626 net.cpp:325] res2a_branch2b/bn does not need backward computation.
I0917 00:23:44.741073 17626 net.cpp:325] res2a_branch2b does not need backward computation.
I0917 00:23:44.741077 17626 net.cpp:325] res2a_branch2a/relu does not need backward computation.
I0917 00:23:44.741081 17626 net.cpp:325] res2a_branch2a/bn does not need backward computation.
I0917 00:23:44.741084 17626 net.cpp:325] res2a_branch2a does not need backward computation.
I0917 00:23:44.741088 17626 net.cpp:325] pool1 does not need backward computation.
I0917 00:23:44.741092 17626 net.cpp:325] conv1b/relu does not need backward computation.
I0917 00:23:44.741096 17626 net.cpp:325] conv1b/bn does not need backward computation.
I0917 00:23:44.741101 17626 net.cpp:325] conv1b does not need backward computation.
I0917 00:23:44.741104 17626 net.cpp:325] conv1a/relu does not need backward computation.
I0917 00:23:44.741107 17626 net.cpp:325] conv1a/bn does not need backward computation.
I0917 00:23:44.741111 17626 net.cpp:325] conv1a does not need backward computation.
I0917 00:23:44.741116 17626 net.cpp:325] data/bias does not need backward computation.
I0917 00:23:44.741119 17626 net.cpp:325] input does not need backward computation.
I0917 00:23:44.741123 17626 net.cpp:367] This network produces output argMaxOut
I0917 00:23:44.741163 17626 net.cpp:389] Top memory (TEST) required for data: 1224736768 diff: 1224736768
I0917 00:23:44.741168 17626 net.cpp:392] Bottom memory (TEST) required for data: 1216348160 diff: 1216348160
I0917 00:23:44.741170 17626 net.cpp:395] Shared (in-place) memory (TEST) by data: 659554304 diff: 659554304
I0917 00:23:44.741173 17626 net.cpp:398] Parameters memory (TEST) required for data: 10817840 diff: 10817840
I0917 00:23:44.741176 17626 net.cpp:401] Parameters shared memory (TEST) by data: 0 diff: 0
I0917 00:23:44.741179 17626 net.cpp:407] Network initialization done.
I0917 00:23:44.746223 17626 net.cpp:1078] Ignoring source layer data
I0917 00:23:44.746243 17626 net.cpp:1094] Copying source layer data/bias Type:Bias #blobs=1
I0917 00:23:44.746275 17626 net.cpp:1094] Copying source layer conv1a Type:Convolution #blobs=2
I0917 00:23:44.746302 17626 net.cpp:1094] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0917 00:23:44.746469 17626 net.cpp:1094] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0917 00:23:44.746475 17626 net.cpp:1094] Copying source layer conv1b Type:Convolution #blobs=2
I0917 00:23:44.746484 17626 net.cpp:1094] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0917 00:23:44.746598 17626 net.cpp:1094] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0917 00:23:44.746603 17626 net.cpp:1094] Copying source layer pool1 Type:Pooling #blobs=0
I0917 00:23:44.746608 17626 net.cpp:1094] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0917 00:23:44.746625 17626 net.cpp:1094] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0917 00:23:44.746742 17626 net.cpp:1094] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0917 00:23:44.746748 17626 net.cpp:1094] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0917 00:23:44.746764 17626 net.cpp:1094] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0917 00:23:44.746879 17626 net.cpp:1094] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0917 00:23:44.746886 17626 net.cpp:1094] Copying source layer pool2 Type:Pooling #blobs=0
I0917 00:23:44.746891 17626 net.cpp:1094] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0917 00:23:44.746929 17626 net.cpp:1094] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0917 00:23:44.747040 17626 net.cpp:1094] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0917 00:23:44.747046 17626 net.cpp:1094] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0917 00:23:44.747072 17626 net.cpp:1094] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0917 00:23:44.747174 17626 net.cpp:1094] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0917 00:23:44.747179 17626 net.cpp:1094] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I0917 00:23:44.747181 17626 net.cpp:1094] Copying source layer pool3 Type:Pooling #blobs=0
I0917 00:23:44.747185 17626 net.cpp:1094] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0917 00:23:44.747300 17626 net.cpp:1094] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0917 00:23:44.747395 17626 net.cpp:1094] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0917 00:23:44.747400 17626 net.cpp:1094] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0917 00:23:44.747460 17626 net.cpp:1094] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0917 00:23:44.747553 17626 net.cpp:1094] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0917 00:23:44.747558 17626 net.cpp:1094] Copying source layer pool4 Type:Pooling #blobs=0
I0917 00:23:44.747561 17626 net.cpp:1094] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0917 00:23:44.747910 17626 net.cpp:1094] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0917 00:23:44.747999 17626 net.cpp:1094] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0917 00:23:44.748004 17626 net.cpp:1094] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0917 00:23:44.748180 17626 net.cpp:1094] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0917 00:23:44.748267 17626 net.cpp:1094] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0917 00:23:44.748273 17626 net.cpp:1094] Copying source layer out5a Type:Convolution #blobs=2
I0917 00:23:44.748335 17626 net.cpp:1094] Copying source layer out5a/bn Type:BatchNorm #blobs=5
I0917 00:23:44.748441 17626 net.cpp:1094] Copying source layer out5a/relu Type:ReLU #blobs=0
I0917 00:23:44.748446 17626 net.cpp:1094] Copying source layer out5a_up2 Type:Deconvolution #blobs=1
I0917 00:23:44.748453 17626 net.cpp:1094] Copying source layer out3a Type:Convolution #blobs=2
I0917 00:23:44.748476 17626 net.cpp:1094] Copying source layer out3a/bn Type:BatchNorm #blobs=5
I0917 00:23:44.748597 17626 net.cpp:1094] Copying source layer out3a/relu Type:ReLU #blobs=0
I0917 00:23:44.748602 17626 net.cpp:1094] Copying source layer out3_out5_combined Type:Eltwise #blobs=0
I0917 00:23:44.748605 17626 net.cpp:1094] Copying source layer ctx_conv1 Type:Convolution #blobs=2
I0917 00:23:44.748626 17626 net.cpp:1094] Copying source layer ctx_conv1/bn Type:BatchNorm #blobs=5
I0917 00:23:44.748741 17626 net.cpp:1094] Copying source layer ctx_conv1/relu Type:ReLU #blobs=0
I0917 00:23:44.748747 17626 net.cpp:1094] Copying source layer ctx_conv2 Type:Convolution #blobs=2
I0917 00:23:44.748770 17626 net.cpp:1094] Copying source layer ctx_conv2/bn Type:BatchNorm #blobs=5
I0917 00:23:44.748883 17626 net.cpp:1094] Copying source layer ctx_conv2/relu Type:ReLU #blobs=0
I0917 00:23:44.748888 17626 net.cpp:1094] Copying source layer ctx_conv3 Type:Convolution #blobs=2
I0917 00:23:44.748908 17626 net.cpp:1094] Copying source layer ctx_conv3/bn Type:BatchNorm #blobs=5
I0917 00:23:44.749019 17626 net.cpp:1094] Copying source layer ctx_conv3/relu Type:ReLU #blobs=0
I0917 00:23:44.749024 17626 net.cpp:1094] Copying source layer ctx_conv4 Type:Convolution #blobs=2
I0917 00:23:44.749043 17626 net.cpp:1094] Copying source layer ctx_conv4/bn Type:BatchNorm #blobs=5
I0917 00:23:44.749155 17626 net.cpp:1094] Copying source layer ctx_conv4/relu Type:ReLU #blobs=0
I0917 00:23:44.749161 17626 net.cpp:1094] Copying source layer ctx_final Type:Convolution #blobs=2
I0917 00:23:44.749171 17626 net.cpp:1094] Copying source layer ctx_final/relu Type:ReLU #blobs=0
I0917 00:23:44.749176 17626 net.cpp:1094] Copying source layer out_deconv_final_up2 Type:Deconvolution #blobs=1
I0917 00:23:44.749184 17626 net.cpp:1094] Copying source layer out_deconv_final_up4 Type:Deconvolution #blobs=1
I0917 00:23:44.749191 17626 net.cpp:1094] Copying source layer out_deconv_final_up8 Type:Deconvolution #blobs=1
I0917 00:23:44.749199 17626 net.cpp:1078] Ignoring source layer loss
Namespace(batch_size=1, blend=False, class_dict='', crop=['0'], input='./data/val-image-list.txt', label='./data/val-label-list.txt', label_dict='', model='../trained/image_segmentation/cityscapes5_jsegnet21v2/initial/deploy.prototxt', num_classes=5, num_images=500, output=None, palette='', resize=['0'], resize_back=True, search='*.png', weights='../trained/image_segmentation/cityscapes5_jsegnet21v2/initial/cityscapes5_jsegnet21v2_iter_120000.caffemodel')
Infering list
Getting list of images...running inference for  500  images...
('frankfurt_000000_000294_leftImg8bit.png', 'frankfurt_000000_000294_gtFine_labelIds.png', 0)
('frankfurt_000000_000576_leftImg8bit.png', 'frankfurt_000000_000576_gtFine_labelIds.png', 0)
('frankfurt_000000_001016_leftImg8bit.png', 'frankfurt_000000_001016_gtFine_labelIds.png', 0)
I0917 00:23:45.590288 17626 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'conv1a' with space 0.01G 3/1 1 	(avail 5.92G, req 0.01G)	t: 0
I0917 00:23:45.719091 17626 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'conv1b' with space 0.01G 32/4 6 	(avail 5.92G, req 0.01G)	t: 0
I0917 00:23:45.736428 17626 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res2a_branch2a' with space 0.01G 32/1 6 	(avail 5.92G, req 0.01G)	t: 0
I0917 00:23:45.777616 17626 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res2a_branch2b' with space 0.01G 64/4 6 	(avail 5.92G, req 0.01G)	t: 0
I0917 00:23:45.790611 17626 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res3a_branch2a' with space 0.01G 64/1 6 	(avail 5.92G, req 0.01G)	t: 0
I0917 00:23:45.794773 17626 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res3a_branch2b' with space 0.01G 128/4 6 	(avail 5.92G, req 0.01G)	t: 0
I0917 00:23:45.805006 17626 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res4a_branch2a' with space 0.01G 128/1 6 	(avail 5.92G, req 0.01G)	t: 0
I0917 00:23:45.816469 17626 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res4a_branch2b' with space 0.01G 256/4 6 	(avail 5.92G, req 0.01G)	t: 0
I0917 00:23:45.832594 17626 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'out3a' with space 0.01G 128/2 6 	(avail 5.92G, req 0.01G)	t: 0
I0917 00:23:45.839223 17626 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'ctx_conv1' with space 0.01G 64/1 6 	(avail 5.92G, req 0.01G)	t: 0
I0917 00:23:45.865464 17626 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'ctx_final' with space 0.01G 64/1 6 	(avail 5.92G, req 0.01G)	t: 0
('frankfurt_000000_001236_leftImg8bit.png', 'frankfurt_000000_001236_gtFine_labelIds.png', 0)
('frankfurt_000000_001751_leftImg8bit.png', 'frankfurt_000000_001751_gtFine_labelIds.png', 0)
('frankfurt_000000_002196_leftImg8bit.png', 'frankfurt_000000_002196_gtFine_labelIds.png', 1)
('frankfurt_000000_002963_leftImg8bit.png', 'frankfurt_000000_002963_gtFine_labelIds.png', 1)
('frankfurt_000000_003025_leftImg8bit.png', 'frankfurt_000000_003025_gtFine_labelIds.png', 1)
('frankfurt_000000_003357_leftImg8bit.png', 'frankfurt_000000_003357_gtFine_labelIds.png', 1)
('frankfurt_000000_003920_leftImg8bit.png', 'frankfurt_000000_003920_gtFine_labelIds.png', 1)
('frankfurt_000000_004617_leftImg8bit.png', 'frankfurt_000000_004617_gtFine_labelIds.png', 2)
('frankfurt_000000_005543_leftImg8bit.png', 'frankfurt_000000_005543_gtFine_labelIds.png', 2)
('frankfurt_000000_005898_leftImg8bit.png', 'frankfurt_000000_005898_gtFine_labelIds.png', 2)
('frankfurt_000000_006589_leftImg8bit.png', 'frankfurt_000000_006589_gtFine_labelIds.png', 2)
('frankfurt_000000_007365_leftImg8bit.png', 'frankfurt_000000_007365_gtFine_labelIds.png', 2)
('frankfurt_000000_008206_leftImg8bit.png', 'frankfurt_000000_008206_gtFine_labelIds.png', 3)
('frankfurt_000000_008451_leftImg8bit.png', 'frankfurt_000000_008451_gtFine_labelIds.png', 3)
('frankfurt_000000_009291_leftImg8bit.png', 'frankfurt_000000_009291_gtFine_labelIds.png', 3)
('frankfurt_000000_009561_leftImg8bit.png', 'frankfurt_000000_009561_gtFine_labelIds.png', 3)
('frankfurt_000000_009688_leftImg8bit.png', 'frankfurt_000000_009688_gtFine_labelIds.png', 3)
('frankfurt_000000_009969_leftImg8bit.png', 'frankfurt_000000_009969_gtFine_labelIds.png', 4)
('frankfurt_000000_010351_leftImg8bit.png', 'frankfurt_000000_010351_gtFine_labelIds.png', 4)
('frankfurt_000000_010763_leftImg8bit.png', 'frankfurt_000000_010763_gtFine_labelIds.png', 4)
('frankfurt_000000_011007_leftImg8bit.png', 'frankfurt_000000_011007_gtFine_labelIds.png', 4)
('frankfurt_000000_011074_leftImg8bit.png', 'frankfurt_000000_011074_gtFine_labelIds.png', 4)
pixel_accuracy=0.97455676307, mean_iou=0.825654093029, iou=[ 0.95761541  0.96552323  0.77736137  0.55684367  0.87092677]
('frankfurt_000000_011461_leftImg8bit.png', 'frankfurt_000000_011461_gtFine_labelIds.png', 5)
('frankfurt_000000_011810_leftImg8bit.png', 'frankfurt_000000_011810_gtFine_labelIds.png', 5)
('frankfurt_000000_012009_leftImg8bit.png', 'frankfurt_000000_012009_gtFine_labelIds.png', 5)
('frankfurt_000000_012121_leftImg8bit.png', 'frankfurt_000000_012121_gtFine_labelIds.png', 5)
('frankfurt_000000_012868_leftImg8bit.png', 'frankfurt_000000_012868_gtFine_labelIds.png', 5)
('frankfurt_000000_013067_leftImg8bit.png', 'frankfurt_000000_013067_gtFine_labelIds.png', 6)
('frankfurt_000000_013240_leftImg8bit.png', 'frankfurt_000000_013240_gtFine_labelIds.png', 6)
('frankfurt_000000_013382_leftImg8bit.png', 'frankfurt_000000_013382_gtFine_labelIds.png', 6)
('frankfurt_000000_013942_leftImg8bit.png', 'frankfurt_000000_013942_gtFine_labelIds.png', 6)
('frankfurt_000000_014480_leftImg8bit.png', 'frankfurt_000000_014480_gtFine_labelIds.png', 6)
('frankfurt_000000_015389_leftImg8bit.png', 'frankfurt_000000_015389_gtFine_labelIds.png', 7)
('frankfurt_000000_015676_leftImg8bit.png', 'frankfurt_000000_015676_gtFine_labelIds.png', 7)
('frankfurt_000000_016005_leftImg8bit.png', 'frankfurt_000000_016005_gtFine_labelIds.png', 7)
('frankfurt_000000_016286_leftImg8bit.png', 'frankfurt_000000_016286_gtFine_labelIds.png', 7)
('frankfurt_000000_017228_leftImg8bit.png', 'frankfurt_000000_017228_gtFine_labelIds.png', 7)
('frankfurt_000000_017476_leftImg8bit.png', 'frankfurt_000000_017476_gtFine_labelIds.png', 8)
('frankfurt_000000_018797_leftImg8bit.png', 'frankfurt_000000_018797_gtFine_labelIds.png', 8)
('frankfurt_000000_019607_leftImg8bit.png', 'frankfurt_000000_019607_gtFine_labelIds.png', 8)
('frankfurt_000000_020215_leftImg8bit.png', 'frankfurt_000000_020215_gtFine_labelIds.png', 8)
('frankfurt_000000_020321_leftImg8bit.png', 'frankfurt_000000_020321_gtFine_labelIds.png', 8)
('frankfurt_000000_020880_leftImg8bit.png', 'frankfurt_000000_020880_gtFine_labelIds.png', 9)
('frankfurt_000000_021667_leftImg8bit.png', 'frankfurt_000000_021667_gtFine_labelIds.png', 9)
('frankfurt_000000_021879_leftImg8bit.png', 'frankfurt_000000_021879_gtFine_labelIds.png', 9)
('frankfurt_000000_022254_leftImg8bit.png', 'frankfurt_000000_022254_gtFine_labelIds.png', 9)
('frankfurt_000000_022797_leftImg8bit.png', 'frankfurt_000000_022797_gtFine_labelIds.png', 9)
pixel_accuracy=0.974087220335, mean_iou=0.835573666559, iou=[ 0.95694168  0.96616225  0.77317111  0.60001612  0.88157717]
('frankfurt_000001_000538_leftImg8bit.png', 'frankfurt_000001_000538_gtFine_labelIds.png', 10)
('frankfurt_000001_001464_leftImg8bit.png', 'frankfurt_000001_001464_gtFine_labelIds.png', 10)
('frankfurt_000001_002512_leftImg8bit.png', 'frankfurt_000001_002512_gtFine_labelIds.png', 10)
('frankfurt_000001_002646_leftImg8bit.png', 'frankfurt_000001_002646_gtFine_labelIds.png', 10)
('frankfurt_000001_002759_leftImg8bit.png', 'frankfurt_000001_002759_gtFine_labelIds.png', 10)
('frankfurt_000001_003056_leftImg8bit.png', 'frankfurt_000001_003056_gtFine_labelIds.png', 11)
('frankfurt_000001_003588_leftImg8bit.png', 'frankfurt_000001_003588_gtFine_labelIds.png', 11)
('frankfurt_000001_004327_leftImg8bit.png', 'frankfurt_000001_004327_gtFine_labelIds.png', 11)
('frankfurt_000001_004736_leftImg8bit.png', 'frankfurt_000001_004736_gtFine_labelIds.png', 11)
('frankfurt_000001_004859_leftImg8bit.png', 'frankfurt_000001_004859_gtFine_labelIds.png', 11)
('frankfurt_000001_005184_leftImg8bit.png', 'frankfurt_000001_005184_gtFine_labelIds.png', 12)
('frankfurt_000001_005410_leftImg8bit.png', 'frankfurt_000001_005410_gtFine_labelIds.png', 12)
('frankfurt_000001_005703_leftImg8bit.png', 'frankfurt_000001_005703_gtFine_labelIds.png', 12)
('frankfurt_000001_005898_leftImg8bit.png', 'frankfurt_000001_005898_gtFine_labelIds.png', 12)
('frankfurt_000001_007285_leftImg8bit.png', 'frankfurt_000001_007285_gtFine_labelIds.png', 12)
('frankfurt_000001_007407_leftImg8bit.png', 'frankfurt_000001_007407_gtFine_labelIds.png', 13)
('frankfurt_000001_007622_leftImg8bit.png', 'frankfurt_000001_007622_gtFine_labelIds.png', 13)
('frankfurt_000001_007857_leftImg8bit.png', 'frankfurt_000001_007857_gtFine_labelIds.png', 13)
('frankfurt_000001_007973_leftImg8bit.png', 'frankfurt_000001_007973_gtFine_labelIds.png', 13)
('frankfurt_000001_008200_leftImg8bit.png', 'frankfurt_000001_008200_gtFine_labelIds.png', 13)
('frankfurt_000001_008688_leftImg8bit.png', 'frankfurt_000001_008688_gtFine_labelIds.png', 14)
('frankfurt_000001_009058_leftImg8bit.png', 'frankfurt_000001_009058_gtFine_labelIds.png', 14)
('frankfurt_000001_009504_leftImg8bit.png', 'frankfurt_000001_009504_gtFine_labelIds.png', 14)
('frankfurt_000001_009854_leftImg8bit.png', 'frankfurt_000001_009854_gtFine_labelIds.png', 14)
('frankfurt_000001_010156_leftImg8bit.png', 'frankfurt_000001_010156_gtFine_labelIds.png', 14)
pixel_accuracy=0.972279479265, mean_iou=0.831190916937, iou=[ 0.95383573  0.96290285  0.75695414  0.5989778   0.88328406]
('frankfurt_000001_010444_leftImg8bit.png', 'frankfurt_000001_010444_gtFine_labelIds.png', 15)
('frankfurt_000001_010600_leftImg8bit.png', 'frankfurt_000001_010600_gtFine_labelIds.png', 15)
('frankfurt_000001_010830_leftImg8bit.png', 'frankfurt_000001_010830_gtFine_labelIds.png', 15)
('frankfurt_000001_011162_leftImg8bit.png', 'frankfurt_000001_011162_gtFine_labelIds.png', 15)
('frankfurt_000001_011715_leftImg8bit.png', 'frankfurt_000001_011715_gtFine_labelIds.png', 15)
('frankfurt_000001_011835_leftImg8bit.png', 'frankfurt_000001_011835_gtFine_labelIds.png', 16)
('frankfurt_000001_012038_leftImg8bit.png', 'frankfurt_000001_012038_gtFine_labelIds.png', 16)
('frankfurt_000001_012519_leftImg8bit.png', 'frankfurt_000001_012519_gtFine_labelIds.png', 16)
('frankfurt_000001_012699_leftImg8bit.png', 'frankfurt_000001_012699_gtFine_labelIds.png', 16)
('frankfurt_000001_012738_leftImg8bit.png', 'frankfurt_000001_012738_gtFine_labelIds.png', 16)
('frankfurt_000001_012870_leftImg8bit.png', 'frankfurt_000001_012870_gtFine_labelIds.png', 17)
('frankfurt_000001_013016_leftImg8bit.png', 'frankfurt_000001_013016_gtFine_labelIds.png', 17)
('frankfurt_000001_013496_leftImg8bit.png', 'frankfurt_000001_013496_gtFine_labelIds.png', 17)
('frankfurt_000001_013710_leftImg8bit.png', 'frankfurt_000001_013710_gtFine_labelIds.png', 17)
('frankfurt_000001_014221_leftImg8bit.png', 'frankfurt_000001_014221_gtFine_labelIds.png', 17)
('frankfurt_000001_014406_leftImg8bit.png', 'frankfurt_000001_014406_gtFine_labelIds.png', 18)
('frankfurt_000001_014565_leftImg8bit.png', 'frankfurt_000001_014565_gtFine_labelIds.png', 18)
('frankfurt_000001_014741_leftImg8bit.png', 'frankfurt_000001_014741_gtFine_labelIds.png', 18)
('frankfurt_000001_015091_leftImg8bit.png', 'frankfurt_000001_015091_gtFine_labelIds.png', 18)
('frankfurt_000001_015328_leftImg8bit.png', 'frankfurt_000001_015328_gtFine_labelIds.png', 18)
('frankfurt_000001_015768_leftImg8bit.png', 'frankfurt_000001_015768_gtFine_labelIds.png', 19)
('frankfurt_000001_016029_leftImg8bit.png', 'frankfurt_000001_016029_gtFine_labelIds.png', 19)
('frankfurt_000001_016273_leftImg8bit.png', 'frankfurt_000001_016273_gtFine_labelIds.png', 19)
('frankfurt_000001_016462_leftImg8bit.png', 'frankfurt_000001_016462_gtFine_labelIds.png', 19)
('frankfurt_000001_017101_leftImg8bit.png', 'frankfurt_000001_017101_gtFine_labelIds.png', 19)
pixel_accuracy=0.968052986999, mean_iou=0.834122352618, iou=[ 0.94755897  0.95598472  0.7779777   0.61160269  0.87748767]
('frankfurt_000001_017459_leftImg8bit.png', 'frankfurt_000001_017459_gtFine_labelIds.png', 20)
('frankfurt_000001_017842_leftImg8bit.png', 'frankfurt_000001_017842_gtFine_labelIds.png', 20)
('frankfurt_000001_018113_leftImg8bit.png', 'frankfurt_000001_018113_gtFine_labelIds.png', 20)
('frankfurt_000001_019698_leftImg8bit.png', 'frankfurt_000001_019698_gtFine_labelIds.png', 20)
('frankfurt_000001_019854_leftImg8bit.png', 'frankfurt_000001_019854_gtFine_labelIds.png', 20)
('frankfurt_000001_019969_leftImg8bit.png', 'frankfurt_000001_019969_gtFine_labelIds.png', 21)
('frankfurt_000001_020046_leftImg8bit.png', 'frankfurt_000001_020046_gtFine_labelIds.png', 21)
('frankfurt_000001_020287_leftImg8bit.png', 'frankfurt_000001_020287_gtFine_labelIds.png', 21)
('frankfurt_000001_020693_leftImg8bit.png', 'frankfurt_000001_020693_gtFine_labelIds.png', 21)
('frankfurt_000001_021406_leftImg8bit.png', 'frankfurt_000001_021406_gtFine_labelIds.png', 21)
('frankfurt_000001_021825_leftImg8bit.png', 'frankfurt_000001_021825_gtFine_labelIds.png', 22)
('frankfurt_000001_023235_leftImg8bit.png', 'frankfurt_000001_023235_gtFine_labelIds.png', 22)
('frankfurt_000001_023369_leftImg8bit.png', 'frankfurt_000001_023369_gtFine_labelIds.png', 22)
('frankfurt_000001_023769_leftImg8bit.png', 'frankfurt_000001_023769_gtFine_labelIds.png', 22)
('frankfurt_000001_024927_leftImg8bit.png', 'frankfurt_000001_024927_gtFine_labelIds.png', 22)
('frankfurt_000001_025512_leftImg8bit.png', 'frankfurt_000001_025512_gtFine_labelIds.png', 23)
('frankfurt_000001_025713_leftImg8bit.png', 'frankfurt_000001_025713_gtFine_labelIds.png', 23)
('frankfurt_000001_025921_leftImg8bit.png', 'frankfurt_000001_025921_gtFine_labelIds.png', 23)
('frankfurt_000001_027325_leftImg8bit.png', 'frankfurt_000001_027325_gtFine_labelIds.png', 23)
('frankfurt_000001_028232_leftImg8bit.png', 'frankfurt_000001_028232_gtFine_labelIds.png', 23)
('frankfurt_000001_028335_leftImg8bit.png', 'frankfurt_000001_028335_gtFine_labelIds.png', 24)
('frankfurt_000001_028590_leftImg8bit.png', 'frankfurt_000001_028590_gtFine_labelIds.png', 24)
('frankfurt_000001_028854_leftImg8bit.png', 'frankfurt_000001_028854_gtFine_labelIds.png', 24)
('frankfurt_000001_029086_leftImg8bit.png', 'frankfurt_000001_029086_gtFine_labelIds.png', 24)
('frankfurt_000001_029236_leftImg8bit.png', 'frankfurt_000001_029236_gtFine_labelIds.png', 24)
pixel_accuracy=0.968415051219, mean_iou=0.832107341958, iou=[ 0.94849567  0.95594567  0.77530833  0.60325509  0.87753194]
('frankfurt_000001_029600_leftImg8bit.png', 'frankfurt_000001_029600_gtFine_labelIds.png', 25)
('frankfurt_000001_030067_leftImg8bit.png', 'frankfurt_000001_030067_gtFine_labelIds.png', 25)
('frankfurt_000001_030310_leftImg8bit.png', 'frankfurt_000001_030310_gtFine_labelIds.png', 25)
('frankfurt_000001_030669_leftImg8bit.png', 'frankfurt_000001_030669_gtFine_labelIds.png', 25)
('frankfurt_000001_031266_leftImg8bit.png', 'frankfurt_000001_031266_gtFine_labelIds.png', 25)
('frankfurt_000001_031416_leftImg8bit.png', 'frankfurt_000001_031416_gtFine_labelIds.png', 26)
('frankfurt_000001_032018_leftImg8bit.png', 'frankfurt_000001_032018_gtFine_labelIds.png', 26)
('frankfurt_000001_032556_leftImg8bit.png', 'frankfurt_000001_032556_gtFine_labelIds.png', 26)
('frankfurt_000001_032711_leftImg8bit.png', 'frankfurt_000001_032711_gtFine_labelIds.png', 26)
('frankfurt_000001_032942_leftImg8bit.png', 'frankfurt_000001_032942_gtFine_labelIds.png', 26)
('frankfurt_000001_033655_leftImg8bit.png', 'frankfurt_000001_033655_gtFine_labelIds.png', 27)
('frankfurt_000001_034047_leftImg8bit.png', 'frankfurt_000001_034047_gtFine_labelIds.png', 27)
('frankfurt_000001_034816_leftImg8bit.png', 'frankfurt_000001_034816_gtFine_labelIds.png', 27)
('frankfurt_000001_035144_leftImg8bit.png', 'frankfurt_000001_035144_gtFine_labelIds.png', 27)
('frankfurt_000001_035864_leftImg8bit.png', 'frankfurt_000001_035864_gtFine_labelIds.png', 27)
('frankfurt_000001_037705_leftImg8bit.png', 'frankfurt_000001_037705_gtFine_labelIds.png', 28)
('frankfurt_000001_038245_leftImg8bit.png', 'frankfurt_000001_038245_gtFine_labelIds.png', 28)
('frankfurt_000001_038418_leftImg8bit.png', 'frankfurt_000001_038418_gtFine_labelIds.png', 28)
('frankfurt_000001_038645_leftImg8bit.png', 'frankfurt_000001_038645_gtFine_labelIds.png', 28)
('frankfurt_000001_038844_leftImg8bit.png', 'frankfurt_000001_038844_gtFine_labelIds.png', 28)
('frankfurt_000001_039895_leftImg8bit.png', 'frankfurt_000001_039895_gtFine_labelIds.png', 29)
('frankfurt_000001_040575_leftImg8bit.png', 'frankfurt_000001_040575_gtFine_labelIds.png', 29)
('frankfurt_000001_040732_leftImg8bit.png', 'frankfurt_000001_040732_gtFine_labelIds.png', 29)
('frankfurt_000001_041074_leftImg8bit.png', 'frankfurt_000001_041074_gtFine_labelIds.png', 29)
('frankfurt_000001_041354_leftImg8bit.png', 'frankfurt_000001_041354_gtFine_labelIds.png', 29)
pixel_accuracy=0.968699133756, mean_iou=0.831438460416, iou=[ 0.94871221  0.95732821  0.76329873  0.60415327  0.88369987]
('frankfurt_000001_041517_leftImg8bit.png', 'frankfurt_000001_041517_gtFine_labelIds.png', 30)
('frankfurt_000001_041664_leftImg8bit.png', 'frankfurt_000001_041664_gtFine_labelIds.png', 30)
('frankfurt_000001_042098_leftImg8bit.png', 'frankfurt_000001_042098_gtFine_labelIds.png', 30)
('frankfurt_000001_042384_leftImg8bit.png', 'frankfurt_000001_042384_gtFine_labelIds.png', 30)
('frankfurt_000001_042733_leftImg8bit.png', 'frankfurt_000001_042733_gtFine_labelIds.png', 30)
('frankfurt_000001_043395_leftImg8bit.png', 'frankfurt_000001_043395_gtFine_labelIds.png', 31)
('frankfurt_000001_043564_leftImg8bit.png', 'frankfurt_000001_043564_gtFine_labelIds.png', 31)
('frankfurt_000001_044227_leftImg8bit.png', 'frankfurt_000001_044227_gtFine_labelIds.png', 31)
('frankfurt_000001_044413_leftImg8bit.png', 'frankfurt_000001_044413_gtFine_labelIds.png', 31)
('frankfurt_000001_044525_leftImg8bit.png', 'frankfurt_000001_044525_gtFine_labelIds.png', 31)
('frankfurt_000001_044658_leftImg8bit.png', 'frankfurt_000001_044658_gtFine_labelIds.png', 32)
('frankfurt_000001_044787_leftImg8bit.png', 'frankfurt_000001_044787_gtFine_labelIds.png', 32)
('frankfurt_000001_046126_leftImg8bit.png', 'frankfurt_000001_046126_gtFine_labelIds.png', 32)
('frankfurt_000001_046272_leftImg8bit.png', 'frankfurt_000001_046272_gtFine_labelIds.png', 32)
('frankfurt_000001_046504_leftImg8bit.png', 'frankfurt_000001_046504_gtFine_labelIds.png', 32)
('frankfurt_000001_046779_leftImg8bit.png', 'frankfurt_000001_046779_gtFine_labelIds.png', 33)
('frankfurt_000001_047178_leftImg8bit.png', 'frankfurt_000001_047178_gtFine_labelIds.png', 33)
('frankfurt_000001_047552_leftImg8bit.png', 'frankfurt_000001_047552_gtFine_labelIds.png', 33)
('frankfurt_000001_048196_leftImg8bit.png', 'frankfurt_000001_048196_gtFine_labelIds.png', 33)
('frankfurt_000001_048355_leftImg8bit.png', 'frankfurt_000001_048355_gtFine_labelIds.png', 33)
('frankfurt_000001_048654_leftImg8bit.png', 'frankfurt_000001_048654_gtFine_labelIds.png', 34)
('frankfurt_000001_049078_leftImg8bit.png', 'frankfurt_000001_049078_gtFine_labelIds.png', 34)
('frankfurt_000001_049209_leftImg8bit.png', 'frankfurt_000001_049209_gtFine_labelIds.png', 34)
('frankfurt_000001_049298_leftImg8bit.png', 'frankfurt_000001_049298_gtFine_labelIds.png', 34)
('frankfurt_000001_049698_leftImg8bit.png', 'frankfurt_000001_049698_gtFine_labelIds.png', 34)
pixel_accuracy=0.970359993272, mean_iou=0.833659281884, iou=[ 0.95158456  0.95964065  0.76359984  0.60063193  0.89283943]
('frankfurt_000001_049770_leftImg8bit.png', 'frankfurt_000001_049770_gtFine_labelIds.png', 35)
('frankfurt_000001_050149_leftImg8bit.png', 'frankfurt_000001_050149_gtFine_labelIds.png', 35)
('frankfurt_000001_050686_leftImg8bit.png', 'frankfurt_000001_050686_gtFine_labelIds.png', 35)
('frankfurt_000001_051516_leftImg8bit.png', 'frankfurt_000001_051516_gtFine_labelIds.png', 35)
('frankfurt_000001_051737_leftImg8bit.png', 'frankfurt_000001_051737_gtFine_labelIds.png', 35)
('frankfurt_000001_051807_leftImg8bit.png', 'frankfurt_000001_051807_gtFine_labelIds.png', 36)
('frankfurt_000001_052120_leftImg8bit.png', 'frankfurt_000001_052120_gtFine_labelIds.png', 36)
('frankfurt_000001_052594_leftImg8bit.png', 'frankfurt_000001_052594_gtFine_labelIds.png', 36)
('frankfurt_000001_053102_leftImg8bit.png', 'frankfurt_000001_053102_gtFine_labelIds.png', 36)
('frankfurt_000001_054077_leftImg8bit.png', 'frankfurt_000001_054077_gtFine_labelIds.png', 36)
('frankfurt_000001_054219_leftImg8bit.png', 'frankfurt_000001_054219_gtFine_labelIds.png', 37)
('frankfurt_000001_054415_leftImg8bit.png', 'frankfurt_000001_054415_gtFine_labelIds.png', 37)
('frankfurt_000001_054640_leftImg8bit.png', 'frankfurt_000001_054640_gtFine_labelIds.png', 37)
('frankfurt_000001_054884_leftImg8bit.png', 'frankfurt_000001_054884_gtFine_labelIds.png', 37)
('frankfurt_000001_055062_leftImg8bit.png', 'frankfurt_000001_055062_gtFine_labelIds.png', 37)
('frankfurt_000001_055172_leftImg8bit.png', 'frankfurt_000001_055172_gtFine_labelIds.png', 38)
('frankfurt_000001_055306_leftImg8bit.png', 'frankfurt_000001_055306_gtFine_labelIds.png', 38)
('frankfurt_000001_055387_leftImg8bit.png', 'frankfurt_000001_055387_gtFine_labelIds.png', 38)
('frankfurt_000001_055538_leftImg8bit.png', 'frankfurt_000001_055538_gtFine_labelIds.png', 38)
('frankfurt_000001_055603_leftImg8bit.png', 'frankfurt_000001_055603_gtFine_labelIds.png', 38)
('frankfurt_000001_055709_leftImg8bit.png', 'frankfurt_000001_055709_gtFine_labelIds.png', 39)
('frankfurt_000001_056580_leftImg8bit.png', 'frankfurt_000001_056580_gtFine_labelIds.png', 39)
('frankfurt_000001_057181_leftImg8bit.png', 'frankfurt_000001_057181_gtFine_labelIds.png', 39)
('frankfurt_000001_057478_leftImg8bit.png', 'frankfurt_000001_057478_gtFine_labelIds.png', 39)
('frankfurt_000001_057954_leftImg8bit.png', 'frankfurt_000001_057954_gtFine_labelIds.png', 39)
pixel_accuracy=0.970072399923, mean_iou=0.836577431454, iou=[ 0.95109814  0.96016518  0.77522614  0.60331524  0.89308245]
('frankfurt_000001_058057_leftImg8bit.png', 'frankfurt_000001_058057_gtFine_labelIds.png', 40)
('frankfurt_000001_058176_leftImg8bit.png', 'frankfurt_000001_058176_gtFine_labelIds.png', 40)
('frankfurt_000001_058504_leftImg8bit.png', 'frankfurt_000001_058504_gtFine_labelIds.png', 40)
('frankfurt_000001_058914_leftImg8bit.png', 'frankfurt_000001_058914_gtFine_labelIds.png', 40)
('frankfurt_000001_059119_leftImg8bit.png', 'frankfurt_000001_059119_gtFine_labelIds.png', 40)
('frankfurt_000001_059642_leftImg8bit.png', 'frankfurt_000001_059642_gtFine_labelIds.png', 41)
('frankfurt_000001_059789_leftImg8bit.png', 'frankfurt_000001_059789_gtFine_labelIds.png', 41)
('frankfurt_000001_060135_leftImg8bit.png', 'frankfurt_000001_060135_gtFine_labelIds.png', 41)
('frankfurt_000001_060422_leftImg8bit.png', 'frankfurt_000001_060422_gtFine_labelIds.png', 41)
('frankfurt_000001_060545_leftImg8bit.png', 'frankfurt_000001_060545_gtFine_labelIds.png', 41)
('frankfurt_000001_060906_leftImg8bit.png', 'frankfurt_000001_060906_gtFine_labelIds.png', 42)
('frankfurt_000001_061682_leftImg8bit.png', 'frankfurt_000001_061682_gtFine_labelIds.png', 42)
('frankfurt_000001_061763_leftImg8bit.png', 'frankfurt_000001_061763_gtFine_labelIds.png', 42)
('frankfurt_000001_062016_leftImg8bit.png', 'frankfurt_000001_062016_gtFine_labelIds.png', 42)
('frankfurt_000001_062250_leftImg8bit.png', 'frankfurt_000001_062250_gtFine_labelIds.png', 42)
('frankfurt_000001_062396_leftImg8bit.png', 'frankfurt_000001_062396_gtFine_labelIds.png', 43)
('frankfurt_000001_062509_leftImg8bit.png', 'frankfurt_000001_062509_gtFine_labelIds.png', 43)
('frankfurt_000001_062653_leftImg8bit.png', 'frankfurt_000001_062653_gtFine_labelIds.png', 43)
('frankfurt_000001_062793_leftImg8bit.png', 'frankfurt_000001_062793_gtFine_labelIds.png', 43)
('frankfurt_000001_063045_leftImg8bit.png', 'frankfurt_000001_063045_gtFine_labelIds.png', 43)
('frankfurt_000001_064130_leftImg8bit.png', 'frankfurt_000001_064130_gtFine_labelIds.png', 44)
('frankfurt_000001_064305_leftImg8bit.png', 'frankfurt_000001_064305_gtFine_labelIds.png', 44)
('frankfurt_000001_064651_leftImg8bit.png', 'frankfurt_000001_064651_gtFine_labelIds.png', 44)
('frankfurt_000001_064798_leftImg8bit.png', 'frankfurt_000001_064798_gtFine_labelIds.png', 44)
('frankfurt_000001_064925_leftImg8bit.png', 'frankfurt_000001_064925_gtFine_labelIds.png', 44)
pixel_accuracy=0.970829195916, mean_iou=0.836909934376, iou=[ 0.95236795  0.96181186  0.77214726  0.60211117  0.89611143]
('frankfurt_000001_065160_leftImg8bit.png', 'frankfurt_000001_065160_gtFine_labelIds.png', 45)
('frankfurt_000001_065617_leftImg8bit.png', 'frankfurt_000001_065617_gtFine_labelIds.png', 45)
('frankfurt_000001_065850_leftImg8bit.png', 'frankfurt_000001_065850_gtFine_labelIds.png', 45)
('frankfurt_000001_066092_leftImg8bit.png', 'frankfurt_000001_066092_gtFine_labelIds.png', 45)
('frankfurt_000001_066438_leftImg8bit.png', 'frankfurt_000001_066438_gtFine_labelIds.png', 45)
('frankfurt_000001_066574_leftImg8bit.png', 'frankfurt_000001_066574_gtFine_labelIds.png', 46)
('frankfurt_000001_066832_leftImg8bit.png', 'frankfurt_000001_066832_gtFine_labelIds.png', 46)
('frankfurt_000001_067092_leftImg8bit.png', 'frankfurt_000001_067092_gtFine_labelIds.png', 46)
('frankfurt_000001_067178_leftImg8bit.png', 'frankfurt_000001_067178_gtFine_labelIds.png', 46)
('frankfurt_000001_067295_leftImg8bit.png', 'frankfurt_000001_067295_gtFine_labelIds.png', 46)
('frankfurt_000001_067474_leftImg8bit.png', 'frankfurt_000001_067474_gtFine_labelIds.png', 47)
('frankfurt_000001_067735_leftImg8bit.png', 'frankfurt_000001_067735_gtFine_labelIds.png', 47)
('frankfurt_000001_068063_leftImg8bit.png', 'frankfurt_000001_068063_gtFine_labelIds.png', 47)
('frankfurt_000001_068208_leftImg8bit.png', 'frankfurt_000001_068208_gtFine_labelIds.png', 47)
('frankfurt_000001_068682_leftImg8bit.png', 'frankfurt_000001_068682_gtFine_labelIds.png', 47)
('frankfurt_000001_068772_leftImg8bit.png', 'frankfurt_000001_068772_gtFine_labelIds.png', 48)
('frankfurt_000001_069633_leftImg8bit.png', 'frankfurt_000001_069633_gtFine_labelIds.png', 48)
('frankfurt_000001_070099_leftImg8bit.png', 'frankfurt_000001_070099_gtFine_labelIds.png', 48)
('frankfurt_000001_071288_leftImg8bit.png', 'frankfurt_000001_071288_gtFine_labelIds.png', 48)
('frankfurt_000001_071781_leftImg8bit.png', 'frankfurt_000001_071781_gtFine_labelIds.png', 48)
('frankfurt_000001_072155_leftImg8bit.png', 'frankfurt_000001_072155_gtFine_labelIds.png', 49)
('frankfurt_000001_072295_leftImg8bit.png', 'frankfurt_000001_072295_gtFine_labelIds.png', 49)
('frankfurt_000001_073088_leftImg8bit.png', 'frankfurt_000001_073088_gtFine_labelIds.png', 49)
('frankfurt_000001_073243_leftImg8bit.png', 'frankfurt_000001_073243_gtFine_labelIds.png', 49)
('frankfurt_000001_073464_leftImg8bit.png', 'frankfurt_000001_073464_gtFine_labelIds.png', 49)
pixel_accuracy=0.970090030393, mean_iou=0.834795998338, iou=[ 0.95095104  0.96003117  0.76996983  0.59613745  0.8968905 ]
('frankfurt_000001_073911_leftImg8bit.png', 'frankfurt_000001_073911_gtFine_labelIds.png', 50)
('frankfurt_000001_075296_leftImg8bit.png', 'frankfurt_000001_075296_gtFine_labelIds.png', 50)
('frankfurt_000001_075984_leftImg8bit.png', 'frankfurt_000001_075984_gtFine_labelIds.png', 50)
('frankfurt_000001_076502_leftImg8bit.png', 'frankfurt_000001_076502_gtFine_labelIds.png', 50)
('frankfurt_000001_077092_leftImg8bit.png', 'frankfurt_000001_077092_gtFine_labelIds.png', 50)
('frankfurt_000001_077233_leftImg8bit.png', 'frankfurt_000001_077233_gtFine_labelIds.png', 51)
('frankfurt_000001_077434_leftImg8bit.png', 'frankfurt_000001_077434_gtFine_labelIds.png', 51)
('frankfurt_000001_078803_leftImg8bit.png', 'frankfurt_000001_078803_gtFine_labelIds.png', 51)
('frankfurt_000001_079206_leftImg8bit.png', 'frankfurt_000001_079206_gtFine_labelIds.png', 51)
('frankfurt_000001_080091_leftImg8bit.png', 'frankfurt_000001_080091_gtFine_labelIds.png', 51)
('frankfurt_000001_080391_leftImg8bit.png', 'frankfurt_000001_080391_gtFine_labelIds.png', 52)
('frankfurt_000001_080830_leftImg8bit.png', 'frankfurt_000001_080830_gtFine_labelIds.png', 52)
('frankfurt_000001_082087_leftImg8bit.png', 'frankfurt_000001_082087_gtFine_labelIds.png', 52)
('frankfurt_000001_082466_leftImg8bit.png', 'frankfurt_000001_082466_gtFine_labelIds.png', 52)
('frankfurt_000001_083029_leftImg8bit.png', 'frankfurt_000001_083029_gtFine_labelIds.png', 52)
('frankfurt_000001_083199_leftImg8bit.png', 'frankfurt_000001_083199_gtFine_labelIds.png', 53)
('frankfurt_000001_083852_leftImg8bit.png', 'frankfurt_000001_083852_gtFine_labelIds.png', 53)
('lindau_000000_000019_leftImg8bit.png', 'lindau_000000_000019_gtFine_labelIds.png', 53)
('lindau_000001_000019_leftImg8bit.png', 'lindau_000001_000019_gtFine_labelIds.png', 53)
('lindau_000002_000019_leftImg8bit.png', 'lindau_000002_000019_gtFine_labelIds.png', 53)
('lindau_000003_000019_leftImg8bit.png', 'lindau_000003_000019_gtFine_labelIds.png', 54)
('lindau_000004_000019_leftImg8bit.png', 'lindau_000004_000019_gtFine_labelIds.png', 54)
('lindau_000005_000019_leftImg8bit.png', 'lindau_000005_000019_gtFine_labelIds.png', 54)
('lindau_000006_000019_leftImg8bit.png', 'lindau_000006_000019_gtFine_labelIds.png', 54)
('lindau_000007_000019_leftImg8bit.png', 'lindau_000007_000019_gtFine_labelIds.png', 54)
pixel_accuracy=0.970688072964, mean_iou=0.83419376851, iou=[ 0.9519074   0.96142499  0.76600342  0.59530888  0.89632416]
('lindau_000008_000019_leftImg8bit.png', 'lindau_000008_000019_gtFine_labelIds.png', 55)
('lindau_000009_000019_leftImg8bit.png', 'lindau_000009_000019_gtFine_labelIds.png', 55)
('lindau_000010_000019_leftImg8bit.png', 'lindau_000010_000019_gtFine_labelIds.png', 55)
('lindau_000011_000019_leftImg8bit.png', 'lindau_000011_000019_gtFine_labelIds.png', 55)
('lindau_000012_000019_leftImg8bit.png', 'lindau_000012_000019_gtFine_labelIds.png', 55)
('lindau_000013_000019_leftImg8bit.png', 'lindau_000013_000019_gtFine_labelIds.png', 56)
('lindau_000014_000019_leftImg8bit.png', 'lindau_000014_000019_gtFine_labelIds.png', 56)
('lindau_000015_000019_leftImg8bit.png', 'lindau_000015_000019_gtFine_labelIds.png', 56)
('lindau_000016_000019_leftImg8bit.png', 'lindau_000016_000019_gtFine_labelIds.png', 56)
('lindau_000017_000019_leftImg8bit.png', 'lindau_000017_000019_gtFine_labelIds.png', 56)
('lindau_000018_000019_leftImg8bit.png', 'lindau_000018_000019_gtFine_labelIds.png', 57)
('lindau_000019_000019_leftImg8bit.png', 'lindau_000019_000019_gtFine_labelIds.png', 57)
('lindau_000020_000019_leftImg8bit.png', 'lindau_000020_000019_gtFine_labelIds.png', 57)
('lindau_000021_000019_leftImg8bit.png', 'lindau_000021_000019_gtFine_labelIds.png', 57)
('lindau_000022_000019_leftImg8bit.png', 'lindau_000022_000019_gtFine_labelIds.png', 57)
('lindau_000023_000019_leftImg8bit.png', 'lindau_000023_000019_gtFine_labelIds.png', 58)
('lindau_000024_000019_leftImg8bit.png', 'lindau_000024_000019_gtFine_labelIds.png', 58)
('lindau_000025_000019_leftImg8bit.png', 'lindau_000025_000019_gtFine_labelIds.png', 58)
('lindau_000026_000019_leftImg8bit.png', 'lindau_000026_000019_gtFine_labelIds.png', 58)
('lindau_000027_000019_leftImg8bit.png', 'lindau_000027_000019_gtFine_labelIds.png', 58)
('lindau_000028_000019_leftImg8bit.png', 'lindau_000028_000019_gtFine_labelIds.png', 59)
('lindau_000029_000019_leftImg8bit.png', 'lindau_000029_000019_gtFine_labelIds.png', 59)
('lindau_000030_000019_leftImg8bit.png', 'lindau_000030_000019_gtFine_labelIds.png', 59)
('lindau_000031_000019_leftImg8bit.png', 'lindau_000031_000019_gtFine_labelIds.png', 59)
('lindau_000032_000019_leftImg8bit.png', 'lindau_000032_000019_gtFine_labelIds.png', 59)
pixel_accuracy=0.966411284777, mean_iou=0.829865895596, iou=[ 0.94531399  0.94781238  0.76353566  0.59883879  0.89382866]
('lindau_000033_000019_leftImg8bit.png', 'lindau_000033_000019_gtFine_labelIds.png', 60)
('lindau_000034_000019_leftImg8bit.png', 'lindau_000034_000019_gtFine_labelIds.png', 60)
('lindau_000035_000019_leftImg8bit.png', 'lindau_000035_000019_gtFine_labelIds.png', 60)
('lindau_000036_000019_leftImg8bit.png', 'lindau_000036_000019_gtFine_labelIds.png', 60)
('lindau_000037_000019_leftImg8bit.png', 'lindau_000037_000019_gtFine_labelIds.png', 60)
('lindau_000038_000019_leftImg8bit.png', 'lindau_000038_000019_gtFine_labelIds.png', 61)
('lindau_000039_000019_leftImg8bit.png', 'lindau_000039_000019_gtFine_labelIds.png', 61)
('lindau_000040_000019_leftImg8bit.png', 'lindau_000040_000019_gtFine_labelIds.png', 61)
('lindau_000041_000019_leftImg8bit.png', 'lindau_000041_000019_gtFine_labelIds.png', 61)
('lindau_000042_000019_leftImg8bit.png', 'lindau_000042_000019_gtFine_labelIds.png', 61)
('lindau_000043_000019_leftImg8bit.png', 'lindau_000043_000019_gtFine_labelIds.png', 62)
('lindau_000044_000019_leftImg8bit.png', 'lindau_000044_000019_gtFine_labelIds.png', 62)
('lindau_000045_000019_leftImg8bit.png', 'lindau_000045_000019_gtFine_labelIds.png', 62)
('lindau_000046_000019_leftImg8bit.png', 'lindau_000046_000019_gtFine_labelIds.png', 62)
('lindau_000047_000019_leftImg8bit.png', 'lindau_000047_000019_gtFine_labelIds.png', 62)
('lindau_000048_000019_leftImg8bit.png', 'lindau_000048_000019_gtFine_labelIds.png', 63)
('lindau_000049_000019_leftImg8bit.png', 'lindau_000049_000019_gtFine_labelIds.png', 63)
('lindau_000050_000019_leftImg8bit.png', 'lindau_000050_000019_gtFine_labelIds.png', 63)
('lindau_000051_000019_leftImg8bit.png', 'lindau_000051_000019_gtFine_labelIds.png', 63)
('lindau_000052_000019_leftImg8bit.png', 'lindau_000052_000019_gtFine_labelIds.png', 63)
('lindau_000053_000019_leftImg8bit.png', 'lindau_000053_000019_gtFine_labelIds.png', 64)
('lindau_000054_000019_leftImg8bit.png', 'lindau_000054_000019_gtFine_labelIds.png', 64)
('lindau_000055_000019_leftImg8bit.png', 'lindau_000055_000019_gtFine_labelIds.png', 64)
('lindau_000056_000019_leftImg8bit.png', 'lindau_000056_000019_gtFine_labelIds.png', 64)
('lindau_000057_000019_leftImg8bit.png', 'lindau_000057_000019_gtFine_labelIds.png', 64)
pixel_accuracy=0.956796906605, mean_iou=0.820687013607, iou=[ 0.93037939  0.91851504  0.76018755  0.59997672  0.89437636]
('lindau_000058_000019_leftImg8bit.png', 'lindau_000058_000019_gtFine_labelIds.png', 65)
('munster_000000_000019_leftImg8bit.png', 'munster_000000_000019_gtFine_labelIds.png', 65)
('munster_000001_000019_leftImg8bit.png', 'munster_000001_000019_gtFine_labelIds.png', 65)
('munster_000002_000019_leftImg8bit.png', 'munster_000002_000019_gtFine_labelIds.png', 65)
('munster_000003_000019_leftImg8bit.png', 'munster_000003_000019_gtFine_labelIds.png', 65)
('munster_000004_000019_leftImg8bit.png', 'munster_000004_000019_gtFine_labelIds.png', 66)
('munster_000005_000019_leftImg8bit.png', 'munster_000005_000019_gtFine_labelIds.png', 66)
('munster_000006_000019_leftImg8bit.png', 'munster_000006_000019_gtFine_labelIds.png', 66)
('munster_000007_000019_leftImg8bit.png', 'munster_000007_000019_gtFine_labelIds.png', 66)
('munster_000008_000019_leftImg8bit.png', 'munster_000008_000019_gtFine_labelIds.png', 66)
('munster_000009_000019_leftImg8bit.png', 'munster_000009_000019_gtFine_labelIds.png', 67)
('munster_000010_000019_leftImg8bit.png', 'munster_000010_000019_gtFine_labelIds.png', 67)
('munster_000011_000019_leftImg8bit.png', 'munster_000011_000019_gtFine_labelIds.png', 67)
('munster_000012_000019_leftImg8bit.png', 'munster_000012_000019_gtFine_labelIds.png', 67)
('munster_000013_000019_leftImg8bit.png', 'munster_000013_000019_gtFine_labelIds.png', 67)
('munster_000014_000019_leftImg8bit.png', 'munster_000014_000019_gtFine_labelIds.png', 68)
('munster_000015_000019_leftImg8bit.png', 'munster_000015_000019_gtFine_labelIds.png', 68)
('munster_000016_000019_leftImg8bit.png', 'munster_000016_000019_gtFine_labelIds.png', 68)
('munster_000017_000019_leftImg8bit.png', 'munster_000017_000019_gtFine_labelIds.png', 68)
('munster_000018_000019_leftImg8bit.png', 'munster_000018_000019_gtFine_labelIds.png', 68)
('munster_000019_000019_leftImg8bit.png', 'munster_000019_000019_gtFine_labelIds.png', 69)
('munster_000020_000019_leftImg8bit.png', 'munster_000020_000019_gtFine_labelIds.png', 69)
('munster_000021_000019_leftImg8bit.png', 'munster_000021_000019_gtFine_labelIds.png', 69)
('munster_000022_000019_leftImg8bit.png', 'munster_000022_000019_gtFine_labelIds.png', 69)
('munster_000023_000019_leftImg8bit.png', 'munster_000023_000019_gtFine_labelIds.png', 69)
pixel_accuracy=0.958541529725, mean_iou=0.823647375804, iou=[ 0.9330663   0.92254256  0.75990567  0.60525262  0.89746974]
('munster_000024_000019_leftImg8bit.png', 'munster_000024_000019_gtFine_labelIds.png', 70)
('munster_000025_000019_leftImg8bit.png', 'munster_000025_000019_gtFine_labelIds.png', 70)
('munster_000026_000019_leftImg8bit.png', 'munster_000026_000019_gtFine_labelIds.png', 70)
('munster_000027_000019_leftImg8bit.png', 'munster_000027_000019_gtFine_labelIds.png', 70)
('munster_000028_000019_leftImg8bit.png', 'munster_000028_000019_gtFine_labelIds.png', 70)
('munster_000029_000019_leftImg8bit.png', 'munster_000029_000019_gtFine_labelIds.png', 71)
('munster_000030_000019_leftImg8bit.png', 'munster_000030_000019_gtFine_labelIds.png', 71)
('munster_000031_000019_leftImg8bit.png', 'munster_000031_000019_gtFine_labelIds.png', 71)
('munster_000032_000019_leftImg8bit.png', 'munster_000032_000019_gtFine_labelIds.png', 71)
('munster_000033_000019_leftImg8bit.png', 'munster_000033_000019_gtFine_labelIds.png', 71)
('munster_000034_000019_leftImg8bit.png', 'munster_000034_000019_gtFine_labelIds.png', 72)
('munster_000035_000019_leftImg8bit.png', 'munster_000035_000019_gtFine_labelIds.png', 72)
('munster_000036_000019_leftImg8bit.png', 'munster_000036_000019_gtFine_labelIds.png', 72)
('munster_000037_000019_leftImg8bit.png', 'munster_000037_000019_gtFine_labelIds.png', 72)
('munster_000038_000019_leftImg8bit.png', 'munster_000038_000019_gtFine_labelIds.png', 72)
('munster_000039_000019_leftImg8bit.png', 'munster_000039_000019_gtFine_labelIds.png', 73)
('munster_000040_000019_leftImg8bit.png', 'munster_000040_000019_gtFine_labelIds.png', 73)
('munster_000041_000019_leftImg8bit.png', 'munster_000041_000019_gtFine_labelIds.png', 73)
('munster_000042_000019_leftImg8bit.png', 'munster_000042_000019_gtFine_labelIds.png', 73)
('munster_000043_000019_leftImg8bit.png', 'munster_000043_000019_gtFine_labelIds.png', 73)
('munster_000044_000019_leftImg8bit.png', 'munster_000044_000019_gtFine_labelIds.png', 74)
('munster_000045_000019_leftImg8bit.png', 'munster_000045_000019_gtFine_labelIds.png', 74)
('munster_000046_000019_leftImg8bit.png', 'munster_000046_000019_gtFine_labelIds.png', 74)
('munster_000047_000019_leftImg8bit.png', 'munster_000047_000019_gtFine_labelIds.png', 74)
('munster_000048_000019_leftImg8bit.png', 'munster_000048_000019_gtFine_labelIds.png', 74)
pixel_accuracy=0.959750649819, mean_iou=0.827369767594, iou=[ 0.93499136  0.92570648  0.76819407  0.61111176  0.89684516]
('munster_000049_000019_leftImg8bit.png', 'munster_000049_000019_gtFine_labelIds.png', 75)
('munster_000050_000019_leftImg8bit.png', 'munster_000050_000019_gtFine_labelIds.png', 75)
('munster_000051_000019_leftImg8bit.png', 'munster_000051_000019_gtFine_labelIds.png', 75)
('munster_000052_000019_leftImg8bit.png', 'munster_000052_000019_gtFine_labelIds.png', 75)
('munster_000053_000019_leftImg8bit.png', 'munster_000053_000019_gtFine_labelIds.png', 75)
('munster_000054_000019_leftImg8bit.png', 'munster_000054_000019_gtFine_labelIds.png', 76)
('munster_000055_000019_leftImg8bit.png', 'munster_000055_000019_gtFine_labelIds.png', 76)
('munster_000056_000019_leftImg8bit.png', 'munster_000056_000019_gtFine_labelIds.png', 76)
('munster_000057_000019_leftImg8bit.png', 'munster_000057_000019_gtFine_labelIds.png', 76)
('munster_000058_000019_leftImg8bit.png', 'munster_000058_000019_gtFine_labelIds.png', 76)
('munster_000059_000019_leftImg8bit.png', 'munster_000059_000019_gtFine_labelIds.png', 77)
('munster_000060_000019_leftImg8bit.png', 'munster_000060_000019_gtFine_labelIds.png', 77)
('munster_000061_000019_leftImg8bit.png', 'munster_000061_000019_gtFine_labelIds.png', 77)
('munster_000062_000019_leftImg8bit.png', 'munster_000062_000019_gtFine_labelIds.png', 77)
('munster_000063_000019_leftImg8bit.png', 'munster_000063_000019_gtFine_labelIds.png', 77)
('munster_000064_000019_leftImg8bit.png', 'munster_000064_000019_gtFine_labelIds.png', 78)
('munster_000065_000019_leftImg8bit.png', 'munster_000065_000019_gtFine_labelIds.png', 78)
('munster_000066_000019_leftImg8bit.png', 'munster_000066_000019_gtFine_labelIds.png', 78)
('munster_000067_000019_leftImg8bit.png', 'munster_000067_000019_gtFine_labelIds.png', 78)
('munster_000068_000019_leftImg8bit.png', 'munster_000068_000019_gtFine_labelIds.png', 78)
('munster_000069_000019_leftImg8bit.png', 'munster_000069_000019_gtFine_labelIds.png', 79)
('munster_000070_000019_leftImg8bit.png', 'munster_000070_000019_gtFine_labelIds.png', 79)
('munster_000071_000019_leftImg8bit.png', 'munster_000071_000019_gtFine_labelIds.png', 79)
('munster_000072_000019_leftImg8bit.png', 'munster_000072_000019_gtFine_labelIds.png', 79)
('munster_000073_000019_leftImg8bit.png', 'munster_000073_000019_gtFine_labelIds.png', 79)
pixel_accuracy=0.960953154021, mean_iou=0.829570079657, iou=[ 0.93684065  0.92900417  0.76911842  0.61402358  0.89886357]
('munster_000074_000019_leftImg8bit.png', 'munster_000074_000019_gtFine_labelIds.png', 80)
('munster_000075_000019_leftImg8bit.png', 'munster_000075_000019_gtFine_labelIds.png', 80)
('munster_000076_000019_leftImg8bit.png', 'munster_000076_000019_gtFine_labelIds.png', 80)
('munster_000077_000019_leftImg8bit.png', 'munster_000077_000019_gtFine_labelIds.png', 80)
('munster_000078_000019_leftImg8bit.png', 'munster_000078_000019_gtFine_labelIds.png', 80)
('munster_000079_000019_leftImg8bit.png', 'munster_000079_000019_gtFine_labelIds.png', 81)
('munster_000080_000019_leftImg8bit.png', 'munster_000080_000019_gtFine_labelIds.png', 81)
('munster_000081_000019_leftImg8bit.png', 'munster_000081_000019_gtFine_labelIds.png', 81)
('munster_000082_000019_leftImg8bit.png', 'munster_000082_000019_gtFine_labelIds.png', 81)
('munster_000083_000019_leftImg8bit.png', 'munster_000083_000019_gtFine_labelIds.png', 81)
('munster_000084_000019_leftImg8bit.png', 'munster_000084_000019_gtFine_labelIds.png', 82)
('munster_000085_000019_leftImg8bit.png', 'munster_000085_000019_gtFine_labelIds.png', 82)
('munster_000086_000019_leftImg8bit.png', 'munster_000086_000019_gtFine_labelIds.png', 82)
('munster_000087_000019_leftImg8bit.png', 'munster_000087_000019_gtFine_labelIds.png', 82)
('munster_000088_000019_leftImg8bit.png', 'munster_000088_000019_gtFine_labelIds.png', 82)
('munster_000089_000019_leftImg8bit.png', 'munster_000089_000019_gtFine_labelIds.png', 83)
('munster_000090_000019_leftImg8bit.png', 'munster_000090_000019_gtFine_labelIds.png', 83)
('munster_000091_000019_leftImg8bit.png', 'munster_000091_000019_gtFine_labelIds.png', 83)
('munster_000092_000019_leftImg8bit.png', 'munster_000092_000019_gtFine_labelIds.png', 83)
('munster_000093_000019_leftImg8bit.png', 'munster_000093_000019_gtFine_labelIds.png', 83)
('munster_000094_000019_leftImg8bit.png', 'munster_000094_000019_gtFine_labelIds.png', 84)
('munster_000095_000019_leftImg8bit.png', 'munster_000095_000019_gtFine_labelIds.png', 84)
('munster_000096_000019_leftImg8bit.png', 'munster_000096_000019_gtFine_labelIds.png', 84)
('munster_000097_000019_leftImg8bit.png', 'munster_000097_000019_gtFine_labelIds.png', 84)
('munster_000098_000019_leftImg8bit.png', 'munster_000098_000019_gtFine_labelIds.png', 84)
pixel_accuracy=0.96223073922, mean_iou=0.831926170326, iou=[ 0.93875248  0.93172754  0.76830677  0.61649376  0.9043503 ]
('munster_000099_000019_leftImg8bit.png', 'munster_000099_000019_gtFine_labelIds.png', 85)
('munster_000100_000019_leftImg8bit.png', 'munster_000100_000019_gtFine_labelIds.png', 85)
('munster_000101_000019_leftImg8bit.png', 'munster_000101_000019_gtFine_labelIds.png', 85)
('munster_000102_000019_leftImg8bit.png', 'munster_000102_000019_gtFine_labelIds.png', 85)
('munster_000103_000019_leftImg8bit.png', 'munster_000103_000019_gtFine_labelIds.png', 85)
('munster_000104_000019_leftImg8bit.png', 'munster_000104_000019_gtFine_labelIds.png', 86)
('munster_000105_000019_leftImg8bit.png', 'munster_000105_000019_gtFine_labelIds.png', 86)
('munster_000106_000019_leftImg8bit.png', 'munster_000106_000019_gtFine_labelIds.png', 86)
('munster_000107_000019_leftImg8bit.png', 'munster_000107_000019_gtFine_labelIds.png', 86)
('munster_000108_000019_leftImg8bit.png', 'munster_000108_000019_gtFine_labelIds.png', 86)
('munster_000109_000019_leftImg8bit.png', 'munster_000109_000019_gtFine_labelIds.png', 87)
('munster_000110_000019_leftImg8bit.png', 'munster_000110_000019_gtFine_labelIds.png', 87)
('munster_000111_000019_leftImg8bit.png', 'munster_000111_000019_gtFine_labelIds.png', 87)
('munster_000112_000019_leftImg8bit.png', 'munster_000112_000019_gtFine_labelIds.png', 87)
('munster_000113_000019_leftImg8bit.png', 'munster_000113_000019_gtFine_labelIds.png', 87)
('munster_000114_000019_leftImg8bit.png', 'munster_000114_000019_gtFine_labelIds.png', 88)
('munster_000115_000019_leftImg8bit.png', 'munster_000115_000019_gtFine_labelIds.png', 88)
('munster_000116_000019_leftImg8bit.png', 'munster_000116_000019_gtFine_labelIds.png', 88)
('munster_000117_000019_leftImg8bit.png', 'munster_000117_000019_gtFine_labelIds.png', 88)
('munster_000118_000019_leftImg8bit.png', 'munster_000118_000019_gtFine_labelIds.png', 88)
('munster_000119_000019_leftImg8bit.png', 'munster_000119_000019_gtFine_labelIds.png', 89)
('munster_000120_000019_leftImg8bit.png', 'munster_000120_000019_gtFine_labelIds.png', 89)
('munster_000121_000019_leftImg8bit.png', 'munster_000121_000019_gtFine_labelIds.png', 89)
('munster_000122_000019_leftImg8bit.png', 'munster_000122_000019_gtFine_labelIds.png', 89)
('munster_000123_000019_leftImg8bit.png', 'munster_000123_000019_gtFine_labelIds.png', 89)
pixel_accuracy=0.963611155642, mean_iou=0.832859802092, iou=[ 0.9409029   0.93473389  0.76774481  0.61393505  0.90698236]
('munster_000124_000019_leftImg8bit.png', 'munster_000124_000019_gtFine_labelIds.png', 90)
('munster_000125_000019_leftImg8bit.png', 'munster_000125_000019_gtFine_labelIds.png', 90)
('munster_000126_000019_leftImg8bit.png', 'munster_000126_000019_gtFine_labelIds.png', 90)
('munster_000127_000019_leftImg8bit.png', 'munster_000127_000019_gtFine_labelIds.png', 90)
('munster_000128_000019_leftImg8bit.png', 'munster_000128_000019_gtFine_labelIds.png', 90)
('munster_000129_000019_leftImg8bit.png', 'munster_000129_000019_gtFine_labelIds.png', 91)
('munster_000130_000019_leftImg8bit.png', 'munster_000130_000019_gtFine_labelIds.png', 91)
('munster_000131_000019_leftImg8bit.png', 'munster_000131_000019_gtFine_labelIds.png', 91)
('munster_000132_000019_leftImg8bit.png', 'munster_000132_000019_gtFine_labelIds.png', 91)
('munster_000133_000019_leftImg8bit.png', 'munster_000133_000019_gtFine_labelIds.png', 91)
('munster_000134_000019_leftImg8bit.png', 'munster_000134_000019_gtFine_labelIds.png', 92)
('munster_000135_000019_leftImg8bit.png', 'munster_000135_000019_gtFine_labelIds.png', 92)
('munster_000136_000019_leftImg8bit.png', 'munster_000136_000019_gtFine_labelIds.png', 92)
('munster_000137_000019_leftImg8bit.png', 'munster_000137_000019_gtFine_labelIds.png', 92)
('munster_000138_000019_leftImg8bit.png', 'munster_000138_000019_gtFine_labelIds.png', 92)
('munster_000139_000019_leftImg8bit.png', 'munster_000139_000019_gtFine_labelIds.png', 93)
('munster_000140_000019_leftImg8bit.png', 'munster_000140_000019_gtFine_labelIds.png', 93)
('munster_000141_000019_leftImg8bit.png', 'munster_000141_000019_gtFine_labelIds.png', 93)
('munster_000142_000019_leftImg8bit.png', 'munster_000142_000019_gtFine_labelIds.png', 93)
('munster_000143_000019_leftImg8bit.png', 'munster_000143_000019_gtFine_labelIds.png', 93)
('munster_000144_000019_leftImg8bit.png', 'munster_000144_000019_gtFine_labelIds.png', 94)
('munster_000145_000019_leftImg8bit.png', 'munster_000145_000019_gtFine_labelIds.png', 94)
('munster_000146_000019_leftImg8bit.png', 'munster_000146_000019_gtFine_labelIds.png', 94)
('munster_000147_000019_leftImg8bit.png', 'munster_000147_000019_gtFine_labelIds.png', 94)
('munster_000148_000019_leftImg8bit.png', 'munster_000148_000019_gtFine_labelIds.png', 94)
pixel_accuracy=0.960983104341, mean_iou=0.831626139192, iou=[ 0.93662951  0.92718749  0.7718439   0.61488053  0.90758926]
('munster_000149_000019_leftImg8bit.png', 'munster_000149_000019_gtFine_labelIds.png', 95)
('munster_000150_000019_leftImg8bit.png', 'munster_000150_000019_gtFine_labelIds.png', 95)
('munster_000151_000019_leftImg8bit.png', 'munster_000151_000019_gtFine_labelIds.png', 95)
('munster_000152_000019_leftImg8bit.png', 'munster_000152_000019_gtFine_labelIds.png', 95)
('munster_000153_000019_leftImg8bit.png', 'munster_000153_000019_gtFine_labelIds.png', 95)
('munster_000154_000019_leftImg8bit.png', 'munster_000154_000019_gtFine_labelIds.png', 96)
('munster_000155_000019_leftImg8bit.png', 'munster_000155_000019_gtFine_labelIds.png', 96)
('munster_000156_000019_leftImg8bit.png', 'munster_000156_000019_gtFine_labelIds.png', 96)
('munster_000157_000019_leftImg8bit.png', 'munster_000157_000019_gtFine_labelIds.png', 96)
('munster_000158_000019_leftImg8bit.png', 'munster_000158_000019_gtFine_labelIds.png', 96)
('munster_000159_000019_leftImg8bit.png', 'munster_000159_000019_gtFine_labelIds.png', 97)
('munster_000160_000019_leftImg8bit.png', 'munster_000160_000019_gtFine_labelIds.png', 97)
('munster_000161_000019_leftImg8bit.png', 'munster_000161_000019_gtFine_labelIds.png', 97)
('munster_000162_000019_leftImg8bit.png', 'munster_000162_000019_gtFine_labelIds.png', 97)
('munster_000163_000019_leftImg8bit.png', 'munster_000163_000019_gtFine_labelIds.png', 97)
('munster_000164_000019_leftImg8bit.png', 'munster_000164_000019_gtFine_labelIds.png', 98)
('munster_000165_000019_leftImg8bit.png', 'munster_000165_000019_gtFine_labelIds.png', 98)
('munster_000166_000019_leftImg8bit.png', 'munster_000166_000019_gtFine_labelIds.png', 98)
('munster_000167_000019_leftImg8bit.png', 'munster_000167_000019_gtFine_labelIds.png', 98)
('munster_000168_000019_leftImg8bit.png', 'munster_000168_000019_gtFine_labelIds.png', 98)
('munster_000169_000019_leftImg8bit.png', 'munster_000169_000019_gtFine_labelIds.png', 99)
('munster_000170_000019_leftImg8bit.png', 'munster_000170_000019_gtFine_labelIds.png', 99)
('munster_000171_000019_leftImg8bit.png', 'munster_000171_000019_gtFine_labelIds.png', 99)
('munster_000172_000019_leftImg8bit.png', 'munster_000172_000019_gtFine_labelIds.png', 99)
('munster_000173_000019_leftImg8bit.png', 'munster_000173_000019_gtFine_labelIds.png', 99)
pixel_accuracy=0.96181507313, mean_iou=0.833717946324, iou=[ 0.93801851  0.92912061  0.77197472  0.62122001  0.90825587]
-------------------------------------------------------------
Final: pixel_accuracy=0.96181507313, mean_iou=0.833717946324, iou=[ 0.93801851  0.92912061  0.77197472  0.62122001  0.90825587]
-------------------------------------------------------------
initial eval.
I0917 00:26:21.453339 21164 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0917 00:26:21.454044 21164 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I0917 00:26:21.454643 21164 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0917 00:26:21.455198 21164 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I0917 00:26:21.456856 21164 upgrade_proto.cpp:66] Attempting to upgrade input file specified using deprecated input fields: ../trained/image_segmentation/cityscapes5_jsegnet21v2/l1reg/deploy.prototxt
I0917 00:26:21.456869 21164 upgrade_proto.cpp:69] Successfully upgraded file specified using deprecated input fields.
W0917 00:26:21.456872 21164 upgrade_proto.cpp:71] Note that future Caffe releases will only support input layers and not input fields.
I0917 00:26:21.457137 21164 net.cpp:72] Initializing net from parameters: 
name: "jsegnet21v2_deploy"
state {
  phase: TEST
}
layer {
  name: "input"
  type: "Input"
  top: "data"
  input_param {
    shape {
      dim: 1
      dim: 3
      dim: 1024
      dim: 2048
    }
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "argMaxOut"
  type: "ArgMax"
  bottom: "out_deconv_final_up8"
  top: "argMaxOut"
  argmax_param {
    axis: 1
  }
}
I0917 00:26:21.457252 21164 net.cpp:104] Using FLOAT as default forward math type
I0917 00:26:21.457257 21164 net.cpp:110] Using FLOAT as default backward math type
I0917 00:26:21.457259 21164 layer_factory.hpp:136] Creating layer 'input' of type 'Input'
I0917 00:26:21.457262 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.457268 21164 net.cpp:184] Created Layer input (0)
I0917 00:26:21.457270 21164 net.cpp:530] input -> data
I0917 00:26:21.457883 21164 net.cpp:245] Setting up input
I0917 00:26:21.457893 21164 net.cpp:252] TEST Top shape for layer 0 'input' 1 3 1024 2048 (6291456)
I0917 00:26:21.457897 21164 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0917 00:26:21.457906 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.457916 21164 net.cpp:184] Created Layer data/bias (1)
I0917 00:26:21.457917 21164 net.cpp:561] data/bias <- data
I0917 00:26:21.457921 21164 net.cpp:530] data/bias -> data/bias
I0917 00:26:21.462204 21164 net.cpp:245] Setting up data/bias
I0917 00:26:21.462221 21164 net.cpp:252] TEST Top shape for layer 1 'data/bias' 1 3 1024 2048 (6291456)
I0917 00:26:21.462229 21164 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0917 00:26:21.462234 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.462251 21164 net.cpp:184] Created Layer conv1a (2)
I0917 00:26:21.462255 21164 net.cpp:561] conv1a <- data/bias
I0917 00:26:21.462257 21164 net.cpp:530] conv1a -> conv1a
I0917 00:26:21.753191 21164 net.cpp:245] Setting up conv1a
I0917 00:26:21.753213 21164 net.cpp:252] TEST Top shape for layer 2 'conv1a' 1 32 512 1024 (16777216)
I0917 00:26:21.753224 21164 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0917 00:26:21.753228 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.753237 21164 net.cpp:184] Created Layer conv1a/bn (3)
I0917 00:26:21.753240 21164 net.cpp:561] conv1a/bn <- conv1a
I0917 00:26:21.753243 21164 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0917 00:26:21.754063 21164 net.cpp:245] Setting up conv1a/bn
I0917 00:26:21.754078 21164 net.cpp:252] TEST Top shape for layer 3 'conv1a/bn' 1 32 512 1024 (16777216)
I0917 00:26:21.754089 21164 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0917 00:26:21.754093 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.754098 21164 net.cpp:184] Created Layer conv1a/relu (4)
I0917 00:26:21.754101 21164 net.cpp:561] conv1a/relu <- conv1a
I0917 00:26:21.754103 21164 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0917 00:26:21.754112 21164 net.cpp:245] Setting up conv1a/relu
I0917 00:26:21.754115 21164 net.cpp:252] TEST Top shape for layer 4 'conv1a/relu' 1 32 512 1024 (16777216)
I0917 00:26:21.754117 21164 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0917 00:26:21.754119 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.754130 21164 net.cpp:184] Created Layer conv1b (5)
I0917 00:26:21.754133 21164 net.cpp:561] conv1b <- conv1a
I0917 00:26:21.754135 21164 net.cpp:530] conv1b -> conv1b
I0917 00:26:21.755652 21164 net.cpp:245] Setting up conv1b
I0917 00:26:21.755662 21164 net.cpp:252] TEST Top shape for layer 5 'conv1b' 1 32 512 1024 (16777216)
I0917 00:26:21.755668 21164 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0917 00:26:21.755671 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.755676 21164 net.cpp:184] Created Layer conv1b/bn (6)
I0917 00:26:21.755679 21164 net.cpp:561] conv1b/bn <- conv1b
I0917 00:26:21.755681 21164 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0917 00:26:21.756487 21164 net.cpp:245] Setting up conv1b/bn
I0917 00:26:21.756497 21164 net.cpp:252] TEST Top shape for layer 6 'conv1b/bn' 1 32 512 1024 (16777216)
I0917 00:26:21.756503 21164 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0917 00:26:21.756506 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.756510 21164 net.cpp:184] Created Layer conv1b/relu (7)
I0917 00:26:21.756511 21164 net.cpp:561] conv1b/relu <- conv1b
I0917 00:26:21.756515 21164 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0917 00:26:21.756518 21164 net.cpp:245] Setting up conv1b/relu
I0917 00:26:21.756520 21164 net.cpp:252] TEST Top shape for layer 7 'conv1b/relu' 1 32 512 1024 (16777216)
I0917 00:26:21.756522 21164 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0917 00:26:21.756534 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.756538 21164 net.cpp:184] Created Layer pool1 (8)
I0917 00:26:21.756541 21164 net.cpp:561] pool1 <- conv1b
I0917 00:26:21.756544 21164 net.cpp:530] pool1 -> pool1
I0917 00:26:21.756582 21164 net.cpp:245] Setting up pool1
I0917 00:26:21.756587 21164 net.cpp:252] TEST Top shape for layer 8 'pool1' 1 32 256 512 (4194304)
I0917 00:26:21.756588 21164 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0917 00:26:21.756592 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.756603 21164 net.cpp:184] Created Layer res2a_branch2a (9)
I0917 00:26:21.756605 21164 net.cpp:561] res2a_branch2a <- pool1
I0917 00:26:21.756608 21164 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0917 00:26:21.757750 21164 net.cpp:245] Setting up res2a_branch2a
I0917 00:26:21.757761 21164 net.cpp:252] TEST Top shape for layer 9 'res2a_branch2a' 1 64 256 512 (8388608)
I0917 00:26:21.757766 21164 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0917 00:26:21.757769 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.757774 21164 net.cpp:184] Created Layer res2a_branch2a/bn (10)
I0917 00:26:21.757776 21164 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0917 00:26:21.757778 21164 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0917 00:26:21.758191 21164 net.cpp:245] Setting up res2a_branch2a/bn
I0917 00:26:21.758198 21164 net.cpp:252] TEST Top shape for layer 10 'res2a_branch2a/bn' 1 64 256 512 (8388608)
I0917 00:26:21.758204 21164 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0917 00:26:21.758206 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.758209 21164 net.cpp:184] Created Layer res2a_branch2a/relu (11)
I0917 00:26:21.758211 21164 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0917 00:26:21.758213 21164 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0917 00:26:21.758218 21164 net.cpp:245] Setting up res2a_branch2a/relu
I0917 00:26:21.758220 21164 net.cpp:252] TEST Top shape for layer 11 'res2a_branch2a/relu' 1 64 256 512 (8388608)
I0917 00:26:21.758222 21164 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0917 00:26:21.758224 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.758230 21164 net.cpp:184] Created Layer res2a_branch2b (12)
I0917 00:26:21.758232 21164 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0917 00:26:21.758234 21164 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0917 00:26:21.759162 21164 net.cpp:245] Setting up res2a_branch2b
I0917 00:26:21.759172 21164 net.cpp:252] TEST Top shape for layer 12 'res2a_branch2b' 1 64 256 512 (8388608)
I0917 00:26:21.759176 21164 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0917 00:26:21.759179 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.759183 21164 net.cpp:184] Created Layer res2a_branch2b/bn (13)
I0917 00:26:21.759186 21164 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0917 00:26:21.759188 21164 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0917 00:26:21.759961 21164 net.cpp:245] Setting up res2a_branch2b/bn
I0917 00:26:21.759969 21164 net.cpp:252] TEST Top shape for layer 13 'res2a_branch2b/bn' 1 64 256 512 (8388608)
I0917 00:26:21.759975 21164 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0917 00:26:21.759977 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.759980 21164 net.cpp:184] Created Layer res2a_branch2b/relu (14)
I0917 00:26:21.759984 21164 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0917 00:26:21.759985 21164 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0917 00:26:21.759997 21164 net.cpp:245] Setting up res2a_branch2b/relu
I0917 00:26:21.760000 21164 net.cpp:252] TEST Top shape for layer 14 'res2a_branch2b/relu' 1 64 256 512 (8388608)
I0917 00:26:21.760002 21164 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0917 00:26:21.760004 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.760009 21164 net.cpp:184] Created Layer pool2 (15)
I0917 00:26:21.760010 21164 net.cpp:561] pool2 <- res2a_branch2b
I0917 00:26:21.760011 21164 net.cpp:530] pool2 -> pool2
I0917 00:26:21.760041 21164 net.cpp:245] Setting up pool2
I0917 00:26:21.760046 21164 net.cpp:252] TEST Top shape for layer 15 'pool2' 1 64 128 256 (2097152)
I0917 00:26:21.760048 21164 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0917 00:26:21.760051 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.760056 21164 net.cpp:184] Created Layer res3a_branch2a (16)
I0917 00:26:21.760058 21164 net.cpp:561] res3a_branch2a <- pool2
I0917 00:26:21.760061 21164 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0917 00:26:21.762595 21164 net.cpp:245] Setting up res3a_branch2a
I0917 00:26:21.762605 21164 net.cpp:252] TEST Top shape for layer 16 'res3a_branch2a' 1 128 128 256 (4194304)
I0917 00:26:21.762610 21164 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0917 00:26:21.762612 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.762619 21164 net.cpp:184] Created Layer res3a_branch2a/bn (17)
I0917 00:26:21.762620 21164 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0917 00:26:21.762622 21164 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0917 00:26:21.763013 21164 net.cpp:245] Setting up res3a_branch2a/bn
I0917 00:26:21.763020 21164 net.cpp:252] TEST Top shape for layer 17 'res3a_branch2a/bn' 1 128 128 256 (4194304)
I0917 00:26:21.763026 21164 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0917 00:26:21.763029 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.763032 21164 net.cpp:184] Created Layer res3a_branch2a/relu (18)
I0917 00:26:21.763034 21164 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0917 00:26:21.763036 21164 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0917 00:26:21.763039 21164 net.cpp:245] Setting up res3a_branch2a/relu
I0917 00:26:21.763042 21164 net.cpp:252] TEST Top shape for layer 18 'res3a_branch2a/relu' 1 128 128 256 (4194304)
I0917 00:26:21.763044 21164 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0917 00:26:21.763046 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.763051 21164 net.cpp:184] Created Layer res3a_branch2b (19)
I0917 00:26:21.763053 21164 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0917 00:26:21.763056 21164 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0917 00:26:21.763957 21164 net.cpp:245] Setting up res3a_branch2b
I0917 00:26:21.763963 21164 net.cpp:252] TEST Top shape for layer 19 'res3a_branch2b' 1 128 128 256 (4194304)
I0917 00:26:21.763967 21164 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0917 00:26:21.763970 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.763974 21164 net.cpp:184] Created Layer res3a_branch2b/bn (20)
I0917 00:26:21.763978 21164 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0917 00:26:21.763980 21164 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0917 00:26:21.764364 21164 net.cpp:245] Setting up res3a_branch2b/bn
I0917 00:26:21.764370 21164 net.cpp:252] TEST Top shape for layer 20 'res3a_branch2b/bn' 1 128 128 256 (4194304)
I0917 00:26:21.764376 21164 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0917 00:26:21.764379 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.764391 21164 net.cpp:184] Created Layer res3a_branch2b/relu (21)
I0917 00:26:21.764396 21164 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0917 00:26:21.764398 21164 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0917 00:26:21.764401 21164 net.cpp:245] Setting up res3a_branch2b/relu
I0917 00:26:21.764405 21164 net.cpp:252] TEST Top shape for layer 21 'res3a_branch2b/relu' 1 128 128 256 (4194304)
I0917 00:26:21.764407 21164 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0917 00:26:21.764410 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.764412 21164 net.cpp:184] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (22)
I0917 00:26:21.764415 21164 net.cpp:561] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0917 00:26:21.764416 21164 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0917 00:26:21.764420 21164 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0917 00:26:21.764441 21164 net.cpp:245] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0917 00:26:21.764446 21164 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 1 128 128 256 (4194304)
I0917 00:26:21.764447 21164 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 1 128 128 256 (4194304)
I0917 00:26:21.764449 21164 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0917 00:26:21.764451 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.764456 21164 net.cpp:184] Created Layer pool3 (23)
I0917 00:26:21.764457 21164 net.cpp:561] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0917 00:26:21.764459 21164 net.cpp:530] pool3 -> pool3
I0917 00:26:21.764487 21164 net.cpp:245] Setting up pool3
I0917 00:26:21.764492 21164 net.cpp:252] TEST Top shape for layer 23 'pool3' 1 128 64 128 (1048576)
I0917 00:26:21.764493 21164 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0917 00:26:21.764495 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.764500 21164 net.cpp:184] Created Layer res4a_branch2a (24)
I0917 00:26:21.764503 21164 net.cpp:561] res4a_branch2a <- pool3
I0917 00:26:21.764505 21164 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0917 00:26:21.770526 21164 net.cpp:245] Setting up res4a_branch2a
I0917 00:26:21.770539 21164 net.cpp:252] TEST Top shape for layer 24 'res4a_branch2a' 1 256 64 128 (2097152)
I0917 00:26:21.770545 21164 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0917 00:26:21.770547 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.770553 21164 net.cpp:184] Created Layer res4a_branch2a/bn (25)
I0917 00:26:21.770555 21164 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0917 00:26:21.770558 21164 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0917 00:26:21.770948 21164 net.cpp:245] Setting up res4a_branch2a/bn
I0917 00:26:21.770954 21164 net.cpp:252] TEST Top shape for layer 25 'res4a_branch2a/bn' 1 256 64 128 (2097152)
I0917 00:26:21.770961 21164 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0917 00:26:21.770962 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.770965 21164 net.cpp:184] Created Layer res4a_branch2a/relu (26)
I0917 00:26:21.770967 21164 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0917 00:26:21.770969 21164 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0917 00:26:21.770972 21164 net.cpp:245] Setting up res4a_branch2a/relu
I0917 00:26:21.770975 21164 net.cpp:252] TEST Top shape for layer 26 'res4a_branch2a/relu' 1 256 64 128 (2097152)
I0917 00:26:21.770977 21164 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0917 00:26:21.770987 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.770994 21164 net.cpp:184] Created Layer res4a_branch2b (27)
I0917 00:26:21.770997 21164 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0917 00:26:21.770998 21164 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0917 00:26:21.774085 21164 net.cpp:245] Setting up res4a_branch2b
I0917 00:26:21.774092 21164 net.cpp:252] TEST Top shape for layer 27 'res4a_branch2b' 1 256 64 128 (2097152)
I0917 00:26:21.774096 21164 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0917 00:26:21.774099 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.774103 21164 net.cpp:184] Created Layer res4a_branch2b/bn (28)
I0917 00:26:21.774106 21164 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0917 00:26:21.774108 21164 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0917 00:26:21.774495 21164 net.cpp:245] Setting up res4a_branch2b/bn
I0917 00:26:21.774502 21164 net.cpp:252] TEST Top shape for layer 28 'res4a_branch2b/bn' 1 256 64 128 (2097152)
I0917 00:26:21.774507 21164 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0917 00:26:21.774509 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.774513 21164 net.cpp:184] Created Layer res4a_branch2b/relu (29)
I0917 00:26:21.774514 21164 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0917 00:26:21.774516 21164 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0917 00:26:21.774520 21164 net.cpp:245] Setting up res4a_branch2b/relu
I0917 00:26:21.774523 21164 net.cpp:252] TEST Top shape for layer 29 'res4a_branch2b/relu' 1 256 64 128 (2097152)
I0917 00:26:21.774524 21164 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0917 00:26:21.774526 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.774530 21164 net.cpp:184] Created Layer pool4 (30)
I0917 00:26:21.774533 21164 net.cpp:561] pool4 <- res4a_branch2b
I0917 00:26:21.774534 21164 net.cpp:530] pool4 -> pool4
I0917 00:26:21.774566 21164 net.cpp:245] Setting up pool4
I0917 00:26:21.774570 21164 net.cpp:252] TEST Top shape for layer 30 'pool4' 1 256 64 128 (2097152)
I0917 00:26:21.774572 21164 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0917 00:26:21.774574 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.774580 21164 net.cpp:184] Created Layer res5a_branch2a (31)
I0917 00:26:21.774582 21164 net.cpp:561] res5a_branch2a <- pool4
I0917 00:26:21.774585 21164 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0917 00:26:21.799680 21164 net.cpp:245] Setting up res5a_branch2a
I0917 00:26:21.799700 21164 net.cpp:252] TEST Top shape for layer 31 'res5a_branch2a' 1 512 64 128 (4194304)
I0917 00:26:21.799706 21164 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0917 00:26:21.799710 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.799718 21164 net.cpp:184] Created Layer res5a_branch2a/bn (32)
I0917 00:26:21.799722 21164 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0917 00:26:21.799726 21164 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0917 00:26:21.800498 21164 net.cpp:245] Setting up res5a_branch2a/bn
I0917 00:26:21.800506 21164 net.cpp:252] TEST Top shape for layer 32 'res5a_branch2a/bn' 1 512 64 128 (4194304)
I0917 00:26:21.800513 21164 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0917 00:26:21.800515 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.800519 21164 net.cpp:184] Created Layer res5a_branch2a/relu (33)
I0917 00:26:21.800521 21164 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0917 00:26:21.800532 21164 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0917 00:26:21.800536 21164 net.cpp:245] Setting up res5a_branch2a/relu
I0917 00:26:21.800539 21164 net.cpp:252] TEST Top shape for layer 33 'res5a_branch2a/relu' 1 512 64 128 (4194304)
I0917 00:26:21.800540 21164 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0917 00:26:21.800544 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.800549 21164 net.cpp:184] Created Layer res5a_branch2b (34)
I0917 00:26:21.800552 21164 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0917 00:26:21.800555 21164 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0917 00:26:21.812968 21164 net.cpp:245] Setting up res5a_branch2b
I0917 00:26:21.812980 21164 net.cpp:252] TEST Top shape for layer 34 'res5a_branch2b' 1 512 64 128 (4194304)
I0917 00:26:21.812990 21164 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0917 00:26:21.812994 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.812999 21164 net.cpp:184] Created Layer res5a_branch2b/bn (35)
I0917 00:26:21.813002 21164 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0917 00:26:21.813005 21164 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0917 00:26:21.813422 21164 net.cpp:245] Setting up res5a_branch2b/bn
I0917 00:26:21.813429 21164 net.cpp:252] TEST Top shape for layer 35 'res5a_branch2b/bn' 1 512 64 128 (4194304)
I0917 00:26:21.813436 21164 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0917 00:26:21.813438 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.813441 21164 net.cpp:184] Created Layer res5a_branch2b/relu (36)
I0917 00:26:21.813444 21164 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0917 00:26:21.813446 21164 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0917 00:26:21.813450 21164 net.cpp:245] Setting up res5a_branch2b/relu
I0917 00:26:21.813452 21164 net.cpp:252] TEST Top shape for layer 36 'res5a_branch2b/relu' 1 512 64 128 (4194304)
I0917 00:26:21.813454 21164 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I0917 00:26:21.813457 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.813463 21164 net.cpp:184] Created Layer out5a (37)
I0917 00:26:21.813465 21164 net.cpp:561] out5a <- res5a_branch2b
I0917 00:26:21.813468 21164 net.cpp:530] out5a -> out5a
I0917 00:26:21.817188 21164 net.cpp:245] Setting up out5a
I0917 00:26:21.817199 21164 net.cpp:252] TEST Top shape for layer 37 'out5a' 1 64 64 128 (524288)
I0917 00:26:21.817204 21164 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I0917 00:26:21.817207 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.817212 21164 net.cpp:184] Created Layer out5a/bn (38)
I0917 00:26:21.817215 21164 net.cpp:561] out5a/bn <- out5a
I0917 00:26:21.817217 21164 net.cpp:513] out5a/bn -> out5a (in-place)
I0917 00:26:21.817627 21164 net.cpp:245] Setting up out5a/bn
I0917 00:26:21.817634 21164 net.cpp:252] TEST Top shape for layer 38 'out5a/bn' 1 64 64 128 (524288)
I0917 00:26:21.817639 21164 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I0917 00:26:21.817642 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.817644 21164 net.cpp:184] Created Layer out5a/relu (39)
I0917 00:26:21.817646 21164 net.cpp:561] out5a/relu <- out5a
I0917 00:26:21.817649 21164 net.cpp:513] out5a/relu -> out5a (in-place)
I0917 00:26:21.817652 21164 net.cpp:245] Setting up out5a/relu
I0917 00:26:21.817654 21164 net.cpp:252] TEST Top shape for layer 39 'out5a/relu' 1 64 64 128 (524288)
I0917 00:26:21.817656 21164 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I0917 00:26:21.817658 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.817682 21164 net.cpp:184] Created Layer out5a_up2 (40)
I0917 00:26:21.817684 21164 net.cpp:561] out5a_up2 <- out5a
I0917 00:26:21.817687 21164 net.cpp:530] out5a_up2 -> out5a_up2
I0917 00:26:21.817822 21164 net.cpp:245] Setting up out5a_up2
I0917 00:26:21.817827 21164 net.cpp:252] TEST Top shape for layer 40 'out5a_up2' 1 64 128 256 (2097152)
I0917 00:26:21.817831 21164 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I0917 00:26:21.817833 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.817842 21164 net.cpp:184] Created Layer out3a (41)
I0917 00:26:21.817844 21164 net.cpp:561] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0917 00:26:21.817847 21164 net.cpp:530] out3a -> out3a
I0917 00:26:21.818769 21164 net.cpp:245] Setting up out3a
I0917 00:26:21.818776 21164 net.cpp:252] TEST Top shape for layer 41 'out3a' 1 64 128 256 (2097152)
I0917 00:26:21.818780 21164 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I0917 00:26:21.818783 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.818787 21164 net.cpp:184] Created Layer out3a/bn (42)
I0917 00:26:21.818789 21164 net.cpp:561] out3a/bn <- out3a
I0917 00:26:21.818791 21164 net.cpp:513] out3a/bn -> out3a (in-place)
I0917 00:26:21.819192 21164 net.cpp:245] Setting up out3a/bn
I0917 00:26:21.819198 21164 net.cpp:252] TEST Top shape for layer 42 'out3a/bn' 1 64 128 256 (2097152)
I0917 00:26:21.819203 21164 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I0917 00:26:21.819205 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.819207 21164 net.cpp:184] Created Layer out3a/relu (43)
I0917 00:26:21.819211 21164 net.cpp:561] out3a/relu <- out3a
I0917 00:26:21.819212 21164 net.cpp:513] out3a/relu -> out3a (in-place)
I0917 00:26:21.819216 21164 net.cpp:245] Setting up out3a/relu
I0917 00:26:21.819217 21164 net.cpp:252] TEST Top shape for layer 43 'out3a/relu' 1 64 128 256 (2097152)
I0917 00:26:21.819219 21164 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I0917 00:26:21.819221 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.819227 21164 net.cpp:184] Created Layer out3_out5_combined (44)
I0917 00:26:21.819231 21164 net.cpp:561] out3_out5_combined <- out5a_up2
I0917 00:26:21.819232 21164 net.cpp:561] out3_out5_combined <- out3a
I0917 00:26:21.819234 21164 net.cpp:530] out3_out5_combined -> out3_out5_combined
I0917 00:26:21.819247 21164 net.cpp:245] Setting up out3_out5_combined
I0917 00:26:21.819252 21164 net.cpp:252] TEST Top shape for layer 44 'out3_out5_combined' 1 64 128 256 (2097152)
I0917 00:26:21.819253 21164 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I0917 00:26:21.819257 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.819262 21164 net.cpp:184] Created Layer ctx_conv1 (45)
I0917 00:26:21.819263 21164 net.cpp:561] ctx_conv1 <- out3_out5_combined
I0917 00:26:21.819265 21164 net.cpp:530] ctx_conv1 -> ctx_conv1
I0917 00:26:21.820158 21164 net.cpp:245] Setting up ctx_conv1
I0917 00:26:21.820163 21164 net.cpp:252] TEST Top shape for layer 45 'ctx_conv1' 1 64 128 256 (2097152)
I0917 00:26:21.820168 21164 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I0917 00:26:21.820169 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.820173 21164 net.cpp:184] Created Layer ctx_conv1/bn (46)
I0917 00:26:21.820175 21164 net.cpp:561] ctx_conv1/bn <- ctx_conv1
I0917 00:26:21.820178 21164 net.cpp:513] ctx_conv1/bn -> ctx_conv1 (in-place)
I0917 00:26:21.820572 21164 net.cpp:245] Setting up ctx_conv1/bn
I0917 00:26:21.820578 21164 net.cpp:252] TEST Top shape for layer 46 'ctx_conv1/bn' 1 64 128 256 (2097152)
I0917 00:26:21.820583 21164 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I0917 00:26:21.820591 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.820595 21164 net.cpp:184] Created Layer ctx_conv1/relu (47)
I0917 00:26:21.820597 21164 net.cpp:561] ctx_conv1/relu <- ctx_conv1
I0917 00:26:21.820600 21164 net.cpp:513] ctx_conv1/relu -> ctx_conv1 (in-place)
I0917 00:26:21.820602 21164 net.cpp:245] Setting up ctx_conv1/relu
I0917 00:26:21.820605 21164 net.cpp:252] TEST Top shape for layer 47 'ctx_conv1/relu' 1 64 128 256 (2097152)
I0917 00:26:21.820607 21164 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I0917 00:26:21.820610 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.820614 21164 net.cpp:184] Created Layer ctx_conv2 (48)
I0917 00:26:21.820617 21164 net.cpp:561] ctx_conv2 <- ctx_conv1
I0917 00:26:21.820619 21164 net.cpp:530] ctx_conv2 -> ctx_conv2
I0917 00:26:21.821506 21164 net.cpp:245] Setting up ctx_conv2
I0917 00:26:21.821512 21164 net.cpp:252] TEST Top shape for layer 48 'ctx_conv2' 1 64 128 256 (2097152)
I0917 00:26:21.821516 21164 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I0917 00:26:21.821518 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.821522 21164 net.cpp:184] Created Layer ctx_conv2/bn (49)
I0917 00:26:21.821524 21164 net.cpp:561] ctx_conv2/bn <- ctx_conv2
I0917 00:26:21.821527 21164 net.cpp:513] ctx_conv2/bn -> ctx_conv2 (in-place)
I0917 00:26:21.821918 21164 net.cpp:245] Setting up ctx_conv2/bn
I0917 00:26:21.821924 21164 net.cpp:252] TEST Top shape for layer 49 'ctx_conv2/bn' 1 64 128 256 (2097152)
I0917 00:26:21.821929 21164 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I0917 00:26:21.821931 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.821934 21164 net.cpp:184] Created Layer ctx_conv2/relu (50)
I0917 00:26:21.821936 21164 net.cpp:561] ctx_conv2/relu <- ctx_conv2
I0917 00:26:21.821938 21164 net.cpp:513] ctx_conv2/relu -> ctx_conv2 (in-place)
I0917 00:26:21.821941 21164 net.cpp:245] Setting up ctx_conv2/relu
I0917 00:26:21.821943 21164 net.cpp:252] TEST Top shape for layer 50 'ctx_conv2/relu' 1 64 128 256 (2097152)
I0917 00:26:21.821945 21164 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I0917 00:26:21.821947 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.821952 21164 net.cpp:184] Created Layer ctx_conv3 (51)
I0917 00:26:21.821954 21164 net.cpp:561] ctx_conv3 <- ctx_conv2
I0917 00:26:21.821956 21164 net.cpp:530] ctx_conv3 -> ctx_conv3
I0917 00:26:21.822840 21164 net.cpp:245] Setting up ctx_conv3
I0917 00:26:21.822846 21164 net.cpp:252] TEST Top shape for layer 51 'ctx_conv3' 1 64 128 256 (2097152)
I0917 00:26:21.822850 21164 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I0917 00:26:21.822852 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.822856 21164 net.cpp:184] Created Layer ctx_conv3/bn (52)
I0917 00:26:21.822859 21164 net.cpp:561] ctx_conv3/bn <- ctx_conv3
I0917 00:26:21.822860 21164 net.cpp:513] ctx_conv3/bn -> ctx_conv3 (in-place)
I0917 00:26:21.823266 21164 net.cpp:245] Setting up ctx_conv3/bn
I0917 00:26:21.823274 21164 net.cpp:252] TEST Top shape for layer 52 'ctx_conv3/bn' 1 64 128 256 (2097152)
I0917 00:26:21.823279 21164 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I0917 00:26:21.823282 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.823284 21164 net.cpp:184] Created Layer ctx_conv3/relu (53)
I0917 00:26:21.823287 21164 net.cpp:561] ctx_conv3/relu <- ctx_conv3
I0917 00:26:21.823288 21164 net.cpp:513] ctx_conv3/relu -> ctx_conv3 (in-place)
I0917 00:26:21.823292 21164 net.cpp:245] Setting up ctx_conv3/relu
I0917 00:26:21.823294 21164 net.cpp:252] TEST Top shape for layer 53 'ctx_conv3/relu' 1 64 128 256 (2097152)
I0917 00:26:21.823302 21164 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I0917 00:26:21.823304 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.823308 21164 net.cpp:184] Created Layer ctx_conv4 (54)
I0917 00:26:21.823310 21164 net.cpp:561] ctx_conv4 <- ctx_conv3
I0917 00:26:21.823312 21164 net.cpp:530] ctx_conv4 -> ctx_conv4
I0917 00:26:21.824203 21164 net.cpp:245] Setting up ctx_conv4
I0917 00:26:21.824208 21164 net.cpp:252] TEST Top shape for layer 54 'ctx_conv4' 1 64 128 256 (2097152)
I0917 00:26:21.824213 21164 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I0917 00:26:21.824215 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.824218 21164 net.cpp:184] Created Layer ctx_conv4/bn (55)
I0917 00:26:21.824220 21164 net.cpp:561] ctx_conv4/bn <- ctx_conv4
I0917 00:26:21.824223 21164 net.cpp:513] ctx_conv4/bn -> ctx_conv4 (in-place)
I0917 00:26:21.824615 21164 net.cpp:245] Setting up ctx_conv4/bn
I0917 00:26:21.824620 21164 net.cpp:252] TEST Top shape for layer 55 'ctx_conv4/bn' 1 64 128 256 (2097152)
I0917 00:26:21.824625 21164 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I0917 00:26:21.824627 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.824630 21164 net.cpp:184] Created Layer ctx_conv4/relu (56)
I0917 00:26:21.824633 21164 net.cpp:561] ctx_conv4/relu <- ctx_conv4
I0917 00:26:21.824635 21164 net.cpp:513] ctx_conv4/relu -> ctx_conv4 (in-place)
I0917 00:26:21.824638 21164 net.cpp:245] Setting up ctx_conv4/relu
I0917 00:26:21.824640 21164 net.cpp:252] TEST Top shape for layer 56 'ctx_conv4/relu' 1 64 128 256 (2097152)
I0917 00:26:21.824642 21164 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I0917 00:26:21.824645 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.824650 21164 net.cpp:184] Created Layer ctx_final (57)
I0917 00:26:21.824651 21164 net.cpp:561] ctx_final <- ctx_conv4
I0917 00:26:21.824653 21164 net.cpp:530] ctx_final -> ctx_final
I0917 00:26:21.824918 21164 net.cpp:245] Setting up ctx_final
I0917 00:26:21.824923 21164 net.cpp:252] TEST Top shape for layer 57 'ctx_final' 1 8 128 256 (262144)
I0917 00:26:21.824928 21164 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I0917 00:26:21.824929 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.824932 21164 net.cpp:184] Created Layer ctx_final/relu (58)
I0917 00:26:21.824934 21164 net.cpp:561] ctx_final/relu <- ctx_final
I0917 00:26:21.824936 21164 net.cpp:513] ctx_final/relu -> ctx_final (in-place)
I0917 00:26:21.824939 21164 net.cpp:245] Setting up ctx_final/relu
I0917 00:26:21.824942 21164 net.cpp:252] TEST Top shape for layer 58 'ctx_final/relu' 1 8 128 256 (262144)
I0917 00:26:21.824944 21164 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I0917 00:26:21.824946 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.824954 21164 net.cpp:184] Created Layer out_deconv_final_up2 (59)
I0917 00:26:21.824956 21164 net.cpp:561] out_deconv_final_up2 <- ctx_final
I0917 00:26:21.824959 21164 net.cpp:530] out_deconv_final_up2 -> out_deconv_final_up2
I0917 00:26:21.825073 21164 net.cpp:245] Setting up out_deconv_final_up2
I0917 00:26:21.825078 21164 net.cpp:252] TEST Top shape for layer 59 'out_deconv_final_up2' 1 8 256 512 (1048576)
I0917 00:26:21.825079 21164 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I0917 00:26:21.825081 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.825086 21164 net.cpp:184] Created Layer out_deconv_final_up4 (60)
I0917 00:26:21.825088 21164 net.cpp:561] out_deconv_final_up4 <- out_deconv_final_up2
I0917 00:26:21.825095 21164 net.cpp:530] out_deconv_final_up4 -> out_deconv_final_up4
I0917 00:26:21.825209 21164 net.cpp:245] Setting up out_deconv_final_up4
I0917 00:26:21.825214 21164 net.cpp:252] TEST Top shape for layer 60 'out_deconv_final_up4' 1 8 512 1024 (4194304)
I0917 00:26:21.825217 21164 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I0917 00:26:21.825219 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.825229 21164 net.cpp:184] Created Layer out_deconv_final_up8 (61)
I0917 00:26:21.825232 21164 net.cpp:561] out_deconv_final_up8 <- out_deconv_final_up4
I0917 00:26:21.825234 21164 net.cpp:530] out_deconv_final_up8 -> out_deconv_final_up8
I0917 00:26:21.825346 21164 net.cpp:245] Setting up out_deconv_final_up8
I0917 00:26:21.825350 21164 net.cpp:252] TEST Top shape for layer 61 'out_deconv_final_up8' 1 8 1024 2048 (16777216)
I0917 00:26:21.825353 21164 layer_factory.hpp:136] Creating layer 'argMaxOut' of type 'ArgMax'
I0917 00:26:21.825356 21164 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:26:21.825361 21164 net.cpp:184] Created Layer argMaxOut (62)
I0917 00:26:21.825363 21164 net.cpp:561] argMaxOut <- out_deconv_final_up8
I0917 00:26:21.825366 21164 net.cpp:530] argMaxOut -> argMaxOut
I0917 00:26:21.825378 21164 net.cpp:245] Setting up argMaxOut
I0917 00:26:21.825381 21164 net.cpp:252] TEST Top shape for layer 62 'argMaxOut' 1 1 1024 2048 (2097152)
I0917 00:26:21.825383 21164 net.cpp:325] argMaxOut does not need backward computation.
I0917 00:26:21.825386 21164 net.cpp:325] out_deconv_final_up8 does not need backward computation.
I0917 00:26:21.825387 21164 net.cpp:325] out_deconv_final_up4 does not need backward computation.
I0917 00:26:21.825389 21164 net.cpp:325] out_deconv_final_up2 does not need backward computation.
I0917 00:26:21.825392 21164 net.cpp:325] ctx_final/relu does not need backward computation.
I0917 00:26:21.825392 21164 net.cpp:325] ctx_final does not need backward computation.
I0917 00:26:21.825394 21164 net.cpp:325] ctx_conv4/relu does not need backward computation.
I0917 00:26:21.825397 21164 net.cpp:325] ctx_conv4/bn does not need backward computation.
I0917 00:26:21.825398 21164 net.cpp:325] ctx_conv4 does not need backward computation.
I0917 00:26:21.825400 21164 net.cpp:325] ctx_conv3/relu does not need backward computation.
I0917 00:26:21.825402 21164 net.cpp:325] ctx_conv3/bn does not need backward computation.
I0917 00:26:21.825403 21164 net.cpp:325] ctx_conv3 does not need backward computation.
I0917 00:26:21.825405 21164 net.cpp:325] ctx_conv2/relu does not need backward computation.
I0917 00:26:21.825407 21164 net.cpp:325] ctx_conv2/bn does not need backward computation.
I0917 00:26:21.825408 21164 net.cpp:325] ctx_conv2 does not need backward computation.
I0917 00:26:21.825410 21164 net.cpp:325] ctx_conv1/relu does not need backward computation.
I0917 00:26:21.825412 21164 net.cpp:325] ctx_conv1/bn does not need backward computation.
I0917 00:26:21.825413 21164 net.cpp:325] ctx_conv1 does not need backward computation.
I0917 00:26:21.825415 21164 net.cpp:325] out3_out5_combined does not need backward computation.
I0917 00:26:21.825417 21164 net.cpp:325] out3a/relu does not need backward computation.
I0917 00:26:21.825419 21164 net.cpp:325] out3a/bn does not need backward computation.
I0917 00:26:21.825422 21164 net.cpp:325] out3a does not need backward computation.
I0917 00:26:21.825423 21164 net.cpp:325] out5a_up2 does not need backward computation.
I0917 00:26:21.825425 21164 net.cpp:325] out5a/relu does not need backward computation.
I0917 00:26:21.825426 21164 net.cpp:325] out5a/bn does not need backward computation.
I0917 00:26:21.825428 21164 net.cpp:325] out5a does not need backward computation.
I0917 00:26:21.825431 21164 net.cpp:325] res5a_branch2b/relu does not need backward computation.
I0917 00:26:21.825433 21164 net.cpp:325] res5a_branch2b/bn does not need backward computation.
I0917 00:26:21.825440 21164 net.cpp:325] res5a_branch2b does not need backward computation.
I0917 00:26:21.825443 21164 net.cpp:325] res5a_branch2a/relu does not need backward computation.
I0917 00:26:21.825444 21164 net.cpp:325] res5a_branch2a/bn does not need backward computation.
I0917 00:26:21.825446 21164 net.cpp:325] res5a_branch2a does not need backward computation.
I0917 00:26:21.825448 21164 net.cpp:325] pool4 does not need backward computation.
I0917 00:26:21.825450 21164 net.cpp:325] res4a_branch2b/relu does not need backward computation.
I0917 00:26:21.825453 21164 net.cpp:325] res4a_branch2b/bn does not need backward computation.
I0917 00:26:21.825454 21164 net.cpp:325] res4a_branch2b does not need backward computation.
I0917 00:26:21.825456 21164 net.cpp:325] res4a_branch2a/relu does not need backward computation.
I0917 00:26:21.825459 21164 net.cpp:325] res4a_branch2a/bn does not need backward computation.
I0917 00:26:21.825459 21164 net.cpp:325] res4a_branch2a does not need backward computation.
I0917 00:26:21.825461 21164 net.cpp:325] pool3 does not need backward computation.
I0917 00:26:21.825464 21164 net.cpp:325] res3a_branch2b_res3a_branch2b/relu_0_split does not need backward computation.
I0917 00:26:21.825466 21164 net.cpp:325] res3a_branch2b/relu does not need backward computation.
I0917 00:26:21.825467 21164 net.cpp:325] res3a_branch2b/bn does not need backward computation.
I0917 00:26:21.825469 21164 net.cpp:325] res3a_branch2b does not need backward computation.
I0917 00:26:21.825471 21164 net.cpp:325] res3a_branch2a/relu does not need backward computation.
I0917 00:26:21.825474 21164 net.cpp:325] res3a_branch2a/bn does not need backward computation.
I0917 00:26:21.825475 21164 net.cpp:325] res3a_branch2a does not need backward computation.
I0917 00:26:21.825477 21164 net.cpp:325] pool2 does not need backward computation.
I0917 00:26:21.825479 21164 net.cpp:325] res2a_branch2b/relu does not need backward computation.
I0917 00:26:21.825481 21164 net.cpp:325] res2a_branch2b/bn does not need backward computation.
I0917 00:26:21.825482 21164 net.cpp:325] res2a_branch2b does not need backward computation.
I0917 00:26:21.825485 21164 net.cpp:325] res2a_branch2a/relu does not need backward computation.
I0917 00:26:21.825486 21164 net.cpp:325] res2a_branch2a/bn does not need backward computation.
I0917 00:26:21.825489 21164 net.cpp:325] res2a_branch2a does not need backward computation.
I0917 00:26:21.825491 21164 net.cpp:325] pool1 does not need backward computation.
I0917 00:26:21.825494 21164 net.cpp:325] conv1b/relu does not need backward computation.
I0917 00:26:21.825495 21164 net.cpp:325] conv1b/bn does not need backward computation.
I0917 00:26:21.825496 21164 net.cpp:325] conv1b does not need backward computation.
I0917 00:26:21.825498 21164 net.cpp:325] conv1a/relu does not need backward computation.
I0917 00:26:21.825500 21164 net.cpp:325] conv1a/bn does not need backward computation.
I0917 00:26:21.825502 21164 net.cpp:325] conv1a does not need backward computation.
I0917 00:26:21.825505 21164 net.cpp:325] data/bias does not need backward computation.
I0917 00:26:21.825506 21164 net.cpp:325] input does not need backward computation.
I0917 00:26:21.825508 21164 net.cpp:367] This network produces output argMaxOut
I0917 00:26:21.825547 21164 net.cpp:389] Top memory (TEST) required for data: 1224736768 diff: 1224736768
I0917 00:26:21.825551 21164 net.cpp:392] Bottom memory (TEST) required for data: 1216348160 diff: 1216348160
I0917 00:26:21.825552 21164 net.cpp:395] Shared (in-place) memory (TEST) by data: 659554304 diff: 659554304
I0917 00:26:21.825553 21164 net.cpp:398] Parameters memory (TEST) required for data: 10817840 diff: 10817840
I0917 00:26:21.825556 21164 net.cpp:401] Parameters shared memory (TEST) by data: 0 diff: 0
I0917 00:26:21.825557 21164 net.cpp:407] Network initialization done.
I0917 00:26:21.830544 21164 net.cpp:1078] Ignoring source layer data
I0917 00:26:21.830561 21164 net.cpp:1094] Copying source layer data/bias Type:Bias #blobs=1
I0917 00:26:21.830587 21164 net.cpp:1094] Copying source layer conv1a Type:Convolution #blobs=2
I0917 00:26:21.830611 21164 net.cpp:1094] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0917 00:26:21.830757 21164 net.cpp:1094] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0917 00:26:21.830761 21164 net.cpp:1094] Copying source layer conv1b Type:Convolution #blobs=2
I0917 00:26:21.830770 21164 net.cpp:1094] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0917 00:26:21.830862 21164 net.cpp:1094] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0917 00:26:21.830865 21164 net.cpp:1094] Copying source layer pool1 Type:Pooling #blobs=0
I0917 00:26:21.830868 21164 net.cpp:1094] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0917 00:26:21.830883 21164 net.cpp:1094] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0917 00:26:21.830971 21164 net.cpp:1094] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0917 00:26:21.830976 21164 net.cpp:1094] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0917 00:26:21.830986 21164 net.cpp:1094] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0917 00:26:21.831073 21164 net.cpp:1094] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0917 00:26:21.831076 21164 net.cpp:1094] Copying source layer pool2 Type:Pooling #blobs=0
I0917 00:26:21.831079 21164 net.cpp:1094] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0917 00:26:21.831115 21164 net.cpp:1094] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0917 00:26:21.831195 21164 net.cpp:1094] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0917 00:26:21.831199 21164 net.cpp:1094] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0917 00:26:21.831219 21164 net.cpp:1094] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0917 00:26:21.831295 21164 net.cpp:1094] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0917 00:26:21.831298 21164 net.cpp:1094] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I0917 00:26:21.831300 21164 net.cpp:1094] Copying source layer pool3 Type:Pooling #blobs=0
I0917 00:26:21.831302 21164 net.cpp:1094] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0917 00:26:21.831406 21164 net.cpp:1094] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0917 00:26:21.831482 21164 net.cpp:1094] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0917 00:26:21.831486 21164 net.cpp:1094] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0917 00:26:21.831542 21164 net.cpp:1094] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0917 00:26:21.831622 21164 net.cpp:1094] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0917 00:26:21.831625 21164 net.cpp:1094] Copying source layer pool4 Type:Pooling #blobs=0
I0917 00:26:21.831629 21164 net.cpp:1094] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0917 00:26:21.831970 21164 net.cpp:1094] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0917 00:26:21.832047 21164 net.cpp:1094] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0917 00:26:21.832051 21164 net.cpp:1094] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0917 00:26:21.832234 21164 net.cpp:1094] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0917 00:26:21.832309 21164 net.cpp:1094] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0917 00:26:21.832314 21164 net.cpp:1094] Copying source layer out5a Type:Convolution #blobs=2
I0917 00:26:21.832365 21164 net.cpp:1094] Copying source layer out5a/bn Type:BatchNorm #blobs=5
I0917 00:26:21.832453 21164 net.cpp:1094] Copying source layer out5a/relu Type:ReLU #blobs=0
I0917 00:26:21.832458 21164 net.cpp:1094] Copying source layer out5a_up2 Type:Deconvolution #blobs=1
I0917 00:26:21.832463 21164 net.cpp:1094] Copying source layer out3a Type:Convolution #blobs=2
I0917 00:26:21.832479 21164 net.cpp:1094] Copying source layer out3a/bn Type:BatchNorm #blobs=5
I0917 00:26:21.832568 21164 net.cpp:1094] Copying source layer out3a/relu Type:ReLU #blobs=0
I0917 00:26:21.832572 21164 net.cpp:1094] Copying source layer out3_out5_combined Type:Eltwise #blobs=0
I0917 00:26:21.832576 21164 net.cpp:1094] Copying source layer ctx_conv1 Type:Convolution #blobs=2
I0917 00:26:21.832590 21164 net.cpp:1094] Copying source layer ctx_conv1/bn Type:BatchNorm #blobs=5
I0917 00:26:21.832675 21164 net.cpp:1094] Copying source layer ctx_conv1/relu Type:ReLU #blobs=0
I0917 00:26:21.832679 21164 net.cpp:1094] Copying source layer ctx_conv2 Type:Convolution #blobs=2
I0917 00:26:21.832697 21164 net.cpp:1094] Copying source layer ctx_conv2/bn Type:BatchNorm #blobs=5
I0917 00:26:21.832783 21164 net.cpp:1094] Copying source layer ctx_conv2/relu Type:ReLU #blobs=0
I0917 00:26:21.832787 21164 net.cpp:1094] Copying source layer ctx_conv3 Type:Convolution #blobs=2
I0917 00:26:21.832803 21164 net.cpp:1094] Copying source layer ctx_conv3/bn Type:BatchNorm #blobs=5
I0917 00:26:21.832887 21164 net.cpp:1094] Copying source layer ctx_conv3/relu Type:ReLU #blobs=0
I0917 00:26:21.832891 21164 net.cpp:1094] Copying source layer ctx_conv4 Type:Convolution #blobs=2
I0917 00:26:21.832909 21164 net.cpp:1094] Copying source layer ctx_conv4/bn Type:BatchNorm #blobs=5
I0917 00:26:21.832994 21164 net.cpp:1094] Copying source layer ctx_conv4/relu Type:ReLU #blobs=0
I0917 00:26:21.832998 21164 net.cpp:1094] Copying source layer ctx_final Type:Convolution #blobs=2
I0917 00:26:21.833006 21164 net.cpp:1094] Copying source layer ctx_final/relu Type:ReLU #blobs=0
I0917 00:26:21.833009 21164 net.cpp:1094] Copying source layer out_deconv_final_up2 Type:Deconvolution #blobs=1
I0917 00:26:21.833014 21164 net.cpp:1094] Copying source layer out_deconv_final_up4 Type:Deconvolution #blobs=1
I0917 00:26:21.833019 21164 net.cpp:1094] Copying source layer out_deconv_final_up8 Type:Deconvolution #blobs=1
I0917 00:26:21.833024 21164 net.cpp:1078] Ignoring source layer loss
Namespace(batch_size=1, blend=False, class_dict='', crop=['0'], input='./data/val-image-list.txt', label='./data/val-label-list.txt', label_dict='', model='../trained/image_segmentation/cityscapes5_jsegnet21v2/l1reg/deploy.prototxt', num_classes=5, num_images=500, output=None, palette='', resize=['0'], resize_back=True, search='*.png', weights='../trained/image_segmentation/cityscapes5_jsegnet21v2/l1reg/cityscapes5_jsegnet21v2_iter_60000.caffemodel')
Infering list
Getting list of images...running inference for  500  images...
('frankfurt_000000_000294_leftImg8bit.png', 'frankfurt_000000_000294_gtFine_labelIds.png', 0)
('frankfurt_000000_000576_leftImg8bit.png', 'frankfurt_000000_000576_gtFine_labelIds.png', 0)
('frankfurt_000000_001016_leftImg8bit.png', 'frankfurt_000000_001016_gtFine_labelIds.png', 0)
I0917 00:26:22.658463 21164 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'conv1a' with space 0.01G 3/1 1 	(avail 5.92G, req 0.01G)	t: 0
I0917 00:26:22.787812 21164 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'conv1b' with space 0.01G 32/4 6 	(avail 5.92G, req 0.01G)	t: 0
I0917 00:26:22.805382 21164 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res2a_branch2a' with space 0.01G 32/1 6 	(avail 5.92G, req 0.01G)	t: 0
I0917 00:26:22.843713 21164 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res2a_branch2b' with space 0.01G 64/4 6 	(avail 5.92G, req 0.01G)	t: 0
I0917 00:26:22.854486 21164 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res3a_branch2a' with space 0.01G 64/1 6 	(avail 5.92G, req 0.01G)	t: 0
I0917 00:26:22.858628 21164 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res3a_branch2b' with space 0.01G 128/4 6 	(avail 5.92G, req 0.01G)	t: 0
I0917 00:26:22.867944 21164 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res4a_branch2a' with space 0.01G 128/1 6 	(avail 5.92G, req 0.01G)	t: 0
I0917 00:26:22.871460 21164 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res4a_branch2b' with space 0.01G 256/4 6 	(avail 5.92G, req 0.01G)	t: 0
I0917 00:26:22.885634 21164 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'out3a' with space 0.01G 128/2 6 	(avail 5.92G, req 0.01G)	t: 0
I0917 00:26:22.892406 21164 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'ctx_conv1' with space 0.01G 64/1 6 	(avail 5.92G, req 0.01G)	t: 0
I0917 00:26:22.917933 21164 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'ctx_final' with space 0.01G 64/1 6 	(avail 5.92G, req 0.01G)	t: 0
('frankfurt_000000_001236_leftImg8bit.png', 'frankfurt_000000_001236_gtFine_labelIds.png', 0)
('frankfurt_000000_001751_leftImg8bit.png', 'frankfurt_000000_001751_gtFine_labelIds.png', 0)
('frankfurt_000000_002196_leftImg8bit.png', 'frankfurt_000000_002196_gtFine_labelIds.png', 1)
('frankfurt_000000_002963_leftImg8bit.png', 'frankfurt_000000_002963_gtFine_labelIds.png', 1)
('frankfurt_000000_003025_leftImg8bit.png', 'frankfurt_000000_003025_gtFine_labelIds.png', 1)
('frankfurt_000000_003357_leftImg8bit.png', 'frankfurt_000000_003357_gtFine_labelIds.png', 1)
('frankfurt_000000_003920_leftImg8bit.png', 'frankfurt_000000_003920_gtFine_labelIds.png', 1)
('frankfurt_000000_004617_leftImg8bit.png', 'frankfurt_000000_004617_gtFine_labelIds.png', 2)
('frankfurt_000000_005543_leftImg8bit.png', 'frankfurt_000000_005543_gtFine_labelIds.png', 2)
('frankfurt_000000_005898_leftImg8bit.png', 'frankfurt_000000_005898_gtFine_labelIds.png', 2)
('frankfurt_000000_006589_leftImg8bit.png', 'frankfurt_000000_006589_gtFine_labelIds.png', 2)
('frankfurt_000000_007365_leftImg8bit.png', 'frankfurt_000000_007365_gtFine_labelIds.png', 2)
('frankfurt_000000_008206_leftImg8bit.png', 'frankfurt_000000_008206_gtFine_labelIds.png', 3)
('frankfurt_000000_008451_leftImg8bit.png', 'frankfurt_000000_008451_gtFine_labelIds.png', 3)
('frankfurt_000000_009291_leftImg8bit.png', 'frankfurt_000000_009291_gtFine_labelIds.png', 3)
('frankfurt_000000_009561_leftImg8bit.png', 'frankfurt_000000_009561_gtFine_labelIds.png', 3)
('frankfurt_000000_009688_leftImg8bit.png', 'frankfurt_000000_009688_gtFine_labelIds.png', 3)
('frankfurt_000000_009969_leftImg8bit.png', 'frankfurt_000000_009969_gtFine_labelIds.png', 4)
('frankfurt_000000_010351_leftImg8bit.png', 'frankfurt_000000_010351_gtFine_labelIds.png', 4)
('frankfurt_000000_010763_leftImg8bit.png', 'frankfurt_000000_010763_gtFine_labelIds.png', 4)
('frankfurt_000000_011007_leftImg8bit.png', 'frankfurt_000000_011007_gtFine_labelIds.png', 4)
('frankfurt_000000_011074_leftImg8bit.png', 'frankfurt_000000_011074_gtFine_labelIds.png', 4)
pixel_accuracy=0.974895376059, mean_iou=0.830725874432, iou=[ 0.95835131  0.96192297  0.76209772  0.57632984  0.89492754]
('frankfurt_000000_011461_leftImg8bit.png', 'frankfurt_000000_011461_gtFine_labelIds.png', 5)
('frankfurt_000000_011810_leftImg8bit.png', 'frankfurt_000000_011810_gtFine_labelIds.png', 5)
('frankfurt_000000_012009_leftImg8bit.png', 'frankfurt_000000_012009_gtFine_labelIds.png', 5)
('frankfurt_000000_012121_leftImg8bit.png', 'frankfurt_000000_012121_gtFine_labelIds.png', 5)
('frankfurt_000000_012868_leftImg8bit.png', 'frankfurt_000000_012868_gtFine_labelIds.png', 5)
('frankfurt_000000_013067_leftImg8bit.png', 'frankfurt_000000_013067_gtFine_labelIds.png', 6)
('frankfurt_000000_013240_leftImg8bit.png', 'frankfurt_000000_013240_gtFine_labelIds.png', 6)
('frankfurt_000000_013382_leftImg8bit.png', 'frankfurt_000000_013382_gtFine_labelIds.png', 6)
('frankfurt_000000_013942_leftImg8bit.png', 'frankfurt_000000_013942_gtFine_labelIds.png', 6)
('frankfurt_000000_014480_leftImg8bit.png', 'frankfurt_000000_014480_gtFine_labelIds.png', 6)
('frankfurt_000000_015389_leftImg8bit.png', 'frankfurt_000000_015389_gtFine_labelIds.png', 7)
('frankfurt_000000_015676_leftImg8bit.png', 'frankfurt_000000_015676_gtFine_labelIds.png', 7)
('frankfurt_000000_016005_leftImg8bit.png', 'frankfurt_000000_016005_gtFine_labelIds.png', 7)
('frankfurt_000000_016286_leftImg8bit.png', 'frankfurt_000000_016286_gtFine_labelIds.png', 7)
('frankfurt_000000_017228_leftImg8bit.png', 'frankfurt_000000_017228_gtFine_labelIds.png', 7)
('frankfurt_000000_017476_leftImg8bit.png', 'frankfurt_000000_017476_gtFine_labelIds.png', 8)
('frankfurt_000000_018797_leftImg8bit.png', 'frankfurt_000000_018797_gtFine_labelIds.png', 8)
('frankfurt_000000_019607_leftImg8bit.png', 'frankfurt_000000_019607_gtFine_labelIds.png', 8)
('frankfurt_000000_020215_leftImg8bit.png', 'frankfurt_000000_020215_gtFine_labelIds.png', 8)
('frankfurt_000000_020321_leftImg8bit.png', 'frankfurt_000000_020321_gtFine_labelIds.png', 8)
('frankfurt_000000_020880_leftImg8bit.png', 'frankfurt_000000_020880_gtFine_labelIds.png', 9)
('frankfurt_000000_021667_leftImg8bit.png', 'frankfurt_000000_021667_gtFine_labelIds.png', 9)
('frankfurt_000000_021879_leftImg8bit.png', 'frankfurt_000000_021879_gtFine_labelIds.png', 9)
('frankfurt_000000_022254_leftImg8bit.png', 'frankfurt_000000_022254_gtFine_labelIds.png', 9)
('frankfurt_000000_022797_leftImg8bit.png', 'frankfurt_000000_022797_gtFine_labelIds.png', 9)
pixel_accuracy=0.974940431396, mean_iou=0.842585442508, iou=[ 0.95851373  0.9654392   0.77501327  0.61850047  0.89546054]
('frankfurt_000001_000538_leftImg8bit.png', 'frankfurt_000001_000538_gtFine_labelIds.png', 10)
('frankfurt_000001_001464_leftImg8bit.png', 'frankfurt_000001_001464_gtFine_labelIds.png', 10)
('frankfurt_000001_002512_leftImg8bit.png', 'frankfurt_000001_002512_gtFine_labelIds.png', 10)
('frankfurt_000001_002646_leftImg8bit.png', 'frankfurt_000001_002646_gtFine_labelIds.png', 10)
('frankfurt_000001_002759_leftImg8bit.png', 'frankfurt_000001_002759_gtFine_labelIds.png', 10)
('frankfurt_000001_003056_leftImg8bit.png', 'frankfurt_000001_003056_gtFine_labelIds.png', 11)
('frankfurt_000001_003588_leftImg8bit.png', 'frankfurt_000001_003588_gtFine_labelIds.png', 11)
('frankfurt_000001_004327_leftImg8bit.png', 'frankfurt_000001_004327_gtFine_labelIds.png', 11)
('frankfurt_000001_004736_leftImg8bit.png', 'frankfurt_000001_004736_gtFine_labelIds.png', 11)
('frankfurt_000001_004859_leftImg8bit.png', 'frankfurt_000001_004859_gtFine_labelIds.png', 11)
('frankfurt_000001_005184_leftImg8bit.png', 'frankfurt_000001_005184_gtFine_labelIds.png', 12)
('frankfurt_000001_005410_leftImg8bit.png', 'frankfurt_000001_005410_gtFine_labelIds.png', 12)
('frankfurt_000001_005703_leftImg8bit.png', 'frankfurt_000001_005703_gtFine_labelIds.png', 12)
('frankfurt_000001_005898_leftImg8bit.png', 'frankfurt_000001_005898_gtFine_labelIds.png', 12)
('frankfurt_000001_007285_leftImg8bit.png', 'frankfurt_000001_007285_gtFine_labelIds.png', 12)
('frankfurt_000001_007407_leftImg8bit.png', 'frankfurt_000001_007407_gtFine_labelIds.png', 13)
('frankfurt_000001_007622_leftImg8bit.png', 'frankfurt_000001_007622_gtFine_labelIds.png', 13)
('frankfurt_000001_007857_leftImg8bit.png', 'frankfurt_000001_007857_gtFine_labelIds.png', 13)
('frankfurt_000001_007973_leftImg8bit.png', 'frankfurt_000001_007973_gtFine_labelIds.png', 13)
('frankfurt_000001_008200_leftImg8bit.png', 'frankfurt_000001_008200_gtFine_labelIds.png', 13)
('frankfurt_000001_008688_leftImg8bit.png', 'frankfurt_000001_008688_gtFine_labelIds.png', 14)
('frankfurt_000001_009058_leftImg8bit.png', 'frankfurt_000001_009058_gtFine_labelIds.png', 14)
('frankfurt_000001_009504_leftImg8bit.png', 'frankfurt_000001_009504_gtFine_labelIds.png', 14)
('frankfurt_000001_009854_leftImg8bit.png', 'frankfurt_000001_009854_gtFine_labelIds.png', 14)
('frankfurt_000001_010156_leftImg8bit.png', 'frankfurt_000001_010156_gtFine_labelIds.png', 14)
pixel_accuracy=0.973365629312, mean_iou=0.838990334377, iou=[ 0.9558037   0.96321194  0.76492054  0.6177781   0.89323739]
('frankfurt_000001_010444_leftImg8bit.png', 'frankfurt_000001_010444_gtFine_labelIds.png', 15)
('frankfurt_000001_010600_leftImg8bit.png', 'frankfurt_000001_010600_gtFine_labelIds.png', 15)
('frankfurt_000001_010830_leftImg8bit.png', 'frankfurt_000001_010830_gtFine_labelIds.png', 15)
('frankfurt_000001_011162_leftImg8bit.png', 'frankfurt_000001_011162_gtFine_labelIds.png', 15)
('frankfurt_000001_011715_leftImg8bit.png', 'frankfurt_000001_011715_gtFine_labelIds.png', 15)
('frankfurt_000001_011835_leftImg8bit.png', 'frankfurt_000001_011835_gtFine_labelIds.png', 16)
('frankfurt_000001_012038_leftImg8bit.png', 'frankfurt_000001_012038_gtFine_labelIds.png', 16)
('frankfurt_000001_012519_leftImg8bit.png', 'frankfurt_000001_012519_gtFine_labelIds.png', 16)
('frankfurt_000001_012699_leftImg8bit.png', 'frankfurt_000001_012699_gtFine_labelIds.png', 16)
('frankfurt_000001_012738_leftImg8bit.png', 'frankfurt_000001_012738_gtFine_labelIds.png', 16)
('frankfurt_000001_012870_leftImg8bit.png', 'frankfurt_000001_012870_gtFine_labelIds.png', 17)
('frankfurt_000001_013016_leftImg8bit.png', 'frankfurt_000001_013016_gtFine_labelIds.png', 17)
('frankfurt_000001_013496_leftImg8bit.png', 'frankfurt_000001_013496_gtFine_labelIds.png', 17)
('frankfurt_000001_013710_leftImg8bit.png', 'frankfurt_000001_013710_gtFine_labelIds.png', 17)
('frankfurt_000001_014221_leftImg8bit.png', 'frankfurt_000001_014221_gtFine_labelIds.png', 17)
('frankfurt_000001_014406_leftImg8bit.png', 'frankfurt_000001_014406_gtFine_labelIds.png', 18)
('frankfurt_000001_014565_leftImg8bit.png', 'frankfurt_000001_014565_gtFine_labelIds.png', 18)
('frankfurt_000001_014741_leftImg8bit.png', 'frankfurt_000001_014741_gtFine_labelIds.png', 18)
('frankfurt_000001_015091_leftImg8bit.png', 'frankfurt_000001_015091_gtFine_labelIds.png', 18)
('frankfurt_000001_015328_leftImg8bit.png', 'frankfurt_000001_015328_gtFine_labelIds.png', 18)
('frankfurt_000001_015768_leftImg8bit.png', 'frankfurt_000001_015768_gtFine_labelIds.png', 19)
('frankfurt_000001_016029_leftImg8bit.png', 'frankfurt_000001_016029_gtFine_labelIds.png', 19)
('frankfurt_000001_016273_leftImg8bit.png', 'frankfurt_000001_016273_gtFine_labelIds.png', 19)
('frankfurt_000001_016462_leftImg8bit.png', 'frankfurt_000001_016462_gtFine_labelIds.png', 19)
('frankfurt_000001_017101_leftImg8bit.png', 'frankfurt_000001_017101_gtFine_labelIds.png', 19)
pixel_accuracy=0.969792194239, mean_iou=0.842326522189, iou=[ 0.9507911   0.95792195  0.78721321  0.63172349  0.88398287]
('frankfurt_000001_017459_leftImg8bit.png', 'frankfurt_000001_017459_gtFine_labelIds.png', 20)
('frankfurt_000001_017842_leftImg8bit.png', 'frankfurt_000001_017842_gtFine_labelIds.png', 20)
('frankfurt_000001_018113_leftImg8bit.png', 'frankfurt_000001_018113_gtFine_labelIds.png', 20)
('frankfurt_000001_019698_leftImg8bit.png', 'frankfurt_000001_019698_gtFine_labelIds.png', 20)
('frankfurt_000001_019854_leftImg8bit.png', 'frankfurt_000001_019854_gtFine_labelIds.png', 20)
('frankfurt_000001_019969_leftImg8bit.png', 'frankfurt_000001_019969_gtFine_labelIds.png', 21)
('frankfurt_000001_020046_leftImg8bit.png', 'frankfurt_000001_020046_gtFine_labelIds.png', 21)
('frankfurt_000001_020287_leftImg8bit.png', 'frankfurt_000001_020287_gtFine_labelIds.png', 21)
('frankfurt_000001_020693_leftImg8bit.png', 'frankfurt_000001_020693_gtFine_labelIds.png', 21)
('frankfurt_000001_021406_leftImg8bit.png', 'frankfurt_000001_021406_gtFine_labelIds.png', 21)
('frankfurt_000001_021825_leftImg8bit.png', 'frankfurt_000001_021825_gtFine_labelIds.png', 22)
('frankfurt_000001_023235_leftImg8bit.png', 'frankfurt_000001_023235_gtFine_labelIds.png', 22)
('frankfurt_000001_023369_leftImg8bit.png', 'frankfurt_000001_023369_gtFine_labelIds.png', 22)
('frankfurt_000001_023769_leftImg8bit.png', 'frankfurt_000001_023769_gtFine_labelIds.png', 22)
('frankfurt_000001_024927_leftImg8bit.png', 'frankfurt_000001_024927_gtFine_labelIds.png', 22)
('frankfurt_000001_025512_leftImg8bit.png', 'frankfurt_000001_025512_gtFine_labelIds.png', 23)
('frankfurt_000001_025713_leftImg8bit.png', 'frankfurt_000001_025713_gtFine_labelIds.png', 23)
('frankfurt_000001_025921_leftImg8bit.png', 'frankfurt_000001_025921_gtFine_labelIds.png', 23)
('frankfurt_000001_027325_leftImg8bit.png', 'frankfurt_000001_027325_gtFine_labelIds.png', 23)
('frankfurt_000001_028232_leftImg8bit.png', 'frankfurt_000001_028232_gtFine_labelIds.png', 23)
('frankfurt_000001_028335_leftImg8bit.png', 'frankfurt_000001_028335_gtFine_labelIds.png', 24)
('frankfurt_000001_028590_leftImg8bit.png', 'frankfurt_000001_028590_gtFine_labelIds.png', 24)
('frankfurt_000001_028854_leftImg8bit.png', 'frankfurt_000001_028854_gtFine_labelIds.png', 24)
('frankfurt_000001_029086_leftImg8bit.png', 'frankfurt_000001_029086_gtFine_labelIds.png', 24)
('frankfurt_000001_029236_leftImg8bit.png', 'frankfurt_000001_029236_gtFine_labelIds.png', 24)
pixel_accuracy=0.969829148462, mean_iou=0.8407784266, iou=[ 0.95095913  0.95686799  0.78552621  0.62466419  0.88587461]
('frankfurt_000001_029600_leftImg8bit.png', 'frankfurt_000001_029600_gtFine_labelIds.png', 25)
('frankfurt_000001_030067_leftImg8bit.png', 'frankfurt_000001_030067_gtFine_labelIds.png', 25)
('frankfurt_000001_030310_leftImg8bit.png', 'frankfurt_000001_030310_gtFine_labelIds.png', 25)
('frankfurt_000001_030669_leftImg8bit.png', 'frankfurt_000001_030669_gtFine_labelIds.png', 25)
('frankfurt_000001_031266_leftImg8bit.png', 'frankfurt_000001_031266_gtFine_labelIds.png', 25)
('frankfurt_000001_031416_leftImg8bit.png', 'frankfurt_000001_031416_gtFine_labelIds.png', 26)
('frankfurt_000001_032018_leftImg8bit.png', 'frankfurt_000001_032018_gtFine_labelIds.png', 26)
('frankfurt_000001_032556_leftImg8bit.png', 'frankfurt_000001_032556_gtFine_labelIds.png', 26)
('frankfurt_000001_032711_leftImg8bit.png', 'frankfurt_000001_032711_gtFine_labelIds.png', 26)
('frankfurt_000001_032942_leftImg8bit.png', 'frankfurt_000001_032942_gtFine_labelIds.png', 26)
('frankfurt_000001_033655_leftImg8bit.png', 'frankfurt_000001_033655_gtFine_labelIds.png', 27)
('frankfurt_000001_034047_leftImg8bit.png', 'frankfurt_000001_034047_gtFine_labelIds.png', 27)
('frankfurt_000001_034816_leftImg8bit.png', 'frankfurt_000001_034816_gtFine_labelIds.png', 27)
('frankfurt_000001_035144_leftImg8bit.png', 'frankfurt_000001_035144_gtFine_labelIds.png', 27)
('frankfurt_000001_035864_leftImg8bit.png', 'frankfurt_000001_035864_gtFine_labelIds.png', 27)
('frankfurt_000001_037705_leftImg8bit.png', 'frankfurt_000001_037705_gtFine_labelIds.png', 28)
('frankfurt_000001_038245_leftImg8bit.png', 'frankfurt_000001_038245_gtFine_labelIds.png', 28)
('frankfurt_000001_038418_leftImg8bit.png', 'frankfurt_000001_038418_gtFine_labelIds.png', 28)
('frankfurt_000001_038645_leftImg8bit.png', 'frankfurt_000001_038645_gtFine_labelIds.png', 28)
('frankfurt_000001_038844_leftImg8bit.png', 'frankfurt_000001_038844_gtFine_labelIds.png', 28)
('frankfurt_000001_039895_leftImg8bit.png', 'frankfurt_000001_039895_gtFine_labelIds.png', 29)
('frankfurt_000001_040575_leftImg8bit.png', 'frankfurt_000001_040575_gtFine_labelIds.png', 29)
('frankfurt_000001_040732_leftImg8bit.png', 'frankfurt_000001_040732_gtFine_labelIds.png', 29)
('frankfurt_000001_041074_leftImg8bit.png', 'frankfurt_000001_041074_gtFine_labelIds.png', 29)
('frankfurt_000001_041354_leftImg8bit.png', 'frankfurt_000001_041354_gtFine_labelIds.png', 29)
pixel_accuracy=0.970399632244, mean_iou=0.839846558827, iou=[ 0.95164908  0.95867018  0.77524078  0.6197995   0.89387326]
('frankfurt_000001_041517_leftImg8bit.png', 'frankfurt_000001_041517_gtFine_labelIds.png', 30)
('frankfurt_000001_041664_leftImg8bit.png', 'frankfurt_000001_041664_gtFine_labelIds.png', 30)
('frankfurt_000001_042098_leftImg8bit.png', 'frankfurt_000001_042098_gtFine_labelIds.png', 30)
('frankfurt_000001_042384_leftImg8bit.png', 'frankfurt_000001_042384_gtFine_labelIds.png', 30)
('frankfurt_000001_042733_leftImg8bit.png', 'frankfurt_000001_042733_gtFine_labelIds.png', 30)
('frankfurt_000001_043395_leftImg8bit.png', 'frankfurt_000001_043395_gtFine_labelIds.png', 31)
('frankfurt_000001_043564_leftImg8bit.png', 'frankfurt_000001_043564_gtFine_labelIds.png', 31)
('frankfurt_000001_044227_leftImg8bit.png', 'frankfurt_000001_044227_gtFine_labelIds.png', 31)
('frankfurt_000001_044413_leftImg8bit.png', 'frankfurt_000001_044413_gtFine_labelIds.png', 31)
('frankfurt_000001_044525_leftImg8bit.png', 'frankfurt_000001_044525_gtFine_labelIds.png', 31)
('frankfurt_000001_044658_leftImg8bit.png', 'frankfurt_000001_044658_gtFine_labelIds.png', 32)
('frankfurt_000001_044787_leftImg8bit.png', 'frankfurt_000001_044787_gtFine_labelIds.png', 32)
('frankfurt_000001_046126_leftImg8bit.png', 'frankfurt_000001_046126_gtFine_labelIds.png', 32)
('frankfurt_000001_046272_leftImg8bit.png', 'frankfurt_000001_046272_gtFine_labelIds.png', 32)
('frankfurt_000001_046504_leftImg8bit.png', 'frankfurt_000001_046504_gtFine_labelIds.png', 32)
('frankfurt_000001_046779_leftImg8bit.png', 'frankfurt_000001_046779_gtFine_labelIds.png', 33)
('frankfurt_000001_047178_leftImg8bit.png', 'frankfurt_000001_047178_gtFine_labelIds.png', 33)
('frankfurt_000001_047552_leftImg8bit.png', 'frankfurt_000001_047552_gtFine_labelIds.png', 33)
('frankfurt_000001_048196_leftImg8bit.png', 'frankfurt_000001_048196_gtFine_labelIds.png', 33)
('frankfurt_000001_048355_leftImg8bit.png', 'frankfurt_000001_048355_gtFine_labelIds.png', 33)
('frankfurt_000001_048654_leftImg8bit.png', 'frankfurt_000001_048654_gtFine_labelIds.png', 34)
('frankfurt_000001_049078_leftImg8bit.png', 'frankfurt_000001_049078_gtFine_labelIds.png', 34)
('frankfurt_000001_049209_leftImg8bit.png', 'frankfurt_000001_049209_gtFine_labelIds.png', 34)
('frankfurt_000001_049298_leftImg8bit.png', 'frankfurt_000001_049298_gtFine_labelIds.png', 34)
('frankfurt_000001_049698_leftImg8bit.png', 'frankfurt_000001_049698_gtFine_labelIds.png', 34)
pixel_accuracy=0.971826848895, mean_iou=0.841308593818, iou=[ 0.95410677  0.96075466  0.77492692  0.6153653   0.90138933]
('frankfurt_000001_049770_leftImg8bit.png', 'frankfurt_000001_049770_gtFine_labelIds.png', 35)
('frankfurt_000001_050149_leftImg8bit.png', 'frankfurt_000001_050149_gtFine_labelIds.png', 35)
('frankfurt_000001_050686_leftImg8bit.png', 'frankfurt_000001_050686_gtFine_labelIds.png', 35)
('frankfurt_000001_051516_leftImg8bit.png', 'frankfurt_000001_051516_gtFine_labelIds.png', 35)
('frankfurt_000001_051737_leftImg8bit.png', 'frankfurt_000001_051737_gtFine_labelIds.png', 35)
('frankfurt_000001_051807_leftImg8bit.png', 'frankfurt_000001_051807_gtFine_labelIds.png', 36)
('frankfurt_000001_052120_leftImg8bit.png', 'frankfurt_000001_052120_gtFine_labelIds.png', 36)
('frankfurt_000001_052594_leftImg8bit.png', 'frankfurt_000001_052594_gtFine_labelIds.png', 36)
('frankfurt_000001_053102_leftImg8bit.png', 'frankfurt_000001_053102_gtFine_labelIds.png', 36)
('frankfurt_000001_054077_leftImg8bit.png', 'frankfurt_000001_054077_gtFine_labelIds.png', 36)
('frankfurt_000001_054219_leftImg8bit.png', 'frankfurt_000001_054219_gtFine_labelIds.png', 37)
('frankfurt_000001_054415_leftImg8bit.png', 'frankfurt_000001_054415_gtFine_labelIds.png', 37)
('frankfurt_000001_054640_leftImg8bit.png', 'frankfurt_000001_054640_gtFine_labelIds.png', 37)
('frankfurt_000001_054884_leftImg8bit.png', 'frankfurt_000001_054884_gtFine_labelIds.png', 37)
('frankfurt_000001_055062_leftImg8bit.png', 'frankfurt_000001_055062_gtFine_labelIds.png', 37)
('frankfurt_000001_055172_leftImg8bit.png', 'frankfurt_000001_055172_gtFine_labelIds.png', 38)
('frankfurt_000001_055306_leftImg8bit.png', 'frankfurt_000001_055306_gtFine_labelIds.png', 38)
('frankfurt_000001_055387_leftImg8bit.png', 'frankfurt_000001_055387_gtFine_labelIds.png', 38)
('frankfurt_000001_055538_leftImg8bit.png', 'frankfurt_000001_055538_gtFine_labelIds.png', 38)
('frankfurt_000001_055603_leftImg8bit.png', 'frankfurt_000001_055603_gtFine_labelIds.png', 38)
('frankfurt_000001_055709_leftImg8bit.png', 'frankfurt_000001_055709_gtFine_labelIds.png', 39)
('frankfurt_000001_056580_leftImg8bit.png', 'frankfurt_000001_056580_gtFine_labelIds.png', 39)
('frankfurt_000001_057181_leftImg8bit.png', 'frankfurt_000001_057181_gtFine_labelIds.png', 39)
('frankfurt_000001_057478_leftImg8bit.png', 'frankfurt_000001_057478_gtFine_labelIds.png', 39)
('frankfurt_000001_057954_leftImg8bit.png', 'frankfurt_000001_057954_gtFine_labelIds.png', 39)
pixel_accuracy=0.97147181353, mean_iou=0.843970618086, iou=[ 0.9534511   0.96125984  0.78457955  0.61906035  0.90150225]
('frankfurt_000001_058057_leftImg8bit.png', 'frankfurt_000001_058057_gtFine_labelIds.png', 40)
('frankfurt_000001_058176_leftImg8bit.png', 'frankfurt_000001_058176_gtFine_labelIds.png', 40)
('frankfurt_000001_058504_leftImg8bit.png', 'frankfurt_000001_058504_gtFine_labelIds.png', 40)
('frankfurt_000001_058914_leftImg8bit.png', 'frankfurt_000001_058914_gtFine_labelIds.png', 40)
('frankfurt_000001_059119_leftImg8bit.png', 'frankfurt_000001_059119_gtFine_labelIds.png', 40)
('frankfurt_000001_059642_leftImg8bit.png', 'frankfurt_000001_059642_gtFine_labelIds.png', 41)
('frankfurt_000001_059789_leftImg8bit.png', 'frankfurt_000001_059789_gtFine_labelIds.png', 41)
('frankfurt_000001_060135_leftImg8bit.png', 'frankfurt_000001_060135_gtFine_labelIds.png', 41)
('frankfurt_000001_060422_leftImg8bit.png', 'frankfurt_000001_060422_gtFine_labelIds.png', 41)
('frankfurt_000001_060545_leftImg8bit.png', 'frankfurt_000001_060545_gtFine_labelIds.png', 41)
('frankfurt_000001_060906_leftImg8bit.png', 'frankfurt_000001_060906_gtFine_labelIds.png', 42)
('frankfurt_000001_061682_leftImg8bit.png', 'frankfurt_000001_061682_gtFine_labelIds.png', 42)
('frankfurt_000001_061763_leftImg8bit.png', 'frankfurt_000001_061763_gtFine_labelIds.png', 42)
('frankfurt_000001_062016_leftImg8bit.png', 'frankfurt_000001_062016_gtFine_labelIds.png', 42)
('frankfurt_000001_062250_leftImg8bit.png', 'frankfurt_000001_062250_gtFine_labelIds.png', 42)
('frankfurt_000001_062396_leftImg8bit.png', 'frankfurt_000001_062396_gtFine_labelIds.png', 43)
('frankfurt_000001_062509_leftImg8bit.png', 'frankfurt_000001_062509_gtFine_labelIds.png', 43)
('frankfurt_000001_062653_leftImg8bit.png', 'frankfurt_000001_062653_gtFine_labelIds.png', 43)
('frankfurt_000001_062793_leftImg8bit.png', 'frankfurt_000001_062793_gtFine_labelIds.png', 43)
('frankfurt_000001_063045_leftImg8bit.png', 'frankfurt_000001_063045_gtFine_labelIds.png', 43)
('frankfurt_000001_064130_leftImg8bit.png', 'frankfurt_000001_064130_gtFine_labelIds.png', 44)
('frankfurt_000001_064305_leftImg8bit.png', 'frankfurt_000001_064305_gtFine_labelIds.png', 44)
('frankfurt_000001_064651_leftImg8bit.png', 'frankfurt_000001_064651_gtFine_labelIds.png', 44)
('frankfurt_000001_064798_leftImg8bit.png', 'frankfurt_000001_064798_gtFine_labelIds.png', 44)
('frankfurt_000001_064925_leftImg8bit.png', 'frankfurt_000001_064925_gtFine_labelIds.png', 44)
pixel_accuracy=0.972165665566, mean_iou=0.843972935061, iou=[ 0.9546116   0.96292011  0.78016491  0.6181776   0.90399045]
('frankfurt_000001_065160_leftImg8bit.png', 'frankfurt_000001_065160_gtFine_labelIds.png', 45)
('frankfurt_000001_065617_leftImg8bit.png', 'frankfurt_000001_065617_gtFine_labelIds.png', 45)
('frankfurt_000001_065850_leftImg8bit.png', 'frankfurt_000001_065850_gtFine_labelIds.png', 45)
('frankfurt_000001_066092_leftImg8bit.png', 'frankfurt_000001_066092_gtFine_labelIds.png', 45)
('frankfurt_000001_066438_leftImg8bit.png', 'frankfurt_000001_066438_gtFine_labelIds.png', 45)
('frankfurt_000001_066574_leftImg8bit.png', 'frankfurt_000001_066574_gtFine_labelIds.png', 46)
('frankfurt_000001_066832_leftImg8bit.png', 'frankfurt_000001_066832_gtFine_labelIds.png', 46)
('frankfurt_000001_067092_leftImg8bit.png', 'frankfurt_000001_067092_gtFine_labelIds.png', 46)
('frankfurt_000001_067178_leftImg8bit.png', 'frankfurt_000001_067178_gtFine_labelIds.png', 46)
('frankfurt_000001_067295_leftImg8bit.png', 'frankfurt_000001_067295_gtFine_labelIds.png', 46)
('frankfurt_000001_067474_leftImg8bit.png', 'frankfurt_000001_067474_gtFine_labelIds.png', 47)
('frankfurt_000001_067735_leftImg8bit.png', 'frankfurt_000001_067735_gtFine_labelIds.png', 47)
('frankfurt_000001_068063_leftImg8bit.png', 'frankfurt_000001_068063_gtFine_labelIds.png', 47)
('frankfurt_000001_068208_leftImg8bit.png', 'frankfurt_000001_068208_gtFine_labelIds.png', 47)
('frankfurt_000001_068682_leftImg8bit.png', 'frankfurt_000001_068682_gtFine_labelIds.png', 47)
('frankfurt_000001_068772_leftImg8bit.png', 'frankfurt_000001_068772_gtFine_labelIds.png', 48)
('frankfurt_000001_069633_leftImg8bit.png', 'frankfurt_000001_069633_gtFine_labelIds.png', 48)
('frankfurt_000001_070099_leftImg8bit.png', 'frankfurt_000001_070099_gtFine_labelIds.png', 48)
('frankfurt_000001_071288_leftImg8bit.png', 'frankfurt_000001_071288_gtFine_labelIds.png', 48)
('frankfurt_000001_071781_leftImg8bit.png', 'frankfurt_000001_071781_gtFine_labelIds.png', 48)
('frankfurt_000001_072155_leftImg8bit.png', 'frankfurt_000001_072155_gtFine_labelIds.png', 49)
('frankfurt_000001_072295_leftImg8bit.png', 'frankfurt_000001_072295_gtFine_labelIds.png', 49)
('frankfurt_000001_073088_leftImg8bit.png', 'frankfurt_000001_073088_gtFine_labelIds.png', 49)
('frankfurt_000001_073243_leftImg8bit.png', 'frankfurt_000001_073243_gtFine_labelIds.png', 49)
('frankfurt_000001_073464_leftImg8bit.png', 'frankfurt_000001_073464_gtFine_labelIds.png', 49)
pixel_accuracy=0.971554478501, mean_iou=0.841843411668, iou=[ 0.95345149  0.96149347  0.77825049  0.61172892  0.90429269]
('frankfurt_000001_073911_leftImg8bit.png', 'frankfurt_000001_073911_gtFine_labelIds.png', 50)
('frankfurt_000001_075296_leftImg8bit.png', 'frankfurt_000001_075296_gtFine_labelIds.png', 50)
('frankfurt_000001_075984_leftImg8bit.png', 'frankfurt_000001_075984_gtFine_labelIds.png', 50)
('frankfurt_000001_076502_leftImg8bit.png', 'frankfurt_000001_076502_gtFine_labelIds.png', 50)
('frankfurt_000001_077092_leftImg8bit.png', 'frankfurt_000001_077092_gtFine_labelIds.png', 50)
('frankfurt_000001_077233_leftImg8bit.png', 'frankfurt_000001_077233_gtFine_labelIds.png', 51)
('frankfurt_000001_077434_leftImg8bit.png', 'frankfurt_000001_077434_gtFine_labelIds.png', 51)
('frankfurt_000001_078803_leftImg8bit.png', 'frankfurt_000001_078803_gtFine_labelIds.png', 51)
('frankfurt_000001_079206_leftImg8bit.png', 'frankfurt_000001_079206_gtFine_labelIds.png', 51)
('frankfurt_000001_080091_leftImg8bit.png', 'frankfurt_000001_080091_gtFine_labelIds.png', 51)
('frankfurt_000001_080391_leftImg8bit.png', 'frankfurt_000001_080391_gtFine_labelIds.png', 52)
('frankfurt_000001_080830_leftImg8bit.png', 'frankfurt_000001_080830_gtFine_labelIds.png', 52)
('frankfurt_000001_082087_leftImg8bit.png', 'frankfurt_000001_082087_gtFine_labelIds.png', 52)
('frankfurt_000001_082466_leftImg8bit.png', 'frankfurt_000001_082466_gtFine_labelIds.png', 52)
('frankfurt_000001_083029_leftImg8bit.png', 'frankfurt_000001_083029_gtFine_labelIds.png', 52)
('frankfurt_000001_083199_leftImg8bit.png', 'frankfurt_000001_083199_gtFine_labelIds.png', 53)
('frankfurt_000001_083852_leftImg8bit.png', 'frankfurt_000001_083852_gtFine_labelIds.png', 53)
('lindau_000000_000019_leftImg8bit.png', 'lindau_000000_000019_gtFine_labelIds.png', 53)
('lindau_000001_000019_leftImg8bit.png', 'lindau_000001_000019_gtFine_labelIds.png', 53)
('lindau_000002_000019_leftImg8bit.png', 'lindau_000002_000019_gtFine_labelIds.png', 53)
('lindau_000003_000019_leftImg8bit.png', 'lindau_000003_000019_gtFine_labelIds.png', 54)
('lindau_000004_000019_leftImg8bit.png', 'lindau_000004_000019_gtFine_labelIds.png', 54)
('lindau_000005_000019_leftImg8bit.png', 'lindau_000005_000019_gtFine_labelIds.png', 54)
('lindau_000006_000019_leftImg8bit.png', 'lindau_000006_000019_gtFine_labelIds.png', 54)
('lindau_000007_000019_leftImg8bit.png', 'lindau_000007_000019_gtFine_labelIds.png', 54)
pixel_accuracy=0.97188773219, mean_iou=0.840700260474, iou=[ 0.95400066  0.96212386  0.77387782  0.60944114  0.90405783]
('lindau_000008_000019_leftImg8bit.png', 'lindau_000008_000019_gtFine_labelIds.png', 55)
('lindau_000009_000019_leftImg8bit.png', 'lindau_000009_000019_gtFine_labelIds.png', 55)
('lindau_000010_000019_leftImg8bit.png', 'lindau_000010_000019_gtFine_labelIds.png', 55)
('lindau_000011_000019_leftImg8bit.png', 'lindau_000011_000019_gtFine_labelIds.png', 55)
('lindau_000012_000019_leftImg8bit.png', 'lindau_000012_000019_gtFine_labelIds.png', 55)
('lindau_000013_000019_leftImg8bit.png', 'lindau_000013_000019_gtFine_labelIds.png', 56)
('lindau_000014_000019_leftImg8bit.png', 'lindau_000014_000019_gtFine_labelIds.png', 56)
('lindau_000015_000019_leftImg8bit.png', 'lindau_000015_000019_gtFine_labelIds.png', 56)
('lindau_000016_000019_leftImg8bit.png', 'lindau_000016_000019_gtFine_labelIds.png', 56)
('lindau_000017_000019_leftImg8bit.png', 'lindau_000017_000019_gtFine_labelIds.png', 56)
('lindau_000018_000019_leftImg8bit.png', 'lindau_000018_000019_gtFine_labelIds.png', 57)
('lindau_000019_000019_leftImg8bit.png', 'lindau_000019_000019_gtFine_labelIds.png', 57)
('lindau_000020_000019_leftImg8bit.png', 'lindau_000020_000019_gtFine_labelIds.png', 57)
('lindau_000021_000019_leftImg8bit.png', 'lindau_000021_000019_gtFine_labelIds.png', 57)
('lindau_000022_000019_leftImg8bit.png', 'lindau_000022_000019_gtFine_labelIds.png', 57)
('lindau_000023_000019_leftImg8bit.png', 'lindau_000023_000019_gtFine_labelIds.png', 58)
('lindau_000024_000019_leftImg8bit.png', 'lindau_000024_000019_gtFine_labelIds.png', 58)
('lindau_000025_000019_leftImg8bit.png', 'lindau_000025_000019_gtFine_labelIds.png', 58)
('lindau_000026_000019_leftImg8bit.png', 'lindau_000026_000019_gtFine_labelIds.png', 58)
('lindau_000027_000019_leftImg8bit.png', 'lindau_000027_000019_gtFine_labelIds.png', 58)
('lindau_000028_000019_leftImg8bit.png', 'lindau_000028_000019_gtFine_labelIds.png', 59)
('lindau_000029_000019_leftImg8bit.png', 'lindau_000029_000019_gtFine_labelIds.png', 59)
('lindau_000030_000019_leftImg8bit.png', 'lindau_000030_000019_gtFine_labelIds.png', 59)
('lindau_000031_000019_leftImg8bit.png', 'lindau_000031_000019_gtFine_labelIds.png', 59)
('lindau_000032_000019_leftImg8bit.png', 'lindau_000032_000019_gtFine_labelIds.png', 59)
pixel_accuracy=0.96731822367, mean_iou=0.836409640531, iou=[ 0.94692036  0.94779734  0.77115099  0.61516093  0.90101858]
('lindau_000033_000019_leftImg8bit.png', 'lindau_000033_000019_gtFine_labelIds.png', 60)
('lindau_000034_000019_leftImg8bit.png', 'lindau_000034_000019_gtFine_labelIds.png', 60)
('lindau_000035_000019_leftImg8bit.png', 'lindau_000035_000019_gtFine_labelIds.png', 60)
('lindau_000036_000019_leftImg8bit.png', 'lindau_000036_000019_gtFine_labelIds.png', 60)
('lindau_000037_000019_leftImg8bit.png', 'lindau_000037_000019_gtFine_labelIds.png', 60)
('lindau_000038_000019_leftImg8bit.png', 'lindau_000038_000019_gtFine_labelIds.png', 61)
('lindau_000039_000019_leftImg8bit.png', 'lindau_000039_000019_gtFine_labelIds.png', 61)
('lindau_000040_000019_leftImg8bit.png', 'lindau_000040_000019_gtFine_labelIds.png', 61)
('lindau_000041_000019_leftImg8bit.png', 'lindau_000041_000019_gtFine_labelIds.png', 61)
('lindau_000042_000019_leftImg8bit.png', 'lindau_000042_000019_gtFine_labelIds.png', 61)
('lindau_000043_000019_leftImg8bit.png', 'lindau_000043_000019_gtFine_labelIds.png', 62)
('lindau_000044_000019_leftImg8bit.png', 'lindau_000044_000019_gtFine_labelIds.png', 62)
('lindau_000045_000019_leftImg8bit.png', 'lindau_000045_000019_gtFine_labelIds.png', 62)
('lindau_000046_000019_leftImg8bit.png', 'lindau_000046_000019_gtFine_labelIds.png', 62)
('lindau_000047_000019_leftImg8bit.png', 'lindau_000047_000019_gtFine_labelIds.png', 62)
('lindau_000048_000019_leftImg8bit.png', 'lindau_000048_000019_gtFine_labelIds.png', 63)
('lindau_000049_000019_leftImg8bit.png', 'lindau_000049_000019_gtFine_labelIds.png', 63)
('lindau_000050_000019_leftImg8bit.png', 'lindau_000050_000019_gtFine_labelIds.png', 63)
('lindau_000051_000019_leftImg8bit.png', 'lindau_000051_000019_gtFine_labelIds.png', 63)
('lindau_000052_000019_leftImg8bit.png', 'lindau_000052_000019_gtFine_labelIds.png', 63)
('lindau_000053_000019_leftImg8bit.png', 'lindau_000053_000019_gtFine_labelIds.png', 64)
('lindau_000054_000019_leftImg8bit.png', 'lindau_000054_000019_gtFine_labelIds.png', 64)
('lindau_000055_000019_leftImg8bit.png', 'lindau_000055_000019_gtFine_labelIds.png', 64)
('lindau_000056_000019_leftImg8bit.png', 'lindau_000056_000019_gtFine_labelIds.png', 64)
('lindau_000057_000019_leftImg8bit.png', 'lindau_000057_000019_gtFine_labelIds.png', 64)
pixel_accuracy=0.958405435286, mean_iou=0.827825409707, iou=[ 0.93302602  0.92037946  0.76762549  0.61545232  0.90264376]
('lindau_000058_000019_leftImg8bit.png', 'lindau_000058_000019_gtFine_labelIds.png', 65)
('munster_000000_000019_leftImg8bit.png', 'munster_000000_000019_gtFine_labelIds.png', 65)
('munster_000001_000019_leftImg8bit.png', 'munster_000001_000019_gtFine_labelIds.png', 65)
('munster_000002_000019_leftImg8bit.png', 'munster_000002_000019_gtFine_labelIds.png', 65)
('munster_000003_000019_leftImg8bit.png', 'munster_000003_000019_gtFine_labelIds.png', 65)
('munster_000004_000019_leftImg8bit.png', 'munster_000004_000019_gtFine_labelIds.png', 66)
('munster_000005_000019_leftImg8bit.png', 'munster_000005_000019_gtFine_labelIds.png', 66)
('munster_000006_000019_leftImg8bit.png', 'munster_000006_000019_gtFine_labelIds.png', 66)
('munster_000007_000019_leftImg8bit.png', 'munster_000007_000019_gtFine_labelIds.png', 66)
('munster_000008_000019_leftImg8bit.png', 'munster_000008_000019_gtFine_labelIds.png', 66)
('munster_000009_000019_leftImg8bit.png', 'munster_000009_000019_gtFine_labelIds.png', 67)
('munster_000010_000019_leftImg8bit.png', 'munster_000010_000019_gtFine_labelIds.png', 67)
('munster_000011_000019_leftImg8bit.png', 'munster_000011_000019_gtFine_labelIds.png', 67)
('munster_000012_000019_leftImg8bit.png', 'munster_000012_000019_gtFine_labelIds.png', 67)
('munster_000013_000019_leftImg8bit.png', 'munster_000013_000019_gtFine_labelIds.png', 67)
('munster_000014_000019_leftImg8bit.png', 'munster_000014_000019_gtFine_labelIds.png', 68)
('munster_000015_000019_leftImg8bit.png', 'munster_000015_000019_gtFine_labelIds.png', 68)
('munster_000016_000019_leftImg8bit.png', 'munster_000016_000019_gtFine_labelIds.png', 68)
('munster_000017_000019_leftImg8bit.png', 'munster_000017_000019_gtFine_labelIds.png', 68)
('munster_000018_000019_leftImg8bit.png', 'munster_000018_000019_gtFine_labelIds.png', 68)
('munster_000019_000019_leftImg8bit.png', 'munster_000019_000019_gtFine_labelIds.png', 69)
('munster_000020_000019_leftImg8bit.png', 'munster_000020_000019_gtFine_labelIds.png', 69)
('munster_000021_000019_leftImg8bit.png', 'munster_000021_000019_gtFine_labelIds.png', 69)
('munster_000022_000019_leftImg8bit.png', 'munster_000022_000019_gtFine_labelIds.png', 69)
('munster_000023_000019_leftImg8bit.png', 'munster_000023_000019_gtFine_labelIds.png', 69)
pixel_accuracy=0.960045412699, mean_iou=0.830461133707, iou=[ 0.9355459   0.92424638  0.76665229  0.6204114   0.9054497 ]
('munster_000024_000019_leftImg8bit.png', 'munster_000024_000019_gtFine_labelIds.png', 70)
('munster_000025_000019_leftImg8bit.png', 'munster_000025_000019_gtFine_labelIds.png', 70)
('munster_000026_000019_leftImg8bit.png', 'munster_000026_000019_gtFine_labelIds.png', 70)
('munster_000027_000019_leftImg8bit.png', 'munster_000027_000019_gtFine_labelIds.png', 70)
('munster_000028_000019_leftImg8bit.png', 'munster_000028_000019_gtFine_labelIds.png', 70)
('munster_000029_000019_leftImg8bit.png', 'munster_000029_000019_gtFine_labelIds.png', 71)
('munster_000030_000019_leftImg8bit.png', 'munster_000030_000019_gtFine_labelIds.png', 71)
('munster_000031_000019_leftImg8bit.png', 'munster_000031_000019_gtFine_labelIds.png', 71)
('munster_000032_000019_leftImg8bit.png', 'munster_000032_000019_gtFine_labelIds.png', 71)
('munster_000033_000019_leftImg8bit.png', 'munster_000033_000019_gtFine_labelIds.png', 71)
('munster_000034_000019_leftImg8bit.png', 'munster_000034_000019_gtFine_labelIds.png', 72)
('munster_000035_000019_leftImg8bit.png', 'munster_000035_000019_gtFine_labelIds.png', 72)
('munster_000036_000019_leftImg8bit.png', 'munster_000036_000019_gtFine_labelIds.png', 72)
('munster_000037_000019_leftImg8bit.png', 'munster_000037_000019_gtFine_labelIds.png', 72)
('munster_000038_000019_leftImg8bit.png', 'munster_000038_000019_gtFine_labelIds.png', 72)
('munster_000039_000019_leftImg8bit.png', 'munster_000039_000019_gtFine_labelIds.png', 73)
('munster_000040_000019_leftImg8bit.png', 'munster_000040_000019_gtFine_labelIds.png', 73)
('munster_000041_000019_leftImg8bit.png', 'munster_000041_000019_gtFine_labelIds.png', 73)
('munster_000042_000019_leftImg8bit.png', 'munster_000042_000019_gtFine_labelIds.png', 73)
('munster_000043_000019_leftImg8bit.png', 'munster_000043_000019_gtFine_labelIds.png', 73)
('munster_000044_000019_leftImg8bit.png', 'munster_000044_000019_gtFine_labelIds.png', 74)
('munster_000045_000019_leftImg8bit.png', 'munster_000045_000019_gtFine_labelIds.png', 74)
('munster_000046_000019_leftImg8bit.png', 'munster_000046_000019_gtFine_labelIds.png', 74)
('munster_000047_000019_leftImg8bit.png', 'munster_000047_000019_gtFine_labelIds.png', 74)
('munster_000048_000019_leftImg8bit.png', 'munster_000048_000019_gtFine_labelIds.png', 74)
pixel_accuracy=0.961181643347, mean_iou=0.833986862354, iou=[ 0.93737181  0.92718015  0.77477301  0.62557802  0.90503132]
('munster_000049_000019_leftImg8bit.png', 'munster_000049_000019_gtFine_labelIds.png', 75)
('munster_000050_000019_leftImg8bit.png', 'munster_000050_000019_gtFine_labelIds.png', 75)
('munster_000051_000019_leftImg8bit.png', 'munster_000051_000019_gtFine_labelIds.png', 75)
('munster_000052_000019_leftImg8bit.png', 'munster_000052_000019_gtFine_labelIds.png', 75)
('munster_000053_000019_leftImg8bit.png', 'munster_000053_000019_gtFine_labelIds.png', 75)
('munster_000054_000019_leftImg8bit.png', 'munster_000054_000019_gtFine_labelIds.png', 76)
('munster_000055_000019_leftImg8bit.png', 'munster_000055_000019_gtFine_labelIds.png', 76)
('munster_000056_000019_leftImg8bit.png', 'munster_000056_000019_gtFine_labelIds.png', 76)
('munster_000057_000019_leftImg8bit.png', 'munster_000057_000019_gtFine_labelIds.png', 76)
('munster_000058_000019_leftImg8bit.png', 'munster_000058_000019_gtFine_labelIds.png', 76)
('munster_000059_000019_leftImg8bit.png', 'munster_000059_000019_gtFine_labelIds.png', 77)
('munster_000060_000019_leftImg8bit.png', 'munster_000060_000019_gtFine_labelIds.png', 77)
('munster_000061_000019_leftImg8bit.png', 'munster_000061_000019_gtFine_labelIds.png', 77)
('munster_000062_000019_leftImg8bit.png', 'munster_000062_000019_gtFine_labelIds.png', 77)
('munster_000063_000019_leftImg8bit.png', 'munster_000063_000019_gtFine_labelIds.png', 77)
('munster_000064_000019_leftImg8bit.png', 'munster_000064_000019_gtFine_labelIds.png', 78)
('munster_000065_000019_leftImg8bit.png', 'munster_000065_000019_gtFine_labelIds.png', 78)
('munster_000066_000019_leftImg8bit.png', 'munster_000066_000019_gtFine_labelIds.png', 78)
('munster_000067_000019_leftImg8bit.png', 'munster_000067_000019_gtFine_labelIds.png', 78)
('munster_000068_000019_leftImg8bit.png', 'munster_000068_000019_gtFine_labelIds.png', 78)
('munster_000069_000019_leftImg8bit.png', 'munster_000069_000019_gtFine_labelIds.png', 79)
('munster_000070_000019_leftImg8bit.png', 'munster_000070_000019_gtFine_labelIds.png', 79)
('munster_000071_000019_leftImg8bit.png', 'munster_000071_000019_gtFine_labelIds.png', 79)
('munster_000072_000019_leftImg8bit.png', 'munster_000072_000019_gtFine_labelIds.png', 79)
('munster_000073_000019_leftImg8bit.png', 'munster_000073_000019_gtFine_labelIds.png', 79)
pixel_accuracy=0.96228167395, mean_iou=0.836059045034, iou=[ 0.93905244  0.93027608  0.77454606  0.62946785  0.9069528 ]
('munster_000074_000019_leftImg8bit.png', 'munster_000074_000019_gtFine_labelIds.png', 80)
('munster_000075_000019_leftImg8bit.png', 'munster_000075_000019_gtFine_labelIds.png', 80)
('munster_000076_000019_leftImg8bit.png', 'munster_000076_000019_gtFine_labelIds.png', 80)
('munster_000077_000019_leftImg8bit.png', 'munster_000077_000019_gtFine_labelIds.png', 80)
('munster_000078_000019_leftImg8bit.png', 'munster_000078_000019_gtFine_labelIds.png', 80)
('munster_000079_000019_leftImg8bit.png', 'munster_000079_000019_gtFine_labelIds.png', 81)
('munster_000080_000019_leftImg8bit.png', 'munster_000080_000019_gtFine_labelIds.png', 81)
('munster_000081_000019_leftImg8bit.png', 'munster_000081_000019_gtFine_labelIds.png', 81)
('munster_000082_000019_leftImg8bit.png', 'munster_000082_000019_gtFine_labelIds.png', 81)
('munster_000083_000019_leftImg8bit.png', 'munster_000083_000019_gtFine_labelIds.png', 81)
('munster_000084_000019_leftImg8bit.png', 'munster_000084_000019_gtFine_labelIds.png', 82)
('munster_000085_000019_leftImg8bit.png', 'munster_000085_000019_gtFine_labelIds.png', 82)
('munster_000086_000019_leftImg8bit.png', 'munster_000086_000019_gtFine_labelIds.png', 82)
('munster_000087_000019_leftImg8bit.png', 'munster_000087_000019_gtFine_labelIds.png', 82)
('munster_000088_000019_leftImg8bit.png', 'munster_000088_000019_gtFine_labelIds.png', 82)
('munster_000089_000019_leftImg8bit.png', 'munster_000089_000019_gtFine_labelIds.png', 83)
('munster_000090_000019_leftImg8bit.png', 'munster_000090_000019_gtFine_labelIds.png', 83)
('munster_000091_000019_leftImg8bit.png', 'munster_000091_000019_gtFine_labelIds.png', 83)
('munster_000092_000019_leftImg8bit.png', 'munster_000092_000019_gtFine_labelIds.png', 83)
('munster_000093_000019_leftImg8bit.png', 'munster_000093_000019_gtFine_labelIds.png', 83)
('munster_000094_000019_leftImg8bit.png', 'munster_000094_000019_gtFine_labelIds.png', 84)
('munster_000095_000019_leftImg8bit.png', 'munster_000095_000019_gtFine_labelIds.png', 84)
('munster_000096_000019_leftImg8bit.png', 'munster_000096_000019_gtFine_labelIds.png', 84)
('munster_000097_000019_leftImg8bit.png', 'munster_000097_000019_gtFine_labelIds.png', 84)
('munster_000098_000019_leftImg8bit.png', 'munster_000098_000019_gtFine_labelIds.png', 84)
pixel_accuracy=0.96350338897, mean_iou=0.838305055609, iou=[ 0.94087553  0.93296163  0.77425953  0.63198063  0.91144796]
('munster_000099_000019_leftImg8bit.png', 'munster_000099_000019_gtFine_labelIds.png', 85)
('munster_000100_000019_leftImg8bit.png', 'munster_000100_000019_gtFine_labelIds.png', 85)
('munster_000101_000019_leftImg8bit.png', 'munster_000101_000019_gtFine_labelIds.png', 85)
('munster_000102_000019_leftImg8bit.png', 'munster_000102_000019_gtFine_labelIds.png', 85)
('munster_000103_000019_leftImg8bit.png', 'munster_000103_000019_gtFine_labelIds.png', 85)
('munster_000104_000019_leftImg8bit.png', 'munster_000104_000019_gtFine_labelIds.png', 86)
('munster_000105_000019_leftImg8bit.png', 'munster_000105_000019_gtFine_labelIds.png', 86)
('munster_000106_000019_leftImg8bit.png', 'munster_000106_000019_gtFine_labelIds.png', 86)
('munster_000107_000019_leftImg8bit.png', 'munster_000107_000019_gtFine_labelIds.png', 86)
('munster_000108_000019_leftImg8bit.png', 'munster_000108_000019_gtFine_labelIds.png', 86)
('munster_000109_000019_leftImg8bit.png', 'munster_000109_000019_gtFine_labelIds.png', 87)
('munster_000110_000019_leftImg8bit.png', 'munster_000110_000019_gtFine_labelIds.png', 87)
('munster_000111_000019_leftImg8bit.png', 'munster_000111_000019_gtFine_labelIds.png', 87)
('munster_000112_000019_leftImg8bit.png', 'munster_000112_000019_gtFine_labelIds.png', 87)
('munster_000113_000019_leftImg8bit.png', 'munster_000113_000019_gtFine_labelIds.png', 87)
('munster_000114_000019_leftImg8bit.png', 'munster_000114_000019_gtFine_labelIds.png', 88)
('munster_000115_000019_leftImg8bit.png', 'munster_000115_000019_gtFine_labelIds.png', 88)
('munster_000116_000019_leftImg8bit.png', 'munster_000116_000019_gtFine_labelIds.png', 88)
('munster_000117_000019_leftImg8bit.png', 'munster_000117_000019_gtFine_labelIds.png', 88)
('munster_000118_000019_leftImg8bit.png', 'munster_000118_000019_gtFine_labelIds.png', 88)
('munster_000119_000019_leftImg8bit.png', 'munster_000119_000019_gtFine_labelIds.png', 89)
('munster_000120_000019_leftImg8bit.png', 'munster_000120_000019_gtFine_labelIds.png', 89)
('munster_000121_000019_leftImg8bit.png', 'munster_000121_000019_gtFine_labelIds.png', 89)
('munster_000122_000019_leftImg8bit.png', 'munster_000122_000019_gtFine_labelIds.png', 89)
('munster_000123_000019_leftImg8bit.png', 'munster_000123_000019_gtFine_labelIds.png', 89)
pixel_accuracy=0.964833869587, mean_iou=0.839492197605, iou=[ 0.94295531  0.93594266  0.77350557  0.63168469  0.91337275]
('munster_000124_000019_leftImg8bit.png', 'munster_000124_000019_gtFine_labelIds.png', 90)
('munster_000125_000019_leftImg8bit.png', 'munster_000125_000019_gtFine_labelIds.png', 90)
('munster_000126_000019_leftImg8bit.png', 'munster_000126_000019_gtFine_labelIds.png', 90)
('munster_000127_000019_leftImg8bit.png', 'munster_000127_000019_gtFine_labelIds.png', 90)
('munster_000128_000019_leftImg8bit.png', 'munster_000128_000019_gtFine_labelIds.png', 90)
('munster_000129_000019_leftImg8bit.png', 'munster_000129_000019_gtFine_labelIds.png', 91)
('munster_000130_000019_leftImg8bit.png', 'munster_000130_000019_gtFine_labelIds.png', 91)
('munster_000131_000019_leftImg8bit.png', 'munster_000131_000019_gtFine_labelIds.png', 91)
('munster_000132_000019_leftImg8bit.png', 'munster_000132_000019_gtFine_labelIds.png', 91)
('munster_000133_000019_leftImg8bit.png', 'munster_000133_000019_gtFine_labelIds.png', 91)
('munster_000134_000019_leftImg8bit.png', 'munster_000134_000019_gtFine_labelIds.png', 92)
('munster_000135_000019_leftImg8bit.png', 'munster_000135_000019_gtFine_labelIds.png', 92)
('munster_000136_000019_leftImg8bit.png', 'munster_000136_000019_gtFine_labelIds.png', 92)
('munster_000137_000019_leftImg8bit.png', 'munster_000137_000019_gtFine_labelIds.png', 92)
('munster_000138_000019_leftImg8bit.png', 'munster_000138_000019_gtFine_labelIds.png', 92)
('munster_000139_000019_leftImg8bit.png', 'munster_000139_000019_gtFine_labelIds.png', 93)
('munster_000140_000019_leftImg8bit.png', 'munster_000140_000019_gtFine_labelIds.png', 93)
('munster_000141_000019_leftImg8bit.png', 'munster_000141_000019_gtFine_labelIds.png', 93)
('munster_000142_000019_leftImg8bit.png', 'munster_000142_000019_gtFine_labelIds.png', 93)
('munster_000143_000019_leftImg8bit.png', 'munster_000143_000019_gtFine_labelIds.png', 93)
('munster_000144_000019_leftImg8bit.png', 'munster_000144_000019_gtFine_labelIds.png', 94)
('munster_000145_000019_leftImg8bit.png', 'munster_000145_000019_gtFine_labelIds.png', 94)
('munster_000146_000019_leftImg8bit.png', 'munster_000146_000019_gtFine_labelIds.png', 94)
('munster_000147_000019_leftImg8bit.png', 'munster_000147_000019_gtFine_labelIds.png', 94)
('munster_000148_000019_leftImg8bit.png', 'munster_000148_000019_gtFine_labelIds.png', 94)
pixel_accuracy=0.96160498692, mean_iou=0.837559893079, iou=[ 0.93768933  0.92676659  0.77828509  0.63134555  0.9137129 ]
('munster_000149_000019_leftImg8bit.png', 'munster_000149_000019_gtFine_labelIds.png', 95)
('munster_000150_000019_leftImg8bit.png', 'munster_000150_000019_gtFine_labelIds.png', 95)
('munster_000151_000019_leftImg8bit.png', 'munster_000151_000019_gtFine_labelIds.png', 95)
('munster_000152_000019_leftImg8bit.png', 'munster_000152_000019_gtFine_labelIds.png', 95)
('munster_000153_000019_leftImg8bit.png', 'munster_000153_000019_gtFine_labelIds.png', 95)
('munster_000154_000019_leftImg8bit.png', 'munster_000154_000019_gtFine_labelIds.png', 96)
('munster_000155_000019_leftImg8bit.png', 'munster_000155_000019_gtFine_labelIds.png', 96)
('munster_000156_000019_leftImg8bit.png', 'munster_000156_000019_gtFine_labelIds.png', 96)
('munster_000157_000019_leftImg8bit.png', 'munster_000157_000019_gtFine_labelIds.png', 96)
('munster_000158_000019_leftImg8bit.png', 'munster_000158_000019_gtFine_labelIds.png', 96)
('munster_000159_000019_leftImg8bit.png', 'munster_000159_000019_gtFine_labelIds.png', 97)
('munster_000160_000019_leftImg8bit.png', 'munster_000160_000019_gtFine_labelIds.png', 97)
('munster_000161_000019_leftImg8bit.png', 'munster_000161_000019_gtFine_labelIds.png', 97)
('munster_000162_000019_leftImg8bit.png', 'munster_000162_000019_gtFine_labelIds.png', 97)
('munster_000163_000019_leftImg8bit.png', 'munster_000163_000019_gtFine_labelIds.png', 97)
('munster_000164_000019_leftImg8bit.png', 'munster_000164_000019_gtFine_labelIds.png', 98)
('munster_000165_000019_leftImg8bit.png', 'munster_000165_000019_gtFine_labelIds.png', 98)
('munster_000166_000019_leftImg8bit.png', 'munster_000166_000019_gtFine_labelIds.png', 98)
('munster_000167_000019_leftImg8bit.png', 'munster_000167_000019_gtFine_labelIds.png', 98)
('munster_000168_000019_leftImg8bit.png', 'munster_000168_000019_gtFine_labelIds.png', 98)
('munster_000169_000019_leftImg8bit.png', 'munster_000169_000019_gtFine_labelIds.png', 99)
('munster_000170_000019_leftImg8bit.png', 'munster_000170_000019_gtFine_labelIds.png', 99)
('munster_000171_000019_leftImg8bit.png', 'munster_000171_000019_gtFine_labelIds.png', 99)
('munster_000172_000019_leftImg8bit.png', 'munster_000172_000019_gtFine_labelIds.png', 99)
('munster_000173_000019_leftImg8bit.png', 'munster_000173_000019_gtFine_labelIds.png', 99)
pixel_accuracy=0.962218842053, mean_iou=0.839370371168, iou=[ 0.9387087   0.92810479  0.77845766  0.637117    0.91446371]
-------------------------------------------------------------
Final: pixel_accuracy=0.962218842053, mean_iou=0.839370371168, iou=[ 0.9387087   0.92810479  0.77845766  0.637117    0.91446371]
-------------------------------------------------------------
l1reg eval.
I0917 00:28:57.914832 24374 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0917 00:28:57.915516 24374 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I0917 00:28:57.916087 24374 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0917 00:28:57.916654 24374 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I0917 00:28:57.918225 24374 upgrade_proto.cpp:66] Attempting to upgrade input file specified using deprecated input fields: ../trained/image_segmentation/cityscapes5_jsegnet21v2/sparse/deploy.prototxt
I0917 00:28:57.918238 24374 upgrade_proto.cpp:69] Successfully upgraded file specified using deprecated input fields.
W0917 00:28:57.918241 24374 upgrade_proto.cpp:71] Note that future Caffe releases will only support input layers and not input fields.
I0917 00:28:57.918509 24374 net.cpp:72] Initializing net from parameters: 
name: "jsegnet21v2_deploy"
state {
  phase: TEST
}
layer {
  name: "input"
  type: "Input"
  top: "data"
  input_param {
    shape {
      dim: 1
      dim: 3
      dim: 1024
      dim: 2048
    }
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "argMaxOut"
  type: "ArgMax"
  bottom: "out_deconv_final_up8"
  top: "argMaxOut"
  argmax_param {
    axis: 1
  }
}
I0917 00:28:57.918620 24374 net.cpp:104] Using FLOAT as default forward math type
I0917 00:28:57.918624 24374 net.cpp:110] Using FLOAT as default backward math type
I0917 00:28:57.918627 24374 layer_factory.hpp:136] Creating layer 'input' of type 'Input'
I0917 00:28:57.918629 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:57.918637 24374 net.cpp:184] Created Layer input (0)
I0917 00:28:57.918639 24374 net.cpp:530] input -> data
I0917 00:28:57.919268 24374 net.cpp:245] Setting up input
I0917 00:28:57.919277 24374 net.cpp:252] TEST Top shape for layer 0 'input' 1 3 1024 2048 (6291456)
I0917 00:28:57.919281 24374 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0917 00:28:57.919291 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:57.919299 24374 net.cpp:184] Created Layer data/bias (1)
I0917 00:28:57.919302 24374 net.cpp:561] data/bias <- data
I0917 00:28:57.919306 24374 net.cpp:530] data/bias -> data/bias
I0917 00:28:57.923583 24374 net.cpp:245] Setting up data/bias
I0917 00:28:57.923600 24374 net.cpp:252] TEST Top shape for layer 1 'data/bias' 1 3 1024 2048 (6291456)
I0917 00:28:57.923609 24374 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0917 00:28:57.923612 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:57.923629 24374 net.cpp:184] Created Layer conv1a (2)
I0917 00:28:57.923633 24374 net.cpp:561] conv1a <- data/bias
I0917 00:28:57.923636 24374 net.cpp:530] conv1a -> conv1a
I0917 00:28:58.214577 24374 net.cpp:245] Setting up conv1a
I0917 00:28:58.214598 24374 net.cpp:252] TEST Top shape for layer 2 'conv1a' 1 32 512 1024 (16777216)
I0917 00:28:58.214609 24374 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0917 00:28:58.214613 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.214622 24374 net.cpp:184] Created Layer conv1a/bn (3)
I0917 00:28:58.214625 24374 net.cpp:561] conv1a/bn <- conv1a
I0917 00:28:58.214628 24374 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0917 00:28:58.215430 24374 net.cpp:245] Setting up conv1a/bn
I0917 00:28:58.215438 24374 net.cpp:252] TEST Top shape for layer 3 'conv1a/bn' 1 32 512 1024 (16777216)
I0917 00:28:58.215445 24374 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0917 00:28:58.215447 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.215451 24374 net.cpp:184] Created Layer conv1a/relu (4)
I0917 00:28:58.215453 24374 net.cpp:561] conv1a/relu <- conv1a
I0917 00:28:58.215456 24374 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0917 00:28:58.215466 24374 net.cpp:245] Setting up conv1a/relu
I0917 00:28:58.215468 24374 net.cpp:252] TEST Top shape for layer 4 'conv1a/relu' 1 32 512 1024 (16777216)
I0917 00:28:58.215471 24374 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0917 00:28:58.215473 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.215482 24374 net.cpp:184] Created Layer conv1b (5)
I0917 00:28:58.215486 24374 net.cpp:561] conv1b <- conv1a
I0917 00:28:58.215488 24374 net.cpp:530] conv1b -> conv1b
I0917 00:28:58.216965 24374 net.cpp:245] Setting up conv1b
I0917 00:28:58.216974 24374 net.cpp:252] TEST Top shape for layer 5 'conv1b' 1 32 512 1024 (16777216)
I0917 00:28:58.216979 24374 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0917 00:28:58.216981 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.216985 24374 net.cpp:184] Created Layer conv1b/bn (6)
I0917 00:28:58.216989 24374 net.cpp:561] conv1b/bn <- conv1b
I0917 00:28:58.216990 24374 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0917 00:28:58.217809 24374 net.cpp:245] Setting up conv1b/bn
I0917 00:28:58.217818 24374 net.cpp:252] TEST Top shape for layer 6 'conv1b/bn' 1 32 512 1024 (16777216)
I0917 00:28:58.217823 24374 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0917 00:28:58.217825 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.217828 24374 net.cpp:184] Created Layer conv1b/relu (7)
I0917 00:28:58.217830 24374 net.cpp:561] conv1b/relu <- conv1b
I0917 00:28:58.217833 24374 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0917 00:28:58.217836 24374 net.cpp:245] Setting up conv1b/relu
I0917 00:28:58.217839 24374 net.cpp:252] TEST Top shape for layer 7 'conv1b/relu' 1 32 512 1024 (16777216)
I0917 00:28:58.217840 24374 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0917 00:28:58.217852 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.217856 24374 net.cpp:184] Created Layer pool1 (8)
I0917 00:28:58.217859 24374 net.cpp:561] pool1 <- conv1b
I0917 00:28:58.217861 24374 net.cpp:530] pool1 -> pool1
I0917 00:28:58.217893 24374 net.cpp:245] Setting up pool1
I0917 00:28:58.217898 24374 net.cpp:252] TEST Top shape for layer 8 'pool1' 1 32 256 512 (4194304)
I0917 00:28:58.217900 24374 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0917 00:28:58.217902 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.217908 24374 net.cpp:184] Created Layer res2a_branch2a (9)
I0917 00:28:58.217911 24374 net.cpp:561] res2a_branch2a <- pool1
I0917 00:28:58.217912 24374 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0917 00:28:58.219024 24374 net.cpp:245] Setting up res2a_branch2a
I0917 00:28:58.219033 24374 net.cpp:252] TEST Top shape for layer 9 'res2a_branch2a' 1 64 256 512 (8388608)
I0917 00:28:58.219038 24374 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0917 00:28:58.219041 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.219045 24374 net.cpp:184] Created Layer res2a_branch2a/bn (10)
I0917 00:28:58.219048 24374 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0917 00:28:58.219049 24374 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0917 00:28:58.219434 24374 net.cpp:245] Setting up res2a_branch2a/bn
I0917 00:28:58.219440 24374 net.cpp:252] TEST Top shape for layer 10 'res2a_branch2a/bn' 1 64 256 512 (8388608)
I0917 00:28:58.219446 24374 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0917 00:28:58.219449 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.219451 24374 net.cpp:184] Created Layer res2a_branch2a/relu (11)
I0917 00:28:58.219454 24374 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0917 00:28:58.219455 24374 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0917 00:28:58.219458 24374 net.cpp:245] Setting up res2a_branch2a/relu
I0917 00:28:58.219461 24374 net.cpp:252] TEST Top shape for layer 11 'res2a_branch2a/relu' 1 64 256 512 (8388608)
I0917 00:28:58.219463 24374 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0917 00:28:58.219465 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.219471 24374 net.cpp:184] Created Layer res2a_branch2b (12)
I0917 00:28:58.219473 24374 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0917 00:28:58.219476 24374 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0917 00:28:58.220403 24374 net.cpp:245] Setting up res2a_branch2b
I0917 00:28:58.220413 24374 net.cpp:252] TEST Top shape for layer 12 'res2a_branch2b' 1 64 256 512 (8388608)
I0917 00:28:58.220418 24374 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0917 00:28:58.220422 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.220425 24374 net.cpp:184] Created Layer res2a_branch2b/bn (13)
I0917 00:28:58.220428 24374 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0917 00:28:58.220432 24374 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0917 00:28:58.221199 24374 net.cpp:245] Setting up res2a_branch2b/bn
I0917 00:28:58.221207 24374 net.cpp:252] TEST Top shape for layer 13 'res2a_branch2b/bn' 1 64 256 512 (8388608)
I0917 00:28:58.221213 24374 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0917 00:28:58.221216 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.221221 24374 net.cpp:184] Created Layer res2a_branch2b/relu (14)
I0917 00:28:58.221223 24374 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0917 00:28:58.221225 24374 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0917 00:28:58.221237 24374 net.cpp:245] Setting up res2a_branch2b/relu
I0917 00:28:58.221240 24374 net.cpp:252] TEST Top shape for layer 14 'res2a_branch2b/relu' 1 64 256 512 (8388608)
I0917 00:28:58.221242 24374 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0917 00:28:58.221245 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.221248 24374 net.cpp:184] Created Layer pool2 (15)
I0917 00:28:58.221251 24374 net.cpp:561] pool2 <- res2a_branch2b
I0917 00:28:58.221252 24374 net.cpp:530] pool2 -> pool2
I0917 00:28:58.221282 24374 net.cpp:245] Setting up pool2
I0917 00:28:58.221287 24374 net.cpp:252] TEST Top shape for layer 15 'pool2' 1 64 128 256 (2097152)
I0917 00:28:58.221288 24374 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0917 00:28:58.221292 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.221298 24374 net.cpp:184] Created Layer res3a_branch2a (16)
I0917 00:28:58.221302 24374 net.cpp:561] res3a_branch2a <- pool2
I0917 00:28:58.221305 24374 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0917 00:28:58.223939 24374 net.cpp:245] Setting up res3a_branch2a
I0917 00:28:58.223953 24374 net.cpp:252] TEST Top shape for layer 16 'res3a_branch2a' 1 128 128 256 (4194304)
I0917 00:28:58.223960 24374 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0917 00:28:58.223966 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.223975 24374 net.cpp:184] Created Layer res3a_branch2a/bn (17)
I0917 00:28:58.223979 24374 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0917 00:28:58.223985 24374 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0917 00:28:58.224380 24374 net.cpp:245] Setting up res3a_branch2a/bn
I0917 00:28:58.224388 24374 net.cpp:252] TEST Top shape for layer 17 'res3a_branch2a/bn' 1 128 128 256 (4194304)
I0917 00:28:58.224400 24374 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0917 00:28:58.224403 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.224409 24374 net.cpp:184] Created Layer res3a_branch2a/relu (18)
I0917 00:28:58.224413 24374 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0917 00:28:58.224417 24374 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0917 00:28:58.224423 24374 net.cpp:245] Setting up res3a_branch2a/relu
I0917 00:28:58.224428 24374 net.cpp:252] TEST Top shape for layer 18 'res3a_branch2a/relu' 1 128 128 256 (4194304)
I0917 00:28:58.224432 24374 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0917 00:28:58.224436 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.224444 24374 net.cpp:184] Created Layer res3a_branch2b (19)
I0917 00:28:58.224457 24374 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0917 00:28:58.224462 24374 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0917 00:28:58.225358 24374 net.cpp:245] Setting up res3a_branch2b
I0917 00:28:58.225365 24374 net.cpp:252] TEST Top shape for layer 19 'res3a_branch2b' 1 128 128 256 (4194304)
I0917 00:28:58.225371 24374 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0917 00:28:58.225376 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.225383 24374 net.cpp:184] Created Layer res3a_branch2b/bn (20)
I0917 00:28:58.225388 24374 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0917 00:28:58.225390 24374 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0917 00:28:58.225759 24374 net.cpp:245] Setting up res3a_branch2b/bn
I0917 00:28:58.225765 24374 net.cpp:252] TEST Top shape for layer 20 'res3a_branch2b/bn' 1 128 128 256 (4194304)
I0917 00:28:58.225774 24374 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0917 00:28:58.225777 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.225790 24374 net.cpp:184] Created Layer res3a_branch2b/relu (21)
I0917 00:28:58.225795 24374 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0917 00:28:58.225800 24374 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0917 00:28:58.225805 24374 net.cpp:245] Setting up res3a_branch2b/relu
I0917 00:28:58.225810 24374 net.cpp:252] TEST Top shape for layer 21 'res3a_branch2b/relu' 1 128 128 256 (4194304)
I0917 00:28:58.225813 24374 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0917 00:28:58.225818 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.225823 24374 net.cpp:184] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (22)
I0917 00:28:58.225828 24374 net.cpp:561] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0917 00:28:58.225831 24374 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0917 00:28:58.225836 24374 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0917 00:28:58.225862 24374 net.cpp:245] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0917 00:28:58.225867 24374 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 1 128 128 256 (4194304)
I0917 00:28:58.225872 24374 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 1 128 128 256 (4194304)
I0917 00:28:58.225875 24374 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0917 00:28:58.225878 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.225884 24374 net.cpp:184] Created Layer pool3 (23)
I0917 00:28:58.225889 24374 net.cpp:561] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0917 00:28:58.225893 24374 net.cpp:530] pool3 -> pool3
I0917 00:28:58.225924 24374 net.cpp:245] Setting up pool3
I0917 00:28:58.225929 24374 net.cpp:252] TEST Top shape for layer 23 'pool3' 1 128 64 128 (1048576)
I0917 00:28:58.225932 24374 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0917 00:28:58.225936 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.225945 24374 net.cpp:184] Created Layer res4a_branch2a (24)
I0917 00:28:58.225950 24374 net.cpp:561] res4a_branch2a <- pool3
I0917 00:28:58.225953 24374 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0917 00:28:58.232000 24374 net.cpp:245] Setting up res4a_branch2a
I0917 00:28:58.232009 24374 net.cpp:252] TEST Top shape for layer 24 'res4a_branch2a' 1 256 64 128 (2097152)
I0917 00:28:58.232017 24374 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0917 00:28:58.232020 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.232028 24374 net.cpp:184] Created Layer res4a_branch2a/bn (25)
I0917 00:28:58.232033 24374 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0917 00:28:58.232036 24374 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0917 00:28:58.232424 24374 net.cpp:245] Setting up res4a_branch2a/bn
I0917 00:28:58.232431 24374 net.cpp:252] TEST Top shape for layer 25 'res4a_branch2a/bn' 1 256 64 128 (2097152)
I0917 00:28:58.232439 24374 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0917 00:28:58.232445 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.232450 24374 net.cpp:184] Created Layer res4a_branch2a/relu (26)
I0917 00:28:58.232453 24374 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0917 00:28:58.232457 24374 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0917 00:28:58.232462 24374 net.cpp:245] Setting up res4a_branch2a/relu
I0917 00:28:58.232467 24374 net.cpp:252] TEST Top shape for layer 26 'res4a_branch2a/relu' 1 256 64 128 (2097152)
I0917 00:28:58.232471 24374 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0917 00:28:58.232482 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.232491 24374 net.cpp:184] Created Layer res4a_branch2b (27)
I0917 00:28:58.232496 24374 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0917 00:28:58.232499 24374 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0917 00:28:58.235577 24374 net.cpp:245] Setting up res4a_branch2b
I0917 00:28:58.235586 24374 net.cpp:252] TEST Top shape for layer 27 'res4a_branch2b' 1 256 64 128 (2097152)
I0917 00:28:58.235592 24374 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0917 00:28:58.235597 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.235605 24374 net.cpp:184] Created Layer res4a_branch2b/bn (28)
I0917 00:28:58.235608 24374 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0917 00:28:58.235612 24374 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0917 00:28:58.235980 24374 net.cpp:245] Setting up res4a_branch2b/bn
I0917 00:28:58.235987 24374 net.cpp:252] TEST Top shape for layer 28 'res4a_branch2b/bn' 1 256 64 128 (2097152)
I0917 00:28:58.235996 24374 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0917 00:28:58.236001 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.236006 24374 net.cpp:184] Created Layer res4a_branch2b/relu (29)
I0917 00:28:58.236011 24374 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0917 00:28:58.236014 24374 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0917 00:28:58.236021 24374 net.cpp:245] Setting up res4a_branch2b/relu
I0917 00:28:58.236026 24374 net.cpp:252] TEST Top shape for layer 29 'res4a_branch2b/relu' 1 256 64 128 (2097152)
I0917 00:28:58.236029 24374 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0917 00:28:58.236032 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.236038 24374 net.cpp:184] Created Layer pool4 (30)
I0917 00:28:58.236042 24374 net.cpp:561] pool4 <- res4a_branch2b
I0917 00:28:58.236047 24374 net.cpp:530] pool4 -> pool4
I0917 00:28:58.236080 24374 net.cpp:245] Setting up pool4
I0917 00:28:58.236086 24374 net.cpp:252] TEST Top shape for layer 30 'pool4' 1 256 64 128 (2097152)
I0917 00:28:58.236090 24374 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0917 00:28:58.236093 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.236101 24374 net.cpp:184] Created Layer res5a_branch2a (31)
I0917 00:28:58.236105 24374 net.cpp:561] res5a_branch2a <- pool4
I0917 00:28:58.236109 24374 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0917 00:28:58.265609 24374 net.cpp:245] Setting up res5a_branch2a
I0917 00:28:58.265630 24374 net.cpp:252] TEST Top shape for layer 31 'res5a_branch2a' 1 512 64 128 (4194304)
I0917 00:28:58.265638 24374 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0917 00:28:58.265643 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.265655 24374 net.cpp:184] Created Layer res5a_branch2a/bn (32)
I0917 00:28:58.265661 24374 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0917 00:28:58.265666 24374 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0917 00:28:58.266422 24374 net.cpp:245] Setting up res5a_branch2a/bn
I0917 00:28:58.266430 24374 net.cpp:252] TEST Top shape for layer 32 'res5a_branch2a/bn' 1 512 64 128 (4194304)
I0917 00:28:58.266440 24374 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0917 00:28:58.266445 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.266450 24374 net.cpp:184] Created Layer res5a_branch2a/relu (33)
I0917 00:28:58.266454 24374 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0917 00:28:58.266466 24374 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0917 00:28:58.266474 24374 net.cpp:245] Setting up res5a_branch2a/relu
I0917 00:28:58.266479 24374 net.cpp:252] TEST Top shape for layer 33 'res5a_branch2a/relu' 1 512 64 128 (4194304)
I0917 00:28:58.266482 24374 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0917 00:28:58.266485 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.266496 24374 net.cpp:184] Created Layer res5a_branch2b (34)
I0917 00:28:58.266500 24374 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0917 00:28:58.266504 24374 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0917 00:28:58.278863 24374 net.cpp:245] Setting up res5a_branch2b
I0917 00:28:58.278877 24374 net.cpp:252] TEST Top shape for layer 34 'res5a_branch2b' 1 512 64 128 (4194304)
I0917 00:28:58.278887 24374 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0917 00:28:58.278893 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.278901 24374 net.cpp:184] Created Layer res5a_branch2b/bn (35)
I0917 00:28:58.278905 24374 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0917 00:28:58.278910 24374 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0917 00:28:58.279294 24374 net.cpp:245] Setting up res5a_branch2b/bn
I0917 00:28:58.279302 24374 net.cpp:252] TEST Top shape for layer 35 'res5a_branch2b/bn' 1 512 64 128 (4194304)
I0917 00:28:58.279310 24374 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0917 00:28:58.279315 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.279320 24374 net.cpp:184] Created Layer res5a_branch2b/relu (36)
I0917 00:28:58.279325 24374 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0917 00:28:58.279330 24374 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0917 00:28:58.279335 24374 net.cpp:245] Setting up res5a_branch2b/relu
I0917 00:28:58.279340 24374 net.cpp:252] TEST Top shape for layer 36 'res5a_branch2b/relu' 1 512 64 128 (4194304)
I0917 00:28:58.279345 24374 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I0917 00:28:58.279348 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.279356 24374 net.cpp:184] Created Layer out5a (37)
I0917 00:28:58.279361 24374 net.cpp:561] out5a <- res5a_branch2b
I0917 00:28:58.279364 24374 net.cpp:530] out5a -> out5a
I0917 00:28:58.283028 24374 net.cpp:245] Setting up out5a
I0917 00:28:58.283040 24374 net.cpp:252] TEST Top shape for layer 37 'out5a' 1 64 64 128 (524288)
I0917 00:28:58.283046 24374 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I0917 00:28:58.283051 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.283058 24374 net.cpp:184] Created Layer out5a/bn (38)
I0917 00:28:58.283062 24374 net.cpp:561] out5a/bn <- out5a
I0917 00:28:58.283068 24374 net.cpp:513] out5a/bn -> out5a (in-place)
I0917 00:28:58.283463 24374 net.cpp:245] Setting up out5a/bn
I0917 00:28:58.283471 24374 net.cpp:252] TEST Top shape for layer 38 'out5a/bn' 1 64 64 128 (524288)
I0917 00:28:58.283479 24374 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I0917 00:28:58.283484 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.283488 24374 net.cpp:184] Created Layer out5a/relu (39)
I0917 00:28:58.283493 24374 net.cpp:561] out5a/relu <- out5a
I0917 00:28:58.283496 24374 net.cpp:513] out5a/relu -> out5a (in-place)
I0917 00:28:58.283502 24374 net.cpp:245] Setting up out5a/relu
I0917 00:28:58.283507 24374 net.cpp:252] TEST Top shape for layer 39 'out5a/relu' 1 64 64 128 (524288)
I0917 00:28:58.283510 24374 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I0917 00:28:58.283514 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.283537 24374 net.cpp:184] Created Layer out5a_up2 (40)
I0917 00:28:58.283541 24374 net.cpp:561] out5a_up2 <- out5a
I0917 00:28:58.283545 24374 net.cpp:530] out5a_up2 -> out5a_up2
I0917 00:28:58.283676 24374 net.cpp:245] Setting up out5a_up2
I0917 00:28:58.283682 24374 net.cpp:252] TEST Top shape for layer 40 'out5a_up2' 1 64 128 256 (2097152)
I0917 00:28:58.283687 24374 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I0917 00:28:58.283692 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.283702 24374 net.cpp:184] Created Layer out3a (41)
I0917 00:28:58.283706 24374 net.cpp:561] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0917 00:28:58.283711 24374 net.cpp:530] out3a -> out3a
I0917 00:28:58.284615 24374 net.cpp:245] Setting up out3a
I0917 00:28:58.284622 24374 net.cpp:252] TEST Top shape for layer 41 'out3a' 1 64 128 256 (2097152)
I0917 00:28:58.284628 24374 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I0917 00:28:58.284633 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.284639 24374 net.cpp:184] Created Layer out3a/bn (42)
I0917 00:28:58.284643 24374 net.cpp:561] out3a/bn <- out3a
I0917 00:28:58.284648 24374 net.cpp:513] out3a/bn -> out3a (in-place)
I0917 00:28:58.285037 24374 net.cpp:245] Setting up out3a/bn
I0917 00:28:58.285044 24374 net.cpp:252] TEST Top shape for layer 42 'out3a/bn' 1 64 128 256 (2097152)
I0917 00:28:58.285053 24374 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I0917 00:28:58.285058 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.285061 24374 net.cpp:184] Created Layer out3a/relu (43)
I0917 00:28:58.285066 24374 net.cpp:561] out3a/relu <- out3a
I0917 00:28:58.285070 24374 net.cpp:513] out3a/relu -> out3a (in-place)
I0917 00:28:58.285076 24374 net.cpp:245] Setting up out3a/relu
I0917 00:28:58.285081 24374 net.cpp:252] TEST Top shape for layer 43 'out3a/relu' 1 64 128 256 (2097152)
I0917 00:28:58.285085 24374 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I0917 00:28:58.285090 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.285101 24374 net.cpp:184] Created Layer out3_out5_combined (44)
I0917 00:28:58.285105 24374 net.cpp:561] out3_out5_combined <- out5a_up2
I0917 00:28:58.285109 24374 net.cpp:561] out3_out5_combined <- out3a
I0917 00:28:58.285115 24374 net.cpp:530] out3_out5_combined -> out3_out5_combined
I0917 00:28:58.285133 24374 net.cpp:245] Setting up out3_out5_combined
I0917 00:28:58.285138 24374 net.cpp:252] TEST Top shape for layer 44 'out3_out5_combined' 1 64 128 256 (2097152)
I0917 00:28:58.285142 24374 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I0917 00:28:58.285146 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.285154 24374 net.cpp:184] Created Layer ctx_conv1 (45)
I0917 00:28:58.285158 24374 net.cpp:561] ctx_conv1 <- out3_out5_combined
I0917 00:28:58.285161 24374 net.cpp:530] ctx_conv1 -> ctx_conv1
I0917 00:28:58.286053 24374 net.cpp:245] Setting up ctx_conv1
I0917 00:28:58.286062 24374 net.cpp:252] TEST Top shape for layer 45 'ctx_conv1' 1 64 128 256 (2097152)
I0917 00:28:58.286067 24374 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I0917 00:28:58.286074 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.286082 24374 net.cpp:184] Created Layer ctx_conv1/bn (46)
I0917 00:28:58.286085 24374 net.cpp:561] ctx_conv1/bn <- ctx_conv1
I0917 00:28:58.286088 24374 net.cpp:513] ctx_conv1/bn -> ctx_conv1 (in-place)
I0917 00:28:58.286478 24374 net.cpp:245] Setting up ctx_conv1/bn
I0917 00:28:58.286485 24374 net.cpp:252] TEST Top shape for layer 46 'ctx_conv1/bn' 1 64 128 256 (2097152)
I0917 00:28:58.286494 24374 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I0917 00:28:58.286504 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.286509 24374 net.cpp:184] Created Layer ctx_conv1/relu (47)
I0917 00:28:58.286514 24374 net.cpp:561] ctx_conv1/relu <- ctx_conv1
I0917 00:28:58.286516 24374 net.cpp:513] ctx_conv1/relu -> ctx_conv1 (in-place)
I0917 00:28:58.286522 24374 net.cpp:245] Setting up ctx_conv1/relu
I0917 00:28:58.286527 24374 net.cpp:252] TEST Top shape for layer 47 'ctx_conv1/relu' 1 64 128 256 (2097152)
I0917 00:28:58.286532 24374 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I0917 00:28:58.286535 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.286545 24374 net.cpp:184] Created Layer ctx_conv2 (48)
I0917 00:28:58.286548 24374 net.cpp:561] ctx_conv2 <- ctx_conv1
I0917 00:28:58.286551 24374 net.cpp:530] ctx_conv2 -> ctx_conv2
I0917 00:28:58.287434 24374 net.cpp:245] Setting up ctx_conv2
I0917 00:28:58.287442 24374 net.cpp:252] TEST Top shape for layer 48 'ctx_conv2' 1 64 128 256 (2097152)
I0917 00:28:58.287448 24374 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I0917 00:28:58.287452 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.287459 24374 net.cpp:184] Created Layer ctx_conv2/bn (49)
I0917 00:28:58.287463 24374 net.cpp:561] ctx_conv2/bn <- ctx_conv2
I0917 00:28:58.287467 24374 net.cpp:513] ctx_conv2/bn -> ctx_conv2 (in-place)
I0917 00:28:58.287852 24374 net.cpp:245] Setting up ctx_conv2/bn
I0917 00:28:58.287859 24374 net.cpp:252] TEST Top shape for layer 49 'ctx_conv2/bn' 1 64 128 256 (2097152)
I0917 00:28:58.287868 24374 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I0917 00:28:58.287871 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.287875 24374 net.cpp:184] Created Layer ctx_conv2/relu (50)
I0917 00:28:58.287879 24374 net.cpp:561] ctx_conv2/relu <- ctx_conv2
I0917 00:28:58.287884 24374 net.cpp:513] ctx_conv2/relu -> ctx_conv2 (in-place)
I0917 00:28:58.287890 24374 net.cpp:245] Setting up ctx_conv2/relu
I0917 00:28:58.287895 24374 net.cpp:252] TEST Top shape for layer 50 'ctx_conv2/relu' 1 64 128 256 (2097152)
I0917 00:28:58.287899 24374 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I0917 00:28:58.287904 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.287910 24374 net.cpp:184] Created Layer ctx_conv3 (51)
I0917 00:28:58.287914 24374 net.cpp:561] ctx_conv3 <- ctx_conv2
I0917 00:28:58.287919 24374 net.cpp:530] ctx_conv3 -> ctx_conv3
I0917 00:28:58.288800 24374 net.cpp:245] Setting up ctx_conv3
I0917 00:28:58.288806 24374 net.cpp:252] TEST Top shape for layer 51 'ctx_conv3' 1 64 128 256 (2097152)
I0917 00:28:58.288812 24374 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I0917 00:28:58.288817 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.288825 24374 net.cpp:184] Created Layer ctx_conv3/bn (52)
I0917 00:28:58.288828 24374 net.cpp:561] ctx_conv3/bn <- ctx_conv3
I0917 00:28:58.288831 24374 net.cpp:513] ctx_conv3/bn -> ctx_conv3 (in-place)
I0917 00:28:58.289221 24374 net.cpp:245] Setting up ctx_conv3/bn
I0917 00:28:58.289228 24374 net.cpp:252] TEST Top shape for layer 52 'ctx_conv3/bn' 1 64 128 256 (2097152)
I0917 00:28:58.289237 24374 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I0917 00:28:58.289240 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.289245 24374 net.cpp:184] Created Layer ctx_conv3/relu (53)
I0917 00:28:58.289249 24374 net.cpp:561] ctx_conv3/relu <- ctx_conv3
I0917 00:28:58.289253 24374 net.cpp:513] ctx_conv3/relu -> ctx_conv3 (in-place)
I0917 00:28:58.289258 24374 net.cpp:245] Setting up ctx_conv3/relu
I0917 00:28:58.289263 24374 net.cpp:252] TEST Top shape for layer 53 'ctx_conv3/relu' 1 64 128 256 (2097152)
I0917 00:28:58.289271 24374 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I0917 00:28:58.289275 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.289283 24374 net.cpp:184] Created Layer ctx_conv4 (54)
I0917 00:28:58.289288 24374 net.cpp:561] ctx_conv4 <- ctx_conv3
I0917 00:28:58.289290 24374 net.cpp:530] ctx_conv4 -> ctx_conv4
I0917 00:28:58.290179 24374 net.cpp:245] Setting up ctx_conv4
I0917 00:28:58.290187 24374 net.cpp:252] TEST Top shape for layer 54 'ctx_conv4' 1 64 128 256 (2097152)
I0917 00:28:58.290194 24374 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I0917 00:28:58.290199 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.290205 24374 net.cpp:184] Created Layer ctx_conv4/bn (55)
I0917 00:28:58.290208 24374 net.cpp:561] ctx_conv4/bn <- ctx_conv4
I0917 00:28:58.290212 24374 net.cpp:513] ctx_conv4/bn -> ctx_conv4 (in-place)
I0917 00:28:58.290601 24374 net.cpp:245] Setting up ctx_conv4/bn
I0917 00:28:58.290607 24374 net.cpp:252] TEST Top shape for layer 55 'ctx_conv4/bn' 1 64 128 256 (2097152)
I0917 00:28:58.290616 24374 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I0917 00:28:58.290619 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.290624 24374 net.cpp:184] Created Layer ctx_conv4/relu (56)
I0917 00:28:58.290628 24374 net.cpp:561] ctx_conv4/relu <- ctx_conv4
I0917 00:28:58.290632 24374 net.cpp:513] ctx_conv4/relu -> ctx_conv4 (in-place)
I0917 00:28:58.290637 24374 net.cpp:245] Setting up ctx_conv4/relu
I0917 00:28:58.290642 24374 net.cpp:252] TEST Top shape for layer 56 'ctx_conv4/relu' 1 64 128 256 (2097152)
I0917 00:28:58.290645 24374 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I0917 00:28:58.290649 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.290657 24374 net.cpp:184] Created Layer ctx_final (57)
I0917 00:28:58.290662 24374 net.cpp:561] ctx_final <- ctx_conv4
I0917 00:28:58.290665 24374 net.cpp:530] ctx_final -> ctx_final
I0917 00:28:58.290935 24374 net.cpp:245] Setting up ctx_final
I0917 00:28:58.290943 24374 net.cpp:252] TEST Top shape for layer 57 'ctx_final' 1 8 128 256 (262144)
I0917 00:28:58.290949 24374 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I0917 00:28:58.290954 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.290959 24374 net.cpp:184] Created Layer ctx_final/relu (58)
I0917 00:28:58.290963 24374 net.cpp:561] ctx_final/relu <- ctx_final
I0917 00:28:58.290967 24374 net.cpp:513] ctx_final/relu -> ctx_final (in-place)
I0917 00:28:58.290974 24374 net.cpp:245] Setting up ctx_final/relu
I0917 00:28:58.290979 24374 net.cpp:252] TEST Top shape for layer 58 'ctx_final/relu' 1 8 128 256 (262144)
I0917 00:28:58.290983 24374 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I0917 00:28:58.290988 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.290997 24374 net.cpp:184] Created Layer out_deconv_final_up2 (59)
I0917 00:28:58.291002 24374 net.cpp:561] out_deconv_final_up2 <- ctx_final
I0917 00:28:58.291004 24374 net.cpp:530] out_deconv_final_up2 -> out_deconv_final_up2
I0917 00:28:58.291116 24374 net.cpp:245] Setting up out_deconv_final_up2
I0917 00:28:58.291122 24374 net.cpp:252] TEST Top shape for layer 59 'out_deconv_final_up2' 1 8 256 512 (1048576)
I0917 00:28:58.291127 24374 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I0917 00:28:58.291132 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.291138 24374 net.cpp:184] Created Layer out_deconv_final_up4 (60)
I0917 00:28:58.291142 24374 net.cpp:561] out_deconv_final_up4 <- out_deconv_final_up2
I0917 00:28:58.291152 24374 net.cpp:530] out_deconv_final_up4 -> out_deconv_final_up4
I0917 00:28:58.291262 24374 net.cpp:245] Setting up out_deconv_final_up4
I0917 00:28:58.291268 24374 net.cpp:252] TEST Top shape for layer 60 'out_deconv_final_up4' 1 8 512 1024 (4194304)
I0917 00:28:58.291273 24374 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I0917 00:28:58.291277 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.291285 24374 net.cpp:184] Created Layer out_deconv_final_up8 (61)
I0917 00:28:58.291290 24374 net.cpp:561] out_deconv_final_up8 <- out_deconv_final_up4
I0917 00:28:58.291293 24374 net.cpp:530] out_deconv_final_up8 -> out_deconv_final_up8
I0917 00:28:58.291405 24374 net.cpp:245] Setting up out_deconv_final_up8
I0917 00:28:58.291411 24374 net.cpp:252] TEST Top shape for layer 61 'out_deconv_final_up8' 1 8 1024 2048 (16777216)
I0917 00:28:58.291416 24374 layer_factory.hpp:136] Creating layer 'argMaxOut' of type 'ArgMax'
I0917 00:28:58.291420 24374 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:28:58.291429 24374 net.cpp:184] Created Layer argMaxOut (62)
I0917 00:28:58.291432 24374 net.cpp:561] argMaxOut <- out_deconv_final_up8
I0917 00:28:58.291436 24374 net.cpp:530] argMaxOut -> argMaxOut
I0917 00:28:58.291453 24374 net.cpp:245] Setting up argMaxOut
I0917 00:28:58.291458 24374 net.cpp:252] TEST Top shape for layer 62 'argMaxOut' 1 1 1024 2048 (2097152)
I0917 00:28:58.291462 24374 net.cpp:325] argMaxOut does not need backward computation.
I0917 00:28:58.291466 24374 net.cpp:325] out_deconv_final_up8 does not need backward computation.
I0917 00:28:58.291470 24374 net.cpp:325] out_deconv_final_up4 does not need backward computation.
I0917 00:28:58.291473 24374 net.cpp:325] out_deconv_final_up2 does not need backward computation.
I0917 00:28:58.291477 24374 net.cpp:325] ctx_final/relu does not need backward computation.
I0917 00:28:58.291481 24374 net.cpp:325] ctx_final does not need backward computation.
I0917 00:28:58.291484 24374 net.cpp:325] ctx_conv4/relu does not need backward computation.
I0917 00:28:58.291487 24374 net.cpp:325] ctx_conv4/bn does not need backward computation.
I0917 00:28:58.291491 24374 net.cpp:325] ctx_conv4 does not need backward computation.
I0917 00:28:58.291494 24374 net.cpp:325] ctx_conv3/relu does not need backward computation.
I0917 00:28:58.291498 24374 net.cpp:325] ctx_conv3/bn does not need backward computation.
I0917 00:28:58.291501 24374 net.cpp:325] ctx_conv3 does not need backward computation.
I0917 00:28:58.291505 24374 net.cpp:325] ctx_conv2/relu does not need backward computation.
I0917 00:28:58.291508 24374 net.cpp:325] ctx_conv2/bn does not need backward computation.
I0917 00:28:58.291512 24374 net.cpp:325] ctx_conv2 does not need backward computation.
I0917 00:28:58.291515 24374 net.cpp:325] ctx_conv1/relu does not need backward computation.
I0917 00:28:58.291518 24374 net.cpp:325] ctx_conv1/bn does not need backward computation.
I0917 00:28:58.291522 24374 net.cpp:325] ctx_conv1 does not need backward computation.
I0917 00:28:58.291527 24374 net.cpp:325] out3_out5_combined does not need backward computation.
I0917 00:28:58.291530 24374 net.cpp:325] out3a/relu does not need backward computation.
I0917 00:28:58.291534 24374 net.cpp:325] out3a/bn does not need backward computation.
I0917 00:28:58.291538 24374 net.cpp:325] out3a does not need backward computation.
I0917 00:28:58.291543 24374 net.cpp:325] out5a_up2 does not need backward computation.
I0917 00:28:58.291548 24374 net.cpp:325] out5a/relu does not need backward computation.
I0917 00:28:58.291550 24374 net.cpp:325] out5a/bn does not need backward computation.
I0917 00:28:58.291554 24374 net.cpp:325] out5a does not need backward computation.
I0917 00:28:58.291558 24374 net.cpp:325] res5a_branch2b/relu does not need backward computation.
I0917 00:28:58.291561 24374 net.cpp:325] res5a_branch2b/bn does not need backward computation.
I0917 00:28:58.291569 24374 net.cpp:325] res5a_branch2b does not need backward computation.
I0917 00:28:58.291574 24374 net.cpp:325] res5a_branch2a/relu does not need backward computation.
I0917 00:28:58.291577 24374 net.cpp:325] res5a_branch2a/bn does not need backward computation.
I0917 00:28:58.291581 24374 net.cpp:325] res5a_branch2a does not need backward computation.
I0917 00:28:58.291585 24374 net.cpp:325] pool4 does not need backward computation.
I0917 00:28:58.291589 24374 net.cpp:325] res4a_branch2b/relu does not need backward computation.
I0917 00:28:58.291594 24374 net.cpp:325] res4a_branch2b/bn does not need backward computation.
I0917 00:28:58.291597 24374 net.cpp:325] res4a_branch2b does not need backward computation.
I0917 00:28:58.291601 24374 net.cpp:325] res4a_branch2a/relu does not need backward computation.
I0917 00:28:58.291605 24374 net.cpp:325] res4a_branch2a/bn does not need backward computation.
I0917 00:28:58.291609 24374 net.cpp:325] res4a_branch2a does not need backward computation.
I0917 00:28:58.291612 24374 net.cpp:325] pool3 does not need backward computation.
I0917 00:28:58.291616 24374 net.cpp:325] res3a_branch2b_res3a_branch2b/relu_0_split does not need backward computation.
I0917 00:28:58.291620 24374 net.cpp:325] res3a_branch2b/relu does not need backward computation.
I0917 00:28:58.291625 24374 net.cpp:325] res3a_branch2b/bn does not need backward computation.
I0917 00:28:58.291628 24374 net.cpp:325] res3a_branch2b does not need backward computation.
I0917 00:28:58.291633 24374 net.cpp:325] res3a_branch2a/relu does not need backward computation.
I0917 00:28:58.291637 24374 net.cpp:325] res3a_branch2a/bn does not need backward computation.
I0917 00:28:58.291640 24374 net.cpp:325] res3a_branch2a does not need backward computation.
I0917 00:28:58.291645 24374 net.cpp:325] pool2 does not need backward computation.
I0917 00:28:58.291649 24374 net.cpp:325] res2a_branch2b/relu does not need backward computation.
I0917 00:28:58.291653 24374 net.cpp:325] res2a_branch2b/bn does not need backward computation.
I0917 00:28:58.291656 24374 net.cpp:325] res2a_branch2b does not need backward computation.
I0917 00:28:58.291661 24374 net.cpp:325] res2a_branch2a/relu does not need backward computation.
I0917 00:28:58.291664 24374 net.cpp:325] res2a_branch2a/bn does not need backward computation.
I0917 00:28:58.291668 24374 net.cpp:325] res2a_branch2a does not need backward computation.
I0917 00:28:58.291671 24374 net.cpp:325] pool1 does not need backward computation.
I0917 00:28:58.291676 24374 net.cpp:325] conv1b/relu does not need backward computation.
I0917 00:28:58.291680 24374 net.cpp:325] conv1b/bn does not need backward computation.
I0917 00:28:58.291684 24374 net.cpp:325] conv1b does not need backward computation.
I0917 00:28:58.291688 24374 net.cpp:325] conv1a/relu does not need backward computation.
I0917 00:28:58.291693 24374 net.cpp:325] conv1a/bn does not need backward computation.
I0917 00:28:58.291697 24374 net.cpp:325] conv1a does not need backward computation.
I0917 00:28:58.291700 24374 net.cpp:325] data/bias does not need backward computation.
I0917 00:28:58.291705 24374 net.cpp:325] input does not need backward computation.
I0917 00:28:58.291708 24374 net.cpp:367] This network produces output argMaxOut
I0917 00:28:58.291748 24374 net.cpp:389] Top memory (TEST) required for data: 1224736768 diff: 1224736768
I0917 00:28:58.291751 24374 net.cpp:392] Bottom memory (TEST) required for data: 1216348160 diff: 1216348160
I0917 00:28:58.291754 24374 net.cpp:395] Shared (in-place) memory (TEST) by data: 659554304 diff: 659554304
I0917 00:28:58.291757 24374 net.cpp:398] Parameters memory (TEST) required for data: 10817840 diff: 10817840
I0917 00:28:58.291761 24374 net.cpp:401] Parameters shared memory (TEST) by data: 0 diff: 0
I0917 00:28:58.291764 24374 net.cpp:407] Network initialization done.
I0917 00:28:58.296322 24374 net.cpp:1078] Ignoring source layer data
I0917 00:28:58.296342 24374 net.cpp:1094] Copying source layer data/bias Type:Bias #blobs=1
I0917 00:28:58.296373 24374 net.cpp:1094] Copying source layer conv1a Type:Convolution #blobs=2
I0917 00:28:58.296397 24374 net.cpp:1094] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0917 00:28:58.296538 24374 net.cpp:1094] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0917 00:28:58.296543 24374 net.cpp:1094] Copying source layer conv1b Type:Convolution #blobs=2
I0917 00:28:58.296555 24374 net.cpp:1094] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0917 00:28:58.296643 24374 net.cpp:1094] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0917 00:28:58.296648 24374 net.cpp:1094] Copying source layer pool1 Type:Pooling #blobs=0
I0917 00:28:58.296651 24374 net.cpp:1094] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0917 00:28:58.296669 24374 net.cpp:1094] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0917 00:28:58.296759 24374 net.cpp:1094] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0917 00:28:58.296764 24374 net.cpp:1094] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0917 00:28:58.296778 24374 net.cpp:1094] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0917 00:28:58.296865 24374 net.cpp:1094] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0917 00:28:58.296870 24374 net.cpp:1094] Copying source layer pool2 Type:Pooling #blobs=0
I0917 00:28:58.296875 24374 net.cpp:1094] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0917 00:28:58.296911 24374 net.cpp:1094] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0917 00:28:58.296994 24374 net.cpp:1094] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0917 00:28:58.296998 24374 net.cpp:1094] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0917 00:28:58.297022 24374 net.cpp:1094] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0917 00:28:58.297101 24374 net.cpp:1094] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0917 00:28:58.297106 24374 net.cpp:1094] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I0917 00:28:58.297108 24374 net.cpp:1094] Copying source layer pool3 Type:Pooling #blobs=0
I0917 00:28:58.297112 24374 net.cpp:1094] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0917 00:28:58.297224 24374 net.cpp:1094] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0917 00:28:58.297302 24374 net.cpp:1094] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0917 00:28:58.297307 24374 net.cpp:1094] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0917 00:28:58.297366 24374 net.cpp:1094] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0917 00:28:58.297444 24374 net.cpp:1094] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0917 00:28:58.297449 24374 net.cpp:1094] Copying source layer pool4 Type:Pooling #blobs=0
I0917 00:28:58.297452 24374 net.cpp:1094] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0917 00:28:58.297786 24374 net.cpp:1094] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0917 00:28:58.297864 24374 net.cpp:1094] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0917 00:28:58.297869 24374 net.cpp:1094] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0917 00:28:58.298038 24374 net.cpp:1094] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0917 00:28:58.298117 24374 net.cpp:1094] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0917 00:28:58.298123 24374 net.cpp:1094] Copying source layer out5a Type:Convolution #blobs=2
I0917 00:28:58.298177 24374 net.cpp:1094] Copying source layer out5a/bn Type:BatchNorm #blobs=5
I0917 00:28:58.298266 24374 net.cpp:1094] Copying source layer out5a/relu Type:ReLU #blobs=0
I0917 00:28:58.298271 24374 net.cpp:1094] Copying source layer out5a_up2 Type:Deconvolution #blobs=1
I0917 00:28:58.298280 24374 net.cpp:1094] Copying source layer out3a Type:Convolution #blobs=2
I0917 00:28:58.298300 24374 net.cpp:1094] Copying source layer out3a/bn Type:BatchNorm #blobs=5
I0917 00:28:58.298391 24374 net.cpp:1094] Copying source layer out3a/relu Type:ReLU #blobs=0
I0917 00:28:58.298396 24374 net.cpp:1094] Copying source layer out3_out5_combined Type:Eltwise #blobs=0
I0917 00:28:58.298399 24374 net.cpp:1094] Copying source layer ctx_conv1 Type:Convolution #blobs=2
I0917 00:28:58.298418 24374 net.cpp:1094] Copying source layer ctx_conv1/bn Type:BatchNorm #blobs=5
I0917 00:28:58.298503 24374 net.cpp:1094] Copying source layer ctx_conv1/relu Type:ReLU #blobs=0
I0917 00:28:58.298508 24374 net.cpp:1094] Copying source layer ctx_conv2 Type:Convolution #blobs=2
I0917 00:28:58.298529 24374 net.cpp:1094] Copying source layer ctx_conv2/bn Type:BatchNorm #blobs=5
I0917 00:28:58.298615 24374 net.cpp:1094] Copying source layer ctx_conv2/relu Type:ReLU #blobs=0
I0917 00:28:58.298620 24374 net.cpp:1094] Copying source layer ctx_conv3 Type:Convolution #blobs=2
I0917 00:28:58.298641 24374 net.cpp:1094] Copying source layer ctx_conv3/bn Type:BatchNorm #blobs=5
I0917 00:28:58.298724 24374 net.cpp:1094] Copying source layer ctx_conv3/relu Type:ReLU #blobs=0
I0917 00:28:58.298729 24374 net.cpp:1094] Copying source layer ctx_conv4 Type:Convolution #blobs=2
I0917 00:28:58.298750 24374 net.cpp:1094] Copying source layer ctx_conv4/bn Type:BatchNorm #blobs=5
I0917 00:28:58.298835 24374 net.cpp:1094] Copying source layer ctx_conv4/relu Type:ReLU #blobs=0
I0917 00:28:58.298840 24374 net.cpp:1094] Copying source layer ctx_final Type:Convolution #blobs=2
I0917 00:28:58.298852 24374 net.cpp:1094] Copying source layer ctx_final/relu Type:ReLU #blobs=0
I0917 00:28:58.298856 24374 net.cpp:1094] Copying source layer out_deconv_final_up2 Type:Deconvolution #blobs=1
I0917 00:28:58.298863 24374 net.cpp:1094] Copying source layer out_deconv_final_up4 Type:Deconvolution #blobs=1
I0917 00:28:58.298871 24374 net.cpp:1094] Copying source layer out_deconv_final_up8 Type:Deconvolution #blobs=1
I0917 00:28:58.298879 24374 net.cpp:1078] Ignoring source layer loss
Namespace(batch_size=1, blend=False, class_dict='', crop=['0'], input='./data/val-image-list.txt', label='./data/val-label-list.txt', label_dict='', model='../trained/image_segmentation/cityscapes5_jsegnet21v2/sparse/deploy.prototxt', num_classes=5, num_images=500, output=None, palette='', resize=['0'], resize_back=True, search='*.png', weights='../trained/image_segmentation/cityscapes5_jsegnet21v2/sparse/cityscapes5_jsegnet21v2_iter_60000.caffemodel')
Infering list
Getting list of images...running inference for  500  images...
('frankfurt_000000_000294_leftImg8bit.png', 'frankfurt_000000_000294_gtFine_labelIds.png', 0)
('frankfurt_000000_000576_leftImg8bit.png', 'frankfurt_000000_000576_gtFine_labelIds.png', 0)
('frankfurt_000000_001016_leftImg8bit.png', 'frankfurt_000000_001016_gtFine_labelIds.png', 0)
I0917 00:28:59.119210 24374 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'conv1a' with space 0.01G 3/1 1 	(avail 5.92G, req 0.01G)	t: 0
I0917 00:28:59.248353 24374 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'conv1b' with space 0.01G 32/4 6 	(avail 5.92G, req 0.01G)	t: 0
I0917 00:28:59.265583 24374 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res2a_branch2a' with space 0.01G 32/1 6 	(avail 5.92G, req 0.01G)	t: 0
I0917 00:28:59.306931 24374 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res2a_branch2b' with space 0.01G 64/4 6 	(avail 5.92G, req 0.01G)	t: 0
I0917 00:28:59.320924 24374 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res3a_branch2a' with space 0.01G 64/1 6 	(avail 5.92G, req 0.01G)	t: 0
I0917 00:28:59.324803 24374 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res3a_branch2b' with space 0.01G 128/4 6 	(avail 5.92G, req 0.01G)	t: 0
I0917 00:28:59.334179 24374 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res4a_branch2a' with space 0.01G 128/1 6 	(avail 5.92G, req 0.01G)	t: 0
I0917 00:28:59.337685 24374 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res4a_branch2b' with space 0.01G 256/4 6 	(avail 5.92G, req 0.01G)	t: 0
I0917 00:28:59.351878 24374 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'out3a' with space 0.01G 128/2 6 	(avail 5.92G, req 0.01G)	t: 0
I0917 00:28:59.358691 24374 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'ctx_conv1' with space 0.01G 64/1 6 	(avail 5.92G, req 0.01G)	t: 0
I0917 00:28:59.384270 24374 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'ctx_final' with space 0.01G 64/1 6 	(avail 5.92G, req 0.01G)	t: 0
('frankfurt_000000_001236_leftImg8bit.png', 'frankfurt_000000_001236_gtFine_labelIds.png', 0)
('frankfurt_000000_001751_leftImg8bit.png', 'frankfurt_000000_001751_gtFine_labelIds.png', 0)
('frankfurt_000000_002196_leftImg8bit.png', 'frankfurt_000000_002196_gtFine_labelIds.png', 1)
('frankfurt_000000_002963_leftImg8bit.png', 'frankfurt_000000_002963_gtFine_labelIds.png', 1)
('frankfurt_000000_003025_leftImg8bit.png', 'frankfurt_000000_003025_gtFine_labelIds.png', 1)
('frankfurt_000000_003357_leftImg8bit.png', 'frankfurt_000000_003357_gtFine_labelIds.png', 1)
('frankfurt_000000_003920_leftImg8bit.png', 'frankfurt_000000_003920_gtFine_labelIds.png', 1)
('frankfurt_000000_004617_leftImg8bit.png', 'frankfurt_000000_004617_gtFine_labelIds.png', 2)
('frankfurt_000000_005543_leftImg8bit.png', 'frankfurt_000000_005543_gtFine_labelIds.png', 2)
('frankfurt_000000_005898_leftImg8bit.png', 'frankfurt_000000_005898_gtFine_labelIds.png', 2)
('frankfurt_000000_006589_leftImg8bit.png', 'frankfurt_000000_006589_gtFine_labelIds.png', 2)
('frankfurt_000000_007365_leftImg8bit.png', 'frankfurt_000000_007365_gtFine_labelIds.png', 2)
('frankfurt_000000_008206_leftImg8bit.png', 'frankfurt_000000_008206_gtFine_labelIds.png', 3)
('frankfurt_000000_008451_leftImg8bit.png', 'frankfurt_000000_008451_gtFine_labelIds.png', 3)
('frankfurt_000000_009291_leftImg8bit.png', 'frankfurt_000000_009291_gtFine_labelIds.png', 3)
('frankfurt_000000_009561_leftImg8bit.png', 'frankfurt_000000_009561_gtFine_labelIds.png', 3)
('frankfurt_000000_009688_leftImg8bit.png', 'frankfurt_000000_009688_gtFine_labelIds.png', 3)
('frankfurt_000000_009969_leftImg8bit.png', 'frankfurt_000000_009969_gtFine_labelIds.png', 4)
('frankfurt_000000_010351_leftImg8bit.png', 'frankfurt_000000_010351_gtFine_labelIds.png', 4)
('frankfurt_000000_010763_leftImg8bit.png', 'frankfurt_000000_010763_gtFine_labelIds.png', 4)
('frankfurt_000000_011007_leftImg8bit.png', 'frankfurt_000000_011007_gtFine_labelIds.png', 4)
('frankfurt_000000_011074_leftImg8bit.png', 'frankfurt_000000_011074_gtFine_labelIds.png', 4)
pixel_accuracy=0.972364897776, mean_iou=0.817655129745, iou=[ 0.95356142  0.95784151  0.74255553  0.54269814  0.89161904]
('frankfurt_000000_011461_leftImg8bit.png', 'frankfurt_000000_011461_gtFine_labelIds.png', 5)
('frankfurt_000000_011810_leftImg8bit.png', 'frankfurt_000000_011810_gtFine_labelIds.png', 5)
('frankfurt_000000_012009_leftImg8bit.png', 'frankfurt_000000_012009_gtFine_labelIds.png', 5)
('frankfurt_000000_012121_leftImg8bit.png', 'frankfurt_000000_012121_gtFine_labelIds.png', 5)
('frankfurt_000000_012868_leftImg8bit.png', 'frankfurt_000000_012868_gtFine_labelIds.png', 5)
('frankfurt_000000_013067_leftImg8bit.png', 'frankfurt_000000_013067_gtFine_labelIds.png', 6)
('frankfurt_000000_013240_leftImg8bit.png', 'frankfurt_000000_013240_gtFine_labelIds.png', 6)
('frankfurt_000000_013382_leftImg8bit.png', 'frankfurt_000000_013382_gtFine_labelIds.png', 6)
('frankfurt_000000_013942_leftImg8bit.png', 'frankfurt_000000_013942_gtFine_labelIds.png', 6)
('frankfurt_000000_014480_leftImg8bit.png', 'frankfurt_000000_014480_gtFine_labelIds.png', 6)
('frankfurt_000000_015389_leftImg8bit.png', 'frankfurt_000000_015389_gtFine_labelIds.png', 7)
('frankfurt_000000_015676_leftImg8bit.png', 'frankfurt_000000_015676_gtFine_labelIds.png', 7)
('frankfurt_000000_016005_leftImg8bit.png', 'frankfurt_000000_016005_gtFine_labelIds.png', 7)
('frankfurt_000000_016286_leftImg8bit.png', 'frankfurt_000000_016286_gtFine_labelIds.png', 7)
('frankfurt_000000_017228_leftImg8bit.png', 'frankfurt_000000_017228_gtFine_labelIds.png', 7)
('frankfurt_000000_017476_leftImg8bit.png', 'frankfurt_000000_017476_gtFine_labelIds.png', 8)
('frankfurt_000000_018797_leftImg8bit.png', 'frankfurt_000000_018797_gtFine_labelIds.png', 8)
('frankfurt_000000_019607_leftImg8bit.png', 'frankfurt_000000_019607_gtFine_labelIds.png', 8)
('frankfurt_000000_020215_leftImg8bit.png', 'frankfurt_000000_020215_gtFine_labelIds.png', 8)
('frankfurt_000000_020321_leftImg8bit.png', 'frankfurt_000000_020321_gtFine_labelIds.png', 8)
('frankfurt_000000_020880_leftImg8bit.png', 'frankfurt_000000_020880_gtFine_labelIds.png', 9)
('frankfurt_000000_021667_leftImg8bit.png', 'frankfurt_000000_021667_gtFine_labelIds.png', 9)
('frankfurt_000000_021879_leftImg8bit.png', 'frankfurt_000000_021879_gtFine_labelIds.png', 9)
('frankfurt_000000_022254_leftImg8bit.png', 'frankfurt_000000_022254_gtFine_labelIds.png', 9)
('frankfurt_000000_022797_leftImg8bit.png', 'frankfurt_000000_022797_gtFine_labelIds.png', 9)
pixel_accuracy=0.972172484357, mean_iou=0.830269847041, iou=[ 0.95362855  0.96093656  0.76502044  0.58200525  0.88975843]
('frankfurt_000001_000538_leftImg8bit.png', 'frankfurt_000001_000538_gtFine_labelIds.png', 10)
('frankfurt_000001_001464_leftImg8bit.png', 'frankfurt_000001_001464_gtFine_labelIds.png', 10)
('frankfurt_000001_002512_leftImg8bit.png', 'frankfurt_000001_002512_gtFine_labelIds.png', 10)
('frankfurt_000001_002646_leftImg8bit.png', 'frankfurt_000001_002646_gtFine_labelIds.png', 10)
('frankfurt_000001_002759_leftImg8bit.png', 'frankfurt_000001_002759_gtFine_labelIds.png', 10)
('frankfurt_000001_003056_leftImg8bit.png', 'frankfurt_000001_003056_gtFine_labelIds.png', 11)
('frankfurt_000001_003588_leftImg8bit.png', 'frankfurt_000001_003588_gtFine_labelIds.png', 11)
('frankfurt_000001_004327_leftImg8bit.png', 'frankfurt_000001_004327_gtFine_labelIds.png', 11)
('frankfurt_000001_004736_leftImg8bit.png', 'frankfurt_000001_004736_gtFine_labelIds.png', 11)
('frankfurt_000001_004859_leftImg8bit.png', 'frankfurt_000001_004859_gtFine_labelIds.png', 11)
('frankfurt_000001_005184_leftImg8bit.png', 'frankfurt_000001_005184_gtFine_labelIds.png', 12)
('frankfurt_000001_005410_leftImg8bit.png', 'frankfurt_000001_005410_gtFine_labelIds.png', 12)
('frankfurt_000001_005703_leftImg8bit.png', 'frankfurt_000001_005703_gtFine_labelIds.png', 12)
('frankfurt_000001_005898_leftImg8bit.png', 'frankfurt_000001_005898_gtFine_labelIds.png', 12)
('frankfurt_000001_007285_leftImg8bit.png', 'frankfurt_000001_007285_gtFine_labelIds.png', 12)
('frankfurt_000001_007407_leftImg8bit.png', 'frankfurt_000001_007407_gtFine_labelIds.png', 13)
('frankfurt_000001_007622_leftImg8bit.png', 'frankfurt_000001_007622_gtFine_labelIds.png', 13)
('frankfurt_000001_007857_leftImg8bit.png', 'frankfurt_000001_007857_gtFine_labelIds.png', 13)
('frankfurt_000001_007973_leftImg8bit.png', 'frankfurt_000001_007973_gtFine_labelIds.png', 13)
('frankfurt_000001_008200_leftImg8bit.png', 'frankfurt_000001_008200_gtFine_labelIds.png', 13)
('frankfurt_000001_008688_leftImg8bit.png', 'frankfurt_000001_008688_gtFine_labelIds.png', 14)
('frankfurt_000001_009058_leftImg8bit.png', 'frankfurt_000001_009058_gtFine_labelIds.png', 14)
('frankfurt_000001_009504_leftImg8bit.png', 'frankfurt_000001_009504_gtFine_labelIds.png', 14)
('frankfurt_000001_009854_leftImg8bit.png', 'frankfurt_000001_009854_gtFine_labelIds.png', 14)
('frankfurt_000001_010156_leftImg8bit.png', 'frankfurt_000001_010156_gtFine_labelIds.png', 14)
pixel_accuracy=0.970242655143, mean_iou=0.825644358948, iou=[ 0.95040902  0.95815978  0.75332678  0.58047226  0.88585396]
('frankfurt_000001_010444_leftImg8bit.png', 'frankfurt_000001_010444_gtFine_labelIds.png', 15)
('frankfurt_000001_010600_leftImg8bit.png', 'frankfurt_000001_010600_gtFine_labelIds.png', 15)
('frankfurt_000001_010830_leftImg8bit.png', 'frankfurt_000001_010830_gtFine_labelIds.png', 15)
('frankfurt_000001_011162_leftImg8bit.png', 'frankfurt_000001_011162_gtFine_labelIds.png', 15)
('frankfurt_000001_011715_leftImg8bit.png', 'frankfurt_000001_011715_gtFine_labelIds.png', 15)
('frankfurt_000001_011835_leftImg8bit.png', 'frankfurt_000001_011835_gtFine_labelIds.png', 16)
('frankfurt_000001_012038_leftImg8bit.png', 'frankfurt_000001_012038_gtFine_labelIds.png', 16)
('frankfurt_000001_012519_leftImg8bit.png', 'frankfurt_000001_012519_gtFine_labelIds.png', 16)
('frankfurt_000001_012699_leftImg8bit.png', 'frankfurt_000001_012699_gtFine_labelIds.png', 16)
('frankfurt_000001_012738_leftImg8bit.png', 'frankfurt_000001_012738_gtFine_labelIds.png', 16)
('frankfurt_000001_012870_leftImg8bit.png', 'frankfurt_000001_012870_gtFine_labelIds.png', 17)
('frankfurt_000001_013016_leftImg8bit.png', 'frankfurt_000001_013016_gtFine_labelIds.png', 17)
('frankfurt_000001_013496_leftImg8bit.png', 'frankfurt_000001_013496_gtFine_labelIds.png', 17)
('frankfurt_000001_013710_leftImg8bit.png', 'frankfurt_000001_013710_gtFine_labelIds.png', 17)
('frankfurt_000001_014221_leftImg8bit.png', 'frankfurt_000001_014221_gtFine_labelIds.png', 17)
('frankfurt_000001_014406_leftImg8bit.png', 'frankfurt_000001_014406_gtFine_labelIds.png', 18)
('frankfurt_000001_014565_leftImg8bit.png', 'frankfurt_000001_014565_gtFine_labelIds.png', 18)
('frankfurt_000001_014741_leftImg8bit.png', 'frankfurt_000001_014741_gtFine_labelIds.png', 18)
('frankfurt_000001_015091_leftImg8bit.png', 'frankfurt_000001_015091_gtFine_labelIds.png', 18)
('frankfurt_000001_015328_leftImg8bit.png', 'frankfurt_000001_015328_gtFine_labelIds.png', 18)
('frankfurt_000001_015768_leftImg8bit.png', 'frankfurt_000001_015768_gtFine_labelIds.png', 19)
('frankfurt_000001_016029_leftImg8bit.png', 'frankfurt_000001_016029_gtFine_labelIds.png', 19)
('frankfurt_000001_016273_leftImg8bit.png', 'frankfurt_000001_016273_gtFine_labelIds.png', 19)
('frankfurt_000001_016462_leftImg8bit.png', 'frankfurt_000001_016462_gtFine_labelIds.png', 19)
('frankfurt_000001_017101_leftImg8bit.png', 'frankfurt_000001_017101_gtFine_labelIds.png', 19)
pixel_accuracy=0.966892854828, mean_iou=0.83098582832, iou=[ 0.94558539  0.95296638  0.77796298  0.59663589  0.8817785 ]
('frankfurt_000001_017459_leftImg8bit.png', 'frankfurt_000001_017459_gtFine_labelIds.png', 20)
('frankfurt_000001_017842_leftImg8bit.png', 'frankfurt_000001_017842_gtFine_labelIds.png', 20)
('frankfurt_000001_018113_leftImg8bit.png', 'frankfurt_000001_018113_gtFine_labelIds.png', 20)
('frankfurt_000001_019698_leftImg8bit.png', 'frankfurt_000001_019698_gtFine_labelIds.png', 20)
('frankfurt_000001_019854_leftImg8bit.png', 'frankfurt_000001_019854_gtFine_labelIds.png', 20)
('frankfurt_000001_019969_leftImg8bit.png', 'frankfurt_000001_019969_gtFine_labelIds.png', 21)
('frankfurt_000001_020046_leftImg8bit.png', 'frankfurt_000001_020046_gtFine_labelIds.png', 21)
('frankfurt_000001_020287_leftImg8bit.png', 'frankfurt_000001_020287_gtFine_labelIds.png', 21)
('frankfurt_000001_020693_leftImg8bit.png', 'frankfurt_000001_020693_gtFine_labelIds.png', 21)
('frankfurt_000001_021406_leftImg8bit.png', 'frankfurt_000001_021406_gtFine_labelIds.png', 21)
('frankfurt_000001_021825_leftImg8bit.png', 'frankfurt_000001_021825_gtFine_labelIds.png', 22)
('frankfurt_000001_023235_leftImg8bit.png', 'frankfurt_000001_023235_gtFine_labelIds.png', 22)
('frankfurt_000001_023369_leftImg8bit.png', 'frankfurt_000001_023369_gtFine_labelIds.png', 22)
('frankfurt_000001_023769_leftImg8bit.png', 'frankfurt_000001_023769_gtFine_labelIds.png', 22)
('frankfurt_000001_024927_leftImg8bit.png', 'frankfurt_000001_024927_gtFine_labelIds.png', 22)
('frankfurt_000001_025512_leftImg8bit.png', 'frankfurt_000001_025512_gtFine_labelIds.png', 23)
('frankfurt_000001_025713_leftImg8bit.png', 'frankfurt_000001_025713_gtFine_labelIds.png', 23)
('frankfurt_000001_025921_leftImg8bit.png', 'frankfurt_000001_025921_gtFine_labelIds.png', 23)
('frankfurt_000001_027325_leftImg8bit.png', 'frankfurt_000001_027325_gtFine_labelIds.png', 23)
('frankfurt_000001_028232_leftImg8bit.png', 'frankfurt_000001_028232_gtFine_labelIds.png', 23)
('frankfurt_000001_028335_leftImg8bit.png', 'frankfurt_000001_028335_gtFine_labelIds.png', 24)
('frankfurt_000001_028590_leftImg8bit.png', 'frankfurt_000001_028590_gtFine_labelIds.png', 24)
('frankfurt_000001_028854_leftImg8bit.png', 'frankfurt_000001_028854_gtFine_labelIds.png', 24)
('frankfurt_000001_029086_leftImg8bit.png', 'frankfurt_000001_029086_gtFine_labelIds.png', 24)
('frankfurt_000001_029236_leftImg8bit.png', 'frankfurt_000001_029236_gtFine_labelIds.png', 24)
pixel_accuracy=0.967147861519, mean_iou=0.829038609404, iou=[ 0.94634179  0.952824    0.77408868  0.59224687  0.87969171]
('frankfurt_000001_029600_leftImg8bit.png', 'frankfurt_000001_029600_gtFine_labelIds.png', 25)
('frankfurt_000001_030067_leftImg8bit.png', 'frankfurt_000001_030067_gtFine_labelIds.png', 25)
('frankfurt_000001_030310_leftImg8bit.png', 'frankfurt_000001_030310_gtFine_labelIds.png', 25)
('frankfurt_000001_030669_leftImg8bit.png', 'frankfurt_000001_030669_gtFine_labelIds.png', 25)
('frankfurt_000001_031266_leftImg8bit.png', 'frankfurt_000001_031266_gtFine_labelIds.png', 25)
('frankfurt_000001_031416_leftImg8bit.png', 'frankfurt_000001_031416_gtFine_labelIds.png', 26)
('frankfurt_000001_032018_leftImg8bit.png', 'frankfurt_000001_032018_gtFine_labelIds.png', 26)
('frankfurt_000001_032556_leftImg8bit.png', 'frankfurt_000001_032556_gtFine_labelIds.png', 26)
('frankfurt_000001_032711_leftImg8bit.png', 'frankfurt_000001_032711_gtFine_labelIds.png', 26)
('frankfurt_000001_032942_leftImg8bit.png', 'frankfurt_000001_032942_gtFine_labelIds.png', 26)
('frankfurt_000001_033655_leftImg8bit.png', 'frankfurt_000001_033655_gtFine_labelIds.png', 27)
('frankfurt_000001_034047_leftImg8bit.png', 'frankfurt_000001_034047_gtFine_labelIds.png', 27)
('frankfurt_000001_034816_leftImg8bit.png', 'frankfurt_000001_034816_gtFine_labelIds.png', 27)
('frankfurt_000001_035144_leftImg8bit.png', 'frankfurt_000001_035144_gtFine_labelIds.png', 27)
('frankfurt_000001_035864_leftImg8bit.png', 'frankfurt_000001_035864_gtFine_labelIds.png', 27)
('frankfurt_000001_037705_leftImg8bit.png', 'frankfurt_000001_037705_gtFine_labelIds.png', 28)
('frankfurt_000001_038245_leftImg8bit.png', 'frankfurt_000001_038245_gtFine_labelIds.png', 28)
('frankfurt_000001_038418_leftImg8bit.png', 'frankfurt_000001_038418_gtFine_labelIds.png', 28)
('frankfurt_000001_038645_leftImg8bit.png', 'frankfurt_000001_038645_gtFine_labelIds.png', 28)
('frankfurt_000001_038844_leftImg8bit.png', 'frankfurt_000001_038844_gtFine_labelIds.png', 28)
('frankfurt_000001_039895_leftImg8bit.png', 'frankfurt_000001_039895_gtFine_labelIds.png', 29)
('frankfurt_000001_040575_leftImg8bit.png', 'frankfurt_000001_040575_gtFine_labelIds.png', 29)
('frankfurt_000001_040732_leftImg8bit.png', 'frankfurt_000001_040732_gtFine_labelIds.png', 29)
('frankfurt_000001_041074_leftImg8bit.png', 'frankfurt_000001_041074_gtFine_labelIds.png', 29)
('frankfurt_000001_041354_leftImg8bit.png', 'frankfurt_000001_041354_gtFine_labelIds.png', 29)
pixel_accuracy=0.967435370994, mean_iou=0.826524930162, iou=[ 0.94663493  0.95408334  0.75999297  0.58403553  0.88787789]
('frankfurt_000001_041517_leftImg8bit.png', 'frankfurt_000001_041517_gtFine_labelIds.png', 30)
('frankfurt_000001_041664_leftImg8bit.png', 'frankfurt_000001_041664_gtFine_labelIds.png', 30)
('frankfurt_000001_042098_leftImg8bit.png', 'frankfurt_000001_042098_gtFine_labelIds.png', 30)
('frankfurt_000001_042384_leftImg8bit.png', 'frankfurt_000001_042384_gtFine_labelIds.png', 30)
('frankfurt_000001_042733_leftImg8bit.png', 'frankfurt_000001_042733_gtFine_labelIds.png', 30)
('frankfurt_000001_043395_leftImg8bit.png', 'frankfurt_000001_043395_gtFine_labelIds.png', 31)
('frankfurt_000001_043564_leftImg8bit.png', 'frankfurt_000001_043564_gtFine_labelIds.png', 31)
('frankfurt_000001_044227_leftImg8bit.png', 'frankfurt_000001_044227_gtFine_labelIds.png', 31)
('frankfurt_000001_044413_leftImg8bit.png', 'frankfurt_000001_044413_gtFine_labelIds.png', 31)
('frankfurt_000001_044525_leftImg8bit.png', 'frankfurt_000001_044525_gtFine_labelIds.png', 31)
('frankfurt_000001_044658_leftImg8bit.png', 'frankfurt_000001_044658_gtFine_labelIds.png', 32)
('frankfurt_000001_044787_leftImg8bit.png', 'frankfurt_000001_044787_gtFine_labelIds.png', 32)
('frankfurt_000001_046126_leftImg8bit.png', 'frankfurt_000001_046126_gtFine_labelIds.png', 32)
('frankfurt_000001_046272_leftImg8bit.png', 'frankfurt_000001_046272_gtFine_labelIds.png', 32)
('frankfurt_000001_046504_leftImg8bit.png', 'frankfurt_000001_046504_gtFine_labelIds.png', 32)
('frankfurt_000001_046779_leftImg8bit.png', 'frankfurt_000001_046779_gtFine_labelIds.png', 33)
('frankfurt_000001_047178_leftImg8bit.png', 'frankfurt_000001_047178_gtFine_labelIds.png', 33)
('frankfurt_000001_047552_leftImg8bit.png', 'frankfurt_000001_047552_gtFine_labelIds.png', 33)
('frankfurt_000001_048196_leftImg8bit.png', 'frankfurt_000001_048196_gtFine_labelIds.png', 33)
('frankfurt_000001_048355_leftImg8bit.png', 'frankfurt_000001_048355_gtFine_labelIds.png', 33)
('frankfurt_000001_048654_leftImg8bit.png', 'frankfurt_000001_048654_gtFine_labelIds.png', 34)
('frankfurt_000001_049078_leftImg8bit.png', 'frankfurt_000001_049078_gtFine_labelIds.png', 34)
('frankfurt_000001_049209_leftImg8bit.png', 'frankfurt_000001_049209_gtFine_labelIds.png', 34)
('frankfurt_000001_049298_leftImg8bit.png', 'frankfurt_000001_049298_gtFine_labelIds.png', 34)
('frankfurt_000001_049698_leftImg8bit.png', 'frankfurt_000001_049698_gtFine_labelIds.png', 34)
pixel_accuracy=0.969032879766, mean_iou=0.827986171152, iou=[ 0.94941185  0.95639145  0.75906772  0.57889037  0.89616946]
('frankfurt_000001_049770_leftImg8bit.png', 'frankfurt_000001_049770_gtFine_labelIds.png', 35)
('frankfurt_000001_050149_leftImg8bit.png', 'frankfurt_000001_050149_gtFine_labelIds.png', 35)
('frankfurt_000001_050686_leftImg8bit.png', 'frankfurt_000001_050686_gtFine_labelIds.png', 35)
('frankfurt_000001_051516_leftImg8bit.png', 'frankfurt_000001_051516_gtFine_labelIds.png', 35)
('frankfurt_000001_051737_leftImg8bit.png', 'frankfurt_000001_051737_gtFine_labelIds.png', 35)
('frankfurt_000001_051807_leftImg8bit.png', 'frankfurt_000001_051807_gtFine_labelIds.png', 36)
('frankfurt_000001_052120_leftImg8bit.png', 'frankfurt_000001_052120_gtFine_labelIds.png', 36)
('frankfurt_000001_052594_leftImg8bit.png', 'frankfurt_000001_052594_gtFine_labelIds.png', 36)
('frankfurt_000001_053102_leftImg8bit.png', 'frankfurt_000001_053102_gtFine_labelIds.png', 36)
('frankfurt_000001_054077_leftImg8bit.png', 'frankfurt_000001_054077_gtFine_labelIds.png', 36)
('frankfurt_000001_054219_leftImg8bit.png', 'frankfurt_000001_054219_gtFine_labelIds.png', 37)
('frankfurt_000001_054415_leftImg8bit.png', 'frankfurt_000001_054415_gtFine_labelIds.png', 37)
('frankfurt_000001_054640_leftImg8bit.png', 'frankfurt_000001_054640_gtFine_labelIds.png', 37)
('frankfurt_000001_054884_leftImg8bit.png', 'frankfurt_000001_054884_gtFine_labelIds.png', 37)
('frankfurt_000001_055062_leftImg8bit.png', 'frankfurt_000001_055062_gtFine_labelIds.png', 37)
('frankfurt_000001_055172_leftImg8bit.png', 'frankfurt_000001_055172_gtFine_labelIds.png', 38)
('frankfurt_000001_055306_leftImg8bit.png', 'frankfurt_000001_055306_gtFine_labelIds.png', 38)
('frankfurt_000001_055387_leftImg8bit.png', 'frankfurt_000001_055387_gtFine_labelIds.png', 38)
('frankfurt_000001_055538_leftImg8bit.png', 'frankfurt_000001_055538_gtFine_labelIds.png', 38)
('frankfurt_000001_055603_leftImg8bit.png', 'frankfurt_000001_055603_gtFine_labelIds.png', 38)
('frankfurt_000001_055709_leftImg8bit.png', 'frankfurt_000001_055709_gtFine_labelIds.png', 39)
('frankfurt_000001_056580_leftImg8bit.png', 'frankfurt_000001_056580_gtFine_labelIds.png', 39)
('frankfurt_000001_057181_leftImg8bit.png', 'frankfurt_000001_057181_gtFine_labelIds.png', 39)
('frankfurt_000001_057478_leftImg8bit.png', 'frankfurt_000001_057478_gtFine_labelIds.png', 39)
('frankfurt_000001_057954_leftImg8bit.png', 'frankfurt_000001_057954_gtFine_labelIds.png', 39)
pixel_accuracy=0.968472301451, mean_iou=0.830297777157, iou=[ 0.94842627  0.95663519  0.76729101  0.58338208  0.89575433]
('frankfurt_000001_058057_leftImg8bit.png', 'frankfurt_000001_058057_gtFine_labelIds.png', 40)
('frankfurt_000001_058176_leftImg8bit.png', 'frankfurt_000001_058176_gtFine_labelIds.png', 40)
('frankfurt_000001_058504_leftImg8bit.png', 'frankfurt_000001_058504_gtFine_labelIds.png', 40)
('frankfurt_000001_058914_leftImg8bit.png', 'frankfurt_000001_058914_gtFine_labelIds.png', 40)
('frankfurt_000001_059119_leftImg8bit.png', 'frankfurt_000001_059119_gtFine_labelIds.png', 40)
('frankfurt_000001_059642_leftImg8bit.png', 'frankfurt_000001_059642_gtFine_labelIds.png', 41)
('frankfurt_000001_059789_leftImg8bit.png', 'frankfurt_000001_059789_gtFine_labelIds.png', 41)
('frankfurt_000001_060135_leftImg8bit.png', 'frankfurt_000001_060135_gtFine_labelIds.png', 41)
('frankfurt_000001_060422_leftImg8bit.png', 'frankfurt_000001_060422_gtFine_labelIds.png', 41)
('frankfurt_000001_060545_leftImg8bit.png', 'frankfurt_000001_060545_gtFine_labelIds.png', 41)
('frankfurt_000001_060906_leftImg8bit.png', 'frankfurt_000001_060906_gtFine_labelIds.png', 42)
('frankfurt_000001_061682_leftImg8bit.png', 'frankfurt_000001_061682_gtFine_labelIds.png', 42)
('frankfurt_000001_061763_leftImg8bit.png', 'frankfurt_000001_061763_gtFine_labelIds.png', 42)
('frankfurt_000001_062016_leftImg8bit.png', 'frankfurt_000001_062016_gtFine_labelIds.png', 42)
('frankfurt_000001_062250_leftImg8bit.png', 'frankfurt_000001_062250_gtFine_labelIds.png', 42)
('frankfurt_000001_062396_leftImg8bit.png', 'frankfurt_000001_062396_gtFine_labelIds.png', 43)
('frankfurt_000001_062509_leftImg8bit.png', 'frankfurt_000001_062509_gtFine_labelIds.png', 43)
('frankfurt_000001_062653_leftImg8bit.png', 'frankfurt_000001_062653_gtFine_labelIds.png', 43)
('frankfurt_000001_062793_leftImg8bit.png', 'frankfurt_000001_062793_gtFine_labelIds.png', 43)
('frankfurt_000001_063045_leftImg8bit.png', 'frankfurt_000001_063045_gtFine_labelIds.png', 43)
('frankfurt_000001_064130_leftImg8bit.png', 'frankfurt_000001_064130_gtFine_labelIds.png', 44)
('frankfurt_000001_064305_leftImg8bit.png', 'frankfurt_000001_064305_gtFine_labelIds.png', 44)
('frankfurt_000001_064651_leftImg8bit.png', 'frankfurt_000001_064651_gtFine_labelIds.png', 44)
('frankfurt_000001_064798_leftImg8bit.png', 'frankfurt_000001_064798_gtFine_labelIds.png', 44)
('frankfurt_000001_064925_leftImg8bit.png', 'frankfurt_000001_064925_gtFine_labelIds.png', 44)
pixel_accuracy=0.96926302292, mean_iou=0.830017609244, iou=[ 0.94976135  0.95861115  0.76263776  0.58118607  0.89789172]
('frankfurt_000001_065160_leftImg8bit.png', 'frankfurt_000001_065160_gtFine_labelIds.png', 45)
('frankfurt_000001_065617_leftImg8bit.png', 'frankfurt_000001_065617_gtFine_labelIds.png', 45)
('frankfurt_000001_065850_leftImg8bit.png', 'frankfurt_000001_065850_gtFine_labelIds.png', 45)
('frankfurt_000001_066092_leftImg8bit.png', 'frankfurt_000001_066092_gtFine_labelIds.png', 45)
('frankfurt_000001_066438_leftImg8bit.png', 'frankfurt_000001_066438_gtFine_labelIds.png', 45)
('frankfurt_000001_066574_leftImg8bit.png', 'frankfurt_000001_066574_gtFine_labelIds.png', 46)
('frankfurt_000001_066832_leftImg8bit.png', 'frankfurt_000001_066832_gtFine_labelIds.png', 46)
('frankfurt_000001_067092_leftImg8bit.png', 'frankfurt_000001_067092_gtFine_labelIds.png', 46)
('frankfurt_000001_067178_leftImg8bit.png', 'frankfurt_000001_067178_gtFine_labelIds.png', 46)
('frankfurt_000001_067295_leftImg8bit.png', 'frankfurt_000001_067295_gtFine_labelIds.png', 46)
('frankfurt_000001_067474_leftImg8bit.png', 'frankfurt_000001_067474_gtFine_labelIds.png', 47)
('frankfurt_000001_067735_leftImg8bit.png', 'frankfurt_000001_067735_gtFine_labelIds.png', 47)
('frankfurt_000001_068063_leftImg8bit.png', 'frankfurt_000001_068063_gtFine_labelIds.png', 47)
('frankfurt_000001_068208_leftImg8bit.png', 'frankfurt_000001_068208_gtFine_labelIds.png', 47)
('frankfurt_000001_068682_leftImg8bit.png', 'frankfurt_000001_068682_gtFine_labelIds.png', 47)
('frankfurt_000001_068772_leftImg8bit.png', 'frankfurt_000001_068772_gtFine_labelIds.png', 48)
('frankfurt_000001_069633_leftImg8bit.png', 'frankfurt_000001_069633_gtFine_labelIds.png', 48)
('frankfurt_000001_070099_leftImg8bit.png', 'frankfurt_000001_070099_gtFine_labelIds.png', 48)
('frankfurt_000001_071288_leftImg8bit.png', 'frankfurt_000001_071288_gtFine_labelIds.png', 48)
('frankfurt_000001_071781_leftImg8bit.png', 'frankfurt_000001_071781_gtFine_labelIds.png', 48)
('frankfurt_000001_072155_leftImg8bit.png', 'frankfurt_000001_072155_gtFine_labelIds.png', 49)
('frankfurt_000001_072295_leftImg8bit.png', 'frankfurt_000001_072295_gtFine_labelIds.png', 49)
('frankfurt_000001_073088_leftImg8bit.png', 'frankfurt_000001_073088_gtFine_labelIds.png', 49)
('frankfurt_000001_073243_leftImg8bit.png', 'frankfurt_000001_073243_gtFine_labelIds.png', 49)
('frankfurt_000001_073464_leftImg8bit.png', 'frankfurt_000001_073464_gtFine_labelIds.png', 49)
pixel_accuracy=0.968398057576, mean_iou=0.828100494194, iou=[ 0.94822088  0.95637352  0.76017004  0.57761965  0.89811838]
('frankfurt_000001_073911_leftImg8bit.png', 'frankfurt_000001_073911_gtFine_labelIds.png', 50)
('frankfurt_000001_075296_leftImg8bit.png', 'frankfurt_000001_075296_gtFine_labelIds.png', 50)
('frankfurt_000001_075984_leftImg8bit.png', 'frankfurt_000001_075984_gtFine_labelIds.png', 50)
('frankfurt_000001_076502_leftImg8bit.png', 'frankfurt_000001_076502_gtFine_labelIds.png', 50)
('frankfurt_000001_077092_leftImg8bit.png', 'frankfurt_000001_077092_gtFine_labelIds.png', 50)
('frankfurt_000001_077233_leftImg8bit.png', 'frankfurt_000001_077233_gtFine_labelIds.png', 51)
('frankfurt_000001_077434_leftImg8bit.png', 'frankfurt_000001_077434_gtFine_labelIds.png', 51)
('frankfurt_000001_078803_leftImg8bit.png', 'frankfurt_000001_078803_gtFine_labelIds.png', 51)
('frankfurt_000001_079206_leftImg8bit.png', 'frankfurt_000001_079206_gtFine_labelIds.png', 51)
('frankfurt_000001_080091_leftImg8bit.png', 'frankfurt_000001_080091_gtFine_labelIds.png', 51)
('frankfurt_000001_080391_leftImg8bit.png', 'frankfurt_000001_080391_gtFine_labelIds.png', 52)
('frankfurt_000001_080830_leftImg8bit.png', 'frankfurt_000001_080830_gtFine_labelIds.png', 52)
('frankfurt_000001_082087_leftImg8bit.png', 'frankfurt_000001_082087_gtFine_labelIds.png', 52)
('frankfurt_000001_082466_leftImg8bit.png', 'frankfurt_000001_082466_gtFine_labelIds.png', 52)
('frankfurt_000001_083029_leftImg8bit.png', 'frankfurt_000001_083029_gtFine_labelIds.png', 52)
('frankfurt_000001_083199_leftImg8bit.png', 'frankfurt_000001_083199_gtFine_labelIds.png', 53)
('frankfurt_000001_083852_leftImg8bit.png', 'frankfurt_000001_083852_gtFine_labelIds.png', 53)
('lindau_000000_000019_leftImg8bit.png', 'lindau_000000_000019_gtFine_labelIds.png', 53)
('lindau_000001_000019_leftImg8bit.png', 'lindau_000001_000019_gtFine_labelIds.png', 53)
('lindau_000002_000019_leftImg8bit.png', 'lindau_000002_000019_gtFine_labelIds.png', 53)
('lindau_000003_000019_leftImg8bit.png', 'lindau_000003_000019_gtFine_labelIds.png', 54)
('lindau_000004_000019_leftImg8bit.png', 'lindau_000004_000019_gtFine_labelIds.png', 54)
('lindau_000005_000019_leftImg8bit.png', 'lindau_000005_000019_gtFine_labelIds.png', 54)
('lindau_000006_000019_leftImg8bit.png', 'lindau_000006_000019_gtFine_labelIds.png', 54)
('lindau_000007_000019_leftImg8bit.png', 'lindau_000007_000019_gtFine_labelIds.png', 54)
pixel_accuracy=0.968821866829, mean_iou=0.827307213506, iou=[ 0.9488758   0.95714064  0.75572392  0.5764012   0.89839451]
('lindau_000008_000019_leftImg8bit.png', 'lindau_000008_000019_gtFine_labelIds.png', 55)
('lindau_000009_000019_leftImg8bit.png', 'lindau_000009_000019_gtFine_labelIds.png', 55)
('lindau_000010_000019_leftImg8bit.png', 'lindau_000010_000019_gtFine_labelIds.png', 55)
('lindau_000011_000019_leftImg8bit.png', 'lindau_000011_000019_gtFine_labelIds.png', 55)
('lindau_000012_000019_leftImg8bit.png', 'lindau_000012_000019_gtFine_labelIds.png', 55)
('lindau_000013_000019_leftImg8bit.png', 'lindau_000013_000019_gtFine_labelIds.png', 56)
('lindau_000014_000019_leftImg8bit.png', 'lindau_000014_000019_gtFine_labelIds.png', 56)
('lindau_000015_000019_leftImg8bit.png', 'lindau_000015_000019_gtFine_labelIds.png', 56)
('lindau_000016_000019_leftImg8bit.png', 'lindau_000016_000019_gtFine_labelIds.png', 56)
('lindau_000017_000019_leftImg8bit.png', 'lindau_000017_000019_gtFine_labelIds.png', 56)
('lindau_000018_000019_leftImg8bit.png', 'lindau_000018_000019_gtFine_labelIds.png', 57)
('lindau_000019_000019_leftImg8bit.png', 'lindau_000019_000019_gtFine_labelIds.png', 57)
('lindau_000020_000019_leftImg8bit.png', 'lindau_000020_000019_gtFine_labelIds.png', 57)
('lindau_000021_000019_leftImg8bit.png', 'lindau_000021_000019_gtFine_labelIds.png', 57)
('lindau_000022_000019_leftImg8bit.png', 'lindau_000022_000019_gtFine_labelIds.png', 57)
('lindau_000023_000019_leftImg8bit.png', 'lindau_000023_000019_gtFine_labelIds.png', 58)
('lindau_000024_000019_leftImg8bit.png', 'lindau_000024_000019_gtFine_labelIds.png', 58)
('lindau_000025_000019_leftImg8bit.png', 'lindau_000025_000019_gtFine_labelIds.png', 58)
('lindau_000026_000019_leftImg8bit.png', 'lindau_000026_000019_gtFine_labelIds.png', 58)
('lindau_000027_000019_leftImg8bit.png', 'lindau_000027_000019_gtFine_labelIds.png', 58)
('lindau_000028_000019_leftImg8bit.png', 'lindau_000028_000019_gtFine_labelIds.png', 59)
('lindau_000029_000019_leftImg8bit.png', 'lindau_000029_000019_gtFine_labelIds.png', 59)
('lindau_000030_000019_leftImg8bit.png', 'lindau_000030_000019_gtFine_labelIds.png', 59)
('lindau_000031_000019_leftImg8bit.png', 'lindau_000031_000019_gtFine_labelIds.png', 59)
('lindau_000032_000019_leftImg8bit.png', 'lindau_000032_000019_gtFine_labelIds.png', 59)
pixel_accuracy=0.963847905274, mean_iou=0.822080822994, iou=[ 0.94129907  0.9419753   0.75171017  0.58207504  0.89334453]
('lindau_000033_000019_leftImg8bit.png', 'lindau_000033_000019_gtFine_labelIds.png', 60)
('lindau_000034_000019_leftImg8bit.png', 'lindau_000034_000019_gtFine_labelIds.png', 60)
('lindau_000035_000019_leftImg8bit.png', 'lindau_000035_000019_gtFine_labelIds.png', 60)
('lindau_000036_000019_leftImg8bit.png', 'lindau_000036_000019_gtFine_labelIds.png', 60)
('lindau_000037_000019_leftImg8bit.png', 'lindau_000037_000019_gtFine_labelIds.png', 60)
('lindau_000038_000019_leftImg8bit.png', 'lindau_000038_000019_gtFine_labelIds.png', 61)
('lindau_000039_000019_leftImg8bit.png', 'lindau_000039_000019_gtFine_labelIds.png', 61)
('lindau_000040_000019_leftImg8bit.png', 'lindau_000040_000019_gtFine_labelIds.png', 61)
('lindau_000041_000019_leftImg8bit.png', 'lindau_000041_000019_gtFine_labelIds.png', 61)
('lindau_000042_000019_leftImg8bit.png', 'lindau_000042_000019_gtFine_labelIds.png', 61)
('lindau_000043_000019_leftImg8bit.png', 'lindau_000043_000019_gtFine_labelIds.png', 62)
('lindau_000044_000019_leftImg8bit.png', 'lindau_000044_000019_gtFine_labelIds.png', 62)
('lindau_000045_000019_leftImg8bit.png', 'lindau_000045_000019_gtFine_labelIds.png', 62)
('lindau_000046_000019_leftImg8bit.png', 'lindau_000046_000019_gtFine_labelIds.png', 62)
('lindau_000047_000019_leftImg8bit.png', 'lindau_000047_000019_gtFine_labelIds.png', 62)
('lindau_000048_000019_leftImg8bit.png', 'lindau_000048_000019_gtFine_labelIds.png', 63)
('lindau_000049_000019_leftImg8bit.png', 'lindau_000049_000019_gtFine_labelIds.png', 63)
('lindau_000050_000019_leftImg8bit.png', 'lindau_000050_000019_gtFine_labelIds.png', 63)
('lindau_000051_000019_leftImg8bit.png', 'lindau_000051_000019_gtFine_labelIds.png', 63)
('lindau_000052_000019_leftImg8bit.png', 'lindau_000052_000019_gtFine_labelIds.png', 63)
('lindau_000053_000019_leftImg8bit.png', 'lindau_000053_000019_gtFine_labelIds.png', 64)
('lindau_000054_000019_leftImg8bit.png', 'lindau_000054_000019_gtFine_labelIds.png', 64)
('lindau_000055_000019_leftImg8bit.png', 'lindau_000055_000019_gtFine_labelIds.png', 64)
('lindau_000056_000019_leftImg8bit.png', 'lindau_000056_000019_gtFine_labelIds.png', 64)
('lindau_000057_000019_leftImg8bit.png', 'lindau_000057_000019_gtFine_labelIds.png', 64)
pixel_accuracy=0.954557016057, mean_iou=0.811626097841, iou=[ 0.92705511  0.9144279   0.74552023  0.58102628  0.89010097]
('lindau_000058_000019_leftImg8bit.png', 'lindau_000058_000019_gtFine_labelIds.png', 65)
('munster_000000_000019_leftImg8bit.png', 'munster_000000_000019_gtFine_labelIds.png', 65)
('munster_000001_000019_leftImg8bit.png', 'munster_000001_000019_gtFine_labelIds.png', 65)
('munster_000002_000019_leftImg8bit.png', 'munster_000002_000019_gtFine_labelIds.png', 65)
('munster_000003_000019_leftImg8bit.png', 'munster_000003_000019_gtFine_labelIds.png', 65)
('munster_000004_000019_leftImg8bit.png', 'munster_000004_000019_gtFine_labelIds.png', 66)
('munster_000005_000019_leftImg8bit.png', 'munster_000005_000019_gtFine_labelIds.png', 66)
('munster_000006_000019_leftImg8bit.png', 'munster_000006_000019_gtFine_labelIds.png', 66)
('munster_000007_000019_leftImg8bit.png', 'munster_000007_000019_gtFine_labelIds.png', 66)
('munster_000008_000019_leftImg8bit.png', 'munster_000008_000019_gtFine_labelIds.png', 66)
('munster_000009_000019_leftImg8bit.png', 'munster_000009_000019_gtFine_labelIds.png', 67)
('munster_000010_000019_leftImg8bit.png', 'munster_000010_000019_gtFine_labelIds.png', 67)
('munster_000011_000019_leftImg8bit.png', 'munster_000011_000019_gtFine_labelIds.png', 67)
('munster_000012_000019_leftImg8bit.png', 'munster_000012_000019_gtFine_labelIds.png', 67)
('munster_000013_000019_leftImg8bit.png', 'munster_000013_000019_gtFine_labelIds.png', 67)
('munster_000014_000019_leftImg8bit.png', 'munster_000014_000019_gtFine_labelIds.png', 68)
('munster_000015_000019_leftImg8bit.png', 'munster_000015_000019_gtFine_labelIds.png', 68)
('munster_000016_000019_leftImg8bit.png', 'munster_000016_000019_gtFine_labelIds.png', 68)
('munster_000017_000019_leftImg8bit.png', 'munster_000017_000019_gtFine_labelIds.png', 68)
('munster_000018_000019_leftImg8bit.png', 'munster_000018_000019_gtFine_labelIds.png', 68)
('munster_000019_000019_leftImg8bit.png', 'munster_000019_000019_gtFine_labelIds.png', 69)
('munster_000020_000019_leftImg8bit.png', 'munster_000020_000019_gtFine_labelIds.png', 69)
('munster_000021_000019_leftImg8bit.png', 'munster_000021_000019_gtFine_labelIds.png', 69)
('munster_000022_000019_leftImg8bit.png', 'munster_000022_000019_gtFine_labelIds.png', 69)
('munster_000023_000019_leftImg8bit.png', 'munster_000023_000019_gtFine_labelIds.png', 69)
pixel_accuracy=0.956347578932, mean_iou=0.814868926811, iou=[ 0.92980923  0.91852478  0.74550232  0.587496    0.8930123 ]
('munster_000024_000019_leftImg8bit.png', 'munster_000024_000019_gtFine_labelIds.png', 70)
('munster_000025_000019_leftImg8bit.png', 'munster_000025_000019_gtFine_labelIds.png', 70)
('munster_000026_000019_leftImg8bit.png', 'munster_000026_000019_gtFine_labelIds.png', 70)
('munster_000027_000019_leftImg8bit.png', 'munster_000027_000019_gtFine_labelIds.png', 70)
('munster_000028_000019_leftImg8bit.png', 'munster_000028_000019_gtFine_labelIds.png', 70)
('munster_000029_000019_leftImg8bit.png', 'munster_000029_000019_gtFine_labelIds.png', 71)
('munster_000030_000019_leftImg8bit.png', 'munster_000030_000019_gtFine_labelIds.png', 71)
('munster_000031_000019_leftImg8bit.png', 'munster_000031_000019_gtFine_labelIds.png', 71)
('munster_000032_000019_leftImg8bit.png', 'munster_000032_000019_gtFine_labelIds.png', 71)
('munster_000033_000019_leftImg8bit.png', 'munster_000033_000019_gtFine_labelIds.png', 71)
('munster_000034_000019_leftImg8bit.png', 'munster_000034_000019_gtFine_labelIds.png', 72)
('munster_000035_000019_leftImg8bit.png', 'munster_000035_000019_gtFine_labelIds.png', 72)
('munster_000036_000019_leftImg8bit.png', 'munster_000036_000019_gtFine_labelIds.png', 72)
('munster_000037_000019_leftImg8bit.png', 'munster_000037_000019_gtFine_labelIds.png', 72)
('munster_000038_000019_leftImg8bit.png', 'munster_000038_000019_gtFine_labelIds.png', 72)
('munster_000039_000019_leftImg8bit.png', 'munster_000039_000019_gtFine_labelIds.png', 73)
('munster_000040_000019_leftImg8bit.png', 'munster_000040_000019_gtFine_labelIds.png', 73)
('munster_000041_000019_leftImg8bit.png', 'munster_000041_000019_gtFine_labelIds.png', 73)
('munster_000042_000019_leftImg8bit.png', 'munster_000042_000019_gtFine_labelIds.png', 73)
('munster_000043_000019_leftImg8bit.png', 'munster_000043_000019_gtFine_labelIds.png', 73)
('munster_000044_000019_leftImg8bit.png', 'munster_000044_000019_gtFine_labelIds.png', 74)
('munster_000045_000019_leftImg8bit.png', 'munster_000045_000019_gtFine_labelIds.png', 74)
('munster_000046_000019_leftImg8bit.png', 'munster_000046_000019_gtFine_labelIds.png', 74)
('munster_000047_000019_leftImg8bit.png', 'munster_000047_000019_gtFine_labelIds.png', 74)
('munster_000048_000019_leftImg8bit.png', 'munster_000048_000019_gtFine_labelIds.png', 74)
pixel_accuracy=0.957405577184, mean_iou=0.818760981391, iou=[ 0.93149498  0.92113284  0.75528659  0.59326698  0.89262352]
('munster_000049_000019_leftImg8bit.png', 'munster_000049_000019_gtFine_labelIds.png', 75)
('munster_000050_000019_leftImg8bit.png', 'munster_000050_000019_gtFine_labelIds.png', 75)
('munster_000051_000019_leftImg8bit.png', 'munster_000051_000019_gtFine_labelIds.png', 75)
('munster_000052_000019_leftImg8bit.png', 'munster_000052_000019_gtFine_labelIds.png', 75)
('munster_000053_000019_leftImg8bit.png', 'munster_000053_000019_gtFine_labelIds.png', 75)
('munster_000054_000019_leftImg8bit.png', 'munster_000054_000019_gtFine_labelIds.png', 76)
('munster_000055_000019_leftImg8bit.png', 'munster_000055_000019_gtFine_labelIds.png', 76)
('munster_000056_000019_leftImg8bit.png', 'munster_000056_000019_gtFine_labelIds.png', 76)
('munster_000057_000019_leftImg8bit.png', 'munster_000057_000019_gtFine_labelIds.png', 76)
('munster_000058_000019_leftImg8bit.png', 'munster_000058_000019_gtFine_labelIds.png', 76)
('munster_000059_000019_leftImg8bit.png', 'munster_000059_000019_gtFine_labelIds.png', 77)
('munster_000060_000019_leftImg8bit.png', 'munster_000060_000019_gtFine_labelIds.png', 77)
('munster_000061_000019_leftImg8bit.png', 'munster_000061_000019_gtFine_labelIds.png', 77)
('munster_000062_000019_leftImg8bit.png', 'munster_000062_000019_gtFine_labelIds.png', 77)
('munster_000063_000019_leftImg8bit.png', 'munster_000063_000019_gtFine_labelIds.png', 77)
('munster_000064_000019_leftImg8bit.png', 'munster_000064_000019_gtFine_labelIds.png', 78)
('munster_000065_000019_leftImg8bit.png', 'munster_000065_000019_gtFine_labelIds.png', 78)
('munster_000066_000019_leftImg8bit.png', 'munster_000066_000019_gtFine_labelIds.png', 78)
('munster_000067_000019_leftImg8bit.png', 'munster_000067_000019_gtFine_labelIds.png', 78)
('munster_000068_000019_leftImg8bit.png', 'munster_000068_000019_gtFine_labelIds.png', 78)
('munster_000069_000019_leftImg8bit.png', 'munster_000069_000019_gtFine_labelIds.png', 79)
('munster_000070_000019_leftImg8bit.png', 'munster_000070_000019_gtFine_labelIds.png', 79)
('munster_000071_000019_leftImg8bit.png', 'munster_000071_000019_gtFine_labelIds.png', 79)
('munster_000072_000019_leftImg8bit.png', 'munster_000072_000019_gtFine_labelIds.png', 79)
('munster_000073_000019_leftImg8bit.png', 'munster_000073_000019_gtFine_labelIds.png', 79)
pixel_accuracy=0.958679388494, mean_iou=0.821182247948, iou=[ 0.93344193  0.92455594  0.75627015  0.59659842  0.89504479]
('munster_000074_000019_leftImg8bit.png', 'munster_000074_000019_gtFine_labelIds.png', 80)
('munster_000075_000019_leftImg8bit.png', 'munster_000075_000019_gtFine_labelIds.png', 80)
('munster_000076_000019_leftImg8bit.png', 'munster_000076_000019_gtFine_labelIds.png', 80)
('munster_000077_000019_leftImg8bit.png', 'munster_000077_000019_gtFine_labelIds.png', 80)
('munster_000078_000019_leftImg8bit.png', 'munster_000078_000019_gtFine_labelIds.png', 80)
('munster_000079_000019_leftImg8bit.png', 'munster_000079_000019_gtFine_labelIds.png', 81)
('munster_000080_000019_leftImg8bit.png', 'munster_000080_000019_gtFine_labelIds.png', 81)
('munster_000081_000019_leftImg8bit.png', 'munster_000081_000019_gtFine_labelIds.png', 81)
('munster_000082_000019_leftImg8bit.png', 'munster_000082_000019_gtFine_labelIds.png', 81)
('munster_000083_000019_leftImg8bit.png', 'munster_000083_000019_gtFine_labelIds.png', 81)
('munster_000084_000019_leftImg8bit.png', 'munster_000084_000019_gtFine_labelIds.png', 82)
('munster_000085_000019_leftImg8bit.png', 'munster_000085_000019_gtFine_labelIds.png', 82)
('munster_000086_000019_leftImg8bit.png', 'munster_000086_000019_gtFine_labelIds.png', 82)
('munster_000087_000019_leftImg8bit.png', 'munster_000087_000019_gtFine_labelIds.png', 82)
('munster_000088_000019_leftImg8bit.png', 'munster_000088_000019_gtFine_labelIds.png', 82)
('munster_000089_000019_leftImg8bit.png', 'munster_000089_000019_gtFine_labelIds.png', 83)
('munster_000090_000019_leftImg8bit.png', 'munster_000090_000019_gtFine_labelIds.png', 83)
('munster_000091_000019_leftImg8bit.png', 'munster_000091_000019_gtFine_labelIds.png', 83)
('munster_000092_000019_leftImg8bit.png', 'munster_000092_000019_gtFine_labelIds.png', 83)
('munster_000093_000019_leftImg8bit.png', 'munster_000093_000019_gtFine_labelIds.png', 83)
('munster_000094_000019_leftImg8bit.png', 'munster_000094_000019_gtFine_labelIds.png', 84)
('munster_000095_000019_leftImg8bit.png', 'munster_000095_000019_gtFine_labelIds.png', 84)
('munster_000096_000019_leftImg8bit.png', 'munster_000096_000019_gtFine_labelIds.png', 84)
('munster_000097_000019_leftImg8bit.png', 'munster_000097_000019_gtFine_labelIds.png', 84)
('munster_000098_000019_leftImg8bit.png', 'munster_000098_000019_gtFine_labelIds.png', 84)
pixel_accuracy=0.960009795258, mean_iou=0.823796892201, iou=[ 0.93542286  0.92732967  0.75638217  0.59934007  0.90050969]
('munster_000099_000019_leftImg8bit.png', 'munster_000099_000019_gtFine_labelIds.png', 85)
('munster_000100_000019_leftImg8bit.png', 'munster_000100_000019_gtFine_labelIds.png', 85)
('munster_000101_000019_leftImg8bit.png', 'munster_000101_000019_gtFine_labelIds.png', 85)
('munster_000102_000019_leftImg8bit.png', 'munster_000102_000019_gtFine_labelIds.png', 85)
('munster_000103_000019_leftImg8bit.png', 'munster_000103_000019_gtFine_labelIds.png', 85)
('munster_000104_000019_leftImg8bit.png', 'munster_000104_000019_gtFine_labelIds.png', 86)
('munster_000105_000019_leftImg8bit.png', 'munster_000105_000019_gtFine_labelIds.png', 86)
('munster_000106_000019_leftImg8bit.png', 'munster_000106_000019_gtFine_labelIds.png', 86)
('munster_000107_000019_leftImg8bit.png', 'munster_000107_000019_gtFine_labelIds.png', 86)
('munster_000108_000019_leftImg8bit.png', 'munster_000108_000019_gtFine_labelIds.png', 86)
('munster_000109_000019_leftImg8bit.png', 'munster_000109_000019_gtFine_labelIds.png', 87)
('munster_000110_000019_leftImg8bit.png', 'munster_000110_000019_gtFine_labelIds.png', 87)
('munster_000111_000019_leftImg8bit.png', 'munster_000111_000019_gtFine_labelIds.png', 87)
('munster_000112_000019_leftImg8bit.png', 'munster_000112_000019_gtFine_labelIds.png', 87)
('munster_000113_000019_leftImg8bit.png', 'munster_000113_000019_gtFine_labelIds.png', 87)
('munster_000114_000019_leftImg8bit.png', 'munster_000114_000019_gtFine_labelIds.png', 88)
('munster_000115_000019_leftImg8bit.png', 'munster_000115_000019_gtFine_labelIds.png', 88)
('munster_000116_000019_leftImg8bit.png', 'munster_000116_000019_gtFine_labelIds.png', 88)
('munster_000117_000019_leftImg8bit.png', 'munster_000117_000019_gtFine_labelIds.png', 88)
('munster_000118_000019_leftImg8bit.png', 'munster_000118_000019_gtFine_labelIds.png', 88)
('munster_000119_000019_leftImg8bit.png', 'munster_000119_000019_gtFine_labelIds.png', 89)
('munster_000120_000019_leftImg8bit.png', 'munster_000120_000019_gtFine_labelIds.png', 89)
('munster_000121_000019_leftImg8bit.png', 'munster_000121_000019_gtFine_labelIds.png', 89)
('munster_000122_000019_leftImg8bit.png', 'munster_000122_000019_gtFine_labelIds.png', 89)
('munster_000123_000019_leftImg8bit.png', 'munster_000123_000019_gtFine_labelIds.png', 89)
pixel_accuracy=0.961396034715, mean_iou=0.825019394712, iou=[ 0.93767362  0.93030874  0.7556267   0.59941922  0.9020687 ]
('munster_000124_000019_leftImg8bit.png', 'munster_000124_000019_gtFine_labelIds.png', 90)
('munster_000125_000019_leftImg8bit.png', 'munster_000125_000019_gtFine_labelIds.png', 90)
('munster_000126_000019_leftImg8bit.png', 'munster_000126_000019_gtFine_labelIds.png', 90)
('munster_000127_000019_leftImg8bit.png', 'munster_000127_000019_gtFine_labelIds.png', 90)
('munster_000128_000019_leftImg8bit.png', 'munster_000128_000019_gtFine_labelIds.png', 90)
('munster_000129_000019_leftImg8bit.png', 'munster_000129_000019_gtFine_labelIds.png', 91)
('munster_000130_000019_leftImg8bit.png', 'munster_000130_000019_gtFine_labelIds.png', 91)
('munster_000131_000019_leftImg8bit.png', 'munster_000131_000019_gtFine_labelIds.png', 91)
('munster_000132_000019_leftImg8bit.png', 'munster_000132_000019_gtFine_labelIds.png', 91)
('munster_000133_000019_leftImg8bit.png', 'munster_000133_000019_gtFine_labelIds.png', 91)
('munster_000134_000019_leftImg8bit.png', 'munster_000134_000019_gtFine_labelIds.png', 92)
('munster_000135_000019_leftImg8bit.png', 'munster_000135_000019_gtFine_labelIds.png', 92)
('munster_000136_000019_leftImg8bit.png', 'munster_000136_000019_gtFine_labelIds.png', 92)
('munster_000137_000019_leftImg8bit.png', 'munster_000137_000019_gtFine_labelIds.png', 92)
('munster_000138_000019_leftImg8bit.png', 'munster_000138_000019_gtFine_labelIds.png', 92)
('munster_000139_000019_leftImg8bit.png', 'munster_000139_000019_gtFine_labelIds.png', 93)
('munster_000140_000019_leftImg8bit.png', 'munster_000140_000019_gtFine_labelIds.png', 93)
('munster_000141_000019_leftImg8bit.png', 'munster_000141_000019_gtFine_labelIds.png', 93)
('munster_000142_000019_leftImg8bit.png', 'munster_000142_000019_gtFine_labelIds.png', 93)
('munster_000143_000019_leftImg8bit.png', 'munster_000143_000019_gtFine_labelIds.png', 93)
('munster_000144_000019_leftImg8bit.png', 'munster_000144_000019_gtFine_labelIds.png', 94)
('munster_000145_000019_leftImg8bit.png', 'munster_000145_000019_gtFine_labelIds.png', 94)
('munster_000146_000019_leftImg8bit.png', 'munster_000146_000019_gtFine_labelIds.png', 94)
('munster_000147_000019_leftImg8bit.png', 'munster_000147_000019_gtFine_labelIds.png', 94)
('munster_000148_000019_leftImg8bit.png', 'munster_000148_000019_gtFine_labelIds.png', 94)
pixel_accuracy=0.958316211934, mean_iou=0.823486448009, iou=[ 0.93268274  0.921441    0.76102142  0.59950104  0.90278604]
('munster_000149_000019_leftImg8bit.png', 'munster_000149_000019_gtFine_labelIds.png', 95)
('munster_000150_000019_leftImg8bit.png', 'munster_000150_000019_gtFine_labelIds.png', 95)
('munster_000151_000019_leftImg8bit.png', 'munster_000151_000019_gtFine_labelIds.png', 95)
('munster_000152_000019_leftImg8bit.png', 'munster_000152_000019_gtFine_labelIds.png', 95)
('munster_000153_000019_leftImg8bit.png', 'munster_000153_000019_gtFine_labelIds.png', 95)
('munster_000154_000019_leftImg8bit.png', 'munster_000154_000019_gtFine_labelIds.png', 96)
('munster_000155_000019_leftImg8bit.png', 'munster_000155_000019_gtFine_labelIds.png', 96)
('munster_000156_000019_leftImg8bit.png', 'munster_000156_000019_gtFine_labelIds.png', 96)
('munster_000157_000019_leftImg8bit.png', 'munster_000157_000019_gtFine_labelIds.png', 96)
('munster_000158_000019_leftImg8bit.png', 'munster_000158_000019_gtFine_labelIds.png', 96)
('munster_000159_000019_leftImg8bit.png', 'munster_000159_000019_gtFine_labelIds.png', 97)
('munster_000160_000019_leftImg8bit.png', 'munster_000160_000019_gtFine_labelIds.png', 97)
('munster_000161_000019_leftImg8bit.png', 'munster_000161_000019_gtFine_labelIds.png', 97)
('munster_000162_000019_leftImg8bit.png', 'munster_000162_000019_gtFine_labelIds.png', 97)
('munster_000163_000019_leftImg8bit.png', 'munster_000163_000019_gtFine_labelIds.png', 97)
('munster_000164_000019_leftImg8bit.png', 'munster_000164_000019_gtFine_labelIds.png', 98)
('munster_000165_000019_leftImg8bit.png', 'munster_000165_000019_gtFine_labelIds.png', 98)
('munster_000166_000019_leftImg8bit.png', 'munster_000166_000019_gtFine_labelIds.png', 98)
('munster_000167_000019_leftImg8bit.png', 'munster_000167_000019_gtFine_labelIds.png', 98)
('munster_000168_000019_leftImg8bit.png', 'munster_000168_000019_gtFine_labelIds.png', 98)
('munster_000169_000019_leftImg8bit.png', 'munster_000169_000019_gtFine_labelIds.png', 99)
('munster_000170_000019_leftImg8bit.png', 'munster_000170_000019_gtFine_labelIds.png', 99)
('munster_000171_000019_leftImg8bit.png', 'munster_000171_000019_gtFine_labelIds.png', 99)
('munster_000172_000019_leftImg8bit.png', 'munster_000172_000019_gtFine_labelIds.png', 99)
('munster_000173_000019_leftImg8bit.png', 'munster_000173_000019_gtFine_labelIds.png', 99)
pixel_accuracy=0.959006259319, mean_iou=0.825381953171, iou=[ 0.93382825  0.92294517  0.76133137  0.60518775  0.90361722]
-------------------------------------------------------------
Final: pixel_accuracy=0.959006259319, mean_iou=0.825381953171, iou=[ 0.93382825  0.92294517  0.76133137  0.60518775  0.90361722]
-------------------------------------------------------------
sparse eval.
I0917 00:31:34.344310 27690 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0917 00:31:34.345001 27690 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I0917 00:31:34.345585 27690 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0917 00:31:34.346144 27690 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I0917 00:31:34.347806 27690 upgrade_proto.cpp:66] Attempting to upgrade input file specified using deprecated input fields: ../trained/image_segmentation/cityscapes5_jsegnet21v2/test_quantize/deploy.prototxt
I0917 00:31:34.347820 27690 upgrade_proto.cpp:69] Successfully upgraded file specified using deprecated input fields.
W0917 00:31:34.347822 27690 upgrade_proto.cpp:71] Note that future Caffe releases will only support input layers and not input fields.
I0917 00:31:34.348091 27690 net.cpp:72] Initializing net from parameters: 
name: "jsegnet21v2_deploy"
state {
  phase: TEST
}
layer {
  name: "input"
  type: "Input"
  top: "data"
  input_param {
    shape {
      dim: 1
      dim: 3
      dim: 1024
      dim: 2048
    }
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "argMaxOut"
  type: "ArgMax"
  bottom: "out_deconv_final_up8"
  top: "argMaxOut"
  argmax_param {
    axis: 1
  }
}
quantize: true
I0917 00:31:34.348207 27690 net.cpp:104] Using FLOAT as default forward math type
I0917 00:31:34.348212 27690 net.cpp:110] Using FLOAT as default backward math type
I0917 00:31:34.348213 27690 layer_factory.hpp:136] Creating layer 'input' of type 'Input'
I0917 00:31:34.348217 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.348223 27690 net.cpp:184] Created Layer input (0)
I0917 00:31:34.348228 27690 net.cpp:530] input -> data
I0917 00:31:34.348837 27690 net.cpp:245] Setting up input
I0917 00:31:34.348850 27690 net.cpp:252] TEST Top shape for layer 0 'input' 1 3 1024 2048 (6291456)
I0917 00:31:34.348863 27690 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0917 00:31:34.348867 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.348877 27690 net.cpp:184] Created Layer data/bias (1)
I0917 00:31:34.348881 27690 net.cpp:561] data/bias <- data
I0917 00:31:34.348883 27690 net.cpp:530] data/bias -> data/bias
I0917 00:31:34.353176 27690 net.cpp:245] Setting up data/bias
I0917 00:31:34.353194 27690 net.cpp:252] TEST Top shape for layer 1 'data/bias' 1 3 1024 2048 (6291456)
I0917 00:31:34.353202 27690 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0917 00:31:34.353206 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.353225 27690 net.cpp:184] Created Layer conv1a (2)
I0917 00:31:34.353229 27690 net.cpp:561] conv1a <- data/bias
I0917 00:31:34.353231 27690 net.cpp:530] conv1a -> conv1a
I0917 00:31:34.648684 27690 net.cpp:245] Setting up conv1a
I0917 00:31:34.648707 27690 net.cpp:252] TEST Top shape for layer 2 'conv1a' 1 32 512 1024 (16777216)
I0917 00:31:34.648718 27690 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0917 00:31:34.648722 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.648732 27690 net.cpp:184] Created Layer conv1a/bn (3)
I0917 00:31:34.648736 27690 net.cpp:561] conv1a/bn <- conv1a
I0917 00:31:34.648738 27690 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0917 00:31:34.649538 27690 net.cpp:245] Setting up conv1a/bn
I0917 00:31:34.649546 27690 net.cpp:252] TEST Top shape for layer 3 'conv1a/bn' 1 32 512 1024 (16777216)
I0917 00:31:34.649554 27690 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0917 00:31:34.649556 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.649560 27690 net.cpp:184] Created Layer conv1a/relu (4)
I0917 00:31:34.649562 27690 net.cpp:561] conv1a/relu <- conv1a
I0917 00:31:34.649564 27690 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0917 00:31:34.649574 27690 net.cpp:245] Setting up conv1a/relu
I0917 00:31:34.649577 27690 net.cpp:252] TEST Top shape for layer 4 'conv1a/relu' 1 32 512 1024 (16777216)
I0917 00:31:34.649580 27690 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0917 00:31:34.649581 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.649591 27690 net.cpp:184] Created Layer conv1b (5)
I0917 00:31:34.649595 27690 net.cpp:561] conv1b <- conv1a
I0917 00:31:34.649596 27690 net.cpp:530] conv1b -> conv1b
I0917 00:31:34.651077 27690 net.cpp:245] Setting up conv1b
I0917 00:31:34.651087 27690 net.cpp:252] TEST Top shape for layer 5 'conv1b' 1 32 512 1024 (16777216)
I0917 00:31:34.651091 27690 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0917 00:31:34.651094 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.651100 27690 net.cpp:184] Created Layer conv1b/bn (6)
I0917 00:31:34.651103 27690 net.cpp:561] conv1b/bn <- conv1b
I0917 00:31:34.651105 27690 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0917 00:31:34.651871 27690 net.cpp:245] Setting up conv1b/bn
I0917 00:31:34.651880 27690 net.cpp:252] TEST Top shape for layer 6 'conv1b/bn' 1 32 512 1024 (16777216)
I0917 00:31:34.651885 27690 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0917 00:31:34.651888 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.651891 27690 net.cpp:184] Created Layer conv1b/relu (7)
I0917 00:31:34.651893 27690 net.cpp:561] conv1b/relu <- conv1b
I0917 00:31:34.651895 27690 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0917 00:31:34.651898 27690 net.cpp:245] Setting up conv1b/relu
I0917 00:31:34.651901 27690 net.cpp:252] TEST Top shape for layer 7 'conv1b/relu' 1 32 512 1024 (16777216)
I0917 00:31:34.651903 27690 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0917 00:31:34.651916 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.651921 27690 net.cpp:184] Created Layer pool1 (8)
I0917 00:31:34.651923 27690 net.cpp:561] pool1 <- conv1b
I0917 00:31:34.651926 27690 net.cpp:530] pool1 -> pool1
I0917 00:31:34.651959 27690 net.cpp:245] Setting up pool1
I0917 00:31:34.651963 27690 net.cpp:252] TEST Top shape for layer 8 'pool1' 1 32 256 512 (4194304)
I0917 00:31:34.651965 27690 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0917 00:31:34.651968 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.651973 27690 net.cpp:184] Created Layer res2a_branch2a (9)
I0917 00:31:34.651975 27690 net.cpp:561] res2a_branch2a <- pool1
I0917 00:31:34.651978 27690 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0917 00:31:34.653077 27690 net.cpp:245] Setting up res2a_branch2a
I0917 00:31:34.653085 27690 net.cpp:252] TEST Top shape for layer 9 'res2a_branch2a' 1 64 256 512 (8388608)
I0917 00:31:34.653091 27690 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0917 00:31:34.653093 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.653097 27690 net.cpp:184] Created Layer res2a_branch2a/bn (10)
I0917 00:31:34.653100 27690 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0917 00:31:34.653102 27690 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0917 00:31:34.653579 27690 net.cpp:245] Setting up res2a_branch2a/bn
I0917 00:31:34.653586 27690 net.cpp:252] TEST Top shape for layer 10 'res2a_branch2a/bn' 1 64 256 512 (8388608)
I0917 00:31:34.653592 27690 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0917 00:31:34.653594 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.653597 27690 net.cpp:184] Created Layer res2a_branch2a/relu (11)
I0917 00:31:34.653599 27690 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0917 00:31:34.653602 27690 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0917 00:31:34.653605 27690 net.cpp:245] Setting up res2a_branch2a/relu
I0917 00:31:34.653607 27690 net.cpp:252] TEST Top shape for layer 11 'res2a_branch2a/relu' 1 64 256 512 (8388608)
I0917 00:31:34.653609 27690 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0917 00:31:34.653611 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.653617 27690 net.cpp:184] Created Layer res2a_branch2b (12)
I0917 00:31:34.653620 27690 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0917 00:31:34.653621 27690 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0917 00:31:34.654536 27690 net.cpp:245] Setting up res2a_branch2b
I0917 00:31:34.654543 27690 net.cpp:252] TEST Top shape for layer 12 'res2a_branch2b' 1 64 256 512 (8388608)
I0917 00:31:34.654547 27690 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0917 00:31:34.654551 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.654554 27690 net.cpp:184] Created Layer res2a_branch2b/bn (13)
I0917 00:31:34.654556 27690 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0917 00:31:34.654559 27690 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0917 00:31:34.655335 27690 net.cpp:245] Setting up res2a_branch2b/bn
I0917 00:31:34.655344 27690 net.cpp:252] TEST Top shape for layer 13 'res2a_branch2b/bn' 1 64 256 512 (8388608)
I0917 00:31:34.655349 27690 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0917 00:31:34.655351 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.655355 27690 net.cpp:184] Created Layer res2a_branch2b/relu (14)
I0917 00:31:34.655357 27690 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0917 00:31:34.655359 27690 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0917 00:31:34.655370 27690 net.cpp:245] Setting up res2a_branch2b/relu
I0917 00:31:34.655374 27690 net.cpp:252] TEST Top shape for layer 14 'res2a_branch2b/relu' 1 64 256 512 (8388608)
I0917 00:31:34.655375 27690 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0917 00:31:34.655377 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.655381 27690 net.cpp:184] Created Layer pool2 (15)
I0917 00:31:34.655383 27690 net.cpp:561] pool2 <- res2a_branch2b
I0917 00:31:34.655385 27690 net.cpp:530] pool2 -> pool2
I0917 00:31:34.655414 27690 net.cpp:245] Setting up pool2
I0917 00:31:34.655418 27690 net.cpp:252] TEST Top shape for layer 15 'pool2' 1 64 128 256 (2097152)
I0917 00:31:34.655421 27690 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0917 00:31:34.655422 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.655428 27690 net.cpp:184] Created Layer res3a_branch2a (16)
I0917 00:31:34.655431 27690 net.cpp:561] res3a_branch2a <- pool2
I0917 00:31:34.655432 27690 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0917 00:31:34.658308 27690 net.cpp:245] Setting up res3a_branch2a
I0917 00:31:34.658319 27690 net.cpp:252] TEST Top shape for layer 16 'res3a_branch2a' 1 128 128 256 (4194304)
I0917 00:31:34.658324 27690 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0917 00:31:34.658327 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.658331 27690 net.cpp:184] Created Layer res3a_branch2a/bn (17)
I0917 00:31:34.658334 27690 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0917 00:31:34.658337 27690 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0917 00:31:34.658746 27690 net.cpp:245] Setting up res3a_branch2a/bn
I0917 00:31:34.658752 27690 net.cpp:252] TEST Top shape for layer 17 'res3a_branch2a/bn' 1 128 128 256 (4194304)
I0917 00:31:34.658761 27690 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0917 00:31:34.658763 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.658766 27690 net.cpp:184] Created Layer res3a_branch2a/relu (18)
I0917 00:31:34.658768 27690 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0917 00:31:34.658771 27690 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0917 00:31:34.658773 27690 net.cpp:245] Setting up res3a_branch2a/relu
I0917 00:31:34.658776 27690 net.cpp:252] TEST Top shape for layer 18 'res3a_branch2a/relu' 1 128 128 256 (4194304)
I0917 00:31:34.658778 27690 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0917 00:31:34.658780 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.658785 27690 net.cpp:184] Created Layer res3a_branch2b (19)
I0917 00:31:34.658788 27690 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0917 00:31:34.658790 27690 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0917 00:31:34.659683 27690 net.cpp:245] Setting up res3a_branch2b
I0917 00:31:34.659690 27690 net.cpp:252] TEST Top shape for layer 19 'res3a_branch2b' 1 128 128 256 (4194304)
I0917 00:31:34.659695 27690 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0917 00:31:34.659698 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.659701 27690 net.cpp:184] Created Layer res3a_branch2b/bn (20)
I0917 00:31:34.659703 27690 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0917 00:31:34.659706 27690 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0917 00:31:34.660091 27690 net.cpp:245] Setting up res3a_branch2b/bn
I0917 00:31:34.660099 27690 net.cpp:252] TEST Top shape for layer 20 'res3a_branch2b/bn' 1 128 128 256 (4194304)
I0917 00:31:34.660104 27690 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0917 00:31:34.660105 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.660116 27690 net.cpp:184] Created Layer res3a_branch2b/relu (21)
I0917 00:31:34.660118 27690 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0917 00:31:34.660120 27690 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0917 00:31:34.660123 27690 net.cpp:245] Setting up res3a_branch2b/relu
I0917 00:31:34.660126 27690 net.cpp:252] TEST Top shape for layer 21 'res3a_branch2b/relu' 1 128 128 256 (4194304)
I0917 00:31:34.660128 27690 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0917 00:31:34.660130 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.660133 27690 net.cpp:184] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (22)
I0917 00:31:34.660135 27690 net.cpp:561] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0917 00:31:34.660137 27690 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0917 00:31:34.660140 27690 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0917 00:31:34.660162 27690 net.cpp:245] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0917 00:31:34.660166 27690 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 1 128 128 256 (4194304)
I0917 00:31:34.660168 27690 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 1 128 128 256 (4194304)
I0917 00:31:34.660171 27690 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0917 00:31:34.660172 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.660176 27690 net.cpp:184] Created Layer pool3 (23)
I0917 00:31:34.660178 27690 net.cpp:561] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0917 00:31:34.660181 27690 net.cpp:530] pool3 -> pool3
I0917 00:31:34.660207 27690 net.cpp:245] Setting up pool3
I0917 00:31:34.660210 27690 net.cpp:252] TEST Top shape for layer 23 'pool3' 1 128 64 128 (1048576)
I0917 00:31:34.660212 27690 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0917 00:31:34.660215 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.660221 27690 net.cpp:184] Created Layer res4a_branch2a (24)
I0917 00:31:34.660225 27690 net.cpp:561] res4a_branch2a <- pool3
I0917 00:31:34.660228 27690 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0917 00:31:34.666227 27690 net.cpp:245] Setting up res4a_branch2a
I0917 00:31:34.666236 27690 net.cpp:252] TEST Top shape for layer 24 'res4a_branch2a' 1 256 64 128 (2097152)
I0917 00:31:34.666240 27690 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0917 00:31:34.666244 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.666250 27690 net.cpp:184] Created Layer res4a_branch2a/bn (25)
I0917 00:31:34.666254 27690 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0917 00:31:34.666256 27690 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0917 00:31:34.666649 27690 net.cpp:245] Setting up res4a_branch2a/bn
I0917 00:31:34.666656 27690 net.cpp:252] TEST Top shape for layer 25 'res4a_branch2a/bn' 1 256 64 128 (2097152)
I0917 00:31:34.666661 27690 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0917 00:31:34.666663 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.666666 27690 net.cpp:184] Created Layer res4a_branch2a/relu (26)
I0917 00:31:34.666668 27690 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0917 00:31:34.666671 27690 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0917 00:31:34.666673 27690 net.cpp:245] Setting up res4a_branch2a/relu
I0917 00:31:34.666676 27690 net.cpp:252] TEST Top shape for layer 26 'res4a_branch2a/relu' 1 256 64 128 (2097152)
I0917 00:31:34.666683 27690 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0917 00:31:34.666687 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.666692 27690 net.cpp:184] Created Layer res4a_branch2b (27)
I0917 00:31:34.666693 27690 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0917 00:31:34.666695 27690 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0917 00:31:34.669755 27690 net.cpp:245] Setting up res4a_branch2b
I0917 00:31:34.669762 27690 net.cpp:252] TEST Top shape for layer 27 'res4a_branch2b' 1 256 64 128 (2097152)
I0917 00:31:34.669765 27690 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0917 00:31:34.669769 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.669772 27690 net.cpp:184] Created Layer res4a_branch2b/bn (28)
I0917 00:31:34.669775 27690 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0917 00:31:34.669776 27690 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0917 00:31:34.670163 27690 net.cpp:245] Setting up res4a_branch2b/bn
I0917 00:31:34.670171 27690 net.cpp:252] TEST Top shape for layer 28 'res4a_branch2b/bn' 1 256 64 128 (2097152)
I0917 00:31:34.670176 27690 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0917 00:31:34.670177 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.670181 27690 net.cpp:184] Created Layer res4a_branch2b/relu (29)
I0917 00:31:34.670182 27690 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0917 00:31:34.670184 27690 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0917 00:31:34.670187 27690 net.cpp:245] Setting up res4a_branch2b/relu
I0917 00:31:34.670189 27690 net.cpp:252] TEST Top shape for layer 29 'res4a_branch2b/relu' 1 256 64 128 (2097152)
I0917 00:31:34.670192 27690 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0917 00:31:34.670193 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.670197 27690 net.cpp:184] Created Layer pool4 (30)
I0917 00:31:34.670198 27690 net.cpp:561] pool4 <- res4a_branch2b
I0917 00:31:34.670202 27690 net.cpp:530] pool4 -> pool4
I0917 00:31:34.670233 27690 net.cpp:245] Setting up pool4
I0917 00:31:34.670238 27690 net.cpp:252] TEST Top shape for layer 30 'pool4' 1 256 64 128 (2097152)
I0917 00:31:34.670239 27690 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0917 00:31:34.670241 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.670246 27690 net.cpp:184] Created Layer res5a_branch2a (31)
I0917 00:31:34.670249 27690 net.cpp:561] res5a_branch2a <- pool4
I0917 00:31:34.670251 27690 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0917 00:31:34.695365 27690 net.cpp:245] Setting up res5a_branch2a
I0917 00:31:34.695385 27690 net.cpp:252] TEST Top shape for layer 31 'res5a_branch2a' 1 512 64 128 (4194304)
I0917 00:31:34.695392 27690 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0917 00:31:34.695396 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.695410 27690 net.cpp:184] Created Layer res5a_branch2a/bn (32)
I0917 00:31:34.695412 27690 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0917 00:31:34.695415 27690 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0917 00:31:34.696187 27690 net.cpp:245] Setting up res5a_branch2a/bn
I0917 00:31:34.696197 27690 net.cpp:252] TEST Top shape for layer 32 'res5a_branch2a/bn' 1 512 64 128 (4194304)
I0917 00:31:34.696202 27690 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0917 00:31:34.696204 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.696208 27690 net.cpp:184] Created Layer res5a_branch2a/relu (33)
I0917 00:31:34.696211 27690 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0917 00:31:34.696223 27690 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0917 00:31:34.696228 27690 net.cpp:245] Setting up res5a_branch2a/relu
I0917 00:31:34.696230 27690 net.cpp:252] TEST Top shape for layer 33 'res5a_branch2a/relu' 1 512 64 128 (4194304)
I0917 00:31:34.696233 27690 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0917 00:31:34.696234 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.696240 27690 net.cpp:184] Created Layer res5a_branch2b (34)
I0917 00:31:34.696244 27690 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0917 00:31:34.696245 27690 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0917 00:31:34.708629 27690 net.cpp:245] Setting up res5a_branch2b
I0917 00:31:34.708648 27690 net.cpp:252] TEST Top shape for layer 34 'res5a_branch2b' 1 512 64 128 (4194304)
I0917 00:31:34.708657 27690 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0917 00:31:34.708660 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.708667 27690 net.cpp:184] Created Layer res5a_branch2b/bn (35)
I0917 00:31:34.708670 27690 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0917 00:31:34.708674 27690 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0917 00:31:34.709070 27690 net.cpp:245] Setting up res5a_branch2b/bn
I0917 00:31:34.709077 27690 net.cpp:252] TEST Top shape for layer 35 'res5a_branch2b/bn' 1 512 64 128 (4194304)
I0917 00:31:34.709082 27690 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0917 00:31:34.709085 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.709089 27690 net.cpp:184] Created Layer res5a_branch2b/relu (36)
I0917 00:31:34.709090 27690 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0917 00:31:34.709094 27690 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0917 00:31:34.709096 27690 net.cpp:245] Setting up res5a_branch2b/relu
I0917 00:31:34.709100 27690 net.cpp:252] TEST Top shape for layer 36 'res5a_branch2b/relu' 1 512 64 128 (4194304)
I0917 00:31:34.709101 27690 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I0917 00:31:34.709103 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.709110 27690 net.cpp:184] Created Layer out5a (37)
I0917 00:31:34.709111 27690 net.cpp:561] out5a <- res5a_branch2b
I0917 00:31:34.709113 27690 net.cpp:530] out5a -> out5a
I0917 00:31:34.712822 27690 net.cpp:245] Setting up out5a
I0917 00:31:34.712833 27690 net.cpp:252] TEST Top shape for layer 37 'out5a' 1 64 64 128 (524288)
I0917 00:31:34.712837 27690 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I0917 00:31:34.712841 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.712846 27690 net.cpp:184] Created Layer out5a/bn (38)
I0917 00:31:34.712848 27690 net.cpp:561] out5a/bn <- out5a
I0917 00:31:34.712851 27690 net.cpp:513] out5a/bn -> out5a (in-place)
I0917 00:31:34.713266 27690 net.cpp:245] Setting up out5a/bn
I0917 00:31:34.713273 27690 net.cpp:252] TEST Top shape for layer 38 'out5a/bn' 1 64 64 128 (524288)
I0917 00:31:34.713277 27690 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I0917 00:31:34.713280 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.713282 27690 net.cpp:184] Created Layer out5a/relu (39)
I0917 00:31:34.713284 27690 net.cpp:561] out5a/relu <- out5a
I0917 00:31:34.713287 27690 net.cpp:513] out5a/relu -> out5a (in-place)
I0917 00:31:34.713290 27690 net.cpp:245] Setting up out5a/relu
I0917 00:31:34.713292 27690 net.cpp:252] TEST Top shape for layer 39 'out5a/relu' 1 64 64 128 (524288)
I0917 00:31:34.713294 27690 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I0917 00:31:34.713296 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.713316 27690 net.cpp:184] Created Layer out5a_up2 (40)
I0917 00:31:34.713320 27690 net.cpp:561] out5a_up2 <- out5a
I0917 00:31:34.713322 27690 net.cpp:530] out5a_up2 -> out5a_up2
I0917 00:31:34.713460 27690 net.cpp:245] Setting up out5a_up2
I0917 00:31:34.713465 27690 net.cpp:252] TEST Top shape for layer 40 'out5a_up2' 1 64 128 256 (2097152)
I0917 00:31:34.713469 27690 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I0917 00:31:34.713470 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.713482 27690 net.cpp:184] Created Layer out3a (41)
I0917 00:31:34.713485 27690 net.cpp:561] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0917 00:31:34.713488 27690 net.cpp:530] out3a -> out3a
I0917 00:31:34.714452 27690 net.cpp:245] Setting up out3a
I0917 00:31:34.714460 27690 net.cpp:252] TEST Top shape for layer 41 'out3a' 1 64 128 256 (2097152)
I0917 00:31:34.714464 27690 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I0917 00:31:34.714467 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.714470 27690 net.cpp:184] Created Layer out3a/bn (42)
I0917 00:31:34.714473 27690 net.cpp:561] out3a/bn <- out3a
I0917 00:31:34.714475 27690 net.cpp:513] out3a/bn -> out3a (in-place)
I0917 00:31:34.714884 27690 net.cpp:245] Setting up out3a/bn
I0917 00:31:34.714891 27690 net.cpp:252] TEST Top shape for layer 42 'out3a/bn' 1 64 128 256 (2097152)
I0917 00:31:34.714896 27690 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I0917 00:31:34.714898 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.714901 27690 net.cpp:184] Created Layer out3a/relu (43)
I0917 00:31:34.714903 27690 net.cpp:561] out3a/relu <- out3a
I0917 00:31:34.714905 27690 net.cpp:513] out3a/relu -> out3a (in-place)
I0917 00:31:34.714908 27690 net.cpp:245] Setting up out3a/relu
I0917 00:31:34.714911 27690 net.cpp:252] TEST Top shape for layer 43 'out3a/relu' 1 64 128 256 (2097152)
I0917 00:31:34.714913 27690 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I0917 00:31:34.714915 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.714923 27690 net.cpp:184] Created Layer out3_out5_combined (44)
I0917 00:31:34.714926 27690 net.cpp:561] out3_out5_combined <- out5a_up2
I0917 00:31:34.714927 27690 net.cpp:561] out3_out5_combined <- out3a
I0917 00:31:34.714931 27690 net.cpp:530] out3_out5_combined -> out3_out5_combined
I0917 00:31:34.714946 27690 net.cpp:245] Setting up out3_out5_combined
I0917 00:31:34.714949 27690 net.cpp:252] TEST Top shape for layer 44 'out3_out5_combined' 1 64 128 256 (2097152)
I0917 00:31:34.714951 27690 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I0917 00:31:34.714953 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.714958 27690 net.cpp:184] Created Layer ctx_conv1 (45)
I0917 00:31:34.714962 27690 net.cpp:561] ctx_conv1 <- out3_out5_combined
I0917 00:31:34.714963 27690 net.cpp:530] ctx_conv1 -> ctx_conv1
I0917 00:31:34.715862 27690 net.cpp:245] Setting up ctx_conv1
I0917 00:31:34.715868 27690 net.cpp:252] TEST Top shape for layer 45 'ctx_conv1' 1 64 128 256 (2097152)
I0917 00:31:34.715873 27690 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I0917 00:31:34.715874 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.715878 27690 net.cpp:184] Created Layer ctx_conv1/bn (46)
I0917 00:31:34.715880 27690 net.cpp:561] ctx_conv1/bn <- ctx_conv1
I0917 00:31:34.715883 27690 net.cpp:513] ctx_conv1/bn -> ctx_conv1 (in-place)
I0917 00:31:34.716307 27690 net.cpp:245] Setting up ctx_conv1/bn
I0917 00:31:34.716315 27690 net.cpp:252] TEST Top shape for layer 46 'ctx_conv1/bn' 1 64 128 256 (2097152)
I0917 00:31:34.716320 27690 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I0917 00:31:34.716328 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.716332 27690 net.cpp:184] Created Layer ctx_conv1/relu (47)
I0917 00:31:34.716334 27690 net.cpp:561] ctx_conv1/relu <- ctx_conv1
I0917 00:31:34.716336 27690 net.cpp:513] ctx_conv1/relu -> ctx_conv1 (in-place)
I0917 00:31:34.716341 27690 net.cpp:245] Setting up ctx_conv1/relu
I0917 00:31:34.716343 27690 net.cpp:252] TEST Top shape for layer 47 'ctx_conv1/relu' 1 64 128 256 (2097152)
I0917 00:31:34.716346 27690 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I0917 00:31:34.716347 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.716352 27690 net.cpp:184] Created Layer ctx_conv2 (48)
I0917 00:31:34.716354 27690 net.cpp:561] ctx_conv2 <- ctx_conv1
I0917 00:31:34.716357 27690 net.cpp:530] ctx_conv2 -> ctx_conv2
I0917 00:31:34.717252 27690 net.cpp:245] Setting up ctx_conv2
I0917 00:31:34.717257 27690 net.cpp:252] TEST Top shape for layer 48 'ctx_conv2' 1 64 128 256 (2097152)
I0917 00:31:34.717262 27690 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I0917 00:31:34.717263 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.717267 27690 net.cpp:184] Created Layer ctx_conv2/bn (49)
I0917 00:31:34.717269 27690 net.cpp:561] ctx_conv2/bn <- ctx_conv2
I0917 00:31:34.717272 27690 net.cpp:513] ctx_conv2/bn -> ctx_conv2 (in-place)
I0917 00:31:34.717667 27690 net.cpp:245] Setting up ctx_conv2/bn
I0917 00:31:34.717674 27690 net.cpp:252] TEST Top shape for layer 49 'ctx_conv2/bn' 1 64 128 256 (2097152)
I0917 00:31:34.717679 27690 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I0917 00:31:34.717681 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.717684 27690 net.cpp:184] Created Layer ctx_conv2/relu (50)
I0917 00:31:34.717686 27690 net.cpp:561] ctx_conv2/relu <- ctx_conv2
I0917 00:31:34.717689 27690 net.cpp:513] ctx_conv2/relu -> ctx_conv2 (in-place)
I0917 00:31:34.717691 27690 net.cpp:245] Setting up ctx_conv2/relu
I0917 00:31:34.717694 27690 net.cpp:252] TEST Top shape for layer 50 'ctx_conv2/relu' 1 64 128 256 (2097152)
I0917 00:31:34.717695 27690 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I0917 00:31:34.717697 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.717702 27690 net.cpp:184] Created Layer ctx_conv3 (51)
I0917 00:31:34.717705 27690 net.cpp:561] ctx_conv3 <- ctx_conv2
I0917 00:31:34.717706 27690 net.cpp:530] ctx_conv3 -> ctx_conv3
I0917 00:31:34.718595 27690 net.cpp:245] Setting up ctx_conv3
I0917 00:31:34.718602 27690 net.cpp:252] TEST Top shape for layer 51 'ctx_conv3' 1 64 128 256 (2097152)
I0917 00:31:34.718605 27690 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I0917 00:31:34.718608 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.718612 27690 net.cpp:184] Created Layer ctx_conv3/bn (52)
I0917 00:31:34.718614 27690 net.cpp:561] ctx_conv3/bn <- ctx_conv3
I0917 00:31:34.718616 27690 net.cpp:513] ctx_conv3/bn -> ctx_conv3 (in-place)
I0917 00:31:34.719028 27690 net.cpp:245] Setting up ctx_conv3/bn
I0917 00:31:34.719034 27690 net.cpp:252] TEST Top shape for layer 52 'ctx_conv3/bn' 1 64 128 256 (2097152)
I0917 00:31:34.719039 27690 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I0917 00:31:34.719041 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.719044 27690 net.cpp:184] Created Layer ctx_conv3/relu (53)
I0917 00:31:34.719046 27690 net.cpp:561] ctx_conv3/relu <- ctx_conv3
I0917 00:31:34.719048 27690 net.cpp:513] ctx_conv3/relu -> ctx_conv3 (in-place)
I0917 00:31:34.719051 27690 net.cpp:245] Setting up ctx_conv3/relu
I0917 00:31:34.719063 27690 net.cpp:252] TEST Top shape for layer 53 'ctx_conv3/relu' 1 64 128 256 (2097152)
I0917 00:31:34.719064 27690 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I0917 00:31:34.719066 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.719071 27690 net.cpp:184] Created Layer ctx_conv4 (54)
I0917 00:31:34.719074 27690 net.cpp:561] ctx_conv4 <- ctx_conv3
I0917 00:31:34.719076 27690 net.cpp:530] ctx_conv4 -> ctx_conv4
I0917 00:31:34.719964 27690 net.cpp:245] Setting up ctx_conv4
I0917 00:31:34.719970 27690 net.cpp:252] TEST Top shape for layer 54 'ctx_conv4' 1 64 128 256 (2097152)
I0917 00:31:34.719974 27690 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I0917 00:31:34.719976 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.719980 27690 net.cpp:184] Created Layer ctx_conv4/bn (55)
I0917 00:31:34.719981 27690 net.cpp:561] ctx_conv4/bn <- ctx_conv4
I0917 00:31:34.719983 27690 net.cpp:513] ctx_conv4/bn -> ctx_conv4 (in-place)
I0917 00:31:34.720386 27690 net.cpp:245] Setting up ctx_conv4/bn
I0917 00:31:34.720392 27690 net.cpp:252] TEST Top shape for layer 55 'ctx_conv4/bn' 1 64 128 256 (2097152)
I0917 00:31:34.720397 27690 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I0917 00:31:34.720401 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.720407 27690 net.cpp:184] Created Layer ctx_conv4/relu (56)
I0917 00:31:34.720409 27690 net.cpp:561] ctx_conv4/relu <- ctx_conv4
I0917 00:31:34.720412 27690 net.cpp:513] ctx_conv4/relu -> ctx_conv4 (in-place)
I0917 00:31:34.720413 27690 net.cpp:245] Setting up ctx_conv4/relu
I0917 00:31:34.720417 27690 net.cpp:252] TEST Top shape for layer 56 'ctx_conv4/relu' 1 64 128 256 (2097152)
I0917 00:31:34.720418 27690 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I0917 00:31:34.720420 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.720424 27690 net.cpp:184] Created Layer ctx_final (57)
I0917 00:31:34.720427 27690 net.cpp:561] ctx_final <- ctx_conv4
I0917 00:31:34.720429 27690 net.cpp:530] ctx_final -> ctx_final
I0917 00:31:34.720695 27690 net.cpp:245] Setting up ctx_final
I0917 00:31:34.720701 27690 net.cpp:252] TEST Top shape for layer 57 'ctx_final' 1 8 128 256 (262144)
I0917 00:31:34.720705 27690 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I0917 00:31:34.720707 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.720710 27690 net.cpp:184] Created Layer ctx_final/relu (58)
I0917 00:31:34.720713 27690 net.cpp:561] ctx_final/relu <- ctx_final
I0917 00:31:34.720715 27690 net.cpp:513] ctx_final/relu -> ctx_final (in-place)
I0917 00:31:34.720719 27690 net.cpp:245] Setting up ctx_final/relu
I0917 00:31:34.720722 27690 net.cpp:252] TEST Top shape for layer 58 'ctx_final/relu' 1 8 128 256 (262144)
I0917 00:31:34.720724 27690 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I0917 00:31:34.720728 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.720736 27690 net.cpp:184] Created Layer out_deconv_final_up2 (59)
I0917 00:31:34.720738 27690 net.cpp:561] out_deconv_final_up2 <- ctx_final
I0917 00:31:34.720741 27690 net.cpp:530] out_deconv_final_up2 -> out_deconv_final_up2
I0917 00:31:34.720855 27690 net.cpp:245] Setting up out_deconv_final_up2
I0917 00:31:34.720860 27690 net.cpp:252] TEST Top shape for layer 59 'out_deconv_final_up2' 1 8 256 512 (1048576)
I0917 00:31:34.720862 27690 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I0917 00:31:34.720865 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.720868 27690 net.cpp:184] Created Layer out_deconv_final_up4 (60)
I0917 00:31:34.720870 27690 net.cpp:561] out_deconv_final_up4 <- out_deconv_final_up2
I0917 00:31:34.720877 27690 net.cpp:530] out_deconv_final_up4 -> out_deconv_final_up4
I0917 00:31:34.720996 27690 net.cpp:245] Setting up out_deconv_final_up4
I0917 00:31:34.721000 27690 net.cpp:252] TEST Top shape for layer 60 'out_deconv_final_up4' 1 8 512 1024 (4194304)
I0917 00:31:34.721004 27690 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I0917 00:31:34.721005 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.721014 27690 net.cpp:184] Created Layer out_deconv_final_up8 (61)
I0917 00:31:34.721016 27690 net.cpp:561] out_deconv_final_up8 <- out_deconv_final_up4
I0917 00:31:34.721019 27690 net.cpp:530] out_deconv_final_up8 -> out_deconv_final_up8
I0917 00:31:34.721134 27690 net.cpp:245] Setting up out_deconv_final_up8
I0917 00:31:34.721138 27690 net.cpp:252] TEST Top shape for layer 61 'out_deconv_final_up8' 1 8 1024 2048 (16777216)
I0917 00:31:34.721141 27690 layer_factory.hpp:136] Creating layer 'argMaxOut' of type 'ArgMax'
I0917 00:31:34.721143 27690 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0917 00:31:34.721146 27690 net.cpp:184] Created Layer argMaxOut (62)
I0917 00:31:34.721148 27690 net.cpp:561] argMaxOut <- out_deconv_final_up8
I0917 00:31:34.721150 27690 net.cpp:530] argMaxOut -> argMaxOut
I0917 00:31:34.721163 27690 net.cpp:245] Setting up argMaxOut
I0917 00:31:34.721168 27690 net.cpp:252] TEST Top shape for layer 62 'argMaxOut' 1 1 1024 2048 (2097152)
I0917 00:31:34.721169 27690 net.cpp:325] argMaxOut does not need backward computation.
I0917 00:31:34.721171 27690 net.cpp:325] out_deconv_final_up8 does not need backward computation.
I0917 00:31:34.721174 27690 net.cpp:325] out_deconv_final_up4 does not need backward computation.
I0917 00:31:34.721175 27690 net.cpp:325] out_deconv_final_up2 does not need backward computation.
I0917 00:31:34.721177 27690 net.cpp:325] ctx_final/relu does not need backward computation.
I0917 00:31:34.721179 27690 net.cpp:325] ctx_final does not need backward computation.
I0917 00:31:34.721181 27690 net.cpp:325] ctx_conv4/relu does not need backward computation.
I0917 00:31:34.721182 27690 net.cpp:325] ctx_conv4/bn does not need backward computation.
I0917 00:31:34.721184 27690 net.cpp:325] ctx_conv4 does not need backward computation.
I0917 00:31:34.721186 27690 net.cpp:325] ctx_conv3/relu does not need backward computation.
I0917 00:31:34.721189 27690 net.cpp:325] ctx_conv3/bn does not need backward computation.
I0917 00:31:34.721190 27690 net.cpp:325] ctx_conv3 does not need backward computation.
I0917 00:31:34.721191 27690 net.cpp:325] ctx_conv2/relu does not need backward computation.
I0917 00:31:34.721194 27690 net.cpp:325] ctx_conv2/bn does not need backward computation.
I0917 00:31:34.721195 27690 net.cpp:325] ctx_conv2 does not need backward computation.
I0917 00:31:34.721197 27690 net.cpp:325] ctx_conv1/relu does not need backward computation.
I0917 00:31:34.721199 27690 net.cpp:325] ctx_conv1/bn does not need backward computation.
I0917 00:31:34.721200 27690 net.cpp:325] ctx_conv1 does not need backward computation.
I0917 00:31:34.721204 27690 net.cpp:325] out3_out5_combined does not need backward computation.
I0917 00:31:34.721205 27690 net.cpp:325] out3a/relu does not need backward computation.
I0917 00:31:34.721207 27690 net.cpp:325] out3a/bn does not need backward computation.
I0917 00:31:34.721209 27690 net.cpp:325] out3a does not need backward computation.
I0917 00:31:34.721211 27690 net.cpp:325] out5a_up2 does not need backward computation.
I0917 00:31:34.721213 27690 net.cpp:325] out5a/relu does not need backward computation.
I0917 00:31:34.721215 27690 net.cpp:325] out5a/bn does not need backward computation.
I0917 00:31:34.721217 27690 net.cpp:325] out5a does not need backward computation.
I0917 00:31:34.721220 27690 net.cpp:325] res5a_branch2b/relu does not need backward computation.
I0917 00:31:34.721221 27690 net.cpp:325] res5a_branch2b/bn does not need backward computation.
I0917 00:31:34.721230 27690 net.cpp:325] res5a_branch2b does not need backward computation.
I0917 00:31:34.721231 27690 net.cpp:325] res5a_branch2a/relu does not need backward computation.
I0917 00:31:34.721233 27690 net.cpp:325] res5a_branch2a/bn does not need backward computation.
I0917 00:31:34.721235 27690 net.cpp:325] res5a_branch2a does not need backward computation.
I0917 00:31:34.721237 27690 net.cpp:325] pool4 does not need backward computation.
I0917 00:31:34.721240 27690 net.cpp:325] res4a_branch2b/relu does not need backward computation.
I0917 00:31:34.721241 27690 net.cpp:325] res4a_branch2b/bn does not need backward computation.
I0917 00:31:34.721243 27690 net.cpp:325] res4a_branch2b does not need backward computation.
I0917 00:31:34.721246 27690 net.cpp:325] res4a_branch2a/relu does not need backward computation.
I0917 00:31:34.721247 27690 net.cpp:325] res4a_branch2a/bn does not need backward computation.
I0917 00:31:34.721249 27690 net.cpp:325] res4a_branch2a does not need backward computation.
I0917 00:31:34.721251 27690 net.cpp:325] pool3 does not need backward computation.
I0917 00:31:34.721254 27690 net.cpp:325] res3a_branch2b_res3a_branch2b/relu_0_split does not need backward computation.
I0917 00:31:34.721256 27690 net.cpp:325] res3a_branch2b/relu does not need backward computation.
I0917 00:31:34.721258 27690 net.cpp:325] res3a_branch2b/bn does not need backward computation.
I0917 00:31:34.721259 27690 net.cpp:325] res3a_branch2b does not need backward computation.
I0917 00:31:34.721262 27690 net.cpp:325] res3a_branch2a/relu does not need backward computation.
I0917 00:31:34.721263 27690 net.cpp:325] res3a_branch2a/bn does not need backward computation.
I0917 00:31:34.721266 27690 net.cpp:325] res3a_branch2a does not need backward computation.
I0917 00:31:34.721268 27690 net.cpp:325] pool2 does not need backward computation.
I0917 00:31:34.721271 27690 net.cpp:325] res2a_branch2b/relu does not need backward computation.
I0917 00:31:34.721272 27690 net.cpp:325] res2a_branch2b/bn does not need backward computation.
I0917 00:31:34.721274 27690 net.cpp:325] res2a_branch2b does not need backward computation.
I0917 00:31:34.721276 27690 net.cpp:325] res2a_branch2a/relu does not need backward computation.
I0917 00:31:34.721278 27690 net.cpp:325] res2a_branch2a/bn does not need backward computation.
I0917 00:31:34.721279 27690 net.cpp:325] res2a_branch2a does not need backward computation.
I0917 00:31:34.721282 27690 net.cpp:325] pool1 does not need backward computation.
I0917 00:31:34.721283 27690 net.cpp:325] conv1b/relu does not need backward computation.
I0917 00:31:34.721285 27690 net.cpp:325] conv1b/bn does not need backward computation.
I0917 00:31:34.721287 27690 net.cpp:325] conv1b does not need backward computation.
I0917 00:31:34.721289 27690 net.cpp:325] conv1a/relu does not need backward computation.
I0917 00:31:34.721292 27690 net.cpp:325] conv1a/bn does not need backward computation.
I0917 00:31:34.721293 27690 net.cpp:325] conv1a does not need backward computation.
I0917 00:31:34.721295 27690 net.cpp:325] data/bias does not need backward computation.
I0917 00:31:34.721297 27690 net.cpp:325] input does not need backward computation.
I0917 00:31:34.721299 27690 net.cpp:367] This network produces output argMaxOut
I0917 00:31:34.721338 27690 net.cpp:389] Top memory (TEST) required for data: 1224736768 diff: 1224736768
I0917 00:31:34.721340 27690 net.cpp:392] Bottom memory (TEST) required for data: 1216348160 diff: 1216348160
I0917 00:31:34.721343 27690 net.cpp:395] Shared (in-place) memory (TEST) by data: 659554304 diff: 659554304
I0917 00:31:34.721344 27690 net.cpp:398] Parameters memory (TEST) required for data: 10817840 diff: 10817840
I0917 00:31:34.721345 27690 net.cpp:401] Parameters shared memory (TEST) by data: 0 diff: 0
I0917 00:31:34.721348 27690 net.cpp:407] Network initialization done.
I0917 00:31:34.726286 27690 net.cpp:1078] Ignoring source layer data
I0917 00:31:34.726305 27690 net.cpp:1094] Copying source layer data/bias Type:Bias #blobs=1
I0917 00:31:34.726333 27690 net.cpp:1094] Copying source layer conv1a Type:Convolution #blobs=2
I0917 00:31:34.726356 27690 net.cpp:1094] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0917 00:31:34.726495 27690 net.cpp:1094] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0917 00:31:34.726500 27690 net.cpp:1094] Copying source layer conv1b Type:Convolution #blobs=2
I0917 00:31:34.726507 27690 net.cpp:1094] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0917 00:31:34.726593 27690 net.cpp:1094] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0917 00:31:34.726598 27690 net.cpp:1094] Copying source layer pool1 Type:Pooling #blobs=0
I0917 00:31:34.726599 27690 net.cpp:1094] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0917 00:31:34.726614 27690 net.cpp:1094] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0917 00:31:34.726703 27690 net.cpp:1094] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0917 00:31:34.726707 27690 net.cpp:1094] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0917 00:31:34.726717 27690 net.cpp:1094] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0917 00:31:34.726801 27690 net.cpp:1094] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0917 00:31:34.726805 27690 net.cpp:1094] Copying source layer pool2 Type:Pooling #blobs=0
I0917 00:31:34.726807 27690 net.cpp:1094] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0917 00:31:34.726843 27690 net.cpp:1094] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0917 00:31:34.726924 27690 net.cpp:1094] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0917 00:31:34.726927 27690 net.cpp:1094] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0917 00:31:34.726948 27690 net.cpp:1094] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0917 00:31:34.727023 27690 net.cpp:1094] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0917 00:31:34.727027 27690 net.cpp:1094] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I0917 00:31:34.727030 27690 net.cpp:1094] Copying source layer pool3 Type:Pooling #blobs=0
I0917 00:31:34.727031 27690 net.cpp:1094] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0917 00:31:34.727144 27690 net.cpp:1094] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0917 00:31:34.727219 27690 net.cpp:1094] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0917 00:31:34.727223 27690 net.cpp:1094] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0917 00:31:34.727280 27690 net.cpp:1094] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0917 00:31:34.727356 27690 net.cpp:1094] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0917 00:31:34.727360 27690 net.cpp:1094] Copying source layer pool4 Type:Pooling #blobs=0
I0917 00:31:34.727362 27690 net.cpp:1094] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0917 00:31:34.727691 27690 net.cpp:1094] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0917 00:31:34.727768 27690 net.cpp:1094] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0917 00:31:34.727772 27690 net.cpp:1094] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0917 00:31:34.727936 27690 net.cpp:1094] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0917 00:31:34.728008 27690 net.cpp:1094] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0917 00:31:34.728011 27690 net.cpp:1094] Copying source layer out5a Type:Convolution #blobs=2
I0917 00:31:34.728061 27690 net.cpp:1094] Copying source layer out5a/bn Type:BatchNorm #blobs=5
I0917 00:31:34.728152 27690 net.cpp:1094] Copying source layer out5a/relu Type:ReLU #blobs=0
I0917 00:31:34.728155 27690 net.cpp:1094] Copying source layer out5a_up2 Type:Deconvolution #blobs=1
I0917 00:31:34.728160 27690 net.cpp:1094] Copying source layer out3a Type:Convolution #blobs=2
I0917 00:31:34.728178 27690 net.cpp:1094] Copying source layer out3a/bn Type:BatchNorm #blobs=5
I0917 00:31:34.728267 27690 net.cpp:1094] Copying source layer out3a/relu Type:ReLU #blobs=0
I0917 00:31:34.728271 27690 net.cpp:1094] Copying source layer out3_out5_combined Type:Eltwise #blobs=0
I0917 00:31:34.728273 27690 net.cpp:1094] Copying source layer ctx_conv1 Type:Convolution #blobs=2
I0917 00:31:34.728291 27690 net.cpp:1094] Copying source layer ctx_conv1/bn Type:BatchNorm #blobs=5
I0917 00:31:34.728376 27690 net.cpp:1094] Copying source layer ctx_conv1/relu Type:ReLU #blobs=0
I0917 00:31:34.728380 27690 net.cpp:1094] Copying source layer ctx_conv2 Type:Convolution #blobs=2
I0917 00:31:34.728399 27690 net.cpp:1094] Copying source layer ctx_conv2/bn Type:BatchNorm #blobs=5
I0917 00:31:34.728484 27690 net.cpp:1094] Copying source layer ctx_conv2/relu Type:ReLU #blobs=0
I0917 00:31:34.728488 27690 net.cpp:1094] Copying source layer ctx_conv3 Type:Convolution #blobs=2
I0917 00:31:34.728505 27690 net.cpp:1094] Copying source layer ctx_conv3/bn Type:BatchNorm #blobs=5
I0917 00:31:34.728587 27690 net.cpp:1094] Copying source layer ctx_conv3/relu Type:ReLU #blobs=0
I0917 00:31:34.728591 27690 net.cpp:1094] Copying source layer ctx_conv4 Type:Convolution #blobs=2
I0917 00:31:34.728608 27690 net.cpp:1094] Copying source layer ctx_conv4/bn Type:BatchNorm #blobs=5
I0917 00:31:34.728691 27690 net.cpp:1094] Copying source layer ctx_conv4/relu Type:ReLU #blobs=0
I0917 00:31:34.728694 27690 net.cpp:1094] Copying source layer ctx_final Type:Convolution #blobs=2
I0917 00:31:34.728703 27690 net.cpp:1094] Copying source layer ctx_final/relu Type:ReLU #blobs=0
I0917 00:31:34.728705 27690 net.cpp:1094] Copying source layer out_deconv_final_up2 Type:Deconvolution #blobs=1
I0917 00:31:34.728711 27690 net.cpp:1094] Copying source layer out_deconv_final_up4 Type:Deconvolution #blobs=1
I0917 00:31:34.728715 27690 net.cpp:1094] Copying source layer out_deconv_final_up8 Type:Deconvolution #blobs=1
I0917 00:31:34.728721 27690 net.cpp:1078] Ignoring source layer loss
Namespace(batch_size=1, blend=False, class_dict='', crop=['0'], input='./data/val-image-list.txt', label='./data/val-label-list.txt', label_dict='', model='../trained/image_segmentation/cityscapes5_jsegnet21v2/test_quantize/deploy.prototxt', num_classes=5, num_images=500, output=None, palette='', resize=['0'], resize_back=True, search='*.png', weights='../trained/image_segmentation/cityscapes5_jsegnet21v2/sparse/cityscapes5_jsegnet21v2_iter_60000.caffemodel')
Infering list
Getting list of images...running inference for  500  images...
('frankfurt_000000_000294_leftImg8bit.png', 'frankfurt_000000_000294_gtFine_labelIds.png', 0)
('frankfurt_000000_000576_leftImg8bit.png', 'frankfurt_000000_000576_gtFine_labelIds.png', 0)
I0917 00:31:35.271744 27690 net.cpp:1597] Adding quantization params at infer/iter index: 1
('frankfurt_000000_001016_leftImg8bit.png', 'frankfurt_000000_001016_gtFine_labelIds.png', 0)
I0917 00:31:35.640244 27690 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'conv1a' with space 0.01G 3/1 1 	(avail 5.9G, req 0.01G)	t: 0
I0917 00:31:35.770251 27690 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'conv1b' with space 0.01G 32/4 6 	(avail 5.9G, req 0.01G)	t: 0
I0917 00:31:35.788607 27690 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res2a_branch2a' with space 0.01G 32/1 6 	(avail 5.9G, req 0.01G)	t: 0
I0917 00:31:35.832178 27690 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res2a_branch2b' with space 0.01G 64/4 6 	(avail 5.9G, req 0.01G)	t: 0
I0917 00:31:35.843528 27690 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res3a_branch2a' with space 0.01G 64/1 6 	(avail 5.9G, req 0.01G)	t: 0
I0917 00:31:35.847448 27690 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res3a_branch2b' with space 0.01G 128/4 6 	(avail 5.9G, req 0.01G)	t: 0
I0917 00:31:35.856807 27690 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res4a_branch2a' with space 0.01G 128/1 6 	(avail 5.9G, req 0.01G)	t: 0
I0917 00:31:35.860172 27690 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res4a_branch2b' with space 0.01G 256/4 6 	(avail 5.9G, req 0.01G)	t: 0
I0917 00:31:35.874677 27690 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'out3a' with space 0.01G 128/2 6 	(avail 5.9G, req 0.01G)	t: 0
I0917 00:31:35.881469 27690 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'ctx_conv1' with space 0.01G 64/1 6 	(avail 5.9G, req 0.01G)	t: 0
I0917 00:31:35.907300 27690 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'ctx_final' with space 0.01G 64/1 6 	(avail 5.9G, req 0.01G)	t: 0
('frankfurt_000000_001236_leftImg8bit.png', 'frankfurt_000000_001236_gtFine_labelIds.png', 0)
('frankfurt_000000_001751_leftImg8bit.png', 'frankfurt_000000_001751_gtFine_labelIds.png', 0)
('frankfurt_000000_002196_leftImg8bit.png', 'frankfurt_000000_002196_gtFine_labelIds.png', 1)
('frankfurt_000000_002963_leftImg8bit.png', 'frankfurt_000000_002963_gtFine_labelIds.png', 1)
('frankfurt_000000_003025_leftImg8bit.png', 'frankfurt_000000_003025_gtFine_labelIds.png', 1)
('frankfurt_000000_003357_leftImg8bit.png', 'frankfurt_000000_003357_gtFine_labelIds.png', 1)
('frankfurt_000000_003920_leftImg8bit.png', 'frankfurt_000000_003920_gtFine_labelIds.png', 1)
('frankfurt_000000_004617_leftImg8bit.png', 'frankfurt_000000_004617_gtFine_labelIds.png', 2)
('frankfurt_000000_005543_leftImg8bit.png', 'frankfurt_000000_005543_gtFine_labelIds.png', 2)
('frankfurt_000000_005898_leftImg8bit.png', 'frankfurt_000000_005898_gtFine_labelIds.png', 2)
('frankfurt_000000_006589_leftImg8bit.png', 'frankfurt_000000_006589_gtFine_labelIds.png', 2)
('frankfurt_000000_007365_leftImg8bit.png', 'frankfurt_000000_007365_gtFine_labelIds.png', 2)
('frankfurt_000000_008206_leftImg8bit.png', 'frankfurt_000000_008206_gtFine_labelIds.png', 3)
('frankfurt_000000_008451_leftImg8bit.png', 'frankfurt_000000_008451_gtFine_labelIds.png', 3)
('frankfurt_000000_009291_leftImg8bit.png', 'frankfurt_000000_009291_gtFine_labelIds.png', 3)
('frankfurt_000000_009561_leftImg8bit.png', 'frankfurt_000000_009561_gtFine_labelIds.png', 3)
('frankfurt_000000_009688_leftImg8bit.png', 'frankfurt_000000_009688_gtFine_labelIds.png', 3)
('frankfurt_000000_009969_leftImg8bit.png', 'frankfurt_000000_009969_gtFine_labelIds.png', 4)
('frankfurt_000000_010351_leftImg8bit.png', 'frankfurt_000000_010351_gtFine_labelIds.png', 4)
('frankfurt_000000_010763_leftImg8bit.png', 'frankfurt_000000_010763_gtFine_labelIds.png', 4)
('frankfurt_000000_011007_leftImg8bit.png', 'frankfurt_000000_011007_gtFine_labelIds.png', 4)
('frankfurt_000000_011074_leftImg8bit.png', 'frankfurt_000000_011074_gtFine_labelIds.png', 4)
pixel_accuracy=0.972103234981, mean_iou=0.819429230157, iou=[ 0.95302516  0.95683763  0.75310554  0.54249557  0.89168226]
('frankfurt_000000_011461_leftImg8bit.png', 'frankfurt_000000_011461_gtFine_labelIds.png', 5)
('frankfurt_000000_011810_leftImg8bit.png', 'frankfurt_000000_011810_gtFine_labelIds.png', 5)
('frankfurt_000000_012009_leftImg8bit.png', 'frankfurt_000000_012009_gtFine_labelIds.png', 5)
('frankfurt_000000_012121_leftImg8bit.png', 'frankfurt_000000_012121_gtFine_labelIds.png', 5)
('frankfurt_000000_012868_leftImg8bit.png', 'frankfurt_000000_012868_gtFine_labelIds.png', 5)
('frankfurt_000000_013067_leftImg8bit.png', 'frankfurt_000000_013067_gtFine_labelIds.png', 6)
('frankfurt_000000_013240_leftImg8bit.png', 'frankfurt_000000_013240_gtFine_labelIds.png', 6)
('frankfurt_000000_013382_leftImg8bit.png', 'frankfurt_000000_013382_gtFine_labelIds.png', 6)
('frankfurt_000000_013942_leftImg8bit.png', 'frankfurt_000000_013942_gtFine_labelIds.png', 6)
('frankfurt_000000_014480_leftImg8bit.png', 'frankfurt_000000_014480_gtFine_labelIds.png', 6)
('frankfurt_000000_015389_leftImg8bit.png', 'frankfurt_000000_015389_gtFine_labelIds.png', 7)
('frankfurt_000000_015676_leftImg8bit.png', 'frankfurt_000000_015676_gtFine_labelIds.png', 7)
('frankfurt_000000_016005_leftImg8bit.png', 'frankfurt_000000_016005_gtFine_labelIds.png', 7)
('frankfurt_000000_016286_leftImg8bit.png', 'frankfurt_000000_016286_gtFine_labelIds.png', 7)
('frankfurt_000000_017228_leftImg8bit.png', 'frankfurt_000000_017228_gtFine_labelIds.png', 7)
('frankfurt_000000_017476_leftImg8bit.png', 'frankfurt_000000_017476_gtFine_labelIds.png', 8)
('frankfurt_000000_018797_leftImg8bit.png', 'frankfurt_000000_018797_gtFine_labelIds.png', 8)
('frankfurt_000000_019607_leftImg8bit.png', 'frankfurt_000000_019607_gtFine_labelIds.png', 8)
('frankfurt_000000_020215_leftImg8bit.png', 'frankfurt_000000_020215_gtFine_labelIds.png', 8)
('frankfurt_000000_020321_leftImg8bit.png', 'frankfurt_000000_020321_gtFine_labelIds.png', 8)
('frankfurt_000000_020880_leftImg8bit.png', 'frankfurt_000000_020880_gtFine_labelIds.png', 9)
('frankfurt_000000_021667_leftImg8bit.png', 'frankfurt_000000_021667_gtFine_labelIds.png', 9)
('frankfurt_000000_021879_leftImg8bit.png', 'frankfurt_000000_021879_gtFine_labelIds.png', 9)
('frankfurt_000000_022254_leftImg8bit.png', 'frankfurt_000000_022254_gtFine_labelIds.png', 9)
('frankfurt_000000_022797_leftImg8bit.png', 'frankfurt_000000_022797_gtFine_labelIds.png', 9)
pixel_accuracy=0.971919920626, mean_iou=0.829700417555, iou=[ 0.95314985  0.96021638  0.76686822  0.5787225   0.88954515]
('frankfurt_000001_000538_leftImg8bit.png', 'frankfurt_000001_000538_gtFine_labelIds.png', 10)
('frankfurt_000001_001464_leftImg8bit.png', 'frankfurt_000001_001464_gtFine_labelIds.png', 10)
('frankfurt_000001_002512_leftImg8bit.png', 'frankfurt_000001_002512_gtFine_labelIds.png', 10)
('frankfurt_000001_002646_leftImg8bit.png', 'frankfurt_000001_002646_gtFine_labelIds.png', 10)
('frankfurt_000001_002759_leftImg8bit.png', 'frankfurt_000001_002759_gtFine_labelIds.png', 10)
('frankfurt_000001_003056_leftImg8bit.png', 'frankfurt_000001_003056_gtFine_labelIds.png', 11)
('frankfurt_000001_003588_leftImg8bit.png', 'frankfurt_000001_003588_gtFine_labelIds.png', 11)
('frankfurt_000001_004327_leftImg8bit.png', 'frankfurt_000001_004327_gtFine_labelIds.png', 11)
('frankfurt_000001_004736_leftImg8bit.png', 'frankfurt_000001_004736_gtFine_labelIds.png', 11)
('frankfurt_000001_004859_leftImg8bit.png', 'frankfurt_000001_004859_gtFine_labelIds.png', 11)
('frankfurt_000001_005184_leftImg8bit.png', 'frankfurt_000001_005184_gtFine_labelIds.png', 12)
('frankfurt_000001_005410_leftImg8bit.png', 'frankfurt_000001_005410_gtFine_labelIds.png', 12)
('frankfurt_000001_005703_leftImg8bit.png', 'frankfurt_000001_005703_gtFine_labelIds.png', 12)
('frankfurt_000001_005898_leftImg8bit.png', 'frankfurt_000001_005898_gtFine_labelIds.png', 12)
('frankfurt_000001_007285_leftImg8bit.png', 'frankfurt_000001_007285_gtFine_labelIds.png', 12)
('frankfurt_000001_007407_leftImg8bit.png', 'frankfurt_000001_007407_gtFine_labelIds.png', 13)
('frankfurt_000001_007622_leftImg8bit.png', 'frankfurt_000001_007622_gtFine_labelIds.png', 13)
('frankfurt_000001_007857_leftImg8bit.png', 'frankfurt_000001_007857_gtFine_labelIds.png', 13)
('frankfurt_000001_007973_leftImg8bit.png', 'frankfurt_000001_007973_gtFine_labelIds.png', 13)
('frankfurt_000001_008200_leftImg8bit.png', 'frankfurt_000001_008200_gtFine_labelIds.png', 13)
('frankfurt_000001_008688_leftImg8bit.png', 'frankfurt_000001_008688_gtFine_labelIds.png', 14)
('frankfurt_000001_009058_leftImg8bit.png', 'frankfurt_000001_009058_gtFine_labelIds.png', 14)
('frankfurt_000001_009504_leftImg8bit.png', 'frankfurt_000001_009504_gtFine_labelIds.png', 14)
('frankfurt_000001_009854_leftImg8bit.png', 'frankfurt_000001_009854_gtFine_labelIds.png', 14)
('frankfurt_000001_010156_leftImg8bit.png', 'frankfurt_000001_010156_gtFine_labelIds.png', 14)
pixel_accuracy=0.969984461538, mean_iou=0.824309722796, iou=[ 0.94994581  0.95765815  0.75175655  0.57705509  0.88513302]
('frankfurt_000001_010444_leftImg8bit.png', 'frankfurt_000001_010444_gtFine_labelIds.png', 15)
('frankfurt_000001_010600_leftImg8bit.png', 'frankfurt_000001_010600_gtFine_labelIds.png', 15)
('frankfurt_000001_010830_leftImg8bit.png', 'frankfurt_000001_010830_gtFine_labelIds.png', 15)
('frankfurt_000001_011162_leftImg8bit.png', 'frankfurt_000001_011162_gtFine_labelIds.png', 15)
('frankfurt_000001_011715_leftImg8bit.png', 'frankfurt_000001_011715_gtFine_labelIds.png', 15)
('frankfurt_000001_011835_leftImg8bit.png', 'frankfurt_000001_011835_gtFine_labelIds.png', 16)
('frankfurt_000001_012038_leftImg8bit.png', 'frankfurt_000001_012038_gtFine_labelIds.png', 16)
('frankfurt_000001_012519_leftImg8bit.png', 'frankfurt_000001_012519_gtFine_labelIds.png', 16)
('frankfurt_000001_012699_leftImg8bit.png', 'frankfurt_000001_012699_gtFine_labelIds.png', 16)
('frankfurt_000001_012738_leftImg8bit.png', 'frankfurt_000001_012738_gtFine_labelIds.png', 16)
('frankfurt_000001_012870_leftImg8bit.png', 'frankfurt_000001_012870_gtFine_labelIds.png', 17)
('frankfurt_000001_013016_leftImg8bit.png', 'frankfurt_000001_013016_gtFine_labelIds.png', 17)
('frankfurt_000001_013496_leftImg8bit.png', 'frankfurt_000001_013496_gtFine_labelIds.png', 17)
('frankfurt_000001_013710_leftImg8bit.png', 'frankfurt_000001_013710_gtFine_labelIds.png', 17)
('frankfurt_000001_014221_leftImg8bit.png', 'frankfurt_000001_014221_gtFine_labelIds.png', 17)
('frankfurt_000001_014406_leftImg8bit.png', 'frankfurt_000001_014406_gtFine_labelIds.png', 18)
('frankfurt_000001_014565_leftImg8bit.png', 'frankfurt_000001_014565_gtFine_labelIds.png', 18)
('frankfurt_000001_014741_leftImg8bit.png', 'frankfurt_000001_014741_gtFine_labelIds.png', 18)
('frankfurt_000001_015091_leftImg8bit.png', 'frankfurt_000001_015091_gtFine_labelIds.png', 18)
('frankfurt_000001_015328_leftImg8bit.png', 'frankfurt_000001_015328_gtFine_labelIds.png', 18)
('frankfurt_000001_015768_leftImg8bit.png', 'frankfurt_000001_015768_gtFine_labelIds.png', 19)
('frankfurt_000001_016029_leftImg8bit.png', 'frankfurt_000001_016029_gtFine_labelIds.png', 19)
('frankfurt_000001_016273_leftImg8bit.png', 'frankfurt_000001_016273_gtFine_labelIds.png', 19)
('frankfurt_000001_016462_leftImg8bit.png', 'frankfurt_000001_016462_gtFine_labelIds.png', 19)
('frankfurt_000001_017101_leftImg8bit.png', 'frankfurt_000001_017101_gtFine_labelIds.png', 19)
pixel_accuracy=0.967011308861, mean_iou=0.830535940714, iou=[ 0.94576202  0.95313129  0.77788688  0.59354225  0.88235726]
('frankfurt_000001_017459_leftImg8bit.png', 'frankfurt_000001_017459_gtFine_labelIds.png', 20)
('frankfurt_000001_017842_leftImg8bit.png', 'frankfurt_000001_017842_gtFine_labelIds.png', 20)
('frankfurt_000001_018113_leftImg8bit.png', 'frankfurt_000001_018113_gtFine_labelIds.png', 20)
('frankfurt_000001_019698_leftImg8bit.png', 'frankfurt_000001_019698_gtFine_labelIds.png', 20)
('frankfurt_000001_019854_leftImg8bit.png', 'frankfurt_000001_019854_gtFine_labelIds.png', 20)
('frankfurt_000001_019969_leftImg8bit.png', 'frankfurt_000001_019969_gtFine_labelIds.png', 21)
('frankfurt_000001_020046_leftImg8bit.png', 'frankfurt_000001_020046_gtFine_labelIds.png', 21)
('frankfurt_000001_020287_leftImg8bit.png', 'frankfurt_000001_020287_gtFine_labelIds.png', 21)
('frankfurt_000001_020693_leftImg8bit.png', 'frankfurt_000001_020693_gtFine_labelIds.png', 21)
('frankfurt_000001_021406_leftImg8bit.png', 'frankfurt_000001_021406_gtFine_labelIds.png', 21)
('frankfurt_000001_021825_leftImg8bit.png', 'frankfurt_000001_021825_gtFine_labelIds.png', 22)
('frankfurt_000001_023235_leftImg8bit.png', 'frankfurt_000001_023235_gtFine_labelIds.png', 22)
('frankfurt_000001_023369_leftImg8bit.png', 'frankfurt_000001_023369_gtFine_labelIds.png', 22)
('frankfurt_000001_023769_leftImg8bit.png', 'frankfurt_000001_023769_gtFine_labelIds.png', 22)
('frankfurt_000001_024927_leftImg8bit.png', 'frankfurt_000001_024927_gtFine_labelIds.png', 22)
('frankfurt_000001_025512_leftImg8bit.png', 'frankfurt_000001_025512_gtFine_labelIds.png', 23)
('frankfurt_000001_025713_leftImg8bit.png', 'frankfurt_000001_025713_gtFine_labelIds.png', 23)
('frankfurt_000001_025921_leftImg8bit.png', 'frankfurt_000001_025921_gtFine_labelIds.png', 23)
('frankfurt_000001_027325_leftImg8bit.png', 'frankfurt_000001_027325_gtFine_labelIds.png', 23)
('frankfurt_000001_028232_leftImg8bit.png', 'frankfurt_000001_028232_gtFine_labelIds.png', 23)
('frankfurt_000001_028335_leftImg8bit.png', 'frankfurt_000001_028335_gtFine_labelIds.png', 24)
('frankfurt_000001_028590_leftImg8bit.png', 'frankfurt_000001_028590_gtFine_labelIds.png', 24)
('frankfurt_000001_028854_leftImg8bit.png', 'frankfurt_000001_028854_gtFine_labelIds.png', 24)
('frankfurt_000001_029086_leftImg8bit.png', 'frankfurt_000001_029086_gtFine_labelIds.png', 24)
('frankfurt_000001_029236_leftImg8bit.png', 'frankfurt_000001_029236_gtFine_labelIds.png', 24)
pixel_accuracy=0.96723987938, mean_iou=0.827874731874, iou=[ 0.94649993  0.95309287  0.77385117  0.58623987  0.87968981]
('frankfurt_000001_029600_leftImg8bit.png', 'frankfurt_000001_029600_gtFine_labelIds.png', 25)
('frankfurt_000001_030067_leftImg8bit.png', 'frankfurt_000001_030067_gtFine_labelIds.png', 25)
('frankfurt_000001_030310_leftImg8bit.png', 'frankfurt_000001_030310_gtFine_labelIds.png', 25)
('frankfurt_000001_030669_leftImg8bit.png', 'frankfurt_000001_030669_gtFine_labelIds.png', 25)
('frankfurt_000001_031266_leftImg8bit.png', 'frankfurt_000001_031266_gtFine_labelIds.png', 25)
('frankfurt_000001_031416_leftImg8bit.png', 'frankfurt_000001_031416_gtFine_labelIds.png', 26)
('frankfurt_000001_032018_leftImg8bit.png', 'frankfurt_000001_032018_gtFine_labelIds.png', 26)
('frankfurt_000001_032556_leftImg8bit.png', 'frankfurt_000001_032556_gtFine_labelIds.png', 26)
('frankfurt_000001_032711_leftImg8bit.png', 'frankfurt_000001_032711_gtFine_labelIds.png', 26)
('frankfurt_000001_032942_leftImg8bit.png', 'frankfurt_000001_032942_gtFine_labelIds.png', 26)
('frankfurt_000001_033655_leftImg8bit.png', 'frankfurt_000001_033655_gtFine_labelIds.png', 27)
('frankfurt_000001_034047_leftImg8bit.png', 'frankfurt_000001_034047_gtFine_labelIds.png', 27)
('frankfurt_000001_034816_leftImg8bit.png', 'frankfurt_000001_034816_gtFine_labelIds.png', 27)
('frankfurt_000001_035144_leftImg8bit.png', 'frankfurt_000001_035144_gtFine_labelIds.png', 27)
('frankfurt_000001_035864_leftImg8bit.png', 'frankfurt_000001_035864_gtFine_labelIds.png', 27)
('frankfurt_000001_037705_leftImg8bit.png', 'frankfurt_000001_037705_gtFine_labelIds.png', 28)
('frankfurt_000001_038245_leftImg8bit.png', 'frankfurt_000001_038245_gtFine_labelIds.png', 28)
('frankfurt_000001_038418_leftImg8bit.png', 'frankfurt_000001_038418_gtFine_labelIds.png', 28)
('frankfurt_000001_038645_leftImg8bit.png', 'frankfurt_000001_038645_gtFine_labelIds.png', 28)
('frankfurt_000001_038844_leftImg8bit.png', 'frankfurt_000001_038844_gtFine_labelIds.png', 28)
('frankfurt_000001_039895_leftImg8bit.png', 'frankfurt_000001_039895_gtFine_labelIds.png', 29)
('frankfurt_000001_040575_leftImg8bit.png', 'frankfurt_000001_040575_gtFine_labelIds.png', 29)
('frankfurt_000001_040732_leftImg8bit.png', 'frankfurt_000001_040732_gtFine_labelIds.png', 29)
('frankfurt_000001_041074_leftImg8bit.png', 'frankfurt_000001_041074_gtFine_labelIds.png', 29)
('frankfurt_000001_041354_leftImg8bit.png', 'frankfurt_000001_041354_gtFine_labelIds.png', 29)
pixel_accuracy=0.967260027145, mean_iou=0.824175800156, iou=[ 0.94638278  0.95387554  0.75942759  0.57423959  0.8869535 ]
('frankfurt_000001_041517_leftImg8bit.png', 'frankfurt_000001_041517_gtFine_labelIds.png', 30)
('frankfurt_000001_041664_leftImg8bit.png', 'frankfurt_000001_041664_gtFine_labelIds.png', 30)
('frankfurt_000001_042098_leftImg8bit.png', 'frankfurt_000001_042098_gtFine_labelIds.png', 30)
('frankfurt_000001_042384_leftImg8bit.png', 'frankfurt_000001_042384_gtFine_labelIds.png', 30)
('frankfurt_000001_042733_leftImg8bit.png', 'frankfurt_000001_042733_gtFine_labelIds.png', 30)
('frankfurt_000001_043395_leftImg8bit.png', 'frankfurt_000001_043395_gtFine_labelIds.png', 31)
('frankfurt_000001_043564_leftImg8bit.png', 'frankfurt_000001_043564_gtFine_labelIds.png', 31)
('frankfurt_000001_044227_leftImg8bit.png', 'frankfurt_000001_044227_gtFine_labelIds.png', 31)
('frankfurt_000001_044413_leftImg8bit.png', 'frankfurt_000001_044413_gtFine_labelIds.png', 31)
('frankfurt_000001_044525_leftImg8bit.png', 'frankfurt_000001_044525_gtFine_labelIds.png', 31)
('frankfurt_000001_044658_leftImg8bit.png', 'frankfurt_000001_044658_gtFine_labelIds.png', 32)
('frankfurt_000001_044787_leftImg8bit.png', 'frankfurt_000001_044787_gtFine_labelIds.png', 32)
('frankfurt_000001_046126_leftImg8bit.png', 'frankfurt_000001_046126_gtFine_labelIds.png', 32)
('frankfurt_000001_046272_leftImg8bit.png', 'frankfurt_000001_046272_gtFine_labelIds.png', 32)
('frankfurt_000001_046504_leftImg8bit.png', 'frankfurt_000001_046504_gtFine_labelIds.png', 32)
('frankfurt_000001_046779_leftImg8bit.png', 'frankfurt_000001_046779_gtFine_labelIds.png', 33)
('frankfurt_000001_047178_leftImg8bit.png', 'frankfurt_000001_047178_gtFine_labelIds.png', 33)
('frankfurt_000001_047552_leftImg8bit.png', 'frankfurt_000001_047552_gtFine_labelIds.png', 33)
('frankfurt_000001_048196_leftImg8bit.png', 'frankfurt_000001_048196_gtFine_labelIds.png', 33)
('frankfurt_000001_048355_leftImg8bit.png', 'frankfurt_000001_048355_gtFine_labelIds.png', 33)
('frankfurt_000001_048654_leftImg8bit.png', 'frankfurt_000001_048654_gtFine_labelIds.png', 34)
('frankfurt_000001_049078_leftImg8bit.png', 'frankfurt_000001_049078_gtFine_labelIds.png', 34)
('frankfurt_000001_049209_leftImg8bit.png', 'frankfurt_000001_049209_gtFine_labelIds.png', 34)
('frankfurt_000001_049298_leftImg8bit.png', 'frankfurt_000001_049298_gtFine_labelIds.png', 34)
('frankfurt_000001_049698_leftImg8bit.png', 'frankfurt_000001_049698_gtFine_labelIds.png', 34)
pixel_accuracy=0.968883285327, mean_iou=0.825729283742, iou=[ 0.94919881  0.95617556  0.75885736  0.56878557  0.89562911]
('frankfurt_000001_049770_leftImg8bit.png', 'frankfurt_000001_049770_gtFine_labelIds.png', 35)
('frankfurt_000001_050149_leftImg8bit.png', 'frankfurt_000001_050149_gtFine_labelIds.png', 35)
('frankfurt_000001_050686_leftImg8bit.png', 'frankfurt_000001_050686_gtFine_labelIds.png', 35)
('frankfurt_000001_051516_leftImg8bit.png', 'frankfurt_000001_051516_gtFine_labelIds.png', 35)
('frankfurt_000001_051737_leftImg8bit.png', 'frankfurt_000001_051737_gtFine_labelIds.png', 35)
('frankfurt_000001_051807_leftImg8bit.png', 'frankfurt_000001_051807_gtFine_labelIds.png', 36)
('frankfurt_000001_052120_leftImg8bit.png', 'frankfurt_000001_052120_gtFine_labelIds.png', 36)
('frankfurt_000001_052594_leftImg8bit.png', 'frankfurt_000001_052594_gtFine_labelIds.png', 36)
('frankfurt_000001_053102_leftImg8bit.png', 'frankfurt_000001_053102_gtFine_labelIds.png', 36)
('frankfurt_000001_054077_leftImg8bit.png', 'frankfurt_000001_054077_gtFine_labelIds.png', 36)
('frankfurt_000001_054219_leftImg8bit.png', 'frankfurt_000001_054219_gtFine_labelIds.png', 37)
('frankfurt_000001_054415_leftImg8bit.png', 'frankfurt_000001_054415_gtFine_labelIds.png', 37)
('frankfurt_000001_054640_leftImg8bit.png', 'frankfurt_000001_054640_gtFine_labelIds.png', 37)
('frankfurt_000001_054884_leftImg8bit.png', 'frankfurt_000001_054884_gtFine_labelIds.png', 37)
('frankfurt_000001_055062_leftImg8bit.png', 'frankfurt_000001_055062_gtFine_labelIds.png', 37)
('frankfurt_000001_055172_leftImg8bit.png', 'frankfurt_000001_055172_gtFine_labelIds.png', 38)
('frankfurt_000001_055306_leftImg8bit.png', 'frankfurt_000001_055306_gtFine_labelIds.png', 38)
('frankfurt_000001_055387_leftImg8bit.png', 'frankfurt_000001_055387_gtFine_labelIds.png', 38)
('frankfurt_000001_055538_leftImg8bit.png', 'frankfurt_000001_055538_gtFine_labelIds.png', 38)
('frankfurt_000001_055603_leftImg8bit.png', 'frankfurt_000001_055603_gtFine_labelIds.png', 38)
('frankfurt_000001_055709_leftImg8bit.png', 'frankfurt_000001_055709_gtFine_labelIds.png', 39)
('frankfurt_000001_056580_leftImg8bit.png', 'frankfurt_000001_056580_gtFine_labelIds.png', 39)
('frankfurt_000001_057181_leftImg8bit.png', 'frankfurt_000001_057181_gtFine_labelIds.png', 39)
('frankfurt_000001_057478_leftImg8bit.png', 'frankfurt_000001_057478_gtFine_labelIds.png', 39)
('frankfurt_000001_057954_leftImg8bit.png', 'frankfurt_000001_057954_gtFine_labelIds.png', 39)
pixel_accuracy=0.968314381208, mean_iou=0.827702678888, iou=[ 0.94819123  0.95647784  0.76711341  0.5715639   0.89516701]
('frankfurt_000001_058057_leftImg8bit.png', 'frankfurt_000001_058057_gtFine_labelIds.png', 40)
('frankfurt_000001_058176_leftImg8bit.png', 'frankfurt_000001_058176_gtFine_labelIds.png', 40)
('frankfurt_000001_058504_leftImg8bit.png', 'frankfurt_000001_058504_gtFine_labelIds.png', 40)
('frankfurt_000001_058914_leftImg8bit.png', 'frankfurt_000001_058914_gtFine_labelIds.png', 40)
('frankfurt_000001_059119_leftImg8bit.png', 'frankfurt_000001_059119_gtFine_labelIds.png', 40)
('frankfurt_000001_059642_leftImg8bit.png', 'frankfurt_000001_059642_gtFine_labelIds.png', 41)
('frankfurt_000001_059789_leftImg8bit.png', 'frankfurt_000001_059789_gtFine_labelIds.png', 41)
('frankfurt_000001_060135_leftImg8bit.png', 'frankfurt_000001_060135_gtFine_labelIds.png', 41)
('frankfurt_000001_060422_leftImg8bit.png', 'frankfurt_000001_060422_gtFine_labelIds.png', 41)
('frankfurt_000001_060545_leftImg8bit.png', 'frankfurt_000001_060545_gtFine_labelIds.png', 41)
('frankfurt_000001_060906_leftImg8bit.png', 'frankfurt_000001_060906_gtFine_labelIds.png', 42)
('frankfurt_000001_061682_leftImg8bit.png', 'frankfurt_000001_061682_gtFine_labelIds.png', 42)
('frankfurt_000001_061763_leftImg8bit.png', 'frankfurt_000001_061763_gtFine_labelIds.png', 42)
('frankfurt_000001_062016_leftImg8bit.png', 'frankfurt_000001_062016_gtFine_labelIds.png', 42)
('frankfurt_000001_062250_leftImg8bit.png', 'frankfurt_000001_062250_gtFine_labelIds.png', 42)
('frankfurt_000001_062396_leftImg8bit.png', 'frankfurt_000001_062396_gtFine_labelIds.png', 43)
('frankfurt_000001_062509_leftImg8bit.png', 'frankfurt_000001_062509_gtFine_labelIds.png', 43)
('frankfurt_000001_062653_leftImg8bit.png', 'frankfurt_000001_062653_gtFine_labelIds.png', 43)
('frankfurt_000001_062793_leftImg8bit.png', 'frankfurt_000001_062793_gtFine_labelIds.png', 43)
('frankfurt_000001_063045_leftImg8bit.png', 'frankfurt_000001_063045_gtFine_labelIds.png', 43)
('frankfurt_000001_064130_leftImg8bit.png', 'frankfurt_000001_064130_gtFine_labelIds.png', 44)
('frankfurt_000001_064305_leftImg8bit.png', 'frankfurt_000001_064305_gtFine_labelIds.png', 44)
('frankfurt_000001_064651_leftImg8bit.png', 'frankfurt_000001_064651_gtFine_labelIds.png', 44)
('frankfurt_000001_064798_leftImg8bit.png', 'frankfurt_000001_064798_gtFine_labelIds.png', 44)
('frankfurt_000001_064925_leftImg8bit.png', 'frankfurt_000001_064925_gtFine_labelIds.png', 44)
pixel_accuracy=0.969141948438, mean_iou=0.8274632578, iou=[ 0.94958033  0.9585697   0.76228678  0.56949859  0.89738089]
('frankfurt_000001_065160_leftImg8bit.png', 'frankfurt_000001_065160_gtFine_labelIds.png', 45)
('frankfurt_000001_065617_leftImg8bit.png', 'frankfurt_000001_065617_gtFine_labelIds.png', 45)
('frankfurt_000001_065850_leftImg8bit.png', 'frankfurt_000001_065850_gtFine_labelIds.png', 45)
('frankfurt_000001_066092_leftImg8bit.png', 'frankfurt_000001_066092_gtFine_labelIds.png', 45)
('frankfurt_000001_066438_leftImg8bit.png', 'frankfurt_000001_066438_gtFine_labelIds.png', 45)
('frankfurt_000001_066574_leftImg8bit.png', 'frankfurt_000001_066574_gtFine_labelIds.png', 46)
('frankfurt_000001_066832_leftImg8bit.png', 'frankfurt_000001_066832_gtFine_labelIds.png', 46)
('frankfurt_000001_067092_leftImg8bit.png', 'frankfurt_000001_067092_gtFine_labelIds.png', 46)
('frankfurt_000001_067178_leftImg8bit.png', 'frankfurt_000001_067178_gtFine_labelIds.png', 46)
('frankfurt_000001_067295_leftImg8bit.png', 'frankfurt_000001_067295_gtFine_labelIds.png', 46)
('frankfurt_000001_067474_leftImg8bit.png', 'frankfurt_000001_067474_gtFine_labelIds.png', 47)
('frankfurt_000001_067735_leftImg8bit.png', 'frankfurt_000001_067735_gtFine_labelIds.png', 47)
('frankfurt_000001_068063_leftImg8bit.png', 'frankfurt_000001_068063_gtFine_labelIds.png', 47)
('frankfurt_000001_068208_leftImg8bit.png', 'frankfurt_000001_068208_gtFine_labelIds.png', 47)
('frankfurt_000001_068682_leftImg8bit.png', 'frankfurt_000001_068682_gtFine_labelIds.png', 47)
('frankfurt_000001_068772_leftImg8bit.png', 'frankfurt_000001_068772_gtFine_labelIds.png', 48)
('frankfurt_000001_069633_leftImg8bit.png', 'frankfurt_000001_069633_gtFine_labelIds.png', 48)
('frankfurt_000001_070099_leftImg8bit.png', 'frankfurt_000001_070099_gtFine_labelIds.png', 48)
('frankfurt_000001_071288_leftImg8bit.png', 'frankfurt_000001_071288_gtFine_labelIds.png', 48)
('frankfurt_000001_071781_leftImg8bit.png', 'frankfurt_000001_071781_gtFine_labelIds.png', 48)
('frankfurt_000001_072155_leftImg8bit.png', 'frankfurt_000001_072155_gtFine_labelIds.png', 49)
('frankfurt_000001_072295_leftImg8bit.png', 'frankfurt_000001_072295_gtFine_labelIds.png', 49)
('frankfurt_000001_073088_leftImg8bit.png', 'frankfurt_000001_073088_gtFine_labelIds.png', 49)
('frankfurt_000001_073243_leftImg8bit.png', 'frankfurt_000001_073243_gtFine_labelIds.png', 49)
('frankfurt_000001_073464_leftImg8bit.png', 'frankfurt_000001_073464_gtFine_labelIds.png', 49)
pixel_accuracy=0.968423328272, mean_iou=0.826020229607, iou=[ 0.94827356  0.95654051  0.76003683  0.56673925  0.898511  ]
('frankfurt_000001_073911_leftImg8bit.png', 'frankfurt_000001_073911_gtFine_labelIds.png', 50)
('frankfurt_000001_075296_leftImg8bit.png', 'frankfurt_000001_075296_gtFine_labelIds.png', 50)
('frankfurt_000001_075984_leftImg8bit.png', 'frankfurt_000001_075984_gtFine_labelIds.png', 50)
('frankfurt_000001_076502_leftImg8bit.png', 'frankfurt_000001_076502_gtFine_labelIds.png', 50)
('frankfurt_000001_077092_leftImg8bit.png', 'frankfurt_000001_077092_gtFine_labelIds.png', 50)
('frankfurt_000001_077233_leftImg8bit.png', 'frankfurt_000001_077233_gtFine_labelIds.png', 51)
('frankfurt_000001_077434_leftImg8bit.png', 'frankfurt_000001_077434_gtFine_labelIds.png', 51)
('frankfurt_000001_078803_leftImg8bit.png', 'frankfurt_000001_078803_gtFine_labelIds.png', 51)
('frankfurt_000001_079206_leftImg8bit.png', 'frankfurt_000001_079206_gtFine_labelIds.png', 51)
('frankfurt_000001_080091_leftImg8bit.png', 'frankfurt_000001_080091_gtFine_labelIds.png', 51)
('frankfurt_000001_080391_leftImg8bit.png', 'frankfurt_000001_080391_gtFine_labelIds.png', 52)
('frankfurt_000001_080830_leftImg8bit.png', 'frankfurt_000001_080830_gtFine_labelIds.png', 52)
('frankfurt_000001_082087_leftImg8bit.png', 'frankfurt_000001_082087_gtFine_labelIds.png', 52)
('frankfurt_000001_082466_leftImg8bit.png', 'frankfurt_000001_082466_gtFine_labelIds.png', 52)
('frankfurt_000001_083029_leftImg8bit.png', 'frankfurt_000001_083029_gtFine_labelIds.png', 52)
('frankfurt_000001_083199_leftImg8bit.png', 'frankfurt_000001_083199_gtFine_labelIds.png', 53)
('frankfurt_000001_083852_leftImg8bit.png', 'frankfurt_000001_083852_gtFine_labelIds.png', 53)
('lindau_000000_000019_leftImg8bit.png', 'lindau_000000_000019_gtFine_labelIds.png', 53)
('lindau_000001_000019_leftImg8bit.png', 'lindau_000001_000019_gtFine_labelIds.png', 53)
('lindau_000002_000019_leftImg8bit.png', 'lindau_000002_000019_gtFine_labelIds.png', 53)
('lindau_000003_000019_leftImg8bit.png', 'lindau_000003_000019_gtFine_labelIds.png', 54)
('lindau_000004_000019_leftImg8bit.png', 'lindau_000004_000019_gtFine_labelIds.png', 54)
('lindau_000005_000019_leftImg8bit.png', 'lindau_000005_000019_gtFine_labelIds.png', 54)
('lindau_000006_000019_leftImg8bit.png', 'lindau_000006_000019_gtFine_labelIds.png', 54)
('lindau_000007_000019_leftImg8bit.png', 'lindau_000007_000019_gtFine_labelIds.png', 54)
pixel_accuracy=0.968793597422, mean_iou=0.825240035313, iou=[ 0.94883289  0.95720774  0.75538191  0.56626895  0.89850869]
('lindau_000008_000019_leftImg8bit.png', 'lindau_000008_000019_gtFine_labelIds.png', 55)
('lindau_000009_000019_leftImg8bit.png', 'lindau_000009_000019_gtFine_labelIds.png', 55)
('lindau_000010_000019_leftImg8bit.png', 'lindau_000010_000019_gtFine_labelIds.png', 55)
('lindau_000011_000019_leftImg8bit.png', 'lindau_000011_000019_gtFine_labelIds.png', 55)
('lindau_000012_000019_leftImg8bit.png', 'lindau_000012_000019_gtFine_labelIds.png', 55)
('lindau_000013_000019_leftImg8bit.png', 'lindau_000013_000019_gtFine_labelIds.png', 56)
('lindau_000014_000019_leftImg8bit.png', 'lindau_000014_000019_gtFine_labelIds.png', 56)
('lindau_000015_000019_leftImg8bit.png', 'lindau_000015_000019_gtFine_labelIds.png', 56)
('lindau_000016_000019_leftImg8bit.png', 'lindau_000016_000019_gtFine_labelIds.png', 56)
('lindau_000017_000019_leftImg8bit.png', 'lindau_000017_000019_gtFine_labelIds.png', 56)
('lindau_000018_000019_leftImg8bit.png', 'lindau_000018_000019_gtFine_labelIds.png', 57)
('lindau_000019_000019_leftImg8bit.png', 'lindau_000019_000019_gtFine_labelIds.png', 57)
('lindau_000020_000019_leftImg8bit.png', 'lindau_000020_000019_gtFine_labelIds.png', 57)
('lindau_000021_000019_leftImg8bit.png', 'lindau_000021_000019_gtFine_labelIds.png', 57)
('lindau_000022_000019_leftImg8bit.png', 'lindau_000022_000019_gtFine_labelIds.png', 57)
('lindau_000023_000019_leftImg8bit.png', 'lindau_000023_000019_gtFine_labelIds.png', 58)
('lindau_000024_000019_leftImg8bit.png', 'lindau_000024_000019_gtFine_labelIds.png', 58)
('lindau_000025_000019_leftImg8bit.png', 'lindau_000025_000019_gtFine_labelIds.png', 58)
('lindau_000026_000019_leftImg8bit.png', 'lindau_000026_000019_gtFine_labelIds.png', 58)
('lindau_000027_000019_leftImg8bit.png', 'lindau_000027_000019_gtFine_labelIds.png', 58)
('lindau_000028_000019_leftImg8bit.png', 'lindau_000028_000019_gtFine_labelIds.png', 59)
('lindau_000029_000019_leftImg8bit.png', 'lindau_000029_000019_gtFine_labelIds.png', 59)
('lindau_000030_000019_leftImg8bit.png', 'lindau_000030_000019_gtFine_labelIds.png', 59)
('lindau_000031_000019_leftImg8bit.png', 'lindau_000031_000019_gtFine_labelIds.png', 59)
('lindau_000032_000019_leftImg8bit.png', 'lindau_000032_000019_gtFine_labelIds.png', 59)
pixel_accuracy=0.963866270304, mean_iou=0.820130163352, iou=[ 0.94134029  0.94211571  0.75131665  0.57218646  0.89369169]
('lindau_000033_000019_leftImg8bit.png', 'lindau_000033_000019_gtFine_labelIds.png', 60)
('lindau_000034_000019_leftImg8bit.png', 'lindau_000034_000019_gtFine_labelIds.png', 60)
('lindau_000035_000019_leftImg8bit.png', 'lindau_000035_000019_gtFine_labelIds.png', 60)
('lindau_000036_000019_leftImg8bit.png', 'lindau_000036_000019_gtFine_labelIds.png', 60)
('lindau_000037_000019_leftImg8bit.png', 'lindau_000037_000019_gtFine_labelIds.png', 60)
('lindau_000038_000019_leftImg8bit.png', 'lindau_000038_000019_gtFine_labelIds.png', 61)
('lindau_000039_000019_leftImg8bit.png', 'lindau_000039_000019_gtFine_labelIds.png', 61)
('lindau_000040_000019_leftImg8bit.png', 'lindau_000040_000019_gtFine_labelIds.png', 61)
('lindau_000041_000019_leftImg8bit.png', 'lindau_000041_000019_gtFine_labelIds.png', 61)
('lindau_000042_000019_leftImg8bit.png', 'lindau_000042_000019_gtFine_labelIds.png', 61)
('lindau_000043_000019_leftImg8bit.png', 'lindau_000043_000019_gtFine_labelIds.png', 62)
('lindau_000044_000019_leftImg8bit.png', 'lindau_000044_000019_gtFine_labelIds.png', 62)
('lindau_000045_000019_leftImg8bit.png', 'lindau_000045_000019_gtFine_labelIds.png', 62)
('lindau_000046_000019_leftImg8bit.png', 'lindau_000046_000019_gtFine_labelIds.png', 62)
('lindau_000047_000019_leftImg8bit.png', 'lindau_000047_000019_gtFine_labelIds.png', 62)
('lindau_000048_000019_leftImg8bit.png', 'lindau_000048_000019_gtFine_labelIds.png', 63)
('lindau_000049_000019_leftImg8bit.png', 'lindau_000049_000019_gtFine_labelIds.png', 63)
('lindau_000050_000019_leftImg8bit.png', 'lindau_000050_000019_gtFine_labelIds.png', 63)
('lindau_000051_000019_leftImg8bit.png', 'lindau_000051_000019_gtFine_labelIds.png', 63)
('lindau_000052_000019_leftImg8bit.png', 'lindau_000052_000019_gtFine_labelIds.png', 63)
('lindau_000053_000019_leftImg8bit.png', 'lindau_000053_000019_gtFine_labelIds.png', 64)
('lindau_000054_000019_leftImg8bit.png', 'lindau_000054_000019_gtFine_labelIds.png', 64)
('lindau_000055_000019_leftImg8bit.png', 'lindau_000055_000019_gtFine_labelIds.png', 64)
('lindau_000056_000019_leftImg8bit.png', 'lindau_000056_000019_gtFine_labelIds.png', 64)
('lindau_000057_000019_leftImg8bit.png', 'lindau_000057_000019_gtFine_labelIds.png', 64)
pixel_accuracy=0.954415690431, mean_iou=0.809565076708, iou=[ 0.92683737  0.91405707  0.74525772  0.5707186   0.89095462]
('lindau_000058_000019_leftImg8bit.png', 'lindau_000058_000019_gtFine_labelIds.png', 65)
('munster_000000_000019_leftImg8bit.png', 'munster_000000_000019_gtFine_labelIds.png', 65)
('munster_000001_000019_leftImg8bit.png', 'munster_000001_000019_gtFine_labelIds.png', 65)
('munster_000002_000019_leftImg8bit.png', 'munster_000002_000019_gtFine_labelIds.png', 65)
('munster_000003_000019_leftImg8bit.png', 'munster_000003_000019_gtFine_labelIds.png', 65)
('munster_000004_000019_leftImg8bit.png', 'munster_000004_000019_gtFine_labelIds.png', 66)
('munster_000005_000019_leftImg8bit.png', 'munster_000005_000019_gtFine_labelIds.png', 66)
('munster_000006_000019_leftImg8bit.png', 'munster_000006_000019_gtFine_labelIds.png', 66)
('munster_000007_000019_leftImg8bit.png', 'munster_000007_000019_gtFine_labelIds.png', 66)
('munster_000008_000019_leftImg8bit.png', 'munster_000008_000019_gtFine_labelIds.png', 66)
('munster_000009_000019_leftImg8bit.png', 'munster_000009_000019_gtFine_labelIds.png', 67)
('munster_000010_000019_leftImg8bit.png', 'munster_000010_000019_gtFine_labelIds.png', 67)
('munster_000011_000019_leftImg8bit.png', 'munster_000011_000019_gtFine_labelIds.png', 67)
('munster_000012_000019_leftImg8bit.png', 'munster_000012_000019_gtFine_labelIds.png', 67)
('munster_000013_000019_leftImg8bit.png', 'munster_000013_000019_gtFine_labelIds.png', 67)
('munster_000014_000019_leftImg8bit.png', 'munster_000014_000019_gtFine_labelIds.png', 68)
('munster_000015_000019_leftImg8bit.png', 'munster_000015_000019_gtFine_labelIds.png', 68)
('munster_000016_000019_leftImg8bit.png', 'munster_000016_000019_gtFine_labelIds.png', 68)
('munster_000017_000019_leftImg8bit.png', 'munster_000017_000019_gtFine_labelIds.png', 68)
('munster_000018_000019_leftImg8bit.png', 'munster_000018_000019_gtFine_labelIds.png', 68)
('munster_000019_000019_leftImg8bit.png', 'munster_000019_000019_gtFine_labelIds.png', 69)
('munster_000020_000019_leftImg8bit.png', 'munster_000020_000019_gtFine_labelIds.png', 69)
('munster_000021_000019_leftImg8bit.png', 'munster_000021_000019_gtFine_labelIds.png', 69)
('munster_000022_000019_leftImg8bit.png', 'munster_000022_000019_gtFine_labelIds.png', 69)
('munster_000023_000019_leftImg8bit.png', 'munster_000023_000019_gtFine_labelIds.png', 69)
pixel_accuracy=0.956137963887, mean_iou=0.812727454227, iou=[ 0.92947851  0.91802172  0.74503481  0.57749033  0.89361191]
('munster_000024_000019_leftImg8bit.png', 'munster_000024_000019_gtFine_labelIds.png', 70)
('munster_000025_000019_leftImg8bit.png', 'munster_000025_000019_gtFine_labelIds.png', 70)
('munster_000026_000019_leftImg8bit.png', 'munster_000026_000019_gtFine_labelIds.png', 70)
('munster_000027_000019_leftImg8bit.png', 'munster_000027_000019_gtFine_labelIds.png', 70)
('munster_000028_000019_leftImg8bit.png', 'munster_000028_000019_gtFine_labelIds.png', 70)
('munster_000029_000019_leftImg8bit.png', 'munster_000029_000019_gtFine_labelIds.png', 71)
('munster_000030_000019_leftImg8bit.png', 'munster_000030_000019_gtFine_labelIds.png', 71)
('munster_000031_000019_leftImg8bit.png', 'munster_000031_000019_gtFine_labelIds.png', 71)
('munster_000032_000019_leftImg8bit.png', 'munster_000032_000019_gtFine_labelIds.png', 71)
('munster_000033_000019_leftImg8bit.png', 'munster_000033_000019_gtFine_labelIds.png', 71)
('munster_000034_000019_leftImg8bit.png', 'munster_000034_000019_gtFine_labelIds.png', 72)
('munster_000035_000019_leftImg8bit.png', 'munster_000035_000019_gtFine_labelIds.png', 72)
('munster_000036_000019_leftImg8bit.png', 'munster_000036_000019_gtFine_labelIds.png', 72)
('munster_000037_000019_leftImg8bit.png', 'munster_000037_000019_gtFine_labelIds.png', 72)
('munster_000038_000019_leftImg8bit.png', 'munster_000038_000019_gtFine_labelIds.png', 72)
('munster_000039_000019_leftImg8bit.png', 'munster_000039_000019_gtFine_labelIds.png', 73)
('munster_000040_000019_leftImg8bit.png', 'munster_000040_000019_gtFine_labelIds.png', 73)
('munster_000041_000019_leftImg8bit.png', 'munster_000041_000019_gtFine_labelIds.png', 73)
('munster_000042_000019_leftImg8bit.png', 'munster_000042_000019_gtFine_labelIds.png', 73)
('munster_000043_000019_leftImg8bit.png', 'munster_000043_000019_gtFine_labelIds.png', 73)
('munster_000044_000019_leftImg8bit.png', 'munster_000044_000019_gtFine_labelIds.png', 74)
('munster_000045_000019_leftImg8bit.png', 'munster_000045_000019_gtFine_labelIds.png', 74)
('munster_000046_000019_leftImg8bit.png', 'munster_000046_000019_gtFine_labelIds.png', 74)
('munster_000047_000019_leftImg8bit.png', 'munster_000047_000019_gtFine_labelIds.png', 74)
('munster_000048_000019_leftImg8bit.png', 'munster_000048_000019_gtFine_labelIds.png', 74)
pixel_accuracy=0.957209651867, mean_iou=0.816757341514, iou=[ 0.9311797   0.92067978  0.75467606  0.58413265  0.89311852]
('munster_000049_000019_leftImg8bit.png', 'munster_000049_000019_gtFine_labelIds.png', 75)
('munster_000050_000019_leftImg8bit.png', 'munster_000050_000019_gtFine_labelIds.png', 75)
('munster_000051_000019_leftImg8bit.png', 'munster_000051_000019_gtFine_labelIds.png', 75)
('munster_000052_000019_leftImg8bit.png', 'munster_000052_000019_gtFine_labelIds.png', 75)
('munster_000053_000019_leftImg8bit.png', 'munster_000053_000019_gtFine_labelIds.png', 75)
('munster_000054_000019_leftImg8bit.png', 'munster_000054_000019_gtFine_labelIds.png', 76)
('munster_000055_000019_leftImg8bit.png', 'munster_000055_000019_gtFine_labelIds.png', 76)
('munster_000056_000019_leftImg8bit.png', 'munster_000056_000019_gtFine_labelIds.png', 76)
('munster_000057_000019_leftImg8bit.png', 'munster_000057_000019_gtFine_labelIds.png', 76)
('munster_000058_000019_leftImg8bit.png', 'munster_000058_000019_gtFine_labelIds.png', 76)
('munster_000059_000019_leftImg8bit.png', 'munster_000059_000019_gtFine_labelIds.png', 77)
('munster_000060_000019_leftImg8bit.png', 'munster_000060_000019_gtFine_labelIds.png', 77)
('munster_000061_000019_leftImg8bit.png', 'munster_000061_000019_gtFine_labelIds.png', 77)
('munster_000062_000019_leftImg8bit.png', 'munster_000062_000019_gtFine_labelIds.png', 77)
('munster_000063_000019_leftImg8bit.png', 'munster_000063_000019_gtFine_labelIds.png', 77)
('munster_000064_000019_leftImg8bit.png', 'munster_000064_000019_gtFine_labelIds.png', 78)
('munster_000065_000019_leftImg8bit.png', 'munster_000065_000019_gtFine_labelIds.png', 78)
('munster_000066_000019_leftImg8bit.png', 'munster_000066_000019_gtFine_labelIds.png', 78)
('munster_000067_000019_leftImg8bit.png', 'munster_000067_000019_gtFine_labelIds.png', 78)
('munster_000068_000019_leftImg8bit.png', 'munster_000068_000019_gtFine_labelIds.png', 78)
('munster_000069_000019_leftImg8bit.png', 'munster_000069_000019_gtFine_labelIds.png', 79)
('munster_000070_000019_leftImg8bit.png', 'munster_000070_000019_gtFine_labelIds.png', 79)
('munster_000071_000019_leftImg8bit.png', 'munster_000071_000019_gtFine_labelIds.png', 79)
('munster_000072_000019_leftImg8bit.png', 'munster_000072_000019_gtFine_labelIds.png', 79)
('munster_000073_000019_leftImg8bit.png', 'munster_000073_000019_gtFine_labelIds.png', 79)
pixel_accuracy=0.958473699503, mean_iou=0.819207482281, iou=[ 0.93311745  0.92409356  0.75553857  0.58789616  0.89539168]
('munster_000074_000019_leftImg8bit.png', 'munster_000074_000019_gtFine_labelIds.png', 80)
('munster_000075_000019_leftImg8bit.png', 'munster_000075_000019_gtFine_labelIds.png', 80)
('munster_000076_000019_leftImg8bit.png', 'munster_000076_000019_gtFine_labelIds.png', 80)
('munster_000077_000019_leftImg8bit.png', 'munster_000077_000019_gtFine_labelIds.png', 80)
('munster_000078_000019_leftImg8bit.png', 'munster_000078_000019_gtFine_labelIds.png', 80)
('munster_000079_000019_leftImg8bit.png', 'munster_000079_000019_gtFine_labelIds.png', 81)
('munster_000080_000019_leftImg8bit.png', 'munster_000080_000019_gtFine_labelIds.png', 81)
('munster_000081_000019_leftImg8bit.png', 'munster_000081_000019_gtFine_labelIds.png', 81)
('munster_000082_000019_leftImg8bit.png', 'munster_000082_000019_gtFine_labelIds.png', 81)
('munster_000083_000019_leftImg8bit.png', 'munster_000083_000019_gtFine_labelIds.png', 81)
('munster_000084_000019_leftImg8bit.png', 'munster_000084_000019_gtFine_labelIds.png', 82)
('munster_000085_000019_leftImg8bit.png', 'munster_000085_000019_gtFine_labelIds.png', 82)
('munster_000086_000019_leftImg8bit.png', 'munster_000086_000019_gtFine_labelIds.png', 82)
('munster_000087_000019_leftImg8bit.png', 'munster_000087_000019_gtFine_labelIds.png', 82)
('munster_000088_000019_leftImg8bit.png', 'munster_000088_000019_gtFine_labelIds.png', 82)
('munster_000089_000019_leftImg8bit.png', 'munster_000089_000019_gtFine_labelIds.png', 83)
('munster_000090_000019_leftImg8bit.png', 'munster_000090_000019_gtFine_labelIds.png', 83)
('munster_000091_000019_leftImg8bit.png', 'munster_000091_000019_gtFine_labelIds.png', 83)
('munster_000092_000019_leftImg8bit.png', 'munster_000092_000019_gtFine_labelIds.png', 83)
('munster_000093_000019_leftImg8bit.png', 'munster_000093_000019_gtFine_labelIds.png', 83)
('munster_000094_000019_leftImg8bit.png', 'munster_000094_000019_gtFine_labelIds.png', 84)
('munster_000095_000019_leftImg8bit.png', 'munster_000095_000019_gtFine_labelIds.png', 84)
('munster_000096_000019_leftImg8bit.png', 'munster_000096_000019_gtFine_labelIds.png', 84)
('munster_000097_000019_leftImg8bit.png', 'munster_000097_000019_gtFine_labelIds.png', 84)
('munster_000098_000019_leftImg8bit.png', 'munster_000098_000019_gtFine_labelIds.png', 84)
pixel_accuracy=0.959836095824, mean_iou=0.82190690496, iou=[ 0.93515283  0.92693323  0.75564277  0.5909301   0.90087559]
('munster_000099_000019_leftImg8bit.png', 'munster_000099_000019_gtFine_labelIds.png', 85)
('munster_000100_000019_leftImg8bit.png', 'munster_000100_000019_gtFine_labelIds.png', 85)
('munster_000101_000019_leftImg8bit.png', 'munster_000101_000019_gtFine_labelIds.png', 85)
('munster_000102_000019_leftImg8bit.png', 'munster_000102_000019_gtFine_labelIds.png', 85)
('munster_000103_000019_leftImg8bit.png', 'munster_000103_000019_gtFine_labelIds.png', 85)
('munster_000104_000019_leftImg8bit.png', 'munster_000104_000019_gtFine_labelIds.png', 86)
('munster_000105_000019_leftImg8bit.png', 'munster_000105_000019_gtFine_labelIds.png', 86)
('munster_000106_000019_leftImg8bit.png', 'munster_000106_000019_gtFine_labelIds.png', 86)
('munster_000107_000019_leftImg8bit.png', 'munster_000107_000019_gtFine_labelIds.png', 86)
('munster_000108_000019_leftImg8bit.png', 'munster_000108_000019_gtFine_labelIds.png', 86)
('munster_000109_000019_leftImg8bit.png', 'munster_000109_000019_gtFine_labelIds.png', 87)
('munster_000110_000019_leftImg8bit.png', 'munster_000110_000019_gtFine_labelIds.png', 87)
('munster_000111_000019_leftImg8bit.png', 'munster_000111_000019_gtFine_labelIds.png', 87)
('munster_000112_000019_leftImg8bit.png', 'munster_000112_000019_gtFine_labelIds.png', 87)
('munster_000113_000019_leftImg8bit.png', 'munster_000113_000019_gtFine_labelIds.png', 87)
('munster_000114_000019_leftImg8bit.png', 'munster_000114_000019_gtFine_labelIds.png', 88)
('munster_000115_000019_leftImg8bit.png', 'munster_000115_000019_gtFine_labelIds.png', 88)
('munster_000116_000019_leftImg8bit.png', 'munster_000116_000019_gtFine_labelIds.png', 88)
('munster_000117_000019_leftImg8bit.png', 'munster_000117_000019_gtFine_labelIds.png', 88)
('munster_000118_000019_leftImg8bit.png', 'munster_000118_000019_gtFine_labelIds.png', 88)
('munster_000119_000019_leftImg8bit.png', 'munster_000119_000019_gtFine_labelIds.png', 89)
('munster_000120_000019_leftImg8bit.png', 'munster_000120_000019_gtFine_labelIds.png', 89)
('munster_000121_000019_leftImg8bit.png', 'munster_000121_000019_gtFine_labelIds.png', 89)
('munster_000122_000019_leftImg8bit.png', 'munster_000122_000019_gtFine_labelIds.png', 89)
('munster_000123_000019_leftImg8bit.png', 'munster_000123_000019_gtFine_labelIds.png', 89)
pixel_accuracy=0.961220105418, mean_iou=0.822883727668, iou=[ 0.93737358  0.92993385  0.75492561  0.58954644  0.90263914]
('munster_000124_000019_leftImg8bit.png', 'munster_000124_000019_gtFine_labelIds.png', 90)
('munster_000125_000019_leftImg8bit.png', 'munster_000125_000019_gtFine_labelIds.png', 90)
('munster_000126_000019_leftImg8bit.png', 'munster_000126_000019_gtFine_labelIds.png', 90)
('munster_000127_000019_leftImg8bit.png', 'munster_000127_000019_gtFine_labelIds.png', 90)
('munster_000128_000019_leftImg8bit.png', 'munster_000128_000019_gtFine_labelIds.png', 90)
('munster_000129_000019_leftImg8bit.png', 'munster_000129_000019_gtFine_labelIds.png', 91)
('munster_000130_000019_leftImg8bit.png', 'munster_000130_000019_gtFine_labelIds.png', 91)
('munster_000131_000019_leftImg8bit.png', 'munster_000131_000019_gtFine_labelIds.png', 91)
('munster_000132_000019_leftImg8bit.png', 'munster_000132_000019_gtFine_labelIds.png', 91)
('munster_000133_000019_leftImg8bit.png', 'munster_000133_000019_gtFine_labelIds.png', 91)
('munster_000134_000019_leftImg8bit.png', 'munster_000134_000019_gtFine_labelIds.png', 92)
('munster_000135_000019_leftImg8bit.png', 'munster_000135_000019_gtFine_labelIds.png', 92)
('munster_000136_000019_leftImg8bit.png', 'munster_000136_000019_gtFine_labelIds.png', 92)
('munster_000137_000019_leftImg8bit.png', 'munster_000137_000019_gtFine_labelIds.png', 92)
('munster_000138_000019_leftImg8bit.png', 'munster_000138_000019_gtFine_labelIds.png', 92)
('munster_000139_000019_leftImg8bit.png', 'munster_000139_000019_gtFine_labelIds.png', 93)
('munster_000140_000019_leftImg8bit.png', 'munster_000140_000019_gtFine_labelIds.png', 93)
('munster_000141_000019_leftImg8bit.png', 'munster_000141_000019_gtFine_labelIds.png', 93)
('munster_000142_000019_leftImg8bit.png', 'munster_000142_000019_gtFine_labelIds.png', 93)
('munster_000143_000019_leftImg8bit.png', 'munster_000143_000019_gtFine_labelIds.png', 93)
('munster_000144_000019_leftImg8bit.png', 'munster_000144_000019_gtFine_labelIds.png', 94)
('munster_000145_000019_leftImg8bit.png', 'munster_000145_000019_gtFine_labelIds.png', 94)
('munster_000146_000019_leftImg8bit.png', 'munster_000146_000019_gtFine_labelIds.png', 94)
('munster_000147_000019_leftImg8bit.png', 'munster_000147_000019_gtFine_labelIds.png', 94)
('munster_000148_000019_leftImg8bit.png', 'munster_000148_000019_gtFine_labelIds.png', 94)
pixel_accuracy=0.958389933956, mean_iou=0.821643235185, iou=[ 0.93278108  0.92178434  0.76037665  0.58993933  0.90333477]
('munster_000149_000019_leftImg8bit.png', 'munster_000149_000019_gtFine_labelIds.png', 95)
('munster_000150_000019_leftImg8bit.png', 'munster_000150_000019_gtFine_labelIds.png', 95)
('munster_000151_000019_leftImg8bit.png', 'munster_000151_000019_gtFine_labelIds.png', 95)
('munster_000152_000019_leftImg8bit.png', 'munster_000152_000019_gtFine_labelIds.png', 95)
('munster_000153_000019_leftImg8bit.png', 'munster_000153_000019_gtFine_labelIds.png', 95)
('munster_000154_000019_leftImg8bit.png', 'munster_000154_000019_gtFine_labelIds.png', 96)
('munster_000155_000019_leftImg8bit.png', 'munster_000155_000019_gtFine_labelIds.png', 96)
('munster_000156_000019_leftImg8bit.png', 'munster_000156_000019_gtFine_labelIds.png', 96)
('munster_000157_000019_leftImg8bit.png', 'munster_000157_000019_gtFine_labelIds.png', 96)
('munster_000158_000019_leftImg8bit.png', 'munster_000158_000019_gtFine_labelIds.png', 96)
('munster_000159_000019_leftImg8bit.png', 'munster_000159_000019_gtFine_labelIds.png', 97)
('munster_000160_000019_leftImg8bit.png', 'munster_000160_000019_gtFine_labelIds.png', 97)
('munster_000161_000019_leftImg8bit.png', 'munster_000161_000019_gtFine_labelIds.png', 97)
('munster_000162_000019_leftImg8bit.png', 'munster_000162_000019_gtFine_labelIds.png', 97)
('munster_000163_000019_leftImg8bit.png', 'munster_000163_000019_gtFine_labelIds.png', 97)
('munster_000164_000019_leftImg8bit.png', 'munster_000164_000019_gtFine_labelIds.png', 98)
('munster_000165_000019_leftImg8bit.png', 'munster_000165_000019_gtFine_labelIds.png', 98)
('munster_000166_000019_leftImg8bit.png', 'munster_000166_000019_gtFine_labelIds.png', 98)
('munster_000167_000019_leftImg8bit.png', 'munster_000167_000019_gtFine_labelIds.png', 98)
('munster_000168_000019_leftImg8bit.png', 'munster_000168_000019_gtFine_labelIds.png', 98)
('munster_000169_000019_leftImg8bit.png', 'munster_000169_000019_gtFine_labelIds.png', 99)
('munster_000170_000019_leftImg8bit.png', 'munster_000170_000019_gtFine_labelIds.png', 99)
('munster_000171_000019_leftImg8bit.png', 'munster_000171_000019_gtFine_labelIds.png', 99)
('munster_000172_000019_leftImg8bit.png', 'munster_000172_000019_gtFine_labelIds.png', 99)
('munster_000173_000019_leftImg8bit.png', 'munster_000173_000019_gtFine_labelIds.png', 99)
pixel_accuracy=0.959111058951, mean_iou=0.823618974954, iou=[ 0.93397978  0.9233838   0.76062293  0.59601216  0.90409621]
-------------------------------------------------------------
Final: pixel_accuracy=0.959111058951, mean_iou=0.823618974954, iou=[ 0.93397978  0.9233838   0.76062293  0.59601216  0.90409621]
-------------------------------------------------------------
sparse eval.
