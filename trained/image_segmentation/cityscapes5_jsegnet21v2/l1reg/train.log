I0711 20:08:11.738207 25943 caffe.cpp:209] Using GPUs 0, 1, 2
I0711 20:08:11.740319 25943 caffe.cpp:214] GPU 0: GeForce GTX 1080
I0711 20:08:11.740659 25943 caffe.cpp:214] GPU 1: GeForce GTX 1080
I0711 20:08:11.741004 25943 caffe.cpp:214] GPU 2: GeForce GTX 1080
I0711 20:08:12.614281 25943 solver.cpp:48] Initializing solver from parameters: 
train_net: "training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/l1reg/train.prototxt"
test_net: "training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/l1reg/test.prototxt"
test_iter: 125
test_interval: 2000
base_lr: 1e-05
display: 100
max_iter: 32000
lr_policy: "multistep"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 1e-05
snapshot: 10000
snapshot_prefix: "training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/l1reg/cityscapes5_jsegnet21v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
regularization_type: "L1"
test_initialization: false
stepvalue: 24000
iter_size: 1
type: "Adam"
I0711 20:08:12.632733 25943 solver.cpp:82] Creating training net from train_net file: training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/l1reg/train.prototxt
I0711 20:08:12.635576 25943 net.cpp:327] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0711 20:08:12.635584 25943 net.cpp:327] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
I0711 20:08:12.635821 25943 net.cpp:56] Initializing net from parameters: 
name: "jsegnet21v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/train-image-lmdb"
    label_list_path: "data/train-label-lmdb"
    batch_size: 5
    shuffle: false
    threads: 4
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b/bn"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a/bn"
  top: "out5a/bn"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a/bn"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b/bn"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a/bn"
  top: "out3a/bn"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a/bn"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv1/bn"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv2/bn"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv3/bn"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4/bn"
  top: "ctx_conv4/bn"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4/bn"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
I0711 20:08:12.636162 25943 layer_factory.hpp:77] Creating layer data
I0711 20:08:12.636183 25943 net.cpp:98] Creating Layer data
I0711 20:08:12.636188 25943 net.cpp:413] data -> data
I0711 20:08:12.636207 25943 net.cpp:413] data -> label
I0711 20:08:12.663363 26012 db_lmdb.cpp:35] Opened lmdb data/train-image-lmdb
I0711 20:08:12.663627 26017 db_lmdb.cpp:35] Opened lmdb data/train-label-lmdb
I0711 20:08:12.668056 25943 data_layer.cpp:78] ReshapePrefetch 5, 3, 640, 640
I0711 20:08:12.668126 25943 data_layer.cpp:83] output data size: 5,3,640,640
I0711 20:08:12.696697 25943 data_layer.cpp:78] ReshapePrefetch 5, 1, 640, 640
I0711 20:08:12.696758 25943 data_layer.cpp:83] output data size: 5,1,640,640
I0711 20:08:12.705622 26022 blocking_queue.cpp:50] Waiting for data
I0711 20:08:12.709240 25943 net.cpp:148] Setting up data
I0711 20:08:12.709262 25943 net.cpp:155] Top shape: 5 3 640 640 (6144000)
I0711 20:08:12.709265 25943 net.cpp:155] Top shape: 5 1 640 640 (2048000)
I0711 20:08:12.709267 25943 net.cpp:163] Memory required for data: 32768000
I0711 20:08:12.709275 25943 layer_factory.hpp:77] Creating layer data/bias
I0711 20:08:12.709282 25943 net.cpp:98] Creating Layer data/bias
I0711 20:08:12.709285 25943 net.cpp:439] data/bias <- data
I0711 20:08:12.709295 25943 net.cpp:413] data/bias -> data/bias
I0711 20:08:12.710577 25943 net.cpp:148] Setting up data/bias
I0711 20:08:12.710587 25943 net.cpp:155] Top shape: 5 3 640 640 (6144000)
I0711 20:08:12.710590 25943 net.cpp:163] Memory required for data: 57344000
I0711 20:08:12.710599 25943 layer_factory.hpp:77] Creating layer conv1a
I0711 20:08:12.710609 25943 net.cpp:98] Creating Layer conv1a
I0711 20:08:12.710611 25943 net.cpp:439] conv1a <- data/bias
I0711 20:08:12.710615 25943 net.cpp:413] conv1a -> conv1a
I0711 20:08:12.712355 25943 net.cpp:148] Setting up conv1a
I0711 20:08:12.712370 25943 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0711 20:08:12.712373 25943 net.cpp:163] Memory required for data: 122880000
I0711 20:08:12.712379 25943 layer_factory.hpp:77] Creating layer conv1a/bn
I0711 20:08:12.712390 25943 net.cpp:98] Creating Layer conv1a/bn
I0711 20:08:12.712393 25943 net.cpp:439] conv1a/bn <- conv1a
I0711 20:08:12.712399 25943 net.cpp:413] conv1a/bn -> conv1a/bn
I0711 20:08:12.714072 25943 net.cpp:148] Setting up conv1a/bn
I0711 20:08:12.714082 25943 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0711 20:08:12.714085 25943 net.cpp:163] Memory required for data: 188416000
I0711 20:08:12.714092 25943 layer_factory.hpp:77] Creating layer conv1a/relu
I0711 20:08:12.714097 25943 net.cpp:98] Creating Layer conv1a/relu
I0711 20:08:12.714098 25943 net.cpp:439] conv1a/relu <- conv1a/bn
I0711 20:08:12.714102 25943 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0711 20:08:12.714109 25943 net.cpp:148] Setting up conv1a/relu
I0711 20:08:12.714112 25943 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0711 20:08:12.714113 25943 net.cpp:163] Memory required for data: 253952000
I0711 20:08:12.714115 25943 layer_factory.hpp:77] Creating layer conv1b
I0711 20:08:12.714120 25943 net.cpp:98] Creating Layer conv1b
I0711 20:08:12.714123 25943 net.cpp:439] conv1b <- conv1a/bn
I0711 20:08:12.714125 25943 net.cpp:413] conv1b -> conv1b
I0711 20:08:12.714498 25943 net.cpp:148] Setting up conv1b
I0711 20:08:12.714504 25943 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0711 20:08:12.714505 25943 net.cpp:163] Memory required for data: 319488000
I0711 20:08:12.714509 25943 layer_factory.hpp:77] Creating layer conv1b/bn
I0711 20:08:12.714514 25943 net.cpp:98] Creating Layer conv1b/bn
I0711 20:08:12.714515 25943 net.cpp:439] conv1b/bn <- conv1b
I0711 20:08:12.714519 25943 net.cpp:413] conv1b/bn -> conv1b/bn
I0711 20:08:12.715205 25943 net.cpp:148] Setting up conv1b/bn
I0711 20:08:12.715211 25943 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0711 20:08:12.715214 25943 net.cpp:163] Memory required for data: 385024000
I0711 20:08:12.715219 25943 layer_factory.hpp:77] Creating layer conv1b/relu
I0711 20:08:12.715221 25943 net.cpp:98] Creating Layer conv1b/relu
I0711 20:08:12.715224 25943 net.cpp:439] conv1b/relu <- conv1b/bn
I0711 20:08:12.715225 25943 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0711 20:08:12.715229 25943 net.cpp:148] Setting up conv1b/relu
I0711 20:08:12.715231 25943 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0711 20:08:12.715232 25943 net.cpp:163] Memory required for data: 450560000
I0711 20:08:12.715234 25943 layer_factory.hpp:77] Creating layer pool1
I0711 20:08:12.715240 25943 net.cpp:98] Creating Layer pool1
I0711 20:08:12.715241 25943 net.cpp:439] pool1 <- conv1b/bn
I0711 20:08:12.715243 25943 net.cpp:413] pool1 -> pool1
I0711 20:08:12.729033 25943 net.cpp:148] Setting up pool1
I0711 20:08:12.729044 25943 net.cpp:155] Top shape: 5 32 160 160 (4096000)
I0711 20:08:12.729046 25943 net.cpp:163] Memory required for data: 466944000
I0711 20:08:12.729049 25943 layer_factory.hpp:77] Creating layer res2a_branch2a
I0711 20:08:12.729054 25943 net.cpp:98] Creating Layer res2a_branch2a
I0711 20:08:12.729058 25943 net.cpp:439] res2a_branch2a <- pool1
I0711 20:08:12.729060 25943 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0711 20:08:12.730670 25943 net.cpp:148] Setting up res2a_branch2a
I0711 20:08:12.730679 25943 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0711 20:08:12.730681 25943 net.cpp:163] Memory required for data: 499712000
I0711 20:08:12.730686 25943 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0711 20:08:12.730691 25943 net.cpp:98] Creating Layer res2a_branch2a/bn
I0711 20:08:12.730695 25943 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0711 20:08:12.730696 25943 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0711 20:08:12.731365 25943 net.cpp:148] Setting up res2a_branch2a/bn
I0711 20:08:12.731371 25943 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0711 20:08:12.731374 25943 net.cpp:163] Memory required for data: 532480000
I0711 20:08:12.731379 25943 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0711 20:08:12.731381 25943 net.cpp:98] Creating Layer res2a_branch2a/relu
I0711 20:08:12.731384 25943 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0711 20:08:12.731385 25943 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0711 20:08:12.731389 25943 net.cpp:148] Setting up res2a_branch2a/relu
I0711 20:08:12.731392 25943 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0711 20:08:12.731393 25943 net.cpp:163] Memory required for data: 565248000
I0711 20:08:12.731395 25943 layer_factory.hpp:77] Creating layer res2a_branch2b
I0711 20:08:12.731400 25943 net.cpp:98] Creating Layer res2a_branch2b
I0711 20:08:12.731401 25943 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0711 20:08:12.731403 25943 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0711 20:08:12.732748 25943 net.cpp:148] Setting up res2a_branch2b
I0711 20:08:12.732756 25943 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0711 20:08:12.732758 25943 net.cpp:163] Memory required for data: 598016000
I0711 20:08:12.732762 25943 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0711 20:08:12.732766 25943 net.cpp:98] Creating Layer res2a_branch2b/bn
I0711 20:08:12.732769 25943 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0711 20:08:12.732771 25943 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0711 20:08:12.733477 25943 net.cpp:148] Setting up res2a_branch2b/bn
I0711 20:08:12.733484 25943 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0711 20:08:12.733485 25943 net.cpp:163] Memory required for data: 630784000
I0711 20:08:12.733490 25943 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0711 20:08:12.733494 25943 net.cpp:98] Creating Layer res2a_branch2b/relu
I0711 20:08:12.733495 25943 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0711 20:08:12.733499 25943 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0711 20:08:12.733503 25943 net.cpp:148] Setting up res2a_branch2b/relu
I0711 20:08:12.733506 25943 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0711 20:08:12.733507 25943 net.cpp:163] Memory required for data: 663552000
I0711 20:08:12.733510 25943 layer_factory.hpp:77] Creating layer pool2
I0711 20:08:12.733513 25943 net.cpp:98] Creating Layer pool2
I0711 20:08:12.733515 25943 net.cpp:439] pool2 <- res2a_branch2b/bn
I0711 20:08:12.733517 25943 net.cpp:413] pool2 -> pool2
I0711 20:08:12.733553 25943 net.cpp:148] Setting up pool2
I0711 20:08:12.733557 25943 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 20:08:12.733559 25943 net.cpp:163] Memory required for data: 671744000
I0711 20:08:12.733561 25943 layer_factory.hpp:77] Creating layer res3a_branch2a
I0711 20:08:12.733566 25943 net.cpp:98] Creating Layer res3a_branch2a
I0711 20:08:12.733568 25943 net.cpp:439] res3a_branch2a <- pool2
I0711 20:08:12.733570 25943 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0711 20:08:12.735291 25943 net.cpp:148] Setting up res3a_branch2a
I0711 20:08:12.735296 25943 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0711 20:08:12.735298 25943 net.cpp:163] Memory required for data: 688128000
I0711 20:08:12.735302 25943 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0711 20:08:12.735306 25943 net.cpp:98] Creating Layer res3a_branch2a/bn
I0711 20:08:12.735307 25943 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0711 20:08:12.735309 25943 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0711 20:08:12.735908 25943 net.cpp:148] Setting up res3a_branch2a/bn
I0711 20:08:12.735914 25943 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0711 20:08:12.735916 25943 net.cpp:163] Memory required for data: 704512000
I0711 20:08:12.735921 25943 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0711 20:08:12.735924 25943 net.cpp:98] Creating Layer res3a_branch2a/relu
I0711 20:08:12.735926 25943 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0711 20:08:12.735929 25943 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0711 20:08:12.735931 25943 net.cpp:148] Setting up res3a_branch2a/relu
I0711 20:08:12.735934 25943 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0711 20:08:12.735935 25943 net.cpp:163] Memory required for data: 720896000
I0711 20:08:12.735937 25943 layer_factory.hpp:77] Creating layer res3a_branch2b
I0711 20:08:12.735941 25943 net.cpp:98] Creating Layer res3a_branch2b
I0711 20:08:12.735944 25943 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0711 20:08:12.735946 25943 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0711 20:08:12.736943 25943 net.cpp:148] Setting up res3a_branch2b
I0711 20:08:12.736948 25943 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0711 20:08:12.736949 25943 net.cpp:163] Memory required for data: 737280000
I0711 20:08:12.736953 25943 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0711 20:08:12.736956 25943 net.cpp:98] Creating Layer res3a_branch2b/bn
I0711 20:08:12.736958 25943 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0711 20:08:12.736961 25943 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0711 20:08:12.737566 25943 net.cpp:148] Setting up res3a_branch2b/bn
I0711 20:08:12.737572 25943 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0711 20:08:12.737574 25943 net.cpp:163] Memory required for data: 753664000
I0711 20:08:12.737578 25943 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0711 20:08:12.737581 25943 net.cpp:98] Creating Layer res3a_branch2b/relu
I0711 20:08:12.737583 25943 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0711 20:08:12.737592 25943 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0711 20:08:12.737596 25943 net.cpp:148] Setting up res3a_branch2b/relu
I0711 20:08:12.737598 25943 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0711 20:08:12.737599 25943 net.cpp:163] Memory required for data: 770048000
I0711 20:08:12.737601 25943 layer_factory.hpp:77] Creating layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0711 20:08:12.737604 25943 net.cpp:98] Creating Layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0711 20:08:12.737607 25943 net.cpp:439] res3a_branch2b/bn_res3a_branch2b/relu_0_split <- res3a_branch2b/bn
I0711 20:08:12.737608 25943 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0711 20:08:12.737612 25943 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0711 20:08:12.737650 25943 net.cpp:148] Setting up res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0711 20:08:12.737658 25943 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0711 20:08:12.737659 25943 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0711 20:08:12.737661 25943 net.cpp:163] Memory required for data: 802816000
I0711 20:08:12.737663 25943 layer_factory.hpp:77] Creating layer pool3
I0711 20:08:12.737666 25943 net.cpp:98] Creating Layer pool3
I0711 20:08:12.737668 25943 net.cpp:439] pool3 <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0711 20:08:12.737670 25943 net.cpp:413] pool3 -> pool3
I0711 20:08:12.737709 25943 net.cpp:148] Setting up pool3
I0711 20:08:12.737713 25943 net.cpp:155] Top shape: 5 128 40 40 (1024000)
I0711 20:08:12.737715 25943 net.cpp:163] Memory required for data: 806912000
I0711 20:08:12.737717 25943 layer_factory.hpp:77] Creating layer res4a_branch2a
I0711 20:08:12.737720 25943 net.cpp:98] Creating Layer res4a_branch2a
I0711 20:08:12.737723 25943 net.cpp:439] res4a_branch2a <- pool3
I0711 20:08:12.737725 25943 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0711 20:08:12.744794 25943 net.cpp:148] Setting up res4a_branch2a
I0711 20:08:12.744804 25943 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0711 20:08:12.744807 25943 net.cpp:163] Memory required for data: 815104000
I0711 20:08:12.744810 25943 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0711 20:08:12.744818 25943 net.cpp:98] Creating Layer res4a_branch2a/bn
I0711 20:08:12.744822 25943 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0711 20:08:12.744824 25943 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0711 20:08:12.745501 25943 net.cpp:148] Setting up res4a_branch2a/bn
I0711 20:08:12.745509 25943 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0711 20:08:12.745512 25943 net.cpp:163] Memory required for data: 823296000
I0711 20:08:12.745517 25943 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0711 20:08:12.745519 25943 net.cpp:98] Creating Layer res4a_branch2a/relu
I0711 20:08:12.745522 25943 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0711 20:08:12.745523 25943 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0711 20:08:12.745527 25943 net.cpp:148] Setting up res4a_branch2a/relu
I0711 20:08:12.745529 25943 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0711 20:08:12.745532 25943 net.cpp:163] Memory required for data: 831488000
I0711 20:08:12.745533 25943 layer_factory.hpp:77] Creating layer res4a_branch2b
I0711 20:08:12.745537 25943 net.cpp:98] Creating Layer res4a_branch2b
I0711 20:08:12.745540 25943 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0711 20:08:12.745543 25943 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0711 20:08:12.748811 25943 net.cpp:148] Setting up res4a_branch2b
I0711 20:08:12.748821 25943 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0711 20:08:12.748822 25943 net.cpp:163] Memory required for data: 839680000
I0711 20:08:12.748826 25943 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0711 20:08:12.748829 25943 net.cpp:98] Creating Layer res4a_branch2b/bn
I0711 20:08:12.748831 25943 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0711 20:08:12.748842 25943 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0711 20:08:12.749518 25943 net.cpp:148] Setting up res4a_branch2b/bn
I0711 20:08:12.749526 25943 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0711 20:08:12.749527 25943 net.cpp:163] Memory required for data: 847872000
I0711 20:08:12.749532 25943 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0711 20:08:12.749536 25943 net.cpp:98] Creating Layer res4a_branch2b/relu
I0711 20:08:12.749537 25943 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0711 20:08:12.749539 25943 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0711 20:08:12.749543 25943 net.cpp:148] Setting up res4a_branch2b/relu
I0711 20:08:12.749546 25943 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0711 20:08:12.749547 25943 net.cpp:163] Memory required for data: 856064000
I0711 20:08:12.749549 25943 layer_factory.hpp:77] Creating layer pool4
I0711 20:08:12.749552 25943 net.cpp:98] Creating Layer pool4
I0711 20:08:12.749554 25943 net.cpp:439] pool4 <- res4a_branch2b/bn
I0711 20:08:12.749557 25943 net.cpp:413] pool4 -> pool4
I0711 20:08:12.749593 25943 net.cpp:148] Setting up pool4
I0711 20:08:12.749598 25943 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0711 20:08:12.749599 25943 net.cpp:163] Memory required for data: 864256000
I0711 20:08:12.749601 25943 layer_factory.hpp:77] Creating layer res5a_branch2a
I0711 20:08:12.749605 25943 net.cpp:98] Creating Layer res5a_branch2a
I0711 20:08:12.749608 25943 net.cpp:439] res5a_branch2a <- pool4
I0711 20:08:12.749613 25943 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0711 20:08:12.780974 25943 net.cpp:148] Setting up res5a_branch2a
I0711 20:08:12.780992 25943 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0711 20:08:12.780994 25943 net.cpp:163] Memory required for data: 880640000
I0711 20:08:12.781000 25943 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0711 20:08:12.781010 25943 net.cpp:98] Creating Layer res5a_branch2a/bn
I0711 20:08:12.781013 25943 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0711 20:08:12.781018 25943 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0711 20:08:12.781738 25943 net.cpp:148] Setting up res5a_branch2a/bn
I0711 20:08:12.781745 25943 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0711 20:08:12.781747 25943 net.cpp:163] Memory required for data: 897024000
I0711 20:08:12.781752 25943 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0711 20:08:12.781756 25943 net.cpp:98] Creating Layer res5a_branch2a/relu
I0711 20:08:12.781759 25943 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0711 20:08:12.781761 25943 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0711 20:08:12.781765 25943 net.cpp:148] Setting up res5a_branch2a/relu
I0711 20:08:12.781767 25943 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0711 20:08:12.781769 25943 net.cpp:163] Memory required for data: 913408000
I0711 20:08:12.781771 25943 layer_factory.hpp:77] Creating layer res5a_branch2b
I0711 20:08:12.781776 25943 net.cpp:98] Creating Layer res5a_branch2b
I0711 20:08:12.781777 25943 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0711 20:08:12.781780 25943 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0711 20:08:12.799232 25943 net.cpp:148] Setting up res5a_branch2b
I0711 20:08:12.799249 25943 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0711 20:08:12.799252 25943 net.cpp:163] Memory required for data: 929792000
I0711 20:08:12.799260 25943 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0711 20:08:12.799268 25943 net.cpp:98] Creating Layer res5a_branch2b/bn
I0711 20:08:12.799270 25943 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0711 20:08:12.799273 25943 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0711 20:08:12.799937 25943 net.cpp:148] Setting up res5a_branch2b/bn
I0711 20:08:12.799942 25943 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0711 20:08:12.799944 25943 net.cpp:163] Memory required for data: 946176000
I0711 20:08:12.799949 25943 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0711 20:08:12.799962 25943 net.cpp:98] Creating Layer res5a_branch2b/relu
I0711 20:08:12.799965 25943 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0711 20:08:12.799968 25943 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0711 20:08:12.799973 25943 net.cpp:148] Setting up res5a_branch2b/relu
I0711 20:08:12.799974 25943 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0711 20:08:12.799976 25943 net.cpp:163] Memory required for data: 962560000
I0711 20:08:12.799978 25943 layer_factory.hpp:77] Creating layer out5a
I0711 20:08:12.799983 25943 net.cpp:98] Creating Layer out5a
I0711 20:08:12.799985 25943 net.cpp:439] out5a <- res5a_branch2b/bn
I0711 20:08:12.799991 25943 net.cpp:413] out5a -> out5a
I0711 20:08:12.804133 25943 net.cpp:148] Setting up out5a
I0711 20:08:12.804142 25943 net.cpp:155] Top shape: 5 64 40 40 (512000)
I0711 20:08:12.804145 25943 net.cpp:163] Memory required for data: 964608000
I0711 20:08:12.804148 25943 layer_factory.hpp:77] Creating layer out5a/bn
I0711 20:08:12.804152 25943 net.cpp:98] Creating Layer out5a/bn
I0711 20:08:12.804154 25943 net.cpp:439] out5a/bn <- out5a
I0711 20:08:12.804157 25943 net.cpp:413] out5a/bn -> out5a/bn
I0711 20:08:12.804870 25943 net.cpp:148] Setting up out5a/bn
I0711 20:08:12.804877 25943 net.cpp:155] Top shape: 5 64 40 40 (512000)
I0711 20:08:12.804879 25943 net.cpp:163] Memory required for data: 966656000
I0711 20:08:12.804884 25943 layer_factory.hpp:77] Creating layer out5a/relu
I0711 20:08:12.804888 25943 net.cpp:98] Creating Layer out5a/relu
I0711 20:08:12.804889 25943 net.cpp:439] out5a/relu <- out5a/bn
I0711 20:08:12.804891 25943 net.cpp:400] out5a/relu -> out5a/bn (in-place)
I0711 20:08:12.804895 25943 net.cpp:148] Setting up out5a/relu
I0711 20:08:12.804898 25943 net.cpp:155] Top shape: 5 64 40 40 (512000)
I0711 20:08:12.804899 25943 net.cpp:163] Memory required for data: 968704000
I0711 20:08:12.804901 25943 layer_factory.hpp:77] Creating layer out5a_up2
I0711 20:08:12.804909 25943 net.cpp:98] Creating Layer out5a_up2
I0711 20:08:12.804913 25943 net.cpp:439] out5a_up2 <- out5a/bn
I0711 20:08:12.804915 25943 net.cpp:413] out5a_up2 -> out5a_up2
I0711 20:08:12.805163 25943 net.cpp:148] Setting up out5a_up2
I0711 20:08:12.805168 25943 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 20:08:12.805169 25943 net.cpp:163] Memory required for data: 976896000
I0711 20:08:12.805172 25943 layer_factory.hpp:77] Creating layer out3a
I0711 20:08:12.805176 25943 net.cpp:98] Creating Layer out3a
I0711 20:08:12.805178 25943 net.cpp:439] out3a <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0711 20:08:12.805181 25943 net.cpp:413] out3a -> out3a
I0711 20:08:12.806435 25943 net.cpp:148] Setting up out3a
I0711 20:08:12.806447 25943 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 20:08:12.806450 25943 net.cpp:163] Memory required for data: 985088000
I0711 20:08:12.806455 25943 layer_factory.hpp:77] Creating layer out3a/bn
I0711 20:08:12.806464 25943 net.cpp:98] Creating Layer out3a/bn
I0711 20:08:12.806468 25943 net.cpp:439] out3a/bn <- out3a
I0711 20:08:12.806473 25943 net.cpp:413] out3a/bn -> out3a/bn
I0711 20:08:12.807474 25943 net.cpp:148] Setting up out3a/bn
I0711 20:08:12.807482 25943 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 20:08:12.807485 25943 net.cpp:163] Memory required for data: 993280000
I0711 20:08:12.807492 25943 layer_factory.hpp:77] Creating layer out3a/relu
I0711 20:08:12.807497 25943 net.cpp:98] Creating Layer out3a/relu
I0711 20:08:12.807500 25943 net.cpp:439] out3a/relu <- out3a/bn
I0711 20:08:12.807503 25943 net.cpp:400] out3a/relu -> out3a/bn (in-place)
I0711 20:08:12.807509 25943 net.cpp:148] Setting up out3a/relu
I0711 20:08:12.807512 25943 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 20:08:12.807516 25943 net.cpp:163] Memory required for data: 1001472000
I0711 20:08:12.807520 25943 layer_factory.hpp:77] Creating layer out3_out5_combined
I0711 20:08:12.807525 25943 net.cpp:98] Creating Layer out3_out5_combined
I0711 20:08:12.807529 25943 net.cpp:439] out3_out5_combined <- out5a_up2
I0711 20:08:12.807533 25943 net.cpp:439] out3_out5_combined <- out3a/bn
I0711 20:08:12.807549 25943 net.cpp:413] out3_out5_combined -> out3_out5_combined
I0711 20:08:12.807585 25943 net.cpp:148] Setting up out3_out5_combined
I0711 20:08:12.807590 25943 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 20:08:12.807592 25943 net.cpp:163] Memory required for data: 1009664000
I0711 20:08:12.807596 25943 layer_factory.hpp:77] Creating layer ctx_conv1
I0711 20:08:12.807601 25943 net.cpp:98] Creating Layer ctx_conv1
I0711 20:08:12.807605 25943 net.cpp:439] ctx_conv1 <- out3_out5_combined
I0711 20:08:12.807610 25943 net.cpp:413] ctx_conv1 -> ctx_conv1
I0711 20:08:12.809015 25943 net.cpp:148] Setting up ctx_conv1
I0711 20:08:12.809025 25943 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 20:08:12.809027 25943 net.cpp:163] Memory required for data: 1017856000
I0711 20:08:12.809032 25943 layer_factory.hpp:77] Creating layer ctx_conv1/bn
I0711 20:08:12.809037 25943 net.cpp:98] Creating Layer ctx_conv1/bn
I0711 20:08:12.809041 25943 net.cpp:439] ctx_conv1/bn <- ctx_conv1
I0711 20:08:12.809044 25943 net.cpp:413] ctx_conv1/bn -> ctx_conv1/bn
I0711 20:08:12.810044 25943 net.cpp:148] Setting up ctx_conv1/bn
I0711 20:08:12.810051 25943 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 20:08:12.810055 25943 net.cpp:163] Memory required for data: 1026048000
I0711 20:08:12.810061 25943 layer_factory.hpp:77] Creating layer ctx_conv1/relu
I0711 20:08:12.810065 25943 net.cpp:98] Creating Layer ctx_conv1/relu
I0711 20:08:12.810070 25943 net.cpp:439] ctx_conv1/relu <- ctx_conv1/bn
I0711 20:08:12.810072 25943 net.cpp:400] ctx_conv1/relu -> ctx_conv1/bn (in-place)
I0711 20:08:12.810077 25943 net.cpp:148] Setting up ctx_conv1/relu
I0711 20:08:12.810081 25943 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 20:08:12.810083 25943 net.cpp:163] Memory required for data: 1034240000
I0711 20:08:12.810087 25943 layer_factory.hpp:77] Creating layer ctx_conv2
I0711 20:08:12.810096 25943 net.cpp:98] Creating Layer ctx_conv2
I0711 20:08:12.810098 25943 net.cpp:439] ctx_conv2 <- ctx_conv1/bn
I0711 20:08:12.810102 25943 net.cpp:413] ctx_conv2 -> ctx_conv2
I0711 20:08:12.811172 25943 net.cpp:148] Setting up ctx_conv2
I0711 20:08:12.811179 25943 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 20:08:12.811182 25943 net.cpp:163] Memory required for data: 1042432000
I0711 20:08:12.811184 25943 layer_factory.hpp:77] Creating layer ctx_conv2/bn
I0711 20:08:12.811188 25943 net.cpp:98] Creating Layer ctx_conv2/bn
I0711 20:08:12.811190 25943 net.cpp:439] ctx_conv2/bn <- ctx_conv2
I0711 20:08:12.811192 25943 net.cpp:413] ctx_conv2/bn -> ctx_conv2/bn
I0711 20:08:12.811923 25943 net.cpp:148] Setting up ctx_conv2/bn
I0711 20:08:12.811930 25943 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 20:08:12.811933 25943 net.cpp:163] Memory required for data: 1050624000
I0711 20:08:12.811938 25943 layer_factory.hpp:77] Creating layer ctx_conv2/relu
I0711 20:08:12.811941 25943 net.cpp:98] Creating Layer ctx_conv2/relu
I0711 20:08:12.811944 25943 net.cpp:439] ctx_conv2/relu <- ctx_conv2/bn
I0711 20:08:12.811946 25943 net.cpp:400] ctx_conv2/relu -> ctx_conv2/bn (in-place)
I0711 20:08:12.811949 25943 net.cpp:148] Setting up ctx_conv2/relu
I0711 20:08:12.811952 25943 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 20:08:12.811954 25943 net.cpp:163] Memory required for data: 1058816000
I0711 20:08:12.811955 25943 layer_factory.hpp:77] Creating layer ctx_conv3
I0711 20:08:12.811959 25943 net.cpp:98] Creating Layer ctx_conv3
I0711 20:08:12.811961 25943 net.cpp:439] ctx_conv3 <- ctx_conv2/bn
I0711 20:08:12.811964 25943 net.cpp:413] ctx_conv3 -> ctx_conv3
I0711 20:08:12.813072 25943 net.cpp:148] Setting up ctx_conv3
I0711 20:08:12.813079 25943 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 20:08:12.813081 25943 net.cpp:163] Memory required for data: 1067008000
I0711 20:08:12.813084 25943 layer_factory.hpp:77] Creating layer ctx_conv3/bn
I0711 20:08:12.813087 25943 net.cpp:98] Creating Layer ctx_conv3/bn
I0711 20:08:12.813091 25943 net.cpp:439] ctx_conv3/bn <- ctx_conv3
I0711 20:08:12.813092 25943 net.cpp:413] ctx_conv3/bn -> ctx_conv3/bn
I0711 20:08:12.813861 25943 net.cpp:148] Setting up ctx_conv3/bn
I0711 20:08:12.813869 25943 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 20:08:12.813871 25943 net.cpp:163] Memory required for data: 1075200000
I0711 20:08:12.813876 25943 layer_factory.hpp:77] Creating layer ctx_conv3/relu
I0711 20:08:12.813879 25943 net.cpp:98] Creating Layer ctx_conv3/relu
I0711 20:08:12.813881 25943 net.cpp:439] ctx_conv3/relu <- ctx_conv3/bn
I0711 20:08:12.813884 25943 net.cpp:400] ctx_conv3/relu -> ctx_conv3/bn (in-place)
I0711 20:08:12.813887 25943 net.cpp:148] Setting up ctx_conv3/relu
I0711 20:08:12.813890 25943 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 20:08:12.813891 25943 net.cpp:163] Memory required for data: 1083392000
I0711 20:08:12.813894 25943 layer_factory.hpp:77] Creating layer ctx_conv4
I0711 20:08:12.813897 25943 net.cpp:98] Creating Layer ctx_conv4
I0711 20:08:12.813899 25943 net.cpp:439] ctx_conv4 <- ctx_conv3/bn
I0711 20:08:12.813901 25943 net.cpp:413] ctx_conv4 -> ctx_conv4
I0711 20:08:12.815003 25943 net.cpp:148] Setting up ctx_conv4
I0711 20:08:12.815011 25943 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 20:08:12.815012 25943 net.cpp:163] Memory required for data: 1091584000
I0711 20:08:12.815016 25943 layer_factory.hpp:77] Creating layer ctx_conv4/bn
I0711 20:08:12.815019 25943 net.cpp:98] Creating Layer ctx_conv4/bn
I0711 20:08:12.815021 25943 net.cpp:439] ctx_conv4/bn <- ctx_conv4
I0711 20:08:12.815023 25943 net.cpp:413] ctx_conv4/bn -> ctx_conv4/bn
I0711 20:08:12.815778 25943 net.cpp:148] Setting up ctx_conv4/bn
I0711 20:08:12.815784 25943 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 20:08:12.815786 25943 net.cpp:163] Memory required for data: 1099776000
I0711 20:08:12.815791 25943 layer_factory.hpp:77] Creating layer ctx_conv4/relu
I0711 20:08:12.815794 25943 net.cpp:98] Creating Layer ctx_conv4/relu
I0711 20:08:12.815796 25943 net.cpp:439] ctx_conv4/relu <- ctx_conv4/bn
I0711 20:08:12.815798 25943 net.cpp:400] ctx_conv4/relu -> ctx_conv4/bn (in-place)
I0711 20:08:12.815801 25943 net.cpp:148] Setting up ctx_conv4/relu
I0711 20:08:12.815804 25943 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 20:08:12.815805 25943 net.cpp:163] Memory required for data: 1107968000
I0711 20:08:12.815807 25943 layer_factory.hpp:77] Creating layer ctx_final
I0711 20:08:12.815811 25943 net.cpp:98] Creating Layer ctx_final
I0711 20:08:12.815814 25943 net.cpp:439] ctx_final <- ctx_conv4/bn
I0711 20:08:12.815815 25943 net.cpp:413] ctx_final -> ctx_final
I0711 20:08:12.816287 25943 net.cpp:148] Setting up ctx_final
I0711 20:08:12.816293 25943 net.cpp:155] Top shape: 5 8 80 80 (256000)
I0711 20:08:12.816295 25943 net.cpp:163] Memory required for data: 1108992000
I0711 20:08:12.816299 25943 layer_factory.hpp:77] Creating layer ctx_final/relu
I0711 20:08:12.816301 25943 net.cpp:98] Creating Layer ctx_final/relu
I0711 20:08:12.816303 25943 net.cpp:439] ctx_final/relu <- ctx_final
I0711 20:08:12.816306 25943 net.cpp:400] ctx_final/relu -> ctx_final (in-place)
I0711 20:08:12.816309 25943 net.cpp:148] Setting up ctx_final/relu
I0711 20:08:12.816313 25943 net.cpp:155] Top shape: 5 8 80 80 (256000)
I0711 20:08:12.816313 25943 net.cpp:163] Memory required for data: 1110016000
I0711 20:08:12.816315 25943 layer_factory.hpp:77] Creating layer out_deconv_final_up2
I0711 20:08:12.816319 25943 net.cpp:98] Creating Layer out_deconv_final_up2
I0711 20:08:12.816321 25943 net.cpp:439] out_deconv_final_up2 <- ctx_final
I0711 20:08:12.816323 25943 net.cpp:413] out_deconv_final_up2 -> out_deconv_final_up2
I0711 20:08:12.816617 25943 net.cpp:148] Setting up out_deconv_final_up2
I0711 20:08:12.816625 25943 net.cpp:155] Top shape: 5 8 160 160 (1024000)
I0711 20:08:12.816628 25943 net.cpp:163] Memory required for data: 1114112000
I0711 20:08:12.816633 25943 layer_factory.hpp:77] Creating layer out_deconv_final_up4
I0711 20:08:12.816638 25943 net.cpp:98] Creating Layer out_deconv_final_up4
I0711 20:08:12.816642 25943 net.cpp:439] out_deconv_final_up4 <- out_deconv_final_up2
I0711 20:08:12.816646 25943 net.cpp:413] out_deconv_final_up4 -> out_deconv_final_up4
I0711 20:08:12.816958 25943 net.cpp:148] Setting up out_deconv_final_up4
I0711 20:08:12.816964 25943 net.cpp:155] Top shape: 5 8 320 320 (4096000)
I0711 20:08:12.816967 25943 net.cpp:163] Memory required for data: 1130496000
I0711 20:08:12.816969 25943 layer_factory.hpp:77] Creating layer out_deconv_final_up8
I0711 20:08:12.816972 25943 net.cpp:98] Creating Layer out_deconv_final_up8
I0711 20:08:12.816974 25943 net.cpp:439] out_deconv_final_up8 <- out_deconv_final_up4
I0711 20:08:12.816977 25943 net.cpp:413] out_deconv_final_up8 -> out_deconv_final_up8
I0711 20:08:12.817252 25943 net.cpp:148] Setting up out_deconv_final_up8
I0711 20:08:12.817258 25943 net.cpp:155] Top shape: 5 8 640 640 (16384000)
I0711 20:08:12.817260 25943 net.cpp:163] Memory required for data: 1196032000
I0711 20:08:12.817263 25943 layer_factory.hpp:77] Creating layer loss
I0711 20:08:12.817270 25943 net.cpp:98] Creating Layer loss
I0711 20:08:12.817272 25943 net.cpp:439] loss <- out_deconv_final_up8
I0711 20:08:12.817275 25943 net.cpp:439] loss <- label
I0711 20:08:12.817278 25943 net.cpp:413] loss -> loss
I0711 20:08:12.817286 25943 layer_factory.hpp:77] Creating layer loss
I0711 20:08:12.838810 25943 net.cpp:148] Setting up loss
I0711 20:08:12.838831 25943 net.cpp:155] Top shape: (1)
I0711 20:08:12.838834 25943 net.cpp:158]     with loss weight 1
I0711 20:08:12.838846 25943 net.cpp:163] Memory required for data: 1196032004
I0711 20:08:12.838850 25943 net.cpp:224] loss needs backward computation.
I0711 20:08:12.838852 25943 net.cpp:224] out_deconv_final_up8 needs backward computation.
I0711 20:08:12.838855 25943 net.cpp:224] out_deconv_final_up4 needs backward computation.
I0711 20:08:12.838856 25943 net.cpp:224] out_deconv_final_up2 needs backward computation.
I0711 20:08:12.838858 25943 net.cpp:224] ctx_final/relu needs backward computation.
I0711 20:08:12.838860 25943 net.cpp:224] ctx_final needs backward computation.
I0711 20:08:12.838862 25943 net.cpp:224] ctx_conv4/relu needs backward computation.
I0711 20:08:12.838863 25943 net.cpp:224] ctx_conv4/bn needs backward computation.
I0711 20:08:12.838865 25943 net.cpp:224] ctx_conv4 needs backward computation.
I0711 20:08:12.838867 25943 net.cpp:224] ctx_conv3/relu needs backward computation.
I0711 20:08:12.838870 25943 net.cpp:224] ctx_conv3/bn needs backward computation.
I0711 20:08:12.838871 25943 net.cpp:224] ctx_conv3 needs backward computation.
I0711 20:08:12.838873 25943 net.cpp:224] ctx_conv2/relu needs backward computation.
I0711 20:08:12.838876 25943 net.cpp:224] ctx_conv2/bn needs backward computation.
I0711 20:08:12.838877 25943 net.cpp:224] ctx_conv2 needs backward computation.
I0711 20:08:12.838881 25943 net.cpp:224] ctx_conv1/relu needs backward computation.
I0711 20:08:12.838882 25943 net.cpp:224] ctx_conv1/bn needs backward computation.
I0711 20:08:12.838886 25943 net.cpp:224] ctx_conv1 needs backward computation.
I0711 20:08:12.838887 25943 net.cpp:224] out3_out5_combined needs backward computation.
I0711 20:08:12.838891 25943 net.cpp:224] out3a/relu needs backward computation.
I0711 20:08:12.838892 25943 net.cpp:224] out3a/bn needs backward computation.
I0711 20:08:12.838894 25943 net.cpp:224] out3a needs backward computation.
I0711 20:08:12.838897 25943 net.cpp:224] out5a_up2 needs backward computation.
I0711 20:08:12.838899 25943 net.cpp:224] out5a/relu needs backward computation.
I0711 20:08:12.838901 25943 net.cpp:224] out5a/bn needs backward computation.
I0711 20:08:12.838903 25943 net.cpp:224] out5a needs backward computation.
I0711 20:08:12.838907 25943 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0711 20:08:12.838909 25943 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0711 20:08:12.838912 25943 net.cpp:224] res5a_branch2b needs backward computation.
I0711 20:08:12.838913 25943 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0711 20:08:12.838915 25943 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0711 20:08:12.838917 25943 net.cpp:224] res5a_branch2a needs backward computation.
I0711 20:08:12.838932 25943 net.cpp:224] pool4 needs backward computation.
I0711 20:08:12.838935 25943 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0711 20:08:12.838939 25943 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0711 20:08:12.838943 25943 net.cpp:224] res4a_branch2b needs backward computation.
I0711 20:08:12.838946 25943 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0711 20:08:12.838949 25943 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0711 20:08:12.838953 25943 net.cpp:224] res4a_branch2a needs backward computation.
I0711 20:08:12.838958 25943 net.cpp:224] pool3 needs backward computation.
I0711 20:08:12.838961 25943 net.cpp:224] res3a_branch2b/bn_res3a_branch2b/relu_0_split needs backward computation.
I0711 20:08:12.838965 25943 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0711 20:08:12.838969 25943 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0711 20:08:12.838973 25943 net.cpp:224] res3a_branch2b needs backward computation.
I0711 20:08:12.838977 25943 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0711 20:08:12.838981 25943 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0711 20:08:12.838985 25943 net.cpp:224] res3a_branch2a needs backward computation.
I0711 20:08:12.838989 25943 net.cpp:224] pool2 needs backward computation.
I0711 20:08:12.838994 25943 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0711 20:08:12.838999 25943 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0711 20:08:12.839002 25943 net.cpp:224] res2a_branch2b needs backward computation.
I0711 20:08:12.839006 25943 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0711 20:08:12.839010 25943 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0711 20:08:12.839015 25943 net.cpp:224] res2a_branch2a needs backward computation.
I0711 20:08:12.839018 25943 net.cpp:224] pool1 needs backward computation.
I0711 20:08:12.839022 25943 net.cpp:224] conv1b/relu needs backward computation.
I0711 20:08:12.839026 25943 net.cpp:224] conv1b/bn needs backward computation.
I0711 20:08:12.839030 25943 net.cpp:224] conv1b needs backward computation.
I0711 20:08:12.839035 25943 net.cpp:224] conv1a/relu needs backward computation.
I0711 20:08:12.839037 25943 net.cpp:224] conv1a/bn needs backward computation.
I0711 20:08:12.839041 25943 net.cpp:224] conv1a needs backward computation.
I0711 20:08:12.839046 25943 net.cpp:226] data/bias does not need backward computation.
I0711 20:08:12.839051 25943 net.cpp:226] data does not need backward computation.
I0711 20:08:12.839054 25943 net.cpp:268] This network produces output loss
I0711 20:08:12.839097 25943 net.cpp:288] Network initialization done.
I0711 20:08:12.839880 25943 solver.cpp:182] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/l1reg/test.prototxt
I0711 20:08:12.840175 25943 net.cpp:56] Initializing net from parameters: 
name: "jsegnet21v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/val-image-lmdb"
    label_list_path: "data/val-label-lmdb"
    batch_size: 4
    threads: 4
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b/bn"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a/bn"
  top: "out5a/bn"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a/bn"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b/bn"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a/bn"
  top: "out3a/bn"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a/bn"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv1/bn"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv2/bn"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv3/bn"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4/bn"
  top: "ctx_conv4/bn"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4/bn"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
  accuracy_param {
    ignore_label: 255
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
    ignore_label: 255
  }
}
I0711 20:08:12.840304 25943 layer_factory.hpp:77] Creating layer data
I0711 20:08:12.840314 25943 net.cpp:98] Creating Layer data
I0711 20:08:12.840319 25943 net.cpp:413] data -> data
I0711 20:08:12.840327 25943 net.cpp:413] data -> label
I0711 20:08:12.855726 26027 db_lmdb.cpp:35] Opened lmdb data/val-image-lmdb
I0711 20:08:12.859331 25943 data_layer.cpp:78] ReshapePrefetch 4, 3, 640, 640
I0711 20:08:12.859444 25943 data_layer.cpp:83] output data size: 4,3,640,640
I0711 20:08:12.869568 26032 db_lmdb.cpp:35] Opened lmdb data/val-label-lmdb
I0711 20:08:12.886693 25943 data_layer.cpp:78] ReshapePrefetch 4, 1, 640, 640
I0711 20:08:12.886765 25943 data_layer.cpp:83] output data size: 4,1,640,640
I0711 20:08:12.903172 25943 net.cpp:148] Setting up data
I0711 20:08:12.903203 25943 net.cpp:155] Top shape: 4 3 640 640 (4915200)
I0711 20:08:12.903208 25943 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0711 20:08:12.903209 25943 net.cpp:163] Memory required for data: 26214400
I0711 20:08:12.903214 25943 layer_factory.hpp:77] Creating layer label_data_1_split
I0711 20:08:12.903228 25943 net.cpp:98] Creating Layer label_data_1_split
I0711 20:08:12.903230 25943 net.cpp:439] label_data_1_split <- label
I0711 20:08:12.903275 25943 net.cpp:413] label_data_1_split -> label_data_1_split_0
I0711 20:08:12.903296 25943 net.cpp:413] label_data_1_split -> label_data_1_split_1
I0711 20:08:12.903301 25943 net.cpp:413] label_data_1_split -> label_data_1_split_2
I0711 20:08:12.904938 25943 net.cpp:148] Setting up label_data_1_split
I0711 20:08:12.904945 25943 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0711 20:08:12.904948 25943 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0711 20:08:12.904950 25943 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0711 20:08:12.904953 25943 net.cpp:163] Memory required for data: 45875200
I0711 20:08:12.904954 25943 layer_factory.hpp:77] Creating layer data/bias
I0711 20:08:12.904960 25943 net.cpp:98] Creating Layer data/bias
I0711 20:08:12.904963 25943 net.cpp:439] data/bias <- data
I0711 20:08:12.904965 25943 net.cpp:413] data/bias -> data/bias
I0711 20:08:12.906335 25943 net.cpp:148] Setting up data/bias
I0711 20:08:12.906345 25943 net.cpp:155] Top shape: 4 3 640 640 (4915200)
I0711 20:08:12.906347 25943 net.cpp:163] Memory required for data: 65536000
I0711 20:08:12.906353 25943 layer_factory.hpp:77] Creating layer conv1a
I0711 20:08:12.906360 25943 net.cpp:98] Creating Layer conv1a
I0711 20:08:12.906363 25943 net.cpp:439] conv1a <- data/bias
I0711 20:08:12.906366 25943 net.cpp:413] conv1a -> conv1a
I0711 20:08:12.908598 25943 net.cpp:148] Setting up conv1a
I0711 20:08:12.908608 25943 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0711 20:08:12.908612 25943 net.cpp:163] Memory required for data: 117964800
I0711 20:08:12.908617 25943 layer_factory.hpp:77] Creating layer conv1a/bn
I0711 20:08:12.908627 25943 net.cpp:98] Creating Layer conv1a/bn
I0711 20:08:12.908629 25943 net.cpp:439] conv1a/bn <- conv1a
I0711 20:08:12.908633 25943 net.cpp:413] conv1a/bn -> conv1a/bn
I0711 20:08:12.916867 25943 net.cpp:148] Setting up conv1a/bn
I0711 20:08:12.916929 25943 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0711 20:08:12.916931 25943 net.cpp:163] Memory required for data: 170393600
I0711 20:08:12.916957 25943 layer_factory.hpp:77] Creating layer conv1a/relu
I0711 20:08:12.916973 25943 net.cpp:98] Creating Layer conv1a/relu
I0711 20:08:12.916981 25943 net.cpp:439] conv1a/relu <- conv1a/bn
I0711 20:08:12.916988 25943 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0711 20:08:12.917217 25943 net.cpp:148] Setting up conv1a/relu
I0711 20:08:12.917227 25943 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0711 20:08:12.917230 25943 net.cpp:163] Memory required for data: 222822400
I0711 20:08:12.917235 25943 layer_factory.hpp:77] Creating layer conv1b
I0711 20:08:12.917251 25943 net.cpp:98] Creating Layer conv1b
I0711 20:08:12.917256 25943 net.cpp:439] conv1b <- conv1a/bn
I0711 20:08:12.917263 25943 net.cpp:413] conv1b -> conv1b
I0711 20:08:12.918009 25943 net.cpp:148] Setting up conv1b
I0711 20:08:12.918020 25943 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0711 20:08:12.918021 25943 net.cpp:163] Memory required for data: 275251200
I0711 20:08:12.918027 25943 layer_factory.hpp:77] Creating layer conv1b/bn
I0711 20:08:12.918040 25943 net.cpp:98] Creating Layer conv1b/bn
I0711 20:08:12.918046 25943 net.cpp:439] conv1b/bn <- conv1b
I0711 20:08:12.918053 25943 net.cpp:413] conv1b/bn -> conv1b/bn
I0711 20:08:12.921620 25943 net.cpp:148] Setting up conv1b/bn
I0711 20:08:12.921747 25943 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0711 20:08:12.921761 25943 net.cpp:163] Memory required for data: 327680000
I0711 20:08:12.921864 25943 layer_factory.hpp:77] Creating layer conv1b/relu
I0711 20:08:12.921875 25943 net.cpp:98] Creating Layer conv1b/relu
I0711 20:08:12.921880 25943 net.cpp:439] conv1b/relu <- conv1b/bn
I0711 20:08:12.921886 25943 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0711 20:08:12.921984 25943 net.cpp:148] Setting up conv1b/relu
I0711 20:08:12.921993 25943 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0711 20:08:12.921995 25943 net.cpp:163] Memory required for data: 380108800
I0711 20:08:12.921999 25943 layer_factory.hpp:77] Creating layer pool1
I0711 20:08:12.922127 25943 net.cpp:98] Creating Layer pool1
I0711 20:08:12.922135 25943 net.cpp:439] pool1 <- conv1b/bn
I0711 20:08:12.922142 25943 net.cpp:413] pool1 -> pool1
I0711 20:08:12.922281 25943 net.cpp:148] Setting up pool1
I0711 20:08:12.922289 25943 net.cpp:155] Top shape: 4 32 160 160 (3276800)
I0711 20:08:12.922293 25943 net.cpp:163] Memory required for data: 393216000
I0711 20:08:12.922353 25943 layer_factory.hpp:77] Creating layer res2a_branch2a
I0711 20:08:12.922410 25943 net.cpp:98] Creating Layer res2a_branch2a
I0711 20:08:12.922449 25943 net.cpp:439] res2a_branch2a <- pool1
I0711 20:08:12.922489 25943 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0711 20:08:12.923413 25943 net.cpp:148] Setting up res2a_branch2a
I0711 20:08:12.923421 25943 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0711 20:08:12.923424 25943 net.cpp:163] Memory required for data: 419430400
I0711 20:08:12.923434 25943 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0711 20:08:12.923549 25943 net.cpp:98] Creating Layer res2a_branch2a/bn
I0711 20:08:12.923555 25943 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0711 20:08:12.923560 25943 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0711 20:08:12.926616 25943 net.cpp:148] Setting up res2a_branch2a/bn
I0711 20:08:12.926628 25943 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0711 20:08:12.926630 25943 net.cpp:163] Memory required for data: 445644800
I0711 20:08:12.926636 25943 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0711 20:08:12.926641 25943 net.cpp:98] Creating Layer res2a_branch2a/relu
I0711 20:08:12.926643 25943 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0711 20:08:12.926646 25943 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0711 20:08:12.926651 25943 net.cpp:148] Setting up res2a_branch2a/relu
I0711 20:08:12.926653 25943 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0711 20:08:12.926656 25943 net.cpp:163] Memory required for data: 471859200
I0711 20:08:12.926657 25943 layer_factory.hpp:77] Creating layer res2a_branch2b
I0711 20:08:12.926664 25943 net.cpp:98] Creating Layer res2a_branch2b
I0711 20:08:12.926857 25943 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0711 20:08:12.926865 25943 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0711 20:08:12.928807 25943 net.cpp:148] Setting up res2a_branch2b
I0711 20:08:12.928860 25943 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0711 20:08:12.928872 25943 net.cpp:163] Memory required for data: 498073600
I0711 20:08:12.928889 25943 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0711 20:08:12.928910 25943 net.cpp:98] Creating Layer res2a_branch2b/bn
I0711 20:08:12.928923 25943 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0711 20:08:12.928938 25943 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0711 20:08:12.933954 25943 net.cpp:148] Setting up res2a_branch2b/bn
I0711 20:08:12.933986 25943 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0711 20:08:12.933990 25943 net.cpp:163] Memory required for data: 524288000
I0711 20:08:12.934002 25943 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0711 20:08:12.934015 25943 net.cpp:98] Creating Layer res2a_branch2b/relu
I0711 20:08:12.934023 25943 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0711 20:08:12.934029 25943 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0711 20:08:12.934042 25943 net.cpp:148] Setting up res2a_branch2b/relu
I0711 20:08:12.934048 25943 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0711 20:08:12.934051 25943 net.cpp:163] Memory required for data: 550502400
I0711 20:08:12.934056 25943 layer_factory.hpp:77] Creating layer pool2
I0711 20:08:12.934067 25943 net.cpp:98] Creating Layer pool2
I0711 20:08:12.934072 25943 net.cpp:439] pool2 <- res2a_branch2b/bn
I0711 20:08:12.934077 25943 net.cpp:413] pool2 -> pool2
I0711 20:08:12.934150 25943 net.cpp:148] Setting up pool2
I0711 20:08:12.934156 25943 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 20:08:12.934159 25943 net.cpp:163] Memory required for data: 557056000
I0711 20:08:12.934164 25943 layer_factory.hpp:77] Creating layer res3a_branch2a
I0711 20:08:12.934202 25943 net.cpp:98] Creating Layer res3a_branch2a
I0711 20:08:12.934207 25943 net.cpp:439] res3a_branch2a <- pool2
I0711 20:08:12.934213 25943 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0711 20:08:12.936187 25943 net.cpp:148] Setting up res3a_branch2a
I0711 20:08:12.936197 25943 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 20:08:12.936200 25943 net.cpp:163] Memory required for data: 570163200
I0711 20:08:12.936205 25943 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0711 20:08:12.936218 25943 net.cpp:98] Creating Layer res3a_branch2a/bn
I0711 20:08:12.936223 25943 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0711 20:08:12.936228 25943 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0711 20:08:12.937463 25943 net.cpp:148] Setting up res3a_branch2a/bn
I0711 20:08:12.937477 25943 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 20:08:12.937480 25943 net.cpp:163] Memory required for data: 583270400
I0711 20:08:12.937496 25943 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0711 20:08:12.937503 25943 net.cpp:98] Creating Layer res3a_branch2a/relu
I0711 20:08:12.937506 25943 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0711 20:08:12.937511 25943 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0711 20:08:12.937517 25943 net.cpp:148] Setting up res3a_branch2a/relu
I0711 20:08:12.937521 25943 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 20:08:12.937525 25943 net.cpp:163] Memory required for data: 596377600
I0711 20:08:12.937527 25943 layer_factory.hpp:77] Creating layer res3a_branch2b
I0711 20:08:12.937535 25943 net.cpp:98] Creating Layer res3a_branch2b
I0711 20:08:12.937537 25943 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0711 20:08:12.937542 25943 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0711 20:08:12.939002 25943 net.cpp:148] Setting up res3a_branch2b
I0711 20:08:12.939015 25943 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 20:08:12.939018 25943 net.cpp:163] Memory required for data: 609484800
I0711 20:08:12.939026 25943 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0711 20:08:12.939033 25943 net.cpp:98] Creating Layer res3a_branch2b/bn
I0711 20:08:12.939038 25943 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0711 20:08:12.939043 25943 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0711 20:08:12.940212 25943 net.cpp:148] Setting up res3a_branch2b/bn
I0711 20:08:12.940225 25943 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 20:08:12.940229 25943 net.cpp:163] Memory required for data: 622592000
I0711 20:08:12.940238 25943 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0711 20:08:12.940244 25943 net.cpp:98] Creating Layer res3a_branch2b/relu
I0711 20:08:12.940248 25943 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0711 20:08:12.940253 25943 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0711 20:08:12.940258 25943 net.cpp:148] Setting up res3a_branch2b/relu
I0711 20:08:12.940263 25943 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 20:08:12.940279 25943 net.cpp:163] Memory required for data: 635699200
I0711 20:08:12.940282 25943 layer_factory.hpp:77] Creating layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0711 20:08:12.940291 25943 net.cpp:98] Creating Layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0711 20:08:12.940295 25943 net.cpp:439] res3a_branch2b/bn_res3a_branch2b/relu_0_split <- res3a_branch2b/bn
I0711 20:08:12.940299 25943 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0711 20:08:12.940307 25943 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0711 20:08:12.940373 25943 net.cpp:148] Setting up res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0711 20:08:12.940381 25943 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 20:08:12.940385 25943 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 20:08:12.940388 25943 net.cpp:163] Memory required for data: 661913600
I0711 20:08:12.940409 25943 layer_factory.hpp:77] Creating layer pool3
I0711 20:08:12.940415 25943 net.cpp:98] Creating Layer pool3
I0711 20:08:12.940420 25943 net.cpp:439] pool3 <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0711 20:08:12.940425 25943 net.cpp:413] pool3 -> pool3
I0711 20:08:12.940500 25943 net.cpp:148] Setting up pool3
I0711 20:08:12.940505 25943 net.cpp:155] Top shape: 4 128 40 40 (819200)
I0711 20:08:12.940510 25943 net.cpp:163] Memory required for data: 665190400
I0711 20:08:12.940513 25943 layer_factory.hpp:77] Creating layer res4a_branch2a
I0711 20:08:12.940522 25943 net.cpp:98] Creating Layer res4a_branch2a
I0711 20:08:12.940526 25943 net.cpp:439] res4a_branch2a <- pool3
I0711 20:08:12.940532 25943 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0711 20:08:12.948571 25943 net.cpp:148] Setting up res4a_branch2a
I0711 20:08:12.948582 25943 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0711 20:08:12.948585 25943 net.cpp:163] Memory required for data: 671744000
I0711 20:08:12.948588 25943 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0711 20:08:12.948595 25943 net.cpp:98] Creating Layer res4a_branch2a/bn
I0711 20:08:12.948597 25943 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0711 20:08:12.948602 25943 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0711 20:08:12.949304 25943 net.cpp:148] Setting up res4a_branch2a/bn
I0711 20:08:12.949311 25943 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0711 20:08:12.949312 25943 net.cpp:163] Memory required for data: 678297600
I0711 20:08:12.949317 25943 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0711 20:08:12.949321 25943 net.cpp:98] Creating Layer res4a_branch2a/relu
I0711 20:08:12.949323 25943 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0711 20:08:12.949326 25943 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0711 20:08:12.949331 25943 net.cpp:148] Setting up res4a_branch2a/relu
I0711 20:08:12.949334 25943 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0711 20:08:12.949337 25943 net.cpp:163] Memory required for data: 684851200
I0711 20:08:12.949338 25943 layer_factory.hpp:77] Creating layer res4a_branch2b
I0711 20:08:12.949342 25943 net.cpp:98] Creating Layer res4a_branch2b
I0711 20:08:12.949345 25943 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0711 20:08:12.949348 25943 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0711 20:08:12.952553 25943 net.cpp:148] Setting up res4a_branch2b
I0711 20:08:12.952558 25943 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0711 20:08:12.952560 25943 net.cpp:163] Memory required for data: 691404800
I0711 20:08:12.952564 25943 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0711 20:08:12.952567 25943 net.cpp:98] Creating Layer res4a_branch2b/bn
I0711 20:08:12.952570 25943 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0711 20:08:12.952574 25943 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0711 20:08:12.953260 25943 net.cpp:148] Setting up res4a_branch2b/bn
I0711 20:08:12.953265 25943 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0711 20:08:12.953268 25943 net.cpp:163] Memory required for data: 697958400
I0711 20:08:12.953272 25943 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0711 20:08:12.953275 25943 net.cpp:98] Creating Layer res4a_branch2b/relu
I0711 20:08:12.953277 25943 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0711 20:08:12.953279 25943 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0711 20:08:12.953284 25943 net.cpp:148] Setting up res4a_branch2b/relu
I0711 20:08:12.953285 25943 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0711 20:08:12.953286 25943 net.cpp:163] Memory required for data: 704512000
I0711 20:08:12.953289 25943 layer_factory.hpp:77] Creating layer pool4
I0711 20:08:12.953292 25943 net.cpp:98] Creating Layer pool4
I0711 20:08:12.953294 25943 net.cpp:439] pool4 <- res4a_branch2b/bn
I0711 20:08:12.953295 25943 net.cpp:413] pool4 -> pool4
I0711 20:08:12.953336 25943 net.cpp:148] Setting up pool4
I0711 20:08:12.953339 25943 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0711 20:08:12.953349 25943 net.cpp:163] Memory required for data: 711065600
I0711 20:08:12.953351 25943 layer_factory.hpp:77] Creating layer res5a_branch2a
I0711 20:08:12.953356 25943 net.cpp:98] Creating Layer res5a_branch2a
I0711 20:08:12.953358 25943 net.cpp:439] res5a_branch2a <- pool4
I0711 20:08:12.953361 25943 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0711 20:08:12.978256 25943 net.cpp:148] Setting up res5a_branch2a
I0711 20:08:12.978272 25943 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0711 20:08:12.978274 25943 net.cpp:163] Memory required for data: 724172800
I0711 20:08:12.978279 25943 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0711 20:08:12.978288 25943 net.cpp:98] Creating Layer res5a_branch2a/bn
I0711 20:08:12.978291 25943 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0711 20:08:12.978294 25943 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0711 20:08:12.978998 25943 net.cpp:148] Setting up res5a_branch2a/bn
I0711 20:08:12.979004 25943 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0711 20:08:12.979007 25943 net.cpp:163] Memory required for data: 737280000
I0711 20:08:12.979012 25943 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0711 20:08:12.979014 25943 net.cpp:98] Creating Layer res5a_branch2a/relu
I0711 20:08:12.979017 25943 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0711 20:08:12.979019 25943 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0711 20:08:12.979023 25943 net.cpp:148] Setting up res5a_branch2a/relu
I0711 20:08:12.979025 25943 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0711 20:08:12.979027 25943 net.cpp:163] Memory required for data: 750387200
I0711 20:08:12.979029 25943 layer_factory.hpp:77] Creating layer res5a_branch2b
I0711 20:08:12.979033 25943 net.cpp:98] Creating Layer res5a_branch2b
I0711 20:08:12.979035 25943 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0711 20:08:12.979038 25943 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0711 20:08:12.992012 25943 net.cpp:148] Setting up res5a_branch2b
I0711 20:08:12.992033 25943 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0711 20:08:12.992034 25943 net.cpp:163] Memory required for data: 763494400
I0711 20:08:12.992043 25943 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0711 20:08:12.992049 25943 net.cpp:98] Creating Layer res5a_branch2b/bn
I0711 20:08:12.992053 25943 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0711 20:08:12.992058 25943 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0711 20:08:12.992774 25943 net.cpp:148] Setting up res5a_branch2b/bn
I0711 20:08:12.992780 25943 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0711 20:08:12.992782 25943 net.cpp:163] Memory required for data: 776601600
I0711 20:08:12.992787 25943 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0711 20:08:12.992790 25943 net.cpp:98] Creating Layer res5a_branch2b/relu
I0711 20:08:12.992794 25943 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0711 20:08:12.992795 25943 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0711 20:08:12.992799 25943 net.cpp:148] Setting up res5a_branch2b/relu
I0711 20:08:12.992801 25943 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0711 20:08:12.992804 25943 net.cpp:163] Memory required for data: 789708800
I0711 20:08:12.992805 25943 layer_factory.hpp:77] Creating layer out5a
I0711 20:08:12.992810 25943 net.cpp:98] Creating Layer out5a
I0711 20:08:12.992812 25943 net.cpp:439] out5a <- res5a_branch2b/bn
I0711 20:08:12.992818 25943 net.cpp:413] out5a -> out5a
I0711 20:08:12.997078 25943 net.cpp:148] Setting up out5a
I0711 20:08:12.997087 25943 net.cpp:155] Top shape: 4 64 40 40 (409600)
I0711 20:08:12.997089 25943 net.cpp:163] Memory required for data: 791347200
I0711 20:08:12.997093 25943 layer_factory.hpp:77] Creating layer out5a/bn
I0711 20:08:12.997098 25943 net.cpp:98] Creating Layer out5a/bn
I0711 20:08:12.997100 25943 net.cpp:439] out5a/bn <- out5a
I0711 20:08:12.997103 25943 net.cpp:413] out5a/bn -> out5a/bn
I0711 20:08:12.997874 25943 net.cpp:148] Setting up out5a/bn
I0711 20:08:12.997880 25943 net.cpp:155] Top shape: 4 64 40 40 (409600)
I0711 20:08:12.997890 25943 net.cpp:163] Memory required for data: 792985600
I0711 20:08:12.997895 25943 layer_factory.hpp:77] Creating layer out5a/relu
I0711 20:08:12.997900 25943 net.cpp:98] Creating Layer out5a/relu
I0711 20:08:12.997901 25943 net.cpp:439] out5a/relu <- out5a/bn
I0711 20:08:12.997903 25943 net.cpp:400] out5a/relu -> out5a/bn (in-place)
I0711 20:08:12.997907 25943 net.cpp:148] Setting up out5a/relu
I0711 20:08:12.997910 25943 net.cpp:155] Top shape: 4 64 40 40 (409600)
I0711 20:08:12.997911 25943 net.cpp:163] Memory required for data: 794624000
I0711 20:08:12.997913 25943 layer_factory.hpp:77] Creating layer out5a_up2
I0711 20:08:12.997917 25943 net.cpp:98] Creating Layer out5a_up2
I0711 20:08:12.997920 25943 net.cpp:439] out5a_up2 <- out5a/bn
I0711 20:08:12.997922 25943 net.cpp:413] out5a_up2 -> out5a_up2
I0711 20:08:12.998190 25943 net.cpp:148] Setting up out5a_up2
I0711 20:08:12.998195 25943 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 20:08:12.998196 25943 net.cpp:163] Memory required for data: 801177600
I0711 20:08:12.998199 25943 layer_factory.hpp:77] Creating layer out3a
I0711 20:08:12.998203 25943 net.cpp:98] Creating Layer out3a
I0711 20:08:12.998205 25943 net.cpp:439] out3a <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0711 20:08:12.998209 25943 net.cpp:413] out3a -> out3a
I0711 20:08:13.000190 25943 net.cpp:148] Setting up out3a
I0711 20:08:13.000197 25943 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 20:08:13.000200 25943 net.cpp:163] Memory required for data: 807731200
I0711 20:08:13.000203 25943 layer_factory.hpp:77] Creating layer out3a/bn
I0711 20:08:13.000207 25943 net.cpp:98] Creating Layer out3a/bn
I0711 20:08:13.000210 25943 net.cpp:439] out3a/bn <- out3a
I0711 20:08:13.000212 25943 net.cpp:413] out3a/bn -> out3a/bn
I0711 20:08:13.001003 25943 net.cpp:148] Setting up out3a/bn
I0711 20:08:13.001009 25943 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 20:08:13.001011 25943 net.cpp:163] Memory required for data: 814284800
I0711 20:08:13.001016 25943 layer_factory.hpp:77] Creating layer out3a/relu
I0711 20:08:13.001019 25943 net.cpp:98] Creating Layer out3a/relu
I0711 20:08:13.001021 25943 net.cpp:439] out3a/relu <- out3a/bn
I0711 20:08:13.001024 25943 net.cpp:400] out3a/relu -> out3a/bn (in-place)
I0711 20:08:13.001027 25943 net.cpp:148] Setting up out3a/relu
I0711 20:08:13.001029 25943 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 20:08:13.001030 25943 net.cpp:163] Memory required for data: 820838400
I0711 20:08:13.001032 25943 layer_factory.hpp:77] Creating layer out3_out5_combined
I0711 20:08:13.001035 25943 net.cpp:98] Creating Layer out3_out5_combined
I0711 20:08:13.001037 25943 net.cpp:439] out3_out5_combined <- out5a_up2
I0711 20:08:13.001039 25943 net.cpp:439] out3_out5_combined <- out3a/bn
I0711 20:08:13.001042 25943 net.cpp:413] out3_out5_combined -> out3_out5_combined
I0711 20:08:13.001066 25943 net.cpp:148] Setting up out3_out5_combined
I0711 20:08:13.001070 25943 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 20:08:13.001071 25943 net.cpp:163] Memory required for data: 827392000
I0711 20:08:13.001073 25943 layer_factory.hpp:77] Creating layer ctx_conv1
I0711 20:08:13.001077 25943 net.cpp:98] Creating Layer ctx_conv1
I0711 20:08:13.001080 25943 net.cpp:439] ctx_conv1 <- out3_out5_combined
I0711 20:08:13.001082 25943 net.cpp:413] ctx_conv1 -> ctx_conv1
I0711 20:08:13.002125 25943 net.cpp:148] Setting up ctx_conv1
I0711 20:08:13.002130 25943 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 20:08:13.002131 25943 net.cpp:163] Memory required for data: 833945600
I0711 20:08:13.002135 25943 layer_factory.hpp:77] Creating layer ctx_conv1/bn
I0711 20:08:13.002137 25943 net.cpp:98] Creating Layer ctx_conv1/bn
I0711 20:08:13.002140 25943 net.cpp:439] ctx_conv1/bn <- ctx_conv1
I0711 20:08:13.002142 25943 net.cpp:413] ctx_conv1/bn -> ctx_conv1/bn
I0711 20:08:13.002915 25943 net.cpp:148] Setting up ctx_conv1/bn
I0711 20:08:13.002920 25943 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 20:08:13.002923 25943 net.cpp:163] Memory required for data: 840499200
I0711 20:08:13.002934 25943 layer_factory.hpp:77] Creating layer ctx_conv1/relu
I0711 20:08:13.002938 25943 net.cpp:98] Creating Layer ctx_conv1/relu
I0711 20:08:13.002939 25943 net.cpp:439] ctx_conv1/relu <- ctx_conv1/bn
I0711 20:08:13.002941 25943 net.cpp:400] ctx_conv1/relu -> ctx_conv1/bn (in-place)
I0711 20:08:13.002945 25943 net.cpp:148] Setting up ctx_conv1/relu
I0711 20:08:13.002948 25943 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 20:08:13.002949 25943 net.cpp:163] Memory required for data: 847052800
I0711 20:08:13.002951 25943 layer_factory.hpp:77] Creating layer ctx_conv2
I0711 20:08:13.002956 25943 net.cpp:98] Creating Layer ctx_conv2
I0711 20:08:13.002959 25943 net.cpp:439] ctx_conv2 <- ctx_conv1/bn
I0711 20:08:13.002961 25943 net.cpp:413] ctx_conv2 -> ctx_conv2
I0711 20:08:13.004005 25943 net.cpp:148] Setting up ctx_conv2
I0711 20:08:13.004010 25943 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 20:08:13.004012 25943 net.cpp:163] Memory required for data: 853606400
I0711 20:08:13.004015 25943 layer_factory.hpp:77] Creating layer ctx_conv2/bn
I0711 20:08:13.004019 25943 net.cpp:98] Creating Layer ctx_conv2/bn
I0711 20:08:13.004020 25943 net.cpp:439] ctx_conv2/bn <- ctx_conv2
I0711 20:08:13.004024 25943 net.cpp:413] ctx_conv2/bn -> ctx_conv2/bn
I0711 20:08:13.004806 25943 net.cpp:148] Setting up ctx_conv2/bn
I0711 20:08:13.004811 25943 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 20:08:13.004812 25943 net.cpp:163] Memory required for data: 860160000
I0711 20:08:13.004820 25943 layer_factory.hpp:77] Creating layer ctx_conv2/relu
I0711 20:08:13.004823 25943 net.cpp:98] Creating Layer ctx_conv2/relu
I0711 20:08:13.004827 25943 net.cpp:439] ctx_conv2/relu <- ctx_conv2/bn
I0711 20:08:13.004828 25943 net.cpp:400] ctx_conv2/relu -> ctx_conv2/bn (in-place)
I0711 20:08:13.004832 25943 net.cpp:148] Setting up ctx_conv2/relu
I0711 20:08:13.004834 25943 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 20:08:13.004835 25943 net.cpp:163] Memory required for data: 866713600
I0711 20:08:13.004837 25943 layer_factory.hpp:77] Creating layer ctx_conv3
I0711 20:08:13.004842 25943 net.cpp:98] Creating Layer ctx_conv3
I0711 20:08:13.004843 25943 net.cpp:439] ctx_conv3 <- ctx_conv2/bn
I0711 20:08:13.004847 25943 net.cpp:413] ctx_conv3 -> ctx_conv3
I0711 20:08:13.005889 25943 net.cpp:148] Setting up ctx_conv3
I0711 20:08:13.005894 25943 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 20:08:13.005895 25943 net.cpp:163] Memory required for data: 873267200
I0711 20:08:13.005898 25943 layer_factory.hpp:77] Creating layer ctx_conv3/bn
I0711 20:08:13.005903 25943 net.cpp:98] Creating Layer ctx_conv3/bn
I0711 20:08:13.005904 25943 net.cpp:439] ctx_conv3/bn <- ctx_conv3
I0711 20:08:13.005908 25943 net.cpp:413] ctx_conv3/bn -> ctx_conv3/bn
I0711 20:08:13.006677 25943 net.cpp:148] Setting up ctx_conv3/bn
I0711 20:08:13.006682 25943 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 20:08:13.006685 25943 net.cpp:163] Memory required for data: 879820800
I0711 20:08:13.006690 25943 layer_factory.hpp:77] Creating layer ctx_conv3/relu
I0711 20:08:13.006692 25943 net.cpp:98] Creating Layer ctx_conv3/relu
I0711 20:08:13.006695 25943 net.cpp:439] ctx_conv3/relu <- ctx_conv3/bn
I0711 20:08:13.006696 25943 net.cpp:400] ctx_conv3/relu -> ctx_conv3/bn (in-place)
I0711 20:08:13.006700 25943 net.cpp:148] Setting up ctx_conv3/relu
I0711 20:08:13.006702 25943 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 20:08:13.006703 25943 net.cpp:163] Memory required for data: 886374400
I0711 20:08:13.006705 25943 layer_factory.hpp:77] Creating layer ctx_conv4
I0711 20:08:13.006709 25943 net.cpp:98] Creating Layer ctx_conv4
I0711 20:08:13.006711 25943 net.cpp:439] ctx_conv4 <- ctx_conv3/bn
I0711 20:08:13.006713 25943 net.cpp:413] ctx_conv4 -> ctx_conv4
I0711 20:08:13.007755 25943 net.cpp:148] Setting up ctx_conv4
I0711 20:08:13.007760 25943 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 20:08:13.007761 25943 net.cpp:163] Memory required for data: 892928000
I0711 20:08:13.007764 25943 layer_factory.hpp:77] Creating layer ctx_conv4/bn
I0711 20:08:13.007772 25943 net.cpp:98] Creating Layer ctx_conv4/bn
I0711 20:08:13.007774 25943 net.cpp:439] ctx_conv4/bn <- ctx_conv4
I0711 20:08:13.007777 25943 net.cpp:413] ctx_conv4/bn -> ctx_conv4/bn
I0711 20:08:13.008548 25943 net.cpp:148] Setting up ctx_conv4/bn
I0711 20:08:13.008553 25943 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 20:08:13.008555 25943 net.cpp:163] Memory required for data: 899481600
I0711 20:08:13.008559 25943 layer_factory.hpp:77] Creating layer ctx_conv4/relu
I0711 20:08:13.008563 25943 net.cpp:98] Creating Layer ctx_conv4/relu
I0711 20:08:13.008564 25943 net.cpp:439] ctx_conv4/relu <- ctx_conv4/bn
I0711 20:08:13.008566 25943 net.cpp:400] ctx_conv4/relu -> ctx_conv4/bn (in-place)
I0711 20:08:13.008569 25943 net.cpp:148] Setting up ctx_conv4/relu
I0711 20:08:13.008572 25943 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 20:08:13.008574 25943 net.cpp:163] Memory required for data: 906035200
I0711 20:08:13.008575 25943 layer_factory.hpp:77] Creating layer ctx_final
I0711 20:08:13.008579 25943 net.cpp:98] Creating Layer ctx_final
I0711 20:08:13.008580 25943 net.cpp:439] ctx_final <- ctx_conv4/bn
I0711 20:08:13.008582 25943 net.cpp:413] ctx_final -> ctx_final
I0711 20:08:13.008999 25943 net.cpp:148] Setting up ctx_final
I0711 20:08:13.009006 25943 net.cpp:155] Top shape: 4 8 80 80 (204800)
I0711 20:08:13.009007 25943 net.cpp:163] Memory required for data: 906854400
I0711 20:08:13.009011 25943 layer_factory.hpp:77] Creating layer ctx_final/relu
I0711 20:08:13.009012 25943 net.cpp:98] Creating Layer ctx_final/relu
I0711 20:08:13.009014 25943 net.cpp:439] ctx_final/relu <- ctx_final
I0711 20:08:13.009016 25943 net.cpp:400] ctx_final/relu -> ctx_final (in-place)
I0711 20:08:13.009019 25943 net.cpp:148] Setting up ctx_final/relu
I0711 20:08:13.009022 25943 net.cpp:155] Top shape: 4 8 80 80 (204800)
I0711 20:08:13.009023 25943 net.cpp:163] Memory required for data: 907673600
I0711 20:08:13.009026 25943 layer_factory.hpp:77] Creating layer out_deconv_final_up2
I0711 20:08:13.009028 25943 net.cpp:98] Creating Layer out_deconv_final_up2
I0711 20:08:13.009030 25943 net.cpp:439] out_deconv_final_up2 <- ctx_final
I0711 20:08:13.009032 25943 net.cpp:413] out_deconv_final_up2 -> out_deconv_final_up2
I0711 20:08:13.009279 25943 net.cpp:148] Setting up out_deconv_final_up2
I0711 20:08:13.009284 25943 net.cpp:155] Top shape: 4 8 160 160 (819200)
I0711 20:08:13.009285 25943 net.cpp:163] Memory required for data: 910950400
I0711 20:08:13.009287 25943 layer_factory.hpp:77] Creating layer out_deconv_final_up4
I0711 20:08:13.009290 25943 net.cpp:98] Creating Layer out_deconv_final_up4
I0711 20:08:13.009292 25943 net.cpp:439] out_deconv_final_up4 <- out_deconv_final_up2
I0711 20:08:13.009295 25943 net.cpp:413] out_deconv_final_up4 -> out_deconv_final_up4
I0711 20:08:13.009536 25943 net.cpp:148] Setting up out_deconv_final_up4
I0711 20:08:13.009541 25943 net.cpp:155] Top shape: 4 8 320 320 (3276800)
I0711 20:08:13.009542 25943 net.cpp:163] Memory required for data: 924057600
I0711 20:08:13.009546 25943 layer_factory.hpp:77] Creating layer out_deconv_final_up8
I0711 20:08:13.009547 25943 net.cpp:98] Creating Layer out_deconv_final_up8
I0711 20:08:13.009549 25943 net.cpp:439] out_deconv_final_up8 <- out_deconv_final_up4
I0711 20:08:13.009552 25943 net.cpp:413] out_deconv_final_up8 -> out_deconv_final_up8
I0711 20:08:13.009790 25943 net.cpp:148] Setting up out_deconv_final_up8
I0711 20:08:13.009794 25943 net.cpp:155] Top shape: 4 8 640 640 (13107200)
I0711 20:08:13.009796 25943 net.cpp:163] Memory required for data: 976486400
I0711 20:08:13.009799 25943 layer_factory.hpp:77] Creating layer out_deconv_final_up8_out_deconv_final_up8_0_split
I0711 20:08:13.009802 25943 net.cpp:98] Creating Layer out_deconv_final_up8_out_deconv_final_up8_0_split
I0711 20:08:13.009804 25943 net.cpp:439] out_deconv_final_up8_out_deconv_final_up8_0_split <- out_deconv_final_up8
I0711 20:08:13.009806 25943 net.cpp:413] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0711 20:08:13.009814 25943 net.cpp:413] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0711 20:08:13.009817 25943 net.cpp:413] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0711 20:08:13.009876 25943 net.cpp:148] Setting up out_deconv_final_up8_out_deconv_final_up8_0_split
I0711 20:08:13.009881 25943 net.cpp:155] Top shape: 4 8 640 640 (13107200)
I0711 20:08:13.009882 25943 net.cpp:155] Top shape: 4 8 640 640 (13107200)
I0711 20:08:13.009884 25943 net.cpp:155] Top shape: 4 8 640 640 (13107200)
I0711 20:08:13.009886 25943 net.cpp:163] Memory required for data: 1133772800
I0711 20:08:13.009888 25943 layer_factory.hpp:77] Creating layer loss
I0711 20:08:13.009891 25943 net.cpp:98] Creating Layer loss
I0711 20:08:13.009894 25943 net.cpp:439] loss <- out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0711 20:08:13.009896 25943 net.cpp:439] loss <- label_data_1_split_0
I0711 20:08:13.009899 25943 net.cpp:413] loss -> loss
I0711 20:08:13.009903 25943 layer_factory.hpp:77] Creating layer loss
I0711 20:08:13.027357 25943 net.cpp:148] Setting up loss
I0711 20:08:13.027376 25943 net.cpp:155] Top shape: (1)
I0711 20:08:13.027379 25943 net.cpp:158]     with loss weight 1
I0711 20:08:13.027386 25943 net.cpp:163] Memory required for data: 1133772804
I0711 20:08:13.027390 25943 layer_factory.hpp:77] Creating layer accuracy/top1
I0711 20:08:13.027397 25943 net.cpp:98] Creating Layer accuracy/top1
I0711 20:08:13.027400 25943 net.cpp:439] accuracy/top1 <- out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0711 20:08:13.027405 25943 net.cpp:439] accuracy/top1 <- label_data_1_split_1
I0711 20:08:13.027408 25943 net.cpp:413] accuracy/top1 -> accuracy/top1
I0711 20:08:13.027416 25943 net.cpp:148] Setting up accuracy/top1
I0711 20:08:13.027420 25943 net.cpp:155] Top shape: (1)
I0711 20:08:13.027422 25943 net.cpp:163] Memory required for data: 1133772808
I0711 20:08:13.027425 25943 layer_factory.hpp:77] Creating layer accuracy/top5
I0711 20:08:13.027428 25943 net.cpp:98] Creating Layer accuracy/top5
I0711 20:08:13.027431 25943 net.cpp:439] accuracy/top5 <- out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0711 20:08:13.027433 25943 net.cpp:439] accuracy/top5 <- label_data_1_split_2
I0711 20:08:13.027436 25943 net.cpp:413] accuracy/top5 -> accuracy/top5
I0711 20:08:13.027441 25943 net.cpp:148] Setting up accuracy/top5
I0711 20:08:13.027443 25943 net.cpp:155] Top shape: (1)
I0711 20:08:13.027444 25943 net.cpp:163] Memory required for data: 1133772812
I0711 20:08:13.027446 25943 net.cpp:226] accuracy/top5 does not need backward computation.
I0711 20:08:13.027449 25943 net.cpp:226] accuracy/top1 does not need backward computation.
I0711 20:08:13.027451 25943 net.cpp:224] loss needs backward computation.
I0711 20:08:13.027454 25943 net.cpp:224] out_deconv_final_up8_out_deconv_final_up8_0_split needs backward computation.
I0711 20:08:13.027456 25943 net.cpp:224] out_deconv_final_up8 needs backward computation.
I0711 20:08:13.027459 25943 net.cpp:224] out_deconv_final_up4 needs backward computation.
I0711 20:08:13.027462 25943 net.cpp:224] out_deconv_final_up2 needs backward computation.
I0711 20:08:13.027463 25943 net.cpp:224] ctx_final/relu needs backward computation.
I0711 20:08:13.027465 25943 net.cpp:224] ctx_final needs backward computation.
I0711 20:08:13.027467 25943 net.cpp:224] ctx_conv4/relu needs backward computation.
I0711 20:08:13.027469 25943 net.cpp:224] ctx_conv4/bn needs backward computation.
I0711 20:08:13.027472 25943 net.cpp:224] ctx_conv4 needs backward computation.
I0711 20:08:13.027474 25943 net.cpp:224] ctx_conv3/relu needs backward computation.
I0711 20:08:13.027477 25943 net.cpp:224] ctx_conv3/bn needs backward computation.
I0711 20:08:13.027479 25943 net.cpp:224] ctx_conv3 needs backward computation.
I0711 20:08:13.027482 25943 net.cpp:224] ctx_conv2/relu needs backward computation.
I0711 20:08:13.027484 25943 net.cpp:224] ctx_conv2/bn needs backward computation.
I0711 20:08:13.027496 25943 net.cpp:224] ctx_conv2 needs backward computation.
I0711 20:08:13.027498 25943 net.cpp:224] ctx_conv1/relu needs backward computation.
I0711 20:08:13.027500 25943 net.cpp:224] ctx_conv1/bn needs backward computation.
I0711 20:08:13.027503 25943 net.cpp:224] ctx_conv1 needs backward computation.
I0711 20:08:13.027504 25943 net.cpp:224] out3_out5_combined needs backward computation.
I0711 20:08:13.027508 25943 net.cpp:224] out3a/relu needs backward computation.
I0711 20:08:13.027509 25943 net.cpp:224] out3a/bn needs backward computation.
I0711 20:08:13.027511 25943 net.cpp:224] out3a needs backward computation.
I0711 20:08:13.027513 25943 net.cpp:224] out5a_up2 needs backward computation.
I0711 20:08:13.027515 25943 net.cpp:224] out5a/relu needs backward computation.
I0711 20:08:13.027518 25943 net.cpp:224] out5a/bn needs backward computation.
I0711 20:08:13.027519 25943 net.cpp:224] out5a needs backward computation.
I0711 20:08:13.027521 25943 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0711 20:08:13.027523 25943 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0711 20:08:13.027525 25943 net.cpp:224] res5a_branch2b needs backward computation.
I0711 20:08:13.027529 25943 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0711 20:08:13.027529 25943 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0711 20:08:13.027531 25943 net.cpp:224] res5a_branch2a needs backward computation.
I0711 20:08:13.027534 25943 net.cpp:224] pool4 needs backward computation.
I0711 20:08:13.027537 25943 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0711 20:08:13.027540 25943 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0711 20:08:13.027542 25943 net.cpp:224] res4a_branch2b needs backward computation.
I0711 20:08:13.027545 25943 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0711 20:08:13.027547 25943 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0711 20:08:13.027549 25943 net.cpp:224] res4a_branch2a needs backward computation.
I0711 20:08:13.027552 25943 net.cpp:224] pool3 needs backward computation.
I0711 20:08:13.027554 25943 net.cpp:224] res3a_branch2b/bn_res3a_branch2b/relu_0_split needs backward computation.
I0711 20:08:13.027557 25943 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0711 20:08:13.027559 25943 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0711 20:08:13.027562 25943 net.cpp:224] res3a_branch2b needs backward computation.
I0711 20:08:13.027565 25943 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0711 20:08:13.027567 25943 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0711 20:08:13.027570 25943 net.cpp:224] res3a_branch2a needs backward computation.
I0711 20:08:13.027572 25943 net.cpp:224] pool2 needs backward computation.
I0711 20:08:13.027575 25943 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0711 20:08:13.027577 25943 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0711 20:08:13.027580 25943 net.cpp:224] res2a_branch2b needs backward computation.
I0711 20:08:13.027582 25943 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0711 20:08:13.027585 25943 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0711 20:08:13.027587 25943 net.cpp:224] res2a_branch2a needs backward computation.
I0711 20:08:13.027590 25943 net.cpp:224] pool1 needs backward computation.
I0711 20:08:13.027591 25943 net.cpp:224] conv1b/relu needs backward computation.
I0711 20:08:13.027595 25943 net.cpp:224] conv1b/bn needs backward computation.
I0711 20:08:13.027596 25943 net.cpp:224] conv1b needs backward computation.
I0711 20:08:13.027600 25943 net.cpp:224] conv1a/relu needs backward computation.
I0711 20:08:13.027601 25943 net.cpp:224] conv1a/bn needs backward computation.
I0711 20:08:13.027603 25943 net.cpp:224] conv1a needs backward computation.
I0711 20:08:13.027606 25943 net.cpp:226] data/bias does not need backward computation.
I0711 20:08:13.027609 25943 net.cpp:226] label_data_1_split does not need backward computation.
I0711 20:08:13.027614 25943 net.cpp:226] data does not need backward computation.
I0711 20:08:13.027616 25943 net.cpp:268] This network produces output accuracy/top1
I0711 20:08:13.027619 25943 net.cpp:268] This network produces output accuracy/top5
I0711 20:08:13.027621 25943 net.cpp:268] This network produces output loss
I0711 20:08:13.027652 25943 net.cpp:288] Network initialization done.
I0711 20:08:13.027743 25943 solver.cpp:60] Solver scaffolding done.
I0711 20:08:13.035576 25943 caffe.cpp:145] Finetuning from training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/initial/cityscapes5_jsegnet21v2_iter_32000.caffemodel
I0711 20:08:13.067530 25943 data_layer.cpp:78] ReshapePrefetch 5, 3, 640, 640
I0711 20:08:13.067596 25943 data_layer.cpp:83] output data size: 5,3,640,640
I0711 20:08:13.103935 25943 data_layer.cpp:78] ReshapePrefetch 5, 1, 640, 640
I0711 20:08:13.104014 25943 data_layer.cpp:83] output data size: 5,1,640,640
I0711 20:08:13.636772 25943 data_layer.cpp:78] ReshapePrefetch 5, 3, 640, 640
I0711 20:08:13.637950 25943 data_layer.cpp:83] output data size: 5,3,640,640
I0711 20:08:13.739501 25943 data_layer.cpp:78] ReshapePrefetch 5, 1, 640, 640
I0711 20:08:13.739666 25943 data_layer.cpp:83] output data size: 5,1,640,640
I0711 20:08:14.755390 25943 parallel.cpp:334] Starting Optimization
I0711 20:08:14.755450 25943 solver.cpp:409] Solving jsegnet21v2_train
I0711 20:08:14.755453 25943 solver.cpp:410] Learning Rate Policy: multistep
I0711 20:08:15.206455 25943 solver.cpp:290] Iteration 0 (0 iter/s, 0.450967s/100 iter), loss = 0.0168362
I0711 20:08:15.206477 25943 solver.cpp:309]     Train net output #0: loss = 0.0168362 (* 1 = 0.0168362 loss)
I0711 20:08:15.206485 25943 sgd_solver.cpp:106] Iteration 0, lr = 1e-05
I0711 20:08:32.432417 25943 solver.cpp:290] Iteration 100 (5.80536 iter/s, 17.2255s/100 iter), loss = 0.0228012
I0711 20:08:32.432443 25943 solver.cpp:309]     Train net output #0: loss = 0.0228012 (* 1 = 0.0228012 loss)
I0711 20:08:32.432451 25943 sgd_solver.cpp:106] Iteration 100, lr = 1e-05
I0711 20:08:46.582504 26105 blocking_queue.cpp:50] Data layer prefetch queue empty
I0711 20:09:05.486091 25943 solver.cpp:290] Iteration 200 (3.02547 iter/s, 33.0527s/100 iter), loss = 0.0217583
I0711 20:09:05.486119 25943 solver.cpp:309]     Train net output #0: loss = 0.0217583 (* 1 = 0.0217583 loss)
I0711 20:09:05.486129 25943 sgd_solver.cpp:106] Iteration 200, lr = 1e-05
I0711 20:09:31.778726 25943 solver.cpp:290] Iteration 300 (3.80345 iter/s, 26.2919s/100 iter), loss = 0.0295088
I0711 20:09:31.778801 25943 solver.cpp:309]     Train net output #0: loss = 0.0295088 (* 1 = 0.0295088 loss)
I0711 20:09:31.778808 25943 sgd_solver.cpp:106] Iteration 300, lr = 1e-05
I0711 20:09:54.513065 25943 solver.cpp:290] Iteration 400 (4.39877 iter/s, 22.7336s/100 iter), loss = 0.0187619
I0711 20:09:54.513092 25943 solver.cpp:309]     Train net output #0: loss = 0.0187619 (* 1 = 0.0187619 loss)
I0711 20:09:54.513101 25943 sgd_solver.cpp:106] Iteration 400, lr = 1e-05
I0711 20:10:11.678979 25943 solver.cpp:290] Iteration 500 (5.82567 iter/s, 17.1654s/100 iter), loss = 0.0248106
I0711 20:10:11.679062 25943 solver.cpp:309]     Train net output #0: loss = 0.0248106 (* 1 = 0.0248106 loss)
I0711 20:10:11.679074 25943 sgd_solver.cpp:106] Iteration 500, lr = 1e-05
I0711 20:10:28.960304 25943 solver.cpp:290] Iteration 600 (5.78678 iter/s, 17.2808s/100 iter), loss = 0.016377
I0711 20:10:28.960327 25943 solver.cpp:309]     Train net output #0: loss = 0.016377 (* 1 = 0.016377 loss)
I0711 20:10:28.960335 25943 sgd_solver.cpp:106] Iteration 600, lr = 1e-05
I0711 20:10:46.109261 25943 solver.cpp:290] Iteration 700 (5.83143 iter/s, 17.1485s/100 iter), loss = 0.0246561
I0711 20:10:46.109308 25943 solver.cpp:309]     Train net output #0: loss = 0.0246561 (* 1 = 0.0246561 loss)
I0711 20:10:46.109314 25943 sgd_solver.cpp:106] Iteration 700, lr = 1e-05
I0711 20:11:03.247766 25943 solver.cpp:290] Iteration 800 (5.83499 iter/s, 17.138s/100 iter), loss = 0.0199883
I0711 20:11:03.247790 25943 solver.cpp:309]     Train net output #0: loss = 0.0199883 (* 1 = 0.0199883 loss)
I0711 20:11:03.247797 25943 sgd_solver.cpp:106] Iteration 800, lr = 1e-05
I0711 20:11:20.375475 25943 solver.cpp:290] Iteration 900 (5.83866 iter/s, 17.1272s/100 iter), loss = 0.0125264
I0711 20:11:20.375602 25943 solver.cpp:309]     Train net output #0: loss = 0.0125264 (* 1 = 0.0125264 loss)
I0711 20:11:20.375612 25943 sgd_solver.cpp:106] Iteration 900, lr = 1e-05
I0711 20:11:37.540657 25943 solver.cpp:290] Iteration 1000 (5.82595 iter/s, 17.1646s/100 iter), loss = 0.0273993
I0711 20:11:37.540680 25943 solver.cpp:309]     Train net output #0: loss = 0.0273993 (* 1 = 0.0273993 loss)
I0711 20:11:37.540688 25943 sgd_solver.cpp:106] Iteration 1000, lr = 1e-05
I0711 20:11:54.742923 25943 solver.cpp:290] Iteration 1100 (5.81336 iter/s, 17.2018s/100 iter), loss = 0.0271729
I0711 20:11:54.742976 25943 solver.cpp:309]     Train net output #0: loss = 0.0271729 (* 1 = 0.0271729 loss)
I0711 20:11:54.742983 25943 sgd_solver.cpp:106] Iteration 1100, lr = 1e-05
I0711 20:12:11.894206 25943 solver.cpp:290] Iteration 1200 (5.83065 iter/s, 17.1508s/100 iter), loss = 0.0293711
I0711 20:12:11.894229 25943 solver.cpp:309]     Train net output #0: loss = 0.0293711 (* 1 = 0.0293711 loss)
I0711 20:12:11.894237 25943 sgd_solver.cpp:106] Iteration 1200, lr = 1e-05
I0711 20:12:29.084630 25943 solver.cpp:290] Iteration 1300 (5.81736 iter/s, 17.1899s/100 iter), loss = 0.0162563
I0711 20:12:29.084677 25943 solver.cpp:309]     Train net output #0: loss = 0.0162563 (* 1 = 0.0162563 loss)
I0711 20:12:29.084684 25943 sgd_solver.cpp:106] Iteration 1300, lr = 1e-05
I0711 20:12:46.339589 25943 solver.cpp:290] Iteration 1400 (5.79561 iter/s, 17.2544s/100 iter), loss = 0.0423282
I0711 20:12:46.339613 25943 solver.cpp:309]     Train net output #0: loss = 0.0423282 (* 1 = 0.0423282 loss)
I0711 20:12:46.339620 25943 sgd_solver.cpp:106] Iteration 1400, lr = 1e-05
I0711 20:13:03.597748 25943 solver.cpp:290] Iteration 1500 (5.79453 iter/s, 17.2577s/100 iter), loss = 0.0172704
I0711 20:13:03.597838 25943 solver.cpp:309]     Train net output #0: loss = 0.0172704 (* 1 = 0.0172704 loss)
I0711 20:13:03.597848 25943 sgd_solver.cpp:106] Iteration 1500, lr = 1e-05
I0711 20:13:20.801759 25943 solver.cpp:290] Iteration 1600 (5.81279 iter/s, 17.2034s/100 iter), loss = 0.0226167
I0711 20:13:20.801782 25943 solver.cpp:309]     Train net output #0: loss = 0.0226167 (* 1 = 0.0226167 loss)
I0711 20:13:20.801790 25943 sgd_solver.cpp:106] Iteration 1600, lr = 1e-05
I0711 20:13:37.971395 25943 solver.cpp:290] Iteration 1700 (5.8244 iter/s, 17.1691s/100 iter), loss = 0.0162588
I0711 20:13:37.971489 25943 solver.cpp:309]     Train net output #0: loss = 0.0162588 (* 1 = 0.0162588 loss)
I0711 20:13:37.971498 25943 sgd_solver.cpp:106] Iteration 1700, lr = 1e-05
I0711 20:13:55.079634 25943 solver.cpp:290] Iteration 1800 (5.84533 iter/s, 17.1077s/100 iter), loss = 0.0327368
I0711 20:13:55.079658 25943 solver.cpp:309]     Train net output #0: loss = 0.0327368 (* 1 = 0.0327368 loss)
I0711 20:13:55.079665 25943 sgd_solver.cpp:106] Iteration 1800, lr = 1e-05
I0711 20:14:12.228041 25943 solver.cpp:290] Iteration 1900 (5.83161 iter/s, 17.1479s/100 iter), loss = 0.0276456
I0711 20:14:12.228458 25943 solver.cpp:309]     Train net output #0: loss = 0.0276455 (* 1 = 0.0276455 loss)
I0711 20:14:12.228469 25943 sgd_solver.cpp:106] Iteration 1900, lr = 1e-05
I0711 20:14:29.334561 25943 solver.cpp:467] Iteration 2000, Testing net (#0)
I0711 20:15:15.848827 25943 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.954646
I0711 20:15:15.848927 25943 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999677
I0711 20:15:15.848934 25943 solver.cpp:540]     Test net output #2: loss = 0.145338 (* 1 = 0.145338 loss)
I0711 20:15:16.036445 25943 solver.cpp:290] Iteration 2000 (1.56724 iter/s, 63.8063s/100 iter), loss = 0.0139777
I0711 20:15:16.036470 25943 solver.cpp:309]     Train net output #0: loss = 0.0139777 (* 1 = 0.0139777 loss)
I0711 20:15:16.036478 25943 sgd_solver.cpp:106] Iteration 2000, lr = 1e-05
I0711 20:15:31.814342 26061 blocking_queue.cpp:50] Waiting for data
I0711 20:15:44.022194 25943 solver.cpp:290] Iteration 2100 (3.57335 iter/s, 27.985s/100 iter), loss = 0.0228779
I0711 20:15:44.022229 25943 solver.cpp:309]     Train net output #0: loss = 0.0228779 (* 1 = 0.0228779 loss)
I0711 20:15:44.022238 25943 sgd_solver.cpp:106] Iteration 2100, lr = 1e-05
I0711 20:16:19.163923 26061 blocking_queue.cpp:50] Waiting for data
I0711 20:16:51.417625 25943 solver.cpp:290] Iteration 2200 (1.48382 iter/s, 67.3935s/100 iter), loss = 0.0235983
I0711 20:16:51.417732 25943 solver.cpp:309]     Train net output #0: loss = 0.0235983 (* 1 = 0.0235983 loss)
I0711 20:16:51.417739 25943 sgd_solver.cpp:106] Iteration 2200, lr = 1e-05
I0711 20:17:09.192875 25943 solver.cpp:290] Iteration 2300 (5.62599 iter/s, 17.7747s/100 iter), loss = 0.0269009
I0711 20:17:09.192903 25943 solver.cpp:309]     Train net output #0: loss = 0.0269009 (* 1 = 0.0269009 loss)
I0711 20:17:09.192911 25943 sgd_solver.cpp:106] Iteration 2300, lr = 1e-05
I0711 20:17:26.193935 25943 solver.cpp:290] Iteration 2400 (5.88216 iter/s, 17.0006s/100 iter), loss = 0.0172797
I0711 20:17:26.194012 25943 solver.cpp:309]     Train net output #0: loss = 0.0172797 (* 1 = 0.0172797 loss)
I0711 20:17:26.194020 25943 sgd_solver.cpp:106] Iteration 2400, lr = 1e-05
I0711 20:17:43.161309 25943 solver.cpp:290] Iteration 2500 (5.89385 iter/s, 16.9668s/100 iter), loss = 0.0167326
I0711 20:17:43.161336 25943 solver.cpp:309]     Train net output #0: loss = 0.0167326 (* 1 = 0.0167326 loss)
I0711 20:17:43.161345 25943 sgd_solver.cpp:106] Iteration 2500, lr = 1e-05
I0711 20:18:00.285012 25943 solver.cpp:290] Iteration 2600 (5.84003 iter/s, 17.1232s/100 iter), loss = 0.0283394
I0711 20:18:00.285092 25943 solver.cpp:309]     Train net output #0: loss = 0.0283394 (* 1 = 0.0283394 loss)
I0711 20:18:00.285099 25943 sgd_solver.cpp:106] Iteration 2600, lr = 1e-05
I0711 20:18:17.217105 25943 solver.cpp:290] Iteration 2700 (5.90613 iter/s, 16.9315s/100 iter), loss = 0.013547
I0711 20:18:17.217129 25943 solver.cpp:309]     Train net output #0: loss = 0.013547 (* 1 = 0.013547 loss)
I0711 20:18:17.217136 25943 sgd_solver.cpp:106] Iteration 2700, lr = 1e-05
I0711 20:18:34.297808 25943 solver.cpp:290] Iteration 2800 (5.85473 iter/s, 17.0802s/100 iter), loss = 0.0404717
I0711 20:18:34.297895 25943 solver.cpp:309]     Train net output #0: loss = 0.0404717 (* 1 = 0.0404717 loss)
I0711 20:18:34.297906 25943 sgd_solver.cpp:106] Iteration 2800, lr = 1e-05
I0711 20:18:51.259418 25943 solver.cpp:290] Iteration 2900 (5.89586 iter/s, 16.9611s/100 iter), loss = 0.0251611
I0711 20:18:51.259447 25943 solver.cpp:309]     Train net output #0: loss = 0.0251611 (* 1 = 0.0251611 loss)
I0711 20:18:51.259457 25943 sgd_solver.cpp:106] Iteration 2900, lr = 1e-05
I0711 20:19:08.177590 25943 solver.cpp:290] Iteration 3000 (5.91098 iter/s, 16.9177s/100 iter), loss = 0.0372716
I0711 20:19:08.177629 25943 solver.cpp:309]     Train net output #0: loss = 0.0372716 (* 1 = 0.0372716 loss)
I0711 20:19:08.177637 25943 sgd_solver.cpp:106] Iteration 3000, lr = 1e-05
I0711 20:19:25.251134 25943 solver.cpp:290] Iteration 3100 (5.85719 iter/s, 17.073s/100 iter), loss = 0.0252866
I0711 20:19:25.251158 25943 solver.cpp:309]     Train net output #0: loss = 0.0252866 (* 1 = 0.0252866 loss)
I0711 20:19:25.251165 25943 sgd_solver.cpp:106] Iteration 3100, lr = 1e-05
I0711 20:19:42.238703 25943 solver.cpp:290] Iteration 3200 (5.88683 iter/s, 16.9871s/100 iter), loss = 0.0307177
I0711 20:19:42.238775 25943 solver.cpp:309]     Train net output #0: loss = 0.0307176 (* 1 = 0.0307176 loss)
I0711 20:19:42.238785 25943 sgd_solver.cpp:106] Iteration 3200, lr = 1e-05
I0711 20:19:59.269680 25943 solver.cpp:290] Iteration 3300 (5.87184 iter/s, 17.0304s/100 iter), loss = 0.0221793
I0711 20:19:59.269703 25943 solver.cpp:309]     Train net output #0: loss = 0.0221793 (* 1 = 0.0221793 loss)
I0711 20:19:59.269711 25943 sgd_solver.cpp:106] Iteration 3300, lr = 1e-05
I0711 20:20:16.243834 25943 solver.cpp:290] Iteration 3400 (5.89148 iter/s, 16.9737s/100 iter), loss = 0.019686
I0711 20:20:16.243921 25943 solver.cpp:309]     Train net output #0: loss = 0.019686 (* 1 = 0.019686 loss)
I0711 20:20:16.243932 25943 sgd_solver.cpp:106] Iteration 3400, lr = 1e-05
I0711 20:20:33.349153 25943 solver.cpp:290] Iteration 3500 (5.84632 iter/s, 17.1048s/100 iter), loss = 0.0301555
I0711 20:20:33.349177 25943 solver.cpp:309]     Train net output #0: loss = 0.0301555 (* 1 = 0.0301555 loss)
I0711 20:20:33.349184 25943 sgd_solver.cpp:106] Iteration 3500, lr = 1e-05
I0711 20:20:50.405519 25943 solver.cpp:290] Iteration 3600 (5.86308 iter/s, 17.0559s/100 iter), loss = 0.0178519
I0711 20:20:50.405611 25943 solver.cpp:309]     Train net output #0: loss = 0.0178519 (* 1 = 0.0178519 loss)
I0711 20:20:50.405622 25943 sgd_solver.cpp:106] Iteration 3600, lr = 1e-05
I0711 20:21:07.323068 25943 solver.cpp:290] Iteration 3700 (5.91121 iter/s, 16.917s/100 iter), loss = 0.0162452
I0711 20:21:07.323096 25943 solver.cpp:309]     Train net output #0: loss = 0.0162451 (* 1 = 0.0162451 loss)
I0711 20:21:07.323102 25943 sgd_solver.cpp:106] Iteration 3700, lr = 1e-05
I0711 20:21:24.394871 25943 solver.cpp:290] Iteration 3800 (5.85778 iter/s, 17.0713s/100 iter), loss = 0.0253355
I0711 20:21:24.394933 25943 solver.cpp:309]     Train net output #0: loss = 0.0253355 (* 1 = 0.0253355 loss)
I0711 20:21:24.394943 25943 sgd_solver.cpp:106] Iteration 3800, lr = 1e-05
I0711 20:21:41.360647 25943 solver.cpp:290] Iteration 3900 (5.8944 iter/s, 16.9653s/100 iter), loss = 0.0282103
I0711 20:21:41.360676 25943 solver.cpp:309]     Train net output #0: loss = 0.0282103 (* 1 = 0.0282103 loss)
I0711 20:21:41.360684 25943 sgd_solver.cpp:106] Iteration 3900, lr = 1e-05
I0711 20:21:58.341869 25943 solver.cpp:467] Iteration 4000, Testing net (#0)
I0711 20:22:45.104122 25943 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.955371
I0711 20:22:45.104204 25943 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999722
I0711 20:22:45.104212 25943 solver.cpp:540]     Test net output #2: loss = 0.138928 (* 1 = 0.138928 loss)
I0711 20:22:45.293092 25943 solver.cpp:290] Iteration 4000 (1.56419 iter/s, 63.9307s/100 iter), loss = 0.0181555
I0711 20:22:45.293118 25943 solver.cpp:309]     Train net output #0: loss = 0.0181555 (* 1 = 0.0181555 loss)
I0711 20:22:45.293125 25943 sgd_solver.cpp:106] Iteration 4000, lr = 1e-05
I0711 20:23:22.401427 26061 blocking_queue.cpp:50] Waiting for data
I0711 20:23:41.246528 25943 solver.cpp:290] Iteration 4100 (1.78725 iter/s, 55.9519s/100 iter), loss = 0.0489419
I0711 20:23:41.246556 25943 solver.cpp:309]     Train net output #0: loss = 0.0489419 (* 1 = 0.0489419 loss)
I0711 20:23:41.246562 25943 sgd_solver.cpp:106] Iteration 4100, lr = 1e-05
I0711 20:23:56.382218 26080 blocking_queue.cpp:50] Waiting for data
I0711 20:24:49.433560 25943 solver.cpp:290] Iteration 4200 (1.4666 iter/s, 68.1851s/100 iter), loss = 0.0226377
I0711 20:24:49.433670 25943 solver.cpp:309]     Train net output #0: loss = 0.0226377 (* 1 = 0.0226377 loss)
I0711 20:24:49.433681 25943 sgd_solver.cpp:106] Iteration 4200, lr = 1e-05
I0711 20:25:06.369508 25943 solver.cpp:290] Iteration 4300 (5.9048 iter/s, 16.9354s/100 iter), loss = 0.0327388
I0711 20:25:06.369535 25943 solver.cpp:309]     Train net output #0: loss = 0.0327388 (* 1 = 0.0327388 loss)
I0711 20:25:06.369544 25943 sgd_solver.cpp:106] Iteration 4300, lr = 1e-05
I0711 20:25:23.344054 25943 solver.cpp:290] Iteration 4400 (5.89135 iter/s, 16.9741s/100 iter), loss = 0.0308824
I0711 20:25:23.344112 25943 solver.cpp:309]     Train net output #0: loss = 0.0308823 (* 1 = 0.0308823 loss)
I0711 20:25:23.344120 25943 sgd_solver.cpp:106] Iteration 4400, lr = 1e-05
I0711 20:25:40.355330 25943 solver.cpp:290] Iteration 4500 (5.87864 iter/s, 17.0107s/100 iter), loss = 0.0182774
I0711 20:25:40.355358 25943 solver.cpp:309]     Train net output #0: loss = 0.0182774 (* 1 = 0.0182774 loss)
I0711 20:25:40.355367 25943 sgd_solver.cpp:106] Iteration 4500, lr = 1e-05
I0711 20:25:57.378783 25943 solver.cpp:290] Iteration 4600 (5.87442 iter/s, 17.023s/100 iter), loss = 0.0211551
I0711 20:25:57.378916 25943 solver.cpp:309]     Train net output #0: loss = 0.0211551 (* 1 = 0.0211551 loss)
I0711 20:25:57.378926 25943 sgd_solver.cpp:106] Iteration 4600, lr = 1e-05
I0711 20:26:14.349987 25943 solver.cpp:290] Iteration 4700 (5.89254 iter/s, 16.9706s/100 iter), loss = 0.0281167
I0711 20:26:14.350010 25943 solver.cpp:309]     Train net output #0: loss = 0.0281166 (* 1 = 0.0281166 loss)
I0711 20:26:14.350018 25943 sgd_solver.cpp:106] Iteration 4700, lr = 1e-05
I0711 20:26:31.381795 25943 solver.cpp:290] Iteration 4800 (5.87154 iter/s, 17.0313s/100 iter), loss = 0.0404244
I0711 20:26:31.381870 25943 solver.cpp:309]     Train net output #0: loss = 0.0404243 (* 1 = 0.0404243 loss)
I0711 20:26:31.381880 25943 sgd_solver.cpp:106] Iteration 4800, lr = 1e-05
I0711 20:26:48.553793 25943 solver.cpp:290] Iteration 4900 (5.82362 iter/s, 17.1715s/100 iter), loss = 0.0244342
I0711 20:26:48.553817 25943 solver.cpp:309]     Train net output #0: loss = 0.0244342 (* 1 = 0.0244342 loss)
I0711 20:26:48.553823 25943 sgd_solver.cpp:106] Iteration 4900, lr = 1e-05
I0711 20:27:05.611884 25943 solver.cpp:290] Iteration 5000 (5.86249 iter/s, 17.0576s/100 iter), loss = 0.0123206
I0711 20:27:05.611937 25943 solver.cpp:309]     Train net output #0: loss = 0.0123206 (* 1 = 0.0123206 loss)
I0711 20:27:05.611945 25943 sgd_solver.cpp:106] Iteration 5000, lr = 1e-05
I0711 20:27:22.815099 25943 solver.cpp:290] Iteration 5100 (5.81304 iter/s, 17.2027s/100 iter), loss = 0.0145455
I0711 20:27:22.815122 25943 solver.cpp:309]     Train net output #0: loss = 0.0145454 (* 1 = 0.0145454 loss)
I0711 20:27:22.815129 25943 sgd_solver.cpp:106] Iteration 5100, lr = 1e-05
I0711 20:27:39.819196 25943 solver.cpp:290] Iteration 5200 (5.8811 iter/s, 17.0036s/100 iter), loss = 0.0150289
I0711 20:27:39.819249 25943 solver.cpp:309]     Train net output #0: loss = 0.0150288 (* 1 = 0.0150288 loss)
I0711 20:27:39.819257 25943 sgd_solver.cpp:106] Iteration 5200, lr = 1e-05
I0711 20:27:56.853770 25943 solver.cpp:290] Iteration 5300 (5.87059 iter/s, 17.0341s/100 iter), loss = 0.0201549
I0711 20:27:56.853793 25943 solver.cpp:309]     Train net output #0: loss = 0.0201549 (* 1 = 0.0201549 loss)
I0711 20:27:56.853801 25943 sgd_solver.cpp:106] Iteration 5300, lr = 1e-05
I0711 20:28:13.738481 25943 solver.cpp:290] Iteration 5400 (5.92269 iter/s, 16.8842s/100 iter), loss = 0.0134872
I0711 20:28:13.738570 25943 solver.cpp:309]     Train net output #0: loss = 0.0134872 (* 1 = 0.0134872 loss)
I0711 20:28:13.738580 25943 sgd_solver.cpp:106] Iteration 5400, lr = 1e-05
I0711 20:28:30.877629 25943 solver.cpp:290] Iteration 5500 (5.83478 iter/s, 17.1386s/100 iter), loss = 0.0224444
I0711 20:28:30.877655 25943 solver.cpp:309]     Train net output #0: loss = 0.0224443 (* 1 = 0.0224443 loss)
I0711 20:28:30.877661 25943 sgd_solver.cpp:106] Iteration 5500, lr = 1e-05
I0711 20:28:47.712836 25943 solver.cpp:290] Iteration 5600 (5.9401 iter/s, 16.8347s/100 iter), loss = 0.0258914
I0711 20:28:47.712910 25943 solver.cpp:309]     Train net output #0: loss = 0.0258913 (* 1 = 0.0258913 loss)
I0711 20:28:47.712918 25943 sgd_solver.cpp:106] Iteration 5600, lr = 1e-05
I0711 20:29:04.752789 25943 solver.cpp:290] Iteration 5700 (5.86875 iter/s, 17.0394s/100 iter), loss = 0.0215494
I0711 20:29:04.752812 25943 solver.cpp:309]     Train net output #0: loss = 0.0215493 (* 1 = 0.0215493 loss)
I0711 20:29:04.752823 25943 sgd_solver.cpp:106] Iteration 5700, lr = 1e-05
I0711 20:29:21.863195 25943 solver.cpp:290] Iteration 5800 (5.84456 iter/s, 17.1099s/100 iter), loss = 0.0419161
I0711 20:29:21.863296 25943 solver.cpp:309]     Train net output #0: loss = 0.041916 (* 1 = 0.041916 loss)
I0711 20:29:21.863308 25943 sgd_solver.cpp:106] Iteration 5800, lr = 1e-05
I0711 20:29:38.841120 25943 solver.cpp:290] Iteration 5900 (5.8902 iter/s, 16.9774s/100 iter), loss = 0.0279855
I0711 20:29:38.841146 25943 solver.cpp:309]     Train net output #0: loss = 0.0279854 (* 1 = 0.0279854 loss)
I0711 20:29:38.841152 25943 sgd_solver.cpp:106] Iteration 5900, lr = 1e-05
I0711 20:29:55.766366 25943 solver.cpp:467] Iteration 6000, Testing net (#0)
I0711 20:30:43.436558 25943 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.953748
I0711 20:30:43.436776 25943 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999672
I0711 20:30:43.436784 25943 solver.cpp:540]     Test net output #2: loss = 0.152524 (* 1 = 0.152524 loss)
I0711 20:30:43.627130 25943 solver.cpp:290] Iteration 6000 (1.54359 iter/s, 64.7842s/100 iter), loss = 0.0268469
I0711 20:30:43.627156 25943 solver.cpp:309]     Train net output #0: loss = 0.0268469 (* 1 = 0.0268469 loss)
I0711 20:30:43.627162 25943 sgd_solver.cpp:106] Iteration 6000, lr = 1e-05
I0711 20:31:00.362442 25943 solver.cpp:290] Iteration 6100 (5.97556 iter/s, 16.7348s/100 iter), loss = 0.0241744
I0711 20:31:00.362468 25943 solver.cpp:309]     Train net output #0: loss = 0.0241743 (* 1 = 0.0241743 loss)
I0711 20:31:00.362476 25943 sgd_solver.cpp:106] Iteration 6100, lr = 1e-05
I0711 20:31:23.528726 26022 blocking_queue.cpp:50] Waiting for data
I0711 20:31:25.911176 26106 blocking_queue.cpp:50] Data layer prefetch queue empty
I0711 20:31:40.596856 25943 solver.cpp:290] Iteration 6200 (2.4855 iter/s, 40.2333s/100 iter), loss = 0.0269419
I0711 20:31:40.596961 25943 solver.cpp:309]     Train net output #0: loss = 0.0269418 (* 1 = 0.0269418 loss)
I0711 20:31:40.597002 25943 sgd_solver.cpp:106] Iteration 6200, lr = 1e-05
I0711 20:31:58.739675 25943 solver.cpp:290] Iteration 6300 (5.512 iter/s, 18.1422s/100 iter), loss = 0.0258023
I0711 20:31:58.739725 25943 solver.cpp:309]     Train net output #0: loss = 0.0258022 (* 1 = 0.0258022 loss)
I0711 20:31:58.739737 25943 sgd_solver.cpp:106] Iteration 6300, lr = 1e-05
I0711 20:32:15.766396 25943 solver.cpp:290] Iteration 6400 (5.8733 iter/s, 17.0262s/100 iter), loss = 0.0290447
I0711 20:32:15.766420 25943 solver.cpp:309]     Train net output #0: loss = 0.0290447 (* 1 = 0.0290447 loss)
I0711 20:32:15.766427 25943 sgd_solver.cpp:106] Iteration 6400, lr = 1e-05
I0711 20:32:32.675943 25943 solver.cpp:290] Iteration 6500 (5.91399 iter/s, 16.9091s/100 iter), loss = 0.0342241
I0711 20:32:32.675992 25943 solver.cpp:309]     Train net output #0: loss = 0.034224 (* 1 = 0.034224 loss)
I0711 20:32:32.675999 25943 sgd_solver.cpp:106] Iteration 6500, lr = 1e-05
I0711 20:32:49.586468 25943 solver.cpp:290] Iteration 6600 (5.91365 iter/s, 16.91s/100 iter), loss = 0.0179108
I0711 20:32:49.586493 25943 solver.cpp:309]     Train net output #0: loss = 0.0179108 (* 1 = 0.0179108 loss)
I0711 20:32:49.586499 25943 sgd_solver.cpp:106] Iteration 6600, lr = 1e-05
I0711 20:33:06.643301 25943 solver.cpp:290] Iteration 6700 (5.86292 iter/s, 17.0563s/100 iter), loss = 0.0154283
I0711 20:33:06.643357 25943 solver.cpp:309]     Train net output #0: loss = 0.0154282 (* 1 = 0.0154282 loss)
I0711 20:33:06.643365 25943 sgd_solver.cpp:106] Iteration 6700, lr = 1e-05
I0711 20:33:23.681645 25943 solver.cpp:290] Iteration 6800 (5.8693 iter/s, 17.0378s/100 iter), loss = 0.0218045
I0711 20:33:23.681668 25943 solver.cpp:309]     Train net output #0: loss = 0.0218044 (* 1 = 0.0218044 loss)
I0711 20:33:23.681675 25943 sgd_solver.cpp:106] Iteration 6800, lr = 1e-05
I0711 20:33:40.835078 25943 solver.cpp:290] Iteration 6900 (5.8299 iter/s, 17.1529s/100 iter), loss = 0.0257398
I0711 20:33:40.835144 25943 solver.cpp:309]     Train net output #0: loss = 0.0257398 (* 1 = 0.0257398 loss)
I0711 20:33:40.835155 25943 sgd_solver.cpp:106] Iteration 6900, lr = 1e-05
I0711 20:33:57.872064 25943 solver.cpp:290] Iteration 7000 (5.86976 iter/s, 17.0365s/100 iter), loss = 0.018077
I0711 20:33:57.872086 25943 solver.cpp:309]     Train net output #0: loss = 0.0180769 (* 1 = 0.0180769 loss)
I0711 20:33:57.872092 25943 sgd_solver.cpp:106] Iteration 7000, lr = 1e-05
I0711 20:34:14.982858 25943 solver.cpp:290] Iteration 7100 (5.84443 iter/s, 17.1103s/100 iter), loss = 0.0449575
I0711 20:34:14.982970 25943 solver.cpp:309]     Train net output #0: loss = 0.0449574 (* 1 = 0.0449574 loss)
I0711 20:34:14.982980 25943 sgd_solver.cpp:106] Iteration 7100, lr = 1e-05
I0711 20:34:32.027902 25943 solver.cpp:290] Iteration 7200 (5.867 iter/s, 17.0445s/100 iter), loss = 0.0160386
I0711 20:34:32.027928 25943 solver.cpp:309]     Train net output #0: loss = 0.0160386 (* 1 = 0.0160386 loss)
I0711 20:34:32.027936 25943 sgd_solver.cpp:106] Iteration 7200, lr = 1e-05
I0711 20:34:49.151813 25943 solver.cpp:290] Iteration 7300 (5.83996 iter/s, 17.1234s/100 iter), loss = 0.0323531
I0711 20:34:49.151943 25943 solver.cpp:309]     Train net output #0: loss = 0.032353 (* 1 = 0.032353 loss)
I0711 20:34:49.151954 25943 sgd_solver.cpp:106] Iteration 7300, lr = 1e-05
I0711 20:35:06.062863 25943 solver.cpp:290] Iteration 7400 (5.9135 iter/s, 16.9105s/100 iter), loss = 0.0325191
I0711 20:35:06.062889 25943 solver.cpp:309]     Train net output #0: loss = 0.032519 (* 1 = 0.032519 loss)
I0711 20:35:06.062898 25943 sgd_solver.cpp:106] Iteration 7400, lr = 1e-05
I0711 20:35:23.019309 25943 solver.cpp:290] Iteration 7500 (5.89763 iter/s, 16.956s/100 iter), loss = 0.0353778
I0711 20:35:23.019382 25943 solver.cpp:309]     Train net output #0: loss = 0.0353777 (* 1 = 0.0353777 loss)
I0711 20:35:23.019389 25943 sgd_solver.cpp:106] Iteration 7500, lr = 1e-05
I0711 20:35:39.918733 25943 solver.cpp:290] Iteration 7600 (5.91755 iter/s, 16.8989s/100 iter), loss = 0.021995
I0711 20:35:39.918756 25943 solver.cpp:309]     Train net output #0: loss = 0.021995 (* 1 = 0.021995 loss)
I0711 20:35:39.918763 25943 sgd_solver.cpp:106] Iteration 7600, lr = 1e-05
I0711 20:35:57.068563 25943 solver.cpp:290] Iteration 7700 (5.83113 iter/s, 17.1493s/100 iter), loss = 0.0245847
I0711 20:35:57.068640 25943 solver.cpp:309]     Train net output #0: loss = 0.0245847 (* 1 = 0.0245847 loss)
I0711 20:35:57.068647 25943 sgd_solver.cpp:106] Iteration 7700, lr = 1e-05
I0711 20:36:14.286115 25943 solver.cpp:290] Iteration 7800 (5.80821 iter/s, 17.217s/100 iter), loss = 0.0166087
I0711 20:36:14.286144 25943 solver.cpp:309]     Train net output #0: loss = 0.0166087 (* 1 = 0.0166087 loss)
I0711 20:36:14.286152 25943 sgd_solver.cpp:106] Iteration 7800, lr = 1e-05
I0711 20:36:31.441951 25943 solver.cpp:290] Iteration 7900 (5.82909 iter/s, 17.1553s/100 iter), loss = 0.0361252
I0711 20:36:31.442005 25943 solver.cpp:309]     Train net output #0: loss = 0.0361251 (* 1 = 0.0361251 loss)
I0711 20:36:31.442013 25943 sgd_solver.cpp:106] Iteration 7900, lr = 1e-05
I0711 20:36:48.332142 25943 solver.cpp:467] Iteration 8000, Testing net (#0)
I0711 20:37:35.173249 25943 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.954554
I0711 20:37:35.173328 25943 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999809
I0711 20:37:35.173336 25943 solver.cpp:540]     Test net output #2: loss = 0.144035 (* 1 = 0.144035 loss)
I0711 20:37:35.357697 25943 solver.cpp:290] Iteration 8000 (1.5646 iter/s, 63.914s/100 iter), loss = 0.017464
I0711 20:37:35.357724 25943 solver.cpp:309]     Train net output #0: loss = 0.017464 (* 1 = 0.017464 loss)
I0711 20:37:35.357734 25943 sgd_solver.cpp:106] Iteration 8000, lr = 1e-05
I0711 20:37:53.251627 25943 solver.cpp:290] Iteration 8100 (5.58865 iter/s, 17.8934s/100 iter), loss = 0.0271712
I0711 20:37:53.251652 25943 solver.cpp:309]     Train net output #0: loss = 0.0271711 (* 1 = 0.0271711 loss)
I0711 20:37:53.251659 25943 sgd_solver.cpp:106] Iteration 8100, lr = 1e-05
I0711 20:38:27.107126 25943 solver.cpp:290] Iteration 8200 (2.95381 iter/s, 33.8545s/100 iter), loss = 0.0220642
I0711 20:38:27.107214 25943 solver.cpp:309]     Train net output #0: loss = 0.0220641 (* 1 = 0.0220641 loss)
I0711 20:38:27.107226 25943 sgd_solver.cpp:106] Iteration 8200, lr = 1e-05
I0711 20:38:44.216553 25943 solver.cpp:290] Iteration 8300 (5.84492 iter/s, 17.1089s/100 iter), loss = 0.0339385
I0711 20:38:44.216576 25943 solver.cpp:309]     Train net output #0: loss = 0.0339385 (* 1 = 0.0339385 loss)
I0711 20:38:44.216583 25943 sgd_solver.cpp:106] Iteration 8300, lr = 1e-05
I0711 20:39:01.188951 25943 solver.cpp:290] Iteration 8400 (5.89209 iter/s, 16.9719s/100 iter), loss = 0.0183916
I0711 20:39:01.188998 25943 solver.cpp:309]     Train net output #0: loss = 0.0183916 (* 1 = 0.0183916 loss)
I0711 20:39:01.189005 25943 sgd_solver.cpp:106] Iteration 8400, lr = 1e-05
I0711 20:39:18.201339 25943 solver.cpp:290] Iteration 8500 (5.87825 iter/s, 17.0119s/100 iter), loss = 0.0418022
I0711 20:39:18.201361 25943 solver.cpp:309]     Train net output #0: loss = 0.0418021 (* 1 = 0.0418021 loss)
I0711 20:39:18.201369 25943 sgd_solver.cpp:106] Iteration 8500, lr = 1e-05
I0711 20:39:35.411183 25943 solver.cpp:290] Iteration 8600 (5.8108 iter/s, 17.2093s/100 iter), loss = 0.0348546
I0711 20:39:35.411295 25943 solver.cpp:309]     Train net output #0: loss = 0.0348546 (* 1 = 0.0348546 loss)
I0711 20:39:35.411306 25943 sgd_solver.cpp:106] Iteration 8600, lr = 1e-05
I0711 20:39:52.382225 25943 solver.cpp:290] Iteration 8700 (5.89259 iter/s, 16.9705s/100 iter), loss = 0.0253528
I0711 20:39:52.382247 25943 solver.cpp:309]     Train net output #0: loss = 0.0253527 (* 1 = 0.0253527 loss)
I0711 20:39:52.382254 25943 sgd_solver.cpp:106] Iteration 8700, lr = 1e-05
I0711 20:40:09.457489 25943 solver.cpp:290] Iteration 8800 (5.85659 iter/s, 17.0748s/100 iter), loss = 0.0250733
I0711 20:40:09.457542 25943 solver.cpp:309]     Train net output #0: loss = 0.0250732 (* 1 = 0.0250732 loss)
I0711 20:40:09.457552 25943 sgd_solver.cpp:106] Iteration 8800, lr = 1e-05
I0711 20:40:26.475515 25943 solver.cpp:290] Iteration 8900 (5.8763 iter/s, 17.0175s/100 iter), loss = 0.0237738
I0711 20:40:26.475541 25943 solver.cpp:309]     Train net output #0: loss = 0.0237737 (* 1 = 0.0237737 loss)
I0711 20:40:26.475551 25943 sgd_solver.cpp:106] Iteration 8900, lr = 1e-05
I0711 20:40:43.406347 25943 solver.cpp:290] Iteration 9000 (5.90655 iter/s, 16.9303s/100 iter), loss = 0.0258604
I0711 20:40:43.406396 25943 solver.cpp:309]     Train net output #0: loss = 0.0258604 (* 1 = 0.0258604 loss)
I0711 20:40:43.406404 25943 sgd_solver.cpp:106] Iteration 9000, lr = 1e-05
I0711 20:41:00.372617 25943 solver.cpp:290] Iteration 9100 (5.89423 iter/s, 16.9658s/100 iter), loss = 0.0429117
I0711 20:41:00.372651 25943 solver.cpp:309]     Train net output #0: loss = 0.0429116 (* 1 = 0.0429116 loss)
I0711 20:41:00.372663 25943 sgd_solver.cpp:106] Iteration 9100, lr = 1e-05
I0711 20:41:17.446990 25943 solver.cpp:290] Iteration 9200 (5.8569 iter/s, 17.0739s/100 iter), loss = 0.0309905
I0711 20:41:17.447103 25943 solver.cpp:309]     Train net output #0: loss = 0.0309904 (* 1 = 0.0309904 loss)
I0711 20:41:17.447113 25943 sgd_solver.cpp:106] Iteration 9200, lr = 1e-05
I0711 20:41:34.467952 25943 solver.cpp:290] Iteration 9300 (5.87531 iter/s, 17.0204s/100 iter), loss = 0.0323133
I0711 20:41:34.467978 25943 solver.cpp:309]     Train net output #0: loss = 0.0323132 (* 1 = 0.0323132 loss)
I0711 20:41:34.467986 25943 sgd_solver.cpp:106] Iteration 9300, lr = 1e-05
I0711 20:41:51.434613 25943 solver.cpp:290] Iteration 9400 (5.89408 iter/s, 16.9662s/100 iter), loss = 0.019191
I0711 20:41:51.434660 25943 solver.cpp:309]     Train net output #0: loss = 0.0191909 (* 1 = 0.0191909 loss)
I0711 20:41:51.434669 25943 sgd_solver.cpp:106] Iteration 9400, lr = 1e-05
I0711 20:42:08.490960 25943 solver.cpp:290] Iteration 9500 (5.8631 iter/s, 17.0558s/100 iter), loss = 0.0303161
I0711 20:42:08.490983 25943 solver.cpp:309]     Train net output #0: loss = 0.030316 (* 1 = 0.030316 loss)
I0711 20:42:08.490989 25943 sgd_solver.cpp:106] Iteration 9500, lr = 1e-05
I0711 20:42:25.623143 25943 solver.cpp:290] Iteration 9600 (5.83713 iter/s, 17.1317s/100 iter), loss = 0.0267962
I0711 20:42:25.623198 25943 solver.cpp:309]     Train net output #0: loss = 0.0267961 (* 1 = 0.0267961 loss)
I0711 20:42:25.623206 25943 sgd_solver.cpp:106] Iteration 9600, lr = 1e-05
I0711 20:42:42.713452 25943 solver.cpp:290] Iteration 9700 (5.85145 iter/s, 17.0898s/100 iter), loss = 0.0233109
I0711 20:42:42.713476 25943 solver.cpp:309]     Train net output #0: loss = 0.0233108 (* 1 = 0.0233108 loss)
I0711 20:42:42.713484 25943 sgd_solver.cpp:106] Iteration 9700, lr = 1e-05
I0711 20:42:59.812858 25943 solver.cpp:290] Iteration 9800 (5.84832 iter/s, 17.0989s/100 iter), loss = 0.0192291
I0711 20:42:59.812991 25943 solver.cpp:309]     Train net output #0: loss = 0.019229 (* 1 = 0.019229 loss)
I0711 20:42:59.813001 25943 sgd_solver.cpp:106] Iteration 9800, lr = 1e-05
I0711 20:43:16.959740 25943 solver.cpp:290] Iteration 9900 (5.83217 iter/s, 17.1463s/100 iter), loss = 0.0167835
I0711 20:43:16.959764 25943 solver.cpp:309]     Train net output #0: loss = 0.0167834 (* 1 = 0.0167834 loss)
I0711 20:43:16.959772 25943 sgd_solver.cpp:106] Iteration 9900, lr = 1e-05
I0711 20:43:33.887284 25943 solver.cpp:594] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/l1reg/cityscapes5_jsegnet21v2_iter_10000.caffemodel
I0711 20:43:34.085353 25943 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/l1reg/cityscapes5_jsegnet21v2_iter_10000.solverstate
I0711 20:43:34.102954 25943 solver.cpp:467] Iteration 10000, Testing net (#0)
I0711 20:44:21.142468 25943 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.954543
I0711 20:44:21.142565 25943 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999591
I0711 20:44:21.142575 25943 solver.cpp:540]     Test net output #2: loss = 0.150033 (* 1 = 0.150033 loss)
I0711 20:44:21.331518 25943 solver.cpp:290] Iteration 10000 (1.55352 iter/s, 64.37s/100 iter), loss = 0.0188189
I0711 20:44:21.331544 25943 solver.cpp:309]     Train net output #0: loss = 0.0188188 (* 1 = 0.0188188 loss)
I0711 20:44:21.331552 25943 sgd_solver.cpp:106] Iteration 10000, lr = 1e-05
I0711 20:44:45.866601 25943 solver.cpp:290] Iteration 10100 (4.07591 iter/s, 24.5344s/100 iter), loss = 0.0239857
I0711 20:44:45.866637 25943 solver.cpp:309]     Train net output #0: loss = 0.0239856 (* 1 = 0.0239856 loss)
I0711 20:44:45.866647 25943 sgd_solver.cpp:106] Iteration 10100, lr = 1e-05
I0711 20:45:10.418066 26061 blocking_queue.cpp:50] Waiting for data
I0711 20:45:10.925334 25943 solver.cpp:290] Iteration 10200 (3.99074 iter/s, 25.058s/100 iter), loss = 0.0187356
I0711 20:45:10.925364 25943 solver.cpp:309]     Train net output #0: loss = 0.0187356 (* 1 = 0.0187356 loss)
I0711 20:45:10.925372 25943 sgd_solver.cpp:106] Iteration 10200, lr = 1e-05
I0711 20:45:29.385540 25943 solver.cpp:290] Iteration 10300 (5.41722 iter/s, 18.4597s/100 iter), loss = 0.0215167
I0711 20:45:29.385571 25943 solver.cpp:309]     Train net output #0: loss = 0.0215167 (* 1 = 0.0215167 loss)
I0711 20:45:29.385577 25943 sgd_solver.cpp:106] Iteration 10300, lr = 1e-05
I0711 20:45:46.492204 25943 solver.cpp:290] Iteration 10400 (5.84585 iter/s, 17.1062s/100 iter), loss = 0.0402743
I0711 20:45:46.492274 25943 solver.cpp:309]     Train net output #0: loss = 0.0402743 (* 1 = 0.0402743 loss)
I0711 20:45:46.492446 25943 sgd_solver.cpp:106] Iteration 10400, lr = 1e-05
I0711 20:46:03.628031 25943 solver.cpp:290] Iteration 10500 (5.83591 iter/s, 17.1353s/100 iter), loss = 0.00722957
I0711 20:46:03.628051 25943 solver.cpp:309]     Train net output #0: loss = 0.00722949 (* 1 = 0.00722949 loss)
I0711 20:46:03.628058 25943 sgd_solver.cpp:106] Iteration 10500, lr = 1e-05
I0711 20:46:20.663421 25943 solver.cpp:290] Iteration 10600 (5.8703 iter/s, 17.0349s/100 iter), loss = 0.0173038
I0711 20:46:20.663466 25943 solver.cpp:309]     Train net output #0: loss = 0.0173037 (* 1 = 0.0173037 loss)
I0711 20:46:20.663475 25943 sgd_solver.cpp:106] Iteration 10600, lr = 1e-05
I0711 20:46:37.743257 25943 solver.cpp:290] Iteration 10700 (5.85503 iter/s, 17.0793s/100 iter), loss = 0.0278885
I0711 20:46:37.743279 25943 solver.cpp:309]     Train net output #0: loss = 0.0278884 (* 1 = 0.0278884 loss)
I0711 20:46:37.743288 25943 sgd_solver.cpp:106] Iteration 10700, lr = 1e-05
I0711 20:46:54.806211 25943 solver.cpp:290] Iteration 10800 (5.86082 iter/s, 17.0625s/100 iter), loss = 0.0215059
I0711 20:46:54.806262 25943 solver.cpp:309]     Train net output #0: loss = 0.0215059 (* 1 = 0.0215059 loss)
I0711 20:46:54.806269 25943 sgd_solver.cpp:106] Iteration 10800, lr = 1e-05
I0711 20:47:11.786801 25943 solver.cpp:290] Iteration 10900 (5.88926 iter/s, 16.9801s/100 iter), loss = 0.0195726
I0711 20:47:11.786823 25943 solver.cpp:309]     Train net output #0: loss = 0.0195725 (* 1 = 0.0195725 loss)
I0711 20:47:11.786829 25943 sgd_solver.cpp:106] Iteration 10900, lr = 1e-05
I0711 20:47:28.886574 25943 solver.cpp:290] Iteration 11000 (5.8482 iter/s, 17.0993s/100 iter), loss = 0.018587
I0711 20:47:28.886694 25943 solver.cpp:309]     Train net output #0: loss = 0.0185869 (* 1 = 0.0185869 loss)
I0711 20:47:28.886705 25943 sgd_solver.cpp:106] Iteration 11000, lr = 1e-05
I0711 20:47:45.765758 25943 solver.cpp:290] Iteration 11100 (5.92466 iter/s, 16.8786s/100 iter), loss = 0.0181337
I0711 20:47:45.765780 25943 solver.cpp:309]     Train net output #0: loss = 0.0181336 (* 1 = 0.0181336 loss)
I0711 20:47:45.765786 25943 sgd_solver.cpp:106] Iteration 11100, lr = 1e-05
I0711 20:48:02.836508 25943 solver.cpp:290] Iteration 11200 (5.85814 iter/s, 17.0703s/100 iter), loss = 0.0267851
I0711 20:48:02.836554 25943 solver.cpp:309]     Train net output #0: loss = 0.026785 (* 1 = 0.026785 loss)
I0711 20:48:02.836563 25943 sgd_solver.cpp:106] Iteration 11200, lr = 1e-05
I0711 20:48:19.910207 25943 solver.cpp:290] Iteration 11300 (5.85714 iter/s, 17.0732s/100 iter), loss = 0.0266452
I0711 20:48:19.910231 25943 solver.cpp:309]     Train net output #0: loss = 0.0266451 (* 1 = 0.0266451 loss)
I0711 20:48:19.910238 25943 sgd_solver.cpp:106] Iteration 11300, lr = 1e-05
I0711 20:48:36.960624 25943 solver.cpp:290] Iteration 11400 (5.86513 iter/s, 17.0499s/100 iter), loss = 0.0165269
I0711 20:48:36.960686 25943 solver.cpp:309]     Train net output #0: loss = 0.0165268 (* 1 = 0.0165268 loss)
I0711 20:48:36.960697 25943 sgd_solver.cpp:106] Iteration 11400, lr = 1e-05
I0711 20:48:53.910198 25943 solver.cpp:290] Iteration 11500 (5.90003 iter/s, 16.9491s/100 iter), loss = 0.0201895
I0711 20:48:53.910220 25943 solver.cpp:309]     Train net output #0: loss = 0.0201895 (* 1 = 0.0201895 loss)
I0711 20:48:53.910228 25943 sgd_solver.cpp:106] Iteration 11500, lr = 1e-05
I0711 20:49:10.877393 25943 solver.cpp:290] Iteration 11600 (5.89389 iter/s, 16.9667s/100 iter), loss = 0.0214503
I0711 20:49:10.877437 25943 solver.cpp:309]     Train net output #0: loss = 0.0214502 (* 1 = 0.0214502 loss)
I0711 20:49:10.877444 25943 sgd_solver.cpp:106] Iteration 11600, lr = 1e-05
I0711 20:49:28.115718 25943 solver.cpp:290] Iteration 11700 (5.8012 iter/s, 17.2378s/100 iter), loss = 0.0217999
I0711 20:49:28.115742 25943 solver.cpp:309]     Train net output #0: loss = 0.0217998 (* 1 = 0.0217998 loss)
I0711 20:49:28.115748 25943 sgd_solver.cpp:106] Iteration 11700, lr = 1e-05
I0711 20:49:45.135345 25943 solver.cpp:290] Iteration 11800 (5.87574 iter/s, 17.0191s/100 iter), loss = 0.0142061
I0711 20:49:45.135396 25943 solver.cpp:309]     Train net output #0: loss = 0.0142061 (* 1 = 0.0142061 loss)
I0711 20:49:45.135403 25943 sgd_solver.cpp:106] Iteration 11800, lr = 1e-05
I0711 20:50:02.123481 25943 solver.cpp:290] Iteration 11900 (5.88664 iter/s, 16.9876s/100 iter), loss = 0.0256931
I0711 20:50:02.123504 25943 solver.cpp:309]     Train net output #0: loss = 0.025693 (* 1 = 0.025693 loss)
I0711 20:50:02.123512 25943 sgd_solver.cpp:106] Iteration 11900, lr = 1e-05
I0711 20:50:18.855257 25943 solver.cpp:467] Iteration 12000, Testing net (#0)
I0711 20:51:05.536178 25943 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.954518
I0711 20:51:05.536275 25943 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999654
I0711 20:51:05.536283 25943 solver.cpp:540]     Test net output #2: loss = 0.15673 (* 1 = 0.15673 loss)
I0711 20:51:05.723222 25943 solver.cpp:290] Iteration 12000 (1.57238 iter/s, 63.598s/100 iter), loss = 0.0208287
I0711 20:51:05.723248 25943 solver.cpp:309]     Train net output #0: loss = 0.0208286 (* 1 = 0.0208286 loss)
I0711 20:51:05.723253 25943 sgd_solver.cpp:106] Iteration 12000, lr = 1e-05
I0711 20:51:22.350327 25943 solver.cpp:290] Iteration 12100 (6.01445 iter/s, 16.6266s/100 iter), loss = 0.0238717
I0711 20:51:22.350350 25943 solver.cpp:309]     Train net output #0: loss = 0.0238716 (* 1 = 0.0238716 loss)
I0711 20:51:22.350356 25943 sgd_solver.cpp:106] Iteration 12100, lr = 1e-05
I0711 20:51:43.115567 25943 solver.cpp:290] Iteration 12200 (4.81588 iter/s, 20.7646s/100 iter), loss = 0.0163873
I0711 20:51:43.115633 25943 solver.cpp:309]     Train net output #0: loss = 0.0163872 (* 1 = 0.0163872 loss)
I0711 20:51:43.115641 25943 sgd_solver.cpp:106] Iteration 12200, lr = 1e-05
I0711 20:52:00.748456 25943 solver.cpp:290] Iteration 12300 (5.6714 iter/s, 17.6323s/100 iter), loss = 0.0366571
I0711 20:52:00.748484 25943 solver.cpp:309]     Train net output #0: loss = 0.036657 (* 1 = 0.036657 loss)
I0711 20:52:00.748493 25943 sgd_solver.cpp:106] Iteration 12300, lr = 1e-05
I0711 20:52:17.744170 25943 solver.cpp:290] Iteration 12400 (5.88401 iter/s, 16.9952s/100 iter), loss = 0.0424687
I0711 20:52:17.744246 25943 solver.cpp:309]     Train net output #0: loss = 0.0424687 (* 1 = 0.0424687 loss)
I0711 20:52:17.744253 25943 sgd_solver.cpp:106] Iteration 12400, lr = 1e-05
I0711 20:52:34.796059 25943 solver.cpp:290] Iteration 12500 (5.86464 iter/s, 17.0513s/100 iter), loss = 0.0165205
I0711 20:52:34.796084 25943 solver.cpp:309]     Train net output #0: loss = 0.0165204 (* 1 = 0.0165204 loss)
I0711 20:52:34.796090 25943 sgd_solver.cpp:106] Iteration 12500, lr = 1e-05
I0711 20:52:51.705153 25943 solver.cpp:290] Iteration 12600 (5.91415 iter/s, 16.9086s/100 iter), loss = 0.0186354
I0711 20:52:51.705241 25943 solver.cpp:309]     Train net output #0: loss = 0.0186353 (* 1 = 0.0186353 loss)
I0711 20:52:51.705252 25943 sgd_solver.cpp:106] Iteration 12600, lr = 1e-05
I0711 20:53:08.725724 25943 solver.cpp:290] Iteration 12700 (5.87543 iter/s, 17.02s/100 iter), loss = 0.0203239
I0711 20:53:08.725749 25943 solver.cpp:309]     Train net output #0: loss = 0.0203238 (* 1 = 0.0203238 loss)
I0711 20:53:08.725756 25943 sgd_solver.cpp:106] Iteration 12700, lr = 1e-05
I0711 20:53:25.784169 25943 solver.cpp:290] Iteration 12800 (5.86237 iter/s, 17.058s/100 iter), loss = 0.0167286
I0711 20:53:25.784224 25943 solver.cpp:309]     Train net output #0: loss = 0.0167285 (* 1 = 0.0167285 loss)
I0711 20:53:25.784231 25943 sgd_solver.cpp:106] Iteration 12800, lr = 1e-05
I0711 20:53:42.803282 25943 solver.cpp:290] Iteration 12900 (5.87593 iter/s, 17.0186s/100 iter), loss = 0.0277644
I0711 20:53:42.803310 25943 solver.cpp:309]     Train net output #0: loss = 0.0277643 (* 1 = 0.0277643 loss)
I0711 20:53:42.803319 25943 sgd_solver.cpp:106] Iteration 12900, lr = 1e-05
I0711 20:53:59.951207 25943 solver.cpp:290] Iteration 13000 (5.83178 iter/s, 17.1474s/100 iter), loss = 0.0339986
I0711 20:53:59.951283 25943 solver.cpp:309]     Train net output #0: loss = 0.0339985 (* 1 = 0.0339985 loss)
I0711 20:53:59.951292 25943 sgd_solver.cpp:106] Iteration 13000, lr = 1e-05
I0711 20:54:16.925348 25943 solver.cpp:290] Iteration 13100 (5.8915 iter/s, 16.9736s/100 iter), loss = 0.0155797
I0711 20:54:16.925377 25943 solver.cpp:309]     Train net output #0: loss = 0.0155796 (* 1 = 0.0155796 loss)
I0711 20:54:16.925387 25943 sgd_solver.cpp:106] Iteration 13100, lr = 1e-05
I0711 20:54:34.038751 25943 solver.cpp:290] Iteration 13200 (5.84354 iter/s, 17.1129s/100 iter), loss = 0.0210055
I0711 20:54:34.038803 25943 solver.cpp:309]     Train net output #0: loss = 0.0210054 (* 1 = 0.0210054 loss)
I0711 20:54:34.038811 25943 sgd_solver.cpp:106] Iteration 13200, lr = 1e-05
I0711 20:54:51.078543 25943 solver.cpp:290] Iteration 13300 (5.86879 iter/s, 17.0393s/100 iter), loss = 0.032986
I0711 20:54:51.078567 25943 solver.cpp:309]     Train net output #0: loss = 0.0329859 (* 1 = 0.0329859 loss)
I0711 20:54:51.078573 25943 sgd_solver.cpp:106] Iteration 13300, lr = 1e-05
I0711 20:55:08.148159 25943 solver.cpp:290] Iteration 13400 (5.85853 iter/s, 17.0691s/100 iter), loss = 0.0205173
I0711 20:55:08.148218 25943 solver.cpp:309]     Train net output #0: loss = 0.0205172 (* 1 = 0.0205172 loss)
I0711 20:55:08.148226 25943 sgd_solver.cpp:106] Iteration 13400, lr = 1e-05
I0711 20:55:25.240427 25943 solver.cpp:290] Iteration 13500 (5.85078 iter/s, 17.0917s/100 iter), loss = 0.0281079
I0711 20:55:25.240455 25943 solver.cpp:309]     Train net output #0: loss = 0.0281078 (* 1 = 0.0281078 loss)
I0711 20:55:25.240463 25943 sgd_solver.cpp:106] Iteration 13500, lr = 1e-05
I0711 20:55:42.287737 25943 solver.cpp:290] Iteration 13600 (5.8662 iter/s, 17.0468s/100 iter), loss = 0.0176351
I0711 20:55:42.287809 25943 solver.cpp:309]     Train net output #0: loss = 0.017635 (* 1 = 0.017635 loss)
I0711 20:55:42.287818 25943 sgd_solver.cpp:106] Iteration 13600, lr = 1e-05
I0711 20:55:59.322574 25943 solver.cpp:290] Iteration 13700 (5.87051 iter/s, 17.0343s/100 iter), loss = 0.0327023
I0711 20:55:59.322598 25943 solver.cpp:309]     Train net output #0: loss = 0.0327022 (* 1 = 0.0327022 loss)
I0711 20:55:59.322609 25943 sgd_solver.cpp:106] Iteration 13700, lr = 1e-05
I0711 20:56:16.282027 25943 solver.cpp:290] Iteration 13800 (5.89659 iter/s, 16.959s/100 iter), loss = 0.0190158
I0711 20:56:16.282107 25943 solver.cpp:309]     Train net output #0: loss = 0.0190157 (* 1 = 0.0190157 loss)
I0711 20:56:16.282119 25943 sgd_solver.cpp:106] Iteration 13800, lr = 1e-05
I0711 20:56:33.297926 25943 solver.cpp:290] Iteration 13900 (5.87704 iter/s, 17.0154s/100 iter), loss = 0.0194357
I0711 20:56:33.297951 25943 solver.cpp:309]     Train net output #0: loss = 0.0194356 (* 1 = 0.0194356 loss)
I0711 20:56:33.297958 25943 sgd_solver.cpp:106] Iteration 13900, lr = 1e-05
I0711 20:56:50.250952 25943 solver.cpp:467] Iteration 14000, Testing net (#0)
I0711 20:57:37.141640 25943 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.953702
I0711 20:57:37.141729 25943 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999395
I0711 20:57:37.141736 25943 solver.cpp:540]     Test net output #2: loss = 0.16174 (* 1 = 0.16174 loss)
I0711 20:57:37.326373 25943 solver.cpp:290] Iteration 14000 (1.56185 iter/s, 64.0267s/100 iter), loss = 0.0212054
I0711 20:57:37.326397 25943 solver.cpp:309]     Train net output #0: loss = 0.0212053 (* 1 = 0.0212053 loss)
I0711 20:57:37.326403 25943 sgd_solver.cpp:106] Iteration 14000, lr = 1e-05
I0711 20:57:54.110023 25943 solver.cpp:290] Iteration 14100 (5.95836 iter/s, 16.7832s/100 iter), loss = 0.016821
I0711 20:57:54.110054 25943 solver.cpp:309]     Train net output #0: loss = 0.016821 (* 1 = 0.016821 loss)
I0711 20:57:54.110064 25943 sgd_solver.cpp:106] Iteration 14100, lr = 1e-05
I0711 20:58:11.240679 25943 solver.cpp:290] Iteration 14200 (5.83766 iter/s, 17.1302s/100 iter), loss = 0.0191788
I0711 20:58:11.240782 25943 solver.cpp:309]     Train net output #0: loss = 0.0191787 (* 1 = 0.0191787 loss)
I0711 20:58:11.240792 25943 sgd_solver.cpp:106] Iteration 14200, lr = 1e-05
I0711 20:58:28.242133 25943 solver.cpp:290] Iteration 14300 (5.88205 iter/s, 17.0009s/100 iter), loss = 0.0124078
I0711 20:58:28.242161 25943 solver.cpp:309]     Train net output #0: loss = 0.0124077 (* 1 = 0.0124077 loss)
I0711 20:58:28.242171 25943 sgd_solver.cpp:106] Iteration 14300, lr = 1e-05
I0711 20:58:45.283980 25943 solver.cpp:290] Iteration 14400 (5.86808 iter/s, 17.0414s/100 iter), loss = 0.0181368
I0711 20:58:45.284039 25943 solver.cpp:309]     Train net output #0: loss = 0.0181367 (* 1 = 0.0181367 loss)
I0711 20:58:45.284046 25943 sgd_solver.cpp:106] Iteration 14400, lr = 1e-05
I0711 20:59:02.212841 25943 solver.cpp:290] Iteration 14500 (5.90725 iter/s, 16.9283s/100 iter), loss = 0.0361383
I0711 20:59:02.212865 25943 solver.cpp:309]     Train net output #0: loss = 0.0361383 (* 1 = 0.0361383 loss)
I0711 20:59:02.212872 25943 sgd_solver.cpp:106] Iteration 14500, lr = 1e-05
I0711 20:59:19.188208 25943 solver.cpp:290] Iteration 14600 (5.89106 iter/s, 16.9749s/100 iter), loss = 0.0198914
I0711 20:59:19.188254 25943 solver.cpp:309]     Train net output #0: loss = 0.0198913 (* 1 = 0.0198913 loss)
I0711 20:59:19.188261 25943 sgd_solver.cpp:106] Iteration 14600, lr = 1e-05
I0711 20:59:36.048640 25943 solver.cpp:290] Iteration 14700 (5.93122 iter/s, 16.8599s/100 iter), loss = 0.0575131
I0711 20:59:36.048661 25943 solver.cpp:309]     Train net output #0: loss = 0.057513 (* 1 = 0.057513 loss)
I0711 20:59:36.048667 25943 sgd_solver.cpp:106] Iteration 14700, lr = 1e-05
I0711 20:59:53.077683 25943 solver.cpp:290] Iteration 14800 (5.87249 iter/s, 17.0286s/100 iter), loss = 0.0293133
I0711 20:59:53.077759 25943 solver.cpp:309]     Train net output #0: loss = 0.0293132 (* 1 = 0.0293132 loss)
I0711 20:59:53.077766 25943 sgd_solver.cpp:106] Iteration 14800, lr = 1e-05
I0711 21:00:10.064061 25943 solver.cpp:290] Iteration 14900 (5.88726 iter/s, 16.9858s/100 iter), loss = 0.0360433
I0711 21:00:10.064086 25943 solver.cpp:309]     Train net output #0: loss = 0.0360432 (* 1 = 0.0360432 loss)
I0711 21:00:10.064095 25943 sgd_solver.cpp:106] Iteration 14900, lr = 1e-05
I0711 21:00:27.068130 25943 solver.cpp:290] Iteration 15000 (5.88111 iter/s, 17.0036s/100 iter), loss = 0.0372117
I0711 21:00:27.068181 25943 solver.cpp:309]     Train net output #0: loss = 0.0372116 (* 1 = 0.0372116 loss)
I0711 21:00:27.068189 25943 sgd_solver.cpp:106] Iteration 15000, lr = 1e-05
I0711 21:00:44.059842 25943 solver.cpp:290] Iteration 15100 (5.8854 iter/s, 16.9912s/100 iter), loss = 0.0167423
I0711 21:00:44.059867 25943 solver.cpp:309]     Train net output #0: loss = 0.0167422 (* 1 = 0.0167422 loss)
I0711 21:00:44.059876 25943 sgd_solver.cpp:106] Iteration 15100, lr = 1e-05
I0711 21:01:01.063447 25943 solver.cpp:290] Iteration 15200 (5.88128 iter/s, 17.0031s/100 iter), loss = 0.01946
I0711 21:01:01.063503 25943 solver.cpp:309]     Train net output #0: loss = 0.01946 (* 1 = 0.01946 loss)
I0711 21:01:01.063511 25943 sgd_solver.cpp:106] Iteration 15200, lr = 1e-05
I0711 21:01:18.007068 25943 solver.cpp:290] Iteration 15300 (5.9021 iter/s, 16.9431s/100 iter), loss = 0.0238182
I0711 21:01:18.007097 25943 solver.cpp:309]     Train net output #0: loss = 0.0238181 (* 1 = 0.0238181 loss)
I0711 21:01:18.007105 25943 sgd_solver.cpp:106] Iteration 15300, lr = 1e-05
I0711 21:01:35.013906 25943 solver.cpp:290] Iteration 15400 (5.88016 iter/s, 17.0063s/100 iter), loss = 0.0267364
I0711 21:01:35.013993 25943 solver.cpp:309]     Train net output #0: loss = 0.0267363 (* 1 = 0.0267363 loss)
I0711 21:01:35.014006 25943 sgd_solver.cpp:106] Iteration 15400, lr = 1e-05
I0711 21:01:51.904183 25943 solver.cpp:290] Iteration 15500 (5.92076 iter/s, 16.8897s/100 iter), loss = 0.0236173
I0711 21:01:51.904211 25943 solver.cpp:309]     Train net output #0: loss = 0.0236172 (* 1 = 0.0236172 loss)
I0711 21:01:51.904219 25943 sgd_solver.cpp:106] Iteration 15500, lr = 1e-05
I0711 21:02:09.020572 25943 solver.cpp:290] Iteration 15600 (5.84252 iter/s, 17.1159s/100 iter), loss = 0.0274421
I0711 21:02:09.020632 25943 solver.cpp:309]     Train net output #0: loss = 0.027442 (* 1 = 0.027442 loss)
I0711 21:02:09.020642 25943 sgd_solver.cpp:106] Iteration 15600, lr = 1e-05
I0711 21:02:26.040443 25943 solver.cpp:290] Iteration 15700 (5.87567 iter/s, 17.0194s/100 iter), loss = 0.0243506
I0711 21:02:26.040464 25943 solver.cpp:309]     Train net output #0: loss = 0.0243505 (* 1 = 0.0243505 loss)
I0711 21:02:26.040472 25943 sgd_solver.cpp:106] Iteration 15700, lr = 1e-05
I0711 21:02:43.154892 25943 solver.cpp:290] Iteration 15800 (5.84318 iter/s, 17.114s/100 iter), loss = 0.0318157
I0711 21:02:43.154966 25943 solver.cpp:309]     Train net output #0: loss = 0.0318157 (* 1 = 0.0318157 loss)
I0711 21:02:43.154975 25943 sgd_solver.cpp:106] Iteration 15800, lr = 1e-05
I0711 21:03:00.227533 25943 solver.cpp:290] Iteration 15900 (5.85751 iter/s, 17.0721s/100 iter), loss = 0.0310758
I0711 21:03:00.227556 25943 solver.cpp:309]     Train net output #0: loss = 0.0310757 (* 1 = 0.0310757 loss)
I0711 21:03:00.227563 25943 sgd_solver.cpp:106] Iteration 15900, lr = 1e-05
I0711 21:03:16.962541 25943 solver.cpp:467] Iteration 16000, Testing net (#0)
I0711 21:04:03.534816 25943 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.953378
I0711 21:04:03.534900 25943 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999586
I0711 21:04:03.534909 25943 solver.cpp:540]     Test net output #2: loss = 0.168213 (* 1 = 0.168213 loss)
I0711 21:04:03.710425 25943 solver.cpp:290] Iteration 16000 (1.57527 iter/s, 63.4811s/100 iter), loss = 0.0281878
I0711 21:04:03.710449 25943 solver.cpp:309]     Train net output #0: loss = 0.0281877 (* 1 = 0.0281877 loss)
I0711 21:04:03.710456 25943 sgd_solver.cpp:106] Iteration 16000, lr = 1e-05
I0711 21:04:20.546418 25943 solver.cpp:290] Iteration 16100 (5.93983 iter/s, 16.8355s/100 iter), loss = 0.0181807
I0711 21:04:20.546458 25943 solver.cpp:309]     Train net output #0: loss = 0.0181807 (* 1 = 0.0181807 loss)
I0711 21:04:20.546468 25943 sgd_solver.cpp:106] Iteration 16100, lr = 1e-05
I0711 21:04:37.678592 25943 solver.cpp:290] Iteration 16200 (5.83714 iter/s, 17.1317s/100 iter), loss = 0.0348326
I0711 21:04:37.684973 25943 solver.cpp:309]     Train net output #0: loss = 0.0348325 (* 1 = 0.0348325 loss)
I0711 21:04:37.685027 25943 sgd_solver.cpp:106] Iteration 16200, lr = 1e-05
I0711 21:04:54.831936 25943 solver.cpp:290] Iteration 16300 (5.83209 iter/s, 17.1465s/100 iter), loss = 0.0334909
I0711 21:04:54.831959 25943 solver.cpp:309]     Train net output #0: loss = 0.0334909 (* 1 = 0.0334909 loss)
I0711 21:04:54.831966 25943 sgd_solver.cpp:106] Iteration 16300, lr = 1e-05
I0711 21:05:11.806557 25943 solver.cpp:290] Iteration 16400 (5.89132 iter/s, 16.9741s/100 iter), loss = 0.0238516
I0711 21:05:11.806669 25943 solver.cpp:309]     Train net output #0: loss = 0.0238516 (* 1 = 0.0238516 loss)
I0711 21:05:11.806679 25943 sgd_solver.cpp:106] Iteration 16400, lr = 1e-05
I0711 21:05:28.677492 25943 solver.cpp:290] Iteration 16500 (5.92756 iter/s, 16.8704s/100 iter), loss = 0.0145279
I0711 21:05:28.677515 25943 solver.cpp:309]     Train net output #0: loss = 0.0145278 (* 1 = 0.0145278 loss)
I0711 21:05:28.677522 25943 sgd_solver.cpp:106] Iteration 16500, lr = 1e-05
I0711 21:05:45.598369 25943 solver.cpp:290] Iteration 16600 (5.91003 iter/s, 16.9204s/100 iter), loss = 0.0275113
I0711 21:05:45.598445 25943 solver.cpp:309]     Train net output #0: loss = 0.0275113 (* 1 = 0.0275113 loss)
I0711 21:05:45.598453 25943 sgd_solver.cpp:106] Iteration 16600, lr = 1e-05
I0711 21:06:02.505666 25943 solver.cpp:290] Iteration 16700 (5.91479 iter/s, 16.9068s/100 iter), loss = 0.0346397
I0711 21:06:02.505690 25943 solver.cpp:309]     Train net output #0: loss = 0.0346396 (* 1 = 0.0346396 loss)
I0711 21:06:02.505699 25943 sgd_solver.cpp:106] Iteration 16700, lr = 1e-05
I0711 21:06:19.654618 25943 solver.cpp:290] Iteration 16800 (5.83143 iter/s, 17.1485s/100 iter), loss = 0.0487344
I0711 21:06:19.654682 25943 solver.cpp:309]     Train net output #0: loss = 0.0487344 (* 1 = 0.0487344 loss)
I0711 21:06:19.654693 25943 sgd_solver.cpp:106] Iteration 16800, lr = 1e-05
I0711 21:06:36.734182 25943 solver.cpp:290] Iteration 16900 (5.85513 iter/s, 17.079s/100 iter), loss = 0.0191461
I0711 21:06:36.734208 25943 solver.cpp:309]     Train net output #0: loss = 0.019146 (* 1 = 0.019146 loss)
I0711 21:06:36.734217 25943 sgd_solver.cpp:106] Iteration 16900, lr = 1e-05
I0711 21:06:53.742188 25943 solver.cpp:290] Iteration 17000 (5.87975 iter/s, 17.0075s/100 iter), loss = 0.0166016
I0711 21:06:53.742246 25943 solver.cpp:309]     Train net output #0: loss = 0.0166016 (* 1 = 0.0166016 loss)
I0711 21:06:53.742257 25943 sgd_solver.cpp:106] Iteration 17000, lr = 1e-05
I0711 21:07:10.777400 25943 solver.cpp:290] Iteration 17100 (5.87037 iter/s, 17.0347s/100 iter), loss = 0.0133025
I0711 21:07:10.777423 25943 solver.cpp:309]     Train net output #0: loss = 0.0133024 (* 1 = 0.0133024 loss)
I0711 21:07:10.777431 25943 sgd_solver.cpp:106] Iteration 17100, lr = 1e-05
I0711 21:07:27.779268 25943 solver.cpp:290] Iteration 17200 (5.88188 iter/s, 17.0014s/100 iter), loss = 0.0230211
I0711 21:07:27.779320 25943 solver.cpp:309]     Train net output #0: loss = 0.023021 (* 1 = 0.023021 loss)
I0711 21:07:27.779328 25943 sgd_solver.cpp:106] Iteration 17200, lr = 1e-05
I0711 21:07:44.741129 25943 solver.cpp:290] Iteration 17300 (5.89576 iter/s, 16.9613s/100 iter), loss = 0.0189019
I0711 21:07:44.741153 25943 solver.cpp:309]     Train net output #0: loss = 0.0189019 (* 1 = 0.0189019 loss)
I0711 21:07:44.741160 25943 sgd_solver.cpp:106] Iteration 17300, lr = 1e-05
I0711 21:08:01.804019 25943 solver.cpp:290] Iteration 17400 (5.86084 iter/s, 17.0624s/100 iter), loss = 0.0201729
I0711 21:08:01.805253 25943 solver.cpp:309]     Train net output #0: loss = 0.0201729 (* 1 = 0.0201729 loss)
I0711 21:08:01.805265 25943 sgd_solver.cpp:106] Iteration 17400, lr = 1e-05
I0711 21:08:19.116080 25943 solver.cpp:290] Iteration 17500 (5.77689 iter/s, 17.3104s/100 iter), loss = 0.0503401
I0711 21:08:19.116104 25943 solver.cpp:309]     Train net output #0: loss = 0.05034 (* 1 = 0.05034 loss)
I0711 21:08:19.116111 25943 sgd_solver.cpp:106] Iteration 17500, lr = 1e-05
I0711 21:08:36.098492 25943 solver.cpp:290] Iteration 17600 (5.88861 iter/s, 16.9819s/100 iter), loss = 0.0163527
I0711 21:08:36.098570 25943 solver.cpp:309]     Train net output #0: loss = 0.0163526 (* 1 = 0.0163526 loss)
I0711 21:08:36.098578 25943 sgd_solver.cpp:106] Iteration 17600, lr = 1e-05
I0711 21:08:52.982316 25943 solver.cpp:290] Iteration 17700 (5.92302 iter/s, 16.8833s/100 iter), loss = 0.026588
I0711 21:08:52.982342 25943 solver.cpp:309]     Train net output #0: loss = 0.026588 (* 1 = 0.026588 loss)
I0711 21:08:52.982352 25943 sgd_solver.cpp:106] Iteration 17700, lr = 1e-05
I0711 21:09:10.041040 25943 solver.cpp:290] Iteration 17800 (5.86227 iter/s, 17.0582s/100 iter), loss = 0.0565402
I0711 21:09:10.041168 25943 solver.cpp:309]     Train net output #0: loss = 0.0565402 (* 1 = 0.0565402 loss)
I0711 21:09:10.041179 25943 sgd_solver.cpp:106] Iteration 17800, lr = 1e-05
I0711 21:09:27.107363 25943 solver.cpp:290] Iteration 17900 (5.8597 iter/s, 17.0657s/100 iter), loss = 0.0300097
I0711 21:09:27.107393 25943 solver.cpp:309]     Train net output #0: loss = 0.0300097 (* 1 = 0.0300097 loss)
I0711 21:09:27.107401 25943 sgd_solver.cpp:106] Iteration 17900, lr = 1e-05
I0711 21:09:44.140404 25943 solver.cpp:467] Iteration 18000, Testing net (#0)
I0711 21:10:30.983666 25943 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.951366
I0711 21:10:30.983767 25943 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999734
I0711 21:10:30.983774 25943 solver.cpp:540]     Test net output #2: loss = 0.174061 (* 1 = 0.174061 loss)
I0711 21:10:31.178889 25943 solver.cpp:290] Iteration 18000 (1.5608 iter/s, 64.0698s/100 iter), loss = 0.020442
I0711 21:10:31.178917 25943 solver.cpp:309]     Train net output #0: loss = 0.0204419 (* 1 = 0.0204419 loss)
I0711 21:10:31.178923 25943 sgd_solver.cpp:106] Iteration 18000, lr = 1e-05
I0711 21:10:48.097239 25943 solver.cpp:290] Iteration 18100 (5.91092 iter/s, 16.9179s/100 iter), loss = 0.0305118
I0711 21:10:48.097263 25943 solver.cpp:309]     Train net output #0: loss = 0.0305117 (* 1 = 0.0305117 loss)
I0711 21:10:48.097270 25943 sgd_solver.cpp:106] Iteration 18100, lr = 1e-05
I0711 21:11:05.259099 25943 solver.cpp:290] Iteration 18200 (5.82704 iter/s, 17.1614s/100 iter), loss = 0.0251192
I0711 21:11:05.259177 25943 solver.cpp:309]     Train net output #0: loss = 0.0251191 (* 1 = 0.0251191 loss)
I0711 21:11:05.259187 25943 sgd_solver.cpp:106] Iteration 18200, lr = 1e-05
I0711 21:11:22.332021 25943 solver.cpp:290] Iteration 18300 (5.85742 iter/s, 17.0724s/100 iter), loss = 0.026263
I0711 21:11:22.332046 25943 solver.cpp:309]     Train net output #0: loss = 0.0262629 (* 1 = 0.0262629 loss)
I0711 21:11:22.332051 25943 sgd_solver.cpp:106] Iteration 18300, lr = 1e-05
I0711 21:11:39.459152 25943 solver.cpp:290] Iteration 18400 (5.83886 iter/s, 17.1266s/100 iter), loss = 0.0255635
I0711 21:11:39.459237 25943 solver.cpp:309]     Train net output #0: loss = 0.0255634 (* 1 = 0.0255634 loss)
I0711 21:11:39.459249 25943 sgd_solver.cpp:106] Iteration 18400, lr = 1e-05
I0711 21:11:56.419777 25943 solver.cpp:290] Iteration 18500 (5.8962 iter/s, 16.9601s/100 iter), loss = 0.041522
I0711 21:11:56.419806 25943 solver.cpp:309]     Train net output #0: loss = 0.041522 (* 1 = 0.041522 loss)
I0711 21:11:56.419811 25943 sgd_solver.cpp:106] Iteration 18500, lr = 1e-05
I0711 21:12:13.589259 25943 solver.cpp:290] Iteration 18600 (5.82446 iter/s, 17.169s/100 iter), loss = 0.0156392
I0711 21:12:13.589314 25943 solver.cpp:309]     Train net output #0: loss = 0.0156392 (* 1 = 0.0156392 loss)
I0711 21:12:13.589329 25943 sgd_solver.cpp:106] Iteration 18600, lr = 1e-05
I0711 21:12:30.528435 25943 solver.cpp:290] Iteration 18700 (5.90366 iter/s, 16.9387s/100 iter), loss = 0.0258387
I0711 21:12:30.528463 25943 solver.cpp:309]     Train net output #0: loss = 0.0258386 (* 1 = 0.0258386 loss)
I0711 21:12:30.528470 25943 sgd_solver.cpp:106] Iteration 18700, lr = 1e-05
I0711 21:12:47.397866 25943 solver.cpp:290] Iteration 18800 (5.92805 iter/s, 16.8689s/100 iter), loss = 0.0195042
I0711 21:12:47.397948 25943 solver.cpp:309]     Train net output #0: loss = 0.0195042 (* 1 = 0.0195042 loss)
I0711 21:12:47.397959 25943 sgd_solver.cpp:106] Iteration 18800, lr = 1e-05
I0711 21:13:04.524559 25943 solver.cpp:290] Iteration 18900 (5.83903 iter/s, 17.1261s/100 iter), loss = 0.0267715
I0711 21:13:04.524585 25943 solver.cpp:309]     Train net output #0: loss = 0.0267715 (* 1 = 0.0267715 loss)
I0711 21:13:04.524592 25943 sgd_solver.cpp:106] Iteration 18900, lr = 1e-05
I0711 21:13:21.560497 25943 solver.cpp:290] Iteration 19000 (5.87011 iter/s, 17.0354s/100 iter), loss = 0.0296351
I0711 21:13:21.560561 25943 solver.cpp:309]     Train net output #0: loss = 0.029635 (* 1 = 0.029635 loss)
I0711 21:13:21.560573 25943 sgd_solver.cpp:106] Iteration 19000, lr = 1e-05
I0711 21:13:38.664460 25943 solver.cpp:290] Iteration 19100 (5.84678 iter/s, 17.1034s/100 iter), loss = 0.0197047
I0711 21:13:38.664482 25943 solver.cpp:309]     Train net output #0: loss = 0.0197046 (* 1 = 0.0197046 loss)
I0711 21:13:38.664489 25943 sgd_solver.cpp:106] Iteration 19100, lr = 1e-05
I0711 21:13:55.893720 25943 solver.cpp:290] Iteration 19200 (5.80425 iter/s, 17.2288s/100 iter), loss = 0.0193212
I0711 21:13:55.893800 25943 solver.cpp:309]     Train net output #0: loss = 0.0193212 (* 1 = 0.0193212 loss)
I0711 21:13:55.893810 25943 sgd_solver.cpp:106] Iteration 19200, lr = 1e-05
I0711 21:14:12.954246 25943 solver.cpp:290] Iteration 19300 (5.86167 iter/s, 17.06s/100 iter), loss = 0.0433611
I0711 21:14:12.954274 25943 solver.cpp:309]     Train net output #0: loss = 0.0433611 (* 1 = 0.0433611 loss)
I0711 21:14:12.954283 25943 sgd_solver.cpp:106] Iteration 19300, lr = 1e-05
I0711 21:14:29.899994 25943 solver.cpp:290] Iteration 19400 (5.90136 iter/s, 16.9453s/100 iter), loss = 0.0257819
I0711 21:14:29.900102 25943 solver.cpp:309]     Train net output #0: loss = 0.0257819 (* 1 = 0.0257819 loss)
I0711 21:14:29.900112 25943 sgd_solver.cpp:106] Iteration 19400, lr = 1e-05
I0711 21:14:46.924943 25943 solver.cpp:290] Iteration 19500 (5.87393 iter/s, 17.0244s/100 iter), loss = 0.0315655
I0711 21:14:46.924967 25943 solver.cpp:309]     Train net output #0: loss = 0.0315654 (* 1 = 0.0315654 loss)
I0711 21:14:46.924973 25943 sgd_solver.cpp:106] Iteration 19500, lr = 1e-05
I0711 21:15:03.976450 25943 solver.cpp:290] Iteration 19600 (5.86475 iter/s, 17.051s/100 iter), loss = 0.0135738
I0711 21:15:03.976534 25943 solver.cpp:309]     Train net output #0: loss = 0.0135737 (* 1 = 0.0135737 loss)
I0711 21:15:03.976544 25943 sgd_solver.cpp:106] Iteration 19600, lr = 1e-05
I0711 21:15:21.171227 25943 solver.cpp:290] Iteration 19700 (5.81591 iter/s, 17.1942s/100 iter), loss = 0.017043
I0711 21:15:21.171255 25943 solver.cpp:309]     Train net output #0: loss = 0.017043 (* 1 = 0.017043 loss)
I0711 21:15:21.171264 25943 sgd_solver.cpp:106] Iteration 19700, lr = 1e-05
I0711 21:15:38.421727 25943 solver.cpp:290] Iteration 19800 (5.7971 iter/s, 17.25s/100 iter), loss = 0.0165171
I0711 21:15:38.421808 25943 solver.cpp:309]     Train net output #0: loss = 0.0165171 (* 1 = 0.0165171 loss)
I0711 21:15:38.421818 25943 sgd_solver.cpp:106] Iteration 19800, lr = 1e-05
I0711 21:15:55.528056 25943 solver.cpp:290] Iteration 19900 (5.84598 iter/s, 17.1058s/100 iter), loss = 0.0414882
I0711 21:15:55.528080 25943 solver.cpp:309]     Train net output #0: loss = 0.0414881 (* 1 = 0.0414881 loss)
I0711 21:15:55.528087 25943 sgd_solver.cpp:106] Iteration 19900, lr = 1e-05
I0711 21:16:12.315646 25943 solver.cpp:594] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/l1reg/cityscapes5_jsegnet21v2_iter_20000.caffemodel
I0711 21:16:12.368604 25943 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/l1reg/cityscapes5_jsegnet21v2_iter_20000.solverstate
I0711 21:16:12.385255 25943 solver.cpp:467] Iteration 20000, Testing net (#0)
I0711 21:16:58.867396 25943 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.953757
I0711 21:16:58.867462 25943 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999702
I0711 21:16:58.867470 25943 solver.cpp:540]     Test net output #2: loss = 0.162254 (* 1 = 0.162254 loss)
I0711 21:16:59.058522 25943 solver.cpp:290] Iteration 20000 (1.57409 iter/s, 63.5287s/100 iter), loss = 0.0195255
I0711 21:16:59.058545 25943 solver.cpp:309]     Train net output #0: loss = 0.0195255 (* 1 = 0.0195255 loss)
I0711 21:16:59.058552 25943 sgd_solver.cpp:106] Iteration 20000, lr = 1e-05
I0711 21:17:15.857130 25943 solver.cpp:290] Iteration 20100 (5.95305 iter/s, 16.7981s/100 iter), loss = 0.0316774
I0711 21:17:15.857156 25943 solver.cpp:309]     Train net output #0: loss = 0.0316773 (* 1 = 0.0316773 loss)
I0711 21:17:15.857164 25943 sgd_solver.cpp:106] Iteration 20100, lr = 1e-05
I0711 21:17:32.909037 25943 solver.cpp:290] Iteration 20200 (5.86462 iter/s, 17.0514s/100 iter), loss = 0.0167925
I0711 21:17:32.909087 25943 solver.cpp:309]     Train net output #0: loss = 0.0167925 (* 1 = 0.0167925 loss)
I0711 21:17:32.909096 25943 sgd_solver.cpp:106] Iteration 20200, lr = 1e-05
I0711 21:17:49.940822 25943 solver.cpp:290] Iteration 20300 (5.87156 iter/s, 17.0313s/100 iter), loss = 0.0180656
I0711 21:17:49.940845 25943 solver.cpp:309]     Train net output #0: loss = 0.0180656 (* 1 = 0.0180656 loss)
I0711 21:17:49.940850 25943 sgd_solver.cpp:106] Iteration 20300, lr = 1e-05
I0711 21:18:07.374302 25943 solver.cpp:290] Iteration 20400 (5.73625 iter/s, 17.433s/100 iter), loss = 0.0462751
I0711 21:18:07.374382 25943 solver.cpp:309]     Train net output #0: loss = 0.046275 (* 1 = 0.046275 loss)
I0711 21:18:07.374389 25943 sgd_solver.cpp:106] Iteration 20400, lr = 1e-05
I0711 21:18:24.634522 25943 solver.cpp:290] Iteration 20500 (5.79385 iter/s, 17.2597s/100 iter), loss = 0.0311642
I0711 21:18:24.634546 25943 solver.cpp:309]     Train net output #0: loss = 0.0311642 (* 1 = 0.0311642 loss)
I0711 21:18:24.634552 25943 sgd_solver.cpp:106] Iteration 20500, lr = 1e-05
I0711 21:18:41.608544 25943 solver.cpp:290] Iteration 20600 (5.89153 iter/s, 16.9735s/100 iter), loss = 0.0198744
I0711 21:18:41.608594 25943 solver.cpp:309]     Train net output #0: loss = 0.0198743 (* 1 = 0.0198743 loss)
I0711 21:18:41.608603 25943 sgd_solver.cpp:106] Iteration 20600, lr = 1e-05
I0711 21:18:58.576478 25943 solver.cpp:290] Iteration 20700 (5.89365 iter/s, 16.9674s/100 iter), loss = 0.0329889
I0711 21:18:58.576500 25943 solver.cpp:309]     Train net output #0: loss = 0.0329888 (* 1 = 0.0329888 loss)
I0711 21:18:58.576508 25943 sgd_solver.cpp:106] Iteration 20700, lr = 1e-05
I0711 21:19:15.656713 25943 solver.cpp:290] Iteration 20800 (5.85489 iter/s, 17.0797s/100 iter), loss = 0.0208864
I0711 21:19:15.656764 25943 solver.cpp:309]     Train net output #0: loss = 0.0208863 (* 1 = 0.0208863 loss)
I0711 21:19:15.656771 25943 sgd_solver.cpp:106] Iteration 20800, lr = 1e-05
I0711 21:19:32.508682 25943 solver.cpp:290] Iteration 20900 (5.9342 iter/s, 16.8515s/100 iter), loss = 0.019833
I0711 21:19:32.508707 25943 solver.cpp:309]     Train net output #0: loss = 0.0198329 (* 1 = 0.0198329 loss)
I0711 21:19:32.508715 25943 sgd_solver.cpp:106] Iteration 20900, lr = 1e-05
I0711 21:19:49.734633 25943 solver.cpp:290] Iteration 21000 (5.80536 iter/s, 17.2255s/100 iter), loss = 0.0355665
I0711 21:19:49.734711 25943 solver.cpp:309]     Train net output #0: loss = 0.0355665 (* 1 = 0.0355665 loss)
I0711 21:19:49.734719 25943 sgd_solver.cpp:106] Iteration 21000, lr = 1e-05
I0711 21:20:06.816458 25943 solver.cpp:290] Iteration 21100 (5.85436 iter/s, 17.0813s/100 iter), loss = 0.0238393
I0711 21:20:06.816488 25943 solver.cpp:309]     Train net output #0: loss = 0.0238392 (* 1 = 0.0238392 loss)
I0711 21:20:06.816496 25943 sgd_solver.cpp:106] Iteration 21100, lr = 1e-05
I0711 21:20:24.113580 25943 solver.cpp:290] Iteration 21200 (5.78148 iter/s, 17.2966s/100 iter), loss = 0.0384313
I0711 21:20:24.113653 25943 solver.cpp:309]     Train net output #0: loss = 0.0384312 (* 1 = 0.0384312 loss)
I0711 21:20:24.113662 25943 sgd_solver.cpp:106] Iteration 21200, lr = 1e-05
I0711 21:20:41.090298 25943 solver.cpp:290] Iteration 21300 (5.89061 iter/s, 16.9762s/100 iter), loss = 0.0180406
I0711 21:20:41.090325 25943 solver.cpp:309]     Train net output #0: loss = 0.0180406 (* 1 = 0.0180406 loss)
I0711 21:20:41.090335 25943 sgd_solver.cpp:106] Iteration 21300, lr = 1e-05
I0711 21:20:58.058152 25943 solver.cpp:290] Iteration 21400 (5.89367 iter/s, 16.9674s/100 iter), loss = 0.0244459
I0711 21:20:58.058204 25943 solver.cpp:309]     Train net output #0: loss = 0.0244458 (* 1 = 0.0244458 loss)
I0711 21:20:58.058212 25943 sgd_solver.cpp:106] Iteration 21400, lr = 1e-05
I0711 21:21:15.314074 25943 solver.cpp:290] Iteration 21500 (5.79529 iter/s, 17.2554s/100 iter), loss = 0.0341594
I0711 21:21:15.314100 25943 solver.cpp:309]     Train net output #0: loss = 0.0341594 (* 1 = 0.0341594 loss)
I0711 21:21:15.314106 25943 sgd_solver.cpp:106] Iteration 21500, lr = 1e-05
I0711 21:21:32.558256 25943 solver.cpp:290] Iteration 21600 (5.79922 iter/s, 17.2437s/100 iter), loss = 0.0317659
I0711 21:21:32.558354 25943 solver.cpp:309]     Train net output #0: loss = 0.0317659 (* 1 = 0.0317659 loss)
I0711 21:21:32.558365 25943 sgd_solver.cpp:106] Iteration 21600, lr = 1e-05
I0711 21:21:49.630949 25943 solver.cpp:290] Iteration 21700 (5.8575 iter/s, 17.0721s/100 iter), loss = 0.0304673
I0711 21:21:49.630978 25943 solver.cpp:309]     Train net output #0: loss = 0.0304672 (* 1 = 0.0304672 loss)
I0711 21:21:49.630987 25943 sgd_solver.cpp:106] Iteration 21700, lr = 1e-05
I0711 21:22:06.759912 25943 solver.cpp:290] Iteration 21800 (5.83823 iter/s, 17.1285s/100 iter), loss = 0.0177323
I0711 21:22:06.759984 25943 solver.cpp:309]     Train net output #0: loss = 0.0177323 (* 1 = 0.0177323 loss)
I0711 21:22:06.759991 25943 sgd_solver.cpp:106] Iteration 21800, lr = 1e-05
I0711 21:22:23.825608 25943 solver.cpp:290] Iteration 21900 (5.85989 iter/s, 17.0652s/100 iter), loss = 0.0130767
I0711 21:22:23.825630 25943 solver.cpp:309]     Train net output #0: loss = 0.0130767 (* 1 = 0.0130767 loss)
I0711 21:22:23.825637 25943 sgd_solver.cpp:106] Iteration 21900, lr = 1e-05
I0711 21:22:40.810356 25943 solver.cpp:467] Iteration 22000, Testing net (#0)
I0711 21:23:27.818897 25943 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.952638
I0711 21:23:27.818994 25943 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999636
I0711 21:23:27.819002 25943 solver.cpp:540]     Test net output #2: loss = 0.168657 (* 1 = 0.168657 loss)
I0711 21:23:28.012353 25943 solver.cpp:290] Iteration 22000 (1.558 iter/s, 64.185s/100 iter), loss = 0.0289806
I0711 21:23:28.012382 25943 solver.cpp:309]     Train net output #0: loss = 0.0289805 (* 1 = 0.0289805 loss)
I0711 21:23:28.012387 25943 sgd_solver.cpp:106] Iteration 22000, lr = 1e-05
I0711 21:23:47.468971 25943 solver.cpp:290] Iteration 22100 (5.13979 iter/s, 19.4561s/100 iter), loss = 0.0226487
I0711 21:23:47.468992 25943 solver.cpp:309]     Train net output #0: loss = 0.0226487 (* 1 = 0.0226487 loss)
I0711 21:23:47.468999 25943 sgd_solver.cpp:106] Iteration 22100, lr = 1e-05
I0711 21:24:04.388950 25943 solver.cpp:290] Iteration 22200 (5.91034 iter/s, 16.9195s/100 iter), loss = 0.0218852
I0711 21:24:04.388996 25943 solver.cpp:309]     Train net output #0: loss = 0.0218851 (* 1 = 0.0218851 loss)
I0711 21:24:04.389004 25943 sgd_solver.cpp:106] Iteration 22200, lr = 1e-05
I0711 21:24:21.485424 25943 solver.cpp:290] Iteration 22300 (5.84934 iter/s, 17.096s/100 iter), loss = 0.0272808
I0711 21:24:21.485451 25943 solver.cpp:309]     Train net output #0: loss = 0.0272808 (* 1 = 0.0272808 loss)
I0711 21:24:21.485460 25943 sgd_solver.cpp:106] Iteration 22300, lr = 1e-05
I0711 21:24:38.561431 25943 solver.cpp:290] Iteration 22400 (5.85634 iter/s, 17.0755s/100 iter), loss = 0.0215519
I0711 21:24:38.561513 25943 solver.cpp:309]     Train net output #0: loss = 0.0215518 (* 1 = 0.0215518 loss)
I0711 21:24:38.561522 25943 sgd_solver.cpp:106] Iteration 22400, lr = 1e-05
I0711 21:24:55.650635 25943 solver.cpp:290] Iteration 22500 (5.85184 iter/s, 17.0887s/100 iter), loss = 0.0259647
I0711 21:24:55.650657 25943 solver.cpp:309]     Train net output #0: loss = 0.0259646 (* 1 = 0.0259646 loss)
I0711 21:24:55.650665 25943 sgd_solver.cpp:106] Iteration 22500, lr = 1e-05
I0711 21:25:12.816769 25943 solver.cpp:290] Iteration 22600 (5.82559 iter/s, 17.1656s/100 iter), loss = 0.0325643
I0711 21:25:12.816876 25943 solver.cpp:309]     Train net output #0: loss = 0.0325643 (* 1 = 0.0325643 loss)
I0711 21:25:12.816884 25943 sgd_solver.cpp:106] Iteration 22600, lr = 1e-05
I0711 21:25:30.122490 25943 solver.cpp:290] Iteration 22700 (5.77863 iter/s, 17.3051s/100 iter), loss = 0.0191631
I0711 21:25:30.122519 25943 solver.cpp:309]     Train net output #0: loss = 0.0191631 (* 1 = 0.0191631 loss)
I0711 21:25:30.122525 25943 sgd_solver.cpp:106] Iteration 22700, lr = 1e-05
I0711 21:25:47.406225 25943 solver.cpp:290] Iteration 22800 (5.78595 iter/s, 17.2832s/100 iter), loss = 0.0215036
I0711 21:25:47.406288 25943 solver.cpp:309]     Train net output #0: loss = 0.0215036 (* 1 = 0.0215036 loss)
I0711 21:25:47.406296 25943 sgd_solver.cpp:106] Iteration 22800, lr = 1e-05
I0711 21:26:04.651540 25943 solver.cpp:290] Iteration 22900 (5.79886 iter/s, 17.2448s/100 iter), loss = 0.0181543
I0711 21:26:04.651567 25943 solver.cpp:309]     Train net output #0: loss = 0.0181543 (* 1 = 0.0181543 loss)
I0711 21:26:04.651574 25943 sgd_solver.cpp:106] Iteration 22900, lr = 1e-05
I0711 21:26:21.904258 25943 solver.cpp:290] Iteration 23000 (5.79636 iter/s, 17.2522s/100 iter), loss = 0.0244345
I0711 21:26:21.904350 25943 solver.cpp:309]     Train net output #0: loss = 0.0244344 (* 1 = 0.0244344 loss)
I0711 21:26:21.904361 25943 sgd_solver.cpp:106] Iteration 23000, lr = 1e-05
I0711 21:26:39.047128 25943 solver.cpp:290] Iteration 23100 (5.83352 iter/s, 17.1423s/100 iter), loss = 0.0236133
I0711 21:26:39.047150 25943 solver.cpp:309]     Train net output #0: loss = 0.0236133 (* 1 = 0.0236133 loss)
I0711 21:26:39.047158 25943 sgd_solver.cpp:106] Iteration 23100, lr = 1e-05
I0711 21:26:56.133622 25943 solver.cpp:290] Iteration 23200 (5.85274 iter/s, 17.086s/100 iter), loss = 0.0201395
I0711 21:26:56.133673 25943 solver.cpp:309]     Train net output #0: loss = 0.0201394 (* 1 = 0.0201394 loss)
I0711 21:26:56.133683 25943 sgd_solver.cpp:106] Iteration 23200, lr = 1e-05
I0711 21:27:13.310495 25943 solver.cpp:290] Iteration 23300 (5.82195 iter/s, 17.1764s/100 iter), loss = 0.0170816
I0711 21:27:13.310518 25943 solver.cpp:309]     Train net output #0: loss = 0.0170815 (* 1 = 0.0170815 loss)
I0711 21:27:13.310524 25943 sgd_solver.cpp:106] Iteration 23300, lr = 1e-05
I0711 21:27:30.372288 25943 solver.cpp:290] Iteration 23400 (5.86121 iter/s, 17.0613s/100 iter), loss = 0.0288858
I0711 21:27:30.372349 25943 solver.cpp:309]     Train net output #0: loss = 0.0288857 (* 1 = 0.0288857 loss)
I0711 21:27:30.372357 25943 sgd_solver.cpp:106] Iteration 23400, lr = 1e-05
I0711 21:27:47.410856 25943 solver.cpp:290] Iteration 23500 (5.86922 iter/s, 17.0381s/100 iter), loss = 0.0211866
I0711 21:27:47.410882 25943 solver.cpp:309]     Train net output #0: loss = 0.0211865 (* 1 = 0.0211865 loss)
I0711 21:27:47.410889 25943 sgd_solver.cpp:106] Iteration 23500, lr = 1e-05
I0711 21:28:04.365897 25943 solver.cpp:290] Iteration 23600 (5.89812 iter/s, 16.9546s/100 iter), loss = 0.02923
I0711 21:28:04.365977 25943 solver.cpp:309]     Train net output #0: loss = 0.02923 (* 1 = 0.02923 loss)
I0711 21:28:04.365983 25943 sgd_solver.cpp:106] Iteration 23600, lr = 1e-05
I0711 21:28:21.517388 25943 solver.cpp:290] Iteration 23700 (5.83058 iter/s, 17.1509s/100 iter), loss = 0.0228848
I0711 21:28:21.517413 25943 solver.cpp:309]     Train net output #0: loss = 0.0228848 (* 1 = 0.0228848 loss)
I0711 21:28:21.517421 25943 sgd_solver.cpp:106] Iteration 23700, lr = 1e-05
I0711 21:28:38.679882 25943 solver.cpp:290] Iteration 23800 (5.82682 iter/s, 17.162s/100 iter), loss = 0.0297706
I0711 21:28:38.679996 25943 solver.cpp:309]     Train net output #0: loss = 0.0297706 (* 1 = 0.0297706 loss)
I0711 21:28:38.680022 25943 sgd_solver.cpp:106] Iteration 23800, lr = 1e-05
I0711 21:28:55.734840 25943 solver.cpp:290] Iteration 23900 (5.86359 iter/s, 17.0544s/100 iter), loss = 0.0186841
I0711 21:28:55.734866 25943 solver.cpp:309]     Train net output #0: loss = 0.0186841 (* 1 = 0.0186841 loss)
I0711 21:28:55.734875 25943 sgd_solver.cpp:106] Iteration 23900, lr = 1e-05
I0711 21:29:12.533357 25943 solver.cpp:467] Iteration 24000, Testing net (#0)
I0711 21:29:59.716284 25943 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.95235
I0711 21:29:59.716329 25943 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999729
I0711 21:29:59.716336 25943 solver.cpp:540]     Test net output #2: loss = 0.166549 (* 1 = 0.166549 loss)
I0711 21:29:59.905439 26106 sgd_solver.cpp:46] MultiStep Status: Iteration 24000, step = 1
I0711 21:29:59.905433 25943 solver.cpp:290] Iteration 24000 (1.55839 iter/s, 64.1689s/100 iter), loss = 0.0213653
I0711 21:29:59.905439 26105 sgd_solver.cpp:46] MultiStep Status: Iteration 24000, step = 1
I0711 21:29:59.905470 25943 solver.cpp:309]     Train net output #0: loss = 0.0213652 (* 1 = 0.0213652 loss)
I0711 21:29:59.905481 25943 sgd_solver.cpp:46] MultiStep Status: Iteration 24000, step = 1
I0711 21:29:59.905486 25943 sgd_solver.cpp:106] Iteration 24000, lr = 1e-06
I0711 21:30:16.680512 25943 solver.cpp:290] Iteration 24100 (5.9614 iter/s, 16.7746s/100 iter), loss = 0.0192085
I0711 21:30:16.680536 25943 solver.cpp:309]     Train net output #0: loss = 0.0192085 (* 1 = 0.0192085 loss)
I0711 21:30:16.680543 25943 sgd_solver.cpp:106] Iteration 24100, lr = 1e-06
I0711 21:30:33.829064 25943 solver.cpp:290] Iteration 24200 (5.83156 iter/s, 17.1481s/100 iter), loss = 0.0318975
I0711 21:30:33.829712 25943 solver.cpp:309]     Train net output #0: loss = 0.0318975 (* 1 = 0.0318975 loss)
I0711 21:30:33.829736 25943 sgd_solver.cpp:106] Iteration 24200, lr = 1e-06
I0711 21:30:50.966624 25943 solver.cpp:290] Iteration 24300 (5.83551 iter/s, 17.1364s/100 iter), loss = 0.0290704
I0711 21:30:50.966653 25943 solver.cpp:309]     Train net output #0: loss = 0.0290704 (* 1 = 0.0290704 loss)
I0711 21:30:50.966662 25943 sgd_solver.cpp:106] Iteration 24300, lr = 1e-06
I0711 21:31:08.186177 25943 solver.cpp:290] Iteration 24400 (5.80752 iter/s, 17.2191s/100 iter), loss = 0.0149766
I0711 21:31:08.186228 25943 solver.cpp:309]     Train net output #0: loss = 0.0149766 (* 1 = 0.0149766 loss)
I0711 21:31:08.186235 25943 sgd_solver.cpp:106] Iteration 24400, lr = 1e-06
I0711 21:31:25.302429 25943 solver.cpp:290] Iteration 24500 (5.84258 iter/s, 17.1157s/100 iter), loss = 0.0211232
I0711 21:31:25.302456 25943 solver.cpp:309]     Train net output #0: loss = 0.0211232 (* 1 = 0.0211232 loss)
I0711 21:31:25.302462 25943 sgd_solver.cpp:106] Iteration 24500, lr = 1e-06
I0711 21:31:42.741037 25943 solver.cpp:290] Iteration 24600 (5.73457 iter/s, 17.4381s/100 iter), loss = 0.0329412
I0711 21:31:42.741117 25943 solver.cpp:309]     Train net output #0: loss = 0.0329412 (* 1 = 0.0329412 loss)
I0711 21:31:42.741125 25943 sgd_solver.cpp:106] Iteration 24600, lr = 1e-06
I0711 21:31:59.768709 25943 solver.cpp:290] Iteration 24700 (5.87298 iter/s, 17.0271s/100 iter), loss = 0.0214737
I0711 21:31:59.768734 25943 solver.cpp:309]     Train net output #0: loss = 0.0214736 (* 1 = 0.0214736 loss)
I0711 21:31:59.768741 25943 sgd_solver.cpp:106] Iteration 24700, lr = 1e-06
I0711 21:32:16.897677 25943 solver.cpp:290] Iteration 24800 (5.83823 iter/s, 17.1285s/100 iter), loss = 0.0198339
I0711 21:32:16.897774 25943 solver.cpp:309]     Train net output #0: loss = 0.0198339 (* 1 = 0.0198339 loss)
I0711 21:32:16.897783 25943 sgd_solver.cpp:106] Iteration 24800, lr = 1e-06
I0711 21:32:33.944886 25943 solver.cpp:290] Iteration 24900 (5.86625 iter/s, 17.0467s/100 iter), loss = 0.0461802
I0711 21:32:33.944911 25943 solver.cpp:309]     Train net output #0: loss = 0.0461802 (* 1 = 0.0461802 loss)
I0711 21:32:33.944917 25943 sgd_solver.cpp:106] Iteration 24900, lr = 1e-06
I0711 21:32:50.902076 25943 solver.cpp:290] Iteration 25000 (5.89737 iter/s, 16.9567s/100 iter), loss = 0.0267457
I0711 21:32:50.902204 25943 solver.cpp:309]     Train net output #0: loss = 0.0267457 (* 1 = 0.0267457 loss)
I0711 21:32:50.902214 25943 sgd_solver.cpp:106] Iteration 25000, lr = 1e-06
I0711 21:33:08.086933 25943 solver.cpp:290] Iteration 25100 (5.81928 iter/s, 17.1843s/100 iter), loss = 0.0227588
I0711 21:33:08.086958 25943 solver.cpp:309]     Train net output #0: loss = 0.0227588 (* 1 = 0.0227588 loss)
I0711 21:33:08.086966 25943 sgd_solver.cpp:106] Iteration 25100, lr = 1e-06
I0711 21:33:25.116335 25943 solver.cpp:290] Iteration 25200 (5.87236 iter/s, 17.0289s/100 iter), loss = 0.0601136
I0711 21:33:25.116389 25943 solver.cpp:309]     Train net output #0: loss = 0.0601136 (* 1 = 0.0601136 loss)
I0711 21:33:25.116400 25943 sgd_solver.cpp:106] Iteration 25200, lr = 1e-06
I0711 21:33:42.289834 25943 solver.cpp:290] Iteration 25300 (5.8231 iter/s, 17.173s/100 iter), loss = 0.0201969
I0711 21:33:42.289860 25943 solver.cpp:309]     Train net output #0: loss = 0.0201969 (* 1 = 0.0201969 loss)
I0711 21:33:42.289867 25943 sgd_solver.cpp:106] Iteration 25300, lr = 1e-06
I0711 21:33:59.397464 25943 solver.cpp:290] Iteration 25400 (5.84551 iter/s, 17.1071s/100 iter), loss = 0.0243403
I0711 21:33:59.397543 25943 solver.cpp:309]     Train net output #0: loss = 0.0243403 (* 1 = 0.0243403 loss)
I0711 21:33:59.397554 25943 sgd_solver.cpp:106] Iteration 25400, lr = 1e-06
I0711 21:34:16.499430 25943 solver.cpp:290] Iteration 25500 (5.84746 iter/s, 17.1014s/100 iter), loss = 0.0197478
I0711 21:34:16.499455 25943 solver.cpp:309]     Train net output #0: loss = 0.0197478 (* 1 = 0.0197478 loss)
I0711 21:34:16.499461 25943 sgd_solver.cpp:106] Iteration 25500, lr = 1e-06
I0711 21:34:33.613507 25943 solver.cpp:290] Iteration 25600 (5.84331 iter/s, 17.1136s/100 iter), loss = 0.0307461
I0711 21:34:33.613579 25943 solver.cpp:309]     Train net output #0: loss = 0.0307461 (* 1 = 0.0307461 loss)
I0711 21:34:33.613587 25943 sgd_solver.cpp:106] Iteration 25600, lr = 1e-06
I0711 21:34:51.061378 25943 solver.cpp:290] Iteration 25700 (5.73154 iter/s, 17.4473s/100 iter), loss = 0.0211181
I0711 21:34:51.061403 25943 solver.cpp:309]     Train net output #0: loss = 0.0211181 (* 1 = 0.0211181 loss)
I0711 21:34:51.061408 25943 sgd_solver.cpp:106] Iteration 25700, lr = 1e-06
I0711 21:35:08.198822 25943 solver.cpp:290] Iteration 25800 (5.83534 iter/s, 17.137s/100 iter), loss = 0.030157
I0711 21:35:08.198875 25943 solver.cpp:309]     Train net output #0: loss = 0.030157 (* 1 = 0.030157 loss)
I0711 21:35:08.198885 25943 sgd_solver.cpp:106] Iteration 25800, lr = 1e-06
I0711 21:35:25.393620 25943 solver.cpp:290] Iteration 25900 (5.81589 iter/s, 17.1943s/100 iter), loss = 0.0165308
I0711 21:35:25.393647 25943 solver.cpp:309]     Train net output #0: loss = 0.0165308 (* 1 = 0.0165308 loss)
I0711 21:35:25.393656 25943 sgd_solver.cpp:106] Iteration 25900, lr = 1e-06
I0711 21:35:42.346119 25943 solver.cpp:467] Iteration 26000, Testing net (#0)
I0711 21:36:28.713353 25943 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.953353
I0711 21:36:28.713435 25943 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999623
I0711 21:36:28.713443 25943 solver.cpp:540]     Test net output #2: loss = 0.165946 (* 1 = 0.165946 loss)
I0711 21:36:28.894820 25943 solver.cpp:290] Iteration 26000 (1.57482 iter/s, 63.4995s/100 iter), loss = 0.020636
I0711 21:36:28.894843 25943 solver.cpp:309]     Train net output #0: loss = 0.020636 (* 1 = 0.020636 loss)
I0711 21:36:28.894850 25943 sgd_solver.cpp:106] Iteration 26000, lr = 1e-06
I0711 21:36:45.676606 25943 solver.cpp:290] Iteration 26100 (5.95901 iter/s, 16.7813s/100 iter), loss = 0.0201982
I0711 21:36:45.676631 25943 solver.cpp:309]     Train net output #0: loss = 0.0201982 (* 1 = 0.0201982 loss)
I0711 21:36:45.676637 25943 sgd_solver.cpp:106] Iteration 26100, lr = 1e-06
I0711 21:37:02.683965 25943 solver.cpp:290] Iteration 26200 (5.87998 iter/s, 17.0069s/100 iter), loss = 0.0258706
I0711 21:37:02.684077 25943 solver.cpp:309]     Train net output #0: loss = 0.0258706 (* 1 = 0.0258706 loss)
I0711 21:37:02.684088 25943 sgd_solver.cpp:106] Iteration 26200, lr = 1e-06
I0711 21:37:19.588196 25943 solver.cpp:290] Iteration 26300 (5.91588 iter/s, 16.9037s/100 iter), loss = 0.0208248
I0711 21:37:19.588224 25943 solver.cpp:309]     Train net output #0: loss = 0.0208248 (* 1 = 0.0208248 loss)
I0711 21:37:19.588233 25943 sgd_solver.cpp:106] Iteration 26300, lr = 1e-06
I0711 21:37:36.680995 25943 solver.cpp:290] Iteration 26400 (5.85059 iter/s, 17.0923s/100 iter), loss = 0.0190843
I0711 21:37:36.681104 25943 solver.cpp:309]     Train net output #0: loss = 0.0190843 (* 1 = 0.0190843 loss)
I0711 21:37:36.681114 25943 sgd_solver.cpp:106] Iteration 26400, lr = 1e-06
I0711 21:37:53.794028 25943 solver.cpp:290] Iteration 26500 (5.8437 iter/s, 17.1125s/100 iter), loss = 0.0260022
I0711 21:37:53.794052 25943 solver.cpp:309]     Train net output #0: loss = 0.0260022 (* 1 = 0.0260022 loss)
I0711 21:37:53.794059 25943 sgd_solver.cpp:106] Iteration 26500, lr = 1e-06
I0711 21:38:10.790796 25943 solver.cpp:290] Iteration 26600 (5.88364 iter/s, 16.9963s/100 iter), loss = 0.036152
I0711 21:38:10.790853 25943 solver.cpp:309]     Train net output #0: loss = 0.036152 (* 1 = 0.036152 loss)
I0711 21:38:10.790863 25943 sgd_solver.cpp:106] Iteration 26600, lr = 1e-06
I0711 21:38:28.402905 25943 solver.cpp:290] Iteration 26700 (5.67808 iter/s, 17.6116s/100 iter), loss = 0.0186075
I0711 21:38:28.402938 25943 solver.cpp:309]     Train net output #0: loss = 0.0186075 (* 1 = 0.0186075 loss)
I0711 21:38:28.402947 25943 sgd_solver.cpp:106] Iteration 26700, lr = 1e-06
I0711 21:38:46.076254 25943 solver.cpp:290] Iteration 26800 (5.6584 iter/s, 17.6728s/100 iter), loss = 0.036938
I0711 21:38:46.076335 25943 solver.cpp:309]     Train net output #0: loss = 0.036938 (* 1 = 0.036938 loss)
I0711 21:38:46.076345 25943 sgd_solver.cpp:106] Iteration 26800, lr = 1e-06
I0711 21:39:03.626986 25943 solver.cpp:290] Iteration 26900 (5.69795 iter/s, 17.5502s/100 iter), loss = 0.0278506
I0711 21:39:03.627013 25943 solver.cpp:309]     Train net output #0: loss = 0.0278506 (* 1 = 0.0278506 loss)
I0711 21:39:03.627022 25943 sgd_solver.cpp:106] Iteration 26900, lr = 1e-06
I0711 21:39:20.909793 25943 solver.cpp:290] Iteration 27000 (5.78626 iter/s, 17.2823s/100 iter), loss = 0.0172372
I0711 21:39:20.909862 25943 solver.cpp:309]     Train net output #0: loss = 0.0172372 (* 1 = 0.0172372 loss)
I0711 21:39:20.909871 25943 sgd_solver.cpp:106] Iteration 27000, lr = 1e-06
I0711 21:39:38.461256 25943 solver.cpp:290] Iteration 27100 (5.69771 iter/s, 17.5509s/100 iter), loss = 0.0162051
I0711 21:39:38.461284 25943 solver.cpp:309]     Train net output #0: loss = 0.0162051 (* 1 = 0.0162051 loss)
I0711 21:39:38.461293 25943 sgd_solver.cpp:106] Iteration 27100, lr = 1e-06
I0711 21:39:56.000830 25943 solver.cpp:290] Iteration 27200 (5.70156 iter/s, 17.5391s/100 iter), loss = 0.0246519
I0711 21:39:56.000869 25943 solver.cpp:309]     Train net output #0: loss = 0.0246519 (* 1 = 0.0246519 loss)
I0711 21:39:56.000876 25943 sgd_solver.cpp:106] Iteration 27200, lr = 1e-06
I0711 21:40:13.459985 25943 solver.cpp:290] Iteration 27300 (5.72782 iter/s, 17.4586s/100 iter), loss = 0.0281194
I0711 21:40:13.460011 25943 solver.cpp:309]     Train net output #0: loss = 0.0281194 (* 1 = 0.0281194 loss)
I0711 21:40:13.460018 25943 sgd_solver.cpp:106] Iteration 27300, lr = 1e-06
I0711 21:40:30.977088 25943 solver.cpp:290] Iteration 27400 (5.70887 iter/s, 17.5166s/100 iter), loss = 0.0182657
I0711 21:40:30.977197 25943 solver.cpp:309]     Train net output #0: loss = 0.0182657 (* 1 = 0.0182657 loss)
I0711 21:40:30.977206 25943 sgd_solver.cpp:106] Iteration 27400, lr = 1e-06
I0711 21:40:48.362385 25943 solver.cpp:290] Iteration 27500 (5.75218 iter/s, 17.3847s/100 iter), loss = 0.0198907
I0711 21:40:48.362409 25943 solver.cpp:309]     Train net output #0: loss = 0.0198907 (* 1 = 0.0198907 loss)
I0711 21:40:48.362416 25943 sgd_solver.cpp:106] Iteration 27500, lr = 1e-06
I0711 21:41:05.745446 25943 solver.cpp:290] Iteration 27600 (5.75289 iter/s, 17.3826s/100 iter), loss = 0.0210297
I0711 21:41:05.745571 25943 solver.cpp:309]     Train net output #0: loss = 0.0210297 (* 1 = 0.0210297 loss)
I0711 21:41:05.745581 25943 sgd_solver.cpp:106] Iteration 27600, lr = 1e-06
I0711 21:41:23.190008 25943 solver.cpp:290] Iteration 27700 (5.73264 iter/s, 17.444s/100 iter), loss = 0.0386934
I0711 21:41:23.190033 25943 solver.cpp:309]     Train net output #0: loss = 0.0386934 (* 1 = 0.0386934 loss)
I0711 21:41:23.190042 25943 sgd_solver.cpp:106] Iteration 27700, lr = 1e-06
I0711 21:41:40.591486 25943 solver.cpp:290] Iteration 27800 (5.7468 iter/s, 17.401s/100 iter), loss = 0.0340361
I0711 21:41:40.591588 25943 solver.cpp:309]     Train net output #0: loss = 0.0340361 (* 1 = 0.0340361 loss)
I0711 21:41:40.591600 25943 sgd_solver.cpp:106] Iteration 27800, lr = 1e-06
I0711 21:41:58.073849 25943 solver.cpp:290] Iteration 27900 (5.72024 iter/s, 17.4818s/100 iter), loss = 0.0388233
I0711 21:41:58.073873 25943 solver.cpp:309]     Train net output #0: loss = 0.0388233 (* 1 = 0.0388233 loss)
I0711 21:41:58.073879 25943 sgd_solver.cpp:106] Iteration 27900, lr = 1e-06
I0711 21:42:15.356613 25943 solver.cpp:467] Iteration 28000, Testing net (#0)
I0711 21:43:05.083967 25943 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.953367
I0711 21:43:05.084040 25943 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999601
I0711 21:43:05.084049 25943 solver.cpp:540]     Test net output #2: loss = 0.167421 (* 1 = 0.167421 loss)
I0711 21:43:05.286123 25943 solver.cpp:290] Iteration 28000 (1.48786 iter/s, 67.2104s/100 iter), loss = 0.0209917
I0711 21:43:05.286146 25943 solver.cpp:309]     Train net output #0: loss = 0.0209917 (* 1 = 0.0209917 loss)
I0711 21:43:05.286152 25943 sgd_solver.cpp:106] Iteration 28000, lr = 1e-06
I0711 21:43:22.751329 25943 solver.cpp:290] Iteration 28100 (5.72584 iter/s, 17.4647s/100 iter), loss = 0.0392563
I0711 21:43:22.751381 25943 solver.cpp:309]     Train net output #0: loss = 0.0392563 (* 1 = 0.0392563 loss)
I0711 21:43:22.751395 25943 sgd_solver.cpp:106] Iteration 28100, lr = 1e-06
I0711 21:43:40.278662 25943 solver.cpp:290] Iteration 28200 (5.70555 iter/s, 17.5268s/100 iter), loss = 0.0238705
I0711 21:43:40.278710 25943 solver.cpp:309]     Train net output #0: loss = 0.0238705 (* 1 = 0.0238705 loss)
I0711 21:43:40.278720 25943 sgd_solver.cpp:106] Iteration 28200, lr = 1e-06
I0711 21:43:57.328510 25943 solver.cpp:290] Iteration 28300 (5.86533 iter/s, 17.0493s/100 iter), loss = 0.0234409
I0711 21:43:57.328538 25943 solver.cpp:309]     Train net output #0: loss = 0.0234409 (* 1 = 0.0234409 loss)
I0711 21:43:57.328547 25943 sgd_solver.cpp:106] Iteration 28300, lr = 1e-06
I0711 21:44:14.852725 25943 solver.cpp:290] Iteration 28400 (5.70656 iter/s, 17.5237s/100 iter), loss = 0.0176735
I0711 21:44:14.852787 25943 solver.cpp:309]     Train net output #0: loss = 0.0176735 (* 1 = 0.0176735 loss)
I0711 21:44:14.852798 25943 sgd_solver.cpp:106] Iteration 28400, lr = 1e-06
I0711 21:44:32.207509 25943 solver.cpp:290] Iteration 28500 (5.76228 iter/s, 17.3543s/100 iter), loss = 0.0183483
I0711 21:44:32.207536 25943 solver.cpp:309]     Train net output #0: loss = 0.0183482 (* 1 = 0.0183482 loss)
I0711 21:44:32.207545 25943 sgd_solver.cpp:106] Iteration 28500, lr = 1e-06
I0711 21:44:49.703330 25943 solver.cpp:290] Iteration 28600 (5.71582 iter/s, 17.4953s/100 iter), loss = 0.0224893
I0711 21:44:49.703702 25943 solver.cpp:309]     Train net output #0: loss = 0.0224893 (* 1 = 0.0224893 loss)
I0711 21:44:49.703709 25943 sgd_solver.cpp:106] Iteration 28600, lr = 1e-06
I0711 21:45:06.898773 25943 solver.cpp:290] Iteration 28700 (5.81578 iter/s, 17.1946s/100 iter), loss = 0.0246763
I0711 21:45:06.898815 25943 solver.cpp:309]     Train net output #0: loss = 0.0246763 (* 1 = 0.0246763 loss)
I0711 21:45:06.898828 25943 sgd_solver.cpp:106] Iteration 28700, lr = 1e-06
I0711 21:45:24.236251 25943 solver.cpp:290] Iteration 28800 (5.76802 iter/s, 17.337s/100 iter), loss = 0.01676
I0711 21:45:24.236359 25943 solver.cpp:309]     Train net output #0: loss = 0.01676 (* 1 = 0.01676 loss)
I0711 21:45:24.236371 25943 sgd_solver.cpp:106] Iteration 28800, lr = 1e-06
I0711 21:45:41.705723 25943 solver.cpp:290] Iteration 28900 (5.72446 iter/s, 17.4689s/100 iter), loss = 0.0236212
I0711 21:45:41.705746 25943 solver.cpp:309]     Train net output #0: loss = 0.0236212 (* 1 = 0.0236212 loss)
I0711 21:45:41.705752 25943 sgd_solver.cpp:106] Iteration 28900, lr = 1e-06
I0711 21:45:59.040514 25943 solver.cpp:290] Iteration 29000 (5.76891 iter/s, 17.3343s/100 iter), loss = 0.0189185
I0711 21:45:59.040594 25943 solver.cpp:309]     Train net output #0: loss = 0.0189185 (* 1 = 0.0189185 loss)
I0711 21:45:59.040602 25943 sgd_solver.cpp:106] Iteration 29000, lr = 1e-06
I0711 21:46:16.488613 25943 solver.cpp:290] Iteration 29100 (5.73146 iter/s, 17.4475s/100 iter), loss = 0.018771
I0711 21:46:16.488637 25943 solver.cpp:309]     Train net output #0: loss = 0.018771 (* 1 = 0.018771 loss)
I0711 21:46:16.488644 25943 sgd_solver.cpp:106] Iteration 29100, lr = 1e-06
I0711 21:46:34.117612 25943 solver.cpp:290] Iteration 29200 (5.67263 iter/s, 17.6285s/100 iter), loss = 0.01425
I0711 21:46:34.117689 25943 solver.cpp:309]     Train net output #0: loss = 0.01425 (* 1 = 0.01425 loss)
I0711 21:46:34.117697 25943 sgd_solver.cpp:106] Iteration 29200, lr = 1e-06
I0711 21:46:51.525640 25943 solver.cpp:290] Iteration 29300 (5.74466 iter/s, 17.4075s/100 iter), loss = 0.0384129
I0711 21:46:51.525671 25943 solver.cpp:309]     Train net output #0: loss = 0.0384129 (* 1 = 0.0384129 loss)
I0711 21:46:51.525679 25943 sgd_solver.cpp:106] Iteration 29300, lr = 1e-06
I0711 21:47:08.910213 25943 solver.cpp:290] Iteration 29400 (5.75239 iter/s, 17.3841s/100 iter), loss = 0.0276954
I0711 21:47:08.910303 25943 solver.cpp:309]     Train net output #0: loss = 0.0276954 (* 1 = 0.0276954 loss)
I0711 21:47:08.910315 25943 sgd_solver.cpp:106] Iteration 29400, lr = 1e-06
I0711 21:47:26.214078 25943 solver.cpp:290] Iteration 29500 (5.77924 iter/s, 17.3033s/100 iter), loss = 0.0209448
I0711 21:47:26.214100 25943 solver.cpp:309]     Train net output #0: loss = 0.0209448 (* 1 = 0.0209448 loss)
I0711 21:47:26.214107 25943 sgd_solver.cpp:106] Iteration 29500, lr = 1e-06
I0711 21:47:43.621053 25943 solver.cpp:290] Iteration 29600 (5.74499 iter/s, 17.4065s/100 iter), loss = 0.0419535
I0711 21:47:43.621109 25943 solver.cpp:309]     Train net output #0: loss = 0.0419535 (* 1 = 0.0419535 loss)
I0711 21:47:43.621116 25943 sgd_solver.cpp:106] Iteration 29600, lr = 1e-06
I0711 21:48:00.959818 25943 solver.cpp:290] Iteration 29700 (5.7676 iter/s, 17.3382s/100 iter), loss = 0.033433
I0711 21:48:00.959843 25943 solver.cpp:309]     Train net output #0: loss = 0.033433 (* 1 = 0.033433 loss)
I0711 21:48:00.959851 25943 sgd_solver.cpp:106] Iteration 29700, lr = 1e-06
I0711 21:48:18.601085 25943 solver.cpp:290] Iteration 29800 (5.66869 iter/s, 17.6408s/100 iter), loss = 0.0203752
I0711 21:48:18.601205 25943 solver.cpp:309]     Train net output #0: loss = 0.0203752 (* 1 = 0.0203752 loss)
I0711 21:48:18.601240 25943 sgd_solver.cpp:106] Iteration 29800, lr = 1e-06
I0711 21:48:35.931627 25943 solver.cpp:290] Iteration 29900 (5.77036 iter/s, 17.33s/100 iter), loss = 0.0252933
I0711 21:48:35.931654 25943 solver.cpp:309]     Train net output #0: loss = 0.0252933 (* 1 = 0.0252933 loss)
I0711 21:48:35.931663 25943 sgd_solver.cpp:106] Iteration 29900, lr = 1e-06
I0711 21:48:53.380795 25943 solver.cpp:594] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/l1reg/cityscapes5_jsegnet21v2_iter_30000.caffemodel
I0711 21:48:53.464057 25943 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/l1reg/cityscapes5_jsegnet21v2_iter_30000.solverstate
I0711 21:48:53.497918 25943 solver.cpp:467] Iteration 30000, Testing net (#0)
I0711 21:49:44.728150 25943 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.953073
I0711 21:49:44.728221 25943 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999642
I0711 21:49:44.728229 25943 solver.cpp:540]     Test net output #2: loss = 0.167014 (* 1 = 0.167014 loss)
I0711 21:49:44.918792 25943 solver.cpp:290] Iteration 30000 (1.44958 iter/s, 68.9853s/100 iter), loss = 0.0174868
I0711 21:49:44.918817 25943 solver.cpp:309]     Train net output #0: loss = 0.0174868 (* 1 = 0.0174868 loss)
I0711 21:49:44.918823 25943 sgd_solver.cpp:106] Iteration 30000, lr = 1e-06
I0711 21:50:02.162859 25943 solver.cpp:290] Iteration 30100 (5.79926 iter/s, 17.2436s/100 iter), loss = 0.0282022
I0711 21:50:02.162933 25943 solver.cpp:309]     Train net output #0: loss = 0.0282022 (* 1 = 0.0282022 loss)
I0711 21:50:02.162952 25943 sgd_solver.cpp:106] Iteration 30100, lr = 1e-06
I0711 21:50:19.784894 25943 solver.cpp:290] Iteration 30200 (5.67489 iter/s, 17.6215s/100 iter), loss = 0.0409297
I0711 21:50:19.784989 25943 solver.cpp:309]     Train net output #0: loss = 0.0409297 (* 1 = 0.0409297 loss)
I0711 21:50:19.785001 25943 sgd_solver.cpp:106] Iteration 30200, lr = 1e-06
I0711 21:50:37.266784 25943 solver.cpp:290] Iteration 30300 (5.72039 iter/s, 17.4813s/100 iter), loss = 0.0354923
I0711 21:50:37.266808 25943 solver.cpp:309]     Train net output #0: loss = 0.0354923 (* 1 = 0.0354923 loss)
I0711 21:50:37.266813 25943 sgd_solver.cpp:106] Iteration 30300, lr = 1e-06
I0711 21:50:54.205816 25943 solver.cpp:290] Iteration 30400 (5.9037 iter/s, 16.9385s/100 iter), loss = 0.020506
I0711 21:50:54.206663 25943 solver.cpp:309]     Train net output #0: loss = 0.020506 (* 1 = 0.020506 loss)
I0711 21:50:54.206679 25943 sgd_solver.cpp:106] Iteration 30400, lr = 1e-06
I0711 21:51:11.236852 25943 solver.cpp:290] Iteration 30500 (5.87208 iter/s, 17.0297s/100 iter), loss = 0.0163799
I0711 21:51:11.236876 25943 solver.cpp:309]     Train net output #0: loss = 0.0163799 (* 1 = 0.0163799 loss)
I0711 21:51:11.236886 25943 sgd_solver.cpp:106] Iteration 30500, lr = 1e-06
I0711 21:51:28.416508 25943 solver.cpp:290] Iteration 30600 (5.821 iter/s, 17.1792s/100 iter), loss = 0.0177153
I0711 21:51:28.416594 25943 solver.cpp:309]     Train net output #0: loss = 0.0177153 (* 1 = 0.0177153 loss)
I0711 21:51:28.416605 25943 sgd_solver.cpp:106] Iteration 30600, lr = 1e-06
I0711 21:51:45.430645 25943 solver.cpp:290] Iteration 30700 (5.87765 iter/s, 17.0136s/100 iter), loss = 0.0227873
I0711 21:51:45.430670 25943 solver.cpp:309]     Train net output #0: loss = 0.0227873 (* 1 = 0.0227873 loss)
I0711 21:51:45.430676 25943 sgd_solver.cpp:106] Iteration 30700, lr = 1e-06
I0711 21:52:02.395417 25943 solver.cpp:290] Iteration 30800 (5.89474 iter/s, 16.9643s/100 iter), loss = 0.0213565
I0711 21:52:02.395473 25943 solver.cpp:309]     Train net output #0: loss = 0.0213564 (* 1 = 0.0213564 loss)
I0711 21:52:02.395481 25943 sgd_solver.cpp:106] Iteration 30800, lr = 1e-06
I0711 21:52:19.436425 25943 solver.cpp:290] Iteration 30900 (5.86838 iter/s, 17.0405s/100 iter), loss = 0.0441528
I0711 21:52:19.436451 25943 solver.cpp:309]     Train net output #0: loss = 0.0441528 (* 1 = 0.0441528 loss)
I0711 21:52:19.436458 25943 sgd_solver.cpp:106] Iteration 30900, lr = 1e-06
I0711 21:52:36.382746 25943 solver.cpp:290] Iteration 31000 (5.90116 iter/s, 16.9458s/100 iter), loss = 0.0198541
I0711 21:52:36.382800 25943 solver.cpp:309]     Train net output #0: loss = 0.0198541 (* 1 = 0.0198541 loss)
I0711 21:52:36.382808 25943 sgd_solver.cpp:106] Iteration 31000, lr = 1e-06
I0711 21:52:53.287277 25943 solver.cpp:290] Iteration 31100 (5.91575 iter/s, 16.904s/100 iter), loss = 0.0268283
I0711 21:52:53.287302 25943 solver.cpp:309]     Train net output #0: loss = 0.0268283 (* 1 = 0.0268283 loss)
I0711 21:52:53.287308 25943 sgd_solver.cpp:106] Iteration 31100, lr = 1e-06
I0711 21:53:10.507638 25943 solver.cpp:290] Iteration 31200 (5.80724 iter/s, 17.2199s/100 iter), loss = 0.025954
I0711 21:53:10.507709 25943 solver.cpp:309]     Train net output #0: loss = 0.025954 (* 1 = 0.025954 loss)
I0711 21:53:10.507716 25943 sgd_solver.cpp:106] Iteration 31200, lr = 1e-06
I0711 21:53:27.581179 25943 solver.cpp:290] Iteration 31300 (5.8572 iter/s, 17.073s/100 iter), loss = 0.0397632
I0711 21:53:27.581208 25943 solver.cpp:309]     Train net output #0: loss = 0.0397631 (* 1 = 0.0397631 loss)
I0711 21:53:27.581218 25943 sgd_solver.cpp:106] Iteration 31300, lr = 1e-06
I0711 21:53:44.498623 25943 solver.cpp:290] Iteration 31400 (5.91123 iter/s, 16.917s/100 iter), loss = 0.0259564
I0711 21:53:44.498714 25943 solver.cpp:309]     Train net output #0: loss = 0.0259564 (* 1 = 0.0259564 loss)
I0711 21:53:44.498723 25943 sgd_solver.cpp:106] Iteration 31400, lr = 1e-06
I0711 21:54:01.403022 25943 solver.cpp:290] Iteration 31500 (5.91581 iter/s, 16.9039s/100 iter), loss = 0.0163565
I0711 21:54:01.403046 25943 solver.cpp:309]     Train net output #0: loss = 0.0163564 (* 1 = 0.0163564 loss)
I0711 21:54:01.403053 25943 sgd_solver.cpp:106] Iteration 31500, lr = 1e-06
I0711 21:54:18.570349 25943 solver.cpp:290] Iteration 31600 (5.82518 iter/s, 17.1668s/100 iter), loss = 0.0293581
I0711 21:54:18.570399 25943 solver.cpp:309]     Train net output #0: loss = 0.029358 (* 1 = 0.029358 loss)
I0711 21:54:18.570406 25943 sgd_solver.cpp:106] Iteration 31600, lr = 1e-06
I0711 21:54:35.520968 25943 solver.cpp:290] Iteration 31700 (5.89967 iter/s, 16.9501s/100 iter), loss = 0.0335596
I0711 21:54:35.520992 25943 solver.cpp:309]     Train net output #0: loss = 0.0335596 (* 1 = 0.0335596 loss)
I0711 21:54:35.520998 25943 sgd_solver.cpp:106] Iteration 31700, lr = 1e-06
I0711 21:54:52.613523 25943 solver.cpp:290] Iteration 31800 (5.85067 iter/s, 17.0921s/100 iter), loss = 0.0180552
I0711 21:54:52.613569 25943 solver.cpp:309]     Train net output #0: loss = 0.0180552 (* 1 = 0.0180552 loss)
I0711 21:54:52.613575 25943 sgd_solver.cpp:106] Iteration 31800, lr = 1e-06
I0711 21:55:09.732168 25943 solver.cpp:290] Iteration 31900 (5.84176 iter/s, 17.1181s/100 iter), loss = 0.0356738
I0711 21:55:09.732192 25943 solver.cpp:309]     Train net output #0: loss = 0.0356738 (* 1 = 0.0356738 loss)
I0711 21:55:09.732198 25943 sgd_solver.cpp:106] Iteration 31900, lr = 1e-06
I0711 21:55:26.506573 25943 solver.cpp:594] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/l1reg/cityscapes5_jsegnet21v2_iter_32000.caffemodel
I0711 21:55:26.532850 25943 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/l1reg/cityscapes5_jsegnet21v2_iter_32000.solverstate
I0711 21:55:26.598388 25943 solver.cpp:447] Iteration 32000, loss = 0.0255415
I0711 21:55:26.598410 25943 solver.cpp:467] Iteration 32000, Testing net (#0)
I0711 21:56:12.927333 25943 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.95353
I0711 21:56:12.927438 25943 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999614
I0711 21:56:12.927445 25943 solver.cpp:540]     Test net output #2: loss = 0.165123 (* 1 = 0.165123 loss)
I0711 21:56:12.927449 25943 solver.cpp:452] Optimization Done.
I0711 21:56:13.280091 25943 caffe.cpp:246] Optimization Done.
