I0731 19:54:14.041030   830 caffe.cpp:608] This is NVCaffe 0.16.3 started at Mon Jul 31 19:54:13 2017
I0731 19:54:14.043110   830 caffe.cpp:611] CuDNN version: 6021
I0731 19:54:14.043138   830 caffe.cpp:612] CuBLAS version: 8000
I0731 19:54:14.043148   830 caffe.cpp:613] CUDA version: 8000
I0731 19:54:14.043159   830 caffe.cpp:614] CUDA driver version: 8000
I0731 19:54:14.369033   830 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0731 19:54:14.369599   830 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I0731 19:54:14.370120   830 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[1]: total=8508145664 free=8379236352
I0731 19:54:14.370630   830 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[2]: total=8508145664 free=8379236352
I0731 19:54:14.370638   830 caffe.cpp:208] Using GPUs 0, 1, 2
I0731 19:54:14.370959   830 caffe.cpp:213] GPU 0: GeForce GTX 1080
I0731 19:54:14.371280   830 caffe.cpp:213] GPU 1: GeForce GTX 1080
I0731 19:54:14.371605   830 caffe.cpp:213] GPU 2: GeForce GTX 1080
I0731 19:54:14.372409   830 solver.cpp:42] Solver data type: FLOAT
I0731 19:54:14.372448   830 solver.cpp:45] Initializing solver from parameters: 
train_net: "training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/l1reg/train.prototxt"
test_net: "training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/l1reg/test.prototxt"
test_iter: 125
test_interval: 2000
base_lr: 1e-05
display: 100
max_iter: 32000
lr_policy: "multistep"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 1e-05
snapshot: 10000
snapshot_prefix: "training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/l1reg/cityscapes5_jsegnet21v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
regularization_type: "L1"
test_initialization: false
stepvalue: 24000
iter_size: 1
type: "Adam"
I0731 19:54:14.395118   830 solver.cpp:77] Creating training net from train_net file: training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/l1reg/train.prototxt
I0731 19:54:14.398308   830 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0731 19:54:14.398347   830 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
W0731 19:54:14.398500   830 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0731 19:54:14.399991   830 net.cpp:72] Initializing net from parameters: 
name: "jsegnet21v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/train-image-lmdb"
    label_list_path: "data/train-label-lmdb"
    batch_size: 6
    shuffle: false
    threads: 1
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
I0731 19:54:14.400849   830 net.cpp:104] Using FLOAT as default forward math type
I0731 19:54:14.400892   830 net.cpp:110] Using FLOAT as default backward math type
I0731 19:54:14.400914   830 layer_factory.hpp:136] Creating layer 'data' of type 'ImageLabelData'
I0731 19:54:14.400941   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:14.401002   830 net.cpp:184] Created Layer data (0)
I0731 19:54:14.401028   830 net.cpp:530] data -> data
I0731 19:54:14.401197   830 net.cpp:530] data -> label
I0731 19:54:14.402330   830 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0731 19:54:14.402423   830 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 19:54:14.437793   892 db_lmdb.cpp:35] Opened lmdb data/train-image-lmdb
I0731 19:54:14.447075   830 data_layer.cpp:184] [0] ReshapePrefetch 6, 3, 640, 640
I0731 19:54:14.447247   830 data_layer.cpp:208] [0] Output data size: 6, 3, 640, 640
I0731 19:54:14.447273   830 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 19:54:14.447398   830 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0731 19:54:14.447438   830 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 19:54:14.449883   893 data_layer.cpp:97] [0] Parser threads: 1
I0731 19:54:14.449940   893 data_layer.cpp:99] [0] Transformer threads: 1
I0731 19:54:14.485596   894 db_lmdb.cpp:35] Opened lmdb data/train-label-lmdb
I0731 19:54:14.486945   830 data_layer.cpp:184] [0] ReshapePrefetch 6, 1, 640, 640
I0731 19:54:14.487015   830 data_layer.cpp:208] [0] Output data size: 6, 1, 640, 640
I0731 19:54:14.487025   830 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 19:54:14.487088   830 net.cpp:245] Setting up data
I0731 19:54:14.487104   830 net.cpp:252] TRAIN Top shape for layer 0 'data' 6 3 640 640 (7372800)
I0731 19:54:14.487123   830 net.cpp:252] TRAIN Top shape for layer 0 'data' 6 1 640 640 (2457600)
I0731 19:54:14.487133   830 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0731 19:54:14.487143   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:14.487165   830 net.cpp:184] Created Layer data/bias (1)
I0731 19:54:14.487174   830 net.cpp:561] data/bias <- data
I0731 19:54:14.487187   830 net.cpp:530] data/bias -> data/bias
I0731 19:54:14.488534   895 data_layer.cpp:97] [0] Parser threads: 1
I0731 19:54:14.488556   895 data_layer.cpp:99] [0] Transformer threads: 1
I0731 19:54:14.493466   830 net.cpp:245] Setting up data/bias
I0731 19:54:14.493525   830 net.cpp:252] TRAIN Top shape for layer 1 'data/bias' 6 3 640 640 (7372800)
I0731 19:54:14.493557   830 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0731 19:54:14.493567   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:14.493638   830 net.cpp:184] Created Layer conv1a (2)
I0731 19:54:14.493645   830 net.cpp:561] conv1a <- data/bias
I0731 19:54:14.493655   830 net.cpp:530] conv1a -> conv1a
I0731 19:54:15.228180   830 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 7.9G, req 0G)
I0731 19:54:15.228217   830 net.cpp:245] Setting up conv1a
I0731 19:54:15.228229   830 net.cpp:252] TRAIN Top shape for layer 2 'conv1a' 6 32 320 320 (19660800)
I0731 19:54:15.228245   830 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0731 19:54:15.228253   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.228269   830 net.cpp:184] Created Layer conv1a/bn (3)
I0731 19:54:15.228278   830 net.cpp:561] conv1a/bn <- conv1a
I0731 19:54:15.228286   830 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0731 19:54:15.229715   830 net.cpp:245] Setting up conv1a/bn
I0731 19:54:15.229735   830 net.cpp:252] TRAIN Top shape for layer 3 'conv1a/bn' 6 32 320 320 (19660800)
I0731 19:54:15.229751   830 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0731 19:54:15.229758   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.229768   830 net.cpp:184] Created Layer conv1a/relu (4)
I0731 19:54:15.229774   830 net.cpp:561] conv1a/relu <- conv1a
I0731 19:54:15.229784   830 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0731 19:54:15.229807   830 net.cpp:245] Setting up conv1a/relu
I0731 19:54:15.229815   830 net.cpp:252] TRAIN Top shape for layer 4 'conv1a/relu' 6 32 320 320 (19660800)
I0731 19:54:15.229820   830 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0731 19:54:15.229826   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.229851   830 net.cpp:184] Created Layer conv1b (5)
I0731 19:54:15.229857   830 net.cpp:561] conv1b <- conv1a
I0731 19:54:15.229863   830 net.cpp:530] conv1b -> conv1b
I0731 19:54:15.289197   830 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.73G, req 0G)
I0731 19:54:15.289222   830 net.cpp:245] Setting up conv1b
I0731 19:54:15.289229   830 net.cpp:252] TRAIN Top shape for layer 5 'conv1b' 6 32 320 320 (19660800)
I0731 19:54:15.289242   830 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0731 19:54:15.289247   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.289258   830 net.cpp:184] Created Layer conv1b/bn (6)
I0731 19:54:15.289263   830 net.cpp:561] conv1b/bn <- conv1b
I0731 19:54:15.289266   830 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0731 19:54:15.290153   830 net.cpp:245] Setting up conv1b/bn
I0731 19:54:15.290165   830 net.cpp:252] TRAIN Top shape for layer 6 'conv1b/bn' 6 32 320 320 (19660800)
I0731 19:54:15.290175   830 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0731 19:54:15.290180   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.290185   830 net.cpp:184] Created Layer conv1b/relu (7)
I0731 19:54:15.290189   830 net.cpp:561] conv1b/relu <- conv1b
I0731 19:54:15.290194   830 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0731 19:54:15.290199   830 net.cpp:245] Setting up conv1b/relu
I0731 19:54:15.290205   830 net.cpp:252] TRAIN Top shape for layer 7 'conv1b/relu' 6 32 320 320 (19660800)
I0731 19:54:15.290207   830 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0731 19:54:15.290211   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.290221   830 net.cpp:184] Created Layer pool1 (8)
I0731 19:54:15.290226   830 net.cpp:561] pool1 <- conv1b
I0731 19:54:15.290230   830 net.cpp:530] pool1 -> pool1
I0731 19:54:15.290822   830 net.cpp:245] Setting up pool1
I0731 19:54:15.290834   830 net.cpp:252] TRAIN Top shape for layer 8 'pool1' 6 32 160 160 (4915200)
I0731 19:54:15.290839   830 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0731 19:54:15.290855   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.290870   830 net.cpp:184] Created Layer res2a_branch2a (9)
I0731 19:54:15.290875   830 net.cpp:561] res2a_branch2a <- pool1
I0731 19:54:15.290880   830 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0731 19:54:15.331277   830 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.61G, req 0G)
I0731 19:54:15.331349   830 net.cpp:245] Setting up res2a_branch2a
I0731 19:54:15.331373   830 net.cpp:252] TRAIN Top shape for layer 9 'res2a_branch2a' 6 64 160 160 (9830400)
I0731 19:54:15.331406   830 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0731 19:54:15.331424   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.331451   830 net.cpp:184] Created Layer res2a_branch2a/bn (10)
I0731 19:54:15.331470   830 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0731 19:54:15.331508   830 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0731 19:54:15.336916   830 net.cpp:245] Setting up res2a_branch2a/bn
I0731 19:54:15.336966   830 net.cpp:252] TRAIN Top shape for layer 10 'res2a_branch2a/bn' 6 64 160 160 (9830400)
I0731 19:54:15.337000   830 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0731 19:54:15.337014   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.337033   830 net.cpp:184] Created Layer res2a_branch2a/relu (11)
I0731 19:54:15.337050   830 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0731 19:54:15.337065   830 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0731 19:54:15.337090   830 net.cpp:245] Setting up res2a_branch2a/relu
I0731 19:54:15.337113   830 net.cpp:252] TRAIN Top shape for layer 11 'res2a_branch2a/relu' 6 64 160 160 (9830400)
I0731 19:54:15.337141   830 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0731 19:54:15.337167   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.337215   830 net.cpp:184] Created Layer res2a_branch2b (12)
I0731 19:54:15.337239   830 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0731 19:54:15.337263   830 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0731 19:54:15.370810   830 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.52G, req 0G)
I0731 19:54:15.370877   830 net.cpp:245] Setting up res2a_branch2b
I0731 19:54:15.370898   830 net.cpp:252] TRAIN Top shape for layer 12 'res2a_branch2b' 6 64 160 160 (9830400)
I0731 19:54:15.370924   830 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0731 19:54:15.370939   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.370965   830 net.cpp:184] Created Layer res2a_branch2b/bn (13)
I0731 19:54:15.370981   830 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0731 19:54:15.370996   830 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0731 19:54:15.374055   830 net.cpp:245] Setting up res2a_branch2b/bn
I0731 19:54:15.374095   830 net.cpp:252] TRAIN Top shape for layer 13 'res2a_branch2b/bn' 6 64 160 160 (9830400)
I0731 19:54:15.374125   830 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0731 19:54:15.374138   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.374155   830 net.cpp:184] Created Layer res2a_branch2b/relu (14)
I0731 19:54:15.374167   830 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0731 19:54:15.374179   830 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0731 19:54:15.374204   830 net.cpp:245] Setting up res2a_branch2b/relu
I0731 19:54:15.374222   830 net.cpp:252] TRAIN Top shape for layer 14 'res2a_branch2b/relu' 6 64 160 160 (9830400)
I0731 19:54:15.374274   830 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0731 19:54:15.374289   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.374310   830 net.cpp:184] Created Layer pool2 (15)
I0731 19:54:15.374325   830 net.cpp:561] pool2 <- res2a_branch2b
I0731 19:54:15.374336   830 net.cpp:530] pool2 -> pool2
I0731 19:54:15.374637   830 net.cpp:245] Setting up pool2
I0731 19:54:15.374662   830 net.cpp:252] TRAIN Top shape for layer 15 'pool2' 6 64 80 80 (2457600)
I0731 19:54:15.374675   830 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0731 19:54:15.374687   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.374718   830 net.cpp:184] Created Layer res3a_branch2a (16)
I0731 19:54:15.374732   830 net.cpp:561] res3a_branch2a <- pool2
I0731 19:54:15.374744   830 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0731 19:54:15.405437   830 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 7.46G, req 0G)
I0731 19:54:15.405463   830 net.cpp:245] Setting up res3a_branch2a
I0731 19:54:15.405472   830 net.cpp:252] TRAIN Top shape for layer 16 'res3a_branch2a' 6 128 80 80 (4915200)
I0731 19:54:15.405481   830 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0731 19:54:15.405488   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.405498   830 net.cpp:184] Created Layer res3a_branch2a/bn (17)
I0731 19:54:15.405503   830 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0731 19:54:15.405508   830 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0731 19:54:15.406613   830 net.cpp:245] Setting up res3a_branch2a/bn
I0731 19:54:15.406626   830 net.cpp:252] TRAIN Top shape for layer 17 'res3a_branch2a/bn' 6 128 80 80 (4915200)
I0731 19:54:15.406639   830 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0731 19:54:15.406644   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.406651   830 net.cpp:184] Created Layer res3a_branch2a/relu (18)
I0731 19:54:15.406654   830 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0731 19:54:15.406658   830 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0731 19:54:15.406666   830 net.cpp:245] Setting up res3a_branch2a/relu
I0731 19:54:15.406671   830 net.cpp:252] TRAIN Top shape for layer 18 'res3a_branch2a/relu' 6 128 80 80 (4915200)
I0731 19:54:15.406677   830 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0731 19:54:15.406680   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.406692   830 net.cpp:184] Created Layer res3a_branch2b (19)
I0731 19:54:15.406697   830 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0731 19:54:15.406700   830 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0731 19:54:15.419273   830 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.42G, req 0G)
I0731 19:54:15.419293   830 net.cpp:245] Setting up res3a_branch2b
I0731 19:54:15.419301   830 net.cpp:252] TRAIN Top shape for layer 19 'res3a_branch2b' 6 128 80 80 (4915200)
I0731 19:54:15.419309   830 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0731 19:54:15.419314   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.419322   830 net.cpp:184] Created Layer res3a_branch2b/bn (20)
I0731 19:54:15.419327   830 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0731 19:54:15.419332   830 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0731 19:54:15.420379   830 net.cpp:245] Setting up res3a_branch2b/bn
I0731 19:54:15.420392   830 net.cpp:252] TRAIN Top shape for layer 20 'res3a_branch2b/bn' 6 128 80 80 (4915200)
I0731 19:54:15.420403   830 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0731 19:54:15.420426   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.420434   830 net.cpp:184] Created Layer res3a_branch2b/relu (21)
I0731 19:54:15.420441   830 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0731 19:54:15.420445   830 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0731 19:54:15.420454   830 net.cpp:245] Setting up res3a_branch2b/relu
I0731 19:54:15.420459   830 net.cpp:252] TRAIN Top shape for layer 21 'res3a_branch2b/relu' 6 128 80 80 (4915200)
I0731 19:54:15.420462   830 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0731 19:54:15.420466   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.420473   830 net.cpp:184] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (22)
I0731 19:54:15.420477   830 net.cpp:561] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0731 19:54:15.420481   830 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0731 19:54:15.420487   830 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0731 19:54:15.420570   830 net.cpp:245] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0731 19:54:15.420578   830 net.cpp:252] TRAIN Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 6 128 80 80 (4915200)
I0731 19:54:15.420583   830 net.cpp:252] TRAIN Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 6 128 80 80 (4915200)
I0731 19:54:15.420588   830 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0731 19:54:15.420591   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.420598   830 net.cpp:184] Created Layer pool3 (23)
I0731 19:54:15.420603   830 net.cpp:561] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0731 19:54:15.420606   830 net.cpp:530] pool3 -> pool3
I0731 19:54:15.420711   830 net.cpp:245] Setting up pool3
I0731 19:54:15.420718   830 net.cpp:252] TRAIN Top shape for layer 23 'pool3' 6 128 40 40 (1228800)
I0731 19:54:15.420722   830 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0731 19:54:15.420727   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.420737   830 net.cpp:184] Created Layer res4a_branch2a (24)
I0731 19:54:15.420742   830 net.cpp:561] res4a_branch2a <- pool3
I0731 19:54:15.420745   830 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0731 19:54:15.454989   830 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.38G, req 0G)
I0731 19:54:15.455009   830 net.cpp:245] Setting up res4a_branch2a
I0731 19:54:15.455016   830 net.cpp:252] TRAIN Top shape for layer 24 'res4a_branch2a' 6 256 40 40 (2457600)
I0731 19:54:15.455024   830 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0731 19:54:15.455029   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.455044   830 net.cpp:184] Created Layer res4a_branch2a/bn (25)
I0731 19:54:15.455049   830 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0731 19:54:15.455052   830 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0731 19:54:15.455919   830 net.cpp:245] Setting up res4a_branch2a/bn
I0731 19:54:15.455929   830 net.cpp:252] TRAIN Top shape for layer 25 'res4a_branch2a/bn' 6 256 40 40 (2457600)
I0731 19:54:15.455936   830 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0731 19:54:15.455940   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.455945   830 net.cpp:184] Created Layer res4a_branch2a/relu (26)
I0731 19:54:15.455948   830 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0731 19:54:15.455951   830 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0731 19:54:15.455970   830 net.cpp:245] Setting up res4a_branch2a/relu
I0731 19:54:15.455979   830 net.cpp:252] TRAIN Top shape for layer 26 'res4a_branch2a/relu' 6 256 40 40 (2457600)
I0731 19:54:15.455983   830 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0731 19:54:15.455986   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.455996   830 net.cpp:184] Created Layer res4a_branch2b (27)
I0731 19:54:15.455998   830 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0731 19:54:15.456002   830 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0731 19:54:15.466856   830 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.36G, req 0G)
I0731 19:54:15.466873   830 net.cpp:245] Setting up res4a_branch2b
I0731 19:54:15.466879   830 net.cpp:252] TRAIN Top shape for layer 27 'res4a_branch2b' 6 256 40 40 (2457600)
I0731 19:54:15.466886   830 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0731 19:54:15.466889   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.466897   830 net.cpp:184] Created Layer res4a_branch2b/bn (28)
I0731 19:54:15.466900   830 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0731 19:54:15.466904   830 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0731 19:54:15.467813   830 net.cpp:245] Setting up res4a_branch2b/bn
I0731 19:54:15.467823   830 net.cpp:252] TRAIN Top shape for layer 28 'res4a_branch2b/bn' 6 256 40 40 (2457600)
I0731 19:54:15.467831   830 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0731 19:54:15.467834   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.467839   830 net.cpp:184] Created Layer res4a_branch2b/relu (29)
I0731 19:54:15.467842   830 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0731 19:54:15.467845   830 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0731 19:54:15.467849   830 net.cpp:245] Setting up res4a_branch2b/relu
I0731 19:54:15.467854   830 net.cpp:252] TRAIN Top shape for layer 29 'res4a_branch2b/relu' 6 256 40 40 (2457600)
I0731 19:54:15.467856   830 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0731 19:54:15.467859   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.467866   830 net.cpp:184] Created Layer pool4 (30)
I0731 19:54:15.467869   830 net.cpp:561] pool4 <- res4a_branch2b
I0731 19:54:15.467874   830 net.cpp:530] pool4 -> pool4
I0731 19:54:15.467958   830 net.cpp:245] Setting up pool4
I0731 19:54:15.467964   830 net.cpp:252] TRAIN Top shape for layer 30 'pool4' 6 256 40 40 (2457600)
I0731 19:54:15.467968   830 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0731 19:54:15.467972   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.467985   830 net.cpp:184] Created Layer res5a_branch2a (31)
I0731 19:54:15.467989   830 net.cpp:561] res5a_branch2a <- pool4
I0731 19:54:15.467993   830 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0731 19:54:15.496901   830 net.cpp:245] Setting up res5a_branch2a
I0731 19:54:15.496920   830 net.cpp:252] TRAIN Top shape for layer 31 'res5a_branch2a' 6 512 40 40 (4915200)
I0731 19:54:15.496927   830 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0731 19:54:15.496930   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.496938   830 net.cpp:184] Created Layer res5a_branch2a/bn (32)
I0731 19:54:15.496942   830 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0731 19:54:15.496944   830 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0731 19:54:15.497577   830 net.cpp:245] Setting up res5a_branch2a/bn
I0731 19:54:15.497586   830 net.cpp:252] TRAIN Top shape for layer 32 'res5a_branch2a/bn' 6 512 40 40 (4915200)
I0731 19:54:15.497601   830 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0731 19:54:15.497604   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.497607   830 net.cpp:184] Created Layer res5a_branch2a/relu (33)
I0731 19:54:15.497609   830 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0731 19:54:15.497612   830 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0731 19:54:15.497617   830 net.cpp:245] Setting up res5a_branch2a/relu
I0731 19:54:15.497618   830 net.cpp:252] TRAIN Top shape for layer 33 'res5a_branch2a/relu' 6 512 40 40 (4915200)
I0731 19:54:15.497620   830 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0731 19:54:15.497623   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.497629   830 net.cpp:184] Created Layer res5a_branch2b (34)
I0731 19:54:15.497632   830 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0731 19:54:15.497634   830 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0731 19:54:15.510490   830 net.cpp:245] Setting up res5a_branch2b
I0731 19:54:15.510500   830 net.cpp:252] TRAIN Top shape for layer 34 'res5a_branch2b' 6 512 40 40 (4915200)
I0731 19:54:15.510507   830 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0731 19:54:15.510510   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.510514   830 net.cpp:184] Created Layer res5a_branch2b/bn (35)
I0731 19:54:15.510517   830 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0731 19:54:15.510519   830 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0731 19:54:15.511140   830 net.cpp:245] Setting up res5a_branch2b/bn
I0731 19:54:15.511147   830 net.cpp:252] TRAIN Top shape for layer 35 'res5a_branch2b/bn' 6 512 40 40 (4915200)
I0731 19:54:15.511152   830 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0731 19:54:15.511155   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.511158   830 net.cpp:184] Created Layer res5a_branch2b/relu (36)
I0731 19:54:15.511160   830 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0731 19:54:15.511162   830 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0731 19:54:15.511165   830 net.cpp:245] Setting up res5a_branch2b/relu
I0731 19:54:15.511168   830 net.cpp:252] TRAIN Top shape for layer 36 'res5a_branch2b/relu' 6 512 40 40 (4915200)
I0731 19:54:15.511170   830 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I0731 19:54:15.511173   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.511183   830 net.cpp:184] Created Layer out5a (37)
I0731 19:54:15.511185   830 net.cpp:561] out5a <- res5a_branch2b
I0731 19:54:15.511188   830 net.cpp:530] out5a -> out5a
I0731 19:54:15.515357   830 net.cpp:245] Setting up out5a
I0731 19:54:15.515367   830 net.cpp:252] TRAIN Top shape for layer 37 'out5a' 6 64 40 40 (614400)
I0731 19:54:15.515372   830 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I0731 19:54:15.515374   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.515384   830 net.cpp:184] Created Layer out5a/bn (38)
I0731 19:54:15.515388   830 net.cpp:561] out5a/bn <- out5a
I0731 19:54:15.515389   830 net.cpp:513] out5a/bn -> out5a (in-place)
I0731 19:54:15.516047   830 net.cpp:245] Setting up out5a/bn
I0731 19:54:15.516053   830 net.cpp:252] TRAIN Top shape for layer 38 'out5a/bn' 6 64 40 40 (614400)
I0731 19:54:15.516059   830 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I0731 19:54:15.516062   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.516064   830 net.cpp:184] Created Layer out5a/relu (39)
I0731 19:54:15.516067   830 net.cpp:561] out5a/relu <- out5a
I0731 19:54:15.516068   830 net.cpp:513] out5a/relu -> out5a (in-place)
I0731 19:54:15.516080   830 net.cpp:245] Setting up out5a/relu
I0731 19:54:15.516083   830 net.cpp:252] TRAIN Top shape for layer 39 'out5a/relu' 6 64 40 40 (614400)
I0731 19:54:15.516085   830 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I0731 19:54:15.516088   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.516099   830 net.cpp:184] Created Layer out5a_up2 (40)
I0731 19:54:15.516103   830 net.cpp:561] out5a_up2 <- out5a
I0731 19:54:15.516104   830 net.cpp:530] out5a_up2 -> out5a_up2
I0731 19:54:15.516396   830 net.cpp:245] Setting up out5a_up2
I0731 19:54:15.516402   830 net.cpp:252] TRAIN Top shape for layer 40 'out5a_up2' 6 64 80 80 (2457600)
I0731 19:54:15.516405   830 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I0731 19:54:15.516407   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.516423   830 net.cpp:184] Created Layer out3a (41)
I0731 19:54:15.516427   830 net.cpp:561] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0731 19:54:15.516429   830 net.cpp:530] out3a -> out3a
I0731 19:54:15.529728   830 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 7.3G, req 0G)
I0731 19:54:15.529739   830 net.cpp:245] Setting up out3a
I0731 19:54:15.529744   830 net.cpp:252] TRAIN Top shape for layer 41 'out3a' 6 64 80 80 (2457600)
I0731 19:54:15.529748   830 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I0731 19:54:15.529750   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.529755   830 net.cpp:184] Created Layer out3a/bn (42)
I0731 19:54:15.529757   830 net.cpp:561] out3a/bn <- out3a
I0731 19:54:15.529760   830 net.cpp:513] out3a/bn -> out3a (in-place)
I0731 19:54:15.530438   830 net.cpp:245] Setting up out3a/bn
I0731 19:54:15.530447   830 net.cpp:252] TRAIN Top shape for layer 42 'out3a/bn' 6 64 80 80 (2457600)
I0731 19:54:15.530452   830 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I0731 19:54:15.530454   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.530457   830 net.cpp:184] Created Layer out3a/relu (43)
I0731 19:54:15.530460   830 net.cpp:561] out3a/relu <- out3a
I0731 19:54:15.530462   830 net.cpp:513] out3a/relu -> out3a (in-place)
I0731 19:54:15.530465   830 net.cpp:245] Setting up out3a/relu
I0731 19:54:15.530468   830 net.cpp:252] TRAIN Top shape for layer 43 'out3a/relu' 6 64 80 80 (2457600)
I0731 19:54:15.530470   830 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I0731 19:54:15.530472   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.530933   830 net.cpp:184] Created Layer out3_out5_combined (44)
I0731 19:54:15.530942   830 net.cpp:561] out3_out5_combined <- out5a_up2
I0731 19:54:15.530946   830 net.cpp:561] out3_out5_combined <- out3a
I0731 19:54:15.530948   830 net.cpp:530] out3_out5_combined -> out3_out5_combined
I0731 19:54:15.531929   830 net.cpp:245] Setting up out3_out5_combined
I0731 19:54:15.531937   830 net.cpp:252] TRAIN Top shape for layer 44 'out3_out5_combined' 6 64 80 80 (2457600)
I0731 19:54:15.531940   830 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I0731 19:54:15.531944   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.531949   830 net.cpp:184] Created Layer ctx_conv1 (45)
I0731 19:54:15.531951   830 net.cpp:561] ctx_conv1 <- out3_out5_combined
I0731 19:54:15.531955   830 net.cpp:530] ctx_conv1 -> ctx_conv1
I0731 19:54:15.545760   830 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 7.25G, req 0G)
I0731 19:54:15.545786   830 net.cpp:245] Setting up ctx_conv1
I0731 19:54:15.545794   830 net.cpp:252] TRAIN Top shape for layer 45 'ctx_conv1' 6 64 80 80 (2457600)
I0731 19:54:15.545826   830 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I0731 19:54:15.545833   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.545845   830 net.cpp:184] Created Layer ctx_conv1/bn (46)
I0731 19:54:15.545850   830 net.cpp:561] ctx_conv1/bn <- ctx_conv1
I0731 19:54:15.545855   830 net.cpp:513] ctx_conv1/bn -> ctx_conv1 (in-place)
I0731 19:54:15.546903   830 net.cpp:245] Setting up ctx_conv1/bn
I0731 19:54:15.546914   830 net.cpp:252] TRAIN Top shape for layer 46 'ctx_conv1/bn' 6 64 80 80 (2457600)
I0731 19:54:15.546924   830 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I0731 19:54:15.546929   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.546936   830 net.cpp:184] Created Layer ctx_conv1/relu (47)
I0731 19:54:15.546939   830 net.cpp:561] ctx_conv1/relu <- ctx_conv1
I0731 19:54:15.546943   830 net.cpp:513] ctx_conv1/relu -> ctx_conv1 (in-place)
I0731 19:54:15.546949   830 net.cpp:245] Setting up ctx_conv1/relu
I0731 19:54:15.546955   830 net.cpp:252] TRAIN Top shape for layer 47 'ctx_conv1/relu' 6 64 80 80 (2457600)
I0731 19:54:15.546959   830 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I0731 19:54:15.546962   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.546973   830 net.cpp:184] Created Layer ctx_conv2 (48)
I0731 19:54:15.546977   830 net.cpp:561] ctx_conv2 <- ctx_conv1
I0731 19:54:15.546980   830 net.cpp:530] ctx_conv2 -> ctx_conv2
I0731 19:54:15.548498   830 net.cpp:245] Setting up ctx_conv2
I0731 19:54:15.548508   830 net.cpp:252] TRAIN Top shape for layer 48 'ctx_conv2' 6 64 80 80 (2457600)
I0731 19:54:15.548514   830 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I0731 19:54:15.548518   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.548524   830 net.cpp:184] Created Layer ctx_conv2/bn (49)
I0731 19:54:15.548527   830 net.cpp:561] ctx_conv2/bn <- ctx_conv2
I0731 19:54:15.548532   830 net.cpp:513] ctx_conv2/bn -> ctx_conv2 (in-place)
I0731 19:54:15.549443   830 net.cpp:245] Setting up ctx_conv2/bn
I0731 19:54:15.549453   830 net.cpp:252] TRAIN Top shape for layer 49 'ctx_conv2/bn' 6 64 80 80 (2457600)
I0731 19:54:15.549460   830 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I0731 19:54:15.549464   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.549469   830 net.cpp:184] Created Layer ctx_conv2/relu (50)
I0731 19:54:15.549473   830 net.cpp:561] ctx_conv2/relu <- ctx_conv2
I0731 19:54:15.549476   830 net.cpp:513] ctx_conv2/relu -> ctx_conv2 (in-place)
I0731 19:54:15.549481   830 net.cpp:245] Setting up ctx_conv2/relu
I0731 19:54:15.549484   830 net.cpp:252] TRAIN Top shape for layer 50 'ctx_conv2/relu' 6 64 80 80 (2457600)
I0731 19:54:15.549489   830 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I0731 19:54:15.549494   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.549506   830 net.cpp:184] Created Layer ctx_conv3 (51)
I0731 19:54:15.549510   830 net.cpp:561] ctx_conv3 <- ctx_conv2
I0731 19:54:15.549515   830 net.cpp:530] ctx_conv3 -> ctx_conv3
I0731 19:54:15.551018   830 net.cpp:245] Setting up ctx_conv3
I0731 19:54:15.551028   830 net.cpp:252] TRAIN Top shape for layer 51 'ctx_conv3' 6 64 80 80 (2457600)
I0731 19:54:15.551034   830 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I0731 19:54:15.551039   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.551045   830 net.cpp:184] Created Layer ctx_conv3/bn (52)
I0731 19:54:15.551050   830 net.cpp:561] ctx_conv3/bn <- ctx_conv3
I0731 19:54:15.551054   830 net.cpp:513] ctx_conv3/bn -> ctx_conv3 (in-place)
I0731 19:54:15.551951   830 net.cpp:245] Setting up ctx_conv3/bn
I0731 19:54:15.551968   830 net.cpp:252] TRAIN Top shape for layer 52 'ctx_conv3/bn' 6 64 80 80 (2457600)
I0731 19:54:15.551977   830 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I0731 19:54:15.551981   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.551986   830 net.cpp:184] Created Layer ctx_conv3/relu (53)
I0731 19:54:15.551990   830 net.cpp:561] ctx_conv3/relu <- ctx_conv3
I0731 19:54:15.551993   830 net.cpp:513] ctx_conv3/relu -> ctx_conv3 (in-place)
I0731 19:54:15.551998   830 net.cpp:245] Setting up ctx_conv3/relu
I0731 19:54:15.552003   830 net.cpp:252] TRAIN Top shape for layer 53 'ctx_conv3/relu' 6 64 80 80 (2457600)
I0731 19:54:15.552007   830 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I0731 19:54:15.552011   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.552019   830 net.cpp:184] Created Layer ctx_conv4 (54)
I0731 19:54:15.552023   830 net.cpp:561] ctx_conv4 <- ctx_conv3
I0731 19:54:15.552027   830 net.cpp:530] ctx_conv4 -> ctx_conv4
I0731 19:54:15.553649   830 net.cpp:245] Setting up ctx_conv4
I0731 19:54:15.553663   830 net.cpp:252] TRAIN Top shape for layer 54 'ctx_conv4' 6 64 80 80 (2457600)
I0731 19:54:15.553673   830 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I0731 19:54:15.553678   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.553694   830 net.cpp:184] Created Layer ctx_conv4/bn (55)
I0731 19:54:15.553701   830 net.cpp:561] ctx_conv4/bn <- ctx_conv4
I0731 19:54:15.553706   830 net.cpp:513] ctx_conv4/bn -> ctx_conv4 (in-place)
I0731 19:54:15.554651   830 net.cpp:245] Setting up ctx_conv4/bn
I0731 19:54:15.554659   830 net.cpp:252] TRAIN Top shape for layer 55 'ctx_conv4/bn' 6 64 80 80 (2457600)
I0731 19:54:15.554668   830 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I0731 19:54:15.554673   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.554678   830 net.cpp:184] Created Layer ctx_conv4/relu (56)
I0731 19:54:15.554682   830 net.cpp:561] ctx_conv4/relu <- ctx_conv4
I0731 19:54:15.554687   830 net.cpp:513] ctx_conv4/relu -> ctx_conv4 (in-place)
I0731 19:54:15.554692   830 net.cpp:245] Setting up ctx_conv4/relu
I0731 19:54:15.554697   830 net.cpp:252] TRAIN Top shape for layer 56 'ctx_conv4/relu' 6 64 80 80 (2457600)
I0731 19:54:15.554699   830 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I0731 19:54:15.554702   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.554715   830 net.cpp:184] Created Layer ctx_final (57)
I0731 19:54:15.554718   830 net.cpp:561] ctx_final <- ctx_conv4
I0731 19:54:15.554723   830 net.cpp:530] ctx_final -> ctx_final
I0731 19:54:15.570250   830 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 7.22G, req 0G)
I0731 19:54:15.570263   830 net.cpp:245] Setting up ctx_final
I0731 19:54:15.570267   830 net.cpp:252] TRAIN Top shape for layer 57 'ctx_final' 6 8 80 80 (307200)
I0731 19:54:15.570272   830 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I0731 19:54:15.570276   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.570281   830 net.cpp:184] Created Layer ctx_final/relu (58)
I0731 19:54:15.570283   830 net.cpp:561] ctx_final/relu <- ctx_final
I0731 19:54:15.570286   830 net.cpp:513] ctx_final/relu -> ctx_final (in-place)
I0731 19:54:15.570291   830 net.cpp:245] Setting up ctx_final/relu
I0731 19:54:15.570294   830 net.cpp:252] TRAIN Top shape for layer 58 'ctx_final/relu' 6 8 80 80 (307200)
I0731 19:54:15.570297   830 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I0731 19:54:15.570300   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.570314   830 net.cpp:184] Created Layer out_deconv_final_up2 (59)
I0731 19:54:15.570317   830 net.cpp:561] out_deconv_final_up2 <- ctx_final
I0731 19:54:15.570320   830 net.cpp:530] out_deconv_final_up2 -> out_deconv_final_up2
I0731 19:54:15.570632   830 net.cpp:245] Setting up out_deconv_final_up2
I0731 19:54:15.570641   830 net.cpp:252] TRAIN Top shape for layer 59 'out_deconv_final_up2' 6 8 160 160 (1228800)
I0731 19:54:15.570644   830 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I0731 19:54:15.570647   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.570652   830 net.cpp:184] Created Layer out_deconv_final_up4 (60)
I0731 19:54:15.570655   830 net.cpp:561] out_deconv_final_up4 <- out_deconv_final_up2
I0731 19:54:15.570658   830 net.cpp:530] out_deconv_final_up4 -> out_deconv_final_up4
I0731 19:54:15.570945   830 net.cpp:245] Setting up out_deconv_final_up4
I0731 19:54:15.570952   830 net.cpp:252] TRAIN Top shape for layer 60 'out_deconv_final_up4' 6 8 320 320 (4915200)
I0731 19:54:15.570956   830 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I0731 19:54:15.570960   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.570966   830 net.cpp:184] Created Layer out_deconv_final_up8 (61)
I0731 19:54:15.570968   830 net.cpp:561] out_deconv_final_up8 <- out_deconv_final_up4
I0731 19:54:15.570971   830 net.cpp:530] out_deconv_final_up8 -> out_deconv_final_up8
I0731 19:54:15.571333   830 net.cpp:245] Setting up out_deconv_final_up8
I0731 19:54:15.571341   830 net.cpp:252] TRAIN Top shape for layer 61 'out_deconv_final_up8' 6 8 640 640 (19660800)
I0731 19:54:15.571344   830 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0731 19:54:15.571347   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.571360   830 net.cpp:184] Created Layer loss (62)
I0731 19:54:15.571363   830 net.cpp:561] loss <- out_deconv_final_up8
I0731 19:54:15.571367   830 net.cpp:561] loss <- label
I0731 19:54:15.571370   830 net.cpp:530] loss -> loss
I0731 19:54:15.572674   830 net.cpp:245] Setting up loss
I0731 19:54:15.572684   830 net.cpp:252] TRAIN Top shape for layer 62 'loss' (1)
I0731 19:54:15.572686   830 net.cpp:256]     with loss weight 1
I0731 19:54:15.572690   830 net.cpp:323] loss needs backward computation.
I0731 19:54:15.572695   830 net.cpp:323] out_deconv_final_up8 needs backward computation.
I0731 19:54:15.572696   830 net.cpp:323] out_deconv_final_up4 needs backward computation.
I0731 19:54:15.572700   830 net.cpp:323] out_deconv_final_up2 needs backward computation.
I0731 19:54:15.572701   830 net.cpp:323] ctx_final/relu needs backward computation.
I0731 19:54:15.572705   830 net.cpp:323] ctx_final needs backward computation.
I0731 19:54:15.572707   830 net.cpp:323] ctx_conv4/relu needs backward computation.
I0731 19:54:15.572710   830 net.cpp:323] ctx_conv4/bn needs backward computation.
I0731 19:54:15.572713   830 net.cpp:323] ctx_conv4 needs backward computation.
I0731 19:54:15.572717   830 net.cpp:323] ctx_conv3/relu needs backward computation.
I0731 19:54:15.572721   830 net.cpp:323] ctx_conv3/bn needs backward computation.
I0731 19:54:15.572726   830 net.cpp:323] ctx_conv3 needs backward computation.
I0731 19:54:15.572728   830 net.cpp:323] ctx_conv2/relu needs backward computation.
I0731 19:54:15.572731   830 net.cpp:323] ctx_conv2/bn needs backward computation.
I0731 19:54:15.572732   830 net.cpp:323] ctx_conv2 needs backward computation.
I0731 19:54:15.572736   830 net.cpp:323] ctx_conv1/relu needs backward computation.
I0731 19:54:15.572737   830 net.cpp:323] ctx_conv1/bn needs backward computation.
I0731 19:54:15.572739   830 net.cpp:323] ctx_conv1 needs backward computation.
I0731 19:54:15.572742   830 net.cpp:323] out3_out5_combined needs backward computation.
I0731 19:54:15.572744   830 net.cpp:323] out3a/relu needs backward computation.
I0731 19:54:15.572753   830 net.cpp:323] out3a/bn needs backward computation.
I0731 19:54:15.572757   830 net.cpp:323] out3a needs backward computation.
I0731 19:54:15.572759   830 net.cpp:323] out5a_up2 needs backward computation.
I0731 19:54:15.572762   830 net.cpp:323] out5a/relu needs backward computation.
I0731 19:54:15.572764   830 net.cpp:323] out5a/bn needs backward computation.
I0731 19:54:15.572767   830 net.cpp:323] out5a needs backward computation.
I0731 19:54:15.572769   830 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0731 19:54:15.572772   830 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0731 19:54:15.572774   830 net.cpp:323] res5a_branch2b needs backward computation.
I0731 19:54:15.572777   830 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0731 19:54:15.572779   830 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0731 19:54:15.572782   830 net.cpp:323] res5a_branch2a needs backward computation.
I0731 19:54:15.572783   830 net.cpp:323] pool4 needs backward computation.
I0731 19:54:15.572787   830 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0731 19:54:15.572788   830 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0731 19:54:15.572790   830 net.cpp:323] res4a_branch2b needs backward computation.
I0731 19:54:15.572793   830 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0731 19:54:15.572795   830 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0731 19:54:15.572798   830 net.cpp:323] res4a_branch2a needs backward computation.
I0731 19:54:15.572800   830 net.cpp:323] pool3 needs backward computation.
I0731 19:54:15.572803   830 net.cpp:323] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I0731 19:54:15.572805   830 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0731 19:54:15.572808   830 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0731 19:54:15.572810   830 net.cpp:323] res3a_branch2b needs backward computation.
I0731 19:54:15.572821   830 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0731 19:54:15.572826   830 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0731 19:54:15.572830   830 net.cpp:323] res3a_branch2a needs backward computation.
I0731 19:54:15.572834   830 net.cpp:323] pool2 needs backward computation.
I0731 19:54:15.572839   830 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0731 19:54:15.572842   830 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0731 19:54:15.572845   830 net.cpp:323] res2a_branch2b needs backward computation.
I0731 19:54:15.572849   830 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0731 19:54:15.572854   830 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0731 19:54:15.572856   830 net.cpp:323] res2a_branch2a needs backward computation.
I0731 19:54:15.572860   830 net.cpp:323] pool1 needs backward computation.
I0731 19:54:15.572865   830 net.cpp:323] conv1b/relu needs backward computation.
I0731 19:54:15.572867   830 net.cpp:323] conv1b/bn needs backward computation.
I0731 19:54:15.572871   830 net.cpp:323] conv1b needs backward computation.
I0731 19:54:15.572875   830 net.cpp:323] conv1a/relu needs backward computation.
I0731 19:54:15.572877   830 net.cpp:323] conv1a/bn needs backward computation.
I0731 19:54:15.572880   830 net.cpp:323] conv1a needs backward computation.
I0731 19:54:15.572882   830 net.cpp:325] data/bias does not need backward computation.
I0731 19:54:15.572885   830 net.cpp:325] data does not need backward computation.
I0731 19:54:15.572887   830 net.cpp:367] This network produces output loss
I0731 19:54:15.572933   830 net.cpp:389] Top memory (TRAIN) required for data: 956006400 diff: 946176008
I0731 19:54:15.572937   830 net.cpp:392] Bottom memory (TRAIN) required for data: 956006400 diff: 956006400
I0731 19:54:15.572939   830 net.cpp:395] Shared (in-place) memory (TRAIN) by data: 630374400 diff: 630374400
I0731 19:54:15.572942   830 net.cpp:398] Parameters memory (TRAIN) required for data: 2692608 diff: 2692608
I0731 19:54:15.572948   830 net.cpp:401] Parameters shared memory (TRAIN) by data: 0 diff: 0
I0731 19:54:15.572952   830 net.cpp:407] Network initialization done.
I0731 19:54:15.573524   830 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/l1reg/test.prototxt
W0731 19:54:15.573621   830 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0731 19:54:15.573918   830 net.cpp:72] Initializing net from parameters: 
name: "jsegnet21v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/val-image-lmdb"
    label_list_path: "data/val-label-lmdb"
    batch_size: 2
    threads: 1
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
  accuracy_param {
    ignore_label: 255
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
    ignore_label: 255
  }
}
I0731 19:54:15.574112   830 net.cpp:104] Using FLOAT as default forward math type
I0731 19:54:15.574120   830 net.cpp:110] Using FLOAT as default backward math type
I0731 19:54:15.574121   830 layer_factory.hpp:136] Creating layer 'data' of type 'ImageLabelData'
I0731 19:54:15.574124   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.574133   830 net.cpp:184] Created Layer data (0)
I0731 19:54:15.574136   830 net.cpp:530] data -> data
I0731 19:54:15.574139   830 net.cpp:530] data -> label
I0731 19:54:15.574156   830 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0731 19:54:15.574162   830 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 19:54:15.592660   934 db_lmdb.cpp:35] Opened lmdb data/val-image-lmdb
I0731 19:54:15.594195   830 data_layer.cpp:184] (0) ReshapePrefetch 2, 3, 640, 640
I0731 19:54:15.594256   830 data_layer.cpp:208] (0) Output data size: 2, 3, 640, 640
I0731 19:54:15.594262   830 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 19:54:15.594383   830 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0731 19:54:15.594390   830 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 19:54:15.595160   935 data_layer.cpp:97] (0) Parser threads: 1
I0731 19:54:15.595171   935 data_layer.cpp:99] (0) Transformer threads: 1
I0731 19:54:15.615459   936 db_lmdb.cpp:35] Opened lmdb data/val-label-lmdb
I0731 19:54:15.616344   830 data_layer.cpp:184] (0) ReshapePrefetch 2, 1, 640, 640
I0731 19:54:15.616402   830 data_layer.cpp:208] (0) Output data size: 2, 1, 640, 640
I0731 19:54:15.616408   830 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 19:54:15.616446   830 net.cpp:245] Setting up data
I0731 19:54:15.616453   830 net.cpp:252] TEST Top shape for layer 0 'data' 2 3 640 640 (2457600)
I0731 19:54:15.616464   830 net.cpp:252] TEST Top shape for layer 0 'data' 2 1 640 640 (819200)
I0731 19:54:15.616469   830 layer_factory.hpp:136] Creating layer 'label_data_1_split' of type 'Split'
I0731 19:54:15.616474   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.616482   830 net.cpp:184] Created Layer label_data_1_split (1)
I0731 19:54:15.616485   830 net.cpp:561] label_data_1_split <- label
I0731 19:54:15.616489   830 net.cpp:530] label_data_1_split -> label_data_1_split_0
I0731 19:54:15.616494   830 net.cpp:530] label_data_1_split -> label_data_1_split_1
I0731 19:54:15.616497   830 net.cpp:530] label_data_1_split -> label_data_1_split_2
I0731 19:54:15.616580   830 net.cpp:245] Setting up label_data_1_split
I0731 19:54:15.616583   830 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0731 19:54:15.616586   830 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0731 19:54:15.616590   830 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0731 19:54:15.616592   830 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0731 19:54:15.616595   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.616601   830 net.cpp:184] Created Layer data/bias (2)
I0731 19:54:15.616605   830 net.cpp:561] data/bias <- data
I0731 19:54:15.616606   830 net.cpp:530] data/bias -> data/bias
I0731 19:54:15.617771   938 data_layer.cpp:97] (0) Parser threads: 1
I0731 19:54:15.617786   938 data_layer.cpp:99] (0) Transformer threads: 1
I0731 19:54:15.619474   830 net.cpp:245] Setting up data/bias
I0731 19:54:15.619488   830 net.cpp:252] TEST Top shape for layer 2 'data/bias' 2 3 640 640 (2457600)
I0731 19:54:15.619498   830 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0731 19:54:15.619503   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.619515   830 net.cpp:184] Created Layer conv1a (3)
I0731 19:54:15.619519   830 net.cpp:561] conv1a <- data/bias
I0731 19:54:15.619523   830 net.cpp:530] conv1a -> conv1a
I0731 19:54:15.624616   830 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.09G, req 0G)
I0731 19:54:15.624631   830 net.cpp:245] Setting up conv1a
I0731 19:54:15.624636   830 net.cpp:252] TEST Top shape for layer 3 'conv1a' 2 32 320 320 (6553600)
I0731 19:54:15.624653   830 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0731 19:54:15.624658   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.624665   830 net.cpp:184] Created Layer conv1a/bn (4)
I0731 19:54:15.624668   830 net.cpp:561] conv1a/bn <- conv1a
I0731 19:54:15.624672   830 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0731 19:54:15.625429   830 net.cpp:245] Setting up conv1a/bn
I0731 19:54:15.625437   830 net.cpp:252] TEST Top shape for layer 4 'conv1a/bn' 2 32 320 320 (6553600)
I0731 19:54:15.625444   830 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0731 19:54:15.625447   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.625455   830 net.cpp:184] Created Layer conv1a/relu (5)
I0731 19:54:15.625458   830 net.cpp:561] conv1a/relu <- conv1a
I0731 19:54:15.625460   830 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0731 19:54:15.625464   830 net.cpp:245] Setting up conv1a/relu
I0731 19:54:15.625468   830 net.cpp:252] TEST Top shape for layer 5 'conv1a/relu' 2 32 320 320 (6553600)
I0731 19:54:15.625469   830 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0731 19:54:15.625473   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.625480   830 net.cpp:184] Created Layer conv1b (6)
I0731 19:54:15.625483   830 net.cpp:561] conv1b <- conv1a
I0731 19:54:15.625485   830 net.cpp:530] conv1b -> conv1b
I0731 19:54:15.639390   830 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.06G, req 0G)
I0731 19:54:15.639415   830 net.cpp:245] Setting up conv1b
I0731 19:54:15.639420   830 net.cpp:252] TEST Top shape for layer 6 'conv1b' 2 32 320 320 (6553600)
I0731 19:54:15.639431   830 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0731 19:54:15.639436   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.639446   830 net.cpp:184] Created Layer conv1b/bn (7)
I0731 19:54:15.639451   830 net.cpp:561] conv1b/bn <- conv1b
I0731 19:54:15.639456   830 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0731 19:54:15.640745   830 net.cpp:245] Setting up conv1b/bn
I0731 19:54:15.640763   830 net.cpp:252] TEST Top shape for layer 7 'conv1b/bn' 2 32 320 320 (6553600)
I0731 19:54:15.640774   830 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0731 19:54:15.640779   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.640785   830 net.cpp:184] Created Layer conv1b/relu (8)
I0731 19:54:15.640789   830 net.cpp:561] conv1b/relu <- conv1b
I0731 19:54:15.640794   830 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0731 19:54:15.640801   830 net.cpp:245] Setting up conv1b/relu
I0731 19:54:15.640806   830 net.cpp:252] TEST Top shape for layer 8 'conv1b/relu' 2 32 320 320 (6553600)
I0731 19:54:15.640810   830 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0731 19:54:15.640828   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.640835   830 net.cpp:184] Created Layer pool1 (9)
I0731 19:54:15.640838   830 net.cpp:561] pool1 <- conv1b
I0731 19:54:15.640843   830 net.cpp:530] pool1 -> pool1
I0731 19:54:15.640950   830 net.cpp:245] Setting up pool1
I0731 19:54:15.640959   830 net.cpp:252] TEST Top shape for layer 9 'pool1' 2 32 160 160 (1638400)
I0731 19:54:15.640964   830 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0731 19:54:15.640967   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.640978   830 net.cpp:184] Created Layer res2a_branch2a (10)
I0731 19:54:15.640982   830 net.cpp:561] res2a_branch2a <- pool1
I0731 19:54:15.640986   830 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0731 19:54:15.650307   830 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 1  (limit 7.03G, req 0G)
I0731 19:54:15.650331   830 net.cpp:245] Setting up res2a_branch2a
I0731 19:54:15.650336   830 net.cpp:252] TEST Top shape for layer 10 'res2a_branch2a' 2 64 160 160 (3276800)
I0731 19:54:15.650342   830 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0731 19:54:15.650346   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.650357   830 net.cpp:184] Created Layer res2a_branch2a/bn (11)
I0731 19:54:15.650360   830 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0731 19:54:15.650364   830 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0731 19:54:15.651113   830 net.cpp:245] Setting up res2a_branch2a/bn
I0731 19:54:15.651121   830 net.cpp:252] TEST Top shape for layer 11 'res2a_branch2a/bn' 2 64 160 160 (3276800)
I0731 19:54:15.651126   830 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0731 19:54:15.651129   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.651134   830 net.cpp:184] Created Layer res2a_branch2a/relu (12)
I0731 19:54:15.651135   830 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0731 19:54:15.651139   830 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0731 19:54:15.651141   830 net.cpp:245] Setting up res2a_branch2a/relu
I0731 19:54:15.651144   830 net.cpp:252] TEST Top shape for layer 12 'res2a_branch2a/relu' 2 64 160 160 (3276800)
I0731 19:54:15.651146   830 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0731 19:54:15.651149   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.651154   830 net.cpp:184] Created Layer res2a_branch2b (13)
I0731 19:54:15.651157   830 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0731 19:54:15.651160   830 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0731 19:54:15.658759   830 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.02G, req 0G)
I0731 19:54:15.658771   830 net.cpp:245] Setting up res2a_branch2b
I0731 19:54:15.658774   830 net.cpp:252] TEST Top shape for layer 13 'res2a_branch2b' 2 64 160 160 (3276800)
I0731 19:54:15.658778   830 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0731 19:54:15.658782   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.658785   830 net.cpp:184] Created Layer res2a_branch2b/bn (14)
I0731 19:54:15.658788   830 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0731 19:54:15.658790   830 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0731 19:54:15.659502   830 net.cpp:245] Setting up res2a_branch2b/bn
I0731 19:54:15.659510   830 net.cpp:252] TEST Top shape for layer 14 'res2a_branch2b/bn' 2 64 160 160 (3276800)
I0731 19:54:15.659515   830 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0731 19:54:15.659518   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.659521   830 net.cpp:184] Created Layer res2a_branch2b/relu (15)
I0731 19:54:15.659523   830 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0731 19:54:15.659525   830 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0731 19:54:15.659529   830 net.cpp:245] Setting up res2a_branch2b/relu
I0731 19:54:15.659531   830 net.cpp:252] TEST Top shape for layer 15 'res2a_branch2b/relu' 2 64 160 160 (3276800)
I0731 19:54:15.659533   830 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0731 19:54:15.659536   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.659540   830 net.cpp:184] Created Layer pool2 (16)
I0731 19:54:15.659543   830 net.cpp:561] pool2 <- res2a_branch2b
I0731 19:54:15.659544   830 net.cpp:530] pool2 -> pool2
I0731 19:54:15.659611   830 net.cpp:245] Setting up pool2
I0731 19:54:15.659616   830 net.cpp:252] TEST Top shape for layer 16 'pool2' 2 64 80 80 (819200)
I0731 19:54:15.659626   830 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0731 19:54:15.659628   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.659639   830 net.cpp:184] Created Layer res3a_branch2a (17)
I0731 19:54:15.659641   830 net.cpp:561] res3a_branch2a <- pool2
I0731 19:54:15.659644   830 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0731 19:54:15.666054   830 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.01G, req 0G)
I0731 19:54:15.666065   830 net.cpp:245] Setting up res3a_branch2a
I0731 19:54:15.666069   830 net.cpp:252] TEST Top shape for layer 17 'res3a_branch2a' 2 128 80 80 (1638400)
I0731 19:54:15.666074   830 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0731 19:54:15.666077   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.666081   830 net.cpp:184] Created Layer res3a_branch2a/bn (18)
I0731 19:54:15.666085   830 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0731 19:54:15.666086   830 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0731 19:54:15.666787   830 net.cpp:245] Setting up res3a_branch2a/bn
I0731 19:54:15.666795   830 net.cpp:252] TEST Top shape for layer 18 'res3a_branch2a/bn' 2 128 80 80 (1638400)
I0731 19:54:15.666802   830 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0731 19:54:15.666805   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.666808   830 net.cpp:184] Created Layer res3a_branch2a/relu (19)
I0731 19:54:15.666810   830 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0731 19:54:15.666813   830 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0731 19:54:15.666816   830 net.cpp:245] Setting up res3a_branch2a/relu
I0731 19:54:15.666820   830 net.cpp:252] TEST Top shape for layer 19 'res3a_branch2a/relu' 2 128 80 80 (1638400)
I0731 19:54:15.666821   830 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0731 19:54:15.666823   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.666828   830 net.cpp:184] Created Layer res3a_branch2b (20)
I0731 19:54:15.666831   830 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0731 19:54:15.666834   830 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0731 19:54:15.671862   830 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7G, req 0G)
I0731 19:54:15.671874   830 net.cpp:245] Setting up res3a_branch2b
I0731 19:54:15.671877   830 net.cpp:252] TEST Top shape for layer 20 'res3a_branch2b' 2 128 80 80 (1638400)
I0731 19:54:15.671881   830 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0731 19:54:15.671885   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.671890   830 net.cpp:184] Created Layer res3a_branch2b/bn (21)
I0731 19:54:15.671891   830 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0731 19:54:15.671895   830 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0731 19:54:15.672601   830 net.cpp:245] Setting up res3a_branch2b/bn
I0731 19:54:15.672610   830 net.cpp:252] TEST Top shape for layer 21 'res3a_branch2b/bn' 2 128 80 80 (1638400)
I0731 19:54:15.672616   830 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0731 19:54:15.672617   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.672621   830 net.cpp:184] Created Layer res3a_branch2b/relu (22)
I0731 19:54:15.672623   830 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0731 19:54:15.672626   830 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0731 19:54:15.672629   830 net.cpp:245] Setting up res3a_branch2b/relu
I0731 19:54:15.672631   830 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b/relu' 2 128 80 80 (1638400)
I0731 19:54:15.672641   830 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0731 19:54:15.672644   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.672647   830 net.cpp:184] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (23)
I0731 19:54:15.672649   830 net.cpp:561] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0731 19:54:15.672652   830 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0731 19:54:15.672655   830 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0731 19:54:15.672701   830 net.cpp:245] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0731 19:54:15.672708   830 net.cpp:252] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 2 128 80 80 (1638400)
I0731 19:54:15.672709   830 net.cpp:252] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 2 128 80 80 (1638400)
I0731 19:54:15.672711   830 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0731 19:54:15.672714   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.672724   830 net.cpp:184] Created Layer pool3 (24)
I0731 19:54:15.672729   830 net.cpp:561] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0731 19:54:15.672732   830 net.cpp:530] pool3 -> pool3
I0731 19:54:15.672801   830 net.cpp:245] Setting up pool3
I0731 19:54:15.672806   830 net.cpp:252] TEST Top shape for layer 24 'pool3' 2 128 40 40 (409600)
I0731 19:54:15.672808   830 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0731 19:54:15.672811   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.672823   830 net.cpp:184] Created Layer res4a_branch2a (25)
I0731 19:54:15.672827   830 net.cpp:561] res4a_branch2a <- pool3
I0731 19:54:15.672830   830 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0731 19:54:15.685107   830 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.99G, req 0G)
I0731 19:54:15.685118   830 net.cpp:245] Setting up res4a_branch2a
I0731 19:54:15.685122   830 net.cpp:252] TEST Top shape for layer 25 'res4a_branch2a' 2 256 40 40 (819200)
I0731 19:54:15.685127   830 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0731 19:54:15.685128   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.685138   830 net.cpp:184] Created Layer res4a_branch2a/bn (26)
I0731 19:54:15.685142   830 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0731 19:54:15.685144   830 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0731 19:54:15.685891   830 net.cpp:245] Setting up res4a_branch2a/bn
I0731 19:54:15.685899   830 net.cpp:252] TEST Top shape for layer 26 'res4a_branch2a/bn' 2 256 40 40 (819200)
I0731 19:54:15.685904   830 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0731 19:54:15.685907   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.685910   830 net.cpp:184] Created Layer res4a_branch2a/relu (27)
I0731 19:54:15.685914   830 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0731 19:54:15.685921   830 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0731 19:54:15.685925   830 net.cpp:245] Setting up res4a_branch2a/relu
I0731 19:54:15.685927   830 net.cpp:252] TEST Top shape for layer 27 'res4a_branch2a/relu' 2 256 40 40 (819200)
I0731 19:54:15.685930   830 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0731 19:54:15.685933   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.685943   830 net.cpp:184] Created Layer res4a_branch2b (28)
I0731 19:54:15.685946   830 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0731 19:54:15.685956   830 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0731 19:54:15.694806   830 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.98G, req 0G)
I0731 19:54:15.694825   830 net.cpp:245] Setting up res4a_branch2b
I0731 19:54:15.694833   830 net.cpp:252] TEST Top shape for layer 28 'res4a_branch2b' 2 256 40 40 (819200)
I0731 19:54:15.694841   830 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0731 19:54:15.694846   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.694855   830 net.cpp:184] Created Layer res4a_branch2b/bn (29)
I0731 19:54:15.694860   830 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0731 19:54:15.694864   830 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0731 19:54:15.695830   830 net.cpp:245] Setting up res4a_branch2b/bn
I0731 19:54:15.695840   830 net.cpp:252] TEST Top shape for layer 29 'res4a_branch2b/bn' 2 256 40 40 (819200)
I0731 19:54:15.695849   830 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0731 19:54:15.695853   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.695858   830 net.cpp:184] Created Layer res4a_branch2b/relu (30)
I0731 19:54:15.695863   830 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0731 19:54:15.695865   830 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0731 19:54:15.695871   830 net.cpp:245] Setting up res4a_branch2b/relu
I0731 19:54:15.695876   830 net.cpp:252] TEST Top shape for layer 30 'res4a_branch2b/relu' 2 256 40 40 (819200)
I0731 19:54:15.695880   830 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0731 19:54:15.695884   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.695890   830 net.cpp:184] Created Layer pool4 (31)
I0731 19:54:15.695894   830 net.cpp:561] pool4 <- res4a_branch2b
I0731 19:54:15.695899   830 net.cpp:530] pool4 -> pool4
I0731 19:54:15.696002   830 net.cpp:245] Setting up pool4
I0731 19:54:15.696008   830 net.cpp:252] TEST Top shape for layer 31 'pool4' 2 256 40 40 (819200)
I0731 19:54:15.696013   830 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0731 19:54:15.696017   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.696028   830 net.cpp:184] Created Layer res5a_branch2a (32)
I0731 19:54:15.696033   830 net.cpp:561] res5a_branch2a <- pool4
I0731 19:54:15.696038   830 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0731 19:54:15.724181   830 net.cpp:245] Setting up res5a_branch2a
I0731 19:54:15.724205   830 net.cpp:252] TEST Top shape for layer 32 'res5a_branch2a' 2 512 40 40 (1638400)
I0731 19:54:15.724211   830 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0731 19:54:15.724215   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.724221   830 net.cpp:184] Created Layer res5a_branch2a/bn (33)
I0731 19:54:15.724225   830 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0731 19:54:15.724230   830 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0731 19:54:15.724921   830 net.cpp:245] Setting up res5a_branch2a/bn
I0731 19:54:15.724934   830 net.cpp:252] TEST Top shape for layer 33 'res5a_branch2a/bn' 2 512 40 40 (1638400)
I0731 19:54:15.724941   830 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0731 19:54:15.724943   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.724946   830 net.cpp:184] Created Layer res5a_branch2a/relu (34)
I0731 19:54:15.724949   830 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0731 19:54:15.724951   830 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0731 19:54:15.724956   830 net.cpp:245] Setting up res5a_branch2a/relu
I0731 19:54:15.724958   830 net.cpp:252] TEST Top shape for layer 34 'res5a_branch2a/relu' 2 512 40 40 (1638400)
I0731 19:54:15.724970   830 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0731 19:54:15.724974   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.724980   830 net.cpp:184] Created Layer res5a_branch2b (35)
I0731 19:54:15.724983   830 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0731 19:54:15.724985   830 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0731 19:54:15.738055   830 net.cpp:245] Setting up res5a_branch2b
I0731 19:54:15.738067   830 net.cpp:252] TEST Top shape for layer 35 'res5a_branch2b' 2 512 40 40 (1638400)
I0731 19:54:15.738085   830 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0731 19:54:15.738087   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.738097   830 net.cpp:184] Created Layer res5a_branch2b/bn (36)
I0731 19:54:15.738101   830 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0731 19:54:15.738103   830 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0731 19:54:15.738788   830 net.cpp:245] Setting up res5a_branch2b/bn
I0731 19:54:15.738795   830 net.cpp:252] TEST Top shape for layer 36 'res5a_branch2b/bn' 2 512 40 40 (1638400)
I0731 19:54:15.738801   830 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0731 19:54:15.738803   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.738806   830 net.cpp:184] Created Layer res5a_branch2b/relu (37)
I0731 19:54:15.738808   830 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0731 19:54:15.738811   830 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0731 19:54:15.738814   830 net.cpp:245] Setting up res5a_branch2b/relu
I0731 19:54:15.738817   830 net.cpp:252] TEST Top shape for layer 37 'res5a_branch2b/relu' 2 512 40 40 (1638400)
I0731 19:54:15.738819   830 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I0731 19:54:15.738822   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.738831   830 net.cpp:184] Created Layer out5a (38)
I0731 19:54:15.738834   830 net.cpp:561] out5a <- res5a_branch2b
I0731 19:54:15.738837   830 net.cpp:530] out5a -> out5a
I0731 19:54:15.742130   830 net.cpp:245] Setting up out5a
I0731 19:54:15.742137   830 net.cpp:252] TEST Top shape for layer 38 'out5a' 2 64 40 40 (204800)
I0731 19:54:15.742141   830 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I0731 19:54:15.742144   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.742147   830 net.cpp:184] Created Layer out5a/bn (39)
I0731 19:54:15.742151   830 net.cpp:561] out5a/bn <- out5a
I0731 19:54:15.742152   830 net.cpp:513] out5a/bn -> out5a (in-place)
I0731 19:54:15.742842   830 net.cpp:245] Setting up out5a/bn
I0731 19:54:15.742849   830 net.cpp:252] TEST Top shape for layer 39 'out5a/bn' 2 64 40 40 (204800)
I0731 19:54:15.742854   830 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I0731 19:54:15.742856   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.742859   830 net.cpp:184] Created Layer out5a/relu (40)
I0731 19:54:15.742861   830 net.cpp:561] out5a/relu <- out5a
I0731 19:54:15.742864   830 net.cpp:513] out5a/relu -> out5a (in-place)
I0731 19:54:15.742867   830 net.cpp:245] Setting up out5a/relu
I0731 19:54:15.742871   830 net.cpp:252] TEST Top shape for layer 40 'out5a/relu' 2 64 40 40 (204800)
I0731 19:54:15.742872   830 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I0731 19:54:15.742874   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.742878   830 net.cpp:184] Created Layer out5a_up2 (41)
I0731 19:54:15.742880   830 net.cpp:561] out5a_up2 <- out5a
I0731 19:54:15.742882   830 net.cpp:530] out5a_up2 -> out5a_up2
I0731 19:54:15.743196   830 net.cpp:245] Setting up out5a_up2
I0731 19:54:15.743202   830 net.cpp:252] TEST Top shape for layer 41 'out5a_up2' 2 64 80 80 (819200)
I0731 19:54:15.743206   830 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I0731 19:54:15.743207   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.743213   830 net.cpp:184] Created Layer out3a (42)
I0731 19:54:15.743216   830 net.cpp:561] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0731 19:54:15.743219   830 net.cpp:530] out3a -> out3a
I0731 19:54:15.747632   830 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.97G, req 0G)
I0731 19:54:15.747648   830 net.cpp:245] Setting up out3a
I0731 19:54:15.747654   830 net.cpp:252] TEST Top shape for layer 42 'out3a' 2 64 80 80 (819200)
I0731 19:54:15.747661   830 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I0731 19:54:15.747668   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.747676   830 net.cpp:184] Created Layer out3a/bn (43)
I0731 19:54:15.747681   830 net.cpp:561] out3a/bn <- out3a
I0731 19:54:15.747686   830 net.cpp:513] out3a/bn -> out3a (in-place)
I0731 19:54:15.748723   830 net.cpp:245] Setting up out3a/bn
I0731 19:54:15.748733   830 net.cpp:252] TEST Top shape for layer 43 'out3a/bn' 2 64 80 80 (819200)
I0731 19:54:15.748742   830 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I0731 19:54:15.748745   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.748750   830 net.cpp:184] Created Layer out3a/relu (44)
I0731 19:54:15.748754   830 net.cpp:561] out3a/relu <- out3a
I0731 19:54:15.748759   830 net.cpp:513] out3a/relu -> out3a (in-place)
I0731 19:54:15.748764   830 net.cpp:245] Setting up out3a/relu
I0731 19:54:15.748769   830 net.cpp:252] TEST Top shape for layer 44 'out3a/relu' 2 64 80 80 (819200)
I0731 19:54:15.748772   830 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I0731 19:54:15.748776   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.748780   830 net.cpp:184] Created Layer out3_out5_combined (45)
I0731 19:54:15.748785   830 net.cpp:561] out3_out5_combined <- out5a_up2
I0731 19:54:15.748790   830 net.cpp:561] out3_out5_combined <- out3a
I0731 19:54:15.748793   830 net.cpp:530] out3_out5_combined -> out3_out5_combined
I0731 19:54:15.750098   830 net.cpp:245] Setting up out3_out5_combined
I0731 19:54:15.750109   830 net.cpp:252] TEST Top shape for layer 45 'out3_out5_combined' 2 64 80 80 (819200)
I0731 19:54:15.750115   830 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I0731 19:54:15.750120   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.750131   830 net.cpp:184] Created Layer ctx_conv1 (46)
I0731 19:54:15.750136   830 net.cpp:561] ctx_conv1 <- out3_out5_combined
I0731 19:54:15.750143   830 net.cpp:530] ctx_conv1 -> ctx_conv1
I0731 19:54:15.754809   830 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.96G, req 0G)
I0731 19:54:15.754822   830 net.cpp:245] Setting up ctx_conv1
I0731 19:54:15.754828   830 net.cpp:252] TEST Top shape for layer 46 'ctx_conv1' 2 64 80 80 (819200)
I0731 19:54:15.754835   830 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I0731 19:54:15.754839   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.754851   830 net.cpp:184] Created Layer ctx_conv1/bn (47)
I0731 19:54:15.754856   830 net.cpp:561] ctx_conv1/bn <- ctx_conv1
I0731 19:54:15.754860   830 net.cpp:513] ctx_conv1/bn -> ctx_conv1 (in-place)
I0731 19:54:15.755856   830 net.cpp:245] Setting up ctx_conv1/bn
I0731 19:54:15.755864   830 net.cpp:252] TEST Top shape for layer 47 'ctx_conv1/bn' 2 64 80 80 (819200)
I0731 19:54:15.755873   830 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I0731 19:54:15.755889   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.755895   830 net.cpp:184] Created Layer ctx_conv1/relu (48)
I0731 19:54:15.755899   830 net.cpp:561] ctx_conv1/relu <- ctx_conv1
I0731 19:54:15.755903   830 net.cpp:513] ctx_conv1/relu -> ctx_conv1 (in-place)
I0731 19:54:15.755909   830 net.cpp:245] Setting up ctx_conv1/relu
I0731 19:54:15.755914   830 net.cpp:252] TEST Top shape for layer 48 'ctx_conv1/relu' 2 64 80 80 (819200)
I0731 19:54:15.755918   830 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I0731 19:54:15.755921   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.755931   830 net.cpp:184] Created Layer ctx_conv2 (49)
I0731 19:54:15.755935   830 net.cpp:561] ctx_conv2 <- ctx_conv1
I0731 19:54:15.755939   830 net.cpp:530] ctx_conv2 -> ctx_conv2
I0731 19:54:15.757103   830 net.cpp:245] Setting up ctx_conv2
I0731 19:54:15.757112   830 net.cpp:252] TEST Top shape for layer 49 'ctx_conv2' 2 64 80 80 (819200)
I0731 19:54:15.757117   830 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I0731 19:54:15.757119   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.757123   830 net.cpp:184] Created Layer ctx_conv2/bn (50)
I0731 19:54:15.757127   830 net.cpp:561] ctx_conv2/bn <- ctx_conv2
I0731 19:54:15.757128   830 net.cpp:513] ctx_conv2/bn -> ctx_conv2 (in-place)
I0731 19:54:15.757840   830 net.cpp:245] Setting up ctx_conv2/bn
I0731 19:54:15.757846   830 net.cpp:252] TEST Top shape for layer 50 'ctx_conv2/bn' 2 64 80 80 (819200)
I0731 19:54:15.757853   830 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I0731 19:54:15.757855   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.757859   830 net.cpp:184] Created Layer ctx_conv2/relu (51)
I0731 19:54:15.757861   830 net.cpp:561] ctx_conv2/relu <- ctx_conv2
I0731 19:54:15.757864   830 net.cpp:513] ctx_conv2/relu -> ctx_conv2 (in-place)
I0731 19:54:15.757869   830 net.cpp:245] Setting up ctx_conv2/relu
I0731 19:54:15.757871   830 net.cpp:252] TEST Top shape for layer 51 'ctx_conv2/relu' 2 64 80 80 (819200)
I0731 19:54:15.757874   830 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I0731 19:54:15.757876   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.757881   830 net.cpp:184] Created Layer ctx_conv3 (52)
I0731 19:54:15.757884   830 net.cpp:561] ctx_conv3 <- ctx_conv2
I0731 19:54:15.757886   830 net.cpp:530] ctx_conv3 -> ctx_conv3
I0731 19:54:15.759006   830 net.cpp:245] Setting up ctx_conv3
I0731 19:54:15.759013   830 net.cpp:252] TEST Top shape for layer 52 'ctx_conv3' 2 64 80 80 (819200)
I0731 19:54:15.759018   830 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I0731 19:54:15.759021   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.759026   830 net.cpp:184] Created Layer ctx_conv3/bn (53)
I0731 19:54:15.759028   830 net.cpp:561] ctx_conv3/bn <- ctx_conv3
I0731 19:54:15.759032   830 net.cpp:513] ctx_conv3/bn -> ctx_conv3 (in-place)
I0731 19:54:15.759735   830 net.cpp:245] Setting up ctx_conv3/bn
I0731 19:54:15.759742   830 net.cpp:252] TEST Top shape for layer 53 'ctx_conv3/bn' 2 64 80 80 (819200)
I0731 19:54:15.759748   830 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I0731 19:54:15.759752   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.759754   830 net.cpp:184] Created Layer ctx_conv3/relu (54)
I0731 19:54:15.759757   830 net.cpp:561] ctx_conv3/relu <- ctx_conv3
I0731 19:54:15.759760   830 net.cpp:513] ctx_conv3/relu -> ctx_conv3 (in-place)
I0731 19:54:15.759763   830 net.cpp:245] Setting up ctx_conv3/relu
I0731 19:54:15.759766   830 net.cpp:252] TEST Top shape for layer 54 'ctx_conv3/relu' 2 64 80 80 (819200)
I0731 19:54:15.759775   830 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I0731 19:54:15.759778   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.759788   830 net.cpp:184] Created Layer ctx_conv4 (55)
I0731 19:54:15.759791   830 net.cpp:561] ctx_conv4 <- ctx_conv3
I0731 19:54:15.759793   830 net.cpp:530] ctx_conv4 -> ctx_conv4
I0731 19:54:15.760910   830 net.cpp:245] Setting up ctx_conv4
I0731 19:54:15.760918   830 net.cpp:252] TEST Top shape for layer 55 'ctx_conv4' 2 64 80 80 (819200)
I0731 19:54:15.760922   830 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I0731 19:54:15.760926   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.760931   830 net.cpp:184] Created Layer ctx_conv4/bn (56)
I0731 19:54:15.760933   830 net.cpp:561] ctx_conv4/bn <- ctx_conv4
I0731 19:54:15.760936   830 net.cpp:513] ctx_conv4/bn -> ctx_conv4 (in-place)
I0731 19:54:15.761652   830 net.cpp:245] Setting up ctx_conv4/bn
I0731 19:54:15.761658   830 net.cpp:252] TEST Top shape for layer 56 'ctx_conv4/bn' 2 64 80 80 (819200)
I0731 19:54:15.761664   830 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I0731 19:54:15.761667   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.761670   830 net.cpp:184] Created Layer ctx_conv4/relu (57)
I0731 19:54:15.761673   830 net.cpp:561] ctx_conv4/relu <- ctx_conv4
I0731 19:54:15.761677   830 net.cpp:513] ctx_conv4/relu -> ctx_conv4 (in-place)
I0731 19:54:15.761679   830 net.cpp:245] Setting up ctx_conv4/relu
I0731 19:54:15.761682   830 net.cpp:252] TEST Top shape for layer 57 'ctx_conv4/relu' 2 64 80 80 (819200)
I0731 19:54:15.761685   830 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I0731 19:54:15.761687   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.761693   830 net.cpp:184] Created Layer ctx_final (58)
I0731 19:54:15.761696   830 net.cpp:561] ctx_final <- ctx_conv4
I0731 19:54:15.761698   830 net.cpp:530] ctx_final -> ctx_final
I0731 19:54:15.766535   830 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'ctx_final' with space 0.02G/1 1  (limit 6.96G, req 0G)
I0731 19:54:15.766548   830 net.cpp:245] Setting up ctx_final
I0731 19:54:15.766553   830 net.cpp:252] TEST Top shape for layer 58 'ctx_final' 2 8 80 80 (102400)
I0731 19:54:15.766558   830 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I0731 19:54:15.766561   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.766564   830 net.cpp:184] Created Layer ctx_final/relu (59)
I0731 19:54:15.766567   830 net.cpp:561] ctx_final/relu <- ctx_final
I0731 19:54:15.766571   830 net.cpp:513] ctx_final/relu -> ctx_final (in-place)
I0731 19:54:15.766579   830 net.cpp:245] Setting up ctx_final/relu
I0731 19:54:15.766583   830 net.cpp:252] TEST Top shape for layer 59 'ctx_final/relu' 2 8 80 80 (102400)
I0731 19:54:15.766587   830 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I0731 19:54:15.766588   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.766593   830 net.cpp:184] Created Layer out_deconv_final_up2 (60)
I0731 19:54:15.766597   830 net.cpp:561] out_deconv_final_up2 <- ctx_final
I0731 19:54:15.766599   830 net.cpp:530] out_deconv_final_up2 -> out_deconv_final_up2
I0731 19:54:15.766942   830 net.cpp:245] Setting up out_deconv_final_up2
I0731 19:54:15.766949   830 net.cpp:252] TEST Top shape for layer 60 'out_deconv_final_up2' 2 8 160 160 (409600)
I0731 19:54:15.766953   830 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I0731 19:54:15.766955   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.766960   830 net.cpp:184] Created Layer out_deconv_final_up4 (61)
I0731 19:54:15.766971   830 net.cpp:561] out_deconv_final_up4 <- out_deconv_final_up2
I0731 19:54:15.766974   830 net.cpp:530] out_deconv_final_up4 -> out_deconv_final_up4
I0731 19:54:15.767274   830 net.cpp:245] Setting up out_deconv_final_up4
I0731 19:54:15.767279   830 net.cpp:252] TEST Top shape for layer 61 'out_deconv_final_up4' 2 8 320 320 (1638400)
I0731 19:54:15.767283   830 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I0731 19:54:15.767285   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.767292   830 net.cpp:184] Created Layer out_deconv_final_up8 (62)
I0731 19:54:15.767295   830 net.cpp:561] out_deconv_final_up8 <- out_deconv_final_up4
I0731 19:54:15.767297   830 net.cpp:530] out_deconv_final_up8 -> out_deconv_final_up8
I0731 19:54:15.767593   830 net.cpp:245] Setting up out_deconv_final_up8
I0731 19:54:15.767598   830 net.cpp:252] TEST Top shape for layer 62 'out_deconv_final_up8' 2 8 640 640 (6553600)
I0731 19:54:15.767601   830 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8_out_deconv_final_up8_0_split' of type 'Split'
I0731 19:54:15.767603   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.767606   830 net.cpp:184] Created Layer out_deconv_final_up8_out_deconv_final_up8_0_split (63)
I0731 19:54:15.767608   830 net.cpp:561] out_deconv_final_up8_out_deconv_final_up8_0_split <- out_deconv_final_up8
I0731 19:54:15.767611   830 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0731 19:54:15.767614   830 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0731 19:54:15.767616   830 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0731 19:54:15.767680   830 net.cpp:245] Setting up out_deconv_final_up8_out_deconv_final_up8_0_split
I0731 19:54:15.767685   830 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 8 640 640 (6553600)
I0731 19:54:15.767688   830 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 8 640 640 (6553600)
I0731 19:54:15.767690   830 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 8 640 640 (6553600)
I0731 19:54:15.767693   830 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0731 19:54:15.767694   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.767699   830 net.cpp:184] Created Layer loss (64)
I0731 19:54:15.767702   830 net.cpp:561] loss <- out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0731 19:54:15.767704   830 net.cpp:561] loss <- label_data_1_split_0
I0731 19:54:15.767707   830 net.cpp:530] loss -> loss
I0731 19:54:15.768566   830 net.cpp:245] Setting up loss
I0731 19:54:15.768575   830 net.cpp:252] TEST Top shape for layer 64 'loss' (1)
I0731 19:54:15.768579   830 net.cpp:256]     with loss weight 1
I0731 19:54:15.768582   830 layer_factory.hpp:136] Creating layer 'accuracy/top1' of type 'Accuracy'
I0731 19:54:15.768585   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.768591   830 net.cpp:184] Created Layer accuracy/top1 (65)
I0731 19:54:15.768594   830 net.cpp:561] accuracy/top1 <- out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0731 19:54:15.768596   830 net.cpp:561] accuracy/top1 <- label_data_1_split_1
I0731 19:54:15.768599   830 net.cpp:530] accuracy/top1 -> accuracy/top1
I0731 19:54:15.768604   830 net.cpp:245] Setting up accuracy/top1
I0731 19:54:15.768606   830 net.cpp:252] TEST Top shape for layer 65 'accuracy/top1' (1)
I0731 19:54:15.768610   830 layer_factory.hpp:136] Creating layer 'accuracy/top5' of type 'Accuracy'
I0731 19:54:15.768611   830 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 19:54:15.768620   830 net.cpp:184] Created Layer accuracy/top5 (66)
I0731 19:54:15.768623   830 net.cpp:561] accuracy/top5 <- out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0731 19:54:15.768626   830 net.cpp:561] accuracy/top5 <- label_data_1_split_2
I0731 19:54:15.768630   830 net.cpp:530] accuracy/top5 -> accuracy/top5
I0731 19:54:15.768633   830 net.cpp:245] Setting up accuracy/top5
I0731 19:54:15.768637   830 net.cpp:252] TEST Top shape for layer 66 'accuracy/top5' (1)
I0731 19:54:15.768640   830 net.cpp:325] accuracy/top5 does not need backward computation.
I0731 19:54:15.768645   830 net.cpp:325] accuracy/top1 does not need backward computation.
I0731 19:54:15.768648   830 net.cpp:323] loss needs backward computation.
I0731 19:54:15.768652   830 net.cpp:323] out_deconv_final_up8_out_deconv_final_up8_0_split needs backward computation.
I0731 19:54:15.768656   830 net.cpp:323] out_deconv_final_up8 needs backward computation.
I0731 19:54:15.768659   830 net.cpp:323] out_deconv_final_up4 needs backward computation.
I0731 19:54:15.768663   830 net.cpp:323] out_deconv_final_up2 needs backward computation.
I0731 19:54:15.768667   830 net.cpp:323] ctx_final/relu needs backward computation.
I0731 19:54:15.768671   830 net.cpp:323] ctx_final needs backward computation.
I0731 19:54:15.768674   830 net.cpp:323] ctx_conv4/relu needs backward computation.
I0731 19:54:15.768678   830 net.cpp:323] ctx_conv4/bn needs backward computation.
I0731 19:54:15.768682   830 net.cpp:323] ctx_conv4 needs backward computation.
I0731 19:54:15.768685   830 net.cpp:323] ctx_conv3/relu needs backward computation.
I0731 19:54:15.768689   830 net.cpp:323] ctx_conv3/bn needs backward computation.
I0731 19:54:15.768692   830 net.cpp:323] ctx_conv3 needs backward computation.
I0731 19:54:15.768697   830 net.cpp:323] ctx_conv2/relu needs backward computation.
I0731 19:54:15.768700   830 net.cpp:323] ctx_conv2/bn needs backward computation.
I0731 19:54:15.768704   830 net.cpp:323] ctx_conv2 needs backward computation.
I0731 19:54:15.768708   830 net.cpp:323] ctx_conv1/relu needs backward computation.
I0731 19:54:15.768712   830 net.cpp:323] ctx_conv1/bn needs backward computation.
I0731 19:54:15.768715   830 net.cpp:323] ctx_conv1 needs backward computation.
I0731 19:54:15.768719   830 net.cpp:323] out3_out5_combined needs backward computation.
I0731 19:54:15.768723   830 net.cpp:323] out3a/relu needs backward computation.
I0731 19:54:15.768728   830 net.cpp:323] out3a/bn needs backward computation.
I0731 19:54:15.768731   830 net.cpp:323] out3a needs backward computation.
I0731 19:54:15.768736   830 net.cpp:323] out5a_up2 needs backward computation.
I0731 19:54:15.768740   830 net.cpp:323] out5a/relu needs backward computation.
I0731 19:54:15.768744   830 net.cpp:323] out5a/bn needs backward computation.
I0731 19:54:15.768748   830 net.cpp:323] out5a needs backward computation.
I0731 19:54:15.768752   830 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0731 19:54:15.768755   830 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0731 19:54:15.768759   830 net.cpp:323] res5a_branch2b needs backward computation.
I0731 19:54:15.768764   830 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0731 19:54:15.768767   830 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0731 19:54:15.768771   830 net.cpp:323] res5a_branch2a needs backward computation.
I0731 19:54:15.768775   830 net.cpp:323] pool4 needs backward computation.
I0731 19:54:15.768779   830 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0731 19:54:15.768784   830 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0731 19:54:15.768788   830 net.cpp:323] res4a_branch2b needs backward computation.
I0731 19:54:15.768792   830 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0731 19:54:15.768796   830 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0731 19:54:15.768800   830 net.cpp:323] res4a_branch2a needs backward computation.
I0731 19:54:15.768808   830 net.cpp:323] pool3 needs backward computation.
I0731 19:54:15.768813   830 net.cpp:323] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I0731 19:54:15.768826   830 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0731 19:54:15.768831   830 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0731 19:54:15.768834   830 net.cpp:323] res3a_branch2b needs backward computation.
I0731 19:54:15.768838   830 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0731 19:54:15.768842   830 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0731 19:54:15.768846   830 net.cpp:323] res3a_branch2a needs backward computation.
I0731 19:54:15.768851   830 net.cpp:323] pool2 needs backward computation.
I0731 19:54:15.768854   830 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0731 19:54:15.768858   830 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0731 19:54:15.768862   830 net.cpp:323] res2a_branch2b needs backward computation.
I0731 19:54:15.768867   830 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0731 19:54:15.768870   830 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0731 19:54:15.768874   830 net.cpp:323] res2a_branch2a needs backward computation.
I0731 19:54:15.768879   830 net.cpp:323] pool1 needs backward computation.
I0731 19:54:15.768883   830 net.cpp:323] conv1b/relu needs backward computation.
I0731 19:54:15.768887   830 net.cpp:323] conv1b/bn needs backward computation.
I0731 19:54:15.768892   830 net.cpp:323] conv1b needs backward computation.
I0731 19:54:15.768895   830 net.cpp:323] conv1a/relu needs backward computation.
I0731 19:54:15.768899   830 net.cpp:323] conv1a/bn needs backward computation.
I0731 19:54:15.768903   830 net.cpp:323] conv1a needs backward computation.
I0731 19:54:15.768908   830 net.cpp:325] data/bias does not need backward computation.
I0731 19:54:15.768913   830 net.cpp:325] label_data_1_split does not need backward computation.
I0731 19:54:15.768918   830 net.cpp:325] data does not need backward computation.
I0731 19:54:15.768921   830 net.cpp:367] This network produces output accuracy/top1
I0731 19:54:15.768925   830 net.cpp:367] This network produces output accuracy/top5
I0731 19:54:15.768929   830 net.cpp:367] This network produces output loss
I0731 19:54:15.768991   830 net.cpp:389] Top memory (TEST) required for data: 318668800 diff: 8
I0731 19:54:15.768996   830 net.cpp:392] Bottom memory (TEST) required for data: 318668800 diff: 318668800
I0731 19:54:15.769001   830 net.cpp:395] Shared (in-place) memory (TEST) by data: 210124800 diff: 210124800
I0731 19:54:15.769004   830 net.cpp:398] Parameters memory (TEST) required for data: 2692608 diff: 2692608
I0731 19:54:15.769008   830 net.cpp:401] Parameters shared memory (TEST) by data: 0 diff: 0
I0731 19:54:15.769012   830 net.cpp:407] Network initialization done.
I0731 19:54:15.769125   830 solver.cpp:56] Solver scaffolding done.
I0731 19:54:15.779228   830 caffe.cpp:137] Finetuning from training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/initial/cityscapes5_jsegnet21v2_iter_32000.caffemodel
I0731 19:54:15.784060   830 net.cpp:1089] Copying source layer data Type:ImageLabelData #blobs=0
I0731 19:54:15.784081   830 net.cpp:1089] Copying source layer data/bias Type:Bias #blobs=1
I0731 19:54:15.784112   830 net.cpp:1089] Copying source layer conv1a Type:Convolution #blobs=2
I0731 19:54:15.784126   830 net.cpp:1089] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0731 19:54:15.784687   830 net.cpp:1089] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0731 19:54:15.784694   830 net.cpp:1089] Copying source layer conv1b Type:Convolution #blobs=2
I0731 19:54:15.784703   830 net.cpp:1089] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0731 19:54:15.785145   830 net.cpp:1089] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0731 19:54:15.785152   830 net.cpp:1089] Copying source layer pool1 Type:Pooling #blobs=0
I0731 19:54:15.785156   830 net.cpp:1089] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0731 19:54:15.785182   830 net.cpp:1089] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0731 19:54:15.785617   830 net.cpp:1089] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0731 19:54:15.785624   830 net.cpp:1089] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0731 19:54:15.785636   830 net.cpp:1089] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0731 19:54:15.786051   830 net.cpp:1089] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0731 19:54:15.786057   830 net.cpp:1089] Copying source layer pool2 Type:Pooling #blobs=0
I0731 19:54:15.786061   830 net.cpp:1089] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0731 19:54:15.786098   830 net.cpp:1089] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0731 19:54:15.786536   830 net.cpp:1089] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0731 19:54:15.786548   830 net.cpp:1089] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0731 19:54:15.786602   830 net.cpp:1089] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0731 19:54:15.787401   830 net.cpp:1089] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0731 19:54:15.787410   830 net.cpp:1089] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I0731 19:54:15.787415   830 net.cpp:1089] Copying source layer pool3 Type:Pooling #blobs=0
I0731 19:54:15.787420   830 net.cpp:1089] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0731 19:54:15.787683   830 net.cpp:1089] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0731 19:54:15.788213   830 net.cpp:1089] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0731 19:54:15.788221   830 net.cpp:1089] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0731 19:54:15.788300   830 net.cpp:1089] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0731 19:54:15.788776   830 net.cpp:1089] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0731 19:54:15.788784   830 net.cpp:1089] Copying source layer pool4 Type:Pooling #blobs=0
I0731 19:54:15.788789   830 net.cpp:1089] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0731 19:54:15.789212   830 net.cpp:1089] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0731 19:54:15.789686   830 net.cpp:1089] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0731 19:54:15.789695   830 net.cpp:1089] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0731 19:54:15.789901   830 net.cpp:1089] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0731 19:54:15.790376   830 net.cpp:1089] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0731 19:54:15.790385   830 net.cpp:1089] Copying source layer out5a Type:Convolution #blobs=2
I0731 19:54:15.790449   830 net.cpp:1089] Copying source layer out5a/bn Type:BatchNorm #blobs=5
I0731 19:54:15.790673   830 net.cpp:1089] Copying source layer out5a/relu Type:ReLU #blobs=0
I0731 19:54:15.790679   830 net.cpp:1089] Copying source layer out5a_up2 Type:Deconvolution #blobs=1
I0731 19:54:15.790688   830 net.cpp:1089] Copying source layer out3a Type:Convolution #blobs=2
I0731 19:54:15.790712   830 net.cpp:1089] Copying source layer out3a/bn Type:BatchNorm #blobs=5
I0731 19:54:15.790916   830 net.cpp:1089] Copying source layer out3a/relu Type:ReLU #blobs=0
I0731 19:54:15.790922   830 net.cpp:1089] Copying source layer out3_out5_combined Type:Eltwise #blobs=0
I0731 19:54:15.790926   830 net.cpp:1089] Copying source layer ctx_conv1 Type:Convolution #blobs=2
I0731 19:54:15.790951   830 net.cpp:1089] Copying source layer ctx_conv1/bn Type:BatchNorm #blobs=5
I0731 19:54:15.791152   830 net.cpp:1089] Copying source layer ctx_conv1/relu Type:ReLU #blobs=0
I0731 19:54:15.791158   830 net.cpp:1089] Copying source layer ctx_conv2 Type:Convolution #blobs=2
I0731 19:54:15.791189   830 net.cpp:1089] Copying source layer ctx_conv2/bn Type:BatchNorm #blobs=5
I0731 19:54:15.791388   830 net.cpp:1089] Copying source layer ctx_conv2/relu Type:ReLU #blobs=0
I0731 19:54:15.791405   830 net.cpp:1089] Copying source layer ctx_conv3 Type:Convolution #blobs=2
I0731 19:54:15.791432   830 net.cpp:1089] Copying source layer ctx_conv3/bn Type:BatchNorm #blobs=5
I0731 19:54:15.791637   830 net.cpp:1089] Copying source layer ctx_conv3/relu Type:ReLU #blobs=0
I0731 19:54:15.791643   830 net.cpp:1089] Copying source layer ctx_conv4 Type:Convolution #blobs=2
I0731 19:54:15.791668   830 net.cpp:1089] Copying source layer ctx_conv4/bn Type:BatchNorm #blobs=5
I0731 19:54:15.791868   830 net.cpp:1089] Copying source layer ctx_conv4/relu Type:ReLU #blobs=0
I0731 19:54:15.791875   830 net.cpp:1089] Copying source layer ctx_final Type:Convolution #blobs=2
I0731 19:54:15.791889   830 net.cpp:1089] Copying source layer ctx_final/relu Type:ReLU #blobs=0
I0731 19:54:15.791893   830 net.cpp:1089] Copying source layer out_deconv_final_up2 Type:Deconvolution #blobs=1
I0731 19:54:15.791901   830 net.cpp:1089] Copying source layer out_deconv_final_up4 Type:Deconvolution #blobs=1
I0731 19:54:15.791910   830 net.cpp:1089] Copying source layer out_deconv_final_up8 Type:Deconvolution #blobs=1
I0731 19:54:15.791921   830 net.cpp:1089] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0731 19:54:15.796180   830 net.cpp:1089] Copying source layer data Type:ImageLabelData #blobs=0
I0731 19:54:15.796203   830 net.cpp:1089] Copying source layer data/bias Type:Bias #blobs=1
I0731 19:54:15.796234   830 net.cpp:1089] Copying source layer conv1a Type:Convolution #blobs=2
I0731 19:54:15.796252   830 net.cpp:1089] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0731 19:54:15.796957   830 net.cpp:1089] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0731 19:54:15.796965   830 net.cpp:1089] Copying source layer conv1b Type:Convolution #blobs=2
I0731 19:54:15.796977   830 net.cpp:1089] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0731 19:54:15.797461   830 net.cpp:1089] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0731 19:54:15.797468   830 net.cpp:1089] Copying source layer pool1 Type:Pooling #blobs=0
I0731 19:54:15.797472   830 net.cpp:1089] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0731 19:54:15.797488   830 net.cpp:1089] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0731 19:54:15.797904   830 net.cpp:1089] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0731 19:54:15.797911   830 net.cpp:1089] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0731 19:54:15.797924   830 net.cpp:1089] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0731 19:54:15.798310   830 net.cpp:1089] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0731 19:54:15.798316   830 net.cpp:1089] Copying source layer pool2 Type:Pooling #blobs=0
I0731 19:54:15.798318   830 net.cpp:1089] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0731 19:54:15.798354   830 net.cpp:1089] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0731 19:54:15.798717   830 net.cpp:1089] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0731 19:54:15.798722   830 net.cpp:1089] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0731 19:54:15.798743   830 net.cpp:1089] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0731 19:54:15.799094   830 net.cpp:1089] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0731 19:54:15.799099   830 net.cpp:1089] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I0731 19:54:15.799103   830 net.cpp:1089] Copying source layer pool3 Type:Pooling #blobs=0
I0731 19:54:15.799104   830 net.cpp:1089] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0731 19:54:15.799214   830 net.cpp:1089] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0731 19:54:15.799567   830 net.cpp:1089] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0731 19:54:15.799572   830 net.cpp:1089] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0731 19:54:15.799644   830 net.cpp:1089] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0731 19:54:15.800000   830 net.cpp:1089] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0731 19:54:15.800005   830 net.cpp:1089] Copying source layer pool4 Type:Pooling #blobs=0
I0731 19:54:15.800009   830 net.cpp:1089] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0731 19:54:15.800374   830 net.cpp:1089] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0731 19:54:15.800734   830 net.cpp:1089] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0731 19:54:15.800740   830 net.cpp:1089] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0731 19:54:15.800927   830 net.cpp:1089] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0731 19:54:15.801281   830 net.cpp:1089] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0731 19:54:15.801286   830 net.cpp:1089] Copying source layer out5a Type:Convolution #blobs=2
I0731 19:54:15.801338   830 net.cpp:1089] Copying source layer out5a/bn Type:BatchNorm #blobs=5
I0731 19:54:15.801499   830 net.cpp:1089] Copying source layer out5a/relu Type:ReLU #blobs=0
I0731 19:54:15.801504   830 net.cpp:1089] Copying source layer out5a_up2 Type:Deconvolution #blobs=1
I0731 19:54:15.801509   830 net.cpp:1089] Copying source layer out3a Type:Convolution #blobs=2
I0731 19:54:15.801528   830 net.cpp:1089] Copying source layer out3a/bn Type:BatchNorm #blobs=5
I0731 19:54:15.801674   830 net.cpp:1089] Copying source layer out3a/relu Type:ReLU #blobs=0
I0731 19:54:15.801679   830 net.cpp:1089] Copying source layer out3_out5_combined Type:Eltwise #blobs=0
I0731 19:54:15.801681   830 net.cpp:1089] Copying source layer ctx_conv1 Type:Convolution #blobs=2
I0731 19:54:15.801702   830 net.cpp:1089] Copying source layer ctx_conv1/bn Type:BatchNorm #blobs=5
I0731 19:54:15.801846   830 net.cpp:1089] Copying source layer ctx_conv1/relu Type:ReLU #blobs=0
I0731 19:54:15.801851   830 net.cpp:1089] Copying source layer ctx_conv2 Type:Convolution #blobs=2
I0731 19:54:15.801870   830 net.cpp:1089] Copying source layer ctx_conv2/bn Type:BatchNorm #blobs=5
I0731 19:54:15.802016   830 net.cpp:1089] Copying source layer ctx_conv2/relu Type:ReLU #blobs=0
I0731 19:54:15.802021   830 net.cpp:1089] Copying source layer ctx_conv3 Type:Convolution #blobs=2
I0731 19:54:15.802040   830 net.cpp:1089] Copying source layer ctx_conv3/bn Type:BatchNorm #blobs=5
I0731 19:54:15.802186   830 net.cpp:1089] Copying source layer ctx_conv3/relu Type:ReLU #blobs=0
I0731 19:54:15.802191   830 net.cpp:1089] Copying source layer ctx_conv4 Type:Convolution #blobs=2
I0731 19:54:15.802211   830 net.cpp:1089] Copying source layer ctx_conv4/bn Type:BatchNorm #blobs=5
I0731 19:54:15.802356   830 net.cpp:1089] Copying source layer ctx_conv4/relu Type:ReLU #blobs=0
I0731 19:54:15.802361   830 net.cpp:1089] Copying source layer ctx_final Type:Convolution #blobs=2
I0731 19:54:15.802369   830 net.cpp:1089] Copying source layer ctx_final/relu Type:ReLU #blobs=0
I0731 19:54:15.802371   830 net.cpp:1089] Copying source layer out_deconv_final_up2 Type:Deconvolution #blobs=1
I0731 19:54:15.802376   830 net.cpp:1089] Copying source layer out_deconv_final_up4 Type:Deconvolution #blobs=1
I0731 19:54:15.802382   830 net.cpp:1089] Copying source layer out_deconv_final_up8 Type:Deconvolution #blobs=1
I0731 19:54:15.802387   830 net.cpp:1089] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0731 19:54:15.802477   830 parallel.cpp:108] [0 - 0] P2pSync adding callback
I0731 19:54:15.802482   830 parallel.cpp:108] [1 - 1] P2pSync adding callback
I0731 19:54:15.802484   830 parallel.cpp:108] [2 - 2] P2pSync adding callback
I0731 19:54:15.802486   830 parallel.cpp:61] Starting Optimization
I0731 19:54:15.802489   830 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 19:54:15.802515   830 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 19:54:15.802525   830 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 19:54:15.803225   939 device_alternate.hpp:116] NVML initialized on thread 136111348954880
I0731 19:54:15.821187   939 common.cpp:583] NVML succeeded to set CPU affinity on device 0
I0731 19:54:15.821238   940 device_alternate.hpp:116] NVML initialized on thread 136111340562176
I0731 19:54:15.822722   940 common.cpp:583] NVML succeeded to set CPU affinity on device 1
I0731 19:54:15.822763   941 device_alternate.hpp:116] NVML initialized on thread 136111332169472
I0731 19:54:15.823607   941 common.cpp:583] NVML succeeded to set CPU affinity on device 2
I0731 19:54:15.827250   940 solver.cpp:42] Solver data type: FLOAT
W0731 19:54:15.827801   940 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0731 19:54:15.827908   940 net.cpp:104] Using FLOAT as default forward math type
I0731 19:54:15.827913   940 net.cpp:110] Using FLOAT as default backward math type
I0731 19:54:15.827941   940 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0731 19:54:15.827949   940 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 19:54:15.831079   941 solver.cpp:42] Solver data type: FLOAT
W0731 19:54:15.831598   941 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0731 19:54:15.831703   941 net.cpp:104] Using FLOAT as default forward math type
I0731 19:54:15.831708   941 net.cpp:110] Using FLOAT as default backward math type
I0731 19:54:15.831733   941 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0731 19:54:15.831738   941 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 19:54:15.831769   942 db_lmdb.cpp:35] Opened lmdb data/train-image-lmdb
I0731 19:54:15.832594   943 db_lmdb.cpp:35] Opened lmdb data/train-image-lmdb
I0731 19:54:15.836418   940 data_layer.cpp:184] [1] ReshapePrefetch 6, 3, 640, 640
I0731 19:54:15.836861   941 data_layer.cpp:184] [2] ReshapePrefetch 6, 3, 640, 640
I0731 19:54:15.836912   940 data_layer.cpp:208] [1] Output data size: 6, 3, 640, 640
I0731 19:54:15.836925   940 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 19:54:15.836941   941 data_layer.cpp:208] [2] Output data size: 6, 3, 640, 640
I0731 19:54:15.836946   941 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 19:54:15.837121   941 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0731 19:54:15.837131   941 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 19:54:15.837170   940 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0731 19:54:15.837182   940 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 19:54:15.837942   944 data_layer.cpp:97] [2] Parser threads: 1
I0731 19:54:15.837954   944 data_layer.cpp:99] [2] Transformer threads: 1
I0731 19:54:15.843523   945 data_layer.cpp:97] [1] Parser threads: 1
I0731 19:54:15.843556   945 data_layer.cpp:99] [1] Transformer threads: 1
I0731 19:54:15.849803   946 db_lmdb.cpp:35] Opened lmdb data/train-label-lmdb
I0731 19:54:15.851642   947 db_lmdb.cpp:35] Opened lmdb data/train-label-lmdb
I0731 19:54:15.851671   941 data_layer.cpp:184] [2] ReshapePrefetch 6, 1, 640, 640
I0731 19:54:15.852005   941 data_layer.cpp:208] [2] Output data size: 6, 1, 640, 640
I0731 19:54:15.852027   941 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 19:54:15.854154   948 data_layer.cpp:97] [2] Parser threads: 1
I0731 19:54:15.854248   948 data_layer.cpp:99] [2] Transformer threads: 1
I0731 19:54:15.859083   940 data_layer.cpp:184] [1] ReshapePrefetch 6, 1, 640, 640
I0731 19:54:15.859390   944 blocking_queue.cpp:40] Waiting for datum
I0731 19:54:15.860092   940 data_layer.cpp:208] [1] Output data size: 6, 1, 640, 640
I0731 19:54:15.860122   940 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 19:54:15.865092   949 data_layer.cpp:97] [1] Parser threads: 1
I0731 19:54:15.865133   949 data_layer.cpp:99] [1] Transformer threads: 1
I0731 19:54:16.398991   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 7.99G, req 0G)
I0731 19:54:16.420138   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 0  (limit 7.99G, req 0G)
I0731 19:54:16.449851   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.83G, req 0G)
I0731 19:54:16.474705   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.83G, req 0G)
I0731 19:54:16.499472   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.7G, req 0G)
I0731 19:54:16.525719   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.7G, req 0G)
I0731 19:54:16.527765   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.62G, req 0G)
I0731 19:54:16.556030   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.62G, req 0G)
I0731 19:54:16.559341   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 7.56G, req 0G)
I0731 19:54:16.573261   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.52G, req 0G)
I0731 19:54:16.583153   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 7.56G, req 0G)
I0731 19:54:16.599076   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.52G, req 0G)
I0731 19:54:16.604113   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.48G, req 0G)
I0731 19:54:16.615919   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.46G, req 0G)
I0731 19:54:16.632206   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.48G, req 0G)
I0731 19:54:16.643980   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.46G, req 0G)
I0731 19:54:16.675894   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 7.39G, req 0G)
I0731 19:54:16.694242   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 7.34G, req 0G)
I0731 19:54:16.712069   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 7.39G, req 0G)
I0731 19:54:16.716853   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 7.32G, req 0G)
I0731 19:54:16.719987   940 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/l1reg/test.prototxt
W0731 19:54:16.720055   940 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0731 19:54:16.720193   940 net.cpp:104] Using FLOAT as default forward math type
I0731 19:54:16.720198   940 net.cpp:110] Using FLOAT as default backward math type
I0731 19:54:16.720227   940 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0731 19:54:16.720232   940 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 19:54:16.721117   964 db_lmdb.cpp:35] Opened lmdb data/val-image-lmdb
I0731 19:54:16.722640   940 data_layer.cpp:184] (1) ReshapePrefetch 2, 3, 640, 640
I0731 19:54:16.722724   940 data_layer.cpp:208] (1) Output data size: 2, 3, 640, 640
I0731 19:54:16.722743   940 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 19:54:16.722790   940 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0731 19:54:16.722811   940 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 19:54:16.723523   965 data_layer.cpp:97] (1) Parser threads: 1
I0731 19:54:16.723552   965 data_layer.cpp:99] (1) Transformer threads: 1
I0731 19:54:16.726212   966 db_lmdb.cpp:35] Opened lmdb data/val-label-lmdb
I0731 19:54:16.727339   940 data_layer.cpp:184] (1) ReshapePrefetch 2, 1, 640, 640
I0731 19:54:16.727438   940 data_layer.cpp:208] (1) Output data size: 2, 1, 640, 640
I0731 19:54:16.727445   940 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 19:54:16.728826   967 data_layer.cpp:97] (1) Parser threads: 1
I0731 19:54:16.728840   967 data_layer.cpp:99] (1) Transformer threads: 1
I0731 19:54:16.731699   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 7.34G, req 0G)
I0731 19:54:16.738030   940 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.18G, req 0G)
I0731 19:54:16.754645   940 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.15G, req 0G)
I0731 19:54:16.758494   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 7.32G, req 0G)
I0731 19:54:16.761687   941 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/l1reg/test.prototxt
W0731 19:54:16.761760   941 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0731 19:54:16.761901   941 net.cpp:104] Using FLOAT as default forward math type
I0731 19:54:16.761906   941 net.cpp:110] Using FLOAT as default backward math type
I0731 19:54:16.761925   941 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0731 19:54:16.761930   941 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 19:54:16.762670   968 db_lmdb.cpp:35] Opened lmdb data/val-image-lmdb
I0731 19:54:16.764781   940 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 1  (limit 7.13G, req 0G)
I0731 19:54:16.764847   941 data_layer.cpp:184] (2) ReshapePrefetch 2, 3, 640, 640
I0731 19:54:16.764971   941 data_layer.cpp:208] (2) Output data size: 2, 3, 640, 640
I0731 19:54:16.764979   941 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 19:54:16.765024   941 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0731 19:54:16.765033   941 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 19:54:16.765763   970 data_layer.cpp:97] (2) Parser threads: 1
I0731 19:54:16.765780   970 data_layer.cpp:99] (2) Transformer threads: 1
I0731 19:54:16.768791   971 db_lmdb.cpp:35] Opened lmdb data/val-label-lmdb
I0731 19:54:16.770108   941 data_layer.cpp:184] (2) ReshapePrefetch 2, 1, 640, 640
I0731 19:54:16.770187   941 data_layer.cpp:208] (2) Output data size: 2, 1, 640, 640
I0731 19:54:16.770193   941 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 19:54:16.771611   972 data_layer.cpp:97] (2) Parser threads: 1
I0731 19:54:16.771627   972 data_layer.cpp:99] (2) Transformer threads: 1
I0731 19:54:16.778089   940 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.12G, req 0G)
I0731 19:54:16.781675   941 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.18G, req 0G)
I0731 19:54:16.786357   940 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.1G, req 0G)
I0731 19:54:16.792389   940 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.09G, req 0G)
I0731 19:54:16.799931   941 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.15G, req 0G)
I0731 19:54:16.805920   940 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.09G, req 0G)
I0731 19:54:16.812507   941 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.13G, req 0G)
I0731 19:54:16.814327   940 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.08G, req 0G)
I0731 19:54:16.821131   941 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.12G, req 0G)
I0731 19:54:16.828341   941 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.1G, req 0G)
I0731 19:54:16.834671   941 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.09G, req 0G)
I0731 19:54:16.848481   941 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.09G, req 0G)
I0731 19:54:16.856329   941 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.08G, req 0G)
I0731 19:54:16.866878   940 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 7.07G, req 0G)
I0731 19:54:16.872887   940 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 7.06G, req 0G)
I0731 19:54:16.884866   940 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 7.05G, req 0G)
I0731 19:54:16.887861   940 solver.cpp:56] Solver scaffolding done.
I0731 19:54:16.908162   941 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 7.07G, req 0G)
I0731 19:54:16.914868   941 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 7.06G, req 0G)
I0731 19:54:16.927439   941 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 7.05G, req 0G)
I0731 19:54:16.929726   941 solver.cpp:56] Solver scaffolding done.
I0731 19:54:16.989797   941 parallel.cpp:164] [2 - 2] P2pSync adding callback
I0731 19:54:16.989797   940 parallel.cpp:164] [1 - 1] P2pSync adding callback
I0731 19:54:16.989822   939 parallel.cpp:164] [0 - 0] P2pSync adding callback
I0731 19:54:17.201231   939 solver.cpp:479] Solving jsegnet21v2_train
I0731 19:54:17.201248   939 solver.cpp:480] Learning Rate Policy: multistep
I0731 19:54:17.201257   941 solver.cpp:479] Solving jsegnet21v2_train
I0731 19:54:17.201266   941 solver.cpp:480] Learning Rate Policy: multistep
I0731 19:54:17.201321   940 solver.cpp:479] Solving jsegnet21v2_train
I0731 19:54:17.201330   940 solver.cpp:480] Learning Rate Policy: multistep
I0731 19:54:17.215128   940 solver.cpp:268] Starting Optimization on GPU 1
I0731 19:54:17.215134   939 solver.cpp:268] Starting Optimization on GPU 0
I0731 19:54:17.215129   941 solver.cpp:268] Starting Optimization on GPU 2
I0731 19:54:17.215330   993 device_alternate.hpp:116] NVML initialized on thread 128111172302592
I0731 19:54:17.215340   939 solver.cpp:550] Iteration 0, Testing net (#0)
I0731 19:54:17.215353   993 common.cpp:583] NVML succeeded to set CPU affinity on device 2
I0731 19:54:17.215363   994 device_alternate.hpp:116] NVML initialized on thread 128111180695296
I0731 19:54:17.215376   994 common.cpp:583] NVML succeeded to set CPU affinity on device 1
I0731 19:54:17.215386   995 device_alternate.hpp:116] NVML initialized on thread 128111163909888
I0731 19:54:17.215396   995 common.cpp:583] NVML succeeded to set CPU affinity on device 0
I0731 19:54:17.231560   941 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 6.96G, req 0G)
I0731 19:54:17.233786   940 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 6.96G, req 0G)
I0731 19:54:17.252002   941 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 6.9G, req 0G)
I0731 19:54:17.252372   940 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 6.9G, req 0G)
I0731 19:54:17.266181   939 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1a' with space 0.01G/1 1  (limit 6.88G, req 0G)
I0731 19:54:17.268043   940 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 6.84G, req 0G)
I0731 19:54:17.269210   941 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 6.84G, req 0G)
I0731 19:54:17.278429   940 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 6.81G, req 0G)
I0731 19:54:17.280594   941 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 6.81G, req 0G)
I0731 19:54:17.285269   939 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 6.82G, req 0G)
I0731 19:54:17.286295   940 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 6.78G, req 0G)
I0731 19:54:17.287683   941 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 6.78G, req 0G)
I0731 19:54:17.293311   940 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 6.76G, req 0G)
I0731 19:54:17.295183   941 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 6.76G, req 0G)
I0731 19:54:17.302363   939 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 6.75G, req 0G)
I0731 19:54:17.309957   940 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.74G, req 0G)
I0731 19:54:17.325206   941 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.74G, req 0G)
I0731 19:54:17.339277   939 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 6.73G, req 0G)
I0731 19:54:17.339521   940 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.74G, req 0G)
I0731 19:54:17.340183   941 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.74G, req 0G)
I0731 19:54:17.348664   939 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 6.69G, req 0G)
I0731 19:54:17.353886   939 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 6.67G, req 0G)
I0731 19:54:17.362522   939 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.66G, req 0G)
I0731 19:54:17.365991   939 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.65G, req 0G)
I0731 19:54:17.369945   940 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.58G, req 0G)
I0731 19:54:17.371572   941 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.58G, req 0G)
I0731 19:54:17.377269   940 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.57G, req 0G)
I0731 19:54:17.377672   941 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.57G, req 0G)
I0731 19:54:17.394341   939 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.5G, req 0G)
I0731 19:54:17.401204   939 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.49G, req 0G)
I0731 19:54:17.403079   940 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.48G, req 0G)
I0731 19:54:17.403528   941 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.48G, req 0G)
I0731 19:54:17.422211   939 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.39G, req 0G)
I0731 19:54:17.518098   939 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.913131
I0731 19:54:17.518123   939 solver.cpp:635]     Test net output #1: accuracy/top5 = 1
I0731 19:54:17.518128   939 solver.cpp:635]     Test net output #2: loss = 0.203904 (* 1 = 0.203904 loss)
I0731 19:54:17.518187   939 solver.cpp:295] [MultiGPU] Initial Test completed
I0731 19:54:17.600782   939 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 6.19G, req 0G)
I0731 19:54:17.603092   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 6.28G, req 0G)
I0731 19:54:17.605846   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 0  (limit 6.28G, req 0G)
I0731 19:54:17.666666   939 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 6.03G, req 0G)
I0731 19:54:17.673681   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 6.12G, req 0G)
I0731 19:54:17.683203   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 6.12G, req 0G)
I0731 19:54:17.730412   939 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 5.85G, req 0G)
I0731 19:54:17.740702   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 1 4 3  (limit 5.94G, req 0G)
I0731 19:54:17.746624   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 5.94G, req 0G)
I0731 19:54:17.762652   939 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 5.77G, req 0G)
I0731 19:54:17.774299   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 5.86G, req 0G)
I0731 19:54:17.779783   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 5.86G, req 0G)
I0731 19:54:17.787881   939 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 5.68G, req 0G)
I0731 19:54:17.799230   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 5.77G, req 0G)
I0731 19:54:17.800715   939 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 5.64G, req 0G)
I0731 19:54:17.806252   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 5.77G, req 0G)
I0731 19:54:17.815181   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 5.72G, req 0G)
I0731 19:54:17.820674   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 5.72G, req 0G)
I0731 19:54:17.824702   939 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 5.61G, req 0G)
I0731 19:54:17.832846   939 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 5.59G, req 0G)
I0731 19:54:17.835887   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 5.7G, req 0G)
I0731 19:54:17.843008   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 5.7G, req 0G)
I0731 19:54:17.843982   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 5.68G, req 0G)
I0731 19:54:17.852718   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 5.68G, req 0G)
I0731 19:54:17.879532   939 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 5.32G, req 0G)
I0731 19:54:17.891922   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 5.41G, req 0G)
I0731 19:54:17.892807   939 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 5.3G, req 0G)
I0731 19:54:17.900441   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 5.41G, req 0G)
I0731 19:54:17.906700   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 5.39G, req 0G)
I0731 19:54:17.917001   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 5.39G, req 0G)
I0731 19:54:17.921819   939 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 5.14G, req 0G)
I0731 19:54:17.938982   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 5.23G, req 0G)
I0731 19:54:17.947180   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 5.23G, req 0G)
I0731 19:54:18.120656   939 solver.cpp:358] Iteration 0 (0.602424 s), loss = 0.100357
I0731 19:54:18.120676   939 solver.cpp:375]     Train net output #0: loss = 0.100357 (* 1 = 0.100357 loss)
I0731 19:54:18.120682   939 sgd_solver.cpp:136] Iteration 0, lr = 1e-05, m = 0.9
I0731 19:54:18.340522   939 solver.cpp:358] Iteration 1 (0.219851 s), loss = 0.115906
I0731 19:54:18.340543   939 solver.cpp:375]     Train net output #0: loss = 0.115906 (* 1 = 0.115906 loss)
I0731 19:54:18.412173   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1a' with space 1.29G/1 1 0 3  (limit 3.08G, req 0G)
I0731 19:54:18.428428   939 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1a' with space 1.29G/1 1 0 3  (limit 2.99G, req 0G)
I0731 19:54:18.430943   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1a' with space 1.29G/1 1 0 3  (limit 3.08G, req 0G)
I0731 19:54:18.488687   939 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1b' with space 2.57G/2 6 4 3  (limit 1.71G, req 0G)
I0731 19:54:18.490432   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0G)
I0731 19:54:18.495631   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0G)
I0731 19:54:18.618576   939 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 2.57G/1 1 4 3  (limit 1.71G, req 0G)
I0731 19:54:18.627189   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 2.57G/1 6 4 3  (limit 1.79G, req 0G)
I0731 19:54:18.634335   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 2.57G/1 6 4 3  (limit 1.79G, req 0G)
I0731 19:54:18.665750   939 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 2.57G/2 6 4 3  (limit 1.71G, req 0G)
I0731 19:54:18.681157   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0G)
I0731 19:54:18.682981   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0G)
I0731 19:54:18.765100   939 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 2.57G/1 6 4 5  (limit 1.71G, req 0.07G)
I0731 19:54:18.781916   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 2.57G/1 6 4 5  (limit 1.79G, req 0.07G)
I0731 19:54:18.783591   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 2.57G/1 6 4 5  (limit 1.79G, req 0.07G)
I0731 19:54:18.789046   939 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 2.57G/2 6 4 3  (limit 1.71G, req 0.07G)
I0731 19:54:18.806897   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0.07G)
I0731 19:54:18.810487   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0.07G)
I0731 19:54:18.853585   939 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 2.57G/1 6 4 5  (limit 1.71G, req 0.07G)
I0731 19:54:18.867079   939 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 2.57G/2 6 4 5  (limit 1.71G, req 0.07G)
I0731 19:54:18.877096   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 2.57G/1 6 4 5  (limit 1.79G, req 0.07G)
I0731 19:54:18.881006   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 2.57G/1 6 4 5  (limit 1.79G, req 0.07G)
I0731 19:54:18.891497   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 2.57G/2 6 4 5  (limit 1.79G, req 0.07G)
I0731 19:54:18.895179   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0.07G)
I0731 19:54:18.917647   939 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'out3a' with space 2.57G/2 6 4 3  (limit 1.71G, req 0.07G)
I0731 19:54:18.947290   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'out3a' with space 2.57G/2 6 4 3  (limit 1.79G, req 0.07G)
I0731 19:54:18.952214   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'out3a' with space 2.57G/2 6 4 3  (limit 1.79G, req 0.07G)
I0731 19:54:18.972178   939 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 2.57G/1 6 4 3  (limit 1.71G, req 0.07G)
I0731 19:54:18.998374   939 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 2.57G/1 6 1 5  (limit 1.71G, req 0.07G)
I0731 19:54:19.003435   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 2.57G/1 6 4 3  (limit 1.79G, req 0.07G)
I0731 19:54:19.009099   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 2.57G/1 6 4 3  (limit 1.79G, req 0.07G)
I0731 19:54:19.029953   940 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 2.57G/1 6 1 5  (limit 1.79G, req 0.07G)
I0731 19:54:19.036820   941 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 2.57G/1 6 1 5  (limit 1.79G, req 0.07G)
I0731 19:54:19.166751   939 solver.cpp:358] Iteration 2 (0.826196 s), loss = 0.0694385
I0731 19:54:19.166779   939 solver.cpp:375]     Train net output #0: loss = 0.0694385 (* 1 = 0.0694385 loss)
I0731 19:54:19.167405   940 cudnn_conv_layer.cpp:292] [1] Layer 'conv1a' reallocating workspace: 2.57G -> 0.14G
I0731 19:54:19.167407   941 cudnn_conv_layer.cpp:292] [2] Layer 'conv1a' reallocating workspace: 2.57G -> 0.14G
I0731 19:54:19.170024   939 cudnn_conv_layer.cpp:292] [0] Layer 'conv1a' reallocating workspace: 2.57G -> 0.14G
I0731 19:54:37.237655   939 solver.cpp:353] Iteration 100 (5.42323 iter/s, 18.0704s/98 iter), loss = 0.0749862
I0731 19:54:37.237681   939 solver.cpp:375]     Train net output #0: loss = 0.0749862 (* 1 = 0.0749862 loss)
I0731 19:54:37.237685   939 sgd_solver.cpp:136] Iteration 100, lr = 1e-05, m = 0.9
I0731 19:54:48.729470   894 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 19:54:55.686940   939 solver.cpp:353] Iteration 200 (5.42041 iter/s, 18.4488s/100 iter), loss = 0.116966
I0731 19:54:55.686966   939 solver.cpp:375]     Train net output #0: loss = 0.116966 (* 1 = 0.116966 loss)
I0731 19:54:55.686971   939 sgd_solver.cpp:136] Iteration 200, lr = 1e-05, m = 0.9
I0731 19:55:14.210649   939 solver.cpp:353] Iteration 300 (5.39864 iter/s, 18.5232s/100 iter), loss = 0.0817837
I0731 19:55:14.210675   939 solver.cpp:375]     Train net output #0: loss = 0.0817837 (* 1 = 0.0817837 loss)
I0731 19:55:14.210680   939 sgd_solver.cpp:136] Iteration 300, lr = 1e-05, m = 0.9
I0731 19:55:19.226295   943 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 19:55:32.647809   939 solver.cpp:353] Iteration 400 (5.42398 iter/s, 18.4366s/100 iter), loss = 0.138529
I0731 19:55:32.647831   939 solver.cpp:375]     Train net output #0: loss = 0.138529 (* 1 = 0.138529 loss)
I0731 19:55:32.647837   939 sgd_solver.cpp:136] Iteration 400, lr = 1e-05, m = 0.9
I0731 19:55:51.185993   939 solver.cpp:353] Iteration 500 (5.39442 iter/s, 18.5377s/100 iter), loss = 0.0804452
I0731 19:55:51.186046   939 solver.cpp:375]     Train net output #0: loss = 0.0804452 (* 1 = 0.0804452 loss)
I0731 19:55:51.186053   939 sgd_solver.cpp:136] Iteration 500, lr = 1e-05, m = 0.9
I0731 19:56:09.673460   939 solver.cpp:353] Iteration 600 (5.40922 iter/s, 18.487s/100 iter), loss = 0.0726721
I0731 19:56:09.673482   939 solver.cpp:375]     Train net output #0: loss = 0.0726721 (* 1 = 0.0726721 loss)
I0731 19:56:09.673486   939 sgd_solver.cpp:136] Iteration 600, lr = 1e-05, m = 0.9
I0731 19:56:20.427151   946 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 19:56:28.175742   939 solver.cpp:353] Iteration 700 (5.40489 iter/s, 18.5018s/100 iter), loss = 0.0959757
I0731 19:56:28.175873   939 solver.cpp:375]     Train net output #0: loss = 0.0959756 (* 1 = 0.0959756 loss)
I0731 19:56:28.175879   939 sgd_solver.cpp:136] Iteration 700, lr = 1e-05, m = 0.9
I0731 19:56:46.673211   939 solver.cpp:353] Iteration 800 (5.40629 iter/s, 18.497s/100 iter), loss = 0.0967733
I0731 19:56:46.673235   939 solver.cpp:375]     Train net output #0: loss = 0.0967733 (* 1 = 0.0967733 loss)
I0731 19:56:46.673240   939 sgd_solver.cpp:136] Iteration 800, lr = 1e-05, m = 0.9
I0731 19:57:05.260005   939 solver.cpp:353] Iteration 900 (5.38031 iter/s, 18.5863s/100 iter), loss = 0.0776402
I0731 19:57:05.260056   939 solver.cpp:375]     Train net output #0: loss = 0.0776401 (* 1 = 0.0776401 loss)
I0731 19:57:05.260061   939 sgd_solver.cpp:136] Iteration 900, lr = 1e-05, m = 0.9
I0731 19:57:21.630036   946 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 19:57:23.920120   939 solver.cpp:353] Iteration 1000 (5.35917 iter/s, 18.6596s/100 iter), loss = 0.0954359
I0731 19:57:23.920143   939 solver.cpp:375]     Train net output #0: loss = 0.0954358 (* 1 = 0.0954358 loss)
I0731 19:57:23.920148   939 sgd_solver.cpp:136] Iteration 1000, lr = 1e-05, m = 0.9
I0731 19:57:42.503890   939 solver.cpp:353] Iteration 1100 (5.38119 iter/s, 18.5833s/100 iter), loss = 0.0649557
I0731 19:57:42.503949   939 solver.cpp:375]     Train net output #0: loss = 0.0649557 (* 1 = 0.0649557 loss)
I0731 19:57:42.503957   939 sgd_solver.cpp:136] Iteration 1100, lr = 1e-05, m = 0.9
I0731 19:57:52.270619   942 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 19:58:00.923698   939 solver.cpp:353] Iteration 1200 (5.42909 iter/s, 18.4193s/100 iter), loss = 0.0829385
I0731 19:58:00.923722   939 solver.cpp:375]     Train net output #0: loss = 0.0829385 (* 1 = 0.0829385 loss)
I0731 19:58:00.923727   939 sgd_solver.cpp:136] Iteration 1200, lr = 1e-05, m = 0.9
I0731 19:58:19.426764   939 solver.cpp:353] Iteration 1300 (5.40466 iter/s, 18.5026s/100 iter), loss = 0.0973721
I0731 19:58:19.426895   939 solver.cpp:375]     Train net output #0: loss = 0.0973721 (* 1 = 0.0973721 loss)
I0731 19:58:19.426903   939 sgd_solver.cpp:136] Iteration 1300, lr = 1e-05, m = 0.9
I0731 19:58:37.950187   939 solver.cpp:353] Iteration 1400 (5.39872 iter/s, 18.5229s/100 iter), loss = 0.117299
I0731 19:58:37.950211   939 solver.cpp:375]     Train net output #0: loss = 0.117299 (* 1 = 0.117299 loss)
I0731 19:58:37.950217   939 sgd_solver.cpp:136] Iteration 1400, lr = 1e-05, m = 0.9
I0731 19:58:53.856534   947 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 19:58:56.796560   939 solver.cpp:353] Iteration 1500 (5.30621 iter/s, 18.8458s/100 iter), loss = 0.0997248
I0731 19:58:56.796584   939 solver.cpp:375]     Train net output #0: loss = 0.0997248 (* 1 = 0.0997248 loss)
I0731 19:58:56.796591   939 sgd_solver.cpp:136] Iteration 1500, lr = 1e-05, m = 0.9
I0731 19:59:15.566650   939 solver.cpp:353] Iteration 1600 (5.32777 iter/s, 18.7696s/100 iter), loss = 0.0740124
I0731 19:59:15.566674   939 solver.cpp:375]     Train net output #0: loss = 0.0740123 (* 1 = 0.0740123 loss)
I0731 19:59:15.566679   939 sgd_solver.cpp:136] Iteration 1600, lr = 1e-05, m = 0.9
I0731 19:59:34.040174   939 solver.cpp:353] Iteration 1700 (5.4133 iter/s, 18.473s/100 iter), loss = 0.101538
I0731 19:59:34.040230   939 solver.cpp:375]     Train net output #0: loss = 0.101538 (* 1 = 0.101538 loss)
I0731 19:59:34.040236   939 sgd_solver.cpp:136] Iteration 1700, lr = 1e-05, m = 0.9
I0731 19:59:52.632609   939 solver.cpp:353] Iteration 1800 (5.37868 iter/s, 18.5919s/100 iter), loss = 0.0664867
I0731 19:59:52.632634   939 solver.cpp:375]     Train net output #0: loss = 0.0664867 (* 1 = 0.0664867 loss)
I0731 19:59:52.632639   939 sgd_solver.cpp:136] Iteration 1800, lr = 1e-05, m = 0.9
I0731 19:59:55.419172   947 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 20:00:11.236517   939 solver.cpp:353] Iteration 1900 (5.37536 iter/s, 18.6034s/100 iter), loss = 0.0580823
I0731 20:00:11.236594   939 solver.cpp:375]     Train net output #0: loss = 0.0580823 (* 1 = 0.0580823 loss)
I0731 20:00:11.236600   939 sgd_solver.cpp:136] Iteration 1900, lr = 1e-05, m = 0.9
I0731 20:00:26.083421   892 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 20:00:29.596447   939 solver.cpp:550] Iteration 2000, Testing net (#0)
I0731 20:00:29.772799   941 blocking_queue.cpp:40] Data layer prefetch queue empty
I0731 20:00:46.653820   939 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.950703
I0731 20:00:46.653937   939 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999653
I0731 20:00:46.653946   939 solver.cpp:635]     Test net output #2: loss = 0.179264 (* 1 = 0.179264 loss)
I0731 20:00:46.653972   939 solver.cpp:305] [MultiGPU] Tests completed in 17.0571s
I0731 20:00:46.861771   939 solver.cpp:353] Iteration 2000 (2.80707 iter/s, 35.6243s/100 iter), loss = 0.070615
I0731 20:00:46.861795   939 solver.cpp:375]     Train net output #0: loss = 0.0706149 (* 1 = 0.0706149 loss)
I0731 20:00:46.861799   939 sgd_solver.cpp:136] Iteration 2000, lr = 1e-05, m = 0.9
I0731 20:01:05.610003   939 solver.cpp:353] Iteration 2100 (5.33398 iter/s, 18.7477s/100 iter), loss = 0.0744172
I0731 20:01:05.610038   939 solver.cpp:375]     Train net output #0: loss = 0.0744171 (* 1 = 0.0744171 loss)
I0731 20:01:05.610043   939 sgd_solver.cpp:136] Iteration 2100, lr = 1e-05, m = 0.9
I0731 20:01:13.919586   894 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 20:01:23.970795   939 solver.cpp:353] Iteration 2200 (5.44654 iter/s, 18.3603s/100 iter), loss = 0.143696
I0731 20:01:23.970846   939 solver.cpp:375]     Train net output #0: loss = 0.143696 (* 1 = 0.143696 loss)
I0731 20:01:23.970854   939 sgd_solver.cpp:136] Iteration 2200, lr = 1e-05, m = 0.9
I0731 20:01:42.425518   939 solver.cpp:353] Iteration 2300 (5.41882 iter/s, 18.4542s/100 iter), loss = 0.0730889
I0731 20:01:42.425547   939 solver.cpp:375]     Train net output #0: loss = 0.0730888 (* 1 = 0.0730888 loss)
I0731 20:01:42.425554   939 sgd_solver.cpp:136] Iteration 2300, lr = 1e-05, m = 0.9
I0731 20:02:00.903017   939 solver.cpp:353] Iteration 2400 (5.41214 iter/s, 18.477s/100 iter), loss = 0.0677369
I0731 20:02:00.914434   939 solver.cpp:375]     Train net output #0: loss = 0.0677368 (* 1 = 0.0677368 loss)
I0731 20:02:00.914484   939 sgd_solver.cpp:136] Iteration 2400, lr = 1e-05, m = 0.9
I0731 20:02:14.999907   947 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 20:02:19.421376   939 solver.cpp:353] Iteration 2500 (5.40019 iter/s, 18.5179s/100 iter), loss = 0.0919619
I0731 20:02:19.421397   939 solver.cpp:375]     Train net output #0: loss = 0.0919618 (* 1 = 0.0919618 loss)
I0731 20:02:19.421401   939 sgd_solver.cpp:136] Iteration 2500, lr = 1e-05, m = 0.9
I0731 20:02:37.976142   939 solver.cpp:353] Iteration 2600 (5.3896 iter/s, 18.5543s/100 iter), loss = 0.0849303
I0731 20:02:37.976192   939 solver.cpp:375]     Train net output #0: loss = 0.0849302 (* 1 = 0.0849302 loss)
I0731 20:02:37.976197   939 sgd_solver.cpp:136] Iteration 2600, lr = 1e-05, m = 0.9
I0731 20:02:45.599664   892 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 20:02:56.503707   939 solver.cpp:353] Iteration 2700 (5.39751 iter/s, 18.5271s/100 iter), loss = 0.0709556
I0731 20:02:56.503731   939 solver.cpp:375]     Train net output #0: loss = 0.0709555 (* 1 = 0.0709555 loss)
I0731 20:02:56.503736   939 sgd_solver.cpp:136] Iteration 2700, lr = 1e-05, m = 0.9
I0731 20:03:15.121366   939 solver.cpp:353] Iteration 2800 (5.37139 iter/s, 18.6171s/100 iter), loss = 0.0639438
I0731 20:03:15.121423   939 solver.cpp:375]     Train net output #0: loss = 0.0639438 (* 1 = 0.0639438 loss)
I0731 20:03:15.121428   939 sgd_solver.cpp:136] Iteration 2800, lr = 1e-05, m = 0.9
I0731 20:03:33.567334   939 solver.cpp:353] Iteration 2900 (5.42139 iter/s, 18.4455s/100 iter), loss = 0.0912324
I0731 20:03:33.567358   939 solver.cpp:375]     Train net output #0: loss = 0.0912323 (* 1 = 0.0912323 loss)
I0731 20:03:33.567363   939 sgd_solver.cpp:136] Iteration 2900, lr = 1e-05, m = 0.9
I0731 20:03:46.884724   947 data_reader.cpp:264] Starting prefetch of epoch 4
I0731 20:03:52.304348   939 solver.cpp:353] Iteration 3000 (5.33718 iter/s, 18.7365s/100 iter), loss = 0.105364
I0731 20:03:52.304371   939 solver.cpp:375]     Train net output #0: loss = 0.105364 (* 1 = 0.105364 loss)
I0731 20:03:52.304376   939 sgd_solver.cpp:136] Iteration 3000, lr = 1e-05, m = 0.9
I0731 20:04:10.861722   939 solver.cpp:353] Iteration 3100 (5.38884 iter/s, 18.5569s/100 iter), loss = 0.0453951
I0731 20:04:10.861747   939 solver.cpp:375]     Train net output #0: loss = 0.0453951 (* 1 = 0.0453951 loss)
I0731 20:04:10.861750   939 sgd_solver.cpp:136] Iteration 3100, lr = 1e-05, m = 0.9
I0731 20:04:29.356855   939 solver.cpp:353] Iteration 3200 (5.40698 iter/s, 18.4946s/100 iter), loss = 0.162707
I0731 20:04:29.356936   939 solver.cpp:375]     Train net output #0: loss = 0.162707 (* 1 = 0.162707 loss)
I0731 20:04:29.356941   939 sgd_solver.cpp:136] Iteration 3200, lr = 1e-05, m = 0.9
I0731 20:04:47.825956   939 solver.cpp:353] Iteration 3300 (5.4146 iter/s, 18.4686s/100 iter), loss = 0.0656438
I0731 20:04:47.825981   939 solver.cpp:375]     Train net output #0: loss = 0.0656438 (* 1 = 0.0656438 loss)
I0731 20:04:47.825985   939 sgd_solver.cpp:136] Iteration 3300, lr = 1e-05, m = 0.9
I0731 20:04:48.220846   894 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 20:05:06.317966   939 solver.cpp:353] Iteration 3400 (5.40789 iter/s, 18.4915s/100 iter), loss = 0.0466187
I0731 20:05:06.318048   939 solver.cpp:375]     Train net output #0: loss = 0.0466186 (* 1 = 0.0466186 loss)
I0731 20:05:06.318054   939 sgd_solver.cpp:136] Iteration 3400, lr = 1e-05, m = 0.9
I0731 20:05:18.655464   942 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 20:05:24.686235   939 solver.cpp:353] Iteration 3500 (5.44432 iter/s, 18.3678s/100 iter), loss = 0.0843866
I0731 20:05:24.686259   939 solver.cpp:375]     Train net output #0: loss = 0.0843865 (* 1 = 0.0843865 loss)
I0731 20:05:24.686264   939 sgd_solver.cpp:136] Iteration 3500, lr = 1e-05, m = 0.9
I0731 20:05:43.515466   939 solver.cpp:353] Iteration 3600 (5.31104 iter/s, 18.8287s/100 iter), loss = 0.0760281
I0731 20:05:43.515545   939 solver.cpp:375]     Train net output #0: loss = 0.076028 (* 1 = 0.076028 loss)
I0731 20:05:43.515550   939 sgd_solver.cpp:136] Iteration 3600, lr = 1e-05, m = 0.9
I0731 20:06:02.265388   939 solver.cpp:353] Iteration 3700 (5.3335 iter/s, 18.7494s/100 iter), loss = 0.0852726
I0731 20:06:02.265410   939 solver.cpp:375]     Train net output #0: loss = 0.0852725 (* 1 = 0.0852725 loss)
I0731 20:06:02.265414   939 sgd_solver.cpp:136] Iteration 3700, lr = 1e-05, m = 0.9
I0731 20:06:20.483669   942 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 20:06:20.812911   939 solver.cpp:353] Iteration 3800 (5.3917 iter/s, 18.547s/100 iter), loss = 0.0879581
I0731 20:06:20.812933   939 solver.cpp:375]     Train net output #0: loss = 0.087958 (* 1 = 0.087958 loss)
I0731 20:06:20.812940   939 sgd_solver.cpp:136] Iteration 3800, lr = 1e-05, m = 0.9
I0731 20:06:39.275703   939 solver.cpp:353] Iteration 3900 (5.41645 iter/s, 18.4623s/100 iter), loss = 0.073381
I0731 20:06:39.275727   939 solver.cpp:375]     Train net output #0: loss = 0.0733809 (* 1 = 0.0733809 loss)
I0731 20:06:39.275732   939 sgd_solver.cpp:136] Iteration 3900, lr = 1e-05, m = 0.9
I0731 20:06:57.611049   939 solver.cpp:550] Iteration 4000, Testing net (#0)
I0731 20:07:01.454674   936 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 20:07:17.577641   934 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 20:07:18.010118   939 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.950099
I0731 20:07:18.010136   939 solver.cpp:635]     Test net output #1: accuracy/top5 = 1
I0731 20:07:18.010143   939 solver.cpp:635]     Test net output #2: loss = 0.159117 (* 1 = 0.159117 loss)
I0731 20:07:18.010174   939 solver.cpp:305] [MultiGPU] Tests completed in 20.3986s
I0731 20:07:18.222491   939 solver.cpp:353] Iteration 4000 (2.56768 iter/s, 38.9457s/100 iter), loss = 0.0602668
I0731 20:07:18.222533   939 solver.cpp:375]     Train net output #0: loss = 0.0602667 (* 1 = 0.0602667 loss)
I0731 20:07:18.222539   939 sgd_solver.cpp:136] Iteration 4000, lr = 1e-05, m = 0.9
I0731 20:07:37.197731   939 solver.cpp:353] Iteration 4100 (5.27017 iter/s, 18.9747s/100 iter), loss = 0.0962674
I0731 20:07:37.197803   939 solver.cpp:375]     Train net output #0: loss = 0.0962673 (* 1 = 0.0962673 loss)
I0731 20:07:37.197809   939 sgd_solver.cpp:136] Iteration 4100, lr = 1e-05, m = 0.9
I0731 20:07:55.800204   939 solver.cpp:353] Iteration 4200 (5.37578 iter/s, 18.602s/100 iter), loss = 0.0778708
I0731 20:07:55.800228   939 solver.cpp:375]     Train net output #0: loss = 0.0778707 (* 1 = 0.0778707 loss)
I0731 20:07:55.800232   939 sgd_solver.cpp:136] Iteration 4200, lr = 1e-05, m = 0.9
I0731 20:08:13.188974   942 data_reader.cpp:264] Starting prefetch of epoch 4
I0731 20:08:14.262632   939 solver.cpp:353] Iteration 4300 (5.41655 iter/s, 18.4619s/100 iter), loss = 0.100781
I0731 20:08:14.262655   939 solver.cpp:375]     Train net output #0: loss = 0.100781 (* 1 = 0.100781 loss)
I0731 20:08:14.262660   939 sgd_solver.cpp:136] Iteration 4300, lr = 1e-05, m = 0.9
I0731 20:08:32.717310   939 solver.cpp:353] Iteration 4400 (5.41883 iter/s, 18.4542s/100 iter), loss = 0.0572364
I0731 20:08:32.717336   939 solver.cpp:375]     Train net output #0: loss = 0.0572363 (* 1 = 0.0572363 loss)
I0731 20:08:32.717341   939 sgd_solver.cpp:136] Iteration 4400, lr = 1e-05, m = 0.9
I0731 20:08:51.187919   939 solver.cpp:353] Iteration 4500 (5.41416 iter/s, 18.4701s/100 iter), loss = 0.0502841
I0731 20:08:51.187973   939 solver.cpp:375]     Train net output #0: loss = 0.050284 (* 1 = 0.050284 loss)
I0731 20:08:51.187978   939 sgd_solver.cpp:136] Iteration 4500, lr = 1e-05, m = 0.9
I0731 20:09:09.735152   939 solver.cpp:353] Iteration 4600 (5.39179 iter/s, 18.5467s/100 iter), loss = 0.113455
I0731 20:09:09.735177   939 solver.cpp:375]     Train net output #0: loss = 0.113455 (* 1 = 0.113455 loss)
I0731 20:09:09.735182   939 sgd_solver.cpp:136] Iteration 4600, lr = 1e-05, m = 0.9
I0731 20:09:14.204813   947 data_reader.cpp:264] Starting prefetch of epoch 5
I0731 20:09:28.158460   939 solver.cpp:353] Iteration 4700 (5.42806 iter/s, 18.4228s/100 iter), loss = 0.0757026
I0731 20:09:28.158540   939 solver.cpp:375]     Train net output #0: loss = 0.0757025 (* 1 = 0.0757025 loss)
I0731 20:09:28.158547   939 sgd_solver.cpp:136] Iteration 4700, lr = 1e-05, m = 0.9
I0731 20:09:44.784705   942 data_reader.cpp:264] Starting prefetch of epoch 5
I0731 20:09:46.580257   939 solver.cpp:353] Iteration 4800 (5.4285 iter/s, 18.4213s/100 iter), loss = 0.0857558
I0731 20:09:46.580279   939 solver.cpp:375]     Train net output #0: loss = 0.0857557 (* 1 = 0.0857557 loss)
I0731 20:09:46.580284   939 sgd_solver.cpp:136] Iteration 4800, lr = 1e-05, m = 0.9
I0731 20:10:05.165266   939 solver.cpp:353] Iteration 4900 (5.38083 iter/s, 18.5845s/100 iter), loss = 0.0899845
I0731 20:10:05.165323   939 solver.cpp:375]     Train net output #0: loss = 0.0899843 (* 1 = 0.0899843 loss)
I0731 20:10:05.165330   939 sgd_solver.cpp:136] Iteration 4900, lr = 1e-05, m = 0.9
I0731 20:10:23.715363   939 solver.cpp:353] Iteration 5000 (5.39096 iter/s, 18.5496s/100 iter), loss = 0.087764
I0731 20:10:23.715385   939 solver.cpp:375]     Train net output #0: loss = 0.0877639 (* 1 = 0.0877639 loss)
I0731 20:10:23.715390   939 sgd_solver.cpp:136] Iteration 5000, lr = 1e-05, m = 0.9
I0731 20:10:42.339678   939 solver.cpp:353] Iteration 5100 (5.36947 iter/s, 18.6238s/100 iter), loss = 0.133236
I0731 20:10:42.339782   939 solver.cpp:375]     Train net output #0: loss = 0.133236 (* 1 = 0.133236 loss)
I0731 20:10:42.339788   939 sgd_solver.cpp:136] Iteration 5100, lr = 1e-05, m = 0.9
I0731 20:10:46.068114   892 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 20:11:00.891099   939 solver.cpp:353] Iteration 5200 (5.39057 iter/s, 18.5509s/100 iter), loss = 0.0919368
I0731 20:11:00.891126   939 solver.cpp:375]     Train net output #0: loss = 0.0919367 (* 1 = 0.0919367 loss)
I0731 20:11:00.891130   939 sgd_solver.cpp:136] Iteration 5200, lr = 1e-05, m = 0.9
I0731 20:11:19.412878   939 solver.cpp:353] Iteration 5300 (5.3992 iter/s, 18.5213s/100 iter), loss = 0.0541902
I0731 20:11:19.412950   939 solver.cpp:375]     Train net output #0: loss = 0.0541901 (* 1 = 0.0541901 loss)
I0731 20:11:19.412955   939 sgd_solver.cpp:136] Iteration 5300, lr = 1e-05, m = 0.9
I0731 20:11:37.959484   939 solver.cpp:353] Iteration 5400 (5.39197 iter/s, 18.5461s/100 iter), loss = 0.0600524
I0731 20:11:37.959517   939 solver.cpp:375]     Train net output #0: loss = 0.0600523 (* 1 = 0.0600523 loss)
I0731 20:11:37.959522   939 sgd_solver.cpp:136] Iteration 5400, lr = 1e-05, m = 0.9
I0731 20:11:47.429708   894 data_reader.cpp:264] Starting prefetch of epoch 4
I0731 20:11:56.482910   939 solver.cpp:353] Iteration 5500 (5.39872 iter/s, 18.5229s/100 iter), loss = 0.0745925
I0731 20:11:56.482960   939 solver.cpp:375]     Train net output #0: loss = 0.0745924 (* 1 = 0.0745924 loss)
I0731 20:11:56.482966   939 sgd_solver.cpp:136] Iteration 5500, lr = 1e-05, m = 0.9
I0731 20:12:14.933490   939 solver.cpp:353] Iteration 5600 (5.42003 iter/s, 18.4501s/100 iter), loss = 0.0400955
I0731 20:12:14.933512   939 solver.cpp:375]     Train net output #0: loss = 0.0400954 (* 1 = 0.0400954 loss)
I0731 20:12:14.933518   939 sgd_solver.cpp:136] Iteration 5600, lr = 1e-05, m = 0.9
I0731 20:12:17.917033   943 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 20:12:33.374260   939 solver.cpp:353] Iteration 5700 (5.42291 iter/s, 18.4403s/100 iter), loss = 0.125437
I0731 20:12:33.374341   939 solver.cpp:375]     Train net output #0: loss = 0.125437 (* 1 = 0.125437 loss)
I0731 20:12:33.374348   939 sgd_solver.cpp:136] Iteration 5700, lr = 1e-05, m = 0.9
I0731 20:12:51.947352   939 solver.cpp:353] Iteration 5800 (5.38428 iter/s, 18.5726s/100 iter), loss = 0.0759582
I0731 20:12:51.947376   939 solver.cpp:375]     Train net output #0: loss = 0.0759582 (* 1 = 0.0759582 loss)
I0731 20:12:51.947381   939 sgd_solver.cpp:136] Iteration 5800, lr = 1e-05, m = 0.9
I0731 20:13:10.675426   939 solver.cpp:353] Iteration 5900 (5.33973 iter/s, 18.7276s/100 iter), loss = 0.080704
I0731 20:13:10.675496   939 solver.cpp:375]     Train net output #0: loss = 0.0807039 (* 1 = 0.0807039 loss)
I0731 20:13:10.675503   939 sgd_solver.cpp:136] Iteration 5900, lr = 1e-05, m = 0.9
I0731 20:13:19.266425   942 data_reader.cpp:264] Starting prefetch of epoch 6
I0731 20:13:28.994153   939 solver.cpp:550] Iteration 6000, Testing net (#0)
I0731 20:13:42.002018   939 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.949194
I0731 20:13:42.002070   939 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999431
I0731 20:13:42.002080   939 solver.cpp:635]     Test net output #2: loss = 0.196036 (* 1 = 0.196036 loss)
I0731 20:13:42.002110   939 solver.cpp:305] [MultiGPU] Tests completed in 13.0076s
I0731 20:13:42.192071   939 solver.cpp:353] Iteration 6000 (3.17301 iter/s, 31.5158s/100 iter), loss = 0.104554
I0731 20:13:42.192097   939 solver.cpp:375]     Train net output #0: loss = 0.104554 (* 1 = 0.104554 loss)
I0731 20:13:42.192101   939 sgd_solver.cpp:136] Iteration 6000, lr = 1e-05, m = 0.9
I0731 20:14:00.649153   939 solver.cpp:353] Iteration 6100 (5.41812 iter/s, 18.4566s/100 iter), loss = 0.0782115
I0731 20:14:00.649178   939 solver.cpp:375]     Train net output #0: loss = 0.0782115 (* 1 = 0.0782115 loss)
I0731 20:14:00.649181   939 sgd_solver.cpp:136] Iteration 6100, lr = 1e-05, m = 0.9
I0731 20:14:02.889444   947 data_reader.cpp:264] Starting prefetch of epoch 6
I0731 20:14:19.263712   939 solver.cpp:353] Iteration 6200 (5.37229 iter/s, 18.614s/100 iter), loss = 0.0862987
I0731 20:14:19.263813   939 solver.cpp:375]     Train net output #0: loss = 0.0862987 (* 1 = 0.0862987 loss)
I0731 20:14:19.263819   939 sgd_solver.cpp:136] Iteration 6200, lr = 1e-05, m = 0.9
I0731 20:14:33.519976   942 data_reader.cpp:264] Starting prefetch of epoch 7
I0731 20:14:37.700903   939 solver.cpp:353] Iteration 6300 (5.42397 iter/s, 18.4367s/100 iter), loss = 0.0810728
I0731 20:14:37.700930   939 solver.cpp:375]     Train net output #0: loss = 0.0810728 (* 1 = 0.0810728 loss)
I0731 20:14:37.700937   939 sgd_solver.cpp:136] Iteration 6300, lr = 1e-05, m = 0.9
I0731 20:14:56.202558   939 solver.cpp:353] Iteration 6400 (5.40507 iter/s, 18.5011s/100 iter), loss = 0.0908732
I0731 20:14:56.202623   939 solver.cpp:375]     Train net output #0: loss = 0.0908731 (* 1 = 0.0908731 loss)
I0731 20:14:56.202630   939 sgd_solver.cpp:136] Iteration 6400, lr = 1e-05, m = 0.9
I0731 20:15:14.788594   939 solver.cpp:353] Iteration 6500 (5.38053 iter/s, 18.5855s/100 iter), loss = 0.0888236
I0731 20:15:14.788619   939 solver.cpp:375]     Train net output #0: loss = 0.0888236 (* 1 = 0.0888236 loss)
I0731 20:15:14.788622   939 sgd_solver.cpp:136] Iteration 6500, lr = 1e-05, m = 0.9
I0731 20:15:33.447649   939 solver.cpp:353] Iteration 6600 (5.35948 iter/s, 18.6585s/100 iter), loss = 0.0854795
I0731 20:15:33.447724   939 solver.cpp:375]     Train net output #0: loss = 0.0854794 (* 1 = 0.0854794 loss)
I0731 20:15:33.447729   939 sgd_solver.cpp:136] Iteration 6600, lr = 1e-05, m = 0.9
I0731 20:15:34.940228   943 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 20:15:51.948253   939 solver.cpp:353] Iteration 6700 (5.40538 iter/s, 18.5001s/100 iter), loss = 0.0935336
I0731 20:15:51.948276   939 solver.cpp:375]     Train net output #0: loss = 0.0935336 (* 1 = 0.0935336 loss)
I0731 20:15:51.948279   939 sgd_solver.cpp:136] Iteration 6700, lr = 1e-05, m = 0.9
I0731 20:16:10.536573   939 solver.cpp:353] Iteration 6800 (5.37987 iter/s, 18.5878s/100 iter), loss = 0.0498113
I0731 20:16:10.536628   939 solver.cpp:375]     Train net output #0: loss = 0.0498113 (* 1 = 0.0498113 loss)
I0731 20:16:10.536633   939 sgd_solver.cpp:136] Iteration 6800, lr = 1e-05, m = 0.9
I0731 20:16:29.048372   939 solver.cpp:353] Iteration 6900 (5.40211 iter/s, 18.5113s/100 iter), loss = 0.100908
I0731 20:16:29.048393   939 solver.cpp:375]     Train net output #0: loss = 0.100908 (* 1 = 0.100908 loss)
I0731 20:16:29.048398   939 sgd_solver.cpp:136] Iteration 6900, lr = 1e-05, m = 0.9
I0731 20:16:36.121834   894 data_reader.cpp:264] Starting prefetch of epoch 5
I0731 20:16:47.675128   939 solver.cpp:353] Iteration 7000 (5.36877 iter/s, 18.6262s/100 iter), loss = 0.105827
I0731 20:16:47.675184   939 solver.cpp:375]     Train net output #0: loss = 0.105827 (* 1 = 0.105827 loss)
I0731 20:16:47.675191   939 sgd_solver.cpp:136] Iteration 7000, lr = 1e-05, m = 0.9
I0731 20:17:06.195106   939 solver.cpp:353] Iteration 7100 (5.39972 iter/s, 18.5195s/100 iter), loss = 0.0794259
I0731 20:17:06.195133   939 solver.cpp:375]     Train net output #0: loss = 0.0794259 (* 1 = 0.0794259 loss)
I0731 20:17:06.195139   939 sgd_solver.cpp:136] Iteration 7100, lr = 1e-05, m = 0.9
I0731 20:17:06.798491   943 data_reader.cpp:264] Starting prefetch of epoch 4
I0731 20:17:24.712141   939 solver.cpp:353] Iteration 7200 (5.40058 iter/s, 18.5165s/100 iter), loss = 0.0679529
I0731 20:17:24.712244   939 solver.cpp:375]     Train net output #0: loss = 0.0679528 (* 1 = 0.0679528 loss)
I0731 20:17:24.712252   939 sgd_solver.cpp:136] Iteration 7200, lr = 1e-05, m = 0.9
I0731 20:17:43.216641   939 solver.cpp:353] Iteration 7300 (5.40424 iter/s, 18.504s/100 iter), loss = 0.0660755
I0731 20:17:43.216665   939 solver.cpp:375]     Train net output #0: loss = 0.0660754 (* 1 = 0.0660754 loss)
I0731 20:17:43.216668   939 sgd_solver.cpp:136] Iteration 7300, lr = 1e-05, m = 0.9
I0731 20:18:01.707326   939 solver.cpp:353] Iteration 7400 (5.40828 iter/s, 18.4902s/100 iter), loss = 0.110046
I0731 20:18:01.707406   939 solver.cpp:375]     Train net output #0: loss = 0.110046 (* 1 = 0.110046 loss)
I0731 20:18:01.707412   939 sgd_solver.cpp:136] Iteration 7400, lr = 1e-05, m = 0.9
I0731 20:18:08.054198   942 data_reader.cpp:264] Starting prefetch of epoch 8
I0731 20:18:20.241487   939 solver.cpp:353] Iteration 7500 (5.39559 iter/s, 18.5336s/100 iter), loss = 0.104816
I0731 20:18:20.241510   939 solver.cpp:375]     Train net output #0: loss = 0.104816 (* 1 = 0.104816 loss)
I0731 20:18:20.241515   939 sgd_solver.cpp:136] Iteration 7500, lr = 1e-05, m = 0.9
I0731 20:18:38.722410   939 solver.cpp:353] Iteration 7600 (5.41113 iter/s, 18.4804s/100 iter), loss = 0.0641242
I0731 20:18:38.722473   939 solver.cpp:375]     Train net output #0: loss = 0.0641241 (* 1 = 0.0641241 loss)
I0731 20:18:38.722478   939 sgd_solver.cpp:136] Iteration 7600, lr = 1e-05, m = 0.9
I0731 20:18:57.205937   939 solver.cpp:353] Iteration 7700 (5.41037 iter/s, 18.483s/100 iter), loss = 0.0723037
I0731 20:18:57.205960   939 solver.cpp:375]     Train net output #0: loss = 0.0723037 (* 1 = 0.0723037 loss)
I0731 20:18:57.205963   939 sgd_solver.cpp:136] Iteration 7700, lr = 1e-05, m = 0.9
I0731 20:19:09.222554   894 data_reader.cpp:264] Starting prefetch of epoch 6
I0731 20:19:15.661316   939 solver.cpp:353] Iteration 7800 (5.41862 iter/s, 18.4549s/100 iter), loss = 0.0975931
I0731 20:19:15.661342   939 solver.cpp:375]     Train net output #0: loss = 0.0975931 (* 1 = 0.0975931 loss)
I0731 20:19:15.661346   939 sgd_solver.cpp:136] Iteration 7800, lr = 1e-05, m = 0.9
I0731 20:19:34.246060   939 solver.cpp:353] Iteration 7900 (5.3809 iter/s, 18.5842s/100 iter), loss = 0.056816
I0731 20:19:34.246083   939 solver.cpp:375]     Train net output #0: loss = 0.056816 (* 1 = 0.056816 loss)
I0731 20:19:34.246086   939 sgd_solver.cpp:136] Iteration 7900, lr = 1e-05, m = 0.9
I0731 20:19:39.832788   892 data_reader.cpp:264] Starting prefetch of epoch 4
I0731 20:19:52.522413   939 solver.cpp:550] Iteration 8000, Testing net (#0)
I0731 20:20:03.785984   968 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 20:20:04.253296   939 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.951444
I0731 20:20:04.253343   939 solver.cpp:635]     Test net output #1: accuracy/top5 = 1
I0731 20:20:04.253361   939 solver.cpp:635]     Test net output #2: loss = 0.154188 (* 1 = 0.154188 loss)
I0731 20:20:04.253410   939 solver.cpp:305] [MultiGPU] Tests completed in 11.7307s
I0731 20:20:04.479298   939 solver.cpp:353] Iteration 8000 (3.30771 iter/s, 30.2324s/100 iter), loss = 0.101574
I0731 20:20:04.479349   939 solver.cpp:375]     Train net output #0: loss = 0.101574 (* 1 = 0.101574 loss)
I0731 20:20:04.479360   939 sgd_solver.cpp:136] Iteration 8000, lr = 1e-05, m = 0.9
I0731 20:20:23.634474   939 solver.cpp:353] Iteration 8100 (5.22066 iter/s, 19.1546s/100 iter), loss = 0.0697335
I0731 20:20:23.634526   939 solver.cpp:375]     Train net output #0: loss = 0.0697334 (* 1 = 0.0697334 loss)
I0731 20:20:23.634531   939 sgd_solver.cpp:136] Iteration 8100, lr = 1e-05, m = 0.9
I0731 20:20:42.084396   939 solver.cpp:353] Iteration 8200 (5.42023 iter/s, 18.4494s/100 iter), loss = 0.0632941
I0731 20:20:42.084419   939 solver.cpp:375]     Train net output #0: loss = 0.063294 (* 1 = 0.063294 loss)
I0731 20:20:42.084424   939 sgd_solver.cpp:136] Iteration 8200, lr = 1e-05, m = 0.9
I0731 20:20:53.156318   894 data_reader.cpp:264] Starting prefetch of epoch 7
I0731 20:21:00.578996   939 solver.cpp:353] Iteration 8300 (5.40713 iter/s, 18.4941s/100 iter), loss = 0.135063
I0731 20:21:00.579049   939 solver.cpp:375]     Train net output #0: loss = 0.135063 (* 1 = 0.135063 loss)
I0731 20:21:00.579054   939 sgd_solver.cpp:136] Iteration 8300, lr = 1e-05, m = 0.9
I0731 20:21:19.183478   939 solver.cpp:353] Iteration 8400 (5.3752 iter/s, 18.604s/100 iter), loss = 0.0690997
I0731 20:21:19.183502   939 solver.cpp:375]     Train net output #0: loss = 0.0690996 (* 1 = 0.0690996 loss)
I0731 20:21:19.183506   939 sgd_solver.cpp:136] Iteration 8400, lr = 1e-05, m = 0.9
I0731 20:21:24.070121   942 data_reader.cpp:264] Starting prefetch of epoch 9
I0731 20:21:37.676601   939 solver.cpp:353] Iteration 8500 (5.40756 iter/s, 18.4926s/100 iter), loss = 0.0575547
I0731 20:21:37.676651   939 solver.cpp:375]     Train net output #0: loss = 0.0575546 (* 1 = 0.0575546 loss)
I0731 20:21:37.676657   939 sgd_solver.cpp:136] Iteration 8500, lr = 1e-05, m = 0.9
I0731 20:21:56.312517   939 solver.cpp:353] Iteration 8600 (5.36613 iter/s, 18.6354s/100 iter), loss = 0.110705
I0731 20:21:56.312546   939 solver.cpp:375]     Train net output #0: loss = 0.110705 (* 1 = 0.110705 loss)
I0731 20:21:56.312553   939 sgd_solver.cpp:136] Iteration 8600, lr = 1e-05, m = 0.9
I0731 20:22:14.772946   939 solver.cpp:353] Iteration 8700 (5.41714 iter/s, 18.4599s/100 iter), loss = 0.077508
I0731 20:22:14.773025   939 solver.cpp:375]     Train net output #0: loss = 0.077508 (* 1 = 0.077508 loss)
I0731 20:22:14.773030   939 sgd_solver.cpp:136] Iteration 8700, lr = 1e-05, m = 0.9
I0731 20:22:25.286682   943 data_reader.cpp:264] Starting prefetch of epoch 5
I0731 20:22:33.379261   939 solver.cpp:353] Iteration 8800 (5.37467 iter/s, 18.6058s/100 iter), loss = 0.0759625
I0731 20:22:33.379282   939 solver.cpp:375]     Train net output #0: loss = 0.0759625 (* 1 = 0.0759625 loss)
I0731 20:22:33.379287   939 sgd_solver.cpp:136] Iteration 8800, lr = 1e-05, m = 0.9
I0731 20:22:52.004930   939 solver.cpp:353] Iteration 8900 (5.36908 iter/s, 18.6252s/100 iter), loss = 0.0751425
I0731 20:22:52.004981   939 solver.cpp:375]     Train net output #0: loss = 0.0751425 (* 1 = 0.0751425 loss)
I0731 20:22:52.004987   939 sgd_solver.cpp:136] Iteration 8900, lr = 1e-05, m = 0.9
I0731 20:23:10.669811   939 solver.cpp:353] Iteration 9000 (5.3578 iter/s, 18.6644s/100 iter), loss = 0.0667758
I0731 20:23:10.669837   939 solver.cpp:375]     Train net output #0: loss = 0.0667757 (* 1 = 0.0667757 loss)
I0731 20:23:10.669842   939 sgd_solver.cpp:136] Iteration 9000, lr = 1e-05, m = 0.9
I0731 20:23:26.746210   946 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 20:23:29.140506   939 solver.cpp:353] Iteration 9100 (5.41413 iter/s, 18.4702s/100 iter), loss = 0.086671
I0731 20:23:29.140530   939 solver.cpp:375]     Train net output #0: loss = 0.0866709 (* 1 = 0.0866709 loss)
I0731 20:23:29.140534   939 sgd_solver.cpp:136] Iteration 9100, lr = 1e-05, m = 0.9
I0731 20:23:47.650341   939 solver.cpp:353] Iteration 9200 (5.40268 iter/s, 18.5093s/100 iter), loss = 0.0697889
I0731 20:23:47.650365   939 solver.cpp:375]     Train net output #0: loss = 0.0697889 (* 1 = 0.0697889 loss)
I0731 20:23:47.650370   939 sgd_solver.cpp:136] Iteration 9200, lr = 1e-05, m = 0.9
I0731 20:23:57.240258   942 data_reader.cpp:264] Starting prefetch of epoch 10
I0731 20:24:06.115327   939 solver.cpp:353] Iteration 9300 (5.4158 iter/s, 18.4645s/100 iter), loss = 0.0786486
I0731 20:24:06.115352   939 solver.cpp:375]     Train net output #0: loss = 0.0786485 (* 1 = 0.0786485 loss)
I0731 20:24:06.115356   939 sgd_solver.cpp:136] Iteration 9300, lr = 1e-05, m = 0.9
I0731 20:24:24.633124   939 solver.cpp:353] Iteration 9400 (5.40036 iter/s, 18.5173s/100 iter), loss = 0.102437
I0731 20:24:24.633147   939 solver.cpp:375]     Train net output #0: loss = 0.102437 (* 1 = 0.102437 loss)
I0731 20:24:24.633152   939 sgd_solver.cpp:136] Iteration 9400, lr = 1e-05, m = 0.9
I0731 20:24:43.233778   939 solver.cpp:353] Iteration 9500 (5.3763 iter/s, 18.6001s/100 iter), loss = 0.0656029
I0731 20:24:43.233834   939 solver.cpp:375]     Train net output #0: loss = 0.0656028 (* 1 = 0.0656028 loss)
I0731 20:24:43.233839   939 sgd_solver.cpp:136] Iteration 9500, lr = 1e-05, m = 0.9
I0731 20:24:58.675091   894 data_reader.cpp:264] Starting prefetch of epoch 8
I0731 20:25:01.840579   939 solver.cpp:353] Iteration 9600 (5.37453 iter/s, 18.6063s/100 iter), loss = 0.0687463
I0731 20:25:01.840600   939 solver.cpp:375]     Train net output #0: loss = 0.0687463 (* 1 = 0.0687463 loss)
I0731 20:25:01.840605   939 sgd_solver.cpp:136] Iteration 9600, lr = 1e-05, m = 0.9
I0731 20:25:20.366802   939 solver.cpp:353] Iteration 9700 (5.3979 iter/s, 18.5257s/100 iter), loss = 0.0698831
I0731 20:25:20.366858   939 solver.cpp:375]     Train net output #0: loss = 0.0698831 (* 1 = 0.0698831 loss)
I0731 20:25:20.366863   939 sgd_solver.cpp:136] Iteration 9700, lr = 1e-05, m = 0.9
I0731 20:25:38.909775   939 solver.cpp:353] Iteration 9800 (5.39303 iter/s, 18.5425s/100 iter), loss = 0.0663345
I0731 20:25:38.909797   939 solver.cpp:375]     Train net output #0: loss = 0.0663344 (* 1 = 0.0663344 loss)
I0731 20:25:38.909802   939 sgd_solver.cpp:136] Iteration 9800, lr = 1e-05, m = 0.9
I0731 20:25:57.401998   939 solver.cpp:353] Iteration 9900 (5.40783 iter/s, 18.4917s/100 iter), loss = 0.0775769
I0731 20:25:57.402070   939 solver.cpp:375]     Train net output #0: loss = 0.0775769 (* 1 = 0.0775769 loss)
I0731 20:25:57.402074   939 sgd_solver.cpp:136] Iteration 9900, lr = 1e-05, m = 0.9
I0731 20:25:59.809309   947 data_reader.cpp:264] Starting prefetch of epoch 7
I0731 20:26:15.696808   939 solver.cpp:680] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/l1reg/cityscapes5_jsegnet21v2_iter_10000.caffemodel
I0731 20:26:15.813272   939 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/l1reg/cityscapes5_jsegnet21v2_iter_10000.solverstate
I0731 20:26:15.823809   939 solver.cpp:550] Iteration 10000, Testing net (#0)
I0731 20:26:23.907428   964 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 20:26:29.066617   939 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.950826
I0731 20:26:29.067124   939 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999442
I0731 20:26:29.067134   939 solver.cpp:635]     Test net output #2: loss = 0.191844 (* 1 = 0.191844 loss)
I0731 20:26:29.067159   939 solver.cpp:305] [MultiGPU] Tests completed in 13.243s
I0731 20:26:29.275454   939 solver.cpp:353] Iteration 10000 (3.13749 iter/s, 31.8726s/100 iter), loss = 0.0737782
I0731 20:26:29.275481   939 solver.cpp:375]     Train net output #0: loss = 0.0737782 (* 1 = 0.0737782 loss)
I0731 20:26:29.275488   939 sgd_solver.cpp:136] Iteration 10000, lr = 1e-05, m = 0.9
I0731 20:26:48.895581   939 solver.cpp:353] Iteration 10100 (5.09695 iter/s, 19.6196s/100 iter), loss = 0.0733768
I0731 20:26:48.895627   939 solver.cpp:375]     Train net output #0: loss = 0.0733768 (* 1 = 0.0733768 loss)
I0731 20:26:48.895634   939 sgd_solver.cpp:136] Iteration 10100, lr = 1e-05, m = 0.9
I0731 20:27:08.037338   939 solver.cpp:353] Iteration 10200 (5.22432 iter/s, 19.1412s/100 iter), loss = 0.0894128
I0731 20:27:08.037395   939 solver.cpp:375]     Train net output #0: loss = 0.0894128 (* 1 = 0.0894128 loss)
I0731 20:27:08.037400   939 sgd_solver.cpp:136] Iteration 10200, lr = 1e-05, m = 0.9
I0731 20:27:16.135689   943 data_reader.cpp:264] Starting prefetch of epoch 6
I0731 20:27:26.419374   939 solver.cpp:353] Iteration 10300 (5.44024 iter/s, 18.3815s/100 iter), loss = 0.0611011
I0731 20:27:26.419404   939 solver.cpp:375]     Train net output #0: loss = 0.0611011 (* 1 = 0.0611011 loss)
I0731 20:27:26.419409   939 sgd_solver.cpp:136] Iteration 10300, lr = 1e-05, m = 0.9
I0731 20:27:44.832520   939 solver.cpp:353] Iteration 10400 (5.43105 iter/s, 18.4126s/100 iter), loss = 0.0542672
I0731 20:27:44.832561   939 solver.cpp:375]     Train net output #0: loss = 0.0542671 (* 1 = 0.0542671 loss)
I0731 20:27:44.832566   939 sgd_solver.cpp:136] Iteration 10400, lr = 1e-05, m = 0.9
I0731 20:28:03.323395   939 solver.cpp:353] Iteration 10500 (5.40822 iter/s, 18.4904s/100 iter), loss = 0.0591627
I0731 20:28:03.323423   939 solver.cpp:375]     Train net output #0: loss = 0.0591627 (* 1 = 0.0591627 loss)
I0731 20:28:03.323431   939 sgd_solver.cpp:136] Iteration 10500, lr = 1e-05, m = 0.9
I0731 20:28:17.060617   946 data_reader.cpp:264] Starting prefetch of epoch 4
I0731 20:28:21.916247   939 solver.cpp:353] Iteration 10600 (5.37856 iter/s, 18.5923s/100 iter), loss = 0.0853715
I0731 20:28:21.916270   939 solver.cpp:375]     Train net output #0: loss = 0.0853715 (* 1 = 0.0853715 loss)
I0731 20:28:21.916273   939 sgd_solver.cpp:136] Iteration 10600, lr = 1e-05, m = 0.9
I0731 20:28:40.443987   939 solver.cpp:353] Iteration 10700 (5.39746 iter/s, 18.5272s/100 iter), loss = 0.0453151
I0731 20:28:40.444010   939 solver.cpp:375]     Train net output #0: loss = 0.0453151 (* 1 = 0.0453151 loss)
I0731 20:28:40.444015   939 sgd_solver.cpp:136] Iteration 10700, lr = 1e-05, m = 0.9
I0731 20:28:47.846539   943 data_reader.cpp:264] Starting prefetch of epoch 7
I0731 20:28:58.862195   939 solver.cpp:353] Iteration 10800 (5.42956 iter/s, 18.4177s/100 iter), loss = 0.0818395
I0731 20:28:58.862217   939 solver.cpp:375]     Train net output #0: loss = 0.0818394 (* 1 = 0.0818394 loss)
I0731 20:28:58.862222   939 sgd_solver.cpp:136] Iteration 10800, lr = 1e-05, m = 0.9
I0731 20:29:17.341419   939 solver.cpp:353] Iteration 10900 (5.41163 iter/s, 18.4787s/100 iter), loss = 0.0490002
I0731 20:29:17.341442   939 solver.cpp:375]     Train net output #0: loss = 0.0490001 (* 1 = 0.0490001 loss)
I0731 20:29:17.341447   939 sgd_solver.cpp:136] Iteration 10900, lr = 1e-05, m = 0.9
I0731 20:29:35.814685   939 solver.cpp:353] Iteration 11000 (5.41338 iter/s, 18.4728s/100 iter), loss = 0.0660167
I0731 20:29:35.814743   939 solver.cpp:375]     Train net output #0: loss = 0.0660167 (* 1 = 0.0660167 loss)
I0731 20:29:35.814749   939 sgd_solver.cpp:136] Iteration 11000, lr = 1e-05, m = 0.9
I0731 20:29:48.860930   942 data_reader.cpp:264] Starting prefetch of epoch 11
I0731 20:29:54.318528   939 solver.cpp:353] Iteration 11100 (5.40443 iter/s, 18.5033s/100 iter), loss = 0.0813252
I0731 20:29:54.318552   939 solver.cpp:375]     Train net output #0: loss = 0.0813252 (* 1 = 0.0813252 loss)
I0731 20:29:54.318557   939 sgd_solver.cpp:136] Iteration 11100, lr = 1e-05, m = 0.9
I0731 20:30:12.852248   939 solver.cpp:353] Iteration 11200 (5.39572 iter/s, 18.5332s/100 iter), loss = 0.0831627
I0731 20:30:12.852304   939 solver.cpp:375]     Train net output #0: loss = 0.0831627 (* 1 = 0.0831627 loss)
I0731 20:30:12.852311   939 sgd_solver.cpp:136] Iteration 11200, lr = 1e-05, m = 0.9
I0731 20:30:31.360620   939 solver.cpp:353] Iteration 11300 (5.40311 iter/s, 18.5079s/100 iter), loss = 0.0618568
I0731 20:30:31.360643   939 solver.cpp:375]     Train net output #0: loss = 0.0618568 (* 1 = 0.0618568 loss)
I0731 20:30:31.360647   939 sgd_solver.cpp:136] Iteration 11300, lr = 1e-05, m = 0.9
I0731 20:30:49.963982   939 solver.cpp:353] Iteration 11400 (5.37552 iter/s, 18.6029s/100 iter), loss = 0.0708982
I0731 20:30:49.964035   939 solver.cpp:375]     Train net output #0: loss = 0.0708982 (* 1 = 0.0708982 loss)
I0731 20:30:49.964040   939 sgd_solver.cpp:136] Iteration 11400, lr = 1e-05, m = 0.9
I0731 20:30:50.160792   946 data_reader.cpp:264] Starting prefetch of epoch 5
I0731 20:31:08.436131   939 solver.cpp:353] Iteration 11500 (5.4137 iter/s, 18.4716s/100 iter), loss = 0.0850143
I0731 20:31:08.436153   939 solver.cpp:375]     Train net output #0: loss = 0.0850143 (* 1 = 0.0850143 loss)
I0731 20:31:08.436158   939 sgd_solver.cpp:136] Iteration 11500, lr = 1e-05, m = 0.9
I0731 20:31:20.769757   943 data_reader.cpp:264] Starting prefetch of epoch 8
I0731 20:31:27.042646   939 solver.cpp:353] Iteration 11600 (5.37461 iter/s, 18.606s/100 iter), loss = 0.111212
I0731 20:31:27.042670   939 solver.cpp:375]     Train net output #0: loss = 0.111212 (* 1 = 0.111212 loss)
I0731 20:31:27.042673   939 sgd_solver.cpp:136] Iteration 11600, lr = 1e-05, m = 0.9
I0731 20:31:45.735303   939 solver.cpp:353] Iteration 11700 (5.34984 iter/s, 18.6921s/100 iter), loss = 0.0431036
I0731 20:31:45.735333   939 solver.cpp:375]     Train net output #0: loss = 0.0431036 (* 1 = 0.0431036 loss)
I0731 20:31:45.735340   939 sgd_solver.cpp:136] Iteration 11700, lr = 1e-05, m = 0.9
I0731 20:32:04.242347   939 solver.cpp:353] Iteration 11800 (5.4035 iter/s, 18.5065s/100 iter), loss = 0.0665061
I0731 20:32:04.242413   939 solver.cpp:375]     Train net output #0: loss = 0.0665061 (* 1 = 0.0665061 loss)
I0731 20:32:04.242420   939 sgd_solver.cpp:136] Iteration 11800, lr = 1e-05, m = 0.9
I0731 20:32:22.039140   943 data_reader.cpp:264] Starting prefetch of epoch 9
I0731 20:32:22.745910   939 solver.cpp:353] Iteration 11900 (5.40451 iter/s, 18.5031s/100 iter), loss = 0.0672774
I0731 20:32:22.745939   939 solver.cpp:375]     Train net output #0: loss = 0.0672773 (* 1 = 0.0672773 loss)
I0731 20:32:22.745944   939 sgd_solver.cpp:136] Iteration 11900, lr = 1e-05, m = 0.9
I0731 20:32:41.004896   939 solver.cpp:550] Iteration 12000, Testing net (#0)
I0731 20:32:51.620846   934 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 20:32:52.335296   939 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.950892
I0731 20:32:52.335321   939 solver.cpp:635]     Test net output #1: accuracy/top5 = 1
I0731 20:32:52.335326   939 solver.cpp:635]     Test net output #2: loss = 0.155945 (* 1 = 0.155945 loss)
I0731 20:32:52.335405   939 solver.cpp:305] [MultiGPU] Tests completed in 11.3302s
I0731 20:32:52.545044   939 solver.cpp:353] Iteration 12000 (3.35589 iter/s, 29.7983s/100 iter), loss = 0.0421853
I0731 20:32:52.545071   939 solver.cpp:375]     Train net output #0: loss = 0.0421853 (* 1 = 0.0421853 loss)
I0731 20:32:52.545078   939 sgd_solver.cpp:136] Iteration 12000, lr = 1e-05, m = 0.9
I0731 20:33:04.062234   942 data_reader.cpp:264] Starting prefetch of epoch 12
I0731 20:33:10.969849   939 solver.cpp:353] Iteration 12100 (5.42761 iter/s, 18.4243s/100 iter), loss = 0.0975126
I0731 20:33:10.969873   939 solver.cpp:375]     Train net output #0: loss = 0.0975126 (* 1 = 0.0975126 loss)
I0731 20:33:10.969877   939 sgd_solver.cpp:136] Iteration 12100, lr = 1e-05, m = 0.9
I0731 20:33:29.420258   939 solver.cpp:353] Iteration 12200 (5.42008 iter/s, 18.4499s/100 iter), loss = 0.062694
I0731 20:33:29.420339   939 solver.cpp:375]     Train net output #0: loss = 0.062694 (* 1 = 0.062694 loss)
I0731 20:33:29.420344   939 sgd_solver.cpp:136] Iteration 12200, lr = 1e-05, m = 0.9
I0731 20:33:47.866955   939 solver.cpp:353] Iteration 12300 (5.42117 iter/s, 18.4462s/100 iter), loss = 0.055874
I0731 20:33:47.866981   939 solver.cpp:375]     Train net output #0: loss = 0.055874 (* 1 = 0.055874 loss)
I0731 20:33:47.866984   939 sgd_solver.cpp:136] Iteration 12300, lr = 1e-05, m = 0.9
I0731 20:34:05.065181   946 data_reader.cpp:264] Starting prefetch of epoch 6
I0731 20:34:06.514569   939 solver.cpp:353] Iteration 12400 (5.36276 iter/s, 18.6471s/100 iter), loss = 0.0554459
I0731 20:34:06.514592   939 solver.cpp:375]     Train net output #0: loss = 0.0554459 (* 1 = 0.0554459 loss)
I0731 20:34:06.514597   939 sgd_solver.cpp:136] Iteration 12400, lr = 1e-05, m = 0.9
I0731 20:34:25.117856   939 solver.cpp:353] Iteration 12500 (5.37554 iter/s, 18.6028s/100 iter), loss = 0.0760807
I0731 20:34:25.117882   939 solver.cpp:375]     Train net output #0: loss = 0.0760807 (* 1 = 0.0760807 loss)
I0731 20:34:25.117885   939 sgd_solver.cpp:136] Iteration 12500, lr = 1e-05, m = 0.9
I0731 20:34:43.727246   939 solver.cpp:353] Iteration 12600 (5.37378 iter/s, 18.6089s/100 iter), loss = 0.0486876
I0731 20:34:43.727299   939 solver.cpp:375]     Train net output #0: loss = 0.0486875 (* 1 = 0.0486875 loss)
I0731 20:34:43.727304   939 sgd_solver.cpp:136] Iteration 12600, lr = 1e-05, m = 0.9
I0731 20:35:02.533051   939 solver.cpp:353] Iteration 12700 (5.31765 iter/s, 18.8053s/100 iter), loss = 0.118348
I0731 20:35:02.533078   939 solver.cpp:375]     Train net output #0: loss = 0.118348 (* 1 = 0.118348 loss)
I0731 20:35:02.533085   939 sgd_solver.cpp:136] Iteration 12700, lr = 1e-05, m = 0.9
I0731 20:35:06.848325   894 data_reader.cpp:264] Starting prefetch of epoch 9
I0731 20:35:21.010911   939 solver.cpp:353] Iteration 12800 (5.41203 iter/s, 18.4774s/100 iter), loss = 0.0518142
I0731 20:35:21.011021   939 solver.cpp:375]     Train net output #0: loss = 0.0518142 (* 1 = 0.0518142 loss)
I0731 20:35:21.011029   939 sgd_solver.cpp:136] Iteration 12800, lr = 1e-05, m = 0.9
I0731 20:35:37.404165   892 data_reader.cpp:264] Starting prefetch of epoch 5
I0731 20:35:39.564046   939 solver.cpp:353] Iteration 12900 (5.39007 iter/s, 18.5526s/100 iter), loss = 0.080278
I0731 20:35:39.564070   939 solver.cpp:375]     Train net output #0: loss = 0.0802779 (* 1 = 0.0802779 loss)
I0731 20:35:39.564074   939 sgd_solver.cpp:136] Iteration 12900, lr = 1e-05, m = 0.9
I0731 20:35:58.186383   939 solver.cpp:353] Iteration 13000 (5.37004 iter/s, 18.6218s/100 iter), loss = 0.0585534
I0731 20:35:58.186439   939 solver.cpp:375]     Train net output #0: loss = 0.0585534 (* 1 = 0.0585534 loss)
I0731 20:35:58.186445   939 sgd_solver.cpp:136] Iteration 13000, lr = 1e-05, m = 0.9
I0731 20:36:16.702384   939 solver.cpp:353] Iteration 13100 (5.40088 iter/s, 18.5155s/100 iter), loss = 0.0921866
I0731 20:36:16.702409   939 solver.cpp:375]     Train net output #0: loss = 0.0921866 (* 1 = 0.0921866 loss)
I0731 20:36:16.702412   939 sgd_solver.cpp:136] Iteration 13100, lr = 1e-05, m = 0.9
I0731 20:36:35.172974   939 solver.cpp:353] Iteration 13200 (5.41416 iter/s, 18.4701s/100 iter), loss = 0.0562943
I0731 20:36:35.173049   939 solver.cpp:375]     Train net output #0: loss = 0.0562943 (* 1 = 0.0562943 loss)
I0731 20:36:35.173055   939 sgd_solver.cpp:136] Iteration 13200, lr = 1e-05, m = 0.9
I0731 20:36:38.740443   892 data_reader.cpp:264] Starting prefetch of epoch 6
I0731 20:36:53.725924   939 solver.cpp:353] Iteration 13300 (5.39013 iter/s, 18.5524s/100 iter), loss = 0.0706098
I0731 20:36:53.725949   939 solver.cpp:375]     Train net output #0: loss = 0.0706097 (* 1 = 0.0706097 loss)
I0731 20:36:53.725952   939 sgd_solver.cpp:136] Iteration 13300, lr = 1e-05, m = 0.9
I0731 20:37:12.324774   939 solver.cpp:353] Iteration 13400 (5.37682 iter/s, 18.5983s/100 iter), loss = 0.0531397
I0731 20:37:12.324834   939 solver.cpp:375]     Train net output #0: loss = 0.0531397 (* 1 = 0.0531397 loss)
I0731 20:37:12.324841   939 sgd_solver.cpp:136] Iteration 13400, lr = 1e-05, m = 0.9
I0731 20:37:30.853324   939 solver.cpp:353] Iteration 13500 (5.39722 iter/s, 18.528s/100 iter), loss = 0.077297
I0731 20:37:30.853353   939 solver.cpp:375]     Train net output #0: loss = 0.0772969 (* 1 = 0.0772969 loss)
I0731 20:37:30.853359   939 sgd_solver.cpp:136] Iteration 13500, lr = 1e-05, m = 0.9
I0731 20:37:39.922035   946 data_reader.cpp:264] Starting prefetch of epoch 7
I0731 20:37:49.421243   939 solver.cpp:353] Iteration 13600 (5.38578 iter/s, 18.5674s/100 iter), loss = 0.0852952
I0731 20:37:49.422781   939 solver.cpp:375]     Train net output #0: loss = 0.0852952 (* 1 = 0.0852952 loss)
I0731 20:37:49.422816   939 sgd_solver.cpp:136] Iteration 13600, lr = 1e-05, m = 0.9
I0731 20:38:07.858697   939 solver.cpp:353] Iteration 13700 (5.42389 iter/s, 18.4369s/100 iter), loss = 0.0586072
I0731 20:38:07.858723   939 solver.cpp:375]     Train net output #0: loss = 0.0586072 (* 1 = 0.0586072 loss)
I0731 20:38:07.858727   939 sgd_solver.cpp:136] Iteration 13700, lr = 1e-05, m = 0.9
I0731 20:38:10.704278   946 data_reader.cpp:264] Starting prefetch of epoch 8
I0731 20:38:26.472170   939 solver.cpp:353] Iteration 13800 (5.3726 iter/s, 18.613s/100 iter), loss = 0.0635349
I0731 20:38:26.472223   939 solver.cpp:375]     Train net output #0: loss = 0.0635349 (* 1 = 0.0635349 loss)
I0731 20:38:26.472229   939 sgd_solver.cpp:136] Iteration 13800, lr = 1e-05, m = 0.9
I0731 20:38:45.645328   939 solver.cpp:353] Iteration 13900 (5.21577 iter/s, 19.1726s/100 iter), loss = 0.0795874
I0731 20:38:45.645350   939 solver.cpp:375]     Train net output #0: loss = 0.0795874 (* 1 = 0.0795874 loss)
I0731 20:38:45.645355   939 sgd_solver.cpp:136] Iteration 13900, lr = 1e-05, m = 0.9
I0731 20:39:04.079917   939 solver.cpp:550] Iteration 14000, Testing net (#0)
I0731 20:39:11.365726   964 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 20:39:15.424510   939 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.951185
I0731 20:39:15.424532   939 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999377
I0731 20:39:15.424538   939 solver.cpp:635]     Test net output #2: loss = 0.189781 (* 1 = 0.189781 loss)
I0731 20:39:15.424615   939 solver.cpp:305] [MultiGPU] Tests completed in 11.3444s
I0731 20:39:15.628978   939 solver.cpp:353] Iteration 14000 (3.33524 iter/s, 29.9828s/100 iter), loss = 0.0917251
I0731 20:39:15.629000   939 solver.cpp:375]     Train net output #0: loss = 0.0917251 (* 1 = 0.0917251 loss)
I0731 20:39:15.629005   939 sgd_solver.cpp:136] Iteration 14000, lr = 1e-05, m = 0.9
I0731 20:39:34.119830   939 solver.cpp:353] Iteration 14100 (5.40823 iter/s, 18.4903s/100 iter), loss = 0.0957247
I0731 20:39:34.119877   939 solver.cpp:375]     Train net output #0: loss = 0.0957247 (* 1 = 0.0957247 loss)
I0731 20:39:34.119882   939 sgd_solver.cpp:136] Iteration 14100, lr = 1e-05, m = 0.9
I0731 20:39:52.626406   939 solver.cpp:353] Iteration 14200 (5.40363 iter/s, 18.5061s/100 iter), loss = 0.0651215
I0731 20:39:52.626428   939 solver.cpp:375]     Train net output #0: loss = 0.0651215 (* 1 = 0.0651215 loss)
I0731 20:39:52.626433   939 sgd_solver.cpp:136] Iteration 14200, lr = 1e-05, m = 0.9
I0731 20:39:54.541522   894 data_reader.cpp:264] Starting prefetch of epoch 10
I0731 20:40:11.305819   939 solver.cpp:353] Iteration 14300 (5.35364 iter/s, 18.6789s/100 iter), loss = 0.11724
I0731 20:40:11.305904   939 solver.cpp:375]     Train net output #0: loss = 0.11724 (* 1 = 0.11724 loss)
I0731 20:40:11.305909   939 sgd_solver.cpp:136] Iteration 14300, lr = 1e-05, m = 0.9
I0731 20:40:25.386140   942 data_reader.cpp:264] Starting prefetch of epoch 13
I0731 20:40:29.807031   939 solver.cpp:353] Iteration 14400 (5.4052 iter/s, 18.5007s/100 iter), loss = 0.083212
I0731 20:40:29.807054   939 solver.cpp:375]     Train net output #0: loss = 0.083212 (* 1 = 0.083212 loss)
I0731 20:40:29.807059   939 sgd_solver.cpp:136] Iteration 14400, lr = 1e-05, m = 0.9
I0731 20:40:48.395483   939 solver.cpp:353] Iteration 14500 (5.37983 iter/s, 18.5879s/100 iter), loss = 0.082013
I0731 20:40:48.395540   939 solver.cpp:375]     Train net output #0: loss = 0.0820129 (* 1 = 0.0820129 loss)
I0731 20:40:48.395545   939 sgd_solver.cpp:136] Iteration 14500, lr = 1e-05, m = 0.9
I0731 20:41:06.939455   939 solver.cpp:353] Iteration 14600 (5.39274 iter/s, 18.5435s/100 iter), loss = 0.0690233
I0731 20:41:06.939479   939 solver.cpp:375]     Train net output #0: loss = 0.0690233 (* 1 = 0.0690233 loss)
I0731 20:41:06.939483   939 sgd_solver.cpp:136] Iteration 14600, lr = 1e-05, m = 0.9
I0731 20:41:25.420676   939 solver.cpp:353] Iteration 14700 (5.41105 iter/s, 18.4807s/100 iter), loss = 0.0692289
I0731 20:41:25.420729   939 solver.cpp:375]     Train net output #0: loss = 0.0692289 (* 1 = 0.0692289 loss)
I0731 20:41:25.420735   939 sgd_solver.cpp:136] Iteration 14700, lr = 1e-05, m = 0.9
I0731 20:41:26.572103   943 data_reader.cpp:264] Starting prefetch of epoch 10
I0731 20:41:44.034658   939 solver.cpp:353] Iteration 14800 (5.37245 iter/s, 18.6135s/100 iter), loss = 0.0715818
I0731 20:41:44.034682   939 solver.cpp:375]     Train net output #0: loss = 0.0715818 (* 1 = 0.0715818 loss)
I0731 20:41:44.034685   939 sgd_solver.cpp:136] Iteration 14800, lr = 1e-05, m = 0.9
I0731 20:42:02.501672   939 solver.cpp:353] Iteration 14900 (5.41521 iter/s, 18.4665s/100 iter), loss = 0.0848472
I0731 20:42:02.501782   939 solver.cpp:375]     Train net output #0: loss = 0.0848472 (* 1 = 0.0848472 loss)
I0731 20:42:02.501790   939 sgd_solver.cpp:136] Iteration 14900, lr = 1e-05, m = 0.9
I0731 20:42:21.115388   939 solver.cpp:353] Iteration 15000 (5.37253 iter/s, 18.6132s/100 iter), loss = 0.0622132
I0731 20:42:21.115417   939 solver.cpp:375]     Train net output #0: loss = 0.0622132 (* 1 = 0.0622132 loss)
I0731 20:42:21.115422   939 sgd_solver.cpp:136] Iteration 15000, lr = 1e-05, m = 0.9
I0731 20:42:28.015661   946 data_reader.cpp:264] Starting prefetch of epoch 9
I0731 20:42:39.626898   939 solver.cpp:353] Iteration 15100 (5.40219 iter/s, 18.511s/100 iter), loss = 0.0991368
I0731 20:42:39.626947   939 solver.cpp:375]     Train net output #0: loss = 0.0991368 (* 1 = 0.0991368 loss)
I0731 20:42:39.626952   939 sgd_solver.cpp:136] Iteration 15100, lr = 1e-05, m = 0.9
I0731 20:42:58.179714   939 solver.cpp:353] Iteration 15200 (5.39017 iter/s, 18.5523s/100 iter), loss = 0.0838387
I0731 20:42:58.179738   939 solver.cpp:375]     Train net output #0: loss = 0.0838387 (* 1 = 0.0838387 loss)
I0731 20:42:58.179742   939 sgd_solver.cpp:136] Iteration 15200, lr = 1e-05, m = 0.9
I0731 20:43:16.757056   939 solver.cpp:353] Iteration 15300 (5.38305 iter/s, 18.5768s/100 iter), loss = 0.0382647
I0731 20:43:16.757130   939 solver.cpp:375]     Train net output #0: loss = 0.0382647 (* 1 = 0.0382647 loss)
I0731 20:43:16.757135   939 sgd_solver.cpp:136] Iteration 15300, lr = 1e-05, m = 0.9
I0731 20:43:29.233465   894 data_reader.cpp:264] Starting prefetch of epoch 11
I0731 20:43:29.233465   947 data_reader.cpp:264] Starting prefetch of epoch 8
I0731 20:43:35.380658   939 solver.cpp:353] Iteration 15400 (5.36968 iter/s, 18.6231s/100 iter), loss = 0.0742465
I0731 20:43:35.380713   939 solver.cpp:375]     Train net output #0: loss = 0.0742465 (* 1 = 0.0742465 loss)
I0731 20:43:35.380728   939 sgd_solver.cpp:136] Iteration 15400, lr = 1e-05, m = 0.9
I0731 20:43:54.797899   939 solver.cpp:353] Iteration 15500 (5.1502 iter/s, 19.4167s/100 iter), loss = 0.0759017
I0731 20:43:54.797973   939 solver.cpp:375]     Train net output #0: loss = 0.0759017 (* 1 = 0.0759017 loss)
I0731 20:43:54.797979   939 sgd_solver.cpp:136] Iteration 15500, lr = 1e-05, m = 0.9
I0731 20:44:00.971457   946 data_reader.cpp:264] Starting prefetch of epoch 10
I0731 20:44:13.351469   939 solver.cpp:353] Iteration 15600 (5.38995 iter/s, 18.5531s/100 iter), loss = 0.0672114
I0731 20:44:13.351493   939 solver.cpp:375]     Train net output #0: loss = 0.0672114 (* 1 = 0.0672114 loss)
I0731 20:44:13.351498   939 sgd_solver.cpp:136] Iteration 15600, lr = 1e-05, m = 0.9
I0731 20:44:31.897680   939 solver.cpp:353] Iteration 15700 (5.39209 iter/s, 18.5457s/100 iter), loss = 0.0813442
I0731 20:44:31.897733   939 solver.cpp:375]     Train net output #0: loss = 0.0813442 (* 1 = 0.0813442 loss)
I0731 20:44:31.897740   939 sgd_solver.cpp:136] Iteration 15700, lr = 1e-05, m = 0.9
I0731 20:44:50.403766   939 solver.cpp:353] Iteration 15800 (5.40378 iter/s, 18.5056s/100 iter), loss = 0.0727335
I0731 20:44:50.403794   939 solver.cpp:375]     Train net output #0: loss = 0.0727336 (* 1 = 0.0727336 loss)
I0731 20:44:50.403800   939 sgd_solver.cpp:136] Iteration 15800, lr = 1e-05, m = 0.9
I0731 20:45:02.088675   892 data_reader.cpp:264] Starting prefetch of epoch 7
I0731 20:45:08.874780   939 solver.cpp:353] Iteration 15900 (5.41404 iter/s, 18.4705s/100 iter), loss = 0.053651
I0731 20:45:08.874804   939 solver.cpp:375]     Train net output #0: loss = 0.0536511 (* 1 = 0.0536511 loss)
I0731 20:45:08.874809   939 sgd_solver.cpp:136] Iteration 15900, lr = 1e-05, m = 0.9
I0731 20:45:27.160521   939 solver.cpp:550] Iteration 16000, Testing net (#0)
I0731 20:45:38.169436   936 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 20:45:38.579989   939 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.952044
I0731 20:45:38.580005   939 solver.cpp:635]     Test net output #1: accuracy/top5 = 1
I0731 20:45:38.580011   939 solver.cpp:635]     Test net output #2: loss = 0.155514 (* 1 = 0.155514 loss)
I0731 20:45:38.580075   939 solver.cpp:305] [MultiGPU] Tests completed in 11.4192s
I0731 20:45:38.793963   939 solver.cpp:353] Iteration 16000 (3.34243 iter/s, 29.9184s/100 iter), loss = 0.0828593
I0731 20:45:38.793987   939 solver.cpp:375]     Train net output #0: loss = 0.0828593 (* 1 = 0.0828593 loss)
I0731 20:45:38.793992   939 sgd_solver.cpp:136] Iteration 16000, lr = 1e-05, m = 0.9
I0731 20:45:57.273690   939 solver.cpp:353] Iteration 16100 (5.41148 iter/s, 18.4792s/100 iter), loss = 0.0791324
I0731 20:45:57.273715   939 solver.cpp:375]     Train net output #0: loss = 0.0791325 (* 1 = 0.0791325 loss)
I0731 20:45:57.273720   939 sgd_solver.cpp:136] Iteration 16100, lr = 1e-05, m = 0.9
I0731 20:46:14.680006   894 data_reader.cpp:264] Starting prefetch of epoch 12
I0731 20:46:15.771966   939 solver.cpp:353] Iteration 16200 (5.40606 iter/s, 18.4978s/100 iter), loss = 0.0830605
I0731 20:46:15.771991   939 solver.cpp:375]     Train net output #0: loss = 0.0830605 (* 1 = 0.0830605 loss)
I0731 20:46:15.771996   939 sgd_solver.cpp:136] Iteration 16200, lr = 1e-05, m = 0.9
I0731 20:46:34.286165   939 solver.cpp:353] Iteration 16300 (5.40141 iter/s, 18.5137s/100 iter), loss = 0.0732633
I0731 20:46:34.286190   939 solver.cpp:375]     Train net output #0: loss = 0.0732634 (* 1 = 0.0732634 loss)
I0731 20:46:34.286195   939 sgd_solver.cpp:136] Iteration 16300, lr = 1e-05, m = 0.9
I0731 20:46:45.221496   942 data_reader.cpp:264] Starting prefetch of epoch 14
I0731 20:46:52.802165   939 solver.cpp:353] Iteration 16400 (5.40088 iter/s, 18.5155s/100 iter), loss = 0.0457898
I0731 20:46:52.802192   939 solver.cpp:375]     Train net output #0: loss = 0.0457899 (* 1 = 0.0457899 loss)
I0731 20:46:52.802197   939 sgd_solver.cpp:136] Iteration 16400, lr = 1e-05, m = 0.9
I0731 20:47:11.395292   939 solver.cpp:353] Iteration 16500 (5.37848 iter/s, 18.5926s/100 iter), loss = 0.0700195
I0731 20:47:11.395316   939 solver.cpp:375]     Train net output #0: loss = 0.0700195 (* 1 = 0.0700195 loss)
I0731 20:47:11.395321   939 sgd_solver.cpp:136] Iteration 16500, lr = 1e-05, m = 0.9
I0731 20:47:29.756744   939 solver.cpp:353] Iteration 16600 (5.44634 iter/s, 18.3609s/100 iter), loss = 0.07245
I0731 20:47:29.756826   939 solver.cpp:375]     Train net output #0: loss = 0.0724501 (* 1 = 0.0724501 loss)
I0731 20:47:29.756832   939 sgd_solver.cpp:136] Iteration 16600, lr = 1e-05, m = 0.9
I0731 20:47:46.397233   892 data_reader.cpp:264] Starting prefetch of epoch 8
I0731 20:47:48.268729   939 solver.cpp:353] Iteration 16700 (5.40205 iter/s, 18.5115s/100 iter), loss = 0.0987817
I0731 20:47:48.268752   939 solver.cpp:375]     Train net output #0: loss = 0.0987818 (* 1 = 0.0987818 loss)
I0731 20:47:48.268757   939 sgd_solver.cpp:136] Iteration 16700, lr = 1e-05, m = 0.9
I0731 20:48:07.185649   939 solver.cpp:353] Iteration 16800 (5.28642 iter/s, 18.9164s/100 iter), loss = 0.0810787
I0731 20:48:07.185806   939 solver.cpp:375]     Train net output #0: loss = 0.0810788 (* 1 = 0.0810788 loss)
I0731 20:48:07.185828   939 sgd_solver.cpp:136] Iteration 16800, lr = 1e-05, m = 0.9
I0731 20:48:26.308109   939 solver.cpp:353] Iteration 16900 (5.22959 iter/s, 19.1219s/100 iter), loss = 0.0680191
I0731 20:48:26.308132   939 solver.cpp:375]     Train net output #0: loss = 0.0680192 (* 1 = 0.0680192 loss)
I0731 20:48:26.308137   939 sgd_solver.cpp:136] Iteration 16900, lr = 1e-05, m = 0.9
I0731 20:48:44.817047   939 solver.cpp:353] Iteration 17000 (5.40294 iter/s, 18.5084s/100 iter), loss = 0.109556
I0731 20:48:44.817103   939 solver.cpp:375]     Train net output #0: loss = 0.109556 (* 1 = 0.109556 loss)
I0731 20:48:44.817108   939 sgd_solver.cpp:136] Iteration 17000, lr = 1e-05, m = 0.9
I0731 20:48:48.538607   947 data_reader.cpp:264] Starting prefetch of epoch 9
I0731 20:49:03.393391   939 solver.cpp:353] Iteration 17100 (5.38334 iter/s, 18.5758s/100 iter), loss = 0.0767635
I0731 20:49:03.393414   939 solver.cpp:375]     Train net output #0: loss = 0.0767636 (* 1 = 0.0767636 loss)
I0731 20:49:03.393419   939 sgd_solver.cpp:136] Iteration 17100, lr = 1e-05, m = 0.9
I0731 20:49:19.157716   892 data_reader.cpp:264] Starting prefetch of epoch 9
I0731 20:49:21.889629   939 solver.cpp:353] Iteration 17200 (5.40665 iter/s, 18.4957s/100 iter), loss = 0.0895705
I0731 20:49:21.889654   939 solver.cpp:375]     Train net output #0: loss = 0.0895706 (* 1 = 0.0895706 loss)
I0731 20:49:21.889659   939 sgd_solver.cpp:136] Iteration 17200, lr = 1e-05, m = 0.9
I0731 20:49:40.475559   939 solver.cpp:353] Iteration 17300 (5.38056 iter/s, 18.5854s/100 iter), loss = 0.0540904
I0731 20:49:40.475585   939 solver.cpp:375]     Train net output #0: loss = 0.0540904 (* 1 = 0.0540904 loss)
I0731 20:49:40.475589   939 sgd_solver.cpp:136] Iteration 17300, lr = 1e-05, m = 0.9
I0731 20:49:59.110576   939 solver.cpp:353] Iteration 17400 (5.36639 iter/s, 18.6345s/100 iter), loss = 0.0569095
I0731 20:49:59.110679   939 solver.cpp:375]     Train net output #0: loss = 0.0569096 (* 1 = 0.0569096 loss)
I0731 20:49:59.110687   939 sgd_solver.cpp:136] Iteration 17400, lr = 1e-05, m = 0.9
I0731 20:50:17.710886   939 solver.cpp:353] Iteration 17500 (5.3764 iter/s, 18.5998s/100 iter), loss = 0.0497751
I0731 20:50:17.710909   939 solver.cpp:375]     Train net output #0: loss = 0.0497752 (* 1 = 0.0497752 loss)
I0731 20:50:17.710913   939 sgd_solver.cpp:136] Iteration 17500, lr = 1e-05, m = 0.9
I0731 20:50:20.737745   892 data_reader.cpp:264] Starting prefetch of epoch 10
I0731 20:50:36.311942   939 solver.cpp:353] Iteration 17600 (5.37619 iter/s, 18.6005s/100 iter), loss = 0.0828262
I0731 20:50:36.320150   939 solver.cpp:375]     Train net output #0: loss = 0.0828262 (* 1 = 0.0828262 loss)
I0731 20:50:36.320201   939 sgd_solver.cpp:136] Iteration 17600, lr = 1e-05, m = 0.9
I0731 20:50:54.850461   939 solver.cpp:353] Iteration 17700 (5.39432 iter/s, 18.538s/100 iter), loss = 0.0636286
I0731 20:50:54.850482   939 solver.cpp:375]     Train net output #0: loss = 0.0636286 (* 1 = 0.0636286 loss)
I0731 20:50:54.850486   939 sgd_solver.cpp:136] Iteration 17700, lr = 1e-05, m = 0.9
I0731 20:51:13.427798   939 solver.cpp:353] Iteration 17800 (5.38305 iter/s, 18.5768s/100 iter), loss = 0.0625521
I0731 20:51:13.427850   939 solver.cpp:375]     Train net output #0: loss = 0.0625522 (* 1 = 0.0625522 loss)
I0731 20:51:13.427855   939 sgd_solver.cpp:136] Iteration 17800, lr = 1e-05, m = 0.9
I0731 20:51:21.991533   947 data_reader.cpp:264] Starting prefetch of epoch 10
I0731 20:51:32.029822   939 solver.cpp:353] Iteration 17900 (5.37591 iter/s, 18.6015s/100 iter), loss = 0.0881784
I0731 20:51:32.029845   939 solver.cpp:375]     Train net output #0: loss = 0.0881785 (* 1 = 0.0881785 loss)
I0731 20:51:32.029850   939 sgd_solver.cpp:136] Iteration 17900, lr = 1e-05, m = 0.9
I0731 20:51:50.482477   939 solver.cpp:550] Iteration 18000, Testing net (#0)
I0731 20:51:57.703033   964 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 20:52:01.706744   939 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.951585
I0731 20:52:01.706771   939 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999445
I0731 20:52:01.706778   939 solver.cpp:635]     Test net output #2: loss = 0.189355 (* 1 = 0.189355 loss)
I0731 20:52:01.706799   939 solver.cpp:305] [MultiGPU] Tests completed in 11.224s
I0731 20:52:01.904304   939 solver.cpp:353] Iteration 18000 (3.34743 iter/s, 29.8737s/100 iter), loss = 0.0490804
I0731 20:52:01.904327   939 solver.cpp:375]     Train net output #0: loss = 0.0490804 (* 1 = 0.0490804 loss)
I0731 20:52:01.904332   939 sgd_solver.cpp:136] Iteration 18000, lr = 1e-05, m = 0.9
I0731 20:52:20.324607   939 solver.cpp:353] Iteration 18100 (5.42894 iter/s, 18.4198s/100 iter), loss = 0.058745
I0731 20:52:20.324630   939 solver.cpp:375]     Train net output #0: loss = 0.058745 (* 1 = 0.058745 loss)
I0731 20:52:20.324635   939 sgd_solver.cpp:136] Iteration 18100, lr = 1e-05, m = 0.9
I0731 20:52:34.636981   946 data_reader.cpp:264] Starting prefetch of epoch 11
I0731 20:52:38.852813   939 solver.cpp:353] Iteration 18200 (5.39733 iter/s, 18.5277s/100 iter), loss = 0.0679373
I0731 20:52:38.852843   939 solver.cpp:375]     Train net output #0: loss = 0.0679373 (* 1 = 0.0679373 loss)
I0731 20:52:38.852849   939 sgd_solver.cpp:136] Iteration 18200, lr = 1e-05, m = 0.9
I0731 20:52:57.369592   939 solver.cpp:353] Iteration 18300 (5.40066 iter/s, 18.5163s/100 iter), loss = 0.0761983
I0731 20:52:57.369618   939 solver.cpp:375]     Train net output #0: loss = 0.0761984 (* 1 = 0.0761984 loss)
I0731 20:52:57.369622   939 sgd_solver.cpp:136] Iteration 18300, lr = 1e-05, m = 0.9
I0731 20:53:15.846679   939 solver.cpp:353] Iteration 18400 (5.41226 iter/s, 18.4766s/100 iter), loss = 0.0698998
I0731 20:53:15.846736   939 solver.cpp:375]     Train net output #0: loss = 0.0698999 (* 1 = 0.0698999 loss)
I0731 20:53:15.846741   939 sgd_solver.cpp:136] Iteration 18400, lr = 1e-05, m = 0.9
I0731 20:53:34.331626   939 solver.cpp:353] Iteration 18500 (5.40996 iter/s, 18.4844s/100 iter), loss = 0.0821422
I0731 20:53:34.331647   939 solver.cpp:375]     Train net output #0: loss = 0.0821422 (* 1 = 0.0821422 loss)
I0731 20:53:34.331651   939 sgd_solver.cpp:136] Iteration 18500, lr = 1e-05, m = 0.9
I0731 20:53:35.803184   946 data_reader.cpp:264] Starting prefetch of epoch 12
I0731 20:53:52.934165   939 solver.cpp:353] Iteration 18600 (5.37576 iter/s, 18.602s/100 iter), loss = 0.066207
I0731 20:53:52.934267   939 solver.cpp:375]     Train net output #0: loss = 0.0662071 (* 1 = 0.0662071 loss)
I0731 20:53:52.934274   939 sgd_solver.cpp:136] Iteration 18600, lr = 1e-05, m = 0.9
I0731 20:54:11.327471   939 solver.cpp:353] Iteration 18700 (5.43691 iter/s, 18.3928s/100 iter), loss = 0.0471658
I0731 20:54:11.327497   939 solver.cpp:375]     Train net output #0: loss = 0.0471658 (* 1 = 0.0471658 loss)
I0731 20:54:11.327503   939 sgd_solver.cpp:136] Iteration 18700, lr = 1e-05, m = 0.9
I0731 20:54:29.884342   939 solver.cpp:353] Iteration 18800 (5.38899 iter/s, 18.5564s/100 iter), loss = 0.139459
I0731 20:54:29.884467   939 solver.cpp:375]     Train net output #0: loss = 0.139459 (* 1 = 0.139459 loss)
I0731 20:54:29.884474   939 sgd_solver.cpp:136] Iteration 18800, lr = 1e-05, m = 0.9
I0731 20:54:36.961314   946 data_reader.cpp:264] Starting prefetch of epoch 13
I0731 20:54:48.527128   939 solver.cpp:353] Iteration 18900 (5.36415 iter/s, 18.6423s/100 iter), loss = 0.0740938
I0731 20:54:48.527164   939 solver.cpp:375]     Train net output #0: loss = 0.0740939 (* 1 = 0.0740939 loss)
I0731 20:54:48.527171   939 sgd_solver.cpp:136] Iteration 18900, lr = 1e-05, m = 0.9
I0731 20:55:07.043957   939 solver.cpp:353] Iteration 19000 (5.40064 iter/s, 18.5163s/100 iter), loss = 0.117273
I0731 20:55:07.044014   939 solver.cpp:375]     Train net output #0: loss = 0.117273 (* 1 = 0.117273 loss)
I0731 20:55:07.044019   939 sgd_solver.cpp:136] Iteration 19000, lr = 1e-05, m = 0.9
I0731 20:55:07.634112   892 data_reader.cpp:264] Starting prefetch of epoch 11
I0731 20:55:25.515283   939 solver.cpp:353] Iteration 19100 (5.41395 iter/s, 18.4708s/100 iter), loss = 0.0563159
I0731 20:55:25.515306   939 solver.cpp:375]     Train net output #0: loss = 0.056316 (* 1 = 0.056316 loss)
I0731 20:55:25.515311   939 sgd_solver.cpp:136] Iteration 19100, lr = 1e-05, m = 0.9
I0731 20:55:43.966562   939 solver.cpp:353] Iteration 19200 (5.41983 iter/s, 18.4508s/100 iter), loss = 0.0902543
I0731 20:55:43.966614   939 solver.cpp:375]     Train net output #0: loss = 0.0902544 (* 1 = 0.0902544 loss)
I0731 20:55:43.966619   939 sgd_solver.cpp:136] Iteration 19200, lr = 1e-05, m = 0.9
I0731 20:56:02.476471   939 solver.cpp:353] Iteration 19300 (5.40266 iter/s, 18.5094s/100 iter), loss = 0.09839
I0731 20:56:02.476495   939 solver.cpp:375]     Train net output #0: loss = 0.0983901 (* 1 = 0.0983901 loss)
I0731 20:56:02.476501   939 sgd_solver.cpp:136] Iteration 19300, lr = 1e-05, m = 0.9
I0731 20:56:08.778620   943 data_reader.cpp:264] Starting prefetch of epoch 11
I0731 20:56:21.025800   939 solver.cpp:353] Iteration 19400 (5.39118 iter/s, 18.5488s/100 iter), loss = 0.0960012
I0731 20:56:21.025853   939 solver.cpp:375]     Train net output #0: loss = 0.0960013 (* 1 = 0.0960013 loss)
I0731 20:56:21.025859   939 sgd_solver.cpp:136] Iteration 19400, lr = 1e-05, m = 0.9
I0731 20:56:39.417237   939 solver.cpp:353] Iteration 19500 (5.43746 iter/s, 18.3909s/100 iter), loss = 0.0759387
I0731 20:56:39.417261   939 solver.cpp:375]     Train net output #0: loss = 0.0759388 (* 1 = 0.0759388 loss)
I0731 20:56:39.417265   939 sgd_solver.cpp:136] Iteration 19500, lr = 1e-05, m = 0.9
I0731 20:56:57.874951   939 solver.cpp:353] Iteration 19600 (5.41794 iter/s, 18.4572s/100 iter), loss = 0.0866476
I0731 20:56:57.875003   939 solver.cpp:375]     Train net output #0: loss = 0.0866476 (* 1 = 0.0866476 loss)
I0731 20:56:57.875010   939 sgd_solver.cpp:136] Iteration 19600, lr = 1e-05, m = 0.9
I0731 20:57:09.954059   947 data_reader.cpp:264] Starting prefetch of epoch 11
I0731 20:57:16.490694   939 solver.cpp:353] Iteration 19700 (5.37194 iter/s, 18.6152s/100 iter), loss = 0.393477
I0731 20:57:16.490718   939 solver.cpp:375]     Train net output #0: loss = 0.393477 (* 1 = 0.393477 loss)
I0731 20:57:16.490725   939 sgd_solver.cpp:136] Iteration 19700, lr = 1e-05, m = 0.9
I0731 20:57:34.917773   939 solver.cpp:353] Iteration 19800 (5.42695 iter/s, 18.4266s/100 iter), loss = 0.0709639
I0731 20:57:34.917835   939 solver.cpp:375]     Train net output #0: loss = 0.070964 (* 1 = 0.070964 loss)
I0731 20:57:34.917845   939 sgd_solver.cpp:136] Iteration 19800, lr = 1e-05, m = 0.9
I0731 20:57:40.489784   892 data_reader.cpp:264] Starting prefetch of epoch 12
I0731 20:57:53.635242   939 solver.cpp:353] Iteration 19900 (5.34275 iter/s, 18.717s/100 iter), loss = 0.0659377
I0731 20:57:53.635267   939 solver.cpp:375]     Train net output #0: loss = 0.0659378 (* 1 = 0.0659378 loss)
I0731 20:57:53.635272   939 sgd_solver.cpp:136] Iteration 19900, lr = 1e-05, m = 0.9
I0731 20:58:12.020771   939 solver.cpp:680] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/l1reg/cityscapes5_jsegnet21v2_iter_20000.caffemodel
I0731 20:58:12.081610   939 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/l1reg/cityscapes5_jsegnet21v2_iter_20000.solverstate
I0731 20:58:12.092514   939 solver.cpp:550] Iteration 20000, Testing net (#0)
I0731 20:58:15.439329   968 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 20:58:23.383131   939 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.950996
I0731 20:58:23.383153   939 solver.cpp:635]     Test net output #1: accuracy/top5 = 1
I0731 20:58:23.383158   939 solver.cpp:635]     Test net output #2: loss = 0.158346 (* 1 = 0.158346 loss)
I0731 20:58:23.383235   939 solver.cpp:305] [MultiGPU] Tests completed in 11.2904s
I0731 20:58:23.574241   939 solver.cpp:353] Iteration 20000 (3.34022 iter/s, 29.9382s/100 iter), loss = 0.0512986
I0731 20:58:23.574265   939 solver.cpp:375]     Train net output #0: loss = 0.0512986 (* 1 = 0.0512986 loss)
I0731 20:58:23.574268   939 sgd_solver.cpp:136] Iteration 20000, lr = 1e-05, m = 0.9
I0731 20:58:42.067914   939 solver.cpp:353] Iteration 20100 (5.40741 iter/s, 18.4932s/100 iter), loss = 0.0434395
I0731 20:58:42.067966   939 solver.cpp:375]     Train net output #0: loss = 0.0434396 (* 1 = 0.0434396 loss)
I0731 20:58:42.067971   939 sgd_solver.cpp:136] Iteration 20100, lr = 1e-05, m = 0.9
I0731 20:58:53.194483   947 data_reader.cpp:264] Starting prefetch of epoch 12
I0731 20:59:00.596474   939 solver.cpp:353] Iteration 20200 (5.39722 iter/s, 18.528s/100 iter), loss = 0.0937129
I0731 20:59:00.596498   939 solver.cpp:375]     Train net output #0: loss = 0.0937129 (* 1 = 0.0937129 loss)
I0731 20:59:00.596503   939 sgd_solver.cpp:136] Iteration 20200, lr = 1e-05, m = 0.9
I0731 20:59:19.105165   939 solver.cpp:353] Iteration 20300 (5.40302 iter/s, 18.5082s/100 iter), loss = 0.0662116
I0731 20:59:19.105216   939 solver.cpp:375]     Train net output #0: loss = 0.0662116 (* 1 = 0.0662116 loss)
I0731 20:59:19.105221   939 sgd_solver.cpp:136] Iteration 20300, lr = 1e-05, m = 0.9
I0731 20:59:23.969076   942 data_reader.cpp:264] Starting prefetch of epoch 15
I0731 20:59:37.544450   939 solver.cpp:353] Iteration 20400 (5.42335 iter/s, 18.4388s/100 iter), loss = 0.0727837
I0731 20:59:37.544473   939 solver.cpp:375]     Train net output #0: loss = 0.0727838 (* 1 = 0.0727838 loss)
I0731 20:59:37.544477   939 sgd_solver.cpp:136] Iteration 20400, lr = 1e-05, m = 0.9
I0731 20:59:56.009604   939 solver.cpp:353] Iteration 20500 (5.41575 iter/s, 18.4646s/100 iter), loss = 0.0600298
I0731 20:59:56.009654   939 solver.cpp:375]     Train net output #0: loss = 0.0600299 (* 1 = 0.0600299 loss)
I0731 20:59:56.009660   939 sgd_solver.cpp:136] Iteration 20500, lr = 1e-05, m = 0.9
I0731 21:00:14.679563   939 solver.cpp:353] Iteration 20600 (5.35635 iter/s, 18.6694s/100 iter), loss = 0.0593681
I0731 21:00:14.679584   939 solver.cpp:375]     Train net output #0: loss = 0.0593682 (* 1 = 0.0593682 loss)
I0731 21:00:14.679589   939 sgd_solver.cpp:136] Iteration 20600, lr = 1e-05, m = 0.9
I0731 21:00:25.087661   943 data_reader.cpp:264] Starting prefetch of epoch 12
I0731 21:00:33.129458   939 solver.cpp:353] Iteration 20700 (5.42023 iter/s, 18.4494s/100 iter), loss = 0.0493263
I0731 21:00:33.129513   939 solver.cpp:375]     Train net output #0: loss = 0.0493263 (* 1 = 0.0493263 loss)
I0731 21:00:33.129518   939 sgd_solver.cpp:136] Iteration 20700, lr = 1e-05, m = 0.9
I0731 21:00:51.621429   939 solver.cpp:353] Iteration 20800 (5.4079 iter/s, 18.4915s/100 iter), loss = 0.0438492
I0731 21:00:51.621451   939 solver.cpp:375]     Train net output #0: loss = 0.0438492 (* 1 = 0.0438492 loss)
I0731 21:00:51.621455   939 sgd_solver.cpp:136] Iteration 20800, lr = 1e-05, m = 0.9
I0731 21:01:10.072343   939 solver.cpp:353] Iteration 20900 (5.41993 iter/s, 18.4504s/100 iter), loss = 0.0516629
I0731 21:01:10.072438   939 solver.cpp:375]     Train net output #0: loss = 0.051663 (* 1 = 0.051663 loss)
I0731 21:01:10.072445   939 sgd_solver.cpp:136] Iteration 20900, lr = 1e-05, m = 0.9
I0731 21:01:26.192036   946 data_reader.cpp:264] Starting prefetch of epoch 14
I0731 21:01:28.562803   939 solver.cpp:353] Iteration 21000 (5.40834 iter/s, 18.49s/100 iter), loss = 0.0778184
I0731 21:01:28.562829   939 solver.cpp:375]     Train net output #0: loss = 0.0778185 (* 1 = 0.0778185 loss)
I0731 21:01:28.562834   939 sgd_solver.cpp:136] Iteration 21000, lr = 1e-05, m = 0.9
I0731 21:01:47.017199   939 solver.cpp:353] Iteration 21100 (5.41891 iter/s, 18.4539s/100 iter), loss = 0.0584773
I0731 21:01:47.017248   939 solver.cpp:375]     Train net output #0: loss = 0.0584773 (* 1 = 0.0584773 loss)
I0731 21:01:47.017254   939 sgd_solver.cpp:136] Iteration 21100, lr = 1e-05, m = 0.9
I0731 21:01:56.673998   942 data_reader.cpp:264] Starting prefetch of epoch 16
I0731 21:02:05.480778   939 solver.cpp:353] Iteration 21200 (5.41622 iter/s, 18.4631s/100 iter), loss = 0.0687939
I0731 21:02:05.480803   939 solver.cpp:375]     Train net output #0: loss = 0.0687939 (* 1 = 0.0687939 loss)
I0731 21:02:05.480808   939 sgd_solver.cpp:136] Iteration 21200, lr = 1e-05, m = 0.9
I0731 21:02:24.125124   939 solver.cpp:353] Iteration 21300 (5.3637 iter/s, 18.6438s/100 iter), loss = 0.0833647
I0731 21:02:24.125205   939 solver.cpp:375]     Train net output #0: loss = 0.0833647 (* 1 = 0.0833647 loss)
I0731 21:02:24.125210   939 sgd_solver.cpp:136] Iteration 21300, lr = 1e-05, m = 0.9
I0731 21:02:42.688063   939 solver.cpp:353] Iteration 21400 (5.38723 iter/s, 18.5624s/100 iter), loss = 0.0602999
I0731 21:02:42.688087   939 solver.cpp:375]     Train net output #0: loss = 0.0603 (* 1 = 0.0603 loss)
I0731 21:02:42.688092   939 sgd_solver.cpp:136] Iteration 21400, lr = 1e-05, m = 0.9
I0731 21:02:58.139914   892 data_reader.cpp:264] Starting prefetch of epoch 13
I0731 21:03:01.240658   939 solver.cpp:353] Iteration 21500 (5.39023 iter/s, 18.5521s/100 iter), loss = 0.0496321
I0731 21:03:01.240679   939 solver.cpp:375]     Train net output #0: loss = 0.0496322 (* 1 = 0.0496322 loss)
I0731 21:03:01.240684   939 sgd_solver.cpp:136] Iteration 21500, lr = 1e-05, m = 0.9
I0731 21:03:19.712658   939 solver.cpp:353] Iteration 21600 (5.41375 iter/s, 18.4715s/100 iter), loss = 0.0773465
I0731 21:03:19.712683   939 solver.cpp:375]     Train net output #0: loss = 0.0773465 (* 1 = 0.0773465 loss)
I0731 21:03:19.712687   939 sgd_solver.cpp:136] Iteration 21600, lr = 1e-05, m = 0.9
I0731 21:03:38.317005   939 solver.cpp:353] Iteration 21700 (5.37524 iter/s, 18.6038s/100 iter), loss = 0.0566411
I0731 21:03:38.317059   939 solver.cpp:375]     Train net output #0: loss = 0.0566412 (* 1 = 0.0566412 loss)
I0731 21:03:38.317065   939 sgd_solver.cpp:136] Iteration 21700, lr = 1e-05, m = 0.9
I0731 21:03:56.924098   939 solver.cpp:353] Iteration 21800 (5.37444 iter/s, 18.6066s/100 iter), loss = 0.0675101
I0731 21:03:56.924123   939 solver.cpp:375]     Train net output #0: loss = 0.0675102 (* 1 = 0.0675102 loss)
I0731 21:03:56.924126   939 sgd_solver.cpp:136] Iteration 21800, lr = 1e-05, m = 0.9
I0731 21:03:59.365932   942 data_reader.cpp:264] Starting prefetch of epoch 17
I0731 21:04:15.356528   939 solver.cpp:353] Iteration 21900 (5.42537 iter/s, 18.4319s/100 iter), loss = 0.0664937
I0731 21:04:15.356578   939 solver.cpp:375]     Train net output #0: loss = 0.0664937 (* 1 = 0.0664937 loss)
I0731 21:04:15.356585   939 sgd_solver.cpp:136] Iteration 21900, lr = 1e-05, m = 0.9
I0731 21:04:33.742339   939 solver.cpp:550] Iteration 22000, Testing net (#0)
I0731 21:04:44.735383   968 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 21:04:50.559931   939 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.952044
I0731 21:04:50.560056   939 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999556
I0731 21:04:50.560066   939 solver.cpp:635]     Test net output #2: loss = 0.187762 (* 1 = 0.187762 loss)
I0731 21:04:50.560092   939 solver.cpp:305] [MultiGPU] Tests completed in 16.8173s
I0731 21:04:50.769841   939 solver.cpp:353] Iteration 22000 (2.82387 iter/s, 35.4123s/100 iter), loss = 0.0866886
I0731 21:04:50.769865   939 solver.cpp:375]     Train net output #0: loss = 0.0866887 (* 1 = 0.0866887 loss)
I0731 21:04:50.769870   939 sgd_solver.cpp:136] Iteration 22000, lr = 1e-05, m = 0.9
I0731 21:05:10.767659   939 solver.cpp:353] Iteration 22100 (5.00068 iter/s, 19.9973s/100 iter), loss = 0.0588401
I0731 21:05:10.767688   939 solver.cpp:375]     Train net output #0: loss = 0.0588401 (* 1 = 0.0588401 loss)
I0731 21:05:10.767691   939 sgd_solver.cpp:136] Iteration 22100, lr = 1e-05, m = 0.9
I0731 21:05:19.094565   943 data_reader.cpp:264] Starting prefetch of epoch 13
I0731 21:05:29.424554   939 solver.cpp:353] Iteration 22200 (5.3601 iter/s, 18.6564s/100 iter), loss = 0.0782181
I0731 21:05:29.424628   939 solver.cpp:375]     Train net output #0: loss = 0.0782181 (* 1 = 0.0782181 loss)
I0731 21:05:29.424633   939 sgd_solver.cpp:136] Iteration 22200, lr = 1e-05, m = 0.9
I0731 21:05:47.859737   939 solver.cpp:353] Iteration 22300 (5.42456 iter/s, 18.4347s/100 iter), loss = 0.0477387
I0731 21:05:47.859763   939 solver.cpp:375]     Train net output #0: loss = 0.0477387 (* 1 = 0.0477387 loss)
I0731 21:05:47.859767   939 sgd_solver.cpp:136] Iteration 22300, lr = 1e-05, m = 0.9
I0731 21:06:06.467952   939 solver.cpp:353] Iteration 22400 (5.37412 iter/s, 18.6077s/100 iter), loss = 0.0560148
I0731 21:06:06.468052   939 solver.cpp:375]     Train net output #0: loss = 0.0560149 (* 1 = 0.0560149 loss)
I0731 21:06:06.468060   939 sgd_solver.cpp:136] Iteration 22400, lr = 1e-05, m = 0.9
I0731 21:06:20.336583   943 data_reader.cpp:264] Starting prefetch of epoch 14
I0731 21:06:25.116791   939 solver.cpp:353] Iteration 22500 (5.36241 iter/s, 18.6483s/100 iter), loss = 0.0693913
I0731 21:06:25.116813   939 solver.cpp:375]     Train net output #0: loss = 0.0693914 (* 1 = 0.0693914 loss)
I0731 21:06:25.116822   939 sgd_solver.cpp:136] Iteration 22500, lr = 1e-05, m = 0.9
I0731 21:06:43.553745   939 solver.cpp:353] Iteration 22600 (5.42404 iter/s, 18.4364s/100 iter), loss = 0.0473462
I0731 21:06:43.553802   939 solver.cpp:375]     Train net output #0: loss = 0.0473463 (* 1 = 0.0473463 loss)
I0731 21:06:43.553807   939 sgd_solver.cpp:136] Iteration 22600, lr = 1e-05, m = 0.9
I0731 21:07:02.062702   939 solver.cpp:353] Iteration 22700 (5.40294 iter/s, 18.5084s/100 iter), loss = 0.0760019
I0731 21:07:02.062729   939 solver.cpp:375]     Train net output #0: loss = 0.0760019 (* 1 = 0.0760019 loss)
I0731 21:07:02.062734   939 sgd_solver.cpp:136] Iteration 22700, lr = 1e-05, m = 0.9
I0731 21:07:20.495075   939 solver.cpp:353] Iteration 22800 (5.42539 iter/s, 18.4319s/100 iter), loss = 0.0444027
I0731 21:07:20.495131   939 solver.cpp:375]     Train net output #0: loss = 0.0444027 (* 1 = 0.0444027 loss)
I0731 21:07:20.495138   939 sgd_solver.cpp:136] Iteration 22800, lr = 1e-05, m = 0.9
I0731 21:07:21.483717   947 data_reader.cpp:264] Starting prefetch of epoch 13
I0731 21:07:39.116727   939 solver.cpp:353] Iteration 22900 (5.37024 iter/s, 18.6211s/100 iter), loss = 0.0751983
I0731 21:07:39.116750   939 solver.cpp:375]     Train net output #0: loss = 0.0751984 (* 1 = 0.0751984 loss)
I0731 21:07:39.116755   939 sgd_solver.cpp:136] Iteration 22900, lr = 1e-05, m = 0.9
I0731 21:07:57.673069   939 solver.cpp:353] Iteration 23000 (5.38914 iter/s, 18.5558s/100 iter), loss = 0.086472
I0731 21:07:57.673117   939 solver.cpp:375]     Train net output #0: loss = 0.0864721 (* 1 = 0.0864721 loss)
I0731 21:07:57.673123   939 sgd_solver.cpp:136] Iteration 23000, lr = 1e-05, m = 0.9
I0731 21:08:16.235271   939 solver.cpp:353] Iteration 23100 (5.38744 iter/s, 18.5617s/100 iter), loss = 0.0576055
I0731 21:08:16.235296   939 solver.cpp:375]     Train net output #0: loss = 0.0576056 (* 1 = 0.0576056 loss)
I0731 21:08:16.235299   939 sgd_solver.cpp:136] Iteration 23100, lr = 1e-05, m = 0.9
I0731 21:08:22.733002   946 data_reader.cpp:264] Starting prefetch of epoch 15
I0731 21:08:34.806177   939 solver.cpp:353] Iteration 23200 (5.38491 iter/s, 18.5704s/100 iter), loss = 0.0519157
I0731 21:08:34.806251   939 solver.cpp:375]     Train net output #0: loss = 0.0519157 (* 1 = 0.0519157 loss)
I0731 21:08:34.806257   939 sgd_solver.cpp:136] Iteration 23200, lr = 1e-05, m = 0.9
I0731 21:08:53.147899   939 solver.cpp:353] Iteration 23300 (5.4522 iter/s, 18.3412s/100 iter), loss = 0.0422721
I0731 21:08:53.147923   939 solver.cpp:375]     Train net output #0: loss = 0.0422721 (* 1 = 0.0422721 loss)
I0731 21:08:53.147927   939 sgd_solver.cpp:136] Iteration 23300, lr = 1e-05, m = 0.9
I0731 21:08:53.363098   942 data_reader.cpp:264] Starting prefetch of epoch 18
I0731 21:09:11.718912   939 solver.cpp:353] Iteration 23400 (5.38488 iter/s, 18.5705s/100 iter), loss = 0.0534979
I0731 21:09:11.718986   939 solver.cpp:375]     Train net output #0: loss = 0.0534979 (* 1 = 0.0534979 loss)
I0731 21:09:11.718991   939 sgd_solver.cpp:136] Iteration 23400, lr = 1e-05, m = 0.9
I0731 21:09:30.239451   939 solver.cpp:353] Iteration 23500 (5.39956 iter/s, 18.52s/100 iter), loss = 0.200214
I0731 21:09:30.239476   939 solver.cpp:375]     Train net output #0: loss = 0.200214 (* 1 = 0.200214 loss)
I0731 21:09:30.239480   939 sgd_solver.cpp:136] Iteration 23500, lr = 1e-05, m = 0.9
I0731 21:09:48.747464   939 solver.cpp:353] Iteration 23600 (5.40321 iter/s, 18.5075s/100 iter), loss = 0.0441283
I0731 21:09:48.747514   939 solver.cpp:375]     Train net output #0: loss = 0.0441284 (* 1 = 0.0441284 loss)
I0731 21:09:48.747520   939 sgd_solver.cpp:136] Iteration 23600, lr = 1e-05, m = 0.9
I0731 21:09:54.480120   892 data_reader.cpp:264] Starting prefetch of epoch 14
I0731 21:10:07.384369   939 solver.cpp:353] Iteration 23700 (5.36585 iter/s, 18.6364s/100 iter), loss = 0.064901
I0731 21:10:07.384394   939 solver.cpp:375]     Train net output #0: loss = 0.0649011 (* 1 = 0.0649011 loss)
I0731 21:10:07.384399   939 sgd_solver.cpp:136] Iteration 23700, lr = 1e-05, m = 0.9
I0731 21:10:25.946593   939 solver.cpp:353] Iteration 23800 (5.38743 iter/s, 18.5617s/100 iter), loss = 0.0935597
I0731 21:10:25.946650   939 solver.cpp:375]     Train net output #0: loss = 0.0935598 (* 1 = 0.0935598 loss)
I0731 21:10:25.946655   939 sgd_solver.cpp:136] Iteration 23800, lr = 1e-05, m = 0.9
I0731 21:10:44.486723   939 solver.cpp:353] Iteration 23900 (5.39385 iter/s, 18.5396s/100 iter), loss = 0.0616524
I0731 21:10:44.486752   939 solver.cpp:375]     Train net output #0: loss = 0.0616525 (* 1 = 0.0616525 loss)
I0731 21:10:44.486758   939 sgd_solver.cpp:136] Iteration 23900, lr = 1e-05, m = 0.9
I0731 21:10:56.213419   946 data_reader.cpp:264] Starting prefetch of epoch 16
I0731 21:11:03.049525   939 solver.cpp:550] Iteration 24000, Testing net (#0)
I0731 21:11:06.443019   964 data_reader.cpp:264] Starting prefetch of epoch 4
I0731 21:11:14.191648   939 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.950746
I0731 21:11:14.191673   939 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999997
I0731 21:11:14.191679   939 solver.cpp:635]     Test net output #2: loss = 0.160371 (* 1 = 0.160371 loss)
I0731 21:11:14.191701   939 solver.cpp:305] [MultiGPU] Tests completed in 11.1419s
I0731 21:11:14.292558   995 sgd_solver.cpp:48] MultiStep Status: Iteration 24000, step = 1
I0731 21:11:14.292558   994 sgd_solver.cpp:48] MultiStep Status: Iteration 24000, step = 1
I0731 21:11:14.292558   993 sgd_solver.cpp:48] MultiStep Status: Iteration 24000, step = 1
I0731 21:11:14.399313   939 solver.cpp:353] Iteration 24000 (3.34316 iter/s, 29.9118s/100 iter), loss = 0.084097
I0731 21:11:14.399336   939 solver.cpp:375]     Train net output #0: loss = 0.0840971 (* 1 = 0.0840971 loss)
I0731 21:11:14.399341   939 sgd_solver.cpp:136] Iteration 24000, lr = 1e-06, m = 0.9
I0731 21:11:32.837975   939 solver.cpp:353] Iteration 24100 (5.42354 iter/s, 18.4382s/100 iter), loss = 0.0736763
I0731 21:11:32.845600   939 solver.cpp:375]     Train net output #0: loss = 0.0736764 (* 1 = 0.0736764 loss)
I0731 21:11:32.845657   939 sgd_solver.cpp:136] Iteration 24100, lr = 1e-06, m = 0.9
I0731 21:11:37.857512   892 data_reader.cpp:264] Starting prefetch of epoch 15
I0731 21:11:51.422911   939 solver.cpp:353] Iteration 24200 (5.38085 iter/s, 18.5844s/100 iter), loss = 0.0923543
I0731 21:11:51.422938   939 solver.cpp:375]     Train net output #0: loss = 0.0923543 (* 1 = 0.0923543 loss)
I0731 21:11:51.422945   939 sgd_solver.cpp:136] Iteration 24200, lr = 1e-06, m = 0.9
I0731 21:12:09.868685   939 solver.cpp:353] Iteration 24300 (5.42145 iter/s, 18.4453s/100 iter), loss = 0.0561257
I0731 21:12:09.868760   939 solver.cpp:375]     Train net output #0: loss = 0.0561258 (* 1 = 0.0561258 loss)
I0731 21:12:09.868767   939 sgd_solver.cpp:136] Iteration 24300, lr = 1e-06, m = 0.9
I0731 21:12:28.413172   939 solver.cpp:353] Iteration 24400 (5.39259 iter/s, 18.544s/100 iter), loss = 0.072673
I0731 21:12:28.413197   939 solver.cpp:375]     Train net output #0: loss = 0.0726731 (* 1 = 0.0726731 loss)
I0731 21:12:28.413203   939 sgd_solver.cpp:136] Iteration 24400, lr = 1e-06, m = 0.9
I0731 21:12:39.173022   947 data_reader.cpp:264] Starting prefetch of epoch 14
I0731 21:12:46.928208   939 solver.cpp:353] Iteration 24500 (5.40116 iter/s, 18.5145s/100 iter), loss = 0.0502452
I0731 21:12:46.928256   939 solver.cpp:375]     Train net output #0: loss = 0.0502452 (* 1 = 0.0502452 loss)
I0731 21:12:46.928261   939 sgd_solver.cpp:136] Iteration 24500, lr = 1e-06, m = 0.9
I0731 21:13:05.487732   939 solver.cpp:353] Iteration 24600 (5.38822 iter/s, 18.559s/100 iter), loss = 0.098129
I0731 21:13:05.487756   939 solver.cpp:375]     Train net output #0: loss = 0.0981291 (* 1 = 0.0981291 loss)
I0731 21:13:05.487761   939 sgd_solver.cpp:136] Iteration 24600, lr = 1e-06, m = 0.9
I0731 21:13:23.963021   939 solver.cpp:353] Iteration 24700 (5.41278 iter/s, 18.4748s/100 iter), loss = 0.0681087
I0731 21:13:23.963121   939 solver.cpp:375]     Train net output #0: loss = 0.0681087 (* 1 = 0.0681087 loss)
I0731 21:13:23.963129   939 sgd_solver.cpp:136] Iteration 24700, lr = 1e-06, m = 0.9
I0731 21:13:40.206693   947 data_reader.cpp:264] Starting prefetch of epoch 15
I0731 21:13:42.409831   939 solver.cpp:353] Iteration 24800 (5.42114 iter/s, 18.4463s/100 iter), loss = 0.101672
I0731 21:13:42.409857   939 solver.cpp:375]     Train net output #0: loss = 0.101673 (* 1 = 0.101673 loss)
I0731 21:13:42.409862   939 sgd_solver.cpp:136] Iteration 24800, lr = 1e-06, m = 0.9
I0731 21:14:00.982034   939 solver.cpp:353] Iteration 24900 (5.38454 iter/s, 18.5717s/100 iter), loss = 0.063454
I0731 21:14:00.982142   939 solver.cpp:375]     Train net output #0: loss = 0.0634541 (* 1 = 0.0634541 loss)
I0731 21:14:00.982149   939 sgd_solver.cpp:136] Iteration 24900, lr = 1e-06, m = 0.9
I0731 21:14:11.018896   943 data_reader.cpp:264] Starting prefetch of epoch 15
I0731 21:14:19.797041   939 solver.cpp:353] Iteration 25000 (5.31505 iter/s, 18.8145s/100 iter), loss = 0.0726288
I0731 21:14:19.797067   939 solver.cpp:375]     Train net output #0: loss = 0.0726288 (* 1 = 0.0726288 loss)
I0731 21:14:19.797072   939 sgd_solver.cpp:136] Iteration 25000, lr = 1e-06, m = 0.9
I0731 21:14:39.881033   939 solver.cpp:353] Iteration 25100 (4.97923 iter/s, 20.0834s/100 iter), loss = 0.0593327
I0731 21:14:39.881150   939 solver.cpp:375]     Train net output #0: loss = 0.0593327 (* 1 = 0.0593327 loss)
I0731 21:14:39.881156   939 sgd_solver.cpp:136] Iteration 25100, lr = 1e-06, m = 0.9
I0731 21:14:59.970604   939 solver.cpp:353] Iteration 25200 (4.97784 iter/s, 20.089s/100 iter), loss = 0.0865271
I0731 21:14:59.970628   939 solver.cpp:375]     Train net output #0: loss = 0.0865271 (* 1 = 0.0865271 loss)
I0731 21:14:59.970633   939 sgd_solver.cpp:136] Iteration 25200, lr = 1e-06, m = 0.9
I0731 21:15:15.667268   947 data_reader.cpp:264] Starting prefetch of epoch 16
I0731 21:15:18.563088   939 solver.cpp:353] Iteration 25300 (5.37867 iter/s, 18.592s/100 iter), loss = 0.0835865
I0731 21:15:18.563112   939 solver.cpp:375]     Train net output #0: loss = 0.0835865 (* 1 = 0.0835865 loss)
I0731 21:15:18.563117   939 sgd_solver.cpp:136] Iteration 25300, lr = 1e-06, m = 0.9
I0731 21:15:37.133354   939 solver.cpp:353] Iteration 25400 (5.3851 iter/s, 18.5698s/100 iter), loss = 0.0782822
I0731 21:15:37.133378   939 solver.cpp:375]     Train net output #0: loss = 0.0782822 (* 1 = 0.0782822 loss)
I0731 21:15:37.133383   939 sgd_solver.cpp:136] Iteration 25400, lr = 1e-06, m = 0.9
I0731 21:15:55.650367   939 solver.cpp:353] Iteration 25500 (5.40059 iter/s, 18.5165s/100 iter), loss = 0.0535741
I0731 21:15:55.650496   939 solver.cpp:375]     Train net output #0: loss = 0.0535741 (* 1 = 0.0535741 loss)
I0731 21:15:55.650503   939 sgd_solver.cpp:136] Iteration 25500, lr = 1e-06, m = 0.9
I0731 21:16:14.129472   939 solver.cpp:353] Iteration 25600 (5.41167 iter/s, 18.4786s/100 iter), loss = 0.0638608
I0731 21:16:14.129501   939 solver.cpp:375]     Train net output #0: loss = 0.0638609 (* 1 = 0.0638609 loss)
I0731 21:16:14.129508   939 sgd_solver.cpp:136] Iteration 25600, lr = 1e-06, m = 0.9
I0731 21:16:16.948148   894 data_reader.cpp:264] Starting prefetch of epoch 13
I0731 21:16:32.617113   939 solver.cpp:353] Iteration 25700 (5.40917 iter/s, 18.4871s/100 iter), loss = 0.0458184
I0731 21:16:32.617167   939 solver.cpp:375]     Train net output #0: loss = 0.0458185 (* 1 = 0.0458185 loss)
I0731 21:16:32.617173   939 sgd_solver.cpp:136] Iteration 25700, lr = 1e-06, m = 0.9
I0731 21:16:47.459816   892 data_reader.cpp:264] Starting prefetch of epoch 16
I0731 21:16:51.093902   939 solver.cpp:353] Iteration 25800 (5.41234 iter/s, 18.4763s/100 iter), loss = 0.0698513
I0731 21:16:51.093927   939 solver.cpp:375]     Train net output #0: loss = 0.0698514 (* 1 = 0.0698514 loss)
I0731 21:16:51.093933   939 sgd_solver.cpp:136] Iteration 25800, lr = 1e-06, m = 0.9
I0731 21:17:09.550305   939 solver.cpp:353] Iteration 25900 (5.41832 iter/s, 18.4559s/100 iter), loss = 0.0549739
I0731 21:17:09.550355   939 solver.cpp:375]     Train net output #0: loss = 0.054974 (* 1 = 0.054974 loss)
I0731 21:17:09.550361   939 sgd_solver.cpp:136] Iteration 25900, lr = 1e-06, m = 0.9
I0731 21:17:27.833823   939 solver.cpp:550] Iteration 26000, Testing net (#0)
I0731 21:17:34.967453   971 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 21:17:39.045761   939 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.952092
I0731 21:17:39.045781   939 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999373
I0731 21:17:39.045788   939 solver.cpp:635]     Test net output #2: loss = 0.194979 (* 1 = 0.194979 loss)
I0731 21:17:39.045878   939 solver.cpp:305] [MultiGPU] Tests completed in 11.2117s
I0731 21:17:39.248468   939 solver.cpp:353] Iteration 26000 (3.3673 iter/s, 29.6974s/100 iter), loss = 0.0848278
I0731 21:17:39.248492   939 solver.cpp:375]     Train net output #0: loss = 0.0848278 (* 1 = 0.0848278 loss)
I0731 21:17:39.248495   939 sgd_solver.cpp:136] Iteration 26000, lr = 1e-06, m = 0.9
I0731 21:17:57.709962   939 solver.cpp:353] Iteration 26100 (5.41683 iter/s, 18.461s/100 iter), loss = 0.0440621
I0731 21:17:57.710072   939 solver.cpp:375]     Train net output #0: loss = 0.0440622 (* 1 = 0.0440622 loss)
I0731 21:17:57.710079   939 sgd_solver.cpp:136] Iteration 26100, lr = 1e-06, m = 0.9
I0731 21:18:16.163537   939 solver.cpp:353] Iteration 26200 (5.41915 iter/s, 18.4531s/100 iter), loss = 0.0912828
I0731 21:18:16.163560   939 solver.cpp:375]     Train net output #0: loss = 0.0912829 (* 1 = 0.0912829 loss)
I0731 21:18:16.163564   939 sgd_solver.cpp:136] Iteration 26200, lr = 1e-06, m = 0.9
I0731 21:18:30.247458   946 data_reader.cpp:264] Starting prefetch of epoch 17
I0731 21:18:34.655086   939 solver.cpp:353] Iteration 26300 (5.40803 iter/s, 18.491s/100 iter), loss = 0.0777734
I0731 21:18:34.655108   939 solver.cpp:375]     Train net output #0: loss = 0.0777734 (* 1 = 0.0777734 loss)
I0731 21:18:34.655113   939 sgd_solver.cpp:136] Iteration 26300, lr = 1e-06, m = 0.9
I0731 21:18:53.174218   939 solver.cpp:353] Iteration 26400 (5.39997 iter/s, 18.5186s/100 iter), loss = 0.0900482
I0731 21:18:53.174242   939 solver.cpp:375]     Train net output #0: loss = 0.0900483 (* 1 = 0.0900483 loss)
I0731 21:18:53.174245   939 sgd_solver.cpp:136] Iteration 26400, lr = 1e-06, m = 0.9
I0731 21:19:00.780333   942 data_reader.cpp:264] Starting prefetch of epoch 19
I0731 21:19:11.697208   939 solver.cpp:353] Iteration 26500 (5.39884 iter/s, 18.5225s/100 iter), loss = 0.0465642
I0731 21:19:11.697234   939 solver.cpp:375]     Train net output #0: loss = 0.0465643 (* 1 = 0.0465643 loss)
I0731 21:19:11.697239   939 sgd_solver.cpp:136] Iteration 26500, lr = 1e-06, m = 0.9
I0731 21:19:30.300449   939 solver.cpp:353] Iteration 26600 (5.37556 iter/s, 18.6027s/100 iter), loss = 0.0779839
I0731 21:19:30.300478   939 solver.cpp:375]     Train net output #0: loss = 0.077984 (* 1 = 0.077984 loss)
I0731 21:19:30.300485   939 sgd_solver.cpp:136] Iteration 26600, lr = 1e-06, m = 0.9
I0731 21:19:48.860736   939 solver.cpp:353] Iteration 26700 (5.38799 iter/s, 18.5598s/100 iter), loss = 0.0678253
I0731 21:19:48.860795   939 solver.cpp:375]     Train net output #0: loss = 0.0678253 (* 1 = 0.0678253 loss)
I0731 21:19:48.860801   939 sgd_solver.cpp:136] Iteration 26700, lr = 1e-06, m = 0.9
I0731 21:20:02.187693   894 data_reader.cpp:264] Starting prefetch of epoch 14
I0731 21:20:07.479032   939 solver.cpp:353] Iteration 26800 (5.37121 iter/s, 18.6178s/100 iter), loss = 0.0836179
I0731 21:20:07.479053   939 solver.cpp:375]     Train net output #0: loss = 0.083618 (* 1 = 0.083618 loss)
I0731 21:20:07.479058   939 sgd_solver.cpp:136] Iteration 26800, lr = 1e-06, m = 0.9
I0731 21:20:26.180583   939 solver.cpp:353] Iteration 26900 (5.3473 iter/s, 18.701s/100 iter), loss = 0.0544827
I0731 21:20:26.180693   939 solver.cpp:375]     Train net output #0: loss = 0.0544828 (* 1 = 0.0544828 loss)
I0731 21:20:26.180701   939 sgd_solver.cpp:136] Iteration 26900, lr = 1e-06, m = 0.9
I0731 21:20:44.663292   939 solver.cpp:353] Iteration 27000 (5.41061 iter/s, 18.4822s/100 iter), loss = 0.108636
I0731 21:20:44.663319   939 solver.cpp:375]     Train net output #0: loss = 0.108637 (* 1 = 0.108637 loss)
I0731 21:20:44.663326   939 sgd_solver.cpp:136] Iteration 27000, lr = 1e-06, m = 0.9
I0731 21:21:03.217550   939 solver.cpp:353] Iteration 27100 (5.38975 iter/s, 18.5538s/100 iter), loss = 0.0650538
I0731 21:21:03.217602   939 solver.cpp:375]     Train net output #0: loss = 0.0650539 (* 1 = 0.0650539 loss)
I0731 21:21:03.217607   939 sgd_solver.cpp:136] Iteration 27100, lr = 1e-06, m = 0.9
I0731 21:21:03.594910   947 data_reader.cpp:264] Starting prefetch of epoch 17
I0731 21:21:21.725080   939 solver.cpp:353] Iteration 27200 (5.40335 iter/s, 18.507s/100 iter), loss = 0.0440861
I0731 21:21:21.725105   939 solver.cpp:375]     Train net output #0: loss = 0.0440861 (* 1 = 0.0440861 loss)
I0731 21:21:21.725108   939 sgd_solver.cpp:136] Iteration 27200, lr = 1e-06, m = 0.9
I0731 21:21:34.155483   942 data_reader.cpp:264] Starting prefetch of epoch 20
I0731 21:21:40.129714   939 solver.cpp:353] Iteration 27300 (5.43356 iter/s, 18.4041s/100 iter), loss = 0.065299
I0731 21:21:40.129737   939 solver.cpp:375]     Train net output #0: loss = 0.065299 (* 1 = 0.065299 loss)
I0731 21:21:40.129742   939 sgd_solver.cpp:136] Iteration 27300, lr = 1e-06, m = 0.9
I0731 21:21:58.777196   939 solver.cpp:353] Iteration 27400 (5.3628 iter/s, 18.647s/100 iter), loss = 0.105187
I0731 21:21:58.777225   939 solver.cpp:375]     Train net output #0: loss = 0.105188 (* 1 = 0.105188 loss)
I0731 21:21:58.777232   939 sgd_solver.cpp:136] Iteration 27400, lr = 1e-06, m = 0.9
I0731 21:22:17.395074   939 solver.cpp:353] Iteration 27500 (5.37133 iter/s, 18.6174s/100 iter), loss = 0.0597829
I0731 21:22:17.395170   939 solver.cpp:375]     Train net output #0: loss = 0.059783 (* 1 = 0.059783 loss)
I0731 21:22:17.395176   939 sgd_solver.cpp:136] Iteration 27500, lr = 1e-06, m = 0.9
I0731 21:22:35.555429   946 data_reader.cpp:264] Starting prefetch of epoch 18
I0731 21:22:35.898489   939 solver.cpp:353] Iteration 27600 (5.40456 iter/s, 18.5029s/100 iter), loss = 0.0826088
I0731 21:22:35.898514   939 solver.cpp:375]     Train net output #0: loss = 0.0826088 (* 1 = 0.0826088 loss)
I0731 21:22:35.898519   939 sgd_solver.cpp:136] Iteration 27600, lr = 1e-06, m = 0.9
I0731 21:22:54.458612   939 solver.cpp:353] Iteration 27700 (5.38804 iter/s, 18.5596s/100 iter), loss = 0.0817283
I0731 21:22:54.458691   939 solver.cpp:375]     Train net output #0: loss = 0.0817284 (* 1 = 0.0817284 loss)
I0731 21:22:54.458698   939 sgd_solver.cpp:136] Iteration 27700, lr = 1e-06, m = 0.9
I0731 21:23:13.043653   939 solver.cpp:353] Iteration 27800 (5.38082 iter/s, 18.5845s/100 iter), loss = 0.0427709
I0731 21:23:13.043680   939 solver.cpp:375]     Train net output #0: loss = 0.0427709 (* 1 = 0.0427709 loss)
I0731 21:23:13.043687   939 sgd_solver.cpp:136] Iteration 27800, lr = 1e-06, m = 0.9
I0731 21:23:31.618603   939 solver.cpp:353] Iteration 27900 (5.38374 iter/s, 18.5744s/100 iter), loss = 0.0579976
I0731 21:23:31.618711   939 solver.cpp:375]     Train net output #0: loss = 0.0579976 (* 1 = 0.0579976 loss)
I0731 21:23:31.618718   939 sgd_solver.cpp:136] Iteration 27900, lr = 1e-06, m = 0.9
I0731 21:23:36.867759   947 data_reader.cpp:264] Starting prefetch of epoch 18
I0731 21:23:50.100430   939 solver.cpp:550] Iteration 28000, Testing net (#0)
I0731 21:23:53.488512   971 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 21:24:01.198395   939 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.950973
I0731 21:24:01.198417   939 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999952
I0731 21:24:01.198422   939 solver.cpp:635]     Test net output #2: loss = 0.160489 (* 1 = 0.160489 loss)
I0731 21:24:01.198447   939 solver.cpp:305] [MultiGPU] Tests completed in 11.0977s
I0731 21:24:01.405356   939 solver.cpp:353] Iteration 28000 (3.35729 iter/s, 29.7859s/100 iter), loss = 0.0789846
I0731 21:24:01.405380   939 solver.cpp:375]     Train net output #0: loss = 0.0789846 (* 1 = 0.0789846 loss)
I0731 21:24:01.405385   939 sgd_solver.cpp:136] Iteration 28000, lr = 1e-06, m = 0.9
I0731 21:24:18.812129   946 data_reader.cpp:264] Starting prefetch of epoch 19
I0731 21:24:19.916771   939 solver.cpp:353] Iteration 28100 (5.40222 iter/s, 18.5109s/100 iter), loss = 0.0770678
I0731 21:24:19.916798   939 solver.cpp:375]     Train net output #0: loss = 0.0770678 (* 1 = 0.0770678 loss)
I0731 21:24:19.916803   939 sgd_solver.cpp:136] Iteration 28100, lr = 1e-06, m = 0.9
I0731 21:24:38.463917   939 solver.cpp:353] Iteration 28200 (5.39181 iter/s, 18.5466s/100 iter), loss = 0.0738573
I0731 21:24:38.463939   939 solver.cpp:375]     Train net output #0: loss = 0.0738574 (* 1 = 0.0738574 loss)
I0731 21:24:38.463943   939 sgd_solver.cpp:136] Iteration 28200, lr = 1e-06, m = 0.9
I0731 21:24:58.198614   939 solver.cpp:353] Iteration 28300 (5.06736 iter/s, 19.7341s/100 iter), loss = 0.0477171
I0731 21:24:58.198700   939 solver.cpp:375]     Train net output #0: loss = 0.0477172 (* 1 = 0.0477172 loss)
I0731 21:24:58.198712   939 sgd_solver.cpp:136] Iteration 28300, lr = 1e-06, m = 0.9
I0731 21:25:18.225245   939 solver.cpp:353] Iteration 28400 (4.99349 iter/s, 20.0261s/100 iter), loss = 0.084747
I0731 21:25:18.225270   939 solver.cpp:375]     Train net output #0: loss = 0.084747 (* 1 = 0.084747 loss)
I0731 21:25:18.225275   939 sgd_solver.cpp:136] Iteration 28400, lr = 1e-06, m = 0.9
I0731 21:25:22.903429   947 data_reader.cpp:264] Starting prefetch of epoch 19
I0731 21:25:37.993486   939 solver.cpp:353] Iteration 28500 (5.05876 iter/s, 19.7677s/100 iter), loss = 0.0768826
I0731 21:25:37.993551   939 solver.cpp:375]     Train net output #0: loss = 0.0768827 (* 1 = 0.0768827 loss)
I0731 21:25:37.993558   939 sgd_solver.cpp:136] Iteration 28500, lr = 1e-06, m = 0.9
I0731 21:25:55.370597   942 data_reader.cpp:264] Starting prefetch of epoch 21
I0731 21:25:57.393331   939 solver.cpp:353] Iteration 28600 (5.15482 iter/s, 19.3993s/100 iter), loss = 0.0897997
I0731 21:25:57.393386   939 solver.cpp:375]     Train net output #0: loss = 0.0897998 (* 1 = 0.0897998 loss)
I0731 21:25:57.393400   939 sgd_solver.cpp:136] Iteration 28600, lr = 1e-06, m = 0.9
I0731 21:26:17.163039   939 solver.cpp:353] Iteration 28700 (5.05838 iter/s, 19.7692s/100 iter), loss = 0.0854878
I0731 21:26:17.163107   939 solver.cpp:375]     Train net output #0: loss = 0.0854879 (* 1 = 0.0854879 loss)
I0731 21:26:17.163113   939 sgd_solver.cpp:136] Iteration 28700, lr = 1e-06, m = 0.9
I0731 21:26:37.184747   939 solver.cpp:353] Iteration 28800 (4.99472 iter/s, 20.0212s/100 iter), loss = 0.0554338
I0731 21:26:37.184772   939 solver.cpp:375]     Train net output #0: loss = 0.0554338 (* 1 = 0.0554338 loss)
I0731 21:26:37.184777   939 sgd_solver.cpp:136] Iteration 28800, lr = 1e-06, m = 0.9
I0731 21:26:56.661568   939 solver.cpp:353] Iteration 28900 (5.13445 iter/s, 19.4763s/100 iter), loss = 0.130536
I0731 21:26:56.661615   939 solver.cpp:375]     Train net output #0: loss = 0.130536 (* 1 = 0.130536 loss)
I0731 21:26:56.661620   939 sgd_solver.cpp:136] Iteration 28900, lr = 1e-06, m = 0.9
I0731 21:27:00.746701   947 data_reader.cpp:264] Starting prefetch of epoch 20
I0731 21:27:16.213573   939 solver.cpp:353] Iteration 29000 (5.11471 iter/s, 19.5515s/100 iter), loss = 0.0780841
I0731 21:27:16.213596   939 solver.cpp:375]     Train net output #0: loss = 0.0780842 (* 1 = 0.0780842 loss)
I0731 21:27:16.213603   939 sgd_solver.cpp:136] Iteration 29000, lr = 1e-06, m = 0.9
I0731 21:27:36.240200   939 solver.cpp:353] Iteration 29100 (4.99349 iter/s, 20.0261s/100 iter), loss = 0.0772361
I0731 21:27:36.240257   939 solver.cpp:375]     Train net output #0: loss = 0.0772362 (* 1 = 0.0772362 loss)
I0731 21:27:36.240262   939 sgd_solver.cpp:136] Iteration 29100, lr = 1e-06, m = 0.9
I0731 21:27:56.177099   939 solver.cpp:353] Iteration 29200 (5.01596 iter/s, 19.9363s/100 iter), loss = 0.0566756
I0731 21:27:56.177331   939 solver.cpp:375]     Train net output #0: loss = 0.0566757 (* 1 = 0.0566757 loss)
I0731 21:27:56.177429   939 sgd_solver.cpp:136] Iteration 29200, lr = 1e-06, m = 0.9
I0731 21:28:05.859292   894 data_reader.cpp:264] Starting prefetch of epoch 15
I0731 21:28:15.283607   939 solver.cpp:353] Iteration 29300 (5.23396 iter/s, 19.106s/100 iter), loss = 0.0556381
I0731 21:28:15.283694   939 solver.cpp:375]     Train net output #0: loss = 0.0556382 (* 1 = 0.0556382 loss)
I0731 21:28:15.283717   939 sgd_solver.cpp:136] Iteration 29300, lr = 1e-06, m = 0.9
I0731 21:28:35.038472   939 solver.cpp:353] Iteration 29400 (5.06218 iter/s, 19.7543s/100 iter), loss = 0.0528643
I0731 21:28:35.038528   939 solver.cpp:375]     Train net output #0: loss = 0.0528644 (* 1 = 0.0528644 loss)
I0731 21:28:35.038555   939 sgd_solver.cpp:136] Iteration 29400, lr = 1e-06, m = 0.9
I0731 21:28:38.132155   892 data_reader.cpp:264] Starting prefetch of epoch 17
I0731 21:28:53.990371   939 solver.cpp:353] Iteration 29500 (5.27666 iter/s, 18.9514s/100 iter), loss = 0.0770322
I0731 21:28:53.990427   939 solver.cpp:375]     Train net output #0: loss = 0.0770323 (* 1 = 0.0770323 loss)
I0731 21:28:53.990432   939 sgd_solver.cpp:136] Iteration 29500, lr = 1e-06, m = 0.9
I0731 21:29:12.433998   939 solver.cpp:353] Iteration 29600 (5.42208 iter/s, 18.4431s/100 iter), loss = 0.0793818
I0731 21:29:12.434023   939 solver.cpp:375]     Train net output #0: loss = 0.079382 (* 1 = 0.079382 loss)
I0731 21:29:12.434027   939 sgd_solver.cpp:136] Iteration 29600, lr = 1e-06, m = 0.9
I0731 21:29:30.965664   939 solver.cpp:353] Iteration 29700 (5.39632 iter/s, 18.5312s/100 iter), loss = 0.0617822
I0731 21:29:30.965718   939 solver.cpp:375]     Train net output #0: loss = 0.0617823 (* 1 = 0.0617823 loss)
I0731 21:29:30.965724   939 sgd_solver.cpp:136] Iteration 29700, lr = 1e-06, m = 0.9
I0731 21:29:39.493511   942 data_reader.cpp:264] Starting prefetch of epoch 22
I0731 21:29:49.460578   939 solver.cpp:353] Iteration 29800 (5.40704 iter/s, 18.4944s/100 iter), loss = 0.131377
I0731 21:29:49.460602   939 solver.cpp:375]     Train net output #0: loss = 0.131377 (* 1 = 0.131377 loss)
I0731 21:29:49.460606   939 sgd_solver.cpp:136] Iteration 29800, lr = 1e-06, m = 0.9
I0731 21:30:07.837761   939 solver.cpp:353] Iteration 29900 (5.44168 iter/s, 18.3767s/100 iter), loss = 0.074149
I0731 21:30:07.837823   939 solver.cpp:375]     Train net output #0: loss = 0.0741491 (* 1 = 0.0741491 loss)
I0731 21:30:07.837831   939 sgd_solver.cpp:136] Iteration 29900, lr = 1e-06, m = 0.9
I0731 21:30:26.306776   939 solver.cpp:680] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/l1reg/cityscapes5_jsegnet21v2_iter_30000.caffemodel
I0731 21:30:26.366845   939 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/l1reg/cityscapes5_jsegnet21v2_iter_30000.solverstate
I0731 21:30:26.377286   939 solver.cpp:550] Iteration 30000, Testing net (#0)
I0731 21:30:33.613379   936 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 21:30:37.803828   939 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.951811
I0731 21:30:37.803851   939 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999342
I0731 21:30:37.803858   939 solver.cpp:635]     Test net output #2: loss = 0.196373 (* 1 = 0.196373 loss)
I0731 21:30:37.803887   939 solver.cpp:305] [MultiGPU] Tests completed in 11.4263s
I0731 21:30:38.016870   939 solver.cpp:353] Iteration 30000 (3.31364 iter/s, 30.1783s/100 iter), loss = 0.0826424
I0731 21:30:38.017313   939 solver.cpp:375]     Train net output #0: loss = 0.0826425 (* 1 = 0.0826425 loss)
I0731 21:30:38.017328   939 sgd_solver.cpp:136] Iteration 30000, lr = 1e-06, m = 0.9
I0731 21:30:56.555650   939 solver.cpp:353] Iteration 30100 (5.39425 iter/s, 18.5383s/100 iter), loss = 0.0634634
I0731 21:30:56.555675   939 solver.cpp:375]     Train net output #0: loss = 0.0634635 (* 1 = 0.0634635 loss)
I0731 21:30:56.555678   939 sgd_solver.cpp:136] Iteration 30100, lr = 1e-06, m = 0.9
I0731 21:31:15.066757   939 solver.cpp:353] Iteration 30200 (5.40231 iter/s, 18.5106s/100 iter), loss = 0.0636802
I0731 21:31:15.066809   939 solver.cpp:375]     Train net output #0: loss = 0.0636803 (* 1 = 0.0636803 loss)
I0731 21:31:15.066814   939 sgd_solver.cpp:136] Iteration 30200, lr = 1e-06, m = 0.9
I0731 21:31:22.925766   947 data_reader.cpp:264] Starting prefetch of epoch 21
I0731 21:31:33.685940   939 solver.cpp:353] Iteration 30300 (5.37095 iter/s, 18.6187s/100 iter), loss = 0.0849341
I0731 21:31:33.685968   939 solver.cpp:375]     Train net output #0: loss = 0.0849342 (* 1 = 0.0849342 loss)
I0731 21:31:33.685976   939 sgd_solver.cpp:136] Iteration 30300, lr = 1e-06, m = 0.9
I0731 21:31:52.189103   939 solver.cpp:353] Iteration 30400 (5.40463 iter/s, 18.5027s/100 iter), loss = 0.073473
I0731 21:31:52.189302   939 solver.cpp:375]     Train net output #0: loss = 0.0734731 (* 1 = 0.0734731 loss)
I0731 21:31:52.189311   939 sgd_solver.cpp:136] Iteration 30400, lr = 1e-06, m = 0.9
I0731 21:31:53.682375   892 data_reader.cpp:264] Starting prefetch of epoch 18
I0731 21:32:10.763114   939 solver.cpp:353] Iteration 30500 (5.38402 iter/s, 18.5735s/100 iter), loss = 0.0796906
I0731 21:32:10.763139   939 solver.cpp:375]     Train net output #0: loss = 0.0796907 (* 1 = 0.0796907 loss)
I0731 21:32:10.763144   939 sgd_solver.cpp:136] Iteration 30500, lr = 1e-06, m = 0.9
I0731 21:32:29.367702   939 solver.cpp:353] Iteration 30600 (5.37517 iter/s, 18.6041s/100 iter), loss = 0.0587438
I0731 21:32:29.367748   939 solver.cpp:375]     Train net output #0: loss = 0.058744 (* 1 = 0.058744 loss)
I0731 21:32:29.367753   939 sgd_solver.cpp:136] Iteration 30600, lr = 1e-06, m = 0.9
I0731 21:32:47.972957   939 solver.cpp:353] Iteration 30700 (5.37497 iter/s, 18.6047s/100 iter), loss = 0.13812
I0731 21:32:47.972982   939 solver.cpp:375]     Train net output #0: loss = 0.13812 (* 1 = 0.13812 loss)
I0731 21:32:47.972987   939 sgd_solver.cpp:136] Iteration 30700, lr = 1e-06, m = 0.9
I0731 21:32:55.137670   946 data_reader.cpp:264] Starting prefetch of epoch 20
I0731 21:33:06.721060   939 solver.cpp:353] Iteration 30800 (5.33402 iter/s, 18.7476s/100 iter), loss = 0.0703356
I0731 21:33:06.721108   939 solver.cpp:375]     Train net output #0: loss = 0.0703357 (* 1 = 0.0703357 loss)
I0731 21:33:06.721113   939 sgd_solver.cpp:136] Iteration 30800, lr = 1e-06, m = 0.9
I0731 21:33:25.082556   939 solver.cpp:353] Iteration 30900 (5.44633 iter/s, 18.361s/100 iter), loss = 0.0714601
I0731 21:33:25.082581   939 solver.cpp:375]     Train net output #0: loss = 0.0714602 (* 1 = 0.0714602 loss)
I0731 21:33:25.082586   939 sgd_solver.cpp:136] Iteration 30900, lr = 1e-06, m = 0.9
I0731 21:33:43.613636   939 solver.cpp:353] Iteration 31000 (5.39649 iter/s, 18.5306s/100 iter), loss = 0.0650416
I0731 21:33:43.613725   939 solver.cpp:375]     Train net output #0: loss = 0.0650417 (* 1 = 0.0650417 loss)
I0731 21:33:43.613732   939 sgd_solver.cpp:136] Iteration 31000, lr = 1e-06, m = 0.9
I0731 21:33:56.446012   946 data_reader.cpp:264] Starting prefetch of epoch 21
I0731 21:34:02.164999   939 solver.cpp:353] Iteration 31100 (5.39059 iter/s, 18.5509s/100 iter), loss = 0.0602407
I0731 21:34:02.165026   939 solver.cpp:375]     Train net output #0: loss = 0.0602408 (* 1 = 0.0602408 loss)
I0731 21:34:02.165032   939 sgd_solver.cpp:136] Iteration 31100, lr = 1e-06, m = 0.9
I0731 21:34:20.862089   939 solver.cpp:353] Iteration 31200 (5.34857 iter/s, 18.6966s/100 iter), loss = 0.113415
I0731 21:34:20.862138   939 solver.cpp:375]     Train net output #0: loss = 0.113415 (* 1 = 0.113415 loss)
I0731 21:34:20.862144   939 sgd_solver.cpp:136] Iteration 31200, lr = 1e-06, m = 0.9
I0731 21:34:39.345957   939 solver.cpp:353] Iteration 31300 (5.41027 iter/s, 18.4834s/100 iter), loss = 0.0949569
I0731 21:34:39.345981   939 solver.cpp:375]     Train net output #0: loss = 0.094957 (* 1 = 0.094957 loss)
I0731 21:34:39.345986   939 sgd_solver.cpp:136] Iteration 31300, lr = 1e-06, m = 0.9
I0731 21:34:57.799237   894 data_reader.cpp:264] Starting prefetch of epoch 16
I0731 21:34:57.967116   939 solver.cpp:353] Iteration 31400 (5.37038 iter/s, 18.6206s/100 iter), loss = 0.0605311
I0731 21:34:57.967141   939 solver.cpp:375]     Train net output #0: loss = 0.0605312 (* 1 = 0.0605312 loss)
I0731 21:34:57.967145   939 sgd_solver.cpp:136] Iteration 31400, lr = 1e-06, m = 0.9
I0731 21:35:16.556733   939 solver.cpp:353] Iteration 31500 (5.3795 iter/s, 18.5891s/100 iter), loss = 0.0635378
I0731 21:35:16.556757   939 solver.cpp:375]     Train net output #0: loss = 0.0635379 (* 1 = 0.0635379 loss)
I0731 21:35:16.556761   939 sgd_solver.cpp:136] Iteration 31500, lr = 1e-06, m = 0.9
I0731 21:35:28.561239   946 data_reader.cpp:264] Starting prefetch of epoch 22
I0731 21:35:35.036737   939 solver.cpp:353] Iteration 31600 (5.4114 iter/s, 18.4795s/100 iter), loss = 0.136079
I0731 21:35:35.036761   939 solver.cpp:375]     Train net output #0: loss = 0.136079 (* 1 = 0.136079 loss)
I0731 21:35:35.036765   939 sgd_solver.cpp:136] Iteration 31600, lr = 1e-06, m = 0.9
I0731 21:35:53.457433   939 solver.cpp:353] Iteration 31700 (5.42883 iter/s, 18.4202s/100 iter), loss = 0.0829734
I0731 21:35:53.457463   939 solver.cpp:375]     Train net output #0: loss = 0.0829735 (* 1 = 0.0829735 loss)
I0731 21:35:53.457468   939 sgd_solver.cpp:136] Iteration 31700, lr = 1e-06, m = 0.9
I0731 21:36:12.067144   939 solver.cpp:353] Iteration 31800 (5.37369 iter/s, 18.6092s/100 iter), loss = 0.0745872
I0731 21:36:12.067198   939 solver.cpp:375]     Train net output #0: loss = 0.0745873 (* 1 = 0.0745873 loss)
I0731 21:36:12.067204   939 sgd_solver.cpp:136] Iteration 31800, lr = 1e-06, m = 0.9
I0731 21:36:29.661536   943 data_reader.cpp:264] Starting prefetch of epoch 16
I0731 21:36:30.541697   939 solver.cpp:353] Iteration 31900 (5.413 iter/s, 18.474s/100 iter), loss = 0.0456669
I0731 21:36:30.541723   939 solver.cpp:375]     Train net output #0: loss = 0.045667 (* 1 = 0.045667 loss)
I0731 21:36:30.541730   939 sgd_solver.cpp:136] Iteration 31900, lr = 1e-06, m = 0.9
I0731 21:36:49.033942   939 solver.cpp:353] Iteration 31999 (5.35374 iter/s, 18.4917s/99 iter), loss = 0.0634366
I0731 21:36:49.033993   939 solver.cpp:375]     Train net output #0: loss = 0.0634367 (* 1 = 0.0634367 loss)
I0731 21:36:49.109697   939 solver.cpp:680] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/l1reg/cityscapes5_jsegnet21v2_iter_32000.caffemodel
I0731 21:36:49.179056   939 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/l1reg/cityscapes5_jsegnet21v2_iter_32000.solverstate
I0731 21:36:49.276705   939 solver.cpp:527] Iteration 32000, loss = 0.0613403
I0731 21:36:49.276733   939 solver.cpp:550] Iteration 32000, Testing net (#0)
I0731 21:36:59.885489   971 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 21:37:00.562300   939 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.950704
I0731 21:37:00.562320   939 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999972
I0731 21:37:00.562325   939 solver.cpp:635]     Test net output #2: loss = 0.161054 (* 1 = 0.161054 loss)
I0731 21:37:00.633184   830 parallel.cpp:73] Root Solver performance on device 0: 5.203 * 6 = 31.22 img/sec (32000 itr in 6150 sec)
I0731 21:37:00.633242   830 parallel.cpp:78]      Solver performance on device 1: 5.203 * 6 = 31.22 img/sec (32000 itr in 6150 sec)
I0731 21:37:00.633262   830 parallel.cpp:78]      Solver performance on device 2: 5.203 * 6 = 31.22 img/sec (32000 itr in 6150 sec)
I0731 21:37:00.633270   830 parallel.cpp:81] Overall multi-GPU performance: 93.656 img/sec
I0731 21:37:01.845003   830 caffe.cpp:247] Optimization Done in 1h 42m 48s
