I0815 20:55:04.030964 20241 caffe.cpp:608] This is NVCaffe 0.16.3 started at Tue Aug 15 20:55:03 2017
I0815 20:55:04.032111 20241 caffe.cpp:611] CuDNN version: 6021
I0815 20:55:04.032115 20241 caffe.cpp:612] CuBLAS version: 8000
I0815 20:55:04.032117 20241 caffe.cpp:613] CUDA version: 8000
I0815 20:55:04.032119 20241 caffe.cpp:614] CUDA driver version: 8000
I0815 20:55:04.321586 20241 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0815 20:55:04.322156 20241 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I0815 20:55:04.322679 20241 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[1]: total=8508145664 free=8379236352
I0815 20:55:04.323196 20241 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[2]: total=8508145664 free=8379236352
I0815 20:55:04.323205 20241 caffe.cpp:208] Using GPUs 0, 1, 2
I0815 20:55:04.323529 20241 caffe.cpp:213] GPU 0: GeForce GTX 1080
I0815 20:55:04.323855 20241 caffe.cpp:213] GPU 1: GeForce GTX 1080
I0815 20:55:04.324185 20241 caffe.cpp:213] GPU 2: GeForce GTX 1080
I0815 20:55:04.331199 20241 solver.cpp:42] Solver data type: FLOAT
I0815 20:55:04.331264 20241 solver.cpp:45] Initializing solver from parameters: 
train_net: "training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/l1reg/train.prototxt"
test_net: "training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/l1reg/test.prototxt"
test_iter: 125
test_interval: 2000
base_lr: 1e-05
display: 100
max_iter: 32000
lr_policy: "multistep"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 1e-05
snapshot: 10000
snapshot_prefix: "training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/l1reg/cityscapes5_jsegnet21v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
regularization_type: "L1"
test_initialization: false
stepvalue: 24000
iter_size: 1
type: "Adam"
I0815 20:55:04.347368 20241 solver.cpp:77] Creating training net from train_net file: training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/l1reg/train.prototxt
I0815 20:55:04.358194 20241 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0815 20:55:04.358220 20241 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
W0815 20:55:04.358280 20241 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0815 20:55:04.359346 20241 net.cpp:72] Initializing net from parameters: 
name: "jsegnet21v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/train-image-lmdb"
    label_list_path: "data/train-label-lmdb"
    batch_size: 6
    shuffle: false
    threads: 1
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
I0815 20:55:04.359563 20241 net.cpp:104] Using FLOAT as default forward math type
I0815 20:55:04.359570 20241 net.cpp:110] Using FLOAT as default backward math type
I0815 20:55:04.359573 20241 layer_factory.hpp:136] Creating layer 'data' of type 'ImageLabelData'
I0815 20:55:04.359578 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:04.359592 20241 net.cpp:184] Created Layer data (0)
I0815 20:55:04.359597 20241 net.cpp:530] data -> data
I0815 20:55:04.359611 20241 net.cpp:530] data -> label
I0815 20:55:04.359923 20241 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0815 20:55:04.359943 20241 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 20:55:04.393926 20314 db_lmdb.cpp:24] Opened lmdb data/train-image-lmdb
I0815 20:55:04.397159 20241 data_layer.cpp:185] [0] ReshapePrefetch 6, 3, 640, 640
I0815 20:55:04.397229 20241 data_layer.cpp:209] [0] Output data size: 6, 3, 640, 640
I0815 20:55:04.397239 20241 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 20:55:04.397348 20241 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0815 20:55:04.397359 20241 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 20:55:04.398102 20315 data_layer.cpp:97] [0] Parser threads: 1
I0815 20:55:04.398111 20315 data_layer.cpp:99] [0] Transformer threads: 1
I0815 20:55:04.403543 20316 db_lmdb.cpp:24] Opened lmdb data/train-label-lmdb
I0815 20:55:04.404803 20241 data_layer.cpp:185] [0] ReshapePrefetch 6, 1, 640, 640
I0815 20:55:04.404897 20241 data_layer.cpp:209] [0] Output data size: 6, 1, 640, 640
I0815 20:55:04.404907 20241 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 20:55:04.404999 20241 net.cpp:245] Setting up data
I0815 20:55:04.405017 20241 net.cpp:252] TRAIN Top shape for layer 0 'data' 6 3 640 640 (7372800)
I0815 20:55:04.405026 20241 net.cpp:252] TRAIN Top shape for layer 0 'data' 6 1 640 640 (2457600)
I0815 20:55:04.405037 20241 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0815 20:55:04.405046 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:04.405081 20241 net.cpp:184] Created Layer data/bias (1)
I0815 20:55:04.405087 20241 net.cpp:561] data/bias <- data
I0815 20:55:04.405100 20241 net.cpp:530] data/bias -> data/bias
I0815 20:55:04.406790 20317 data_layer.cpp:97] [0] Parser threads: 1
I0815 20:55:04.406826 20317 data_layer.cpp:99] [0] Transformer threads: 1
I0815 20:55:04.411610 20241 net.cpp:245] Setting up data/bias
I0815 20:55:04.411761 20241 net.cpp:252] TRAIN Top shape for layer 1 'data/bias' 6 3 640 640 (7372800)
I0815 20:55:04.411801 20241 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0815 20:55:04.411835 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:04.411950 20241 net.cpp:184] Created Layer conv1a (2)
I0815 20:55:04.411967 20241 net.cpp:561] conv1a <- data/bias
I0815 20:55:04.411984 20241 net.cpp:530] conv1a -> conv1a
I0815 20:55:05.081058 20241 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 0  (limit 7.9G, req 0G)
I0815 20:55:05.081075 20241 net.cpp:245] Setting up conv1a
I0815 20:55:05.081081 20241 net.cpp:252] TRAIN Top shape for layer 2 'conv1a' 6 32 320 320 (19660800)
I0815 20:55:05.081090 20241 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0815 20:55:05.081094 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.081104 20241 net.cpp:184] Created Layer conv1a/bn (3)
I0815 20:55:05.081109 20241 net.cpp:561] conv1a/bn <- conv1a
I0815 20:55:05.081112 20241 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0815 20:55:05.081791 20241 net.cpp:245] Setting up conv1a/bn
I0815 20:55:05.081800 20241 net.cpp:252] TRAIN Top shape for layer 3 'conv1a/bn' 6 32 320 320 (19660800)
I0815 20:55:05.081809 20241 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0815 20:55:05.081811 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.081816 20241 net.cpp:184] Created Layer conv1a/relu (4)
I0815 20:55:05.081820 20241 net.cpp:561] conv1a/relu <- conv1a
I0815 20:55:05.081822 20241 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0815 20:55:05.081835 20241 net.cpp:245] Setting up conv1a/relu
I0815 20:55:05.081840 20241 net.cpp:252] TRAIN Top shape for layer 4 'conv1a/relu' 6 32 320 320 (19660800)
I0815 20:55:05.081841 20241 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0815 20:55:05.081843 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.081854 20241 net.cpp:184] Created Layer conv1b (5)
I0815 20:55:05.081857 20241 net.cpp:561] conv1b <- conv1a
I0815 20:55:05.081861 20241 net.cpp:530] conv1b -> conv1b
I0815 20:55:05.127974 20241 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.73G, req 0G)
I0815 20:55:05.127986 20241 net.cpp:245] Setting up conv1b
I0815 20:55:05.127990 20241 net.cpp:252] TRAIN Top shape for layer 5 'conv1b' 6 32 320 320 (19660800)
I0815 20:55:05.127996 20241 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0815 20:55:05.128000 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.128005 20241 net.cpp:184] Created Layer conv1b/bn (6)
I0815 20:55:05.128007 20241 net.cpp:561] conv1b/bn <- conv1b
I0815 20:55:05.128010 20241 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0815 20:55:05.128664 20241 net.cpp:245] Setting up conv1b/bn
I0815 20:55:05.128672 20241 net.cpp:252] TRAIN Top shape for layer 6 'conv1b/bn' 6 32 320 320 (19660800)
I0815 20:55:05.128679 20241 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0815 20:55:05.128681 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.128689 20241 net.cpp:184] Created Layer conv1b/relu (7)
I0815 20:55:05.128691 20241 net.cpp:561] conv1b/relu <- conv1b
I0815 20:55:05.128693 20241 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0815 20:55:05.128697 20241 net.cpp:245] Setting up conv1b/relu
I0815 20:55:05.128700 20241 net.cpp:252] TRAIN Top shape for layer 7 'conv1b/relu' 6 32 320 320 (19660800)
I0815 20:55:05.128702 20241 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0815 20:55:05.128706 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.128711 20241 net.cpp:184] Created Layer pool1 (8)
I0815 20:55:05.128715 20241 net.cpp:561] pool1 <- conv1b
I0815 20:55:05.128716 20241 net.cpp:530] pool1 -> pool1
I0815 20:55:05.128785 20241 net.cpp:245] Setting up pool1
I0815 20:55:05.128790 20241 net.cpp:252] TRAIN Top shape for layer 8 'pool1' 6 32 160 160 (4915200)
I0815 20:55:05.128793 20241 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0815 20:55:05.128804 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.128811 20241 net.cpp:184] Created Layer res2a_branch2a (9)
I0815 20:55:05.128814 20241 net.cpp:561] res2a_branch2a <- pool1
I0815 20:55:05.128818 20241 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0815 20:55:05.168486 20241 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.61G, req 0G)
I0815 20:55:05.168499 20241 net.cpp:245] Setting up res2a_branch2a
I0815 20:55:05.168504 20241 net.cpp:252] TRAIN Top shape for layer 9 'res2a_branch2a' 6 64 160 160 (9830400)
I0815 20:55:05.168510 20241 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0815 20:55:05.168514 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.168519 20241 net.cpp:184] Created Layer res2a_branch2a/bn (10)
I0815 20:55:05.168522 20241 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0815 20:55:05.168525 20241 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0815 20:55:05.169811 20241 net.cpp:245] Setting up res2a_branch2a/bn
I0815 20:55:05.169819 20241 net.cpp:252] TRAIN Top shape for layer 10 'res2a_branch2a/bn' 6 64 160 160 (9830400)
I0815 20:55:05.169826 20241 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0815 20:55:05.169829 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.169833 20241 net.cpp:184] Created Layer res2a_branch2a/relu (11)
I0815 20:55:05.169836 20241 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0815 20:55:05.169838 20241 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0815 20:55:05.169842 20241 net.cpp:245] Setting up res2a_branch2a/relu
I0815 20:55:05.169845 20241 net.cpp:252] TRAIN Top shape for layer 11 'res2a_branch2a/relu' 6 64 160 160 (9830400)
I0815 20:55:05.169848 20241 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0815 20:55:05.169850 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.169857 20241 net.cpp:184] Created Layer res2a_branch2b (12)
I0815 20:55:05.169859 20241 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0815 20:55:05.169862 20241 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0815 20:55:05.190790 20241 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.52G, req 0G)
I0815 20:55:05.190814 20241 net.cpp:245] Setting up res2a_branch2b
I0815 20:55:05.190819 20241 net.cpp:252] TRAIN Top shape for layer 12 'res2a_branch2b' 6 64 160 160 (9830400)
I0815 20:55:05.190827 20241 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0815 20:55:05.190832 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.190842 20241 net.cpp:184] Created Layer res2a_branch2b/bn (13)
I0815 20:55:05.190846 20241 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0815 20:55:05.190850 20241 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0815 20:55:05.191596 20241 net.cpp:245] Setting up res2a_branch2b/bn
I0815 20:55:05.191604 20241 net.cpp:252] TRAIN Top shape for layer 13 'res2a_branch2b/bn' 6 64 160 160 (9830400)
I0815 20:55:05.191612 20241 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0815 20:55:05.191615 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.191619 20241 net.cpp:184] Created Layer res2a_branch2b/relu (14)
I0815 20:55:05.191622 20241 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0815 20:55:05.191624 20241 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0815 20:55:05.191629 20241 net.cpp:245] Setting up res2a_branch2b/relu
I0815 20:55:05.191632 20241 net.cpp:252] TRAIN Top shape for layer 14 'res2a_branch2b/relu' 6 64 160 160 (9830400)
I0815 20:55:05.191646 20241 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0815 20:55:05.191650 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.191656 20241 net.cpp:184] Created Layer pool2 (15)
I0815 20:55:05.191658 20241 net.cpp:561] pool2 <- res2a_branch2b
I0815 20:55:05.191661 20241 net.cpp:530] pool2 -> pool2
I0815 20:55:05.191725 20241 net.cpp:245] Setting up pool2
I0815 20:55:05.191730 20241 net.cpp:252] TRAIN Top shape for layer 15 'pool2' 6 64 80 80 (2457600)
I0815 20:55:05.191732 20241 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0815 20:55:05.191735 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.191743 20241 net.cpp:184] Created Layer res3a_branch2a (16)
I0815 20:55:05.191746 20241 net.cpp:561] res3a_branch2a <- pool2
I0815 20:55:05.191748 20241 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0815 20:55:05.212394 20241 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 7.46G, req 0G)
I0815 20:55:05.212412 20241 net.cpp:245] Setting up res3a_branch2a
I0815 20:55:05.212419 20241 net.cpp:252] TRAIN Top shape for layer 16 'res3a_branch2a' 6 128 80 80 (4915200)
I0815 20:55:05.212426 20241 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0815 20:55:05.212431 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.212440 20241 net.cpp:184] Created Layer res3a_branch2a/bn (17)
I0815 20:55:05.212443 20241 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0815 20:55:05.212446 20241 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0815 20:55:05.213122 20241 net.cpp:245] Setting up res3a_branch2a/bn
I0815 20:55:05.213130 20241 net.cpp:252] TRAIN Top shape for layer 17 'res3a_branch2a/bn' 6 128 80 80 (4915200)
I0815 20:55:05.213140 20241 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0815 20:55:05.213143 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.213147 20241 net.cpp:184] Created Layer res3a_branch2a/relu (18)
I0815 20:55:05.213150 20241 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0815 20:55:05.213152 20241 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0815 20:55:05.213156 20241 net.cpp:245] Setting up res3a_branch2a/relu
I0815 20:55:05.213160 20241 net.cpp:252] TRAIN Top shape for layer 18 'res3a_branch2a/relu' 6 128 80 80 (4915200)
I0815 20:55:05.213161 20241 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0815 20:55:05.213165 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.213171 20241 net.cpp:184] Created Layer res3a_branch2b (19)
I0815 20:55:05.213174 20241 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0815 20:55:05.213177 20241 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0815 20:55:05.227406 20241 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.42G, req 0G)
I0815 20:55:05.227429 20241 net.cpp:245] Setting up res3a_branch2b
I0815 20:55:05.227435 20241 net.cpp:252] TRAIN Top shape for layer 19 'res3a_branch2b' 6 128 80 80 (4915200)
I0815 20:55:05.227442 20241 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0815 20:55:05.227447 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.227458 20241 net.cpp:184] Created Layer res3a_branch2b/bn (20)
I0815 20:55:05.227460 20241 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0815 20:55:05.227464 20241 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0815 20:55:05.228173 20241 net.cpp:245] Setting up res3a_branch2b/bn
I0815 20:55:05.228181 20241 net.cpp:252] TRAIN Top shape for layer 20 'res3a_branch2b/bn' 6 128 80 80 (4915200)
I0815 20:55:05.228189 20241 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0815 20:55:05.228202 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.228206 20241 net.cpp:184] Created Layer res3a_branch2b/relu (21)
I0815 20:55:05.228209 20241 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0815 20:55:05.228212 20241 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0815 20:55:05.228217 20241 net.cpp:245] Setting up res3a_branch2b/relu
I0815 20:55:05.228220 20241 net.cpp:252] TRAIN Top shape for layer 21 'res3a_branch2b/relu' 6 128 80 80 (4915200)
I0815 20:55:05.228222 20241 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0815 20:55:05.228225 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.228231 20241 net.cpp:184] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (22)
I0815 20:55:05.228235 20241 net.cpp:561] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0815 20:55:05.228236 20241 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0815 20:55:05.228240 20241 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0815 20:55:05.228281 20241 net.cpp:245] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0815 20:55:05.228284 20241 net.cpp:252] TRAIN Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 6 128 80 80 (4915200)
I0815 20:55:05.228287 20241 net.cpp:252] TRAIN Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 6 128 80 80 (4915200)
I0815 20:55:05.228291 20241 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0815 20:55:05.228292 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.228297 20241 net.cpp:184] Created Layer pool3 (23)
I0815 20:55:05.228301 20241 net.cpp:561] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0815 20:55:05.228302 20241 net.cpp:530] pool3 -> pool3
I0815 20:55:05.228364 20241 net.cpp:245] Setting up pool3
I0815 20:55:05.228368 20241 net.cpp:252] TRAIN Top shape for layer 23 'pool3' 6 128 40 40 (1228800)
I0815 20:55:05.228371 20241 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0815 20:55:05.228374 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.228380 20241 net.cpp:184] Created Layer res4a_branch2a (24)
I0815 20:55:05.228384 20241 net.cpp:561] res4a_branch2a <- pool3
I0815 20:55:05.228386 20241 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0815 20:55:05.253123 20241 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.38G, req 0G)
I0815 20:55:05.253139 20241 net.cpp:245] Setting up res4a_branch2a
I0815 20:55:05.253144 20241 net.cpp:252] TRAIN Top shape for layer 24 'res4a_branch2a' 6 256 40 40 (2457600)
I0815 20:55:05.253150 20241 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0815 20:55:05.253154 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.253163 20241 net.cpp:184] Created Layer res4a_branch2a/bn (25)
I0815 20:55:05.253167 20241 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0815 20:55:05.253170 20241 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0815 20:55:05.253814 20241 net.cpp:245] Setting up res4a_branch2a/bn
I0815 20:55:05.253823 20241 net.cpp:252] TRAIN Top shape for layer 25 'res4a_branch2a/bn' 6 256 40 40 (2457600)
I0815 20:55:05.253828 20241 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0815 20:55:05.253831 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.253834 20241 net.cpp:184] Created Layer res4a_branch2a/relu (26)
I0815 20:55:05.253837 20241 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0815 20:55:05.253840 20241 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0815 20:55:05.253854 20241 net.cpp:245] Setting up res4a_branch2a/relu
I0815 20:55:05.253857 20241 net.cpp:252] TRAIN Top shape for layer 26 'res4a_branch2a/relu' 6 256 40 40 (2457600)
I0815 20:55:05.253859 20241 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0815 20:55:05.253862 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.253870 20241 net.cpp:184] Created Layer res4a_branch2b (27)
I0815 20:55:05.253873 20241 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0815 20:55:05.253876 20241 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0815 20:55:05.262922 20241 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.36G, req 0G)
I0815 20:55:05.262936 20241 net.cpp:245] Setting up res4a_branch2b
I0815 20:55:05.262943 20241 net.cpp:252] TRAIN Top shape for layer 27 'res4a_branch2b' 6 256 40 40 (2457600)
I0815 20:55:05.262948 20241 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0815 20:55:05.262953 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.262959 20241 net.cpp:184] Created Layer res4a_branch2b/bn (28)
I0815 20:55:05.262962 20241 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0815 20:55:05.262965 20241 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0815 20:55:05.263644 20241 net.cpp:245] Setting up res4a_branch2b/bn
I0815 20:55:05.263653 20241 net.cpp:252] TRAIN Top shape for layer 28 'res4a_branch2b/bn' 6 256 40 40 (2457600)
I0815 20:55:05.263659 20241 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0815 20:55:05.263664 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.263669 20241 net.cpp:184] Created Layer res4a_branch2b/relu (29)
I0815 20:55:05.263672 20241 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0815 20:55:05.263676 20241 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0815 20:55:05.263684 20241 net.cpp:245] Setting up res4a_branch2b/relu
I0815 20:55:05.263689 20241 net.cpp:252] TRAIN Top shape for layer 29 'res4a_branch2b/relu' 6 256 40 40 (2457600)
I0815 20:55:05.263692 20241 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0815 20:55:05.263696 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.263703 20241 net.cpp:184] Created Layer pool4 (30)
I0815 20:55:05.263706 20241 net.cpp:561] pool4 <- res4a_branch2b
I0815 20:55:05.263710 20241 net.cpp:530] pool4 -> pool4
I0815 20:55:05.263780 20241 net.cpp:245] Setting up pool4
I0815 20:55:05.263787 20241 net.cpp:252] TRAIN Top shape for layer 30 'pool4' 6 256 40 40 (2457600)
I0815 20:55:05.263792 20241 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0815 20:55:05.263795 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.263808 20241 net.cpp:184] Created Layer res5a_branch2a (31)
I0815 20:55:05.263811 20241 net.cpp:561] res5a_branch2a <- pool4
I0815 20:55:05.263815 20241 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0815 20:55:05.291147 20241 net.cpp:245] Setting up res5a_branch2a
I0815 20:55:05.291167 20241 net.cpp:252] TRAIN Top shape for layer 31 'res5a_branch2a' 6 512 40 40 (4915200)
I0815 20:55:05.291175 20241 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0815 20:55:05.291182 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.291194 20241 net.cpp:184] Created Layer res5a_branch2a/bn (32)
I0815 20:55:05.291198 20241 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0815 20:55:05.291203 20241 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0815 20:55:05.291891 20241 net.cpp:245] Setting up res5a_branch2a/bn
I0815 20:55:05.291904 20241 net.cpp:252] TRAIN Top shape for layer 32 'res5a_branch2a/bn' 6 512 40 40 (4915200)
I0815 20:55:05.291924 20241 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0815 20:55:05.291929 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.291934 20241 net.cpp:184] Created Layer res5a_branch2a/relu (33)
I0815 20:55:05.291937 20241 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0815 20:55:05.291942 20241 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0815 20:55:05.291949 20241 net.cpp:245] Setting up res5a_branch2a/relu
I0815 20:55:05.291954 20241 net.cpp:252] TRAIN Top shape for layer 33 'res5a_branch2a/relu' 6 512 40 40 (4915200)
I0815 20:55:05.291957 20241 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0815 20:55:05.291961 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.291970 20241 net.cpp:184] Created Layer res5a_branch2b (34)
I0815 20:55:05.291975 20241 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0815 20:55:05.291978 20241 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0815 20:55:05.305256 20241 net.cpp:245] Setting up res5a_branch2b
I0815 20:55:05.305279 20241 net.cpp:252] TRAIN Top shape for layer 34 'res5a_branch2b' 6 512 40 40 (4915200)
I0815 20:55:05.305289 20241 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0815 20:55:05.305294 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.305300 20241 net.cpp:184] Created Layer res5a_branch2b/bn (35)
I0815 20:55:05.305304 20241 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0815 20:55:05.305306 20241 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0815 20:55:05.305930 20241 net.cpp:245] Setting up res5a_branch2b/bn
I0815 20:55:05.305938 20241 net.cpp:252] TRAIN Top shape for layer 35 'res5a_branch2b/bn' 6 512 40 40 (4915200)
I0815 20:55:05.305943 20241 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0815 20:55:05.305946 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.305949 20241 net.cpp:184] Created Layer res5a_branch2b/relu (36)
I0815 20:55:05.305951 20241 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0815 20:55:05.305954 20241 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0815 20:55:05.305958 20241 net.cpp:245] Setting up res5a_branch2b/relu
I0815 20:55:05.305960 20241 net.cpp:252] TRAIN Top shape for layer 36 'res5a_branch2b/relu' 6 512 40 40 (4915200)
I0815 20:55:05.305963 20241 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I0815 20:55:05.305964 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.305972 20241 net.cpp:184] Created Layer out5a (37)
I0815 20:55:05.305975 20241 net.cpp:561] out5a <- res5a_branch2b
I0815 20:55:05.305977 20241 net.cpp:530] out5a -> out5a
I0815 20:55:05.310181 20241 net.cpp:245] Setting up out5a
I0815 20:55:05.310191 20241 net.cpp:252] TRAIN Top shape for layer 37 'out5a' 6 64 40 40 (614400)
I0815 20:55:05.310196 20241 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I0815 20:55:05.310199 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.310204 20241 net.cpp:184] Created Layer out5a/bn (38)
I0815 20:55:05.310205 20241 net.cpp:561] out5a/bn <- out5a
I0815 20:55:05.310209 20241 net.cpp:513] out5a/bn -> out5a (in-place)
I0815 20:55:05.310834 20241 net.cpp:245] Setting up out5a/bn
I0815 20:55:05.310842 20241 net.cpp:252] TRAIN Top shape for layer 38 'out5a/bn' 6 64 40 40 (614400)
I0815 20:55:05.310847 20241 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I0815 20:55:05.310849 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.310853 20241 net.cpp:184] Created Layer out5a/relu (39)
I0815 20:55:05.310855 20241 net.cpp:561] out5a/relu <- out5a
I0815 20:55:05.310858 20241 net.cpp:513] out5a/relu -> out5a (in-place)
I0815 20:55:05.310870 20241 net.cpp:245] Setting up out5a/relu
I0815 20:55:05.310873 20241 net.cpp:252] TRAIN Top shape for layer 39 'out5a/relu' 6 64 40 40 (614400)
I0815 20:55:05.310875 20241 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I0815 20:55:05.310878 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.310889 20241 net.cpp:184] Created Layer out5a_up2 (40)
I0815 20:55:05.310892 20241 net.cpp:561] out5a_up2 <- out5a
I0815 20:55:05.310894 20241 net.cpp:530] out5a_up2 -> out5a_up2
I0815 20:55:05.311173 20241 net.cpp:245] Setting up out5a_up2
I0815 20:55:05.311178 20241 net.cpp:252] TRAIN Top shape for layer 40 'out5a_up2' 6 64 80 80 (2457600)
I0815 20:55:05.311182 20241 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I0815 20:55:05.311183 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.311192 20241 net.cpp:184] Created Layer out3a (41)
I0815 20:55:05.311195 20241 net.cpp:561] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0815 20:55:05.311197 20241 net.cpp:530] out3a -> out3a
I0815 20:55:05.322540 20241 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 7.3G, req 0G)
I0815 20:55:05.322556 20241 net.cpp:245] Setting up out3a
I0815 20:55:05.322561 20241 net.cpp:252] TRAIN Top shape for layer 41 'out3a' 6 64 80 80 (2457600)
I0815 20:55:05.322567 20241 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I0815 20:55:05.322571 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.322577 20241 net.cpp:184] Created Layer out3a/bn (42)
I0815 20:55:05.322580 20241 net.cpp:561] out3a/bn <- out3a
I0815 20:55:05.322585 20241 net.cpp:513] out3a/bn -> out3a (in-place)
I0815 20:55:05.323366 20241 net.cpp:245] Setting up out3a/bn
I0815 20:55:05.323374 20241 net.cpp:252] TRAIN Top shape for layer 42 'out3a/bn' 6 64 80 80 (2457600)
I0815 20:55:05.323381 20241 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I0815 20:55:05.323384 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.323387 20241 net.cpp:184] Created Layer out3a/relu (43)
I0815 20:55:05.323390 20241 net.cpp:561] out3a/relu <- out3a
I0815 20:55:05.323391 20241 net.cpp:513] out3a/relu -> out3a (in-place)
I0815 20:55:05.323395 20241 net.cpp:245] Setting up out3a/relu
I0815 20:55:05.323397 20241 net.cpp:252] TRAIN Top shape for layer 43 'out3a/relu' 6 64 80 80 (2457600)
I0815 20:55:05.323400 20241 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I0815 20:55:05.323402 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.323858 20241 net.cpp:184] Created Layer out3_out5_combined (44)
I0815 20:55:05.323863 20241 net.cpp:561] out3_out5_combined <- out5a_up2
I0815 20:55:05.323865 20241 net.cpp:561] out3_out5_combined <- out3a
I0815 20:55:05.323868 20241 net.cpp:530] out3_out5_combined -> out3_out5_combined
I0815 20:55:05.324877 20241 net.cpp:245] Setting up out3_out5_combined
I0815 20:55:05.324885 20241 net.cpp:252] TRAIN Top shape for layer 44 'out3_out5_combined' 6 64 80 80 (2457600)
I0815 20:55:05.324888 20241 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I0815 20:55:05.324892 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.324899 20241 net.cpp:184] Created Layer ctx_conv1 (45)
I0815 20:55:05.324903 20241 net.cpp:561] ctx_conv1 <- out3_out5_combined
I0815 20:55:05.324904 20241 net.cpp:530] ctx_conv1 -> ctx_conv1
I0815 20:55:05.338371 20241 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 7.25G, req 0G)
I0815 20:55:05.338384 20241 net.cpp:245] Setting up ctx_conv1
I0815 20:55:05.338388 20241 net.cpp:252] TRAIN Top shape for layer 45 'ctx_conv1' 6 64 80 80 (2457600)
I0815 20:55:05.338403 20241 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I0815 20:55:05.338407 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.338413 20241 net.cpp:184] Created Layer ctx_conv1/bn (46)
I0815 20:55:05.338415 20241 net.cpp:561] ctx_conv1/bn <- ctx_conv1
I0815 20:55:05.338418 20241 net.cpp:513] ctx_conv1/bn -> ctx_conv1 (in-place)
I0815 20:55:05.339083 20241 net.cpp:245] Setting up ctx_conv1/bn
I0815 20:55:05.339092 20241 net.cpp:252] TRAIN Top shape for layer 46 'ctx_conv1/bn' 6 64 80 80 (2457600)
I0815 20:55:05.339097 20241 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I0815 20:55:05.339099 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.339102 20241 net.cpp:184] Created Layer ctx_conv1/relu (47)
I0815 20:55:05.339104 20241 net.cpp:561] ctx_conv1/relu <- ctx_conv1
I0815 20:55:05.339107 20241 net.cpp:513] ctx_conv1/relu -> ctx_conv1 (in-place)
I0815 20:55:05.339110 20241 net.cpp:245] Setting up ctx_conv1/relu
I0815 20:55:05.339113 20241 net.cpp:252] TRAIN Top shape for layer 47 'ctx_conv1/relu' 6 64 80 80 (2457600)
I0815 20:55:05.339115 20241 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I0815 20:55:05.339118 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.339123 20241 net.cpp:184] Created Layer ctx_conv2 (48)
I0815 20:55:05.339124 20241 net.cpp:561] ctx_conv2 <- ctx_conv1
I0815 20:55:05.339128 20241 net.cpp:530] ctx_conv2 -> ctx_conv2
I0815 20:55:05.340204 20241 net.cpp:245] Setting up ctx_conv2
I0815 20:55:05.340212 20241 net.cpp:252] TRAIN Top shape for layer 48 'ctx_conv2' 6 64 80 80 (2457600)
I0815 20:55:05.340216 20241 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I0815 20:55:05.340219 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.340222 20241 net.cpp:184] Created Layer ctx_conv2/bn (49)
I0815 20:55:05.340224 20241 net.cpp:561] ctx_conv2/bn <- ctx_conv2
I0815 20:55:05.340226 20241 net.cpp:513] ctx_conv2/bn -> ctx_conv2 (in-place)
I0815 20:55:05.340843 20241 net.cpp:245] Setting up ctx_conv2/bn
I0815 20:55:05.340849 20241 net.cpp:252] TRAIN Top shape for layer 49 'ctx_conv2/bn' 6 64 80 80 (2457600)
I0815 20:55:05.340854 20241 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I0815 20:55:05.340857 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.340860 20241 net.cpp:184] Created Layer ctx_conv2/relu (50)
I0815 20:55:05.340862 20241 net.cpp:561] ctx_conv2/relu <- ctx_conv2
I0815 20:55:05.340864 20241 net.cpp:513] ctx_conv2/relu -> ctx_conv2 (in-place)
I0815 20:55:05.340867 20241 net.cpp:245] Setting up ctx_conv2/relu
I0815 20:55:05.340869 20241 net.cpp:252] TRAIN Top shape for layer 50 'ctx_conv2/relu' 6 64 80 80 (2457600)
I0815 20:55:05.340872 20241 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I0815 20:55:05.340873 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.340878 20241 net.cpp:184] Created Layer ctx_conv3 (51)
I0815 20:55:05.340880 20241 net.cpp:561] ctx_conv3 <- ctx_conv2
I0815 20:55:05.340883 20241 net.cpp:530] ctx_conv3 -> ctx_conv3
I0815 20:55:05.341936 20241 net.cpp:245] Setting up ctx_conv3
I0815 20:55:05.341943 20241 net.cpp:252] TRAIN Top shape for layer 51 'ctx_conv3' 6 64 80 80 (2457600)
I0815 20:55:05.341948 20241 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I0815 20:55:05.341949 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.341953 20241 net.cpp:184] Created Layer ctx_conv3/bn (52)
I0815 20:55:05.341955 20241 net.cpp:561] ctx_conv3/bn <- ctx_conv3
I0815 20:55:05.341958 20241 net.cpp:513] ctx_conv3/bn -> ctx_conv3 (in-place)
I0815 20:55:05.342569 20241 net.cpp:245] Setting up ctx_conv3/bn
I0815 20:55:05.342582 20241 net.cpp:252] TRAIN Top shape for layer 52 'ctx_conv3/bn' 6 64 80 80 (2457600)
I0815 20:55:05.342587 20241 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I0815 20:55:05.342589 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.342592 20241 net.cpp:184] Created Layer ctx_conv3/relu (53)
I0815 20:55:05.342594 20241 net.cpp:561] ctx_conv3/relu <- ctx_conv3
I0815 20:55:05.342597 20241 net.cpp:513] ctx_conv3/relu -> ctx_conv3 (in-place)
I0815 20:55:05.342599 20241 net.cpp:245] Setting up ctx_conv3/relu
I0815 20:55:05.342602 20241 net.cpp:252] TRAIN Top shape for layer 53 'ctx_conv3/relu' 6 64 80 80 (2457600)
I0815 20:55:05.342604 20241 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I0815 20:55:05.342607 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.342612 20241 net.cpp:184] Created Layer ctx_conv4 (54)
I0815 20:55:05.342613 20241 net.cpp:561] ctx_conv4 <- ctx_conv3
I0815 20:55:05.342615 20241 net.cpp:530] ctx_conv4 -> ctx_conv4
I0815 20:55:05.343672 20241 net.cpp:245] Setting up ctx_conv4
I0815 20:55:05.343677 20241 net.cpp:252] TRAIN Top shape for layer 54 'ctx_conv4' 6 64 80 80 (2457600)
I0815 20:55:05.343682 20241 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I0815 20:55:05.343683 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.343689 20241 net.cpp:184] Created Layer ctx_conv4/bn (55)
I0815 20:55:05.343691 20241 net.cpp:561] ctx_conv4/bn <- ctx_conv4
I0815 20:55:05.343693 20241 net.cpp:513] ctx_conv4/bn -> ctx_conv4 (in-place)
I0815 20:55:05.344313 20241 net.cpp:245] Setting up ctx_conv4/bn
I0815 20:55:05.344321 20241 net.cpp:252] TRAIN Top shape for layer 55 'ctx_conv4/bn' 6 64 80 80 (2457600)
I0815 20:55:05.344326 20241 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I0815 20:55:05.344327 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.344331 20241 net.cpp:184] Created Layer ctx_conv4/relu (56)
I0815 20:55:05.344332 20241 net.cpp:561] ctx_conv4/relu <- ctx_conv4
I0815 20:55:05.344334 20241 net.cpp:513] ctx_conv4/relu -> ctx_conv4 (in-place)
I0815 20:55:05.344338 20241 net.cpp:245] Setting up ctx_conv4/relu
I0815 20:55:05.344341 20241 net.cpp:252] TRAIN Top shape for layer 56 'ctx_conv4/relu' 6 64 80 80 (2457600)
I0815 20:55:05.344342 20241 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I0815 20:55:05.344344 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.344349 20241 net.cpp:184] Created Layer ctx_final (57)
I0815 20:55:05.344352 20241 net.cpp:561] ctx_final <- ctx_conv4
I0815 20:55:05.344353 20241 net.cpp:530] ctx_final -> ctx_final
I0815 20:55:05.356672 20241 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 7.22G, req 0G)
I0815 20:55:05.356683 20241 net.cpp:245] Setting up ctx_final
I0815 20:55:05.356688 20241 net.cpp:252] TRAIN Top shape for layer 57 'ctx_final' 6 8 80 80 (307200)
I0815 20:55:05.356693 20241 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I0815 20:55:05.356696 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.356700 20241 net.cpp:184] Created Layer ctx_final/relu (58)
I0815 20:55:05.356703 20241 net.cpp:561] ctx_final/relu <- ctx_final
I0815 20:55:05.356705 20241 net.cpp:513] ctx_final/relu -> ctx_final (in-place)
I0815 20:55:05.356709 20241 net.cpp:245] Setting up ctx_final/relu
I0815 20:55:05.356714 20241 net.cpp:252] TRAIN Top shape for layer 58 'ctx_final/relu' 6 8 80 80 (307200)
I0815 20:55:05.356715 20241 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I0815 20:55:05.356717 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.356730 20241 net.cpp:184] Created Layer out_deconv_final_up2 (59)
I0815 20:55:05.356734 20241 net.cpp:561] out_deconv_final_up2 <- ctx_final
I0815 20:55:05.356736 20241 net.cpp:530] out_deconv_final_up2 -> out_deconv_final_up2
I0815 20:55:05.357017 20241 net.cpp:245] Setting up out_deconv_final_up2
I0815 20:55:05.357023 20241 net.cpp:252] TRAIN Top shape for layer 59 'out_deconv_final_up2' 6 8 160 160 (1228800)
I0815 20:55:05.357025 20241 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I0815 20:55:05.357028 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.357033 20241 net.cpp:184] Created Layer out_deconv_final_up4 (60)
I0815 20:55:05.357035 20241 net.cpp:561] out_deconv_final_up4 <- out_deconv_final_up2
I0815 20:55:05.357038 20241 net.cpp:530] out_deconv_final_up4 -> out_deconv_final_up4
I0815 20:55:05.357290 20241 net.cpp:245] Setting up out_deconv_final_up4
I0815 20:55:05.357295 20241 net.cpp:252] TRAIN Top shape for layer 60 'out_deconv_final_up4' 6 8 320 320 (4915200)
I0815 20:55:05.357298 20241 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I0815 20:55:05.357300 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.357306 20241 net.cpp:184] Created Layer out_deconv_final_up8 (61)
I0815 20:55:05.357308 20241 net.cpp:561] out_deconv_final_up8 <- out_deconv_final_up4
I0815 20:55:05.357311 20241 net.cpp:530] out_deconv_final_up8 -> out_deconv_final_up8
I0815 20:55:05.357563 20241 net.cpp:245] Setting up out_deconv_final_up8
I0815 20:55:05.357568 20241 net.cpp:252] TRAIN Top shape for layer 61 'out_deconv_final_up8' 6 8 640 640 (19660800)
I0815 20:55:05.357570 20241 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0815 20:55:05.357573 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.357583 20241 net.cpp:184] Created Layer loss (62)
I0815 20:55:05.357586 20241 net.cpp:561] loss <- out_deconv_final_up8
I0815 20:55:05.357589 20241 net.cpp:561] loss <- label
I0815 20:55:05.357592 20241 net.cpp:530] loss -> loss
I0815 20:55:05.358876 20241 net.cpp:245] Setting up loss
I0815 20:55:05.358886 20241 net.cpp:252] TRAIN Top shape for layer 62 'loss' (1)
I0815 20:55:05.358888 20241 net.cpp:256]     with loss weight 1
I0815 20:55:05.358892 20241 net.cpp:323] loss needs backward computation.
I0815 20:55:05.358896 20241 net.cpp:323] out_deconv_final_up8 needs backward computation.
I0815 20:55:05.358897 20241 net.cpp:323] out_deconv_final_up4 needs backward computation.
I0815 20:55:05.358901 20241 net.cpp:323] out_deconv_final_up2 needs backward computation.
I0815 20:55:05.358902 20241 net.cpp:323] ctx_final/relu needs backward computation.
I0815 20:55:05.358906 20241 net.cpp:323] ctx_final needs backward computation.
I0815 20:55:05.358907 20241 net.cpp:323] ctx_conv4/relu needs backward computation.
I0815 20:55:05.358909 20241 net.cpp:323] ctx_conv4/bn needs backward computation.
I0815 20:55:05.358911 20241 net.cpp:323] ctx_conv4 needs backward computation.
I0815 20:55:05.358913 20241 net.cpp:323] ctx_conv3/relu needs backward computation.
I0815 20:55:05.358916 20241 net.cpp:323] ctx_conv3/bn needs backward computation.
I0815 20:55:05.358918 20241 net.cpp:323] ctx_conv3 needs backward computation.
I0815 20:55:05.358919 20241 net.cpp:323] ctx_conv2/relu needs backward computation.
I0815 20:55:05.358922 20241 net.cpp:323] ctx_conv2/bn needs backward computation.
I0815 20:55:05.358924 20241 net.cpp:323] ctx_conv2 needs backward computation.
I0815 20:55:05.358927 20241 net.cpp:323] ctx_conv1/relu needs backward computation.
I0815 20:55:05.358928 20241 net.cpp:323] ctx_conv1/bn needs backward computation.
I0815 20:55:05.358930 20241 net.cpp:323] ctx_conv1 needs backward computation.
I0815 20:55:05.358932 20241 net.cpp:323] out3_out5_combined needs backward computation.
I0815 20:55:05.358934 20241 net.cpp:323] out3a/relu needs backward computation.
I0815 20:55:05.358942 20241 net.cpp:323] out3a/bn needs backward computation.
I0815 20:55:05.358944 20241 net.cpp:323] out3a needs backward computation.
I0815 20:55:05.358947 20241 net.cpp:323] out5a_up2 needs backward computation.
I0815 20:55:05.358949 20241 net.cpp:323] out5a/relu needs backward computation.
I0815 20:55:05.358952 20241 net.cpp:323] out5a/bn needs backward computation.
I0815 20:55:05.358954 20241 net.cpp:323] out5a needs backward computation.
I0815 20:55:05.358956 20241 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0815 20:55:05.358959 20241 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0815 20:55:05.358961 20241 net.cpp:323] res5a_branch2b needs backward computation.
I0815 20:55:05.358963 20241 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0815 20:55:05.358965 20241 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0815 20:55:05.358968 20241 net.cpp:323] res5a_branch2a needs backward computation.
I0815 20:55:05.358969 20241 net.cpp:323] pool4 needs backward computation.
I0815 20:55:05.358973 20241 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0815 20:55:05.358973 20241 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0815 20:55:05.358975 20241 net.cpp:323] res4a_branch2b needs backward computation.
I0815 20:55:05.358978 20241 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0815 20:55:05.358980 20241 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0815 20:55:05.358981 20241 net.cpp:323] res4a_branch2a needs backward computation.
I0815 20:55:05.358984 20241 net.cpp:323] pool3 needs backward computation.
I0815 20:55:05.358986 20241 net.cpp:323] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I0815 20:55:05.358989 20241 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0815 20:55:05.358991 20241 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0815 20:55:05.358994 20241 net.cpp:323] res3a_branch2b needs backward computation.
I0815 20:55:05.358996 20241 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0815 20:55:05.358999 20241 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0815 20:55:05.359000 20241 net.cpp:323] res3a_branch2a needs backward computation.
I0815 20:55:05.359002 20241 net.cpp:323] pool2 needs backward computation.
I0815 20:55:05.359005 20241 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0815 20:55:05.359007 20241 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0815 20:55:05.359009 20241 net.cpp:323] res2a_branch2b needs backward computation.
I0815 20:55:05.359012 20241 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0815 20:55:05.359014 20241 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0815 20:55:05.359016 20241 net.cpp:323] res2a_branch2a needs backward computation.
I0815 20:55:05.359019 20241 net.cpp:323] pool1 needs backward computation.
I0815 20:55:05.359021 20241 net.cpp:323] conv1b/relu needs backward computation.
I0815 20:55:05.359024 20241 net.cpp:323] conv1b/bn needs backward computation.
I0815 20:55:05.359025 20241 net.cpp:323] conv1b needs backward computation.
I0815 20:55:05.359028 20241 net.cpp:323] conv1a/relu needs backward computation.
I0815 20:55:05.359030 20241 net.cpp:323] conv1a/bn needs backward computation.
I0815 20:55:05.359031 20241 net.cpp:323] conv1a needs backward computation.
I0815 20:55:05.359035 20241 net.cpp:325] data/bias does not need backward computation.
I0815 20:55:05.359038 20241 net.cpp:325] data does not need backward computation.
I0815 20:55:05.359040 20241 net.cpp:367] This network produces output loss
I0815 20:55:05.359083 20241 net.cpp:389] Top memory (TRAIN) required for data: 956006400 diff: 946176008
I0815 20:55:05.359086 20241 net.cpp:392] Bottom memory (TRAIN) required for data: 956006400 diff: 956006400
I0815 20:55:05.359088 20241 net.cpp:395] Shared (in-place) memory (TRAIN) by data: 630374400 diff: 630374400
I0815 20:55:05.359091 20241 net.cpp:398] Parameters memory (TRAIN) required for data: 2692608 diff: 2692608
I0815 20:55:05.359097 20241 net.cpp:401] Parameters shared memory (TRAIN) by data: 0 diff: 0
I0815 20:55:05.359099 20241 net.cpp:407] Network initialization done.
I0815 20:55:05.359787 20241 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/l1reg/test.prototxt
W0815 20:55:05.359844 20241 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0815 20:55:05.360016 20241 net.cpp:72] Initializing net from parameters: 
name: "jsegnet21v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/val-image-lmdb"
    label_list_path: "data/val-label-lmdb"
    batch_size: 2
    threads: 1
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
  accuracy_param {
    ignore_label: 255
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
    ignore_label: 255
  }
}
I0815 20:55:05.360146 20241 net.cpp:104] Using FLOAT as default forward math type
I0815 20:55:05.360152 20241 net.cpp:110] Using FLOAT as default backward math type
I0815 20:55:05.360154 20241 layer_factory.hpp:136] Creating layer 'data' of type 'ImageLabelData'
I0815 20:55:05.360158 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.360172 20241 net.cpp:184] Created Layer data (0)
I0815 20:55:05.360175 20241 net.cpp:530] data -> data
I0815 20:55:05.360180 20241 net.cpp:530] data -> label
I0815 20:55:05.360200 20241 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0815 20:55:05.360206 20241 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 20:55:05.360929 20336 db_lmdb.cpp:24] Opened lmdb data/val-image-lmdb
I0815 20:55:05.362357 20241 data_layer.cpp:185] (0) ReshapePrefetch 2, 3, 640, 640
I0815 20:55:05.362448 20241 data_layer.cpp:209] (0) Output data size: 2, 3, 640, 640
I0815 20:55:05.362454 20241 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 20:55:05.362573 20241 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0815 20:55:05.362586 20241 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 20:55:05.363306 20337 data_layer.cpp:97] (0) Parser threads: 1
I0815 20:55:05.363315 20337 data_layer.cpp:99] (0) Transformer threads: 1
I0815 20:55:05.365850 20338 db_lmdb.cpp:24] Opened lmdb data/val-label-lmdb
I0815 20:55:05.367210 20241 data_layer.cpp:185] (0) ReshapePrefetch 2, 1, 640, 640
I0815 20:55:05.367383 20241 data_layer.cpp:209] (0) Output data size: 2, 1, 640, 640
I0815 20:55:05.367394 20241 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 20:55:05.367462 20241 net.cpp:245] Setting up data
I0815 20:55:05.367476 20241 net.cpp:252] TEST Top shape for layer 0 'data' 2 3 640 640 (2457600)
I0815 20:55:05.367491 20241 net.cpp:252] TEST Top shape for layer 0 'data' 2 1 640 640 (819200)
I0815 20:55:05.367504 20241 layer_factory.hpp:136] Creating layer 'label_data_1_split' of type 'Split'
I0815 20:55:05.367517 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.367537 20241 net.cpp:184] Created Layer label_data_1_split (1)
I0815 20:55:05.367544 20241 net.cpp:561] label_data_1_split <- label
I0815 20:55:05.367558 20241 net.cpp:530] label_data_1_split -> label_data_1_split_0
I0815 20:55:05.367571 20241 net.cpp:530] label_data_1_split -> label_data_1_split_1
I0815 20:55:05.367579 20241 net.cpp:530] label_data_1_split -> label_data_1_split_2
I0815 20:55:05.367727 20241 net.cpp:245] Setting up label_data_1_split
I0815 20:55:05.367734 20241 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0815 20:55:05.367741 20241 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0815 20:55:05.367748 20241 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0815 20:55:05.367753 20241 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0815 20:55:05.367759 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.367771 20241 net.cpp:184] Created Layer data/bias (2)
I0815 20:55:05.367775 20241 net.cpp:561] data/bias <- data
I0815 20:55:05.367782 20241 net.cpp:530] data/bias -> data/bias
I0815 20:55:05.368913 20339 data_layer.cpp:97] (0) Parser threads: 1
I0815 20:55:05.368922 20339 data_layer.cpp:99] (0) Transformer threads: 1
I0815 20:55:05.370781 20241 net.cpp:245] Setting up data/bias
I0815 20:55:05.370807 20241 net.cpp:252] TEST Top shape for layer 2 'data/bias' 2 3 640 640 (2457600)
I0815 20:55:05.370831 20241 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0815 20:55:05.370842 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.370869 20241 net.cpp:184] Created Layer conv1a (3)
I0815 20:55:05.370877 20241 net.cpp:561] conv1a <- data/bias
I0815 20:55:05.370885 20241 net.cpp:530] conv1a -> conv1a
I0815 20:55:05.377547 20241 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.09G, req 0G)
I0815 20:55:05.377563 20241 net.cpp:245] Setting up conv1a
I0815 20:55:05.377568 20241 net.cpp:252] TEST Top shape for layer 3 'conv1a' 2 32 320 320 (6553600)
I0815 20:55:05.377589 20241 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0815 20:55:05.377593 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.377602 20241 net.cpp:184] Created Layer conv1a/bn (4)
I0815 20:55:05.377607 20241 net.cpp:561] conv1a/bn <- conv1a
I0815 20:55:05.377611 20241 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0815 20:55:05.378350 20241 net.cpp:245] Setting up conv1a/bn
I0815 20:55:05.378356 20241 net.cpp:252] TEST Top shape for layer 4 'conv1a/bn' 2 32 320 320 (6553600)
I0815 20:55:05.378363 20241 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0815 20:55:05.378366 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.378370 20241 net.cpp:184] Created Layer conv1a/relu (5)
I0815 20:55:05.378371 20241 net.cpp:561] conv1a/relu <- conv1a
I0815 20:55:05.378374 20241 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0815 20:55:05.378377 20241 net.cpp:245] Setting up conv1a/relu
I0815 20:55:05.378381 20241 net.cpp:252] TEST Top shape for layer 5 'conv1a/relu' 2 32 320 320 (6553600)
I0815 20:55:05.378382 20241 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0815 20:55:05.378384 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.378391 20241 net.cpp:184] Created Layer conv1b (6)
I0815 20:55:05.378396 20241 net.cpp:561] conv1b <- conv1a
I0815 20:55:05.378399 20241 net.cpp:530] conv1b -> conv1b
I0815 20:55:05.392042 20241 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.06G, req 0G)
I0815 20:55:05.392055 20241 net.cpp:245] Setting up conv1b
I0815 20:55:05.392060 20241 net.cpp:252] TEST Top shape for layer 6 'conv1b' 2 32 320 320 (6553600)
I0815 20:55:05.392066 20241 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0815 20:55:05.392069 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.392076 20241 net.cpp:184] Created Layer conv1b/bn (7)
I0815 20:55:05.392077 20241 net.cpp:561] conv1b/bn <- conv1b
I0815 20:55:05.392081 20241 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0815 20:55:05.392802 20241 net.cpp:245] Setting up conv1b/bn
I0815 20:55:05.392810 20241 net.cpp:252] TEST Top shape for layer 7 'conv1b/bn' 2 32 320 320 (6553600)
I0815 20:55:05.392817 20241 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0815 20:55:05.392819 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.392823 20241 net.cpp:184] Created Layer conv1b/relu (8)
I0815 20:55:05.392825 20241 net.cpp:561] conv1b/relu <- conv1b
I0815 20:55:05.392828 20241 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0815 20:55:05.392832 20241 net.cpp:245] Setting up conv1b/relu
I0815 20:55:05.392834 20241 net.cpp:252] TEST Top shape for layer 8 'conv1b/relu' 2 32 320 320 (6553600)
I0815 20:55:05.392837 20241 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0815 20:55:05.392838 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.392843 20241 net.cpp:184] Created Layer pool1 (9)
I0815 20:55:05.392844 20241 net.cpp:561] pool1 <- conv1b
I0815 20:55:05.392848 20241 net.cpp:530] pool1 -> pool1
I0815 20:55:05.392917 20241 net.cpp:245] Setting up pool1
I0815 20:55:05.392922 20241 net.cpp:252] TEST Top shape for layer 9 'pool1' 2 32 160 160 (1638400)
I0815 20:55:05.392925 20241 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0815 20:55:05.392927 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.392933 20241 net.cpp:184] Created Layer res2a_branch2a (10)
I0815 20:55:05.392936 20241 net.cpp:561] res2a_branch2a <- pool1
I0815 20:55:05.392938 20241 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0815 20:55:05.401135 20241 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.03G, req 0G)
I0815 20:55:05.401154 20241 net.cpp:245] Setting up res2a_branch2a
I0815 20:55:05.401160 20241 net.cpp:252] TEST Top shape for layer 10 'res2a_branch2a' 2 64 160 160 (3276800)
I0815 20:55:05.401165 20241 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0815 20:55:05.401168 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.401173 20241 net.cpp:184] Created Layer res2a_branch2a/bn (11)
I0815 20:55:05.401176 20241 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0815 20:55:05.401180 20241 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0815 20:55:05.401872 20241 net.cpp:245] Setting up res2a_branch2a/bn
I0815 20:55:05.401880 20241 net.cpp:252] TEST Top shape for layer 11 'res2a_branch2a/bn' 2 64 160 160 (3276800)
I0815 20:55:05.401885 20241 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0815 20:55:05.401887 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.401890 20241 net.cpp:184] Created Layer res2a_branch2a/relu (12)
I0815 20:55:05.401892 20241 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0815 20:55:05.401895 20241 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0815 20:55:05.401898 20241 net.cpp:245] Setting up res2a_branch2a/relu
I0815 20:55:05.401901 20241 net.cpp:252] TEST Top shape for layer 12 'res2a_branch2a/relu' 2 64 160 160 (3276800)
I0815 20:55:05.401902 20241 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0815 20:55:05.401904 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.401911 20241 net.cpp:184] Created Layer res2a_branch2b (13)
I0815 20:55:05.401913 20241 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0815 20:55:05.401916 20241 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0815 20:55:05.408524 20241 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.02G, req 0G)
I0815 20:55:05.408534 20241 net.cpp:245] Setting up res2a_branch2b
I0815 20:55:05.408537 20241 net.cpp:252] TEST Top shape for layer 13 'res2a_branch2b' 2 64 160 160 (3276800)
I0815 20:55:05.408542 20241 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0815 20:55:05.408545 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.408548 20241 net.cpp:184] Created Layer res2a_branch2b/bn (14)
I0815 20:55:05.408551 20241 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0815 20:55:05.408553 20241 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0815 20:55:05.409238 20241 net.cpp:245] Setting up res2a_branch2b/bn
I0815 20:55:05.409246 20241 net.cpp:252] TEST Top shape for layer 14 'res2a_branch2b/bn' 2 64 160 160 (3276800)
I0815 20:55:05.409251 20241 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0815 20:55:05.409255 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.409257 20241 net.cpp:184] Created Layer res2a_branch2b/relu (15)
I0815 20:55:05.409260 20241 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0815 20:55:05.409261 20241 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0815 20:55:05.409265 20241 net.cpp:245] Setting up res2a_branch2b/relu
I0815 20:55:05.409267 20241 net.cpp:252] TEST Top shape for layer 15 'res2a_branch2b/relu' 2 64 160 160 (3276800)
I0815 20:55:05.409270 20241 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0815 20:55:05.409271 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.409274 20241 net.cpp:184] Created Layer pool2 (16)
I0815 20:55:05.409277 20241 net.cpp:561] pool2 <- res2a_branch2b
I0815 20:55:05.409281 20241 net.cpp:530] pool2 -> pool2
I0815 20:55:05.409349 20241 net.cpp:245] Setting up pool2
I0815 20:55:05.409354 20241 net.cpp:252] TEST Top shape for layer 16 'pool2' 2 64 80 80 (819200)
I0815 20:55:05.409364 20241 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0815 20:55:05.409368 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.409374 20241 net.cpp:184] Created Layer res3a_branch2a (17)
I0815 20:55:05.409377 20241 net.cpp:561] res3a_branch2a <- pool2
I0815 20:55:05.409380 20241 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0815 20:55:05.414916 20241 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.01G, req 0G)
I0815 20:55:05.414928 20241 net.cpp:245] Setting up res3a_branch2a
I0815 20:55:05.414932 20241 net.cpp:252] TEST Top shape for layer 17 'res3a_branch2a' 2 128 80 80 (1638400)
I0815 20:55:05.414937 20241 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0815 20:55:05.414940 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.414947 20241 net.cpp:184] Created Layer res3a_branch2a/bn (18)
I0815 20:55:05.414948 20241 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0815 20:55:05.414952 20241 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0815 20:55:05.415666 20241 net.cpp:245] Setting up res3a_branch2a/bn
I0815 20:55:05.415673 20241 net.cpp:252] TEST Top shape for layer 18 'res3a_branch2a/bn' 2 128 80 80 (1638400)
I0815 20:55:05.415680 20241 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0815 20:55:05.415683 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.415686 20241 net.cpp:184] Created Layer res3a_branch2a/relu (19)
I0815 20:55:05.415688 20241 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0815 20:55:05.415691 20241 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0815 20:55:05.415695 20241 net.cpp:245] Setting up res3a_branch2a/relu
I0815 20:55:05.415697 20241 net.cpp:252] TEST Top shape for layer 19 'res3a_branch2a/relu' 2 128 80 80 (1638400)
I0815 20:55:05.415699 20241 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0815 20:55:05.415701 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.415710 20241 net.cpp:184] Created Layer res3a_branch2b (20)
I0815 20:55:05.415715 20241 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0815 20:55:05.415717 20241 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0815 20:55:05.420872 20241 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7G, req 0G)
I0815 20:55:05.420884 20241 net.cpp:245] Setting up res3a_branch2b
I0815 20:55:05.420888 20241 net.cpp:252] TEST Top shape for layer 20 'res3a_branch2b' 2 128 80 80 (1638400)
I0815 20:55:05.420893 20241 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0815 20:55:05.420897 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.420902 20241 net.cpp:184] Created Layer res3a_branch2b/bn (21)
I0815 20:55:05.420903 20241 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0815 20:55:05.420907 20241 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0815 20:55:05.421656 20241 net.cpp:245] Setting up res3a_branch2b/bn
I0815 20:55:05.421665 20241 net.cpp:252] TEST Top shape for layer 21 'res3a_branch2b/bn' 2 128 80 80 (1638400)
I0815 20:55:05.421671 20241 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0815 20:55:05.421674 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.421679 20241 net.cpp:184] Created Layer res3a_branch2b/relu (22)
I0815 20:55:05.421680 20241 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0815 20:55:05.421684 20241 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0815 20:55:05.421687 20241 net.cpp:245] Setting up res3a_branch2b/relu
I0815 20:55:05.421689 20241 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b/relu' 2 128 80 80 (1638400)
I0815 20:55:05.421708 20241 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0815 20:55:05.421713 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.421716 20241 net.cpp:184] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (23)
I0815 20:55:05.421720 20241 net.cpp:561] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0815 20:55:05.421721 20241 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0815 20:55:05.421725 20241 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0815 20:55:05.421772 20241 net.cpp:245] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0815 20:55:05.421777 20241 net.cpp:252] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 2 128 80 80 (1638400)
I0815 20:55:05.421779 20241 net.cpp:252] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 2 128 80 80 (1638400)
I0815 20:55:05.421782 20241 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0815 20:55:05.421784 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.421788 20241 net.cpp:184] Created Layer pool3 (24)
I0815 20:55:05.421790 20241 net.cpp:561] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0815 20:55:05.421793 20241 net.cpp:530] pool3 -> pool3
I0815 20:55:05.421865 20241 net.cpp:245] Setting up pool3
I0815 20:55:05.421871 20241 net.cpp:252] TEST Top shape for layer 24 'pool3' 2 128 40 40 (409600)
I0815 20:55:05.421874 20241 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0815 20:55:05.421875 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.421885 20241 net.cpp:184] Created Layer res4a_branch2a (25)
I0815 20:55:05.421888 20241 net.cpp:561] res4a_branch2a <- pool3
I0815 20:55:05.421890 20241 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0815 20:55:05.433149 20241 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.99G, req 0G)
I0815 20:55:05.433161 20241 net.cpp:245] Setting up res4a_branch2a
I0815 20:55:05.433166 20241 net.cpp:252] TEST Top shape for layer 25 'res4a_branch2a' 2 256 40 40 (819200)
I0815 20:55:05.433169 20241 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0815 20:55:05.433172 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.433183 20241 net.cpp:184] Created Layer res4a_branch2a/bn (26)
I0815 20:55:05.433187 20241 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0815 20:55:05.433188 20241 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0815 20:55:05.433881 20241 net.cpp:245] Setting up res4a_branch2a/bn
I0815 20:55:05.433888 20241 net.cpp:252] TEST Top shape for layer 26 'res4a_branch2a/bn' 2 256 40 40 (819200)
I0815 20:55:05.433893 20241 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0815 20:55:05.433897 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.433899 20241 net.cpp:184] Created Layer res4a_branch2a/relu (27)
I0815 20:55:05.433902 20241 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0815 20:55:05.433903 20241 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0815 20:55:05.433907 20241 net.cpp:245] Setting up res4a_branch2a/relu
I0815 20:55:05.433909 20241 net.cpp:252] TEST Top shape for layer 27 'res4a_branch2a/relu' 2 256 40 40 (819200)
I0815 20:55:05.433912 20241 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0815 20:55:05.433914 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.433920 20241 net.cpp:184] Created Layer res4a_branch2b (28)
I0815 20:55:05.433923 20241 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0815 20:55:05.433938 20241 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0815 20:55:05.440789 20241 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.98G, req 0G)
I0815 20:55:05.440804 20241 net.cpp:245] Setting up res4a_branch2b
I0815 20:55:05.440809 20241 net.cpp:252] TEST Top shape for layer 28 'res4a_branch2b' 2 256 40 40 (819200)
I0815 20:55:05.440815 20241 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0815 20:55:05.440819 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.440830 20241 net.cpp:184] Created Layer res4a_branch2b/bn (29)
I0815 20:55:05.440834 20241 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0815 20:55:05.440841 20241 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0815 20:55:05.441576 20241 net.cpp:245] Setting up res4a_branch2b/bn
I0815 20:55:05.441586 20241 net.cpp:252] TEST Top shape for layer 29 'res4a_branch2b/bn' 2 256 40 40 (819200)
I0815 20:55:05.441591 20241 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0815 20:55:05.441594 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.441599 20241 net.cpp:184] Created Layer res4a_branch2b/relu (30)
I0815 20:55:05.441602 20241 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0815 20:55:05.441604 20241 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0815 20:55:05.441610 20241 net.cpp:245] Setting up res4a_branch2b/relu
I0815 20:55:05.441613 20241 net.cpp:252] TEST Top shape for layer 30 'res4a_branch2b/relu' 2 256 40 40 (819200)
I0815 20:55:05.441617 20241 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0815 20:55:05.441618 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.441623 20241 net.cpp:184] Created Layer pool4 (31)
I0815 20:55:05.441625 20241 net.cpp:561] pool4 <- res4a_branch2b
I0815 20:55:05.441628 20241 net.cpp:530] pool4 -> pool4
I0815 20:55:05.441695 20241 net.cpp:245] Setting up pool4
I0815 20:55:05.441700 20241 net.cpp:252] TEST Top shape for layer 31 'pool4' 2 256 40 40 (819200)
I0815 20:55:05.441704 20241 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0815 20:55:05.441707 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.441721 20241 net.cpp:184] Created Layer res5a_branch2a (32)
I0815 20:55:05.441725 20241 net.cpp:561] res5a_branch2a <- pool4
I0815 20:55:05.441727 20241 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0815 20:55:05.466642 20241 net.cpp:245] Setting up res5a_branch2a
I0815 20:55:05.466667 20241 net.cpp:252] TEST Top shape for layer 32 'res5a_branch2a' 2 512 40 40 (1638400)
I0815 20:55:05.466675 20241 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0815 20:55:05.466678 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.466684 20241 net.cpp:184] Created Layer res5a_branch2a/bn (33)
I0815 20:55:05.466688 20241 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0815 20:55:05.466691 20241 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0815 20:55:05.467378 20241 net.cpp:245] Setting up res5a_branch2a/bn
I0815 20:55:05.467386 20241 net.cpp:252] TEST Top shape for layer 33 'res5a_branch2a/bn' 2 512 40 40 (1638400)
I0815 20:55:05.467392 20241 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0815 20:55:05.467394 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.467397 20241 net.cpp:184] Created Layer res5a_branch2a/relu (34)
I0815 20:55:05.467401 20241 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0815 20:55:05.467402 20241 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0815 20:55:05.467406 20241 net.cpp:245] Setting up res5a_branch2a/relu
I0815 20:55:05.467408 20241 net.cpp:252] TEST Top shape for layer 34 'res5a_branch2a/relu' 2 512 40 40 (1638400)
I0815 20:55:05.467424 20241 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0815 20:55:05.467428 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.467434 20241 net.cpp:184] Created Layer res5a_branch2b (35)
I0815 20:55:05.467437 20241 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0815 20:55:05.467439 20241 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0815 20:55:05.480543 20241 net.cpp:245] Setting up res5a_branch2b
I0815 20:55:05.480569 20241 net.cpp:252] TEST Top shape for layer 35 'res5a_branch2b' 2 512 40 40 (1638400)
I0815 20:55:05.480578 20241 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0815 20:55:05.480582 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.480589 20241 net.cpp:184] Created Layer res5a_branch2b/bn (36)
I0815 20:55:05.480592 20241 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0815 20:55:05.480595 20241 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0815 20:55:05.481307 20241 net.cpp:245] Setting up res5a_branch2b/bn
I0815 20:55:05.481315 20241 net.cpp:252] TEST Top shape for layer 36 'res5a_branch2b/bn' 2 512 40 40 (1638400)
I0815 20:55:05.481320 20241 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0815 20:55:05.481323 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.481328 20241 net.cpp:184] Created Layer res5a_branch2b/relu (37)
I0815 20:55:05.481329 20241 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0815 20:55:05.481331 20241 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0815 20:55:05.481335 20241 net.cpp:245] Setting up res5a_branch2b/relu
I0815 20:55:05.481338 20241 net.cpp:252] TEST Top shape for layer 37 'res5a_branch2b/relu' 2 512 40 40 (1638400)
I0815 20:55:05.481339 20241 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I0815 20:55:05.481341 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.481351 20241 net.cpp:184] Created Layer out5a (38)
I0815 20:55:05.481355 20241 net.cpp:561] out5a <- res5a_branch2b
I0815 20:55:05.481361 20241 net.cpp:530] out5a -> out5a
I0815 20:55:05.484740 20241 net.cpp:245] Setting up out5a
I0815 20:55:05.484747 20241 net.cpp:252] TEST Top shape for layer 38 'out5a' 2 64 40 40 (204800)
I0815 20:55:05.484751 20241 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I0815 20:55:05.484755 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.484758 20241 net.cpp:184] Created Layer out5a/bn (39)
I0815 20:55:05.484761 20241 net.cpp:561] out5a/bn <- out5a
I0815 20:55:05.484763 20241 net.cpp:513] out5a/bn -> out5a (in-place)
I0815 20:55:05.485461 20241 net.cpp:245] Setting up out5a/bn
I0815 20:55:05.485467 20241 net.cpp:252] TEST Top shape for layer 39 'out5a/bn' 2 64 40 40 (204800)
I0815 20:55:05.485473 20241 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I0815 20:55:05.485476 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.485478 20241 net.cpp:184] Created Layer out5a/relu (40)
I0815 20:55:05.485481 20241 net.cpp:561] out5a/relu <- out5a
I0815 20:55:05.485482 20241 net.cpp:513] out5a/relu -> out5a (in-place)
I0815 20:55:05.485486 20241 net.cpp:245] Setting up out5a/relu
I0815 20:55:05.485488 20241 net.cpp:252] TEST Top shape for layer 40 'out5a/relu' 2 64 40 40 (204800)
I0815 20:55:05.485491 20241 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I0815 20:55:05.485492 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.485497 20241 net.cpp:184] Created Layer out5a_up2 (41)
I0815 20:55:05.485499 20241 net.cpp:561] out5a_up2 <- out5a
I0815 20:55:05.485502 20241 net.cpp:530] out5a_up2 -> out5a_up2
I0815 20:55:05.485818 20241 net.cpp:245] Setting up out5a_up2
I0815 20:55:05.485824 20241 net.cpp:252] TEST Top shape for layer 41 'out5a_up2' 2 64 80 80 (819200)
I0815 20:55:05.485827 20241 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I0815 20:55:05.485829 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.485834 20241 net.cpp:184] Created Layer out3a (42)
I0815 20:55:05.485836 20241 net.cpp:561] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0815 20:55:05.485839 20241 net.cpp:530] out3a -> out3a
I0815 20:55:05.489984 20241 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.97G, req 0G)
I0815 20:55:05.489995 20241 net.cpp:245] Setting up out3a
I0815 20:55:05.490000 20241 net.cpp:252] TEST Top shape for layer 42 'out3a' 2 64 80 80 (819200)
I0815 20:55:05.490003 20241 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I0815 20:55:05.490006 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.490011 20241 net.cpp:184] Created Layer out3a/bn (43)
I0815 20:55:05.490013 20241 net.cpp:561] out3a/bn <- out3a
I0815 20:55:05.490016 20241 net.cpp:513] out3a/bn -> out3a (in-place)
I0815 20:55:05.490748 20241 net.cpp:245] Setting up out3a/bn
I0815 20:55:05.490756 20241 net.cpp:252] TEST Top shape for layer 43 'out3a/bn' 2 64 80 80 (819200)
I0815 20:55:05.490761 20241 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I0815 20:55:05.490763 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.490767 20241 net.cpp:184] Created Layer out3a/relu (44)
I0815 20:55:05.490768 20241 net.cpp:561] out3a/relu <- out3a
I0815 20:55:05.490772 20241 net.cpp:513] out3a/relu -> out3a (in-place)
I0815 20:55:05.490774 20241 net.cpp:245] Setting up out3a/relu
I0815 20:55:05.490777 20241 net.cpp:252] TEST Top shape for layer 44 'out3a/relu' 2 64 80 80 (819200)
I0815 20:55:05.490778 20241 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I0815 20:55:05.490782 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.490785 20241 net.cpp:184] Created Layer out3_out5_combined (45)
I0815 20:55:05.490787 20241 net.cpp:561] out3_out5_combined <- out5a_up2
I0815 20:55:05.490789 20241 net.cpp:561] out3_out5_combined <- out3a
I0815 20:55:05.490792 20241 net.cpp:530] out3_out5_combined -> out3_out5_combined
I0815 20:55:05.491699 20241 net.cpp:245] Setting up out3_out5_combined
I0815 20:55:05.491708 20241 net.cpp:252] TEST Top shape for layer 45 'out3_out5_combined' 2 64 80 80 (819200)
I0815 20:55:05.491710 20241 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I0815 20:55:05.491714 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.491719 20241 net.cpp:184] Created Layer ctx_conv1 (46)
I0815 20:55:05.491721 20241 net.cpp:561] ctx_conv1 <- out3_out5_combined
I0815 20:55:05.491724 20241 net.cpp:530] ctx_conv1 -> ctx_conv1
I0815 20:55:05.495625 20241 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.96G, req 0G)
I0815 20:55:05.495635 20241 net.cpp:245] Setting up ctx_conv1
I0815 20:55:05.495640 20241 net.cpp:252] TEST Top shape for layer 46 'ctx_conv1' 2 64 80 80 (819200)
I0815 20:55:05.495643 20241 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I0815 20:55:05.495645 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.495651 20241 net.cpp:184] Created Layer ctx_conv1/bn (47)
I0815 20:55:05.495653 20241 net.cpp:561] ctx_conv1/bn <- ctx_conv1
I0815 20:55:05.495656 20241 net.cpp:513] ctx_conv1/bn -> ctx_conv1 (in-place)
I0815 20:55:05.496384 20241 net.cpp:245] Setting up ctx_conv1/bn
I0815 20:55:05.496392 20241 net.cpp:252] TEST Top shape for layer 47 'ctx_conv1/bn' 2 64 80 80 (819200)
I0815 20:55:05.496398 20241 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I0815 20:55:05.496407 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.496412 20241 net.cpp:184] Created Layer ctx_conv1/relu (48)
I0815 20:55:05.496413 20241 net.cpp:561] ctx_conv1/relu <- ctx_conv1
I0815 20:55:05.496417 20241 net.cpp:513] ctx_conv1/relu -> ctx_conv1 (in-place)
I0815 20:55:05.496419 20241 net.cpp:245] Setting up ctx_conv1/relu
I0815 20:55:05.496423 20241 net.cpp:252] TEST Top shape for layer 48 'ctx_conv1/relu' 2 64 80 80 (819200)
I0815 20:55:05.496424 20241 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I0815 20:55:05.496426 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.496434 20241 net.cpp:184] Created Layer ctx_conv2 (49)
I0815 20:55:05.496438 20241 net.cpp:561] ctx_conv2 <- ctx_conv1
I0815 20:55:05.496440 20241 net.cpp:530] ctx_conv2 -> ctx_conv2
I0815 20:55:05.497553 20241 net.cpp:245] Setting up ctx_conv2
I0815 20:55:05.497560 20241 net.cpp:252] TEST Top shape for layer 49 'ctx_conv2' 2 64 80 80 (819200)
I0815 20:55:05.497565 20241 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I0815 20:55:05.497566 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.497570 20241 net.cpp:184] Created Layer ctx_conv2/bn (50)
I0815 20:55:05.497572 20241 net.cpp:561] ctx_conv2/bn <- ctx_conv2
I0815 20:55:05.497575 20241 net.cpp:513] ctx_conv2/bn -> ctx_conv2 (in-place)
I0815 20:55:05.498284 20241 net.cpp:245] Setting up ctx_conv2/bn
I0815 20:55:05.498291 20241 net.cpp:252] TEST Top shape for layer 50 'ctx_conv2/bn' 2 64 80 80 (819200)
I0815 20:55:05.498296 20241 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I0815 20:55:05.498299 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.498301 20241 net.cpp:184] Created Layer ctx_conv2/relu (51)
I0815 20:55:05.498304 20241 net.cpp:561] ctx_conv2/relu <- ctx_conv2
I0815 20:55:05.498306 20241 net.cpp:513] ctx_conv2/relu -> ctx_conv2 (in-place)
I0815 20:55:05.498309 20241 net.cpp:245] Setting up ctx_conv2/relu
I0815 20:55:05.498311 20241 net.cpp:252] TEST Top shape for layer 51 'ctx_conv2/relu' 2 64 80 80 (819200)
I0815 20:55:05.498313 20241 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I0815 20:55:05.498316 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.498320 20241 net.cpp:184] Created Layer ctx_conv3 (52)
I0815 20:55:05.498322 20241 net.cpp:561] ctx_conv3 <- ctx_conv2
I0815 20:55:05.498324 20241 net.cpp:530] ctx_conv3 -> ctx_conv3
I0815 20:55:05.499433 20241 net.cpp:245] Setting up ctx_conv3
I0815 20:55:05.499439 20241 net.cpp:252] TEST Top shape for layer 52 'ctx_conv3' 2 64 80 80 (819200)
I0815 20:55:05.499444 20241 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I0815 20:55:05.499445 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.499449 20241 net.cpp:184] Created Layer ctx_conv3/bn (53)
I0815 20:55:05.499451 20241 net.cpp:561] ctx_conv3/bn <- ctx_conv3
I0815 20:55:05.499454 20241 net.cpp:513] ctx_conv3/bn -> ctx_conv3 (in-place)
I0815 20:55:05.500483 20241 net.cpp:245] Setting up ctx_conv3/bn
I0815 20:55:05.500495 20241 net.cpp:252] TEST Top shape for layer 53 'ctx_conv3/bn' 2 64 80 80 (819200)
I0815 20:55:05.500504 20241 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I0815 20:55:05.500509 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.500514 20241 net.cpp:184] Created Layer ctx_conv3/relu (54)
I0815 20:55:05.500516 20241 net.cpp:561] ctx_conv3/relu <- ctx_conv3
I0815 20:55:05.500520 20241 net.cpp:513] ctx_conv3/relu -> ctx_conv3 (in-place)
I0815 20:55:05.500525 20241 net.cpp:245] Setting up ctx_conv3/relu
I0815 20:55:05.500530 20241 net.cpp:252] TEST Top shape for layer 54 'ctx_conv3/relu' 2 64 80 80 (819200)
I0815 20:55:05.500541 20241 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I0815 20:55:05.500545 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.500552 20241 net.cpp:184] Created Layer ctx_conv4 (55)
I0815 20:55:05.500556 20241 net.cpp:561] ctx_conv4 <- ctx_conv3
I0815 20:55:05.500560 20241 net.cpp:530] ctx_conv4 -> ctx_conv4
I0815 20:55:05.502034 20241 net.cpp:245] Setting up ctx_conv4
I0815 20:55:05.502044 20241 net.cpp:252] TEST Top shape for layer 55 'ctx_conv4' 2 64 80 80 (819200)
I0815 20:55:05.502049 20241 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I0815 20:55:05.502053 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.502058 20241 net.cpp:184] Created Layer ctx_conv4/bn (56)
I0815 20:55:05.502063 20241 net.cpp:561] ctx_conv4/bn <- ctx_conv4
I0815 20:55:05.502065 20241 net.cpp:513] ctx_conv4/bn -> ctx_conv4 (in-place)
I0815 20:55:05.502971 20241 net.cpp:245] Setting up ctx_conv4/bn
I0815 20:55:05.502979 20241 net.cpp:252] TEST Top shape for layer 56 'ctx_conv4/bn' 2 64 80 80 (819200)
I0815 20:55:05.502987 20241 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I0815 20:55:05.502991 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.502995 20241 net.cpp:184] Created Layer ctx_conv4/relu (57)
I0815 20:55:05.502998 20241 net.cpp:561] ctx_conv4/relu <- ctx_conv4
I0815 20:55:05.503002 20241 net.cpp:513] ctx_conv4/relu -> ctx_conv4 (in-place)
I0815 20:55:05.503006 20241 net.cpp:245] Setting up ctx_conv4/relu
I0815 20:55:05.503011 20241 net.cpp:252] TEST Top shape for layer 57 'ctx_conv4/relu' 2 64 80 80 (819200)
I0815 20:55:05.503015 20241 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I0815 20:55:05.503018 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.503026 20241 net.cpp:184] Created Layer ctx_final (58)
I0815 20:55:05.503031 20241 net.cpp:561] ctx_final <- ctx_conv4
I0815 20:55:05.503034 20241 net.cpp:530] ctx_final -> ctx_final
I0815 20:55:05.508097 20241 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.96G, req 0G)
I0815 20:55:05.508108 20241 net.cpp:245] Setting up ctx_final
I0815 20:55:05.508114 20241 net.cpp:252] TEST Top shape for layer 58 'ctx_final' 2 8 80 80 (102400)
I0815 20:55:05.508121 20241 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I0815 20:55:05.508123 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.508136 20241 net.cpp:184] Created Layer ctx_final/relu (59)
I0815 20:55:05.508139 20241 net.cpp:561] ctx_final/relu <- ctx_final
I0815 20:55:05.508143 20241 net.cpp:513] ctx_final/relu -> ctx_final (in-place)
I0815 20:55:05.508158 20241 net.cpp:245] Setting up ctx_final/relu
I0815 20:55:05.508163 20241 net.cpp:252] TEST Top shape for layer 59 'ctx_final/relu' 2 8 80 80 (102400)
I0815 20:55:05.508167 20241 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I0815 20:55:05.508170 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.508182 20241 net.cpp:184] Created Layer out_deconv_final_up2 (60)
I0815 20:55:05.508184 20241 net.cpp:561] out_deconv_final_up2 <- ctx_final
I0815 20:55:05.508188 20241 net.cpp:530] out_deconv_final_up2 -> out_deconv_final_up2
I0815 20:55:05.508597 20241 net.cpp:245] Setting up out_deconv_final_up2
I0815 20:55:05.508605 20241 net.cpp:252] TEST Top shape for layer 60 'out_deconv_final_up2' 2 8 160 160 (409600)
I0815 20:55:05.508610 20241 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I0815 20:55:05.508613 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.508620 20241 net.cpp:184] Created Layer out_deconv_final_up4 (61)
I0815 20:55:05.508631 20241 net.cpp:561] out_deconv_final_up4 <- out_deconv_final_up2
I0815 20:55:05.508635 20241 net.cpp:530] out_deconv_final_up4 -> out_deconv_final_up4
I0815 20:55:05.509016 20241 net.cpp:245] Setting up out_deconv_final_up4
I0815 20:55:05.509022 20241 net.cpp:252] TEST Top shape for layer 61 'out_deconv_final_up4' 2 8 320 320 (1638400)
I0815 20:55:05.509027 20241 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I0815 20:55:05.509029 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.509037 20241 net.cpp:184] Created Layer out_deconv_final_up8 (62)
I0815 20:55:05.509040 20241 net.cpp:561] out_deconv_final_up8 <- out_deconv_final_up4
I0815 20:55:05.509043 20241 net.cpp:530] out_deconv_final_up8 -> out_deconv_final_up8
I0815 20:55:05.509419 20241 net.cpp:245] Setting up out_deconv_final_up8
I0815 20:55:05.509425 20241 net.cpp:252] TEST Top shape for layer 62 'out_deconv_final_up8' 2 8 640 640 (6553600)
I0815 20:55:05.509430 20241 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8_out_deconv_final_up8_0_split' of type 'Split'
I0815 20:55:05.509433 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.509438 20241 net.cpp:184] Created Layer out_deconv_final_up8_out_deconv_final_up8_0_split (63)
I0815 20:55:05.509441 20241 net.cpp:561] out_deconv_final_up8_out_deconv_final_up8_0_split <- out_deconv_final_up8
I0815 20:55:05.509444 20241 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0815 20:55:05.509449 20241 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0815 20:55:05.509452 20241 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0815 20:55:05.509541 20241 net.cpp:245] Setting up out_deconv_final_up8_out_deconv_final_up8_0_split
I0815 20:55:05.509548 20241 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 8 640 640 (6553600)
I0815 20:55:05.509552 20241 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 8 640 640 (6553600)
I0815 20:55:05.509557 20241 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 8 640 640 (6553600)
I0815 20:55:05.509562 20241 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0815 20:55:05.509567 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.509573 20241 net.cpp:184] Created Layer loss (64)
I0815 20:55:05.509577 20241 net.cpp:561] loss <- out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0815 20:55:05.509582 20241 net.cpp:561] loss <- label_data_1_split_0
I0815 20:55:05.509587 20241 net.cpp:530] loss -> loss
I0815 20:55:05.510603 20241 net.cpp:245] Setting up loss
I0815 20:55:05.510613 20241 net.cpp:252] TEST Top shape for layer 64 'loss' (1)
I0815 20:55:05.510617 20241 net.cpp:256]     with loss weight 1
I0815 20:55:05.510623 20241 layer_factory.hpp:136] Creating layer 'accuracy/top1' of type 'Accuracy'
I0815 20:55:05.510627 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.510640 20241 net.cpp:184] Created Layer accuracy/top1 (65)
I0815 20:55:05.510645 20241 net.cpp:561] accuracy/top1 <- out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0815 20:55:05.510649 20241 net.cpp:561] accuracy/top1 <- label_data_1_split_1
I0815 20:55:05.510654 20241 net.cpp:530] accuracy/top1 -> accuracy/top1
I0815 20:55:05.510661 20241 net.cpp:245] Setting up accuracy/top1
I0815 20:55:05.510665 20241 net.cpp:252] TEST Top shape for layer 65 'accuracy/top1' (1)
I0815 20:55:05.510669 20241 layer_factory.hpp:136] Creating layer 'accuracy/top5' of type 'Accuracy'
I0815 20:55:05.510673 20241 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 20:55:05.510684 20241 net.cpp:184] Created Layer accuracy/top5 (66)
I0815 20:55:05.510689 20241 net.cpp:561] accuracy/top5 <- out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0815 20:55:05.510694 20241 net.cpp:561] accuracy/top5 <- label_data_1_split_2
I0815 20:55:05.510697 20241 net.cpp:530] accuracy/top5 -> accuracy/top5
I0815 20:55:05.510704 20241 net.cpp:245] Setting up accuracy/top5
I0815 20:55:05.510709 20241 net.cpp:252] TEST Top shape for layer 66 'accuracy/top5' (1)
I0815 20:55:05.510712 20241 net.cpp:325] accuracy/top5 does not need backward computation.
I0815 20:55:05.510716 20241 net.cpp:325] accuracy/top1 does not need backward computation.
I0815 20:55:05.510720 20241 net.cpp:323] loss needs backward computation.
I0815 20:55:05.510725 20241 net.cpp:323] out_deconv_final_up8_out_deconv_final_up8_0_split needs backward computation.
I0815 20:55:05.510728 20241 net.cpp:323] out_deconv_final_up8 needs backward computation.
I0815 20:55:05.510732 20241 net.cpp:323] out_deconv_final_up4 needs backward computation.
I0815 20:55:05.510736 20241 net.cpp:323] out_deconv_final_up2 needs backward computation.
I0815 20:55:05.510740 20241 net.cpp:323] ctx_final/relu needs backward computation.
I0815 20:55:05.510743 20241 net.cpp:323] ctx_final needs backward computation.
I0815 20:55:05.510747 20241 net.cpp:323] ctx_conv4/relu needs backward computation.
I0815 20:55:05.510751 20241 net.cpp:323] ctx_conv4/bn needs backward computation.
I0815 20:55:05.510754 20241 net.cpp:323] ctx_conv4 needs backward computation.
I0815 20:55:05.510758 20241 net.cpp:323] ctx_conv3/relu needs backward computation.
I0815 20:55:05.510762 20241 net.cpp:323] ctx_conv3/bn needs backward computation.
I0815 20:55:05.510766 20241 net.cpp:323] ctx_conv3 needs backward computation.
I0815 20:55:05.510769 20241 net.cpp:323] ctx_conv2/relu needs backward computation.
I0815 20:55:05.510772 20241 net.cpp:323] ctx_conv2/bn needs backward computation.
I0815 20:55:05.510776 20241 net.cpp:323] ctx_conv2 needs backward computation.
I0815 20:55:05.510779 20241 net.cpp:323] ctx_conv1/relu needs backward computation.
I0815 20:55:05.510783 20241 net.cpp:323] ctx_conv1/bn needs backward computation.
I0815 20:55:05.510787 20241 net.cpp:323] ctx_conv1 needs backward computation.
I0815 20:55:05.510792 20241 net.cpp:323] out3_out5_combined needs backward computation.
I0815 20:55:05.510795 20241 net.cpp:323] out3a/relu needs backward computation.
I0815 20:55:05.510798 20241 net.cpp:323] out3a/bn needs backward computation.
I0815 20:55:05.510802 20241 net.cpp:323] out3a needs backward computation.
I0815 20:55:05.510807 20241 net.cpp:323] out5a_up2 needs backward computation.
I0815 20:55:05.510810 20241 net.cpp:323] out5a/relu needs backward computation.
I0815 20:55:05.510814 20241 net.cpp:323] out5a/bn needs backward computation.
I0815 20:55:05.510818 20241 net.cpp:323] out5a needs backward computation.
I0815 20:55:05.510821 20241 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0815 20:55:05.510825 20241 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0815 20:55:05.510829 20241 net.cpp:323] res5a_branch2b needs backward computation.
I0815 20:55:05.510833 20241 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0815 20:55:05.510836 20241 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0815 20:55:05.510840 20241 net.cpp:323] res5a_branch2a needs backward computation.
I0815 20:55:05.510844 20241 net.cpp:323] pool4 needs backward computation.
I0815 20:55:05.510848 20241 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0815 20:55:05.510851 20241 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0815 20:55:05.510855 20241 net.cpp:323] res4a_branch2b needs backward computation.
I0815 20:55:05.510859 20241 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0815 20:55:05.510864 20241 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0815 20:55:05.510866 20241 net.cpp:323] res4a_branch2a needs backward computation.
I0815 20:55:05.510874 20241 net.cpp:323] pool3 needs backward computation.
I0815 20:55:05.510879 20241 net.cpp:323] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I0815 20:55:05.510884 20241 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0815 20:55:05.510886 20241 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0815 20:55:05.510890 20241 net.cpp:323] res3a_branch2b needs backward computation.
I0815 20:55:05.510895 20241 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0815 20:55:05.510898 20241 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0815 20:55:05.510901 20241 net.cpp:323] res3a_branch2a needs backward computation.
I0815 20:55:05.510905 20241 net.cpp:323] pool2 needs backward computation.
I0815 20:55:05.510910 20241 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0815 20:55:05.510913 20241 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0815 20:55:05.510916 20241 net.cpp:323] res2a_branch2b needs backward computation.
I0815 20:55:05.510921 20241 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0815 20:55:05.510924 20241 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0815 20:55:05.510928 20241 net.cpp:323] res2a_branch2a needs backward computation.
I0815 20:55:05.510932 20241 net.cpp:323] pool1 needs backward computation.
I0815 20:55:05.510936 20241 net.cpp:323] conv1b/relu needs backward computation.
I0815 20:55:05.510941 20241 net.cpp:323] conv1b/bn needs backward computation.
I0815 20:55:05.510943 20241 net.cpp:323] conv1b needs backward computation.
I0815 20:55:05.510947 20241 net.cpp:323] conv1a/relu needs backward computation.
I0815 20:55:05.510951 20241 net.cpp:323] conv1a/bn needs backward computation.
I0815 20:55:05.510956 20241 net.cpp:323] conv1a needs backward computation.
I0815 20:55:05.510959 20241 net.cpp:325] data/bias does not need backward computation.
I0815 20:55:05.510963 20241 net.cpp:325] label_data_1_split does not need backward computation.
I0815 20:55:05.510968 20241 net.cpp:325] data does not need backward computation.
I0815 20:55:05.510972 20241 net.cpp:367] This network produces output accuracy/top1
I0815 20:55:05.510975 20241 net.cpp:367] This network produces output accuracy/top5
I0815 20:55:05.510979 20241 net.cpp:367] This network produces output loss
I0815 20:55:05.511036 20241 net.cpp:389] Top memory (TEST) required for data: 318668800 diff: 8
I0815 20:55:05.511040 20241 net.cpp:392] Bottom memory (TEST) required for data: 318668800 diff: 318668800
I0815 20:55:05.511044 20241 net.cpp:395] Shared (in-place) memory (TEST) by data: 210124800 diff: 210124800
I0815 20:55:05.511047 20241 net.cpp:398] Parameters memory (TEST) required for data: 2692608 diff: 2692608
I0815 20:55:05.511051 20241 net.cpp:401] Parameters shared memory (TEST) by data: 0 diff: 0
I0815 20:55:05.511055 20241 net.cpp:407] Network initialization done.
I0815 20:55:05.511155 20241 solver.cpp:56] Solver scaffolding done.
I0815 20:55:05.523864 20241 caffe.cpp:137] Finetuning from training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/initial/cityscapes5_jsegnet21v2_iter_32000.caffemodel
I0815 20:55:05.530268 20241 net.cpp:1095] Copying source layer data Type:ImageLabelData #blobs=0
I0815 20:55:05.530294 20241 net.cpp:1095] Copying source layer data/bias Type:Bias #blobs=1
I0815 20:55:05.530328 20241 net.cpp:1095] Copying source layer conv1a Type:Convolution #blobs=2
I0815 20:55:05.530346 20241 net.cpp:1095] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0815 20:55:05.531006 20241 net.cpp:1095] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0815 20:55:05.531014 20241 net.cpp:1095] Copying source layer conv1b Type:Convolution #blobs=2
I0815 20:55:05.531026 20241 net.cpp:1095] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0815 20:55:05.531504 20241 net.cpp:1095] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0815 20:55:05.531512 20241 net.cpp:1095] Copying source layer pool1 Type:Pooling #blobs=0
I0815 20:55:05.531514 20241 net.cpp:1095] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0815 20:55:05.531545 20241 net.cpp:1095] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0815 20:55:05.532037 20241 net.cpp:1095] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0815 20:55:05.532044 20241 net.cpp:1095] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0815 20:55:05.532059 20241 net.cpp:1095] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0815 20:55:05.532543 20241 net.cpp:1095] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0815 20:55:05.532551 20241 net.cpp:1095] Copying source layer pool2 Type:Pooling #blobs=0
I0815 20:55:05.532554 20241 net.cpp:1095] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0815 20:55:05.532596 20241 net.cpp:1095] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0815 20:55:05.533051 20241 net.cpp:1095] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0815 20:55:05.533058 20241 net.cpp:1095] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0815 20:55:05.533083 20241 net.cpp:1095] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0815 20:55:05.533527 20241 net.cpp:1095] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0815 20:55:05.533535 20241 net.cpp:1095] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I0815 20:55:05.533538 20241 net.cpp:1095] Copying source layer pool3 Type:Pooling #blobs=0
I0815 20:55:05.533541 20241 net.cpp:1095] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0815 20:55:05.533743 20241 net.cpp:1095] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0815 20:55:05.534224 20241 net.cpp:1095] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0815 20:55:05.534231 20241 net.cpp:1095] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0815 20:55:05.534296 20241 net.cpp:1095] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0815 20:55:05.534739 20241 net.cpp:1095] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0815 20:55:05.534745 20241 net.cpp:1095] Copying source layer pool4 Type:Pooling #blobs=0
I0815 20:55:05.534749 20241 net.cpp:1095] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0815 20:55:05.535133 20241 net.cpp:1095] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0815 20:55:05.535569 20241 net.cpp:1095] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0815 20:55:05.535575 20241 net.cpp:1095] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0815 20:55:05.535773 20241 net.cpp:1095] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0815 20:55:05.536238 20241 net.cpp:1095] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0815 20:55:05.536248 20241 net.cpp:1095] Copying source layer out5a Type:Convolution #blobs=2
I0815 20:55:05.536306 20241 net.cpp:1095] Copying source layer out5a/bn Type:BatchNorm #blobs=5
I0815 20:55:05.536478 20241 net.cpp:1095] Copying source layer out5a/relu Type:ReLU #blobs=0
I0815 20:55:05.536484 20241 net.cpp:1095] Copying source layer out5a_up2 Type:Deconvolution #blobs=1
I0815 20:55:05.536490 20241 net.cpp:1095] Copying source layer out3a Type:Convolution #blobs=2
I0815 20:55:05.536511 20241 net.cpp:1095] Copying source layer out3a/bn Type:BatchNorm #blobs=5
I0815 20:55:05.536660 20241 net.cpp:1095] Copying source layer out3a/relu Type:ReLU #blobs=0
I0815 20:55:05.536665 20241 net.cpp:1095] Copying source layer out3_out5_combined Type:Eltwise #blobs=0
I0815 20:55:05.536667 20241 net.cpp:1095] Copying source layer ctx_conv1 Type:Convolution #blobs=2
I0815 20:55:05.536687 20241 net.cpp:1095] Copying source layer ctx_conv1/bn Type:BatchNorm #blobs=5
I0815 20:55:05.536839 20241 net.cpp:1095] Copying source layer ctx_conv1/relu Type:ReLU #blobs=0
I0815 20:55:05.536844 20241 net.cpp:1095] Copying source layer ctx_conv2 Type:Convolution #blobs=2
I0815 20:55:05.536860 20241 net.cpp:1095] Copying source layer ctx_conv2/bn Type:BatchNorm #blobs=5
I0815 20:55:05.537010 20241 net.cpp:1095] Copying source layer ctx_conv2/relu Type:ReLU #blobs=0
I0815 20:55:05.537022 20241 net.cpp:1095] Copying source layer ctx_conv3 Type:Convolution #blobs=2
I0815 20:55:05.537042 20241 net.cpp:1095] Copying source layer ctx_conv3/bn Type:BatchNorm #blobs=5
I0815 20:55:05.537194 20241 net.cpp:1095] Copying source layer ctx_conv3/relu Type:ReLU #blobs=0
I0815 20:55:05.537199 20241 net.cpp:1095] Copying source layer ctx_conv4 Type:Convolution #blobs=2
I0815 20:55:05.537216 20241 net.cpp:1095] Copying source layer ctx_conv4/bn Type:BatchNorm #blobs=5
I0815 20:55:05.537364 20241 net.cpp:1095] Copying source layer ctx_conv4/relu Type:ReLU #blobs=0
I0815 20:55:05.537369 20241 net.cpp:1095] Copying source layer ctx_final Type:Convolution #blobs=2
I0815 20:55:05.537379 20241 net.cpp:1095] Copying source layer ctx_final/relu Type:ReLU #blobs=0
I0815 20:55:05.537380 20241 net.cpp:1095] Copying source layer out_deconv_final_up2 Type:Deconvolution #blobs=1
I0815 20:55:05.537385 20241 net.cpp:1095] Copying source layer out_deconv_final_up4 Type:Deconvolution #blobs=1
I0815 20:55:05.537390 20241 net.cpp:1095] Copying source layer out_deconv_final_up8 Type:Deconvolution #blobs=1
I0815 20:55:05.537395 20241 net.cpp:1095] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0815 20:55:05.540998 20241 net.cpp:1095] Copying source layer data Type:ImageLabelData #blobs=0
I0815 20:55:05.541015 20241 net.cpp:1095] Copying source layer data/bias Type:Bias #blobs=1
I0815 20:55:05.541039 20241 net.cpp:1095] Copying source layer conv1a Type:Convolution #blobs=2
I0815 20:55:05.541051 20241 net.cpp:1095] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0815 20:55:05.541577 20241 net.cpp:1095] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0815 20:55:05.541584 20241 net.cpp:1095] Copying source layer conv1b Type:Convolution #blobs=2
I0815 20:55:05.541594 20241 net.cpp:1095] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0815 20:55:05.541976 20241 net.cpp:1095] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0815 20:55:05.541982 20241 net.cpp:1095] Copying source layer pool1 Type:Pooling #blobs=0
I0815 20:55:05.541985 20241 net.cpp:1095] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0815 20:55:05.542001 20241 net.cpp:1095] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0815 20:55:05.542387 20241 net.cpp:1095] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0815 20:55:05.542393 20241 net.cpp:1095] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0815 20:55:05.542405 20241 net.cpp:1095] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0815 20:55:05.542784 20241 net.cpp:1095] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0815 20:55:05.542790 20241 net.cpp:1095] Copying source layer pool2 Type:Pooling #blobs=0
I0815 20:55:05.542793 20241 net.cpp:1095] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0815 20:55:05.542831 20241 net.cpp:1095] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0815 20:55:05.543202 20241 net.cpp:1095] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0815 20:55:05.543208 20241 net.cpp:1095] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0815 20:55:05.543231 20241 net.cpp:1095] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0815 20:55:05.543588 20241 net.cpp:1095] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0815 20:55:05.543594 20241 net.cpp:1095] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I0815 20:55:05.543596 20241 net.cpp:1095] Copying source layer pool3 Type:Pooling #blobs=0
I0815 20:55:05.543599 20241 net.cpp:1095] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0815 20:55:05.543710 20241 net.cpp:1095] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0815 20:55:05.544073 20241 net.cpp:1095] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0815 20:55:05.544080 20241 net.cpp:1095] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0815 20:55:05.544159 20241 net.cpp:1095] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0815 20:55:05.544523 20241 net.cpp:1095] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0815 20:55:05.544528 20241 net.cpp:1095] Copying source layer pool4 Type:Pooling #blobs=0
I0815 20:55:05.544530 20241 net.cpp:1095] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0815 20:55:05.544893 20241 net.cpp:1095] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0815 20:55:05.545245 20241 net.cpp:1095] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0815 20:55:05.545251 20241 net.cpp:1095] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0815 20:55:05.545429 20241 net.cpp:1095] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0815 20:55:05.545795 20241 net.cpp:1095] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0815 20:55:05.545801 20241 net.cpp:1095] Copying source layer out5a Type:Convolution #blobs=2
I0815 20:55:05.545850 20241 net.cpp:1095] Copying source layer out5a/bn Type:BatchNorm #blobs=5
I0815 20:55:05.546017 20241 net.cpp:1095] Copying source layer out5a/relu Type:ReLU #blobs=0
I0815 20:55:05.546022 20241 net.cpp:1095] Copying source layer out5a_up2 Type:Deconvolution #blobs=1
I0815 20:55:05.546028 20241 net.cpp:1095] Copying source layer out3a Type:Convolution #blobs=2
I0815 20:55:05.546047 20241 net.cpp:1095] Copying source layer out3a/bn Type:BatchNorm #blobs=5
I0815 20:55:05.546198 20241 net.cpp:1095] Copying source layer out3a/relu Type:ReLU #blobs=0
I0815 20:55:05.546205 20241 net.cpp:1095] Copying source layer out3_out5_combined Type:Eltwise #blobs=0
I0815 20:55:05.546207 20241 net.cpp:1095] Copying source layer ctx_conv1 Type:Convolution #blobs=2
I0815 20:55:05.546232 20241 net.cpp:1095] Copying source layer ctx_conv1/bn Type:BatchNorm #blobs=5
I0815 20:55:05.546383 20241 net.cpp:1095] Copying source layer ctx_conv1/relu Type:ReLU #blobs=0
I0815 20:55:05.546388 20241 net.cpp:1095] Copying source layer ctx_conv2 Type:Convolution #blobs=2
I0815 20:55:05.546413 20241 net.cpp:1095] Copying source layer ctx_conv2/bn Type:BatchNorm #blobs=5
I0815 20:55:05.546589 20241 net.cpp:1095] Copying source layer ctx_conv2/relu Type:ReLU #blobs=0
I0815 20:55:05.546596 20241 net.cpp:1095] Copying source layer ctx_conv3 Type:Convolution #blobs=2
I0815 20:55:05.546617 20241 net.cpp:1095] Copying source layer ctx_conv3/bn Type:BatchNorm #blobs=5
I0815 20:55:05.546768 20241 net.cpp:1095] Copying source layer ctx_conv3/relu Type:ReLU #blobs=0
I0815 20:55:05.546774 20241 net.cpp:1095] Copying source layer ctx_conv4 Type:Convolution #blobs=2
I0815 20:55:05.546797 20241 net.cpp:1095] Copying source layer ctx_conv4/bn Type:BatchNorm #blobs=5
I0815 20:55:05.546964 20241 net.cpp:1095] Copying source layer ctx_conv4/relu Type:ReLU #blobs=0
I0815 20:55:05.546972 20241 net.cpp:1095] Copying source layer ctx_final Type:Convolution #blobs=2
I0815 20:55:05.546980 20241 net.cpp:1095] Copying source layer ctx_final/relu Type:ReLU #blobs=0
I0815 20:55:05.546984 20241 net.cpp:1095] Copying source layer out_deconv_final_up2 Type:Deconvolution #blobs=1
I0815 20:55:05.546988 20241 net.cpp:1095] Copying source layer out_deconv_final_up4 Type:Deconvolution #blobs=1
I0815 20:55:05.546994 20241 net.cpp:1095] Copying source layer out_deconv_final_up8 Type:Deconvolution #blobs=1
I0815 20:55:05.547000 20241 net.cpp:1095] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0815 20:55:05.547106 20241 parallel.cpp:106] [0 - 0] P2pSync adding callback
I0815 20:55:05.547112 20241 parallel.cpp:106] [1 - 1] P2pSync adding callback
I0815 20:55:05.547114 20241 parallel.cpp:106] [2 - 2] P2pSync adding callback
I0815 20:55:05.547116 20241 parallel.cpp:59] Starting Optimization
I0815 20:55:05.547118 20241 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 20:55:05.547147 20241 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 20:55:05.547159 20241 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 20:55:05.547842 20340 device_alternate.hpp:116] NVML initialized on thread 135814615820032
I0815 20:55:05.568855 20340 common.cpp:583] NVML succeeded to set CPU affinity on device 0
I0815 20:55:05.568903 20341 device_alternate.hpp:116] NVML initialized on thread 135814607427328
I0815 20:55:05.569682 20341 common.cpp:583] NVML succeeded to set CPU affinity on device 1
I0815 20:55:05.569723 20342 device_alternate.hpp:116] NVML initialized on thread 135814599034624
I0815 20:55:05.570462 20342 common.cpp:583] NVML succeeded to set CPU affinity on device 2
I0815 20:55:05.574446 20341 solver.cpp:42] Solver data type: FLOAT
W0815 20:55:05.575291 20341 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0815 20:55:05.575400 20341 net.cpp:104] Using FLOAT as default forward math type
I0815 20:55:05.575405 20341 net.cpp:110] Using FLOAT as default backward math type
I0815 20:55:05.575435 20341 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0815 20:55:05.575443 20341 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 20:55:05.578485 20342 solver.cpp:42] Solver data type: FLOAT
W0815 20:55:05.579005 20342 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0815 20:55:05.579107 20342 net.cpp:104] Using FLOAT as default forward math type
I0815 20:55:05.579113 20342 net.cpp:110] Using FLOAT as default backward math type
I0815 20:55:05.579138 20342 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0815 20:55:05.579144 20342 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 20:55:05.579170 20343 db_lmdb.cpp:24] Opened lmdb data/train-image-lmdb
I0815 20:55:05.579887 20344 db_lmdb.cpp:24] Opened lmdb data/train-image-lmdb
I0815 20:55:05.583672 20341 data_layer.cpp:185] [1] ReshapePrefetch 6, 3, 640, 640
I0815 20:55:05.584003 20342 data_layer.cpp:185] [2] ReshapePrefetch 6, 3, 640, 640
I0815 20:55:05.584041 20341 data_layer.cpp:209] [1] Output data size: 6, 3, 640, 640
I0815 20:55:05.584051 20341 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 20:55:05.584065 20342 data_layer.cpp:209] [2] Output data size: 6, 3, 640, 640
I0815 20:55:05.584071 20342 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 20:55:05.584262 20342 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0815 20:55:05.584262 20341 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0815 20:55:05.584278 20341 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 20:55:05.584280 20342 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 20:55:05.585237 20346 data_layer.cpp:97] [1] Parser threads: 1
I0815 20:55:05.585268 20346 data_layer.cpp:99] [1] Transformer threads: 1
I0815 20:55:05.590941 20345 data_layer.cpp:97] [2] Parser threads: 1
I0815 20:55:05.591079 20345 data_layer.cpp:99] [2] Transformer threads: 1
I0815 20:55:05.597431 20347 db_lmdb.cpp:24] Opened lmdb data/train-label-lmdb
I0815 20:55:05.599140 20348 db_lmdb.cpp:24] Opened lmdb data/train-label-lmdb
I0815 20:55:05.599731 20341 data_layer.cpp:185] [1] ReshapePrefetch 6, 1, 640, 640
I0815 20:55:05.599917 20341 data_layer.cpp:209] [1] Output data size: 6, 1, 640, 640
I0815 20:55:05.599934 20341 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 20:55:05.603819 20349 data_layer.cpp:97] [1] Parser threads: 1
I0815 20:55:05.603853 20349 data_layer.cpp:99] [1] Transformer threads: 1
I0815 20:55:05.607861 20342 data_layer.cpp:185] [2] ReshapePrefetch 6, 1, 640, 640
I0815 20:55:05.608286 20342 data_layer.cpp:209] [2] Output data size: 6, 1, 640, 640
I0815 20:55:05.608410 20342 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 20:55:05.610231 20346 blocking_queue.cpp:40] Waiting for datum
I0815 20:55:05.620275 20350 data_layer.cpp:97] [2] Parser threads: 1
I0815 20:55:05.620321 20350 data_layer.cpp:99] [2] Transformer threads: 1
I0815 20:55:06.116806 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 7.99G, req 0G)
I0815 20:55:06.166353 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.82G, req 0G)
I0815 20:55:06.176913 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 7.99G, req 0G)
I0815 20:55:06.208263 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.7G, req 0G)
I0815 20:55:06.226871 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.83G, req 0G)
I0815 20:55:06.232194 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.62G, req 0G)
I0815 20:55:06.255568 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 7.56G, req 0G)
I0815 20:55:06.267984 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.51G, req 0G)
I0815 20:55:06.270162 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.7G, req 0G)
I0815 20:55:06.295128 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.62G, req 0G)
I0815 20:55:06.299576 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.48G, req 0G)
I0815 20:55:06.311136 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.46G, req 0G)
I0815 20:55:06.319959 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 7.56G, req 0G)
I0815 20:55:06.332059 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.52G, req 0G)
I0815 20:55:06.360409 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.48G, req 0G)
I0815 20:55:06.371050 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 7.39G, req 0G)
I0815 20:55:06.372742 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.46G, req 0G)
I0815 20:55:06.389614 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 7.34G, req 0G)
I0815 20:55:06.410089 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 7.31G, req 0G)
I0815 20:55:06.413079 20341 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/l1reg/test.prototxt
W0815 20:55:06.413146 20341 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0815 20:55:06.413257 20341 net.cpp:104] Using FLOAT as default forward math type
I0815 20:55:06.413261 20341 net.cpp:110] Using FLOAT as default backward math type
I0815 20:55:06.413282 20341 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0815 20:55:06.413287 20341 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 20:55:06.413985 20385 db_lmdb.cpp:24] Opened lmdb data/val-image-lmdb
I0815 20:55:06.416313 20341 data_layer.cpp:185] (1) ReshapePrefetch 2, 3, 640, 640
I0815 20:55:06.416709 20341 data_layer.cpp:209] (1) Output data size: 2, 3, 640, 640
I0815 20:55:06.416723 20341 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 20:55:06.416842 20341 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0815 20:55:06.416857 20341 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 20:55:06.417670 20386 data_layer.cpp:97] (1) Parser threads: 1
I0815 20:55:06.417685 20386 data_layer.cpp:99] (1) Transformer threads: 1
I0815 20:55:06.419996 20387 db_lmdb.cpp:24] Opened lmdb data/val-label-lmdb
I0815 20:55:06.421015 20341 data_layer.cpp:185] (1) ReshapePrefetch 2, 1, 640, 640
I0815 20:55:06.421188 20341 data_layer.cpp:209] (1) Output data size: 2, 1, 640, 640
I0815 20:55:06.421223 20341 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 20:55:06.422780 20388 data_layer.cpp:97] (1) Parser threads: 1
I0815 20:55:06.422796 20388 data_layer.cpp:99] (1) Transformer threads: 1
I0815 20:55:06.433030 20341 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.18G, req 0G)
I0815 20:55:06.441244 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 7.39G, req 0G)
I0815 20:55:06.450827 20341 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.15G, req 0G)
I0815 20:55:06.458376 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 7.34G, req 0G)
I0815 20:55:06.461164 20341 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.13G, req 0G)
I0815 20:55:06.470839 20341 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.11G, req 0G)
I0815 20:55:06.478215 20341 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.1G, req 0G)
I0815 20:55:06.483044 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 7.32G, req 0G)
I0815 20:55:06.484740 20341 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.09G, req 0G)
I0815 20:55:06.487237 20342 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/l1reg/test.prototxt
W0815 20:55:06.487371 20342 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0815 20:55:06.487560 20342 net.cpp:104] Using FLOAT as default forward math type
I0815 20:55:06.487565 20342 net.cpp:110] Using FLOAT as default backward math type
I0815 20:55:06.487604 20342 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0815 20:55:06.487615 20342 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 20:55:06.488385 20389 db_lmdb.cpp:24] Opened lmdb data/val-image-lmdb
I0815 20:55:06.489799 20342 data_layer.cpp:185] (2) ReshapePrefetch 2, 3, 640, 640
I0815 20:55:06.489888 20342 data_layer.cpp:209] (2) Output data size: 2, 3, 640, 640
I0815 20:55:06.489895 20342 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 20:55:06.489936 20342 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0815 20:55:06.489945 20342 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 20:55:06.490726 20390 data_layer.cpp:97] (2) Parser threads: 1
I0815 20:55:06.490741 20390 data_layer.cpp:99] (2) Transformer threads: 1
I0815 20:55:06.493162 20391 db_lmdb.cpp:24] Opened lmdb data/val-label-lmdb
I0815 20:55:06.494156 20342 data_layer.cpp:185] (2) ReshapePrefetch 2, 1, 640, 640
I0815 20:55:06.494273 20342 data_layer.cpp:209] (2) Output data size: 2, 1, 640, 640
I0815 20:55:06.494282 20342 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 20:55:06.495847 20392 data_layer.cpp:97] (2) Parser threads: 1
I0815 20:55:06.495859 20392 data_layer.cpp:99] (2) Transformer threads: 1
I0815 20:55:06.502408 20341 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.08G, req 0G)
I0815 20:55:06.506176 20342 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.18G, req 0G)
I0815 20:55:06.512889 20341 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.08G, req 0G)
I0815 20:55:06.523638 20342 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.15G, req 0G)
I0815 20:55:06.534602 20342 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.13G, req 0G)
I0815 20:55:06.543901 20342 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.12G, req 0G)
I0815 20:55:06.553189 20342 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.1G, req 0G)
I0815 20:55:06.560873 20342 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.09G, req 0G)
I0815 20:55:06.568480 20341 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 7.07G, req 0G)
I0815 20:55:06.577419 20341 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 7.05G, req 0G)
I0815 20:55:06.577932 20342 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.09G, req 0G)
I0815 20:55:06.588953 20342 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.08G, req 0G)
I0815 20:55:06.593104 20341 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 7.05G, req 0G)
I0815 20:55:06.595505 20341 solver.cpp:56] Solver scaffolding done.
I0815 20:55:06.654067 20342 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 7.07G, req 0G)
I0815 20:55:06.661461 20342 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 7.06G, req 0G)
I0815 20:55:06.676239 20342 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 7.05G, req 0G)
I0815 20:55:06.679085 20342 solver.cpp:56] Solver scaffolding done.
I0815 20:55:06.745313 20341 parallel.cpp:161] [1 - 1] P2pSync adding callback
I0815 20:55:06.745339 20340 parallel.cpp:161] [0 - 0] P2pSync adding callback
I0815 20:55:06.745362 20342 parallel.cpp:161] [2 - 2] P2pSync adding callback
I0815 20:55:06.927949 20342 solver.cpp:438] Solving jsegnet21v2_train
I0815 20:55:06.927966 20342 solver.cpp:439] Learning Rate Policy: multistep
I0815 20:55:06.927975 20340 solver.cpp:438] Solving jsegnet21v2_train
I0815 20:55:06.927975 20341 solver.cpp:438] Solving jsegnet21v2_train
I0815 20:55:06.927985 20340 solver.cpp:439] Learning Rate Policy: multistep
I0815 20:55:06.928000 20341 solver.cpp:439] Learning Rate Policy: multistep
I0815 20:55:06.941558 20341 solver.cpp:227] Starting Optimization on GPU 1
I0815 20:55:06.941561 20342 solver.cpp:227] Starting Optimization on GPU 2
I0815 20:55:06.941587 20340 solver.cpp:227] Starting Optimization on GPU 0
I0815 20:55:06.941751 20340 solver.cpp:509] Iteration 0, Testing net (#0)
I0815 20:55:06.941779 20393 device_alternate.hpp:116] NVML initialized on thread 127814439167744
I0815 20:55:06.941794 20393 common.cpp:583] NVML succeeded to set CPU affinity on device 2
I0815 20:55:06.941803 20394 device_alternate.hpp:116] NVML initialized on thread 127814447560448
I0815 20:55:06.941814 20394 common.cpp:583] NVML succeeded to set CPU affinity on device 1
I0815 20:55:06.941889 20395 device_alternate.hpp:116] NVML initialized on thread 127814430775040
I0815 20:55:06.941905 20395 common.cpp:583] NVML succeeded to set CPU affinity on device 0
I0815 20:55:06.954310 20342 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 6.96G, req 0G)
I0815 20:55:06.959652 20341 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 6.95G, req 0G)
I0815 20:55:06.990298 20342 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 6.9G, req 0G)
I0815 20:55:07.001977 20341 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 6.9G, req 0G)
I0815 20:55:07.012328 20342 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 6.84G, req 0G)
I0815 20:55:07.017201 20341 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 6.84G, req 0G)
I0815 20:55:07.026374 20342 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 6.81G, req 0G)
I0815 20:55:07.027462 20341 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 6.81G, req 0G)
I0815 20:55:07.031258 20340 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.01G/1 1  (limit 6.88G, req 0G)
I0815 20:55:07.034688 20342 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 6.78G, req 0G)
I0815 20:55:07.037163 20341 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 6.77G, req 0G)
I0815 20:55:07.041093 20342 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 6.76G, req 0G)
I0815 20:55:07.043346 20341 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 6.76G, req 0G)
I0815 20:55:07.051816 20340 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 6.82G, req 0G)
I0815 20:55:07.054707 20342 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.74G, req 0G)
I0815 20:55:07.056615 20341 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.74G, req 0G)
I0815 20:55:07.059545 20342 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.74G, req 0G)
I0815 20:55:07.060940 20341 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.73G, req 0G)
I0815 20:55:07.065738 20340 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 6.75G, req 0G)
I0815 20:55:07.074611 20340 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 6.73G, req 0G)
I0815 20:55:07.084028 20340 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 6.69G, req 0G)
I0815 20:55:07.086625 20342 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.58G, req 0G)
I0815 20:55:07.088049 20341 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.58G, req 0G)
I0815 20:55:07.090801 20340 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 6.67G, req 0G)
I0815 20:55:07.092324 20342 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.57G, req 0G)
I0815 20:55:07.093493 20341 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.57G, req 0G)
I0815 20:55:07.099001 20340 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.66G, req 0G)
I0815 20:55:07.104575 20340 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.65G, req 0G)
I0815 20:55:07.115103 20342 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.48G, req 0G)
I0815 20:55:07.116997 20341 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.48G, req 0G)
I0815 20:55:07.130798 20340 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.5G, req 0G)
I0815 20:55:07.139092 20340 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.49G, req 0G)
I0815 20:55:07.158143 20340 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.39G, req 0G)
I0815 20:55:07.265287 20340 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.912905
I0815 20:55:07.265306 20340 solver.cpp:594]     Test net output #1: accuracy/top5 = 1
I0815 20:55:07.265313 20340 solver.cpp:594]     Test net output #2: loss = 0.207345 (* 1 = 0.207345 loss)
I0815 20:55:07.265321 20340 solver.cpp:254] [MultiGPU] Initial Test completed
I0815 20:55:07.375529 20340 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 6.19G, req 0G)
I0815 20:55:07.382421 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 6.27G, req 0G)
I0815 20:55:07.382601 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 6.28G, req 0G)
I0815 20:55:07.427453 20340 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 6.03G, req 0G)
I0815 20:55:07.437292 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 6.12G, req 0G)
I0815 20:55:07.437455 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 6.11G, req 0G)
I0815 20:55:07.470415 20340 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 1 4 3  (limit 5.85G, req 0G)
I0815 20:55:07.485597 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 5.93G, req 0G)
I0815 20:55:07.486256 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 5.94G, req 0G)
I0815 20:55:07.492489 20340 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 5.77G, req 0G)
I0815 20:55:07.509840 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 5.85G, req 0G)
I0815 20:55:07.510592 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 5.86G, req 0G)
I0815 20:55:07.514379 20340 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 5.68G, req 0G)
I0815 20:55:07.526824 20340 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 5.64G, req 0G)
I0815 20:55:07.535346 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 1  (limit 5.76G, req 0G)
I0815 20:55:07.537411 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 5.77G, req 0G)
I0815 20:55:07.547621 20340 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 5.61G, req 0G)
I0815 20:55:07.548118 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 5.72G, req 0G)
I0815 20:55:07.552491 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 5.72G, req 0G)
I0815 20:55:07.555820 20340 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 5.59G, req 0G)
I0815 20:55:07.571069 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 5.69G, req 0G)
I0815 20:55:07.575003 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 5.7G, req 0G)
I0815 20:55:07.579576 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 5.67G, req 0G)
I0815 20:55:07.584277 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 5.68G, req 0G)
I0815 20:55:07.602763 20340 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 5.32G, req 0G)
I0815 20:55:07.616556 20340 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 5.3G, req 0G)
I0815 20:55:07.629063 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 5.4G, req 0G)
I0815 20:55:07.634802 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 5.41G, req 0G)
I0815 20:55:07.646373 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 5.38G, req 0G)
I0815 20:55:07.646746 20340 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 5.14G, req 0G)
I0815 20:55:07.649763 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 5.39G, req 0G)
I0815 20:55:07.691057 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 5.23G, req 0G)
I0815 20:55:07.693806 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 5.23G, req 0G)
I0815 20:55:07.879531 20340 solver.cpp:317] Iteration 0 (0.614164 s), loss = 0.0996983
I0815 20:55:07.879551 20340 solver.cpp:334]     Train net output #0: loss = 0.0996983 (* 1 = 0.0996983 loss)
I0815 20:55:07.879557 20340 sgd_solver.cpp:136] Iteration 0, lr = 1e-05, m = 0.9
I0815 20:55:08.085065 20340 solver.cpp:317] Iteration 1 (0.205519 s), loss = 0.114394
I0815 20:55:08.085094 20340 solver.cpp:334]     Train net output #0: loss = 0.114394 (* 1 = 0.114394 loss)
I0815 20:55:08.173406 20340 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1a' with space 1.29G/1 1 0 3  (limit 2.99G, req 0G)
I0815 20:55:08.182677 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1a' with space 1.29G/1 1 0 0  (limit 3.08G, req 0G)
I0815 20:55:08.183941 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1a' with space 1.29G/1 1 0 3  (limit 3.08G, req 0G)
I0815 20:55:08.234228 20340 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1b' with space 2.57G/2 6 4 3  (limit 1.71G, req 0G)
I0815 20:55:08.246400 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0G)
I0815 20:55:08.249250 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0G)
I0815 20:55:08.359125 20340 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 2.57G/1 6 4 3  (limit 1.71G, req 0G)
I0815 20:55:08.379487 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 2.57G/1 6 4 3  (limit 1.79G, req 0G)
I0815 20:55:08.383771 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 2.57G/1 6 4 3  (limit 1.79G, req 0G)
I0815 20:55:08.397979 20340 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 2.57G/2 6 4 3  (limit 1.71G, req 0G)
I0815 20:55:08.422083 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0G)
I0815 20:55:08.426616 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0G)
I0815 20:55:08.491181 20340 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 2.57G/1 6 4 5  (limit 1.71G, req 0.07G)
I0815 20:55:08.512953 20340 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 2.57G/2 6 4 3  (limit 1.71G, req 0.07G)
I0815 20:55:08.521044 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 2.57G/1 6 4 5  (limit 1.79G, req 0.07G)
I0815 20:55:08.526093 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 2.57G/1 6 4 5  (limit 1.79G, req 0.07G)
I0815 20:55:08.544591 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0.07G)
I0815 20:55:08.550048 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0.07G)
I0815 20:55:08.575569 20340 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 2.57G/1 6 4 5  (limit 1.71G, req 0.07G)
I0815 20:55:08.588523 20340 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 2.57G/2 6 4 3  (limit 1.71G, req 0.07G)
I0815 20:55:08.614038 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 2.57G/1 6 4 5  (limit 1.79G, req 0.07G)
I0815 20:55:08.619843 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 2.57G/1 6 4 5  (limit 1.79G, req 0.07G)
I0815 20:55:08.627851 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0.07G)
I0815 20:55:08.633585 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0.07G)
I0815 20:55:08.638731 20340 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'out3a' with space 2.57G/2 6 4 3  (limit 1.71G, req 0.07G)
I0815 20:55:08.681958 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'out3a' with space 2.57G/2 6 4 3  (limit 1.79G, req 0.07G)
I0815 20:55:08.689226 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'out3a' with space 2.57G/2 6 4 3  (limit 1.79G, req 0.07G)
I0815 20:55:08.691432 20340 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 2.57G/1 6 4 3  (limit 1.71G, req 0.07G)
I0815 20:55:08.717326 20340 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 2.57G/1 6 1 3  (limit 1.71G, req 0.07G)
I0815 20:55:08.738029 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 2.57G/1 6 4 3  (limit 1.79G, req 0.07G)
I0815 20:55:08.745391 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 2.57G/1 6 4 3  (limit 1.79G, req 0.07G)
I0815 20:55:08.765918 20342 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 2.57G/1 6 1 5  (limit 1.79G, req 0.07G)
I0815 20:55:08.772995 20341 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 2.57G/1 6 1 5  (limit 1.79G, req 0.07G)
I0815 20:55:08.903450 20340 solver.cpp:317] Iteration 2 (0.818358 s), loss = 0.0715012
I0815 20:55:08.903475 20340 solver.cpp:334]     Train net output #0: loss = 0.0715012 (* 1 = 0.0715012 loss)
I0815 20:55:08.904083 20342 cudnn_conv_layer.cpp:292] [2] Layer 'conv1a' reallocating workspace: 2.57G -> 0.14G
I0815 20:55:08.911759 20341 cudnn_conv_layer.cpp:292] [1] Layer 'conv1a' reallocating workspace: 2.57G -> 0.14G
I0815 20:55:08.914017 20340 cudnn_conv_layer.cpp:292] [0] Layer 'conv1a' reallocating workspace: 2.57G -> 0.14G
I0815 20:55:27.785809 20340 solver.cpp:312] Iteration 100 (5.19017 iter/s, 18.8818s/98 iter), loss = 0.0744315
I0815 20:55:27.785831 20340 solver.cpp:334]     Train net output #0: loss = 0.0744315 (* 1 = 0.0744315 loss)
I0815 20:55:27.785837 20340 sgd_solver.cpp:136] Iteration 100, lr = 1e-05, m = 0.9
I0815 20:55:39.810969 20316 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 20:55:47.184540 20340 solver.cpp:312] Iteration 200 (5.15512 iter/s, 19.3982s/100 iter), loss = 0.114652
I0815 20:55:47.184564 20340 solver.cpp:334]     Train net output #0: loss = 0.114652 (* 1 = 0.114652 loss)
I0815 20:55:47.184571 20340 sgd_solver.cpp:136] Iteration 200, lr = 1e-05, m = 0.9
I0815 20:56:06.807045 20340 solver.cpp:312] Iteration 300 (5.09633 iter/s, 19.622s/100 iter), loss = 0.0806429
I0815 20:56:06.807075 20340 solver.cpp:334]     Train net output #0: loss = 0.0806429 (* 1 = 0.0806429 loss)
I0815 20:56:06.807082 20340 sgd_solver.cpp:136] Iteration 300, lr = 1e-05, m = 0.9
I0815 20:56:12.025400 20343 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 20:56:26.173642 20340 solver.cpp:312] Iteration 400 (5.16367 iter/s, 19.3661s/100 iter), loss = 0.131283
I0815 20:56:26.173665 20340 solver.cpp:334]     Train net output #0: loss = 0.131283 (* 1 = 0.131283 loss)
I0815 20:56:26.173669 20340 sgd_solver.cpp:136] Iteration 400, lr = 1e-05, m = 0.9
I0815 20:56:45.718008 20340 solver.cpp:312] Iteration 500 (5.1167 iter/s, 19.5438s/100 iter), loss = 0.0862497
I0815 20:56:45.718065 20340 solver.cpp:334]     Train net output #0: loss = 0.0862497 (* 1 = 0.0862497 loss)
I0815 20:56:45.718071 20340 sgd_solver.cpp:136] Iteration 500, lr = 1e-05, m = 0.9
I0815 20:57:04.919220 20340 solver.cpp:312] Iteration 600 (5.20815 iter/s, 19.2007s/100 iter), loss = 0.0724129
I0815 20:57:04.919242 20340 solver.cpp:334]     Train net output #0: loss = 0.0724129 (* 1 = 0.0724129 loss)
I0815 20:57:04.919247 20340 sgd_solver.cpp:136] Iteration 600, lr = 1e-05, m = 0.9
I0815 20:57:16.190793 20348 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 20:57:24.244139 20340 solver.cpp:312] Iteration 700 (5.17481 iter/s, 19.3244s/100 iter), loss = 0.0915192
I0815 20:57:24.244165 20340 solver.cpp:334]     Train net output #0: loss = 0.0915191 (* 1 = 0.0915191 loss)
I0815 20:57:24.244171 20340 sgd_solver.cpp:136] Iteration 700, lr = 1e-05, m = 0.9
I0815 20:57:43.718833 20340 solver.cpp:312] Iteration 800 (5.13501 iter/s, 19.4742s/100 iter), loss = 0.0979365
I0815 20:57:43.718859 20340 solver.cpp:334]     Train net output #0: loss = 0.0979365 (* 1 = 0.0979365 loss)
I0815 20:57:43.718864 20340 sgd_solver.cpp:136] Iteration 800, lr = 1e-05, m = 0.9
I0815 20:58:03.339042 20340 solver.cpp:312] Iteration 900 (5.09693 iter/s, 19.6197s/100 iter), loss = 0.0786026
I0815 20:58:03.339159 20340 solver.cpp:334]     Train net output #0: loss = 0.0786026 (* 1 = 0.0786026 loss)
I0815 20:58:03.339167 20340 sgd_solver.cpp:136] Iteration 900, lr = 1e-05, m = 0.9
I0815 20:58:20.497809 20316 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 20:58:22.843394 20340 solver.cpp:312] Iteration 1000 (5.1272 iter/s, 19.5038s/100 iter), loss = 0.0951657
I0815 20:58:22.843417 20340 solver.cpp:334]     Train net output #0: loss = 0.0951657 (* 1 = 0.0951657 loss)
I0815 20:58:22.843423 20340 sgd_solver.cpp:136] Iteration 1000, lr = 1e-05, m = 0.9
I0815 20:58:42.616354 20340 solver.cpp:312] Iteration 1100 (5.05755 iter/s, 19.7724s/100 iter), loss = 0.0658986
I0815 20:58:42.616405 20340 solver.cpp:334]     Train net output #0: loss = 0.0658986 (* 1 = 0.0658986 loss)
I0815 20:58:42.616412 20340 sgd_solver.cpp:136] Iteration 1100, lr = 1e-05, m = 0.9
I0815 20:58:52.853037 20344 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 20:59:01.904433 20340 solver.cpp:312] Iteration 1200 (5.18469 iter/s, 19.2875s/100 iter), loss = 0.0840935
I0815 20:59:01.904458 20340 solver.cpp:334]     Train net output #0: loss = 0.0840935 (* 1 = 0.0840935 loss)
I0815 20:59:01.904462 20340 sgd_solver.cpp:136] Iteration 1200, lr = 1e-05, m = 0.9
I0815 20:59:21.213376 20340 solver.cpp:312] Iteration 1300 (5.17909 iter/s, 19.3084s/100 iter), loss = 0.094995
I0815 20:59:21.213435 20340 solver.cpp:334]     Train net output #0: loss = 0.0949949 (* 1 = 0.0949949 loss)
I0815 20:59:21.213443 20340 sgd_solver.cpp:136] Iteration 1300, lr = 1e-05, m = 0.9
I0815 20:59:40.597743 20340 solver.cpp:312] Iteration 1400 (5.15894 iter/s, 19.3838s/100 iter), loss = 0.117895
I0815 20:59:40.597774 20340 solver.cpp:334]     Train net output #0: loss = 0.117895 (* 1 = 0.117895 loss)
I0815 20:59:40.597780 20340 sgd_solver.cpp:136] Iteration 1400, lr = 1e-05, m = 0.9
I0815 20:59:56.967591 20343 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 21:00:00.062680 20340 solver.cpp:312] Iteration 1500 (5.13758 iter/s, 19.4644s/100 iter), loss = 0.103425
I0815 21:00:00.062703 20340 solver.cpp:334]     Train net output #0: loss = 0.103425 (* 1 = 0.103425 loss)
I0815 21:00:00.062711 20340 sgd_solver.cpp:136] Iteration 1500, lr = 1e-05, m = 0.9
I0815 21:00:19.572815 20340 solver.cpp:312] Iteration 1600 (5.12568 iter/s, 19.5096s/100 iter), loss = 0.0738339
I0815 21:00:19.572840 20340 solver.cpp:334]     Train net output #0: loss = 0.0738338 (* 1 = 0.0738338 loss)
I0815 21:00:19.572844 20340 sgd_solver.cpp:136] Iteration 1600, lr = 1e-05, m = 0.9
I0815 21:00:39.184069 20340 solver.cpp:312] Iteration 1700 (5.09925 iter/s, 19.6107s/100 iter), loss = 0.101573
I0815 21:00:39.184123 20340 solver.cpp:334]     Train net output #0: loss = 0.101573 (* 1 = 0.101573 loss)
I0815 21:00:39.184135 20340 sgd_solver.cpp:136] Iteration 1700, lr = 1e-05, m = 0.9
I0815 21:00:58.712644 20340 solver.cpp:312] Iteration 1800 (5.12084 iter/s, 19.528s/100 iter), loss = 0.0650994
I0815 21:00:58.712673 20340 solver.cpp:334]     Train net output #0: loss = 0.0650993 (* 1 = 0.0650993 loss)
I0815 21:00:58.712679 20340 sgd_solver.cpp:136] Iteration 1800, lr = 1e-05, m = 0.9
I0815 21:01:01.710003 20347 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 21:01:18.367207 20340 solver.cpp:312] Iteration 1900 (5.08802 iter/s, 19.654s/100 iter), loss = 0.057876
I0815 21:01:18.367271 20340 solver.cpp:334]     Train net output #0: loss = 0.0578759 (* 1 = 0.0578759 loss)
I0815 21:01:18.367280 20340 sgd_solver.cpp:136] Iteration 1900, lr = 1e-05, m = 0.9
I0815 21:01:33.820943 20314 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 21:01:37.350908 20340 solver.cpp:509] Iteration 2000, Testing net (#0)
I0815 21:01:49.513990 20340 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.951425
I0815 21:01:49.514056 20340 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999695
I0815 21:01:49.514065 20340 solver.cpp:594]     Test net output #2: loss = 0.172523 (* 1 = 0.172523 loss)
I0815 21:01:49.514094 20340 solver.cpp:264] [MultiGPU] Tests completed in 12.1629s
I0815 21:01:49.706238 20340 solver.cpp:312] Iteration 2000 (3.191 iter/s, 31.3382s/100 iter), loss = 0.0721825
I0815 21:01:49.706262 20340 solver.cpp:334]     Train net output #0: loss = 0.0721824 (* 1 = 0.0721824 loss)
I0815 21:01:49.706269 20340 sgd_solver.cpp:136] Iteration 2000, lr = 1e-05, m = 0.9
I0815 21:02:09.114236 20340 solver.cpp:312] Iteration 2100 (5.15266 iter/s, 19.4075s/100 iter), loss = 0.0743474
I0815 21:02:09.114256 20340 solver.cpp:334]     Train net output #0: loss = 0.0743472 (* 1 = 0.0743472 loss)
I0815 21:02:09.114261 20340 sgd_solver.cpp:136] Iteration 2100, lr = 1e-05, m = 0.9
I0815 21:02:17.851065 20344 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 21:02:28.476832 20340 solver.cpp:312] Iteration 2200 (5.16474 iter/s, 19.3621s/100 iter), loss = 0.141381
I0815 21:02:28.476912 20340 solver.cpp:334]     Train net output #0: loss = 0.141381 (* 1 = 0.141381 loss)
I0815 21:02:28.476918 20340 sgd_solver.cpp:136] Iteration 2200, lr = 1e-05, m = 0.9
I0815 21:02:48.023267 20340 solver.cpp:312] Iteration 2300 (5.11616 iter/s, 19.5459s/100 iter), loss = 0.0734314
I0815 21:02:48.023288 20340 solver.cpp:334]     Train net output #0: loss = 0.0734312 (* 1 = 0.0734312 loss)
I0815 21:02:48.023291 20340 sgd_solver.cpp:136] Iteration 2300, lr = 1e-05, m = 0.9
I0815 21:03:07.396823 20340 solver.cpp:312] Iteration 2400 (5.16182 iter/s, 19.373s/100 iter), loss = 0.0668185
I0815 21:03:07.396906 20340 solver.cpp:334]     Train net output #0: loss = 0.0668184 (* 1 = 0.0668184 loss)
I0815 21:03:07.396914 20340 sgd_solver.cpp:136] Iteration 2400, lr = 1e-05, m = 0.9
I0815 21:03:22.203213 20344 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 21:03:26.845836 20340 solver.cpp:312] Iteration 2500 (5.14179 iter/s, 19.4485s/100 iter), loss = 0.0924095
I0815 21:03:26.845865 20340 solver.cpp:334]     Train net output #0: loss = 0.0924094 (* 1 = 0.0924094 loss)
I0815 21:03:26.845871 20340 sgd_solver.cpp:136] Iteration 2500, lr = 1e-05, m = 0.9
I0815 21:03:46.345978 20340 solver.cpp:312] Iteration 2600 (5.12831 iter/s, 19.4996s/100 iter), loss = 0.0842264
I0815 21:03:46.346060 20340 solver.cpp:334]     Train net output #0: loss = 0.0842263 (* 1 = 0.0842263 loss)
I0815 21:03:46.346073 20340 sgd_solver.cpp:136] Iteration 2600, lr = 1e-05, m = 0.9
I0815 21:03:54.447190 20344 data_reader.cpp:288] Starting prefetch of epoch 4
I0815 21:04:05.783952 20340 solver.cpp:312] Iteration 2700 (5.14471 iter/s, 19.4374s/100 iter), loss = 0.0704088
I0815 21:04:05.783977 20340 solver.cpp:334]     Train net output #0: loss = 0.0704086 (* 1 = 0.0704086 loss)
I0815 21:04:05.783983 20340 sgd_solver.cpp:136] Iteration 2700, lr = 1e-05, m = 0.9
I0815 21:04:24.779595 20340 solver.cpp:312] Iteration 2800 (5.26451 iter/s, 18.9951s/100 iter), loss = 0.0638916
I0815 21:04:24.779646 20340 solver.cpp:334]     Train net output #0: loss = 0.0638915 (* 1 = 0.0638915 loss)
I0815 21:04:24.779654 20340 sgd_solver.cpp:136] Iteration 2800, lr = 1e-05, m = 0.9
I0815 21:04:44.095306 20340 solver.cpp:312] Iteration 2900 (5.17728 iter/s, 19.3152s/100 iter), loss = 0.0920668
I0815 21:04:44.095330 20340 solver.cpp:334]     Train net output #0: loss = 0.0920667 (* 1 = 0.0920667 loss)
I0815 21:04:44.095338 20340 sgd_solver.cpp:136] Iteration 2900, lr = 1e-05, m = 0.9
I0815 21:04:57.798789 20314 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 21:05:03.358461 20340 solver.cpp:312] Iteration 3000 (5.1914 iter/s, 19.2626s/100 iter), loss = 0.104681
I0815 21:05:03.358489 20340 solver.cpp:334]     Train net output #0: loss = 0.104681 (* 1 = 0.104681 loss)
I0815 21:05:03.358496 20340 sgd_solver.cpp:136] Iteration 3000, lr = 1e-05, m = 0.9
I0815 21:05:22.978044 20340 solver.cpp:312] Iteration 3100 (5.09709 iter/s, 19.619s/100 iter), loss = 0.0455306
I0815 21:05:22.978065 20340 solver.cpp:334]     Train net output #0: loss = 0.0455305 (* 1 = 0.0455305 loss)
I0815 21:05:22.978070 20340 sgd_solver.cpp:136] Iteration 3100, lr = 1e-05, m = 0.9
I0815 21:05:42.476125 20340 solver.cpp:312] Iteration 3200 (5.12885 iter/s, 19.4975s/100 iter), loss = 0.169136
I0815 21:05:42.476187 20340 solver.cpp:334]     Train net output #0: loss = 0.169135 (* 1 = 0.169135 loss)
I0815 21:05:42.476192 20340 sgd_solver.cpp:136] Iteration 3200, lr = 1e-05, m = 0.9
I0815 21:06:01.859913 20340 solver.cpp:312] Iteration 3300 (5.15909 iter/s, 19.3833s/100 iter), loss = 0.0666914
I0815 21:06:01.859941 20340 solver.cpp:334]     Train net output #0: loss = 0.0666912 (* 1 = 0.0666912 loss)
I0815 21:06:01.859946 20340 sgd_solver.cpp:136] Iteration 3300, lr = 1e-05, m = 0.9
I0815 21:06:02.288660 20344 data_reader.cpp:288] Starting prefetch of epoch 5
I0815 21:06:21.409353 20340 solver.cpp:312] Iteration 3400 (5.11538 iter/s, 19.5489s/100 iter), loss = 0.0469009
I0815 21:06:21.409415 20340 solver.cpp:334]     Train net output #0: loss = 0.0469008 (* 1 = 0.0469008 loss)
I0815 21:06:21.409421 20340 sgd_solver.cpp:136] Iteration 3400, lr = 1e-05, m = 0.9
I0815 21:06:40.949355 20340 solver.cpp:312] Iteration 3500 (5.11785 iter/s, 19.5395s/100 iter), loss = 0.082622
I0815 21:06:40.949380 20340 solver.cpp:334]     Train net output #0: loss = 0.0826219 (* 1 = 0.0826219 loss)
I0815 21:06:40.949386 20340 sgd_solver.cpp:136] Iteration 3500, lr = 1e-05, m = 0.9
I0815 21:07:00.364210 20340 solver.cpp:312] Iteration 3600 (5.15084 iter/s, 19.4143s/100 iter), loss = 0.0767218
I0815 21:07:00.364293 20340 solver.cpp:334]     Train net output #0: loss = 0.0767217 (* 1 = 0.0767217 loss)
I0815 21:07:00.364300 20340 sgd_solver.cpp:136] Iteration 3600, lr = 1e-05, m = 0.9
I0815 21:07:06.820622 20344 data_reader.cpp:288] Starting prefetch of epoch 6
I0815 21:07:19.870383 20340 solver.cpp:312] Iteration 3700 (5.12672 iter/s, 19.5056s/100 iter), loss = 0.0821178
I0815 21:07:19.870405 20340 solver.cpp:334]     Train net output #0: loss = 0.0821177 (* 1 = 0.0821177 loss)
I0815 21:07:19.870409 20340 sgd_solver.cpp:136] Iteration 3700, lr = 1e-05, m = 0.9
I0815 21:07:38.910761 20343 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 21:07:39.269497 20340 solver.cpp:312] Iteration 3800 (5.15502 iter/s, 19.3986s/100 iter), loss = 0.0910058
I0815 21:07:39.269520 20340 solver.cpp:334]     Train net output #0: loss = 0.0910057 (* 1 = 0.0910057 loss)
I0815 21:07:39.269527 20340 sgd_solver.cpp:136] Iteration 3800, lr = 1e-05, m = 0.9
I0815 21:07:58.771150 20340 solver.cpp:312] Iteration 3900 (5.12791 iter/s, 19.5011s/100 iter), loss = 0.0697615
I0815 21:07:58.771175 20340 solver.cpp:334]     Train net output #0: loss = 0.0697614 (* 1 = 0.0697614 loss)
I0815 21:07:58.771181 20340 sgd_solver.cpp:136] Iteration 3900, lr = 1e-05, m = 0.9
I0815 21:08:17.858283 20340 solver.cpp:509] Iteration 4000, Testing net (#0)
I0815 21:08:21.569129 20387 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 21:08:30.276677 20340 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.950551
I0815 21:08:30.276697 20340 solver.cpp:594]     Test net output #1: accuracy/top5 = 1
I0815 21:08:30.276702 20340 solver.cpp:594]     Test net output #2: loss = 0.154649 (* 1 = 0.154649 loss)
I0815 21:08:30.276726 20340 solver.cpp:264] [MultiGPU] Tests completed in 12.4181s
I0815 21:08:30.473649 20340 solver.cpp:312] Iteration 4000 (3.15441 iter/s, 31.7016s/100 iter), loss = 0.0599244
I0815 21:08:30.473675 20340 solver.cpp:334]     Train net output #0: loss = 0.0599243 (* 1 = 0.0599243 loss)
I0815 21:08:30.473682 20340 sgd_solver.cpp:136] Iteration 4000, lr = 1e-05, m = 0.9
I0815 21:08:50.115857 20340 solver.cpp:312] Iteration 4100 (5.09122 iter/s, 19.6417s/100 iter), loss = 0.0955899
I0815 21:08:50.115926 20340 solver.cpp:334]     Train net output #0: loss = 0.0955898 (* 1 = 0.0955898 loss)
I0815 21:08:50.115933 20340 sgd_solver.cpp:136] Iteration 4100, lr = 1e-05, m = 0.9
I0815 21:08:55.483667 20316 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 21:09:09.510843 20340 solver.cpp:312] Iteration 4200 (5.15612 iter/s, 19.3944s/100 iter), loss = 0.0782703
I0815 21:09:09.510867 20340 solver.cpp:334]     Train net output #0: loss = 0.0782702 (* 1 = 0.0782702 loss)
I0815 21:09:09.510874 20340 sgd_solver.cpp:136] Iteration 4200, lr = 1e-05, m = 0.9
I0815 21:09:27.665464 20344 data_reader.cpp:288] Starting prefetch of epoch 7
I0815 21:09:28.819283 20340 solver.cpp:312] Iteration 4300 (5.17922 iter/s, 19.3079s/100 iter), loss = 0.0994151
I0815 21:09:28.819305 20340 solver.cpp:334]     Train net output #0: loss = 0.099415 (* 1 = 0.099415 loss)
I0815 21:09:28.819310 20340 sgd_solver.cpp:136] Iteration 4300, lr = 1e-05, m = 0.9
I0815 21:09:48.089741 20340 solver.cpp:312] Iteration 4400 (5.18943 iter/s, 19.2699s/100 iter), loss = 0.0585641
I0815 21:09:48.089768 20340 solver.cpp:334]     Train net output #0: loss = 0.058564 (* 1 = 0.058564 loss)
I0815 21:09:48.089776 20340 sgd_solver.cpp:136] Iteration 4400, lr = 1e-05, m = 0.9
I0815 21:10:07.548296 20340 solver.cpp:312] Iteration 4500 (5.13927 iter/s, 19.458s/100 iter), loss = 0.0499164
I0815 21:10:07.548344 20340 solver.cpp:334]     Train net output #0: loss = 0.0499162 (* 1 = 0.0499162 loss)
I0815 21:10:07.548352 20340 sgd_solver.cpp:136] Iteration 4500, lr = 1e-05, m = 0.9
I0815 21:10:26.849522 20340 solver.cpp:312] Iteration 4600 (5.18116 iter/s, 19.3007s/100 iter), loss = 0.111439
I0815 21:10:26.849547 20340 solver.cpp:334]     Train net output #0: loss = 0.111439 (* 1 = 0.111439 loss)
I0815 21:10:26.849551 20340 sgd_solver.cpp:136] Iteration 4600, lr = 1e-05, m = 0.9
I0815 21:10:31.547060 20316 data_reader.cpp:288] Starting prefetch of epoch 4
I0815 21:10:46.309079 20340 solver.cpp:312] Iteration 4700 (5.139 iter/s, 19.459s/100 iter), loss = 0.0744868
I0815 21:10:46.309159 20340 solver.cpp:334]     Train net output #0: loss = 0.0744867 (* 1 = 0.0744867 loss)
I0815 21:10:46.309166 20340 sgd_solver.cpp:136] Iteration 4700, lr = 1e-05, m = 0.9
I0815 21:11:05.603715 20340 solver.cpp:312] Iteration 4800 (5.18293 iter/s, 19.2941s/100 iter), loss = 0.0873141
I0815 21:11:05.603739 20340 solver.cpp:334]     Train net output #0: loss = 0.087314 (* 1 = 0.087314 loss)
I0815 21:11:05.603744 20340 sgd_solver.cpp:136] Iteration 4800, lr = 1e-05, m = 0.9
I0815 21:11:24.956303 20340 solver.cpp:312] Iteration 4900 (5.16741 iter/s, 19.3521s/100 iter), loss = 0.0928402
I0815 21:11:24.956357 20340 solver.cpp:334]     Train net output #0: loss = 0.0928401 (* 1 = 0.0928401 loss)
I0815 21:11:24.956362 20340 sgd_solver.cpp:136] Iteration 4900, lr = 1e-05, m = 0.9
I0815 21:11:35.660697 20347 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 21:11:44.431219 20340 solver.cpp:312] Iteration 5000 (5.13495 iter/s, 19.4744s/100 iter), loss = 0.0894408
I0815 21:11:44.431247 20340 solver.cpp:334]     Train net output #0: loss = 0.0894406 (* 1 = 0.0894406 loss)
I0815 21:11:44.431254 20340 sgd_solver.cpp:136] Iteration 5000, lr = 1e-05, m = 0.9
I0815 21:12:03.778939 20340 solver.cpp:312] Iteration 5100 (5.16871 iter/s, 19.3472s/100 iter), loss = 0.131269
I0815 21:12:03.778987 20340 solver.cpp:334]     Train net output #0: loss = 0.131268 (* 1 = 0.131268 loss)
I0815 21:12:03.778992 20340 sgd_solver.cpp:136] Iteration 5100, lr = 1e-05, m = 0.9
I0815 21:12:07.662277 20343 data_reader.cpp:288] Starting prefetch of epoch 4
I0815 21:12:23.216048 20340 solver.cpp:312] Iteration 5200 (5.14494 iter/s, 19.4366s/100 iter), loss = 0.0921541
I0815 21:12:23.216073 20340 solver.cpp:334]     Train net output #0: loss = 0.092154 (* 1 = 0.092154 loss)
I0815 21:12:23.216078 20340 sgd_solver.cpp:136] Iteration 5200, lr = 1e-05, m = 0.9
I0815 21:12:42.789566 20340 solver.cpp:312] Iteration 5300 (5.10908 iter/s, 19.573s/100 iter), loss = 0.055003
I0815 21:12:42.789635 20340 solver.cpp:334]     Train net output #0: loss = 0.0550028 (* 1 = 0.0550028 loss)
I0815 21:12:42.789643 20340 sgd_solver.cpp:136] Iteration 5300, lr = 1e-05, m = 0.9
I0815 21:13:02.127333 20340 solver.cpp:312] Iteration 5400 (5.17137 iter/s, 19.3372s/100 iter), loss = 0.0596323
I0815 21:13:02.127357 20340 solver.cpp:334]     Train net output #0: loss = 0.0596321 (* 1 = 0.0596321 loss)
I0815 21:13:02.127360 20340 sgd_solver.cpp:136] Iteration 5400, lr = 1e-05, m = 0.9
I0815 21:13:12.056370 20344 data_reader.cpp:288] Starting prefetch of epoch 8
I0815 21:13:21.520181 20340 solver.cpp:312] Iteration 5500 (5.15668 iter/s, 19.3923s/100 iter), loss = 0.0748437
I0815 21:13:21.520236 20340 solver.cpp:334]     Train net output #0: loss = 0.0748436 (* 1 = 0.0748436 loss)
I0815 21:13:21.520243 20340 sgd_solver.cpp:136] Iteration 5500, lr = 1e-05, m = 0.9
I0815 21:13:41.093893 20340 solver.cpp:312] Iteration 5600 (5.10903 iter/s, 19.5732s/100 iter), loss = 0.0395246
I0815 21:13:41.093919 20340 solver.cpp:334]     Train net output #0: loss = 0.0395244 (* 1 = 0.0395244 loss)
I0815 21:13:41.093925 20340 sgd_solver.cpp:136] Iteration 5600, lr = 1e-05, m = 0.9
I0815 21:14:00.504632 20340 solver.cpp:312] Iteration 5700 (5.15193 iter/s, 19.4102s/100 iter), loss = 0.126977
I0815 21:14:00.504684 20340 solver.cpp:334]     Train net output #0: loss = 0.126977 (* 1 = 0.126977 loss)
I0815 21:14:00.504689 20340 sgd_solver.cpp:136] Iteration 5700, lr = 1e-05, m = 0.9
I0815 21:14:16.295114 20347 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 21:14:20.037794 20340 solver.cpp:312] Iteration 5800 (5.11964 iter/s, 19.5326s/100 iter), loss = 0.0756217
I0815 21:14:20.037817 20340 solver.cpp:334]     Train net output #0: loss = 0.0756215 (* 1 = 0.0756215 loss)
I0815 21:14:20.037822 20340 sgd_solver.cpp:136] Iteration 5800, lr = 1e-05, m = 0.9
I0815 21:14:39.578085 20340 solver.cpp:312] Iteration 5900 (5.11777 iter/s, 19.5398s/100 iter), loss = 0.0763264
I0815 21:14:39.578136 20340 solver.cpp:334]     Train net output #0: loss = 0.0763262 (* 1 = 0.0763262 loss)
I0815 21:14:39.578142 20340 sgd_solver.cpp:136] Iteration 5900, lr = 1e-05, m = 0.9
I0815 21:14:48.560659 20314 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 21:14:58.900734 20340 solver.cpp:509] Iteration 6000, Testing net (#0)
I0815 21:15:10.856832 20340 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.950058
I0815 21:15:10.856887 20340 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.99956
I0815 21:15:10.856895 20340 solver.cpp:594]     Test net output #2: loss = 0.187184 (* 1 = 0.187184 loss)
I0815 21:15:10.856925 20340 solver.cpp:264] [MultiGPU] Tests completed in 11.9559s
I0815 21:15:11.070072 20340 solver.cpp:312] Iteration 6000 (3.1755 iter/s, 31.4911s/100 iter), loss = 0.106352
I0815 21:15:11.070096 20340 solver.cpp:334]     Train net output #0: loss = 0.106352 (* 1 = 0.106352 loss)
I0815 21:15:11.070102 20340 sgd_solver.cpp:136] Iteration 6000, lr = 1e-05, m = 0.9
I0815 21:15:30.490276 20340 solver.cpp:312] Iteration 6100 (5.14942 iter/s, 19.4197s/100 iter), loss = 0.0766106
I0815 21:15:30.490303 20340 solver.cpp:334]     Train net output #0: loss = 0.0766105 (* 1 = 0.0766105 loss)
I0815 21:15:30.490309 20340 sgd_solver.cpp:136] Iteration 6100, lr = 1e-05, m = 0.9
I0815 21:15:32.816047 20348 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 21:15:50.018755 20340 solver.cpp:312] Iteration 6200 (5.12087 iter/s, 19.5279s/100 iter), loss = 0.0864399
I0815 21:15:50.018805 20340 solver.cpp:334]     Train net output #0: loss = 0.0864398 (* 1 = 0.0864398 loss)
I0815 21:15:50.018810 20340 sgd_solver.cpp:136] Iteration 6200, lr = 1e-05, m = 0.9
I0815 21:16:09.358423 20340 solver.cpp:312] Iteration 6300 (5.17086 iter/s, 19.3391s/100 iter), loss = 0.0808005
I0815 21:16:09.358445 20340 solver.cpp:334]     Train net output #0: loss = 0.0808003 (* 1 = 0.0808003 loss)
I0815 21:16:09.358449 20340 sgd_solver.cpp:136] Iteration 6300, lr = 1e-05, m = 0.9
I0815 21:16:28.783330 20340 solver.cpp:312] Iteration 6400 (5.14817 iter/s, 19.4244s/100 iter), loss = 0.0938407
I0815 21:16:28.783404 20340 solver.cpp:334]     Train net output #0: loss = 0.0938406 (* 1 = 0.0938406 loss)
I0815 21:16:28.783411 20340 sgd_solver.cpp:136] Iteration 6400, lr = 1e-05, m = 0.9
I0815 21:16:36.943466 20316 data_reader.cpp:288] Starting prefetch of epoch 5
I0815 21:16:48.052723 20340 solver.cpp:312] Iteration 6500 (5.18972 iter/s, 19.2689s/100 iter), loss = 0.0890595
I0815 21:16:48.052744 20340 solver.cpp:334]     Train net output #0: loss = 0.0890594 (* 1 = 0.0890594 loss)
I0815 21:16:48.052750 20340 sgd_solver.cpp:136] Iteration 6500, lr = 1e-05, m = 0.9
I0815 21:17:07.677852 20340 solver.cpp:312] Iteration 6600 (5.09565 iter/s, 19.6246s/100 iter), loss = 0.087179
I0815 21:17:07.677906 20340 solver.cpp:334]     Train net output #0: loss = 0.0871789 (* 1 = 0.0871789 loss)
I0815 21:17:07.677911 20340 sgd_solver.cpp:136] Iteration 6600, lr = 1e-05, m = 0.9
I0815 21:17:09.248491 20343 data_reader.cpp:288] Starting prefetch of epoch 5
I0815 21:17:27.323156 20340 solver.cpp:312] Iteration 6700 (5.09042 iter/s, 19.6448s/100 iter), loss = 0.0934402
I0815 21:17:27.323179 20340 solver.cpp:334]     Train net output #0: loss = 0.0934401 (* 1 = 0.0934401 loss)
I0815 21:17:27.323184 20340 sgd_solver.cpp:136] Iteration 6700, lr = 1e-05, m = 0.9
I0815 21:17:46.874676 20340 solver.cpp:312] Iteration 6800 (5.11483 iter/s, 19.551s/100 iter), loss = 0.0503814
I0815 21:17:46.874758 20340 solver.cpp:334]     Train net output #0: loss = 0.0503813 (* 1 = 0.0503813 loss)
I0815 21:17:46.874765 20340 sgd_solver.cpp:136] Iteration 6800, lr = 1e-05, m = 0.9
I0815 21:18:06.299929 20340 solver.cpp:312] Iteration 6900 (5.14808 iter/s, 19.4247s/100 iter), loss = 0.100402
I0815 21:18:06.299948 20340 solver.cpp:334]     Train net output #0: loss = 0.100401 (* 1 = 0.100401 loss)
I0815 21:18:06.299953 20340 sgd_solver.cpp:136] Iteration 6900, lr = 1e-05, m = 0.9
I0815 21:18:13.806743 20348 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 21:18:26.096057 20340 solver.cpp:312] Iteration 7000 (5.05163 iter/s, 19.7956s/100 iter), loss = 0.106453
I0815 21:18:26.096103 20340 solver.cpp:334]     Train net output #0: loss = 0.106453 (* 1 = 0.106453 loss)
I0815 21:18:26.096110 20340 sgd_solver.cpp:136] Iteration 7000, lr = 1e-05, m = 0.9
I0815 21:18:45.513262 20340 solver.cpp:312] Iteration 7100 (5.15021 iter/s, 19.4167s/100 iter), loss = 0.0812609
I0815 21:18:45.513288 20340 solver.cpp:334]     Train net output #0: loss = 0.0812607 (* 1 = 0.0812607 loss)
I0815 21:18:45.513293 20340 sgd_solver.cpp:136] Iteration 7100, lr = 1e-05, m = 0.9
I0815 21:19:05.088687 20340 solver.cpp:312] Iteration 7200 (5.10859 iter/s, 19.5749s/100 iter), loss = 0.0669766
I0815 21:19:05.088737 20340 solver.cpp:334]     Train net output #0: loss = 0.0669765 (* 1 = 0.0669765 loss)
I0815 21:19:05.088744 20340 sgd_solver.cpp:136] Iteration 7200, lr = 1e-05, m = 0.9
I0815 21:19:18.349217 20344 data_reader.cpp:288] Starting prefetch of epoch 9
I0815 21:19:24.240247 20340 solver.cpp:312] Iteration 7300 (5.22165 iter/s, 19.151s/100 iter), loss = 0.0655575
I0815 21:19:24.240270 20340 solver.cpp:334]     Train net output #0: loss = 0.0655574 (* 1 = 0.0655574 loss)
I0815 21:19:24.240274 20340 sgd_solver.cpp:136] Iteration 7300, lr = 1e-05, m = 0.9
I0815 21:19:43.500478 20340 solver.cpp:312] Iteration 7400 (5.19219 iter/s, 19.2597s/100 iter), loss = 0.112614
I0815 21:19:43.500564 20340 solver.cpp:334]     Train net output #0: loss = 0.112614 (* 1 = 0.112614 loss)
I0815 21:19:43.500571 20340 sgd_solver.cpp:136] Iteration 7400, lr = 1e-05, m = 0.9
I0815 21:19:50.244405 20344 data_reader.cpp:288] Starting prefetch of epoch 10
I0815 21:20:03.213490 20340 solver.cpp:312] Iteration 7500 (5.07293 iter/s, 19.7125s/100 iter), loss = 0.106126
I0815 21:20:03.213511 20340 solver.cpp:334]     Train net output #0: loss = 0.106126 (* 1 = 0.106126 loss)
I0815 21:20:03.213516 20340 sgd_solver.cpp:136] Iteration 7500, lr = 1e-05, m = 0.9
I0815 21:20:22.714553 20340 solver.cpp:312] Iteration 7600 (5.12807 iter/s, 19.5005s/100 iter), loss = 0.0640639
I0815 21:20:22.714617 20340 solver.cpp:334]     Train net output #0: loss = 0.0640638 (* 1 = 0.0640638 loss)
I0815 21:20:22.714624 20340 sgd_solver.cpp:136] Iteration 7600, lr = 1e-05, m = 0.9
I0815 21:20:42.082278 20340 solver.cpp:312] Iteration 7700 (5.16337 iter/s, 19.3672s/100 iter), loss = 0.0706793
I0815 21:20:42.082345 20340 solver.cpp:334]     Train net output #0: loss = 0.0706792 (* 1 = 0.0706792 loss)
I0815 21:20:42.082365 20340 sgd_solver.cpp:136] Iteration 7700, lr = 1e-05, m = 0.9
I0815 21:20:54.809898 20314 data_reader.cpp:288] Starting prefetch of epoch 4
I0815 21:21:01.686642 20340 solver.cpp:312] Iteration 7800 (5.10105 iter/s, 19.6038s/100 iter), loss = 0.0994551
I0815 21:21:01.686667 20340 solver.cpp:334]     Train net output #0: loss = 0.099455 (* 1 = 0.099455 loss)
I0815 21:21:01.686672 20340 sgd_solver.cpp:136] Iteration 7800, lr = 1e-05, m = 0.9
I0815 21:21:21.376860 20340 solver.cpp:312] Iteration 7900 (5.0788 iter/s, 19.6897s/100 iter), loss = 0.0576541
I0815 21:21:21.376883 20340 solver.cpp:334]     Train net output #0: loss = 0.057654 (* 1 = 0.057654 loss)
I0815 21:21:21.376888 20340 sgd_solver.cpp:136] Iteration 7900, lr = 1e-05, m = 0.9
I0815 21:21:40.562413 20340 solver.cpp:509] Iteration 8000, Testing net (#0)
I0815 21:21:44.077246 20338 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 21:21:48.921495 20340 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 21:21:52.093289 20387 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 21:21:52.444936 20340 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.951278
I0815 21:21:52.444958 20340 solver.cpp:594]     Test net output #1: accuracy/top5 = 1
I0815 21:21:52.444963 20340 solver.cpp:594]     Test net output #2: loss = 0.149912 (* 1 = 0.149912 loss)
I0815 21:21:52.444990 20340 solver.cpp:264] [MultiGPU] Tests completed in 11.8823s
I0815 21:21:52.668609 20340 solver.cpp:312] Iteration 8000 (3.19582 iter/s, 31.2909s/100 iter), loss = 0.102859
I0815 21:21:52.668637 20340 solver.cpp:334]     Train net output #0: loss = 0.102859 (* 1 = 0.102859 loss)
I0815 21:21:52.668644 20340 sgd_solver.cpp:136] Iteration 8000, lr = 1e-05, m = 0.9
I0815 21:22:12.094183 20340 solver.cpp:312] Iteration 8100 (5.14799 iter/s, 19.425s/100 iter), loss = 0.072124
I0815 21:22:12.094230 20340 solver.cpp:334]     Train net output #0: loss = 0.0721239 (* 1 = 0.0721239 loss)
I0815 21:22:12.094235 20340 sgd_solver.cpp:136] Iteration 8100, lr = 1e-05, m = 0.9
I0815 21:22:31.926692 20340 solver.cpp:312] Iteration 8200 (5.04237 iter/s, 19.832s/100 iter), loss = 0.0641626
I0815 21:22:31.926715 20340 solver.cpp:334]     Train net output #0: loss = 0.0641624 (* 1 = 0.0641624 loss)
I0815 21:22:31.926720 20340 sgd_solver.cpp:136] Iteration 8200, lr = 1e-05, m = 0.9
I0815 21:22:43.628002 20347 data_reader.cpp:288] Starting prefetch of epoch 4
I0815 21:22:51.826402 20340 solver.cpp:312] Iteration 8300 (5.02534 iter/s, 19.8992s/100 iter), loss = 0.140638
I0815 21:22:51.826427 20340 solver.cpp:334]     Train net output #0: loss = 0.140637 (* 1 = 0.140637 loss)
I0815 21:22:51.826433 20340 sgd_solver.cpp:136] Iteration 8300, lr = 1e-05, m = 0.9
I0815 21:23:11.368651 20340 solver.cpp:312] Iteration 8400 (5.11726 iter/s, 19.5417s/100 iter), loss = 0.0682535
I0815 21:23:11.368672 20340 solver.cpp:334]     Train net output #0: loss = 0.0682534 (* 1 = 0.0682534 loss)
I0815 21:23:11.368676 20340 sgd_solver.cpp:136] Iteration 8400, lr = 1e-05, m = 0.9
I0815 21:23:30.678675 20340 solver.cpp:312] Iteration 8500 (5.1788 iter/s, 19.3095s/100 iter), loss = 0.0569647
I0815 21:23:30.678731 20340 solver.cpp:334]     Train net output #0: loss = 0.0569645 (* 1 = 0.0569645 loss)
I0815 21:23:30.678740 20340 sgd_solver.cpp:136] Iteration 8500, lr = 1e-05, m = 0.9
I0815 21:23:48.194772 20347 data_reader.cpp:288] Starting prefetch of epoch 5
I0815 21:23:49.884723 20340 solver.cpp:312] Iteration 8600 (5.20684 iter/s, 19.2055s/100 iter), loss = 0.113253
I0815 21:23:49.884745 20340 solver.cpp:334]     Train net output #0: loss = 0.113253 (* 1 = 0.113253 loss)
I0815 21:23:49.884749 20340 sgd_solver.cpp:136] Iteration 8600, lr = 1e-05, m = 0.9
I0815 21:24:09.404042 20340 solver.cpp:312] Iteration 8700 (5.12327 iter/s, 19.5188s/100 iter), loss = 0.0774181
I0815 21:24:09.404109 20340 solver.cpp:334]     Train net output #0: loss = 0.077418 (* 1 = 0.077418 loss)
I0815 21:24:09.404114 20340 sgd_solver.cpp:136] Iteration 8700, lr = 1e-05, m = 0.9
I0815 21:24:20.264587 20344 data_reader.cpp:288] Starting prefetch of epoch 11
I0815 21:24:28.818883 20340 solver.cpp:312] Iteration 8800 (5.15084 iter/s, 19.4143s/100 iter), loss = 0.073973
I0815 21:24:28.818910 20340 solver.cpp:334]     Train net output #0: loss = 0.0739729 (* 1 = 0.0739729 loss)
I0815 21:24:28.818917 20340 sgd_solver.cpp:136] Iteration 8800, lr = 1e-05, m = 0.9
I0815 21:24:48.274982 20340 solver.cpp:312] Iteration 8900 (5.13992 iter/s, 19.4556s/100 iter), loss = 0.0746262
I0815 21:24:48.275037 20340 solver.cpp:334]     Train net output #0: loss = 0.074626 (* 1 = 0.074626 loss)
I0815 21:24:48.275041 20340 sgd_solver.cpp:136] Iteration 8900, lr = 1e-05, m = 0.9
I0815 21:25:07.681324 20340 solver.cpp:312] Iteration 9000 (5.1531 iter/s, 19.4058s/100 iter), loss = 0.0669641
I0815 21:25:07.681351 20340 solver.cpp:334]     Train net output #0: loss = 0.0669639 (* 1 = 0.0669639 loss)
I0815 21:25:07.681358 20340 sgd_solver.cpp:136] Iteration 9000, lr = 1e-05, m = 0.9
I0815 21:25:24.756458 20348 data_reader.cpp:288] Starting prefetch of epoch 4
I0815 21:25:27.216897 20340 solver.cpp:312] Iteration 9100 (5.11901 iter/s, 19.535s/100 iter), loss = 0.0870504
I0815 21:25:27.216925 20340 solver.cpp:334]     Train net output #0: loss = 0.0870502 (* 1 = 0.0870502 loss)
I0815 21:25:27.216931 20340 sgd_solver.cpp:136] Iteration 9100, lr = 1e-05, m = 0.9
I0815 21:25:46.638818 20340 solver.cpp:312] Iteration 9200 (5.14896 iter/s, 19.4214s/100 iter), loss = 0.0709906
I0815 21:25:46.638844 20340 solver.cpp:334]     Train net output #0: loss = 0.0709905 (* 1 = 0.0709905 loss)
I0815 21:25:46.638849 20340 sgd_solver.cpp:136] Iteration 9200, lr = 1e-05, m = 0.9
I0815 21:26:06.162968 20340 solver.cpp:312] Iteration 9300 (5.122 iter/s, 19.5236s/100 iter), loss = 0.0750414
I0815 21:26:06.163015 20340 solver.cpp:334]     Train net output #0: loss = 0.0750413 (* 1 = 0.0750413 loss)
I0815 21:26:06.163022 20340 sgd_solver.cpp:136] Iteration 9300, lr = 1e-05, m = 0.9
I0815 21:26:25.707492 20340 solver.cpp:312] Iteration 9400 (5.11666 iter/s, 19.544s/100 iter), loss = 0.102015
I0815 21:26:25.707515 20340 solver.cpp:334]     Train net output #0: loss = 0.102015 (* 1 = 0.102015 loss)
I0815 21:26:25.707521 20340 sgd_solver.cpp:136] Iteration 9400, lr = 1e-05, m = 0.9
I0815 21:26:29.031188 20348 data_reader.cpp:288] Starting prefetch of epoch 5
I0815 21:26:45.041190 20340 solver.cpp:312] Iteration 9500 (5.17246 iter/s, 19.3332s/100 iter), loss = 0.0655118
I0815 21:26:45.041266 20340 solver.cpp:334]     Train net output #0: loss = 0.0655118 (* 1 = 0.0655118 loss)
I0815 21:26:45.041275 20340 sgd_solver.cpp:136] Iteration 9500, lr = 1e-05, m = 0.9
I0815 21:27:01.196401 20344 data_reader.cpp:288] Starting prefetch of epoch 12
I0815 21:27:04.524194 20340 solver.cpp:312] Iteration 9600 (5.13282 iter/s, 19.4825s/100 iter), loss = 0.0680127
I0815 21:27:04.524224 20340 solver.cpp:334]     Train net output #0: loss = 0.0680126 (* 1 = 0.0680126 loss)
I0815 21:27:04.524230 20340 sgd_solver.cpp:136] Iteration 9600, lr = 1e-05, m = 0.9
I0815 21:27:23.985097 20340 solver.cpp:312] Iteration 9700 (5.13865 iter/s, 19.4604s/100 iter), loss = 0.0685929
I0815 21:27:23.996358 20340 solver.cpp:334]     Train net output #0: loss = 0.0685928 (* 1 = 0.0685928 loss)
I0815 21:27:23.996397 20340 sgd_solver.cpp:136] Iteration 9700, lr = 1e-05, m = 0.9
I0815 21:27:43.376627 20340 solver.cpp:312] Iteration 9800 (5.15703 iter/s, 19.391s/100 iter), loss = 0.0669615
I0815 21:27:43.376652 20340 solver.cpp:334]     Train net output #0: loss = 0.0669614 (* 1 = 0.0669614 loss)
I0815 21:27:43.376658 20340 sgd_solver.cpp:136] Iteration 9800, lr = 1e-05, m = 0.9
I0815 21:28:02.715863 20340 solver.cpp:312] Iteration 9900 (5.17098 iter/s, 19.3387s/100 iter), loss = 0.0750452
I0815 21:28:02.715935 20340 solver.cpp:334]     Train net output #0: loss = 0.075045 (* 1 = 0.075045 loss)
I0815 21:28:02.715942 20340 sgd_solver.cpp:136] Iteration 9900, lr = 1e-05, m = 0.9
I0815 21:28:05.250067 20316 data_reader.cpp:288] Starting prefetch of epoch 6
I0815 21:28:22.046216 20340 solver.cpp:639] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/l1reg/cityscapes5_jsegnet21v2_iter_10000.caffemodel
I0815 21:28:22.154637 20340 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/l1reg/cityscapes5_jsegnet21v2_iter_10000.solverstate
I0815 21:28:22.166014 20340 solver.cpp:509] Iteration 10000, Testing net (#0)
I0815 21:28:50.910784 20340 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.951444
I0815 21:28:50.910898 20340 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999501
I0815 21:28:50.910908 20340 solver.cpp:594]     Test net output #2: loss = 0.183997 (* 1 = 0.183997 loss)
I0815 21:28:50.910936 20340 solver.cpp:264] [MultiGPU] Tests completed in 28.7441s
I0815 21:28:51.113185 20340 solver.cpp:312] Iteration 10000 (2.06629 iter/s, 48.396s/100 iter), loss = 0.0750551
I0815 21:28:51.113209 20340 solver.cpp:334]     Train net output #0: loss = 0.075055 (* 1 = 0.075055 loss)
I0815 21:28:51.113215 20340 sgd_solver.cpp:136] Iteration 10000, lr = 1e-05, m = 0.9
I0815 21:29:06.092814 20316 data_reader.cpp:288] Starting prefetch of epoch 7
I0815 21:29:10.357316 20340 solver.cpp:312] Iteration 10100 (5.19654 iter/s, 19.2436s/100 iter), loss = 0.0738182
I0815 21:29:10.357342 20340 solver.cpp:334]     Train net output #0: loss = 0.0738181 (* 1 = 0.0738181 loss)
I0815 21:29:10.357347 20340 sgd_solver.cpp:136] Iteration 10100, lr = 1e-05, m = 0.9
I0815 21:29:29.968673 20340 solver.cpp:312] Iteration 10200 (5.09923 iter/s, 19.6108s/100 iter), loss = 0.0972948
I0815 21:29:29.968758 20340 solver.cpp:334]     Train net output #0: loss = 0.0972947 (* 1 = 0.0972947 loss)
I0815 21:29:29.968765 20340 sgd_solver.cpp:136] Iteration 10200, lr = 1e-05, m = 0.9
I0815 21:29:49.602535 20340 solver.cpp:312] Iteration 10300 (5.09338 iter/s, 19.6333s/100 iter), loss = 0.0636069
I0815 21:29:49.602560 20340 solver.cpp:334]     Train net output #0: loss = 0.0636068 (* 1 = 0.0636068 loss)
I0815 21:29:49.602563 20340 sgd_solver.cpp:136] Iteration 10300, lr = 1e-05, m = 0.9
I0815 21:30:09.052209 20340 solver.cpp:312] Iteration 10400 (5.14162 iter/s, 19.4491s/100 iter), loss = 0.0551724
I0815 21:30:09.052258 20340 solver.cpp:334]     Train net output #0: loss = 0.0551723 (* 1 = 0.0551723 loss)
I0815 21:30:09.052264 20340 sgd_solver.cpp:136] Iteration 10400, lr = 1e-05, m = 0.9
I0815 21:30:10.830199 20316 data_reader.cpp:288] Starting prefetch of epoch 8
I0815 21:30:28.516858 20340 solver.cpp:312] Iteration 10500 (5.13766 iter/s, 19.4641s/100 iter), loss = 0.0607413
I0815 21:30:28.516885 20340 solver.cpp:334]     Train net output #0: loss = 0.0607413 (* 1 = 0.0607413 loss)
I0815 21:30:28.516891 20340 sgd_solver.cpp:136] Iteration 10500, lr = 1e-05, m = 0.9
I0815 21:30:42.921372 20314 data_reader.cpp:288] Starting prefetch of epoch 5
I0815 21:30:47.973812 20340 solver.cpp:312] Iteration 10600 (5.13969 iter/s, 19.4564s/100 iter), loss = 0.0873066
I0815 21:30:47.973834 20340 solver.cpp:334]     Train net output #0: loss = 0.0873065 (* 1 = 0.0873065 loss)
I0815 21:30:47.973837 20340 sgd_solver.cpp:136] Iteration 10600, lr = 1e-05, m = 0.9
I0815 21:31:07.500941 20340 solver.cpp:312] Iteration 10700 (5.12122 iter/s, 19.5266s/100 iter), loss = 0.0450957
I0815 21:31:07.500967 20340 solver.cpp:334]     Train net output #0: loss = 0.0450956 (* 1 = 0.0450956 loss)
I0815 21:31:07.500972 20340 sgd_solver.cpp:136] Iteration 10700, lr = 1e-05, m = 0.9
I0815 21:31:26.876904 20340 solver.cpp:312] Iteration 10800 (5.16118 iter/s, 19.3754s/100 iter), loss = 0.0815366
I0815 21:31:26.876977 20340 solver.cpp:334]     Train net output #0: loss = 0.0815365 (* 1 = 0.0815365 loss)
I0815 21:31:26.876984 20340 sgd_solver.cpp:136] Iteration 10800, lr = 1e-05, m = 0.9
I0815 21:31:46.415328 20340 solver.cpp:312] Iteration 10900 (5.11826 iter/s, 19.5379s/100 iter), loss = 0.0497804
I0815 21:31:46.415351 20340 solver.cpp:334]     Train net output #0: loss = 0.0497803 (* 1 = 0.0497803 loss)
I0815 21:31:46.415355 20340 sgd_solver.cpp:136] Iteration 10900, lr = 1e-05, m = 0.9
I0815 21:31:47.389114 20347 data_reader.cpp:288] Starting prefetch of epoch 6
I0815 21:32:05.850765 20340 solver.cpp:312] Iteration 11000 (5.14538 iter/s, 19.4349s/100 iter), loss = 0.0671847
I0815 21:32:05.850816 20340 solver.cpp:334]     Train net output #0: loss = 0.0671846 (* 1 = 0.0671846 loss)
I0815 21:32:05.850821 20340 sgd_solver.cpp:136] Iteration 11000, lr = 1e-05, m = 0.9
I0815 21:32:25.149065 20340 solver.cpp:312] Iteration 11100 (5.18195 iter/s, 19.2978s/100 iter), loss = 0.0813203
I0815 21:32:25.149088 20340 solver.cpp:334]     Train net output #0: loss = 0.0813202 (* 1 = 0.0813202 loss)
I0815 21:32:25.149093 20340 sgd_solver.cpp:136] Iteration 11100, lr = 1e-05, m = 0.9
I0815 21:32:44.631494 20340 solver.cpp:312] Iteration 11200 (5.13297 iter/s, 19.4819s/100 iter), loss = 0.0828284
I0815 21:32:44.631544 20340 solver.cpp:334]     Train net output #0: loss = 0.0828283 (* 1 = 0.0828283 loss)
I0815 21:32:44.631549 20340 sgd_solver.cpp:136] Iteration 11200, lr = 1e-05, m = 0.9
I0815 21:32:51.223830 20348 data_reader.cpp:288] Starting prefetch of epoch 6
I0815 21:33:03.655544 20340 solver.cpp:312] Iteration 11300 (5.25665 iter/s, 19.0235s/100 iter), loss = 0.0640767
I0815 21:33:03.655571 20340 solver.cpp:334]     Train net output #0: loss = 0.0640766 (* 1 = 0.0640766 loss)
I0815 21:33:03.655578 20340 sgd_solver.cpp:136] Iteration 11300, lr = 1e-05, m = 0.9
I0815 21:33:23.188393 20340 solver.cpp:312] Iteration 11400 (5.11972 iter/s, 19.5323s/100 iter), loss = 0.0671259
I0815 21:33:23.188450 20340 solver.cpp:334]     Train net output #0: loss = 0.0671258 (* 1 = 0.0671258 loss)
I0815 21:33:23.188459 20340 sgd_solver.cpp:136] Iteration 11400, lr = 1e-05, m = 0.9
I0815 21:33:23.416414 20348 data_reader.cpp:288] Starting prefetch of epoch 7
I0815 21:33:42.527842 20340 solver.cpp:312] Iteration 11500 (5.17092 iter/s, 19.3389s/100 iter), loss = 0.0854167
I0815 21:33:42.527866 20340 solver.cpp:334]     Train net output #0: loss = 0.0854166 (* 1 = 0.0854166 loss)
I0815 21:33:42.527871 20340 sgd_solver.cpp:136] Iteration 11500, lr = 1e-05, m = 0.9
I0815 21:34:01.886310 20340 solver.cpp:312] Iteration 11600 (5.16584 iter/s, 19.3579s/100 iter), loss = 0.114739
I0815 21:34:01.886394 20340 solver.cpp:334]     Train net output #0: loss = 0.114739 (* 1 = 0.114739 loss)
I0815 21:34:01.886400 20340 sgd_solver.cpp:136] Iteration 11600, lr = 1e-05, m = 0.9
I0815 21:34:21.521194 20340 solver.cpp:312] Iteration 11700 (5.09312 iter/s, 19.6343s/100 iter), loss = 0.0433572
I0815 21:34:21.521224 20340 solver.cpp:334]     Train net output #0: loss = 0.0433571 (* 1 = 0.0433571 loss)
I0815 21:34:21.521230 20340 sgd_solver.cpp:136] Iteration 11700, lr = 1e-05, m = 0.9
I0815 21:34:27.497308 20348 data_reader.cpp:288] Starting prefetch of epoch 8
I0815 21:34:40.781050 20340 solver.cpp:312] Iteration 11800 (5.19229 iter/s, 19.2593s/100 iter), loss = 0.0669257
I0815 21:34:40.781101 20340 solver.cpp:334]     Train net output #0: loss = 0.0669256 (* 1 = 0.0669256 loss)
I0815 21:34:40.781108 20340 sgd_solver.cpp:136] Iteration 11800, lr = 1e-05, m = 0.9
I0815 21:35:00.349812 20340 solver.cpp:312] Iteration 11900 (5.11033 iter/s, 19.5682s/100 iter), loss = 0.0672635
I0815 21:35:00.349839 20340 solver.cpp:334]     Train net output #0: loss = 0.0672634 (* 1 = 0.0672634 loss)
I0815 21:35:00.349844 20340 sgd_solver.cpp:136] Iteration 11900, lr = 1e-05, m = 0.9
I0815 21:35:19.667872 20340 solver.cpp:509] Iteration 12000, Testing net (#0)
I0815 21:35:23.378932 20387 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 21:35:31.584599 20391 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 21:35:31.938318 20340 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.950933
I0815 21:35:31.938345 20340 solver.cpp:594]     Test net output #1: accuracy/top5 = 1
I0815 21:35:31.938350 20340 solver.cpp:594]     Test net output #2: loss = 0.153057 (* 1 = 0.153057 loss)
I0815 21:35:31.938459 20340 solver.cpp:264] [MultiGPU] Tests completed in 12.2703s
I0815 21:35:32.153844 20340 solver.cpp:312] Iteration 12000 (3.14434 iter/s, 31.8032s/100 iter), loss = 0.0411355
I0815 21:35:32.153867 20340 solver.cpp:334]     Train net output #0: loss = 0.0411354 (* 1 = 0.0411354 loss)
I0815 21:35:32.153870 20340 sgd_solver.cpp:136] Iteration 12000, lr = 1e-05, m = 0.9
I0815 21:35:51.588374 20340 solver.cpp:312] Iteration 12100 (5.14562 iter/s, 19.434s/100 iter), loss = 0.0967906
I0815 21:35:51.588459 20340 solver.cpp:334]     Train net output #0: loss = 0.0967905 (* 1 = 0.0967905 loss)
I0815 21:35:51.588466 20340 sgd_solver.cpp:136] Iteration 12100, lr = 1e-05, m = 0.9
I0815 21:36:10.954427 20340 solver.cpp:312] Iteration 12200 (5.16382 iter/s, 19.3655s/100 iter), loss = 0.061021
I0815 21:36:10.954455 20340 solver.cpp:334]     Train net output #0: loss = 0.0610209 (* 1 = 0.0610209 loss)
I0815 21:36:10.954463 20340 sgd_solver.cpp:136] Iteration 12200, lr = 1e-05, m = 0.9
I0815 21:36:16.121666 20348 data_reader.cpp:288] Starting prefetch of epoch 9
I0815 21:36:30.213063 20340 solver.cpp:312] Iteration 12300 (5.19262 iter/s, 19.2581s/100 iter), loss = 0.055112
I0815 21:36:30.213163 20340 solver.cpp:334]     Train net output #0: loss = 0.0551119 (* 1 = 0.0551119 loss)
I0815 21:36:30.213183 20340 sgd_solver.cpp:136] Iteration 12300, lr = 1e-05, m = 0.9
I0815 21:36:49.746166 20340 solver.cpp:312] Iteration 12400 (5.11966 iter/s, 19.5326s/100 iter), loss = 0.0558946
I0815 21:36:49.746191 20340 solver.cpp:334]     Train net output #0: loss = 0.0558945 (* 1 = 0.0558945 loss)
I0815 21:36:49.746197 20340 sgd_solver.cpp:136] Iteration 12400, lr = 1e-05, m = 0.9
I0815 21:37:08.761080 20340 solver.cpp:312] Iteration 12500 (5.25917 iter/s, 19.0144s/100 iter), loss = 0.0756989
I0815 21:37:08.761186 20340 solver.cpp:334]     Train net output #0: loss = 0.0756987 (* 1 = 0.0756987 loss)
I0815 21:37:08.761193 20340 sgd_solver.cpp:136] Iteration 12500, lr = 1e-05, m = 0.9
I0815 21:37:20.091063 20316 data_reader.cpp:288] Starting prefetch of epoch 9
I0815 21:37:28.347288 20340 solver.cpp:312] Iteration 12600 (5.10577 iter/s, 19.5857s/100 iter), loss = 0.0488623
I0815 21:37:28.347311 20340 solver.cpp:334]     Train net output #0: loss = 0.0488622 (* 1 = 0.0488622 loss)
I0815 21:37:28.347316 20340 sgd_solver.cpp:136] Iteration 12600, lr = 1e-05, m = 0.9
I0815 21:37:48.862893 20340 solver.cpp:312] Iteration 12700 (4.87447 iter/s, 20.515s/100 iter), loss = 0.119463
I0815 21:37:48.862977 20340 solver.cpp:334]     Train net output #0: loss = 0.119463 (* 1 = 0.119463 loss)
I0815 21:37:48.862983 20340 sgd_solver.cpp:136] Iteration 12700, lr = 1e-05, m = 0.9
I0815 21:37:53.365496 20314 data_reader.cpp:288] Starting prefetch of epoch 6
I0815 21:38:08.358737 20340 solver.cpp:312] Iteration 12800 (5.12944 iter/s, 19.4953s/100 iter), loss = 0.0505349
I0815 21:38:08.358758 20340 solver.cpp:334]     Train net output #0: loss = 0.0505348 (* 1 = 0.0505348 loss)
I0815 21:38:08.358762 20340 sgd_solver.cpp:136] Iteration 12800, lr = 1e-05, m = 0.9
I0815 21:38:27.596653 20340 solver.cpp:312] Iteration 12900 (5.19821 iter/s, 19.2374s/100 iter), loss = 0.0779058
I0815 21:38:27.596704 20340 solver.cpp:334]     Train net output #0: loss = 0.0779057 (* 1 = 0.0779057 loss)
I0815 21:38:27.596709 20340 sgd_solver.cpp:136] Iteration 12900, lr = 1e-05, m = 0.9
I0815 21:38:47.156220 20340 solver.cpp:312] Iteration 13000 (5.11273 iter/s, 19.559s/100 iter), loss = 0.0587579
I0815 21:38:47.156241 20340 solver.cpp:334]     Train net output #0: loss = 0.0587578 (* 1 = 0.0587578 loss)
I0815 21:38:47.156245 20340 sgd_solver.cpp:136] Iteration 13000, lr = 1e-05, m = 0.9
I0815 21:38:57.729100 20316 data_reader.cpp:288] Starting prefetch of epoch 10
I0815 21:39:06.808943 20340 solver.cpp:312] Iteration 13100 (5.08849 iter/s, 19.6522s/100 iter), loss = 0.0896918
I0815 21:39:06.808967 20340 solver.cpp:334]     Train net output #0: loss = 0.0896917 (* 1 = 0.0896917 loss)
I0815 21:39:06.808972 20340 sgd_solver.cpp:136] Iteration 13100, lr = 1e-05, m = 0.9
I0815 21:39:26.133759 20340 solver.cpp:312] Iteration 13200 (5.17484 iter/s, 19.3243s/100 iter), loss = 0.055454
I0815 21:39:26.133780 20340 solver.cpp:334]     Train net output #0: loss = 0.0554539 (* 1 = 0.0554539 loss)
I0815 21:39:26.133785 20340 sgd_solver.cpp:136] Iteration 13200, lr = 1e-05, m = 0.9
I0815 21:39:45.594086 20340 solver.cpp:312] Iteration 13300 (5.1388 iter/s, 19.4598s/100 iter), loss = 0.0706577
I0815 21:39:45.594136 20340 solver.cpp:334]     Train net output #0: loss = 0.0706575 (* 1 = 0.0706575 loss)
I0815 21:39:45.594143 20340 sgd_solver.cpp:136] Iteration 13300, lr = 1e-05, m = 0.9
I0815 21:40:01.897001 20316 data_reader.cpp:288] Starting prefetch of epoch 11
I0815 21:40:05.032793 20340 solver.cpp:312] Iteration 13400 (5.14452 iter/s, 19.4382s/100 iter), loss = 0.053825
I0815 21:40:05.032812 20340 solver.cpp:334]     Train net output #0: loss = 0.0538249 (* 1 = 0.0538249 loss)
I0815 21:40:05.032819 20340 sgd_solver.cpp:136] Iteration 13400, lr = 1e-05, m = 0.9
I0815 21:40:24.496829 20340 solver.cpp:312] Iteration 13500 (5.13782 iter/s, 19.4635s/100 iter), loss = 0.0782919
I0815 21:40:24.509099 20340 solver.cpp:334]     Train net output #0: loss = 0.0782918 (* 1 = 0.0782918 loss)
I0815 21:40:24.509133 20340 sgd_solver.cpp:136] Iteration 13500, lr = 1e-05, m = 0.9
I0815 21:40:34.236438 20343 data_reader.cpp:288] Starting prefetch of epoch 6
I0815 21:40:44.061805 20340 solver.cpp:312] Iteration 13600 (5.11132 iter/s, 19.5644s/100 iter), loss = 0.0852463
I0815 21:40:44.061827 20340 solver.cpp:334]     Train net output #0: loss = 0.0852462 (* 1 = 0.0852462 loss)
I0815 21:40:44.061833 20340 sgd_solver.cpp:136] Iteration 13600, lr = 1e-05, m = 0.9
I0815 21:41:03.400585 20340 solver.cpp:312] Iteration 13700 (5.1711 iter/s, 19.3383s/100 iter), loss = 0.0578729
I0815 21:41:03.400645 20340 solver.cpp:334]     Train net output #0: loss = 0.0578727 (* 1 = 0.0578727 loss)
I0815 21:41:03.400652 20340 sgd_solver.cpp:136] Iteration 13700, lr = 1e-05, m = 0.9
I0815 21:41:22.930440 20340 solver.cpp:312] Iteration 13800 (5.12051 iter/s, 19.5293s/100 iter), loss = 0.0616944
I0815 21:41:22.930474 20340 solver.cpp:334]     Train net output #0: loss = 0.0616942 (* 1 = 0.0616942 loss)
I0815 21:41:22.930480 20340 sgd_solver.cpp:136] Iteration 13800, lr = 1e-05, m = 0.9
I0815 21:41:38.458358 20314 data_reader.cpp:288] Starting prefetch of epoch 7
I0815 21:41:42.311100 20340 solver.cpp:312] Iteration 13900 (5.15992 iter/s, 19.3801s/100 iter), loss = 0.078685
I0815 21:41:42.311125 20340 solver.cpp:334]     Train net output #0: loss = 0.0786849 (* 1 = 0.0786849 loss)
I0815 21:41:42.311131 20340 sgd_solver.cpp:136] Iteration 13900, lr = 1e-05, m = 0.9
I0815 21:42:01.532940 20340 solver.cpp:509] Iteration 14000, Testing net (#0)
I0815 21:42:14.400473 20340 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.951847
I0815 21:42:14.400575 20340 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999406
I0815 21:42:14.400584 20340 solver.cpp:594]     Test net output #2: loss = 0.183419 (* 1 = 0.183419 loss)
I0815 21:42:14.400610 20340 solver.cpp:264] [MultiGPU] Tests completed in 12.8673s
I0815 21:42:14.606631 20340 solver.cpp:312] Iteration 14000 (3.09649 iter/s, 32.2947s/100 iter), loss = 0.0910856
I0815 21:42:14.606654 20340 solver.cpp:334]     Train net output #0: loss = 0.0910855 (* 1 = 0.0910855 loss)
I0815 21:42:14.606660 20340 sgd_solver.cpp:136] Iteration 14000, lr = 1e-05, m = 0.9
I0815 21:42:23.459728 20343 data_reader.cpp:288] Starting prefetch of epoch 7
I0815 21:42:34.143970 20340 solver.cpp:312] Iteration 14100 (5.11855 iter/s, 19.5368s/100 iter), loss = 0.0938018
I0815 21:42:34.143990 20340 solver.cpp:334]     Train net output #0: loss = 0.0938017 (* 1 = 0.0938017 loss)
I0815 21:42:34.143996 20340 sgd_solver.cpp:136] Iteration 14100, lr = 1e-05, m = 0.9
I0815 21:42:53.881539 20340 solver.cpp:312] Iteration 14200 (5.06662 iter/s, 19.737s/100 iter), loss = 0.0654386
I0815 21:42:53.881602 20340 solver.cpp:334]     Train net output #0: loss = 0.0654384 (* 1 = 0.0654384 loss)
I0815 21:42:53.881606 20340 sgd_solver.cpp:136] Iteration 14200, lr = 1e-05, m = 0.9
I0815 21:43:13.472937 20340 solver.cpp:312] Iteration 14300 (5.10442 iter/s, 19.5909s/100 iter), loss = 0.119391
I0815 21:43:13.472966 20340 solver.cpp:334]     Train net output #0: loss = 0.11939 (* 1 = 0.11939 loss)
I0815 21:43:13.472973 20340 sgd_solver.cpp:136] Iteration 14300, lr = 1e-05, m = 0.9
I0815 21:43:28.209612 20347 data_reader.cpp:288] Starting prefetch of epoch 7
I0815 21:43:32.912039 20340 solver.cpp:312] Iteration 14400 (5.14441 iter/s, 19.4386s/100 iter), loss = 0.0835157
I0815 21:43:32.912190 20340 solver.cpp:334]     Train net output #0: loss = 0.0835155 (* 1 = 0.0835155 loss)
I0815 21:43:32.912209 20340 sgd_solver.cpp:136] Iteration 14400, lr = 1e-05, m = 0.9
I0815 21:43:52.604413 20340 solver.cpp:312] Iteration 14500 (5.07825 iter/s, 19.6918s/100 iter), loss = 0.0809736
I0815 21:43:52.604441 20340 solver.cpp:334]     Train net output #0: loss = 0.0809734 (* 1 = 0.0809734 loss)
I0815 21:43:52.604449 20340 sgd_solver.cpp:136] Iteration 14500, lr = 1e-05, m = 0.9
I0815 21:44:00.605223 20314 data_reader.cpp:288] Starting prefetch of epoch 8
I0815 21:44:12.004859 20340 solver.cpp:312] Iteration 14600 (5.15466 iter/s, 19.3999s/100 iter), loss = 0.0698945
I0815 21:44:12.004884 20340 solver.cpp:334]     Train net output #0: loss = 0.0698943 (* 1 = 0.0698943 loss)
I0815 21:44:12.004890 20340 sgd_solver.cpp:136] Iteration 14600, lr = 1e-05, m = 0.9
I0815 21:44:31.482828 20340 solver.cpp:312] Iteration 14700 (5.13415 iter/s, 19.4774s/100 iter), loss = 0.0675951
I0815 21:44:31.482888 20340 solver.cpp:334]     Train net output #0: loss = 0.0675949 (* 1 = 0.0675949 loss)
I0815 21:44:31.482897 20340 sgd_solver.cpp:136] Iteration 14700, lr = 1e-05, m = 0.9
I0815 21:44:50.836637 20340 solver.cpp:312] Iteration 14800 (5.16708 iter/s, 19.3533s/100 iter), loss = 0.0724857
I0815 21:44:50.836663 20340 solver.cpp:334]     Train net output #0: loss = 0.0724856 (* 1 = 0.0724856 loss)
I0815 21:44:50.836669 20340 sgd_solver.cpp:136] Iteration 14800, lr = 1e-05, m = 0.9
I0815 21:45:04.788949 20348 data_reader.cpp:288] Starting prefetch of epoch 10
I0815 21:45:10.517057 20340 solver.cpp:312] Iteration 14900 (5.08133 iter/s, 19.6799s/100 iter), loss = 0.0845488
I0815 21:45:10.517083 20340 solver.cpp:334]     Train net output #0: loss = 0.0845486 (* 1 = 0.0845486 loss)
I0815 21:45:10.517089 20340 sgd_solver.cpp:136] Iteration 14900, lr = 1e-05, m = 0.9
I0815 21:45:30.248164 20340 solver.cpp:312] Iteration 15000 (5.06828 iter/s, 19.7306s/100 iter), loss = 0.063867
I0815 21:45:30.248263 20340 solver.cpp:334]     Train net output #0: loss = 0.0638669 (* 1 = 0.0638669 loss)
I0815 21:45:30.248277 20340 sgd_solver.cpp:136] Iteration 15000, lr = 1e-05, m = 0.9
I0815 21:45:49.713414 20340 solver.cpp:312] Iteration 15100 (5.1375 iter/s, 19.4647s/100 iter), loss = 0.101518
I0815 21:45:49.713472 20340 solver.cpp:334]     Train net output #0: loss = 0.101518 (* 1 = 0.101518 loss)
I0815 21:45:49.713479 20340 sgd_solver.cpp:136] Iteration 15100, lr = 1e-05, m = 0.9
I0815 21:46:09.016561 20340 solver.cpp:312] Iteration 15200 (5.18064 iter/s, 19.3026s/100 iter), loss = 0.0832158
I0815 21:46:09.016580 20340 solver.cpp:334]     Train net output #0: loss = 0.0832156 (* 1 = 0.0832156 loss)
I0815 21:46:09.016584 20340 sgd_solver.cpp:136] Iteration 15200, lr = 1e-05, m = 0.9
I0815 21:46:09.397882 20347 data_reader.cpp:288] Starting prefetch of epoch 8
I0815 21:46:28.385699 20340 solver.cpp:312] Iteration 15300 (5.16299 iter/s, 19.3686s/100 iter), loss = 0.0371951
I0815 21:46:28.385751 20340 solver.cpp:334]     Train net output #0: loss = 0.0371949 (* 1 = 0.0371949 loss)
I0815 21:46:28.385759 20340 sgd_solver.cpp:136] Iteration 15300, lr = 1e-05, m = 0.9
I0815 21:46:41.551926 20343 data_reader.cpp:288] Starting prefetch of epoch 8
I0815 21:46:47.900740 20340 solver.cpp:312] Iteration 15400 (5.12439 iter/s, 19.5145s/100 iter), loss = 0.0757225
I0815 21:46:47.900764 20340 solver.cpp:334]     Train net output #0: loss = 0.0757224 (* 1 = 0.0757224 loss)
I0815 21:46:47.900770 20340 sgd_solver.cpp:136] Iteration 15400, lr = 1e-05, m = 0.9
I0815 21:47:07.385529 20340 solver.cpp:312] Iteration 15500 (5.13235 iter/s, 19.4843s/100 iter), loss = 0.0756475
I0815 21:47:07.385628 20340 solver.cpp:334]     Train net output #0: loss = 0.0756473 (* 1 = 0.0756473 loss)
I0815 21:47:07.385634 20340 sgd_solver.cpp:136] Iteration 15500, lr = 1e-05, m = 0.9
I0815 21:47:26.757582 20340 solver.cpp:312] Iteration 15600 (5.16222 iter/s, 19.3715s/100 iter), loss = 0.0672814
I0815 21:47:26.757606 20340 solver.cpp:334]     Train net output #0: loss = 0.0672812 (* 1 = 0.0672812 loss)
I0815 21:47:26.757611 20340 sgd_solver.cpp:136] Iteration 15600, lr = 1e-05, m = 0.9
I0815 21:47:45.696593 20314 data_reader.cpp:288] Starting prefetch of epoch 9
I0815 21:47:46.029026 20340 solver.cpp:312] Iteration 15700 (5.18917 iter/s, 19.2709s/100 iter), loss = 0.0821114
I0815 21:47:46.029047 20340 solver.cpp:334]     Train net output #0: loss = 0.0821112 (* 1 = 0.0821112 loss)
I0815 21:47:46.029052 20340 sgd_solver.cpp:136] Iteration 15700, lr = 1e-05, m = 0.9
I0815 21:48:05.356068 20340 solver.cpp:312] Iteration 15800 (5.17424 iter/s, 19.3265s/100 iter), loss = 0.0730028
I0815 21:48:05.356091 20340 solver.cpp:334]     Train net output #0: loss = 0.0730027 (* 1 = 0.0730027 loss)
I0815 21:48:05.356096 20340 sgd_solver.cpp:136] Iteration 15800, lr = 1e-05, m = 0.9
I0815 21:48:24.805800 20340 solver.cpp:312] Iteration 15900 (5.1416 iter/s, 19.4492s/100 iter), loss = 0.053871
I0815 21:48:24.805850 20340 solver.cpp:334]     Train net output #0: loss = 0.0538708 (* 1 = 0.0538708 loss)
I0815 21:48:24.805856 20340 sgd_solver.cpp:136] Iteration 15900, lr = 1e-05, m = 0.9
I0815 21:48:43.894271 20340 solver.cpp:509] Iteration 16000, Testing net (#0)
I0815 21:48:47.276491 20336 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 21:48:55.873617 20385 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 21:48:56.218099 20340 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.95205
I0815 21:48:56.218122 20340 solver.cpp:594]     Test net output #1: accuracy/top5 = 1
I0815 21:48:56.218127 20340 solver.cpp:594]     Test net output #2: loss = 0.152022 (* 1 = 0.152022 loss)
I0815 21:48:56.218154 20340 solver.cpp:264] [MultiGPU] Tests completed in 12.3235s
I0815 21:48:56.436861 20340 solver.cpp:312] Iteration 16000 (3.16154 iter/s, 31.6302s/100 iter), loss = 0.0829468
I0815 21:48:56.436885 20340 solver.cpp:334]     Train net output #0: loss = 0.0829466 (* 1 = 0.0829466 loss)
I0815 21:48:56.436892 20340 sgd_solver.cpp:136] Iteration 16000, lr = 1e-05, m = 0.9
I0815 21:49:15.877687 20340 solver.cpp:312] Iteration 16100 (5.14396 iter/s, 19.4403s/100 iter), loss = 0.0783559
I0815 21:49:15.877715 20340 solver.cpp:334]     Train net output #0: loss = 0.0783557 (* 1 = 0.0783557 loss)
I0815 21:49:15.877722 20340 sgd_solver.cpp:136] Iteration 16100, lr = 1e-05, m = 0.9
I0815 21:49:34.186939 20344 data_reader.cpp:288] Starting prefetch of epoch 13
I0815 21:49:35.333593 20340 solver.cpp:312] Iteration 16200 (5.13997 iter/s, 19.4554s/100 iter), loss = 0.0824674
I0815 21:49:35.333616 20340 solver.cpp:334]     Train net output #0: loss = 0.0824672 (* 1 = 0.0824672 loss)
I0815 21:49:35.333621 20340 sgd_solver.cpp:136] Iteration 16200, lr = 1e-05, m = 0.9
I0815 21:49:54.739296 20340 solver.cpp:312] Iteration 16300 (5.15327 iter/s, 19.4052s/100 iter), loss = 0.072472
I0815 21:49:54.739326 20340 solver.cpp:334]     Train net output #0: loss = 0.0724718 (* 1 = 0.0724718 loss)
I0815 21:49:54.739331 20340 sgd_solver.cpp:136] Iteration 16300, lr = 1e-05, m = 0.9
I0815 21:50:14.124903 20340 solver.cpp:312] Iteration 16400 (5.15861 iter/s, 19.3851s/100 iter), loss = 0.0451146
I0815 21:50:14.124951 20340 solver.cpp:334]     Train net output #0: loss = 0.0451145 (* 1 = 0.0451145 loss)
I0815 21:50:14.124958 20340 sgd_solver.cpp:136] Iteration 16400, lr = 1e-05, m = 0.9
I0815 21:50:33.656622 20340 solver.cpp:312] Iteration 16500 (5.12002 iter/s, 19.5312s/100 iter), loss = 0.0696338
I0815 21:50:33.656646 20340 solver.cpp:334]     Train net output #0: loss = 0.0696336 (* 1 = 0.0696336 loss)
I0815 21:50:33.656649 20340 sgd_solver.cpp:136] Iteration 16500, lr = 1e-05, m = 0.9
I0815 21:50:38.265662 20348 data_reader.cpp:288] Starting prefetch of epoch 11
I0815 21:50:53.009505 20340 solver.cpp:312] Iteration 16600 (5.16733 iter/s, 19.3524s/100 iter), loss = 0.0729952
I0815 21:50:53.009627 20340 solver.cpp:334]     Train net output #0: loss = 0.072995 (* 1 = 0.072995 loss)
I0815 21:50:53.009634 20340 sgd_solver.cpp:136] Iteration 16600, lr = 1e-05, m = 0.9
I0815 21:51:10.513000 20314 data_reader.cpp:288] Starting prefetch of epoch 10
I0815 21:51:12.455721 20340 solver.cpp:312] Iteration 16700 (5.14253 iter/s, 19.4457s/100 iter), loss = 0.0970855
I0815 21:51:12.455746 20340 solver.cpp:334]     Train net output #0: loss = 0.0970853 (* 1 = 0.0970853 loss)
I0815 21:51:12.455754 20340 sgd_solver.cpp:136] Iteration 16700, lr = 1e-05, m = 0.9
I0815 21:51:31.845571 20340 solver.cpp:312] Iteration 16800 (5.15748 iter/s, 19.3893s/100 iter), loss = 0.0821061
I0815 21:51:31.845633 20340 solver.cpp:334]     Train net output #0: loss = 0.082106 (* 1 = 0.082106 loss)
I0815 21:51:31.845640 20340 sgd_solver.cpp:136] Iteration 16800, lr = 1e-05, m = 0.9
I0815 21:51:51.234591 20340 solver.cpp:312] Iteration 16900 (5.1577 iter/s, 19.3885s/100 iter), loss = 0.0667725
I0815 21:51:51.234613 20340 solver.cpp:334]     Train net output #0: loss = 0.0667724 (* 1 = 0.0667724 loss)
I0815 21:51:51.234617 20340 sgd_solver.cpp:136] Iteration 16900, lr = 1e-05, m = 0.9
I0815 21:52:10.647358 20340 solver.cpp:312] Iteration 17000 (5.15139 iter/s, 19.4122s/100 iter), loss = 0.106915
I0815 21:52:10.647406 20340 solver.cpp:334]     Train net output #0: loss = 0.106914 (* 1 = 0.106914 loss)
I0815 21:52:10.647413 20340 sgd_solver.cpp:136] Iteration 17000, lr = 1e-05, m = 0.9
I0815 21:52:14.568336 20314 data_reader.cpp:288] Starting prefetch of epoch 11
I0815 21:52:30.222038 20340 solver.cpp:312] Iteration 17100 (5.10878 iter/s, 19.5741s/100 iter), loss = 0.0752588
I0815 21:52:30.222065 20340 solver.cpp:334]     Train net output #0: loss = 0.0752586 (* 1 = 0.0752586 loss)
I0815 21:52:30.222072 20340 sgd_solver.cpp:136] Iteration 17100, lr = 1e-05, m = 0.9
I0815 21:52:50.064724 20340 solver.cpp:312] Iteration 17200 (5.03978 iter/s, 19.8421s/100 iter), loss = 0.0900699
I0815 21:52:50.064775 20340 solver.cpp:334]     Train net output #0: loss = 0.0900698 (* 1 = 0.0900698 loss)
I0815 21:52:50.064782 20340 sgd_solver.cpp:136] Iteration 17200, lr = 1e-05, m = 0.9
I0815 21:53:09.546895 20340 solver.cpp:312] Iteration 17300 (5.13304 iter/s, 19.4816s/100 iter), loss = 0.0545636
I0815 21:53:09.546914 20340 solver.cpp:334]     Train net output #0: loss = 0.0545634 (* 1 = 0.0545634 loss)
I0815 21:53:09.546918 20340 sgd_solver.cpp:136] Iteration 17300, lr = 1e-05, m = 0.9
I0815 21:53:19.455206 20347 data_reader.cpp:288] Starting prefetch of epoch 9
I0815 21:53:29.137043 20340 solver.cpp:312] Iteration 17400 (5.10475 iter/s, 19.5896s/100 iter), loss = 0.056833
I0815 21:53:29.137092 20340 solver.cpp:334]     Train net output #0: loss = 0.0568329 (* 1 = 0.0568329 loss)
I0815 21:53:29.137099 20340 sgd_solver.cpp:136] Iteration 17400, lr = 1e-05, m = 0.9
I0815 21:53:48.687974 20340 solver.cpp:312] Iteration 17500 (5.11499 iter/s, 19.5504s/100 iter), loss = 0.0504352
I0815 21:53:48.687996 20340 solver.cpp:334]     Train net output #0: loss = 0.0504351 (* 1 = 0.0504351 loss)
I0815 21:53:48.688000 20340 sgd_solver.cpp:136] Iteration 17500, lr = 1e-05, m = 0.9
I0815 21:53:51.860638 20343 data_reader.cpp:288] Starting prefetch of epoch 9
I0815 21:54:08.071648 20340 solver.cpp:312] Iteration 17600 (5.15912 iter/s, 19.3831s/100 iter), loss = 0.0820081
I0815 21:54:08.088259 20340 solver.cpp:334]     Train net output #0: loss = 0.082008 (* 1 = 0.082008 loss)
I0815 21:54:08.088304 20340 sgd_solver.cpp:136] Iteration 17600, lr = 1e-05, m = 0.9
I0815 21:54:27.696913 20340 solver.cpp:312] Iteration 17700 (5.09561 iter/s, 19.6247s/100 iter), loss = 0.0646257
I0815 21:54:27.696938 20340 solver.cpp:334]     Train net output #0: loss = 0.0646256 (* 1 = 0.0646256 loss)
I0815 21:54:27.696944 20340 sgd_solver.cpp:136] Iteration 17700, lr = 1e-05, m = 0.9
I0815 21:54:47.068032 20340 solver.cpp:312] Iteration 17800 (5.16246 iter/s, 19.3706s/100 iter), loss = 0.0627478
I0815 21:54:47.068096 20340 solver.cpp:334]     Train net output #0: loss = 0.0627476 (* 1 = 0.0627476 loss)
I0815 21:54:47.068104 20340 sgd_solver.cpp:136] Iteration 17800, lr = 1e-05, m = 0.9
I0815 21:54:56.051527 20314 data_reader.cpp:288] Starting prefetch of epoch 12
I0815 21:55:06.488045 20340 solver.cpp:312] Iteration 17900 (5.14947 iter/s, 19.4195s/100 iter), loss = 0.0881428
I0815 21:55:06.488075 20340 solver.cpp:334]     Train net output #0: loss = 0.0881427 (* 1 = 0.0881427 loss)
I0815 21:55:06.488081 20340 sgd_solver.cpp:136] Iteration 17900, lr = 1e-05, m = 0.9
I0815 21:55:25.607326 20340 solver.cpp:509] Iteration 18000, Testing net (#0)
I0815 21:55:37.405570 20340 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.952128
I0815 21:55:37.405596 20340 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999398
I0815 21:55:37.405601 20340 solver.cpp:594]     Test net output #2: loss = 0.185021 (* 1 = 0.185021 loss)
I0815 21:55:37.405632 20340 solver.cpp:264] [MultiGPU] Tests completed in 11.798s
I0815 21:55:37.619376 20340 solver.cpp:312] Iteration 18000 (3.21229 iter/s, 31.1305s/100 iter), loss = 0.0493416
I0815 21:55:37.619421 20340 solver.cpp:334]     Train net output #0: loss = 0.0493414 (* 1 = 0.0493414 loss)
I0815 21:55:37.619431 20340 sgd_solver.cpp:136] Iteration 18000, lr = 1e-05, m = 0.9
I0815 21:55:40.080613 20348 data_reader.cpp:288] Starting prefetch of epoch 12
I0815 21:55:57.452165 20340 solver.cpp:312] Iteration 18100 (5.04229 iter/s, 19.8322s/100 iter), loss = 0.0614519
I0815 21:55:57.452217 20340 solver.cpp:334]     Train net output #0: loss = 0.0614518 (* 1 = 0.0614518 loss)
I0815 21:55:57.452224 20340 sgd_solver.cpp:136] Iteration 18100, lr = 1e-05, m = 0.9
I0815 21:56:13.913151 20343 data_reader.cpp:288] Starting prefetch of epoch 10
I0815 21:56:18.293324 20340 solver.cpp:312] Iteration 18200 (4.79833 iter/s, 20.8406s/100 iter), loss = 0.0687756
I0815 21:56:18.293349 20340 solver.cpp:334]     Train net output #0: loss = 0.0687755 (* 1 = 0.0687755 loss)
I0815 21:56:18.293354 20340 sgd_solver.cpp:136] Iteration 18200, lr = 1e-05, m = 0.9
I0815 21:56:37.594873 20340 solver.cpp:312] Iteration 18300 (5.18107 iter/s, 19.301s/100 iter), loss = 0.0756623
I0815 21:56:37.594929 20340 solver.cpp:334]     Train net output #0: loss = 0.0756622 (* 1 = 0.0756622 loss)
I0815 21:56:37.594936 20340 sgd_solver.cpp:136] Iteration 18300, lr = 1e-05, m = 0.9
I0815 21:56:57.143744 20340 solver.cpp:312] Iteration 18400 (5.11553 iter/s, 19.5483s/100 iter), loss = 0.0706558
I0815 21:56:57.143765 20340 solver.cpp:334]     Train net output #0: loss = 0.0706557 (* 1 = 0.0706557 loss)
I0815 21:56:57.143771 20340 sgd_solver.cpp:136] Iteration 18400, lr = 1e-05, m = 0.9
I0815 21:57:16.402951 20340 solver.cpp:312] Iteration 18500 (5.19247 iter/s, 19.2587s/100 iter), loss = 0.0791771
I0815 21:57:16.403005 20340 solver.cpp:334]     Train net output #0: loss = 0.079177 (* 1 = 0.079177 loss)
I0815 21:57:16.403012 20340 sgd_solver.cpp:136] Iteration 18500, lr = 1e-05, m = 0.9
I0815 21:57:17.984721 20344 data_reader.cpp:288] Starting prefetch of epoch 14
I0815 21:57:35.825989 20340 solver.cpp:312] Iteration 18600 (5.14867 iter/s, 19.4225s/100 iter), loss = 0.0654734
I0815 21:57:35.826010 20340 solver.cpp:334]     Train net output #0: loss = 0.0654733 (* 1 = 0.0654733 loss)
I0815 21:57:35.826015 20340 sgd_solver.cpp:136] Iteration 18600, lr = 1e-05, m = 0.9
I0815 21:57:55.172663 20340 solver.cpp:312] Iteration 18700 (5.16899 iter/s, 19.3461s/100 iter), loss = 0.0472058
I0815 21:57:55.172711 20340 solver.cpp:334]     Train net output #0: loss = 0.0472057 (* 1 = 0.0472057 loss)
I0815 21:57:55.172716 20340 sgd_solver.cpp:136] Iteration 18700, lr = 1e-05, m = 0.9
I0815 21:58:14.705561 20340 solver.cpp:312] Iteration 18800 (5.11971 iter/s, 19.5324s/100 iter), loss = 0.138831
I0815 21:58:14.705590 20340 solver.cpp:334]     Train net output #0: loss = 0.138831 (* 1 = 0.138831 loss)
I0815 21:58:14.705596 20340 sgd_solver.cpp:136] Iteration 18800, lr = 1e-05, m = 0.9
I0815 21:58:22.031708 20347 data_reader.cpp:288] Starting prefetch of epoch 10
I0815 21:58:34.004164 20340 solver.cpp:312] Iteration 18900 (5.18186 iter/s, 19.2981s/100 iter), loss = 0.072826
I0815 21:58:34.004231 20340 solver.cpp:334]     Train net output #0: loss = 0.0728259 (* 1 = 0.0728259 loss)
I0815 21:58:34.004236 20340 sgd_solver.cpp:136] Iteration 18900, lr = 1e-05, m = 0.9
I0815 21:58:53.449409 20340 solver.cpp:312] Iteration 19000 (5.14279 iter/s, 19.4447s/100 iter), loss = 0.110535
I0815 21:58:53.449437 20340 solver.cpp:334]     Train net output #0: loss = 0.110534 (* 1 = 0.110534 loss)
I0815 21:58:53.449443 20340 sgd_solver.cpp:136] Iteration 19000, lr = 1e-05, m = 0.9
I0815 21:59:12.824458 20340 solver.cpp:312] Iteration 19100 (5.16142 iter/s, 19.3745s/100 iter), loss = 0.0570659
I0815 21:59:12.833165 20340 solver.cpp:334]     Train net output #0: loss = 0.0570658 (* 1 = 0.0570658 loss)
I0815 21:59:12.833200 20340 sgd_solver.cpp:136] Iteration 19100, lr = 1e-05, m = 0.9
I0815 21:59:26.347614 20347 data_reader.cpp:288] Starting prefetch of epoch 11
I0815 21:59:32.306257 20340 solver.cpp:312] Iteration 19200 (5.13314 iter/s, 19.4813s/100 iter), loss = 0.0899752
I0815 21:59:32.306284 20340 solver.cpp:334]     Train net output #0: loss = 0.0899751 (* 1 = 0.0899751 loss)
I0815 21:59:32.306290 20340 sgd_solver.cpp:136] Iteration 19200, lr = 1e-05, m = 0.9
I0815 21:59:51.909133 20340 solver.cpp:312] Iteration 19300 (5.10143 iter/s, 19.6023s/100 iter), loss = 0.0984874
I0815 21:59:51.909204 20340 solver.cpp:334]     Train net output #0: loss = 0.0984873 (* 1 = 0.0984873 loss)
I0815 21:59:51.909211 20340 sgd_solver.cpp:136] Iteration 19300, lr = 1e-05, m = 0.9
I0815 21:59:58.405031 20314 data_reader.cpp:288] Starting prefetch of epoch 13
I0815 22:00:11.192304 20340 solver.cpp:312] Iteration 19400 (5.18601 iter/s, 19.2826s/100 iter), loss = 0.0957336
I0815 22:00:11.192328 20340 solver.cpp:334]     Train net output #0: loss = 0.0957335 (* 1 = 0.0957335 loss)
I0815 22:00:11.192333 20340 sgd_solver.cpp:136] Iteration 19400, lr = 1e-05, m = 0.9
I0815 22:00:30.523510 20340 solver.cpp:312] Iteration 19500 (5.17313 iter/s, 19.3307s/100 iter), loss = 0.0758252
I0815 22:00:30.523603 20340 solver.cpp:334]     Train net output #0: loss = 0.0758251 (* 1 = 0.0758251 loss)
I0815 22:00:30.523612 20340 sgd_solver.cpp:136] Iteration 19500, lr = 1e-05, m = 0.9
I0815 22:00:49.775473 20340 solver.cpp:312] Iteration 19600 (5.19442 iter/s, 19.2514s/100 iter), loss = 0.0855744
I0815 22:00:49.775497 20340 solver.cpp:334]     Train net output #0: loss = 0.0855743 (* 1 = 0.0855743 loss)
I0815 22:00:49.775503 20340 sgd_solver.cpp:136] Iteration 19600, lr = 1e-05, m = 0.9
I0815 22:01:02.425889 20316 data_reader.cpp:288] Starting prefetch of epoch 12
I0815 22:01:09.278808 20340 solver.cpp:312] Iteration 19700 (5.12747 iter/s, 19.5028s/100 iter), loss = 0.413458
I0815 22:01:09.278834 20340 solver.cpp:334]     Train net output #0: loss = 0.413458 (* 1 = 0.413458 loss)
I0815 22:01:09.278841 20340 sgd_solver.cpp:136] Iteration 19700, lr = 1e-05, m = 0.9
I0815 22:01:28.862474 20340 solver.cpp:312] Iteration 19800 (5.10644 iter/s, 19.5831s/100 iter), loss = 0.0701182
I0815 22:01:28.862498 20340 solver.cpp:334]     Train net output #0: loss = 0.0701181 (* 1 = 0.0701181 loss)
I0815 22:01:28.862504 20340 sgd_solver.cpp:136] Iteration 19800, lr = 1e-05, m = 0.9
I0815 22:01:49.119892 20340 solver.cpp:312] Iteration 19900 (4.9366 iter/s, 20.2569s/100 iter), loss = 0.0664639
I0815 22:01:49.119946 20340 solver.cpp:334]     Train net output #0: loss = 0.0664638 (* 1 = 0.0664638 loss)
I0815 22:01:49.119951 20340 sgd_solver.cpp:136] Iteration 19900, lr = 1e-05, m = 0.9
I0815 22:02:07.906409 20348 data_reader.cpp:288] Starting prefetch of epoch 13
I0815 22:02:08.712378 20340 solver.cpp:639] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/l1reg/cityscapes5_jsegnet21v2_iter_20000.caffemodel
I0815 22:02:08.762676 20340 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/l1reg/cityscapes5_jsegnet21v2_iter_20000.solverstate
I0815 22:02:08.772105 20340 solver.cpp:509] Iteration 20000, Testing net (#0)
I0815 22:02:12.329313 20389 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 22:02:20.591318 20340 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.950655
I0815 22:02:20.591392 20340 solver.cpp:594]     Test net output #1: accuracy/top5 = 1
I0815 22:02:20.591401 20340 solver.cpp:594]     Test net output #2: loss = 0.155207 (* 1 = 0.155207 loss)
I0815 22:02:20.591426 20340 solver.cpp:264] [MultiGPU] Tests completed in 11.819s
I0815 22:02:20.793279 20340 solver.cpp:312] Iteration 20000 (3.15731 iter/s, 31.6725s/100 iter), loss = 0.0496259
I0815 22:02:20.793304 20340 solver.cpp:334]     Train net output #0: loss = 0.0496258 (* 1 = 0.0496258 loss)
I0815 22:02:20.793308 20340 sgd_solver.cpp:136] Iteration 20000, lr = 1e-05, m = 0.9
I0815 22:02:40.202034 20340 solver.cpp:312] Iteration 20100 (5.15246 iter/s, 19.4082s/100 iter), loss = 0.0427253
I0815 22:02:40.202056 20340 solver.cpp:334]     Train net output #0: loss = 0.0427252 (* 1 = 0.0427252 loss)
I0815 22:02:40.202062 20340 sgd_solver.cpp:136] Iteration 20100, lr = 1e-05, m = 0.9
I0815 22:02:51.756261 20347 data_reader.cpp:288] Starting prefetch of epoch 12
I0815 22:02:59.521330 20340 solver.cpp:312] Iteration 20200 (5.17632 iter/s, 19.3188s/100 iter), loss = 0.0955568
I0815 22:02:59.521376 20340 solver.cpp:334]     Train net output #0: loss = 0.0955567 (* 1 = 0.0955567 loss)
I0815 22:02:59.521390 20340 sgd_solver.cpp:136] Iteration 20200, lr = 1e-05, m = 0.9
I0815 22:03:18.780706 20340 solver.cpp:312] Iteration 20300 (5.19242 iter/s, 19.2588s/100 iter), loss = 0.0656151
I0815 22:03:18.780730 20340 solver.cpp:334]     Train net output #0: loss = 0.0656149 (* 1 = 0.0656149 loss)
I0815 22:03:18.780733 20340 sgd_solver.cpp:136] Iteration 20300, lr = 1e-05, m = 0.9
I0815 22:03:38.576925 20340 solver.cpp:312] Iteration 20400 (5.05161 iter/s, 19.7957s/100 iter), loss = 0.0735535
I0815 22:03:38.577006 20340 solver.cpp:334]     Train net output #0: loss = 0.0735534 (* 1 = 0.0735534 loss)
I0815 22:03:38.577011 20340 sgd_solver.cpp:136] Iteration 20400, lr = 1e-05, m = 0.9
I0815 22:03:56.354912 20347 data_reader.cpp:288] Starting prefetch of epoch 13
I0815 22:03:58.111500 20340 solver.cpp:312] Iteration 20500 (5.11927 iter/s, 19.534s/100 iter), loss = 0.0594301
I0815 22:03:58.111522 20340 solver.cpp:334]     Train net output #0: loss = 0.0594299 (* 1 = 0.0594299 loss)
I0815 22:03:58.111528 20340 sgd_solver.cpp:136] Iteration 20500, lr = 1e-05, m = 0.9
I0815 22:04:17.643035 20340 solver.cpp:312] Iteration 20600 (5.12007 iter/s, 19.531s/100 iter), loss = 0.0590846
I0815 22:04:17.643086 20340 solver.cpp:334]     Train net output #0: loss = 0.0590845 (* 1 = 0.0590845 loss)
I0815 22:04:17.643091 20340 sgd_solver.cpp:136] Iteration 20600, lr = 1e-05, m = 0.9
I0815 22:04:28.562645 20343 data_reader.cpp:288] Starting prefetch of epoch 11
I0815 22:04:37.012163 20340 solver.cpp:312] Iteration 20700 (5.163 iter/s, 19.3686s/100 iter), loss = 0.0488809
I0815 22:04:37.012188 20340 solver.cpp:334]     Train net output #0: loss = 0.0488807 (* 1 = 0.0488807 loss)
I0815 22:04:37.012194 20340 sgd_solver.cpp:136] Iteration 20700, lr = 1e-05, m = 0.9
I0815 22:04:56.424635 20340 solver.cpp:312] Iteration 20800 (5.15147 iter/s, 19.4119s/100 iter), loss = 0.0438402
I0815 22:04:56.424688 20340 solver.cpp:334]     Train net output #0: loss = 0.04384 (* 1 = 0.04384 loss)
I0815 22:04:56.424695 20340 sgd_solver.cpp:136] Iteration 20800, lr = 1e-05, m = 0.9
I0815 22:05:15.932956 20340 solver.cpp:312] Iteration 20900 (5.12616 iter/s, 19.5078s/100 iter), loss = 0.0521751
I0815 22:05:15.932986 20340 solver.cpp:334]     Train net output #0: loss = 0.0521749 (* 1 = 0.0521749 loss)
I0815 22:05:15.932992 20340 sgd_solver.cpp:136] Iteration 20900, lr = 1e-05, m = 0.9
I0815 22:05:32.497756 20347 data_reader.cpp:288] Starting prefetch of epoch 14
I0815 22:05:34.959592 20340 solver.cpp:312] Iteration 21000 (5.25593 iter/s, 19.0261s/100 iter), loss = 0.0762028
I0815 22:05:34.959619 20340 solver.cpp:334]     Train net output #0: loss = 0.0762027 (* 1 = 0.0762027 loss)
I0815 22:05:34.959626 20340 sgd_solver.cpp:136] Iteration 21000, lr = 1e-05, m = 0.9
I0815 22:05:54.157135 20340 solver.cpp:312] Iteration 21100 (5.20914 iter/s, 19.197s/100 iter), loss = 0.0607682
I0815 22:05:54.157158 20340 solver.cpp:334]     Train net output #0: loss = 0.060768 (* 1 = 0.060768 loss)
I0815 22:05:54.157166 20340 sgd_solver.cpp:136] Iteration 21100, lr = 1e-05, m = 0.9
I0815 22:06:13.660311 20340 solver.cpp:312] Iteration 21200 (5.12751 iter/s, 19.5026s/100 iter), loss = 0.06693
I0815 22:06:13.660362 20340 solver.cpp:334]     Train net output #0: loss = 0.0669299 (* 1 = 0.0669299 loss)
I0815 22:06:13.660368 20340 sgd_solver.cpp:136] Iteration 21200, lr = 1e-05, m = 0.9
I0815 22:06:33.177750 20340 solver.cpp:312] Iteration 21300 (5.12376 iter/s, 19.5169s/100 iter), loss = 0.0820231
I0815 22:06:33.177778 20340 solver.cpp:334]     Train net output #0: loss = 0.082023 (* 1 = 0.082023 loss)
I0815 22:06:33.177784 20340 sgd_solver.cpp:136] Iteration 21300, lr = 1e-05, m = 0.9
I0815 22:06:36.597865 20316 data_reader.cpp:288] Starting prefetch of epoch 13
I0815 22:06:52.449029 20340 solver.cpp:312] Iteration 21400 (5.18921 iter/s, 19.2708s/100 iter), loss = 0.060342
I0815 22:06:52.449108 20340 solver.cpp:334]     Train net output #0: loss = 0.0603419 (* 1 = 0.0603419 loss)
I0815 22:06:52.449116 20340 sgd_solver.cpp:136] Iteration 21400, lr = 1e-05, m = 0.9
I0815 22:07:08.778952 20344 data_reader.cpp:288] Starting prefetch of epoch 15
I0815 22:07:12.021885 20340 solver.cpp:312] Iteration 21500 (5.10926 iter/s, 19.5723s/100 iter), loss = 0.0496411
I0815 22:07:12.021909 20340 solver.cpp:334]     Train net output #0: loss = 0.049641 (* 1 = 0.049641 loss)
I0815 22:07:12.021916 20340 sgd_solver.cpp:136] Iteration 21500, lr = 1e-05, m = 0.9
I0815 22:07:31.756603 20340 solver.cpp:312] Iteration 21600 (5.06735 iter/s, 19.7342s/100 iter), loss = 0.0783045
I0815 22:07:31.756654 20340 solver.cpp:334]     Train net output #0: loss = 0.0783044 (* 1 = 0.0783044 loss)
I0815 22:07:31.756660 20340 sgd_solver.cpp:136] Iteration 21600, lr = 1e-05, m = 0.9
I0815 22:07:51.060309 20340 solver.cpp:312] Iteration 21700 (5.18049 iter/s, 19.3032s/100 iter), loss = 0.0596376
I0815 22:07:51.060331 20340 solver.cpp:334]     Train net output #0: loss = 0.0596375 (* 1 = 0.0596375 loss)
I0815 22:07:51.060336 20340 sgd_solver.cpp:136] Iteration 21700, lr = 1e-05, m = 0.9
I0815 22:08:10.644134 20340 solver.cpp:312] Iteration 21800 (5.1064 iter/s, 19.5833s/100 iter), loss = 0.0668501
I0815 22:08:10.644179 20340 solver.cpp:334]     Train net output #0: loss = 0.06685 (* 1 = 0.06685 loss)
I0815 22:08:10.644186 20340 sgd_solver.cpp:136] Iteration 21800, lr = 1e-05, m = 0.9
I0815 22:08:13.171921 20314 data_reader.cpp:288] Starting prefetch of epoch 14
I0815 22:08:30.026271 20340 solver.cpp:312] Iteration 21900 (5.15953 iter/s, 19.3816s/100 iter), loss = 0.0672619
I0815 22:08:30.026296 20340 solver.cpp:334]     Train net output #0: loss = 0.0672618 (* 1 = 0.0672618 loss)
I0815 22:08:30.026304 20340 sgd_solver.cpp:136] Iteration 21900, lr = 1e-05, m = 0.9
I0815 22:08:49.162571 20340 solver.cpp:509] Iteration 22000, Testing net (#0)
I0815 22:08:56.525483 20389 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 22:09:01.167242 20340 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.952361
I0815 22:09:01.167268 20340 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999535
I0815 22:09:01.167277 20340 solver.cpp:594]     Test net output #2: loss = 0.183424 (* 1 = 0.183424 loss)
I0815 22:09:01.167332 20340 solver.cpp:264] [MultiGPU] Tests completed in 12.0044s
I0815 22:09:01.384243 20340 solver.cpp:312] Iteration 22000 (3.18907 iter/s, 31.3571s/100 iter), loss = 0.0870393
I0815 22:09:01.384268 20340 solver.cpp:334]     Train net output #0: loss = 0.0870392 (* 1 = 0.0870392 loss)
I0815 22:09:01.384274 20340 sgd_solver.cpp:136] Iteration 22000, lr = 1e-05, m = 0.9
I0815 22:09:20.811606 20340 solver.cpp:312] Iteration 22100 (5.14752 iter/s, 19.4268s/100 iter), loss = 0.0597997
I0815 22:09:20.811668 20340 solver.cpp:334]     Train net output #0: loss = 0.0597996 (* 1 = 0.0597996 loss)
I0815 22:09:20.811674 20340 sgd_solver.cpp:136] Iteration 22100, lr = 1e-05, m = 0.9
I0815 22:09:29.444658 20343 data_reader.cpp:288] Starting prefetch of epoch 12
I0815 22:09:40.206831 20340 solver.cpp:312] Iteration 22200 (5.15605 iter/s, 19.3947s/100 iter), loss = 0.0774289
I0815 22:09:40.206883 20340 solver.cpp:334]     Train net output #0: loss = 0.0774288 (* 1 = 0.0774288 loss)
I0815 22:09:40.206897 20340 sgd_solver.cpp:136] Iteration 22200, lr = 1e-05, m = 0.9
I0815 22:09:59.623039 20340 solver.cpp:312] Iteration 22300 (5.15048 iter/s, 19.4157s/100 iter), loss = 0.0496672
I0815 22:09:59.623087 20340 solver.cpp:334]     Train net output #0: loss = 0.0496671 (* 1 = 0.0496671 loss)
I0815 22:09:59.623095 20340 sgd_solver.cpp:136] Iteration 22300, lr = 1e-05, m = 0.9
I0815 22:10:19.056504 20340 solver.cpp:312] Iteration 22400 (5.1459 iter/s, 19.4329s/100 iter), loss = 0.0538586
I0815 22:10:19.056526 20340 solver.cpp:334]     Train net output #0: loss = 0.0538585 (* 1 = 0.0538585 loss)
I0815 22:10:19.056531 20340 sgd_solver.cpp:136] Iteration 22400, lr = 1e-05, m = 0.9
I0815 22:10:33.685928 20347 data_reader.cpp:288] Starting prefetch of epoch 15
I0815 22:10:38.718520 20340 solver.cpp:312] Iteration 22500 (5.08609 iter/s, 19.6615s/100 iter), loss = 0.0693155
I0815 22:10:38.718544 20340 solver.cpp:334]     Train net output #0: loss = 0.0693154 (* 1 = 0.0693154 loss)
I0815 22:10:38.718549 20340 sgd_solver.cpp:136] Iteration 22500, lr = 1e-05, m = 0.9
I0815 22:10:58.059425 20340 solver.cpp:312] Iteration 22600 (5.17053 iter/s, 19.3404s/100 iter), loss = 0.0467074
I0815 22:10:58.059447 20340 solver.cpp:334]     Train net output #0: loss = 0.0467073 (* 1 = 0.0467073 loss)
I0815 22:10:58.059453 20340 sgd_solver.cpp:136] Iteration 22600, lr = 1e-05, m = 0.9
I0815 22:11:17.471494 20340 solver.cpp:312] Iteration 22700 (5.15158 iter/s, 19.4115s/100 iter), loss = 0.0760816
I0815 22:11:17.471549 20340 solver.cpp:334]     Train net output #0: loss = 0.0760815 (* 1 = 0.0760815 loss)
I0815 22:11:17.471555 20340 sgd_solver.cpp:136] Iteration 22700, lr = 1e-05, m = 0.9
I0815 22:11:36.916137 20340 solver.cpp:312] Iteration 22800 (5.14295 iter/s, 19.4441s/100 iter), loss = 0.0443761
I0815 22:11:36.916162 20340 solver.cpp:334]     Train net output #0: loss = 0.0443761 (* 1 = 0.0443761 loss)
I0815 22:11:36.916168 20340 sgd_solver.cpp:136] Iteration 22800, lr = 1e-05, m = 0.9
I0815 22:11:37.899402 20316 data_reader.cpp:288] Starting prefetch of epoch 14
I0815 22:11:56.505901 20340 solver.cpp:312] Iteration 22900 (5.10485 iter/s, 19.5892s/100 iter), loss = 0.0744037
I0815 22:11:56.505964 20340 solver.cpp:334]     Train net output #0: loss = 0.0744036 (* 1 = 0.0744036 loss)
I0815 22:11:56.505970 20340 sgd_solver.cpp:136] Iteration 22900, lr = 1e-05, m = 0.9
I0815 22:12:10.373841 20314 data_reader.cpp:288] Starting prefetch of epoch 15
I0815 22:12:16.110270 20340 solver.cpp:312] Iteration 23000 (5.10104 iter/s, 19.6038s/100 iter), loss = 0.0867242
I0815 22:12:16.110294 20340 solver.cpp:334]     Train net output #0: loss = 0.0867241 (* 1 = 0.0867241 loss)
I0815 22:12:16.110301 20340 sgd_solver.cpp:136] Iteration 23000, lr = 1e-05, m = 0.9
I0815 22:12:35.688491 20340 solver.cpp:312] Iteration 23100 (5.10786 iter/s, 19.5777s/100 iter), loss = 0.0581464
I0815 22:12:35.688575 20340 solver.cpp:334]     Train net output #0: loss = 0.0581464 (* 1 = 0.0581464 loss)
I0815 22:12:35.688582 20340 sgd_solver.cpp:136] Iteration 23100, lr = 1e-05, m = 0.9
I0815 22:12:55.341956 20340 solver.cpp:312] Iteration 23200 (5.0883 iter/s, 19.6529s/100 iter), loss = 0.0512201
I0815 22:12:55.341981 20340 solver.cpp:334]     Train net output #0: loss = 0.05122 (* 1 = 0.05122 loss)
I0815 22:12:55.341985 20340 sgd_solver.cpp:136] Iteration 23200, lr = 1e-05, m = 0.9
I0815 22:13:14.760393 20340 solver.cpp:312] Iteration 23300 (5.14989 iter/s, 19.4179s/100 iter), loss = 0.0418441
I0815 22:13:14.760465 20340 solver.cpp:334]     Train net output #0: loss = 0.0418441 (* 1 = 0.0418441 loss)
I0815 22:13:14.760473 20340 sgd_solver.cpp:136] Iteration 23300, lr = 1e-05, m = 0.9
I0815 22:13:14.978732 20316 data_reader.cpp:288] Starting prefetch of epoch 15
I0815 22:13:34.263622 20340 solver.cpp:312] Iteration 23400 (5.1275 iter/s, 19.5027s/100 iter), loss = 0.0540716
I0815 22:13:34.263645 20340 solver.cpp:334]     Train net output #0: loss = 0.0540715 (* 1 = 0.0540715 loss)
I0815 22:13:34.263649 20340 sgd_solver.cpp:136] Iteration 23400, lr = 1e-05, m = 0.9
I0815 22:13:53.613840 20340 solver.cpp:312] Iteration 23500 (5.16804 iter/s, 19.3497s/100 iter), loss = 0.205649
I0815 22:13:53.613914 20340 solver.cpp:334]     Train net output #0: loss = 0.205649 (* 1 = 0.205649 loss)
I0815 22:13:53.613919 20340 sgd_solver.cpp:136] Iteration 23500, lr = 1e-05, m = 0.9
I0815 22:14:13.048552 20340 solver.cpp:312] Iteration 23600 (5.14557 iter/s, 19.4342s/100 iter), loss = 0.0442324
I0815 22:14:13.048573 20340 solver.cpp:334]     Train net output #0: loss = 0.0442323 (* 1 = 0.0442323 loss)
I0815 22:14:13.048579 20340 sgd_solver.cpp:136] Iteration 23600, lr = 1e-05, m = 0.9
I0815 22:14:19.050833 20347 data_reader.cpp:288] Starting prefetch of epoch 16
I0815 22:14:32.408994 20340 solver.cpp:312] Iteration 23700 (5.16531 iter/s, 19.3599s/100 iter), loss = 0.0640697
I0815 22:14:32.409071 20340 solver.cpp:334]     Train net output #0: loss = 0.0640696 (* 1 = 0.0640696 loss)
I0815 22:14:32.409080 20340 sgd_solver.cpp:136] Iteration 23700, lr = 1e-05, m = 0.9
I0815 22:14:51.282626 20344 data_reader.cpp:288] Starting prefetch of epoch 16
I0815 22:14:51.995898 20340 solver.cpp:312] Iteration 23800 (5.10559 iter/s, 19.5864s/100 iter), loss = 0.094189
I0815 22:14:51.995921 20340 solver.cpp:334]     Train net output #0: loss = 0.0941889 (* 1 = 0.0941889 loss)
I0815 22:14:51.995929 20340 sgd_solver.cpp:136] Iteration 23800, lr = 1e-05, m = 0.9
I0815 22:15:11.188769 20340 solver.cpp:312] Iteration 23900 (5.21041 iter/s, 19.1924s/100 iter), loss = 0.0615151
I0815 22:15:11.188818 20340 solver.cpp:334]     Train net output #0: loss = 0.061515 (* 1 = 0.061515 loss)
I0815 22:15:11.188825 20340 sgd_solver.cpp:136] Iteration 23900, lr = 1e-05, m = 0.9
I0815 22:15:30.388159 20340 solver.cpp:509] Iteration 24000, Testing net (#0)
I0815 22:15:33.902541 20389 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 22:15:42.710198 20340 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.951106
I0815 22:15:42.710252 20340 solver.cpp:594]     Test net output #1: accuracy/top5 = 1
I0815 22:15:42.710258 20340 solver.cpp:594]     Test net output #2: loss = 0.155748 (* 1 = 0.155748 loss)
I0815 22:15:42.710285 20340 solver.cpp:264] [MultiGPU] Tests completed in 12.3218s
I0815 22:15:42.799706 20394 sgd_solver.cpp:48] MultiStep Status: Iteration 24000, step = 1
I0815 22:15:42.799706 20393 sgd_solver.cpp:48] MultiStep Status: Iteration 24000, step = 1
I0815 22:15:42.799706 20395 sgd_solver.cpp:48] MultiStep Status: Iteration 24000, step = 1
I0815 22:15:42.910908 20340 solver.cpp:312] Iteration 24000 (3.15246 iter/s, 31.7213s/100 iter), loss = 0.0837923
I0815 22:15:42.910936 20340 solver.cpp:334]     Train net output #0: loss = 0.0837922 (* 1 = 0.0837922 loss)
I0815 22:15:42.910940 20340 sgd_solver.cpp:136] Iteration 24000, lr = 1e-06, m = 0.9
I0815 22:16:02.377009 20340 solver.cpp:312] Iteration 24100 (5.13728 iter/s, 19.4656s/100 iter), loss = 0.0733487
I0815 22:16:02.377030 20340 solver.cpp:334]     Train net output #0: loss = 0.0733486 (* 1 = 0.0733486 loss)
I0815 22:16:02.377034 20340 sgd_solver.cpp:136] Iteration 24100, lr = 1e-06, m = 0.9
I0815 22:16:07.545810 20348 data_reader.cpp:288] Starting prefetch of epoch 14
I0815 22:16:21.720396 20340 solver.cpp:312] Iteration 24200 (5.16987 iter/s, 19.3429s/100 iter), loss = 0.0984448
I0815 22:16:21.720453 20340 solver.cpp:334]     Train net output #0: loss = 0.0984447 (* 1 = 0.0984447 loss)
I0815 22:16:21.720460 20340 sgd_solver.cpp:136] Iteration 24200, lr = 1e-06, m = 0.9
I0815 22:16:39.794972 20344 data_reader.cpp:288] Starting prefetch of epoch 17
I0815 22:16:41.339844 20340 solver.cpp:312] Iteration 24300 (5.09712 iter/s, 19.6189s/100 iter), loss = 0.0563466
I0815 22:16:41.339867 20340 solver.cpp:334]     Train net output #0: loss = 0.0563465 (* 1 = 0.0563465 loss)
I0815 22:16:41.339872 20340 sgd_solver.cpp:136] Iteration 24300, lr = 1e-06, m = 0.9
I0815 22:17:00.812769 20340 solver.cpp:312] Iteration 24400 (5.13548 iter/s, 19.4724s/100 iter), loss = 0.0737933
I0815 22:17:00.812855 20340 solver.cpp:334]     Train net output #0: loss = 0.0737932 (* 1 = 0.0737932 loss)
I0815 22:17:00.812862 20340 sgd_solver.cpp:136] Iteration 24400, lr = 1e-06, m = 0.9
I0815 22:17:20.270941 20340 solver.cpp:312] Iteration 24500 (5.13937 iter/s, 19.4576s/100 iter), loss = 0.0552021
I0815 22:17:20.270964 20340 solver.cpp:334]     Train net output #0: loss = 0.055202 (* 1 = 0.055202 loss)
I0815 22:17:20.270972 20340 sgd_solver.cpp:136] Iteration 24500, lr = 1e-06, m = 0.9
I0815 22:17:39.834749 20340 solver.cpp:312] Iteration 24600 (5.11162 iter/s, 19.5633s/100 iter), loss = 0.095897
I0815 22:17:39.834805 20340 solver.cpp:334]     Train net output #0: loss = 0.0958969 (* 1 = 0.0958969 loss)
I0815 22:17:39.834811 20340 sgd_solver.cpp:136] Iteration 24600, lr = 1e-06, m = 0.9
I0815 22:17:44.395795 20343 data_reader.cpp:288] Starting prefetch of epoch 13
I0815 22:17:59.419193 20340 solver.cpp:312] Iteration 24700 (5.10623 iter/s, 19.5839s/100 iter), loss = 0.0697245
I0815 22:17:59.419217 20340 solver.cpp:334]     Train net output #0: loss = 0.0697244 (* 1 = 0.0697244 loss)
I0815 22:17:59.419221 20340 sgd_solver.cpp:136] Iteration 24700, lr = 1e-06, m = 0.9
I0815 22:18:18.745676 20340 solver.cpp:312] Iteration 24800 (5.17439 iter/s, 19.326s/100 iter), loss = 0.095143
I0815 22:18:18.745751 20340 solver.cpp:334]     Train net output #0: loss = 0.0951429 (* 1 = 0.0951429 loss)
I0815 22:18:18.745759 20340 sgd_solver.cpp:136] Iteration 24800, lr = 1e-06, m = 0.9
I0815 22:18:38.808868 20340 solver.cpp:312] Iteration 24900 (4.98439 iter/s, 20.0626s/100 iter), loss = 0.0641684
I0815 22:18:38.808892 20340 solver.cpp:334]     Train net output #0: loss = 0.0641683 (* 1 = 0.0641683 loss)
I0815 22:18:38.808897 20340 sgd_solver.cpp:136] Iteration 24900, lr = 1e-06, m = 0.9
I0815 22:18:49.493021 20316 data_reader.cpp:288] Starting prefetch of epoch 16
I0815 22:18:58.674554 20340 solver.cpp:312] Iteration 25000 (5.03394 iter/s, 19.8651s/100 iter), loss = 0.0744963
I0815 22:18:58.674578 20340 solver.cpp:334]     Train net output #0: loss = 0.0744962 (* 1 = 0.0744962 loss)
I0815 22:18:58.674584 20340 sgd_solver.cpp:136] Iteration 25000, lr = 1e-06, m = 0.9
I0815 22:19:17.933603 20340 solver.cpp:312] Iteration 25100 (5.19251 iter/s, 19.2585s/100 iter), loss = 0.0591091
I0815 22:19:17.933631 20340 solver.cpp:334]     Train net output #0: loss = 0.059109 (* 1 = 0.059109 loss)
I0815 22:19:17.933639 20340 sgd_solver.cpp:136] Iteration 25100, lr = 1e-06, m = 0.9
I0815 22:19:21.731133 20344 data_reader.cpp:288] Starting prefetch of epoch 18
I0815 22:19:37.511253 20340 solver.cpp:312] Iteration 25200 (5.108 iter/s, 19.5771s/100 iter), loss = 0.0868782
I0815 22:19:37.511281 20340 solver.cpp:334]     Train net output #0: loss = 0.0868781 (* 1 = 0.0868781 loss)
I0815 22:19:37.511287 20340 sgd_solver.cpp:136] Iteration 25200, lr = 1e-06, m = 0.9
I0815 22:19:56.976060 20340 solver.cpp:312] Iteration 25300 (5.13762 iter/s, 19.4643s/100 iter), loss = 0.0825749
I0815 22:19:56.976111 20340 solver.cpp:334]     Train net output #0: loss = 0.0825748 (* 1 = 0.0825748 loss)
I0815 22:19:56.976119 20340 sgd_solver.cpp:136] Iteration 25300, lr = 1e-06, m = 0.9
I0815 22:20:16.314599 20340 solver.cpp:312] Iteration 25400 (5.17116 iter/s, 19.338s/100 iter), loss = 0.0750396
I0815 22:20:16.314625 20340 solver.cpp:334]     Train net output #0: loss = 0.0750395 (* 1 = 0.0750395 loss)
I0815 22:20:16.314632 20340 sgd_solver.cpp:136] Iteration 25400, lr = 1e-06, m = 0.9
I0815 22:20:25.908221 20347 data_reader.cpp:288] Starting prefetch of epoch 17
I0815 22:20:35.801107 20340 solver.cpp:312] Iteration 25500 (5.1319 iter/s, 19.486s/100 iter), loss = 0.0534591
I0815 22:20:35.801214 20340 solver.cpp:334]     Train net output #0: loss = 0.053459 (* 1 = 0.053459 loss)
I0815 22:20:35.801223 20340 sgd_solver.cpp:136] Iteration 25500, lr = 1e-06, m = 0.9
I0815 22:20:55.368726 20340 solver.cpp:312] Iteration 25600 (5.11062 iter/s, 19.5671s/100 iter), loss = 0.0635905
I0815 22:20:55.368747 20340 solver.cpp:334]     Train net output #0: loss = 0.0635904 (* 1 = 0.0635904 loss)
I0815 22:20:55.368753 20340 sgd_solver.cpp:136] Iteration 25600, lr = 1e-06, m = 0.9
I0815 22:21:14.512449 20340 solver.cpp:312] Iteration 25700 (5.22379 iter/s, 19.1432s/100 iter), loss = 0.0463915
I0815 22:21:14.512500 20340 solver.cpp:334]     Train net output #0: loss = 0.0463914 (* 1 = 0.0463914 loss)
I0815 22:21:14.512504 20340 sgd_solver.cpp:136] Iteration 25700, lr = 1e-06, m = 0.9
I0815 22:21:29.986714 20347 data_reader.cpp:288] Starting prefetch of epoch 18
I0815 22:21:33.836220 20340 solver.cpp:312] Iteration 25800 (5.17512 iter/s, 19.3232s/100 iter), loss = 0.0699915
I0815 22:21:33.836246 20340 solver.cpp:334]     Train net output #0: loss = 0.0699914 (* 1 = 0.0699914 loss)
I0815 22:21:33.836251 20340 sgd_solver.cpp:136] Iteration 25800, lr = 1e-06, m = 0.9
I0815 22:21:53.215855 20340 solver.cpp:312] Iteration 25900 (5.1602 iter/s, 19.3791s/100 iter), loss = 0.0557967
I0815 22:21:53.215935 20340 solver.cpp:334]     Train net output #0: loss = 0.0557966 (* 1 = 0.0557966 loss)
I0815 22:21:53.215950 20340 sgd_solver.cpp:136] Iteration 25900, lr = 1e-06, m = 0.9
I0815 22:22:02.106600 20343 data_reader.cpp:288] Starting prefetch of epoch 14
I0815 22:22:12.651248 20340 solver.cpp:509] Iteration 26000, Testing net (#0)
I0815 22:22:24.555025 20340 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.952398
I0815 22:22:24.555124 20340 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999381
I0815 22:22:24.555136 20340 solver.cpp:594]     Test net output #2: loss = 0.18951 (* 1 = 0.18951 loss)
I0815 22:22:24.555199 20340 solver.cpp:264] [MultiGPU] Tests completed in 11.9036s
I0815 22:22:24.750391 20340 solver.cpp:312] Iteration 26000 (3.17121 iter/s, 31.5337s/100 iter), loss = 0.0852029
I0815 22:22:24.750414 20340 solver.cpp:334]     Train net output #0: loss = 0.0852028 (* 1 = 0.0852028 loss)
I0815 22:22:24.750421 20340 sgd_solver.cpp:136] Iteration 26000, lr = 1e-06, m = 0.9
I0815 22:22:44.310104 20340 solver.cpp:312] Iteration 26100 (5.11269 iter/s, 19.5592s/100 iter), loss = 0.0439342
I0815 22:22:44.310130 20340 solver.cpp:334]     Train net output #0: loss = 0.0439341 (* 1 = 0.0439341 loss)
I0815 22:22:44.310137 20340 sgd_solver.cpp:136] Iteration 26100, lr = 1e-06, m = 0.9
I0815 22:22:46.268014 20343 data_reader.cpp:288] Starting prefetch of epoch 15
I0815 22:23:03.708930 20340 solver.cpp:312] Iteration 26200 (5.15509 iter/s, 19.3983s/100 iter), loss = 0.0908666
I0815 22:23:03.709002 20340 solver.cpp:334]     Train net output #0: loss = 0.0908665 (* 1 = 0.0908665 loss)
I0815 22:23:03.709007 20340 sgd_solver.cpp:136] Iteration 26200, lr = 1e-06, m = 0.9
I0815 22:23:23.202327 20340 solver.cpp:312] Iteration 26300 (5.13008 iter/s, 19.4929s/100 iter), loss = 0.0796469
I0815 22:23:23.202352 20340 solver.cpp:334]     Train net output #0: loss = 0.0796468 (* 1 = 0.0796468 loss)
I0815 22:23:23.202356 20340 sgd_solver.cpp:136] Iteration 26300, lr = 1e-06, m = 0.9
I0815 22:23:42.695250 20340 solver.cpp:312] Iteration 26400 (5.13021 iter/s, 19.4924s/100 iter), loss = 0.0893417
I0815 22:23:42.695294 20340 solver.cpp:334]     Train net output #0: loss = 0.0893415 (* 1 = 0.0893415 loss)
I0815 22:23:42.695302 20340 sgd_solver.cpp:136] Iteration 26400, lr = 1e-06, m = 0.9
I0815 22:23:50.590639 20348 data_reader.cpp:288] Starting prefetch of epoch 15
I0815 22:24:02.215854 20340 solver.cpp:312] Iteration 26500 (5.12293 iter/s, 19.5201s/100 iter), loss = 0.0461657
I0815 22:24:02.215876 20340 solver.cpp:334]     Train net output #0: loss = 0.0461656 (* 1 = 0.0461656 loss)
I0815 22:24:02.215883 20340 sgd_solver.cpp:136] Iteration 26500, lr = 1e-06, m = 0.9
I0815 22:24:21.706744 20340 solver.cpp:312] Iteration 26600 (5.13074 iter/s, 19.4904s/100 iter), loss = 0.0786646
I0815 22:24:21.706810 20340 solver.cpp:334]     Train net output #0: loss = 0.0786645 (* 1 = 0.0786645 loss)
I0815 22:24:21.706815 20340 sgd_solver.cpp:136] Iteration 26600, lr = 1e-06, m = 0.9
I0815 22:24:22.933387 20343 data_reader.cpp:288] Starting prefetch of epoch 16
I0815 22:24:41.157234 20340 solver.cpp:312] Iteration 26700 (5.1414 iter/s, 19.45s/100 iter), loss = 0.0674228
I0815 22:24:41.157259 20340 solver.cpp:334]     Train net output #0: loss = 0.0674227 (* 1 = 0.0674227 loss)
I0815 22:24:41.157265 20340 sgd_solver.cpp:136] Iteration 26700, lr = 1e-06, m = 0.9
I0815 22:25:00.837994 20340 solver.cpp:312] Iteration 26800 (5.08124 iter/s, 19.6802s/100 iter), loss = 0.0851516
I0815 22:25:00.838078 20340 solver.cpp:334]     Train net output #0: loss = 0.0851515 (* 1 = 0.0851515 loss)
I0815 22:25:00.838086 20340 sgd_solver.cpp:136] Iteration 26800, lr = 1e-06, m = 0.9
I0815 22:25:20.232151 20340 solver.cpp:312] Iteration 26900 (5.15633 iter/s, 19.3936s/100 iter), loss = 0.0548329
I0815 22:25:20.232175 20340 solver.cpp:334]     Train net output #0: loss = 0.0548328 (* 1 = 0.0548328 loss)
I0815 22:25:20.232182 20340 sgd_solver.cpp:136] Iteration 26900, lr = 1e-06, m = 0.9
I0815 22:25:27.507388 20343 data_reader.cpp:288] Starting prefetch of epoch 17
I0815 22:25:39.795857 20340 solver.cpp:312] Iteration 27000 (5.11165 iter/s, 19.5632s/100 iter), loss = 0.107937
I0815 22:25:39.795904 20340 solver.cpp:334]     Train net output #0: loss = 0.107937 (* 1 = 0.107937 loss)
I0815 22:25:39.795909 20340 sgd_solver.cpp:136] Iteration 27000, lr = 1e-06, m = 0.9
I0815 22:25:59.325578 20340 solver.cpp:312] Iteration 27100 (5.12054 iter/s, 19.5292s/100 iter), loss = 0.0667986
I0815 22:25:59.325604 20340 solver.cpp:334]     Train net output #0: loss = 0.0667985 (* 1 = 0.0667985 loss)
I0815 22:25:59.325610 20340 sgd_solver.cpp:136] Iteration 27100, lr = 1e-06, m = 0.9
I0815 22:26:19.656445 20340 solver.cpp:312] Iteration 27200 (4.91876 iter/s, 20.3303s/100 iter), loss = 0.0433044
I0815 22:26:19.656502 20340 solver.cpp:334]     Train net output #0: loss = 0.0433042 (* 1 = 0.0433042 loss)
I0815 22:26:19.656507 20340 sgd_solver.cpp:136] Iteration 27200, lr = 1e-06, m = 0.9
I0815 22:26:32.593287 20348 data_reader.cpp:288] Starting prefetch of epoch 16
I0815 22:26:39.027997 20340 solver.cpp:312] Iteration 27300 (5.16235 iter/s, 19.371s/100 iter), loss = 0.0662793
I0815 22:26:39.028019 20340 solver.cpp:334]     Train net output #0: loss = 0.0662792 (* 1 = 0.0662792 loss)
I0815 22:26:39.028023 20340 sgd_solver.cpp:136] Iteration 27300, lr = 1e-06, m = 0.9
I0815 22:26:58.145345 20340 solver.cpp:312] Iteration 27400 (5.23099 iter/s, 19.1168s/100 iter), loss = 0.103427
I0815 22:26:58.145395 20340 solver.cpp:334]     Train net output #0: loss = 0.103427 (* 1 = 0.103427 loss)
I0815 22:26:58.145401 20340 sgd_solver.cpp:136] Iteration 27400, lr = 1e-06, m = 0.9
I0815 22:27:04.695806 20344 data_reader.cpp:288] Starting prefetch of epoch 19
I0815 22:27:17.730026 20340 solver.cpp:312] Iteration 27500 (5.10617 iter/s, 19.5841s/100 iter), loss = 0.0615918
I0815 22:27:17.730054 20340 solver.cpp:334]     Train net output #0: loss = 0.0615917 (* 1 = 0.0615917 loss)
I0815 22:27:17.730062 20340 sgd_solver.cpp:136] Iteration 27500, lr = 1e-06, m = 0.9
I0815 22:27:37.054550 20340 solver.cpp:312] Iteration 27600 (5.17491 iter/s, 19.324s/100 iter), loss = 0.0811987
I0815 22:27:37.054606 20340 solver.cpp:334]     Train net output #0: loss = 0.0811986 (* 1 = 0.0811986 loss)
I0815 22:27:37.054613 20340 sgd_solver.cpp:136] Iteration 27600, lr = 1e-06, m = 0.9
I0815 22:27:56.253489 20340 solver.cpp:312] Iteration 27700 (5.20876 iter/s, 19.1984s/100 iter), loss = 0.0820305
I0815 22:27:56.253513 20340 solver.cpp:334]     Train net output #0: loss = 0.0820304 (* 1 = 0.0820304 loss)
I0815 22:27:56.253520 20340 sgd_solver.cpp:136] Iteration 27700, lr = 1e-06, m = 0.9
I0815 22:28:08.465520 20343 data_reader.cpp:288] Starting prefetch of epoch 18
I0815 22:28:15.674504 20340 solver.cpp:312] Iteration 27800 (5.1492 iter/s, 19.4205s/100 iter), loss = 0.043198
I0815 22:28:15.674530 20340 solver.cpp:334]     Train net output #0: loss = 0.0431979 (* 1 = 0.0431979 loss)
I0815 22:28:15.674535 20340 sgd_solver.cpp:136] Iteration 27800, lr = 1e-06, m = 0.9
I0815 22:28:35.175464 20340 solver.cpp:312] Iteration 27900 (5.12809 iter/s, 19.5004s/100 iter), loss = 0.0571961
I0815 22:28:35.175487 20340 solver.cpp:334]     Train net output #0: loss = 0.057196 (* 1 = 0.057196 loss)
I0815 22:28:35.175490 20340 sgd_solver.cpp:136] Iteration 27900, lr = 1e-06, m = 0.9
I0815 22:28:54.466284 20340 solver.cpp:509] Iteration 28000, Testing net (#0)
I0815 22:28:58.036579 20391 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 22:29:06.398869 20340 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.951358
I0815 22:29:06.398893 20340 solver.cpp:594]     Test net output #1: accuracy/top5 = 1
I0815 22:29:06.398900 20340 solver.cpp:594]     Test net output #2: loss = 0.155065 (* 1 = 0.155065 loss)
I0815 22:29:06.399416 20340 solver.cpp:264] [MultiGPU] Tests completed in 11.9328s
I0815 22:29:06.589465 20340 solver.cpp:312] Iteration 28000 (3.18338 iter/s, 31.4131s/100 iter), loss = 0.0782925
I0815 22:29:06.589489 20340 solver.cpp:334]     Train net output #0: loss = 0.0782924 (* 1 = 0.0782924 loss)
I0815 22:29:06.589493 20340 sgd_solver.cpp:136] Iteration 28000, lr = 1e-06, m = 0.9
I0815 22:29:24.824710 20316 data_reader.cpp:288] Starting prefetch of epoch 17
I0815 22:29:26.020182 20340 solver.cpp:312] Iteration 28100 (5.14663 iter/s, 19.4302s/100 iter), loss = 0.075721
I0815 22:29:26.020205 20340 solver.cpp:334]     Train net output #0: loss = 0.0757209 (* 1 = 0.0757209 loss)
I0815 22:29:26.020210 20340 sgd_solver.cpp:136] Iteration 28100, lr = 1e-06, m = 0.9
I0815 22:29:45.540186 20340 solver.cpp:312] Iteration 28200 (5.12309 iter/s, 19.5195s/100 iter), loss = 0.0751694
I0815 22:29:45.540213 20340 solver.cpp:334]     Train net output #0: loss = 0.0751693 (* 1 = 0.0751693 loss)
I0815 22:29:45.540220 20340 sgd_solver.cpp:136] Iteration 28200, lr = 1e-06, m = 0.9
I0815 22:29:56.930387 20344 data_reader.cpp:288] Starting prefetch of epoch 20
I0815 22:30:04.770489 20340 solver.cpp:312] Iteration 28300 (5.20027 iter/s, 19.2298s/100 iter), loss = 0.0474279
I0815 22:30:04.770514 20340 solver.cpp:334]     Train net output #0: loss = 0.0474277 (* 1 = 0.0474277 loss)
I0815 22:30:04.770519 20340 sgd_solver.cpp:136] Iteration 28300, lr = 1e-06, m = 0.9
I0815 22:30:24.491544 20340 solver.cpp:312] Iteration 28400 (5.07086 iter/s, 19.7205s/100 iter), loss = 0.0843493
I0815 22:30:24.491570 20340 solver.cpp:334]     Train net output #0: loss = 0.0843492 (* 1 = 0.0843492 loss)
I0815 22:30:24.491576 20340 sgd_solver.cpp:136] Iteration 28400, lr = 1e-06, m = 0.9
I0815 22:30:44.186442 20340 solver.cpp:312] Iteration 28500 (5.0776 iter/s, 19.6944s/100 iter), loss = 0.075355
I0815 22:30:44.186491 20340 solver.cpp:334]     Train net output #0: loss = 0.0753548 (* 1 = 0.0753548 loss)
I0815 22:30:44.186496 20340 sgd_solver.cpp:136] Iteration 28500, lr = 1e-06, m = 0.9
I0815 22:31:01.707175 20314 data_reader.cpp:288] Starting prefetch of epoch 16
I0815 22:31:03.627086 20340 solver.cpp:312] Iteration 28600 (5.144 iter/s, 19.4401s/100 iter), loss = 0.0869939
I0815 22:31:03.627110 20340 solver.cpp:334]     Train net output #0: loss = 0.0869938 (* 1 = 0.0869938 loss)
I0815 22:31:03.627115 20340 sgd_solver.cpp:136] Iteration 28600, lr = 1e-06, m = 0.9
I0815 22:31:23.021142 20340 solver.cpp:312] Iteration 28700 (5.15636 iter/s, 19.3935s/100 iter), loss = 0.0861619
I0815 22:31:23.021212 20340 solver.cpp:334]     Train net output #0: loss = 0.0861618 (* 1 = 0.0861618 loss)
I0815 22:31:23.021219 20340 sgd_solver.cpp:136] Iteration 28700, lr = 1e-06, m = 0.9
I0815 22:31:42.343809 20340 solver.cpp:312] Iteration 28800 (5.17541 iter/s, 19.3221s/100 iter), loss = 0.0557022
I0815 22:31:42.343832 20340 solver.cpp:334]     Train net output #0: loss = 0.0557021 (* 1 = 0.0557021 loss)
I0815 22:31:42.343837 20340 sgd_solver.cpp:136] Iteration 28800, lr = 1e-06, m = 0.9
I0815 22:32:01.904758 20340 solver.cpp:312] Iteration 28900 (5.11237 iter/s, 19.5604s/100 iter), loss = 0.134713
I0815 22:32:01.904827 20340 solver.cpp:334]     Train net output #0: loss = 0.134713 (* 1 = 0.134713 loss)
I0815 22:32:01.904834 20340 sgd_solver.cpp:136] Iteration 28900, lr = 1e-06, m = 0.9
I0815 22:32:05.814517 20348 data_reader.cpp:288] Starting prefetch of epoch 17
I0815 22:32:21.546865 20340 solver.cpp:312] Iteration 29000 (5.09124 iter/s, 19.6416s/100 iter), loss = 0.0807742
I0815 22:32:21.546891 20340 solver.cpp:334]     Train net output #0: loss = 0.0807741 (* 1 = 0.0807741 loss)
I0815 22:32:21.546896 20340 sgd_solver.cpp:136] Iteration 29000, lr = 1e-06, m = 0.9
I0815 22:32:38.172719 20344 data_reader.cpp:288] Starting prefetch of epoch 21
I0815 22:32:41.088184 20340 solver.cpp:312] Iteration 29100 (5.1175 iter/s, 19.5408s/100 iter), loss = 0.076932
I0815 22:32:41.088205 20340 solver.cpp:334]     Train net output #0: loss = 0.0769319 (* 1 = 0.0769319 loss)
I0815 22:32:41.088209 20340 sgd_solver.cpp:136] Iteration 29100, lr = 1e-06, m = 0.9
I0815 22:33:00.861202 20340 solver.cpp:312] Iteration 29200 (5.05754 iter/s, 19.7725s/100 iter), loss = 0.0558579
I0815 22:33:00.861227 20340 solver.cpp:334]     Train net output #0: loss = 0.0558578 (* 1 = 0.0558578 loss)
I0815 22:33:00.861232 20340 sgd_solver.cpp:136] Iteration 29200, lr = 1e-06, m = 0.9
I0815 22:33:20.546221 20340 solver.cpp:312] Iteration 29300 (5.08014 iter/s, 19.6845s/100 iter), loss = 0.0560971
I0815 22:33:20.546273 20340 solver.cpp:334]     Train net output #0: loss = 0.056097 (* 1 = 0.056097 loss)
I0815 22:33:20.546278 20340 sgd_solver.cpp:136] Iteration 29300, lr = 1e-06, m = 0.9
I0815 22:33:39.778661 20340 solver.cpp:312] Iteration 29400 (5.19969 iter/s, 19.2319s/100 iter), loss = 0.054485
I0815 22:33:39.778687 20340 solver.cpp:334]     Train net output #0: loss = 0.0544849 (* 1 = 0.0544849 loss)
I0815 22:33:39.778692 20340 sgd_solver.cpp:136] Iteration 29400, lr = 1e-06, m = 0.9
I0815 22:33:42.919169 20348 data_reader.cpp:288] Starting prefetch of epoch 18
I0815 22:33:59.421006 20340 solver.cpp:312] Iteration 29500 (5.09118 iter/s, 19.6418s/100 iter), loss = 0.0801201
I0815 22:33:59.421061 20340 solver.cpp:334]     Train net output #0: loss = 0.08012 (* 1 = 0.08012 loss)
I0815 22:33:59.421066 20340 sgd_solver.cpp:136] Iteration 29500, lr = 1e-06, m = 0.9
I0815 22:34:19.268265 20340 solver.cpp:312] Iteration 29600 (5.03862 iter/s, 19.8467s/100 iter), loss = 0.0782107
I0815 22:34:19.268288 20340 solver.cpp:334]     Train net output #0: loss = 0.0782106 (* 1 = 0.0782106 loss)
I0815 22:34:19.268295 20340 sgd_solver.cpp:136] Iteration 29600, lr = 1e-06, m = 0.9
I0815 22:34:38.937443 20340 solver.cpp:312] Iteration 29700 (5.08424 iter/s, 19.6686s/100 iter), loss = 0.0615155
I0815 22:34:38.937495 20340 solver.cpp:334]     Train net output #0: loss = 0.0615154 (* 1 = 0.0615154 loss)
I0815 22:34:38.937500 20340 sgd_solver.cpp:136] Iteration 29700, lr = 1e-06, m = 0.9
I0815 22:34:47.871922 20348 data_reader.cpp:288] Starting prefetch of epoch 19
I0815 22:34:58.276587 20340 solver.cpp:312] Iteration 29800 (5.171 iter/s, 19.3386s/100 iter), loss = 0.131275
I0815 22:34:58.276610 20340 solver.cpp:334]     Train net output #0: loss = 0.131275 (* 1 = 0.131275 loss)
I0815 22:34:58.276614 20340 sgd_solver.cpp:136] Iteration 29800, lr = 1e-06, m = 0.9
I0815 22:35:17.858021 20340 solver.cpp:312] Iteration 29900 (5.10702 iter/s, 19.5809s/100 iter), loss = 0.0739719
I0815 22:35:17.858114 20340 solver.cpp:334]     Train net output #0: loss = 0.0739718 (* 1 = 0.0739718 loss)
I0815 22:35:17.858120 20340 sgd_solver.cpp:136] Iteration 29900, lr = 1e-06, m = 0.9
I0815 22:35:20.264994 20344 data_reader.cpp:288] Starting prefetch of epoch 22
I0815 22:35:37.316536 20340 solver.cpp:639] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/l1reg/cityscapes5_jsegnet21v2_iter_30000.caffemodel
I0815 22:35:37.501479 20340 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/l1reg/cityscapes5_jsegnet21v2_iter_30000.solverstate
I0815 22:35:37.516894 20340 solver.cpp:509] Iteration 30000, Testing net (#0)
I0815 22:35:49.276396 20340 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.952278
I0815 22:35:49.276526 20340 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999355
I0815 22:35:49.276536 20340 solver.cpp:594]     Test net output #2: loss = 0.189736 (* 1 = 0.189736 loss)
I0815 22:35:49.276563 20340 solver.cpp:264] [MultiGPU] Tests completed in 11.7593s
I0815 22:35:49.507056 20340 solver.cpp:312] Iteration 30000 (3.15974 iter/s, 31.6482s/100 iter), loss = 0.0795342
I0815 22:35:49.507083 20340 solver.cpp:334]     Train net output #0: loss = 0.0795341 (* 1 = 0.0795341 loss)
I0815 22:35:49.507091 20340 sgd_solver.cpp:136] Iteration 30000, lr = 1e-06, m = 0.9
I0815 22:36:04.757462 20344 data_reader.cpp:288] Starting prefetch of epoch 23
I0815 22:36:09.182349 20340 solver.cpp:312] Iteration 30100 (5.08266 iter/s, 19.6747s/100 iter), loss = 0.0667023
I0815 22:36:09.182376 20340 solver.cpp:334]     Train net output #0: loss = 0.0667022 (* 1 = 0.0667022 loss)
I0815 22:36:09.182381 20340 sgd_solver.cpp:136] Iteration 30100, lr = 1e-06, m = 0.9
I0815 22:36:28.444442 20340 solver.cpp:312] Iteration 30200 (5.19169 iter/s, 19.2616s/100 iter), loss = 0.0621481
I0815 22:36:28.444497 20340 solver.cpp:334]     Train net output #0: loss = 0.062148 (* 1 = 0.062148 loss)
I0815 22:36:28.444505 20340 sgd_solver.cpp:136] Iteration 30200, lr = 1e-06, m = 0.9
I0815 22:36:48.019304 20340 solver.cpp:312] Iteration 30300 (5.10873 iter/s, 19.5743s/100 iter), loss = 0.0854767
I0815 22:36:48.019326 20340 solver.cpp:334]     Train net output #0: loss = 0.0854766 (* 1 = 0.0854766 loss)
I0815 22:36:48.019331 20340 sgd_solver.cpp:136] Iteration 30300, lr = 1e-06, m = 0.9
I0815 22:37:07.434895 20340 solver.cpp:312] Iteration 30400 (5.15064 iter/s, 19.4151s/100 iter), loss = 0.074081
I0815 22:37:07.434954 20340 solver.cpp:334]     Train net output #0: loss = 0.0740809 (* 1 = 0.0740809 loss)
I0815 22:37:07.434962 20340 sgd_solver.cpp:136] Iteration 30400, lr = 1e-06, m = 0.9
I0815 22:37:08.951863 20316 data_reader.cpp:288] Starting prefetch of epoch 18
I0815 22:37:26.741588 20340 solver.cpp:312] Iteration 30500 (5.17969 iter/s, 19.3062s/100 iter), loss = 0.0791669
I0815 22:37:26.741614 20340 solver.cpp:334]     Train net output #0: loss = 0.0791669 (* 1 = 0.0791669 loss)
I0815 22:37:26.741621 20340 sgd_solver.cpp:136] Iteration 30500, lr = 1e-06, m = 0.9
I0815 22:37:40.870120 20344 data_reader.cpp:288] Starting prefetch of epoch 24
I0815 22:37:46.065138 20340 solver.cpp:312] Iteration 30600 (5.17518 iter/s, 19.323s/100 iter), loss = 0.0582716
I0815 22:37:46.065165 20340 solver.cpp:334]     Train net output #0: loss = 0.0582715 (* 1 = 0.0582715 loss)
I0815 22:37:46.065172 20340 sgd_solver.cpp:136] Iteration 30600, lr = 1e-06, m = 0.9
I0815 22:38:05.399670 20340 solver.cpp:312] Iteration 30700 (5.17224 iter/s, 19.334s/100 iter), loss = 0.147601
I0815 22:38:05.399694 20340 solver.cpp:334]     Train net output #0: loss = 0.1476 (* 1 = 0.1476 loss)
I0815 22:38:05.399698 20340 sgd_solver.cpp:136] Iteration 30700, lr = 1e-06, m = 0.9
I0815 22:38:24.763720 20340 solver.cpp:312] Iteration 30800 (5.16435 iter/s, 19.3635s/100 iter), loss = 0.0727277
I0815 22:38:24.763762 20340 solver.cpp:334]     Train net output #0: loss = 0.0727276 (* 1 = 0.0727276 loss)
I0815 22:38:24.763767 20340 sgd_solver.cpp:136] Iteration 30800, lr = 1e-06, m = 0.9
I0815 22:38:44.141718 20340 solver.cpp:312] Iteration 30900 (5.16063 iter/s, 19.3775s/100 iter), loss = 0.0703209
I0815 22:38:44.141741 20340 solver.cpp:334]     Train net output #0: loss = 0.0703208 (* 1 = 0.0703208 loss)
I0815 22:38:44.141744 20340 sgd_solver.cpp:136] Iteration 30900, lr = 1e-06, m = 0.9
I0815 22:38:44.732020 20344 data_reader.cpp:288] Starting prefetch of epoch 25
I0815 22:39:03.562834 20340 solver.cpp:312] Iteration 31000 (5.14918 iter/s, 19.4206s/100 iter), loss = 0.0646855
I0815 22:39:03.562904 20340 solver.cpp:334]     Train net output #0: loss = 0.0646854 (* 1 = 0.0646854 loss)
I0815 22:39:03.562911 20340 sgd_solver.cpp:136] Iteration 31000, lr = 1e-06, m = 0.9
I0815 22:39:23.030905 20340 solver.cpp:312] Iteration 31100 (5.13676 iter/s, 19.4675s/100 iter), loss = 0.0592646
I0815 22:39:23.030926 20340 solver.cpp:334]     Train net output #0: loss = 0.0592645 (* 1 = 0.0592645 loss)
I0815 22:39:23.030931 20340 sgd_solver.cpp:136] Iteration 31100, lr = 1e-06, m = 0.9
I0815 22:39:42.582192 20340 solver.cpp:312] Iteration 31200 (5.11489 iter/s, 19.5508s/100 iter), loss = 0.107137
I0815 22:39:42.582937 20340 solver.cpp:334]     Train net output #0: loss = 0.107137 (* 1 = 0.107137 loss)
I0815 22:39:42.582959 20340 sgd_solver.cpp:136] Iteration 31200, lr = 1e-06, m = 0.9
I0815 22:39:49.234690 20316 data_reader.cpp:288] Starting prefetch of epoch 19
I0815 22:40:01.998394 20340 solver.cpp:312] Iteration 31300 (5.15048 iter/s, 19.4157s/100 iter), loss = 0.0967286
I0815 22:40:01.998421 20340 solver.cpp:334]     Train net output #0: loss = 0.0967285 (* 1 = 0.0967285 loss)
I0815 22:40:01.998428 20340 sgd_solver.cpp:136] Iteration 31300, lr = 1e-06, m = 0.9
I0815 22:40:21.310854 20314 data_reader.cpp:288] Starting prefetch of epoch 17
I0815 22:40:21.450029 20340 solver.cpp:312] Iteration 31400 (5.1411 iter/s, 19.4511s/100 iter), loss = 0.0616355
I0815 22:40:21.450053 20340 solver.cpp:334]     Train net output #0: loss = 0.0616354 (* 1 = 0.0616354 loss)
I0815 22:40:21.450057 20340 sgd_solver.cpp:136] Iteration 31400, lr = 1e-06, m = 0.9
I0815 22:40:40.877178 20340 solver.cpp:312] Iteration 31500 (5.14758 iter/s, 19.4266s/100 iter), loss = 0.0671232
I0815 22:40:40.877197 20340 solver.cpp:334]     Train net output #0: loss = 0.0671231 (* 1 = 0.0671231 loss)
I0815 22:40:40.877202 20340 sgd_solver.cpp:136] Iteration 31500, lr = 1e-06, m = 0.9
I0815 22:41:00.141348 20340 solver.cpp:312] Iteration 31600 (5.19113 iter/s, 19.2636s/100 iter), loss = 0.139039
I0815 22:41:00.141402 20340 solver.cpp:334]     Train net output #0: loss = 0.139039 (* 1 = 0.139039 loss)
I0815 22:41:00.141407 20340 sgd_solver.cpp:136] Iteration 31600, lr = 1e-06, m = 0.9
I0815 22:41:19.675767 20340 solver.cpp:312] Iteration 31700 (5.11931 iter/s, 19.5339s/100 iter), loss = 0.0763931
I0815 22:41:19.675793 20340 solver.cpp:334]     Train net output #0: loss = 0.076393 (* 1 = 0.076393 loss)
I0815 22:41:19.675799 20340 sgd_solver.cpp:136] Iteration 31700, lr = 1e-06, m = 0.9
I0815 22:41:25.555701 20344 data_reader.cpp:288] Starting prefetch of epoch 26
I0815 22:41:39.007856 20340 solver.cpp:312] Iteration 31800 (5.17289 iter/s, 19.3316s/100 iter), loss = 0.0745503
I0815 22:41:39.007930 20340 solver.cpp:334]     Train net output #0: loss = 0.0745502 (* 1 = 0.0745502 loss)
I0815 22:41:39.007937 20340 sgd_solver.cpp:136] Iteration 31800, lr = 1e-06, m = 0.9
I0815 22:41:58.246103 20340 solver.cpp:312] Iteration 31900 (5.19812 iter/s, 19.2377s/100 iter), loss = 0.0444371
I0815 22:41:58.246129 20340 solver.cpp:334]     Train net output #0: loss = 0.044437 (* 1 = 0.044437 loss)
I0815 22:41:58.246134 20340 sgd_solver.cpp:136] Iteration 31900, lr = 1e-06, m = 0.9
I0815 22:42:17.742462 20340 solver.cpp:312] Iteration 31999 (5.07801 iter/s, 19.4958s/99 iter), loss = 0.0614077
I0815 22:42:17.742566 20340 solver.cpp:334]     Train net output #0: loss = 0.0614076 (* 1 = 0.0614076 loss)
I0815 22:42:17.784222 20340 solver.cpp:639] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/l1reg/cityscapes5_jsegnet21v2_iter_32000.caffemodel
I0815 22:42:17.829357 20340 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/l1reg/cityscapes5_jsegnet21v2_iter_32000.solverstate
I0815 22:42:17.908219 20340 solver.cpp:486] Iteration 32000, loss = 0.0621046
I0815 22:42:17.908239 20340 solver.cpp:509] Iteration 32000, Testing net (#0)
I0815 22:42:21.377460 20336 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 22:42:29.276087 20385 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 22:42:29.621076 20340 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.951159
I0815 22:42:29.621105 20340 solver.cpp:594]     Test net output #1: accuracy/top5 = 1
I0815 22:42:29.621110 20340 solver.cpp:594]     Test net output #2: loss = 0.156164 (* 1 = 0.156164 loss)
I0815 22:42:29.665659 20241 parallel.cpp:71] Root Solver performance on device 0: 4.977 * 6 = 29.86 img/sec (32000 itr in 6429 sec)
I0815 22:42:29.665681 20241 parallel.cpp:76]      Solver performance on device 1: 4.977 * 6 = 29.86 img/sec (32000 itr in 6429 sec)
I0815 22:42:29.665688 20241 parallel.cpp:76]      Solver performance on device 2: 4.977 * 6 = 29.86 img/sec (32000 itr in 6429 sec)
I0815 22:42:29.665690 20241 parallel.cpp:79] Overall multi-GPU performance: 89.5918 img/sec
I0815 22:42:30.982499 20241 caffe.cpp:247] Optimization Done in 1h 47m 27s
