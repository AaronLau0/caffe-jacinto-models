I0711 21:56:25.143950 13090 caffe.cpp:209] Using GPUs 0, 1, 2
I0711 21:56:25.144436 13090 caffe.cpp:214] GPU 0: GeForce GTX 1080
I0711 21:56:25.144773 13090 caffe.cpp:214] GPU 1: GeForce GTX 1080
I0711 21:56:25.145126 13090 caffe.cpp:214] GPU 2: GeForce GTX 1080
I0711 21:56:26.118041 13090 solver.cpp:48] Initializing solver from parameters: 
train_net: "training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/sparse/train.prototxt"
test_net: "training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/sparse/test.prototxt"
test_iter: 125
test_interval: 2000
base_lr: 1e-05
display: 100
max_iter: 32000
lr_policy: "multistep"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 1e-05
snapshot: 10000
snapshot_prefix: "training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/sparse/cityscapes5_jsegnet21v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
regularization_type: "L1"
test_initialization: false
stepvalue: 24000
iter_size: 1
type: "Adam"
display_sparsity: 1000
sparse_mode: SPARSE_UPDATE
sparsity_target: 0.8
sparsity_step_factor: 0.01
sparsity_step_iter: 1000
sparsity_start_iter: 0
sparsity_start_factor: 0.8
I0711 21:56:26.118132 13090 solver.cpp:82] Creating training net from train_net file: training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/sparse/train.prototxt
I0711 21:56:26.133301 13090 net.cpp:327] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0711 21:56:26.133332 13090 net.cpp:327] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
I0711 21:56:26.134222 13090 net.cpp:56] Initializing net from parameters: 
name: "jsegnet21v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/train-image-lmdb"
    label_list_path: "data/train-label-lmdb"
    batch_size: 5
    shuffle: false
    threads: 4
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b/bn"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a/bn"
  top: "out5a/bn"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a/bn"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b/bn"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a/bn"
  top: "out3a/bn"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a/bn"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv1/bn"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv2/bn"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv3/bn"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4/bn"
  top: "ctx_conv4/bn"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4/bn"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
I0711 21:56:26.142734 13090 layer_factory.hpp:77] Creating layer data
I0711 21:56:26.142781 13090 net.cpp:98] Creating Layer data
I0711 21:56:26.142802 13090 net.cpp:413] data -> data
I0711 21:56:26.142856 13090 net.cpp:413] data -> label
I0711 21:56:26.161072 13154 db_lmdb.cpp:35] Opened lmdb data/train-image-lmdb
I0711 21:56:26.161672 13159 db_lmdb.cpp:35] Opened lmdb data/train-label-lmdb
I0711 21:56:26.169936 13090 data_layer.cpp:78] ReshapePrefetch 5, 3, 640, 640
I0711 21:56:26.170123 13090 data_layer.cpp:83] output data size: 5,3,640,640
I0711 21:56:26.212430 13090 data_layer.cpp:78] ReshapePrefetch 5, 1, 640, 640
I0711 21:56:26.212502 13090 data_layer.cpp:83] output data size: 5,1,640,640
I0711 21:56:26.219228 13164 blocking_queue.cpp:50] Waiting for data
I0711 21:56:26.225298 13090 net.cpp:148] Setting up data
I0711 21:56:26.225318 13090 net.cpp:155] Top shape: 5 3 640 640 (6144000)
I0711 21:56:26.225322 13090 net.cpp:155] Top shape: 5 1 640 640 (2048000)
I0711 21:56:26.225323 13090 net.cpp:163] Memory required for data: 32768000
I0711 21:56:26.225329 13090 layer_factory.hpp:77] Creating layer data/bias
I0711 21:56:26.225337 13090 net.cpp:98] Creating Layer data/bias
I0711 21:56:26.225342 13090 net.cpp:439] data/bias <- data
I0711 21:56:26.225349 13090 net.cpp:413] data/bias -> data/bias
I0711 21:56:26.226560 13090 net.cpp:148] Setting up data/bias
I0711 21:56:26.226572 13090 net.cpp:155] Top shape: 5 3 640 640 (6144000)
I0711 21:56:26.226573 13090 net.cpp:163] Memory required for data: 57344000
I0711 21:56:26.226583 13090 layer_factory.hpp:77] Creating layer conv1a
I0711 21:56:26.226598 13090 net.cpp:98] Creating Layer conv1a
I0711 21:56:26.226600 13090 net.cpp:439] conv1a <- data/bias
I0711 21:56:26.226604 13090 net.cpp:413] conv1a -> conv1a
I0711 21:56:26.229665 13090 net.cpp:148] Setting up conv1a
I0711 21:56:26.229681 13090 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0711 21:56:26.229684 13090 net.cpp:163] Memory required for data: 122880000
I0711 21:56:26.229691 13090 layer_factory.hpp:77] Creating layer conv1a/bn
I0711 21:56:26.229696 13090 net.cpp:98] Creating Layer conv1a/bn
I0711 21:56:26.229699 13090 net.cpp:439] conv1a/bn <- conv1a
I0711 21:56:26.229702 13090 net.cpp:413] conv1a/bn -> conv1a/bn
I0711 21:56:26.231371 13090 net.cpp:148] Setting up conv1a/bn
I0711 21:56:26.231380 13090 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0711 21:56:26.231384 13090 net.cpp:163] Memory required for data: 188416000
I0711 21:56:26.231389 13090 layer_factory.hpp:77] Creating layer conv1a/relu
I0711 21:56:26.242259 13090 net.cpp:98] Creating Layer conv1a/relu
I0711 21:56:26.242269 13090 net.cpp:439] conv1a/relu <- conv1a/bn
I0711 21:56:26.242271 13090 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0711 21:56:26.242280 13090 net.cpp:148] Setting up conv1a/relu
I0711 21:56:26.242283 13090 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0711 21:56:26.242285 13090 net.cpp:163] Memory required for data: 253952000
I0711 21:56:26.242287 13090 layer_factory.hpp:77] Creating layer conv1b
I0711 21:56:26.242303 13090 net.cpp:98] Creating Layer conv1b
I0711 21:56:26.242306 13090 net.cpp:439] conv1b <- conv1a/bn
I0711 21:56:26.242308 13090 net.cpp:413] conv1b -> conv1b
I0711 21:56:26.242678 13090 net.cpp:148] Setting up conv1b
I0711 21:56:26.242684 13090 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0711 21:56:26.242686 13090 net.cpp:163] Memory required for data: 319488000
I0711 21:56:26.242691 13090 layer_factory.hpp:77] Creating layer conv1b/bn
I0711 21:56:26.242696 13090 net.cpp:98] Creating Layer conv1b/bn
I0711 21:56:26.242697 13090 net.cpp:439] conv1b/bn <- conv1b
I0711 21:56:26.242702 13090 net.cpp:413] conv1b/bn -> conv1b/bn
I0711 21:56:26.243401 13090 net.cpp:148] Setting up conv1b/bn
I0711 21:56:26.243407 13090 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0711 21:56:26.243409 13090 net.cpp:163] Memory required for data: 385024000
I0711 21:56:26.243413 13090 layer_factory.hpp:77] Creating layer conv1b/relu
I0711 21:56:26.243417 13090 net.cpp:98] Creating Layer conv1b/relu
I0711 21:56:26.243418 13090 net.cpp:439] conv1b/relu <- conv1b/bn
I0711 21:56:26.243420 13090 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0711 21:56:26.243424 13090 net.cpp:148] Setting up conv1b/relu
I0711 21:56:26.243427 13090 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0711 21:56:26.243428 13090 net.cpp:163] Memory required for data: 450560000
I0711 21:56:26.243430 13090 layer_factory.hpp:77] Creating layer pool1
I0711 21:56:26.243438 13090 net.cpp:98] Creating Layer pool1
I0711 21:56:26.243440 13090 net.cpp:439] pool1 <- conv1b/bn
I0711 21:56:26.243443 13090 net.cpp:413] pool1 -> pool1
I0711 21:56:26.246986 13090 net.cpp:148] Setting up pool1
I0711 21:56:26.247036 13090 net.cpp:155] Top shape: 5 32 160 160 (4096000)
I0711 21:56:26.247050 13090 net.cpp:163] Memory required for data: 466944000
I0711 21:56:26.247061 13090 layer_factory.hpp:77] Creating layer res2a_branch2a
I0711 21:56:26.247090 13090 net.cpp:98] Creating Layer res2a_branch2a
I0711 21:56:26.247102 13090 net.cpp:439] res2a_branch2a <- pool1
I0711 21:56:26.247123 13090 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0711 21:56:26.253763 13090 net.cpp:148] Setting up res2a_branch2a
I0711 21:56:26.253808 13090 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0711 21:56:26.253819 13090 net.cpp:163] Memory required for data: 499712000
I0711 21:56:26.253844 13090 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0711 21:56:26.253864 13090 net.cpp:98] Creating Layer res2a_branch2a/bn
I0711 21:56:26.253875 13090 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0711 21:56:26.253901 13090 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0711 21:56:26.256338 13090 net.cpp:148] Setting up res2a_branch2a/bn
I0711 21:56:26.256359 13090 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0711 21:56:26.256367 13090 net.cpp:163] Memory required for data: 532480000
I0711 21:56:26.256383 13090 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0711 21:56:26.256393 13090 net.cpp:98] Creating Layer res2a_branch2a/relu
I0711 21:56:26.256402 13090 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0711 21:56:26.256409 13090 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0711 21:56:26.256423 13090 net.cpp:148] Setting up res2a_branch2a/relu
I0711 21:56:26.256433 13090 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0711 21:56:26.256439 13090 net.cpp:163] Memory required for data: 565248000
I0711 21:56:26.256446 13090 layer_factory.hpp:77] Creating layer res2a_branch2b
I0711 21:56:26.256458 13090 net.cpp:98] Creating Layer res2a_branch2b
I0711 21:56:26.256467 13090 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0711 21:56:26.256476 13090 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0711 21:56:26.260087 13090 net.cpp:148] Setting up res2a_branch2b
I0711 21:56:26.260108 13090 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0711 21:56:26.260114 13090 net.cpp:163] Memory required for data: 598016000
I0711 21:56:26.260124 13090 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0711 21:56:26.260133 13090 net.cpp:98] Creating Layer res2a_branch2b/bn
I0711 21:56:26.260157 13090 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0711 21:56:26.260166 13090 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0711 21:56:26.261951 13090 net.cpp:148] Setting up res2a_branch2b/bn
I0711 21:56:26.261968 13090 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0711 21:56:26.261975 13090 net.cpp:163] Memory required for data: 630784000
I0711 21:56:26.261986 13090 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0711 21:56:26.261994 13090 net.cpp:98] Creating Layer res2a_branch2b/relu
I0711 21:56:26.262001 13090 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0711 21:56:26.262006 13090 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0711 21:56:26.262020 13090 net.cpp:148] Setting up res2a_branch2b/relu
I0711 21:56:26.262029 13090 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0711 21:56:26.262033 13090 net.cpp:163] Memory required for data: 663552000
I0711 21:56:26.262039 13090 layer_factory.hpp:77] Creating layer pool2
I0711 21:56:26.262046 13090 net.cpp:98] Creating Layer pool2
I0711 21:56:26.262053 13090 net.cpp:439] pool2 <- res2a_branch2b/bn
I0711 21:56:26.262059 13090 net.cpp:413] pool2 -> pool2
I0711 21:56:26.262152 13090 net.cpp:148] Setting up pool2
I0711 21:56:26.262162 13090 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 21:56:26.262167 13090 net.cpp:163] Memory required for data: 671744000
I0711 21:56:26.262172 13090 layer_factory.hpp:77] Creating layer res3a_branch2a
I0711 21:56:26.262182 13090 net.cpp:98] Creating Layer res3a_branch2a
I0711 21:56:26.262188 13090 net.cpp:439] res3a_branch2a <- pool2
I0711 21:56:26.262195 13090 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0711 21:56:26.266203 13090 net.cpp:148] Setting up res3a_branch2a
I0711 21:56:26.266216 13090 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0711 21:56:26.266219 13090 net.cpp:163] Memory required for data: 688128000
I0711 21:56:26.266225 13090 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0711 21:56:26.266232 13090 net.cpp:98] Creating Layer res3a_branch2a/bn
I0711 21:56:26.266237 13090 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0711 21:56:26.266242 13090 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0711 21:56:26.267498 13090 net.cpp:148] Setting up res3a_branch2a/bn
I0711 21:56:26.267508 13090 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0711 21:56:26.267513 13090 net.cpp:163] Memory required for data: 704512000
I0711 21:56:26.267524 13090 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0711 21:56:26.267531 13090 net.cpp:98] Creating Layer res3a_branch2a/relu
I0711 21:56:26.267535 13090 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0711 21:56:26.267540 13090 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0711 21:56:26.267549 13090 net.cpp:148] Setting up res3a_branch2a/relu
I0711 21:56:26.267555 13090 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0711 21:56:26.267558 13090 net.cpp:163] Memory required for data: 720896000
I0711 21:56:26.267563 13090 layer_factory.hpp:77] Creating layer res3a_branch2b
I0711 21:56:26.267571 13090 net.cpp:98] Creating Layer res3a_branch2b
I0711 21:56:26.267576 13090 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0711 21:56:26.267582 13090 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0711 21:56:26.269656 13090 net.cpp:148] Setting up res3a_branch2b
I0711 21:56:26.269670 13090 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0711 21:56:26.269673 13090 net.cpp:163] Memory required for data: 737280000
I0711 21:56:26.269678 13090 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0711 21:56:26.269685 13090 net.cpp:98] Creating Layer res3a_branch2b/bn
I0711 21:56:26.269688 13090 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0711 21:56:26.269695 13090 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0711 21:56:26.270802 13090 net.cpp:148] Setting up res3a_branch2b/bn
I0711 21:56:26.270814 13090 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0711 21:56:26.270818 13090 net.cpp:163] Memory required for data: 753664000
I0711 21:56:26.270838 13090 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0711 21:56:26.270843 13090 net.cpp:98] Creating Layer res3a_branch2b/relu
I0711 21:56:26.270846 13090 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0711 21:56:26.270850 13090 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0711 21:56:26.270856 13090 net.cpp:148] Setting up res3a_branch2b/relu
I0711 21:56:26.270862 13090 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0711 21:56:26.270865 13090 net.cpp:163] Memory required for data: 770048000
I0711 21:56:26.270869 13090 layer_factory.hpp:77] Creating layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0711 21:56:26.270874 13090 net.cpp:98] Creating Layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0711 21:56:26.270879 13090 net.cpp:439] res3a_branch2b/bn_res3a_branch2b/relu_0_split <- res3a_branch2b/bn
I0711 21:56:26.270882 13090 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0711 21:56:26.270889 13090 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0711 21:56:26.270967 13090 net.cpp:148] Setting up res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0711 21:56:26.270978 13090 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0711 21:56:26.270987 13090 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0711 21:56:26.270992 13090 net.cpp:163] Memory required for data: 802816000
I0711 21:56:26.270999 13090 layer_factory.hpp:77] Creating layer pool3
I0711 21:56:26.271008 13090 net.cpp:98] Creating Layer pool3
I0711 21:56:26.271014 13090 net.cpp:439] pool3 <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0711 21:56:26.271023 13090 net.cpp:413] pool3 -> pool3
I0711 21:56:26.271107 13090 net.cpp:148] Setting up pool3
I0711 21:56:26.271117 13090 net.cpp:155] Top shape: 5 128 40 40 (1024000)
I0711 21:56:26.271124 13090 net.cpp:163] Memory required for data: 806912000
I0711 21:56:26.271131 13090 layer_factory.hpp:77] Creating layer res4a_branch2a
I0711 21:56:26.271142 13090 net.cpp:98] Creating Layer res4a_branch2a
I0711 21:56:26.271149 13090 net.cpp:439] res4a_branch2a <- pool3
I0711 21:56:26.271157 13090 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0711 21:56:26.281518 13090 net.cpp:148] Setting up res4a_branch2a
I0711 21:56:26.281529 13090 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0711 21:56:26.281533 13090 net.cpp:163] Memory required for data: 815104000
I0711 21:56:26.281538 13090 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0711 21:56:26.281546 13090 net.cpp:98] Creating Layer res4a_branch2a/bn
I0711 21:56:26.281550 13090 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0711 21:56:26.281555 13090 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0711 21:56:26.282407 13090 net.cpp:148] Setting up res4a_branch2a/bn
I0711 21:56:26.282415 13090 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0711 21:56:26.282418 13090 net.cpp:163] Memory required for data: 823296000
I0711 21:56:26.282425 13090 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0711 21:56:26.282430 13090 net.cpp:98] Creating Layer res4a_branch2a/relu
I0711 21:56:26.282434 13090 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0711 21:56:26.282438 13090 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0711 21:56:26.282444 13090 net.cpp:148] Setting up res4a_branch2a/relu
I0711 21:56:26.282447 13090 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0711 21:56:26.282450 13090 net.cpp:163] Memory required for data: 831488000
I0711 21:56:26.282454 13090 layer_factory.hpp:77] Creating layer res4a_branch2b
I0711 21:56:26.282461 13090 net.cpp:98] Creating Layer res4a_branch2b
I0711 21:56:26.282465 13090 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0711 21:56:26.282469 13090 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0711 21:56:26.286404 13090 net.cpp:148] Setting up res4a_branch2b
I0711 21:56:26.286412 13090 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0711 21:56:26.286415 13090 net.cpp:163] Memory required for data: 839680000
I0711 21:56:26.286419 13090 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0711 21:56:26.286432 13090 net.cpp:98] Creating Layer res4a_branch2b/bn
I0711 21:56:26.286437 13090 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0711 21:56:26.286440 13090 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0711 21:56:26.287204 13090 net.cpp:148] Setting up res4a_branch2b/bn
I0711 21:56:26.287214 13090 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0711 21:56:26.287216 13090 net.cpp:163] Memory required for data: 847872000
I0711 21:56:26.287222 13090 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0711 21:56:26.287226 13090 net.cpp:98] Creating Layer res4a_branch2b/relu
I0711 21:56:26.287230 13090 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0711 21:56:26.287232 13090 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0711 21:56:26.287237 13090 net.cpp:148] Setting up res4a_branch2b/relu
I0711 21:56:26.287241 13090 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0711 21:56:26.287243 13090 net.cpp:163] Memory required for data: 856064000
I0711 21:56:26.287245 13090 layer_factory.hpp:77] Creating layer pool4
I0711 21:56:26.287250 13090 net.cpp:98] Creating Layer pool4
I0711 21:56:26.287252 13090 net.cpp:439] pool4 <- res4a_branch2b/bn
I0711 21:56:26.287256 13090 net.cpp:413] pool4 -> pool4
I0711 21:56:26.287302 13090 net.cpp:148] Setting up pool4
I0711 21:56:26.287308 13090 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0711 21:56:26.287313 13090 net.cpp:163] Memory required for data: 864256000
I0711 21:56:26.287317 13090 layer_factory.hpp:77] Creating layer res5a_branch2a
I0711 21:56:26.287325 13090 net.cpp:98] Creating Layer res5a_branch2a
I0711 21:56:26.287330 13090 net.cpp:439] res5a_branch2a <- pool4
I0711 21:56:26.287339 13090 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0711 21:56:26.313550 13090 net.cpp:148] Setting up res5a_branch2a
I0711 21:56:26.313565 13090 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0711 21:56:26.313568 13090 net.cpp:163] Memory required for data: 880640000
I0711 21:56:26.313573 13090 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0711 21:56:26.313581 13090 net.cpp:98] Creating Layer res5a_branch2a/bn
I0711 21:56:26.313585 13090 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0711 21:56:26.313590 13090 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0711 21:56:26.314270 13090 net.cpp:148] Setting up res5a_branch2a/bn
I0711 21:56:26.314276 13090 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0711 21:56:26.314278 13090 net.cpp:163] Memory required for data: 897024000
I0711 21:56:26.314283 13090 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0711 21:56:26.314287 13090 net.cpp:98] Creating Layer res5a_branch2a/relu
I0711 21:56:26.314290 13090 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0711 21:56:26.314291 13090 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0711 21:56:26.314296 13090 net.cpp:148] Setting up res5a_branch2a/relu
I0711 21:56:26.314298 13090 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0711 21:56:26.314299 13090 net.cpp:163] Memory required for data: 913408000
I0711 21:56:26.314301 13090 layer_factory.hpp:77] Creating layer res5a_branch2b
I0711 21:56:26.314306 13090 net.cpp:98] Creating Layer res5a_branch2b
I0711 21:56:26.314308 13090 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0711 21:56:26.314312 13090 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0711 21:56:26.327107 13090 net.cpp:148] Setting up res5a_branch2b
I0711 21:56:26.327116 13090 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0711 21:56:26.327118 13090 net.cpp:163] Memory required for data: 929792000
I0711 21:56:26.327124 13090 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0711 21:56:26.327128 13090 net.cpp:98] Creating Layer res5a_branch2b/bn
I0711 21:56:26.327131 13090 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0711 21:56:26.327133 13090 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0711 21:56:26.327795 13090 net.cpp:148] Setting up res5a_branch2b/bn
I0711 21:56:26.327802 13090 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0711 21:56:26.327813 13090 net.cpp:163] Memory required for data: 946176000
I0711 21:56:26.327819 13090 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0711 21:56:26.327822 13090 net.cpp:98] Creating Layer res5a_branch2b/relu
I0711 21:56:26.327824 13090 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0711 21:56:26.327826 13090 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0711 21:56:26.327831 13090 net.cpp:148] Setting up res5a_branch2b/relu
I0711 21:56:26.327832 13090 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0711 21:56:26.327834 13090 net.cpp:163] Memory required for data: 962560000
I0711 21:56:26.327836 13090 layer_factory.hpp:77] Creating layer out5a
I0711 21:56:26.327841 13090 net.cpp:98] Creating Layer out5a
I0711 21:56:26.327843 13090 net.cpp:439] out5a <- res5a_branch2b/bn
I0711 21:56:26.327847 13090 net.cpp:413] out5a -> out5a
I0711 21:56:26.331962 13090 net.cpp:148] Setting up out5a
I0711 21:56:26.331971 13090 net.cpp:155] Top shape: 5 64 40 40 (512000)
I0711 21:56:26.331974 13090 net.cpp:163] Memory required for data: 964608000
I0711 21:56:26.331977 13090 layer_factory.hpp:77] Creating layer out5a/bn
I0711 21:56:26.331981 13090 net.cpp:98] Creating Layer out5a/bn
I0711 21:56:26.331984 13090 net.cpp:439] out5a/bn <- out5a
I0711 21:56:26.331986 13090 net.cpp:413] out5a/bn -> out5a/bn
I0711 21:56:26.332720 13090 net.cpp:148] Setting up out5a/bn
I0711 21:56:26.332727 13090 net.cpp:155] Top shape: 5 64 40 40 (512000)
I0711 21:56:26.332729 13090 net.cpp:163] Memory required for data: 966656000
I0711 21:56:26.332734 13090 layer_factory.hpp:77] Creating layer out5a/relu
I0711 21:56:26.332737 13090 net.cpp:98] Creating Layer out5a/relu
I0711 21:56:26.332739 13090 net.cpp:439] out5a/relu <- out5a/bn
I0711 21:56:26.332741 13090 net.cpp:400] out5a/relu -> out5a/bn (in-place)
I0711 21:56:26.332746 13090 net.cpp:148] Setting up out5a/relu
I0711 21:56:26.332747 13090 net.cpp:155] Top shape: 5 64 40 40 (512000)
I0711 21:56:26.332749 13090 net.cpp:163] Memory required for data: 968704000
I0711 21:56:26.332751 13090 layer_factory.hpp:77] Creating layer out5a_up2
I0711 21:56:26.332759 13090 net.cpp:98] Creating Layer out5a_up2
I0711 21:56:26.332762 13090 net.cpp:439] out5a_up2 <- out5a/bn
I0711 21:56:26.332767 13090 net.cpp:413] out5a_up2 -> out5a_up2
I0711 21:56:26.333055 13090 net.cpp:148] Setting up out5a_up2
I0711 21:56:26.333061 13090 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 21:56:26.333063 13090 net.cpp:163] Memory required for data: 976896000
I0711 21:56:26.333066 13090 layer_factory.hpp:77] Creating layer out3a
I0711 21:56:26.333070 13090 net.cpp:98] Creating Layer out3a
I0711 21:56:26.333072 13090 net.cpp:439] out3a <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0711 21:56:26.333076 13090 net.cpp:413] out3a -> out3a
I0711 21:56:26.334121 13090 net.cpp:148] Setting up out3a
I0711 21:56:26.334128 13090 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 21:56:26.334131 13090 net.cpp:163] Memory required for data: 985088000
I0711 21:56:26.334133 13090 layer_factory.hpp:77] Creating layer out3a/bn
I0711 21:56:26.334137 13090 net.cpp:98] Creating Layer out3a/bn
I0711 21:56:26.334141 13090 net.cpp:439] out3a/bn <- out3a
I0711 21:56:26.334143 13090 net.cpp:413] out3a/bn -> out3a/bn
I0711 21:56:26.334877 13090 net.cpp:148] Setting up out3a/bn
I0711 21:56:26.334884 13090 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 21:56:26.334887 13090 net.cpp:163] Memory required for data: 993280000
I0711 21:56:26.334892 13090 layer_factory.hpp:77] Creating layer out3a/relu
I0711 21:56:26.334893 13090 net.cpp:98] Creating Layer out3a/relu
I0711 21:56:26.334895 13090 net.cpp:439] out3a/relu <- out3a/bn
I0711 21:56:26.334898 13090 net.cpp:400] out3a/relu -> out3a/bn (in-place)
I0711 21:56:26.334902 13090 net.cpp:148] Setting up out3a/relu
I0711 21:56:26.334904 13090 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 21:56:26.334905 13090 net.cpp:163] Memory required for data: 1001472000
I0711 21:56:26.334908 13090 layer_factory.hpp:77] Creating layer out3_out5_combined
I0711 21:56:26.334919 13090 net.cpp:98] Creating Layer out3_out5_combined
I0711 21:56:26.334921 13090 net.cpp:439] out3_out5_combined <- out5a_up2
I0711 21:56:26.334925 13090 net.cpp:439] out3_out5_combined <- out3a/bn
I0711 21:56:26.334928 13090 net.cpp:413] out3_out5_combined -> out3_out5_combined
I0711 21:56:26.334954 13090 net.cpp:148] Setting up out3_out5_combined
I0711 21:56:26.334957 13090 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 21:56:26.334959 13090 net.cpp:163] Memory required for data: 1009664000
I0711 21:56:26.334960 13090 layer_factory.hpp:77] Creating layer ctx_conv1
I0711 21:56:26.334964 13090 net.cpp:98] Creating Layer ctx_conv1
I0711 21:56:26.334966 13090 net.cpp:439] ctx_conv1 <- out3_out5_combined
I0711 21:56:26.334969 13090 net.cpp:413] ctx_conv1 -> ctx_conv1
I0711 21:56:26.336050 13090 net.cpp:148] Setting up ctx_conv1
I0711 21:56:26.336056 13090 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 21:56:26.336058 13090 net.cpp:163] Memory required for data: 1017856000
I0711 21:56:26.336062 13090 layer_factory.hpp:77] Creating layer ctx_conv1/bn
I0711 21:56:26.336066 13090 net.cpp:98] Creating Layer ctx_conv1/bn
I0711 21:56:26.336067 13090 net.cpp:439] ctx_conv1/bn <- ctx_conv1
I0711 21:56:26.336071 13090 net.cpp:413] ctx_conv1/bn -> ctx_conv1/bn
I0711 21:56:26.336805 13090 net.cpp:148] Setting up ctx_conv1/bn
I0711 21:56:26.336812 13090 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 21:56:26.336813 13090 net.cpp:163] Memory required for data: 1026048000
I0711 21:56:26.336822 13090 layer_factory.hpp:77] Creating layer ctx_conv1/relu
I0711 21:56:26.336825 13090 net.cpp:98] Creating Layer ctx_conv1/relu
I0711 21:56:26.336827 13090 net.cpp:439] ctx_conv1/relu <- ctx_conv1/bn
I0711 21:56:26.336830 13090 net.cpp:400] ctx_conv1/relu -> ctx_conv1/bn (in-place)
I0711 21:56:26.336833 13090 net.cpp:148] Setting up ctx_conv1/relu
I0711 21:56:26.336836 13090 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 21:56:26.336838 13090 net.cpp:163] Memory required for data: 1034240000
I0711 21:56:26.336839 13090 layer_factory.hpp:77] Creating layer ctx_conv2
I0711 21:56:26.336843 13090 net.cpp:98] Creating Layer ctx_conv2
I0711 21:56:26.336846 13090 net.cpp:439] ctx_conv2 <- ctx_conv1/bn
I0711 21:56:26.336848 13090 net.cpp:413] ctx_conv2 -> ctx_conv2
I0711 21:56:26.337903 13090 net.cpp:148] Setting up ctx_conv2
I0711 21:56:26.337908 13090 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 21:56:26.337910 13090 net.cpp:163] Memory required for data: 1042432000
I0711 21:56:26.337914 13090 layer_factory.hpp:77] Creating layer ctx_conv2/bn
I0711 21:56:26.337918 13090 net.cpp:98] Creating Layer ctx_conv2/bn
I0711 21:56:26.337919 13090 net.cpp:439] ctx_conv2/bn <- ctx_conv2
I0711 21:56:26.337923 13090 net.cpp:413] ctx_conv2/bn -> ctx_conv2/bn
I0711 21:56:26.338667 13090 net.cpp:148] Setting up ctx_conv2/bn
I0711 21:56:26.338675 13090 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 21:56:26.338676 13090 net.cpp:163] Memory required for data: 1050624000
I0711 21:56:26.338681 13090 layer_factory.hpp:77] Creating layer ctx_conv2/relu
I0711 21:56:26.338685 13090 net.cpp:98] Creating Layer ctx_conv2/relu
I0711 21:56:26.338687 13090 net.cpp:439] ctx_conv2/relu <- ctx_conv2/bn
I0711 21:56:26.338690 13090 net.cpp:400] ctx_conv2/relu -> ctx_conv2/bn (in-place)
I0711 21:56:26.338693 13090 net.cpp:148] Setting up ctx_conv2/relu
I0711 21:56:26.338696 13090 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 21:56:26.338697 13090 net.cpp:163] Memory required for data: 1058816000
I0711 21:56:26.338698 13090 layer_factory.hpp:77] Creating layer ctx_conv3
I0711 21:56:26.338702 13090 net.cpp:98] Creating Layer ctx_conv3
I0711 21:56:26.338704 13090 net.cpp:439] ctx_conv3 <- ctx_conv2/bn
I0711 21:56:26.338706 13090 net.cpp:413] ctx_conv3 -> ctx_conv3
I0711 21:56:26.339768 13090 net.cpp:148] Setting up ctx_conv3
I0711 21:56:26.339774 13090 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 21:56:26.339777 13090 net.cpp:163] Memory required for data: 1067008000
I0711 21:56:26.339781 13090 layer_factory.hpp:77] Creating layer ctx_conv3/bn
I0711 21:56:26.339789 13090 net.cpp:98] Creating Layer ctx_conv3/bn
I0711 21:56:26.339792 13090 net.cpp:439] ctx_conv3/bn <- ctx_conv3
I0711 21:56:26.339794 13090 net.cpp:413] ctx_conv3/bn -> ctx_conv3/bn
I0711 21:56:26.340534 13090 net.cpp:148] Setting up ctx_conv3/bn
I0711 21:56:26.340540 13090 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 21:56:26.340543 13090 net.cpp:163] Memory required for data: 1075200000
I0711 21:56:26.340548 13090 layer_factory.hpp:77] Creating layer ctx_conv3/relu
I0711 21:56:26.340550 13090 net.cpp:98] Creating Layer ctx_conv3/relu
I0711 21:56:26.340553 13090 net.cpp:439] ctx_conv3/relu <- ctx_conv3/bn
I0711 21:56:26.340555 13090 net.cpp:400] ctx_conv3/relu -> ctx_conv3/bn (in-place)
I0711 21:56:26.340560 13090 net.cpp:148] Setting up ctx_conv3/relu
I0711 21:56:26.340564 13090 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 21:56:26.340564 13090 net.cpp:163] Memory required for data: 1083392000
I0711 21:56:26.340566 13090 layer_factory.hpp:77] Creating layer ctx_conv4
I0711 21:56:26.340570 13090 net.cpp:98] Creating Layer ctx_conv4
I0711 21:56:26.340572 13090 net.cpp:439] ctx_conv4 <- ctx_conv3/bn
I0711 21:56:26.340575 13090 net.cpp:413] ctx_conv4 -> ctx_conv4
I0711 21:56:26.341640 13090 net.cpp:148] Setting up ctx_conv4
I0711 21:56:26.341647 13090 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 21:56:26.341650 13090 net.cpp:163] Memory required for data: 1091584000
I0711 21:56:26.341652 13090 layer_factory.hpp:77] Creating layer ctx_conv4/bn
I0711 21:56:26.341655 13090 net.cpp:98] Creating Layer ctx_conv4/bn
I0711 21:56:26.341657 13090 net.cpp:439] ctx_conv4/bn <- ctx_conv4
I0711 21:56:26.341660 13090 net.cpp:413] ctx_conv4/bn -> ctx_conv4/bn
I0711 21:56:26.342406 13090 net.cpp:148] Setting up ctx_conv4/bn
I0711 21:56:26.342411 13090 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 21:56:26.342414 13090 net.cpp:163] Memory required for data: 1099776000
I0711 21:56:26.342418 13090 layer_factory.hpp:77] Creating layer ctx_conv4/relu
I0711 21:56:26.342422 13090 net.cpp:98] Creating Layer ctx_conv4/relu
I0711 21:56:26.342424 13090 net.cpp:439] ctx_conv4/relu <- ctx_conv4/bn
I0711 21:56:26.342427 13090 net.cpp:400] ctx_conv4/relu -> ctx_conv4/bn (in-place)
I0711 21:56:26.342429 13090 net.cpp:148] Setting up ctx_conv4/relu
I0711 21:56:26.342432 13090 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 21:56:26.342433 13090 net.cpp:163] Memory required for data: 1107968000
I0711 21:56:26.342435 13090 layer_factory.hpp:77] Creating layer ctx_final
I0711 21:56:26.342439 13090 net.cpp:98] Creating Layer ctx_final
I0711 21:56:26.342442 13090 net.cpp:439] ctx_final <- ctx_conv4/bn
I0711 21:56:26.342444 13090 net.cpp:413] ctx_final -> ctx_final
I0711 21:56:26.342871 13090 net.cpp:148] Setting up ctx_final
I0711 21:56:26.342877 13090 net.cpp:155] Top shape: 5 8 80 80 (256000)
I0711 21:56:26.342880 13090 net.cpp:163] Memory required for data: 1108992000
I0711 21:56:26.342882 13090 layer_factory.hpp:77] Creating layer ctx_final/relu
I0711 21:56:26.342885 13090 net.cpp:98] Creating Layer ctx_final/relu
I0711 21:56:26.342887 13090 net.cpp:439] ctx_final/relu <- ctx_final
I0711 21:56:26.342890 13090 net.cpp:400] ctx_final/relu -> ctx_final (in-place)
I0711 21:56:26.342893 13090 net.cpp:148] Setting up ctx_final/relu
I0711 21:56:26.342895 13090 net.cpp:155] Top shape: 5 8 80 80 (256000)
I0711 21:56:26.342896 13090 net.cpp:163] Memory required for data: 1110016000
I0711 21:56:26.342898 13090 layer_factory.hpp:77] Creating layer out_deconv_final_up2
I0711 21:56:26.342901 13090 net.cpp:98] Creating Layer out_deconv_final_up2
I0711 21:56:26.342905 13090 net.cpp:439] out_deconv_final_up2 <- ctx_final
I0711 21:56:26.342906 13090 net.cpp:413] out_deconv_final_up2 -> out_deconv_final_up2
I0711 21:56:26.343173 13090 net.cpp:148] Setting up out_deconv_final_up2
I0711 21:56:26.343178 13090 net.cpp:155] Top shape: 5 8 160 160 (1024000)
I0711 21:56:26.343180 13090 net.cpp:163] Memory required for data: 1114112000
I0711 21:56:26.343183 13090 layer_factory.hpp:77] Creating layer out_deconv_final_up4
I0711 21:56:26.343192 13090 net.cpp:98] Creating Layer out_deconv_final_up4
I0711 21:56:26.343194 13090 net.cpp:439] out_deconv_final_up4 <- out_deconv_final_up2
I0711 21:56:26.343197 13090 net.cpp:413] out_deconv_final_up4 -> out_deconv_final_up4
I0711 21:56:26.343446 13090 net.cpp:148] Setting up out_deconv_final_up4
I0711 21:56:26.343452 13090 net.cpp:155] Top shape: 5 8 320 320 (4096000)
I0711 21:56:26.343454 13090 net.cpp:163] Memory required for data: 1130496000
I0711 21:56:26.343456 13090 layer_factory.hpp:77] Creating layer out_deconv_final_up8
I0711 21:56:26.343459 13090 net.cpp:98] Creating Layer out_deconv_final_up8
I0711 21:56:26.343462 13090 net.cpp:439] out_deconv_final_up8 <- out_deconv_final_up4
I0711 21:56:26.343464 13090 net.cpp:413] out_deconv_final_up8 -> out_deconv_final_up8
I0711 21:56:26.343710 13090 net.cpp:148] Setting up out_deconv_final_up8
I0711 21:56:26.343716 13090 net.cpp:155] Top shape: 5 8 640 640 (16384000)
I0711 21:56:26.343719 13090 net.cpp:163] Memory required for data: 1196032000
I0711 21:56:26.343720 13090 layer_factory.hpp:77] Creating layer loss
I0711 21:56:26.343726 13090 net.cpp:98] Creating Layer loss
I0711 21:56:26.343729 13090 net.cpp:439] loss <- out_deconv_final_up8
I0711 21:56:26.343731 13090 net.cpp:439] loss <- label
I0711 21:56:26.343734 13090 net.cpp:413] loss -> loss
I0711 21:56:26.343741 13090 layer_factory.hpp:77] Creating layer loss
I0711 21:56:26.364989 13090 net.cpp:148] Setting up loss
I0711 21:56:26.365010 13090 net.cpp:155] Top shape: (1)
I0711 21:56:26.365012 13090 net.cpp:158]     with loss weight 1
I0711 21:56:26.365026 13090 net.cpp:163] Memory required for data: 1196032004
I0711 21:56:26.365031 13090 net.cpp:224] loss needs backward computation.
I0711 21:56:26.365033 13090 net.cpp:224] out_deconv_final_up8 needs backward computation.
I0711 21:56:26.365036 13090 net.cpp:224] out_deconv_final_up4 needs backward computation.
I0711 21:56:26.365037 13090 net.cpp:224] out_deconv_final_up2 needs backward computation.
I0711 21:56:26.365039 13090 net.cpp:224] ctx_final/relu needs backward computation.
I0711 21:56:26.365041 13090 net.cpp:224] ctx_final needs backward computation.
I0711 21:56:26.365043 13090 net.cpp:224] ctx_conv4/relu needs backward computation.
I0711 21:56:26.365046 13090 net.cpp:224] ctx_conv4/bn needs backward computation.
I0711 21:56:26.365047 13090 net.cpp:224] ctx_conv4 needs backward computation.
I0711 21:56:26.365049 13090 net.cpp:224] ctx_conv3/relu needs backward computation.
I0711 21:56:26.365051 13090 net.cpp:224] ctx_conv3/bn needs backward computation.
I0711 21:56:26.365054 13090 net.cpp:224] ctx_conv3 needs backward computation.
I0711 21:56:26.365056 13090 net.cpp:224] ctx_conv2/relu needs backward computation.
I0711 21:56:26.365059 13090 net.cpp:224] ctx_conv2/bn needs backward computation.
I0711 21:56:26.365061 13090 net.cpp:224] ctx_conv2 needs backward computation.
I0711 21:56:26.365063 13090 net.cpp:224] ctx_conv1/relu needs backward computation.
I0711 21:56:26.365067 13090 net.cpp:224] ctx_conv1/bn needs backward computation.
I0711 21:56:26.365068 13090 net.cpp:224] ctx_conv1 needs backward computation.
I0711 21:56:26.365072 13090 net.cpp:224] out3_out5_combined needs backward computation.
I0711 21:56:26.365073 13090 net.cpp:224] out3a/relu needs backward computation.
I0711 21:56:26.365077 13090 net.cpp:224] out3a/bn needs backward computation.
I0711 21:56:26.365079 13090 net.cpp:224] out3a needs backward computation.
I0711 21:56:26.365082 13090 net.cpp:224] out5a_up2 needs backward computation.
I0711 21:56:26.365084 13090 net.cpp:224] out5a/relu needs backward computation.
I0711 21:56:26.365087 13090 net.cpp:224] out5a/bn needs backward computation.
I0711 21:56:26.365088 13090 net.cpp:224] out5a needs backward computation.
I0711 21:56:26.365092 13090 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0711 21:56:26.365093 13090 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0711 21:56:26.365095 13090 net.cpp:224] res5a_branch2b needs backward computation.
I0711 21:56:26.365098 13090 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0711 21:56:26.365109 13090 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0711 21:56:26.365113 13090 net.cpp:224] res5a_branch2a needs backward computation.
I0711 21:56:26.365118 13090 net.cpp:224] pool4 needs backward computation.
I0711 21:56:26.365121 13090 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0711 21:56:26.365124 13090 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0711 21:56:26.365128 13090 net.cpp:224] res4a_branch2b needs backward computation.
I0711 21:56:26.365131 13090 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0711 21:56:26.365135 13090 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0711 21:56:26.365139 13090 net.cpp:224] res4a_branch2a needs backward computation.
I0711 21:56:26.365142 13090 net.cpp:224] pool3 needs backward computation.
I0711 21:56:26.365146 13090 net.cpp:224] res3a_branch2b/bn_res3a_branch2b/relu_0_split needs backward computation.
I0711 21:56:26.365150 13090 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0711 21:56:26.365154 13090 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0711 21:56:26.365159 13090 net.cpp:224] res3a_branch2b needs backward computation.
I0711 21:56:26.365162 13090 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0711 21:56:26.365166 13090 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0711 21:56:26.365170 13090 net.cpp:224] res3a_branch2a needs backward computation.
I0711 21:56:26.365175 13090 net.cpp:224] pool2 needs backward computation.
I0711 21:56:26.365178 13090 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0711 21:56:26.365182 13090 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0711 21:56:26.365186 13090 net.cpp:224] res2a_branch2b needs backward computation.
I0711 21:56:26.365190 13090 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0711 21:56:26.365195 13090 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0711 21:56:26.365198 13090 net.cpp:224] res2a_branch2a needs backward computation.
I0711 21:56:26.365202 13090 net.cpp:224] pool1 needs backward computation.
I0711 21:56:26.365206 13090 net.cpp:224] conv1b/relu needs backward computation.
I0711 21:56:26.365211 13090 net.cpp:224] conv1b/bn needs backward computation.
I0711 21:56:26.365216 13090 net.cpp:224] conv1b needs backward computation.
I0711 21:56:26.365219 13090 net.cpp:224] conv1a/relu needs backward computation.
I0711 21:56:26.365222 13090 net.cpp:224] conv1a/bn needs backward computation.
I0711 21:56:26.365226 13090 net.cpp:224] conv1a needs backward computation.
I0711 21:56:26.365231 13090 net.cpp:226] data/bias does not need backward computation.
I0711 21:56:26.365236 13090 net.cpp:226] data does not need backward computation.
I0711 21:56:26.365239 13090 net.cpp:268] This network produces output loss
I0711 21:56:26.365283 13090 net.cpp:288] Network initialization done.
I0711 21:56:26.366222 13090 solver.cpp:182] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/sparse/test.prototxt
I0711 21:56:26.366515 13090 net.cpp:56] Initializing net from parameters: 
name: "jsegnet21v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/val-image-lmdb"
    label_list_path: "data/val-label-lmdb"
    batch_size: 4
    threads: 4
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b/bn"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a/bn"
  top: "out5a/bn"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a/bn"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b/bn"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a/bn"
  top: "out3a/bn"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a/bn"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv1/bn"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv2/bn"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv3/bn"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4/bn"
  top: "ctx_conv4/bn"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4/bn"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
  accuracy_param {
    ignore_label: 255
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
    ignore_label: 255
  }
}
I0711 21:56:26.366688 13090 layer_factory.hpp:77] Creating layer data
I0711 21:56:26.366696 13090 net.cpp:98] Creating Layer data
I0711 21:56:26.366700 13090 net.cpp:413] data -> data
I0711 21:56:26.366704 13090 net.cpp:413] data -> label
I0711 21:56:26.383443 13166 db_lmdb.cpp:35] Opened lmdb data/val-image-lmdb
I0711 21:56:26.386066 13090 data_layer.cpp:78] ReshapePrefetch 4, 3, 640, 640
I0711 21:56:26.386175 13090 data_layer.cpp:83] output data size: 4,3,640,640
I0711 21:56:26.396155 13171 db_lmdb.cpp:35] Opened lmdb data/val-label-lmdb
I0711 21:56:26.413851 13090 data_layer.cpp:78] ReshapePrefetch 4, 1, 640, 640
I0711 21:56:26.413929 13090 data_layer.cpp:83] output data size: 4,1,640,640
I0711 21:56:26.427109 13090 net.cpp:148] Setting up data
I0711 21:56:26.427201 13090 net.cpp:155] Top shape: 4 3 640 640 (4915200)
I0711 21:56:26.427220 13090 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0711 21:56:26.427242 13090 net.cpp:163] Memory required for data: 26214400
I0711 21:56:26.427266 13090 layer_factory.hpp:77] Creating layer label_data_1_split
I0711 21:56:26.427304 13090 net.cpp:98] Creating Layer label_data_1_split
I0711 21:56:26.427314 13090 net.cpp:439] label_data_1_split <- label
I0711 21:56:26.427327 13090 net.cpp:413] label_data_1_split -> label_data_1_split_0
I0711 21:56:26.427340 13090 net.cpp:413] label_data_1_split -> label_data_1_split_1
I0711 21:56:26.427350 13090 net.cpp:413] label_data_1_split -> label_data_1_split_2
I0711 21:56:26.427520 13090 net.cpp:148] Setting up label_data_1_split
I0711 21:56:26.427532 13090 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0711 21:56:26.427541 13090 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0711 21:56:26.427549 13090 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0711 21:56:26.427556 13090 net.cpp:163] Memory required for data: 45875200
I0711 21:56:26.427563 13090 layer_factory.hpp:77] Creating layer data/bias
I0711 21:56:26.427575 13090 net.cpp:98] Creating Layer data/bias
I0711 21:56:26.427583 13090 net.cpp:439] data/bias <- data
I0711 21:56:26.427592 13090 net.cpp:413] data/bias -> data/bias
I0711 21:56:26.429461 13090 net.cpp:148] Setting up data/bias
I0711 21:56:26.429524 13090 net.cpp:155] Top shape: 4 3 640 640 (4915200)
I0711 21:56:26.429538 13090 net.cpp:163] Memory required for data: 65536000
I0711 21:56:26.429558 13090 layer_factory.hpp:77] Creating layer conv1a
I0711 21:56:26.429595 13090 net.cpp:98] Creating Layer conv1a
I0711 21:56:26.429607 13090 net.cpp:439] conv1a <- data/bias
I0711 21:56:26.429618 13090 net.cpp:413] conv1a -> conv1a
I0711 21:56:26.430541 13090 net.cpp:148] Setting up conv1a
I0711 21:56:26.430554 13090 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0711 21:56:26.430557 13090 net.cpp:163] Memory required for data: 117964800
I0711 21:56:26.430565 13090 layer_factory.hpp:77] Creating layer conv1a/bn
I0711 21:56:26.430580 13090 net.cpp:98] Creating Layer conv1a/bn
I0711 21:56:26.430595 13090 net.cpp:439] conv1a/bn <- conv1a
I0711 21:56:26.430608 13090 net.cpp:413] conv1a/bn -> conv1a/bn
I0711 21:56:26.434708 13090 net.cpp:148] Setting up conv1a/bn
I0711 21:56:26.434737 13090 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0711 21:56:26.434741 13090 net.cpp:163] Memory required for data: 170393600
I0711 21:56:26.434763 13090 layer_factory.hpp:77] Creating layer conv1a/relu
I0711 21:56:26.434775 13090 net.cpp:98] Creating Layer conv1a/relu
I0711 21:56:26.434782 13090 net.cpp:439] conv1a/relu <- conv1a/bn
I0711 21:56:26.434795 13090 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0711 21:56:26.434828 13090 net.cpp:148] Setting up conv1a/relu
I0711 21:56:26.434835 13090 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0711 21:56:26.434840 13090 net.cpp:163] Memory required for data: 222822400
I0711 21:56:26.434844 13090 layer_factory.hpp:77] Creating layer conv1b
I0711 21:56:26.434857 13090 net.cpp:98] Creating Layer conv1b
I0711 21:56:26.434861 13090 net.cpp:439] conv1b <- conv1a/bn
I0711 21:56:26.434867 13090 net.cpp:413] conv1b -> conv1b
I0711 21:56:26.435618 13090 net.cpp:148] Setting up conv1b
I0711 21:56:26.435628 13090 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0711 21:56:26.435631 13090 net.cpp:163] Memory required for data: 275251200
I0711 21:56:26.435638 13090 layer_factory.hpp:77] Creating layer conv1b/bn
I0711 21:56:26.435647 13090 net.cpp:98] Creating Layer conv1b/bn
I0711 21:56:26.435649 13090 net.cpp:439] conv1b/bn <- conv1b
I0711 21:56:26.435654 13090 net.cpp:413] conv1b/bn -> conv1b/bn
I0711 21:56:26.436786 13090 net.cpp:148] Setting up conv1b/bn
I0711 21:56:26.436795 13090 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0711 21:56:26.436797 13090 net.cpp:163] Memory required for data: 327680000
I0711 21:56:26.436803 13090 layer_factory.hpp:77] Creating layer conv1b/relu
I0711 21:56:26.436812 13090 net.cpp:98] Creating Layer conv1b/relu
I0711 21:56:26.436836 13090 net.cpp:439] conv1b/relu <- conv1b/bn
I0711 21:56:26.436841 13090 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0711 21:56:26.436847 13090 net.cpp:148] Setting up conv1b/relu
I0711 21:56:26.436852 13090 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0711 21:56:26.436877 13090 net.cpp:163] Memory required for data: 380108800
I0711 21:56:26.436882 13090 layer_factory.hpp:77] Creating layer pool1
I0711 21:56:26.436893 13090 net.cpp:98] Creating Layer pool1
I0711 21:56:26.436897 13090 net.cpp:439] pool1 <- conv1b/bn
I0711 21:56:26.436902 13090 net.cpp:413] pool1 -> pool1
I0711 21:56:26.436957 13090 net.cpp:148] Setting up pool1
I0711 21:56:26.436962 13090 net.cpp:155] Top shape: 4 32 160 160 (3276800)
I0711 21:56:26.436966 13090 net.cpp:163] Memory required for data: 393216000
I0711 21:56:26.436970 13090 layer_factory.hpp:77] Creating layer res2a_branch2a
I0711 21:56:26.436980 13090 net.cpp:98] Creating Layer res2a_branch2a
I0711 21:56:26.436983 13090 net.cpp:439] res2a_branch2a <- pool1
I0711 21:56:26.436990 13090 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0711 21:56:26.440515 13090 net.cpp:148] Setting up res2a_branch2a
I0711 21:56:26.440567 13090 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0711 21:56:26.440589 13090 net.cpp:163] Memory required for data: 419430400
I0711 21:56:26.440631 13090 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0711 21:56:26.440670 13090 net.cpp:98] Creating Layer res2a_branch2a/bn
I0711 21:56:26.440699 13090 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0711 21:56:26.440732 13090 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0711 21:56:26.443466 13090 net.cpp:148] Setting up res2a_branch2a/bn
I0711 21:56:26.443506 13090 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0711 21:56:26.443521 13090 net.cpp:163] Memory required for data: 445644800
I0711 21:56:26.443547 13090 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0711 21:56:26.443567 13090 net.cpp:98] Creating Layer res2a_branch2a/relu
I0711 21:56:26.443583 13090 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0711 21:56:26.443600 13090 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0711 21:56:26.443620 13090 net.cpp:148] Setting up res2a_branch2a/relu
I0711 21:56:26.443637 13090 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0711 21:56:26.443650 13090 net.cpp:163] Memory required for data: 471859200
I0711 21:56:26.443666 13090 layer_factory.hpp:77] Creating layer res2a_branch2b
I0711 21:56:26.443692 13090 net.cpp:98] Creating Layer res2a_branch2b
I0711 21:56:26.443706 13090 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0711 21:56:26.443722 13090 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0711 21:56:26.444669 13090 net.cpp:148] Setting up res2a_branch2b
I0711 21:56:26.444689 13090 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0711 21:56:26.444699 13090 net.cpp:163] Memory required for data: 498073600
I0711 21:56:26.444710 13090 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0711 21:56:26.444722 13090 net.cpp:98] Creating Layer res2a_branch2b/bn
I0711 21:56:26.444731 13090 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0711 21:56:26.444741 13090 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0711 21:56:26.446015 13090 net.cpp:148] Setting up res2a_branch2b/bn
I0711 21:56:26.446082 13090 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0711 21:56:26.446094 13090 net.cpp:163] Memory required for data: 524288000
I0711 21:56:26.446110 13090 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0711 21:56:26.446125 13090 net.cpp:98] Creating Layer res2a_branch2b/relu
I0711 21:56:26.446135 13090 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0711 21:56:26.446146 13090 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0711 21:56:26.446159 13090 net.cpp:148] Setting up res2a_branch2b/relu
I0711 21:56:26.446168 13090 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0711 21:56:26.446177 13090 net.cpp:163] Memory required for data: 550502400
I0711 21:56:26.446184 13090 layer_factory.hpp:77] Creating layer pool2
I0711 21:56:26.446197 13090 net.cpp:98] Creating Layer pool2
I0711 21:56:26.446205 13090 net.cpp:439] pool2 <- res2a_branch2b/bn
I0711 21:56:26.446214 13090 net.cpp:413] pool2 -> pool2
I0711 21:56:26.446313 13090 net.cpp:148] Setting up pool2
I0711 21:56:26.446352 13090 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 21:56:26.446359 13090 net.cpp:163] Memory required for data: 557056000
I0711 21:56:26.446367 13090 layer_factory.hpp:77] Creating layer res3a_branch2a
I0711 21:56:26.446386 13090 net.cpp:98] Creating Layer res3a_branch2a
I0711 21:56:26.446395 13090 net.cpp:439] res3a_branch2a <- pool2
I0711 21:56:26.446405 13090 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0711 21:56:26.448885 13090 net.cpp:148] Setting up res3a_branch2a
I0711 21:56:26.448900 13090 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 21:56:26.448904 13090 net.cpp:163] Memory required for data: 570163200
I0711 21:56:26.448909 13090 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0711 21:56:26.448920 13090 net.cpp:98] Creating Layer res3a_branch2a/bn
I0711 21:56:26.448923 13090 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0711 21:56:26.448931 13090 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0711 21:56:26.449734 13090 net.cpp:148] Setting up res3a_branch2a/bn
I0711 21:56:26.449743 13090 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 21:56:26.449745 13090 net.cpp:163] Memory required for data: 583270400
I0711 21:56:26.449759 13090 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0711 21:56:26.449765 13090 net.cpp:98] Creating Layer res3a_branch2a/relu
I0711 21:56:26.449769 13090 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0711 21:56:26.449779 13090 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0711 21:56:26.449784 13090 net.cpp:148] Setting up res3a_branch2a/relu
I0711 21:56:26.449790 13090 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 21:56:26.449792 13090 net.cpp:163] Memory required for data: 596377600
I0711 21:56:26.449796 13090 layer_factory.hpp:77] Creating layer res3a_branch2b
I0711 21:56:26.449805 13090 net.cpp:98] Creating Layer res3a_branch2b
I0711 21:56:26.449808 13090 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0711 21:56:26.449813 13090 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0711 21:56:26.450959 13090 net.cpp:148] Setting up res3a_branch2b
I0711 21:56:26.450968 13090 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 21:56:26.450971 13090 net.cpp:163] Memory required for data: 609484800
I0711 21:56:26.450978 13090 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0711 21:56:26.450995 13090 net.cpp:98] Creating Layer res3a_branch2b/bn
I0711 21:56:26.451001 13090 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0711 21:56:26.451012 13090 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0711 21:56:26.454614 13090 net.cpp:148] Setting up res3a_branch2b/bn
I0711 21:56:26.454668 13090 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 21:56:26.454689 13090 net.cpp:163] Memory required for data: 622592000
I0711 21:56:26.454715 13090 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0711 21:56:26.454747 13090 net.cpp:98] Creating Layer res3a_branch2b/relu
I0711 21:56:26.454769 13090 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0711 21:56:26.454789 13090 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0711 21:56:26.454813 13090 net.cpp:148] Setting up res3a_branch2b/relu
I0711 21:56:26.454834 13090 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 21:56:26.454851 13090 net.cpp:163] Memory required for data: 635699200
I0711 21:56:26.454869 13090 layer_factory.hpp:77] Creating layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0711 21:56:26.454895 13090 net.cpp:98] Creating Layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0711 21:56:26.454915 13090 net.cpp:439] res3a_branch2b/bn_res3a_branch2b/relu_0_split <- res3a_branch2b/bn
I0711 21:56:26.454936 13090 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0711 21:56:26.454957 13090 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0711 21:56:26.455052 13090 net.cpp:148] Setting up res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0711 21:56:26.455081 13090 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 21:56:26.455121 13090 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 21:56:26.455139 13090 net.cpp:163] Memory required for data: 661913600
I0711 21:56:26.455154 13090 layer_factory.hpp:77] Creating layer pool3
I0711 21:56:26.455175 13090 net.cpp:98] Creating Layer pool3
I0711 21:56:26.455193 13090 net.cpp:439] pool3 <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0711 21:56:26.455209 13090 net.cpp:413] pool3 -> pool3
I0711 21:56:26.455313 13090 net.cpp:148] Setting up pool3
I0711 21:56:26.455332 13090 net.cpp:155] Top shape: 4 128 40 40 (819200)
I0711 21:56:26.455350 13090 net.cpp:163] Memory required for data: 665190400
I0711 21:56:26.455368 13090 layer_factory.hpp:77] Creating layer res4a_branch2a
I0711 21:56:26.455394 13090 net.cpp:98] Creating Layer res4a_branch2a
I0711 21:56:26.455411 13090 net.cpp:439] res4a_branch2a <- pool3
I0711 21:56:26.455436 13090 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0711 21:56:26.465858 13090 net.cpp:148] Setting up res4a_branch2a
I0711 21:56:26.465960 13090 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0711 21:56:26.465978 13090 net.cpp:163] Memory required for data: 671744000
I0711 21:56:26.465997 13090 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0711 21:56:26.466019 13090 net.cpp:98] Creating Layer res4a_branch2a/bn
I0711 21:56:26.466034 13090 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0711 21:56:26.466054 13090 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0711 21:56:26.467051 13090 net.cpp:148] Setting up res4a_branch2a/bn
I0711 21:56:26.467061 13090 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0711 21:56:26.467063 13090 net.cpp:163] Memory required for data: 678297600
I0711 21:56:26.467072 13090 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0711 21:56:26.467077 13090 net.cpp:98] Creating Layer res4a_branch2a/relu
I0711 21:56:26.467079 13090 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0711 21:56:26.467083 13090 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0711 21:56:26.467090 13090 net.cpp:148] Setting up res4a_branch2a/relu
I0711 21:56:26.467094 13090 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0711 21:56:26.467097 13090 net.cpp:163] Memory required for data: 684851200
I0711 21:56:26.467102 13090 layer_factory.hpp:77] Creating layer res4a_branch2b
I0711 21:56:26.467109 13090 net.cpp:98] Creating Layer res4a_branch2b
I0711 21:56:26.467113 13090 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0711 21:56:26.467116 13090 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0711 21:56:26.470491 13090 net.cpp:148] Setting up res4a_branch2b
I0711 21:56:26.470499 13090 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0711 21:56:26.470500 13090 net.cpp:163] Memory required for data: 691404800
I0711 21:56:26.470504 13090 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0711 21:56:26.470507 13090 net.cpp:98] Creating Layer res4a_branch2b/bn
I0711 21:56:26.470510 13090 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0711 21:56:26.470512 13090 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0711 21:56:26.471202 13090 net.cpp:148] Setting up res4a_branch2b/bn
I0711 21:56:26.471209 13090 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0711 21:56:26.471210 13090 net.cpp:163] Memory required for data: 697958400
I0711 21:56:26.471215 13090 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0711 21:56:26.471217 13090 net.cpp:98] Creating Layer res4a_branch2b/relu
I0711 21:56:26.471220 13090 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0711 21:56:26.471222 13090 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0711 21:56:26.471225 13090 net.cpp:148] Setting up res4a_branch2b/relu
I0711 21:56:26.471227 13090 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0711 21:56:26.471230 13090 net.cpp:163] Memory required for data: 704512000
I0711 21:56:26.471231 13090 layer_factory.hpp:77] Creating layer pool4
I0711 21:56:26.471235 13090 net.cpp:98] Creating Layer pool4
I0711 21:56:26.471237 13090 net.cpp:439] pool4 <- res4a_branch2b/bn
I0711 21:56:26.471251 13090 net.cpp:413] pool4 -> pool4
I0711 21:56:26.471293 13090 net.cpp:148] Setting up pool4
I0711 21:56:26.471297 13090 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0711 21:56:26.471299 13090 net.cpp:163] Memory required for data: 711065600
I0711 21:56:26.471302 13090 layer_factory.hpp:77] Creating layer res5a_branch2a
I0711 21:56:26.471307 13090 net.cpp:98] Creating Layer res5a_branch2a
I0711 21:56:26.471310 13090 net.cpp:439] res5a_branch2a <- pool4
I0711 21:56:26.471313 13090 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0711 21:56:26.496119 13090 net.cpp:148] Setting up res5a_branch2a
I0711 21:56:26.496131 13090 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0711 21:56:26.496134 13090 net.cpp:163] Memory required for data: 724172800
I0711 21:56:26.496139 13090 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0711 21:56:26.496146 13090 net.cpp:98] Creating Layer res5a_branch2a/bn
I0711 21:56:26.496150 13090 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0711 21:56:26.496152 13090 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0711 21:56:26.496858 13090 net.cpp:148] Setting up res5a_branch2a/bn
I0711 21:56:26.496865 13090 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0711 21:56:26.496867 13090 net.cpp:163] Memory required for data: 737280000
I0711 21:56:26.496872 13090 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0711 21:56:26.496876 13090 net.cpp:98] Creating Layer res5a_branch2a/relu
I0711 21:56:26.496877 13090 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0711 21:56:26.496881 13090 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0711 21:56:26.496884 13090 net.cpp:148] Setting up res5a_branch2a/relu
I0711 21:56:26.496886 13090 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0711 21:56:26.496888 13090 net.cpp:163] Memory required for data: 750387200
I0711 21:56:26.496891 13090 layer_factory.hpp:77] Creating layer res5a_branch2b
I0711 21:56:26.496894 13090 net.cpp:98] Creating Layer res5a_branch2b
I0711 21:56:26.496896 13090 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0711 21:56:26.496899 13090 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0711 21:56:26.509726 13090 net.cpp:148] Setting up res5a_branch2b
I0711 21:56:26.509734 13090 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0711 21:56:26.509737 13090 net.cpp:163] Memory required for data: 763494400
I0711 21:56:26.509743 13090 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0711 21:56:26.509747 13090 net.cpp:98] Creating Layer res5a_branch2b/bn
I0711 21:56:26.509749 13090 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0711 21:56:26.509752 13090 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0711 21:56:26.510453 13090 net.cpp:148] Setting up res5a_branch2b/bn
I0711 21:56:26.510459 13090 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0711 21:56:26.510462 13090 net.cpp:163] Memory required for data: 776601600
I0711 21:56:26.510466 13090 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0711 21:56:26.510469 13090 net.cpp:98] Creating Layer res5a_branch2b/relu
I0711 21:56:26.510471 13090 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0711 21:56:26.510473 13090 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0711 21:56:26.510478 13090 net.cpp:148] Setting up res5a_branch2b/relu
I0711 21:56:26.510479 13090 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0711 21:56:26.510481 13090 net.cpp:163] Memory required for data: 789708800
I0711 21:56:26.510483 13090 layer_factory.hpp:77] Creating layer out5a
I0711 21:56:26.510488 13090 net.cpp:98] Creating Layer out5a
I0711 21:56:26.510489 13090 net.cpp:439] out5a <- res5a_branch2b/bn
I0711 21:56:26.510491 13090 net.cpp:413] out5a -> out5a
I0711 21:56:26.514597 13090 net.cpp:148] Setting up out5a
I0711 21:56:26.514606 13090 net.cpp:155] Top shape: 4 64 40 40 (409600)
I0711 21:56:26.514608 13090 net.cpp:163] Memory required for data: 791347200
I0711 21:56:26.514612 13090 layer_factory.hpp:77] Creating layer out5a/bn
I0711 21:56:26.514617 13090 net.cpp:98] Creating Layer out5a/bn
I0711 21:56:26.514621 13090 net.cpp:439] out5a/bn <- out5a
I0711 21:56:26.514632 13090 net.cpp:413] out5a/bn -> out5a/bn
I0711 21:56:26.515403 13090 net.cpp:148] Setting up out5a/bn
I0711 21:56:26.515408 13090 net.cpp:155] Top shape: 4 64 40 40 (409600)
I0711 21:56:26.515410 13090 net.cpp:163] Memory required for data: 792985600
I0711 21:56:26.515415 13090 layer_factory.hpp:77] Creating layer out5a/relu
I0711 21:56:26.515419 13090 net.cpp:98] Creating Layer out5a/relu
I0711 21:56:26.515420 13090 net.cpp:439] out5a/relu <- out5a/bn
I0711 21:56:26.515424 13090 net.cpp:400] out5a/relu -> out5a/bn (in-place)
I0711 21:56:26.515426 13090 net.cpp:148] Setting up out5a/relu
I0711 21:56:26.515429 13090 net.cpp:155] Top shape: 4 64 40 40 (409600)
I0711 21:56:26.515430 13090 net.cpp:163] Memory required for data: 794624000
I0711 21:56:26.515432 13090 layer_factory.hpp:77] Creating layer out5a_up2
I0711 21:56:26.515436 13090 net.cpp:98] Creating Layer out5a_up2
I0711 21:56:26.515439 13090 net.cpp:439] out5a_up2 <- out5a/bn
I0711 21:56:26.515440 13090 net.cpp:413] out5a_up2 -> out5a_up2
I0711 21:56:26.515705 13090 net.cpp:148] Setting up out5a_up2
I0711 21:56:26.515710 13090 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 21:56:26.515712 13090 net.cpp:163] Memory required for data: 801177600
I0711 21:56:26.515715 13090 layer_factory.hpp:77] Creating layer out3a
I0711 21:56:26.515719 13090 net.cpp:98] Creating Layer out3a
I0711 21:56:26.515722 13090 net.cpp:439] out3a <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0711 21:56:26.515724 13090 net.cpp:413] out3a -> out3a
I0711 21:56:26.517662 13090 net.cpp:148] Setting up out3a
I0711 21:56:26.517669 13090 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 21:56:26.517673 13090 net.cpp:163] Memory required for data: 807731200
I0711 21:56:26.517676 13090 layer_factory.hpp:77] Creating layer out3a/bn
I0711 21:56:26.517679 13090 net.cpp:98] Creating Layer out3a/bn
I0711 21:56:26.517683 13090 net.cpp:439] out3a/bn <- out3a
I0711 21:56:26.517685 13090 net.cpp:413] out3a/bn -> out3a/bn
I0711 21:56:26.518471 13090 net.cpp:148] Setting up out3a/bn
I0711 21:56:26.518476 13090 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 21:56:26.518478 13090 net.cpp:163] Memory required for data: 814284800
I0711 21:56:26.518483 13090 layer_factory.hpp:77] Creating layer out3a/relu
I0711 21:56:26.518486 13090 net.cpp:98] Creating Layer out3a/relu
I0711 21:56:26.518488 13090 net.cpp:439] out3a/relu <- out3a/bn
I0711 21:56:26.518491 13090 net.cpp:400] out3a/relu -> out3a/bn (in-place)
I0711 21:56:26.518494 13090 net.cpp:148] Setting up out3a/relu
I0711 21:56:26.518496 13090 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 21:56:26.518498 13090 net.cpp:163] Memory required for data: 820838400
I0711 21:56:26.518501 13090 layer_factory.hpp:77] Creating layer out3_out5_combined
I0711 21:56:26.518502 13090 net.cpp:98] Creating Layer out3_out5_combined
I0711 21:56:26.518504 13090 net.cpp:439] out3_out5_combined <- out5a_up2
I0711 21:56:26.518507 13090 net.cpp:439] out3_out5_combined <- out3a/bn
I0711 21:56:26.518509 13090 net.cpp:413] out3_out5_combined -> out3_out5_combined
I0711 21:56:26.518532 13090 net.cpp:148] Setting up out3_out5_combined
I0711 21:56:26.518537 13090 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 21:56:26.518537 13090 net.cpp:163] Memory required for data: 827392000
I0711 21:56:26.518539 13090 layer_factory.hpp:77] Creating layer ctx_conv1
I0711 21:56:26.518543 13090 net.cpp:98] Creating Layer ctx_conv1
I0711 21:56:26.518545 13090 net.cpp:439] ctx_conv1 <- out3_out5_combined
I0711 21:56:26.518548 13090 net.cpp:413] ctx_conv1 -> ctx_conv1
I0711 21:56:26.519593 13090 net.cpp:148] Setting up ctx_conv1
I0711 21:56:26.519598 13090 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 21:56:26.519600 13090 net.cpp:163] Memory required for data: 833945600
I0711 21:56:26.519604 13090 layer_factory.hpp:77] Creating layer ctx_conv1/bn
I0711 21:56:26.519608 13090 net.cpp:98] Creating Layer ctx_conv1/bn
I0711 21:56:26.519609 13090 net.cpp:439] ctx_conv1/bn <- ctx_conv1
I0711 21:56:26.519611 13090 net.cpp:413] ctx_conv1/bn -> ctx_conv1/bn
I0711 21:56:26.520397 13090 net.cpp:148] Setting up ctx_conv1/bn
I0711 21:56:26.520403 13090 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 21:56:26.520406 13090 net.cpp:163] Memory required for data: 840499200
I0711 21:56:26.520411 13090 layer_factory.hpp:77] Creating layer ctx_conv1/relu
I0711 21:56:26.520413 13090 net.cpp:98] Creating Layer ctx_conv1/relu
I0711 21:56:26.520416 13090 net.cpp:439] ctx_conv1/relu <- ctx_conv1/bn
I0711 21:56:26.520417 13090 net.cpp:400] ctx_conv1/relu -> ctx_conv1/bn (in-place)
I0711 21:56:26.520421 13090 net.cpp:148] Setting up ctx_conv1/relu
I0711 21:56:26.520423 13090 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 21:56:26.520426 13090 net.cpp:163] Memory required for data: 847052800
I0711 21:56:26.520426 13090 layer_factory.hpp:77] Creating layer ctx_conv2
I0711 21:56:26.520432 13090 net.cpp:98] Creating Layer ctx_conv2
I0711 21:56:26.520434 13090 net.cpp:439] ctx_conv2 <- ctx_conv1/bn
I0711 21:56:26.520437 13090 net.cpp:413] ctx_conv2 -> ctx_conv2
I0711 21:56:26.521483 13090 net.cpp:148] Setting up ctx_conv2
I0711 21:56:26.521489 13090 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 21:56:26.521492 13090 net.cpp:163] Memory required for data: 853606400
I0711 21:56:26.521494 13090 layer_factory.hpp:77] Creating layer ctx_conv2/bn
I0711 21:56:26.521497 13090 net.cpp:98] Creating Layer ctx_conv2/bn
I0711 21:56:26.521499 13090 net.cpp:439] ctx_conv2/bn <- ctx_conv2
I0711 21:56:26.521502 13090 net.cpp:413] ctx_conv2/bn -> ctx_conv2/bn
I0711 21:56:26.522287 13090 net.cpp:148] Setting up ctx_conv2/bn
I0711 21:56:26.522292 13090 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 21:56:26.522294 13090 net.cpp:163] Memory required for data: 860160000
I0711 21:56:26.522299 13090 layer_factory.hpp:77] Creating layer ctx_conv2/relu
I0711 21:56:26.522301 13090 net.cpp:98] Creating Layer ctx_conv2/relu
I0711 21:56:26.522303 13090 net.cpp:439] ctx_conv2/relu <- ctx_conv2/bn
I0711 21:56:26.522305 13090 net.cpp:400] ctx_conv2/relu -> ctx_conv2/bn (in-place)
I0711 21:56:26.522310 13090 net.cpp:148] Setting up ctx_conv2/relu
I0711 21:56:26.522311 13090 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 21:56:26.522312 13090 net.cpp:163] Memory required for data: 866713600
I0711 21:56:26.522315 13090 layer_factory.hpp:77] Creating layer ctx_conv3
I0711 21:56:26.522318 13090 net.cpp:98] Creating Layer ctx_conv3
I0711 21:56:26.522320 13090 net.cpp:439] ctx_conv3 <- ctx_conv2/bn
I0711 21:56:26.522323 13090 net.cpp:413] ctx_conv3 -> ctx_conv3
I0711 21:56:26.523368 13090 net.cpp:148] Setting up ctx_conv3
I0711 21:56:26.523373 13090 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 21:56:26.523375 13090 net.cpp:163] Memory required for data: 873267200
I0711 21:56:26.523378 13090 layer_factory.hpp:77] Creating layer ctx_conv3/bn
I0711 21:56:26.523381 13090 net.cpp:98] Creating Layer ctx_conv3/bn
I0711 21:56:26.523383 13090 net.cpp:439] ctx_conv3/bn <- ctx_conv3
I0711 21:56:26.523386 13090 net.cpp:413] ctx_conv3/bn -> ctx_conv3/bn
I0711 21:56:26.524163 13090 net.cpp:148] Setting up ctx_conv3/bn
I0711 21:56:26.524168 13090 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 21:56:26.524169 13090 net.cpp:163] Memory required for data: 879820800
I0711 21:56:26.524174 13090 layer_factory.hpp:77] Creating layer ctx_conv3/relu
I0711 21:56:26.524178 13090 net.cpp:98] Creating Layer ctx_conv3/relu
I0711 21:56:26.524179 13090 net.cpp:439] ctx_conv3/relu <- ctx_conv3/bn
I0711 21:56:26.524183 13090 net.cpp:400] ctx_conv3/relu -> ctx_conv3/bn (in-place)
I0711 21:56:26.524185 13090 net.cpp:148] Setting up ctx_conv3/relu
I0711 21:56:26.524188 13090 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 21:56:26.524189 13090 net.cpp:163] Memory required for data: 886374400
I0711 21:56:26.524191 13090 layer_factory.hpp:77] Creating layer ctx_conv4
I0711 21:56:26.524194 13090 net.cpp:98] Creating Layer ctx_conv4
I0711 21:56:26.524196 13090 net.cpp:439] ctx_conv4 <- ctx_conv3/bn
I0711 21:56:26.524199 13090 net.cpp:413] ctx_conv4 -> ctx_conv4
I0711 21:56:26.525246 13090 net.cpp:148] Setting up ctx_conv4
I0711 21:56:26.525257 13090 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 21:56:26.525259 13090 net.cpp:163] Memory required for data: 892928000
I0711 21:56:26.525264 13090 layer_factory.hpp:77] Creating layer ctx_conv4/bn
I0711 21:56:26.525266 13090 net.cpp:98] Creating Layer ctx_conv4/bn
I0711 21:56:26.525269 13090 net.cpp:439] ctx_conv4/bn <- ctx_conv4
I0711 21:56:26.525271 13090 net.cpp:413] ctx_conv4/bn -> ctx_conv4/bn
I0711 21:56:26.526054 13090 net.cpp:148] Setting up ctx_conv4/bn
I0711 21:56:26.526059 13090 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 21:56:26.526062 13090 net.cpp:163] Memory required for data: 899481600
I0711 21:56:26.526067 13090 layer_factory.hpp:77] Creating layer ctx_conv4/relu
I0711 21:56:26.526069 13090 net.cpp:98] Creating Layer ctx_conv4/relu
I0711 21:56:26.526072 13090 net.cpp:439] ctx_conv4/relu <- ctx_conv4/bn
I0711 21:56:26.526073 13090 net.cpp:400] ctx_conv4/relu -> ctx_conv4/bn (in-place)
I0711 21:56:26.526077 13090 net.cpp:148] Setting up ctx_conv4/relu
I0711 21:56:26.526078 13090 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 21:56:26.526080 13090 net.cpp:163] Memory required for data: 906035200
I0711 21:56:26.526082 13090 layer_factory.hpp:77] Creating layer ctx_final
I0711 21:56:26.526085 13090 net.cpp:98] Creating Layer ctx_final
I0711 21:56:26.526087 13090 net.cpp:439] ctx_final <- ctx_conv4/bn
I0711 21:56:26.526089 13090 net.cpp:413] ctx_final -> ctx_final
I0711 21:56:26.526509 13090 net.cpp:148] Setting up ctx_final
I0711 21:56:26.526513 13090 net.cpp:155] Top shape: 4 8 80 80 (204800)
I0711 21:56:26.526515 13090 net.cpp:163] Memory required for data: 906854400
I0711 21:56:26.526518 13090 layer_factory.hpp:77] Creating layer ctx_final/relu
I0711 21:56:26.526521 13090 net.cpp:98] Creating Layer ctx_final/relu
I0711 21:56:26.526523 13090 net.cpp:439] ctx_final/relu <- ctx_final
I0711 21:56:26.526525 13090 net.cpp:400] ctx_final/relu -> ctx_final (in-place)
I0711 21:56:26.526528 13090 net.cpp:148] Setting up ctx_final/relu
I0711 21:56:26.526531 13090 net.cpp:155] Top shape: 4 8 80 80 (204800)
I0711 21:56:26.526532 13090 net.cpp:163] Memory required for data: 907673600
I0711 21:56:26.526535 13090 layer_factory.hpp:77] Creating layer out_deconv_final_up2
I0711 21:56:26.526537 13090 net.cpp:98] Creating Layer out_deconv_final_up2
I0711 21:56:26.526540 13090 net.cpp:439] out_deconv_final_up2 <- ctx_final
I0711 21:56:26.526541 13090 net.cpp:413] out_deconv_final_up2 -> out_deconv_final_up2
I0711 21:56:26.526792 13090 net.cpp:148] Setting up out_deconv_final_up2
I0711 21:56:26.526796 13090 net.cpp:155] Top shape: 4 8 160 160 (819200)
I0711 21:56:26.526798 13090 net.cpp:163] Memory required for data: 910950400
I0711 21:56:26.526800 13090 layer_factory.hpp:77] Creating layer out_deconv_final_up4
I0711 21:56:26.526803 13090 net.cpp:98] Creating Layer out_deconv_final_up4
I0711 21:56:26.526805 13090 net.cpp:439] out_deconv_final_up4 <- out_deconv_final_up2
I0711 21:56:26.526808 13090 net.cpp:413] out_deconv_final_up4 -> out_deconv_final_up4
I0711 21:56:26.527055 13090 net.cpp:148] Setting up out_deconv_final_up4
I0711 21:56:26.527060 13090 net.cpp:155] Top shape: 4 8 320 320 (3276800)
I0711 21:56:26.527061 13090 net.cpp:163] Memory required for data: 924057600
I0711 21:56:26.527063 13090 layer_factory.hpp:77] Creating layer out_deconv_final_up8
I0711 21:56:26.527066 13090 net.cpp:98] Creating Layer out_deconv_final_up8
I0711 21:56:26.527068 13090 net.cpp:439] out_deconv_final_up8 <- out_deconv_final_up4
I0711 21:56:26.527070 13090 net.cpp:413] out_deconv_final_up8 -> out_deconv_final_up8
I0711 21:56:26.527313 13090 net.cpp:148] Setting up out_deconv_final_up8
I0711 21:56:26.527318 13090 net.cpp:155] Top shape: 4 8 640 640 (13107200)
I0711 21:56:26.527319 13090 net.cpp:163] Memory required for data: 976486400
I0711 21:56:26.527321 13090 layer_factory.hpp:77] Creating layer out_deconv_final_up8_out_deconv_final_up8_0_split
I0711 21:56:26.527324 13090 net.cpp:98] Creating Layer out_deconv_final_up8_out_deconv_final_up8_0_split
I0711 21:56:26.527328 13090 net.cpp:439] out_deconv_final_up8_out_deconv_final_up8_0_split <- out_deconv_final_up8
I0711 21:56:26.527334 13090 net.cpp:413] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0711 21:56:26.527338 13090 net.cpp:413] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0711 21:56:26.527340 13090 net.cpp:413] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0711 21:56:26.527397 13090 net.cpp:148] Setting up out_deconv_final_up8_out_deconv_final_up8_0_split
I0711 21:56:26.527401 13090 net.cpp:155] Top shape: 4 8 640 640 (13107200)
I0711 21:56:26.527405 13090 net.cpp:155] Top shape: 4 8 640 640 (13107200)
I0711 21:56:26.527406 13090 net.cpp:155] Top shape: 4 8 640 640 (13107200)
I0711 21:56:26.527407 13090 net.cpp:163] Memory required for data: 1133772800
I0711 21:56:26.527410 13090 layer_factory.hpp:77] Creating layer loss
I0711 21:56:26.527413 13090 net.cpp:98] Creating Layer loss
I0711 21:56:26.527416 13090 net.cpp:439] loss <- out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0711 21:56:26.527418 13090 net.cpp:439] loss <- label_data_1_split_0
I0711 21:56:26.527421 13090 net.cpp:413] loss -> loss
I0711 21:56:26.527426 13090 layer_factory.hpp:77] Creating layer loss
I0711 21:56:26.544015 13090 net.cpp:148] Setting up loss
I0711 21:56:26.544039 13090 net.cpp:155] Top shape: (1)
I0711 21:56:26.544040 13090 net.cpp:158]     with loss weight 1
I0711 21:56:26.544049 13090 net.cpp:163] Memory required for data: 1133772804
I0711 21:56:26.544051 13090 layer_factory.hpp:77] Creating layer accuracy/top1
I0711 21:56:26.544059 13090 net.cpp:98] Creating Layer accuracy/top1
I0711 21:56:26.544062 13090 net.cpp:439] accuracy/top1 <- out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0711 21:56:26.544066 13090 net.cpp:439] accuracy/top1 <- label_data_1_split_1
I0711 21:56:26.544070 13090 net.cpp:413] accuracy/top1 -> accuracy/top1
I0711 21:56:26.544078 13090 net.cpp:148] Setting up accuracy/top1
I0711 21:56:26.544080 13090 net.cpp:155] Top shape: (1)
I0711 21:56:26.544082 13090 net.cpp:163] Memory required for data: 1133772808
I0711 21:56:26.544085 13090 layer_factory.hpp:77] Creating layer accuracy/top5
I0711 21:56:26.544087 13090 net.cpp:98] Creating Layer accuracy/top5
I0711 21:56:26.544090 13090 net.cpp:439] accuracy/top5 <- out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0711 21:56:26.544092 13090 net.cpp:439] accuracy/top5 <- label_data_1_split_2
I0711 21:56:26.544095 13090 net.cpp:413] accuracy/top5 -> accuracy/top5
I0711 21:56:26.544100 13090 net.cpp:148] Setting up accuracy/top5
I0711 21:56:26.544102 13090 net.cpp:155] Top shape: (1)
I0711 21:56:26.544104 13090 net.cpp:163] Memory required for data: 1133772812
I0711 21:56:26.544106 13090 net.cpp:226] accuracy/top5 does not need backward computation.
I0711 21:56:26.544108 13090 net.cpp:226] accuracy/top1 does not need backward computation.
I0711 21:56:26.544111 13090 net.cpp:224] loss needs backward computation.
I0711 21:56:26.544113 13090 net.cpp:224] out_deconv_final_up8_out_deconv_final_up8_0_split needs backward computation.
I0711 21:56:26.544116 13090 net.cpp:224] out_deconv_final_up8 needs backward computation.
I0711 21:56:26.544118 13090 net.cpp:224] out_deconv_final_up4 needs backward computation.
I0711 21:56:26.544121 13090 net.cpp:224] out_deconv_final_up2 needs backward computation.
I0711 21:56:26.544123 13090 net.cpp:224] ctx_final/relu needs backward computation.
I0711 21:56:26.544126 13090 net.cpp:224] ctx_final needs backward computation.
I0711 21:56:26.544128 13090 net.cpp:224] ctx_conv4/relu needs backward computation.
I0711 21:56:26.544131 13090 net.cpp:224] ctx_conv4/bn needs backward computation.
I0711 21:56:26.544133 13090 net.cpp:224] ctx_conv4 needs backward computation.
I0711 21:56:26.544136 13090 net.cpp:224] ctx_conv3/relu needs backward computation.
I0711 21:56:26.544137 13090 net.cpp:224] ctx_conv3/bn needs backward computation.
I0711 21:56:26.544139 13090 net.cpp:224] ctx_conv3 needs backward computation.
I0711 21:56:26.544152 13090 net.cpp:224] ctx_conv2/relu needs backward computation.
I0711 21:56:26.544155 13090 net.cpp:224] ctx_conv2/bn needs backward computation.
I0711 21:56:26.544157 13090 net.cpp:224] ctx_conv2 needs backward computation.
I0711 21:56:26.544158 13090 net.cpp:224] ctx_conv1/relu needs backward computation.
I0711 21:56:26.544162 13090 net.cpp:224] ctx_conv1/bn needs backward computation.
I0711 21:56:26.544163 13090 net.cpp:224] ctx_conv1 needs backward computation.
I0711 21:56:26.544167 13090 net.cpp:224] out3_out5_combined needs backward computation.
I0711 21:56:26.544170 13090 net.cpp:224] out3a/relu needs backward computation.
I0711 21:56:26.544173 13090 net.cpp:224] out3a/bn needs backward computation.
I0711 21:56:26.544175 13090 net.cpp:224] out3a needs backward computation.
I0711 21:56:26.544178 13090 net.cpp:224] out5a_up2 needs backward computation.
I0711 21:56:26.544181 13090 net.cpp:224] out5a/relu needs backward computation.
I0711 21:56:26.544184 13090 net.cpp:224] out5a/bn needs backward computation.
I0711 21:56:26.544186 13090 net.cpp:224] out5a needs backward computation.
I0711 21:56:26.544189 13090 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0711 21:56:26.544193 13090 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0711 21:56:26.544194 13090 net.cpp:224] res5a_branch2b needs backward computation.
I0711 21:56:26.544196 13090 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0711 21:56:26.544199 13090 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0711 21:56:26.544201 13090 net.cpp:224] res5a_branch2a needs backward computation.
I0711 21:56:26.544204 13090 net.cpp:224] pool4 needs backward computation.
I0711 21:56:26.544206 13090 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0711 21:56:26.544209 13090 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0711 21:56:26.544211 13090 net.cpp:224] res4a_branch2b needs backward computation.
I0711 21:56:26.544214 13090 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0711 21:56:26.544215 13090 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0711 21:56:26.544217 13090 net.cpp:224] res4a_branch2a needs backward computation.
I0711 21:56:26.544220 13090 net.cpp:224] pool3 needs backward computation.
I0711 21:56:26.544224 13090 net.cpp:224] res3a_branch2b/bn_res3a_branch2b/relu_0_split needs backward computation.
I0711 21:56:26.544225 13090 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0711 21:56:26.544227 13090 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0711 21:56:26.544230 13090 net.cpp:224] res3a_branch2b needs backward computation.
I0711 21:56:26.544232 13090 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0711 21:56:26.544234 13090 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0711 21:56:26.544237 13090 net.cpp:224] res3a_branch2a needs backward computation.
I0711 21:56:26.544239 13090 net.cpp:224] pool2 needs backward computation.
I0711 21:56:26.544241 13090 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0711 21:56:26.544245 13090 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0711 21:56:26.544246 13090 net.cpp:224] res2a_branch2b needs backward computation.
I0711 21:56:26.544250 13090 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0711 21:56:26.544251 13090 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0711 21:56:26.544253 13090 net.cpp:224] res2a_branch2a needs backward computation.
I0711 21:56:26.544255 13090 net.cpp:224] pool1 needs backward computation.
I0711 21:56:26.544257 13090 net.cpp:224] conv1b/relu needs backward computation.
I0711 21:56:26.544260 13090 net.cpp:224] conv1b/bn needs backward computation.
I0711 21:56:26.544263 13090 net.cpp:224] conv1b needs backward computation.
I0711 21:56:26.544265 13090 net.cpp:224] conv1a/relu needs backward computation.
I0711 21:56:26.544267 13090 net.cpp:224] conv1a/bn needs backward computation.
I0711 21:56:26.544270 13090 net.cpp:224] conv1a needs backward computation.
I0711 21:56:26.544275 13090 net.cpp:226] data/bias does not need backward computation.
I0711 21:56:26.544278 13090 net.cpp:226] label_data_1_split does not need backward computation.
I0711 21:56:26.544281 13090 net.cpp:226] data does not need backward computation.
I0711 21:56:26.544283 13090 net.cpp:268] This network produces output accuracy/top1
I0711 21:56:26.544286 13090 net.cpp:268] This network produces output accuracy/top5
I0711 21:56:26.544288 13090 net.cpp:268] This network produces output loss
I0711 21:56:26.544322 13090 net.cpp:288] Network initialization done.
I0711 21:56:26.544412 13090 solver.cpp:60] Solver scaffolding done.
I0711 21:56:26.552425 13090 caffe.cpp:145] Finetuning from training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/l1reg/cityscapes5_jsegnet21v2_iter_32000.caffemodel
I0711 21:56:26.584442 13090 data_layer.cpp:78] ReshapePrefetch 5, 3, 640, 640
I0711 21:56:26.584507 13090 data_layer.cpp:83] output data size: 5,3,640,640
I0711 21:56:26.624512 13090 data_layer.cpp:78] ReshapePrefetch 5, 1, 640, 640
I0711 21:56:26.624583 13090 data_layer.cpp:83] output data size: 5,1,640,640
I0711 21:56:27.137807 13090 data_layer.cpp:78] ReshapePrefetch 5, 3, 640, 640
I0711 21:56:27.138151 13090 data_layer.cpp:83] output data size: 5,3,640,640
I0711 21:56:27.216341 13090 data_layer.cpp:78] ReshapePrefetch 5, 1, 640, 640
I0711 21:56:27.218072 13090 data_layer.cpp:83] output data size: 5,1,640,640
I0711 21:56:29.592989 13090 parallel.cpp:334] Starting Optimization
I0711 21:56:29.593044 13090 net.cpp:1907] All zero weights of convolution layers are frozen
I0711 21:56:29.604212 13090 solver.cpp:409] Solving jsegnet21v2_train
I0711 21:56:29.604228 13090 solver.cpp:410] Learning Rate Policy: multistep
I0711 21:56:29.999721 13090 solver.cpp:290] Iteration 0 (0 iter/s, 0.395462s/100 iter), loss = 0.0155229
I0711 21:56:29.999743 13090 solver.cpp:309]     Train net output #0: loss = 0.0155229 (* 1 = 0.0155229 loss)
I0711 21:56:29.999752 13090 sgd_solver.cpp:106] Iteration 0, lr = 1e-05
I0711 21:56:30.048492 13090 solver.cpp:376] Finding and applying sparsity: 0.8
I0711 21:57:38.113512 13090 net.cpp:1907] All zero weights of convolution layers are frozen
I0711 21:57:45.196447 13090 blocking_queue.cpp:50] Data layer prefetch queue empty
I0711 21:58:14.992141 13090 solver.cpp:290] Iteration 100 (0.952476 iter/s, 104.99s/100 iter), loss = 0.096618
I0711 21:58:14.992214 13090 solver.cpp:309]     Train net output #0: loss = 0.0966179 (* 1 = 0.0966179 loss)
I0711 21:58:14.992223 13090 sgd_solver.cpp:106] Iteration 100, lr = 1e-05
I0711 21:58:40.420359 13227 blocking_queue.cpp:50] Waiting for data
I0711 21:59:00.279531 13090 solver.cpp:290] Iteration 200 (2.20818 iter/s, 45.2861s/100 iter), loss = 0.0521534
I0711 21:59:00.279633 13090 solver.cpp:309]     Train net output #0: loss = 0.0521534 (* 1 = 0.0521534 loss)
I0711 21:59:00.279644 13090 sgd_solver.cpp:106] Iteration 200, lr = 1e-05
I0711 21:59:24.184201 13090 solver.cpp:290] Iteration 300 (4.18342 iter/s, 23.9039s/100 iter), loss = 0.130307
I0711 21:59:24.184224 13090 solver.cpp:309]     Train net output #0: loss = 0.130307 (* 1 = 0.130307 loss)
I0711 21:59:24.184231 13090 sgd_solver.cpp:106] Iteration 300, lr = 1e-05
I0711 21:59:41.302057 13090 solver.cpp:290] Iteration 400 (5.84202 iter/s, 17.1174s/100 iter), loss = 0.0622129
I0711 21:59:41.302145 13090 solver.cpp:309]     Train net output #0: loss = 0.0622129 (* 1 = 0.0622129 loss)
I0711 21:59:41.302156 13090 sgd_solver.cpp:106] Iteration 400, lr = 1e-05
I0711 21:59:58.426431 13090 solver.cpp:290] Iteration 500 (5.83982 iter/s, 17.1238s/100 iter), loss = 0.0362475
I0711 21:59:58.426455 13090 solver.cpp:309]     Train net output #0: loss = 0.0362475 (* 1 = 0.0362475 loss)
I0711 21:59:58.426462 13090 sgd_solver.cpp:106] Iteration 500, lr = 1e-05
I0711 22:00:15.763095 13090 solver.cpp:290] Iteration 600 (5.76829 iter/s, 17.3362s/100 iter), loss = 0.0217607
I0711 22:00:15.763162 13090 solver.cpp:309]     Train net output #0: loss = 0.0217607 (* 1 = 0.0217607 loss)
I0711 22:00:15.763173 13090 sgd_solver.cpp:106] Iteration 600, lr = 1e-05
I0711 22:00:32.991034 13090 solver.cpp:290] Iteration 700 (5.80471 iter/s, 17.2274s/100 iter), loss = 0.0590531
I0711 22:00:32.991057 13090 solver.cpp:309]     Train net output #0: loss = 0.0590531 (* 1 = 0.0590531 loss)
I0711 22:00:32.991065 13090 sgd_solver.cpp:106] Iteration 700, lr = 1e-05
I0711 22:00:50.041280 13090 solver.cpp:290] Iteration 800 (5.86519 iter/s, 17.0498s/100 iter), loss = 0.030436
I0711 22:00:50.041357 13090 solver.cpp:309]     Train net output #0: loss = 0.0304361 (* 1 = 0.0304361 loss)
I0711 22:00:50.041364 13090 sgd_solver.cpp:106] Iteration 800, lr = 1e-05
I0711 22:01:07.099623 13090 solver.cpp:290] Iteration 900 (5.86242 iter/s, 17.0578s/100 iter), loss = 0.0380209
I0711 22:01:07.099647 13090 solver.cpp:309]     Train net output #0: loss = 0.038021 (* 1 = 0.038021 loss)
I0711 22:01:07.099653 13090 sgd_solver.cpp:106] Iteration 900, lr = 1e-05
I0711 22:01:24.015879 13090 solver.cpp:354] Sparsity after update:
I0711 22:01:24.071175 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 22:01:24.071190 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 22:01:24.071202 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 22:01:24.071205 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 22:01:24.071208 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 22:01:24.071208 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 22:01:24.071210 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 22:01:24.071213 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 22:01:24.071214 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 22:01:24.071216 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 22:01:24.071218 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 22:01:24.071220 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 22:01:24.071223 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 22:01:24.071224 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 22:01:24.071226 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 22:01:24.071228 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 22:01:24.071230 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 22:01:24.071234 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 22:01:24.071238 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 22:01:24.228116 13090 solver.cpp:290] Iteration 1000 (5.83839 iter/s, 17.128s/100 iter), loss = 0.0826203
I0711 22:01:24.228144 13090 solver.cpp:309]     Train net output #0: loss = 0.0826204 (* 1 = 0.0826204 loss)
I0711 22:01:24.228153 13090 sgd_solver.cpp:106] Iteration 1000, lr = 1e-05
I0711 22:01:41.458732 13090 solver.cpp:290] Iteration 1100 (5.80379 iter/s, 17.2301s/100 iter), loss = 0.0616464
I0711 22:01:41.458766 13090 solver.cpp:309]     Train net output #0: loss = 0.0616465 (* 1 = 0.0616465 loss)
I0711 22:01:41.458777 13090 sgd_solver.cpp:106] Iteration 1100, lr = 1e-05
I0711 22:01:58.489960 13090 solver.cpp:290] Iteration 1200 (5.87174 iter/s, 17.0307s/100 iter), loss = 0.0614037
I0711 22:01:58.490039 13090 solver.cpp:309]     Train net output #0: loss = 0.0614038 (* 1 = 0.0614038 loss)
I0711 22:01:58.490051 13090 sgd_solver.cpp:106] Iteration 1200, lr = 1e-05
I0711 22:02:15.693430 13090 solver.cpp:290] Iteration 1300 (5.81297 iter/s, 17.2029s/100 iter), loss = 0.045411
I0711 22:02:15.693454 13090 solver.cpp:309]     Train net output #0: loss = 0.0454111 (* 1 = 0.0454111 loss)
I0711 22:02:15.693460 13090 sgd_solver.cpp:106] Iteration 1300, lr = 1e-05
I0711 22:02:33.036651 13090 solver.cpp:290] Iteration 1400 (5.76611 iter/s, 17.3427s/100 iter), loss = 0.0956192
I0711 22:02:33.036762 13090 solver.cpp:309]     Train net output #0: loss = 0.0956194 (* 1 = 0.0956194 loss)
I0711 22:02:33.036774 13090 sgd_solver.cpp:106] Iteration 1400, lr = 1e-05
I0711 22:02:50.139547 13090 solver.cpp:290] Iteration 1500 (5.84716 iter/s, 17.1023s/100 iter), loss = 0.0223211
I0711 22:02:50.139593 13090 solver.cpp:309]     Train net output #0: loss = 0.0223212 (* 1 = 0.0223212 loss)
I0711 22:02:50.139607 13090 sgd_solver.cpp:106] Iteration 1500, lr = 1e-05
I0711 22:03:08.143749 13090 solver.cpp:290] Iteration 1600 (5.55442 iter/s, 18.0037s/100 iter), loss = 0.0694799
I0711 22:03:08.143843 13090 solver.cpp:309]     Train net output #0: loss = 0.06948 (* 1 = 0.06948 loss)
I0711 22:03:08.143851 13090 sgd_solver.cpp:106] Iteration 1600, lr = 1e-05
I0711 22:03:25.346989 13090 solver.cpp:290] Iteration 1700 (5.81305 iter/s, 17.2027s/100 iter), loss = 0.036184
I0711 22:03:25.347013 13090 solver.cpp:309]     Train net output #0: loss = 0.0361841 (* 1 = 0.0361841 loss)
I0711 22:03:25.347020 13090 sgd_solver.cpp:106] Iteration 1700, lr = 1e-05
I0711 22:03:42.439709 13090 solver.cpp:290] Iteration 1800 (5.85061 iter/s, 17.0922s/100 iter), loss = 0.0989872
I0711 22:03:42.439800 13090 solver.cpp:309]     Train net output #0: loss = 0.0989873 (* 1 = 0.0989873 loss)
I0711 22:03:42.439810 13090 sgd_solver.cpp:106] Iteration 1800, lr = 1e-05
I0711 22:03:59.657243 13090 solver.cpp:290] Iteration 1900 (5.80822 iter/s, 17.217s/100 iter), loss = 0.0762458
I0711 22:03:59.657270 13090 solver.cpp:309]     Train net output #0: loss = 0.076246 (* 1 = 0.076246 loss)
I0711 22:03:59.657279 13090 sgd_solver.cpp:106] Iteration 1900, lr = 1e-05
I0711 22:04:16.571547 13090 solver.cpp:354] Sparsity after update:
I0711 22:04:16.573709 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 22:04:16.573717 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 22:04:16.573724 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 22:04:16.573726 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 22:04:16.573729 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 22:04:16.573730 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 22:04:16.573732 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 22:04:16.573734 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 22:04:16.573736 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 22:04:16.573738 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 22:04:16.573740 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 22:04:16.573742 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 22:04:16.573745 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 22:04:16.573746 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 22:04:16.573748 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 22:04:16.573750 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 22:04:16.573752 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 22:04:16.573755 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 22:04:16.573757 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 22:04:16.573973 13090 solver.cpp:467] Iteration 2000, Testing net (#0)
I0711 22:05:04.685932 13090 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.944677
I0711 22:05:04.686033 13090 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999859
I0711 22:05:04.686040 13090 solver.cpp:540]     Test net output #2: loss = 0.136176 (* 1 = 0.136176 loss)
I0711 22:05:04.861604 13090 solver.cpp:290] Iteration 2000 (1.53368 iter/s, 65.2026s/100 iter), loss = 0.0226161
I0711 22:05:04.861627 13090 solver.cpp:309]     Train net output #0: loss = 0.0226163 (* 1 = 0.0226163 loss)
I0711 22:05:04.861634 13090 sgd_solver.cpp:106] Iteration 2000, lr = 1e-05
I0711 22:05:27.375010 13090 solver.cpp:290] Iteration 2100 (4.44193 iter/s, 22.5128s/100 iter), loss = 0.0300786
I0711 22:05:27.375036 13090 solver.cpp:309]     Train net output #0: loss = 0.0300788 (* 1 = 0.0300788 loss)
I0711 22:05:27.375042 13090 sgd_solver.cpp:106] Iteration 2100, lr = 1e-05
I0711 22:05:40.109591 13227 blocking_queue.cpp:50] Waiting for data
I0711 22:06:04.296638 13090 solver.cpp:290] Iteration 2200 (2.70852 iter/s, 36.9206s/100 iter), loss = 0.0509574
I0711 22:06:04.296663 13090 solver.cpp:309]     Train net output #0: loss = 0.0509576 (* 1 = 0.0509576 loss)
I0711 22:06:04.296671 13090 sgd_solver.cpp:106] Iteration 2200, lr = 1e-05
I0711 22:06:21.282992 13090 solver.cpp:290] Iteration 2300 (5.88725 iter/s, 16.9859s/100 iter), loss = 0.0609007
I0711 22:06:21.283089 13090 solver.cpp:309]     Train net output #0: loss = 0.0609009 (* 1 = 0.0609009 loss)
I0711 22:06:21.283100 13090 sgd_solver.cpp:106] Iteration 2300, lr = 1e-05
I0711 22:06:38.321895 13090 solver.cpp:290] Iteration 2400 (5.86912 iter/s, 17.0383s/100 iter), loss = 0.0347756
I0711 22:06:38.321921 13090 solver.cpp:309]     Train net output #0: loss = 0.0347758 (* 1 = 0.0347758 loss)
I0711 22:06:38.321930 13090 sgd_solver.cpp:106] Iteration 2400, lr = 1e-05
I0711 22:06:55.754662 13090 solver.cpp:290] Iteration 2500 (5.7365 iter/s, 17.4322s/100 iter), loss = 0.0267127
I0711 22:06:55.754865 13090 solver.cpp:309]     Train net output #0: loss = 0.0267129 (* 1 = 0.0267129 loss)
I0711 22:06:55.754904 13090 sgd_solver.cpp:106] Iteration 2500, lr = 1e-05
I0711 22:07:13.010782 13090 solver.cpp:290] Iteration 2600 (5.79526 iter/s, 17.2555s/100 iter), loss = 0.0383124
I0711 22:07:13.010807 13090 solver.cpp:309]     Train net output #0: loss = 0.0383126 (* 1 = 0.0383126 loss)
I0711 22:07:13.010813 13090 sgd_solver.cpp:106] Iteration 2600, lr = 1e-05
I0711 22:07:30.119976 13090 solver.cpp:290] Iteration 2700 (5.84498 iter/s, 17.1087s/100 iter), loss = 0.022857
I0711 22:07:30.120034 13090 solver.cpp:309]     Train net output #0: loss = 0.0228572 (* 1 = 0.0228572 loss)
I0711 22:07:30.120045 13090 sgd_solver.cpp:106] Iteration 2700, lr = 1e-05
I0711 22:07:47.061837 13090 solver.cpp:290] Iteration 2800 (5.90272 iter/s, 16.9413s/100 iter), loss = 0.0561066
I0711 22:07:47.061864 13090 solver.cpp:309]     Train net output #0: loss = 0.0561068 (* 1 = 0.0561068 loss)
I0711 22:07:47.061880 13090 sgd_solver.cpp:106] Iteration 2800, lr = 1e-05
I0711 22:08:04.014853 13090 solver.cpp:290] Iteration 2900 (5.89883 iter/s, 16.9525s/100 iter), loss = 0.0330383
I0711 22:08:04.014909 13090 solver.cpp:309]     Train net output #0: loss = 0.0330384 (* 1 = 0.0330384 loss)
I0711 22:08:04.014917 13090 sgd_solver.cpp:106] Iteration 2900, lr = 1e-05
I0711 22:08:20.660935 13090 solver.cpp:354] Sparsity after update:
I0711 22:08:20.716143 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 22:08:20.716162 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 22:08:20.716174 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 22:08:20.716178 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 22:08:20.716182 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 22:08:20.716187 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 22:08:20.716190 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 22:08:20.716194 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 22:08:20.716198 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 22:08:20.716202 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 22:08:20.716207 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 22:08:20.716212 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 22:08:20.716217 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 22:08:20.716222 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 22:08:20.716225 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 22:08:20.716229 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 22:08:20.716233 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 22:08:20.716243 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 22:08:20.716246 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 22:08:20.865573 13090 solver.cpp:290] Iteration 3000 (5.93465 iter/s, 16.8502s/100 iter), loss = 0.0514176
I0711 22:08:20.865602 13090 solver.cpp:309]     Train net output #0: loss = 0.0514177 (* 1 = 0.0514177 loss)
I0711 22:08:20.865609 13090 sgd_solver.cpp:106] Iteration 3000, lr = 1e-05
I0711 22:08:37.865110 13090 solver.cpp:290] Iteration 3100 (5.88268 iter/s, 16.999s/100 iter), loss = 0.0384769
I0711 22:08:37.865164 13090 solver.cpp:309]     Train net output #0: loss = 0.0384771 (* 1 = 0.0384771 loss)
I0711 22:08:37.865173 13090 sgd_solver.cpp:106] Iteration 3100, lr = 1e-05
I0711 22:08:54.854112 13090 solver.cpp:290] Iteration 3200 (5.88634 iter/s, 16.9885s/100 iter), loss = 0.0411953
I0711 22:08:54.854136 13090 solver.cpp:309]     Train net output #0: loss = 0.0411955 (* 1 = 0.0411955 loss)
I0711 22:08:54.854143 13090 sgd_solver.cpp:106] Iteration 3200, lr = 1e-05
I0711 22:09:11.738801 13090 solver.cpp:290] Iteration 3300 (5.9227 iter/s, 16.8842s/100 iter), loss = 0.0281996
I0711 22:09:11.738880 13090 solver.cpp:309]     Train net output #0: loss = 0.0281998 (* 1 = 0.0281998 loss)
I0711 22:09:11.738899 13090 sgd_solver.cpp:106] Iteration 3300, lr = 1e-05
I0711 22:09:28.680209 13090 solver.cpp:290] Iteration 3400 (5.90288 iter/s, 16.9409s/100 iter), loss = 0.0293484
I0711 22:09:28.680232 13090 solver.cpp:309]     Train net output #0: loss = 0.0293486 (* 1 = 0.0293486 loss)
I0711 22:09:28.680238 13090 sgd_solver.cpp:106] Iteration 3400, lr = 1e-05
I0711 22:09:45.742753 13090 solver.cpp:290] Iteration 3500 (5.86096 iter/s, 17.0621s/100 iter), loss = 0.0522294
I0711 22:09:45.742859 13090 solver.cpp:309]     Train net output #0: loss = 0.0522296 (* 1 = 0.0522296 loss)
I0711 22:09:45.742869 13090 sgd_solver.cpp:106] Iteration 3500, lr = 1e-05
I0711 22:10:02.793740 13090 solver.cpp:290] Iteration 3600 (5.86496 iter/s, 17.0504s/100 iter), loss = 0.0223704
I0711 22:10:02.793763 13090 solver.cpp:309]     Train net output #0: loss = 0.0223706 (* 1 = 0.0223706 loss)
I0711 22:10:02.793771 13090 sgd_solver.cpp:106] Iteration 3600, lr = 1e-05
I0711 22:10:19.745550 13090 solver.cpp:290] Iteration 3700 (5.89924 iter/s, 16.9513s/100 iter), loss = 0.0241995
I0711 22:10:19.745597 13090 solver.cpp:309]     Train net output #0: loss = 0.0241997 (* 1 = 0.0241997 loss)
I0711 22:10:19.745604 13090 sgd_solver.cpp:106] Iteration 3700, lr = 1e-05
I0711 22:10:36.913959 13090 solver.cpp:290] Iteration 3800 (5.82483 iter/s, 17.1679s/100 iter), loss = 0.0321409
I0711 22:10:36.913981 13090 solver.cpp:309]     Train net output #0: loss = 0.0321411 (* 1 = 0.0321411 loss)
I0711 22:10:36.913990 13090 sgd_solver.cpp:106] Iteration 3800, lr = 1e-05
I0711 22:10:53.852423 13090 solver.cpp:290] Iteration 3900 (5.90389 iter/s, 16.938s/100 iter), loss = 0.0388618
I0711 22:10:53.852493 13090 solver.cpp:309]     Train net output #0: loss = 0.038862 (* 1 = 0.038862 loss)
I0711 22:10:53.852500 13090 sgd_solver.cpp:106] Iteration 3900, lr = 1e-05
I0711 22:11:10.703076 13090 solver.cpp:354] Sparsity after update:
I0711 22:11:10.705353 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 22:11:10.705361 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 22:11:10.705370 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 22:11:10.705375 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 22:11:10.705380 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 22:11:10.705384 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 22:11:10.705387 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 22:11:10.705392 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 22:11:10.705396 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 22:11:10.705400 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 22:11:10.705404 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 22:11:10.705409 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 22:11:10.705412 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 22:11:10.705416 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 22:11:10.705420 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 22:11:10.705425 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 22:11:10.705428 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 22:11:10.705433 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 22:11:10.705437 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 22:11:10.705577 13090 solver.cpp:467] Iteration 4000, Testing net (#0)
I0711 22:11:57.570858 13090 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.946874
I0711 22:11:57.570976 13090 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999814
I0711 22:11:57.570983 13090 solver.cpp:540]     Test net output #2: loss = 0.13827 (* 1 = 0.13827 loss)
I0711 22:11:57.771574 13090 solver.cpp:290] Iteration 4000 (1.56452 iter/s, 63.9173s/100 iter), loss = 0.0226881
I0711 22:11:57.771600 13090 solver.cpp:309]     Train net output #0: loss = 0.0226883 (* 1 = 0.0226883 loss)
I0711 22:11:57.771613 13090 sgd_solver.cpp:106] Iteration 4000, lr = 1e-05
I0711 22:12:23.962608 13090 solver.cpp:290] Iteration 4100 (3.81821 iter/s, 26.1903s/100 iter), loss = 0.0608795
I0711 22:12:23.962631 13090 solver.cpp:309]     Train net output #0: loss = 0.0608797 (* 1 = 0.0608797 loss)
I0711 22:12:23.962638 13090 sgd_solver.cpp:106] Iteration 4100, lr = 1e-05
I0711 22:12:27.619036 13199 blocking_queue.cpp:50] Waiting for data
I0711 22:13:20.395987 13090 solver.cpp:290] Iteration 4200 (1.77205 iter/s, 56.4318s/100 iter), loss = 0.0321732
I0711 22:13:20.396040 13090 solver.cpp:309]     Train net output #0: loss = 0.0321733 (* 1 = 0.0321733 loss)
I0711 22:13:20.396049 13090 sgd_solver.cpp:106] Iteration 4200, lr = 1e-05
I0711 22:13:39.091428 13090 solver.cpp:290] Iteration 4300 (5.34906 iter/s, 18.6949s/100 iter), loss = 0.0460296
I0711 22:13:39.091452 13090 solver.cpp:309]     Train net output #0: loss = 0.0460298 (* 1 = 0.0460298 loss)
I0711 22:13:39.091459 13090 sgd_solver.cpp:106] Iteration 4300, lr = 1e-05
I0711 22:13:56.035197 13090 solver.cpp:290] Iteration 4400 (5.90205 iter/s, 16.9433s/100 iter), loss = 0.0748695
I0711 22:13:56.035248 13090 solver.cpp:309]     Train net output #0: loss = 0.0748696 (* 1 = 0.0748696 loss)
I0711 22:13:56.035256 13090 sgd_solver.cpp:106] Iteration 4400, lr = 1e-05
I0711 22:14:12.984359 13090 solver.cpp:290] Iteration 4500 (5.90018 iter/s, 16.9486s/100 iter), loss = 0.0299632
I0711 22:14:12.984383 13090 solver.cpp:309]     Train net output #0: loss = 0.0299634 (* 1 = 0.0299634 loss)
I0711 22:14:12.984392 13090 sgd_solver.cpp:106] Iteration 4500, lr = 1e-05
I0711 22:14:30.049432 13090 solver.cpp:290] Iteration 4600 (5.86009 iter/s, 17.0646s/100 iter), loss = 0.0309133
I0711 22:14:30.049479 13090 solver.cpp:309]     Train net output #0: loss = 0.0309134 (* 1 = 0.0309134 loss)
I0711 22:14:30.049487 13090 sgd_solver.cpp:106] Iteration 4600, lr = 1e-05
I0711 22:14:47.066073 13090 solver.cpp:290] Iteration 4700 (5.87678 iter/s, 17.0161s/100 iter), loss = 0.0956195
I0711 22:14:47.066097 13090 solver.cpp:309]     Train net output #0: loss = 0.0956197 (* 1 = 0.0956197 loss)
I0711 22:14:47.066103 13090 sgd_solver.cpp:106] Iteration 4700, lr = 1e-05
I0711 22:15:04.170584 13090 solver.cpp:290] Iteration 4800 (5.84658 iter/s, 17.104s/100 iter), loss = 0.0599974
I0711 22:15:04.170636 13090 solver.cpp:309]     Train net output #0: loss = 0.0599976 (* 1 = 0.0599976 loss)
I0711 22:15:04.170644 13090 sgd_solver.cpp:106] Iteration 4800, lr = 1e-05
I0711 22:15:21.184660 13090 solver.cpp:290] Iteration 4900 (5.87767 iter/s, 17.0136s/100 iter), loss = 0.0397377
I0711 22:15:21.184687 13090 solver.cpp:309]     Train net output #0: loss = 0.0397379 (* 1 = 0.0397379 loss)
I0711 22:15:21.184696 13090 sgd_solver.cpp:106] Iteration 4900, lr = 1e-05
I0711 22:15:38.024374 13090 solver.cpp:354] Sparsity after update:
I0711 22:15:38.076504 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 22:15:38.076520 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 22:15:38.076529 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 22:15:38.076530 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 22:15:38.076532 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 22:15:38.076534 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 22:15:38.076536 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 22:15:38.076539 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 22:15:38.076540 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 22:15:38.076542 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 22:15:38.076545 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 22:15:38.076547 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 22:15:38.076550 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 22:15:38.076550 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 22:15:38.076552 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 22:15:38.076555 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 22:15:38.076556 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 22:15:38.076558 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 22:15:38.076560 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 22:15:38.227149 13090 solver.cpp:290] Iteration 5000 (5.86786 iter/s, 17.042s/100 iter), loss = 0.021821
I0711 22:15:38.227171 13090 solver.cpp:309]     Train net output #0: loss = 0.0218212 (* 1 = 0.0218212 loss)
I0711 22:15:38.227179 13090 sgd_solver.cpp:106] Iteration 5000, lr = 1e-05
I0711 22:15:55.180563 13090 solver.cpp:290] Iteration 5100 (5.89869 iter/s, 16.9529s/100 iter), loss = 0.0410414
I0711 22:15:55.180588 13090 solver.cpp:309]     Train net output #0: loss = 0.0410416 (* 1 = 0.0410416 loss)
I0711 22:15:55.180594 13090 sgd_solver.cpp:106] Iteration 5100, lr = 1e-05
I0711 22:16:12.060828 13090 solver.cpp:290] Iteration 5200 (5.92425 iter/s, 16.8798s/100 iter), loss = 0.0207933
I0711 22:16:12.060899 13090 solver.cpp:309]     Train net output #0: loss = 0.0207935 (* 1 = 0.0207935 loss)
I0711 22:16:12.060909 13090 sgd_solver.cpp:106] Iteration 5200, lr = 1e-05
I0711 22:16:29.034729 13090 solver.cpp:290] Iteration 5300 (5.89158 iter/s, 16.9734s/100 iter), loss = 0.0301292
I0711 22:16:29.034752 13090 solver.cpp:309]     Train net output #0: loss = 0.0301294 (* 1 = 0.0301294 loss)
I0711 22:16:29.034759 13090 sgd_solver.cpp:106] Iteration 5300, lr = 1e-05
I0711 22:16:46.002617 13090 solver.cpp:290] Iteration 5400 (5.89365 iter/s, 16.9674s/100 iter), loss = 0.0206035
I0711 22:16:46.002709 13090 solver.cpp:309]     Train net output #0: loss = 0.0206037 (* 1 = 0.0206037 loss)
I0711 22:16:46.002722 13090 sgd_solver.cpp:106] Iteration 5400, lr = 1e-05
I0711 22:17:03.035387 13090 solver.cpp:290] Iteration 5500 (5.87123 iter/s, 17.0322s/100 iter), loss = 0.0284124
I0711 22:17:03.035409 13090 solver.cpp:309]     Train net output #0: loss = 0.0284126 (* 1 = 0.0284126 loss)
I0711 22:17:03.035416 13090 sgd_solver.cpp:106] Iteration 5500, lr = 1e-05
I0711 22:17:20.002794 13090 solver.cpp:290] Iteration 5600 (5.89382 iter/s, 16.9669s/100 iter), loss = 0.0523131
I0711 22:17:20.002887 13090 solver.cpp:309]     Train net output #0: loss = 0.0523133 (* 1 = 0.0523133 loss)
I0711 22:17:20.002898 13090 sgd_solver.cpp:106] Iteration 5600, lr = 1e-05
I0711 22:17:37.129174 13090 solver.cpp:290] Iteration 5700 (5.83914 iter/s, 17.1258s/100 iter), loss = 0.0325364
I0711 22:17:37.129197 13090 solver.cpp:309]     Train net output #0: loss = 0.0325366 (* 1 = 0.0325366 loss)
I0711 22:17:37.129204 13090 sgd_solver.cpp:106] Iteration 5700, lr = 1e-05
I0711 22:17:54.291031 13090 solver.cpp:290] Iteration 5800 (5.82704 iter/s, 17.1614s/100 iter), loss = 0.0557424
I0711 22:17:54.291076 13090 solver.cpp:309]     Train net output #0: loss = 0.0557427 (* 1 = 0.0557427 loss)
I0711 22:17:54.291085 13090 sgd_solver.cpp:106] Iteration 5800, lr = 1e-05
I0711 22:18:11.377801 13090 solver.cpp:290] Iteration 5900 (5.85266 iter/s, 17.0863s/100 iter), loss = 0.037098
I0711 22:18:11.377825 13090 solver.cpp:309]     Train net output #0: loss = 0.0370982 (* 1 = 0.0370982 loss)
I0711 22:18:11.377832 13090 sgd_solver.cpp:106] Iteration 5900, lr = 1e-05
I0711 22:18:28.220989 13090 solver.cpp:354] Sparsity after update:
I0711 22:18:28.222954 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 22:18:28.222961 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 22:18:28.222970 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 22:18:28.222971 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 22:18:28.222973 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 22:18:28.222975 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 22:18:28.222977 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 22:18:28.222980 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 22:18:28.222980 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 22:18:28.222982 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 22:18:28.222985 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 22:18:28.222986 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 22:18:28.222990 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 22:18:28.222990 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 22:18:28.222992 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 22:18:28.222995 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 22:18:28.222996 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 22:18:28.222998 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 22:18:28.223001 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 22:18:28.223136 13090 solver.cpp:467] Iteration 6000, Testing net (#0)
I0711 22:18:32.728492 13176 blocking_queue.cpp:50] Waiting for data
I0711 22:19:18.094111 13090 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.947429
I0711 22:19:18.094260 13090 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999626
I0711 22:19:18.094270 13090 solver.cpp:540]     Test net output #2: loss = 0.146325 (* 1 = 0.146325 loss)
I0711 22:19:18.279904 13090 solver.cpp:290] Iteration 6000 (1.49476 iter/s, 66.9003s/100 iter), loss = 0.0333571
I0711 22:19:18.279927 13090 solver.cpp:309]     Train net output #0: loss = 0.0333574 (* 1 = 0.0333574 loss)
I0711 22:19:18.279934 13090 sgd_solver.cpp:106] Iteration 6000, lr = 1e-05
I0711 22:19:36.883790 13090 solver.cpp:290] Iteration 6100 (5.37538 iter/s, 18.6033s/100 iter), loss = 0.0347468
I0711 22:19:36.883813 13090 solver.cpp:309]     Train net output #0: loss = 0.0347471 (* 1 = 0.0347471 loss)
I0711 22:19:36.883819 13090 sgd_solver.cpp:106] Iteration 6100, lr = 1e-05
I0711 22:20:00.619102 13090 solver.cpp:290] Iteration 6200 (4.21325 iter/s, 23.7346s/100 iter), loss = 0.0352523
I0711 22:20:00.619182 13090 solver.cpp:309]     Train net output #0: loss = 0.0352526 (* 1 = 0.0352526 loss)
I0711 22:20:00.619190 13090 sgd_solver.cpp:106] Iteration 6200, lr = 1e-05
I0711 22:20:17.665707 13090 solver.cpp:290] Iteration 6300 (5.86646 iter/s, 17.0461s/100 iter), loss = 0.0860286
I0711 22:20:17.665730 13090 solver.cpp:309]     Train net output #0: loss = 0.0860288 (* 1 = 0.0860288 loss)
I0711 22:20:17.665737 13090 sgd_solver.cpp:106] Iteration 6300, lr = 1e-05
I0711 22:20:34.569309 13090 solver.cpp:290] Iteration 6400 (5.91607 iter/s, 16.9031s/100 iter), loss = 0.0417691
I0711 22:20:34.569363 13090 solver.cpp:309]     Train net output #0: loss = 0.0417694 (* 1 = 0.0417694 loss)
I0711 22:20:34.569371 13090 sgd_solver.cpp:106] Iteration 6400, lr = 1e-05
I0711 22:20:51.372922 13090 solver.cpp:290] Iteration 6500 (5.95128 iter/s, 16.8031s/100 iter), loss = 0.0667606
I0711 22:20:51.372953 13090 solver.cpp:309]     Train net output #0: loss = 0.0667608 (* 1 = 0.0667608 loss)
I0711 22:20:51.372961 13090 sgd_solver.cpp:106] Iteration 6500, lr = 1e-05
I0711 22:21:08.260952 13090 solver.cpp:290] Iteration 6600 (5.92153 iter/s, 16.8875s/100 iter), loss = 0.0244727
I0711 22:21:08.261036 13090 solver.cpp:309]     Train net output #0: loss = 0.0244729 (* 1 = 0.0244729 loss)
I0711 22:21:08.261044 13090 sgd_solver.cpp:106] Iteration 6600, lr = 1e-05
I0711 22:21:25.198943 13090 solver.cpp:290] Iteration 6700 (5.90408 iter/s, 16.9374s/100 iter), loss = 0.018781
I0711 22:21:25.198968 13090 solver.cpp:309]     Train net output #0: loss = 0.0187813 (* 1 = 0.0187813 loss)
I0711 22:21:25.198976 13090 sgd_solver.cpp:106] Iteration 6700, lr = 1e-05
I0711 22:21:42.311765 13090 solver.cpp:290] Iteration 6800 (5.84374 iter/s, 17.1123s/100 iter), loss = 0.0331572
I0711 22:21:42.311862 13090 solver.cpp:309]     Train net output #0: loss = 0.0331574 (* 1 = 0.0331574 loss)
I0711 22:21:42.311873 13090 sgd_solver.cpp:106] Iteration 6800, lr = 1e-05
I0711 22:21:59.478801 13090 solver.cpp:290] Iteration 6900 (5.82531 iter/s, 17.1665s/100 iter), loss = 0.0452859
I0711 22:21:59.478857 13090 solver.cpp:309]     Train net output #0: loss = 0.0452861 (* 1 = 0.0452861 loss)
I0711 22:21:59.478876 13090 sgd_solver.cpp:106] Iteration 6900, lr = 1e-05
I0711 22:22:17.830461 13090 solver.cpp:354] Sparsity after update:
I0711 22:22:17.881878 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 22:22:17.881893 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 22:22:17.881901 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 22:22:17.881903 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 22:22:17.881906 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 22:22:17.881907 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 22:22:17.881909 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 22:22:17.881911 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 22:22:17.881913 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 22:22:17.881916 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 22:22:17.881918 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 22:22:17.881922 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 22:22:17.881923 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 22:22:17.881925 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 22:22:17.881927 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 22:22:17.881929 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 22:22:17.881932 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 22:22:17.881934 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 22:22:17.881937 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 22:22:18.032176 13090 solver.cpp:290] Iteration 7000 (5.39002 iter/s, 18.5528s/100 iter), loss = 0.0239244
I0711 22:22:18.032202 13090 solver.cpp:309]     Train net output #0: loss = 0.0239246 (* 1 = 0.0239246 loss)
I0711 22:22:18.032208 13090 sgd_solver.cpp:106] Iteration 7000, lr = 1e-05
I0711 22:22:35.348201 13090 solver.cpp:290] Iteration 7100 (5.77516 iter/s, 17.3155s/100 iter), loss = 0.0490725
I0711 22:22:35.348227 13090 solver.cpp:309]     Train net output #0: loss = 0.0490727 (* 1 = 0.0490727 loss)
I0711 22:22:35.348232 13090 sgd_solver.cpp:106] Iteration 7100, lr = 1e-05
I0711 22:22:52.558437 13090 solver.cpp:290] Iteration 7200 (5.81066 iter/s, 17.2097s/100 iter), loss = 0.0219998
I0711 22:22:52.558502 13090 solver.cpp:309]     Train net output #0: loss = 0.022 (* 1 = 0.022 loss)
I0711 22:22:52.558513 13090 sgd_solver.cpp:106] Iteration 7200, lr = 1e-05
I0711 22:23:09.711829 13090 solver.cpp:290] Iteration 7300 (5.82993 iter/s, 17.1529s/100 iter), loss = 0.0394309
I0711 22:23:09.711858 13090 solver.cpp:309]     Train net output #0: loss = 0.0394312 (* 1 = 0.0394312 loss)
I0711 22:23:09.711868 13090 sgd_solver.cpp:106] Iteration 7300, lr = 1e-05
I0711 22:23:26.944391 13090 solver.cpp:290] Iteration 7400 (5.80313 iter/s, 17.2321s/100 iter), loss = 0.0497859
I0711 22:23:26.944432 13090 solver.cpp:309]     Train net output #0: loss = 0.0497862 (* 1 = 0.0497862 loss)
I0711 22:23:26.944440 13090 sgd_solver.cpp:106] Iteration 7400, lr = 1e-05
I0711 22:23:44.069950 13090 solver.cpp:290] Iteration 7500 (5.8394 iter/s, 17.1251s/100 iter), loss = 0.0447849
I0711 22:23:44.069973 13090 solver.cpp:309]     Train net output #0: loss = 0.0447852 (* 1 = 0.0447852 loss)
I0711 22:23:44.069980 13090 sgd_solver.cpp:106] Iteration 7500, lr = 1e-05
I0711 22:24:01.151614 13090 solver.cpp:290] Iteration 7600 (5.8544 iter/s, 17.0812s/100 iter), loss = 0.0385858
I0711 22:24:01.151695 13090 solver.cpp:309]     Train net output #0: loss = 0.038586 (* 1 = 0.038586 loss)
I0711 22:24:01.151701 13090 sgd_solver.cpp:106] Iteration 7600, lr = 1e-05
I0711 22:24:18.389837 13090 solver.cpp:290] Iteration 7700 (5.80125 iter/s, 17.2377s/100 iter), loss = 0.0270684
I0711 22:24:18.389866 13090 solver.cpp:309]     Train net output #0: loss = 0.0270686 (* 1 = 0.0270686 loss)
I0711 22:24:18.389874 13090 sgd_solver.cpp:106] Iteration 7700, lr = 1e-05
I0711 22:24:35.562696 13090 solver.cpp:290] Iteration 7800 (5.82331 iter/s, 17.1724s/100 iter), loss = 0.0191226
I0711 22:24:35.562832 13090 solver.cpp:309]     Train net output #0: loss = 0.0191229 (* 1 = 0.0191229 loss)
I0711 22:24:35.562841 13090 sgd_solver.cpp:106] Iteration 7800, lr = 1e-05
I0711 22:24:52.823011 13090 solver.cpp:290] Iteration 7900 (5.79384 iter/s, 17.2597s/100 iter), loss = 0.0418846
I0711 22:24:52.823043 13090 solver.cpp:309]     Train net output #0: loss = 0.0418849 (* 1 = 0.0418849 loss)
I0711 22:24:52.823053 13090 sgd_solver.cpp:106] Iteration 7900, lr = 1e-05
I0711 22:25:09.832254 13090 solver.cpp:354] Sparsity after update:
I0711 22:25:09.834468 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 22:25:09.834477 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 22:25:09.834484 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 22:25:09.834486 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 22:25:09.834488 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 22:25:09.834491 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 22:25:09.834492 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 22:25:09.834494 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 22:25:09.834496 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 22:25:09.834498 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 22:25:09.834501 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 22:25:09.834502 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 22:25:09.834504 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 22:25:09.834506 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 22:25:09.834508 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 22:25:09.834511 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 22:25:09.834512 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 22:25:09.834514 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 22:25:09.834517 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 22:25:09.834656 13090 solver.cpp:467] Iteration 8000, Testing net (#0)
I0711 22:25:59.378186 13090 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.948537
I0711 22:25:59.378284 13090 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.99976
I0711 22:25:59.378291 13090 solver.cpp:540]     Test net output #2: loss = 0.141758 (* 1 = 0.141758 loss)
I0711 22:25:59.574710 13090 solver.cpp:290] Iteration 8000 (1.49813 iter/s, 66.7499s/100 iter), loss = 0.0480269
I0711 22:25:59.574738 13090 solver.cpp:309]     Train net output #0: loss = 0.0480271 (* 1 = 0.0480271 loss)
I0711 22:25:59.574744 13090 sgd_solver.cpp:106] Iteration 8000, lr = 1e-05
I0711 22:26:21.698289 13090 solver.cpp:290] Iteration 8100 (4.52019 iter/s, 22.1229s/100 iter), loss = 0.0378518
I0711 22:26:21.698318 13090 solver.cpp:309]     Train net output #0: loss = 0.037852 (* 1 = 0.037852 loss)
I0711 22:26:21.698324 13090 sgd_solver.cpp:106] Iteration 8100, lr = 1e-05
I0711 22:27:03.250861 13227 blocking_queue.cpp:50] Waiting for data
I0711 22:27:12.591315 13090 solver.cpp:290] Iteration 8200 (1.96496 iter/s, 50.8916s/100 iter), loss = 0.0369764
I0711 22:27:12.591344 13090 solver.cpp:309]     Train net output #0: loss = 0.0369767 (* 1 = 0.0369767 loss)
I0711 22:27:12.591351 13090 sgd_solver.cpp:106] Iteration 8200, lr = 1e-05
I0711 22:27:29.834616 13090 solver.cpp:290] Iteration 8300 (5.79952 iter/s, 17.2428s/100 iter), loss = 0.0347742
I0711 22:27:29.834642 13090 solver.cpp:309]     Train net output #0: loss = 0.0347744 (* 1 = 0.0347744 loss)
I0711 22:27:29.834650 13090 sgd_solver.cpp:106] Iteration 8300, lr = 1e-05
I0711 22:27:47.079068 13090 solver.cpp:290] Iteration 8400 (5.79914 iter/s, 17.2439s/100 iter), loss = 0.0219216
I0711 22:27:47.079157 13090 solver.cpp:309]     Train net output #0: loss = 0.0219219 (* 1 = 0.0219219 loss)
I0711 22:27:47.079169 13090 sgd_solver.cpp:106] Iteration 8400, lr = 1e-05
I0711 22:28:04.040101 13090 solver.cpp:290] Iteration 8500 (5.89606 iter/s, 16.9605s/100 iter), loss = 0.0627958
I0711 22:28:04.040127 13090 solver.cpp:309]     Train net output #0: loss = 0.062796 (* 1 = 0.062796 loss)
I0711 22:28:04.040135 13090 sgd_solver.cpp:106] Iteration 8500, lr = 1e-05
I0711 22:28:21.343745 13090 solver.cpp:290] Iteration 8600 (5.7793 iter/s, 17.3031s/100 iter), loss = 0.0721623
I0711 22:28:21.343817 13090 solver.cpp:309]     Train net output #0: loss = 0.0721625 (* 1 = 0.0721625 loss)
I0711 22:28:21.343825 13090 sgd_solver.cpp:106] Iteration 8600, lr = 1e-05
I0711 22:28:38.650297 13090 solver.cpp:290] Iteration 8700 (5.77834 iter/s, 17.306s/100 iter), loss = 0.0345917
I0711 22:28:38.650321 13090 solver.cpp:309]     Train net output #0: loss = 0.034592 (* 1 = 0.034592 loss)
I0711 22:28:38.650328 13090 sgd_solver.cpp:106] Iteration 8700, lr = 1e-05
I0711 22:28:55.805294 13090 solver.cpp:290] Iteration 8800 (5.82938 iter/s, 17.1545s/100 iter), loss = 0.0299143
I0711 22:28:55.805377 13090 solver.cpp:309]     Train net output #0: loss = 0.0299146 (* 1 = 0.0299146 loss)
I0711 22:28:55.805390 13090 sgd_solver.cpp:106] Iteration 8800, lr = 1e-05
I0711 22:29:12.938206 13090 solver.cpp:290] Iteration 8900 (5.83691 iter/s, 17.1324s/100 iter), loss = 0.0379329
I0711 22:29:12.938230 13090 solver.cpp:309]     Train net output #0: loss = 0.0379331 (* 1 = 0.0379331 loss)
I0711 22:29:12.938237 13090 sgd_solver.cpp:106] Iteration 8900, lr = 1e-05
I0711 22:29:29.881036 13090 solver.cpp:354] Sparsity after update:
I0711 22:29:29.936976 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 22:29:29.936992 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 22:29:29.937000 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 22:29:29.937001 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 22:29:29.937003 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 22:29:29.937005 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 22:29:29.937007 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 22:29:29.937010 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 22:29:29.937011 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 22:29:29.937013 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 22:29:29.937016 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 22:29:29.937017 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 22:29:29.937019 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 22:29:29.937021 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 22:29:29.937023 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 22:29:29.937026 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 22:29:29.937027 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 22:29:29.937029 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 22:29:29.937031 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 22:29:30.086614 13090 solver.cpp:290] Iteration 9000 (5.83161 iter/s, 17.1479s/100 iter), loss = 0.0320511
I0711 22:29:30.086638 13090 solver.cpp:309]     Train net output #0: loss = 0.0320513 (* 1 = 0.0320513 loss)
I0711 22:29:30.086645 13090 sgd_solver.cpp:106] Iteration 9000, lr = 1e-05
I0711 22:29:47.300457 13090 solver.cpp:290] Iteration 9100 (5.80945 iter/s, 17.2133s/100 iter), loss = 0.0479693
I0711 22:29:47.300479 13090 solver.cpp:309]     Train net output #0: loss = 0.0479695 (* 1 = 0.0479695 loss)
I0711 22:29:47.300487 13090 sgd_solver.cpp:106] Iteration 9100, lr = 1e-05
I0711 22:30:04.405527 13090 solver.cpp:290] Iteration 9200 (5.84639 iter/s, 17.1046s/100 iter), loss = 0.0392525
I0711 22:30:04.405576 13090 solver.cpp:309]     Train net output #0: loss = 0.0392527 (* 1 = 0.0392527 loss)
I0711 22:30:04.405583 13090 sgd_solver.cpp:106] Iteration 9200, lr = 1e-05
I0711 22:30:21.415416 13090 solver.cpp:290] Iteration 9300 (5.87911 iter/s, 17.0094s/100 iter), loss = 0.0433125
I0711 22:30:21.415441 13090 solver.cpp:309]     Train net output #0: loss = 0.0433127 (* 1 = 0.0433127 loss)
I0711 22:30:21.415447 13090 sgd_solver.cpp:106] Iteration 9300, lr = 1e-05
I0711 22:30:38.517249 13090 solver.cpp:290] Iteration 9400 (5.8475 iter/s, 17.1013s/100 iter), loss = 0.0221018
I0711 22:30:38.517343 13090 solver.cpp:309]     Train net output #0: loss = 0.0221021 (* 1 = 0.0221021 loss)
I0711 22:30:38.517351 13090 sgd_solver.cpp:106] Iteration 9400, lr = 1e-05
I0711 22:30:55.558162 13090 solver.cpp:290] Iteration 9500 (5.86842 iter/s, 17.0404s/100 iter), loss = 0.0413281
I0711 22:30:55.558187 13090 solver.cpp:309]     Train net output #0: loss = 0.0413283 (* 1 = 0.0413283 loss)
I0711 22:30:55.558192 13090 sgd_solver.cpp:106] Iteration 9500, lr = 1e-05
I0711 22:31:12.597360 13090 solver.cpp:290] Iteration 9600 (5.86899 iter/s, 17.0387s/100 iter), loss = 0.0340849
I0711 22:31:12.597488 13090 solver.cpp:309]     Train net output #0: loss = 0.0340851 (* 1 = 0.0340851 loss)
I0711 22:31:12.597498 13090 sgd_solver.cpp:106] Iteration 9600, lr = 1e-05
I0711 22:31:29.751783 13090 solver.cpp:290] Iteration 9700 (5.8296 iter/s, 17.1538s/100 iter), loss = 0.0332697
I0711 22:31:29.751806 13090 solver.cpp:309]     Train net output #0: loss = 0.0332699 (* 1 = 0.0332699 loss)
I0711 22:31:29.751813 13090 sgd_solver.cpp:106] Iteration 9700, lr = 1e-05
I0711 22:31:46.865011 13090 solver.cpp:290] Iteration 9800 (5.8436 iter/s, 17.1127s/100 iter), loss = 0.0252254
I0711 22:31:46.865068 13090 solver.cpp:309]     Train net output #0: loss = 0.0252256 (* 1 = 0.0252256 loss)
I0711 22:31:46.865079 13090 sgd_solver.cpp:106] Iteration 9800, lr = 1e-05
I0711 22:32:04.034013 13090 solver.cpp:290] Iteration 9900 (5.82463 iter/s, 17.1685s/100 iter), loss = 0.0238339
I0711 22:32:04.034039 13090 solver.cpp:309]     Train net output #0: loss = 0.0238341 (* 1 = 0.0238341 loss)
I0711 22:32:04.034047 13090 sgd_solver.cpp:106] Iteration 9900, lr = 1e-05
I0711 22:32:20.982661 13090 solver.cpp:594] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/sparse/cityscapes5_jsegnet21v2_iter_10000.caffemodel
I0711 22:32:21.130033 13090 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/sparse/cityscapes5_jsegnet21v2_iter_10000.solverstate
I0711 22:32:21.147176 13090 solver.cpp:354] Sparsity after update:
I0711 22:32:21.148767 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 22:32:21.148775 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 22:32:21.148782 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 22:32:21.148784 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 22:32:21.148787 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 22:32:21.148788 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 22:32:21.148790 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 22:32:21.148792 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 22:32:21.148794 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 22:32:21.148797 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 22:32:21.148798 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 22:32:21.148800 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 22:32:21.148802 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 22:32:21.148804 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 22:32:21.148805 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 22:32:21.148808 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 22:32:21.148809 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 22:32:21.148813 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 22:32:21.148826 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 22:32:21.148972 13090 solver.cpp:467] Iteration 10000, Testing net (#0)
I0711 22:33:10.022608 13090 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.949002
I0711 22:33:10.023175 13090 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999482
I0711 22:33:10.023187 13090 solver.cpp:540]     Test net output #2: loss = 0.150472 (* 1 = 0.150472 loss)
I0711 22:33:10.208782 13090 solver.cpp:290] Iteration 10000 (1.51119 iter/s, 66.1729s/100 iter), loss = 0.044427
I0711 22:33:10.208806 13090 solver.cpp:309]     Train net output #0: loss = 0.0444272 (* 1 = 0.0444272 loss)
I0711 22:33:10.208812 13090 sgd_solver.cpp:106] Iteration 10000, lr = 1e-05
I0711 22:33:27.442476 13090 solver.cpp:290] Iteration 10100 (5.80276 iter/s, 17.2332s/100 iter), loss = 0.035033
I0711 22:33:27.442505 13090 solver.cpp:309]     Train net output #0: loss = 0.0350332 (* 1 = 0.0350332 loss)
I0711 22:33:27.442515 13090 sgd_solver.cpp:106] Iteration 10100, lr = 1e-05
I0711 22:33:44.759806 13090 solver.cpp:290] Iteration 10200 (5.77473 iter/s, 17.3168s/100 iter), loss = 0.0205332
I0711 22:33:44.759907 13090 solver.cpp:309]     Train net output #0: loss = 0.0205334 (* 1 = 0.0205334 loss)
I0711 22:33:44.759914 13090 sgd_solver.cpp:106] Iteration 10200, lr = 1e-05
I0711 22:34:02.009871 13090 solver.cpp:290] Iteration 10300 (5.79727 iter/s, 17.2495s/100 iter), loss = 0.0360116
I0711 22:34:02.009899 13090 solver.cpp:309]     Train net output #0: loss = 0.0360118 (* 1 = 0.0360118 loss)
I0711 22:34:02.009917 13090 sgd_solver.cpp:106] Iteration 10300, lr = 1e-05
I0711 22:34:19.376343 13090 solver.cpp:290] Iteration 10400 (5.75839 iter/s, 17.366s/100 iter), loss = 0.0505369
I0711 22:34:19.376446 13090 solver.cpp:309]     Train net output #0: loss = 0.050537 (* 1 = 0.050537 loss)
I0711 22:34:19.376453 13090 sgd_solver.cpp:106] Iteration 10400, lr = 1e-05
I0711 22:34:36.584698 13090 solver.cpp:290] Iteration 10500 (5.81133 iter/s, 17.2078s/100 iter), loss = 0.010596
I0711 22:34:36.584728 13090 solver.cpp:309]     Train net output #0: loss = 0.0105962 (* 1 = 0.0105962 loss)
I0711 22:34:36.584738 13090 sgd_solver.cpp:106] Iteration 10500, lr = 1e-05
I0711 22:34:53.763732 13090 solver.cpp:290] Iteration 10600 (5.82122 iter/s, 17.1785s/100 iter), loss = 0.0299925
I0711 22:34:53.763811 13090 solver.cpp:309]     Train net output #0: loss = 0.0299927 (* 1 = 0.0299927 loss)
I0711 22:34:53.763819 13090 sgd_solver.cpp:106] Iteration 10600, lr = 1e-05
I0711 22:35:10.791610 13090 solver.cpp:290] Iteration 10700 (5.87291 iter/s, 17.0273s/100 iter), loss = 0.0292596
I0711 22:35:10.791633 13090 solver.cpp:309]     Train net output #0: loss = 0.0292598 (* 1 = 0.0292598 loss)
I0711 22:35:10.791640 13090 sgd_solver.cpp:106] Iteration 10700, lr = 1e-05
I0711 22:35:27.820221 13090 solver.cpp:290] Iteration 10800 (5.87264 iter/s, 17.0281s/100 iter), loss = 0.0262356
I0711 22:35:27.820315 13090 solver.cpp:309]     Train net output #0: loss = 0.0262358 (* 1 = 0.0262358 loss)
I0711 22:35:27.820328 13090 sgd_solver.cpp:106] Iteration 10800, lr = 1e-05
I0711 22:35:44.790834 13090 solver.cpp:290] Iteration 10900 (5.89273 iter/s, 16.9701s/100 iter), loss = 0.0247454
I0711 22:35:44.790858 13090 solver.cpp:309]     Train net output #0: loss = 0.0247456 (* 1 = 0.0247456 loss)
I0711 22:35:44.790866 13090 sgd_solver.cpp:106] Iteration 10900, lr = 1e-05
I0711 22:36:01.632617 13090 solver.cpp:354] Sparsity after update:
I0711 22:36:01.694007 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 22:36:01.694025 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 22:36:01.694032 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 22:36:01.694034 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 22:36:01.694036 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 22:36:01.694038 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 22:36:01.694041 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 22:36:01.694042 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 22:36:01.694044 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 22:36:01.694046 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 22:36:01.694048 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 22:36:01.694051 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 22:36:01.694053 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 22:36:01.694056 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 22:36:01.694057 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 22:36:01.694059 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 22:36:01.694061 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 22:36:01.694062 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 22:36:01.694064 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 22:36:01.844369 13090 solver.cpp:290] Iteration 11000 (5.86406 iter/s, 17.053s/100 iter), loss = 0.023616
I0711 22:36:01.844393 13090 solver.cpp:309]     Train net output #0: loss = 0.0236162 (* 1 = 0.0236162 loss)
I0711 22:36:01.844399 13090 sgd_solver.cpp:106] Iteration 11000, lr = 1e-05
I0711 22:36:19.004046 13090 solver.cpp:290] Iteration 11100 (5.82778 iter/s, 17.1592s/100 iter), loss = 0.0207858
I0711 22:36:19.004068 13090 solver.cpp:309]     Train net output #0: loss = 0.0207859 (* 1 = 0.0207859 loss)
I0711 22:36:19.004076 13090 sgd_solver.cpp:106] Iteration 11100, lr = 1e-05
I0711 22:36:36.180647 13090 solver.cpp:290] Iteration 11200 (5.82204 iter/s, 17.1761s/100 iter), loss = 0.0366037
I0711 22:36:36.180718 13090 solver.cpp:309]     Train net output #0: loss = 0.0366039 (* 1 = 0.0366039 loss)
I0711 22:36:36.180727 13090 sgd_solver.cpp:106] Iteration 11200, lr = 1e-05
I0711 22:36:53.199452 13090 solver.cpp:290] Iteration 11300 (5.87603 iter/s, 17.0183s/100 iter), loss = 0.0284623
I0711 22:36:53.199477 13090 solver.cpp:309]     Train net output #0: loss = 0.0284625 (* 1 = 0.0284625 loss)
I0711 22:36:53.199483 13090 sgd_solver.cpp:106] Iteration 11300, lr = 1e-05
I0711 22:37:10.153646 13090 solver.cpp:290] Iteration 11400 (5.89841 iter/s, 16.9537s/100 iter), loss = 0.0199253
I0711 22:37:10.153703 13090 solver.cpp:309]     Train net output #0: loss = 0.0199254 (* 1 = 0.0199254 loss)
I0711 22:37:10.153712 13090 sgd_solver.cpp:106] Iteration 11400, lr = 1e-05
I0711 22:37:27.125118 13090 solver.cpp:290] Iteration 11500 (5.89242 iter/s, 16.971s/100 iter), loss = 0.0245555
I0711 22:37:27.125145 13090 solver.cpp:309]     Train net output #0: loss = 0.0245557 (* 1 = 0.0245557 loss)
I0711 22:37:27.125155 13090 sgd_solver.cpp:106] Iteration 11500, lr = 1e-05
I0711 22:37:44.172281 13090 solver.cpp:290] Iteration 11600 (5.86624 iter/s, 17.0467s/100 iter), loss = 0.0351137
I0711 22:37:44.172333 13090 solver.cpp:309]     Train net output #0: loss = 0.0351138 (* 1 = 0.0351138 loss)
I0711 22:37:44.172341 13090 sgd_solver.cpp:106] Iteration 11600, lr = 1e-05
I0711 22:38:01.096693 13090 solver.cpp:290] Iteration 11700 (5.9088 iter/s, 16.9239s/100 iter), loss = 0.0247386
I0711 22:38:01.096716 13090 solver.cpp:309]     Train net output #0: loss = 0.0247388 (* 1 = 0.0247388 loss)
I0711 22:38:01.096724 13090 sgd_solver.cpp:106] Iteration 11700, lr = 1e-05
I0711 22:38:18.158308 13090 solver.cpp:290] Iteration 11800 (5.86128 iter/s, 17.0611s/100 iter), loss = 0.0191639
I0711 22:38:18.158368 13090 solver.cpp:309]     Train net output #0: loss = 0.0191641 (* 1 = 0.0191641 loss)
I0711 22:38:18.158380 13090 sgd_solver.cpp:106] Iteration 11800, lr = 1e-05
I0711 22:38:35.239903 13090 solver.cpp:290] Iteration 11900 (5.85443 iter/s, 17.0811s/100 iter), loss = 0.0354869
I0711 22:38:35.239929 13090 solver.cpp:309]     Train net output #0: loss = 0.0354871 (* 1 = 0.0354871 loss)
I0711 22:38:35.239936 13090 sgd_solver.cpp:106] Iteration 11900, lr = 1e-05
I0711 22:38:52.158083 13090 solver.cpp:354] Sparsity after update:
I0711 22:38:52.160271 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 22:38:52.160279 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 22:38:52.160286 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 22:38:52.160289 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 22:38:52.160290 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 22:38:52.160292 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 22:38:52.160295 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 22:38:52.160296 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 22:38:52.160298 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 22:38:52.160300 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 22:38:52.160301 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 22:38:52.160303 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 22:38:52.160305 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 22:38:52.160307 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 22:38:52.160310 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 22:38:52.160311 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 22:38:52.160313 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 22:38:52.160316 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 22:38:52.160320 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 22:38:52.160514 13090 solver.cpp:467] Iteration 12000, Testing net (#0)
I0711 22:39:38.487416 13090 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.949087
I0711 22:39:38.487506 13090 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999251
I0711 22:39:38.487514 13090 solver.cpp:540]     Test net output #2: loss = 0.161603 (* 1 = 0.161603 loss)
I0711 22:39:38.683210 13090 solver.cpp:290] Iteration 12000 (1.57625 iter/s, 63.4416s/100 iter), loss = 0.0305345
I0711 22:39:38.683238 13090 solver.cpp:309]     Train net output #0: loss = 0.0305347 (* 1 = 0.0305347 loss)
I0711 22:39:38.683246 13090 sgd_solver.cpp:106] Iteration 12000, lr = 1e-05
I0711 22:39:55.511530 13090 solver.cpp:290] Iteration 12100 (5.94254 iter/s, 16.8278s/100 iter), loss = 0.0309101
I0711 22:39:55.511554 13090 solver.cpp:309]     Train net output #0: loss = 0.0309102 (* 1 = 0.0309102 loss)
I0711 22:39:55.511560 13090 sgd_solver.cpp:106] Iteration 12100, lr = 1e-05
I0711 22:40:12.473546 13090 solver.cpp:290] Iteration 12200 (5.89569 iter/s, 16.9615s/100 iter), loss = 0.0195374
I0711 22:40:12.473599 13090 solver.cpp:309]     Train net output #0: loss = 0.0195376 (* 1 = 0.0195376 loss)
I0711 22:40:12.473606 13090 sgd_solver.cpp:106] Iteration 12200, lr = 1e-05
I0711 22:40:29.371021 13090 solver.cpp:290] Iteration 12300 (5.91822 iter/s, 16.897s/100 iter), loss = 0.0575116
I0711 22:40:29.371043 13090 solver.cpp:309]     Train net output #0: loss = 0.0575118 (* 1 = 0.0575118 loss)
I0711 22:40:29.371050 13090 sgd_solver.cpp:106] Iteration 12300, lr = 1e-05
I0711 22:40:46.310914 13090 solver.cpp:290] Iteration 12400 (5.90339 iter/s, 16.9394s/100 iter), loss = 0.045838
I0711 22:40:46.311003 13090 solver.cpp:309]     Train net output #0: loss = 0.0458382 (* 1 = 0.0458382 loss)
I0711 22:40:46.311010 13090 sgd_solver.cpp:106] Iteration 12400, lr = 1e-05
I0711 22:41:03.331689 13090 solver.cpp:290] Iteration 12500 (5.87536 iter/s, 17.0202s/100 iter), loss = 0.0204423
I0711 22:41:03.331717 13090 solver.cpp:309]     Train net output #0: loss = 0.0204425 (* 1 = 0.0204425 loss)
I0711 22:41:03.331727 13090 sgd_solver.cpp:106] Iteration 12500, lr = 1e-05
I0711 22:41:20.356171 13090 solver.cpp:290] Iteration 12600 (5.87406 iter/s, 17.024s/100 iter), loss = 0.0247841
I0711 22:41:20.356271 13090 solver.cpp:309]     Train net output #0: loss = 0.0247843 (* 1 = 0.0247843 loss)
I0711 22:41:20.356282 13090 sgd_solver.cpp:106] Iteration 12600, lr = 1e-05
I0711 22:41:37.361292 13090 solver.cpp:290] Iteration 12700 (5.88077 iter/s, 17.0046s/100 iter), loss = 0.0254945
I0711 22:41:37.361315 13090 solver.cpp:309]     Train net output #0: loss = 0.0254947 (* 1 = 0.0254947 loss)
I0711 22:41:37.361322 13090 sgd_solver.cpp:106] Iteration 12700, lr = 1e-05
I0711 22:41:54.363878 13090 solver.cpp:290] Iteration 12800 (5.88163 iter/s, 17.0021s/100 iter), loss = 0.0207313
I0711 22:41:54.363927 13090 solver.cpp:309]     Train net output #0: loss = 0.0207314 (* 1 = 0.0207314 loss)
I0711 22:41:54.363935 13090 sgd_solver.cpp:106] Iteration 12800, lr = 1e-05
I0711 22:42:11.325501 13090 solver.cpp:290] Iteration 12900 (5.89584 iter/s, 16.9611s/100 iter), loss = 0.034211
I0711 22:42:11.325523 13090 solver.cpp:309]     Train net output #0: loss = 0.0342111 (* 1 = 0.0342111 loss)
I0711 22:42:11.325531 13090 sgd_solver.cpp:106] Iteration 12900, lr = 1e-05
I0711 22:42:28.285490 13090 solver.cpp:354] Sparsity after update:
I0711 22:42:28.336429 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 22:42:28.336444 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 22:42:28.336454 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 22:42:28.336457 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 22:42:28.336460 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 22:42:28.336465 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 22:42:28.336469 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 22:42:28.336473 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 22:42:28.336477 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 22:42:28.336480 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 22:42:28.336483 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 22:42:28.336488 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 22:42:28.336493 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 22:42:28.336496 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 22:42:28.336501 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 22:42:28.336504 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 22:42:28.336508 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 22:42:28.336513 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 22:42:28.336516 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 22:42:28.486263 13090 solver.cpp:290] Iteration 13000 (5.82741 iter/s, 17.1603s/100 iter), loss = 0.033472
I0711 22:42:28.486289 13090 solver.cpp:309]     Train net output #0: loss = 0.0334722 (* 1 = 0.0334722 loss)
I0711 22:42:28.486299 13090 sgd_solver.cpp:106] Iteration 13000, lr = 1e-05
I0711 22:42:45.462153 13090 solver.cpp:290] Iteration 13100 (5.89087 iter/s, 16.9754s/100 iter), loss = 0.0207863
I0711 22:42:45.462177 13090 solver.cpp:309]     Train net output #0: loss = 0.0207865 (* 1 = 0.0207865 loss)
I0711 22:42:45.462183 13090 sgd_solver.cpp:106] Iteration 13100, lr = 1e-05
I0711 22:43:02.509974 13090 solver.cpp:290] Iteration 13200 (5.86602 iter/s, 17.0473s/100 iter), loss = 0.0258876
I0711 22:43:02.510041 13090 solver.cpp:309]     Train net output #0: loss = 0.0258877 (* 1 = 0.0258877 loss)
I0711 22:43:02.510049 13090 sgd_solver.cpp:106] Iteration 13200, lr = 1e-05
I0711 22:43:19.598533 13090 solver.cpp:290] Iteration 13300 (5.85205 iter/s, 17.088s/100 iter), loss = 0.0621381
I0711 22:43:19.598556 13090 solver.cpp:309]     Train net output #0: loss = 0.0621383 (* 1 = 0.0621383 loss)
I0711 22:43:19.598563 13090 sgd_solver.cpp:106] Iteration 13300, lr = 1e-05
I0711 22:43:36.682808 13090 solver.cpp:290] Iteration 13400 (5.8535 iter/s, 17.0838s/100 iter), loss = 0.027458
I0711 22:43:36.682883 13090 solver.cpp:309]     Train net output #0: loss = 0.0274581 (* 1 = 0.0274581 loss)
I0711 22:43:36.682890 13090 sgd_solver.cpp:106] Iteration 13400, lr = 1e-05
I0711 22:43:53.699093 13090 solver.cpp:290] Iteration 13500 (5.87691 iter/s, 17.0158s/100 iter), loss = 0.0377032
I0711 22:43:53.699117 13090 solver.cpp:309]     Train net output #0: loss = 0.0377033 (* 1 = 0.0377033 loss)
I0711 22:43:53.699124 13090 sgd_solver.cpp:106] Iteration 13500, lr = 1e-05
I0711 22:44:10.636605 13090 solver.cpp:290] Iteration 13600 (5.90422 iter/s, 16.937s/100 iter), loss = 0.0264189
I0711 22:44:10.636682 13090 solver.cpp:309]     Train net output #0: loss = 0.0264191 (* 1 = 0.0264191 loss)
I0711 22:44:10.636689 13090 sgd_solver.cpp:106] Iteration 13600, lr = 1e-05
I0711 22:44:27.587496 13090 solver.cpp:290] Iteration 13700 (5.89958 iter/s, 16.9504s/100 iter), loss = 0.041518
I0711 22:44:27.587523 13090 solver.cpp:309]     Train net output #0: loss = 0.0415182 (* 1 = 0.0415182 loss)
I0711 22:44:27.587532 13090 sgd_solver.cpp:106] Iteration 13700, lr = 1e-05
I0711 22:44:44.565479 13090 solver.cpp:290] Iteration 13800 (5.89015 iter/s, 16.9775s/100 iter), loss = 0.0238753
I0711 22:44:44.565536 13090 solver.cpp:309]     Train net output #0: loss = 0.0238754 (* 1 = 0.0238754 loss)
I0711 22:44:44.565547 13090 sgd_solver.cpp:106] Iteration 13800, lr = 1e-05
I0711 22:45:01.616145 13090 solver.cpp:290] Iteration 13900 (5.86505 iter/s, 17.0502s/100 iter), loss = 0.0330012
I0711 22:45:01.616170 13090 solver.cpp:309]     Train net output #0: loss = 0.0330014 (* 1 = 0.0330014 loss)
I0711 22:45:01.616179 13090 sgd_solver.cpp:106] Iteration 13900, lr = 1e-05
I0711 22:45:18.477501 13090 solver.cpp:354] Sparsity after update:
I0711 22:45:18.479681 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 22:45:18.479689 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 22:45:18.479696 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 22:45:18.479699 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 22:45:18.479701 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 22:45:18.479703 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 22:45:18.479706 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 22:45:18.479707 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 22:45:18.479709 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 22:45:18.479712 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 22:45:18.479712 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 22:45:18.479714 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 22:45:18.479717 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 22:45:18.479719 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 22:45:18.479722 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 22:45:18.479722 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 22:45:18.479724 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 22:45:18.479727 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 22:45:18.479728 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 22:45:18.479867 13090 solver.cpp:467] Iteration 14000, Testing net (#0)
I0711 22:46:04.984632 13090 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.948772
I0711 22:46:04.984705 13090 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999299
I0711 22:46:04.984712 13090 solver.cpp:540]     Test net output #2: loss = 0.164605 (* 1 = 0.164605 loss)
I0711 22:46:05.168081 13090 solver.cpp:290] Iteration 14000 (1.57356 iter/s, 63.5502s/100 iter), loss = 0.0259977
I0711 22:46:05.168107 13090 solver.cpp:309]     Train net output #0: loss = 0.0259979 (* 1 = 0.0259979 loss)
I0711 22:46:05.168112 13090 sgd_solver.cpp:106] Iteration 14000, lr = 1e-05
I0711 22:46:21.965914 13090 solver.cpp:290] Iteration 14100 (5.95332 iter/s, 16.7974s/100 iter), loss = 0.0199034
I0711 22:46:21.965940 13090 solver.cpp:309]     Train net output #0: loss = 0.0199036 (* 1 = 0.0199036 loss)
I0711 22:46:21.965947 13090 sgd_solver.cpp:106] Iteration 14100, lr = 1e-05
I0711 22:46:39.457710 13090 solver.cpp:290] Iteration 14200 (5.71713 iter/s, 17.4913s/100 iter), loss = 0.021262
I0711 22:46:39.457806 13090 solver.cpp:309]     Train net output #0: loss = 0.0212622 (* 1 = 0.0212622 loss)
I0711 22:46:39.457818 13090 sgd_solver.cpp:106] Iteration 14200, lr = 1e-05
I0711 22:46:58.293897 13090 solver.cpp:290] Iteration 14300 (5.3091 iter/s, 18.8356s/100 iter), loss = 0.0226287
I0711 22:46:58.293941 13090 solver.cpp:309]     Train net output #0: loss = 0.0226289 (* 1 = 0.0226289 loss)
I0711 22:46:58.293956 13090 sgd_solver.cpp:106] Iteration 14300, lr = 1e-05
I0711 22:47:16.213651 13090 solver.cpp:290] Iteration 14400 (5.5806 iter/s, 17.9192s/100 iter), loss = 0.0208534
I0711 22:47:16.213728 13090 solver.cpp:309]     Train net output #0: loss = 0.0208535 (* 1 = 0.0208535 loss)
I0711 22:47:16.213737 13090 sgd_solver.cpp:106] Iteration 14400, lr = 1e-05
I0711 22:47:34.119474 13090 solver.cpp:290] Iteration 14500 (5.58495 iter/s, 17.9053s/100 iter), loss = 0.0438681
I0711 22:47:34.119498 13090 solver.cpp:309]     Train net output #0: loss = 0.0438683 (* 1 = 0.0438683 loss)
I0711 22:47:34.119505 13090 sgd_solver.cpp:106] Iteration 14500, lr = 1e-05
I0711 22:47:51.643415 13090 solver.cpp:290] Iteration 14600 (5.70664 iter/s, 17.5234s/100 iter), loss = 0.0225341
I0711 22:47:51.643486 13090 solver.cpp:309]     Train net output #0: loss = 0.0225343 (* 1 = 0.0225343 loss)
I0711 22:47:51.643501 13090 sgd_solver.cpp:106] Iteration 14600, lr = 1e-05
I0711 22:48:09.113994 13090 solver.cpp:290] Iteration 14700 (5.72409 iter/s, 17.47s/100 iter), loss = 0.0631895
I0711 22:48:09.114022 13090 solver.cpp:309]     Train net output #0: loss = 0.0631896 (* 1 = 0.0631896 loss)
I0711 22:48:09.114030 13090 sgd_solver.cpp:106] Iteration 14700, lr = 1e-05
I0711 22:48:26.511963 13090 solver.cpp:290] Iteration 14800 (5.74796 iter/s, 17.3975s/100 iter), loss = 0.0356657
I0711 22:48:26.512055 13090 solver.cpp:309]     Train net output #0: loss = 0.0356659 (* 1 = 0.0356659 loss)
I0711 22:48:26.512065 13090 sgd_solver.cpp:106] Iteration 14800, lr = 1e-05
I0711 22:48:43.523710 13090 solver.cpp:290] Iteration 14900 (5.87848 iter/s, 17.0112s/100 iter), loss = 0.0406048
I0711 22:48:43.523731 13090 solver.cpp:309]     Train net output #0: loss = 0.0406049 (* 1 = 0.0406049 loss)
I0711 22:48:43.523738 13090 sgd_solver.cpp:106] Iteration 14900, lr = 1e-05
I0711 22:49:00.420719 13090 solver.cpp:354] Sparsity after update:
I0711 22:49:00.485332 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 22:49:00.485350 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 22:49:00.485358 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 22:49:00.485360 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 22:49:00.485363 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 22:49:00.485364 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 22:49:00.485366 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 22:49:00.485368 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 22:49:00.485370 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 22:49:00.485373 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 22:49:00.485374 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 22:49:00.485376 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 22:49:00.485378 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 22:49:00.485380 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 22:49:00.485383 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 22:49:00.485384 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 22:49:00.485386 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 22:49:00.485388 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 22:49:00.485390 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 22:49:00.635735 13090 solver.cpp:290] Iteration 15000 (5.84401 iter/s, 17.1115s/100 iter), loss = 0.0401788
I0711 22:49:00.635764 13090 solver.cpp:309]     Train net output #0: loss = 0.040179 (* 1 = 0.040179 loss)
I0711 22:49:00.635773 13090 sgd_solver.cpp:106] Iteration 15000, lr = 1e-05
I0711 22:49:17.682315 13090 solver.cpp:290] Iteration 15100 (5.86645 iter/s, 17.0461s/100 iter), loss = 0.0208675
I0711 22:49:17.682338 13090 solver.cpp:309]     Train net output #0: loss = 0.0208677 (* 1 = 0.0208677 loss)
I0711 22:49:17.682345 13090 sgd_solver.cpp:106] Iteration 15100, lr = 1e-05
I0711 22:49:35.951843 13090 solver.cpp:290] Iteration 15200 (5.47375 iter/s, 18.269s/100 iter), loss = 0.024015
I0711 22:49:35.951920 13090 solver.cpp:309]     Train net output #0: loss = 0.0240151 (* 1 = 0.0240151 loss)
I0711 22:49:35.951928 13090 sgd_solver.cpp:106] Iteration 15200, lr = 1e-05
I0711 22:49:53.491266 13090 solver.cpp:290] Iteration 15300 (5.70162 iter/s, 17.5389s/100 iter), loss = 0.0279693
I0711 22:49:53.491322 13090 solver.cpp:309]     Train net output #0: loss = 0.0279695 (* 1 = 0.0279695 loss)
I0711 22:49:53.491345 13090 sgd_solver.cpp:106] Iteration 15300, lr = 1e-05
I0711 22:50:10.765221 13090 solver.cpp:290] Iteration 15400 (5.78923 iter/s, 17.2734s/100 iter), loss = 0.0312528
I0711 22:50:10.765278 13090 solver.cpp:309]     Train net output #0: loss = 0.031253 (* 1 = 0.031253 loss)
I0711 22:50:10.765290 13090 sgd_solver.cpp:106] Iteration 15400, lr = 1e-05
I0711 22:50:27.915601 13090 solver.cpp:290] Iteration 15500 (5.83095 iter/s, 17.1499s/100 iter), loss = 0.0266507
I0711 22:50:27.915624 13090 solver.cpp:309]     Train net output #0: loss = 0.0266509 (* 1 = 0.0266509 loss)
I0711 22:50:27.915632 13090 sgd_solver.cpp:106] Iteration 15500, lr = 1e-05
I0711 22:50:45.042088 13090 solver.cpp:290] Iteration 15600 (5.83907 iter/s, 17.126s/100 iter), loss = 0.0320553
I0711 22:50:45.042146 13090 solver.cpp:309]     Train net output #0: loss = 0.0320555 (* 1 = 0.0320555 loss)
I0711 22:50:45.042161 13090 sgd_solver.cpp:106] Iteration 15600, lr = 1e-05
I0711 22:51:02.077728 13090 solver.cpp:290] Iteration 15700 (5.87022 iter/s, 17.0351s/100 iter), loss = 0.0273836
I0711 22:51:02.077754 13090 solver.cpp:309]     Train net output #0: loss = 0.0273838 (* 1 = 0.0273838 loss)
I0711 22:51:02.077764 13090 sgd_solver.cpp:106] Iteration 15700, lr = 1e-05
I0711 22:51:19.085429 13090 solver.cpp:290] Iteration 15800 (5.87986 iter/s, 17.0072s/100 iter), loss = 0.040705
I0711 22:51:19.085528 13090 solver.cpp:309]     Train net output #0: loss = 0.0407051 (* 1 = 0.0407051 loss)
I0711 22:51:19.085536 13090 sgd_solver.cpp:106] Iteration 15800, lr = 1e-05
I0711 22:51:36.117259 13090 solver.cpp:290] Iteration 15900 (5.87155 iter/s, 17.0313s/100 iter), loss = 0.0357982
I0711 22:51:36.117282 13090 solver.cpp:309]     Train net output #0: loss = 0.0357983 (* 1 = 0.0357983 loss)
I0711 22:51:36.117290 13090 sgd_solver.cpp:106] Iteration 15900, lr = 1e-05
I0711 22:51:53.005621 13090 solver.cpp:354] Sparsity after update:
I0711 22:51:53.007807 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 22:51:53.007814 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 22:51:53.007824 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 22:51:53.007829 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 22:51:53.007833 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 22:51:53.007838 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 22:51:53.007841 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 22:51:53.007846 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 22:51:53.007849 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 22:51:53.007854 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 22:51:53.007858 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 22:51:53.007861 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 22:51:53.007865 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 22:51:53.007869 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 22:51:53.007874 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 22:51:53.007877 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 22:51:53.007882 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 22:51:53.007886 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 22:51:53.007890 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 22:51:53.008028 13090 solver.cpp:467] Iteration 16000, Testing net (#0)
I0711 22:52:39.336042 13090 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.949381
I0711 22:52:39.336122 13090 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999031
I0711 22:52:39.336132 13090 solver.cpp:540]     Test net output #2: loss = 0.171221 (* 1 = 0.171221 loss)
I0711 22:52:39.522639 13090 solver.cpp:290] Iteration 16000 (1.5772 iter/s, 63.4037s/100 iter), loss = 0.0365465
I0711 22:52:39.522663 13090 solver.cpp:309]     Train net output #0: loss = 0.0365466 (* 1 = 0.0365466 loss)
I0711 22:52:39.522672 13090 sgd_solver.cpp:106] Iteration 16000, lr = 1e-05
I0711 22:52:57.076520 13090 solver.cpp:290] Iteration 16100 (5.69691 iter/s, 17.5534s/100 iter), loss = 0.0346034
I0711 22:52:57.076566 13090 solver.cpp:309]     Train net output #0: loss = 0.0346035 (* 1 = 0.0346035 loss)
I0711 22:52:57.076581 13090 sgd_solver.cpp:106] Iteration 16100, lr = 1e-05
I0711 22:53:15.458091 13090 solver.cpp:290] Iteration 16200 (5.44039 iter/s, 18.381s/100 iter), loss = 0.0542311
I0711 22:53:15.458178 13090 solver.cpp:309]     Train net output #0: loss = 0.0542312 (* 1 = 0.0542312 loss)
I0711 22:53:15.458189 13090 sgd_solver.cpp:106] Iteration 16200, lr = 1e-05
I0711 22:53:34.016510 13090 solver.cpp:290] Iteration 16300 (5.38856 iter/s, 18.5578s/100 iter), loss = 0.0410569
I0711 22:53:34.016649 13090 solver.cpp:309]     Train net output #0: loss = 0.041057 (* 1 = 0.041057 loss)
I0711 22:53:34.016712 13090 sgd_solver.cpp:106] Iteration 16300, lr = 1e-05
I0711 22:53:52.088068 13090 solver.cpp:290] Iteration 16400 (5.53374 iter/s, 18.0709s/100 iter), loss = 0.0282318
I0711 22:53:52.088390 13090 solver.cpp:309]     Train net output #0: loss = 0.0282319 (* 1 = 0.0282319 loss)
I0711 22:53:52.088505 13090 sgd_solver.cpp:106] Iteration 16400, lr = 1e-05
I0711 22:54:09.877806 13090 solver.cpp:290] Iteration 16500 (5.62147 iter/s, 17.7889s/100 iter), loss = 0.0180467
I0711 22:54:09.877933 13090 solver.cpp:309]     Train net output #0: loss = 0.0180469 (* 1 = 0.0180469 loss)
I0711 22:54:09.877976 13090 sgd_solver.cpp:106] Iteration 16500, lr = 1e-05
I0711 22:54:27.137202 13090 solver.cpp:290] Iteration 16600 (5.79414 iter/s, 17.2588s/100 iter), loss = 0.0307938
I0711 22:54:27.137316 13090 solver.cpp:309]     Train net output #0: loss = 0.030794 (* 1 = 0.030794 loss)
I0711 22:54:27.137331 13090 sgd_solver.cpp:106] Iteration 16600, lr = 1e-05
I0711 22:54:44.307500 13090 solver.cpp:290] Iteration 16700 (5.8242 iter/s, 17.1697s/100 iter), loss = 0.0660511
I0711 22:54:44.307526 13090 solver.cpp:309]     Train net output #0: loss = 0.0660513 (* 1 = 0.0660513 loss)
I0711 22:54:44.307533 13090 sgd_solver.cpp:106] Iteration 16700, lr = 1e-05
I0711 22:55:01.233558 13090 solver.cpp:290] Iteration 16800 (5.90822 iter/s, 16.9256s/100 iter), loss = 0.0720282
I0711 22:55:01.233669 13090 solver.cpp:309]     Train net output #0: loss = 0.0720283 (* 1 = 0.0720283 loss)
I0711 22:55:01.233677 13090 sgd_solver.cpp:106] Iteration 16800, lr = 1e-05
I0711 22:55:18.245760 13090 solver.cpp:290] Iteration 16900 (5.87833 iter/s, 17.0116s/100 iter), loss = 0.0233099
I0711 22:55:18.245784 13090 solver.cpp:309]     Train net output #0: loss = 0.0233101 (* 1 = 0.0233101 loss)
I0711 22:55:18.245790 13090 sgd_solver.cpp:106] Iteration 16900, lr = 1e-05
I0711 22:55:35.016396 13090 solver.cpp:354] Sparsity after update:
I0711 22:55:35.082880 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 22:55:35.082895 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 22:55:35.082901 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 22:55:35.082903 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 22:55:35.082906 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 22:55:35.082907 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 22:55:35.082909 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 22:55:35.082911 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 22:55:35.082913 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 22:55:35.082916 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 22:55:35.082917 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 22:55:35.082919 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 22:55:35.082921 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 22:55:35.082923 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 22:55:35.082926 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 22:55:35.082927 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 22:55:35.082929 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 22:55:35.082931 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 22:55:35.082933 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 22:55:35.234064 13090 solver.cpp:290] Iteration 17000 (5.88657 iter/s, 16.9878s/100 iter), loss = 0.017767
I0711 22:55:35.234091 13090 solver.cpp:309]     Train net output #0: loss = 0.0177671 (* 1 = 0.0177671 loss)
I0711 22:55:35.234097 13090 sgd_solver.cpp:106] Iteration 17000, lr = 1e-05
I0711 22:55:52.315641 13090 solver.cpp:290] Iteration 17100 (5.85443 iter/s, 17.0811s/100 iter), loss = 0.0181443
I0711 22:55:52.315665 13090 solver.cpp:309]     Train net output #0: loss = 0.0181444 (* 1 = 0.0181444 loss)
I0711 22:55:52.315671 13090 sgd_solver.cpp:106] Iteration 17100, lr = 1e-05
I0711 22:56:09.405721 13090 solver.cpp:290] Iteration 17200 (5.85151 iter/s, 17.0896s/100 iter), loss = 0.0250677
I0711 22:56:09.405817 13090 solver.cpp:309]     Train net output #0: loss = 0.0250679 (* 1 = 0.0250679 loss)
I0711 22:56:09.405827 13090 sgd_solver.cpp:106] Iteration 17200, lr = 1e-05
I0711 22:56:26.454802 13090 solver.cpp:290] Iteration 17300 (5.86561 iter/s, 17.0485s/100 iter), loss = 0.0226854
I0711 22:56:26.454830 13090 solver.cpp:309]     Train net output #0: loss = 0.0226855 (* 1 = 0.0226855 loss)
I0711 22:56:26.454839 13090 sgd_solver.cpp:106] Iteration 17300, lr = 1e-05
I0711 22:56:43.442368 13090 solver.cpp:290] Iteration 17400 (5.88683 iter/s, 16.9871s/100 iter), loss = 0.0245536
I0711 22:56:43.442437 13090 solver.cpp:309]     Train net output #0: loss = 0.0245537 (* 1 = 0.0245537 loss)
I0711 22:56:43.442445 13090 sgd_solver.cpp:106] Iteration 17400, lr = 1e-05
I0711 22:57:00.605675 13090 solver.cpp:290] Iteration 17500 (5.82656 iter/s, 17.1628s/100 iter), loss = 0.0506717
I0711 22:57:00.605702 13090 solver.cpp:309]     Train net output #0: loss = 0.0506718 (* 1 = 0.0506718 loss)
I0711 22:57:00.605711 13090 sgd_solver.cpp:106] Iteration 17500, lr = 1e-05
I0711 22:57:17.613247 13090 solver.cpp:290] Iteration 17600 (5.8799 iter/s, 17.0071s/100 iter), loss = 0.0194577
I0711 22:57:17.613348 13090 solver.cpp:309]     Train net output #0: loss = 0.0194578 (* 1 = 0.0194578 loss)
I0711 22:57:17.613356 13090 sgd_solver.cpp:106] Iteration 17600, lr = 1e-05
I0711 22:57:34.638491 13090 solver.cpp:290] Iteration 17700 (5.87382 iter/s, 17.0247s/100 iter), loss = 0.0383852
I0711 22:57:34.638517 13090 solver.cpp:309]     Train net output #0: loss = 0.0383854 (* 1 = 0.0383854 loss)
I0711 22:57:34.638525 13090 sgd_solver.cpp:106] Iteration 17700, lr = 1e-05
I0711 22:57:51.645143 13090 solver.cpp:290] Iteration 17800 (5.88022 iter/s, 17.0062s/100 iter), loss = 0.121997
I0711 22:57:51.645192 13090 solver.cpp:309]     Train net output #0: loss = 0.121997 (* 1 = 0.121997 loss)
I0711 22:57:51.645200 13090 sgd_solver.cpp:106] Iteration 17800, lr = 1e-05
I0711 22:58:08.658669 13090 solver.cpp:290] Iteration 17900 (5.87785 iter/s, 17.013s/100 iter), loss = 0.0378805
I0711 22:58:08.658694 13090 solver.cpp:309]     Train net output #0: loss = 0.0378807 (* 1 = 0.0378807 loss)
I0711 22:58:08.658701 13090 sgd_solver.cpp:106] Iteration 17900, lr = 1e-05
I0711 22:58:25.496639 13090 solver.cpp:354] Sparsity after update:
I0711 22:58:25.498575 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 22:58:25.498584 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 22:58:25.498590 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 22:58:25.498594 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 22:58:25.498594 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 22:58:25.498596 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 22:58:25.498598 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 22:58:25.498600 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 22:58:25.498602 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 22:58:25.498605 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 22:58:25.498606 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 22:58:25.498608 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 22:58:25.498610 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 22:58:25.498612 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 22:58:25.498615 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 22:58:25.498616 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 22:58:25.498618 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 22:58:25.498621 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 22:58:25.498625 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 22:58:25.498757 13090 solver.cpp:467] Iteration 18000, Testing net (#0)
I0711 22:59:18.816855 13090 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.949302
I0711 22:59:18.816928 13090 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.99915
I0711 22:59:18.816936 13090 solver.cpp:540]     Test net output #2: loss = 0.175334 (* 1 = 0.175334 loss)
I0711 22:59:19.006059 13090 solver.cpp:290] Iteration 18000 (1.42156 iter/s, 70.3455s/100 iter), loss = 0.027849
I0711 22:59:19.006083 13090 solver.cpp:309]     Train net output #0: loss = 0.0278491 (* 1 = 0.0278491 loss)
I0711 22:59:19.006089 13090 sgd_solver.cpp:106] Iteration 18000, lr = 1e-05
I0711 22:59:35.817966 13090 solver.cpp:290] Iteration 18100 (5.94834 iter/s, 16.8114s/100 iter), loss = 0.0374286
I0711 22:59:35.817991 13090 solver.cpp:309]     Train net output #0: loss = 0.0374287 (* 1 = 0.0374287 loss)
I0711 22:59:35.817999 13090 sgd_solver.cpp:106] Iteration 18100, lr = 1e-05
I0711 22:59:52.746487 13090 solver.cpp:290] Iteration 18200 (5.90736 iter/s, 16.928s/100 iter), loss = 0.0305814
I0711 22:59:52.746574 13090 solver.cpp:309]     Train net output #0: loss = 0.0305815 (* 1 = 0.0305815 loss)
I0711 22:59:52.746587 13090 sgd_solver.cpp:106] Iteration 18200, lr = 1e-05
I0711 23:00:09.785383 13090 solver.cpp:290] Iteration 18300 (5.86911 iter/s, 17.0383s/100 iter), loss = 0.0303097
I0711 23:00:09.785406 13090 solver.cpp:309]     Train net output #0: loss = 0.0303098 (* 1 = 0.0303098 loss)
I0711 23:00:09.785413 13090 sgd_solver.cpp:106] Iteration 18300, lr = 1e-05
I0711 23:00:26.856071 13090 solver.cpp:290] Iteration 18400 (5.85816 iter/s, 17.0702s/100 iter), loss = 0.0278692
I0711 23:00:26.856153 13090 solver.cpp:309]     Train net output #0: loss = 0.0278693 (* 1 = 0.0278693 loss)
I0711 23:00:26.856160 13090 sgd_solver.cpp:106] Iteration 18400, lr = 1e-05
I0711 23:00:44.009124 13090 solver.cpp:290] Iteration 18500 (5.83005 iter/s, 17.1525s/100 iter), loss = 0.0392567
I0711 23:00:44.009147 13090 solver.cpp:309]     Train net output #0: loss = 0.0392568 (* 1 = 0.0392568 loss)
I0711 23:00:44.009155 13090 sgd_solver.cpp:106] Iteration 18500, lr = 1e-05
I0711 23:01:01.275208 13090 solver.cpp:290] Iteration 18600 (5.79187 iter/s, 17.2656s/100 iter), loss = 0.0218392
I0711 23:01:01.275259 13090 solver.cpp:309]     Train net output #0: loss = 0.0218394 (* 1 = 0.0218394 loss)
I0711 23:01:01.275266 13090 sgd_solver.cpp:106] Iteration 18600, lr = 1e-05
I0711 23:01:18.279551 13090 solver.cpp:290] Iteration 18700 (5.88103 iter/s, 17.0038s/100 iter), loss = 0.0304103
I0711 23:01:18.279606 13090 solver.cpp:309]     Train net output #0: loss = 0.0304104 (* 1 = 0.0304104 loss)
I0711 23:01:18.279623 13090 sgd_solver.cpp:106] Iteration 18700, lr = 1e-05
I0711 23:01:35.372611 13090 solver.cpp:290] Iteration 18800 (5.8505 iter/s, 17.0925s/100 iter), loss = 0.0253967
I0711 23:01:35.372655 13090 solver.cpp:309]     Train net output #0: loss = 0.0253968 (* 1 = 0.0253968 loss)
I0711 23:01:35.372663 13090 sgd_solver.cpp:106] Iteration 18800, lr = 1e-05
I0711 23:01:52.502574 13090 solver.cpp:290] Iteration 18900 (5.8379 iter/s, 17.1295s/100 iter), loss = 0.0326898
I0711 23:01:52.502602 13090 solver.cpp:309]     Train net output #0: loss = 0.0326899 (* 1 = 0.0326899 loss)
I0711 23:01:52.502611 13090 sgd_solver.cpp:106] Iteration 18900, lr = 1e-05
I0711 23:02:09.450700 13090 solver.cpp:354] Sparsity after update:
I0711 23:02:09.508078 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 23:02:09.508103 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 23:02:09.508116 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 23:02:09.508121 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 23:02:09.508127 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 23:02:09.508132 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 23:02:09.508138 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 23:02:09.508144 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 23:02:09.508149 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 23:02:09.508155 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 23:02:09.508160 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 23:02:09.508164 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 23:02:09.508169 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 23:02:09.508173 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 23:02:09.508178 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 23:02:09.508183 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 23:02:09.508188 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 23:02:09.508191 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 23:02:09.508196 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 23:02:09.660467 13090 solver.cpp:290] Iteration 19000 (5.82839 iter/s, 17.1574s/100 iter), loss = 0.0354918
I0711 23:02:09.660497 13090 solver.cpp:309]     Train net output #0: loss = 0.035492 (* 1 = 0.035492 loss)
I0711 23:02:09.660504 13090 sgd_solver.cpp:106] Iteration 19000, lr = 1e-05
I0711 23:02:28.087865 13090 solver.cpp:290] Iteration 19100 (5.42686 iter/s, 18.4269s/100 iter), loss = 0.0274867
I0711 23:02:28.087890 13090 solver.cpp:309]     Train net output #0: loss = 0.0274868 (* 1 = 0.0274868 loss)
I0711 23:02:28.087898 13090 sgd_solver.cpp:106] Iteration 19100, lr = 1e-05
I0711 23:02:46.305212 13090 solver.cpp:290] Iteration 19200 (5.48943 iter/s, 18.2168s/100 iter), loss = 0.0236527
I0711 23:02:46.305338 13090 solver.cpp:309]     Train net output #0: loss = 0.0236529 (* 1 = 0.0236529 loss)
I0711 23:02:46.305450 13090 sgd_solver.cpp:106] Iteration 19200, lr = 1e-05
I0711 23:03:04.231107 13090 solver.cpp:290] Iteration 19300 (5.57871 iter/s, 17.9253s/100 iter), loss = 0.0497121
I0711 23:03:04.231130 13090 solver.cpp:309]     Train net output #0: loss = 0.0497122 (* 1 = 0.0497122 loss)
I0711 23:03:04.231137 13090 sgd_solver.cpp:106] Iteration 19300, lr = 1e-05
I0711 23:03:21.875320 13090 solver.cpp:290] Iteration 19400 (5.66774 iter/s, 17.6437s/100 iter), loss = 0.0342339
I0711 23:03:21.875412 13090 solver.cpp:309]     Train net output #0: loss = 0.0342341 (* 1 = 0.0342341 loss)
I0711 23:03:21.875430 13090 sgd_solver.cpp:106] Iteration 19400, lr = 1e-05
I0711 23:03:40.174504 13090 solver.cpp:290] Iteration 19500 (5.4649 iter/s, 18.2986s/100 iter), loss = 0.037036
I0711 23:03:40.174526 13090 solver.cpp:309]     Train net output #0: loss = 0.0370362 (* 1 = 0.0370362 loss)
I0711 23:03:40.174533 13090 sgd_solver.cpp:106] Iteration 19500, lr = 1e-05
I0711 23:03:58.789693 13090 solver.cpp:290] Iteration 19600 (5.37211 iter/s, 18.6147s/100 iter), loss = 0.0163714
I0711 23:03:58.789777 13090 solver.cpp:309]     Train net output #0: loss = 0.0163716 (* 1 = 0.0163716 loss)
I0711 23:03:58.789795 13090 sgd_solver.cpp:106] Iteration 19600, lr = 1e-05
I0711 23:04:16.865332 13090 solver.cpp:290] Iteration 19700 (5.53248 iter/s, 18.0751s/100 iter), loss = 0.0211029
I0711 23:04:16.865356 13090 solver.cpp:309]     Train net output #0: loss = 0.0211031 (* 1 = 0.0211031 loss)
I0711 23:04:16.865362 13090 sgd_solver.cpp:106] Iteration 19700, lr = 1e-05
I0711 23:04:34.555263 13090 solver.cpp:290] Iteration 19800 (5.65309 iter/s, 17.6894s/100 iter), loss = 0.0204564
I0711 23:04:34.555311 13090 solver.cpp:309]     Train net output #0: loss = 0.0204565 (* 1 = 0.0204565 loss)
I0711 23:04:34.555320 13090 sgd_solver.cpp:106] Iteration 19800, lr = 1e-05
I0711 23:04:52.722084 13090 solver.cpp:290] Iteration 19900 (5.5047 iter/s, 18.1663s/100 iter), loss = 0.035623
I0711 23:04:52.722108 13090 solver.cpp:309]     Train net output #0: loss = 0.0356232 (* 1 = 0.0356232 loss)
I0711 23:04:52.722115 13090 sgd_solver.cpp:106] Iteration 19900, lr = 1e-05
I0711 23:05:10.616458 13090 solver.cpp:594] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/sparse/cityscapes5_jsegnet21v2_iter_20000.caffemodel
I0711 23:05:10.658710 13090 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/sparse/cityscapes5_jsegnet21v2_iter_20000.solverstate
I0711 23:05:10.675606 13090 solver.cpp:354] Sparsity after update:
I0711 23:05:10.677319 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 23:05:10.677330 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 23:05:10.677340 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 23:05:10.677345 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 23:05:10.677350 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 23:05:10.677355 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 23:05:10.677358 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 23:05:10.677362 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 23:05:10.677366 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 23:05:10.677371 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 23:05:10.677374 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 23:05:10.677378 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 23:05:10.677382 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 23:05:10.677386 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 23:05:10.677389 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 23:05:10.677393 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 23:05:10.677397 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 23:05:10.677402 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 23:05:10.677405 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 23:05:10.677561 13090 solver.cpp:467] Iteration 20000, Testing net (#0)
I0711 23:06:14.682797 13090 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.950321
I0711 23:06:14.682927 13090 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999324
I0711 23:06:14.682937 13090 solver.cpp:540]     Test net output #2: loss = 0.1677 (* 1 = 0.1677 loss)
I0711 23:06:14.880645 13090 solver.cpp:290] Iteration 20000 (1.21719 iter/s, 82.1563s/100 iter), loss = 0.037898
I0711 23:06:14.880692 13090 solver.cpp:309]     Train net output #0: loss = 0.0378981 (* 1 = 0.0378981 loss)
I0711 23:06:14.880703 13090 sgd_solver.cpp:106] Iteration 20000, lr = 1e-05
I0711 23:06:32.951225 13090 solver.cpp:290] Iteration 20100 (5.53402 iter/s, 18.07s/100 iter), loss = 0.0398808
I0711 23:06:32.951267 13090 solver.cpp:309]     Train net output #0: loss = 0.039881 (* 1 = 0.039881 loss)
I0711 23:06:32.951277 13090 sgd_solver.cpp:106] Iteration 20100, lr = 1e-05
I0711 23:06:50.827599 13090 solver.cpp:290] Iteration 20200 (5.59414 iter/s, 17.8758s/100 iter), loss = 0.0194085
I0711 23:06:50.827678 13090 solver.cpp:309]     Train net output #0: loss = 0.0194087 (* 1 = 0.0194087 loss)
I0711 23:06:50.827687 13090 sgd_solver.cpp:106] Iteration 20200, lr = 1e-05
I0711 23:07:08.134421 13090 solver.cpp:290] Iteration 20300 (5.77825 iter/s, 17.3063s/100 iter), loss = 0.0191318
I0711 23:07:08.134443 13090 solver.cpp:309]     Train net output #0: loss = 0.019132 (* 1 = 0.019132 loss)
I0711 23:07:08.134450 13090 sgd_solver.cpp:106] Iteration 20300, lr = 1e-05
I0711 23:07:25.344804 13090 solver.cpp:290] Iteration 20400 (5.81061 iter/s, 17.2099s/100 iter), loss = 0.0533764
I0711 23:07:25.344897 13090 solver.cpp:309]     Train net output #0: loss = 0.0533765 (* 1 = 0.0533765 loss)
I0711 23:07:25.344907 13090 sgd_solver.cpp:106] Iteration 20400, lr = 1e-05
I0711 23:07:42.323738 13090 solver.cpp:290] Iteration 20500 (5.88984 iter/s, 16.9784s/100 iter), loss = 0.0484974
I0711 23:07:42.323765 13090 solver.cpp:309]     Train net output #0: loss = 0.0484975 (* 1 = 0.0484975 loss)
I0711 23:07:42.323774 13090 sgd_solver.cpp:106] Iteration 20500, lr = 1e-05
I0711 23:07:59.350783 13090 solver.cpp:290] Iteration 20600 (5.87318 iter/s, 17.0266s/100 iter), loss = 0.0279792
I0711 23:07:59.350842 13090 solver.cpp:309]     Train net output #0: loss = 0.0279793 (* 1 = 0.0279793 loss)
I0711 23:07:59.350853 13090 sgd_solver.cpp:106] Iteration 20600, lr = 1e-05
I0711 23:08:16.512347 13090 solver.cpp:290] Iteration 20700 (5.82715 iter/s, 17.161s/100 iter), loss = 0.0386273
I0711 23:08:16.512377 13090 solver.cpp:309]     Train net output #0: loss = 0.0386275 (* 1 = 0.0386275 loss)
I0711 23:08:16.512385 13090 sgd_solver.cpp:106] Iteration 20700, lr = 1e-05
I0711 23:08:33.437607 13090 solver.cpp:290] Iteration 20800 (5.9085 iter/s, 16.9248s/100 iter), loss = 0.0216204
I0711 23:08:33.437660 13090 solver.cpp:309]     Train net output #0: loss = 0.0216206 (* 1 = 0.0216206 loss)
I0711 23:08:33.437667 13090 sgd_solver.cpp:106] Iteration 20800, lr = 1e-05
I0711 23:08:50.463656 13090 solver.cpp:290] Iteration 20900 (5.87353 iter/s, 17.0255s/100 iter), loss = 0.0218961
I0711 23:08:50.463682 13090 solver.cpp:309]     Train net output #0: loss = 0.0218962 (* 1 = 0.0218962 loss)
I0711 23:08:50.463691 13090 sgd_solver.cpp:106] Iteration 20900, lr = 1e-05
I0711 23:09:07.311870 13090 solver.cpp:354] Sparsity after update:
I0711 23:09:07.362685 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 23:09:07.362701 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 23:09:07.362709 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 23:09:07.362711 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 23:09:07.362713 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 23:09:07.362715 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 23:09:07.362716 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 23:09:07.362718 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 23:09:07.362720 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 23:09:07.362722 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 23:09:07.362725 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 23:09:07.362728 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 23:09:07.362730 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 23:09:07.362733 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 23:09:07.362738 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 23:09:07.362742 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 23:09:07.362746 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 23:09:07.362751 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 23:09:07.362754 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 23:09:07.513600 13090 solver.cpp:290] Iteration 21000 (5.86529 iter/s, 17.0495s/100 iter), loss = 0.0455522
I0711 23:09:07.513623 13090 solver.cpp:309]     Train net output #0: loss = 0.0455523 (* 1 = 0.0455523 loss)
I0711 23:09:07.513630 13090 sgd_solver.cpp:106] Iteration 21000, lr = 1e-05
I0711 23:09:24.592250 13090 solver.cpp:290] Iteration 21100 (5.85543 iter/s, 17.0782s/100 iter), loss = 0.0243547
I0711 23:09:24.592275 13090 solver.cpp:309]     Train net output #0: loss = 0.0243548 (* 1 = 0.0243548 loss)
I0711 23:09:24.592283 13090 sgd_solver.cpp:106] Iteration 21100, lr = 1e-05
I0711 23:09:41.469256 13090 solver.cpp:290] Iteration 21200 (5.92539 iter/s, 16.8765s/100 iter), loss = 0.0411851
I0711 23:09:41.469307 13090 solver.cpp:309]     Train net output #0: loss = 0.0411852 (* 1 = 0.0411852 loss)
I0711 23:09:41.469316 13090 sgd_solver.cpp:106] Iteration 21200, lr = 1e-05
I0711 23:09:58.532106 13090 solver.cpp:290] Iteration 21300 (5.86086 iter/s, 17.0623s/100 iter), loss = 0.0237201
I0711 23:09:58.532130 13090 solver.cpp:309]     Train net output #0: loss = 0.0237203 (* 1 = 0.0237203 loss)
I0711 23:09:58.532137 13090 sgd_solver.cpp:106] Iteration 21300, lr = 1e-05
I0711 23:10:15.476426 13090 solver.cpp:290] Iteration 21400 (5.90185 iter/s, 16.9438s/100 iter), loss = 0.0320165
I0711 23:10:15.476475 13090 solver.cpp:309]     Train net output #0: loss = 0.0320166 (* 1 = 0.0320166 loss)
I0711 23:10:15.476485 13090 sgd_solver.cpp:106] Iteration 21400, lr = 1e-05
I0711 23:10:33.527720 13090 solver.cpp:290] Iteration 21500 (5.53993 iter/s, 18.0508s/100 iter), loss = 0.0316272
I0711 23:10:33.527765 13090 solver.cpp:309]     Train net output #0: loss = 0.0316274 (* 1 = 0.0316274 loss)
I0711 23:10:33.527791 13090 sgd_solver.cpp:106] Iteration 21500, lr = 1e-05
I0711 23:10:50.987340 13090 solver.cpp:290] Iteration 21600 (5.72767 iter/s, 17.4591s/100 iter), loss = 0.0360489
I0711 23:10:50.987391 13090 solver.cpp:309]     Train net output #0: loss = 0.0360491 (* 1 = 0.0360491 loss)
I0711 23:10:50.987399 13090 sgd_solver.cpp:106] Iteration 21600, lr = 1e-05
I0711 23:11:08.018837 13090 solver.cpp:290] Iteration 21700 (5.87165 iter/s, 17.031s/100 iter), loss = 0.0355756
I0711 23:11:08.018867 13090 solver.cpp:309]     Train net output #0: loss = 0.0355757 (* 1 = 0.0355757 loss)
I0711 23:11:08.018878 13090 sgd_solver.cpp:106] Iteration 21700, lr = 1e-05
I0711 23:11:25.096484 13090 solver.cpp:290] Iteration 21800 (5.85578 iter/s, 17.0772s/100 iter), loss = 0.0229224
I0711 23:11:25.096541 13090 solver.cpp:309]     Train net output #0: loss = 0.0229226 (* 1 = 0.0229226 loss)
I0711 23:11:25.096552 13090 sgd_solver.cpp:106] Iteration 21800, lr = 1e-05
I0711 23:11:42.257591 13090 solver.cpp:290] Iteration 21900 (5.82731 iter/s, 17.1606s/100 iter), loss = 0.020808
I0711 23:11:42.257616 13090 solver.cpp:309]     Train net output #0: loss = 0.0208081 (* 1 = 0.0208081 loss)
I0711 23:11:42.257625 13090 sgd_solver.cpp:106] Iteration 21900, lr = 1e-05
I0711 23:11:58.985618 13090 solver.cpp:354] Sparsity after update:
I0711 23:11:58.988215 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 23:11:58.988231 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 23:11:58.988242 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 23:11:58.988247 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 23:11:58.988251 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 23:11:58.988256 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 23:11:58.988260 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 23:11:58.988265 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 23:11:58.988268 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 23:11:58.988272 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 23:11:58.988276 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 23:11:58.988281 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 23:11:58.988286 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 23:11:58.988289 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 23:11:58.988293 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 23:11:58.988297 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 23:11:58.988301 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 23:11:58.988304 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 23:11:58.988307 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 23:11:58.988507 13090 solver.cpp:467] Iteration 22000, Testing net (#0)
I0711 23:12:45.359262 13090 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.950097
I0711 23:12:45.359356 13090 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999519
I0711 23:12:45.359364 13090 solver.cpp:540]     Test net output #2: loss = 0.162282 (* 1 = 0.162282 loss)
I0711 23:12:45.543015 13090 solver.cpp:290] Iteration 22000 (1.58019 iter/s, 63.2837s/100 iter), loss = 0.0368986
I0711 23:12:45.543041 13090 solver.cpp:309]     Train net output #0: loss = 0.0368987 (* 1 = 0.0368987 loss)
I0711 23:12:45.543048 13090 sgd_solver.cpp:106] Iteration 22000, lr = 1e-05
I0711 23:13:03.499867 13090 solver.cpp:290] Iteration 22100 (5.56907 iter/s, 17.9563s/100 iter), loss = 0.0260617
I0711 23:13:03.499923 13090 solver.cpp:309]     Train net output #0: loss = 0.0260618 (* 1 = 0.0260618 loss)
I0711 23:13:03.499948 13090 sgd_solver.cpp:106] Iteration 22100, lr = 1e-05
I0711 23:13:21.806524 13090 solver.cpp:290] Iteration 22200 (5.46266 iter/s, 18.3061s/100 iter), loss = 0.0320386
I0711 23:13:21.806612 13090 solver.cpp:309]     Train net output #0: loss = 0.0320387 (* 1 = 0.0320387 loss)
I0711 23:13:21.806628 13090 sgd_solver.cpp:106] Iteration 22200, lr = 1e-05
I0711 23:13:40.094108 13090 solver.cpp:290] Iteration 22300 (5.46836 iter/s, 18.287s/100 iter), loss = 0.0317626
I0711 23:13:40.094154 13090 solver.cpp:309]     Train net output #0: loss = 0.0317627 (* 1 = 0.0317627 loss)
I0711 23:13:40.094179 13090 sgd_solver.cpp:106] Iteration 22300, lr = 1e-05
I0711 23:13:57.354511 13090 solver.cpp:290] Iteration 22400 (5.79378 iter/s, 17.2599s/100 iter), loss = 0.0256372
I0711 23:13:57.354554 13090 solver.cpp:309]     Train net output #0: loss = 0.0256373 (* 1 = 0.0256373 loss)
I0711 23:13:57.354564 13090 sgd_solver.cpp:106] Iteration 22400, lr = 1e-05
I0711 23:14:14.436930 13090 solver.cpp:290] Iteration 22500 (5.85415 iter/s, 17.0819s/100 iter), loss = 0.0295089
I0711 23:14:14.436954 13090 solver.cpp:309]     Train net output #0: loss = 0.029509 (* 1 = 0.029509 loss)
I0711 23:14:14.436960 13090 sgd_solver.cpp:106] Iteration 22500, lr = 1e-05
I0711 23:14:31.509920 13090 solver.cpp:290] Iteration 22600 (5.85737 iter/s, 17.0725s/100 iter), loss = 0.0378212
I0711 23:14:31.510026 13090 solver.cpp:309]     Train net output #0: loss = 0.0378213 (* 1 = 0.0378213 loss)
I0711 23:14:31.510036 13090 sgd_solver.cpp:106] Iteration 22600, lr = 1e-05
I0711 23:14:48.512086 13090 solver.cpp:290] Iteration 22700 (5.8818 iter/s, 17.0016s/100 iter), loss = 0.0250482
I0711 23:14:48.512117 13090 solver.cpp:309]     Train net output #0: loss = 0.0250483 (* 1 = 0.0250483 loss)
I0711 23:14:48.512127 13090 sgd_solver.cpp:106] Iteration 22700, lr = 1e-05
I0711 23:15:05.606510 13090 solver.cpp:290] Iteration 22800 (5.85003 iter/s, 17.0939s/100 iter), loss = 0.0296388
I0711 23:15:05.606621 13090 solver.cpp:309]     Train net output #0: loss = 0.0296389 (* 1 = 0.0296389 loss)
I0711 23:15:05.606631 13090 sgd_solver.cpp:106] Iteration 22800, lr = 1e-05
I0711 23:15:22.676869 13090 solver.cpp:290] Iteration 22900 (5.8583 iter/s, 17.0698s/100 iter), loss = 0.0215033
I0711 23:15:22.676893 13090 solver.cpp:309]     Train net output #0: loss = 0.0215034 (* 1 = 0.0215034 loss)
I0711 23:15:22.676900 13090 sgd_solver.cpp:106] Iteration 22900, lr = 1e-05
I0711 23:15:39.435776 13090 solver.cpp:354] Sparsity after update:
I0711 23:15:39.497758 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 23:15:39.497773 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 23:15:39.497781 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 23:15:39.497783 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 23:15:39.497786 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 23:15:39.497787 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 23:15:39.497789 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 23:15:39.497792 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 23:15:39.497793 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 23:15:39.497797 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 23:15:39.497798 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 23:15:39.497800 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 23:15:39.497803 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 23:15:39.497805 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 23:15:39.497807 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 23:15:39.497809 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 23:15:39.497812 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 23:15:39.497814 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 23:15:39.497817 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 23:15:39.649562 13090 solver.cpp:290] Iteration 23000 (5.89198 iter/s, 16.9722s/100 iter), loss = 0.0291
I0711 23:15:39.649588 13090 solver.cpp:309]     Train net output #0: loss = 0.0291001 (* 1 = 0.0291001 loss)
I0711 23:15:39.649595 13090 sgd_solver.cpp:106] Iteration 23000, lr = 1e-05
I0711 23:15:56.861435 13090 solver.cpp:290] Iteration 23100 (5.81011 iter/s, 17.2114s/100 iter), loss = 0.0297656
I0711 23:15:56.861461 13090 solver.cpp:309]     Train net output #0: loss = 0.0297658 (* 1 = 0.0297658 loss)
I0711 23:15:56.861470 13090 sgd_solver.cpp:106] Iteration 23100, lr = 1e-05
I0711 23:16:13.843976 13090 solver.cpp:290] Iteration 23200 (5.88857 iter/s, 16.9821s/100 iter), loss = 0.0218417
I0711 23:16:13.844053 13090 solver.cpp:309]     Train net output #0: loss = 0.0218418 (* 1 = 0.0218418 loss)
I0711 23:16:13.844061 13090 sgd_solver.cpp:106] Iteration 23200, lr = 1e-05
I0711 23:16:31.496510 13090 solver.cpp:290] Iteration 23300 (5.66509 iter/s, 17.652s/100 iter), loss = 0.0204261
I0711 23:16:31.496554 13090 solver.cpp:309]     Train net output #0: loss = 0.0204263 (* 1 = 0.0204263 loss)
I0711 23:16:31.496579 13090 sgd_solver.cpp:106] Iteration 23300, lr = 1e-05
I0711 23:16:48.598891 13090 solver.cpp:290] Iteration 23400 (5.84731 iter/s, 17.1019s/100 iter), loss = 0.0331327
I0711 23:16:48.598942 13090 solver.cpp:309]     Train net output #0: loss = 0.0331328 (* 1 = 0.0331328 loss)
I0711 23:16:48.598948 13090 sgd_solver.cpp:106] Iteration 23400, lr = 1e-05
I0711 23:17:05.621903 13090 solver.cpp:290] Iteration 23500 (5.87458 iter/s, 17.0225s/100 iter), loss = 0.0242694
I0711 23:17:05.621928 13090 solver.cpp:309]     Train net output #0: loss = 0.0242695 (* 1 = 0.0242695 loss)
I0711 23:17:05.621935 13090 sgd_solver.cpp:106] Iteration 23500, lr = 1e-05
I0711 23:17:22.564499 13090 solver.cpp:290] Iteration 23600 (5.90245 iter/s, 16.9421s/100 iter), loss = 0.0370463
I0711 23:17:22.564574 13090 solver.cpp:309]     Train net output #0: loss = 0.0370464 (* 1 = 0.0370464 loss)
I0711 23:17:22.564585 13090 sgd_solver.cpp:106] Iteration 23600, lr = 1e-05
I0711 23:17:39.618728 13090 solver.cpp:290] Iteration 23700 (5.86383 iter/s, 17.0537s/100 iter), loss = 0.0310658
I0711 23:17:39.618752 13090 solver.cpp:309]     Train net output #0: loss = 0.0310659 (* 1 = 0.0310659 loss)
I0711 23:17:39.618758 13090 sgd_solver.cpp:106] Iteration 23700, lr = 1e-05
I0711 23:17:56.691367 13090 solver.cpp:290] Iteration 23800 (5.85749 iter/s, 17.0722s/100 iter), loss = 0.0329062
I0711 23:17:56.691419 13090 solver.cpp:309]     Train net output #0: loss = 0.0329063 (* 1 = 0.0329063 loss)
I0711 23:17:56.691428 13090 sgd_solver.cpp:106] Iteration 23800, lr = 1e-05
I0711 23:18:13.681408 13090 solver.cpp:290] Iteration 23900 (5.88598 iter/s, 16.9895s/100 iter), loss = 0.0215785
I0711 23:18:13.681432 13090 solver.cpp:309]     Train net output #0: loss = 0.0215786 (* 1 = 0.0215786 loss)
I0711 23:18:13.681439 13090 sgd_solver.cpp:106] Iteration 23900, lr = 1e-05
I0711 23:18:30.499063 13090 solver.cpp:354] Sparsity after update:
I0711 23:18:30.501225 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 23:18:30.501235 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 23:18:30.501240 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 23:18:30.501242 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 23:18:30.501245 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 23:18:30.501246 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 23:18:30.501248 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 23:18:30.501250 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 23:18:30.501252 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 23:18:30.501255 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 23:18:30.501256 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 23:18:30.501258 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 23:18:30.501260 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 23:18:30.501262 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 23:18:30.501263 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 23:18:30.501266 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 23:18:30.501267 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 23:18:30.501269 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 23:18:30.501271 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 23:18:30.501405 13090 solver.cpp:467] Iteration 24000, Testing net (#0)
I0711 23:19:17.032886 13090 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.949969
I0711 23:19:17.032956 13090 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999499
I0711 23:19:17.032964 13090 solver.cpp:540]     Test net output #2: loss = 0.163111 (* 1 = 0.163111 loss)
I0711 23:19:17.232530 13090 solver.cpp:290] Iteration 24000 (1.57358 iter/s, 63.5494s/100 iter), loss = 0.0264291
I0711 23:19:17.232558 13090 solver.cpp:309]     Train net output #0: loss = 0.0264292 (* 1 = 0.0264292 loss)
I0711 23:19:17.232560 13271 sgd_solver.cpp:46] MultiStep Status: Iteration 24000, step = 1
I0711 23:19:17.232560 13270 sgd_solver.cpp:46] MultiStep Status: Iteration 24000, step = 1
I0711 23:19:17.232568 13090 sgd_solver.cpp:46] MultiStep Status: Iteration 24000, step = 1
I0711 23:19:17.232573 13090 sgd_solver.cpp:106] Iteration 24000, lr = 1e-06
I0711 23:19:34.317178 13090 solver.cpp:290] Iteration 24100 (5.85338 iter/s, 17.0842s/100 iter), loss = 0.0218588
I0711 23:19:34.317206 13090 solver.cpp:309]     Train net output #0: loss = 0.0218589 (* 1 = 0.0218589 loss)
I0711 23:19:34.317215 13090 sgd_solver.cpp:106] Iteration 24100, lr = 1e-06
I0711 23:19:51.253504 13090 solver.cpp:290] Iteration 24200 (5.90464 iter/s, 16.9358s/100 iter), loss = 0.0458334
I0711 23:19:51.253612 13090 solver.cpp:309]     Train net output #0: loss = 0.0458335 (* 1 = 0.0458335 loss)
I0711 23:19:51.253624 13090 sgd_solver.cpp:106] Iteration 24200, lr = 1e-06
I0711 23:20:09.305176 13090 solver.cpp:290] Iteration 24300 (5.53984 iter/s, 18.0511s/100 iter), loss = 0.0334719
I0711 23:20:09.305222 13090 solver.cpp:309]     Train net output #0: loss = 0.033472 (* 1 = 0.033472 loss)
I0711 23:20:09.305234 13090 sgd_solver.cpp:106] Iteration 24300, lr = 1e-06
I0711 23:20:27.510360 13090 solver.cpp:290] Iteration 24400 (5.4931 iter/s, 18.2046s/100 iter), loss = 0.0139647
I0711 23:20:27.510491 13090 solver.cpp:309]     Train net output #0: loss = 0.0139649 (* 1 = 0.0139649 loss)
I0711 23:20:27.510501 13090 sgd_solver.cpp:106] Iteration 24400, lr = 1e-06
I0711 23:20:45.625725 13090 solver.cpp:290] Iteration 24500 (5.52036 iter/s, 18.1147s/100 iter), loss = 0.0282061
I0711 23:20:45.625752 13090 solver.cpp:309]     Train net output #0: loss = 0.0282062 (* 1 = 0.0282062 loss)
I0711 23:20:45.625761 13090 sgd_solver.cpp:106] Iteration 24500, lr = 1e-06
I0711 23:21:03.028956 13090 solver.cpp:290] Iteration 24600 (5.74623 iter/s, 17.4027s/100 iter), loss = 0.034823
I0711 23:21:03.029048 13090 solver.cpp:309]     Train net output #0: loss = 0.0348231 (* 1 = 0.0348231 loss)
I0711 23:21:03.029078 13090 sgd_solver.cpp:106] Iteration 24600, lr = 1e-06
I0711 23:21:21.306694 13090 solver.cpp:290] Iteration 24700 (5.47131 iter/s, 18.2772s/100 iter), loss = 0.0367567
I0711 23:21:21.306738 13090 solver.cpp:309]     Train net output #0: loss = 0.0367568 (* 1 = 0.0367568 loss)
I0711 23:21:21.306751 13090 sgd_solver.cpp:106] Iteration 24700, lr = 1e-06
I0711 23:21:39.351783 13090 solver.cpp:290] Iteration 24800 (5.54184 iter/s, 18.0446s/100 iter), loss = 0.0218172
I0711 23:21:39.372876 13090 solver.cpp:309]     Train net output #0: loss = 0.0218173 (* 1 = 0.0218173 loss)
I0711 23:21:39.372927 13090 sgd_solver.cpp:106] Iteration 24800, lr = 1e-06
I0711 23:21:57.258693 13090 solver.cpp:290] Iteration 24900 (5.59117 iter/s, 17.8854s/100 iter), loss = 0.0390552
I0711 23:21:57.258723 13090 solver.cpp:309]     Train net output #0: loss = 0.0390553 (* 1 = 0.0390553 loss)
I0711 23:21:57.258733 13090 sgd_solver.cpp:106] Iteration 24900, lr = 1e-06
I0711 23:22:14.543519 13090 solver.cpp:354] Sparsity after update:
I0711 23:22:14.566345 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 23:22:14.566458 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 23:22:14.566498 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 23:22:14.566506 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 23:22:14.566515 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 23:22:14.566522 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 23:22:14.566530 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 23:22:14.566539 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 23:22:14.566546 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 23:22:14.566555 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 23:22:14.566562 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 23:22:14.566570 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 23:22:14.566578 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 23:22:14.566586 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 23:22:14.566593 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 23:22:14.566601 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 23:22:14.566608 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 23:22:14.566617 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 23:22:14.566624 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 23:22:14.727476 13090 solver.cpp:290] Iteration 25000 (5.72467 iter/s, 17.4683s/100 iter), loss = 0.0303252
I0711 23:22:14.727605 13090 solver.cpp:309]     Train net output #0: loss = 0.0303253 (* 1 = 0.0303253 loss)
I0711 23:22:14.727680 13090 sgd_solver.cpp:106] Iteration 25000, lr = 1e-06
I0711 23:22:32.863739 13090 solver.cpp:290] Iteration 25100 (5.514 iter/s, 18.1357s/100 iter), loss = 0.0271073
I0711 23:22:32.863796 13090 solver.cpp:309]     Train net output #0: loss = 0.0271074 (* 1 = 0.0271074 loss)
I0711 23:22:32.863821 13090 sgd_solver.cpp:106] Iteration 25100, lr = 1e-06
I0711 23:22:51.097621 13090 solver.cpp:290] Iteration 25200 (5.48447 iter/s, 18.2333s/100 iter), loss = 0.0681001
I0711 23:22:51.097911 13090 solver.cpp:309]     Train net output #0: loss = 0.0681002 (* 1 = 0.0681002 loss)
I0711 23:22:51.097951 13090 sgd_solver.cpp:106] Iteration 25200, lr = 1e-06
I0711 23:23:09.067803 13090 solver.cpp:290] Iteration 25300 (5.56501 iter/s, 17.9694s/100 iter), loss = 0.0246263
I0711 23:23:09.067826 13090 solver.cpp:309]     Train net output #0: loss = 0.0246265 (* 1 = 0.0246265 loss)
I0711 23:23:09.067833 13090 sgd_solver.cpp:106] Iteration 25300, lr = 1e-06
I0711 23:23:26.473212 13090 solver.cpp:290] Iteration 25400 (5.7455 iter/s, 17.4049s/100 iter), loss = 0.0358568
I0711 23:23:26.473266 13090 solver.cpp:309]     Train net output #0: loss = 0.0358569 (* 1 = 0.0358569 loss)
I0711 23:23:26.473274 13090 sgd_solver.cpp:106] Iteration 25400, lr = 1e-06
I0711 23:23:44.578817 13090 solver.cpp:290] Iteration 25500 (5.52332 iter/s, 18.1051s/100 iter), loss = 0.0235712
I0711 23:23:44.578842 13090 solver.cpp:309]     Train net output #0: loss = 0.0235713 (* 1 = 0.0235713 loss)
I0711 23:23:44.578850 13090 sgd_solver.cpp:106] Iteration 25500, lr = 1e-06
I0711 23:24:02.645995 13090 solver.cpp:290] Iteration 25600 (5.53506 iter/s, 18.0667s/100 iter), loss = 0.072245
I0711 23:24:02.646100 13090 solver.cpp:309]     Train net output #0: loss = 0.0722451 (* 1 = 0.0722451 loss)
I0711 23:24:02.646109 13090 sgd_solver.cpp:106] Iteration 25600, lr = 1e-06
I0711 23:24:20.906924 13090 solver.cpp:290] Iteration 25700 (5.47635 iter/s, 18.2603s/100 iter), loss = 0.032931
I0711 23:24:20.906949 13090 solver.cpp:309]     Train net output #0: loss = 0.0329311 (* 1 = 0.0329311 loss)
I0711 23:24:20.906958 13090 sgd_solver.cpp:106] Iteration 25700, lr = 1e-06
I0711 23:24:38.227826 13090 solver.cpp:290] Iteration 25800 (5.77354 iter/s, 17.3204s/100 iter), loss = 0.0377947
I0711 23:24:38.227993 13090 solver.cpp:309]     Train net output #0: loss = 0.0377949 (* 1 = 0.0377949 loss)
I0711 23:24:38.228006 13090 sgd_solver.cpp:106] Iteration 25800, lr = 1e-06
I0711 23:24:56.209313 13090 solver.cpp:290] Iteration 25900 (5.56148 iter/s, 17.9808s/100 iter), loss = 0.0207783
I0711 23:24:56.209339 13090 solver.cpp:309]     Train net output #0: loss = 0.0207784 (* 1 = 0.0207784 loss)
I0711 23:24:56.209347 13090 sgd_solver.cpp:106] Iteration 25900, lr = 1e-06
I0711 23:25:14.464498 13090 solver.cpp:354] Sparsity after update:
I0711 23:25:14.466426 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 23:25:14.466435 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 23:25:14.466442 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 23:25:14.466444 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 23:25:14.466446 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 23:25:14.466449 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 23:25:14.466450 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 23:25:14.466452 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 23:25:14.466454 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 23:25:14.466456 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 23:25:14.466459 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 23:25:14.466460 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 23:25:14.466462 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 23:25:14.466464 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 23:25:14.466466 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 23:25:14.466469 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 23:25:14.466470 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 23:25:14.466473 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 23:25:14.466476 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 23:25:14.466609 13090 solver.cpp:467] Iteration 26000, Testing net (#0)
I0711 23:26:12.136562 13090 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.950884
I0711 23:26:12.136708 13090 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999384
I0711 23:26:12.136729 13090 solver.cpp:540]     Test net output #2: loss = 0.161358 (* 1 = 0.161358 loss)
I0711 23:26:12.344475 13090 solver.cpp:290] Iteration 26000 (1.31349 iter/s, 76.1331s/100 iter), loss = 0.0304259
I0711 23:26:12.344519 13090 solver.cpp:309]     Train net output #0: loss = 0.030426 (* 1 = 0.030426 loss)
I0711 23:26:12.344532 13090 sgd_solver.cpp:106] Iteration 26000, lr = 1e-06
I0711 23:26:30.052742 13090 solver.cpp:290] Iteration 26100 (5.64725 iter/s, 17.7077s/100 iter), loss = 0.0306903
I0711 23:26:30.052821 13090 solver.cpp:309]     Train net output #0: loss = 0.0306904 (* 1 = 0.0306904 loss)
I0711 23:26:30.052851 13090 sgd_solver.cpp:106] Iteration 26100, lr = 1e-06
I0711 23:26:47.372018 13090 solver.cpp:290] Iteration 26200 (5.7741 iter/s, 17.3187s/100 iter), loss = 0.0294115
I0711 23:26:47.372069 13090 solver.cpp:309]     Train net output #0: loss = 0.0294116 (* 1 = 0.0294116 loss)
I0711 23:26:47.372076 13090 sgd_solver.cpp:106] Iteration 26200, lr = 1e-06
I0711 23:27:04.488898 13090 solver.cpp:290] Iteration 26300 (5.84237 iter/s, 17.1164s/100 iter), loss = 0.0224892
I0711 23:27:04.488922 13090 solver.cpp:309]     Train net output #0: loss = 0.0224893 (* 1 = 0.0224893 loss)
I0711 23:27:04.488931 13090 sgd_solver.cpp:106] Iteration 26300, lr = 1e-06
I0711 23:27:21.485622 13090 solver.cpp:290] Iteration 26400 (5.88366 iter/s, 16.9962s/100 iter), loss = 0.0214195
I0711 23:27:21.485690 13090 solver.cpp:309]     Train net output #0: loss = 0.0214196 (* 1 = 0.0214196 loss)
I0711 23:27:21.485700 13090 sgd_solver.cpp:106] Iteration 26400, lr = 1e-06
I0711 23:27:38.480819 13090 solver.cpp:290] Iteration 26500 (5.8842 iter/s, 16.9947s/100 iter), loss = 0.0374695
I0711 23:27:38.480842 13090 solver.cpp:309]     Train net output #0: loss = 0.0374697 (* 1 = 0.0374697 loss)
I0711 23:27:38.480849 13090 sgd_solver.cpp:106] Iteration 26500, lr = 1e-06
I0711 23:27:55.538967 13090 solver.cpp:290] Iteration 26600 (5.86247 iter/s, 17.0577s/100 iter), loss = 0.040984
I0711 23:27:55.539041 13090 solver.cpp:309]     Train net output #0: loss = 0.0409841 (* 1 = 0.0409841 loss)
I0711 23:27:55.539052 13090 sgd_solver.cpp:106] Iteration 26600, lr = 1e-06
I0711 23:28:12.483398 13090 solver.cpp:290] Iteration 26700 (5.90183 iter/s, 16.9439s/100 iter), loss = 0.0229501
I0711 23:28:12.483423 13090 solver.cpp:309]     Train net output #0: loss = 0.0229503 (* 1 = 0.0229503 loss)
I0711 23:28:12.483430 13090 sgd_solver.cpp:106] Iteration 26700, lr = 1e-06
I0711 23:28:29.413085 13090 solver.cpp:290] Iteration 26800 (5.90695 iter/s, 16.9292s/100 iter), loss = 0.0390385
I0711 23:28:29.413192 13090 solver.cpp:309]     Train net output #0: loss = 0.0390386 (* 1 = 0.0390386 loss)
I0711 23:28:29.413202 13090 sgd_solver.cpp:106] Iteration 26800, lr = 1e-06
I0711 23:28:46.338415 13090 solver.cpp:290] Iteration 26900 (5.9085 iter/s, 16.9248s/100 iter), loss = 0.0335077
I0711 23:28:46.338439 13090 solver.cpp:309]     Train net output #0: loss = 0.0335078 (* 1 = 0.0335078 loss)
I0711 23:28:46.338446 13090 sgd_solver.cpp:106] Iteration 26900, lr = 1e-06
I0711 23:29:03.212093 13090 solver.cpp:354] Sparsity after update:
I0711 23:29:03.273775 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 23:29:03.273789 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 23:29:03.273799 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 23:29:03.273802 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 23:29:03.273807 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 23:29:03.273810 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 23:29:03.273814 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 23:29:03.273819 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 23:29:03.273823 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 23:29:03.273828 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 23:29:03.273833 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 23:29:03.273836 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 23:29:03.273840 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 23:29:03.273844 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 23:29:03.273846 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 23:29:03.273849 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 23:29:03.273854 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 23:29:03.273857 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 23:29:03.273861 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 23:29:03.423825 13090 solver.cpp:290] Iteration 27000 (5.85312 iter/s, 17.0849s/100 iter), loss = 0.0199302
I0711 23:29:03.423856 13090 solver.cpp:309]     Train net output #0: loss = 0.0199303 (* 1 = 0.0199303 loss)
I0711 23:29:03.423864 13090 sgd_solver.cpp:106] Iteration 27000, lr = 1e-06
I0711 23:29:21.092767 13090 solver.cpp:290] Iteration 27100 (5.65981 iter/s, 17.6684s/100 iter), loss = 0.0183119
I0711 23:29:21.092795 13090 solver.cpp:309]     Train net output #0: loss = 0.018312 (* 1 = 0.018312 loss)
I0711 23:29:21.092802 13090 sgd_solver.cpp:106] Iteration 27100, lr = 1e-06
I0711 23:29:39.824089 13090 solver.cpp:290] Iteration 27200 (5.33881 iter/s, 18.7308s/100 iter), loss = 0.0318305
I0711 23:29:39.824158 13090 solver.cpp:309]     Train net output #0: loss = 0.0318306 (* 1 = 0.0318306 loss)
I0711 23:29:39.824167 13090 sgd_solver.cpp:106] Iteration 27200, lr = 1e-06
I0711 23:29:57.856497 13090 solver.cpp:290] Iteration 27300 (5.54574 iter/s, 18.0318s/100 iter), loss = 0.0384612
I0711 23:29:57.856520 13090 solver.cpp:309]     Train net output #0: loss = 0.0384613 (* 1 = 0.0384613 loss)
I0711 23:29:57.856528 13090 sgd_solver.cpp:106] Iteration 27300, lr = 1e-06
I0711 23:30:15.923835 13090 solver.cpp:290] Iteration 27400 (5.53501 iter/s, 18.0668s/100 iter), loss = 0.0214975
I0711 23:30:15.923892 13090 solver.cpp:309]     Train net output #0: loss = 0.0214976 (* 1 = 0.0214976 loss)
I0711 23:30:15.923899 13090 sgd_solver.cpp:106] Iteration 27400, lr = 1e-06
I0711 23:30:33.736333 13090 solver.cpp:290] Iteration 27500 (5.61421 iter/s, 17.8119s/100 iter), loss = 0.0224518
I0711 23:30:33.736378 13090 solver.cpp:309]     Train net output #0: loss = 0.0224519 (* 1 = 0.0224519 loss)
I0711 23:30:33.736394 13090 sgd_solver.cpp:106] Iteration 27500, lr = 1e-06
I0711 23:30:52.483155 13090 solver.cpp:290] Iteration 27600 (5.3344 iter/s, 18.7463s/100 iter), loss = 0.0275236
I0711 23:30:52.483230 13090 solver.cpp:309]     Train net output #0: loss = 0.0275237 (* 1 = 0.0275237 loss)
I0711 23:30:52.483248 13090 sgd_solver.cpp:106] Iteration 27600, lr = 1e-06
I0711 23:31:11.064462 13090 solver.cpp:290] Iteration 27700 (5.38193 iter/s, 18.5807s/100 iter), loss = 0.0416203
I0711 23:31:11.064589 13090 solver.cpp:309]     Train net output #0: loss = 0.0416204 (* 1 = 0.0416204 loss)
I0711 23:31:11.064632 13090 sgd_solver.cpp:106] Iteration 27700, lr = 1e-06
I0711 23:31:28.975713 13090 solver.cpp:290] Iteration 27800 (5.58327 iter/s, 17.9107s/100 iter), loss = 0.0380873
I0711 23:31:28.975801 13090 solver.cpp:309]     Train net output #0: loss = 0.0380874 (* 1 = 0.0380874 loss)
I0711 23:31:28.975812 13090 sgd_solver.cpp:106] Iteration 27800, lr = 1e-06
I0711 23:31:47.009313 13090 solver.cpp:290] Iteration 27900 (5.54538 iter/s, 18.033s/100 iter), loss = 0.0493147
I0711 23:31:47.009362 13090 solver.cpp:309]     Train net output #0: loss = 0.0493148 (* 1 = 0.0493148 loss)
I0711 23:31:47.009376 13090 sgd_solver.cpp:106] Iteration 27900, lr = 1e-06
I0711 23:32:05.078130 13090 solver.cpp:354] Sparsity after update:
I0711 23:32:05.081531 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 23:32:05.081549 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 23:32:05.081565 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 23:32:05.081570 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 23:32:05.081575 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 23:32:05.081580 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 23:32:05.081584 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 23:32:05.081589 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 23:32:05.081594 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 23:32:05.081600 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 23:32:05.081604 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 23:32:05.081610 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 23:32:05.081615 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 23:32:05.081620 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 23:32:05.081622 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 23:32:05.081626 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 23:32:05.081631 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 23:32:05.081636 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 23:32:05.081642 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 23:32:05.081957 13090 solver.cpp:467] Iteration 28000, Testing net (#0)
I0711 23:33:11.183135 13090 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.95093
I0711 23:33:11.183228 13090 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999355
I0711 23:33:11.183235 13090 solver.cpp:540]     Test net output #2: loss = 0.163173 (* 1 = 0.163173 loss)
I0711 23:33:11.373442 13090 solver.cpp:290] Iteration 28000 (1.18537 iter/s, 84.3618s/100 iter), loss = 0.0250614
I0711 23:33:11.373486 13090 solver.cpp:309]     Train net output #0: loss = 0.0250615 (* 1 = 0.0250615 loss)
I0711 23:33:11.373498 13090 sgd_solver.cpp:106] Iteration 28000, lr = 1e-06
I0711 23:33:29.406929 13090 solver.cpp:290] Iteration 28100 (5.54541 iter/s, 18.0329s/100 iter), loss = 0.046102
I0711 23:33:29.406960 13090 solver.cpp:309]     Train net output #0: loss = 0.0461021 (* 1 = 0.0461021 loss)
I0711 23:33:29.406967 13090 sgd_solver.cpp:106] Iteration 28100, lr = 1e-06
I0711 23:33:47.270596 13090 solver.cpp:290] Iteration 28200 (5.59812 iter/s, 17.8631s/100 iter), loss = 0.0366167
I0711 23:33:47.270653 13090 solver.cpp:309]     Train net output #0: loss = 0.0366168 (* 1 = 0.0366168 loss)
I0711 23:33:47.270659 13090 sgd_solver.cpp:106] Iteration 28200, lr = 1e-06
I0711 23:34:05.010177 13090 solver.cpp:290] Iteration 28300 (5.63729 iter/s, 17.739s/100 iter), loss = 0.0262924
I0711 23:34:05.010226 13090 solver.cpp:309]     Train net output #0: loss = 0.0262925 (* 1 = 0.0262925 loss)
I0711 23:34:05.010249 13090 sgd_solver.cpp:106] Iteration 28300, lr = 1e-06
I0711 23:34:23.441498 13090 solver.cpp:290] Iteration 28400 (5.42571 iter/s, 18.4308s/100 iter), loss = 0.0207001
I0711 23:34:23.441577 13090 solver.cpp:309]     Train net output #0: loss = 0.0207002 (* 1 = 0.0207002 loss)
I0711 23:34:23.441592 13090 sgd_solver.cpp:106] Iteration 28400, lr = 1e-06
I0711 23:34:41.792889 13090 solver.cpp:290] Iteration 28500 (5.44935 iter/s, 18.3508s/100 iter), loss = 0.0250474
I0711 23:34:41.792913 13090 solver.cpp:309]     Train net output #0: loss = 0.0250475 (* 1 = 0.0250475 loss)
I0711 23:34:41.792922 13090 sgd_solver.cpp:106] Iteration 28500, lr = 1e-06
I0711 23:34:59.817782 13090 solver.cpp:290] Iteration 28600 (5.54804 iter/s, 18.0244s/100 iter), loss = 0.0287275
I0711 23:34:59.817828 13090 solver.cpp:309]     Train net output #0: loss = 0.0287276 (* 1 = 0.0287276 loss)
I0711 23:34:59.817837 13090 sgd_solver.cpp:106] Iteration 28600, lr = 1e-06
I0711 23:35:17.614001 13090 solver.cpp:290] Iteration 28700 (5.61934 iter/s, 17.7957s/100 iter), loss = 0.031977
I0711 23:35:17.614027 13090 solver.cpp:309]     Train net output #0: loss = 0.0319771 (* 1 = 0.0319771 loss)
I0711 23:35:17.614033 13090 sgd_solver.cpp:106] Iteration 28700, lr = 1e-06
I0711 23:35:35.922763 13090 solver.cpp:290] Iteration 28800 (5.46202 iter/s, 18.3082s/100 iter), loss = 0.0210419
I0711 23:35:35.922845 13090 solver.cpp:309]     Train net output #0: loss = 0.021042 (* 1 = 0.021042 loss)
I0711 23:35:35.922855 13090 sgd_solver.cpp:106] Iteration 28800, lr = 1e-06
I0711 23:35:54.302749 13090 solver.cpp:290] Iteration 28900 (5.44087 iter/s, 18.3794s/100 iter), loss = 0.0232541
I0711 23:35:54.302775 13090 solver.cpp:309]     Train net output #0: loss = 0.0232542 (* 1 = 0.0232542 loss)
I0711 23:35:54.302783 13090 sgd_solver.cpp:106] Iteration 28900, lr = 1e-06
I0711 23:36:11.926832 13090 solver.cpp:354] Sparsity after update:
I0711 23:36:11.990162 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 23:36:11.990188 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 23:36:11.990203 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 23:36:11.990209 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 23:36:11.990213 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 23:36:11.990218 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 23:36:11.990222 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 23:36:11.990226 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 23:36:11.990236 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 23:36:11.990242 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 23:36:11.990247 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 23:36:11.990254 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 23:36:11.990259 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 23:36:11.990267 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 23:36:11.990270 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 23:36:11.990276 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 23:36:11.990281 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 23:36:11.990288 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 23:36:11.990293 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 23:36:12.148118 13090 solver.cpp:290] Iteration 29000 (5.60386 iter/s, 17.8448s/100 iter), loss = 0.0248232
I0711 23:36:12.148241 13090 solver.cpp:309]     Train net output #0: loss = 0.0248232 (* 1 = 0.0248232 loss)
I0711 23:36:12.148353 13090 sgd_solver.cpp:106] Iteration 29000, lr = 1e-06
I0711 23:36:29.751746 13090 solver.cpp:290] Iteration 29100 (5.68084 iter/s, 17.603s/100 iter), loss = 0.0271547
I0711 23:36:29.751801 13090 solver.cpp:309]     Train net output #0: loss = 0.0271548 (* 1 = 0.0271548 loss)
I0711 23:36:29.751822 13090 sgd_solver.cpp:106] Iteration 29100, lr = 1e-06
I0711 23:36:48.223091 13090 solver.cpp:290] Iteration 29200 (5.41396 iter/s, 18.4708s/100 iter), loss = 0.0141449
I0711 23:36:48.223234 13090 solver.cpp:309]     Train net output #0: loss = 0.014145 (* 1 = 0.014145 loss)
I0711 23:36:48.223259 13090 sgd_solver.cpp:106] Iteration 29200, lr = 1e-06
I0711 23:37:06.246634 13090 solver.cpp:290] Iteration 29300 (5.5485 iter/s, 18.0229s/100 iter), loss = 0.0472151
I0711 23:37:06.246693 13090 solver.cpp:309]     Train net output #0: loss = 0.0472152 (* 1 = 0.0472152 loss)
I0711 23:37:06.246717 13090 sgd_solver.cpp:106] Iteration 29300, lr = 1e-06
I0711 23:37:23.842552 13090 solver.cpp:290] Iteration 29400 (5.68332 iter/s, 17.5954s/100 iter), loss = 0.045624
I0711 23:37:23.842764 13090 solver.cpp:309]     Train net output #0: loss = 0.045624 (* 1 = 0.045624 loss)
I0711 23:37:23.842813 13090 sgd_solver.cpp:106] Iteration 29400, lr = 1e-06
I0711 23:37:41.552708 13090 solver.cpp:290] Iteration 29500 (5.64669 iter/s, 17.7095s/100 iter), loss = 0.0240131
I0711 23:37:41.552757 13090 solver.cpp:309]     Train net output #0: loss = 0.0240132 (* 1 = 0.0240132 loss)
I0711 23:37:41.552781 13090 sgd_solver.cpp:106] Iteration 29500, lr = 1e-06
I0711 23:37:59.778403 13090 solver.cpp:290] Iteration 29600 (5.48692 iter/s, 18.2252s/100 iter), loss = 0.0550521
I0711 23:37:59.778477 13090 solver.cpp:309]     Train net output #0: loss = 0.0550522 (* 1 = 0.0550522 loss)
I0711 23:37:59.778484 13090 sgd_solver.cpp:106] Iteration 29600, lr = 1e-06
I0711 23:38:18.012768 13090 solver.cpp:290] Iteration 29700 (5.48432 iter/s, 18.2338s/100 iter), loss = 0.0404446
I0711 23:38:18.012789 13090 solver.cpp:309]     Train net output #0: loss = 0.0404447 (* 1 = 0.0404447 loss)
I0711 23:38:18.012796 13090 sgd_solver.cpp:106] Iteration 29700, lr = 1e-06
I0711 23:38:36.559736 13090 solver.cpp:290] Iteration 29800 (5.39187 iter/s, 18.5464s/100 iter), loss = 0.0233942
I0711 23:38:36.559797 13090 solver.cpp:309]     Train net output #0: loss = 0.0233942 (* 1 = 0.0233942 loss)
I0711 23:38:36.559804 13090 sgd_solver.cpp:106] Iteration 29800, lr = 1e-06
I0711 23:38:54.476697 13090 solver.cpp:290] Iteration 29900 (5.58148 iter/s, 17.9164s/100 iter), loss = 0.0319873
I0711 23:38:54.476724 13090 solver.cpp:309]     Train net output #0: loss = 0.0319874 (* 1 = 0.0319874 loss)
I0711 23:38:54.476733 13090 sgd_solver.cpp:106] Iteration 29900, lr = 1e-06
I0711 23:39:11.356868 13090 solver.cpp:594] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/sparse/cityscapes5_jsegnet21v2_iter_30000.caffemodel
I0711 23:39:11.382587 13090 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/sparse/cityscapes5_jsegnet21v2_iter_30000.solverstate
I0711 23:39:11.400962 13090 solver.cpp:354] Sparsity after update:
I0711 23:39:11.402576 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 23:39:11.402583 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 23:39:11.402591 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 23:39:11.402593 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 23:39:11.402595 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 23:39:11.402597 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 23:39:11.402600 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 23:39:11.402601 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 23:39:11.402603 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 23:39:11.402606 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 23:39:11.402607 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 23:39:11.402608 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 23:39:11.402611 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 23:39:11.402612 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 23:39:11.402614 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 23:39:11.402616 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 23:39:11.402618 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 23:39:11.402621 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 23:39:11.402622 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 23:39:11.402767 13090 solver.cpp:467] Iteration 30000, Testing net (#0)
I0711 23:39:57.948081 13090 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.950868
I0711 23:39:57.948151 13090 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999455
I0711 23:39:57.948158 13090 solver.cpp:540]     Test net output #2: loss = 0.160663 (* 1 = 0.160663 loss)
I0711 23:39:58.146100 13090 solver.cpp:290] Iteration 30000 (1.57066 iter/s, 63.6676s/100 iter), loss = 0.0203139
I0711 23:39:58.146122 13090 solver.cpp:309]     Train net output #0: loss = 0.0203139 (* 1 = 0.0203139 loss)
I0711 23:39:58.146129 13090 sgd_solver.cpp:106] Iteration 30000, lr = 1e-06
I0711 23:40:15.204337 13090 solver.cpp:290] Iteration 30100 (5.86244 iter/s, 17.0577s/100 iter), loss = 0.0353606
I0711 23:40:15.204363 13090 solver.cpp:309]     Train net output #0: loss = 0.0353607 (* 1 = 0.0353607 loss)
I0711 23:40:15.204372 13090 sgd_solver.cpp:106] Iteration 30100, lr = 1e-06
I0711 23:40:32.237046 13090 solver.cpp:290] Iteration 30200 (5.87123 iter/s, 17.0322s/100 iter), loss = 0.0456484
I0711 23:40:32.237154 13090 solver.cpp:309]     Train net output #0: loss = 0.0456484 (* 1 = 0.0456484 loss)
I0711 23:40:32.237165 13090 sgd_solver.cpp:106] Iteration 30200, lr = 1e-06
I0711 23:40:49.418305 13090 solver.cpp:290] Iteration 30300 (5.82049 iter/s, 17.1807s/100 iter), loss = 0.0527802
I0711 23:40:49.418336 13090 solver.cpp:309]     Train net output #0: loss = 0.0527803 (* 1 = 0.0527803 loss)
I0711 23:40:49.418345 13090 sgd_solver.cpp:106] Iteration 30300, lr = 1e-06
I0711 23:41:06.613790 13090 solver.cpp:290] Iteration 30400 (5.81565 iter/s, 17.195s/100 iter), loss = 0.0269117
I0711 23:41:06.613869 13090 solver.cpp:309]     Train net output #0: loss = 0.0269118 (* 1 = 0.0269118 loss)
I0711 23:41:06.613878 13090 sgd_solver.cpp:106] Iteration 30400, lr = 1e-06
I0711 23:41:23.730533 13090 solver.cpp:290] Iteration 30500 (5.84242 iter/s, 17.1162s/100 iter), loss = 0.016581
I0711 23:41:23.730561 13090 solver.cpp:309]     Train net output #0: loss = 0.0165811 (* 1 = 0.0165811 loss)
I0711 23:41:23.730571 13090 sgd_solver.cpp:106] Iteration 30500, lr = 1e-06
I0711 23:41:40.771013 13090 solver.cpp:290] Iteration 30600 (5.86855 iter/s, 17.04s/100 iter), loss = 0.0183129
I0711 23:41:40.771123 13090 solver.cpp:309]     Train net output #0: loss = 0.018313 (* 1 = 0.018313 loss)
I0711 23:41:40.771133 13090 sgd_solver.cpp:106] Iteration 30600, lr = 1e-06
I0711 23:41:57.770750 13090 solver.cpp:290] Iteration 30700 (5.88264 iter/s, 16.9992s/100 iter), loss = 0.0298751
I0711 23:41:57.770772 13090 solver.cpp:309]     Train net output #0: loss = 0.0298751 (* 1 = 0.0298751 loss)
I0711 23:41:57.770779 13090 sgd_solver.cpp:106] Iteration 30700, lr = 1e-06
I0711 23:42:14.869175 13090 solver.cpp:290] Iteration 30800 (5.84866 iter/s, 17.0979s/100 iter), loss = 0.0273925
I0711 23:42:14.869226 13090 solver.cpp:309]     Train net output #0: loss = 0.0273926 (* 1 = 0.0273926 loss)
I0711 23:42:14.869233 13090 sgd_solver.cpp:106] Iteration 30800, lr = 1e-06
I0711 23:42:32.720424 13090 solver.cpp:290] Iteration 30900 (5.60202 iter/s, 17.8507s/100 iter), loss = 0.0397752
I0711 23:42:32.720453 13090 solver.cpp:309]     Train net output #0: loss = 0.0397753 (* 1 = 0.0397753 loss)
I0711 23:42:32.720466 13090 sgd_solver.cpp:106] Iteration 30900, lr = 1e-06
I0711 23:42:50.134551 13090 solver.cpp:354] Sparsity after update:
I0711 23:42:50.203680 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 23:42:50.203698 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 23:42:50.203704 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 23:42:50.203707 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 23:42:50.203709 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 23:42:50.203711 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 23:42:50.203713 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 23:42:50.203716 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 23:42:50.203722 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 23:42:50.203727 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 23:42:50.203729 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 23:42:50.203732 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 23:42:50.203734 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 23:42:50.203737 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 23:42:50.203739 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 23:42:50.203742 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 23:42:50.203744 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 23:42:50.203747 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 23:42:50.203748 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 23:42:50.358139 13090 solver.cpp:290] Iteration 31000 (5.66983 iter/s, 17.6372s/100 iter), loss = 0.0201152
I0711 23:42:50.358165 13090 solver.cpp:309]     Train net output #0: loss = 0.0201153 (* 1 = 0.0201153 loss)
I0711 23:42:50.358170 13090 sgd_solver.cpp:106] Iteration 31000, lr = 1e-06
I0711 23:43:07.370579 13090 solver.cpp:290] Iteration 31100 (5.87822 iter/s, 17.0119s/100 iter), loss = 0.032176
I0711 23:43:07.370605 13090 solver.cpp:309]     Train net output #0: loss = 0.0321761 (* 1 = 0.0321761 loss)
I0711 23:43:07.370615 13090 sgd_solver.cpp:106] Iteration 31100, lr = 1e-06
I0711 23:43:24.326025 13090 solver.cpp:290] Iteration 31200 (5.89798 iter/s, 16.955s/100 iter), loss = 0.037313
I0711 23:43:24.326081 13090 solver.cpp:309]     Train net output #0: loss = 0.037313 (* 1 = 0.037313 loss)
I0711 23:43:24.326093 13090 sgd_solver.cpp:106] Iteration 31200, lr = 1e-06
I0711 23:43:41.221808 13090 solver.cpp:290] Iteration 31300 (5.91882 iter/s, 16.8953s/100 iter), loss = 0.0450762
I0711 23:43:41.221830 13090 solver.cpp:309]     Train net output #0: loss = 0.0450763 (* 1 = 0.0450763 loss)
I0711 23:43:41.221837 13090 sgd_solver.cpp:106] Iteration 31300, lr = 1e-06
I0711 23:43:58.296365 13090 solver.cpp:290] Iteration 31400 (5.85684 iter/s, 17.0741s/100 iter), loss = 0.027245
I0711 23:43:58.296440 13090 solver.cpp:309]     Train net output #0: loss = 0.027245 (* 1 = 0.027245 loss)
I0711 23:43:58.296449 13090 sgd_solver.cpp:106] Iteration 31400, lr = 1e-06
I0711 23:44:15.368424 13090 solver.cpp:290] Iteration 31500 (5.85771 iter/s, 17.0715s/100 iter), loss = 0.0222524
I0711 23:44:15.368446 13090 solver.cpp:309]     Train net output #0: loss = 0.0222524 (* 1 = 0.0222524 loss)
I0711 23:44:15.368453 13090 sgd_solver.cpp:106] Iteration 31500, lr = 1e-06
I0711 23:44:32.424859 13090 solver.cpp:290] Iteration 31600 (5.86306 iter/s, 17.0559s/100 iter), loss = 0.0344909
I0711 23:44:32.424937 13090 solver.cpp:309]     Train net output #0: loss = 0.034491 (* 1 = 0.034491 loss)
I0711 23:44:32.424947 13090 sgd_solver.cpp:106] Iteration 31600, lr = 1e-06
I0711 23:44:49.544421 13090 solver.cpp:290] Iteration 31700 (5.84145 iter/s, 17.119s/100 iter), loss = 0.0668461
I0711 23:44:49.544445 13090 solver.cpp:309]     Train net output #0: loss = 0.0668462 (* 1 = 0.0668462 loss)
I0711 23:44:49.544450 13090 sgd_solver.cpp:106] Iteration 31700, lr = 1e-06
I0711 23:45:06.451910 13090 solver.cpp:290] Iteration 31800 (5.91471 iter/s, 16.907s/100 iter), loss = 0.0189023
I0711 23:45:06.451983 13090 solver.cpp:309]     Train net output #0: loss = 0.0189023 (* 1 = 0.0189023 loss)
I0711 23:45:06.451992 13090 sgd_solver.cpp:106] Iteration 31800, lr = 1e-06
I0711 23:45:23.665891 13090 solver.cpp:290] Iteration 31900 (5.80941 iter/s, 17.2134s/100 iter), loss = 0.042641
I0711 23:45:23.665917 13090 solver.cpp:309]     Train net output #0: loss = 0.0426411 (* 1 = 0.0426411 loss)
I0711 23:45:23.665926 13090 sgd_solver.cpp:106] Iteration 31900, lr = 1e-06
I0711 23:45:40.588008 13090 solver.cpp:354] Sparsity after update:
I0711 23:45:40.589913 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 23:45:40.589922 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 23:45:40.589931 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 23:45:40.589936 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 23:45:40.589939 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 23:45:40.589943 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 23:45:40.589948 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 23:45:40.589951 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 23:45:40.589956 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 23:45:40.589960 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 23:45:40.589964 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 23:45:40.589968 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 23:45:40.589972 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 23:45:40.589977 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 23:45:40.589982 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 23:45:40.589985 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 23:45:40.589990 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 23:45:40.589994 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 23:45:40.589998 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 23:45:40.590011 13090 solver.cpp:594] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/sparse/cityscapes5_jsegnet21v2_iter_32000.caffemodel
I0711 23:45:40.617529 13090 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/sparse/cityscapes5_jsegnet21v2_iter_32000.solverstate
I0711 23:45:40.678750 13090 solver.cpp:447] Iteration 32000, loss = 0.0343516
I0711 23:45:40.678772 13090 solver.cpp:467] Iteration 32000, Testing net (#0)
I0711 23:46:44.827744 13090 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.951162
I0711 23:46:44.827855 13090 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.99937
I0711 23:46:44.827863 13090 solver.cpp:540]     Test net output #2: loss = 0.161306 (* 1 = 0.161306 loss)
I0711 23:46:44.827867 13090 solver.cpp:452] Optimization Done.
I0711 23:46:45.112176 13090 caffe.cpp:246] Optimization Done.
