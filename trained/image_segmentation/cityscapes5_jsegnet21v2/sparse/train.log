I0703 01:28:03.574080 31050 caffe.cpp:209] Using GPUs 0, 1
I0703 01:28:03.576458 31050 caffe.cpp:214] GPU 0: GeForce GTX 1080
I0703 01:28:03.576812 31050 caffe.cpp:214] GPU 1: GeForce GTX 1080
I0703 01:28:04.459381 31050 solver.cpp:48] Initializing solver from parameters: 
train_net: "training/cityscapes5_jsegnet21v2_2017-07-02_23-02-42/sparse/train.prototxt"
test_net: "training/cityscapes5_jsegnet21v2_2017-07-02_23-02-42/sparse/test.prototxt"
test_iter: 125
test_interval: 2000
base_lr: 1e-05
display: 100
max_iter: 32000
lr_policy: "multistep"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 1e-05
snapshot: 10000
snapshot_prefix: "training/cityscapes5_jsegnet21v2_2017-07-02_23-02-42/sparse/cityscapes5_jsegnet21v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
regularization_type: "L1"
test_initialization: false
stepvalue: 24000
iter_size: 1
type: "Adam"
display_sparsity: 1000
sparse_mode: SPARSE_UPDATE
sparsity_target: 0.8
sparsity_step_factor: 0.05
sparsity_step_iter: 1000
sparsity_start_iter: 4000
sparsity_start_factor: 0
I0703 01:28:04.482028 31050 solver.cpp:82] Creating training net from train_net file: training/cityscapes5_jsegnet21v2_2017-07-02_23-02-42/sparse/train.prototxt
I0703 01:28:04.490907 31050 net.cpp:327] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0703 01:28:04.490917 31050 net.cpp:327] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
W0703 01:28:04.490933 31050 parallel.cpp:400] Batch size must be divisible by the number of solvers (GPUs)
I0703 01:28:04.491334 31050 net.cpp:56] Initializing net from parameters: 
name: "jsegnet21v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/train-image-lmdb"
    label_list_path: "data/train-label-lmdb"
    batch_size: 8
    threads: 4
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b/bn"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a/bn"
  top: "out5a/bn"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a/bn"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b/bn"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a/bn"
  top: "out3a/bn"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a/bn"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv1/bn"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv2/bn"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv3/bn"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4/bn"
  top: "ctx_conv4/bn"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4/bn"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
I0703 01:28:04.494552 31050 layer_factory.hpp:77] Creating layer data
I0703 01:28:04.494566 31050 net.cpp:98] Creating Layer data
I0703 01:28:04.494571 31050 net.cpp:413] data -> data
I0703 01:28:04.494590 31050 net.cpp:413] data -> label
I0703 01:28:04.529078 31118 db_lmdb.cpp:35] Opened lmdb data/train-image-lmdb
I0703 01:28:04.529698 31123 db_lmdb.cpp:35] Opened lmdb data/train-label-lmdb
I0703 01:28:04.543244 31050 data_layer.cpp:78] ReshapePrefetch 8, 3, 640, 640
I0703 01:28:04.543464 31050 data_layer.cpp:83] output data size: 8,3,640,640
I0703 01:28:04.603737 31050 data_layer.cpp:78] ReshapePrefetch 8, 1, 640, 640
I0703 01:28:04.603785 31050 data_layer.cpp:83] output data size: 8,1,640,640
I0703 01:28:04.612304 31128 blocking_queue.cpp:50] Waiting for data
I0703 01:28:04.620362 31050 net.cpp:148] Setting up data
I0703 01:28:04.620383 31050 net.cpp:155] Top shape: 8 3 640 640 (9830400)
I0703 01:28:04.620386 31050 net.cpp:155] Top shape: 8 1 640 640 (3276800)
I0703 01:28:04.620388 31050 net.cpp:163] Memory required for data: 52428800
I0703 01:28:04.620396 31050 layer_factory.hpp:77] Creating layer data/bias
I0703 01:28:04.620407 31050 net.cpp:98] Creating Layer data/bias
I0703 01:28:04.620411 31050 net.cpp:439] data/bias <- data
I0703 01:28:04.620420 31050 net.cpp:413] data/bias -> data/bias
I0703 01:28:04.621345 31050 net.cpp:148] Setting up data/bias
I0703 01:28:04.621353 31050 net.cpp:155] Top shape: 8 3 640 640 (9830400)
I0703 01:28:04.621356 31050 net.cpp:163] Memory required for data: 91750400
I0703 01:28:04.621363 31050 layer_factory.hpp:77] Creating layer conv1a
I0703 01:28:04.621372 31050 net.cpp:98] Creating Layer conv1a
I0703 01:28:04.621376 31050 net.cpp:439] conv1a <- data/bias
I0703 01:28:04.621378 31050 net.cpp:413] conv1a -> conv1a
I0703 01:28:04.622454 31050 net.cpp:148] Setting up conv1a
I0703 01:28:04.622465 31050 net.cpp:155] Top shape: 8 32 320 320 (26214400)
I0703 01:28:04.622467 31050 net.cpp:163] Memory required for data: 196608000
I0703 01:28:04.622473 31050 layer_factory.hpp:77] Creating layer conv1a/bn
I0703 01:28:04.622510 31050 net.cpp:98] Creating Layer conv1a/bn
I0703 01:28:04.622514 31050 net.cpp:439] conv1a/bn <- conv1a
I0703 01:28:04.622519 31050 net.cpp:413] conv1a/bn -> conv1a/bn
I0703 01:28:04.624825 31050 net.cpp:148] Setting up conv1a/bn
I0703 01:28:04.624836 31050 net.cpp:155] Top shape: 8 32 320 320 (26214400)
I0703 01:28:04.624840 31050 net.cpp:163] Memory required for data: 301465600
I0703 01:28:04.624846 31050 layer_factory.hpp:77] Creating layer conv1a/relu
I0703 01:28:04.624852 31050 net.cpp:98] Creating Layer conv1a/relu
I0703 01:28:04.624855 31050 net.cpp:439] conv1a/relu <- conv1a/bn
I0703 01:28:04.624860 31050 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0703 01:28:04.624871 31050 net.cpp:148] Setting up conv1a/relu
I0703 01:28:04.624876 31050 net.cpp:155] Top shape: 8 32 320 320 (26214400)
I0703 01:28:04.624877 31050 net.cpp:163] Memory required for data: 406323200
I0703 01:28:04.624879 31050 layer_factory.hpp:77] Creating layer conv1b
I0703 01:28:04.624894 31050 net.cpp:98] Creating Layer conv1b
I0703 01:28:04.624898 31050 net.cpp:439] conv1b <- conv1a/bn
I0703 01:28:04.624902 31050 net.cpp:413] conv1b -> conv1b
I0703 01:28:04.625190 31050 net.cpp:148] Setting up conv1b
I0703 01:28:04.625196 31050 net.cpp:155] Top shape: 8 32 320 320 (26214400)
I0703 01:28:04.625198 31050 net.cpp:163] Memory required for data: 511180800
I0703 01:28:04.625203 31050 layer_factory.hpp:77] Creating layer conv1b/bn
I0703 01:28:04.625208 31050 net.cpp:98] Creating Layer conv1b/bn
I0703 01:28:04.625211 31050 net.cpp:439] conv1b/bn <- conv1b
I0703 01:28:04.625213 31050 net.cpp:413] conv1b/bn -> conv1b/bn
I0703 01:28:04.625720 31050 net.cpp:148] Setting up conv1b/bn
I0703 01:28:04.625725 31050 net.cpp:155] Top shape: 8 32 320 320 (26214400)
I0703 01:28:04.625726 31050 net.cpp:163] Memory required for data: 616038400
I0703 01:28:04.625731 31050 layer_factory.hpp:77] Creating layer conv1b/relu
I0703 01:28:04.625735 31050 net.cpp:98] Creating Layer conv1b/relu
I0703 01:28:04.625737 31050 net.cpp:439] conv1b/relu <- conv1b/bn
I0703 01:28:04.625741 31050 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0703 01:28:04.625743 31050 net.cpp:148] Setting up conv1b/relu
I0703 01:28:04.625746 31050 net.cpp:155] Top shape: 8 32 320 320 (26214400)
I0703 01:28:04.625747 31050 net.cpp:163] Memory required for data: 720896000
I0703 01:28:04.625751 31050 layer_factory.hpp:77] Creating layer pool1
I0703 01:28:04.625761 31050 net.cpp:98] Creating Layer pool1
I0703 01:28:04.625762 31050 net.cpp:439] pool1 <- conv1b/bn
I0703 01:28:04.625766 31050 net.cpp:413] pool1 -> pool1
I0703 01:28:04.626039 31050 net.cpp:148] Setting up pool1
I0703 01:28:04.626045 31050 net.cpp:155] Top shape: 8 32 160 160 (6553600)
I0703 01:28:04.626049 31050 net.cpp:163] Memory required for data: 747110400
I0703 01:28:04.626050 31050 layer_factory.hpp:77] Creating layer res2a_branch2a
I0703 01:28:04.626055 31050 net.cpp:98] Creating Layer res2a_branch2a
I0703 01:28:04.626058 31050 net.cpp:439] res2a_branch2a <- pool1
I0703 01:28:04.626061 31050 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0703 01:28:04.627339 31050 net.cpp:148] Setting up res2a_branch2a
I0703 01:28:04.627348 31050 net.cpp:155] Top shape: 8 64 160 160 (13107200)
I0703 01:28:04.627351 31050 net.cpp:163] Memory required for data: 799539200
I0703 01:28:04.627355 31050 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0703 01:28:04.627360 31050 net.cpp:98] Creating Layer res2a_branch2a/bn
I0703 01:28:04.627363 31050 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0703 01:28:04.627367 31050 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0703 01:28:04.627845 31050 net.cpp:148] Setting up res2a_branch2a/bn
I0703 01:28:04.627851 31050 net.cpp:155] Top shape: 8 64 160 160 (13107200)
I0703 01:28:04.627852 31050 net.cpp:163] Memory required for data: 851968000
I0703 01:28:04.627857 31050 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0703 01:28:04.627861 31050 net.cpp:98] Creating Layer res2a_branch2a/relu
I0703 01:28:04.627863 31050 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0703 01:28:04.627866 31050 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0703 01:28:04.627871 31050 net.cpp:148] Setting up res2a_branch2a/relu
I0703 01:28:04.627873 31050 net.cpp:155] Top shape: 8 64 160 160 (13107200)
I0703 01:28:04.627876 31050 net.cpp:163] Memory required for data: 904396800
I0703 01:28:04.627877 31050 layer_factory.hpp:77] Creating layer res2a_branch2b
I0703 01:28:04.627882 31050 net.cpp:98] Creating Layer res2a_branch2b
I0703 01:28:04.627884 31050 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0703 01:28:04.627887 31050 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0703 01:28:04.628957 31050 net.cpp:148] Setting up res2a_branch2b
I0703 01:28:04.628967 31050 net.cpp:155] Top shape: 8 64 160 160 (13107200)
I0703 01:28:04.628968 31050 net.cpp:163] Memory required for data: 956825600
I0703 01:28:04.628973 31050 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0703 01:28:04.628985 31050 net.cpp:98] Creating Layer res2a_branch2b/bn
I0703 01:28:04.628988 31050 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0703 01:28:04.628991 31050 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0703 01:28:04.629472 31050 net.cpp:148] Setting up res2a_branch2b/bn
I0703 01:28:04.629477 31050 net.cpp:155] Top shape: 8 64 160 160 (13107200)
I0703 01:28:04.629479 31050 net.cpp:163] Memory required for data: 1009254400
I0703 01:28:04.629485 31050 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0703 01:28:04.629488 31050 net.cpp:98] Creating Layer res2a_branch2b/relu
I0703 01:28:04.629492 31050 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0703 01:28:04.629494 31050 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0703 01:28:04.629498 31050 net.cpp:148] Setting up res2a_branch2b/relu
I0703 01:28:04.629501 31050 net.cpp:155] Top shape: 8 64 160 160 (13107200)
I0703 01:28:04.629503 31050 net.cpp:163] Memory required for data: 1061683200
I0703 01:28:04.629505 31050 layer_factory.hpp:77] Creating layer pool2
I0703 01:28:04.629508 31050 net.cpp:98] Creating Layer pool2
I0703 01:28:04.629511 31050 net.cpp:439] pool2 <- res2a_branch2b/bn
I0703 01:28:04.629513 31050 net.cpp:413] pool2 -> pool2
I0703 01:28:04.629540 31050 net.cpp:148] Setting up pool2
I0703 01:28:04.629544 31050 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0703 01:28:04.629546 31050 net.cpp:163] Memory required for data: 1074790400
I0703 01:28:04.629549 31050 layer_factory.hpp:77] Creating layer res3a_branch2a
I0703 01:28:04.629554 31050 net.cpp:98] Creating Layer res3a_branch2a
I0703 01:28:04.629556 31050 net.cpp:439] res3a_branch2a <- pool2
I0703 01:28:04.629559 31050 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0703 01:28:04.631212 31050 net.cpp:148] Setting up res3a_branch2a
I0703 01:28:04.631218 31050 net.cpp:155] Top shape: 8 128 80 80 (6553600)
I0703 01:28:04.631220 31050 net.cpp:163] Memory required for data: 1101004800
I0703 01:28:04.631224 31050 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0703 01:28:04.631227 31050 net.cpp:98] Creating Layer res3a_branch2a/bn
I0703 01:28:04.631230 31050 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0703 01:28:04.631234 31050 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0703 01:28:04.631695 31050 net.cpp:148] Setting up res3a_branch2a/bn
I0703 01:28:04.631705 31050 net.cpp:155] Top shape: 8 128 80 80 (6553600)
I0703 01:28:04.631707 31050 net.cpp:163] Memory required for data: 1127219200
I0703 01:28:04.631719 31050 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0703 01:28:04.631724 31050 net.cpp:98] Creating Layer res3a_branch2a/relu
I0703 01:28:04.631727 31050 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0703 01:28:04.631731 31050 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0703 01:28:04.631739 31050 net.cpp:148] Setting up res3a_branch2a/relu
I0703 01:28:04.631743 31050 net.cpp:155] Top shape: 8 128 80 80 (6553600)
I0703 01:28:04.631747 31050 net.cpp:163] Memory required for data: 1153433600
I0703 01:28:04.631749 31050 layer_factory.hpp:77] Creating layer res3a_branch2b
I0703 01:28:04.631755 31050 net.cpp:98] Creating Layer res3a_branch2b
I0703 01:28:04.631759 31050 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0703 01:28:04.631767 31050 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0703 01:28:04.632817 31050 net.cpp:148] Setting up res3a_branch2b
I0703 01:28:04.632824 31050 net.cpp:155] Top shape: 8 128 80 80 (6553600)
I0703 01:28:04.632827 31050 net.cpp:163] Memory required for data: 1179648000
I0703 01:28:04.632832 31050 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0703 01:28:04.632835 31050 net.cpp:98] Creating Layer res3a_branch2b/bn
I0703 01:28:04.632838 31050 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0703 01:28:04.632843 31050 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0703 01:28:04.633273 31050 net.cpp:148] Setting up res3a_branch2b/bn
I0703 01:28:04.633280 31050 net.cpp:155] Top shape: 8 128 80 80 (6553600)
I0703 01:28:04.633281 31050 net.cpp:163] Memory required for data: 1205862400
I0703 01:28:04.633292 31050 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0703 01:28:04.633296 31050 net.cpp:98] Creating Layer res3a_branch2b/relu
I0703 01:28:04.633299 31050 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0703 01:28:04.633302 31050 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0703 01:28:04.633306 31050 net.cpp:148] Setting up res3a_branch2b/relu
I0703 01:28:04.633309 31050 net.cpp:155] Top shape: 8 128 80 80 (6553600)
I0703 01:28:04.633311 31050 net.cpp:163] Memory required for data: 1232076800
I0703 01:28:04.633313 31050 layer_factory.hpp:77] Creating layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0703 01:28:04.633317 31050 net.cpp:98] Creating Layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0703 01:28:04.633319 31050 net.cpp:439] res3a_branch2b/bn_res3a_branch2b/relu_0_split <- res3a_branch2b/bn
I0703 01:28:04.633322 31050 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0703 01:28:04.633324 31050 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0703 01:28:04.633350 31050 net.cpp:148] Setting up res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0703 01:28:04.633354 31050 net.cpp:155] Top shape: 8 128 80 80 (6553600)
I0703 01:28:04.633357 31050 net.cpp:155] Top shape: 8 128 80 80 (6553600)
I0703 01:28:04.633359 31050 net.cpp:163] Memory required for data: 1284505600
I0703 01:28:04.633361 31050 layer_factory.hpp:77] Creating layer pool3
I0703 01:28:04.633364 31050 net.cpp:98] Creating Layer pool3
I0703 01:28:04.633368 31050 net.cpp:439] pool3 <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0703 01:28:04.633370 31050 net.cpp:413] pool3 -> pool3
I0703 01:28:04.633396 31050 net.cpp:148] Setting up pool3
I0703 01:28:04.633400 31050 net.cpp:155] Top shape: 8 128 40 40 (1638400)
I0703 01:28:04.633402 31050 net.cpp:163] Memory required for data: 1291059200
I0703 01:28:04.633405 31050 layer_factory.hpp:77] Creating layer res4a_branch2a
I0703 01:28:04.633410 31050 net.cpp:98] Creating Layer res4a_branch2a
I0703 01:28:04.633412 31050 net.cpp:439] res4a_branch2a <- pool3
I0703 01:28:04.633416 31050 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0703 01:28:04.640305 31050 net.cpp:148] Setting up res4a_branch2a
I0703 01:28:04.640316 31050 net.cpp:155] Top shape: 8 256 40 40 (3276800)
I0703 01:28:04.640317 31050 net.cpp:163] Memory required for data: 1304166400
I0703 01:28:04.640321 31050 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0703 01:28:04.640326 31050 net.cpp:98] Creating Layer res4a_branch2a/bn
I0703 01:28:04.640329 31050 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0703 01:28:04.640333 31050 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0703 01:28:04.640777 31050 net.cpp:148] Setting up res4a_branch2a/bn
I0703 01:28:04.640782 31050 net.cpp:155] Top shape: 8 256 40 40 (3276800)
I0703 01:28:04.640785 31050 net.cpp:163] Memory required for data: 1317273600
I0703 01:28:04.640790 31050 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0703 01:28:04.640794 31050 net.cpp:98] Creating Layer res4a_branch2a/relu
I0703 01:28:04.640796 31050 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0703 01:28:04.640799 31050 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0703 01:28:04.640802 31050 net.cpp:148] Setting up res4a_branch2a/relu
I0703 01:28:04.640805 31050 net.cpp:155] Top shape: 8 256 40 40 (3276800)
I0703 01:28:04.640807 31050 net.cpp:163] Memory required for data: 1330380800
I0703 01:28:04.640810 31050 layer_factory.hpp:77] Creating layer res4a_branch2b
I0703 01:28:04.640815 31050 net.cpp:98] Creating Layer res4a_branch2b
I0703 01:28:04.640816 31050 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0703 01:28:04.640820 31050 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0703 01:28:04.643913 31050 net.cpp:148] Setting up res4a_branch2b
I0703 01:28:04.643918 31050 net.cpp:155] Top shape: 8 256 40 40 (3276800)
I0703 01:28:04.643920 31050 net.cpp:163] Memory required for data: 1343488000
I0703 01:28:04.643930 31050 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0703 01:28:04.643935 31050 net.cpp:98] Creating Layer res4a_branch2b/bn
I0703 01:28:04.643939 31050 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0703 01:28:04.643942 31050 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0703 01:28:04.644376 31050 net.cpp:148] Setting up res4a_branch2b/bn
I0703 01:28:04.644381 31050 net.cpp:155] Top shape: 8 256 40 40 (3276800)
I0703 01:28:04.644384 31050 net.cpp:163] Memory required for data: 1356595200
I0703 01:28:04.644389 31050 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0703 01:28:04.644392 31050 net.cpp:98] Creating Layer res4a_branch2b/relu
I0703 01:28:04.644395 31050 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0703 01:28:04.644398 31050 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0703 01:28:04.644402 31050 net.cpp:148] Setting up res4a_branch2b/relu
I0703 01:28:04.644404 31050 net.cpp:155] Top shape: 8 256 40 40 (3276800)
I0703 01:28:04.644407 31050 net.cpp:163] Memory required for data: 1369702400
I0703 01:28:04.644408 31050 layer_factory.hpp:77] Creating layer pool4
I0703 01:28:04.644412 31050 net.cpp:98] Creating Layer pool4
I0703 01:28:04.644414 31050 net.cpp:439] pool4 <- res4a_branch2b/bn
I0703 01:28:04.644418 31050 net.cpp:413] pool4 -> pool4
I0703 01:28:04.644443 31050 net.cpp:148] Setting up pool4
I0703 01:28:04.644448 31050 net.cpp:155] Top shape: 8 256 40 40 (3276800)
I0703 01:28:04.644449 31050 net.cpp:163] Memory required for data: 1382809600
I0703 01:28:04.644453 31050 layer_factory.hpp:77] Creating layer res5a_branch2a
I0703 01:28:04.644456 31050 net.cpp:98] Creating Layer res5a_branch2a
I0703 01:28:04.644459 31050 net.cpp:439] res5a_branch2a <- pool4
I0703 01:28:04.644462 31050 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0703 01:28:04.669868 31050 net.cpp:148] Setting up res5a_branch2a
I0703 01:28:04.669888 31050 net.cpp:155] Top shape: 8 512 40 40 (6553600)
I0703 01:28:04.669891 31050 net.cpp:163] Memory required for data: 1409024000
I0703 01:28:04.669896 31050 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0703 01:28:04.669906 31050 net.cpp:98] Creating Layer res5a_branch2a/bn
I0703 01:28:04.669909 31050 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0703 01:28:04.669914 31050 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0703 01:28:04.670379 31050 net.cpp:148] Setting up res5a_branch2a/bn
I0703 01:28:04.670392 31050 net.cpp:155] Top shape: 8 512 40 40 (6553600)
I0703 01:28:04.670395 31050 net.cpp:163] Memory required for data: 1435238400
I0703 01:28:04.670400 31050 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0703 01:28:04.670404 31050 net.cpp:98] Creating Layer res5a_branch2a/relu
I0703 01:28:04.670406 31050 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0703 01:28:04.670409 31050 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0703 01:28:04.670413 31050 net.cpp:148] Setting up res5a_branch2a/relu
I0703 01:28:04.670415 31050 net.cpp:155] Top shape: 8 512 40 40 (6553600)
I0703 01:28:04.670418 31050 net.cpp:163] Memory required for data: 1461452800
I0703 01:28:04.670419 31050 layer_factory.hpp:77] Creating layer res5a_branch2b
I0703 01:28:04.670424 31050 net.cpp:98] Creating Layer res5a_branch2b
I0703 01:28:04.670426 31050 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0703 01:28:04.670428 31050 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0703 01:28:04.683050 31050 net.cpp:148] Setting up res5a_branch2b
I0703 01:28:04.683059 31050 net.cpp:155] Top shape: 8 512 40 40 (6553600)
I0703 01:28:04.683063 31050 net.cpp:163] Memory required for data: 1487667200
I0703 01:28:04.683069 31050 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0703 01:28:04.683074 31050 net.cpp:98] Creating Layer res5a_branch2b/bn
I0703 01:28:04.683076 31050 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0703 01:28:04.683079 31050 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0703 01:28:04.683534 31050 net.cpp:148] Setting up res5a_branch2b/bn
I0703 01:28:04.683550 31050 net.cpp:155] Top shape: 8 512 40 40 (6553600)
I0703 01:28:04.683552 31050 net.cpp:163] Memory required for data: 1513881600
I0703 01:28:04.683557 31050 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0703 01:28:04.683562 31050 net.cpp:98] Creating Layer res5a_branch2b/relu
I0703 01:28:04.683563 31050 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0703 01:28:04.683565 31050 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0703 01:28:04.683569 31050 net.cpp:148] Setting up res5a_branch2b/relu
I0703 01:28:04.683571 31050 net.cpp:155] Top shape: 8 512 40 40 (6553600)
I0703 01:28:04.683573 31050 net.cpp:163] Memory required for data: 1540096000
I0703 01:28:04.683575 31050 layer_factory.hpp:77] Creating layer out5a
I0703 01:28:04.683579 31050 net.cpp:98] Creating Layer out5a
I0703 01:28:04.683581 31050 net.cpp:439] out5a <- res5a_branch2b/bn
I0703 01:28:04.683584 31050 net.cpp:413] out5a -> out5a
I0703 01:28:04.687404 31050 net.cpp:148] Setting up out5a
I0703 01:28:04.687412 31050 net.cpp:155] Top shape: 8 64 40 40 (819200)
I0703 01:28:04.687414 31050 net.cpp:163] Memory required for data: 1543372800
I0703 01:28:04.687418 31050 layer_factory.hpp:77] Creating layer out5a/bn
I0703 01:28:04.687422 31050 net.cpp:98] Creating Layer out5a/bn
I0703 01:28:04.687425 31050 net.cpp:439] out5a/bn <- out5a
I0703 01:28:04.687428 31050 net.cpp:413] out5a/bn -> out5a/bn
I0703 01:28:04.687930 31050 net.cpp:148] Setting up out5a/bn
I0703 01:28:04.687937 31050 net.cpp:155] Top shape: 8 64 40 40 (819200)
I0703 01:28:04.687939 31050 net.cpp:163] Memory required for data: 1546649600
I0703 01:28:04.687944 31050 layer_factory.hpp:77] Creating layer out5a/relu
I0703 01:28:04.687947 31050 net.cpp:98] Creating Layer out5a/relu
I0703 01:28:04.687950 31050 net.cpp:439] out5a/relu <- out5a/bn
I0703 01:28:04.687952 31050 net.cpp:400] out5a/relu -> out5a/bn (in-place)
I0703 01:28:04.687958 31050 net.cpp:148] Setting up out5a/relu
I0703 01:28:04.687960 31050 net.cpp:155] Top shape: 8 64 40 40 (819200)
I0703 01:28:04.687963 31050 net.cpp:163] Memory required for data: 1549926400
I0703 01:28:04.687964 31050 layer_factory.hpp:77] Creating layer out5a_up2
I0703 01:28:04.687973 31050 net.cpp:98] Creating Layer out5a_up2
I0703 01:28:04.687975 31050 net.cpp:439] out5a_up2 <- out5a/bn
I0703 01:28:04.687978 31050 net.cpp:413] out5a_up2 -> out5a_up2
I0703 01:28:04.688159 31050 net.cpp:148] Setting up out5a_up2
I0703 01:28:04.688163 31050 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0703 01:28:04.688165 31050 net.cpp:163] Memory required for data: 1563033600
I0703 01:28:04.688169 31050 layer_factory.hpp:77] Creating layer out3a
I0703 01:28:04.688172 31050 net.cpp:98] Creating Layer out3a
I0703 01:28:04.688175 31050 net.cpp:439] out3a <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0703 01:28:04.688177 31050 net.cpp:413] out3a -> out3a
I0703 01:28:04.689134 31050 net.cpp:148] Setting up out3a
I0703 01:28:04.689141 31050 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0703 01:28:04.689143 31050 net.cpp:163] Memory required for data: 1576140800
I0703 01:28:04.689146 31050 layer_factory.hpp:77] Creating layer out3a/bn
I0703 01:28:04.689151 31050 net.cpp:98] Creating Layer out3a/bn
I0703 01:28:04.689153 31050 net.cpp:439] out3a/bn <- out3a
I0703 01:28:04.689155 31050 net.cpp:413] out3a/bn -> out3a/bn
I0703 01:28:04.689707 31050 net.cpp:148] Setting up out3a/bn
I0703 01:28:04.689713 31050 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0703 01:28:04.689715 31050 net.cpp:163] Memory required for data: 1589248000
I0703 01:28:04.689720 31050 layer_factory.hpp:77] Creating layer out3a/relu
I0703 01:28:04.689723 31050 net.cpp:98] Creating Layer out3a/relu
I0703 01:28:04.689725 31050 net.cpp:439] out3a/relu <- out3a/bn
I0703 01:28:04.689728 31050 net.cpp:400] out3a/relu -> out3a/bn (in-place)
I0703 01:28:04.689731 31050 net.cpp:148] Setting up out3a/relu
I0703 01:28:04.689733 31050 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0703 01:28:04.689735 31050 net.cpp:163] Memory required for data: 1602355200
I0703 01:28:04.689738 31050 layer_factory.hpp:77] Creating layer out3_out5_combined
I0703 01:28:04.689750 31050 net.cpp:98] Creating Layer out3_out5_combined
I0703 01:28:04.689754 31050 net.cpp:439] out3_out5_combined <- out5a_up2
I0703 01:28:04.689755 31050 net.cpp:439] out3_out5_combined <- out3a/bn
I0703 01:28:04.689757 31050 net.cpp:413] out3_out5_combined -> out3_out5_combined
I0703 01:28:04.689775 31050 net.cpp:148] Setting up out3_out5_combined
I0703 01:28:04.689779 31050 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0703 01:28:04.689779 31050 net.cpp:163] Memory required for data: 1615462400
I0703 01:28:04.689781 31050 layer_factory.hpp:77] Creating layer ctx_conv1
I0703 01:28:04.689785 31050 net.cpp:98] Creating Layer ctx_conv1
I0703 01:28:04.689787 31050 net.cpp:439] ctx_conv1 <- out3_out5_combined
I0703 01:28:04.689791 31050 net.cpp:413] ctx_conv1 -> ctx_conv1
I0703 01:28:04.690776 31050 net.cpp:148] Setting up ctx_conv1
I0703 01:28:04.690783 31050 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0703 01:28:04.690784 31050 net.cpp:163] Memory required for data: 1628569600
I0703 01:28:04.690788 31050 layer_factory.hpp:77] Creating layer ctx_conv1/bn
I0703 01:28:04.690791 31050 net.cpp:98] Creating Layer ctx_conv1/bn
I0703 01:28:04.690793 31050 net.cpp:439] ctx_conv1/bn <- ctx_conv1
I0703 01:28:04.690796 31050 net.cpp:413] ctx_conv1/bn -> ctx_conv1/bn
I0703 01:28:04.691311 31050 net.cpp:148] Setting up ctx_conv1/bn
I0703 01:28:04.691318 31050 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0703 01:28:04.691319 31050 net.cpp:163] Memory required for data: 1641676800
I0703 01:28:04.691324 31050 layer_factory.hpp:77] Creating layer ctx_conv1/relu
I0703 01:28:04.691328 31050 net.cpp:98] Creating Layer ctx_conv1/relu
I0703 01:28:04.691330 31050 net.cpp:439] ctx_conv1/relu <- ctx_conv1/bn
I0703 01:28:04.691332 31050 net.cpp:400] ctx_conv1/relu -> ctx_conv1/bn (in-place)
I0703 01:28:04.691335 31050 net.cpp:148] Setting up ctx_conv1/relu
I0703 01:28:04.691339 31050 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0703 01:28:04.691339 31050 net.cpp:163] Memory required for data: 1654784000
I0703 01:28:04.691341 31050 layer_factory.hpp:77] Creating layer ctx_conv2
I0703 01:28:04.691345 31050 net.cpp:98] Creating Layer ctx_conv2
I0703 01:28:04.691349 31050 net.cpp:439] ctx_conv2 <- ctx_conv1/bn
I0703 01:28:04.691350 31050 net.cpp:413] ctx_conv2 -> ctx_conv2
I0703 01:28:04.692312 31050 net.cpp:148] Setting up ctx_conv2
I0703 01:28:04.692317 31050 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0703 01:28:04.692319 31050 net.cpp:163] Memory required for data: 1667891200
I0703 01:28:04.692323 31050 layer_factory.hpp:77] Creating layer ctx_conv2/bn
I0703 01:28:04.692327 31050 net.cpp:98] Creating Layer ctx_conv2/bn
I0703 01:28:04.692328 31050 net.cpp:439] ctx_conv2/bn <- ctx_conv2
I0703 01:28:04.692332 31050 net.cpp:413] ctx_conv2/bn -> ctx_conv2/bn
I0703 01:28:04.692922 31050 net.cpp:148] Setting up ctx_conv2/bn
I0703 01:28:04.692929 31050 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0703 01:28:04.692931 31050 net.cpp:163] Memory required for data: 1680998400
I0703 01:28:04.692936 31050 layer_factory.hpp:77] Creating layer ctx_conv2/relu
I0703 01:28:04.692939 31050 net.cpp:98] Creating Layer ctx_conv2/relu
I0703 01:28:04.692940 31050 net.cpp:439] ctx_conv2/relu <- ctx_conv2/bn
I0703 01:28:04.692945 31050 net.cpp:400] ctx_conv2/relu -> ctx_conv2/bn (in-place)
I0703 01:28:04.692948 31050 net.cpp:148] Setting up ctx_conv2/relu
I0703 01:28:04.692950 31050 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0703 01:28:04.692952 31050 net.cpp:163] Memory required for data: 1694105600
I0703 01:28:04.692955 31050 layer_factory.hpp:77] Creating layer ctx_conv3
I0703 01:28:04.692958 31050 net.cpp:98] Creating Layer ctx_conv3
I0703 01:28:04.692960 31050 net.cpp:439] ctx_conv3 <- ctx_conv2/bn
I0703 01:28:04.692962 31050 net.cpp:413] ctx_conv3 -> ctx_conv3
I0703 01:28:04.693976 31050 net.cpp:148] Setting up ctx_conv3
I0703 01:28:04.693982 31050 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0703 01:28:04.693984 31050 net.cpp:163] Memory required for data: 1707212800
I0703 01:28:04.693994 31050 layer_factory.hpp:77] Creating layer ctx_conv3/bn
I0703 01:28:04.693997 31050 net.cpp:98] Creating Layer ctx_conv3/bn
I0703 01:28:04.694000 31050 net.cpp:439] ctx_conv3/bn <- ctx_conv3
I0703 01:28:04.694002 31050 net.cpp:413] ctx_conv3/bn -> ctx_conv3/bn
I0703 01:28:04.694576 31050 net.cpp:148] Setting up ctx_conv3/bn
I0703 01:28:04.694583 31050 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0703 01:28:04.694586 31050 net.cpp:163] Memory required for data: 1720320000
I0703 01:28:04.694591 31050 layer_factory.hpp:77] Creating layer ctx_conv3/relu
I0703 01:28:04.694593 31050 net.cpp:98] Creating Layer ctx_conv3/relu
I0703 01:28:04.694597 31050 net.cpp:439] ctx_conv3/relu <- ctx_conv3/bn
I0703 01:28:04.694598 31050 net.cpp:400] ctx_conv3/relu -> ctx_conv3/bn (in-place)
I0703 01:28:04.694602 31050 net.cpp:148] Setting up ctx_conv3/relu
I0703 01:28:04.694604 31050 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0703 01:28:04.694607 31050 net.cpp:163] Memory required for data: 1733427200
I0703 01:28:04.694608 31050 layer_factory.hpp:77] Creating layer ctx_conv4
I0703 01:28:04.694612 31050 net.cpp:98] Creating Layer ctx_conv4
I0703 01:28:04.694613 31050 net.cpp:439] ctx_conv4 <- ctx_conv3/bn
I0703 01:28:04.694617 31050 net.cpp:413] ctx_conv4 -> ctx_conv4
I0703 01:28:04.695638 31050 net.cpp:148] Setting up ctx_conv4
I0703 01:28:04.695644 31050 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0703 01:28:04.695647 31050 net.cpp:163] Memory required for data: 1746534400
I0703 01:28:04.695649 31050 layer_factory.hpp:77] Creating layer ctx_conv4/bn
I0703 01:28:04.695654 31050 net.cpp:98] Creating Layer ctx_conv4/bn
I0703 01:28:04.695657 31050 net.cpp:439] ctx_conv4/bn <- ctx_conv4
I0703 01:28:04.695660 31050 net.cpp:413] ctx_conv4/bn -> ctx_conv4/bn
I0703 01:28:04.696224 31050 net.cpp:148] Setting up ctx_conv4/bn
I0703 01:28:04.696230 31050 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0703 01:28:04.696233 31050 net.cpp:163] Memory required for data: 1759641600
I0703 01:28:04.696238 31050 layer_factory.hpp:77] Creating layer ctx_conv4/relu
I0703 01:28:04.696240 31050 net.cpp:98] Creating Layer ctx_conv4/relu
I0703 01:28:04.696243 31050 net.cpp:439] ctx_conv4/relu <- ctx_conv4/bn
I0703 01:28:04.696245 31050 net.cpp:400] ctx_conv4/relu -> ctx_conv4/bn (in-place)
I0703 01:28:04.696249 31050 net.cpp:148] Setting up ctx_conv4/relu
I0703 01:28:04.696250 31050 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0703 01:28:04.696252 31050 net.cpp:163] Memory required for data: 1772748800
I0703 01:28:04.696254 31050 layer_factory.hpp:77] Creating layer ctx_final
I0703 01:28:04.696259 31050 net.cpp:98] Creating Layer ctx_final
I0703 01:28:04.696260 31050 net.cpp:439] ctx_final <- ctx_conv4/bn
I0703 01:28:04.696262 31050 net.cpp:413] ctx_final -> ctx_final
I0703 01:28:04.696655 31050 net.cpp:148] Setting up ctx_final
I0703 01:28:04.696661 31050 net.cpp:155] Top shape: 8 8 80 80 (409600)
I0703 01:28:04.696663 31050 net.cpp:163] Memory required for data: 1774387200
I0703 01:28:04.696666 31050 layer_factory.hpp:77] Creating layer ctx_final/relu
I0703 01:28:04.696669 31050 net.cpp:98] Creating Layer ctx_final/relu
I0703 01:28:04.696671 31050 net.cpp:439] ctx_final/relu <- ctx_final
I0703 01:28:04.696674 31050 net.cpp:400] ctx_final/relu -> ctx_final (in-place)
I0703 01:28:04.696677 31050 net.cpp:148] Setting up ctx_final/relu
I0703 01:28:04.696681 31050 net.cpp:155] Top shape: 8 8 80 80 (409600)
I0703 01:28:04.696681 31050 net.cpp:163] Memory required for data: 1776025600
I0703 01:28:04.696683 31050 layer_factory.hpp:77] Creating layer out_deconv_final_up2
I0703 01:28:04.696686 31050 net.cpp:98] Creating Layer out_deconv_final_up2
I0703 01:28:04.696688 31050 net.cpp:439] out_deconv_final_up2 <- ctx_final
I0703 01:28:04.696691 31050 net.cpp:413] out_deconv_final_up2 -> out_deconv_final_up2
I0703 01:28:04.696899 31050 net.cpp:148] Setting up out_deconv_final_up2
I0703 01:28:04.696907 31050 net.cpp:155] Top shape: 8 8 160 160 (1638400)
I0703 01:28:04.696910 31050 net.cpp:163] Memory required for data: 1782579200
I0703 01:28:04.696915 31050 layer_factory.hpp:77] Creating layer out_deconv_final_up4
I0703 01:28:04.696929 31050 net.cpp:98] Creating Layer out_deconv_final_up4
I0703 01:28:04.696933 31050 net.cpp:439] out_deconv_final_up4 <- out_deconv_final_up2
I0703 01:28:04.696938 31050 net.cpp:413] out_deconv_final_up4 -> out_deconv_final_up4
I0703 01:28:04.697160 31050 net.cpp:148] Setting up out_deconv_final_up4
I0703 01:28:04.697167 31050 net.cpp:155] Top shape: 8 8 320 320 (6553600)
I0703 01:28:04.697170 31050 net.cpp:163] Memory required for data: 1808793600
I0703 01:28:04.697175 31050 layer_factory.hpp:77] Creating layer out_deconv_final_up8
I0703 01:28:04.697180 31050 net.cpp:98] Creating Layer out_deconv_final_up8
I0703 01:28:04.697182 31050 net.cpp:439] out_deconv_final_up8 <- out_deconv_final_up4
I0703 01:28:04.697187 31050 net.cpp:413] out_deconv_final_up8 -> out_deconv_final_up8
I0703 01:28:04.697401 31050 net.cpp:148] Setting up out_deconv_final_up8
I0703 01:28:04.697407 31050 net.cpp:155] Top shape: 8 8 640 640 (26214400)
I0703 01:28:04.697410 31050 net.cpp:163] Memory required for data: 1913651200
I0703 01:28:04.697414 31050 layer_factory.hpp:77] Creating layer loss
I0703 01:28:04.697419 31050 net.cpp:98] Creating Layer loss
I0703 01:28:04.697422 31050 net.cpp:439] loss <- out_deconv_final_up8
I0703 01:28:04.697427 31050 net.cpp:439] loss <- label
I0703 01:28:04.697432 31050 net.cpp:413] loss -> loss
I0703 01:28:04.697443 31050 layer_factory.hpp:77] Creating layer loss
I0703 01:28:04.729910 31050 net.cpp:148] Setting up loss
I0703 01:28:04.729934 31050 net.cpp:155] Top shape: (1)
I0703 01:28:04.729936 31050 net.cpp:158]     with loss weight 1
I0703 01:28:04.729949 31050 net.cpp:163] Memory required for data: 1913651204
I0703 01:28:04.729954 31050 net.cpp:224] loss needs backward computation.
I0703 01:28:04.729956 31050 net.cpp:224] out_deconv_final_up8 needs backward computation.
I0703 01:28:04.729959 31050 net.cpp:224] out_deconv_final_up4 needs backward computation.
I0703 01:28:04.729960 31050 net.cpp:224] out_deconv_final_up2 needs backward computation.
I0703 01:28:04.729962 31050 net.cpp:224] ctx_final/relu needs backward computation.
I0703 01:28:04.729964 31050 net.cpp:224] ctx_final needs backward computation.
I0703 01:28:04.729966 31050 net.cpp:224] ctx_conv4/relu needs backward computation.
I0703 01:28:04.729969 31050 net.cpp:224] ctx_conv4/bn needs backward computation.
I0703 01:28:04.729970 31050 net.cpp:224] ctx_conv4 needs backward computation.
I0703 01:28:04.729972 31050 net.cpp:224] ctx_conv3/relu needs backward computation.
I0703 01:28:04.729974 31050 net.cpp:224] ctx_conv3/bn needs backward computation.
I0703 01:28:04.729977 31050 net.cpp:224] ctx_conv3 needs backward computation.
I0703 01:28:04.729980 31050 net.cpp:224] ctx_conv2/relu needs backward computation.
I0703 01:28:04.729982 31050 net.cpp:224] ctx_conv2/bn needs backward computation.
I0703 01:28:04.729984 31050 net.cpp:224] ctx_conv2 needs backward computation.
I0703 01:28:04.729986 31050 net.cpp:224] ctx_conv1/relu needs backward computation.
I0703 01:28:04.729988 31050 net.cpp:224] ctx_conv1/bn needs backward computation.
I0703 01:28:04.729990 31050 net.cpp:224] ctx_conv1 needs backward computation.
I0703 01:28:04.729993 31050 net.cpp:224] out3_out5_combined needs backward computation.
I0703 01:28:04.729995 31050 net.cpp:224] out3a/relu needs backward computation.
I0703 01:28:04.729997 31050 net.cpp:224] out3a/bn needs backward computation.
I0703 01:28:04.730000 31050 net.cpp:224] out3a needs backward computation.
I0703 01:28:04.730002 31050 net.cpp:224] out5a_up2 needs backward computation.
I0703 01:28:04.730005 31050 net.cpp:224] out5a/relu needs backward computation.
I0703 01:28:04.730007 31050 net.cpp:224] out5a/bn needs backward computation.
I0703 01:28:04.730010 31050 net.cpp:224] out5a needs backward computation.
I0703 01:28:04.730012 31050 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0703 01:28:04.730015 31050 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0703 01:28:04.730016 31050 net.cpp:224] res5a_branch2b needs backward computation.
I0703 01:28:04.730031 31050 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0703 01:28:04.730032 31050 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0703 01:28:04.730034 31050 net.cpp:224] res5a_branch2a needs backward computation.
I0703 01:28:04.730036 31050 net.cpp:224] pool4 needs backward computation.
I0703 01:28:04.730039 31050 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0703 01:28:04.730041 31050 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0703 01:28:04.730043 31050 net.cpp:224] res4a_branch2b needs backward computation.
I0703 01:28:04.730046 31050 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0703 01:28:04.730049 31050 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0703 01:28:04.730051 31050 net.cpp:224] res4a_branch2a needs backward computation.
I0703 01:28:04.730053 31050 net.cpp:224] pool3 needs backward computation.
I0703 01:28:04.730056 31050 net.cpp:224] res3a_branch2b/bn_res3a_branch2b/relu_0_split needs backward computation.
I0703 01:28:04.730058 31050 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0703 01:28:04.730068 31050 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0703 01:28:04.730070 31050 net.cpp:224] res3a_branch2b needs backward computation.
I0703 01:28:04.730073 31050 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0703 01:28:04.730077 31050 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0703 01:28:04.730079 31050 net.cpp:224] res3a_branch2a needs backward computation.
I0703 01:28:04.730082 31050 net.cpp:224] pool2 needs backward computation.
I0703 01:28:04.730085 31050 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0703 01:28:04.730088 31050 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0703 01:28:04.730090 31050 net.cpp:224] res2a_branch2b needs backward computation.
I0703 01:28:04.730093 31050 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0703 01:28:04.730095 31050 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0703 01:28:04.730098 31050 net.cpp:224] res2a_branch2a needs backward computation.
I0703 01:28:04.730099 31050 net.cpp:224] pool1 needs backward computation.
I0703 01:28:04.730103 31050 net.cpp:224] conv1b/relu needs backward computation.
I0703 01:28:04.730105 31050 net.cpp:224] conv1b/bn needs backward computation.
I0703 01:28:04.730108 31050 net.cpp:224] conv1b needs backward computation.
I0703 01:28:04.730110 31050 net.cpp:224] conv1a/relu needs backward computation.
I0703 01:28:04.730111 31050 net.cpp:224] conv1a/bn needs backward computation.
I0703 01:28:04.730113 31050 net.cpp:224] conv1a needs backward computation.
I0703 01:28:04.730116 31050 net.cpp:226] data/bias does not need backward computation.
I0703 01:28:04.730119 31050 net.cpp:226] data does not need backward computation.
I0703 01:28:04.730121 31050 net.cpp:268] This network produces output loss
I0703 01:28:04.730151 31050 net.cpp:288] Network initialization done.
I0703 01:28:04.730891 31050 solver.cpp:182] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-07-02_23-02-42/sparse/test.prototxt
I0703 01:28:04.731187 31050 net.cpp:56] Initializing net from parameters: 
name: "jsegnet21v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/val-image-lmdb"
    label_list_path: "data/val-label-lmdb"
    batch_size: 4
    threads: 4
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b/bn"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a/bn"
  top: "out5a/bn"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a/bn"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b/bn"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a/bn"
  top: "out3a/bn"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a/bn"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv1/bn"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv2/bn"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv3/bn"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4/bn"
  top: "ctx_conv4/bn"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4/bn"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
  accuracy_param {
    ignore_label: 255
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
    ignore_label: 255
  }
}
I0703 01:28:04.731325 31050 layer_factory.hpp:77] Creating layer data
I0703 01:28:04.731335 31050 net.cpp:98] Creating Layer data
I0703 01:28:04.731340 31050 net.cpp:413] data -> data
I0703 01:28:04.731348 31050 net.cpp:413] data -> label
I0703 01:28:04.732861 31135 db_lmdb.cpp:35] Opened lmdb data/val-label-lmdb
I0703 01:28:04.761632 31130 db_lmdb.cpp:35] Opened lmdb data/val-image-lmdb
I0703 01:28:04.771430 31050 data_layer.cpp:78] ReshapePrefetch 4, 3, 640, 640
I0703 01:28:04.771621 31050 data_layer.cpp:83] output data size: 4,3,640,640
I0703 01:28:04.803767 31050 data_layer.cpp:78] ReshapePrefetch 4, 1, 640, 640
I0703 01:28:04.803853 31050 data_layer.cpp:83] output data size: 4,1,640,640
I0703 01:28:04.816761 31050 net.cpp:148] Setting up data
I0703 01:28:04.816793 31050 net.cpp:155] Top shape: 4 3 640 640 (4915200)
I0703 01:28:04.816798 31050 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0703 01:28:04.816802 31050 net.cpp:163] Memory required for data: 26214400
I0703 01:28:04.816835 31050 layer_factory.hpp:77] Creating layer label_data_1_split
I0703 01:28:04.816850 31050 net.cpp:98] Creating Layer label_data_1_split
I0703 01:28:04.816855 31050 net.cpp:439] label_data_1_split <- label
I0703 01:28:04.816862 31050 net.cpp:413] label_data_1_split -> label_data_1_split_0
I0703 01:28:04.816870 31050 net.cpp:413] label_data_1_split -> label_data_1_split_1
I0703 01:28:04.816874 31050 net.cpp:413] label_data_1_split -> label_data_1_split_2
I0703 01:28:04.817032 31050 net.cpp:148] Setting up label_data_1_split
I0703 01:28:04.817041 31050 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0703 01:28:04.817045 31050 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0703 01:28:04.817049 31050 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0703 01:28:04.817052 31050 net.cpp:163] Memory required for data: 45875200
I0703 01:28:04.817055 31050 layer_factory.hpp:77] Creating layer data/bias
I0703 01:28:04.817065 31050 net.cpp:98] Creating Layer data/bias
I0703 01:28:04.817068 31050 net.cpp:439] data/bias <- data
I0703 01:28:04.817075 31050 net.cpp:413] data/bias -> data/bias
I0703 01:28:04.818848 31050 net.cpp:148] Setting up data/bias
I0703 01:28:04.818877 31050 net.cpp:155] Top shape: 4 3 640 640 (4915200)
I0703 01:28:04.818881 31050 net.cpp:163] Memory required for data: 65536000
I0703 01:28:04.818894 31050 layer_factory.hpp:77] Creating layer conv1a
I0703 01:28:04.818914 31050 net.cpp:98] Creating Layer conv1a
I0703 01:28:04.818919 31050 net.cpp:439] conv1a <- data/bias
I0703 01:28:04.818925 31050 net.cpp:413] conv1a -> conv1a
I0703 01:28:04.819674 31050 net.cpp:148] Setting up conv1a
I0703 01:28:04.819689 31050 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0703 01:28:04.819691 31050 net.cpp:163] Memory required for data: 117964800
I0703 01:28:04.819700 31050 layer_factory.hpp:77] Creating layer conv1a/bn
I0703 01:28:04.819713 31050 net.cpp:98] Creating Layer conv1a/bn
I0703 01:28:04.819716 31050 net.cpp:439] conv1a/bn <- conv1a
I0703 01:28:04.819726 31050 net.cpp:413] conv1a/bn -> conv1a/bn
I0703 01:28:04.821094 31050 net.cpp:148] Setting up conv1a/bn
I0703 01:28:04.821117 31050 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0703 01:28:04.821120 31050 net.cpp:163] Memory required for data: 170393600
I0703 01:28:04.821133 31050 layer_factory.hpp:77] Creating layer conv1a/relu
I0703 01:28:04.821153 31050 net.cpp:98] Creating Layer conv1a/relu
I0703 01:28:04.821175 31050 net.cpp:439] conv1a/relu <- conv1a/bn
I0703 01:28:04.821192 31050 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0703 01:28:04.821210 31050 net.cpp:148] Setting up conv1a/relu
I0703 01:28:04.821221 31050 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0703 01:28:04.821230 31050 net.cpp:163] Memory required for data: 222822400
I0703 01:28:04.821240 31050 layer_factory.hpp:77] Creating layer conv1b
I0703 01:28:04.821260 31050 net.cpp:98] Creating Layer conv1b
I0703 01:28:04.821269 31050 net.cpp:439] conv1b <- conv1a/bn
I0703 01:28:04.821288 31050 net.cpp:413] conv1b -> conv1b
I0703 01:28:04.821844 31050 net.cpp:148] Setting up conv1b
I0703 01:28:04.821852 31050 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0703 01:28:04.821854 31050 net.cpp:163] Memory required for data: 275251200
I0703 01:28:04.821861 31050 layer_factory.hpp:77] Creating layer conv1b/bn
I0703 01:28:04.821872 31050 net.cpp:98] Creating Layer conv1b/bn
I0703 01:28:04.821877 31050 net.cpp:439] conv1b/bn <- conv1b
I0703 01:28:04.821882 31050 net.cpp:413] conv1b/bn -> conv1b/bn
I0703 01:28:04.822856 31050 net.cpp:148] Setting up conv1b/bn
I0703 01:28:04.822880 31050 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0703 01:28:04.822885 31050 net.cpp:163] Memory required for data: 327680000
I0703 01:28:04.822896 31050 layer_factory.hpp:77] Creating layer conv1b/relu
I0703 01:28:04.822906 31050 net.cpp:98] Creating Layer conv1b/relu
I0703 01:28:04.822911 31050 net.cpp:439] conv1b/relu <- conv1b/bn
I0703 01:28:04.822916 31050 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0703 01:28:04.822922 31050 net.cpp:148] Setting up conv1b/relu
I0703 01:28:04.822948 31050 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0703 01:28:04.822952 31050 net.cpp:163] Memory required for data: 380108800
I0703 01:28:04.822957 31050 layer_factory.hpp:77] Creating layer pool1
I0703 01:28:04.822966 31050 net.cpp:98] Creating Layer pool1
I0703 01:28:04.822970 31050 net.cpp:439] pool1 <- conv1b/bn
I0703 01:28:04.822976 31050 net.cpp:413] pool1 -> pool1
I0703 01:28:04.823024 31050 net.cpp:148] Setting up pool1
I0703 01:28:04.823030 31050 net.cpp:155] Top shape: 4 32 160 160 (3276800)
I0703 01:28:04.823035 31050 net.cpp:163] Memory required for data: 393216000
I0703 01:28:04.823038 31050 layer_factory.hpp:77] Creating layer res2a_branch2a
I0703 01:28:04.823047 31050 net.cpp:98] Creating Layer res2a_branch2a
I0703 01:28:04.823051 31050 net.cpp:439] res2a_branch2a <- pool1
I0703 01:28:04.823057 31050 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0703 01:28:04.824012 31050 net.cpp:148] Setting up res2a_branch2a
I0703 01:28:04.824026 31050 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0703 01:28:04.824031 31050 net.cpp:163] Memory required for data: 419430400
I0703 01:28:04.824041 31050 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0703 01:28:04.824049 31050 net.cpp:98] Creating Layer res2a_branch2a/bn
I0703 01:28:04.824054 31050 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0703 01:28:04.824060 31050 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0703 01:28:04.824805 31050 net.cpp:148] Setting up res2a_branch2a/bn
I0703 01:28:04.824813 31050 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0703 01:28:04.824817 31050 net.cpp:163] Memory required for data: 445644800
I0703 01:28:04.824832 31050 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0703 01:28:04.824839 31050 net.cpp:98] Creating Layer res2a_branch2a/relu
I0703 01:28:04.824844 31050 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0703 01:28:04.824849 31050 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0703 01:28:04.824856 31050 net.cpp:148] Setting up res2a_branch2a/relu
I0703 01:28:04.824862 31050 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0703 01:28:04.824867 31050 net.cpp:163] Memory required for data: 471859200
I0703 01:28:04.824870 31050 layer_factory.hpp:77] Creating layer res2a_branch2b
I0703 01:28:04.824878 31050 net.cpp:98] Creating Layer res2a_branch2b
I0703 01:28:04.824882 31050 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0703 01:28:04.824888 31050 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0703 01:28:04.825492 31050 net.cpp:148] Setting up res2a_branch2b
I0703 01:28:04.825501 31050 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0703 01:28:04.825505 31050 net.cpp:163] Memory required for data: 498073600
I0703 01:28:04.825510 31050 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0703 01:28:04.825517 31050 net.cpp:98] Creating Layer res2a_branch2b/bn
I0703 01:28:04.825522 31050 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0703 01:28:04.825526 31050 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0703 01:28:04.826239 31050 net.cpp:148] Setting up res2a_branch2b/bn
I0703 01:28:04.826246 31050 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0703 01:28:04.826249 31050 net.cpp:163] Memory required for data: 524288000
I0703 01:28:04.826256 31050 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0703 01:28:04.826264 31050 net.cpp:98] Creating Layer res2a_branch2b/relu
I0703 01:28:04.826268 31050 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0703 01:28:04.826274 31050 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0703 01:28:04.826280 31050 net.cpp:148] Setting up res2a_branch2b/relu
I0703 01:28:04.826285 31050 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0703 01:28:04.826289 31050 net.cpp:163] Memory required for data: 550502400
I0703 01:28:04.826293 31050 layer_factory.hpp:77] Creating layer pool2
I0703 01:28:04.826300 31050 net.cpp:98] Creating Layer pool2
I0703 01:28:04.826304 31050 net.cpp:439] pool2 <- res2a_branch2b/bn
I0703 01:28:04.826309 31050 net.cpp:413] pool2 -> pool2
I0703 01:28:04.826361 31050 net.cpp:148] Setting up pool2
I0703 01:28:04.826369 31050 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 01:28:04.826372 31050 net.cpp:163] Memory required for data: 557056000
I0703 01:28:04.826375 31050 layer_factory.hpp:77] Creating layer res3a_branch2a
I0703 01:28:04.826382 31050 net.cpp:98] Creating Layer res3a_branch2a
I0703 01:28:04.826390 31050 net.cpp:439] res3a_branch2a <- pool2
I0703 01:28:04.826396 31050 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0703 01:28:04.829180 31050 net.cpp:148] Setting up res3a_branch2a
I0703 01:28:04.829195 31050 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0703 01:28:04.829197 31050 net.cpp:163] Memory required for data: 570163200
I0703 01:28:04.829205 31050 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0703 01:28:04.829213 31050 net.cpp:98] Creating Layer res3a_branch2a/bn
I0703 01:28:04.829221 31050 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0703 01:28:04.829228 31050 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0703 01:28:04.830379 31050 net.cpp:148] Setting up res3a_branch2a/bn
I0703 01:28:04.830410 31050 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0703 01:28:04.830416 31050 net.cpp:163] Memory required for data: 583270400
I0703 01:28:04.830430 31050 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0703 01:28:04.830438 31050 net.cpp:98] Creating Layer res3a_branch2a/relu
I0703 01:28:04.830443 31050 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0703 01:28:04.830449 31050 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0703 01:28:04.830456 31050 net.cpp:148] Setting up res3a_branch2a/relu
I0703 01:28:04.830461 31050 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0703 01:28:04.830463 31050 net.cpp:163] Memory required for data: 596377600
I0703 01:28:04.830467 31050 layer_factory.hpp:77] Creating layer res3a_branch2b
I0703 01:28:04.830476 31050 net.cpp:98] Creating Layer res3a_branch2b
I0703 01:28:04.830480 31050 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0703 01:28:04.830484 31050 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0703 01:28:04.831899 31050 net.cpp:148] Setting up res3a_branch2b
I0703 01:28:04.831910 31050 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0703 01:28:04.831913 31050 net.cpp:163] Memory required for data: 609484800
I0703 01:28:04.831919 31050 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0703 01:28:04.831925 31050 net.cpp:98] Creating Layer res3a_branch2b/bn
I0703 01:28:04.831928 31050 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0703 01:28:04.831933 31050 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0703 01:28:04.832545 31050 net.cpp:148] Setting up res3a_branch2b/bn
I0703 01:28:04.832551 31050 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0703 01:28:04.832553 31050 net.cpp:163] Memory required for data: 622592000
I0703 01:28:04.832558 31050 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0703 01:28:04.832561 31050 net.cpp:98] Creating Layer res3a_branch2b/relu
I0703 01:28:04.832563 31050 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0703 01:28:04.832566 31050 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0703 01:28:04.832569 31050 net.cpp:148] Setting up res3a_branch2b/relu
I0703 01:28:04.832571 31050 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0703 01:28:04.832573 31050 net.cpp:163] Memory required for data: 635699200
I0703 01:28:04.832576 31050 layer_factory.hpp:77] Creating layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0703 01:28:04.832579 31050 net.cpp:98] Creating Layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0703 01:28:04.832581 31050 net.cpp:439] res3a_branch2b/bn_res3a_branch2b/relu_0_split <- res3a_branch2b/bn
I0703 01:28:04.832583 31050 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0703 01:28:04.832587 31050 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0703 01:28:04.832624 31050 net.cpp:148] Setting up res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0703 01:28:04.832644 31050 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0703 01:28:04.832648 31050 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0703 01:28:04.832648 31050 net.cpp:163] Memory required for data: 661913600
I0703 01:28:04.832650 31050 layer_factory.hpp:77] Creating layer pool3
I0703 01:28:04.832656 31050 net.cpp:98] Creating Layer pool3
I0703 01:28:04.832659 31050 net.cpp:439] pool3 <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0703 01:28:04.832664 31050 net.cpp:413] pool3 -> pool3
I0703 01:28:04.832698 31050 net.cpp:148] Setting up pool3
I0703 01:28:04.832717 31050 net.cpp:155] Top shape: 4 128 40 40 (819200)
I0703 01:28:04.832722 31050 net.cpp:163] Memory required for data: 665190400
I0703 01:28:04.832726 31050 layer_factory.hpp:77] Creating layer res4a_branch2a
I0703 01:28:04.832736 31050 net.cpp:98] Creating Layer res4a_branch2a
I0703 01:28:04.832741 31050 net.cpp:439] res4a_branch2a <- pool3
I0703 01:28:04.832746 31050 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0703 01:28:04.841529 31050 net.cpp:148] Setting up res4a_branch2a
I0703 01:28:04.841632 31050 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0703 01:28:04.841646 31050 net.cpp:163] Memory required for data: 671744000
I0703 01:28:04.841665 31050 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0703 01:28:04.841686 31050 net.cpp:98] Creating Layer res4a_branch2a/bn
I0703 01:28:04.841696 31050 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0703 01:28:04.841709 31050 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0703 01:28:04.842458 31050 net.cpp:148] Setting up res4a_branch2a/bn
I0703 01:28:04.842479 31050 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0703 01:28:04.842488 31050 net.cpp:163] Memory required for data: 678297600
I0703 01:28:04.842499 31050 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0703 01:28:04.842509 31050 net.cpp:98] Creating Layer res4a_branch2a/relu
I0703 01:28:04.842516 31050 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0703 01:28:04.842525 31050 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0703 01:28:04.842536 31050 net.cpp:148] Setting up res4a_branch2a/relu
I0703 01:28:04.842545 31050 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0703 01:28:04.842551 31050 net.cpp:163] Memory required for data: 684851200
I0703 01:28:04.842558 31050 layer_factory.hpp:77] Creating layer res4a_branch2b
I0703 01:28:04.842571 31050 net.cpp:98] Creating Layer res4a_branch2b
I0703 01:28:04.842577 31050 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0703 01:28:04.842586 31050 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0703 01:28:04.846402 31050 net.cpp:148] Setting up res4a_branch2b
I0703 01:28:04.846436 31050 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0703 01:28:04.846439 31050 net.cpp:163] Memory required for data: 691404800
I0703 01:28:04.846448 31050 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0703 01:28:04.846463 31050 net.cpp:98] Creating Layer res4a_branch2b/bn
I0703 01:28:04.846468 31050 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0703 01:28:04.846478 31050 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0703 01:28:04.847033 31050 net.cpp:148] Setting up res4a_branch2b/bn
I0703 01:28:04.847040 31050 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0703 01:28:04.847043 31050 net.cpp:163] Memory required for data: 697958400
I0703 01:28:04.847051 31050 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0703 01:28:04.847056 31050 net.cpp:98] Creating Layer res4a_branch2b/relu
I0703 01:28:04.847060 31050 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0703 01:28:04.847065 31050 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0703 01:28:04.847071 31050 net.cpp:148] Setting up res4a_branch2b/relu
I0703 01:28:04.847076 31050 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0703 01:28:04.847079 31050 net.cpp:163] Memory required for data: 704512000
I0703 01:28:04.847084 31050 layer_factory.hpp:77] Creating layer pool4
I0703 01:28:04.847091 31050 net.cpp:98] Creating Layer pool4
I0703 01:28:04.847095 31050 net.cpp:439] pool4 <- res4a_branch2b/bn
I0703 01:28:04.847112 31050 net.cpp:413] pool4 -> pool4
I0703 01:28:04.847151 31050 net.cpp:148] Setting up pool4
I0703 01:28:04.847157 31050 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0703 01:28:04.847160 31050 net.cpp:163] Memory required for data: 711065600
I0703 01:28:04.847164 31050 layer_factory.hpp:77] Creating layer res5a_branch2a
I0703 01:28:04.847174 31050 net.cpp:98] Creating Layer res5a_branch2a
I0703 01:28:04.847185 31050 net.cpp:439] res5a_branch2a <- pool4
I0703 01:28:04.847192 31050 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0703 01:28:04.874207 31050 net.cpp:148] Setting up res5a_branch2a
I0703 01:28:04.874224 31050 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0703 01:28:04.874228 31050 net.cpp:163] Memory required for data: 724172800
I0703 01:28:04.874234 31050 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0703 01:28:04.874244 31050 net.cpp:98] Creating Layer res5a_branch2a/bn
I0703 01:28:04.874248 31050 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0703 01:28:04.874254 31050 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0703 01:28:04.874765 31050 net.cpp:148] Setting up res5a_branch2a/bn
I0703 01:28:04.874773 31050 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0703 01:28:04.874776 31050 net.cpp:163] Memory required for data: 737280000
I0703 01:28:04.874784 31050 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0703 01:28:04.874789 31050 net.cpp:98] Creating Layer res5a_branch2a/relu
I0703 01:28:04.874794 31050 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0703 01:28:04.874799 31050 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0703 01:28:04.874805 31050 net.cpp:148] Setting up res5a_branch2a/relu
I0703 01:28:04.874811 31050 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0703 01:28:04.874814 31050 net.cpp:163] Memory required for data: 750387200
I0703 01:28:04.874816 31050 layer_factory.hpp:77] Creating layer res5a_branch2b
I0703 01:28:04.874821 31050 net.cpp:98] Creating Layer res5a_branch2b
I0703 01:28:04.874824 31050 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0703 01:28:04.874830 31050 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0703 01:28:04.887445 31050 net.cpp:148] Setting up res5a_branch2b
I0703 01:28:04.887456 31050 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0703 01:28:04.887459 31050 net.cpp:163] Memory required for data: 763494400
I0703 01:28:04.887465 31050 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0703 01:28:04.887470 31050 net.cpp:98] Creating Layer res5a_branch2b/bn
I0703 01:28:04.887473 31050 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0703 01:28:04.887476 31050 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0703 01:28:04.887969 31050 net.cpp:148] Setting up res5a_branch2b/bn
I0703 01:28:04.887974 31050 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0703 01:28:04.887975 31050 net.cpp:163] Memory required for data: 776601600
I0703 01:28:04.887980 31050 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0703 01:28:04.887984 31050 net.cpp:98] Creating Layer res5a_branch2b/relu
I0703 01:28:04.887986 31050 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0703 01:28:04.887989 31050 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0703 01:28:04.887992 31050 net.cpp:148] Setting up res5a_branch2b/relu
I0703 01:28:04.887995 31050 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0703 01:28:04.887996 31050 net.cpp:163] Memory required for data: 789708800
I0703 01:28:04.887998 31050 layer_factory.hpp:77] Creating layer out5a
I0703 01:28:04.888002 31050 net.cpp:98] Creating Layer out5a
I0703 01:28:04.888005 31050 net.cpp:439] out5a <- res5a_branch2b/bn
I0703 01:28:04.888007 31050 net.cpp:413] out5a -> out5a
I0703 01:28:04.891876 31050 net.cpp:148] Setting up out5a
I0703 01:28:04.891885 31050 net.cpp:155] Top shape: 4 64 40 40 (409600)
I0703 01:28:04.891887 31050 net.cpp:163] Memory required for data: 791347200
I0703 01:28:04.891891 31050 layer_factory.hpp:77] Creating layer out5a/bn
I0703 01:28:04.891896 31050 net.cpp:98] Creating Layer out5a/bn
I0703 01:28:04.891907 31050 net.cpp:439] out5a/bn <- out5a
I0703 01:28:04.891911 31050 net.cpp:413] out5a/bn -> out5a/bn
I0703 01:28:04.892448 31050 net.cpp:148] Setting up out5a/bn
I0703 01:28:04.892453 31050 net.cpp:155] Top shape: 4 64 40 40 (409600)
I0703 01:28:04.892457 31050 net.cpp:163] Memory required for data: 792985600
I0703 01:28:04.892463 31050 layer_factory.hpp:77] Creating layer out5a/relu
I0703 01:28:04.892467 31050 net.cpp:98] Creating Layer out5a/relu
I0703 01:28:04.892468 31050 net.cpp:439] out5a/relu <- out5a/bn
I0703 01:28:04.892472 31050 net.cpp:400] out5a/relu -> out5a/bn (in-place)
I0703 01:28:04.892474 31050 net.cpp:148] Setting up out5a/relu
I0703 01:28:04.892477 31050 net.cpp:155] Top shape: 4 64 40 40 (409600)
I0703 01:28:04.892478 31050 net.cpp:163] Memory required for data: 794624000
I0703 01:28:04.892480 31050 layer_factory.hpp:77] Creating layer out5a_up2
I0703 01:28:04.892484 31050 net.cpp:98] Creating Layer out5a_up2
I0703 01:28:04.892487 31050 net.cpp:439] out5a_up2 <- out5a/bn
I0703 01:28:04.892488 31050 net.cpp:413] out5a_up2 -> out5a_up2
I0703 01:28:04.892678 31050 net.cpp:148] Setting up out5a_up2
I0703 01:28:04.892681 31050 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 01:28:04.892684 31050 net.cpp:163] Memory required for data: 801177600
I0703 01:28:04.892688 31050 layer_factory.hpp:77] Creating layer out3a
I0703 01:28:04.892691 31050 net.cpp:98] Creating Layer out3a
I0703 01:28:04.892693 31050 net.cpp:439] out3a <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0703 01:28:04.892699 31050 net.cpp:413] out3a -> out3a
I0703 01:28:04.894356 31050 net.cpp:148] Setting up out3a
I0703 01:28:04.894364 31050 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 01:28:04.894366 31050 net.cpp:163] Memory required for data: 807731200
I0703 01:28:04.894371 31050 layer_factory.hpp:77] Creating layer out3a/bn
I0703 01:28:04.894374 31050 net.cpp:98] Creating Layer out3a/bn
I0703 01:28:04.894377 31050 net.cpp:439] out3a/bn <- out3a
I0703 01:28:04.894381 31050 net.cpp:413] out3a/bn -> out3a/bn
I0703 01:28:04.894922 31050 net.cpp:148] Setting up out3a/bn
I0703 01:28:04.894927 31050 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 01:28:04.894929 31050 net.cpp:163] Memory required for data: 814284800
I0703 01:28:04.894934 31050 layer_factory.hpp:77] Creating layer out3a/relu
I0703 01:28:04.894937 31050 net.cpp:98] Creating Layer out3a/relu
I0703 01:28:04.894938 31050 net.cpp:439] out3a/relu <- out3a/bn
I0703 01:28:04.894942 31050 net.cpp:400] out3a/relu -> out3a/bn (in-place)
I0703 01:28:04.894944 31050 net.cpp:148] Setting up out3a/relu
I0703 01:28:04.894946 31050 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 01:28:04.894948 31050 net.cpp:163] Memory required for data: 820838400
I0703 01:28:04.894950 31050 layer_factory.hpp:77] Creating layer out3_out5_combined
I0703 01:28:04.894953 31050 net.cpp:98] Creating Layer out3_out5_combined
I0703 01:28:04.894955 31050 net.cpp:439] out3_out5_combined <- out5a_up2
I0703 01:28:04.894958 31050 net.cpp:439] out3_out5_combined <- out3a/bn
I0703 01:28:04.894959 31050 net.cpp:413] out3_out5_combined -> out3_out5_combined
I0703 01:28:04.894979 31050 net.cpp:148] Setting up out3_out5_combined
I0703 01:28:04.894984 31050 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 01:28:04.894985 31050 net.cpp:163] Memory required for data: 827392000
I0703 01:28:04.894987 31050 layer_factory.hpp:77] Creating layer ctx_conv1
I0703 01:28:04.894990 31050 net.cpp:98] Creating Layer ctx_conv1
I0703 01:28:04.894992 31050 net.cpp:439] ctx_conv1 <- out3_out5_combined
I0703 01:28:04.894995 31050 net.cpp:413] ctx_conv1 -> ctx_conv1
I0703 01:28:04.895939 31050 net.cpp:148] Setting up ctx_conv1
I0703 01:28:04.895944 31050 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 01:28:04.895946 31050 net.cpp:163] Memory required for data: 833945600
I0703 01:28:04.895949 31050 layer_factory.hpp:77] Creating layer ctx_conv1/bn
I0703 01:28:04.895953 31050 net.cpp:98] Creating Layer ctx_conv1/bn
I0703 01:28:04.895956 31050 net.cpp:439] ctx_conv1/bn <- ctx_conv1
I0703 01:28:04.895964 31050 net.cpp:413] ctx_conv1/bn -> ctx_conv1/bn
I0703 01:28:04.896492 31050 net.cpp:148] Setting up ctx_conv1/bn
I0703 01:28:04.896498 31050 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 01:28:04.896500 31050 net.cpp:163] Memory required for data: 840499200
I0703 01:28:04.896504 31050 layer_factory.hpp:77] Creating layer ctx_conv1/relu
I0703 01:28:04.896507 31050 net.cpp:98] Creating Layer ctx_conv1/relu
I0703 01:28:04.896509 31050 net.cpp:439] ctx_conv1/relu <- ctx_conv1/bn
I0703 01:28:04.896512 31050 net.cpp:400] ctx_conv1/relu -> ctx_conv1/bn (in-place)
I0703 01:28:04.896515 31050 net.cpp:148] Setting up ctx_conv1/relu
I0703 01:28:04.896517 31050 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 01:28:04.896519 31050 net.cpp:163] Memory required for data: 847052800
I0703 01:28:04.896522 31050 layer_factory.hpp:77] Creating layer ctx_conv2
I0703 01:28:04.896524 31050 net.cpp:98] Creating Layer ctx_conv2
I0703 01:28:04.896526 31050 net.cpp:439] ctx_conv2 <- ctx_conv1/bn
I0703 01:28:04.896529 31050 net.cpp:413] ctx_conv2 -> ctx_conv2
I0703 01:28:04.897475 31050 net.cpp:148] Setting up ctx_conv2
I0703 01:28:04.897480 31050 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 01:28:04.897481 31050 net.cpp:163] Memory required for data: 853606400
I0703 01:28:04.897485 31050 layer_factory.hpp:77] Creating layer ctx_conv2/bn
I0703 01:28:04.897488 31050 net.cpp:98] Creating Layer ctx_conv2/bn
I0703 01:28:04.897490 31050 net.cpp:439] ctx_conv2/bn <- ctx_conv2
I0703 01:28:04.897493 31050 net.cpp:413] ctx_conv2/bn -> ctx_conv2/bn
I0703 01:28:04.898030 31050 net.cpp:148] Setting up ctx_conv2/bn
I0703 01:28:04.898035 31050 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 01:28:04.898036 31050 net.cpp:163] Memory required for data: 860160000
I0703 01:28:04.898041 31050 layer_factory.hpp:77] Creating layer ctx_conv2/relu
I0703 01:28:04.898046 31050 net.cpp:98] Creating Layer ctx_conv2/relu
I0703 01:28:04.898047 31050 net.cpp:439] ctx_conv2/relu <- ctx_conv2/bn
I0703 01:28:04.898049 31050 net.cpp:400] ctx_conv2/relu -> ctx_conv2/bn (in-place)
I0703 01:28:04.898053 31050 net.cpp:148] Setting up ctx_conv2/relu
I0703 01:28:04.898056 31050 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 01:28:04.898056 31050 net.cpp:163] Memory required for data: 866713600
I0703 01:28:04.898058 31050 layer_factory.hpp:77] Creating layer ctx_conv3
I0703 01:28:04.898062 31050 net.cpp:98] Creating Layer ctx_conv3
I0703 01:28:04.898064 31050 net.cpp:439] ctx_conv3 <- ctx_conv2/bn
I0703 01:28:04.898067 31050 net.cpp:413] ctx_conv3 -> ctx_conv3
I0703 01:28:04.899019 31050 net.cpp:148] Setting up ctx_conv3
I0703 01:28:04.899024 31050 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 01:28:04.899026 31050 net.cpp:163] Memory required for data: 873267200
I0703 01:28:04.899029 31050 layer_factory.hpp:77] Creating layer ctx_conv3/bn
I0703 01:28:04.899034 31050 net.cpp:98] Creating Layer ctx_conv3/bn
I0703 01:28:04.899035 31050 net.cpp:439] ctx_conv3/bn <- ctx_conv3
I0703 01:28:04.899039 31050 net.cpp:413] ctx_conv3/bn -> ctx_conv3/bn
I0703 01:28:04.899581 31050 net.cpp:148] Setting up ctx_conv3/bn
I0703 01:28:04.899586 31050 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 01:28:04.899588 31050 net.cpp:163] Memory required for data: 879820800
I0703 01:28:04.899593 31050 layer_factory.hpp:77] Creating layer ctx_conv3/relu
I0703 01:28:04.899596 31050 net.cpp:98] Creating Layer ctx_conv3/relu
I0703 01:28:04.899598 31050 net.cpp:439] ctx_conv3/relu <- ctx_conv3/bn
I0703 01:28:04.899603 31050 net.cpp:400] ctx_conv3/relu -> ctx_conv3/bn (in-place)
I0703 01:28:04.899607 31050 net.cpp:148] Setting up ctx_conv3/relu
I0703 01:28:04.899610 31050 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 01:28:04.899611 31050 net.cpp:163] Memory required for data: 886374400
I0703 01:28:04.899613 31050 layer_factory.hpp:77] Creating layer ctx_conv4
I0703 01:28:04.899616 31050 net.cpp:98] Creating Layer ctx_conv4
I0703 01:28:04.899619 31050 net.cpp:439] ctx_conv4 <- ctx_conv3/bn
I0703 01:28:04.899622 31050 net.cpp:413] ctx_conv4 -> ctx_conv4
I0703 01:28:04.900574 31050 net.cpp:148] Setting up ctx_conv4
I0703 01:28:04.900585 31050 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 01:28:04.900588 31050 net.cpp:163] Memory required for data: 892928000
I0703 01:28:04.900591 31050 layer_factory.hpp:77] Creating layer ctx_conv4/bn
I0703 01:28:04.900595 31050 net.cpp:98] Creating Layer ctx_conv4/bn
I0703 01:28:04.900598 31050 net.cpp:439] ctx_conv4/bn <- ctx_conv4
I0703 01:28:04.900600 31050 net.cpp:413] ctx_conv4/bn -> ctx_conv4/bn
I0703 01:28:04.901127 31050 net.cpp:148] Setting up ctx_conv4/bn
I0703 01:28:04.901132 31050 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 01:28:04.901134 31050 net.cpp:163] Memory required for data: 899481600
I0703 01:28:04.901139 31050 layer_factory.hpp:77] Creating layer ctx_conv4/relu
I0703 01:28:04.901142 31050 net.cpp:98] Creating Layer ctx_conv4/relu
I0703 01:28:04.901144 31050 net.cpp:439] ctx_conv4/relu <- ctx_conv4/bn
I0703 01:28:04.901146 31050 net.cpp:400] ctx_conv4/relu -> ctx_conv4/bn (in-place)
I0703 01:28:04.901149 31050 net.cpp:148] Setting up ctx_conv4/relu
I0703 01:28:04.901152 31050 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 01:28:04.901154 31050 net.cpp:163] Memory required for data: 906035200
I0703 01:28:04.901155 31050 layer_factory.hpp:77] Creating layer ctx_final
I0703 01:28:04.901160 31050 net.cpp:98] Creating Layer ctx_final
I0703 01:28:04.901161 31050 net.cpp:439] ctx_final <- ctx_conv4/bn
I0703 01:28:04.901163 31050 net.cpp:413] ctx_final -> ctx_final
I0703 01:28:04.901484 31050 net.cpp:148] Setting up ctx_final
I0703 01:28:04.901489 31050 net.cpp:155] Top shape: 4 8 80 80 (204800)
I0703 01:28:04.901491 31050 net.cpp:163] Memory required for data: 906854400
I0703 01:28:04.901495 31050 layer_factory.hpp:77] Creating layer ctx_final/relu
I0703 01:28:04.901497 31050 net.cpp:98] Creating Layer ctx_final/relu
I0703 01:28:04.901499 31050 net.cpp:439] ctx_final/relu <- ctx_final
I0703 01:28:04.901502 31050 net.cpp:400] ctx_final/relu -> ctx_final (in-place)
I0703 01:28:04.901504 31050 net.cpp:148] Setting up ctx_final/relu
I0703 01:28:04.901507 31050 net.cpp:155] Top shape: 4 8 80 80 (204800)
I0703 01:28:04.901509 31050 net.cpp:163] Memory required for data: 907673600
I0703 01:28:04.901510 31050 layer_factory.hpp:77] Creating layer out_deconv_final_up2
I0703 01:28:04.901515 31050 net.cpp:98] Creating Layer out_deconv_final_up2
I0703 01:28:04.901515 31050 net.cpp:439] out_deconv_final_up2 <- ctx_final
I0703 01:28:04.901518 31050 net.cpp:413] out_deconv_final_up2 -> out_deconv_final_up2
I0703 01:28:04.901687 31050 net.cpp:148] Setting up out_deconv_final_up2
I0703 01:28:04.901692 31050 net.cpp:155] Top shape: 4 8 160 160 (819200)
I0703 01:28:04.901693 31050 net.cpp:163] Memory required for data: 910950400
I0703 01:28:04.901696 31050 layer_factory.hpp:77] Creating layer out_deconv_final_up4
I0703 01:28:04.901700 31050 net.cpp:98] Creating Layer out_deconv_final_up4
I0703 01:28:04.901701 31050 net.cpp:439] out_deconv_final_up4 <- out_deconv_final_up2
I0703 01:28:04.901705 31050 net.cpp:413] out_deconv_final_up4 -> out_deconv_final_up4
I0703 01:28:04.901871 31050 net.cpp:148] Setting up out_deconv_final_up4
I0703 01:28:04.901875 31050 net.cpp:155] Top shape: 4 8 320 320 (3276800)
I0703 01:28:04.901877 31050 net.cpp:163] Memory required for data: 924057600
I0703 01:28:04.901880 31050 layer_factory.hpp:77] Creating layer out_deconv_final_up8
I0703 01:28:04.901882 31050 net.cpp:98] Creating Layer out_deconv_final_up8
I0703 01:28:04.901885 31050 net.cpp:439] out_deconv_final_up8 <- out_deconv_final_up4
I0703 01:28:04.901887 31050 net.cpp:413] out_deconv_final_up8 -> out_deconv_final_up8
I0703 01:28:04.902055 31050 net.cpp:148] Setting up out_deconv_final_up8
I0703 01:28:04.902058 31050 net.cpp:155] Top shape: 4 8 640 640 (13107200)
I0703 01:28:04.902060 31050 net.cpp:163] Memory required for data: 976486400
I0703 01:28:04.902063 31050 layer_factory.hpp:77] Creating layer out_deconv_final_up8_out_deconv_final_up8_0_split
I0703 01:28:04.902066 31050 net.cpp:98] Creating Layer out_deconv_final_up8_out_deconv_final_up8_0_split
I0703 01:28:04.902073 31050 net.cpp:439] out_deconv_final_up8_out_deconv_final_up8_0_split <- out_deconv_final_up8
I0703 01:28:04.902076 31050 net.cpp:413] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0703 01:28:04.902079 31050 net.cpp:413] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0703 01:28:04.902082 31050 net.cpp:413] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0703 01:28:04.902122 31050 net.cpp:148] Setting up out_deconv_final_up8_out_deconv_final_up8_0_split
I0703 01:28:04.902127 31050 net.cpp:155] Top shape: 4 8 640 640 (13107200)
I0703 01:28:04.902128 31050 net.cpp:155] Top shape: 4 8 640 640 (13107200)
I0703 01:28:04.902130 31050 net.cpp:155] Top shape: 4 8 640 640 (13107200)
I0703 01:28:04.902132 31050 net.cpp:163] Memory required for data: 1133772800
I0703 01:28:04.902134 31050 layer_factory.hpp:77] Creating layer loss
I0703 01:28:04.902138 31050 net.cpp:98] Creating Layer loss
I0703 01:28:04.902140 31050 net.cpp:439] loss <- out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0703 01:28:04.902143 31050 net.cpp:439] loss <- label_data_1_split_0
I0703 01:28:04.902145 31050 net.cpp:413] loss -> loss
I0703 01:28:04.902150 31050 layer_factory.hpp:77] Creating layer loss
I0703 01:28:04.917938 31050 net.cpp:148] Setting up loss
I0703 01:28:04.917961 31050 net.cpp:155] Top shape: (1)
I0703 01:28:04.917963 31050 net.cpp:158]     with loss weight 1
I0703 01:28:04.917970 31050 net.cpp:163] Memory required for data: 1133772804
I0703 01:28:04.917974 31050 layer_factory.hpp:77] Creating layer accuracy/top1
I0703 01:28:04.917982 31050 net.cpp:98] Creating Layer accuracy/top1
I0703 01:28:04.917985 31050 net.cpp:439] accuracy/top1 <- out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0703 01:28:04.917990 31050 net.cpp:439] accuracy/top1 <- label_data_1_split_1
I0703 01:28:04.917994 31050 net.cpp:413] accuracy/top1 -> accuracy/top1
I0703 01:28:04.918001 31050 net.cpp:148] Setting up accuracy/top1
I0703 01:28:04.918004 31050 net.cpp:155] Top shape: (1)
I0703 01:28:04.918007 31050 net.cpp:163] Memory required for data: 1133772808
I0703 01:28:04.918009 31050 layer_factory.hpp:77] Creating layer accuracy/top5
I0703 01:28:04.918014 31050 net.cpp:98] Creating Layer accuracy/top5
I0703 01:28:04.918015 31050 net.cpp:439] accuracy/top5 <- out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0703 01:28:04.918018 31050 net.cpp:439] accuracy/top5 <- label_data_1_split_2
I0703 01:28:04.918021 31050 net.cpp:413] accuracy/top5 -> accuracy/top5
I0703 01:28:04.918025 31050 net.cpp:148] Setting up accuracy/top5
I0703 01:28:04.918028 31050 net.cpp:155] Top shape: (1)
I0703 01:28:04.918032 31050 net.cpp:163] Memory required for data: 1133772812
I0703 01:28:04.918036 31050 net.cpp:226] accuracy/top5 does not need backward computation.
I0703 01:28:04.918038 31050 net.cpp:226] accuracy/top1 does not need backward computation.
I0703 01:28:04.918041 31050 net.cpp:224] loss needs backward computation.
I0703 01:28:04.918046 31050 net.cpp:224] out_deconv_final_up8_out_deconv_final_up8_0_split needs backward computation.
I0703 01:28:04.918051 31050 net.cpp:224] out_deconv_final_up8 needs backward computation.
I0703 01:28:04.918053 31050 net.cpp:224] out_deconv_final_up4 needs backward computation.
I0703 01:28:04.918056 31050 net.cpp:224] out_deconv_final_up2 needs backward computation.
I0703 01:28:04.918061 31050 net.cpp:224] ctx_final/relu needs backward computation.
I0703 01:28:04.918064 31050 net.cpp:224] ctx_final needs backward computation.
I0703 01:28:04.918067 31050 net.cpp:224] ctx_conv4/relu needs backward computation.
I0703 01:28:04.918071 31050 net.cpp:224] ctx_conv4/bn needs backward computation.
I0703 01:28:04.918073 31050 net.cpp:224] ctx_conv4 needs backward computation.
I0703 01:28:04.918078 31050 net.cpp:224] ctx_conv3/relu needs backward computation.
I0703 01:28:04.918081 31050 net.cpp:224] ctx_conv3/bn needs backward computation.
I0703 01:28:04.918085 31050 net.cpp:224] ctx_conv3 needs backward computation.
I0703 01:28:04.918098 31050 net.cpp:224] ctx_conv2/relu needs backward computation.
I0703 01:28:04.918102 31050 net.cpp:224] ctx_conv2/bn needs backward computation.
I0703 01:28:04.918104 31050 net.cpp:224] ctx_conv2 needs backward computation.
I0703 01:28:04.918107 31050 net.cpp:224] ctx_conv1/relu needs backward computation.
I0703 01:28:04.918108 31050 net.cpp:224] ctx_conv1/bn needs backward computation.
I0703 01:28:04.918112 31050 net.cpp:224] ctx_conv1 needs backward computation.
I0703 01:28:04.918114 31050 net.cpp:224] out3_out5_combined needs backward computation.
I0703 01:28:04.918118 31050 net.cpp:224] out3a/relu needs backward computation.
I0703 01:28:04.918120 31050 net.cpp:224] out3a/bn needs backward computation.
I0703 01:28:04.918123 31050 net.cpp:224] out3a needs backward computation.
I0703 01:28:04.918126 31050 net.cpp:224] out5a_up2 needs backward computation.
I0703 01:28:04.918130 31050 net.cpp:224] out5a/relu needs backward computation.
I0703 01:28:04.918133 31050 net.cpp:224] out5a/bn needs backward computation.
I0703 01:28:04.918136 31050 net.cpp:224] out5a needs backward computation.
I0703 01:28:04.918141 31050 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0703 01:28:04.918144 31050 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0703 01:28:04.918148 31050 net.cpp:224] res5a_branch2b needs backward computation.
I0703 01:28:04.918151 31050 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0703 01:28:04.918155 31050 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0703 01:28:04.918157 31050 net.cpp:224] res5a_branch2a needs backward computation.
I0703 01:28:04.918161 31050 net.cpp:224] pool4 needs backward computation.
I0703 01:28:04.918165 31050 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0703 01:28:04.918169 31050 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0703 01:28:04.918172 31050 net.cpp:224] res4a_branch2b needs backward computation.
I0703 01:28:04.918176 31050 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0703 01:28:04.918179 31050 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0703 01:28:04.918184 31050 net.cpp:224] res4a_branch2a needs backward computation.
I0703 01:28:04.918187 31050 net.cpp:224] pool3 needs backward computation.
I0703 01:28:04.918191 31050 net.cpp:224] res3a_branch2b/bn_res3a_branch2b/relu_0_split needs backward computation.
I0703 01:28:04.918195 31050 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0703 01:28:04.918198 31050 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0703 01:28:04.918201 31050 net.cpp:224] res3a_branch2b needs backward computation.
I0703 01:28:04.918205 31050 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0703 01:28:04.918208 31050 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0703 01:28:04.918212 31050 net.cpp:224] res3a_branch2a needs backward computation.
I0703 01:28:04.918216 31050 net.cpp:224] pool2 needs backward computation.
I0703 01:28:04.918220 31050 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0703 01:28:04.918223 31050 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0703 01:28:04.918227 31050 net.cpp:224] res2a_branch2b needs backward computation.
I0703 01:28:04.918231 31050 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0703 01:28:04.918234 31050 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0703 01:28:04.918238 31050 net.cpp:224] res2a_branch2a needs backward computation.
I0703 01:28:04.918242 31050 net.cpp:224] pool1 needs backward computation.
I0703 01:28:04.918246 31050 net.cpp:224] conv1b/relu needs backward computation.
I0703 01:28:04.918249 31050 net.cpp:224] conv1b/bn needs backward computation.
I0703 01:28:04.918253 31050 net.cpp:224] conv1b needs backward computation.
I0703 01:28:04.918257 31050 net.cpp:224] conv1a/relu needs backward computation.
I0703 01:28:04.918261 31050 net.cpp:224] conv1a/bn needs backward computation.
I0703 01:28:04.918264 31050 net.cpp:224] conv1a needs backward computation.
I0703 01:28:04.918272 31050 net.cpp:226] data/bias does not need backward computation.
I0703 01:28:04.918277 31050 net.cpp:226] label_data_1_split does not need backward computation.
I0703 01:28:04.918280 31050 net.cpp:226] data does not need backward computation.
I0703 01:28:04.918283 31050 net.cpp:268] This network produces output accuracy/top1
I0703 01:28:04.918287 31050 net.cpp:268] This network produces output accuracy/top5
I0703 01:28:04.918290 31050 net.cpp:268] This network produces output loss
I0703 01:28:04.918321 31050 net.cpp:288] Network initialization done.
I0703 01:28:04.918424 31050 solver.cpp:60] Solver scaffolding done.
I0703 01:28:04.923869 31050 caffe.cpp:145] Finetuning from training/cityscapes5_jsegnet21v2_2017-07-02_23-02-42/initial/cityscapes5_jsegnet21v2_iter_32000.caffemodel
W0703 01:28:04.952996 31050 parallel.cpp:400] Batch size must be divisible by the number of solvers (GPUs)
I0703 01:28:04.959111 31050 data_layer.cpp:78] ReshapePrefetch 8, 3, 640, 640
I0703 01:28:04.959216 31050 data_layer.cpp:83] output data size: 8,3,640,640
I0703 01:28:05.032171 31050 data_layer.cpp:78] ReshapePrefetch 8, 1, 640, 640
I0703 01:28:05.032281 31050 data_layer.cpp:83] output data size: 8,1,640,640
I0703 01:28:05.811269 31050 parallel.cpp:334] Starting Optimization
I0703 01:28:05.811312 31050 net.cpp:1824] All zero weights of convolution layers are frozen
I0703 01:28:05.819553 31050 solver.cpp:415] Solving jsegnet21v2_train
I0703 01:28:05.819568 31050 solver.cpp:416] Learning Rate Policy: multistep
I0703 01:28:06.270140 31050 solver.cpp:290] Iteration 0 (0 iter/s, 0.450541s/100 iter), loss = 0.022395
I0703 01:28:06.270164 31050 solver.cpp:309]     Train net output #0: loss = 0.022395 (* 1 = 0.022395 loss)
I0703 01:28:06.270171 31050 sgd_solver.cpp:106] Iteration 0, lr = 1e-05
I0703 01:28:26.186594 31166 blocking_queue.cpp:50] Data layer prefetch queue empty
I0703 01:28:30.763746 31050 solver.cpp:290] Iteration 100 (4.08282 iter/s, 24.4929s/100 iter), loss = 0.0334281
I0703 01:28:30.763768 31050 solver.cpp:309]     Train net output #0: loss = 0.0334281 (* 1 = 0.0334281 loss)
I0703 01:28:30.763775 31050 sgd_solver.cpp:106] Iteration 100, lr = 1e-05
I0703 01:28:55.264039 31050 solver.cpp:290] Iteration 200 (4.0817 iter/s, 24.4996s/100 iter), loss = 0.0173251
I0703 01:28:55.264284 31050 solver.cpp:309]     Train net output #0: loss = 0.0173251 (* 1 = 0.0173251 loss)
I0703 01:28:55.264295 31050 sgd_solver.cpp:106] Iteration 200, lr = 1e-05
I0703 01:29:19.749325 31050 solver.cpp:290] Iteration 300 (4.08424 iter/s, 24.4844s/100 iter), loss = 0.0198397
I0703 01:29:19.749351 31050 solver.cpp:309]     Train net output #0: loss = 0.0198397 (* 1 = 0.0198397 loss)
I0703 01:29:19.749359 31050 sgd_solver.cpp:106] Iteration 300, lr = 1e-05
I0703 01:29:44.247344 31050 solver.cpp:290] Iteration 400 (4.08208 iter/s, 24.4973s/100 iter), loss = 0.0171196
I0703 01:29:44.247448 31050 solver.cpp:309]     Train net output #0: loss = 0.0171196 (* 1 = 0.0171196 loss)
I0703 01:29:44.247459 31050 sgd_solver.cpp:106] Iteration 400, lr = 1e-05
I0703 01:30:08.755560 31050 solver.cpp:290] Iteration 500 (4.08039 iter/s, 24.5074s/100 iter), loss = 0.0194807
I0703 01:30:08.755585 31050 solver.cpp:309]     Train net output #0: loss = 0.0194807 (* 1 = 0.0194807 loss)
I0703 01:30:08.755592 31050 sgd_solver.cpp:106] Iteration 500, lr = 1e-05
I0703 01:30:33.219681 31050 solver.cpp:290] Iteration 600 (4.08773 iter/s, 24.4634s/100 iter), loss = 0.0119504
I0703 01:30:33.219728 31050 solver.cpp:309]     Train net output #0: loss = 0.0119504 (* 1 = 0.0119504 loss)
I0703 01:30:33.219738 31050 sgd_solver.cpp:106] Iteration 600, lr = 1e-05
I0703 01:30:57.760022 31050 solver.cpp:290] Iteration 700 (4.07504 iter/s, 24.5396s/100 iter), loss = 0.0248599
I0703 01:30:57.760044 31050 solver.cpp:309]     Train net output #0: loss = 0.0248599 (* 1 = 0.0248599 loss)
I0703 01:30:57.760052 31050 sgd_solver.cpp:106] Iteration 700, lr = 1e-05
I0703 01:31:22.242445 31050 solver.cpp:290] Iteration 800 (4.08468 iter/s, 24.4817s/100 iter), loss = 0.0217321
I0703 01:31:22.242568 31050 solver.cpp:309]     Train net output #0: loss = 0.0217321 (* 1 = 0.0217321 loss)
I0703 01:31:22.242584 31050 sgd_solver.cpp:106] Iteration 800, lr = 1e-05
I0703 01:31:46.759042 31050 solver.cpp:290] Iteration 900 (4.079 iter/s, 24.5158s/100 iter), loss = 0.0282669
I0703 01:31:46.759064 31050 solver.cpp:309]     Train net output #0: loss = 0.0282669 (* 1 = 0.0282669 loss)
I0703 01:31:46.759071 31050 sgd_solver.cpp:106] Iteration 900, lr = 1e-05
I0703 01:32:11.013408 31050 solver.cpp:354] Sparsity after update:
I0703 01:32:11.064505 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 01:32:11.064522 31050 net.cpp:1851] conv1a_param_0(0) 
I0703 01:32:11.064534 31050 net.cpp:1851] conv1b_param_0(0) 
I0703 01:32:11.064537 31050 net.cpp:1851] ctx_conv1_param_0(0) 
I0703 01:32:11.064538 31050 net.cpp:1851] ctx_conv2_param_0(0) 
I0703 01:32:11.064540 31050 net.cpp:1851] ctx_conv3_param_0(0) 
I0703 01:32:11.064543 31050 net.cpp:1851] ctx_conv4_param_0(0) 
I0703 01:32:11.064544 31050 net.cpp:1851] ctx_final_param_0(0) 
I0703 01:32:11.064545 31050 net.cpp:1851] out3a_param_0(0) 
I0703 01:32:11.064548 31050 net.cpp:1851] out5a_param_0(0) 
I0703 01:32:11.064550 31050 net.cpp:1851] res2a_branch2a_param_0(0) 
I0703 01:32:11.064553 31050 net.cpp:1851] res2a_branch2b_param_0(0) 
I0703 01:32:11.064554 31050 net.cpp:1851] res3a_branch2a_param_0(0) 
I0703 01:32:11.064556 31050 net.cpp:1851] res3a_branch2b_param_0(0) 
I0703 01:32:11.064558 31050 net.cpp:1851] res4a_branch2a_param_0(0) 
I0703 01:32:11.064560 31050 net.cpp:1851] res4a_branch2b_param_0(0) 
I0703 01:32:11.064561 31050 net.cpp:1851] res5a_branch2a_param_0(0) 
I0703 01:32:11.064563 31050 net.cpp:1851] res5a_branch2b_param_0(0) 
I0703 01:32:11.064565 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0703 01:32:11.297063 31050 solver.cpp:290] Iteration 1000 (4.07542 iter/s, 24.5373s/100 iter), loss = 0.0128293
I0703 01:32:11.297089 31050 solver.cpp:309]     Train net output #0: loss = 0.0128293 (* 1 = 0.0128293 loss)
I0703 01:32:11.297096 31050 sgd_solver.cpp:106] Iteration 1000, lr = 1e-05
I0703 01:32:35.792799 31050 solver.cpp:290] Iteration 1100 (4.08246 iter/s, 24.495s/100 iter), loss = 0.0340053
I0703 01:32:35.792822 31050 solver.cpp:309]     Train net output #0: loss = 0.0340053 (* 1 = 0.0340053 loss)
I0703 01:32:35.792830 31050 sgd_solver.cpp:106] Iteration 1100, lr = 1e-05
I0703 01:33:00.284731 31050 solver.cpp:290] Iteration 1200 (4.08309 iter/s, 24.4912s/100 iter), loss = 0.0291324
I0703 01:33:00.284842 31050 solver.cpp:309]     Train net output #0: loss = 0.0291324 (* 1 = 0.0291324 loss)
I0703 01:33:00.284852 31050 sgd_solver.cpp:106] Iteration 1200, lr = 1e-05
I0703 01:33:24.776051 31050 solver.cpp:290] Iteration 1300 (4.08321 iter/s, 24.4906s/100 iter), loss = 0.0219733
I0703 01:33:24.776073 31050 solver.cpp:309]     Train net output #0: loss = 0.0219733 (* 1 = 0.0219733 loss)
I0703 01:33:24.776080 31050 sgd_solver.cpp:106] Iteration 1300, lr = 1e-05
I0703 01:33:49.277182 31050 solver.cpp:290] Iteration 1400 (4.08156 iter/s, 24.5004s/100 iter), loss = 0.0245267
I0703 01:33:49.277289 31050 solver.cpp:309]     Train net output #0: loss = 0.0245267 (* 1 = 0.0245267 loss)
I0703 01:33:49.277299 31050 sgd_solver.cpp:106] Iteration 1400, lr = 1e-05
I0703 01:34:13.745435 31050 solver.cpp:290] Iteration 1500 (4.08706 iter/s, 24.4675s/100 iter), loss = 0.0298389
I0703 01:34:13.745457 31050 solver.cpp:309]     Train net output #0: loss = 0.0298389 (* 1 = 0.0298389 loss)
I0703 01:34:13.745465 31050 sgd_solver.cpp:106] Iteration 1500, lr = 1e-05
I0703 01:34:38.260201 31050 solver.cpp:290] Iteration 1600 (4.07929 iter/s, 24.5141s/100 iter), loss = 0.0261295
I0703 01:34:38.260313 31050 solver.cpp:309]     Train net output #0: loss = 0.0261295 (* 1 = 0.0261295 loss)
I0703 01:34:38.260324 31050 sgd_solver.cpp:106] Iteration 1600, lr = 1e-05
I0703 01:35:02.752092 31050 solver.cpp:290] Iteration 1700 (4.08311 iter/s, 24.4911s/100 iter), loss = 0.0210975
I0703 01:35:02.752116 31050 solver.cpp:309]     Train net output #0: loss = 0.0210975 (* 1 = 0.0210975 loss)
I0703 01:35:02.752123 31050 sgd_solver.cpp:106] Iteration 1700, lr = 1e-05
I0703 01:35:27.264531 31050 solver.cpp:290] Iteration 1800 (4.07968 iter/s, 24.5118s/100 iter), loss = 0.0328745
I0703 01:35:27.264660 31050 solver.cpp:309]     Train net output #0: loss = 0.0328745 (* 1 = 0.0328745 loss)
I0703 01:35:27.264670 31050 sgd_solver.cpp:106] Iteration 1800, lr = 1e-05
I0703 01:35:51.778623 31050 solver.cpp:290] Iteration 1900 (4.07942 iter/s, 24.5133s/100 iter), loss = 0.0226709
I0703 01:35:51.778650 31050 solver.cpp:309]     Train net output #0: loss = 0.0226709 (* 1 = 0.0226709 loss)
I0703 01:35:51.778657 31050 sgd_solver.cpp:106] Iteration 1900, lr = 1e-05
I0703 01:36:16.027067 31050 solver.cpp:354] Sparsity after update:
I0703 01:36:16.028832 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 01:36:16.028839 31050 net.cpp:1851] conv1a_param_0(0) 
I0703 01:36:16.028846 31050 net.cpp:1851] conv1b_param_0(0) 
I0703 01:36:16.028848 31050 net.cpp:1851] ctx_conv1_param_0(0) 
I0703 01:36:16.028851 31050 net.cpp:1851] ctx_conv2_param_0(0) 
I0703 01:36:16.028852 31050 net.cpp:1851] ctx_conv3_param_0(0) 
I0703 01:36:16.028854 31050 net.cpp:1851] ctx_conv4_param_0(0) 
I0703 01:36:16.028856 31050 net.cpp:1851] ctx_final_param_0(0) 
I0703 01:36:16.028858 31050 net.cpp:1851] out3a_param_0(0) 
I0703 01:36:16.028861 31050 net.cpp:1851] out5a_param_0(0) 
I0703 01:36:16.028862 31050 net.cpp:1851] res2a_branch2a_param_0(0) 
I0703 01:36:16.028863 31050 net.cpp:1851] res2a_branch2b_param_0(0) 
I0703 01:36:16.028865 31050 net.cpp:1851] res3a_branch2a_param_0(0) 
I0703 01:36:16.028868 31050 net.cpp:1851] res3a_branch2b_param_0(0) 
I0703 01:36:16.028869 31050 net.cpp:1851] res4a_branch2a_param_0(0) 
I0703 01:36:16.028872 31050 net.cpp:1851] res4a_branch2b_param_0(0) 
I0703 01:36:16.028873 31050 net.cpp:1851] res5a_branch2a_param_0(0) 
I0703 01:36:16.028875 31050 net.cpp:1851] res5a_branch2b_param_0(0) 
I0703 01:36:16.028878 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0703 01:36:16.029022 31050 solver.cpp:473] Iteration 2000, Testing net (#0)
I0703 01:37:05.767746 31050 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.954833
I0703 01:37:05.767829 31050 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.999852
I0703 01:37:05.767838 31050 solver.cpp:546]     Test net output #2: loss = 0.141555 (* 1 = 0.141555 loss)
I0703 01:37:06.017689 31050 solver.cpp:290] Iteration 2000 (1.34704 iter/s, 74.237s/100 iter), loss = 0.0238302
I0703 01:37:06.017717 31050 solver.cpp:309]     Train net output #0: loss = 0.0238302 (* 1 = 0.0238302 loss)
I0703 01:37:06.017724 31050 sgd_solver.cpp:106] Iteration 2000, lr = 1e-05
I0703 01:37:29.540304 31050 solver.cpp:290] Iteration 2100 (4.25135 iter/s, 23.5219s/100 iter), loss = 0.0234076
I0703 01:37:29.540329 31050 solver.cpp:309]     Train net output #0: loss = 0.0234076 (* 1 = 0.0234076 loss)
I0703 01:37:29.540336 31050 sgd_solver.cpp:106] Iteration 2100, lr = 1e-05
I0703 01:37:55.113137 31050 solver.cpp:290] Iteration 2200 (3.91051 iter/s, 25.5721s/100 iter), loss = 0.0172971
I0703 01:37:55.113183 31050 solver.cpp:309]     Train net output #0: loss = 0.017297 (* 1 = 0.017297 loss)
I0703 01:37:55.113193 31050 sgd_solver.cpp:106] Iteration 2200, lr = 1e-05
I0703 01:38:19.269459 31050 solver.cpp:290] Iteration 2300 (4.13982 iter/s, 24.1556s/100 iter), loss = 0.0182444
I0703 01:38:19.269482 31050 solver.cpp:309]     Train net output #0: loss = 0.0182443 (* 1 = 0.0182443 loss)
I0703 01:38:19.269490 31050 sgd_solver.cpp:106] Iteration 2300, lr = 1e-05
I0703 01:38:43.435806 31050 solver.cpp:290] Iteration 2400 (4.1381 iter/s, 24.1657s/100 iter), loss = 0.0199583
I0703 01:38:43.435922 31050 solver.cpp:309]     Train net output #0: loss = 0.0199583 (* 1 = 0.0199583 loss)
I0703 01:38:43.435932 31050 sgd_solver.cpp:106] Iteration 2400, lr = 1e-05
I0703 01:39:07.571527 31050 solver.cpp:290] Iteration 2500 (4.14337 iter/s, 24.135s/100 iter), loss = 0.0260882
I0703 01:39:07.571552 31050 solver.cpp:309]     Train net output #0: loss = 0.0260882 (* 1 = 0.0260882 loss)
I0703 01:39:07.571558 31050 sgd_solver.cpp:106] Iteration 2500, lr = 1e-05
I0703 01:39:31.793802 31050 solver.cpp:290] Iteration 2600 (4.12855 iter/s, 24.2216s/100 iter), loss = 0.0204204
I0703 01:39:31.793931 31050 solver.cpp:309]     Train net output #0: loss = 0.0204204 (* 1 = 0.0204204 loss)
I0703 01:39:31.793941 31050 sgd_solver.cpp:106] Iteration 2600, lr = 1e-05
I0703 01:39:55.946151 31050 solver.cpp:290] Iteration 2700 (4.14052 iter/s, 24.1516s/100 iter), loss = 0.014554
I0703 01:39:55.946202 31050 solver.cpp:309]     Train net output #0: loss = 0.014554 (* 1 = 0.014554 loss)
I0703 01:39:55.946213 31050 sgd_solver.cpp:106] Iteration 2700, lr = 1e-05
I0703 01:40:20.138648 31050 solver.cpp:290] Iteration 2800 (4.13363 iter/s, 24.1918s/100 iter), loss = 0.0493881
I0703 01:40:20.138756 31050 solver.cpp:309]     Train net output #0: loss = 0.0493881 (* 1 = 0.0493881 loss)
I0703 01:40:20.138767 31050 sgd_solver.cpp:106] Iteration 2800, lr = 1e-05
I0703 01:40:44.319144 31050 solver.cpp:290] Iteration 2900 (4.13569 iter/s, 24.1797s/100 iter), loss = 0.0251712
I0703 01:40:44.319167 31050 solver.cpp:309]     Train net output #0: loss = 0.0251712 (* 1 = 0.0251712 loss)
I0703 01:40:44.319175 31050 sgd_solver.cpp:106] Iteration 2900, lr = 1e-05
I0703 01:41:08.241856 31050 solver.cpp:354] Sparsity after update:
I0703 01:41:08.282577 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 01:41:08.282594 31050 net.cpp:1851] conv1a_param_0(0) 
I0703 01:41:08.282603 31050 net.cpp:1851] conv1b_param_0(0) 
I0703 01:41:08.282604 31050 net.cpp:1851] ctx_conv1_param_0(0) 
I0703 01:41:08.282606 31050 net.cpp:1851] ctx_conv2_param_0(0) 
I0703 01:41:08.282608 31050 net.cpp:1851] ctx_conv3_param_0(0) 
I0703 01:41:08.282610 31050 net.cpp:1851] ctx_conv4_param_0(0) 
I0703 01:41:08.282613 31050 net.cpp:1851] ctx_final_param_0(0) 
I0703 01:41:08.282614 31050 net.cpp:1851] out3a_param_0(0) 
I0703 01:41:08.282616 31050 net.cpp:1851] out5a_param_0(0) 
I0703 01:41:08.282618 31050 net.cpp:1851] res2a_branch2a_param_0(0) 
I0703 01:41:08.282620 31050 net.cpp:1851] res2a_branch2b_param_0(0) 
I0703 01:41:08.282622 31050 net.cpp:1851] res3a_branch2a_param_0(0) 
I0703 01:41:08.282624 31050 net.cpp:1851] res3a_branch2b_param_0(0) 
I0703 01:41:08.282626 31050 net.cpp:1851] res4a_branch2a_param_0(0) 
I0703 01:41:08.282627 31050 net.cpp:1851] res4a_branch2b_param_0(0) 
I0703 01:41:08.282629 31050 net.cpp:1851] res5a_branch2a_param_0(0) 
I0703 01:41:08.282631 31050 net.cpp:1851] res5a_branch2b_param_0(0) 
I0703 01:41:08.282634 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0703 01:41:08.513631 31050 solver.cpp:290] Iteration 3000 (4.13329 iter/s, 24.1938s/100 iter), loss = 0.0305134
I0703 01:41:08.513655 31050 solver.cpp:309]     Train net output #0: loss = 0.0305134 (* 1 = 0.0305134 loss)
I0703 01:41:08.513662 31050 sgd_solver.cpp:106] Iteration 3000, lr = 1e-05
I0703 01:41:32.649389 31050 solver.cpp:290] Iteration 3100 (4.14335 iter/s, 24.135s/100 iter), loss = 0.0275533
I0703 01:41:32.649413 31050 solver.cpp:309]     Train net output #0: loss = 0.0275533 (* 1 = 0.0275533 loss)
I0703 01:41:32.649420 31050 sgd_solver.cpp:106] Iteration 3100, lr = 1e-05
I0703 01:41:56.824509 31050 solver.cpp:290] Iteration 3200 (4.13661 iter/s, 24.1744s/100 iter), loss = 0.0196518
I0703 01:41:56.824568 31050 solver.cpp:309]     Train net output #0: loss = 0.0196518 (* 1 = 0.0196518 loss)
I0703 01:41:56.824575 31050 sgd_solver.cpp:106] Iteration 3200, lr = 1e-05
I0703 01:42:20.954430 31050 solver.cpp:290] Iteration 3300 (4.14436 iter/s, 24.1291s/100 iter), loss = 0.0285065
I0703 01:42:20.954454 31050 solver.cpp:309]     Train net output #0: loss = 0.0285065 (* 1 = 0.0285065 loss)
I0703 01:42:20.954461 31050 sgd_solver.cpp:106] Iteration 3300, lr = 1e-05
I0703 01:42:45.131654 31050 solver.cpp:290] Iteration 3400 (4.13625 iter/s, 24.1765s/100 iter), loss = 0.0164425
I0703 01:42:45.131780 31050 solver.cpp:309]     Train net output #0: loss = 0.0164425 (* 1 = 0.0164425 loss)
I0703 01:42:45.131790 31050 sgd_solver.cpp:106] Iteration 3400, lr = 1e-05
I0703 01:43:09.340595 31050 solver.cpp:290] Iteration 3500 (4.13085 iter/s, 24.2081s/100 iter), loss = 0.0205454
I0703 01:43:09.340620 31050 solver.cpp:309]     Train net output #0: loss = 0.0205454 (* 1 = 0.0205454 loss)
I0703 01:43:09.340627 31050 sgd_solver.cpp:106] Iteration 3500, lr = 1e-05
I0703 01:43:33.482087 31050 solver.cpp:290] Iteration 3600 (4.14237 iter/s, 24.1408s/100 iter), loss = 0.0269406
I0703 01:43:33.482179 31050 solver.cpp:309]     Train net output #0: loss = 0.0269406 (* 1 = 0.0269406 loss)
I0703 01:43:33.482190 31050 sgd_solver.cpp:106] Iteration 3600, lr = 1e-05
I0703 01:43:57.653404 31050 solver.cpp:290] Iteration 3700 (4.13727 iter/s, 24.1705s/100 iter), loss = 0.0221964
I0703 01:43:57.653430 31050 solver.cpp:309]     Train net output #0: loss = 0.0221964 (* 1 = 0.0221964 loss)
I0703 01:43:57.653440 31050 sgd_solver.cpp:106] Iteration 3700, lr = 1e-05
I0703 01:44:21.818341 31050 solver.cpp:290] Iteration 3800 (4.13835 iter/s, 24.1642s/100 iter), loss = 0.0284568
I0703 01:44:21.818378 31050 solver.cpp:309]     Train net output #0: loss = 0.0284568 (* 1 = 0.0284568 loss)
I0703 01:44:21.818390 31050 sgd_solver.cpp:106] Iteration 3800, lr = 1e-05
I0703 01:44:45.951690 31050 solver.cpp:290] Iteration 3900 (4.14377 iter/s, 24.1326s/100 iter), loss = 0.0155338
I0703 01:44:45.951714 31050 solver.cpp:309]     Train net output #0: loss = 0.0155338 (* 1 = 0.0155338 loss)
I0703 01:44:45.951721 31050 sgd_solver.cpp:106] Iteration 3900, lr = 1e-05
I0703 01:45:09.869546 31050 solver.cpp:354] Sparsity after update:
I0703 01:45:09.871435 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 01:45:09.871444 31050 net.cpp:1851] conv1a_param_0(0) 
I0703 01:45:09.871451 31050 net.cpp:1851] conv1b_param_0(0) 
I0703 01:45:09.871454 31050 net.cpp:1851] ctx_conv1_param_0(0) 
I0703 01:45:09.871456 31050 net.cpp:1851] ctx_conv2_param_0(0) 
I0703 01:45:09.871459 31050 net.cpp:1851] ctx_conv3_param_0(0) 
I0703 01:45:09.871461 31050 net.cpp:1851] ctx_conv4_param_0(0) 
I0703 01:45:09.871464 31050 net.cpp:1851] ctx_final_param_0(0) 
I0703 01:45:09.871465 31050 net.cpp:1851] out3a_param_0(0) 
I0703 01:45:09.871467 31050 net.cpp:1851] out5a_param_0(0) 
I0703 01:45:09.871469 31050 net.cpp:1851] res2a_branch2a_param_0(0) 
I0703 01:45:09.871471 31050 net.cpp:1851] res2a_branch2b_param_0(0) 
I0703 01:45:09.871474 31050 net.cpp:1851] res3a_branch2a_param_0(0) 
I0703 01:45:09.871476 31050 net.cpp:1851] res3a_branch2b_param_0(0) 
I0703 01:45:09.871479 31050 net.cpp:1851] res4a_branch2a_param_0(0) 
I0703 01:45:09.871480 31050 net.cpp:1851] res4a_branch2b_param_0(0) 
I0703 01:45:09.871482 31050 net.cpp:1851] res5a_branch2a_param_0(0) 
I0703 01:45:09.871484 31050 net.cpp:1851] res5a_branch2b_param_0(0) 
I0703 01:45:09.871487 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0703 01:45:09.871696 31050 solver.cpp:473] Iteration 4000, Testing net (#0)
I0703 01:45:58.852124 31050 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.954527
I0703 01:45:58.852211 31050 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.999733
I0703 01:45:58.852218 31050 solver.cpp:546]     Test net output #2: loss = 0.148122 (* 1 = 0.148122 loss)
I0703 01:45:59.114356 31050 solver.cpp:290] Iteration 4000 (1.36686 iter/s, 73.1605s/100 iter), loss = 0.0221232
I0703 01:45:59.114390 31050 solver.cpp:309]     Train net output #0: loss = 0.0221232 (* 1 = 0.0221232 loss)
I0703 01:45:59.114400 31050 sgd_solver.cpp:106] Iteration 4000, lr = 1e-05
I0703 01:45:59.115792 31050 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.05
I0703 01:45:59.323696 31050 net.cpp:1824] All zero weights of convolution layers are frozen
I0703 01:46:23.006377 31050 solver.cpp:290] Iteration 4100 (4.18563 iter/s, 23.8913s/100 iter), loss = 0.0215287
I0703 01:46:23.006409 31050 solver.cpp:309]     Train net output #0: loss = 0.0215286 (* 1 = 0.0215286 loss)
I0703 01:46:23.006417 31050 sgd_solver.cpp:106] Iteration 4100, lr = 1e-05
I0703 01:46:37.299903 31129 blocking_queue.cpp:50] Waiting for data
I0703 01:47:14.330363 31050 solver.cpp:290] Iteration 4200 (1.94847 iter/s, 51.3224s/100 iter), loss = 0.0265287
I0703 01:47:14.330484 31050 solver.cpp:309]     Train net output #0: loss = 0.0265287 (* 1 = 0.0265287 loss)
I0703 01:47:14.330494 31050 sgd_solver.cpp:106] Iteration 4200, lr = 1e-05
I0703 01:47:38.611388 31050 solver.cpp:290] Iteration 4300 (4.11859 iter/s, 24.2802s/100 iter), loss = 0.0211112
I0703 01:47:38.611410 31050 solver.cpp:309]     Train net output #0: loss = 0.0211111 (* 1 = 0.0211111 loss)
I0703 01:47:38.611418 31050 sgd_solver.cpp:106] Iteration 4300, lr = 1e-05
I0703 01:48:02.806324 31050 solver.cpp:290] Iteration 4400 (4.13322 iter/s, 24.1942s/100 iter), loss = 0.0233011
I0703 01:48:02.806432 31050 solver.cpp:309]     Train net output #0: loss = 0.0233011 (* 1 = 0.0233011 loss)
I0703 01:48:02.806447 31050 sgd_solver.cpp:106] Iteration 4400, lr = 1e-05
I0703 01:48:26.954427 31050 solver.cpp:290] Iteration 4500 (4.14125 iter/s, 24.1473s/100 iter), loss = 0.0272907
I0703 01:48:26.954453 31050 solver.cpp:309]     Train net output #0: loss = 0.0272906 (* 1 = 0.0272906 loss)
I0703 01:48:26.954460 31050 sgd_solver.cpp:106] Iteration 4500, lr = 1e-05
I0703 01:48:51.127960 31050 solver.cpp:290] Iteration 4600 (4.13688 iter/s, 24.1728s/100 iter), loss = 0.0179839
I0703 01:48:51.128069 31050 solver.cpp:309]     Train net output #0: loss = 0.0179839 (* 1 = 0.0179839 loss)
I0703 01:48:51.128084 31050 sgd_solver.cpp:106] Iteration 4600, lr = 1e-05
I0703 01:49:15.340709 31050 solver.cpp:290] Iteration 4700 (4.13019 iter/s, 24.2119s/100 iter), loss = 0.0204298
I0703 01:49:15.340734 31050 solver.cpp:309]     Train net output #0: loss = 0.0204298 (* 1 = 0.0204298 loss)
I0703 01:49:15.340740 31050 sgd_solver.cpp:106] Iteration 4700, lr = 1e-05
I0703 01:49:39.475651 31050 solver.cpp:290] Iteration 4800 (4.1435 iter/s, 24.1342s/100 iter), loss = 0.0262297
I0703 01:49:39.475692 31050 solver.cpp:309]     Train net output #0: loss = 0.0262296 (* 1 = 0.0262296 loss)
I0703 01:49:39.475702 31050 sgd_solver.cpp:106] Iteration 4800, lr = 1e-05
I0703 01:50:03.673775 31050 solver.cpp:290] Iteration 4900 (4.13268 iter/s, 24.1974s/100 iter), loss = 0.0203736
I0703 01:50:03.673796 31050 solver.cpp:309]     Train net output #0: loss = 0.0203735 (* 1 = 0.0203735 loss)
I0703 01:50:03.673804 31050 sgd_solver.cpp:106] Iteration 4900, lr = 1e-05
I0703 01:50:27.617161 31050 solver.cpp:354] Sparsity after update:
I0703 01:50:27.659281 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 01:50:27.659297 31050 net.cpp:1851] conv1a_param_0(0) 
I0703 01:50:27.659306 31050 net.cpp:1851] conv1b_param_0(0.0499) 
I0703 01:50:27.659308 31050 net.cpp:1851] ctx_conv1_param_0(0.05) 
I0703 01:50:27.659310 31050 net.cpp:1851] ctx_conv2_param_0(0.05) 
I0703 01:50:27.659312 31050 net.cpp:1851] ctx_conv3_param_0(0.05) 
I0703 01:50:27.659314 31050 net.cpp:1851] ctx_conv4_param_0(0.05) 
I0703 01:50:27.659317 31050 net.cpp:1851] ctx_final_param_0(0.00195) 
I0703 01:50:27.659322 31050 net.cpp:1851] out3a_param_0(0.05) 
I0703 01:50:27.659329 31050 net.cpp:1851] out5a_param_0(0.0499) 
I0703 01:50:27.659334 31050 net.cpp:1851] res2a_branch2a_param_0(0.05) 
I0703 01:50:27.659340 31050 net.cpp:1851] res2a_branch2b_param_0(0.0499) 
I0703 01:50:27.659346 31050 net.cpp:1851] res3a_branch2a_param_0(0.05) 
I0703 01:50:27.659350 31050 net.cpp:1851] res3a_branch2b_param_0(0.05) 
I0703 01:50:27.659355 31050 net.cpp:1851] res4a_branch2a_param_0(0.05) 
I0703 01:50:27.659359 31050 net.cpp:1851] res4a_branch2b_param_0(0.05) 
I0703 01:50:27.659363 31050 net.cpp:1851] res5a_branch2a_param_0(0.0498) 
I0703 01:50:27.659368 31050 net.cpp:1851] res5a_branch2b_param_0(0.0499) 
I0703 01:50:27.659370 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (133917/2.69117e+06) 0.0498
I0703 01:50:27.889353 31050 solver.cpp:290] Iteration 5000 (4.1297 iter/s, 24.2149s/100 iter), loss = 0.0279282
I0703 01:50:27.889374 31050 solver.cpp:309]     Train net output #0: loss = 0.0279282 (* 1 = 0.0279282 loss)
I0703 01:50:27.889381 31050 sgd_solver.cpp:106] Iteration 5000, lr = 1e-05
I0703 01:50:27.890333 31050 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.1
I0703 01:50:28.134716 31050 net.cpp:1824] All zero weights of convolution layers are frozen
I0703 01:50:52.342419 31050 solver.cpp:290] Iteration 5100 (4.08959 iter/s, 24.4523s/100 iter), loss = 0.0217704
I0703 01:50:52.342445 31050 solver.cpp:309]     Train net output #0: loss = 0.0217704 (* 1 = 0.0217704 loss)
I0703 01:50:52.342453 31050 sgd_solver.cpp:106] Iteration 5100, lr = 1e-05
I0703 01:51:16.504767 31050 solver.cpp:290] Iteration 5200 (4.13879 iter/s, 24.1616s/100 iter), loss = 0.0368335
I0703 01:51:16.504890 31050 solver.cpp:309]     Train net output #0: loss = 0.0368335 (* 1 = 0.0368335 loss)
I0703 01:51:16.504900 31050 sgd_solver.cpp:106] Iteration 5200, lr = 1e-05
I0703 01:51:40.687316 31050 solver.cpp:290] Iteration 5300 (4.13535 iter/s, 24.1817s/100 iter), loss = 0.0176255
I0703 01:51:40.687340 31050 solver.cpp:309]     Train net output #0: loss = 0.0176254 (* 1 = 0.0176254 loss)
I0703 01:51:40.687347 31050 sgd_solver.cpp:106] Iteration 5300, lr = 1e-05
I0703 01:52:04.847888 31050 solver.cpp:290] Iteration 5400 (4.1391 iter/s, 24.1599s/100 iter), loss = 0.0177825
I0703 01:52:04.848000 31050 solver.cpp:309]     Train net output #0: loss = 0.0177825 (* 1 = 0.0177825 loss)
I0703 01:52:04.848009 31050 sgd_solver.cpp:106] Iteration 5400, lr = 1e-05
I0703 01:52:29.013370 31050 solver.cpp:290] Iteration 5500 (4.13827 iter/s, 24.1647s/100 iter), loss = 0.0255562
I0703 01:52:29.013393 31050 solver.cpp:309]     Train net output #0: loss = 0.0255562 (* 1 = 0.0255562 loss)
I0703 01:52:29.013401 31050 sgd_solver.cpp:106] Iteration 5500, lr = 1e-05
I0703 01:52:53.178630 31050 solver.cpp:290] Iteration 5600 (4.13829 iter/s, 24.1645s/100 iter), loss = 0.0267434
I0703 01:52:53.178759 31050 solver.cpp:309]     Train net output #0: loss = 0.0267433 (* 1 = 0.0267433 loss)
I0703 01:52:53.178771 31050 sgd_solver.cpp:106] Iteration 5600, lr = 1e-05
I0703 01:53:17.351961 31050 solver.cpp:290] Iteration 5700 (4.13693 iter/s, 24.1725s/100 iter), loss = 0.0290653
I0703 01:53:17.351986 31050 solver.cpp:309]     Train net output #0: loss = 0.0290652 (* 1 = 0.0290652 loss)
I0703 01:53:17.351994 31050 sgd_solver.cpp:106] Iteration 5700, lr = 1e-05
I0703 01:53:41.529356 31050 solver.cpp:290] Iteration 5800 (4.13622 iter/s, 24.1767s/100 iter), loss = 0.0345044
I0703 01:53:41.529469 31050 solver.cpp:309]     Train net output #0: loss = 0.0345044 (* 1 = 0.0345044 loss)
I0703 01:53:41.529479 31050 sgd_solver.cpp:106] Iteration 5800, lr = 1e-05
I0703 01:54:05.654340 31050 solver.cpp:290] Iteration 5900 (4.14522 iter/s, 24.1242s/100 iter), loss = 0.0220204
I0703 01:54:05.654362 31050 solver.cpp:309]     Train net output #0: loss = 0.0220203 (* 1 = 0.0220203 loss)
I0703 01:54:05.654369 31050 sgd_solver.cpp:106] Iteration 5900, lr = 1e-05
I0703 01:54:29.564985 31050 solver.cpp:354] Sparsity after update:
I0703 01:54:29.566843 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 01:54:29.566853 31050 net.cpp:1851] conv1a_param_0(0.05) 
I0703 01:54:29.566862 31050 net.cpp:1851] conv1b_param_0(0.0998) 
I0703 01:54:29.566867 31050 net.cpp:1851] ctx_conv1_param_0(0.1) 
I0703 01:54:29.566871 31050 net.cpp:1851] ctx_conv2_param_0(0.1) 
I0703 01:54:29.566875 31050 net.cpp:1851] ctx_conv3_param_0(0.1) 
I0703 01:54:29.566879 31050 net.cpp:1851] ctx_conv4_param_0(0.1) 
I0703 01:54:29.566884 31050 net.cpp:1851] ctx_final_param_0(0.00629) 
I0703 01:54:29.566887 31050 net.cpp:1851] out3a_param_0(0.1) 
I0703 01:54:29.566891 31050 net.cpp:1851] out5a_param_0(0.1) 
I0703 01:54:29.566895 31050 net.cpp:1851] res2a_branch2a_param_0(0.1) 
I0703 01:54:29.566900 31050 net.cpp:1851] res2a_branch2b_param_0(0.0999) 
I0703 01:54:29.566902 31050 net.cpp:1851] res3a_branch2a_param_0(0.1) 
I0703 01:54:29.566906 31050 net.cpp:1851] res3a_branch2b_param_0(0.1) 
I0703 01:54:29.566911 31050 net.cpp:1851] res4a_branch2a_param_0(0.1) 
I0703 01:54:29.566915 31050 net.cpp:1851] res4a_branch2b_param_0(0.1) 
I0703 01:54:29.566918 31050 net.cpp:1851] res5a_branch2a_param_0(0.0995) 
I0703 01:54:29.566921 31050 net.cpp:1851] res5a_branch2b_param_0(0.0993) 
I0703 01:54:29.566926 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (267512/2.69117e+06) 0.0994
I0703 01:54:29.567070 31050 solver.cpp:473] Iteration 6000, Testing net (#0)
I0703 01:55:17.595726 31050 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.954825
I0703 01:55:17.595831 31050 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.999647
I0703 01:55:17.595839 31050 solver.cpp:546]     Test net output #2: loss = 0.151565 (* 1 = 0.151565 loss)
I0703 01:55:17.857275 31050 solver.cpp:290] Iteration 6000 (1.38502 iter/s, 72.2009s/100 iter), loss = 0.030773
I0703 01:55:17.857298 31050 solver.cpp:309]     Train net output #0: loss = 0.030773 (* 1 = 0.030773 loss)
I0703 01:55:17.857306 31050 sgd_solver.cpp:106] Iteration 6000, lr = 1e-05
I0703 01:55:17.858268 31050 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.15
I0703 01:55:18.160099 31050 net.cpp:1824] All zero weights of convolution layers are frozen
I0703 01:55:41.734575 31050 solver.cpp:290] Iteration 6100 (4.1882 iter/s, 23.8766s/100 iter), loss = 0.0239002
I0703 01:55:41.734599 31050 solver.cpp:309]     Train net output #0: loss = 0.0239002 (* 1 = 0.0239002 loss)
I0703 01:55:41.734606 31050 sgd_solver.cpp:106] Iteration 6100, lr = 1e-05
I0703 01:56:05.903573 31050 solver.cpp:290] Iteration 6200 (4.13765 iter/s, 24.1683s/100 iter), loss = 0.0312084
I0703 01:56:05.903682 31050 solver.cpp:309]     Train net output #0: loss = 0.0312084 (* 1 = 0.0312084 loss)
I0703 01:56:05.903693 31050 sgd_solver.cpp:106] Iteration 6200, lr = 1e-05
I0703 01:56:30.131775 31050 solver.cpp:290] Iteration 6300 (4.12756 iter/s, 24.2274s/100 iter), loss = 0.0308336
I0703 01:56:30.131799 31050 solver.cpp:309]     Train net output #0: loss = 0.0308336 (* 1 = 0.0308336 loss)
I0703 01:56:30.131806 31050 sgd_solver.cpp:106] Iteration 6300, lr = 1e-05
I0703 01:56:54.291769 31050 solver.cpp:290] Iteration 6400 (4.1392 iter/s, 24.1593s/100 iter), loss = 0.0155741
I0703 01:56:54.291811 31050 solver.cpp:309]     Train net output #0: loss = 0.0155741 (* 1 = 0.0155741 loss)
I0703 01:56:54.291820 31050 sgd_solver.cpp:106] Iteration 6400, lr = 1e-05
I0703 01:57:18.442163 31050 solver.cpp:290] Iteration 6500 (4.14084 iter/s, 24.1497s/100 iter), loss = 0.0216166
I0703 01:57:18.442188 31050 solver.cpp:309]     Train net output #0: loss = 0.0216165 (* 1 = 0.0216165 loss)
I0703 01:57:18.442194 31050 sgd_solver.cpp:106] Iteration 6500, lr = 1e-05
I0703 01:57:42.615197 31050 solver.cpp:290] Iteration 6600 (4.13696 iter/s, 24.1723s/100 iter), loss = 0.0208513
I0703 01:57:42.615305 31050 solver.cpp:309]     Train net output #0: loss = 0.0208513 (* 1 = 0.0208513 loss)
I0703 01:57:42.615314 31050 sgd_solver.cpp:106] Iteration 6600, lr = 1e-05
I0703 01:58:06.784607 31050 solver.cpp:290] Iteration 6700 (4.1376 iter/s, 24.1686s/100 iter), loss = 0.0315002
I0703 01:58:06.784629 31050 solver.cpp:309]     Train net output #0: loss = 0.0315002 (* 1 = 0.0315002 loss)
I0703 01:58:06.784636 31050 sgd_solver.cpp:106] Iteration 6700, lr = 1e-05
I0703 01:58:30.923121 31050 solver.cpp:290] Iteration 6800 (4.14288 iter/s, 24.1378s/100 iter), loss = 0.0171216
I0703 01:58:30.923162 31050 solver.cpp:309]     Train net output #0: loss = 0.0171216 (* 1 = 0.0171216 loss)
I0703 01:58:30.923171 31050 sgd_solver.cpp:106] Iteration 6800, lr = 1e-05
I0703 01:58:55.071374 31050 solver.cpp:290] Iteration 6900 (4.14121 iter/s, 24.1475s/100 iter), loss = 0.0244313
I0703 01:58:55.071398 31050 solver.cpp:309]     Train net output #0: loss = 0.0244313 (* 1 = 0.0244313 loss)
I0703 01:58:55.071404 31050 sgd_solver.cpp:106] Iteration 6900, lr = 1e-05
I0703 01:59:18.978595 31050 solver.cpp:354] Sparsity after update:
I0703 01:59:19.022222 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 01:59:19.022238 31050 net.cpp:1851] conv1a_param_0(0.075) 
I0703 01:59:19.022244 31050 net.cpp:1851] conv1b_param_0(0.15) 
I0703 01:59:19.022248 31050 net.cpp:1851] ctx_conv1_param_0(0.15) 
I0703 01:59:19.022249 31050 net.cpp:1851] ctx_conv2_param_0(0.15) 
I0703 01:59:19.022250 31050 net.cpp:1851] ctx_conv3_param_0(0.15) 
I0703 01:59:19.022253 31050 net.cpp:1851] ctx_conv4_param_0(0.15) 
I0703 01:59:19.022254 31050 net.cpp:1851] ctx_final_param_0(0.00152) 
I0703 01:59:19.022256 31050 net.cpp:1851] out3a_param_0(0.15) 
I0703 01:59:19.022258 31050 net.cpp:1851] out5a_param_0(0.15) 
I0703 01:59:19.022260 31050 net.cpp:1851] res2a_branch2a_param_0(0.15) 
I0703 01:59:19.022263 31050 net.cpp:1851] res2a_branch2b_param_0(0.15) 
I0703 01:59:19.022264 31050 net.cpp:1851] res3a_branch2a_param_0(0.15) 
I0703 01:59:19.022266 31050 net.cpp:1851] res3a_branch2b_param_0(0.15) 
I0703 01:59:19.022269 31050 net.cpp:1851] res4a_branch2a_param_0(0.15) 
I0703 01:59:19.022270 31050 net.cpp:1851] res4a_branch2b_param_0(0.15) 
I0703 01:59:19.022274 31050 net.cpp:1851] res5a_branch2a_param_0(0.149) 
I0703 01:59:19.022276 31050 net.cpp:1851] res5a_branch2b_param_0(0.15) 
I0703 01:59:19.022279 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (401925/2.69117e+06) 0.149
I0703 01:59:19.253029 31050 solver.cpp:290] Iteration 7000 (4.13549 iter/s, 24.181s/100 iter), loss = 0.0218743
I0703 01:59:19.253054 31050 solver.cpp:309]     Train net output #0: loss = 0.0218743 (* 1 = 0.0218743 loss)
I0703 01:59:19.253062 31050 sgd_solver.cpp:106] Iteration 7000, lr = 1e-05
I0703 01:59:19.254082 31050 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.2
I0703 01:59:19.572204 31050 net.cpp:1824] All zero weights of convolution layers are frozen
I0703 01:59:43.718422 31050 solver.cpp:290] Iteration 7100 (4.08752 iter/s, 24.4647s/100 iter), loss = 0.0270131
I0703 01:59:43.718448 31050 solver.cpp:309]     Train net output #0: loss = 0.0270131 (* 1 = 0.0270131 loss)
I0703 01:59:43.718456 31050 sgd_solver.cpp:106] Iteration 7100, lr = 1e-05
I0703 02:00:07.864836 31050 solver.cpp:290] Iteration 7200 (4.14152 iter/s, 24.1457s/100 iter), loss = 0.0240564
I0703 02:00:07.864946 31050 solver.cpp:309]     Train net output #0: loss = 0.0240564 (* 1 = 0.0240564 loss)
I0703 02:00:07.864958 31050 sgd_solver.cpp:106] Iteration 7200, lr = 1e-05
I0703 02:00:32.047348 31050 solver.cpp:290] Iteration 7300 (4.13535 iter/s, 24.1817s/100 iter), loss = 0.0215663
I0703 02:00:32.047369 31050 solver.cpp:309]     Train net output #0: loss = 0.0215662 (* 1 = 0.0215662 loss)
I0703 02:00:32.047376 31050 sgd_solver.cpp:106] Iteration 7300, lr = 1e-05
I0703 02:00:56.247280 31050 solver.cpp:290] Iteration 7400 (4.13236 iter/s, 24.1992s/100 iter), loss = 0.0214787
I0703 02:00:56.247385 31050 solver.cpp:309]     Train net output #0: loss = 0.0214787 (* 1 = 0.0214787 loss)
I0703 02:00:56.247396 31050 sgd_solver.cpp:106] Iteration 7400, lr = 1e-05
I0703 02:01:20.384719 31050 solver.cpp:290] Iteration 7500 (4.14307 iter/s, 24.1367s/100 iter), loss = 0.0434618
I0703 02:01:20.384744 31050 solver.cpp:309]     Train net output #0: loss = 0.0434618 (* 1 = 0.0434618 loss)
I0703 02:01:20.384754 31050 sgd_solver.cpp:106] Iteration 7500, lr = 1e-05
I0703 02:01:44.550561 31050 solver.cpp:290] Iteration 7600 (4.13819 iter/s, 24.1651s/100 iter), loss = 0.0159459
I0703 02:01:44.550603 31050 solver.cpp:309]     Train net output #0: loss = 0.0159458 (* 1 = 0.0159458 loss)
I0703 02:01:44.550611 31050 sgd_solver.cpp:106] Iteration 7600, lr = 1e-05
I0703 02:02:08.678195 31050 solver.cpp:290] Iteration 7700 (4.14475 iter/s, 24.1269s/100 iter), loss = 0.028682
I0703 02:02:08.678217 31050 solver.cpp:309]     Train net output #0: loss = 0.0286819 (* 1 = 0.0286819 loss)
I0703 02:02:08.678225 31050 sgd_solver.cpp:106] Iteration 7700, lr = 1e-05
I0703 02:02:32.839287 31050 solver.cpp:290] Iteration 7800 (4.139 iter/s, 24.1604s/100 iter), loss = 0.029975
I0703 02:02:32.839344 31050 solver.cpp:309]     Train net output #0: loss = 0.029975 (* 1 = 0.029975 loss)
I0703 02:02:32.839354 31050 sgd_solver.cpp:106] Iteration 7800, lr = 1e-05
I0703 02:02:57.051514 31050 solver.cpp:290] Iteration 7900 (4.13027 iter/s, 24.2115s/100 iter), loss = 0.0293056
I0703 02:02:57.051538 31050 solver.cpp:309]     Train net output #0: loss = 0.0293055 (* 1 = 0.0293055 loss)
I0703 02:02:57.051545 31050 sgd_solver.cpp:106] Iteration 7900, lr = 1e-05
I0703 02:03:20.959692 31050 solver.cpp:354] Sparsity after update:
I0703 02:03:20.961550 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 02:03:20.961558 31050 net.cpp:1851] conv1a_param_0(0.1) 
I0703 02:03:20.961566 31050 net.cpp:1851] conv1b_param_0(0.2) 
I0703 02:03:20.961570 31050 net.cpp:1851] ctx_conv1_param_0(0.2) 
I0703 02:03:20.961571 31050 net.cpp:1851] ctx_conv2_param_0(0.2) 
I0703 02:03:20.961575 31050 net.cpp:1851] ctx_conv3_param_0(0.2) 
I0703 02:03:20.961576 31050 net.cpp:1851] ctx_conv4_param_0(0.2) 
I0703 02:03:20.961580 31050 net.cpp:1851] ctx_final_param_0(0.000217) 
I0703 02:03:20.961581 31050 net.cpp:1851] out3a_param_0(0.2) 
I0703 02:03:20.961583 31050 net.cpp:1851] out5a_param_0(0.2) 
I0703 02:03:20.961586 31050 net.cpp:1851] res2a_branch2a_param_0(0.2) 
I0703 02:03:20.961588 31050 net.cpp:1851] res2a_branch2b_param_0(0.2) 
I0703 02:03:20.961591 31050 net.cpp:1851] res3a_branch2a_param_0(0.2) 
I0703 02:03:20.961594 31050 net.cpp:1851] res3a_branch2b_param_0(0.2) 
I0703 02:03:20.961598 31050 net.cpp:1851] res4a_branch2a_param_0(0.2) 
I0703 02:03:20.961601 31050 net.cpp:1851] res4a_branch2b_param_0(0.2) 
I0703 02:03:20.961604 31050 net.cpp:1851] res5a_branch2a_param_0(0.2) 
I0703 02:03:20.961608 31050 net.cpp:1851] res5a_branch2b_param_0(0.2) 
I0703 02:03:20.961611 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (536886/2.69117e+06) 0.199
I0703 02:03:20.961746 31050 solver.cpp:473] Iteration 8000, Testing net (#0)
I0703 02:04:08.928622 31050 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.954519
I0703 02:04:08.928717 31050 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.999689
I0703 02:04:08.928725 31050 solver.cpp:546]     Test net output #2: loss = 0.152622 (* 1 = 0.152622 loss)
I0703 02:04:09.179185 31050 solver.cpp:290] Iteration 8000 (1.38647 iter/s, 72.1257s/100 iter), loss = 0.0221487
I0703 02:04:09.179209 31050 solver.cpp:309]     Train net output #0: loss = 0.0221487 (* 1 = 0.0221487 loss)
I0703 02:04:09.179216 31050 sgd_solver.cpp:106] Iteration 8000, lr = 1e-05
I0703 02:04:09.180209 31050 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.25
I0703 02:04:09.558549 31050 net.cpp:1824] All zero weights of convolution layers are frozen
I0703 02:04:33.058699 31050 solver.cpp:290] Iteration 8100 (4.18781 iter/s, 23.8788s/100 iter), loss = 0.0280045
I0703 02:04:33.058722 31050 solver.cpp:309]     Train net output #0: loss = 0.0280045 (* 1 = 0.0280045 loss)
I0703 02:04:33.058729 31050 sgd_solver.cpp:106] Iteration 8100, lr = 1e-05
I0703 02:04:57.196285 31050 solver.cpp:290] Iteration 8200 (4.14304 iter/s, 24.1369s/100 iter), loss = 0.0237745
I0703 02:04:57.196393 31050 solver.cpp:309]     Train net output #0: loss = 0.0237745 (* 1 = 0.0237745 loss)
I0703 02:04:57.196405 31050 sgd_solver.cpp:106] Iteration 8200, lr = 1e-05
I0703 02:05:21.375666 31050 solver.cpp:290] Iteration 8300 (4.13589 iter/s, 24.1786s/100 iter), loss = 0.0198021
I0703 02:05:21.375690 31050 solver.cpp:309]     Train net output #0: loss = 0.0198021 (* 1 = 0.0198021 loss)
I0703 02:05:21.375699 31050 sgd_solver.cpp:106] Iteration 8300, lr = 1e-05
I0703 02:05:45.541051 31050 solver.cpp:290] Iteration 8400 (4.13827 iter/s, 24.1647s/100 iter), loss = 0.0246437
I0703 02:05:45.541133 31050 solver.cpp:309]     Train net output #0: loss = 0.0246437 (* 1 = 0.0246437 loss)
I0703 02:05:45.541146 31050 sgd_solver.cpp:106] Iteration 8400, lr = 1e-05
I0703 02:06:09.709440 31050 solver.cpp:290] Iteration 8500 (4.13776 iter/s, 24.1676s/100 iter), loss = 0.0164813
I0703 02:06:09.709465 31050 solver.cpp:309]     Train net output #0: loss = 0.0164813 (* 1 = 0.0164813 loss)
I0703 02:06:09.709472 31050 sgd_solver.cpp:106] Iteration 8500, lr = 1e-05
I0703 02:06:33.883728 31050 solver.cpp:290] Iteration 8600 (4.13674 iter/s, 24.1736s/100 iter), loss = 0.039042
I0703 02:06:33.883848 31050 solver.cpp:309]     Train net output #0: loss = 0.039042 (* 1 = 0.039042 loss)
I0703 02:06:33.883859 31050 sgd_solver.cpp:106] Iteration 8600, lr = 1e-05
I0703 02:06:58.040222 31050 solver.cpp:290] Iteration 8700 (4.13981 iter/s, 24.1557s/100 iter), loss = 0.0260481
I0703 02:06:58.040243 31050 solver.cpp:309]     Train net output #0: loss = 0.0260481 (* 1 = 0.0260481 loss)
I0703 02:06:58.040251 31050 sgd_solver.cpp:106] Iteration 8700, lr = 1e-05
I0703 02:07:22.200534 31050 solver.cpp:290] Iteration 8800 (4.13914 iter/s, 24.1596s/100 iter), loss = 0.0231128
I0703 02:07:22.200644 31050 solver.cpp:309]     Train net output #0: loss = 0.0231128 (* 1 = 0.0231128 loss)
I0703 02:07:22.200659 31050 sgd_solver.cpp:106] Iteration 8800, lr = 1e-05
I0703 02:07:46.355247 31050 solver.cpp:290] Iteration 8900 (4.14011 iter/s, 24.1539s/100 iter), loss = 0.0351508
I0703 02:07:46.355269 31050 solver.cpp:309]     Train net output #0: loss = 0.0351508 (* 1 = 0.0351508 loss)
I0703 02:07:46.355276 31050 sgd_solver.cpp:106] Iteration 8900, lr = 1e-05
I0703 02:08:10.248133 31050 solver.cpp:354] Sparsity after update:
I0703 02:08:10.293740 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 02:08:10.293756 31050 net.cpp:1851] conv1a_param_0(0.125) 
I0703 02:08:10.293767 31050 net.cpp:1851] conv1b_param_0(0.25) 
I0703 02:08:10.293771 31050 net.cpp:1851] ctx_conv1_param_0(0.25) 
I0703 02:08:10.293773 31050 net.cpp:1851] ctx_conv2_param_0(0.25) 
I0703 02:08:10.293778 31050 net.cpp:1851] ctx_conv3_param_0(0.25) 
I0703 02:08:10.293783 31050 net.cpp:1851] ctx_conv4_param_0(0.25) 
I0703 02:08:10.293787 31050 net.cpp:1851] ctx_final_param_0(0.0013) 
I0703 02:08:10.293792 31050 net.cpp:1851] out3a_param_0(0.25) 
I0703 02:08:10.293797 31050 net.cpp:1851] out5a_param_0(0.25) 
I0703 02:08:10.293800 31050 net.cpp:1851] res2a_branch2a_param_0(0.25) 
I0703 02:08:10.293803 31050 net.cpp:1851] res2a_branch2b_param_0(0.25) 
I0703 02:08:10.293808 31050 net.cpp:1851] res3a_branch2a_param_0(0.25) 
I0703 02:08:10.293812 31050 net.cpp:1851] res3a_branch2b_param_0(0.25) 
I0703 02:08:10.293817 31050 net.cpp:1851] res4a_branch2a_param_0(0.25) 
I0703 02:08:10.293820 31050 net.cpp:1851] res4a_branch2b_param_0(0.25) 
I0703 02:08:10.293824 31050 net.cpp:1851] res5a_branch2a_param_0(0.25) 
I0703 02:08:10.293829 31050 net.cpp:1851] res5a_branch2b_param_0(0.249) 
I0703 02:08:10.293833 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (670847/2.69117e+06) 0.249
I0703 02:08:10.525452 31050 solver.cpp:290] Iteration 9000 (4.13744 iter/s, 24.1695s/100 iter), loss = 0.0226108
I0703 02:08:10.525478 31050 solver.cpp:309]     Train net output #0: loss = 0.0226108 (* 1 = 0.0226108 loss)
I0703 02:08:10.525487 31050 sgd_solver.cpp:106] Iteration 9000, lr = 1e-05
I0703 02:08:10.526499 31050 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.3
I0703 02:08:10.949002 31050 net.cpp:1824] All zero weights of convolution layers are frozen
I0703 02:08:35.075460 31050 solver.cpp:290] Iteration 9100 (4.07343 iter/s, 24.5493s/100 iter), loss = 0.0216661
I0703 02:08:35.075482 31050 solver.cpp:309]     Train net output #0: loss = 0.0216661 (* 1 = 0.0216661 loss)
I0703 02:08:35.075489 31050 sgd_solver.cpp:106] Iteration 9100, lr = 1e-05
I0703 02:08:59.313623 31050 solver.cpp:290] Iteration 9200 (4.12584 iter/s, 24.2375s/100 iter), loss = 0.0187891
I0703 02:08:59.313732 31050 solver.cpp:309]     Train net output #0: loss = 0.018789 (* 1 = 0.018789 loss)
I0703 02:08:59.313745 31050 sgd_solver.cpp:106] Iteration 9200, lr = 1e-05
I0703 02:09:23.485093 31050 solver.cpp:290] Iteration 9300 (4.13724 iter/s, 24.1707s/100 iter), loss = 0.0219785
I0703 02:09:23.485118 31050 solver.cpp:309]     Train net output #0: loss = 0.0219784 (* 1 = 0.0219784 loss)
I0703 02:09:23.485126 31050 sgd_solver.cpp:106] Iteration 9300, lr = 1e-05
I0703 02:09:47.653913 31050 solver.cpp:290] Iteration 9400 (4.13768 iter/s, 24.1681s/100 iter), loss = 0.0302146
I0703 02:09:47.653988 31050 solver.cpp:309]     Train net output #0: loss = 0.0302146 (* 1 = 0.0302146 loss)
I0703 02:09:47.653997 31050 sgd_solver.cpp:106] Iteration 9400, lr = 1e-05
I0703 02:10:11.813563 31050 solver.cpp:290] Iteration 9500 (4.13926 iter/s, 24.1589s/100 iter), loss = 0.0197126
I0703 02:10:11.813587 31050 solver.cpp:309]     Train net output #0: loss = 0.0197125 (* 1 = 0.0197125 loss)
I0703 02:10:11.813594 31050 sgd_solver.cpp:106] Iteration 9500, lr = 1e-05
I0703 02:10:35.964177 31050 solver.cpp:290] Iteration 9600 (4.1408 iter/s, 24.1499s/100 iter), loss = 0.02523
I0703 02:10:35.964272 31050 solver.cpp:309]     Train net output #0: loss = 0.02523 (* 1 = 0.02523 loss)
I0703 02:10:35.964283 31050 sgd_solver.cpp:106] Iteration 9600, lr = 1e-05
I0703 02:11:00.139766 31050 solver.cpp:290] Iteration 9700 (4.13653 iter/s, 24.1748s/100 iter), loss = 0.0196739
I0703 02:11:00.139791 31050 solver.cpp:309]     Train net output #0: loss = 0.0196739 (* 1 = 0.0196739 loss)
I0703 02:11:00.139797 31050 sgd_solver.cpp:106] Iteration 9700, lr = 1e-05
I0703 02:11:24.363668 31050 solver.cpp:290] Iteration 9800 (4.12827 iter/s, 24.2232s/100 iter), loss = 0.0207201
I0703 02:11:24.363747 31050 solver.cpp:309]     Train net output #0: loss = 0.02072 (* 1 = 0.02072 loss)
I0703 02:11:24.363756 31050 sgd_solver.cpp:106] Iteration 9800, lr = 1e-05
I0703 02:11:48.509763 31050 solver.cpp:290] Iteration 9900 (4.14158 iter/s, 24.1454s/100 iter), loss = 0.037172
I0703 02:11:48.509788 31050 solver.cpp:309]     Train net output #0: loss = 0.0371719 (* 1 = 0.0371719 loss)
I0703 02:11:48.509794 31050 sgd_solver.cpp:106] Iteration 9900, lr = 1e-05
I0703 02:12:12.436151 31050 solver.cpp:600] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-02_23-02-42/sparse/cityscapes5_jsegnet21v2_iter_10000.caffemodel
I0703 02:12:12.505095 31050 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-02_23-02-42/sparse/cityscapes5_jsegnet21v2_iter_10000.solverstate
I0703 02:12:12.521217 31050 solver.cpp:354] Sparsity after update:
I0703 02:12:12.522421 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 02:12:12.522429 31050 net.cpp:1851] conv1a_param_0(0.15) 
I0703 02:12:12.522438 31050 net.cpp:1851] conv1b_param_0(0.3) 
I0703 02:12:12.522439 31050 net.cpp:1851] ctx_conv1_param_0(0.3) 
I0703 02:12:12.522441 31050 net.cpp:1851] ctx_conv2_param_0(0.3) 
I0703 02:12:12.522444 31050 net.cpp:1851] ctx_conv3_param_0(0.3) 
I0703 02:12:12.522445 31050 net.cpp:1851] ctx_conv4_param_0(0.3) 
I0703 02:12:12.522447 31050 net.cpp:1851] ctx_final_param_0(0.0076) 
I0703 02:12:12.522449 31050 net.cpp:1851] out3a_param_0(0.3) 
I0703 02:12:12.522452 31050 net.cpp:1851] out5a_param_0(0.3) 
I0703 02:12:12.522454 31050 net.cpp:1851] res2a_branch2a_param_0(0.3) 
I0703 02:12:12.522455 31050 net.cpp:1851] res2a_branch2b_param_0(0.3) 
I0703 02:12:12.522457 31050 net.cpp:1851] res3a_branch2a_param_0(0.3) 
I0703 02:12:12.522459 31050 net.cpp:1851] res3a_branch2b_param_0(0.3) 
I0703 02:12:12.522461 31050 net.cpp:1851] res4a_branch2a_param_0(0.3) 
I0703 02:12:12.522464 31050 net.cpp:1851] res4a_branch2b_param_0(0.3) 
I0703 02:12:12.522465 31050 net.cpp:1851] res5a_branch2a_param_0(0.3) 
I0703 02:12:12.522467 31050 net.cpp:1851] res5a_branch2b_param_0(0.3) 
I0703 02:12:12.522469 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (805328/2.69117e+06) 0.299
I0703 02:12:12.522617 31050 solver.cpp:473] Iteration 10000, Testing net (#0)
I0703 02:13:00.543434 31050 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.954991
I0703 02:13:00.543525 31050 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.999878
I0703 02:13:00.543534 31050 solver.cpp:546]     Test net output #2: loss = 0.139855 (* 1 = 0.139855 loss)
I0703 02:13:00.789273 31050 solver.cpp:290] Iteration 10000 (1.38356 iter/s, 72.2775s/100 iter), loss = 0.0175528
I0703 02:13:00.789296 31050 solver.cpp:309]     Train net output #0: loss = 0.0175528 (* 1 = 0.0175528 loss)
I0703 02:13:00.789302 31050 sgd_solver.cpp:106] Iteration 10000, lr = 1e-05
I0703 02:13:00.790309 31050 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.35
I0703 02:13:01.270591 31050 net.cpp:1824] All zero weights of convolution layers are frozen
I0703 02:13:24.788717 31050 solver.cpp:290] Iteration 10100 (4.16688 iter/s, 23.9988s/100 iter), loss = 0.0189557
I0703 02:13:24.788739 31050 solver.cpp:309]     Train net output #0: loss = 0.0189557 (* 1 = 0.0189557 loss)
I0703 02:13:24.788746 31050 sgd_solver.cpp:106] Iteration 10100, lr = 1e-05
I0703 02:13:48.932126 31050 solver.cpp:290] Iteration 10200 (4.14204 iter/s, 24.1427s/100 iter), loss = 0.0189588
I0703 02:13:48.932199 31050 solver.cpp:309]     Train net output #0: loss = 0.0189587 (* 1 = 0.0189587 loss)
I0703 02:13:48.932209 31050 sgd_solver.cpp:106] Iteration 10200, lr = 1e-05
I0703 02:14:13.128303 31050 solver.cpp:290] Iteration 10300 (4.13302 iter/s, 24.1954s/100 iter), loss = 0.026732
I0703 02:14:13.128325 31050 solver.cpp:309]     Train net output #0: loss = 0.0267319 (* 1 = 0.0267319 loss)
I0703 02:14:13.128334 31050 sgd_solver.cpp:106] Iteration 10300, lr = 1e-05
I0703 02:14:37.309788 31050 solver.cpp:290] Iteration 10400 (4.13552 iter/s, 24.1808s/100 iter), loss = 0.0248204
I0703 02:14:37.309839 31050 solver.cpp:309]     Train net output #0: loss = 0.0248203 (* 1 = 0.0248203 loss)
I0703 02:14:37.309850 31050 sgd_solver.cpp:106] Iteration 10400, lr = 1e-05
I0703 02:15:01.457960 31050 solver.cpp:290] Iteration 10500 (4.14123 iter/s, 24.1474s/100 iter), loss = 0.0284437
I0703 02:15:01.457988 31050 solver.cpp:309]     Train net output #0: loss = 0.0284437 (* 1 = 0.0284437 loss)
I0703 02:15:01.457994 31050 sgd_solver.cpp:106] Iteration 10500, lr = 1e-05
I0703 02:15:25.811691 31050 solver.cpp:290] Iteration 10600 (4.10627 iter/s, 24.353s/100 iter), loss = 0.0247938
I0703 02:15:25.811805 31050 solver.cpp:309]     Train net output #0: loss = 0.0247938 (* 1 = 0.0247938 loss)
I0703 02:15:25.811815 31050 sgd_solver.cpp:106] Iteration 10600, lr = 1e-05
I0703 02:15:50.002423 31050 solver.cpp:290] Iteration 10700 (4.13395 iter/s, 24.1899s/100 iter), loss = 0.0184781
I0703 02:15:50.002447 31050 solver.cpp:309]     Train net output #0: loss = 0.0184781 (* 1 = 0.0184781 loss)
I0703 02:15:50.002454 31050 sgd_solver.cpp:106] Iteration 10700, lr = 1e-05
I0703 02:16:14.138563 31050 solver.cpp:290] Iteration 10800 (4.14329 iter/s, 24.1354s/100 iter), loss = 0.0174261
I0703 02:16:14.138697 31050 solver.cpp:309]     Train net output #0: loss = 0.0174261 (* 1 = 0.0174261 loss)
I0703 02:16:14.138708 31050 sgd_solver.cpp:106] Iteration 10800, lr = 1e-05
I0703 02:16:38.310183 31050 solver.cpp:290] Iteration 10900 (4.13722 iter/s, 24.1708s/100 iter), loss = 0.0166005
I0703 02:16:38.310206 31050 solver.cpp:309]     Train net output #0: loss = 0.0166005 (* 1 = 0.0166005 loss)
I0703 02:16:38.310214 31050 sgd_solver.cpp:106] Iteration 10900, lr = 1e-05
I0703 02:17:02.257072 31050 solver.cpp:354] Sparsity after update:
I0703 02:17:02.297730 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 02:17:02.297747 31050 net.cpp:1851] conv1a_param_0(0.175) 
I0703 02:17:02.297755 31050 net.cpp:1851] conv1b_param_0(0.35) 
I0703 02:17:02.297757 31050 net.cpp:1851] ctx_conv1_param_0(0.35) 
I0703 02:17:02.297760 31050 net.cpp:1851] ctx_conv2_param_0(0.35) 
I0703 02:17:02.297761 31050 net.cpp:1851] ctx_conv3_param_0(0.35) 
I0703 02:17:02.297763 31050 net.cpp:1851] ctx_conv4_param_0(0.35) 
I0703 02:17:02.297765 31050 net.cpp:1851] ctx_final_param_0(0.00195) 
I0703 02:17:02.297767 31050 net.cpp:1851] out3a_param_0(0.35) 
I0703 02:17:02.297770 31050 net.cpp:1851] out5a_param_0(0.35) 
I0703 02:17:02.297771 31050 net.cpp:1851] res2a_branch2a_param_0(0.35) 
I0703 02:17:02.297773 31050 net.cpp:1851] res2a_branch2b_param_0(0.35) 
I0703 02:17:02.297775 31050 net.cpp:1851] res3a_branch2a_param_0(0.35) 
I0703 02:17:02.297777 31050 net.cpp:1851] res3a_branch2b_param_0(0.35) 
I0703 02:17:02.297780 31050 net.cpp:1851] res4a_branch2a_param_0(0.35) 
I0703 02:17:02.297781 31050 net.cpp:1851] res4a_branch2b_param_0(0.35) 
I0703 02:17:02.297782 31050 net.cpp:1851] res5a_branch2a_param_0(0.35) 
I0703 02:17:02.297785 31050 net.cpp:1851] res5a_branch2b_param_0(0.35) 
I0703 02:17:02.297786 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (939608/2.69117e+06) 0.349
I0703 02:17:02.529042 31050 solver.cpp:290] Iteration 11000 (4.12913 iter/s, 24.2182s/100 iter), loss = 0.0339202
I0703 02:17:02.529067 31050 solver.cpp:309]     Train net output #0: loss = 0.0339202 (* 1 = 0.0339202 loss)
I0703 02:17:02.529074 31050 sgd_solver.cpp:106] Iteration 11000, lr = 1e-05
I0703 02:17:02.530058 31050 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.4
I0703 02:17:03.073974 31050 net.cpp:1824] All zero weights of convolution layers are frozen
I0703 02:17:27.208683 31050 solver.cpp:290] Iteration 11100 (4.05204 iter/s, 24.6789s/100 iter), loss = 0.0179118
I0703 02:17:27.208709 31050 solver.cpp:309]     Train net output #0: loss = 0.0179118 (* 1 = 0.0179118 loss)
I0703 02:17:27.208719 31050 sgd_solver.cpp:106] Iteration 11100, lr = 1e-05
I0703 02:17:51.393795 31050 solver.cpp:290] Iteration 11200 (4.13489 iter/s, 24.1844s/100 iter), loss = 0.0203856
I0703 02:17:51.393924 31050 solver.cpp:309]     Train net output #0: loss = 0.0203856 (* 1 = 0.0203856 loss)
I0703 02:17:51.393934 31050 sgd_solver.cpp:106] Iteration 11200, lr = 1e-05
I0703 02:18:15.527884 31050 solver.cpp:290] Iteration 11300 (4.14365 iter/s, 24.1333s/100 iter), loss = 0.0294114
I0703 02:18:15.527918 31050 solver.cpp:309]     Train net output #0: loss = 0.0294114 (* 1 = 0.0294114 loss)
I0703 02:18:15.527930 31050 sgd_solver.cpp:106] Iteration 11300, lr = 1e-05
I0703 02:18:39.670099 31050 solver.cpp:290] Iteration 11400 (4.14224 iter/s, 24.1415s/100 iter), loss = 0.0262624
I0703 02:18:39.670202 31050 solver.cpp:309]     Train net output #0: loss = 0.0262624 (* 1 = 0.0262624 loss)
I0703 02:18:39.670212 31050 sgd_solver.cpp:106] Iteration 11400, lr = 1e-05
I0703 02:19:04.067530 31050 solver.cpp:290] Iteration 11500 (4.09892 iter/s, 24.3967s/100 iter), loss = 0.0246447
I0703 02:19:04.067553 31050 solver.cpp:309]     Train net output #0: loss = 0.0246447 (* 1 = 0.0246447 loss)
I0703 02:19:04.067562 31050 sgd_solver.cpp:106] Iteration 11500, lr = 1e-05
I0703 02:19:28.179029 31050 solver.cpp:290] Iteration 11600 (4.14752 iter/s, 24.1108s/100 iter), loss = 0.0248632
I0703 02:19:28.179132 31050 solver.cpp:309]     Train net output #0: loss = 0.0248632 (* 1 = 0.0248632 loss)
I0703 02:19:28.179144 31050 sgd_solver.cpp:106] Iteration 11600, lr = 1e-05
I0703 02:19:52.361197 31050 solver.cpp:290] Iteration 11700 (4.13541 iter/s, 24.1814s/100 iter), loss = 0.0228802
I0703 02:19:52.361223 31050 solver.cpp:309]     Train net output #0: loss = 0.0228801 (* 1 = 0.0228801 loss)
I0703 02:19:52.361229 31050 sgd_solver.cpp:106] Iteration 11700, lr = 1e-05
I0703 02:20:16.528949 31050 solver.cpp:290] Iteration 11800 (4.13786 iter/s, 24.1671s/100 iter), loss = 0.025833
I0703 02:20:16.529058 31050 solver.cpp:309]     Train net output #0: loss = 0.0258329 (* 1 = 0.0258329 loss)
I0703 02:20:16.529068 31050 sgd_solver.cpp:106] Iteration 11800, lr = 1e-05
I0703 02:20:40.706142 31050 solver.cpp:290] Iteration 11900 (4.13626 iter/s, 24.1764s/100 iter), loss = 0.0259833
I0703 02:20:40.706166 31050 solver.cpp:309]     Train net output #0: loss = 0.0259833 (* 1 = 0.0259833 loss)
I0703 02:20:40.706174 31050 sgd_solver.cpp:106] Iteration 11900, lr = 1e-05
I0703 02:21:04.658290 31050 solver.cpp:354] Sparsity after update:
I0703 02:21:04.660158 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 02:21:04.660167 31050 net.cpp:1851] conv1a_param_0(0.2) 
I0703 02:21:04.660177 31050 net.cpp:1851] conv1b_param_0(0.4) 
I0703 02:21:04.660182 31050 net.cpp:1851] ctx_conv1_param_0(0.4) 
I0703 02:21:04.660187 31050 net.cpp:1851] ctx_conv2_param_0(0.4) 
I0703 02:21:04.660190 31050 net.cpp:1851] ctx_conv3_param_0(0.4) 
I0703 02:21:04.660194 31050 net.cpp:1851] ctx_conv4_param_0(0.4) 
I0703 02:21:04.660199 31050 net.cpp:1851] ctx_final_param_0(0.00608) 
I0703 02:21:04.660204 31050 net.cpp:1851] out3a_param_0(0.4) 
I0703 02:21:04.660207 31050 net.cpp:1851] out5a_param_0(0.4) 
I0703 02:21:04.660210 31050 net.cpp:1851] res2a_branch2a_param_0(0.4) 
I0703 02:21:04.660213 31050 net.cpp:1851] res2a_branch2b_param_0(0.4) 
I0703 02:21:04.660218 31050 net.cpp:1851] res3a_branch2a_param_0(0.4) 
I0703 02:21:04.660220 31050 net.cpp:1851] res3a_branch2b_param_0(0.4) 
I0703 02:21:04.660223 31050 net.cpp:1851] res4a_branch2a_param_0(0.4) 
I0703 02:21:04.660231 31050 net.cpp:1851] res4a_branch2b_param_0(0.4) 
I0703 02:21:04.660235 31050 net.cpp:1851] res5a_branch2a_param_0(0.4) 
I0703 02:21:04.660239 31050 net.cpp:1851] res5a_branch2b_param_0(0.4) 
I0703 02:21:04.660244 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.07404e+06/2.69117e+06) 0.399
I0703 02:21:04.660384 31050 solver.cpp:473] Iteration 12000, Testing net (#0)
I0703 02:21:52.565165 31050 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.954547
I0703 02:21:52.565268 31050 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.999663
I0703 02:21:52.565274 31050 solver.cpp:546]     Test net output #2: loss = 0.147363 (* 1 = 0.147363 loss)
I0703 02:21:52.837323 31050 solver.cpp:290] Iteration 12000 (1.3864 iter/s, 72.1292s/100 iter), loss = 0.0207261
I0703 02:21:52.837347 31050 solver.cpp:309]     Train net output #0: loss = 0.0207261 (* 1 = 0.0207261 loss)
I0703 02:21:52.837353 31050 sgd_solver.cpp:106] Iteration 12000, lr = 1e-05
I0703 02:21:52.838320 31050 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.45
I0703 02:21:53.441296 31050 net.cpp:1824] All zero weights of convolution layers are frozen
I0703 02:22:16.925705 31050 solver.cpp:290] Iteration 12100 (4.1515 iter/s, 24.0877s/100 iter), loss = 0.0194228
I0703 02:22:16.925731 31050 solver.cpp:309]     Train net output #0: loss = 0.0194228 (* 1 = 0.0194228 loss)
I0703 02:22:16.925739 31050 sgd_solver.cpp:106] Iteration 12100, lr = 1e-05
I0703 02:22:41.285823 31050 solver.cpp:290] Iteration 12200 (4.10519 iter/s, 24.3594s/100 iter), loss = 0.0399094
I0703 02:22:41.285933 31050 solver.cpp:309]     Train net output #0: loss = 0.0399093 (* 1 = 0.0399093 loss)
I0703 02:22:41.285944 31050 sgd_solver.cpp:106] Iteration 12200, lr = 1e-05
I0703 02:23:05.422039 31050 solver.cpp:290] Iteration 12300 (4.14328 iter/s, 24.1354s/100 iter), loss = 0.0278212
I0703 02:23:05.422063 31050 solver.cpp:309]     Train net output #0: loss = 0.0278212 (* 1 = 0.0278212 loss)
I0703 02:23:05.422070 31050 sgd_solver.cpp:106] Iteration 12300, lr = 1e-05
I0703 02:23:29.535526 31050 solver.cpp:290] Iteration 12400 (4.14717 iter/s, 24.1128s/100 iter), loss = 0.0174581
I0703 02:23:29.535637 31050 solver.cpp:309]     Train net output #0: loss = 0.0174581 (* 1 = 0.0174581 loss)
I0703 02:23:29.535653 31050 sgd_solver.cpp:106] Iteration 12400, lr = 1e-05
I0703 02:23:53.697687 31050 solver.cpp:290] Iteration 12500 (4.13883 iter/s, 24.1614s/100 iter), loss = 0.0175629
I0703 02:23:53.697710 31050 solver.cpp:309]     Train net output #0: loss = 0.0175629 (* 1 = 0.0175629 loss)
I0703 02:23:53.697717 31050 sgd_solver.cpp:106] Iteration 12500, lr = 1e-05
I0703 02:24:17.845249 31050 solver.cpp:290] Iteration 12600 (4.14132 iter/s, 24.1469s/100 iter), loss = 0.0321011
I0703 02:24:17.845345 31050 solver.cpp:309]     Train net output #0: loss = 0.0321011 (* 1 = 0.0321011 loss)
I0703 02:24:17.845355 31050 sgd_solver.cpp:106] Iteration 12600, lr = 1e-05
I0703 02:24:42.023074 31050 solver.cpp:290] Iteration 12700 (4.13615 iter/s, 24.1771s/100 iter), loss = 0.0348097
I0703 02:24:42.023098 31050 solver.cpp:309]     Train net output #0: loss = 0.0348097 (* 1 = 0.0348097 loss)
I0703 02:24:42.023105 31050 sgd_solver.cpp:106] Iteration 12700, lr = 1e-05
I0703 02:25:06.206933 31050 solver.cpp:290] Iteration 12800 (4.13511 iter/s, 24.1832s/100 iter), loss = 0.0296377
I0703 02:25:06.206989 31050 solver.cpp:309]     Train net output #0: loss = 0.0296377 (* 1 = 0.0296377 loss)
I0703 02:25:06.206997 31050 sgd_solver.cpp:106] Iteration 12800, lr = 1e-05
I0703 02:25:30.393307 31050 solver.cpp:290] Iteration 12900 (4.13468 iter/s, 24.1857s/100 iter), loss = 0.0210725
I0703 02:25:30.393329 31050 solver.cpp:309]     Train net output #0: loss = 0.0210725 (* 1 = 0.0210725 loss)
I0703 02:25:30.393337 31050 sgd_solver.cpp:106] Iteration 12900, lr = 1e-05
I0703 02:25:54.325242 31050 solver.cpp:354] Sparsity after update:
I0703 02:25:54.370759 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 02:25:54.370774 31050 net.cpp:1851] conv1a_param_0(0.225) 
I0703 02:25:54.370784 31050 net.cpp:1851] conv1b_param_0(0.45) 
I0703 02:25:54.370789 31050 net.cpp:1851] ctx_conv1_param_0(0.45) 
I0703 02:25:54.370791 31050 net.cpp:1851] ctx_conv2_param_0(0.45) 
I0703 02:25:54.370795 31050 net.cpp:1851] ctx_conv3_param_0(0.45) 
I0703 02:25:54.370800 31050 net.cpp:1851] ctx_conv4_param_0(0.45) 
I0703 02:25:54.370805 31050 net.cpp:1851] ctx_final_param_0(0.00174) 
I0703 02:25:54.370810 31050 net.cpp:1851] out3a_param_0(0.45) 
I0703 02:25:54.370812 31050 net.cpp:1851] out5a_param_0(0.45) 
I0703 02:25:54.370816 31050 net.cpp:1851] res2a_branch2a_param_0(0.45) 
I0703 02:25:54.370821 31050 net.cpp:1851] res2a_branch2b_param_0(0.45) 
I0703 02:25:54.370824 31050 net.cpp:1851] res3a_branch2a_param_0(0.45) 
I0703 02:25:54.370828 31050 net.cpp:1851] res3a_branch2b_param_0(0.45) 
I0703 02:25:54.370833 31050 net.cpp:1851] res4a_branch2a_param_0(0.45) 
I0703 02:25:54.370837 31050 net.cpp:1851] res4a_branch2b_param_0(0.45) 
I0703 02:25:54.370841 31050 net.cpp:1851] res5a_branch2a_param_0(0.45) 
I0703 02:25:54.370843 31050 net.cpp:1851] res5a_branch2b_param_0(0.45) 
I0703 02:25:54.370846 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.2082e+06/2.69117e+06) 0.449
I0703 02:25:54.600826 31050 solver.cpp:290] Iteration 13000 (4.13106 iter/s, 24.2068s/100 iter), loss = 0.025335
I0703 02:25:54.600852 31050 solver.cpp:309]     Train net output #0: loss = 0.025335 (* 1 = 0.025335 loss)
I0703 02:25:54.600860 31050 sgd_solver.cpp:106] Iteration 13000, lr = 1e-05
I0703 02:25:54.601835 31050 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.5
I0703 02:25:55.263425 31050 net.cpp:1824] All zero weights of convolution layers are frozen
I0703 02:26:19.736353 31050 solver.cpp:290] Iteration 13100 (3.97854 iter/s, 25.1348s/100 iter), loss = 0.0343361
I0703 02:26:19.736378 31050 solver.cpp:309]     Train net output #0: loss = 0.034336 (* 1 = 0.034336 loss)
I0703 02:26:19.736384 31050 sgd_solver.cpp:106] Iteration 13100, lr = 1e-05
I0703 02:26:43.920722 31050 solver.cpp:290] Iteration 13200 (4.13502 iter/s, 24.1837s/100 iter), loss = 0.0268732
I0703 02:26:43.920837 31050 solver.cpp:309]     Train net output #0: loss = 0.0268732 (* 1 = 0.0268732 loss)
I0703 02:26:43.920848 31050 sgd_solver.cpp:106] Iteration 13200, lr = 1e-05
I0703 02:27:08.064492 31050 solver.cpp:290] Iteration 13300 (4.14199 iter/s, 24.143s/100 iter), loss = 0.0308546
I0703 02:27:08.064515 31050 solver.cpp:309]     Train net output #0: loss = 0.0308546 (* 1 = 0.0308546 loss)
I0703 02:27:08.064522 31050 sgd_solver.cpp:106] Iteration 13300, lr = 1e-05
I0703 02:27:32.215654 31050 solver.cpp:290] Iteration 13400 (4.1407 iter/s, 24.1505s/100 iter), loss = 0.026775
I0703 02:27:32.215770 31050 solver.cpp:309]     Train net output #0: loss = 0.0267749 (* 1 = 0.0267749 loss)
I0703 02:27:32.215780 31050 sgd_solver.cpp:106] Iteration 13400, lr = 1e-05
I0703 02:27:56.443253 31050 solver.cpp:290] Iteration 13500 (4.12765 iter/s, 24.2268s/100 iter), loss = 0.0266327
I0703 02:27:56.443275 31050 solver.cpp:309]     Train net output #0: loss = 0.0266327 (* 1 = 0.0266327 loss)
I0703 02:27:56.443282 31050 sgd_solver.cpp:106] Iteration 13500, lr = 1e-05
I0703 02:28:20.589872 31050 solver.cpp:290] Iteration 13600 (4.14148 iter/s, 24.1459s/100 iter), loss = 0.0251182
I0703 02:28:20.589969 31050 solver.cpp:309]     Train net output #0: loss = 0.0251182 (* 1 = 0.0251182 loss)
I0703 02:28:20.589977 31050 sgd_solver.cpp:106] Iteration 13600, lr = 1e-05
I0703 02:28:44.765090 31050 solver.cpp:290] Iteration 13700 (4.1366 iter/s, 24.1745s/100 iter), loss = 0.0243018
I0703 02:28:44.765116 31050 solver.cpp:309]     Train net output #0: loss = 0.0243018 (* 1 = 0.0243018 loss)
I0703 02:28:44.765125 31050 sgd_solver.cpp:106] Iteration 13700, lr = 1e-05
I0703 02:29:08.932721 31050 solver.cpp:290] Iteration 13800 (4.13788 iter/s, 24.167s/100 iter), loss = 0.0240143
I0703 02:29:08.932854 31050 solver.cpp:309]     Train net output #0: loss = 0.0240142 (* 1 = 0.0240142 loss)
I0703 02:29:08.932864 31050 sgd_solver.cpp:106] Iteration 13800, lr = 1e-05
I0703 02:29:33.099200 31050 solver.cpp:290] Iteration 13900 (4.1381 iter/s, 24.1657s/100 iter), loss = 0.0246344
I0703 02:29:33.099222 31050 solver.cpp:309]     Train net output #0: loss = 0.0246344 (* 1 = 0.0246344 loss)
I0703 02:29:33.099230 31050 sgd_solver.cpp:106] Iteration 13900, lr = 1e-05
I0703 02:29:57.056411 31050 solver.cpp:354] Sparsity after update:
I0703 02:29:57.058238 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 02:29:57.058245 31050 net.cpp:1851] conv1a_param_0(0.25) 
I0703 02:29:57.058256 31050 net.cpp:1851] conv1b_param_0(0.5) 
I0703 02:29:57.058261 31050 net.cpp:1851] ctx_conv1_param_0(0.5) 
I0703 02:29:57.058265 31050 net.cpp:1851] ctx_conv2_param_0(0.5) 
I0703 02:29:57.058269 31050 net.cpp:1851] ctx_conv3_param_0(0.5) 
I0703 02:29:57.058274 31050 net.cpp:1851] ctx_conv4_param_0(0.5) 
I0703 02:29:57.058277 31050 net.cpp:1851] ctx_final_param_0(0.00738) 
I0703 02:29:57.058281 31050 net.cpp:1851] out3a_param_0(0.5) 
I0703 02:29:57.058285 31050 net.cpp:1851] out5a_param_0(0.5) 
I0703 02:29:57.058290 31050 net.cpp:1851] res2a_branch2a_param_0(0.5) 
I0703 02:29:57.058293 31050 net.cpp:1851] res2a_branch2b_param_0(0.5) 
I0703 02:29:57.058297 31050 net.cpp:1851] res3a_branch2a_param_0(0.5) 
I0703 02:29:57.058301 31050 net.cpp:1851] res3a_branch2b_param_0(0.5) 
I0703 02:29:57.058305 31050 net.cpp:1851] res4a_branch2a_param_0(0.5) 
I0703 02:29:57.058310 31050 net.cpp:1851] res4a_branch2b_param_0(0.5) 
I0703 02:29:57.058313 31050 net.cpp:1851] res5a_branch2a_param_0(0.5) 
I0703 02:29:57.058318 31050 net.cpp:1851] res5a_branch2b_param_0(0.5) 
I0703 02:29:57.058322 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.34268e+06/2.69117e+06) 0.499
I0703 02:29:57.058467 31050 solver.cpp:473] Iteration 14000, Testing net (#0)
I0703 02:30:46.914063 31050 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.95353
I0703 02:30:46.914218 31050 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.999878
I0703 02:30:46.914229 31050 solver.cpp:546]     Test net output #2: loss = 0.146663 (* 1 = 0.146663 loss)
I0703 02:30:47.155061 31050 solver.cpp:290] Iteration 14000 (1.35037 iter/s, 74.0538s/100 iter), loss = 0.0329122
I0703 02:30:47.155086 31050 solver.cpp:309]     Train net output #0: loss = 0.0329121 (* 1 = 0.0329121 loss)
I0703 02:30:47.155092 31050 sgd_solver.cpp:106] Iteration 14000, lr = 1e-05
I0703 02:30:47.156056 31050 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.55
I0703 02:30:47.892984 31050 net.cpp:1824] All zero weights of convolution layers are frozen
I0703 02:31:11.381537 31050 solver.cpp:290] Iteration 14100 (4.12783 iter/s, 24.2258s/100 iter), loss = 0.0249478
I0703 02:31:11.381561 31050 solver.cpp:309]     Train net output #0: loss = 0.0249478 (* 1 = 0.0249478 loss)
I0703 02:31:11.381568 31050 sgd_solver.cpp:106] Iteration 14100, lr = 1e-05
I0703 02:31:35.580016 31050 solver.cpp:290] Iteration 14200 (4.13261 iter/s, 24.1978s/100 iter), loss = 0.0196924
I0703 02:31:35.580124 31050 solver.cpp:309]     Train net output #0: loss = 0.0196924 (* 1 = 0.0196924 loss)
I0703 02:31:35.580135 31050 sgd_solver.cpp:106] Iteration 14200, lr = 1e-05
I0703 02:31:59.732265 31050 solver.cpp:290] Iteration 14300 (4.14053 iter/s, 24.1515s/100 iter), loss = 0.0206623
I0703 02:31:59.732291 31050 solver.cpp:309]     Train net output #0: loss = 0.0206623 (* 1 = 0.0206623 loss)
I0703 02:31:59.732300 31050 sgd_solver.cpp:106] Iteration 14300, lr = 1e-05
I0703 02:32:23.918612 31050 solver.cpp:290] Iteration 14400 (4.13468 iter/s, 24.1857s/100 iter), loss = 0.0355147
I0703 02:32:23.918679 31050 solver.cpp:309]     Train net output #0: loss = 0.0355147 (* 1 = 0.0355147 loss)
I0703 02:32:23.918687 31050 sgd_solver.cpp:106] Iteration 14400, lr = 1e-05
I0703 02:32:48.104125 31050 solver.cpp:290] Iteration 14500 (4.13483 iter/s, 24.1848s/100 iter), loss = 0.0300684
I0703 02:32:48.104149 31050 solver.cpp:309]     Train net output #0: loss = 0.0300684 (* 1 = 0.0300684 loss)
I0703 02:32:48.104159 31050 sgd_solver.cpp:106] Iteration 14500, lr = 1e-05
I0703 02:33:12.258070 31050 solver.cpp:290] Iteration 14600 (4.14023 iter/s, 24.1533s/100 iter), loss = 0.0400281
I0703 02:33:12.258184 31050 solver.cpp:309]     Train net output #0: loss = 0.040028 (* 1 = 0.040028 loss)
I0703 02:33:12.258194 31050 sgd_solver.cpp:106] Iteration 14600, lr = 1e-05
I0703 02:33:36.426980 31050 solver.cpp:290] Iteration 14700 (4.13768 iter/s, 24.1681s/100 iter), loss = 0.0428297
I0703 02:33:36.427006 31050 solver.cpp:309]     Train net output #0: loss = 0.0428296 (* 1 = 0.0428296 loss)
I0703 02:33:36.427013 31050 sgd_solver.cpp:106] Iteration 14700, lr = 1e-05
I0703 02:34:00.610229 31050 solver.cpp:290] Iteration 14800 (4.13521 iter/s, 24.1826s/100 iter), loss = 0.0263512
I0703 02:34:00.610339 31050 solver.cpp:309]     Train net output #0: loss = 0.0263511 (* 1 = 0.0263511 loss)
I0703 02:34:00.610349 31050 sgd_solver.cpp:106] Iteration 14800, lr = 1e-05
I0703 02:34:24.791357 31050 solver.cpp:290] Iteration 14900 (4.13559 iter/s, 24.1804s/100 iter), loss = 0.0300247
I0703 02:34:24.791380 31050 solver.cpp:309]     Train net output #0: loss = 0.0300247 (* 1 = 0.0300247 loss)
I0703 02:34:24.791388 31050 sgd_solver.cpp:106] Iteration 14900, lr = 1e-05
I0703 02:34:48.745493 31050 solver.cpp:354] Sparsity after update:
I0703 02:34:48.791347 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 02:34:48.791363 31050 net.cpp:1851] conv1a_param_0(0.275) 
I0703 02:34:48.791370 31050 net.cpp:1851] conv1b_param_0(0.55) 
I0703 02:34:48.791373 31050 net.cpp:1851] ctx_conv1_param_0(0.55) 
I0703 02:34:48.791375 31050 net.cpp:1851] ctx_conv2_param_0(0.55) 
I0703 02:34:48.791376 31050 net.cpp:1851] ctx_conv3_param_0(0.55) 
I0703 02:34:48.791378 31050 net.cpp:1851] ctx_conv4_param_0(0.55) 
I0703 02:34:48.791380 31050 net.cpp:1851] ctx_final_param_0(0.00694) 
I0703 02:34:48.791383 31050 net.cpp:1851] out3a_param_0(0.55) 
I0703 02:34:48.791384 31050 net.cpp:1851] out5a_param_0(0.55) 
I0703 02:34:48.791386 31050 net.cpp:1851] res2a_branch2a_param_0(0.55) 
I0703 02:34:48.791388 31050 net.cpp:1851] res2a_branch2b_param_0(0.55) 
I0703 02:34:48.791391 31050 net.cpp:1851] res3a_branch2a_param_0(0.55) 
I0703 02:34:48.791393 31050 net.cpp:1851] res3a_branch2b_param_0(0.55) 
I0703 02:34:48.791395 31050 net.cpp:1851] res4a_branch2a_param_0(0.55) 
I0703 02:34:48.791396 31050 net.cpp:1851] res4a_branch2b_param_0(0.55) 
I0703 02:34:48.791399 31050 net.cpp:1851] res5a_branch2a_param_0(0.55) 
I0703 02:34:48.791400 31050 net.cpp:1851] res5a_branch2b_param_0(0.55) 
I0703 02:34:48.791402 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.47692e+06/2.69117e+06) 0.549
I0703 02:34:49.022296 31050 solver.cpp:290] Iteration 15000 (4.12707 iter/s, 24.2303s/100 iter), loss = 0.0291666
I0703 02:34:49.022320 31050 solver.cpp:309]     Train net output #0: loss = 0.0291666 (* 1 = 0.0291666 loss)
I0703 02:34:49.022326 31050 sgd_solver.cpp:106] Iteration 15000, lr = 1e-05
I0703 02:34:49.023340 31050 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.6
I0703 02:34:49.838785 31050 net.cpp:1824] All zero weights of convolution layers are frozen
I0703 02:35:14.098301 31050 solver.cpp:290] Iteration 15100 (3.98799 iter/s, 25.0753s/100 iter), loss = 0.020981
I0703 02:35:14.098325 31050 solver.cpp:309]     Train net output #0: loss = 0.020981 (* 1 = 0.020981 loss)
I0703 02:35:14.098332 31050 sgd_solver.cpp:106] Iteration 15100, lr = 1e-05
I0703 02:35:38.599553 31050 solver.cpp:290] Iteration 15200 (4.08154 iter/s, 24.5006s/100 iter), loss = 0.0314504
I0703 02:35:38.600082 31050 solver.cpp:309]     Train net output #0: loss = 0.0314504 (* 1 = 0.0314504 loss)
I0703 02:35:38.600093 31050 sgd_solver.cpp:106] Iteration 15200, lr = 1e-05
I0703 02:36:02.790343 31050 solver.cpp:290] Iteration 15300 (4.13401 iter/s, 24.1896s/100 iter), loss = 0.0267294
I0703 02:36:02.790366 31050 solver.cpp:309]     Train net output #0: loss = 0.0267294 (* 1 = 0.0267294 loss)
I0703 02:36:02.790374 31050 sgd_solver.cpp:106] Iteration 15300, lr = 1e-05
I0703 02:36:26.976207 31050 solver.cpp:290] Iteration 15400 (4.13476 iter/s, 24.1852s/100 iter), loss = 0.0267781
I0703 02:36:26.976281 31050 solver.cpp:309]     Train net output #0: loss = 0.0267781 (* 1 = 0.0267781 loss)
I0703 02:36:26.976290 31050 sgd_solver.cpp:106] Iteration 15400, lr = 1e-05
I0703 02:36:51.139251 31050 solver.cpp:290] Iteration 15500 (4.13867 iter/s, 24.1623s/100 iter), loss = 0.0420833
I0703 02:36:51.139273 31050 solver.cpp:309]     Train net output #0: loss = 0.0420833 (* 1 = 0.0420833 loss)
I0703 02:36:51.139281 31050 sgd_solver.cpp:106] Iteration 15500, lr = 1e-05
I0703 02:37:15.348136 31050 solver.cpp:290] Iteration 15600 (4.13083 iter/s, 24.2082s/100 iter), loss = 0.0235285
I0703 02:37:15.348248 31050 solver.cpp:309]     Train net output #0: loss = 0.0235285 (* 1 = 0.0235285 loss)
I0703 02:37:15.348258 31050 sgd_solver.cpp:106] Iteration 15600, lr = 1e-05
I0703 02:37:39.518260 31050 solver.cpp:290] Iteration 15700 (4.13747 iter/s, 24.1694s/100 iter), loss = 0.036093
I0703 02:37:39.518285 31050 solver.cpp:309]     Train net output #0: loss = 0.036093 (* 1 = 0.036093 loss)
I0703 02:37:39.518295 31050 sgd_solver.cpp:106] Iteration 15700, lr = 1e-05
I0703 02:38:03.695063 31050 solver.cpp:290] Iteration 15800 (4.13631 iter/s, 24.1761s/100 iter), loss = 0.0295454
I0703 02:38:03.695171 31050 solver.cpp:309]     Train net output #0: loss = 0.0295454 (* 1 = 0.0295454 loss)
I0703 02:38:03.695181 31050 sgd_solver.cpp:106] Iteration 15800, lr = 1e-05
I0703 02:38:27.866125 31050 solver.cpp:290] Iteration 15900 (4.13731 iter/s, 24.1703s/100 iter), loss = 0.0255017
I0703 02:38:27.866149 31050 solver.cpp:309]     Train net output #0: loss = 0.0255016 (* 1 = 0.0255016 loss)
I0703 02:38:27.866156 31050 sgd_solver.cpp:106] Iteration 15900, lr = 1e-05
I0703 02:38:51.905207 31050 solver.cpp:354] Sparsity after update:
I0703 02:38:51.907065 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 02:38:51.907073 31050 net.cpp:1851] conv1a_param_0(0.3) 
I0703 02:38:51.907080 31050 net.cpp:1851] conv1b_param_0(0.6) 
I0703 02:38:51.907083 31050 net.cpp:1851] ctx_conv1_param_0(0.6) 
I0703 02:38:51.907085 31050 net.cpp:1851] ctx_conv2_param_0(0.6) 
I0703 02:38:51.907088 31050 net.cpp:1851] ctx_conv3_param_0(0.6) 
I0703 02:38:51.907088 31050 net.cpp:1851] ctx_conv4_param_0(0.6) 
I0703 02:38:51.907090 31050 net.cpp:1851] ctx_final_param_0(0.00109) 
I0703 02:38:51.907093 31050 net.cpp:1851] out3a_param_0(0.6) 
I0703 02:38:51.907094 31050 net.cpp:1851] out5a_param_0(0.6) 
I0703 02:38:51.907096 31050 net.cpp:1851] res2a_branch2a_param_0(0.6) 
I0703 02:38:51.907099 31050 net.cpp:1851] res2a_branch2b_param_0(0.6) 
I0703 02:38:51.907101 31050 net.cpp:1851] res3a_branch2a_param_0(0.6) 
I0703 02:38:51.907104 31050 net.cpp:1851] res3a_branch2b_param_0(0.6) 
I0703 02:38:51.907104 31050 net.cpp:1851] res4a_branch2a_param_0(0.6) 
I0703 02:38:51.907106 31050 net.cpp:1851] res4a_branch2b_param_0(0.6) 
I0703 02:38:51.907109 31050 net.cpp:1851] res5a_branch2a_param_0(0.6) 
I0703 02:38:51.907111 31050 net.cpp:1851] res5a_branch2b_param_0(0.6) 
I0703 02:38:51.907114 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.6112e+06/2.69117e+06) 0.599
I0703 02:38:51.907248 31050 solver.cpp:473] Iteration 16000, Testing net (#0)
I0703 02:39:40.007951 31050 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.952019
I0703 02:39:40.008052 31050 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.99993
I0703 02:39:40.008059 31050 solver.cpp:546]     Test net output #2: loss = 0.144492 (* 1 = 0.144492 loss)
I0703 02:39:40.261543 31050 solver.cpp:290] Iteration 16000 (1.38134 iter/s, 72.3935s/100 iter), loss = 0.0405432
I0703 02:39:40.261566 31050 solver.cpp:309]     Train net output #0: loss = 0.0405432 (* 1 = 0.0405432 loss)
I0703 02:39:40.261574 31050 sgd_solver.cpp:106] Iteration 16000, lr = 1e-05
I0703 02:39:40.262557 31050 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.65
I0703 02:39:41.146764 31050 net.cpp:1824] All zero weights of convolution layers are frozen
I0703 02:40:04.634600 31050 solver.cpp:290] Iteration 16100 (4.10301 iter/s, 24.3724s/100 iter), loss = 0.0284019
I0703 02:40:04.634626 31050 solver.cpp:309]     Train net output #0: loss = 0.0284019 (* 1 = 0.0284019 loss)
I0703 02:40:04.634634 31050 sgd_solver.cpp:106] Iteration 16100, lr = 1e-05
I0703 02:40:28.755959 31050 solver.cpp:290] Iteration 16200 (4.14582 iter/s, 24.1207s/100 iter), loss = 0.0298297
I0703 02:40:28.756089 31050 solver.cpp:309]     Train net output #0: loss = 0.0298297 (* 1 = 0.0298297 loss)
I0703 02:40:28.756099 31050 sgd_solver.cpp:106] Iteration 16200, lr = 1e-05
I0703 02:40:52.947152 31050 solver.cpp:290] Iteration 16300 (4.13387 iter/s, 24.1904s/100 iter), loss = 0.0281382
I0703 02:40:52.947176 31050 solver.cpp:309]     Train net output #0: loss = 0.0281382 (* 1 = 0.0281382 loss)
I0703 02:40:52.947183 31050 sgd_solver.cpp:106] Iteration 16300, lr = 1e-05
I0703 02:41:17.140566 31050 solver.cpp:290] Iteration 16400 (4.13347 iter/s, 24.1927s/100 iter), loss = 0.0383234
I0703 02:41:17.140678 31050 solver.cpp:309]     Train net output #0: loss = 0.0383234 (* 1 = 0.0383234 loss)
I0703 02:41:17.140688 31050 sgd_solver.cpp:106] Iteration 16400, lr = 1e-05
I0703 02:41:41.323052 31050 solver.cpp:290] Iteration 16500 (4.13535 iter/s, 24.1817s/100 iter), loss = 0.017105
I0703 02:41:41.323077 31050 solver.cpp:309]     Train net output #0: loss = 0.017105 (* 1 = 0.017105 loss)
I0703 02:41:41.323084 31050 sgd_solver.cpp:106] Iteration 16500, lr = 1e-05
I0703 02:42:05.444658 31050 solver.cpp:290] Iteration 16600 (4.14578 iter/s, 24.1209s/100 iter), loss = 0.0304796
I0703 02:42:05.444764 31050 solver.cpp:309]     Train net output #0: loss = 0.0304796 (* 1 = 0.0304796 loss)
I0703 02:42:05.444777 31050 sgd_solver.cpp:106] Iteration 16600, lr = 1e-05
I0703 02:42:29.856042 31050 solver.cpp:290] Iteration 16700 (4.09658 iter/s, 24.4106s/100 iter), loss = 0.0474211
I0703 02:42:29.856094 31050 solver.cpp:309]     Train net output #0: loss = 0.0474211 (* 1 = 0.0474211 loss)
I0703 02:42:29.856109 31050 sgd_solver.cpp:106] Iteration 16700, lr = 1e-05
I0703 02:42:54.567157 31050 solver.cpp:290] Iteration 16800 (4.04688 iter/s, 24.7104s/100 iter), loss = 0.0270917
I0703 02:42:54.567268 31050 solver.cpp:309]     Train net output #0: loss = 0.0270917 (* 1 = 0.0270917 loss)
I0703 02:42:54.567278 31050 sgd_solver.cpp:106] Iteration 16800, lr = 1e-05
I0703 02:43:18.737538 31050 solver.cpp:290] Iteration 16900 (4.13742 iter/s, 24.1696s/100 iter), loss = 0.0314104
I0703 02:43:18.737561 31050 solver.cpp:309]     Train net output #0: loss = 0.0314104 (* 1 = 0.0314104 loss)
I0703 02:43:18.737568 31050 sgd_solver.cpp:106] Iteration 16900, lr = 1e-05
I0703 02:43:42.657943 31050 solver.cpp:354] Sparsity after update:
I0703 02:43:42.664575 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 02:43:42.664608 31050 net.cpp:1851] conv1a_param_0(0.325) 
I0703 02:43:42.664626 31050 net.cpp:1851] conv1b_param_0(0.65) 
I0703 02:43:42.664628 31050 net.cpp:1851] ctx_conv1_param_0(0.65) 
I0703 02:43:42.664630 31050 net.cpp:1851] ctx_conv2_param_0(0.65) 
I0703 02:43:42.664633 31050 net.cpp:1851] ctx_conv3_param_0(0.65) 
I0703 02:43:42.664634 31050 net.cpp:1851] ctx_conv4_param_0(0.65) 
I0703 02:43:42.664636 31050 net.cpp:1851] ctx_final_param_0(0) 
I0703 02:43:42.664638 31050 net.cpp:1851] out3a_param_0(0.65) 
I0703 02:43:42.664640 31050 net.cpp:1851] out5a_param_0(0.65) 
I0703 02:43:42.664643 31050 net.cpp:1851] res2a_branch2a_param_0(0.65) 
I0703 02:43:42.664644 31050 net.cpp:1851] res2a_branch2b_param_0(0.65) 
I0703 02:43:42.664646 31050 net.cpp:1851] res3a_branch2a_param_0(0.65) 
I0703 02:43:42.664649 31050 net.cpp:1851] res3a_branch2b_param_0(0.65) 
I0703 02:43:42.664650 31050 net.cpp:1851] res4a_branch2a_param_0(0.65) 
I0703 02:43:42.664651 31050 net.cpp:1851] res4a_branch2b_param_0(0.65) 
I0703 02:43:42.664654 31050 net.cpp:1851] res5a_branch2a_param_0(0.65) 
I0703 02:43:42.664655 31050 net.cpp:1851] res5a_branch2b_param_0(0.65) 
I0703 02:43:42.664657 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.74543e+06/2.69117e+06) 0.649
I0703 02:43:42.914911 31050 solver.cpp:290] Iteration 17000 (4.13621 iter/s, 24.1767s/100 iter), loss = 0.0254071
I0703 02:43:42.914934 31050 solver.cpp:309]     Train net output #0: loss = 0.0254071 (* 1 = 0.0254071 loss)
I0703 02:43:42.914942 31050 sgd_solver.cpp:106] Iteration 17000, lr = 1e-05
I0703 02:43:42.915923 31050 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.7
I0703 02:43:43.920914 31050 net.cpp:1824] All zero weights of convolution layers are frozen
I0703 02:44:08.054103 31050 solver.cpp:290] Iteration 17100 (3.97796 iter/s, 25.1385s/100 iter), loss = 0.0268872
I0703 02:44:08.054126 31050 solver.cpp:309]     Train net output #0: loss = 0.0268872 (* 1 = 0.0268872 loss)
I0703 02:44:08.054133 31050 sgd_solver.cpp:106] Iteration 17100, lr = 1e-05
I0703 02:44:32.246744 31050 solver.cpp:290] Iteration 17200 (4.1336 iter/s, 24.192s/100 iter), loss = 0.0592871
I0703 02:44:32.246821 31050 solver.cpp:309]     Train net output #0: loss = 0.0592871 (* 1 = 0.0592871 loss)
I0703 02:44:32.246830 31050 sgd_solver.cpp:106] Iteration 17200, lr = 1e-05
I0703 02:44:56.421237 31050 solver.cpp:290] Iteration 17300 (4.13671 iter/s, 24.1738s/100 iter), loss = 0.0399308
I0703 02:44:56.421262 31050 solver.cpp:309]     Train net output #0: loss = 0.0399307 (* 1 = 0.0399307 loss)
I0703 02:44:56.421268 31050 sgd_solver.cpp:106] Iteration 17300, lr = 1e-05
I0703 02:45:20.583539 31050 solver.cpp:290] Iteration 17400 (4.13879 iter/s, 24.1616s/100 iter), loss = 0.0461443
I0703 02:45:20.583649 31050 solver.cpp:309]     Train net output #0: loss = 0.0461443 (* 1 = 0.0461443 loss)
I0703 02:45:20.583659 31050 sgd_solver.cpp:106] Iteration 17400, lr = 1e-05
I0703 02:45:44.759505 31050 solver.cpp:290] Iteration 17500 (4.13647 iter/s, 24.1752s/100 iter), loss = 0.0331534
I0703 02:45:44.759527 31050 solver.cpp:309]     Train net output #0: loss = 0.0331534 (* 1 = 0.0331534 loss)
I0703 02:45:44.759536 31050 sgd_solver.cpp:106] Iteration 17500, lr = 1e-05
I0703 02:46:09.281416 31050 solver.cpp:290] Iteration 17600 (4.0781 iter/s, 24.5212s/100 iter), loss = 0.0531705
I0703 02:46:09.281529 31050 solver.cpp:309]     Train net output #0: loss = 0.0531705 (* 1 = 0.0531705 loss)
I0703 02:46:09.281543 31050 sgd_solver.cpp:106] Iteration 17600, lr = 1e-05
I0703 02:46:33.956682 31050 solver.cpp:290] Iteration 17700 (4.05277 iter/s, 24.6745s/100 iter), loss = 0.0343455
I0703 02:46:33.956707 31050 solver.cpp:309]     Train net output #0: loss = 0.0343454 (* 1 = 0.0343454 loss)
I0703 02:46:33.956713 31050 sgd_solver.cpp:106] Iteration 17700, lr = 1e-05
I0703 02:46:58.142078 31050 solver.cpp:290] Iteration 17800 (4.13484 iter/s, 24.1847s/100 iter), loss = 0.0347701
I0703 02:46:58.142110 31050 solver.cpp:309]     Train net output #0: loss = 0.0347701 (* 1 = 0.0347701 loss)
I0703 02:46:58.142117 31050 sgd_solver.cpp:106] Iteration 17800, lr = 1e-05
I0703 02:47:22.283491 31050 solver.cpp:290] Iteration 17900 (4.14238 iter/s, 24.1407s/100 iter), loss = 0.0465073
I0703 02:47:22.283516 31050 solver.cpp:309]     Train net output #0: loss = 0.0465073 (* 1 = 0.0465073 loss)
I0703 02:47:22.283524 31050 sgd_solver.cpp:106] Iteration 17900, lr = 1e-05
I0703 02:47:46.221604 31050 solver.cpp:354] Sparsity after update:
I0703 02:47:46.223436 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 02:47:46.223444 31050 net.cpp:1851] conv1a_param_0(0.35) 
I0703 02:47:46.223451 31050 net.cpp:1851] conv1b_param_0(0.7) 
I0703 02:47:46.223453 31050 net.cpp:1851] ctx_conv1_param_0(0.7) 
I0703 02:47:46.223455 31050 net.cpp:1851] ctx_conv2_param_0(0.7) 
I0703 02:47:46.223457 31050 net.cpp:1851] ctx_conv3_param_0(0.7) 
I0703 02:47:46.223459 31050 net.cpp:1851] ctx_conv4_param_0(0.7) 
I0703 02:47:46.223461 31050 net.cpp:1851] ctx_final_param_0(0.000217) 
I0703 02:47:46.223464 31050 net.cpp:1851] out3a_param_0(0.7) 
I0703 02:47:46.223465 31050 net.cpp:1851] out5a_param_0(0.7) 
I0703 02:47:46.223467 31050 net.cpp:1851] res2a_branch2a_param_0(0.7) 
I0703 02:47:46.223469 31050 net.cpp:1851] res2a_branch2b_param_0(0.7) 
I0703 02:47:46.223471 31050 net.cpp:1851] res3a_branch2a_param_0(0.7) 
I0703 02:47:46.223474 31050 net.cpp:1851] res3a_branch2b_param_0(0.7) 
I0703 02:47:46.223474 31050 net.cpp:1851] res4a_branch2a_param_0(0.7) 
I0703 02:47:46.223476 31050 net.cpp:1851] res4a_branch2b_param_0(0.7) 
I0703 02:47:46.223479 31050 net.cpp:1851] res5a_branch2a_param_0(0.7) 
I0703 02:47:46.223480 31050 net.cpp:1851] res5a_branch2b_param_0(0.7) 
I0703 02:47:46.223482 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.87972e+06/2.69117e+06) 0.698
I0703 02:47:46.223618 31050 solver.cpp:473] Iteration 18000, Testing net (#0)
I0703 02:48:33.917950 31050 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9489
I0703 02:48:33.918004 31050 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.999852
I0703 02:48:33.918010 31050 solver.cpp:546]     Test net output #2: loss = 0.14286 (* 1 = 0.14286 loss)
I0703 02:48:34.198074 31050 solver.cpp:290] Iteration 18000 (1.39058 iter/s, 71.9126s/100 iter), loss = 0.0315998
I0703 02:48:34.198102 31050 solver.cpp:309]     Train net output #0: loss = 0.0315998 (* 1 = 0.0315998 loss)
I0703 02:48:34.198110 31050 sgd_solver.cpp:106] Iteration 18000, lr = 1e-05
I0703 02:48:34.199120 31050 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.75
I0703 02:48:35.321730 31050 net.cpp:1824] All zero weights of convolution layers are frozen
I0703 02:48:58.816195 31050 solver.cpp:290] Iteration 18100 (4.06216 iter/s, 24.6174s/100 iter), loss = 0.0873555
I0703 02:48:58.816220 31050 solver.cpp:309]     Train net output #0: loss = 0.0873555 (* 1 = 0.0873555 loss)
I0703 02:48:58.816227 31050 sgd_solver.cpp:106] Iteration 18100, lr = 1e-05
I0703 02:49:23.049517 31050 solver.cpp:290] Iteration 18200 (4.12667 iter/s, 24.2326s/100 iter), loss = 0.0468204
I0703 02:49:23.049610 31050 solver.cpp:309]     Train net output #0: loss = 0.0468204 (* 1 = 0.0468204 loss)
I0703 02:49:23.049630 31050 sgd_solver.cpp:106] Iteration 18200, lr = 1e-05
I0703 02:49:47.553936 31050 solver.cpp:290] Iteration 18300 (4.08102 iter/s, 24.5037s/100 iter), loss = 0.0574592
I0703 02:49:47.553961 31050 solver.cpp:309]     Train net output #0: loss = 0.0574591 (* 1 = 0.0574591 loss)
I0703 02:49:47.553967 31050 sgd_solver.cpp:106] Iteration 18300, lr = 1e-05
I0703 02:50:12.206954 31050 solver.cpp:290] Iteration 18400 (4.05641 iter/s, 24.6523s/100 iter), loss = 0.0398709
I0703 02:50:12.207036 31050 solver.cpp:309]     Train net output #0: loss = 0.0398708 (* 1 = 0.0398708 loss)
I0703 02:50:12.207056 31050 sgd_solver.cpp:106] Iteration 18400, lr = 1e-05
I0703 02:50:36.797056 31050 solver.cpp:290] Iteration 18500 (4.0668 iter/s, 24.5894s/100 iter), loss = 0.034691
I0703 02:50:36.797080 31050 solver.cpp:309]     Train net output #0: loss = 0.0346909 (* 1 = 0.0346909 loss)
I0703 02:50:36.797086 31050 sgd_solver.cpp:106] Iteration 18500, lr = 1e-05
I0703 02:51:01.567848 31050 solver.cpp:290] Iteration 18600 (4.03713 iter/s, 24.7701s/100 iter), loss = 0.0543521
I0703 02:51:01.567899 31050 solver.cpp:309]     Train net output #0: loss = 0.0543521 (* 1 = 0.0543521 loss)
I0703 02:51:01.567908 31050 sgd_solver.cpp:106] Iteration 18600, lr = 1e-05
I0703 02:51:26.330118 31050 solver.cpp:290] Iteration 18700 (4.03852 iter/s, 24.7615s/100 iter), loss = 0.0418341
I0703 02:51:26.330152 31050 solver.cpp:309]     Train net output #0: loss = 0.041834 (* 1 = 0.041834 loss)
I0703 02:51:26.330160 31050 sgd_solver.cpp:106] Iteration 18700, lr = 1e-05
I0703 02:51:50.957901 31050 solver.cpp:290] Iteration 18800 (4.06057 iter/s, 24.6271s/100 iter), loss = 0.0536919
I0703 02:51:50.957968 31050 solver.cpp:309]     Train net output #0: loss = 0.0536919 (* 1 = 0.0536919 loss)
I0703 02:51:50.957979 31050 sgd_solver.cpp:106] Iteration 18800, lr = 1e-05
I0703 02:52:15.701690 31050 solver.cpp:290] Iteration 18900 (4.04154 iter/s, 24.743s/100 iter), loss = 0.0301538
I0703 02:52:15.701738 31050 solver.cpp:309]     Train net output #0: loss = 0.0301538 (* 1 = 0.0301538 loss)
I0703 02:52:15.701755 31050 sgd_solver.cpp:106] Iteration 18900, lr = 1e-05
I0703 02:52:40.216657 31050 solver.cpp:354] Sparsity after update:
I0703 02:52:40.260915 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 02:52:40.260932 31050 net.cpp:1851] conv1a_param_0(0.375) 
I0703 02:52:40.260939 31050 net.cpp:1851] conv1b_param_0(0.75) 
I0703 02:52:40.260942 31050 net.cpp:1851] ctx_conv1_param_0(0.75) 
I0703 02:52:40.260944 31050 net.cpp:1851] ctx_conv2_param_0(0.75) 
I0703 02:52:40.260946 31050 net.cpp:1851] ctx_conv3_param_0(0.75) 
I0703 02:52:40.260948 31050 net.cpp:1851] ctx_conv4_param_0(0.75) 
I0703 02:52:40.260951 31050 net.cpp:1851] ctx_final_param_0(0.00977) 
I0703 02:52:40.260952 31050 net.cpp:1851] out3a_param_0(0.75) 
I0703 02:52:40.260954 31050 net.cpp:1851] out5a_param_0(0.75) 
I0703 02:52:40.260956 31050 net.cpp:1851] res2a_branch2a_param_0(0.75) 
I0703 02:52:40.260958 31050 net.cpp:1851] res2a_branch2b_param_0(0.75) 
I0703 02:52:40.260960 31050 net.cpp:1851] res3a_branch2a_param_0(0.75) 
I0703 02:52:40.260962 31050 net.cpp:1851] res3a_branch2b_param_0(0.75) 
I0703 02:52:40.260964 31050 net.cpp:1851] res4a_branch2a_param_0(0.75) 
I0703 02:52:40.260967 31050 net.cpp:1851] res4a_branch2b_param_0(0.75) 
I0703 02:52:40.260968 31050 net.cpp:1851] res5a_branch2a_param_0(0.75) 
I0703 02:52:40.260970 31050 net.cpp:1851] res5a_branch2b_param_0(0.75) 
I0703 02:52:40.260972 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.01404e+06/2.69117e+06) 0.748
I0703 02:52:40.490803 31050 solver.cpp:290] Iteration 19000 (4.03415 iter/s, 24.7884s/100 iter), loss = 0.0562804
I0703 02:52:40.490826 31050 solver.cpp:309]     Train net output #0: loss = 0.0562804 (* 1 = 0.0562804 loss)
I0703 02:52:40.490833 31050 sgd_solver.cpp:106] Iteration 19000, lr = 1e-05
I0703 02:53:05.113171 31050 solver.cpp:290] Iteration 19100 (4.06146 iter/s, 24.6217s/100 iter), loss = 0.043082
I0703 02:53:05.113198 31050 solver.cpp:309]     Train net output #0: loss = 0.043082 (* 1 = 0.043082 loss)
I0703 02:53:05.113206 31050 sgd_solver.cpp:106] Iteration 19100, lr = 1e-05
I0703 02:53:29.763869 31050 solver.cpp:290] Iteration 19200 (4.0568 iter/s, 24.65s/100 iter), loss = 0.039303
I0703 02:53:29.764020 31050 solver.cpp:309]     Train net output #0: loss = 0.039303 (* 1 = 0.039303 loss)
I0703 02:53:29.764044 31050 sgd_solver.cpp:106] Iteration 19200, lr = 1e-05
I0703 02:53:54.463573 31050 solver.cpp:290] Iteration 19300 (4.04876 iter/s, 24.6989s/100 iter), loss = 0.0388101
I0703 02:53:54.463616 31050 solver.cpp:309]     Train net output #0: loss = 0.0388101 (* 1 = 0.0388101 loss)
I0703 02:53:54.463631 31050 sgd_solver.cpp:106] Iteration 19300, lr = 1e-05
I0703 02:54:19.055598 31050 solver.cpp:290] Iteration 19400 (4.06648 iter/s, 24.5913s/100 iter), loss = 0.0489562
I0703 02:54:19.055650 31050 solver.cpp:309]     Train net output #0: loss = 0.0489562 (* 1 = 0.0489562 loss)
I0703 02:54:19.055658 31050 sgd_solver.cpp:106] Iteration 19400, lr = 1e-05
I0703 02:54:43.209043 31050 solver.cpp:290] Iteration 19500 (4.14032 iter/s, 24.1527s/100 iter), loss = 0.0328271
I0703 02:54:43.209069 31050 solver.cpp:309]     Train net output #0: loss = 0.032827 (* 1 = 0.032827 loss)
I0703 02:54:43.209075 31050 sgd_solver.cpp:106] Iteration 19500, lr = 1e-05
I0703 02:55:07.347430 31050 solver.cpp:290] Iteration 19600 (4.14289 iter/s, 24.1377s/100 iter), loss = 0.0351995
I0703 02:55:07.347534 31050 solver.cpp:309]     Train net output #0: loss = 0.0351995 (* 1 = 0.0351995 loss)
I0703 02:55:07.347544 31050 sgd_solver.cpp:106] Iteration 19600, lr = 1e-05
I0703 02:55:31.513504 31050 solver.cpp:290] Iteration 19700 (4.13816 iter/s, 24.1653s/100 iter), loss = 0.0530345
I0703 02:55:31.513528 31050 solver.cpp:309]     Train net output #0: loss = 0.0530345 (* 1 = 0.0530345 loss)
I0703 02:55:31.513538 31050 sgd_solver.cpp:106] Iteration 19700, lr = 1e-05
I0703 02:55:55.688936 31050 solver.cpp:290] Iteration 19800 (4.13655 iter/s, 24.1748s/100 iter), loss = 0.0353925
I0703 02:55:55.689070 31050 solver.cpp:309]     Train net output #0: loss = 0.0353925 (* 1 = 0.0353925 loss)
I0703 02:55:55.689080 31050 sgd_solver.cpp:106] Iteration 19800, lr = 1e-05
I0703 02:56:19.848155 31050 solver.cpp:290] Iteration 19900 (4.13934 iter/s, 24.1584s/100 iter), loss = 0.0244907
I0703 02:56:19.848181 31050 solver.cpp:309]     Train net output #0: loss = 0.0244906 (* 1 = 0.0244906 loss)
I0703 02:56:19.848189 31050 sgd_solver.cpp:106] Iteration 19900, lr = 1e-05
I0703 02:56:43.761096 31050 solver.cpp:600] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-02_23-02-42/sparse/cityscapes5_jsegnet21v2_iter_20000.caffemodel
I0703 02:56:43.786293 31050 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-02_23-02-42/sparse/cityscapes5_jsegnet21v2_iter_20000.solverstate
I0703 02:56:43.802760 31050 solver.cpp:354] Sparsity after update:
I0703 02:56:43.803973 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 02:56:43.803982 31050 net.cpp:1851] conv1a_param_0(0.375) 
I0703 02:56:43.803992 31050 net.cpp:1851] conv1b_param_0(0.75) 
I0703 02:56:43.803997 31050 net.cpp:1851] ctx_conv1_param_0(0.75) 
I0703 02:56:43.804002 31050 net.cpp:1851] ctx_conv2_param_0(0.75) 
I0703 02:56:43.804005 31050 net.cpp:1851] ctx_conv3_param_0(0.75) 
I0703 02:56:43.804009 31050 net.cpp:1851] ctx_conv4_param_0(0.75) 
I0703 02:56:43.804013 31050 net.cpp:1851] ctx_final_param_0(0.00977) 
I0703 02:56:43.804018 31050 net.cpp:1851] out3a_param_0(0.75) 
I0703 02:56:43.804023 31050 net.cpp:1851] out5a_param_0(0.75) 
I0703 02:56:43.804025 31050 net.cpp:1851] res2a_branch2a_param_0(0.75) 
I0703 02:56:43.804029 31050 net.cpp:1851] res2a_branch2b_param_0(0.75) 
I0703 02:56:43.804033 31050 net.cpp:1851] res3a_branch2a_param_0(0.75) 
I0703 02:56:43.804038 31050 net.cpp:1851] res3a_branch2b_param_0(0.75) 
I0703 02:56:43.804041 31050 net.cpp:1851] res4a_branch2a_param_0(0.75) 
I0703 02:56:43.804046 31050 net.cpp:1851] res4a_branch2b_param_0(0.75) 
I0703 02:56:43.804050 31050 net.cpp:1851] res5a_branch2a_param_0(0.75) 
I0703 02:56:43.804054 31050 net.cpp:1851] res5a_branch2b_param_0(0.75) 
I0703 02:56:43.804059 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.01404e+06/2.69117e+06) 0.748
I0703 02:56:43.804211 31050 solver.cpp:473] Iteration 20000, Testing net (#0)
I0703 02:57:31.249930 31050 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.948467
I0703 02:57:31.250025 31050 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.999931
I0703 02:57:31.250032 31050 solver.cpp:546]     Test net output #2: loss = 0.126963 (* 1 = 0.126963 loss)
I0703 02:57:31.497972 31050 solver.cpp:290] Iteration 20000 (1.39571 iter/s, 71.6479s/100 iter), loss = 0.0165047
I0703 02:57:31.497995 31050 solver.cpp:309]     Train net output #0: loss = 0.0165046 (* 1 = 0.0165046 loss)
I0703 02:57:31.498003 31050 sgd_solver.cpp:106] Iteration 20000, lr = 1e-05
I0703 02:57:55.024128 31050 solver.cpp:290] Iteration 20100 (4.25071 iter/s, 23.5255s/100 iter), loss = 0.0452961
I0703 02:57:55.024152 31050 solver.cpp:309]     Train net output #0: loss = 0.045296 (* 1 = 0.045296 loss)
I0703 02:57:55.024159 31050 sgd_solver.cpp:106] Iteration 20100, lr = 1e-05
I0703 02:58:19.173615 31050 solver.cpp:290] Iteration 20200 (4.14099 iter/s, 24.1488s/100 iter), loss = 0.0228615
I0703 02:58:19.173728 31050 solver.cpp:309]     Train net output #0: loss = 0.0228615 (* 1 = 0.0228615 loss)
I0703 02:58:19.173738 31050 sgd_solver.cpp:106] Iteration 20200, lr = 1e-05
I0703 02:58:43.357830 31050 solver.cpp:290] Iteration 20300 (4.13506 iter/s, 24.1834s/100 iter), loss = 0.0144135
I0703 02:58:43.357856 31050 solver.cpp:309]     Train net output #0: loss = 0.0144134 (* 1 = 0.0144134 loss)
I0703 02:58:43.357862 31050 sgd_solver.cpp:106] Iteration 20300, lr = 1e-05
I0703 02:59:07.542026 31050 solver.cpp:290] Iteration 20400 (4.13505 iter/s, 24.1835s/100 iter), loss = 0.0410269
I0703 02:59:07.542153 31050 solver.cpp:309]     Train net output #0: loss = 0.0410269 (* 1 = 0.0410269 loss)
I0703 02:59:07.542165 31050 sgd_solver.cpp:106] Iteration 20400, lr = 1e-05
I0703 02:59:31.695586 31050 solver.cpp:290] Iteration 20500 (4.14031 iter/s, 24.1528s/100 iter), loss = 0.0543041
I0703 02:59:31.695610 31050 solver.cpp:309]     Train net output #0: loss = 0.054304 (* 1 = 0.054304 loss)
I0703 02:59:31.695616 31050 sgd_solver.cpp:106] Iteration 20500, lr = 1e-05
I0703 02:59:55.878149 31050 solver.cpp:290] Iteration 20600 (4.13533 iter/s, 24.1819s/100 iter), loss = 0.043004
I0703 02:59:55.878197 31050 solver.cpp:309]     Train net output #0: loss = 0.0430039 (* 1 = 0.0430039 loss)
I0703 02:59:55.878206 31050 sgd_solver.cpp:106] Iteration 20600, lr = 1e-05
I0703 03:00:20.048941 31050 solver.cpp:290] Iteration 20700 (4.13734 iter/s, 24.1701s/100 iter), loss = 0.0338639
I0703 03:00:20.048966 31050 solver.cpp:309]     Train net output #0: loss = 0.0338639 (* 1 = 0.0338639 loss)
I0703 03:00:20.048974 31050 sgd_solver.cpp:106] Iteration 20700, lr = 1e-05
I0703 03:00:44.215281 31050 solver.cpp:290] Iteration 20800 (4.1381 iter/s, 24.1657s/100 iter), loss = 0.0347515
I0703 03:00:44.215389 31050 solver.cpp:309]     Train net output #0: loss = 0.0347514 (* 1 = 0.0347514 loss)
I0703 03:00:44.215399 31050 sgd_solver.cpp:106] Iteration 20800, lr = 1e-05
I0703 03:01:08.489589 31050 solver.cpp:290] Iteration 20900 (4.11971 iter/s, 24.2735s/100 iter), loss = 0.0370786
I0703 03:01:08.489614 31050 solver.cpp:309]     Train net output #0: loss = 0.0370786 (* 1 = 0.0370786 loss)
I0703 03:01:08.489620 31050 sgd_solver.cpp:106] Iteration 20900, lr = 1e-05
I0703 03:01:32.464588 31050 solver.cpp:354] Sparsity after update:
I0703 03:01:32.479277 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 03:01:32.479307 31050 net.cpp:1851] conv1a_param_0(0.375) 
I0703 03:01:32.479324 31050 net.cpp:1851] conv1b_param_0(0.75) 
I0703 03:01:32.479327 31050 net.cpp:1851] ctx_conv1_param_0(0.75) 
I0703 03:01:32.479331 31050 net.cpp:1851] ctx_conv2_param_0(0.75) 
I0703 03:01:32.479333 31050 net.cpp:1851] ctx_conv3_param_0(0.75) 
I0703 03:01:32.479336 31050 net.cpp:1851] ctx_conv4_param_0(0.75) 
I0703 03:01:32.479339 31050 net.cpp:1851] ctx_final_param_0(0.00977) 
I0703 03:01:32.479342 31050 net.cpp:1851] out3a_param_0(0.75) 
I0703 03:01:32.479349 31050 net.cpp:1851] out5a_param_0(0.75) 
I0703 03:01:32.479352 31050 net.cpp:1851] res2a_branch2a_param_0(0.75) 
I0703 03:01:32.479356 31050 net.cpp:1851] res2a_branch2b_param_0(0.75) 
I0703 03:01:32.479359 31050 net.cpp:1851] res3a_branch2a_param_0(0.75) 
I0703 03:01:32.479362 31050 net.cpp:1851] res3a_branch2b_param_0(0.75) 
I0703 03:01:32.479365 31050 net.cpp:1851] res4a_branch2a_param_0(0.75) 
I0703 03:01:32.479368 31050 net.cpp:1851] res4a_branch2b_param_0(0.75) 
I0703 03:01:32.479372 31050 net.cpp:1851] res5a_branch2a_param_0(0.75) 
I0703 03:01:32.479374 31050 net.cpp:1851] res5a_branch2b_param_0(0.75) 
I0703 03:01:32.479378 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.01404e+06/2.69117e+06) 0.748
I0703 03:01:32.710271 31050 solver.cpp:290] Iteration 21000 (4.12882 iter/s, 24.22s/100 iter), loss = 0.0307623
I0703 03:01:32.710299 31050 solver.cpp:309]     Train net output #0: loss = 0.0307622 (* 1 = 0.0307622 loss)
I0703 03:01:32.710309 31050 sgd_solver.cpp:106] Iteration 21000, lr = 1e-05
I0703 03:01:56.875646 31050 solver.cpp:290] Iteration 21100 (4.13827 iter/s, 24.1647s/100 iter), loss = 0.0303104
I0703 03:01:56.875670 31050 solver.cpp:309]     Train net output #0: loss = 0.0303103 (* 1 = 0.0303103 loss)
I0703 03:01:56.875677 31050 sgd_solver.cpp:106] Iteration 21100, lr = 1e-05
I0703 03:02:21.030570 31050 solver.cpp:290] Iteration 21200 (4.14006 iter/s, 24.1543s/100 iter), loss = 0.0303785
I0703 03:02:21.030607 31050 solver.cpp:309]     Train net output #0: loss = 0.0303784 (* 1 = 0.0303784 loss)
I0703 03:02:21.030616 31050 sgd_solver.cpp:106] Iteration 21200, lr = 1e-05
I0703 03:02:45.202353 31050 solver.cpp:290] Iteration 21300 (4.13717 iter/s, 24.1711s/100 iter), loss = 0.0540421
I0703 03:02:45.202376 31050 solver.cpp:309]     Train net output #0: loss = 0.0540421 (* 1 = 0.0540421 loss)
I0703 03:02:45.202389 31050 sgd_solver.cpp:106] Iteration 21300, lr = 1e-05
I0703 03:03:09.355053 31050 solver.cpp:290] Iteration 21400 (4.14044 iter/s, 24.152s/100 iter), loss = 0.0288539
I0703 03:03:09.355166 31050 solver.cpp:309]     Train net output #0: loss = 0.0288539 (* 1 = 0.0288539 loss)
I0703 03:03:09.355175 31050 sgd_solver.cpp:106] Iteration 21400, lr = 1e-05
I0703 03:03:33.503053 31050 solver.cpp:290] Iteration 21500 (4.14126 iter/s, 24.1472s/100 iter), loss = 0.0497019
I0703 03:03:33.503079 31050 solver.cpp:309]     Train net output #0: loss = 0.0497018 (* 1 = 0.0497018 loss)
I0703 03:03:33.503087 31050 sgd_solver.cpp:106] Iteration 21500, lr = 1e-05
I0703 03:03:57.658751 31050 solver.cpp:290] Iteration 21600 (4.13992 iter/s, 24.155s/100 iter), loss = 0.0396138
I0703 03:03:57.658859 31050 solver.cpp:309]     Train net output #0: loss = 0.0396137 (* 1 = 0.0396137 loss)
I0703 03:03:57.658869 31050 sgd_solver.cpp:106] Iteration 21600, lr = 1e-05
I0703 03:04:21.848273 31050 solver.cpp:290] Iteration 21700 (4.13415 iter/s, 24.1888s/100 iter), loss = 0.0507819
I0703 03:04:21.848296 31050 solver.cpp:309]     Train net output #0: loss = 0.0507818 (* 1 = 0.0507818 loss)
I0703 03:04:21.848304 31050 sgd_solver.cpp:106] Iteration 21700, lr = 1e-05
I0703 03:04:46.034953 31050 solver.cpp:290] Iteration 21800 (4.13462 iter/s, 24.186s/100 iter), loss = 0.0599834
I0703 03:04:46.034994 31050 solver.cpp:309]     Train net output #0: loss = 0.0599833 (* 1 = 0.0599833 loss)
I0703 03:04:46.035006 31050 sgd_solver.cpp:106] Iteration 21800, lr = 1e-05
I0703 03:05:10.233281 31050 solver.cpp:290] Iteration 21900 (4.13263 iter/s, 24.1976s/100 iter), loss = 0.0238899
I0703 03:05:10.233306 31050 solver.cpp:309]     Train net output #0: loss = 0.0238898 (* 1 = 0.0238898 loss)
I0703 03:05:10.233314 31050 sgd_solver.cpp:106] Iteration 21900, lr = 1e-05
I0703 03:05:34.143856 31050 solver.cpp:354] Sparsity after update:
I0703 03:05:34.145685 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 03:05:34.145694 31050 net.cpp:1851] conv1a_param_0(0.375) 
I0703 03:05:34.145704 31050 net.cpp:1851] conv1b_param_0(0.75) 
I0703 03:05:34.145709 31050 net.cpp:1851] ctx_conv1_param_0(0.75) 
I0703 03:05:34.145711 31050 net.cpp:1851] ctx_conv2_param_0(0.75) 
I0703 03:05:34.145714 31050 net.cpp:1851] ctx_conv3_param_0(0.75) 
I0703 03:05:34.145714 31050 net.cpp:1851] ctx_conv4_param_0(0.75) 
I0703 03:05:34.145716 31050 net.cpp:1851] ctx_final_param_0(0.00977) 
I0703 03:05:34.145719 31050 net.cpp:1851] out3a_param_0(0.75) 
I0703 03:05:34.145720 31050 net.cpp:1851] out5a_param_0(0.75) 
I0703 03:05:34.145722 31050 net.cpp:1851] res2a_branch2a_param_0(0.75) 
I0703 03:05:34.145725 31050 net.cpp:1851] res2a_branch2b_param_0(0.75) 
I0703 03:05:34.145726 31050 net.cpp:1851] res3a_branch2a_param_0(0.75) 
I0703 03:05:34.145728 31050 net.cpp:1851] res3a_branch2b_param_0(0.75) 
I0703 03:05:34.145730 31050 net.cpp:1851] res4a_branch2a_param_0(0.75) 
I0703 03:05:34.145732 31050 net.cpp:1851] res4a_branch2b_param_0(0.75) 
I0703 03:05:34.145735 31050 net.cpp:1851] res5a_branch2a_param_0(0.75) 
I0703 03:05:34.145736 31050 net.cpp:1851] res5a_branch2b_param_0(0.75) 
I0703 03:05:34.145738 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.01404e+06/2.69117e+06) 0.748
I0703 03:05:34.145884 31050 solver.cpp:473] Iteration 22000, Testing net (#0)
I0703 03:06:23.633872 31050 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.949011
I0703 03:06:23.633961 31050 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.999913
I0703 03:06:23.633968 31050 solver.cpp:546]     Test net output #2: loss = 0.131509 (* 1 = 0.131509 loss)
I0703 03:06:23.877291 31050 solver.cpp:290] Iteration 22000 (1.35792 iter/s, 73.642s/100 iter), loss = 0.0275534
I0703 03:06:23.877316 31050 solver.cpp:309]     Train net output #0: loss = 0.0275533 (* 1 = 0.0275533 loss)
I0703 03:06:23.877323 31050 sgd_solver.cpp:106] Iteration 22000, lr = 1e-05
I0703 03:06:47.378621 31050 solver.cpp:290] Iteration 22100 (4.2552 iter/s, 23.5007s/100 iter), loss = 0.0423177
I0703 03:06:47.378644 31050 solver.cpp:309]     Train net output #0: loss = 0.0423176 (* 1 = 0.0423176 loss)
I0703 03:06:47.378651 31050 sgd_solver.cpp:106] Iteration 22100, lr = 1e-05
I0703 03:07:11.563503 31050 solver.cpp:290] Iteration 22200 (4.13493 iter/s, 24.1842s/100 iter), loss = 0.0474546
I0703 03:07:11.563611 31050 solver.cpp:309]     Train net output #0: loss = 0.0474546 (* 1 = 0.0474546 loss)
I0703 03:07:11.563621 31050 sgd_solver.cpp:106] Iteration 22200, lr = 1e-05
I0703 03:07:35.713687 31050 solver.cpp:290] Iteration 22300 (4.14089 iter/s, 24.1494s/100 iter), loss = 0.0264324
I0703 03:07:35.713711 31050 solver.cpp:309]     Train net output #0: loss = 0.0264324 (* 1 = 0.0264324 loss)
I0703 03:07:35.713717 31050 sgd_solver.cpp:106] Iteration 22300, lr = 1e-05
I0703 03:07:59.903123 31050 solver.cpp:290] Iteration 22400 (4.13415 iter/s, 24.1888s/100 iter), loss = 0.0331791
I0703 03:07:59.903236 31050 solver.cpp:309]     Train net output #0: loss = 0.033179 (* 1 = 0.033179 loss)
I0703 03:07:59.903251 31050 sgd_solver.cpp:106] Iteration 22400, lr = 1e-05
I0703 03:08:24.090335 31050 solver.cpp:290] Iteration 22500 (4.13455 iter/s, 24.1865s/100 iter), loss = 0.0348935
I0703 03:08:24.090358 31050 solver.cpp:309]     Train net output #0: loss = 0.0348934 (* 1 = 0.0348934 loss)
I0703 03:08:24.090365 31050 sgd_solver.cpp:106] Iteration 22500, lr = 1e-05
I0703 03:08:48.293485 31050 solver.cpp:290] Iteration 22600 (4.13181 iter/s, 24.2025s/100 iter), loss = 0.0357021
I0703 03:08:48.293596 31050 solver.cpp:309]     Train net output #0: loss = 0.035702 (* 1 = 0.035702 loss)
I0703 03:08:48.293611 31050 sgd_solver.cpp:106] Iteration 22600, lr = 1e-05
I0703 03:09:12.462152 31050 solver.cpp:290] Iteration 22700 (4.13772 iter/s, 24.1679s/100 iter), loss = 0.0304754
I0703 03:09:12.462173 31050 solver.cpp:309]     Train net output #0: loss = 0.0304753 (* 1 = 0.0304753 loss)
I0703 03:09:12.462180 31050 sgd_solver.cpp:106] Iteration 22700, lr = 1e-05
I0703 03:09:36.969413 31050 solver.cpp:290] Iteration 22800 (4.08054 iter/s, 24.5066s/100 iter), loss = 0.0259729
I0703 03:09:36.969542 31050 solver.cpp:309]     Train net output #0: loss = 0.0259728 (* 1 = 0.0259728 loss)
I0703 03:09:36.969552 31050 sgd_solver.cpp:106] Iteration 22800, lr = 1e-05
I0703 03:10:01.162405 31050 solver.cpp:290] Iteration 22900 (4.13356 iter/s, 24.1922s/100 iter), loss = 0.0547392
I0703 03:10:01.162428 31050 solver.cpp:309]     Train net output #0: loss = 0.0547391 (* 1 = 0.0547391 loss)
I0703 03:10:01.162436 31050 sgd_solver.cpp:106] Iteration 22900, lr = 1e-05
I0703 03:10:25.118348 31050 solver.cpp:354] Sparsity after update:
I0703 03:10:25.165635 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 03:10:25.165652 31050 net.cpp:1851] conv1a_param_0(0.375) 
I0703 03:10:25.165660 31050 net.cpp:1851] conv1b_param_0(0.75) 
I0703 03:10:25.165663 31050 net.cpp:1851] ctx_conv1_param_0(0.75) 
I0703 03:10:25.165664 31050 net.cpp:1851] ctx_conv2_param_0(0.75) 
I0703 03:10:25.165666 31050 net.cpp:1851] ctx_conv3_param_0(0.75) 
I0703 03:10:25.165668 31050 net.cpp:1851] ctx_conv4_param_0(0.75) 
I0703 03:10:25.165670 31050 net.cpp:1851] ctx_final_param_0(0.00977) 
I0703 03:10:25.165673 31050 net.cpp:1851] out3a_param_0(0.75) 
I0703 03:10:25.165674 31050 net.cpp:1851] out5a_param_0(0.75) 
I0703 03:10:25.165676 31050 net.cpp:1851] res2a_branch2a_param_0(0.75) 
I0703 03:10:25.165678 31050 net.cpp:1851] res2a_branch2b_param_0(0.75) 
I0703 03:10:25.165680 31050 net.cpp:1851] res3a_branch2a_param_0(0.75) 
I0703 03:10:25.165683 31050 net.cpp:1851] res3a_branch2b_param_0(0.75) 
I0703 03:10:25.165684 31050 net.cpp:1851] res4a_branch2a_param_0(0.75) 
I0703 03:10:25.165686 31050 net.cpp:1851] res4a_branch2b_param_0(0.75) 
I0703 03:10:25.165688 31050 net.cpp:1851] res5a_branch2a_param_0(0.75) 
I0703 03:10:25.165689 31050 net.cpp:1851] res5a_branch2b_param_0(0.75) 
I0703 03:10:25.165691 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.01404e+06/2.69117e+06) 0.748
I0703 03:10:25.396705 31050 solver.cpp:290] Iteration 23000 (4.1265 iter/s, 24.2336s/100 iter), loss = 0.0220846
I0703 03:10:25.396729 31050 solver.cpp:309]     Train net output #0: loss = 0.0220845 (* 1 = 0.0220845 loss)
I0703 03:10:25.396736 31050 sgd_solver.cpp:106] Iteration 23000, lr = 1e-05
I0703 03:10:49.569432 31050 solver.cpp:290] Iteration 23100 (4.13701 iter/s, 24.1721s/100 iter), loss = 0.0300033
I0703 03:10:49.569454 31050 solver.cpp:309]     Train net output #0: loss = 0.0300033 (* 1 = 0.0300033 loss)
I0703 03:10:49.569461 31050 sgd_solver.cpp:106] Iteration 23100, lr = 1e-05
I0703 03:11:13.732787 31050 solver.cpp:290] Iteration 23200 (4.13861 iter/s, 24.1627s/100 iter), loss = 0.0402656
I0703 03:11:13.732903 31050 solver.cpp:309]     Train net output #0: loss = 0.0402655 (* 1 = 0.0402655 loss)
I0703 03:11:13.732911 31050 sgd_solver.cpp:106] Iteration 23200, lr = 1e-05
I0703 03:11:37.999940 31050 solver.cpp:290] Iteration 23300 (4.12093 iter/s, 24.2664s/100 iter), loss = 0.0337778
I0703 03:11:37.999965 31050 solver.cpp:309]     Train net output #0: loss = 0.0337777 (* 1 = 0.0337777 loss)
I0703 03:11:37.999974 31050 sgd_solver.cpp:106] Iteration 23300, lr = 1e-05
I0703 03:12:02.238126 31050 solver.cpp:290] Iteration 23400 (4.12584 iter/s, 24.2375s/100 iter), loss = 0.0217381
I0703 03:12:02.238241 31050 solver.cpp:309]     Train net output #0: loss = 0.021738 (* 1 = 0.021738 loss)
I0703 03:12:02.238251 31050 sgd_solver.cpp:106] Iteration 23400, lr = 1e-05
I0703 03:12:26.412237 31050 solver.cpp:290] Iteration 23500 (4.13679 iter/s, 24.1734s/100 iter), loss = 0.0441397
I0703 03:12:26.412261 31050 solver.cpp:309]     Train net output #0: loss = 0.0441397 (* 1 = 0.0441397 loss)
I0703 03:12:26.412268 31050 sgd_solver.cpp:106] Iteration 23500, lr = 1e-05
I0703 03:12:50.573282 31050 solver.cpp:290] Iteration 23600 (4.13901 iter/s, 24.1604s/100 iter), loss = 0.026131
I0703 03:12:50.573333 31050 solver.cpp:309]     Train net output #0: loss = 0.0261309 (* 1 = 0.0261309 loss)
I0703 03:12:50.573343 31050 sgd_solver.cpp:106] Iteration 23600, lr = 1e-05
I0703 03:13:14.922617 31050 solver.cpp:290] Iteration 23700 (4.10701 iter/s, 24.3486s/100 iter), loss = 0.0239946
I0703 03:13:14.922641 31050 solver.cpp:309]     Train net output #0: loss = 0.0239945 (* 1 = 0.0239945 loss)
I0703 03:13:14.922648 31050 sgd_solver.cpp:106] Iteration 23700, lr = 1e-05
I0703 03:13:39.105244 31050 solver.cpp:290] Iteration 23800 (4.13532 iter/s, 24.182s/100 iter), loss = 0.0279383
I0703 03:13:39.105351 31050 solver.cpp:309]     Train net output #0: loss = 0.0279382 (* 1 = 0.0279382 loss)
I0703 03:13:39.105361 31050 sgd_solver.cpp:106] Iteration 23800, lr = 1e-05
I0703 03:14:03.277520 31050 solver.cpp:290] Iteration 23900 (4.1371 iter/s, 24.1715s/100 iter), loss = 0.0501442
I0703 03:14:03.277546 31050 solver.cpp:309]     Train net output #0: loss = 0.0501442 (* 1 = 0.0501442 loss)
I0703 03:14:03.277554 31050 sgd_solver.cpp:106] Iteration 23900, lr = 1e-05
I0703 03:14:27.246575 31050 solver.cpp:354] Sparsity after update:
I0703 03:14:27.248405 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 03:14:27.248414 31050 net.cpp:1851] conv1a_param_0(0.375) 
I0703 03:14:27.248420 31050 net.cpp:1851] conv1b_param_0(0.75) 
I0703 03:14:27.248422 31050 net.cpp:1851] ctx_conv1_param_0(0.75) 
I0703 03:14:27.248425 31050 net.cpp:1851] ctx_conv2_param_0(0.75) 
I0703 03:14:27.248426 31050 net.cpp:1851] ctx_conv3_param_0(0.75) 
I0703 03:14:27.248428 31050 net.cpp:1851] ctx_conv4_param_0(0.75) 
I0703 03:14:27.248430 31050 net.cpp:1851] ctx_final_param_0(0.00977) 
I0703 03:14:27.248432 31050 net.cpp:1851] out3a_param_0(0.75) 
I0703 03:14:27.248435 31050 net.cpp:1851] out5a_param_0(0.75) 
I0703 03:14:27.248436 31050 net.cpp:1851] res2a_branch2a_param_0(0.75) 
I0703 03:14:27.248438 31050 net.cpp:1851] res2a_branch2b_param_0(0.75) 
I0703 03:14:27.248440 31050 net.cpp:1851] res3a_branch2a_param_0(0.75) 
I0703 03:14:27.248442 31050 net.cpp:1851] res3a_branch2b_param_0(0.75) 
I0703 03:14:27.248445 31050 net.cpp:1851] res4a_branch2a_param_0(0.75) 
I0703 03:14:27.248445 31050 net.cpp:1851] res4a_branch2b_param_0(0.75) 
I0703 03:14:27.248447 31050 net.cpp:1851] res5a_branch2a_param_0(0.75) 
I0703 03:14:27.248450 31050 net.cpp:1851] res5a_branch2b_param_0(0.75) 
I0703 03:14:27.248451 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.01404e+06/2.69117e+06) 0.748
I0703 03:14:27.248589 31050 solver.cpp:473] Iteration 24000, Testing net (#0)
I0703 03:15:14.734251 31050 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.949682
I0703 03:15:14.734346 31050 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.999885
I0703 03:15:14.734352 31050 solver.cpp:546]     Test net output #2: loss = 0.132505 (* 1 = 0.132505 loss)
I0703 03:15:15.004395 31050 solver.cpp:290] Iteration 24000 (1.39422 iter/s, 71.7249s/100 iter), loss = 0.0391522
I0703 03:15:15.004407 31166 sgd_solver.cpp:46] MultiStep Status: Iteration 24000, step = 1
I0703 03:15:15.004416 31050 solver.cpp:309]     Train net output #0: loss = 0.0391521 (* 1 = 0.0391521 loss)
I0703 03:15:15.004422 31050 sgd_solver.cpp:46] MultiStep Status: Iteration 24000, step = 1
I0703 03:15:15.004426 31050 sgd_solver.cpp:106] Iteration 24000, lr = 1e-06
I0703 03:15:38.463296 31050 solver.cpp:290] Iteration 24100 (4.26289 iter/s, 23.4582s/100 iter), loss = 0.0239242
I0703 03:15:38.463323 31050 solver.cpp:309]     Train net output #0: loss = 0.0239241 (* 1 = 0.0239241 loss)
I0703 03:15:38.463333 31050 sgd_solver.cpp:106] Iteration 24100, lr = 1e-06
I0703 03:16:02.611448 31050 solver.cpp:290] Iteration 24200 (4.14122 iter/s, 24.1475s/100 iter), loss = 0.0363424
I0703 03:16:02.611500 31050 solver.cpp:309]     Train net output #0: loss = 0.0363424 (* 1 = 0.0363424 loss)
I0703 03:16:02.611508 31050 sgd_solver.cpp:106] Iteration 24200, lr = 1e-06
I0703 03:16:26.858849 31050 solver.cpp:290] Iteration 24300 (4.12427 iter/s, 24.2467s/100 iter), loss = 0.0253505
I0703 03:16:26.858875 31050 solver.cpp:309]     Train net output #0: loss = 0.0253504 (* 1 = 0.0253504 loss)
I0703 03:16:26.858881 31050 sgd_solver.cpp:106] Iteration 24300, lr = 1e-06
I0703 03:16:51.550264 31050 solver.cpp:290] Iteration 24400 (4.0501 iter/s, 24.6907s/100 iter), loss = 0.0240232
I0703 03:16:51.550303 31050 solver.cpp:309]     Train net output #0: loss = 0.0240231 (* 1 = 0.0240231 loss)
I0703 03:16:51.550310 31050 sgd_solver.cpp:106] Iteration 24400, lr = 1e-06
I0703 03:17:15.719720 31050 solver.cpp:290] Iteration 24500 (4.13757 iter/s, 24.1688s/100 iter), loss = 0.0346604
I0703 03:17:15.719745 31050 solver.cpp:309]     Train net output #0: loss = 0.0346603 (* 1 = 0.0346603 loss)
I0703 03:17:15.719753 31050 sgd_solver.cpp:106] Iteration 24500, lr = 1e-06
I0703 03:17:39.937722 31050 solver.cpp:290] Iteration 24600 (4.12927 iter/s, 24.2173s/100 iter), loss = 0.0306895
I0703 03:17:39.937831 31050 solver.cpp:309]     Train net output #0: loss = 0.0306894 (* 1 = 0.0306894 loss)
I0703 03:17:39.937842 31050 sgd_solver.cpp:106] Iteration 24600, lr = 1e-06
I0703 03:18:04.156302 31050 solver.cpp:290] Iteration 24700 (4.12919 iter/s, 24.2178s/100 iter), loss = 0.0417974
I0703 03:18:04.156327 31050 solver.cpp:309]     Train net output #0: loss = 0.0417973 (* 1 = 0.0417973 loss)
I0703 03:18:04.156332 31050 sgd_solver.cpp:106] Iteration 24700, lr = 1e-06
I0703 03:18:28.319452 31050 solver.cpp:290] Iteration 24800 (4.13865 iter/s, 24.1625s/100 iter), loss = 0.0247898
I0703 03:18:28.319504 31050 solver.cpp:309]     Train net output #0: loss = 0.0247897 (* 1 = 0.0247897 loss)
I0703 03:18:28.319511 31050 sgd_solver.cpp:106] Iteration 24800, lr = 1e-06
I0703 03:18:52.498226 31050 solver.cpp:290] Iteration 24900 (4.13598 iter/s, 24.1781s/100 iter), loss = 0.0319622
I0703 03:18:52.498251 31050 solver.cpp:309]     Train net output #0: loss = 0.0319621 (* 1 = 0.0319621 loss)
I0703 03:18:52.498261 31050 sgd_solver.cpp:106] Iteration 24900, lr = 1e-06
I0703 03:19:16.449127 31050 solver.cpp:354] Sparsity after update:
I0703 03:19:16.496312 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 03:19:16.496330 31050 net.cpp:1851] conv1a_param_0(0.375) 
I0703 03:19:16.496337 31050 net.cpp:1851] conv1b_param_0(0.75) 
I0703 03:19:16.496340 31050 net.cpp:1851] ctx_conv1_param_0(0.75) 
I0703 03:19:16.496342 31050 net.cpp:1851] ctx_conv2_param_0(0.75) 
I0703 03:19:16.496345 31050 net.cpp:1851] ctx_conv3_param_0(0.75) 
I0703 03:19:16.496346 31050 net.cpp:1851] ctx_conv4_param_0(0.75) 
I0703 03:19:16.496347 31050 net.cpp:1851] ctx_final_param_0(0.00977) 
I0703 03:19:16.496350 31050 net.cpp:1851] out3a_param_0(0.75) 
I0703 03:19:16.496351 31050 net.cpp:1851] out5a_param_0(0.75) 
I0703 03:19:16.496353 31050 net.cpp:1851] res2a_branch2a_param_0(0.75) 
I0703 03:19:16.496356 31050 net.cpp:1851] res2a_branch2b_param_0(0.75) 
I0703 03:19:16.496357 31050 net.cpp:1851] res3a_branch2a_param_0(0.75) 
I0703 03:19:16.496359 31050 net.cpp:1851] res3a_branch2b_param_0(0.75) 
I0703 03:19:16.496361 31050 net.cpp:1851] res4a_branch2a_param_0(0.75) 
I0703 03:19:16.496363 31050 net.cpp:1851] res4a_branch2b_param_0(0.75) 
I0703 03:19:16.496366 31050 net.cpp:1851] res5a_branch2a_param_0(0.75) 
I0703 03:19:16.496367 31050 net.cpp:1851] res5a_branch2b_param_0(0.75) 
I0703 03:19:16.496369 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.01404e+06/2.69117e+06) 0.748
I0703 03:19:16.727283 31050 solver.cpp:290] Iteration 25000 (4.12739 iter/s, 24.2284s/100 iter), loss = 0.052638
I0703 03:19:16.727305 31050 solver.cpp:309]     Train net output #0: loss = 0.0526379 (* 1 = 0.0526379 loss)
I0703 03:19:16.727313 31050 sgd_solver.cpp:106] Iteration 25000, lr = 1e-06
I0703 03:19:40.910704 31050 solver.cpp:290] Iteration 25100 (4.13518 iter/s, 24.1828s/100 iter), loss = 0.017421
I0703 03:19:40.910728 31050 solver.cpp:309]     Train net output #0: loss = 0.017421 (* 1 = 0.017421 loss)
I0703 03:19:40.910735 31050 sgd_solver.cpp:106] Iteration 25100, lr = 1e-06
I0703 03:20:05.057654 31050 solver.cpp:290] Iteration 25200 (4.14142 iter/s, 24.1463s/100 iter), loss = 0.0360861
I0703 03:20:05.057761 31050 solver.cpp:309]     Train net output #0: loss = 0.036086 (* 1 = 0.036086 loss)
I0703 03:20:05.057771 31050 sgd_solver.cpp:106] Iteration 25200, lr = 1e-06
I0703 03:20:29.270552 31050 solver.cpp:290] Iteration 25300 (4.13016 iter/s, 24.2121s/100 iter), loss = 0.0467008
I0703 03:20:29.270575 31050 solver.cpp:309]     Train net output #0: loss = 0.0467008 (* 1 = 0.0467008 loss)
I0703 03:20:29.270581 31050 sgd_solver.cpp:106] Iteration 25300, lr = 1e-06
I0703 03:20:53.506413 31050 solver.cpp:290] Iteration 25400 (4.12623 iter/s, 24.2352s/100 iter), loss = 0.0185537
I0703 03:20:53.506525 31050 solver.cpp:309]     Train net output #0: loss = 0.0185536 (* 1 = 0.0185536 loss)
I0703 03:20:53.506536 31050 sgd_solver.cpp:106] Iteration 25400, lr = 1e-06
I0703 03:21:17.685900 31050 solver.cpp:290] Iteration 25500 (4.13587 iter/s, 24.1787s/100 iter), loss = 0.0366022
I0703 03:21:17.685923 31050 solver.cpp:309]     Train net output #0: loss = 0.0366021 (* 1 = 0.0366021 loss)
I0703 03:21:17.685930 31050 sgd_solver.cpp:106] Iteration 25500, lr = 1e-06
I0703 03:21:41.901273 31050 solver.cpp:290] Iteration 25600 (4.12972 iter/s, 24.2147s/100 iter), loss = 0.0324306
I0703 03:21:41.901309 31050 solver.cpp:309]     Train net output #0: loss = 0.0324305 (* 1 = 0.0324305 loss)
I0703 03:21:41.901316 31050 sgd_solver.cpp:106] Iteration 25600, lr = 1e-06
I0703 03:22:06.074038 31050 solver.cpp:290] Iteration 25700 (4.137 iter/s, 24.1721s/100 iter), loss = 0.0406754
I0703 03:22:06.074062 31050 solver.cpp:309]     Train net output #0: loss = 0.0406753 (* 1 = 0.0406753 loss)
I0703 03:22:06.074069 31050 sgd_solver.cpp:106] Iteration 25700, lr = 1e-06
I0703 03:22:30.278983 31050 solver.cpp:290] Iteration 25800 (4.1315 iter/s, 24.2043s/100 iter), loss = 0.0403722
I0703 03:22:30.279037 31050 solver.cpp:309]     Train net output #0: loss = 0.0403721 (* 1 = 0.0403721 loss)
I0703 03:22:30.279047 31050 sgd_solver.cpp:106] Iteration 25800, lr = 1e-06
I0703 03:22:54.439364 31050 solver.cpp:290] Iteration 25900 (4.13913 iter/s, 24.1597s/100 iter), loss = 0.0327967
I0703 03:22:54.439388 31050 solver.cpp:309]     Train net output #0: loss = 0.0327966 (* 1 = 0.0327966 loss)
I0703 03:22:54.439399 31050 sgd_solver.cpp:106] Iteration 25900, lr = 1e-06
I0703 03:23:18.367319 31050 solver.cpp:354] Sparsity after update:
I0703 03:23:18.369148 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 03:23:18.369155 31050 net.cpp:1851] conv1a_param_0(0.375) 
I0703 03:23:18.369163 31050 net.cpp:1851] conv1b_param_0(0.75) 
I0703 03:23:18.369165 31050 net.cpp:1851] ctx_conv1_param_0(0.75) 
I0703 03:23:18.369168 31050 net.cpp:1851] ctx_conv2_param_0(0.75) 
I0703 03:23:18.369169 31050 net.cpp:1851] ctx_conv3_param_0(0.75) 
I0703 03:23:18.369171 31050 net.cpp:1851] ctx_conv4_param_0(0.75) 
I0703 03:23:18.369174 31050 net.cpp:1851] ctx_final_param_0(0.00977) 
I0703 03:23:18.369177 31050 net.cpp:1851] out3a_param_0(0.75) 
I0703 03:23:18.369180 31050 net.cpp:1851] out5a_param_0(0.75) 
I0703 03:23:18.369184 31050 net.cpp:1851] res2a_branch2a_param_0(0.75) 
I0703 03:23:18.369187 31050 net.cpp:1851] res2a_branch2b_param_0(0.75) 
I0703 03:23:18.369191 31050 net.cpp:1851] res3a_branch2a_param_0(0.75) 
I0703 03:23:18.369194 31050 net.cpp:1851] res3a_branch2b_param_0(0.75) 
I0703 03:23:18.369197 31050 net.cpp:1851] res4a_branch2a_param_0(0.75) 
I0703 03:23:18.369201 31050 net.cpp:1851] res4a_branch2b_param_0(0.75) 
I0703 03:23:18.369204 31050 net.cpp:1851] res5a_branch2a_param_0(0.75) 
I0703 03:23:18.369207 31050 net.cpp:1851] res5a_branch2b_param_0(0.75) 
I0703 03:23:18.369211 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.01404e+06/2.69117e+06) 0.748
I0703 03:23:18.369352 31050 solver.cpp:473] Iteration 26000, Testing net (#0)
I0703 03:24:05.880182 31050 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.949922
I0703 03:24:05.880262 31050 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.999886
I0703 03:24:05.880270 31050 solver.cpp:546]     Test net output #2: loss = 0.134294 (* 1 = 0.134294 loss)
I0703 03:24:06.124701 31050 solver.cpp:290] Iteration 26000 (1.39502 iter/s, 71.6834s/100 iter), loss = 0.0258299
I0703 03:24:06.124724 31050 solver.cpp:309]     Train net output #0: loss = 0.0258298 (* 1 = 0.0258298 loss)
I0703 03:24:06.124732 31050 sgd_solver.cpp:106] Iteration 26000, lr = 1e-06
I0703 03:24:29.681589 31050 solver.cpp:290] Iteration 26100 (4.24516 iter/s, 23.5562s/100 iter), loss = 0.0220823
I0703 03:24:29.681613 31050 solver.cpp:309]     Train net output #0: loss = 0.0220822 (* 1 = 0.0220822 loss)
I0703 03:24:29.681619 31050 sgd_solver.cpp:106] Iteration 26100, lr = 1e-06
I0703 03:24:53.836769 31050 solver.cpp:290] Iteration 26200 (4.14001 iter/s, 24.1545s/100 iter), loss = 0.0351768
I0703 03:24:53.836875 31050 solver.cpp:309]     Train net output #0: loss = 0.0351767 (* 1 = 0.0351767 loss)
I0703 03:24:53.836885 31050 sgd_solver.cpp:106] Iteration 26200, lr = 1e-06
I0703 03:25:18.015760 31050 solver.cpp:290] Iteration 26300 (4.13595 iter/s, 24.1782s/100 iter), loss = 0.0512115
I0703 03:25:18.015787 31050 solver.cpp:309]     Train net output #0: loss = 0.0512114 (* 1 = 0.0512114 loss)
I0703 03:25:18.015797 31050 sgd_solver.cpp:106] Iteration 26300, lr = 1e-06
I0703 03:25:42.228476 31050 solver.cpp:290] Iteration 26400 (4.13018 iter/s, 24.212s/100 iter), loss = 0.0236228
I0703 03:25:42.228582 31050 solver.cpp:309]     Train net output #0: loss = 0.0236227 (* 1 = 0.0236227 loss)
I0703 03:25:42.228592 31050 sgd_solver.cpp:106] Iteration 26400, lr = 1e-06
I0703 03:26:06.927119 31050 solver.cpp:290] Iteration 26500 (4.04893 iter/s, 24.6979s/100 iter), loss = 0.0396236
I0703 03:26:06.927201 31050 solver.cpp:309]     Train net output #0: loss = 0.0396235 (* 1 = 0.0396235 loss)
I0703 03:26:06.927224 31050 sgd_solver.cpp:106] Iteration 26500, lr = 1e-06
I0703 03:26:31.345096 31050 solver.cpp:290] Iteration 26600 (4.09547 iter/s, 24.4172s/100 iter), loss = 0.0393066
I0703 03:26:31.345213 31050 solver.cpp:309]     Train net output #0: loss = 0.0393065 (* 1 = 0.0393065 loss)
I0703 03:26:31.345223 31050 sgd_solver.cpp:106] Iteration 26600, lr = 1e-06
I0703 03:26:55.522601 31050 solver.cpp:290] Iteration 26700 (4.13621 iter/s, 24.1767s/100 iter), loss = 0.0229388
I0703 03:26:55.522625 31050 solver.cpp:309]     Train net output #0: loss = 0.0229387 (* 1 = 0.0229387 loss)
I0703 03:26:55.522632 31050 sgd_solver.cpp:106] Iteration 26700, lr = 1e-06
I0703 03:27:19.771209 31050 solver.cpp:290] Iteration 26800 (4.12406 iter/s, 24.2479s/100 iter), loss = 0.040493
I0703 03:27:19.771329 31050 solver.cpp:309]     Train net output #0: loss = 0.0404929 (* 1 = 0.0404929 loss)
I0703 03:27:19.771339 31050 sgd_solver.cpp:106] Iteration 26800, lr = 1e-06
I0703 03:27:43.948935 31050 solver.cpp:290] Iteration 26900 (4.13617 iter/s, 24.177s/100 iter), loss = 0.0385041
I0703 03:27:43.948958 31050 solver.cpp:309]     Train net output #0: loss = 0.038504 (* 1 = 0.038504 loss)
I0703 03:27:43.948966 31050 sgd_solver.cpp:106] Iteration 26900, lr = 1e-06
I0703 03:28:07.880100 31050 solver.cpp:354] Sparsity after update:
I0703 03:28:07.933230 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 03:28:07.933246 31050 net.cpp:1851] conv1a_param_0(0.375) 
I0703 03:28:07.933254 31050 net.cpp:1851] conv1b_param_0(0.75) 
I0703 03:28:07.933256 31050 net.cpp:1851] ctx_conv1_param_0(0.75) 
I0703 03:28:07.933259 31050 net.cpp:1851] ctx_conv2_param_0(0.75) 
I0703 03:28:07.933260 31050 net.cpp:1851] ctx_conv3_param_0(0.75) 
I0703 03:28:07.933262 31050 net.cpp:1851] ctx_conv4_param_0(0.75) 
I0703 03:28:07.933264 31050 net.cpp:1851] ctx_final_param_0(0.00977) 
I0703 03:28:07.933266 31050 net.cpp:1851] out3a_param_0(0.75) 
I0703 03:28:07.933269 31050 net.cpp:1851] out5a_param_0(0.75) 
I0703 03:28:07.933270 31050 net.cpp:1851] res2a_branch2a_param_0(0.75) 
I0703 03:28:07.933272 31050 net.cpp:1851] res2a_branch2b_param_0(0.75) 
I0703 03:28:07.933274 31050 net.cpp:1851] res3a_branch2a_param_0(0.75) 
I0703 03:28:07.933276 31050 net.cpp:1851] res3a_branch2b_param_0(0.75) 
I0703 03:28:07.933279 31050 net.cpp:1851] res4a_branch2a_param_0(0.75) 
I0703 03:28:07.933280 31050 net.cpp:1851] res4a_branch2b_param_0(0.75) 
I0703 03:28:07.933284 31050 net.cpp:1851] res5a_branch2a_param_0(0.75) 
I0703 03:28:07.933286 31050 net.cpp:1851] res5a_branch2b_param_0(0.75) 
I0703 03:28:07.933290 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.01404e+06/2.69117e+06) 0.748
I0703 03:28:08.165176 31050 solver.cpp:290] Iteration 27000 (4.12957 iter/s, 24.2156s/100 iter), loss = 0.0343487
I0703 03:28:08.165204 31050 solver.cpp:309]     Train net output #0: loss = 0.0343486 (* 1 = 0.0343486 loss)
I0703 03:28:08.165210 31050 sgd_solver.cpp:106] Iteration 27000, lr = 1e-06
I0703 03:28:32.328593 31050 solver.cpp:290] Iteration 27100 (4.1386 iter/s, 24.1627s/100 iter), loss = 0.0350823
I0703 03:28:32.328618 31050 solver.cpp:309]     Train net output #0: loss = 0.0350822 (* 1 = 0.0350822 loss)
I0703 03:28:32.328624 31050 sgd_solver.cpp:106] Iteration 27100, lr = 1e-06
I0703 03:28:56.512454 31050 solver.cpp:290] Iteration 27200 (4.1351 iter/s, 24.1832s/100 iter), loss = 0.0316266
I0703 03:28:56.512567 31050 solver.cpp:309]     Train net output #0: loss = 0.0316265 (* 1 = 0.0316265 loss)
I0703 03:28:56.512576 31050 sgd_solver.cpp:106] Iteration 27200, lr = 1e-06
I0703 03:29:20.784421 31050 solver.cpp:290] Iteration 27300 (4.12011 iter/s, 24.2712s/100 iter), loss = 0.0281622
I0703 03:29:20.784443 31050 solver.cpp:309]     Train net output #0: loss = 0.0281621 (* 1 = 0.0281621 loss)
I0703 03:29:20.784451 31050 sgd_solver.cpp:106] Iteration 27300, lr = 1e-06
I0703 03:29:45.447849 31050 solver.cpp:290] Iteration 27400 (4.0547 iter/s, 24.6627s/100 iter), loss = 0.0337269
I0703 03:29:45.448129 31050 solver.cpp:309]     Train net output #0: loss = 0.0337268 (* 1 = 0.0337268 loss)
I0703 03:29:45.448139 31050 sgd_solver.cpp:106] Iteration 27400, lr = 1e-06
I0703 03:30:09.667512 31050 solver.cpp:290] Iteration 27500 (4.12903 iter/s, 24.2187s/100 iter), loss = 0.0464873
I0703 03:30:09.667536 31050 solver.cpp:309]     Train net output #0: loss = 0.0464872 (* 1 = 0.0464872 loss)
I0703 03:30:09.667543 31050 sgd_solver.cpp:106] Iteration 27500, lr = 1e-06
I0703 03:30:33.862308 31050 solver.cpp:290] Iteration 27600 (4.13323 iter/s, 24.1941s/100 iter), loss = 0.0331539
I0703 03:30:33.862426 31050 solver.cpp:309]     Train net output #0: loss = 0.0331538 (* 1 = 0.0331538 loss)
I0703 03:30:33.862437 31050 sgd_solver.cpp:106] Iteration 27600, lr = 1e-06
I0703 03:30:58.025126 31050 solver.cpp:290] Iteration 27700 (4.13872 iter/s, 24.1621s/100 iter), loss = 0.0314248
I0703 03:30:58.025149 31050 solver.cpp:309]     Train net output #0: loss = 0.0314247 (* 1 = 0.0314247 loss)
I0703 03:30:58.025156 31050 sgd_solver.cpp:106] Iteration 27700, lr = 1e-06
I0703 03:31:22.208746 31050 solver.cpp:290] Iteration 27800 (4.13514 iter/s, 24.183s/100 iter), loss = 0.029132
I0703 03:31:22.208854 31050 solver.cpp:309]     Train net output #0: loss = 0.0291318 (* 1 = 0.0291318 loss)
I0703 03:31:22.208865 31050 sgd_solver.cpp:106] Iteration 27800, lr = 1e-06
I0703 03:31:46.463614 31050 solver.cpp:290] Iteration 27900 (4.12301 iter/s, 24.2541s/100 iter), loss = 0.0386961
I0703 03:31:46.463639 31050 solver.cpp:309]     Train net output #0: loss = 0.0386959 (* 1 = 0.0386959 loss)
I0703 03:31:46.463646 31050 sgd_solver.cpp:106] Iteration 27900, lr = 1e-06
I0703 03:32:10.388208 31050 solver.cpp:354] Sparsity after update:
I0703 03:32:10.390059 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 03:32:10.390066 31050 net.cpp:1851] conv1a_param_0(0.375) 
I0703 03:32:10.390074 31050 net.cpp:1851] conv1b_param_0(0.75) 
I0703 03:32:10.390075 31050 net.cpp:1851] ctx_conv1_param_0(0.75) 
I0703 03:32:10.390077 31050 net.cpp:1851] ctx_conv2_param_0(0.75) 
I0703 03:32:10.390079 31050 net.cpp:1851] ctx_conv3_param_0(0.75) 
I0703 03:32:10.390081 31050 net.cpp:1851] ctx_conv4_param_0(0.75) 
I0703 03:32:10.390084 31050 net.cpp:1851] ctx_final_param_0(0.00977) 
I0703 03:32:10.390085 31050 net.cpp:1851] out3a_param_0(0.75) 
I0703 03:32:10.390087 31050 net.cpp:1851] out5a_param_0(0.75) 
I0703 03:32:10.390089 31050 net.cpp:1851] res2a_branch2a_param_0(0.75) 
I0703 03:32:10.390091 31050 net.cpp:1851] res2a_branch2b_param_0(0.75) 
I0703 03:32:10.390094 31050 net.cpp:1851] res3a_branch2a_param_0(0.75) 
I0703 03:32:10.390095 31050 net.cpp:1851] res3a_branch2b_param_0(0.75) 
I0703 03:32:10.390097 31050 net.cpp:1851] res4a_branch2a_param_0(0.75) 
I0703 03:32:10.390100 31050 net.cpp:1851] res4a_branch2b_param_0(0.75) 
I0703 03:32:10.390102 31050 net.cpp:1851] res5a_branch2a_param_0(0.75) 
I0703 03:32:10.390105 31050 net.cpp:1851] res5a_branch2b_param_0(0.75) 
I0703 03:32:10.390107 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.01404e+06/2.69117e+06) 0.748
I0703 03:32:10.390259 31050 solver.cpp:473] Iteration 28000, Testing net (#0)
I0703 03:32:57.916357 31050 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.950002
I0703 03:32:57.916452 31050 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.999899
I0703 03:32:57.916461 31050 solver.cpp:546]     Test net output #2: loss = 0.132145 (* 1 = 0.132145 loss)
I0703 03:32:58.173029 31050 solver.cpp:290] Iteration 28000 (1.39455 iter/s, 71.7075s/100 iter), loss = 0.0417616
I0703 03:32:58.173056 31050 solver.cpp:309]     Train net output #0: loss = 0.0417615 (* 1 = 0.0417615 loss)
I0703 03:32:58.173066 31050 sgd_solver.cpp:106] Iteration 28000, lr = 1e-06
I0703 03:33:21.691663 31050 solver.cpp:290] Iteration 28100 (4.25207 iter/s, 23.518s/100 iter), loss = 0.041887
I0703 03:33:21.691685 31050 solver.cpp:309]     Train net output #0: loss = 0.0418869 (* 1 = 0.0418869 loss)
I0703 03:33:21.691692 31050 sgd_solver.cpp:106] Iteration 28100, lr = 1e-06
I0703 03:33:45.887434 31050 solver.cpp:290] Iteration 28200 (4.13307 iter/s, 24.1951s/100 iter), loss = 0.0404967
I0703 03:33:45.887543 31050 solver.cpp:309]     Train net output #0: loss = 0.0404966 (* 1 = 0.0404966 loss)
I0703 03:33:45.887553 31050 sgd_solver.cpp:106] Iteration 28200, lr = 1e-06
I0703 03:34:10.591101 31050 solver.cpp:290] Iteration 28300 (4.04811 iter/s, 24.7029s/100 iter), loss = 0.0355308
I0703 03:34:10.591130 31050 solver.cpp:309]     Train net output #0: loss = 0.0355307 (* 1 = 0.0355307 loss)
I0703 03:34:10.591136 31050 sgd_solver.cpp:106] Iteration 28300, lr = 1e-06
I0703 03:34:35.317490 31050 solver.cpp:290] Iteration 28400 (4.04438 iter/s, 24.7257s/100 iter), loss = 0.0295667
I0703 03:34:35.317646 31050 solver.cpp:309]     Train net output #0: loss = 0.0295665 (* 1 = 0.0295665 loss)
I0703 03:34:35.317675 31050 sgd_solver.cpp:106] Iteration 28400, lr = 1e-06
I0703 03:34:59.902120 31050 solver.cpp:290] Iteration 28500 (4.06772 iter/s, 24.5838s/100 iter), loss = 0.0491516
I0703 03:34:59.902144 31050 solver.cpp:309]     Train net output #0: loss = 0.0491514 (* 1 = 0.0491514 loss)
I0703 03:34:59.902151 31050 sgd_solver.cpp:106] Iteration 28500, lr = 1e-06
I0703 03:35:24.252833 31050 solver.cpp:290] Iteration 28600 (4.10677 iter/s, 24.35s/100 iter), loss = 0.0273414
I0703 03:35:24.252878 31050 solver.cpp:309]     Train net output #0: loss = 0.0273413 (* 1 = 0.0273413 loss)
I0703 03:35:24.252885 31050 sgd_solver.cpp:106] Iteration 28600, lr = 1e-06
I0703 03:35:48.427219 31050 solver.cpp:290] Iteration 28700 (4.13673 iter/s, 24.1737s/100 iter), loss = 0.0343721
I0703 03:35:48.427242 31050 solver.cpp:309]     Train net output #0: loss = 0.034372 (* 1 = 0.034372 loss)
I0703 03:35:48.427248 31050 sgd_solver.cpp:106] Iteration 28700, lr = 1e-06
I0703 03:36:12.605129 31050 solver.cpp:290] Iteration 28800 (4.13612 iter/s, 24.1772s/100 iter), loss = 0.028265
I0703 03:36:12.605180 31050 solver.cpp:309]     Train net output #0: loss = 0.0282649 (* 1 = 0.0282649 loss)
I0703 03:36:12.605187 31050 sgd_solver.cpp:106] Iteration 28800, lr = 1e-06
I0703 03:36:36.774639 31050 solver.cpp:290] Iteration 28900 (4.13757 iter/s, 24.1688s/100 iter), loss = 0.0483962
I0703 03:36:36.774662 31050 solver.cpp:309]     Train net output #0: loss = 0.0483961 (* 1 = 0.0483961 loss)
I0703 03:36:36.774668 31050 sgd_solver.cpp:106] Iteration 28900, lr = 1e-06
I0703 03:37:00.702466 31050 solver.cpp:354] Sparsity after update:
I0703 03:37:00.710651 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 03:37:00.710680 31050 net.cpp:1851] conv1a_param_0(0.375) 
I0703 03:37:00.710697 31050 net.cpp:1851] conv1b_param_0(0.75) 
I0703 03:37:00.710701 31050 net.cpp:1851] ctx_conv1_param_0(0.75) 
I0703 03:37:00.710705 31050 net.cpp:1851] ctx_conv2_param_0(0.75) 
I0703 03:37:00.710707 31050 net.cpp:1851] ctx_conv3_param_0(0.75) 
I0703 03:37:00.710711 31050 net.cpp:1851] ctx_conv4_param_0(0.75) 
I0703 03:37:00.710713 31050 net.cpp:1851] ctx_final_param_0(0.00977) 
I0703 03:37:00.710719 31050 net.cpp:1851] out3a_param_0(0.75) 
I0703 03:37:00.710722 31050 net.cpp:1851] out5a_param_0(0.75) 
I0703 03:37:00.710726 31050 net.cpp:1851] res2a_branch2a_param_0(0.75) 
I0703 03:37:00.710728 31050 net.cpp:1851] res2a_branch2b_param_0(0.75) 
I0703 03:37:00.710731 31050 net.cpp:1851] res3a_branch2a_param_0(0.75) 
I0703 03:37:00.710734 31050 net.cpp:1851] res3a_branch2b_param_0(0.75) 
I0703 03:37:00.710736 31050 net.cpp:1851] res4a_branch2a_param_0(0.75) 
I0703 03:37:00.710739 31050 net.cpp:1851] res4a_branch2b_param_0(0.75) 
I0703 03:37:00.710742 31050 net.cpp:1851] res5a_branch2a_param_0(0.75) 
I0703 03:37:00.710746 31050 net.cpp:1851] res5a_branch2b_param_0(0.75) 
I0703 03:37:00.710748 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.01404e+06/2.69117e+06) 0.748
I0703 03:37:00.955595 31050 solver.cpp:290] Iteration 29000 (4.1356 iter/s, 24.1803s/100 iter), loss = 0.023284
I0703 03:37:00.955621 31050 solver.cpp:309]     Train net output #0: loss = 0.0232838 (* 1 = 0.0232838 loss)
I0703 03:37:00.955631 31050 sgd_solver.cpp:106] Iteration 29000, lr = 1e-06
I0703 03:37:25.122290 31050 solver.cpp:290] Iteration 29100 (4.13804 iter/s, 24.166s/100 iter), loss = 0.0296128
I0703 03:37:25.122313 31050 solver.cpp:309]     Train net output #0: loss = 0.0296126 (* 1 = 0.0296126 loss)
I0703 03:37:25.122320 31050 sgd_solver.cpp:106] Iteration 29100, lr = 1e-06
I0703 03:37:49.314083 31050 solver.cpp:290] Iteration 29200 (4.13375 iter/s, 24.1911s/100 iter), loss = 0.0554765
I0703 03:37:49.314159 31050 solver.cpp:309]     Train net output #0: loss = 0.0554764 (* 1 = 0.0554764 loss)
I0703 03:37:49.314170 31050 sgd_solver.cpp:106] Iteration 29200, lr = 1e-06
I0703 03:38:13.573786 31050 solver.cpp:290] Iteration 29300 (4.12219 iter/s, 24.259s/100 iter), loss = 0.0473359
I0703 03:38:13.573812 31050 solver.cpp:309]     Train net output #0: loss = 0.0473358 (* 1 = 0.0473358 loss)
I0703 03:38:13.573818 31050 sgd_solver.cpp:106] Iteration 29300, lr = 1e-06
I0703 03:38:37.973940 31050 solver.cpp:290] Iteration 29400 (4.09845 iter/s, 24.3995s/100 iter), loss = 0.0299759
I0703 03:38:37.974017 31050 solver.cpp:309]     Train net output #0: loss = 0.0299758 (* 1 = 0.0299758 loss)
I0703 03:38:37.974025 31050 sgd_solver.cpp:106] Iteration 29400, lr = 1e-06
I0703 03:39:02.217223 31050 solver.cpp:290] Iteration 29500 (4.12498 iter/s, 24.2426s/100 iter), loss = 0.0348946
I0703 03:39:02.217247 31050 solver.cpp:309]     Train net output #0: loss = 0.0348945 (* 1 = 0.0348945 loss)
I0703 03:39:02.217254 31050 sgd_solver.cpp:106] Iteration 29500, lr = 1e-06
I0703 03:39:26.375445 31050 solver.cpp:290] Iteration 29600 (4.13949 iter/s, 24.1575s/100 iter), loss = 0.0284152
I0703 03:39:26.375552 31050 solver.cpp:309]     Train net output #0: loss = 0.0284151 (* 1 = 0.0284151 loss)
I0703 03:39:26.375562 31050 sgd_solver.cpp:106] Iteration 29600, lr = 1e-06
I0703 03:39:50.575717 31050 solver.cpp:290] Iteration 29700 (4.13231 iter/s, 24.1995s/100 iter), loss = 0.0269243
I0703 03:39:50.575742 31050 solver.cpp:309]     Train net output #0: loss = 0.0269242 (* 1 = 0.0269242 loss)
I0703 03:39:50.575749 31050 sgd_solver.cpp:106] Iteration 29700, lr = 1e-06
I0703 03:40:14.771692 31050 solver.cpp:290] Iteration 29800 (4.13303 iter/s, 24.1953s/100 iter), loss = 0.0367221
I0703 03:40:14.771795 31050 solver.cpp:309]     Train net output #0: loss = 0.036722 (* 1 = 0.036722 loss)
I0703 03:40:14.771806 31050 sgd_solver.cpp:106] Iteration 29800, lr = 1e-06
I0703 03:40:38.986148 31050 solver.cpp:290] Iteration 29900 (4.12989 iter/s, 24.2137s/100 iter), loss = 0.0321402
I0703 03:40:38.986172 31050 solver.cpp:309]     Train net output #0: loss = 0.0321401 (* 1 = 0.0321401 loss)
I0703 03:40:38.986178 31050 sgd_solver.cpp:106] Iteration 29900, lr = 1e-06
I0703 03:41:02.889514 31050 solver.cpp:600] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-02_23-02-42/sparse/cityscapes5_jsegnet21v2_iter_30000.caffemodel
I0703 03:41:02.915658 31050 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-02_23-02-42/sparse/cityscapes5_jsegnet21v2_iter_30000.solverstate
I0703 03:41:02.935480 31050 solver.cpp:354] Sparsity after update:
I0703 03:41:02.936686 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 03:41:02.936695 31050 net.cpp:1851] conv1a_param_0(0.375) 
I0703 03:41:02.936702 31050 net.cpp:1851] conv1b_param_0(0.75) 
I0703 03:41:02.936704 31050 net.cpp:1851] ctx_conv1_param_0(0.75) 
I0703 03:41:02.936707 31050 net.cpp:1851] ctx_conv2_param_0(0.75) 
I0703 03:41:02.936708 31050 net.cpp:1851] ctx_conv3_param_0(0.75) 
I0703 03:41:02.936710 31050 net.cpp:1851] ctx_conv4_param_0(0.75) 
I0703 03:41:02.936712 31050 net.cpp:1851] ctx_final_param_0(0.00977) 
I0703 03:41:02.936714 31050 net.cpp:1851] out3a_param_0(0.75) 
I0703 03:41:02.936717 31050 net.cpp:1851] out5a_param_0(0.75) 
I0703 03:41:02.936718 31050 net.cpp:1851] res2a_branch2a_param_0(0.75) 
I0703 03:41:02.936720 31050 net.cpp:1851] res2a_branch2b_param_0(0.75) 
I0703 03:41:02.936722 31050 net.cpp:1851] res3a_branch2a_param_0(0.75) 
I0703 03:41:02.936724 31050 net.cpp:1851] res3a_branch2b_param_0(0.75) 
I0703 03:41:02.936727 31050 net.cpp:1851] res4a_branch2a_param_0(0.75) 
I0703 03:41:02.936728 31050 net.cpp:1851] res4a_branch2b_param_0(0.75) 
I0703 03:41:02.936730 31050 net.cpp:1851] res5a_branch2a_param_0(0.75) 
I0703 03:41:02.936731 31050 net.cpp:1851] res5a_branch2b_param_0(0.75) 
I0703 03:41:02.936735 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.01404e+06/2.69117e+06) 0.748
I0703 03:41:02.936933 31050 solver.cpp:473] Iteration 30000, Testing net (#0)
I0703 03:41:50.404796 31050 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.949785
I0703 03:41:50.404914 31050 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.999895
I0703 03:41:50.404923 31050 solver.cpp:546]     Test net output #2: loss = 0.135386 (* 1 = 0.135386 loss)
I0703 03:41:50.648829 31050 solver.cpp:290] Iteration 30000 (1.39546 iter/s, 71.6607s/100 iter), loss = 0.0329481
I0703 03:41:50.648850 31050 solver.cpp:309]     Train net output #0: loss = 0.032948 (* 1 = 0.032948 loss)
I0703 03:41:50.648857 31050 sgd_solver.cpp:106] Iteration 30000, lr = 1e-06
I0703 03:42:14.129989 31050 solver.cpp:290] Iteration 30100 (4.25885 iter/s, 23.4805s/100 iter), loss = 0.0334141
I0703 03:42:14.130012 31050 solver.cpp:309]     Train net output #0: loss = 0.033414 (* 1 = 0.033414 loss)
I0703 03:42:14.130020 31050 sgd_solver.cpp:106] Iteration 30100, lr = 1e-06
I0703 03:42:38.296923 31050 solver.cpp:290] Iteration 30200 (4.138 iter/s, 24.1662s/100 iter), loss = 0.0540834
I0703 03:42:38.296975 31050 solver.cpp:309]     Train net output #0: loss = 0.0540833 (* 1 = 0.0540833 loss)
I0703 03:42:38.296984 31050 sgd_solver.cpp:106] Iteration 30200, lr = 1e-06
I0703 03:43:02.531020 31050 solver.cpp:290] Iteration 30300 (4.12654 iter/s, 24.2334s/100 iter), loss = 0.0427723
I0703 03:43:02.531044 31050 solver.cpp:309]     Train net output #0: loss = 0.0427722 (* 1 = 0.0427722 loss)
I0703 03:43:02.531051 31050 sgd_solver.cpp:106] Iteration 30300, lr = 1e-06
I0703 03:43:26.748347 31050 solver.cpp:290] Iteration 30400 (4.12939 iter/s, 24.2166s/100 iter), loss = 0.0438154
I0703 03:43:26.748618 31050 solver.cpp:309]     Train net output #0: loss = 0.0438153 (* 1 = 0.0438153 loss)
I0703 03:43:26.748628 31050 sgd_solver.cpp:106] Iteration 30400, lr = 1e-06
I0703 03:43:50.956190 31050 solver.cpp:290] Iteration 30500 (4.13105 iter/s, 24.2069s/100 iter), loss = 0.0802295
I0703 03:43:50.956212 31050 solver.cpp:309]     Train net output #0: loss = 0.0802294 (* 1 = 0.0802294 loss)
I0703 03:43:50.956219 31050 sgd_solver.cpp:106] Iteration 30500, lr = 1e-06
I0703 03:44:15.092629 31050 solver.cpp:290] Iteration 30600 (4.14323 iter/s, 24.1358s/100 iter), loss = 0.0346827
I0703 03:44:15.092736 31050 solver.cpp:309]     Train net output #0: loss = 0.0346826 (* 1 = 0.0346826 loss)
I0703 03:44:15.092746 31050 sgd_solver.cpp:106] Iteration 30600, lr = 1e-06
I0703 03:44:39.367669 31050 solver.cpp:290] Iteration 30700 (4.11959 iter/s, 24.2743s/100 iter), loss = 0.0552618
I0703 03:44:39.367692 31050 solver.cpp:309]     Train net output #0: loss = 0.0552617 (* 1 = 0.0552617 loss)
I0703 03:44:39.367698 31050 sgd_solver.cpp:106] Iteration 30700, lr = 1e-06
I0703 03:45:03.556989 31050 solver.cpp:290] Iteration 30800 (4.13417 iter/s, 24.1886s/100 iter), loss = 0.0326711
I0703 03:45:03.557101 31050 solver.cpp:309]     Train net output #0: loss = 0.032671 (* 1 = 0.032671 loss)
I0703 03:45:03.557111 31050 sgd_solver.cpp:106] Iteration 30800, lr = 1e-06
I0703 03:45:27.752732 31050 solver.cpp:290] Iteration 30900 (4.13309 iter/s, 24.195s/100 iter), loss = 0.0389502
I0703 03:45:27.752756 31050 solver.cpp:309]     Train net output #0: loss = 0.0389501 (* 1 = 0.0389501 loss)
I0703 03:45:27.752763 31050 sgd_solver.cpp:106] Iteration 30900, lr = 1e-06
I0703 03:45:51.965190 31050 solver.cpp:354] Sparsity after update:
I0703 03:45:52.014014 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 03:45:52.014124 31050 net.cpp:1851] conv1a_param_0(0.375) 
I0703 03:45:52.014223 31050 net.cpp:1851] conv1b_param_0(0.75) 
I0703 03:45:52.014264 31050 net.cpp:1851] ctx_conv1_param_0(0.75) 
I0703 03:45:52.014276 31050 net.cpp:1851] ctx_conv2_param_0(0.75) 
I0703 03:45:52.014323 31050 net.cpp:1851] ctx_conv3_param_0(0.75) 
I0703 03:45:52.014364 31050 net.cpp:1851] ctx_conv4_param_0(0.75) 
I0703 03:45:52.014418 31050 net.cpp:1851] ctx_final_param_0(0.00977) 
I0703 03:45:52.014462 31050 net.cpp:1851] out3a_param_0(0.75) 
I0703 03:45:52.014506 31050 net.cpp:1851] out5a_param_0(0.75) 
I0703 03:45:52.014557 31050 net.cpp:1851] res2a_branch2a_param_0(0.75) 
I0703 03:45:52.014595 31050 net.cpp:1851] res2a_branch2b_param_0(0.75) 
I0703 03:45:52.014632 31050 net.cpp:1851] res3a_branch2a_param_0(0.75) 
I0703 03:45:52.014683 31050 net.cpp:1851] res3a_branch2b_param_0(0.75) 
I0703 03:45:52.014724 31050 net.cpp:1851] res4a_branch2a_param_0(0.75) 
I0703 03:45:52.014765 31050 net.cpp:1851] res4a_branch2b_param_0(0.75) 
I0703 03:45:52.014794 31050 net.cpp:1851] res5a_branch2a_param_0(0.75) 
I0703 03:45:52.014837 31050 net.cpp:1851] res5a_branch2b_param_0(0.75) 
I0703 03:45:52.014852 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.01404e+06/2.69117e+06) 0.748
I0703 03:45:52.251433 31050 solver.cpp:290] Iteration 31000 (4.08196 iter/s, 24.498s/100 iter), loss = 0.0451159
I0703 03:45:52.251463 31050 solver.cpp:309]     Train net output #0: loss = 0.0451158 (* 1 = 0.0451158 loss)
I0703 03:45:52.251474 31050 sgd_solver.cpp:106] Iteration 31000, lr = 1e-06
I0703 03:46:17.142480 31050 solver.cpp:290] Iteration 31100 (4.01762 iter/s, 24.8903s/100 iter), loss = 0.050652
I0703 03:46:17.142536 31050 solver.cpp:309]     Train net output #0: loss = 0.0506519 (* 1 = 0.0506519 loss)
I0703 03:46:17.142558 31050 sgd_solver.cpp:106] Iteration 31100, lr = 1e-06
I0703 03:46:41.972113 31050 solver.cpp:290] Iteration 31200 (4.02756 iter/s, 24.8289s/100 iter), loss = 0.0332733
I0703 03:46:41.972240 31050 solver.cpp:309]     Train net output #0: loss = 0.0332732 (* 1 = 0.0332732 loss)
I0703 03:46:41.972250 31050 sgd_solver.cpp:106] Iteration 31200, lr = 1e-06
I0703 03:47:06.836203 31050 solver.cpp:290] Iteration 31300 (4.02199 iter/s, 24.8633s/100 iter), loss = 0.0357578
I0703 03:47:06.836249 31050 solver.cpp:309]     Train net output #0: loss = 0.0357576 (* 1 = 0.0357576 loss)
I0703 03:47:06.836263 31050 sgd_solver.cpp:106] Iteration 31300, lr = 1e-06
I0703 03:47:31.668491 31050 solver.cpp:290] Iteration 31400 (4.02713 iter/s, 24.8316s/100 iter), loss = 0.0269662
I0703 03:47:31.668547 31050 solver.cpp:309]     Train net output #0: loss = 0.026966 (* 1 = 0.026966 loss)
I0703 03:47:31.668555 31050 sgd_solver.cpp:106] Iteration 31400, lr = 1e-06
I0703 03:47:56.226133 31050 solver.cpp:290] Iteration 31500 (4.07217 iter/s, 24.5569s/100 iter), loss = 0.0236665
I0703 03:47:56.226166 31050 solver.cpp:309]     Train net output #0: loss = 0.0236664 (* 1 = 0.0236664 loss)
I0703 03:47:56.226173 31050 sgd_solver.cpp:106] Iteration 31500, lr = 1e-06
I0703 03:48:21.010291 31050 solver.cpp:290] Iteration 31600 (4.03495 iter/s, 24.7835s/100 iter), loss = 0.0387683
I0703 03:48:21.010381 31050 solver.cpp:309]     Train net output #0: loss = 0.0387682 (* 1 = 0.0387682 loss)
I0703 03:48:21.010401 31050 sgd_solver.cpp:106] Iteration 31600, lr = 1e-06
I0703 03:48:46.033171 31050 solver.cpp:290] Iteration 31700 (3.99646 iter/s, 25.0221s/100 iter), loss = 0.0333781
I0703 03:48:46.033195 31050 solver.cpp:309]     Train net output #0: loss = 0.033378 (* 1 = 0.033378 loss)
I0703 03:48:46.033201 31050 sgd_solver.cpp:106] Iteration 31700, lr = 1e-06
I0703 03:49:10.592350 31050 solver.cpp:290] Iteration 31800 (4.07191 iter/s, 24.5585s/100 iter), loss = 0.0268927
I0703 03:49:10.592403 31050 solver.cpp:309]     Train net output #0: loss = 0.0268926 (* 1 = 0.0268926 loss)
I0703 03:49:10.592414 31050 sgd_solver.cpp:106] Iteration 31800, lr = 1e-06
I0703 03:49:34.753767 31050 solver.cpp:290] Iteration 31900 (4.13895 iter/s, 24.1607s/100 iter), loss = 0.0230631
I0703 03:49:34.753792 31050 solver.cpp:309]     Train net output #0: loss = 0.023063 (* 1 = 0.023063 loss)
I0703 03:49:34.753798 31050 sgd_solver.cpp:106] Iteration 31900, lr = 1e-06
I0703 03:49:58.667731 31050 solver.cpp:354] Sparsity after update:
I0703 03:49:58.669435 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 03:49:58.669442 31050 net.cpp:1851] conv1a_param_0(0.375) 
I0703 03:49:58.669450 31050 net.cpp:1851] conv1b_param_0(0.75) 
I0703 03:49:58.669451 31050 net.cpp:1851] ctx_conv1_param_0(0.75) 
I0703 03:49:58.669453 31050 net.cpp:1851] ctx_conv2_param_0(0.75) 
I0703 03:49:58.669456 31050 net.cpp:1851] ctx_conv3_param_0(0.75) 
I0703 03:49:58.669457 31050 net.cpp:1851] ctx_conv4_param_0(0.75) 
I0703 03:49:58.669459 31050 net.cpp:1851] ctx_final_param_0(0.00977) 
I0703 03:49:58.669461 31050 net.cpp:1851] out3a_param_0(0.75) 
I0703 03:49:58.669463 31050 net.cpp:1851] out5a_param_0(0.75) 
I0703 03:49:58.669466 31050 net.cpp:1851] res2a_branch2a_param_0(0.75) 
I0703 03:49:58.669467 31050 net.cpp:1851] res2a_branch2b_param_0(0.75) 
I0703 03:49:58.669469 31050 net.cpp:1851] res3a_branch2a_param_0(0.75) 
I0703 03:49:58.669471 31050 net.cpp:1851] res3a_branch2b_param_0(0.75) 
I0703 03:49:58.669473 31050 net.cpp:1851] res4a_branch2a_param_0(0.75) 
I0703 03:49:58.669476 31050 net.cpp:1851] res4a_branch2b_param_0(0.75) 
I0703 03:49:58.669477 31050 net.cpp:1851] res5a_branch2a_param_0(0.75) 
I0703 03:49:58.669479 31050 net.cpp:1851] res5a_branch2b_param_0(0.75) 
I0703 03:49:58.669481 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.01404e+06/2.69117e+06) 0.748
I0703 03:49:58.669490 31050 solver.cpp:600] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-02_23-02-42/sparse/cityscapes5_jsegnet21v2_iter_32000.caffemodel
I0703 03:49:58.694672 31050 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-02_23-02-42/sparse/cityscapes5_jsegnet21v2_iter_32000.solverstate
I0703 03:49:58.781277 31050 solver.cpp:453] Iteration 32000, loss = 0.0732625
I0703 03:49:58.781298 31050 solver.cpp:473] Iteration 32000, Testing net (#0)
I0703 03:50:46.871357 31050 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.95027
I0703 03:50:46.871454 31050 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.99989
I0703 03:50:46.871462 31050 solver.cpp:546]     Test net output #2: loss = 0.133052 (* 1 = 0.133052 loss)
I0703 03:50:46.871465 31050 solver.cpp:458] Optimization Done.
I0703 03:50:47.025221 31050 caffe.cpp:246] Optimization Done.
