I0815 22:42:37.046985 14815 caffe.cpp:608] This is NVCaffe 0.16.3 started at Tue Aug 15 22:42:36 2017
I0815 22:42:37.048161 14815 caffe.cpp:611] CuDNN version: 6021
I0815 22:42:37.048166 14815 caffe.cpp:612] CuBLAS version: 8000
I0815 22:42:37.048169 14815 caffe.cpp:613] CUDA version: 8000
I0815 22:42:37.048172 14815 caffe.cpp:614] CUDA driver version: 8000
I0815 22:42:37.366562 14815 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0815 22:42:37.367226 14815 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I0815 22:42:37.367828 14815 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[1]: total=8508145664 free=8379236352
I0815 22:42:37.368413 14815 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[2]: total=8508145664 free=8379236352
I0815 22:42:37.368424 14815 caffe.cpp:208] Using GPUs 0, 1, 2
I0815 22:42:37.368785 14815 caffe.cpp:213] GPU 0: GeForce GTX 1080
I0815 22:42:37.369148 14815 caffe.cpp:213] GPU 1: GeForce GTX 1080
I0815 22:42:37.369508 14815 caffe.cpp:213] GPU 2: GeForce GTX 1080
I0815 22:42:37.369551 14815 solver.cpp:42] Solver data type: FLOAT
I0815 22:42:37.369596 14815 solver.cpp:45] Initializing solver from parameters: 
train_net: "training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/sparse/train.prototxt"
test_net: "training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/sparse/test.prototxt"
test_iter: 125
test_interval: 2000
base_lr: 1e-05
display: 100
max_iter: 32000
lr_policy: "multistep"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 1e-05
snapshot: 10000
snapshot_prefix: "training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/sparse/cityscapes5_jsegnet21v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
regularization_type: "L1"
test_initialization: false
stepvalue: 24000
iter_size: 1
type: "Adam"
display_sparsity: 1000
sparse_mode: SPARSE_UPDATE
sparsity_target: 0.8
sparsity_step_factor: 0.01
sparsity_step_iter: 1000
sparsity_start_iter: 0
sparsity_start_factor: 0.8
I0815 22:42:37.385466 14815 solver.cpp:77] Creating training net from train_net file: training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/sparse/train.prototxt
I0815 22:42:37.386652 14815 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0815 22:42:37.386662 14815 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
W0815 22:42:37.386710 14815 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0815 22:42:37.387645 14815 net.cpp:72] Initializing net from parameters: 
name: "jsegnet21v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/train-image-lmdb"
    label_list_path: "data/train-label-lmdb"
    batch_size: 6
    shuffle: false
    threads: 1
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
I0815 22:42:37.387857 14815 net.cpp:104] Using FLOAT as default forward math type
I0815 22:42:37.387863 14815 net.cpp:110] Using FLOAT as default backward math type
I0815 22:42:37.387867 14815 layer_factory.hpp:136] Creating layer 'data' of type 'ImageLabelData'
I0815 22:42:37.387873 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:37.387887 14815 net.cpp:184] Created Layer data (0)
I0815 22:42:37.387892 14815 net.cpp:530] data -> data
I0815 22:42:37.387917 14815 net.cpp:530] data -> label
I0815 22:42:37.398145 14815 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0815 22:42:37.398164 14815 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 22:42:37.428581 14870 db_lmdb.cpp:24] Opened lmdb data/train-image-lmdb
I0815 22:42:37.431659 14815 data_layer.cpp:185] [0] ReshapePrefetch 6, 3, 640, 640
I0815 22:42:37.431738 14815 data_layer.cpp:209] [0] Output data size: 6, 3, 640, 640
I0815 22:42:37.431749 14815 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 22:42:37.431829 14815 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0815 22:42:37.431841 14815 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 22:42:37.432718 14871 data_layer.cpp:97] [0] Parser threads: 1
I0815 22:42:37.432730 14871 data_layer.cpp:99] [0] Transformer threads: 1
I0815 22:42:37.438060 14872 db_lmdb.cpp:24] Opened lmdb data/train-label-lmdb
I0815 22:42:37.439265 14815 data_layer.cpp:185] [0] ReshapePrefetch 6, 1, 640, 640
I0815 22:42:37.439353 14815 data_layer.cpp:209] [0] Output data size: 6, 1, 640, 640
I0815 22:42:37.439363 14815 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 22:42:37.439466 14815 net.cpp:245] Setting up data
I0815 22:42:37.439486 14815 net.cpp:252] TRAIN Top shape for layer 0 'data' 6 3 640 640 (7372800)
I0815 22:42:37.439496 14815 net.cpp:252] TRAIN Top shape for layer 0 'data' 6 1 640 640 (2457600)
I0815 22:42:37.439530 14815 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0815 22:42:37.439540 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:37.439569 14815 net.cpp:184] Created Layer data/bias (1)
I0815 22:42:37.439575 14815 net.cpp:561] data/bias <- data
I0815 22:42:37.439592 14815 net.cpp:530] data/bias -> data/bias
I0815 22:42:37.444911 14873 data_layer.cpp:97] [0] Parser threads: 1
I0815 22:42:37.444939 14873 data_layer.cpp:99] [0] Transformer threads: 1
I0815 22:42:37.447520 14815 net.cpp:245] Setting up data/bias
I0815 22:42:37.447597 14815 net.cpp:252] TRAIN Top shape for layer 1 'data/bias' 6 3 640 640 (7372800)
I0815 22:42:37.447621 14815 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0815 22:42:37.447652 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:37.447692 14815 net.cpp:184] Created Layer conv1a (2)
I0815 22:42:37.447697 14815 net.cpp:561] conv1a <- data/bias
I0815 22:42:37.447703 14815 net.cpp:530] conv1a -> conv1a
I0815 22:42:38.114511 14815 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 7.9G, req 0G)
I0815 22:42:38.114533 14815 net.cpp:245] Setting up conv1a
I0815 22:42:38.114540 14815 net.cpp:252] TRAIN Top shape for layer 2 'conv1a' 6 32 320 320 (19660800)
I0815 22:42:38.114552 14815 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0815 22:42:38.114557 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.114570 14815 net.cpp:184] Created Layer conv1a/bn (3)
I0815 22:42:38.114574 14815 net.cpp:561] conv1a/bn <- conv1a
I0815 22:42:38.114581 14815 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0815 22:42:38.115439 14815 net.cpp:245] Setting up conv1a/bn
I0815 22:42:38.115449 14815 net.cpp:252] TRAIN Top shape for layer 3 'conv1a/bn' 6 32 320 320 (19660800)
I0815 22:42:38.115459 14815 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0815 22:42:38.115463 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.115469 14815 net.cpp:184] Created Layer conv1a/relu (4)
I0815 22:42:38.115473 14815 net.cpp:561] conv1a/relu <- conv1a
I0815 22:42:38.115476 14815 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0815 22:42:38.115489 14815 net.cpp:245] Setting up conv1a/relu
I0815 22:42:38.115494 14815 net.cpp:252] TRAIN Top shape for layer 4 'conv1a/relu' 6 32 320 320 (19660800)
I0815 22:42:38.115499 14815 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0815 22:42:38.115501 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.115515 14815 net.cpp:184] Created Layer conv1b (5)
I0815 22:42:38.115520 14815 net.cpp:561] conv1b <- conv1a
I0815 22:42:38.115525 14815 net.cpp:530] conv1b -> conv1b
I0815 22:42:38.162139 14815 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.73G, req 0G)
I0815 22:42:38.162151 14815 net.cpp:245] Setting up conv1b
I0815 22:42:38.162158 14815 net.cpp:252] TRAIN Top shape for layer 5 'conv1b' 6 32 320 320 (19660800)
I0815 22:42:38.162165 14815 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0815 22:42:38.162169 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.162175 14815 net.cpp:184] Created Layer conv1b/bn (6)
I0815 22:42:38.162178 14815 net.cpp:561] conv1b/bn <- conv1b
I0815 22:42:38.162183 14815 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0815 22:42:38.162966 14815 net.cpp:245] Setting up conv1b/bn
I0815 22:42:38.162974 14815 net.cpp:252] TRAIN Top shape for layer 6 'conv1b/bn' 6 32 320 320 (19660800)
I0815 22:42:38.162983 14815 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0815 22:42:38.162986 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.162992 14815 net.cpp:184] Created Layer conv1b/relu (7)
I0815 22:42:38.162995 14815 net.cpp:561] conv1b/relu <- conv1b
I0815 22:42:38.162998 14815 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0815 22:42:38.163003 14815 net.cpp:245] Setting up conv1b/relu
I0815 22:42:38.163008 14815 net.cpp:252] TRAIN Top shape for layer 7 'conv1b/relu' 6 32 320 320 (19660800)
I0815 22:42:38.163012 14815 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0815 22:42:38.163017 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.163023 14815 net.cpp:184] Created Layer pool1 (8)
I0815 22:42:38.163027 14815 net.cpp:561] pool1 <- conv1b
I0815 22:42:38.163030 14815 net.cpp:530] pool1 -> pool1
I0815 22:42:38.163132 14815 net.cpp:245] Setting up pool1
I0815 22:42:38.163138 14815 net.cpp:252] TRAIN Top shape for layer 8 'pool1' 6 32 160 160 (4915200)
I0815 22:42:38.163142 14815 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0815 22:42:38.163146 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.163154 14815 net.cpp:184] Created Layer res2a_branch2a (9)
I0815 22:42:38.163157 14815 net.cpp:561] res2a_branch2a <- pool1
I0815 22:42:38.163161 14815 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0815 22:42:38.204340 14815 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.61G, req 0G)
I0815 22:42:38.204365 14815 net.cpp:245] Setting up res2a_branch2a
I0815 22:42:38.204373 14815 net.cpp:252] TRAIN Top shape for layer 9 'res2a_branch2a' 6 64 160 160 (9830400)
I0815 22:42:38.204385 14815 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0815 22:42:38.204391 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.204402 14815 net.cpp:184] Created Layer res2a_branch2a/bn (10)
I0815 22:42:38.204407 14815 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0815 22:42:38.204412 14815 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0815 22:42:38.205930 14815 net.cpp:245] Setting up res2a_branch2a/bn
I0815 22:42:38.205941 14815 net.cpp:252] TRAIN Top shape for layer 10 'res2a_branch2a/bn' 6 64 160 160 (9830400)
I0815 22:42:38.205951 14815 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0815 22:42:38.205955 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.205960 14815 net.cpp:184] Created Layer res2a_branch2a/relu (11)
I0815 22:42:38.205965 14815 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0815 22:42:38.205968 14815 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0815 22:42:38.205974 14815 net.cpp:245] Setting up res2a_branch2a/relu
I0815 22:42:38.205979 14815 net.cpp:252] TRAIN Top shape for layer 11 'res2a_branch2a/relu' 6 64 160 160 (9830400)
I0815 22:42:38.205982 14815 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0815 22:42:38.205986 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.205996 14815 net.cpp:184] Created Layer res2a_branch2b (12)
I0815 22:42:38.206001 14815 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0815 22:42:38.206003 14815 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0815 22:42:38.227565 14815 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.52G, req 0G)
I0815 22:42:38.227578 14815 net.cpp:245] Setting up res2a_branch2b
I0815 22:42:38.227583 14815 net.cpp:252] TRAIN Top shape for layer 12 'res2a_branch2b' 6 64 160 160 (9830400)
I0815 22:42:38.227589 14815 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0815 22:42:38.227593 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.227600 14815 net.cpp:184] Created Layer res2a_branch2b/bn (13)
I0815 22:42:38.227603 14815 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0815 22:42:38.227607 14815 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0815 22:42:38.228422 14815 net.cpp:245] Setting up res2a_branch2b/bn
I0815 22:42:38.228431 14815 net.cpp:252] TRAIN Top shape for layer 13 'res2a_branch2b/bn' 6 64 160 160 (9830400)
I0815 22:42:38.228440 14815 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0815 22:42:38.228444 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.228448 14815 net.cpp:184] Created Layer res2a_branch2b/relu (14)
I0815 22:42:38.228451 14815 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0815 22:42:38.228456 14815 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0815 22:42:38.228469 14815 net.cpp:245] Setting up res2a_branch2b/relu
I0815 22:42:38.228474 14815 net.cpp:252] TRAIN Top shape for layer 14 'res2a_branch2b/relu' 6 64 160 160 (9830400)
I0815 22:42:38.228478 14815 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0815 22:42:38.228482 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.228489 14815 net.cpp:184] Created Layer pool2 (15)
I0815 22:42:38.228492 14815 net.cpp:561] pool2 <- res2a_branch2b
I0815 22:42:38.228497 14815 net.cpp:530] pool2 -> pool2
I0815 22:42:38.228574 14815 net.cpp:245] Setting up pool2
I0815 22:42:38.228579 14815 net.cpp:252] TRAIN Top shape for layer 15 'pool2' 6 64 80 80 (2457600)
I0815 22:42:38.228584 14815 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0815 22:42:38.228586 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.228595 14815 net.cpp:184] Created Layer res3a_branch2a (16)
I0815 22:42:38.228598 14815 net.cpp:561] res3a_branch2a <- pool2
I0815 22:42:38.228601 14815 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0815 22:42:38.250130 14815 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 7.46G, req 0G)
I0815 22:42:38.250151 14815 net.cpp:245] Setting up res3a_branch2a
I0815 22:42:38.250159 14815 net.cpp:252] TRAIN Top shape for layer 16 'res3a_branch2a' 6 128 80 80 (4915200)
I0815 22:42:38.250167 14815 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0815 22:42:38.250172 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.250181 14815 net.cpp:184] Created Layer res3a_branch2a/bn (17)
I0815 22:42:38.250186 14815 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0815 22:42:38.250190 14815 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0815 22:42:38.251045 14815 net.cpp:245] Setting up res3a_branch2a/bn
I0815 22:42:38.251055 14815 net.cpp:252] TRAIN Top shape for layer 17 'res3a_branch2a/bn' 6 128 80 80 (4915200)
I0815 22:42:38.251066 14815 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0815 22:42:38.251070 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.251075 14815 net.cpp:184] Created Layer res3a_branch2a/relu (18)
I0815 22:42:38.251078 14815 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0815 22:42:38.251081 14815 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0815 22:42:38.251086 14815 net.cpp:245] Setting up res3a_branch2a/relu
I0815 22:42:38.251091 14815 net.cpp:252] TRAIN Top shape for layer 18 'res3a_branch2a/relu' 6 128 80 80 (4915200)
I0815 22:42:38.251096 14815 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0815 22:42:38.251099 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.251107 14815 net.cpp:184] Created Layer res3a_branch2b (19)
I0815 22:42:38.251111 14815 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0815 22:42:38.251114 14815 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0815 22:42:38.262527 14815 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.42G, req 0G)
I0815 22:42:38.262540 14815 net.cpp:245] Setting up res3a_branch2b
I0815 22:42:38.262545 14815 net.cpp:252] TRAIN Top shape for layer 19 'res3a_branch2b' 6 128 80 80 (4915200)
I0815 22:42:38.262552 14815 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0815 22:42:38.262555 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.262562 14815 net.cpp:184] Created Layer res3a_branch2b/bn (20)
I0815 22:42:38.262565 14815 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0815 22:42:38.262569 14815 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0815 22:42:38.263373 14815 net.cpp:245] Setting up res3a_branch2b/bn
I0815 22:42:38.263393 14815 net.cpp:252] TRAIN Top shape for layer 20 'res3a_branch2b/bn' 6 128 80 80 (4915200)
I0815 22:42:38.263402 14815 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0815 22:42:38.263406 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.263411 14815 net.cpp:184] Created Layer res3a_branch2b/relu (21)
I0815 22:42:38.263414 14815 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0815 22:42:38.263418 14815 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0815 22:42:38.263423 14815 net.cpp:245] Setting up res3a_branch2b/relu
I0815 22:42:38.263428 14815 net.cpp:252] TRAIN Top shape for layer 21 'res3a_branch2b/relu' 6 128 80 80 (4915200)
I0815 22:42:38.263432 14815 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0815 22:42:38.263435 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.263443 14815 net.cpp:184] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (22)
I0815 22:42:38.263447 14815 net.cpp:561] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0815 22:42:38.263449 14815 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0815 22:42:38.263453 14815 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0815 22:42:38.263506 14815 net.cpp:245] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0815 22:42:38.263512 14815 net.cpp:252] TRAIN Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 6 128 80 80 (4915200)
I0815 22:42:38.263517 14815 net.cpp:252] TRAIN Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 6 128 80 80 (4915200)
I0815 22:42:38.263520 14815 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0815 22:42:38.263525 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.263530 14815 net.cpp:184] Created Layer pool3 (23)
I0815 22:42:38.263533 14815 net.cpp:561] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0815 22:42:38.263537 14815 net.cpp:530] pool3 -> pool3
I0815 22:42:38.263623 14815 net.cpp:245] Setting up pool3
I0815 22:42:38.263629 14815 net.cpp:252] TRAIN Top shape for layer 23 'pool3' 6 128 40 40 (1228800)
I0815 22:42:38.263633 14815 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0815 22:42:38.263636 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.263646 14815 net.cpp:184] Created Layer res4a_branch2a (24)
I0815 22:42:38.263669 14815 net.cpp:561] res4a_branch2a <- pool3
I0815 22:42:38.263674 14815 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0815 22:42:38.293735 14815 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.38G, req 0G)
I0815 22:42:38.293761 14815 net.cpp:245] Setting up res4a_branch2a
I0815 22:42:38.293767 14815 net.cpp:252] TRAIN Top shape for layer 24 'res4a_branch2a' 6 256 40 40 (2457600)
I0815 22:42:38.293776 14815 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0815 22:42:38.293781 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.293794 14815 net.cpp:184] Created Layer res4a_branch2a/bn (25)
I0815 22:42:38.293799 14815 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0815 22:42:38.293804 14815 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0815 22:42:38.294672 14815 net.cpp:245] Setting up res4a_branch2a/bn
I0815 22:42:38.294682 14815 net.cpp:252] TRAIN Top shape for layer 25 'res4a_branch2a/bn' 6 256 40 40 (2457600)
I0815 22:42:38.294692 14815 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0815 22:42:38.294695 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.294700 14815 net.cpp:184] Created Layer res4a_branch2a/relu (26)
I0815 22:42:38.294713 14815 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0815 22:42:38.294718 14815 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0815 22:42:38.294724 14815 net.cpp:245] Setting up res4a_branch2a/relu
I0815 22:42:38.294728 14815 net.cpp:252] TRAIN Top shape for layer 26 'res4a_branch2a/relu' 6 256 40 40 (2457600)
I0815 22:42:38.294733 14815 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0815 22:42:38.294736 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.294749 14815 net.cpp:184] Created Layer res4a_branch2b (27)
I0815 22:42:38.294752 14815 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0815 22:42:38.294756 14815 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0815 22:42:38.304922 14815 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.36G, req 0G)
I0815 22:42:38.304939 14815 net.cpp:245] Setting up res4a_branch2b
I0815 22:42:38.304945 14815 net.cpp:252] TRAIN Top shape for layer 27 'res4a_branch2b' 6 256 40 40 (2457600)
I0815 22:42:38.304953 14815 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0815 22:42:38.304958 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.304967 14815 net.cpp:184] Created Layer res4a_branch2b/bn (28)
I0815 22:42:38.304971 14815 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0815 22:42:38.304976 14815 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0815 22:42:38.306119 14815 net.cpp:245] Setting up res4a_branch2b/bn
I0815 22:42:38.306129 14815 net.cpp:252] TRAIN Top shape for layer 28 'res4a_branch2b/bn' 6 256 40 40 (2457600)
I0815 22:42:38.306138 14815 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0815 22:42:38.306143 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.306149 14815 net.cpp:184] Created Layer res4a_branch2b/relu (29)
I0815 22:42:38.306152 14815 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0815 22:42:38.306156 14815 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0815 22:42:38.306162 14815 net.cpp:245] Setting up res4a_branch2b/relu
I0815 22:42:38.306167 14815 net.cpp:252] TRAIN Top shape for layer 29 'res4a_branch2b/relu' 6 256 40 40 (2457600)
I0815 22:42:38.306170 14815 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0815 22:42:38.306174 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.306182 14815 net.cpp:184] Created Layer pool4 (30)
I0815 22:42:38.306186 14815 net.cpp:561] pool4 <- res4a_branch2b
I0815 22:42:38.306190 14815 net.cpp:530] pool4 -> pool4
I0815 22:42:38.306270 14815 net.cpp:245] Setting up pool4
I0815 22:42:38.306277 14815 net.cpp:252] TRAIN Top shape for layer 30 'pool4' 6 256 40 40 (2457600)
I0815 22:42:38.306282 14815 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0815 22:42:38.306285 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.306296 14815 net.cpp:184] Created Layer res5a_branch2a (31)
I0815 22:42:38.306300 14815 net.cpp:561] res5a_branch2a <- pool4
I0815 22:42:38.306304 14815 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0815 22:42:38.340586 14815 net.cpp:245] Setting up res5a_branch2a
I0815 22:42:38.340611 14815 net.cpp:252] TRAIN Top shape for layer 31 'res5a_branch2a' 6 512 40 40 (4915200)
I0815 22:42:38.340620 14815 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0815 22:42:38.340626 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.340636 14815 net.cpp:184] Created Layer res5a_branch2a/bn (32)
I0815 22:42:38.340641 14815 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0815 22:42:38.340646 14815 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0815 22:42:38.341481 14815 net.cpp:245] Setting up res5a_branch2a/bn
I0815 22:42:38.341495 14815 net.cpp:252] TRAIN Top shape for layer 32 'res5a_branch2a/bn' 6 512 40 40 (4915200)
I0815 22:42:38.341505 14815 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0815 22:42:38.341508 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.341513 14815 net.cpp:184] Created Layer res5a_branch2a/relu (33)
I0815 22:42:38.341516 14815 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0815 22:42:38.341521 14815 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0815 22:42:38.341526 14815 net.cpp:245] Setting up res5a_branch2a/relu
I0815 22:42:38.341531 14815 net.cpp:252] TRAIN Top shape for layer 33 'res5a_branch2a/relu' 6 512 40 40 (4915200)
I0815 22:42:38.341534 14815 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0815 22:42:38.341538 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.341547 14815 net.cpp:184] Created Layer res5a_branch2b (34)
I0815 22:42:38.341552 14815 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0815 22:42:38.341554 14815 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0815 22:42:38.358665 14815 net.cpp:245] Setting up res5a_branch2b
I0815 22:42:38.358691 14815 net.cpp:252] TRAIN Top shape for layer 34 'res5a_branch2b' 6 512 40 40 (4915200)
I0815 22:42:38.358707 14815 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0815 22:42:38.358713 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.358722 14815 net.cpp:184] Created Layer res5a_branch2b/bn (35)
I0815 22:42:38.358727 14815 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0815 22:42:38.358732 14815 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0815 22:42:38.359558 14815 net.cpp:245] Setting up res5a_branch2b/bn
I0815 22:42:38.359567 14815 net.cpp:252] TRAIN Top shape for layer 35 'res5a_branch2b/bn' 6 512 40 40 (4915200)
I0815 22:42:38.359575 14815 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0815 22:42:38.359580 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.359585 14815 net.cpp:184] Created Layer res5a_branch2b/relu (36)
I0815 22:42:38.359587 14815 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0815 22:42:38.359591 14815 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0815 22:42:38.359596 14815 net.cpp:245] Setting up res5a_branch2b/relu
I0815 22:42:38.359601 14815 net.cpp:252] TRAIN Top shape for layer 36 'res5a_branch2b/relu' 6 512 40 40 (4915200)
I0815 22:42:38.359604 14815 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I0815 22:42:38.359607 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.359622 14815 net.cpp:184] Created Layer out5a (37)
I0815 22:42:38.359625 14815 net.cpp:561] out5a <- res5a_branch2b
I0815 22:42:38.359629 14815 net.cpp:530] out5a -> out5a
I0815 22:42:38.365226 14815 net.cpp:245] Setting up out5a
I0815 22:42:38.365238 14815 net.cpp:252] TRAIN Top shape for layer 37 'out5a' 6 64 40 40 (614400)
I0815 22:42:38.365245 14815 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I0815 22:42:38.365249 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.365257 14815 net.cpp:184] Created Layer out5a/bn (38)
I0815 22:42:38.365259 14815 net.cpp:561] out5a/bn <- out5a
I0815 22:42:38.365264 14815 net.cpp:513] out5a/bn -> out5a (in-place)
I0815 22:42:38.366106 14815 net.cpp:245] Setting up out5a/bn
I0815 22:42:38.366113 14815 net.cpp:252] TRAIN Top shape for layer 38 'out5a/bn' 6 64 40 40 (614400)
I0815 22:42:38.366122 14815 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I0815 22:42:38.366125 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.366139 14815 net.cpp:184] Created Layer out5a/relu (39)
I0815 22:42:38.366142 14815 net.cpp:561] out5a/relu <- out5a
I0815 22:42:38.366147 14815 net.cpp:513] out5a/relu -> out5a (in-place)
I0815 22:42:38.366152 14815 net.cpp:245] Setting up out5a/relu
I0815 22:42:38.366156 14815 net.cpp:252] TRAIN Top shape for layer 39 'out5a/relu' 6 64 40 40 (614400)
I0815 22:42:38.366159 14815 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I0815 22:42:38.366163 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.366179 14815 net.cpp:184] Created Layer out5a_up2 (40)
I0815 22:42:38.366183 14815 net.cpp:561] out5a_up2 <- out5a
I0815 22:42:38.366186 14815 net.cpp:530] out5a_up2 -> out5a_up2
I0815 22:42:38.366602 14815 net.cpp:245] Setting up out5a_up2
I0815 22:42:38.366611 14815 net.cpp:252] TRAIN Top shape for layer 40 'out5a_up2' 6 64 80 80 (2457600)
I0815 22:42:38.366616 14815 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I0815 22:42:38.366619 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.366636 14815 net.cpp:184] Created Layer out3a (41)
I0815 22:42:38.366638 14815 net.cpp:561] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0815 22:42:38.366643 14815 net.cpp:530] out3a -> out3a
I0815 22:42:38.378798 14815 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 7.3G, req 0G)
I0815 22:42:38.378809 14815 net.cpp:245] Setting up out3a
I0815 22:42:38.378815 14815 net.cpp:252] TRAIN Top shape for layer 41 'out3a' 6 64 80 80 (2457600)
I0815 22:42:38.378821 14815 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I0815 22:42:38.378824 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.378830 14815 net.cpp:184] Created Layer out3a/bn (42)
I0815 22:42:38.378834 14815 net.cpp:561] out3a/bn <- out3a
I0815 22:42:38.378839 14815 net.cpp:513] out3a/bn -> out3a (in-place)
I0815 22:42:38.379775 14815 net.cpp:245] Setting up out3a/bn
I0815 22:42:38.379784 14815 net.cpp:252] TRAIN Top shape for layer 42 'out3a/bn' 6 64 80 80 (2457600)
I0815 22:42:38.379792 14815 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I0815 22:42:38.379796 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.379801 14815 net.cpp:184] Created Layer out3a/relu (43)
I0815 22:42:38.379804 14815 net.cpp:561] out3a/relu <- out3a
I0815 22:42:38.379807 14815 net.cpp:513] out3a/relu -> out3a (in-place)
I0815 22:42:38.379813 14815 net.cpp:245] Setting up out3a/relu
I0815 22:42:38.379817 14815 net.cpp:252] TRAIN Top shape for layer 43 'out3a/relu' 6 64 80 80 (2457600)
I0815 22:42:38.379822 14815 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I0815 22:42:38.379825 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.395547 14815 net.cpp:184] Created Layer out3_out5_combined (44)
I0815 22:42:38.395566 14815 net.cpp:561] out3_out5_combined <- out5a_up2
I0815 22:42:38.395572 14815 net.cpp:561] out3_out5_combined <- out3a
I0815 22:42:38.395576 14815 net.cpp:530] out3_out5_combined -> out3_out5_combined
I0815 22:42:38.397056 14815 net.cpp:245] Setting up out3_out5_combined
I0815 22:42:38.397069 14815 net.cpp:252] TRAIN Top shape for layer 44 'out3_out5_combined' 6 64 80 80 (2457600)
I0815 22:42:38.397075 14815 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I0815 22:42:38.397080 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.397099 14815 net.cpp:184] Created Layer ctx_conv1 (45)
I0815 22:42:38.397104 14815 net.cpp:561] ctx_conv1 <- out3_out5_combined
I0815 22:42:38.397109 14815 net.cpp:530] ctx_conv1 -> ctx_conv1
I0815 22:42:38.411305 14815 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 7.25G, req 0G)
I0815 22:42:38.411327 14815 net.cpp:245] Setting up ctx_conv1
I0815 22:42:38.411332 14815 net.cpp:252] TRAIN Top shape for layer 45 'ctx_conv1' 6 64 80 80 (2457600)
I0815 22:42:38.411339 14815 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I0815 22:42:38.411343 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.411350 14815 net.cpp:184] Created Layer ctx_conv1/bn (46)
I0815 22:42:38.411353 14815 net.cpp:561] ctx_conv1/bn <- ctx_conv1
I0815 22:42:38.411357 14815 net.cpp:513] ctx_conv1/bn -> ctx_conv1 (in-place)
I0815 22:42:38.412258 14815 net.cpp:245] Setting up ctx_conv1/bn
I0815 22:42:38.412267 14815 net.cpp:252] TRAIN Top shape for layer 46 'ctx_conv1/bn' 6 64 80 80 (2457600)
I0815 22:42:38.412276 14815 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I0815 22:42:38.412279 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.412284 14815 net.cpp:184] Created Layer ctx_conv1/relu (47)
I0815 22:42:38.412287 14815 net.cpp:561] ctx_conv1/relu <- ctx_conv1
I0815 22:42:38.412292 14815 net.cpp:513] ctx_conv1/relu -> ctx_conv1 (in-place)
I0815 22:42:38.412297 14815 net.cpp:245] Setting up ctx_conv1/relu
I0815 22:42:38.412300 14815 net.cpp:252] TRAIN Top shape for layer 47 'ctx_conv1/relu' 6 64 80 80 (2457600)
I0815 22:42:38.412305 14815 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I0815 22:42:38.412309 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.412317 14815 net.cpp:184] Created Layer ctx_conv2 (48)
I0815 22:42:38.412322 14815 net.cpp:561] ctx_conv2 <- ctx_conv1
I0815 22:42:38.412324 14815 net.cpp:530] ctx_conv2 -> ctx_conv2
I0815 22:42:38.413818 14815 net.cpp:245] Setting up ctx_conv2
I0815 22:42:38.413827 14815 net.cpp:252] TRAIN Top shape for layer 48 'ctx_conv2' 6 64 80 80 (2457600)
I0815 22:42:38.413833 14815 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I0815 22:42:38.413836 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.413841 14815 net.cpp:184] Created Layer ctx_conv2/bn (49)
I0815 22:42:38.413846 14815 net.cpp:561] ctx_conv2/bn <- ctx_conv2
I0815 22:42:38.413848 14815 net.cpp:513] ctx_conv2/bn -> ctx_conv2 (in-place)
I0815 22:42:38.414758 14815 net.cpp:245] Setting up ctx_conv2/bn
I0815 22:42:38.414767 14815 net.cpp:252] TRAIN Top shape for layer 49 'ctx_conv2/bn' 6 64 80 80 (2457600)
I0815 22:42:38.414774 14815 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I0815 22:42:38.414778 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.414783 14815 net.cpp:184] Created Layer ctx_conv2/relu (50)
I0815 22:42:38.414786 14815 net.cpp:561] ctx_conv2/relu <- ctx_conv2
I0815 22:42:38.414789 14815 net.cpp:513] ctx_conv2/relu -> ctx_conv2 (in-place)
I0815 22:42:38.414794 14815 net.cpp:245] Setting up ctx_conv2/relu
I0815 22:42:38.414798 14815 net.cpp:252] TRAIN Top shape for layer 50 'ctx_conv2/relu' 6 64 80 80 (2457600)
I0815 22:42:38.414803 14815 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I0815 22:42:38.414806 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.414813 14815 net.cpp:184] Created Layer ctx_conv3 (51)
I0815 22:42:38.414816 14815 net.cpp:561] ctx_conv3 <- ctx_conv2
I0815 22:42:38.414819 14815 net.cpp:530] ctx_conv3 -> ctx_conv3
I0815 22:42:38.416352 14815 net.cpp:245] Setting up ctx_conv3
I0815 22:42:38.416368 14815 net.cpp:252] TRAIN Top shape for layer 51 'ctx_conv3' 6 64 80 80 (2457600)
I0815 22:42:38.416375 14815 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I0815 22:42:38.416380 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.416388 14815 net.cpp:184] Created Layer ctx_conv3/bn (52)
I0815 22:42:38.416407 14815 net.cpp:561] ctx_conv3/bn <- ctx_conv3
I0815 22:42:38.416414 14815 net.cpp:513] ctx_conv3/bn -> ctx_conv3 (in-place)
I0815 22:42:38.417325 14815 net.cpp:245] Setting up ctx_conv3/bn
I0815 22:42:38.417333 14815 net.cpp:252] TRAIN Top shape for layer 52 'ctx_conv3/bn' 6 64 80 80 (2457600)
I0815 22:42:38.417342 14815 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I0815 22:42:38.417346 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.417351 14815 net.cpp:184] Created Layer ctx_conv3/relu (53)
I0815 22:42:38.417356 14815 net.cpp:561] ctx_conv3/relu <- ctx_conv3
I0815 22:42:38.417359 14815 net.cpp:513] ctx_conv3/relu -> ctx_conv3 (in-place)
I0815 22:42:38.417366 14815 net.cpp:245] Setting up ctx_conv3/relu
I0815 22:42:38.417371 14815 net.cpp:252] TRAIN Top shape for layer 53 'ctx_conv3/relu' 6 64 80 80 (2457600)
I0815 22:42:38.417373 14815 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I0815 22:42:38.417378 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.417392 14815 net.cpp:184] Created Layer ctx_conv4 (54)
I0815 22:42:38.417397 14815 net.cpp:561] ctx_conv4 <- ctx_conv3
I0815 22:42:38.417402 14815 net.cpp:530] ctx_conv4 -> ctx_conv4
I0815 22:42:38.418908 14815 net.cpp:245] Setting up ctx_conv4
I0815 22:42:38.418917 14815 net.cpp:252] TRAIN Top shape for layer 54 'ctx_conv4' 6 64 80 80 (2457600)
I0815 22:42:38.418923 14815 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I0815 22:42:38.418928 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.418944 14815 net.cpp:184] Created Layer ctx_conv4/bn (55)
I0815 22:42:38.418949 14815 net.cpp:561] ctx_conv4/bn <- ctx_conv4
I0815 22:42:38.418953 14815 net.cpp:513] ctx_conv4/bn -> ctx_conv4 (in-place)
I0815 22:42:38.419853 14815 net.cpp:245] Setting up ctx_conv4/bn
I0815 22:42:38.419862 14815 net.cpp:252] TRAIN Top shape for layer 55 'ctx_conv4/bn' 6 64 80 80 (2457600)
I0815 22:42:38.419870 14815 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I0815 22:42:38.419874 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.419885 14815 net.cpp:184] Created Layer ctx_conv4/relu (56)
I0815 22:42:38.419889 14815 net.cpp:561] ctx_conv4/relu <- ctx_conv4
I0815 22:42:38.419893 14815 net.cpp:513] ctx_conv4/relu -> ctx_conv4 (in-place)
I0815 22:42:38.419899 14815 net.cpp:245] Setting up ctx_conv4/relu
I0815 22:42:38.419903 14815 net.cpp:252] TRAIN Top shape for layer 56 'ctx_conv4/relu' 6 64 80 80 (2457600)
I0815 22:42:38.419908 14815 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I0815 22:42:38.419911 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.419919 14815 net.cpp:184] Created Layer ctx_final (57)
I0815 22:42:38.419924 14815 net.cpp:561] ctx_final <- ctx_conv4
I0815 22:42:38.419927 14815 net.cpp:530] ctx_final -> ctx_final
I0815 22:42:38.433146 14815 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 7.22G, req 0G)
I0815 22:42:38.433161 14815 net.cpp:245] Setting up ctx_final
I0815 22:42:38.433166 14815 net.cpp:252] TRAIN Top shape for layer 57 'ctx_final' 6 8 80 80 (307200)
I0815 22:42:38.433171 14815 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I0815 22:42:38.433174 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.433178 14815 net.cpp:184] Created Layer ctx_final/relu (58)
I0815 22:42:38.433182 14815 net.cpp:561] ctx_final/relu <- ctx_final
I0815 22:42:38.433183 14815 net.cpp:513] ctx_final/relu -> ctx_final (in-place)
I0815 22:42:38.433188 14815 net.cpp:245] Setting up ctx_final/relu
I0815 22:42:38.433192 14815 net.cpp:252] TRAIN Top shape for layer 58 'ctx_final/relu' 6 8 80 80 (307200)
I0815 22:42:38.433194 14815 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I0815 22:42:38.433209 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.433215 14815 net.cpp:184] Created Layer out_deconv_final_up2 (59)
I0815 22:42:38.433218 14815 net.cpp:561] out_deconv_final_up2 <- ctx_final
I0815 22:42:38.433220 14815 net.cpp:530] out_deconv_final_up2 -> out_deconv_final_up2
I0815 22:42:38.433555 14815 net.cpp:245] Setting up out_deconv_final_up2
I0815 22:42:38.433562 14815 net.cpp:252] TRAIN Top shape for layer 59 'out_deconv_final_up2' 6 8 160 160 (1228800)
I0815 22:42:38.433567 14815 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I0815 22:42:38.433568 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.433576 14815 net.cpp:184] Created Layer out_deconv_final_up4 (60)
I0815 22:42:38.433578 14815 net.cpp:561] out_deconv_final_up4 <- out_deconv_final_up2
I0815 22:42:38.433581 14815 net.cpp:530] out_deconv_final_up4 -> out_deconv_final_up4
I0815 22:42:38.433857 14815 net.cpp:245] Setting up out_deconv_final_up4
I0815 22:42:38.433863 14815 net.cpp:252] TRAIN Top shape for layer 60 'out_deconv_final_up4' 6 8 320 320 (4915200)
I0815 22:42:38.433866 14815 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I0815 22:42:38.433868 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.433873 14815 net.cpp:184] Created Layer out_deconv_final_up8 (61)
I0815 22:42:38.433876 14815 net.cpp:561] out_deconv_final_up8 <- out_deconv_final_up4
I0815 22:42:38.433878 14815 net.cpp:530] out_deconv_final_up8 -> out_deconv_final_up8
I0815 22:42:38.434152 14815 net.cpp:245] Setting up out_deconv_final_up8
I0815 22:42:38.434157 14815 net.cpp:252] TRAIN Top shape for layer 61 'out_deconv_final_up8' 6 8 640 640 (19660800)
I0815 22:42:38.434160 14815 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0815 22:42:38.434164 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.434175 14815 net.cpp:184] Created Layer loss (62)
I0815 22:42:38.434177 14815 net.cpp:561] loss <- out_deconv_final_up8
I0815 22:42:38.434180 14815 net.cpp:561] loss <- label
I0815 22:42:38.434182 14815 net.cpp:530] loss -> loss
I0815 22:42:38.435469 14815 net.cpp:245] Setting up loss
I0815 22:42:38.435477 14815 net.cpp:252] TRAIN Top shape for layer 62 'loss' (1)
I0815 22:42:38.435479 14815 net.cpp:256]     with loss weight 1
I0815 22:42:38.435483 14815 net.cpp:323] loss needs backward computation.
I0815 22:42:38.435487 14815 net.cpp:323] out_deconv_final_up8 needs backward computation.
I0815 22:42:38.435488 14815 net.cpp:323] out_deconv_final_up4 needs backward computation.
I0815 22:42:38.435489 14815 net.cpp:323] out_deconv_final_up2 needs backward computation.
I0815 22:42:38.435492 14815 net.cpp:323] ctx_final/relu needs backward computation.
I0815 22:42:38.435493 14815 net.cpp:323] ctx_final needs backward computation.
I0815 22:42:38.435495 14815 net.cpp:323] ctx_conv4/relu needs backward computation.
I0815 22:42:38.435497 14815 net.cpp:323] ctx_conv4/bn needs backward computation.
I0815 22:42:38.435498 14815 net.cpp:323] ctx_conv4 needs backward computation.
I0815 22:42:38.435500 14815 net.cpp:323] ctx_conv3/relu needs backward computation.
I0815 22:42:38.435503 14815 net.cpp:323] ctx_conv3/bn needs backward computation.
I0815 22:42:38.435504 14815 net.cpp:323] ctx_conv3 needs backward computation.
I0815 22:42:38.435506 14815 net.cpp:323] ctx_conv2/relu needs backward computation.
I0815 22:42:38.435508 14815 net.cpp:323] ctx_conv2/bn needs backward computation.
I0815 22:42:38.435510 14815 net.cpp:323] ctx_conv2 needs backward computation.
I0815 22:42:38.435513 14815 net.cpp:323] ctx_conv1/relu needs backward computation.
I0815 22:42:38.435514 14815 net.cpp:323] ctx_conv1/bn needs backward computation.
I0815 22:42:38.435518 14815 net.cpp:323] ctx_conv1 needs backward computation.
I0815 22:42:38.435525 14815 net.cpp:323] out3_out5_combined needs backward computation.
I0815 22:42:38.435528 14815 net.cpp:323] out3a/relu needs backward computation.
I0815 22:42:38.435529 14815 net.cpp:323] out3a/bn needs backward computation.
I0815 22:42:38.435531 14815 net.cpp:323] out3a needs backward computation.
I0815 22:42:38.435534 14815 net.cpp:323] out5a_up2 needs backward computation.
I0815 22:42:38.435536 14815 net.cpp:323] out5a/relu needs backward computation.
I0815 22:42:38.435539 14815 net.cpp:323] out5a/bn needs backward computation.
I0815 22:42:38.435540 14815 net.cpp:323] out5a needs backward computation.
I0815 22:42:38.435544 14815 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0815 22:42:38.435546 14815 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0815 22:42:38.435549 14815 net.cpp:323] res5a_branch2b needs backward computation.
I0815 22:42:38.435552 14815 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0815 22:42:38.435555 14815 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0815 22:42:38.435559 14815 net.cpp:323] res5a_branch2a needs backward computation.
I0815 22:42:38.435564 14815 net.cpp:323] pool4 needs backward computation.
I0815 22:42:38.435565 14815 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0815 22:42:38.435567 14815 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0815 22:42:38.435570 14815 net.cpp:323] res4a_branch2b needs backward computation.
I0815 22:42:38.435572 14815 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0815 22:42:38.435573 14815 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0815 22:42:38.435575 14815 net.cpp:323] res4a_branch2a needs backward computation.
I0815 22:42:38.435577 14815 net.cpp:323] pool3 needs backward computation.
I0815 22:42:38.435580 14815 net.cpp:323] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I0815 22:42:38.435582 14815 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0815 22:42:38.435585 14815 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0815 22:42:38.435586 14815 net.cpp:323] res3a_branch2b needs backward computation.
I0815 22:42:38.435588 14815 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0815 22:42:38.435591 14815 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0815 22:42:38.435593 14815 net.cpp:323] res3a_branch2a needs backward computation.
I0815 22:42:38.435595 14815 net.cpp:323] pool2 needs backward computation.
I0815 22:42:38.435597 14815 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0815 22:42:38.435600 14815 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0815 22:42:38.435602 14815 net.cpp:323] res2a_branch2b needs backward computation.
I0815 22:42:38.435603 14815 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0815 22:42:38.435606 14815 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0815 22:42:38.435607 14815 net.cpp:323] res2a_branch2a needs backward computation.
I0815 22:42:38.435609 14815 net.cpp:323] pool1 needs backward computation.
I0815 22:42:38.435612 14815 net.cpp:323] conv1b/relu needs backward computation.
I0815 22:42:38.435614 14815 net.cpp:323] conv1b/bn needs backward computation.
I0815 22:42:38.435616 14815 net.cpp:323] conv1b needs backward computation.
I0815 22:42:38.435618 14815 net.cpp:323] conv1a/relu needs backward computation.
I0815 22:42:38.435621 14815 net.cpp:323] conv1a/bn needs backward computation.
I0815 22:42:38.435623 14815 net.cpp:323] conv1a needs backward computation.
I0815 22:42:38.435626 14815 net.cpp:325] data/bias does not need backward computation.
I0815 22:42:38.435628 14815 net.cpp:325] data does not need backward computation.
I0815 22:42:38.435631 14815 net.cpp:367] This network produces output loss
I0815 22:42:38.435678 14815 net.cpp:389] Top memory (TRAIN) required for data: 956006400 diff: 946176008
I0815 22:42:38.435681 14815 net.cpp:392] Bottom memory (TRAIN) required for data: 956006400 diff: 956006400
I0815 22:42:38.435683 14815 net.cpp:395] Shared (in-place) memory (TRAIN) by data: 630374400 diff: 630374400
I0815 22:42:38.435689 14815 net.cpp:398] Parameters memory (TRAIN) required for data: 2692608 diff: 2692608
I0815 22:42:38.435691 14815 net.cpp:401] Parameters shared memory (TRAIN) by data: 0 diff: 0
I0815 22:42:38.435693 14815 net.cpp:407] Network initialization done.
I0815 22:42:38.448155 14815 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/sparse/test.prototxt
W0815 22:42:38.448247 14815 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0815 22:42:38.448547 14815 net.cpp:72] Initializing net from parameters: 
name: "jsegnet21v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/val-image-lmdb"
    label_list_path: "data/val-label-lmdb"
    batch_size: 2
    threads: 1
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
  accuracy_param {
    ignore_label: 255
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
    ignore_label: 255
  }
}
I0815 22:42:38.448760 14815 net.cpp:104] Using FLOAT as default forward math type
I0815 22:42:38.448766 14815 net.cpp:110] Using FLOAT as default backward math type
I0815 22:42:38.448776 14815 layer_factory.hpp:136] Creating layer 'data' of type 'ImageLabelData'
I0815 22:42:38.448779 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.448786 14815 net.cpp:184] Created Layer data (0)
I0815 22:42:38.448789 14815 net.cpp:530] data -> data
I0815 22:42:38.448793 14815 net.cpp:530] data -> label
I0815 22:42:38.448817 14815 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0815 22:42:38.448823 14815 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 22:42:38.450192 14888 db_lmdb.cpp:24] Opened lmdb data/val-image-lmdb
I0815 22:42:38.451623 14815 data_layer.cpp:185] (0) ReshapePrefetch 2, 3, 640, 640
I0815 22:42:38.451696 14815 data_layer.cpp:209] (0) Output data size: 2, 3, 640, 640
I0815 22:42:38.451704 14815 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 22:42:38.451758 14815 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0815 22:42:38.451771 14815 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 22:42:38.452522 14889 data_layer.cpp:97] (0) Parser threads: 1
I0815 22:42:38.452536 14889 data_layer.cpp:99] (0) Transformer threads: 1
I0815 22:42:38.454927 14890 db_lmdb.cpp:24] Opened lmdb data/val-label-lmdb
I0815 22:42:38.456228 14815 data_layer.cpp:185] (0) ReshapePrefetch 2, 1, 640, 640
I0815 22:42:38.456341 14815 data_layer.cpp:209] (0) Output data size: 2, 1, 640, 640
I0815 22:42:38.456348 14815 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 22:42:38.456400 14815 net.cpp:245] Setting up data
I0815 22:42:38.456411 14815 net.cpp:252] TEST Top shape for layer 0 'data' 2 3 640 640 (2457600)
I0815 22:42:38.456418 14815 net.cpp:252] TEST Top shape for layer 0 'data' 2 1 640 640 (819200)
I0815 22:42:38.456425 14815 layer_factory.hpp:136] Creating layer 'label_data_1_split' of type 'Split'
I0815 22:42:38.456446 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.456459 14815 net.cpp:184] Created Layer label_data_1_split (1)
I0815 22:42:38.456463 14815 net.cpp:561] label_data_1_split <- label
I0815 22:42:38.456470 14815 net.cpp:530] label_data_1_split -> label_data_1_split_0
I0815 22:42:38.456478 14815 net.cpp:530] label_data_1_split -> label_data_1_split_1
I0815 22:42:38.456483 14815 net.cpp:530] label_data_1_split -> label_data_1_split_2
I0815 22:42:38.456605 14815 net.cpp:245] Setting up label_data_1_split
I0815 22:42:38.456612 14815 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0815 22:42:38.456617 14815 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0815 22:42:38.456622 14815 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0815 22:42:38.456627 14815 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0815 22:42:38.456638 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.456646 14815 net.cpp:184] Created Layer data/bias (2)
I0815 22:42:38.456660 14815 net.cpp:561] data/bias <- data
I0815 22:42:38.456670 14815 net.cpp:530] data/bias -> data/bias
I0815 22:42:38.457923 14891 data_layer.cpp:97] (0) Parser threads: 1
I0815 22:42:38.457934 14891 data_layer.cpp:99] (0) Transformer threads: 1
I0815 22:42:38.459810 14815 net.cpp:245] Setting up data/bias
I0815 22:42:38.459831 14815 net.cpp:252] TEST Top shape for layer 2 'data/bias' 2 3 640 640 (2457600)
I0815 22:42:38.459846 14815 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0815 22:42:38.459852 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.459877 14815 net.cpp:184] Created Layer conv1a (3)
I0815 22:42:38.459893 14815 net.cpp:561] conv1a <- data/bias
I0815 22:42:38.459899 14815 net.cpp:530] conv1a -> conv1a
I0815 22:42:38.466639 14815 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.09G, req 0G)
I0815 22:42:38.466678 14815 net.cpp:245] Setting up conv1a
I0815 22:42:38.466686 14815 net.cpp:252] TEST Top shape for layer 3 'conv1a' 2 32 320 320 (6553600)
I0815 22:42:38.466698 14815 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0815 22:42:38.466706 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.466722 14815 net.cpp:184] Created Layer conv1a/bn (4)
I0815 22:42:38.466732 14815 net.cpp:561] conv1a/bn <- conv1a
I0815 22:42:38.466737 14815 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0815 22:42:38.467732 14815 net.cpp:245] Setting up conv1a/bn
I0815 22:42:38.467742 14815 net.cpp:252] TEST Top shape for layer 4 'conv1a/bn' 2 32 320 320 (6553600)
I0815 22:42:38.467752 14815 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0815 22:42:38.467756 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.467761 14815 net.cpp:184] Created Layer conv1a/relu (5)
I0815 22:42:38.467763 14815 net.cpp:561] conv1a/relu <- conv1a
I0815 22:42:38.467767 14815 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0815 22:42:38.467773 14815 net.cpp:245] Setting up conv1a/relu
I0815 22:42:38.467777 14815 net.cpp:252] TEST Top shape for layer 5 'conv1a/relu' 2 32 320 320 (6553600)
I0815 22:42:38.467782 14815 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0815 22:42:38.467784 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.467794 14815 net.cpp:184] Created Layer conv1b (6)
I0815 22:42:38.467798 14815 net.cpp:561] conv1b <- conv1a
I0815 22:42:38.467803 14815 net.cpp:530] conv1b -> conv1b
I0815 22:42:38.481931 14815 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.06G, req 0G)
I0815 22:42:38.481945 14815 net.cpp:245] Setting up conv1b
I0815 22:42:38.481951 14815 net.cpp:252] TEST Top shape for layer 6 'conv1b' 2 32 320 320 (6553600)
I0815 22:42:38.481959 14815 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0815 22:42:38.481964 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.481971 14815 net.cpp:184] Created Layer conv1b/bn (7)
I0815 22:42:38.481976 14815 net.cpp:561] conv1b/bn <- conv1b
I0815 22:42:38.481979 14815 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0815 22:42:38.482906 14815 net.cpp:245] Setting up conv1b/bn
I0815 22:42:38.482915 14815 net.cpp:252] TEST Top shape for layer 7 'conv1b/bn' 2 32 320 320 (6553600)
I0815 22:42:38.482924 14815 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0815 22:42:38.482928 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.482937 14815 net.cpp:184] Created Layer conv1b/relu (8)
I0815 22:42:38.482941 14815 net.cpp:561] conv1b/relu <- conv1b
I0815 22:42:38.482945 14815 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0815 22:42:38.482950 14815 net.cpp:245] Setting up conv1b/relu
I0815 22:42:38.482955 14815 net.cpp:252] TEST Top shape for layer 8 'conv1b/relu' 2 32 320 320 (6553600)
I0815 22:42:38.482959 14815 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0815 22:42:38.482962 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.482969 14815 net.cpp:184] Created Layer pool1 (9)
I0815 22:42:38.482973 14815 net.cpp:561] pool1 <- conv1b
I0815 22:42:38.482976 14815 net.cpp:530] pool1 -> pool1
I0815 22:42:38.483062 14815 net.cpp:245] Setting up pool1
I0815 22:42:38.483068 14815 net.cpp:252] TEST Top shape for layer 9 'pool1' 2 32 160 160 (1638400)
I0815 22:42:38.483072 14815 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0815 22:42:38.483077 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.483085 14815 net.cpp:184] Created Layer res2a_branch2a (10)
I0815 22:42:38.483090 14815 net.cpp:561] res2a_branch2a <- pool1
I0815 22:42:38.483103 14815 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0815 22:42:38.491924 14815 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.03G, req 0G)
I0815 22:42:38.491935 14815 net.cpp:245] Setting up res2a_branch2a
I0815 22:42:38.491940 14815 net.cpp:252] TEST Top shape for layer 10 'res2a_branch2a' 2 64 160 160 (3276800)
I0815 22:42:38.491948 14815 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0815 22:42:38.491952 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.491958 14815 net.cpp:184] Created Layer res2a_branch2a/bn (11)
I0815 22:42:38.491962 14815 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0815 22:42:38.491966 14815 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0815 22:42:38.492897 14815 net.cpp:245] Setting up res2a_branch2a/bn
I0815 22:42:38.492907 14815 net.cpp:252] TEST Top shape for layer 11 'res2a_branch2a/bn' 2 64 160 160 (3276800)
I0815 22:42:38.492914 14815 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0815 22:42:38.492918 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.492923 14815 net.cpp:184] Created Layer res2a_branch2a/relu (12)
I0815 22:42:38.492926 14815 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0815 22:42:38.492930 14815 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0815 22:42:38.492935 14815 net.cpp:245] Setting up res2a_branch2a/relu
I0815 22:42:38.492939 14815 net.cpp:252] TEST Top shape for layer 12 'res2a_branch2a/relu' 2 64 160 160 (3276800)
I0815 22:42:38.492944 14815 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0815 22:42:38.492946 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.492954 14815 net.cpp:184] Created Layer res2a_branch2b (13)
I0815 22:42:38.492959 14815 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0815 22:42:38.492961 14815 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0815 22:42:38.499900 14815 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.02G, req 0G)
I0815 22:42:38.499910 14815 net.cpp:245] Setting up res2a_branch2b
I0815 22:42:38.499917 14815 net.cpp:252] TEST Top shape for layer 13 'res2a_branch2b' 2 64 160 160 (3276800)
I0815 22:42:38.499922 14815 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0815 22:42:38.499927 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.499933 14815 net.cpp:184] Created Layer res2a_branch2b/bn (14)
I0815 22:42:38.499938 14815 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0815 22:42:38.499941 14815 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0815 22:42:38.500844 14815 net.cpp:245] Setting up res2a_branch2b/bn
I0815 22:42:38.500852 14815 net.cpp:252] TEST Top shape for layer 14 'res2a_branch2b/bn' 2 64 160 160 (3276800)
I0815 22:42:38.500861 14815 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0815 22:42:38.500865 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.500870 14815 net.cpp:184] Created Layer res2a_branch2b/relu (15)
I0815 22:42:38.500874 14815 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0815 22:42:38.500879 14815 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0815 22:42:38.500885 14815 net.cpp:245] Setting up res2a_branch2b/relu
I0815 22:42:38.500888 14815 net.cpp:252] TEST Top shape for layer 15 'res2a_branch2b/relu' 2 64 160 160 (3276800)
I0815 22:42:38.500892 14815 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0815 22:42:38.500896 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.500902 14815 net.cpp:184] Created Layer pool2 (16)
I0815 22:42:38.500906 14815 net.cpp:561] pool2 <- res2a_branch2b
I0815 22:42:38.500918 14815 net.cpp:530] pool2 -> pool2
I0815 22:42:38.501010 14815 net.cpp:245] Setting up pool2
I0815 22:42:38.501016 14815 net.cpp:252] TEST Top shape for layer 16 'pool2' 2 64 80 80 (819200)
I0815 22:42:38.501021 14815 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0815 22:42:38.501025 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.501034 14815 net.cpp:184] Created Layer res3a_branch2a (17)
I0815 22:42:38.501039 14815 net.cpp:561] res3a_branch2a <- pool2
I0815 22:42:38.501042 14815 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0815 22:42:38.507601 14815 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.01G, req 0G)
I0815 22:42:38.507614 14815 net.cpp:245] Setting up res3a_branch2a
I0815 22:42:38.507621 14815 net.cpp:252] TEST Top shape for layer 17 'res3a_branch2a' 2 128 80 80 (1638400)
I0815 22:42:38.507627 14815 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0815 22:42:38.507632 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.507638 14815 net.cpp:184] Created Layer res3a_branch2a/bn (18)
I0815 22:42:38.507642 14815 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0815 22:42:38.507647 14815 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0815 22:42:38.508569 14815 net.cpp:245] Setting up res3a_branch2a/bn
I0815 22:42:38.508579 14815 net.cpp:252] TEST Top shape for layer 18 'res3a_branch2a/bn' 2 128 80 80 (1638400)
I0815 22:42:38.508591 14815 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0815 22:42:38.508595 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.508600 14815 net.cpp:184] Created Layer res3a_branch2a/relu (19)
I0815 22:42:38.508604 14815 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0815 22:42:38.508607 14815 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0815 22:42:38.508612 14815 net.cpp:245] Setting up res3a_branch2a/relu
I0815 22:42:38.508617 14815 net.cpp:252] TEST Top shape for layer 19 'res3a_branch2a/relu' 2 128 80 80 (1638400)
I0815 22:42:38.508621 14815 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0815 22:42:38.508625 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.508638 14815 net.cpp:184] Created Layer res3a_branch2b (20)
I0815 22:42:38.508642 14815 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0815 22:42:38.508646 14815 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0815 22:42:38.514272 14815 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7G, req 0G)
I0815 22:42:38.514287 14815 net.cpp:245] Setting up res3a_branch2b
I0815 22:42:38.514293 14815 net.cpp:252] TEST Top shape for layer 20 'res3a_branch2b' 2 128 80 80 (1638400)
I0815 22:42:38.514300 14815 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0815 22:42:38.514304 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.514312 14815 net.cpp:184] Created Layer res3a_branch2b/bn (21)
I0815 22:42:38.514317 14815 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0815 22:42:38.514320 14815 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0815 22:42:38.515285 14815 net.cpp:245] Setting up res3a_branch2b/bn
I0815 22:42:38.515296 14815 net.cpp:252] TEST Top shape for layer 21 'res3a_branch2b/bn' 2 128 80 80 (1638400)
I0815 22:42:38.515305 14815 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0815 22:42:38.515310 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.515314 14815 net.cpp:184] Created Layer res3a_branch2b/relu (22)
I0815 22:42:38.515318 14815 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0815 22:42:38.515322 14815 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0815 22:42:38.515338 14815 net.cpp:245] Setting up res3a_branch2b/relu
I0815 22:42:38.515343 14815 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b/relu' 2 128 80 80 (1638400)
I0815 22:42:38.515347 14815 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0815 22:42:38.515349 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.515354 14815 net.cpp:184] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (23)
I0815 22:42:38.515357 14815 net.cpp:561] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0815 22:42:38.515360 14815 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0815 22:42:38.515367 14815 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0815 22:42:38.515430 14815 net.cpp:245] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0815 22:42:38.515436 14815 net.cpp:252] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 2 128 80 80 (1638400)
I0815 22:42:38.515441 14815 net.cpp:252] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 2 128 80 80 (1638400)
I0815 22:42:38.515450 14815 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0815 22:42:38.515455 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.515461 14815 net.cpp:184] Created Layer pool3 (24)
I0815 22:42:38.515465 14815 net.cpp:561] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0815 22:42:38.515470 14815 net.cpp:530] pool3 -> pool3
I0815 22:42:38.515555 14815 net.cpp:245] Setting up pool3
I0815 22:42:38.515561 14815 net.cpp:252] TEST Top shape for layer 24 'pool3' 2 128 40 40 (409600)
I0815 22:42:38.515566 14815 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0815 22:42:38.515570 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.515662 14815 net.cpp:184] Created Layer res4a_branch2a (25)
I0815 22:42:38.515667 14815 net.cpp:561] res4a_branch2a <- pool3
I0815 22:42:38.515672 14815 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0815 22:42:38.529551 14815 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.99G, req 0G)
I0815 22:42:38.529563 14815 net.cpp:245] Setting up res4a_branch2a
I0815 22:42:38.529568 14815 net.cpp:252] TEST Top shape for layer 25 'res4a_branch2a' 2 256 40 40 (819200)
I0815 22:42:38.529575 14815 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0815 22:42:38.529580 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.529593 14815 net.cpp:184] Created Layer res4a_branch2a/bn (26)
I0815 22:42:38.529597 14815 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0815 22:42:38.529602 14815 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0815 22:42:38.530496 14815 net.cpp:245] Setting up res4a_branch2a/bn
I0815 22:42:38.530505 14815 net.cpp:252] TEST Top shape for layer 26 'res4a_branch2a/bn' 2 256 40 40 (819200)
I0815 22:42:38.530514 14815 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0815 22:42:38.530517 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.530521 14815 net.cpp:184] Created Layer res4a_branch2a/relu (27)
I0815 22:42:38.530525 14815 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0815 22:42:38.530529 14815 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0815 22:42:38.530534 14815 net.cpp:245] Setting up res4a_branch2a/relu
I0815 22:42:38.530539 14815 net.cpp:252] TEST Top shape for layer 27 'res4a_branch2a/relu' 2 256 40 40 (819200)
I0815 22:42:38.530542 14815 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0815 22:42:38.530546 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.530565 14815 net.cpp:184] Created Layer res4a_branch2b (28)
I0815 22:42:38.530568 14815 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0815 22:42:38.530571 14815 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0815 22:42:38.538686 14815 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.98G, req 0G)
I0815 22:42:38.538699 14815 net.cpp:245] Setting up res4a_branch2b
I0815 22:42:38.538705 14815 net.cpp:252] TEST Top shape for layer 28 'res4a_branch2b' 2 256 40 40 (819200)
I0815 22:42:38.538712 14815 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0815 22:42:38.538717 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.538730 14815 net.cpp:184] Created Layer res4a_branch2b/bn (29)
I0815 22:42:38.538734 14815 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0815 22:42:38.538748 14815 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0815 22:42:38.539665 14815 net.cpp:245] Setting up res4a_branch2b/bn
I0815 22:42:38.539674 14815 net.cpp:252] TEST Top shape for layer 29 'res4a_branch2b/bn' 2 256 40 40 (819200)
I0815 22:42:38.539682 14815 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0815 22:42:38.539686 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.539691 14815 net.cpp:184] Created Layer res4a_branch2b/relu (30)
I0815 22:42:38.539695 14815 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0815 22:42:38.539698 14815 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0815 22:42:38.539703 14815 net.cpp:245] Setting up res4a_branch2b/relu
I0815 22:42:38.539708 14815 net.cpp:252] TEST Top shape for layer 30 'res4a_branch2b/relu' 2 256 40 40 (819200)
I0815 22:42:38.539712 14815 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0815 22:42:38.539717 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.539723 14815 net.cpp:184] Created Layer pool4 (31)
I0815 22:42:38.539726 14815 net.cpp:561] pool4 <- res4a_branch2b
I0815 22:42:38.539731 14815 net.cpp:530] pool4 -> pool4
I0815 22:42:38.539818 14815 net.cpp:245] Setting up pool4
I0815 22:42:38.539824 14815 net.cpp:252] TEST Top shape for layer 31 'pool4' 2 256 40 40 (819200)
I0815 22:42:38.539827 14815 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0815 22:42:38.539831 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.539846 14815 net.cpp:184] Created Layer res5a_branch2a (32)
I0815 22:42:38.539851 14815 net.cpp:561] res5a_branch2a <- pool4
I0815 22:42:38.539855 14815 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0815 22:42:38.573050 14815 net.cpp:245] Setting up res5a_branch2a
I0815 22:42:38.573079 14815 net.cpp:252] TEST Top shape for layer 32 'res5a_branch2a' 2 512 40 40 (1638400)
I0815 22:42:38.573088 14815 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0815 22:42:38.573093 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.573103 14815 net.cpp:184] Created Layer res5a_branch2a/bn (33)
I0815 22:42:38.573108 14815 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0815 22:42:38.573112 14815 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0815 22:42:38.573984 14815 net.cpp:245] Setting up res5a_branch2a/bn
I0815 22:42:38.573994 14815 net.cpp:252] TEST Top shape for layer 33 'res5a_branch2a/bn' 2 512 40 40 (1638400)
I0815 22:42:38.574002 14815 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0815 22:42:38.574007 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.574010 14815 net.cpp:184] Created Layer res5a_branch2a/relu (34)
I0815 22:42:38.574014 14815 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0815 22:42:38.574018 14815 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0815 22:42:38.574033 14815 net.cpp:245] Setting up res5a_branch2a/relu
I0815 22:42:38.574038 14815 net.cpp:252] TEST Top shape for layer 34 'res5a_branch2a/relu' 2 512 40 40 (1638400)
I0815 22:42:38.574043 14815 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0815 22:42:38.574046 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.574056 14815 net.cpp:184] Created Layer res5a_branch2b (35)
I0815 22:42:38.574060 14815 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0815 22:42:38.574064 14815 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0815 22:42:38.591336 14815 net.cpp:245] Setting up res5a_branch2b
I0815 22:42:38.591358 14815 net.cpp:252] TEST Top shape for layer 35 'res5a_branch2b' 2 512 40 40 (1638400)
I0815 22:42:38.591370 14815 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0815 22:42:38.591375 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.591385 14815 net.cpp:184] Created Layer res5a_branch2b/bn (36)
I0815 22:42:38.591389 14815 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0815 22:42:38.591394 14815 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0815 22:42:38.592288 14815 net.cpp:245] Setting up res5a_branch2b/bn
I0815 22:42:38.592296 14815 net.cpp:252] TEST Top shape for layer 36 'res5a_branch2b/bn' 2 512 40 40 (1638400)
I0815 22:42:38.592305 14815 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0815 22:42:38.592308 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.592314 14815 net.cpp:184] Created Layer res5a_branch2b/relu (37)
I0815 22:42:38.592316 14815 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0815 22:42:38.592319 14815 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0815 22:42:38.592325 14815 net.cpp:245] Setting up res5a_branch2b/relu
I0815 22:42:38.592329 14815 net.cpp:252] TEST Top shape for layer 37 'res5a_branch2b/relu' 2 512 40 40 (1638400)
I0815 22:42:38.592334 14815 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I0815 22:42:38.592339 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.592352 14815 net.cpp:184] Created Layer out5a (38)
I0815 22:42:38.592356 14815 net.cpp:561] out5a <- res5a_branch2b
I0815 22:42:38.592360 14815 net.cpp:530] out5a -> out5a
I0815 22:42:38.596719 14815 net.cpp:245] Setting up out5a
I0815 22:42:38.596729 14815 net.cpp:252] TEST Top shape for layer 38 'out5a' 2 64 40 40 (204800)
I0815 22:42:38.596735 14815 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I0815 22:42:38.596738 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.596745 14815 net.cpp:184] Created Layer out5a/bn (39)
I0815 22:42:38.596747 14815 net.cpp:561] out5a/bn <- out5a
I0815 22:42:38.596751 14815 net.cpp:513] out5a/bn -> out5a (in-place)
I0815 22:42:38.597973 14815 net.cpp:245] Setting up out5a/bn
I0815 22:42:38.597983 14815 net.cpp:252] TEST Top shape for layer 39 'out5a/bn' 2 64 40 40 (204800)
I0815 22:42:38.597991 14815 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I0815 22:42:38.597995 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.598001 14815 net.cpp:184] Created Layer out5a/relu (40)
I0815 22:42:38.598003 14815 net.cpp:561] out5a/relu <- out5a
I0815 22:42:38.598007 14815 net.cpp:513] out5a/relu -> out5a (in-place)
I0815 22:42:38.598012 14815 net.cpp:245] Setting up out5a/relu
I0815 22:42:38.598016 14815 net.cpp:252] TEST Top shape for layer 40 'out5a/relu' 2 64 40 40 (204800)
I0815 22:42:38.598021 14815 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I0815 22:42:38.598024 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.598042 14815 net.cpp:184] Created Layer out5a_up2 (41)
I0815 22:42:38.598045 14815 net.cpp:561] out5a_up2 <- out5a
I0815 22:42:38.598048 14815 net.cpp:530] out5a_up2 -> out5a_up2
I0815 22:42:38.598459 14815 net.cpp:245] Setting up out5a_up2
I0815 22:42:38.598467 14815 net.cpp:252] TEST Top shape for layer 41 'out5a_up2' 2 64 80 80 (819200)
I0815 22:42:38.598471 14815 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I0815 22:42:38.598475 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.598489 14815 net.cpp:184] Created Layer out3a (42)
I0815 22:42:38.598492 14815 net.cpp:561] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0815 22:42:38.598496 14815 net.cpp:530] out3a -> out3a
I0815 22:42:38.603363 14815 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.97G, req 0G)
I0815 22:42:38.603377 14815 net.cpp:245] Setting up out3a
I0815 22:42:38.603384 14815 net.cpp:252] TEST Top shape for layer 42 'out3a' 2 64 80 80 (819200)
I0815 22:42:38.603390 14815 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I0815 22:42:38.603395 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.603404 14815 net.cpp:184] Created Layer out3a/bn (43)
I0815 22:42:38.603408 14815 net.cpp:561] out3a/bn <- out3a
I0815 22:42:38.603412 14815 net.cpp:513] out3a/bn -> out3a (in-place)
I0815 22:42:38.604383 14815 net.cpp:245] Setting up out3a/bn
I0815 22:42:38.604393 14815 net.cpp:252] TEST Top shape for layer 43 'out3a/bn' 2 64 80 80 (819200)
I0815 22:42:38.604401 14815 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I0815 22:42:38.604405 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.604409 14815 net.cpp:184] Created Layer out3a/relu (44)
I0815 22:42:38.604413 14815 net.cpp:561] out3a/relu <- out3a
I0815 22:42:38.604416 14815 net.cpp:513] out3a/relu -> out3a (in-place)
I0815 22:42:38.604423 14815 net.cpp:245] Setting up out3a/relu
I0815 22:42:38.604426 14815 net.cpp:252] TEST Top shape for layer 44 'out3a/relu' 2 64 80 80 (819200)
I0815 22:42:38.604430 14815 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I0815 22:42:38.604434 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.604439 14815 net.cpp:184] Created Layer out3_out5_combined (45)
I0815 22:42:38.604444 14815 net.cpp:561] out3_out5_combined <- out5a_up2
I0815 22:42:38.604447 14815 net.cpp:561] out3_out5_combined <- out3a
I0815 22:42:38.604451 14815 net.cpp:530] out3_out5_combined -> out3_out5_combined
I0815 22:42:38.605695 14815 net.cpp:245] Setting up out3_out5_combined
I0815 22:42:38.605705 14815 net.cpp:252] TEST Top shape for layer 45 'out3_out5_combined' 2 64 80 80 (819200)
I0815 22:42:38.605708 14815 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I0815 22:42:38.605711 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.605721 14815 net.cpp:184] Created Layer ctx_conv1 (46)
I0815 22:42:38.605725 14815 net.cpp:561] ctx_conv1 <- out3_out5_combined
I0815 22:42:38.605728 14815 net.cpp:530] ctx_conv1 -> ctx_conv1
I0815 22:42:38.610385 14815 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.96G, req 0G)
I0815 22:42:38.610404 14815 net.cpp:245] Setting up ctx_conv1
I0815 22:42:38.610409 14815 net.cpp:252] TEST Top shape for layer 46 'ctx_conv1' 2 64 80 80 (819200)
I0815 22:42:38.610417 14815 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I0815 22:42:38.610422 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.610431 14815 net.cpp:184] Created Layer ctx_conv1/bn (47)
I0815 22:42:38.610435 14815 net.cpp:561] ctx_conv1/bn <- ctx_conv1
I0815 22:42:38.610440 14815 net.cpp:513] ctx_conv1/bn -> ctx_conv1 (in-place)
I0815 22:42:38.611415 14815 net.cpp:245] Setting up ctx_conv1/bn
I0815 22:42:38.611438 14815 net.cpp:252] TEST Top shape for layer 47 'ctx_conv1/bn' 2 64 80 80 (819200)
I0815 22:42:38.611448 14815 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I0815 22:42:38.611451 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.611456 14815 net.cpp:184] Created Layer ctx_conv1/relu (48)
I0815 22:42:38.611459 14815 net.cpp:561] ctx_conv1/relu <- ctx_conv1
I0815 22:42:38.611464 14815 net.cpp:513] ctx_conv1/relu -> ctx_conv1 (in-place)
I0815 22:42:38.611469 14815 net.cpp:245] Setting up ctx_conv1/relu
I0815 22:42:38.611474 14815 net.cpp:252] TEST Top shape for layer 48 'ctx_conv1/relu' 2 64 80 80 (819200)
I0815 22:42:38.611477 14815 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I0815 22:42:38.611481 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.611495 14815 net.cpp:184] Created Layer ctx_conv2 (49)
I0815 22:42:38.611498 14815 net.cpp:561] ctx_conv2 <- ctx_conv1
I0815 22:42:38.611502 14815 net.cpp:530] ctx_conv2 -> ctx_conv2
I0815 22:42:38.613015 14815 net.cpp:245] Setting up ctx_conv2
I0815 22:42:38.613025 14815 net.cpp:252] TEST Top shape for layer 49 'ctx_conv2' 2 64 80 80 (819200)
I0815 22:42:38.613030 14815 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I0815 22:42:38.613034 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.613040 14815 net.cpp:184] Created Layer ctx_conv2/bn (50)
I0815 22:42:38.613044 14815 net.cpp:561] ctx_conv2/bn <- ctx_conv2
I0815 22:42:38.613047 14815 net.cpp:513] ctx_conv2/bn -> ctx_conv2 (in-place)
I0815 22:42:38.613965 14815 net.cpp:245] Setting up ctx_conv2/bn
I0815 22:42:38.613975 14815 net.cpp:252] TEST Top shape for layer 50 'ctx_conv2/bn' 2 64 80 80 (819200)
I0815 22:42:38.613982 14815 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I0815 22:42:38.613986 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.613992 14815 net.cpp:184] Created Layer ctx_conv2/relu (51)
I0815 22:42:38.613996 14815 net.cpp:561] ctx_conv2/relu <- ctx_conv2
I0815 22:42:38.613999 14815 net.cpp:513] ctx_conv2/relu -> ctx_conv2 (in-place)
I0815 22:42:38.614004 14815 net.cpp:245] Setting up ctx_conv2/relu
I0815 22:42:38.614009 14815 net.cpp:252] TEST Top shape for layer 51 'ctx_conv2/relu' 2 64 80 80 (819200)
I0815 22:42:38.614015 14815 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I0815 22:42:38.614019 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.614027 14815 net.cpp:184] Created Layer ctx_conv3 (52)
I0815 22:42:38.614030 14815 net.cpp:561] ctx_conv3 <- ctx_conv2
I0815 22:42:38.614033 14815 net.cpp:530] ctx_conv3 -> ctx_conv3
I0815 22:42:38.615587 14815 net.cpp:245] Setting up ctx_conv3
I0815 22:42:38.615604 14815 net.cpp:252] TEST Top shape for layer 52 'ctx_conv3' 2 64 80 80 (819200)
I0815 22:42:38.615612 14815 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I0815 22:42:38.615617 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.615624 14815 net.cpp:184] Created Layer ctx_conv3/bn (53)
I0815 22:42:38.615628 14815 net.cpp:561] ctx_conv3/bn <- ctx_conv3
I0815 22:42:38.615633 14815 net.cpp:513] ctx_conv3/bn -> ctx_conv3 (in-place)
I0815 22:42:38.616565 14815 net.cpp:245] Setting up ctx_conv3/bn
I0815 22:42:38.616575 14815 net.cpp:252] TEST Top shape for layer 53 'ctx_conv3/bn' 2 64 80 80 (819200)
I0815 22:42:38.616582 14815 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I0815 22:42:38.616585 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.616590 14815 net.cpp:184] Created Layer ctx_conv3/relu (54)
I0815 22:42:38.616593 14815 net.cpp:561] ctx_conv3/relu <- ctx_conv3
I0815 22:42:38.616605 14815 net.cpp:513] ctx_conv3/relu -> ctx_conv3 (in-place)
I0815 22:42:38.616611 14815 net.cpp:245] Setting up ctx_conv3/relu
I0815 22:42:38.616616 14815 net.cpp:252] TEST Top shape for layer 54 'ctx_conv3/relu' 2 64 80 80 (819200)
I0815 22:42:38.616621 14815 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I0815 22:42:38.616623 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.616631 14815 net.cpp:184] Created Layer ctx_conv4 (55)
I0815 22:42:38.616636 14815 net.cpp:561] ctx_conv4 <- ctx_conv3
I0815 22:42:38.616639 14815 net.cpp:530] ctx_conv4 -> ctx_conv4
I0815 22:42:38.618109 14815 net.cpp:245] Setting up ctx_conv4
I0815 22:42:38.618118 14815 net.cpp:252] TEST Top shape for layer 55 'ctx_conv4' 2 64 80 80 (819200)
I0815 22:42:38.618124 14815 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I0815 22:42:38.618126 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.618132 14815 net.cpp:184] Created Layer ctx_conv4/bn (56)
I0815 22:42:38.618135 14815 net.cpp:561] ctx_conv4/bn <- ctx_conv4
I0815 22:42:38.618139 14815 net.cpp:513] ctx_conv4/bn -> ctx_conv4 (in-place)
I0815 22:42:38.619069 14815 net.cpp:245] Setting up ctx_conv4/bn
I0815 22:42:38.619077 14815 net.cpp:252] TEST Top shape for layer 56 'ctx_conv4/bn' 2 64 80 80 (819200)
I0815 22:42:38.619086 14815 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I0815 22:42:38.619089 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.619093 14815 net.cpp:184] Created Layer ctx_conv4/relu (57)
I0815 22:42:38.619096 14815 net.cpp:561] ctx_conv4/relu <- ctx_conv4
I0815 22:42:38.619101 14815 net.cpp:513] ctx_conv4/relu -> ctx_conv4 (in-place)
I0815 22:42:38.619104 14815 net.cpp:245] Setting up ctx_conv4/relu
I0815 22:42:38.619108 14815 net.cpp:252] TEST Top shape for layer 57 'ctx_conv4/relu' 2 64 80 80 (819200)
I0815 22:42:38.619112 14815 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I0815 22:42:38.619117 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.619123 14815 net.cpp:184] Created Layer ctx_final (58)
I0815 22:42:38.619127 14815 net.cpp:561] ctx_final <- ctx_conv4
I0815 22:42:38.619130 14815 net.cpp:530] ctx_final -> ctx_final
I0815 22:42:38.624081 14815 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.96G, req 0G)
I0815 22:42:38.624094 14815 net.cpp:245] Setting up ctx_final
I0815 22:42:38.624099 14815 net.cpp:252] TEST Top shape for layer 58 'ctx_final' 2 8 80 80 (102400)
I0815 22:42:38.624105 14815 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I0815 22:42:38.624109 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.624114 14815 net.cpp:184] Created Layer ctx_final/relu (59)
I0815 22:42:38.624117 14815 net.cpp:561] ctx_final/relu <- ctx_final
I0815 22:42:38.624121 14815 net.cpp:513] ctx_final/relu -> ctx_final (in-place)
I0815 22:42:38.624143 14815 net.cpp:245] Setting up ctx_final/relu
I0815 22:42:38.624150 14815 net.cpp:252] TEST Top shape for layer 59 'ctx_final/relu' 2 8 80 80 (102400)
I0815 22:42:38.624152 14815 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I0815 22:42:38.624156 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.624168 14815 net.cpp:184] Created Layer out_deconv_final_up2 (60)
I0815 22:42:38.624172 14815 net.cpp:561] out_deconv_final_up2 <- ctx_final
I0815 22:42:38.624176 14815 net.cpp:530] out_deconv_final_up2 -> out_deconv_final_up2
I0815 22:42:38.624590 14815 net.cpp:245] Setting up out_deconv_final_up2
I0815 22:42:38.624598 14815 net.cpp:252] TEST Top shape for layer 60 'out_deconv_final_up2' 2 8 160 160 (409600)
I0815 22:42:38.624603 14815 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I0815 22:42:38.624614 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.624620 14815 net.cpp:184] Created Layer out_deconv_final_up4 (61)
I0815 22:42:38.624624 14815 net.cpp:561] out_deconv_final_up4 <- out_deconv_final_up2
I0815 22:42:38.624629 14815 net.cpp:530] out_deconv_final_up4 -> out_deconv_final_up4
I0815 22:42:38.625008 14815 net.cpp:245] Setting up out_deconv_final_up4
I0815 22:42:38.625015 14815 net.cpp:252] TEST Top shape for layer 61 'out_deconv_final_up4' 2 8 320 320 (1638400)
I0815 22:42:38.625020 14815 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I0815 22:42:38.625023 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.625028 14815 net.cpp:184] Created Layer out_deconv_final_up8 (62)
I0815 22:42:38.625031 14815 net.cpp:561] out_deconv_final_up8 <- out_deconv_final_up4
I0815 22:42:38.625036 14815 net.cpp:530] out_deconv_final_up8 -> out_deconv_final_up8
I0815 22:42:38.625412 14815 net.cpp:245] Setting up out_deconv_final_up8
I0815 22:42:38.625419 14815 net.cpp:252] TEST Top shape for layer 62 'out_deconv_final_up8' 2 8 640 640 (6553600)
I0815 22:42:38.625424 14815 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8_out_deconv_final_up8_0_split' of type 'Split'
I0815 22:42:38.625427 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.625432 14815 net.cpp:184] Created Layer out_deconv_final_up8_out_deconv_final_up8_0_split (63)
I0815 22:42:38.625435 14815 net.cpp:561] out_deconv_final_up8_out_deconv_final_up8_0_split <- out_deconv_final_up8
I0815 22:42:38.625438 14815 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0815 22:42:38.625442 14815 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0815 22:42:38.625447 14815 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0815 22:42:38.625535 14815 net.cpp:245] Setting up out_deconv_final_up8_out_deconv_final_up8_0_split
I0815 22:42:38.625542 14815 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 8 640 640 (6553600)
I0815 22:42:38.625546 14815 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 8 640 640 (6553600)
I0815 22:42:38.625550 14815 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 8 640 640 (6553600)
I0815 22:42:38.625555 14815 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0815 22:42:38.625560 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.625566 14815 net.cpp:184] Created Layer loss (64)
I0815 22:42:38.625569 14815 net.cpp:561] loss <- out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0815 22:42:38.625573 14815 net.cpp:561] loss <- label_data_1_split_0
I0815 22:42:38.625578 14815 net.cpp:530] loss -> loss
I0815 22:42:38.626617 14815 net.cpp:245] Setting up loss
I0815 22:42:38.626627 14815 net.cpp:252] TEST Top shape for layer 64 'loss' (1)
I0815 22:42:38.626631 14815 net.cpp:256]     with loss weight 1
I0815 22:42:38.626636 14815 layer_factory.hpp:136] Creating layer 'accuracy/top1' of type 'Accuracy'
I0815 22:42:38.626639 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.626648 14815 net.cpp:184] Created Layer accuracy/top1 (65)
I0815 22:42:38.626652 14815 net.cpp:561] accuracy/top1 <- out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0815 22:42:38.626655 14815 net.cpp:561] accuracy/top1 <- label_data_1_split_1
I0815 22:42:38.626659 14815 net.cpp:530] accuracy/top1 -> accuracy/top1
I0815 22:42:38.626667 14815 net.cpp:245] Setting up accuracy/top1
I0815 22:42:38.626672 14815 net.cpp:252] TEST Top shape for layer 65 'accuracy/top1' (1)
I0815 22:42:38.626682 14815 layer_factory.hpp:136] Creating layer 'accuracy/top5' of type 'Accuracy'
I0815 22:42:38.626685 14815 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 22:42:38.626691 14815 net.cpp:184] Created Layer accuracy/top5 (66)
I0815 22:42:38.626695 14815 net.cpp:561] accuracy/top5 <- out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0815 22:42:38.626700 14815 net.cpp:561] accuracy/top5 <- label_data_1_split_2
I0815 22:42:38.626704 14815 net.cpp:530] accuracy/top5 -> accuracy/top5
I0815 22:42:38.626710 14815 net.cpp:245] Setting up accuracy/top5
I0815 22:42:38.626715 14815 net.cpp:252] TEST Top shape for layer 66 'accuracy/top5' (1)
I0815 22:42:38.626719 14815 net.cpp:325] accuracy/top5 does not need backward computation.
I0815 22:42:38.626724 14815 net.cpp:325] accuracy/top1 does not need backward computation.
I0815 22:42:38.626727 14815 net.cpp:323] loss needs backward computation.
I0815 22:42:38.626732 14815 net.cpp:323] out_deconv_final_up8_out_deconv_final_up8_0_split needs backward computation.
I0815 22:42:38.626736 14815 net.cpp:323] out_deconv_final_up8 needs backward computation.
I0815 22:42:38.626740 14815 net.cpp:323] out_deconv_final_up4 needs backward computation.
I0815 22:42:38.626744 14815 net.cpp:323] out_deconv_final_up2 needs backward computation.
I0815 22:42:38.626747 14815 net.cpp:323] ctx_final/relu needs backward computation.
I0815 22:42:38.626750 14815 net.cpp:323] ctx_final needs backward computation.
I0815 22:42:38.626754 14815 net.cpp:323] ctx_conv4/relu needs backward computation.
I0815 22:42:38.626757 14815 net.cpp:323] ctx_conv4/bn needs backward computation.
I0815 22:42:38.626761 14815 net.cpp:323] ctx_conv4 needs backward computation.
I0815 22:42:38.626765 14815 net.cpp:323] ctx_conv3/relu needs backward computation.
I0815 22:42:38.626768 14815 net.cpp:323] ctx_conv3/bn needs backward computation.
I0815 22:42:38.626771 14815 net.cpp:323] ctx_conv3 needs backward computation.
I0815 22:42:38.626775 14815 net.cpp:323] ctx_conv2/relu needs backward computation.
I0815 22:42:38.626778 14815 net.cpp:323] ctx_conv2/bn needs backward computation.
I0815 22:42:38.626781 14815 net.cpp:323] ctx_conv2 needs backward computation.
I0815 22:42:38.626785 14815 net.cpp:323] ctx_conv1/relu needs backward computation.
I0815 22:42:38.626788 14815 net.cpp:323] ctx_conv1/bn needs backward computation.
I0815 22:42:38.626792 14815 net.cpp:323] ctx_conv1 needs backward computation.
I0815 22:42:38.626796 14815 net.cpp:323] out3_out5_combined needs backward computation.
I0815 22:42:38.626799 14815 net.cpp:323] out3a/relu needs backward computation.
I0815 22:42:38.626803 14815 net.cpp:323] out3a/bn needs backward computation.
I0815 22:42:38.626807 14815 net.cpp:323] out3a needs backward computation.
I0815 22:42:38.626811 14815 net.cpp:323] out5a_up2 needs backward computation.
I0815 22:42:38.626814 14815 net.cpp:323] out5a/relu needs backward computation.
I0815 22:42:38.626817 14815 net.cpp:323] out5a/bn needs backward computation.
I0815 22:42:38.626821 14815 net.cpp:323] out5a needs backward computation.
I0815 22:42:38.626826 14815 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0815 22:42:38.626829 14815 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0815 22:42:38.626833 14815 net.cpp:323] res5a_branch2b needs backward computation.
I0815 22:42:38.626837 14815 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0815 22:42:38.626840 14815 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0815 22:42:38.626843 14815 net.cpp:323] res5a_branch2a needs backward computation.
I0815 22:42:38.626847 14815 net.cpp:323] pool4 needs backward computation.
I0815 22:42:38.626850 14815 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0815 22:42:38.626854 14815 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0815 22:42:38.626857 14815 net.cpp:323] res4a_branch2b needs backward computation.
I0815 22:42:38.626862 14815 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0815 22:42:38.626869 14815 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0815 22:42:38.626873 14815 net.cpp:323] res4a_branch2a needs backward computation.
I0815 22:42:38.626878 14815 net.cpp:323] pool3 needs backward computation.
I0815 22:42:38.626881 14815 net.cpp:323] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I0815 22:42:38.626885 14815 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0815 22:42:38.626889 14815 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0815 22:42:38.626893 14815 net.cpp:323] res3a_branch2b needs backward computation.
I0815 22:42:38.626896 14815 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0815 22:42:38.626900 14815 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0815 22:42:38.626904 14815 net.cpp:323] res3a_branch2a needs backward computation.
I0815 22:42:38.626907 14815 net.cpp:323] pool2 needs backward computation.
I0815 22:42:38.626911 14815 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0815 22:42:38.626915 14815 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0815 22:42:38.626919 14815 net.cpp:323] res2a_branch2b needs backward computation.
I0815 22:42:38.626922 14815 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0815 22:42:38.626925 14815 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0815 22:42:38.626929 14815 net.cpp:323] res2a_branch2a needs backward computation.
I0815 22:42:38.626932 14815 net.cpp:323] pool1 needs backward computation.
I0815 22:42:38.626936 14815 net.cpp:323] conv1b/relu needs backward computation.
I0815 22:42:38.626940 14815 net.cpp:323] conv1b/bn needs backward computation.
I0815 22:42:38.626945 14815 net.cpp:323] conv1b needs backward computation.
I0815 22:42:38.626948 14815 net.cpp:323] conv1a/relu needs backward computation.
I0815 22:42:38.626952 14815 net.cpp:323] conv1a/bn needs backward computation.
I0815 22:42:38.626956 14815 net.cpp:323] conv1a needs backward computation.
I0815 22:42:38.626960 14815 net.cpp:325] data/bias does not need backward computation.
I0815 22:42:38.626965 14815 net.cpp:325] label_data_1_split does not need backward computation.
I0815 22:42:38.626968 14815 net.cpp:325] data does not need backward computation.
I0815 22:42:38.626971 14815 net.cpp:367] This network produces output accuracy/top1
I0815 22:42:38.626976 14815 net.cpp:367] This network produces output accuracy/top5
I0815 22:42:38.626979 14815 net.cpp:367] This network produces output loss
I0815 22:42:38.627041 14815 net.cpp:389] Top memory (TEST) required for data: 318668800 diff: 8
I0815 22:42:38.627046 14815 net.cpp:392] Bottom memory (TEST) required for data: 318668800 diff: 318668800
I0815 22:42:38.627049 14815 net.cpp:395] Shared (in-place) memory (TEST) by data: 210124800 diff: 210124800
I0815 22:42:38.627053 14815 net.cpp:398] Parameters memory (TEST) required for data: 2692608 diff: 2692608
I0815 22:42:38.627058 14815 net.cpp:401] Parameters shared memory (TEST) by data: 0 diff: 0
I0815 22:42:38.627060 14815 net.cpp:407] Network initialization done.
I0815 22:42:38.627182 14815 solver.cpp:56] Solver scaffolding done.
I0815 22:42:38.639786 14815 caffe.cpp:137] Finetuning from training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/l1reg/cityscapes5_jsegnet21v2_iter_32000.caffemodel
I0815 22:42:38.646469 14815 net.cpp:1095] Copying source layer data Type:ImageLabelData #blobs=0
I0815 22:42:38.646492 14815 net.cpp:1095] Copying source layer data/bias Type:Bias #blobs=1
I0815 22:42:38.646533 14815 net.cpp:1095] Copying source layer conv1a Type:Convolution #blobs=2
I0815 22:42:38.646554 14815 net.cpp:1095] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0815 22:42:38.647317 14815 net.cpp:1095] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0815 22:42:38.647328 14815 net.cpp:1095] Copying source layer conv1b Type:Convolution #blobs=2
I0815 22:42:38.647346 14815 net.cpp:1095] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0815 22:42:38.647919 14815 net.cpp:1095] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0815 22:42:38.647940 14815 net.cpp:1095] Copying source layer pool1 Type:Pooling #blobs=0
I0815 22:42:38.647945 14815 net.cpp:1095] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0815 22:42:38.647967 14815 net.cpp:1095] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0815 22:42:38.648558 14815 net.cpp:1095] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0815 22:42:38.648568 14815 net.cpp:1095] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0815 22:42:38.648587 14815 net.cpp:1095] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0815 22:42:38.649158 14815 net.cpp:1095] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0815 22:42:38.649168 14815 net.cpp:1095] Copying source layer pool2 Type:Pooling #blobs=0
I0815 22:42:38.649173 14815 net.cpp:1095] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0815 22:42:38.649219 14815 net.cpp:1095] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0815 22:42:38.649749 14815 net.cpp:1095] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0815 22:42:38.649757 14815 net.cpp:1095] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0815 22:42:38.649788 14815 net.cpp:1095] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0815 22:42:38.650321 14815 net.cpp:1095] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0815 22:42:38.650331 14815 net.cpp:1095] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I0815 22:42:38.650336 14815 net.cpp:1095] Copying source layer pool3 Type:Pooling #blobs=0
I0815 22:42:38.650341 14815 net.cpp:1095] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0815 22:42:38.650607 14815 net.cpp:1095] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0815 22:42:38.651162 14815 net.cpp:1095] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0815 22:42:38.651172 14815 net.cpp:1095] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0815 22:42:38.651253 14815 net.cpp:1095] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0815 22:42:38.651777 14815 net.cpp:1095] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0815 22:42:38.651787 14815 net.cpp:1095] Copying source layer pool4 Type:Pooling #blobs=0
I0815 22:42:38.651793 14815 net.cpp:1095] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0815 22:42:38.652236 14815 net.cpp:1095] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0815 22:42:38.652770 14815 net.cpp:1095] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0815 22:42:38.652781 14815 net.cpp:1095] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0815 22:42:38.653012 14815 net.cpp:1095] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0815 22:42:38.653545 14815 net.cpp:1095] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0815 22:42:38.653555 14815 net.cpp:1095] Copying source layer out5a Type:Convolution #blobs=2
I0815 22:42:38.653630 14815 net.cpp:1095] Copying source layer out5a/bn Type:BatchNorm #blobs=5
I0815 22:42:38.653870 14815 net.cpp:1095] Copying source layer out5a/relu Type:ReLU #blobs=0
I0815 22:42:38.653879 14815 net.cpp:1095] Copying source layer out5a_up2 Type:Deconvolution #blobs=1
I0815 22:42:38.653890 14815 net.cpp:1095] Copying source layer out3a Type:Convolution #blobs=2
I0815 22:42:38.653920 14815 net.cpp:1095] Copying source layer out3a/bn Type:BatchNorm #blobs=5
I0815 22:42:38.654145 14815 net.cpp:1095] Copying source layer out3a/relu Type:ReLU #blobs=0
I0815 22:42:38.654155 14815 net.cpp:1095] Copying source layer out3_out5_combined Type:Eltwise #blobs=0
I0815 22:42:38.654162 14815 net.cpp:1095] Copying source layer ctx_conv1 Type:Convolution #blobs=2
I0815 22:42:38.654191 14815 net.cpp:1095] Copying source layer ctx_conv1/bn Type:BatchNorm #blobs=5
I0815 22:42:38.654417 14815 net.cpp:1095] Copying source layer ctx_conv1/relu Type:ReLU #blobs=0
I0815 22:42:38.654428 14815 net.cpp:1095] Copying source layer ctx_conv2 Type:Convolution #blobs=2
I0815 22:42:38.654469 14815 net.cpp:1095] Copying source layer ctx_conv2/bn Type:BatchNorm #blobs=5
I0815 22:42:38.654692 14815 net.cpp:1095] Copying source layer ctx_conv2/relu Type:ReLU #blobs=0
I0815 22:42:38.654701 14815 net.cpp:1095] Copying source layer ctx_conv3 Type:Convolution #blobs=2
I0815 22:42:38.654732 14815 net.cpp:1095] Copying source layer ctx_conv3/bn Type:BatchNorm #blobs=5
I0815 22:42:38.654953 14815 net.cpp:1095] Copying source layer ctx_conv3/relu Type:ReLU #blobs=0
I0815 22:42:38.654963 14815 net.cpp:1095] Copying source layer ctx_conv4 Type:Convolution #blobs=2
I0815 22:42:38.654991 14815 net.cpp:1095] Copying source layer ctx_conv4/bn Type:BatchNorm #blobs=5
I0815 22:42:38.655210 14815 net.cpp:1095] Copying source layer ctx_conv4/relu Type:ReLU #blobs=0
I0815 22:42:38.655218 14815 net.cpp:1095] Copying source layer ctx_final Type:Convolution #blobs=2
I0815 22:42:38.655234 14815 net.cpp:1095] Copying source layer ctx_final/relu Type:ReLU #blobs=0
I0815 22:42:38.655239 14815 net.cpp:1095] Copying source layer out_deconv_final_up2 Type:Deconvolution #blobs=1
I0815 22:42:38.655248 14815 net.cpp:1095] Copying source layer out_deconv_final_up4 Type:Deconvolution #blobs=1
I0815 22:42:38.655257 14815 net.cpp:1095] Copying source layer out_deconv_final_up8 Type:Deconvolution #blobs=1
I0815 22:42:38.655267 14815 net.cpp:1095] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0815 22:42:38.660143 14815 net.cpp:1095] Copying source layer data Type:ImageLabelData #blobs=0
I0815 22:42:38.660166 14815 net.cpp:1095] Copying source layer data/bias Type:Bias #blobs=1
I0815 22:42:38.660197 14815 net.cpp:1095] Copying source layer conv1a Type:Convolution #blobs=2
I0815 22:42:38.660208 14815 net.cpp:1095] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0815 22:42:38.660787 14815 net.cpp:1095] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0815 22:42:38.660797 14815 net.cpp:1095] Copying source layer conv1b Type:Convolution #blobs=2
I0815 22:42:38.660810 14815 net.cpp:1095] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0815 22:42:38.661371 14815 net.cpp:1095] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0815 22:42:38.661382 14815 net.cpp:1095] Copying source layer pool1 Type:Pooling #blobs=0
I0815 22:42:38.661388 14815 net.cpp:1095] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0815 22:42:38.661409 14815 net.cpp:1095] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0815 22:42:38.661998 14815 net.cpp:1095] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0815 22:42:38.662008 14815 net.cpp:1095] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0815 22:42:38.662027 14815 net.cpp:1095] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0815 22:42:38.662607 14815 net.cpp:1095] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0815 22:42:38.662617 14815 net.cpp:1095] Copying source layer pool2 Type:Pooling #blobs=0
I0815 22:42:38.662623 14815 net.cpp:1095] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0815 22:42:38.662669 14815 net.cpp:1095] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0815 22:42:38.663223 14815 net.cpp:1095] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0815 22:42:38.663233 14815 net.cpp:1095] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0815 22:42:38.663262 14815 net.cpp:1095] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0815 22:42:38.663801 14815 net.cpp:1095] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0815 22:42:38.663812 14815 net.cpp:1095] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I0815 22:42:38.663817 14815 net.cpp:1095] Copying source layer pool3 Type:Pooling #blobs=0
I0815 22:42:38.663821 14815 net.cpp:1095] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0815 22:42:38.663945 14815 net.cpp:1095] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0815 22:42:38.664515 14815 net.cpp:1095] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0815 22:42:38.664525 14815 net.cpp:1095] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0815 22:42:38.664593 14815 net.cpp:1095] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0815 22:42:38.665138 14815 net.cpp:1095] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0815 22:42:38.665148 14815 net.cpp:1095] Copying source layer pool4 Type:Pooling #blobs=0
I0815 22:42:38.665153 14815 net.cpp:1095] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0815 22:42:38.665550 14815 net.cpp:1095] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0815 22:42:38.666081 14815 net.cpp:1095] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0815 22:42:38.666091 14815 net.cpp:1095] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0815 22:42:38.666307 14815 net.cpp:1095] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0815 22:42:38.666838 14815 net.cpp:1095] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0815 22:42:38.666848 14815 net.cpp:1095] Copying source layer out5a Type:Convolution #blobs=2
I0815 22:42:38.666916 14815 net.cpp:1095] Copying source layer out5a/bn Type:BatchNorm #blobs=5
I0815 22:42:38.667157 14815 net.cpp:1095] Copying source layer out5a/relu Type:ReLU #blobs=0
I0815 22:42:38.667166 14815 net.cpp:1095] Copying source layer out5a_up2 Type:Deconvolution #blobs=1
I0815 22:42:38.667176 14815 net.cpp:1095] Copying source layer out3a Type:Convolution #blobs=2
I0815 22:42:38.667202 14815 net.cpp:1095] Copying source layer out3a/bn Type:BatchNorm #blobs=5
I0815 22:42:38.667429 14815 net.cpp:1095] Copying source layer out3a/relu Type:ReLU #blobs=0
I0815 22:42:38.667438 14815 net.cpp:1095] Copying source layer out3_out5_combined Type:Eltwise #blobs=0
I0815 22:42:38.667443 14815 net.cpp:1095] Copying source layer ctx_conv1 Type:Convolution #blobs=2
I0815 22:42:38.667469 14815 net.cpp:1095] Copying source layer ctx_conv1/bn Type:BatchNorm #blobs=5
I0815 22:42:38.667695 14815 net.cpp:1095] Copying source layer ctx_conv1/relu Type:ReLU #blobs=0
I0815 22:42:38.667704 14815 net.cpp:1095] Copying source layer ctx_conv2 Type:Convolution #blobs=2
I0815 22:42:38.667732 14815 net.cpp:1095] Copying source layer ctx_conv2/bn Type:BatchNorm #blobs=5
I0815 22:42:38.667955 14815 net.cpp:1095] Copying source layer ctx_conv2/relu Type:ReLU #blobs=0
I0815 22:42:38.667964 14815 net.cpp:1095] Copying source layer ctx_conv3 Type:Convolution #blobs=2
I0815 22:42:38.667994 14815 net.cpp:1095] Copying source layer ctx_conv3/bn Type:BatchNorm #blobs=5
I0815 22:42:38.668223 14815 net.cpp:1095] Copying source layer ctx_conv3/relu Type:ReLU #blobs=0
I0815 22:42:38.668232 14815 net.cpp:1095] Copying source layer ctx_conv4 Type:Convolution #blobs=2
I0815 22:42:38.668261 14815 net.cpp:1095] Copying source layer ctx_conv4/bn Type:BatchNorm #blobs=5
I0815 22:42:38.668486 14815 net.cpp:1095] Copying source layer ctx_conv4/relu Type:ReLU #blobs=0
I0815 22:42:38.668496 14815 net.cpp:1095] Copying source layer ctx_final Type:Convolution #blobs=2
I0815 22:42:38.668512 14815 net.cpp:1095] Copying source layer ctx_final/relu Type:ReLU #blobs=0
I0815 22:42:38.668517 14815 net.cpp:1095] Copying source layer out_deconv_final_up2 Type:Deconvolution #blobs=1
I0815 22:42:38.668525 14815 net.cpp:1095] Copying source layer out_deconv_final_up4 Type:Deconvolution #blobs=1
I0815 22:42:38.668535 14815 net.cpp:1095] Copying source layer out_deconv_final_up8 Type:Deconvolution #blobs=1
I0815 22:42:38.668545 14815 net.cpp:1095] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0815 22:42:38.668668 14815 parallel.cpp:106] [0 - 0] P2pSync adding callback
I0815 22:42:38.668678 14815 parallel.cpp:106] [1 - 1] P2pSync adding callback
I0815 22:42:38.668682 14815 parallel.cpp:106] [2 - 2] P2pSync adding callback
I0815 22:42:38.668686 14815 parallel.cpp:59] Starting Optimization
I0815 22:42:38.668690 14815 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 22:42:38.668727 14815 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 22:42:38.668773 14815 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 22:42:38.671696 14907 device_alternate.hpp:116] NVML initialized on thread 136480662079232
I0815 22:42:38.703514 14907 common.cpp:583] NVML succeeded to set CPU affinity on device 0
I0815 22:42:38.703567 14908 device_alternate.hpp:116] NVML initialized on thread 136480653686528
I0815 22:42:38.704160 14908 common.cpp:583] NVML succeeded to set CPU affinity on device 1
I0815 22:42:38.704179 14909 device_alternate.hpp:116] NVML initialized on thread 136480645293824
I0815 22:42:38.704754 14909 common.cpp:583] NVML succeeded to set CPU affinity on device 2
I0815 22:42:38.709017 14908 solver.cpp:42] Solver data type: FLOAT
W0815 22:42:38.709815 14908 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0815 22:42:38.709969 14908 net.cpp:104] Using FLOAT as default forward math type
I0815 22:42:38.709976 14908 net.cpp:110] Using FLOAT as default backward math type
I0815 22:42:38.710016 14908 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0815 22:42:38.710026 14908 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 22:42:38.713348 14909 solver.cpp:42] Solver data type: FLOAT
W0815 22:42:38.713912 14909 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0815 22:42:38.714030 14909 net.cpp:104] Using FLOAT as default forward math type
I0815 22:42:38.714035 14909 net.cpp:110] Using FLOAT as default backward math type
I0815 22:42:38.714038 14915 db_lmdb.cpp:24] Opened lmdb data/train-image-lmdb
I0815 22:42:38.714128 14909 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0815 22:42:38.714134 14909 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 22:42:38.715008 14916 db_lmdb.cpp:24] Opened lmdb data/train-image-lmdb
I0815 22:42:38.717324 14908 data_layer.cpp:185] [1] ReshapePrefetch 6, 3, 640, 640
I0815 22:42:38.717434 14908 data_layer.cpp:209] [1] Output data size: 6, 3, 640, 640
I0815 22:42:38.717443 14908 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 22:42:38.717511 14908 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0815 22:42:38.717523 14908 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 22:42:38.718389 14917 data_layer.cpp:97] [1] Parser threads: 1
I0815 22:42:38.718399 14917 data_layer.cpp:99] [1] Transformer threads: 1
I0815 22:42:38.724099 14909 data_layer.cpp:185] [2] ReshapePrefetch 6, 3, 640, 640
I0815 22:42:38.724102 14918 db_lmdb.cpp:24] Opened lmdb data/train-label-lmdb
I0815 22:42:38.725055 14909 data_layer.cpp:209] [2] Output data size: 6, 3, 640, 640
I0815 22:42:38.725114 14909 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 22:42:38.725373 14909 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0815 22:42:38.725450 14909 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 22:42:38.726379 14908 data_layer.cpp:185] [1] ReshapePrefetch 6, 1, 640, 640
I0815 22:42:38.726521 14908 data_layer.cpp:209] [1] Output data size: 6, 1, 640, 640
I0815 22:42:38.726531 14908 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 22:42:38.726971 14919 data_layer.cpp:97] [2] Parser threads: 1
I0815 22:42:38.727007 14919 data_layer.cpp:99] [2] Transformer threads: 1
I0815 22:42:38.734621 14920 db_lmdb.cpp:24] Opened lmdb data/train-label-lmdb
I0815 22:42:38.737197 14909 data_layer.cpp:185] [2] ReshapePrefetch 6, 1, 640, 640
I0815 22:42:38.737510 14909 data_layer.cpp:209] [2] Output data size: 6, 1, 640, 640
I0815 22:42:38.737546 14909 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 22:42:38.740530 14921 data_layer.cpp:97] [1] Parser threads: 1
I0815 22:42:38.740593 14921 data_layer.cpp:99] [1] Transformer threads: 1
I0815 22:42:38.742661 14917 blocking_queue.cpp:40] Waiting for datum
I0815 22:42:38.743232 14922 data_layer.cpp:97] [2] Parser threads: 1
I0815 22:42:38.743288 14922 data_layer.cpp:99] [2] Transformer threads: 1
I0815 22:42:39.279404 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 7.99G, req 0G)
I0815 22:42:39.293010 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 7.99G, req 0G)
I0815 22:42:39.328670 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.83G, req 0G)
I0815 22:42:39.342290 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.83G, req 0G)
I0815 22:42:39.371150 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.7G, req 0G)
I0815 22:42:39.384855 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.7G, req 0G)
I0815 22:42:39.394398 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.62G, req 0G)
I0815 22:42:39.408059 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.62G, req 0G)
I0815 22:42:39.417698 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 7.56G, req 0G)
I0815 22:42:39.430300 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.52G, req 0G)
I0815 22:42:39.432096 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 7.56G, req 0G)
I0815 22:42:39.445282 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.52G, req 0G)
I0815 22:42:39.459321 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.48G, req 0G)
I0815 22:42:39.470358 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.46G, req 0G)
I0815 22:42:39.473407 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.48G, req 0G)
I0815 22:42:39.485296 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.46G, req 0G)
I0815 22:42:39.545521 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 7.39G, req 0G)
I0815 22:42:39.559749 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 7.39G, req 0G)
I0815 22:42:39.564873 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 7.34G, req 0G)
I0815 22:42:39.577150 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 7.34G, req 0G)
I0815 22:42:39.587128 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 4 3  (limit 7.32G, req 0G)
I0815 22:42:39.590557 14908 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/sparse/test.prototxt
W0815 22:42:39.590631 14908 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0815 22:42:39.590767 14908 net.cpp:104] Using FLOAT as default forward math type
I0815 22:42:39.590772 14908 net.cpp:110] Using FLOAT as default backward math type
I0815 22:42:39.590801 14908 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0815 22:42:39.590808 14908 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 22:42:39.591625 14938 db_lmdb.cpp:24] Opened lmdb data/val-image-lmdb
I0815 22:42:39.593129 14908 data_layer.cpp:185] (1) ReshapePrefetch 2, 3, 640, 640
I0815 22:42:39.593200 14908 data_layer.cpp:209] (1) Output data size: 2, 3, 640, 640
I0815 22:42:39.593206 14908 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 22:42:39.593255 14908 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0815 22:42:39.593264 14908 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 22:42:39.594053 14939 data_layer.cpp:97] (1) Parser threads: 1
I0815 22:42:39.594069 14939 data_layer.cpp:99] (1) Transformer threads: 1
I0815 22:42:39.596352 14940 db_lmdb.cpp:24] Opened lmdb data/val-label-lmdb
I0815 22:42:39.597757 14908 data_layer.cpp:185] (1) ReshapePrefetch 2, 1, 640, 640
I0815 22:42:39.597883 14908 data_layer.cpp:209] (1) Output data size: 2, 1, 640, 640
I0815 22:42:39.597890 14908 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 22:42:39.598996 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 7.32G, req 0G)
I0815 22:42:39.599375 14941 data_layer.cpp:97] (1) Parser threads: 1
I0815 22:42:39.599385 14941 data_layer.cpp:99] (1) Transformer threads: 1
I0815 22:42:39.608963 14909 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/sparse/test.prototxt
W0815 22:42:39.609086 14909 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0815 22:42:39.609267 14909 net.cpp:104] Using FLOAT as default forward math type
I0815 22:42:39.609274 14909 net.cpp:110] Using FLOAT as default backward math type
I0815 22:42:39.609303 14909 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0815 22:42:39.609325 14909 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 22:42:39.610144 14942 db_lmdb.cpp:24] Opened lmdb data/val-image-lmdb
I0815 22:42:39.612097 14909 data_layer.cpp:185] (2) ReshapePrefetch 2, 3, 640, 640
I0815 22:42:39.612236 14909 data_layer.cpp:209] (2) Output data size: 2, 3, 640, 640
I0815 22:42:39.612246 14909 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 22:42:39.612285 14909 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0815 22:42:39.612293 14909 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 22:42:39.612387 14908 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.18G, req 0G)
I0815 22:42:39.613189 14943 data_layer.cpp:97] (2) Parser threads: 1
I0815 22:42:39.613203 14943 data_layer.cpp:99] (2) Transformer threads: 1
I0815 22:42:39.615862 14944 db_lmdb.cpp:24] Opened lmdb data/val-label-lmdb
I0815 22:42:39.617158 14909 data_layer.cpp:185] (2) ReshapePrefetch 2, 1, 640, 640
I0815 22:42:39.617266 14909 data_layer.cpp:209] (2) Output data size: 2, 1, 640, 640
I0815 22:42:39.617274 14909 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 22:42:39.618893 14945 data_layer.cpp:97] (2) Parser threads: 1
I0815 22:42:39.618906 14945 data_layer.cpp:99] (2) Transformer threads: 1
I0815 22:42:39.628881 14909 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.18G, req 0G)
I0815 22:42:39.637356 14908 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.15G, req 0G)
I0815 22:42:39.646371 14909 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.15G, req 0G)
I0815 22:42:39.647997 14908 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.13G, req 0G)
I0815 22:42:39.656929 14909 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.13G, req 0G)
I0815 22:42:39.657852 14908 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.12G, req 0G)
I0815 22:42:39.666260 14909 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.12G, req 0G)
I0815 22:42:39.666448 14908 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.1G, req 0G)
I0815 22:42:39.674228 14908 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.09G, req 0G)
I0815 22:42:39.676362 14909 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.1G, req 0G)
I0815 22:42:39.683136 14909 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.09G, req 0G)
I0815 22:42:39.687414 14908 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.09G, req 0G)
I0815 22:42:39.695914 14908 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.08G, req 0G)
I0815 22:42:39.696897 14909 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.09G, req 0G)
I0815 22:42:39.708438 14909 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.08G, req 0G)
I0815 22:42:39.746639 14908 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 7.07G, req 0G)
I0815 22:42:39.753126 14908 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 7.06G, req 0G)
I0815 22:42:39.766201 14909 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 7.07G, req 0G)
I0815 22:42:39.767442 14908 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 7.05G, req 0G)
I0815 22:42:39.771782 14908 solver.cpp:56] Solver scaffolding done.
I0815 22:42:39.774127 14909 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 7.06G, req 0G)
I0815 22:42:39.791589 14909 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 7.05G, req 0G)
I0815 22:42:39.794790 14909 solver.cpp:56] Solver scaffolding done.
I0815 22:42:39.855839 14908 parallel.cpp:161] [1 - 1] P2pSync adding callback
I0815 22:42:39.855839 14909 parallel.cpp:161] [2 - 2] P2pSync adding callback
I0815 22:42:39.855839 14907 parallel.cpp:161] [0 - 0] P2pSync adding callback
I0815 22:42:40.044247 14907 net.cpp:2166] All zero weights of convolution layers are frozen
I0815 22:42:40.063479 14908 solver.cpp:438] Solving jsegnet21v2_train
I0815 22:42:40.063495 14908 solver.cpp:439] Learning Rate Policy: multistep
I0815 22:42:40.063848 14907 solver.cpp:438] Solving jsegnet21v2_train
I0815 22:42:40.063856 14907 solver.cpp:439] Learning Rate Policy: multistep
I0815 22:42:40.064180 14909 solver.cpp:438] Solving jsegnet21v2_train
I0815 22:42:40.064188 14909 solver.cpp:439] Learning Rate Policy: multistep
I0815 22:42:40.079131 14908 solver.cpp:227] Starting Optimization on GPU 1
I0815 22:42:40.079133 14909 solver.cpp:227] Starting Optimization on GPU 2
I0815 22:42:40.079133 14907 solver.cpp:227] Starting Optimization on GPU 0
I0815 22:42:40.079304 14907 solver.cpp:509] Iteration 0, Testing net (#0)
I0815 22:42:40.079308 14960 device_alternate.hpp:116] NVML initialized on thread 128480493819648
I0815 22:42:40.079347 14960 common.cpp:583] NVML succeeded to set CPU affinity on device 1
I0815 22:42:40.079357 14961 device_alternate.hpp:116] NVML initialized on thread 128480485426944
I0815 22:42:40.079372 14961 common.cpp:583] NVML succeeded to set CPU affinity on device 2
I0815 22:42:40.079382 14962 device_alternate.hpp:116] NVML initialized on thread 128480477034240
I0815 22:42:40.079396 14962 common.cpp:583] NVML succeeded to set CPU affinity on device 0
I0815 22:42:40.090656 14909 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 6.95G, req 0G)
I0815 22:42:40.098800 14908 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 6.95G, req 0G)
I0815 22:42:40.113490 14909 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 6.9G, req 0G)
I0815 22:42:40.117839 14908 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 6.9G, req 0G)
I0815 22:42:40.128267 14909 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 6.83G, req 0G)
I0815 22:42:40.132247 14908 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 6.83G, req 0G)
I0815 22:42:40.139905 14909 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 6.8G, req 0G)
I0815 22:42:40.140966 14907 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.01G/1 1  (limit 6.88G, req 0G)
I0815 22:42:40.143402 14908 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 1  (limit 6.8G, req 0G)
I0815 22:42:40.148938 14909 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 6.77G, req 0G)
I0815 22:42:40.152717 14908 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 6.77G, req 0G)
I0815 22:42:40.154927 14909 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 6.75G, req 0G)
I0815 22:42:40.158126 14908 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 6.75G, req 0G)
I0815 22:42:40.162106 14907 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 6.82G, req 0G)
I0815 22:42:40.165175 14909 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.74G, req 0G)
I0815 22:42:40.166488 14908 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.74G, req 0G)
I0815 22:42:40.170428 14909 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.73G, req 0G)
I0815 22:42:40.170855 14908 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.73G, req 0G)
I0815 22:42:40.176751 14907 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 6.75G, req 0G)
I0815 22:42:40.185240 14907 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 6.73G, req 0G)
I0815 22:42:40.192723 14907 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 6.69G, req 0G)
I0815 22:42:40.198454 14907 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 6.67G, req 0G)
I0815 22:42:40.198678 14908 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.58G, req 0G)
I0815 22:42:40.198902 14909 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.58G, req 0G)
I0815 22:42:40.205545 14908 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.57G, req 0G)
I0815 22:42:40.205971 14909 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.57G, req 0G)
I0815 22:42:40.206626 14907 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.66G, req 0G)
I0815 22:42:40.213539 14907 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.65G, req 0G)
I0815 22:42:40.234558 14908 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.47G, req 0G)
I0815 22:42:40.235227 14909 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.47G, req 0G)
I0815 22:42:40.246263 14907 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.5G, req 0G)
I0815 22:42:40.252600 14907 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.49G, req 0G)
I0815 22:42:40.276795 14907 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.39G, req 0G)
I0815 22:42:40.379930 14907 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.896719
I0815 22:42:40.379973 14907 solver.cpp:594]     Test net output #1: accuracy/top5 = 1
I0815 22:42:40.379984 14907 solver.cpp:594]     Test net output #2: loss = 0.299862 (* 1 = 0.299862 loss)
I0815 22:42:40.379992 14907 solver.cpp:254] [MultiGPU] Initial Test completed
I0815 22:42:40.478572 14907 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 6.19G, req 0G)
I0815 22:42:40.485781 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 6.27G, req 0G)
I0815 22:42:40.486399 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 6.27G, req 0G)
I0815 22:42:40.526414 14907 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 6.03G, req 0G)
I0815 22:42:40.540319 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 6.11G, req 0G)
I0815 22:42:40.540871 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 6.11G, req 0G)
I0815 22:42:40.569708 14907 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 5.85G, req 0G)
I0815 22:42:40.593789 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 5.93G, req 0G)
I0815 22:42:40.595212 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 5.93G, req 0G)
I0815 22:42:40.595358 14907 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 5.77G, req 0G)
I0815 22:42:40.621081 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 5.85G, req 0G)
I0815 22:42:40.621309 14907 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 5.68G, req 0G)
I0815 22:42:40.621474 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 5.85G, req 0G)
I0815 22:42:40.634471 14907 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 5.64G, req 0G)
I0815 22:42:40.646179 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 5.76G, req 0G)
I0815 22:42:40.646410 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 5.76G, req 0G)
I0815 22:42:40.654546 14907 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 5.61G, req 0G)
I0815 22:42:40.659328 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 5.72G, req 0G)
I0815 22:42:40.659647 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 5.72G, req 0G)
I0815 22:42:40.661916 14907 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 5.59G, req 0G)
I0815 22:42:40.682024 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 5.69G, req 0G)
I0815 22:42:40.683169 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 5.69G, req 0G)
I0815 22:42:40.694177 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 5.67G, req 0G)
I0815 22:42:40.696897 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 5.67G, req 0G)
I0815 22:42:40.727841 14907 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 5.32G, req 0G)
I0815 22:42:40.745527 14907 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 5.3G, req 0G)
I0815 22:42:40.774261 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 5.4G, req 0G)
I0815 22:42:40.776185 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 5.4G, req 0G)
I0815 22:42:40.780616 14907 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 5.14G, req 0G)
I0815 22:42:40.788974 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 5.38G, req 0G)
I0815 22:42:40.790817 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 5.38G, req 0G)
I0815 22:42:40.824991 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 5.23G, req 0G)
I0815 22:42:40.825572 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 5.23G, req 0G)
I0815 22:42:41.056231 14907 solver.cpp:317] Iteration 0 (0.676165 s), loss = 0.0815038
I0815 22:42:41.056294 14907 solver.cpp:334]     Train net output #0: loss = 0.0815038 (* 1 = 0.0815038 loss)
I0815 22:42:41.056313 14907 sgd_solver.cpp:136] Iteration 0, lr = 1e-05, m = 0.9
I0815 22:42:41.265873 14907 solver.cpp:317] Iteration 1 (0.209639 s), loss = 0.106995
I0815 22:42:41.265903 14907 solver.cpp:334]     Train net output #0: loss = 0.106995 (* 1 = 0.106995 loss)
I0815 22:42:41.369432 14907 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1a' with space 1.29G/1 1 0 0  (limit 2.98G, req 0G)
I0815 22:42:41.370842 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1a' with space 1.29G/1 1 0 3  (limit 3.06G, req 0G)
I0815 22:42:41.379101 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1a' with space 1.29G/1 1 0 3  (limit 3.06G, req 0G)
I0815 22:42:41.429605 14907 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1b' with space 2.57G/2 6 4 3  (limit 1.69G, req 0G)
I0815 22:42:41.438313 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1b' with space 2.57G/2 6 4 3  (limit 1.78G, req 0G)
I0815 22:42:41.453783 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1b' with space 2.57G/2 6 4 3  (limit 1.78G, req 0G)
I0815 22:42:41.556071 14907 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 2.57G/1 6 4 3  (limit 1.69G, req 0G)
I0815 22:42:41.571681 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 2.57G/1 6 4 3  (limit 1.78G, req 0G)
I0815 22:42:41.587641 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 2.57G/1 6 4 3  (limit 1.78G, req 0G)
I0815 22:42:41.596122 14907 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 2.57G/2 6 4 3  (limit 1.69G, req 0G)
I0815 22:42:41.614511 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 2.57G/2 6 4 3  (limit 1.78G, req 0G)
I0815 22:42:41.630661 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 2.57G/2 6 4 3  (limit 1.78G, req 0G)
I0815 22:42:41.689532 14907 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 2.57G/1 6 4 5  (limit 1.69G, req 0.07G)
I0815 22:42:41.712231 14907 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 2.57G/2 6 4 3  (limit 1.69G, req 0.07G)
I0815 22:42:41.713217 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 2.57G/1 6 4 5  (limit 1.78G, req 0.07G)
I0815 22:42:41.730388 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 2.57G/1 6 4 5  (limit 1.78G, req 0.07G)
I0815 22:42:41.736629 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 2.57G/2 6 4 3  (limit 1.78G, req 0.07G)
I0815 22:42:41.754385 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 2.57G/2 6 4 3  (limit 1.78G, req 0.07G)
I0815 22:42:41.775269 14907 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 2.57G/1 6 4 5  (limit 1.69G, req 0.07G)
I0815 22:42:41.788725 14907 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 2.57G/2 6 4 3  (limit 1.69G, req 0.07G)
I0815 22:42:41.806126 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 2.57G/1 6 4 5  (limit 1.78G, req 0.07G)
I0815 22:42:41.820251 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 2.57G/2 6 4 3  (limit 1.78G, req 0.07G)
I0815 22:42:41.823521 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 2.57G/1 6 4 5  (limit 1.78G, req 0.07G)
I0815 22:42:41.836963 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 2.57G/2 6 4 3  (limit 1.78G, req 0.07G)
I0815 22:42:41.841683 14907 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'out3a' with space 2.57G/2 6 4 3  (limit 1.69G, req 0.07G)
I0815 22:42:41.874892 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'out3a' with space 2.57G/2 6 4 3  (limit 1.78G, req 0.07G)
I0815 22:42:41.892287 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'out3a' with space 2.57G/2 6 4 3  (limit 1.78G, req 0.07G)
I0815 22:42:41.895052 14907 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 2.57G/1 6 4 3  (limit 1.69G, req 0.07G)
I0815 22:42:41.922404 14907 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 2.57G/1 6 1 5  (limit 1.69G, req 0.07G)
I0815 22:42:41.931397 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 2.57G/1 6 4 3  (limit 1.78G, req 0.07G)
I0815 22:42:41.948889 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 2.57G/1 6 4 3  (limit 1.78G, req 0.07G)
I0815 22:42:41.959044 14909 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 2.57G/1 6 1 5  (limit 1.78G, req 0.07G)
I0815 22:42:41.976095 14908 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 2.57G/1 6 1 5  (limit 1.78G, req 0.07G)
I0815 22:42:42.106993 14907 solver.cpp:317] Iteration 2 (0.841092 s), loss = 0.066865
I0815 22:42:42.107014 14907 solver.cpp:334]     Train net output #0: loss = 0.066865 (* 1 = 0.066865 loss)
I0815 22:42:42.107635 14909 cudnn_conv_layer.cpp:292] [2] Layer 'conv1a' reallocating workspace: 2.57G -> 0.14G
I0815 22:42:42.107638 14908 cudnn_conv_layer.cpp:292] [1] Layer 'conv1a' reallocating workspace: 2.57G -> 0.14G
I0815 22:42:42.125629 14907 cudnn_conv_layer.cpp:292] [0] Layer 'conv1a' reallocating workspace: 2.57G -> 0.14G
I0815 22:43:01.341945 14907 solver.cpp:312] Iteration 100 (5.09503 iter/s, 19.2344s/98 iter), loss = 0.0686244
I0815 22:43:01.341969 14907 solver.cpp:334]     Train net output #0: loss = 0.0686244 (* 1 = 0.0686244 loss)
I0815 22:43:01.341974 14907 sgd_solver.cpp:136] Iteration 100, lr = 1e-05, m = 0.9
I0815 22:43:13.654039 14872 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 22:43:21.066071 14907 solver.cpp:312] Iteration 200 (5.07007 iter/s, 19.7236s/100 iter), loss = 0.0976446
I0815 22:43:21.066092 14907 solver.cpp:334]     Train net output #0: loss = 0.0976446 (* 1 = 0.0976446 loss)
I0815 22:43:21.066097 14907 sgd_solver.cpp:136] Iteration 200, lr = 1e-05, m = 0.9
I0815 22:43:40.758436 14907 solver.cpp:312] Iteration 300 (5.07825 iter/s, 19.6918s/100 iter), loss = 0.0637274
I0815 22:43:40.758461 14907 solver.cpp:334]     Train net output #0: loss = 0.0637274 (* 1 = 0.0637274 loss)
I0815 22:43:40.758466 14907 sgd_solver.cpp:136] Iteration 300, lr = 1e-05, m = 0.9
I0815 22:43:46.044625 14870 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 22:44:00.336427 14907 solver.cpp:312] Iteration 400 (5.10792 iter/s, 19.5774s/100 iter), loss = 0.0752905
I0815 22:44:00.336449 14907 solver.cpp:334]     Train net output #0: loss = 0.0752905 (* 1 = 0.0752905 loss)
I0815 22:44:00.336454 14907 sgd_solver.cpp:136] Iteration 400, lr = 1e-05, m = 0.9
I0815 22:44:19.959313 14907 solver.cpp:312] Iteration 500 (5.09623 iter/s, 19.6223s/100 iter), loss = 0.0865745
I0815 22:44:19.959381 14907 solver.cpp:334]     Train net output #0: loss = 0.0865745 (* 1 = 0.0865745 loss)
I0815 22:44:19.959388 14907 sgd_solver.cpp:136] Iteration 500, lr = 1e-05, m = 0.9
I0815 22:44:39.328218 14907 solver.cpp:312] Iteration 600 (5.16306 iter/s, 19.3684s/100 iter), loss = 0.0658122
I0815 22:44:39.328243 14907 solver.cpp:334]     Train net output #0: loss = 0.0658122 (* 1 = 0.0658122 loss)
I0815 22:44:39.328248 14907 sgd_solver.cpp:136] Iteration 600, lr = 1e-05, m = 0.9
I0815 22:44:50.650686 14916 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 22:44:58.887686 14907 solver.cpp:312] Iteration 700 (5.11275 iter/s, 19.5589s/100 iter), loss = 0.0645232
I0815 22:44:58.887711 14907 solver.cpp:334]     Train net output #0: loss = 0.0645232 (* 1 = 0.0645232 loss)
I0815 22:44:58.887714 14907 sgd_solver.cpp:136] Iteration 700, lr = 1e-05, m = 0.9
I0815 22:45:18.239030 14907 solver.cpp:312] Iteration 800 (5.16774 iter/s, 19.3508s/100 iter), loss = 0.0804667
I0815 22:45:18.239053 14907 solver.cpp:334]     Train net output #0: loss = 0.0804667 (* 1 = 0.0804667 loss)
I0815 22:45:18.239058 14907 sgd_solver.cpp:136] Iteration 800, lr = 1e-05, m = 0.9
I0815 22:45:37.573274 14907 solver.cpp:312] Iteration 900 (5.17231 iter/s, 19.3337s/100 iter), loss = 0.0687623
I0815 22:45:37.573351 14907 solver.cpp:334]     Train net output #0: loss = 0.0687622 (* 1 = 0.0687622 loss)
I0815 22:45:37.573359 14907 sgd_solver.cpp:136] Iteration 900, lr = 1e-05, m = 0.9
I0815 22:45:54.648846 14918 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 22:45:56.796686 14907 solver.cpp:363] Sparsity after update:
I0815 22:45:56.823575 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0815 22:45:56.823614 14907 net.cpp:2192] conv1a_param_0(0) 
I0815 22:45:56.823637 14907 net.cpp:2192] conv1b_param_0(0) 
I0815 22:45:56.823662 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0815 22:45:56.823675 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0815 22:45:56.823688 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0815 22:45:56.823699 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0815 22:45:56.823710 14907 net.cpp:2192] ctx_final_param_0(0) 
I0815 22:45:56.823722 14907 net.cpp:2192] out3a_param_0(0) 
I0815 22:45:56.823735 14907 net.cpp:2192] out5a_param_0(0) 
I0815 22:45:56.823747 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 22:45:56.823760 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 22:45:56.823770 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 22:45:56.823782 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 22:45:56.823793 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 22:45:56.823803 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 22:45:56.823815 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 22:45:56.823825 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 22:45:56.823837 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0815 22:45:57.017827 14907 solver.cpp:312] Iteration 1000 (5.14297 iter/s, 19.444s/100 iter), loss = 0.0936237
I0815 22:45:57.017853 14907 solver.cpp:334]     Train net output #0: loss = 0.0936236 (* 1 = 0.0936236 loss)
I0815 22:45:57.017856 14907 sgd_solver.cpp:136] Iteration 1000, lr = 1e-05, m = 0.9
I0815 22:46:16.472687 14907 solver.cpp:312] Iteration 1100 (5.14025 iter/s, 19.4543s/100 iter), loss = 0.0615981
I0815 22:46:16.472740 14907 solver.cpp:334]     Train net output #0: loss = 0.061598 (* 1 = 0.061598 loss)
I0815 22:46:16.472748 14907 sgd_solver.cpp:136] Iteration 1100, lr = 1e-05, m = 0.9
I0815 22:46:26.689368 14916 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 22:46:35.830919 14907 solver.cpp:312] Iteration 1200 (5.1659 iter/s, 19.3577s/100 iter), loss = 0.0758057
I0815 22:46:35.830948 14907 solver.cpp:334]     Train net output #0: loss = 0.0758057 (* 1 = 0.0758057 loss)
I0815 22:46:35.830952 14907 sgd_solver.cpp:136] Iteration 1200, lr = 1e-05, m = 0.9
I0815 22:46:55.302230 14907 solver.cpp:312] Iteration 1300 (5.1359 iter/s, 19.4708s/100 iter), loss = 0.0853191
I0815 22:46:55.302299 14907 solver.cpp:334]     Train net output #0: loss = 0.085319 (* 1 = 0.085319 loss)
I0815 22:46:55.302305 14907 sgd_solver.cpp:136] Iteration 1300, lr = 1e-05, m = 0.9
I0815 22:47:14.772589 14907 solver.cpp:312] Iteration 1400 (5.13615 iter/s, 19.4698s/100 iter), loss = 0.10685
I0815 22:47:14.772617 14907 solver.cpp:334]     Train net output #0: loss = 0.106849 (* 1 = 0.106849 loss)
I0815 22:47:14.772624 14907 sgd_solver.cpp:136] Iteration 1400, lr = 1e-05, m = 0.9
I0815 22:47:31.254215 14870 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 22:47:34.286036 14907 solver.cpp:312] Iteration 1500 (5.12481 iter/s, 19.5129s/100 iter), loss = 0.0866053
I0815 22:47:34.286056 14907 solver.cpp:334]     Train net output #0: loss = 0.0866053 (* 1 = 0.0866053 loss)
I0815 22:47:34.286061 14907 sgd_solver.cpp:136] Iteration 1500, lr = 1e-05, m = 0.9
I0815 22:47:53.923362 14907 solver.cpp:312] Iteration 1600 (5.09248 iter/s, 19.6368s/100 iter), loss = 0.0663926
I0815 22:47:53.923384 14907 solver.cpp:334]     Train net output #0: loss = 0.0663925 (* 1 = 0.0663925 loss)
I0815 22:47:53.923388 14907 sgd_solver.cpp:136] Iteration 1600, lr = 1e-05, m = 0.9
I0815 22:48:13.516434 14907 solver.cpp:312] Iteration 1700 (5.10399 iter/s, 19.5925s/100 iter), loss = 0.0921687
I0815 22:48:13.516485 14907 solver.cpp:334]     Train net output #0: loss = 0.0921686 (* 1 = 0.0921686 loss)
I0815 22:48:13.516492 14907 sgd_solver.cpp:136] Iteration 1700, lr = 1e-05, m = 0.9
I0815 22:48:32.992612 14907 solver.cpp:312] Iteration 1800 (5.13462 iter/s, 19.4756s/100 iter), loss = 0.0580605
I0815 22:48:32.992637 14907 solver.cpp:334]     Train net output #0: loss = 0.0580604 (* 1 = 0.0580604 loss)
I0815 22:48:32.992641 14907 sgd_solver.cpp:136] Iteration 1800, lr = 1e-05, m = 0.9
I0815 22:48:35.903399 14918 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 22:48:52.329174 14907 solver.cpp:312] Iteration 1900 (5.17169 iter/s, 19.336s/100 iter), loss = 0.0500722
I0815 22:48:52.336280 14907 solver.cpp:334]     Train net output #0: loss = 0.0500722 (* 1 = 0.0500722 loss)
I0815 22:48:52.336331 14907 sgd_solver.cpp:136] Iteration 1900, lr = 1e-05, m = 0.9
I0815 22:49:08.012506 14870 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 22:49:11.682070 14907 solver.cpp:363] Sparsity after update:
I0815 22:49:11.691974 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0815 22:49:11.692030 14907 net.cpp:2192] conv1a_param_0(0) 
I0815 22:49:11.692059 14907 net.cpp:2192] conv1b_param_0(0) 
I0815 22:49:11.692066 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0815 22:49:11.692076 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0815 22:49:11.692085 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0815 22:49:11.692090 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0815 22:49:11.692096 14907 net.cpp:2192] ctx_final_param_0(0) 
I0815 22:49:11.692101 14907 net.cpp:2192] out3a_param_0(0) 
I0815 22:49:11.692106 14907 net.cpp:2192] out5a_param_0(0) 
I0815 22:49:11.692113 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 22:49:11.692121 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 22:49:11.692143 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 22:49:11.692152 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 22:49:11.692160 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 22:49:11.692167 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 22:49:11.692173 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 22:49:11.692181 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 22:49:11.692188 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0815 22:49:11.692205 14907 solver.cpp:509] Iteration 2000, Testing net (#0)
I0815 22:49:24.000157 14907 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.953107
I0815 22:49:24.000216 14907 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999531
I0815 22:49:24.000222 14907 solver.cpp:594]     Test net output #2: loss = 0.181906 (* 1 = 0.181906 loss)
I0815 22:49:24.000244 14907 solver.cpp:264] [MultiGPU] Tests completed in 12.3077s
I0815 22:49:24.216711 14907 solver.cpp:312] Iteration 2000 (3.13611 iter/s, 31.8867s/100 iter), loss = 0.0583783
I0815 22:49:24.216740 14907 solver.cpp:334]     Train net output #0: loss = 0.0583783 (* 1 = 0.0583783 loss)
I0815 22:49:24.216745 14907 sgd_solver.cpp:136] Iteration 2000, lr = 1e-05, m = 0.9
I0815 22:49:43.916226 14907 solver.cpp:312] Iteration 2100 (5.07641 iter/s, 19.699s/100 iter), loss = 0.0676623
I0815 22:49:43.916357 14907 solver.cpp:334]     Train net output #0: loss = 0.0676623 (* 1 = 0.0676623 loss)
I0815 22:49:43.916374 14907 sgd_solver.cpp:136] Iteration 2100, lr = 1e-05, m = 0.9
I0815 22:49:52.864326 14915 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 22:50:03.459097 14907 solver.cpp:312] Iteration 2200 (5.11709 iter/s, 19.5423s/100 iter), loss = 0.120557
I0815 22:50:03.459161 14907 solver.cpp:334]     Train net output #0: loss = 0.120557 (* 1 = 0.120557 loss)
I0815 22:50:03.459166 14907 sgd_solver.cpp:136] Iteration 2200, lr = 1e-05, m = 0.9
I0815 22:50:23.080025 14907 solver.cpp:312] Iteration 2300 (5.09674 iter/s, 19.6204s/100 iter), loss = 0.0684635
I0815 22:50:23.080047 14907 solver.cpp:334]     Train net output #0: loss = 0.0684635 (* 1 = 0.0684635 loss)
I0815 22:50:23.080051 14907 sgd_solver.cpp:136] Iteration 2300, lr = 1e-05, m = 0.9
I0815 22:50:42.554600 14907 solver.cpp:312] Iteration 2400 (5.13504 iter/s, 19.474s/100 iter), loss = 0.0598489
I0815 22:50:42.554675 14907 solver.cpp:334]     Train net output #0: loss = 0.0598489 (* 1 = 0.0598489 loss)
I0815 22:50:42.554683 14907 sgd_solver.cpp:136] Iteration 2400, lr = 1e-05, m = 0.9
I0815 22:50:57.440361 14872 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 22:51:02.194344 14907 solver.cpp:312] Iteration 2500 (5.09186 iter/s, 19.6392s/100 iter), loss = 0.0856105
I0815 22:51:02.194367 14907 solver.cpp:334]     Train net output #0: loss = 0.0856104 (* 1 = 0.0856104 loss)
I0815 22:51:02.194373 14907 sgd_solver.cpp:136] Iteration 2500, lr = 1e-05, m = 0.9
I0815 22:51:21.817829 14907 solver.cpp:312] Iteration 2600 (5.09608 iter/s, 19.6229s/100 iter), loss = 0.0766372
I0815 22:51:21.817910 14907 solver.cpp:334]     Train net output #0: loss = 0.0766372 (* 1 = 0.0766372 loss)
I0815 22:51:21.817919 14907 sgd_solver.cpp:136] Iteration 2600, lr = 1e-05, m = 0.9
I0815 22:51:29.945611 14870 data_reader.cpp:288] Starting prefetch of epoch 4
I0815 22:51:41.358024 14907 solver.cpp:312] Iteration 2700 (5.1178 iter/s, 19.5397s/100 iter), loss = 0.0653204
I0815 22:51:41.358049 14907 solver.cpp:334]     Train net output #0: loss = 0.0653204 (* 1 = 0.0653204 loss)
I0815 22:51:41.358055 14907 sgd_solver.cpp:136] Iteration 2700, lr = 1e-05, m = 0.9
I0815 22:52:00.715930 14907 solver.cpp:312] Iteration 2800 (5.16599 iter/s, 19.3574s/100 iter), loss = 0.0598765
I0815 22:52:00.715978 14907 solver.cpp:334]     Train net output #0: loss = 0.0598765 (* 1 = 0.0598765 loss)
I0815 22:52:00.715983 14907 sgd_solver.cpp:136] Iteration 2800, lr = 1e-05, m = 0.9
I0815 22:52:20.248504 14907 solver.cpp:312] Iteration 2900 (5.11979 iter/s, 19.532s/100 iter), loss = 0.0834499
I0815 22:52:20.248531 14907 solver.cpp:334]     Train net output #0: loss = 0.0834498 (* 1 = 0.0834498 loss)
I0815 22:52:20.248538 14907 sgd_solver.cpp:136] Iteration 2900, lr = 1e-05, m = 0.9
I0815 22:52:34.267211 14870 data_reader.cpp:288] Starting prefetch of epoch 5
I0815 22:52:39.787719 14907 solver.cpp:363] Sparsity after update:
I0815 22:52:39.805766 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0815 22:52:39.806150 14907 net.cpp:2192] conv1a_param_0(0) 
I0815 22:52:39.806267 14907 net.cpp:2192] conv1b_param_0(0) 
I0815 22:52:39.806354 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0815 22:52:39.806435 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0815 22:52:39.806517 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0815 22:52:39.806599 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0815 22:52:39.806681 14907 net.cpp:2192] ctx_final_param_0(0) 
I0815 22:52:39.806761 14907 net.cpp:2192] out3a_param_0(0) 
I0815 22:52:39.806839 14907 net.cpp:2192] out5a_param_0(0) 
I0815 22:52:39.806918 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 22:52:39.806999 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 22:52:39.807080 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 22:52:39.807160 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 22:52:39.807240 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 22:52:39.807322 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 22:52:39.807409 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 22:52:39.807492 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 22:52:39.807581 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0815 22:52:40.004289 14907 solver.cpp:312] Iteration 3000 (5.06195 iter/s, 19.7552s/100 iter), loss = 0.0853676
I0815 22:52:40.004313 14907 solver.cpp:334]     Train net output #0: loss = 0.0853676 (* 1 = 0.0853676 loss)
I0815 22:52:40.004318 14907 sgd_solver.cpp:136] Iteration 3000, lr = 1e-05, m = 0.9
I0815 22:52:59.686667 14907 solver.cpp:312] Iteration 3100 (5.08083 iter/s, 19.6818s/100 iter), loss = 0.0427826
I0815 22:52:59.686697 14907 solver.cpp:334]     Train net output #0: loss = 0.0427826 (* 1 = 0.0427826 loss)
I0815 22:52:59.686703 14907 sgd_solver.cpp:136] Iteration 3100, lr = 1e-05, m = 0.9
I0815 22:53:19.285200 14907 solver.cpp:312] Iteration 3200 (5.10256 iter/s, 19.598s/100 iter), loss = 0.124939
I0815 22:53:19.285266 14907 solver.cpp:334]     Train net output #0: loss = 0.124939 (* 1 = 0.124939 loss)
I0815 22:53:19.285272 14907 sgd_solver.cpp:136] Iteration 3200, lr = 1e-05, m = 0.9
I0815 22:53:38.839561 14907 solver.cpp:312] Iteration 3300 (5.11409 iter/s, 19.5538s/100 iter), loss = 0.0616888
I0815 22:53:38.839587 14907 solver.cpp:334]     Train net output #0: loss = 0.0616888 (* 1 = 0.0616888 loss)
I0815 22:53:38.839593 14907 sgd_solver.cpp:136] Iteration 3300, lr = 1e-05, m = 0.9
I0815 22:53:39.250320 14920 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 22:53:58.335009 14907 solver.cpp:312] Iteration 3400 (5.12954 iter/s, 19.4949s/100 iter), loss = 0.0444929
I0815 22:53:58.340167 14907 solver.cpp:334]     Train net output #0: loss = 0.0444929 (* 1 = 0.0444929 loss)
I0815 22:53:58.340184 14907 sgd_solver.cpp:136] Iteration 3400, lr = 1e-05, m = 0.9
I0815 22:54:17.797096 14907 solver.cpp:312] Iteration 3500 (5.13834 iter/s, 19.4616s/100 iter), loss = 0.0751494
I0815 22:54:17.797118 14907 solver.cpp:334]     Train net output #0: loss = 0.0751494 (* 1 = 0.0751494 loss)
I0815 22:54:17.797124 14907 sgd_solver.cpp:136] Iteration 3500, lr = 1e-05, m = 0.9
I0815 22:54:37.255991 14907 solver.cpp:312] Iteration 3600 (5.13918 iter/s, 19.4584s/100 iter), loss = 0.0727355
I0815 22:54:37.256044 14907 solver.cpp:334]     Train net output #0: loss = 0.0727355 (* 1 = 0.0727355 loss)
I0815 22:54:37.256049 14907 sgd_solver.cpp:136] Iteration 3600, lr = 1e-05, m = 0.9
I0815 22:54:43.644078 14920 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 22:54:56.902101 14907 solver.cpp:312] Iteration 3700 (5.09021 iter/s, 19.6456s/100 iter), loss = 0.0723288
I0815 22:54:56.902124 14907 solver.cpp:334]     Train net output #0: loss = 0.0723288 (* 1 = 0.0723288 loss)
I0815 22:54:56.902129 14907 sgd_solver.cpp:136] Iteration 3700, lr = 1e-05, m = 0.9
I0815 22:55:16.268573 14870 data_reader.cpp:288] Starting prefetch of epoch 6
I0815 22:55:16.622700 14907 solver.cpp:312] Iteration 3800 (5.07098 iter/s, 19.7201s/100 iter), loss = 0.0794883
I0815 22:55:16.622725 14907 solver.cpp:334]     Train net output #0: loss = 0.0794883 (* 1 = 0.0794883 loss)
I0815 22:55:16.622730 14907 sgd_solver.cpp:136] Iteration 3800, lr = 1e-05, m = 0.9
I0815 22:55:35.985396 14907 solver.cpp:312] Iteration 3900 (5.16471 iter/s, 19.3622s/100 iter), loss = 0.0614188
I0815 22:55:35.985415 14907 solver.cpp:334]     Train net output #0: loss = 0.0614188 (* 1 = 0.0614188 loss)
I0815 22:55:35.985419 14907 sgd_solver.cpp:136] Iteration 3900, lr = 1e-05, m = 0.9
I0815 22:55:55.248708 14907 solver.cpp:363] Sparsity after update:
I0815 22:55:55.263154 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0815 22:55:55.263172 14907 net.cpp:2192] conv1a_param_0(0) 
I0815 22:55:55.263181 14907 net.cpp:2192] conv1b_param_0(0) 
I0815 22:55:55.263185 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0815 22:55:55.263187 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0815 22:55:55.263190 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0815 22:55:55.263195 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0815 22:55:55.263198 14907 net.cpp:2192] ctx_final_param_0(0) 
I0815 22:55:55.263200 14907 net.cpp:2192] out3a_param_0(0) 
I0815 22:55:55.263203 14907 net.cpp:2192] out5a_param_0(0) 
I0815 22:55:55.263206 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 22:55:55.263209 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 22:55:55.263212 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 22:55:55.263216 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 22:55:55.263218 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 22:55:55.263221 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 22:55:55.263226 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 22:55:55.263229 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 22:55:55.263232 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0815 22:55:55.263245 14907 solver.cpp:509] Iteration 4000, Testing net (#0)
I0815 22:55:58.885042 14890 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 22:56:07.345016 14907 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.951925
I0815 22:56:07.345036 14907 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999899
I0815 22:56:07.345041 14907 solver.cpp:594]     Test net output #2: loss = 0.161988 (* 1 = 0.161988 loss)
I0815 22:56:07.345067 14907 solver.cpp:264] [MultiGPU] Tests completed in 12.0815s
I0815 22:56:07.552745 14907 solver.cpp:312] Iteration 4000 (3.16792 iter/s, 31.5665s/100 iter), loss = 0.0564036
I0815 22:56:07.552769 14907 solver.cpp:334]     Train net output #0: loss = 0.0564036 (* 1 = 0.0564036 loss)
I0815 22:56:07.552773 14907 sgd_solver.cpp:136] Iteration 4000, lr = 1e-05, m = 0.9
I0815 22:56:26.949587 14907 solver.cpp:312] Iteration 4100 (5.15562 iter/s, 19.3963s/100 iter), loss = 0.0878937
I0815 22:56:26.949657 14907 solver.cpp:334]     Train net output #0: loss = 0.0878937 (* 1 = 0.0878937 loss)
I0815 22:56:26.949666 14907 sgd_solver.cpp:136] Iteration 4100, lr = 1e-05, m = 0.9
I0815 22:56:32.450440 14920 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 22:56:46.377604 14907 solver.cpp:312] Iteration 4200 (5.14735 iter/s, 19.4275s/100 iter), loss = 0.0718417
I0815 22:56:46.377634 14907 solver.cpp:334]     Train net output #0: loss = 0.0718417 (* 1 = 0.0718417 loss)
I0815 22:56:46.377641 14907 sgd_solver.cpp:136] Iteration 4200, lr = 1e-05, m = 0.9
I0815 22:57:05.022186 14915 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 22:57:06.193083 14907 solver.cpp:312] Iteration 4300 (5.0467 iter/s, 19.8149s/100 iter), loss = 0.0941952
I0815 22:57:06.193109 14907 solver.cpp:334]     Train net output #0: loss = 0.0941952 (* 1 = 0.0941952 loss)
I0815 22:57:06.193114 14907 sgd_solver.cpp:136] Iteration 4300, lr = 1e-05, m = 0.9
I0815 22:57:25.773568 14907 solver.cpp:312] Iteration 4400 (5.10727 iter/s, 19.5799s/100 iter), loss = 0.0549483
I0815 22:57:25.773591 14907 solver.cpp:334]     Train net output #0: loss = 0.0549483 (* 1 = 0.0549483 loss)
I0815 22:57:25.773596 14907 sgd_solver.cpp:136] Iteration 4400, lr = 1e-05, m = 0.9
I0815 22:57:45.270709 14907 solver.cpp:312] Iteration 4500 (5.1291 iter/s, 19.4966s/100 iter), loss = 0.0456376
I0815 22:57:45.270756 14907 solver.cpp:334]     Train net output #0: loss = 0.0456377 (* 1 = 0.0456377 loss)
I0815 22:57:45.270761 14907 sgd_solver.cpp:136] Iteration 4500, lr = 1e-05, m = 0.9
I0815 22:58:04.690323 14907 solver.cpp:312] Iteration 4600 (5.14957 iter/s, 19.4191s/100 iter), loss = 0.104379
I0815 22:58:04.690354 14907 solver.cpp:334]     Train net output #0: loss = 0.104379 (* 1 = 0.104379 loss)
I0815 22:58:04.690361 14907 sgd_solver.cpp:136] Iteration 4600, lr = 1e-05, m = 0.9
I0815 22:58:09.471403 14916 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 22:58:24.092232 14907 solver.cpp:312] Iteration 4700 (5.15427 iter/s, 19.4014s/100 iter), loss = 0.0703168
I0815 22:58:24.092284 14907 solver.cpp:334]     Train net output #0: loss = 0.0703169 (* 1 = 0.0703169 loss)
I0815 22:58:24.092290 14907 sgd_solver.cpp:136] Iteration 4700, lr = 1e-05, m = 0.9
I0815 22:58:43.965000 14907 solver.cpp:312] Iteration 4800 (5.03215 iter/s, 19.8722s/100 iter), loss = 0.0764964
I0815 22:58:43.965034 14907 solver.cpp:334]     Train net output #0: loss = 0.0764965 (* 1 = 0.0764965 loss)
I0815 22:58:43.965041 14907 sgd_solver.cpp:136] Iteration 4800, lr = 1e-05, m = 0.9
I0815 22:59:03.653935 14907 solver.cpp:312] Iteration 4900 (5.07913 iter/s, 19.6884s/100 iter), loss = 0.0702351
I0815 22:59:03.656162 14907 solver.cpp:334]     Train net output #0: loss = 0.0702352 (* 1 = 0.0702352 loss)
I0815 22:59:03.656172 14907 sgd_solver.cpp:136] Iteration 4900, lr = 1e-05, m = 0.9
I0815 22:59:14.468473 14918 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 22:59:23.072685 14907 solver.cpp:363] Sparsity after update:
I0815 22:59:23.074623 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0815 22:59:23.074650 14907 net.cpp:2192] conv1a_param_0(0) 
I0815 22:59:23.074661 14907 net.cpp:2192] conv1b_param_0(0) 
I0815 22:59:23.074663 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0815 22:59:23.074666 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0815 22:59:23.074669 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0815 22:59:23.074684 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0815 22:59:23.074695 14907 net.cpp:2192] ctx_final_param_0(0) 
I0815 22:59:23.074703 14907 net.cpp:2192] out3a_param_0(0) 
I0815 22:59:23.074712 14907 net.cpp:2192] out5a_param_0(0) 
I0815 22:59:23.074720 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 22:59:23.074730 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 22:59:23.074738 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 22:59:23.074748 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 22:59:23.074754 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 22:59:23.074764 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 22:59:23.074771 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 22:59:23.074779 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 22:59:23.074789 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0815 22:59:23.267906 14907 solver.cpp:312] Iteration 5000 (5.09855 iter/s, 19.6134s/100 iter), loss = 0.0795705
I0815 22:59:23.267930 14907 solver.cpp:334]     Train net output #0: loss = 0.0795706 (* 1 = 0.0795706 loss)
I0815 22:59:23.267935 14907 sgd_solver.cpp:136] Iteration 5000, lr = 1e-05, m = 0.9
I0815 22:59:42.678761 14907 solver.cpp:312] Iteration 5100 (5.1519 iter/s, 19.4103s/100 iter), loss = 0.110932
I0815 22:59:42.679008 14907 solver.cpp:334]     Train net output #0: loss = 0.110932 (* 1 = 0.110932 loss)
I0815 22:59:42.679016 14907 sgd_solver.cpp:136] Iteration 5100, lr = 1e-05, m = 0.9
I0815 22:59:46.756242 14916 data_reader.cpp:288] Starting prefetch of epoch 4
I0815 23:00:02.266089 14907 solver.cpp:312] Iteration 5200 (5.10548 iter/s, 19.5868s/100 iter), loss = 0.0796664
I0815 23:00:02.266111 14907 solver.cpp:334]     Train net output #0: loss = 0.0796664 (* 1 = 0.0796664 loss)
I0815 23:00:02.266115 14907 sgd_solver.cpp:136] Iteration 5200, lr = 1e-05, m = 0.9
I0815 23:00:21.637356 14907 solver.cpp:312] Iteration 5300 (5.16243 iter/s, 19.3707s/100 iter), loss = 0.0495893
I0815 23:00:21.637430 14907 solver.cpp:334]     Train net output #0: loss = 0.0495893 (* 1 = 0.0495893 loss)
I0815 23:00:21.637437 14907 sgd_solver.cpp:136] Iteration 5300, lr = 1e-05, m = 0.9
I0815 23:00:41.357671 14907 solver.cpp:312] Iteration 5400 (5.07105 iter/s, 19.7198s/100 iter), loss = 0.0557451
I0815 23:00:41.357693 14907 solver.cpp:334]     Train net output #0: loss = 0.0557451 (* 1 = 0.0557451 loss)
I0815 23:00:41.357698 14907 sgd_solver.cpp:136] Iteration 5400, lr = 1e-05, m = 0.9
I0815 23:00:51.293861 14920 data_reader.cpp:288] Starting prefetch of epoch 4
I0815 23:01:00.758060 14907 solver.cpp:312] Iteration 5500 (5.15468 iter/s, 19.3999s/100 iter), loss = 0.0740712
I0815 23:01:00.758111 14907 solver.cpp:334]     Train net output #0: loss = 0.0740712 (* 1 = 0.0740712 loss)
I0815 23:01:00.758116 14907 sgd_solver.cpp:136] Iteration 5500, lr = 1e-05, m = 0.9
I0815 23:01:20.217659 14907 solver.cpp:312] Iteration 5600 (5.13899 iter/s, 19.4591s/100 iter), loss = 0.0366421
I0815 23:01:20.217730 14907 solver.cpp:334]     Train net output #0: loss = 0.0366422 (* 1 = 0.0366422 loss)
I0815 23:01:20.217747 14907 sgd_solver.cpp:136] Iteration 5600, lr = 1e-05, m = 0.9
I0815 23:01:39.560231 14907 solver.cpp:312] Iteration 5700 (5.17008 iter/s, 19.342s/100 iter), loss = 0.0773739
I0815 23:01:39.560300 14907 solver.cpp:334]     Train net output #0: loss = 0.0773739 (* 1 = 0.0773739 loss)
I0815 23:01:39.560307 14907 sgd_solver.cpp:136] Iteration 5700, lr = 1e-05, m = 0.9
I0815 23:01:55.246150 14920 data_reader.cpp:288] Starting prefetch of epoch 5
I0815 23:01:59.031536 14907 solver.cpp:312] Iteration 5800 (5.1359 iter/s, 19.4708s/100 iter), loss = 0.0719006
I0815 23:01:59.031558 14907 solver.cpp:334]     Train net output #0: loss = 0.0719006 (* 1 = 0.0719006 loss)
I0815 23:01:59.031565 14907 sgd_solver.cpp:136] Iteration 5800, lr = 1e-05, m = 0.9
I0815 23:02:18.630785 14907 solver.cpp:312] Iteration 5900 (5.10238 iter/s, 19.5987s/100 iter), loss = 0.0605701
I0815 23:02:18.630864 14907 solver.cpp:334]     Train net output #0: loss = 0.0605701 (* 1 = 0.0605701 loss)
I0815 23:02:18.630872 14907 sgd_solver.cpp:136] Iteration 5900, lr = 1e-05, m = 0.9
I0815 23:02:27.648569 14915 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 23:02:37.941296 14907 solver.cpp:363] Sparsity after update:
I0815 23:02:37.946640 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0815 23:02:37.946665 14907 net.cpp:2192] conv1a_param_0(0) 
I0815 23:02:37.946674 14907 net.cpp:2192] conv1b_param_0(0) 
I0815 23:02:37.946677 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0815 23:02:37.946681 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0815 23:02:37.946682 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0815 23:02:37.946686 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0815 23:02:37.946688 14907 net.cpp:2192] ctx_final_param_0(0) 
I0815 23:02:37.946707 14907 net.cpp:2192] out3a_param_0(0) 
I0815 23:02:37.946715 14907 net.cpp:2192] out5a_param_0(0) 
I0815 23:02:37.946722 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 23:02:37.946730 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 23:02:37.946738 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 23:02:37.946745 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 23:02:37.946753 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 23:02:37.946760 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 23:02:37.946768 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 23:02:37.946775 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 23:02:37.946782 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0815 23:02:37.946799 14907 solver.cpp:509] Iteration 6000, Testing net (#0)
I0815 23:02:48.891825 14907 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 23:02:49.942041 14907 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.950637
I0815 23:02:49.942067 14907 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999224
I0815 23:02:49.942073 14907 solver.cpp:594]     Test net output #2: loss = 0.207568 (* 1 = 0.207568 loss)
I0815 23:02:49.942129 14907 solver.cpp:264] [MultiGPU] Tests completed in 11.995s
I0815 23:02:50.149562 14907 solver.cpp:312] Iteration 6000 (3.1728 iter/s, 31.5179s/100 iter), loss = 0.0930146
I0815 23:02:50.149585 14907 solver.cpp:334]     Train net output #0: loss = 0.0930146 (* 1 = 0.0930146 loss)
I0815 23:02:50.149591 14907 sgd_solver.cpp:136] Iteration 6000, lr = 1e-05, m = 0.9
I0815 23:03:09.902242 14907 solver.cpp:312] Iteration 6100 (5.06275 iter/s, 19.7521s/100 iter), loss = 0.072984
I0815 23:03:09.902271 14907 solver.cpp:334]     Train net output #0: loss = 0.072984 (* 1 = 0.072984 loss)
I0815 23:03:09.902277 14907 sgd_solver.cpp:136] Iteration 6100, lr = 1e-05, m = 0.9
I0815 23:03:12.309451 14920 data_reader.cpp:288] Starting prefetch of epoch 6
I0815 23:03:29.164685 14907 solver.cpp:312] Iteration 6200 (5.19159 iter/s, 19.2619s/100 iter), loss = 0.0693349
I0815 23:03:29.164753 14907 solver.cpp:334]     Train net output #0: loss = 0.0693349 (* 1 = 0.0693349 loss)
I0815 23:03:29.164760 14907 sgd_solver.cpp:136] Iteration 6200, lr = 1e-05, m = 0.9
I0815 23:03:48.949785 14907 solver.cpp:312] Iteration 6300 (5.05445 iter/s, 19.7846s/100 iter), loss = 0.0692507
I0815 23:03:48.949810 14907 solver.cpp:334]     Train net output #0: loss = 0.0692507 (* 1 = 0.0692507 loss)
I0815 23:03:48.949816 14907 sgd_solver.cpp:136] Iteration 6300, lr = 1e-05, m = 0.9
I0815 23:04:08.870555 14907 solver.cpp:312] Iteration 6400 (5.02002 iter/s, 19.9202s/100 iter), loss = 0.0821214
I0815 23:04:08.870617 14907 solver.cpp:334]     Train net output #0: loss = 0.0821214 (* 1 = 0.0821214 loss)
I0815 23:04:08.870625 14907 sgd_solver.cpp:136] Iteration 6400, lr = 1e-05, m = 0.9
I0815 23:04:17.101155 14872 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 23:04:28.438601 14907 solver.cpp:312] Iteration 6500 (5.11051 iter/s, 19.5675s/100 iter), loss = 0.0804752
I0815 23:04:28.438629 14907 solver.cpp:334]     Train net output #0: loss = 0.0804751 (* 1 = 0.0804751 loss)
I0815 23:04:28.438658 14907 sgd_solver.cpp:136] Iteration 6500, lr = 1e-05, m = 0.9
I0815 23:04:48.086793 14907 solver.cpp:312] Iteration 6600 (5.08967 iter/s, 19.6477s/100 iter), loss = 0.0809348
I0815 23:04:48.087064 14907 solver.cpp:334]     Train net output #0: loss = 0.0809348 (* 1 = 0.0809348 loss)
I0815 23:04:48.087071 14907 sgd_solver.cpp:136] Iteration 6600, lr = 1e-05, m = 0.9
I0815 23:04:49.699412 14915 data_reader.cpp:288] Starting prefetch of epoch 4
I0815 23:05:07.585875 14907 solver.cpp:312] Iteration 6700 (5.12859 iter/s, 19.4985s/100 iter), loss = 0.0904878
I0815 23:05:07.585896 14907 solver.cpp:334]     Train net output #0: loss = 0.0904878 (* 1 = 0.0904878 loss)
I0815 23:05:07.585902 14907 sgd_solver.cpp:136] Iteration 6700, lr = 1e-05, m = 0.9
I0815 23:05:27.089349 14907 solver.cpp:312] Iteration 6800 (5.12743 iter/s, 19.5029s/100 iter), loss = 0.0484946
I0815 23:05:27.089398 14907 solver.cpp:334]     Train net output #0: loss = 0.0484946 (* 1 = 0.0484946 loss)
I0815 23:05:27.089403 14907 sgd_solver.cpp:136] Iteration 6800, lr = 1e-05, m = 0.9
I0815 23:05:46.704663 14907 solver.cpp:312] Iteration 6900 (5.0982 iter/s, 19.6148s/100 iter), loss = 0.0911915
I0815 23:05:46.704684 14907 solver.cpp:334]     Train net output #0: loss = 0.0911915 (* 1 = 0.0911915 loss)
I0815 23:05:46.704689 14907 sgd_solver.cpp:136] Iteration 6900, lr = 1e-05, m = 0.9
I0815 23:05:53.983722 14918 data_reader.cpp:288] Starting prefetch of epoch 4
I0815 23:06:05.844784 14907 solver.cpp:363] Sparsity after update:
I0815 23:06:05.850344 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0815 23:06:05.850404 14907 net.cpp:2192] conv1a_param_0(0) 
I0815 23:06:05.850421 14907 net.cpp:2192] conv1b_param_0(0) 
I0815 23:06:05.850425 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0815 23:06:05.850427 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0815 23:06:05.850430 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0815 23:06:05.850432 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0815 23:06:05.850435 14907 net.cpp:2192] ctx_final_param_0(0) 
I0815 23:06:05.850438 14907 net.cpp:2192] out3a_param_0(0) 
I0815 23:06:05.850445 14907 net.cpp:2192] out5a_param_0(0) 
I0815 23:06:05.850448 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 23:06:05.850451 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 23:06:05.850455 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 23:06:05.850457 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 23:06:05.850461 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 23:06:05.850463 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 23:06:05.850466 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 23:06:05.850469 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 23:06:05.850471 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0815 23:06:06.045689 14907 solver.cpp:312] Iteration 7000 (5.1705 iter/s, 19.3405s/100 iter), loss = 0.0982227
I0815 23:06:06.045713 14907 solver.cpp:334]     Train net output #0: loss = 0.0982227 (* 1 = 0.0982227 loss)
I0815 23:06:06.045718 14907 sgd_solver.cpp:136] Iteration 7000, lr = 1e-05, m = 0.9
I0815 23:06:25.702143 14907 solver.cpp:312] Iteration 7100 (5.08753 iter/s, 19.6559s/100 iter), loss = 0.0707592
I0815 23:06:25.702165 14907 solver.cpp:334]     Train net output #0: loss = 0.0707592 (* 1 = 0.0707592 loss)
I0815 23:06:25.702172 14907 sgd_solver.cpp:136] Iteration 7100, lr = 1e-05, m = 0.9
I0815 23:06:45.145412 14907 solver.cpp:312] Iteration 7200 (5.14331 iter/s, 19.4427s/100 iter), loss = 0.0644682
I0815 23:06:45.145512 14907 solver.cpp:334]     Train net output #0: loss = 0.0644682 (* 1 = 0.0644682 loss)
I0815 23:06:45.145519 14907 sgd_solver.cpp:136] Iteration 7200, lr = 1e-05, m = 0.9
I0815 23:06:58.612114 14872 data_reader.cpp:288] Starting prefetch of epoch 4
I0815 23:07:04.671530 14907 solver.cpp:312] Iteration 7300 (5.12149 iter/s, 19.5256s/100 iter), loss = 0.0602461
I0815 23:07:04.671553 14907 solver.cpp:334]     Train net output #0: loss = 0.0602461 (* 1 = 0.0602461 loss)
I0815 23:07:04.671557 14907 sgd_solver.cpp:136] Iteration 7300, lr = 1e-05, m = 0.9
I0815 23:07:24.400293 14907 solver.cpp:312] Iteration 7400 (5.06888 iter/s, 19.7282s/100 iter), loss = 0.0916521
I0815 23:07:24.400377 14907 solver.cpp:334]     Train net output #0: loss = 0.091652 (* 1 = 0.091652 loss)
I0815 23:07:24.400384 14907 sgd_solver.cpp:136] Iteration 7400, lr = 1e-05, m = 0.9
I0815 23:07:31.144485 14916 data_reader.cpp:288] Starting prefetch of epoch 5
I0815 23:07:43.904687 14907 solver.cpp:312] Iteration 7500 (5.12719 iter/s, 19.5039s/100 iter), loss = 0.0932663
I0815 23:07:43.904716 14907 solver.cpp:334]     Train net output #0: loss = 0.0932663 (* 1 = 0.0932663 loss)
I0815 23:07:43.904722 14907 sgd_solver.cpp:136] Iteration 7500, lr = 1e-05, m = 0.9
I0815 23:08:03.419869 14907 solver.cpp:312] Iteration 7600 (5.12436 iter/s, 19.5146s/100 iter), loss = 0.0603292
I0815 23:08:03.420286 14907 solver.cpp:334]     Train net output #0: loss = 0.0603292 (* 1 = 0.0603292 loss)
I0815 23:08:03.420295 14907 sgd_solver.cpp:136] Iteration 7600, lr = 1e-05, m = 0.9
I0815 23:08:23.002099 14907 solver.cpp:312] Iteration 7700 (5.10681 iter/s, 19.5817s/100 iter), loss = 0.0677599
I0815 23:08:23.002126 14907 solver.cpp:334]     Train net output #0: loss = 0.0677599 (* 1 = 0.0677599 loss)
I0815 23:08:23.002132 14907 sgd_solver.cpp:136] Iteration 7700, lr = 1e-05, m = 0.9
I0815 23:08:35.965867 14870 data_reader.cpp:288] Starting prefetch of epoch 7
I0815 23:08:43.790622 14907 solver.cpp:312] Iteration 7800 (4.81048 iter/s, 20.788s/100 iter), loss = 0.0733587
I0815 23:08:43.790648 14907 solver.cpp:334]     Train net output #0: loss = 0.0733587 (* 1 = 0.0733587 loss)
I0815 23:08:43.790654 14907 sgd_solver.cpp:136] Iteration 7800, lr = 1e-05, m = 0.9
I0815 23:09:06.545853 14907 solver.cpp:312] Iteration 7900 (4.39471 iter/s, 22.7546s/100 iter), loss = 0.0530147
I0815 23:09:06.545907 14907 solver.cpp:334]     Train net output #0: loss = 0.0530147 (* 1 = 0.0530147 loss)
I0815 23:09:06.545914 14907 sgd_solver.cpp:136] Iteration 7900, lr = 1e-05, m = 0.9
I0815 23:09:25.824373 14907 solver.cpp:363] Sparsity after update:
I0815 23:09:25.840075 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0815 23:09:25.840095 14907 net.cpp:2192] conv1a_param_0(0) 
I0815 23:09:25.840103 14907 net.cpp:2192] conv1b_param_0(0) 
I0815 23:09:25.840106 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0815 23:09:25.840111 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0815 23:09:25.840113 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0815 23:09:25.840117 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0815 23:09:25.840121 14907 net.cpp:2192] ctx_final_param_0(0) 
I0815 23:09:25.840126 14907 net.cpp:2192] out3a_param_0(0) 
I0815 23:09:25.840137 14907 net.cpp:2192] out5a_param_0(0) 
I0815 23:09:25.840140 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 23:09:25.840143 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 23:09:25.840147 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 23:09:25.840152 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 23:09:25.840157 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 23:09:25.840160 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 23:09:25.840163 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 23:09:25.840167 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 23:09:25.840170 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0815 23:09:25.840181 14907 solver.cpp:509] Iteration 8000, Testing net (#0)
I0815 23:09:29.461092 14940 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 23:09:37.753823 14944 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 23:09:38.163487 14907 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.9517
I0815 23:09:38.163516 14907 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999938
I0815 23:09:38.163523 14907 solver.cpp:594]     Test net output #2: loss = 0.160049 (* 1 = 0.160049 loss)
I0815 23:09:38.163549 14907 solver.cpp:264] [MultiGPU] Tests completed in 12.323s
I0815 23:09:38.362529 14907 solver.cpp:312] Iteration 8000 (3.14309 iter/s, 31.8158s/100 iter), loss = 0.0947565
I0815 23:09:38.362551 14907 solver.cpp:334]     Train net output #0: loss = 0.0947564 (* 1 = 0.0947564 loss)
I0815 23:09:38.362555 14907 sgd_solver.cpp:136] Iteration 8000, lr = 1e-05, m = 0.9
I0815 23:09:57.799269 14907 solver.cpp:312] Iteration 8100 (5.14504 iter/s, 19.4362s/100 iter), loss = 0.060784
I0815 23:09:57.799299 14907 solver.cpp:334]     Train net output #0: loss = 0.060784 (* 1 = 0.060784 loss)
I0815 23:09:57.799306 14907 sgd_solver.cpp:136] Iteration 8100, lr = 1e-05, m = 0.9
I0815 23:10:17.763748 14907 solver.cpp:312] Iteration 8200 (5.00903 iter/s, 19.9639s/100 iter), loss = 0.0574155
I0815 23:10:17.763801 14907 solver.cpp:334]     Train net output #0: loss = 0.0574155 (* 1 = 0.0574155 loss)
I0815 23:10:17.763808 14907 sgd_solver.cpp:136] Iteration 8200, lr = 1e-05, m = 0.9
I0815 23:10:29.616750 14872 data_reader.cpp:288] Starting prefetch of epoch 5
I0815 23:10:37.472920 14907 solver.cpp:312] Iteration 8300 (5.07392 iter/s, 19.7086s/100 iter), loss = 0.115171
I0815 23:10:37.472945 14907 solver.cpp:334]     Train net output #0: loss = 0.115171 (* 1 = 0.115171 loss)
I0815 23:10:37.472951 14907 sgd_solver.cpp:136] Iteration 8300, lr = 1e-05, m = 0.9
I0815 23:10:57.266960 14907 solver.cpp:312] Iteration 8400 (5.05217 iter/s, 19.7935s/100 iter), loss = 0.0623324
I0815 23:10:57.267045 14907 solver.cpp:334]     Train net output #0: loss = 0.0623324 (* 1 = 0.0623324 loss)
I0815 23:10:57.267052 14907 sgd_solver.cpp:136] Iteration 8400, lr = 1e-05, m = 0.9
I0815 23:11:16.855150 14907 solver.cpp:312] Iteration 8500 (5.10526 iter/s, 19.5877s/100 iter), loss = 0.0530169
I0815 23:11:16.855181 14907 solver.cpp:334]     Train net output #0: loss = 0.0530169 (* 1 = 0.0530169 loss)
I0815 23:11:16.855188 14907 sgd_solver.cpp:136] Iteration 8500, lr = 1e-05, m = 0.9
I0815 23:11:34.769525 14915 data_reader.cpp:288] Starting prefetch of epoch 5
I0815 23:11:36.481268 14907 solver.cpp:312] Iteration 8600 (5.09539 iter/s, 19.6256s/100 iter), loss = 0.0936117
I0815 23:11:36.481292 14907 solver.cpp:334]     Train net output #0: loss = 0.0936117 (* 1 = 0.0936117 loss)
I0815 23:11:36.481297 14907 sgd_solver.cpp:136] Iteration 8600, lr = 1e-05, m = 0.9
I0815 23:11:56.138574 14907 solver.cpp:312] Iteration 8700 (5.08731 iter/s, 19.6568s/100 iter), loss = 0.0720299
I0815 23:11:56.138600 14907 solver.cpp:334]     Train net output #0: loss = 0.0720299 (* 1 = 0.0720299 loss)
I0815 23:11:56.138607 14907 sgd_solver.cpp:136] Iteration 8700, lr = 1e-05, m = 0.9
I0815 23:12:15.777654 14907 solver.cpp:312] Iteration 8800 (5.09203 iter/s, 19.6385s/100 iter), loss = 0.0713198
I0815 23:12:15.777705 14907 solver.cpp:334]     Train net output #0: loss = 0.0713198 (* 1 = 0.0713198 loss)
I0815 23:12:15.777712 14907 sgd_solver.cpp:136] Iteration 8800, lr = 1e-05, m = 0.9
I0815 23:12:35.427063 14907 solver.cpp:312] Iteration 8900 (5.08935 iter/s, 19.6489s/100 iter), loss = 0.0714195
I0815 23:12:35.427091 14907 solver.cpp:334]     Train net output #0: loss = 0.0714195 (* 1 = 0.0714195 loss)
I0815 23:12:35.427098 14907 sgd_solver.cpp:136] Iteration 8900, lr = 1e-05, m = 0.9
I0815 23:12:39.612243 14872 data_reader.cpp:288] Starting prefetch of epoch 6
I0815 23:12:55.103649 14907 solver.cpp:363] Sparsity after update:
I0815 23:12:55.105824 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0815 23:12:55.105837 14907 net.cpp:2192] conv1a_param_0(0) 
I0815 23:12:55.105845 14907 net.cpp:2192] conv1b_param_0(0) 
I0815 23:12:55.105847 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0815 23:12:55.105849 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0815 23:12:55.105851 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0815 23:12:55.105854 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0815 23:12:55.105855 14907 net.cpp:2192] ctx_final_param_0(0) 
I0815 23:12:55.105857 14907 net.cpp:2192] out3a_param_0(0) 
I0815 23:12:55.105859 14907 net.cpp:2192] out5a_param_0(0) 
I0815 23:12:55.105861 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 23:12:55.105865 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 23:12:55.105866 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 23:12:55.105868 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 23:12:55.105871 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 23:12:55.105875 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 23:12:55.105878 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 23:12:55.105880 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 23:12:55.105883 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0815 23:12:55.295634 14907 solver.cpp:312] Iteration 9000 (5.03321 iter/s, 19.868s/100 iter), loss = 0.0637469
I0815 23:12:55.295660 14907 solver.cpp:334]     Train net output #0: loss = 0.0637469 (* 1 = 0.0637469 loss)
I0815 23:12:55.295665 14907 sgd_solver.cpp:136] Iteration 9000, lr = 1e-05, m = 0.9
I0815 23:13:14.874986 14907 solver.cpp:312] Iteration 9100 (5.10756 iter/s, 19.5788s/100 iter), loss = 0.0791558
I0815 23:13:14.875011 14907 solver.cpp:334]     Train net output #0: loss = 0.0791558 (* 1 = 0.0791558 loss)
I0815 23:13:14.875015 14907 sgd_solver.cpp:136] Iteration 9100, lr = 1e-05, m = 0.9
I0815 23:13:34.505937 14907 solver.cpp:312] Iteration 9200 (5.09414 iter/s, 19.6304s/100 iter), loss = 0.0625818
I0815 23:13:34.505993 14907 solver.cpp:334]     Train net output #0: loss = 0.0625818 (* 1 = 0.0625818 loss)
I0815 23:13:34.506000 14907 sgd_solver.cpp:136] Iteration 9200, lr = 1e-05, m = 0.9
I0815 23:13:44.718806 14918 data_reader.cpp:288] Starting prefetch of epoch 5
I0815 23:13:54.027995 14907 solver.cpp:312] Iteration 9300 (5.12255 iter/s, 19.5215s/100 iter), loss = 0.0689121
I0815 23:13:54.028020 14907 solver.cpp:334]     Train net output #0: loss = 0.0689121 (* 1 = 0.0689121 loss)
I0815 23:13:54.028028 14907 sgd_solver.cpp:136] Iteration 9300, lr = 1e-05, m = 0.9
I0815 23:14:13.596858 14907 solver.cpp:312] Iteration 9400 (5.1103 iter/s, 19.5683s/100 iter), loss = 0.0893542
I0815 23:14:13.596909 14907 solver.cpp:334]     Train net output #0: loss = 0.0893542 (* 1 = 0.0893542 loss)
I0815 23:14:13.596915 14907 sgd_solver.cpp:136] Iteration 9400, lr = 1e-05, m = 0.9
I0815 23:14:16.953148 14915 data_reader.cpp:288] Starting prefetch of epoch 6
I0815 23:14:33.092782 14907 solver.cpp:312] Iteration 9500 (5.12942 iter/s, 19.4954s/100 iter), loss = 0.0610399
I0815 23:14:33.092806 14907 solver.cpp:334]     Train net output #0: loss = 0.0610399 (* 1 = 0.0610399 loss)
I0815 23:14:33.092813 14907 sgd_solver.cpp:136] Iteration 9500, lr = 1e-05, m = 0.9
I0815 23:14:52.540289 14907 solver.cpp:312] Iteration 9600 (5.14219 iter/s, 19.447s/100 iter), loss = 0.0645963
I0815 23:14:52.540338 14907 solver.cpp:334]     Train net output #0: loss = 0.0645963 (* 1 = 0.0645963 loss)
I0815 23:14:52.540343 14907 sgd_solver.cpp:136] Iteration 9600, lr = 1e-05, m = 0.9
I0815 23:15:11.983840 14907 solver.cpp:312] Iteration 9700 (5.14323 iter/s, 19.443s/100 iter), loss = 0.0634353
I0815 23:15:11.983870 14907 solver.cpp:334]     Train net output #0: loss = 0.0634353 (* 1 = 0.0634353 loss)
I0815 23:15:11.983875 14907 sgd_solver.cpp:136] Iteration 9700, lr = 1e-05, m = 0.9
I0815 23:15:21.364152 14872 data_reader.cpp:288] Starting prefetch of epoch 7
I0815 23:15:31.541091 14907 solver.cpp:312] Iteration 9800 (5.11334 iter/s, 19.5567s/100 iter), loss = 0.0613361
I0815 23:15:31.541277 14907 solver.cpp:334]     Train net output #0: loss = 0.0613361 (* 1 = 0.0613361 loss)
I0815 23:15:31.541304 14907 sgd_solver.cpp:136] Iteration 9800, lr = 1e-05, m = 0.9
I0815 23:15:51.274864 14907 solver.cpp:312] Iteration 9900 (5.06759 iter/s, 19.7332s/100 iter), loss = 0.0665242
I0815 23:15:51.274888 14907 solver.cpp:334]     Train net output #0: loss = 0.0665242 (* 1 = 0.0665242 loss)
I0815 23:15:51.274891 14907 sgd_solver.cpp:136] Iteration 9900, lr = 1e-05, m = 0.9
I0815 23:16:10.775241 14907 solver.cpp:639] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/sparse/cityscapes5_jsegnet21v2_iter_10000.caffemodel
I0815 23:16:10.878458 14907 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/sparse/cityscapes5_jsegnet21v2_iter_10000.solverstate
I0815 23:16:10.886278 14907 solver.cpp:363] Sparsity after update:
I0815 23:16:10.896919 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0815 23:16:10.896934 14907 net.cpp:2192] conv1a_param_0(0) 
I0815 23:16:10.896944 14907 net.cpp:2192] conv1b_param_0(0) 
I0815 23:16:10.896946 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0815 23:16:10.896950 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0815 23:16:10.896955 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0815 23:16:10.896957 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0815 23:16:10.896960 14907 net.cpp:2192] ctx_final_param_0(0) 
I0815 23:16:10.896962 14907 net.cpp:2192] out3a_param_0(0) 
I0815 23:16:10.896966 14907 net.cpp:2192] out5a_param_0(0) 
I0815 23:16:10.896970 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 23:16:10.896973 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 23:16:10.896977 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 23:16:10.896980 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 23:16:10.896983 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 23:16:10.896986 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 23:16:10.896991 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 23:16:10.896993 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 23:16:10.896996 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0815 23:16:10.897009 14907 solver.cpp:509] Iteration 10000, Testing net (#0)
I0815 23:16:19.003911 14944 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 23:16:24.619354 14907 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.951972
I0815 23:16:24.619380 14907 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999155
I0815 23:16:24.619386 14907 solver.cpp:594]     Test net output #2: loss = 0.200229 (* 1 = 0.200229 loss)
I0815 23:16:24.619468 14907 solver.cpp:264] [MultiGPU] Tests completed in 13.7221s
I0815 23:16:24.828840 14907 solver.cpp:312] Iteration 10000 (2.98035 iter/s, 33.5531s/100 iter), loss = 0.0718695
I0815 23:16:24.828868 14907 solver.cpp:334]     Train net output #0: loss = 0.0718694 (* 1 = 0.0718694 loss)
I0815 23:16:24.828873 14907 sgd_solver.cpp:136] Iteration 10000, lr = 1e-05, m = 0.9
I0815 23:16:44.600798 14907 solver.cpp:312] Iteration 10100 (5.05781 iter/s, 19.7714s/100 iter), loss = 0.067439
I0815 23:16:44.600857 14907 solver.cpp:334]     Train net output #0: loss = 0.067439 (* 1 = 0.067439 loss)
I0815 23:16:44.600864 14907 sgd_solver.cpp:136] Iteration 10100, lr = 1e-05, m = 0.9
I0815 23:17:04.055550 14907 solver.cpp:312] Iteration 10200 (5.14028 iter/s, 19.4542s/100 iter), loss = 0.0815444
I0815 23:17:04.055584 14907 solver.cpp:334]     Train net output #0: loss = 0.0815444 (* 1 = 0.0815444 loss)
I0815 23:17:04.055589 14907 sgd_solver.cpp:136] Iteration 10200, lr = 1e-05, m = 0.9
I0815 23:17:12.745636 14872 data_reader.cpp:288] Starting prefetch of epoch 8
I0815 23:17:23.714076 14907 solver.cpp:312] Iteration 10300 (5.08699 iter/s, 19.658s/100 iter), loss = 0.0557319
I0815 23:17:23.714174 14907 solver.cpp:334]     Train net output #0: loss = 0.0557318 (* 1 = 0.0557318 loss)
I0815 23:17:23.714197 14907 sgd_solver.cpp:136] Iteration 10300, lr = 1e-05, m = 0.9
I0815 23:17:43.242667 14907 solver.cpp:312] Iteration 10400 (5.12084 iter/s, 19.5281s/100 iter), loss = 0.0518824
I0815 23:17:43.242693 14907 solver.cpp:334]     Train net output #0: loss = 0.0518824 (* 1 = 0.0518824 loss)
I0815 23:17:43.242699 14907 sgd_solver.cpp:136] Iteration 10400, lr = 1e-05, m = 0.9
I0815 23:17:45.106705 14916 data_reader.cpp:288] Starting prefetch of epoch 6
I0815 23:18:02.945128 14907 solver.cpp:312] Iteration 10500 (5.07565 iter/s, 19.7019s/100 iter), loss = 0.0599832
I0815 23:18:02.945185 14907 solver.cpp:334]     Train net output #0: loss = 0.0599832 (* 1 = 0.0599832 loss)
I0815 23:18:02.945191 14907 sgd_solver.cpp:136] Iteration 10500, lr = 1e-05, m = 0.9
I0815 23:18:22.617050 14907 solver.cpp:312] Iteration 10600 (5.08353 iter/s, 19.6714s/100 iter), loss = 0.081204
I0815 23:18:22.617079 14907 solver.cpp:334]     Train net output #0: loss = 0.081204 (* 1 = 0.081204 loss)
I0815 23:18:22.617086 14907 sgd_solver.cpp:136] Iteration 10600, lr = 1e-05, m = 0.9
I0815 23:18:42.104506 14907 solver.cpp:312] Iteration 10700 (5.13165 iter/s, 19.4869s/100 iter), loss = 0.0398074
I0815 23:18:42.104586 14907 solver.cpp:334]     Train net output #0: loss = 0.0398074 (* 1 = 0.0398074 loss)
I0815 23:18:42.104604 14907 sgd_solver.cpp:136] Iteration 10700, lr = 1e-05, m = 0.9
I0815 23:18:49.957062 14918 data_reader.cpp:288] Starting prefetch of epoch 6
I0815 23:19:01.762235 14907 solver.cpp:312] Iteration 10800 (5.0872 iter/s, 19.6572s/100 iter), loss = 0.0742281
I0815 23:19:01.762259 14907 solver.cpp:334]     Train net output #0: loss = 0.0742281 (* 1 = 0.0742281 loss)
I0815 23:19:01.762264 14907 sgd_solver.cpp:136] Iteration 10800, lr = 1e-05, m = 0.9
I0815 23:19:21.296891 14907 solver.cpp:312] Iteration 10900 (5.11925 iter/s, 19.5341s/100 iter), loss = 0.0459606
I0815 23:19:21.296942 14907 solver.cpp:334]     Train net output #0: loss = 0.0459606 (* 1 = 0.0459606 loss)
I0815 23:19:21.296952 14907 sgd_solver.cpp:136] Iteration 10900, lr = 1e-05, m = 0.9
I0815 23:19:40.590901 14907 solver.cpp:363] Sparsity after update:
I0815 23:19:40.609973 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0815 23:19:40.610100 14907 net.cpp:2192] conv1a_param_0(0) 
I0815 23:19:40.610123 14907 net.cpp:2192] conv1b_param_0(0) 
I0815 23:19:40.610131 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0815 23:19:40.610139 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0815 23:19:40.610147 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0815 23:19:40.610157 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0815 23:19:40.610163 14907 net.cpp:2192] ctx_final_param_0(0) 
I0815 23:19:40.610170 14907 net.cpp:2192] out3a_param_0(0) 
I0815 23:19:40.610179 14907 net.cpp:2192] out5a_param_0(0) 
I0815 23:19:40.610188 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 23:19:40.610195 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 23:19:40.610204 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 23:19:40.610211 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 23:19:40.610219 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 23:19:40.610226 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 23:19:40.610234 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 23:19:40.610240 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 23:19:40.610249 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0815 23:19:40.789026 14907 solver.cpp:312] Iteration 11000 (5.13042 iter/s, 19.4916s/100 iter), loss = 0.0596991
I0815 23:19:40.789053 14907 solver.cpp:334]     Train net output #0: loss = 0.0596991 (* 1 = 0.0596991 loss)
I0815 23:19:40.789059 14907 sgd_solver.cpp:136] Iteration 11000, lr = 1e-05, m = 0.9
I0815 23:19:54.446727 14872 data_reader.cpp:288] Starting prefetch of epoch 9
I0815 23:20:00.388139 14907 solver.cpp:312] Iteration 11100 (5.10241 iter/s, 19.5986s/100 iter), loss = 0.0762926
I0815 23:20:00.388170 14907 solver.cpp:334]     Train net output #0: loss = 0.0762926 (* 1 = 0.0762926 loss)
I0815 23:20:00.388175 14907 sgd_solver.cpp:136] Iteration 11100, lr = 1e-05, m = 0.9
I0815 23:20:20.132601 14907 solver.cpp:312] Iteration 11200 (5.06485 iter/s, 19.7439s/100 iter), loss = 0.0720218
I0815 23:20:20.132628 14907 solver.cpp:334]     Train net output #0: loss = 0.0720218 (* 1 = 0.0720218 loss)
I0815 23:20:20.132637 14907 sgd_solver.cpp:136] Iteration 11200, lr = 1e-05, m = 0.9
I0815 23:20:27.071640 14916 data_reader.cpp:288] Starting prefetch of epoch 7
I0815 23:20:39.629201 14907 solver.cpp:312] Iteration 11300 (5.12924 iter/s, 19.4961s/100 iter), loss = 0.0555065
I0815 23:20:39.629222 14907 solver.cpp:334]     Train net output #0: loss = 0.0555065 (* 1 = 0.0555065 loss)
I0815 23:20:39.629228 14907 sgd_solver.cpp:136] Iteration 11300, lr = 1e-05, m = 0.9
I0815 23:20:59.403218 14907 solver.cpp:312] Iteration 11400 (5.05728 iter/s, 19.7735s/100 iter), loss = 0.0621033
I0815 23:20:59.403271 14907 solver.cpp:334]     Train net output #0: loss = 0.0621033 (* 1 = 0.0621033 loss)
I0815 23:20:59.403278 14907 sgd_solver.cpp:136] Iteration 11400, lr = 1e-05, m = 0.9
I0815 23:21:18.855098 14907 solver.cpp:312] Iteration 11500 (5.14103 iter/s, 19.4513s/100 iter), loss = 0.080125
I0815 23:21:18.855123 14907 solver.cpp:334]     Train net output #0: loss = 0.080125 (* 1 = 0.080125 loss)
I0815 23:21:18.855128 14907 sgd_solver.cpp:136] Iteration 11500, lr = 1e-05, m = 0.9
I0815 23:21:31.731226 14872 data_reader.cpp:288] Starting prefetch of epoch 10
I0815 23:21:38.337175 14907 solver.cpp:312] Iteration 11600 (5.13306 iter/s, 19.4815s/100 iter), loss = 0.105912
I0815 23:21:38.337201 14907 solver.cpp:334]     Train net output #0: loss = 0.105912 (* 1 = 0.105912 loss)
I0815 23:21:38.337208 14907 sgd_solver.cpp:136] Iteration 11600, lr = 1e-05, m = 0.9
I0815 23:21:57.986567 14907 solver.cpp:312] Iteration 11700 (5.08936 iter/s, 19.6489s/100 iter), loss = 0.0412071
I0815 23:21:57.986593 14907 solver.cpp:334]     Train net output #0: loss = 0.041207 (* 1 = 0.041207 loss)
I0815 23:21:57.986598 14907 sgd_solver.cpp:136] Iteration 11700, lr = 1e-05, m = 0.9
I0815 23:22:17.524937 14907 solver.cpp:312] Iteration 11800 (5.11827 iter/s, 19.5378s/100 iter), loss = 0.0606172
I0815 23:22:17.524996 14907 solver.cpp:334]     Train net output #0: loss = 0.0606172 (* 1 = 0.0606172 loss)
I0815 23:22:17.525004 14907 sgd_solver.cpp:136] Iteration 11800, lr = 1e-05, m = 0.9
I0815 23:22:36.297773 14920 data_reader.cpp:288] Starting prefetch of epoch 7
I0815 23:22:37.050910 14907 solver.cpp:312] Iteration 11900 (5.12152 iter/s, 19.5254s/100 iter), loss = 0.0632305
I0815 23:22:37.050937 14907 solver.cpp:334]     Train net output #0: loss = 0.0632305 (* 1 = 0.0632305 loss)
I0815 23:22:37.050943 14907 sgd_solver.cpp:136] Iteration 11900, lr = 1e-05, m = 0.9
I0815 23:22:56.733485 14907 solver.cpp:363] Sparsity after update:
I0815 23:22:56.749357 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0815 23:22:56.749399 14907 net.cpp:2192] conv1a_param_0(0) 
I0815 23:22:56.749414 14907 net.cpp:2192] conv1b_param_0(0) 
I0815 23:22:56.749424 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0815 23:22:56.749433 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0815 23:22:56.749442 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0815 23:22:56.749451 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0815 23:22:56.749460 14907 net.cpp:2192] ctx_final_param_0(0) 
I0815 23:22:56.749469 14907 net.cpp:2192] out3a_param_0(0) 
I0815 23:22:56.749478 14907 net.cpp:2192] out5a_param_0(0) 
I0815 23:22:56.749487 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 23:22:56.749496 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 23:22:56.749505 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 23:22:56.749514 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 23:22:56.749523 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 23:22:56.749531 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 23:22:56.749541 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 23:22:56.749550 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 23:22:56.749558 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0815 23:22:56.749583 14907 solver.cpp:509] Iteration 12000, Testing net (#0)
I0815 23:23:01.146621 14940 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 23:23:10.263531 14907 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.951132
I0815 23:23:10.263552 14907 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999868
I0815 23:23:10.263557 14907 solver.cpp:594]     Test net output #2: loss = 0.165016 (* 1 = 0.165016 loss)
I0815 23:23:10.263588 14907 solver.cpp:264] [MultiGPU] Tests completed in 13.5136s
I0815 23:23:10.475903 14907 solver.cpp:312] Iteration 12000 (2.99185 iter/s, 33.4241s/100 iter), loss = 0.0378173
I0815 23:23:10.475929 14907 solver.cpp:334]     Train net output #0: loss = 0.0378173 (* 1 = 0.0378173 loss)
I0815 23:23:10.475934 14907 sgd_solver.cpp:136] Iteration 12000, lr = 1e-05, m = 0.9
I0815 23:23:22.549295 14920 data_reader.cpp:288] Starting prefetch of epoch 8
I0815 23:23:29.967221 14907 solver.cpp:312] Iteration 12100 (5.13063 iter/s, 19.4908s/100 iter), loss = 0.0885455
I0815 23:23:29.967285 14907 solver.cpp:334]     Train net output #0: loss = 0.0885455 (* 1 = 0.0885455 loss)
I0815 23:23:29.967290 14907 sgd_solver.cpp:136] Iteration 12100, lr = 1e-05, m = 0.9
I0815 23:23:49.566907 14907 solver.cpp:312] Iteration 12200 (5.10226 iter/s, 19.5991s/100 iter), loss = 0.0548304
I0815 23:23:49.566933 14907 solver.cpp:334]     Train net output #0: loss = 0.0548304 (* 1 = 0.0548304 loss)
I0815 23:23:49.566937 14907 sgd_solver.cpp:136] Iteration 12200, lr = 1e-05, m = 0.9
I0815 23:24:08.966152 14907 solver.cpp:312] Iteration 12300 (5.15498 iter/s, 19.3987s/100 iter), loss = 0.0497524
I0815 23:24:08.966197 14907 solver.cpp:334]     Train net output #0: loss = 0.0497523 (* 1 = 0.0497523 loss)
I0815 23:24:08.966202 14907 sgd_solver.cpp:136] Iteration 12300, lr = 1e-05, m = 0.9
I0815 23:24:26.863000 14872 data_reader.cpp:288] Starting prefetch of epoch 11
I0815 23:24:28.459403 14907 solver.cpp:312] Iteration 12400 (5.13012 iter/s, 19.4927s/100 iter), loss = 0.0532798
I0815 23:24:28.459429 14907 solver.cpp:334]     Train net output #0: loss = 0.0532798 (* 1 = 0.0532798 loss)
I0815 23:24:28.459435 14907 sgd_solver.cpp:136] Iteration 12400, lr = 1e-05, m = 0.9
I0815 23:24:48.184952 14907 solver.cpp:312] Iteration 12500 (5.06971 iter/s, 19.725s/100 iter), loss = 0.0715397
I0815 23:24:48.184999 14907 solver.cpp:334]     Train net output #0: loss = 0.0715396 (* 1 = 0.0715396 loss)
I0815 23:24:48.185004 14907 sgd_solver.cpp:136] Iteration 12500, lr = 1e-05, m = 0.9
I0815 23:24:59.573421 14870 data_reader.cpp:288] Starting prefetch of epoch 8
I0815 23:25:07.756774 14907 solver.cpp:312] Iteration 12600 (5.10953 iter/s, 19.5713s/100 iter), loss = 0.0459288
I0815 23:25:07.756799 14907 solver.cpp:334]     Train net output #0: loss = 0.0459288 (* 1 = 0.0459288 loss)
I0815 23:25:07.756803 14907 sgd_solver.cpp:136] Iteration 12600, lr = 1e-05, m = 0.9
I0815 23:25:27.724364 14907 solver.cpp:312] Iteration 12700 (5.00825 iter/s, 19.967s/100 iter), loss = 0.115026
I0815 23:25:27.724422 14907 solver.cpp:334]     Train net output #0: loss = 0.115026 (* 1 = 0.115026 loss)
I0815 23:25:27.724431 14907 sgd_solver.cpp:136] Iteration 12700, lr = 1e-05, m = 0.9
I0815 23:25:47.231850 14907 solver.cpp:312] Iteration 12800 (5.12638 iter/s, 19.507s/100 iter), loss = 0.0464258
I0815 23:25:47.231874 14907 solver.cpp:334]     Train net output #0: loss = 0.0464257 (* 1 = 0.0464257 loss)
I0815 23:25:47.231878 14907 sgd_solver.cpp:136] Iteration 12800, lr = 1e-05, m = 0.9
I0815 23:26:04.440820 14918 data_reader.cpp:288] Starting prefetch of epoch 7
I0815 23:26:06.746907 14907 solver.cpp:312] Iteration 12900 (5.12439 iter/s, 19.5145s/100 iter), loss = 0.0737448
I0815 23:26:06.746932 14907 solver.cpp:334]     Train net output #0: loss = 0.0737448 (* 1 = 0.0737448 loss)
I0815 23:26:06.746937 14907 sgd_solver.cpp:136] Iteration 12900, lr = 1e-05, m = 0.9
I0815 23:26:25.933656 14907 solver.cpp:363] Sparsity after update:
I0815 23:26:25.940088 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0815 23:26:25.940165 14907 net.cpp:2192] conv1a_param_0(0) 
I0815 23:26:25.940183 14907 net.cpp:2192] conv1b_param_0(0) 
I0815 23:26:25.940186 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0815 23:26:25.940189 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0815 23:26:25.940192 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0815 23:26:25.940196 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0815 23:26:25.940198 14907 net.cpp:2192] ctx_final_param_0(0) 
I0815 23:26:25.940201 14907 net.cpp:2192] out3a_param_0(0) 
I0815 23:26:25.940203 14907 net.cpp:2192] out5a_param_0(0) 
I0815 23:26:25.940207 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 23:26:25.940209 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 23:26:25.940212 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 23:26:25.940214 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 23:26:25.940217 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 23:26:25.940220 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 23:26:25.940223 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 23:26:25.940227 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 23:26:25.940229 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0815 23:26:26.138111 14907 solver.cpp:312] Iteration 13000 (5.15712 iter/s, 19.3907s/100 iter), loss = 0.0568267
I0815 23:26:26.138144 14907 solver.cpp:334]     Train net output #0: loss = 0.0568267 (* 1 = 0.0568267 loss)
I0815 23:26:26.138151 14907 sgd_solver.cpp:136] Iteration 13000, lr = 1e-05, m = 0.9
I0815 23:26:45.741760 14907 solver.cpp:312] Iteration 13100 (5.10123 iter/s, 19.6031s/100 iter), loss = 0.0793199
I0815 23:26:45.741833 14907 solver.cpp:334]     Train net output #0: loss = 0.0793198 (* 1 = 0.0793198 loss)
I0815 23:26:45.741840 14907 sgd_solver.cpp:136] Iteration 13100, lr = 1e-05, m = 0.9
I0815 23:27:05.313426 14907 solver.cpp:312] Iteration 13200 (5.10957 iter/s, 19.5711s/100 iter), loss = 0.0529609
I0815 23:27:05.313450 14907 solver.cpp:334]     Train net output #0: loss = 0.0529609 (* 1 = 0.0529609 loss)
I0815 23:27:05.313457 14907 sgd_solver.cpp:136] Iteration 13200, lr = 1e-05, m = 0.9
I0815 23:27:08.935531 14920 data_reader.cpp:288] Starting prefetch of epoch 9
I0815 23:27:24.666550 14907 solver.cpp:312] Iteration 13300 (5.16727 iter/s, 19.3526s/100 iter), loss = 0.0630468
I0815 23:27:24.666604 14907 solver.cpp:334]     Train net output #0: loss = 0.0630467 (* 1 = 0.0630467 loss)
I0815 23:27:24.666611 14907 sgd_solver.cpp:136] Iteration 13300, lr = 1e-05, m = 0.9
I0815 23:27:41.114593 14870 data_reader.cpp:288] Starting prefetch of epoch 9
I0815 23:27:44.169436 14907 solver.cpp:312] Iteration 13400 (5.12759 iter/s, 19.5024s/100 iter), loss = 0.0508843
I0815 23:27:44.169457 14907 solver.cpp:334]     Train net output #0: loss = 0.0508842 (* 1 = 0.0508842 loss)
I0815 23:27:44.169463 14907 sgd_solver.cpp:136] Iteration 13400, lr = 1e-05, m = 0.9
I0815 23:28:03.563326 14907 solver.cpp:312] Iteration 13500 (5.1564 iter/s, 19.3934s/100 iter), loss = 0.0685883
I0815 23:28:03.563386 14907 solver.cpp:334]     Train net output #0: loss = 0.0685882 (* 1 = 0.0685882 loss)
I0815 23:28:03.563393 14907 sgd_solver.cpp:136] Iteration 13500, lr = 1e-05, m = 0.9
I0815 23:28:22.974452 14907 solver.cpp:312] Iteration 13600 (5.15183 iter/s, 19.4106s/100 iter), loss = 0.0735402
I0815 23:28:22.974486 14907 solver.cpp:334]     Train net output #0: loss = 0.0735401 (* 1 = 0.0735401 loss)
I0815 23:28:22.974493 14907 sgd_solver.cpp:136] Iteration 13600, lr = 1e-05, m = 0.9
I0815 23:28:42.704449 14907 solver.cpp:312] Iteration 13700 (5.06856 iter/s, 19.7295s/100 iter), loss = 0.0544922
I0815 23:28:42.704937 14907 solver.cpp:334]     Train net output #0: loss = 0.0544921 (* 1 = 0.0544921 loss)
I0815 23:28:42.704944 14907 sgd_solver.cpp:136] Iteration 13700, lr = 1e-05, m = 0.9
I0815 23:28:45.623405 14920 data_reader.cpp:288] Starting prefetch of epoch 10
I0815 23:29:02.291692 14907 solver.cpp:312] Iteration 13800 (5.1055 iter/s, 19.5867s/100 iter), loss = 0.0542544
I0815 23:29:02.291716 14907 solver.cpp:334]     Train net output #0: loss = 0.0542543 (* 1 = 0.0542543 loss)
I0815 23:29:02.291720 14907 sgd_solver.cpp:136] Iteration 13800, lr = 1e-05, m = 0.9
I0815 23:29:21.718762 14907 solver.cpp:312] Iteration 13900 (5.1476 iter/s, 19.4265s/100 iter), loss = 0.0673829
I0815 23:29:21.718863 14907 solver.cpp:334]     Train net output #0: loss = 0.0673829 (* 1 = 0.0673829 loss)
I0815 23:29:21.718869 14907 sgd_solver.cpp:136] Iteration 13900, lr = 1e-05, m = 0.9
I0815 23:29:40.923331 14907 solver.cpp:363] Sparsity after update:
I0815 23:29:40.933768 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0815 23:29:40.933790 14907 net.cpp:2192] conv1a_param_0(0) 
I0815 23:29:40.933800 14907 net.cpp:2192] conv1b_param_0(0) 
I0815 23:29:40.933804 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0815 23:29:40.933809 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0815 23:29:40.933814 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0815 23:29:40.933817 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0815 23:29:40.933820 14907 net.cpp:2192] ctx_final_param_0(0) 
I0815 23:29:40.933825 14907 net.cpp:2192] out3a_param_0(0) 
I0815 23:29:40.933828 14907 net.cpp:2192] out5a_param_0(0) 
I0815 23:29:40.933832 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 23:29:40.933835 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 23:29:40.933840 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 23:29:40.933843 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 23:29:40.933847 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 23:29:40.933851 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 23:29:40.933853 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 23:29:40.933857 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 23:29:40.933861 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0815 23:29:40.933871 14907 solver.cpp:509] Iteration 14000, Testing net (#0)
I0815 23:29:48.329011 14888 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 23:29:52.869362 14907 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.952773
I0815 23:29:52.869467 14907 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.99917
I0815 23:29:52.869475 14907 solver.cpp:594]     Test net output #2: loss = 0.196637 (* 1 = 0.196637 loss)
I0815 23:29:52.869499 14907 solver.cpp:264] [MultiGPU] Tests completed in 11.9353s
I0815 23:29:53.075677 14907 solver.cpp:312] Iteration 14000 (3.18918 iter/s, 31.3561s/100 iter), loss = 0.0848091
I0815 23:29:53.075700 14907 solver.cpp:334]     Train net output #0: loss = 0.084809 (* 1 = 0.084809 loss)
I0815 23:29:53.075706 14907 sgd_solver.cpp:136] Iteration 14000, lr = 1e-05, m = 0.9
I0815 23:30:02.083956 14872 data_reader.cpp:288] Starting prefetch of epoch 12
I0815 23:30:12.612432 14907 solver.cpp:312] Iteration 14100 (5.1187 iter/s, 19.5362s/100 iter), loss = 0.0855483
I0815 23:30:12.612455 14907 solver.cpp:334]     Train net output #0: loss = 0.0855483 (* 1 = 0.0855483 loss)
I0815 23:30:12.612460 14907 sgd_solver.cpp:136] Iteration 14100, lr = 1e-05, m = 0.9
I0815 23:30:32.567929 14907 solver.cpp:312] Iteration 14200 (5.01129 iter/s, 19.9549s/100 iter), loss = 0.0530053
I0815 23:30:32.567980 14907 solver.cpp:334]     Train net output #0: loss = 0.0530052 (* 1 = 0.0530052 loss)
I0815 23:30:32.567986 14907 sgd_solver.cpp:136] Iteration 14200, lr = 1e-05, m = 0.9
I0815 23:30:52.216013 14907 solver.cpp:312] Iteration 14300 (5.0897 iter/s, 19.6475s/100 iter), loss = 0.107677
I0815 23:30:52.216039 14907 solver.cpp:334]     Train net output #0: loss = 0.107677 (* 1 = 0.107677 loss)
I0815 23:30:52.216044 14907 sgd_solver.cpp:136] Iteration 14300, lr = 1e-05, m = 0.9
I0815 23:31:07.105198 14920 data_reader.cpp:288] Starting prefetch of epoch 11
I0815 23:31:11.843602 14907 solver.cpp:312] Iteration 14400 (5.09501 iter/s, 19.627s/100 iter), loss = 0.0781074
I0815 23:31:11.843631 14907 solver.cpp:334]     Train net output #0: loss = 0.0781073 (* 1 = 0.0781073 loss)
I0815 23:31:11.843637 14907 sgd_solver.cpp:136] Iteration 14400, lr = 1e-05, m = 0.9
I0815 23:31:31.295197 14907 solver.cpp:312] Iteration 14500 (5.14111 iter/s, 19.4511s/100 iter), loss = 0.0770872
I0815 23:31:31.295220 14907 solver.cpp:334]     Train net output #0: loss = 0.0770872 (* 1 = 0.0770872 loss)
I0815 23:31:31.295225 14907 sgd_solver.cpp:136] Iteration 14500, lr = 1e-05, m = 0.9
I0815 23:31:50.810127 14907 solver.cpp:312] Iteration 14600 (5.12442 iter/s, 19.5144s/100 iter), loss = 0.0659908
I0815 23:31:50.810225 14907 solver.cpp:334]     Train net output #0: loss = 0.0659907 (* 1 = 0.0659907 loss)
I0815 23:31:50.810231 14907 sgd_solver.cpp:136] Iteration 14600, lr = 1e-05, m = 0.9
I0815 23:32:10.276232 14907 solver.cpp:312] Iteration 14700 (5.13728 iter/s, 19.4656s/100 iter), loss = 0.0613687
I0815 23:32:10.276257 14907 solver.cpp:334]     Train net output #0: loss = 0.0613687 (* 1 = 0.0613687 loss)
I0815 23:32:10.276262 14907 sgd_solver.cpp:136] Iteration 14700, lr = 1e-05, m = 0.9
I0815 23:32:11.462498 14915 data_reader.cpp:288] Starting prefetch of epoch 7
I0815 23:32:29.663789 14907 solver.cpp:312] Iteration 14800 (5.15809 iter/s, 19.387s/100 iter), loss = 0.0648248
I0815 23:32:29.663835 14907 solver.cpp:334]     Train net output #0: loss = 0.0648248 (* 1 = 0.0648248 loss)
I0815 23:32:29.663839 14907 sgd_solver.cpp:136] Iteration 14800, lr = 1e-05, m = 0.9
I0815 23:32:49.253484 14907 solver.cpp:312] Iteration 14900 (5.10487 iter/s, 19.5891s/100 iter), loss = 0.0774729
I0815 23:32:49.253517 14907 solver.cpp:334]     Train net output #0: loss = 0.0774729 (* 1 = 0.0774729 loss)
I0815 23:32:49.253522 14907 sgd_solver.cpp:136] Iteration 14900, lr = 1e-05, m = 0.9
I0815 23:33:08.669517 14907 solver.cpp:363] Sparsity after update:
I0815 23:33:08.695283 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0815 23:33:08.695385 14907 net.cpp:2192] conv1a_param_0(0) 
I0815 23:33:08.695411 14907 net.cpp:2192] conv1b_param_0(0) 
I0815 23:33:08.695425 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0815 23:33:08.695441 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0815 23:33:08.695454 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0815 23:33:08.695468 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0815 23:33:08.695487 14907 net.cpp:2192] ctx_final_param_0(0) 
I0815 23:33:08.695502 14907 net.cpp:2192] out3a_param_0(0) 
I0815 23:33:08.695515 14907 net.cpp:2192] out5a_param_0(0) 
I0815 23:33:08.695529 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 23:33:08.695544 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 23:33:08.695557 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 23:33:08.695570 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 23:33:08.695585 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 23:33:08.695598 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 23:33:08.695612 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 23:33:08.695626 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 23:33:08.695639 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0815 23:33:08.871237 14907 solver.cpp:312] Iteration 15000 (5.09756 iter/s, 19.6172s/100 iter), loss = 0.0576991
I0815 23:33:08.871268 14907 solver.cpp:334]     Train net output #0: loss = 0.0576991 (* 1 = 0.0576991 loss)
I0815 23:33:08.871274 14907 sgd_solver.cpp:136] Iteration 15000, lr = 1e-05, m = 0.9
I0815 23:33:16.192296 14872 data_reader.cpp:288] Starting prefetch of epoch 13
I0815 23:33:28.537195 14907 solver.cpp:312] Iteration 15100 (5.08507 iter/s, 19.6654s/100 iter), loss = 0.0907492
I0815 23:33:28.537219 14907 solver.cpp:334]     Train net output #0: loss = 0.0907491 (* 1 = 0.0907491 loss)
I0815 23:33:28.537223 14907 sgd_solver.cpp:136] Iteration 15100, lr = 1e-05, m = 0.9
I0815 23:33:48.138231 14907 solver.cpp:312] Iteration 15200 (5.10191 iter/s, 19.6005s/100 iter), loss = 0.0765603
I0815 23:33:48.138283 14907 solver.cpp:334]     Train net output #0: loss = 0.0765602 (* 1 = 0.0765602 loss)
I0815 23:33:48.138289 14907 sgd_solver.cpp:136] Iteration 15200, lr = 1e-05, m = 0.9
I0815 23:34:07.600464 14907 solver.cpp:312] Iteration 15300 (5.1383 iter/s, 19.4617s/100 iter), loss = 0.035469
I0815 23:34:07.600487 14907 solver.cpp:334]     Train net output #0: loss = 0.0354689 (* 1 = 0.0354689 loss)
I0815 23:34:07.600493 14907 sgd_solver.cpp:136] Iteration 15300, lr = 1e-05, m = 0.9
I0815 23:34:20.689438 14918 data_reader.cpp:288] Starting prefetch of epoch 8
I0815 23:34:27.110848 14907 solver.cpp:312] Iteration 15400 (5.12562 iter/s, 19.5099s/100 iter), loss = 0.0717578
I0815 23:34:27.110874 14907 solver.cpp:334]     Train net output #0: loss = 0.0717577 (* 1 = 0.0717577 loss)
I0815 23:34:27.110880 14907 sgd_solver.cpp:136] Iteration 15400, lr = 1e-05, m = 0.9
I0815 23:34:46.839047 14907 solver.cpp:312] Iteration 15500 (5.06903 iter/s, 19.7277s/100 iter), loss = 0.0702776
I0815 23:34:46.839073 14907 solver.cpp:334]     Train net output #0: loss = 0.0702775 (* 1 = 0.0702775 loss)
I0815 23:34:46.839079 14907 sgd_solver.cpp:136] Iteration 15500, lr = 1e-05, m = 0.9
I0815 23:34:53.325502 14916 data_reader.cpp:288] Starting prefetch of epoch 8
I0815 23:35:06.459787 14907 solver.cpp:312] Iteration 15600 (5.09679 iter/s, 19.6202s/100 iter), loss = 0.0613694
I0815 23:35:06.459811 14907 solver.cpp:334]     Train net output #0: loss = 0.0613693 (* 1 = 0.0613693 loss)
I0815 23:35:06.459816 14907 sgd_solver.cpp:136] Iteration 15600, lr = 1e-05, m = 0.9
I0815 23:35:25.958520 14907 solver.cpp:312] Iteration 15700 (5.12868 iter/s, 19.4982s/100 iter), loss = 0.0745149
I0815 23:35:25.958573 14907 solver.cpp:334]     Train net output #0: loss = 0.0745148 (* 1 = 0.0745148 loss)
I0815 23:35:25.958578 14907 sgd_solver.cpp:136] Iteration 15700, lr = 1e-05, m = 0.9
I0815 23:35:45.473044 14907 solver.cpp:312] Iteration 15800 (5.12453 iter/s, 19.514s/100 iter), loss = 0.0713325
I0815 23:35:45.473067 14907 solver.cpp:334]     Train net output #0: loss = 0.0713324 (* 1 = 0.0713324 loss)
I0815 23:35:45.473071 14907 sgd_solver.cpp:136] Iteration 15800, lr = 1e-05, m = 0.9
I0815 23:35:57.994663 14920 data_reader.cpp:288] Starting prefetch of epoch 12
I0815 23:36:05.059864 14907 solver.cpp:312] Iteration 15900 (5.10561 iter/s, 19.5863s/100 iter), loss = 0.0515843
I0815 23:36:05.059885 14907 solver.cpp:334]     Train net output #0: loss = 0.0515842 (* 1 = 0.0515842 loss)
I0815 23:36:05.059891 14907 sgd_solver.cpp:136] Iteration 15900, lr = 1e-05, m = 0.9
I0815 23:36:24.281293 14907 solver.cpp:363] Sparsity after update:
I0815 23:36:24.292484 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0815 23:36:24.292501 14907 net.cpp:2192] conv1a_param_0(0) 
I0815 23:36:24.292508 14907 net.cpp:2192] conv1b_param_0(0) 
I0815 23:36:24.292510 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0815 23:36:24.292512 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0815 23:36:24.292513 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0815 23:36:24.292515 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0815 23:36:24.292517 14907 net.cpp:2192] ctx_final_param_0(0) 
I0815 23:36:24.292520 14907 net.cpp:2192] out3a_param_0(0) 
I0815 23:36:24.292521 14907 net.cpp:2192] out5a_param_0(0) 
I0815 23:36:24.292523 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 23:36:24.292526 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 23:36:24.292527 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 23:36:24.292529 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 23:36:24.292531 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 23:36:24.292533 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 23:36:24.292538 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 23:36:24.292541 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 23:36:24.292544 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0815 23:36:24.292562 14907 solver.cpp:509] Iteration 16000, Testing net (#0)
I0815 23:36:35.296324 14890 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 23:36:36.688540 14907 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.95209
I0815 23:36:36.688561 14907 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999961
I0815 23:36:36.688570 14907 solver.cpp:594]     Test net output #2: loss = 0.160567 (* 1 = 0.160567 loss)
I0815 23:36:36.688604 14907 solver.cpp:264] [MultiGPU] Tests completed in 12.3957s
I0815 23:36:36.885869 14907 solver.cpp:312] Iteration 16000 (3.14217 iter/s, 31.8251s/100 iter), loss = 0.0762705
I0815 23:36:36.885893 14907 solver.cpp:334]     Train net output #0: loss = 0.0762704 (* 1 = 0.0762704 loss)
I0815 23:36:36.885898 14907 sgd_solver.cpp:136] Iteration 16000, lr = 1e-05, m = 0.9
I0815 23:36:42.358113 14870 data_reader.cpp:288] Starting prefetch of epoch 10
I0815 23:36:56.437661 14907 solver.cpp:312] Iteration 16100 (5.11476 iter/s, 19.5513s/100 iter), loss = 0.0716365
I0815 23:36:56.437687 14907 solver.cpp:334]     Train net output #0: loss = 0.0716364 (* 1 = 0.0716364 loss)
I0815 23:36:56.437693 14907 sgd_solver.cpp:136] Iteration 16100, lr = 1e-05, m = 0.9
I0815 23:37:15.732719 14907 solver.cpp:312] Iteration 16200 (5.18282 iter/s, 19.2945s/100 iter), loss = 0.0784553
I0815 23:37:15.732784 14907 solver.cpp:334]     Train net output #0: loss = 0.0784552 (* 1 = 0.0784552 loss)
I0815 23:37:15.732789 14907 sgd_solver.cpp:136] Iteration 16200, lr = 1e-05, m = 0.9
I0815 23:37:35.077229 14907 solver.cpp:312] Iteration 16300 (5.16957 iter/s, 19.344s/100 iter), loss = 0.0690488
I0815 23:37:35.077253 14907 solver.cpp:334]     Train net output #0: loss = 0.0690487 (* 1 = 0.0690487 loss)
I0815 23:37:35.077261 14907 sgd_solver.cpp:136] Iteration 16300, lr = 1e-05, m = 0.9
I0815 23:37:46.493077 14918 data_reader.cpp:288] Starting prefetch of epoch 9
I0815 23:37:54.441293 14907 solver.cpp:312] Iteration 16400 (5.16435 iter/s, 19.3635s/100 iter), loss = 0.0419087
I0815 23:37:54.441361 14907 solver.cpp:334]     Train net output #0: loss = 0.0419086 (* 1 = 0.0419086 loss)
I0815 23:37:54.441383 14907 sgd_solver.cpp:136] Iteration 16400, lr = 1e-05, m = 0.9
I0815 23:38:13.945461 14907 solver.cpp:312] Iteration 16500 (5.12725 iter/s, 19.5036s/100 iter), loss = 0.0660976
I0815 23:38:13.945482 14907 solver.cpp:334]     Train net output #0: loss = 0.0660975 (* 1 = 0.0660975 loss)
I0815 23:38:13.945487 14907 sgd_solver.cpp:136] Iteration 16500, lr = 1e-05, m = 0.9
I0815 23:38:33.219328 14907 solver.cpp:312] Iteration 16600 (5.18852 iter/s, 19.2733s/100 iter), loss = 0.0680654
I0815 23:38:33.219383 14907 solver.cpp:334]     Train net output #0: loss = 0.0680653 (* 1 = 0.0680653 loss)
I0815 23:38:33.219388 14907 sgd_solver.cpp:136] Iteration 16600, lr = 1e-05, m = 0.9
I0815 23:38:50.789521 14872 data_reader.cpp:288] Starting prefetch of epoch 14
I0815 23:38:52.739527 14907 solver.cpp:312] Iteration 16700 (5.12304 iter/s, 19.5197s/100 iter), loss = 0.0746607
I0815 23:38:52.739555 14907 solver.cpp:334]     Train net output #0: loss = 0.0746606 (* 1 = 0.0746606 loss)
I0815 23:38:52.739560 14907 sgd_solver.cpp:136] Iteration 16700, lr = 1e-05, m = 0.9
I0815 23:39:12.203140 14907 solver.cpp:312] Iteration 16800 (5.13793 iter/s, 19.4631s/100 iter), loss = 0.0784965
I0815 23:39:12.203222 14907 solver.cpp:334]     Train net output #0: loss = 0.0784964 (* 1 = 0.0784964 loss)
I0815 23:39:12.203229 14907 sgd_solver.cpp:136] Iteration 16800, lr = 1e-05, m = 0.9
I0815 23:39:23.201671 14916 data_reader.cpp:288] Starting prefetch of epoch 9
I0815 23:39:32.230306 14907 solver.cpp:312] Iteration 16900 (4.99336 iter/s, 20.0266s/100 iter), loss = 0.0630666
I0815 23:39:32.230334 14907 solver.cpp:334]     Train net output #0: loss = 0.0630665 (* 1 = 0.0630665 loss)
I0815 23:39:32.230340 14907 sgd_solver.cpp:136] Iteration 16900, lr = 1e-05, m = 0.9
I0815 23:39:51.578822 14907 solver.cpp:363] Sparsity after update:
I0815 23:39:51.582172 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0815 23:39:51.582209 14907 net.cpp:2192] conv1a_param_0(0) 
I0815 23:39:51.582226 14907 net.cpp:2192] conv1b_param_0(0) 
I0815 23:39:51.582228 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0815 23:39:51.582231 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0815 23:39:51.582234 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0815 23:39:51.582237 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0815 23:39:51.582240 14907 net.cpp:2192] ctx_final_param_0(0) 
I0815 23:39:51.582243 14907 net.cpp:2192] out3a_param_0(0) 
I0815 23:39:51.582247 14907 net.cpp:2192] out5a_param_0(0) 
I0815 23:39:51.582249 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 23:39:51.582252 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 23:39:51.582255 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 23:39:51.582258 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 23:39:51.582260 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 23:39:51.582264 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 23:39:51.582271 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 23:39:51.582274 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 23:39:51.582278 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0815 23:39:51.778899 14907 solver.cpp:312] Iteration 17000 (5.1156 iter/s, 19.5481s/100 iter), loss = 0.0951022
I0815 23:39:51.778925 14907 solver.cpp:334]     Train net output #0: loss = 0.0951021 (* 1 = 0.0951021 loss)
I0815 23:39:51.778930 14907 sgd_solver.cpp:136] Iteration 17000, lr = 1e-05, m = 0.9
I0815 23:40:11.133471 14907 solver.cpp:312] Iteration 17100 (5.16688 iter/s, 19.354s/100 iter), loss = 0.0676104
I0815 23:40:11.133492 14907 solver.cpp:334]     Train net output #0: loss = 0.0676103 (* 1 = 0.0676103 loss)
I0815 23:40:11.133496 14907 sgd_solver.cpp:136] Iteration 17100, lr = 1e-05, m = 0.9
I0815 23:40:27.751163 14920 data_reader.cpp:288] Starting prefetch of epoch 13
I0815 23:40:30.661355 14907 solver.cpp:312] Iteration 17200 (5.12102 iter/s, 19.5273s/100 iter), loss = 0.0815321
I0815 23:40:30.661381 14907 solver.cpp:334]     Train net output #0: loss = 0.081532 (* 1 = 0.081532 loss)
I0815 23:40:30.661386 14907 sgd_solver.cpp:136] Iteration 17200, lr = 1e-05, m = 0.9
I0815 23:40:50.269695 14907 solver.cpp:312] Iteration 17300 (5.10001 iter/s, 19.6078s/100 iter), loss = 0.051093
I0815 23:40:50.269722 14907 solver.cpp:334]     Train net output #0: loss = 0.0510929 (* 1 = 0.0510929 loss)
I0815 23:40:50.269727 14907 sgd_solver.cpp:136] Iteration 17300, lr = 1e-05, m = 0.9
I0815 23:41:09.890775 14907 solver.cpp:312] Iteration 17400 (5.0967 iter/s, 19.6205s/100 iter), loss = 0.0536433
I0815 23:41:09.890857 14907 solver.cpp:334]     Train net output #0: loss = 0.0536432 (* 1 = 0.0536432 loss)
I0815 23:41:09.890863 14907 sgd_solver.cpp:136] Iteration 17400, lr = 1e-05, m = 0.9
I0815 23:41:29.464340 14907 solver.cpp:312] Iteration 17500 (5.10907 iter/s, 19.573s/100 iter), loss = 0.0465695
I0815 23:41:29.464368 14907 solver.cpp:334]     Train net output #0: loss = 0.0465693 (* 1 = 0.0465693 loss)
I0815 23:41:29.464375 14907 sgd_solver.cpp:136] Iteration 17500, lr = 1e-05, m = 0.9
I0815 23:41:32.670158 14872 data_reader.cpp:288] Starting prefetch of epoch 15
I0815 23:41:49.121115 14907 solver.cpp:312] Iteration 17600 (5.08744 iter/s, 19.6562s/100 iter), loss = 0.0751181
I0815 23:41:49.121163 14907 solver.cpp:334]     Train net output #0: loss = 0.075118 (* 1 = 0.075118 loss)
I0815 23:41:49.121168 14907 sgd_solver.cpp:136] Iteration 17600, lr = 1e-05, m = 0.9
I0815 23:42:04.887600 14870 data_reader.cpp:288] Starting prefetch of epoch 11
I0815 23:42:08.574489 14907 solver.cpp:312] Iteration 17700 (5.14064 iter/s, 19.4528s/100 iter), loss = 0.0622319
I0815 23:42:08.574518 14907 solver.cpp:334]     Train net output #0: loss = 0.0622318 (* 1 = 0.0622318 loss)
I0815 23:42:08.574524 14907 sgd_solver.cpp:136] Iteration 17700, lr = 1e-05, m = 0.9
I0815 23:42:28.150519 14907 solver.cpp:312] Iteration 17800 (5.10843 iter/s, 19.5755s/100 iter), loss = 0.0590967
I0815 23:42:28.150576 14907 solver.cpp:334]     Train net output #0: loss = 0.0590966 (* 1 = 0.0590966 loss)
I0815 23:42:28.150581 14907 sgd_solver.cpp:136] Iteration 17800, lr = 1e-05, m = 0.9
I0815 23:42:47.611865 14907 solver.cpp:312] Iteration 17900 (5.13853 iter/s, 19.4608s/100 iter), loss = 0.0812782
I0815 23:42:47.611889 14907 solver.cpp:334]     Train net output #0: loss = 0.081278 (* 1 = 0.081278 loss)
I0815 23:42:47.611893 14907 sgd_solver.cpp:136] Iteration 17900, lr = 1e-05, m = 0.9
I0815 23:43:06.874465 14907 solver.cpp:363] Sparsity after update:
I0815 23:43:06.888511 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0815 23:43:06.888530 14907 net.cpp:2192] conv1a_param_0(0) 
I0815 23:43:06.888540 14907 net.cpp:2192] conv1b_param_0(0) 
I0815 23:43:06.888542 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0815 23:43:06.888545 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0815 23:43:06.888548 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0815 23:43:06.888555 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0815 23:43:06.888557 14907 net.cpp:2192] ctx_final_param_0(0) 
I0815 23:43:06.888561 14907 net.cpp:2192] out3a_param_0(0) 
I0815 23:43:06.888563 14907 net.cpp:2192] out5a_param_0(0) 
I0815 23:43:06.888566 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 23:43:06.888569 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 23:43:06.888574 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 23:43:06.888577 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 23:43:06.888581 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 23:43:06.888586 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 23:43:06.888589 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 23:43:06.888593 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 23:43:06.888597 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0815 23:43:06.888610 14907 solver.cpp:509] Iteration 18000, Testing net (#0)
I0815 23:43:15.176524 14890 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 23:43:19.871060 14907 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.952486
I0815 23:43:19.871081 14907 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999226
I0815 23:43:19.871088 14907 solver.cpp:594]     Test net output #2: loss = 0.202347 (* 1 = 0.202347 loss)
I0815 23:43:19.871116 14907 solver.cpp:264] [MultiGPU] Tests completed in 12.9821s
I0815 23:43:20.073647 14907 solver.cpp:312] Iteration 18000 (3.08063 iter/s, 32.4609s/100 iter), loss = 0.0480344
I0815 23:43:20.073668 14907 solver.cpp:334]     Train net output #0: loss = 0.0480343 (* 1 = 0.0480343 loss)
I0815 23:43:20.073674 14907 sgd_solver.cpp:136] Iteration 18000, lr = 1e-05, m = 0.9
I0815 23:43:39.949877 14907 solver.cpp:312] Iteration 18100 (5.03127 iter/s, 19.8757s/100 iter), loss = 0.0564225
I0815 23:43:39.949967 14907 solver.cpp:334]     Train net output #0: loss = 0.0564223 (* 1 = 0.0564223 loss)
I0815 23:43:39.949975 14907 sgd_solver.cpp:136] Iteration 18100, lr = 1e-05, m = 0.9
I0815 23:43:54.870635 14872 data_reader.cpp:288] Starting prefetch of epoch 16
I0815 23:43:59.437064 14907 solver.cpp:312] Iteration 18200 (5.13172 iter/s, 19.4866s/100 iter), loss = 0.0640341
I0815 23:43:59.437093 14907 solver.cpp:334]     Train net output #0: loss = 0.0640339 (* 1 = 0.0640339 loss)
I0815 23:43:59.437098 14907 sgd_solver.cpp:136] Iteration 18200, lr = 1e-05, m = 0.9
I0815 23:44:19.167204 14907 solver.cpp:312] Iteration 18300 (5.06853 iter/s, 19.7296s/100 iter), loss = 0.070324
I0815 23:44:19.167260 14907 solver.cpp:334]     Train net output #0: loss = 0.0703239 (* 1 = 0.0703239 loss)
I0815 23:44:19.167265 14907 sgd_solver.cpp:136] Iteration 18300, lr = 1e-05, m = 0.9
I0815 23:44:27.441990 14915 data_reader.cpp:288] Starting prefetch of epoch 8
I0815 23:44:38.818123 14907 solver.cpp:312] Iteration 18400 (5.08896 iter/s, 19.6504s/100 iter), loss = 0.0655307
I0815 23:44:38.818146 14907 solver.cpp:334]     Train net output #0: loss = 0.0655305 (* 1 = 0.0655305 loss)
I0815 23:44:38.818150 14907 sgd_solver.cpp:136] Iteration 18400, lr = 1e-05, m = 0.9
I0815 23:44:58.352896 14907 solver.cpp:312] Iteration 18500 (5.11922 iter/s, 19.5342s/100 iter), loss = 0.0746135
I0815 23:44:58.352946 14907 solver.cpp:334]     Train net output #0: loss = 0.0746134 (* 1 = 0.0746134 loss)
I0815 23:44:58.352952 14907 sgd_solver.cpp:136] Iteration 18500, lr = 1e-05, m = 0.9
I0815 23:45:17.941558 14907 solver.cpp:312] Iteration 18600 (5.10513 iter/s, 19.5881s/100 iter), loss = 0.0604071
I0815 23:45:17.941584 14907 solver.cpp:334]     Train net output #0: loss = 0.0604069 (* 1 = 0.0604069 loss)
I0815 23:45:17.941589 14907 sgd_solver.cpp:136] Iteration 18600, lr = 1e-05, m = 0.9
I0815 23:45:32.064332 14920 data_reader.cpp:288] Starting prefetch of epoch 14
I0815 23:45:37.276218 14907 solver.cpp:312] Iteration 18700 (5.1722 iter/s, 19.3341s/100 iter), loss = 0.0455412
I0815 23:45:37.276247 14907 solver.cpp:334]     Train net output #0: loss = 0.045541 (* 1 = 0.045541 loss)
I0815 23:45:37.276252 14907 sgd_solver.cpp:136] Iteration 18700, lr = 1e-05, m = 0.9
I0815 23:45:56.866417 14907 solver.cpp:312] Iteration 18800 (5.10473 iter/s, 19.5897s/100 iter), loss = 0.112264
I0815 23:45:56.866442 14907 solver.cpp:334]     Train net output #0: loss = 0.112263 (* 1 = 0.112263 loss)
I0815 23:45:56.866446 14907 sgd_solver.cpp:136] Iteration 18800, lr = 1e-05, m = 0.9
I0815 23:46:16.550223 14907 solver.cpp:312] Iteration 18900 (5.08046 iter/s, 19.6833s/100 iter), loss = 0.0652813
I0815 23:46:16.550343 14907 solver.cpp:334]     Train net output #0: loss = 0.0652812 (* 1 = 0.0652812 loss)
I0815 23:46:16.550350 14907 sgd_solver.cpp:136] Iteration 18900, lr = 1e-05, m = 0.9
I0815 23:46:35.724416 14907 solver.cpp:363] Sparsity after update:
I0815 23:46:35.744755 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0815 23:46:35.744786 14907 net.cpp:2192] conv1a_param_0(0) 
I0815 23:46:35.744797 14907 net.cpp:2192] conv1b_param_0(0) 
I0815 23:46:35.744801 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0815 23:46:35.744802 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0815 23:46:35.744805 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0815 23:46:35.744808 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0815 23:46:35.744810 14907 net.cpp:2192] ctx_final_param_0(0) 
I0815 23:46:35.744813 14907 net.cpp:2192] out3a_param_0(0) 
I0815 23:46:35.744817 14907 net.cpp:2192] out5a_param_0(0) 
I0815 23:46:35.744822 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 23:46:35.744825 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 23:46:35.744827 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 23:46:35.744830 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 23:46:35.744833 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 23:46:35.744837 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 23:46:35.744838 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 23:46:35.744843 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 23:46:35.744845 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0815 23:46:35.920790 14907 solver.cpp:312] Iteration 19000 (5.16261 iter/s, 19.37s/100 iter), loss = 0.0802453
I0815 23:46:35.920820 14907 solver.cpp:334]     Train net output #0: loss = 0.0802452 (* 1 = 0.0802452 loss)
I0815 23:46:35.920826 14907 sgd_solver.cpp:136] Iteration 19000, lr = 1e-05, m = 0.9
I0815 23:46:36.508731 14918 data_reader.cpp:288] Starting prefetch of epoch 10
I0815 23:46:55.322057 14907 solver.cpp:312] Iteration 19100 (5.15444 iter/s, 19.4007s/100 iter), loss = 0.0533587
I0815 23:46:55.322110 14907 solver.cpp:334]     Train net output #0: loss = 0.0533586 (* 1 = 0.0533586 loss)
I0815 23:46:55.322115 14907 sgd_solver.cpp:136] Iteration 19100, lr = 1e-05, m = 0.9
I0815 23:47:08.772284 14916 data_reader.cpp:288] Starting prefetch of epoch 10
I0815 23:47:14.774088 14907 solver.cpp:312] Iteration 19200 (5.14099 iter/s, 19.4515s/100 iter), loss = 0.0815216
I0815 23:47:14.774113 14907 solver.cpp:334]     Train net output #0: loss = 0.0815215 (* 1 = 0.0815215 loss)
I0815 23:47:14.774119 14907 sgd_solver.cpp:136] Iteration 19200, lr = 1e-05, m = 0.9
I0815 23:47:34.348359 14907 solver.cpp:312] Iteration 19300 (5.10889 iter/s, 19.5737s/100 iter), loss = 0.0875267
I0815 23:47:34.348414 14907 solver.cpp:334]     Train net output #0: loss = 0.0875266 (* 1 = 0.0875266 loss)
I0815 23:47:34.348422 14907 sgd_solver.cpp:136] Iteration 19300, lr = 1e-05, m = 0.9
I0815 23:47:56.267796 14907 solver.cpp:312] Iteration 19400 (4.56228 iter/s, 21.9188s/100 iter), loss = 0.0917167
I0815 23:47:56.267818 14907 solver.cpp:334]     Train net output #0: loss = 0.0917166 (* 1 = 0.0917166 loss)
I0815 23:47:56.267822 14907 sgd_solver.cpp:136] Iteration 19400, lr = 1e-05, m = 0.9
I0815 23:48:16.987520 14872 data_reader.cpp:288] Starting prefetch of epoch 17
I0815 23:48:17.136850 14907 solver.cpp:312] Iteration 19500 (4.79192 iter/s, 20.8685s/100 iter), loss = 0.071186
I0815 23:48:17.136878 14907 solver.cpp:334]     Train net output #0: loss = 0.0711858 (* 1 = 0.0711858 loss)
I0815 23:48:17.136883 14907 sgd_solver.cpp:136] Iteration 19500, lr = 1e-05, m = 0.9
I0815 23:48:37.187561 14907 solver.cpp:312] Iteration 19600 (4.98749 iter/s, 20.0502s/100 iter), loss = 0.0805104
I0815 23:48:37.187593 14907 solver.cpp:334]     Train net output #0: loss = 0.0805103 (* 1 = 0.0805103 loss)
I0815 23:48:37.187600 14907 sgd_solver.cpp:136] Iteration 19600, lr = 1e-05, m = 0.9
I0815 23:48:56.949928 14907 solver.cpp:312] Iteration 19700 (5.06026 iter/s, 19.7618s/100 iter), loss = 0.373376
I0815 23:48:56.949988 14907 solver.cpp:334]     Train net output #0: loss = 0.373376 (* 1 = 0.373376 loss)
I0815 23:48:56.949995 14907 sgd_solver.cpp:136] Iteration 19700, lr = 1e-05, m = 0.9
I0815 23:49:16.817250 14907 solver.cpp:312] Iteration 19800 (5.03353 iter/s, 19.8668s/100 iter), loss = 0.0672699
I0815 23:49:16.817275 14907 solver.cpp:334]     Train net output #0: loss = 0.0672698 (* 1 = 0.0672698 loss)
I0815 23:49:16.817278 14907 sgd_solver.cpp:136] Iteration 19800, lr = 1e-05, m = 0.9
I0815 23:49:22.731447 14920 data_reader.cpp:288] Starting prefetch of epoch 15
I0815 23:49:36.493877 14907 solver.cpp:312] Iteration 19900 (5.08231 iter/s, 19.6761s/100 iter), loss = 0.0644941
I0815 23:49:36.493929 14907 solver.cpp:334]     Train net output #0: loss = 0.0644939 (* 1 = 0.0644939 loss)
I0815 23:49:36.493934 14907 sgd_solver.cpp:136] Iteration 19900, lr = 1e-05, m = 0.9
I0815 23:49:54.862450 14916 data_reader.cpp:288] Starting prefetch of epoch 11
I0815 23:49:55.561434 14907 solver.cpp:639] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/sparse/cityscapes5_jsegnet21v2_iter_20000.caffemodel
I0815 23:49:55.582094 14907 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/sparse/cityscapes5_jsegnet21v2_iter_20000.solverstate
I0815 23:49:55.590448 14907 solver.cpp:363] Sparsity after update:
I0815 23:49:55.601428 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0815 23:49:55.601474 14907 net.cpp:2192] conv1a_param_0(0) 
I0815 23:49:55.601485 14907 net.cpp:2192] conv1b_param_0(0) 
I0815 23:49:55.601493 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0815 23:49:55.601500 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0815 23:49:55.601507 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0815 23:49:55.601514 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0815 23:49:55.601521 14907 net.cpp:2192] ctx_final_param_0(0) 
I0815 23:49:55.601528 14907 net.cpp:2192] out3a_param_0(0) 
I0815 23:49:55.601536 14907 net.cpp:2192] out5a_param_0(0) 
I0815 23:49:55.601542 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 23:49:55.601549 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 23:49:55.601557 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 23:49:55.601563 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 23:49:55.601570 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 23:49:55.601577 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 23:49:55.601584 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 23:49:55.601593 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 23:49:55.601601 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0815 23:49:55.601620 14907 solver.cpp:509] Iteration 20000, Testing net (#0)
I0815 23:50:07.146203 14942 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 23:50:07.605367 14907 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.951085
I0815 23:50:07.605387 14907 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999951
I0815 23:50:07.605394 14907 solver.cpp:594]     Test net output #2: loss = 0.167501 (* 1 = 0.167501 loss)
I0815 23:50:07.605428 14907 solver.cpp:264] [MultiGPU] Tests completed in 12.0035s
I0815 23:50:07.817020 14907 solver.cpp:312] Iteration 20000 (3.19261 iter/s, 31.3223s/100 iter), loss = 0.0470906
I0815 23:50:07.817044 14907 solver.cpp:334]     Train net output #0: loss = 0.0470905 (* 1 = 0.0470905 loss)
I0815 23:50:07.817047 14907 sgd_solver.cpp:136] Iteration 20000, lr = 1e-05, m = 0.9
I0815 23:50:27.355743 14907 solver.cpp:312] Iteration 20100 (5.11818 iter/s, 19.5382s/100 iter), loss = 0.0418307
I0815 23:50:27.355770 14907 solver.cpp:334]     Train net output #0: loss = 0.0418306 (* 1 = 0.0418306 loss)
I0815 23:50:27.355774 14907 sgd_solver.cpp:136] Iteration 20100, lr = 1e-05, m = 0.9
I0815 23:50:46.760087 14907 solver.cpp:312] Iteration 20200 (5.15363 iter/s, 19.4038s/100 iter), loss = 0.0870871
I0815 23:50:46.760154 14907 solver.cpp:334]     Train net output #0: loss = 0.087087 (* 1 = 0.087087 loss)
I0815 23:50:46.760160 14907 sgd_solver.cpp:136] Iteration 20200, lr = 1e-05, m = 0.9
I0815 23:51:06.246397 14907 solver.cpp:312] Iteration 20300 (5.13195 iter/s, 19.4858s/100 iter), loss = 0.0619633
I0815 23:51:06.246423 14907 solver.cpp:334]     Train net output #0: loss = 0.0619632 (* 1 = 0.0619632 loss)
I0815 23:51:06.246429 14907 sgd_solver.cpp:136] Iteration 20300, lr = 1e-05, m = 0.9
I0815 23:51:11.357270 14915 data_reader.cpp:288] Starting prefetch of epoch 9
I0815 23:51:25.894989 14907 solver.cpp:312] Iteration 20400 (5.08956 iter/s, 19.648s/100 iter), loss = 0.0683923
I0815 23:51:25.895035 14907 solver.cpp:334]     Train net output #0: loss = 0.0683922 (* 1 = 0.0683922 loss)
I0815 23:51:25.895651 14907 sgd_solver.cpp:136] Iteration 20400, lr = 1e-05, m = 0.9
I0815 23:51:43.816572 14870 data_reader.cpp:288] Starting prefetch of epoch 12
I0815 23:51:45.542307 14907 solver.cpp:312] Iteration 20500 (5.08989 iter/s, 19.6468s/100 iter), loss = 0.0573736
I0815 23:51:45.542340 14907 solver.cpp:334]     Train net output #0: loss = 0.0573735 (* 1 = 0.0573735 loss)
I0815 23:51:45.542369 14907 sgd_solver.cpp:136] Iteration 20500, lr = 1e-05, m = 0.9
I0815 23:52:05.147907 14907 solver.cpp:312] Iteration 20600 (5.10072 iter/s, 19.6051s/100 iter), loss = 0.0546356
I0815 23:52:05.147956 14907 solver.cpp:334]     Train net output #0: loss = 0.0546355 (* 1 = 0.0546355 loss)
I0815 23:52:05.147961 14907 sgd_solver.cpp:136] Iteration 20600, lr = 1e-05, m = 0.9
I0815 23:52:24.700646 14907 solver.cpp:312] Iteration 20700 (5.11451 iter/s, 19.5522s/100 iter), loss = 0.0457091
I0815 23:52:24.700670 14907 solver.cpp:334]     Train net output #0: loss = 0.045709 (* 1 = 0.045709 loss)
I0815 23:52:24.700673 14907 sgd_solver.cpp:136] Iteration 20700, lr = 1e-05, m = 0.9
I0815 23:52:44.200448 14907 solver.cpp:312] Iteration 20800 (5.1284 iter/s, 19.4993s/100 iter), loss = 0.0406756
I0815 23:52:44.200505 14907 solver.cpp:334]     Train net output #0: loss = 0.0406755 (* 1 = 0.0406755 loss)
I0815 23:52:44.200510 14907 sgd_solver.cpp:136] Iteration 20800, lr = 1e-05, m = 0.9
I0815 23:52:48.308429 14916 data_reader.cpp:288] Starting prefetch of epoch 12
I0815 23:53:03.657646 14907 solver.cpp:312] Iteration 20900 (5.13963 iter/s, 19.4567s/100 iter), loss = 0.0486119
I0815 23:53:03.657675 14907 solver.cpp:334]     Train net output #0: loss = 0.0486118 (* 1 = 0.0486118 loss)
I0815 23:53:03.657680 14907 sgd_solver.cpp:136] Iteration 20900, lr = 1e-05, m = 0.9
I0815 23:53:22.973522 14907 solver.cpp:363] Sparsity after update:
I0815 23:53:22.978873 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0815 23:53:22.978914 14907 net.cpp:2192] conv1a_param_0(0) 
I0815 23:53:22.978929 14907 net.cpp:2192] conv1b_param_0(0) 
I0815 23:53:22.978931 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0815 23:53:22.978935 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0815 23:53:22.978936 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0815 23:53:22.978940 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0815 23:53:22.978942 14907 net.cpp:2192] ctx_final_param_0(0) 
I0815 23:53:22.978945 14907 net.cpp:2192] out3a_param_0(0) 
I0815 23:53:22.978947 14907 net.cpp:2192] out5a_param_0(0) 
I0815 23:53:22.978950 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 23:53:22.978953 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 23:53:22.978955 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 23:53:22.978958 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 23:53:22.978961 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 23:53:22.978965 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 23:53:22.978966 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 23:53:22.978970 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 23:53:22.978972 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0815 23:53:23.186875 14907 solver.cpp:312] Iteration 21000 (5.12067 iter/s, 19.5287s/100 iter), loss = 0.073563
I0815 23:53:23.186902 14907 solver.cpp:334]     Train net output #0: loss = 0.0735629 (* 1 = 0.0735629 loss)
I0815 23:53:23.186908 14907 sgd_solver.cpp:136] Iteration 21000, lr = 1e-05, m = 0.9
I0815 23:53:42.880239 14907 solver.cpp:312] Iteration 21100 (5.07799 iter/s, 19.6928s/100 iter), loss = 0.0546189
I0815 23:53:42.880262 14907 solver.cpp:334]     Train net output #0: loss = 0.0546187 (* 1 = 0.0546187 loss)
I0815 23:53:42.880269 14907 sgd_solver.cpp:136] Iteration 21100, lr = 1e-05, m = 0.9
I0815 23:53:53.167446 14920 data_reader.cpp:288] Starting prefetch of epoch 16
I0815 23:54:02.527252 14907 solver.cpp:312] Iteration 21200 (5.08997 iter/s, 19.6465s/100 iter), loss = 0.0613557
I0815 23:54:02.527279 14907 solver.cpp:334]     Train net output #0: loss = 0.0613556 (* 1 = 0.0613556 loss)
I0815 23:54:02.527287 14907 sgd_solver.cpp:136] Iteration 21200, lr = 1e-05, m = 0.9
I0815 23:54:22.275456 14907 solver.cpp:312] Iteration 21300 (5.06389 iter/s, 19.7477s/100 iter), loss = 0.0770313
I0815 23:54:22.275481 14907 solver.cpp:334]     Train net output #0: loss = 0.0770311 (* 1 = 0.0770311 loss)
I0815 23:54:22.275487 14907 sgd_solver.cpp:136] Iteration 21300, lr = 1e-05, m = 0.9
I0815 23:54:25.668941 14916 data_reader.cpp:288] Starting prefetch of epoch 13
I0815 23:54:41.872234 14907 solver.cpp:312] Iteration 21400 (5.10302 iter/s, 19.5962s/100 iter), loss = 0.0566219
I0815 23:54:41.872263 14907 solver.cpp:334]     Train net output #0: loss = 0.0566218 (* 1 = 0.0566218 loss)
I0815 23:54:41.872270 14907 sgd_solver.cpp:136] Iteration 21400, lr = 1e-05, m = 0.9
I0815 23:55:01.509882 14907 solver.cpp:312] Iteration 21500 (5.0924 iter/s, 19.6371s/100 iter), loss = 0.0473402
I0815 23:55:01.509979 14907 solver.cpp:334]     Train net output #0: loss = 0.0473401 (* 1 = 0.0473401 loss)
I0815 23:55:01.509986 14907 sgd_solver.cpp:136] Iteration 21500, lr = 1e-05, m = 0.9
I0815 23:55:21.145774 14907 solver.cpp:312] Iteration 21600 (5.09285 iter/s, 19.6354s/100 iter), loss = 0.0743124
I0815 23:55:21.145800 14907 solver.cpp:334]     Train net output #0: loss = 0.0743122 (* 1 = 0.0743122 loss)
I0815 23:55:21.145807 14907 sgd_solver.cpp:136] Iteration 21600, lr = 1e-05, m = 0.9
I0815 23:55:30.473955 14870 data_reader.cpp:288] Starting prefetch of epoch 13
I0815 23:55:40.459172 14907 solver.cpp:312] Iteration 21700 (5.1779 iter/s, 19.3129s/100 iter), loss = 0.0564789
I0815 23:55:40.459254 14907 solver.cpp:334]     Train net output #0: loss = 0.0564788 (* 1 = 0.0564788 loss)
I0815 23:55:40.459261 14907 sgd_solver.cpp:136] Iteration 21700, lr = 1e-05, m = 0.9
I0815 23:56:00.035748 14907 solver.cpp:312] Iteration 21800 (5.10829 iter/s, 19.576s/100 iter), loss = 0.0597349
I0815 23:56:00.035810 14907 solver.cpp:334]     Train net output #0: loss = 0.0597348 (* 1 = 0.0597348 loss)
I0815 23:56:00.035827 14907 sgd_solver.cpp:136] Iteration 21800, lr = 1e-05, m = 0.9
I0815 23:56:19.737989 14907 solver.cpp:312] Iteration 21900 (5.0757 iter/s, 19.7017s/100 iter), loss = 0.0650271
I0815 23:56:19.738087 14907 solver.cpp:334]     Train net output #0: loss = 0.0650269 (* 1 = 0.0650269 loss)
I0815 23:56:19.738095 14907 sgd_solver.cpp:136] Iteration 21900, lr = 1e-05, m = 0.9
I0815 23:56:35.157990 14872 data_reader.cpp:288] Starting prefetch of epoch 18
I0815 23:56:39.328775 14907 solver.cpp:363] Sparsity after update:
I0815 23:56:39.331538 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0815 23:56:39.331576 14907 net.cpp:2192] conv1a_param_0(0) 
I0815 23:56:39.331588 14907 net.cpp:2192] conv1b_param_0(0) 
I0815 23:56:39.331591 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0815 23:56:39.331594 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0815 23:56:39.331598 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0815 23:56:39.331600 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0815 23:56:39.331603 14907 net.cpp:2192] ctx_final_param_0(0) 
I0815 23:56:39.331605 14907 net.cpp:2192] out3a_param_0(0) 
I0815 23:56:39.331609 14907 net.cpp:2192] out5a_param_0(0) 
I0815 23:56:39.331610 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0815 23:56:39.331614 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0815 23:56:39.331616 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0815 23:56:39.331619 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0815 23:56:39.331621 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0815 23:56:39.331624 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0815 23:56:39.331626 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0815 23:56:39.331629 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0815 23:56:39.331632 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0815 23:56:39.331645 14907 solver.cpp:509] Iteration 22000, Testing net (#0)
I0815 23:56:47.201822 14942 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 23:56:51.513540 14907 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.953033
I0815 23:56:51.513661 14907 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999359
I0815 23:56:51.513674 14907 solver.cpp:594]     Test net output #2: loss = 0.195634 (* 1 = 0.195634 loss)
I0815 23:56:51.513698 14907 solver.cpp:264] [MultiGPU] Tests completed in 12.1817s
I0815 23:56:51.733094 14907 solver.cpp:312] Iteration 22000 (3.12556 iter/s, 31.9942s/100 iter), loss = 0.0793808
I0815 23:56:51.733119 14907 solver.cpp:334]     Train net output #0: loss = 0.0793807 (* 1 = 0.0793807 loss)
I0815 23:56:51.733124 14907 sgd_solver.cpp:136] Iteration 22000, lr = 1e-05, m = 0.9
I0815 23:57:11.341295 14907 solver.cpp:312] Iteration 22100 (5.10005 iter/s, 19.6077s/100 iter), loss = 0.0533418
I0815 23:57:11.341321 14907 solver.cpp:334]     Train net output #0: loss = 0.0533417 (* 1 = 0.0533417 loss)
I0815 23:57:11.341326 14907 sgd_solver.cpp:136] Iteration 22100, lr = 1e-05, m = 0.9
I0815 23:57:30.695421 14907 solver.cpp:312] Iteration 22200 (5.167 iter/s, 19.3536s/100 iter), loss = 0.0714366
I0815 23:57:30.695472 14907 solver.cpp:334]     Train net output #0: loss = 0.0714365 (* 1 = 0.0714365 loss)
I0815 23:57:30.695477 14907 sgd_solver.cpp:136] Iteration 22200, lr = 1e-05, m = 0.9
I0815 23:57:50.161085 14907 solver.cpp:312] Iteration 22300 (5.1374 iter/s, 19.4651s/100 iter), loss = 0.0450926
I0815 23:57:50.161145 14907 solver.cpp:334]     Train net output #0: loss = 0.0450924 (* 1 = 0.0450924 loss)
I0815 23:57:50.161159 14907 sgd_solver.cpp:136] Iteration 22300, lr = 1e-05, m = 0.9
I0815 23:57:51.924901 14870 data_reader.cpp:288] Starting prefetch of epoch 14
I0815 23:58:09.702455 14907 solver.cpp:312] Iteration 22400 (5.11749 iter/s, 19.5408s/100 iter), loss = 0.0477054
I0815 23:58:09.702538 14907 solver.cpp:334]     Train net output #0: loss = 0.0477053 (* 1 = 0.0477053 loss)
I0815 23:58:09.702545 14907 sgd_solver.cpp:136] Iteration 22400, lr = 1e-05, m = 0.9
I0815 23:58:29.210021 14907 solver.cpp:312] Iteration 22500 (5.12636 iter/s, 19.507s/100 iter), loss = 0.0658046
I0815 23:58:29.210047 14907 solver.cpp:334]     Train net output #0: loss = 0.0658045 (* 1 = 0.0658045 loss)
I0815 23:58:29.210052 14907 sgd_solver.cpp:136] Iteration 22500, lr = 1e-05, m = 0.9
I0815 23:58:49.048873 14907 solver.cpp:312] Iteration 22600 (5.04075 iter/s, 19.8383s/100 iter), loss = 0.0439003
I0815 23:58:49.048945 14907 solver.cpp:334]     Train net output #0: loss = 0.0439002 (* 1 = 0.0439002 loss)
I0815 23:58:49.048950 14907 sgd_solver.cpp:136] Iteration 22600, lr = 1e-05, m = 0.9
I0815 23:58:56.923501 14872 data_reader.cpp:288] Starting prefetch of epoch 19
I0815 23:59:08.686321 14907 solver.cpp:312] Iteration 22700 (5.09245 iter/s, 19.6369s/100 iter), loss = 0.0712108
I0815 23:59:08.686349 14907 solver.cpp:334]     Train net output #0: loss = 0.0712107 (* 1 = 0.0712107 loss)
I0815 23:59:08.686357 14907 sgd_solver.cpp:136] Iteration 22700, lr = 1e-05, m = 0.9
I0815 23:59:28.223240 14907 solver.cpp:312] Iteration 22800 (5.11866 iter/s, 19.5364s/100 iter), loss = 0.0414534
I0815 23:59:28.223364 14907 solver.cpp:334]     Train net output #0: loss = 0.0414533 (* 1 = 0.0414533 loss)
I0815 23:59:28.223371 14907 sgd_solver.cpp:136] Iteration 22800, lr = 1e-05, m = 0.9
I0815 23:59:29.269711 14915 data_reader.cpp:288] Starting prefetch of epoch 10
I0815 23:59:47.747740 14907 solver.cpp:312] Iteration 22900 (5.12191 iter/s, 19.524s/100 iter), loss = 0.0685426
I0815 23:59:47.747794 14907 solver.cpp:334]     Train net output #0: loss = 0.0685425 (* 1 = 0.0685425 loss)
I0815 23:59:47.747809 14907 sgd_solver.cpp:136] Iteration 22900, lr = 1e-05, m = 0.9
I0816 00:00:07.189261 14907 solver.cpp:363] Sparsity after update:
I0816 00:00:07.214704 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0816 00:00:07.214764 14907 net.cpp:2192] conv1a_param_0(0) 
I0816 00:00:07.214778 14907 net.cpp:2192] conv1b_param_0(0) 
I0816 00:00:07.214781 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0816 00:00:07.214784 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0816 00:00:07.214787 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0816 00:00:07.214789 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0816 00:00:07.214792 14907 net.cpp:2192] ctx_final_param_0(0) 
I0816 00:00:07.214795 14907 net.cpp:2192] out3a_param_0(0) 
I0816 00:00:07.214798 14907 net.cpp:2192] out5a_param_0(0) 
I0816 00:00:07.214804 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0816 00:00:07.214807 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0816 00:00:07.214810 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0816 00:00:07.214813 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0816 00:00:07.214818 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0816 00:00:07.214819 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0816 00:00:07.214823 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0816 00:00:07.214825 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0816 00:00:07.214828 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0816 00:00:07.401695 14907 solver.cpp:312] Iteration 23000 (5.08817 iter/s, 19.6534s/100 iter), loss = 0.0848114
I0816 00:00:07.401717 14907 solver.cpp:334]     Train net output #0: loss = 0.0848113 (* 1 = 0.0848113 loss)
I0816 00:00:07.401722 14907 sgd_solver.cpp:136] Iteration 23000, lr = 1e-05, m = 0.9
I0816 00:00:27.302656 14907 solver.cpp:312] Iteration 23100 (5.02502 iter/s, 19.9004s/100 iter), loss = 0.0521976
I0816 00:00:27.302681 14907 solver.cpp:334]     Train net output #0: loss = 0.0521975 (* 1 = 0.0521975 loss)
I0816 00:00:27.302685 14907 sgd_solver.cpp:136] Iteration 23100, lr = 1e-05, m = 0.9
I0816 00:00:34.416801 14918 data_reader.cpp:288] Starting prefetch of epoch 11
I0816 00:00:47.030138 14907 solver.cpp:312] Iteration 23200 (5.06921 iter/s, 19.7269s/100 iter), loss = 0.0476554
I0816 00:00:47.030211 14907 solver.cpp:334]     Train net output #0: loss = 0.0476553 (* 1 = 0.0476553 loss)
I0816 00:00:47.030217 14907 sgd_solver.cpp:136] Iteration 23200, lr = 1e-05, m = 0.9
I0816 00:01:06.613134 14907 solver.cpp:312] Iteration 23300 (5.10661 iter/s, 19.5825s/100 iter), loss = 0.0394216
I0816 00:01:06.613188 14907 solver.cpp:334]     Train net output #0: loss = 0.0394215 (* 1 = 0.0394215 loss)
I0816 00:01:06.613200 14907 sgd_solver.cpp:136] Iteration 23300, lr = 1e-05, m = 0.9
I0816 00:01:26.184684 14907 solver.cpp:312] Iteration 23400 (5.1096 iter/s, 19.571s/100 iter), loss = 0.0516496
I0816 00:01:26.184743 14907 solver.cpp:334]     Train net output #0: loss = 0.0516495 (* 1 = 0.0516495 loss)
I0816 00:01:26.184749 14907 sgd_solver.cpp:136] Iteration 23400, lr = 1e-05, m = 0.9
I0816 00:01:39.124276 14918 data_reader.cpp:288] Starting prefetch of epoch 12
I0816 00:01:45.708436 14907 solver.cpp:312] Iteration 23500 (5.12211 iter/s, 19.5232s/100 iter), loss = 0.192318
I0816 00:01:45.708461 14907 solver.cpp:334]     Train net output #0: loss = 0.192318 (* 1 = 0.192318 loss)
I0816 00:01:45.708465 14907 sgd_solver.cpp:136] Iteration 23500, lr = 1e-05, m = 0.9
I0816 00:02:05.438953 14907 solver.cpp:312] Iteration 23600 (5.06843 iter/s, 19.73s/100 iter), loss = 0.0414409
I0816 00:02:05.439055 14907 solver.cpp:334]     Train net output #0: loss = 0.0414408 (* 1 = 0.0414408 loss)
I0816 00:02:05.439069 14907 sgd_solver.cpp:136] Iteration 23600, lr = 1e-05, m = 0.9
I0816 00:02:11.577553 14916 data_reader.cpp:288] Starting prefetch of epoch 14
I0816 00:02:25.074669 14907 solver.cpp:312] Iteration 23700 (5.0929 iter/s, 19.6352s/100 iter), loss = 0.0620911
I0816 00:02:25.074697 14907 solver.cpp:334]     Train net output #0: loss = 0.062091 (* 1 = 0.062091 loss)
I0816 00:02:25.074702 14907 sgd_solver.cpp:136] Iteration 23700, lr = 1e-05, m = 0.9
I0816 00:02:44.692782 14907 solver.cpp:312] Iteration 23800 (5.09747 iter/s, 19.6176s/100 iter), loss = 0.0884648
I0816 00:02:44.692881 14907 solver.cpp:334]     Train net output #0: loss = 0.0884647 (* 1 = 0.0884647 loss)
I0816 00:02:44.692901 14907 sgd_solver.cpp:136] Iteration 23800, lr = 1e-05, m = 0.9
I0816 00:03:04.754312 14907 solver.cpp:312] Iteration 23900 (4.9848 iter/s, 20.061s/100 iter), loss = 0.0586731
I0816 00:03:04.754340 14907 solver.cpp:334]     Train net output #0: loss = 0.058673 (* 1 = 0.058673 loss)
I0816 00:03:04.754346 14907 sgd_solver.cpp:136] Iteration 23900, lr = 1e-05, m = 0.9
I0816 00:03:16.898092 14870 data_reader.cpp:288] Starting prefetch of epoch 15
I0816 00:03:24.141130 14907 solver.cpp:363] Sparsity after update:
I0816 00:03:24.151890 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0816 00:03:24.151908 14907 net.cpp:2192] conv1a_param_0(0) 
I0816 00:03:24.151916 14907 net.cpp:2192] conv1b_param_0(0) 
I0816 00:03:24.151919 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0816 00:03:24.151922 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0816 00:03:24.151924 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0816 00:03:24.151927 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0816 00:03:24.151932 14907 net.cpp:2192] ctx_final_param_0(0) 
I0816 00:03:24.151937 14907 net.cpp:2192] out3a_param_0(0) 
I0816 00:03:24.151940 14907 net.cpp:2192] out5a_param_0(0) 
I0816 00:03:24.151943 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0816 00:03:24.151947 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0816 00:03:24.151952 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0816 00:03:24.151957 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0816 00:03:24.151959 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0816 00:03:24.151963 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0816 00:03:24.151968 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0816 00:03:24.151970 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0816 00:03:24.151974 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0816 00:03:24.151988 14907 solver.cpp:509] Iteration 24000, Testing net (#0)
I0816 00:03:35.324319 14888 data_reader.cpp:288] Starting prefetch of epoch 2
I0816 00:03:36.222295 14907 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.95118
I0816 00:03:36.222319 14907 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999949
I0816 00:03:36.222326 14907 solver.cpp:594]     Test net output #2: loss = 0.165574 (* 1 = 0.165574 loss)
I0816 00:03:36.222352 14907 solver.cpp:264] [MultiGPU] Tests completed in 12.07s
I0816 00:03:36.319121 14962 sgd_solver.cpp:48] MultiStep Status: Iteration 24000, step = 1
I0816 00:03:36.319182 14961 sgd_solver.cpp:48] MultiStep Status: Iteration 24000, step = 1
I0816 00:03:36.319206 14960 sgd_solver.cpp:48] MultiStep Status: Iteration 24000, step = 1
I0816 00:03:36.424865 14907 solver.cpp:312] Iteration 24000 (3.15759 iter/s, 31.6697s/100 iter), loss = 0.0784022
I0816 00:03:36.424890 14907 solver.cpp:334]     Train net output #0: loss = 0.0784021 (* 1 = 0.0784021 loss)
I0816 00:03:36.424896 14907 sgd_solver.cpp:136] Iteration 24000, lr = 1e-06, m = 0.9
I0816 00:03:55.882346 14907 solver.cpp:312] Iteration 24100 (5.13955 iter/s, 19.4569s/100 iter), loss = 0.0666187
I0816 00:03:55.882412 14907 solver.cpp:334]     Train net output #0: loss = 0.0666185 (* 1 = 0.0666185 loss)
I0816 00:03:55.882417 14907 sgd_solver.cpp:136] Iteration 24100, lr = 1e-06, m = 0.9
I0816 00:04:01.147586 14916 data_reader.cpp:288] Starting prefetch of epoch 15
I0816 00:04:15.434377 14907 solver.cpp:312] Iteration 24200 (5.1147 iter/s, 19.5515s/100 iter), loss = 0.0800891
I0816 00:04:15.434403 14907 solver.cpp:334]     Train net output #0: loss = 0.080089 (* 1 = 0.080089 loss)
I0816 00:04:15.434407 14907 sgd_solver.cpp:136] Iteration 24200, lr = 1e-06, m = 0.9
I0816 00:04:35.086583 14907 solver.cpp:312] Iteration 24300 (5.08863 iter/s, 19.6517s/100 iter), loss = 0.0538179
I0816 00:04:35.086848 14907 solver.cpp:334]     Train net output #0: loss = 0.0538178 (* 1 = 0.0538178 loss)
I0816 00:04:35.086854 14907 sgd_solver.cpp:136] Iteration 24300, lr = 1e-06, m = 0.9
I0816 00:04:54.695070 14907 solver.cpp:312] Iteration 24400 (5.09997 iter/s, 19.6079s/100 iter), loss = 0.0710455
I0816 00:04:54.695092 14907 solver.cpp:334]     Train net output #0: loss = 0.0710454 (* 1 = 0.0710454 loss)
I0816 00:04:54.695098 14907 sgd_solver.cpp:136] Iteration 24400, lr = 1e-06, m = 0.9
I0816 00:05:05.814119 14870 data_reader.cpp:288] Starting prefetch of epoch 16
I0816 00:05:13.902207 14907 solver.cpp:312] Iteration 24500 (5.20654 iter/s, 19.2066s/100 iter), loss = 0.0540409
I0816 00:05:13.902232 14907 solver.cpp:334]     Train net output #0: loss = 0.0540408 (* 1 = 0.0540408 loss)
I0816 00:05:13.902236 14907 sgd_solver.cpp:136] Iteration 24500, lr = 1e-06, m = 0.9
I0816 00:05:33.435394 14907 solver.cpp:312] Iteration 24600 (5.11963 iter/s, 19.5327s/100 iter), loss = 0.0912744
I0816 00:05:33.435417 14907 solver.cpp:334]     Train net output #0: loss = 0.0912743 (* 1 = 0.0912743 loss)
I0816 00:05:33.435422 14907 sgd_solver.cpp:136] Iteration 24600, lr = 1e-06, m = 0.9
I0816 00:05:53.158977 14907 solver.cpp:312] Iteration 24700 (5.07021 iter/s, 19.723s/100 iter), loss = 0.0676383
I0816 00:05:53.159029 14907 solver.cpp:334]     Train net output #0: loss = 0.0676382 (* 1 = 0.0676382 loss)
I0816 00:05:53.159034 14907 sgd_solver.cpp:136] Iteration 24700, lr = 1e-06, m = 0.9
I0816 00:06:10.501845 14918 data_reader.cpp:288] Starting prefetch of epoch 13
I0816 00:06:12.822973 14907 solver.cpp:312] Iteration 24800 (5.08558 iter/s, 19.6635s/100 iter), loss = 0.0885494
I0816 00:06:12.822996 14907 solver.cpp:334]     Train net output #0: loss = 0.0885493 (* 1 = 0.0885493 loss)
I0816 00:06:12.823001 14907 sgd_solver.cpp:136] Iteration 24800, lr = 1e-06, m = 0.9
I0816 00:06:32.218603 14907 solver.cpp:312] Iteration 24900 (5.15594 iter/s, 19.3951s/100 iter), loss = 0.0614068
I0816 00:06:32.218655 14907 solver.cpp:334]     Train net output #0: loss = 0.0614067 (* 1 = 0.0614067 loss)
I0816 00:06:32.218662 14907 sgd_solver.cpp:136] Iteration 24900, lr = 1e-06, m = 0.9
I0816 00:06:42.681551 14916 data_reader.cpp:288] Starting prefetch of epoch 16
I0816 00:06:51.545001 14907 solver.cpp:363] Sparsity after update:
I0816 00:06:51.579069 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0816 00:06:51.579090 14907 net.cpp:2192] conv1a_param_0(0) 
I0816 00:06:51.579097 14907 net.cpp:2192] conv1b_param_0(0) 
I0816 00:06:51.579099 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0816 00:06:51.579102 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0816 00:06:51.579102 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0816 00:06:51.579104 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0816 00:06:51.579107 14907 net.cpp:2192] ctx_final_param_0(0) 
I0816 00:06:51.579108 14907 net.cpp:2192] out3a_param_0(0) 
I0816 00:06:51.579110 14907 net.cpp:2192] out5a_param_0(0) 
I0816 00:06:51.579113 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0816 00:06:51.579113 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0816 00:06:51.579115 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0816 00:06:51.579118 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0816 00:06:51.579123 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0816 00:06:51.579126 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0816 00:06:51.579129 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0816 00:06:51.579133 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0816 00:06:51.579135 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0816 00:06:51.752192 14907 solver.cpp:312] Iteration 25000 (5.11953 iter/s, 19.5331s/100 iter), loss = 0.0695945
I0816 00:06:51.752218 14907 solver.cpp:334]     Train net output #0: loss = 0.0695944 (* 1 = 0.0695944 loss)
I0816 00:06:51.752224 14907 sgd_solver.cpp:136] Iteration 25000, lr = 1e-06, m = 0.9
I0816 00:07:11.468677 14907 solver.cpp:312] Iteration 25100 (5.07204 iter/s, 19.7159s/100 iter), loss = 0.0561501
I0816 00:07:11.468768 14907 solver.cpp:334]     Train net output #0: loss = 0.05615 (* 1 = 0.05615 loss)
I0816 00:07:11.468776 14907 sgd_solver.cpp:136] Iteration 25100, lr = 1e-06, m = 0.9
I0816 00:07:30.966171 14907 solver.cpp:312] Iteration 25200 (5.129 iter/s, 19.497s/100 iter), loss = 0.0804161
I0816 00:07:30.966198 14907 solver.cpp:334]     Train net output #0: loss = 0.080416 (* 1 = 0.080416 loss)
I0816 00:07:30.966204 14907 sgd_solver.cpp:136] Iteration 25200, lr = 1e-06, m = 0.9
I0816 00:07:47.416268 14915 data_reader.cpp:288] Starting prefetch of epoch 11
I0816 00:07:50.545424 14907 solver.cpp:312] Iteration 25300 (5.10759 iter/s, 19.5787s/100 iter), loss = 0.075048
I0816 00:07:50.545450 14907 solver.cpp:334]     Train net output #0: loss = 0.0750479 (* 1 = 0.0750479 loss)
I0816 00:07:50.545455 14907 sgd_solver.cpp:136] Iteration 25300, lr = 1e-06, m = 0.9
I0816 00:08:10.232336 14907 solver.cpp:312] Iteration 25400 (5.07966 iter/s, 19.6864s/100 iter), loss = 0.0658098
I0816 00:08:10.232362 14907 solver.cpp:334]     Train net output #0: loss = 0.0658097 (* 1 = 0.0658097 loss)
I0816 00:08:10.232367 14907 sgd_solver.cpp:136] Iteration 25400, lr = 1e-06, m = 0.9
I0816 00:08:29.864226 14907 solver.cpp:312] Iteration 25500 (5.09389 iter/s, 19.6314s/100 iter), loss = 0.0508077
I0816 00:08:29.864282 14907 solver.cpp:334]     Train net output #0: loss = 0.0508076 (* 1 = 0.0508076 loss)
I0816 00:08:29.864289 14907 sgd_solver.cpp:136] Iteration 25500, lr = 1e-06, m = 0.9
I0816 00:08:49.316244 14907 solver.cpp:312] Iteration 25600 (5.141 iter/s, 19.4515s/100 iter), loss = 0.060869
I0816 00:08:49.316272 14907 solver.cpp:334]     Train net output #0: loss = 0.0608689 (* 1 = 0.0608689 loss)
I0816 00:08:49.316275 14907 sgd_solver.cpp:136] Iteration 25600, lr = 1e-06, m = 0.9
I0816 00:08:52.249717 14918 data_reader.cpp:288] Starting prefetch of epoch 14
I0816 00:09:09.117440 14907 solver.cpp:312] Iteration 25700 (5.05034 iter/s, 19.8007s/100 iter), loss = 0.0438456
I0816 00:09:09.117496 14907 solver.cpp:334]     Train net output #0: loss = 0.0438455 (* 1 = 0.0438455 loss)
I0816 00:09:09.117503 14907 sgd_solver.cpp:136] Iteration 25700, lr = 1e-06, m = 0.9
I0816 00:09:28.561662 14907 solver.cpp:312] Iteration 25800 (5.14306 iter/s, 19.4437s/100 iter), loss = 0.0661617
I0816 00:09:28.561687 14907 solver.cpp:334]     Train net output #0: loss = 0.0661616 (* 1 = 0.0661616 loss)
I0816 00:09:28.561691 14907 sgd_solver.cpp:136] Iteration 25800, lr = 1e-06, m = 0.9
I0816 00:09:48.081126 14907 solver.cpp:312] Iteration 25900 (5.12323 iter/s, 19.5189s/100 iter), loss = 0.0532603
I0816 00:09:48.081174 14907 solver.cpp:334]     Train net output #0: loss = 0.0532602 (* 1 = 0.0532602 loss)
I0816 00:09:48.081179 14907 sgd_solver.cpp:136] Iteration 25900, lr = 1e-06, m = 0.9
I0816 00:09:56.882515 14918 data_reader.cpp:288] Starting prefetch of epoch 15
I0816 00:10:07.287201 14907 solver.cpp:363] Sparsity after update:
I0816 00:10:07.300873 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0816 00:10:07.300896 14907 net.cpp:2192] conv1a_param_0(0) 
I0816 00:10:07.300904 14907 net.cpp:2192] conv1b_param_0(0) 
I0816 00:10:07.300907 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0816 00:10:07.300910 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0816 00:10:07.300914 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0816 00:10:07.300916 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0816 00:10:07.300920 14907 net.cpp:2192] ctx_final_param_0(0) 
I0816 00:10:07.300923 14907 net.cpp:2192] out3a_param_0(0) 
I0816 00:10:07.300926 14907 net.cpp:2192] out5a_param_0(0) 
I0816 00:10:07.300930 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0816 00:10:07.300932 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0816 00:10:07.300935 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0816 00:10:07.300938 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0816 00:10:07.300941 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0816 00:10:07.300945 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0816 00:10:07.300948 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0816 00:10:07.300951 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0816 00:10:07.300954 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0816 00:10:07.300971 14907 solver.cpp:509] Iteration 26000, Testing net (#0)
I0816 00:10:15.144742 14944 data_reader.cpp:288] Starting prefetch of epoch 3
I0816 00:10:19.369318 14907 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.952633
I0816 00:10:19.369441 14907 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999248
I0816 00:10:19.369451 14907 solver.cpp:594]     Test net output #2: loss = 0.203218 (* 1 = 0.203218 loss)
I0816 00:10:19.369477 14907 solver.cpp:264] [MultiGPU] Tests completed in 12.0682s
I0816 00:10:19.592017 14907 solver.cpp:312] Iteration 26000 (3.17359 iter/s, 31.51s/100 iter), loss = 0.0821776
I0816 00:10:19.592046 14907 solver.cpp:334]     Train net output #0: loss = 0.0821775 (* 1 = 0.0821775 loss)
I0816 00:10:19.592052 14907 sgd_solver.cpp:136] Iteration 26000, lr = 1e-06, m = 0.9
I0816 00:10:38.996999 14907 solver.cpp:312] Iteration 26100 (5.15346 iter/s, 19.4044s/100 iter), loss = 0.0409351
I0816 00:10:38.997022 14907 solver.cpp:334]     Train net output #0: loss = 0.040935 (* 1 = 0.040935 loss)
I0816 00:10:38.997027 14907 sgd_solver.cpp:136] Iteration 26100, lr = 1e-06, m = 0.9
I0816 00:10:58.694727 14907 solver.cpp:312] Iteration 26200 (5.07687 iter/s, 19.6972s/100 iter), loss = 0.0884714
I0816 00:10:58.694782 14907 solver.cpp:334]     Train net output #0: loss = 0.0884713 (* 1 = 0.0884713 loss)
I0816 00:10:58.694789 14907 sgd_solver.cpp:136] Iteration 26200, lr = 1e-06, m = 0.9
I0816 00:11:13.464109 14918 data_reader.cpp:288] Starting prefetch of epoch 16
I0816 00:11:18.124089 14907 solver.cpp:312] Iteration 26300 (5.14699 iter/s, 19.4288s/100 iter), loss = 0.0742401
I0816 00:11:18.124109 14907 solver.cpp:334]     Train net output #0: loss = 0.07424 (* 1 = 0.07424 loss)
I0816 00:11:18.124114 14907 sgd_solver.cpp:136] Iteration 26300, lr = 1e-06, m = 0.9
I0816 00:11:37.469102 14907 solver.cpp:312] Iteration 26400 (5.16943 iter/s, 19.3445s/100 iter), loss = 0.0862997
I0816 00:11:37.469183 14907 solver.cpp:334]     Train net output #0: loss = 0.0862996 (* 1 = 0.0862996 loss)
I0816 00:11:37.469190 14907 sgd_solver.cpp:136] Iteration 26400, lr = 1e-06, m = 0.9
I0816 00:11:57.026494 14907 solver.cpp:312] Iteration 26500 (5.1133 iter/s, 19.5569s/100 iter), loss = 0.044197
I0816 00:11:57.026515 14907 solver.cpp:334]     Train net output #0: loss = 0.0441969 (* 1 = 0.0441969 loss)
I0816 00:11:57.026518 14907 sgd_solver.cpp:136] Iteration 26500, lr = 1e-06, m = 0.9
I0816 00:12:16.701149 14907 solver.cpp:312] Iteration 26600 (5.08282 iter/s, 19.6741s/100 iter), loss = 0.0747174
I0816 00:12:16.701196 14907 solver.cpp:334]     Train net output #0: loss = 0.0747173 (* 1 = 0.0747173 loss)
I0816 00:12:16.701201 14907 sgd_solver.cpp:136] Iteration 26600, lr = 1e-06, m = 0.9
I0816 00:12:17.879559 14920 data_reader.cpp:288] Starting prefetch of epoch 17
I0816 00:12:35.962353 14907 solver.cpp:312] Iteration 26700 (5.19193 iter/s, 19.2607s/100 iter), loss = 0.0631532
I0816 00:12:35.962378 14907 solver.cpp:334]     Train net output #0: loss = 0.0631531 (* 1 = 0.0631531 loss)
I0816 00:12:35.962383 14907 sgd_solver.cpp:136] Iteration 26700, lr = 1e-06, m = 0.9
I0816 00:12:50.081393 14915 data_reader.cpp:288] Starting prefetch of epoch 12
I0816 00:12:55.803917 14907 solver.cpp:312] Iteration 26800 (5.04006 iter/s, 19.841s/100 iter), loss = 0.0761298
I0816 00:12:55.803941 14907 solver.cpp:334]     Train net output #0: loss = 0.0761297 (* 1 = 0.0761297 loss)
I0816 00:12:55.803946 14907 sgd_solver.cpp:136] Iteration 26800, lr = 1e-06, m = 0.9
I0816 00:13:15.540285 14907 solver.cpp:312] Iteration 26900 (5.06693 iter/s, 19.7358s/100 iter), loss = 0.0533744
I0816 00:13:15.540309 14907 solver.cpp:334]     Train net output #0: loss = 0.0533743 (* 1 = 0.0533743 loss)
I0816 00:13:15.540313 14907 sgd_solver.cpp:136] Iteration 26900, lr = 1e-06, m = 0.9
I0816 00:13:35.301862 14907 solver.cpp:363] Sparsity after update:
I0816 00:13:35.321022 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0816 00:13:35.321069 14907 net.cpp:2192] conv1a_param_0(0) 
I0816 00:13:35.321084 14907 net.cpp:2192] conv1b_param_0(0) 
I0816 00:13:35.321086 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0816 00:13:35.321089 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0816 00:13:35.321091 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0816 00:13:35.321094 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0816 00:13:35.321097 14907 net.cpp:2192] ctx_final_param_0(0) 
I0816 00:13:35.321102 14907 net.cpp:2192] out3a_param_0(0) 
I0816 00:13:35.321105 14907 net.cpp:2192] out5a_param_0(0) 
I0816 00:13:35.321108 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0816 00:13:35.321111 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0816 00:13:35.321113 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0816 00:13:35.321116 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0816 00:13:35.321120 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0816 00:13:35.321121 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0816 00:13:35.321125 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0816 00:13:35.321127 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0816 00:13:35.321130 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0816 00:13:35.504619 14907 solver.cpp:312] Iteration 27000 (5.00907 iter/s, 19.9638s/100 iter), loss = 0.0966118
I0816 00:13:35.504645 14907 solver.cpp:334]     Train net output #0: loss = 0.0966117 (* 1 = 0.0966117 loss)
I0816 00:13:35.504649 14907 sgd_solver.cpp:136] Iteration 27000, lr = 1e-06, m = 0.9
I0816 00:13:54.948097 14907 solver.cpp:312] Iteration 27100 (5.14325 iter/s, 19.4429s/100 iter), loss = 0.0652169
I0816 00:13:54.948122 14907 solver.cpp:334]     Train net output #0: loss = 0.0652168 (* 1 = 0.0652168 loss)
I0816 00:13:54.948125 14907 sgd_solver.cpp:136] Iteration 27100, lr = 1e-06, m = 0.9
I0816 00:13:55.387607 14918 data_reader.cpp:288] Starting prefetch of epoch 17
I0816 00:14:14.626161 14907 solver.cpp:312] Iteration 27200 (5.08194 iter/s, 19.6775s/100 iter), loss = 0.0413523
I0816 00:14:14.626241 14907 solver.cpp:334]     Train net output #0: loss = 0.0413522 (* 1 = 0.0413522 loss)
I0816 00:14:14.626248 14907 sgd_solver.cpp:136] Iteration 27200, lr = 1e-06, m = 0.9
I0816 00:14:34.288139 14907 solver.cpp:312] Iteration 27300 (5.0861 iter/s, 19.6614s/100 iter), loss = 0.062718
I0816 00:14:34.288193 14907 solver.cpp:334]     Train net output #0: loss = 0.0627179 (* 1 = 0.0627179 loss)
I0816 00:14:34.288211 14907 sgd_solver.cpp:136] Iteration 27300, lr = 1e-06, m = 0.9
I0816 00:14:54.368080 14907 solver.cpp:312] Iteration 27400 (4.98023 iter/s, 20.0794s/100 iter), loss = 0.0911269
I0816 00:14:54.368192 14907 solver.cpp:334]     Train net output #0: loss = 0.0911268 (* 1 = 0.0911268 loss)
I0816 00:14:54.368207 14907 sgd_solver.cpp:136] Iteration 27400, lr = 1e-06, m = 0.9
I0816 00:15:00.749447 14872 data_reader.cpp:288] Starting prefetch of epoch 20
I0816 00:15:13.785629 14907 solver.cpp:312] Iteration 27500 (5.15012 iter/s, 19.417s/100 iter), loss = 0.058027
I0816 00:15:13.785681 14907 solver.cpp:334]     Train net output #0: loss = 0.058027 (* 1 = 0.058027 loss)
I0816 00:15:13.785693 14907 sgd_solver.cpp:136] Iteration 27500, lr = 1e-06, m = 0.9
I0816 00:15:32.828485 14915 data_reader.cpp:288] Starting prefetch of epoch 13
I0816 00:15:33.180001 14907 solver.cpp:312] Iteration 27600 (5.15628 iter/s, 19.3938s/100 iter), loss = 0.0757859
I0816 00:15:33.180029 14907 solver.cpp:334]     Train net output #0: loss = 0.0757858 (* 1 = 0.0757858 loss)
I0816 00:15:33.180035 14907 sgd_solver.cpp:136] Iteration 27600, lr = 1e-06, m = 0.9
I0816 00:15:52.794209 14907 solver.cpp:312] Iteration 27700 (5.09849 iter/s, 19.6137s/100 iter), loss = 0.0796964
I0816 00:15:52.794234 14907 solver.cpp:334]     Train net output #0: loss = 0.0796963 (* 1 = 0.0796963 loss)
I0816 00:15:52.794239 14907 sgd_solver.cpp:136] Iteration 27700, lr = 1e-06, m = 0.9
I0816 00:16:12.554177 14907 solver.cpp:312] Iteration 27800 (5.06088 iter/s, 19.7594s/100 iter), loss = 0.0413635
I0816 00:16:12.554255 14907 solver.cpp:334]     Train net output #0: loss = 0.0413634 (* 1 = 0.0413634 loss)
I0816 00:16:12.554261 14907 sgd_solver.cpp:136] Iteration 27800, lr = 1e-06, m = 0.9
I0816 00:16:32.258548 14907 solver.cpp:312] Iteration 27900 (5.07516 iter/s, 19.7038s/100 iter), loss = 0.054623
I0816 00:16:32.258576 14907 solver.cpp:334]     Train net output #0: loss = 0.0546229 (* 1 = 0.0546229 loss)
I0816 00:16:32.258582 14907 sgd_solver.cpp:136] Iteration 27900, lr = 1e-06, m = 0.9
I0816 00:16:37.719362 14872 data_reader.cpp:288] Starting prefetch of epoch 21
I0816 00:16:51.734525 14907 solver.cpp:363] Sparsity after update:
I0816 00:16:51.739277 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0816 00:16:51.739307 14907 net.cpp:2192] conv1a_param_0(0) 
I0816 00:16:51.739320 14907 net.cpp:2192] conv1b_param_0(0) 
I0816 00:16:51.739323 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0816 00:16:51.739326 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0816 00:16:51.739329 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0816 00:16:51.739332 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0816 00:16:51.739336 14907 net.cpp:2192] ctx_final_param_0(0) 
I0816 00:16:51.739337 14907 net.cpp:2192] out3a_param_0(0) 
I0816 00:16:51.739341 14907 net.cpp:2192] out5a_param_0(0) 
I0816 00:16:51.739343 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0816 00:16:51.739361 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0816 00:16:51.739369 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0816 00:16:51.739377 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0816 00:16:51.739393 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0816 00:16:51.739399 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0816 00:16:51.739403 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0816 00:16:51.739406 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0816 00:16:51.739414 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0816 00:16:51.739430 14907 solver.cpp:509] Iteration 28000, Testing net (#0)
I0816 00:17:03.025506 14890 data_reader.cpp:288] Starting prefetch of epoch 4
I0816 00:17:04.050485 14907 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.951441
I0816 00:17:04.050508 14907 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999908
I0816 00:17:04.050514 14907 solver.cpp:594]     Test net output #2: loss = 0.165095 (* 1 = 0.165095 loss)
I0816 00:17:04.050544 14907 solver.cpp:264] [MultiGPU] Tests completed in 12.3108s
I0816 00:17:04.261812 14907 solver.cpp:312] Iteration 28000 (3.12477 iter/s, 32.0024s/100 iter), loss = 0.0744268
I0816 00:17:04.261842 14907 solver.cpp:334]     Train net output #0: loss = 0.0744268 (* 1 = 0.0744268 loss)
I0816 00:17:04.261848 14907 sgd_solver.cpp:136] Iteration 28000, lr = 1e-06, m = 0.9
I0816 00:17:22.592229 14872 data_reader.cpp:288] Starting prefetch of epoch 22
I0816 00:17:23.692824 14907 solver.cpp:312] Iteration 28100 (5.14655 iter/s, 19.4305s/100 iter), loss = 0.0730648
I0816 00:17:23.692849 14907 solver.cpp:334]     Train net output #0: loss = 0.0730647 (* 1 = 0.0730647 loss)
I0816 00:17:23.692855 14907 sgd_solver.cpp:136] Iteration 28100, lr = 1e-06, m = 0.9
I0816 00:17:43.084321 14907 solver.cpp:312] Iteration 28200 (5.15704 iter/s, 19.391s/100 iter), loss = 0.0709689
I0816 00:17:43.084344 14907 solver.cpp:334]     Train net output #0: loss = 0.0709688 (* 1 = 0.0709688 loss)
I0816 00:17:43.084350 14907 sgd_solver.cpp:136] Iteration 28200, lr = 1e-06, m = 0.9
I0816 00:18:02.720535 14907 solver.cpp:312] Iteration 28300 (5.09277 iter/s, 19.6357s/100 iter), loss = 0.0454328
I0816 00:18:02.720633 14907 solver.cpp:334]     Train net output #0: loss = 0.0454327 (* 1 = 0.0454327 loss)
I0816 00:18:02.720639 14907 sgd_solver.cpp:136] Iteration 28300, lr = 1e-06, m = 0.9
I0816 00:18:22.312999 14907 solver.cpp:312] Iteration 28400 (5.10414 iter/s, 19.5919s/100 iter), loss = 0.0788459
I0816 00:18:22.313025 14907 solver.cpp:334]     Train net output #0: loss = 0.0788458 (* 1 = 0.0788458 loss)
I0816 00:18:22.313030 14907 sgd_solver.cpp:136] Iteration 28400, lr = 1e-06, m = 0.9
I0816 00:18:27.019403 14920 data_reader.cpp:288] Starting prefetch of epoch 18
I0816 00:18:41.993448 14907 solver.cpp:312] Iteration 28500 (5.08132 iter/s, 19.6799s/100 iter), loss = 0.0710122
I0816 00:18:41.993530 14907 solver.cpp:334]     Train net output #0: loss = 0.0710121 (* 1 = 0.0710121 loss)
I0816 00:18:41.993537 14907 sgd_solver.cpp:136] Iteration 28500, lr = 1e-06, m = 0.9
I0816 00:19:01.797464 14907 solver.cpp:312] Iteration 28600 (5.04962 iter/s, 19.8035s/100 iter), loss = 0.0836511
I0816 00:19:01.797487 14907 solver.cpp:334]     Train net output #0: loss = 0.083651 (* 1 = 0.083651 loss)
I0816 00:19:01.797492 14907 sgd_solver.cpp:136] Iteration 28600, lr = 1e-06, m = 0.9
I0816 00:19:21.281172 14907 solver.cpp:312] Iteration 28700 (5.13263 iter/s, 19.4832s/100 iter), loss = 0.0810614
I0816 00:19:21.281227 14907 solver.cpp:334]     Train net output #0: loss = 0.0810613 (* 1 = 0.0810613 loss)
I0816 00:19:21.281234 14907 sgd_solver.cpp:136] Iteration 28700, lr = 1e-06, m = 0.9
I0816 00:19:32.105643 14872 data_reader.cpp:288] Starting prefetch of epoch 23
I0816 00:19:41.096441 14907 solver.cpp:312] Iteration 28800 (5.04675 iter/s, 19.8147s/100 iter), loss = 0.053383
I0816 00:19:41.096463 14907 solver.cpp:334]     Train net output #0: loss = 0.0533829 (* 1 = 0.0533829 loss)
I0816 00:19:41.096467 14907 sgd_solver.cpp:136] Iteration 28800, lr = 1e-06, m = 0.9
I0816 00:20:00.601825 14907 solver.cpp:312] Iteration 28900 (5.12693 iter/s, 19.5048s/100 iter), loss = 0.118967
I0816 00:20:00.601909 14907 solver.cpp:334]     Train net output #0: loss = 0.118967 (* 1 = 0.118967 loss)
I0816 00:20:00.601917 14907 sgd_solver.cpp:136] Iteration 28900, lr = 1e-06, m = 0.9
I0816 00:20:04.456065 14870 data_reader.cpp:288] Starting prefetch of epoch 17
I0816 00:20:19.794955 14907 solver.cpp:363] Sparsity after update:
I0816 00:20:19.810505 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0816 00:20:19.810631 14907 net.cpp:2192] conv1a_param_0(0) 
I0816 00:20:19.810678 14907 net.cpp:2192] conv1b_param_0(0) 
I0816 00:20:19.810691 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0816 00:20:19.810704 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0816 00:20:19.810716 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0816 00:20:19.810729 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0816 00:20:19.810740 14907 net.cpp:2192] ctx_final_param_0(0) 
I0816 00:20:19.810752 14907 net.cpp:2192] out3a_param_0(0) 
I0816 00:20:19.810765 14907 net.cpp:2192] out5a_param_0(0) 
I0816 00:20:19.810777 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0816 00:20:19.810788 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0816 00:20:19.810801 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0816 00:20:19.810812 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0816 00:20:19.810822 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0816 00:20:19.810833 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0816 00:20:19.810845 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0816 00:20:19.810858 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0816 00:20:19.810868 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0816 00:20:20.005827 14907 solver.cpp:312] Iteration 29000 (5.15372 iter/s, 19.4035s/100 iter), loss = 0.0755216
I0816 00:20:20.005853 14907 solver.cpp:334]     Train net output #0: loss = 0.0755215 (* 1 = 0.0755215 loss)
I0816 00:20:20.005859 14907 sgd_solver.cpp:136] Iteration 29000, lr = 1e-06, m = 0.9
I0816 00:20:39.737541 14907 solver.cpp:312] Iteration 29100 (5.06812 iter/s, 19.7312s/100 iter), loss = 0.0723439
I0816 00:20:39.737642 14907 solver.cpp:334]     Train net output #0: loss = 0.0723438 (* 1 = 0.0723438 loss)
I0816 00:20:39.737649 14907 sgd_solver.cpp:136] Iteration 29100, lr = 1e-06, m = 0.9
I0816 00:20:59.180016 14907 solver.cpp:312] Iteration 29200 (5.14352 iter/s, 19.4419s/100 iter), loss = 0.0525972
I0816 00:20:59.180042 14907 solver.cpp:334]     Train net output #0: loss = 0.0525971 (* 1 = 0.0525971 loss)
I0816 00:20:59.180048 14907 sgd_solver.cpp:136] Iteration 29200, lr = 1e-06, m = 0.9
I0816 00:21:09.261040 14918 data_reader.cpp:288] Starting prefetch of epoch 18
I0816 00:21:18.783612 14907 solver.cpp:312] Iteration 29300 (5.10124 iter/s, 19.6031s/100 iter), loss = 0.0531815
I0816 00:21:18.783677 14907 solver.cpp:334]     Train net output #0: loss = 0.0531814 (* 1 = 0.0531814 loss)
I0816 00:21:18.783685 14907 sgd_solver.cpp:136] Iteration 29300, lr = 1e-06, m = 0.9
I0816 00:21:38.293993 14907 solver.cpp:312] Iteration 29400 (5.12562 iter/s, 19.5098s/100 iter), loss = 0.0505311
I0816 00:21:38.294018 14907 solver.cpp:334]     Train net output #0: loss = 0.050531 (* 1 = 0.050531 loss)
I0816 00:21:38.294023 14907 sgd_solver.cpp:136] Iteration 29400, lr = 1e-06, m = 0.9
I0816 00:21:57.717576 14907 solver.cpp:312] Iteration 29500 (5.14852 iter/s, 19.423s/100 iter), loss = 0.0613772
I0816 00:21:57.717664 14907 solver.cpp:334]     Train net output #0: loss = 0.0613771 (* 1 = 0.0613771 loss)
I0816 00:21:57.717671 14907 sgd_solver.cpp:136] Iteration 29500, lr = 1e-06, m = 0.9
I0816 00:22:13.609127 14872 data_reader.cpp:288] Starting prefetch of epoch 24
I0816 00:22:17.401620 14907 solver.cpp:312] Iteration 29600 (5.0804 iter/s, 19.6835s/100 iter), loss = 0.0723489
I0816 00:22:17.401644 14907 solver.cpp:334]     Train net output #0: loss = 0.0723488 (* 1 = 0.0723488 loss)
I0816 00:22:17.401649 14907 sgd_solver.cpp:136] Iteration 29600, lr = 1e-06, m = 0.9
I0816 00:22:36.916424 14907 solver.cpp:312] Iteration 29700 (5.12446 iter/s, 19.5143s/100 iter), loss = 0.0579867
I0816 00:22:36.916479 14907 solver.cpp:334]     Train net output #0: loss = 0.0579866 (* 1 = 0.0579866 loss)
I0816 00:22:36.916486 14907 sgd_solver.cpp:136] Iteration 29700, lr = 1e-06, m = 0.9
I0816 00:22:45.967703 14915 data_reader.cpp:288] Starting prefetch of epoch 14
I0816 00:22:56.467890 14907 solver.cpp:312] Iteration 29800 (5.11485 iter/s, 19.5509s/100 iter), loss = 0.132043
I0816 00:22:56.467922 14907 solver.cpp:334]     Train net output #0: loss = 0.132043 (* 1 = 0.132043 loss)
I0816 00:22:56.467929 14907 sgd_solver.cpp:136] Iteration 29800, lr = 1e-06, m = 0.9
I0816 00:23:15.872185 14907 solver.cpp:312] Iteration 29900 (5.15364 iter/s, 19.4038s/100 iter), loss = 0.0700822
I0816 00:23:15.872231 14907 solver.cpp:334]     Train net output #0: loss = 0.0700821 (* 1 = 0.0700821 loss)
I0816 00:23:15.872238 14907 sgd_solver.cpp:136] Iteration 29900, lr = 1e-06, m = 0.9
I0816 00:23:35.135635 14907 solver.cpp:639] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/sparse/cityscapes5_jsegnet21v2_iter_30000.caffemodel
I0816 00:23:35.226835 14907 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/sparse/cityscapes5_jsegnet21v2_iter_30000.solverstate
I0816 00:23:35.239183 14907 solver.cpp:363] Sparsity after update:
I0816 00:23:35.252444 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0816 00:23:35.252508 14907 net.cpp:2192] conv1a_param_0(0) 
I0816 00:23:35.252530 14907 net.cpp:2192] conv1b_param_0(0) 
I0816 00:23:35.252545 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0816 00:23:35.252560 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0816 00:23:35.252574 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0816 00:23:35.252588 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0816 00:23:35.252602 14907 net.cpp:2192] ctx_final_param_0(0) 
I0816 00:23:35.252624 14907 net.cpp:2192] out3a_param_0(0) 
I0816 00:23:35.252638 14907 net.cpp:2192] out5a_param_0(0) 
I0816 00:23:35.252652 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0816 00:23:35.252666 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0816 00:23:35.252679 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0816 00:23:35.252692 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0816 00:23:35.252707 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0816 00:23:35.252719 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0816 00:23:35.252734 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0816 00:23:35.252748 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0816 00:23:35.252761 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0816 00:23:35.252789 14907 solver.cpp:509] Iteration 30000, Testing net (#0)
I0816 00:23:42.889487 14890 data_reader.cpp:288] Starting prefetch of epoch 5
I0816 00:23:47.226872 14907 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.952419
I0816 00:23:47.226999 14907 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999238
I0816 00:23:47.227010 14907 solver.cpp:594]     Test net output #2: loss = 0.202727 (* 1 = 0.202727 loss)
I0816 00:23:47.227036 14907 solver.cpp:264] [MultiGPU] Tests completed in 11.9739s
I0816 00:23:47.442519 14907 solver.cpp:312] Iteration 30000 (3.16762 iter/s, 31.5695s/100 iter), loss = 0.0717813
I0816 00:23:47.442550 14907 solver.cpp:334]     Train net output #0: loss = 0.0717812 (* 1 = 0.0717812 loss)
I0816 00:23:47.442556 14907 sgd_solver.cpp:136] Iteration 30000, lr = 1e-06, m = 0.9
I0816 00:24:07.017861 14907 solver.cpp:312] Iteration 30100 (5.10861 iter/s, 19.5748s/100 iter), loss = 0.0595228
I0816 00:24:07.017889 14907 solver.cpp:334]     Train net output #0: loss = 0.0595227 (* 1 = 0.0595227 loss)
I0816 00:24:07.017925 14907 sgd_solver.cpp:136] Iteration 30100, lr = 1e-06, m = 0.9
I0816 00:24:26.720572 14907 solver.cpp:312] Iteration 30200 (5.07558 iter/s, 19.7022s/100 iter), loss = 0.0602127
I0816 00:24:26.720620 14907 solver.cpp:334]     Train net output #0: loss = 0.0602126 (* 1 = 0.0602126 loss)
I0816 00:24:26.720625 14907 sgd_solver.cpp:136] Iteration 30200, lr = 1e-06, m = 0.9
I0816 00:24:34.985824 14918 data_reader.cpp:288] Starting prefetch of epoch 19
I0816 00:24:46.272783 14907 solver.cpp:312] Iteration 30300 (5.11465 iter/s, 19.5517s/100 iter), loss = 0.0801672
I0816 00:24:46.272805 14907 solver.cpp:334]     Train net output #0: loss = 0.0801672 (* 1 = 0.0801672 loss)
I0816 00:24:46.272809 14907 sgd_solver.cpp:136] Iteration 30300, lr = 1e-06, m = 0.9
I0816 00:25:05.830607 14907 solver.cpp:312] Iteration 30400 (5.11318 iter/s, 19.5573s/100 iter), loss = 0.07121
I0816 00:25:05.830653 14907 solver.cpp:334]     Train net output #0: loss = 0.07121 (* 1 = 0.07121 loss)
I0816 00:25:05.830660 14907 sgd_solver.cpp:136] Iteration 30400, lr = 1e-06, m = 0.9
I0816 00:25:07.434259 14920 data_reader.cpp:288] Starting prefetch of epoch 19
I0816 00:25:25.524360 14907 solver.cpp:312] Iteration 30500 (5.07789 iter/s, 19.6932s/100 iter), loss = 0.0745704
I0816 00:25:25.524391 14907 solver.cpp:334]     Train net output #0: loss = 0.0745703 (* 1 = 0.0745703 loss)
I0816 00:25:25.524397 14907 sgd_solver.cpp:136] Iteration 30500, lr = 1e-06, m = 0.9
I0816 00:25:44.957881 14907 solver.cpp:312] Iteration 30600 (5.14589 iter/s, 19.433s/100 iter), loss = 0.0562695
I0816 00:25:44.957931 14907 solver.cpp:334]     Train net output #0: loss = 0.0562694 (* 1 = 0.0562694 loss)
I0816 00:25:44.957938 14907 sgd_solver.cpp:136] Iteration 30600, lr = 1e-06, m = 0.9
I0816 00:26:04.498692 14907 solver.cpp:312] Iteration 30700 (5.11764 iter/s, 19.5403s/100 iter), loss = 0.112671
I0816 00:26:04.498718 14907 solver.cpp:334]     Train net output #0: loss = 0.112671 (* 1 = 0.112671 loss)
I0816 00:26:04.498723 14907 sgd_solver.cpp:136] Iteration 30700, lr = 1e-06, m = 0.9
I0816 00:26:11.892210 14920 data_reader.cpp:288] Starting prefetch of epoch 20
I0816 00:26:23.875689 14907 solver.cpp:312] Iteration 30800 (5.1609 iter/s, 19.3765s/100 iter), loss = 0.0672035
I0816 00:26:23.875762 14907 solver.cpp:334]     Train net output #0: loss = 0.0672034 (* 1 = 0.0672034 loss)
I0816 00:26:23.875767 14907 sgd_solver.cpp:136] Iteration 30800, lr = 1e-06, m = 0.9
I0816 00:26:43.227187 14907 solver.cpp:312] Iteration 30900 (5.1677 iter/s, 19.351s/100 iter), loss = 0.0649902
I0816 00:26:43.227210 14907 solver.cpp:334]     Train net output #0: loss = 0.0649901 (* 1 = 0.0649901 loss)
I0816 00:26:43.227216 14907 sgd_solver.cpp:136] Iteration 30900, lr = 1e-06, m = 0.9
I0816 00:27:02.686569 14907 solver.cpp:363] Sparsity after update:
I0816 00:27:02.706657 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0816 00:27:02.706727 14907 net.cpp:2192] conv1a_param_0(0) 
I0816 00:27:02.706743 14907 net.cpp:2192] conv1b_param_0(0) 
I0816 00:27:02.706753 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0816 00:27:02.706761 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0816 00:27:02.706770 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0816 00:27:02.706779 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0816 00:27:02.706789 14907 net.cpp:2192] ctx_final_param_0(0) 
I0816 00:27:02.706797 14907 net.cpp:2192] out3a_param_0(0) 
I0816 00:27:02.706806 14907 net.cpp:2192] out5a_param_0(0) 
I0816 00:27:02.706815 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0816 00:27:02.706825 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0816 00:27:02.706833 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0816 00:27:02.706842 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0816 00:27:02.706851 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0816 00:27:02.706861 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0816 00:27:02.706869 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0816 00:27:02.706878 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0816 00:27:02.706887 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0816 00:27:02.889801 14907 solver.cpp:312] Iteration 31000 (5.08593 iter/s, 19.6621s/100 iter), loss = 0.0610258
I0816 00:27:02.889829 14907 solver.cpp:334]     Train net output #0: loss = 0.0610258 (* 1 = 0.0610258 loss)
I0816 00:27:02.889837 14907 sgd_solver.cpp:136] Iteration 31000, lr = 1e-06, m = 0.9
I0816 00:27:16.303589 14920 data_reader.cpp:288] Starting prefetch of epoch 21
I0816 00:27:22.298532 14907 solver.cpp:312] Iteration 31100 (5.15246 iter/s, 19.4082s/100 iter), loss = 0.0546895
I0816 00:27:22.298553 14907 solver.cpp:334]     Train net output #0: loss = 0.0546894 (* 1 = 0.0546894 loss)
I0816 00:27:22.298559 14907 sgd_solver.cpp:136] Iteration 31100, lr = 1e-06, m = 0.9
I0816 00:27:42.172782 14907 solver.cpp:312] Iteration 31200 (5.03178 iter/s, 19.8737s/100 iter), loss = 0.0896979
I0816 00:27:42.172835 14907 solver.cpp:334]     Train net output #0: loss = 0.0896978 (* 1 = 0.0896978 loss)
I0816 00:27:42.172842 14907 sgd_solver.cpp:136] Iteration 31200, lr = 1e-06, m = 0.9
I0816 00:27:48.820549 14916 data_reader.cpp:288] Starting prefetch of epoch 17
I0816 00:28:01.617902 14907 solver.cpp:312] Iteration 31300 (5.14282 iter/s, 19.4446s/100 iter), loss = 0.0925379
I0816 00:28:01.617929 14907 solver.cpp:334]     Train net output #0: loss = 0.0925378 (* 1 = 0.0925378 loss)
I0816 00:28:01.617936 14907 sgd_solver.cpp:136] Iteration 31300, lr = 1e-06, m = 0.9
I0816 00:28:20.948731 14907 solver.cpp:312] Iteration 31400 (5.17323 iter/s, 19.3303s/100 iter), loss = 0.0583722
I0816 00:28:20.948786 14907 solver.cpp:334]     Train net output #0: loss = 0.0583722 (* 1 = 0.0583722 loss)
I0816 00:28:20.948792 14907 sgd_solver.cpp:136] Iteration 31400, lr = 1e-06, m = 0.9
I0816 00:28:44.077503 14907 solver.cpp:312] Iteration 31500 (4.32374 iter/s, 23.1281s/100 iter), loss = 0.0629486
I0816 00:28:44.077553 14907 solver.cpp:334]     Train net output #0: loss = 0.0629485 (* 1 = 0.0629485 loss)
I0816 00:28:44.077569 14907 sgd_solver.cpp:136] Iteration 31500, lr = 1e-06, m = 0.9
I0816 00:28:58.417264 14870 data_reader.cpp:288] Starting prefetch of epoch 18
I0816 00:29:05.642498 14907 solver.cpp:312] Iteration 31600 (4.63727 iter/s, 21.5644s/100 iter), loss = 0.113864
I0816 00:29:05.642525 14907 solver.cpp:334]     Train net output #0: loss = 0.113864 (* 1 = 0.113864 loss)
I0816 00:29:05.642531 14907 sgd_solver.cpp:136] Iteration 31600, lr = 1e-06, m = 0.9
I0816 00:29:25.678930 14907 solver.cpp:312] Iteration 31700 (4.99105 iter/s, 20.0359s/100 iter), loss = 0.0639655
I0816 00:29:25.678973 14907 solver.cpp:334]     Train net output #0: loss = 0.0639654 (* 1 = 0.0639654 loss)
I0816 00:29:25.678983 14907 sgd_solver.cpp:136] Iteration 31700, lr = 1e-06, m = 0.9
I0816 00:29:45.209295 14907 solver.cpp:312] Iteration 31800 (5.12037 iter/s, 19.5298s/100 iter), loss = 0.071197
I0816 00:29:45.209352 14907 solver.cpp:334]     Train net output #0: loss = 0.0711969 (* 1 = 0.0711969 loss)
I0816 00:29:45.209357 14907 sgd_solver.cpp:136] Iteration 31800, lr = 1e-06, m = 0.9
I0816 00:30:03.730185 14920 data_reader.cpp:288] Starting prefetch of epoch 22
I0816 00:30:04.740299 14907 solver.cpp:312] Iteration 31900 (5.1202 iter/s, 19.5305s/100 iter), loss = 0.0422064
I0816 00:30:04.740325 14907 solver.cpp:334]     Train net output #0: loss = 0.0422063 (* 1 = 0.0422063 loss)
I0816 00:30:04.740329 14907 sgd_solver.cpp:136] Iteration 31900, lr = 1e-06, m = 0.9
I0816 00:30:23.986390 14907 solver.cpp:312] Iteration 31999 (5.14404 iter/s, 19.2456s/99 iter), loss = 0.0607484
I0816 00:30:23.986444 14907 solver.cpp:334]     Train net output #0: loss = 0.0607483 (* 1 = 0.0607483 loss)
I0816 00:30:23.986449 14907 solver.cpp:363] Sparsity after update:
I0816 00:30:23.988098 14907 net.cpp:2183] Num Params(17), Sparsity (zero_weights/count): 
I0816 00:30:23.988106 14907 net.cpp:2192] conv1a_param_0(0) 
I0816 00:30:23.988109 14907 net.cpp:2192] conv1b_param_0(0) 
I0816 00:30:23.988111 14907 net.cpp:2192] ctx_conv1_param_0(0) 
I0816 00:30:23.988113 14907 net.cpp:2192] ctx_conv2_param_0(0) 
I0816 00:30:23.988116 14907 net.cpp:2192] ctx_conv3_param_0(0) 
I0816 00:30:23.988121 14907 net.cpp:2192] ctx_conv4_param_0(0) 
I0816 00:30:23.988123 14907 net.cpp:2192] ctx_final_param_0(0) 
I0816 00:30:23.988127 14907 net.cpp:2192] out3a_param_0(0) 
I0816 00:30:23.988137 14907 net.cpp:2192] out5a_param_0(0) 
I0816 00:30:23.988140 14907 net.cpp:2192] res2a_branch2a_param_0(0) 
I0816 00:30:23.988144 14907 net.cpp:2192] res2a_branch2b_param_0(0) 
I0816 00:30:23.988148 14907 net.cpp:2192] res3a_branch2a_param_0(0) 
I0816 00:30:23.988152 14907 net.cpp:2192] res3a_branch2b_param_0(0) 
I0816 00:30:23.988155 14907 net.cpp:2192] res4a_branch2a_param_0(0) 
I0816 00:30:23.988158 14907 net.cpp:2192] res4a_branch2b_param_0(0) 
I0816 00:30:23.988162 14907 net.cpp:2192] res5a_branch2a_param_0(0) 
I0816 00:30:23.988165 14907 net.cpp:2192] res5a_branch2b_param_0(0) 
I0816 00:30:23.988168 14907 net.cpp:2194] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0816 00:30:24.041965 14907 solver.cpp:639] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/sparse/cityscapes5_jsegnet21v2_iter_32000.caffemodel
I0816 00:30:24.111923 14907 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/sparse/cityscapes5_jsegnet21v2_iter_32000.solverstate
I0816 00:30:24.188591 14907 solver.cpp:486] Iteration 32000, loss = 0.0566738
I0816 00:30:24.188621 14907 solver.cpp:509] Iteration 32000, Testing net (#0)
I0816 00:30:36.700569 14888 data_reader.cpp:288] Starting prefetch of epoch 3
I0816 00:30:37.308720 14907 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.951346
I0816 00:30:37.308745 14907 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999917
I0816 00:30:37.308751 14907 solver.cpp:594]     Test net output #2: loss = 0.166435 (* 1 = 0.166435 loss)
I0816 00:30:37.360635 14815 parallel.cpp:71] Root Solver performance on device 0: 4.952 * 6 = 29.71 img/sec (32000 itr in 6462 sec)
I0816 00:30:37.360656 14815 parallel.cpp:76]      Solver performance on device 1: 4.952 * 6 = 29.71 img/sec (32000 itr in 6462 sec)
I0816 00:30:37.360662 14815 parallel.cpp:76]      Solver performance on device 2: 4.952 * 6 = 29.71 img/sec (32000 itr in 6462 sec)
I0816 00:30:37.360664 14815 parallel.cpp:79] Overall multi-GPU performance: 89.1337 img/sec
I0816 00:30:38.622130 14815 caffe.cpp:247] Optimization Done in 1h 48m 2s
