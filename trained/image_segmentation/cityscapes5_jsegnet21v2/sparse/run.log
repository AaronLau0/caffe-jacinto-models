I0731 21:37:16.223815 20312 caffe.cpp:608] This is NVCaffe 0.16.3 started at Mon Jul 31 21:37:15 2017
I0731 21:37:16.224097 20312 caffe.cpp:611] CuDNN version: 6021
I0731 21:37:16.224100 20312 caffe.cpp:612] CuBLAS version: 8000
I0731 21:37:16.224103 20312 caffe.cpp:613] CUDA version: 8000
I0731 21:37:16.224105 20312 caffe.cpp:614] CUDA driver version: 8000
I0731 21:37:16.516619 20312 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0731 21:37:16.517204 20312 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I0731 21:37:16.517742 20312 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[1]: total=8508145664 free=8379236352
I0731 21:37:16.518259 20312 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[2]: total=8508145664 free=8379236352
I0731 21:37:16.518266 20312 caffe.cpp:208] Using GPUs 0, 1, 2
I0731 21:37:16.518587 20312 caffe.cpp:213] GPU 0: GeForce GTX 1080
I0731 21:37:16.518909 20312 caffe.cpp:213] GPU 1: GeForce GTX 1080
I0731 21:37:16.519229 20312 caffe.cpp:213] GPU 2: GeForce GTX 1080
I0731 21:37:16.519265 20312 solver.cpp:42] Solver data type: FLOAT
I0731 21:37:16.519327 20312 solver.cpp:45] Initializing solver from parameters: 
train_net: "training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/sparse/train.prototxt"
test_net: "training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/sparse/test.prototxt"
test_iter: 125
test_interval: 2000
base_lr: 1e-05
display: 100
max_iter: 32000
lr_policy: "multistep"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 1e-05
snapshot: 10000
snapshot_prefix: "training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/sparse/cityscapes5_jsegnet21v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
regularization_type: "L1"
test_initialization: false
stepvalue: 24000
iter_size: 1
type: "Adam"
display_sparsity: 1000
sparse_mode: SPARSE_UPDATE
sparsity_target: 0.8
sparsity_step_factor: 0.01
sparsity_step_iter: 1000
sparsity_start_iter: 0
sparsity_start_factor: 0.8
I0731 21:37:16.536775 20312 solver.cpp:77] Creating training net from train_net file: training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/sparse/train.prototxt
I0731 21:37:16.537390 20312 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0731 21:37:16.537398 20312 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
W0731 21:37:16.537431 20312 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0731 21:37:16.537672 20312 net.cpp:72] Initializing net from parameters: 
name: "jsegnet21v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/train-image-lmdb"
    label_list_path: "data/train-label-lmdb"
    batch_size: 6
    shuffle: false
    threads: 1
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
I0731 21:37:16.537825 20312 net.cpp:104] Using FLOAT as default forward math type
I0731 21:37:16.537832 20312 net.cpp:110] Using FLOAT as default backward math type
I0731 21:37:16.537834 20312 layer_factory.hpp:136] Creating layer 'data' of type 'ImageLabelData'
I0731 21:37:16.537837 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:16.537852 20312 net.cpp:184] Created Layer data (0)
I0731 21:37:16.537856 20312 net.cpp:530] data -> data
I0731 21:37:16.546272 20312 net.cpp:530] data -> label
I0731 21:37:16.554443 20312 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0731 21:37:16.554462 20312 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 21:37:16.572679 20346 db_lmdb.cpp:35] Opened lmdb data/train-image-lmdb
I0731 21:37:16.575791 20312 data_layer.cpp:184] [0] ReshapePrefetch 6, 3, 640, 640
I0731 21:37:16.575894 20312 data_layer.cpp:208] [0] Output data size: 6, 3, 640, 640
I0731 21:37:16.575903 20312 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 21:37:16.575968 20312 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0731 21:37:16.575979 20312 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 21:37:16.576671 20363 data_layer.cpp:97] [0] Parser threads: 1
I0731 21:37:16.576678 20363 data_layer.cpp:99] [0] Transformer threads: 1
I0731 21:37:16.600420 20364 db_lmdb.cpp:35] Opened lmdb data/train-label-lmdb
I0731 21:37:16.602892 20312 data_layer.cpp:184] [0] ReshapePrefetch 6, 1, 640, 640
I0731 21:37:16.602931 20312 data_layer.cpp:208] [0] Output data size: 6, 1, 640, 640
I0731 21:37:16.602936 20312 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 21:37:16.602984 20312 net.cpp:245] Setting up data
I0731 21:37:16.602995 20312 net.cpp:252] TRAIN Top shape for layer 0 'data' 6 3 640 640 (7372800)
I0731 21:37:16.603003 20312 net.cpp:252] TRAIN Top shape for layer 0 'data' 6 1 640 640 (2457600)
I0731 21:37:16.603010 20312 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0731 21:37:16.603016 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:16.603036 20312 net.cpp:184] Created Layer data/bias (1)
I0731 21:37:16.603044 20312 net.cpp:561] data/bias <- data
I0731 21:37:16.603055 20312 net.cpp:530] data/bias -> data/bias
I0731 21:37:16.605535 20369 data_layer.cpp:97] [0] Parser threads: 1
I0731 21:37:16.605566 20369 data_layer.cpp:99] [0] Transformer threads: 1
I0731 21:37:16.609308 20312 net.cpp:245] Setting up data/bias
I0731 21:37:16.609344 20312 net.cpp:252] TRAIN Top shape for layer 1 'data/bias' 6 3 640 640 (7372800)
I0731 21:37:16.609365 20312 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0731 21:37:16.609419 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:16.609463 20312 net.cpp:184] Created Layer conv1a (2)
I0731 21:37:16.609470 20312 net.cpp:561] conv1a <- data/bias
I0731 21:37:16.609477 20312 net.cpp:530] conv1a -> conv1a
I0731 21:37:17.346477 20312 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 7.9G, req 0G)
I0731 21:37:17.346510 20312 net.cpp:245] Setting up conv1a
I0731 21:37:17.346520 20312 net.cpp:252] TRAIN Top shape for layer 2 'conv1a' 6 32 320 320 (19660800)
I0731 21:37:17.346536 20312 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0731 21:37:17.346544 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.346565 20312 net.cpp:184] Created Layer conv1a/bn (3)
I0731 21:37:17.346571 20312 net.cpp:561] conv1a/bn <- conv1a
I0731 21:37:17.346578 20312 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0731 21:37:17.347837 20312 net.cpp:245] Setting up conv1a/bn
I0731 21:37:17.347854 20312 net.cpp:252] TRAIN Top shape for layer 3 'conv1a/bn' 6 32 320 320 (19660800)
I0731 21:37:17.347868 20312 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0731 21:37:17.347874 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.347892 20312 net.cpp:184] Created Layer conv1a/relu (4)
I0731 21:37:17.347898 20312 net.cpp:561] conv1a/relu <- conv1a
I0731 21:37:17.347903 20312 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0731 21:37:17.347923 20312 net.cpp:245] Setting up conv1a/relu
I0731 21:37:17.347929 20312 net.cpp:252] TRAIN Top shape for layer 4 'conv1a/relu' 6 32 320 320 (19660800)
I0731 21:37:17.347934 20312 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0731 21:37:17.347939 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.347961 20312 net.cpp:184] Created Layer conv1b (5)
I0731 21:37:17.347966 20312 net.cpp:561] conv1b <- conv1a
I0731 21:37:17.347971 20312 net.cpp:530] conv1b -> conv1b
I0731 21:37:17.401226 20312 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.73G, req 0G)
I0731 21:37:17.401298 20312 net.cpp:245] Setting up conv1b
I0731 21:37:17.401324 20312 net.cpp:252] TRAIN Top shape for layer 5 'conv1b' 6 32 320 320 (19660800)
I0731 21:37:17.401356 20312 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0731 21:37:17.401376 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.401406 20312 net.cpp:184] Created Layer conv1b/bn (6)
I0731 21:37:17.401420 20312 net.cpp:561] conv1b/bn <- conv1b
I0731 21:37:17.401435 20312 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0731 21:37:17.404876 20312 net.cpp:245] Setting up conv1b/bn
I0731 21:37:17.404906 20312 net.cpp:252] TRAIN Top shape for layer 6 'conv1b/bn' 6 32 320 320 (19660800)
I0731 21:37:17.404927 20312 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0731 21:37:17.404937 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.404948 20312 net.cpp:184] Created Layer conv1b/relu (7)
I0731 21:37:17.404956 20312 net.cpp:561] conv1b/relu <- conv1b
I0731 21:37:17.404969 20312 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0731 21:37:17.404983 20312 net.cpp:245] Setting up conv1b/relu
I0731 21:37:17.404992 20312 net.cpp:252] TRAIN Top shape for layer 7 'conv1b/relu' 6 32 320 320 (19660800)
I0731 21:37:17.405001 20312 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0731 21:37:17.405014 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.405059 20312 net.cpp:184] Created Layer pool1 (8)
I0731 21:37:17.405076 20312 net.cpp:561] pool1 <- conv1b
I0731 21:37:17.405089 20312 net.cpp:530] pool1 -> pool1
I0731 21:37:17.405946 20312 net.cpp:245] Setting up pool1
I0731 21:37:17.405977 20312 net.cpp:252] TRAIN Top shape for layer 8 'pool1' 6 32 160 160 (4915200)
I0731 21:37:17.405992 20312 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0731 21:37:17.406008 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.406040 20312 net.cpp:184] Created Layer res2a_branch2a (9)
I0731 21:37:17.406057 20312 net.cpp:561] res2a_branch2a <- pool1
I0731 21:37:17.406071 20312 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0731 21:37:17.454162 20312 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.61G, req 0G)
I0731 21:37:17.454186 20312 net.cpp:245] Setting up res2a_branch2a
I0731 21:37:17.454191 20312 net.cpp:252] TRAIN Top shape for layer 9 'res2a_branch2a' 6 64 160 160 (9830400)
I0731 21:37:17.454202 20312 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0731 21:37:17.454206 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.454215 20312 net.cpp:184] Created Layer res2a_branch2a/bn (10)
I0731 21:37:17.454217 20312 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0731 21:37:17.454229 20312 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0731 21:37:17.455521 20312 net.cpp:245] Setting up res2a_branch2a/bn
I0731 21:37:17.455531 20312 net.cpp:252] TRAIN Top shape for layer 10 'res2a_branch2a/bn' 6 64 160 160 (9830400)
I0731 21:37:17.455538 20312 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0731 21:37:17.455540 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.455544 20312 net.cpp:184] Created Layer res2a_branch2a/relu (11)
I0731 21:37:17.455548 20312 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0731 21:37:17.455549 20312 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0731 21:37:17.455554 20312 net.cpp:245] Setting up res2a_branch2a/relu
I0731 21:37:17.455556 20312 net.cpp:252] TRAIN Top shape for layer 11 'res2a_branch2a/relu' 6 64 160 160 (9830400)
I0731 21:37:17.455559 20312 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0731 21:37:17.455560 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.455567 20312 net.cpp:184] Created Layer res2a_branch2b (12)
I0731 21:37:17.455570 20312 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0731 21:37:17.455572 20312 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0731 21:37:17.478261 20312 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.52G, req 0G)
I0731 21:37:17.478274 20312 net.cpp:245] Setting up res2a_branch2b
I0731 21:37:17.478279 20312 net.cpp:252] TRAIN Top shape for layer 12 'res2a_branch2b' 6 64 160 160 (9830400)
I0731 21:37:17.478284 20312 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0731 21:37:17.478287 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.478292 20312 net.cpp:184] Created Layer res2a_branch2b/bn (13)
I0731 21:37:17.478296 20312 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0731 21:37:17.478299 20312 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0731 21:37:17.479029 20312 net.cpp:245] Setting up res2a_branch2b/bn
I0731 21:37:17.479038 20312 net.cpp:252] TRAIN Top shape for layer 13 'res2a_branch2b/bn' 6 64 160 160 (9830400)
I0731 21:37:17.479044 20312 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0731 21:37:17.479048 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.479050 20312 net.cpp:184] Created Layer res2a_branch2b/relu (14)
I0731 21:37:17.479053 20312 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0731 21:37:17.479055 20312 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0731 21:37:17.479068 20312 net.cpp:245] Setting up res2a_branch2b/relu
I0731 21:37:17.479071 20312 net.cpp:252] TRAIN Top shape for layer 14 'res2a_branch2b/relu' 6 64 160 160 (9830400)
I0731 21:37:17.479074 20312 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0731 21:37:17.479077 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.479081 20312 net.cpp:184] Created Layer pool2 (15)
I0731 21:37:17.479084 20312 net.cpp:561] pool2 <- res2a_branch2b
I0731 21:37:17.479086 20312 net.cpp:530] pool2 -> pool2
I0731 21:37:17.479154 20312 net.cpp:245] Setting up pool2
I0731 21:37:17.479163 20312 net.cpp:252] TRAIN Top shape for layer 15 'pool2' 6 64 80 80 (2457600)
I0731 21:37:17.479167 20312 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0731 21:37:17.479171 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.479179 20312 net.cpp:184] Created Layer res3a_branch2a (16)
I0731 21:37:17.479183 20312 net.cpp:561] res3a_branch2a <- pool2
I0731 21:37:17.479187 20312 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0731 21:37:17.501444 20312 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 7.46G, req 0G)
I0731 21:37:17.501456 20312 net.cpp:245] Setting up res3a_branch2a
I0731 21:37:17.501461 20312 net.cpp:252] TRAIN Top shape for layer 16 'res3a_branch2a' 6 128 80 80 (4915200)
I0731 21:37:17.501466 20312 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0731 21:37:17.501467 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.501472 20312 net.cpp:184] Created Layer res3a_branch2a/bn (17)
I0731 21:37:17.501474 20312 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0731 21:37:17.501477 20312 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0731 21:37:17.502207 20312 net.cpp:245] Setting up res3a_branch2a/bn
I0731 21:37:17.502216 20312 net.cpp:252] TRAIN Top shape for layer 17 'res3a_branch2a/bn' 6 128 80 80 (4915200)
I0731 21:37:17.502223 20312 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0731 21:37:17.502226 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.502229 20312 net.cpp:184] Created Layer res3a_branch2a/relu (18)
I0731 21:37:17.502231 20312 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0731 21:37:17.502234 20312 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0731 21:37:17.502238 20312 net.cpp:245] Setting up res3a_branch2a/relu
I0731 21:37:17.502240 20312 net.cpp:252] TRAIN Top shape for layer 18 'res3a_branch2a/relu' 6 128 80 80 (4915200)
I0731 21:37:17.502243 20312 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0731 21:37:17.502244 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.502250 20312 net.cpp:184] Created Layer res3a_branch2b (19)
I0731 21:37:17.502252 20312 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0731 21:37:17.502255 20312 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0731 21:37:17.516034 20312 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.42G, req 0G)
I0731 21:37:17.516047 20312 net.cpp:245] Setting up res3a_branch2b
I0731 21:37:17.516050 20312 net.cpp:252] TRAIN Top shape for layer 19 'res3a_branch2b' 6 128 80 80 (4915200)
I0731 21:37:17.516055 20312 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0731 21:37:17.516057 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.516062 20312 net.cpp:184] Created Layer res3a_branch2b/bn (20)
I0731 21:37:17.516064 20312 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0731 21:37:17.516067 20312 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0731 21:37:17.516767 20312 net.cpp:245] Setting up res3a_branch2b/bn
I0731 21:37:17.516783 20312 net.cpp:252] TRAIN Top shape for layer 20 'res3a_branch2b/bn' 6 128 80 80 (4915200)
I0731 21:37:17.516789 20312 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0731 21:37:17.516793 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.516795 20312 net.cpp:184] Created Layer res3a_branch2b/relu (21)
I0731 21:37:17.516798 20312 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0731 21:37:17.516800 20312 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0731 21:37:17.516803 20312 net.cpp:245] Setting up res3a_branch2b/relu
I0731 21:37:17.516806 20312 net.cpp:252] TRAIN Top shape for layer 21 'res3a_branch2b/relu' 6 128 80 80 (4915200)
I0731 21:37:17.516808 20312 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0731 21:37:17.516810 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.516819 20312 net.cpp:184] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (22)
I0731 21:37:17.516822 20312 net.cpp:561] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0731 21:37:17.516824 20312 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0731 21:37:17.516829 20312 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0731 21:37:17.516880 20312 net.cpp:245] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0731 21:37:17.516886 20312 net.cpp:252] TRAIN Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 6 128 80 80 (4915200)
I0731 21:37:17.516891 20312 net.cpp:252] TRAIN Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 6 128 80 80 (4915200)
I0731 21:37:17.516896 20312 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0731 21:37:17.516901 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.516906 20312 net.cpp:184] Created Layer pool3 (23)
I0731 21:37:17.516911 20312 net.cpp:561] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0731 21:37:17.516914 20312 net.cpp:530] pool3 -> pool3
I0731 21:37:17.516993 20312 net.cpp:245] Setting up pool3
I0731 21:37:17.516999 20312 net.cpp:252] TRAIN Top shape for layer 23 'pool3' 6 128 40 40 (1228800)
I0731 21:37:17.517004 20312 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0731 21:37:17.517007 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.517015 20312 net.cpp:184] Created Layer res4a_branch2a (24)
I0731 21:37:17.517019 20312 net.cpp:561] res4a_branch2a <- pool3
I0731 21:37:17.517024 20312 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0731 21:37:17.546128 20312 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.38G, req 0G)
I0731 21:37:17.546147 20312 net.cpp:245] Setting up res4a_branch2a
I0731 21:37:17.546154 20312 net.cpp:252] TRAIN Top shape for layer 24 'res4a_branch2a' 6 256 40 40 (2457600)
I0731 21:37:17.546161 20312 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0731 21:37:17.546165 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.546180 20312 net.cpp:184] Created Layer res4a_branch2a/bn (25)
I0731 21:37:17.546183 20312 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0731 21:37:17.546186 20312 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0731 21:37:17.546965 20312 net.cpp:245] Setting up res4a_branch2a/bn
I0731 21:37:17.546973 20312 net.cpp:252] TRAIN Top shape for layer 25 'res4a_branch2a/bn' 6 256 40 40 (2457600)
I0731 21:37:17.546980 20312 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0731 21:37:17.546983 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.546986 20312 net.cpp:184] Created Layer res4a_branch2a/relu (26)
I0731 21:37:17.546998 20312 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0731 21:37:17.547000 20312 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0731 21:37:17.547004 20312 net.cpp:245] Setting up res4a_branch2a/relu
I0731 21:37:17.547011 20312 net.cpp:252] TRAIN Top shape for layer 26 'res4a_branch2a/relu' 6 256 40 40 (2457600)
I0731 21:37:17.547014 20312 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0731 21:37:17.547018 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.547024 20312 net.cpp:184] Created Layer res4a_branch2b (27)
I0731 21:37:17.547026 20312 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0731 21:37:17.547029 20312 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0731 21:37:17.557807 20312 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.36G, req 0G)
I0731 21:37:17.557821 20312 net.cpp:245] Setting up res4a_branch2b
I0731 21:37:17.557826 20312 net.cpp:252] TRAIN Top shape for layer 27 'res4a_branch2b' 6 256 40 40 (2457600)
I0731 21:37:17.557829 20312 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0731 21:37:17.557832 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.557837 20312 net.cpp:184] Created Layer res4a_branch2b/bn (28)
I0731 21:37:17.557839 20312 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0731 21:37:17.557842 20312 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0731 21:37:17.558540 20312 net.cpp:245] Setting up res4a_branch2b/bn
I0731 21:37:17.558564 20312 net.cpp:252] TRAIN Top shape for layer 28 'res4a_branch2b/bn' 6 256 40 40 (2457600)
I0731 21:37:17.558583 20312 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0731 21:37:17.558594 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.558605 20312 net.cpp:184] Created Layer res4a_branch2b/relu (29)
I0731 21:37:17.558615 20312 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0731 21:37:17.558624 20312 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0731 21:37:17.558634 20312 net.cpp:245] Setting up res4a_branch2b/relu
I0731 21:37:17.558645 20312 net.cpp:252] TRAIN Top shape for layer 29 'res4a_branch2b/relu' 6 256 40 40 (2457600)
I0731 21:37:17.558652 20312 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0731 21:37:17.558661 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.558673 20312 net.cpp:184] Created Layer pool4 (30)
I0731 21:37:17.558681 20312 net.cpp:561] pool4 <- res4a_branch2b
I0731 21:37:17.558691 20312 net.cpp:530] pool4 -> pool4
I0731 21:37:17.558801 20312 net.cpp:245] Setting up pool4
I0731 21:37:17.558815 20312 net.cpp:252] TRAIN Top shape for layer 30 'pool4' 6 256 40 40 (2457600)
I0731 21:37:17.558825 20312 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0731 21:37:17.558835 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.558857 20312 net.cpp:184] Created Layer res5a_branch2a (31)
I0731 21:37:17.558867 20312 net.cpp:561] res5a_branch2a <- pool4
I0731 21:37:17.558876 20312 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0731 21:37:17.586984 20312 net.cpp:245] Setting up res5a_branch2a
I0731 21:37:17.587003 20312 net.cpp:252] TRAIN Top shape for layer 31 'res5a_branch2a' 6 512 40 40 (4915200)
I0731 21:37:17.587010 20312 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0731 21:37:17.587014 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.587023 20312 net.cpp:184] Created Layer res5a_branch2a/bn (32)
I0731 21:37:17.587025 20312 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0731 21:37:17.587028 20312 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0731 21:37:17.587679 20312 net.cpp:245] Setting up res5a_branch2a/bn
I0731 21:37:17.587687 20312 net.cpp:252] TRAIN Top shape for layer 32 'res5a_branch2a/bn' 6 512 40 40 (4915200)
I0731 21:37:17.587692 20312 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0731 21:37:17.587695 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.587698 20312 net.cpp:184] Created Layer res5a_branch2a/relu (33)
I0731 21:37:17.587700 20312 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0731 21:37:17.587702 20312 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0731 21:37:17.587707 20312 net.cpp:245] Setting up res5a_branch2a/relu
I0731 21:37:17.587709 20312 net.cpp:252] TRAIN Top shape for layer 33 'res5a_branch2a/relu' 6 512 40 40 (4915200)
I0731 21:37:17.587712 20312 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0731 21:37:17.587713 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.587719 20312 net.cpp:184] Created Layer res5a_branch2b (34)
I0731 21:37:17.587723 20312 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0731 21:37:17.587724 20312 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0731 21:37:17.601045 20312 net.cpp:245] Setting up res5a_branch2b
I0731 21:37:17.601068 20312 net.cpp:252] TRAIN Top shape for layer 34 'res5a_branch2b' 6 512 40 40 (4915200)
I0731 21:37:17.601079 20312 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0731 21:37:17.601083 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.601091 20312 net.cpp:184] Created Layer res5a_branch2b/bn (35)
I0731 21:37:17.601094 20312 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0731 21:37:17.601097 20312 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0731 21:37:17.601738 20312 net.cpp:245] Setting up res5a_branch2b/bn
I0731 21:37:17.601745 20312 net.cpp:252] TRAIN Top shape for layer 35 'res5a_branch2b/bn' 6 512 40 40 (4915200)
I0731 21:37:17.601752 20312 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0731 21:37:17.601753 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.601757 20312 net.cpp:184] Created Layer res5a_branch2b/relu (36)
I0731 21:37:17.601759 20312 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0731 21:37:17.601761 20312 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0731 21:37:17.601765 20312 net.cpp:245] Setting up res5a_branch2b/relu
I0731 21:37:17.601768 20312 net.cpp:252] TRAIN Top shape for layer 36 'res5a_branch2b/relu' 6 512 40 40 (4915200)
I0731 21:37:17.601769 20312 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I0731 21:37:17.601773 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.601783 20312 net.cpp:184] Created Layer out5a (37)
I0731 21:37:17.601785 20312 net.cpp:561] out5a <- res5a_branch2b
I0731 21:37:17.601788 20312 net.cpp:530] out5a -> out5a
I0731 21:37:17.606081 20312 net.cpp:245] Setting up out5a
I0731 21:37:17.606091 20312 net.cpp:252] TRAIN Top shape for layer 37 'out5a' 6 64 40 40 (614400)
I0731 21:37:17.606096 20312 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I0731 21:37:17.606098 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.606107 20312 net.cpp:184] Created Layer out5a/bn (38)
I0731 21:37:17.606111 20312 net.cpp:561] out5a/bn <- out5a
I0731 21:37:17.606113 20312 net.cpp:513] out5a/bn -> out5a (in-place)
I0731 21:37:17.606830 20312 net.cpp:245] Setting up out5a/bn
I0731 21:37:17.606838 20312 net.cpp:252] TRAIN Top shape for layer 38 'out5a/bn' 6 64 40 40 (614400)
I0731 21:37:17.606844 20312 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I0731 21:37:17.606848 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.606858 20312 net.cpp:184] Created Layer out5a/relu (39)
I0731 21:37:17.606861 20312 net.cpp:561] out5a/relu <- out5a
I0731 21:37:17.606863 20312 net.cpp:513] out5a/relu -> out5a (in-place)
I0731 21:37:17.606868 20312 net.cpp:245] Setting up out5a/relu
I0731 21:37:17.606870 20312 net.cpp:252] TRAIN Top shape for layer 39 'out5a/relu' 6 64 40 40 (614400)
I0731 21:37:17.606873 20312 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I0731 21:37:17.606874 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.606885 20312 net.cpp:184] Created Layer out5a_up2 (40)
I0731 21:37:17.606889 20312 net.cpp:561] out5a_up2 <- out5a
I0731 21:37:17.606890 20312 net.cpp:530] out5a_up2 -> out5a_up2
I0731 21:37:17.607254 20312 net.cpp:245] Setting up out5a_up2
I0731 21:37:17.607262 20312 net.cpp:252] TRAIN Top shape for layer 40 'out5a_up2' 6 64 80 80 (2457600)
I0731 21:37:17.607267 20312 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I0731 21:37:17.607271 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.607290 20312 net.cpp:184] Created Layer out3a (41)
I0731 21:37:17.607295 20312 net.cpp:561] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0731 21:37:17.607301 20312 net.cpp:530] out3a -> out3a
I0731 21:37:17.621115 20312 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 7.3G, req 0G)
I0731 21:37:17.621131 20312 net.cpp:245] Setting up out3a
I0731 21:37:17.621139 20312 net.cpp:252] TRAIN Top shape for layer 41 'out3a' 6 64 80 80 (2457600)
I0731 21:37:17.621145 20312 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I0731 21:37:17.621150 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.621156 20312 net.cpp:184] Created Layer out3a/bn (42)
I0731 21:37:17.621160 20312 net.cpp:561] out3a/bn <- out3a
I0731 21:37:17.621165 20312 net.cpp:513] out3a/bn -> out3a (in-place)
I0731 21:37:17.621956 20312 net.cpp:245] Setting up out3a/bn
I0731 21:37:17.621965 20312 net.cpp:252] TRAIN Top shape for layer 42 'out3a/bn' 6 64 80 80 (2457600)
I0731 21:37:17.621971 20312 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I0731 21:37:17.621974 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.621978 20312 net.cpp:184] Created Layer out3a/relu (43)
I0731 21:37:17.621980 20312 net.cpp:561] out3a/relu <- out3a
I0731 21:37:17.621982 20312 net.cpp:513] out3a/relu -> out3a (in-place)
I0731 21:37:17.621986 20312 net.cpp:245] Setting up out3a/relu
I0731 21:37:17.621989 20312 net.cpp:252] TRAIN Top shape for layer 43 'out3a/relu' 6 64 80 80 (2457600)
I0731 21:37:17.621990 20312 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I0731 21:37:17.621992 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.622429 20312 net.cpp:184] Created Layer out3_out5_combined (44)
I0731 21:37:17.622436 20312 net.cpp:561] out3_out5_combined <- out5a_up2
I0731 21:37:17.622438 20312 net.cpp:561] out3_out5_combined <- out3a
I0731 21:37:17.622440 20312 net.cpp:530] out3_out5_combined -> out3_out5_combined
I0731 21:37:17.623430 20312 net.cpp:245] Setting up out3_out5_combined
I0731 21:37:17.623438 20312 net.cpp:252] TRAIN Top shape for layer 44 'out3_out5_combined' 6 64 80 80 (2457600)
I0731 21:37:17.623441 20312 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I0731 21:37:17.623445 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.623450 20312 net.cpp:184] Created Layer ctx_conv1 (45)
I0731 21:37:17.623452 20312 net.cpp:561] ctx_conv1 <- out3_out5_combined
I0731 21:37:17.623456 20312 net.cpp:530] ctx_conv1 -> ctx_conv1
I0731 21:37:17.639556 20312 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 7.25G, req 0G)
I0731 21:37:17.639576 20312 net.cpp:245] Setting up ctx_conv1
I0731 21:37:17.639581 20312 net.cpp:252] TRAIN Top shape for layer 45 'ctx_conv1' 6 64 80 80 (2457600)
I0731 21:37:17.639585 20312 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I0731 21:37:17.639588 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.639598 20312 net.cpp:184] Created Layer ctx_conv1/bn (46)
I0731 21:37:17.639601 20312 net.cpp:561] ctx_conv1/bn <- ctx_conv1
I0731 21:37:17.639603 20312 net.cpp:513] ctx_conv1/bn -> ctx_conv1 (in-place)
I0731 21:37:17.640291 20312 net.cpp:245] Setting up ctx_conv1/bn
I0731 21:37:17.640298 20312 net.cpp:252] TRAIN Top shape for layer 46 'ctx_conv1/bn' 6 64 80 80 (2457600)
I0731 21:37:17.640305 20312 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I0731 21:37:17.640306 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.640310 20312 net.cpp:184] Created Layer ctx_conv1/relu (47)
I0731 21:37:17.640312 20312 net.cpp:561] ctx_conv1/relu <- ctx_conv1
I0731 21:37:17.640314 20312 net.cpp:513] ctx_conv1/relu -> ctx_conv1 (in-place)
I0731 21:37:17.640317 20312 net.cpp:245] Setting up ctx_conv1/relu
I0731 21:37:17.640321 20312 net.cpp:252] TRAIN Top shape for layer 47 'ctx_conv1/relu' 6 64 80 80 (2457600)
I0731 21:37:17.640322 20312 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I0731 21:37:17.640324 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.640329 20312 net.cpp:184] Created Layer ctx_conv2 (48)
I0731 21:37:17.640332 20312 net.cpp:561] ctx_conv2 <- ctx_conv1
I0731 21:37:17.640334 20312 net.cpp:530] ctx_conv2 -> ctx_conv2
I0731 21:37:17.641469 20312 net.cpp:245] Setting up ctx_conv2
I0731 21:37:17.641480 20312 net.cpp:252] TRAIN Top shape for layer 48 'ctx_conv2' 6 64 80 80 (2457600)
I0731 21:37:17.641485 20312 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I0731 21:37:17.641489 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.641492 20312 net.cpp:184] Created Layer ctx_conv2/bn (49)
I0731 21:37:17.641495 20312 net.cpp:561] ctx_conv2/bn <- ctx_conv2
I0731 21:37:17.641499 20312 net.cpp:513] ctx_conv2/bn -> ctx_conv2 (in-place)
I0731 21:37:17.642170 20312 net.cpp:245] Setting up ctx_conv2/bn
I0731 21:37:17.642177 20312 net.cpp:252] TRAIN Top shape for layer 49 'ctx_conv2/bn' 6 64 80 80 (2457600)
I0731 21:37:17.642184 20312 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I0731 21:37:17.642185 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.642189 20312 net.cpp:184] Created Layer ctx_conv2/relu (50)
I0731 21:37:17.642190 20312 net.cpp:561] ctx_conv2/relu <- ctx_conv2
I0731 21:37:17.642194 20312 net.cpp:513] ctx_conv2/relu -> ctx_conv2 (in-place)
I0731 21:37:17.642196 20312 net.cpp:245] Setting up ctx_conv2/relu
I0731 21:37:17.642199 20312 net.cpp:252] TRAIN Top shape for layer 50 'ctx_conv2/relu' 6 64 80 80 (2457600)
I0731 21:37:17.642200 20312 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I0731 21:37:17.642204 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.642212 20312 net.cpp:184] Created Layer ctx_conv3 (51)
I0731 21:37:17.642215 20312 net.cpp:561] ctx_conv3 <- ctx_conv2
I0731 21:37:17.642218 20312 net.cpp:530] ctx_conv3 -> ctx_conv3
I0731 21:37:17.643343 20312 net.cpp:245] Setting up ctx_conv3
I0731 21:37:17.643350 20312 net.cpp:252] TRAIN Top shape for layer 51 'ctx_conv3' 6 64 80 80 (2457600)
I0731 21:37:17.643355 20312 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I0731 21:37:17.643357 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.643362 20312 net.cpp:184] Created Layer ctx_conv3/bn (52)
I0731 21:37:17.643371 20312 net.cpp:561] ctx_conv3/bn <- ctx_conv3
I0731 21:37:17.643374 20312 net.cpp:513] ctx_conv3/bn -> ctx_conv3 (in-place)
I0731 21:37:17.644084 20312 net.cpp:245] Setting up ctx_conv3/bn
I0731 21:37:17.644093 20312 net.cpp:252] TRAIN Top shape for layer 52 'ctx_conv3/bn' 6 64 80 80 (2457600)
I0731 21:37:17.644098 20312 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I0731 21:37:17.644100 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.644104 20312 net.cpp:184] Created Layer ctx_conv3/relu (53)
I0731 21:37:17.644105 20312 net.cpp:561] ctx_conv3/relu <- ctx_conv3
I0731 21:37:17.644107 20312 net.cpp:513] ctx_conv3/relu -> ctx_conv3 (in-place)
I0731 21:37:17.644110 20312 net.cpp:245] Setting up ctx_conv3/relu
I0731 21:37:17.644114 20312 net.cpp:252] TRAIN Top shape for layer 53 'ctx_conv3/relu' 6 64 80 80 (2457600)
I0731 21:37:17.644114 20312 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I0731 21:37:17.644117 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.644122 20312 net.cpp:184] Created Layer ctx_conv4 (54)
I0731 21:37:17.644125 20312 net.cpp:561] ctx_conv4 <- ctx_conv3
I0731 21:37:17.644130 20312 net.cpp:530] ctx_conv4 -> ctx_conv4
I0731 21:37:17.645510 20312 net.cpp:245] Setting up ctx_conv4
I0731 21:37:17.645520 20312 net.cpp:252] TRAIN Top shape for layer 54 'ctx_conv4' 6 64 80 80 (2457600)
I0731 21:37:17.645526 20312 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I0731 21:37:17.645530 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.645541 20312 net.cpp:184] Created Layer ctx_conv4/bn (55)
I0731 21:37:17.645545 20312 net.cpp:561] ctx_conv4/bn <- ctx_conv4
I0731 21:37:17.645548 20312 net.cpp:513] ctx_conv4/bn -> ctx_conv4 (in-place)
I0731 21:37:17.646446 20312 net.cpp:245] Setting up ctx_conv4/bn
I0731 21:37:17.646456 20312 net.cpp:252] TRAIN Top shape for layer 55 'ctx_conv4/bn' 6 64 80 80 (2457600)
I0731 21:37:17.646466 20312 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I0731 21:37:17.646469 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.646474 20312 net.cpp:184] Created Layer ctx_conv4/relu (56)
I0731 21:37:17.646477 20312 net.cpp:561] ctx_conv4/relu <- ctx_conv4
I0731 21:37:17.646481 20312 net.cpp:513] ctx_conv4/relu -> ctx_conv4 (in-place)
I0731 21:37:17.646486 20312 net.cpp:245] Setting up ctx_conv4/relu
I0731 21:37:17.646492 20312 net.cpp:252] TRAIN Top shape for layer 56 'ctx_conv4/relu' 6 64 80 80 (2457600)
I0731 21:37:17.646495 20312 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I0731 21:37:17.646498 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.646512 20312 net.cpp:184] Created Layer ctx_final (57)
I0731 21:37:17.646515 20312 net.cpp:561] ctx_final <- ctx_conv4
I0731 21:37:17.646519 20312 net.cpp:530] ctx_final -> ctx_final
I0731 21:37:17.661491 20312 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 7.22G, req 0G)
I0731 21:37:17.661514 20312 net.cpp:245] Setting up ctx_final
I0731 21:37:17.661520 20312 net.cpp:252] TRAIN Top shape for layer 57 'ctx_final' 6 8 80 80 (307200)
I0731 21:37:17.661530 20312 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I0731 21:37:17.661533 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.661540 20312 net.cpp:184] Created Layer ctx_final/relu (58)
I0731 21:37:17.661545 20312 net.cpp:561] ctx_final/relu <- ctx_final
I0731 21:37:17.661550 20312 net.cpp:513] ctx_final/relu -> ctx_final (in-place)
I0731 21:37:17.661556 20312 net.cpp:245] Setting up ctx_final/relu
I0731 21:37:17.661558 20312 net.cpp:252] TRAIN Top shape for layer 58 'ctx_final/relu' 6 8 80 80 (307200)
I0731 21:37:17.661561 20312 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I0731 21:37:17.661576 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.661586 20312 net.cpp:184] Created Layer out_deconv_final_up2 (59)
I0731 21:37:17.661588 20312 net.cpp:561] out_deconv_final_up2 <- ctx_final
I0731 21:37:17.661592 20312 net.cpp:530] out_deconv_final_up2 -> out_deconv_final_up2
I0731 21:37:17.661972 20312 net.cpp:245] Setting up out_deconv_final_up2
I0731 21:37:17.661981 20312 net.cpp:252] TRAIN Top shape for layer 59 'out_deconv_final_up2' 6 8 160 160 (1228800)
I0731 21:37:17.661986 20312 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I0731 21:37:17.661990 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.661998 20312 net.cpp:184] Created Layer out_deconv_final_up4 (60)
I0731 21:37:17.662001 20312 net.cpp:561] out_deconv_final_up4 <- out_deconv_final_up2
I0731 21:37:17.662005 20312 net.cpp:530] out_deconv_final_up4 -> out_deconv_final_up4
I0731 21:37:17.662395 20312 net.cpp:245] Setting up out_deconv_final_up4
I0731 21:37:17.662405 20312 net.cpp:252] TRAIN Top shape for layer 60 'out_deconv_final_up4' 6 8 320 320 (4915200)
I0731 21:37:17.662410 20312 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I0731 21:37:17.662413 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.662421 20312 net.cpp:184] Created Layer out_deconv_final_up8 (61)
I0731 21:37:17.662425 20312 net.cpp:561] out_deconv_final_up8 <- out_deconv_final_up4
I0731 21:37:17.662430 20312 net.cpp:530] out_deconv_final_up8 -> out_deconv_final_up8
I0731 21:37:17.662784 20312 net.cpp:245] Setting up out_deconv_final_up8
I0731 21:37:17.662793 20312 net.cpp:252] TRAIN Top shape for layer 61 'out_deconv_final_up8' 6 8 640 640 (19660800)
I0731 21:37:17.662799 20312 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0731 21:37:17.662803 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.662819 20312 net.cpp:184] Created Layer loss (62)
I0731 21:37:17.662822 20312 net.cpp:561] loss <- out_deconv_final_up8
I0731 21:37:17.662825 20312 net.cpp:561] loss <- label
I0731 21:37:17.662829 20312 net.cpp:530] loss -> loss
I0731 21:37:17.664244 20312 net.cpp:245] Setting up loss
I0731 21:37:17.664252 20312 net.cpp:252] TRAIN Top shape for layer 62 'loss' (1)
I0731 21:37:17.664255 20312 net.cpp:256]     with loss weight 1
I0731 21:37:17.664259 20312 net.cpp:323] loss needs backward computation.
I0731 21:37:17.664261 20312 net.cpp:323] out_deconv_final_up8 needs backward computation.
I0731 21:37:17.664263 20312 net.cpp:323] out_deconv_final_up4 needs backward computation.
I0731 21:37:17.664265 20312 net.cpp:323] out_deconv_final_up2 needs backward computation.
I0731 21:37:17.664268 20312 net.cpp:323] ctx_final/relu needs backward computation.
I0731 21:37:17.664269 20312 net.cpp:323] ctx_final needs backward computation.
I0731 21:37:17.664271 20312 net.cpp:323] ctx_conv4/relu needs backward computation.
I0731 21:37:17.664273 20312 net.cpp:323] ctx_conv4/bn needs backward computation.
I0731 21:37:17.664275 20312 net.cpp:323] ctx_conv4 needs backward computation.
I0731 21:37:17.664278 20312 net.cpp:323] ctx_conv3/relu needs backward computation.
I0731 21:37:17.664279 20312 net.cpp:323] ctx_conv3/bn needs backward computation.
I0731 21:37:17.664280 20312 net.cpp:323] ctx_conv3 needs backward computation.
I0731 21:37:17.664283 20312 net.cpp:323] ctx_conv2/relu needs backward computation.
I0731 21:37:17.664284 20312 net.cpp:323] ctx_conv2/bn needs backward computation.
I0731 21:37:17.664288 20312 net.cpp:323] ctx_conv2 needs backward computation.
I0731 21:37:17.664289 20312 net.cpp:323] ctx_conv1/relu needs backward computation.
I0731 21:37:17.664291 20312 net.cpp:323] ctx_conv1/bn needs backward computation.
I0731 21:37:17.664294 20312 net.cpp:323] ctx_conv1 needs backward computation.
I0731 21:37:17.664302 20312 net.cpp:323] out3_out5_combined needs backward computation.
I0731 21:37:17.664304 20312 net.cpp:323] out3a/relu needs backward computation.
I0731 21:37:17.664306 20312 net.cpp:323] out3a/bn needs backward computation.
I0731 21:37:17.664309 20312 net.cpp:323] out3a needs backward computation.
I0731 21:37:17.664311 20312 net.cpp:323] out5a_up2 needs backward computation.
I0731 21:37:17.664314 20312 net.cpp:323] out5a/relu needs backward computation.
I0731 21:37:17.664319 20312 net.cpp:323] out5a/bn needs backward computation.
I0731 21:37:17.664321 20312 net.cpp:323] out5a needs backward computation.
I0731 21:37:17.664325 20312 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0731 21:37:17.664330 20312 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0731 21:37:17.664333 20312 net.cpp:323] res5a_branch2b needs backward computation.
I0731 21:37:17.664336 20312 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0731 21:37:17.664340 20312 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0731 21:37:17.664343 20312 net.cpp:323] res5a_branch2a needs backward computation.
I0731 21:37:17.664347 20312 net.cpp:323] pool4 needs backward computation.
I0731 21:37:17.664351 20312 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0731 21:37:17.664355 20312 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0731 21:37:17.664358 20312 net.cpp:323] res4a_branch2b needs backward computation.
I0731 21:37:17.664363 20312 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0731 21:37:17.664366 20312 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0731 21:37:17.664371 20312 net.cpp:323] res4a_branch2a needs backward computation.
I0731 21:37:17.664376 20312 net.cpp:323] pool3 needs backward computation.
I0731 21:37:17.664379 20312 net.cpp:323] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I0731 21:37:17.664383 20312 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0731 21:37:17.664387 20312 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0731 21:37:17.664391 20312 net.cpp:323] res3a_branch2b needs backward computation.
I0731 21:37:17.664396 20312 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0731 21:37:17.664399 20312 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0731 21:37:17.664403 20312 net.cpp:323] res3a_branch2a needs backward computation.
I0731 21:37:17.664407 20312 net.cpp:323] pool2 needs backward computation.
I0731 21:37:17.664412 20312 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0731 21:37:17.664415 20312 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0731 21:37:17.664419 20312 net.cpp:323] res2a_branch2b needs backward computation.
I0731 21:37:17.664423 20312 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0731 21:37:17.664427 20312 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0731 21:37:17.664432 20312 net.cpp:323] res2a_branch2a needs backward computation.
I0731 21:37:17.664435 20312 net.cpp:323] pool1 needs backward computation.
I0731 21:37:17.664440 20312 net.cpp:323] conv1b/relu needs backward computation.
I0731 21:37:17.664444 20312 net.cpp:323] conv1b/bn needs backward computation.
I0731 21:37:17.664448 20312 net.cpp:323] conv1b needs backward computation.
I0731 21:37:17.664453 20312 net.cpp:323] conv1a/relu needs backward computation.
I0731 21:37:17.664455 20312 net.cpp:323] conv1a/bn needs backward computation.
I0731 21:37:17.664459 20312 net.cpp:323] conv1a needs backward computation.
I0731 21:37:17.664464 20312 net.cpp:325] data/bias does not need backward computation.
I0731 21:37:17.664469 20312 net.cpp:325] data does not need backward computation.
I0731 21:37:17.664472 20312 net.cpp:367] This network produces output loss
I0731 21:37:17.664542 20312 net.cpp:389] Top memory (TRAIN) required for data: 956006400 diff: 946176008
I0731 21:37:17.664548 20312 net.cpp:392] Bottom memory (TRAIN) required for data: 956006400 diff: 956006400
I0731 21:37:17.664552 20312 net.cpp:395] Shared (in-place) memory (TRAIN) by data: 630374400 diff: 630374400
I0731 21:37:17.664561 20312 net.cpp:398] Parameters memory (TRAIN) required for data: 2692608 diff: 2692608
I0731 21:37:17.664564 20312 net.cpp:401] Parameters shared memory (TRAIN) by data: 0 diff: 0
I0731 21:37:17.664569 20312 net.cpp:407] Network initialization done.
I0731 21:37:17.665467 20312 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/sparse/test.prototxt
W0731 21:37:17.665568 20312 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0731 21:37:17.665882 20312 net.cpp:72] Initializing net from parameters: 
name: "jsegnet21v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/val-image-lmdb"
    label_list_path: "data/val-label-lmdb"
    batch_size: 2
    threads: 1
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
  accuracy_param {
    ignore_label: 255
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
    ignore_label: 255
  }
}
I0731 21:37:17.666095 20312 net.cpp:104] Using FLOAT as default forward math type
I0731 21:37:17.666101 20312 net.cpp:110] Using FLOAT as default backward math type
I0731 21:37:17.666110 20312 layer_factory.hpp:136] Creating layer 'data' of type 'ImageLabelData'
I0731 21:37:17.666115 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.666122 20312 net.cpp:184] Created Layer data (0)
I0731 21:37:17.666126 20312 net.cpp:530] data -> data
I0731 21:37:17.666132 20312 net.cpp:530] data -> label
I0731 21:37:17.666154 20312 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0731 21:37:17.666162 20312 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 21:37:17.707537 20402 db_lmdb.cpp:35] Opened lmdb data/val-image-lmdb
I0731 21:37:17.709429 20312 data_layer.cpp:184] (0) ReshapePrefetch 2, 3, 640, 640
I0731 21:37:17.709525 20312 data_layer.cpp:208] (0) Output data size: 2, 3, 640, 640
I0731 21:37:17.709535 20312 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 21:37:17.709595 20312 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0731 21:37:17.709607 20312 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 21:37:17.710530 20403 data_layer.cpp:97] (0) Parser threads: 1
I0731 21:37:17.710544 20403 data_layer.cpp:99] (0) Transformer threads: 1
I0731 21:37:17.740299 20404 db_lmdb.cpp:35] Opened lmdb data/val-label-lmdb
I0731 21:37:17.741274 20312 data_layer.cpp:184] (0) ReshapePrefetch 2, 1, 640, 640
I0731 21:37:17.741356 20312 data_layer.cpp:208] (0) Output data size: 2, 1, 640, 640
I0731 21:37:17.741365 20312 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 21:37:17.741411 20312 net.cpp:245] Setting up data
I0731 21:37:17.741422 20312 net.cpp:252] TEST Top shape for layer 0 'data' 2 3 640 640 (2457600)
I0731 21:37:17.741431 20312 net.cpp:252] TEST Top shape for layer 0 'data' 2 1 640 640 (819200)
I0731 21:37:17.741438 20312 layer_factory.hpp:136] Creating layer 'label_data_1_split' of type 'Split'
I0731 21:37:17.741446 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.741458 20312 net.cpp:184] Created Layer label_data_1_split (1)
I0731 21:37:17.741464 20312 net.cpp:561] label_data_1_split <- label
I0731 21:37:17.741472 20312 net.cpp:530] label_data_1_split -> label_data_1_split_0
I0731 21:37:17.741480 20312 net.cpp:530] label_data_1_split -> label_data_1_split_1
I0731 21:37:17.741487 20312 net.cpp:530] label_data_1_split -> label_data_1_split_2
I0731 21:37:17.741613 20312 net.cpp:245] Setting up label_data_1_split
I0731 21:37:17.741623 20312 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0731 21:37:17.741631 20312 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0731 21:37:17.741636 20312 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0731 21:37:17.741641 20312 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0731 21:37:17.741648 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.741659 20312 net.cpp:184] Created Layer data/bias (2)
I0731 21:37:17.741675 20312 net.cpp:561] data/bias <- data
I0731 21:37:17.741681 20312 net.cpp:530] data/bias -> data/bias
I0731 21:37:17.743319 20405 data_layer.cpp:97] (0) Parser threads: 1
I0731 21:37:17.743335 20405 data_layer.cpp:99] (0) Transformer threads: 1
I0731 21:37:17.745365 20312 net.cpp:245] Setting up data/bias
I0731 21:37:17.745386 20312 net.cpp:252] TEST Top shape for layer 2 'data/bias' 2 3 640 640 (2457600)
I0731 21:37:17.745398 20312 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0731 21:37:17.745404 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.745416 20312 net.cpp:184] Created Layer conv1a (3)
I0731 21:37:17.745421 20312 net.cpp:561] conv1a <- data/bias
I0731 21:37:17.745425 20312 net.cpp:530] conv1a -> conv1a
I0731 21:37:17.751524 20312 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.09G, req 0G)
I0731 21:37:17.751560 20312 net.cpp:245] Setting up conv1a
I0731 21:37:17.751567 20312 net.cpp:252] TEST Top shape for layer 3 'conv1a' 2 32 320 320 (6553600)
I0731 21:37:17.751579 20312 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0731 21:37:17.751585 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.751593 20312 net.cpp:184] Created Layer conv1a/bn (4)
I0731 21:37:17.751597 20312 net.cpp:561] conv1a/bn <- conv1a
I0731 21:37:17.751601 20312 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0731 21:37:17.752620 20312 net.cpp:245] Setting up conv1a/bn
I0731 21:37:17.752630 20312 net.cpp:252] TEST Top shape for layer 4 'conv1a/bn' 2 32 320 320 (6553600)
I0731 21:37:17.752640 20312 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0731 21:37:17.752643 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.752712 20312 net.cpp:184] Created Layer conv1a/relu (5)
I0731 21:37:17.752718 20312 net.cpp:561] conv1a/relu <- conv1a
I0731 21:37:17.752722 20312 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0731 21:37:17.752727 20312 net.cpp:245] Setting up conv1a/relu
I0731 21:37:17.752732 20312 net.cpp:252] TEST Top shape for layer 5 'conv1a/relu' 2 32 320 320 (6553600)
I0731 21:37:17.752735 20312 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0731 21:37:17.752739 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.752748 20312 net.cpp:184] Created Layer conv1b (6)
I0731 21:37:17.752753 20312 net.cpp:561] conv1b <- conv1a
I0731 21:37:17.752755 20312 net.cpp:530] conv1b -> conv1b
I0731 21:37:17.767802 20312 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.06G, req 0G)
I0731 21:37:17.767817 20312 net.cpp:245] Setting up conv1b
I0731 21:37:17.767822 20312 net.cpp:252] TEST Top shape for layer 6 'conv1b' 2 32 320 320 (6553600)
I0731 21:37:17.767830 20312 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0731 21:37:17.767834 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.767840 20312 net.cpp:184] Created Layer conv1b/bn (7)
I0731 21:37:17.767844 20312 net.cpp:561] conv1b/bn <- conv1b
I0731 21:37:17.767848 20312 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0731 21:37:17.768808 20312 net.cpp:245] Setting up conv1b/bn
I0731 21:37:17.768826 20312 net.cpp:252] TEST Top shape for layer 7 'conv1b/bn' 2 32 320 320 (6553600)
I0731 21:37:17.768836 20312 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0731 21:37:17.768839 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.768843 20312 net.cpp:184] Created Layer conv1b/relu (8)
I0731 21:37:17.768847 20312 net.cpp:561] conv1b/relu <- conv1b
I0731 21:37:17.768851 20312 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0731 21:37:17.768854 20312 net.cpp:245] Setting up conv1b/relu
I0731 21:37:17.768859 20312 net.cpp:252] TEST Top shape for layer 8 'conv1b/relu' 2 32 320 320 (6553600)
I0731 21:37:17.768863 20312 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0731 21:37:17.768867 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.768872 20312 net.cpp:184] Created Layer pool1 (9)
I0731 21:37:17.768877 20312 net.cpp:561] pool1 <- conv1b
I0731 21:37:17.768880 20312 net.cpp:530] pool1 -> pool1
I0731 21:37:17.768981 20312 net.cpp:245] Setting up pool1
I0731 21:37:17.768988 20312 net.cpp:252] TEST Top shape for layer 9 'pool1' 2 32 160 160 (1638400)
I0731 21:37:17.768991 20312 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0731 21:37:17.768996 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.769003 20312 net.cpp:184] Created Layer res2a_branch2a (10)
I0731 21:37:17.769007 20312 net.cpp:561] res2a_branch2a <- pool1
I0731 21:37:17.769021 20312 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0731 21:37:17.778324 20312 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.03G, req 0G)
I0731 21:37:17.778347 20312 net.cpp:245] Setting up res2a_branch2a
I0731 21:37:17.778353 20312 net.cpp:252] TEST Top shape for layer 10 'res2a_branch2a' 2 64 160 160 (3276800)
I0731 21:37:17.778367 20312 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0731 21:37:17.778372 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.778390 20312 net.cpp:184] Created Layer res2a_branch2a/bn (11)
I0731 21:37:17.778395 20312 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0731 21:37:17.778399 20312 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0731 21:37:17.779481 20312 net.cpp:245] Setting up res2a_branch2a/bn
I0731 21:37:17.779492 20312 net.cpp:252] TEST Top shape for layer 11 'res2a_branch2a/bn' 2 64 160 160 (3276800)
I0731 21:37:17.779500 20312 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0731 21:37:17.779505 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.779510 20312 net.cpp:184] Created Layer res2a_branch2a/relu (12)
I0731 21:37:17.779512 20312 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0731 21:37:17.779516 20312 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0731 21:37:17.779521 20312 net.cpp:245] Setting up res2a_branch2a/relu
I0731 21:37:17.779525 20312 net.cpp:252] TEST Top shape for layer 12 'res2a_branch2a/relu' 2 64 160 160 (3276800)
I0731 21:37:17.779527 20312 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0731 21:37:17.779532 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.779541 20312 net.cpp:184] Created Layer res2a_branch2b (13)
I0731 21:37:17.779546 20312 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0731 21:37:17.779548 20312 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0731 21:37:17.787411 20312 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.02G, req 0G)
I0731 21:37:17.787426 20312 net.cpp:245] Setting up res2a_branch2b
I0731 21:37:17.787432 20312 net.cpp:252] TEST Top shape for layer 13 'res2a_branch2b' 2 64 160 160 (3276800)
I0731 21:37:17.787438 20312 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0731 21:37:17.787442 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.787448 20312 net.cpp:184] Created Layer res2a_branch2b/bn (14)
I0731 21:37:17.787452 20312 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0731 21:37:17.787456 20312 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0731 21:37:17.788439 20312 net.cpp:245] Setting up res2a_branch2b/bn
I0731 21:37:17.788450 20312 net.cpp:252] TEST Top shape for layer 14 'res2a_branch2b/bn' 2 64 160 160 (3276800)
I0731 21:37:17.788457 20312 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0731 21:37:17.788461 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.788466 20312 net.cpp:184] Created Layer res2a_branch2b/relu (15)
I0731 21:37:17.788470 20312 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0731 21:37:17.788472 20312 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0731 21:37:17.788477 20312 net.cpp:245] Setting up res2a_branch2b/relu
I0731 21:37:17.788481 20312 net.cpp:252] TEST Top shape for layer 15 'res2a_branch2b/relu' 2 64 160 160 (3276800)
I0731 21:37:17.788483 20312 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0731 21:37:17.788488 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.788494 20312 net.cpp:184] Created Layer pool2 (16)
I0731 21:37:17.788498 20312 net.cpp:561] pool2 <- res2a_branch2b
I0731 21:37:17.788513 20312 net.cpp:530] pool2 -> pool2
I0731 21:37:17.788606 20312 net.cpp:245] Setting up pool2
I0731 21:37:17.788612 20312 net.cpp:252] TEST Top shape for layer 16 'pool2' 2 64 80 80 (819200)
I0731 21:37:17.788616 20312 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0731 21:37:17.788620 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.788638 20312 net.cpp:184] Created Layer res3a_branch2a (17)
I0731 21:37:17.788642 20312 net.cpp:561] res3a_branch2a <- pool2
I0731 21:37:17.788646 20312 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0731 21:37:17.795866 20312 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.01G, req 0G)
I0731 21:37:17.795879 20312 net.cpp:245] Setting up res3a_branch2a
I0731 21:37:17.795884 20312 net.cpp:252] TEST Top shape for layer 17 'res3a_branch2a' 2 128 80 80 (1638400)
I0731 21:37:17.795892 20312 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0731 21:37:17.795898 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.795907 20312 net.cpp:184] Created Layer res3a_branch2a/bn (18)
I0731 21:37:17.795912 20312 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0731 21:37:17.795914 20312 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0731 21:37:17.796939 20312 net.cpp:245] Setting up res3a_branch2a/bn
I0731 21:37:17.796952 20312 net.cpp:252] TEST Top shape for layer 18 'res3a_branch2a/bn' 2 128 80 80 (1638400)
I0731 21:37:17.796962 20312 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0731 21:37:17.796965 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.796969 20312 net.cpp:184] Created Layer res3a_branch2a/relu (19)
I0731 21:37:17.796972 20312 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0731 21:37:17.796977 20312 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0731 21:37:17.796980 20312 net.cpp:245] Setting up res3a_branch2a/relu
I0731 21:37:17.796984 20312 net.cpp:252] TEST Top shape for layer 19 'res3a_branch2a/relu' 2 128 80 80 (1638400)
I0731 21:37:17.796988 20312 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0731 21:37:17.796993 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.797000 20312 net.cpp:184] Created Layer res3a_branch2b (20)
I0731 21:37:17.797003 20312 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0731 21:37:17.797008 20312 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0731 21:37:17.803287 20312 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7G, req 0G)
I0731 21:37:17.803304 20312 net.cpp:245] Setting up res3a_branch2b
I0731 21:37:17.803313 20312 net.cpp:252] TEST Top shape for layer 20 'res3a_branch2b' 2 128 80 80 (1638400)
I0731 21:37:17.803323 20312 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0731 21:37:17.803328 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.803336 20312 net.cpp:184] Created Layer res3a_branch2b/bn (21)
I0731 21:37:17.803344 20312 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0731 21:37:17.803350 20312 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0731 21:37:17.805080 20312 net.cpp:245] Setting up res3a_branch2b/bn
I0731 21:37:17.805100 20312 net.cpp:252] TEST Top shape for layer 21 'res3a_branch2b/bn' 2 128 80 80 (1638400)
I0731 21:37:17.805115 20312 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0731 21:37:17.805120 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.805129 20312 net.cpp:184] Created Layer res3a_branch2b/relu (22)
I0731 21:37:17.805133 20312 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0731 21:37:17.805141 20312 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0731 21:37:17.805163 20312 net.cpp:245] Setting up res3a_branch2b/relu
I0731 21:37:17.805169 20312 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b/relu' 2 128 80 80 (1638400)
I0731 21:37:17.805174 20312 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0731 21:37:17.805179 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.805184 20312 net.cpp:184] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (23)
I0731 21:37:17.805189 20312 net.cpp:561] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0731 21:37:17.805193 20312 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0731 21:37:17.805199 20312 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0731 21:37:17.805292 20312 net.cpp:245] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0731 21:37:17.805302 20312 net.cpp:252] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 2 128 80 80 (1638400)
I0731 21:37:17.805308 20312 net.cpp:252] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 2 128 80 80 (1638400)
I0731 21:37:17.805313 20312 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0731 21:37:17.805318 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.805335 20312 net.cpp:184] Created Layer pool3 (24)
I0731 21:37:17.805341 20312 net.cpp:561] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0731 21:37:17.805348 20312 net.cpp:530] pool3 -> pool3
I0731 21:37:17.805477 20312 net.cpp:245] Setting up pool3
I0731 21:37:17.805497 20312 net.cpp:252] TEST Top shape for layer 24 'pool3' 2 128 40 40 (409600)
I0731 21:37:17.805503 20312 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0731 21:37:17.805508 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.805526 20312 net.cpp:184] Created Layer res4a_branch2a (25)
I0731 21:37:17.805531 20312 net.cpp:561] res4a_branch2a <- pool3
I0731 21:37:17.805536 20312 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0731 21:37:17.822706 20312 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.99G, req 0G)
I0731 21:37:17.822720 20312 net.cpp:245] Setting up res4a_branch2a
I0731 21:37:17.822723 20312 net.cpp:252] TEST Top shape for layer 25 'res4a_branch2a' 2 256 40 40 (819200)
I0731 21:37:17.822728 20312 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0731 21:37:17.822731 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.822742 20312 net.cpp:184] Created Layer res4a_branch2a/bn (26)
I0731 21:37:17.822746 20312 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0731 21:37:17.822748 20312 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0731 21:37:17.823456 20312 net.cpp:245] Setting up res4a_branch2a/bn
I0731 21:37:17.823462 20312 net.cpp:252] TEST Top shape for layer 26 'res4a_branch2a/bn' 2 256 40 40 (819200)
I0731 21:37:17.823468 20312 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0731 21:37:17.823472 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.823474 20312 net.cpp:184] Created Layer res4a_branch2a/relu (27)
I0731 21:37:17.823477 20312 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0731 21:37:17.823485 20312 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0731 21:37:17.823489 20312 net.cpp:245] Setting up res4a_branch2a/relu
I0731 21:37:17.823492 20312 net.cpp:252] TEST Top shape for layer 27 'res4a_branch2a/relu' 2 256 40 40 (819200)
I0731 21:37:17.823494 20312 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0731 21:37:17.823498 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.823521 20312 net.cpp:184] Created Layer res4a_branch2b (28)
I0731 21:37:17.823524 20312 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0731 21:37:17.823526 20312 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0731 21:37:17.830296 20312 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.98G, req 0G)
I0731 21:37:17.830307 20312 net.cpp:245] Setting up res4a_branch2b
I0731 21:37:17.830312 20312 net.cpp:252] TEST Top shape for layer 28 'res4a_branch2b' 2 256 40 40 (819200)
I0731 21:37:17.830317 20312 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0731 21:37:17.830319 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.830324 20312 net.cpp:184] Created Layer res4a_branch2b/bn (29)
I0731 21:37:17.830327 20312 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0731 21:37:17.830330 20312 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0731 21:37:17.831032 20312 net.cpp:245] Setting up res4a_branch2b/bn
I0731 21:37:17.831039 20312 net.cpp:252] TEST Top shape for layer 29 'res4a_branch2b/bn' 2 256 40 40 (819200)
I0731 21:37:17.831045 20312 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0731 21:37:17.831048 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.831053 20312 net.cpp:184] Created Layer res4a_branch2b/relu (30)
I0731 21:37:17.831055 20312 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0731 21:37:17.831058 20312 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0731 21:37:17.831063 20312 net.cpp:245] Setting up res4a_branch2b/relu
I0731 21:37:17.831065 20312 net.cpp:252] TEST Top shape for layer 30 'res4a_branch2b/relu' 2 256 40 40 (819200)
I0731 21:37:17.831068 20312 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0731 21:37:17.831070 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.831074 20312 net.cpp:184] Created Layer pool4 (31)
I0731 21:37:17.831077 20312 net.cpp:561] pool4 <- res4a_branch2b
I0731 21:37:17.831080 20312 net.cpp:530] pool4 -> pool4
I0731 21:37:17.831156 20312 net.cpp:245] Setting up pool4
I0731 21:37:17.831161 20312 net.cpp:252] TEST Top shape for layer 31 'pool4' 2 256 40 40 (819200)
I0731 21:37:17.831164 20312 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0731 21:37:17.831167 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.831178 20312 net.cpp:184] Created Layer res5a_branch2a (32)
I0731 21:37:17.831182 20312 net.cpp:561] res5a_branch2a <- pool4
I0731 21:37:17.831184 20312 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0731 21:37:17.856552 20312 net.cpp:245] Setting up res5a_branch2a
I0731 21:37:17.856576 20312 net.cpp:252] TEST Top shape for layer 32 'res5a_branch2a' 2 512 40 40 (1638400)
I0731 21:37:17.856583 20312 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0731 21:37:17.856587 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.856593 20312 net.cpp:184] Created Layer res5a_branch2a/bn (33)
I0731 21:37:17.856597 20312 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0731 21:37:17.856601 20312 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0731 21:37:17.857312 20312 net.cpp:245] Setting up res5a_branch2a/bn
I0731 21:37:17.857321 20312 net.cpp:252] TEST Top shape for layer 33 'res5a_branch2a/bn' 2 512 40 40 (1638400)
I0731 21:37:17.857326 20312 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0731 21:37:17.857329 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.857332 20312 net.cpp:184] Created Layer res5a_branch2a/relu (34)
I0731 21:37:17.857336 20312 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0731 21:37:17.857337 20312 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0731 21:37:17.857350 20312 net.cpp:245] Setting up res5a_branch2a/relu
I0731 21:37:17.857353 20312 net.cpp:252] TEST Top shape for layer 34 'res5a_branch2a/relu' 2 512 40 40 (1638400)
I0731 21:37:17.857355 20312 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0731 21:37:17.857358 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.857365 20312 net.cpp:184] Created Layer res5a_branch2b (35)
I0731 21:37:17.857368 20312 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0731 21:37:17.857370 20312 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0731 21:37:17.870718 20312 net.cpp:245] Setting up res5a_branch2b
I0731 21:37:17.870740 20312 net.cpp:252] TEST Top shape for layer 35 'res5a_branch2b' 2 512 40 40 (1638400)
I0731 21:37:17.870759 20312 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0731 21:37:17.870764 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.870774 20312 net.cpp:184] Created Layer res5a_branch2b/bn (36)
I0731 21:37:17.870777 20312 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0731 21:37:17.870780 20312 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0731 21:37:17.871495 20312 net.cpp:245] Setting up res5a_branch2b/bn
I0731 21:37:17.871503 20312 net.cpp:252] TEST Top shape for layer 36 'res5a_branch2b/bn' 2 512 40 40 (1638400)
I0731 21:37:17.871510 20312 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0731 21:37:17.871512 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.871515 20312 net.cpp:184] Created Layer res5a_branch2b/relu (37)
I0731 21:37:17.871517 20312 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0731 21:37:17.871520 20312 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0731 21:37:17.871523 20312 net.cpp:245] Setting up res5a_branch2b/relu
I0731 21:37:17.871526 20312 net.cpp:252] TEST Top shape for layer 37 'res5a_branch2b/relu' 2 512 40 40 (1638400)
I0731 21:37:17.871528 20312 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I0731 21:37:17.871531 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.871541 20312 net.cpp:184] Created Layer out5a (38)
I0731 21:37:17.871543 20312 net.cpp:561] out5a <- res5a_branch2b
I0731 21:37:17.871546 20312 net.cpp:530] out5a -> out5a
I0731 21:37:17.874876 20312 net.cpp:245] Setting up out5a
I0731 21:37:17.874883 20312 net.cpp:252] TEST Top shape for layer 38 'out5a' 2 64 40 40 (204800)
I0731 21:37:17.874888 20312 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I0731 21:37:17.874891 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.874894 20312 net.cpp:184] Created Layer out5a/bn (39)
I0731 21:37:17.874897 20312 net.cpp:561] out5a/bn <- out5a
I0731 21:37:17.874899 20312 net.cpp:513] out5a/bn -> out5a (in-place)
I0731 21:37:17.875659 20312 net.cpp:245] Setting up out5a/bn
I0731 21:37:17.875666 20312 net.cpp:252] TEST Top shape for layer 39 'out5a/bn' 2 64 40 40 (204800)
I0731 21:37:17.875672 20312 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I0731 21:37:17.875674 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.875677 20312 net.cpp:184] Created Layer out5a/relu (40)
I0731 21:37:17.875680 20312 net.cpp:561] out5a/relu <- out5a
I0731 21:37:17.875682 20312 net.cpp:513] out5a/relu -> out5a (in-place)
I0731 21:37:17.875685 20312 net.cpp:245] Setting up out5a/relu
I0731 21:37:17.875687 20312 net.cpp:252] TEST Top shape for layer 40 'out5a/relu' 2 64 40 40 (204800)
I0731 21:37:17.875689 20312 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I0731 21:37:17.875692 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.875708 20312 net.cpp:184] Created Layer out5a_up2 (41)
I0731 21:37:17.875711 20312 net.cpp:561] out5a_up2 <- out5a
I0731 21:37:17.875713 20312 net.cpp:530] out5a_up2 -> out5a_up2
I0731 21:37:17.876020 20312 net.cpp:245] Setting up out5a_up2
I0731 21:37:17.876025 20312 net.cpp:252] TEST Top shape for layer 41 'out5a_up2' 2 64 80 80 (819200)
I0731 21:37:17.876029 20312 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I0731 21:37:17.876031 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.876036 20312 net.cpp:184] Created Layer out3a (42)
I0731 21:37:17.876039 20312 net.cpp:561] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0731 21:37:17.876041 20312 net.cpp:530] out3a -> out3a
I0731 21:37:17.880188 20312 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.97G, req 0G)
I0731 21:37:17.880198 20312 net.cpp:245] Setting up out3a
I0731 21:37:17.880201 20312 net.cpp:252] TEST Top shape for layer 42 'out3a' 2 64 80 80 (819200)
I0731 21:37:17.880205 20312 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I0731 21:37:17.880208 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.880213 20312 net.cpp:184] Created Layer out3a/bn (43)
I0731 21:37:17.880215 20312 net.cpp:561] out3a/bn <- out3a
I0731 21:37:17.880218 20312 net.cpp:513] out3a/bn -> out3a (in-place)
I0731 21:37:17.880962 20312 net.cpp:245] Setting up out3a/bn
I0731 21:37:17.880970 20312 net.cpp:252] TEST Top shape for layer 43 'out3a/bn' 2 64 80 80 (819200)
I0731 21:37:17.880975 20312 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I0731 21:37:17.880978 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.880981 20312 net.cpp:184] Created Layer out3a/relu (44)
I0731 21:37:17.880983 20312 net.cpp:561] out3a/relu <- out3a
I0731 21:37:17.880985 20312 net.cpp:513] out3a/relu -> out3a (in-place)
I0731 21:37:17.880990 20312 net.cpp:245] Setting up out3a/relu
I0731 21:37:17.880991 20312 net.cpp:252] TEST Top shape for layer 44 'out3a/relu' 2 64 80 80 (819200)
I0731 21:37:17.880993 20312 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I0731 21:37:17.880995 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.880998 20312 net.cpp:184] Created Layer out3_out5_combined (45)
I0731 21:37:17.881000 20312 net.cpp:561] out3_out5_combined <- out5a_up2
I0731 21:37:17.881003 20312 net.cpp:561] out3_out5_combined <- out3a
I0731 21:37:17.881006 20312 net.cpp:530] out3_out5_combined -> out3_out5_combined
I0731 21:37:17.881903 20312 net.cpp:245] Setting up out3_out5_combined
I0731 21:37:17.881912 20312 net.cpp:252] TEST Top shape for layer 45 'out3_out5_combined' 2 64 80 80 (819200)
I0731 21:37:17.881916 20312 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I0731 21:37:17.881917 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.881923 20312 net.cpp:184] Created Layer ctx_conv1 (46)
I0731 21:37:17.881925 20312 net.cpp:561] ctx_conv1 <- out3_out5_combined
I0731 21:37:17.881928 20312 net.cpp:530] ctx_conv1 -> ctx_conv1
I0731 21:37:17.886366 20312 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.96G, req 0G)
I0731 21:37:17.886375 20312 net.cpp:245] Setting up ctx_conv1
I0731 21:37:17.886379 20312 net.cpp:252] TEST Top shape for layer 46 'ctx_conv1' 2 64 80 80 (819200)
I0731 21:37:17.886384 20312 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I0731 21:37:17.886385 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.886394 20312 net.cpp:184] Created Layer ctx_conv1/bn (47)
I0731 21:37:17.886396 20312 net.cpp:561] ctx_conv1/bn <- ctx_conv1
I0731 21:37:17.886399 20312 net.cpp:513] ctx_conv1/bn -> ctx_conv1 (in-place)
I0731 21:37:17.887125 20312 net.cpp:245] Setting up ctx_conv1/bn
I0731 21:37:17.887138 20312 net.cpp:252] TEST Top shape for layer 47 'ctx_conv1/bn' 2 64 80 80 (819200)
I0731 21:37:17.887145 20312 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I0731 21:37:17.887147 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.887151 20312 net.cpp:184] Created Layer ctx_conv1/relu (48)
I0731 21:37:17.887153 20312 net.cpp:561] ctx_conv1/relu <- ctx_conv1
I0731 21:37:17.887156 20312 net.cpp:513] ctx_conv1/relu -> ctx_conv1 (in-place)
I0731 21:37:17.887159 20312 net.cpp:245] Setting up ctx_conv1/relu
I0731 21:37:17.887161 20312 net.cpp:252] TEST Top shape for layer 48 'ctx_conv1/relu' 2 64 80 80 (819200)
I0731 21:37:17.887163 20312 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I0731 21:37:17.887166 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.887171 20312 net.cpp:184] Created Layer ctx_conv2 (49)
I0731 21:37:17.887173 20312 net.cpp:561] ctx_conv2 <- ctx_conv1
I0731 21:37:17.887176 20312 net.cpp:530] ctx_conv2 -> ctx_conv2
I0731 21:37:17.888293 20312 net.cpp:245] Setting up ctx_conv2
I0731 21:37:17.888299 20312 net.cpp:252] TEST Top shape for layer 49 'ctx_conv2' 2 64 80 80 (819200)
I0731 21:37:17.888303 20312 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I0731 21:37:17.888305 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.888309 20312 net.cpp:184] Created Layer ctx_conv2/bn (50)
I0731 21:37:17.888311 20312 net.cpp:561] ctx_conv2/bn <- ctx_conv2
I0731 21:37:17.888314 20312 net.cpp:513] ctx_conv2/bn -> ctx_conv2 (in-place)
I0731 21:37:17.889037 20312 net.cpp:245] Setting up ctx_conv2/bn
I0731 21:37:17.889045 20312 net.cpp:252] TEST Top shape for layer 50 'ctx_conv2/bn' 2 64 80 80 (819200)
I0731 21:37:17.889050 20312 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I0731 21:37:17.889053 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.889055 20312 net.cpp:184] Created Layer ctx_conv2/relu (51)
I0731 21:37:17.889057 20312 net.cpp:561] ctx_conv2/relu <- ctx_conv2
I0731 21:37:17.889060 20312 net.cpp:513] ctx_conv2/relu -> ctx_conv2 (in-place)
I0731 21:37:17.889063 20312 net.cpp:245] Setting up ctx_conv2/relu
I0731 21:37:17.889065 20312 net.cpp:252] TEST Top shape for layer 51 'ctx_conv2/relu' 2 64 80 80 (819200)
I0731 21:37:17.889067 20312 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I0731 21:37:17.889070 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.889075 20312 net.cpp:184] Created Layer ctx_conv3 (52)
I0731 21:37:17.889076 20312 net.cpp:561] ctx_conv3 <- ctx_conv2
I0731 21:37:17.889078 20312 net.cpp:530] ctx_conv3 -> ctx_conv3
I0731 21:37:17.890188 20312 net.cpp:245] Setting up ctx_conv3
I0731 21:37:17.890195 20312 net.cpp:252] TEST Top shape for layer 52 'ctx_conv3' 2 64 80 80 (819200)
I0731 21:37:17.890199 20312 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I0731 21:37:17.890202 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.890205 20312 net.cpp:184] Created Layer ctx_conv3/bn (53)
I0731 21:37:17.890208 20312 net.cpp:561] ctx_conv3/bn <- ctx_conv3
I0731 21:37:17.890210 20312 net.cpp:513] ctx_conv3/bn -> ctx_conv3 (in-place)
I0731 21:37:17.890960 20312 net.cpp:245] Setting up ctx_conv3/bn
I0731 21:37:17.890969 20312 net.cpp:252] TEST Top shape for layer 53 'ctx_conv3/bn' 2 64 80 80 (819200)
I0731 21:37:17.890974 20312 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I0731 21:37:17.890977 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.890980 20312 net.cpp:184] Created Layer ctx_conv3/relu (54)
I0731 21:37:17.890982 20312 net.cpp:561] ctx_conv3/relu <- ctx_conv3
I0731 21:37:17.890990 20312 net.cpp:513] ctx_conv3/relu -> ctx_conv3 (in-place)
I0731 21:37:17.890995 20312 net.cpp:245] Setting up ctx_conv3/relu
I0731 21:37:17.890997 20312 net.cpp:252] TEST Top shape for layer 54 'ctx_conv3/relu' 2 64 80 80 (819200)
I0731 21:37:17.891000 20312 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I0731 21:37:17.891001 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.891010 20312 net.cpp:184] Created Layer ctx_conv4 (55)
I0731 21:37:17.891013 20312 net.cpp:561] ctx_conv4 <- ctx_conv3
I0731 21:37:17.891016 20312 net.cpp:530] ctx_conv4 -> ctx_conv4
I0731 21:37:17.892292 20312 net.cpp:245] Setting up ctx_conv4
I0731 21:37:17.892300 20312 net.cpp:252] TEST Top shape for layer 55 'ctx_conv4' 2 64 80 80 (819200)
I0731 21:37:17.892304 20312 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I0731 21:37:17.892307 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.892310 20312 net.cpp:184] Created Layer ctx_conv4/bn (56)
I0731 21:37:17.892313 20312 net.cpp:561] ctx_conv4/bn <- ctx_conv4
I0731 21:37:17.892315 20312 net.cpp:513] ctx_conv4/bn -> ctx_conv4 (in-place)
I0731 21:37:17.893079 20312 net.cpp:245] Setting up ctx_conv4/bn
I0731 21:37:17.893086 20312 net.cpp:252] TEST Top shape for layer 56 'ctx_conv4/bn' 2 64 80 80 (819200)
I0731 21:37:17.893092 20312 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I0731 21:37:17.893095 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.893097 20312 net.cpp:184] Created Layer ctx_conv4/relu (57)
I0731 21:37:17.893100 20312 net.cpp:561] ctx_conv4/relu <- ctx_conv4
I0731 21:37:17.893102 20312 net.cpp:513] ctx_conv4/relu -> ctx_conv4 (in-place)
I0731 21:37:17.893105 20312 net.cpp:245] Setting up ctx_conv4/relu
I0731 21:37:17.893108 20312 net.cpp:252] TEST Top shape for layer 57 'ctx_conv4/relu' 2 64 80 80 (819200)
I0731 21:37:17.893110 20312 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I0731 21:37:17.893111 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.893116 20312 net.cpp:184] Created Layer ctx_final (58)
I0731 21:37:17.893118 20312 net.cpp:561] ctx_final <- ctx_conv4
I0731 21:37:17.893121 20312 net.cpp:530] ctx_final -> ctx_final
I0731 21:37:17.897956 20312 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.96G, req 0G)
I0731 21:37:17.897966 20312 net.cpp:245] Setting up ctx_final
I0731 21:37:17.897971 20312 net.cpp:252] TEST Top shape for layer 58 'ctx_final' 2 8 80 80 (102400)
I0731 21:37:17.897975 20312 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I0731 21:37:17.897979 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.897981 20312 net.cpp:184] Created Layer ctx_final/relu (59)
I0731 21:37:17.897984 20312 net.cpp:561] ctx_final/relu <- ctx_final
I0731 21:37:17.897986 20312 net.cpp:513] ctx_final/relu -> ctx_final (in-place)
I0731 21:37:17.897996 20312 net.cpp:245] Setting up ctx_final/relu
I0731 21:37:17.898000 20312 net.cpp:252] TEST Top shape for layer 59 'ctx_final/relu' 2 8 80 80 (102400)
I0731 21:37:17.898001 20312 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I0731 21:37:17.898003 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.898010 20312 net.cpp:184] Created Layer out_deconv_final_up2 (60)
I0731 21:37:17.898012 20312 net.cpp:561] out_deconv_final_up2 <- ctx_final
I0731 21:37:17.898015 20312 net.cpp:530] out_deconv_final_up2 -> out_deconv_final_up2
I0731 21:37:17.898360 20312 net.cpp:245] Setting up out_deconv_final_up2
I0731 21:37:17.898368 20312 net.cpp:252] TEST Top shape for layer 60 'out_deconv_final_up2' 2 8 160 160 (409600)
I0731 21:37:17.898373 20312 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I0731 21:37:17.898386 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.898393 20312 net.cpp:184] Created Layer out_deconv_final_up4 (61)
I0731 21:37:17.898397 20312 net.cpp:561] out_deconv_final_up4 <- out_deconv_final_up2
I0731 21:37:17.898401 20312 net.cpp:530] out_deconv_final_up4 -> out_deconv_final_up4
I0731 21:37:17.898810 20312 net.cpp:245] Setting up out_deconv_final_up4
I0731 21:37:17.898818 20312 net.cpp:252] TEST Top shape for layer 61 'out_deconv_final_up4' 2 8 320 320 (1638400)
I0731 21:37:17.898821 20312 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I0731 21:37:17.898823 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.898831 20312 net.cpp:184] Created Layer out_deconv_final_up8 (62)
I0731 21:37:17.898834 20312 net.cpp:561] out_deconv_final_up8 <- out_deconv_final_up4
I0731 21:37:17.898838 20312 net.cpp:530] out_deconv_final_up8 -> out_deconv_final_up8
I0731 21:37:17.899199 20312 net.cpp:245] Setting up out_deconv_final_up8
I0731 21:37:17.899204 20312 net.cpp:252] TEST Top shape for layer 62 'out_deconv_final_up8' 2 8 640 640 (6553600)
I0731 21:37:17.899209 20312 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8_out_deconv_final_up8_0_split' of type 'Split'
I0731 21:37:17.899210 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.899214 20312 net.cpp:184] Created Layer out_deconv_final_up8_out_deconv_final_up8_0_split (63)
I0731 21:37:17.899216 20312 net.cpp:561] out_deconv_final_up8_out_deconv_final_up8_0_split <- out_deconv_final_up8
I0731 21:37:17.899219 20312 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0731 21:37:17.899221 20312 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0731 21:37:17.899224 20312 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0731 21:37:17.899296 20312 net.cpp:245] Setting up out_deconv_final_up8_out_deconv_final_up8_0_split
I0731 21:37:17.899303 20312 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 8 640 640 (6553600)
I0731 21:37:17.899307 20312 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 8 640 640 (6553600)
I0731 21:37:17.899312 20312 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 8 640 640 (6553600)
I0731 21:37:17.899315 20312 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0731 21:37:17.899319 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.899327 20312 net.cpp:184] Created Layer loss (64)
I0731 21:37:17.899330 20312 net.cpp:561] loss <- out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0731 21:37:17.899335 20312 net.cpp:561] loss <- label_data_1_split_0
I0731 21:37:17.899340 20312 net.cpp:530] loss -> loss
I0731 21:37:17.900352 20312 net.cpp:245] Setting up loss
I0731 21:37:17.900362 20312 net.cpp:252] TEST Top shape for layer 64 'loss' (1)
I0731 21:37:17.900363 20312 net.cpp:256]     with loss weight 1
I0731 21:37:17.900367 20312 layer_factory.hpp:136] Creating layer 'accuracy/top1' of type 'Accuracy'
I0731 21:37:17.900370 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.900377 20312 net.cpp:184] Created Layer accuracy/top1 (65)
I0731 21:37:17.900379 20312 net.cpp:561] accuracy/top1 <- out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0731 21:37:17.900382 20312 net.cpp:561] accuracy/top1 <- label_data_1_split_1
I0731 21:37:17.900385 20312 net.cpp:530] accuracy/top1 -> accuracy/top1
I0731 21:37:17.900390 20312 net.cpp:245] Setting up accuracy/top1
I0731 21:37:17.900393 20312 net.cpp:252] TEST Top shape for layer 65 'accuracy/top1' (1)
I0731 21:37:17.900401 20312 layer_factory.hpp:136] Creating layer 'accuracy/top5' of type 'Accuracy'
I0731 21:37:17.900404 20312 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 21:37:17.900408 20312 net.cpp:184] Created Layer accuracy/top5 (66)
I0731 21:37:17.900410 20312 net.cpp:561] accuracy/top5 <- out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0731 21:37:17.900413 20312 net.cpp:561] accuracy/top5 <- label_data_1_split_2
I0731 21:37:17.900415 20312 net.cpp:530] accuracy/top5 -> accuracy/top5
I0731 21:37:17.900419 20312 net.cpp:245] Setting up accuracy/top5
I0731 21:37:17.900423 20312 net.cpp:252] TEST Top shape for layer 66 'accuracy/top5' (1)
I0731 21:37:17.900425 20312 net.cpp:325] accuracy/top5 does not need backward computation.
I0731 21:37:17.900429 20312 net.cpp:325] accuracy/top1 does not need backward computation.
I0731 21:37:17.900434 20312 net.cpp:323] loss needs backward computation.
I0731 21:37:17.900437 20312 net.cpp:323] out_deconv_final_up8_out_deconv_final_up8_0_split needs backward computation.
I0731 21:37:17.900441 20312 net.cpp:323] out_deconv_final_up8 needs backward computation.
I0731 21:37:17.900445 20312 net.cpp:323] out_deconv_final_up4 needs backward computation.
I0731 21:37:17.900449 20312 net.cpp:323] out_deconv_final_up2 needs backward computation.
I0731 21:37:17.900452 20312 net.cpp:323] ctx_final/relu needs backward computation.
I0731 21:37:17.900455 20312 net.cpp:323] ctx_final needs backward computation.
I0731 21:37:17.900460 20312 net.cpp:323] ctx_conv4/relu needs backward computation.
I0731 21:37:17.900463 20312 net.cpp:323] ctx_conv4/bn needs backward computation.
I0731 21:37:17.900466 20312 net.cpp:323] ctx_conv4 needs backward computation.
I0731 21:37:17.900470 20312 net.cpp:323] ctx_conv3/relu needs backward computation.
I0731 21:37:17.900475 20312 net.cpp:323] ctx_conv3/bn needs backward computation.
I0731 21:37:17.900478 20312 net.cpp:323] ctx_conv3 needs backward computation.
I0731 21:37:17.900482 20312 net.cpp:323] ctx_conv2/relu needs backward computation.
I0731 21:37:17.900486 20312 net.cpp:323] ctx_conv2/bn needs backward computation.
I0731 21:37:17.900490 20312 net.cpp:323] ctx_conv2 needs backward computation.
I0731 21:37:17.900493 20312 net.cpp:323] ctx_conv1/relu needs backward computation.
I0731 21:37:17.900497 20312 net.cpp:323] ctx_conv1/bn needs backward computation.
I0731 21:37:17.900501 20312 net.cpp:323] ctx_conv1 needs backward computation.
I0731 21:37:17.900506 20312 net.cpp:323] out3_out5_combined needs backward computation.
I0731 21:37:17.900509 20312 net.cpp:323] out3a/relu needs backward computation.
I0731 21:37:17.900513 20312 net.cpp:323] out3a/bn needs backward computation.
I0731 21:37:17.900517 20312 net.cpp:323] out3a needs backward computation.
I0731 21:37:17.900522 20312 net.cpp:323] out5a_up2 needs backward computation.
I0731 21:37:17.900527 20312 net.cpp:323] out5a/relu needs backward computation.
I0731 21:37:17.900530 20312 net.cpp:323] out5a/bn needs backward computation.
I0731 21:37:17.900533 20312 net.cpp:323] out5a needs backward computation.
I0731 21:37:17.900538 20312 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0731 21:37:17.900542 20312 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0731 21:37:17.900545 20312 net.cpp:323] res5a_branch2b needs backward computation.
I0731 21:37:17.900549 20312 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0731 21:37:17.900553 20312 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0731 21:37:17.900557 20312 net.cpp:323] res5a_branch2a needs backward computation.
I0731 21:37:17.900562 20312 net.cpp:323] pool4 needs backward computation.
I0731 21:37:17.900565 20312 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0731 21:37:17.900569 20312 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0731 21:37:17.900573 20312 net.cpp:323] res4a_branch2b needs backward computation.
I0731 21:37:17.900578 20312 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0731 21:37:17.900586 20312 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0731 21:37:17.900590 20312 net.cpp:323] res4a_branch2a needs backward computation.
I0731 21:37:17.900594 20312 net.cpp:323] pool3 needs backward computation.
I0731 21:37:17.900599 20312 net.cpp:323] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I0731 21:37:17.900602 20312 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0731 21:37:17.900606 20312 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0731 21:37:17.900610 20312 net.cpp:323] res3a_branch2b needs backward computation.
I0731 21:37:17.900615 20312 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0731 21:37:17.900619 20312 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0731 21:37:17.900624 20312 net.cpp:323] res3a_branch2a needs backward computation.
I0731 21:37:17.900627 20312 net.cpp:323] pool2 needs backward computation.
I0731 21:37:17.900631 20312 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0731 21:37:17.900635 20312 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0731 21:37:17.900640 20312 net.cpp:323] res2a_branch2b needs backward computation.
I0731 21:37:17.900643 20312 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0731 21:37:17.900647 20312 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0731 21:37:17.900651 20312 net.cpp:323] res2a_branch2a needs backward computation.
I0731 21:37:17.900655 20312 net.cpp:323] pool1 needs backward computation.
I0731 21:37:17.900660 20312 net.cpp:323] conv1b/relu needs backward computation.
I0731 21:37:17.900663 20312 net.cpp:323] conv1b/bn needs backward computation.
I0731 21:37:17.900667 20312 net.cpp:323] conv1b needs backward computation.
I0731 21:37:17.900671 20312 net.cpp:323] conv1a/relu needs backward computation.
I0731 21:37:17.900676 20312 net.cpp:323] conv1a/bn needs backward computation.
I0731 21:37:17.900679 20312 net.cpp:323] conv1a needs backward computation.
I0731 21:37:17.900684 20312 net.cpp:325] data/bias does not need backward computation.
I0731 21:37:17.900689 20312 net.cpp:325] label_data_1_split does not need backward computation.
I0731 21:37:17.900693 20312 net.cpp:325] data does not need backward computation.
I0731 21:37:17.900697 20312 net.cpp:367] This network produces output accuracy/top1
I0731 21:37:17.900702 20312 net.cpp:367] This network produces output accuracy/top5
I0731 21:37:17.900707 20312 net.cpp:367] This network produces output loss
I0731 21:37:17.900765 20312 net.cpp:389] Top memory (TEST) required for data: 318668800 diff: 8
I0731 21:37:17.900770 20312 net.cpp:392] Bottom memory (TEST) required for data: 318668800 diff: 318668800
I0731 21:37:17.900774 20312 net.cpp:395] Shared (in-place) memory (TEST) by data: 210124800 diff: 210124800
I0731 21:37:17.900779 20312 net.cpp:398] Parameters memory (TEST) required for data: 2692608 diff: 2692608
I0731 21:37:17.900781 20312 net.cpp:401] Parameters shared memory (TEST) by data: 0 diff: 0
I0731 21:37:17.900785 20312 net.cpp:407] Network initialization done.
I0731 21:37:17.900904 20312 solver.cpp:56] Solver scaffolding done.
I0731 21:37:17.910991 20312 caffe.cpp:137] Finetuning from training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/l1reg/cityscapes5_jsegnet21v2_iter_32000.caffemodel
I0731 21:37:17.916599 20312 net.cpp:1089] Copying source layer data Type:ImageLabelData #blobs=0
I0731 21:37:17.916632 20312 net.cpp:1089] Copying source layer data/bias Type:Bias #blobs=1
I0731 21:37:17.916683 20312 net.cpp:1089] Copying source layer conv1a Type:Convolution #blobs=2
I0731 21:37:17.916703 20312 net.cpp:1089] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0731 21:37:17.917369 20312 net.cpp:1089] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0731 21:37:17.917377 20312 net.cpp:1089] Copying source layer conv1b Type:Convolution #blobs=2
I0731 21:37:17.917387 20312 net.cpp:1089] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0731 21:37:17.917804 20312 net.cpp:1089] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0731 21:37:17.917821 20312 net.cpp:1089] Copying source layer pool1 Type:Pooling #blobs=0
I0731 21:37:17.917824 20312 net.cpp:1089] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0731 21:37:17.917840 20312 net.cpp:1089] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0731 21:37:17.918282 20312 net.cpp:1089] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0731 21:37:17.918289 20312 net.cpp:1089] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0731 21:37:17.918303 20312 net.cpp:1089] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0731 21:37:17.918730 20312 net.cpp:1089] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0731 21:37:17.918736 20312 net.cpp:1089] Copying source layer pool2 Type:Pooling #blobs=0
I0731 21:37:17.918738 20312 net.cpp:1089] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0731 21:37:17.918776 20312 net.cpp:1089] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0731 21:37:17.919179 20312 net.cpp:1089] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0731 21:37:17.919185 20312 net.cpp:1089] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0731 21:37:17.919209 20312 net.cpp:1089] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0731 21:37:17.919603 20312 net.cpp:1089] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0731 21:37:17.919610 20312 net.cpp:1089] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I0731 21:37:17.919612 20312 net.cpp:1089] Copying source layer pool3 Type:Pooling #blobs=0
I0731 21:37:17.919615 20312 net.cpp:1089] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0731 21:37:17.919734 20312 net.cpp:1089] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0731 21:37:17.920122 20312 net.cpp:1089] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0731 21:37:17.920128 20312 net.cpp:1089] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0731 21:37:17.920200 20312 net.cpp:1089] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0731 21:37:17.920585 20312 net.cpp:1089] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0731 21:37:17.920593 20312 net.cpp:1089] Copying source layer pool4 Type:Pooling #blobs=0
I0731 21:37:17.920594 20312 net.cpp:1089] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0731 21:37:17.921011 20312 net.cpp:1089] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0731 21:37:17.921386 20312 net.cpp:1089] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0731 21:37:17.921392 20312 net.cpp:1089] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0731 21:37:17.921599 20312 net.cpp:1089] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0731 21:37:17.921967 20312 net.cpp:1089] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0731 21:37:17.921974 20312 net.cpp:1089] Copying source layer out5a Type:Convolution #blobs=2
I0731 21:37:17.922036 20312 net.cpp:1089] Copying source layer out5a/bn Type:BatchNorm #blobs=5
I0731 21:37:17.922232 20312 net.cpp:1089] Copying source layer out5a/relu Type:ReLU #blobs=0
I0731 21:37:17.922238 20312 net.cpp:1089] Copying source layer out5a_up2 Type:Deconvolution #blobs=1
I0731 21:37:17.922245 20312 net.cpp:1089] Copying source layer out3a Type:Convolution #blobs=2
I0731 21:37:17.922266 20312 net.cpp:1089] Copying source layer out3a/bn Type:BatchNorm #blobs=5
I0731 21:37:17.922430 20312 net.cpp:1089] Copying source layer out3a/relu Type:ReLU #blobs=0
I0731 21:37:17.922437 20312 net.cpp:1089] Copying source layer out3_out5_combined Type:Eltwise #blobs=0
I0731 21:37:17.922441 20312 net.cpp:1089] Copying source layer ctx_conv1 Type:Convolution #blobs=2
I0731 21:37:17.922466 20312 net.cpp:1089] Copying source layer ctx_conv1/bn Type:BatchNorm #blobs=5
I0731 21:37:17.922647 20312 net.cpp:1089] Copying source layer ctx_conv1/relu Type:ReLU #blobs=0
I0731 21:37:17.922653 20312 net.cpp:1089] Copying source layer ctx_conv2 Type:Convolution #blobs=2
I0731 21:37:17.922684 20312 net.cpp:1089] Copying source layer ctx_conv2/bn Type:BatchNorm #blobs=5
I0731 21:37:17.922888 20312 net.cpp:1089] Copying source layer ctx_conv2/relu Type:ReLU #blobs=0
I0731 21:37:17.922894 20312 net.cpp:1089] Copying source layer ctx_conv3 Type:Convolution #blobs=2
I0731 21:37:17.922914 20312 net.cpp:1089] Copying source layer ctx_conv3/bn Type:BatchNorm #blobs=5
I0731 21:37:17.923091 20312 net.cpp:1089] Copying source layer ctx_conv3/relu Type:ReLU #blobs=0
I0731 21:37:17.923096 20312 net.cpp:1089] Copying source layer ctx_conv4 Type:Convolution #blobs=2
I0731 21:37:17.923117 20312 net.cpp:1089] Copying source layer ctx_conv4/bn Type:BatchNorm #blobs=5
I0731 21:37:17.923295 20312 net.cpp:1089] Copying source layer ctx_conv4/relu Type:ReLU #blobs=0
I0731 21:37:17.923300 20312 net.cpp:1089] Copying source layer ctx_final Type:Convolution #blobs=2
I0731 21:37:17.923310 20312 net.cpp:1089] Copying source layer ctx_final/relu Type:ReLU #blobs=0
I0731 21:37:17.923313 20312 net.cpp:1089] Copying source layer out_deconv_final_up2 Type:Deconvolution #blobs=1
I0731 21:37:17.923318 20312 net.cpp:1089] Copying source layer out_deconv_final_up4 Type:Deconvolution #blobs=1
I0731 21:37:17.923323 20312 net.cpp:1089] Copying source layer out_deconv_final_up8 Type:Deconvolution #blobs=1
I0731 21:37:17.923329 20312 net.cpp:1089] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0731 21:37:17.927064 20312 net.cpp:1089] Copying source layer data Type:ImageLabelData #blobs=0
I0731 21:37:17.927083 20312 net.cpp:1089] Copying source layer data/bias Type:Bias #blobs=1
I0731 21:37:17.927111 20312 net.cpp:1089] Copying source layer conv1a Type:Convolution #blobs=2
I0731 21:37:17.927124 20312 net.cpp:1089] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0731 21:37:17.927645 20312 net.cpp:1089] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0731 21:37:17.927650 20312 net.cpp:1089] Copying source layer conv1b Type:Convolution #blobs=2
I0731 21:37:17.927659 20312 net.cpp:1089] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0731 21:37:17.928040 20312 net.cpp:1089] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0731 21:37:17.928045 20312 net.cpp:1089] Copying source layer pool1 Type:Pooling #blobs=0
I0731 21:37:17.928047 20312 net.cpp:1089] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0731 21:37:17.928063 20312 net.cpp:1089] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0731 21:37:17.928444 20312 net.cpp:1089] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0731 21:37:17.928449 20312 net.cpp:1089] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0731 21:37:17.928460 20312 net.cpp:1089] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0731 21:37:17.928845 20312 net.cpp:1089] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0731 21:37:17.928851 20312 net.cpp:1089] Copying source layer pool2 Type:Pooling #blobs=0
I0731 21:37:17.928853 20312 net.cpp:1089] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0731 21:37:17.928891 20312 net.cpp:1089] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0731 21:37:17.929246 20312 net.cpp:1089] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0731 21:37:17.929251 20312 net.cpp:1089] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0731 21:37:17.929273 20312 net.cpp:1089] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0731 21:37:17.929630 20312 net.cpp:1089] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0731 21:37:17.929636 20312 net.cpp:1089] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I0731 21:37:17.929637 20312 net.cpp:1089] Copying source layer pool3 Type:Pooling #blobs=0
I0731 21:37:17.929639 20312 net.cpp:1089] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0731 21:37:17.929745 20312 net.cpp:1089] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0731 21:37:17.930109 20312 net.cpp:1089] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0731 21:37:17.930114 20312 net.cpp:1089] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0731 21:37:17.930172 20312 net.cpp:1089] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0731 21:37:17.930526 20312 net.cpp:1089] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0731 21:37:17.930531 20312 net.cpp:1089] Copying source layer pool4 Type:Pooling #blobs=0
I0731 21:37:17.930533 20312 net.cpp:1089] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0731 21:37:17.930899 20312 net.cpp:1089] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0731 21:37:17.931255 20312 net.cpp:1089] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0731 21:37:17.931260 20312 net.cpp:1089] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0731 21:37:17.931437 20312 net.cpp:1089] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0731 21:37:17.931790 20312 net.cpp:1089] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0731 21:37:17.931797 20312 net.cpp:1089] Copying source layer out5a Type:Convolution #blobs=2
I0731 21:37:17.931843 20312 net.cpp:1089] Copying source layer out5a/bn Type:BatchNorm #blobs=5
I0731 21:37:17.932006 20312 net.cpp:1089] Copying source layer out5a/relu Type:ReLU #blobs=0
I0731 21:37:17.932010 20312 net.cpp:1089] Copying source layer out5a_up2 Type:Deconvolution #blobs=1
I0731 21:37:17.932016 20312 net.cpp:1089] Copying source layer out3a Type:Convolution #blobs=2
I0731 21:37:17.932034 20312 net.cpp:1089] Copying source layer out3a/bn Type:BatchNorm #blobs=5
I0731 21:37:17.932181 20312 net.cpp:1089] Copying source layer out3a/relu Type:ReLU #blobs=0
I0731 21:37:17.932186 20312 net.cpp:1089] Copying source layer out3_out5_combined Type:Eltwise #blobs=0
I0731 21:37:17.932188 20312 net.cpp:1089] Copying source layer ctx_conv1 Type:Convolution #blobs=2
I0731 21:37:17.932207 20312 net.cpp:1089] Copying source layer ctx_conv1/bn Type:BatchNorm #blobs=5
I0731 21:37:17.932401 20312 net.cpp:1089] Copying source layer ctx_conv1/relu Type:ReLU #blobs=0
I0731 21:37:17.932410 20312 net.cpp:1089] Copying source layer ctx_conv2 Type:Convolution #blobs=2
I0731 21:37:17.932433 20312 net.cpp:1089] Copying source layer ctx_conv2/bn Type:BatchNorm #blobs=5
I0731 21:37:17.932621 20312 net.cpp:1089] Copying source layer ctx_conv2/relu Type:ReLU #blobs=0
I0731 21:37:17.932626 20312 net.cpp:1089] Copying source layer ctx_conv3 Type:Convolution #blobs=2
I0731 21:37:17.932646 20312 net.cpp:1089] Copying source layer ctx_conv3/bn Type:BatchNorm #blobs=5
I0731 21:37:17.932835 20312 net.cpp:1089] Copying source layer ctx_conv3/relu Type:ReLU #blobs=0
I0731 21:37:17.932842 20312 net.cpp:1089] Copying source layer ctx_conv4 Type:Convolution #blobs=2
I0731 21:37:17.932860 20312 net.cpp:1089] Copying source layer ctx_conv4/bn Type:BatchNorm #blobs=5
I0731 21:37:17.933039 20312 net.cpp:1089] Copying source layer ctx_conv4/relu Type:ReLU #blobs=0
I0731 21:37:17.933045 20312 net.cpp:1089] Copying source layer ctx_final Type:Convolution #blobs=2
I0731 21:37:17.933054 20312 net.cpp:1089] Copying source layer ctx_final/relu Type:ReLU #blobs=0
I0731 21:37:17.933058 20312 net.cpp:1089] Copying source layer out_deconv_final_up2 Type:Deconvolution #blobs=1
I0731 21:37:17.933063 20312 net.cpp:1089] Copying source layer out_deconv_final_up4 Type:Deconvolution #blobs=1
I0731 21:37:17.933068 20312 net.cpp:1089] Copying source layer out_deconv_final_up8 Type:Deconvolution #blobs=1
I0731 21:37:17.933073 20312 net.cpp:1089] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0731 21:37:17.933193 20312 parallel.cpp:108] [0 - 0] P2pSync adding callback
I0731 21:37:17.933200 20312 parallel.cpp:108] [1 - 1] P2pSync adding callback
I0731 21:37:17.933204 20312 parallel.cpp:108] [2 - 2] P2pSync adding callback
I0731 21:37:17.933208 20312 parallel.cpp:61] Starting Optimization
I0731 21:37:17.933212 20312 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 21:37:17.933251 20312 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 21:37:17.933269 20312 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 21:37:17.933954 20406 device_alternate.hpp:116] NVML initialized on thread 136079995094784
I0731 21:37:17.948607 20406 common.cpp:583] NVML succeeded to set CPU affinity on device 0
I0731 21:37:17.948662 20407 device_alternate.hpp:116] NVML initialized on thread 136079986702080
I0731 21:37:17.949589 20407 common.cpp:583] NVML succeeded to set CPU affinity on device 1
I0731 21:37:17.949641 20408 device_alternate.hpp:116] NVML initialized on thread 136079978309376
I0731 21:37:17.950461 20408 common.cpp:583] NVML succeeded to set CPU affinity on device 2
I0731 21:37:17.953948 20407 solver.cpp:42] Solver data type: FLOAT
W0731 21:37:17.954510 20407 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0731 21:37:17.954638 20407 net.cpp:104] Using FLOAT as default forward math type
I0731 21:37:17.954643 20407 net.cpp:110] Using FLOAT as default backward math type
I0731 21:37:17.954674 20407 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0731 21:37:17.954679 20407 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 21:37:17.958235 20408 solver.cpp:42] Solver data type: FLOAT
W0731 21:37:17.958777 20408 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0731 21:37:17.958886 20408 net.cpp:104] Using FLOAT as default forward math type
I0731 21:37:17.958892 20408 net.cpp:110] Using FLOAT as default backward math type
I0731 21:37:17.958921 20408 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0731 21:37:17.958928 20408 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 21:37:17.958966 20409 db_lmdb.cpp:35] Opened lmdb data/train-image-lmdb
I0731 21:37:17.961843 20407 data_layer.cpp:184] [1] ReshapePrefetch 6, 3, 640, 640
I0731 21:37:17.961854 20410 db_lmdb.cpp:35] Opened lmdb data/train-image-lmdb
I0731 21:37:17.961968 20407 data_layer.cpp:208] [1] Output data size: 6, 3, 640, 640
I0731 21:37:17.961977 20407 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 21:37:17.962038 20407 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0731 21:37:17.962050 20407 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 21:37:17.962954 20411 data_layer.cpp:97] [1] Parser threads: 1
I0731 21:37:17.962990 20411 data_layer.cpp:99] [1] Transformer threads: 1
I0731 21:37:17.969084 20412 db_lmdb.cpp:35] Opened lmdb data/train-label-lmdb
I0731 21:37:17.971240 20408 data_layer.cpp:184] [2] ReshapePrefetch 6, 3, 640, 640
I0731 21:37:17.971310 20407 data_layer.cpp:184] [1] ReshapePrefetch 6, 1, 640, 640
I0731 21:37:17.971328 20408 data_layer.cpp:208] [2] Output data size: 6, 3, 640, 640
I0731 21:37:17.971339 20408 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 21:37:17.971369 20407 data_layer.cpp:208] [1] Output data size: 6, 1, 640, 640
I0731 21:37:17.971375 20407 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 21:37:17.971698 20408 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0731 21:37:17.971715 20408 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 21:37:17.972748 20413 data_layer.cpp:97] [1] Parser threads: 1
I0731 21:37:17.972779 20413 data_layer.cpp:99] [1] Transformer threads: 1
I0731 21:37:17.975131 20414 data_layer.cpp:97] [2] Parser threads: 1
I0731 21:37:17.975175 20414 data_layer.cpp:99] [2] Transformer threads: 1
I0731 21:37:17.982218 20415 db_lmdb.cpp:35] Opened lmdb data/train-label-lmdb
I0731 21:37:17.986227 20408 data_layer.cpp:184] [2] ReshapePrefetch 6, 1, 640, 640
I0731 21:37:17.986925 20408 data_layer.cpp:208] [2] Output data size: 6, 1, 640, 640
I0731 21:37:17.986946 20408 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 21:37:17.992627 20413 blocking_queue.cpp:40] Waiting for datum
I0731 21:37:17.993294 20416 data_layer.cpp:97] [2] Parser threads: 1
I0731 21:37:17.993372 20416 data_layer.cpp:99] [2] Transformer threads: 1
I0731 21:37:18.519807 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 7.99G, req 0G)
I0731 21:37:18.535709 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 7.99G, req 0G)
I0731 21:37:18.572796 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.83G, req 0G)
I0731 21:37:18.590581 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.83G, req 0G)
I0731 21:37:18.618458 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.7G, req 0G)
I0731 21:37:18.640271 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.7G, req 0G)
I0731 21:37:18.655947 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.62G, req 0G)
I0731 21:37:18.675751 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.62G, req 0G)
I0731 21:37:18.681535 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 7.56G, req 0G)
I0731 21:37:18.696717 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.52G, req 0G)
I0731 21:37:18.700321 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 7.56G, req 0G)
I0731 21:37:18.713639 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.52G, req 0G)
I0731 21:37:18.726083 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.48G, req 0G)
I0731 21:37:18.738119 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.46G, req 0G)
I0731 21:37:18.742321 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.48G, req 0G)
I0731 21:37:18.753047 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.46G, req 0G)
I0731 21:37:18.803251 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 7.39G, req 0G)
I0731 21:37:18.814246 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 7.39G, req 0G)
I0731 21:37:18.824282 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 7.34G, req 0G)
I0731 21:37:18.833729 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 7.34G, req 0G)
I0731 21:37:18.845731 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 7.32G, req 0G)
I0731 21:37:18.848871 20407 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/sparse/test.prototxt
W0731 21:37:18.848958 20407 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0731 21:37:18.849095 20407 net.cpp:104] Using FLOAT as default forward math type
I0731 21:37:18.849100 20407 net.cpp:110] Using FLOAT as default backward math type
I0731 21:37:18.849128 20407 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0731 21:37:18.849135 20407 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 21:37:18.849853 20434 db_lmdb.cpp:35] Opened lmdb data/val-image-lmdb
I0731 21:37:18.851527 20407 data_layer.cpp:184] (1) ReshapePrefetch 2, 3, 640, 640
I0731 21:37:18.851593 20407 data_layer.cpp:208] (1) Output data size: 2, 3, 640, 640
I0731 21:37:18.851600 20407 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 21:37:18.851647 20407 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0731 21:37:18.851655 20407 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 21:37:18.852494 20435 data_layer.cpp:97] (1) Parser threads: 1
I0731 21:37:18.852506 20435 data_layer.cpp:99] (1) Transformer threads: 1
I0731 21:37:18.854698 20436 db_lmdb.cpp:35] Opened lmdb data/val-label-lmdb
I0731 21:37:18.856030 20407 data_layer.cpp:184] (1) ReshapePrefetch 2, 1, 640, 640
I0731 21:37:18.856045 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 7.32G, req 0G)
I0731 21:37:18.856202 20407 data_layer.cpp:208] (1) Output data size: 2, 1, 640, 640
I0731 21:37:18.856211 20407 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 21:37:18.857125 20437 data_layer.cpp:97] (1) Parser threads: 1
I0731 21:37:18.857136 20437 data_layer.cpp:99] (1) Transformer threads: 1
I0731 21:37:18.866287 20408 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/sparse/test.prototxt
W0731 21:37:18.866366 20408 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0731 21:37:18.866492 20408 net.cpp:104] Using FLOAT as default forward math type
I0731 21:37:18.866497 20408 net.cpp:110] Using FLOAT as default backward math type
I0731 21:37:18.866523 20408 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0731 21:37:18.866528 20408 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 21:37:18.867341 20438 db_lmdb.cpp:35] Opened lmdb data/val-image-lmdb
I0731 21:37:18.868343 20407 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.18G, req 0G)
I0731 21:37:18.868980 20408 data_layer.cpp:184] (2) ReshapePrefetch 2, 3, 640, 640
I0731 21:37:18.869122 20408 data_layer.cpp:208] (2) Output data size: 2, 3, 640, 640
I0731 21:37:18.869135 20408 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 21:37:18.869187 20408 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0731 21:37:18.869201 20408 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 21:37:18.869972 20439 data_layer.cpp:97] (2) Parser threads: 1
I0731 21:37:18.869983 20439 data_layer.cpp:99] (2) Transformer threads: 1
I0731 21:37:18.872686 20440 db_lmdb.cpp:35] Opened lmdb data/val-label-lmdb
I0731 21:37:18.874047 20408 data_layer.cpp:184] (2) ReshapePrefetch 2, 1, 640, 640
I0731 21:37:18.874261 20408 data_layer.cpp:208] (2) Output data size: 2, 1, 640, 640
I0731 21:37:18.874272 20408 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 21:37:18.875777 20441 data_layer.cpp:97] (2) Parser threads: 1
I0731 21:37:18.875792 20441 data_layer.cpp:99] (2) Transformer threads: 1
I0731 21:37:18.886585 20408 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.18G, req 0G)
I0731 21:37:18.893762 20407 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.15G, req 0G)
I0731 21:37:18.904338 20408 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.15G, req 0G)
I0731 21:37:18.904808 20407 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.13G, req 0G)
I0731 21:37:18.914539 20407 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 1  (limit 7.12G, req 0G)
I0731 21:37:18.916216 20408 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.13G, req 0G)
I0731 21:37:18.922758 20407 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.1G, req 0G)
I0731 21:37:18.925755 20408 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.12G, req 0G)
I0731 21:37:18.929806 20407 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.09G, req 0G)
I0731 21:37:18.935590 20408 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.1G, req 0G)
I0731 21:37:18.942962 20408 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.09G, req 0G)
I0731 21:37:18.944257 20407 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.09G, req 0G)
I0731 21:37:18.952741 20407 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.08G, req 0G)
I0731 21:37:18.957046 20408 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.09G, req 0G)
I0731 21:37:18.967164 20408 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.08G, req 0G)
I0731 21:37:19.004966 20407 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 7.07G, req 0G)
I0731 21:37:19.011307 20407 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 7.06G, req 0G)
I0731 21:37:19.025051 20408 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 7.07G, req 0G)
I0731 21:37:19.025792 20407 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 7.05G, req 0G)
I0731 21:37:19.029878 20407 solver.cpp:56] Solver scaffolding done.
I0731 21:37:19.032649 20408 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 7.06G, req 0G)
I0731 21:37:19.049891 20408 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 7.05G, req 0G)
I0731 21:37:19.052213 20408 solver.cpp:56] Solver scaffolding done.
I0731 21:37:19.112440 20406 parallel.cpp:164] [0 - 0] P2pSync adding callback
I0731 21:37:19.112440 20407 parallel.cpp:164] [1 - 1] P2pSync adding callback
I0731 21:37:19.112440 20408 parallel.cpp:164] [2 - 2] P2pSync adding callback
I0731 21:37:19.320559 20406 net.cpp:2244] All zero weights of convolution layers are frozen
I0731 21:37:19.336573 20408 solver.cpp:479] Solving jsegnet21v2_train
I0731 21:37:19.336591 20408 solver.cpp:480] Learning Rate Policy: multistep
I0731 21:37:19.338251 20407 solver.cpp:479] Solving jsegnet21v2_train
I0731 21:37:19.338260 20407 solver.cpp:480] Learning Rate Policy: multistep
I0731 21:37:19.338660 20406 solver.cpp:479] Solving jsegnet21v2_train
I0731 21:37:19.338670 20406 solver.cpp:480] Learning Rate Policy: multistep
I0731 21:37:19.352705 20406 solver.cpp:268] Starting Optimization on GPU 0
I0731 21:37:19.352705 20408 solver.cpp:268] Starting Optimization on GPU 2
I0731 21:37:19.352705 20407 solver.cpp:268] Starting Optimization on GPU 1
I0731 21:37:19.353006 20406 solver.cpp:550] Iteration 0, Testing net (#0)
I0731 21:37:19.353034 20460 device_alternate.hpp:116] NVML initialized on thread 128079826835200
I0731 21:37:19.353067 20460 common.cpp:583] NVML succeeded to set CPU affinity on device 2
I0731 21:37:19.353080 20459 device_alternate.hpp:116] NVML initialized on thread 128079810049792
I0731 21:37:19.353091 20459 common.cpp:583] NVML succeeded to set CPU affinity on device 1
I0731 21:37:19.353822 20461 device_alternate.hpp:116] NVML initialized on thread 128079818442496
I0731 21:37:19.353834 20461 common.cpp:583] NVML succeeded to set CPU affinity on device 0
I0731 21:37:19.369244 20408 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 6.95G, req 0G)
I0731 21:37:19.372304 20407 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 6.95G, req 0G)
I0731 21:37:19.390118 20408 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 6.9G, req 0G)
I0731 21:37:19.390686 20407 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 6.9G, req 0G)
I0731 21:37:19.406363 20406 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1a' with space 0.01G/1 1  (limit 6.88G, req 0G)
I0731 21:37:19.408455 20408 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 6.83G, req 0G)
I0731 21:37:19.408644 20407 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 6.83G, req 0G)
I0731 21:37:19.419778 20408 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 6.8G, req 0G)
I0731 21:37:19.420095 20407 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 6.8G, req 0G)
I0731 21:37:19.427031 20406 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 6.82G, req 0G)
I0731 21:37:19.429046 20408 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 6.77G, req 0G)
I0731 21:37:19.430095 20407 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 6.77G, req 0G)
I0731 21:37:19.435477 20408 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 6.75G, req 0G)
I0731 21:37:19.436337 20407 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 6.75G, req 0G)
I0731 21:37:19.442193 20406 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 6.75G, req 0G)
I0731 21:37:19.444633 20408 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.74G, req 0G)
I0731 21:37:19.446466 20407 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.74G, req 0G)
I0731 21:37:19.449489 20408 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.73G, req 0G)
I0731 21:37:19.450136 20407 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.73G, req 0G)
I0731 21:37:19.452047 20406 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 6.73G, req 0G)
I0731 21:37:19.462432 20406 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 6.69G, req 0G)
I0731 21:37:19.470166 20406 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 6.67G, req 0G)
I0731 21:37:19.477594 20406 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.66G, req 0G)
I0731 21:37:19.479645 20407 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.58G, req 0G)
I0731 21:37:19.480876 20408 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.58G, req 0G)
I0731 21:37:19.483157 20406 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.65G, req 0G)
I0731 21:37:19.485607 20407 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.57G, req 0G)
I0731 21:37:19.486870 20408 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.57G, req 0G)
I0731 21:37:19.513156 20407 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.47G, req 0G)
I0731 21:37:19.513636 20406 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.5G, req 0G)
I0731 21:37:19.514708 20408 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.47G, req 0G)
I0731 21:37:19.520865 20406 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.49G, req 0G)
I0731 21:37:19.550586 20406 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.39G, req 0G)
I0731 21:37:19.635999 20406 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.900861
I0731 21:37:19.636036 20406 solver.cpp:635]     Test net output #1: accuracy/top5 = 1
I0731 21:37:19.636042 20406 solver.cpp:635]     Test net output #2: loss = 0.266714 (* 1 = 0.266714 loss)
I0731 21:37:19.636046 20406 solver.cpp:295] [MultiGPU] Initial Test completed
I0731 21:37:19.730851 20406 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 6.19G, req 0G)
I0731 21:37:19.735430 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 6.27G, req 0G)
I0731 21:37:19.736721 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 6.27G, req 0G)
I0731 21:37:19.784109 20406 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 6.03G, req 0G)
I0731 21:37:19.789625 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 6.11G, req 0G)
I0731 21:37:19.796519 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 6.11G, req 0G)
I0731 21:37:19.836803 20406 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 5.85G, req 0G)
I0731 21:37:19.843951 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 5.93G, req 0G)
I0731 21:37:19.847156 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 5.93G, req 0G)
I0731 21:37:19.861817 20406 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 5.77G, req 0G)
I0731 21:37:19.870528 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 5.85G, req 0G)
I0731 21:37:19.872687 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 5.85G, req 0G)
I0731 21:37:19.885849 20406 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 5.68G, req 0G)
I0731 21:37:19.896317 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 5.76G, req 0G)
I0731 21:37:19.899260 20406 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 5.64G, req 0G)
I0731 21:37:19.900467 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 5.76G, req 0G)
I0731 21:37:19.911617 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 5.72G, req 0G)
I0731 21:37:19.917472 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 5.72G, req 0G)
I0731 21:37:19.923041 20406 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 5.61G, req 0G)
I0731 21:37:19.931219 20406 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 5.59G, req 0G)
I0731 21:37:19.935911 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 5.69G, req 0G)
I0731 21:37:19.949374 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 5.69G, req 0G)
I0731 21:37:19.952692 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 5.67G, req 0G)
I0731 21:37:19.959426 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 5.67G, req 0G)
I0731 21:37:19.980346 20406 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 5.32G, req 0G)
I0731 21:37:19.994748 20406 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 5.3G, req 0G)
I0731 21:37:19.999917 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 5.4G, req 0G)
I0731 21:37:20.005347 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 5.4G, req 0G)
I0731 21:37:20.015234 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 5.38G, req 0G)
I0731 21:37:20.020908 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 5.38G, req 0G)
I0731 21:37:20.023792 20406 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 5.14G, req 0G)
I0731 21:37:20.049309 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 5.23G, req 0G)
I0731 21:37:20.055439 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 5.23G, req 0G)
I0731 21:37:20.268901 20406 solver.cpp:358] Iteration 0 (0.6328 s), loss = 0.0844328
I0731 21:37:20.268951 20406 solver.cpp:375]     Train net output #0: loss = 0.0844328 (* 1 = 0.0844328 loss)
I0731 21:37:20.268963 20406 sgd_solver.cpp:136] Iteration 0, lr = 1e-05, m = 0.9
I0731 21:37:20.457412 20406 solver.cpp:358] Iteration 1 (0.188507 s), loss = 0.105994
I0731 21:37:20.457438 20406 solver.cpp:375]     Train net output #0: loss = 0.105994 (* 1 = 0.105994 loss)
I0731 21:37:20.547955 20406 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1a' with space 1.29G/1 1 0 3  (limit 2.98G, req 0G)
I0731 21:37:20.563175 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1a' with space 1.29G/1 1 0 3  (limit 3.06G, req 0G)
I0731 21:37:20.563724 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1a' with space 1.29G/1 1 0 3  (limit 3.06G, req 0G)
I0731 21:37:20.613157 20406 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1b' with space 2.57G/2 6 4 3  (limit 1.69G, req 0G)
I0731 21:37:20.638814 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1b' with space 2.57G/2 6 4 3  (limit 1.78G, req 0G)
I0731 21:37:20.641772 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1b' with space 2.57G/2 6 4 3  (limit 1.78G, req 0G)
I0731 21:37:20.745617 20406 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 2.57G/1 1 4 3  (limit 1.69G, req 0G)
I0731 21:37:20.783813 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 2.57G/1 6 4 3  (limit 1.78G, req 0G)
I0731 21:37:20.784180 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 2.57G/1 6 4 3  (limit 1.78G, req 0G)
I0731 21:37:20.794327 20406 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 2.57G/2 6 4 3  (limit 1.69G, req 0G)
I0731 21:37:20.835959 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 2.57G/2 6 4 3  (limit 1.78G, req 0G)
I0731 21:37:20.838064 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 2.57G/2 6 4 3  (limit 1.78G, req 0G)
I0731 21:37:20.890305 20406 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 2.57G/1 6 4 5  (limit 1.69G, req 0.07G)
I0731 21:37:20.913028 20406 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 2.57G/2 6 4 3  (limit 1.69G, req 0.07G)
I0731 21:37:20.936583 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 2.57G/1 6 4 5  (limit 1.78G, req 0.07G)
I0731 21:37:20.940023 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 2.57G/1 6 4 5  (limit 1.78G, req 0.07G)
I0731 21:37:20.960263 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 2.57G/2 6 4 3  (limit 1.78G, req 0.07G)
I0731 21:37:20.963724 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 2.57G/2 6 4 3  (limit 1.78G, req 0.07G)
I0731 21:37:20.978600 20406 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 2.57G/1 6 4 5  (limit 1.69G, req 0.07G)
I0731 21:37:20.996453 20406 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 2.57G/2 1 4 5  (limit 1.69G, req 0.07G)
I0731 21:37:21.032729 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 2.57G/1 6 4 5  (limit 1.78G, req 0.07G)
I0731 21:37:21.036015 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 2.57G/1 6 4 5  (limit 1.78G, req 0.07G)
I0731 21:37:21.048055 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 2.57G/2 6 4 3  (limit 1.78G, req 0.07G)
I0731 21:37:21.051266 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 2.57G/2 6 4 3  (limit 1.78G, req 0.07G)
I0731 21:37:21.053794 20406 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'out3a' with space 2.57G/2 6 4 3  (limit 1.69G, req 0.07G)
I0731 21:37:21.103407 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'out3a' with space 2.57G/2 6 4 3  (limit 1.78G, req 0.07G)
I0731 21:37:21.106115 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'out3a' with space 2.57G/2 6 4 3  (limit 1.78G, req 0.07G)
I0731 21:37:21.107122 20406 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 2.57G/1 6 4 3  (limit 1.69G, req 0.07G)
I0731 21:37:21.133007 20406 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 2.57G/1 6 1 5  (limit 1.69G, req 0.07G)
I0731 21:37:21.160446 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 2.57G/1 6 4 3  (limit 1.78G, req 0.07G)
I0731 21:37:21.162037 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 2.57G/1 6 4 3  (limit 1.78G, req 0.07G)
I0731 21:37:21.188170 20408 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 2.57G/1 6 1 5  (limit 1.78G, req 0.07G)
I0731 21:37:21.190227 20407 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 2.57G/1 6 1 5  (limit 1.78G, req 0.07G)
I0731 21:37:21.319000 20406 solver.cpp:358] Iteration 2 (0.861559 s), loss = 0.0655569
I0731 21:37:21.319022 20406 solver.cpp:375]     Train net output #0: loss = 0.0655569 (* 1 = 0.0655569 loss)
I0731 21:37:21.319627 20407 cudnn_conv_layer.cpp:292] [1] Layer 'conv1a' reallocating workspace: 2.57G -> 0.14G
I0731 21:37:21.319633 20408 cudnn_conv_layer.cpp:292] [2] Layer 'conv1a' reallocating workspace: 2.57G -> 0.14G
I0731 21:37:21.319653 20406 cudnn_conv_layer.cpp:292] [0] Layer 'conv1a' reallocating workspace: 2.57G -> 0.14G
I0731 21:37:39.563769 20406 solver.cpp:353] Iteration 100 (5.37155 iter/s, 18.2443s/98 iter), loss = 0.0701501
I0731 21:37:39.563796 20406 solver.cpp:375]     Train net output #0: loss = 0.0701501 (* 1 = 0.0701501 loss)
I0731 21:37:39.563802 20406 sgd_solver.cpp:136] Iteration 100, lr = 1e-05, m = 0.9
I0731 21:37:51.142010 20412 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 21:37:58.234972 20406 solver.cpp:353] Iteration 200 (5.35599 iter/s, 18.6707s/100 iter), loss = 0.0988953
I0731 21:37:58.234998 20406 solver.cpp:375]     Train net output #0: loss = 0.0988953 (* 1 = 0.0988953 loss)
I0731 21:37:58.235003 20406 sgd_solver.cpp:136] Iteration 200, lr = 1e-05, m = 0.9
I0731 21:38:16.678007 20406 solver.cpp:353] Iteration 300 (5.42225 iter/s, 18.4425s/100 iter), loss = 0.0645709
I0731 21:38:16.678032 20406 solver.cpp:375]     Train net output #0: loss = 0.0645709 (* 1 = 0.0645709 loss)
I0731 21:38:16.678037 20406 sgd_solver.cpp:136] Iteration 300, lr = 1e-05, m = 0.9
I0731 21:38:21.708425 20346 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 21:38:35.196321 20406 solver.cpp:353] Iteration 400 (5.40021 iter/s, 18.5178s/100 iter), loss = 0.0785404
I0731 21:38:35.196346 20406 solver.cpp:375]     Train net output #0: loss = 0.0785404 (* 1 = 0.0785404 loss)
I0731 21:38:35.196350 20406 sgd_solver.cpp:136] Iteration 400, lr = 1e-05, m = 0.9
I0731 21:38:53.810977 20406 solver.cpp:353] Iteration 500 (5.37226 iter/s, 18.6141s/100 iter), loss = 0.0787777
I0731 21:38:53.811064 20406 solver.cpp:375]     Train net output #0: loss = 0.0787776 (* 1 = 0.0787776 loss)
I0731 21:38:53.811072 20406 sgd_solver.cpp:136] Iteration 500, lr = 1e-05, m = 0.9
I0731 21:39:12.550768 20406 solver.cpp:353] Iteration 600 (5.33639 iter/s, 18.7393s/100 iter), loss = 0.0653573
I0731 21:39:12.550791 20406 solver.cpp:375]     Train net output #0: loss = 0.0653572 (* 1 = 0.0653572 loss)
I0731 21:39:12.550796 20406 sgd_solver.cpp:136] Iteration 600, lr = 1e-05, m = 0.9
I0731 21:39:23.241492 20415 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 21:39:31.063150 20406 solver.cpp:353] Iteration 700 (5.40194 iter/s, 18.5119s/100 iter), loss = 0.0634848
I0731 21:39:31.063230 20406 solver.cpp:375]     Train net output #0: loss = 0.0634848 (* 1 = 0.0634848 loss)
I0731 21:39:31.063237 20406 sgd_solver.cpp:136] Iteration 700, lr = 1e-05, m = 0.9
I0731 21:39:49.444386 20406 solver.cpp:353] Iteration 800 (5.44048 iter/s, 18.3807s/100 iter), loss = 0.0783888
I0731 21:39:49.444411 20406 solver.cpp:375]     Train net output #0: loss = 0.0783888 (* 1 = 0.0783888 loss)
I0731 21:39:49.444416 20406 sgd_solver.cpp:136] Iteration 800, lr = 1e-05, m = 0.9
I0731 21:40:08.108201 20406 solver.cpp:353] Iteration 900 (5.35811 iter/s, 18.6633s/100 iter), loss = 0.0674016
I0731 21:40:08.108286 20406 solver.cpp:375]     Train net output #0: loss = 0.0674016 (* 1 = 0.0674016 loss)
I0731 21:40:08.108294 20406 sgd_solver.cpp:136] Iteration 900, lr = 1e-05, m = 0.9
I0731 21:40:24.653353 20412 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 21:40:26.651243 20406 solver.cpp:404] Sparsity after update:
I0731 21:40:26.679087 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 21:40:26.679116 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 21:40:26.679136 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 21:40:26.679139 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 21:40:26.679142 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 21:40:26.679144 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 21:40:26.679149 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 21:40:26.679162 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 21:40:26.679164 20406 net.cpp:2270] out3a_param_0(0) 
I0731 21:40:26.679167 20406 net.cpp:2270] out5a_param_0(0) 
I0731 21:40:26.679169 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 21:40:26.679173 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 21:40:26.679178 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 21:40:26.679184 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 21:40:26.679190 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 21:40:26.679194 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 21:40:26.679198 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 21:40:26.679201 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 21:40:26.679205 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 21:40:26.848171 20406 solver.cpp:353] Iteration 1000 (5.33634 iter/s, 18.7395s/100 iter), loss = 0.0937093
I0731 21:40:26.848196 20406 solver.cpp:375]     Train net output #0: loss = 0.0937093 (* 1 = 0.0937093 loss)
I0731 21:40:26.848202 20406 sgd_solver.cpp:136] Iteration 1000, lr = 1e-05, m = 0.9
I0731 21:40:45.409927 20406 solver.cpp:353] Iteration 1100 (5.38757 iter/s, 18.5612s/100 iter), loss = 0.0610806
I0731 21:40:45.409979 20406 solver.cpp:375]     Train net output #0: loss = 0.0610805 (* 1 = 0.0610805 loss)
I0731 21:40:45.409984 20406 sgd_solver.cpp:136] Iteration 1100, lr = 1e-05, m = 0.9
I0731 21:40:55.259168 20346 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 21:41:03.978263 20406 solver.cpp:353] Iteration 1200 (5.38566 iter/s, 18.5678s/100 iter), loss = 0.0726554
I0731 21:41:03.978288 20406 solver.cpp:375]     Train net output #0: loss = 0.0726554 (* 1 = 0.0726554 loss)
I0731 21:41:03.978292 20406 sgd_solver.cpp:136] Iteration 1200, lr = 1e-05, m = 0.9
I0731 21:41:22.482134 20406 solver.cpp:353] Iteration 1300 (5.40442 iter/s, 18.5034s/100 iter), loss = 0.0864815
I0731 21:41:22.482218 20406 solver.cpp:375]     Train net output #0: loss = 0.0864814 (* 1 = 0.0864814 loss)
I0731 21:41:22.482226 20406 sgd_solver.cpp:136] Iteration 1300, lr = 1e-05, m = 0.9
I0731 21:41:41.052461 20406 solver.cpp:353] Iteration 1400 (5.38508 iter/s, 18.5698s/100 iter), loss = 0.106898
I0731 21:41:41.052484 20406 solver.cpp:375]     Train net output #0: loss = 0.106898 (* 1 = 0.106898 loss)
I0731 21:41:41.052489 20406 sgd_solver.cpp:136] Iteration 1400, lr = 1e-05, m = 0.9
I0731 21:41:56.657925 20409 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 21:41:59.542181 20406 solver.cpp:353] Iteration 1500 (5.40856 iter/s, 18.4892s/100 iter), loss = 0.0855621
I0731 21:41:59.542207 20406 solver.cpp:375]     Train net output #0: loss = 0.0855621 (* 1 = 0.0855621 loss)
I0731 21:41:59.542212 20406 sgd_solver.cpp:136] Iteration 1500, lr = 1e-05, m = 0.9
I0731 21:42:18.021134 20406 solver.cpp:353] Iteration 1600 (5.41171 iter/s, 18.4784s/100 iter), loss = 0.0664962
I0731 21:42:18.021159 20406 solver.cpp:375]     Train net output #0: loss = 0.0664961 (* 1 = 0.0664961 loss)
I0731 21:42:18.021163 20406 sgd_solver.cpp:136] Iteration 1600, lr = 1e-05, m = 0.9
I0731 21:42:36.526711 20406 solver.cpp:353] Iteration 1700 (5.40392 iter/s, 18.5051s/100 iter), loss = 0.0906552
I0731 21:42:36.526770 20406 solver.cpp:375]     Train net output #0: loss = 0.0906552 (* 1 = 0.0906552 loss)
I0731 21:42:36.526777 20406 sgd_solver.cpp:136] Iteration 1700, lr = 1e-05, m = 0.9
I0731 21:42:55.183761 20406 solver.cpp:353] Iteration 1800 (5.36005 iter/s, 18.6565s/100 iter), loss = 0.0591703
I0731 21:42:55.183789 20406 solver.cpp:375]     Train net output #0: loss = 0.0591702 (* 1 = 0.0591702 loss)
I0731 21:42:55.183795 20406 sgd_solver.cpp:136] Iteration 1800, lr = 1e-05, m = 0.9
I0731 21:42:57.993700 20364 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 21:43:13.772389 20406 solver.cpp:353] Iteration 1900 (5.37978 iter/s, 18.5881s/100 iter), loss = 0.0506905
I0731 21:43:13.772470 20406 solver.cpp:375]     Train net output #0: loss = 0.0506904 (* 1 = 0.0506904 loss)
I0731 21:43:13.772475 20406 sgd_solver.cpp:136] Iteration 1900, lr = 1e-05, m = 0.9
I0731 21:43:32.105382 20406 solver.cpp:404] Sparsity after update:
I0731 21:43:32.120806 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 21:43:32.120838 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 21:43:32.120847 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 21:43:32.120851 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 21:43:32.120856 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 21:43:32.120859 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 21:43:32.120862 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 21:43:32.120867 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 21:43:32.120869 20406 net.cpp:2270] out3a_param_0(0) 
I0731 21:43:32.120873 20406 net.cpp:2270] out5a_param_0(0) 
I0731 21:43:32.120882 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 21:43:32.120885 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 21:43:32.120888 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 21:43:32.120893 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 21:43:32.120895 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 21:43:32.120899 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 21:43:32.120903 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 21:43:32.120906 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 21:43:32.120909 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 21:43:32.120920 20406 solver.cpp:550] Iteration 2000, Testing net (#0)
I0731 21:43:33.387984 20406 blocking_queue.cpp:40] Data layer prefetch queue empty
I0731 21:43:39.108537 20404 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 21:43:43.264416 20406 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.952441
I0731 21:43:43.264441 20406 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999476
I0731 21:43:43.264446 20406 solver.cpp:635]     Test net output #2: loss = 0.189906 (* 1 = 0.189906 loss)
I0731 21:43:43.264533 20406 solver.cpp:305] [MultiGPU] Tests completed in 11.1433s
I0731 21:43:43.461644 20406 solver.cpp:353] Iteration 2000 (3.36831 iter/s, 29.6884s/100 iter), loss = 0.0578856
I0731 21:43:43.461671 20406 solver.cpp:375]     Train net output #0: loss = 0.0578856 (* 1 = 0.0578856 loss)
I0731 21:43:43.461679 20406 sgd_solver.cpp:136] Iteration 2000, lr = 1e-05, m = 0.9
I0731 21:44:02.015105 20406 solver.cpp:353] Iteration 2100 (5.38998 iter/s, 18.5529s/100 iter), loss = 0.067486
I0731 21:44:02.015177 20406 solver.cpp:375]     Train net output #0: loss = 0.067486 (* 1 = 0.067486 loss)
I0731 21:44:02.015182 20406 sgd_solver.cpp:136] Iteration 2100, lr = 1e-05, m = 0.9
I0731 21:44:10.427899 20409 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 21:44:20.628748 20406 solver.cpp:353] Iteration 2200 (5.37255 iter/s, 18.6131s/100 iter), loss = 0.120577
I0731 21:44:20.628773 20406 solver.cpp:375]     Train net output #0: loss = 0.120577 (* 1 = 0.120577 loss)
I0731 21:44:20.628777 20406 sgd_solver.cpp:136] Iteration 2200, lr = 1e-05, m = 0.9
I0731 21:44:39.139827 20406 solver.cpp:353] Iteration 2300 (5.40232 iter/s, 18.5106s/100 iter), loss = 0.0680242
I0731 21:44:39.148630 20406 solver.cpp:375]     Train net output #0: loss = 0.0680242 (* 1 = 0.0680242 loss)
I0731 21:44:39.148811 20406 sgd_solver.cpp:136] Iteration 2300, lr = 1e-05, m = 0.9
I0731 21:44:57.724230 20406 solver.cpp:353] Iteration 2400 (5.381 iter/s, 18.5839s/100 iter), loss = 0.0602255
I0731 21:44:57.724263 20406 solver.cpp:375]     Train net output #0: loss = 0.0602255 (* 1 = 0.0602255 loss)
I0731 21:44:57.724268 20406 sgd_solver.cpp:136] Iteration 2400, lr = 1e-05, m = 0.9
I0731 21:45:11.780216 20409 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 21:45:16.209568 20406 solver.cpp:353] Iteration 2500 (5.40984 iter/s, 18.4848s/100 iter), loss = 0.0849564
I0731 21:45:16.209592 20406 solver.cpp:375]     Train net output #0: loss = 0.0849564 (* 1 = 0.0849564 loss)
I0731 21:45:16.209596 20406 sgd_solver.cpp:136] Iteration 2500, lr = 1e-05, m = 0.9
I0731 21:45:34.908954 20406 solver.cpp:353] Iteration 2600 (5.34792 iter/s, 18.6989s/100 iter), loss = 0.0769499
I0731 21:45:34.908983 20406 solver.cpp:375]     Train net output #0: loss = 0.0769499 (* 1 = 0.0769499 loss)
I0731 21:45:34.908987 20406 sgd_solver.cpp:136] Iteration 2600, lr = 1e-05, m = 0.9
I0731 21:45:53.592036 20406 solver.cpp:353] Iteration 2700 (5.35258 iter/s, 18.6826s/100 iter), loss = 0.0661474
I0731 21:45:53.592090 20406 solver.cpp:375]     Train net output #0: loss = 0.0661474 (* 1 = 0.0661474 loss)
I0731 21:45:53.592097 20406 sgd_solver.cpp:136] Iteration 2700, lr = 1e-05, m = 0.9
I0731 21:46:12.182863 20406 solver.cpp:353] Iteration 2800 (5.37915 iter/s, 18.5903s/100 iter), loss = 0.0604884
I0731 21:46:12.182886 20406 solver.cpp:375]     Train net output #0: loss = 0.0604884 (* 1 = 0.0604884 loss)
I0731 21:46:12.182890 20406 sgd_solver.cpp:136] Iteration 2800, lr = 1e-05, m = 0.9
I0731 21:46:13.286644 20364 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 21:46:30.717608 20406 solver.cpp:353] Iteration 2900 (5.39542 iter/s, 18.5342s/100 iter), loss = 0.0828159
I0731 21:46:30.717663 20406 solver.cpp:375]     Train net output #0: loss = 0.0828159 (* 1 = 0.0828159 loss)
I0731 21:46:30.717667 20406 sgd_solver.cpp:136] Iteration 2900, lr = 1e-05, m = 0.9
I0731 21:46:43.870417 20346 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 21:46:48.969892 20406 solver.cpp:404] Sparsity after update:
I0731 21:46:48.986460 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 21:46:48.986508 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 21:46:48.986526 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 21:46:48.986529 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 21:46:48.986532 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 21:46:48.986536 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 21:46:48.986538 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 21:46:48.986541 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 21:46:48.986544 20406 net.cpp:2270] out3a_param_0(0) 
I0731 21:46:48.986546 20406 net.cpp:2270] out5a_param_0(0) 
I0731 21:46:48.986549 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 21:46:48.986552 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 21:46:48.986554 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 21:46:48.986558 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 21:46:48.986596 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 21:46:48.986608 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 21:46:48.986618 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 21:46:48.986625 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 21:46:48.986634 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 21:46:49.162711 20406 solver.cpp:353] Iteration 3000 (5.42164 iter/s, 18.4446s/100 iter), loss = 0.0867729
I0731 21:46:49.162739 20406 solver.cpp:375]     Train net output #0: loss = 0.0867729 (* 1 = 0.0867729 loss)
I0731 21:46:49.162746 20406 sgd_solver.cpp:136] Iteration 3000, lr = 1e-05, m = 0.9
I0731 21:47:07.838109 20406 solver.cpp:353] Iteration 3100 (5.35479 iter/s, 18.6749s/100 iter), loss = 0.042687
I0731 21:47:07.838197 20406 solver.cpp:375]     Train net output #0: loss = 0.042687 (* 1 = 0.042687 loss)
I0731 21:47:07.838207 20406 sgd_solver.cpp:136] Iteration 3100, lr = 1e-05, m = 0.9
I0731 21:47:26.493017 20406 solver.cpp:353] Iteration 3200 (5.36067 iter/s, 18.6544s/100 iter), loss = 0.122004
I0731 21:47:26.493044 20406 solver.cpp:375]     Train net output #0: loss = 0.122004 (* 1 = 0.122004 loss)
I0731 21:47:26.493050 20406 sgd_solver.cpp:136] Iteration 3200, lr = 1e-05, m = 0.9
I0731 21:47:45.015565 20406 solver.cpp:353] Iteration 3300 (5.39897 iter/s, 18.522s/100 iter), loss = 0.0614537
I0731 21:47:45.015625 20406 solver.cpp:375]     Train net output #0: loss = 0.0614537 (* 1 = 0.0614537 loss)
I0731 21:47:45.015630 20406 sgd_solver.cpp:136] Iteration 3300, lr = 1e-05, m = 0.9
I0731 21:47:45.416330 20412 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 21:48:03.574316 20406 solver.cpp:353] Iteration 3400 (5.38844 iter/s, 18.5582s/100 iter), loss = 0.0442674
I0731 21:48:03.574339 20406 solver.cpp:375]     Train net output #0: loss = 0.0442674 (* 1 = 0.0442674 loss)
I0731 21:48:03.574344 20406 sgd_solver.cpp:136] Iteration 3400, lr = 1e-05, m = 0.9
I0731 21:48:22.069855 20406 solver.cpp:353] Iteration 3500 (5.40686 iter/s, 18.495s/100 iter), loss = 0.0757211
I0731 21:48:22.069907 20406 solver.cpp:375]     Train net output #0: loss = 0.0757211 (* 1 = 0.0757211 loss)
I0731 21:48:22.069912 20406 sgd_solver.cpp:136] Iteration 3500, lr = 1e-05, m = 0.9
I0731 21:48:40.623047 20406 solver.cpp:353] Iteration 3600 (5.39006 iter/s, 18.5527s/100 iter), loss = 0.0711497
I0731 21:48:40.623073 20406 solver.cpp:375]     Train net output #0: loss = 0.0711497 (* 1 = 0.0711497 loss)
I0731 21:48:40.623080 20406 sgd_solver.cpp:136] Iteration 3600, lr = 1e-05, m = 0.9
I0731 21:48:46.736836 20412 data_reader.cpp:264] Starting prefetch of epoch 4
I0731 21:48:59.188287 20406 solver.cpp:353] Iteration 3700 (5.38656 iter/s, 18.5647s/100 iter), loss = 0.0752344
I0731 21:48:59.188344 20406 solver.cpp:375]     Train net output #0: loss = 0.0752344 (* 1 = 0.0752344 loss)
I0731 21:48:59.188349 20406 sgd_solver.cpp:136] Iteration 3700, lr = 1e-05, m = 0.9
I0731 21:49:17.256353 20410 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 21:49:17.594110 20406 solver.cpp:353] Iteration 3800 (5.43321 iter/s, 18.4053s/100 iter), loss = 0.078594
I0731 21:49:17.594136 20406 solver.cpp:375]     Train net output #0: loss = 0.078594 (* 1 = 0.078594 loss)
I0731 21:49:17.594141 20406 sgd_solver.cpp:136] Iteration 3800, lr = 1e-05, m = 0.9
I0731 21:49:37.080976 20406 solver.cpp:353] Iteration 3900 (5.1318 iter/s, 19.4863s/100 iter), loss = 0.0646251
I0731 21:49:37.081033 20406 solver.cpp:375]     Train net output #0: loss = 0.0646251 (* 1 = 0.0646251 loss)
I0731 21:49:37.081038 20406 sgd_solver.cpp:136] Iteration 3900, lr = 1e-05, m = 0.9
I0731 21:49:55.430253 20406 solver.cpp:404] Sparsity after update:
I0731 21:49:55.441603 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 21:49:55.441622 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 21:49:55.441627 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 21:49:55.441629 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 21:49:55.441632 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 21:49:55.441633 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 21:49:55.441635 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 21:49:55.441637 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 21:49:55.441639 20406 net.cpp:2270] out3a_param_0(0) 
I0731 21:49:55.441642 20406 net.cpp:2270] out5a_param_0(0) 
I0731 21:49:55.441642 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 21:49:55.441644 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 21:49:55.441646 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 21:49:55.441648 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 21:49:55.441651 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 21:49:55.441653 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 21:49:55.441656 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 21:49:55.441658 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 21:49:55.441660 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 21:49:55.441669 20406 solver.cpp:550] Iteration 4000, Testing net (#0)
I0731 21:49:58.852716 20436 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 21:50:06.984479 20406 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.951996
I0731 21:50:06.984498 20406 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999815
I0731 21:50:06.984504 20406 solver.cpp:635]     Test net output #2: loss = 0.165038 (* 1 = 0.165038 loss)
I0731 21:50:06.984532 20406 solver.cpp:305] [MultiGPU] Tests completed in 11.5425s
I0731 21:50:07.194308 20406 solver.cpp:353] Iteration 4000 (3.32088 iter/s, 30.1125s/100 iter), loss = 0.0569631
I0731 21:50:07.194388 20406 solver.cpp:375]     Train net output #0: loss = 0.0569631 (* 1 = 0.0569631 loss)
I0731 21:50:07.194396 20406 sgd_solver.cpp:136] Iteration 4000, lr = 1e-05, m = 0.9
I0731 21:50:25.932782 20406 solver.cpp:353] Iteration 4100 (5.33676 iter/s, 18.738s/100 iter), loss = 0.0895072
I0731 21:50:25.932811 20406 solver.cpp:375]     Train net output #0: loss = 0.0895072 (* 1 = 0.0895072 loss)
I0731 21:50:25.932821 20406 sgd_solver.cpp:136] Iteration 4100, lr = 1e-05, m = 0.9
I0731 21:50:31.149884 20412 data_reader.cpp:264] Starting prefetch of epoch 5
I0731 21:50:44.486593 20406 solver.cpp:353] Iteration 4200 (5.38988 iter/s, 18.5533s/100 iter), loss = 0.0702297
I0731 21:50:44.486649 20406 solver.cpp:375]     Train net output #0: loss = 0.0702297 (* 1 = 0.0702297 loss)
I0731 21:50:44.486654 20406 sgd_solver.cpp:136] Iteration 4200, lr = 1e-05, m = 0.9
I0731 21:51:01.828342 20409 data_reader.cpp:264] Starting prefetch of epoch 4
I0731 21:51:02.913709 20406 solver.cpp:353] Iteration 4300 (5.42694 iter/s, 18.4266s/100 iter), loss = 0.0939048
I0731 21:51:02.913735 20406 solver.cpp:375]     Train net output #0: loss = 0.0939048 (* 1 = 0.0939048 loss)
I0731 21:51:02.913739 20406 sgd_solver.cpp:136] Iteration 4300, lr = 1e-05, m = 0.9
I0731 21:51:21.705718 20406 solver.cpp:353] Iteration 4400 (5.32156 iter/s, 18.7915s/100 iter), loss = 0.0542645
I0731 21:51:21.705768 20406 solver.cpp:375]     Train net output #0: loss = 0.0542645 (* 1 = 0.0542645 loss)
I0731 21:51:21.705773 20406 sgd_solver.cpp:136] Iteration 4400, lr = 1e-05, m = 0.9
I0731 21:51:40.295115 20406 solver.cpp:353] Iteration 4500 (5.37956 iter/s, 18.5889s/100 iter), loss = 0.0463979
I0731 21:51:40.295140 20406 solver.cpp:375]     Train net output #0: loss = 0.0463979 (* 1 = 0.0463979 loss)
I0731 21:51:40.295145 20406 sgd_solver.cpp:136] Iteration 4500, lr = 1e-05, m = 0.9
I0731 21:51:58.650795 20406 solver.cpp:353] Iteration 4600 (5.44806 iter/s, 18.3552s/100 iter), loss = 0.106224
I0731 21:51:58.650846 20406 solver.cpp:375]     Train net output #0: loss = 0.106224 (* 1 = 0.106224 loss)
I0731 21:51:58.650851 20406 sgd_solver.cpp:136] Iteration 4600, lr = 1e-05, m = 0.9
I0731 21:52:03.071838 20415 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 21:52:17.075870 20406 solver.cpp:353] Iteration 4700 (5.42754 iter/s, 18.4246s/100 iter), loss = 0.0710359
I0731 21:52:17.075897 20406 solver.cpp:375]     Train net output #0: loss = 0.0710359 (* 1 = 0.0710359 loss)
I0731 21:52:17.075902 20406 sgd_solver.cpp:136] Iteration 4700, lr = 1e-05, m = 0.9
I0731 21:52:35.619675 20406 solver.cpp:353] Iteration 4800 (5.39279 iter/s, 18.5433s/100 iter), loss = 0.0751191
I0731 21:52:35.619797 20406 solver.cpp:375]     Train net output #0: loss = 0.0751191 (* 1 = 0.0751191 loss)
I0731 21:52:35.619803 20406 sgd_solver.cpp:136] Iteration 4800, lr = 1e-05, m = 0.9
I0731 21:52:54.262728 20406 solver.cpp:353] Iteration 4900 (5.36408 iter/s, 18.6425s/100 iter), loss = 0.0706066
I0731 21:52:54.262753 20406 solver.cpp:375]     Train net output #0: loss = 0.0706066 (* 1 = 0.0706066 loss)
I0731 21:52:54.262758 20406 sgd_solver.cpp:136] Iteration 4900, lr = 1e-05, m = 0.9
I0731 21:53:04.551415 20364 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 21:53:13.067399 20406 solver.cpp:404] Sparsity after update:
I0731 21:53:13.101663 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 21:53:13.101707 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 21:53:13.101722 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 21:53:13.101725 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 21:53:13.101728 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 21:53:13.101733 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 21:53:13.101737 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 21:53:13.101739 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 21:53:13.101742 20406 net.cpp:2270] out3a_param_0(0) 
I0731 21:53:13.101745 20406 net.cpp:2270] out5a_param_0(0) 
I0731 21:53:13.101747 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 21:53:13.101752 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 21:53:13.101754 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 21:53:13.101758 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 21:53:13.101761 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 21:53:13.101764 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 21:53:13.101768 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 21:53:13.101771 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 21:53:13.101775 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 21:53:13.272439 20406 solver.cpp:353] Iteration 5000 (5.26062 iter/s, 19.0092s/100 iter), loss = 0.0778511
I0731 21:53:13.272461 20406 solver.cpp:375]     Train net output #0: loss = 0.0778511 (* 1 = 0.0778511 loss)
I0731 21:53:13.272465 20406 sgd_solver.cpp:136] Iteration 5000, lr = 1e-05, m = 0.9
I0731 21:53:32.827229 20406 solver.cpp:353] Iteration 5100 (5.11398 iter/s, 19.5543s/100 iter), loss = 0.111352
I0731 21:53:32.827256 20406 solver.cpp:375]     Train net output #0: loss = 0.111352 (* 1 = 0.111352 loss)
I0731 21:53:32.827260 20406 sgd_solver.cpp:136] Iteration 5100, lr = 1e-05, m = 0.9
I0731 21:53:36.776298 20346 data_reader.cpp:264] Starting prefetch of epoch 4
I0731 21:53:52.396360 20406 solver.cpp:353] Iteration 5200 (5.11023 iter/s, 19.5686s/100 iter), loss = 0.078714
I0731 21:53:52.396423 20406 solver.cpp:375]     Train net output #0: loss = 0.078714 (* 1 = 0.078714 loss)
I0731 21:53:52.396430 20406 sgd_solver.cpp:136] Iteration 5200, lr = 1e-05, m = 0.9
I0731 21:54:11.641379 20406 solver.cpp:353] Iteration 5300 (5.19629 iter/s, 19.2445s/100 iter), loss = 0.0496281
I0731 21:54:11.641402 20406 solver.cpp:375]     Train net output #0: loss = 0.0496281 (* 1 = 0.0496281 loss)
I0731 21:54:11.641407 20406 sgd_solver.cpp:136] Iteration 5300, lr = 1e-05, m = 0.9
I0731 21:54:30.878854 20406 solver.cpp:353] Iteration 5400 (5.19833 iter/s, 19.2369s/100 iter), loss = 0.0563819
I0731 21:54:30.878914 20406 solver.cpp:375]     Train net output #0: loss = 0.0563819 (* 1 = 0.0563819 loss)
I0731 21:54:30.878919 20406 sgd_solver.cpp:136] Iteration 5400, lr = 1e-05, m = 0.9
I0731 21:54:40.893900 20364 data_reader.cpp:264] Starting prefetch of epoch 4
I0731 21:54:50.300691 20406 solver.cpp:353] Iteration 5500 (5.14899 iter/s, 19.4213s/100 iter), loss = 0.0743792
I0731 21:54:50.300720 20406 solver.cpp:375]     Train net output #0: loss = 0.0743792 (* 1 = 0.0743792 loss)
I0731 21:54:50.300727 20406 sgd_solver.cpp:136] Iteration 5500, lr = 1e-05, m = 0.9
I0731 21:55:09.272109 20406 solver.cpp:353] Iteration 5600 (5.27123 iter/s, 18.9709s/100 iter), loss = 0.0368734
I0731 21:55:09.272195 20406 solver.cpp:375]     Train net output #0: loss = 0.0368734 (* 1 = 0.0368734 loss)
I0731 21:55:09.272203 20406 sgd_solver.cpp:136] Iteration 5600, lr = 1e-05, m = 0.9
I0731 21:55:28.394176 20406 solver.cpp:353] Iteration 5700 (5.2297 iter/s, 19.1215s/100 iter), loss = 0.080994
I0731 21:55:28.394206 20406 solver.cpp:375]     Train net output #0: loss = 0.0809939 (* 1 = 0.0809939 loss)
I0731 21:55:28.394209 20406 sgd_solver.cpp:136] Iteration 5700, lr = 1e-05, m = 0.9
I0731 21:55:43.991722 20412 data_reader.cpp:264] Starting prefetch of epoch 6
I0731 21:55:47.614729 20406 solver.cpp:353] Iteration 5800 (5.20291 iter/s, 19.22s/100 iter), loss = 0.0724273
I0731 21:55:47.614754 20406 solver.cpp:375]     Train net output #0: loss = 0.0724272 (* 1 = 0.0724272 loss)
I0731 21:55:47.614759 20406 sgd_solver.cpp:136] Iteration 5800, lr = 1e-05, m = 0.9
I0731 21:56:07.083497 20406 solver.cpp:353] Iteration 5900 (5.13658 iter/s, 19.4682s/100 iter), loss = 0.0636804
I0731 21:56:07.083539 20406 solver.cpp:375]     Train net output #0: loss = 0.0636804 (* 1 = 0.0636804 loss)
I0731 21:56:07.083547 20406 sgd_solver.cpp:136] Iteration 5900, lr = 1e-05, m = 0.9
I0731 21:56:16.203260 20346 data_reader.cpp:264] Starting prefetch of epoch 5
I0731 21:56:26.232123 20406 solver.cpp:404] Sparsity after update:
I0731 21:56:26.237597 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 21:56:26.237615 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 21:56:26.237625 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 21:56:26.237628 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 21:56:26.237632 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 21:56:26.237634 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 21:56:26.237637 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 21:56:26.237645 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 21:56:26.237651 20406 net.cpp:2270] out3a_param_0(0) 
I0731 21:56:26.237656 20406 net.cpp:2270] out5a_param_0(0) 
I0731 21:56:26.237661 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 21:56:26.237666 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 21:56:26.237670 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 21:56:26.237674 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 21:56:26.237679 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 21:56:26.237682 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 21:56:26.237686 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 21:56:26.237690 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 21:56:26.237692 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 21:56:26.237704 20406 solver.cpp:550] Iteration 6000, Testing net (#0)
I0731 21:56:38.814949 20406 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.95024
I0731 21:56:38.814970 20406 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999182
I0731 21:56:38.814976 20406 solver.cpp:635]     Test net output #2: loss = 0.213933 (* 1 = 0.213933 loss)
I0731 21:56:38.815003 20406 solver.cpp:305] [MultiGPU] Tests completed in 12.577s
I0731 21:56:39.010752 20406 solver.cpp:353] Iteration 6000 (3.13221 iter/s, 31.9263s/100 iter), loss = 0.0937672
I0731 21:56:39.010852 20406 solver.cpp:375]     Train net output #0: loss = 0.0937671 (* 1 = 0.0937671 loss)
I0731 21:56:39.010874 20406 sgd_solver.cpp:136] Iteration 6000, lr = 1e-05, m = 0.9
I0731 21:56:58.428895 20406 solver.cpp:353] Iteration 6100 (5.14996 iter/s, 19.4176s/100 iter), loss = 0.0725928
I0731 21:56:58.428978 20406 solver.cpp:375]     Train net output #0: loss = 0.0725928 (* 1 = 0.0725928 loss)
I0731 21:56:58.428983 20406 sgd_solver.cpp:136] Iteration 6100, lr = 1e-05, m = 0.9
I0731 21:57:00.778717 20346 data_reader.cpp:264] Starting prefetch of epoch 6
I0731 21:57:17.461510 20406 solver.cpp:353] Iteration 6200 (5.25428 iter/s, 19.0321s/100 iter), loss = 0.070183
I0731 21:57:17.461539 20406 solver.cpp:375]     Train net output #0: loss = 0.070183 (* 1 = 0.070183 loss)
I0731 21:57:17.461544 20406 sgd_solver.cpp:136] Iteration 6200, lr = 1e-05, m = 0.9
I0731 21:57:36.800494 20406 solver.cpp:353] Iteration 6300 (5.17105 iter/s, 19.3384s/100 iter), loss = 0.0700748
I0731 21:57:36.800623 20406 solver.cpp:375]     Train net output #0: loss = 0.0700748 (* 1 = 0.0700748 loss)
I0731 21:57:36.800631 20406 sgd_solver.cpp:136] Iteration 6300, lr = 1e-05, m = 0.9
I0731 21:57:55.336874 20406 solver.cpp:353] Iteration 6400 (5.39495 iter/s, 18.5359s/100 iter), loss = 0.0789097
I0731 21:57:55.336905 20406 solver.cpp:375]     Train net output #0: loss = 0.0789096 (* 1 = 0.0789096 loss)
I0731 21:57:55.336908 20406 sgd_solver.cpp:136] Iteration 6400, lr = 1e-05, m = 0.9
I0731 21:58:03.121816 20364 data_reader.cpp:264] Starting prefetch of epoch 5
I0731 21:58:13.852633 20406 solver.cpp:353] Iteration 6500 (5.40095 iter/s, 18.5152s/100 iter), loss = 0.0808699
I0731 21:58:13.852694 20406 solver.cpp:375]     Train net output #0: loss = 0.0808699 (* 1 = 0.0808699 loss)
I0731 21:58:13.852699 20406 sgd_solver.cpp:136] Iteration 6500, lr = 1e-05, m = 0.9
I0731 21:58:32.525014 20406 solver.cpp:353] Iteration 6600 (5.35565 iter/s, 18.6719s/100 iter), loss = 0.0788466
I0731 21:58:32.525054 20406 solver.cpp:375]     Train net output #0: loss = 0.0788466 (* 1 = 0.0788466 loss)
I0731 21:58:32.525063 20406 sgd_solver.cpp:136] Iteration 6600, lr = 1e-05, m = 0.9
I0731 21:58:34.031359 20409 data_reader.cpp:264] Starting prefetch of epoch 5
I0731 21:58:50.992234 20406 solver.cpp:353] Iteration 6700 (5.41515 iter/s, 18.4667s/100 iter), loss = 0.088701
I0731 21:58:50.992295 20406 solver.cpp:375]     Train net output #0: loss = 0.088701 (* 1 = 0.088701 loss)
I0731 21:58:50.992300 20406 sgd_solver.cpp:136] Iteration 6700, lr = 1e-05, m = 0.9
I0731 21:59:09.476020 20406 solver.cpp:353] Iteration 6800 (5.4103 iter/s, 18.4833s/100 iter), loss = 0.0480586
I0731 21:59:09.476045 20406 solver.cpp:375]     Train net output #0: loss = 0.0480586 (* 1 = 0.0480586 loss)
I0731 21:59:09.476050 20406 sgd_solver.cpp:136] Iteration 6800, lr = 1e-05, m = 0.9
I0731 21:59:28.011332 20406 solver.cpp:353] Iteration 6900 (5.39526 iter/s, 18.5348s/100 iter), loss = 0.0902742
I0731 21:59:28.011389 20406 solver.cpp:375]     Train net output #0: loss = 0.0902741 (* 1 = 0.0902741 loss)
I0731 21:59:28.011395 20406 sgd_solver.cpp:136] Iteration 6900, lr = 1e-05, m = 0.9
I0731 21:59:35.055663 20346 data_reader.cpp:264] Starting prefetch of epoch 7
I0731 21:59:46.304666 20406 solver.cpp:404] Sparsity after update:
I0731 21:59:46.323163 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 21:59:46.323220 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 21:59:46.323236 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 21:59:46.323240 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 21:59:46.323242 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 21:59:46.323246 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 21:59:46.323249 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 21:59:46.323252 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 21:59:46.323256 20406 net.cpp:2270] out3a_param_0(0) 
I0731 21:59:46.323258 20406 net.cpp:2270] out5a_param_0(0) 
I0731 21:59:46.323261 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 21:59:46.323264 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 21:59:46.323267 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 21:59:46.323288 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 21:59:46.323298 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 21:59:46.323305 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 21:59:46.323314 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 21:59:46.323323 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 21:59:46.323333 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 21:59:46.495177 20406 solver.cpp:353] Iteration 7000 (5.41028 iter/s, 18.4833s/100 iter), loss = 0.0949538
I0731 21:59:46.495203 20406 solver.cpp:375]     Train net output #0: loss = 0.0949538 (* 1 = 0.0949538 loss)
I0731 21:59:46.495208 20406 sgd_solver.cpp:136] Iteration 7000, lr = 1e-05, m = 0.9
I0731 22:00:04.934856 20406 solver.cpp:353] Iteration 7100 (5.42324 iter/s, 18.4392s/100 iter), loss = 0.0711184
I0731 22:00:04.934988 20406 solver.cpp:375]     Train net output #0: loss = 0.0711184 (* 1 = 0.0711184 loss)
I0731 22:00:04.934995 20406 sgd_solver.cpp:136] Iteration 7100, lr = 1e-05, m = 0.9
I0731 22:00:23.411299 20406 solver.cpp:353] Iteration 7200 (5.41245 iter/s, 18.4759s/100 iter), loss = 0.0660203
I0731 22:00:23.411329 20406 solver.cpp:375]     Train net output #0: loss = 0.0660202 (* 1 = 0.0660202 loss)
I0731 22:00:23.411332 20406 sgd_solver.cpp:136] Iteration 7200, lr = 1e-05, m = 0.9
I0731 22:00:36.178922 20412 data_reader.cpp:264] Starting prefetch of epoch 7
I0731 22:00:41.896205 20406 solver.cpp:353] Iteration 7300 (5.40997 iter/s, 18.4844s/100 iter), loss = 0.0607179
I0731 22:00:41.896235 20406 solver.cpp:375]     Train net output #0: loss = 0.0607179 (* 1 = 0.0607179 loss)
I0731 22:00:41.896244 20406 sgd_solver.cpp:136] Iteration 7300, lr = 1e-05, m = 0.9
I0731 22:01:00.498553 20406 solver.cpp:353] Iteration 7400 (5.37581 iter/s, 18.6018s/100 iter), loss = 0.0909598
I0731 22:01:00.498576 20406 solver.cpp:375]     Train net output #0: loss = 0.0909598 (* 1 = 0.0909598 loss)
I0731 22:01:00.498581 20406 sgd_solver.cpp:136] Iteration 7400, lr = 1e-05, m = 0.9
I0731 22:01:06.771844 20346 data_reader.cpp:264] Starting prefetch of epoch 8
I0731 22:01:18.948786 20406 solver.cpp:353] Iteration 7500 (5.42014 iter/s, 18.4497s/100 iter), loss = 0.0923757
I0731 22:01:18.948808 20406 solver.cpp:375]     Train net output #0: loss = 0.0923757 (* 1 = 0.0923757 loss)
I0731 22:01:18.948824 20406 sgd_solver.cpp:136] Iteration 7500, lr = 1e-05, m = 0.9
I0731 22:01:37.425094 20406 solver.cpp:353] Iteration 7600 (5.41249 iter/s, 18.4758s/100 iter), loss = 0.0605909
I0731 22:01:37.425143 20406 solver.cpp:375]     Train net output #0: loss = 0.0605908 (* 1 = 0.0605908 loss)
I0731 22:01:37.425148 20406 sgd_solver.cpp:136] Iteration 7600, lr = 1e-05, m = 0.9
I0731 22:01:56.038457 20406 solver.cpp:353] Iteration 7700 (5.37263 iter/s, 18.6129s/100 iter), loss = 0.069707
I0731 22:01:56.038483 20406 solver.cpp:375]     Train net output #0: loss = 0.069707 (* 1 = 0.069707 loss)
I0731 22:01:56.038487 20406 sgd_solver.cpp:136] Iteration 7700, lr = 1e-05, m = 0.9
I0731 22:02:08.065588 20364 data_reader.cpp:264] Starting prefetch of epoch 6
I0731 22:02:14.501930 20406 solver.cpp:353] Iteration 7800 (5.41625 iter/s, 18.463s/100 iter), loss = 0.074102
I0731 22:02:14.501957 20406 solver.cpp:375]     Train net output #0: loss = 0.0741019 (* 1 = 0.0741019 loss)
I0731 22:02:14.501962 20406 sgd_solver.cpp:136] Iteration 7800, lr = 1e-05, m = 0.9
I0731 22:02:33.304854 20406 solver.cpp:353] Iteration 7900 (5.31847 iter/s, 18.8024s/100 iter), loss = 0.0530646
I0731 22:02:33.304888 20406 solver.cpp:375]     Train net output #0: loss = 0.0530645 (* 1 = 0.0530645 loss)
I0731 22:02:33.304895 20406 sgd_solver.cpp:136] Iteration 7900, lr = 1e-05, m = 0.9
I0731 22:02:51.651887 20406 solver.cpp:404] Sparsity after update:
I0731 22:02:51.662370 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 22:02:51.662390 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 22:02:51.662398 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 22:02:51.662400 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 22:02:51.662402 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 22:02:51.662405 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 22:02:51.662406 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 22:02:51.662408 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 22:02:51.662410 20406 net.cpp:2270] out3a_param_0(0) 
I0731 22:02:51.662411 20406 net.cpp:2270] out5a_param_0(0) 
I0731 22:02:51.662413 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 22:02:51.662415 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 22:02:51.664041 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 22:02:51.664049 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 22:02:51.664054 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 22:02:51.664059 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 22:02:51.664062 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 22:02:51.664065 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 22:02:51.664068 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 22:02:51.664083 20406 solver.cpp:550] Iteration 8000, Testing net (#0)
I0731 22:02:54.894074 20402 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 22:03:02.504953 20438 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 22:03:02.848737 20406 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.95212
I0731 22:03:02.848762 20406 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999833
I0731 22:03:02.848768 20406 solver.cpp:635]     Test net output #2: loss = 0.163685 (* 1 = 0.163685 loss)
I0731 22:03:02.848848 20406 solver.cpp:305] [MultiGPU] Tests completed in 11.1845s
I0731 22:03:03.047128 20406 solver.cpp:353] Iteration 8000 (3.36231 iter/s, 29.7415s/100 iter), loss = 0.092974
I0731 22:03:03.047155 20406 solver.cpp:375]     Train net output #0: loss = 0.0929739 (* 1 = 0.0929739 loss)
I0731 22:03:03.047159 20406 sgd_solver.cpp:136] Iteration 8000, lr = 1e-05, m = 0.9
I0731 22:03:21.552589 20406 solver.cpp:353] Iteration 8100 (5.40396 iter/s, 18.505s/100 iter), loss = 0.0602612
I0731 22:03:21.552613 20406 solver.cpp:375]     Train net output #0: loss = 0.0602611 (* 1 = 0.0602611 loss)
I0731 22:03:21.552618 20406 sgd_solver.cpp:136] Iteration 8100, lr = 1e-05, m = 0.9
I0731 22:03:40.151628 20406 solver.cpp:353] Iteration 8200 (5.37677 iter/s, 18.5985s/100 iter), loss = 0.0572736
I0731 22:03:40.151701 20406 solver.cpp:375]     Train net output #0: loss = 0.0572735 (* 1 = 0.0572735 loss)
I0731 22:03:40.151710 20406 sgd_solver.cpp:136] Iteration 8200, lr = 1e-05, m = 0.9
I0731 22:03:51.375408 20409 data_reader.cpp:264] Starting prefetch of epoch 6
I0731 22:03:58.743124 20406 solver.cpp:353] Iteration 8300 (5.37895 iter/s, 18.591s/100 iter), loss = 0.110949
I0731 22:03:58.743149 20406 solver.cpp:375]     Train net output #0: loss = 0.110948 (* 1 = 0.110948 loss)
I0731 22:03:58.743152 20406 sgd_solver.cpp:136] Iteration 8300, lr = 1e-05, m = 0.9
I0731 22:04:17.272485 20406 solver.cpp:353] Iteration 8400 (5.39699 iter/s, 18.5288s/100 iter), loss = 0.063459
I0731 22:04:17.272541 20406 solver.cpp:375]     Train net output #0: loss = 0.0634589 (* 1 = 0.0634589 loss)
I0731 22:04:17.272547 20406 sgd_solver.cpp:136] Iteration 8400, lr = 1e-05, m = 0.9
I0731 22:04:35.865823 20406 solver.cpp:353] Iteration 8500 (5.37842 iter/s, 18.5928s/100 iter), loss = 0.0531844
I0731 22:04:35.865851 20406 solver.cpp:375]     Train net output #0: loss = 0.0531844 (* 1 = 0.0531844 loss)
I0731 22:04:35.865855 20406 sgd_solver.cpp:136] Iteration 8500, lr = 1e-05, m = 0.9
I0731 22:04:52.984211 20410 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 22:04:54.609482 20406 solver.cpp:353] Iteration 8600 (5.33528 iter/s, 18.7431s/100 iter), loss = 0.0927126
I0731 22:04:54.609506 20406 solver.cpp:375]     Train net output #0: loss = 0.0927126 (* 1 = 0.0927126 loss)
I0731 22:04:54.609511 20406 sgd_solver.cpp:136] Iteration 8600, lr = 1e-05, m = 0.9
I0731 22:05:13.299134 20406 solver.cpp:353] Iteration 8700 (5.3507 iter/s, 18.6891s/100 iter), loss = 0.0728258
I0731 22:05:13.299161 20406 solver.cpp:375]     Train net output #0: loss = 0.0728257 (* 1 = 0.0728257 loss)
I0731 22:05:13.299165 20406 sgd_solver.cpp:136] Iteration 8700, lr = 1e-05, m = 0.9
I0731 22:05:31.820360 20406 solver.cpp:353] Iteration 8800 (5.39936 iter/s, 18.5207s/100 iter), loss = 0.0725976
I0731 22:05:31.820416 20406 solver.cpp:375]     Train net output #0: loss = 0.0725975 (* 1 = 0.0725975 loss)
I0731 22:05:31.820420 20406 sgd_solver.cpp:136] Iteration 8800, lr = 1e-05, m = 0.9
I0731 22:05:50.536602 20406 solver.cpp:353] Iteration 8900 (5.3431 iter/s, 18.7157s/100 iter), loss = 0.070402
I0731 22:05:50.536631 20406 solver.cpp:375]     Train net output #0: loss = 0.070402 (* 1 = 0.070402 loss)
I0731 22:05:50.536636 20406 sgd_solver.cpp:136] Iteration 8900, lr = 1e-05, m = 0.9
I0731 22:05:54.485249 20364 data_reader.cpp:264] Starting prefetch of epoch 7
I0731 22:06:08.925165 20406 solver.cpp:404] Sparsity after update:
I0731 22:06:08.952764 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 22:06:08.952790 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 22:06:08.952797 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 22:06:08.952800 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 22:06:08.952801 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 22:06:08.952803 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 22:06:08.952805 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 22:06:08.952807 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 22:06:08.952808 20406 net.cpp:2270] out3a_param_0(0) 
I0731 22:06:08.952811 20406 net.cpp:2270] out5a_param_0(0) 
I0731 22:06:08.952812 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 22:06:08.952826 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 22:06:08.952828 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 22:06:08.952831 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 22:06:08.952832 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 22:06:08.952834 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 22:06:08.952836 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 22:06:08.952838 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 22:06:08.952841 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 22:06:09.122265 20406 solver.cpp:353] Iteration 9000 (5.38064 iter/s, 18.5851s/100 iter), loss = 0.0637801
I0731 22:06:09.122344 20406 solver.cpp:375]     Train net output #0: loss = 0.06378 (* 1 = 0.06378 loss)
I0731 22:06:09.122364 20406 sgd_solver.cpp:136] Iteration 9000, lr = 1e-05, m = 0.9
I0731 22:06:27.687109 20406 solver.cpp:353] Iteration 9100 (5.38667 iter/s, 18.5643s/100 iter), loss = 0.0805036
I0731 22:06:27.687132 20406 solver.cpp:375]     Train net output #0: loss = 0.0805036 (* 1 = 0.0805036 loss)
I0731 22:06:27.687136 20406 sgd_solver.cpp:136] Iteration 9100, lr = 1e-05, m = 0.9
I0731 22:06:46.344805 20406 solver.cpp:353] Iteration 9200 (5.35987 iter/s, 18.6572s/100 iter), loss = 0.061881
I0731 22:06:46.344892 20406 solver.cpp:375]     Train net output #0: loss = 0.0618809 (* 1 = 0.0618809 loss)
I0731 22:06:46.344900 20406 sgd_solver.cpp:136] Iteration 9200, lr = 1e-05, m = 0.9
I0731 22:06:56.098182 20364 data_reader.cpp:264] Starting prefetch of epoch 8
I0731 22:07:05.091732 20406 solver.cpp:353] Iteration 9300 (5.33435 iter/s, 18.7464s/100 iter), loss = 0.0722495
I0731 22:07:05.091758 20406 solver.cpp:375]     Train net output #0: loss = 0.0722494 (* 1 = 0.0722494 loss)
I0731 22:07:05.091765 20406 sgd_solver.cpp:136] Iteration 9300, lr = 1e-05, m = 0.9
I0731 22:07:23.675930 20406 solver.cpp:353] Iteration 9400 (5.38106 iter/s, 18.5837s/100 iter), loss = 0.0891466
I0731 22:07:23.675984 20406 solver.cpp:375]     Train net output #0: loss = 0.0891465 (* 1 = 0.0891465 loss)
I0731 22:07:23.675989 20406 sgd_solver.cpp:136] Iteration 9400, lr = 1e-05, m = 0.9
I0731 22:07:26.819880 20409 data_reader.cpp:264] Starting prefetch of epoch 7
I0731 22:07:42.135563 20406 solver.cpp:353] Iteration 9500 (5.41738 iter/s, 18.4591s/100 iter), loss = 0.0603324
I0731 22:07:42.135589 20406 solver.cpp:375]     Train net output #0: loss = 0.0603323 (* 1 = 0.0603323 loss)
I0731 22:07:42.135593 20406 sgd_solver.cpp:136] Iteration 9500, lr = 1e-05, m = 0.9
I0731 22:08:00.589334 20406 solver.cpp:353] Iteration 9600 (5.4191 iter/s, 18.4533s/100 iter), loss = 0.0653687
I0731 22:08:00.589395 20406 solver.cpp:375]     Train net output #0: loss = 0.0653686 (* 1 = 0.0653686 loss)
I0731 22:08:00.589401 20406 sgd_solver.cpp:136] Iteration 9600, lr = 1e-05, m = 0.9
I0731 22:08:19.111280 20406 solver.cpp:353] Iteration 9700 (5.39915 iter/s, 18.5214s/100 iter), loss = 0.0654426
I0731 22:08:19.111307 20406 solver.cpp:375]     Train net output #0: loss = 0.0654425 (* 1 = 0.0654425 loss)
I0731 22:08:19.111313 20406 sgd_solver.cpp:136] Iteration 9700, lr = 1e-05, m = 0.9
I0731 22:08:28.053586 20415 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 22:08:37.682633 20406 solver.cpp:353] Iteration 9800 (5.38479 iter/s, 18.5708s/100 iter), loss = 0.0611385
I0731 22:08:37.682705 20406 solver.cpp:375]     Train net output #0: loss = 0.0611384 (* 1 = 0.0611384 loss)
I0731 22:08:37.682711 20406 sgd_solver.cpp:136] Iteration 9800, lr = 1e-05, m = 0.9
I0731 22:08:56.794173 20406 solver.cpp:353] Iteration 9900 (5.23258 iter/s, 19.111s/100 iter), loss = 0.0677815
I0731 22:08:56.794196 20406 solver.cpp:375]     Train net output #0: loss = 0.0677814 (* 1 = 0.0677814 loss)
I0731 22:08:56.794201 20406 sgd_solver.cpp:136] Iteration 9900, lr = 1e-05, m = 0.9
I0731 22:09:15.276203 20406 solver.cpp:680] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/sparse/cityscapes5_jsegnet21v2_iter_10000.caffemodel
I0731 22:09:15.356381 20406 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/sparse/cityscapes5_jsegnet21v2_iter_10000.solverstate
I0731 22:09:15.364439 20406 solver.cpp:404] Sparsity after update:
I0731 22:09:15.370259 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 22:09:15.370326 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 22:09:15.370344 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 22:09:15.370348 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 22:09:15.370352 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 22:09:15.370357 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 22:09:15.370360 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 22:09:15.370364 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 22:09:15.370370 20406 net.cpp:2270] out3a_param_0(0) 
I0731 22:09:15.370374 20406 net.cpp:2270] out5a_param_0(0) 
I0731 22:09:15.370379 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 22:09:15.370383 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 22:09:15.370388 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 22:09:15.370391 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 22:09:15.370395 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 22:09:15.370399 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 22:09:15.370402 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 22:09:15.370406 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 22:09:15.370410 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 22:09:15.370426 20406 solver.cpp:550] Iteration 10000, Testing net (#0)
I0731 22:09:22.336493 20434 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 22:09:26.336403 20406 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.951388
I0731 22:09:26.336429 20406 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999208
I0731 22:09:26.336436 20406 solver.cpp:635]     Test net output #2: loss = 0.207556 (* 1 = 0.207556 loss)
I0731 22:09:26.336457 20406 solver.cpp:305] [MultiGPU] Tests completed in 10.9657s
I0731 22:09:26.532377 20406 solver.cpp:353] Iteration 10000 (3.36277 iter/s, 29.7374s/100 iter), loss = 0.069129
I0731 22:09:26.532402 20406 solver.cpp:375]     Train net output #0: loss = 0.069129 (* 1 = 0.069129 loss)
I0731 22:09:26.532405 20406 sgd_solver.cpp:136] Iteration 10000, lr = 1e-05, m = 0.9
I0731 22:09:41.032611 20409 data_reader.cpp:264] Starting prefetch of epoch 8
I0731 22:09:45.117739 20406 solver.cpp:353] Iteration 10100 (5.38073 iter/s, 18.5849s/100 iter), loss = 0.0670187
I0731 22:09:45.117763 20406 solver.cpp:375]     Train net output #0: loss = 0.0670186 (* 1 = 0.0670186 loss)
I0731 22:09:45.117769 20406 sgd_solver.cpp:136] Iteration 10100, lr = 1e-05, m = 0.9
I0731 22:10:03.456010 20406 solver.cpp:353] Iteration 10200 (5.45323 iter/s, 18.3378s/100 iter), loss = 0.0747474
I0731 22:10:03.456060 20406 solver.cpp:375]     Train net output #0: loss = 0.0747473 (* 1 = 0.0747473 loss)
I0731 22:10:03.456068 20406 sgd_solver.cpp:136] Iteration 10200, lr = 1e-05, m = 0.9
I0731 22:10:21.852144 20406 solver.cpp:353] Iteration 10300 (5.43608 iter/s, 18.3956s/100 iter), loss = 0.0542392
I0731 22:10:21.852174 20406 solver.cpp:375]     Train net output #0: loss = 0.0542391 (* 1 = 0.0542391 loss)
I0731 22:10:21.852177 20406 sgd_solver.cpp:136] Iteration 10300, lr = 1e-05, m = 0.9
I0731 22:10:40.361291 20406 solver.cpp:353] Iteration 10400 (5.40288 iter/s, 18.5086s/100 iter), loss = 0.0519043
I0731 22:10:40.361392 20406 solver.cpp:375]     Train net output #0: loss = 0.0519042 (* 1 = 0.0519042 loss)
I0731 22:10:40.361398 20406 sgd_solver.cpp:136] Iteration 10400, lr = 1e-05, m = 0.9
I0731 22:10:42.073048 20346 data_reader.cpp:264] Starting prefetch of epoch 9
I0731 22:10:59.120299 20406 solver.cpp:353] Iteration 10500 (5.33092 iter/s, 18.7585s/100 iter), loss = 0.0571081
I0731 22:10:59.120323 20406 solver.cpp:375]     Train net output #0: loss = 0.057108 (* 1 = 0.057108 loss)
I0731 22:10:59.120329 20406 sgd_solver.cpp:136] Iteration 10500, lr = 1e-05, m = 0.9
I0731 22:11:17.674078 20406 solver.cpp:353] Iteration 10600 (5.38989 iter/s, 18.5533s/100 iter), loss = 0.0796069
I0731 22:11:17.674154 20406 solver.cpp:375]     Train net output #0: loss = 0.0796068 (* 1 = 0.0796068 loss)
I0731 22:11:17.674159 20406 sgd_solver.cpp:136] Iteration 10600, lr = 1e-05, m = 0.9
I0731 22:11:36.133182 20406 solver.cpp:353] Iteration 10700 (5.41753 iter/s, 18.4586s/100 iter), loss = 0.0400956
I0731 22:11:36.133206 20406 solver.cpp:375]     Train net output #0: loss = 0.0400955 (* 1 = 0.0400955 loss)
I0731 22:11:36.133213 20406 sgd_solver.cpp:136] Iteration 10700, lr = 1e-05, m = 0.9
I0731 22:11:43.606480 20346 data_reader.cpp:264] Starting prefetch of epoch 10
I0731 22:11:54.689424 20406 solver.cpp:353] Iteration 10800 (5.38917 iter/s, 18.5557s/100 iter), loss = 0.0753683
I0731 22:11:54.689504 20406 solver.cpp:375]     Train net output #0: loss = 0.0753682 (* 1 = 0.0753682 loss)
I0731 22:11:54.689512 20406 sgd_solver.cpp:136] Iteration 10800, lr = 1e-05, m = 0.9
I0731 22:12:13.199458 20406 solver.cpp:353] Iteration 10900 (5.40262 iter/s, 18.5095s/100 iter), loss = 0.0453636
I0731 22:12:13.199484 20406 solver.cpp:375]     Train net output #0: loss = 0.0453635 (* 1 = 0.0453635 loss)
I0731 22:12:13.199488 20406 sgd_solver.cpp:136] Iteration 10900, lr = 1e-05, m = 0.9
I0731 22:12:31.853662 20406 solver.cpp:404] Sparsity after update:
I0731 22:12:31.869884 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 22:12:31.869912 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 22:12:31.869925 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 22:12:31.869930 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 22:12:31.869931 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 22:12:31.869935 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 22:12:31.869937 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 22:12:31.869940 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 22:12:31.869943 20406 net.cpp:2270] out3a_param_0(0) 
I0731 22:12:31.869956 20406 net.cpp:2270] out5a_param_0(0) 
I0731 22:12:31.869961 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 22:12:31.869966 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 22:12:31.869969 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 22:12:31.869974 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 22:12:31.869978 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 22:12:31.869982 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 22:12:31.869987 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 22:12:31.869990 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 22:12:31.869994 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 22:12:32.046416 20406 solver.cpp:353] Iteration 11000 (5.30604 iter/s, 18.8464s/100 iter), loss = 0.0590418
I0731 22:12:32.046445 20406 solver.cpp:375]     Train net output #0: loss = 0.0590418 (* 1 = 0.0590418 loss)
I0731 22:12:32.046452 20406 sgd_solver.cpp:136] Iteration 11000, lr = 1e-05, m = 0.9
I0731 22:12:45.105818 20364 data_reader.cpp:264] Starting prefetch of epoch 9
I0731 22:12:50.689982 20406 solver.cpp:353] Iteration 11100 (5.36393 iter/s, 18.6431s/100 iter), loss = 0.0758334
I0731 22:12:50.690006 20406 solver.cpp:375]     Train net output #0: loss = 0.0758334 (* 1 = 0.0758334 loss)
I0731 22:12:50.690011 20406 sgd_solver.cpp:136] Iteration 11100, lr = 1e-05, m = 0.9
I0731 22:13:09.305512 20406 solver.cpp:353] Iteration 11200 (5.37201 iter/s, 18.615s/100 iter), loss = 0.0704695
I0731 22:13:09.305584 20406 solver.cpp:375]     Train net output #0: loss = 0.0704695 (* 1 = 0.0704695 loss)
I0731 22:13:09.305589 20406 sgd_solver.cpp:136] Iteration 11200, lr = 1e-05, m = 0.9
I0731 22:13:15.813253 20346 data_reader.cpp:264] Starting prefetch of epoch 11
I0731 22:13:27.798015 20406 solver.cpp:353] Iteration 11300 (5.40775 iter/s, 18.492s/100 iter), loss = 0.0537021
I0731 22:13:27.798040 20406 solver.cpp:375]     Train net output #0: loss = 0.053702 (* 1 = 0.053702 loss)
I0731 22:13:27.798045 20406 sgd_solver.cpp:136] Iteration 11300, lr = 1e-05, m = 0.9
I0731 22:13:46.346863 20406 solver.cpp:353] Iteration 11400 (5.39132 iter/s, 18.5483s/100 iter), loss = 0.0618375
I0731 22:13:46.346941 20406 solver.cpp:375]     Train net output #0: loss = 0.0618374 (* 1 = 0.0618374 loss)
I0731 22:13:46.346946 20406 sgd_solver.cpp:136] Iteration 11400, lr = 1e-05, m = 0.9
I0731 22:14:04.862558 20406 solver.cpp:353] Iteration 11500 (5.40097 iter/s, 18.5152s/100 iter), loss = 0.0782867
I0731 22:14:04.862581 20406 solver.cpp:375]     Train net output #0: loss = 0.0782866 (* 1 = 0.0782866 loss)
I0731 22:14:04.862586 20406 sgd_solver.cpp:136] Iteration 11500, lr = 1e-05, m = 0.9
I0731 22:14:17.274116 20415 data_reader.cpp:264] Starting prefetch of epoch 4
I0731 22:14:23.541743 20406 solver.cpp:353] Iteration 11600 (5.3537 iter/s, 18.6787s/100 iter), loss = 0.103513
I0731 22:14:23.541766 20406 solver.cpp:375]     Train net output #0: loss = 0.103513 (* 1 = 0.103513 loss)
I0731 22:14:23.541770 20406 sgd_solver.cpp:136] Iteration 11600, lr = 1e-05, m = 0.9
I0731 22:14:42.136912 20406 solver.cpp:353] Iteration 11700 (5.37789 iter/s, 18.5947s/100 iter), loss = 0.0416332
I0731 22:14:42.136939 20406 solver.cpp:375]     Train net output #0: loss = 0.0416331 (* 1 = 0.0416331 loss)
I0731 22:14:42.136943 20406 sgd_solver.cpp:136] Iteration 11700, lr = 1e-05, m = 0.9
I0731 22:15:00.604954 20406 solver.cpp:353] Iteration 11800 (5.41491 iter/s, 18.4675s/100 iter), loss = 0.0590762
I0731 22:15:00.605005 20406 solver.cpp:375]     Train net output #0: loss = 0.0590761 (* 1 = 0.0590761 loss)
I0731 22:15:00.605010 20406 sgd_solver.cpp:136] Iteration 11800, lr = 1e-05, m = 0.9
I0731 22:15:18.402215 20415 data_reader.cpp:264] Starting prefetch of epoch 5
I0731 22:15:19.158397 20406 solver.cpp:353] Iteration 11900 (5.38998 iter/s, 18.5529s/100 iter), loss = 0.063318
I0731 22:15:19.158421 20406 solver.cpp:375]     Train net output #0: loss = 0.0633179 (* 1 = 0.0633179 loss)
I0731 22:15:19.158426 20406 sgd_solver.cpp:136] Iteration 11900, lr = 1e-05, m = 0.9
I0731 22:15:37.448199 20406 solver.cpp:404] Sparsity after update:
I0731 22:15:37.457264 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 22:15:37.457304 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 22:15:37.457320 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 22:15:37.457325 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 22:15:37.457330 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 22:15:37.457334 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 22:15:37.457340 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 22:15:37.457343 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 22:15:37.457347 20406 net.cpp:2270] out3a_param_0(0) 
I0731 22:15:37.457351 20406 net.cpp:2270] out5a_param_0(0) 
I0731 22:15:37.457357 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 22:15:37.457362 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 22:15:37.457367 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 22:15:37.457372 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 22:15:37.457377 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 22:15:37.457383 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 22:15:37.457388 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 22:15:37.457392 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 22:15:37.457398 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 22:15:37.457417 20406 solver.cpp:550] Iteration 12000, Testing net (#0)
I0731 22:15:40.930235 20436 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 22:15:48.670322 20406 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.951711
I0731 22:15:48.670347 20406 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999813
I0731 22:15:48.670353 20406 solver.cpp:635]     Test net output #2: loss = 0.169579 (* 1 = 0.169579 loss)
I0731 22:15:48.670428 20406 solver.cpp:305] [MultiGPU] Tests completed in 11.2127s
I0731 22:15:48.879675 20406 solver.cpp:353] Iteration 12000 (3.36469 iter/s, 29.7205s/100 iter), loss = 0.0383862
I0731 22:15:48.879705 20406 solver.cpp:375]     Train net output #0: loss = 0.0383861 (* 1 = 0.0383861 loss)
I0731 22:15:48.879712 20406 sgd_solver.cpp:136] Iteration 12000, lr = 1e-05, m = 0.9
I0731 22:16:00.438796 20412 data_reader.cpp:264] Starting prefetch of epoch 8
I0731 22:16:07.409868 20406 solver.cpp:353] Iteration 12100 (5.39675 iter/s, 18.5297s/100 iter), loss = 0.088828
I0731 22:16:07.409893 20406 solver.cpp:375]     Train net output #0: loss = 0.0888279 (* 1 = 0.0888279 loss)
I0731 22:16:07.409898 20406 sgd_solver.cpp:136] Iteration 12100, lr = 1e-05, m = 0.9
I0731 22:16:26.030901 20406 solver.cpp:353] Iteration 12200 (5.37042 iter/s, 18.6205s/100 iter), loss = 0.0564426
I0731 22:16:26.030973 20406 solver.cpp:375]     Train net output #0: loss = 0.0564426 (* 1 = 0.0564426 loss)
I0731 22:16:26.030978 20406 sgd_solver.cpp:136] Iteration 12200, lr = 1e-05, m = 0.9
I0731 22:16:44.446223 20406 solver.cpp:353] Iteration 12300 (5.43041 iter/s, 18.4148s/100 iter), loss = 0.0506161
I0731 22:16:44.446246 20406 solver.cpp:375]     Train net output #0: loss = 0.050616 (* 1 = 0.050616 loss)
I0731 22:16:44.446251 20406 sgd_solver.cpp:136] Iteration 12300, lr = 1e-05, m = 0.9
I0731 22:17:01.710795 20412 data_reader.cpp:264] Starting prefetch of epoch 9
I0731 22:17:03.160410 20406 solver.cpp:353] Iteration 12400 (5.34369 iter/s, 18.7137s/100 iter), loss = 0.0525824
I0731 22:17:03.160435 20406 solver.cpp:375]     Train net output #0: loss = 0.0525823 (* 1 = 0.0525823 loss)
I0731 22:17:03.160440 20406 sgd_solver.cpp:136] Iteration 12400, lr = 1e-05, m = 0.9
I0731 22:17:21.767268 20406 solver.cpp:353] Iteration 12500 (5.37451 iter/s, 18.6063s/100 iter), loss = 0.0724006
I0731 22:17:21.767294 20406 solver.cpp:375]     Train net output #0: loss = 0.0724006 (* 1 = 0.0724006 loss)
I0731 22:17:21.767298 20406 sgd_solver.cpp:136] Iteration 12500, lr = 1e-05, m = 0.9
I0731 22:17:32.524216 20410 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 22:17:40.332470 20406 solver.cpp:353] Iteration 12600 (5.38657 iter/s, 18.5647s/100 iter), loss = 0.0455852
I0731 22:17:40.332495 20406 solver.cpp:375]     Train net output #0: loss = 0.0455851 (* 1 = 0.0455851 loss)
I0731 22:17:40.332499 20406 sgd_solver.cpp:136] Iteration 12600, lr = 1e-05, m = 0.9
I0731 22:17:58.981226 20406 solver.cpp:353] Iteration 12700 (5.36244 iter/s, 18.6482s/100 iter), loss = 0.116025
I0731 22:17:58.981250 20406 solver.cpp:375]     Train net output #0: loss = 0.116025 (* 1 = 0.116025 loss)
I0731 22:17:58.981253 20406 sgd_solver.cpp:136] Iteration 12700, lr = 1e-05, m = 0.9
I0731 22:18:18.461277 20406 solver.cpp:353] Iteration 12800 (5.1336 iter/s, 19.4795s/100 iter), loss = 0.046678
I0731 22:18:18.469159 20406 solver.cpp:375]     Train net output #0: loss = 0.0466779 (* 1 = 0.0466779 loss)
I0731 22:18:18.469208 20406 sgd_solver.cpp:136] Iteration 12800, lr = 1e-05, m = 0.9
I0731 22:18:35.177151 20415 data_reader.cpp:264] Starting prefetch of epoch 6
I0731 22:18:37.395769 20406 solver.cpp:353] Iteration 12900 (5.28151 iter/s, 18.934s/100 iter), loss = 0.0751797
I0731 22:18:37.395794 20406 solver.cpp:375]     Train net output #0: loss = 0.0751796 (* 1 = 0.0751796 loss)
I0731 22:18:37.395798 20406 sgd_solver.cpp:136] Iteration 12900, lr = 1e-05, m = 0.9
I0731 22:18:55.926450 20406 solver.cpp:404] Sparsity after update:
I0731 22:18:55.941293 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 22:18:55.941332 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 22:18:55.941346 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 22:18:55.941349 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 22:18:55.941352 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 22:18:55.941355 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 22:18:55.941357 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 22:18:55.941360 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 22:18:55.941362 20406 net.cpp:2270] out3a_param_0(0) 
I0731 22:18:55.941365 20406 net.cpp:2270] out5a_param_0(0) 
I0731 22:18:55.941368 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 22:18:55.941371 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 22:18:55.941375 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 22:18:55.941377 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 22:18:55.941380 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 22:18:55.941382 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 22:18:55.941385 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 22:18:55.941387 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 22:18:55.941390 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 22:18:56.121577 20406 solver.cpp:353] Iteration 13000 (5.34037 iter/s, 18.7253s/100 iter), loss = 0.057671
I0731 22:18:56.121608 20406 solver.cpp:375]     Train net output #0: loss = 0.0576709 (* 1 = 0.0576709 loss)
I0731 22:18:56.121614 20406 sgd_solver.cpp:136] Iteration 13000, lr = 1e-05, m = 0.9
I0731 22:19:14.957764 20406 solver.cpp:353] Iteration 13100 (5.30908 iter/s, 18.8357s/100 iter), loss = 0.0787123
I0731 22:19:14.957789 20406 solver.cpp:375]     Train net output #0: loss = 0.0787122 (* 1 = 0.0787122 loss)
I0731 22:19:14.957793 20406 sgd_solver.cpp:136] Iteration 13100, lr = 1e-05, m = 0.9
I0731 22:19:33.436477 20406 solver.cpp:353] Iteration 13200 (5.41178 iter/s, 18.4782s/100 iter), loss = 0.0535732
I0731 22:19:33.436558 20406 solver.cpp:375]     Train net output #0: loss = 0.0535731 (* 1 = 0.0535731 loss)
I0731 22:19:33.436563 20406 sgd_solver.cpp:136] Iteration 13200, lr = 1e-05, m = 0.9
I0731 22:19:36.981868 20412 data_reader.cpp:264] Starting prefetch of epoch 10
I0731 22:19:52.126524 20406 solver.cpp:353] Iteration 13300 (5.35059 iter/s, 18.6895s/100 iter), loss = 0.0631193
I0731 22:19:52.126549 20406 solver.cpp:375]     Train net output #0: loss = 0.0631192 (* 1 = 0.0631192 loss)
I0731 22:19:52.126554 20406 sgd_solver.cpp:136] Iteration 13300, lr = 1e-05, m = 0.9
I0731 22:20:07.789438 20409 data_reader.cpp:264] Starting prefetch of epoch 9
I0731 22:20:10.730262 20406 solver.cpp:353] Iteration 13400 (5.37541 iter/s, 18.6032s/100 iter), loss = 0.0496634
I0731 22:20:10.730286 20406 solver.cpp:375]     Train net output #0: loss = 0.0496633 (* 1 = 0.0496633 loss)
I0731 22:20:10.730290 20406 sgd_solver.cpp:136] Iteration 13400, lr = 1e-05, m = 0.9
I0731 22:20:29.294134 20406 solver.cpp:353] Iteration 13500 (5.38696 iter/s, 18.5634s/100 iter), loss = 0.0689355
I0731 22:20:29.294160 20406 solver.cpp:375]     Train net output #0: loss = 0.0689354 (* 1 = 0.0689354 loss)
I0731 22:20:29.294164 20406 sgd_solver.cpp:136] Iteration 13500, lr = 1e-05, m = 0.9
I0731 22:20:47.988234 20406 solver.cpp:353] Iteration 13600 (5.34943 iter/s, 18.6936s/100 iter), loss = 0.0741932
I0731 22:20:47.988289 20406 solver.cpp:375]     Train net output #0: loss = 0.0741931 (* 1 = 0.0741931 loss)
I0731 22:20:47.988296 20406 sgd_solver.cpp:136] Iteration 13600, lr = 1e-05, m = 0.9
I0731 22:21:06.499389 20406 solver.cpp:353] Iteration 13700 (5.4023 iter/s, 18.5106s/100 iter), loss = 0.0543656
I0731 22:21:06.499414 20406 solver.cpp:375]     Train net output #0: loss = 0.0543655 (* 1 = 0.0543655 loss)
I0731 22:21:06.499419 20406 sgd_solver.cpp:136] Iteration 13700, lr = 1e-05, m = 0.9
I0731 22:21:09.286660 20409 data_reader.cpp:264] Starting prefetch of epoch 10
I0731 22:21:25.009263 20406 solver.cpp:353] Iteration 13800 (5.40267 iter/s, 18.5094s/100 iter), loss = 0.0559696
I0731 22:21:25.009338 20406 solver.cpp:375]     Train net output #0: loss = 0.0559695 (* 1 = 0.0559695 loss)
I0731 22:21:25.009346 20406 sgd_solver.cpp:136] Iteration 13800, lr = 1e-05, m = 0.9
I0731 22:21:43.667062 20406 solver.cpp:353] Iteration 13900 (5.35984 iter/s, 18.6573s/100 iter), loss = 0.0668986
I0731 22:21:43.667088 20406 solver.cpp:375]     Train net output #0: loss = 0.0668985 (* 1 = 0.0668985 loss)
I0731 22:21:43.667094 20406 sgd_solver.cpp:136] Iteration 13900, lr = 1e-05, m = 0.9
I0731 22:22:02.058190 20406 solver.cpp:404] Sparsity after update:
I0731 22:22:02.068070 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 22:22:02.068145 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 22:22:02.068167 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 22:22:02.068178 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 22:22:02.068188 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 22:22:02.068197 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 22:22:02.068202 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 22:22:02.068205 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 22:22:02.068208 20406 net.cpp:2270] out3a_param_0(0) 
I0731 22:22:02.068212 20406 net.cpp:2270] out5a_param_0(0) 
I0731 22:22:02.068217 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 22:22:02.068222 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 22:22:02.068225 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 22:22:02.068228 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 22:22:02.068233 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 22:22:02.068236 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 22:22:02.068240 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 22:22:02.068244 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 22:22:02.068248 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 22:22:02.068261 20406 solver.cpp:550] Iteration 14000, Testing net (#0)
I0731 22:22:09.104611 20440 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 22:22:13.314414 20406 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.952114
I0731 22:22:13.314438 20406 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999226
I0731 22:22:13.314445 20406 solver.cpp:635]     Test net output #2: loss = 0.20303 (* 1 = 0.20303 loss)
I0731 22:22:13.314509 20406 solver.cpp:305] [MultiGPU] Tests completed in 11.2459s
I0731 22:22:13.526702 20406 solver.cpp:353] Iteration 14000 (3.34909 iter/s, 29.8588s/100 iter), loss = 0.0864668
I0731 22:22:13.526726 20406 solver.cpp:375]     Train net output #0: loss = 0.0864667 (* 1 = 0.0864667 loss)
I0731 22:22:13.526729 20406 sgd_solver.cpp:136] Iteration 14000, lr = 1e-05, m = 0.9
I0731 22:22:21.886937 20409 data_reader.cpp:264] Starting prefetch of epoch 11
I0731 22:22:32.118086 20406 solver.cpp:353] Iteration 14100 (5.37898 iter/s, 18.5909s/100 iter), loss = 0.0871592
I0731 22:22:32.118140 20406 solver.cpp:375]     Train net output #0: loss = 0.0871591 (* 1 = 0.0871591 loss)
I0731 22:22:32.118144 20406 sgd_solver.cpp:136] Iteration 14100, lr = 1e-05, m = 0.9
I0731 22:22:50.655429 20406 solver.cpp:353] Iteration 14200 (5.39467 iter/s, 18.5368s/100 iter), loss = 0.0522425
I0731 22:22:50.655454 20406 solver.cpp:375]     Train net output #0: loss = 0.0522424 (* 1 = 0.0522424 loss)
I0731 22:22:50.655459 20406 sgd_solver.cpp:136] Iteration 14200, lr = 1e-05, m = 0.9
I0731 22:23:09.223960 20406 solver.cpp:353] Iteration 14300 (5.3856 iter/s, 18.568s/100 iter), loss = 0.105552
I0731 22:23:09.224014 20406 solver.cpp:375]     Train net output #0: loss = 0.105551 (* 1 = 0.105551 loss)
I0731 22:23:09.224020 20406 sgd_solver.cpp:136] Iteration 14300, lr = 1e-05, m = 0.9
I0731 22:23:23.309267 20409 data_reader.cpp:264] Starting prefetch of epoch 12
I0731 22:23:27.684556 20406 solver.cpp:353] Iteration 14400 (5.41709 iter/s, 18.4601s/100 iter), loss = 0.078282
I0731 22:23:27.684582 20406 solver.cpp:375]     Train net output #0: loss = 0.0782819 (* 1 = 0.0782819 loss)
I0731 22:23:27.684587 20406 sgd_solver.cpp:136] Iteration 14400, lr = 1e-05, m = 0.9
I0731 22:23:46.245316 20406 solver.cpp:353] Iteration 14500 (5.38786 iter/s, 18.5602s/100 iter), loss = 0.0778428
I0731 22:23:46.245605 20406 solver.cpp:375]     Train net output #0: loss = 0.0778427 (* 1 = 0.0778427 loss)
I0731 22:23:46.245611 20406 sgd_solver.cpp:136] Iteration 14500, lr = 1e-05, m = 0.9
I0731 22:24:04.788801 20406 solver.cpp:353] Iteration 14600 (5.39288 iter/s, 18.543s/100 iter), loss = 0.0653354
I0731 22:24:04.788830 20406 solver.cpp:375]     Train net output #0: loss = 0.0653353 (* 1 = 0.0653353 loss)
I0731 22:24:04.788835 20406 sgd_solver.cpp:136] Iteration 14600, lr = 1e-05, m = 0.9
I0731 22:24:23.291146 20406 solver.cpp:353] Iteration 14700 (5.40487 iter/s, 18.5018s/100 iter), loss = 0.0624376
I0731 22:24:23.291195 20406 solver.cpp:375]     Train net output #0: loss = 0.0624375 (* 1 = 0.0624375 loss)
I0731 22:24:23.291200 20406 sgd_solver.cpp:136] Iteration 14700, lr = 1e-05, m = 0.9
I0731 22:24:24.397909 20415 data_reader.cpp:264] Starting prefetch of epoch 7
I0731 22:24:24.397909 20412 data_reader.cpp:264] Starting prefetch of epoch 11
I0731 22:24:41.829017 20406 solver.cpp:353] Iteration 14800 (5.39451 iter/s, 18.5374s/100 iter), loss = 0.0656156
I0731 22:24:41.829041 20406 solver.cpp:375]     Train net output #0: loss = 0.0656155 (* 1 = 0.0656155 loss)
I0731 22:24:41.829046 20406 sgd_solver.cpp:136] Iteration 14800, lr = 1e-05, m = 0.9
I0731 22:25:00.368556 20406 solver.cpp:353] Iteration 14900 (5.39403 iter/s, 18.539s/100 iter), loss = 0.0778251
I0731 22:25:00.368618 20406 solver.cpp:375]     Train net output #0: loss = 0.077825 (* 1 = 0.077825 loss)
I0731 22:25:00.368624 20406 sgd_solver.cpp:136] Iteration 14900, lr = 1e-05, m = 0.9
I0731 22:25:18.686828 20406 solver.cpp:404] Sparsity after update:
I0731 22:25:18.714058 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 22:25:18.714087 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 22:25:18.714097 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 22:25:18.714098 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 22:25:18.714100 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 22:25:18.714102 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 22:25:18.714104 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 22:25:18.714105 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 22:25:18.714107 20406 net.cpp:2270] out3a_param_0(0) 
I0731 22:25:18.714109 20406 net.cpp:2270] out5a_param_0(0) 
I0731 22:25:18.714112 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 22:25:18.714113 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 22:25:18.714115 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 22:25:18.714118 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 22:25:18.714119 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 22:25:18.714128 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 22:25:18.714130 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 22:25:18.714133 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 22:25:18.714134 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 22:25:18.883098 20406 solver.cpp:353] Iteration 15000 (5.40131 iter/s, 18.514s/100 iter), loss = 0.0579527
I0731 22:25:18.883126 20406 solver.cpp:375]     Train net output #0: loss = 0.0579526 (* 1 = 0.0579526 loss)
I0731 22:25:18.883129 20406 sgd_solver.cpp:136] Iteration 15000, lr = 1e-05, m = 0.9
I0731 22:25:25.755148 20412 data_reader.cpp:264] Starting prefetch of epoch 12
I0731 22:25:37.395508 20406 solver.cpp:353] Iteration 15100 (5.40193 iter/s, 18.5119s/100 iter), loss = 0.0888706
I0731 22:25:37.396010 20406 solver.cpp:375]     Train net output #0: loss = 0.0888705 (* 1 = 0.0888705 loss)
I0731 22:25:37.396018 20406 sgd_solver.cpp:136] Iteration 15100, lr = 1e-05, m = 0.9
I0731 22:25:55.950124 20406 solver.cpp:353] Iteration 15200 (5.38964 iter/s, 18.5541s/100 iter), loss = 0.077178
I0731 22:25:55.950152 20406 solver.cpp:375]     Train net output #0: loss = 0.0771779 (* 1 = 0.0771779 loss)
I0731 22:25:55.950156 20406 sgd_solver.cpp:136] Iteration 15200, lr = 1e-05, m = 0.9
I0731 22:25:56.355211 20410 data_reader.cpp:264] Starting prefetch of epoch 4
I0731 22:26:14.499464 20406 solver.cpp:353] Iteration 15300 (5.39118 iter/s, 18.5488s/100 iter), loss = 0.0363722
I0731 22:26:14.499532 20406 solver.cpp:375]     Train net output #0: loss = 0.0363721 (* 1 = 0.0363721 loss)
I0731 22:26:14.499538 20406 sgd_solver.cpp:136] Iteration 15300, lr = 1e-05, m = 0.9
I0731 22:26:32.837043 20406 solver.cpp:353] Iteration 15400 (5.45343 iter/s, 18.3371s/100 iter), loss = 0.0712054
I0731 22:26:32.837064 20406 solver.cpp:375]     Train net output #0: loss = 0.0712053 (* 1 = 0.0712053 loss)
I0731 22:26:32.837067 20406 sgd_solver.cpp:136] Iteration 15400, lr = 1e-05, m = 0.9
I0731 22:26:51.374306 20406 solver.cpp:353] Iteration 15500 (5.39469 iter/s, 18.5368s/100 iter), loss = 0.0720965
I0731 22:26:51.374366 20406 solver.cpp:375]     Train net output #0: loss = 0.0720964 (* 1 = 0.0720964 loss)
I0731 22:26:51.374373 20406 sgd_solver.cpp:136] Iteration 15500, lr = 1e-05, m = 0.9
I0731 22:26:57.546051 20364 data_reader.cpp:264] Starting prefetch of epoch 10
I0731 22:27:09.956176 20406 solver.cpp:353] Iteration 15600 (5.38174 iter/s, 18.5814s/100 iter), loss = 0.0608944
I0731 22:27:09.956202 20406 solver.cpp:375]     Train net output #0: loss = 0.0608943 (* 1 = 0.0608943 loss)
I0731 22:27:09.956207 20406 sgd_solver.cpp:136] Iteration 15600, lr = 1e-05, m = 0.9
I0731 22:27:28.480444 20406 solver.cpp:353] Iteration 15700 (5.39847 iter/s, 18.5238s/100 iter), loss = 0.0735068
I0731 22:27:28.480525 20406 solver.cpp:375]     Train net output #0: loss = 0.0735067 (* 1 = 0.0735067 loss)
I0731 22:27:28.480530 20406 sgd_solver.cpp:136] Iteration 15700, lr = 1e-05, m = 0.9
I0731 22:27:46.976012 20406 solver.cpp:353] Iteration 15800 (5.40685 iter/s, 18.4951s/100 iter), loss = 0.0705278
I0731 22:27:46.976042 20406 solver.cpp:375]     Train net output #0: loss = 0.0705277 (* 1 = 0.0705277 loss)
I0731 22:27:46.976045 20406 sgd_solver.cpp:136] Iteration 15800, lr = 1e-05, m = 0.9
I0731 22:27:58.698298 20364 data_reader.cpp:264] Starting prefetch of epoch 11
I0731 22:28:05.492270 20406 solver.cpp:353] Iteration 15900 (5.40081 iter/s, 18.5157s/100 iter), loss = 0.0512759
I0731 22:28:05.492295 20406 solver.cpp:375]     Train net output #0: loss = 0.0512758 (* 1 = 0.0512758 loss)
I0731 22:28:05.492300 20406 sgd_solver.cpp:136] Iteration 15900, lr = 1e-05, m = 0.9
I0731 22:28:24.011626 20406 solver.cpp:404] Sparsity after update:
I0731 22:28:24.019908 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 22:28:24.019965 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 22:28:24.019981 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 22:28:24.020000 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 22:28:24.020011 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 22:28:24.020022 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 22:28:24.020031 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 22:28:24.020041 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 22:28:24.020051 20406 net.cpp:2270] out3a_param_0(0) 
I0731 22:28:24.020059 20406 net.cpp:2270] out5a_param_0(0) 
I0731 22:28:24.020066 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 22:28:24.020074 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 22:28:24.020083 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 22:28:24.020090 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 22:28:24.020099 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 22:28:24.020107 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 22:28:24.020114 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 22:28:24.020122 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 22:28:24.020129 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 22:28:24.020146 20406 solver.cpp:550] Iteration 16000, Testing net (#0)
I0731 22:28:27.421789 20440 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 22:28:35.163816 20406 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.952327
I0731 22:28:35.163935 20406 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999847
I0731 22:28:35.163944 20406 solver.cpp:635]     Test net output #2: loss = 0.168726 (* 1 = 0.168726 loss)
I0731 22:28:35.163975 20406 solver.cpp:305] [MultiGPU] Tests completed in 11.1435s
I0731 22:28:35.349607 20406 solver.cpp:353] Iteration 16000 (3.34935 iter/s, 29.8565s/100 iter), loss = 0.0764273
I0731 22:28:35.349629 20406 solver.cpp:375]     Train net output #0: loss = 0.0764272 (* 1 = 0.0764272 loss)
I0731 22:28:35.349634 20406 sgd_solver.cpp:136] Iteration 16000, lr = 1e-05, m = 0.9
I0731 22:28:40.547849 20364 data_reader.cpp:264] Starting prefetch of epoch 12
I0731 22:28:53.809666 20406 solver.cpp:353] Iteration 16100 (5.41725 iter/s, 18.4595s/100 iter), loss = 0.0727638
I0731 22:28:53.809693 20406 solver.cpp:375]     Train net output #0: loss = 0.0727637 (* 1 = 0.0727637 loss)
I0731 22:28:53.809698 20406 sgd_solver.cpp:136] Iteration 16100, lr = 1e-05, m = 0.9
I0731 22:29:12.283141 20406 solver.cpp:353] Iteration 16200 (5.41332 iter/s, 18.473s/100 iter), loss = 0.078281
I0731 22:29:12.283195 20406 solver.cpp:375]     Train net output #0: loss = 0.0782809 (* 1 = 0.0782809 loss)
I0731 22:29:12.283201 20406 sgd_solver.cpp:136] Iteration 16200, lr = 1e-05, m = 0.9
I0731 22:29:30.821733 20406 solver.cpp:353] Iteration 16300 (5.3943 iter/s, 18.5381s/100 iter), loss = 0.0705002
I0731 22:29:30.821761 20406 solver.cpp:375]     Train net output #0: loss = 0.0705001 (* 1 = 0.0705001 loss)
I0731 22:29:30.821765 20406 sgd_solver.cpp:136] Iteration 16300, lr = 1e-05, m = 0.9
I0731 22:29:41.699110 20364 data_reader.cpp:264] Starting prefetch of epoch 13
I0731 22:29:49.242982 20406 solver.cpp:353] Iteration 16400 (5.42866 iter/s, 18.4207s/100 iter), loss = 0.0426735
I0731 22:29:49.243039 20406 solver.cpp:375]     Train net output #0: loss = 0.0426734 (* 1 = 0.0426734 loss)
I0731 22:29:49.243044 20406 sgd_solver.cpp:136] Iteration 16400, lr = 1e-05, m = 0.9
I0731 22:30:08.469020 20406 solver.cpp:353] Iteration 16500 (5.20142 iter/s, 19.2255s/100 iter), loss = 0.0657932
I0731 22:30:08.469044 20406 solver.cpp:375]     Train net output #0: loss = 0.0657931 (* 1 = 0.0657931 loss)
I0731 22:30:08.469050 20406 sgd_solver.cpp:136] Iteration 16500, lr = 1e-05, m = 0.9
I0731 22:30:12.943063 20409 data_reader.cpp:264] Starting prefetch of epoch 13
I0731 22:30:26.922237 20406 solver.cpp:353] Iteration 16600 (5.41926 iter/s, 18.4527s/100 iter), loss = 0.0681183
I0731 22:30:26.922289 20406 solver.cpp:375]     Train net output #0: loss = 0.0681182 (* 1 = 0.0681182 loss)
I0731 22:30:26.922294 20406 sgd_solver.cpp:136] Iteration 16600, lr = 1e-05, m = 0.9
I0731 22:30:45.409776 20406 solver.cpp:353] Iteration 16700 (5.4092 iter/s, 18.487s/100 iter), loss = 0.0768582
I0731 22:30:45.409801 20406 solver.cpp:375]     Train net output #0: loss = 0.076858 (* 1 = 0.076858 loss)
I0731 22:30:45.409807 20406 sgd_solver.cpp:136] Iteration 16700, lr = 1e-05, m = 0.9
I0731 22:31:03.858206 20406 solver.cpp:353] Iteration 16800 (5.42067 iter/s, 18.4479s/100 iter), loss = 0.077025
I0731 22:31:03.858264 20406 solver.cpp:375]     Train net output #0: loss = 0.0770249 (* 1 = 0.0770249 loss)
I0731 22:31:03.858269 20406 sgd_solver.cpp:136] Iteration 16800, lr = 1e-05, m = 0.9
I0731 22:31:14.055524 20364 data_reader.cpp:264] Starting prefetch of epoch 14
I0731 22:31:22.365020 20406 solver.cpp:353] Iteration 16900 (5.40356 iter/s, 18.5063s/100 iter), loss = 0.064495
I0731 22:31:22.365048 20406 solver.cpp:375]     Train net output #0: loss = 0.0644949 (* 1 = 0.0644949 loss)
I0731 22:31:22.365052 20406 sgd_solver.cpp:136] Iteration 16900, lr = 1e-05, m = 0.9
I0731 22:31:40.773885 20406 solver.cpp:404] Sparsity after update:
I0731 22:31:40.802433 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 22:31:40.802450 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 22:31:40.802458 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 22:31:40.802459 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 22:31:40.802461 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 22:31:40.802464 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 22:31:40.802465 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 22:31:40.802467 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 22:31:40.802469 20406 net.cpp:2270] out3a_param_0(0) 
I0731 22:31:40.802471 20406 net.cpp:2270] out5a_param_0(0) 
I0731 22:31:40.802474 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 22:31:40.802474 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 22:31:40.802476 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 22:31:40.802479 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 22:31:40.802480 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 22:31:40.802482 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 22:31:40.802484 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 22:31:40.802489 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 22:31:40.802490 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 22:31:40.971263 20406 solver.cpp:353] Iteration 17000 (5.37469 iter/s, 18.6057s/100 iter), loss = 0.0967323
I0731 22:31:40.971289 20406 solver.cpp:375]     Train net output #0: loss = 0.0967322 (* 1 = 0.0967322 loss)
I0731 22:31:40.971294 20406 sgd_solver.cpp:136] Iteration 17000, lr = 1e-05, m = 0.9
I0731 22:31:59.504187 20406 solver.cpp:353] Iteration 17100 (5.39595 iter/s, 18.5324s/100 iter), loss = 0.0692452
I0731 22:31:59.504211 20406 solver.cpp:375]     Train net output #0: loss = 0.0692451 (* 1 = 0.0692451 loss)
I0731 22:31:59.504216 20406 sgd_solver.cpp:136] Iteration 17100, lr = 1e-05, m = 0.9
I0731 22:32:15.242388 20412 data_reader.cpp:264] Starting prefetch of epoch 13
I0731 22:32:18.009256 20406 solver.cpp:353] Iteration 17200 (5.40407 iter/s, 18.5046s/100 iter), loss = 0.0813413
I0731 22:32:18.009282 20406 solver.cpp:375]     Train net output #0: loss = 0.0813412 (* 1 = 0.0813412 loss)
I0731 22:32:18.009289 20406 sgd_solver.cpp:136] Iteration 17200, lr = 1e-05, m = 0.9
I0731 22:32:36.450795 20406 solver.cpp:353] Iteration 17300 (5.42269 iter/s, 18.441s/100 iter), loss = 0.0510494
I0731 22:32:36.450826 20406 solver.cpp:375]     Train net output #0: loss = 0.0510493 (* 1 = 0.0510493 loss)
I0731 22:32:36.450834 20406 sgd_solver.cpp:136] Iteration 17300, lr = 1e-05, m = 0.9
I0731 22:32:45.972179 20346 data_reader.cpp:264] Starting prefetch of epoch 12
I0731 22:32:55.035815 20406 solver.cpp:353] Iteration 17400 (5.38083 iter/s, 18.5845s/100 iter), loss = 0.0539015
I0731 22:32:55.035838 20406 solver.cpp:375]     Train net output #0: loss = 0.0539014 (* 1 = 0.0539014 loss)
I0731 22:32:55.035842 20406 sgd_solver.cpp:136] Iteration 17400, lr = 1e-05, m = 0.9
I0731 22:33:13.565027 20406 solver.cpp:353] Iteration 17500 (5.39703 iter/s, 18.5287s/100 iter), loss = 0.045859
I0731 22:33:13.565049 20406 solver.cpp:375]     Train net output #0: loss = 0.0458589 (* 1 = 0.0458589 loss)
I0731 22:33:13.565053 20406 sgd_solver.cpp:136] Iteration 17500, lr = 1e-05, m = 0.9
I0731 22:33:32.675581 20406 solver.cpp:353] Iteration 17600 (5.23285 iter/s, 19.11s/100 iter), loss = 0.0769661
I0731 22:33:32.675688 20406 solver.cpp:375]     Train net output #0: loss = 0.076966 (* 1 = 0.076966 loss)
I0731 22:33:32.675695 20406 sgd_solver.cpp:136] Iteration 17600, lr = 1e-05, m = 0.9
I0731 22:33:47.759150 20412 data_reader.cpp:264] Starting prefetch of epoch 14
I0731 22:33:51.355846 20406 solver.cpp:353] Iteration 17700 (5.35339 iter/s, 18.6798s/100 iter), loss = 0.0616923
I0731 22:33:51.355871 20406 solver.cpp:375]     Train net output #0: loss = 0.0616922 (* 1 = 0.0616922 loss)
I0731 22:33:51.355876 20406 sgd_solver.cpp:136] Iteration 17700, lr = 1e-05, m = 0.9
I0731 22:34:09.889245 20406 solver.cpp:353] Iteration 17800 (5.39581 iter/s, 18.5329s/100 iter), loss = 0.0590552
I0731 22:34:09.889364 20406 solver.cpp:375]     Train net output #0: loss = 0.0590551 (* 1 = 0.0590551 loss)
I0731 22:34:09.889369 20406 sgd_solver.cpp:136] Iteration 17800, lr = 1e-05, m = 0.9
I0731 22:34:28.399505 20406 solver.cpp:353] Iteration 17900 (5.40256 iter/s, 18.5098s/100 iter), loss = 0.0815268
I0731 22:34:28.399531 20406 solver.cpp:375]     Train net output #0: loss = 0.0815267 (* 1 = 0.0815267 loss)
I0731 22:34:28.399535 20406 sgd_solver.cpp:136] Iteration 17900, lr = 1e-05, m = 0.9
I0731 22:34:46.826324 20406 solver.cpp:404] Sparsity after update:
I0731 22:34:46.836097 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 22:34:46.836115 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 22:34:46.836122 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 22:34:46.836124 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 22:34:46.836127 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 22:34:46.836128 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 22:34:46.836130 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 22:34:46.836133 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 22:34:46.836134 20406 net.cpp:2270] out3a_param_0(0) 
I0731 22:34:46.836136 20406 net.cpp:2270] out5a_param_0(0) 
I0731 22:34:46.836138 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 22:34:46.836139 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 22:34:46.836143 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 22:34:46.836145 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 22:34:46.836146 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 22:34:46.836148 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 22:34:46.836150 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 22:34:46.836153 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 22:34:46.836154 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 22:34:46.836163 20406 solver.cpp:550] Iteration 18000, Testing net (#0)
I0731 22:34:53.971143 20436 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 22:34:58.095144 20406 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.952041
I0731 22:34:58.095162 20406 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999296
I0731 22:34:58.095168 20406 solver.cpp:635]     Test net output #2: loss = 0.20741 (* 1 = 0.20741 loss)
I0731 22:34:58.095253 20406 solver.cpp:305] [MultiGPU] Tests completed in 11.2588s
I0731 22:34:58.284867 20406 solver.cpp:353] Iteration 18000 (3.34621 iter/s, 29.8845s/100 iter), loss = 0.0482773
I0731 22:34:58.284895 20406 solver.cpp:375]     Train net output #0: loss = 0.0482773 (* 1 = 0.0482773 loss)
I0731 22:34:58.284903 20406 sgd_solver.cpp:136] Iteration 18000, lr = 1e-05, m = 0.9
I0731 22:35:00.619935 20410 data_reader.cpp:264] Starting prefetch of epoch 5
I0731 22:35:16.883488 20406 solver.cpp:353] Iteration 18100 (5.37689 iter/s, 18.5981s/100 iter), loss = 0.0539076
I0731 22:35:16.883543 20406 solver.cpp:375]     Train net output #0: loss = 0.0539076 (* 1 = 0.0539076 loss)
I0731 22:35:16.883548 20406 sgd_solver.cpp:136] Iteration 18100, lr = 1e-05, m = 0.9
I0731 22:35:35.474287 20406 solver.cpp:353] Iteration 18200 (5.37915 iter/s, 18.5903s/100 iter), loss = 0.0636319
I0731 22:35:35.474311 20406 solver.cpp:375]     Train net output #0: loss = 0.0636318 (* 1 = 0.0636318 loss)
I0731 22:35:35.474315 20406 sgd_solver.cpp:136] Iteration 18200, lr = 1e-05, m = 0.9
I0731 22:35:54.081684 20406 solver.cpp:353] Iteration 18300 (5.37436 iter/s, 18.6069s/100 iter), loss = 0.0722368
I0731 22:35:54.081742 20406 solver.cpp:375]     Train net output #0: loss = 0.0722367 (* 1 = 0.0722367 loss)
I0731 22:35:54.081748 20406 sgd_solver.cpp:136] Iteration 18300, lr = 1e-05, m = 0.9
I0731 22:36:01.996889 20412 data_reader.cpp:264] Starting prefetch of epoch 15
I0731 22:36:12.764571 20406 solver.cpp:353] Iteration 18400 (5.35264 iter/s, 18.6824s/100 iter), loss = 0.0652431
I0731 22:36:12.764600 20406 solver.cpp:375]     Train net output #0: loss = 0.0652431 (* 1 = 0.0652431 loss)
I0731 22:36:12.764605 20406 sgd_solver.cpp:136] Iteration 18400, lr = 1e-05, m = 0.9
I0731 22:36:31.474684 20406 solver.cpp:353] Iteration 18500 (5.34485 iter/s, 18.7096s/100 iter), loss = 0.0768533
I0731 22:36:31.474768 20406 solver.cpp:375]     Train net output #0: loss = 0.0768532 (* 1 = 0.0768532 loss)
I0731 22:36:31.474776 20406 sgd_solver.cpp:136] Iteration 18500, lr = 1e-05, m = 0.9
I0731 22:36:50.226991 20406 solver.cpp:353] Iteration 18600 (5.33282 iter/s, 18.7518s/100 iter), loss = 0.0613751
I0731 22:36:50.227020 20406 solver.cpp:375]     Train net output #0: loss = 0.061375 (* 1 = 0.061375 loss)
I0731 22:36:50.227023 20406 sgd_solver.cpp:136] Iteration 18600, lr = 1e-05, m = 0.9
I0731 22:37:03.747638 20364 data_reader.cpp:264] Starting prefetch of epoch 15
I0731 22:37:08.714085 20406 solver.cpp:353] Iteration 18700 (5.40933 iter/s, 18.4866s/100 iter), loss = 0.0454252
I0731 22:37:08.714112 20406 solver.cpp:375]     Train net output #0: loss = 0.0454251 (* 1 = 0.0454251 loss)
I0731 22:37:08.714115 20406 sgd_solver.cpp:136] Iteration 18700, lr = 1e-05, m = 0.9
I0731 22:37:27.639310 20406 solver.cpp:353] Iteration 18800 (5.2841 iter/s, 18.9247s/100 iter), loss = 0.107652
I0731 22:37:27.639340 20406 solver.cpp:375]     Train net output #0: loss = 0.107651 (* 1 = 0.107651 loss)
I0731 22:37:27.639346 20406 sgd_solver.cpp:136] Iteration 18800, lr = 1e-05, m = 0.9
I0731 22:37:34.692612 20410 data_reader.cpp:264] Starting prefetch of epoch 6
I0731 22:37:46.188537 20406 solver.cpp:353] Iteration 18900 (5.39121 iter/s, 18.5487s/100 iter), loss = 0.0654242
I0731 22:37:46.188565 20406 solver.cpp:375]     Train net output #0: loss = 0.0654241 (* 1 = 0.0654241 loss)
I0731 22:37:46.188570 20406 sgd_solver.cpp:136] Iteration 18900, lr = 1e-05, m = 0.9
I0731 22:38:04.664296 20406 solver.cpp:404] Sparsity after update:
I0731 22:38:04.680946 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 22:38:04.681031 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 22:38:04.681059 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 22:38:04.681066 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 22:38:04.681071 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 22:38:04.681074 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 22:38:04.681078 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 22:38:04.681082 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 22:38:04.681087 20406 net.cpp:2270] out3a_param_0(0) 
I0731 22:38:04.681092 20406 net.cpp:2270] out5a_param_0(0) 
I0731 22:38:04.681100 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 22:38:04.681105 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 22:38:04.681109 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 22:38:04.681113 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 22:38:04.681118 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 22:38:04.681123 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 22:38:04.681126 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 22:38:04.681130 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 22:38:04.681135 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 22:38:04.867835 20406 solver.cpp:353] Iteration 19000 (5.35367 iter/s, 18.6788s/100 iter), loss = 0.08694
I0731 22:38:04.867888 20406 solver.cpp:375]     Train net output #0: loss = 0.0869399 (* 1 = 0.0869399 loss)
I0731 22:38:04.867895 20406 sgd_solver.cpp:136] Iteration 19000, lr = 1e-05, m = 0.9
I0731 22:38:23.613698 20406 solver.cpp:353] Iteration 19100 (5.33466 iter/s, 18.7453s/100 iter), loss = 0.0527304
I0731 22:38:23.613718 20406 solver.cpp:375]     Train net output #0: loss = 0.0527303 (* 1 = 0.0527303 loss)
I0731 22:38:23.613723 20406 sgd_solver.cpp:136] Iteration 19100, lr = 1e-05, m = 0.9
I0731 22:38:36.460886 20415 data_reader.cpp:264] Starting prefetch of epoch 8
I0731 22:38:42.182375 20406 solver.cpp:353] Iteration 19200 (5.38556 iter/s, 18.5682s/100 iter), loss = 0.0822689
I0731 22:38:42.182399 20406 solver.cpp:375]     Train net output #0: loss = 0.0822688 (* 1 = 0.0822688 loss)
I0731 22:38:42.182402 20406 sgd_solver.cpp:136] Iteration 19200, lr = 1e-05, m = 0.9
I0731 22:39:00.806689 20406 solver.cpp:353] Iteration 19300 (5.36948 iter/s, 18.6238s/100 iter), loss = 0.0880464
I0731 22:39:00.806715 20406 solver.cpp:375]     Train net output #0: loss = 0.0880463 (* 1 = 0.0880463 loss)
I0731 22:39:00.806722 20406 sgd_solver.cpp:136] Iteration 19300, lr = 1e-05, m = 0.9
I0731 22:39:19.339581 20406 solver.cpp:353] Iteration 19400 (5.39596 iter/s, 18.5324s/100 iter), loss = 0.0917561
I0731 22:39:19.339658 20406 solver.cpp:375]     Train net output #0: loss = 0.091756 (* 1 = 0.091756 loss)
I0731 22:39:19.339663 20406 sgd_solver.cpp:136] Iteration 19400, lr = 1e-05, m = 0.9
I0731 22:39:37.639943 20415 data_reader.cpp:264] Starting prefetch of epoch 9
I0731 22:39:37.813586 20406 solver.cpp:353] Iteration 19500 (5.41316 iter/s, 18.4735s/100 iter), loss = 0.0708254
I0731 22:39:37.813611 20406 solver.cpp:375]     Train net output #0: loss = 0.0708253 (* 1 = 0.0708253 loss)
I0731 22:39:37.813616 20406 sgd_solver.cpp:136] Iteration 19500, lr = 1e-05, m = 0.9
I0731 22:39:56.331344 20406 solver.cpp:353] Iteration 19600 (5.40037 iter/s, 18.5172s/100 iter), loss = 0.0813626
I0731 22:39:56.331430 20406 solver.cpp:375]     Train net output #0: loss = 0.0813625 (* 1 = 0.0813625 loss)
I0731 22:39:56.331437 20406 sgd_solver.cpp:136] Iteration 19600, lr = 1e-05, m = 0.9
I0731 22:40:08.487987 20410 data_reader.cpp:264] Starting prefetch of epoch 7
I0731 22:40:14.975004 20406 solver.cpp:353] Iteration 19700 (5.3639 iter/s, 18.6431s/100 iter), loss = 0.369294
I0731 22:40:14.975029 20406 solver.cpp:375]     Train net output #0: loss = 0.369294 (* 1 = 0.369294 loss)
I0731 22:40:14.975033 20406 sgd_solver.cpp:136] Iteration 19700, lr = 1e-05, m = 0.9
I0731 22:40:33.464076 20406 solver.cpp:353] Iteration 19800 (5.40875 iter/s, 18.4886s/100 iter), loss = 0.0685171
I0731 22:40:33.464128 20406 solver.cpp:375]     Train net output #0: loss = 0.068517 (* 1 = 0.068517 loss)
I0731 22:40:33.464133 20406 sgd_solver.cpp:136] Iteration 19800, lr = 1e-05, m = 0.9
I0731 22:40:53.869384 20406 solver.cpp:353] Iteration 19900 (4.90082 iter/s, 20.4047s/100 iter), loss = 0.0647307
I0731 22:40:53.869426 20406 solver.cpp:375]     Train net output #0: loss = 0.0647306 (* 1 = 0.0647306 loss)
I0731 22:40:53.869433 20406 sgd_solver.cpp:136] Iteration 19900, lr = 1e-05, m = 0.9
I0731 22:41:13.531352 20415 data_reader.cpp:264] Starting prefetch of epoch 10
I0731 22:41:14.272343 20406 solver.cpp:680] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/sparse/cityscapes5_jsegnet21v2_iter_20000.caffemodel
I0731 22:41:14.435808 20406 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/sparse/cityscapes5_jsegnet21v2_iter_20000.solverstate
I0731 22:41:14.447885 20406 solver.cpp:404] Sparsity after update:
I0731 22:41:14.452733 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 22:41:14.452751 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 22:41:14.452760 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 22:41:14.452764 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 22:41:14.452766 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 22:41:14.452770 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 22:41:14.452774 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 22:41:14.452776 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 22:41:14.452780 20406 net.cpp:2270] out3a_param_0(0) 
I0731 22:41:14.452782 20406 net.cpp:2270] out5a_param_0(0) 
I0731 22:41:14.452785 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 22:41:14.452788 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 22:41:14.452792 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 22:41:14.452795 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 22:41:14.452806 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 22:41:14.452811 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 22:41:14.452821 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 22:41:14.452823 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 22:41:14.452826 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 22:41:14.452841 20406 solver.cpp:550] Iteration 20000, Testing net (#0)
I0731 22:41:27.334867 20404 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 22:41:27.933938 20406 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.951425
I0731 22:41:27.933964 20406 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999856
I0731 22:41:27.933969 20406 solver.cpp:635]     Test net output #2: loss = 0.17009 (* 1 = 0.17009 loss)
I0731 22:41:27.933997 20406 solver.cpp:305] [MultiGPU] Tests completed in 13.4808s
I0731 22:41:28.136939 20406 solver.cpp:353] Iteration 20000 (2.91829 iter/s, 34.2666s/100 iter), loss = 0.0482651
I0731 22:41:28.136967 20406 solver.cpp:375]     Train net output #0: loss = 0.048265 (* 1 = 0.048265 loss)
I0731 22:41:28.136975 20406 sgd_solver.cpp:136] Iteration 20000, lr = 1e-05, m = 0.9
I0731 22:41:46.601910 20406 solver.cpp:353] Iteration 20100 (5.41581 iter/s, 18.4645s/100 iter), loss = 0.0427278
I0731 22:41:46.601992 20406 solver.cpp:375]     Train net output #0: loss = 0.0427277 (* 1 = 0.0427277 loss)
I0731 22:41:46.602000 20406 sgd_solver.cpp:136] Iteration 20100, lr = 1e-05, m = 0.9
I0731 22:41:57.773510 20410 data_reader.cpp:264] Starting prefetch of epoch 8
I0731 22:42:05.101105 20406 solver.cpp:353] Iteration 20200 (5.40579 iter/s, 18.4987s/100 iter), loss = 0.0872486
I0731 22:42:05.101130 20406 solver.cpp:375]     Train net output #0: loss = 0.0872486 (* 1 = 0.0872486 loss)
I0731 22:42:05.101133 20406 sgd_solver.cpp:136] Iteration 20200, lr = 1e-05, m = 0.9
I0731 22:42:23.637199 20406 solver.cpp:353] Iteration 20300 (5.39503 iter/s, 18.5356s/100 iter), loss = 0.0626739
I0731 22:42:23.637256 20406 solver.cpp:375]     Train net output #0: loss = 0.0626738 (* 1 = 0.0626738 loss)
I0731 22:42:23.637261 20406 sgd_solver.cpp:136] Iteration 20300, lr = 1e-05, m = 0.9
I0731 22:42:42.181447 20406 solver.cpp:353] Iteration 20400 (5.39266 iter/s, 18.5437s/100 iter), loss = 0.0671409
I0731 22:42:42.181478 20406 solver.cpp:375]     Train net output #0: loss = 0.0671408 (* 1 = 0.0671408 loss)
I0731 22:42:42.181483 20406 sgd_solver.cpp:136] Iteration 20400, lr = 1e-05, m = 0.9
I0731 22:42:59.160432 20412 data_reader.cpp:264] Starting prefetch of epoch 16
I0731 22:43:00.822048 20406 solver.cpp:353] Iteration 20500 (5.36478 iter/s, 18.6401s/100 iter), loss = 0.0578302
I0731 22:43:00.822073 20406 solver.cpp:375]     Train net output #0: loss = 0.0578301 (* 1 = 0.0578301 loss)
I0731 22:43:00.822078 20406 sgd_solver.cpp:136] Iteration 20500, lr = 1e-05, m = 0.9
I0731 22:43:19.306592 20406 solver.cpp:353] Iteration 20600 (5.41008 iter/s, 18.484s/100 iter), loss = 0.0548095
I0731 22:43:19.306614 20406 solver.cpp:375]     Train net output #0: loss = 0.0548094 (* 1 = 0.0548094 loss)
I0731 22:43:19.306619 20406 sgd_solver.cpp:136] Iteration 20600, lr = 1e-05, m = 0.9
I0731 22:43:37.853991 20406 solver.cpp:353] Iteration 20700 (5.39174 iter/s, 18.5469s/100 iter), loss = 0.0465325
I0731 22:43:37.854049 20406 solver.cpp:375]     Train net output #0: loss = 0.0465325 (* 1 = 0.0465325 loss)
I0731 22:43:37.854054 20406 sgd_solver.cpp:136] Iteration 20700, lr = 1e-05, m = 0.9
I0731 22:43:56.429612 20406 solver.cpp:353] Iteration 20800 (5.38355 iter/s, 18.5751s/100 iter), loss = 0.0410007
I0731 22:43:56.429636 20406 solver.cpp:375]     Train net output #0: loss = 0.0410006 (* 1 = 0.0410006 loss)
I0731 22:43:56.429641 20406 sgd_solver.cpp:136] Iteration 20800, lr = 1e-05, m = 0.9
I0731 22:44:00.290374 20364 data_reader.cpp:264] Starting prefetch of epoch 16
I0731 22:44:14.771960 20406 solver.cpp:353] Iteration 20900 (5.45201 iter/s, 18.3418s/100 iter), loss = 0.0490235
I0731 22:44:14.772075 20406 solver.cpp:375]     Train net output #0: loss = 0.0490234 (* 1 = 0.0490234 loss)
I0731 22:44:14.772083 20406 sgd_solver.cpp:136] Iteration 20900, lr = 1e-05, m = 0.9
I0731 22:44:30.925614 20346 data_reader.cpp:264] Starting prefetch of epoch 13
I0731 22:44:33.206984 20406 solver.cpp:404] Sparsity after update:
I0731 22:44:33.228220 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 22:44:33.228322 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 22:44:33.228350 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 22:44:33.228368 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 22:44:33.228384 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 22:44:33.228399 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 22:44:33.228415 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 22:44:33.228428 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 22:44:33.228440 20406 net.cpp:2270] out3a_param_0(0) 
I0731 22:44:33.228461 20406 net.cpp:2270] out5a_param_0(0) 
I0731 22:44:33.228474 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 22:44:33.228488 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 22:44:33.228502 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 22:44:33.228515 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 22:44:33.228529 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 22:44:33.228544 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 22:44:33.228559 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 22:44:33.228572 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 22:44:33.228585 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 22:44:33.401351 20406 solver.cpp:353] Iteration 21000 (5.36801 iter/s, 18.6289s/100 iter), loss = 0.0754625
I0731 22:44:33.401377 20406 solver.cpp:375]     Train net output #0: loss = 0.0754624 (* 1 = 0.0754624 loss)
I0731 22:44:33.401381 20406 sgd_solver.cpp:136] Iteration 21000, lr = 1e-05, m = 0.9
I0731 22:44:51.931226 20406 solver.cpp:353] Iteration 21100 (5.39684 iter/s, 18.5294s/100 iter), loss = 0.053391
I0731 22:44:51.931298 20406 solver.cpp:375]     Train net output #0: loss = 0.0533909 (* 1 = 0.0533909 loss)
I0731 22:44:51.931304 20406 sgd_solver.cpp:136] Iteration 21100, lr = 1e-05, m = 0.9
I0731 22:45:10.558574 20406 solver.cpp:353] Iteration 21200 (5.3686 iter/s, 18.6268s/100 iter), loss = 0.0628085
I0731 22:45:10.558598 20406 solver.cpp:375]     Train net output #0: loss = 0.0628085 (* 1 = 0.0628085 loss)
I0731 22:45:10.558603 20406 sgd_solver.cpp:136] Iteration 21200, lr = 1e-05, m = 0.9
I0731 22:45:29.837936 20406 solver.cpp:353] Iteration 21300 (5.18704 iter/s, 19.2788s/100 iter), loss = 0.0781118
I0731 22:45:29.838060 20406 solver.cpp:375]     Train net output #0: loss = 0.0781117 (* 1 = 0.0781117 loss)
I0731 22:45:29.838075 20406 sgd_solver.cpp:136] Iteration 21300, lr = 1e-05, m = 0.9
I0731 22:45:33.198043 20410 data_reader.cpp:264] Starting prefetch of epoch 9
I0731 22:45:49.820979 20406 solver.cpp:353] Iteration 21400 (5.00438 iter/s, 19.9825s/100 iter), loss = 0.0560582
I0731 22:45:49.821043 20406 solver.cpp:375]     Train net output #0: loss = 0.0560582 (* 1 = 0.0560582 loss)
I0731 22:45:49.821053 20406 sgd_solver.cpp:136] Iteration 21400, lr = 1e-05, m = 0.9
I0731 22:46:09.979542 20406 solver.cpp:353] Iteration 21500 (4.96081 iter/s, 20.158s/100 iter), loss = 0.0467375
I0731 22:46:09.979588 20406 solver.cpp:375]     Train net output #0: loss = 0.0467374 (* 1 = 0.0467374 loss)
I0731 22:46:09.979595 20406 sgd_solver.cpp:136] Iteration 21500, lr = 1e-05, m = 0.9
I0731 22:46:30.000568 20406 solver.cpp:353] Iteration 21600 (4.99489 iter/s, 20.0205s/100 iter), loss = 0.0725782
I0731 22:46:30.000622 20406 solver.cpp:375]     Train net output #0: loss = 0.0725781 (* 1 = 0.0725781 loss)
I0731 22:46:30.000636 20406 sgd_solver.cpp:136] Iteration 21600, lr = 1e-05, m = 0.9
I0731 22:46:39.206610 20412 data_reader.cpp:264] Starting prefetch of epoch 17
I0731 22:46:49.520536 20406 solver.cpp:353] Iteration 21700 (5.1231 iter/s, 19.5194s/100 iter), loss = 0.0536072
I0731 22:46:49.520612 20406 solver.cpp:375]     Train net output #0: loss = 0.0536071 (* 1 = 0.0536071 loss)
I0731 22:46:49.520617 20406 sgd_solver.cpp:136] Iteration 21700, lr = 1e-05, m = 0.9
I0731 22:47:09.719588 20406 solver.cpp:353] Iteration 21800 (4.95087 iter/s, 20.1985s/100 iter), loss = 0.0601684
I0731 22:47:09.719632 20406 solver.cpp:375]     Train net output #0: loss = 0.0601683 (* 1 = 0.0601683 loss)
I0731 22:47:09.719642 20406 sgd_solver.cpp:136] Iteration 21800, lr = 1e-05, m = 0.9
I0731 22:47:12.323323 20410 data_reader.cpp:264] Starting prefetch of epoch 10
I0731 22:47:29.547864 20406 solver.cpp:353] Iteration 21900 (5.04344 iter/s, 19.8277s/100 iter), loss = 0.0651862
I0731 22:47:29.547924 20406 solver.cpp:375]     Train net output #0: loss = 0.0651861 (* 1 = 0.0651861 loss)
I0731 22:47:29.547931 20406 sgd_solver.cpp:136] Iteration 21900, lr = 1e-05, m = 0.9
I0731 22:47:48.024570 20406 solver.cpp:404] Sparsity after update:
I0731 22:47:48.034515 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 22:47:48.034550 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 22:47:48.034559 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 22:47:48.034561 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 22:47:48.034564 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 22:47:48.034569 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 22:47:48.034571 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 22:47:48.034574 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 22:47:48.034577 20406 net.cpp:2270] out3a_param_0(0) 
I0731 22:47:48.034580 20406 net.cpp:2270] out5a_param_0(0) 
I0731 22:47:48.034584 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 22:47:48.034586 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 22:47:48.034590 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 22:47:48.034593 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 22:47:48.034597 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 22:47:48.034600 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 22:47:48.034616 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 22:47:48.034622 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 22:47:48.034626 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 22:47:48.034642 20406 solver.cpp:550] Iteration 22000, Testing net (#0)
I0731 22:47:55.072757 20434 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 22:47:59.249797 20406 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.952781
I0731 22:47:59.249821 20406 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999381
I0731 22:47:59.249827 20406 solver.cpp:635]     Test net output #2: loss = 0.20424 (* 1 = 0.20424 loss)
I0731 22:47:59.249900 20406 solver.cpp:305] [MultiGPU] Tests completed in 11.2149s
I0731 22:47:59.460201 20406 solver.cpp:353] Iteration 22000 (3.34319 iter/s, 29.9115s/100 iter), loss = 0.0811862
I0731 22:47:59.460227 20406 solver.cpp:375]     Train net output #0: loss = 0.0811861 (* 1 = 0.0811861 loss)
I0731 22:47:59.460232 20406 sgd_solver.cpp:136] Iteration 22000, lr = 1e-05, m = 0.9
I0731 22:48:18.025182 20406 solver.cpp:353] Iteration 22100 (5.38664 iter/s, 18.5645s/100 iter), loss = 0.0532882
I0731 22:48:18.025312 20406 solver.cpp:375]     Train net output #0: loss = 0.0532881 (* 1 = 0.0532881 loss)
I0731 22:48:18.025319 20406 sgd_solver.cpp:136] Iteration 22100, lr = 1e-05, m = 0.9
I0731 22:48:36.638259 20406 solver.cpp:353] Iteration 22200 (5.37272 iter/s, 18.6126s/100 iter), loss = 0.0716118
I0731 22:48:36.638283 20406 solver.cpp:375]     Train net output #0: loss = 0.0716117 (* 1 = 0.0716117 loss)
I0731 22:48:36.638288 20406 sgd_solver.cpp:136] Iteration 22200, lr = 1e-05, m = 0.9
I0731 22:48:55.112478 20406 solver.cpp:353] Iteration 22300 (5.4131 iter/s, 18.4737s/100 iter), loss = 0.0447723
I0731 22:48:55.112567 20406 solver.cpp:375]     Train net output #0: loss = 0.0447722 (* 1 = 0.0447722 loss)
I0731 22:48:55.112574 20406 sgd_solver.cpp:136] Iteration 22300, lr = 1e-05, m = 0.9
I0731 22:48:56.806146 20415 data_reader.cpp:264] Starting prefetch of epoch 11
I0731 22:49:13.789340 20406 solver.cpp:353] Iteration 22400 (5.35437 iter/s, 18.6763s/100 iter), loss = 0.0478767
I0731 22:49:13.789366 20406 solver.cpp:375]     Train net output #0: loss = 0.0478766 (* 1 = 0.0478766 loss)
I0731 22:49:13.789369 20406 sgd_solver.cpp:136] Iteration 22400, lr = 1e-05, m = 0.9
I0731 22:49:27.554797 20346 data_reader.cpp:264] Starting prefetch of epoch 14
I0731 22:49:32.329690 20406 solver.cpp:353] Iteration 22500 (5.39379 iter/s, 18.5398s/100 iter), loss = 0.0677726
I0731 22:49:32.329720 20406 solver.cpp:375]     Train net output #0: loss = 0.0677725 (* 1 = 0.0677725 loss)
I0731 22:49:32.329725 20406 sgd_solver.cpp:136] Iteration 22500, lr = 1e-05, m = 0.9
I0731 22:49:50.977135 20406 solver.cpp:353] Iteration 22600 (5.36281 iter/s, 18.6469s/100 iter), loss = 0.0441089
I0731 22:49:50.977160 20406 solver.cpp:375]     Train net output #0: loss = 0.0441088 (* 1 = 0.0441088 loss)
I0731 22:49:50.977165 20406 sgd_solver.cpp:136] Iteration 22600, lr = 1e-05, m = 0.9
I0731 22:50:09.575695 20406 solver.cpp:353] Iteration 22700 (5.37691 iter/s, 18.598s/100 iter), loss = 0.0715134
I0731 22:50:09.575793 20406 solver.cpp:375]     Train net output #0: loss = 0.0715133 (* 1 = 0.0715133 loss)
I0731 22:50:09.575798 20406 sgd_solver.cpp:136] Iteration 22700, lr = 1e-05, m = 0.9
I0731 22:50:28.180476 20406 solver.cpp:353] Iteration 22800 (5.37511 iter/s, 18.6043s/100 iter), loss = 0.041768
I0731 22:50:28.180500 20406 solver.cpp:375]     Train net output #0: loss = 0.0417679 (* 1 = 0.0417679 loss)
I0731 22:50:28.180505 20406 sgd_solver.cpp:136] Iteration 22800, lr = 1e-05, m = 0.9
I0731 22:50:29.157249 20346 data_reader.cpp:264] Starting prefetch of epoch 15
I0731 22:50:46.715844 20406 solver.cpp:353] Iteration 22900 (5.39524 iter/s, 18.5349s/100 iter), loss = 0.069175
I0731 22:50:46.715899 20406 solver.cpp:375]     Train net output #0: loss = 0.0691749 (* 1 = 0.0691749 loss)
I0731 22:50:46.715904 20406 sgd_solver.cpp:136] Iteration 22900, lr = 1e-05, m = 0.9
I0731 22:51:05.092069 20406 solver.cpp:404] Sparsity after update:
I0731 22:51:05.121847 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 22:51:05.121865 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 22:51:05.121871 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 22:51:05.121873 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 22:51:05.121876 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 22:51:05.121877 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 22:51:05.121879 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 22:51:05.121881 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 22:51:05.121883 20406 net.cpp:2270] out3a_param_0(0) 
I0731 22:51:05.121886 20406 net.cpp:2270] out5a_param_0(0) 
I0731 22:51:05.121887 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 22:51:05.121889 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 22:51:05.121891 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 22:51:05.121893 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 22:51:05.121896 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 22:51:05.121897 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 22:51:05.121899 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 22:51:05.121901 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 22:51:05.121902 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 22:51:05.290372 20406 solver.cpp:353] Iteration 23000 (5.38387 iter/s, 18.574s/100 iter), loss = 0.0840957
I0731 22:51:05.290400 20406 solver.cpp:375]     Train net output #0: loss = 0.0840956 (* 1 = 0.0840956 loss)
I0731 22:51:05.290405 20406 sgd_solver.cpp:136] Iteration 23000, lr = 1e-05, m = 0.9
I0731 22:51:23.827229 20406 solver.cpp:353] Iteration 23100 (5.39481 iter/s, 18.5363s/100 iter), loss = 0.0524736
I0731 22:51:23.827309 20406 solver.cpp:375]     Train net output #0: loss = 0.0524735 (* 1 = 0.0524735 loss)
I0731 22:51:23.827317 20406 sgd_solver.cpp:136] Iteration 23100, lr = 1e-05, m = 0.9
I0731 22:51:30.367539 20412 data_reader.cpp:264] Starting prefetch of epoch 18
I0731 22:51:42.403483 20406 solver.cpp:353] Iteration 23200 (5.38336 iter/s, 18.5757s/100 iter), loss = 0.0484776
I0731 22:51:42.403508 20406 solver.cpp:375]     Train net output #0: loss = 0.0484775 (* 1 = 0.0484775 loss)
I0731 22:51:42.403513 20406 sgd_solver.cpp:136] Iteration 23200, lr = 1e-05, m = 0.9
I0731 22:52:01.056681 20406 solver.cpp:353] Iteration 23300 (5.36116 iter/s, 18.6527s/100 iter), loss = 0.0394119
I0731 22:52:01.056736 20406 solver.cpp:375]     Train net output #0: loss = 0.0394118 (* 1 = 0.0394118 loss)
I0731 22:52:01.056741 20406 sgd_solver.cpp:136] Iteration 23300, lr = 1e-05, m = 0.9
I0731 22:52:01.272927 20410 data_reader.cpp:264] Starting prefetch of epoch 11
I0731 22:52:19.559386 20406 solver.cpp:353] Iteration 23400 (5.40476 iter/s, 18.5022s/100 iter), loss = 0.0518391
I0731 22:52:19.559412 20406 solver.cpp:375]     Train net output #0: loss = 0.051839 (* 1 = 0.051839 loss)
I0731 22:52:19.559417 20406 sgd_solver.cpp:136] Iteration 23400, lr = 1e-05, m = 0.9
I0731 22:52:38.250367 20406 solver.cpp:353] Iteration 23500 (5.35032 iter/s, 18.6905s/100 iter), loss = 0.18031
I0731 22:52:38.250493 20406 solver.cpp:375]     Train net output #0: loss = 0.18031 (* 1 = 0.18031 loss)
I0731 22:52:38.250499 20406 sgd_solver.cpp:136] Iteration 23500, lr = 1e-05, m = 0.9
I0731 22:52:56.872737 20406 solver.cpp:353] Iteration 23600 (5.37003 iter/s, 18.6219s/100 iter), loss = 0.0412659
I0731 22:52:56.872766 20406 solver.cpp:375]     Train net output #0: loss = 0.0412658 (* 1 = 0.0412658 loss)
I0731 22:52:56.872771 20406 sgd_solver.cpp:136] Iteration 23600, lr = 1e-05, m = 0.9
I0731 22:53:02.645288 20410 data_reader.cpp:264] Starting prefetch of epoch 12
I0731 22:53:15.428835 20406 solver.cpp:353] Iteration 23700 (5.38922 iter/s, 18.5556s/100 iter), loss = 0.063004
I0731 22:53:15.428905 20406 solver.cpp:375]     Train net output #0: loss = 0.063004 (* 1 = 0.063004 loss)
I0731 22:53:15.428911 20406 sgd_solver.cpp:136] Iteration 23700, lr = 1e-05, m = 0.9
I0731 22:53:33.985683 20406 solver.cpp:353] Iteration 23800 (5.38899 iter/s, 18.5564s/100 iter), loss = 0.0878488
I0731 22:53:33.985707 20406 solver.cpp:375]     Train net output #0: loss = 0.0878487 (* 1 = 0.0878487 loss)
I0731 22:53:33.985711 20406 sgd_solver.cpp:136] Iteration 23800, lr = 1e-05, m = 0.9
I0731 22:53:52.686878 20406 solver.cpp:353] Iteration 23900 (5.3474 iter/s, 18.7007s/100 iter), loss = 0.0586963
I0731 22:53:52.686924 20406 solver.cpp:375]     Train net output #0: loss = 0.0586963 (* 1 = 0.0586963 loss)
I0731 22:53:52.686929 20406 sgd_solver.cpp:136] Iteration 23900, lr = 1e-05, m = 0.9
I0731 22:54:04.263317 20364 data_reader.cpp:264] Starting prefetch of epoch 17
I0731 22:54:11.075690 20406 solver.cpp:404] Sparsity after update:
I0731 22:54:11.086776 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 22:54:11.086792 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 22:54:11.086799 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 22:54:11.086802 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 22:54:11.086804 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 22:54:11.086805 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 22:54:11.086807 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 22:54:11.086809 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 22:54:11.086812 20406 net.cpp:2270] out3a_param_0(0) 
I0731 22:54:11.086813 20406 net.cpp:2270] out5a_param_0(0) 
I0731 22:54:11.086815 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 22:54:11.086817 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 22:54:11.086820 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 22:54:11.086822 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 22:54:11.086824 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 22:54:11.086827 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 22:54:11.086828 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 22:54:11.086829 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 22:54:11.086832 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 22:54:11.086840 20406 solver.cpp:550] Iteration 24000, Testing net (#0)
I0731 22:54:14.640410 20402 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 22:54:22.551545 20406 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.951041
I0731 22:54:22.551563 20406 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999844
I0731 22:54:22.551568 20406 solver.cpp:635]     Test net output #2: loss = 0.169531 (* 1 = 0.169531 loss)
I0731 22:54:22.551590 20406 solver.cpp:305] [MultiGPU] Tests completed in 11.4644s
I0731 22:54:22.637689 20461 sgd_solver.cpp:48] MultiStep Status: Iteration 24000, step = 1
I0731 22:54:22.637689 20460 sgd_solver.cpp:48] MultiStep Status: Iteration 24000, step = 1
I0731 22:54:22.637689 20459 sgd_solver.cpp:48] MultiStep Status: Iteration 24000, step = 1
I0731 22:54:22.747530 20406 solver.cpp:353] Iteration 24000 (3.3267 iter/s, 30.0598s/100 iter), loss = 0.0782841
I0731 22:54:22.747660 20406 solver.cpp:375]     Train net output #0: loss = 0.0782841 (* 1 = 0.0782841 loss)
I0731 22:54:22.747670 20406 sgd_solver.cpp:136] Iteration 24000, lr = 1e-06, m = 0.9
I0731 22:54:41.289474 20406 solver.cpp:353] Iteration 24100 (5.39333 iter/s, 18.5414s/100 iter), loss = 0.06716
I0731 22:54:41.289496 20406 solver.cpp:375]     Train net output #0: loss = 0.0671599 (* 1 = 0.0671599 loss)
I0731 22:54:41.289501 20406 sgd_solver.cpp:136] Iteration 24100, lr = 1e-06, m = 0.9
I0731 22:54:46.338245 20346 data_reader.cpp:264] Starting prefetch of epoch 16
I0731 22:54:59.978909 20406 solver.cpp:353] Iteration 24200 (5.35077 iter/s, 18.6889s/100 iter), loss = 0.0816253
I0731 22:54:59.978961 20406 solver.cpp:375]     Train net output #0: loss = 0.0816252 (* 1 = 0.0816252 loss)
I0731 22:54:59.978966 20406 sgd_solver.cpp:136] Iteration 24200, lr = 1e-06, m = 0.9
I0731 22:55:18.596159 20406 solver.cpp:353] Iteration 24300 (5.37151 iter/s, 18.6167s/100 iter), loss = 0.0532994
I0731 22:55:18.596184 20406 solver.cpp:375]     Train net output #0: loss = 0.0532994 (* 1 = 0.0532994 loss)
I0731 22:55:18.596190 20406 sgd_solver.cpp:136] Iteration 24300, lr = 1e-06, m = 0.9
I0731 22:55:37.129906 20406 solver.cpp:353] Iteration 24400 (5.39571 iter/s, 18.5332s/100 iter), loss = 0.0701097
I0731 22:55:37.129986 20406 solver.cpp:375]     Train net output #0: loss = 0.0701096 (* 1 = 0.0701096 loss)
I0731 22:55:37.129993 20406 sgd_solver.cpp:136] Iteration 24400, lr = 1e-06, m = 0.9
I0731 22:55:47.934613 20364 data_reader.cpp:264] Starting prefetch of epoch 18
I0731 22:55:55.701972 20406 solver.cpp:353] Iteration 24500 (5.38458 iter/s, 18.5716s/100 iter), loss = 0.0492014
I0731 22:55:55.702000 20406 solver.cpp:375]     Train net output #0: loss = 0.0492013 (* 1 = 0.0492013 loss)
I0731 22:55:55.702004 20406 sgd_solver.cpp:136] Iteration 24500, lr = 1e-06, m = 0.9
I0731 22:56:14.341500 20406 solver.cpp:353] Iteration 24600 (5.36509 iter/s, 18.639s/100 iter), loss = 0.092836
I0731 22:56:14.341547 20406 solver.cpp:375]     Train net output #0: loss = 0.0928359 (* 1 = 0.0928359 loss)
I0731 22:56:14.341552 20406 sgd_solver.cpp:136] Iteration 24600, lr = 1e-06, m = 0.9
I0731 22:56:18.583927 20410 data_reader.cpp:264] Starting prefetch of epoch 13
I0731 22:56:32.943590 20406 solver.cpp:353] Iteration 24700 (5.37589 iter/s, 18.6016s/100 iter), loss = 0.0660087
I0731 22:56:32.943615 20406 solver.cpp:375]     Train net output #0: loss = 0.0660086 (* 1 = 0.0660086 loss)
I0731 22:56:32.943619 20406 sgd_solver.cpp:136] Iteration 24700, lr = 1e-06, m = 0.9
I0731 22:56:51.510804 20406 solver.cpp:353] Iteration 24800 (5.38599 iter/s, 18.5667s/100 iter), loss = 0.0936572
I0731 22:56:51.510892 20406 solver.cpp:375]     Train net output #0: loss = 0.0936571 (* 1 = 0.0936571 loss)
I0731 22:56:51.510900 20406 sgd_solver.cpp:136] Iteration 24800, lr = 1e-06, m = 0.9
I0731 22:57:10.346500 20406 solver.cpp:353] Iteration 24900 (5.30922 iter/s, 18.8352s/100 iter), loss = 0.0609188
I0731 22:57:10.346542 20406 solver.cpp:375]     Train net output #0: loss = 0.0609187 (* 1 = 0.0609187 loss)
I0731 22:57:10.346555 20406 sgd_solver.cpp:136] Iteration 24900, lr = 1e-06, m = 0.9
I0731 22:57:21.180764 20409 data_reader.cpp:264] Starting prefetch of epoch 14
I0731 22:57:30.351199 20406 solver.cpp:404] Sparsity after update:
I0731 22:57:30.370857 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 22:57:30.370964 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 22:57:30.370990 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 22:57:30.371001 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 22:57:30.371008 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 22:57:30.371016 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 22:57:30.371026 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 22:57:30.371034 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 22:57:30.371043 20406 net.cpp:2270] out3a_param_0(0) 
I0731 22:57:30.371053 20406 net.cpp:2270] out5a_param_0(0) 
I0731 22:57:30.371062 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 22:57:30.371071 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 22:57:30.371080 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 22:57:30.371099 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 22:57:30.371109 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 22:57:30.371117 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 22:57:30.371126 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 22:57:30.371135 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 22:57:30.371145 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 22:57:30.545156 20406 solver.cpp:353] Iteration 25000 (4.95096 iter/s, 20.1981s/100 iter), loss = 0.0676049
I0731 22:57:30.545184 20406 solver.cpp:375]     Train net output #0: loss = 0.0676048 (* 1 = 0.0676048 loss)
I0731 22:57:30.545192 20406 sgd_solver.cpp:136] Iteration 25000, lr = 1e-06, m = 0.9
I0731 22:57:51.021121 20406 solver.cpp:353] Iteration 25100 (4.88391 iter/s, 20.4754s/100 iter), loss = 0.055798
I0731 22:57:51.021186 20406 solver.cpp:375]     Train net output #0: loss = 0.0557979 (* 1 = 0.0557979 loss)
I0731 22:57:51.021204 20406 sgd_solver.cpp:136] Iteration 25100, lr = 1e-06, m = 0.9
I0731 22:58:11.256671 20406 solver.cpp:353] Iteration 25200 (4.94193 iter/s, 20.235s/100 iter), loss = 0.0796873
I0731 22:58:11.256744 20406 solver.cpp:375]     Train net output #0: loss = 0.0796872 (* 1 = 0.0796872 loss)
I0731 22:58:11.256749 20406 sgd_solver.cpp:136] Iteration 25200, lr = 1e-06, m = 0.9
I0731 22:58:27.825883 20415 data_reader.cpp:264] Starting prefetch of epoch 12
I0731 22:58:31.032615 20406 solver.cpp:353] Iteration 25300 (5.05679 iter/s, 19.7754s/100 iter), loss = 0.0755685
I0731 22:58:31.032658 20406 solver.cpp:375]     Train net output #0: loss = 0.0755684 (* 1 = 0.0755684 loss)
I0731 22:58:31.032666 20406 sgd_solver.cpp:136] Iteration 25300, lr = 1e-06, m = 0.9
I0731 22:58:51.596232 20406 solver.cpp:353] Iteration 25400 (4.86309 iter/s, 20.5631s/100 iter), loss = 0.0677014
I0731 22:58:51.596313 20406 solver.cpp:375]     Train net output #0: loss = 0.0677013 (* 1 = 0.0677013 loss)
I0731 22:58:51.596320 20406 sgd_solver.cpp:136] Iteration 25400, lr = 1e-06, m = 0.9
I0731 22:59:01.249126 20409 data_reader.cpp:264] Starting prefetch of epoch 15
I0731 22:59:11.721197 20406 solver.cpp:353] Iteration 25500 (4.96909 iter/s, 20.1244s/100 iter), loss = 0.0512099
I0731 22:59:11.721256 20406 solver.cpp:375]     Train net output #0: loss = 0.0512098 (* 1 = 0.0512098 loss)
I0731 22:59:11.721284 20406 sgd_solver.cpp:136] Iteration 25500, lr = 1e-06, m = 0.9
I0731 22:59:31.261665 20406 solver.cpp:353] Iteration 25600 (5.11773 iter/s, 19.5399s/100 iter), loss = 0.0609539
I0731 22:59:31.261724 20406 solver.cpp:375]     Train net output #0: loss = 0.0609538 (* 1 = 0.0609538 loss)
I0731 22:59:31.261729 20406 sgd_solver.cpp:136] Iteration 25600, lr = 1e-06, m = 0.9
I0731 22:59:51.431061 20406 solver.cpp:353] Iteration 25700 (4.95814 iter/s, 20.1688s/100 iter), loss = 0.0438565
I0731 22:59:51.431087 20406 solver.cpp:375]     Train net output #0: loss = 0.0438564 (* 1 = 0.0438564 loss)
I0731 22:59:51.431092 20406 sgd_solver.cpp:136] Iteration 25700, lr = 1e-06, m = 0.9
I0731 23:00:07.728149 20410 data_reader.cpp:264] Starting prefetch of epoch 14
I0731 23:00:11.785522 20406 solver.cpp:353] Iteration 25800 (4.91307 iter/s, 20.3539s/100 iter), loss = 0.0662195
I0731 23:00:11.785570 20406 solver.cpp:375]     Train net output #0: loss = 0.0662193 (* 1 = 0.0662193 loss)
I0731 23:00:11.785594 20406 sgd_solver.cpp:136] Iteration 25800, lr = 1e-06, m = 0.9
I0731 23:00:31.578505 20406 solver.cpp:353] Iteration 25900 (5.05244 iter/s, 19.7924s/100 iter), loss = 0.0523201
I0731 23:00:31.578547 20406 solver.cpp:375]     Train net output #0: loss = 0.05232 (* 1 = 0.05232 loss)
I0731 23:00:31.578553 20406 sgd_solver.cpp:136] Iteration 25900, lr = 1e-06, m = 0.9
I0731 23:00:51.283259 20406 solver.cpp:404] Sparsity after update:
I0731 23:00:51.293623 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 23:00:51.293644 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 23:00:51.293651 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 23:00:51.293653 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 23:00:51.293655 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 23:00:51.293658 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 23:00:51.293659 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 23:00:51.293661 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 23:00:51.293663 20406 net.cpp:2270] out3a_param_0(0) 
I0731 23:00:51.293664 20406 net.cpp:2270] out5a_param_0(0) 
I0731 23:00:51.293666 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 23:00:51.293668 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 23:00:51.293670 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 23:00:51.293673 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 23:00:51.293675 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 23:00:51.293678 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 23:00:51.293679 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 23:00:51.293681 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 23:00:51.293684 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 23:00:51.293691 20406 solver.cpp:550] Iteration 26000, Testing net (#0)
I0731 23:00:59.216686 20402 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 23:01:03.524132 20406 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.952684
I0731 23:01:03.524160 20406 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999238
I0731 23:01:03.524165 20406 solver.cpp:635]     Test net output #2: loss = 0.208827 (* 1 = 0.208827 loss)
I0731 23:01:03.524250 20406 solver.cpp:305] [MultiGPU] Tests completed in 12.2302s
I0731 23:01:03.719925 20406 solver.cpp:353] Iteration 26000 (3.11134 iter/s, 32.1405s/100 iter), loss = 0.0822551
I0731 23:01:03.719951 20406 solver.cpp:375]     Train net output #0: loss = 0.082255 (* 1 = 0.082255 loss)
I0731 23:01:03.719956 20406 sgd_solver.cpp:136] Iteration 26000, lr = 1e-06, m = 0.9
I0731 23:01:22.442853 20406 solver.cpp:353] Iteration 26100 (5.34119 iter/s, 18.7224s/100 iter), loss = 0.0404537
I0731 23:01:22.442924 20406 solver.cpp:375]     Train net output #0: loss = 0.0404536 (* 1 = 0.0404536 loss)
I0731 23:01:22.442929 20406 sgd_solver.cpp:136] Iteration 26100, lr = 1e-06, m = 0.9
I0731 23:01:24.340495 20346 data_reader.cpp:264] Starting prefetch of epoch 17
I0731 23:01:40.903657 20406 solver.cpp:353] Iteration 26200 (5.41703 iter/s, 18.4603s/100 iter), loss = 0.0875606
I0731 23:01:40.903692 20406 solver.cpp:375]     Train net output #0: loss = 0.0875604 (* 1 = 0.0875604 loss)
I0731 23:01:40.903695 20406 sgd_solver.cpp:136] Iteration 26200, lr = 1e-06, m = 0.9
I0731 23:01:59.413755 20406 solver.cpp:353] Iteration 26300 (5.40261 iter/s, 18.5096s/100 iter), loss = 0.0730365
I0731 23:01:59.413869 20406 solver.cpp:375]     Train net output #0: loss = 0.0730364 (* 1 = 0.0730364 loss)
I0731 23:01:59.413877 20406 sgd_solver.cpp:136] Iteration 26300, lr = 1e-06, m = 0.9
I0731 23:02:17.949764 20406 solver.cpp:353] Iteration 26400 (5.39506 iter/s, 18.5355s/100 iter), loss = 0.0877131
I0731 23:02:17.949795 20406 solver.cpp:375]     Train net output #0: loss = 0.087713 (* 1 = 0.087713 loss)
I0731 23:02:17.949800 20406 sgd_solver.cpp:136] Iteration 26400, lr = 1e-06, m = 0.9
I0731 23:02:25.607731 20346 data_reader.cpp:264] Starting prefetch of epoch 18
I0731 23:02:36.422958 20406 solver.cpp:353] Iteration 26500 (5.4134 iter/s, 18.4727s/100 iter), loss = 0.0443398
I0731 23:02:36.423017 20406 solver.cpp:375]     Train net output #0: loss = 0.0443396 (* 1 = 0.0443396 loss)
I0731 23:02:36.423022 20406 sgd_solver.cpp:136] Iteration 26500, lr = 1e-06, m = 0.9
I0731 23:02:54.962013 20406 solver.cpp:353] Iteration 26600 (5.39417 iter/s, 18.5385s/100 iter), loss = 0.0731329
I0731 23:02:54.962040 20406 solver.cpp:375]     Train net output #0: loss = 0.0731328 (* 1 = 0.0731328 loss)
I0731 23:02:54.962044 20406 sgd_solver.cpp:136] Iteration 26600, lr = 1e-06, m = 0.9
I0731 23:03:13.516842 20406 solver.cpp:353] Iteration 26700 (5.38958 iter/s, 18.5543s/100 iter), loss = 0.0635741
I0731 23:03:13.516912 20406 solver.cpp:375]     Train net output #0: loss = 0.063574 (* 1 = 0.063574 loss)
I0731 23:03:13.516918 20406 sgd_solver.cpp:136] Iteration 26700, lr = 1e-06, m = 0.9
I0731 23:03:26.660473 20415 data_reader.cpp:264] Starting prefetch of epoch 13
I0731 23:03:32.080206 20406 solver.cpp:353] Iteration 26800 (5.3871 iter/s, 18.5628s/100 iter), loss = 0.0744798
I0731 23:03:32.080230 20406 solver.cpp:375]     Train net output #0: loss = 0.0744797 (* 1 = 0.0744797 loss)
I0731 23:03:32.080235 20406 sgd_solver.cpp:136] Iteration 26800, lr = 1e-06, m = 0.9
I0731 23:03:50.683495 20406 solver.cpp:353] Iteration 26900 (5.37554 iter/s, 18.6028s/100 iter), loss = 0.0530878
I0731 23:03:50.683595 20406 solver.cpp:375]     Train net output #0: loss = 0.0530877 (* 1 = 0.0530877 loss)
I0731 23:03:50.683603 20406 sgd_solver.cpp:136] Iteration 26900, lr = 1e-06, m = 0.9
I0731 23:04:08.978792 20406 solver.cpp:404] Sparsity after update:
I0731 23:04:08.997300 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 23:04:08.997370 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 23:04:08.997398 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 23:04:08.997404 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 23:04:08.997409 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 23:04:08.997413 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 23:04:08.997417 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 23:04:08.997427 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 23:04:08.997433 20406 net.cpp:2270] out3a_param_0(0) 
I0731 23:04:08.997438 20406 net.cpp:2270] out5a_param_0(0) 
I0731 23:04:08.997443 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 23:04:08.997448 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 23:04:08.997453 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 23:04:08.997457 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 23:04:08.997462 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 23:04:08.997467 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 23:04:08.997470 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 23:04:08.997474 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 23:04:08.997479 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 23:04:09.169281 20406 solver.cpp:353] Iteration 27000 (5.40971 iter/s, 18.4853s/100 iter), loss = 0.0954375
I0731 23:04:09.169303 20406 solver.cpp:375]     Train net output #0: loss = 0.0954373 (* 1 = 0.0954373 loss)
I0731 23:04:09.169307 20406 sgd_solver.cpp:136] Iteration 27000, lr = 1e-06, m = 0.9
I0731 23:04:29.191629 20406 solver.cpp:353] Iteration 27100 (4.99456 iter/s, 20.0218s/100 iter), loss = 0.0622642
I0731 23:04:29.191685 20406 solver.cpp:375]     Train net output #0: loss = 0.0622641 (* 1 = 0.0622641 loss)
I0731 23:04:29.191692 20406 sgd_solver.cpp:136] Iteration 27100, lr = 1e-06, m = 0.9
I0731 23:04:29.579996 20364 data_reader.cpp:264] Starting prefetch of epoch 19
I0731 23:04:47.817909 20406 solver.cpp:353] Iteration 27200 (5.36891 iter/s, 18.6258s/100 iter), loss = 0.0424939
I0731 23:04:47.817934 20406 solver.cpp:375]     Train net output #0: loss = 0.0424938 (* 1 = 0.0424938 loss)
I0731 23:04:47.817937 20406 sgd_solver.cpp:136] Iteration 27200, lr = 1e-06, m = 0.9
I0731 23:05:00.224440 20409 data_reader.cpp:264] Starting prefetch of epoch 16
I0731 23:05:06.329279 20406 solver.cpp:353] Iteration 27300 (5.40223 iter/s, 18.5109s/100 iter), loss = 0.0615048
I0731 23:05:06.329306 20406 solver.cpp:375]     Train net output #0: loss = 0.0615047 (* 1 = 0.0615047 loss)
I0731 23:05:06.329313 20406 sgd_solver.cpp:136] Iteration 27300, lr = 1e-06, m = 0.9
I0731 23:05:24.843576 20406 solver.cpp:353] Iteration 27400 (5.40138 iter/s, 18.5138s/100 iter), loss = 0.0924028
I0731 23:05:24.843601 20406 solver.cpp:375]     Train net output #0: loss = 0.0924027 (* 1 = 0.0924027 loss)
I0731 23:05:24.843606 20406 sgd_solver.cpp:136] Iteration 27400, lr = 1e-06, m = 0.9
I0731 23:05:43.510236 20406 solver.cpp:353] Iteration 27500 (5.35729 iter/s, 18.6661s/100 iter), loss = 0.0568531
I0731 23:05:43.510306 20406 solver.cpp:375]     Train net output #0: loss = 0.056853 (* 1 = 0.056853 loss)
I0731 23:05:43.510313 20406 sgd_solver.cpp:136] Iteration 27500, lr = 1e-06, m = 0.9
I0731 23:06:01.826253 20415 data_reader.cpp:264] Starting prefetch of epoch 14
I0731 23:06:02.175640 20406 solver.cpp:353] Iteration 27600 (5.35765 iter/s, 18.6649s/100 iter), loss = 0.0778439
I0731 23:06:02.175664 20406 solver.cpp:375]     Train net output #0: loss = 0.0778438 (* 1 = 0.0778438 loss)
I0731 23:06:02.175670 20406 sgd_solver.cpp:136] Iteration 27600, lr = 1e-06, m = 0.9
I0731 23:06:20.855353 20406 solver.cpp:353] Iteration 27700 (5.35355 iter/s, 18.6792s/100 iter), loss = 0.0780241
I0731 23:06:20.855456 20406 solver.cpp:375]     Train net output #0: loss = 0.0780239 (* 1 = 0.0780239 loss)
I0731 23:06:20.855463 20406 sgd_solver.cpp:136] Iteration 27700, lr = 1e-06, m = 0.9
I0731 23:06:39.440183 20406 solver.cpp:353] Iteration 27800 (5.38088 iter/s, 18.5843s/100 iter), loss = 0.041343
I0731 23:06:39.440208 20406 solver.cpp:375]     Train net output #0: loss = 0.0413428 (* 1 = 0.0413428 loss)
I0731 23:06:39.440212 20406 sgd_solver.cpp:136] Iteration 27800, lr = 1e-06, m = 0.9
I0731 23:06:58.042965 20406 solver.cpp:353] Iteration 27900 (5.37569 iter/s, 18.6023s/100 iter), loss = 0.0553453
I0731 23:06:58.043047 20406 solver.cpp:375]     Train net output #0: loss = 0.0553452 (* 1 = 0.0553452 loss)
I0731 23:06:58.043052 20406 sgd_solver.cpp:136] Iteration 27900, lr = 1e-06, m = 0.9
I0731 23:07:03.275290 20412 data_reader.cpp:264] Starting prefetch of epoch 19
I0731 23:07:16.524415 20406 solver.cpp:404] Sparsity after update:
I0731 23:07:16.533387 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 23:07:16.533439 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 23:07:16.533457 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 23:07:16.533463 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 23:07:16.533466 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 23:07:16.533470 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 23:07:16.533494 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 23:07:16.533504 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 23:07:16.533516 20406 net.cpp:2270] out3a_param_0(0) 
I0731 23:07:16.533529 20406 net.cpp:2270] out5a_param_0(0) 
I0731 23:07:16.533541 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 23:07:16.533555 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 23:07:16.533566 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 23:07:16.533579 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 23:07:16.533592 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 23:07:16.533605 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 23:07:16.533618 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 23:07:16.533632 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 23:07:16.533643 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 23:07:16.533671 20406 solver.cpp:550] Iteration 28000, Testing net (#0)
I0731 23:07:20.025143 20440 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 23:07:27.895768 20406 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.951344
I0731 23:07:27.895789 20406 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999745
I0731 23:07:27.895794 20406 solver.cpp:635]     Test net output #2: loss = 0.170113 (* 1 = 0.170113 loss)
I0731 23:07:27.895824 20406 solver.cpp:305] [MultiGPU] Tests completed in 11.3618s
I0731 23:07:28.114089 20406 solver.cpp:353] Iteration 28000 (3.32554 iter/s, 30.0703s/100 iter), loss = 0.0751927
I0731 23:07:28.114142 20406 solver.cpp:375]     Train net output #0: loss = 0.0751926 (* 1 = 0.0751926 loss)
I0731 23:07:28.114147 20406 sgd_solver.cpp:136] Iteration 28000, lr = 1e-06, m = 0.9
I0731 23:07:45.727375 20364 data_reader.cpp:264] Starting prefetch of epoch 20
I0731 23:07:46.813369 20406 solver.cpp:353] Iteration 28100 (5.34795 iter/s, 18.6988s/100 iter), loss = 0.0741945
I0731 23:07:46.813400 20406 solver.cpp:375]     Train net output #0: loss = 0.0741943 (* 1 = 0.0741943 loss)
I0731 23:07:46.813407 20406 sgd_solver.cpp:136] Iteration 28100, lr = 1e-06, m = 0.9
I0731 23:08:06.419543 20406 solver.cpp:353] Iteration 28200 (5.10058 iter/s, 19.6056s/100 iter), loss = 0.0701177
I0731 23:08:06.419767 20406 solver.cpp:375]     Train net output #0: loss = 0.0701175 (* 1 = 0.0701175 loss)
I0731 23:08:06.419790 20406 sgd_solver.cpp:136] Iteration 28200, lr = 1e-06, m = 0.9
I0731 23:08:25.981039 20406 solver.cpp:353] Iteration 28300 (5.11222 iter/s, 19.561s/100 iter), loss = 0.0454937
I0731 23:08:25.981067 20406 solver.cpp:375]     Train net output #0: loss = 0.0454936 (* 1 = 0.0454936 loss)
I0731 23:08:25.981073 20406 sgd_solver.cpp:136] Iteration 28300, lr = 1e-06, m = 0.9
I0731 23:08:45.471046 20406 solver.cpp:353] Iteration 28400 (5.13098 iter/s, 19.4895s/100 iter), loss = 0.0797288
I0731 23:08:45.471140 20406 solver.cpp:375]     Train net output #0: loss = 0.0797287 (* 1 = 0.0797287 loss)
I0731 23:08:45.471154 20406 sgd_solver.cpp:136] Iteration 28400, lr = 1e-06, m = 0.9
I0731 23:08:50.069488 20415 data_reader.cpp:264] Starting prefetch of epoch 15
I0731 23:09:04.742830 20406 solver.cpp:353] Iteration 28500 (5.18908 iter/s, 19.2712s/100 iter), loss = 0.0719309
I0731 23:09:04.742877 20406 solver.cpp:375]     Train net output #0: loss = 0.0719308 (* 1 = 0.0719308 loss)
I0731 23:09:04.742887 20406 sgd_solver.cpp:136] Iteration 28500, lr = 1e-06, m = 0.9
I0731 23:09:23.116390 20410 data_reader.cpp:264] Starting prefetch of epoch 15
I0731 23:09:25.056561 20406 solver.cpp:353] Iteration 28600 (4.92292 iter/s, 20.3132s/100 iter), loss = 0.0835511
I0731 23:09:25.056651 20406 solver.cpp:375]     Train net output #0: loss = 0.083551 (* 1 = 0.083551 loss)
I0731 23:09:25.056689 20406 sgd_solver.cpp:136] Iteration 28600, lr = 1e-06, m = 0.9
I0731 23:09:45.516777 20406 solver.cpp:353] Iteration 28700 (4.88767 iter/s, 20.4596s/100 iter), loss = 0.079277
I0731 23:09:45.516829 20406 solver.cpp:375]     Train net output #0: loss = 0.0792769 (* 1 = 0.0792769 loss)
I0731 23:09:45.516836 20406 sgd_solver.cpp:136] Iteration 28700, lr = 1e-06, m = 0.9
I0731 23:10:04.848291 20406 solver.cpp:353] Iteration 28800 (5.17304 iter/s, 19.331s/100 iter), loss = 0.0535298
I0731 23:10:04.848924 20406 solver.cpp:375]     Train net output #0: loss = 0.0535297 (* 1 = 0.0535297 loss)
I0731 23:10:04.848940 20406 sgd_solver.cpp:136] Iteration 28800, lr = 1e-06, m = 0.9
I0731 23:10:25.244675 20406 solver.cpp:353] Iteration 28900 (4.90297 iter/s, 20.3958s/100 iter), loss = 0.116229
I0731 23:10:25.244705 20406 solver.cpp:375]     Train net output #0: loss = 0.116228 (* 1 = 0.116228 loss)
I0731 23:10:25.244709 20406 sgd_solver.cpp:136] Iteration 28900, lr = 1e-06, m = 0.9
I0731 23:10:29.442172 20412 data_reader.cpp:264] Starting prefetch of epoch 20
I0731 23:10:45.686597 20406 solver.cpp:404] Sparsity after update:
I0731 23:10:45.741345 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 23:10:45.741401 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 23:10:45.741423 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 23:10:45.741432 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 23:10:45.741438 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 23:10:45.741444 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 23:10:45.741451 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 23:10:45.741458 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 23:10:45.741466 20406 net.cpp:2270] out3a_param_0(0) 
I0731 23:10:45.741473 20406 net.cpp:2270] out5a_param_0(0) 
I0731 23:10:45.741480 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 23:10:45.741488 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 23:10:45.741495 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 23:10:45.741502 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 23:10:45.741509 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 23:10:45.741518 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 23:10:45.741523 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 23:10:45.741530 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 23:10:45.741539 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 23:10:45.939795 20406 solver.cpp:353] Iteration 29000 (4.83219 iter/s, 20.6945s/100 iter), loss = 0.0716947
I0731 23:10:45.939849 20406 solver.cpp:375]     Train net output #0: loss = 0.0716946 (* 1 = 0.0716946 loss)
I0731 23:10:45.939865 20406 sgd_solver.cpp:136] Iteration 29000, lr = 1e-06, m = 0.9
I0731 23:11:06.273299 20406 solver.cpp:353] Iteration 29100 (4.91813 iter/s, 20.3329s/100 iter), loss = 0.0714806
I0731 23:11:06.273469 20406 solver.cpp:375]     Train net output #0: loss = 0.0714805 (* 1 = 0.0714805 loss)
I0731 23:11:06.273494 20406 sgd_solver.cpp:136] Iteration 29100, lr = 1e-06, m = 0.9
I0731 23:11:25.517832 20406 solver.cpp:353] Iteration 29200 (5.19642 iter/s, 19.244s/100 iter), loss = 0.0533539
I0731 23:11:25.517906 20406 solver.cpp:375]     Train net output #0: loss = 0.0533538 (* 1 = 0.0533538 loss)
I0731 23:11:25.517912 20406 sgd_solver.cpp:136] Iteration 29200, lr = 1e-06, m = 0.9
I0731 23:11:34.966653 20364 data_reader.cpp:264] Starting prefetch of epoch 21
I0731 23:11:43.927615 20406 solver.cpp:353] Iteration 29300 (5.43204 iter/s, 18.4093s/100 iter), loss = 0.0527052
I0731 23:11:43.927644 20406 solver.cpp:375]     Train net output #0: loss = 0.052705 (* 1 = 0.052705 loss)
I0731 23:11:43.927647 20406 sgd_solver.cpp:136] Iteration 29300, lr = 1e-06, m = 0.9
I0731 23:12:02.363013 20406 solver.cpp:353] Iteration 29400 (5.4245 iter/s, 18.4349s/100 iter), loss = 0.0487238
I0731 23:12:02.363070 20406 solver.cpp:375]     Train net output #0: loss = 0.0487237 (* 1 = 0.0487237 loss)
I0731 23:12:02.363077 20406 sgd_solver.cpp:136] Iteration 29400, lr = 1e-06, m = 0.9
I0731 23:12:05.391907 20410 data_reader.cpp:264] Starting prefetch of epoch 16
I0731 23:12:21.147363 20406 solver.cpp:353] Iteration 29500 (5.32373 iter/s, 18.7838s/100 iter), loss = 0.0572922
I0731 23:12:21.147387 20406 solver.cpp:375]     Train net output #0: loss = 0.0572921 (* 1 = 0.0572921 loss)
I0731 23:12:21.147390 20406 sgd_solver.cpp:136] Iteration 29500, lr = 1e-06, m = 0.9
I0731 23:12:39.685219 20406 solver.cpp:353] Iteration 29600 (5.39452 iter/s, 18.5373s/100 iter), loss = 0.0737677
I0731 23:12:39.685322 20406 solver.cpp:375]     Train net output #0: loss = 0.0737676 (* 1 = 0.0737676 loss)
I0731 23:12:39.685329 20406 sgd_solver.cpp:136] Iteration 29600, lr = 1e-06, m = 0.9
I0731 23:12:58.215837 20406 solver.cpp:353] Iteration 29700 (5.39662 iter/s, 18.5301s/100 iter), loss = 0.0589148
I0731 23:12:58.215862 20406 solver.cpp:375]     Train net output #0: loss = 0.0589147 (* 1 = 0.0589147 loss)
I0731 23:12:58.215867 20406 sgd_solver.cpp:136] Iteration 29700, lr = 1e-06, m = 0.9
I0731 23:13:06.802382 20409 data_reader.cpp:264] Starting prefetch of epoch 17
I0731 23:13:16.820776 20406 solver.cpp:353] Iteration 29800 (5.37507 iter/s, 18.6044s/100 iter), loss = 0.133701
I0731 23:13:16.820832 20406 solver.cpp:375]     Train net output #0: loss = 0.133701 (* 1 = 0.133701 loss)
I0731 23:13:16.820839 20406 sgd_solver.cpp:136] Iteration 29800, lr = 1e-06, m = 0.9
I0731 23:13:35.330078 20406 solver.cpp:353] Iteration 29900 (5.40284 iter/s, 18.5088s/100 iter), loss = 0.0688835
I0731 23:13:35.330104 20406 solver.cpp:375]     Train net output #0: loss = 0.0688834 (* 1 = 0.0688834 loss)
I0731 23:13:35.330111 20406 sgd_solver.cpp:136] Iteration 29900, lr = 1e-06, m = 0.9
I0731 23:13:53.562196 20406 solver.cpp:680] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/sparse/cityscapes5_jsegnet21v2_iter_30000.caffemodel
I0731 23:13:53.591804 20406 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/sparse/cityscapes5_jsegnet21v2_iter_30000.solverstate
I0731 23:13:53.600965 20406 solver.cpp:404] Sparsity after update:
I0731 23:13:53.602792 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 23:13:53.602802 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 23:13:53.602809 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 23:13:53.602811 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 23:13:53.602813 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 23:13:53.602815 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 23:13:53.602818 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 23:13:53.602819 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 23:13:53.602821 20406 net.cpp:2270] out3a_param_0(0) 
I0731 23:13:53.602823 20406 net.cpp:2270] out5a_param_0(0) 
I0731 23:13:53.602825 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 23:13:53.602828 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 23:13:53.602829 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 23:13:53.602831 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 23:13:53.602833 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 23:13:53.602835 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 23:13:53.602838 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 23:13:53.602838 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 23:13:53.602840 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 23:13:53.602852 20406 solver.cpp:550] Iteration 30000, Testing net (#0)
I0731 23:14:00.413100 20402 data_reader.cpp:264] Starting prefetch of epoch 4
I0731 23:14:04.840283 20406 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.952376
I0731 23:14:04.840312 20406 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999237
I0731 23:14:04.840318 20406 solver.cpp:635]     Test net output #2: loss = 0.209476 (* 1 = 0.209476 loss)
I0731 23:14:04.840343 20406 solver.cpp:305] [MultiGPU] Tests completed in 11.2372s
I0731 23:14:05.032958 20406 solver.cpp:353] Iteration 30000 (3.36677 iter/s, 29.7021s/100 iter), loss = 0.0730025
I0731 23:14:05.032981 20406 solver.cpp:375]     Train net output #0: loss = 0.0730024 (* 1 = 0.0730024 loss)
I0731 23:14:05.032985 20406 sgd_solver.cpp:136] Iteration 30000, lr = 1e-06, m = 0.9
I0731 23:14:19.475546 20346 data_reader.cpp:264] Starting prefetch of epoch 19
I0731 23:14:23.651366 20406 solver.cpp:353] Iteration 30100 (5.37118 iter/s, 18.6179s/100 iter), loss = 0.0565624
I0731 23:14:23.651448 20406 solver.cpp:375]     Train net output #0: loss = 0.0565623 (* 1 = 0.0565623 loss)
I0731 23:14:23.651455 20406 sgd_solver.cpp:136] Iteration 30100, lr = 1e-06, m = 0.9
I0731 23:14:42.362090 20406 solver.cpp:353] Iteration 30200 (5.34468 iter/s, 18.7102s/100 iter), loss = 0.0623254
I0731 23:14:42.362119 20406 solver.cpp:375]     Train net output #0: loss = 0.0623253 (* 1 = 0.0623253 loss)
I0731 23:14:42.362124 20406 sgd_solver.cpp:136] Iteration 30200, lr = 1e-06, m = 0.9
I0731 23:15:00.955533 20406 solver.cpp:353] Iteration 30300 (5.37839 iter/s, 18.5929s/100 iter), loss = 0.0791104
I0731 23:15:00.955593 20406 solver.cpp:375]     Train net output #0: loss = 0.0791102 (* 1 = 0.0791102 loss)
I0731 23:15:00.955598 20406 sgd_solver.cpp:136] Iteration 30300, lr = 1e-06, m = 0.9
I0731 23:15:19.531883 20406 solver.cpp:353] Iteration 30400 (5.38334 iter/s, 18.5758s/100 iter), loss = 0.0707658
I0731 23:15:19.531908 20406 solver.cpp:375]     Train net output #0: loss = 0.0707657 (* 1 = 0.0707657 loss)
I0731 23:15:19.531913 20406 sgd_solver.cpp:136] Iteration 30400, lr = 1e-06, m = 0.9
I0731 23:15:21.015668 20410 data_reader.cpp:264] Starting prefetch of epoch 17
I0731 23:15:38.030635 20406 solver.cpp:353] Iteration 30500 (5.40592 iter/s, 18.4982s/100 iter), loss = 0.0740908
I0731 23:15:38.030694 20406 solver.cpp:375]     Train net output #0: loss = 0.0740907 (* 1 = 0.0740907 loss)
I0731 23:15:38.030699 20406 sgd_solver.cpp:136] Iteration 30500, lr = 1e-06, m = 0.9
I0731 23:15:56.720571 20406 solver.cpp:353] Iteration 30600 (5.35062 iter/s, 18.6894s/100 iter), loss = 0.0565714
I0731 23:15:56.720612 20406 solver.cpp:375]     Train net output #0: loss = 0.0565713 (* 1 = 0.0565713 loss)
I0731 23:15:56.720615 20406 sgd_solver.cpp:136] Iteration 30600, lr = 1e-06, m = 0.9
I0731 23:16:15.262223 20406 solver.cpp:353] Iteration 30700 (5.39341 iter/s, 18.5411s/100 iter), loss = 0.104596
I0731 23:16:15.262334 20406 solver.cpp:375]     Train net output #0: loss = 0.104596 (* 1 = 0.104596 loss)
I0731 23:16:15.262341 20406 sgd_solver.cpp:136] Iteration 30700, lr = 1e-06, m = 0.9
I0731 23:16:22.381150 20364 data_reader.cpp:264] Starting prefetch of epoch 22
I0731 23:16:34.003877 20406 solver.cpp:353] Iteration 30800 (5.33586 iter/s, 18.7411s/100 iter), loss = 0.0648787
I0731 23:16:34.003903 20406 solver.cpp:375]     Train net output #0: loss = 0.0648786 (* 1 = 0.0648786 loss)
I0731 23:16:34.003907 20406 sgd_solver.cpp:136] Iteration 30800, lr = 1e-06, m = 0.9
I0731 23:16:52.765228 20406 solver.cpp:353] Iteration 30900 (5.33025 iter/s, 18.7608s/100 iter), loss = 0.0653854
I0731 23:16:52.765303 20406 solver.cpp:375]     Train net output #0: loss = 0.0653853 (* 1 = 0.0653853 loss)
I0731 23:16:52.765308 20406 sgd_solver.cpp:136] Iteration 30900, lr = 1e-06, m = 0.9
I0731 23:16:53.354302 20346 data_reader.cpp:264] Starting prefetch of epoch 20
I0731 23:17:11.238997 20406 solver.cpp:404] Sparsity after update:
I0731 23:17:11.264940 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 23:17:11.265017 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 23:17:11.265038 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 23:17:11.265048 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 23:17:11.265055 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 23:17:11.265063 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 23:17:11.265070 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 23:17:11.265079 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 23:17:11.265085 20406 net.cpp:2270] out3a_param_0(0) 
I0731 23:17:11.265094 20406 net.cpp:2270] out5a_param_0(0) 
I0731 23:17:11.265100 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 23:17:11.265108 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 23:17:11.265115 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 23:17:11.265122 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 23:17:11.265130 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 23:17:11.265137 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 23:17:11.265146 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 23:17:11.265152 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 23:17:11.265159 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 23:17:11.436771 20406 solver.cpp:353] Iteration 31000 (5.35589 iter/s, 18.671s/100 iter), loss = 0.0622772
I0731 23:17:11.436794 20406 solver.cpp:375]     Train net output #0: loss = 0.0622771 (* 1 = 0.0622771 loss)
I0731 23:17:11.436800 20406 sgd_solver.cpp:136] Iteration 31000, lr = 1e-06, m = 0.9
I0731 23:17:30.196768 20406 solver.cpp:353] Iteration 31100 (5.33064 iter/s, 18.7595s/100 iter), loss = 0.0559277
I0731 23:17:30.196823 20406 solver.cpp:375]     Train net output #0: loss = 0.0559276 (* 1 = 0.0559276 loss)
I0731 23:17:30.196828 20406 sgd_solver.cpp:136] Iteration 31100, lr = 1e-06, m = 0.9
I0731 23:17:48.791985 20406 solver.cpp:353] Iteration 31200 (5.37788 iter/s, 18.5947s/100 iter), loss = 0.0943663
I0731 23:17:48.792007 20406 solver.cpp:375]     Train net output #0: loss = 0.0943662 (* 1 = 0.0943662 loss)
I0731 23:17:48.792011 20406 sgd_solver.cpp:136] Iteration 31200, lr = 1e-06, m = 0.9
I0731 23:17:55.137423 20409 data_reader.cpp:264] Starting prefetch of epoch 18
I0731 23:18:07.416762 20406 solver.cpp:353] Iteration 31300 (5.36934 iter/s, 18.6243s/100 iter), loss = 0.0890799
I0731 23:18:07.417325 20406 solver.cpp:375]     Train net output #0: loss = 0.0890798 (* 1 = 0.0890798 loss)
I0731 23:18:07.417335 20406 sgd_solver.cpp:136] Iteration 31300, lr = 1e-06, m = 0.9
I0731 23:18:26.008054 20406 solver.cpp:353] Iteration 31400 (5.37901 iter/s, 18.5908s/100 iter), loss = 0.0572344
I0731 23:18:26.008080 20406 solver.cpp:375]     Train net output #0: loss = 0.0572344 (* 1 = 0.0572344 loss)
I0731 23:18:26.008083 20406 sgd_solver.cpp:136] Iteration 31400, lr = 1e-06, m = 0.9
I0731 23:18:44.463618 20406 solver.cpp:353] Iteration 31500 (5.41857 iter/s, 18.4551s/100 iter), loss = 0.0598888
I0731 23:18:44.463672 20406 solver.cpp:375]     Train net output #0: loss = 0.0598887 (* 1 = 0.0598887 loss)
I0731 23:18:44.463680 20406 sgd_solver.cpp:136] Iteration 31500, lr = 1e-06, m = 0.9
I0731 23:18:56.596601 20412 data_reader.cpp:264] Starting prefetch of epoch 21
I0731 23:19:03.133800 20406 solver.cpp:353] Iteration 31600 (5.35628 iter/s, 18.6697s/100 iter), loss = 0.112467
I0731 23:19:03.133822 20406 solver.cpp:375]     Train net output #0: loss = 0.112467 (* 1 = 0.112467 loss)
I0731 23:19:03.133826 20406 sgd_solver.cpp:136] Iteration 31600, lr = 1e-06, m = 0.9
I0731 23:19:21.717061 20406 solver.cpp:353] Iteration 31700 (5.38134 iter/s, 18.5827s/100 iter), loss = 0.0669986
I0731 23:19:21.717125 20406 solver.cpp:375]     Train net output #0: loss = 0.0669985 (* 1 = 0.0669985 loss)
I0731 23:19:21.717131 20406 sgd_solver.cpp:136] Iteration 31700, lr = 1e-06, m = 0.9
I0731 23:19:40.221477 20406 solver.cpp:353] Iteration 31800 (5.40426 iter/s, 18.5039s/100 iter), loss = 0.0710424
I0731 23:19:40.221501 20406 solver.cpp:375]     Train net output #0: loss = 0.0710423 (* 1 = 0.0710423 loss)
I0731 23:19:40.221505 20406 sgd_solver.cpp:136] Iteration 31800, lr = 1e-06, m = 0.9
I0731 23:19:57.896847 20415 data_reader.cpp:264] Starting prefetch of epoch 16
I0731 23:19:58.785298 20406 solver.cpp:353] Iteration 31900 (5.38697 iter/s, 18.5633s/100 iter), loss = 0.0436161
I0731 23:19:58.785326 20406 solver.cpp:375]     Train net output #0: loss = 0.043616 (* 1 = 0.043616 loss)
I0731 23:19:58.785332 20406 sgd_solver.cpp:136] Iteration 31900, lr = 1e-06, m = 0.9
I0731 23:20:17.252892 20406 solver.cpp:353] Iteration 31999 (5.36089 iter/s, 18.4671s/99 iter), loss = 0.0635586
I0731 23:20:17.252918 20406 solver.cpp:375]     Train net output #0: loss = 0.0635585 (* 1 = 0.0635585 loss)
I0731 23:20:17.252923 20406 solver.cpp:404] Sparsity after update:
I0731 23:20:17.254719 20406 net.cpp:2261] Num Params(17), Sparsity (zero_weights/count): 
I0731 23:20:17.254727 20406 net.cpp:2270] conv1a_param_0(0) 
I0731 23:20:17.254730 20406 net.cpp:2270] conv1b_param_0(0) 
I0731 23:20:17.254736 20406 net.cpp:2270] ctx_conv1_param_0(0) 
I0731 23:20:17.254739 20406 net.cpp:2270] ctx_conv2_param_0(0) 
I0731 23:20:17.254741 20406 net.cpp:2270] ctx_conv3_param_0(0) 
I0731 23:20:17.254743 20406 net.cpp:2270] ctx_conv4_param_0(0) 
I0731 23:20:17.254746 20406 net.cpp:2270] ctx_final_param_0(0) 
I0731 23:20:17.254748 20406 net.cpp:2270] out3a_param_0(0) 
I0731 23:20:17.254750 20406 net.cpp:2270] out5a_param_0(0) 
I0731 23:20:17.254753 20406 net.cpp:2270] res2a_branch2a_param_0(0) 
I0731 23:20:17.254756 20406 net.cpp:2270] res2a_branch2b_param_0(0) 
I0731 23:20:17.254758 20406 net.cpp:2270] res3a_branch2a_param_0(0) 
I0731 23:20:17.254760 20406 net.cpp:2270] res3a_branch2b_param_0(0) 
I0731 23:20:17.254763 20406 net.cpp:2270] res4a_branch2a_param_0(0) 
I0731 23:20:17.254765 20406 net.cpp:2270] res4a_branch2b_param_0(0) 
I0731 23:20:17.254767 20406 net.cpp:2270] res5a_branch2a_param_0(0) 
I0731 23:20:17.254770 20406 net.cpp:2270] res5a_branch2b_param_0(0) 
I0731 23:20:17.254772 20406 net.cpp:2272] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0731 23:20:17.313537 20406 solver.cpp:680] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/sparse/cityscapes5_jsegnet21v2_iter_32000.caffemodel
I0731 23:20:17.425184 20406 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/sparse/cityscapes5_jsegnet21v2_iter_32000.solverstate
I0731 23:20:17.498436 20406 solver.cpp:527] Iteration 32000, loss = 0.0560478
I0731 23:20:17.498464 20406 solver.cpp:550] Iteration 32000, Testing net (#0)
I0731 23:20:20.769330 20402 data_reader.cpp:264] Starting prefetch of epoch 5
I0731 23:20:28.547997 20406 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.95112
I0731 23:20:28.548146 20406 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999798
I0731 23:20:28.548156 20406 solver.cpp:635]     Test net output #2: loss = 0.171002 (* 1 = 0.171002 loss)
I0731 23:20:28.577919 20312 parallel.cpp:73] Root Solver performance on device 0: 5.181 * 6 = 31.09 img/sec (32000 itr in 6176 sec)
I0731 23:20:28.577985 20312 parallel.cpp:78]      Solver performance on device 1: 5.181 * 6 = 31.09 img/sec (32000 itr in 6176 sec)
I0731 23:20:28.578007 20312 parallel.cpp:78]      Solver performance on device 2: 5.181 * 6 = 31.09 img/sec (32000 itr in 6176 sec)
I0731 23:20:28.578018 20312 parallel.cpp:81] Overall multi-GPU performance: 93.2607 img/sec
I0731 23:20:29.846812 20312 caffe.cpp:247] Optimization Done in 1h 43m 14s
