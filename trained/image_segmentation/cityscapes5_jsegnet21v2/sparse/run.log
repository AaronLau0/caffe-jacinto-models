I0925 00:44:58.239006  4338 caffe.cpp:807] This is NVCaffe 0.16.4 started at Mon Sep 25 00:44:57 2017
I0925 00:44:58.240104  4338 caffe.cpp:810] CuDNN version: 7002
I0925 00:44:58.240109  4338 caffe.cpp:811] CuBLAS version: 8000
I0925 00:44:58.240113  4338 caffe.cpp:812] CUDA version: 8000
I0925 00:44:58.240115  4338 caffe.cpp:813] CUDA driver version: 8000
I0925 00:44:58.845158  4338 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0925 00:44:58.845818  4338 gpu_memory.cpp:161] Total memory: 8506769408, Free: 5687410688, dev_info[0]: total=8506769408 free=5687410688
I0925 00:44:58.846379  4338 gpu_memory.cpp:161] Total memory: 8508145664, Free: 5687410688, dev_info[1]: total=8508145664 free=5857411072
I0925 00:44:58.846930  4338 gpu_memory.cpp:161] Total memory: 8508145664, Free: 5687410688, dev_info[2]: total=8508145664 free=5857411072
I0925 00:44:58.846951  4338 caffe.cpp:214] Using GPUs 0, 1, 2
I0925 00:44:58.847338  4338 caffe.cpp:219] GPU 0: GeForce GTX 1080
I0925 00:44:58.847720  4338 caffe.cpp:219] GPU 1: GeForce GTX 1080
I0925 00:44:58.848093  4338 caffe.cpp:219] GPU 2: GeForce GTX 1080
I0925 00:44:58.848148  4338 solver.cpp:43] Solver data type: FLOAT
I0925 00:44:58.848211  4338 solver.cpp:46] Initializing solver from parameters: 
train_net: "training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/train.prototxt"
test_net: "training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/test.prototxt"
test_iter: 125
test_interval: 2000
base_lr: 0.01
display: 100
max_iter: 60000
lr_policy: "multistep"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 1e-05
snapshot: 10000
snapshot_prefix: "training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/cityscapes5_jsegnet21v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
regularization_type: "L1"
test_initialization: false
stepvalue: 30000
stepvalue: 45000
iter_size: 1
type: "SGD"
display_sparsity: 1000
sparse_mode: SPARSE_UPDATE
sparsity_target: 0.8
sparsity_step_factor: 0.01
sparsity_step_iter: 1000
sparsity_start_iter: 1000
sparsity_start_factor: 0.6
I0925 00:44:58.869094  4338 solver.cpp:78] Creating training net from train_net file: training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/train.prototxt
I0925 00:44:58.870067  4338 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0925 00:44:58.870076  4338 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
W0925 00:44:58.870126  4338 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0925 00:44:58.871130  4338 net.cpp:72] Initializing net from parameters: 
name: "jsegnet21v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/train-image-lmdb"
    label_list_path: "data/train-label-lmdb"
    batch_size: 6
    shuffle: true
    threads: 1
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
I0925 00:44:58.871856  4338 net.cpp:104] Using FLOAT as default forward math type
I0925 00:44:58.871875  4338 net.cpp:110] Using FLOAT as default backward math type
I0925 00:44:58.871887  4338 layer_factory.hpp:136] Creating layer 'data' of type 'ImageLabelData'
I0925 00:44:58.871898  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:44:58.871927  4338 net.cpp:184] Created Layer data (0)
I0925 00:44:58.871956  4338 net.cpp:530] data -> data
I0925 00:44:58.871989  4338 net.cpp:530] data -> label
I0925 00:44:58.872117  4338 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 6
I0925 00:44:58.872148  4338 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0925 00:44:58.895632  4408 db_lmdb.cpp:24] Opened lmdb data/train-image-lmdb
I0925 00:44:58.946844  4338 data_layer.cpp:187] [0] ReshapePrefetch 6, 3, 640, 640
I0925 00:44:58.946961  4338 data_layer.cpp:211] [0] Output data size: 6, 3, 640, 640
I0925 00:44:58.946980  4338 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0925 00:44:58.947038  4338 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 6
I0925 00:44:58.947057  4338 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0925 00:44:58.948222  4409 data_layer.cpp:101] [0] Parser threads: 1
I0925 00:44:58.950256  4409 data_layer.cpp:103] [0] Transformer threads: 1
I0925 00:44:58.956022  4410 db_lmdb.cpp:24] Opened lmdb data/train-label-lmdb
I0925 00:44:58.972615  4338 data_layer.cpp:187] [0] ReshapePrefetch 6, 1, 640, 640
I0925 00:44:58.972678  4338 data_layer.cpp:211] [0] Output data size: 6, 1, 640, 640
I0925 00:44:58.972685  4338 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0925 00:44:58.972741  4338 net.cpp:245] Setting up data
I0925 00:44:58.972753  4338 net.cpp:252] TRAIN Top shape for layer 0 'data' 6 3 640 640 (7372800)
I0925 00:44:58.972759  4338 net.cpp:252] TRAIN Top shape for layer 0 'data' 6 1 640 640 (2457600)
I0925 00:44:58.972766  4338 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0925 00:44:58.972771  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:44:58.972789  4338 net.cpp:184] Created Layer data/bias (1)
I0925 00:44:58.972793  4338 net.cpp:561] data/bias <- data
I0925 00:44:58.972802  4338 net.cpp:530] data/bias -> data/bias
I0925 00:44:58.977051  4411 data_layer.cpp:101] [0] Parser threads: 1
I0925 00:44:58.977072  4411 data_layer.cpp:103] [0] Transformer threads: 1
I0925 00:44:58.983389  4411 blocking_queue.cpp:40] Waiting for datum
I0925 00:44:58.985085  4338 net.cpp:245] Setting up data/bias
I0925 00:44:58.985122  4338 net.cpp:252] TRAIN Top shape for layer 1 'data/bias' 6 3 640 640 (7372800)
I0925 00:44:58.985180  4338 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0925 00:44:58.985210  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:44:58.985237  4338 net.cpp:184] Created Layer conv1a (2)
I0925 00:44:58.985249  4338 net.cpp:561] conv1a <- data/bias
I0925 00:44:58.985260  4338 net.cpp:530] conv1a -> conv1a
I0925 00:45:00.321681  4338 net.cpp:245] Setting up conv1a
I0925 00:45:00.321708  4338 net.cpp:252] TRAIN Top shape for layer 2 'conv1a' 6 32 320 320 (19660800)
I0925 00:45:00.321724  4338 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0925 00:45:00.321730  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.321749  4338 net.cpp:184] Created Layer conv1a/bn (3)
I0925 00:45:00.321756  4338 net.cpp:561] conv1a/bn <- conv1a
I0925 00:45:00.321763  4338 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0925 00:45:00.322935  4338 net.cpp:245] Setting up conv1a/bn
I0925 00:45:00.322947  4338 net.cpp:252] TRAIN Top shape for layer 3 'conv1a/bn' 6 32 320 320 (19660800)
I0925 00:45:00.322958  4338 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0925 00:45:00.322963  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.322970  4338 net.cpp:184] Created Layer conv1a/relu (4)
I0925 00:45:00.322974  4338 net.cpp:561] conv1a/relu <- conv1a
I0925 00:45:00.322978  4338 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0925 00:45:00.322993  4338 net.cpp:245] Setting up conv1a/relu
I0925 00:45:00.322996  4338 net.cpp:252] TRAIN Top shape for layer 4 'conv1a/relu' 6 32 320 320 (19660800)
I0925 00:45:00.323000  4338 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0925 00:45:00.323004  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.323019  4338 net.cpp:184] Created Layer conv1b (5)
I0925 00:45:00.323024  4338 net.cpp:561] conv1b <- conv1a
I0925 00:45:00.323027  4338 net.cpp:530] conv1b -> conv1b
I0925 00:45:00.324934  4338 net.cpp:245] Setting up conv1b
I0925 00:45:00.324946  4338 net.cpp:252] TRAIN Top shape for layer 5 'conv1b' 6 32 320 320 (19660800)
I0925 00:45:00.324955  4338 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0925 00:45:00.324960  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.324967  4338 net.cpp:184] Created Layer conv1b/bn (6)
I0925 00:45:00.324971  4338 net.cpp:561] conv1b/bn <- conv1b
I0925 00:45:00.324975  4338 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0925 00:45:00.326048  4338 net.cpp:245] Setting up conv1b/bn
I0925 00:45:00.326058  4338 net.cpp:252] TRAIN Top shape for layer 6 'conv1b/bn' 6 32 320 320 (19660800)
I0925 00:45:00.326067  4338 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0925 00:45:00.326071  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.326076  4338 net.cpp:184] Created Layer conv1b/relu (7)
I0925 00:45:00.326078  4338 net.cpp:561] conv1b/relu <- conv1b
I0925 00:45:00.326083  4338 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0925 00:45:00.326087  4338 net.cpp:245] Setting up conv1b/relu
I0925 00:45:00.326092  4338 net.cpp:252] TRAIN Top shape for layer 7 'conv1b/relu' 6 32 320 320 (19660800)
I0925 00:45:00.326094  4338 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0925 00:45:00.326098  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.326107  4338 net.cpp:184] Created Layer pool1 (8)
I0925 00:45:00.326112  4338 net.cpp:561] pool1 <- conv1b
I0925 00:45:00.326114  4338 net.cpp:530] pool1 -> pool1
I0925 00:45:00.347424  4338 net.cpp:245] Setting up pool1
I0925 00:45:00.347450  4338 net.cpp:252] TRAIN Top shape for layer 8 'pool1' 6 32 160 160 (4915200)
I0925 00:45:00.347455  4338 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0925 00:45:00.347474  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.347489  4338 net.cpp:184] Created Layer res2a_branch2a (9)
I0925 00:45:00.347496  4338 net.cpp:561] res2a_branch2a <- pool1
I0925 00:45:00.347501  4338 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0925 00:45:00.353942  4338 net.cpp:245] Setting up res2a_branch2a
I0925 00:45:00.353968  4338 net.cpp:252] TRAIN Top shape for layer 9 'res2a_branch2a' 6 64 160 160 (9830400)
I0925 00:45:00.353981  4338 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0925 00:45:00.353986  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.353998  4338 net.cpp:184] Created Layer res2a_branch2a/bn (10)
I0925 00:45:00.354005  4338 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0925 00:45:00.354010  4338 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0925 00:45:00.354908  4338 net.cpp:245] Setting up res2a_branch2a/bn
I0925 00:45:00.354918  4338 net.cpp:252] TRAIN Top shape for layer 10 'res2a_branch2a/bn' 6 64 160 160 (9830400)
I0925 00:45:00.354928  4338 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0925 00:45:00.354933  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.354938  4338 net.cpp:184] Created Layer res2a_branch2a/relu (11)
I0925 00:45:00.354941  4338 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0925 00:45:00.354944  4338 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0925 00:45:00.354949  4338 net.cpp:245] Setting up res2a_branch2a/relu
I0925 00:45:00.354954  4338 net.cpp:252] TRAIN Top shape for layer 11 'res2a_branch2a/relu' 6 64 160 160 (9830400)
I0925 00:45:00.354956  4338 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0925 00:45:00.354960  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.354970  4338 net.cpp:184] Created Layer res2a_branch2b (12)
I0925 00:45:00.354974  4338 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0925 00:45:00.354976  4338 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0925 00:45:00.357110  4338 net.cpp:245] Setting up res2a_branch2b
I0925 00:45:00.357132  4338 net.cpp:252] TRAIN Top shape for layer 12 'res2a_branch2b' 6 64 160 160 (9830400)
I0925 00:45:00.357141  4338 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0925 00:45:00.357146  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.357161  4338 net.cpp:184] Created Layer res2a_branch2b/bn (13)
I0925 00:45:00.357165  4338 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0925 00:45:00.357169  4338 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0925 00:45:00.358127  4338 net.cpp:245] Setting up res2a_branch2b/bn
I0925 00:45:00.358139  4338 net.cpp:252] TRAIN Top shape for layer 13 'res2a_branch2b/bn' 6 64 160 160 (9830400)
I0925 00:45:00.358146  4338 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0925 00:45:00.358152  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.358157  4338 net.cpp:184] Created Layer res2a_branch2b/relu (14)
I0925 00:45:00.358160  4338 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0925 00:45:00.358163  4338 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0925 00:45:00.358170  4338 net.cpp:245] Setting up res2a_branch2b/relu
I0925 00:45:00.358173  4338 net.cpp:252] TRAIN Top shape for layer 14 'res2a_branch2b/relu' 6 64 160 160 (9830400)
I0925 00:45:00.358176  4338 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0925 00:45:00.358180  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.358184  4338 net.cpp:184] Created Layer pool2 (15)
I0925 00:45:00.358201  4338 net.cpp:561] pool2 <- res2a_branch2b
I0925 00:45:00.358204  4338 net.cpp:530] pool2 -> pool2
I0925 00:45:00.358289  4338 net.cpp:245] Setting up pool2
I0925 00:45:00.358295  4338 net.cpp:252] TRAIN Top shape for layer 15 'pool2' 6 64 80 80 (2457600)
I0925 00:45:00.358300  4338 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0925 00:45:00.358304  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.358317  4338 net.cpp:184] Created Layer res3a_branch2a (16)
I0925 00:45:00.358321  4338 net.cpp:561] res3a_branch2a <- pool2
I0925 00:45:00.358326  4338 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0925 00:45:00.360826  4338 net.cpp:245] Setting up res3a_branch2a
I0925 00:45:00.360838  4338 net.cpp:252] TRAIN Top shape for layer 16 'res3a_branch2a' 6 128 80 80 (4915200)
I0925 00:45:00.360846  4338 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0925 00:45:00.360851  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.360857  4338 net.cpp:184] Created Layer res3a_branch2a/bn (17)
I0925 00:45:00.360860  4338 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0925 00:45:00.360864  4338 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0925 00:45:00.361927  4338 net.cpp:245] Setting up res3a_branch2a/bn
I0925 00:45:00.361935  4338 net.cpp:252] TRAIN Top shape for layer 17 'res3a_branch2a/bn' 6 128 80 80 (4915200)
I0925 00:45:00.361945  4338 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0925 00:45:00.361950  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.361955  4338 net.cpp:184] Created Layer res3a_branch2a/relu (18)
I0925 00:45:00.361960  4338 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0925 00:45:00.361963  4338 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0925 00:45:00.361969  4338 net.cpp:245] Setting up res3a_branch2a/relu
I0925 00:45:00.361974  4338 net.cpp:252] TRAIN Top shape for layer 18 'res3a_branch2a/relu' 6 128 80 80 (4915200)
I0925 00:45:00.361977  4338 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0925 00:45:00.361980  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.361994  4338 net.cpp:184] Created Layer res3a_branch2b (19)
I0925 00:45:00.361999  4338 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0925 00:45:00.362002  4338 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0925 00:45:00.363481  4338 net.cpp:245] Setting up res3a_branch2b
I0925 00:45:00.363492  4338 net.cpp:252] TRAIN Top shape for layer 19 'res3a_branch2b' 6 128 80 80 (4915200)
I0925 00:45:00.363497  4338 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0925 00:45:00.363502  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.363507  4338 net.cpp:184] Created Layer res3a_branch2b/bn (20)
I0925 00:45:00.363512  4338 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0925 00:45:00.363514  4338 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0925 00:45:00.364513  4338 net.cpp:245] Setting up res3a_branch2b/bn
I0925 00:45:00.364524  4338 net.cpp:252] TRAIN Top shape for layer 20 'res3a_branch2b/bn' 6 128 80 80 (4915200)
I0925 00:45:00.364532  4338 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0925 00:45:00.364537  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.364542  4338 net.cpp:184] Created Layer res3a_branch2b/relu (21)
I0925 00:45:00.364547  4338 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0925 00:45:00.364550  4338 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0925 00:45:00.364555  4338 net.cpp:245] Setting up res3a_branch2b/relu
I0925 00:45:00.364560  4338 net.cpp:252] TRAIN Top shape for layer 21 'res3a_branch2b/relu' 6 128 80 80 (4915200)
I0925 00:45:00.364574  4338 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0925 00:45:00.364579  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.364588  4338 net.cpp:184] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (22)
I0925 00:45:00.364593  4338 net.cpp:561] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0925 00:45:00.364596  4338 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0925 00:45:00.364603  4338 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0925 00:45:00.364663  4338 net.cpp:245] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0925 00:45:00.364670  4338 net.cpp:252] TRAIN Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 6 128 80 80 (4915200)
I0925 00:45:00.364675  4338 net.cpp:252] TRAIN Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 6 128 80 80 (4915200)
I0925 00:45:00.364678  4338 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0925 00:45:00.364682  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.364688  4338 net.cpp:184] Created Layer pool3 (23)
I0925 00:45:00.364693  4338 net.cpp:561] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0925 00:45:00.364697  4338 net.cpp:530] pool3 -> pool3
I0925 00:45:00.364779  4338 net.cpp:245] Setting up pool3
I0925 00:45:00.364785  4338 net.cpp:252] TRAIN Top shape for layer 23 'pool3' 6 128 40 40 (1228800)
I0925 00:45:00.364790  4338 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0925 00:45:00.364794  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.364802  4338 net.cpp:184] Created Layer res4a_branch2a (24)
I0925 00:45:00.364806  4338 net.cpp:561] res4a_branch2a <- pool3
I0925 00:45:00.364811  4338 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0925 00:45:00.375128  4338 net.cpp:245] Setting up res4a_branch2a
I0925 00:45:00.375149  4338 net.cpp:252] TRAIN Top shape for layer 24 'res4a_branch2a' 6 256 40 40 (2457600)
I0925 00:45:00.375159  4338 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0925 00:45:00.375164  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.375172  4338 net.cpp:184] Created Layer res4a_branch2a/bn (25)
I0925 00:45:00.375178  4338 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0925 00:45:00.375183  4338 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0925 00:45:00.376129  4338 net.cpp:245] Setting up res4a_branch2a/bn
I0925 00:45:00.376139  4338 net.cpp:252] TRAIN Top shape for layer 25 'res4a_branch2a/bn' 6 256 40 40 (2457600)
I0925 00:45:00.376148  4338 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0925 00:45:00.376154  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.376159  4338 net.cpp:184] Created Layer res4a_branch2a/relu (26)
I0925 00:45:00.376164  4338 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0925 00:45:00.376168  4338 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0925 00:45:00.376178  4338 net.cpp:245] Setting up res4a_branch2a/relu
I0925 00:45:00.376185  4338 net.cpp:252] TRAIN Top shape for layer 26 'res4a_branch2a/relu' 6 256 40 40 (2457600)
I0925 00:45:00.376188  4338 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0925 00:45:00.376193  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.376204  4338 net.cpp:184] Created Layer res4a_branch2b (27)
I0925 00:45:00.376207  4338 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0925 00:45:00.376212  4338 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0925 00:45:00.380832  4338 net.cpp:245] Setting up res4a_branch2b
I0925 00:45:00.380882  4338 net.cpp:252] TRAIN Top shape for layer 27 'res4a_branch2b' 6 256 40 40 (2457600)
I0925 00:45:00.380893  4338 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0925 00:45:00.380899  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.380913  4338 net.cpp:184] Created Layer res4a_branch2b/bn (28)
I0925 00:45:00.380918  4338 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0925 00:45:00.380924  4338 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0925 00:45:00.382060  4338 net.cpp:245] Setting up res4a_branch2b/bn
I0925 00:45:00.382071  4338 net.cpp:252] TRAIN Top shape for layer 28 'res4a_branch2b/bn' 6 256 40 40 (2457600)
I0925 00:45:00.382081  4338 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0925 00:45:00.382086  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.382092  4338 net.cpp:184] Created Layer res4a_branch2b/relu (29)
I0925 00:45:00.382097  4338 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0925 00:45:00.382102  4338 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0925 00:45:00.382108  4338 net.cpp:245] Setting up res4a_branch2b/relu
I0925 00:45:00.382113  4338 net.cpp:252] TRAIN Top shape for layer 29 'res4a_branch2b/relu' 6 256 40 40 (2457600)
I0925 00:45:00.382118  4338 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0925 00:45:00.382122  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.382128  4338 net.cpp:184] Created Layer pool4 (30)
I0925 00:45:00.382133  4338 net.cpp:561] pool4 <- res4a_branch2b
I0925 00:45:00.382136  4338 net.cpp:530] pool4 -> pool4
I0925 00:45:00.382225  4338 net.cpp:245] Setting up pool4
I0925 00:45:00.382232  4338 net.cpp:252] TRAIN Top shape for layer 30 'pool4' 6 256 40 40 (2457600)
I0925 00:45:00.382236  4338 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0925 00:45:00.382241  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.382251  4338 net.cpp:184] Created Layer res5a_branch2a (31)
I0925 00:45:00.382256  4338 net.cpp:561] res5a_branch2a <- pool4
I0925 00:45:00.382259  4338 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0925 00:45:00.417426  4338 net.cpp:245] Setting up res5a_branch2a
I0925 00:45:00.417454  4338 net.cpp:252] TRAIN Top shape for layer 31 'res5a_branch2a' 6 512 40 40 (4915200)
I0925 00:45:00.417464  4338 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0925 00:45:00.417469  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.417482  4338 net.cpp:184] Created Layer res5a_branch2a/bn (32)
I0925 00:45:00.417488  4338 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0925 00:45:00.417493  4338 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0925 00:45:00.418464  4338 net.cpp:245] Setting up res5a_branch2a/bn
I0925 00:45:00.418478  4338 net.cpp:252] TRAIN Top shape for layer 32 'res5a_branch2a/bn' 6 512 40 40 (4915200)
I0925 00:45:00.418485  4338 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0925 00:45:00.418506  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.418520  4338 net.cpp:184] Created Layer res5a_branch2a/relu (33)
I0925 00:45:00.418531  4338 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0925 00:45:00.418541  4338 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0925 00:45:00.418553  4338 net.cpp:245] Setting up res5a_branch2a/relu
I0925 00:45:00.418563  4338 net.cpp:252] TRAIN Top shape for layer 33 'res5a_branch2a/relu' 6 512 40 40 (4915200)
I0925 00:45:00.418573  4338 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0925 00:45:00.418587  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.418612  4338 net.cpp:184] Created Layer res5a_branch2b (34)
I0925 00:45:00.418622  4338 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0925 00:45:00.418632  4338 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0925 00:45:00.436352  4338 net.cpp:245] Setting up res5a_branch2b
I0925 00:45:00.436429  4338 net.cpp:252] TRAIN Top shape for layer 34 'res5a_branch2b' 6 512 40 40 (4915200)
I0925 00:45:00.436455  4338 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0925 00:45:00.436468  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.436483  4338 net.cpp:184] Created Layer res5a_branch2b/bn (35)
I0925 00:45:00.436494  4338 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0925 00:45:00.436504  4338 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0925 00:45:00.437490  4338 net.cpp:245] Setting up res5a_branch2b/bn
I0925 00:45:00.437510  4338 net.cpp:252] TRAIN Top shape for layer 35 'res5a_branch2b/bn' 6 512 40 40 (4915200)
I0925 00:45:00.437527  4338 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0925 00:45:00.437537  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.437549  4338 net.cpp:184] Created Layer res5a_branch2b/relu (36)
I0925 00:45:00.437559  4338 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0925 00:45:00.437569  4338 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0925 00:45:00.437580  4338 net.cpp:245] Setting up res5a_branch2b/relu
I0925 00:45:00.437592  4338 net.cpp:252] TRAIN Top shape for layer 36 'res5a_branch2b/relu' 6 512 40 40 (4915200)
I0925 00:45:00.437600  4338 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I0925 00:45:00.437610  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.437624  4338 net.cpp:184] Created Layer out5a (37)
I0925 00:45:00.437634  4338 net.cpp:561] out5a <- res5a_branch2b
I0925 00:45:00.437644  4338 net.cpp:530] out5a -> out5a
I0925 00:45:00.443436  4338 net.cpp:245] Setting up out5a
I0925 00:45:00.443466  4338 net.cpp:252] TRAIN Top shape for layer 37 'out5a' 6 64 40 40 (614400)
I0925 00:45:00.443476  4338 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I0925 00:45:00.443481  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.443491  4338 net.cpp:184] Created Layer out5a/bn (38)
I0925 00:45:00.443498  4338 net.cpp:561] out5a/bn <- out5a
I0925 00:45:00.443503  4338 net.cpp:513] out5a/bn -> out5a (in-place)
I0925 00:45:00.444646  4338 net.cpp:245] Setting up out5a/bn
I0925 00:45:00.444658  4338 net.cpp:252] TRAIN Top shape for layer 38 'out5a/bn' 6 64 40 40 (614400)
I0925 00:45:00.444667  4338 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I0925 00:45:00.444672  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.444677  4338 net.cpp:184] Created Layer out5a/relu (39)
I0925 00:45:00.444680  4338 net.cpp:561] out5a/relu <- out5a
I0925 00:45:00.444684  4338 net.cpp:513] out5a/relu -> out5a (in-place)
I0925 00:45:00.444691  4338 net.cpp:245] Setting up out5a/relu
I0925 00:45:00.444696  4338 net.cpp:252] TRAIN Top shape for layer 39 'out5a/relu' 6 64 40 40 (614400)
I0925 00:45:00.444700  4338 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I0925 00:45:00.444705  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.444725  4338 net.cpp:184] Created Layer out5a_up2 (40)
I0925 00:45:00.444728  4338 net.cpp:561] out5a_up2 <- out5a
I0925 00:45:00.444731  4338 net.cpp:530] out5a_up2 -> out5a_up2
I0925 00:45:00.445142  4338 net.cpp:245] Setting up out5a_up2
I0925 00:45:00.445149  4338 net.cpp:252] TRAIN Top shape for layer 40 'out5a_up2' 6 64 80 80 (2457600)
I0925 00:45:00.445154  4338 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I0925 00:45:00.445170  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.445180  4338 net.cpp:184] Created Layer out3a (41)
I0925 00:45:00.445184  4338 net.cpp:561] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0925 00:45:00.445188  4338 net.cpp:530] out3a -> out3a
I0925 00:45:00.446724  4338 net.cpp:245] Setting up out3a
I0925 00:45:00.446737  4338 net.cpp:252] TRAIN Top shape for layer 41 'out3a' 6 64 80 80 (2457600)
I0925 00:45:00.446743  4338 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I0925 00:45:00.446761  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.446769  4338 net.cpp:184] Created Layer out3a/bn (42)
I0925 00:45:00.446774  4338 net.cpp:561] out3a/bn <- out3a
I0925 00:45:00.446777  4338 net.cpp:513] out3a/bn -> out3a (in-place)
I0925 00:45:00.447938  4338 net.cpp:245] Setting up out3a/bn
I0925 00:45:00.447948  4338 net.cpp:252] TRAIN Top shape for layer 42 'out3a/bn' 6 64 80 80 (2457600)
I0925 00:45:00.447958  4338 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I0925 00:45:00.447963  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.447969  4338 net.cpp:184] Created Layer out3a/relu (43)
I0925 00:45:00.447973  4338 net.cpp:561] out3a/relu <- out3a
I0925 00:45:00.447978  4338 net.cpp:513] out3a/relu -> out3a (in-place)
I0925 00:45:00.447984  4338 net.cpp:245] Setting up out3a/relu
I0925 00:45:00.447989  4338 net.cpp:252] TRAIN Top shape for layer 43 'out3a/relu' 6 64 80 80 (2457600)
I0925 00:45:00.447993  4338 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I0925 00:45:00.447998  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.458956  4338 net.cpp:184] Created Layer out3_out5_combined (44)
I0925 00:45:00.458976  4338 net.cpp:561] out3_out5_combined <- out5a_up2
I0925 00:45:00.458981  4338 net.cpp:561] out3_out5_combined <- out3a
I0925 00:45:00.458986  4338 net.cpp:530] out3_out5_combined -> out3_out5_combined
I0925 00:45:00.459072  4338 net.cpp:245] Setting up out3_out5_combined
I0925 00:45:00.459081  4338 net.cpp:252] TRAIN Top shape for layer 44 'out3_out5_combined' 6 64 80 80 (2457600)
I0925 00:45:00.459086  4338 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I0925 00:45:00.459091  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.459115  4338 net.cpp:184] Created Layer ctx_conv1 (45)
I0925 00:45:00.459126  4338 net.cpp:561] ctx_conv1 <- out3_out5_combined
I0925 00:45:00.459136  4338 net.cpp:530] ctx_conv1 -> ctx_conv1
I0925 00:45:00.460728  4338 net.cpp:245] Setting up ctx_conv1
I0925 00:45:00.460752  4338 net.cpp:252] TRAIN Top shape for layer 45 'ctx_conv1' 6 64 80 80 (2457600)
I0925 00:45:00.460767  4338 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I0925 00:45:00.460777  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.460786  4338 net.cpp:184] Created Layer ctx_conv1/bn (46)
I0925 00:45:00.460791  4338 net.cpp:561] ctx_conv1/bn <- ctx_conv1
I0925 00:45:00.460795  4338 net.cpp:513] ctx_conv1/bn -> ctx_conv1 (in-place)
I0925 00:45:00.461920  4338 net.cpp:245] Setting up ctx_conv1/bn
I0925 00:45:00.461930  4338 net.cpp:252] TRAIN Top shape for layer 46 'ctx_conv1/bn' 6 64 80 80 (2457600)
I0925 00:45:00.461938  4338 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I0925 00:45:00.461948  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.461953  4338 net.cpp:184] Created Layer ctx_conv1/relu (47)
I0925 00:45:00.461957  4338 net.cpp:561] ctx_conv1/relu <- ctx_conv1
I0925 00:45:00.461963  4338 net.cpp:513] ctx_conv1/relu -> ctx_conv1 (in-place)
I0925 00:45:00.461969  4338 net.cpp:245] Setting up ctx_conv1/relu
I0925 00:45:00.461974  4338 net.cpp:252] TRAIN Top shape for layer 47 'ctx_conv1/relu' 6 64 80 80 (2457600)
I0925 00:45:00.461988  4338 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I0925 00:45:00.461992  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.462002  4338 net.cpp:184] Created Layer ctx_conv2 (48)
I0925 00:45:00.462005  4338 net.cpp:561] ctx_conv2 <- ctx_conv1
I0925 00:45:00.462009  4338 net.cpp:530] ctx_conv2 -> ctx_conv2
I0925 00:45:00.463521  4338 net.cpp:245] Setting up ctx_conv2
I0925 00:45:00.463531  4338 net.cpp:252] TRAIN Top shape for layer 48 'ctx_conv2' 6 64 80 80 (2457600)
I0925 00:45:00.463537  4338 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I0925 00:45:00.463541  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.463552  4338 net.cpp:184] Created Layer ctx_conv2/bn (49)
I0925 00:45:00.463557  4338 net.cpp:561] ctx_conv2/bn <- ctx_conv2
I0925 00:45:00.463560  4338 net.cpp:513] ctx_conv2/bn -> ctx_conv2 (in-place)
I0925 00:45:00.464658  4338 net.cpp:245] Setting up ctx_conv2/bn
I0925 00:45:00.464668  4338 net.cpp:252] TRAIN Top shape for layer 49 'ctx_conv2/bn' 6 64 80 80 (2457600)
I0925 00:45:00.464675  4338 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I0925 00:45:00.464680  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.464685  4338 net.cpp:184] Created Layer ctx_conv2/relu (50)
I0925 00:45:00.464690  4338 net.cpp:561] ctx_conv2/relu <- ctx_conv2
I0925 00:45:00.464694  4338 net.cpp:513] ctx_conv2/relu -> ctx_conv2 (in-place)
I0925 00:45:00.464699  4338 net.cpp:245] Setting up ctx_conv2/relu
I0925 00:45:00.464704  4338 net.cpp:252] TRAIN Top shape for layer 50 'ctx_conv2/relu' 6 64 80 80 (2457600)
I0925 00:45:00.464709  4338 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I0925 00:45:00.464711  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.464720  4338 net.cpp:184] Created Layer ctx_conv3 (51)
I0925 00:45:00.464723  4338 net.cpp:561] ctx_conv3 <- ctx_conv2
I0925 00:45:00.464726  4338 net.cpp:530] ctx_conv3 -> ctx_conv3
I0925 00:45:00.466223  4338 net.cpp:245] Setting up ctx_conv3
I0925 00:45:00.466231  4338 net.cpp:252] TRAIN Top shape for layer 51 'ctx_conv3' 6 64 80 80 (2457600)
I0925 00:45:00.466238  4338 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I0925 00:45:00.466243  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.466249  4338 net.cpp:184] Created Layer ctx_conv3/bn (52)
I0925 00:45:00.466253  4338 net.cpp:561] ctx_conv3/bn <- ctx_conv3
I0925 00:45:00.466258  4338 net.cpp:513] ctx_conv3/bn -> ctx_conv3 (in-place)
I0925 00:45:00.467340  4338 net.cpp:245] Setting up ctx_conv3/bn
I0925 00:45:00.467350  4338 net.cpp:252] TRAIN Top shape for layer 52 'ctx_conv3/bn' 6 64 80 80 (2457600)
I0925 00:45:00.467357  4338 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I0925 00:45:00.467362  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.467370  4338 net.cpp:184] Created Layer ctx_conv3/relu (53)
I0925 00:45:00.467375  4338 net.cpp:561] ctx_conv3/relu <- ctx_conv3
I0925 00:45:00.467378  4338 net.cpp:513] ctx_conv3/relu -> ctx_conv3 (in-place)
I0925 00:45:00.467384  4338 net.cpp:245] Setting up ctx_conv3/relu
I0925 00:45:00.467389  4338 net.cpp:252] TRAIN Top shape for layer 53 'ctx_conv3/relu' 6 64 80 80 (2457600)
I0925 00:45:00.467393  4338 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I0925 00:45:00.467397  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.467404  4338 net.cpp:184] Created Layer ctx_conv4 (54)
I0925 00:45:00.467407  4338 net.cpp:561] ctx_conv4 <- ctx_conv3
I0925 00:45:00.467411  4338 net.cpp:530] ctx_conv4 -> ctx_conv4
I0925 00:45:00.468919  4338 net.cpp:245] Setting up ctx_conv4
I0925 00:45:00.468928  4338 net.cpp:252] TRAIN Top shape for layer 54 'ctx_conv4' 6 64 80 80 (2457600)
I0925 00:45:00.468935  4338 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I0925 00:45:00.468940  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.468950  4338 net.cpp:184] Created Layer ctx_conv4/bn (55)
I0925 00:45:00.468955  4338 net.cpp:561] ctx_conv4/bn <- ctx_conv4
I0925 00:45:00.468958  4338 net.cpp:513] ctx_conv4/bn -> ctx_conv4 (in-place)
I0925 00:45:00.469770  4338 net.cpp:245] Setting up ctx_conv4/bn
I0925 00:45:00.469779  4338 net.cpp:252] TRAIN Top shape for layer 55 'ctx_conv4/bn' 6 64 80 80 (2457600)
I0925 00:45:00.469789  4338 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I0925 00:45:00.469792  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.469797  4338 net.cpp:184] Created Layer ctx_conv4/relu (56)
I0925 00:45:00.469801  4338 net.cpp:561] ctx_conv4/relu <- ctx_conv4
I0925 00:45:00.469805  4338 net.cpp:513] ctx_conv4/relu -> ctx_conv4 (in-place)
I0925 00:45:00.469810  4338 net.cpp:245] Setting up ctx_conv4/relu
I0925 00:45:00.469815  4338 net.cpp:252] TRAIN Top shape for layer 56 'ctx_conv4/relu' 6 64 80 80 (2457600)
I0925 00:45:00.469820  4338 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I0925 00:45:00.469823  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.469832  4338 net.cpp:184] Created Layer ctx_final (57)
I0925 00:45:00.469836  4338 net.cpp:561] ctx_final <- ctx_conv4
I0925 00:45:00.469841  4338 net.cpp:530] ctx_final -> ctx_final
I0925 00:45:00.470472  4338 net.cpp:245] Setting up ctx_final
I0925 00:45:00.470481  4338 net.cpp:252] TRAIN Top shape for layer 57 'ctx_final' 6 8 80 80 (307200)
I0925 00:45:00.470487  4338 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I0925 00:45:00.470491  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.470495  4338 net.cpp:184] Created Layer ctx_final/relu (58)
I0925 00:45:00.470500  4338 net.cpp:561] ctx_final/relu <- ctx_final
I0925 00:45:00.470504  4338 net.cpp:513] ctx_final/relu -> ctx_final (in-place)
I0925 00:45:00.470510  4338 net.cpp:245] Setting up ctx_final/relu
I0925 00:45:00.470515  4338 net.cpp:252] TRAIN Top shape for layer 58 'ctx_final/relu' 6 8 80 80 (307200)
I0925 00:45:00.470530  4338 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I0925 00:45:00.470541  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.470554  4338 net.cpp:184] Created Layer out_deconv_final_up2 (59)
I0925 00:45:00.470564  4338 net.cpp:561] out_deconv_final_up2 <- ctx_final
I0925 00:45:00.470573  4338 net.cpp:530] out_deconv_final_up2 -> out_deconv_final_up2
I0925 00:45:00.470932  4338 net.cpp:245] Setting up out_deconv_final_up2
I0925 00:45:00.470947  4338 net.cpp:252] TRAIN Top shape for layer 59 'out_deconv_final_up2' 6 8 160 160 (1228800)
I0925 00:45:00.470957  4338 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I0925 00:45:00.470966  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.470978  4338 net.cpp:184] Created Layer out_deconv_final_up4 (60)
I0925 00:45:00.470988  4338 net.cpp:561] out_deconv_final_up4 <- out_deconv_final_up2
I0925 00:45:00.470998  4338 net.cpp:530] out_deconv_final_up4 -> out_deconv_final_up4
I0925 00:45:00.471355  4338 net.cpp:245] Setting up out_deconv_final_up4
I0925 00:45:00.471371  4338 net.cpp:252] TRAIN Top shape for layer 60 'out_deconv_final_up4' 6 8 320 320 (4915200)
I0925 00:45:00.471384  4338 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I0925 00:45:00.471397  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.471418  4338 net.cpp:184] Created Layer out_deconv_final_up8 (61)
I0925 00:45:00.471429  4338 net.cpp:561] out_deconv_final_up8 <- out_deconv_final_up4
I0925 00:45:00.471438  4338 net.cpp:530] out_deconv_final_up8 -> out_deconv_final_up8
I0925 00:45:00.471796  4338 net.cpp:245] Setting up out_deconv_final_up8
I0925 00:45:00.471812  4338 net.cpp:252] TRAIN Top shape for layer 61 'out_deconv_final_up8' 6 8 640 640 (19660800)
I0925 00:45:00.471823  4338 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0925 00:45:00.471832  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.471851  4338 net.cpp:184] Created Layer loss (62)
I0925 00:45:00.471860  4338 net.cpp:561] loss <- out_deconv_final_up8
I0925 00:45:00.471866  4338 net.cpp:561] loss <- label
I0925 00:45:00.471871  4338 net.cpp:530] loss -> loss
I0925 00:45:00.473589  4338 net.cpp:245] Setting up loss
I0925 00:45:00.473608  4338 net.cpp:252] TRAIN Top shape for layer 62 'loss' (1)
I0925 00:45:00.473613  4338 net.cpp:256]     with loss weight 1
I0925 00:45:00.473631  4338 net.cpp:323] loss needs backward computation.
I0925 00:45:00.473635  4338 net.cpp:323] out_deconv_final_up8 needs backward computation.
I0925 00:45:00.473639  4338 net.cpp:323] out_deconv_final_up4 needs backward computation.
I0925 00:45:00.473644  4338 net.cpp:323] out_deconv_final_up2 needs backward computation.
I0925 00:45:00.473647  4338 net.cpp:323] ctx_final/relu needs backward computation.
I0925 00:45:00.473651  4338 net.cpp:323] ctx_final needs backward computation.
I0925 00:45:00.473655  4338 net.cpp:323] ctx_conv4/relu needs backward computation.
I0925 00:45:00.473659  4338 net.cpp:323] ctx_conv4/bn needs backward computation.
I0925 00:45:00.473662  4338 net.cpp:323] ctx_conv4 needs backward computation.
I0925 00:45:00.473665  4338 net.cpp:323] ctx_conv3/relu needs backward computation.
I0925 00:45:00.473670  4338 net.cpp:323] ctx_conv3/bn needs backward computation.
I0925 00:45:00.473673  4338 net.cpp:323] ctx_conv3 needs backward computation.
I0925 00:45:00.473678  4338 net.cpp:323] ctx_conv2/relu needs backward computation.
I0925 00:45:00.473682  4338 net.cpp:323] ctx_conv2/bn needs backward computation.
I0925 00:45:00.473686  4338 net.cpp:323] ctx_conv2 needs backward computation.
I0925 00:45:00.473690  4338 net.cpp:323] ctx_conv1/relu needs backward computation.
I0925 00:45:00.473692  4338 net.cpp:323] ctx_conv1/bn needs backward computation.
I0925 00:45:00.473695  4338 net.cpp:323] ctx_conv1 needs backward computation.
I0925 00:45:00.473700  4338 net.cpp:323] out3_out5_combined needs backward computation.
I0925 00:45:00.473704  4338 net.cpp:323] out3a/relu needs backward computation.
I0925 00:45:00.473708  4338 net.cpp:323] out3a/bn needs backward computation.
I0925 00:45:00.473711  4338 net.cpp:323] out3a needs backward computation.
I0925 00:45:00.473716  4338 net.cpp:323] out5a_up2 needs backward computation.
I0925 00:45:00.473719  4338 net.cpp:323] out5a/relu needs backward computation.
I0925 00:45:00.473723  4338 net.cpp:323] out5a/bn needs backward computation.
I0925 00:45:00.473727  4338 net.cpp:323] out5a needs backward computation.
I0925 00:45:00.473731  4338 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0925 00:45:00.473736  4338 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0925 00:45:00.473739  4338 net.cpp:323] res5a_branch2b needs backward computation.
I0925 00:45:00.473744  4338 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0925 00:45:00.473748  4338 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0925 00:45:00.473752  4338 net.cpp:323] res5a_branch2a needs backward computation.
I0925 00:45:00.473755  4338 net.cpp:323] pool4 needs backward computation.
I0925 00:45:00.473759  4338 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0925 00:45:00.473763  4338 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0925 00:45:00.473767  4338 net.cpp:323] res4a_branch2b needs backward computation.
I0925 00:45:00.473783  4338 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0925 00:45:00.473788  4338 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0925 00:45:00.473790  4338 net.cpp:323] res4a_branch2a needs backward computation.
I0925 00:45:00.473794  4338 net.cpp:323] pool3 needs backward computation.
I0925 00:45:00.473798  4338 net.cpp:323] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I0925 00:45:00.473803  4338 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0925 00:45:00.473806  4338 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0925 00:45:00.473810  4338 net.cpp:323] res3a_branch2b needs backward computation.
I0925 00:45:00.473825  4338 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0925 00:45:00.473835  4338 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0925 00:45:00.473847  4338 net.cpp:323] res3a_branch2a needs backward computation.
I0925 00:45:00.473860  4338 net.cpp:323] pool2 needs backward computation.
I0925 00:45:00.473871  4338 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0925 00:45:00.473881  4338 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0925 00:45:00.473891  4338 net.cpp:323] res2a_branch2b needs backward computation.
I0925 00:45:00.473901  4338 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0925 00:45:00.473909  4338 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0925 00:45:00.473918  4338 net.cpp:323] res2a_branch2a needs backward computation.
I0925 00:45:00.473928  4338 net.cpp:323] pool1 needs backward computation.
I0925 00:45:00.473937  4338 net.cpp:323] conv1b/relu needs backward computation.
I0925 00:45:00.473948  4338 net.cpp:323] conv1b/bn needs backward computation.
I0925 00:45:00.473956  4338 net.cpp:323] conv1b needs backward computation.
I0925 00:45:00.473965  4338 net.cpp:323] conv1a/relu needs backward computation.
I0925 00:45:00.473974  4338 net.cpp:323] conv1a/bn needs backward computation.
I0925 00:45:00.473984  4338 net.cpp:323] conv1a needs backward computation.
I0925 00:45:00.473994  4338 net.cpp:325] data/bias does not need backward computation.
I0925 00:45:00.474004  4338 net.cpp:325] data does not need backward computation.
I0925 00:45:00.474012  4338 net.cpp:367] This network produces output loss
I0925 00:45:00.474082  4338 net.cpp:389] Top memory (TRAIN) required for data: 1435238408 diff: 1435238408
I0925 00:45:00.474094  4338 net.cpp:392] Bottom memory (TRAIN) required for data: 1435238400 diff: 1435238400
I0925 00:45:00.474103  4338 net.cpp:395] Shared (in-place) memory (TRAIN) by data: 772915200 diff: 772915200
I0925 00:45:00.474112  4338 net.cpp:398] Parameters memory (TRAIN) required for data: 10817840 diff: 10817840
I0925 00:45:00.474123  4338 net.cpp:401] Parameters shared memory (TRAIN) by data: 0 diff: 0
I0925 00:45:00.474130  4338 net.cpp:407] Network initialization done.
I0925 00:45:00.475040  4338 solver.cpp:177] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/test.prototxt
W0925 00:45:00.475139  4338 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0925 00:45:00.475435  4338 net.cpp:72] Initializing net from parameters: 
name: "jsegnet21v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/val-image-lmdb"
    label_list_path: "data/val-label-lmdb"
    batch_size: 2
    threads: 1
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
  accuracy_param {
    ignore_label: 255
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
    ignore_label: 255
  }
}
I0925 00:45:00.475800  4338 net.cpp:104] Using FLOAT as default forward math type
I0925 00:45:00.475805  4338 net.cpp:110] Using FLOAT as default backward math type
I0925 00:45:00.475808  4338 layer_factory.hpp:136] Creating layer 'data' of type 'ImageLabelData'
I0925 00:45:00.475812  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.475818  4338 net.cpp:184] Created Layer data (0)
I0925 00:45:00.475821  4338 net.cpp:530] data -> data
I0925 00:45:00.475827  4338 net.cpp:530] data -> label
I0925 00:45:00.475850  4338 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 2
I0925 00:45:00.475857  4338 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0925 00:45:00.521759  4441 db_lmdb.cpp:24] Opened lmdb data/val-image-lmdb
I0925 00:45:00.585211  4338 data_layer.cpp:187] (0) ReshapePrefetch 2, 3, 640, 640
I0925 00:45:00.585296  4338 data_layer.cpp:211] (0) Output data size: 2, 3, 640, 640
I0925 00:45:00.585304  4338 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0925 00:45:00.585477  4338 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 2
I0925 00:45:00.585500  4338 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0925 00:45:00.586340  4445 data_layer.cpp:101] (0) Parser threads: 1
I0925 00:45:00.586362  4445 data_layer.cpp:103] (0) Transformer threads: 1
I0925 00:45:00.624688  4446 db_lmdb.cpp:24] Opened lmdb data/val-label-lmdb
I0925 00:45:00.705718  4338 data_layer.cpp:187] (0) ReshapePrefetch 2, 1, 640, 640
I0925 00:45:00.705847  4338 data_layer.cpp:211] (0) Output data size: 2, 1, 640, 640
I0925 00:45:00.705890  4338 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0925 00:45:00.705938  4338 net.cpp:245] Setting up data
I0925 00:45:00.706696  4338 net.cpp:252] TEST Top shape for layer 0 'data' 2 3 640 640 (2457600)
I0925 00:45:00.706719  4338 net.cpp:252] TEST Top shape for layer 0 'data' 2 1 640 640 (819200)
I0925 00:45:00.706732  4338 layer_factory.hpp:136] Creating layer 'label_data_1_split' of type 'Split'
I0925 00:45:00.706743  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.706756  4447 data_layer.cpp:101] (0) Parser threads: 1
I0925 00:45:00.706770  4447 data_layer.cpp:103] (0) Transformer threads: 1
I0925 00:45:00.706756  4338 net.cpp:184] Created Layer label_data_1_split (1)
I0925 00:45:00.706787  4338 net.cpp:561] label_data_1_split <- label
I0925 00:45:00.706797  4338 net.cpp:530] label_data_1_split -> label_data_1_split_0
I0925 00:45:00.706809  4338 net.cpp:530] label_data_1_split -> label_data_1_split_1
I0925 00:45:00.706818  4338 net.cpp:530] label_data_1_split -> label_data_1_split_2
I0925 00:45:00.708442  4338 net.cpp:245] Setting up label_data_1_split
I0925 00:45:00.708479  4338 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0925 00:45:00.708492  4338 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0925 00:45:00.708503  4338 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0925 00:45:00.708513  4338 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0925 00:45:00.708524  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.708541  4338 net.cpp:184] Created Layer data/bias (2)
I0925 00:45:00.708551  4338 net.cpp:561] data/bias <- data
I0925 00:45:00.708562  4338 net.cpp:530] data/bias -> data/bias
I0925 00:45:00.710978  4338 net.cpp:245] Setting up data/bias
I0925 00:45:00.711019  4338 net.cpp:252] TEST Top shape for layer 2 'data/bias' 2 3 640 640 (2457600)
I0925 00:45:00.711036  4338 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0925 00:45:00.711047  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.711067  4338 net.cpp:184] Created Layer conv1a (3)
I0925 00:45:00.711077  4338 net.cpp:561] conv1a <- data/bias
I0925 00:45:00.711087  4338 net.cpp:530] conv1a -> conv1a
I0925 00:45:00.711865  4338 net.cpp:245] Setting up conv1a
I0925 00:45:00.711876  4338 net.cpp:252] TEST Top shape for layer 3 'conv1a' 2 32 320 320 (6553600)
I0925 00:45:00.711884  4338 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0925 00:45:00.711889  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.711899  4338 net.cpp:184] Created Layer conv1a/bn (4)
I0925 00:45:00.711905  4338 net.cpp:561] conv1a/bn <- conv1a
I0925 00:45:00.711908  4338 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0925 00:45:00.712869  4338 net.cpp:245] Setting up conv1a/bn
I0925 00:45:00.712882  4338 net.cpp:252] TEST Top shape for layer 4 'conv1a/bn' 2 32 320 320 (6553600)
I0925 00:45:00.712891  4338 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0925 00:45:00.712898  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.712903  4338 net.cpp:184] Created Layer conv1a/relu (5)
I0925 00:45:00.712908  4338 net.cpp:561] conv1a/relu <- conv1a
I0925 00:45:00.712911  4338 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0925 00:45:00.712918  4338 net.cpp:245] Setting up conv1a/relu
I0925 00:45:00.712924  4338 net.cpp:252] TEST Top shape for layer 5 'conv1a/relu' 2 32 320 320 (6553600)
I0925 00:45:00.712927  4338 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0925 00:45:00.712930  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.712941  4338 net.cpp:184] Created Layer conv1b (6)
I0925 00:45:00.712956  4338 net.cpp:561] conv1b <- conv1a
I0925 00:45:00.712961  4338 net.cpp:530] conv1b -> conv1b
I0925 00:45:00.713542  4338 net.cpp:245] Setting up conv1b
I0925 00:45:00.713551  4338 net.cpp:252] TEST Top shape for layer 6 'conv1b' 2 32 320 320 (6553600)
I0925 00:45:00.713559  4338 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0925 00:45:00.713564  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.713572  4338 net.cpp:184] Created Layer conv1b/bn (7)
I0925 00:45:00.713575  4338 net.cpp:561] conv1b/bn <- conv1b
I0925 00:45:00.713579  4338 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0925 00:45:00.871358  4338 net.cpp:245] Setting up conv1b/bn
I0925 00:45:00.871387  4338 net.cpp:252] TEST Top shape for layer 7 'conv1b/bn' 2 32 320 320 (6553600)
I0925 00:45:00.871402  4338 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0925 00:45:00.871408  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.871417  4338 net.cpp:184] Created Layer conv1b/relu (8)
I0925 00:45:00.871423  4338 net.cpp:561] conv1b/relu <- conv1b
I0925 00:45:00.871428  4338 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0925 00:45:00.871433  4338 net.cpp:245] Setting up conv1b/relu
I0925 00:45:00.871438  4338 net.cpp:252] TEST Top shape for layer 8 'conv1b/relu' 2 32 320 320 (6553600)
I0925 00:45:00.871441  4338 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0925 00:45:00.871445  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.871453  4338 net.cpp:184] Created Layer pool1 (9)
I0925 00:45:00.871456  4338 net.cpp:561] pool1 <- conv1b
I0925 00:45:00.871460  4338 net.cpp:530] pool1 -> pool1
I0925 00:45:00.871557  4338 net.cpp:245] Setting up pool1
I0925 00:45:00.871565  4338 net.cpp:252] TEST Top shape for layer 9 'pool1' 2 32 160 160 (1638400)
I0925 00:45:00.871568  4338 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0925 00:45:00.871572  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.871585  4338 net.cpp:184] Created Layer res2a_branch2a (10)
I0925 00:45:00.871589  4338 net.cpp:561] res2a_branch2a <- pool1
I0925 00:45:00.871593  4338 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0925 00:45:00.872725  4338 net.cpp:245] Setting up res2a_branch2a
I0925 00:45:00.872735  4338 net.cpp:252] TEST Top shape for layer 10 'res2a_branch2a' 2 64 160 160 (3276800)
I0925 00:45:00.872742  4338 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0925 00:45:00.872747  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.872755  4338 net.cpp:184] Created Layer res2a_branch2a/bn (11)
I0925 00:45:00.872759  4338 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0925 00:45:00.872763  4338 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0925 00:45:00.873914  4338 net.cpp:245] Setting up res2a_branch2a/bn
I0925 00:45:00.873924  4338 net.cpp:252] TEST Top shape for layer 11 'res2a_branch2a/bn' 2 64 160 160 (3276800)
I0925 00:45:00.873932  4338 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0925 00:45:00.873936  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.873941  4338 net.cpp:184] Created Layer res2a_branch2a/relu (12)
I0925 00:45:00.873944  4338 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0925 00:45:00.873949  4338 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0925 00:45:00.873953  4338 net.cpp:245] Setting up res2a_branch2a/relu
I0925 00:45:00.873957  4338 net.cpp:252] TEST Top shape for layer 12 'res2a_branch2a/relu' 2 64 160 160 (3276800)
I0925 00:45:00.873960  4338 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0925 00:45:00.873986  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.874009  4338 net.cpp:184] Created Layer res2a_branch2b (13)
I0925 00:45:00.874020  4338 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0925 00:45:00.874029  4338 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0925 00:45:00.874912  4338 net.cpp:245] Setting up res2a_branch2b
I0925 00:45:00.874922  4338 net.cpp:252] TEST Top shape for layer 13 'res2a_branch2b' 2 64 160 160 (3276800)
I0925 00:45:00.874928  4338 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0925 00:45:00.874943  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.874958  4338 net.cpp:184] Created Layer res2a_branch2b/bn (14)
I0925 00:45:00.874967  4338 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0925 00:45:00.874977  4338 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0925 00:45:00.876071  4338 net.cpp:245] Setting up res2a_branch2b/bn
I0925 00:45:00.876081  4338 net.cpp:252] TEST Top shape for layer 14 'res2a_branch2b/bn' 2 64 160 160 (3276800)
I0925 00:45:00.876091  4338 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0925 00:45:00.876104  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.876116  4338 net.cpp:184] Created Layer res2a_branch2b/relu (15)
I0925 00:45:00.876127  4338 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0925 00:45:00.876135  4338 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0925 00:45:00.876147  4338 net.cpp:245] Setting up res2a_branch2b/relu
I0925 00:45:00.876158  4338 net.cpp:252] TEST Top shape for layer 15 'res2a_branch2b/relu' 2 64 160 160 (3276800)
I0925 00:45:00.876168  4338 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0925 00:45:00.876181  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.876200  4338 net.cpp:184] Created Layer pool2 (16)
I0925 00:45:00.876210  4338 net.cpp:561] pool2 <- res2a_branch2b
I0925 00:45:00.876220  4338 net.cpp:530] pool2 -> pool2
I0925 00:45:00.876324  4338 net.cpp:245] Setting up pool2
I0925 00:45:00.876332  4338 net.cpp:252] TEST Top shape for layer 16 'pool2' 2 64 80 80 (819200)
I0925 00:45:00.876335  4338 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0925 00:45:00.876338  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.876353  4338 net.cpp:184] Created Layer res3a_branch2a (17)
I0925 00:45:00.876363  4338 net.cpp:561] res3a_branch2a <- pool2
I0925 00:45:00.876374  4338 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0925 00:45:00.878901  4338 net.cpp:245] Setting up res3a_branch2a
I0925 00:45:00.878919  4338 net.cpp:252] TEST Top shape for layer 17 'res3a_branch2a' 2 128 80 80 (1638400)
I0925 00:45:00.878932  4338 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0925 00:45:00.878942  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.878960  4338 net.cpp:184] Created Layer res3a_branch2a/bn (18)
I0925 00:45:00.878971  4338 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0925 00:45:00.878980  4338 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0925 00:45:00.879881  4338 net.cpp:245] Setting up res3a_branch2a/bn
I0925 00:45:00.879899  4338 net.cpp:252] TEST Top shape for layer 18 'res3a_branch2a/bn' 2 128 80 80 (1638400)
I0925 00:45:00.879917  4338 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0925 00:45:00.879927  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.879940  4338 net.cpp:184] Created Layer res3a_branch2a/relu (19)
I0925 00:45:00.879948  4338 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0925 00:45:00.879957  4338 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0925 00:45:00.879972  4338 net.cpp:245] Setting up res3a_branch2a/relu
I0925 00:45:00.879989  4338 net.cpp:252] TEST Top shape for layer 19 'res3a_branch2a/relu' 2 128 80 80 (1638400)
I0925 00:45:00.879998  4338 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0925 00:45:00.880007  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.880022  4338 net.cpp:184] Created Layer res3a_branch2b (20)
I0925 00:45:00.880031  4338 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0925 00:45:00.880040  4338 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0925 00:45:00.881551  4338 net.cpp:245] Setting up res3a_branch2b
I0925 00:45:00.881572  4338 net.cpp:252] TEST Top shape for layer 20 'res3a_branch2b' 2 128 80 80 (1638400)
I0925 00:45:00.881587  4338 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0925 00:45:00.881597  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.881609  4338 net.cpp:184] Created Layer res3a_branch2b/bn (21)
I0925 00:45:00.881619  4338 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0925 00:45:00.881629  4338 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0925 00:45:00.882565  4338 net.cpp:245] Setting up res3a_branch2b/bn
I0925 00:45:00.882586  4338 net.cpp:252] TEST Top shape for layer 21 'res3a_branch2b/bn' 2 128 80 80 (1638400)
I0925 00:45:00.882603  4338 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0925 00:45:00.882613  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.882625  4338 net.cpp:184] Created Layer res3a_branch2b/relu (22)
I0925 00:45:00.882635  4338 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0925 00:45:00.882644  4338 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0925 00:45:00.882656  4338 net.cpp:245] Setting up res3a_branch2b/relu
I0925 00:45:00.882668  4338 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b/relu' 2 128 80 80 (1638400)
I0925 00:45:00.882678  4338 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0925 00:45:00.882688  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.882699  4338 net.cpp:184] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (23)
I0925 00:45:00.882707  4338 net.cpp:561] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0925 00:45:00.882717  4338 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0925 00:45:00.882728  4338 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0925 00:45:00.882804  4338 net.cpp:245] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0925 00:45:00.882819  4338 net.cpp:252] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 2 128 80 80 (1638400)
I0925 00:45:00.882833  4338 net.cpp:252] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 2 128 80 80 (1638400)
I0925 00:45:00.882843  4338 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0925 00:45:00.882853  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.882864  4338 net.cpp:184] Created Layer pool3 (24)
I0925 00:45:00.882874  4338 net.cpp:561] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0925 00:45:00.882884  4338 net.cpp:530] pool3 -> pool3
I0925 00:45:00.882979  4338 net.cpp:245] Setting up pool3
I0925 00:45:00.882993  4338 net.cpp:252] TEST Top shape for layer 24 'pool3' 2 128 40 40 (409600)
I0925 00:45:00.883004  4338 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0925 00:45:00.883014  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.883028  4338 net.cpp:184] Created Layer res4a_branch2a (25)
I0925 00:45:00.883043  4338 net.cpp:561] res4a_branch2a <- pool3
I0925 00:45:00.883054  4338 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0925 00:45:00.891624  4338 net.cpp:245] Setting up res4a_branch2a
I0925 00:45:00.891667  4338 net.cpp:252] TEST Top shape for layer 25 'res4a_branch2a' 2 256 40 40 (819200)
I0925 00:45:00.891683  4338 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0925 00:45:00.891693  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.891707  4338 net.cpp:184] Created Layer res4a_branch2a/bn (26)
I0925 00:45:00.891718  4338 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0925 00:45:00.891728  4338 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0925 00:45:00.892976  4338 net.cpp:245] Setting up res4a_branch2a/bn
I0925 00:45:00.892987  4338 net.cpp:252] TEST Top shape for layer 26 'res4a_branch2a/bn' 2 256 40 40 (819200)
I0925 00:45:00.892997  4338 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0925 00:45:00.893000  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.893007  4338 net.cpp:184] Created Layer res4a_branch2a/relu (27)
I0925 00:45:00.893012  4338 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0925 00:45:00.893015  4338 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0925 00:45:00.893021  4338 net.cpp:245] Setting up res4a_branch2a/relu
I0925 00:45:00.893026  4338 net.cpp:252] TEST Top shape for layer 27 'res4a_branch2a/relu' 2 256 40 40 (819200)
I0925 00:45:00.893029  4338 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0925 00:45:00.893034  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.893043  4338 net.cpp:184] Created Layer res4a_branch2b (28)
I0925 00:45:00.893048  4338 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0925 00:45:00.893051  4338 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0925 00:45:00.900595  4338 net.cpp:245] Setting up res4a_branch2b
I0925 00:45:00.900671  4338 net.cpp:252] TEST Top shape for layer 28 'res4a_branch2b' 2 256 40 40 (819200)
I0925 00:45:00.900696  4338 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0925 00:45:00.900707  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.900738  4338 net.cpp:184] Created Layer res4a_branch2b/bn (29)
I0925 00:45:00.900750  4338 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0925 00:45:00.900761  4338 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0925 00:45:00.901756  4338 net.cpp:245] Setting up res4a_branch2b/bn
I0925 00:45:00.901777  4338 net.cpp:252] TEST Top shape for layer 29 'res4a_branch2b/bn' 2 256 40 40 (819200)
I0925 00:45:00.901793  4338 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0925 00:45:00.901803  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.901818  4338 net.cpp:184] Created Layer res4a_branch2b/relu (30)
I0925 00:45:00.901829  4338 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0925 00:45:00.901839  4338 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0925 00:45:00.901851  4338 net.cpp:245] Setting up res4a_branch2b/relu
I0925 00:45:00.901861  4338 net.cpp:252] TEST Top shape for layer 30 'res4a_branch2b/relu' 2 256 40 40 (819200)
I0925 00:45:00.901870  4338 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0925 00:45:00.901880  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.901890  4338 net.cpp:184] Created Layer pool4 (31)
I0925 00:45:00.901899  4338 net.cpp:561] pool4 <- res4a_branch2b
I0925 00:45:00.901908  4338 net.cpp:530] pool4 -> pool4
I0925 00:45:00.902016  4338 net.cpp:245] Setting up pool4
I0925 00:45:00.902030  4338 net.cpp:252] TEST Top shape for layer 31 'pool4' 2 256 40 40 (819200)
I0925 00:45:00.902045  4338 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0925 00:45:00.902062  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.902083  4338 net.cpp:184] Created Layer res5a_branch2a (32)
I0925 00:45:00.902093  4338 net.cpp:561] res5a_branch2a <- pool4
I0925 00:45:00.902103  4338 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0925 00:45:00.936635  4338 net.cpp:245] Setting up res5a_branch2a
I0925 00:45:00.936691  4338 net.cpp:252] TEST Top shape for layer 32 'res5a_branch2a' 2 512 40 40 (1638400)
I0925 00:45:00.936710  4338 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0925 00:45:00.936722  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.936738  4338 net.cpp:184] Created Layer res5a_branch2a/bn (33)
I0925 00:45:00.936748  4338 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0925 00:45:00.936758  4338 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0925 00:45:00.937780  4338 net.cpp:245] Setting up res5a_branch2a/bn
I0925 00:45:00.937803  4338 net.cpp:252] TEST Top shape for layer 33 'res5a_branch2a/bn' 2 512 40 40 (1638400)
I0925 00:45:00.937819  4338 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0925 00:45:00.937830  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.937842  4338 net.cpp:184] Created Layer res5a_branch2a/relu (34)
I0925 00:45:00.937851  4338 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0925 00:45:00.937860  4338 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0925 00:45:00.937872  4338 net.cpp:245] Setting up res5a_branch2a/relu
I0925 00:45:00.937882  4338 net.cpp:252] TEST Top shape for layer 34 'res5a_branch2a/relu' 2 512 40 40 (1638400)
I0925 00:45:00.937891  4338 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0925 00:45:00.937901  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.937914  4338 net.cpp:184] Created Layer res5a_branch2b (35)
I0925 00:45:00.937925  4338 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0925 00:45:00.937934  4338 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0925 00:45:00.952705  4338 net.cpp:245] Setting up res5a_branch2b
I0925 00:45:00.952729  4338 net.cpp:252] TEST Top shape for layer 35 'res5a_branch2b' 2 512 40 40 (1638400)
I0925 00:45:00.952742  4338 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0925 00:45:00.952747  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.952757  4338 net.cpp:184] Created Layer res5a_branch2b/bn (36)
I0925 00:45:00.952764  4338 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0925 00:45:00.952769  4338 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0925 00:45:00.953718  4338 net.cpp:245] Setting up res5a_branch2b/bn
I0925 00:45:00.953728  4338 net.cpp:252] TEST Top shape for layer 36 'res5a_branch2b/bn' 2 512 40 40 (1638400)
I0925 00:45:00.953738  4338 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0925 00:45:00.953743  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.953749  4338 net.cpp:184] Created Layer res5a_branch2b/relu (37)
I0925 00:45:00.953753  4338 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0925 00:45:00.953758  4338 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0925 00:45:00.953763  4338 net.cpp:245] Setting up res5a_branch2b/relu
I0925 00:45:00.953768  4338 net.cpp:252] TEST Top shape for layer 37 'res5a_branch2b/relu' 2 512 40 40 (1638400)
I0925 00:45:00.953771  4338 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I0925 00:45:00.953775  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.953793  4338 net.cpp:184] Created Layer out5a (38)
I0925 00:45:00.953795  4338 net.cpp:561] out5a <- res5a_branch2b
I0925 00:45:00.953799  4338 net.cpp:530] out5a -> out5a
I0925 00:45:00.957268  4338 net.cpp:245] Setting up out5a
I0925 00:45:00.957279  4338 net.cpp:252] TEST Top shape for layer 38 'out5a' 2 64 40 40 (204800)
I0925 00:45:00.957286  4338 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I0925 00:45:00.957290  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.957299  4338 net.cpp:184] Created Layer out5a/bn (39)
I0925 00:45:00.957304  4338 net.cpp:561] out5a/bn <- out5a
I0925 00:45:00.957309  4338 net.cpp:513] out5a/bn -> out5a (in-place)
I0925 00:45:00.958088  4338 net.cpp:245] Setting up out5a/bn
I0925 00:45:00.958097  4338 net.cpp:252] TEST Top shape for layer 39 'out5a/bn' 2 64 40 40 (204800)
I0925 00:45:00.958107  4338 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I0925 00:45:00.958113  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.958118  4338 net.cpp:184] Created Layer out5a/relu (40)
I0925 00:45:00.958122  4338 net.cpp:561] out5a/relu <- out5a
I0925 00:45:00.958127  4338 net.cpp:513] out5a/relu -> out5a (in-place)
I0925 00:45:00.958132  4338 net.cpp:245] Setting up out5a/relu
I0925 00:45:00.958137  4338 net.cpp:252] TEST Top shape for layer 40 'out5a/relu' 2 64 40 40 (204800)
I0925 00:45:00.958140  4338 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I0925 00:45:00.958143  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.958151  4338 net.cpp:184] Created Layer out5a_up2 (41)
I0925 00:45:00.958154  4338 net.cpp:561] out5a_up2 <- out5a
I0925 00:45:00.958159  4338 net.cpp:530] out5a_up2 -> out5a_up2
I0925 00:45:00.958575  4338 net.cpp:245] Setting up out5a_up2
I0925 00:45:00.958583  4338 net.cpp:252] TEST Top shape for layer 41 'out5a_up2' 2 64 80 80 (819200)
I0925 00:45:00.958590  4338 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I0925 00:45:00.958595  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.958606  4338 net.cpp:184] Created Layer out3a (42)
I0925 00:45:00.958611  4338 net.cpp:561] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0925 00:45:00.958616  4338 net.cpp:530] out3a -> out3a
I0925 00:45:00.959838  4338 net.cpp:245] Setting up out3a
I0925 00:45:00.959847  4338 net.cpp:252] TEST Top shape for layer 42 'out3a' 2 64 80 80 (819200)
I0925 00:45:00.959854  4338 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I0925 00:45:00.959859  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.959868  4338 net.cpp:184] Created Layer out3a/bn (43)
I0925 00:45:00.959872  4338 net.cpp:561] out3a/bn <- out3a
I0925 00:45:00.959877  4338 net.cpp:513] out3a/bn -> out3a (in-place)
I0925 00:45:00.960801  4338 net.cpp:245] Setting up out3a/bn
I0925 00:45:00.960810  4338 net.cpp:252] TEST Top shape for layer 43 'out3a/bn' 2 64 80 80 (819200)
I0925 00:45:00.960819  4338 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I0925 00:45:00.960824  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.960829  4338 net.cpp:184] Created Layer out3a/relu (44)
I0925 00:45:00.960834  4338 net.cpp:561] out3a/relu <- out3a
I0925 00:45:00.960839  4338 net.cpp:513] out3a/relu -> out3a (in-place)
I0925 00:45:00.960845  4338 net.cpp:245] Setting up out3a/relu
I0925 00:45:00.960850  4338 net.cpp:252] TEST Top shape for layer 44 'out3a/relu' 2 64 80 80 (819200)
I0925 00:45:00.960855  4338 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I0925 00:45:00.960858  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.960865  4338 net.cpp:184] Created Layer out3_out5_combined (45)
I0925 00:45:00.960870  4338 net.cpp:561] out3_out5_combined <- out5a_up2
I0925 00:45:00.960875  4338 net.cpp:561] out3_out5_combined <- out3a
I0925 00:45:00.960889  4338 net.cpp:530] out3_out5_combined -> out3_out5_combined
I0925 00:45:00.960922  4338 net.cpp:245] Setting up out3_out5_combined
I0925 00:45:00.960927  4338 net.cpp:252] TEST Top shape for layer 45 'out3_out5_combined' 2 64 80 80 (819200)
I0925 00:45:00.960932  4338 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I0925 00:45:00.960937  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.960947  4338 net.cpp:184] Created Layer ctx_conv1 (46)
I0925 00:45:00.960952  4338 net.cpp:561] ctx_conv1 <- out3_out5_combined
I0925 00:45:00.960955  4338 net.cpp:530] ctx_conv1 -> ctx_conv1
I0925 00:45:00.962162  4338 net.cpp:245] Setting up ctx_conv1
I0925 00:45:00.962175  4338 net.cpp:252] TEST Top shape for layer 46 'ctx_conv1' 2 64 80 80 (819200)
I0925 00:45:00.962182  4338 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I0925 00:45:00.962188  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.962198  4338 net.cpp:184] Created Layer ctx_conv1/bn (47)
I0925 00:45:00.962201  4338 net.cpp:561] ctx_conv1/bn <- ctx_conv1
I0925 00:45:00.962208  4338 net.cpp:513] ctx_conv1/bn -> ctx_conv1 (in-place)
I0925 00:45:00.963151  4338 net.cpp:245] Setting up ctx_conv1/bn
I0925 00:45:00.963163  4338 net.cpp:252] TEST Top shape for layer 47 'ctx_conv1/bn' 2 64 80 80 (819200)
I0925 00:45:00.963171  4338 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I0925 00:45:00.963176  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.963184  4338 net.cpp:184] Created Layer ctx_conv1/relu (48)
I0925 00:45:00.963188  4338 net.cpp:561] ctx_conv1/relu <- ctx_conv1
I0925 00:45:00.963192  4338 net.cpp:513] ctx_conv1/relu -> ctx_conv1 (in-place)
I0925 00:45:00.963198  4338 net.cpp:245] Setting up ctx_conv1/relu
I0925 00:45:00.963202  4338 net.cpp:252] TEST Top shape for layer 48 'ctx_conv1/relu' 2 64 80 80 (819200)
I0925 00:45:00.963207  4338 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I0925 00:45:00.963212  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.963222  4338 net.cpp:184] Created Layer ctx_conv2 (49)
I0925 00:45:00.963227  4338 net.cpp:561] ctx_conv2 <- ctx_conv1
I0925 00:45:00.963230  4338 net.cpp:530] ctx_conv2 -> ctx_conv2
I0925 00:45:00.964423  4338 net.cpp:245] Setting up ctx_conv2
I0925 00:45:00.964432  4338 net.cpp:252] TEST Top shape for layer 49 'ctx_conv2' 2 64 80 80 (819200)
I0925 00:45:00.964438  4338 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I0925 00:45:00.964443  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.964460  4338 net.cpp:184] Created Layer ctx_conv2/bn (50)
I0925 00:45:00.964465  4338 net.cpp:561] ctx_conv2/bn <- ctx_conv2
I0925 00:45:00.964470  4338 net.cpp:513] ctx_conv2/bn -> ctx_conv2 (in-place)
I0925 00:45:00.965345  4338 net.cpp:245] Setting up ctx_conv2/bn
I0925 00:45:00.965354  4338 net.cpp:252] TEST Top shape for layer 50 'ctx_conv2/bn' 2 64 80 80 (819200)
I0925 00:45:00.965363  4338 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I0925 00:45:00.965368  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.965375  4338 net.cpp:184] Created Layer ctx_conv2/relu (51)
I0925 00:45:00.965379  4338 net.cpp:561] ctx_conv2/relu <- ctx_conv2
I0925 00:45:00.965385  4338 net.cpp:513] ctx_conv2/relu -> ctx_conv2 (in-place)
I0925 00:45:00.965391  4338 net.cpp:245] Setting up ctx_conv2/relu
I0925 00:45:00.965396  4338 net.cpp:252] TEST Top shape for layer 51 'ctx_conv2/relu' 2 64 80 80 (819200)
I0925 00:45:00.965401  4338 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I0925 00:45:00.965405  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.965418  4338 net.cpp:184] Created Layer ctx_conv3 (52)
I0925 00:45:00.965430  4338 net.cpp:561] ctx_conv3 <- ctx_conv2
I0925 00:45:00.965435  4338 net.cpp:530] ctx_conv3 -> ctx_conv3
I0925 00:45:00.966615  4338 net.cpp:245] Setting up ctx_conv3
I0925 00:45:00.966624  4338 net.cpp:252] TEST Top shape for layer 52 'ctx_conv3' 2 64 80 80 (819200)
I0925 00:45:00.966629  4338 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I0925 00:45:00.966634  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.966642  4338 net.cpp:184] Created Layer ctx_conv3/bn (53)
I0925 00:45:00.966646  4338 net.cpp:561] ctx_conv3/bn <- ctx_conv3
I0925 00:45:00.966651  4338 net.cpp:513] ctx_conv3/bn -> ctx_conv3 (in-place)
I0925 00:45:00.967366  4338 net.cpp:245] Setting up ctx_conv3/bn
I0925 00:45:00.967375  4338 net.cpp:252] TEST Top shape for layer 53 'ctx_conv3/bn' 2 64 80 80 (819200)
I0925 00:45:00.967383  4338 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I0925 00:45:00.967387  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.967392  4338 net.cpp:184] Created Layer ctx_conv3/relu (54)
I0925 00:45:00.967397  4338 net.cpp:561] ctx_conv3/relu <- ctx_conv3
I0925 00:45:00.967401  4338 net.cpp:513] ctx_conv3/relu -> ctx_conv3 (in-place)
I0925 00:45:00.967407  4338 net.cpp:245] Setting up ctx_conv3/relu
I0925 00:45:00.967412  4338 net.cpp:252] TEST Top shape for layer 54 'ctx_conv3/relu' 2 64 80 80 (819200)
I0925 00:45:00.967417  4338 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I0925 00:45:00.967420  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.967428  4338 net.cpp:184] Created Layer ctx_conv4 (55)
I0925 00:45:00.967432  4338 net.cpp:561] ctx_conv4 <- ctx_conv3
I0925 00:45:00.967437  4338 net.cpp:530] ctx_conv4 -> ctx_conv4
I0925 00:45:00.968605  4338 net.cpp:245] Setting up ctx_conv4
I0925 00:45:00.968613  4338 net.cpp:252] TEST Top shape for layer 55 'ctx_conv4' 2 64 80 80 (819200)
I0925 00:45:00.968621  4338 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I0925 00:45:00.968626  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.968633  4338 net.cpp:184] Created Layer ctx_conv4/bn (56)
I0925 00:45:00.968637  4338 net.cpp:561] ctx_conv4/bn <- ctx_conv4
I0925 00:45:00.968641  4338 net.cpp:513] ctx_conv4/bn -> ctx_conv4 (in-place)
I0925 00:45:00.969396  4338 net.cpp:245] Setting up ctx_conv4/bn
I0925 00:45:00.969404  4338 net.cpp:252] TEST Top shape for layer 56 'ctx_conv4/bn' 2 64 80 80 (819200)
I0925 00:45:00.969413  4338 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I0925 00:45:00.969419  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.969424  4338 net.cpp:184] Created Layer ctx_conv4/relu (57)
I0925 00:45:00.969435  4338 net.cpp:561] ctx_conv4/relu <- ctx_conv4
I0925 00:45:00.969440  4338 net.cpp:513] ctx_conv4/relu -> ctx_conv4 (in-place)
I0925 00:45:00.969446  4338 net.cpp:245] Setting up ctx_conv4/relu
I0925 00:45:00.969452  4338 net.cpp:252] TEST Top shape for layer 57 'ctx_conv4/relu' 2 64 80 80 (819200)
I0925 00:45:00.969462  4338 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I0925 00:45:00.969466  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.969478  4338 net.cpp:184] Created Layer ctx_final (58)
I0925 00:45:00.969482  4338 net.cpp:561] ctx_final <- ctx_conv4
I0925 00:45:00.969486  4338 net.cpp:530] ctx_final -> ctx_final
I0925 00:45:00.970098  4338 net.cpp:245] Setting up ctx_final
I0925 00:45:00.970106  4338 net.cpp:252] TEST Top shape for layer 58 'ctx_final' 2 8 80 80 (102400)
I0925 00:45:00.970113  4338 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I0925 00:45:00.970118  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.970132  4338 net.cpp:184] Created Layer ctx_final/relu (59)
I0925 00:45:00.970137  4338 net.cpp:561] ctx_final/relu <- ctx_final
I0925 00:45:00.970141  4338 net.cpp:513] ctx_final/relu -> ctx_final (in-place)
I0925 00:45:00.970147  4338 net.cpp:245] Setting up ctx_final/relu
I0925 00:45:00.970154  4338 net.cpp:252] TEST Top shape for layer 59 'ctx_final/relu' 2 8 80 80 (102400)
I0925 00:45:00.970157  4338 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I0925 00:45:00.970162  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.970170  4338 net.cpp:184] Created Layer out_deconv_final_up2 (60)
I0925 00:45:00.970173  4338 net.cpp:561] out_deconv_final_up2 <- ctx_final
I0925 00:45:00.970178  4338 net.cpp:530] out_deconv_final_up2 -> out_deconv_final_up2
I0925 00:45:00.970479  4338 net.cpp:245] Setting up out_deconv_final_up2
I0925 00:45:00.970485  4338 net.cpp:252] TEST Top shape for layer 60 'out_deconv_final_up2' 2 8 160 160 (409600)
I0925 00:45:00.970490  4338 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I0925 00:45:00.970495  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.970504  4338 net.cpp:184] Created Layer out_deconv_final_up4 (61)
I0925 00:45:00.970507  4338 net.cpp:561] out_deconv_final_up4 <- out_deconv_final_up2
I0925 00:45:00.970512  4338 net.cpp:530] out_deconv_final_up4 -> out_deconv_final_up4
I0925 00:45:00.970804  4338 net.cpp:245] Setting up out_deconv_final_up4
I0925 00:45:00.970811  4338 net.cpp:252] TEST Top shape for layer 61 'out_deconv_final_up4' 2 8 320 320 (1638400)
I0925 00:45:00.970818  4338 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I0925 00:45:00.970824  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.970831  4338 net.cpp:184] Created Layer out_deconv_final_up8 (62)
I0925 00:45:00.970835  4338 net.cpp:561] out_deconv_final_up8 <- out_deconv_final_up4
I0925 00:45:00.970839  4338 net.cpp:530] out_deconv_final_up8 -> out_deconv_final_up8
I0925 00:45:00.971127  4338 net.cpp:245] Setting up out_deconv_final_up8
I0925 00:45:00.971134  4338 net.cpp:252] TEST Top shape for layer 62 'out_deconv_final_up8' 2 8 640 640 (6553600)
I0925 00:45:00.971139  4338 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8_out_deconv_final_up8_0_split' of type 'Split'
I0925 00:45:00.971144  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.971151  4338 net.cpp:184] Created Layer out_deconv_final_up8_out_deconv_final_up8_0_split (63)
I0925 00:45:00.971155  4338 net.cpp:561] out_deconv_final_up8_out_deconv_final_up8_0_split <- out_deconv_final_up8
I0925 00:45:00.971161  4338 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0925 00:45:00.971166  4338 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0925 00:45:00.971171  4338 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0925 00:45:00.971248  4338 net.cpp:245] Setting up out_deconv_final_up8_out_deconv_final_up8_0_split
I0925 00:45:00.971254  4338 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 8 640 640 (6553600)
I0925 00:45:00.971259  4338 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 8 640 640 (6553600)
I0925 00:45:00.971264  4338 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 8 640 640 (6553600)
I0925 00:45:00.971268  4338 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0925 00:45:00.971273  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.971289  4338 net.cpp:184] Created Layer loss (64)
I0925 00:45:00.971294  4338 net.cpp:561] loss <- out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0925 00:45:00.971300  4338 net.cpp:561] loss <- label_data_1_split_0
I0925 00:45:00.971305  4338 net.cpp:530] loss -> loss
I0925 00:45:00.972288  4338 net.cpp:245] Setting up loss
I0925 00:45:00.972298  4338 net.cpp:252] TEST Top shape for layer 64 'loss' (1)
I0925 00:45:00.972302  4338 net.cpp:256]     with loss weight 1
I0925 00:45:00.972312  4338 layer_factory.hpp:136] Creating layer 'accuracy/top1' of type 'Accuracy'
I0925 00:45:00.972317  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.972327  4338 net.cpp:184] Created Layer accuracy/top1 (65)
I0925 00:45:00.972332  4338 net.cpp:561] accuracy/top1 <- out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0925 00:45:00.972337  4338 net.cpp:561] accuracy/top1 <- label_data_1_split_1
I0925 00:45:00.972342  4338 net.cpp:530] accuracy/top1 -> accuracy/top1
I0925 00:45:00.972350  4338 net.cpp:245] Setting up accuracy/top1
I0925 00:45:00.972354  4338 net.cpp:252] TEST Top shape for layer 65 'accuracy/top1' (1)
I0925 00:45:00.972358  4338 layer_factory.hpp:136] Creating layer 'accuracy/top5' of type 'Accuracy'
I0925 00:45:00.972363  4338 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0925 00:45:00.972369  4338 net.cpp:184] Created Layer accuracy/top5 (66)
I0925 00:45:00.972373  4338 net.cpp:561] accuracy/top5 <- out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0925 00:45:00.972378  4338 net.cpp:561] accuracy/top5 <- label_data_1_split_2
I0925 00:45:00.972383  4338 net.cpp:530] accuracy/top5 -> accuracy/top5
I0925 00:45:00.972390  4338 net.cpp:245] Setting up accuracy/top5
I0925 00:45:00.972395  4338 net.cpp:252] TEST Top shape for layer 66 'accuracy/top5' (1)
I0925 00:45:00.972399  4338 net.cpp:325] accuracy/top5 does not need backward computation.
I0925 00:45:00.972404  4338 net.cpp:325] accuracy/top1 does not need backward computation.
I0925 00:45:00.972409  4338 net.cpp:323] loss needs backward computation.
I0925 00:45:00.972412  4338 net.cpp:323] out_deconv_final_up8_out_deconv_final_up8_0_split needs backward computation.
I0925 00:45:00.972417  4338 net.cpp:323] out_deconv_final_up8 needs backward computation.
I0925 00:45:00.972420  4338 net.cpp:323] out_deconv_final_up4 needs backward computation.
I0925 00:45:00.972425  4338 net.cpp:323] out_deconv_final_up2 needs backward computation.
I0925 00:45:00.972429  4338 net.cpp:323] ctx_final/relu needs backward computation.
I0925 00:45:00.972432  4338 net.cpp:323] ctx_final needs backward computation.
I0925 00:45:00.972437  4338 net.cpp:323] ctx_conv4/relu needs backward computation.
I0925 00:45:00.972441  4338 net.cpp:323] ctx_conv4/bn needs backward computation.
I0925 00:45:00.972445  4338 net.cpp:323] ctx_conv4 needs backward computation.
I0925 00:45:00.972448  4338 net.cpp:323] ctx_conv3/relu needs backward computation.
I0925 00:45:00.972452  4338 net.cpp:323] ctx_conv3/bn needs backward computation.
I0925 00:45:00.972456  4338 net.cpp:323] ctx_conv3 needs backward computation.
I0925 00:45:00.972461  4338 net.cpp:323] ctx_conv2/relu needs backward computation.
I0925 00:45:00.972465  4338 net.cpp:323] ctx_conv2/bn needs backward computation.
I0925 00:45:00.972470  4338 net.cpp:323] ctx_conv2 needs backward computation.
I0925 00:45:00.972472  4338 net.cpp:323] ctx_conv1/relu needs backward computation.
I0925 00:45:00.972474  4338 net.cpp:323] ctx_conv1/bn needs backward computation.
I0925 00:45:00.972476  4338 net.cpp:323] ctx_conv1 needs backward computation.
I0925 00:45:00.972478  4338 net.cpp:323] out3_out5_combined needs backward computation.
I0925 00:45:00.972481  4338 net.cpp:323] out3a/relu needs backward computation.
I0925 00:45:00.972482  4338 net.cpp:323] out3a/bn needs backward computation.
I0925 00:45:00.972484  4338 net.cpp:323] out3a needs backward computation.
I0925 00:45:00.972486  4338 net.cpp:323] out5a_up2 needs backward computation.
I0925 00:45:00.972496  4338 net.cpp:323] out5a/relu needs backward computation.
I0925 00:45:00.972497  4338 net.cpp:323] out5a/bn needs backward computation.
I0925 00:45:00.972499  4338 net.cpp:323] out5a needs backward computation.
I0925 00:45:00.972501  4338 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0925 00:45:00.972503  4338 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0925 00:45:00.972506  4338 net.cpp:323] res5a_branch2b needs backward computation.
I0925 00:45:00.972507  4338 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0925 00:45:00.972509  4338 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0925 00:45:00.972512  4338 net.cpp:323] res5a_branch2a needs backward computation.
I0925 00:45:00.972513  4338 net.cpp:323] pool4 needs backward computation.
I0925 00:45:00.972515  4338 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0925 00:45:00.972517  4338 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0925 00:45:00.972518  4338 net.cpp:323] res4a_branch2b needs backward computation.
I0925 00:45:00.972520  4338 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0925 00:45:00.972522  4338 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0925 00:45:00.972524  4338 net.cpp:323] res4a_branch2a needs backward computation.
I0925 00:45:00.972527  4338 net.cpp:323] pool3 needs backward computation.
I0925 00:45:00.972528  4338 net.cpp:323] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I0925 00:45:00.972530  4338 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0925 00:45:00.972532  4338 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0925 00:45:00.972534  4338 net.cpp:323] res3a_branch2b needs backward computation.
I0925 00:45:00.972537  4338 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0925 00:45:00.972538  4338 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0925 00:45:00.972540  4338 net.cpp:323] res3a_branch2a needs backward computation.
I0925 00:45:00.972543  4338 net.cpp:323] pool2 needs backward computation.
I0925 00:45:00.972544  4338 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0925 00:45:00.972546  4338 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0925 00:45:00.972548  4338 net.cpp:323] res2a_branch2b needs backward computation.
I0925 00:45:00.972549  4338 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0925 00:45:00.972551  4338 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0925 00:45:00.972553  4338 net.cpp:323] res2a_branch2a needs backward computation.
I0925 00:45:00.972555  4338 net.cpp:323] pool1 needs backward computation.
I0925 00:45:00.972558  4338 net.cpp:323] conv1b/relu needs backward computation.
I0925 00:45:00.972559  4338 net.cpp:323] conv1b/bn needs backward computation.
I0925 00:45:00.972561  4338 net.cpp:323] conv1b needs backward computation.
I0925 00:45:00.972563  4338 net.cpp:323] conv1a/relu needs backward computation.
I0925 00:45:00.972565  4338 net.cpp:323] conv1a/bn needs backward computation.
I0925 00:45:00.972568  4338 net.cpp:323] conv1a needs backward computation.
I0925 00:45:00.972569  4338 net.cpp:325] data/bias does not need backward computation.
I0925 00:45:00.972573  4338 net.cpp:325] label_data_1_split does not need backward computation.
I0925 00:45:00.972574  4338 net.cpp:325] data does not need backward computation.
I0925 00:45:00.972576  4338 net.cpp:367] This network produces output accuracy/top1
I0925 00:45:00.972579  4338 net.cpp:367] This network produces output accuracy/top5
I0925 00:45:00.972580  4338 net.cpp:367] This network produces output loss
I0925 00:45:00.972625  4338 net.cpp:389] Top memory (TEST) required for data: 566886424 diff: 566886424
I0925 00:45:00.972628  4338 net.cpp:392] Bottom memory (TEST) required for data: 566886400 diff: 566886400
I0925 00:45:00.972630  4338 net.cpp:395] Shared (in-place) memory (TEST) by data: 257638400 diff: 257638400
I0925 00:45:00.972631  4338 net.cpp:398] Parameters memory (TEST) required for data: 10817840 diff: 10817840
I0925 00:45:00.972636  4338 net.cpp:401] Parameters shared memory (TEST) by data: 0 diff: 0
I0925 00:45:00.972638  4338 net.cpp:407] Network initialization done.
I0925 00:45:00.972712  4338 solver.cpp:57] Solver scaffolding done.
I0925 00:45:00.979725  4338 caffe.cpp:143] Finetuning from training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/l1reg/cityscapes5_jsegnet21v2_iter_60000.caffemodel
I0925 00:45:01.085490  4338 net.cpp:1094] Copying source layer data Type:ImageLabelData #blobs=0
I0925 00:45:01.085513  4338 net.cpp:1094] Copying source layer data/bias Type:Bias #blobs=1
I0925 00:45:01.085546  4338 net.cpp:1094] Copying source layer conv1a Type:Convolution #blobs=2
I0925 00:45:01.085580  4338 net.cpp:1094] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0925 00:45:01.085929  4338 net.cpp:1094] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0925 00:45:01.085937  4338 net.cpp:1094] Copying source layer conv1b Type:Convolution #blobs=2
I0925 00:45:01.085949  4338 net.cpp:1094] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0925 00:45:01.086149  4338 net.cpp:1094] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0925 00:45:01.086155  4338 net.cpp:1094] Copying source layer pool1 Type:Pooling #blobs=0
I0925 00:45:01.086159  4338 net.cpp:1094] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0925 00:45:01.086179  4338 net.cpp:1094] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0925 00:45:01.086380  4338 net.cpp:1094] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0925 00:45:01.086386  4338 net.cpp:1094] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0925 00:45:01.086401  4338 net.cpp:1094] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0925 00:45:01.086585  4338 net.cpp:1094] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0925 00:45:01.086591  4338 net.cpp:1094] Copying source layer pool2 Type:Pooling #blobs=0
I0925 00:45:01.086596  4338 net.cpp:1094] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0925 00:45:01.086645  4338 net.cpp:1094] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0925 00:45:01.086818  4338 net.cpp:1094] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0925 00:45:01.086824  4338 net.cpp:1094] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0925 00:45:01.086853  4338 net.cpp:1094] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0925 00:45:01.087010  4338 net.cpp:1094] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0925 00:45:01.087018  4338 net.cpp:1094] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I0925 00:45:01.087020  4338 net.cpp:1094] Copying source layer pool3 Type:Pooling #blobs=0
I0925 00:45:01.087024  4338 net.cpp:1094] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0925 00:45:01.087153  4338 net.cpp:1094] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0925 00:45:01.087318  4338 net.cpp:1094] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0925 00:45:01.087326  4338 net.cpp:1094] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0925 00:45:01.087390  4338 net.cpp:1094] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0925 00:45:01.087548  4338 net.cpp:1094] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0925 00:45:01.087553  4338 net.cpp:1094] Copying source layer pool4 Type:Pooling #blobs=0
I0925 00:45:01.087558  4338 net.cpp:1094] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0925 00:45:01.088001  4338 net.cpp:1094] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0925 00:45:01.088165  4338 net.cpp:1094] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0925 00:45:01.088171  4338 net.cpp:1094] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0925 00:45:01.088397  4338 net.cpp:1094] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0925 00:45:01.088560  4338 net.cpp:1094] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0925 00:45:01.088577  4338 net.cpp:1094] Copying source layer out5a Type:Convolution #blobs=2
I0925 00:45:01.088644  4338 net.cpp:1094] Copying source layer out5a/bn Type:BatchNorm #blobs=5
I0925 00:45:01.088850  4338 net.cpp:1094] Copying source layer out5a/relu Type:ReLU #blobs=0
I0925 00:45:01.088855  4338 net.cpp:1094] Copying source layer out5a_up2 Type:Deconvolution #blobs=1
I0925 00:45:01.088863  4338 net.cpp:1094] Copying source layer out3a Type:Convolution #blobs=2
I0925 00:45:01.088888  4338 net.cpp:1094] Copying source layer out3a/bn Type:BatchNorm #blobs=5
I0925 00:45:01.089079  4338 net.cpp:1094] Copying source layer out3a/relu Type:ReLU #blobs=0
I0925 00:45:01.089087  4338 net.cpp:1094] Copying source layer out3_out5_combined Type:Eltwise #blobs=0
I0925 00:45:01.089089  4338 net.cpp:1094] Copying source layer ctx_conv1 Type:Convolution #blobs=2
I0925 00:45:01.089113  4338 net.cpp:1094] Copying source layer ctx_conv1/bn Type:BatchNorm #blobs=5
I0925 00:45:01.089303  4338 net.cpp:1094] Copying source layer ctx_conv1/relu Type:ReLU #blobs=0
I0925 00:45:01.089309  4338 net.cpp:1094] Copying source layer ctx_conv2 Type:Convolution #blobs=2
I0925 00:45:01.089335  4338 net.cpp:1094] Copying source layer ctx_conv2/bn Type:BatchNorm #blobs=5
I0925 00:45:01.089525  4338 net.cpp:1094] Copying source layer ctx_conv2/relu Type:ReLU #blobs=0
I0925 00:45:01.089532  4338 net.cpp:1094] Copying source layer ctx_conv3 Type:Convolution #blobs=2
I0925 00:45:01.089560  4338 net.cpp:1094] Copying source layer ctx_conv3/bn Type:BatchNorm #blobs=5
I0925 00:45:01.089747  4338 net.cpp:1094] Copying source layer ctx_conv3/relu Type:ReLU #blobs=0
I0925 00:45:01.089754  4338 net.cpp:1094] Copying source layer ctx_conv4 Type:Convolution #blobs=2
I0925 00:45:01.089783  4338 net.cpp:1094] Copying source layer ctx_conv4/bn Type:BatchNorm #blobs=5
I0925 00:45:01.089973  4338 net.cpp:1094] Copying source layer ctx_conv4/relu Type:ReLU #blobs=0
I0925 00:45:01.089979  4338 net.cpp:1094] Copying source layer ctx_final Type:Convolution #blobs=2
I0925 00:45:01.089993  4338 net.cpp:1094] Copying source layer ctx_final/relu Type:ReLU #blobs=0
I0925 00:45:01.089996  4338 net.cpp:1094] Copying source layer out_deconv_final_up2 Type:Deconvolution #blobs=1
I0925 00:45:01.090003  4338 net.cpp:1094] Copying source layer out_deconv_final_up4 Type:Deconvolution #blobs=1
I0925 00:45:01.090011  4338 net.cpp:1094] Copying source layer out_deconv_final_up8 Type:Deconvolution #blobs=1
I0925 00:45:01.090018  4338 net.cpp:1094] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0925 00:45:01.104526  4338 net.cpp:1094] Copying source layer data Type:ImageLabelData #blobs=0
I0925 00:45:01.104549  4338 net.cpp:1094] Copying source layer data/bias Type:Bias #blobs=1
I0925 00:45:01.104580  4338 net.cpp:1094] Copying source layer conv1a Type:Convolution #blobs=2
I0925 00:45:01.104595  4338 net.cpp:1094] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0925 00:45:01.104926  4338 net.cpp:1094] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0925 00:45:01.104935  4338 net.cpp:1094] Copying source layer conv1b Type:Convolution #blobs=2
I0925 00:45:01.104948  4338 net.cpp:1094] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0925 00:45:01.105154  4338 net.cpp:1094] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0925 00:45:01.105160  4338 net.cpp:1094] Copying source layer pool1 Type:Pooling #blobs=0
I0925 00:45:01.105163  4338 net.cpp:1094] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0925 00:45:01.105182  4338 net.cpp:1094] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0925 00:45:01.105386  4338 net.cpp:1094] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0925 00:45:01.105391  4338 net.cpp:1094] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0925 00:45:01.105406  4338 net.cpp:1094] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0925 00:45:01.105597  4338 net.cpp:1094] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0925 00:45:01.105614  4338 net.cpp:1094] Copying source layer pool2 Type:Pooling #blobs=0
I0925 00:45:01.105618  4338 net.cpp:1094] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0925 00:45:01.105674  4338 net.cpp:1094] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0925 00:45:01.105849  4338 net.cpp:1094] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0925 00:45:01.105854  4338 net.cpp:1094] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0925 00:45:01.105886  4338 net.cpp:1094] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0925 00:45:01.106040  4338 net.cpp:1094] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0925 00:45:01.106045  4338 net.cpp:1094] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I0925 00:45:01.106050  4338 net.cpp:1094] Copying source layer pool3 Type:Pooling #blobs=0
I0925 00:45:01.106052  4338 net.cpp:1094] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0925 00:45:01.106220  4338 net.cpp:1094] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0925 00:45:01.106384  4338 net.cpp:1094] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0925 00:45:01.106390  4338 net.cpp:1094] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0925 00:45:01.106462  4338 net.cpp:1094] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0925 00:45:01.106624  4338 net.cpp:1094] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0925 00:45:01.106643  4338 net.cpp:1094] Copying source layer pool4 Type:Pooling #blobs=0
I0925 00:45:01.106653  4338 net.cpp:1094] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0925 00:45:01.107161  4338 net.cpp:1094] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0925 00:45:01.107322  4338 net.cpp:1094] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0925 00:45:01.107328  4338 net.cpp:1094] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0925 00:45:01.107553  4338 net.cpp:1094] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0925 00:45:01.107713  4338 net.cpp:1094] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0925 00:45:01.107719  4338 net.cpp:1094] Copying source layer out5a Type:Convolution #blobs=2
I0925 00:45:01.107774  4338 net.cpp:1094] Copying source layer out5a/bn Type:BatchNorm #blobs=5
I0925 00:45:01.107980  4338 net.cpp:1094] Copying source layer out5a/relu Type:ReLU #blobs=0
I0925 00:45:01.107986  4338 net.cpp:1094] Copying source layer out5a_up2 Type:Deconvolution #blobs=1
I0925 00:45:01.107995  4338 net.cpp:1094] Copying source layer out3a Type:Convolution #blobs=2
I0925 00:45:01.108018  4338 net.cpp:1094] Copying source layer out3a/bn Type:BatchNorm #blobs=5
I0925 00:45:01.108218  4338 net.cpp:1094] Copying source layer out3a/relu Type:ReLU #blobs=0
I0925 00:45:01.108225  4338 net.cpp:1094] Copying source layer out3_out5_combined Type:Eltwise #blobs=0
I0925 00:45:01.108228  4338 net.cpp:1094] Copying source layer ctx_conv1 Type:Convolution #blobs=2
I0925 00:45:01.108253  4338 net.cpp:1094] Copying source layer ctx_conv1/bn Type:BatchNorm #blobs=5
I0925 00:45:01.108448  4338 net.cpp:1094] Copying source layer ctx_conv1/relu Type:ReLU #blobs=0
I0925 00:45:01.108454  4338 net.cpp:1094] Copying source layer ctx_conv2 Type:Convolution #blobs=2
I0925 00:45:01.108479  4338 net.cpp:1094] Copying source layer ctx_conv2/bn Type:BatchNorm #blobs=5
I0925 00:45:01.108672  4338 net.cpp:1094] Copying source layer ctx_conv2/relu Type:ReLU #blobs=0
I0925 00:45:01.108678  4338 net.cpp:1094] Copying source layer ctx_conv3 Type:Convolution #blobs=2
I0925 00:45:01.108703  4338 net.cpp:1094] Copying source layer ctx_conv3/bn Type:BatchNorm #blobs=5
I0925 00:45:01.108911  4338 net.cpp:1094] Copying source layer ctx_conv3/relu Type:ReLU #blobs=0
I0925 00:45:01.108918  4338 net.cpp:1094] Copying source layer ctx_conv4 Type:Convolution #blobs=2
I0925 00:45:01.108947  4338 net.cpp:1094] Copying source layer ctx_conv4/bn Type:BatchNorm #blobs=5
I0925 00:45:01.109146  4338 net.cpp:1094] Copying source layer ctx_conv4/relu Type:ReLU #blobs=0
I0925 00:45:01.109153  4338 net.cpp:1094] Copying source layer ctx_final Type:Convolution #blobs=2
I0925 00:45:01.109166  4338 net.cpp:1094] Copying source layer ctx_final/relu Type:ReLU #blobs=0
I0925 00:45:01.109170  4338 net.cpp:1094] Copying source layer out_deconv_final_up2 Type:Deconvolution #blobs=1
I0925 00:45:01.109179  4338 net.cpp:1094] Copying source layer out_deconv_final_up4 Type:Deconvolution #blobs=1
I0925 00:45:01.109186  4338 net.cpp:1094] Copying source layer out_deconv_final_up8 Type:Deconvolution #blobs=1
I0925 00:45:01.109194  4338 net.cpp:1094] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0925 00:45:01.109313  4338 parallel.cpp:106] [0 - 0] P2pSync adding callback
I0925 00:45:01.109319  4338 parallel.cpp:106] [1 - 1] P2pSync adding callback
I0925 00:45:01.109323  4338 parallel.cpp:106] [2 - 2] P2pSync adding callback
I0925 00:45:01.109328  4338 parallel.cpp:59] Starting Optimization
I0925 00:45:01.109331  4338 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0925 00:45:01.109364  4338 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0925 00:45:01.109400  4338 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0925 00:45:01.110237  4458 device_alternate.hpp:116] NVML initialized on thread 136440855172864
I0925 00:45:01.138299  4458 common.cpp:585] NVML succeeded to set CPU affinity on device 0
I0925 00:45:01.138432  4459 device_alternate.hpp:116] NVML initialized on thread 136440846780160
I0925 00:45:01.139201  4459 common.cpp:585] NVML succeeded to set CPU affinity on device 1
I0925 00:45:01.140275  4460 device_alternate.hpp:116] NVML initialized on thread 136440838387456
I0925 00:45:01.140736  4460 common.cpp:585] NVML succeeded to set CPU affinity on device 2
I0925 00:45:01.144992  4459 solver.cpp:43] Solver data type: FLOAT
W0925 00:45:01.145861  4459 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0925 00:45:01.146059  4459 net.cpp:104] Using FLOAT as default forward math type
I0925 00:45:01.146078  4459 net.cpp:110] Using FLOAT as default backward math type
I0925 00:45:01.146129  4459 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 6
I0925 00:45:01.146148  4459 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0925 00:45:01.150733  4460 solver.cpp:43] Solver data type: FLOAT
W0925 00:45:01.151348  4460 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0925 00:45:01.151507  4460 net.cpp:104] Using FLOAT as default forward math type
I0925 00:45:01.151513  4460 net.cpp:110] Using FLOAT as default backward math type
I0925 00:45:01.151526  4461 db_lmdb.cpp:24] Opened lmdb data/train-image-lmdb
I0925 00:45:01.151553  4460 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 6
I0925 00:45:01.151562  4460 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0925 00:45:01.156306  4462 db_lmdb.cpp:24] Opened lmdb data/train-image-lmdb
I0925 00:45:01.168536  4460 data_layer.cpp:187] [2] ReshapePrefetch 6, 3, 640, 640
I0925 00:45:01.168622  4460 data_layer.cpp:211] [2] Output data size: 6, 3, 640, 640
I0925 00:45:01.168627  4460 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0925 00:45:01.169009  4459 data_layer.cpp:187] [1] ReshapePrefetch 6, 3, 640, 640
I0925 00:45:01.169142  4459 data_layer.cpp:211] [1] Output data size: 6, 3, 640, 640
I0925 00:45:01.169234  4459 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0925 00:45:01.175703  4460 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 6
I0925 00:45:01.175735  4460 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0925 00:45:01.175861  4459 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 6
I0925 00:45:01.175873  4459 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0925 00:45:01.180536  4463 data_layer.cpp:101] [2] Parser threads: 1
I0925 00:45:01.180558  4463 data_layer.cpp:103] [2] Transformer threads: 1
I0925 00:45:01.184275  4465 data_layer.cpp:101] [1] Parser threads: 1
I0925 00:45:01.184718  4465 data_layer.cpp:103] [1] Transformer threads: 1
I0925 00:45:01.198045  4466 db_lmdb.cpp:24] Opened lmdb data/train-label-lmdb
I0925 00:45:01.205817  4464 db_lmdb.cpp:24] Opened lmdb data/train-label-lmdb
I0925 00:45:01.207442  4459 data_layer.cpp:187] [1] ReshapePrefetch 6, 1, 640, 640
I0925 00:45:01.207581  4459 data_layer.cpp:211] [1] Output data size: 6, 1, 640, 640
I0925 00:45:01.207592  4459 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0925 00:45:01.222724  4467 data_layer.cpp:101] [1] Parser threads: 1
I0925 00:45:01.222762  4467 data_layer.cpp:103] [1] Transformer threads: 1
I0925 00:45:01.229531  4460 data_layer.cpp:187] [2] ReshapePrefetch 6, 1, 640, 640
I0925 00:45:01.229635  4460 data_layer.cpp:211] [2] Output data size: 6, 1, 640, 640
I0925 00:45:01.229643  4460 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0925 00:45:01.238965  4468 data_layer.cpp:101] [2] Parser threads: 1
I0925 00:45:01.242208  4468 data_layer.cpp:103] [2] Transformer threads: 1
I0925 00:45:02.764986  4460 solver.cpp:177] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/test.prototxt
W0925 00:45:02.765100  4460 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0925 00:45:02.765280  4460 net.cpp:104] Using FLOAT as default forward math type
I0925 00:45:02.765288  4460 net.cpp:110] Using FLOAT as default backward math type
I0925 00:45:02.765318  4460 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 2
I0925 00:45:02.765326  4460 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0925 00:45:02.768126  4518 db_lmdb.cpp:24] Opened lmdb data/val-image-lmdb
I0925 00:45:02.774104  4460 data_layer.cpp:187] (2) ReshapePrefetch 2, 3, 640, 640
I0925 00:45:02.774188  4460 data_layer.cpp:211] (2) Output data size: 2, 3, 640, 640
I0925 00:45:02.774199  4460 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0925 00:45:02.776623  4460 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 2
I0925 00:45:02.776679  4460 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0925 00:45:02.793210  4519 data_layer.cpp:101] (2) Parser threads: 1
I0925 00:45:02.793236  4519 data_layer.cpp:103] (2) Transformer threads: 1
I0925 00:45:02.833375  4520 db_lmdb.cpp:24] Opened lmdb data/val-label-lmdb
I0925 00:45:02.836585  4459 solver.cpp:177] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/test.prototxt
W0925 00:45:02.836755  4459 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0925 00:45:02.836971  4459 net.cpp:104] Using FLOAT as default forward math type
I0925 00:45:02.836987  4459 net.cpp:110] Using FLOAT as default backward math type
I0925 00:45:02.837056  4459 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 2
I0925 00:45:02.837074  4459 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0925 00:45:02.837352  4460 data_layer.cpp:187] (2) ReshapePrefetch 2, 1, 640, 640
I0925 00:45:02.837545  4460 data_layer.cpp:211] (2) Output data size: 2, 1, 640, 640
I0925 00:45:02.837555  4460 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0925 00:45:02.901536  4522 data_layer.cpp:101] (2) Parser threads: 1
I0925 00:45:02.901563  4522 data_layer.cpp:103] (2) Transformer threads: 1
I0925 00:45:02.903307  4521 db_lmdb.cpp:24] Opened lmdb data/val-image-lmdb
I0925 00:45:02.946079  4459 data_layer.cpp:187] (1) ReshapePrefetch 2, 3, 640, 640
I0925 00:45:02.948245  4459 data_layer.cpp:211] (1) Output data size: 2, 3, 640, 640
I0925 00:45:02.948261  4459 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0925 00:45:02.971082  4459 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 2
I0925 00:45:02.971148  4459 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0925 00:45:03.053230  4523 data_layer.cpp:101] (1) Parser threads: 1
I0925 00:45:03.053256  4523 data_layer.cpp:103] (1) Transformer threads: 1
I0925 00:45:03.054142  4524 db_lmdb.cpp:24] Opened lmdb data/val-label-lmdb
I0925 00:45:03.075371  4459 data_layer.cpp:187] (1) ReshapePrefetch 2, 1, 640, 640
I0925 00:45:03.075875  4459 data_layer.cpp:211] (1) Output data size: 2, 1, 640, 640
I0925 00:45:03.076005  4459 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0925 00:45:03.094511  4525 data_layer.cpp:101] (1) Parser threads: 1
I0925 00:45:03.094542  4525 data_layer.cpp:103] (1) Transformer threads: 1
I0925 00:45:03.203882  4460 solver.cpp:57] Solver scaffolding done.
I0925 00:45:03.220307  4459 solver.cpp:57] Solver scaffolding done.
I0925 00:45:03.257958  4459 parallel.cpp:161] [1 - 1] P2pSync adding callback
I0925 00:45:03.257958  4458 parallel.cpp:161] [0 - 0] P2pSync adding callback
I0925 00:45:03.257958  4460 parallel.cpp:161] [2 - 2] P2pSync adding callback
I0925 00:45:03.686709  4458 net.cpp:2253] All zero weights of convolution layers are frozen
I0925 00:45:03.713095  4458 solver.cpp:489] Solving jsegnet21v2_train
I0925 00:45:03.713171  4458 solver.cpp:490] Learning Rate Policy: multistep
I0925 00:45:03.716897  4459 solver.cpp:489] Solving jsegnet21v2_train
I0925 00:45:03.716956  4459 solver.cpp:490] Learning Rate Policy: multistep
I0925 00:45:03.722023  4460 solver.cpp:489] Solving jsegnet21v2_train
I0925 00:45:03.722065  4460 solver.cpp:490] Learning Rate Policy: multistep
I0925 00:45:03.739539  4458 net.cpp:1412] [0] Reserving 10800128 bytes of shared learnable space
I0925 00:45:03.740923  4460 net.cpp:1412] [2] Reserving 10800128 bytes of shared learnable space
I0925 00:45:03.741741  4459 net.cpp:1412] [1] Reserving 10800128 bytes of shared learnable space
I0925 00:45:03.752238  4459 solver.cpp:228] Starting Optimization on GPU 1
I0925 00:45:03.753178  4538 device_alternate.hpp:116] NVML initialized on thread 128440686913280
I0925 00:45:03.753207  4538 common.cpp:585] NVML succeeded to set CPU affinity on device 1
I0925 00:45:03.753407  4458 solver.cpp:228] Starting Optimization on GPU 0
I0925 00:45:03.753456  4458 solver.cpp:562] Iteration 0, Testing net (#0)
I0925 00:45:03.753473  4540 device_alternate.hpp:116] NVML initialized on thread 128440678520576
I0925 00:45:03.753427  4460 solver.cpp:228] Starting Optimization on GPU 2
I0925 00:45:03.753509  4540 common.cpp:585] NVML succeeded to set CPU affinity on device 0
I0925 00:45:03.753602  4541 device_alternate.hpp:116] NVML initialized on thread 128440670127872
I0925 00:45:03.753618  4541 common.cpp:585] NVML succeeded to set CPU affinity on device 2
I0925 00:45:04.099838  4458 solver.cpp:654]     Test net output #0: accuracy/top1 = 0.873999
I0925 00:45:04.099891  4458 solver.cpp:654]     Test net output #1: accuracy/top5 = 1
I0925 00:45:04.099907  4458 solver.cpp:654]     Test net output #2: loss = 0.499414 (* 1 = 0.499414 loss)
I0925 00:45:04.099918  4458 solver.cpp:255] [MultiGPU] Initial Test completed
I0925 00:45:04.963732  4458 solver.cpp:319] Iteration 0 (0.863724 s), loss = 0.0710956
I0925 00:45:04.963765  4458 solver.cpp:336]     Train net output #0: loss = 0.0710956 (* 1 = 0.0710956 loss)
I0925 00:45:04.963773  4458 sgd_solver.cpp:136] Iteration 0, lr = 0.01, m = 0.9
I0925 00:45:05.400043  4458 solver.cpp:319] Iteration 1 (0.436289 s), loss = 0.0594584
I0925 00:45:05.400079  4458 solver.cpp:336]     Train net output #0: loss = 0.0594584 (* 1 = 0.0594584 loss)
I0925 00:45:05.573406  4458 cudnn_conv_layer.cpp:872] [0] Conv Algos (F,BD,BF): 'conv1a' with space 1.66G 3/1 1 0 3 	(avail 0.02G, req 0G)	t: 0 3.07 2.53
I0925 00:45:05.580204  4460 cudnn_conv_layer.cpp:872] [2] Conv Algos (F,BD,BF): 'conv1a' with space 1.81G 3/1 1 0 3 	(avail 0.01G, req 0G)	t: 0 3.48 2.84
I0925 00:45:05.593331  4459 cudnn_conv_layer.cpp:872] [1] Conv Algos (F,BD,BF): 'conv1a' with space 1.81G 3/1 1 0 3 	(avail 0.01G, req 0G)	t: 0 3.43 2.96
I0925 00:45:05.700984  4458 cudnn_conv_layer.cpp:872] [0] Conv Algos (F,BD,BF): 'conv1b' with space 1.66G 32/4 6 4 3 	(avail 0.02G, req 0G)	t: 0 0.6 1.27
I0925 00:45:05.736313  4460 cudnn_conv_layer.cpp:872] [2] Conv Algos (F,BD,BF): 'conv1b' with space 1.81G 32/4 6 4 3 	(avail 0.01G, req 0G)	t: 0 0.7 1.34
I0925 00:45:05.740543  4459 cudnn_conv_layer.cpp:872] [1] Conv Algos (F,BD,BF): 'conv1b' with space 1.81G 32/4 6 4 3 	(avail 0.01G, req 0G)	t: 0 0.71 1.35
I0925 00:45:05.999558  4460 cudnn_conv_layer.cpp:872] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.81G 32/1 6 4 3 	(avail 0.01G, req 0G)	t: 0 0.73 1.62
I0925 00:45:06.006547  4459 cudnn_conv_layer.cpp:872] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.81G 32/1 6 4 3 	(avail 0.01G, req 0G)	t: 0 0.73 1.61
I0925 00:45:06.025022  4458 cudnn_conv_layer.cpp:872] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.66G 32/1 6 4 3 	(avail 0.02G, req 0G)	t: 0 0.74 1.48
I0925 00:45:06.098621  4460 cudnn_conv_layer.cpp:872] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.81G 64/4 6 4 3 	(avail 0.01G, req 0G)	t: 0 0.27 0.69
I0925 00:45:06.102512  4458 cudnn_conv_layer.cpp:872] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.66G 64/4 6 4 3 	(avail 0.02G, req 0G)	t: 0 0.27 0.63
I0925 00:45:06.108633  4459 cudnn_conv_layer.cpp:872] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.81G 64/4 6 4 3 	(avail 0.01G, req 0G)	t: 0 0.27 0.71
I0925 00:45:06.267709  4458 cudnn_conv_layer.cpp:872] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.66G 64/1 6 4 5 	(avail 0.02G, req 0.07G)	t: 0 0.45 0.91
I0925 00:45:06.282946  4460 cudnn_conv_layer.cpp:872] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.81G 64/1 6 4 5 	(avail 0.01G, req 0.07G)	t: 0 0.48 0.96
I0925 00:45:06.294493  4459 cudnn_conv_layer.cpp:872] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.81G 64/1 6 4 5 	(avail 0.01G, req 0.07G)	t: 0 0.49 0.97
I0925 00:45:06.332573  4458 cudnn_conv_layer.cpp:872] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.66G 128/4 6 4 3 	(avail 0.02G, req 0.07G)	t: 0 0.13 0.28
I0925 00:45:06.341177  4460 cudnn_conv_layer.cpp:872] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.81G 128/4 6 4 3 	(avail 0.01G, req 0.07G)	t: 0 0.13 0.3
I0925 00:45:06.348074  4459 cudnn_conv_layer.cpp:872] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.81G 128/4 6 4 3 	(avail 0.01G, req 0.07G)	t: 0 0.14 0.3
I0925 00:45:06.475428  4458 cudnn_conv_layer.cpp:872] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.66G 128/1 6 4 5 	(avail 0.02G, req 0.07G)	t: 0 0.45 0.54
I0925 00:45:06.495983  4460 cudnn_conv_layer.cpp:872] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.81G 128/1 6 4 5 	(avail 0.01G, req 0.07G)	t: 0 0.51 0.57
I0925 00:45:06.505255  4459 cudnn_conv_layer.cpp:872] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.81G 128/1 6 4 5 	(avail 0.01G, req 0.07G)	t: 0 0.52 0.57
I0925 00:45:06.531258  4460 cudnn_conv_layer.cpp:872] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.81G 256/4 6 4 3 	(avail 0.01G, req 0.07G)	t: 0 0.1 0.2
I0925 00:45:06.534732  4458 cudnn_conv_layer.cpp:872] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.66G 256/4 6 4 3 	(avail 0.02G, req 0.07G)	t: 0 0.1 0.19
I0925 00:45:06.540614  4459 cudnn_conv_layer.cpp:872] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.81G 256/4 6 4 3 	(avail 0.01G, req 0.07G)	t: 0 0.1 0.2
I0925 00:45:06.627842  4458 cudnn_conv_layer.cpp:872] [0] Conv Algos (F,BD,BF): 'out3a' with space 1.66G 128/2 6 4 3 	(avail 0.02G, req 0.07G)	t: 0 0.22 0.4
I0925 00:45:06.639163  4460 cudnn_conv_layer.cpp:872] [2] Conv Algos (F,BD,BF): 'out3a' with space 1.81G 128/2 6 4 3 	(avail 0.01G, req 0.07G)	t: 0 0.23 0.43
I0925 00:45:06.648810  4459 cudnn_conv_layer.cpp:872] [1] Conv Algos (F,BD,BF): 'out3a' with space 1.81G 128/2 6 4 3 	(avail 0.01G, req 0.07G)	t: 0 0.23 0.43
I0925 00:45:06.736358  4458 cudnn_conv_layer.cpp:872] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 1.66G 64/1 6 4 3 	(avail 0.02G, req 0.07G)	t: 0 0.33 0.64
I0925 00:45:06.753077  4460 cudnn_conv_layer.cpp:872] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 1.81G 64/1 6 4 3 	(avail 0.01G, req 0.07G)	t: 0 0.3 0.65
I0925 00:45:06.757979  4459 cudnn_conv_layer.cpp:872] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 1.81G 64/1 6 4 3 	(avail 0.01G, req 0.07G)	t: 0 0.3 0.66
I0925 00:45:06.794245  4458 cudnn_conv_layer.cpp:872] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 1.66G 64/1 6 1 5 	(avail 0.02G, req 0.07G)	t: 0 0.1 0.35
I0925 00:45:06.835950  4460 cudnn_conv_layer.cpp:872] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 1.81G 64/1 6 1 5 	(avail 0.01G, req 0.07G)	t: 0 0.11 0.35
I0925 00:45:06.836313  4459 cudnn_conv_layer.cpp:872] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 1.81G 64/1 6 1 5 	(avail 0.01G, req 0.07G)	t: 0 0.11 0.38
I0925 00:45:07.091019  4458 solver.cpp:319] Iteration 2 (1.69092 s), loss = 0.052284
I0925 00:45:07.091055  4458 solver.cpp:336]     Train net output #0: loss = 0.052284 (* 1 = 0.052284 loss)
I0925 00:45:07.092192  4458 cudnn_conv_layer.cpp:474] [0] Layer 'conv1a' reallocating workspace 1.66G to 0.14G
I0925 00:45:07.092515  4459 cudnn_conv_layer.cpp:474] [1] Layer 'conv1a' reallocating workspace 1.81G to 0.14G
I0925 00:45:07.112203  4460 cudnn_conv_layer.cpp:474] [2] Layer 'conv1a' reallocating workspace 1.81G to 0.14G
I0925 00:45:08.728646  4460 blocking_queue.cpp:40] Data layer prefetch queue empty
I0925 00:45:52.682804  4458 solver.cpp:314] Iteration 100 (2.14957 iter/s, 45.5905s/98 iter), loss = 0.0633931
I0925 00:45:52.682867  4458 solver.cpp:336]     Train net output #0: loss = 0.0633931 (* 1 = 0.0633931 loss)
I0925 00:45:52.682873  4458 sgd_solver.cpp:136] Iteration 100, lr = 0.01, m = 0.9
I0925 00:46:17.581313  4464 data_reader.cpp:305] Starting prefetch of epoch 1
I0925 00:46:32.225975  4458 solver.cpp:314] Iteration 200 (2.52896 iter/s, 39.542s/100 iter), loss = 0.125474
I0925 00:46:32.226047  4458 solver.cpp:336]     Train net output #0: loss = 0.125474 (* 1 = 0.125474 loss)
I0925 00:46:32.226054  4458 sgd_solver.cpp:136] Iteration 200, lr = 0.01, m = 0.9
I0925 00:47:25.700606  4458 solver.cpp:314] Iteration 300 (1.87011 iter/s, 53.4727s/100 iter), loss = 0.111538
I0925 00:47:25.700953  4458 solver.cpp:336]     Train net output #0: loss = 0.111538 (* 1 = 0.111538 loss)
I0925 00:47:25.700959  4458 sgd_solver.cpp:136] Iteration 300, lr = 0.01, m = 0.9
I0925 00:47:39.865466  4461 data_reader.cpp:305] Starting prefetch of epoch 1
I0925 00:48:18.877774  4458 solver.cpp:314] Iteration 400 (1.88056 iter/s, 53.1757s/100 iter), loss = 0.0950648
I0925 00:48:18.877835  4458 solver.cpp:336]     Train net output #0: loss = 0.0950649 (* 1 = 0.0950649 loss)
I0925 00:48:18.877842  4458 sgd_solver.cpp:136] Iteration 400, lr = 0.01, m = 0.9
I0925 00:49:11.412276  4458 solver.cpp:314] Iteration 500 (1.90356 iter/s, 52.533s/100 iter), loss = 0.0947419
I0925 00:49:11.412331  4458 solver.cpp:336]     Train net output #0: loss = 0.0947419 (* 1 = 0.0947419 loss)
I0925 00:49:11.412338  4458 sgd_solver.cpp:136] Iteration 500, lr = 0.01, m = 0.9
I0925 00:50:05.519070  4458 solver.cpp:314] Iteration 600 (1.84825 iter/s, 54.1052s/100 iter), loss = 0.0804604
I0925 00:50:05.524240  4458 solver.cpp:336]     Train net output #0: loss = 0.0804604 (* 1 = 0.0804604 loss)
I0925 00:50:05.524260  4458 sgd_solver.cpp:136] Iteration 600, lr = 0.01, m = 0.9
I0925 00:50:35.970582  4461 data_reader.cpp:305] Starting prefetch of epoch 2
I0925 00:50:58.464864  4458 solver.cpp:314] Iteration 700 (1.88878 iter/s, 52.9442s/100 iter), loss = 0.067503
I0925 00:50:58.464896  4458 solver.cpp:336]     Train net output #0: loss = 0.0675029 (* 1 = 0.0675029 loss)
I0925 00:50:58.464903  4458 sgd_solver.cpp:136] Iteration 700, lr = 0.01, m = 0.9
I0925 00:51:50.468709  4458 solver.cpp:314] Iteration 800 (1.92299 iter/s, 52.0024s/100 iter), loss = 0.148108
I0925 00:51:50.468853  4458 solver.cpp:336]     Train net output #0: loss = 0.148108 (* 1 = 0.148108 loss)
I0925 00:51:50.468873  4458 sgd_solver.cpp:136] Iteration 800, lr = 0.01, m = 0.9
I0925 00:52:41.852066  4458 solver.cpp:314] Iteration 900 (1.94621 iter/s, 51.3819s/100 iter), loss = 0.0629501
I0925 00:52:41.852221  4458 solver.cpp:336]     Train net output #0: loss = 0.0629501 (* 1 = 0.0629501 loss)
I0925 00:52:41.852241  4458 sgd_solver.cpp:136] Iteration 900, lr = 0.01, m = 0.9
I0925 00:53:28.737354  4410 data_reader.cpp:305] Starting prefetch of epoch 1
I0925 00:53:34.837486  4458 solver.cpp:423] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.6 sparsity_achieved=0 iter=1000
I0925 00:55:58.671037  4458 net.cpp:2253] All zero weights of convolution layers are frozen
I0925 00:55:58.675871  4458 solver.cpp:368] Sparsity after update:
I0925 00:55:58.678172  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 00:55:58.678184  4458 net.cpp:2312] conv1a_param_0(0.202) 
I0925 00:55:58.678190  4458 net.cpp:2312] conv1b_param_0(0.484) 
I0925 00:55:58.678194  4458 net.cpp:2312] ctx_conv1_param_0(0.599) 
I0925 00:55:58.678197  4458 net.cpp:2312] ctx_conv2_param_0(0.599) 
I0925 00:55:58.678201  4458 net.cpp:2312] ctx_conv3_param_0(0.599) 
I0925 00:55:58.678205  4458 net.cpp:2312] ctx_conv4_param_0(0.599) 
I0925 00:55:58.678207  4458 net.cpp:2312] ctx_final_param_0(0.187) 
I0925 00:55:58.678213  4458 net.cpp:2312] out3a_param_0(0.599) 
I0925 00:55:58.678215  4458 net.cpp:2312] out5a_param_0(0.6) 
I0925 00:55:58.678220  4458 net.cpp:2312] res2a_branch2a_param_0(0.573) 
I0925 00:55:58.678222  4458 net.cpp:2312] res2a_branch2b_param_0(0.418) 
I0925 00:55:58.678225  4458 net.cpp:2312] res3a_branch2a_param_0(0.582) 
I0925 00:55:58.678231  4458 net.cpp:2312] res3a_branch2b_param_0(0.521) 
I0925 00:55:58.678232  4458 net.cpp:2312] res4a_branch2a_param_0(0.595) 
I0925 00:55:58.678236  4458 net.cpp:2312] res4a_branch2b_param_0(0.574) 
I0925 00:55:58.678239  4458 net.cpp:2312] res5a_branch2a_param_0(0.599) 
I0925 00:55:58.678242  4458 net.cpp:2312] res5a_branch2b_param_0(0.6) 
I0925 00:55:58.678246  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (1.59794e+06/2.69117e+06) 0.594
I0925 00:55:58.889994  4458 solver.cpp:314] Iteration 1000 (0.507531 iter/s, 197.032s/100 iter), loss = 0.0872838
I0925 00:55:58.890022  4458 solver.cpp:336]     Train net output #0: loss = 0.0872839 (* 1 = 0.0872839 loss)
I0925 00:55:58.890028  4458 sgd_solver.cpp:136] Iteration 1000, lr = 0.01, m = 0.9
I0925 00:56:58.275652  4458 solver.cpp:314] Iteration 1100 (1.68396 iter/s, 59.384s/100 iter), loss = 0.0651384
I0925 00:56:58.275708  4458 solver.cpp:336]     Train net output #0: loss = 0.0651384 (* 1 = 0.0651384 loss)
I0925 00:56:58.275713  4458 sgd_solver.cpp:136] Iteration 1100, lr = 0.01, m = 0.9
I0925 00:57:54.663703  4458 solver.cpp:314] Iteration 1200 (1.77348 iter/s, 56.3865s/100 iter), loss = 0.069872
I0925 00:57:54.663784  4458 solver.cpp:336]     Train net output #0: loss = 0.069872 (* 1 = 0.069872 loss)
I0925 00:57:54.663792  4458 sgd_solver.cpp:136] Iteration 1200, lr = 0.01, m = 0.9
I0925 00:58:47.041468  4458 solver.cpp:314] Iteration 1300 (1.90926 iter/s, 52.3763s/100 iter), loss = 0.0822024
I0925 00:58:47.041530  4458 solver.cpp:336]     Train net output #0: loss = 0.0822024 (* 1 = 0.0822024 loss)
I0925 00:58:47.041538  4458 sgd_solver.cpp:136] Iteration 1300, lr = 0.01, m = 0.9
I0925 00:58:57.102231  4464 data_reader.cpp:305] Starting prefetch of epoch 2
I0925 00:59:40.252830  4458 solver.cpp:314] Iteration 1400 (1.87935 iter/s, 53.2099s/100 iter), loss = 0.102638
I0925 00:59:40.252902  4458 solver.cpp:336]     Train net output #0: loss = 0.102638 (* 1 = 0.102638 loss)
I0925 00:59:40.252908  4458 sgd_solver.cpp:136] Iteration 1400, lr = 0.01, m = 0.9
I0925 01:00:25.992241  4461 data_reader.cpp:305] Starting prefetch of epoch 3
I0925 01:00:33.781298  4458 solver.cpp:314] Iteration 1500 (1.86822 iter/s, 53.527s/100 iter), loss = 0.0931016
I0925 01:00:33.781378  4458 solver.cpp:336]     Train net output #0: loss = 0.0931017 (* 1 = 0.0931017 loss)
I0925 01:00:33.781388  4458 sgd_solver.cpp:136] Iteration 1500, lr = 0.01, m = 0.9
I0925 01:01:27.127400  4458 solver.cpp:314] Iteration 1600 (1.8746 iter/s, 53.3446s/100 iter), loss = 0.0645453
I0925 01:01:27.127482  4458 solver.cpp:336]     Train net output #0: loss = 0.0645453 (* 1 = 0.0645453 loss)
I0925 01:01:27.127490  4458 sgd_solver.cpp:136] Iteration 1600, lr = 0.01, m = 0.9
I0925 01:02:21.044221  4458 solver.cpp:314] Iteration 1700 (1.85476 iter/s, 53.9153s/100 iter), loss = 0.0943129
I0925 01:02:21.046566  4458 solver.cpp:336]     Train net output #0: loss = 0.094313 (* 1 = 0.094313 loss)
I0925 01:02:21.046589  4458 sgd_solver.cpp:136] Iteration 1700, lr = 0.01, m = 0.9
I0925 01:03:03.014638  4458 solver.cpp:314] Iteration 1800 (2.3827 iter/s, 41.9692s/100 iter), loss = 0.0647923
I0925 01:03:03.014753  4458 solver.cpp:336]     Train net output #0: loss = 0.0647923 (* 1 = 0.0647923 loss)
I0925 01:03:03.014760  4458 sgd_solver.cpp:136] Iteration 1800, lr = 0.01, m = 0.9
I0925 01:03:09.239809  4464 data_reader.cpp:305] Starting prefetch of epoch 3
I0925 01:03:54.755127  4458 solver.cpp:314] Iteration 1900 (1.93278 iter/s, 51.739s/100 iter), loss = 0.049361
I0925 01:03:54.755543  4458 solver.cpp:336]     Train net output #0: loss = 0.0493611 (* 1 = 0.0493611 loss)
I0925 01:03:54.755555  4458 sgd_solver.cpp:136] Iteration 1900, lr = 0.01, m = 0.9
I0925 01:04:49.194716  4458 solver.cpp:423] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.61 sparsity_achieved=0.593772 iter=2000
I0925 01:04:49.598151  4459 cudnn_conv_layer.cpp:872] (1) Conv Algo (F): 'conv1a' with space 0.14G 3/1 1 	(avail 1.69G, req 0G)	t: 0
I0925 01:04:49.614619  4460 cudnn_conv_layer.cpp:872] (2) Conv Algo (F): 'conv1a' with space 0.14G 3/1 1 	(avail 1.69G, req 0G)	t: 0
I0925 01:04:49.663100  4459 cudnn_conv_layer.cpp:872] (1) Conv Algo (F): 'conv1b' with space 0.14G 32/4 6 	(avail 1.69G, req 0G)	t: 0
I0925 01:04:49.691702  4459 cudnn_conv_layer.cpp:872] (1) Conv Algo (F): 'res2a_branch2a' with space 0.14G 32/1 6 	(avail 1.69G, req 0G)	t: 0
I0925 01:04:49.707350  4460 cudnn_conv_layer.cpp:872] (2) Conv Algo (F): 'conv1b' with space 0.14G 32/4 6 	(avail 1.69G, req 0G)	t: 0
I0925 01:04:49.718713  4459 cudnn_conv_layer.cpp:872] (1) Conv Algo (F): 'res2a_branch2b' with space 0.14G 64/4 6 	(avail 1.69G, req 0G)	t: 0
I0925 01:04:49.749346  4460 cudnn_conv_layer.cpp:872] (2) Conv Algo (F): 'res2a_branch2a' with space 0.14G 32/1 6 	(avail 1.69G, req 0G)	t: 0
I0925 01:04:49.759173  4459 cudnn_conv_layer.cpp:872] (1) Conv Algo (F): 'res3a_branch2a' with space 0.14G 64/1 6 	(avail 1.69G, req 0G)	t: 0
I0925 01:04:49.767602  4460 cudnn_conv_layer.cpp:872] (2) Conv Algo (F): 'res2a_branch2b' with space 0.14G 64/4 6 	(avail 1.69G, req 0G)	t: 0
I0925 01:04:49.783874  4460 cudnn_conv_layer.cpp:872] (2) Conv Algo (F): 'res3a_branch2a' with space 0.14G 64/1 6 	(avail 1.69G, req 0G)	t: 0
I0925 01:04:49.783970  4459 cudnn_conv_layer.cpp:872] (1) Conv Algo (F): 'res3a_branch2b' with space 0.14G 128/4 6 	(avail 1.69G, req 0G)	t: 0
I0925 01:04:49.792587  4460 cudnn_conv_layer.cpp:872] (2) Conv Algo (F): 'res3a_branch2b' with space 0.14G 128/4 6 	(avail 1.69G, req 0G)	t: 0
I0925 01:04:49.794790  4459 cudnn_conv_layer.cpp:872] (1) Conv Algo (F): 'res4a_branch2a' with space 0.14G 128/1 6 	(avail 1.69G, req 0G)	t: 0
I0925 01:04:49.799386  4459 cudnn_conv_layer.cpp:872] (1) Conv Algo (F): 'res4a_branch2b' with space 0.14G 256/4 6 	(avail 1.69G, req 0G)	t: 0
I0925 01:04:49.803344  4460 cudnn_conv_layer.cpp:872] (2) Conv Algo (F): 'res4a_branch2a' with space 0.14G 128/1 6 	(avail 1.69G, req 0G)	t: 0
I0925 01:04:49.865504  4459 cudnn_conv_layer.cpp:872] (1) Conv Algo (F): 'out3a' with space 0.14G 128/2 6 	(avail 1.69G, req 0G)	t: 0
I0925 01:04:49.872917  4460 cudnn_conv_layer.cpp:872] (2) Conv Algo (F): 'res4a_branch2b' with space 0.14G 256/4 6 	(avail 1.69G, req 0G)	t: 0
I0925 01:04:49.941351  4459 cudnn_conv_layer.cpp:872] (1) Conv Algo (F): 'ctx_conv1' with space 0.14G 64/1 6 	(avail 1.69G, req 0G)	t: 0
I0925 01:04:49.963706  4460 cudnn_conv_layer.cpp:872] (2) Conv Algo (F): 'out3a' with space 0.14G 128/2 6 	(avail 1.69G, req 0G)	t: 0
I0925 01:04:49.980216  4460 cudnn_conv_layer.cpp:872] (2) Conv Algo (F): 'ctx_conv1' with space 0.14G 64/1 6 	(avail 1.69G, req 0G)	t: 0
I0925 01:04:49.994910  4459 cudnn_conv_layer.cpp:872] (1) Conv Algo (F): 'ctx_final' with space 0.14G 64/1 6 	(avail 1.69G, req 0G)	t: 0
I0925 01:04:49.996709  4460 cudnn_conv_layer.cpp:872] (2) Conv Algo (F): 'ctx_final' with space 0.14G 64/1 6 	(avail 1.69G, req 0G)	t: 0
I0925 01:05:08.063262  4518 data_reader.cpp:305] Starting prefetch of epoch 1
I0925 01:06:55.312378  4458 net.cpp:2253] All zero weights of convolution layers are frozen
I0925 01:06:55.317216  4458 solver.cpp:368] Sparsity after update:
I0925 01:06:55.319010  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 01:06:55.319020  4458 net.cpp:2312] conv1a_param_0(0.197) 
I0925 01:06:55.319026  4458 net.cpp:2312] conv1b_param_0(0.5) 
I0925 01:06:55.319030  4458 net.cpp:2312] ctx_conv1_param_0(0.609) 
I0925 01:06:55.319031  4458 net.cpp:2312] ctx_conv2_param_0(0.609) 
I0925 01:06:55.319033  4458 net.cpp:2312] ctx_conv3_param_0(0.609) 
I0925 01:06:55.319034  4458 net.cpp:2312] ctx_conv4_param_0(0.609) 
I0925 01:06:55.319036  4458 net.cpp:2312] ctx_final_param_0(0.19) 
I0925 01:06:55.319038  4458 net.cpp:2312] out3a_param_0(0.609) 
I0925 01:06:55.319041  4458 net.cpp:2312] out5a_param_0(0.61) 
I0925 01:06:55.319042  4458 net.cpp:2312] res2a_branch2a_param_0(0.588) 
I0925 01:06:55.319044  4458 net.cpp:2312] res2a_branch2b_param_0(0.446) 
I0925 01:06:55.319046  4458 net.cpp:2312] res3a_branch2a_param_0(0.599) 
I0925 01:06:55.319048  4458 net.cpp:2312] res3a_branch2b_param_0(0.549) 
I0925 01:06:55.319051  4458 net.cpp:2312] res4a_branch2a_param_0(0.608) 
I0925 01:06:55.319052  4458 net.cpp:2312] res4a_branch2b_param_0(0.595) 
I0925 01:06:55.319054  4458 net.cpp:2312] res5a_branch2a_param_0(0.608) 
I0925 01:06:55.319056  4458 net.cpp:2312] res5a_branch2b_param_0(0.609) 
I0925 01:06:55.319058  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (1.62842e+06/2.69117e+06) 0.605
I0925 01:06:55.319068  4458 solver.cpp:562] Iteration 2000, Testing net (#0)
I0925 01:06:55.402359  4458 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'conv1a' with space 0.14G 3/1 1 	(avail 1.53G, req 0G)	t: 0
I0925 01:06:55.431066  4458 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'conv1b' with space 0.14G 32/4 6 	(avail 1.53G, req 0G)	t: 0
I0925 01:06:55.447516  4458 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res2a_branch2a' with space 0.14G 32/1 6 	(avail 1.53G, req 0G)	t: 0
I0925 01:06:55.459425  4458 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res2a_branch2b' with space 0.14G 64/4 6 	(avail 1.53G, req 0G)	t: 0
I0925 01:06:55.470657  4458 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res3a_branch2a' with space 0.14G 64/1 6 	(avail 1.53G, req 0G)	t: 0
I0925 01:06:55.477782  4458 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res3a_branch2b' with space 0.14G 128/4 6 	(avail 1.53G, req 0G)	t: 0
I0925 01:06:55.486073  4458 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res4a_branch2a' with space 0.14G 128/1 6 	(avail 1.53G, req 0G)	t: 0
I0925 01:06:55.490810  4458 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res4a_branch2b' with space 0.14G 256/4 6 	(avail 1.53G, req 0G)	t: 0
I0925 01:06:55.505345  4458 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'out3a' with space 0.14G 128/2 6 	(avail 1.53G, req 0G)	t: 0
I0925 01:06:55.514979  4458 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'ctx_conv1' with space 0.14G 64/1 6 	(avail 1.53G, req 0G)	t: 0
I0925 01:06:55.530781  4458 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'ctx_final' with space 0.14G 64/1 6 	(avail 1.53G, req 0G)	t: 0
I0925 01:07:06.598469  4458 solver.cpp:654]     Test net output #0: accuracy/top1 = 0.954423
I0925 01:07:06.598492  4458 solver.cpp:654]     Test net output #1: accuracy/top5 = 1
I0925 01:07:06.598500  4458 solver.cpp:654]     Test net output #2: loss = 0.134096 (* 1 = 0.134096 loss)
I0925 01:07:06.624214  4458 solver.cpp:265] [MultiGPU] Tests completed in 11.3048s
I0925 01:07:07.001685  4458 solver.cpp:314] Iteration 2000 (0.52018 iter/s, 192.241s/100 iter), loss = 0.0719299
I0925 01:07:07.001721  4458 solver.cpp:336]     Train net output #0: loss = 0.07193 (* 1 = 0.07193 loss)
I0925 01:07:07.001729  4458 sgd_solver.cpp:136] Iteration 2000, lr = 0.01, m = 0.9
I0925 01:08:02.281584  4458 solver.cpp:314] Iteration 2100 (1.80903 iter/s, 55.2783s/100 iter), loss = 0.0555309
I0925 01:08:02.281801  4458 solver.cpp:336]     Train net output #0: loss = 0.055531 (* 1 = 0.055531 loss)
I0925 01:08:02.281810  4458 sgd_solver.cpp:136] Iteration 2100, lr = 0.01, m = 0.9
I0925 01:08:09.150698  4463 blocking_queue.cpp:40] Waiting for datum
I0925 01:08:56.784605  4458 solver.cpp:314] Iteration 2200 (1.83481 iter/s, 54.5015s/100 iter), loss = 0.110274
I0925 01:08:56.784668  4458 solver.cpp:336]     Train net output #0: loss = 0.110275 (* 1 = 0.110275 loss)
I0925 01:08:56.784678  4458 sgd_solver.cpp:136] Iteration 2200, lr = 0.01, m = 0.9
I0925 01:09:51.553342  4458 solver.cpp:314] Iteration 2300 (1.82591 iter/s, 54.7672s/100 iter), loss = 0.0638129
I0925 01:09:51.558125  4458 solver.cpp:336]     Train net output #0: loss = 0.063813 (* 1 = 0.063813 loss)
I0925 01:09:51.558149  4458 sgd_solver.cpp:136] Iteration 2300, lr = 0.01, m = 0.9
I0925 01:09:57.889430  4410 data_reader.cpp:305] Starting prefetch of epoch 2
I0925 01:10:49.200736  4458 solver.cpp:314] Iteration 2400 (1.73473 iter/s, 57.6458s/100 iter), loss = 0.0593777
I0925 01:10:49.200803  4458 solver.cpp:336]     Train net output #0: loss = 0.0593778 (* 1 = 0.0593778 loss)
I0925 01:10:49.200811  4458 sgd_solver.cpp:136] Iteration 2400, lr = 0.01, m = 0.9
I0925 01:11:46.537686  4458 solver.cpp:314] Iteration 2500 (1.74413 iter/s, 57.3353s/100 iter), loss = 0.0795721
I0925 01:11:46.537811  4458 solver.cpp:336]     Train net output #0: loss = 0.0795722 (* 1 = 0.0795722 loss)
I0925 01:11:46.537822  4458 sgd_solver.cpp:136] Iteration 2500, lr = 0.01, m = 0.9
I0925 01:12:39.185447  4458 solver.cpp:314] Iteration 2600 (1.89947 iter/s, 52.6463s/100 iter), loss = 0.0723751
I0925 01:12:39.188262  4458 solver.cpp:336]     Train net output #0: loss = 0.0723752 (* 1 = 0.0723752 loss)
I0925 01:12:39.188271  4458 sgd_solver.cpp:136] Iteration 2600, lr = 0.01, m = 0.9
I0925 01:13:00.529301  4410 data_reader.cpp:305] Starting prefetch of epoch 3
I0925 01:13:31.110843  4458 solver.cpp:314] Iteration 2700 (1.92589 iter/s, 51.924s/100 iter), loss = 0.0648737
I0925 01:13:31.110913  4458 solver.cpp:336]     Train net output #0: loss = 0.0648739 (* 1 = 0.0648739 loss)
I0925 01:13:31.110918  4458 sgd_solver.cpp:136] Iteration 2700, lr = 0.01, m = 0.9
I0925 01:14:21.037173  4458 solver.cpp:314] Iteration 2800 (2.00301 iter/s, 49.9249s/100 iter), loss = 0.0482054
I0925 01:14:21.037242  4458 solver.cpp:336]     Train net output #0: loss = 0.0482055 (* 1 = 0.0482055 loss)
I0925 01:14:21.037250  4458 sgd_solver.cpp:136] Iteration 2800, lr = 0.01, m = 0.9
I0925 01:14:24.653789  4408 data_reader.cpp:305] Starting prefetch of epoch 1
I0925 01:15:14.235121  4458 solver.cpp:314] Iteration 2900 (1.87982 iter/s, 53.1965s/100 iter), loss = 0.079712
I0925 01:15:14.235183  4458 solver.cpp:336]     Train net output #0: loss = 0.0797122 (* 1 = 0.0797122 loss)
I0925 01:15:14.235191  4458 sgd_solver.cpp:136] Iteration 2900, lr = 0.01, m = 0.9
I0925 01:16:07.881633  4458 solver.cpp:423] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.62 sparsity_achieved=0.605099 iter=3000
I0925 01:17:54.011533  4458 net.cpp:2253] All zero weights of convolution layers are frozen
I0925 01:17:54.017271  4458 solver.cpp:368] Sparsity after update:
I0925 01:17:54.023207  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 01:17:54.023294  4458 net.cpp:2312] conv1a_param_0(0.212) 
I0925 01:17:54.023324  4458 net.cpp:2312] conv1b_param_0(0.511) 
I0925 01:17:54.023337  4458 net.cpp:2312] ctx_conv1_param_0(0.62) 
I0925 01:17:54.023357  4458 net.cpp:2312] ctx_conv2_param_0(0.62) 
I0925 01:17:54.023370  4458 net.cpp:2312] ctx_conv3_param_0(0.62) 
I0925 01:17:54.023396  4458 net.cpp:2312] ctx_conv4_param_0(0.62) 
I0925 01:17:54.023425  4458 net.cpp:2312] ctx_final_param_0(0.229) 
I0925 01:17:54.023452  4458 net.cpp:2312] out3a_param_0(0.62) 
I0925 01:17:54.023483  4458 net.cpp:2312] out5a_param_0(0.62) 
I0925 01:17:54.023504  4458 net.cpp:2312] res2a_branch2a_param_0(0.599) 
I0925 01:17:54.023519  4458 net.cpp:2312] res2a_branch2b_param_0(0.463) 
I0925 01:17:54.023538  4458 net.cpp:2312] res3a_branch2a_param_0(0.613) 
I0925 01:17:54.023555  4458 net.cpp:2312] res3a_branch2b_param_0(0.565) 
I0925 01:17:54.023564  4458 net.cpp:2312] res4a_branch2a_param_0(0.619) 
I0925 01:17:54.023579  4458 net.cpp:2312] res4a_branch2b_param_0(0.609) 
I0925 01:17:54.023592  4458 net.cpp:2312] res5a_branch2a_param_0(0.618) 
I0925 01:17:54.023612  4458 net.cpp:2312] res5a_branch2b_param_0(0.62) 
I0925 01:17:54.023630  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (1.65718e+06/2.69117e+06) 0.616
I0925 01:17:54.300209  4458 solver.cpp:314] Iteration 3000 (0.624763 iter/s, 160.061s/100 iter), loss = 0.0843815
I0925 01:17:54.300288  4458 solver.cpp:336]     Train net output #0: loss = 0.0843816 (* 1 = 0.0843816 loss)
I0925 01:17:54.300305  4458 sgd_solver.cpp:136] Iteration 3000, lr = 0.01, m = 0.9
I0925 01:18:46.360357  4458 solver.cpp:314] Iteration 3100 (1.92091 iter/s, 52.0587s/100 iter), loss = 0.0497417
I0925 01:18:46.360476  4458 solver.cpp:336]     Train net output #0: loss = 0.0497418 (* 1 = 0.0497418 loss)
I0925 01:18:46.360491  4458 sgd_solver.cpp:136] Iteration 3100, lr = 0.01, m = 0.9
I0925 01:19:39.021190  4458 solver.cpp:314] Iteration 3200 (1.899 iter/s, 52.6594s/100 iter), loss = 0.133619
I0925 01:19:39.021885  4458 solver.cpp:336]     Train net output #0: loss = 0.133619 (* 1 = 0.133619 loss)
I0925 01:19:39.021906  4458 sgd_solver.cpp:136] Iteration 3200, lr = 0.01, m = 0.9
I0925 01:20:35.472281  4458 solver.cpp:314] Iteration 3300 (1.77149 iter/s, 56.4495s/100 iter), loss = 0.0670494
I0925 01:20:35.480255  4458 solver.cpp:336]     Train net output #0: loss = 0.0670496 (* 1 = 0.0670496 loss)
I0925 01:20:35.480286  4458 sgd_solver.cpp:136] Iteration 3300, lr = 0.01, m = 0.9
I0925 01:20:36.845343  4410 data_reader.cpp:305] Starting prefetch of epoch 4
I0925 01:21:01.032610  4463 blocking_queue.cpp:40] Waiting for datum
I0925 01:21:32.120131  4458 solver.cpp:314] Iteration 3400 (1.76534 iter/s, 56.6462s/100 iter), loss = 0.042114
I0925 01:21:32.120239  4458 solver.cpp:336]     Train net output #0: loss = 0.0421143 (* 1 = 0.0421143 loss)
I0925 01:21:32.120251  4458 sgd_solver.cpp:136] Iteration 3400, lr = 0.01, m = 0.9
I0925 01:22:09.112004  4461 data_reader.cpp:305] Starting prefetch of epoch 4
I0925 01:22:26.887195  4458 solver.cpp:314] Iteration 3500 (1.82597 iter/s, 54.7655s/100 iter), loss = 0.0822879
I0925 01:22:26.887226  4458 solver.cpp:336]     Train net output #0: loss = 0.0822881 (* 1 = 0.0822881 loss)
I0925 01:22:26.887233  4458 sgd_solver.cpp:136] Iteration 3500, lr = 0.01, m = 0.9
I0925 01:23:20.651252  4458 solver.cpp:314] Iteration 3600 (1.86003 iter/s, 53.7625s/100 iter), loss = 0.0825608
I0925 01:23:20.656659  4458 solver.cpp:336]     Train net output #0: loss = 0.082561 (* 1 = 0.082561 loss)
I0925 01:23:20.656677  4458 sgd_solver.cpp:136] Iteration 3600, lr = 0.01, m = 0.9
I0925 01:24:13.249693  4458 solver.cpp:314] Iteration 3700 (1.90125 iter/s, 52.597s/100 iter), loss = 0.0691699
I0925 01:24:13.249758  4458 solver.cpp:336]     Train net output #0: loss = 0.0691701 (* 1 = 0.0691701 loss)
I0925 01:24:13.249765  4458 sgd_solver.cpp:136] Iteration 3700, lr = 0.01, m = 0.9
I0925 01:24:57.441238  4464 data_reader.cpp:305] Starting prefetch of epoch 4
I0925 01:24:57.806272  4458 solver.cpp:314] Iteration 3800 (2.2444 iter/s, 44.5553s/100 iter), loss = 0.0941469
I0925 01:24:57.806298  4458 solver.cpp:336]     Train net output #0: loss = 0.0941471 (* 1 = 0.0941471 loss)
I0925 01:24:57.806303  4458 sgd_solver.cpp:136] Iteration 3800, lr = 0.01, m = 0.9
I0925 01:25:43.332214  4458 solver.cpp:314] Iteration 3900 (2.19661 iter/s, 45.5246s/100 iter), loss = 0.0516352
I0925 01:25:43.342468  4458 solver.cpp:336]     Train net output #0: loss = 0.0516354 (* 1 = 0.0516354 loss)
I0925 01:25:43.342486  4458 sgd_solver.cpp:136] Iteration 3900, lr = 0.01, m = 0.9
I0925 01:26:34.702335  4458 solver.cpp:423] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.63 sparsity_achieved=0.615785 iter=4000
I0925 01:26:44.060490  4524 data_reader.cpp:305] Starting prefetch of epoch 1
I0925 01:28:50.882848  4458 net.cpp:2253] All zero weights of convolution layers are frozen
I0925 01:28:50.887183  4458 solver.cpp:368] Sparsity after update:
I0925 01:28:50.889250  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 01:28:50.889261  4458 net.cpp:2312] conv1a_param_0(0.216) 
I0925 01:28:50.889271  4458 net.cpp:2312] conv1b_param_0(0.518) 
I0925 01:28:50.889273  4458 net.cpp:2312] ctx_conv1_param_0(0.628) 
I0925 01:28:50.889276  4458 net.cpp:2312] ctx_conv2_param_0(0.628) 
I0925 01:28:50.889277  4458 net.cpp:2312] ctx_conv3_param_0(0.628) 
I0925 01:28:50.889279  4458 net.cpp:2312] ctx_conv4_param_0(0.628) 
I0925 01:28:50.889281  4458 net.cpp:2312] ctx_final_param_0(0.196) 
I0925 01:28:50.889283  4458 net.cpp:2312] out3a_param_0(0.628) 
I0925 01:28:50.889286  4458 net.cpp:2312] out5a_param_0(0.63) 
I0925 01:28:50.889287  4458 net.cpp:2312] res2a_branch2a_param_0(0.61) 
I0925 01:28:50.889289  4458 net.cpp:2312] res2a_branch2b_param_0(0.474) 
I0925 01:28:50.889292  4458 net.cpp:2312] res3a_branch2a_param_0(0.622) 
I0925 01:28:50.889293  4458 net.cpp:2312] res3a_branch2b_param_0(0.579) 
I0925 01:28:50.889295  4458 net.cpp:2312] res4a_branch2a_param_0(0.629) 
I0925 01:28:50.889297  4458 net.cpp:2312] res4a_branch2b_param_0(0.619) 
I0925 01:28:50.889299  4458 net.cpp:2312] res5a_branch2a_param_0(0.629) 
I0925 01:28:50.889302  4458 net.cpp:2312] res5a_branch2b_param_0(0.629) 
I0925 01:28:50.889302  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (1.68368e+06/2.69117e+06) 0.626
I0925 01:28:50.889314  4458 solver.cpp:562] Iteration 4000, Testing net (#0)
I0925 01:29:01.469313  4441 data_reader.cpp:305] Starting prefetch of epoch 1
I0925 01:29:01.836668  4458 solver.cpp:654]     Test net output #0: accuracy/top1 = 0.953589
I0925 01:29:01.836693  4458 solver.cpp:654]     Test net output #1: accuracy/top5 = 1
I0925 01:29:01.836699  4458 solver.cpp:654]     Test net output #2: loss = 0.131769 (* 1 = 0.131769 loss)
I0925 01:29:01.836729  4458 solver.cpp:265] [MultiGPU] Tests completed in 10.9471s
I0925 01:29:02.295228  4458 solver.cpp:314] Iteration 4000 (0.50262 iter/s, 198.957s/100 iter), loss = 0.0535127
I0925 01:29:02.295296  4458 solver.cpp:336]     Train net output #0: loss = 0.0535129 (* 1 = 0.0535129 loss)
I0925 01:29:02.295312  4458 sgd_solver.cpp:136] Iteration 4000, lr = 0.01, m = 0.9
I0925 01:30:01.495846  4458 solver.cpp:314] Iteration 4100 (1.68922 iter/s, 59.1989s/100 iter), loss = 0.0855529
I0925 01:30:01.496026  4458 solver.cpp:336]     Train net output #0: loss = 0.0855531 (* 1 = 0.0855531 loss)
I0925 01:30:01.496037  4458 sgd_solver.cpp:136] Iteration 4100, lr = 0.01, m = 0.9
I0925 01:30:55.378892  4458 solver.cpp:314] Iteration 4200 (1.85592 iter/s, 53.8815s/100 iter), loss = 0.0575099
I0925 01:30:55.379771  4458 solver.cpp:336]     Train net output #0: loss = 0.0575101 (* 1 = 0.0575101 loss)
I0925 01:30:55.379788  4458 sgd_solver.cpp:136] Iteration 4200, lr = 0.01, m = 0.9
I0925 01:31:46.604915  4408 data_reader.cpp:305] Starting prefetch of epoch 2
I0925 01:31:49.497123  4458 solver.cpp:314] Iteration 4300 (1.84786 iter/s, 54.1167s/100 iter), loss = 0.0701713
I0925 01:31:49.497184  4458 solver.cpp:336]     Train net output #0: loss = 0.0701715 (* 1 = 0.0701715 loss)
I0925 01:31:49.497200  4458 sgd_solver.cpp:136] Iteration 4300, lr = 0.01, m = 0.9
I0925 01:32:43.730192  4458 solver.cpp:314] Iteration 4400 (1.84395 iter/s, 54.2315s/100 iter), loss = 0.0517211
I0925 01:32:43.730259  4458 solver.cpp:336]     Train net output #0: loss = 0.0517214 (* 1 = 0.0517214 loss)
I0925 01:32:43.730268  4458 sgd_solver.cpp:136] Iteration 4400, lr = 0.01, m = 0.9
I0925 01:33:40.246224  4458 solver.cpp:314] Iteration 4500 (1.76946 iter/s, 56.5144s/100 iter), loss = 0.0479283
I0925 01:33:40.247745  4458 solver.cpp:336]     Train net output #0: loss = 0.0479285 (* 1 = 0.0479285 loss)
I0925 01:33:40.247758  4458 sgd_solver.cpp:136] Iteration 4500, lr = 0.01, m = 0.9
I0925 01:34:34.400207  4458 solver.cpp:314] Iteration 4600 (1.84664 iter/s, 54.1525s/100 iter), loss = 0.101631
I0925 01:34:34.400387  4458 solver.cpp:336]     Train net output #0: loss = 0.101631 (* 1 = 0.101631 loss)
I0925 01:34:34.400410  4458 sgd_solver.cpp:136] Iteration 4600, lr = 0.01, m = 0.9
I0925 01:34:46.904400  4464 data_reader.cpp:305] Starting prefetch of epoch 5
I0925 01:35:22.841445  4463 blocking_queue.cpp:40] Waiting for datum
I0925 01:35:26.251570  4458 solver.cpp:314] Iteration 4700 (1.92864 iter/s, 51.8499s/100 iter), loss = 0.0670097
I0925 01:35:26.251622  4458 solver.cpp:336]     Train net output #0: loss = 0.0670099 (* 1 = 0.0670099 loss)
I0925 01:35:26.251633  4458 sgd_solver.cpp:136] Iteration 4700, lr = 0.01, m = 0.9
I0925 01:36:13.287379  4458 solver.cpp:314] Iteration 4800 (2.1261 iter/s, 47.0345s/100 iter), loss = 0.0699318
I0925 01:36:13.297664  4458 solver.cpp:336]     Train net output #0: loss = 0.0699321 (* 1 = 0.0699321 loss)
I0925 01:36:13.297703  4458 sgd_solver.cpp:136] Iteration 4800, lr = 0.01, m = 0.9
I0925 01:37:05.485276  4458 solver.cpp:314] Iteration 4900 (1.91584 iter/s, 52.1964s/100 iter), loss = 0.0656094
I0925 01:37:05.485347  4458 solver.cpp:336]     Train net output #0: loss = 0.0656096 (* 1 = 0.0656096 loss)
I0925 01:37:05.485354  4458 sgd_solver.cpp:136] Iteration 4900, lr = 0.01, m = 0.9
I0925 01:37:34.046092  4410 data_reader.cpp:305] Starting prefetch of epoch 5
I0925 01:37:57.595393  4458 solver.cpp:423] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.64 sparsity_achieved=0.625633 iter=5000
I0925 01:39:51.934849  4458 net.cpp:2253] All zero weights of convolution layers are frozen
I0925 01:39:51.939348  4458 solver.cpp:368] Sparsity after update:
I0925 01:39:51.941406  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 01:39:51.941416  4458 net.cpp:2312] conv1a_param_0(0.214) 
I0925 01:39:51.941422  4458 net.cpp:2312] conv1b_param_0(0.526) 
I0925 01:39:51.941426  4458 net.cpp:2312] ctx_conv1_param_0(0.639) 
I0925 01:39:51.941427  4458 net.cpp:2312] ctx_conv2_param_0(0.639) 
I0925 01:39:51.941429  4458 net.cpp:2312] ctx_conv3_param_0(0.639) 
I0925 01:39:51.941431  4458 net.cpp:2312] ctx_conv4_param_0(0.639) 
I0925 01:39:51.941432  4458 net.cpp:2312] ctx_final_param_0(0.228) 
I0925 01:39:51.941434  4458 net.cpp:2312] out3a_param_0(0.639) 
I0925 01:39:51.941437  4458 net.cpp:2312] out5a_param_0(0.64) 
I0925 01:39:51.941438  4458 net.cpp:2312] res2a_branch2a_param_0(0.62) 
I0925 01:39:51.941440  4458 net.cpp:2312] res2a_branch2b_param_0(0.482) 
I0925 01:39:51.941442  4458 net.cpp:2312] res3a_branch2a_param_0(0.633) 
I0925 01:39:51.941444  4458 net.cpp:2312] res3a_branch2b_param_0(0.59) 
I0925 01:39:51.941447  4458 net.cpp:2312] res4a_branch2a_param_0(0.64) 
I0925 01:39:51.941448  4458 net.cpp:2312] res4a_branch2b_param_0(0.63) 
I0925 01:39:51.941449  4458 net.cpp:2312] res5a_branch2a_param_0(0.637) 
I0925 01:39:51.941452  4458 net.cpp:2312] res5a_branch2b_param_0(0.64) 
I0925 01:39:51.941453  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (1.70979e+06/2.69117e+06) 0.635
I0925 01:39:52.152425  4458 solver.cpp:314] Iteration 5000 (0.600015 iter/s, 166.662s/100 iter), loss = 0.0783103
I0925 01:39:52.152459  4458 solver.cpp:336]     Train net output #0: loss = 0.0783105 (* 1 = 0.0783105 loss)
I0925 01:39:52.152465  4458 sgd_solver.cpp:136] Iteration 5000, lr = 0.01, m = 0.9
I0925 01:40:51.093739  4458 solver.cpp:314] Iteration 5100 (1.69665 iter/s, 58.9396s/100 iter), loss = 0.116279
I0925 01:40:51.093796  4458 solver.cpp:336]     Train net output #0: loss = 0.116279 (* 1 = 0.116279 loss)
I0925 01:40:51.093801  4458 sgd_solver.cpp:136] Iteration 5100, lr = 0.01, m = 0.9
I0925 01:41:53.780354  4458 solver.cpp:314] Iteration 5200 (1.59528 iter/s, 62.6848s/100 iter), loss = 0.0713854
I0925 01:41:53.784202  4458 solver.cpp:336]     Train net output #0: loss = 0.0713856 (* 1 = 0.0713856 loss)
I0925 01:41:53.784212  4458 sgd_solver.cpp:136] Iteration 5200, lr = 0.01, m = 0.9
I0925 01:42:43.637039  4410 data_reader.cpp:305] Starting prefetch of epoch 6
I0925 01:42:51.058471  4458 solver.cpp:314] Iteration 5300 (1.74592 iter/s, 57.2765s/100 iter), loss = 0.0544316
I0925 01:42:51.058491  4458 solver.cpp:336]     Train net output #0: loss = 0.0544318 (* 1 = 0.0544318 loss)
I0925 01:42:51.058496  4458 sgd_solver.cpp:136] Iteration 5300, lr = 0.01, m = 0.9
I0925 01:43:44.510455  4458 solver.cpp:314] Iteration 5400 (1.87089 iter/s, 53.4505s/100 iter), loss = 0.0519124
I0925 01:43:44.518133  4458 solver.cpp:336]     Train net output #0: loss = 0.0519126 (* 1 = 0.0519126 loss)
I0925 01:43:44.518165  4458 sgd_solver.cpp:136] Iteration 5400, lr = 0.01, m = 0.9
I0925 01:44:12.109764  4408 data_reader.cpp:305] Starting prefetch of epoch 3
I0925 01:44:37.984426  4458 solver.cpp:314] Iteration 5500 (1.87012 iter/s, 53.4725s/100 iter), loss = 0.0539794
I0925 01:44:37.988770  4458 solver.cpp:336]     Train net output #0: loss = 0.0539797 (* 1 = 0.0539797 loss)
I0925 01:44:37.988788  4458 sgd_solver.cpp:136] Iteration 5500, lr = 0.01, m = 0.9
I0925 01:45:30.350772  4458 solver.cpp:314] Iteration 5600 (1.90968 iter/s, 52.3649s/100 iter), loss = 0.0358533
I0925 01:45:30.350841  4458 solver.cpp:336]     Train net output #0: loss = 0.0358536 (* 1 = 0.0358536 loss)
I0925 01:45:30.350850  4458 sgd_solver.cpp:136] Iteration 5600, lr = 0.01, m = 0.9
I0925 01:46:23.146688  4458 solver.cpp:314] Iteration 5700 (1.89414 iter/s, 52.7944s/100 iter), loss = 0.0880231
I0925 01:46:23.146750  4458 solver.cpp:336]     Train net output #0: loss = 0.0880234 (* 1 = 0.0880234 loss)
I0925 01:46:23.146755  4458 sgd_solver.cpp:136] Iteration 5700, lr = 0.01, m = 0.9
I0925 01:46:58.859629  4462 data_reader.cpp:305] Starting prefetch of epoch 1
I0925 01:47:06.768205  4458 solver.cpp:314] Iteration 5800 (2.29251 iter/s, 43.6203s/100 iter), loss = 0.064471
I0925 01:47:06.768234  4458 solver.cpp:336]     Train net output #0: loss = 0.0644713 (* 1 = 0.0644713 loss)
I0925 01:47:06.768239  4458 sgd_solver.cpp:136] Iteration 5800, lr = 0.01, m = 0.9
I0925 01:47:59.607457  4458 solver.cpp:314] Iteration 5900 (1.89259 iter/s, 52.8378s/100 iter), loss = 0.0953059
I0925 01:47:59.607573  4458 solver.cpp:336]     Train net output #0: loss = 0.0953062 (* 1 = 0.0953062 loss)
I0925 01:47:59.607580  4458 sgd_solver.cpp:136] Iteration 5900, lr = 0.01, m = 0.9
I0925 01:48:51.930580  4458 solver.cpp:423] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.65 sparsity_achieved=0.635335 iter=6000
I0925 01:49:10.075325  4518 data_reader.cpp:305] Starting prefetch of epoch 2
I0925 01:50:53.152897  4458 net.cpp:2253] All zero weights of convolution layers are frozen
I0925 01:50:53.157599  4458 solver.cpp:368] Sparsity after update:
I0925 01:50:53.159934  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 01:50:53.159945  4458 net.cpp:2312] conv1a_param_0(0.224) 
I0925 01:50:53.159973  4458 net.cpp:2312] conv1b_param_0(0.53) 
I0925 01:50:53.159986  4458 net.cpp:2312] ctx_conv1_param_0(0.649) 
I0925 01:50:53.159996  4458 net.cpp:2312] ctx_conv2_param_0(0.649) 
I0925 01:50:53.160004  4458 net.cpp:2312] ctx_conv3_param_0(0.649) 
I0925 01:50:53.160014  4458 net.cpp:2312] ctx_conv4_param_0(0.649) 
I0925 01:50:53.160025  4458 net.cpp:2312] ctx_final_param_0(0.243) 
I0925 01:50:53.160035  4458 net.cpp:2312] out3a_param_0(0.649) 
I0925 01:50:53.160044  4458 net.cpp:2312] out5a_param_0(0.65) 
I0925 01:50:53.160053  4458 net.cpp:2312] res2a_branch2a_param_0(0.629) 
I0925 01:50:53.160063  4458 net.cpp:2312] res2a_branch2b_param_0(0.491) 
I0925 01:50:53.160071  4458 net.cpp:2312] res3a_branch2a_param_0(0.643) 
I0925 01:50:53.160080  4458 net.cpp:2312] res3a_branch2b_param_0(0.602) 
I0925 01:50:53.160089  4458 net.cpp:2312] res4a_branch2a_param_0(0.649) 
I0925 01:50:53.160099  4458 net.cpp:2312] res4a_branch2b_param_0(0.641) 
I0925 01:50:53.160118  4458 net.cpp:2312] res5a_branch2a_param_0(0.648) 
I0925 01:50:53.160130  4458 net.cpp:2312] res5a_branch2b_param_0(0.649) 
I0925 01:50:53.160140  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (1.73774e+06/2.69117e+06) 0.646
I0925 01:50:53.160159  4458 solver.cpp:562] Iteration 6000, Testing net (#0)
I0925 01:51:04.225824  4458 solver.cpp:654]     Test net output #0: accuracy/top1 = 0.950953
I0925 01:51:04.225849  4458 solver.cpp:654]     Test net output #1: accuracy/top5 = 1
I0925 01:51:04.225855  4458 solver.cpp:654]     Test net output #2: loss = 0.14762 (* 1 = 0.14762 loss)
I0925 01:51:04.225881  4458 solver.cpp:265] [MultiGPU] Tests completed in 11.0654s
I0925 01:51:04.600214  4458 solver.cpp:314] Iteration 6000 (0.540577 iter/s, 184.988s/100 iter), loss = 0.116636
I0925 01:51:04.600250  4458 solver.cpp:336]     Train net output #0: loss = 0.116637 (* 1 = 0.116637 loss)
I0925 01:51:04.600255  4458 sgd_solver.cpp:136] Iteration 6000, lr = 0.01, m = 0.9
I0925 01:52:06.196879  4458 solver.cpp:314] Iteration 6100 (1.62351 iter/s, 61.5949s/100 iter), loss = 0.0803168
I0925 01:52:06.196974  4458 solver.cpp:336]     Train net output #0: loss = 0.0803171 (* 1 = 0.0803171 loss)
I0925 01:52:06.196981  4458 sgd_solver.cpp:136] Iteration 6100, lr = 0.01, m = 0.9
I0925 01:53:06.594565  4458 solver.cpp:314] Iteration 6200 (1.65574 iter/s, 60.396s/100 iter), loss = 0.0669521
I0925 01:53:06.594638  4458 solver.cpp:336]     Train net output #0: loss = 0.0669524 (* 1 = 0.0669524 loss)
I0925 01:53:06.594646  4458 sgd_solver.cpp:136] Iteration 6200, lr = 0.01, m = 0.9
I0925 01:53:07.593405  4409 blocking_queue.cpp:40] Waiting for datum
I0925 01:53:50.377650  4466 data_reader.cpp:305] Starting prefetch of epoch 1
I0925 01:54:02.826603  4458 solver.cpp:314] Iteration 6300 (1.7784 iter/s, 56.2305s/100 iter), loss = 0.0628419
I0925 01:54:02.826640  4458 solver.cpp:336]     Train net output #0: loss = 0.0628422 (* 1 = 0.0628422 loss)
I0925 01:54:02.826647  4458 sgd_solver.cpp:136] Iteration 6300, lr = 0.01, m = 0.9
I0925 01:54:58.559576  4458 solver.cpp:314] Iteration 6400 (1.79432 iter/s, 55.7314s/100 iter), loss = 0.0835099
I0925 01:54:58.559757  4458 solver.cpp:336]     Train net output #0: loss = 0.0835102 (* 1 = 0.0835102 loss)
I0925 01:54:58.559765  4458 sgd_solver.cpp:136] Iteration 6400, lr = 0.01, m = 0.9
I0925 01:55:24.415331  4461 data_reader.cpp:305] Starting prefetch of epoch 5
I0925 01:55:55.337072  4458 solver.cpp:314] Iteration 6500 (1.76131 iter/s, 56.7759s/100 iter), loss = 0.082616
I0925 01:55:55.337144  4458 solver.cpp:336]     Train net output #0: loss = 0.0826163 (* 1 = 0.0826163 loss)
I0925 01:55:55.337152  4458 sgd_solver.cpp:136] Iteration 6500, lr = 0.01, m = 0.9
I0925 01:56:49.745275  4458 solver.cpp:314] Iteration 6600 (1.83801 iter/s, 54.4067s/100 iter), loss = 0.0759639
I0925 01:56:49.748242  4458 solver.cpp:336]     Train net output #0: loss = 0.0759641 (* 1 = 0.0759641 loss)
I0925 01:56:49.748265  4458 sgd_solver.cpp:136] Iteration 6600, lr = 0.01, m = 0.9
I0925 01:57:34.472746  4458 solver.cpp:314] Iteration 6700 (2.23583 iter/s, 44.7262s/100 iter), loss = 0.0858847
I0925 01:57:34.472844  4458 solver.cpp:336]     Train net output #0: loss = 0.085885 (* 1 = 0.085885 loss)
I0925 01:57:34.472853  4458 sgd_solver.cpp:136] Iteration 6700, lr = 0.01, m = 0.9
I0925 01:58:11.623003  4408 data_reader.cpp:305] Starting prefetch of epoch 4
I0925 01:58:25.485435  4458 solver.cpp:314] Iteration 6800 (1.96035 iter/s, 51.0112s/100 iter), loss = 0.0425922
I0925 01:58:25.485458  4458 solver.cpp:336]     Train net output #0: loss = 0.0425925 (* 1 = 0.0425925 loss)
I0925 01:58:25.485465  4458 sgd_solver.cpp:136] Iteration 6800, lr = 0.01, m = 0.9
I0925 01:59:17.880154  4458 solver.cpp:314] Iteration 6900 (1.90864 iter/s, 52.3932s/100 iter), loss = 0.0941816
I0925 01:59:17.882586  4458 solver.cpp:336]     Train net output #0: loss = 0.0941818 (* 1 = 0.0941818 loss)
I0925 01:59:17.882596  4458 sgd_solver.cpp:136] Iteration 6900, lr = 0.01, m = 0.9
I0925 02:00:09.966881  4458 solver.cpp:423] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.66 sparsity_achieved=0.645719 iter=7000
I0925 02:01:54.237344  4458 net.cpp:2253] All zero weights of convolution layers are frozen
I0925 02:01:54.242193  4458 solver.cpp:368] Sparsity after update:
I0925 02:01:54.243893  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 02:01:54.243904  4458 net.cpp:2312] conv1a_param_0(0.216) 
I0925 02:01:54.243912  4458 net.cpp:2312] conv1b_param_0(0.539) 
I0925 02:01:54.243916  4458 net.cpp:2312] ctx_conv1_param_0(0.659) 
I0925 02:01:54.243919  4458 net.cpp:2312] ctx_conv2_param_0(0.66) 
I0925 02:01:54.243924  4458 net.cpp:2312] ctx_conv3_param_0(0.659) 
I0925 02:01:54.243927  4458 net.cpp:2312] ctx_conv4_param_0(0.66) 
I0925 02:01:54.243930  4458 net.cpp:2312] ctx_final_param_0(0.236) 
I0925 02:01:54.243933  4458 net.cpp:2312] out3a_param_0(0.66) 
I0925 02:01:54.243937  4458 net.cpp:2312] out5a_param_0(0.66) 
I0925 02:01:54.243940  4458 net.cpp:2312] res2a_branch2a_param_0(0.637) 
I0925 02:01:54.243943  4458 net.cpp:2312] res2a_branch2b_param_0(0.499) 
I0925 02:01:54.243947  4458 net.cpp:2312] res3a_branch2a_param_0(0.654) 
I0925 02:01:54.243949  4458 net.cpp:2312] res3a_branch2b_param_0(0.611) 
I0925 02:01:54.243952  4458 net.cpp:2312] res4a_branch2a_param_0(0.66) 
I0925 02:01:54.243955  4458 net.cpp:2312] res4a_branch2b_param_0(0.651) 
I0925 02:01:54.243958  4458 net.cpp:2312] res5a_branch2a_param_0(0.657) 
I0925 02:01:54.243962  4458 net.cpp:2312] res5a_branch2b_param_0(0.66) 
I0925 02:01:54.243964  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (1.76358e+06/2.69117e+06) 0.655
I0925 02:01:54.458850  4458 solver.cpp:314] Iteration 7000 (0.638674 iter/s, 156.574s/100 iter), loss = 0.0956281
I0925 02:01:54.459086  4458 solver.cpp:336]     Train net output #0: loss = 0.0956284 (* 1 = 0.0956284 loss)
I0925 02:01:54.459210  4458 sgd_solver.cpp:136] Iteration 7000, lr = 0.01, m = 0.9
I0925 02:02:53.890509  4458 solver.cpp:314] Iteration 7100 (1.68265 iter/s, 59.43s/100 iter), loss = 0.0770907
I0925 02:02:53.890565  4458 solver.cpp:336]     Train net output #0: loss = 0.077091 (* 1 = 0.077091 loss)
I0925 02:02:53.890573  4458 sgd_solver.cpp:136] Iteration 7100, lr = 0.01, m = 0.9
I0925 02:02:56.753708  4462 data_reader.cpp:305] Starting prefetch of epoch 2
I0925 02:02:56.753708  4408 data_reader.cpp:305] Starting prefetch of epoch 5
I0925 02:02:56.753708  4461 data_reader.cpp:305] Starting prefetch of epoch 6
I0925 02:03:51.539921  4458 solver.cpp:314] Iteration 7200 (1.73467 iter/s, 57.6478s/100 iter), loss = 0.0851897
I0925 02:03:51.540005  4458 solver.cpp:336]     Train net output #0: loss = 0.08519 (* 1 = 0.08519 loss)
I0925 02:03:51.540014  4458 sgd_solver.cpp:136] Iteration 7200, lr = 0.01, m = 0.9
I0925 02:04:44.861558  4463 blocking_queue.cpp:40] Waiting for datum
I0925 02:04:54.268893  4458 solver.cpp:314] Iteration 7300 (1.5942 iter/s, 62.7272s/100 iter), loss = 0.060052
I0925 02:04:54.269055  4458 solver.cpp:336]     Train net output #0: loss = 0.0600522 (* 1 = 0.0600522 loss)
I0925 02:04:54.269136  4458 sgd_solver.cpp:136] Iteration 7300, lr = 0.01, m = 0.9
I0925 02:05:47.933179  4458 solver.cpp:314] Iteration 7400 (1.86349 iter/s, 53.6628s/100 iter), loss = 0.118153
I0925 02:05:47.933238  4458 solver.cpp:336]     Train net output #0: loss = 0.118153 (* 1 = 0.118153 loss)
I0925 02:05:47.933243  4458 sgd_solver.cpp:136] Iteration 7400, lr = 0.01, m = 0.9
I0925 02:06:40.988456  4458 solver.cpp:314] Iteration 7500 (1.88488 iter/s, 53.0538s/100 iter), loss = 0.0889375
I0925 02:06:40.988531  4458 solver.cpp:336]     Train net output #0: loss = 0.0889377 (* 1 = 0.0889377 loss)
I0925 02:06:40.988539  4458 sgd_solver.cpp:136] Iteration 7500, lr = 0.01, m = 0.9
I0925 02:07:34.917668  4466 data_reader.cpp:305] Starting prefetch of epoch 2
I0925 02:07:35.162915  4458 solver.cpp:314] Iteration 7600 (1.84594 iter/s, 54.1729s/100 iter), loss = 0.0579612
I0925 02:07:35.162940  4458 solver.cpp:336]     Train net output #0: loss = 0.0579615 (* 1 = 0.0579615 loss)
I0925 02:07:35.162946  4458 sgd_solver.cpp:136] Iteration 7600, lr = 0.01, m = 0.9
I0925 02:08:25.290601  4458 solver.cpp:314] Iteration 7700 (1.99496 iter/s, 50.1263s/100 iter), loss = 0.0565666
I0925 02:08:25.290688  4458 solver.cpp:336]     Train net output #0: loss = 0.0565669 (* 1 = 0.0565669 loss)
I0925 02:08:25.290693  4458 sgd_solver.cpp:136] Iteration 7700, lr = 0.01, m = 0.9
I0925 02:09:01.272763  4458 solver.cpp:314] Iteration 7800 (2.77923 iter/s, 35.9811s/100 iter), loss = 0.173881
I0925 02:09:01.272864  4458 solver.cpp:336]     Train net output #0: loss = 0.173882 (* 1 = 0.173882 loss)
I0925 02:09:01.272882  4458 sgd_solver.cpp:136] Iteration 7800, lr = 0.01, m = 0.9
I0925 02:09:51.384215  4458 solver.cpp:314] Iteration 7900 (1.99561 iter/s, 50.11s/100 iter), loss = 0.0640056
I0925 02:09:51.388231  4458 solver.cpp:336]     Train net output #0: loss = 0.0640058 (* 1 = 0.0640058 loss)
I0925 02:09:51.388249  4458 sgd_solver.cpp:136] Iteration 7900, lr = 0.01, m = 0.9
I0925 02:10:07.332602  4410 data_reader.cpp:305] Starting prefetch of epoch 7
I0925 02:10:43.133323  4458 solver.cpp:423] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.67 sparsity_achieved=0.655321 iter=8000
I0925 02:11:11.356217  4518 data_reader.cpp:305] Starting prefetch of epoch 3
I0925 02:12:53.764014  4458 net.cpp:2253] All zero weights of convolution layers are frozen
I0925 02:12:53.775308  4458 solver.cpp:368] Sparsity after update:
I0925 02:12:53.780771  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 02:12:53.780786  4458 net.cpp:2312] conv1a_param_0(0.231) 
I0925 02:12:53.780797  4458 net.cpp:2312] conv1b_param_0(0.545) 
I0925 02:12:53.780799  4458 net.cpp:2312] ctx_conv1_param_0(0.668) 
I0925 02:12:53.780802  4458 net.cpp:2312] ctx_conv2_param_0(0.668) 
I0925 02:12:53.780805  4458 net.cpp:2312] ctx_conv3_param_0(0.668) 
I0925 02:12:53.780808  4458 net.cpp:2312] ctx_conv4_param_0(0.668) 
I0925 02:12:53.780812  4458 net.cpp:2312] ctx_final_param_0(0.208) 
I0925 02:12:53.780815  4458 net.cpp:2312] out3a_param_0(0.668) 
I0925 02:12:53.780818  4458 net.cpp:2312] out5a_param_0(0.67) 
I0925 02:12:53.780833  4458 net.cpp:2312] res2a_branch2a_param_0(0.645) 
I0925 02:12:53.780843  4458 net.cpp:2312] res2a_branch2b_param_0(0.507) 
I0925 02:12:53.780854  4458 net.cpp:2312] res3a_branch2a_param_0(0.662) 
I0925 02:12:53.780864  4458 net.cpp:2312] res3a_branch2b_param_0(0.618) 
I0925 02:12:53.780874  4458 net.cpp:2312] res4a_branch2a_param_0(0.669) 
I0925 02:12:53.780882  4458 net.cpp:2312] res4a_branch2b_param_0(0.66) 
I0925 02:12:53.780892  4458 net.cpp:2312] res5a_branch2a_param_0(0.668) 
I0925 02:12:53.780901  4458 net.cpp:2312] res5a_branch2b_param_0(0.669) 
I0925 02:12:53.780910  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (1.79076e+06/2.69117e+06) 0.665
I0925 02:12:53.780930  4458 solver.cpp:562] Iteration 8000, Testing net (#0)
I0925 02:13:09.159721  4458 solver.cpp:654]     Test net output #0: accuracy/top1 = 0.949476
I0925 02:13:09.159780  4458 solver.cpp:654]     Test net output #1: accuracy/top5 = 1
I0925 02:13:09.159797  4458 solver.cpp:654]     Test net output #2: loss = 0.139229 (* 1 = 0.139229 loss)
I0925 02:13:09.180210  4458 solver.cpp:265] [MultiGPU] Tests completed in 15.3988s
I0925 02:13:09.594492  4458 solver.cpp:314] Iteration 8000 (0.504529 iter/s, 198.205s/100 iter), loss = 0.0891234
I0925 02:13:09.594535  4458 solver.cpp:336]     Train net output #0: loss = 0.0891237 (* 1 = 0.0891237 loss)
I0925 02:13:09.594540  4458 sgd_solver.cpp:136] Iteration 8000, lr = 0.01, m = 0.9
I0925 02:14:12.837617  4464 data_reader.cpp:305] Starting prefetch of epoch 6
I0925 02:14:15.164221  4458 solver.cpp:314] Iteration 8100 (1.52514 iter/s, 65.5679s/100 iter), loss = 0.0562056
I0925 02:14:15.164278  4458 solver.cpp:336]     Train net output #0: loss = 0.0562058 (* 1 = 0.0562058 loss)
I0925 02:14:15.164286  4458 sgd_solver.cpp:136] Iteration 8100, lr = 0.01, m = 0.9
I0925 02:15:14.527389  4458 solver.cpp:314] Iteration 8200 (1.68459 iter/s, 59.3615s/100 iter), loss = 0.0590792
I0925 02:15:14.529856  4458 solver.cpp:336]     Train net output #0: loss = 0.0590794 (* 1 = 0.0590794 loss)
I0925 02:15:14.529868  4458 sgd_solver.cpp:136] Iteration 8200, lr = 0.01, m = 0.9
I0925 02:16:08.881145  4458 solver.cpp:314] Iteration 8300 (1.83985 iter/s, 54.3522s/100 iter), loss = 0.114805
I0925 02:16:08.881202  4458 solver.cpp:336]     Train net output #0: loss = 0.114805 (* 1 = 0.114805 loss)
I0925 02:16:08.881209  4458 sgd_solver.cpp:136] Iteration 8300, lr = 0.01, m = 0.9
I0925 02:17:06.780210  4458 solver.cpp:314] Iteration 8400 (1.72719 iter/s, 57.8974s/100 iter), loss = 0.0576388
I0925 02:17:06.780308  4458 solver.cpp:336]     Train net output #0: loss = 0.0576391 (* 1 = 0.0576391 loss)
I0925 02:17:06.780321  4458 sgd_solver.cpp:136] Iteration 8400, lr = 0.01, m = 0.9
I0925 02:17:23.158313  4410 data_reader.cpp:305] Starting prefetch of epoch 8
I0925 02:18:05.266108  4458 solver.cpp:314] Iteration 8500 (1.70986 iter/s, 58.4842s/100 iter), loss = 0.0564619
I0925 02:18:05.266187  4458 solver.cpp:336]     Train net output #0: loss = 0.0564621 (* 1 = 0.0564621 loss)
I0925 02:18:05.266196  4458 sgd_solver.cpp:136] Iteration 8500, lr = 0.01, m = 0.9
I0925 02:18:57.668068  4458 solver.cpp:314] Iteration 8600 (1.90838 iter/s, 52.4005s/100 iter), loss = 0.0740247
I0925 02:18:57.668155  4458 solver.cpp:336]     Train net output #0: loss = 0.0740249 (* 1 = 0.0740249 loss)
I0925 02:18:57.668169  4458 sgd_solver.cpp:136] Iteration 8600, lr = 0.01, m = 0.9
I0925 02:19:50.173936  4458 solver.cpp:314] Iteration 8700 (1.9046 iter/s, 52.5044s/100 iter), loss = 0.0595687
I0925 02:19:50.174026  4458 solver.cpp:336]     Train net output #0: loss = 0.0595689 (* 1 = 0.0595689 loss)
I0925 02:19:50.174033  4458 sgd_solver.cpp:136] Iteration 8700, lr = 0.01, m = 0.9
I0925 02:20:20.455533  4466 data_reader.cpp:305] Starting prefetch of epoch 3
I0925 02:20:43.768213  4458 solver.cpp:314] Iteration 8800 (1.86592 iter/s, 53.5928s/100 iter), loss = 0.0708111
I0925 02:20:43.768297  4458 solver.cpp:336]     Train net output #0: loss = 0.0708113 (* 1 = 0.0708113 loss)
I0925 02:20:43.768313  4458 sgd_solver.cpp:136] Iteration 8800, lr = 0.01, m = 0.9
I0925 02:21:37.158227  4458 solver.cpp:314] Iteration 8900 (1.87306 iter/s, 53.3885s/100 iter), loss = 0.0576636
I0925 02:21:37.164255  4458 solver.cpp:336]     Train net output #0: loss = 0.0576638 (* 1 = 0.0576638 loss)
I0925 02:21:37.164291  4458 sgd_solver.cpp:136] Iteration 8900, lr = 0.01, m = 0.9
I0925 02:21:48.785161  4461 data_reader.cpp:305] Starting prefetch of epoch 7
I0925 02:22:29.671087  4458 solver.cpp:423] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.68 sparsity_achieved=0.665422 iter=9000
I0925 02:23:58.003547  4458 net.cpp:2253] All zero weights of convolution layers are frozen
I0925 02:23:58.020123  4458 solver.cpp:368] Sparsity after update:
I0925 02:23:58.048926  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 02:23:58.048948  4458 net.cpp:2312] conv1a_param_0(0.222) 
I0925 02:23:58.048961  4458 net.cpp:2312] conv1b_param_0(0.549) 
I0925 02:23:58.048965  4458 net.cpp:2312] ctx_conv1_param_0(0.677) 
I0925 02:23:58.048969  4458 net.cpp:2312] ctx_conv2_param_0(0.678) 
I0925 02:23:58.048971  4458 net.cpp:2312] ctx_conv3_param_0(0.678) 
I0925 02:23:58.048974  4458 net.cpp:2312] ctx_conv4_param_0(0.678) 
I0925 02:23:58.048977  4458 net.cpp:2312] ctx_final_param_0(0.285) 
I0925 02:23:58.048980  4458 net.cpp:2312] out3a_param_0(0.679) 
I0925 02:23:58.048996  4458 net.cpp:2312] out5a_param_0(0.68) 
I0925 02:23:58.049007  4458 net.cpp:2312] res2a_branch2a_param_0(0.653) 
I0925 02:23:58.049016  4458 net.cpp:2312] res2a_branch2b_param_0(0.514) 
I0925 02:23:58.049026  4458 net.cpp:2312] res3a_branch2a_param_0(0.672) 
I0925 02:23:58.049034  4458 net.cpp:2312] res3a_branch2b_param_0(0.626) 
I0925 02:23:58.049043  4458 net.cpp:2312] res4a_branch2a_param_0(0.68) 
I0925 02:23:58.049053  4458 net.cpp:2312] res4a_branch2b_param_0(0.669) 
I0925 02:23:58.049062  4458 net.cpp:2312] res5a_branch2a_param_0(0.678) 
I0925 02:23:58.049072  4458 net.cpp:2312] res5a_branch2b_param_0(0.68) 
I0925 02:23:58.049090  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (1.81776e+06/2.69117e+06) 0.675
I0925 02:23:58.741125  4458 solver.cpp:314] Iteration 9000 (0.70632 iter/s, 141.579s/100 iter), loss = 0.0790174
I0925 02:23:58.741194  4458 solver.cpp:336]     Train net output #0: loss = 0.0790177 (* 1 = 0.0790177 loss)
I0925 02:23:58.741209  4458 sgd_solver.cpp:136] Iteration 9000, lr = 0.01, m = 0.9
I0925 02:24:51.006899  4458 solver.cpp:314] Iteration 9100 (1.91335 iter/s, 52.2643s/100 iter), loss = 0.101818
I0925 02:24:51.007927  4458 solver.cpp:336]     Train net output #0: loss = 0.101819 (* 1 = 0.101819 loss)
I0925 02:24:51.008044  4458 sgd_solver.cpp:136] Iteration 9100, lr = 0.01, m = 0.9
I0925 02:25:05.976357  4463 blocking_queue.cpp:40] Waiting for datum
I0925 02:25:44.892606  4458 solver.cpp:314] Iteration 9200 (1.85583 iter/s, 53.8842s/100 iter), loss = 0.054595
I0925 02:25:44.892737  4458 solver.cpp:336]     Train net output #0: loss = 0.0545953 (* 1 = 0.0545953 loss)
I0925 02:25:44.892746  4458 sgd_solver.cpp:136] Iteration 9200, lr = 0.01, m = 0.9
I0925 02:26:14.204205  4461 data_reader.cpp:305] Starting prefetch of epoch 8
I0925 02:26:39.689431  4458 solver.cpp:314] Iteration 9300 (1.82497 iter/s, 54.7953s/100 iter), loss = 0.0878888
I0925 02:26:39.692633  4458 solver.cpp:336]     Train net output #0: loss = 0.087889 (* 1 = 0.087889 loss)
I0925 02:26:39.692651  4458 sgd_solver.cpp:136] Iteration 9300, lr = 0.01, m = 0.9
I0925 02:27:32.261657  4458 solver.cpp:314] Iteration 9400 (1.9022 iter/s, 52.5707s/100 iter), loss = 0.0799964
I0925 02:27:32.261826  4458 solver.cpp:336]     Train net output #0: loss = 0.0799966 (* 1 = 0.0799966 loss)
I0925 02:27:32.261839  4458 sgd_solver.cpp:136] Iteration 9400, lr = 0.01, m = 0.9
I0925 02:28:27.479095  4458 solver.cpp:314] Iteration 9500 (1.81107 iter/s, 55.2159s/100 iter), loss = 0.0631991
I0925 02:28:27.479178  4458 solver.cpp:336]     Train net output #0: loss = 0.0631993 (* 1 = 0.0631993 loss)
I0925 02:28:27.479194  4458 sgd_solver.cpp:136] Iteration 9500, lr = 0.01, m = 0.9
I0925 02:29:11.478914  4410 data_reader.cpp:305] Starting prefetch of epoch 9
I0925 02:29:20.250174  4458 solver.cpp:314] Iteration 9600 (1.89503 iter/s, 52.7696s/100 iter), loss = 0.0714151
I0925 02:29:20.250242  4458 solver.cpp:336]     Train net output #0: loss = 0.0714153 (* 1 = 0.0714153 loss)
I0925 02:29:20.250257  4458 sgd_solver.cpp:136] Iteration 9600, lr = 0.01, m = 0.9
I0925 02:30:14.729975  4458 solver.cpp:314] Iteration 9700 (1.83559 iter/s, 54.4783s/100 iter), loss = 0.0567478
I0925 02:30:14.730034  4458 solver.cpp:336]     Train net output #0: loss = 0.056748 (* 1 = 0.056748 loss)
I0925 02:30:14.730039  4458 sgd_solver.cpp:136] Iteration 9700, lr = 0.01, m = 0.9
I0925 02:31:08.284211  4458 solver.cpp:314] Iteration 9800 (1.86732 iter/s, 53.5527s/100 iter), loss = 0.0523042
I0925 02:31:08.288203  4458 solver.cpp:336]     Train net output #0: loss = 0.0523044 (* 1 = 0.0523044 loss)
I0925 02:31:08.288213  4458 sgd_solver.cpp:136] Iteration 9800, lr = 0.01, m = 0.9
I0925 02:32:02.366533  4458 solver.cpp:314] Iteration 9900 (1.84909 iter/s, 54.0808s/100 iter), loss = 0.136962
I0925 02:32:02.372277  4458 solver.cpp:336]     Train net output #0: loss = 0.136962 (* 1 = 0.136962 loss)
I0925 02:32:02.372303  4458 sgd_solver.cpp:136] Iteration 9900, lr = 0.01, m = 0.9
I0925 02:32:09.264658  4464 data_reader.cpp:305] Starting prefetch of epoch 7
I0925 02:32:56.519031  4458 solver.cpp:824] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/cityscapes5_jsegnet21v2_iter_10000.caffemodel
I0925 02:32:57.268509  4458 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/cityscapes5_jsegnet21v2_iter_10000.solverstate
I0925 02:32:57.306638  4458 solver.cpp:423] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.69 sparsity_achieved=0.675456 iter=10000
I0925 02:34:34.326874  4458 net.cpp:2253] All zero weights of convolution layers are frozen
I0925 02:34:34.348803  4458 solver.cpp:368] Sparsity after update:
I0925 02:34:34.355057  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 02:34:34.355072  4458 net.cpp:2312] conv1a_param_0(0.222) 
I0925 02:34:34.355085  4458 net.cpp:2312] conv1b_param_0(0.553) 
I0925 02:34:34.355089  4458 net.cpp:2312] ctx_conv1_param_0(0.687) 
I0925 02:34:34.355093  4458 net.cpp:2312] ctx_conv2_param_0(0.688) 
I0925 02:34:34.355095  4458 net.cpp:2312] ctx_conv3_param_0(0.688) 
I0925 02:34:34.355099  4458 net.cpp:2312] ctx_conv4_param_0(0.688) 
I0925 02:34:34.355114  4458 net.cpp:2312] ctx_final_param_0(0.215) 
I0925 02:34:34.355125  4458 net.cpp:2312] out3a_param_0(0.689) 
I0925 02:34:34.355135  4458 net.cpp:2312] out5a_param_0(0.69) 
I0925 02:34:34.355144  4458 net.cpp:2312] res2a_branch2a_param_0(0.662) 
I0925 02:34:34.355154  4458 net.cpp:2312] res2a_branch2b_param_0(0.521) 
I0925 02:34:34.355162  4458 net.cpp:2312] res3a_branch2a_param_0(0.681) 
I0925 02:34:34.355171  4458 net.cpp:2312] res3a_branch2b_param_0(0.633) 
I0925 02:34:34.355180  4458 net.cpp:2312] res4a_branch2a_param_0(0.689) 
I0925 02:34:34.355190  4458 net.cpp:2312] res4a_branch2b_param_0(0.678) 
I0925 02:34:34.355198  4458 net.cpp:2312] res5a_branch2a_param_0(0.687) 
I0925 02:34:34.355207  4458 net.cpp:2312] res5a_branch2b_param_0(0.689) 
I0925 02:34:34.355216  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (1.8421e+06/2.69117e+06) 0.685
I0925 02:34:34.355237  4458 solver.cpp:562] Iteration 10000, Testing net (#0)
I0925 02:34:49.240284  4441 data_reader.cpp:305] Starting prefetch of epoch 2
I0925 02:34:57.267412  4458 solver.cpp:654]     Test net output #0: accuracy/top1 = 0.953072
I0925 02:34:57.267437  4458 solver.cpp:654]     Test net output #1: accuracy/top5 = 1
I0925 02:34:57.267444  4458 solver.cpp:654]     Test net output #2: loss = 0.151977 (* 1 = 0.151977 loss)
I0925 02:34:57.267477  4458 solver.cpp:265] [MultiGPU] Tests completed in 22.9116s
I0925 02:34:57.927826  4458 solver.cpp:314] Iteration 10000 (0.569618 iter/s, 175.556s/100 iter), loss = 0.0574868
I0925 02:34:57.927872  4458 solver.cpp:336]     Train net output #0: loss = 0.057487 (* 1 = 0.057487 loss)
I0925 02:34:57.927880  4458 sgd_solver.cpp:136] Iteration 10000, lr = 0.01, m = 0.9
I0925 02:35:57.979269  4458 solver.cpp:314] Iteration 10100 (1.66529 iter/s, 60.0497s/100 iter), loss = 0.0691246
I0925 02:35:57.980242  4458 solver.cpp:336]     Train net output #0: loss = 0.0691248 (* 1 = 0.0691248 loss)
I0925 02:35:57.980257  4458 sgd_solver.cpp:136] Iteration 10100, lr = 0.01, m = 0.9
I0925 02:36:57.000205  4458 solver.cpp:314] Iteration 10200 (1.69436 iter/s, 59.0193s/100 iter), loss = 0.145585
I0925 02:36:57.000511  4458 solver.cpp:336]     Train net output #0: loss = 0.145585 (* 1 = 0.145585 loss)
I0925 02:36:57.000520  4458 sgd_solver.cpp:136] Iteration 10200, lr = 0.01, m = 0.9
I0925 02:37:52.955345  4458 solver.cpp:314] Iteration 10300 (1.7872 iter/s, 55.9536s/100 iter), loss = 0.0469976
I0925 02:37:52.962414  4458 solver.cpp:336]     Train net output #0: loss = 0.0469978 (* 1 = 0.0469978 loss)
I0925 02:37:52.962421  4458 sgd_solver.cpp:136] Iteration 10300, lr = 0.01, m = 0.9
I0925 02:38:46.613792  4458 solver.cpp:314] Iteration 10400 (1.86369 iter/s, 53.6569s/100 iter), loss = 0.0485417
I0925 02:38:46.613905  4458 solver.cpp:336]     Train net output #0: loss = 0.0485419 (* 1 = 0.0485419 loss)
I0925 02:38:46.613914  4458 sgd_solver.cpp:136] Iteration 10400, lr = 0.01, m = 0.9
I0925 02:38:52.011765  4464 data_reader.cpp:305] Starting prefetch of epoch 8
I0925 02:39:44.559460  4458 solver.cpp:314] Iteration 10500 (1.7258 iter/s, 57.944s/100 iter), loss = 0.053544
I0925 02:39:44.559566  4458 solver.cpp:336]     Train net output #0: loss = 0.0535442 (* 1 = 0.0535442 loss)
I0925 02:39:44.559586  4458 sgd_solver.cpp:136] Iteration 10500, lr = 0.01, m = 0.9
I0925 02:39:56.750705  4465 blocking_queue.cpp:40] Waiting for datum
I0925 02:40:23.730960  4462 data_reader.cpp:305] Starting prefetch of epoch 3
I0925 02:40:37.108989  4458 solver.cpp:314] Iteration 10600 (1.90302 iter/s, 52.548s/100 iter), loss = 0.0772351
I0925 02:40:37.109024  4458 solver.cpp:336]     Train net output #0: loss = 0.0772353 (* 1 = 0.0772353 loss)
I0925 02:40:37.109030  4458 sgd_solver.cpp:136] Iteration 10600, lr = 0.01, m = 0.9
I0925 02:41:29.936211  4458 solver.cpp:314] Iteration 10700 (1.89302 iter/s, 52.8257s/100 iter), loss = 0.0444095
I0925 02:41:29.940196  4458 solver.cpp:336]     Train net output #0: loss = 0.0444098 (* 1 = 0.0444098 loss)
I0925 02:41:29.940203  4458 sgd_solver.cpp:136] Iteration 10700, lr = 0.01, m = 0.9
I0925 02:42:22.265986  4458 solver.cpp:314] Iteration 10800 (1.91101 iter/s, 52.3283s/100 iter), loss = 0.0797014
I0925 02:42:22.266083  4458 solver.cpp:336]     Train net output #0: loss = 0.0797017 (* 1 = 0.0797017 loss)
I0925 02:42:22.266100  4458 sgd_solver.cpp:136] Iteration 10800, lr = 0.01, m = 0.9
I0925 02:43:14.537741  4458 solver.cpp:314] Iteration 10900 (1.91313 iter/s, 52.2703s/100 iter), loss = 0.0433695
I0925 02:43:14.537909  4458 solver.cpp:336]     Train net output #0: loss = 0.0433697 (* 1 = 0.0433697 loss)
I0925 02:43:14.537933  4458 sgd_solver.cpp:136] Iteration 10900, lr = 0.01, m = 0.9
I0925 02:43:17.688758  4408 data_reader.cpp:305] Starting prefetch of epoch 6
I0925 02:44:01.087532  4458 solver.cpp:423] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.7 sparsity_achieved=0.6845 iter=11000
I0925 02:46:24.359772  4458 net.cpp:2253] All zero weights of convolution layers are frozen
I0925 02:46:24.374619  4458 solver.cpp:368] Sparsity after update:
I0925 02:46:24.407706  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 02:46:24.407727  4458 net.cpp:2312] conv1a_param_0(0.228) 
I0925 02:46:24.407744  4458 net.cpp:2312] conv1b_param_0(0.555) 
I0925 02:46:24.407748  4458 net.cpp:2312] ctx_conv1_param_0(0.696) 
I0925 02:46:24.407752  4458 net.cpp:2312] ctx_conv2_param_0(0.698) 
I0925 02:46:24.407754  4458 net.cpp:2312] ctx_conv3_param_0(0.697) 
I0925 02:46:24.407757  4458 net.cpp:2312] ctx_conv4_param_0(0.699) 
I0925 02:46:24.407762  4458 net.cpp:2312] ctx_final_param_0(0.247) 
I0925 02:46:24.407766  4458 net.cpp:2312] out3a_param_0(0.7) 
I0925 02:46:24.407770  4458 net.cpp:2312] out5a_param_0(0.7) 
I0925 02:46:24.407773  4458 net.cpp:2312] res2a_branch2a_param_0(0.67) 
I0925 02:46:24.407775  4458 net.cpp:2312] res2a_branch2b_param_0(0.524) 
I0925 02:46:24.407778  4458 net.cpp:2312] res3a_branch2a_param_0(0.69) 
I0925 02:46:24.407781  4458 net.cpp:2312] res3a_branch2b_param_0(0.639) 
I0925 02:46:24.407783  4458 net.cpp:2312] res4a_branch2a_param_0(0.699) 
I0925 02:46:24.407786  4458 net.cpp:2312] res4a_branch2b_param_0(0.687) 
I0925 02:46:24.407789  4458 net.cpp:2312] res5a_branch2a_param_0(0.698) 
I0925 02:46:24.407791  4458 net.cpp:2312] res5a_branch2b_param_0(0.7) 
I0925 02:46:24.407794  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (1.87008e+06/2.69117e+06) 0.695
I0925 02:46:25.142025  4458 solver.cpp:314] Iteration 11000 (0.524662 iter/s, 190.599s/100 iter), loss = 0.0567584
I0925 02:46:25.142240  4458 solver.cpp:336]     Train net output #0: loss = 0.0567586 (* 1 = 0.0567586 loss)
I0925 02:46:25.142360  4458 sgd_solver.cpp:136] Iteration 11000, lr = 0.01, m = 0.9
I0925 02:47:26.609830  4460 blocking_queue.cpp:40] Data layer prefetch queue empty
I0925 02:47:28.080425  4458 solver.cpp:314] Iteration 11100 (1.5889 iter/s, 62.9366s/100 iter), loss = 0.0789495
I0925 02:47:28.080477  4458 solver.cpp:336]     Train net output #0: loss = 0.0789497 (* 1 = 0.0789497 loss)
I0925 02:47:28.080484  4458 sgd_solver.cpp:136] Iteration 11100, lr = 0.01, m = 0.9
I0925 02:48:33.126354  4458 solver.cpp:314] Iteration 11200 (1.53742 iter/s, 65.0441s/100 iter), loss = 0.0684996
I0925 02:48:33.126435  4458 solver.cpp:336]     Train net output #0: loss = 0.0684998 (* 1 = 0.0684998 loss)
I0925 02:48:33.126447  4458 sgd_solver.cpp:136] Iteration 11200, lr = 0.01, m = 0.9
I0925 02:49:27.392201  4458 solver.cpp:314] Iteration 11300 (1.84283 iter/s, 54.2643s/100 iter), loss = 0.0729855
I0925 02:49:27.392297  4458 solver.cpp:336]     Train net output #0: loss = 0.0729857 (* 1 = 0.0729857 loss)
I0925 02:49:27.392307  4458 sgd_solver.cpp:136] Iteration 11300, lr = 0.01, m = 0.9
I0925 02:50:21.096295  4458 solver.cpp:314] Iteration 11400 (1.86211 iter/s, 53.7026s/100 iter), loss = 0.0530369
I0925 02:50:21.096388  4458 solver.cpp:336]     Train net output #0: loss = 0.0530371 (* 1 = 0.0530371 loss)
I0925 02:50:21.096398  4458 sgd_solver.cpp:136] Iteration 11400, lr = 0.01, m = 0.9
I0925 02:50:21.609905  4410 data_reader.cpp:305] Starting prefetch of epoch 10
I0925 02:51:14.409693  4458 solver.cpp:314] Iteration 11500 (1.87575 iter/s, 53.3119s/100 iter), loss = 0.0719999
I0925 02:51:14.409770  4458 solver.cpp:336]     Train net output #0: loss = 0.0720002 (* 1 = 0.0720002 loss)
I0925 02:51:14.409781  4458 sgd_solver.cpp:136] Iteration 11500, lr = 0.01, m = 0.9
I0925 02:51:49.348461  4462 data_reader.cpp:305] Starting prefetch of epoch 4
I0925 02:52:06.944214  4458 solver.cpp:314] Iteration 11600 (1.90356 iter/s, 52.533s/100 iter), loss = 0.089125
I0925 02:52:06.944252  4458 solver.cpp:336]     Train net output #0: loss = 0.0891252 (* 1 = 0.0891252 loss)
I0925 02:52:06.944265  4458 sgd_solver.cpp:136] Iteration 11600, lr = 0.01, m = 0.9
I0925 02:52:53.983779  4458 solver.cpp:314] Iteration 11700 (2.12593 iter/s, 47.0382s/100 iter), loss = 0.0373272
I0925 02:52:53.983909  4458 solver.cpp:336]     Train net output #0: loss = 0.0373275 (* 1 = 0.0373275 loss)
I0925 02:52:53.983928  4458 sgd_solver.cpp:136] Iteration 11700, lr = 0.01, m = 0.9
I0925 02:53:45.714016  4458 solver.cpp:314] Iteration 11800 (1.93316 iter/s, 51.7288s/100 iter), loss = 0.049687
I0925 02:53:45.714094  4458 solver.cpp:336]     Train net output #0: loss = 0.0496872 (* 1 = 0.0496872 loss)
I0925 02:53:45.714102  4458 sgd_solver.cpp:136] Iteration 11800, lr = 0.01, m = 0.9
I0925 02:54:38.861786  4408 data_reader.cpp:305] Starting prefetch of epoch 7
I0925 02:54:39.999375  4458 solver.cpp:314] Iteration 11900 (1.84217 iter/s, 54.2838s/100 iter), loss = 0.0607983
I0925 02:54:39.999413  4458 solver.cpp:336]     Train net output #0: loss = 0.0607985 (* 1 = 0.0607985 loss)
I0925 02:54:39.999418  4458 sgd_solver.cpp:136] Iteration 11900, lr = 0.01, m = 0.9
I0925 02:55:32.564873  4458 solver.cpp:423] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.71 sparsity_achieved=0.694897 iter=12000
I0925 02:57:55.174417  4458 net.cpp:2253] All zero weights of convolution layers are frozen
I0925 02:57:55.194682  4458 solver.cpp:368] Sparsity after update:
I0925 02:57:55.202855  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 02:57:55.202883  4458 net.cpp:2312] conv1a_param_0(0.229) 
I0925 02:57:55.202893  4458 net.cpp:2312] conv1b_param_0(0.558) 
I0925 02:57:55.202898  4458 net.cpp:2312] ctx_conv1_param_0(0.704) 
I0925 02:57:55.202901  4458 net.cpp:2312] ctx_conv2_param_0(0.706) 
I0925 02:57:55.202904  4458 net.cpp:2312] ctx_conv3_param_0(0.705) 
I0925 02:57:55.202908  4458 net.cpp:2312] ctx_conv4_param_0(0.707) 
I0925 02:57:55.202910  4458 net.cpp:2312] ctx_final_param_0(0.221) 
I0925 02:57:55.202913  4458 net.cpp:2312] out3a_param_0(0.708) 
I0925 02:57:55.202916  4458 net.cpp:2312] out5a_param_0(0.71) 
I0925 02:57:55.202920  4458 net.cpp:2312] res2a_branch2a_param_0(0.677) 
I0925 02:57:55.202924  4458 net.cpp:2312] res2a_branch2b_param_0(0.529) 
I0925 02:57:55.202926  4458 net.cpp:2312] res3a_branch2a_param_0(0.697) 
I0925 02:57:55.202932  4458 net.cpp:2312] res3a_branch2b_param_0(0.645) 
I0925 02:57:55.202936  4458 net.cpp:2312] res4a_branch2a_param_0(0.709) 
I0925 02:57:55.202940  4458 net.cpp:2312] res4a_branch2b_param_0(0.694) 
I0925 02:57:55.202944  4458 net.cpp:2312] res5a_branch2a_param_0(0.707) 
I0925 02:57:55.202947  4458 net.cpp:2312] res5a_branch2b_param_0(0.709) 
I0925 02:57:55.202950  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (1.8939e+06/2.69117e+06) 0.704
I0925 02:57:55.202965  4458 solver.cpp:562] Iteration 12000, Testing net (#0)
I0925 02:58:02.413606  4441 data_reader.cpp:305] Starting prefetch of epoch 3
I0925 02:58:18.502169  4458 solver.cpp:654]     Test net output #0: accuracy/top1 = 0.950741
I0925 02:58:18.502244  4458 solver.cpp:654]     Test net output #1: accuracy/top5 = 1
I0925 02:58:18.502269  4458 solver.cpp:654]     Test net output #2: loss = 0.141723 (* 1 = 0.141723 loss)
I0925 02:58:18.502305  4458 solver.cpp:265] [MultiGPU] Tests completed in 23.2987s
I0925 02:58:18.995242  4458 solver.cpp:314] Iteration 12000 (0.456642 iter/s, 218.99s/100 iter), loss = 0.0451833
I0925 02:58:18.995276  4458 solver.cpp:336]     Train net output #0: loss = 0.0451835 (* 1 = 0.0451835 loss)
I0925 02:58:18.995282  4458 sgd_solver.cpp:136] Iteration 12000, lr = 0.01, m = 0.9
I0925 02:59:16.744987  4458 solver.cpp:314] Iteration 12100 (1.73166 iter/s, 57.7481s/100 iter), loss = 0.0838422
I0925 02:59:16.745092  4458 solver.cpp:336]     Train net output #0: loss = 0.0838425 (* 1 = 0.0838425 loss)
I0925 02:59:16.745112  4458 sgd_solver.cpp:136] Iteration 12100, lr = 0.01, m = 0.9
I0925 03:00:14.000219  4458 solver.cpp:314] Iteration 12200 (1.74661 iter/s, 57.2536s/100 iter), loss = 0.0558449
I0925 03:00:14.002252  4458 solver.cpp:336]     Train net output #0: loss = 0.0558452 (* 1 = 0.0558452 loss)
I0925 03:00:14.002260  4458 sgd_solver.cpp:136] Iteration 12200, lr = 0.01, m = 0.9
I0925 03:00:28.728265  4410 data_reader.cpp:305] Starting prefetch of epoch 11
I0925 03:01:07.905336  4458 solver.cpp:314] Iteration 12300 (1.85516 iter/s, 53.9036s/100 iter), loss = 0.0484546
I0925 03:01:07.905411  4458 solver.cpp:336]     Train net output #0: loss = 0.0484548 (* 1 = 0.0484548 loss)
I0925 03:01:07.905419  4458 sgd_solver.cpp:136] Iteration 12300, lr = 0.01, m = 0.9
I0925 03:01:34.619518  4409 blocking_queue.cpp:40] Waiting for datum
I0925 03:02:04.219661  4458 solver.cpp:314] Iteration 12400 (1.7758 iter/s, 56.3127s/100 iter), loss = 0.0513571
I0925 03:02:04.219736  4458 solver.cpp:336]     Train net output #0: loss = 0.0513574 (* 1 = 0.0513574 loss)
I0925 03:02:04.219744  4458 sgd_solver.cpp:136] Iteration 12400, lr = 0.01, m = 0.9
I0925 03:02:58.987217  4458 solver.cpp:314] Iteration 12500 (1.82595 iter/s, 54.766s/100 iter), loss = 0.0721469
I0925 03:02:58.992275  4458 solver.cpp:336]     Train net output #0: loss = 0.0721471 (* 1 = 0.0721471 loss)
I0925 03:02:58.992321  4458 sgd_solver.cpp:136] Iteration 12500, lr = 0.01, m = 0.9
I0925 03:03:30.463306  4461 data_reader.cpp:305] Starting prefetch of epoch 9
I0925 03:03:52.613782  4458 solver.cpp:314] Iteration 12600 (1.8648 iter/s, 53.625s/100 iter), loss = 0.0415946
I0925 03:03:52.613816  4458 solver.cpp:336]     Train net output #0: loss = 0.0415949 (* 1 = 0.0415949 loss)
I0925 03:03:52.613822  4458 sgd_solver.cpp:136] Iteration 12600, lr = 0.01, m = 0.9
I0925 03:04:45.088219  4458 solver.cpp:314] Iteration 12700 (1.90574 iter/s, 52.4729s/100 iter), loss = 0.127047
I0925 03:04:45.091771  4458 solver.cpp:336]     Train net output #0: loss = 0.127047 (* 1 = 0.127047 loss)
I0925 03:04:45.091790  4458 sgd_solver.cpp:136] Iteration 12700, lr = 0.01, m = 0.9
I0925 03:05:37.950055  4458 solver.cpp:314] Iteration 12800 (1.89178 iter/s, 52.8604s/100 iter), loss = 0.0471135
I0925 03:05:37.950119  4458 solver.cpp:336]     Train net output #0: loss = 0.0471138 (* 1 = 0.0471138 loss)
I0925 03:05:37.950124  4458 sgd_solver.cpp:136] Iteration 12800, lr = 0.01, m = 0.9
I0925 03:06:23.904438  4464 data_reader.cpp:305] Starting prefetch of epoch 9
I0925 03:06:30.122056  4458 solver.cpp:314] Iteration 12900 (1.91679 iter/s, 52.1705s/100 iter), loss = 0.0685682
I0925 03:06:30.122087  4458 solver.cpp:336]     Train net output #0: loss = 0.0685684 (* 1 = 0.0685684 loss)
I0925 03:06:30.122184  4458 sgd_solver.cpp:136] Iteration 12900, lr = 0.01, m = 0.9
I0925 03:07:21.783216  4458 solver.cpp:423] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.72 sparsity_achieved=0.703745 iter=13000
I0925 03:09:52.596448  4458 net.cpp:2253] All zero weights of convolution layers are frozen
I0925 03:09:52.608927  4458 solver.cpp:368] Sparsity after update:
I0925 03:09:52.622547  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 03:09:52.622566  4458 net.cpp:2312] conv1a_param_0(0.23) 
I0925 03:09:52.622575  4458 net.cpp:2312] conv1b_param_0(0.562) 
I0925 03:09:52.622579  4458 net.cpp:2312] ctx_conv1_param_0(0.714) 
I0925 03:09:52.622581  4458 net.cpp:2312] ctx_conv2_param_0(0.716) 
I0925 03:09:52.622584  4458 net.cpp:2312] ctx_conv3_param_0(0.715) 
I0925 03:09:52.622587  4458 net.cpp:2312] ctx_conv4_param_0(0.717) 
I0925 03:09:52.622591  4458 net.cpp:2312] ctx_final_param_0(0.29) 
I0925 03:09:52.622593  4458 net.cpp:2312] out3a_param_0(0.719) 
I0925 03:09:52.622596  4458 net.cpp:2312] out5a_param_0(0.72) 
I0925 03:09:52.622599  4458 net.cpp:2312] res2a_branch2a_param_0(0.684) 
I0925 03:09:52.622602  4458 net.cpp:2312] res2a_branch2b_param_0(0.535) 
I0925 03:09:52.622606  4458 net.cpp:2312] res3a_branch2a_param_0(0.705) 
I0925 03:09:52.622609  4458 net.cpp:2312] res3a_branch2b_param_0(0.652) 
I0925 03:09:52.622625  4458 net.cpp:2312] res4a_branch2a_param_0(0.719) 
I0925 03:09:52.622637  4458 net.cpp:2312] res4a_branch2b_param_0(0.703) 
I0925 03:09:52.622648  4458 net.cpp:2312] res5a_branch2a_param_0(0.718) 
I0925 03:09:52.622658  4458 net.cpp:2312] res5a_branch2b_param_0(0.72) 
I0925 03:09:52.622668  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (1.92173e+06/2.69117e+06) 0.714
I0925 03:09:53.352717  4458 solver.cpp:314] Iteration 13000 (0.492065 iter/s, 203.225s/100 iter), loss = 0.0458598
I0925 03:09:53.352783  4458 solver.cpp:336]     Train net output #0: loss = 0.04586 (* 1 = 0.04586 loss)
I0925 03:09:53.352799  4458 sgd_solver.cpp:136] Iteration 13000, lr = 0.01, m = 0.9
I0925 03:10:21.511025  4461 data_reader.cpp:305] Starting prefetch of epoch 10
I0925 03:10:46.799113  4458 solver.cpp:314] Iteration 13100 (1.87109 iter/s, 53.4449s/100 iter), loss = 0.074997
I0925 03:10:46.799218  4458 solver.cpp:336]     Train net output #0: loss = 0.0749972 (* 1 = 0.0749972 loss)
I0925 03:10:46.799226  4458 sgd_solver.cpp:136] Iteration 13100, lr = 0.01, m = 0.9
I0925 03:11:43.997104  4458 solver.cpp:314] Iteration 13200 (1.74836 iter/s, 57.1964s/100 iter), loss = 0.0496266
I0925 03:11:43.997298  4458 solver.cpp:336]     Train net output #0: loss = 0.0496268 (* 1 = 0.0496268 loss)
I0925 03:11:43.997304  4458 sgd_solver.cpp:136] Iteration 13200, lr = 0.01, m = 0.9
I0925 03:12:41.412755  4458 solver.cpp:314] Iteration 13300 (1.74173 iter/s, 57.414s/100 iter), loss = 0.0603001
I0925 03:12:41.412967  4458 solver.cpp:336]     Train net output #0: loss = 0.0603003 (* 1 = 0.0603003 loss)
I0925 03:12:41.412977  4458 sgd_solver.cpp:136] Iteration 13300, lr = 0.01, m = 0.9
I0925 03:13:29.168575  4408 data_reader.cpp:305] Starting prefetch of epoch 8
I0925 03:13:37.218921  4458 solver.cpp:314] Iteration 13400 (1.79197 iter/s, 55.8046s/100 iter), loss = 0.0482325
I0925 03:13:37.218966  4458 solver.cpp:336]     Train net output #0: loss = 0.0482327 (* 1 = 0.0482327 loss)
I0925 03:13:37.218974  4458 sgd_solver.cpp:136] Iteration 13400, lr = 0.01, m = 0.9
I0925 03:14:32.288216  4458 solver.cpp:314] Iteration 13500 (1.81595 iter/s, 55.0677s/100 iter), loss = 0.0800169
I0925 03:14:32.289916  4458 solver.cpp:336]     Train net output #0: loss = 0.0800171 (* 1 = 0.0800171 loss)
I0925 03:14:32.289932  4458 sgd_solver.cpp:136] Iteration 13500, lr = 0.01, m = 0.9
I0925 03:15:23.978941  4458 solver.cpp:314] Iteration 13600 (1.93464 iter/s, 51.6893s/100 iter), loss = 0.0741117
I0925 03:15:23.984227  4458 solver.cpp:336]     Train net output #0: loss = 0.0741119 (* 1 = 0.0741119 loss)
I0925 03:15:23.984249  4458 sgd_solver.cpp:136] Iteration 13600, lr = 0.01, m = 0.9
I0925 03:16:16.821329  4458 solver.cpp:314] Iteration 13700 (1.89247 iter/s, 52.8409s/100 iter), loss = 0.0528275
I0925 03:16:16.838234  4458 solver.cpp:336]     Train net output #0: loss = 0.0528277 (* 1 = 0.0528277 loss)
I0925 03:16:16.838258  4458 sgd_solver.cpp:136] Iteration 13700, lr = 0.01, m = 0.9
I0925 03:16:25.082258  4408 data_reader.cpp:305] Starting prefetch of epoch 9
I0925 03:17:09.219055  4458 solver.cpp:314] Iteration 13800 (1.90853 iter/s, 52.3963s/100 iter), loss = 0.057132
I0925 03:17:09.225600  4458 solver.cpp:336]     Train net output #0: loss = 0.0571322 (* 1 = 0.0571322 loss)
I0925 03:17:09.225613  4458 sgd_solver.cpp:136] Iteration 13800, lr = 0.01, m = 0.9
I0925 03:18:02.232224  4458 solver.cpp:314] Iteration 13900 (1.88638 iter/s, 53.0117s/100 iter), loss = 0.0615713
I0925 03:18:02.232311  4458 solver.cpp:336]     Train net output #0: loss = 0.0615715 (* 1 = 0.0615715 loss)
I0925 03:18:02.232321  4458 sgd_solver.cpp:136] Iteration 13900, lr = 0.01, m = 0.9
I0925 03:18:53.636216  4458 solver.cpp:423] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.73 sparsity_achieved=0.714088 iter=14000
I0925 03:19:12.226847  4520 data_reader.cpp:305] Starting prefetch of epoch 1
I0925 03:21:04.861763  4458 net.cpp:2253] All zero weights of convolution layers are frozen
I0925 03:21:04.872575  4458 solver.cpp:368] Sparsity after update:
I0925 03:21:04.878127  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 03:21:04.878141  4458 net.cpp:2312] conv1a_param_0(0.236) 
I0925 03:21:04.878150  4458 net.cpp:2312] conv1b_param_0(0.565) 
I0925 03:21:04.878154  4458 net.cpp:2312] ctx_conv1_param_0(0.722) 
I0925 03:21:04.878157  4458 net.cpp:2312] ctx_conv2_param_0(0.725) 
I0925 03:21:04.878160  4458 net.cpp:2312] ctx_conv3_param_0(0.723) 
I0925 03:21:04.878176  4458 net.cpp:2312] ctx_conv4_param_0(0.727) 
I0925 03:21:04.878186  4458 net.cpp:2312] ctx_final_param_0(0.273) 
I0925 03:21:04.878196  4458 net.cpp:2312] out3a_param_0(0.729) 
I0925 03:21:04.878206  4458 net.cpp:2312] out5a_param_0(0.729) 
I0925 03:21:04.878216  4458 net.cpp:2312] res2a_branch2a_param_0(0.691) 
I0925 03:21:04.878224  4458 net.cpp:2312] res2a_branch2b_param_0(0.538) 
I0925 03:21:04.878233  4458 net.cpp:2312] res3a_branch2a_param_0(0.713) 
I0925 03:21:04.878242  4458 net.cpp:2312] res3a_branch2b_param_0(0.657) 
I0925 03:21:04.878252  4458 net.cpp:2312] res4a_branch2a_param_0(0.728) 
I0925 03:21:04.878259  4458 net.cpp:2312] res4a_branch2b_param_0(0.711) 
I0925 03:21:04.878269  4458 net.cpp:2312] res5a_branch2a_param_0(0.728) 
I0925 03:21:04.878278  4458 net.cpp:2312] res5a_branch2b_param_0(0.729) 
I0925 03:21:04.878288  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (1.94751e+06/2.69117e+06) 0.724
I0925 03:21:04.878307  4458 solver.cpp:562] Iteration 14000, Testing net (#0)
I0925 03:21:22.173389  4458 solver.cpp:654]     Test net output #0: accuracy/top1 = 0.952144
I0925 03:21:22.173413  4458 solver.cpp:654]     Test net output #1: accuracy/top5 = 1
I0925 03:21:22.173420  4458 solver.cpp:654]     Test net output #2: loss = 0.155835 (* 1 = 0.155835 loss)
I0925 03:21:22.173450  4458 solver.cpp:265] [MultiGPU] Tests completed in 17.2946s
I0925 03:21:22.687990  4458 solver.cpp:314] Iteration 14000 (0.498877 iter/s, 200.45s/100 iter), loss = 0.0950577
I0925 03:21:22.688021  4458 solver.cpp:336]     Train net output #0: loss = 0.0950579 (* 1 = 0.0950579 loss)
I0925 03:21:22.688029  4458 sgd_solver.cpp:136] Iteration 14000, lr = 0.01, m = 0.9
I0925 03:21:48.152400  4408 data_reader.cpp:305] Starting prefetch of epoch 10
I0925 03:22:18.603883  4458 solver.cpp:314] Iteration 14100 (1.78845 iter/s, 55.9143s/100 iter), loss = 0.0909815
I0925 03:22:18.603992  4458 solver.cpp:336]     Train net output #0: loss = 0.0909817 (* 1 = 0.0909817 loss)
I0925 03:22:18.604012  4458 sgd_solver.cpp:136] Iteration 14100, lr = 0.01, m = 0.9
I0925 03:23:06.052062  4409 blocking_queue.cpp:40] Waiting for datum
I0925 03:23:14.024796  4458 solver.cpp:314] Iteration 14200 (1.80442 iter/s, 55.4194s/100 iter), loss = 0.0576998
I0925 03:23:14.024824  4458 solver.cpp:336]     Train net output #0: loss = 0.0577 (* 1 = 0.0577 loss)
I0925 03:23:14.024829  4458 sgd_solver.cpp:136] Iteration 14200, lr = 0.01, m = 0.9
I0925 03:24:08.490097  4458 solver.cpp:314] Iteration 14300 (1.83608 iter/s, 54.4638s/100 iter), loss = 0.0730513
I0925 03:24:08.490339  4458 solver.cpp:336]     Train net output #0: loss = 0.0730514 (* 1 = 0.0730514 loss)
I0925 03:24:08.490347  4458 sgd_solver.cpp:136] Iteration 14300, lr = 0.01, m = 0.9
I0925 03:24:49.529947  4408 data_reader.cpp:305] Starting prefetch of epoch 11
I0925 03:25:02.360222  4458 solver.cpp:314] Iteration 14400 (1.85637 iter/s, 53.8686s/100 iter), loss = 0.0708511
I0925 03:25:02.360268  4458 solver.cpp:336]     Train net output #0: loss = 0.0708512 (* 1 = 0.0708512 loss)
I0925 03:25:02.360280  4458 sgd_solver.cpp:136] Iteration 14400, lr = 0.01, m = 0.9
I0925 03:25:56.464895  4458 solver.cpp:314] Iteration 14500 (1.84832 iter/s, 54.1032s/100 iter), loss = 0.073586
I0925 03:25:56.464985  4458 solver.cpp:336]     Train net output #0: loss = 0.0735861 (* 1 = 0.0735861 loss)
I0925 03:25:56.464996  4458 sgd_solver.cpp:136] Iteration 14500, lr = 0.01, m = 0.9
I0925 03:26:48.536208  4458 solver.cpp:314] Iteration 14600 (1.9205 iter/s, 52.0699s/100 iter), loss = 0.0713514
I0925 03:26:48.544212  4458 solver.cpp:336]     Train net output #0: loss = 0.0713516 (* 1 = 0.0713516 loss)
I0925 03:26:48.544229  4458 sgd_solver.cpp:136] Iteration 14600, lr = 0.01, m = 0.9
I0925 03:27:43.614231  4458 solver.cpp:314] Iteration 14700 (1.81566 iter/s, 55.0765s/100 iter), loss = 0.0731433
I0925 03:27:43.620211  4458 solver.cpp:336]     Train net output #0: loss = 0.0731435 (* 1 = 0.0731435 loss)
I0925 03:27:43.620229  4458 sgd_solver.cpp:136] Iteration 14700, lr = 0.01, m = 0.9
I0925 03:27:47.080965  4464 data_reader.cpp:305] Starting prefetch of epoch 10
I0925 03:27:47.080965  4410 data_reader.cpp:305] Starting prefetch of epoch 12
I0925 03:28:35.797564  4458 solver.cpp:314] Iteration 14800 (1.91637 iter/s, 52.1819s/100 iter), loss = 0.0572955
I0925 03:28:35.804217  4458 solver.cpp:336]     Train net output #0: loss = 0.0572957 (* 1 = 0.0572957 loss)
I0925 03:28:35.804235  4458 sgd_solver.cpp:136] Iteration 14800, lr = 0.01, m = 0.9
I0925 03:29:28.024852  4458 solver.cpp:314] Iteration 14900 (1.91476 iter/s, 52.2258s/100 iter), loss = 0.0868626
I0925 03:29:28.024911  4458 solver.cpp:336]     Train net output #0: loss = 0.0868628 (* 1 = 0.0868628 loss)
I0925 03:29:28.024917  4458 sgd_solver.cpp:136] Iteration 14900, lr = 0.01, m = 0.9
I0925 03:30:18.908716  4458 solver.cpp:423] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.74 sparsity_achieved=0.723667 iter=15000
I0925 03:32:00.678923  4458 net.cpp:2253] All zero weights of convolution layers are frozen
I0925 03:32:00.721814  4458 solver.cpp:368] Sparsity after update:
I0925 03:32:00.732110  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 03:32:00.732127  4458 net.cpp:2312] conv1a_param_0(0.236) 
I0925 03:32:00.732134  4458 net.cpp:2312] conv1b_param_0(0.568) 
I0925 03:32:00.732137  4458 net.cpp:2312] ctx_conv1_param_0(0.731) 
I0925 03:32:00.732141  4458 net.cpp:2312] ctx_conv2_param_0(0.734) 
I0925 03:32:00.732143  4458 net.cpp:2312] ctx_conv3_param_0(0.732) 
I0925 03:32:00.732146  4458 net.cpp:2312] ctx_conv4_param_0(0.736) 
I0925 03:32:00.732149  4458 net.cpp:2312] ctx_final_param_0(0.272) 
I0925 03:32:00.732151  4458 net.cpp:2312] out3a_param_0(0.74) 
I0925 03:32:00.732154  4458 net.cpp:2312] out5a_param_0(0.739) 
I0925 03:32:00.732157  4458 net.cpp:2312] res2a_branch2a_param_0(0.696) 
I0925 03:32:00.732161  4458 net.cpp:2312] res2a_branch2b_param_0(0.542) 
I0925 03:32:00.732163  4458 net.cpp:2312] res3a_branch2a_param_0(0.72) 
I0925 03:32:00.732168  4458 net.cpp:2312] res3a_branch2b_param_0(0.662) 
I0925 03:32:00.732172  4458 net.cpp:2312] res4a_branch2a_param_0(0.738) 
I0925 03:32:00.732177  4458 net.cpp:2312] res4a_branch2b_param_0(0.718) 
I0925 03:32:00.732182  4458 net.cpp:2312] res5a_branch2a_param_0(0.736) 
I0925 03:32:00.732184  4458 net.cpp:2312] res5a_branch2b_param_0(0.74) 
I0925 03:32:00.732187  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (1.9713e+06/2.69117e+06) 0.733
I0925 03:32:01.412665  4458 solver.cpp:314] Iteration 15000 (0.651961 iter/s, 153.383s/100 iter), loss = 0.055048
I0925 03:32:01.412736  4458 solver.cpp:336]     Train net output #0: loss = 0.0550482 (* 1 = 0.0550482 loss)
I0925 03:32:01.412751  4458 sgd_solver.cpp:136] Iteration 15000, lr = 0.01, m = 0.9
I0925 03:32:21.182512  4410 data_reader.cpp:305] Starting prefetch of epoch 13
I0925 03:32:21.182596  4466 data_reader.cpp:305] Starting prefetch of epoch 4
I0925 03:32:55.498484  4458 solver.cpp:314] Iteration 15100 (1.84897 iter/s, 54.0843s/100 iter), loss = 0.0934869
I0925 03:32:55.498765  4458 solver.cpp:336]     Train net output #0: loss = 0.0934871 (* 1 = 0.0934871 loss)
I0925 03:32:55.498795  4458 sgd_solver.cpp:136] Iteration 15100, lr = 0.01, m = 0.9
I0925 03:33:51.586467  4458 solver.cpp:314] Iteration 15200 (1.78296 iter/s, 56.0864s/100 iter), loss = 0.0855307
I0925 03:33:51.586580  4458 solver.cpp:336]     Train net output #0: loss = 0.0855309 (* 1 = 0.0855309 loss)
I0925 03:33:51.586596  4458 sgd_solver.cpp:136] Iteration 15200, lr = 0.01, m = 0.9
I0925 03:34:49.149747  4458 solver.cpp:314] Iteration 15300 (1.73727 iter/s, 57.5617s/100 iter), loss = 0.0380299
I0925 03:34:49.149842  4458 solver.cpp:336]     Train net output #0: loss = 0.0380301 (* 1 = 0.0380301 loss)
I0925 03:34:49.149852  4458 sgd_solver.cpp:136] Iteration 15300, lr = 0.01, m = 0.9
I0925 03:35:25.837857  4464 data_reader.cpp:305] Starting prefetch of epoch 11
I0925 03:35:42.951481  4458 solver.cpp:314] Iteration 15400 (1.85873 iter/s, 53.8002s/100 iter), loss = 0.0621656
I0925 03:35:42.951542  4458 solver.cpp:336]     Train net output #0: loss = 0.0621658 (* 1 = 0.0621658 loss)
I0925 03:35:42.951563  4458 sgd_solver.cpp:136] Iteration 15400, lr = 0.01, m = 0.9
I0925 03:36:34.809036  4458 solver.cpp:314] Iteration 15500 (1.92841 iter/s, 51.8561s/100 iter), loss = 0.0582683
I0925 03:36:34.824573  4458 solver.cpp:336]     Train net output #0: loss = 0.0582685 (* 1 = 0.0582685 loss)
I0925 03:36:34.824807  4458 sgd_solver.cpp:136] Iteration 15500, lr = 0.01, m = 0.9
I0925 03:36:52.343031  4410 data_reader.cpp:305] Starting prefetch of epoch 14
I0925 03:37:27.780249  4458 solver.cpp:314] Iteration 15600 (1.88787 iter/s, 52.9697s/100 iter), loss = 0.0866777
I0925 03:37:27.784207  4458 solver.cpp:336]     Train net output #0: loss = 0.0866778 (* 1 = 0.0866778 loss)
I0925 03:37:27.784220  4458 sgd_solver.cpp:136] Iteration 15600, lr = 0.01, m = 0.9
I0925 03:38:21.483952  4458 solver.cpp:314] Iteration 15700 (1.86212 iter/s, 53.7022s/100 iter), loss = 0.0838251
I0925 03:38:21.485857  4458 solver.cpp:336]     Train net output #0: loss = 0.0838253 (* 1 = 0.0838253 loss)
I0925 03:38:21.485868  4458 sgd_solver.cpp:136] Iteration 15700, lr = 0.01, m = 0.9
I0925 03:39:14.834507  4458 solver.cpp:314] Iteration 15800 (1.87445 iter/s, 53.349s/100 iter), loss = 0.0811482
I0925 03:39:14.834591  4458 solver.cpp:336]     Train net output #0: loss = 0.0811483 (* 1 = 0.0811483 loss)
I0925 03:39:14.834602  4458 sgd_solver.cpp:136] Iteration 15800, lr = 0.01, m = 0.9
I0925 03:39:48.956790  4464 data_reader.cpp:305] Starting prefetch of epoch 12
I0925 03:40:08.248642  4458 solver.cpp:314] Iteration 15900 (1.87222 iter/s, 53.4126s/100 iter), loss = 0.0495006
I0925 03:40:08.248683  4458 solver.cpp:336]     Train net output #0: loss = 0.0495008 (* 1 = 0.0495008 loss)
I0925 03:40:08.248689  4458 sgd_solver.cpp:136] Iteration 15900, lr = 0.01, m = 0.9
I0925 03:41:01.055651  4458 solver.cpp:423] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.75 sparsity_achieved=0.732507 iter=16000
I0925 03:41:30.067255  4518 data_reader.cpp:305] Starting prefetch of epoch 4
I0925 03:42:52.600458  4458 net.cpp:2253] All zero weights of convolution layers are frozen
I0925 03:42:52.633038  4458 solver.cpp:368] Sparsity after update:
I0925 03:42:52.635597  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 03:42:52.635637  4458 net.cpp:2312] conv1a_param_0(0.241) 
I0925 03:42:52.635655  4458 net.cpp:2312] conv1b_param_0(0.57) 
I0925 03:42:52.635668  4458 net.cpp:2312] ctx_conv1_param_0(0.738) 
I0925 03:42:52.635680  4458 net.cpp:2312] ctx_conv2_param_0(0.742) 
I0925 03:42:52.635694  4458 net.cpp:2312] ctx_conv3_param_0(0.739) 
I0925 03:42:52.635705  4458 net.cpp:2312] ctx_conv4_param_0(0.744) 
I0925 03:42:52.635717  4458 net.cpp:2312] ctx_final_param_0(0.259) 
I0925 03:42:52.635735  4458 net.cpp:2312] out3a_param_0(0.748) 
I0925 03:42:52.635747  4458 net.cpp:2312] out5a_param_0(0.749) 
I0925 03:42:52.635759  4458 net.cpp:2312] res2a_branch2a_param_0(0.7) 
I0925 03:42:52.635772  4458 net.cpp:2312] res2a_branch2b_param_0(0.545) 
I0925 03:42:52.635785  4458 net.cpp:2312] res3a_branch2a_param_0(0.726) 
I0925 03:42:52.635797  4458 net.cpp:2312] res3a_branch2b_param_0(0.666) 
I0925 03:42:52.635808  4458 net.cpp:2312] res4a_branch2a_param_0(0.747) 
I0925 03:42:52.635819  4458 net.cpp:2312] res4a_branch2b_param_0(0.724) 
I0925 03:42:52.635831  4458 net.cpp:2312] res5a_branch2a_param_0(0.748) 
I0925 03:42:52.635841  4458 net.cpp:2312] res5a_branch2b_param_0(0.749) 
I0925 03:42:52.635851  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (1.99746e+06/2.69117e+06) 0.742
I0925 03:42:52.635872  4458 solver.cpp:562] Iteration 16000, Testing net (#0)
I0925 03:43:16.287981  4458 solver.cpp:654]     Test net output #0: accuracy/top1 = 0.945594
I0925 03:43:16.288007  4458 solver.cpp:654]     Test net output #1: accuracy/top5 = 1
I0925 03:43:16.288012  4458 solver.cpp:654]     Test net output #2: loss = 0.150577 (* 1 = 0.150577 loss)
I0925 03:43:16.288036  4458 solver.cpp:265] [MultiGPU] Tests completed in 23.6515s
I0925 03:43:16.871901  4458 solver.cpp:314] Iteration 16000 (0.530172 iter/s, 188.618s/100 iter), loss = 0.0801991
I0925 03:43:16.871930  4458 solver.cpp:336]     Train net output #0: loss = 0.0801993 (* 1 = 0.0801993 loss)
I0925 03:43:16.871937  4458 sgd_solver.cpp:136] Iteration 16000, lr = 0.01, m = 0.9
I0925 03:43:34.715345  4461 data_reader.cpp:305] Starting prefetch of epoch 11
I0925 03:44:14.320926  4458 solver.cpp:314] Iteration 16100 (1.74072 iter/s, 57.4474s/100 iter), loss = 0.0710361
I0925 03:44:14.321017  4458 solver.cpp:336]     Train net output #0: loss = 0.0710362 (* 1 = 0.0710362 loss)
I0925 03:44:14.321028  4458 sgd_solver.cpp:136] Iteration 16100, lr = 0.01, m = 0.9
I0925 03:45:12.696465  4458 solver.cpp:314] Iteration 16200 (1.71309 iter/s, 58.3739s/100 iter), loss = 0.095312
I0925 03:45:12.698381  4458 solver.cpp:336]     Train net output #0: loss = 0.0953122 (* 1 = 0.0953122 loss)
I0925 03:45:12.698390  4458 sgd_solver.cpp:136] Iteration 16200, lr = 0.01, m = 0.9
I0925 03:46:05.908228  4458 solver.cpp:314] Iteration 16300 (1.87934 iter/s, 53.2103s/100 iter), loss = 0.0595437
I0925 03:46:05.908305  4458 solver.cpp:336]     Train net output #0: loss = 0.0595438 (* 1 = 0.0595438 loss)
I0925 03:46:05.908314  4458 sgd_solver.cpp:136] Iteration 16300, lr = 0.01, m = 0.9
I0925 03:46:08.369377  4465 blocking_queue.cpp:40] Waiting for datum
I0925 03:46:43.197880  4408 data_reader.cpp:305] Starting prefetch of epoch 12
I0925 03:47:07.149639  4458 solver.cpp:314] Iteration 16400 (1.63293 iter/s, 61.2397s/100 iter), loss = 0.0456374
I0925 03:47:07.149665  4458 solver.cpp:336]     Train net output #0: loss = 0.0456375 (* 1 = 0.0456375 loss)
I0925 03:47:07.149669  4458 sgd_solver.cpp:136] Iteration 16400, lr = 0.01, m = 0.9
I0925 03:48:03.037286  4458 solver.cpp:314] Iteration 16500 (1.78935 iter/s, 55.8861s/100 iter), loss = 0.054994
I0925 03:48:03.037360  4458 solver.cpp:336]     Train net output #0: loss = 0.0549941 (* 1 = 0.0549941 loss)
I0925 03:48:03.037367  4458 sgd_solver.cpp:136] Iteration 16500, lr = 0.01, m = 0.9
I0925 03:48:56.202253  4458 solver.cpp:314] Iteration 16600 (1.88099 iter/s, 53.1635s/100 iter), loss = 0.0641923
I0925 03:48:56.202327  4458 solver.cpp:336]     Train net output #0: loss = 0.0641924 (* 1 = 0.0641924 loss)
I0925 03:48:56.202337  4458 sgd_solver.cpp:136] Iteration 16600, lr = 0.01, m = 0.9
I0925 03:49:44.119668  4464 data_reader.cpp:305] Starting prefetch of epoch 13
I0925 03:49:49.046010  4458 solver.cpp:314] Iteration 16700 (1.89242 iter/s, 52.8423s/100 iter), loss = 0.0830133
I0925 03:49:49.046037  4458 solver.cpp:336]     Train net output #0: loss = 0.0830134 (* 1 = 0.0830134 loss)
I0925 03:49:49.046043  4458 sgd_solver.cpp:136] Iteration 16700, lr = 0.01, m = 0.9
I0925 03:50:42.162503  4458 solver.cpp:314] Iteration 16800 (1.88271 iter/s, 53.115s/100 iter), loss = 0.0759738
I0925 03:50:42.172260  4458 solver.cpp:336]     Train net output #0: loss = 0.075974 (* 1 = 0.075974 loss)
I0925 03:50:42.172282  4458 sgd_solver.cpp:136] Iteration 16800, lr = 0.01, m = 0.9
I0925 03:51:11.828799  4462 data_reader.cpp:305] Starting prefetch of epoch 5
I0925 03:51:34.696633  4458 solver.cpp:314] Iteration 16900 (1.90358 iter/s, 52.5326s/100 iter), loss = 0.063046
I0925 03:51:34.696765  4458 solver.cpp:336]     Train net output #0: loss = 0.0630461 (* 1 = 0.0630461 loss)
I0925 03:51:34.696781  4458 sgd_solver.cpp:136] Iteration 16900, lr = 0.01, m = 0.9
I0925 03:52:22.758774  4458 solver.cpp:423] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.76 sparsity_achieved=0.742226 iter=17000
I0925 03:54:26.642088  4458 net.cpp:2253] All zero weights of convolution layers are frozen
I0925 03:54:26.671834  4458 solver.cpp:368] Sparsity after update:
I0925 03:54:26.680274  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 03:54:26.680325  4458 net.cpp:2312] conv1a_param_0(0.242) 
I0925 03:54:26.680342  4458 net.cpp:2312] conv1b_param_0(0.572) 
I0925 03:54:26.680354  4458 net.cpp:2312] ctx_conv1_param_0(0.745) 
I0925 03:54:26.680366  4458 net.cpp:2312] ctx_conv2_param_0(0.75) 
I0925 03:54:26.680377  4458 net.cpp:2312] ctx_conv3_param_0(0.747) 
I0925 03:54:26.680388  4458 net.cpp:2312] ctx_conv4_param_0(0.753) 
I0925 03:54:26.680398  4458 net.cpp:2312] ctx_final_param_0(0.323) 
I0925 03:54:26.680410  4458 net.cpp:2312] out3a_param_0(0.759) 
I0925 03:54:26.680423  4458 net.cpp:2312] out5a_param_0(0.76) 
I0925 03:54:26.680434  4458 net.cpp:2312] res2a_branch2a_param_0(0.706) 
I0925 03:54:26.680445  4458 net.cpp:2312] res2a_branch2b_param_0(0.548) 
I0925 03:54:26.680455  4458 net.cpp:2312] res3a_branch2a_param_0(0.732) 
I0925 03:54:26.680465  4458 net.cpp:2312] res3a_branch2b_param_0(0.67) 
I0925 03:54:26.680475  4458 net.cpp:2312] res4a_branch2a_param_0(0.756) 
I0925 03:54:26.680485  4458 net.cpp:2312] res4a_branch2b_param_0(0.731) 
I0925 03:54:26.680495  4458 net.cpp:2312] res5a_branch2a_param_0(0.757) 
I0925 03:54:26.680505  4458 net.cpp:2312] res5a_branch2b_param_0(0.76) 
I0925 03:54:26.680516  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.02226e+06/2.69117e+06) 0.751
I0925 03:54:27.363572  4458 solver.cpp:314] Iteration 17000 (0.579166 iter/s, 172.662s/100 iter), loss = 0.126524
I0925 03:54:27.363600  4458 solver.cpp:336]     Train net output #0: loss = 0.126525 (* 1 = 0.126525 loss)
I0925 03:54:27.363605  4458 sgd_solver.cpp:136] Iteration 17000, lr = 0.01, m = 0.9
I0925 03:55:24.302366  4458 solver.cpp:314] Iteration 17100 (1.75632 iter/s, 56.9372s/100 iter), loss = 0.0684132
I0925 03:55:24.302433  4458 solver.cpp:336]     Train net output #0: loss = 0.0684133 (* 1 = 0.0684133 loss)
I0925 03:55:24.302440  4458 sgd_solver.cpp:136] Iteration 17100, lr = 0.01, m = 0.9
I0925 03:56:18.548418  4461 data_reader.cpp:305] Starting prefetch of epoch 12
I0925 03:56:18.548418  4462 data_reader.cpp:305] Starting prefetch of epoch 6
I0925 03:56:26.390565  4458 solver.cpp:314] Iteration 17200 (1.61066 iter/s, 62.0865s/100 iter), loss = 0.0776955
I0925 03:56:26.390594  4458 solver.cpp:336]     Train net output #0: loss = 0.0776956 (* 1 = 0.0776956 loss)
I0925 03:56:26.390599  4458 sgd_solver.cpp:136] Iteration 17200, lr = 0.01, m = 0.9
I0925 03:57:25.645421  4458 solver.cpp:314] Iteration 17300 (1.68767 iter/s, 59.2532s/100 iter), loss = 0.0530632
I0925 03:57:25.645509  4458 solver.cpp:336]     Train net output #0: loss = 0.0530633 (* 1 = 0.0530633 loss)
I0925 03:57:25.645514  4458 sgd_solver.cpp:136] Iteration 17300, lr = 0.01, m = 0.9
I0925 03:58:18.705844  4458 solver.cpp:314] Iteration 17400 (1.8847 iter/s, 53.0589s/100 iter), loss = 0.0468746
I0925 03:58:18.705934  4458 solver.cpp:336]     Train net output #0: loss = 0.0468748 (* 1 = 0.0468748 loss)
I0925 03:58:18.705941  4458 sgd_solver.cpp:136] Iteration 17400, lr = 0.01, m = 0.9
I0925 03:59:10.943711  4458 solver.cpp:314] Iteration 17500 (1.91437 iter/s, 52.2364s/100 iter), loss = 0.0478384
I0925 03:59:10.946843  4458 solver.cpp:336]     Train net output #0: loss = 0.0478385 (* 1 = 0.0478385 loss)
I0925 03:59:10.946884  4458 sgd_solver.cpp:136] Iteration 17500, lr = 0.01, m = 0.9
I0925 03:59:19.597555  4464 data_reader.cpp:305] Starting prefetch of epoch 14
I0925 04:00:03.878891  4458 solver.cpp:314] Iteration 17600 (1.88916 iter/s, 52.9337s/100 iter), loss = 0.0718924
I0925 04:00:03.880959  4458 solver.cpp:336]     Train net output #0: loss = 0.0718925 (* 1 = 0.0718925 loss)
I0925 04:00:03.880970  4458 sgd_solver.cpp:136] Iteration 17600, lr = 0.01, m = 0.9
I0925 04:00:57.914044  4458 solver.cpp:314] Iteration 17700 (1.8507 iter/s, 54.0336s/100 iter), loss = 0.0621454
I0925 04:00:57.914132  4458 solver.cpp:336]     Train net output #0: loss = 0.0621455 (* 1 = 0.0621455 loss)
I0925 04:00:57.914139  4458 sgd_solver.cpp:136] Iteration 17700, lr = 0.01, m = 0.9
I0925 04:01:48.285727  4458 solver.cpp:314] Iteration 17800 (1.9853 iter/s, 50.3703s/100 iter), loss = 0.0578982
I0925 04:01:48.285782  4458 solver.cpp:336]     Train net output #0: loss = 0.0578983 (* 1 = 0.0578983 loss)
I0925 04:01:48.285787  4458 sgd_solver.cpp:136] Iteration 17800, lr = 0.01, m = 0.9
I0925 04:02:09.960747  4464 data_reader.cpp:305] Starting prefetch of epoch 15
I0925 04:02:35.546569  4458 solver.cpp:314] Iteration 17900 (2.11598 iter/s, 47.2595s/100 iter), loss = 0.0777225
I0925 04:02:35.546643  4458 solver.cpp:336]     Train net output #0: loss = 0.0777226 (* 1 = 0.0777226 loss)
I0925 04:02:35.546654  4458 sgd_solver.cpp:136] Iteration 17900, lr = 0.01, m = 0.9
I0925 04:03:27.256007  4458 solver.cpp:423] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.77 sparsity_achieved=0.751444 iter=18000
I0925 04:05:59.212146  4458 net.cpp:2253] All zero weights of convolution layers are frozen
I0925 04:05:59.246038  4458 solver.cpp:368] Sparsity after update:
I0925 04:05:59.267421  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 04:05:59.267444  4458 net.cpp:2312] conv1a_param_0(0.242) 
I0925 04:05:59.267453  4458 net.cpp:2312] conv1b_param_0(0.574) 
I0925 04:05:59.267457  4458 net.cpp:2312] ctx_conv1_param_0(0.753) 
I0925 04:05:59.267459  4458 net.cpp:2312] ctx_conv2_param_0(0.759) 
I0925 04:05:59.267462  4458 net.cpp:2312] ctx_conv3_param_0(0.754) 
I0925 04:05:59.267465  4458 net.cpp:2312] ctx_conv4_param_0(0.761) 
I0925 04:05:59.267468  4458 net.cpp:2312] ctx_final_param_0(0.24) 
I0925 04:05:59.267472  4458 net.cpp:2312] out3a_param_0(0.769) 
I0925 04:05:59.267474  4458 net.cpp:2312] out5a_param_0(0.77) 
I0925 04:05:59.267478  4458 net.cpp:2312] res2a_branch2a_param_0(0.71) 
I0925 04:05:59.267493  4458 net.cpp:2312] res2a_branch2b_param_0(0.551) 
I0925 04:05:59.267504  4458 net.cpp:2312] res3a_branch2a_param_0(0.738) 
I0925 04:05:59.267515  4458 net.cpp:2312] res3a_branch2b_param_0(0.675) 
I0925 04:05:59.267525  4458 net.cpp:2312] res4a_branch2a_param_0(0.765) 
I0925 04:05:59.267535  4458 net.cpp:2312] res4a_branch2b_param_0(0.737) 
I0925 04:05:59.267545  4458 net.cpp:2312] res5a_branch2a_param_0(0.768) 
I0925 04:05:59.267555  4458 net.cpp:2312] res5a_branch2b_param_0(0.77) 
I0925 04:05:59.267566  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.04824e+06/2.69117e+06) 0.761
I0925 04:05:59.267587  4458 solver.cpp:562] Iteration 18000, Testing net (#0)
I0925 04:06:21.751009  4458 solver.cpp:654]     Test net output #0: accuracy/top1 = 0.948851
I0925 04:06:21.751031  4458 solver.cpp:654]     Test net output #1: accuracy/top5 = 1
I0925 04:06:21.751039  4458 solver.cpp:654]     Test net output #2: loss = 0.169165 (* 1 = 0.169165 loss)
I0925 04:06:21.804209  4458 solver.cpp:265] [MultiGPU] Tests completed in 22.536s
I0925 04:06:22.276430  4458 solver.cpp:314] Iteration 18000 (0.441066 iter/s, 226.723s/100 iter), loss = 0.0599625
I0925 04:06:22.276463  4458 solver.cpp:336]     Train net output #0: loss = 0.0599626 (* 1 = 0.0599626 loss)
I0925 04:06:22.276469  4458 sgd_solver.cpp:136] Iteration 18000, lr = 0.01, m = 0.9
I0925 04:06:30.278113  4466 data_reader.cpp:305] Starting prefetch of epoch 5
I0925 04:07:25.227077  4458 solver.cpp:314] Iteration 18100 (1.58859 iter/s, 62.9489s/100 iter), loss = 0.0547708
I0925 04:07:25.227134  4458 solver.cpp:336]     Train net output #0: loss = 0.0547709 (* 1 = 0.0547709 loss)
I0925 04:07:25.227139  4458 sgd_solver.cpp:136] Iteration 18100, lr = 0.01, m = 0.9
I0925 04:08:13.863528  4461 data_reader.cpp:305] Starting prefetch of epoch 13
I0925 04:08:20.537405  4465 blocking_queue.cpp:40] Waiting for datum
I0925 04:08:25.436094  4458 solver.cpp:314] Iteration 18200 (1.66093 iter/s, 60.2073s/100 iter), loss = 0.0650525
I0925 04:08:25.436125  4458 solver.cpp:336]     Train net output #0: loss = 0.0650526 (* 1 = 0.0650526 loss)
I0925 04:08:25.436130  4458 sgd_solver.cpp:136] Iteration 18200, lr = 0.01, m = 0.9
I0925 04:09:19.636214  4458 solver.cpp:314] Iteration 18300 (1.84507 iter/s, 54.1986s/100 iter), loss = 0.0747349
I0925 04:09:19.643967  4458 solver.cpp:336]     Train net output #0: loss = 0.0747349 (* 1 = 0.0747349 loss)
I0925 04:09:19.643981  4458 sgd_solver.cpp:136] Iteration 18300, lr = 0.01, m = 0.9
I0925 04:10:17.591169  4458 solver.cpp:314] Iteration 18400 (1.72553 iter/s, 57.9533s/100 iter), loss = 0.0592982
I0925 04:10:17.591343  4458 solver.cpp:336]     Train net output #0: loss = 0.0592983 (* 1 = 0.0592983 loss)
I0925 04:10:17.591347  4458 sgd_solver.cpp:136] Iteration 18400, lr = 0.01, m = 0.9
I0925 04:11:17.378480  4458 solver.cpp:314] Iteration 18500 (1.67264 iter/s, 59.7856s/100 iter), loss = 0.0669802
I0925 04:11:17.378556  4458 solver.cpp:336]     Train net output #0: loss = 0.0669803 (* 1 = 0.0669803 loss)
I0925 04:11:17.378566  4458 sgd_solver.cpp:136] Iteration 18500, lr = 0.01, m = 0.9
I0925 04:11:21.936282  4461 data_reader.cpp:305] Starting prefetch of epoch 14
I0925 04:12:09.557762  4458 solver.cpp:314] Iteration 18600 (1.91652 iter/s, 52.1778s/100 iter), loss = 0.0714353
I0925 04:12:09.557822  4458 solver.cpp:336]     Train net output #0: loss = 0.0714353 (* 1 = 0.0714353 loss)
I0925 04:12:09.557827  4458 sgd_solver.cpp:136] Iteration 18600, lr = 0.01, m = 0.9
I0925 04:13:01.914988  4458 solver.cpp:314] Iteration 18700 (1.91001 iter/s, 52.3558s/100 iter), loss = 0.0444046
I0925 04:13:01.915047  4458 solver.cpp:336]     Train net output #0: loss = 0.0444047 (* 1 = 0.0444047 loss)
I0925 04:13:01.915055  4458 sgd_solver.cpp:136] Iteration 18700, lr = 0.01, m = 0.9
I0925 04:13:55.529672  4458 solver.cpp:314] Iteration 18800 (1.86521 iter/s, 53.6132s/100 iter), loss = 0.198595
I0925 04:13:55.540235  4458 solver.cpp:336]     Train net output #0: loss = 0.198595 (* 1 = 0.198595 loss)
I0925 04:13:55.540287  4458 sgd_solver.cpp:136] Iteration 18800, lr = 0.01, m = 0.9
I0925 04:14:15.736884  4410 data_reader.cpp:305] Starting prefetch of epoch 15
I0925 04:14:48.506714  4458 solver.cpp:314] Iteration 18900 (1.88766 iter/s, 52.9756s/100 iter), loss = 0.0667291
I0925 04:14:48.514240  4458 solver.cpp:336]     Train net output #0: loss = 0.0667292 (* 1 = 0.0667292 loss)
I0925 04:14:48.514261  4458 sgd_solver.cpp:136] Iteration 18900, lr = 0.01, m = 0.9
I0925 04:15:41.421049  4458 solver.cpp:423] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.78 sparsity_achieved=0.761097 iter=19000
I0925 04:18:32.869469  4458 net.cpp:2253] All zero weights of convolution layers are frozen
I0925 04:18:32.910153  4458 solver.cpp:368] Sparsity after update:
I0925 04:18:32.918751  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 04:18:32.918768  4458 net.cpp:2312] conv1a_param_0(0.248) 
I0925 04:18:32.918777  4458 net.cpp:2312] conv1b_param_0(0.578) 
I0925 04:18:32.918781  4458 net.cpp:2312] ctx_conv1_param_0(0.76) 
I0925 04:18:32.918783  4458 net.cpp:2312] ctx_conv2_param_0(0.767) 
I0925 04:18:32.918787  4458 net.cpp:2312] ctx_conv3_param_0(0.761) 
I0925 04:18:32.918789  4458 net.cpp:2312] ctx_conv4_param_0(0.769) 
I0925 04:18:32.918793  4458 net.cpp:2312] ctx_final_param_0(0.312) 
I0925 04:18:32.918809  4458 net.cpp:2312] out3a_param_0(0.78) 
I0925 04:18:32.918819  4458 net.cpp:2312] out5a_param_0(0.779) 
I0925 04:18:32.918840  4458 net.cpp:2312] res2a_branch2a_param_0(0.715) 
I0925 04:18:32.918850  4458 net.cpp:2312] res2a_branch2b_param_0(0.554) 
I0925 04:18:32.918859  4458 net.cpp:2312] res3a_branch2a_param_0(0.744) 
I0925 04:18:32.918869  4458 net.cpp:2312] res3a_branch2b_param_0(0.679) 
I0925 04:18:32.918877  4458 net.cpp:2312] res4a_branch2a_param_0(0.772) 
I0925 04:18:32.918886  4458 net.cpp:2312] res4a_branch2b_param_0(0.743) 
I0925 04:18:32.918895  4458 net.cpp:2312] res5a_branch2a_param_0(0.777) 
I0925 04:18:32.918903  4458 net.cpp:2312] res5a_branch2b_param_0(0.78) 
I0925 04:18:32.918912  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.07122e+06/2.69117e+06) 0.77
I0925 04:18:33.747488  4458 solver.cpp:314] Iteration 19000 (0.443982 iter/s, 225.234s/100 iter), loss = 0.128052
I0925 04:18:33.747524  4458 solver.cpp:336]     Train net output #0: loss = 0.128052 (* 1 = 0.128052 loss)
I0925 04:18:33.747530  4458 sgd_solver.cpp:136] Iteration 19000, lr = 0.01, m = 0.9
I0925 04:18:35.920965  4461 data_reader.cpp:305] Starting prefetch of epoch 15
I0925 04:19:28.928925  4458 solver.cpp:314] Iteration 19100 (1.81225 iter/s, 55.1799s/100 iter), loss = 0.0527915
I0925 04:19:28.929013  4458 solver.cpp:336]     Train net output #0: loss = 0.0527916 (* 1 = 0.0527916 loss)
I0925 04:19:28.929023  4458 sgd_solver.cpp:136] Iteration 19100, lr = 0.01, m = 0.9
I0925 04:20:31.714794  4458 solver.cpp:314] Iteration 19200 (1.59276 iter/s, 62.7841s/100 iter), loss = 0.0913219
I0925 04:20:31.714864  4458 solver.cpp:336]     Train net output #0: loss = 0.091322 (* 1 = 0.091322 loss)
I0925 04:20:31.714874  4458 sgd_solver.cpp:136] Iteration 19200, lr = 0.01, m = 0.9
I0925 04:21:28.911391  4458 solver.cpp:314] Iteration 19300 (1.74841 iter/s, 57.195s/100 iter), loss = 0.0833894
I0925 04:21:28.921526  4458 solver.cpp:336]     Train net output #0: loss = 0.0833895 (* 1 = 0.0833895 loss)
I0925 04:21:28.921563  4458 sgd_solver.cpp:136] Iteration 19300, lr = 0.01, m = 0.9
I0925 04:21:47.106263  4408 data_reader.cpp:305] Starting prefetch of epoch 13
I0925 04:22:21.259088  4458 solver.cpp:314] Iteration 19400 (1.91036 iter/s, 52.3462s/100 iter), loss = 0.0958522
I0925 04:22:21.259147  4458 solver.cpp:336]     Train net output #0: loss = 0.0958522 (* 1 = 0.0958522 loss)
I0925 04:22:21.259276  4458 sgd_solver.cpp:136] Iteration 19400, lr = 0.01, m = 0.9
I0925 04:23:14.815443  4458 solver.cpp:314] Iteration 19500 (1.86724 iter/s, 53.5548s/100 iter), loss = 0.0702059
I0925 04:23:14.815513  4458 solver.cpp:336]     Train net output #0: loss = 0.070206 (* 1 = 0.070206 loss)
I0925 04:23:14.815520  4458 sgd_solver.cpp:136] Iteration 19500, lr = 0.01, m = 0.9
I0925 04:24:07.513202  4458 solver.cpp:314] Iteration 19600 (1.89767 iter/s, 52.6963s/100 iter), loss = 0.0729467
I0925 04:24:07.513285  4458 solver.cpp:336]     Train net output #0: loss = 0.0729468 (* 1 = 0.0729468 loss)
I0925 04:24:07.513295  4458 sgd_solver.cpp:136] Iteration 19600, lr = 0.01, m = 0.9
I0925 04:24:42.370354  4464 data_reader.cpp:305] Starting prefetch of epoch 16
I0925 04:25:00.664216  4458 solver.cpp:314] Iteration 19700 (1.88148 iter/s, 53.1495s/100 iter), loss = 0.470903
I0925 04:25:00.664289  4458 solver.cpp:336]     Train net output #0: loss = 0.470903 (* 1 = 0.470903 loss)
I0925 04:25:00.664306  4458 sgd_solver.cpp:136] Iteration 19700, lr = 0.01, m = 0.9
I0925 04:25:52.747148  4458 solver.cpp:314] Iteration 19800 (1.92007 iter/s, 52.0815s/100 iter), loss = 0.0521811
I0925 04:25:52.747232  4458 solver.cpp:336]     Train net output #0: loss = 0.0521812 (* 1 = 0.0521812 loss)
I0925 04:25:52.747241  4458 sgd_solver.cpp:136] Iteration 19800, lr = 0.01, m = 0.9
I0925 04:26:45.141330  4458 solver.cpp:314] Iteration 19900 (1.90866 iter/s, 52.3927s/100 iter), loss = 0.062031
I0925 04:26:45.141407  4458 solver.cpp:336]     Train net output #0: loss = 0.062031 (* 1 = 0.062031 loss)
I0925 04:26:45.141422  4458 sgd_solver.cpp:136] Iteration 19900, lr = 0.01, m = 0.9
I0925 04:27:35.894201  4410 data_reader.cpp:305] Starting prefetch of epoch 16
I0925 04:27:37.664620  4458 solver.cpp:824] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/cityscapes5_jsegnet21v2_iter_20000.caffemodel
I0925 04:27:38.176925  4458 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/cityscapes5_jsegnet21v2_iter_20000.solverstate
I0925 04:27:38.195551  4458 solver.cpp:423] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.79 sparsity_achieved=0.769635 iter=20000
I0925 04:28:04.843148  4524 data_reader.cpp:305] Starting prefetch of epoch 2
I0925 04:29:42.698611  4458 net.cpp:2253] All zero weights of convolution layers are frozen
I0925 04:29:42.709883  4458 solver.cpp:368] Sparsity after update:
I0925 04:29:42.715443  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 04:29:42.715461  4458 net.cpp:2312] conv1a_param_0(0.249) 
I0925 04:29:42.715471  4458 net.cpp:2312] conv1b_param_0(0.58) 
I0925 04:29:42.715474  4458 net.cpp:2312] ctx_conv1_param_0(0.766) 
I0925 04:29:42.715477  4458 net.cpp:2312] ctx_conv2_param_0(0.775) 
I0925 04:29:42.715481  4458 net.cpp:2312] ctx_conv3_param_0(0.768) 
I0925 04:29:42.715483  4458 net.cpp:2312] ctx_conv4_param_0(0.778) 
I0925 04:29:42.715487  4458 net.cpp:2312] ctx_final_param_0(0.344) 
I0925 04:29:42.715492  4458 net.cpp:2312] out3a_param_0(0.79) 
I0925 04:29:42.715494  4458 net.cpp:2312] out5a_param_0(0.789) 
I0925 04:29:42.715498  4458 net.cpp:2312] res2a_branch2a_param_0(0.72) 
I0925 04:29:42.715504  4458 net.cpp:2312] res2a_branch2b_param_0(0.557) 
I0925 04:29:42.715508  4458 net.cpp:2312] res3a_branch2a_param_0(0.748) 
I0925 04:29:42.715513  4458 net.cpp:2312] res3a_branch2b_param_0(0.683) 
I0925 04:29:42.715518  4458 net.cpp:2312] res4a_branch2a_param_0(0.78) 
I0925 04:29:42.715523  4458 net.cpp:2312] res4a_branch2b_param_0(0.748) 
I0925 04:29:42.715528  4458 net.cpp:2312] res5a_branch2a_param_0(0.788) 
I0925 04:29:42.715533  4458 net.cpp:2312] res5a_branch2b_param_0(0.79) 
I0925 04:29:42.715538  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.0972e+06/2.69117e+06) 0.779
I0925 04:29:42.715553  4458 solver.cpp:562] Iteration 20000, Testing net (#0)
I0925 04:30:05.531168  4458 solver.cpp:654]     Test net output #0: accuracy/top1 = 0.951439
I0925 04:30:05.531194  4458 solver.cpp:654]     Test net output #1: accuracy/top5 = 1
I0925 04:30:05.531201  4458 solver.cpp:654]     Test net output #2: loss = 0.146896 (* 1 = 0.146896 loss)
I0925 04:30:05.531230  4458 solver.cpp:265] [MultiGPU] Tests completed in 22.815s
I0925 04:30:06.038658  4458 solver.cpp:314] Iteration 20000 (0.497781 iter/s, 200.892s/100 iter), loss = 0.0462483
I0925 04:30:06.038691  4458 solver.cpp:336]     Train net output #0: loss = 0.0462483 (* 1 = 0.0462483 loss)
I0925 04:30:06.038697  4458 sgd_solver.cpp:136] Iteration 20000, lr = 0.01, m = 0.9
I0925 04:31:06.747704  4458 solver.cpp:314] Iteration 20100 (1.64725 iter/s, 60.7073s/100 iter), loss = 0.0494026
I0925 04:31:06.747773  4458 solver.cpp:336]     Train net output #0: loss = 0.0494026 (* 1 = 0.0494026 loss)
I0925 04:31:06.747781  4458 sgd_solver.cpp:136] Iteration 20100, lr = 0.01, m = 0.9
I0925 04:31:42.503666  4464 data_reader.cpp:305] Starting prefetch of epoch 17
I0925 04:31:58.413417  4465 blocking_queue.cpp:40] Waiting for datum
I0925 04:32:04.877332  4458 solver.cpp:314] Iteration 20200 (1.72034 iter/s, 58.128s/100 iter), loss = 0.0820023
I0925 04:32:04.877394  4458 solver.cpp:336]     Train net output #0: loss = 0.0820023 (* 1 = 0.0820023 loss)
I0925 04:32:04.877413  4458 sgd_solver.cpp:136] Iteration 20200, lr = 0.01, m = 0.9
I0925 04:32:59.652231  4458 solver.cpp:314] Iteration 20300 (1.82571 iter/s, 54.7734s/100 iter), loss = 0.0652416
I0925 04:32:59.660285  4458 solver.cpp:336]     Train net output #0: loss = 0.0652417 (* 1 = 0.0652417 loss)
I0925 04:32:59.660317  4458 sgd_solver.cpp:136] Iteration 20300, lr = 0.01, m = 0.9
I0925 04:33:52.669078  4458 solver.cpp:314] Iteration 20400 (1.88625 iter/s, 53.0154s/100 iter), loss = 0.0649053
I0925 04:33:52.669149  4458 solver.cpp:336]     Train net output #0: loss = 0.0649053 (* 1 = 0.0649053 loss)
I0925 04:33:52.669157  4458 sgd_solver.cpp:136] Iteration 20400, lr = 0.01, m = 0.9
I0925 04:34:45.348021  4461 data_reader.cpp:305] Starting prefetch of epoch 16
I0925 04:34:49.219339  4458 solver.cpp:314] Iteration 20500 (1.76839 iter/s, 56.5487s/100 iter), loss = 0.0561024
I0925 04:34:49.219363  4458 solver.cpp:336]     Train net output #0: loss = 0.0561024 (* 1 = 0.0561024 loss)
I0925 04:34:49.219370  4458 sgd_solver.cpp:136] Iteration 20500, lr = 0.01, m = 0.9
I0925 04:35:42.433217  4458 solver.cpp:314] Iteration 20600 (1.87926 iter/s, 53.2124s/100 iter), loss = 0.0546496
I0925 04:35:42.440258  4458 solver.cpp:336]     Train net output #0: loss = 0.0546496 (* 1 = 0.0546496 loss)
I0925 04:35:42.440290  4458 sgd_solver.cpp:136] Iteration 20600, lr = 0.01, m = 0.9
I0925 04:36:35.608098  4458 solver.cpp:314] Iteration 20700 (1.88064 iter/s, 53.1734s/100 iter), loss = 0.0524297
I0925 04:36:35.608180  4458 solver.cpp:336]     Train net output #0: loss = 0.0524297 (* 1 = 0.0524297 loss)
I0925 04:36:35.608189  4458 sgd_solver.cpp:136] Iteration 20700, lr = 0.01, m = 0.9
I0925 04:37:28.484038  4458 solver.cpp:314] Iteration 20800 (1.89127 iter/s, 52.8745s/100 iter), loss = 0.039546
I0925 04:37:28.488229  4458 solver.cpp:336]     Train net output #0: loss = 0.0395461 (* 1 = 0.0395461 loss)
I0925 04:37:28.488248  4458 sgd_solver.cpp:136] Iteration 20800, lr = 0.01, m = 0.9
I0925 04:37:39.818766  4410 data_reader.cpp:305] Starting prefetch of epoch 17
I0925 04:38:20.938205  4458 solver.cpp:314] Iteration 20900 (1.90648 iter/s, 52.4527s/100 iter), loss = 0.0478596
I0925 04:38:20.938305  4458 solver.cpp:336]     Train net output #0: loss = 0.0478596 (* 1 = 0.0478596 loss)
I0925 04:38:20.938323  4458 sgd_solver.cpp:136] Iteration 20900, lr = 0.01, m = 0.9
I0925 04:39:14.075646  4458 solver.cpp:423] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.8 sparsity_achieved=0.779291 iter=21000
I0925 04:41:06.021791  4458 net.cpp:2253] All zero weights of convolution layers are frozen
I0925 04:41:06.041359  4458 solver.cpp:368] Sparsity after update:
I0925 04:41:06.049953  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 04:41:06.049973  4458 net.cpp:2312] conv1a_param_0(0.249) 
I0925 04:41:06.049981  4458 net.cpp:2312] conv1b_param_0(0.583) 
I0925 04:41:06.049985  4458 net.cpp:2312] ctx_conv1_param_0(0.771) 
I0925 04:41:06.049988  4458 net.cpp:2312] ctx_conv2_param_0(0.782) 
I0925 04:41:06.049990  4458 net.cpp:2312] ctx_conv3_param_0(0.774) 
I0925 04:41:06.049993  4458 net.cpp:2312] ctx_conv4_param_0(0.784) 
I0925 04:41:06.049998  4458 net.cpp:2312] ctx_final_param_0(0.283) 
I0925 04:41:06.050000  4458 net.cpp:2312] out3a_param_0(0.799) 
I0925 04:41:06.050017  4458 net.cpp:2312] out5a_param_0(0.799) 
I0925 04:41:06.050027  4458 net.cpp:2312] res2a_branch2a_param_0(0.724) 
I0925 04:41:06.050036  4458 net.cpp:2312] res2a_branch2b_param_0(0.559) 
I0925 04:41:06.050046  4458 net.cpp:2312] res3a_branch2a_param_0(0.752) 
I0925 04:41:06.050055  4458 net.cpp:2312] res3a_branch2b_param_0(0.686) 
I0925 04:41:06.050065  4458 net.cpp:2312] res4a_branch2a_param_0(0.786) 
I0925 04:41:06.050074  4458 net.cpp:2312] res4a_branch2b_param_0(0.753) 
I0925 04:41:06.050083  4458 net.cpp:2312] res5a_branch2a_param_0(0.798) 
I0925 04:41:06.050092  4458 net.cpp:2312] res5a_branch2b_param_0(0.799) 
I0925 04:41:06.050101  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.12015e+06/2.69117e+06) 0.788
I0925 04:41:06.861536  4458 solver.cpp:314] Iteration 21000 (0.602705 iter/s, 165.919s/100 iter), loss = 0.0732942
I0925 04:41:06.861570  4458 solver.cpp:336]     Train net output #0: loss = 0.0732942 (* 1 = 0.0732942 loss)
I0925 04:41:06.861577  4458 sgd_solver.cpp:136] Iteration 21000, lr = 0.01, m = 0.9
I0925 04:42:05.921274  4458 solver.cpp:314] Iteration 21100 (1.69325 iter/s, 59.0581s/100 iter), loss = 0.0565515
I0925 04:42:05.921349  4458 solver.cpp:336]     Train net output #0: loss = 0.0565516 (* 1 = 0.0565516 loss)
I0925 04:42:05.921355  4458 sgd_solver.cpp:136] Iteration 21100, lr = 0.01, m = 0.9
I0925 04:42:36.954392  4466 data_reader.cpp:305] Starting prefetch of epoch 6
I0925 04:43:02.513653  4458 solver.cpp:314] Iteration 21200 (1.76707 iter/s, 56.5908s/100 iter), loss = 0.0976997
I0925 04:43:02.513687  4458 solver.cpp:336]     Train net output #0: loss = 0.0976998 (* 1 = 0.0976998 loss)
I0925 04:43:02.513694  4458 sgd_solver.cpp:136] Iteration 21200, lr = 0.01, m = 0.9
I0925 04:43:54.863286  4458 solver.cpp:314] Iteration 21300 (1.91029 iter/s, 52.3481s/100 iter), loss = 0.0687067
I0925 04:43:54.868232  4458 solver.cpp:336]     Train net output #0: loss = 0.0687067 (* 1 = 0.0687067 loss)
I0925 04:43:54.868252  4458 sgd_solver.cpp:136] Iteration 21300, lr = 0.01, m = 0.9
I0925 04:44:04.117534  4408 data_reader.cpp:305] Starting prefetch of epoch 14
I0925 04:44:48.068351  4458 solver.cpp:314] Iteration 21400 (1.87957 iter/s, 53.2035s/100 iter), loss = 0.0598402
I0925 04:44:48.068454  4458 solver.cpp:336]     Train net output #0: loss = 0.0598402 (* 1 = 0.0598402 loss)
I0925 04:44:48.068465  4458 sgd_solver.cpp:136] Iteration 21400, lr = 0.01, m = 0.9
I0925 04:45:41.472219  4458 solver.cpp:314] Iteration 21500 (1.87258 iter/s, 53.4023s/100 iter), loss = 0.0513079
I0925 04:45:41.476196  4458 solver.cpp:336]     Train net output #0: loss = 0.0513079 (* 1 = 0.0513079 loss)
I0925 04:45:41.476202  4458 sgd_solver.cpp:136] Iteration 21500, lr = 0.01, m = 0.9
I0925 04:46:34.182067  4458 solver.cpp:314] Iteration 21600 (1.89723 iter/s, 52.7083s/100 iter), loss = 0.0838634
I0925 04:46:34.182157  4458 solver.cpp:336]     Train net output #0: loss = 0.0838635 (* 1 = 0.0838635 loss)
I0925 04:46:34.182173  4458 sgd_solver.cpp:136] Iteration 21600, lr = 0.01, m = 0.9
I0925 04:47:00.305513  4461 data_reader.cpp:305] Starting prefetch of epoch 17
I0925 04:47:27.952211  4458 solver.cpp:314] Iteration 21700 (1.85982 iter/s, 53.7686s/100 iter), loss = 0.0736713
I0925 04:47:27.960211  4458 solver.cpp:336]     Train net output #0: loss = 0.0736714 (* 1 = 0.0736714 loss)
I0925 04:47:27.960225  4458 sgd_solver.cpp:136] Iteration 21700, lr = 0.01, m = 0.9
I0925 04:48:20.908418  4458 solver.cpp:314] Iteration 21800 (1.88841 iter/s, 52.9547s/100 iter), loss = 0.0680175
I0925 04:48:20.919507  4458 solver.cpp:336]     Train net output #0: loss = 0.0680176 (* 1 = 0.0680176 loss)
I0925 04:48:20.919519  4458 sgd_solver.cpp:136] Iteration 21800, lr = 0.01, m = 0.9
I0925 04:49:13.237103  4458 solver.cpp:314] Iteration 21900 (1.91105 iter/s, 52.3272s/100 iter), loss = 0.0530051
I0925 04:49:13.237226  4458 solver.cpp:336]     Train net output #0: loss = 0.0530052 (* 1 = 0.0530052 loss)
I0925 04:49:13.237244  4458 sgd_solver.cpp:136] Iteration 21900, lr = 0.01, m = 0.9
I0925 04:49:54.325136  4466 data_reader.cpp:305] Starting prefetch of epoch 7
I0925 04:50:05.279695  4458 solver.cpp:423] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.81 sparsity_achieved=0.787819 iter=22000
I0925 04:52:16.459635  4458 net.cpp:2253] All zero weights of convolution layers are frozen
I0925 04:52:16.465585  4458 solver.cpp:368] Sparsity after update:
I0925 04:52:16.467968  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 04:52:16.467979  4458 net.cpp:2312] conv1a_param_0(0.255) 
I0925 04:52:16.467999  4458 net.cpp:2312] conv1b_param_0(0.584) 
I0925 04:52:16.468003  4458 net.cpp:2312] ctx_conv1_param_0(0.777) 
I0925 04:52:16.468006  4458 net.cpp:2312] ctx_conv2_param_0(0.788) 
I0925 04:52:16.468009  4458 net.cpp:2312] ctx_conv3_param_0(0.779) 
I0925 04:52:16.468013  4458 net.cpp:2312] ctx_conv4_param_0(0.791) 
I0925 04:52:16.468015  4458 net.cpp:2312] ctx_final_param_0(0.278) 
I0925 04:52:16.468019  4458 net.cpp:2312] out3a_param_0(0.809) 
I0925 04:52:16.468022  4458 net.cpp:2312] out5a_param_0(0.808) 
I0925 04:52:16.468025  4458 net.cpp:2312] res2a_branch2a_param_0(0.727) 
I0925 04:52:16.468029  4458 net.cpp:2312] res2a_branch2b_param_0(0.563) 
I0925 04:52:16.468032  4458 net.cpp:2312] res3a_branch2a_param_0(0.756) 
I0925 04:52:16.468035  4458 net.cpp:2312] res3a_branch2b_param_0(0.689) 
I0925 04:52:16.468039  4458 net.cpp:2312] res4a_branch2a_param_0(0.792) 
I0925 04:52:16.468042  4458 net.cpp:2312] res4a_branch2b_param_0(0.758) 
I0925 04:52:16.468045  4458 net.cpp:2312] res5a_branch2a_param_0(0.806) 
I0925 04:52:16.468049  4458 net.cpp:2312] res5a_branch2b_param_0(0.81) 
I0925 04:52:16.468051  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.1418e+06/2.69117e+06) 0.796
I0925 04:52:16.468065  4458 solver.cpp:562] Iteration 22000, Testing net (#0)
I0925 04:52:30.799685  4441 data_reader.cpp:305] Starting prefetch of epoch 4
I0925 04:52:39.478034  4458 solver.cpp:654]     Test net output #0: accuracy/top1 = 0.951714
I0925 04:52:39.478054  4458 solver.cpp:654]     Test net output #1: accuracy/top5 = 1
I0925 04:52:39.478060  4458 solver.cpp:654]     Test net output #2: loss = 0.151888 (* 1 = 0.151888 loss)
I0925 04:52:39.478091  4458 solver.cpp:265] [MultiGPU] Tests completed in 23.0094s
I0925 04:52:39.884306  4458 solver.cpp:314] Iteration 22000 (0.48393 iter/s, 206.641s/100 iter), loss = 0.0895858
I0925 04:52:39.884361  4458 solver.cpp:336]     Train net output #0: loss = 0.0895859 (* 1 = 0.0895859 loss)
I0925 04:52:39.884377  4458 sgd_solver.cpp:136] Iteration 22000, lr = 0.01, m = 0.9
I0925 04:53:42.172585  4458 solver.cpp:314] Iteration 22100 (1.60548 iter/s, 62.2865s/100 iter), loss = 0.123658
I0925 04:53:42.172662  4458 solver.cpp:336]     Train net output #0: loss = 0.123659 (* 1 = 0.123659 loss)
I0925 04:53:42.172670  4458 sgd_solver.cpp:136] Iteration 22100, lr = 0.01, m = 0.9
I0925 04:54:43.287333  4458 solver.cpp:314] Iteration 22200 (1.63631 iter/s, 61.113s/100 iter), loss = 0.063554
I0925 04:54:43.287401  4458 solver.cpp:336]     Train net output #0: loss = 0.0635541 (* 1 = 0.0635541 loss)
I0925 04:54:43.287410  4458 sgd_solver.cpp:136] Iteration 22200, lr = 0.01, m = 0.9
I0925 04:55:28.067292  4463 blocking_queue.cpp:40] Waiting for datum
I0925 04:55:36.543253  4458 solver.cpp:314] Iteration 22300 (1.87778 iter/s, 53.2544s/100 iter), loss = 0.0425475
I0925 04:55:36.543310  4458 solver.cpp:336]     Train net output #0: loss = 0.0425475 (* 1 = 0.0425475 loss)
I0925 04:55:36.543323  4458 sgd_solver.cpp:136] Iteration 22300, lr = 0.01, m = 0.9
I0925 04:55:41.497341  4408 data_reader.cpp:305] Starting prefetch of epoch 15
I0925 04:56:36.076786  4458 solver.cpp:314] Iteration 22400 (1.67977 iter/s, 59.5318s/100 iter), loss = 0.0823714
I0925 04:56:36.078701  4458 solver.cpp:336]     Train net output #0: loss = 0.0823714 (* 1 = 0.0823714 loss)
I0925 04:56:36.078713  4458 sgd_solver.cpp:136] Iteration 22400, lr = 0.01, m = 0.9
I0925 04:57:33.444222  4458 solver.cpp:314] Iteration 22500 (1.7432 iter/s, 57.3658s/100 iter), loss = 0.0894504
I0925 04:57:33.444309  4458 solver.cpp:336]     Train net output #0: loss = 0.0894505 (* 1 = 0.0894505 loss)
I0925 04:57:33.444319  4458 sgd_solver.cpp:136] Iteration 22500, lr = 0.01, m = 0.9
I0925 04:58:26.799433  4458 solver.cpp:314] Iteration 22600 (1.87428 iter/s, 53.3537s/100 iter), loss = 0.0561915
I0925 04:58:26.804208  4458 solver.cpp:336]     Train net output #0: loss = 0.0561915 (* 1 = 0.0561915 loss)
I0925 04:58:26.804221  4458 sgd_solver.cpp:136] Iteration 22600, lr = 0.01, m = 0.9
I0925 04:58:48.069082  4408 data_reader.cpp:305] Starting prefetch of epoch 16
I0925 04:59:20.079362  4458 solver.cpp:314] Iteration 22700 (1.87693 iter/s, 53.2784s/100 iter), loss = 0.0741816
I0925 04:59:20.079433  4458 solver.cpp:336]     Train net output #0: loss = 0.0741816 (* 1 = 0.0741816 loss)
I0925 04:59:20.079442  4458 sgd_solver.cpp:136] Iteration 22700, lr = 0.01, m = 0.9
I0925 05:00:18.671998  4458 solver.cpp:314] Iteration 22800 (1.70675 iter/s, 58.591s/100 iter), loss = 0.0419146
I0925 05:00:18.672060  4458 solver.cpp:336]     Train net output #0: loss = 0.0419147 (* 1 = 0.0419147 loss)
I0925 05:00:18.672068  4458 sgd_solver.cpp:136] Iteration 22800, lr = 0.01, m = 0.9
I0925 05:01:15.758762  4458 solver.cpp:314] Iteration 22900 (1.75177 iter/s, 57.0851s/100 iter), loss = 0.0577773
I0925 05:01:15.760221  4458 solver.cpp:336]     Train net output #0: loss = 0.0577774 (* 1 = 0.0577774 loss)
I0925 05:01:15.760231  4458 sgd_solver.cpp:136] Iteration 22900, lr = 0.01, m = 0.9
I0925 05:01:52.723577  4408 data_reader.cpp:305] Starting prefetch of epoch 17
I0925 05:02:07.822396  4458 solver.cpp:423] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.82 sparsity_achieved=0.795861 iter=23000
I0925 05:05:02.731215  4458 net.cpp:2253] All zero weights of convolution layers are frozen
I0925 05:05:02.745729  4458 solver.cpp:368] Sparsity after update:
I0925 05:05:02.753132  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 05:05:02.753149  4458 net.cpp:2312] conv1a_param_0(0.255) 
I0925 05:05:02.753157  4458 net.cpp:2312] conv1b_param_0(0.587) 
I0925 05:05:02.753161  4458 net.cpp:2312] ctx_conv1_param_0(0.783) 
I0925 05:05:02.753165  4458 net.cpp:2312] ctx_conv2_param_0(0.795) 
I0925 05:05:02.753167  4458 net.cpp:2312] ctx_conv3_param_0(0.785) 
I0925 05:05:02.753170  4458 net.cpp:2312] ctx_conv4_param_0(0.797) 
I0925 05:05:02.753173  4458 net.cpp:2312] ctx_final_param_0(0.33) 
I0925 05:05:02.753191  4458 net.cpp:2312] out3a_param_0(0.819) 
I0925 05:05:02.753196  4458 net.cpp:2312] out5a_param_0(0.817) 
I0925 05:05:02.753201  4458 net.cpp:2312] res2a_branch2a_param_0(0.729) 
I0925 05:05:02.753203  4458 net.cpp:2312] res2a_branch2b_param_0(0.565) 
I0925 05:05:02.753207  4458 net.cpp:2312] res3a_branch2a_param_0(0.76) 
I0925 05:05:02.753217  4458 net.cpp:2312] res3a_branch2b_param_0(0.693) 
I0925 05:05:02.753222  4458 net.cpp:2312] res4a_branch2a_param_0(0.797) 
I0925 05:05:02.753226  4458 net.cpp:2312] res4a_branch2b_param_0(0.762) 
I0925 05:05:02.753229  4458 net.cpp:2312] res5a_branch2a_param_0(0.817) 
I0925 05:05:02.753240  4458 net.cpp:2312] res5a_branch2b_param_0(0.819) 
I0925 05:05:02.753244  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.16549e+06/2.69117e+06) 0.805
I0925 05:05:03.317375  4458 solver.cpp:314] Iteration 23000 (0.43946 iter/s, 227.552s/100 iter), loss = 0.0791198
I0925 05:05:03.317409  4458 solver.cpp:336]     Train net output #0: loss = 0.0791198 (* 1 = 0.0791198 loss)
I0925 05:05:03.317415  4458 sgd_solver.cpp:136] Iteration 23000, lr = 0.01, m = 0.9
I0925 05:06:05.385309  4458 solver.cpp:314] Iteration 23100 (1.61118 iter/s, 62.0662s/100 iter), loss = 0.0573895
I0925 05:06:05.385377  4458 solver.cpp:336]     Train net output #0: loss = 0.0573896 (* 1 = 0.0573896 loss)
I0925 05:06:05.385385  4458 sgd_solver.cpp:136] Iteration 23100, lr = 0.01, m = 0.9
I0925 05:07:06.764009  4458 solver.cpp:314] Iteration 23200 (1.62928 iter/s, 61.377s/100 iter), loss = 0.0513462
I0925 05:07:06.764073  4458 solver.cpp:336]     Train net output #0: loss = 0.0513462 (* 1 = 0.0513462 loss)
I0925 05:07:06.764081  4458 sgd_solver.cpp:136] Iteration 23200, lr = 0.01, m = 0.9
I0925 05:07:58.237846  4458 solver.cpp:314] Iteration 23300 (1.94279 iter/s, 51.4724s/100 iter), loss = 0.0370315
I0925 05:07:58.237953  4458 solver.cpp:336]     Train net output #0: loss = 0.0370316 (* 1 = 0.0370316 loss)
I0925 05:07:58.237968  4458 sgd_solver.cpp:136] Iteration 23300, lr = 0.01, m = 0.9
I0925 05:07:58.749359  4408 data_reader.cpp:305] Starting prefetch of epoch 18
I0925 05:08:46.279909  4458 solver.cpp:314] Iteration 23400 (2.08157 iter/s, 48.0407s/100 iter), loss = 0.0527497
I0925 05:08:46.280040  4458 solver.cpp:336]     Train net output #0: loss = 0.0527497 (* 1 = 0.0527497 loss)
I0925 05:08:46.280052  4458 sgd_solver.cpp:136] Iteration 23400, lr = 0.01, m = 0.9
I0925 05:09:38.608129  4458 solver.cpp:314] Iteration 23500 (1.91107 iter/s, 52.3267s/100 iter), loss = 0.174703
I0925 05:09:38.608248  4458 solver.cpp:336]     Train net output #0: loss = 0.174703 (* 1 = 0.174703 loss)
I0925 05:09:38.608261  4458 sgd_solver.cpp:136] Iteration 23500, lr = 0.01, m = 0.9
I0925 05:10:32.898748  4458 solver.cpp:314] Iteration 23600 (1.84199 iter/s, 54.2891s/100 iter), loss = 0.0427775
I0925 05:10:32.898869  4458 solver.cpp:336]     Train net output #0: loss = 0.0427776 (* 1 = 0.0427776 loss)
I0925 05:10:32.898885  4458 sgd_solver.cpp:136] Iteration 23600, lr = 0.01, m = 0.9
I0925 05:10:49.429982  4466 data_reader.cpp:305] Starting prefetch of epoch 8
I0925 05:11:26.166903  4458 solver.cpp:314] Iteration 23700 (1.87735 iter/s, 53.2666s/100 iter), loss = 0.0555042
I0925 05:11:26.166973  4458 solver.cpp:336]     Train net output #0: loss = 0.0555043 (* 1 = 0.0555043 loss)
I0925 05:11:26.166980  4458 sgd_solver.cpp:136] Iteration 23700, lr = 0.01, m = 0.9
I0925 05:12:16.250586  4408 data_reader.cpp:305] Starting prefetch of epoch 19
I0925 05:12:18.203303  4458 solver.cpp:314] Iteration 23800 (1.92179 iter/s, 52.0349s/100 iter), loss = 0.095501
I0925 05:12:18.203338  4458 solver.cpp:336]     Train net output #0: loss = 0.095501 (* 1 = 0.095501 loss)
I0925 05:12:18.203346  4458 sgd_solver.cpp:136] Iteration 23800, lr = 0.01, m = 0.9
I0925 05:13:11.608486  4458 solver.cpp:314] Iteration 23900 (1.87253 iter/s, 53.4037s/100 iter), loss = 0.0622199
I0925 05:13:11.611788  4458 solver.cpp:336]     Train net output #0: loss = 0.0622199 (* 1 = 0.0622199 loss)
I0925 05:13:11.611815  4458 sgd_solver.cpp:136] Iteration 23900, lr = 0.01, m = 0.9
I0925 05:14:04.577098  4458 solver.cpp:368] Sparsity after update:
I0925 05:14:04.603924  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 05:14:04.604073  4458 net.cpp:2312] conv1a_param_0(0.255) 
I0925 05:14:04.604094  4458 net.cpp:2312] conv1b_param_0(0.587) 
I0925 05:14:04.604106  4458 net.cpp:2312] ctx_conv1_param_0(0.783) 
I0925 05:14:04.604118  4458 net.cpp:2312] ctx_conv2_param_0(0.795) 
I0925 05:14:04.604130  4458 net.cpp:2312] ctx_conv3_param_0(0.785) 
I0925 05:14:04.604140  4458 net.cpp:2312] ctx_conv4_param_0(0.797) 
I0925 05:14:04.604152  4458 net.cpp:2312] ctx_final_param_0(0.33) 
I0925 05:14:04.604163  4458 net.cpp:2312] out3a_param_0(0.819) 
I0925 05:14:04.604178  4458 net.cpp:2312] out5a_param_0(0.817) 
I0925 05:14:04.604192  4458 net.cpp:2312] res2a_branch2a_param_0(0.729) 
I0925 05:14:04.604202  4458 net.cpp:2312] res2a_branch2b_param_0(0.565) 
I0925 05:14:04.604214  4458 net.cpp:2312] res3a_branch2a_param_0(0.76) 
I0925 05:14:04.604225  4458 net.cpp:2312] res3a_branch2b_param_0(0.693) 
I0925 05:14:04.604259  4458 net.cpp:2312] res4a_branch2a_param_0(0.797) 
I0925 05:14:04.604271  4458 net.cpp:2312] res4a_branch2b_param_0(0.762) 
I0925 05:14:04.604282  4458 net.cpp:2312] res5a_branch2a_param_0(0.817) 
I0925 05:14:04.604293  4458 net.cpp:2312] res5a_branch2b_param_0(0.819) 
I0925 05:14:04.604305  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.16549e+06/2.69117e+06) 0.805
I0925 05:14:04.604327  4458 solver.cpp:562] Iteration 24000, Testing net (#0)
I0925 05:14:13.196010  4518 data_reader.cpp:305] Starting prefetch of epoch 5
I0925 05:14:31.431625  4458 solver.cpp:654]     Test net output #0: accuracy/top1 = 0.952145
I0925 05:14:31.431689  4458 solver.cpp:654]     Test net output #1: accuracy/top5 = 1
I0925 05:14:31.431707  4458 solver.cpp:654]     Test net output #2: loss = 0.142048 (* 1 = 0.142048 loss)
I0925 05:14:31.431749  4458 solver.cpp:265] [MultiGPU] Tests completed in 26.8267s
I0925 05:14:31.952107  4458 solver.cpp:314] Iteration 24000 (1.24469 iter/s, 80.3414s/100 iter), loss = 0.069029
I0925 05:14:31.952147  4458 solver.cpp:336]     Train net output #0: loss = 0.0690291 (* 1 = 0.0690291 loss)
I0925 05:14:31.952158  4458 sgd_solver.cpp:136] Iteration 24000, lr = 0.01, m = 0.9
I0925 05:15:26.854120  4458 solver.cpp:314] Iteration 24100 (1.82148 iter/s, 54.9005s/100 iter), loss = 0.0673808
I0925 05:15:26.854203  4458 solver.cpp:336]     Train net output #0: loss = 0.0673808 (* 1 = 0.0673808 loss)
I0925 05:15:26.854213  4458 sgd_solver.cpp:136] Iteration 24100, lr = 0.01, m = 0.9
I0925 05:15:42.373078  4408 data_reader.cpp:305] Starting prefetch of epoch 20
I0925 05:16:22.925072  4458 solver.cpp:314] Iteration 24200 (1.7835 iter/s, 56.0694s/100 iter), loss = 0.13318
I0925 05:16:22.936339  4458 solver.cpp:336]     Train net output #0: loss = 0.13318 (* 1 = 0.13318 loss)
I0925 05:16:22.936385  4458 sgd_solver.cpp:136] Iteration 24200, lr = 0.01, m = 0.9
I0925 05:16:58.735757  4465 blocking_queue.cpp:40] Waiting for datum
I0925 05:17:19.712235  4458 solver.cpp:314] Iteration 24300 (1.76101 iter/s, 56.7856s/100 iter), loss = 0.0502709
I0925 05:17:19.712272  4458 solver.cpp:336]     Train net output #0: loss = 0.0502709 (* 1 = 0.0502709 loss)
I0925 05:17:19.712280  4458 sgd_solver.cpp:136] Iteration 24300, lr = 0.01, m = 0.9
I0925 05:18:16.334167  4458 solver.cpp:314] Iteration 24400 (1.76615 iter/s, 56.6203s/100 iter), loss = 0.0739662
I0925 05:18:16.334249  4458 solver.cpp:336]     Train net output #0: loss = 0.0739662 (* 1 = 0.0739662 loss)
I0925 05:18:16.334259  4458 sgd_solver.cpp:136] Iteration 24400, lr = 0.01, m = 0.9
I0925 05:18:48.615828  4410 data_reader.cpp:305] Starting prefetch of epoch 18
I0925 05:19:11.219642  4458 solver.cpp:314] Iteration 24500 (1.82203 iter/s, 54.8839s/100 iter), loss = 0.0478495
I0925 05:19:11.219671  4458 solver.cpp:336]     Train net output #0: loss = 0.0478495 (* 1 = 0.0478495 loss)
I0925 05:19:11.219676  4458 sgd_solver.cpp:136] Iteration 24500, lr = 0.01, m = 0.9
I0925 05:20:04.163887  4458 solver.cpp:314] Iteration 24600 (1.88883 iter/s, 52.9428s/100 iter), loss = 0.0879773
I0925 05:20:04.168202  4458 solver.cpp:336]     Train net output #0: loss = 0.0879774 (* 1 = 0.0879774 loss)
I0925 05:20:04.168213  4458 sgd_solver.cpp:136] Iteration 24600, lr = 0.01, m = 0.9
I0925 05:20:17.648660  4462 data_reader.cpp:305] Starting prefetch of epoch 7
I0925 05:20:58.244336  4458 solver.cpp:314] Iteration 24700 (1.84915 iter/s, 54.0789s/100 iter), loss = 0.0560032
I0925 05:20:58.246376  4458 solver.cpp:336]     Train net output #0: loss = 0.0560033 (* 1 = 0.0560033 loss)
I0925 05:20:58.246384  4458 sgd_solver.cpp:136] Iteration 24700, lr = 0.01, m = 0.9
I0925 05:21:51.834909  4458 solver.cpp:314] Iteration 24800 (1.86605 iter/s, 53.5891s/100 iter), loss = 0.063774
I0925 05:21:51.834978  4458 solver.cpp:336]     Train net output #0: loss = 0.063774 (* 1 = 0.063774 loss)
I0925 05:21:51.834985  4458 sgd_solver.cpp:136] Iteration 24800, lr = 0.01, m = 0.9
I0925 05:22:44.308251  4458 solver.cpp:314] Iteration 24900 (1.90578 iter/s, 52.4719s/100 iter), loss = 0.0583712
I0925 05:22:44.308311  4458 solver.cpp:336]     Train net output #0: loss = 0.0583713 (* 1 = 0.0583713 loss)
I0925 05:22:44.308316  4458 sgd_solver.cpp:136] Iteration 24900, lr = 0.01, m = 0.9
I0925 05:23:13.158232  4408 data_reader.cpp:305] Starting prefetch of epoch 21
I0925 05:23:37.462522  4458 solver.cpp:368] Sparsity after update:
I0925 05:23:37.506397  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 05:23:37.506438  4458 net.cpp:2312] conv1a_param_0(0.255) 
I0925 05:23:37.506464  4458 net.cpp:2312] conv1b_param_0(0.587) 
I0925 05:23:37.506469  4458 net.cpp:2312] ctx_conv1_param_0(0.783) 
I0925 05:23:37.506471  4458 net.cpp:2312] ctx_conv2_param_0(0.795) 
I0925 05:23:37.506474  4458 net.cpp:2312] ctx_conv3_param_0(0.785) 
I0925 05:23:37.506477  4458 net.cpp:2312] ctx_conv4_param_0(0.797) 
I0925 05:23:37.506481  4458 net.cpp:2312] ctx_final_param_0(0.33) 
I0925 05:23:37.506487  4458 net.cpp:2312] out3a_param_0(0.819) 
I0925 05:23:37.506492  4458 net.cpp:2312] out5a_param_0(0.817) 
I0925 05:23:37.506495  4458 net.cpp:2312] res2a_branch2a_param_0(0.729) 
I0925 05:23:37.506498  4458 net.cpp:2312] res2a_branch2b_param_0(0.565) 
I0925 05:23:37.506502  4458 net.cpp:2312] res3a_branch2a_param_0(0.76) 
I0925 05:23:37.506505  4458 net.cpp:2312] res3a_branch2b_param_0(0.693) 
I0925 05:23:37.506508  4458 net.cpp:2312] res4a_branch2a_param_0(0.797) 
I0925 05:23:37.506512  4458 net.cpp:2312] res4a_branch2b_param_0(0.762) 
I0925 05:23:37.506520  4458 net.cpp:2312] res5a_branch2a_param_0(0.817) 
I0925 05:23:37.506523  4458 net.cpp:2312] res5a_branch2b_param_0(0.819) 
I0925 05:23:37.506526  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.16549e+06/2.69117e+06) 0.805
I0925 05:23:38.100276  4458 solver.cpp:314] Iteration 25000 (1.85906 iter/s, 53.7905s/100 iter), loss = 0.0645238
I0925 05:23:38.100353  4458 solver.cpp:336]     Train net output #0: loss = 0.0645239 (* 1 = 0.0645239 loss)
I0925 05:23:38.100366  4458 sgd_solver.cpp:136] Iteration 25000, lr = 0.01, m = 0.9
I0925 05:24:31.438166  4458 solver.cpp:314] Iteration 25100 (1.87489 iter/s, 53.3364s/100 iter), loss = 0.0545276
I0925 05:24:31.438248  4458 solver.cpp:336]     Train net output #0: loss = 0.0545276 (* 1 = 0.0545276 loss)
I0925 05:24:31.438256  4458 sgd_solver.cpp:136] Iteration 25100, lr = 0.01, m = 0.9
I0925 05:25:24.304500  4458 solver.cpp:314] Iteration 25200 (1.89162 iter/s, 52.8649s/100 iter), loss = 0.0791232
I0925 05:25:24.304886  4458 solver.cpp:336]     Train net output #0: loss = 0.0791232 (* 1 = 0.0791232 loss)
I0925 05:25:24.304903  4458 sgd_solver.cpp:136] Iteration 25200, lr = 0.01, m = 0.9
I0925 05:26:09.114027  4464 data_reader.cpp:305] Starting prefetch of epoch 18
I0925 05:26:17.113029  4458 solver.cpp:314] Iteration 25300 (1.89369 iter/s, 52.807s/100 iter), loss = 0.0585904
I0925 05:26:17.113060  4458 solver.cpp:336]     Train net output #0: loss = 0.0585904 (* 1 = 0.0585904 loss)
I0925 05:26:17.113067  4458 sgd_solver.cpp:136] Iteration 25300, lr = 0.01, m = 0.9
I0925 05:27:10.149919  4458 solver.cpp:314] Iteration 25400 (1.88553 iter/s, 53.0354s/100 iter), loss = 0.0701619
I0925 05:27:10.150005  4458 solver.cpp:336]     Train net output #0: loss = 0.0701619 (* 1 = 0.0701619 loss)
I0925 05:27:10.150013  4458 sgd_solver.cpp:136] Iteration 25400, lr = 0.01, m = 0.9
I0925 05:27:37.065012  4462 data_reader.cpp:305] Starting prefetch of epoch 8
I0925 05:28:03.198499  4458 solver.cpp:314] Iteration 25500 (1.88512 iter/s, 53.0471s/100 iter), loss = 0.0501337
I0925 05:28:03.198591  4458 solver.cpp:336]     Train net output #0: loss = 0.0501337 (* 1 = 0.0501337 loss)
I0925 05:28:03.198599  4458 sgd_solver.cpp:136] Iteration 25500, lr = 0.01, m = 0.9
I0925 05:28:55.364128  4458 solver.cpp:314] Iteration 25600 (1.91703 iter/s, 52.1641s/100 iter), loss = 0.0763518
I0925 05:28:55.365248  4458 solver.cpp:336]     Train net output #0: loss = 0.0763519 (* 1 = 0.0763519 loss)
I0925 05:28:55.365275  4458 sgd_solver.cpp:136] Iteration 25600, lr = 0.01, m = 0.9
I0925 05:29:48.608917  4458 solver.cpp:314] Iteration 25700 (1.87817 iter/s, 53.2433s/100 iter), loss = 0.0429579
I0925 05:29:48.609009  4458 solver.cpp:336]     Train net output #0: loss = 0.0429579 (* 1 = 0.0429579 loss)
I0925 05:29:48.609022  4458 sgd_solver.cpp:136] Iteration 25700, lr = 0.01, m = 0.9
I0925 05:30:31.180886  4462 data_reader.cpp:305] Starting prefetch of epoch 9
I0925 05:30:41.677867  4458 solver.cpp:314] Iteration 25800 (1.88439 iter/s, 53.0675s/100 iter), loss = 0.0619424
I0925 05:30:41.677896  4458 solver.cpp:336]     Train net output #0: loss = 0.0619424 (* 1 = 0.0619424 loss)
I0925 05:30:41.677903  4458 sgd_solver.cpp:136] Iteration 25800, lr = 0.01, m = 0.9
I0925 05:31:28.876328  4458 solver.cpp:314] Iteration 25900 (2.11877 iter/s, 47.1971s/100 iter), loss = 0.046861
I0925 05:31:28.880277  4458 solver.cpp:336]     Train net output #0: loss = 0.0468611 (* 1 = 0.0468611 loss)
I0925 05:31:28.880303  4458 sgd_solver.cpp:136] Iteration 25900, lr = 0.01, m = 0.9
I0925 05:32:20.004073  4458 solver.cpp:368] Sparsity after update:
I0925 05:32:20.013799  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 05:32:20.013825  4458 net.cpp:2312] conv1a_param_0(0.255) 
I0925 05:32:20.013847  4458 net.cpp:2312] conv1b_param_0(0.587) 
I0925 05:32:20.013854  4458 net.cpp:2312] ctx_conv1_param_0(0.783) 
I0925 05:32:20.013856  4458 net.cpp:2312] ctx_conv2_param_0(0.795) 
I0925 05:32:20.013859  4458 net.cpp:2312] ctx_conv3_param_0(0.785) 
I0925 05:32:20.013862  4458 net.cpp:2312] ctx_conv4_param_0(0.797) 
I0925 05:32:20.013865  4458 net.cpp:2312] ctx_final_param_0(0.33) 
I0925 05:32:20.013869  4458 net.cpp:2312] out3a_param_0(0.819) 
I0925 05:32:20.013875  4458 net.cpp:2312] out5a_param_0(0.817) 
I0925 05:32:20.013882  4458 net.cpp:2312] res2a_branch2a_param_0(0.729) 
I0925 05:32:20.013886  4458 net.cpp:2312] res2a_branch2b_param_0(0.565) 
I0925 05:32:20.013890  4458 net.cpp:2312] res3a_branch2a_param_0(0.76) 
I0925 05:32:20.013892  4458 net.cpp:2312] res3a_branch2b_param_0(0.693) 
I0925 05:32:20.013895  4458 net.cpp:2312] res4a_branch2a_param_0(0.797) 
I0925 05:32:20.013898  4458 net.cpp:2312] res4a_branch2b_param_0(0.762) 
I0925 05:32:20.013905  4458 net.cpp:2312] res5a_branch2a_param_0(0.817) 
I0925 05:32:20.013911  4458 net.cpp:2312] res5a_branch2b_param_0(0.819) 
I0925 05:32:20.013916  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.16549e+06/2.69117e+06) 0.805
I0925 05:32:20.013927  4458 solver.cpp:562] Iteration 26000, Testing net (#0)
I0925 05:32:38.193699  4441 data_reader.cpp:305] Starting prefetch of epoch 5
I0925 05:32:46.565574  4458 solver.cpp:654]     Test net output #0: accuracy/top1 = 0.95437
I0925 05:32:46.565606  4458 solver.cpp:654]     Test net output #1: accuracy/top5 = 1
I0925 05:32:46.565614  4458 solver.cpp:654]     Test net output #2: loss = 0.154112 (* 1 = 0.154112 loss)
I0925 05:32:46.584215  4458 solver.cpp:265] [MultiGPU] Tests completed in 26.5695s
I0925 05:32:47.091703  4458 solver.cpp:314] Iteration 26000 (1.27856 iter/s, 78.2132s/100 iter), loss = 0.0799351
I0925 05:32:47.091733  4458 solver.cpp:336]     Train net output #0: loss = 0.0799351 (* 1 = 0.0799351 loss)
I0925 05:32:47.091740  4458 sgd_solver.cpp:136] Iteration 26000, lr = 0.01, m = 0.9
I0925 05:33:43.125823  4458 solver.cpp:314] Iteration 26100 (1.78468 iter/s, 56.0325s/100 iter), loss = 0.0401241
I0925 05:33:43.125902  4458 solver.cpp:336]     Train net output #0: loss = 0.0401242 (* 1 = 0.0401242 loss)
I0925 05:33:43.125910  4458 sgd_solver.cpp:136] Iteration 26100, lr = 0.01, m = 0.9
I0925 05:34:39.148923  4458 solver.cpp:314] Iteration 26200 (1.78503 iter/s, 56.0215s/100 iter), loss = 0.080918
I0925 05:34:39.152215  4458 solver.cpp:336]     Train net output #0: loss = 0.080918 (* 1 = 0.080918 loss)
I0925 05:34:39.152223  4458 sgd_solver.cpp:136] Iteration 26200, lr = 0.01, m = 0.9
I0925 05:35:21.054790  4466 data_reader.cpp:305] Starting prefetch of epoch 9
I0925 05:35:34.520956  4458 solver.cpp:314] Iteration 26300 (1.80602 iter/s, 55.3705s/100 iter), loss = 0.0611451
I0925 05:35:34.520983  4458 solver.cpp:336]     Train net output #0: loss = 0.0611452 (* 1 = 0.0611452 loss)
I0925 05:35:34.520989  4458 sgd_solver.cpp:136] Iteration 26300, lr = 0.01, m = 0.9
I0925 05:36:33.008225  4458 solver.cpp:314] Iteration 26400 (1.70982 iter/s, 58.4856s/100 iter), loss = 0.0867889
I0925 05:36:33.008308  4458 solver.cpp:336]     Train net output #0: loss = 0.086789 (* 1 = 0.086789 loss)
I0925 05:36:33.008319  4458 sgd_solver.cpp:136] Iteration 26400, lr = 0.01, m = 0.9
I0925 05:36:34.358080  4465 blocking_queue.cpp:40] Waiting for datum
I0925 05:36:56.977118  4408 data_reader.cpp:305] Starting prefetch of epoch 22
I0925 05:37:31.342741  4458 solver.cpp:314] Iteration 26500 (1.7143 iter/s, 58.3329s/100 iter), loss = 0.0464046
I0925 05:37:31.342804  4458 solver.cpp:336]     Train net output #0: loss = 0.0464047 (* 1 = 0.0464047 loss)
I0925 05:37:31.342813  4458 sgd_solver.cpp:136] Iteration 26500, lr = 0.01, m = 0.9
I0925 05:38:25.818670  4458 solver.cpp:314] Iteration 26600 (1.83572 iter/s, 54.4744s/100 iter), loss = 0.0807488
I0925 05:38:25.818761  4458 solver.cpp:336]     Train net output #0: loss = 0.0807489 (* 1 = 0.0807489 loss)
I0925 05:38:25.818769  4458 sgd_solver.cpp:136] Iteration 26600, lr = 0.01, m = 0.9
I0925 05:39:18.704130  4458 solver.cpp:314] Iteration 26700 (1.89093 iter/s, 52.884s/100 iter), loss = 0.0607476
I0925 05:39:18.708225  4458 solver.cpp:336]     Train net output #0: loss = 0.0607477 (* 1 = 0.0607477 loss)
I0925 05:39:18.708246  4458 sgd_solver.cpp:136] Iteration 26700, lr = 0.01, m = 0.9
I0925 05:39:57.302266  4461 data_reader.cpp:305] Starting prefetch of epoch 18
I0925 05:40:12.186570  4458 solver.cpp:314] Iteration 26800 (1.86983 iter/s, 53.4809s/100 iter), loss = 0.0875579
I0925 05:40:12.186622  4458 solver.cpp:336]     Train net output #0: loss = 0.0875579 (* 1 = 0.0875579 loss)
I0925 05:40:12.186633  4458 sgd_solver.cpp:136] Iteration 26800, lr = 0.01, m = 0.9
I0925 05:41:04.287068  4458 solver.cpp:314] Iteration 26900 (1.91942 iter/s, 52.099s/100 iter), loss = 0.0551366
I0925 05:41:04.287149  4458 solver.cpp:336]     Train net output #0: loss = 0.0551367 (* 1 = 0.0551367 loss)
I0925 05:41:04.287158  4458 sgd_solver.cpp:136] Iteration 26900, lr = 0.01, m = 0.9
I0925 05:41:55.931396  4458 solver.cpp:368] Sparsity after update:
I0925 05:41:55.968410  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 05:41:55.968574  4458 net.cpp:2312] conv1a_param_0(0.255) 
I0925 05:41:55.968652  4458 net.cpp:2312] conv1b_param_0(0.587) 
I0925 05:41:55.968721  4458 net.cpp:2312] ctx_conv1_param_0(0.783) 
I0925 05:41:55.968791  4458 net.cpp:2312] ctx_conv2_param_0(0.795) 
I0925 05:41:55.968858  4458 net.cpp:2312] ctx_conv3_param_0(0.785) 
I0925 05:41:55.968926  4458 net.cpp:2312] ctx_conv4_param_0(0.797) 
I0925 05:41:55.968993  4458 net.cpp:2312] ctx_final_param_0(0.33) 
I0925 05:41:55.969058  4458 net.cpp:2312] out3a_param_0(0.819) 
I0925 05:41:55.969120  4458 net.cpp:2312] out5a_param_0(0.817) 
I0925 05:41:55.969187  4458 net.cpp:2312] res2a_branch2a_param_0(0.729) 
I0925 05:41:55.969254  4458 net.cpp:2312] res2a_branch2b_param_0(0.565) 
I0925 05:41:55.969323  4458 net.cpp:2312] res3a_branch2a_param_0(0.76) 
I0925 05:41:55.969389  4458 net.cpp:2312] res3a_branch2b_param_0(0.693) 
I0925 05:41:55.969488  4458 net.cpp:2312] res4a_branch2a_param_0(0.797) 
I0925 05:41:55.969578  4458 net.cpp:2312] res4a_branch2b_param_0(0.762) 
I0925 05:41:55.969667  4458 net.cpp:2312] res5a_branch2a_param_0(0.817) 
I0925 05:41:55.969763  4458 net.cpp:2312] res5a_branch2b_param_0(0.819) 
I0925 05:41:55.969851  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.16549e+06/2.69117e+06) 0.805
I0925 05:41:56.411353  4458 solver.cpp:314] Iteration 27000 (1.91855 iter/s, 52.1228s/100 iter), loss = 0.119813
I0925 05:41:56.411381  4458 solver.cpp:336]     Train net output #0: loss = 0.119814 (* 1 = 0.119814 loss)
I0925 05:41:56.411388  4458 sgd_solver.cpp:136] Iteration 27000, lr = 0.01, m = 0.9
I0925 05:42:41.636219  4458 solver.cpp:314] Iteration 27100 (2.21124 iter/s, 45.2236s/100 iter), loss = 0.0620039
I0925 05:42:41.639775  4458 solver.cpp:336]     Train net output #0: loss = 0.062004 (* 1 = 0.062004 loss)
I0925 05:42:41.639794  4458 sgd_solver.cpp:136] Iteration 27100, lr = 0.01, m = 0.9
I0925 05:42:42.631029  4410 data_reader.cpp:305] Starting prefetch of epoch 19
I0925 05:43:33.790923  4458 solver.cpp:314] Iteration 27200 (1.91743 iter/s, 52.1533s/100 iter), loss = 0.0406306
I0925 05:43:33.798856  4458 solver.cpp:336]     Train net output #0: loss = 0.0406307 (* 1 = 0.0406307 loss)
I0925 05:43:33.798871  4458 sgd_solver.cpp:136] Iteration 27200, lr = 0.01, m = 0.9
I0925 05:44:26.929005  4458 solver.cpp:314] Iteration 27300 (1.88194 iter/s, 53.1366s/100 iter), loss = 0.058784
I0925 05:44:26.929061  4458 solver.cpp:336]     Train net output #0: loss = 0.0587841 (* 1 = 0.0587841 loss)
I0925 05:44:26.929069  4458 sgd_solver.cpp:136] Iteration 27300, lr = 0.01, m = 0.9
I0925 05:45:19.072892  4458 solver.cpp:314] Iteration 27400 (1.91782 iter/s, 52.1424s/100 iter), loss = 0.100339
I0925 05:45:19.072968  4458 solver.cpp:336]     Train net output #0: loss = 0.100339 (* 1 = 0.100339 loss)
I0925 05:45:19.072975  4458 sgd_solver.cpp:136] Iteration 27400, lr = 0.01, m = 0.9
I0925 05:45:36.609099  4410 data_reader.cpp:305] Starting prefetch of epoch 20
I0925 05:46:12.174545  4458 solver.cpp:314] Iteration 27500 (1.88323 iter/s, 53.1002s/100 iter), loss = 0.055889
I0925 05:46:12.174682  4458 solver.cpp:336]     Train net output #0: loss = 0.0558891 (* 1 = 0.0558891 loss)
I0925 05:46:12.174698  4458 sgd_solver.cpp:136] Iteration 27500, lr = 0.01, m = 0.9
I0925 05:47:05.664681  4458 solver.cpp:314] Iteration 27600 (1.86956 iter/s, 53.4886s/100 iter), loss = 0.070958
I0925 05:47:05.672202  4458 solver.cpp:336]     Train net output #0: loss = 0.0709581 (* 1 = 0.0709581 loss)
I0925 05:47:05.672214  4458 sgd_solver.cpp:136] Iteration 27600, lr = 0.01, m = 0.9
I0925 05:47:59.450552  4458 solver.cpp:314] Iteration 27700 (1.85928 iter/s, 53.7844s/100 iter), loss = 0.0813209
I0925 05:47:59.450644  4458 solver.cpp:336]     Train net output #0: loss = 0.081321 (* 1 = 0.081321 loss)
I0925 05:47:59.450651  4458 sgd_solver.cpp:136] Iteration 27700, lr = 0.01, m = 0.9
I0925 05:48:33.216437  4466 data_reader.cpp:305] Starting prefetch of epoch 10
I0925 05:48:51.920756  4458 solver.cpp:314] Iteration 27800 (1.9059 iter/s, 52.4687s/100 iter), loss = 0.0444096
I0925 05:48:51.920806  4458 solver.cpp:336]     Train net output #0: loss = 0.0444097 (* 1 = 0.0444097 loss)
I0925 05:48:51.920815  4458 sgd_solver.cpp:136] Iteration 27800, lr = 0.01, m = 0.9
I0925 05:49:45.203608  4458 solver.cpp:314] Iteration 27900 (1.87683 iter/s, 53.2814s/100 iter), loss = 0.0604775
I0925 05:49:45.208200  4458 solver.cpp:336]     Train net output #0: loss = 0.0604776 (* 1 = 0.0604776 loss)
I0925 05:49:45.208211  4458 sgd_solver.cpp:136] Iteration 27900, lr = 0.01, m = 0.9
I0925 05:50:36.778012  4458 solver.cpp:368] Sparsity after update:
I0925 05:50:36.842574  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 05:50:36.842648  4458 net.cpp:2312] conv1a_param_0(0.255) 
I0925 05:50:36.842670  4458 net.cpp:2312] conv1b_param_0(0.587) 
I0925 05:50:36.842680  4458 net.cpp:2312] ctx_conv1_param_0(0.783) 
I0925 05:50:36.842689  4458 net.cpp:2312] ctx_conv2_param_0(0.795) 
I0925 05:50:36.842699  4458 net.cpp:2312] ctx_conv3_param_0(0.785) 
I0925 05:50:36.842707  4458 net.cpp:2312] ctx_conv4_param_0(0.797) 
I0925 05:50:36.842716  4458 net.cpp:2312] ctx_final_param_0(0.33) 
I0925 05:50:36.842725  4458 net.cpp:2312] out3a_param_0(0.819) 
I0925 05:50:36.842733  4458 net.cpp:2312] out5a_param_0(0.817) 
I0925 05:50:36.842742  4458 net.cpp:2312] res2a_branch2a_param_0(0.729) 
I0925 05:50:36.842751  4458 net.cpp:2312] res2a_branch2b_param_0(0.565) 
I0925 05:50:36.842761  4458 net.cpp:2312] res3a_branch2a_param_0(0.76) 
I0925 05:50:36.842770  4458 net.cpp:2312] res3a_branch2b_param_0(0.693) 
I0925 05:50:36.842779  4458 net.cpp:2312] res4a_branch2a_param_0(0.797) 
I0925 05:50:36.842788  4458 net.cpp:2312] res4a_branch2b_param_0(0.762) 
I0925 05:50:36.842797  4458 net.cpp:2312] res5a_branch2a_param_0(0.817) 
I0925 05:50:36.842806  4458 net.cpp:2312] res5a_branch2b_param_0(0.819) 
I0925 05:50:36.842815  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.16549e+06/2.69117e+06) 0.805
I0925 05:50:36.842834  4458 solver.cpp:562] Iteration 28000, Testing net (#0)
I0925 05:50:41.601583  4458 blocking_queue.cpp:40] Data layer prefetch queue empty
I0925 05:50:44.433998  4520 data_reader.cpp:305] Starting prefetch of epoch 2
I0925 05:51:00.700193  4521 data_reader.cpp:305] Starting prefetch of epoch 1
I0925 05:51:01.225178  4458 solver.cpp:654]     Test net output #0: accuracy/top1 = 0.949356
I0925 05:51:01.225203  4458 solver.cpp:654]     Test net output #1: accuracy/top5 = 1
I0925 05:51:01.225209  4458 solver.cpp:654]     Test net output #2: loss = 0.144462 (* 1 = 0.144462 loss)
I0925 05:51:01.225239  4458 solver.cpp:265] [MultiGPU] Tests completed in 24.3817s
I0925 05:51:01.777281  4458 solver.cpp:314] Iteration 28000 (1.30597 iter/s, 76.5715s/100 iter), loss = 0.0766461
I0925 05:51:01.777312  4458 solver.cpp:336]     Train net output #0: loss = 0.0766462 (* 1 = 0.0766462 loss)
I0925 05:51:01.777318  4458 sgd_solver.cpp:136] Iteration 28000, lr = 0.01, m = 0.9
I0925 05:51:57.834731  4458 solver.cpp:314] Iteration 28100 (1.78393 iter/s, 56.0559s/100 iter), loss = 0.0635601
I0925 05:51:57.834794  4458 solver.cpp:336]     Train net output #0: loss = 0.0635601 (* 1 = 0.0635601 loss)
I0925 05:51:57.834803  4458 sgd_solver.cpp:136] Iteration 28100, lr = 0.01, m = 0.9
I0925 05:52:57.168658  4458 solver.cpp:314] Iteration 28200 (1.68542 iter/s, 59.3323s/100 iter), loss = 0.0668885
I0925 05:52:57.168905  4458 solver.cpp:336]     Train net output #0: loss = 0.0668886 (* 1 = 0.0668886 loss)
I0925 05:52:57.168915  4458 sgd_solver.cpp:136] Iteration 28200, lr = 0.01, m = 0.9
I0925 05:53:32.458401  4408 data_reader.cpp:305] Starting prefetch of epoch 23
I0925 05:53:32.458799  4462 data_reader.cpp:305] Starting prefetch of epoch 10
I0925 05:53:55.378314  4458 solver.cpp:314] Iteration 28300 (1.71798 iter/s, 58.208s/100 iter), loss = 0.0530453
I0925 05:53:55.378350  4458 solver.cpp:336]     Train net output #0: loss = 0.0530453 (* 1 = 0.0530453 loss)
I0925 05:53:55.378355  4458 sgd_solver.cpp:136] Iteration 28300, lr = 0.01, m = 0.9
I0925 05:54:50.841557  4458 solver.cpp:314] Iteration 28400 (1.80305 iter/s, 55.4617s/100 iter), loss = 0.0747693
I0925 05:54:50.841647  4458 solver.cpp:336]     Train net output #0: loss = 0.0747693 (* 1 = 0.0747693 loss)
I0925 05:54:50.841656  4458 sgd_solver.cpp:136] Iteration 28400, lr = 0.01, m = 0.9
I0925 05:55:46.620281  4458 solver.cpp:314] Iteration 28500 (1.79285 iter/s, 55.7772s/100 iter), loss = 0.0783681
I0925 05:55:46.624214  4458 solver.cpp:336]     Train net output #0: loss = 0.0783682 (* 1 = 0.0783682 loss)
I0925 05:55:46.624224  4458 sgd_solver.cpp:136] Iteration 28500, lr = 0.01, m = 0.9
I0925 05:56:09.764190  4463 blocking_queue.cpp:40] Waiting for datum
I0925 05:56:36.687988  4408 data_reader.cpp:305] Starting prefetch of epoch 24
I0925 05:56:41.708780  4458 solver.cpp:314] Iteration 28600 (1.81531 iter/s, 55.087s/100 iter), loss = 0.107891
I0925 05:56:41.708823  4458 solver.cpp:336]     Train net output #0: loss = 0.107891 (* 1 = 0.107891 loss)
I0925 05:56:41.708829  4458 sgd_solver.cpp:136] Iteration 28600, lr = 0.01, m = 0.9
I0925 05:57:35.531186  4458 solver.cpp:314] Iteration 28700 (1.85801 iter/s, 53.8209s/100 iter), loss = 0.10374
I0925 05:57:35.531277  4458 solver.cpp:336]     Train net output #0: loss = 0.10374 (* 1 = 0.10374 loss)
I0925 05:57:35.531287  4458 sgd_solver.cpp:136] Iteration 28700, lr = 0.01, m = 0.9
I0925 05:58:30.324322  4458 solver.cpp:314] Iteration 28800 (1.8251 iter/s, 54.7916s/100 iter), loss = 0.0571784
I0925 05:58:30.324376  4458 solver.cpp:336]     Train net output #0: loss = 0.0571785 (* 1 = 0.0571785 loss)
I0925 05:58:30.324383  4458 sgd_solver.cpp:136] Iteration 28800, lr = 0.01, m = 0.9
I0925 05:59:22.815994  4458 solver.cpp:314] Iteration 28900 (1.90512 iter/s, 52.4902s/100 iter), loss = 0.126835
I0925 05:59:22.824267  4458 solver.cpp:336]     Train net output #0: loss = 0.126835 (* 1 = 0.126835 loss)
I0925 05:59:22.824282  4458 sgd_solver.cpp:136] Iteration 28900, lr = 0.01, m = 0.9
I0925 05:59:33.157507  4466 data_reader.cpp:305] Starting prefetch of epoch 11
I0925 06:00:14.703920  4458 solver.cpp:368] Sparsity after update:
I0925 06:00:14.746232  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 06:00:14.746316  4458 net.cpp:2312] conv1a_param_0(0.255) 
I0925 06:00:14.746340  4458 net.cpp:2312] conv1b_param_0(0.587) 
I0925 06:00:14.746354  4458 net.cpp:2312] ctx_conv1_param_0(0.783) 
I0925 06:00:14.746363  4458 net.cpp:2312] ctx_conv2_param_0(0.795) 
I0925 06:00:14.746372  4458 net.cpp:2312] ctx_conv3_param_0(0.785) 
I0925 06:00:14.746381  4458 net.cpp:2312] ctx_conv4_param_0(0.797) 
I0925 06:00:14.746389  4458 net.cpp:2312] ctx_final_param_0(0.33) 
I0925 06:00:14.746398  4458 net.cpp:2312] out3a_param_0(0.819) 
I0925 06:00:14.746409  4458 net.cpp:2312] out5a_param_0(0.817) 
I0925 06:00:14.746420  4458 net.cpp:2312] res2a_branch2a_param_0(0.729) 
I0925 06:00:14.746430  4458 net.cpp:2312] res2a_branch2b_param_0(0.565) 
I0925 06:00:14.746440  4458 net.cpp:2312] res3a_branch2a_param_0(0.76) 
I0925 06:00:14.746449  4458 net.cpp:2312] res3a_branch2b_param_0(0.693) 
I0925 06:00:14.746457  4458 net.cpp:2312] res4a_branch2a_param_0(0.797) 
I0925 06:00:14.746466  4458 net.cpp:2312] res4a_branch2b_param_0(0.762) 
I0925 06:00:14.746475  4458 net.cpp:2312] res5a_branch2a_param_0(0.817) 
I0925 06:00:14.746484  4458 net.cpp:2312] res5a_branch2b_param_0(0.819) 
I0925 06:00:14.746492  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.16549e+06/2.69117e+06) 0.805
I0925 06:00:15.166795  4458 solver.cpp:314] Iteration 29000 (1.91024 iter/s, 52.3493s/100 iter), loss = 0.0609111
I0925 06:00:15.166824  4458 solver.cpp:336]     Train net output #0: loss = 0.0609111 (* 1 = 0.0609111 loss)
I0925 06:00:15.166831  4458 sgd_solver.cpp:136] Iteration 29000, lr = 0.01, m = 0.9
I0925 06:01:00.601137  4408 data_reader.cpp:305] Starting prefetch of epoch 25
I0925 06:01:08.468514  4458 solver.cpp:314] Iteration 29100 (1.87617 iter/s, 53.3002s/100 iter), loss = 0.0659939
I0925 06:01:08.468577  4458 solver.cpp:336]     Train net output #0: loss = 0.065994 (* 1 = 0.065994 loss)
I0925 06:01:08.468595  4458 sgd_solver.cpp:136] Iteration 29100, lr = 0.01, m = 0.9
I0925 06:02:01.512246  4458 solver.cpp:314] Iteration 29200 (1.88529 iter/s, 53.0422s/100 iter), loss = 0.0545614
I0925 06:02:01.523691  4458 solver.cpp:336]     Train net output #0: loss = 0.0545614 (* 1 = 0.0545614 loss)
I0925 06:02:01.523715  4458 sgd_solver.cpp:136] Iteration 29200, lr = 0.01, m = 0.9
I0925 06:02:53.820207  4458 solver.cpp:314] Iteration 29300 (1.91181 iter/s, 52.3065s/100 iter), loss = 0.0494807
I0925 06:02:53.836374  4458 solver.cpp:336]     Train net output #0: loss = 0.0494808 (* 1 = 0.0494808 loss)
I0925 06:02:53.836417  4458 sgd_solver.cpp:136] Iteration 29300, lr = 0.01, m = 0.9
I0925 06:03:46.906924  4458 solver.cpp:314] Iteration 29400 (1.88376 iter/s, 53.0852s/100 iter), loss = 0.0523796
I0925 06:03:46.907002  4458 solver.cpp:336]     Train net output #0: loss = 0.0523796 (* 1 = 0.0523796 loss)
I0925 06:03:46.907011  4458 sgd_solver.cpp:136] Iteration 29400, lr = 0.01, m = 0.9
I0925 06:03:56.186902  4462 data_reader.cpp:305] Starting prefetch of epoch 11
I0925 06:04:40.405635  4458 solver.cpp:314] Iteration 29500 (1.86926 iter/s, 53.4972s/100 iter), loss = 0.13632
I0925 06:04:40.426199  4458 solver.cpp:336]     Train net output #0: loss = 0.13632 (* 1 = 0.13632 loss)
I0925 06:04:40.426231  4458 sgd_solver.cpp:136] Iteration 29500, lr = 0.01, m = 0.9
I0925 06:05:31.115980  4458 solver.cpp:314] Iteration 29600 (1.97204 iter/s, 50.7089s/100 iter), loss = 0.0601081
I0925 06:05:31.116252  4458 solver.cpp:336]     Train net output #0: loss = 0.0601082 (* 1 = 0.0601082 loss)
I0925 06:05:31.116269  4458 sgd_solver.cpp:136] Iteration 29600, lr = 0.01, m = 0.9
I0925 06:06:19.728220  4458 solver.cpp:314] Iteration 29700 (2.05715 iter/s, 48.6109s/100 iter), loss = 0.0585901
I0925 06:06:19.732197  4458 solver.cpp:336]     Train net output #0: loss = 0.0585902 (* 1 = 0.0585902 loss)
I0925 06:06:19.732205  4458 sgd_solver.cpp:136] Iteration 29700, lr = 0.01, m = 0.9
I0925 06:06:44.662160  4410 data_reader.cpp:305] Starting prefetch of epoch 21
I0925 06:07:13.259434  4458 solver.cpp:314] Iteration 29800 (1.86812 iter/s, 53.5297s/100 iter), loss = 0.105968
I0925 06:07:13.259502  4458 solver.cpp:336]     Train net output #0: loss = 0.105968 (* 1 = 0.105968 loss)
I0925 06:07:13.259510  4458 sgd_solver.cpp:136] Iteration 29800, lr = 0.01, m = 0.9
I0925 06:08:06.379015  4458 solver.cpp:314] Iteration 29900 (1.8826 iter/s, 53.1181s/100 iter), loss = 0.0714626
I0925 06:08:06.379106  4458 solver.cpp:336]     Train net output #0: loss = 0.0714627 (* 1 = 0.0714627 loss)
I0925 06:08:06.379115  4458 sgd_solver.cpp:136] Iteration 29900, lr = 0.01, m = 0.9
I0925 06:08:13.082679  4408 data_reader.cpp:305] Starting prefetch of epoch 26
I0925 06:08:58.391932  4458 solver.cpp:824] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/cityscapes5_jsegnet21v2_iter_30000.caffemodel
I0925 06:08:59.365224  4458 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/cityscapes5_jsegnet21v2_iter_30000.solverstate
I0925 06:08:59.381239  4458 solver.cpp:368] Sparsity after update:
I0925 06:08:59.394229  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 06:08:59.394284  4458 net.cpp:2312] conv1a_param_0(0.255) 
I0925 06:08:59.394302  4458 net.cpp:2312] conv1b_param_0(0.587) 
I0925 06:08:59.394314  4458 net.cpp:2312] ctx_conv1_param_0(0.783) 
I0925 06:08:59.394323  4458 net.cpp:2312] ctx_conv2_param_0(0.795) 
I0925 06:08:59.394333  4458 net.cpp:2312] ctx_conv3_param_0(0.785) 
I0925 06:08:59.394343  4458 net.cpp:2312] ctx_conv4_param_0(0.797) 
I0925 06:08:59.394352  4458 net.cpp:2312] ctx_final_param_0(0.33) 
I0925 06:08:59.394368  4458 net.cpp:2312] out3a_param_0(0.819) 
I0925 06:08:59.394373  4458 net.cpp:2312] out5a_param_0(0.817) 
I0925 06:08:59.394377  4458 net.cpp:2312] res2a_branch2a_param_0(0.729) 
I0925 06:08:59.394381  4458 net.cpp:2312] res2a_branch2b_param_0(0.565) 
I0925 06:08:59.394383  4458 net.cpp:2312] res3a_branch2a_param_0(0.76) 
I0925 06:08:59.394387  4458 net.cpp:2312] res3a_branch2b_param_0(0.693) 
I0925 06:08:59.394389  4458 net.cpp:2312] res4a_branch2a_param_0(0.797) 
I0925 06:08:59.394393  4458 net.cpp:2312] res4a_branch2b_param_0(0.762) 
I0925 06:08:59.394398  4458 net.cpp:2312] res5a_branch2a_param_0(0.817) 
I0925 06:08:59.394402  4458 net.cpp:2312] res5a_branch2b_param_0(0.819) 
I0925 06:08:59.394404  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.16549e+06/2.69117e+06) 0.805
I0925 06:08:59.394417  4458 solver.cpp:562] Iteration 30000, Testing net (#0)
I0925 06:09:24.936214  4458 solver.cpp:654]     Test net output #0: accuracy/top1 = 0.955333
I0925 06:09:24.936296  4458 solver.cpp:654]     Test net output #1: accuracy/top5 = 1
I0925 06:09:24.936316  4458 solver.cpp:654]     Test net output #2: loss = 0.142986 (* 1 = 0.142986 loss)
I0925 06:09:24.936353  4458 solver.cpp:265] [MultiGPU] Tests completed in 25.5412s
I0925 06:09:25.260355  4540 sgd_solver.cpp:48] MultiStep Status: Iteration 30000, step = 1
I0925 06:09:25.264022  4541 sgd_solver.cpp:48] MultiStep Status: Iteration 30000, step = 1
I0925 06:09:25.264917  4538 sgd_solver.cpp:48] MultiStep Status: Iteration 30000, step = 1
I0925 06:09:25.435252  4458 solver.cpp:314] Iteration 30000 (1.26496 iter/s, 79.054s/100 iter), loss = 0.120818
I0925 06:09:25.435281  4458 solver.cpp:336]     Train net output #0: loss = 0.120818 (* 1 = 0.120818 loss)
I0925 06:09:25.435286  4458 sgd_solver.cpp:136] Iteration 30000, lr = 0.001, m = 0.9
I0925 06:10:13.872263  4461 data_reader.cpp:305] Starting prefetch of epoch 19
I0925 06:10:13.872263  4408 data_reader.cpp:305] Starting prefetch of epoch 27
I0925 06:10:25.264664  4458 solver.cpp:314] Iteration 30100 (1.67147 iter/s, 59.8277s/100 iter), loss = 0.0640846
I0925 06:10:25.264708  4458 solver.cpp:336]     Train net output #0: loss = 0.0640846 (* 1 = 0.0640846 loss)
I0925 06:10:25.264715  4458 sgd_solver.cpp:136] Iteration 30100, lr = 0.001, m = 0.9
I0925 06:11:19.346577  4458 solver.cpp:314] Iteration 30200 (1.8491 iter/s, 54.0804s/100 iter), loss = 0.0591502
I0925 06:11:19.346668  4458 solver.cpp:336]     Train net output #0: loss = 0.0591503 (* 1 = 0.0591503 loss)
I0925 06:11:19.346680  4458 sgd_solver.cpp:136] Iteration 30200, lr = 0.001, m = 0.9
I0925 06:12:13.297255  4458 solver.cpp:314] Iteration 30300 (1.8536 iter/s, 53.9492s/100 iter), loss = 0.0738143
I0925 06:12:13.297309  4458 solver.cpp:336]     Train net output #0: loss = 0.0738143 (* 1 = 0.0738143 loss)
I0925 06:12:13.297317  4458 sgd_solver.cpp:136] Iteration 30300, lr = 0.001, m = 0.9
I0925 06:13:09.733108  4458 solver.cpp:314] Iteration 30400 (1.77198 iter/s, 56.4341s/100 iter), loss = 0.0668825
I0925 06:13:09.733177  4458 solver.cpp:336]     Train net output #0: loss = 0.0668826 (* 1 = 0.0668826 loss)
I0925 06:13:09.733186  4458 sgd_solver.cpp:136] Iteration 30400, lr = 0.001, m = 0.9
I0925 06:13:14.651329  4461 data_reader.cpp:305] Starting prefetch of epoch 20
I0925 06:14:06.124336  4458 solver.cpp:314] Iteration 30500 (1.77338 iter/s, 56.3896s/100 iter), loss = 0.0829897
I0925 06:14:06.124446  4458 solver.cpp:336]     Train net output #0: loss = 0.0829898 (* 1 = 0.0829898 loss)
I0925 06:14:06.124464  4458 sgd_solver.cpp:136] Iteration 30500, lr = 0.001, m = 0.9
I0925 06:15:00.530500  4458 solver.cpp:314] Iteration 30600 (1.83808 iter/s, 54.4046s/100 iter), loss = 0.050312
I0925 06:15:00.530594  4458 solver.cpp:336]     Train net output #0: loss = 0.0503121 (* 1 = 0.0503121 loss)
I0925 06:15:00.530607  4458 sgd_solver.cpp:136] Iteration 30600, lr = 0.001, m = 0.9
I0925 06:15:54.189664  4458 solver.cpp:314] Iteration 30700 (1.86367 iter/s, 53.6577s/100 iter), loss = 0.145647
I0925 06:15:54.189741  4458 solver.cpp:336]     Train net output #0: loss = 0.145647 (* 1 = 0.145647 loss)
I0925 06:15:54.189749  4458 sgd_solver.cpp:136] Iteration 30700, lr = 0.001, m = 0.9
I0925 06:16:14.148381  4464 data_reader.cpp:305] Starting prefetch of epoch 19
I0925 06:16:45.159286  4458 solver.cpp:314] Iteration 30800 (1.96201 iter/s, 50.9682s/100 iter), loss = 0.0584624
I0925 06:16:45.159348  4458 solver.cpp:336]     Train net output #0: loss = 0.0584625 (* 1 = 0.0584625 loss)
I0925 06:16:45.159356  4458 sgd_solver.cpp:136] Iteration 30800, lr = 0.001, m = 0.9
I0925 06:17:32.298533  4458 solver.cpp:314] Iteration 30900 (2.12143 iter/s, 47.1379s/100 iter), loss = 0.0767158
I0925 06:17:32.298601  4458 solver.cpp:336]     Train net output #0: loss = 0.0767158 (* 1 = 0.0767158 loss)
I0925 06:17:32.298610  4458 sgd_solver.cpp:136] Iteration 30900, lr = 0.001, m = 0.9
I0925 06:18:24.128341  4458 solver.cpp:368] Sparsity after update:
I0925 06:18:24.159463  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 06:18:24.159489  4458 net.cpp:2312] conv1a_param_0(0.255) 
I0925 06:18:24.159499  4458 net.cpp:2312] conv1b_param_0(0.587) 
I0925 06:18:24.159503  4458 net.cpp:2312] ctx_conv1_param_0(0.783) 
I0925 06:18:24.159507  4458 net.cpp:2312] ctx_conv2_param_0(0.795) 
I0925 06:18:24.159509  4458 net.cpp:2312] ctx_conv3_param_0(0.785) 
I0925 06:18:24.159512  4458 net.cpp:2312] ctx_conv4_param_0(0.797) 
I0925 06:18:24.159515  4458 net.cpp:2312] ctx_final_param_0(0.33) 
I0925 06:18:24.159518  4458 net.cpp:2312] out3a_param_0(0.819) 
I0925 06:18:24.159521  4458 net.cpp:2312] out5a_param_0(0.817) 
I0925 06:18:24.159525  4458 net.cpp:2312] res2a_branch2a_param_0(0.729) 
I0925 06:18:24.159528  4458 net.cpp:2312] res2a_branch2b_param_0(0.565) 
I0925 06:18:24.159531  4458 net.cpp:2312] res3a_branch2a_param_0(0.76) 
I0925 06:18:24.159534  4458 net.cpp:2312] res3a_branch2b_param_0(0.693) 
I0925 06:18:24.159538  4458 net.cpp:2312] res4a_branch2a_param_0(0.797) 
I0925 06:18:24.159540  4458 net.cpp:2312] res4a_branch2b_param_0(0.762) 
I0925 06:18:24.159543  4458 net.cpp:2312] res5a_branch2a_param_0(0.817) 
I0925 06:18:24.159546  4458 net.cpp:2312] res5a_branch2b_param_0(0.819) 
I0925 06:18:24.159549  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.16549e+06/2.69117e+06) 0.805
I0925 06:18:24.640719  4458 solver.cpp:314] Iteration 31000 (1.91056 iter/s, 52.3407s/100 iter), loss = 0.0531785
I0925 06:18:24.640743  4458 solver.cpp:336]     Train net output #0: loss = 0.0531785 (* 1 = 0.0531785 loss)
I0925 06:18:24.640748  4458 sgd_solver.cpp:136] Iteration 31000, lr = 0.001, m = 0.9
I0925 06:19:00.918943  4410 data_reader.cpp:305] Starting prefetch of epoch 22
I0925 06:19:17.458760  4458 solver.cpp:314] Iteration 31100 (1.89335 iter/s, 52.8165s/100 iter), loss = 0.0597157
I0925 06:19:17.458818  4458 solver.cpp:336]     Train net output #0: loss = 0.0597157 (* 1 = 0.0597157 loss)
I0925 06:19:17.458829  4458 sgd_solver.cpp:136] Iteration 31100, lr = 0.001, m = 0.9
I0925 06:20:08.734066  4458 solver.cpp:314] Iteration 31200 (1.95031 iter/s, 51.2739s/100 iter), loss = 0.105834
I0925 06:20:08.734150  4458 solver.cpp:336]     Train net output #0: loss = 0.105834 (* 1 = 0.105834 loss)
I0925 06:20:08.734158  4458 sgd_solver.cpp:136] Iteration 31200, lr = 0.001, m = 0.9
I0925 06:20:27.162634  4461 data_reader.cpp:305] Starting prefetch of epoch 21
I0925 06:21:02.586762  4458 solver.cpp:314] Iteration 31300 (1.85697 iter/s, 53.8512s/100 iter), loss = 0.091652
I0925 06:21:02.586829  4458 solver.cpp:336]     Train net output #0: loss = 0.091652 (* 1 = 0.091652 loss)
I0925 06:21:02.586838  4458 sgd_solver.cpp:136] Iteration 31300, lr = 0.001, m = 0.9
I0925 06:21:56.475029  4458 solver.cpp:314] Iteration 31400 (1.85574 iter/s, 53.8868s/100 iter), loss = 0.0533719
I0925 06:21:56.504218  4458 solver.cpp:336]     Train net output #0: loss = 0.0533719 (* 1 = 0.0533719 loss)
I0925 06:21:56.504236  4458 sgd_solver.cpp:136] Iteration 31400, lr = 0.001, m = 0.9
I0925 06:22:49.593372  4458 solver.cpp:314] Iteration 31500 (1.88264 iter/s, 53.1168s/100 iter), loss = 0.0524048
I0925 06:22:49.594671  4458 solver.cpp:336]     Train net output #0: loss = 0.0524048 (* 1 = 0.0524048 loss)
I0925 06:22:49.594691  4458 sgd_solver.cpp:136] Iteration 31500, lr = 0.001, m = 0.9
I0925 06:23:24.519946  4462 data_reader.cpp:305] Starting prefetch of epoch 12
I0925 06:23:43.309098  4458 solver.cpp:314] Iteration 31600 (1.8617 iter/s, 53.7142s/100 iter), loss = 0.187083
I0925 06:23:43.309128  4458 solver.cpp:336]     Train net output #0: loss = 0.187083 (* 1 = 0.187083 loss)
I0925 06:23:43.309134  4458 sgd_solver.cpp:136] Iteration 31600, lr = 0.001, m = 0.9
I0925 06:24:37.266871  4458 solver.cpp:314] Iteration 31700 (1.85335 iter/s, 53.9563s/100 iter), loss = 0.0542542
I0925 06:24:37.266939  4458 solver.cpp:336]     Train net output #0: loss = 0.0542542 (* 1 = 0.0542542 loss)
I0925 06:24:37.266948  4458 sgd_solver.cpp:136] Iteration 31700, lr = 0.001, m = 0.9
I0925 06:25:30.064214  4458 solver.cpp:314] Iteration 31800 (1.89409 iter/s, 52.7959s/100 iter), loss = 0.071447
I0925 06:25:30.069766  4458 solver.cpp:336]     Train net output #0: loss = 0.071447 (* 1 = 0.071447 loss)
I0925 06:25:30.069793  4458 sgd_solver.cpp:136] Iteration 31800, lr = 0.001, m = 0.9
I0925 06:26:21.924191  4410 data_reader.cpp:305] Starting prefetch of epoch 23
I0925 06:26:24.434109  4458 solver.cpp:314] Iteration 31900 (1.8393 iter/s, 54.3684s/100 iter), loss = 0.03668
I0925 06:26:24.434142  4458 solver.cpp:336]     Train net output #0: loss = 0.0366801 (* 1 = 0.0366801 loss)
I0925 06:26:24.434149  4458 sgd_solver.cpp:136] Iteration 31900, lr = 0.001, m = 0.9
I0925 06:27:16.781307  4458 solver.cpp:368] Sparsity after update:
I0925 06:27:16.819780  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 06:27:16.819991  4458 net.cpp:2312] conv1a_param_0(0.255) 
I0925 06:27:16.820091  4458 net.cpp:2312] conv1b_param_0(0.587) 
I0925 06:27:16.820186  4458 net.cpp:2312] ctx_conv1_param_0(0.783) 
I0925 06:27:16.820279  4458 net.cpp:2312] ctx_conv2_param_0(0.795) 
I0925 06:27:16.820369  4458 net.cpp:2312] ctx_conv3_param_0(0.785) 
I0925 06:27:16.820457  4458 net.cpp:2312] ctx_conv4_param_0(0.797) 
I0925 06:27:16.820547  4458 net.cpp:2312] ctx_final_param_0(0.33) 
I0925 06:27:16.820639  4458 net.cpp:2312] out3a_param_0(0.819) 
I0925 06:27:16.820729  4458 net.cpp:2312] out5a_param_0(0.817) 
I0925 06:27:16.820814  4458 net.cpp:2312] res2a_branch2a_param_0(0.729) 
I0925 06:27:16.820904  4458 net.cpp:2312] res2a_branch2b_param_0(0.565) 
I0925 06:27:16.820994  4458 net.cpp:2312] res3a_branch2a_param_0(0.76) 
I0925 06:27:16.821087  4458 net.cpp:2312] res3a_branch2b_param_0(0.693) 
I0925 06:27:16.821187  4458 net.cpp:2312] res4a_branch2a_param_0(0.797) 
I0925 06:27:16.821280  4458 net.cpp:2312] res4a_branch2b_param_0(0.762) 
I0925 06:27:16.821372  4458 net.cpp:2312] res5a_branch2a_param_0(0.817) 
I0925 06:27:16.821460  4458 net.cpp:2312] res5a_branch2b_param_0(0.819) 
I0925 06:27:16.821547  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.16549e+06/2.69117e+06) 0.805
I0925 06:27:16.821660  4458 solver.cpp:562] Iteration 32000, Testing net (#0)
I0925 06:27:42.419854  4524 data_reader.cpp:305] Starting prefetch of epoch 3
I0925 06:27:43.099148  4458 solver.cpp:654]     Test net output #0: accuracy/top1 = 0.953594
I0925 06:27:43.099201  4458 solver.cpp:654]     Test net output #1: accuracy/top5 = 1
I0925 06:27:43.099220  4458 solver.cpp:654]     Test net output #2: loss = 0.133185 (* 1 = 0.133185 loss)
I0925 06:27:43.099259  4458 solver.cpp:265] [MultiGPU] Tests completed in 26.2769s
I0925 06:27:43.671567  4458 solver.cpp:314] Iteration 32000 (1.26206 iter/s, 79.2352s/100 iter), loss = 0.0568757
I0925 06:27:43.671600  4458 solver.cpp:336]     Train net output #0: loss = 0.0568757 (* 1 = 0.0568757 loss)
I0925 06:27:43.671607  4458 sgd_solver.cpp:136] Iteration 32000, lr = 0.001, m = 0.9
I0925 06:28:30.176934  4463 blocking_queue.cpp:40] Waiting for datum
I0925 06:28:46.152709  4458 solver.cpp:314] Iteration 32100 (1.60053 iter/s, 62.4794s/100 iter), loss = 0.0830456
I0925 06:28:46.152732  4458 solver.cpp:336]     Train net output #0: loss = 0.0830456 (* 1 = 0.0830456 loss)
I0925 06:28:46.152737  4458 sgd_solver.cpp:136] Iteration 32100, lr = 0.001, m = 0.9
I0925 06:30:00.204941  4458 solver.cpp:314] Iteration 32200 (1.35044 iter/s, 74.0501s/100 iter), loss = 0.0565249
I0925 06:30:00.204998  4458 solver.cpp:336]     Train net output #0: loss = 0.056525 (* 1 = 0.056525 loss)
I0925 06:30:00.205003  4458 sgd_solver.cpp:136] Iteration 32200, lr = 0.001, m = 0.9
I0925 06:30:10.034452  4410 data_reader.cpp:305] Starting prefetch of epoch 24
I0925 06:30:10.034624  4466 data_reader.cpp:305] Starting prefetch of epoch 12
I0925 06:30:43.585656  4458 solver.cpp:314] Iteration 32300 (2.30524 iter/s, 43.3795s/100 iter), loss = 0.0686682
I0925 06:30:43.585764  4458 solver.cpp:336]     Train net output #0: loss = 0.0686683 (* 1 = 0.0686683 loss)
I0925 06:30:43.585772  4458 sgd_solver.cpp:136] Iteration 32300, lr = 0.001, m = 0.9
I0925 06:31:06.059866  4458 solver.cpp:314] Iteration 32400 (4.44967 iter/s, 22.4736s/100 iter), loss = 0.0611108
I0925 06:31:06.059887  4458 solver.cpp:336]     Train net output #0: loss = 0.0611109 (* 1 = 0.0611109 loss)
I0925 06:31:06.059891  4458 sgd_solver.cpp:136] Iteration 32400, lr = 0.001, m = 0.9
I0925 06:31:23.948536  4458 solver.cpp:314] Iteration 32500 (5.59029 iter/s, 17.8882s/100 iter), loss = 0.0614675
I0925 06:31:23.948585  4458 solver.cpp:336]     Train net output #0: loss = 0.0614676 (* 1 = 0.0614676 loss)
I0925 06:31:23.948591  4458 sgd_solver.cpp:136] Iteration 32500, lr = 0.001, m = 0.9
I0925 06:31:34.115361  4462 data_reader.cpp:305] Starting prefetch of epoch 13
I0925 06:31:41.991365  4458 solver.cpp:314] Iteration 32600 (5.54253 iter/s, 18.0423s/100 iter), loss = 0.0471066
I0925 06:31:41.991394  4458 solver.cpp:336]     Train net output #0: loss = 0.0471066 (* 1 = 0.0471066 loss)
I0925 06:31:41.991400  4458 sgd_solver.cpp:136] Iteration 32600, lr = 0.001, m = 0.9
I0925 06:32:00.084717  4458 solver.cpp:314] Iteration 32700 (5.52705 iter/s, 18.0928s/100 iter), loss = 0.063888
I0925 06:32:00.084760  4458 solver.cpp:336]     Train net output #0: loss = 0.0638881 (* 1 = 0.0638881 loss)
I0925 06:32:00.084765  4458 sgd_solver.cpp:136] Iteration 32700, lr = 0.001, m = 0.9
I0925 06:32:18.192592  4458 solver.cpp:314] Iteration 32800 (5.52262 iter/s, 18.1074s/100 iter), loss = 0.0649429
I0925 06:32:18.192615  4458 solver.cpp:336]     Train net output #0: loss = 0.064943 (* 1 = 0.064943 loss)
I0925 06:32:18.192620  4458 sgd_solver.cpp:136] Iteration 32800, lr = 0.001, m = 0.9
I0925 06:32:34.045266  4466 data_reader.cpp:305] Starting prefetch of epoch 13
I0925 06:32:36.386380  4458 solver.cpp:314] Iteration 32900 (5.49654 iter/s, 18.1933s/100 iter), loss = 0.0795003
I0925 06:32:36.386404  4458 solver.cpp:336]     Train net output #0: loss = 0.0795003 (* 1 = 0.0795003 loss)
I0925 06:32:36.386409  4458 sgd_solver.cpp:136] Iteration 32900, lr = 0.001, m = 0.9
I0925 06:32:54.349313  4458 solver.cpp:368] Sparsity after update:
I0925 06:32:54.359310  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 06:32:54.359326  4458 net.cpp:2312] conv1a_param_0(0.255) 
I0925 06:32:54.359334  4458 net.cpp:2312] conv1b_param_0(0.587) 
I0925 06:32:54.359338  4458 net.cpp:2312] ctx_conv1_param_0(0.783) 
I0925 06:32:54.359341  4458 net.cpp:2312] ctx_conv2_param_0(0.795) 
I0925 06:32:54.359344  4458 net.cpp:2312] ctx_conv3_param_0(0.785) 
I0925 06:32:54.359347  4458 net.cpp:2312] ctx_conv4_param_0(0.797) 
I0925 06:32:54.359350  4458 net.cpp:2312] ctx_final_param_0(0.33) 
I0925 06:32:54.359355  4458 net.cpp:2312] out3a_param_0(0.819) 
I0925 06:32:54.359359  4458 net.cpp:2312] out5a_param_0(0.817) 
I0925 06:32:54.359362  4458 net.cpp:2312] res2a_branch2a_param_0(0.729) 
I0925 06:32:54.359366  4458 net.cpp:2312] res2a_branch2b_param_0(0.565) 
I0925 06:32:54.359369  4458 net.cpp:2312] res3a_branch2a_param_0(0.76) 
I0925 06:32:54.359372  4458 net.cpp:2312] res3a_branch2b_param_0(0.693) 
I0925 06:32:54.359375  4458 net.cpp:2312] res4a_branch2a_param_0(0.797) 
I0925 06:32:54.359378  4458 net.cpp:2312] res4a_branch2b_param_0(0.762) 
I0925 06:32:54.359382  4458 net.cpp:2312] res5a_branch2a_param_0(0.817) 
I0925 06:32:54.359385  4458 net.cpp:2312] res5a_branch2b_param_0(0.819) 
I0925 06:32:54.359388  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.16549e+06/2.69117e+06) 0.805
I0925 06:32:54.527209  4458 solver.cpp:314] Iteration 33000 (5.51258 iter/s, 18.1403s/100 iter), loss = 0.0559169
I0925 06:32:54.527230  4458 solver.cpp:336]     Train net output #0: loss = 0.055917 (* 1 = 0.055917 loss)
I0925 06:32:54.527235  4458 sgd_solver.cpp:136] Iteration 33000, lr = 0.001, m = 0.9
I0925 06:33:03.948532  4462 data_reader.cpp:305] Starting prefetch of epoch 14
I0925 06:33:12.621947  4458 solver.cpp:314] Iteration 33100 (5.52663 iter/s, 18.0942s/100 iter), loss = 0.0707241
I0925 06:33:12.622045  4458 solver.cpp:336]     Train net output #0: loss = 0.0707241 (* 1 = 0.0707241 loss)
I0925 06:33:12.622051  4458 sgd_solver.cpp:136] Iteration 33100, lr = 0.001, m = 0.9
I0925 06:33:30.738703  4458 solver.cpp:314] Iteration 33200 (5.51991 iter/s, 18.1162s/100 iter), loss = 0.0842716
I0925 06:33:30.738729  4458 solver.cpp:336]     Train net output #0: loss = 0.0842717 (* 1 = 0.0842717 loss)
I0925 06:33:30.738732  4458 sgd_solver.cpp:136] Iteration 33200, lr = 0.001, m = 0.9
I0925 06:33:48.763129  4458 solver.cpp:314] Iteration 33300 (5.54818 iter/s, 18.0239s/100 iter), loss = 0.0655261
I0925 06:33:48.763185  4458 solver.cpp:336]     Train net output #0: loss = 0.0655262 (* 1 = 0.0655262 loss)
I0925 06:33:48.763192  4458 sgd_solver.cpp:136] Iteration 33300, lr = 0.001, m = 0.9
I0925 06:34:03.878854  4464 data_reader.cpp:305] Starting prefetch of epoch 20
I0925 06:34:06.991906  4458 solver.cpp:314] Iteration 33400 (5.48599 iter/s, 18.2283s/100 iter), loss = 0.0675245
I0925 06:34:06.991935  4458 solver.cpp:336]     Train net output #0: loss = 0.0675246 (* 1 = 0.0675246 loss)
I0925 06:34:06.991941  4458 sgd_solver.cpp:136] Iteration 33400, lr = 0.001, m = 0.9
I0925 06:34:25.114727  4458 solver.cpp:314] Iteration 33500 (5.51806 iter/s, 18.1223s/100 iter), loss = 0.073664
I0925 06:34:25.114827  4458 solver.cpp:336]     Train net output #0: loss = 0.0736641 (* 1 = 0.0736641 loss)
I0925 06:34:25.114836  4458 sgd_solver.cpp:136] Iteration 33500, lr = 0.001, m = 0.9
I0925 06:34:43.181334  4458 solver.cpp:314] Iteration 33600 (5.53523 iter/s, 18.0661s/100 iter), loss = 0.0597817
I0925 06:34:43.181357  4458 solver.cpp:336]     Train net output #0: loss = 0.0597818 (* 1 = 0.0597818 loss)
I0925 06:34:43.181361  4458 sgd_solver.cpp:136] Iteration 33600, lr = 0.001, m = 0.9
I0925 06:35:01.316781  4458 solver.cpp:314] Iteration 33700 (5.51422 iter/s, 18.1349s/100 iter), loss = 0.103247
I0925 06:35:01.318367  4458 solver.cpp:336]     Train net output #0: loss = 0.103247 (* 1 = 0.103247 loss)
I0925 06:35:01.318403  4458 sgd_solver.cpp:136] Iteration 33700, lr = 0.001, m = 0.9
I0925 06:35:03.710366  4466 data_reader.cpp:305] Starting prefetch of epoch 14
I0925 06:35:19.505023  4458 solver.cpp:314] Iteration 33800 (5.49821 iter/s, 18.1877s/100 iter), loss = 0.0593545
I0925 06:35:19.505048  4458 solver.cpp:336]     Train net output #0: loss = 0.0593546 (* 1 = 0.0593546 loss)
I0925 06:35:19.505053  4458 sgd_solver.cpp:136] Iteration 33800, lr = 0.001, m = 0.9
I0925 06:35:37.643288  4458 solver.cpp:314] Iteration 33900 (5.51336 iter/s, 18.1378s/100 iter), loss = 0.0522851
I0925 06:35:37.643337  4458 solver.cpp:336]     Train net output #0: loss = 0.0522852 (* 1 = 0.0522852 loss)
I0925 06:35:37.643342  4458 sgd_solver.cpp:136] Iteration 33900, lr = 0.001, m = 0.9
I0925 06:35:55.642598  4458 solver.cpp:368] Sparsity after update:
I0925 06:35:55.644201  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 06:35:55.644207  4458 net.cpp:2312] conv1a_param_0(0.255) 
I0925 06:35:55.644213  4458 net.cpp:2312] conv1b_param_0(0.587) 
I0925 06:35:55.644215  4458 net.cpp:2312] ctx_conv1_param_0(0.783) 
I0925 06:35:55.644217  4458 net.cpp:2312] ctx_conv2_param_0(0.795) 
I0925 06:35:55.644219  4458 net.cpp:2312] ctx_conv3_param_0(0.785) 
I0925 06:35:55.644222  4458 net.cpp:2312] ctx_conv4_param_0(0.797) 
I0925 06:35:55.644223  4458 net.cpp:2312] ctx_final_param_0(0.33) 
I0925 06:35:55.644225  4458 net.cpp:2312] out3a_param_0(0.819) 
I0925 06:35:55.644227  4458 net.cpp:2312] out5a_param_0(0.817) 
I0925 06:35:55.644229  4458 net.cpp:2312] res2a_branch2a_param_0(0.729) 
I0925 06:35:55.644230  4458 net.cpp:2312] res2a_branch2b_param_0(0.565) 
I0925 06:35:55.644232  4458 net.cpp:2312] res3a_branch2a_param_0(0.76) 
I0925 06:35:55.644234  4458 net.cpp:2312] res3a_branch2b_param_0(0.693) 
I0925 06:35:55.644237  4458 net.cpp:2312] res4a_branch2a_param_0(0.797) 
I0925 06:35:55.644238  4458 net.cpp:2312] res4a_branch2b_param_0(0.762) 
I0925 06:35:55.644240  4458 net.cpp:2312] res5a_branch2a_param_0(0.817) 
I0925 06:35:55.644243  4458 net.cpp:2312] res5a_branch2b_param_0(0.819) 
I0925 06:35:55.644243  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.16549e+06/2.69117e+06) 0.805
I0925 06:35:55.646298  4458 solver.cpp:562] Iteration 34000, Testing net (#0)
I0925 06:36:16.723371  4520 data_reader.cpp:305] Starting prefetch of epoch 3
I0925 06:36:27.010118  4458 solver.cpp:654]     Test net output #0: accuracy/top1 = 0.95643
I0925 06:36:27.010141  4458 solver.cpp:654]     Test net output #1: accuracy/top5 = 1
I0925 06:36:27.010146  4458 solver.cpp:654]     Test net output #2: loss = 0.151513 (* 1 = 0.151513 loss)
I0925 06:36:27.010174  4458 solver.cpp:265] [MultiGPU] Tests completed in 31.365s
I0925 06:36:27.189718  4458 solver.cpp:314] Iteration 34000 (2.01837 iter/s, 49.545s/100 iter), loss = 0.13442
I0925 06:36:27.189740  4458 solver.cpp:336]     Train net output #0: loss = 0.13442 (* 1 = 0.13442 loss)
I0925 06:36:27.189745  4458 sgd_solver.cpp:136] Iteration 34000, lr = 0.001, m = 0.9
I0925 06:36:34.995234  4462 data_reader.cpp:305] Starting prefetch of epoch 15
I0925 06:36:44.879588  4458 solver.cpp:314] Iteration 34100 (5.65311 iter/s, 17.6894s/100 iter), loss = 0.0503117
I0925 06:36:44.879611  4458 solver.cpp:336]     Train net output #0: loss = 0.0503118 (* 1 = 0.0503118 loss)
I0925 06:36:44.879616  4458 sgd_solver.cpp:136] Iteration 34100, lr = 0.001, m = 0.9
I0925 06:37:02.829443  4458 solver.cpp:314] Iteration 34200 (5.57123 iter/s, 17.9493s/100 iter), loss = 0.0474099
I0925 06:37:02.829638  4458 solver.cpp:336]     Train net output #0: loss = 0.04741 (* 1 = 0.04741 loss)
I0925 06:37:02.829644  4458 sgd_solver.cpp:136] Iteration 34200, lr = 0.001, m = 0.9
I0925 06:37:20.799959  4458 solver.cpp:314] Iteration 34300 (5.56483 iter/s, 17.97s/100 iter), loss = 0.0524565
I0925 06:37:20.799980  4458 solver.cpp:336]     Train net output #0: loss = 0.0524566 (* 1 = 0.0524566 loss)
I0925 06:37:20.799984  4458 sgd_solver.cpp:136] Iteration 34300, lr = 0.001, m = 0.9
I0925 06:37:34.261976  4462 data_reader.cpp:305] Starting prefetch of epoch 16
I0925 06:37:38.962244  4458 solver.cpp:314] Iteration 34400 (5.50607 iter/s, 18.1618s/100 iter), loss = 0.0577998
I0925 06:37:38.962266  4458 solver.cpp:336]     Train net output #0: loss = 0.0577999 (* 1 = 0.0577999 loss)
I0925 06:37:38.962271  4458 sgd_solver.cpp:136] Iteration 34400, lr = 0.001, m = 0.9
I0925 06:37:57.200729  4458 solver.cpp:314] Iteration 34500 (5.48307 iter/s, 18.238s/100 iter), loss = 0.0709547
I0925 06:37:57.200753  4458 solver.cpp:336]     Train net output #0: loss = 0.0709548 (* 1 = 0.0709548 loss)
I0925 06:37:57.200757  4458 sgd_solver.cpp:136] Iteration 34500, lr = 0.001, m = 0.9
I0925 06:38:15.225811  4458 solver.cpp:314] Iteration 34600 (5.54798 iter/s, 18.0246s/100 iter), loss = 0.0841657
I0925 06:38:15.225917  4458 solver.cpp:336]     Train net output #0: loss = 0.0841658 (* 1 = 0.0841658 loss)
I0925 06:38:15.225924  4458 sgd_solver.cpp:136] Iteration 34600, lr = 0.001, m = 0.9
I0925 06:38:33.268122  4458 solver.cpp:314] Iteration 34700 (5.54268 iter/s, 18.0418s/100 iter), loss = 0.045914
I0925 06:38:33.268147  4458 solver.cpp:336]     Train net output #0: loss = 0.0459141 (* 1 = 0.0459141 loss)
I0925 06:38:33.268151  4458 sgd_solver.cpp:136] Iteration 34700, lr = 0.001, m = 0.9
I0925 06:38:34.209228  4466 data_reader.cpp:305] Starting prefetch of epoch 15
I0925 06:38:51.289644  4458 solver.cpp:314] Iteration 34800 (5.54908 iter/s, 18.021s/100 iter), loss = 0.0586625
I0925 06:38:51.289744  4458 solver.cpp:336]     Train net output #0: loss = 0.0586626 (* 1 = 0.0586626 loss)
I0925 06:38:51.289750  4458 sgd_solver.cpp:136] Iteration 34800, lr = 0.001, m = 0.9
I0925 06:39:09.524422  4458 solver.cpp:314] Iteration 34900 (5.48418 iter/s, 18.2343s/100 iter), loss = 0.0733229
I0925 06:39:09.524448  4458 solver.cpp:336]     Train net output #0: loss = 0.073323 (* 1 = 0.073323 loss)
I0925 06:39:09.524452  4458 sgd_solver.cpp:136] Iteration 34900, lr = 0.001, m = 0.9
I0925 06:39:27.434346  4458 solver.cpp:368] Sparsity after update:
I0925 06:39:27.443482  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 06:39:27.443496  4458 net.cpp:2312] conv1a_param_0(0.255) 
I0925 06:39:27.443505  4458 net.cpp:2312] conv1b_param_0(0.587) 
I0925 06:39:27.443506  4458 net.cpp:2312] ctx_conv1_param_0(0.783) 
I0925 06:39:27.443508  4458 net.cpp:2312] ctx_conv2_param_0(0.795) 
I0925 06:39:27.443511  4458 net.cpp:2312] ctx_conv3_param_0(0.785) 
I0925 06:39:27.443512  4458 net.cpp:2312] ctx_conv4_param_0(0.797) 
I0925 06:39:27.443514  4458 net.cpp:2312] ctx_final_param_0(0.33) 
I0925 06:39:27.443516  4458 net.cpp:2312] out3a_param_0(0.819) 
I0925 06:39:27.443517  4458 net.cpp:2312] out5a_param_0(0.817) 
I0925 06:39:27.443519  4458 net.cpp:2312] res2a_branch2a_param_0(0.729) 
I0925 06:39:27.443521  4458 net.cpp:2312] res2a_branch2b_param_0(0.565) 
I0925 06:39:27.443523  4458 net.cpp:2312] res3a_branch2a_param_0(0.76) 
I0925 06:39:27.443526  4458 net.cpp:2312] res3a_branch2b_param_0(0.693) 
I0925 06:39:27.443527  4458 net.cpp:2312] res4a_branch2a_param_0(0.797) 
I0925 06:39:27.443529  4458 net.cpp:2312] res4a_branch2b_param_0(0.762) 
I0925 06:39:27.443531  4458 net.cpp:2312] res5a_branch2a_param_0(0.817) 
I0925 06:39:27.443533  4458 net.cpp:2312] res5a_branch2b_param_0(0.819) 
I0925 06:39:27.443536  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.16549e+06/2.69117e+06) 0.805
I0925 06:39:27.614285  4458 solver.cpp:314] Iteration 35000 (5.52811 iter/s, 18.0894s/100 iter), loss = 0.062246
I0925 06:39:27.614308  4458 solver.cpp:336]     Train net output #0: loss = 0.0622461 (* 1 = 0.0622461 loss)
I0925 06:39:27.614313  4458 sgd_solver.cpp:136] Iteration 35000, lr = 0.001, m = 0.9
I0925 06:39:33.936740  4464 data_reader.cpp:305] Starting prefetch of epoch 21
I0925 06:39:45.677812  4458 solver.cpp:314] Iteration 35100 (5.53617 iter/s, 18.063s/100 iter), loss = 0.055221
I0925 06:39:45.677841  4458 solver.cpp:336]     Train net output #0: loss = 0.0552211 (* 1 = 0.0552211 loss)
I0925 06:39:45.677848  4458 sgd_solver.cpp:136] Iteration 35100, lr = 0.001, m = 0.9
I0925 06:40:03.850540  4458 solver.cpp:314] Iteration 35200 (5.50291 iter/s, 18.1722s/100 iter), loss = 0.0496707
I0925 06:40:03.850628  4458 solver.cpp:336]     Train net output #0: loss = 0.0496708 (* 1 = 0.0496708 loss)
I0925 06:40:03.850636  4458 sgd_solver.cpp:136] Iteration 35200, lr = 0.001, m = 0.9
I0925 06:40:04.062681  4461 data_reader.cpp:305] Starting prefetch of epoch 22
I0925 06:40:22.010663  4458 solver.cpp:314] Iteration 35300 (5.50673 iter/s, 18.1596s/100 iter), loss = 0.0847447
I0925 06:40:22.010686  4458 solver.cpp:336]     Train net output #0: loss = 0.0847448 (* 1 = 0.0847448 loss)
I0925 06:40:22.010691  4458 sgd_solver.cpp:136] Iteration 35300, lr = 0.001, m = 0.9
I0925 06:40:40.068459  4458 solver.cpp:314] Iteration 35400 (5.53793 iter/s, 18.0573s/100 iter), loss = 0.0724174
I0925 06:40:40.068509  4458 solver.cpp:336]     Train net output #0: loss = 0.0724175 (* 1 = 0.0724175 loss)
I0925 06:40:40.068516  4458 sgd_solver.cpp:136] Iteration 35400, lr = 0.001, m = 0.9
I0925 06:40:58.283917  4458 solver.cpp:314] Iteration 35500 (5.49 iter/s, 18.215s/100 iter), loss = 0.0387971
I0925 06:40:58.283941  4458 solver.cpp:336]     Train net output #0: loss = 0.0387972 (* 1 = 0.0387972 loss)
I0925 06:40:58.283946  4458 sgd_solver.cpp:136] Iteration 35500, lr = 0.001, m = 0.9
I0925 06:41:03.930341  4461 data_reader.cpp:305] Starting prefetch of epoch 23
I0925 06:41:16.221973  4458 solver.cpp:314] Iteration 35600 (5.5749 iter/s, 17.9376s/100 iter), loss = 0.0621994
I0925 06:41:16.222043  4458 solver.cpp:336]     Train net output #0: loss = 0.0621995 (* 1 = 0.0621995 loss)
I0925 06:41:16.222048  4458 sgd_solver.cpp:136] Iteration 35600, lr = 0.001, m = 0.9
I0925 06:41:34.334863  4458 solver.cpp:314] Iteration 35700 (5.52108 iter/s, 18.1124s/100 iter), loss = 0.0508741
I0925 06:41:34.334887  4458 solver.cpp:336]     Train net output #0: loss = 0.0508742 (* 1 = 0.0508742 loss)
I0925 06:41:34.334892  4458 sgd_solver.cpp:136] Iteration 35700, lr = 0.001, m = 0.9
I0925 06:41:52.405871  4458 solver.cpp:314] Iteration 35800 (5.53388 iter/s, 18.0705s/100 iter), loss = 0.0587273
I0925 06:41:52.405956  4458 solver.cpp:336]     Train net output #0: loss = 0.0587274 (* 1 = 0.0587274 loss)
I0925 06:41:52.405962  4458 sgd_solver.cpp:136] Iteration 35800, lr = 0.001, m = 0.9
I0925 06:42:03.644758  4466 data_reader.cpp:305] Starting prefetch of epoch 16
I0925 06:42:10.525666  4458 solver.cpp:314] Iteration 35900 (5.51898 iter/s, 18.1193s/100 iter), loss = 0.0714559
I0925 06:42:10.525688  4458 solver.cpp:336]     Train net output #0: loss = 0.0714559 (* 1 = 0.0714559 loss)
I0925 06:42:10.525692  4458 sgd_solver.cpp:136] Iteration 35900, lr = 0.001, m = 0.9
I0925 06:42:28.323895  4458 solver.cpp:368] Sparsity after update:
I0925 06:42:28.330579  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 06:42:28.330590  4458 net.cpp:2312] conv1a_param_0(0.255) 
I0925 06:42:28.330597  4458 net.cpp:2312] conv1b_param_0(0.587) 
I0925 06:42:28.330601  4458 net.cpp:2312] ctx_conv1_param_0(0.783) 
I0925 06:42:28.330605  4458 net.cpp:2312] ctx_conv2_param_0(0.795) 
I0925 06:42:28.330608  4458 net.cpp:2312] ctx_conv3_param_0(0.785) 
I0925 06:42:28.330611  4458 net.cpp:2312] ctx_conv4_param_0(0.797) 
I0925 06:42:28.330615  4458 net.cpp:2312] ctx_final_param_0(0.33) 
I0925 06:42:28.330621  4458 net.cpp:2312] out3a_param_0(0.819) 
I0925 06:42:28.330624  4458 net.cpp:2312] out5a_param_0(0.817) 
I0925 06:42:28.330628  4458 net.cpp:2312] res2a_branch2a_param_0(0.729) 
I0925 06:42:28.330632  4458 net.cpp:2312] res2a_branch2b_param_0(0.565) 
I0925 06:42:28.330637  4458 net.cpp:2312] res3a_branch2a_param_0(0.76) 
I0925 06:42:28.330642  4458 net.cpp:2312] res3a_branch2b_param_0(0.693) 
I0925 06:42:28.330646  4458 net.cpp:2312] res4a_branch2a_param_0(0.797) 
I0925 06:42:28.330651  4458 net.cpp:2312] res4a_branch2b_param_0(0.762) 
I0925 06:42:28.330655  4458 net.cpp:2312] res5a_branch2a_param_0(0.817) 
I0925 06:42:28.330659  4458 net.cpp:2312] res5a_branch2b_param_0(0.819) 
I0925 06:42:28.330663  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.16549e+06/2.69117e+06) 0.805
I0925 06:42:28.330675  4458 solver.cpp:562] Iteration 36000, Testing net (#0)
I0925 06:42:38.273032  4518 data_reader.cpp:305] Starting prefetch of epoch 6
I0925 06:42:38.587564  4458 solver.cpp:654]     Test net output #0: accuracy/top1 = 0.95449
I0925 06:42:38.587585  4458 solver.cpp:654]     Test net output #1: accuracy/top5 = 1
I0925 06:42:38.587590  4458 solver.cpp:654]     Test net output #2: loss = 0.13362 (* 1 = 0.13362 loss)
I0925 06:42:38.587615  4458 solver.cpp:265] [MultiGPU] Tests completed in 10.2567s
I0925 06:42:38.772121  4458 solver.cpp:314] Iteration 36000 (3.54036 iter/s, 28.2457s/100 iter), loss = 0.0753225
I0925 06:42:38.772143  4458 solver.cpp:336]     Train net output #0: loss = 0.0753226 (* 1 = 0.0753226 loss)
I0925 06:42:38.772148  4458 sgd_solver.cpp:136] Iteration 36000, lr = 0.001, m = 0.9
I0925 06:42:56.761389  4458 solver.cpp:314] Iteration 36100 (5.55903 iter/s, 17.9888s/100 iter), loss = 0.206934
I0925 06:42:56.761415  4458 solver.cpp:336]     Train net output #0: loss = 0.206934 (* 1 = 0.206934 loss)
I0925 06:42:56.761418  4458 sgd_solver.cpp:136] Iteration 36100, lr = 0.001, m = 0.9
I0925 06:43:13.372717  4464 data_reader.cpp:305] Starting prefetch of epoch 22
I0925 06:43:14.824710  4458 solver.cpp:314] Iteration 36200 (5.53624 iter/s, 18.0628s/100 iter), loss = 0.0449843
I0925 06:43:14.824733  4458 solver.cpp:336]     Train net output #0: loss = 0.0449844 (* 1 = 0.0449844 loss)
I0925 06:43:14.824738  4458 sgd_solver.cpp:136] Iteration 36200, lr = 0.001, m = 0.9
I0925 06:43:32.809258  4458 solver.cpp:314] Iteration 36300 (5.56048 iter/s, 17.984s/100 iter), loss = 0.0514938
I0925 06:43:32.809284  4458 solver.cpp:336]     Train net output #0: loss = 0.0514939 (* 1 = 0.0514939 loss)
I0925 06:43:32.809289  4458 sgd_solver.cpp:136] Iteration 36300, lr = 0.001, m = 0.9
I0925 06:43:43.265049  4408 data_reader.cpp:305] Starting prefetch of epoch 28
I0925 06:43:50.861929  4458 solver.cpp:314] Iteration 36400 (5.5395 iter/s, 18.0522s/100 iter), loss = 0.0580921
I0925 06:43:50.861976  4458 solver.cpp:336]     Train net output #0: loss = 0.0580921 (* 1 = 0.0580921 loss)
I0925 06:43:50.861981  4458 sgd_solver.cpp:136] Iteration 36400, lr = 0.001, m = 0.9
I0925 06:44:08.954865  4458 solver.cpp:314] Iteration 36500 (5.52717 iter/s, 18.0924s/100 iter), loss = 0.091809
I0925 06:44:08.954887  4458 solver.cpp:336]     Train net output #0: loss = 0.0918091 (* 1 = 0.0918091 loss)
I0925 06:44:08.954892  4458 sgd_solver.cpp:136] Iteration 36500, lr = 0.001, m = 0.9
I0925 06:44:26.921097  4458 solver.cpp:314] Iteration 36600 (5.56615 iter/s, 17.9657s/100 iter), loss = 0.060046
I0925 06:44:26.921150  4458 solver.cpp:336]     Train net output #0: loss = 0.0600461 (* 1 = 0.0600461 loss)
I0925 06:44:26.921156  4458 sgd_solver.cpp:136] Iteration 36600, lr = 0.001, m = 0.9
I0925 06:44:42.991271  4466 data_reader.cpp:305] Starting prefetch of epoch 17
I0925 06:44:45.150427  4458 solver.cpp:314] Iteration 36700 (5.48582 iter/s, 18.2288s/100 iter), loss = 0.0609812
I0925 06:44:45.150449  4458 solver.cpp:336]     Train net output #0: loss = 0.0609813 (* 1 = 0.0609813 loss)
I0925 06:44:45.150455  4458 sgd_solver.cpp:136] Iteration 36700, lr = 0.001, m = 0.9
I0925 06:45:03.330984  4458 solver.cpp:314] Iteration 36800 (5.50053 iter/s, 18.1801s/100 iter), loss = 0.055742
I0925 06:45:03.331058  4458 solver.cpp:336]     Train net output #0: loss = 0.0557421 (* 1 = 0.0557421 loss)
I0925 06:45:03.331063  4458 sgd_solver.cpp:136] Iteration 36800, lr = 0.001, m = 0.9
I0925 06:45:21.296010  4458 solver.cpp:314] Iteration 36900 (5.56653 iter/s, 17.9645s/100 iter), loss = 0.0588685
I0925 06:45:21.296031  4458 solver.cpp:336]     Train net output #0: loss = 0.0588685 (* 1 = 0.0588685 loss)
I0925 06:45:21.296036  4458 sgd_solver.cpp:136] Iteration 36900, lr = 0.001, m = 0.9
I0925 06:45:39.106771  4458 solver.cpp:368] Sparsity after update:
I0925 06:45:39.115208  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 06:45:39.115222  4458 net.cpp:2312] conv1a_param_0(0.255) 
I0925 06:45:39.115228  4458 net.cpp:2312] conv1b_param_0(0.587) 
I0925 06:45:39.115231  4458 net.cpp:2312] ctx_conv1_param_0(0.783) 
I0925 06:45:39.115234  4458 net.cpp:2312] ctx_conv2_param_0(0.795) 
I0925 06:45:39.115237  4458 net.cpp:2312] ctx_conv3_param_0(0.785) 
I0925 06:45:39.115239  4458 net.cpp:2312] ctx_conv4_param_0(0.797) 
I0925 06:45:39.115242  4458 net.cpp:2312] ctx_final_param_0(0.33) 
I0925 06:45:39.115247  4458 net.cpp:2312] out3a_param_0(0.819) 
I0925 06:45:39.115250  4458 net.cpp:2312] out5a_param_0(0.817) 
I0925 06:45:39.115253  4458 net.cpp:2312] res2a_branch2a_param_0(0.729) 
I0925 06:45:39.115257  4458 net.cpp:2312] res2a_branch2b_param_0(0.565) 
I0925 06:45:39.115260  4458 net.cpp:2312] res3a_branch2a_param_0(0.76) 
I0925 06:45:39.115263  4458 net.cpp:2312] res3a_branch2b_param_0(0.693) 
I0925 06:45:39.115267  4458 net.cpp:2312] res4a_branch2a_param_0(0.797) 
I0925 06:45:39.115269  4458 net.cpp:2312] res4a_branch2b_param_0(0.762) 
I0925 06:45:39.115273  4458 net.cpp:2312] res5a_branch2a_param_0(0.817) 
I0925 06:45:39.115276  4458 net.cpp:2312] res5a_branch2b_param_0(0.819) 
I0925 06:45:39.115279  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.16549e+06/2.69117e+06) 0.805
I0925 06:45:39.287540  4458 solver.cpp:314] Iteration 37000 (5.55833 iter/s, 17.991s/100 iter), loss = 0.0505472
I0925 06:45:39.287562  4458 solver.cpp:336]     Train net output #0: loss = 0.0505473 (* 1 = 0.0505473 loss)
I0925 06:45:39.287567  4458 sgd_solver.cpp:136] Iteration 37000, lr = 0.001, m = 0.9
I0925 06:45:42.681715  4466 data_reader.cpp:305] Starting prefetch of epoch 18
I0925 06:45:57.378595  4458 solver.cpp:314] Iteration 37100 (5.52775 iter/s, 18.0906s/100 iter), loss = 0.0756084
I0925 06:45:57.378617  4458 solver.cpp:336]     Train net output #0: loss = 0.0756085 (* 1 = 0.0756085 loss)
I0925 06:45:57.378623  4458 sgd_solver.cpp:136] Iteration 37100, lr = 0.001, m = 0.9
I0925 06:46:15.582458  4458 solver.cpp:314] Iteration 37200 (5.49349 iter/s, 18.2034s/100 iter), loss = 0.051767
I0925 06:46:15.582532  4458 solver.cpp:336]     Train net output #0: loss = 0.0517671 (* 1 = 0.0517671 loss)
I0925 06:46:15.582540  4458 sgd_solver.cpp:136] Iteration 37200, lr = 0.001, m = 0.9
I0925 06:46:33.729887  4458 solver.cpp:314] Iteration 37300 (5.51058 iter/s, 18.1469s/100 iter), loss = 0.0687264
I0925 06:46:33.729912  4458 solver.cpp:336]     Train net output #0: loss = 0.0687264 (* 1 = 0.0687264 loss)
I0925 06:46:33.729917  4458 sgd_solver.cpp:136] Iteration 37300, lr = 0.001, m = 0.9
I0925 06:46:42.625236  4464 data_reader.cpp:305] Starting prefetch of epoch 23
I0925 06:46:51.785845  4458 solver.cpp:314] Iteration 37400 (5.53849 iter/s, 18.0555s/100 iter), loss = 0.0923759
I0925 06:46:51.785897  4458 solver.cpp:336]     Train net output #0: loss = 0.0923759 (* 1 = 0.0923759 loss)
I0925 06:46:51.785902  4458 sgd_solver.cpp:136] Iteration 37400, lr = 0.001, m = 0.9
I0925 06:47:09.790635  4458 solver.cpp:314] Iteration 37500 (5.55423 iter/s, 18.0043s/100 iter), loss = 0.0467928
I0925 06:47:09.790663  4458 solver.cpp:336]     Train net output #0: loss = 0.0467929 (* 1 = 0.0467929 loss)
I0925 06:47:09.790668  4458 sgd_solver.cpp:136] Iteration 37500, lr = 0.001, m = 0.9
I0925 06:47:12.547715  4408 data_reader.cpp:305] Starting prefetch of epoch 29
I0925 06:47:27.918401  4458 solver.cpp:314] Iteration 37600 (5.51655 iter/s, 18.1273s/100 iter), loss = 0.0403623
I0925 06:47:27.918479  4458 solver.cpp:336]     Train net output #0: loss = 0.0403624 (* 1 = 0.0403624 loss)
I0925 06:47:27.918484  4458 sgd_solver.cpp:136] Iteration 37600, lr = 0.001, m = 0.9
I0925 06:47:45.977129  4458 solver.cpp:314] Iteration 37700 (5.53764 iter/s, 18.0582s/100 iter), loss = 0.0661079
I0925 06:47:45.977152  4458 solver.cpp:336]     Train net output #0: loss = 0.066108 (* 1 = 0.066108 loss)
I0925 06:47:45.977156  4458 sgd_solver.cpp:136] Iteration 37700, lr = 0.001, m = 0.9
I0925 06:48:04.213747  4458 solver.cpp:314] Iteration 37800 (5.48363 iter/s, 18.2361s/100 iter), loss = 0.0508261
I0925 06:48:04.213795  4458 solver.cpp:336]     Train net output #0: loss = 0.0508261 (* 1 = 0.0508261 loss)
I0925 06:48:04.213800  4458 sgd_solver.cpp:136] Iteration 37800, lr = 0.001, m = 0.9
I0925 06:48:12.353158  4462 data_reader.cpp:305] Starting prefetch of epoch 17
I0925 06:48:22.349962  4458 solver.cpp:314] Iteration 37900 (5.51398 iter/s, 18.1357s/100 iter), loss = 0.0698335
I0925 06:48:22.349987  4458 solver.cpp:336]     Train net output #0: loss = 0.0698336 (* 1 = 0.0698336 loss)
I0925 06:48:22.349990  4458 sgd_solver.cpp:136] Iteration 37900, lr = 0.001, m = 0.9
I0925 06:48:40.299958  4458 solver.cpp:368] Sparsity after update:
I0925 06:48:40.303052  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 06:48:40.303062  4458 net.cpp:2312] conv1a_param_0(0.255) 
I0925 06:48:40.303068  4458 net.cpp:2312] conv1b_param_0(0.587) 
I0925 06:48:40.303071  4458 net.cpp:2312] ctx_conv1_param_0(0.783) 
I0925 06:48:40.303072  4458 net.cpp:2312] ctx_conv2_param_0(0.795) 
I0925 06:48:40.303076  4458 net.cpp:2312] ctx_conv3_param_0(0.785) 
I0925 06:48:40.303076  4458 net.cpp:2312] ctx_conv4_param_0(0.797) 
I0925 06:48:40.303078  4458 net.cpp:2312] ctx_final_param_0(0.33) 
I0925 06:48:40.303081  4458 net.cpp:2312] out3a_param_0(0.819) 
I0925 06:48:40.303082  4458 net.cpp:2312] out5a_param_0(0.817) 
I0925 06:48:40.303084  4458 net.cpp:2312] res2a_branch2a_param_0(0.729) 
I0925 06:48:40.303087  4458 net.cpp:2312] res2a_branch2b_param_0(0.565) 
I0925 06:48:40.303088  4458 net.cpp:2312] res3a_branch2a_param_0(0.76) 
I0925 06:48:40.303091  4458 net.cpp:2312] res3a_branch2b_param_0(0.693) 
I0925 06:48:40.303092  4458 net.cpp:2312] res4a_branch2a_param_0(0.797) 
I0925 06:48:40.303094  4458 net.cpp:2312] res4a_branch2b_param_0(0.762) 
I0925 06:48:40.303097  4458 net.cpp:2312] res5a_branch2a_param_0(0.817) 
I0925 06:48:40.303099  4458 net.cpp:2312] res5a_branch2b_param_0(0.819) 
I0925 06:48:40.303102  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.16549e+06/2.69117e+06) 0.805
I0925 06:48:40.303113  4458 solver.cpp:562] Iteration 38000, Testing net (#0)
I0925 06:48:50.238775  4458 solver.cpp:654]     Test net output #0: accuracy/top1 = 0.956122
I0925 06:48:50.238795  4458 solver.cpp:654]     Test net output #1: accuracy/top5 = 1
I0925 06:48:50.238801  4458 solver.cpp:654]     Test net output #2: loss = 0.156519 (* 1 = 0.156519 loss)
I0925 06:48:50.238903  4458 solver.cpp:265] [MultiGPU] Tests completed in 9.93551s
I0925 06:48:50.438086  4458 solver.cpp:314] Iteration 38000 (3.56032 iter/s, 28.0873s/100 iter), loss = 0.0702916
I0925 06:48:50.438110  4458 solver.cpp:336]     Train net output #0: loss = 0.0702917 (* 1 = 0.0702917 loss)
I0925 06:48:50.438114  4458 sgd_solver.cpp:136] Iteration 38000, lr = 0.001, m = 0.9
I0925 06:48:52.232398  4464 data_reader.cpp:305] Starting prefetch of epoch 24
I0925 06:49:08.484233  4458 solver.cpp:314] Iteration 38100 (5.5415 iter/s, 18.0456s/100 iter), loss = 0.0793594
I0925 06:49:08.484266  4458 solver.cpp:336]     Train net output #0: loss = 0.0793594 (* 1 = 0.0793594 loss)
I0925 06:49:08.484272  4458 sgd_solver.cpp:136] Iteration 38100, lr = 0.001, m = 0.9
I0925 06:49:26.612411  4458 solver.cpp:314] Iteration 38200 (5.51643 iter/s, 18.1277s/100 iter), loss = 0.0804861
I0925 06:49:26.612481  4458 solver.cpp:336]     Train net output #0: loss = 0.0804861 (* 1 = 0.0804861 loss)
I0925 06:49:26.612488  4458 sgd_solver.cpp:136] Iteration 38200, lr = 0.001, m = 0.9
I0925 06:49:44.593055  4458 solver.cpp:314] Iteration 38300 (5.56169 iter/s, 17.9801s/100 iter), loss = 0.063961
I0925 06:49:44.593075  4458 solver.cpp:336]     Train net output #0: loss = 0.0639611 (* 1 = 0.0639611 loss)
I0925 06:49:44.593078  4458 sgd_solver.cpp:136] Iteration 38300, lr = 0.001, m = 0.9
I0925 06:49:52.009039  4410 data_reader.cpp:305] Starting prefetch of epoch 25
I0925 06:50:02.686967  4458 solver.cpp:314] Iteration 38400 (5.52687 iter/s, 18.0934s/100 iter), loss = 0.0595814
I0925 06:50:02.687019  4458 solver.cpp:336]     Train net output #0: loss = 0.0595815 (* 1 = 0.0595815 loss)
I0925 06:50:02.687026  4458 sgd_solver.cpp:136] Iteration 38400, lr = 0.001, m = 0.9
I0925 06:50:20.814802  4458 solver.cpp:314] Iteration 38500 (5.51653 iter/s, 18.1273s/100 iter), loss = 0.0571715
I0925 06:50:20.814827  4458 solver.cpp:336]     Train net output #0: loss = 0.0571716 (* 1 = 0.0571716 loss)
I0925 06:50:20.814831  4458 sgd_solver.cpp:136] Iteration 38500, lr = 0.001, m = 0.9
I0925 06:50:21.930917  4462 data_reader.cpp:305] Starting prefetch of epoch 18
I0925 06:50:38.937176  4458 solver.cpp:314] Iteration 38600 (5.51819 iter/s, 18.1219s/100 iter), loss = 0.0522889
I0925 06:50:38.937258  4458 solver.cpp:336]     Train net output #0: loss = 0.052289 (* 1 = 0.052289 loss)
I0925 06:50:38.937265  4458 sgd_solver.cpp:136] Iteration 38600, lr = 0.001, m = 0.9
I0925 06:50:57.101225  4458 solver.cpp:314] Iteration 38700 (5.50553 iter/s, 18.1635s/100 iter), loss = 0.0828312
I0925 06:50:57.101249  4458 solver.cpp:336]     Train net output #0: loss = 0.0828313 (* 1 = 0.0828313 loss)
I0925 06:50:57.101255  4458 sgd_solver.cpp:136] Iteration 38700, lr = 0.001, m = 0.9
I0925 06:51:15.174664  4458 solver.cpp:314] Iteration 38800 (5.53314 iter/s, 18.0729s/100 iter), loss = 0.0619617
I0925 06:51:15.174778  4458 solver.cpp:336]     Train net output #0: loss = 0.0619618 (* 1 = 0.0619618 loss)
I0925 06:51:15.174787  4458 sgd_solver.cpp:136] Iteration 38800, lr = 0.001, m = 0.9
I0925 06:51:22.029989  4410 data_reader.cpp:305] Starting prefetch of epoch 26
I0925 06:51:33.427646  4458 solver.cpp:314] Iteration 38900 (5.47871 iter/s, 18.2525s/100 iter), loss = 0.158876
I0925 06:51:33.427670  4458 solver.cpp:336]     Train net output #0: loss = 0.158876 (* 1 = 0.158876 loss)
I0925 06:51:33.427675  4458 sgd_solver.cpp:136] Iteration 38900, lr = 0.001, m = 0.9
I0925 06:51:51.391263  4458 solver.cpp:368] Sparsity after update:
I0925 06:51:51.409482  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 06:51:51.409503  4458 net.cpp:2312] conv1a_param_0(0.255) 
I0925 06:51:51.409512  4458 net.cpp:2312] conv1b_param_0(0.587) 
I0925 06:51:51.409517  4458 net.cpp:2312] ctx_conv1_param_0(0.783) 
I0925 06:51:51.409519  4458 net.cpp:2312] ctx_conv2_param_0(0.795) 
I0925 06:51:51.409523  4458 net.cpp:2312] ctx_conv3_param_0(0.785) 
I0925 06:51:51.409525  4458 net.cpp:2312] ctx_conv4_param_0(0.797) 
I0925 06:51:51.409529  4458 net.cpp:2312] ctx_final_param_0(0.33) 
I0925 06:51:51.409533  4458 net.cpp:2312] out3a_param_0(0.819) 
I0925 06:51:51.409535  4458 net.cpp:2312] out5a_param_0(0.817) 
I0925 06:51:51.409539  4458 net.cpp:2312] res2a_branch2a_param_0(0.729) 
I0925 06:51:51.409543  4458 net.cpp:2312] res2a_branch2b_param_0(0.565) 
I0925 06:51:51.409545  4458 net.cpp:2312] res3a_branch2a_param_0(0.76) 
I0925 06:51:51.409548  4458 net.cpp:2312] res3a_branch2b_param_0(0.693) 
I0925 06:51:51.409551  4458 net.cpp:2312] res4a_branch2a_param_0(0.797) 
I0925 06:51:51.409554  4458 net.cpp:2312] res4a_branch2b_param_0(0.762) 
I0925 06:51:51.409559  4458 net.cpp:2312] res5a_branch2a_param_0(0.817) 
I0925 06:51:51.409562  4458 net.cpp:2312] res5a_branch2b_param_0(0.819) 
I0925 06:51:51.409565  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.16549e+06/2.69117e+06) 0.805
I0925 06:51:51.580267  4458 solver.cpp:314] Iteration 39000 (5.509 iter/s, 18.1521s/100 iter), loss = 0.0583529
I0925 06:51:51.580292  4458 solver.cpp:336]     Train net output #0: loss = 0.058353 (* 1 = 0.058353 loss)
I0925 06:51:51.580298  4458 sgd_solver.cpp:136] Iteration 39000, lr = 0.001, m = 0.9
I0925 06:52:09.600049  4458 solver.cpp:314] Iteration 39100 (5.54961 iter/s, 18.0193s/100 iter), loss = 0.0335151
I0925 06:52:09.600073  4458 solver.cpp:336]     Train net output #0: loss = 0.0335151 (* 1 = 0.0335151 loss)
I0925 06:52:09.600077  4458 sgd_solver.cpp:136] Iteration 39100, lr = 0.001, m = 0.9
I0925 06:52:21.758267  4466 data_reader.cpp:305] Starting prefetch of epoch 19
I0925 06:52:27.755975  4458 solver.cpp:314] Iteration 39200 (5.508 iter/s, 18.1554s/100 iter), loss = 0.0775811
I0925 06:52:27.756000  4458 solver.cpp:336]     Train net output #0: loss = 0.0775812 (* 1 = 0.0775812 loss)
I0925 06:52:27.756006  4458 sgd_solver.cpp:136] Iteration 39200, lr = 0.001, m = 0.9
I0925 06:52:45.795706  4458 solver.cpp:314] Iteration 39300 (5.54347 iter/s, 18.0392s/100 iter), loss = 0.0566625
I0925 06:52:45.795725  4458 solver.cpp:336]     Train net output #0: loss = 0.0566626 (* 1 = 0.0566626 loss)
I0925 06:52:45.795730  4458 sgd_solver.cpp:136] Iteration 39300, lr = 0.001, m = 0.9
I0925 06:52:51.810127  4461 data_reader.cpp:305] Starting prefetch of epoch 24
I0925 06:53:04.005273  4458 solver.cpp:314] Iteration 39400 (5.49177 iter/s, 18.2091s/100 iter), loss = 0.0553517
I0925 06:53:04.005302  4458 solver.cpp:336]     Train net output #0: loss = 0.0553517 (* 1 = 0.0553517 loss)
I0925 06:53:04.005308  4458 sgd_solver.cpp:136] Iteration 39400, lr = 0.001, m = 0.9
I0925 06:53:22.075875  4458 solver.cpp:314] Iteration 39500 (5.534 iter/s, 18.0701s/100 iter), loss = 0.0543981
I0925 06:53:22.075923  4458 solver.cpp:336]     Train net output #0: loss = 0.0543982 (* 1 = 0.0543982 loss)
I0925 06:53:22.075929  4458 sgd_solver.cpp:136] Iteration 39500, lr = 0.001, m = 0.9
I0925 06:53:40.079923  4458 solver.cpp:314] Iteration 39600 (5.55446 iter/s, 18.0035s/100 iter), loss = 0.0650092
I0925 06:53:40.079947  4458 solver.cpp:336]     Train net output #0: loss = 0.0650093 (* 1 = 0.0650093 loss)
I0925 06:53:40.079952  4458 sgd_solver.cpp:136] Iteration 39600, lr = 0.001, m = 0.9
I0925 06:53:51.587129  4462 data_reader.cpp:305] Starting prefetch of epoch 19
I0925 06:53:58.182245  4458 solver.cpp:314] Iteration 39700 (5.52431 iter/s, 18.1018s/100 iter), loss = 0.0626265
I0925 06:53:58.182337  4458 solver.cpp:336]     Train net output #0: loss = 0.0626265 (* 1 = 0.0626265 loss)
I0925 06:53:58.182344  4458 sgd_solver.cpp:136] Iteration 39700, lr = 0.001, m = 0.9
I0925 06:54:16.231954  4458 solver.cpp:314] Iteration 39800 (5.54041 iter/s, 18.0492s/100 iter), loss = 0.0533902
I0925 06:54:16.231976  4458 solver.cpp:336]     Train net output #0: loss = 0.0533902 (* 1 = 0.0533902 loss)
I0925 06:54:16.231981  4458 sgd_solver.cpp:136] Iteration 39800, lr = 0.001, m = 0.9
I0925 06:54:34.306671  4458 solver.cpp:314] Iteration 39900 (5.53274 iter/s, 18.0742s/100 iter), loss = 0.0698491
I0925 06:54:34.306725  4458 solver.cpp:336]     Train net output #0: loss = 0.0698492 (* 1 = 0.0698492 loss)
I0925 06:54:34.306730  4458 sgd_solver.cpp:136] Iteration 39900, lr = 0.001, m = 0.9
I0925 06:54:51.246561  4464 data_reader.cpp:305] Starting prefetch of epoch 25
I0925 06:54:52.185350  4458 solver.cpp:824] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/cityscapes5_jsegnet21v2_iter_40000.caffemodel
I0925 06:54:52.317387  4458 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/cityscapes5_jsegnet21v2_iter_40000.solverstate
I0925 06:54:52.323978  4458 solver.cpp:368] Sparsity after update:
I0925 06:54:52.325507  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 06:54:52.325515  4458 net.cpp:2312] conv1a_param_0(0.255) 
I0925 06:54:52.325521  4458 net.cpp:2312] conv1b_param_0(0.587) 
I0925 06:54:52.325525  4458 net.cpp:2312] ctx_conv1_param_0(0.783) 
I0925 06:54:52.325529  4458 net.cpp:2312] ctx_conv2_param_0(0.795) 
I0925 06:54:52.325533  4458 net.cpp:2312] ctx_conv3_param_0(0.785) 
I0925 06:54:52.325536  4458 net.cpp:2312] ctx_conv4_param_0(0.797) 
I0925 06:54:52.325538  4458 net.cpp:2312] ctx_final_param_0(0.33) 
I0925 06:54:52.325541  4458 net.cpp:2312] out3a_param_0(0.819) 
I0925 06:54:52.325546  4458 net.cpp:2312] out5a_param_0(0.817) 
I0925 06:54:52.325549  4458 net.cpp:2312] res2a_branch2a_param_0(0.729) 
I0925 06:54:52.325552  4458 net.cpp:2312] res2a_branch2b_param_0(0.565) 
I0925 06:54:52.325556  4458 net.cpp:2312] res3a_branch2a_param_0(0.76) 
I0925 06:54:52.325559  4458 net.cpp:2312] res3a_branch2b_param_0(0.693) 
I0925 06:54:52.325562  4458 net.cpp:2312] res4a_branch2a_param_0(0.797) 
I0925 06:54:52.325565  4458 net.cpp:2312] res4a_branch2b_param_0(0.762) 
I0925 06:54:52.325567  4458 net.cpp:2312] res5a_branch2a_param_0(0.817) 
I0925 06:54:52.325569  4458 net.cpp:2312] res5a_branch2b_param_0(0.819) 
I0925 06:54:52.325572  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.16549e+06/2.69117e+06) 0.805
I0925 06:54:52.325582  4458 solver.cpp:562] Iteration 40000, Testing net (#0)
I0925 06:54:55.345712  4441 data_reader.cpp:305] Starting prefetch of epoch 6
I0925 06:55:02.358744  4458 solver.cpp:654]     Test net output #0: accuracy/top1 = 0.954986
I0925 06:55:02.358781  4458 solver.cpp:654]     Test net output #1: accuracy/top5 = 1
I0925 06:55:02.358788  4458 solver.cpp:654]     Test net output #2: loss = 0.131117 (* 1 = 0.131117 loss)
I0925 06:55:02.358808  4458 solver.cpp:265] [MultiGPU] Tests completed in 10.0329s
I0925 06:55:02.565990  4458 solver.cpp:314] Iteration 40000 (3.53875 iter/s, 28.2585s/100 iter), loss = 0.0767127
I0925 06:55:02.566015  4458 solver.cpp:336]     Train net output #0: loss = 0.0767128 (* 1 = 0.0767128 loss)
I0925 06:55:02.566021  4458 sgd_solver.cpp:136] Iteration 40000, lr = 0.001, m = 0.9
I0925 06:55:20.603433  4458 solver.cpp:314] Iteration 40100 (5.54418 iter/s, 18.0369s/100 iter), loss = 0.0550867
I0925 06:55:20.603554  4458 solver.cpp:336]     Train net output #0: loss = 0.0550867 (* 1 = 0.0550867 loss)
I0925 06:55:20.603562  4458 sgd_solver.cpp:136] Iteration 40100, lr = 0.001, m = 0.9
I0925 06:55:31.319789  4410 data_reader.cpp:305] Starting prefetch of epoch 27
I0925 06:55:38.748800  4458 solver.cpp:314] Iteration 40200 (5.5112 iter/s, 18.1449s/100 iter), loss = 0.0415832
I0925 06:55:38.748824  4458 solver.cpp:336]     Train net output #0: loss = 0.0415832 (* 1 = 0.0415832 loss)
I0925 06:55:38.748828  4458 sgd_solver.cpp:136] Iteration 40200, lr = 0.001, m = 0.9
I0925 06:55:57.015501  4458 solver.cpp:314] Iteration 40300 (5.4746 iter/s, 18.2662s/100 iter), loss = 0.0689403
I0925 06:55:57.015545  4458 solver.cpp:336]     Train net output #0: loss = 0.0689404 (* 1 = 0.0689404 loss)
I0925 06:55:57.015552  4458 sgd_solver.cpp:136] Iteration 40300, lr = 0.001, m = 0.9
I0925 06:56:15.242983  4458 solver.cpp:314] Iteration 40400 (5.48637 iter/s, 18.227s/100 iter), loss = 0.0766232
I0925 06:56:15.243005  4458 solver.cpp:336]     Train net output #0: loss = 0.0766232 (* 1 = 0.0766232 loss)
I0925 06:56:15.243010  4458 sgd_solver.cpp:136] Iteration 40400, lr = 0.001, m = 0.9
I0925 06:56:31.492105  4464 data_reader.cpp:305] Starting prefetch of epoch 26
I0925 06:56:33.308035  4458 solver.cpp:314] Iteration 40500 (5.53571 iter/s, 18.0645s/100 iter), loss = 0.0633166
I0925 06:56:33.308061  4458 solver.cpp:336]     Train net output #0: loss = 0.0633166 (* 1 = 0.0633166 loss)
I0925 06:56:33.308065  4458 sgd_solver.cpp:136] Iteration 40500, lr = 0.001, m = 0.9
I0925 06:56:51.440955  4458 solver.cpp:314] Iteration 40600 (5.51498 iter/s, 18.1324s/100 iter), loss = 0.0675889
I0925 06:56:51.440979  4458 solver.cpp:336]     Train net output #0: loss = 0.067589 (* 1 = 0.067589 loss)
I0925 06:56:51.440984  4458 sgd_solver.cpp:136] Iteration 40600, lr = 0.001, m = 0.9
I0925 06:57:01.522634  4408 data_reader.cpp:305] Starting prefetch of epoch 30
I0925 06:57:09.670485  4458 solver.cpp:314] Iteration 40700 (5.48576 iter/s, 18.229s/100 iter), loss = 0.0745497
I0925 06:57:09.670506  4458 solver.cpp:336]     Train net output #0: loss = 0.0745498 (* 1 = 0.0745498 loss)
I0925 06:57:09.670511  4458 sgd_solver.cpp:136] Iteration 40700, lr = 0.001, m = 0.9
I0925 06:57:27.720464  4458 solver.cpp:314] Iteration 40800 (5.54033 iter/s, 18.0495s/100 iter), loss = 0.0969421
I0925 06:57:27.720489  4458 solver.cpp:336]     Train net output #0: loss = 0.0969421 (* 1 = 0.0969421 loss)
I0925 06:57:27.720495  4458 sgd_solver.cpp:136] Iteration 40800, lr = 0.001, m = 0.9
I0925 06:57:45.818336  4458 solver.cpp:314] Iteration 40900 (5.52567 iter/s, 18.0974s/100 iter), loss = 0.0551075
I0925 06:57:45.818392  4458 solver.cpp:336]     Train net output #0: loss = 0.0551075 (* 1 = 0.0551075 loss)
I0925 06:57:45.818399  4458 sgd_solver.cpp:136] Iteration 40900, lr = 0.001, m = 0.9
I0925 06:58:01.271059  4408 data_reader.cpp:305] Starting prefetch of epoch 31
I0925 06:58:03.801188  4458 solver.cpp:368] Sparsity after update:
I0925 06:58:03.816254  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 06:58:03.816272  4458 net.cpp:2312] conv1a_param_0(0.255) 
I0925 06:58:03.816279  4458 net.cpp:2312] conv1b_param_0(0.587) 
I0925 06:58:03.816282  4458 net.cpp:2312] ctx_conv1_param_0(0.783) 
I0925 06:58:03.816283  4458 net.cpp:2312] ctx_conv2_param_0(0.795) 
I0925 06:58:03.816285  4458 net.cpp:2312] ctx_conv3_param_0(0.785) 
I0925 06:58:03.816287  4458 net.cpp:2312] ctx_conv4_param_0(0.797) 
I0925 06:58:03.816289  4458 net.cpp:2312] ctx_final_param_0(0.33) 
I0925 06:58:03.816292  4458 net.cpp:2312] out3a_param_0(0.819) 
I0925 06:58:03.816293  4458 net.cpp:2312] out5a_param_0(0.817) 
I0925 06:58:03.816295  4458 net.cpp:2312] res2a_branch2a_param_0(0.729) 
I0925 06:58:03.816296  4458 net.cpp:2312] res2a_branch2b_param_0(0.565) 
I0925 06:58:03.816299  4458 net.cpp:2312] res3a_branch2a_param_0(0.76) 
I0925 06:58:03.816301  4458 net.cpp:2312] res3a_branch2b_param_0(0.693) 
I0925 06:58:03.816303  4458 net.cpp:2312] res4a_branch2a_param_0(0.797) 
I0925 06:58:03.816305  4458 net.cpp:2312] res4a_branch2b_param_0(0.762) 
I0925 06:58:03.816308  4458 net.cpp:2312] res5a_branch2a_param_0(0.817) 
I0925 06:58:03.816309  4458 net.cpp:2312] res5a_branch2b_param_0(0.819) 
I0925 06:58:03.816311  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.16549e+06/2.69117e+06) 0.805
I0925 06:58:03.984221  4458 solver.cpp:314] Iteration 41000 (5.50498 iter/s, 18.1654s/100 iter), loss = 0.0806869
I0925 06:58:03.984244  4458 solver.cpp:336]     Train net output #0: loss = 0.080687 (* 1 = 0.080687 loss)
I0925 06:58:03.984248  4458 sgd_solver.cpp:136] Iteration 41000, lr = 0.001, m = 0.9
I0925 06:58:22.028153  4458 solver.cpp:314] Iteration 41100 (5.54218 iter/s, 18.0434s/100 iter), loss = 0.0494009
I0925 06:58:22.028344  4458 solver.cpp:336]     Train net output #0: loss = 0.049401 (* 1 = 0.049401 loss)
I0925 06:58:22.028352  4458 sgd_solver.cpp:136] Iteration 41100, lr = 0.001, m = 0.9
I0925 06:58:40.083400  4458 solver.cpp:314] Iteration 41200 (5.53871 iter/s, 18.0547s/100 iter), loss = 0.0504861
I0925 06:58:40.083421  4458 solver.cpp:336]     Train net output #0: loss = 0.0504862 (* 1 = 0.0504862 loss)
I0925 06:58:40.083427  4458 sgd_solver.cpp:136] Iteration 41200, lr = 0.001, m = 0.9
I0925 06:58:58.124094  4458 solver.cpp:314] Iteration 41300 (5.54318 iter/s, 18.0402s/100 iter), loss = 0.0304566
I0925 06:58:58.124191  4458 solver.cpp:336]     Train net output #0: loss = 0.0304567 (* 1 = 0.0304567 loss)
I0925 06:58:58.124197  4458 sgd_solver.cpp:136] Iteration 41300, lr = 0.001, m = 0.9
I0925 06:59:01.031134  4410 data_reader.cpp:305] Starting prefetch of epoch 28
I0925 06:59:16.291990  4458 solver.cpp:314] Iteration 41400 (5.50437 iter/s, 18.1674s/100 iter), loss = 0.102557
I0925 06:59:16.292011  4458 solver.cpp:336]     Train net output #0: loss = 0.102557 (* 1 = 0.102557 loss)
I0925 06:59:16.292017  4458 sgd_solver.cpp:136] Iteration 41400, lr = 0.001, m = 0.9
I0925 06:59:34.420397  4458 solver.cpp:314] Iteration 41500 (5.51636 iter/s, 18.1279s/100 iter), loss = 0.0760717
I0925 06:59:34.420475  4458 solver.cpp:336]     Train net output #0: loss = 0.0760718 (* 1 = 0.0760718 loss)
I0925 06:59:34.420481  4458 sgd_solver.cpp:136] Iteration 41500, lr = 0.001, m = 0.9
I0925 06:59:52.540522  4458 solver.cpp:314] Iteration 41600 (5.51888 iter/s, 18.1196s/100 iter), loss = 0.0441967
I0925 06:59:52.540545  4458 solver.cpp:336]     Train net output #0: loss = 0.0441968 (* 1 = 0.0441968 loss)
I0925 06:59:52.540551  4458 sgd_solver.cpp:136] Iteration 41600, lr = 0.001, m = 0.9
I0925 07:00:00.834066  4466 data_reader.cpp:305] Starting prefetch of epoch 20
I0925 07:00:10.646733  4458 solver.cpp:314] Iteration 41700 (5.52312 iter/s, 18.1057s/100 iter), loss = 0.0767941
I0925 07:00:10.656167  4458 solver.cpp:336]     Train net output #0: loss = 0.0767941 (* 1 = 0.0767941 loss)
I0925 07:00:10.656374  4458 sgd_solver.cpp:136] Iteration 41700, lr = 0.001, m = 0.9
I0925 07:00:28.794508  4458 solver.cpp:314] Iteration 41800 (5.51047 iter/s, 18.1473s/100 iter), loss = 0.0668929
I0925 07:00:28.794533  4458 solver.cpp:336]     Train net output #0: loss = 0.066893 (* 1 = 0.066893 loss)
I0925 07:00:28.794538  4458 sgd_solver.cpp:136] Iteration 41800, lr = 0.001, m = 0.9
I0925 07:00:30.969482  4462 data_reader.cpp:305] Starting prefetch of epoch 20
I0925 07:00:46.808538  4458 solver.cpp:314] Iteration 41900 (5.55138 iter/s, 18.0135s/100 iter), loss = 0.0814803
I0925 07:00:46.808586  4458 solver.cpp:336]     Train net output #0: loss = 0.0814803 (* 1 = 0.0814803 loss)
I0925 07:00:46.808593  4458 sgd_solver.cpp:136] Iteration 41900, lr = 0.001, m = 0.9
I0925 07:01:04.781683  4458 solver.cpp:368] Sparsity after update:
I0925 07:01:04.785447  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 07:01:04.785456  4458 net.cpp:2312] conv1a_param_0(0.255) 
I0925 07:01:04.785465  4458 net.cpp:2312] conv1b_param_0(0.587) 
I0925 07:01:04.785470  4458 net.cpp:2312] ctx_conv1_param_0(0.783) 
I0925 07:01:04.785472  4458 net.cpp:2312] ctx_conv2_param_0(0.795) 
I0925 07:01:04.785481  4458 net.cpp:2312] ctx_conv3_param_0(0.785) 
I0925 07:01:04.785485  4458 net.cpp:2312] ctx_conv4_param_0(0.797) 
I0925 07:01:04.785490  4458 net.cpp:2312] ctx_final_param_0(0.33) 
I0925 07:01:04.785493  4458 net.cpp:2312] out3a_param_0(0.819) 
I0925 07:01:04.785497  4458 net.cpp:2312] out5a_param_0(0.817) 
I0925 07:01:04.785501  4458 net.cpp:2312] res2a_branch2a_param_0(0.729) 
I0925 07:01:04.785503  4458 net.cpp:2312] res2a_branch2b_param_0(0.565) 
I0925 07:01:04.785506  4458 net.cpp:2312] res3a_branch2a_param_0(0.76) 
I0925 07:01:04.785511  4458 net.cpp:2312] res3a_branch2b_param_0(0.693) 
I0925 07:01:04.785514  4458 net.cpp:2312] res4a_branch2a_param_0(0.797) 
I0925 07:01:04.785518  4458 net.cpp:2312] res4a_branch2b_param_0(0.762) 
I0925 07:01:04.785522  4458 net.cpp:2312] res5a_branch2a_param_0(0.817) 
I0925 07:01:04.785527  4458 net.cpp:2312] res5a_branch2b_param_0(0.819) 
I0925 07:01:04.785531  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.16549e+06/2.69117e+06) 0.805
I0925 07:01:04.785547  4458 solver.cpp:562] Iteration 42000, Testing net (#0)
I0925 07:01:11.071745  4520 data_reader.cpp:305] Starting prefetch of epoch 4
I0925 07:01:14.905863  4458 solver.cpp:654]     Test net output #0: accuracy/top1 = 0.956456
I0925 07:01:14.905884  4458 solver.cpp:654]     Test net output #1: accuracy/top5 = 1
I0925 07:01:14.905889  4458 solver.cpp:654]     Test net output #2: loss = 0.155734 (* 1 = 0.155734 loss)
I0925 07:01:14.905956  4458 solver.cpp:265] [MultiGPU] Tests completed in 10.1201s
I0925 07:01:15.107797  4458 solver.cpp:314] Iteration 42000 (3.53376 iter/s, 28.2985s/100 iter), loss = 0.0529253
I0925 07:01:15.107823  4458 solver.cpp:336]     Train net output #0: loss = 0.0529254 (* 1 = 0.0529254 loss)
I0925 07:01:15.107827  4458 sgd_solver.cpp:136] Iteration 42000, lr = 0.001, m = 0.9
I0925 07:01:33.076820  4458 solver.cpp:314] Iteration 42100 (5.56529 iter/s, 17.9685s/100 iter), loss = 0.0505856
I0925 07:01:33.076886  4458 solver.cpp:336]     Train net output #0: loss = 0.0505856 (* 1 = 0.0505856 loss)
I0925 07:01:33.076892  4458 sgd_solver.cpp:136] Iteration 42100, lr = 0.001, m = 0.9
I0925 07:01:51.032775  4458 solver.cpp:314] Iteration 42200 (5.56934 iter/s, 17.9555s/100 iter), loss = 0.0481304
I0925 07:01:51.032801  4458 solver.cpp:336]     Train net output #0: loss = 0.0481305 (* 1 = 0.0481305 loss)
I0925 07:01:51.032807  4458 sgd_solver.cpp:136] Iteration 42200, lr = 0.001, m = 0.9
I0925 07:02:09.126653  4458 solver.cpp:314] Iteration 42300 (5.52689 iter/s, 18.0934s/100 iter), loss = 0.0830805
I0925 07:02:09.126698  4458 solver.cpp:336]     Train net output #0: loss = 0.0830805 (* 1 = 0.0830805 loss)
I0925 07:02:09.126704  4458 sgd_solver.cpp:136] Iteration 42300, lr = 0.001, m = 0.9
I0925 07:02:10.584081  4466 data_reader.cpp:305] Starting prefetch of epoch 21
I0925 07:02:27.178823  4458 solver.cpp:314] Iteration 42400 (5.53966 iter/s, 18.0517s/100 iter), loss = 0.0610943
I0925 07:02:27.178848  4458 solver.cpp:336]     Train net output #0: loss = 0.0610943 (* 1 = 0.0610943 loss)
I0925 07:02:27.178851  4458 sgd_solver.cpp:136] Iteration 42400, lr = 0.001, m = 0.9
I0925 07:02:40.545984  4462 data_reader.cpp:305] Starting prefetch of epoch 21
I0925 07:02:45.490502  4458 solver.cpp:314] Iteration 42500 (5.46115 iter/s, 18.3112s/100 iter), loss = 0.0512075
I0925 07:02:45.490530  4458 solver.cpp:336]     Train net output #0: loss = 0.0512076 (* 1 = 0.0512076 loss)
I0925 07:02:45.490535  4458 sgd_solver.cpp:136] Iteration 42500, lr = 0.001, m = 0.9
I0925 07:03:03.455729  4458 solver.cpp:314] Iteration 42600 (5.56646 iter/s, 17.9647s/100 iter), loss = 0.0758925
I0925 07:03:03.455754  4458 solver.cpp:336]     Train net output #0: loss = 0.0758926 (* 1 = 0.0758926 loss)
I0925 07:03:03.455759  4458 sgd_solver.cpp:136] Iteration 42600, lr = 0.001, m = 0.9
I0925 07:03:21.460189  4458 solver.cpp:314] Iteration 42700 (5.55433 iter/s, 18.004s/100 iter), loss = 0.0506465
I0925 07:03:21.460244  4458 solver.cpp:336]     Train net output #0: loss = 0.0506466 (* 1 = 0.0506466 loss)
I0925 07:03:21.460250  4458 sgd_solver.cpp:136] Iteration 42700, lr = 0.001, m = 0.9
I0925 07:03:39.467653  4458 solver.cpp:314] Iteration 42800 (5.55341 iter/s, 18.007s/100 iter), loss = 0.0520978
I0925 07:03:39.467677  4458 solver.cpp:336]     Train net output #0: loss = 0.0520979 (* 1 = 0.0520979 loss)
I0925 07:03:39.467682  4458 sgd_solver.cpp:136] Iteration 42800, lr = 0.001, m = 0.9
I0925 07:03:40.053069  4462 data_reader.cpp:305] Starting prefetch of epoch 22
I0925 07:03:57.574868  4458 solver.cpp:314] Iteration 42900 (5.52281 iter/s, 18.1067s/100 iter), loss = 0.0454997
I0925 07:03:57.580348  4458 solver.cpp:336]     Train net output #0: loss = 0.0454998 (* 1 = 0.0454998 loss)
I0925 07:03:57.580399  4458 sgd_solver.cpp:136] Iteration 42900, lr = 0.001, m = 0.9
I0925 07:04:15.315114  4458 solver.cpp:368] Sparsity after update:
I0925 07:04:15.322536  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 07:04:15.322544  4458 net.cpp:2312] conv1a_param_0(0.255) 
I0925 07:04:15.322551  4458 net.cpp:2312] conv1b_param_0(0.587) 
I0925 07:04:15.322553  4458 net.cpp:2312] ctx_conv1_param_0(0.783) 
I0925 07:04:15.322556  4458 net.cpp:2312] ctx_conv2_param_0(0.795) 
I0925 07:04:15.322557  4458 net.cpp:2312] ctx_conv3_param_0(0.785) 
I0925 07:04:15.322559  4458 net.cpp:2312] ctx_conv4_param_0(0.797) 
I0925 07:04:15.322561  4458 net.cpp:2312] ctx_final_param_0(0.33) 
I0925 07:04:15.322563  4458 net.cpp:2312] out3a_param_0(0.819) 
I0925 07:04:15.322564  4458 net.cpp:2312] out5a_param_0(0.817) 
I0925 07:04:15.322566  4458 net.cpp:2312] res2a_branch2a_param_0(0.729) 
I0925 07:04:15.322568  4458 net.cpp:2312] res2a_branch2b_param_0(0.565) 
I0925 07:04:15.322571  4458 net.cpp:2312] res3a_branch2a_param_0(0.76) 
I0925 07:04:15.322572  4458 net.cpp:2312] res3a_branch2b_param_0(0.693) 
I0925 07:04:15.322574  4458 net.cpp:2312] res4a_branch2a_param_0(0.797) 
I0925 07:04:15.322576  4458 net.cpp:2312] res4a_branch2b_param_0(0.762) 
I0925 07:04:15.322578  4458 net.cpp:2312] res5a_branch2a_param_0(0.817) 
I0925 07:04:15.322580  4458 net.cpp:2312] res5a_branch2b_param_0(0.819) 
I0925 07:04:15.322582  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.16549e+06/2.69117e+06) 0.805
I0925 07:04:15.488790  4458 solver.cpp:314] Iteration 43000 (5.58241 iter/s, 17.9134s/100 iter), loss = 0.0459939
I0925 07:04:15.488813  4458 solver.cpp:336]     Train net output #0: loss = 0.045994 (* 1 = 0.045994 loss)
I0925 07:04:15.488817  4458 sgd_solver.cpp:136] Iteration 43000, lr = 0.001, m = 0.9
I0925 07:04:33.578956  4458 solver.cpp:314] Iteration 43100 (5.52802 iter/s, 18.0897s/100 iter), loss = 0.0846864
I0925 07:04:33.579008  4458 solver.cpp:336]     Train net output #0: loss = 0.0846864 (* 1 = 0.0846864 loss)
I0925 07:04:33.579013  4458 sgd_solver.cpp:136] Iteration 43100, lr = 0.001, m = 0.9
I0925 07:04:39.733361  4466 data_reader.cpp:305] Starting prefetch of epoch 22
I0925 07:04:51.700314  4458 solver.cpp:314] Iteration 43200 (5.51851 iter/s, 18.1208s/100 iter), loss = 0.111912
I0925 07:04:51.700335  4458 solver.cpp:336]     Train net output #0: loss = 0.111912 (* 1 = 0.111912 loss)
I0925 07:04:51.700340  4458 sgd_solver.cpp:136] Iteration 43200, lr = 0.001, m = 0.9
I0925 07:05:09.645064  4408 data_reader.cpp:305] Starting prefetch of epoch 32
I0925 07:05:09.781792  4458 solver.cpp:314] Iteration 43300 (5.53068 iter/s, 18.081s/100 iter), loss = 0.0475895
I0925 07:05:09.781813  4458 solver.cpp:336]     Train net output #0: loss = 0.0475896 (* 1 = 0.0475896 loss)
I0925 07:05:09.781819  4458 sgd_solver.cpp:136] Iteration 43300, lr = 0.001, m = 0.9
I0925 07:05:27.985015  4458 solver.cpp:314] Iteration 43400 (5.49369 iter/s, 18.2027s/100 iter), loss = 0.0679647
I0925 07:05:27.985043  4458 solver.cpp:336]     Train net output #0: loss = 0.0679647 (* 1 = 0.0679647 loss)
I0925 07:05:27.985049  4458 sgd_solver.cpp:136] Iteration 43400, lr = 0.001, m = 0.9
I0925 07:05:46.112012  4458 solver.cpp:314] Iteration 43500 (5.51679 iter/s, 18.1265s/100 iter), loss = 0.0654277
I0925 07:05:46.112061  4458 solver.cpp:336]     Train net output #0: loss = 0.0654278 (* 1 = 0.0654278 loss)
I0925 07:05:46.112066  4458 sgd_solver.cpp:136] Iteration 43500, lr = 0.001, m = 0.9
I0925 07:06:04.155333  4458 solver.cpp:314] Iteration 43600 (5.54237 iter/s, 18.0428s/100 iter), loss = 0.0528001
I0925 07:06:04.155360  4458 solver.cpp:336]     Train net output #0: loss = 0.0528002 (* 1 = 0.0528002 loss)
I0925 07:06:04.155367  4458 sgd_solver.cpp:136] Iteration 43600, lr = 0.001, m = 0.9
I0925 07:06:09.600275  4408 data_reader.cpp:305] Starting prefetch of epoch 33
I0925 07:06:22.276095  4458 solver.cpp:314] Iteration 43700 (5.51869 iter/s, 18.1203s/100 iter), loss = 0.0999588
I0925 07:06:22.276221  4458 solver.cpp:336]     Train net output #0: loss = 0.0999589 (* 1 = 0.0999589 loss)
I0925 07:06:22.276227  4458 sgd_solver.cpp:136] Iteration 43700, lr = 0.001, m = 0.9
I0925 07:06:40.416347  4458 solver.cpp:314] Iteration 43800 (5.51276 iter/s, 18.1397s/100 iter), loss = 0.0371842
I0925 07:06:40.416370  4458 solver.cpp:336]     Train net output #0: loss = 0.0371843 (* 1 = 0.0371843 loss)
I0925 07:06:40.416374  4458 sgd_solver.cpp:136] Iteration 43800, lr = 0.001, m = 0.9
I0925 07:06:58.491984  4458 solver.cpp:314] Iteration 43900 (5.53246 iter/s, 18.0751s/100 iter), loss = 0.0443687
I0925 07:06:58.492087  4458 solver.cpp:336]     Train net output #0: loss = 0.0443688 (* 1 = 0.0443688 loss)
I0925 07:06:58.492094  4458 sgd_solver.cpp:136] Iteration 43900, lr = 0.001, m = 0.9
I0925 07:07:09.404109  4466 data_reader.cpp:305] Starting prefetch of epoch 23
I0925 07:07:16.387537  4458 solver.cpp:368] Sparsity after update:
I0925 07:07:16.390478  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 07:07:16.390486  4458 net.cpp:2312] conv1a_param_0(0.255) 
I0925 07:07:16.390492  4458 net.cpp:2312] conv1b_param_0(0.587) 
I0925 07:07:16.390494  4458 net.cpp:2312] ctx_conv1_param_0(0.783) 
I0925 07:07:16.390496  4458 net.cpp:2312] ctx_conv2_param_0(0.795) 
I0925 07:07:16.390498  4458 net.cpp:2312] ctx_conv3_param_0(0.785) 
I0925 07:07:16.390499  4458 net.cpp:2312] ctx_conv4_param_0(0.797) 
I0925 07:07:16.390501  4458 net.cpp:2312] ctx_final_param_0(0.33) 
I0925 07:07:16.390503  4458 net.cpp:2312] out3a_param_0(0.819) 
I0925 07:07:16.390506  4458 net.cpp:2312] out5a_param_0(0.817) 
I0925 07:07:16.390507  4458 net.cpp:2312] res2a_branch2a_param_0(0.729) 
I0925 07:07:16.390509  4458 net.cpp:2312] res2a_branch2b_param_0(0.565) 
I0925 07:07:16.390511  4458 net.cpp:2312] res3a_branch2a_param_0(0.76) 
I0925 07:07:16.390513  4458 net.cpp:2312] res3a_branch2b_param_0(0.693) 
I0925 07:07:16.390516  4458 net.cpp:2312] res4a_branch2a_param_0(0.797) 
I0925 07:07:16.390517  4458 net.cpp:2312] res4a_branch2b_param_0(0.762) 
I0925 07:07:16.390519  4458 net.cpp:2312] res5a_branch2a_param_0(0.817) 
I0925 07:07:16.390522  4458 net.cpp:2312] res5a_branch2b_param_0(0.819) 
I0925 07:07:16.390523  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.16549e+06/2.69117e+06) 0.805
I0925 07:07:16.390532  4458 solver.cpp:562] Iteration 44000, Testing net (#0)
I0925 07:07:19.398516  4441 data_reader.cpp:305] Starting prefetch of epoch 7
I0925 07:07:26.310091  4458 solver.cpp:654]     Test net output #0: accuracy/top1 = 0.955378
I0925 07:07:26.310112  4458 solver.cpp:654]     Test net output #1: accuracy/top5 = 1
I0925 07:07:26.310120  4458 solver.cpp:654]     Test net output #2: loss = 0.132225 (* 1 = 0.132225 loss)
I0925 07:07:26.310139  4458 solver.cpp:265] [MultiGPU] Tests completed in 9.91933s
I0925 07:07:26.493839  4458 solver.cpp:314] Iteration 44000 (3.57129 iter/s, 28.0011s/100 iter), loss = 0.0789046
I0925 07:07:26.493863  4458 solver.cpp:336]     Train net output #0: loss = 0.0789047 (* 1 = 0.0789047 loss)
I0925 07:07:26.493867  4458 sgd_solver.cpp:136] Iteration 44000, lr = 0.001, m = 0.9
I0925 07:07:44.490280  4458 solver.cpp:314] Iteration 44100 (5.55681 iter/s, 17.9959s/100 iter), loss = 0.0633198
I0925 07:07:44.490329  4458 solver.cpp:336]     Train net output #0: loss = 0.0633199 (* 1 = 0.0633199 loss)
I0925 07:07:44.490334  4458 sgd_solver.cpp:136] Iteration 44100, lr = 0.001, m = 0.9
I0925 07:07:49.201858  4462 data_reader.cpp:305] Starting prefetch of epoch 23
I0925 07:08:02.617038  4458 solver.cpp:314] Iteration 44200 (5.51686 iter/s, 18.1262s/100 iter), loss = 0.059996
I0925 07:08:02.617069  4458 solver.cpp:336]     Train net output #0: loss = 0.0599961 (* 1 = 0.0599961 loss)
I0925 07:08:02.617076  4458 sgd_solver.cpp:136] Iteration 44200, lr = 0.001, m = 0.9
I0925 07:08:20.693454  4458 solver.cpp:314] Iteration 44300 (5.53222 iter/s, 18.0759s/100 iter), loss = 0.0419566
I0925 07:08:20.693533  4458 solver.cpp:336]     Train net output #0: loss = 0.0419567 (* 1 = 0.0419567 loss)
I0925 07:08:20.693539  4458 sgd_solver.cpp:136] Iteration 44300, lr = 0.001, m = 0.9
I0925 07:08:38.843996  4458 solver.cpp:314] Iteration 44400 (5.50963 iter/s, 18.15s/100 iter), loss = 0.0498702
I0925 07:08:38.844020  4458 solver.cpp:336]     Train net output #0: loss = 0.0498703 (* 1 = 0.0498703 loss)
I0925 07:08:38.844025  4458 sgd_solver.cpp:136] Iteration 44400, lr = 0.001, m = 0.9
I0925 07:08:49.024715  4408 data_reader.cpp:305] Starting prefetch of epoch 34
I0925 07:08:56.869999  4458 solver.cpp:314] Iteration 44500 (5.5477 iter/s, 18.0255s/100 iter), loss = 0.0554961
I0925 07:08:56.870049  4458 solver.cpp:336]     Train net output #0: loss = 0.0554962 (* 1 = 0.0554962 loss)
I0925 07:08:56.870054  4458 sgd_solver.cpp:136] Iteration 44500, lr = 0.001, m = 0.9
I0925 07:09:15.044163  4458 solver.cpp:314] Iteration 44600 (5.50247 iter/s, 18.1737s/100 iter), loss = 0.0683129
I0925 07:09:15.044196  4458 solver.cpp:336]     Train net output #0: loss = 0.0683129 (* 1 = 0.0683129 loss)
I0925 07:09:15.044201  4458 sgd_solver.cpp:136] Iteration 44600, lr = 0.001, m = 0.9
I0925 07:09:33.113430  4458 solver.cpp:314] Iteration 44700 (5.53441 iter/s, 18.0688s/100 iter), loss = 0.0636192
I0925 07:09:33.113523  4458 solver.cpp:336]     Train net output #0: loss = 0.0636193 (* 1 = 0.0636193 loss)
I0925 07:09:33.113529  4458 sgd_solver.cpp:136] Iteration 44700, lr = 0.001, m = 0.9
I0925 07:09:48.969375  4410 data_reader.cpp:305] Starting prefetch of epoch 29
I0925 07:09:51.360740  4458 solver.cpp:314] Iteration 44800 (5.48041 iter/s, 18.2468s/100 iter), loss = 0.0808575
I0925 07:09:51.360765  4458 solver.cpp:336]     Train net output #0: loss = 0.0808576 (* 1 = 0.0808576 loss)
I0925 07:09:51.360771  4458 sgd_solver.cpp:136] Iteration 44800, lr = 0.001, m = 0.9
I0925 07:10:09.580689  4458 solver.cpp:314] Iteration 44900 (5.48864 iter/s, 18.2194s/100 iter), loss = 0.0523503
I0925 07:10:09.580799  4458 solver.cpp:336]     Train net output #0: loss = 0.0523504 (* 1 = 0.0523504 loss)
I0925 07:10:09.580807  4458 sgd_solver.cpp:136] Iteration 44900, lr = 0.001, m = 0.9
I0925 07:10:27.536473  4458 solver.cpp:368] Sparsity after update:
I0925 07:10:27.547945  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 07:10:27.547963  4458 net.cpp:2312] conv1a_param_0(0.255) 
I0925 07:10:27.547971  4458 net.cpp:2312] conv1b_param_0(0.587) 
I0925 07:10:27.547973  4458 net.cpp:2312] ctx_conv1_param_0(0.783) 
I0925 07:10:27.547977  4458 net.cpp:2312] ctx_conv2_param_0(0.795) 
I0925 07:10:27.547981  4458 net.cpp:2312] ctx_conv3_param_0(0.785) 
I0925 07:10:27.547983  4458 net.cpp:2312] ctx_conv4_param_0(0.797) 
I0925 07:10:27.547986  4458 net.cpp:2312] ctx_final_param_0(0.33) 
I0925 07:10:27.547989  4458 net.cpp:2312] out3a_param_0(0.819) 
I0925 07:10:27.547993  4458 net.cpp:2312] out5a_param_0(0.817) 
I0925 07:10:27.547997  4458 net.cpp:2312] res2a_branch2a_param_0(0.729) 
I0925 07:10:27.548001  4458 net.cpp:2312] res2a_branch2b_param_0(0.565) 
I0925 07:10:27.548004  4458 net.cpp:2312] res3a_branch2a_param_0(0.76) 
I0925 07:10:27.548007  4458 net.cpp:2312] res3a_branch2b_param_0(0.693) 
I0925 07:10:27.548010  4458 net.cpp:2312] res4a_branch2a_param_0(0.797) 
I0925 07:10:27.548013  4458 net.cpp:2312] res4a_branch2b_param_0(0.762) 
I0925 07:10:27.548017  4458 net.cpp:2312] res5a_branch2a_param_0(0.817) 
I0925 07:10:27.548020  4458 net.cpp:2312] res5a_branch2b_param_0(0.819) 
I0925 07:10:27.548023  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.16549e+06/2.69117e+06) 0.805
I0925 07:10:27.613852  4541 sgd_solver.cpp:48] MultiStep Status: Iteration 45000, step = 2
I0925 07:10:27.613873  4538 sgd_solver.cpp:48] MultiStep Status: Iteration 45000, step = 2
I0925 07:10:27.613874  4540 sgd_solver.cpp:48] MultiStep Status: Iteration 45000, step = 2
I0925 07:10:27.714576  4458 solver.cpp:314] Iteration 45000 (5.51469 iter/s, 18.1334s/100 iter), loss = 0.0692548
I0925 07:10:27.714598  4458 solver.cpp:336]     Train net output #0: loss = 0.0692549 (* 1 = 0.0692549 loss)
I0925 07:10:27.714602  4458 sgd_solver.cpp:136] Iteration 45000, lr = 0.0001, m = 0.9
I0925 07:10:45.750922  4458 solver.cpp:314] Iteration 45100 (5.54451 iter/s, 18.0358s/100 iter), loss = 0.0938818
I0925 07:10:45.751197  4458 solver.cpp:336]     Train net output #0: loss = 0.0938818 (* 1 = 0.0938818 loss)
I0925 07:10:45.751204  4458 sgd_solver.cpp:136] Iteration 45100, lr = 0.0001, m = 0.9
I0925 07:10:48.782889  4464 data_reader.cpp:305] Starting prefetch of epoch 27
I0925 07:11:03.827888  4458 solver.cpp:314] Iteration 45200 (5.53206 iter/s, 18.0765s/100 iter), loss = 0.0571101
I0925 07:11:03.827913  4458 solver.cpp:336]     Train net output #0: loss = 0.0571101 (* 1 = 0.0571101 loss)
I0925 07:11:03.827916  4458 sgd_solver.cpp:136] Iteration 45200, lr = 0.0001, m = 0.9
I0925 07:11:18.721776  4462 data_reader.cpp:305] Starting prefetch of epoch 24
I0925 07:11:21.769505  4458 solver.cpp:314] Iteration 45300 (5.57379 iter/s, 17.9411s/100 iter), loss = 0.0655972
I0925 07:11:21.769527  4458 solver.cpp:336]     Train net output #0: loss = 0.0655973 (* 1 = 0.0655973 loss)
I0925 07:11:21.769532  4458 sgd_solver.cpp:136] Iteration 45300, lr = 0.0001, m = 0.9
I0925 07:11:39.785423  4458 solver.cpp:314] Iteration 45400 (5.5508 iter/s, 18.0154s/100 iter), loss = 0.0612365
I0925 07:11:39.785449  4458 solver.cpp:336]     Train net output #0: loss = 0.0612366 (* 1 = 0.0612366 loss)
I0925 07:11:39.785452  4458 sgd_solver.cpp:136] Iteration 45400, lr = 0.0001, m = 0.9
I0925 07:11:57.878957  4458 solver.cpp:314] Iteration 45500 (5.52699 iter/s, 18.093s/100 iter), loss = 0.0600473
I0925 07:11:57.879030  4458 solver.cpp:336]     Train net output #0: loss = 0.0600474 (* 1 = 0.0600474 loss)
I0925 07:11:57.879036  4458 sgd_solver.cpp:136] Iteration 45500, lr = 0.0001, m = 0.9
I0925 07:12:15.855865  4458 solver.cpp:314] Iteration 45600 (5.56285 iter/s, 17.9764s/100 iter), loss = 0.0740276
I0925 07:12:15.855887  4458 solver.cpp:336]     Train net output #0: loss = 0.0740277 (* 1 = 0.0740277 loss)
I0925 07:12:15.855891  4458 sgd_solver.cpp:136] Iteration 45600, lr = 0.0001, m = 0.9
I0925 07:12:18.259428  4461 data_reader.cpp:305] Starting prefetch of epoch 25
I0925 07:12:33.976485  4458 solver.cpp:314] Iteration 45700 (5.51873 iter/s, 18.1201s/100 iter), loss = 0.0399552
I0925 07:12:33.976569  4458 solver.cpp:336]     Train net output #0: loss = 0.0399553 (* 1 = 0.0399553 loss)
I0925 07:12:33.976575  4458 sgd_solver.cpp:136] Iteration 45700, lr = 0.0001, m = 0.9
I0925 07:12:52.012184  4458 solver.cpp:314] Iteration 45800 (5.54471 iter/s, 18.0352s/100 iter), loss = 0.0904042
I0925 07:12:52.012208  4458 solver.cpp:336]     Train net output #0: loss = 0.0904042 (* 1 = 0.0904042 loss)
I0925 07:12:52.012213  4458 sgd_solver.cpp:136] Iteration 45800, lr = 0.0001, m = 0.9
I0925 07:13:10.006197  4458 solver.cpp:314] Iteration 45900 (5.55756 iter/s, 17.9935s/100 iter), loss = 0.0432293
I0925 07:13:10.006273  4458 solver.cpp:336]     Train net output #0: loss = 0.0432294 (* 1 = 0.0432294 loss)
I0925 07:13:10.006278  4458 sgd_solver.cpp:136] Iteration 45900, lr = 0.0001, m = 0.9
I0925 07:13:17.909744  4466 data_reader.cpp:305] Starting prefetch of epoch 24
I0925 07:13:27.901124  4458 solver.cpp:368] Sparsity after update:
I0925 07:13:27.904451  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 07:13:27.904459  4458 net.cpp:2312] conv1a_param_0(0.255) 
I0925 07:13:27.904465  4458 net.cpp:2312] conv1b_param_0(0.587) 
I0925 07:13:27.904469  4458 net.cpp:2312] ctx_conv1_param_0(0.783) 
I0925 07:13:27.904471  4458 net.cpp:2312] ctx_conv2_param_0(0.795) 
I0925 07:13:27.904475  4458 net.cpp:2312] ctx_conv3_param_0(0.785) 
I0925 07:13:27.904479  4458 net.cpp:2312] ctx_conv4_param_0(0.797) 
I0925 07:13:27.904484  4458 net.cpp:2312] ctx_final_param_0(0.33) 
I0925 07:13:27.904486  4458 net.cpp:2312] out3a_param_0(0.819) 
I0925 07:13:27.904490  4458 net.cpp:2312] out5a_param_0(0.817) 
I0925 07:13:27.904494  4458 net.cpp:2312] res2a_branch2a_param_0(0.729) 
I0925 07:13:27.904497  4458 net.cpp:2312] res2a_branch2b_param_0(0.565) 
I0925 07:13:27.904501  4458 net.cpp:2312] res3a_branch2a_param_0(0.76) 
I0925 07:13:27.904505  4458 net.cpp:2312] res3a_branch2b_param_0(0.693) 
I0925 07:13:27.904507  4458 net.cpp:2312] res4a_branch2a_param_0(0.797) 
I0925 07:13:27.904511  4458 net.cpp:2312] res4a_branch2b_param_0(0.762) 
I0925 07:13:27.904515  4458 net.cpp:2312] res5a_branch2a_param_0(0.817) 
I0925 07:13:27.904520  4458 net.cpp:2312] res5a_branch2b_param_0(0.819) 
I0925 07:13:27.904523  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.16549e+06/2.69117e+06) 0.805
I0925 07:13:27.904536  4458 solver.cpp:562] Iteration 46000, Testing net (#0)
I0925 07:13:34.228155  4441 data_reader.cpp:305] Starting prefetch of epoch 8
I0925 07:13:37.773768  4458 solver.cpp:654]     Test net output #0: accuracy/top1 = 0.956837
I0925 07:13:37.773788  4458 solver.cpp:654]     Test net output #1: accuracy/top5 = 1
I0925 07:13:37.773793  4458 solver.cpp:654]     Test net output #2: loss = 0.157845 (* 1 = 0.157845 loss)
I0925 07:13:37.773818  4458 solver.cpp:265] [MultiGPU] Tests completed in 9.86901s
I0925 07:13:37.955461  4458 solver.cpp:314] Iteration 46000 (3.57801 iter/s, 27.9485s/100 iter), loss = 0.0568432
I0925 07:13:37.955487  4458 solver.cpp:336]     Train net output #0: loss = 0.0568432 (* 1 = 0.0568432 loss)
I0925 07:13:37.955492  4458 sgd_solver.cpp:136] Iteration 46000, lr = 0.0001, m = 0.9
I0925 07:13:55.990118  4458 solver.cpp:314] Iteration 46100 (5.54503 iter/s, 18.0342s/100 iter), loss = 0.0437733
I0925 07:13:55.990201  4458 solver.cpp:336]     Train net output #0: loss = 0.0437733 (* 1 = 0.0437733 loss)
I0925 07:13:55.990207  4458 sgd_solver.cpp:136] Iteration 46100, lr = 0.0001, m = 0.9
I0925 07:14:14.054293  4458 solver.cpp:314] Iteration 46200 (5.53597 iter/s, 18.0637s/100 iter), loss = 0.0416872
I0925 07:14:14.054321  4458 solver.cpp:336]     Train net output #0: loss = 0.0416873 (* 1 = 0.0416873 loss)
I0925 07:14:14.054324  4458 sgd_solver.cpp:136] Iteration 46200, lr = 0.0001, m = 0.9
I0925 07:14:27.468225  4408 data_reader.cpp:305] Starting prefetch of epoch 35
I0925 07:14:32.110194  4458 solver.cpp:314] Iteration 46300 (5.53851 iter/s, 18.0554s/100 iter), loss = 0.0577247
I0925 07:14:32.110215  4458 solver.cpp:336]     Train net output #0: loss = 0.0577248 (* 1 = 0.0577248 loss)
I0925 07:14:32.110219  4458 sgd_solver.cpp:136] Iteration 46300, lr = 0.0001, m = 0.9
I0925 07:14:50.313026  4458 solver.cpp:314] Iteration 46400 (5.4938 iter/s, 18.2023s/100 iter), loss = 0.0673874
I0925 07:14:50.313050  4458 solver.cpp:336]     Train net output #0: loss = 0.0673875 (* 1 = 0.0673875 loss)
I0925 07:14:50.313053  4458 sgd_solver.cpp:136] Iteration 46400, lr = 0.0001, m = 0.9
I0925 07:15:08.452074  4458 solver.cpp:314] Iteration 46500 (5.51312 iter/s, 18.1385s/100 iter), loss = 0.0890136
I0925 07:15:08.452126  4458 solver.cpp:336]     Train net output #0: loss = 0.0890137 (* 1 = 0.0890137 loss)
I0925 07:15:08.452132  4458 sgd_solver.cpp:136] Iteration 46500, lr = 0.0001, m = 0.9
I0925 07:15:26.507176  4458 solver.cpp:314] Iteration 46600 (5.53875 iter/s, 18.0546s/100 iter), loss = 0.0407471
I0925 07:15:26.507197  4458 solver.cpp:336]     Train net output #0: loss = 0.0407472 (* 1 = 0.0407472 loss)
I0925 07:15:26.507201  4458 sgd_solver.cpp:136] Iteration 46600, lr = 0.0001, m = 0.9
I0925 07:15:27.426720  4464 data_reader.cpp:305] Starting prefetch of epoch 28
I0925 07:15:44.566144  4458 solver.cpp:314] Iteration 46700 (5.53757 iter/s, 18.0585s/100 iter), loss = 0.0662573
I0925 07:15:44.566190  4458 solver.cpp:336]     Train net output #0: loss = 0.0662574 (* 1 = 0.0662574 loss)
I0925 07:15:44.566197  4458 sgd_solver.cpp:136] Iteration 46700, lr = 0.0001, m = 0.9
I0925 07:16:02.578083  4458 solver.cpp:314] Iteration 46800 (5.55203 iter/s, 18.0114s/100 iter), loss = 0.077989
I0925 07:16:02.578114  4458 solver.cpp:336]     Train net output #0: loss = 0.077989 (* 1 = 0.077989 loss)
I0925 07:16:02.578121  4458 sgd_solver.cpp:136] Iteration 46800, lr = 0.0001, m = 0.9
I0925 07:16:20.717550  4458 solver.cpp:314] Iteration 46900 (5.51299 iter/s, 18.139s/100 iter), loss = 0.0939513
I0925 07:16:20.717648  4458 solver.cpp:336]     Train net output #0: loss = 0.0939514 (* 1 = 0.0939514 loss)
I0925 07:16:20.717654  4458 sgd_solver.cpp:136] Iteration 46900, lr = 0.0001, m = 0.9
I0925 07:16:27.090797  4466 data_reader.cpp:305] Starting prefetch of epoch 25
I0925 07:16:38.652199  4458 solver.cpp:368] Sparsity after update:
I0925 07:16:38.671726  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 07:16:38.671744  4458 net.cpp:2312] conv1a_param_0(0.255) 
I0925 07:16:38.671751  4458 net.cpp:2312] conv1b_param_0(0.587) 
I0925 07:16:38.671753  4458 net.cpp:2312] ctx_conv1_param_0(0.783) 
I0925 07:16:38.671756  4458 net.cpp:2312] ctx_conv2_param_0(0.795) 
I0925 07:16:38.671757  4458 net.cpp:2312] ctx_conv3_param_0(0.785) 
I0925 07:16:38.671758  4458 net.cpp:2312] ctx_conv4_param_0(0.797) 
I0925 07:16:38.671761  4458 net.cpp:2312] ctx_final_param_0(0.33) 
I0925 07:16:38.671762  4458 net.cpp:2312] out3a_param_0(0.819) 
I0925 07:16:38.671764  4458 net.cpp:2312] out5a_param_0(0.817) 
I0925 07:16:38.671766  4458 net.cpp:2312] res2a_branch2a_param_0(0.729) 
I0925 07:16:38.671768  4458 net.cpp:2312] res2a_branch2b_param_0(0.565) 
I0925 07:16:38.671771  4458 net.cpp:2312] res3a_branch2a_param_0(0.76) 
I0925 07:16:38.671772  4458 net.cpp:2312] res3a_branch2b_param_0(0.693) 
I0925 07:16:38.671774  4458 net.cpp:2312] res4a_branch2a_param_0(0.797) 
I0925 07:16:38.671777  4458 net.cpp:2312] res4a_branch2b_param_0(0.762) 
I0925 07:16:38.671778  4458 net.cpp:2312] res5a_branch2a_param_0(0.817) 
I0925 07:16:38.671780  4458 net.cpp:2312] res5a_branch2b_param_0(0.819) 
I0925 07:16:38.671782  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.16549e+06/2.69117e+06) 0.805
I0925 07:16:38.839000  4458 solver.cpp:314] Iteration 47000 (5.51848 iter/s, 18.1209s/100 iter), loss = 0.0419665
I0925 07:16:38.839022  4458 solver.cpp:336]     Train net output #0: loss = 0.0419666 (* 1 = 0.0419666 loss)
I0925 07:16:38.839026  4458 sgd_solver.cpp:136] Iteration 47000, lr = 0.0001, m = 0.9
I0925 07:16:57.031224  4458 solver.cpp:314] Iteration 47100 (5.49701 iter/s, 18.1917s/100 iter), loss = 0.0644104
I0925 07:16:57.031306  4458 solver.cpp:336]     Train net output #0: loss = 0.0644104 (* 1 = 0.0644104 loss)
I0925 07:16:57.031312  4458 sgd_solver.cpp:136] Iteration 47100, lr = 0.0001, m = 0.9
I0925 07:16:57.263473  4461 data_reader.cpp:305] Starting prefetch of epoch 26
I0925 07:17:15.122362  4458 solver.cpp:314] Iteration 47200 (5.52772 iter/s, 18.0906s/100 iter), loss = 0.0517039
I0925 07:17:15.122385  4458 solver.cpp:336]     Train net output #0: loss = 0.051704 (* 1 = 0.051704 loss)
I0925 07:17:15.122388  4458 sgd_solver.cpp:136] Iteration 47200, lr = 0.0001, m = 0.9
I0925 07:17:33.257625  4458 solver.cpp:314] Iteration 47300 (5.51427 iter/s, 18.1348s/100 iter), loss = 0.173012
I0925 07:17:33.257681  4458 solver.cpp:336]     Train net output #0: loss = 0.173012 (* 1 = 0.173012 loss)
I0925 07:17:33.257686  4458 sgd_solver.cpp:136] Iteration 47300, lr = 0.0001, m = 0.9
I0925 07:17:51.298830  4458 solver.cpp:314] Iteration 47400 (5.54302 iter/s, 18.0407s/100 iter), loss = 0.0357617
I0925 07:17:51.298854  4458 solver.cpp:336]     Train net output #0: loss = 0.0357617 (* 1 = 0.0357617 loss)
I0925 07:17:51.298859  4458 sgd_solver.cpp:136] Iteration 47400, lr = 0.0001, m = 0.9
I0925 07:17:56.894183  4408 data_reader.cpp:305] Starting prefetch of epoch 36
I0925 07:18:09.350375  4458 solver.cpp:314] Iteration 47500 (5.53985 iter/s, 18.051s/100 iter), loss = 0.0482762
I0925 07:18:09.350452  4458 solver.cpp:336]     Train net output #0: loss = 0.0482763 (* 1 = 0.0482763 loss)
I0925 07:18:09.350457  4458 sgd_solver.cpp:136] Iteration 47500, lr = 0.0001, m = 0.9
I0925 07:18:27.522879  4458 solver.cpp:314] Iteration 47600 (5.50297 iter/s, 18.172s/100 iter), loss = 0.0760428
I0925 07:18:27.522907  4458 solver.cpp:336]     Train net output #0: loss = 0.0760428 (* 1 = 0.0760428 loss)
I0925 07:18:27.522914  4458 sgd_solver.cpp:136] Iteration 47600, lr = 0.0001, m = 0.9
I0925 07:18:45.688140  4458 solver.cpp:314] Iteration 47700 (5.50517 iter/s, 18.1648s/100 iter), loss = 0.0610519
I0925 07:18:45.688210  4458 solver.cpp:336]     Train net output #0: loss = 0.0610519 (* 1 = 0.0610519 loss)
I0925 07:18:45.688215  4458 sgd_solver.cpp:136] Iteration 47700, lr = 0.0001, m = 0.9
I0925 07:18:57.031774  4464 data_reader.cpp:305] Starting prefetch of epoch 29
I0925 07:19:03.929559  4458 solver.cpp:314] Iteration 47800 (5.48218 iter/s, 18.2409s/100 iter), loss = 0.0868303
I0925 07:19:03.929582  4458 solver.cpp:336]     Train net output #0: loss = 0.0868304 (* 1 = 0.0868304 loss)
I0925 07:19:03.929586  4458 sgd_solver.cpp:136] Iteration 47800, lr = 0.0001, m = 0.9
I0925 07:19:21.955682  4458 solver.cpp:314] Iteration 47900 (5.54766 iter/s, 18.0256s/100 iter), loss = 0.0584209
I0925 07:19:21.955730  4458 solver.cpp:336]     Train net output #0: loss = 0.058421 (* 1 = 0.058421 loss)
I0925 07:19:21.955735  4458 sgd_solver.cpp:136] Iteration 47900, lr = 0.0001, m = 0.9
I0925 07:19:39.944753  4458 solver.cpp:368] Sparsity after update:
I0925 07:19:39.972568  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 07:19:39.972584  4458 net.cpp:2312] conv1a_param_0(0.255) 
I0925 07:19:39.972590  4458 net.cpp:2312] conv1b_param_0(0.587) 
I0925 07:19:39.972592  4458 net.cpp:2312] ctx_conv1_param_0(0.783) 
I0925 07:19:39.972594  4458 net.cpp:2312] ctx_conv2_param_0(0.795) 
I0925 07:19:39.972596  4458 net.cpp:2312] ctx_conv3_param_0(0.785) 
I0925 07:19:39.972599  4458 net.cpp:2312] ctx_conv4_param_0(0.797) 
I0925 07:19:39.972600  4458 net.cpp:2312] ctx_final_param_0(0.33) 
I0925 07:19:39.972602  4458 net.cpp:2312] out3a_param_0(0.819) 
I0925 07:19:39.972604  4458 net.cpp:2312] out5a_param_0(0.817) 
I0925 07:19:39.972606  4458 net.cpp:2312] res2a_branch2a_param_0(0.729) 
I0925 07:19:39.972609  4458 net.cpp:2312] res2a_branch2b_param_0(0.565) 
I0925 07:19:39.972610  4458 net.cpp:2312] res3a_branch2a_param_0(0.76) 
I0925 07:19:39.972611  4458 net.cpp:2312] res3a_branch2b_param_0(0.693) 
I0925 07:19:39.972614  4458 net.cpp:2312] res4a_branch2a_param_0(0.797) 
I0925 07:19:39.972615  4458 net.cpp:2312] res4a_branch2b_param_0(0.762) 
I0925 07:19:39.972617  4458 net.cpp:2312] res5a_branch2a_param_0(0.817) 
I0925 07:19:39.972620  4458 net.cpp:2312] res5a_branch2b_param_0(0.819) 
I0925 07:19:39.972621  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.16549e+06/2.69117e+06) 0.805
I0925 07:19:39.972630  4458 solver.cpp:562] Iteration 48000, Testing net (#0)
I0925 07:19:42.924214  4446 data_reader.cpp:305] Starting prefetch of epoch 1
I0925 07:19:49.664628  4524 data_reader.cpp:305] Starting prefetch of epoch 4
I0925 07:19:49.971365  4458 solver.cpp:654]     Test net output #0: accuracy/top1 = 0.955554
I0925 07:19:49.971385  4458 solver.cpp:654]     Test net output #1: accuracy/top5 = 1
I0925 07:19:49.971390  4458 solver.cpp:654]     Test net output #2: loss = 0.132431 (* 1 = 0.132431 loss)
I0925 07:19:49.971472  4458 solver.cpp:265] [MultiGPU] Tests completed in 9.99856s
I0925 07:19:50.154721  4458 solver.cpp:314] Iteration 48000 (3.54632 iter/s, 28.1983s/100 iter), loss = 0.0725329
I0925 07:19:50.154743  4458 solver.cpp:336]     Train net output #0: loss = 0.0725329 (* 1 = 0.0725329 loss)
I0925 07:19:50.154747  4458 sgd_solver.cpp:136] Iteration 48000, lr = 0.0001, m = 0.9
I0925 07:20:08.101258  4458 solver.cpp:314] Iteration 48100 (5.57226 iter/s, 17.946s/100 iter), loss = 0.0402491
I0925 07:20:08.101337  4458 solver.cpp:336]     Train net output #0: loss = 0.0402492 (* 1 = 0.0402492 loss)
I0925 07:20:08.101346  4458 sgd_solver.cpp:136] Iteration 48100, lr = 0.0001, m = 0.9
I0925 07:20:26.147776  4458 solver.cpp:314] Iteration 48200 (5.54139 iter/s, 18.046s/100 iter), loss = 0.0561561
I0925 07:20:26.147804  4458 solver.cpp:336]     Train net output #0: loss = 0.0561562 (* 1 = 0.0561562 loss)
I0925 07:20:26.147810  4458 sgd_solver.cpp:136] Iteration 48200, lr = 0.0001, m = 0.9
I0925 07:20:36.630612  4410 data_reader.cpp:305] Starting prefetch of epoch 30
I0925 07:20:36.630612  4466 data_reader.cpp:305] Starting prefetch of epoch 26
I0925 07:20:44.216482  4458 solver.cpp:314] Iteration 48300 (5.53459 iter/s, 18.0682s/100 iter), loss = 0.0367575
I0925 07:20:44.216589  4458 solver.cpp:336]     Train net output #0: loss = 0.0367575 (* 1 = 0.0367575 loss)
I0925 07:20:44.216599  4458 sgd_solver.cpp:136] Iteration 48300, lr = 0.0001, m = 0.9
I0925 07:21:02.239847  4458 solver.cpp:314] Iteration 48400 (5.54851 iter/s, 18.0229s/100 iter), loss = 0.0536779
I0925 07:21:02.239871  4458 solver.cpp:336]     Train net output #0: loss = 0.0536779 (* 1 = 0.0536779 loss)
I0925 07:21:02.239876  4458 sgd_solver.cpp:136] Iteration 48400, lr = 0.0001, m = 0.9
I0925 07:21:20.295902  4458 solver.cpp:314] Iteration 48500 (5.53846 iter/s, 18.0556s/100 iter), loss = 0.0494928
I0925 07:21:20.295953  4458 solver.cpp:336]     Train net output #0: loss = 0.0494928 (* 1 = 0.0494928 loss)
I0925 07:21:20.295958  4458 sgd_solver.cpp:136] Iteration 48500, lr = 0.0001, m = 0.9
I0925 07:21:36.198184  4464 data_reader.cpp:305] Starting prefetch of epoch 30
I0925 07:21:38.327203  4458 solver.cpp:314] Iteration 48600 (5.54607 iter/s, 18.0308s/100 iter), loss = 0.0472223
I0925 07:21:38.327227  4458 solver.cpp:336]     Train net output #0: loss = 0.0472223 (* 1 = 0.0472223 loss)
I0925 07:21:38.327231  4458 sgd_solver.cpp:136] Iteration 48600, lr = 0.0001, m = 0.9
I0925 07:21:56.432585  4458 solver.cpp:314] Iteration 48700 (5.52337 iter/s, 18.1049s/100 iter), loss = 0.0440399
I0925 07:21:56.432639  4458 solver.cpp:336]     Train net output #0: loss = 0.04404 (* 1 = 0.04404 loss)
I0925 07:21:56.432644  4458 sgd_solver.cpp:136] Iteration 48700, lr = 0.0001, m = 0.9
I0925 07:22:14.618237  4458 solver.cpp:314] Iteration 48800 (5.49899 iter/s, 18.1851s/100 iter), loss = 0.0795135
I0925 07:22:14.618259  4458 solver.cpp:336]     Train net output #0: loss = 0.0795136 (* 1 = 0.0795136 loss)
I0925 07:22:14.618264  4458 sgd_solver.cpp:136] Iteration 48800, lr = 0.0001, m = 0.9
I0925 07:22:32.603736  4458 solver.cpp:314] Iteration 48900 (5.56019 iter/s, 17.985s/100 iter), loss = 0.0536676
I0925 07:22:32.603816  4458 solver.cpp:336]     Train net output #0: loss = 0.0536677 (* 1 = 0.0536677 loss)
I0925 07:22:32.603821  4458 sgd_solver.cpp:136] Iteration 48900, lr = 0.0001, m = 0.9
I0925 07:22:36.027503  4464 data_reader.cpp:305] Starting prefetch of epoch 31
I0925 07:22:50.619850  4458 solver.cpp:368] Sparsity after update:
I0925 07:22:50.643594  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 07:22:50.643615  4458 net.cpp:2312] conv1a_param_0(0.255) 
I0925 07:22:50.643623  4458 net.cpp:2312] conv1b_param_0(0.587) 
I0925 07:22:50.643627  4458 net.cpp:2312] ctx_conv1_param_0(0.783) 
I0925 07:22:50.643630  4458 net.cpp:2312] ctx_conv2_param_0(0.795) 
I0925 07:22:50.643633  4458 net.cpp:2312] ctx_conv3_param_0(0.785) 
I0925 07:22:50.643636  4458 net.cpp:2312] ctx_conv4_param_0(0.797) 
I0925 07:22:50.643640  4458 net.cpp:2312] ctx_final_param_0(0.33) 
I0925 07:22:50.643643  4458 net.cpp:2312] out3a_param_0(0.819) 
I0925 07:22:50.643646  4458 net.cpp:2312] out5a_param_0(0.817) 
I0925 07:22:50.643649  4458 net.cpp:2312] res2a_branch2a_param_0(0.729) 
I0925 07:22:50.643652  4458 net.cpp:2312] res2a_branch2b_param_0(0.565) 
I0925 07:22:50.643656  4458 net.cpp:2312] res3a_branch2a_param_0(0.76) 
I0925 07:22:50.643658  4458 net.cpp:2312] res3a_branch2b_param_0(0.693) 
I0925 07:22:50.643661  4458 net.cpp:2312] res4a_branch2a_param_0(0.797) 
I0925 07:22:50.643664  4458 net.cpp:2312] res4a_branch2b_param_0(0.762) 
I0925 07:22:50.643669  4458 net.cpp:2312] res5a_branch2a_param_0(0.817) 
I0925 07:22:50.643673  4458 net.cpp:2312] res5a_branch2b_param_0(0.819) 
I0925 07:22:50.643677  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.16549e+06/2.69117e+06) 0.805
I0925 07:22:50.812800  4458 solver.cpp:314] Iteration 49000 (5.49192 iter/s, 18.2086s/100 iter), loss = 0.0746335
I0925 07:22:50.812827  4458 solver.cpp:336]     Train net output #0: loss = 0.0746336 (* 1 = 0.0746336 loss)
I0925 07:22:50.812834  4458 sgd_solver.cpp:136] Iteration 49000, lr = 0.0001, m = 0.9
I0925 07:23:05.962785  4408 data_reader.cpp:305] Starting prefetch of epoch 37
I0925 07:23:08.790634  4458 solver.cpp:314] Iteration 49100 (5.56256 iter/s, 17.9773s/100 iter), loss = 0.0595348
I0925 07:23:08.790658  4458 solver.cpp:336]     Train net output #0: loss = 0.0595348 (* 1 = 0.0595348 loss)
I0925 07:23:08.790663  4458 sgd_solver.cpp:136] Iteration 49100, lr = 0.0001, m = 0.9
I0925 07:23:26.842896  4458 solver.cpp:314] Iteration 49200 (5.53963 iter/s, 18.0518s/100 iter), loss = 0.0631237
I0925 07:23:26.842922  4458 solver.cpp:336]     Train net output #0: loss = 0.0631238 (* 1 = 0.0631238 loss)
I0925 07:23:26.842927  4458 sgd_solver.cpp:136] Iteration 49200, lr = 0.0001, m = 0.9
I0925 07:23:44.956192  4458 solver.cpp:314] Iteration 49300 (5.52096 iter/s, 18.1128s/100 iter), loss = 0.0746806
I0925 07:23:44.956241  4458 solver.cpp:336]     Train net output #0: loss = 0.0746806 (* 1 = 0.0746806 loss)
I0925 07:23:44.956246  4458 sgd_solver.cpp:136] Iteration 49300, lr = 0.0001, m = 0.9
I0925 07:24:02.981817  4458 solver.cpp:314] Iteration 49400 (5.54781 iter/s, 18.0251s/100 iter), loss = 0.0613196
I0925 07:24:02.981840  4458 solver.cpp:336]     Train net output #0: loss = 0.0613197 (* 1 = 0.0613197 loss)
I0925 07:24:02.981844  4458 sgd_solver.cpp:136] Iteration 49400, lr = 0.0001, m = 0.9
I0925 07:24:05.735869  4466 data_reader.cpp:305] Starting prefetch of epoch 27
I0925 07:24:21.030916  4458 solver.cpp:314] Iteration 49500 (5.5406 iter/s, 18.0486s/100 iter), loss = 0.0469481
I0925 07:24:21.031019  4458 solver.cpp:336]     Train net output #0: loss = 0.0469481 (* 1 = 0.0469481 loss)
I0925 07:24:21.031025  4458 sgd_solver.cpp:136] Iteration 49500, lr = 0.0001, m = 0.9
I0925 07:24:38.995169  4458 solver.cpp:314] Iteration 49600 (5.56677 iter/s, 17.9638s/100 iter), loss = 0.0535168
I0925 07:24:38.995193  4458 solver.cpp:336]     Train net output #0: loss = 0.0535169 (* 1 = 0.0535169 loss)
I0925 07:24:38.995198  4458 sgd_solver.cpp:136] Iteration 49600, lr = 0.0001, m = 0.9
I0925 07:24:57.057421  4458 solver.cpp:314] Iteration 49700 (5.53656 iter/s, 18.0617s/100 iter), loss = 0.0615006
I0925 07:24:57.057536  4458 solver.cpp:336]     Train net output #0: loss = 0.0615007 (* 1 = 0.0615007 loss)
I0925 07:24:57.057545  4458 sgd_solver.cpp:136] Iteration 49700, lr = 0.0001, m = 0.9
I0925 07:25:05.071806  4464 data_reader.cpp:305] Starting prefetch of epoch 32
I0925 07:25:15.047001  4458 solver.cpp:314] Iteration 49800 (5.55893 iter/s, 17.9891s/100 iter), loss = 0.0715598
I0925 07:25:15.047021  4458 solver.cpp:336]     Train net output #0: loss = 0.0715599 (* 1 = 0.0715599 loss)
I0925 07:25:15.047025  4458 sgd_solver.cpp:136] Iteration 49800, lr = 0.0001, m = 0.9
I0925 07:25:33.205225  4458 solver.cpp:314] Iteration 49900 (5.5073 iter/s, 18.1577s/100 iter), loss = 0.0407861
I0925 07:25:33.205272  4458 solver.cpp:336]     Train net output #0: loss = 0.0407862 (* 1 = 0.0407862 loss)
I0925 07:25:33.205277  4458 sgd_solver.cpp:136] Iteration 49900, lr = 0.0001, m = 0.9
I0925 07:25:35.078377  4408 data_reader.cpp:305] Starting prefetch of epoch 38
I0925 07:25:51.257911  4458 solver.cpp:824] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/cityscapes5_jsegnet21v2_iter_50000.caffemodel
I0925 07:25:51.285926  4458 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/cityscapes5_jsegnet21v2_iter_50000.solverstate
I0925 07:25:51.294908  4458 solver.cpp:368] Sparsity after update:
I0925 07:25:51.296732  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 07:25:51.296741  4458 net.cpp:2312] conv1a_param_0(0.255) 
I0925 07:25:51.296749  4458 net.cpp:2312] conv1b_param_0(0.587) 
I0925 07:25:51.296753  4458 net.cpp:2312] ctx_conv1_param_0(0.783) 
I0925 07:25:51.296756  4458 net.cpp:2312] ctx_conv2_param_0(0.795) 
I0925 07:25:51.296759  4458 net.cpp:2312] ctx_conv3_param_0(0.785) 
I0925 07:25:51.296762  4458 net.cpp:2312] ctx_conv4_param_0(0.797) 
I0925 07:25:51.296766  4458 net.cpp:2312] ctx_final_param_0(0.33) 
I0925 07:25:51.296769  4458 net.cpp:2312] out3a_param_0(0.819) 
I0925 07:25:51.296772  4458 net.cpp:2312] out5a_param_0(0.817) 
I0925 07:25:51.296775  4458 net.cpp:2312] res2a_branch2a_param_0(0.729) 
I0925 07:25:51.296778  4458 net.cpp:2312] res2a_branch2b_param_0(0.565) 
I0925 07:25:51.296782  4458 net.cpp:2312] res3a_branch2a_param_0(0.76) 
I0925 07:25:51.296784  4458 net.cpp:2312] res3a_branch2b_param_0(0.693) 
I0925 07:25:51.296787  4458 net.cpp:2312] res4a_branch2a_param_0(0.797) 
I0925 07:25:51.296792  4458 net.cpp:2312] res4a_branch2b_param_0(0.762) 
I0925 07:25:51.296795  4458 net.cpp:2312] res5a_branch2a_param_0(0.817) 
I0925 07:25:51.296799  4458 net.cpp:2312] res5a_branch2b_param_0(0.819) 
I0925 07:25:51.296803  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.16549e+06/2.69117e+06) 0.805
I0925 07:25:51.296813  4458 solver.cpp:562] Iteration 50000, Testing net (#0)
I0925 07:26:01.400831  4458 solver.cpp:654]     Test net output #0: accuracy/top1 = 0.956816
I0925 07:26:01.400851  4458 solver.cpp:654]     Test net output #1: accuracy/top5 = 1
I0925 07:26:01.400858  4458 solver.cpp:654]     Test net output #2: loss = 0.158097 (* 1 = 0.158097 loss)
I0925 07:26:01.400939  4458 solver.cpp:265] [MultiGPU] Tests completed in 10.1038s
I0925 07:26:01.586715  4458 solver.cpp:314] Iteration 50000 (3.52352 iter/s, 28.3807s/100 iter), loss = 0.0607437
I0925 07:26:01.586737  4458 solver.cpp:336]     Train net output #0: loss = 0.0607437 (* 1 = 0.0607437 loss)
I0925 07:26:01.586742  4458 sgd_solver.cpp:136] Iteration 50000, lr = 0.0001, m = 0.9
I0925 07:26:15.339640  4408 data_reader.cpp:305] Starting prefetch of epoch 39
I0925 07:26:19.641330  4458 solver.cpp:314] Iteration 50100 (5.53891 iter/s, 18.0541s/100 iter), loss = 0.074336
I0925 07:26:19.641353  4458 solver.cpp:336]     Train net output #0: loss = 0.074336 (* 1 = 0.074336 loss)
I0925 07:26:19.641357  4458 sgd_solver.cpp:136] Iteration 50100, lr = 0.0001, m = 0.9
I0925 07:26:37.809718  4458 solver.cpp:314] Iteration 50200 (5.50422 iter/s, 18.1679s/100 iter), loss = 0.0718567
I0925 07:26:37.809741  4458 solver.cpp:336]     Train net output #0: loss = 0.0718568 (* 1 = 0.0718568 loss)
I0925 07:26:37.809746  4458 sgd_solver.cpp:136] Iteration 50200, lr = 0.0001, m = 0.9
I0925 07:26:55.976100  4458 solver.cpp:314] Iteration 50300 (5.50483 iter/s, 18.1659s/100 iter), loss = 0.0374871
I0925 07:26:55.976160  4458 solver.cpp:336]     Train net output #0: loss = 0.0374872 (* 1 = 0.0374872 loss)
I0925 07:26:55.976166  4458 sgd_solver.cpp:136] Iteration 50300, lr = 0.0001, m = 0.9
I0925 07:27:14.067806  4458 solver.cpp:314] Iteration 50400 (5.52755 iter/s, 18.0912s/100 iter), loss = 0.0508006
I0925 07:27:14.067829  4458 solver.cpp:336]     Train net output #0: loss = 0.0508007 (* 1 = 0.0508007 loss)
I0925 07:27:14.067836  4458 sgd_solver.cpp:136] Iteration 50400, lr = 0.0001, m = 0.9
I0925 07:27:15.184173  4464 data_reader.cpp:305] Starting prefetch of epoch 33
I0925 07:27:32.239755  4458 solver.cpp:314] Iteration 50500 (5.50314 iter/s, 18.1714s/100 iter), loss = 0.0678629
I0925 07:27:32.240366  4458 solver.cpp:336]     Train net output #0: loss = 0.067863 (* 1 = 0.067863 loss)
I0925 07:27:32.240383  4458 sgd_solver.cpp:136] Iteration 50500, lr = 0.0001, m = 0.9
I0925 07:27:45.254364  4462 data_reader.cpp:305] Starting prefetch of epoch 25
I0925 07:27:50.459656  4458 solver.cpp:314] Iteration 50600 (5.48866 iter/s, 18.2194s/100 iter), loss = 0.0662551
I0925 07:27:50.459676  4458 solver.cpp:336]     Train net output #0: loss = 0.0662552 (* 1 = 0.0662552 loss)
I0925 07:27:50.459679  4458 sgd_solver.cpp:136] Iteration 50600, lr = 0.0001, m = 0.9
I0925 07:28:08.627967  4458 solver.cpp:314] Iteration 50700 (5.50424 iter/s, 18.1678s/100 iter), loss = 0.0424727
I0925 07:28:08.628090  4458 solver.cpp:336]     Train net output #0: loss = 0.0424728 (* 1 = 0.0424728 loss)
I0925 07:28:08.628098  4458 sgd_solver.cpp:136] Iteration 50700, lr = 0.0001, m = 0.9
I0925 07:28:26.744863  4458 solver.cpp:314] Iteration 50800 (5.51987 iter/s, 18.1164s/100 iter), loss = 0.0906337
I0925 07:28:26.744891  4458 solver.cpp:336]     Train net output #0: loss = 0.0906338 (* 1 = 0.0906338 loss)
I0925 07:28:26.744895  4458 sgd_solver.cpp:136] Iteration 50800, lr = 0.0001, m = 0.9
I0925 07:28:44.870029  4458 solver.cpp:314] Iteration 50900 (5.51735 iter/s, 18.1247s/100 iter), loss = 0.0632049
I0925 07:28:44.870080  4458 solver.cpp:336]     Train net output #0: loss = 0.063205 (* 1 = 0.063205 loss)
I0925 07:28:44.870085  4458 sgd_solver.cpp:136] Iteration 50900, lr = 0.0001, m = 0.9
I0925 07:28:45.250643  4466 data_reader.cpp:305] Starting prefetch of epoch 28
I0925 07:29:02.916554  4458 solver.cpp:368] Sparsity after update:
I0925 07:29:02.922688  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 07:29:02.922698  4458 net.cpp:2312] conv1a_param_0(0.255) 
I0925 07:29:02.922705  4458 net.cpp:2312] conv1b_param_0(0.587) 
I0925 07:29:02.922708  4458 net.cpp:2312] ctx_conv1_param_0(0.783) 
I0925 07:29:02.922713  4458 net.cpp:2312] ctx_conv2_param_0(0.795) 
I0925 07:29:02.922716  4458 net.cpp:2312] ctx_conv3_param_0(0.785) 
I0925 07:29:02.922719  4458 net.cpp:2312] ctx_conv4_param_0(0.797) 
I0925 07:29:02.922724  4458 net.cpp:2312] ctx_final_param_0(0.33) 
I0925 07:29:02.922729  4458 net.cpp:2312] out3a_param_0(0.819) 
I0925 07:29:02.922734  4458 net.cpp:2312] out5a_param_0(0.817) 
I0925 07:29:02.922739  4458 net.cpp:2312] res2a_branch2a_param_0(0.729) 
I0925 07:29:02.922742  4458 net.cpp:2312] res2a_branch2b_param_0(0.565) 
I0925 07:29:02.922747  4458 net.cpp:2312] res3a_branch2a_param_0(0.76) 
I0925 07:29:02.922751  4458 net.cpp:2312] res3a_branch2b_param_0(0.693) 
I0925 07:29:02.922755  4458 net.cpp:2312] res4a_branch2a_param_0(0.797) 
I0925 07:29:02.922760  4458 net.cpp:2312] res4a_branch2b_param_0(0.762) 
I0925 07:29:02.922765  4458 net.cpp:2312] res5a_branch2a_param_0(0.817) 
I0925 07:29:02.922768  4458 net.cpp:2312] res5a_branch2b_param_0(0.819) 
I0925 07:29:02.922773  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.16549e+06/2.69117e+06) 0.805
I0925 07:29:03.090229  4458 solver.cpp:314] Iteration 51000 (5.48857 iter/s, 18.2197s/100 iter), loss = 0.0240651
I0925 07:29:03.090250  4458 solver.cpp:336]     Train net output #0: loss = 0.0240652 (* 1 = 0.0240652 loss)
I0925 07:29:03.090255  4458 sgd_solver.cpp:136] Iteration 51000, lr = 0.0001, m = 0.9
I0925 07:29:21.152464  4458 solver.cpp:314] Iteration 51100 (5.53657 iter/s, 18.0617s/100 iter), loss = 0.0767218
I0925 07:29:21.152514  4458 solver.cpp:336]     Train net output #0: loss = 0.0767219 (* 1 = 0.0767219 loss)
I0925 07:29:21.152520  4458 sgd_solver.cpp:136] Iteration 51100, lr = 0.0001, m = 0.9
I0925 07:29:39.243427  4458 solver.cpp:314] Iteration 51200 (5.52778 iter/s, 18.0905s/100 iter), loss = 0.0664285
I0925 07:29:39.243453  4458 solver.cpp:336]     Train net output #0: loss = 0.0664286 (* 1 = 0.0664286 loss)
I0925 07:29:39.243460  4458 sgd_solver.cpp:136] Iteration 51200, lr = 0.0001, m = 0.9
I0925 07:29:45.177115  4461 data_reader.cpp:305] Starting prefetch of epoch 27
I0925 07:29:57.135854  4458 solver.cpp:314] Iteration 51300 (5.58911 iter/s, 17.8919s/100 iter), loss = 0.0732866
I0925 07:29:57.135915  4458 solver.cpp:336]     Train net output #0: loss = 0.0732867 (* 1 = 0.0732867 loss)
I0925 07:29:57.135922  4458 sgd_solver.cpp:136] Iteration 51300, lr = 0.0001, m = 0.9
I0925 07:30:14.795621  4461 data_reader.cpp:305] Starting prefetch of epoch 28
I0925 07:30:15.120194  4458 solver.cpp:314] Iteration 51400 (5.56055 iter/s, 17.9838s/100 iter), loss = 0.0585633
I0925 07:30:15.120216  4458 solver.cpp:336]     Train net output #0: loss = 0.0585634 (* 1 = 0.0585634 loss)
I0925 07:30:15.120220  4458 sgd_solver.cpp:136] Iteration 51400, lr = 0.0001, m = 0.9
I0925 07:30:33.253886  4458 solver.cpp:314] Iteration 51500 (5.51475 iter/s, 18.1332s/100 iter), loss = 0.039584
I0925 07:30:33.253957  4458 solver.cpp:336]     Train net output #0: loss = 0.039584 (* 1 = 0.039584 loss)
I0925 07:30:33.253962  4458 sgd_solver.cpp:136] Iteration 51500, lr = 0.0001, m = 0.9
I0925 07:30:51.225800  4458 solver.cpp:314] Iteration 51600 (5.56439 iter/s, 17.9714s/100 iter), loss = 0.0563044
I0925 07:30:51.225826  4458 solver.cpp:336]     Train net output #0: loss = 0.0563044 (* 1 = 0.0563044 loss)
I0925 07:30:51.225829  4458 sgd_solver.cpp:136] Iteration 51600, lr = 0.0001, m = 0.9
I0925 07:31:09.312832  4458 solver.cpp:314] Iteration 51700 (5.52898 iter/s, 18.0865s/100 iter), loss = 0.0610905
I0925 07:31:09.312892  4458 solver.cpp:336]     Train net output #0: loss = 0.0610906 (* 1 = 0.0610906 loss)
I0925 07:31:09.312899  4458 sgd_solver.cpp:136] Iteration 51700, lr = 0.0001, m = 0.9
I0925 07:31:14.428654  4462 data_reader.cpp:305] Starting prefetch of epoch 26
I0925 07:31:27.318228  4458 solver.cpp:314] Iteration 51800 (5.55405 iter/s, 18.0049s/100 iter), loss = 0.0656659
I0925 07:31:27.318249  4458 solver.cpp:336]     Train net output #0: loss = 0.0656659 (* 1 = 0.0656659 loss)
I0925 07:31:27.318254  4458 sgd_solver.cpp:136] Iteration 51800, lr = 0.0001, m = 0.9
I0925 07:31:45.585207  4458 solver.cpp:314] Iteration 51900 (5.47451 iter/s, 18.2665s/100 iter), loss = 0.0747578
I0925 07:31:45.585261  4458 solver.cpp:336]     Train net output #0: loss = 0.0747578 (* 1 = 0.0747578 loss)
I0925 07:31:45.585268  4458 sgd_solver.cpp:136] Iteration 51900, lr = 0.0001, m = 0.9
I0925 07:32:03.412259  4458 solver.cpp:368] Sparsity after update:
I0925 07:32:03.417227  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 07:32:03.417235  4458 net.cpp:2312] conv1a_param_0(0.255) 
I0925 07:32:03.417242  4458 net.cpp:2312] conv1b_param_0(0.587) 
I0925 07:32:03.417244  4458 net.cpp:2312] ctx_conv1_param_0(0.783) 
I0925 07:32:03.417245  4458 net.cpp:2312] ctx_conv2_param_0(0.795) 
I0925 07:32:03.417248  4458 net.cpp:2312] ctx_conv3_param_0(0.785) 
I0925 07:32:03.417249  4458 net.cpp:2312] ctx_conv4_param_0(0.797) 
I0925 07:32:03.417251  4458 net.cpp:2312] ctx_final_param_0(0.33) 
I0925 07:32:03.417253  4458 net.cpp:2312] out3a_param_0(0.819) 
I0925 07:32:03.417255  4458 net.cpp:2312] out5a_param_0(0.817) 
I0925 07:32:03.417258  4458 net.cpp:2312] res2a_branch2a_param_0(0.729) 
I0925 07:32:03.417259  4458 net.cpp:2312] res2a_branch2b_param_0(0.565) 
I0925 07:32:03.417261  4458 net.cpp:2312] res3a_branch2a_param_0(0.76) 
I0925 07:32:03.417263  4458 net.cpp:2312] res3a_branch2b_param_0(0.693) 
I0925 07:32:03.417265  4458 net.cpp:2312] res4a_branch2a_param_0(0.797) 
I0925 07:32:03.417266  4458 net.cpp:2312] res4a_branch2b_param_0(0.762) 
I0925 07:32:03.417268  4458 net.cpp:2312] res5a_branch2a_param_0(0.817) 
I0925 07:32:03.417270  4458 net.cpp:2312] res5a_branch2b_param_0(0.819) 
I0925 07:32:03.417273  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.16549e+06/2.69117e+06) 0.805
I0925 07:32:03.417281  4458 solver.cpp:562] Iteration 52000, Testing net (#0)
I0925 07:32:06.392998  4520 data_reader.cpp:305] Starting prefetch of epoch 5
I0925 07:32:13.249595  4458 solver.cpp:654]     Test net output #0: accuracy/top1 = 0.955533
I0925 07:32:13.249619  4458 solver.cpp:654]     Test net output #1: accuracy/top5 = 1
I0925 07:32:13.249624  4458 solver.cpp:654]     Test net output #2: loss = 0.132238 (* 1 = 0.132238 loss)
I0925 07:32:13.249713  4458 solver.cpp:265] [MultiGPU] Tests completed in 9.83215s
I0925 07:32:13.452831  4458 solver.cpp:314] Iteration 52000 (3.58849 iter/s, 27.8668s/100 iter), loss = 0.0546152
I0925 07:32:13.452858  4458 solver.cpp:336]     Train net output #0: loss = 0.0546152 (* 1 = 0.0546152 loss)
I0925 07:32:13.452863  4458 sgd_solver.cpp:136] Iteration 52000, lr = 0.0001, m = 0.9
I0925 07:32:24.019681  4464 data_reader.cpp:305] Starting prefetch of epoch 34
I0925 07:32:31.351155  4458 solver.cpp:314] Iteration 52100 (5.58727 iter/s, 17.8978s/100 iter), loss = 0.0317594
I0925 07:32:31.351178  4458 solver.cpp:336]     Train net output #0: loss = 0.0317595 (* 1 = 0.0317595 loss)
I0925 07:32:31.351182  4458 sgd_solver.cpp:136] Iteration 52100, lr = 0.0001, m = 0.9
I0925 07:32:49.432612  4458 solver.cpp:314] Iteration 52200 (5.53068 iter/s, 18.0809s/100 iter), loss = 0.0607287
I0925 07:32:49.432634  4458 solver.cpp:336]     Train net output #0: loss = 0.0607288 (* 1 = 0.0607288 loss)
I0925 07:32:49.432638  4458 sgd_solver.cpp:136] Iteration 52200, lr = 0.0001, m = 0.9
I0925 07:32:53.796330  4461 data_reader.cpp:305] Starting prefetch of epoch 29
I0925 07:33:07.458819  4458 solver.cpp:314] Iteration 52300 (5.54764 iter/s, 18.0257s/100 iter), loss = 0.079276
I0925 07:33:07.458925  4458 solver.cpp:336]     Train net output #0: loss = 0.0792761 (* 1 = 0.0792761 loss)
I0925 07:33:07.458931  4458 sgd_solver.cpp:136] Iteration 52300, lr = 0.0001, m = 0.9
I0925 07:33:25.386148  4458 solver.cpp:314] Iteration 52400 (5.57823 iter/s, 17.9268s/100 iter), loss = 0.0498588
I0925 07:33:25.386173  4458 solver.cpp:336]     Train net output #0: loss = 0.0498589 (* 1 = 0.0498589 loss)
I0925 07:33:25.386176  4458 sgd_solver.cpp:136] Iteration 52400, lr = 0.0001, m = 0.9
I0925 07:33:43.318110  4458 solver.cpp:314] Iteration 52500 (5.57679 iter/s, 17.9315s/100 iter), loss = 0.0735461
I0925 07:33:43.318162  4458 solver.cpp:336]     Train net output #0: loss = 0.0735461 (* 1 = 0.0735461 loss)
I0925 07:33:43.318168  4458 sgd_solver.cpp:136] Iteration 52500, lr = 0.0001, m = 0.9
I0925 07:33:53.233359  4461 data_reader.cpp:305] Starting prefetch of epoch 30
I0925 07:34:01.284544  4458 solver.cpp:314] Iteration 52600 (5.56609 iter/s, 17.9659s/100 iter), loss = 0.075351
I0925 07:34:01.284569  4458 solver.cpp:336]     Train net output #0: loss = 0.0753511 (* 1 = 0.0753511 loss)
I0925 07:34:01.284574  4458 sgd_solver.cpp:136] Iteration 52600, lr = 0.0001, m = 0.9
I0925 07:34:19.220064  4458 solver.cpp:314] Iteration 52700 (5.57569 iter/s, 17.935s/100 iter), loss = 0.0972797
I0925 07:34:19.220170  4458 solver.cpp:336]     Train net output #0: loss = 0.0972797 (* 1 = 0.0972797 loss)
I0925 07:34:19.220181  4458 sgd_solver.cpp:136] Iteration 52700, lr = 0.0001, m = 0.9
I0925 07:34:37.224681  4458 solver.cpp:314] Iteration 52800 (5.55429 iter/s, 18.0041s/100 iter), loss = 0.0779009
I0925 07:34:37.224705  4458 solver.cpp:336]     Train net output #0: loss = 0.0779011 (* 1 = 0.0779011 loss)
I0925 07:34:37.224709  4458 sgd_solver.cpp:136] Iteration 52800, lr = 0.0001, m = 0.9
I0925 07:34:52.615361  4464 data_reader.cpp:305] Starting prefetch of epoch 35
I0925 07:34:55.345635  4458 solver.cpp:314] Iteration 52900 (5.51863 iter/s, 18.1204s/100 iter), loss = 0.0529203
I0925 07:34:55.345659  4458 solver.cpp:336]     Train net output #0: loss = 0.0529204 (* 1 = 0.0529204 loss)
I0925 07:34:55.345664  4458 sgd_solver.cpp:136] Iteration 52900, lr = 0.0001, m = 0.9
I0925 07:35:13.253186  4458 solver.cpp:368] Sparsity after update:
I0925 07:35:13.264245  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 07:35:13.264281  4458 net.cpp:2312] conv1a_param_0(0.255) 
I0925 07:35:13.264295  4458 net.cpp:2312] conv1b_param_0(0.587) 
I0925 07:35:13.264298  4458 net.cpp:2312] ctx_conv1_param_0(0.783) 
I0925 07:35:13.264302  4458 net.cpp:2312] ctx_conv2_param_0(0.795) 
I0925 07:35:13.264307  4458 net.cpp:2312] ctx_conv3_param_0(0.785) 
I0925 07:35:13.264309  4458 net.cpp:2312] ctx_conv4_param_0(0.797) 
I0925 07:35:13.264312  4458 net.cpp:2312] ctx_final_param_0(0.33) 
I0925 07:35:13.264315  4458 net.cpp:2312] out3a_param_0(0.819) 
I0925 07:35:13.264319  4458 net.cpp:2312] out5a_param_0(0.817) 
I0925 07:35:13.264322  4458 net.cpp:2312] res2a_branch2a_param_0(0.729) 
I0925 07:35:13.264325  4458 net.cpp:2312] res2a_branch2b_param_0(0.565) 
I0925 07:35:13.264328  4458 net.cpp:2312] res3a_branch2a_param_0(0.76) 
I0925 07:35:13.264331  4458 net.cpp:2312] res3a_branch2b_param_0(0.693) 
I0925 07:35:13.264334  4458 net.cpp:2312] res4a_branch2a_param_0(0.797) 
I0925 07:35:13.264338  4458 net.cpp:2312] res4a_branch2b_param_0(0.762) 
I0925 07:35:13.264341  4458 net.cpp:2312] res5a_branch2a_param_0(0.817) 
I0925 07:35:13.264344  4458 net.cpp:2312] res5a_branch2b_param_0(0.819) 
I0925 07:35:13.264348  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.16549e+06/2.69117e+06) 0.805
I0925 07:35:13.431663  4458 solver.cpp:314] Iteration 53000 (5.52929 iter/s, 18.0855s/100 iter), loss = 0.0452928
I0925 07:35:13.431685  4458 solver.cpp:336]     Train net output #0: loss = 0.0452929 (* 1 = 0.0452929 loss)
I0925 07:35:13.431690  4458 sgd_solver.cpp:136] Iteration 53000, lr = 0.0001, m = 0.9
I0925 07:35:22.737287  4408 data_reader.cpp:305] Starting prefetch of epoch 40
I0925 07:35:31.536154  4458 solver.cpp:314] Iteration 53100 (5.52365 iter/s, 18.104s/100 iter), loss = 0.0528991
I0925 07:35:31.536182  4458 solver.cpp:336]     Train net output #0: loss = 0.0528992 (* 1 = 0.0528992 loss)
I0925 07:35:31.536186  4458 sgd_solver.cpp:136] Iteration 53100, lr = 0.0001, m = 0.9
I0925 07:35:49.478374  4458 solver.cpp:314] Iteration 53200 (5.57361 iter/s, 17.9417s/100 iter), loss = 0.0523743
I0925 07:35:49.478404  4458 solver.cpp:336]     Train net output #0: loss = 0.0523744 (* 1 = 0.0523744 loss)
I0925 07:35:49.478410  4458 sgd_solver.cpp:136] Iteration 53200, lr = 0.0001, m = 0.9
I0925 07:36:07.620833  4458 solver.cpp:314] Iteration 53300 (5.51209 iter/s, 18.1419s/100 iter), loss = 0.0599865
I0925 07:36:07.620923  4458 solver.cpp:336]     Train net output #0: loss = 0.0599866 (* 1 = 0.0599866 loss)
I0925 07:36:07.620930  4458 sgd_solver.cpp:136] Iteration 53300, lr = 0.0001, m = 0.9
I0925 07:36:22.207062  4462 data_reader.cpp:305] Starting prefetch of epoch 27
I0925 07:36:25.575973  4458 solver.cpp:314] Iteration 53400 (5.56959 iter/s, 17.9546s/100 iter), loss = 0.0617363
I0925 07:36:25.575996  4458 solver.cpp:336]     Train net output #0: loss = 0.0617364 (* 1 = 0.0617364 loss)
I0925 07:36:25.576000  4458 sgd_solver.cpp:136] Iteration 53400, lr = 0.0001, m = 0.9
I0925 07:36:43.568022  4458 solver.cpp:314] Iteration 53500 (5.55817 iter/s, 17.9915s/100 iter), loss = 0.0489352
I0925 07:36:43.568127  4458 solver.cpp:336]     Train net output #0: loss = 0.0489353 (* 1 = 0.0489353 loss)
I0925 07:36:43.568136  4458 sgd_solver.cpp:136] Iteration 53500, lr = 0.0001, m = 0.9
I0925 07:37:01.587236  4458 solver.cpp:314] Iteration 53600 (5.54979 iter/s, 18.0187s/100 iter), loss = 0.0760051
I0925 07:37:01.587261  4458 solver.cpp:336]     Train net output #0: loss = 0.0760052 (* 1 = 0.0760052 loss)
I0925 07:37:01.587265  4458 sgd_solver.cpp:136] Iteration 53600, lr = 0.0001, m = 0.9
I0925 07:37:19.702227  4458 solver.cpp:314] Iteration 53700 (5.52045 iter/s, 18.1145s/100 iter), loss = 0.0579503
I0925 07:37:19.702278  4458 solver.cpp:336]     Train net output #0: loss = 0.0579504 (* 1 = 0.0579504 loss)
I0925 07:37:19.702283  4458 sgd_solver.cpp:136] Iteration 53700, lr = 0.0001, m = 0.9
I0925 07:37:21.841310  4410 data_reader.cpp:305] Starting prefetch of epoch 31
I0925 07:37:37.625756  4458 solver.cpp:314] Iteration 53800 (5.57942 iter/s, 17.923s/100 iter), loss = 0.0527744
I0925 07:37:37.625780  4458 solver.cpp:336]     Train net output #0: loss = 0.0527745 (* 1 = 0.0527745 loss)
I0925 07:37:37.625785  4458 sgd_solver.cpp:136] Iteration 53800, lr = 0.0001, m = 0.9
I0925 07:37:51.534824  4462 data_reader.cpp:305] Starting prefetch of epoch 28
I0925 07:37:55.640653  4458 solver.cpp:314] Iteration 53900 (5.55112 iter/s, 18.0144s/100 iter), loss = 0.0523203
I0925 07:37:55.640681  4458 solver.cpp:336]     Train net output #0: loss = 0.0523204 (* 1 = 0.0523204 loss)
I0925 07:37:55.640684  4458 sgd_solver.cpp:136] Iteration 53900, lr = 0.0001, m = 0.9
I0925 07:38:13.610987  4458 solver.cpp:368] Sparsity after update:
I0925 07:38:13.616281  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 07:38:13.616291  4458 net.cpp:2312] conv1a_param_0(0.255) 
I0925 07:38:13.616297  4458 net.cpp:2312] conv1b_param_0(0.587) 
I0925 07:38:13.616298  4458 net.cpp:2312] ctx_conv1_param_0(0.783) 
I0925 07:38:13.616300  4458 net.cpp:2312] ctx_conv2_param_0(0.795) 
I0925 07:38:13.616302  4458 net.cpp:2312] ctx_conv3_param_0(0.785) 
I0925 07:38:13.616304  4458 net.cpp:2312] ctx_conv4_param_0(0.797) 
I0925 07:38:13.616307  4458 net.cpp:2312] ctx_final_param_0(0.33) 
I0925 07:38:13.616308  4458 net.cpp:2312] out3a_param_0(0.819) 
I0925 07:38:13.616309  4458 net.cpp:2312] out5a_param_0(0.817) 
I0925 07:38:13.616312  4458 net.cpp:2312] res2a_branch2a_param_0(0.729) 
I0925 07:38:13.616313  4458 net.cpp:2312] res2a_branch2b_param_0(0.565) 
I0925 07:38:13.616315  4458 net.cpp:2312] res3a_branch2a_param_0(0.76) 
I0925 07:38:13.616317  4458 net.cpp:2312] res3a_branch2b_param_0(0.693) 
I0925 07:38:13.616319  4458 net.cpp:2312] res4a_branch2a_param_0(0.797) 
I0925 07:38:13.616322  4458 net.cpp:2312] res4a_branch2b_param_0(0.762) 
I0925 07:38:13.616323  4458 net.cpp:2312] res5a_branch2a_param_0(0.817) 
I0925 07:38:13.616324  4458 net.cpp:2312] res5a_branch2b_param_0(0.819) 
I0925 07:38:13.616327  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.16549e+06/2.69117e+06) 0.805
I0925 07:38:13.616335  4458 solver.cpp:562] Iteration 54000, Testing net (#0)
I0925 07:38:23.612140  4458 solver.cpp:654]     Test net output #0: accuracy/top1 = 0.956814
I0925 07:38:23.612326  4458 solver.cpp:654]     Test net output #1: accuracy/top5 = 1
I0925 07:38:23.612336  4458 solver.cpp:654]     Test net output #2: loss = 0.158281 (* 1 = 0.158281 loss)
I0925 07:38:23.612356  4458 solver.cpp:265] [MultiGPU] Tests completed in 9.99573s
I0925 07:38:23.796370  4458 solver.cpp:314] Iteration 54000 (3.55178 iter/s, 28.1549s/100 iter), loss = 0.060859
I0925 07:38:23.796393  4458 solver.cpp:336]     Train net output #0: loss = 0.0608591 (* 1 = 0.0608591 loss)
I0925 07:38:23.796397  4458 sgd_solver.cpp:136] Iteration 54000, lr = 0.0001, m = 0.9
I0925 07:38:31.387207  4461 data_reader.cpp:305] Starting prefetch of epoch 31
I0925 07:38:41.704576  4458 solver.cpp:314] Iteration 54100 (5.58419 iter/s, 17.9077s/100 iter), loss = 0.056907
I0925 07:38:41.704599  4458 solver.cpp:336]     Train net output #0: loss = 0.0569071 (* 1 = 0.0569071 loss)
I0925 07:38:41.704604  4458 sgd_solver.cpp:136] Iteration 54100, lr = 0.0001, m = 0.9
I0925 07:38:59.983053  4458 solver.cpp:314] Iteration 54200 (5.47107 iter/s, 18.278s/100 iter), loss = 0.0693044
I0925 07:38:59.983106  4458 solver.cpp:336]     Train net output #0: loss = 0.0693045 (* 1 = 0.0693045 loss)
I0925 07:38:59.983111  4458 sgd_solver.cpp:136] Iteration 54200, lr = 0.0001, m = 0.9
I0925 07:39:17.932001  4458 solver.cpp:314] Iteration 54300 (5.57152 iter/s, 17.9484s/100 iter), loss = 0.0886629
I0925 07:39:17.932026  4458 solver.cpp:336]     Train net output #0: loss = 0.088663 (* 1 = 0.088663 loss)
I0925 07:39:17.932032  4458 sgd_solver.cpp:136] Iteration 54300, lr = 0.0001, m = 0.9
I0925 07:39:31.079934  4466 data_reader.cpp:305] Starting prefetch of epoch 29
I0925 07:39:35.885010  4458 solver.cpp:314] Iteration 54400 (5.57026 iter/s, 17.9525s/100 iter), loss = 0.0393115
I0925 07:39:35.885031  4458 solver.cpp:336]     Train net output #0: loss = 0.0393116 (* 1 = 0.0393116 loss)
I0925 07:39:35.885037  4458 sgd_solver.cpp:136] Iteration 54400, lr = 0.0001, m = 0.9
I0925 07:39:53.854533  4458 solver.cpp:314] Iteration 54500 (5.56514 iter/s, 17.969s/100 iter), loss = 0.11893
I0925 07:39:53.854558  4458 solver.cpp:336]     Train net output #0: loss = 0.11893 (* 1 = 0.11893 loss)
I0925 07:39:53.854562  4458 sgd_solver.cpp:136] Iteration 54500, lr = 0.0001, m = 0.9
I0925 07:40:12.154057  4458 solver.cpp:314] Iteration 54600 (5.46478 iter/s, 18.299s/100 iter), loss = 0.0578891
I0925 07:40:12.154157  4458 solver.cpp:336]     Train net output #0: loss = 0.0578892 (* 1 = 0.0578892 loss)
I0925 07:40:12.154163  4458 sgd_solver.cpp:136] Iteration 54600, lr = 0.0001, m = 0.9
I0925 07:40:30.148869  4458 solver.cpp:314] Iteration 54700 (5.55732 iter/s, 17.9943s/100 iter), loss = 0.0675053
I0925 07:40:30.148891  4458 solver.cpp:336]     Train net output #0: loss = 0.0675054 (* 1 = 0.0675054 loss)
I0925 07:40:30.148896  4458 sgd_solver.cpp:136] Iteration 54700, lr = 0.0001, m = 0.9
I0925 07:40:30.699242  4466 data_reader.cpp:305] Starting prefetch of epoch 30
I0925 07:40:48.256481  4458 solver.cpp:314] Iteration 54800 (5.5227 iter/s, 18.1071s/100 iter), loss = 0.0553339
I0925 07:40:48.256547  4458 solver.cpp:336]     Train net output #0: loss = 0.055334 (* 1 = 0.055334 loss)
I0925 07:40:48.256553  4458 sgd_solver.cpp:136] Iteration 54800, lr = 0.0001, m = 0.9
I0925 07:41:00.720780  4461 data_reader.cpp:305] Starting prefetch of epoch 32
I0925 07:41:06.293886  4458 solver.cpp:314] Iteration 54900 (5.54419 iter/s, 18.0369s/100 iter), loss = 0.0626906
I0925 07:41:06.293907  4458 solver.cpp:336]     Train net output #0: loss = 0.0626907 (* 1 = 0.0626907 loss)
I0925 07:41:06.293913  4458 sgd_solver.cpp:136] Iteration 54900, lr = 0.0001, m = 0.9
I0925 07:41:24.003890  4458 solver.cpp:368] Sparsity after update:
I0925 07:41:24.022526  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 07:41:24.022541  4458 net.cpp:2312] conv1a_param_0(0.255) 
I0925 07:41:24.022548  4458 net.cpp:2312] conv1b_param_0(0.587) 
I0925 07:41:24.022550  4458 net.cpp:2312] ctx_conv1_param_0(0.783) 
I0925 07:41:24.022552  4458 net.cpp:2312] ctx_conv2_param_0(0.795) 
I0925 07:41:24.022554  4458 net.cpp:2312] ctx_conv3_param_0(0.785) 
I0925 07:41:24.022557  4458 net.cpp:2312] ctx_conv4_param_0(0.797) 
I0925 07:41:24.022558  4458 net.cpp:2312] ctx_final_param_0(0.33) 
I0925 07:41:24.022560  4458 net.cpp:2312] out3a_param_0(0.819) 
I0925 07:41:24.022562  4458 net.cpp:2312] out5a_param_0(0.817) 
I0925 07:41:24.022563  4458 net.cpp:2312] res2a_branch2a_param_0(0.729) 
I0925 07:41:24.022565  4458 net.cpp:2312] res2a_branch2b_param_0(0.565) 
I0925 07:41:24.022567  4458 net.cpp:2312] res3a_branch2a_param_0(0.76) 
I0925 07:41:24.022569  4458 net.cpp:2312] res3a_branch2b_param_0(0.693) 
I0925 07:41:24.022572  4458 net.cpp:2312] res4a_branch2a_param_0(0.797) 
I0925 07:41:24.022573  4458 net.cpp:2312] res4a_branch2b_param_0(0.762) 
I0925 07:41:24.022574  4458 net.cpp:2312] res5a_branch2a_param_0(0.817) 
I0925 07:41:24.022577  4458 net.cpp:2312] res5a_branch2b_param_0(0.819) 
I0925 07:41:24.022578  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.16549e+06/2.69117e+06) 0.805
I0925 07:41:24.193243  4458 solver.cpp:314] Iteration 55000 (5.58695 iter/s, 17.8988s/100 iter), loss = 0.111735
I0925 07:41:24.193266  4458 solver.cpp:336]     Train net output #0: loss = 0.111735 (* 1 = 0.111735 loss)
I0925 07:41:24.193270  4458 sgd_solver.cpp:136] Iteration 55000, lr = 0.0001, m = 0.9
I0925 07:41:42.196977  4458 solver.cpp:314] Iteration 55100 (5.55456 iter/s, 18.0032s/100 iter), loss = 0.0982417
I0925 07:41:42.197000  4458 solver.cpp:336]     Train net output #0: loss = 0.0982418 (* 1 = 0.0982418 loss)
I0925 07:41:42.197003  4458 sgd_solver.cpp:136] Iteration 55100, lr = 0.0001, m = 0.9
I0925 07:42:00.203124  4461 data_reader.cpp:305] Starting prefetch of epoch 33
I0925 07:42:00.353170  4458 solver.cpp:314] Iteration 55200 (5.50792 iter/s, 18.1557s/100 iter), loss = 0.0450216
I0925 07:42:00.353194  4458 solver.cpp:336]     Train net output #0: loss = 0.0450217 (* 1 = 0.0450217 loss)
I0925 07:42:00.353200  4458 sgd_solver.cpp:136] Iteration 55200, lr = 0.0001, m = 0.9
I0925 07:42:18.521097  4458 solver.cpp:314] Iteration 55300 (5.50436 iter/s, 18.1674s/100 iter), loss = 0.0652558
I0925 07:42:18.521121  4458 solver.cpp:336]     Train net output #0: loss = 0.0652559 (* 1 = 0.0652559 loss)
I0925 07:42:18.521126  4458 sgd_solver.cpp:136] Iteration 55300, lr = 0.0001, m = 0.9
I0925 07:42:36.625113  4458 solver.cpp:314] Iteration 55400 (5.52379 iter/s, 18.1035s/100 iter), loss = 0.0437055
I0925 07:42:36.625190  4458 solver.cpp:336]     Train net output #0: loss = 0.0437056 (* 1 = 0.0437056 loss)
I0925 07:42:36.625195  4458 sgd_solver.cpp:136] Iteration 55400, lr = 0.0001, m = 0.9
I0925 07:42:54.653203  4458 solver.cpp:314] Iteration 55500 (5.54706 iter/s, 18.0276s/100 iter), loss = 0.0541462
I0925 07:42:54.653228  4458 solver.cpp:336]     Train net output #0: loss = 0.0541463 (* 1 = 0.0541463 loss)
I0925 07:42:54.653231  4458 sgd_solver.cpp:136] Iteration 55500, lr = 0.0001, m = 0.9
I0925 07:43:00.147271  4464 data_reader.cpp:305] Starting prefetch of epoch 36
I0925 07:43:12.845214  4458 solver.cpp:314] Iteration 55600 (5.49707 iter/s, 18.1915s/100 iter), loss = 0.0650167
I0925 07:43:12.845284  4458 solver.cpp:336]     Train net output #0: loss = 0.0650168 (* 1 = 0.0650168 loss)
I0925 07:43:12.845290  4458 sgd_solver.cpp:136] Iteration 55600, lr = 0.0001, m = 0.9
I0925 07:43:30.074616  4461 data_reader.cpp:305] Starting prefetch of epoch 34
I0925 07:43:30.959844  4458 solver.cpp:314] Iteration 55700 (5.52055 iter/s, 18.1141s/100 iter), loss = 0.0336184
I0925 07:43:30.959870  4458 solver.cpp:336]     Train net output #0: loss = 0.0336185 (* 1 = 0.0336185 loss)
I0925 07:43:30.959874  4458 sgd_solver.cpp:136] Iteration 55700, lr = 0.0001, m = 0.9
I0925 07:43:48.902361  4458 solver.cpp:314] Iteration 55800 (5.57351 iter/s, 17.942s/100 iter), loss = 0.0529004
I0925 07:43:48.902417  4458 solver.cpp:336]     Train net output #0: loss = 0.0529005 (* 1 = 0.0529005 loss)
I0925 07:43:48.902422  4458 sgd_solver.cpp:136] Iteration 55800, lr = 0.0001, m = 0.9
I0925 07:44:06.951086  4458 solver.cpp:314] Iteration 55900 (5.54071 iter/s, 18.0482s/100 iter), loss = 0.0988792
I0925 07:44:06.951110  4458 solver.cpp:336]     Train net output #0: loss = 0.0988793 (* 1 = 0.0988793 loss)
I0925 07:44:06.951114  4458 sgd_solver.cpp:136] Iteration 55900, lr = 0.0001, m = 0.9
I0925 07:44:24.764323  4458 solver.cpp:368] Sparsity after update:
I0925 07:44:24.772959  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 07:44:24.772977  4458 net.cpp:2312] conv1a_param_0(0.255) 
I0925 07:44:24.772984  4458 net.cpp:2312] conv1b_param_0(0.587) 
I0925 07:44:24.772985  4458 net.cpp:2312] ctx_conv1_param_0(0.783) 
I0925 07:44:24.772987  4458 net.cpp:2312] ctx_conv2_param_0(0.795) 
I0925 07:44:24.772989  4458 net.cpp:2312] ctx_conv3_param_0(0.785) 
I0925 07:44:24.772991  4458 net.cpp:2312] ctx_conv4_param_0(0.797) 
I0925 07:44:24.772992  4458 net.cpp:2312] ctx_final_param_0(0.33) 
I0925 07:44:24.772994  4458 net.cpp:2312] out3a_param_0(0.819) 
I0925 07:44:24.772996  4458 net.cpp:2312] out5a_param_0(0.817) 
I0925 07:44:24.772999  4458 net.cpp:2312] res2a_branch2a_param_0(0.729) 
I0925 07:44:24.773000  4458 net.cpp:2312] res2a_branch2b_param_0(0.565) 
I0925 07:44:24.773002  4458 net.cpp:2312] res3a_branch2a_param_0(0.76) 
I0925 07:44:24.773005  4458 net.cpp:2312] res3a_branch2b_param_0(0.693) 
I0925 07:44:24.773006  4458 net.cpp:2312] res4a_branch2a_param_0(0.797) 
I0925 07:44:24.773008  4458 net.cpp:2312] res4a_branch2b_param_0(0.762) 
I0925 07:44:24.773010  4458 net.cpp:2312] res5a_branch2a_param_0(0.817) 
I0925 07:44:24.773020  4458 net.cpp:2312] res5a_branch2b_param_0(0.819) 
I0925 07:44:24.773022  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.16549e+06/2.69117e+06) 0.805
I0925 07:44:24.773031  4458 solver.cpp:562] Iteration 56000, Testing net (#0)
I0925 07:44:27.754750  4520 data_reader.cpp:305] Starting prefetch of epoch 6
I0925 07:44:34.763545  4458 solver.cpp:654]     Test net output #0: accuracy/top1 = 0.955398
I0925 07:44:34.763569  4458 solver.cpp:654]     Test net output #1: accuracy/top5 = 1
I0925 07:44:34.763576  4458 solver.cpp:654]     Test net output #2: loss = 0.132642 (* 1 = 0.132642 loss)
I0925 07:44:34.763643  4458 solver.cpp:265] [MultiGPU] Tests completed in 9.99033s
I0925 07:44:34.962327  4458 solver.cpp:314] Iteration 56000 (3.5701 iter/s, 28.0105s/100 iter), loss = 0.0669497
I0925 07:44:34.962353  4458 solver.cpp:336]     Train net output #0: loss = 0.0669498 (* 1 = 0.0669498 loss)
I0925 07:44:34.962357  4458 sgd_solver.cpp:136] Iteration 56000, lr = 0.0001, m = 0.9
I0925 07:44:39.601488  4410 data_reader.cpp:305] Starting prefetch of epoch 32
I0925 07:44:52.801542  4458 solver.cpp:314] Iteration 56100 (5.60579 iter/s, 17.8387s/100 iter), loss = 0.073535
I0925 07:44:52.801563  4458 solver.cpp:336]     Train net output #0: loss = 0.0735351 (* 1 = 0.0735351 loss)
I0925 07:44:52.801568  4458 sgd_solver.cpp:136] Iteration 56100, lr = 0.0001, m = 0.9
I0925 07:45:09.302409  4462 data_reader.cpp:305] Starting prefetch of epoch 29
I0925 07:45:10.916177  4458 solver.cpp:314] Iteration 56200 (5.52055 iter/s, 18.1141s/100 iter), loss = 0.0535934
I0925 07:45:10.916201  4458 solver.cpp:336]     Train net output #0: loss = 0.0535935 (* 1 = 0.0535935 loss)
I0925 07:45:10.916205  4458 sgd_solver.cpp:136] Iteration 56200, lr = 0.0001, m = 0.9
I0925 07:45:29.028899  4458 solver.cpp:314] Iteration 56300 (5.52114 iter/s, 18.1122s/100 iter), loss = 0.0579978
I0925 07:45:29.028920  4458 solver.cpp:336]     Train net output #0: loss = 0.0579978 (* 1 = 0.0579978 loss)
I0925 07:45:29.028925  4458 sgd_solver.cpp:136] Iteration 56300, lr = 0.0001, m = 0.9
I0925 07:45:47.109649  4458 solver.cpp:314] Iteration 56400 (5.5309 iter/s, 18.0802s/100 iter), loss = 0.0717626
I0925 07:45:47.109697  4458 solver.cpp:336]     Train net output #0: loss = 0.0717627 (* 1 = 0.0717627 loss)
I0925 07:45:47.109702  4458 sgd_solver.cpp:136] Iteration 56400, lr = 0.0001, m = 0.9
I0925 07:46:05.017712  4458 solver.cpp:314] Iteration 56500 (5.58423 iter/s, 17.9076s/100 iter), loss = 0.0692334
I0925 07:46:05.017736  4458 solver.cpp:336]     Train net output #0: loss = 0.0692335 (* 1 = 0.0692335 loss)
I0925 07:46:05.017741  4458 sgd_solver.cpp:136] Iteration 56500, lr = 0.0001, m = 0.9
I0925 07:46:08.764644  4410 data_reader.cpp:305] Starting prefetch of epoch 33
I0925 07:46:22.988644  4458 solver.cpp:314] Iteration 56600 (5.5647 iter/s, 17.9704s/100 iter), loss = 0.0589802
I0925 07:46:22.988704  4458 solver.cpp:336]     Train net output #0: loss = 0.0589803 (* 1 = 0.0589803 loss)
I0925 07:46:22.988710  4458 sgd_solver.cpp:136] Iteration 56600, lr = 0.0001, m = 0.9
I0925 07:46:41.063822  4458 solver.cpp:314] Iteration 56700 (5.5326 iter/s, 18.0747s/100 iter), loss = 0.0769279
I0925 07:46:41.063843  4458 solver.cpp:336]     Train net output #0: loss = 0.076928 (* 1 = 0.076928 loss)
I0925 07:46:41.063848  4458 sgd_solver.cpp:136] Iteration 56700, lr = 0.0001, m = 0.9
I0925 07:46:59.270525  4458 solver.cpp:314] Iteration 56800 (5.49264 iter/s, 18.2062s/100 iter), loss = 0.0696381
I0925 07:46:59.270583  4458 solver.cpp:336]     Train net output #0: loss = 0.0696382 (* 1 = 0.0696382 loss)
I0925 07:46:59.270589  4458 sgd_solver.cpp:136] Iteration 56800, lr = 0.0001, m = 0.9
I0925 07:47:08.724313  4464 data_reader.cpp:305] Starting prefetch of epoch 37
I0925 07:47:17.486850  4458 solver.cpp:314] Iteration 56900 (5.48973 iter/s, 18.2158s/100 iter), loss = 0.0494971
I0925 07:47:17.486876  4458 solver.cpp:336]     Train net output #0: loss = 0.0494971 (* 1 = 0.0494971 loss)
I0925 07:47:17.486879  4458 sgd_solver.cpp:136] Iteration 56900, lr = 0.0001, m = 0.9
I0925 07:47:35.228315  4458 solver.cpp:368] Sparsity after update:
I0925 07:47:35.248018  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 07:47:35.248036  4458 net.cpp:2312] conv1a_param_0(0.255) 
I0925 07:47:35.248044  4458 net.cpp:2312] conv1b_param_0(0.587) 
I0925 07:47:35.248049  4458 net.cpp:2312] ctx_conv1_param_0(0.783) 
I0925 07:47:35.248051  4458 net.cpp:2312] ctx_conv2_param_0(0.795) 
I0925 07:47:35.248054  4458 net.cpp:2312] ctx_conv3_param_0(0.785) 
I0925 07:47:35.248057  4458 net.cpp:2312] ctx_conv4_param_0(0.797) 
I0925 07:47:35.248060  4458 net.cpp:2312] ctx_final_param_0(0.33) 
I0925 07:47:35.248064  4458 net.cpp:2312] out3a_param_0(0.819) 
I0925 07:47:35.248066  4458 net.cpp:2312] out5a_param_0(0.817) 
I0925 07:47:35.248070  4458 net.cpp:2312] res2a_branch2a_param_0(0.729) 
I0925 07:47:35.248073  4458 net.cpp:2312] res2a_branch2b_param_0(0.565) 
I0925 07:47:35.248076  4458 net.cpp:2312] res3a_branch2a_param_0(0.76) 
I0925 07:47:35.248080  4458 net.cpp:2312] res3a_branch2b_param_0(0.693) 
I0925 07:47:35.248082  4458 net.cpp:2312] res4a_branch2a_param_0(0.797) 
I0925 07:47:35.248085  4458 net.cpp:2312] res4a_branch2b_param_0(0.762) 
I0925 07:47:35.248090  4458 net.cpp:2312] res5a_branch2a_param_0(0.817) 
I0925 07:47:35.248093  4458 net.cpp:2312] res5a_branch2b_param_0(0.819) 
I0925 07:47:35.248096  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.16549e+06/2.69117e+06) 0.805
I0925 07:47:35.418947  4458 solver.cpp:314] Iteration 57000 (5.57675 iter/s, 17.9316s/100 iter), loss = 0.0764061
I0925 07:47:35.418973  4458 solver.cpp:336]     Train net output #0: loss = 0.0764062 (* 1 = 0.0764062 loss)
I0925 07:47:35.418980  4458 sgd_solver.cpp:136] Iteration 57000, lr = 0.0001, m = 0.9
I0925 07:47:38.470458  4461 data_reader.cpp:305] Starting prefetch of epoch 35
I0925 07:47:53.306582  4458 solver.cpp:314] Iteration 57100 (5.59061 iter/s, 17.8871s/100 iter), loss = 0.0869472
I0925 07:47:53.306608  4458 solver.cpp:336]     Train net output #0: loss = 0.0869473 (* 1 = 0.0869473 loss)
I0925 07:47:53.306612  4458 sgd_solver.cpp:136] Iteration 57100, lr = 0.0001, m = 0.9
I0925 07:48:11.385952  4458 solver.cpp:314] Iteration 57200 (5.53132 iter/s, 18.0789s/100 iter), loss = 0.0496294
I0925 07:48:11.386049  4458 solver.cpp:336]     Train net output #0: loss = 0.0496295 (* 1 = 0.0496295 loss)
I0925 07:48:11.386055  4458 sgd_solver.cpp:136] Iteration 57200, lr = 0.0001, m = 0.9
I0925 07:48:29.355237  4458 solver.cpp:314] Iteration 57300 (5.56521 iter/s, 17.9688s/100 iter), loss = 0.0860465
I0925 07:48:29.355259  4458 solver.cpp:336]     Train net output #0: loss = 0.0860466 (* 1 = 0.0860466 loss)
I0925 07:48:29.355263  4458 sgd_solver.cpp:136] Iteration 57300, lr = 0.0001, m = 0.9
I0925 07:48:38.004547  4461 data_reader.cpp:305] Starting prefetch of epoch 36
I0925 07:48:47.394686  4458 solver.cpp:314] Iteration 57400 (5.54356 iter/s, 18.0389s/100 iter), loss = 0.052383
I0925 07:48:47.394760  4458 solver.cpp:336]     Train net output #0: loss = 0.0523831 (* 1 = 0.0523831 loss)
I0925 07:48:47.394765  4458 sgd_solver.cpp:136] Iteration 57400, lr = 0.0001, m = 0.9
I0925 07:49:05.531883  4458 solver.cpp:314] Iteration 57500 (5.51369 iter/s, 18.1367s/100 iter), loss = 0.072806
I0925 07:49:05.531908  4458 solver.cpp:336]     Train net output #0: loss = 0.0728061 (* 1 = 0.0728061 loss)
I0925 07:49:05.531913  4458 sgd_solver.cpp:136] Iteration 57500, lr = 0.0001, m = 0.9
I0925 07:49:23.719696  4458 solver.cpp:314] Iteration 57600 (5.49834 iter/s, 18.1873s/100 iter), loss = 0.0447184
I0925 07:49:23.719749  4458 solver.cpp:336]     Train net output #0: loss = 0.0447185 (* 1 = 0.0447185 loss)
I0925 07:49:23.719755  4458 sgd_solver.cpp:136] Iteration 57600, lr = 0.0001, m = 0.9
I0925 07:49:37.741142  4466 data_reader.cpp:305] Starting prefetch of epoch 31
I0925 07:49:41.748276  4458 solver.cpp:314] Iteration 57700 (5.5469 iter/s, 18.0281s/100 iter), loss = 0.0667911
I0925 07:49:41.748299  4458 solver.cpp:336]     Train net output #0: loss = 0.0667912 (* 1 = 0.0667912 loss)
I0925 07:49:41.748304  4458 sgd_solver.cpp:136] Iteration 57700, lr = 0.0001, m = 0.9
I0925 07:49:59.967571  4458 solver.cpp:314] Iteration 57800 (5.48884 iter/s, 18.2188s/100 iter), loss = 0.0384064
I0925 07:49:59.967634  4458 solver.cpp:336]     Train net output #0: loss = 0.0384065 (* 1 = 0.0384065 loss)
I0925 07:49:59.967640  4458 sgd_solver.cpp:136] Iteration 57800, lr = 0.0001, m = 0.9
I0925 07:50:07.894681  4461 data_reader.cpp:305] Starting prefetch of epoch 37
I0925 07:50:17.913419  4458 solver.cpp:314] Iteration 57900 (5.57248 iter/s, 17.9453s/100 iter), loss = 0.0369699
I0925 07:50:17.913445  4458 solver.cpp:336]     Train net output #0: loss = 0.03697 (* 1 = 0.03697 loss)
I0925 07:50:17.913451  4458 sgd_solver.cpp:136] Iteration 57900, lr = 0.0001, m = 0.9
I0925 07:50:35.796440  4458 solver.cpp:368] Sparsity after update:
I0925 07:50:35.801280  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 07:50:35.801288  4458 net.cpp:2312] conv1a_param_0(0.255) 
I0925 07:50:35.801295  4458 net.cpp:2312] conv1b_param_0(0.587) 
I0925 07:50:35.801297  4458 net.cpp:2312] ctx_conv1_param_0(0.783) 
I0925 07:50:35.801300  4458 net.cpp:2312] ctx_conv2_param_0(0.795) 
I0925 07:50:35.801301  4458 net.cpp:2312] ctx_conv3_param_0(0.785) 
I0925 07:50:35.801303  4458 net.cpp:2312] ctx_conv4_param_0(0.797) 
I0925 07:50:35.801304  4458 net.cpp:2312] ctx_final_param_0(0.33) 
I0925 07:50:35.801306  4458 net.cpp:2312] out3a_param_0(0.819) 
I0925 07:50:35.801308  4458 net.cpp:2312] out5a_param_0(0.817) 
I0925 07:50:35.801311  4458 net.cpp:2312] res2a_branch2a_param_0(0.729) 
I0925 07:50:35.801312  4458 net.cpp:2312] res2a_branch2b_param_0(0.565) 
I0925 07:50:35.801314  4458 net.cpp:2312] res3a_branch2a_param_0(0.76) 
I0925 07:50:35.801316  4458 net.cpp:2312] res3a_branch2b_param_0(0.693) 
I0925 07:50:35.801318  4458 net.cpp:2312] res4a_branch2a_param_0(0.797) 
I0925 07:50:35.801321  4458 net.cpp:2312] res4a_branch2b_param_0(0.762) 
I0925 07:50:35.801322  4458 net.cpp:2312] res5a_branch2a_param_0(0.817) 
I0925 07:50:35.801324  4458 net.cpp:2312] res5a_branch2b_param_0(0.819) 
I0925 07:50:35.801326  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.16549e+06/2.69117e+06) 0.805
I0925 07:50:35.801334  4458 solver.cpp:562] Iteration 58000, Testing net (#0)
I0925 07:50:45.739270  4458 solver.cpp:654]     Test net output #0: accuracy/top1 = 0.956794
I0925 07:50:45.739293  4458 solver.cpp:654]     Test net output #1: accuracy/top5 = 1
I0925 07:50:45.739300  4458 solver.cpp:654]     Test net output #2: loss = 0.158824 (* 1 = 0.158824 loss)
I0925 07:50:45.739320  4458 solver.cpp:265] [MultiGPU] Tests completed in 9.93771s
I0925 07:50:45.936061  4458 solver.cpp:314] Iteration 58000 (3.56864 iter/s, 28.0219s/100 iter), loss = 0.0376317
I0925 07:50:45.936084  4458 solver.cpp:336]     Train net output #0: loss = 0.0376318 (* 1 = 0.0376318 loss)
I0925 07:50:45.936089  4458 sgd_solver.cpp:136] Iteration 58000, lr = 0.0001, m = 0.9
I0925 07:50:47.587656  4408 data_reader.cpp:305] Starting prefetch of epoch 41
I0925 07:51:03.891367  4458 solver.cpp:314] Iteration 58100 (5.56954 iter/s, 17.9548s/100 iter), loss = 0.0840066
I0925 07:51:03.891391  4458 solver.cpp:336]     Train net output #0: loss = 0.0840067 (* 1 = 0.0840067 loss)
I0925 07:51:03.891396  4458 sgd_solver.cpp:136] Iteration 58100, lr = 0.0001, m = 0.9
I0925 07:51:21.892405  4458 solver.cpp:314] Iteration 58200 (5.55539 iter/s, 18.0005s/100 iter), loss = 0.0703465
I0925 07:51:21.892474  4458 solver.cpp:336]     Train net output #0: loss = 0.0703466 (* 1 = 0.0703466 loss)
I0925 07:51:21.892480  4458 sgd_solver.cpp:136] Iteration 58200, lr = 0.0001, m = 0.9
I0925 07:51:39.974757  4458 solver.cpp:314] Iteration 58300 (5.53041 iter/s, 18.0818s/100 iter), loss = 0.0322683
I0925 07:51:39.974782  4458 solver.cpp:336]     Train net output #0: loss = 0.0322684 (* 1 = 0.0322684 loss)
I0925 07:51:39.974786  4458 sgd_solver.cpp:136] Iteration 58300, lr = 0.0001, m = 0.9
I0925 07:51:47.235466  4466 data_reader.cpp:305] Starting prefetch of epoch 32
I0925 07:51:58.105569  4458 solver.cpp:314] Iteration 58400 (5.51563 iter/s, 18.1303s/100 iter), loss = 0.0785419
I0925 07:51:58.105629  4458 solver.cpp:336]     Train net output #0: loss = 0.078542 (* 1 = 0.078542 loss)
I0925 07:51:58.105635  4458 sgd_solver.cpp:136] Iteration 58400, lr = 0.0001, m = 0.9
I0925 07:52:16.082960  4458 solver.cpp:314] Iteration 58500 (5.5627 iter/s, 17.9769s/100 iter), loss = 0.0339213
I0925 07:52:16.082993  4458 solver.cpp:336]     Train net output #0: loss = 0.0339214 (* 1 = 0.0339214 loss)
I0925 07:52:16.082998  4458 sgd_solver.cpp:136] Iteration 58500, lr = 0.0001, m = 0.9
I0925 07:52:17.025202  4408 data_reader.cpp:305] Starting prefetch of epoch 42
I0925 07:52:34.308351  4458 solver.cpp:314] Iteration 58600 (5.487 iter/s, 18.2249s/100 iter), loss = 0.0733907
I0925 07:52:34.308405  4458 solver.cpp:336]     Train net output #0: loss = 0.0733908 (* 1 = 0.0733908 loss)
I0925 07:52:34.308413  4458 sgd_solver.cpp:136] Iteration 58600, lr = 0.0001, m = 0.9
I0925 07:52:52.467239  4458 solver.cpp:314] Iteration 58700 (5.5071 iter/s, 18.1584s/100 iter), loss = 0.0929155
I0925 07:52:52.467265  4458 solver.cpp:336]     Train net output #0: loss = 0.0929156 (* 1 = 0.0929156 loss)
I0925 07:52:52.467270  4458 sgd_solver.cpp:136] Iteration 58700, lr = 0.0001, m = 0.9
I0925 07:53:10.407546  4458 solver.cpp:314] Iteration 58800 (5.5742 iter/s, 17.9398s/100 iter), loss = 0.067907
I0925 07:53:10.407946  4458 solver.cpp:336]     Train net output #0: loss = 0.0679071 (* 1 = 0.0679071 loss)
I0925 07:53:10.407955  4458 sgd_solver.cpp:136] Iteration 58800, lr = 0.0001, m = 0.9
I0925 07:53:16.782769  4408 data_reader.cpp:305] Starting prefetch of epoch 43
I0925 07:53:28.508641  4458 solver.cpp:314] Iteration 58900 (5.52468 iter/s, 18.1006s/100 iter), loss = 0.0560408
I0925 07:53:28.508664  4458 solver.cpp:336]     Train net output #0: loss = 0.0560409 (* 1 = 0.0560409 loss)
I0925 07:53:28.508668  4458 sgd_solver.cpp:136] Iteration 58900, lr = 0.0001, m = 0.9
I0925 07:53:46.554153  4458 solver.cpp:368] Sparsity after update:
I0925 07:53:46.560977  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 07:53:46.560986  4458 net.cpp:2312] conv1a_param_0(0.255) 
I0925 07:53:46.560992  4458 net.cpp:2312] conv1b_param_0(0.587) 
I0925 07:53:46.560993  4458 net.cpp:2312] ctx_conv1_param_0(0.783) 
I0925 07:53:46.560995  4458 net.cpp:2312] ctx_conv2_param_0(0.795) 
I0925 07:53:46.560997  4458 net.cpp:2312] ctx_conv3_param_0(0.785) 
I0925 07:53:46.560999  4458 net.cpp:2312] ctx_conv4_param_0(0.797) 
I0925 07:53:46.561002  4458 net.cpp:2312] ctx_final_param_0(0.33) 
I0925 07:53:46.561002  4458 net.cpp:2312] out3a_param_0(0.819) 
I0925 07:53:46.561004  4458 net.cpp:2312] out5a_param_0(0.817) 
I0925 07:53:46.561007  4458 net.cpp:2312] res2a_branch2a_param_0(0.729) 
I0925 07:53:46.561008  4458 net.cpp:2312] res2a_branch2b_param_0(0.565) 
I0925 07:53:46.561010  4458 net.cpp:2312] res3a_branch2a_param_0(0.76) 
I0925 07:53:46.561013  4458 net.cpp:2312] res3a_branch2b_param_0(0.693) 
I0925 07:53:46.561014  4458 net.cpp:2312] res4a_branch2a_param_0(0.797) 
I0925 07:53:46.561017  4458 net.cpp:2312] res4a_branch2b_param_0(0.762) 
I0925 07:53:46.561019  4458 net.cpp:2312] res5a_branch2a_param_0(0.817) 
I0925 07:53:46.561023  4458 net.cpp:2312] res5a_branch2b_param_0(0.819) 
I0925 07:53:46.561027  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.16549e+06/2.69117e+06) 0.805
I0925 07:53:46.730818  4458 solver.cpp:314] Iteration 59000 (5.48797 iter/s, 18.2217s/100 iter), loss = 0.0497536
I0925 07:53:46.730840  4458 solver.cpp:336]     Train net output #0: loss = 0.0497536 (* 1 = 0.0497536 loss)
I0925 07:53:46.730844  4458 sgd_solver.cpp:136] Iteration 59000, lr = 0.0001, m = 0.9
I0925 07:54:04.918082  4458 solver.cpp:314] Iteration 59100 (5.49851 iter/s, 18.1868s/100 iter), loss = 0.0522389
I0925 07:54:04.918110  4458 solver.cpp:336]     Train net output #0: loss = 0.052239 (* 1 = 0.052239 loss)
I0925 07:54:04.918114  4458 sgd_solver.cpp:136] Iteration 59100, lr = 0.0001, m = 0.9
I0925 07:54:16.822764  4466 data_reader.cpp:305] Starting prefetch of epoch 33
I0925 07:54:22.945536  4458 solver.cpp:314] Iteration 59200 (5.54725 iter/s, 18.027s/100 iter), loss = 0.0553003
I0925 07:54:22.945561  4458 solver.cpp:336]     Train net output #0: loss = 0.0553004 (* 1 = 0.0553004 loss)
I0925 07:54:22.945567  4458 sgd_solver.cpp:136] Iteration 59200, lr = 0.0001, m = 0.9
I0925 07:54:40.975474  4458 solver.cpp:314] Iteration 59300 (5.54649 iter/s, 18.0294s/100 iter), loss = 0.0427376
I0925 07:54:40.975497  4458 solver.cpp:336]     Train net output #0: loss = 0.0427377 (* 1 = 0.0427377 loss)
I0925 07:54:40.975502  4458 sgd_solver.cpp:136] Iteration 59300, lr = 0.0001, m = 0.9
I0925 07:54:58.952149  4458 solver.cpp:314] Iteration 59400 (5.56292 iter/s, 17.9762s/100 iter), loss = 0.0537405
I0925 07:54:58.952208  4458 solver.cpp:336]     Train net output #0: loss = 0.0537405 (* 1 = 0.0537405 loss)
I0925 07:54:58.952214  4458 sgd_solver.cpp:136] Iteration 59400, lr = 0.0001, m = 0.9
I0925 07:55:16.255244  4466 data_reader.cpp:305] Starting prefetch of epoch 34
I0925 07:55:16.957695  4458 solver.cpp:314] Iteration 59500 (5.554 iter/s, 18.005s/100 iter), loss = 0.0601218
I0925 07:55:16.957717  4458 solver.cpp:336]     Train net output #0: loss = 0.0601219 (* 1 = 0.0601219 loss)
I0925 07:55:16.957722  4458 sgd_solver.cpp:136] Iteration 59500, lr = 0.0001, m = 0.9
I0925 07:55:34.951856  4458 solver.cpp:314] Iteration 59600 (5.55751 iter/s, 17.9937s/100 iter), loss = 0.0477352
I0925 07:55:34.951949  4458 solver.cpp:336]     Train net output #0: loss = 0.0477353 (* 1 = 0.0477353 loss)
I0925 07:55:34.951956  4458 sgd_solver.cpp:136] Iteration 59600, lr = 0.0001, m = 0.9
I0925 07:55:46.247824  4461 data_reader.cpp:305] Starting prefetch of epoch 38
I0925 07:55:53.100174  4458 solver.cpp:314] Iteration 59700 (5.51031 iter/s, 18.1478s/100 iter), loss = 0.0795197
I0925 07:55:53.100200  4458 solver.cpp:336]     Train net output #0: loss = 0.0795198 (* 1 = 0.0795198 loss)
I0925 07:55:53.100204  4458 sgd_solver.cpp:136] Iteration 59700, lr = 0.0001, m = 0.9
I0925 07:56:11.134145  4458 solver.cpp:314] Iteration 59800 (5.54525 iter/s, 18.0335s/100 iter), loss = 0.0950312
I0925 07:56:11.134199  4458 solver.cpp:336]     Train net output #0: loss = 0.0950312 (* 1 = 0.0950312 loss)
I0925 07:56:11.134204  4458 sgd_solver.cpp:136] Iteration 59800, lr = 0.0001, m = 0.9
I0925 07:56:29.451949  4458 solver.cpp:314] Iteration 59900 (5.45932 iter/s, 18.3173s/100 iter), loss = 0.0741841
I0925 07:56:29.451968  4458 solver.cpp:336]     Train net output #0: loss = 0.0741842 (* 1 = 0.0741842 loss)
I0925 07:56:29.451973  4458 sgd_solver.cpp:136] Iteration 59900, lr = 0.0001, m = 0.9
I0925 07:56:46.734032  4410 data_reader.cpp:305] Starting prefetch of epoch 34
I0925 07:56:47.952311  4458 solver.cpp:314] Iteration 59999 (5.3514 iter/s, 18.4998s/99 iter), loss = 0.0565447
I0925 07:56:47.952337  4458 solver.cpp:336]     Train net output #0: loss = 0.0565448 (* 1 = 0.0565448 loss)
I0925 07:56:47.952345  4458 solver.cpp:824] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/cityscapes5_jsegnet21v2_iter_60000.caffemodel
I0925 07:56:48.082417  4458 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/cityscapes5_jsegnet21v2_iter_60000.solverstate
I0925 07:56:48.110386  4458 solver.cpp:368] Sparsity after update:
I0925 07:56:48.116899  4458 net.cpp:2301] Num Params(17), Sparsity (zero_weights/count): 
I0925 07:56:48.116936  4458 net.cpp:2312] conv1a_param_0(0.255) 
I0925 07:56:48.116960  4458 net.cpp:2312] conv1b_param_0(0.587) 
I0925 07:56:48.116971  4458 net.cpp:2312] ctx_conv1_param_0(0.783) 
I0925 07:56:48.116981  4458 net.cpp:2312] ctx_conv2_param_0(0.795) 
I0925 07:56:48.116991  4458 net.cpp:2312] ctx_conv3_param_0(0.785) 
I0925 07:56:48.117002  4458 net.cpp:2312] ctx_conv4_param_0(0.797) 
I0925 07:56:48.117012  4458 net.cpp:2312] ctx_final_param_0(0.33) 
I0925 07:56:48.117022  4458 net.cpp:2312] out3a_param_0(0.819) 
I0925 07:56:48.117031  4458 net.cpp:2312] out5a_param_0(0.817) 
I0925 07:56:48.117041  4458 net.cpp:2312] res2a_branch2a_param_0(0.729) 
I0925 07:56:48.117051  4458 net.cpp:2312] res2a_branch2b_param_0(0.565) 
I0925 07:56:48.117061  4458 net.cpp:2312] res3a_branch2a_param_0(0.76) 
I0925 07:56:48.117071  4458 net.cpp:2312] res3a_branch2b_param_0(0.693) 
I0925 07:56:48.117081  4458 net.cpp:2312] res4a_branch2a_param_0(0.797) 
I0925 07:56:48.117091  4458 net.cpp:2312] res4a_branch2b_param_0(0.762) 
I0925 07:56:48.117101  4458 net.cpp:2312] res5a_branch2a_param_0(0.817) 
I0925 07:56:48.117111  4458 net.cpp:2312] res5a_branch2b_param_0(0.819) 
I0925 07:56:48.117121  4458 net.cpp:2316] Total Sparsity (zero_weights/count) =  (2.16549e+06/2.69117e+06) 0.805
I0925 07:56:48.745121  4458 solver.cpp:537] Iteration 60000, loss = 0.0505027
I0925 07:56:48.745149  4458 solver.cpp:562] Iteration 60000, Testing net (#0)
I0925 07:57:43.393913  4521 data_reader.cpp:305] Starting prefetch of epoch 2
I0925 07:57:43.763092  4458 solver.cpp:654]     Test net output #0: accuracy/top1 = 0.95529
I0925 07:57:43.763110  4458 solver.cpp:654]     Test net output #1: accuracy/top5 = 1
I0925 07:57:43.763115  4458 solver.cpp:654]     Test net output #2: loss = 0.132895 (* 1 = 0.132895 loss)
I0925 07:57:43.938453  4338 parallel.cpp:71] Root Solver performance on device 0: 2.317 * 6 = 13.9 img/sec (60000 itr in 2.59e+04 sec)
I0925 07:57:43.938478  4338 parallel.cpp:76]      Solver performance on device 1: 2.316 * 6 = 13.9 img/sec (60000 itr in 2.59e+04 sec)
I0925 07:57:43.938486  4338 parallel.cpp:76]      Solver performance on device 2: 2.316 * 6 = 13.9 img/sec (60000 itr in 2.59e+04 sec)
I0925 07:57:43.938489  4338 parallel.cpp:79] Overall multi-GPU performance: 41.6968 img/sec
I0925 07:57:45.938951  4338 caffe.cpp:253] Optimization Done in 7h 12m 48s
