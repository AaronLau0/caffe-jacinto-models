I0916 20:26:03.384778 13475 caffe.cpp:807] This is NVCaffe 0.16.4 started at Sat Sep 16 20:26:02 2017
I0916 20:26:03.385712 13475 caffe.cpp:810] CuDNN version: 6021
I0916 20:26:03.385717 13475 caffe.cpp:811] CuBLAS version: 8000
I0916 20:26:03.385720 13475 caffe.cpp:812] CUDA version: 8000
I0916 20:26:03.385723 13475 caffe.cpp:813] CUDA driver version: 8000
I0916 20:26:03.792191 13475 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0916 20:26:03.792779 13475 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I0916 20:26:03.793313 13475 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[1]: total=8508145664 free=8379236352
I0916 20:26:03.793841 13475 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[2]: total=8508145664 free=8379236352
I0916 20:26:03.793853 13475 caffe.cpp:214] Using GPUs 0, 1, 2
I0916 20:26:03.794181 13475 caffe.cpp:219] GPU 0: GeForce GTX 1080
I0916 20:26:03.794505 13475 caffe.cpp:219] GPU 1: GeForce GTX 1080
I0916 20:26:03.794828 13475 caffe.cpp:219] GPU 2: GeForce GTX 1080
I0916 20:26:03.795560 13475 solver.cpp:43] Solver data type: FLOAT
I0916 20:26:03.795631 13475 solver.cpp:46] Initializing solver from parameters: 
train_net: "training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/train.prototxt"
test_net: "training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/test.prototxt"
test_iter: 125
test_interval: 2000
base_lr: 0.01
display: 100
max_iter: 60000
lr_policy: "multistep"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 1e-05
snapshot: 10000
snapshot_prefix: "training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/cityscapes5_jsegnet21v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
regularization_type: "L1"
test_initialization: false
stepvalue: 30000
stepvalue: 45000
iter_size: 1
type: "SGD"
display_sparsity: 1000
sparse_mode: SPARSE_UPDATE
sparsity_target: 0.8
sparsity_step_factor: 0.01
sparsity_step_iter: 1000
sparsity_start_iter: 3000
sparsity_start_factor: 0.8
I0916 20:26:03.814507 13475 solver.cpp:78] Creating training net from train_net file: training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/train.prototxt
I0916 20:26:03.815695 13475 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0916 20:26:03.815703 13475 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
W0916 20:26:03.815739 13475 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0916 20:26:03.816107 13475 net.cpp:72] Initializing net from parameters: 
name: "jsegnet21v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/train-image-lmdb"
    label_list_path: "data/train-label-lmdb"
    batch_size: 6
    shuffle: true
    threads: 1
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
I0916 20:26:03.816318 13475 net.cpp:104] Using FLOAT as default forward math type
I0916 20:26:03.816323 13475 net.cpp:110] Using FLOAT as default backward math type
I0916 20:26:03.816328 13475 layer_factory.hpp:136] Creating layer 'data' of type 'ImageLabelData'
I0916 20:26:03.816331 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:03.816345 13475 net.cpp:184] Created Layer data (0)
I0916 20:26:03.816350 13475 net.cpp:530] data -> data
I0916 20:26:03.816367 13475 net.cpp:530] data -> label
I0916 20:26:03.843883 13475 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 6
I0916 20:26:03.843914 13475 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0916 20:26:03.879007 13544 db_lmdb.cpp:24] Opened lmdb data/train-image-lmdb
I0916 20:26:03.882166 13475 data_layer.cpp:187] [0] ReshapePrefetch 6, 3, 640, 640
I0916 20:26:03.882225 13475 data_layer.cpp:211] [0] Output data size: 6, 3, 640, 640
I0916 20:26:03.882232 13475 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0916 20:26:03.882282 13475 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 6
I0916 20:26:03.882292 13475 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0916 20:26:03.883015 13545 data_layer.cpp:101] [0] Parser threads: 1
I0916 20:26:03.883029 13545 data_layer.cpp:103] [0] Transformer threads: 1
I0916 20:26:03.906769 13546 db_lmdb.cpp:24] Opened lmdb data/train-label-lmdb
I0916 20:26:03.907951 13475 data_layer.cpp:187] [0] ReshapePrefetch 6, 1, 640, 640
I0916 20:26:03.908010 13475 data_layer.cpp:211] [0] Output data size: 6, 1, 640, 640
I0916 20:26:03.908017 13475 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0916 20:26:03.908087 13475 net.cpp:245] Setting up data
I0916 20:26:03.908121 13475 net.cpp:252] TRAIN Top shape for layer 0 'data' 6 3 640 640 (7372800)
I0916 20:26:03.908139 13475 net.cpp:252] TRAIN Top shape for layer 0 'data' 6 1 640 640 (2457600)
I0916 20:26:03.908157 13475 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0916 20:26:03.908172 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:03.908200 13475 net.cpp:184] Created Layer data/bias (1)
I0916 20:26:03.908216 13475 net.cpp:561] data/bias <- data
I0916 20:26:03.908236 13475 net.cpp:530] data/bias -> data/bias
I0916 20:26:03.913107 13547 data_layer.cpp:101] [0] Parser threads: 1
I0916 20:26:03.913128 13547 data_layer.cpp:103] [0] Transformer threads: 1
I0916 20:26:03.916240 13475 net.cpp:245] Setting up data/bias
I0916 20:26:03.916266 13475 net.cpp:252] TRAIN Top shape for layer 1 'data/bias' 6 3 640 640 (7372800)
I0916 20:26:03.916298 13475 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0916 20:26:03.916306 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:03.916329 13475 net.cpp:184] Created Layer conv1a (2)
I0916 20:26:03.916335 13475 net.cpp:561] conv1a <- data/bias
I0916 20:26:03.916339 13475 net.cpp:530] conv1a -> conv1a
I0916 20:26:04.558624 13475 net.cpp:245] Setting up conv1a
I0916 20:26:04.558647 13475 net.cpp:252] TRAIN Top shape for layer 2 'conv1a' 6 32 320 320 (19660800)
I0916 20:26:04.558661 13475 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0916 20:26:04.558666 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.558681 13475 net.cpp:184] Created Layer conv1a/bn (3)
I0916 20:26:04.558686 13475 net.cpp:561] conv1a/bn <- conv1a
I0916 20:26:04.558692 13475 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0916 20:26:04.559540 13475 net.cpp:245] Setting up conv1a/bn
I0916 20:26:04.559551 13475 net.cpp:252] TRAIN Top shape for layer 3 'conv1a/bn' 6 32 320 320 (19660800)
I0916 20:26:04.559561 13475 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0916 20:26:04.559566 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.559573 13475 net.cpp:184] Created Layer conv1a/relu (4)
I0916 20:26:04.559577 13475 net.cpp:561] conv1a/relu <- conv1a
I0916 20:26:04.559581 13475 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0916 20:26:04.577548 13475 net.cpp:245] Setting up conv1a/relu
I0916 20:26:04.577563 13475 net.cpp:252] TRAIN Top shape for layer 4 'conv1a/relu' 6 32 320 320 (19660800)
I0916 20:26:04.577567 13475 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0916 20:26:04.577572 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.577594 13475 net.cpp:184] Created Layer conv1b (5)
I0916 20:26:04.577597 13475 net.cpp:561] conv1b <- conv1a
I0916 20:26:04.577602 13475 net.cpp:530] conv1b -> conv1b
I0916 20:26:04.580039 13475 net.cpp:245] Setting up conv1b
I0916 20:26:04.580051 13475 net.cpp:252] TRAIN Top shape for layer 5 'conv1b' 6 32 320 320 (19660800)
I0916 20:26:04.580060 13475 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0916 20:26:04.580065 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.580072 13475 net.cpp:184] Created Layer conv1b/bn (6)
I0916 20:26:04.580076 13475 net.cpp:561] conv1b/bn <- conv1b
I0916 20:26:04.580080 13475 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0916 20:26:04.580864 13475 net.cpp:245] Setting up conv1b/bn
I0916 20:26:04.580873 13475 net.cpp:252] TRAIN Top shape for layer 6 'conv1b/bn' 6 32 320 320 (19660800)
I0916 20:26:04.580883 13475 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0916 20:26:04.580886 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.580891 13475 net.cpp:184] Created Layer conv1b/relu (7)
I0916 20:26:04.580894 13475 net.cpp:561] conv1b/relu <- conv1b
I0916 20:26:04.580898 13475 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0916 20:26:04.580902 13475 net.cpp:245] Setting up conv1b/relu
I0916 20:26:04.580906 13475 net.cpp:252] TRAIN Top shape for layer 7 'conv1b/relu' 6 32 320 320 (19660800)
I0916 20:26:04.580910 13475 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0916 20:26:04.580914 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.580921 13475 net.cpp:184] Created Layer pool1 (8)
I0916 20:26:04.580924 13475 net.cpp:561] pool1 <- conv1b
I0916 20:26:04.580929 13475 net.cpp:530] pool1 -> pool1
I0916 20:26:04.581018 13475 net.cpp:245] Setting up pool1
I0916 20:26:04.581025 13475 net.cpp:252] TRAIN Top shape for layer 8 'pool1' 6 32 160 160 (4915200)
I0916 20:26:04.581028 13475 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0916 20:26:04.581043 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.581051 13475 net.cpp:184] Created Layer res2a_branch2a (9)
I0916 20:26:04.581054 13475 net.cpp:561] res2a_branch2a <- pool1
I0916 20:26:04.581058 13475 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0916 20:26:04.583695 13475 net.cpp:245] Setting up res2a_branch2a
I0916 20:26:04.583708 13475 net.cpp:252] TRAIN Top shape for layer 9 'res2a_branch2a' 6 64 160 160 (9830400)
I0916 20:26:04.583715 13475 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0916 20:26:04.583719 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.583725 13475 net.cpp:184] Created Layer res2a_branch2a/bn (10)
I0916 20:26:04.583729 13475 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0916 20:26:04.583734 13475 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0916 20:26:04.585078 13475 net.cpp:245] Setting up res2a_branch2a/bn
I0916 20:26:04.585088 13475 net.cpp:252] TRAIN Top shape for layer 10 'res2a_branch2a/bn' 6 64 160 160 (9830400)
I0916 20:26:04.585096 13475 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0916 20:26:04.585100 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.585104 13475 net.cpp:184] Created Layer res2a_branch2a/relu (11)
I0916 20:26:04.585108 13475 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0916 20:26:04.585111 13475 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0916 20:26:04.585119 13475 net.cpp:245] Setting up res2a_branch2a/relu
I0916 20:26:04.585124 13475 net.cpp:252] TRAIN Top shape for layer 11 'res2a_branch2a/relu' 6 64 160 160 (9830400)
I0916 20:26:04.585127 13475 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0916 20:26:04.585130 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.585141 13475 net.cpp:184] Created Layer res2a_branch2b (12)
I0916 20:26:04.585145 13475 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0916 20:26:04.585149 13475 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0916 20:26:04.586973 13475 net.cpp:245] Setting up res2a_branch2b
I0916 20:26:04.586984 13475 net.cpp:252] TRAIN Top shape for layer 12 'res2a_branch2b' 6 64 160 160 (9830400)
I0916 20:26:04.586990 13475 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0916 20:26:04.586994 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.587000 13475 net.cpp:184] Created Layer res2a_branch2b/bn (13)
I0916 20:26:04.587003 13475 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0916 20:26:04.587007 13475 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0916 20:26:04.587779 13475 net.cpp:245] Setting up res2a_branch2b/bn
I0916 20:26:04.587786 13475 net.cpp:252] TRAIN Top shape for layer 13 'res2a_branch2b/bn' 6 64 160 160 (9830400)
I0916 20:26:04.587795 13475 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0916 20:26:04.587798 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.587805 13475 net.cpp:184] Created Layer res2a_branch2b/relu (14)
I0916 20:26:04.587807 13475 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0916 20:26:04.587810 13475 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0916 20:26:04.587815 13475 net.cpp:245] Setting up res2a_branch2b/relu
I0916 20:26:04.587819 13475 net.cpp:252] TRAIN Top shape for layer 14 'res2a_branch2b/relu' 6 64 160 160 (9830400)
I0916 20:26:04.587823 13475 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0916 20:26:04.587826 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.587831 13475 net.cpp:184] Created Layer pool2 (15)
I0916 20:26:04.587836 13475 net.cpp:561] pool2 <- res2a_branch2b
I0916 20:26:04.587847 13475 net.cpp:530] pool2 -> pool2
I0916 20:26:04.587925 13475 net.cpp:245] Setting up pool2
I0916 20:26:04.587931 13475 net.cpp:252] TRAIN Top shape for layer 15 'pool2' 6 64 80 80 (2457600)
I0916 20:26:04.587935 13475 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0916 20:26:04.587939 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.587947 13475 net.cpp:184] Created Layer res3a_branch2a (16)
I0916 20:26:04.587950 13475 net.cpp:561] res3a_branch2a <- pool2
I0916 20:26:04.587954 13475 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0916 20:26:04.590296 13475 net.cpp:245] Setting up res3a_branch2a
I0916 20:26:04.590306 13475 net.cpp:252] TRAIN Top shape for layer 16 'res3a_branch2a' 6 128 80 80 (4915200)
I0916 20:26:04.590312 13475 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0916 20:26:04.590315 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.590320 13475 net.cpp:184] Created Layer res3a_branch2a/bn (17)
I0916 20:26:04.590323 13475 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0916 20:26:04.590327 13475 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0916 20:26:04.591083 13475 net.cpp:245] Setting up res3a_branch2a/bn
I0916 20:26:04.591090 13475 net.cpp:252] TRAIN Top shape for layer 17 'res3a_branch2a/bn' 6 128 80 80 (4915200)
I0916 20:26:04.591102 13475 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0916 20:26:04.591105 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.591110 13475 net.cpp:184] Created Layer res3a_branch2a/relu (18)
I0916 20:26:04.591114 13475 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0916 20:26:04.591117 13475 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0916 20:26:04.591122 13475 net.cpp:245] Setting up res3a_branch2a/relu
I0916 20:26:04.591126 13475 net.cpp:252] TRAIN Top shape for layer 18 'res3a_branch2a/relu' 6 128 80 80 (4915200)
I0916 20:26:04.591130 13475 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0916 20:26:04.591135 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.591141 13475 net.cpp:184] Created Layer res3a_branch2b (19)
I0916 20:26:04.591145 13475 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0916 20:26:04.591148 13475 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0916 20:26:04.592521 13475 net.cpp:245] Setting up res3a_branch2b
I0916 20:26:04.592530 13475 net.cpp:252] TRAIN Top shape for layer 19 'res3a_branch2b' 6 128 80 80 (4915200)
I0916 20:26:04.592536 13475 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0916 20:26:04.592540 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.592545 13475 net.cpp:184] Created Layer res3a_branch2b/bn (20)
I0916 20:26:04.592548 13475 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0916 20:26:04.592551 13475 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0916 20:26:04.593289 13475 net.cpp:245] Setting up res3a_branch2b/bn
I0916 20:26:04.593297 13475 net.cpp:252] TRAIN Top shape for layer 20 'res3a_branch2b/bn' 6 128 80 80 (4915200)
I0916 20:26:04.593305 13475 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0916 20:26:04.593308 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.593312 13475 net.cpp:184] Created Layer res3a_branch2b/relu (21)
I0916 20:26:04.593315 13475 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0916 20:26:04.593319 13475 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0916 20:26:04.593323 13475 net.cpp:245] Setting up res3a_branch2b/relu
I0916 20:26:04.593327 13475 net.cpp:252] TRAIN Top shape for layer 21 'res3a_branch2b/relu' 6 128 80 80 (4915200)
I0916 20:26:04.593331 13475 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0916 20:26:04.593343 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.593349 13475 net.cpp:184] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (22)
I0916 20:26:04.593353 13475 net.cpp:561] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0916 20:26:04.593356 13475 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0916 20:26:04.593360 13475 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0916 20:26:04.593415 13475 net.cpp:245] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0916 20:26:04.593420 13475 net.cpp:252] TRAIN Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 6 128 80 80 (4915200)
I0916 20:26:04.593425 13475 net.cpp:252] TRAIN Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 6 128 80 80 (4915200)
I0916 20:26:04.593428 13475 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0916 20:26:04.593431 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.593436 13475 net.cpp:184] Created Layer pool3 (23)
I0916 20:26:04.593439 13475 net.cpp:561] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0916 20:26:04.593442 13475 net.cpp:530] pool3 -> pool3
I0916 20:26:04.593515 13475 net.cpp:245] Setting up pool3
I0916 20:26:04.593521 13475 net.cpp:252] TRAIN Top shape for layer 23 'pool3' 6 128 40 40 (1228800)
I0916 20:26:04.593525 13475 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0916 20:26:04.593528 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.593536 13475 net.cpp:184] Created Layer res4a_branch2a (24)
I0916 20:26:04.593539 13475 net.cpp:561] res4a_branch2a <- pool3
I0916 20:26:04.593544 13475 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0916 20:26:04.603111 13475 net.cpp:245] Setting up res4a_branch2a
I0916 20:26:04.603122 13475 net.cpp:252] TRAIN Top shape for layer 24 'res4a_branch2a' 6 256 40 40 (2457600)
I0916 20:26:04.603128 13475 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0916 20:26:04.603132 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.603137 13475 net.cpp:184] Created Layer res4a_branch2a/bn (25)
I0916 20:26:04.603138 13475 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0916 20:26:04.603142 13475 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0916 20:26:04.603752 13475 net.cpp:245] Setting up res4a_branch2a/bn
I0916 20:26:04.603760 13475 net.cpp:252] TRAIN Top shape for layer 25 'res4a_branch2a/bn' 6 256 40 40 (2457600)
I0916 20:26:04.603765 13475 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0916 20:26:04.603767 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.603770 13475 net.cpp:184] Created Layer res4a_branch2a/relu (26)
I0916 20:26:04.603772 13475 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0916 20:26:04.603775 13475 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0916 20:26:04.603778 13475 net.cpp:245] Setting up res4a_branch2a/relu
I0916 20:26:04.603781 13475 net.cpp:252] TRAIN Top shape for layer 26 'res4a_branch2a/relu' 6 256 40 40 (2457600)
I0916 20:26:04.603783 13475 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0916 20:26:04.603785 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.603791 13475 net.cpp:184] Created Layer res4a_branch2b (27)
I0916 20:26:04.603794 13475 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0916 20:26:04.603796 13475 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0916 20:26:04.607446 13475 net.cpp:245] Setting up res4a_branch2b
I0916 20:26:04.607461 13475 net.cpp:252] TRAIN Top shape for layer 27 'res4a_branch2b' 6 256 40 40 (2457600)
I0916 20:26:04.607482 13475 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0916 20:26:04.607486 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.607494 13475 net.cpp:184] Created Layer res4a_branch2b/bn (28)
I0916 20:26:04.607498 13475 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0916 20:26:04.607503 13475 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0916 20:26:04.608333 13475 net.cpp:245] Setting up res4a_branch2b/bn
I0916 20:26:04.608343 13475 net.cpp:252] TRAIN Top shape for layer 28 'res4a_branch2b/bn' 6 256 40 40 (2457600)
I0916 20:26:04.608352 13475 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0916 20:26:04.608356 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.608361 13475 net.cpp:184] Created Layer res4a_branch2b/relu (29)
I0916 20:26:04.608364 13475 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0916 20:26:04.608368 13475 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0916 20:26:04.608373 13475 net.cpp:245] Setting up res4a_branch2b/relu
I0916 20:26:04.608378 13475 net.cpp:252] TRAIN Top shape for layer 29 'res4a_branch2b/relu' 6 256 40 40 (2457600)
I0916 20:26:04.608381 13475 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0916 20:26:04.608386 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.608392 13475 net.cpp:184] Created Layer pool4 (30)
I0916 20:26:04.608395 13475 net.cpp:561] pool4 <- res4a_branch2b
I0916 20:26:04.608399 13475 net.cpp:530] pool4 -> pool4
I0916 20:26:04.608486 13475 net.cpp:245] Setting up pool4
I0916 20:26:04.608492 13475 net.cpp:252] TRAIN Top shape for layer 30 'pool4' 6 256 40 40 (2457600)
I0916 20:26:04.608497 13475 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0916 20:26:04.608505 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.608517 13475 net.cpp:184] Created Layer res5a_branch2a (31)
I0916 20:26:04.608522 13475 net.cpp:561] res5a_branch2a <- pool4
I0916 20:26:04.608526 13475 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0916 20:26:04.636540 13475 net.cpp:245] Setting up res5a_branch2a
I0916 20:26:04.636560 13475 net.cpp:252] TRAIN Top shape for layer 31 'res5a_branch2a' 6 512 40 40 (4915200)
I0916 20:26:04.636567 13475 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0916 20:26:04.636571 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.636584 13475 net.cpp:184] Created Layer res5a_branch2a/bn (32)
I0916 20:26:04.636589 13475 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0916 20:26:04.636591 13475 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0916 20:26:04.637228 13475 net.cpp:245] Setting up res5a_branch2a/bn
I0916 20:26:04.637234 13475 net.cpp:252] TRAIN Top shape for layer 32 'res5a_branch2a/bn' 6 512 40 40 (4915200)
I0916 20:26:04.637240 13475 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0916 20:26:04.637243 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.637246 13475 net.cpp:184] Created Layer res5a_branch2a/relu (33)
I0916 20:26:04.637248 13475 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0916 20:26:04.637251 13475 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0916 20:26:04.637255 13475 net.cpp:245] Setting up res5a_branch2a/relu
I0916 20:26:04.637257 13475 net.cpp:252] TRAIN Top shape for layer 33 'res5a_branch2a/relu' 6 512 40 40 (4915200)
I0916 20:26:04.637259 13475 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0916 20:26:04.637262 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.637269 13475 net.cpp:184] Created Layer res5a_branch2b (34)
I0916 20:26:04.637279 13475 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0916 20:26:04.637282 13475 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0916 20:26:04.651511 13475 net.cpp:245] Setting up res5a_branch2b
I0916 20:26:04.651536 13475 net.cpp:252] TRAIN Top shape for layer 34 'res5a_branch2b' 6 512 40 40 (4915200)
I0916 20:26:04.651548 13475 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0916 20:26:04.651553 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.651562 13475 net.cpp:184] Created Layer res5a_branch2b/bn (35)
I0916 20:26:04.651567 13475 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0916 20:26:04.651571 13475 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0916 20:26:04.652423 13475 net.cpp:245] Setting up res5a_branch2b/bn
I0916 20:26:04.652433 13475 net.cpp:252] TRAIN Top shape for layer 35 'res5a_branch2b/bn' 6 512 40 40 (4915200)
I0916 20:26:04.652448 13475 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0916 20:26:04.652453 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.652458 13475 net.cpp:184] Created Layer res5a_branch2b/relu (36)
I0916 20:26:04.652462 13475 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0916 20:26:04.652467 13475 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0916 20:26:04.652473 13475 net.cpp:245] Setting up res5a_branch2b/relu
I0916 20:26:04.652477 13475 net.cpp:252] TRAIN Top shape for layer 36 'res5a_branch2b/relu' 6 512 40 40 (4915200)
I0916 20:26:04.652482 13475 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I0916 20:26:04.652485 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.652494 13475 net.cpp:184] Created Layer out5a (37)
I0916 20:26:04.652498 13475 net.cpp:561] out5a <- res5a_branch2b
I0916 20:26:04.652501 13475 net.cpp:530] out5a -> out5a
I0916 20:26:04.656940 13475 net.cpp:245] Setting up out5a
I0916 20:26:04.656951 13475 net.cpp:252] TRAIN Top shape for layer 37 'out5a' 6 64 40 40 (614400)
I0916 20:26:04.656956 13475 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I0916 20:26:04.656960 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.656963 13475 net.cpp:184] Created Layer out5a/bn (38)
I0916 20:26:04.656966 13475 net.cpp:561] out5a/bn <- out5a
I0916 20:26:04.656970 13475 net.cpp:513] out5a/bn -> out5a (in-place)
I0916 20:26:04.657615 13475 net.cpp:245] Setting up out5a/bn
I0916 20:26:04.657622 13475 net.cpp:252] TRAIN Top shape for layer 38 'out5a/bn' 6 64 40 40 (614400)
I0916 20:26:04.657627 13475 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I0916 20:26:04.657630 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.657634 13475 net.cpp:184] Created Layer out5a/relu (39)
I0916 20:26:04.657635 13475 net.cpp:561] out5a/relu <- out5a
I0916 20:26:04.657637 13475 net.cpp:513] out5a/relu -> out5a (in-place)
I0916 20:26:04.657640 13475 net.cpp:245] Setting up out5a/relu
I0916 20:26:04.657644 13475 net.cpp:252] TRAIN Top shape for layer 39 'out5a/relu' 6 64 40 40 (614400)
I0916 20:26:04.657645 13475 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I0916 20:26:04.657647 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.657660 13475 net.cpp:184] Created Layer out5a_up2 (40)
I0916 20:26:04.657663 13475 net.cpp:561] out5a_up2 <- out5a
I0916 20:26:04.657665 13475 net.cpp:530] out5a_up2 -> out5a_up2
I0916 20:26:04.657950 13475 net.cpp:245] Setting up out5a_up2
I0916 20:26:04.657955 13475 net.cpp:252] TRAIN Top shape for layer 40 'out5a_up2' 6 64 80 80 (2457600)
I0916 20:26:04.657958 13475 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I0916 20:26:04.657961 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.657975 13475 net.cpp:184] Created Layer out3a (41)
I0916 20:26:04.657977 13475 net.cpp:561] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0916 20:26:04.657980 13475 net.cpp:530] out3a -> out3a
I0916 20:26:04.659077 13475 net.cpp:245] Setting up out3a
I0916 20:26:04.659085 13475 net.cpp:252] TRAIN Top shape for layer 41 'out3a' 6 64 80 80 (2457600)
I0916 20:26:04.659088 13475 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I0916 20:26:04.659091 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.659096 13475 net.cpp:184] Created Layer out3a/bn (42)
I0916 20:26:04.659097 13475 net.cpp:561] out3a/bn <- out3a
I0916 20:26:04.659099 13475 net.cpp:513] out3a/bn -> out3a (in-place)
I0916 20:26:04.660078 13475 net.cpp:245] Setting up out3a/bn
I0916 20:26:04.660085 13475 net.cpp:252] TRAIN Top shape for layer 42 'out3a/bn' 6 64 80 80 (2457600)
I0916 20:26:04.660091 13475 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I0916 20:26:04.660094 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.660096 13475 net.cpp:184] Created Layer out3a/relu (43)
I0916 20:26:04.660099 13475 net.cpp:561] out3a/relu <- out3a
I0916 20:26:04.660100 13475 net.cpp:513] out3a/relu -> out3a (in-place)
I0916 20:26:04.660104 13475 net.cpp:245] Setting up out3a/relu
I0916 20:26:04.660106 13475 net.cpp:252] TRAIN Top shape for layer 43 'out3a/relu' 6 64 80 80 (2457600)
I0916 20:26:04.660109 13475 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I0916 20:26:04.660110 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.660606 13475 net.cpp:184] Created Layer out3_out5_combined (44)
I0916 20:26:04.660611 13475 net.cpp:561] out3_out5_combined <- out5a_up2
I0916 20:26:04.660614 13475 net.cpp:561] out3_out5_combined <- out3a
I0916 20:26:04.660615 13475 net.cpp:530] out3_out5_combined -> out3_out5_combined
I0916 20:26:04.660645 13475 net.cpp:245] Setting up out3_out5_combined
I0916 20:26:04.660650 13475 net.cpp:252] TRAIN Top shape for layer 44 'out3_out5_combined' 6 64 80 80 (2457600)
I0916 20:26:04.660652 13475 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I0916 20:26:04.660655 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.660665 13475 net.cpp:184] Created Layer ctx_conv1 (45)
I0916 20:26:04.660668 13475 net.cpp:561] ctx_conv1 <- out3_out5_combined
I0916 20:26:04.660671 13475 net.cpp:530] ctx_conv1 -> ctx_conv1
I0916 20:26:04.661744 13475 net.cpp:245] Setting up ctx_conv1
I0916 20:26:04.661751 13475 net.cpp:252] TRAIN Top shape for layer 45 'ctx_conv1' 6 64 80 80 (2457600)
I0916 20:26:04.661756 13475 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I0916 20:26:04.661757 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.661761 13475 net.cpp:184] Created Layer ctx_conv1/bn (46)
I0916 20:26:04.661763 13475 net.cpp:561] ctx_conv1/bn <- ctx_conv1
I0916 20:26:04.661767 13475 net.cpp:513] ctx_conv1/bn -> ctx_conv1 (in-place)
I0916 20:26:04.662420 13475 net.cpp:245] Setting up ctx_conv1/bn
I0916 20:26:04.662427 13475 net.cpp:252] TRAIN Top shape for layer 46 'ctx_conv1/bn' 6 64 80 80 (2457600)
I0916 20:26:04.662432 13475 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I0916 20:26:04.662436 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.662437 13475 net.cpp:184] Created Layer ctx_conv1/relu (47)
I0916 20:26:04.662439 13475 net.cpp:561] ctx_conv1/relu <- ctx_conv1
I0916 20:26:04.662442 13475 net.cpp:513] ctx_conv1/relu -> ctx_conv1 (in-place)
I0916 20:26:04.662446 13475 net.cpp:245] Setting up ctx_conv1/relu
I0916 20:26:04.662447 13475 net.cpp:252] TRAIN Top shape for layer 47 'ctx_conv1/relu' 6 64 80 80 (2457600)
I0916 20:26:04.662456 13475 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I0916 20:26:04.662458 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.662463 13475 net.cpp:184] Created Layer ctx_conv2 (48)
I0916 20:26:04.662467 13475 net.cpp:561] ctx_conv2 <- ctx_conv1
I0916 20:26:04.662468 13475 net.cpp:530] ctx_conv2 -> ctx_conv2
I0916 20:26:04.663533 13475 net.cpp:245] Setting up ctx_conv2
I0916 20:26:04.663540 13475 net.cpp:252] TRAIN Top shape for layer 48 'ctx_conv2' 6 64 80 80 (2457600)
I0916 20:26:04.663543 13475 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I0916 20:26:04.663547 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.663549 13475 net.cpp:184] Created Layer ctx_conv2/bn (49)
I0916 20:26:04.663552 13475 net.cpp:561] ctx_conv2/bn <- ctx_conv2
I0916 20:26:04.663554 13475 net.cpp:513] ctx_conv2/bn -> ctx_conv2 (in-place)
I0916 20:26:04.664183 13475 net.cpp:245] Setting up ctx_conv2/bn
I0916 20:26:04.664191 13475 net.cpp:252] TRAIN Top shape for layer 49 'ctx_conv2/bn' 6 64 80 80 (2457600)
I0916 20:26:04.664196 13475 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I0916 20:26:04.664198 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.664201 13475 net.cpp:184] Created Layer ctx_conv2/relu (50)
I0916 20:26:04.664203 13475 net.cpp:561] ctx_conv2/relu <- ctx_conv2
I0916 20:26:04.664206 13475 net.cpp:513] ctx_conv2/relu -> ctx_conv2 (in-place)
I0916 20:26:04.664208 13475 net.cpp:245] Setting up ctx_conv2/relu
I0916 20:26:04.664211 13475 net.cpp:252] TRAIN Top shape for layer 50 'ctx_conv2/relu' 6 64 80 80 (2457600)
I0916 20:26:04.664213 13475 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I0916 20:26:04.664214 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.664224 13475 net.cpp:184] Created Layer ctx_conv3 (51)
I0916 20:26:04.664227 13475 net.cpp:561] ctx_conv3 <- ctx_conv2
I0916 20:26:04.664229 13475 net.cpp:530] ctx_conv3 -> ctx_conv3
I0916 20:26:04.665297 13475 net.cpp:245] Setting up ctx_conv3
I0916 20:26:04.665304 13475 net.cpp:252] TRAIN Top shape for layer 51 'ctx_conv3' 6 64 80 80 (2457600)
I0916 20:26:04.665307 13475 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I0916 20:26:04.665310 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.665314 13475 net.cpp:184] Created Layer ctx_conv3/bn (52)
I0916 20:26:04.665316 13475 net.cpp:561] ctx_conv3/bn <- ctx_conv3
I0916 20:26:04.665318 13475 net.cpp:513] ctx_conv3/bn -> ctx_conv3 (in-place)
I0916 20:26:04.665943 13475 net.cpp:245] Setting up ctx_conv3/bn
I0916 20:26:04.665949 13475 net.cpp:252] TRAIN Top shape for layer 52 'ctx_conv3/bn' 6 64 80 80 (2457600)
I0916 20:26:04.665954 13475 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I0916 20:26:04.665957 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.665961 13475 net.cpp:184] Created Layer ctx_conv3/relu (53)
I0916 20:26:04.665962 13475 net.cpp:561] ctx_conv3/relu <- ctx_conv3
I0916 20:26:04.665964 13475 net.cpp:513] ctx_conv3/relu -> ctx_conv3 (in-place)
I0916 20:26:04.665971 13475 net.cpp:245] Setting up ctx_conv3/relu
I0916 20:26:04.665974 13475 net.cpp:252] TRAIN Top shape for layer 53 'ctx_conv3/relu' 6 64 80 80 (2457600)
I0916 20:26:04.665977 13475 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I0916 20:26:04.665978 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.665987 13475 net.cpp:184] Created Layer ctx_conv4 (54)
I0916 20:26:04.665990 13475 net.cpp:561] ctx_conv4 <- ctx_conv3
I0916 20:26:04.665992 13475 net.cpp:530] ctx_conv4 -> ctx_conv4
I0916 20:26:04.667327 13475 net.cpp:245] Setting up ctx_conv4
I0916 20:26:04.667354 13475 net.cpp:252] TRAIN Top shape for layer 54 'ctx_conv4' 6 64 80 80 (2457600)
I0916 20:26:04.667363 13475 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I0916 20:26:04.667369 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.667378 13475 net.cpp:184] Created Layer ctx_conv4/bn (55)
I0916 20:26:04.667384 13475 net.cpp:561] ctx_conv4/bn <- ctx_conv4
I0916 20:26:04.667392 13475 net.cpp:513] ctx_conv4/bn -> ctx_conv4 (in-place)
I0916 20:26:04.668269 13475 net.cpp:245] Setting up ctx_conv4/bn
I0916 20:26:04.668278 13475 net.cpp:252] TRAIN Top shape for layer 55 'ctx_conv4/bn' 6 64 80 80 (2457600)
I0916 20:26:04.668287 13475 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I0916 20:26:04.668290 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.668295 13475 net.cpp:184] Created Layer ctx_conv4/relu (56)
I0916 20:26:04.668298 13475 net.cpp:561] ctx_conv4/relu <- ctx_conv4
I0916 20:26:04.668301 13475 net.cpp:513] ctx_conv4/relu -> ctx_conv4 (in-place)
I0916 20:26:04.668306 13475 net.cpp:245] Setting up ctx_conv4/relu
I0916 20:26:04.668311 13475 net.cpp:252] TRAIN Top shape for layer 56 'ctx_conv4/relu' 6 64 80 80 (2457600)
I0916 20:26:04.668314 13475 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I0916 20:26:04.668319 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.668334 13475 net.cpp:184] Created Layer ctx_final (57)
I0916 20:26:04.668337 13475 net.cpp:561] ctx_final <- ctx_conv4
I0916 20:26:04.668340 13475 net.cpp:530] ctx_final -> ctx_final
I0916 20:26:04.668964 13475 net.cpp:245] Setting up ctx_final
I0916 20:26:04.668973 13475 net.cpp:252] TRAIN Top shape for layer 57 'ctx_final' 6 8 80 80 (307200)
I0916 20:26:04.668979 13475 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I0916 20:26:04.668982 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.668987 13475 net.cpp:184] Created Layer ctx_final/relu (58)
I0916 20:26:04.668990 13475 net.cpp:561] ctx_final/relu <- ctx_final
I0916 20:26:04.668993 13475 net.cpp:513] ctx_final/relu -> ctx_final (in-place)
I0916 20:26:04.669000 13475 net.cpp:245] Setting up ctx_final/relu
I0916 20:26:04.669004 13475 net.cpp:252] TRAIN Top shape for layer 58 'ctx_final/relu' 6 8 80 80 (307200)
I0916 20:26:04.669008 13475 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I0916 20:26:04.669013 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.669020 13475 net.cpp:184] Created Layer out_deconv_final_up2 (59)
I0916 20:26:04.669024 13475 net.cpp:561] out_deconv_final_up2 <- ctx_final
I0916 20:26:04.669029 13475 net.cpp:530] out_deconv_final_up2 -> out_deconv_final_up2
I0916 20:26:04.669391 13475 net.cpp:245] Setting up out_deconv_final_up2
I0916 20:26:04.669399 13475 net.cpp:252] TRAIN Top shape for layer 59 'out_deconv_final_up2' 6 8 160 160 (1228800)
I0916 20:26:04.669404 13475 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I0916 20:26:04.669409 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.669414 13475 net.cpp:184] Created Layer out_deconv_final_up4 (60)
I0916 20:26:04.669420 13475 net.cpp:561] out_deconv_final_up4 <- out_deconv_final_up2
I0916 20:26:04.669425 13475 net.cpp:530] out_deconv_final_up4 -> out_deconv_final_up4
I0916 20:26:04.669870 13475 net.cpp:245] Setting up out_deconv_final_up4
I0916 20:26:04.669878 13475 net.cpp:252] TRAIN Top shape for layer 60 'out_deconv_final_up4' 6 8 320 320 (4915200)
I0916 20:26:04.669884 13475 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I0916 20:26:04.669888 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.669895 13475 net.cpp:184] Created Layer out_deconv_final_up8 (61)
I0916 20:26:04.669906 13475 net.cpp:561] out_deconv_final_up8 <- out_deconv_final_up4
I0916 20:26:04.669911 13475 net.cpp:530] out_deconv_final_up8 -> out_deconv_final_up8
I0916 20:26:04.670281 13475 net.cpp:245] Setting up out_deconv_final_up8
I0916 20:26:04.670289 13475 net.cpp:252] TRAIN Top shape for layer 61 'out_deconv_final_up8' 6 8 640 640 (19660800)
I0916 20:26:04.670295 13475 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0916 20:26:04.670300 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.670311 13475 net.cpp:184] Created Layer loss (62)
I0916 20:26:04.670316 13475 net.cpp:561] loss <- out_deconv_final_up8
I0916 20:26:04.670320 13475 net.cpp:561] loss <- label
I0916 20:26:04.670326 13475 net.cpp:530] loss -> loss
I0916 20:26:04.671946 13475 net.cpp:245] Setting up loss
I0916 20:26:04.671957 13475 net.cpp:252] TRAIN Top shape for layer 62 'loss' (1)
I0916 20:26:04.671960 13475 net.cpp:256]     with loss weight 1
I0916 20:26:04.671977 13475 net.cpp:323] loss needs backward computation.
I0916 20:26:04.671982 13475 net.cpp:323] out_deconv_final_up8 needs backward computation.
I0916 20:26:04.671985 13475 net.cpp:323] out_deconv_final_up4 needs backward computation.
I0916 20:26:04.671988 13475 net.cpp:323] out_deconv_final_up2 needs backward computation.
I0916 20:26:04.671998 13475 net.cpp:323] ctx_final/relu needs backward computation.
I0916 20:26:04.672003 13475 net.cpp:323] ctx_final needs backward computation.
I0916 20:26:04.672006 13475 net.cpp:323] ctx_conv4/relu needs backward computation.
I0916 20:26:04.672009 13475 net.cpp:323] ctx_conv4/bn needs backward computation.
I0916 20:26:04.672014 13475 net.cpp:323] ctx_conv4 needs backward computation.
I0916 20:26:04.672019 13475 net.cpp:323] ctx_conv3/relu needs backward computation.
I0916 20:26:04.672022 13475 net.cpp:323] ctx_conv3/bn needs backward computation.
I0916 20:26:04.672026 13475 net.cpp:323] ctx_conv3 needs backward computation.
I0916 20:26:04.672030 13475 net.cpp:323] ctx_conv2/relu needs backward computation.
I0916 20:26:04.672034 13475 net.cpp:323] ctx_conv2/bn needs backward computation.
I0916 20:26:04.672037 13475 net.cpp:323] ctx_conv2 needs backward computation.
I0916 20:26:04.672041 13475 net.cpp:323] ctx_conv1/relu needs backward computation.
I0916 20:26:04.672045 13475 net.cpp:323] ctx_conv1/bn needs backward computation.
I0916 20:26:04.672049 13475 net.cpp:323] ctx_conv1 needs backward computation.
I0916 20:26:04.672052 13475 net.cpp:323] out3_out5_combined needs backward computation.
I0916 20:26:04.672057 13475 net.cpp:323] out3a/relu needs backward computation.
I0916 20:26:04.672060 13475 net.cpp:323] out3a/bn needs backward computation.
I0916 20:26:04.672063 13475 net.cpp:323] out3a needs backward computation.
I0916 20:26:04.672067 13475 net.cpp:323] out5a_up2 needs backward computation.
I0916 20:26:04.672071 13475 net.cpp:323] out5a/relu needs backward computation.
I0916 20:26:04.672075 13475 net.cpp:323] out5a/bn needs backward computation.
I0916 20:26:04.672078 13475 net.cpp:323] out5a needs backward computation.
I0916 20:26:04.672081 13475 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0916 20:26:04.672086 13475 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0916 20:26:04.672088 13475 net.cpp:323] res5a_branch2b needs backward computation.
I0916 20:26:04.672091 13475 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0916 20:26:04.672096 13475 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0916 20:26:04.672098 13475 net.cpp:323] res5a_branch2a needs backward computation.
I0916 20:26:04.672102 13475 net.cpp:323] pool4 needs backward computation.
I0916 20:26:04.672107 13475 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0916 20:26:04.672111 13475 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0916 20:26:04.672114 13475 net.cpp:323] res4a_branch2b needs backward computation.
I0916 20:26:04.672117 13475 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0916 20:26:04.672127 13475 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0916 20:26:04.672132 13475 net.cpp:323] res4a_branch2a needs backward computation.
I0916 20:26:04.672135 13475 net.cpp:323] pool3 needs backward computation.
I0916 20:26:04.672139 13475 net.cpp:323] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I0916 20:26:04.672143 13475 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0916 20:26:04.672147 13475 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0916 20:26:04.672150 13475 net.cpp:323] res3a_branch2b needs backward computation.
I0916 20:26:04.672154 13475 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0916 20:26:04.672158 13475 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0916 20:26:04.672161 13475 net.cpp:323] res3a_branch2a needs backward computation.
I0916 20:26:04.672165 13475 net.cpp:323] pool2 needs backward computation.
I0916 20:26:04.672169 13475 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0916 20:26:04.672173 13475 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0916 20:26:04.672176 13475 net.cpp:323] res2a_branch2b needs backward computation.
I0916 20:26:04.672180 13475 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0916 20:26:04.672184 13475 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0916 20:26:04.672188 13475 net.cpp:323] res2a_branch2a needs backward computation.
I0916 20:26:04.672190 13475 net.cpp:323] pool1 needs backward computation.
I0916 20:26:04.672194 13475 net.cpp:323] conv1b/relu needs backward computation.
I0916 20:26:04.672197 13475 net.cpp:323] conv1b/bn needs backward computation.
I0916 20:26:04.672200 13475 net.cpp:323] conv1b needs backward computation.
I0916 20:26:04.672204 13475 net.cpp:323] conv1a/relu needs backward computation.
I0916 20:26:04.672207 13475 net.cpp:323] conv1a/bn needs backward computation.
I0916 20:26:04.672211 13475 net.cpp:323] conv1a needs backward computation.
I0916 20:26:04.672216 13475 net.cpp:325] data/bias does not need backward computation.
I0916 20:26:04.672220 13475 net.cpp:325] data does not need backward computation.
I0916 20:26:04.672224 13475 net.cpp:367] This network produces output loss
I0916 20:26:04.672278 13475 net.cpp:389] Top memory (TRAIN) required for data: 1435238408 diff: 1435238408
I0916 20:26:04.672283 13475 net.cpp:392] Bottom memory (TRAIN) required for data: 1435238400 diff: 1435238400
I0916 20:26:04.672287 13475 net.cpp:395] Shared (in-place) memory (TRAIN) by data: 772915200 diff: 772915200
I0916 20:26:04.672291 13475 net.cpp:398] Parameters memory (TRAIN) required for data: 10817840 diff: 10817840
I0916 20:26:04.672294 13475 net.cpp:401] Parameters shared memory (TRAIN) by data: 0 diff: 0
I0916 20:26:04.672297 13475 net.cpp:407] Network initialization done.
I0916 20:26:04.673413 13475 solver.cpp:177] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/test.prototxt
W0916 20:26:04.673502 13475 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0916 20:26:04.673785 13475 net.cpp:72] Initializing net from parameters: 
name: "jsegnet21v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/val-image-lmdb"
    label_list_path: "data/val-label-lmdb"
    batch_size: 2
    threads: 1
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
  accuracy_param {
    ignore_label: 255
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
    ignore_label: 255
  }
}
I0916 20:26:04.673966 13475 net.cpp:104] Using FLOAT as default forward math type
I0916 20:26:04.673972 13475 net.cpp:110] Using FLOAT as default backward math type
I0916 20:26:04.673975 13475 layer_factory.hpp:136] Creating layer 'data' of type 'ImageLabelData'
I0916 20:26:04.673979 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.673985 13475 net.cpp:184] Created Layer data (0)
I0916 20:26:04.673988 13475 net.cpp:530] data -> data
I0916 20:26:04.673992 13475 net.cpp:530] data -> label
I0916 20:26:04.674023 13475 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 2
I0916 20:26:04.674031 13475 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0916 20:26:04.674778 13566 db_lmdb.cpp:24] Opened lmdb data/val-image-lmdb
I0916 20:26:04.676133 13475 data_layer.cpp:187] (0) ReshapePrefetch 2, 3, 640, 640
I0916 20:26:04.676244 13475 data_layer.cpp:211] (0) Output data size: 2, 3, 640, 640
I0916 20:26:04.676250 13475 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0916 20:26:04.676292 13475 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 2
I0916 20:26:04.676300 13475 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0916 20:26:04.677043 13567 data_layer.cpp:101] (0) Parser threads: 1
I0916 20:26:04.677057 13567 data_layer.cpp:103] (0) Transformer threads: 1
I0916 20:26:04.679478 13568 db_lmdb.cpp:24] Opened lmdb data/val-label-lmdb
I0916 20:26:04.680451 13475 data_layer.cpp:187] (0) ReshapePrefetch 2, 1, 640, 640
I0916 20:26:04.680532 13475 data_layer.cpp:211] (0) Output data size: 2, 1, 640, 640
I0916 20:26:04.680541 13475 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0916 20:26:04.680613 13475 net.cpp:245] Setting up data
I0916 20:26:04.680625 13475 net.cpp:252] TEST Top shape for layer 0 'data' 2 3 640 640 (2457600)
I0916 20:26:04.680631 13475 net.cpp:252] TEST Top shape for layer 0 'data' 2 1 640 640 (819200)
I0916 20:26:04.680640 13475 layer_factory.hpp:136] Creating layer 'label_data_1_split' of type 'Split'
I0916 20:26:04.680647 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.680658 13475 net.cpp:184] Created Layer label_data_1_split (1)
I0916 20:26:04.680662 13475 net.cpp:561] label_data_1_split <- label
I0916 20:26:04.680670 13475 net.cpp:530] label_data_1_split -> label_data_1_split_0
I0916 20:26:04.680677 13475 net.cpp:530] label_data_1_split -> label_data_1_split_1
I0916 20:26:04.680681 13475 net.cpp:530] label_data_1_split -> label_data_1_split_2
I0916 20:26:04.680764 13475 net.cpp:245] Setting up label_data_1_split
I0916 20:26:04.680770 13475 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0916 20:26:04.680776 13475 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0916 20:26:04.680780 13475 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0916 20:26:04.680785 13475 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0916 20:26:04.680789 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.680799 13475 net.cpp:184] Created Layer data/bias (2)
I0916 20:26:04.680801 13475 net.cpp:561] data/bias <- data
I0916 20:26:04.680805 13475 net.cpp:530] data/bias -> data/bias
I0916 20:26:04.681757 13569 data_layer.cpp:101] (0) Parser threads: 1
I0916 20:26:04.681771 13569 data_layer.cpp:103] (0) Transformer threads: 1
I0916 20:26:04.683601 13475 net.cpp:245] Setting up data/bias
I0916 20:26:04.683625 13475 net.cpp:252] TEST Top shape for layer 2 'data/bias' 2 3 640 640 (2457600)
I0916 20:26:04.683642 13475 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0916 20:26:04.683650 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.683673 13475 net.cpp:184] Created Layer conv1a (3)
I0916 20:26:04.683679 13475 net.cpp:561] conv1a <- data/bias
I0916 20:26:04.683686 13475 net.cpp:530] conv1a -> conv1a
I0916 20:26:04.684249 13475 net.cpp:245] Setting up conv1a
I0916 20:26:04.684259 13475 net.cpp:252] TEST Top shape for layer 3 'conv1a' 2 32 320 320 (6553600)
I0916 20:26:04.684268 13475 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0916 20:26:04.684281 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.684291 13475 net.cpp:184] Created Layer conv1a/bn (4)
I0916 20:26:04.684296 13475 net.cpp:561] conv1a/bn <- conv1a
I0916 20:26:04.684305 13475 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0916 20:26:04.685003 13475 net.cpp:245] Setting up conv1a/bn
I0916 20:26:04.685012 13475 net.cpp:252] TEST Top shape for layer 4 'conv1a/bn' 2 32 320 320 (6553600)
I0916 20:26:04.685022 13475 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0916 20:26:04.685027 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.685039 13475 net.cpp:184] Created Layer conv1a/relu (5)
I0916 20:26:04.685044 13475 net.cpp:561] conv1a/relu <- conv1a
I0916 20:26:04.685047 13475 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0916 20:26:04.685058 13475 net.cpp:245] Setting up conv1a/relu
I0916 20:26:04.685063 13475 net.cpp:252] TEST Top shape for layer 5 'conv1a/relu' 2 32 320 320 (6553600)
I0916 20:26:04.685071 13475 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0916 20:26:04.685076 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.685096 13475 net.cpp:184] Created Layer conv1b (6)
I0916 20:26:04.685101 13475 net.cpp:561] conv1b <- conv1a
I0916 20:26:04.685113 13475 net.cpp:530] conv1b -> conv1b
I0916 20:26:04.686100 13475 net.cpp:245] Setting up conv1b
I0916 20:26:04.686110 13475 net.cpp:252] TEST Top shape for layer 6 'conv1b' 2 32 320 320 (6553600)
I0916 20:26:04.686120 13475 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0916 20:26:04.686131 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.686152 13475 net.cpp:184] Created Layer conv1b/bn (7)
I0916 20:26:04.686156 13475 net.cpp:561] conv1b/bn <- conv1b
I0916 20:26:04.686166 13475 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0916 20:26:04.689126 13475 net.cpp:245] Setting up conv1b/bn
I0916 20:26:04.689136 13475 net.cpp:252] TEST Top shape for layer 7 'conv1b/bn' 2 32 320 320 (6553600)
I0916 20:26:04.689144 13475 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0916 20:26:04.689148 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.689153 13475 net.cpp:184] Created Layer conv1b/relu (8)
I0916 20:26:04.689157 13475 net.cpp:561] conv1b/relu <- conv1b
I0916 20:26:04.689162 13475 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0916 20:26:04.689167 13475 net.cpp:245] Setting up conv1b/relu
I0916 20:26:04.689172 13475 net.cpp:252] TEST Top shape for layer 8 'conv1b/relu' 2 32 320 320 (6553600)
I0916 20:26:04.689175 13475 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0916 20:26:04.689179 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.689185 13475 net.cpp:184] Created Layer pool1 (9)
I0916 20:26:04.689188 13475 net.cpp:561] pool1 <- conv1b
I0916 20:26:04.689193 13475 net.cpp:530] pool1 -> pool1
I0916 20:26:04.689257 13475 net.cpp:245] Setting up pool1
I0916 20:26:04.689263 13475 net.cpp:252] TEST Top shape for layer 9 'pool1' 2 32 160 160 (1638400)
I0916 20:26:04.689267 13475 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0916 20:26:04.689271 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.689280 13475 net.cpp:184] Created Layer res2a_branch2a (10)
I0916 20:26:04.689285 13475 net.cpp:561] res2a_branch2a <- pool1
I0916 20:26:04.689288 13475 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0916 20:26:04.690331 13475 net.cpp:245] Setting up res2a_branch2a
I0916 20:26:04.690347 13475 net.cpp:252] TEST Top shape for layer 10 'res2a_branch2a' 2 64 160 160 (3276800)
I0916 20:26:04.690368 13475 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0916 20:26:04.690376 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.690393 13475 net.cpp:184] Created Layer res2a_branch2a/bn (11)
I0916 20:26:04.690399 13475 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0916 20:26:04.690404 13475 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0916 20:26:04.691308 13475 net.cpp:245] Setting up res2a_branch2a/bn
I0916 20:26:04.691316 13475 net.cpp:252] TEST Top shape for layer 11 'res2a_branch2a/bn' 2 64 160 160 (3276800)
I0916 20:26:04.691324 13475 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0916 20:26:04.691329 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.691334 13475 net.cpp:184] Created Layer res2a_branch2a/relu (12)
I0916 20:26:04.691336 13475 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0916 20:26:04.691340 13475 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0916 20:26:04.691345 13475 net.cpp:245] Setting up res2a_branch2a/relu
I0916 20:26:04.691354 13475 net.cpp:252] TEST Top shape for layer 12 'res2a_branch2a/relu' 2 64 160 160 (3276800)
I0916 20:26:04.691357 13475 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0916 20:26:04.691360 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.691368 13475 net.cpp:184] Created Layer res2a_branch2b (13)
I0916 20:26:04.691382 13475 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0916 20:26:04.691386 13475 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0916 20:26:04.692164 13475 net.cpp:245] Setting up res2a_branch2b
I0916 20:26:04.692174 13475 net.cpp:252] TEST Top shape for layer 13 'res2a_branch2b' 2 64 160 160 (3276800)
I0916 20:26:04.692181 13475 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0916 20:26:04.692185 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.692191 13475 net.cpp:184] Created Layer res2a_branch2b/bn (14)
I0916 20:26:04.692194 13475 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0916 20:26:04.692198 13475 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0916 20:26:04.693068 13475 net.cpp:245] Setting up res2a_branch2b/bn
I0916 20:26:04.693078 13475 net.cpp:252] TEST Top shape for layer 14 'res2a_branch2b/bn' 2 64 160 160 (3276800)
I0916 20:26:04.693085 13475 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0916 20:26:04.693089 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.693099 13475 net.cpp:184] Created Layer res2a_branch2b/relu (15)
I0916 20:26:04.693101 13475 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0916 20:26:04.693105 13475 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0916 20:26:04.693111 13475 net.cpp:245] Setting up res2a_branch2b/relu
I0916 20:26:04.693116 13475 net.cpp:252] TEST Top shape for layer 15 'res2a_branch2b/relu' 2 64 160 160 (3276800)
I0916 20:26:04.693120 13475 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0916 20:26:04.693125 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.693130 13475 net.cpp:184] Created Layer pool2 (16)
I0916 20:26:04.693135 13475 net.cpp:561] pool2 <- res2a_branch2b
I0916 20:26:04.693138 13475 net.cpp:530] pool2 -> pool2
I0916 20:26:04.693223 13475 net.cpp:245] Setting up pool2
I0916 20:26:04.693229 13475 net.cpp:252] TEST Top shape for layer 16 'pool2' 2 64 80 80 (819200)
I0916 20:26:04.693233 13475 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0916 20:26:04.693236 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.693243 13475 net.cpp:184] Created Layer res3a_branch2a (17)
I0916 20:26:04.693246 13475 net.cpp:561] res3a_branch2a <- pool2
I0916 20:26:04.693249 13475 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0916 20:26:04.695817 13475 net.cpp:245] Setting up res3a_branch2a
I0916 20:26:04.695827 13475 net.cpp:252] TEST Top shape for layer 17 'res3a_branch2a' 2 128 80 80 (1638400)
I0916 20:26:04.695833 13475 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0916 20:26:04.695837 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.695843 13475 net.cpp:184] Created Layer res3a_branch2a/bn (18)
I0916 20:26:04.695847 13475 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0916 20:26:04.695850 13475 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0916 20:26:04.696707 13475 net.cpp:245] Setting up res3a_branch2a/bn
I0916 20:26:04.696717 13475 net.cpp:252] TEST Top shape for layer 18 'res3a_branch2a/bn' 2 128 80 80 (1638400)
I0916 20:26:04.696727 13475 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0916 20:26:04.696732 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.696735 13475 net.cpp:184] Created Layer res3a_branch2a/relu (19)
I0916 20:26:04.696738 13475 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0916 20:26:04.696743 13475 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0916 20:26:04.696748 13475 net.cpp:245] Setting up res3a_branch2a/relu
I0916 20:26:04.696753 13475 net.cpp:252] TEST Top shape for layer 19 'res3a_branch2a/relu' 2 128 80 80 (1638400)
I0916 20:26:04.696764 13475 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0916 20:26:04.696768 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.696786 13475 net.cpp:184] Created Layer res3a_branch2b (20)
I0916 20:26:04.696790 13475 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0916 20:26:04.696794 13475 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0916 20:26:04.698109 13475 net.cpp:245] Setting up res3a_branch2b
I0916 20:26:04.698117 13475 net.cpp:252] TEST Top shape for layer 20 'res3a_branch2b' 2 128 80 80 (1638400)
I0916 20:26:04.698122 13475 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0916 20:26:04.698124 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.698128 13475 net.cpp:184] Created Layer res3a_branch2b/bn (21)
I0916 20:26:04.698130 13475 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0916 20:26:04.698133 13475 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0916 20:26:04.698768 13475 net.cpp:245] Setting up res3a_branch2b/bn
I0916 20:26:04.698776 13475 net.cpp:252] TEST Top shape for layer 21 'res3a_branch2b/bn' 2 128 80 80 (1638400)
I0916 20:26:04.698781 13475 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0916 20:26:04.698783 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.698786 13475 net.cpp:184] Created Layer res3a_branch2b/relu (22)
I0916 20:26:04.698788 13475 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0916 20:26:04.698791 13475 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0916 20:26:04.698793 13475 net.cpp:245] Setting up res3a_branch2b/relu
I0916 20:26:04.698796 13475 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b/relu' 2 128 80 80 (1638400)
I0916 20:26:04.698797 13475 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0916 20:26:04.698801 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.698803 13475 net.cpp:184] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (23)
I0916 20:26:04.698806 13475 net.cpp:561] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0916 20:26:04.698808 13475 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0916 20:26:04.698812 13475 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0916 20:26:04.698859 13475 net.cpp:245] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0916 20:26:04.698863 13475 net.cpp:252] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 2 128 80 80 (1638400)
I0916 20:26:04.698868 13475 net.cpp:252] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 2 128 80 80 (1638400)
I0916 20:26:04.698869 13475 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0916 20:26:04.698873 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.698876 13475 net.cpp:184] Created Layer pool3 (24)
I0916 20:26:04.698879 13475 net.cpp:561] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0916 20:26:04.698882 13475 net.cpp:530] pool3 -> pool3
I0916 20:26:04.698941 13475 net.cpp:245] Setting up pool3
I0916 20:26:04.698945 13475 net.cpp:252] TEST Top shape for layer 24 'pool3' 2 128 40 40 (409600)
I0916 20:26:04.698948 13475 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0916 20:26:04.698951 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.698957 13475 net.cpp:184] Created Layer res4a_branch2a (25)
I0916 20:26:04.698961 13475 net.cpp:561] res4a_branch2a <- pool3
I0916 20:26:04.698963 13475 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0916 20:26:04.705145 13475 net.cpp:245] Setting up res4a_branch2a
I0916 20:26:04.705160 13475 net.cpp:252] TEST Top shape for layer 25 'res4a_branch2a' 2 256 40 40 (819200)
I0916 20:26:04.705165 13475 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0916 20:26:04.705168 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.705173 13475 net.cpp:184] Created Layer res4a_branch2a/bn (26)
I0916 20:26:04.705175 13475 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0916 20:26:04.705178 13475 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0916 20:26:04.705818 13475 net.cpp:245] Setting up res4a_branch2a/bn
I0916 20:26:04.705826 13475 net.cpp:252] TEST Top shape for layer 26 'res4a_branch2a/bn' 2 256 40 40 (819200)
I0916 20:26:04.705832 13475 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0916 20:26:04.705834 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.705838 13475 net.cpp:184] Created Layer res4a_branch2a/relu (27)
I0916 20:26:04.705840 13475 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0916 20:26:04.705843 13475 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0916 20:26:04.705847 13475 net.cpp:245] Setting up res4a_branch2a/relu
I0916 20:26:04.705850 13475 net.cpp:252] TEST Top shape for layer 27 'res4a_branch2a/relu' 2 256 40 40 (819200)
I0916 20:26:04.705852 13475 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0916 20:26:04.705855 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.705864 13475 net.cpp:184] Created Layer res4a_branch2b (28)
I0916 20:26:04.705868 13475 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0916 20:26:04.705870 13475 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0916 20:26:04.710131 13475 net.cpp:245] Setting up res4a_branch2b
I0916 20:26:04.710141 13475 net.cpp:252] TEST Top shape for layer 28 'res4a_branch2b' 2 256 40 40 (819200)
I0916 20:26:04.710146 13475 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0916 20:26:04.710150 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.710153 13475 net.cpp:184] Created Layer res4a_branch2b/bn (29)
I0916 20:26:04.710156 13475 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0916 20:26:04.710160 13475 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0916 20:26:04.710789 13475 net.cpp:245] Setting up res4a_branch2b/bn
I0916 20:26:04.710796 13475 net.cpp:252] TEST Top shape for layer 29 'res4a_branch2b/bn' 2 256 40 40 (819200)
I0916 20:26:04.710801 13475 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0916 20:26:04.710804 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.710808 13475 net.cpp:184] Created Layer res4a_branch2b/relu (30)
I0916 20:26:04.710809 13475 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0916 20:26:04.710813 13475 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0916 20:26:04.710815 13475 net.cpp:245] Setting up res4a_branch2b/relu
I0916 20:26:04.710819 13475 net.cpp:252] TEST Top shape for layer 30 'res4a_branch2b/relu' 2 256 40 40 (819200)
I0916 20:26:04.710820 13475 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0916 20:26:04.710824 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.710827 13475 net.cpp:184] Created Layer pool4 (31)
I0916 20:26:04.710829 13475 net.cpp:561] pool4 <- res4a_branch2b
I0916 20:26:04.710832 13475 net.cpp:530] pool4 -> pool4
I0916 20:26:04.710897 13475 net.cpp:245] Setting up pool4
I0916 20:26:04.710902 13475 net.cpp:252] TEST Top shape for layer 31 'pool4' 2 256 40 40 (819200)
I0916 20:26:04.710904 13475 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0916 20:26:04.710907 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.710921 13475 net.cpp:184] Created Layer res5a_branch2a (32)
I0916 20:26:04.710923 13475 net.cpp:561] res5a_branch2a <- pool4
I0916 20:26:04.710927 13475 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0916 20:26:04.738029 13475 net.cpp:245] Setting up res5a_branch2a
I0916 20:26:04.738050 13475 net.cpp:252] TEST Top shape for layer 32 'res5a_branch2a' 2 512 40 40 (1638400)
I0916 20:26:04.738056 13475 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0916 20:26:04.738060 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.738068 13475 net.cpp:184] Created Layer res5a_branch2a/bn (33)
I0916 20:26:04.738075 13475 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0916 20:26:04.738078 13475 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0916 20:26:04.738735 13475 net.cpp:245] Setting up res5a_branch2a/bn
I0916 20:26:04.738742 13475 net.cpp:252] TEST Top shape for layer 33 'res5a_branch2a/bn' 2 512 40 40 (1638400)
I0916 20:26:04.738749 13475 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0916 20:26:04.738752 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.738756 13475 net.cpp:184] Created Layer res5a_branch2a/relu (34)
I0916 20:26:04.738759 13475 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0916 20:26:04.738760 13475 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0916 20:26:04.738765 13475 net.cpp:245] Setting up res5a_branch2a/relu
I0916 20:26:04.738767 13475 net.cpp:252] TEST Top shape for layer 34 'res5a_branch2a/relu' 2 512 40 40 (1638400)
I0916 20:26:04.738770 13475 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0916 20:26:04.738771 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.738777 13475 net.cpp:184] Created Layer res5a_branch2b (35)
I0916 20:26:04.738780 13475 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0916 20:26:04.738782 13475 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0916 20:26:04.751868 13475 net.cpp:245] Setting up res5a_branch2b
I0916 20:26:04.751883 13475 net.cpp:252] TEST Top shape for layer 35 'res5a_branch2b' 2 512 40 40 (1638400)
I0916 20:26:04.751891 13475 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0916 20:26:04.751894 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.751899 13475 net.cpp:184] Created Layer res5a_branch2b/bn (36)
I0916 20:26:04.751902 13475 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0916 20:26:04.751904 13475 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0916 20:26:04.752605 13475 net.cpp:245] Setting up res5a_branch2b/bn
I0916 20:26:04.752614 13475 net.cpp:252] TEST Top shape for layer 36 'res5a_branch2b/bn' 2 512 40 40 (1638400)
I0916 20:26:04.752620 13475 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0916 20:26:04.752624 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.752626 13475 net.cpp:184] Created Layer res5a_branch2b/relu (37)
I0916 20:26:04.752629 13475 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0916 20:26:04.752631 13475 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0916 20:26:04.752635 13475 net.cpp:245] Setting up res5a_branch2b/relu
I0916 20:26:04.752637 13475 net.cpp:252] TEST Top shape for layer 37 'res5a_branch2b/relu' 2 512 40 40 (1638400)
I0916 20:26:04.752640 13475 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I0916 20:26:04.752642 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.752651 13475 net.cpp:184] Created Layer out5a (38)
I0916 20:26:04.752655 13475 net.cpp:561] out5a <- res5a_branch2b
I0916 20:26:04.752657 13475 net.cpp:530] out5a -> out5a
I0916 20:26:04.755935 13475 net.cpp:245] Setting up out5a
I0916 20:26:04.755950 13475 net.cpp:252] TEST Top shape for layer 38 'out5a' 2 64 40 40 (204800)
I0916 20:26:04.755954 13475 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I0916 20:26:04.755957 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.755962 13475 net.cpp:184] Created Layer out5a/bn (39)
I0916 20:26:04.755964 13475 net.cpp:561] out5a/bn <- out5a
I0916 20:26:04.755967 13475 net.cpp:513] out5a/bn -> out5a (in-place)
I0916 20:26:04.756628 13475 net.cpp:245] Setting up out5a/bn
I0916 20:26:04.756634 13475 net.cpp:252] TEST Top shape for layer 39 'out5a/bn' 2 64 40 40 (204800)
I0916 20:26:04.756639 13475 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I0916 20:26:04.756641 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.756644 13475 net.cpp:184] Created Layer out5a/relu (40)
I0916 20:26:04.756646 13475 net.cpp:561] out5a/relu <- out5a
I0916 20:26:04.756649 13475 net.cpp:513] out5a/relu -> out5a (in-place)
I0916 20:26:04.756651 13475 net.cpp:245] Setting up out5a/relu
I0916 20:26:04.756654 13475 net.cpp:252] TEST Top shape for layer 40 'out5a/relu' 2 64 40 40 (204800)
I0916 20:26:04.756656 13475 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I0916 20:26:04.756659 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.756664 13475 net.cpp:184] Created Layer out5a_up2 (41)
I0916 20:26:04.756665 13475 net.cpp:561] out5a_up2 <- out5a
I0916 20:26:04.756667 13475 net.cpp:530] out5a_up2 -> out5a_up2
I0916 20:26:04.756959 13475 net.cpp:245] Setting up out5a_up2
I0916 20:26:04.756965 13475 net.cpp:252] TEST Top shape for layer 41 'out5a_up2' 2 64 80 80 (819200)
I0916 20:26:04.756968 13475 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I0916 20:26:04.756970 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.756978 13475 net.cpp:184] Created Layer out3a (42)
I0916 20:26:04.756981 13475 net.cpp:561] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0916 20:26:04.756983 13475 net.cpp:530] out3a -> out3a
I0916 20:26:04.758087 13475 net.cpp:245] Setting up out3a
I0916 20:26:04.758095 13475 net.cpp:252] TEST Top shape for layer 42 'out3a' 2 64 80 80 (819200)
I0916 20:26:04.758098 13475 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I0916 20:26:04.758101 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.758105 13475 net.cpp:184] Created Layer out3a/bn (43)
I0916 20:26:04.758107 13475 net.cpp:561] out3a/bn <- out3a
I0916 20:26:04.758111 13475 net.cpp:513] out3a/bn -> out3a (in-place)
I0916 20:26:04.758782 13475 net.cpp:245] Setting up out3a/bn
I0916 20:26:04.758790 13475 net.cpp:252] TEST Top shape for layer 43 'out3a/bn' 2 64 80 80 (819200)
I0916 20:26:04.758795 13475 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I0916 20:26:04.758796 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.758800 13475 net.cpp:184] Created Layer out3a/relu (44)
I0916 20:26:04.758801 13475 net.cpp:561] out3a/relu <- out3a
I0916 20:26:04.758803 13475 net.cpp:513] out3a/relu -> out3a (in-place)
I0916 20:26:04.758807 13475 net.cpp:245] Setting up out3a/relu
I0916 20:26:04.758810 13475 net.cpp:252] TEST Top shape for layer 44 'out3a/relu' 2 64 80 80 (819200)
I0916 20:26:04.758811 13475 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I0916 20:26:04.758813 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.758817 13475 net.cpp:184] Created Layer out3_out5_combined (45)
I0916 20:26:04.758819 13475 net.cpp:561] out3_out5_combined <- out5a_up2
I0916 20:26:04.758821 13475 net.cpp:561] out3_out5_combined <- out3a
I0916 20:26:04.758823 13475 net.cpp:530] out3_out5_combined -> out3_out5_combined
I0916 20:26:04.758855 13475 net.cpp:245] Setting up out3_out5_combined
I0916 20:26:04.758860 13475 net.cpp:252] TEST Top shape for layer 45 'out3_out5_combined' 2 64 80 80 (819200)
I0916 20:26:04.758862 13475 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I0916 20:26:04.758864 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.758870 13475 net.cpp:184] Created Layer ctx_conv1 (46)
I0916 20:26:04.758872 13475 net.cpp:561] ctx_conv1 <- out3_out5_combined
I0916 20:26:04.758875 13475 net.cpp:530] ctx_conv1 -> ctx_conv1
I0916 20:26:04.759973 13475 net.cpp:245] Setting up ctx_conv1
I0916 20:26:04.759979 13475 net.cpp:252] TEST Top shape for layer 46 'ctx_conv1' 2 64 80 80 (819200)
I0916 20:26:04.759984 13475 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I0916 20:26:04.759985 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.759999 13475 net.cpp:184] Created Layer ctx_conv1/bn (47)
I0916 20:26:04.760001 13475 net.cpp:561] ctx_conv1/bn <- ctx_conv1
I0916 20:26:04.760004 13475 net.cpp:513] ctx_conv1/bn -> ctx_conv1 (in-place)
I0916 20:26:04.760658 13475 net.cpp:245] Setting up ctx_conv1/bn
I0916 20:26:04.760665 13475 net.cpp:252] TEST Top shape for layer 47 'ctx_conv1/bn' 2 64 80 80 (819200)
I0916 20:26:04.760670 13475 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I0916 20:26:04.760673 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.760675 13475 net.cpp:184] Created Layer ctx_conv1/relu (48)
I0916 20:26:04.760677 13475 net.cpp:561] ctx_conv1/relu <- ctx_conv1
I0916 20:26:04.760680 13475 net.cpp:513] ctx_conv1/relu -> ctx_conv1 (in-place)
I0916 20:26:04.760684 13475 net.cpp:245] Setting up ctx_conv1/relu
I0916 20:26:04.760685 13475 net.cpp:252] TEST Top shape for layer 48 'ctx_conv1/relu' 2 64 80 80 (819200)
I0916 20:26:04.760687 13475 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I0916 20:26:04.760689 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.760694 13475 net.cpp:184] Created Layer ctx_conv2 (49)
I0916 20:26:04.760696 13475 net.cpp:561] ctx_conv2 <- ctx_conv1
I0916 20:26:04.760699 13475 net.cpp:530] ctx_conv2 -> ctx_conv2
I0916 20:26:04.761792 13475 net.cpp:245] Setting up ctx_conv2
I0916 20:26:04.761798 13475 net.cpp:252] TEST Top shape for layer 49 'ctx_conv2' 2 64 80 80 (819200)
I0916 20:26:04.761802 13475 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I0916 20:26:04.761804 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.761808 13475 net.cpp:184] Created Layer ctx_conv2/bn (50)
I0916 20:26:04.761811 13475 net.cpp:561] ctx_conv2/bn <- ctx_conv2
I0916 20:26:04.761812 13475 net.cpp:513] ctx_conv2/bn -> ctx_conv2 (in-place)
I0916 20:26:04.762473 13475 net.cpp:245] Setting up ctx_conv2/bn
I0916 20:26:04.762481 13475 net.cpp:252] TEST Top shape for layer 50 'ctx_conv2/bn' 2 64 80 80 (819200)
I0916 20:26:04.762486 13475 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I0916 20:26:04.762488 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.762491 13475 net.cpp:184] Created Layer ctx_conv2/relu (51)
I0916 20:26:04.762493 13475 net.cpp:561] ctx_conv2/relu <- ctx_conv2
I0916 20:26:04.762495 13475 net.cpp:513] ctx_conv2/relu -> ctx_conv2 (in-place)
I0916 20:26:04.762498 13475 net.cpp:245] Setting up ctx_conv2/relu
I0916 20:26:04.762501 13475 net.cpp:252] TEST Top shape for layer 51 'ctx_conv2/relu' 2 64 80 80 (819200)
I0916 20:26:04.762503 13475 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I0916 20:26:04.762506 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.762511 13475 net.cpp:184] Created Layer ctx_conv3 (52)
I0916 20:26:04.762512 13475 net.cpp:561] ctx_conv3 <- ctx_conv2
I0916 20:26:04.762521 13475 net.cpp:530] ctx_conv3 -> ctx_conv3
I0916 20:26:04.763665 13475 net.cpp:245] Setting up ctx_conv3
I0916 20:26:04.763672 13475 net.cpp:252] TEST Top shape for layer 52 'ctx_conv3' 2 64 80 80 (819200)
I0916 20:26:04.763676 13475 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I0916 20:26:04.763679 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.763687 13475 net.cpp:184] Created Layer ctx_conv3/bn (53)
I0916 20:26:04.763689 13475 net.cpp:561] ctx_conv3/bn <- ctx_conv3
I0916 20:26:04.763691 13475 net.cpp:513] ctx_conv3/bn -> ctx_conv3 (in-place)
I0916 20:26:04.764358 13475 net.cpp:245] Setting up ctx_conv3/bn
I0916 20:26:04.764365 13475 net.cpp:252] TEST Top shape for layer 53 'ctx_conv3/bn' 2 64 80 80 (819200)
I0916 20:26:04.764370 13475 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I0916 20:26:04.764374 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.764376 13475 net.cpp:184] Created Layer ctx_conv3/relu (54)
I0916 20:26:04.764379 13475 net.cpp:561] ctx_conv3/relu <- ctx_conv3
I0916 20:26:04.764380 13475 net.cpp:513] ctx_conv3/relu -> ctx_conv3 (in-place)
I0916 20:26:04.764384 13475 net.cpp:245] Setting up ctx_conv3/relu
I0916 20:26:04.764385 13475 net.cpp:252] TEST Top shape for layer 54 'ctx_conv3/relu' 2 64 80 80 (819200)
I0916 20:26:04.764387 13475 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I0916 20:26:04.764389 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.764394 13475 net.cpp:184] Created Layer ctx_conv4 (55)
I0916 20:26:04.764396 13475 net.cpp:561] ctx_conv4 <- ctx_conv3
I0916 20:26:04.764398 13475 net.cpp:530] ctx_conv4 -> ctx_conv4
I0916 20:26:04.765486 13475 net.cpp:245] Setting up ctx_conv4
I0916 20:26:04.765493 13475 net.cpp:252] TEST Top shape for layer 55 'ctx_conv4' 2 64 80 80 (819200)
I0916 20:26:04.765496 13475 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I0916 20:26:04.765499 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.765502 13475 net.cpp:184] Created Layer ctx_conv4/bn (56)
I0916 20:26:04.765506 13475 net.cpp:561] ctx_conv4/bn <- ctx_conv4
I0916 20:26:04.765507 13475 net.cpp:513] ctx_conv4/bn -> ctx_conv4 (in-place)
I0916 20:26:04.766186 13475 net.cpp:245] Setting up ctx_conv4/bn
I0916 20:26:04.766194 13475 net.cpp:252] TEST Top shape for layer 56 'ctx_conv4/bn' 2 64 80 80 (819200)
I0916 20:26:04.766199 13475 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I0916 20:26:04.766201 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.766204 13475 net.cpp:184] Created Layer ctx_conv4/relu (57)
I0916 20:26:04.766206 13475 net.cpp:561] ctx_conv4/relu <- ctx_conv4
I0916 20:26:04.766208 13475 net.cpp:513] ctx_conv4/relu -> ctx_conv4 (in-place)
I0916 20:26:04.766212 13475 net.cpp:245] Setting up ctx_conv4/relu
I0916 20:26:04.766214 13475 net.cpp:252] TEST Top shape for layer 57 'ctx_conv4/relu' 2 64 80 80 (819200)
I0916 20:26:04.766216 13475 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I0916 20:26:04.766218 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.766223 13475 net.cpp:184] Created Layer ctx_final (58)
I0916 20:26:04.766225 13475 net.cpp:561] ctx_final <- ctx_conv4
I0916 20:26:04.766227 13475 net.cpp:530] ctx_final -> ctx_final
I0916 20:26:04.766691 13475 net.cpp:245] Setting up ctx_final
I0916 20:26:04.766698 13475 net.cpp:252] TEST Top shape for layer 58 'ctx_final' 2 8 80 80 (102400)
I0916 20:26:04.766702 13475 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I0916 20:26:04.766705 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.766707 13475 net.cpp:184] Created Layer ctx_final/relu (59)
I0916 20:26:04.766715 13475 net.cpp:561] ctx_final/relu <- ctx_final
I0916 20:26:04.766717 13475 net.cpp:513] ctx_final/relu -> ctx_final (in-place)
I0916 20:26:04.766721 13475 net.cpp:245] Setting up ctx_final/relu
I0916 20:26:04.766723 13475 net.cpp:252] TEST Top shape for layer 59 'ctx_final/relu' 2 8 80 80 (102400)
I0916 20:26:04.766726 13475 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I0916 20:26:04.766727 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.766736 13475 net.cpp:184] Created Layer out_deconv_final_up2 (60)
I0916 20:26:04.766739 13475 net.cpp:561] out_deconv_final_up2 <- ctx_final
I0916 20:26:04.766741 13475 net.cpp:530] out_deconv_final_up2 -> out_deconv_final_up2
I0916 20:26:04.767012 13475 net.cpp:245] Setting up out_deconv_final_up2
I0916 20:26:04.767017 13475 net.cpp:252] TEST Top shape for layer 60 'out_deconv_final_up2' 2 8 160 160 (409600)
I0916 20:26:04.767020 13475 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I0916 20:26:04.767022 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.767026 13475 net.cpp:184] Created Layer out_deconv_final_up4 (61)
I0916 20:26:04.767029 13475 net.cpp:561] out_deconv_final_up4 <- out_deconv_final_up2
I0916 20:26:04.767031 13475 net.cpp:530] out_deconv_final_up4 -> out_deconv_final_up4
I0916 20:26:04.767308 13475 net.cpp:245] Setting up out_deconv_final_up4
I0916 20:26:04.767313 13475 net.cpp:252] TEST Top shape for layer 61 'out_deconv_final_up4' 2 8 320 320 (1638400)
I0916 20:26:04.767316 13475 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I0916 20:26:04.767318 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.767323 13475 net.cpp:184] Created Layer out_deconv_final_up8 (62)
I0916 20:26:04.767324 13475 net.cpp:561] out_deconv_final_up8 <- out_deconv_final_up4
I0916 20:26:04.767326 13475 net.cpp:530] out_deconv_final_up8 -> out_deconv_final_up8
I0916 20:26:04.767598 13475 net.cpp:245] Setting up out_deconv_final_up8
I0916 20:26:04.767603 13475 net.cpp:252] TEST Top shape for layer 62 'out_deconv_final_up8' 2 8 640 640 (6553600)
I0916 20:26:04.767606 13475 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8_out_deconv_final_up8_0_split' of type 'Split'
I0916 20:26:04.767608 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.767611 13475 net.cpp:184] Created Layer out_deconv_final_up8_out_deconv_final_up8_0_split (63)
I0916 20:26:04.767613 13475 net.cpp:561] out_deconv_final_up8_out_deconv_final_up8_0_split <- out_deconv_final_up8
I0916 20:26:04.767616 13475 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0916 20:26:04.767618 13475 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0916 20:26:04.767621 13475 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0916 20:26:04.767683 13475 net.cpp:245] Setting up out_deconv_final_up8_out_deconv_final_up8_0_split
I0916 20:26:04.767686 13475 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 8 640 640 (6553600)
I0916 20:26:04.767688 13475 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 8 640 640 (6553600)
I0916 20:26:04.767690 13475 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 8 640 640 (6553600)
I0916 20:26:04.767693 13475 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0916 20:26:04.767695 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.767704 13475 net.cpp:184] Created Layer loss (64)
I0916 20:26:04.767712 13475 net.cpp:561] loss <- out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0916 20:26:04.767714 13475 net.cpp:561] loss <- label_data_1_split_0
I0916 20:26:04.767717 13475 net.cpp:530] loss -> loss
I0916 20:26:04.768613 13475 net.cpp:245] Setting up loss
I0916 20:26:04.768621 13475 net.cpp:252] TEST Top shape for layer 64 'loss' (1)
I0916 20:26:04.768625 13475 net.cpp:256]     with loss weight 1
I0916 20:26:04.768630 13475 layer_factory.hpp:136] Creating layer 'accuracy/top1' of type 'Accuracy'
I0916 20:26:04.768632 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.768642 13475 net.cpp:184] Created Layer accuracy/top1 (65)
I0916 20:26:04.768645 13475 net.cpp:561] accuracy/top1 <- out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0916 20:26:04.768647 13475 net.cpp:561] accuracy/top1 <- label_data_1_split_1
I0916 20:26:04.768651 13475 net.cpp:530] accuracy/top1 -> accuracy/top1
I0916 20:26:04.768656 13475 net.cpp:245] Setting up accuracy/top1
I0916 20:26:04.768658 13475 net.cpp:252] TEST Top shape for layer 65 'accuracy/top1' (1)
I0916 20:26:04.768661 13475 layer_factory.hpp:136] Creating layer 'accuracy/top5' of type 'Accuracy'
I0916 20:26:04.768662 13475 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 20:26:04.768666 13475 net.cpp:184] Created Layer accuracy/top5 (66)
I0916 20:26:04.768667 13475 net.cpp:561] accuracy/top5 <- out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0916 20:26:04.768671 13475 net.cpp:561] accuracy/top5 <- label_data_1_split_2
I0916 20:26:04.768673 13475 net.cpp:530] accuracy/top5 -> accuracy/top5
I0916 20:26:04.768682 13475 net.cpp:245] Setting up accuracy/top5
I0916 20:26:04.768685 13475 net.cpp:252] TEST Top shape for layer 66 'accuracy/top5' (1)
I0916 20:26:04.768687 13475 net.cpp:325] accuracy/top5 does not need backward computation.
I0916 20:26:04.768689 13475 net.cpp:325] accuracy/top1 does not need backward computation.
I0916 20:26:04.768692 13475 net.cpp:323] loss needs backward computation.
I0916 20:26:04.768693 13475 net.cpp:323] out_deconv_final_up8_out_deconv_final_up8_0_split needs backward computation.
I0916 20:26:04.768695 13475 net.cpp:323] out_deconv_final_up8 needs backward computation.
I0916 20:26:04.768697 13475 net.cpp:323] out_deconv_final_up4 needs backward computation.
I0916 20:26:04.768699 13475 net.cpp:323] out_deconv_final_up2 needs backward computation.
I0916 20:26:04.768702 13475 net.cpp:323] ctx_final/relu needs backward computation.
I0916 20:26:04.768703 13475 net.cpp:323] ctx_final needs backward computation.
I0916 20:26:04.768704 13475 net.cpp:323] ctx_conv4/relu needs backward computation.
I0916 20:26:04.768707 13475 net.cpp:323] ctx_conv4/bn needs backward computation.
I0916 20:26:04.768708 13475 net.cpp:323] ctx_conv4 needs backward computation.
I0916 20:26:04.768710 13475 net.cpp:323] ctx_conv3/relu needs backward computation.
I0916 20:26:04.768712 13475 net.cpp:323] ctx_conv3/bn needs backward computation.
I0916 20:26:04.768713 13475 net.cpp:323] ctx_conv3 needs backward computation.
I0916 20:26:04.768715 13475 net.cpp:323] ctx_conv2/relu needs backward computation.
I0916 20:26:04.768717 13475 net.cpp:323] ctx_conv2/bn needs backward computation.
I0916 20:26:04.768718 13475 net.cpp:323] ctx_conv2 needs backward computation.
I0916 20:26:04.768720 13475 net.cpp:323] ctx_conv1/relu needs backward computation.
I0916 20:26:04.768723 13475 net.cpp:323] ctx_conv1/bn needs backward computation.
I0916 20:26:04.768725 13475 net.cpp:323] ctx_conv1 needs backward computation.
I0916 20:26:04.768728 13475 net.cpp:323] out3_out5_combined needs backward computation.
I0916 20:26:04.768730 13475 net.cpp:323] out3a/relu needs backward computation.
I0916 20:26:04.768733 13475 net.cpp:323] out3a/bn needs backward computation.
I0916 20:26:04.768735 13475 net.cpp:323] out3a needs backward computation.
I0916 20:26:04.768738 13475 net.cpp:323] out5a_up2 needs backward computation.
I0916 20:26:04.768739 13475 net.cpp:323] out5a/relu needs backward computation.
I0916 20:26:04.768748 13475 net.cpp:323] out5a/bn needs backward computation.
I0916 20:26:04.768749 13475 net.cpp:323] out5a needs backward computation.
I0916 20:26:04.768752 13475 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0916 20:26:04.768754 13475 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0916 20:26:04.768756 13475 net.cpp:323] res5a_branch2b needs backward computation.
I0916 20:26:04.768759 13475 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0916 20:26:04.768760 13475 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0916 20:26:04.768764 13475 net.cpp:323] res5a_branch2a needs backward computation.
I0916 20:26:04.768765 13475 net.cpp:323] pool4 needs backward computation.
I0916 20:26:04.768767 13475 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0916 20:26:04.768770 13475 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0916 20:26:04.768772 13475 net.cpp:323] res4a_branch2b needs backward computation.
I0916 20:26:04.768775 13475 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0916 20:26:04.768777 13475 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0916 20:26:04.768780 13475 net.cpp:323] res4a_branch2a needs backward computation.
I0916 20:26:04.768784 13475 net.cpp:323] pool3 needs backward computation.
I0916 20:26:04.768785 13475 net.cpp:323] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I0916 20:26:04.768788 13475 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0916 20:26:04.768790 13475 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0916 20:26:04.768792 13475 net.cpp:323] res3a_branch2b needs backward computation.
I0916 20:26:04.768795 13475 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0916 20:26:04.768797 13475 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0916 20:26:04.768800 13475 net.cpp:323] res3a_branch2a needs backward computation.
I0916 20:26:04.768801 13475 net.cpp:323] pool2 needs backward computation.
I0916 20:26:04.768805 13475 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0916 20:26:04.768807 13475 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0916 20:26:04.768810 13475 net.cpp:323] res2a_branch2b needs backward computation.
I0916 20:26:04.768811 13475 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0916 20:26:04.768813 13475 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0916 20:26:04.768815 13475 net.cpp:323] res2a_branch2a needs backward computation.
I0916 20:26:04.768818 13475 net.cpp:323] pool1 needs backward computation.
I0916 20:26:04.768821 13475 net.cpp:323] conv1b/relu needs backward computation.
I0916 20:26:04.768823 13475 net.cpp:323] conv1b/bn needs backward computation.
I0916 20:26:04.768826 13475 net.cpp:323] conv1b needs backward computation.
I0916 20:26:04.768828 13475 net.cpp:323] conv1a/relu needs backward computation.
I0916 20:26:04.768831 13475 net.cpp:323] conv1a/bn needs backward computation.
I0916 20:26:04.768832 13475 net.cpp:323] conv1a needs backward computation.
I0916 20:26:04.768838 13475 net.cpp:325] data/bias does not need backward computation.
I0916 20:26:04.768841 13475 net.cpp:325] label_data_1_split does not need backward computation.
I0916 20:26:04.768844 13475 net.cpp:325] data does not need backward computation.
I0916 20:26:04.768846 13475 net.cpp:367] This network produces output accuracy/top1
I0916 20:26:04.768848 13475 net.cpp:367] This network produces output accuracy/top5
I0916 20:26:04.768851 13475 net.cpp:367] This network produces output loss
I0916 20:26:04.768887 13475 net.cpp:389] Top memory (TEST) required for data: 566886424 diff: 566886424
I0916 20:26:04.768892 13475 net.cpp:392] Bottom memory (TEST) required for data: 566886400 diff: 566886400
I0916 20:26:04.768893 13475 net.cpp:395] Shared (in-place) memory (TEST) by data: 257638400 diff: 257638400
I0916 20:26:04.768896 13475 net.cpp:398] Parameters memory (TEST) required for data: 10817840 diff: 10817840
I0916 20:26:04.768898 13475 net.cpp:401] Parameters shared memory (TEST) by data: 0 diff: 0
I0916 20:26:04.768904 13475 net.cpp:407] Network initialization done.
I0916 20:26:04.768973 13475 solver.cpp:57] Solver scaffolding done.
I0916 20:26:04.777775 13475 caffe.cpp:143] Finetuning from training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/l1reg/cityscapes5_jsegnet21v2_iter_60000.caffemodel
I0916 20:26:04.782775 13475 net.cpp:1094] Copying source layer data Type:ImageLabelData #blobs=0
I0916 20:26:04.782794 13475 net.cpp:1094] Copying source layer data/bias Type:Bias #blobs=1
I0916 20:26:04.782824 13475 net.cpp:1094] Copying source layer conv1a Type:Convolution #blobs=2
I0916 20:26:04.782836 13475 net.cpp:1094] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0916 20:26:04.783092 13475 net.cpp:1094] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0916 20:26:04.783095 13475 net.cpp:1094] Copying source layer conv1b Type:Convolution #blobs=2
I0916 20:26:04.783104 13475 net.cpp:1094] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0916 20:26:04.783252 13475 net.cpp:1094] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0916 20:26:04.783255 13475 net.cpp:1094] Copying source layer pool1 Type:Pooling #blobs=0
I0916 20:26:04.783257 13475 net.cpp:1094] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0916 20:26:04.783272 13475 net.cpp:1094] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0916 20:26:04.783426 13475 net.cpp:1094] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0916 20:26:04.783430 13475 net.cpp:1094] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0916 20:26:04.783442 13475 net.cpp:1094] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0916 20:26:04.783582 13475 net.cpp:1094] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0916 20:26:04.783586 13475 net.cpp:1094] Copying source layer pool2 Type:Pooling #blobs=0
I0916 20:26:04.783588 13475 net.cpp:1094] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0916 20:26:04.783625 13475 net.cpp:1094] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0916 20:26:04.783761 13475 net.cpp:1094] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0916 20:26:04.783764 13475 net.cpp:1094] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0916 20:26:04.783785 13475 net.cpp:1094] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0916 20:26:04.783907 13475 net.cpp:1094] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0916 20:26:04.783911 13475 net.cpp:1094] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I0916 20:26:04.783913 13475 net.cpp:1094] Copying source layer pool3 Type:Pooling #blobs=0
I0916 20:26:04.783915 13475 net.cpp:1094] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0916 20:26:04.784027 13475 net.cpp:1094] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0916 20:26:04.784148 13475 net.cpp:1094] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0916 20:26:04.784153 13475 net.cpp:1094] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0916 20:26:04.784210 13475 net.cpp:1094] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0916 20:26:04.784332 13475 net.cpp:1094] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0916 20:26:04.784335 13475 net.cpp:1094] Copying source layer pool4 Type:Pooling #blobs=0
I0916 20:26:04.784337 13475 net.cpp:1094] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0916 20:26:04.784700 13475 net.cpp:1094] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0916 20:26:04.784823 13475 net.cpp:1094] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0916 20:26:04.784827 13475 net.cpp:1094] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0916 20:26:04.784993 13475 net.cpp:1094] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0916 20:26:04.785109 13475 net.cpp:1094] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0916 20:26:04.785122 13475 net.cpp:1094] Copying source layer out5a Type:Convolution #blobs=2
I0916 20:26:04.785171 13475 net.cpp:1094] Copying source layer out5a/bn Type:BatchNorm #blobs=5
I0916 20:26:04.785323 13475 net.cpp:1094] Copying source layer out5a/relu Type:ReLU #blobs=0
I0916 20:26:04.785327 13475 net.cpp:1094] Copying source layer out5a_up2 Type:Deconvolution #blobs=1
I0916 20:26:04.785333 13475 net.cpp:1094] Copying source layer out3a Type:Convolution #blobs=2
I0916 20:26:04.785349 13475 net.cpp:1094] Copying source layer out3a/bn Type:BatchNorm #blobs=5
I0916 20:26:04.785495 13475 net.cpp:1094] Copying source layer out3a/relu Type:ReLU #blobs=0
I0916 20:26:04.785498 13475 net.cpp:1094] Copying source layer out3_out5_combined Type:Eltwise #blobs=0
I0916 20:26:04.785501 13475 net.cpp:1094] Copying source layer ctx_conv1 Type:Convolution #blobs=2
I0916 20:26:04.785516 13475 net.cpp:1094] Copying source layer ctx_conv1/bn Type:BatchNorm #blobs=5
I0916 20:26:04.785660 13475 net.cpp:1094] Copying source layer ctx_conv1/relu Type:ReLU #blobs=0
I0916 20:26:04.785665 13475 net.cpp:1094] Copying source layer ctx_conv2 Type:Convolution #blobs=2
I0916 20:26:04.785681 13475 net.cpp:1094] Copying source layer ctx_conv2/bn Type:BatchNorm #blobs=5
I0916 20:26:04.785823 13475 net.cpp:1094] Copying source layer ctx_conv2/relu Type:ReLU #blobs=0
I0916 20:26:04.785827 13475 net.cpp:1094] Copying source layer ctx_conv3 Type:Convolution #blobs=2
I0916 20:26:04.785845 13475 net.cpp:1094] Copying source layer ctx_conv3/bn Type:BatchNorm #blobs=5
I0916 20:26:04.785989 13475 net.cpp:1094] Copying source layer ctx_conv3/relu Type:ReLU #blobs=0
I0916 20:26:04.785992 13475 net.cpp:1094] Copying source layer ctx_conv4 Type:Convolution #blobs=2
I0916 20:26:04.786007 13475 net.cpp:1094] Copying source layer ctx_conv4/bn Type:BatchNorm #blobs=5
I0916 20:26:04.786160 13475 net.cpp:1094] Copying source layer ctx_conv4/relu Type:ReLU #blobs=0
I0916 20:26:04.786165 13475 net.cpp:1094] Copying source layer ctx_final Type:Convolution #blobs=2
I0916 20:26:04.786175 13475 net.cpp:1094] Copying source layer ctx_final/relu Type:ReLU #blobs=0
I0916 20:26:04.786177 13475 net.cpp:1094] Copying source layer out_deconv_final_up2 Type:Deconvolution #blobs=1
I0916 20:26:04.786182 13475 net.cpp:1094] Copying source layer out_deconv_final_up4 Type:Deconvolution #blobs=1
I0916 20:26:04.786187 13475 net.cpp:1094] Copying source layer out_deconv_final_up8 Type:Deconvolution #blobs=1
I0916 20:26:04.786192 13475 net.cpp:1094] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0916 20:26:04.789723 13475 net.cpp:1094] Copying source layer data Type:ImageLabelData #blobs=0
I0916 20:26:04.789739 13475 net.cpp:1094] Copying source layer data/bias Type:Bias #blobs=1
I0916 20:26:04.789764 13475 net.cpp:1094] Copying source layer conv1a Type:Convolution #blobs=2
I0916 20:26:04.789775 13475 net.cpp:1094] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0916 20:26:04.790025 13475 net.cpp:1094] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0916 20:26:04.790030 13475 net.cpp:1094] Copying source layer conv1b Type:Convolution #blobs=2
I0916 20:26:04.790040 13475 net.cpp:1094] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0916 20:26:04.790199 13475 net.cpp:1094] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0916 20:26:04.790205 13475 net.cpp:1094] Copying source layer pool1 Type:Pooling #blobs=0
I0916 20:26:04.790206 13475 net.cpp:1094] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0916 20:26:04.790223 13475 net.cpp:1094] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0916 20:26:04.790371 13475 net.cpp:1094] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0916 20:26:04.790375 13475 net.cpp:1094] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0916 20:26:04.790387 13475 net.cpp:1094] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0916 20:26:04.790529 13475 net.cpp:1094] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0916 20:26:04.790534 13475 net.cpp:1094] Copying source layer pool2 Type:Pooling #blobs=0
I0916 20:26:04.790547 13475 net.cpp:1094] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0916 20:26:04.790587 13475 net.cpp:1094] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0916 20:26:04.790724 13475 net.cpp:1094] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0916 20:26:04.790727 13475 net.cpp:1094] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0916 20:26:04.790750 13475 net.cpp:1094] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0916 20:26:04.790869 13475 net.cpp:1094] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0916 20:26:04.790874 13475 net.cpp:1094] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I0916 20:26:04.790876 13475 net.cpp:1094] Copying source layer pool3 Type:Pooling #blobs=0
I0916 20:26:04.790879 13475 net.cpp:1094] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0916 20:26:04.790992 13475 net.cpp:1094] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0916 20:26:04.791112 13475 net.cpp:1094] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0916 20:26:04.791116 13475 net.cpp:1094] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0916 20:26:04.791172 13475 net.cpp:1094] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0916 20:26:04.791292 13475 net.cpp:1094] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0916 20:26:04.791296 13475 net.cpp:1094] Copying source layer pool4 Type:Pooling #blobs=0
I0916 20:26:04.791299 13475 net.cpp:1094] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0916 20:26:04.791671 13475 net.cpp:1094] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0916 20:26:04.791829 13475 net.cpp:1094] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0916 20:26:04.791837 13475 net.cpp:1094] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0916 20:26:04.792047 13475 net.cpp:1094] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0916 20:26:04.792176 13475 net.cpp:1094] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0916 20:26:04.792181 13475 net.cpp:1094] Copying source layer out5a Type:Convolution #blobs=2
I0916 20:26:04.792230 13475 net.cpp:1094] Copying source layer out5a/bn Type:BatchNorm #blobs=5
I0916 20:26:04.792387 13475 net.cpp:1094] Copying source layer out5a/relu Type:ReLU #blobs=0
I0916 20:26:04.792392 13475 net.cpp:1094] Copying source layer out5a_up2 Type:Deconvolution #blobs=1
I0916 20:26:04.792400 13475 net.cpp:1094] Copying source layer out3a Type:Convolution #blobs=2
I0916 20:26:04.792420 13475 net.cpp:1094] Copying source layer out3a/bn Type:BatchNorm #blobs=5
I0916 20:26:04.792568 13475 net.cpp:1094] Copying source layer out3a/relu Type:ReLU #blobs=0
I0916 20:26:04.792573 13475 net.cpp:1094] Copying source layer out3_out5_combined Type:Eltwise #blobs=0
I0916 20:26:04.792577 13475 net.cpp:1094] Copying source layer ctx_conv1 Type:Convolution #blobs=2
I0916 20:26:04.792601 13475 net.cpp:1094] Copying source layer ctx_conv1/bn Type:BatchNorm #blobs=5
I0916 20:26:04.792748 13475 net.cpp:1094] Copying source layer ctx_conv1/relu Type:ReLU #blobs=0
I0916 20:26:04.792752 13475 net.cpp:1094] Copying source layer ctx_conv2 Type:Convolution #blobs=2
I0916 20:26:04.792771 13475 net.cpp:1094] Copying source layer ctx_conv2/bn Type:BatchNorm #blobs=5
I0916 20:26:04.792918 13475 net.cpp:1094] Copying source layer ctx_conv2/relu Type:ReLU #blobs=0
I0916 20:26:04.792923 13475 net.cpp:1094] Copying source layer ctx_conv3 Type:Convolution #blobs=2
I0916 20:26:04.792944 13475 net.cpp:1094] Copying source layer ctx_conv3/bn Type:BatchNorm #blobs=5
I0916 20:26:04.793092 13475 net.cpp:1094] Copying source layer ctx_conv3/relu Type:ReLU #blobs=0
I0916 20:26:04.793097 13475 net.cpp:1094] Copying source layer ctx_conv4 Type:Convolution #blobs=2
I0916 20:26:04.793118 13475 net.cpp:1094] Copying source layer ctx_conv4/bn Type:BatchNorm #blobs=5
I0916 20:26:04.793273 13475 net.cpp:1094] Copying source layer ctx_conv4/relu Type:ReLU #blobs=0
I0916 20:26:04.793278 13475 net.cpp:1094] Copying source layer ctx_final Type:Convolution #blobs=2
I0916 20:26:04.793290 13475 net.cpp:1094] Copying source layer ctx_final/relu Type:ReLU #blobs=0
I0916 20:26:04.793294 13475 net.cpp:1094] Copying source layer out_deconv_final_up2 Type:Deconvolution #blobs=1
I0916 20:26:04.793301 13475 net.cpp:1094] Copying source layer out_deconv_final_up4 Type:Deconvolution #blobs=1
I0916 20:26:04.793309 13475 net.cpp:1094] Copying source layer out_deconv_final_up8 Type:Deconvolution #blobs=1
I0916 20:26:04.793318 13475 net.cpp:1094] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0916 20:26:04.793424 13475 parallel.cpp:106] [0 - 0] P2pSync adding callback
I0916 20:26:04.793431 13475 parallel.cpp:106] [1 - 1] P2pSync adding callback
I0916 20:26:04.793434 13475 parallel.cpp:106] [2 - 2] P2pSync adding callback
I0916 20:26:04.793438 13475 parallel.cpp:59] Starting Optimization
I0916 20:26:04.793442 13475 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0916 20:26:04.793470 13475 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0916 20:26:04.793483 13475 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0916 20:26:04.794203 13570 device_alternate.hpp:116] NVML initialized on thread 135888480253696
I0916 20:26:04.812832 13570 common.cpp:585] NVML succeeded to set CPU affinity on device 0
I0916 20:26:04.812893 13571 device_alternate.hpp:116] NVML initialized on thread 135888471860992
I0916 20:26:04.813771 13571 common.cpp:585] NVML succeeded to set CPU affinity on device 1
I0916 20:26:04.813786 13572 device_alternate.hpp:116] NVML initialized on thread 135888463468288
I0916 20:26:04.814241 13572 common.cpp:585] NVML succeeded to set CPU affinity on device 2
I0916 20:26:04.818327 13571 solver.cpp:43] Solver data type: FLOAT
W0916 20:26:04.819209 13571 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0916 20:26:04.819382 13571 net.cpp:104] Using FLOAT as default forward math type
I0916 20:26:04.819391 13571 net.cpp:110] Using FLOAT as default backward math type
I0916 20:26:04.819438 13571 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 6
I0916 20:26:04.819450 13571 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0916 20:26:04.822481 13572 solver.cpp:43] Solver data type: FLOAT
W0916 20:26:04.822999 13572 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0916 20:26:04.823106 13572 net.cpp:104] Using FLOAT as default forward math type
I0916 20:26:04.823109 13572 net.cpp:110] Using FLOAT as default backward math type
I0916 20:26:04.823133 13572 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 6
I0916 20:26:04.823138 13572 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0916 20:26:04.823155 13573 db_lmdb.cpp:24] Opened lmdb data/train-image-lmdb
I0916 20:26:04.823920 13574 db_lmdb.cpp:24] Opened lmdb data/train-image-lmdb
I0916 20:26:04.826421 13571 data_layer.cpp:187] [1] ReshapePrefetch 6, 3, 640, 640
I0916 20:26:04.827399 13571 data_layer.cpp:211] [1] Output data size: 6, 3, 640, 640
I0916 20:26:04.827419 13571 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0916 20:26:04.827471 13571 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 6
I0916 20:26:04.827486 13571 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0916 20:26:04.827895 13572 data_layer.cpp:187] [2] ReshapePrefetch 6, 3, 640, 640
I0916 20:26:04.828055 13572 data_layer.cpp:211] [2] Output data size: 6, 3, 640, 640
I0916 20:26:04.828064 13572 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0916 20:26:04.828111 13572 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 6
I0916 20:26:04.828142 13572 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0916 20:26:04.828608 13575 data_layer.cpp:101] [1] Parser threads: 1
I0916 20:26:04.828644 13575 data_layer.cpp:103] [1] Transformer threads: 1
I0916 20:26:04.835206 13576 db_lmdb.cpp:24] Opened lmdb data/train-label-lmdb
I0916 20:26:04.836467 13577 data_layer.cpp:101] [2] Parser threads: 1
I0916 20:26:04.836494 13577 data_layer.cpp:103] [2] Transformer threads: 1
I0916 20:26:04.843201 13571 data_layer.cpp:187] [1] ReshapePrefetch 6, 1, 640, 640
I0916 20:26:04.844399 13571 data_layer.cpp:211] [1] Output data size: 6, 1, 640, 640
I0916 20:26:04.844408 13578 db_lmdb.cpp:24] Opened lmdb data/train-label-lmdb
I0916 20:26:04.844424 13571 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0916 20:26:04.846750 13579 data_layer.cpp:101] [1] Parser threads: 1
I0916 20:26:04.846871 13579 data_layer.cpp:103] [1] Transformer threads: 1
I0916 20:26:04.852236 13572 data_layer.cpp:187] [2] ReshapePrefetch 6, 1, 640, 640
I0916 20:26:04.854053 13572 data_layer.cpp:211] [2] Output data size: 6, 1, 640, 640
I0916 20:26:04.854279 13572 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0916 20:26:04.859596 13580 data_layer.cpp:101] [2] Parser threads: 1
I0916 20:26:04.860193 13580 data_layer.cpp:103] [2] Transformer threads: 1
I0916 20:26:04.860373 13575 blocking_queue.cpp:40] Waiting for datum
I0916 20:26:05.555249 13571 solver.cpp:177] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/test.prototxt
W0916 20:26:05.555357 13571 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0916 20:26:05.555634 13571 net.cpp:104] Using FLOAT as default forward math type
I0916 20:26:05.555644 13571 net.cpp:110] Using FLOAT as default backward math type
I0916 20:26:05.555677 13571 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 2
I0916 20:26:05.555686 13571 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0916 20:26:05.558573 13594 db_lmdb.cpp:24] Opened lmdb data/val-image-lmdb
I0916 20:26:05.560694 13572 solver.cpp:177] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/test.prototxt
W0916 20:26:05.560768 13572 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0916 20:26:05.560899 13572 net.cpp:104] Using FLOAT as default forward math type
I0916 20:26:05.560905 13572 net.cpp:110] Using FLOAT as default backward math type
I0916 20:26:05.560930 13572 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 2
I0916 20:26:05.560936 13572 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0916 20:26:05.564471 13596 db_lmdb.cpp:24] Opened lmdb data/val-image-lmdb
I0916 20:26:05.566169 13571 data_layer.cpp:187] (1) ReshapePrefetch 2, 3, 640, 640
I0916 20:26:05.566258 13571 data_layer.cpp:211] (1) Output data size: 2, 3, 640, 640
I0916 20:26:05.566267 13571 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0916 20:26:05.567446 13571 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 2
I0916 20:26:05.567482 13571 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0916 20:26:05.572091 13599 data_layer.cpp:101] (1) Parser threads: 1
I0916 20:26:05.572113 13599 data_layer.cpp:103] (1) Transformer threads: 1
I0916 20:26:05.573895 13600 db_lmdb.cpp:24] Opened lmdb data/val-label-lmdb
I0916 20:26:05.575371 13571 data_layer.cpp:187] (1) ReshapePrefetch 2, 1, 640, 640
I0916 20:26:05.579069 13572 data_layer.cpp:187] (2) ReshapePrefetch 2, 3, 640, 640
I0916 20:26:05.579206 13571 data_layer.cpp:211] (1) Output data size: 2, 1, 640, 640
I0916 20:26:05.579249 13572 data_layer.cpp:211] (2) Output data size: 2, 3, 640, 640
I0916 20:26:05.579277 13572 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0916 20:26:05.579241 13571 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0916 20:26:05.581725 13572 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 2
I0916 20:26:05.581755 13572 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0916 20:26:05.587541 13602 data_layer.cpp:101] (2) Parser threads: 1
I0916 20:26:05.587558 13602 data_layer.cpp:103] (2) Transformer threads: 1
I0916 20:26:05.590925 13603 data_layer.cpp:101] (1) Parser threads: 1
I0916 20:26:05.590971 13603 data_layer.cpp:103] (1) Transformer threads: 1
I0916 20:26:05.592893 13604 db_lmdb.cpp:24] Opened lmdb data/val-label-lmdb
I0916 20:26:05.594987 13572 data_layer.cpp:187] (2) ReshapePrefetch 2, 1, 640, 640
I0916 20:26:05.595340 13572 data_layer.cpp:211] (2) Output data size: 2, 1, 640, 640
I0916 20:26:05.595520 13572 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0916 20:26:05.603196 13605 data_layer.cpp:101] (2) Parser threads: 1
I0916 20:26:05.603217 13605 data_layer.cpp:103] (2) Transformer threads: 1
I0916 20:26:05.691479 13571 solver.cpp:57] Solver scaffolding done.
I0916 20:26:05.698357 13572 solver.cpp:57] Solver scaffolding done.
I0916 20:26:05.733371 13571 parallel.cpp:161] [1 - 1] P2pSync adding callback
I0916 20:26:05.733394 13570 parallel.cpp:161] [0 - 0] P2pSync adding callback
I0916 20:26:05.733413 13572 parallel.cpp:161] [2 - 2] P2pSync adding callback
I0916 20:26:05.936872 13570 net.cpp:2245] All zero weights of convolution layers are frozen
I0916 20:26:05.957839 13571 solver.cpp:490] Solving jsegnet21v2_train
I0916 20:26:05.957857 13571 solver.cpp:491] Learning Rate Policy: multistep
I0916 20:26:05.960305 13570 solver.cpp:490] Solving jsegnet21v2_train
I0916 20:26:05.960321 13570 solver.cpp:491] Learning Rate Policy: multistep
I0916 20:26:05.961205 13572 solver.cpp:490] Solving jsegnet21v2_train
I0916 20:26:05.961213 13572 solver.cpp:491] Learning Rate Policy: multistep
I0916 20:26:05.974102 13571 net.cpp:1412] [1] Reserving 10800128 bytes of shared learnable space
I0916 20:26:05.974112 13572 net.cpp:1412] [2] Reserving 10800128 bytes of shared learnable space
I0916 20:26:05.975805 13570 net.cpp:1412] [0] Reserving 10800128 bytes of shared learnable space
I0916 20:26:05.980725 13571 solver.cpp:228] Starting Optimization on GPU 1
I0916 20:26:05.980726 13572 solver.cpp:228] Starting Optimization on GPU 2
I0916 20:26:05.980726 13570 solver.cpp:228] Starting Optimization on GPU 0
I0916 20:26:05.982080 13570 solver.cpp:563] Iteration 0, Testing net (#0)
I0916 20:26:05.982156 13625 device_alternate.hpp:116] NVML initialized on thread 127888293107456
I0916 20:26:05.982170 13625 common.cpp:585] NVML succeeded to set CPU affinity on device 0
I0916 20:26:05.982180 13624 device_alternate.hpp:116] NVML initialized on thread 127888301500160
I0916 20:26:05.982197 13624 common.cpp:585] NVML succeeded to set CPU affinity on device 1
I0916 20:26:05.982208 13623 device_alternate.hpp:116] NVML initialized on thread 127888309892864
I0916 20:26:05.982216 13623 common.cpp:585] NVML succeeded to set CPU affinity on device 2
I0916 20:26:06.257452 13570 solver.cpp:655]     Test net output #0: accuracy/top1 = 0.873999
I0916 20:26:06.257472 13570 solver.cpp:655]     Test net output #1: accuracy/top5 = 1
I0916 20:26:06.257478 13570 solver.cpp:655]     Test net output #2: loss = 0.499414 (* 1 = 0.499414 loss)
I0916 20:26:06.257483 13570 solver.cpp:255] [MultiGPU] Initial Test completed
I0916 20:26:06.694293 13570 solver.cpp:319] Iteration 0 (0.436766 s), loss = 0.0710956
I0916 20:26:06.694319 13570 solver.cpp:336]     Train net output #0: loss = 0.0710956 (* 1 = 0.0710956 loss)
I0916 20:26:06.694324 13570 sgd_solver.cpp:136] Iteration 0, lr = 0.01, m = 0.9
I0916 20:26:06.899049 13570 solver.cpp:319] Iteration 1 (0.204746 s), loss = 0.0594583
I0916 20:26:06.899070 13570 solver.cpp:336]     Train net output #0: loss = 0.0594583 (* 1 = 0.0594583 loss)
I0916 20:26:07.055265 13570 cudnn_conv_layer.cpp:872] [0] Conv Algos (F,BD,BF): 'conv1a' with space 4.24G 3/1 1 0 3 	(avail 0.06G, req 0G)	t: 0 2.84 2.5
I0916 20:26:07.071058 13571 cudnn_conv_layer.cpp:872] [1] Conv Algos (F,BD,BF): 'conv1a' with space 4.24G 3/1 1 0 3 	(avail 0.14G, req 0G)	t: 0 3.04 2.57
I0916 20:26:07.090965 13572 cudnn_conv_layer.cpp:872] [2] Conv Algos (F,BD,BF): 'conv1a' with space 4.24G 3/1 1 0 0 	(avail 0.14G, req 0G)	t: 0 3.32 2.61
I0916 20:26:07.192945 13570 cudnn_conv_layer.cpp:872] [0] Conv Algos (F,BD,BF): 'conv1b' with space 4.24G 32/4 6 4 3 	(avail 0.06G, req 0G)	t: 0 0.6 1.26
I0916 20:26:07.199384 13571 cudnn_conv_layer.cpp:872] [1] Conv Algos (F,BD,BF): 'conv1b' with space 4.24G 32/4 6 4 3 	(avail 0.14G, req 0G)	t: 0 0.65 1.32
I0916 20:26:07.215052 13572 cudnn_conv_layer.cpp:872] [2] Conv Algos (F,BD,BF): 'conv1b' with space 4.24G 32/4 6 4 3 	(avail 0.14G, req 0G)	t: 0 0.71 1.41
I0916 20:26:07.449370 13570 cudnn_conv_layer.cpp:872] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 4.24G 32/1 6 4 3 	(avail 0.06G, req 0G)	t: 0 0.73 1.46
I0916 20:26:07.458328 13571 cudnn_conv_layer.cpp:872] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 4.24G 32/1 6 4 3 	(avail 0.14G, req 0G)	t: 0 0.74 1.52
I0916 20:26:07.503811 13572 cudnn_conv_layer.cpp:872] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 4.24G 32/1 6 4 3 	(avail 0.14G, req 0G)	t: 0 0.77 1.57
I0916 20:26:07.531684 13570 cudnn_conv_layer.cpp:872] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 4.24G 64/4 6 4 3 	(avail 0.06G, req 0G)	t: 0 0.27 0.64
I0916 20:26:07.566438 13571 cudnn_conv_layer.cpp:872] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 4.24G 64/4 6 4 3 	(avail 0.14G, req 0G)	t: 0 0.28 0.65
I0916 20:26:07.594005 13572 cudnn_conv_layer.cpp:872] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 4.24G 64/4 6 4 3 	(avail 0.14G, req 0G)	t: 0 0.28 0.72
I0916 20:26:07.761878 13571 cudnn_conv_layer.cpp:872] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 4.24G 64/1 6 4 5 	(avail 0.14G, req 0.07G)	t: 0 0.47 0.94
I0916 20:26:07.784217 13570 cudnn_conv_layer.cpp:872] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 4.24G 64/1 6 4 3 	(avail 0.06G, req 0G)	t: 0 0.45 0.91
I0916 20:26:07.802799 13572 cudnn_conv_layer.cpp:872] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 4.24G 64/1 6 4 5 	(avail 0.14G, req 0.07G)	t: 0 0.5 0.98
I0916 20:26:07.823887 13571 cudnn_conv_layer.cpp:872] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 4.24G 128/4 6 4 3 	(avail 0.14G, req 0.07G)	t: 0 0.14 0.29
I0916 20:26:07.841420 13570 cudnn_conv_layer.cpp:872] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 4.24G 128/4 6 4 3 	(avail 0.06G, req 0G)	t: 0 0.13 0.27
I0916 20:26:07.854310 13572 cudnn_conv_layer.cpp:872] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 4.24G 128/4 6 4 3 	(avail 0.14G, req 0.07G)	t: 0 0.14 0.33
I0916 20:26:07.961580 13571 cudnn_conv_layer.cpp:872] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 4.24G 128/1 6 4 5 	(avail 0.14G, req 0.07G)	t: 0 0.48 0.55
I0916 20:26:07.978808 13570 cudnn_conv_layer.cpp:872] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 4.24G 128/1 6 4 5 	(avail 0.06G, req 0.04G)	t: 0 0.45 0.54
I0916 20:26:08.000398 13571 cudnn_conv_layer.cpp:872] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 4.24G 256/4 6 4 3 	(avail 0.14G, req 0.07G)	t: 0 0.11 0.2
I0916 20:26:08.001756 13572 cudnn_conv_layer.cpp:872] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 4.24G 128/1 6 4 5 	(avail 0.14G, req 0.07G)	t: 0 0.52 0.58
I0916 20:26:08.023416 13570 cudnn_conv_layer.cpp:872] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 4.24G 256/4 6 4 3 	(avail 0.06G, req 0.04G)	t: 0 0.1 0.18
I0916 20:26:08.031792 13572 cudnn_conv_layer.cpp:872] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 4.24G 256/4 6 4 3 	(avail 0.14G, req 0.07G)	t: 0 0.11 0.22
I0916 20:26:08.088630 13571 cudnn_conv_layer.cpp:872] [1] Conv Algos (F,BD,BF): 'out3a' with space 4.24G 128/2 6 4 3 	(avail 0.14G, req 0.07G)	t: 0 0.22 0.44
I0916 20:26:08.113046 13570 cudnn_conv_layer.cpp:872] [0] Conv Algos (F,BD,BF): 'out3a' with space 4.24G 128/2 6 4 3 	(avail 0.06G, req 0.04G)	t: 0 0.24 0.4
I0916 20:26:08.132021 13572 cudnn_conv_layer.cpp:872] [2] Conv Algos (F,BD,BF): 'out3a' with space 4.24G 128/2 6 4 3 	(avail 0.14G, req 0.07G)	t: 0 0.23 0.47
I0916 20:26:08.205206 13571 cudnn_conv_layer.cpp:872] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 4.24G 64/1 6 4 3 	(avail 0.14G, req 0.07G)	t: 0 0.29 0.63
I0916 20:26:08.228204 13570 cudnn_conv_layer.cpp:872] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 4.24G 64/1 6 4 3 	(avail 0.06G, req 0.04G)	t: 0 0.28 0.61
I0916 20:26:08.255657 13572 cudnn_conv_layer.cpp:872] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 4.24G 64/1 6 4 3 	(avail 0.14G, req 0.07G)	t: 0 0.31 0.67
I0916 20:26:08.267278 13571 cudnn_conv_layer.cpp:872] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 4.24G 64/1 6 1 5 	(avail 0.14G, req 0.07G)	t: 0 0.11 0.35
I0916 20:26:08.281013 13570 cudnn_conv_layer.cpp:872] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 4.24G 64/1 6 1 5 	(avail 0.06G, req 0.04G)	t: 0 0.11 0.35
I0916 20:26:08.307013 13572 cudnn_conv_layer.cpp:872] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 4.24G 64/1 6 1 5 	(avail 0.14G, req 0.07G)	t: 0 0.12 0.37
I0916 20:26:08.436226 13570 solver.cpp:319] Iteration 2 (1.53713 s), loss = 0.0522841
I0916 20:26:08.436251 13570 solver.cpp:336]     Train net output #0: loss = 0.0522841 (* 1 = 0.0522841 loss)
I0916 20:26:08.436882 13572 cudnn_conv_layer.cpp:474] [2] Layer 'conv1a' reallocating workspace 4.24G to 0.14G
I0916 20:26:08.444725 13570 cudnn_conv_layer.cpp:474] [0] Layer 'conv1a' reallocating workspace 4.24G to 0.08G
I0916 20:26:08.454316 13571 cudnn_conv_layer.cpp:474] [1] Layer 'conv1a' reallocating workspace 4.24G to 0.14G
I0916 20:26:27.207232 13570 solver.cpp:314] Iteration 100 (5.22097 iter/s, 18.7705s/98 iter), loss = 0.0626877
I0916 20:26:27.207257 13570 solver.cpp:336]     Train net output #0: loss = 0.0626877 (* 1 = 0.0626877 loss)
I0916 20:26:27.207262 13570 sgd_solver.cpp:136] Iteration 100, lr = 0.01, m = 0.9
I0916 20:26:39.107138 13578 data_reader.cpp:305] Starting prefetch of epoch 1
I0916 20:26:46.354437 13570 solver.cpp:314] Iteration 200 (5.22284 iter/s, 19.1467s/100 iter), loss = 0.10835
I0916 20:26:46.354467 13570 solver.cpp:336]     Train net output #0: loss = 0.10835 (* 1 = 0.10835 loss)
I0916 20:26:46.354473 13570 sgd_solver.cpp:136] Iteration 200, lr = 0.01, m = 0.9
I0916 20:27:05.671445 13570 solver.cpp:314] Iteration 300 (5.17693 iter/s, 19.3165s/100 iter), loss = 0.0914005
I0916 20:27:05.671465 13570 solver.cpp:336]     Train net output #0: loss = 0.0914004 (* 1 = 0.0914004 loss)
I0916 20:27:05.671470 13570 sgd_solver.cpp:136] Iteration 300, lr = 0.01, m = 0.9
I0916 20:27:24.791748 13570 solver.cpp:314] Iteration 400 (5.23019 iter/s, 19.1198s/100 iter), loss = 0.0800138
I0916 20:27:24.791798 13570 solver.cpp:336]     Train net output #0: loss = 0.0800138 (* 1 = 0.0800138 loss)
I0916 20:27:24.791803 13570 sgd_solver.cpp:136] Iteration 400, lr = 0.01, m = 0.9
I0916 20:27:42.919237 13574 data_reader.cpp:305] Starting prefetch of epoch 1
I0916 20:27:44.450038 13570 solver.cpp:314] Iteration 500 (5.08706 iter/s, 19.6577s/100 iter), loss = 0.127362
I0916 20:27:44.450063 13570 solver.cpp:336]     Train net output #0: loss = 0.127362 (* 1 = 0.127362 loss)
I0916 20:27:44.450069 13570 sgd_solver.cpp:136] Iteration 500, lr = 0.01, m = 0.9
I0916 20:28:03.478564 13570 solver.cpp:314] Iteration 600 (5.25542 iter/s, 19.028s/100 iter), loss = 0.0829966
I0916 20:28:03.478621 13570 solver.cpp:336]     Train net output #0: loss = 0.0829965 (* 1 = 0.0829965 loss)
I0916 20:28:03.478628 13570 sgd_solver.cpp:136] Iteration 600, lr = 0.01, m = 0.9
I0916 20:28:22.781175 13570 solver.cpp:314] Iteration 700 (5.18079 iter/s, 19.3021s/100 iter), loss = 0.0879114
I0916 20:28:22.781198 13570 solver.cpp:336]     Train net output #0: loss = 0.0879113 (* 1 = 0.0879113 loss)
I0916 20:28:22.781205 13570 sgd_solver.cpp:136] Iteration 700, lr = 0.01, m = 0.9
I0916 20:28:42.035109 13570 solver.cpp:314] Iteration 800 (5.19389 iter/s, 19.2534s/100 iter), loss = 0.195807
I0916 20:28:42.035168 13570 solver.cpp:336]     Train net output #0: loss = 0.195807 (* 1 = 0.195807 loss)
I0916 20:28:42.035176 13570 sgd_solver.cpp:136] Iteration 800, lr = 0.01, m = 0.9
I0916 20:28:46.506585 13576 data_reader.cpp:305] Starting prefetch of epoch 1
I0916 20:29:01.453392 13570 solver.cpp:314] Iteration 900 (5.14993 iter/s, 19.4177s/100 iter), loss = 0.0800309
I0916 20:29:01.453418 13570 solver.cpp:336]     Train net output #0: loss = 0.0800309 (* 1 = 0.0800309 loss)
I0916 20:29:01.453424 13570 sgd_solver.cpp:136] Iteration 900, lr = 0.01, m = 0.9
I0916 20:29:18.643040 13544 data_reader.cpp:305] Starting prefetch of epoch 1
I0916 20:29:20.704495 13570 solver.cpp:368] Sparsity after update:
I0916 20:29:20.727072 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 20:29:20.727166 13570 net.cpp:2304] conv1a_param_0(0) 
I0916 20:29:20.727195 13570 net.cpp:2304] conv1b_param_0(0) 
I0916 20:29:20.727205 13570 net.cpp:2304] ctx_conv1_param_0(0) 
I0916 20:29:20.727213 13570 net.cpp:2304] ctx_conv2_param_0(0) 
I0916 20:29:20.727221 13570 net.cpp:2304] ctx_conv3_param_0(0) 
I0916 20:29:20.727229 13570 net.cpp:2304] ctx_conv4_param_0(0) 
I0916 20:29:20.727238 13570 net.cpp:2304] ctx_final_param_0(0) 
I0916 20:29:20.727247 13570 net.cpp:2304] out3a_param_0(0) 
I0916 20:29:20.727257 13570 net.cpp:2304] out5a_param_0(0) 
I0916 20:29:20.727265 13570 net.cpp:2304] res2a_branch2a_param_0(0) 
I0916 20:29:20.727274 13570 net.cpp:2304] res2a_branch2b_param_0(0) 
I0916 20:29:20.727285 13570 net.cpp:2304] res3a_branch2a_param_0(0) 
I0916 20:29:20.727295 13570 net.cpp:2304] res3a_branch2b_param_0(0) 
I0916 20:29:20.727304 13570 net.cpp:2304] res4a_branch2a_param_0(0) 
I0916 20:29:20.727313 13570 net.cpp:2304] res4a_branch2b_param_0(0) 
I0916 20:29:20.727321 13570 net.cpp:2304] res5a_branch2a_param_0(0) 
I0916 20:29:20.727330 13570 net.cpp:2304] res5a_branch2b_param_0(0) 
I0916 20:29:20.727339 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0916 20:29:20.907929 13570 solver.cpp:314] Iteration 1000 (5.14033 iter/s, 19.454s/100 iter), loss = 0.0972464
I0916 20:29:20.907953 13570 solver.cpp:336]     Train net output #0: loss = 0.0972463 (* 1 = 0.0972463 loss)
I0916 20:29:20.907958 13570 sgd_solver.cpp:136] Iteration 1000, lr = 0.01, m = 0.9
I0916 20:29:40.589984 13570 solver.cpp:314] Iteration 1100 (5.08091 iter/s, 19.6815s/100 iter), loss = 0.0659606
I0916 20:29:40.590009 13570 solver.cpp:336]     Train net output #0: loss = 0.0659605 (* 1 = 0.0659605 loss)
I0916 20:29:40.590016 13570 sgd_solver.cpp:136] Iteration 1100, lr = 0.01, m = 0.9
I0916 20:30:00.113064 13570 solver.cpp:314] Iteration 1200 (5.12229 iter/s, 19.5225s/100 iter), loss = 0.0697681
I0916 20:30:00.113144 13570 solver.cpp:336]     Train net output #0: loss = 0.069768 (* 1 = 0.069768 loss)
I0916 20:30:00.113152 13570 sgd_solver.cpp:136] Iteration 1200, lr = 0.01, m = 0.9
I0916 20:30:19.479243 13570 solver.cpp:314] Iteration 1300 (5.16379 iter/s, 19.3656s/100 iter), loss = 0.0953907
I0916 20:30:19.479269 13570 solver.cpp:336]     Train net output #0: loss = 0.0953906 (* 1 = 0.0953906 loss)
I0916 20:30:19.479276 13570 sgd_solver.cpp:136] Iteration 1300, lr = 0.01, m = 0.9
I0916 20:30:23.190732 13573 data_reader.cpp:305] Starting prefetch of epoch 1
I0916 20:30:39.311841 13570 solver.cpp:314] Iteration 1400 (5.04235 iter/s, 19.832s/100 iter), loss = 0.114281
I0916 20:30:39.311889 13570 solver.cpp:336]     Train net output #0: loss = 0.114281 (* 1 = 0.114281 loss)
I0916 20:30:39.311894 13570 sgd_solver.cpp:136] Iteration 1400, lr = 0.01, m = 0.9
I0916 20:30:58.876444 13570 solver.cpp:314] Iteration 1500 (5.11142 iter/s, 19.5641s/100 iter), loss = 0.0907609
I0916 20:30:58.876480 13570 solver.cpp:336]     Train net output #0: loss = 0.0907608 (* 1 = 0.0907608 loss)
I0916 20:30:58.876487 13570 sgd_solver.cpp:136] Iteration 1500, lr = 0.01, m = 0.9
I0916 20:31:18.400101 13570 solver.cpp:314] Iteration 1600 (5.12213 iter/s, 19.5231s/100 iter), loss = 0.076425
I0916 20:31:18.400204 13570 solver.cpp:336]     Train net output #0: loss = 0.0764249 (* 1 = 0.0764249 loss)
I0916 20:31:18.400213 13570 sgd_solver.cpp:136] Iteration 1600, lr = 0.01, m = 0.9
I0916 20:31:28.147564 13546 data_reader.cpp:305] Starting prefetch of epoch 1
I0916 20:31:38.465708 13570 solver.cpp:314] Iteration 1700 (4.98379 iter/s, 20.065s/100 iter), loss = 0.118047
I0916 20:31:38.465734 13570 solver.cpp:336]     Train net output #0: loss = 0.118047 (* 1 = 0.118047 loss)
I0916 20:31:38.465740 13570 sgd_solver.cpp:136] Iteration 1700, lr = 0.01, m = 0.9
I0916 20:31:58.127096 13570 solver.cpp:314] Iteration 1800 (5.08625 iter/s, 19.6608s/100 iter), loss = 0.0790432
I0916 20:31:58.127198 13570 solver.cpp:336]     Train net output #0: loss = 0.0790431 (* 1 = 0.0790431 loss)
I0916 20:31:58.127207 13570 sgd_solver.cpp:136] Iteration 1800, lr = 0.01, m = 0.9
I0916 20:32:01.102394 13578 data_reader.cpp:305] Starting prefetch of epoch 2
I0916 20:32:17.641944 13570 solver.cpp:314] Iteration 1900 (5.12445 iter/s, 19.5143s/100 iter), loss = 0.0668538
I0916 20:32:17.641968 13570 solver.cpp:336]     Train net output #0: loss = 0.0668537 (* 1 = 0.0668537 loss)
I0916 20:32:17.641973 13570 sgd_solver.cpp:136] Iteration 1900, lr = 0.01, m = 0.9
I0916 20:32:36.953130 13570 solver.cpp:368] Sparsity after update:
I0916 20:32:36.958254 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 20:32:36.958370 13570 net.cpp:2304] conv1a_param_0(0) 
I0916 20:32:36.958407 13570 net.cpp:2304] conv1b_param_0(0) 
I0916 20:32:36.958420 13570 net.cpp:2304] ctx_conv1_param_0(0) 
I0916 20:32:36.958431 13570 net.cpp:2304] ctx_conv2_param_0(0) 
I0916 20:32:36.958442 13570 net.cpp:2304] ctx_conv3_param_0(0) 
I0916 20:32:36.958454 13570 net.cpp:2304] ctx_conv4_param_0(0) 
I0916 20:32:36.958467 13570 net.cpp:2304] ctx_final_param_0(0) 
I0916 20:32:36.958477 13570 net.cpp:2304] out3a_param_0(0) 
I0916 20:32:36.958489 13570 net.cpp:2304] out5a_param_0(0) 
I0916 20:32:36.958500 13570 net.cpp:2304] res2a_branch2a_param_0(0) 
I0916 20:32:36.958513 13570 net.cpp:2304] res2a_branch2b_param_0(0) 
I0916 20:32:36.958523 13570 net.cpp:2304] res3a_branch2a_param_0(0) 
I0916 20:32:36.958534 13570 net.cpp:2304] res3a_branch2b_param_0(0) 
I0916 20:32:36.958546 13570 net.cpp:2304] res4a_branch2a_param_0(0) 
I0916 20:32:36.958560 13570 net.cpp:2304] res4a_branch2b_param_0(0) 
I0916 20:32:36.958571 13570 net.cpp:2304] res5a_branch2a_param_0(0) 
I0916 20:32:36.958582 13570 net.cpp:2304] res5a_branch2b_param_0(0) 
I0916 20:32:36.958593 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0916 20:32:36.958619 13570 solver.cpp:563] Iteration 2000, Testing net (#0)
I0916 20:32:37.054013 13570 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'conv1a' with space 0.08G 3/1 1 	(avail 4.2G, req 0G)	t: 0
I0916 20:32:37.061952 13571 cudnn_conv_layer.cpp:872] (1) Conv Algo (F): 'conv1a' with space 0.14G 3/1 1 	(avail 4.24G, req 0G)	t: 0
I0916 20:32:37.073221 13572 cudnn_conv_layer.cpp:872] (2) Conv Algo (F): 'conv1a' with space 0.14G 3/1 1 	(avail 4.24G, req 0G)	t: 0
I0916 20:32:37.091454 13570 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'conv1b' with space 0.08G 32/4 6 	(avail 4.2G, req 0G)	t: 0
I0916 20:32:37.107632 13570 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res2a_branch2a' with space 0.08G 32/1 6 	(avail 4.2G, req 0G)	t: 0
I0916 20:32:37.118402 13571 cudnn_conv_layer.cpp:872] (1) Conv Algo (F): 'conv1b' with space 0.14G 32/4 6 	(avail 4.24G, req 0G)	t: 0
I0916 20:32:37.119091 13572 cudnn_conv_layer.cpp:872] (2) Conv Algo (F): 'conv1b' with space 0.14G 32/4 6 	(avail 4.24G, req 0G)	t: 0
I0916 20:32:37.119379 13570 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res2a_branch2b' with space 0.08G 64/4 6 	(avail 4.2G, req 0G)	t: 0
I0916 20:32:37.130568 13570 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res3a_branch2a' with space 0.08G 64/1 6 	(avail 4.2G, req 0G)	t: 0
I0916 20:32:37.136071 13572 cudnn_conv_layer.cpp:872] (2) Conv Algo (F): 'res2a_branch2a' with space 0.14G 32/1 6 	(avail 4.24G, req 0G)	t: 0
I0916 20:32:37.138041 13570 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res3a_branch2b' with space 0.08G 128/4 1 	(avail 4.2G, req 0G)	t: 0
I0916 20:32:37.141135 13571 cudnn_conv_layer.cpp:872] (1) Conv Algo (F): 'res2a_branch2a' with space 0.14G 32/1 6 	(avail 4.24G, req 0G)	t: 0
I0916 20:32:37.144132 13570 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res4a_branch2a' with space 0.08G 128/1 6 	(avail 4.2G, req 0G)	t: 0
I0916 20:32:37.148849 13572 cudnn_conv_layer.cpp:872] (2) Conv Algo (F): 'res2a_branch2b' with space 0.14G 64/4 6 	(avail 4.24G, req 0G)	t: 0
I0916 20:32:37.149101 13570 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res4a_branch2b' with space 0.08G 256/4 6 	(avail 4.2G, req 0G)	t: 0
I0916 20:32:37.161109 13572 cudnn_conv_layer.cpp:872] (2) Conv Algo (F): 'res3a_branch2a' with space 0.14G 64/1 6 	(avail 4.24G, req 0G)	t: 0
I0916 20:32:37.162648 13570 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'out3a' with space 0.08G 128/2 6 	(avail 4.2G, req 0G)	t: 0
I0916 20:32:37.165089 13571 cudnn_conv_layer.cpp:872] (1) Conv Algo (F): 'res2a_branch2b' with space 0.14G 64/4 6 	(avail 4.24G, req 0G)	t: 0
I0916 20:32:37.168443 13572 cudnn_conv_layer.cpp:872] (2) Conv Algo (F): 'res3a_branch2b' with space 0.14G 128/4 1 	(avail 4.24G, req 0G)	t: 0
I0916 20:32:37.169960 13570 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'ctx_conv1' with space 0.08G 64/1 6 	(avail 4.2G, req 0G)	t: 0
I0916 20:32:37.177242 13571 cudnn_conv_layer.cpp:872] (1) Conv Algo (F): 'res3a_branch2a' with space 0.14G 64/1 6 	(avail 4.24G, req 0G)	t: 0
I0916 20:32:37.177745 13572 cudnn_conv_layer.cpp:872] (2) Conv Algo (F): 'res4a_branch2a' with space 0.14G 128/1 6 	(avail 4.24G, req 0G)	t: 0
I0916 20:32:37.179857 13570 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'ctx_final' with space 0.08G 64/1 6 	(avail 4.2G, req 0G)	t: 0
I0916 20:32:37.182445 13572 cudnn_conv_layer.cpp:872] (2) Conv Algo (F): 'res4a_branch2b' with space 0.14G 256/4 6 	(avail 4.24G, req 0G)	t: 0
I0916 20:32:37.183912 13571 cudnn_conv_layer.cpp:872] (1) Conv Algo (F): 'res3a_branch2b' with space 0.14G 128/4 6 	(avail 4.24G, req 0G)	t: 0
I0916 20:32:37.194756 13571 cudnn_conv_layer.cpp:872] (1) Conv Algo (F): 'res4a_branch2a' with space 0.14G 128/1 6 	(avail 4.24G, req 0G)	t: 0
I0916 20:32:37.197062 13572 cudnn_conv_layer.cpp:872] (2) Conv Algo (F): 'out3a' with space 0.14G 128/2 6 	(avail 4.24G, req 0G)	t: 0
I0916 20:32:37.199546 13571 cudnn_conv_layer.cpp:872] (1) Conv Algo (F): 'res4a_branch2b' with space 0.14G 256/4 6 	(avail 4.24G, req 0G)	t: 0
I0916 20:32:37.206977 13572 cudnn_conv_layer.cpp:872] (2) Conv Algo (F): 'ctx_conv1' with space 0.14G 64/1 6 	(avail 4.24G, req 0G)	t: 0
I0916 20:32:37.213174 13571 cudnn_conv_layer.cpp:872] (1) Conv Algo (F): 'out3a' with space 0.14G 128/2 6 	(avail 4.24G, req 0G)	t: 0
I0916 20:32:37.224254 13572 cudnn_conv_layer.cpp:872] (2) Conv Algo (F): 'ctx_final' with space 0.14G 64/1 6 	(avail 4.24G, req 0G)	t: 0
I0916 20:32:37.224911 13571 cudnn_conv_layer.cpp:872] (1) Conv Algo (F): 'ctx_conv1' with space 0.14G 64/1 6 	(avail 4.24G, req 0G)	t: 0
I0916 20:32:37.239892 13571 cudnn_conv_layer.cpp:872] (1) Conv Algo (F): 'ctx_final' with space 0.14G 64/1 6 	(avail 4.24G, req 0G)	t: 0
I0916 20:32:44.091719 13566 data_reader.cpp:305] Starting prefetch of epoch 1
I0916 20:32:48.376057 13570 solver.cpp:655]     Test net output #0: accuracy/top1 = 0.93846
I0916 20:32:48.376085 13570 solver.cpp:655]     Test net output #1: accuracy/top5 = 0.999999
I0916 20:32:48.376094 13570 solver.cpp:655]     Test net output #2: loss = 0.185009 (* 1 = 0.185009 loss)
I0916 20:32:48.376152 13570 solver.cpp:265] [MultiGPU] Tests completed in 11.4172s
I0916 20:32:48.598438 13570 solver.cpp:314] Iteration 2000 (3.23043 iter/s, 30.9556s/100 iter), loss = 0.190298
I0916 20:32:48.598500 13570 solver.cpp:336]     Train net output #0: loss = 0.190298 (* 1 = 0.190298 loss)
I0916 20:32:48.598516 13570 sgd_solver.cpp:136] Iteration 2000, lr = 0.01, m = 0.9
I0916 20:33:08.242048 13570 solver.cpp:314] Iteration 2100 (5.09086 iter/s, 19.6431s/100 iter), loss = 0.0758436
I0916 20:33:08.242148 13570 solver.cpp:336]     Train net output #0: loss = 0.0758435 (* 1 = 0.0758435 loss)
I0916 20:33:08.242156 13570 sgd_solver.cpp:136] Iteration 2100, lr = 0.01, m = 0.9
I0916 20:33:27.960248 13570 solver.cpp:314] Iteration 2200 (5.0716 iter/s, 19.7177s/100 iter), loss = 0.131059
I0916 20:33:27.960273 13570 solver.cpp:336]     Train net output #0: loss = 0.131059 (* 1 = 0.131059 loss)
I0916 20:33:27.960278 13570 sgd_solver.cpp:136] Iteration 2200, lr = 0.01, m = 0.9
I0916 20:33:47.721277 13570 solver.cpp:314] Iteration 2300 (5.06061 iter/s, 19.7605s/100 iter), loss = 0.0804817
I0916 20:33:47.721362 13570 solver.cpp:336]     Train net output #0: loss = 0.0804817 (* 1 = 0.0804817 loss)
I0916 20:33:47.721370 13570 sgd_solver.cpp:136] Iteration 2300, lr = 0.01, m = 0.9
I0916 20:33:49.788101 13546 data_reader.cpp:305] Starting prefetch of epoch 2
I0916 20:34:07.996366 13570 solver.cpp:314] Iteration 2400 (4.9323 iter/s, 20.2745s/100 iter), loss = 0.0711275
I0916 20:34:07.996392 13570 solver.cpp:336]     Train net output #0: loss = 0.0711275 (* 1 = 0.0711275 loss)
I0916 20:34:07.996399 13570 sgd_solver.cpp:136] Iteration 2400, lr = 0.01, m = 0.9
I0916 20:34:23.174016 13544 data_reader.cpp:305] Starting prefetch of epoch 2
I0916 20:34:27.821038 13570 solver.cpp:314] Iteration 2500 (5.04436 iter/s, 19.8241s/100 iter), loss = 0.0904692
I0916 20:34:27.821069 13570 solver.cpp:336]     Train net output #0: loss = 0.0904692 (* 1 = 0.0904692 loss)
I0916 20:34:27.821077 13570 sgd_solver.cpp:136] Iteration 2500, lr = 0.01, m = 0.9
I0916 20:34:47.511258 13570 solver.cpp:314] Iteration 2600 (5.0788 iter/s, 19.6897s/100 iter), loss = 0.0796041
I0916 20:34:47.511283 13570 solver.cpp:336]     Train net output #0: loss = 0.079604 (* 1 = 0.079604 loss)
I0916 20:34:47.511289 13570 sgd_solver.cpp:136] Iteration 2600, lr = 0.01, m = 0.9
I0916 20:35:07.541353 13570 solver.cpp:314] Iteration 2700 (4.99263 iter/s, 20.0295s/100 iter), loss = 0.0736584
I0916 20:35:07.541509 13570 solver.cpp:336]     Train net output #0: loss = 0.0736582 (* 1 = 0.0736582 loss)
I0916 20:35:07.541517 13570 sgd_solver.cpp:136] Iteration 2700, lr = 0.01, m = 0.9
I0916 20:35:27.690789 13570 solver.cpp:314] Iteration 2800 (4.96306 iter/s, 20.1489s/100 iter), loss = 0.0603484
I0916 20:35:27.690814 13570 solver.cpp:336]     Train net output #0: loss = 0.0603483 (* 1 = 0.0603483 loss)
I0916 20:35:27.690820 13570 sgd_solver.cpp:136] Iteration 2800, lr = 0.01, m = 0.9
I0916 20:35:28.918642 13573 data_reader.cpp:305] Starting prefetch of epoch 2
I0916 20:35:47.448968 13570 solver.cpp:314] Iteration 2900 (5.06134 iter/s, 19.7576s/100 iter), loss = 0.096287
I0916 20:35:47.449048 13570 solver.cpp:336]     Train net output #0: loss = 0.0962868 (* 1 = 0.0962868 loss)
I0916 20:35:47.449055 13570 sgd_solver.cpp:136] Iteration 2900, lr = 0.01, m = 0.9
I0916 20:36:07.070559 13570 solver.cpp:424] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.8 sparsity_achieved=0 iter=3000
I0916 20:36:48.112462 13570 net.cpp:2245] All zero weights of convolution layers are frozen
I0916 20:36:48.116446 13570 solver.cpp:368] Sparsity after update:
I0916 20:36:48.118016 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 20:36:48.118023 13570 net.cpp:2304] conv1a_param_0(0.322) 
I0916 20:36:48.118028 13570 net.cpp:2304] conv1b_param_0(0.724) 
I0916 20:36:48.118031 13570 net.cpp:2304] ctx_conv1_param_0(0.797) 
I0916 20:36:48.118032 13570 net.cpp:2304] ctx_conv2_param_0(0.798) 
I0916 20:36:48.118034 13570 net.cpp:2304] ctx_conv3_param_0(0.797) 
I0916 20:36:48.118036 13570 net.cpp:2304] ctx_conv4_param_0(0.798) 
I0916 20:36:48.118038 13570 net.cpp:2304] ctx_final_param_0(0.333) 
I0916 20:36:48.118041 13570 net.cpp:2304] out3a_param_0(0.799) 
I0916 20:36:48.118042 13570 net.cpp:2304] out5a_param_0(0.8) 
I0916 20:36:48.118044 13570 net.cpp:2304] res2a_branch2a_param_0(0.79) 
I0916 20:36:48.118046 13570 net.cpp:2304] res2a_branch2b_param_0(0.687) 
I0916 20:36:48.118048 13570 net.cpp:2304] res3a_branch2a_param_0(0.794) 
I0916 20:36:48.118050 13570 net.cpp:2304] res3a_branch2b_param_0(0.771) 
I0916 20:36:48.118052 13570 net.cpp:2304] res4a_branch2a_param_0(0.799) 
I0916 20:36:48.118054 13570 net.cpp:2304] res4a_branch2b_param_0(0.79) 
I0916 20:36:48.118057 13570 net.cpp:2304] res5a_branch2a_param_0(0.798) 
I0916 20:36:48.118058 13570 net.cpp:2304] res5a_branch2b_param_0(0.799) 
I0916 20:36:48.118059 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.14224e+06/2.69117e+06) 0.796
I0916 20:36:48.295306 13570 solver.cpp:314] Iteration 3000 (1.64353 iter/s, 60.8446s/100 iter), loss = 0.140066
I0916 20:36:48.295331 13570 solver.cpp:336]     Train net output #0: loss = 0.140066 (* 1 = 0.140066 loss)
I0916 20:36:48.295336 13570 sgd_solver.cpp:136] Iteration 3000, lr = 0.01, m = 0.9
I0916 20:37:07.411907 13570 solver.cpp:314] Iteration 3100 (5.2312 iter/s, 19.1161s/100 iter), loss = 0.0875209
I0916 20:37:07.411931 13570 solver.cpp:336]     Train net output #0: loss = 0.0875207 (* 1 = 0.0875207 loss)
I0916 20:37:07.411937 13570 sgd_solver.cpp:136] Iteration 3100, lr = 0.01, m = 0.9
I0916 20:37:14.608629 13576 data_reader.cpp:305] Starting prefetch of epoch 2
I0916 20:37:26.809803 13570 solver.cpp:314] Iteration 3200 (5.15535 iter/s, 19.3973s/100 iter), loss = 0.195793
I0916 20:37:26.810758 13570 solver.cpp:336]     Train net output #0: loss = 0.195793 (* 1 = 0.195793 loss)
I0916 20:37:26.810828 13570 sgd_solver.cpp:136] Iteration 3200, lr = 0.01, m = 0.9
I0916 20:37:46.103037 13570 solver.cpp:314] Iteration 3300 (5.18331 iter/s, 19.2927s/100 iter), loss = 0.0855092
I0916 20:37:46.103061 13570 solver.cpp:336]     Train net output #0: loss = 0.085509 (* 1 = 0.085509 loss)
I0916 20:37:46.103067 13570 sgd_solver.cpp:136] Iteration 3300, lr = 0.01, m = 0.9
I0916 20:38:05.728325 13570 solver.cpp:314] Iteration 3400 (5.09561 iter/s, 19.6247s/100 iter), loss = 0.0499423
I0916 20:38:05.728379 13570 solver.cpp:336]     Train net output #0: loss = 0.0499421 (* 1 = 0.0499421 loss)
I0916 20:38:05.728384 13570 sgd_solver.cpp:136] Iteration 3400, lr = 0.01, m = 0.9
I0916 20:38:18.708616 13578 data_reader.cpp:305] Starting prefetch of epoch 3
I0916 20:38:25.220371 13570 solver.cpp:314] Iteration 3500 (5.13044 iter/s, 19.4915s/100 iter), loss = 0.13982
I0916 20:38:25.220398 13570 solver.cpp:336]     Train net output #0: loss = 0.13982 (* 1 = 0.13982 loss)
I0916 20:38:25.220404 13570 sgd_solver.cpp:136] Iteration 3500, lr = 0.01, m = 0.9
I0916 20:38:44.839262 13570 solver.cpp:314] Iteration 3600 (5.09727 iter/s, 19.6183s/100 iter), loss = 0.097058
I0916 20:38:44.839324 13570 solver.cpp:336]     Train net output #0: loss = 0.0970579 (* 1 = 0.0970579 loss)
I0916 20:38:44.839332 13570 sgd_solver.cpp:136] Iteration 3600, lr = 0.01, m = 0.9
I0916 20:38:51.470559 13574 data_reader.cpp:305] Starting prefetch of epoch 2
I0916 20:39:04.567071 13570 solver.cpp:314] Iteration 3700 (5.06913 iter/s, 19.7273s/100 iter), loss = 0.0785237
I0916 20:39:04.567095 13570 solver.cpp:336]     Train net output #0: loss = 0.0785236 (* 1 = 0.0785236 loss)
I0916 20:39:04.567101 13570 sgd_solver.cpp:136] Iteration 3700, lr = 0.01, m = 0.9
I0916 20:39:23.848279 13570 solver.cpp:314] Iteration 3800 (5.18654 iter/s, 19.2807s/100 iter), loss = 0.114514
I0916 20:39:23.848332 13570 solver.cpp:336]     Train net output #0: loss = 0.114514 (* 1 = 0.114514 loss)
I0916 20:39:23.848340 13570 sgd_solver.cpp:136] Iteration 3800, lr = 0.01, m = 0.9
I0916 20:39:43.713968 13570 solver.cpp:314] Iteration 3900 (5.03395 iter/s, 19.8651s/100 iter), loss = 0.0800754
I0916 20:39:43.713989 13570 solver.cpp:336]     Train net output #0: loss = 0.0800753 (* 1 = 0.0800753 loss)
I0916 20:39:43.713994 13570 sgd_solver.cpp:136] Iteration 3900, lr = 0.01, m = 0.9
I0916 20:39:55.976768 13574 data_reader.cpp:305] Starting prefetch of epoch 3
I0916 20:40:03.005611 13570 solver.cpp:424] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.81 sparsity_achieved=0.796027 iter=4000
I0916 20:40:03.208127 13571 blocking_queue.cpp:40] Data layer prefetch queue empty
I0916 20:40:21.903847 13596 data_reader.cpp:305] Starting prefetch of epoch 1
I0916 20:40:45.180069 13570 net.cpp:2245] All zero weights of convolution layers are frozen
I0916 20:40:45.184073 13570 solver.cpp:368] Sparsity after update:
I0916 20:40:45.185679 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 20:40:45.185686 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 20:40:45.185693 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 20:40:45.185694 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 20:40:45.185696 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 20:40:45.185698 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 20:40:45.185700 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 20:40:45.185703 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 20:40:45.185704 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 20:40:45.185705 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 20:40:45.185708 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 20:40:45.185709 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 20:40:45.185711 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 20:40:45.185714 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 20:40:45.185715 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 20:40:45.185717 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 20:40:45.185719 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 20:40:45.185721 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 20:40:45.185724 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 20:40:45.185732 13570 solver.cpp:563] Iteration 4000, Testing net (#0)
I0916 20:40:54.782819 13570 solver.cpp:655]     Test net output #0: accuracy/top1 = 0.949551
I0916 20:40:54.782840 13570 solver.cpp:655]     Test net output #1: accuracy/top5 = 1
I0916 20:40:54.782845 13570 solver.cpp:655]     Test net output #2: loss = 0.137472 (* 1 = 0.137472 loss)
I0916 20:40:54.782863 13570 solver.cpp:265] [MultiGPU] Tests completed in 9.59685s
I0916 20:40:54.995769 13570 solver.cpp:314] Iteration 4000 (1.40292 iter/s, 71.2798s/100 iter), loss = 0.0656239
I0916 20:40:54.995793 13570 solver.cpp:336]     Train net output #0: loss = 0.0656237 (* 1 = 0.0656237 loss)
I0916 20:40:54.995797 13570 sgd_solver.cpp:136] Iteration 4000, lr = 0.01, m = 0.9
I0916 20:41:13.925750 13570 solver.cpp:314] Iteration 4100 (5.28278 iter/s, 18.9294s/100 iter), loss = 0.0945595
I0916 20:41:13.925774 13570 solver.cpp:336]     Train net output #0: loss = 0.0945594 (* 1 = 0.0945594 loss)
I0916 20:41:13.925781 13570 sgd_solver.cpp:136] Iteration 4100, lr = 0.01, m = 0.9
I0916 20:41:19.247923 13574 data_reader.cpp:305] Starting prefetch of epoch 4
I0916 20:41:32.778542 13570 solver.cpp:314] Iteration 4200 (5.30441 iter/s, 18.8523s/100 iter), loss = 0.0783329
I0916 20:41:32.778563 13570 solver.cpp:336]     Train net output #0: loss = 0.0783328 (* 1 = 0.0783328 loss)
I0916 20:41:32.778566 13570 sgd_solver.cpp:136] Iteration 4200, lr = 0.01, m = 0.9
I0916 20:41:51.645117 13570 solver.cpp:314] Iteration 4300 (5.30053 iter/s, 18.866s/100 iter), loss = 0.0856312
I0916 20:41:51.645169 13570 solver.cpp:336]     Train net output #0: loss = 0.0856311 (* 1 = 0.0856311 loss)
I0916 20:41:51.645175 13570 sgd_solver.cpp:136] Iteration 4300, lr = 0.01, m = 0.9
I0916 20:42:10.694687 13570 solver.cpp:314] Iteration 4400 (5.24961 iter/s, 19.049s/100 iter), loss = 0.0738094
I0916 20:42:10.694710 13570 solver.cpp:336]     Train net output #0: loss = 0.0738093 (* 1 = 0.0738093 loss)
I0916 20:42:10.694715 13570 sgd_solver.cpp:136] Iteration 4400, lr = 0.01, m = 0.9
I0916 20:42:22.090618 13574 data_reader.cpp:305] Starting prefetch of epoch 5
I0916 20:42:29.873947 13570 solver.cpp:314] Iteration 4500 (5.21411 iter/s, 19.1787s/100 iter), loss = 0.053061
I0916 20:42:29.873977 13570 solver.cpp:336]     Train net output #0: loss = 0.0530608 (* 1 = 0.0530608 loss)
I0916 20:42:29.873982 13570 sgd_solver.cpp:136] Iteration 4500, lr = 0.01, m = 0.9
I0916 20:42:49.007956 13570 solver.cpp:314] Iteration 4600 (5.22644 iter/s, 19.1335s/100 iter), loss = 0.103944
I0916 20:42:49.007977 13570 solver.cpp:336]     Train net output #0: loss = 0.103944 (* 1 = 0.103944 loss)
I0916 20:42:49.007982 13570 sgd_solver.cpp:136] Iteration 4600, lr = 0.01, m = 0.9
I0916 20:43:08.154480 13570 solver.cpp:314] Iteration 4700 (5.22303 iter/s, 19.146s/100 iter), loss = 0.0785829
I0916 20:43:08.154556 13570 solver.cpp:336]     Train net output #0: loss = 0.0785828 (* 1 = 0.0785828 loss)
I0916 20:43:08.154562 13570 sgd_solver.cpp:136] Iteration 4700, lr = 0.01, m = 0.9
I0916 20:43:25.385401 13544 data_reader.cpp:305] Starting prefetch of epoch 3
I0916 20:43:27.274410 13570 solver.cpp:314] Iteration 4800 (5.23029 iter/s, 19.1194s/100 iter), loss = 0.109161
I0916 20:43:27.274435 13570 solver.cpp:336]     Train net output #0: loss = 0.109161 (* 1 = 0.109161 loss)
I0916 20:43:27.274441 13570 sgd_solver.cpp:136] Iteration 4800, lr = 0.01, m = 0.9
I0916 20:43:46.152457 13570 solver.cpp:314] Iteration 4900 (5.29731 iter/s, 18.8775s/100 iter), loss = 0.0804116
I0916 20:43:46.152510 13570 solver.cpp:336]     Train net output #0: loss = 0.0804114 (* 1 = 0.0804114 loss)
I0916 20:43:46.152516 13570 sgd_solver.cpp:136] Iteration 4900, lr = 0.01, m = 0.9
I0916 20:43:56.786227 13574 data_reader.cpp:305] Starting prefetch of epoch 6
I0916 20:44:05.225204 13570 solver.cpp:368] Sparsity after update:
I0916 20:44:05.235925 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 20:44:05.235936 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 20:44:05.235944 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 20:44:05.235947 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 20:44:05.235951 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 20:44:05.235954 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 20:44:05.235957 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 20:44:05.235960 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 20:44:05.235965 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 20:44:05.235967 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 20:44:05.235970 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 20:44:05.235973 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 20:44:05.235982 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 20:44:05.235988 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 20:44:05.235994 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 20:44:05.235999 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 20:44:05.236003 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 20:44:05.236007 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 20:44:05.236011 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 20:44:05.409207 13570 solver.cpp:314] Iteration 5000 (5.19313 iter/s, 19.2562s/100 iter), loss = 0.0795035
I0916 20:44:05.409235 13570 solver.cpp:336]     Train net output #0: loss = 0.0795033 (* 1 = 0.0795033 loss)
I0916 20:44:05.409242 13570 sgd_solver.cpp:136] Iteration 5000, lr = 0.01, m = 0.9
I0916 20:44:24.574054 13570 solver.cpp:314] Iteration 5100 (5.21803 iter/s, 19.1643s/100 iter), loss = 0.140494
I0916 20:44:24.574138 13570 solver.cpp:336]     Train net output #0: loss = 0.140494 (* 1 = 0.140494 loss)
I0916 20:44:24.574146 13570 sgd_solver.cpp:136] Iteration 5100, lr = 0.01, m = 0.9
I0916 20:44:43.988596 13570 solver.cpp:314] Iteration 5200 (5.15092 iter/s, 19.414s/100 iter), loss = 0.0838882
I0916 20:44:43.988622 13570 solver.cpp:336]     Train net output #0: loss = 0.0838881 (* 1 = 0.0838881 loss)
I0916 20:44:43.988626 13570 sgd_solver.cpp:136] Iteration 5200, lr = 0.01, m = 0.9
I0916 20:45:00.501806 13544 data_reader.cpp:305] Starting prefetch of epoch 4
I0916 20:45:03.330526 13570 solver.cpp:314] Iteration 5300 (5.17026 iter/s, 19.3414s/100 iter), loss = 0.0588417
I0916 20:45:03.330554 13570 solver.cpp:336]     Train net output #0: loss = 0.0588415 (* 1 = 0.0588415 loss)
I0916 20:45:03.330560 13570 sgd_solver.cpp:136] Iteration 5300, lr = 0.01, m = 0.9
I0916 20:45:22.737607 13570 solver.cpp:314] Iteration 5400 (5.1529 iter/s, 19.4065s/100 iter), loss = 0.0579417
I0916 20:45:22.737632 13570 solver.cpp:336]     Train net output #0: loss = 0.0579416 (* 1 = 0.0579416 loss)
I0916 20:45:22.737637 13570 sgd_solver.cpp:136] Iteration 5400, lr = 0.01, m = 0.9
I0916 20:45:42.013429 13570 solver.cpp:314] Iteration 5500 (5.18799 iter/s, 19.2753s/100 iter), loss = 0.0605065
I0916 20:45:42.013535 13570 solver.cpp:336]     Train net output #0: loss = 0.0605064 (* 1 = 0.0605064 loss)
I0916 20:45:42.013542 13570 sgd_solver.cpp:136] Iteration 5500, lr = 0.01, m = 0.9
I0916 20:46:01.388849 13570 solver.cpp:314] Iteration 5600 (5.16132 iter/s, 19.3749s/100 iter), loss = 0.0456507
I0916 20:46:01.388875 13570 solver.cpp:336]     Train net output #0: loss = 0.0456506 (* 1 = 0.0456506 loss)
I0916 20:46:01.388880 13570 sgd_solver.cpp:136] Iteration 5600, lr = 0.01, m = 0.9
I0916 20:46:04.574499 13576 data_reader.cpp:305] Starting prefetch of epoch 3
I0916 20:46:20.927963 13570 solver.cpp:314] Iteration 5700 (5.11808 iter/s, 19.5386s/100 iter), loss = 0.124146
I0916 20:46:20.928048 13570 solver.cpp:336]     Train net output #0: loss = 0.124146 (* 1 = 0.124146 loss)
I0916 20:46:20.928056 13570 sgd_solver.cpp:136] Iteration 5700, lr = 0.01, m = 0.9
I0916 20:46:36.535387 13544 data_reader.cpp:305] Starting prefetch of epoch 5
I0916 20:46:40.163218 13570 solver.cpp:314] Iteration 5800 (5.19893 iter/s, 19.2347s/100 iter), loss = 0.0687667
I0916 20:46:40.163239 13570 solver.cpp:336]     Train net output #0: loss = 0.0687666 (* 1 = 0.0687666 loss)
I0916 20:46:40.163244 13570 sgd_solver.cpp:136] Iteration 5800, lr = 0.01, m = 0.9
I0916 20:46:59.475793 13570 solver.cpp:314] Iteration 5900 (5.17812 iter/s, 19.312s/100 iter), loss = 0.135915
I0916 20:46:59.475850 13570 solver.cpp:336]     Train net output #0: loss = 0.135915 (* 1 = 0.135915 loss)
I0916 20:46:59.475857 13570 sgd_solver.cpp:136] Iteration 5900, lr = 0.01, m = 0.9
I0916 20:47:18.462354 13570 solver.cpp:368] Sparsity after update:
I0916 20:47:18.468255 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 20:47:18.468264 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 20:47:18.468272 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 20:47:18.468277 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 20:47:18.468281 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 20:47:18.468286 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 20:47:18.468288 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 20:47:18.468292 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 20:47:18.468297 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 20:47:18.468299 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 20:47:18.468304 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 20:47:18.468308 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 20:47:18.468312 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 20:47:18.468315 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 20:47:18.468318 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 20:47:18.468322 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 20:47:18.468325 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 20:47:18.468328 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 20:47:18.468333 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 20:47:18.468343 13570 solver.cpp:563] Iteration 6000, Testing net (#0)
I0916 20:47:35.940654 13596 data_reader.cpp:305] Starting prefetch of epoch 2
I0916 20:47:45.046319 13570 solver.cpp:655]     Test net output #0: accuracy/top1 = 0.951612
I0916 20:47:45.046341 13570 solver.cpp:655]     Test net output #1: accuracy/top5 = 1
I0916 20:47:45.046347 13570 solver.cpp:655]     Test net output #2: loss = 0.142208 (* 1 = 0.142208 loss)
I0916 20:47:45.046377 13570 solver.cpp:265] [MultiGPU] Tests completed in 26.5773s
I0916 20:47:45.240257 13570 solver.cpp:314] Iteration 6000 (2.18516 iter/s, 45.7632s/100 iter), loss = 0.0974221
I0916 20:47:45.240324 13570 solver.cpp:336]     Train net output #0: loss = 0.0974219 (* 1 = 0.0974219 loss)
I0916 20:47:45.240347 13570 sgd_solver.cpp:136] Iteration 6000, lr = 0.01, m = 0.9
I0916 20:48:04.514008 13570 solver.cpp:314] Iteration 6100 (5.18855 iter/s, 19.2732s/100 iter), loss = 0.0860933
I0916 20:48:04.514037 13570 solver.cpp:336]     Train net output #0: loss = 0.0860932 (* 1 = 0.0860932 loss)
I0916 20:48:04.514042 13570 sgd_solver.cpp:136] Iteration 6100, lr = 0.01, m = 0.9
I0916 20:48:23.972162 13570 solver.cpp:314] Iteration 6200 (5.13938 iter/s, 19.4576s/100 iter), loss = 0.0780754
I0916 20:48:23.972236 13570 solver.cpp:336]     Train net output #0: loss = 0.0780753 (* 1 = 0.0780753 loss)
I0916 20:48:23.972244 13570 sgd_solver.cpp:136] Iteration 6200, lr = 0.01, m = 0.9
I0916 20:48:38.791561 13576 data_reader.cpp:305] Starting prefetch of epoch 4
I0916 20:48:43.318373 13570 solver.cpp:314] Iteration 6300 (5.16912 iter/s, 19.3457s/100 iter), loss = 0.0723286
I0916 20:48:43.318403 13570 solver.cpp:336]     Train net output #0: loss = 0.0723285 (* 1 = 0.0723285 loss)
I0916 20:48:43.318408 13570 sgd_solver.cpp:136] Iteration 6300, lr = 0.01, m = 0.9
I0916 20:49:02.920280 13570 solver.cpp:314] Iteration 6400 (5.10169 iter/s, 19.6014s/100 iter), loss = 0.0976418
I0916 20:49:02.920341 13570 solver.cpp:336]     Train net output #0: loss = 0.0976417 (* 1 = 0.0976417 loss)
I0916 20:49:02.920346 13570 sgd_solver.cpp:136] Iteration 6400, lr = 0.01, m = 0.9
I0916 20:49:23.422045 13570 solver.cpp:314] Iteration 6500 (4.87777 iter/s, 20.5012s/100 iter), loss = 0.0920015
I0916 20:49:23.422070 13570 solver.cpp:336]     Train net output #0: loss = 0.0920014 (* 1 = 0.0920014 loss)
I0916 20:49:23.422082 13570 sgd_solver.cpp:136] Iteration 6500, lr = 0.01, m = 0.9
I0916 20:49:43.037266 13570 solver.cpp:314] Iteration 6600 (5.09822 iter/s, 19.6147s/100 iter), loss = 0.104437
I0916 20:49:43.037356 13570 solver.cpp:336]     Train net output #0: loss = 0.104437 (* 1 = 0.104437 loss)
I0916 20:49:43.037369 13570 sgd_solver.cpp:136] Iteration 6600, lr = 0.01, m = 0.9
I0916 20:49:44.603792 13576 data_reader.cpp:305] Starting prefetch of epoch 5
I0916 20:50:02.159435 13570 solver.cpp:314] Iteration 6700 (5.22968 iter/s, 19.1216s/100 iter), loss = 0.11876
I0916 20:50:02.159458 13570 solver.cpp:336]     Train net output #0: loss = 0.11876 (* 1 = 0.11876 loss)
I0916 20:50:02.159462 13570 sgd_solver.cpp:136] Iteration 6700, lr = 0.01, m = 0.9
I0916 20:50:16.145412 13544 data_reader.cpp:305] Starting prefetch of epoch 6
I0916 20:50:21.399891 13570 solver.cpp:314] Iteration 6800 (5.19753 iter/s, 19.2399s/100 iter), loss = 0.0470434
I0916 20:50:21.399914 13570 solver.cpp:336]     Train net output #0: loss = 0.0470433 (* 1 = 0.0470433 loss)
I0916 20:50:21.399919 13570 sgd_solver.cpp:136] Iteration 6800, lr = 0.01, m = 0.9
I0916 20:50:40.666465 13570 solver.cpp:314] Iteration 6900 (5.19048 iter/s, 19.266s/100 iter), loss = 0.0970809
I0916 20:50:40.666493 13570 solver.cpp:336]     Train net output #0: loss = 0.0970807 (* 1 = 0.0970807 loss)
I0916 20:50:40.666501 13570 sgd_solver.cpp:136] Iteration 6900, lr = 0.01, m = 0.9
I0916 20:50:59.665386 13570 solver.cpp:368] Sparsity after update:
I0916 20:50:59.688340 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 20:50:59.688361 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 20:50:59.688371 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 20:50:59.688374 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 20:50:59.688377 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 20:50:59.688380 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 20:50:59.688383 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 20:50:59.688390 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 20:50:59.688396 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 20:50:59.688403 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 20:50:59.688410 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 20:50:59.688417 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 20:50:59.688426 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 20:50:59.688436 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 20:50:59.688446 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 20:50:59.688452 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 20:50:59.688462 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 20:50:59.688472 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 20:50:59.688478 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 20:50:59.864994 13570 solver.cpp:314] Iteration 7000 (5.20888 iter/s, 19.198s/100 iter), loss = 0.121334
I0916 20:50:59.865022 13570 solver.cpp:336]     Train net output #0: loss = 0.121334 (* 1 = 0.121334 loss)
I0916 20:50:59.865030 13570 sgd_solver.cpp:136] Iteration 7000, lr = 0.01, m = 0.9
I0916 20:51:19.692831 13570 solver.cpp:314] Iteration 7100 (5.04356 iter/s, 19.8273s/100 iter), loss = 0.0898811
I0916 20:51:19.692854 13570 solver.cpp:336]     Train net output #0: loss = 0.089881 (* 1 = 0.089881 loss)
I0916 20:51:19.692862 13570 sgd_solver.cpp:136] Iteration 7100, lr = 0.01, m = 0.9
I0916 20:51:20.348786 13576 data_reader.cpp:305] Starting prefetch of epoch 6
I0916 20:51:39.245790 13570 solver.cpp:314] Iteration 7200 (5.11446 iter/s, 19.5524s/100 iter), loss = 0.0990547
I0916 20:51:39.245896 13570 solver.cpp:336]     Train net output #0: loss = 0.0990545 (* 1 = 0.0990545 loss)
I0916 20:51:39.245905 13570 sgd_solver.cpp:136] Iteration 7200, lr = 0.01, m = 0.9
I0916 20:51:58.723423 13570 solver.cpp:314] Iteration 7300 (5.13424 iter/s, 19.4771s/100 iter), loss = 0.0688318
I0916 20:51:58.723448 13570 solver.cpp:336]     Train net output #0: loss = 0.0688316 (* 1 = 0.0688316 loss)
I0916 20:51:58.723453 13570 sgd_solver.cpp:136] Iteration 7300, lr = 0.01, m = 0.9
I0916 20:52:18.059661 13570 solver.cpp:314] Iteration 7400 (5.17178 iter/s, 19.3357s/100 iter), loss = 0.135724
I0916 20:52:18.059708 13570 solver.cpp:336]     Train net output #0: loss = 0.135723 (* 1 = 0.135723 loss)
I0916 20:52:18.059715 13570 sgd_solver.cpp:136] Iteration 7400, lr = 0.01, m = 0.9
I0916 20:52:24.657635 13576 data_reader.cpp:305] Starting prefetch of epoch 7
I0916 20:52:37.601938 13570 solver.cpp:314] Iteration 7500 (5.11725 iter/s, 19.5417s/100 iter), loss = 0.0933494
I0916 20:52:37.601971 13570 solver.cpp:336]     Train net output #0: loss = 0.0933493 (* 1 = 0.0933493 loss)
I0916 20:52:37.601979 13570 sgd_solver.cpp:136] Iteration 7500, lr = 0.01, m = 0.9
I0916 20:52:57.212220 13573 data_reader.cpp:305] Starting prefetch of epoch 3
I0916 20:52:57.373646 13570 solver.cpp:314] Iteration 7600 (5.05787 iter/s, 19.7712s/100 iter), loss = 0.0627605
I0916 20:52:57.373672 13570 solver.cpp:336]     Train net output #0: loss = 0.0627603 (* 1 = 0.0627603 loss)
I0916 20:52:57.373678 13570 sgd_solver.cpp:136] Iteration 7600, lr = 0.01, m = 0.9
I0916 20:53:17.102192 13570 solver.cpp:314] Iteration 7700 (5.06894 iter/s, 19.728s/100 iter), loss = 0.0649859
I0916 20:53:17.102213 13570 solver.cpp:336]     Train net output #0: loss = 0.0649858 (* 1 = 0.0649858 loss)
I0916 20:53:17.102217 13570 sgd_solver.cpp:136] Iteration 7700, lr = 0.01, m = 0.9
I0916 20:53:36.600600 13570 solver.cpp:314] Iteration 7800 (5.12877 iter/s, 19.4979s/100 iter), loss = 0.118634
I0916 20:53:36.600662 13570 solver.cpp:336]     Train net output #0: loss = 0.118633 (* 1 = 0.118633 loss)
I0916 20:53:36.600672 13570 sgd_solver.cpp:136] Iteration 7800, lr = 0.01, m = 0.9
I0916 20:53:56.229694 13570 solver.cpp:314] Iteration 7900 (5.09462 iter/s, 19.6286s/100 iter), loss = 0.0764244
I0916 20:53:56.229717 13570 solver.cpp:336]     Train net output #0: loss = 0.0764243 (* 1 = 0.0764243 loss)
I0916 20:53:56.229722 13570 sgd_solver.cpp:136] Iteration 7900, lr = 0.01, m = 0.9
I0916 20:54:02.159503 13576 data_reader.cpp:305] Starting prefetch of epoch 8
I0916 20:54:15.745522 13570 solver.cpp:368] Sparsity after update:
I0916 20:54:15.749979 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 20:54:15.749989 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 20:54:15.749997 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 20:54:15.750001 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 20:54:15.750006 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 20:54:15.750010 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 20:54:15.750013 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 20:54:15.750017 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 20:54:15.750022 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 20:54:15.750026 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 20:54:15.750030 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 20:54:15.750033 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 20:54:15.750037 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 20:54:15.750041 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 20:54:15.750046 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 20:54:15.750048 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 20:54:15.750052 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 20:54:15.750056 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 20:54:15.750061 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 20:54:15.750079 13570 solver.cpp:563] Iteration 8000, Testing net (#0)
I0916 20:54:33.951526 13604 data_reader.cpp:305] Starting prefetch of epoch 1
I0916 20:54:34.362218 13570 solver.cpp:655]     Test net output #0: accuracy/top1 = 0.948542
I0916 20:54:34.362238 13570 solver.cpp:655]     Test net output #1: accuracy/top5 = 1
I0916 20:54:34.362243 13570 solver.cpp:655]     Test net output #2: loss = 0.139985 (* 1 = 0.139985 loss)
I0916 20:54:34.362262 13570 solver.cpp:265] [MultiGPU] Tests completed in 18.6117s
I0916 20:54:34.560808 13570 solver.cpp:314] Iteration 8000 (2.60892 iter/s, 38.3301s/100 iter), loss = 0.0943083
I0916 20:54:34.560830 13570 solver.cpp:336]     Train net output #0: loss = 0.0943081 (* 1 = 0.0943081 loss)
I0916 20:54:34.560837 13570 sgd_solver.cpp:136] Iteration 8000, lr = 0.01, m = 0.9
I0916 20:54:52.666412 13573 data_reader.cpp:305] Starting prefetch of epoch 4
I0916 20:54:53.617607 13570 solver.cpp:314] Iteration 8100 (5.24762 iter/s, 19.0563s/100 iter), loss = 0.0759972
I0916 20:54:53.617641 13570 solver.cpp:336]     Train net output #0: loss = 0.075997 (* 1 = 0.075997 loss)
I0916 20:54:53.617647 13570 sgd_solver.cpp:136] Iteration 8100, lr = 0.01, m = 0.9
I0916 20:55:12.998772 13570 solver.cpp:314] Iteration 8200 (5.15979 iter/s, 19.3806s/100 iter), loss = 0.0637579
I0916 20:55:12.998793 13570 solver.cpp:336]     Train net output #0: loss = 0.0637578 (* 1 = 0.0637578 loss)
I0916 20:55:12.998797 13570 sgd_solver.cpp:136] Iteration 8200, lr = 0.01, m = 0.9
I0916 20:55:32.567441 13570 solver.cpp:314] Iteration 8300 (5.11036 iter/s, 19.5681s/100 iter), loss = 0.136909
I0916 20:55:32.567596 13570 solver.cpp:336]     Train net output #0: loss = 0.136909 (* 1 = 0.136909 loss)
I0916 20:55:32.567615 13570 sgd_solver.cpp:136] Iteration 8300, lr = 0.01, m = 0.9
I0916 20:55:52.019778 13570 solver.cpp:314] Iteration 8400 (5.14091 iter/s, 19.4518s/100 iter), loss = 0.0724969
I0916 20:55:52.019803 13570 solver.cpp:336]     Train net output #0: loss = 0.0724967 (* 1 = 0.0724967 loss)
I0916 20:55:52.019809 13570 sgd_solver.cpp:136] Iteration 8400, lr = 0.01, m = 0.9
I0916 20:55:57.230567 13544 data_reader.cpp:305] Starting prefetch of epoch 7
I0916 20:56:11.614483 13570 solver.cpp:314] Iteration 8500 (5.10356 iter/s, 19.5942s/100 iter), loss = 0.058328
I0916 20:56:11.614544 13570 solver.cpp:336]     Train net output #0: loss = 0.0583278 (* 1 = 0.0583278 loss)
I0916 20:56:11.614552 13570 sgd_solver.cpp:136] Iteration 8500, lr = 0.01, m = 0.9
I0916 20:56:31.167523 13570 solver.cpp:314] Iteration 8600 (5.11444 iter/s, 19.5525s/100 iter), loss = 0.0976596
I0916 20:56:31.167548 13570 solver.cpp:336]     Train net output #0: loss = 0.0976595 (* 1 = 0.0976595 loss)
I0916 20:56:31.167554 13570 sgd_solver.cpp:136] Iteration 8600, lr = 0.01, m = 0.9
I0916 20:56:50.728050 13570 solver.cpp:314] Iteration 8700 (5.11248 iter/s, 19.56s/100 iter), loss = 0.0706931
I0916 20:56:50.728134 13570 solver.cpp:336]     Train net output #0: loss = 0.0706929 (* 1 = 0.0706929 loss)
I0916 20:56:50.728144 13570 sgd_solver.cpp:136] Iteration 8700, lr = 0.01, m = 0.9
I0916 20:57:01.528010 13546 data_reader.cpp:305] Starting prefetch of epoch 3
I0916 20:57:10.072648 13570 solver.cpp:314] Iteration 8800 (5.16955 iter/s, 19.3441s/100 iter), loss = 0.0807629
I0916 20:57:10.072671 13570 solver.cpp:336]     Train net output #0: loss = 0.0807628 (* 1 = 0.0807628 loss)
I0916 20:57:10.072676 13570 sgd_solver.cpp:136] Iteration 8800, lr = 0.01, m = 0.9
I0916 20:57:29.814123 13570 solver.cpp:314] Iteration 8900 (5.06562 iter/s, 19.7409s/100 iter), loss = 0.0671901
I0916 20:57:29.814265 13570 solver.cpp:336]     Train net output #0: loss = 0.0671899 (* 1 = 0.0671899 loss)
I0916 20:57:29.814283 13570 sgd_solver.cpp:136] Iteration 8900, lr = 0.01, m = 0.9
I0916 20:57:34.070785 13574 data_reader.cpp:305] Starting prefetch of epoch 7
I0916 20:57:49.365447 13570 solver.cpp:368] Sparsity after update:
I0916 20:57:49.386656 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 20:57:49.386675 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 20:57:49.386683 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 20:57:49.386687 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 20:57:49.386690 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 20:57:49.386693 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 20:57:49.386704 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 20:57:49.386713 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 20:57:49.386721 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 20:57:49.386729 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 20:57:49.386740 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 20:57:49.386746 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 20:57:49.386750 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 20:57:49.386759 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 20:57:49.386764 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 20:57:49.386766 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 20:57:49.386775 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 20:57:49.386785 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 20:57:49.386793 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 20:57:49.559442 13570 solver.cpp:314] Iteration 9000 (5.06463 iter/s, 19.7448s/100 iter), loss = 0.0760027
I0916 20:57:49.559468 13570 solver.cpp:336]     Train net output #0: loss = 0.0760026 (* 1 = 0.0760026 loss)
I0916 20:57:49.559474 13570 sgd_solver.cpp:136] Iteration 9000, lr = 0.01, m = 0.9
I0916 20:58:08.994613 13570 solver.cpp:314] Iteration 9100 (5.14545 iter/s, 19.4346s/100 iter), loss = 0.103087
I0916 20:58:08.994665 13570 solver.cpp:336]     Train net output #0: loss = 0.103086 (* 1 = 0.103086 loss)
I0916 20:58:08.994673 13570 sgd_solver.cpp:136] Iteration 9100, lr = 0.01, m = 0.9
I0916 20:58:28.720744 13570 solver.cpp:314] Iteration 9200 (5.06956 iter/s, 19.7256s/100 iter), loss = 0.0682991
I0916 20:58:28.720769 13570 solver.cpp:336]     Train net output #0: loss = 0.068299 (* 1 = 0.068299 loss)
I0916 20:58:28.720774 13570 sgd_solver.cpp:136] Iteration 9200, lr = 0.01, m = 0.9
I0916 20:58:39.045441 13544 data_reader.cpp:305] Starting prefetch of epoch 8
I0916 20:58:48.406755 13570 solver.cpp:314] Iteration 9300 (5.07989 iter/s, 19.6855s/100 iter), loss = 0.0983355
I0916 20:58:48.406780 13570 solver.cpp:336]     Train net output #0: loss = 0.0983353 (* 1 = 0.0983353 loss)
I0916 20:58:48.406783 13570 sgd_solver.cpp:136] Iteration 9300, lr = 0.01, m = 0.9
I0916 20:59:07.635238 13570 solver.cpp:314] Iteration 9400 (5.20076 iter/s, 19.2279s/100 iter), loss = 0.0957308
I0916 20:59:07.635262 13570 solver.cpp:336]     Train net output #0: loss = 0.0957307 (* 1 = 0.0957307 loss)
I0916 20:59:07.635265 13570 sgd_solver.cpp:136] Iteration 9400, lr = 0.01, m = 0.9
I0916 20:59:26.703521 13570 solver.cpp:314] Iteration 9500 (5.24446 iter/s, 19.0678s/100 iter), loss = 0.0671488
I0916 20:59:26.703585 13570 solver.cpp:336]     Train net output #0: loss = 0.0671486 (* 1 = 0.0671486 loss)
I0916 20:59:26.703594 13570 sgd_solver.cpp:136] Iteration 9500, lr = 0.01, m = 0.9
I0916 20:59:42.969084 13578 data_reader.cpp:305] Starting prefetch of epoch 4
I0916 20:59:46.224812 13570 solver.cpp:314] Iteration 9600 (5.12276 iter/s, 19.5207s/100 iter), loss = 0.0760185
I0916 20:59:46.224839 13570 solver.cpp:336]     Train net output #0: loss = 0.0760184 (* 1 = 0.0760184 loss)
I0916 20:59:46.224845 13570 sgd_solver.cpp:136] Iteration 9600, lr = 0.01, m = 0.9
I0916 21:00:05.707614 13570 solver.cpp:314] Iteration 9700 (5.13287 iter/s, 19.4823s/100 iter), loss = 0.065742
I0916 21:00:05.707686 13570 solver.cpp:336]     Train net output #0: loss = 0.0657418 (* 1 = 0.0657418 loss)
I0916 21:00:05.707693 13570 sgd_solver.cpp:136] Iteration 9700, lr = 0.01, m = 0.9
I0916 21:00:15.103310 13544 data_reader.cpp:305] Starting prefetch of epoch 9
I0916 21:00:24.978108 13570 solver.cpp:314] Iteration 9800 (5.18943 iter/s, 19.27s/100 iter), loss = 0.0613421
I0916 21:00:24.978133 13570 solver.cpp:336]     Train net output #0: loss = 0.061342 (* 1 = 0.061342 loss)
I0916 21:00:24.978139 13570 sgd_solver.cpp:136] Iteration 9800, lr = 0.01, m = 0.9
I0916 21:00:44.559478 13570 solver.cpp:314] Iteration 9900 (5.10704 iter/s, 19.5808s/100 iter), loss = 0.116828
I0916 21:00:44.559540 13570 solver.cpp:336]     Train net output #0: loss = 0.116828 (* 1 = 0.116828 loss)
I0916 21:00:44.559546 13570 sgd_solver.cpp:136] Iteration 9900, lr = 0.01, m = 0.9
I0916 21:01:04.027935 13570 solver.cpp:825] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/cityscapes5_jsegnet21v2_iter_10000.caffemodel
I0916 21:01:04.445638 13570 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/cityscapes5_jsegnet21v2_iter_10000.solverstate
I0916 21:01:04.451661 13570 solver.cpp:368] Sparsity after update:
I0916 21:01:04.453209 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 21:01:04.453218 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 21:01:04.453227 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 21:01:04.453233 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 21:01:04.453235 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 21:01:04.453248 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 21:01:04.453253 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 21:01:04.453258 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 21:01:04.453261 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 21:01:04.453265 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 21:01:04.453269 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 21:01:04.453274 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 21:01:04.453277 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 21:01:04.453281 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 21:01:04.453285 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 21:01:04.453289 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 21:01:04.453294 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 21:01:04.453299 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 21:01:04.453302 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 21:01:04.453315 13570 solver.cpp:563] Iteration 10000, Testing net (#0)
I0916 21:01:22.007033 13596 data_reader.cpp:305] Starting prefetch of epoch 3
I0916 21:01:29.677130 13570 solver.cpp:655]     Test net output #0: accuracy/top1 = 0.954075
I0916 21:01:29.677156 13570 solver.cpp:655]     Test net output #1: accuracy/top5 = 1
I0916 21:01:29.677162 13570 solver.cpp:655]     Test net output #2: loss = 0.141268 (* 1 = 0.141268 loss)
I0916 21:01:29.677184 13570 solver.cpp:265] [MultiGPU] Tests completed in 25.2232s
I0916 21:01:29.871860 13570 solver.cpp:314] Iteration 10000 (2.20696 iter/s, 45.3111s/100 iter), loss = 0.0656963
I0916 21:01:29.871886 13570 solver.cpp:336]     Train net output #0: loss = 0.0656961 (* 1 = 0.0656961 loss)
I0916 21:01:29.871891 13570 sgd_solver.cpp:136] Iteration 10000, lr = 0.01, m = 0.9
I0916 21:01:48.867985 13570 solver.cpp:314] Iteration 10100 (5.26438 iter/s, 18.9956s/100 iter), loss = 0.0723629
I0916 21:01:48.868010 13570 solver.cpp:336]     Train net output #0: loss = 0.0723627 (* 1 = 0.0723627 loss)
I0916 21:01:48.868015 13570 sgd_solver.cpp:136] Iteration 10100, lr = 0.01, m = 0.9
I0916 21:02:07.709514 13570 solver.cpp:314] Iteration 10200 (5.30757 iter/s, 18.841s/100 iter), loss = 0.130504
I0916 21:02:07.709594 13570 solver.cpp:336]     Train net output #0: loss = 0.130504 (* 1 = 0.130504 loss)
I0916 21:02:07.709599 13570 sgd_solver.cpp:136] Iteration 10200, lr = 0.01, m = 0.9
I0916 21:02:16.025394 13576 data_reader.cpp:305] Starting prefetch of epoch 9
I0916 21:02:26.596921 13570 solver.cpp:314] Iteration 10300 (5.29468 iter/s, 18.8869s/100 iter), loss = 0.0596175
I0916 21:02:26.596949 13570 solver.cpp:336]     Train net output #0: loss = 0.0596174 (* 1 = 0.0596174 loss)
I0916 21:02:26.596956 13570 sgd_solver.cpp:136] Iteration 10300, lr = 0.01, m = 0.9
I0916 21:02:45.835069 13570 solver.cpp:314] Iteration 10400 (5.19815 iter/s, 19.2376s/100 iter), loss = 0.0571445
I0916 21:02:45.835129 13570 solver.cpp:336]     Train net output #0: loss = 0.0571443 (* 1 = 0.0571443 loss)
I0916 21:02:45.835135 13570 sgd_solver.cpp:136] Iteration 10400, lr = 0.01, m = 0.9
I0916 21:02:47.611948 13574 data_reader.cpp:305] Starting prefetch of epoch 8
I0916 21:03:04.988039 13570 solver.cpp:314] Iteration 10500 (5.22127 iter/s, 19.1524s/100 iter), loss = 0.0594255
I0916 21:03:04.988066 13570 solver.cpp:336]     Train net output #0: loss = 0.0594253 (* 1 = 0.0594253 loss)
I0916 21:03:04.988073 13570 sgd_solver.cpp:136] Iteration 10500, lr = 0.01, m = 0.9
I0916 21:03:24.177434 13570 solver.cpp:314] Iteration 10600 (5.21136 iter/s, 19.1889s/100 iter), loss = 0.0869555
I0916 21:03:24.177510 13570 solver.cpp:336]     Train net output #0: loss = 0.0869553 (* 1 = 0.0869553 loss)
I0916 21:03:24.177518 13570 sgd_solver.cpp:136] Iteration 10600, lr = 0.01, m = 0.9
I0916 21:03:43.419411 13570 solver.cpp:314] Iteration 10700 (5.19712 iter/s, 19.2414s/100 iter), loss = 0.0519495
I0916 21:03:43.419435 13570 solver.cpp:336]     Train net output #0: loss = 0.0519493 (* 1 = 0.0519493 loss)
I0916 21:03:43.419440 13570 sgd_solver.cpp:136] Iteration 10700, lr = 0.01, m = 0.9
I0916 21:03:51.239315 13544 data_reader.cpp:305] Starting prefetch of epoch 10
I0916 21:04:02.940167 13570 solver.cpp:314] Iteration 10800 (5.1229 iter/s, 19.5202s/100 iter), loss = 0.088924
I0916 21:04:02.940219 13570 solver.cpp:336]     Train net output #0: loss = 0.0889238 (* 1 = 0.0889238 loss)
I0916 21:04:02.940227 13570 sgd_solver.cpp:136] Iteration 10800, lr = 0.01, m = 0.9
I0916 21:04:22.218364 13570 solver.cpp:314] Iteration 10900 (5.18735 iter/s, 19.2777s/100 iter), loss = 0.0477844
I0916 21:04:22.218387 13570 solver.cpp:336]     Train net output #0: loss = 0.0477843 (* 1 = 0.0477843 loss)
I0916 21:04:22.218395 13570 sgd_solver.cpp:136] Iteration 10900, lr = 0.01, m = 0.9
I0916 21:04:40.974375 13570 solver.cpp:368] Sparsity after update:
I0916 21:04:40.996049 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 21:04:40.996071 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 21:04:40.996079 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 21:04:40.996083 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 21:04:40.996086 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 21:04:40.996089 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 21:04:40.996093 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 21:04:40.996095 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 21:04:40.996099 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 21:04:40.996103 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 21:04:40.996105 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 21:04:40.996109 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 21:04:40.996111 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 21:04:40.996114 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 21:04:40.996117 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 21:04:40.996120 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 21:04:40.996124 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 21:04:40.996126 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 21:04:40.996130 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 21:04:41.168543 13570 solver.cpp:314] Iteration 11000 (5.27714 iter/s, 18.9497s/100 iter), loss = 0.0631827
I0916 21:04:41.168565 13570 solver.cpp:336]     Train net output #0: loss = 0.0631826 (* 1 = 0.0631826 loss)
I0916 21:04:41.168570 13570 sgd_solver.cpp:136] Iteration 11000, lr = 0.01, m = 0.9
I0916 21:04:54.841648 13578 data_reader.cpp:305] Starting prefetch of epoch 5
I0916 21:05:00.571024 13570 solver.cpp:314] Iteration 11100 (5.15412 iter/s, 19.4019s/100 iter), loss = 0.0860252
I0916 21:05:00.571048 13570 solver.cpp:336]     Train net output #0: loss = 0.0860251 (* 1 = 0.0860251 loss)
I0916 21:05:00.571053 13570 sgd_solver.cpp:136] Iteration 11100, lr = 0.01, m = 0.9
I0916 21:05:19.583137 13570 solver.cpp:314] Iteration 11200 (5.25995 iter/s, 19.0116s/100 iter), loss = 0.0768156
I0916 21:05:19.583210 13570 solver.cpp:336]     Train net output #0: loss = 0.0768155 (* 1 = 0.0768155 loss)
I0916 21:05:19.583217 13570 sgd_solver.cpp:136] Iteration 11200, lr = 0.01, m = 0.9
I0916 21:05:26.285393 13573 data_reader.cpp:305] Starting prefetch of epoch 5
I0916 21:05:38.695966 13570 solver.cpp:314] Iteration 11300 (5.23223 iter/s, 19.1123s/100 iter), loss = 0.103171
I0916 21:05:38.695991 13570 solver.cpp:336]     Train net output #0: loss = 0.10317 (* 1 = 0.10317 loss)
I0916 21:05:38.695997 13570 sgd_solver.cpp:136] Iteration 11300, lr = 0.01, m = 0.9
I0916 21:05:57.622658 13570 solver.cpp:314] Iteration 11400 (5.28369 iter/s, 18.9262s/100 iter), loss = 0.0588841
I0916 21:05:57.622711 13570 solver.cpp:336]     Train net output #0: loss = 0.0588839 (* 1 = 0.0588839 loss)
I0916 21:05:57.622720 13570 sgd_solver.cpp:136] Iteration 11400, lr = 0.01, m = 0.9
I0916 21:06:16.740694 13570 solver.cpp:314] Iteration 11500 (5.23081 iter/s, 19.1175s/100 iter), loss = 0.0792103
I0916 21:06:16.740717 13570 solver.cpp:336]     Train net output #0: loss = 0.0792102 (* 1 = 0.0792102 loss)
I0916 21:06:16.740721 13570 sgd_solver.cpp:136] Iteration 11500, lr = 0.01, m = 0.9
I0916 21:06:29.325417 13574 data_reader.cpp:305] Starting prefetch of epoch 9
I0916 21:06:35.784657 13570 solver.cpp:314] Iteration 11600 (5.25115 iter/s, 19.0434s/100 iter), loss = 0.10253
I0916 21:06:35.784687 13570 solver.cpp:336]     Train net output #0: loss = 0.10253 (* 1 = 0.10253 loss)
I0916 21:06:35.784693 13570 sgd_solver.cpp:136] Iteration 11600, lr = 0.01, m = 0.9
I0916 21:06:54.931403 13570 solver.cpp:314] Iteration 11700 (5.22296 iter/s, 19.1462s/100 iter), loss = 0.0467354
I0916 21:06:54.931428 13570 solver.cpp:336]     Train net output #0: loss = 0.0467353 (* 1 = 0.0467353 loss)
I0916 21:06:54.931432 13570 sgd_solver.cpp:136] Iteration 11700, lr = 0.01, m = 0.9
I0916 21:07:14.281004 13570 solver.cpp:314] Iteration 11800 (5.16821 iter/s, 19.3491s/100 iter), loss = 0.0561401
I0916 21:07:14.281993 13570 solver.cpp:336]     Train net output #0: loss = 0.05614 (* 1 = 0.05614 loss)
I0916 21:07:14.282002 13570 sgd_solver.cpp:136] Iteration 11800, lr = 0.01, m = 0.9
I0916 21:07:32.593854 13576 data_reader.cpp:305] Starting prefetch of epoch 10
I0916 21:07:33.309309 13570 solver.cpp:314] Iteration 11900 (5.25548 iter/s, 19.0278s/100 iter), loss = 0.0666576
I0916 21:07:33.309332 13570 solver.cpp:336]     Train net output #0: loss = 0.0666575 (* 1 = 0.0666575 loss)
I0916 21:07:33.309336 13570 sgd_solver.cpp:136] Iteration 11900, lr = 0.01, m = 0.9
I0916 21:07:52.164640 13570 solver.cpp:368] Sparsity after update:
I0916 21:07:52.167912 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 21:07:52.167919 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 21:07:52.167925 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 21:07:52.167928 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 21:07:52.167932 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 21:07:52.167934 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 21:07:52.167937 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 21:07:52.167939 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 21:07:52.167943 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 21:07:52.167945 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 21:07:52.167948 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 21:07:52.167953 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 21:07:52.167955 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 21:07:52.167960 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 21:07:52.167963 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 21:07:52.167968 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 21:07:52.167970 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 21:07:52.167973 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 21:07:52.167976 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 21:07:52.167987 13570 solver.cpp:563] Iteration 12000, Testing net (#0)
I0916 21:07:55.478461 13596 data_reader.cpp:305] Starting prefetch of epoch 4
I0916 21:08:06.599689 13570 solver.cpp:655]     Test net output #0: accuracy/top1 = 0.949717
I0916 21:08:06.599709 13570 solver.cpp:655]     Test net output #1: accuracy/top5 = 1
I0916 21:08:06.599714 13570 solver.cpp:655]     Test net output #2: loss = 0.144888 (* 1 = 0.144888 loss)
I0916 21:08:06.599738 13570 solver.cpp:265] [MultiGPU] Tests completed in 14.4313s
I0916 21:08:06.797066 13570 solver.cpp:314] Iteration 12000 (2.98625 iter/s, 33.4868s/100 iter), loss = 0.0449524
I0916 21:08:06.797087 13570 solver.cpp:336]     Train net output #0: loss = 0.0449523 (* 1 = 0.0449523 loss)
I0916 21:08:06.797091 13570 sgd_solver.cpp:136] Iteration 12000, lr = 0.01, m = 0.9
I0916 21:08:25.692981 13570 solver.cpp:314] Iteration 12100 (5.2923 iter/s, 18.8954s/100 iter), loss = 0.101199
I0916 21:08:25.693053 13570 solver.cpp:336]     Train net output #0: loss = 0.101199 (* 1 = 0.101199 loss)
I0916 21:08:25.693059 13570 sgd_solver.cpp:136] Iteration 12100, lr = 0.01, m = 0.9
I0916 21:08:44.979054 13570 solver.cpp:314] Iteration 12200 (5.18523 iter/s, 19.2855s/100 iter), loss = 0.0627368
I0916 21:08:44.979079 13570 solver.cpp:336]     Train net output #0: loss = 0.0627367 (* 1 = 0.0627367 loss)
I0916 21:08:44.979082 13570 sgd_solver.cpp:136] Iteration 12200, lr = 0.01, m = 0.9
I0916 21:08:50.178051 13546 data_reader.cpp:305] Starting prefetch of epoch 4
I0916 21:09:04.027740 13570 solver.cpp:314] Iteration 12300 (5.24985 iter/s, 19.0482s/100 iter), loss = 0.0584331
I0916 21:09:04.027788 13570 solver.cpp:336]     Train net output #0: loss = 0.058433 (* 1 = 0.058433 loss)
I0916 21:09:04.027794 13570 sgd_solver.cpp:136] Iteration 12300, lr = 0.01, m = 0.9
I0916 21:09:21.743950 13573 data_reader.cpp:305] Starting prefetch of epoch 6
I0916 21:09:23.253962 13570 solver.cpp:314] Iteration 12400 (5.20138 iter/s, 19.2257s/100 iter), loss = 0.0607438
I0916 21:09:23.253988 13570 solver.cpp:336]     Train net output #0: loss = 0.0607437 (* 1 = 0.0607437 loss)
I0916 21:09:23.253993 13570 sgd_solver.cpp:136] Iteration 12400, lr = 0.01, m = 0.9
I0916 21:09:42.479306 13570 solver.cpp:314] Iteration 12500 (5.20161 iter/s, 19.2248s/100 iter), loss = 0.0887276
I0916 21:09:42.479365 13570 solver.cpp:336]     Train net output #0: loss = 0.0887275 (* 1 = 0.0887275 loss)
I0916 21:09:42.479370 13570 sgd_solver.cpp:136] Iteration 12500, lr = 0.01, m = 0.9
I0916 21:10:01.794705 13570 solver.cpp:314] Iteration 12600 (5.17736 iter/s, 19.3149s/100 iter), loss = 0.0426616
I0916 21:10:01.794728 13570 solver.cpp:336]     Train net output #0: loss = 0.0426615 (* 1 = 0.0426615 loss)
I0916 21:10:01.794734 13570 sgd_solver.cpp:136] Iteration 12600, lr = 0.01, m = 0.9
I0916 21:10:20.833205 13570 solver.cpp:314] Iteration 12700 (5.25266 iter/s, 19.038s/100 iter), loss = 0.154003
I0916 21:10:20.833261 13570 solver.cpp:336]     Train net output #0: loss = 0.154003 (* 1 = 0.154003 loss)
I0916 21:10:20.833268 13570 sgd_solver.cpp:136] Iteration 12700, lr = 0.01, m = 0.9
I0916 21:10:25.381170 13544 data_reader.cpp:305] Starting prefetch of epoch 11
I0916 21:10:40.209805 13570 solver.cpp:314] Iteration 12800 (5.16101 iter/s, 19.3761s/100 iter), loss = 0.0517557
I0916 21:10:40.209830 13570 solver.cpp:336]     Train net output #0: loss = 0.0517556 (* 1 = 0.0517556 loss)
I0916 21:10:40.209833 13570 sgd_solver.cpp:136] Iteration 12800, lr = 0.01, m = 0.9
I0916 21:10:59.336345 13570 solver.cpp:314] Iteration 12900 (5.22848 iter/s, 19.126s/100 iter), loss = 0.0757267
I0916 21:10:59.336428 13570 solver.cpp:336]     Train net output #0: loss = 0.0757266 (* 1 = 0.0757266 loss)
I0916 21:10:59.336433 13570 sgd_solver.cpp:136] Iteration 12900, lr = 0.01, m = 0.9
I0916 21:11:18.564862 13570 solver.cpp:368] Sparsity after update:
I0916 21:11:18.575023 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 21:11:18.575039 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 21:11:18.575049 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 21:11:18.575052 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 21:11:18.575055 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 21:11:18.575059 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 21:11:18.575068 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 21:11:18.575073 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 21:11:18.575079 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 21:11:18.575083 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 21:11:18.575085 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 21:11:18.575088 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 21:11:18.575091 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 21:11:18.575094 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 21:11:18.575096 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 21:11:18.575101 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 21:11:18.575104 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 21:11:18.575107 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 21:11:18.575109 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 21:11:18.748404 13570 solver.cpp:314] Iteration 13000 (5.15158 iter/s, 19.4115s/100 iter), loss = 0.0557747
I0916 21:11:18.748436 13570 solver.cpp:336]     Train net output #0: loss = 0.0557745 (* 1 = 0.0557745 loss)
I0916 21:11:18.748442 13570 sgd_solver.cpp:136] Iteration 13000, lr = 0.01, m = 0.9
I0916 21:11:29.199301 13546 data_reader.cpp:305] Starting prefetch of epoch 5
I0916 21:11:38.599184 13570 solver.cpp:314] Iteration 13100 (5.03772 iter/s, 19.8502s/100 iter), loss = 0.0857933
I0916 21:11:38.599287 13570 solver.cpp:336]     Train net output #0: loss = 0.0857932 (* 1 = 0.0857932 loss)
I0916 21:11:38.599294 13570 sgd_solver.cpp:136] Iteration 13100, lr = 0.01, m = 0.9
I0916 21:11:58.224076 13570 solver.cpp:314] Iteration 13200 (5.09571 iter/s, 19.6243s/100 iter), loss = 0.0508496
I0916 21:11:58.224104 13570 solver.cpp:336]     Train net output #0: loss = 0.0508495 (* 1 = 0.0508495 loss)
I0916 21:11:58.224112 13570 sgd_solver.cpp:136] Iteration 13200, lr = 0.01, m = 0.9
I0916 21:12:02.025123 13544 data_reader.cpp:305] Starting prefetch of epoch 12
I0916 21:12:17.550422 13570 solver.cpp:314] Iteration 13300 (5.17443 iter/s, 19.3258s/100 iter), loss = 0.062153
I0916 21:12:17.550477 13570 solver.cpp:336]     Train net output #0: loss = 0.062153 (* 1 = 0.062153 loss)
I0916 21:12:17.550483 13570 sgd_solver.cpp:136] Iteration 13300, lr = 0.01, m = 0.9
I0916 21:12:37.070303 13570 solver.cpp:314] Iteration 13400 (5.12312 iter/s, 19.5193s/100 iter), loss = 0.0517103
I0916 21:12:37.070360 13570 solver.cpp:336]     Train net output #0: loss = 0.0517103 (* 1 = 0.0517103 loss)
I0916 21:12:37.070375 13570 sgd_solver.cpp:136] Iteration 13400, lr = 0.01, m = 0.9
I0916 21:12:56.845659 13570 solver.cpp:314] Iteration 13500 (5.05694 iter/s, 19.7748s/100 iter), loss = 0.0984877
I0916 21:12:56.845736 13570 solver.cpp:336]     Train net output #0: loss = 0.0984876 (* 1 = 0.0984876 loss)
I0916 21:12:56.845743 13570 sgd_solver.cpp:136] Iteration 13500, lr = 0.01, m = 0.9
I0916 21:13:06.383947 13578 data_reader.cpp:305] Starting prefetch of epoch 6
I0916 21:13:16.155647 13570 solver.cpp:314] Iteration 13600 (5.17881 iter/s, 19.3095s/100 iter), loss = 0.0755642
I0916 21:13:16.155673 13570 solver.cpp:336]     Train net output #0: loss = 0.0755642 (* 1 = 0.0755642 loss)
I0916 21:13:16.155680 13570 sgd_solver.cpp:136] Iteration 13600, lr = 0.01, m = 0.9
I0916 21:13:35.658274 13570 solver.cpp:314] Iteration 13700 (5.12766 iter/s, 19.5021s/100 iter), loss = 0.062329
I0916 21:13:35.658397 13570 solver.cpp:336]     Train net output #0: loss = 0.062329 (* 1 = 0.062329 loss)
I0916 21:13:35.658414 13570 sgd_solver.cpp:136] Iteration 13700, lr = 0.01, m = 0.9
I0916 21:13:55.145653 13570 solver.cpp:314] Iteration 13800 (5.13167 iter/s, 19.4868s/100 iter), loss = 0.0621055
I0916 21:13:55.145679 13570 solver.cpp:336]     Train net output #0: loss = 0.0621054 (* 1 = 0.0621054 loss)
I0916 21:13:55.145684 13570 sgd_solver.cpp:136] Iteration 13800, lr = 0.01, m = 0.9
I0916 21:14:10.939606 13576 data_reader.cpp:305] Starting prefetch of epoch 11
I0916 21:14:14.823935 13570 solver.cpp:314] Iteration 13900 (5.08189 iter/s, 19.6777s/100 iter), loss = 0.0690783
I0916 21:14:14.823961 13570 solver.cpp:336]     Train net output #0: loss = 0.0690783 (* 1 = 0.0690783 loss)
I0916 21:14:14.823966 13570 sgd_solver.cpp:136] Iteration 13900, lr = 0.01, m = 0.9
I0916 21:14:33.984786 13570 solver.cpp:368] Sparsity after update:
I0916 21:14:33.993494 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 21:14:33.993676 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 21:14:33.993778 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 21:14:33.993840 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 21:14:33.993858 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 21:14:33.993875 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 21:14:33.993891 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 21:14:33.993906 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 21:14:33.993921 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 21:14:33.993937 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 21:14:33.993953 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 21:14:33.993968 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 21:14:33.993983 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 21:14:33.993999 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 21:14:33.994014 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 21:14:33.994029 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 21:14:33.994045 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 21:14:33.994058 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 21:14:33.994083 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 21:14:33.994112 13570 solver.cpp:563] Iteration 14000, Testing net (#0)
I0916 21:14:45.217376 13604 data_reader.cpp:305] Starting prefetch of epoch 2
I0916 21:14:49.127707 13570 solver.cpp:655]     Test net output #0: accuracy/top1 = 0.948661
I0916 21:14:49.127725 13570 solver.cpp:655]     Test net output #1: accuracy/top5 = 1
I0916 21:14:49.127730 13570 solver.cpp:655]     Test net output #2: loss = 0.161936 (* 1 = 0.161936 loss)
I0916 21:14:49.127790 13570 solver.cpp:265] [MultiGPU] Tests completed in 15.1333s
I0916 21:14:49.338237 13570 solver.cpp:314] Iteration 14000 (2.89743 iter/s, 34.5133s/100 iter), loss = 0.110669
I0916 21:14:49.338260 13570 solver.cpp:336]     Train net output #0: loss = 0.110669 (* 1 = 0.110669 loss)
I0916 21:14:49.338266 13570 sgd_solver.cpp:136] Iteration 14000, lr = 0.01, m = 0.9
I0916 21:15:08.151237 13570 solver.cpp:314] Iteration 14100 (5.31562 iter/s, 18.8125s/100 iter), loss = 0.101745
I0916 21:15:08.151262 13570 solver.cpp:336]     Train net output #0: loss = 0.101745 (* 1 = 0.101745 loss)
I0916 21:15:08.151266 13570 sgd_solver.cpp:136] Iteration 14100, lr = 0.01, m = 0.9
I0916 21:15:27.527534 13570 solver.cpp:314] Iteration 14200 (5.16109 iter/s, 19.3758s/100 iter), loss = 0.0611564
I0916 21:15:27.527633 13570 solver.cpp:336]     Train net output #0: loss = 0.0611564 (* 1 = 0.0611564 loss)
I0916 21:15:27.527640 13570 sgd_solver.cpp:136] Iteration 14200, lr = 0.01, m = 0.9
I0916 21:15:29.481547 13576 data_reader.cpp:305] Starting prefetch of epoch 12
I0916 21:15:46.518436 13570 solver.cpp:314] Iteration 14300 (5.26583 iter/s, 18.9904s/100 iter), loss = 0.083291
I0916 21:15:46.518461 13570 solver.cpp:336]     Train net output #0: loss = 0.0832909 (* 1 = 0.0832909 loss)
I0916 21:15:46.518465 13570 sgd_solver.cpp:136] Iteration 14300, lr = 0.01, m = 0.9
I0916 21:16:05.594211 13570 solver.cpp:314] Iteration 14400 (5.2424 iter/s, 19.0752s/100 iter), loss = 0.0749015
I0916 21:16:05.594275 13570 solver.cpp:336]     Train net output #0: loss = 0.0749015 (* 1 = 0.0749015 loss)
I0916 21:16:05.594282 13570 sgd_solver.cpp:136] Iteration 14400, lr = 0.01, m = 0.9
I0916 21:16:24.527070 13570 solver.cpp:314] Iteration 14500 (5.28197 iter/s, 18.9323s/100 iter), loss = 0.0809877
I0916 21:16:24.527099 13570 solver.cpp:336]     Train net output #0: loss = 0.0809876 (* 1 = 0.0809876 loss)
I0916 21:16:24.527107 13570 sgd_solver.cpp:136] Iteration 14500, lr = 0.01, m = 0.9
I0916 21:16:32.353443 13578 data_reader.cpp:305] Starting prefetch of epoch 7
I0916 21:16:43.623692 13570 solver.cpp:314] Iteration 14600 (5.23667 iter/s, 19.0961s/100 iter), loss = 0.09436
I0916 21:16:43.623745 13570 solver.cpp:336]     Train net output #0: loss = 0.09436 (* 1 = 0.09436 loss)
I0916 21:16:43.623750 13570 sgd_solver.cpp:136] Iteration 14600, lr = 0.01, m = 0.9
I0916 21:17:02.716358 13570 solver.cpp:314] Iteration 14700 (5.23776 iter/s, 19.0921s/100 iter), loss = 0.0764878
I0916 21:17:02.716383 13570 solver.cpp:336]     Train net output #0: loss = 0.0764878 (* 1 = 0.0764878 loss)
I0916 21:17:02.716388 13570 sgd_solver.cpp:136] Iteration 14700, lr = 0.01, m = 0.9
I0916 21:17:03.951922 13573 data_reader.cpp:305] Starting prefetch of epoch 7
I0916 21:17:21.981930 13570 solver.cpp:314] Iteration 14800 (5.19075 iter/s, 19.265s/100 iter), loss = 0.0661202
I0916 21:17:21.981979 13570 solver.cpp:336]     Train net output #0: loss = 0.0661202 (* 1 = 0.0661202 loss)
I0916 21:17:21.981984 13570 sgd_solver.cpp:136] Iteration 14800, lr = 0.01, m = 0.9
I0916 21:17:40.898243 13570 solver.cpp:314] Iteration 14900 (5.28659 iter/s, 18.9158s/100 iter), loss = 0.105374
I0916 21:17:40.898272 13570 solver.cpp:336]     Train net output #0: loss = 0.105374 (* 1 = 0.105374 loss)
I0916 21:17:40.898279 13570 sgd_solver.cpp:136] Iteration 14900, lr = 0.01, m = 0.9
I0916 21:17:59.878660 13570 solver.cpp:368] Sparsity after update:
I0916 21:17:59.886874 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 21:17:59.886884 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 21:17:59.886891 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 21:17:59.886893 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 21:17:59.886895 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 21:17:59.886898 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 21:17:59.886899 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 21:17:59.886901 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 21:17:59.886905 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 21:17:59.886907 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 21:17:59.886910 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 21:17:59.886914 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 21:17:59.886916 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 21:17:59.886921 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 21:17:59.886924 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 21:17:59.886929 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 21:17:59.886931 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 21:17:59.886934 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 21:17:59.886940 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 21:18:00.059818 13570 solver.cpp:314] Iteration 15000 (5.21892 iter/s, 19.161s/100 iter), loss = 0.0581385
I0916 21:18:00.059839 13570 solver.cpp:336]     Train net output #0: loss = 0.0581384 (* 1 = 0.0581384 loss)
I0916 21:18:00.059844 13570 sgd_solver.cpp:136] Iteration 15000, lr = 0.01, m = 0.9
I0916 21:18:07.243908 13574 data_reader.cpp:305] Starting prefetch of epoch 10
I0916 21:18:19.244745 13570 solver.cpp:314] Iteration 15100 (5.21257 iter/s, 19.1844s/100 iter), loss = 0.105216
I0916 21:18:19.244768 13570 solver.cpp:336]     Train net output #0: loss = 0.105216 (* 1 = 0.105216 loss)
I0916 21:18:19.244774 13570 sgd_solver.cpp:136] Iteration 15100, lr = 0.01, m = 0.9
I0916 21:18:38.528995 13570 solver.cpp:314] Iteration 15200 (5.18572 iter/s, 19.2837s/100 iter), loss = 0.0863372
I0916 21:18:38.529533 13570 solver.cpp:336]     Train net output #0: loss = 0.0863371 (* 1 = 0.0863371 loss)
I0916 21:18:38.529543 13570 sgd_solver.cpp:136] Iteration 15200, lr = 0.01, m = 0.9
I0916 21:18:57.692705 13570 solver.cpp:314] Iteration 15300 (5.21834 iter/s, 19.1632s/100 iter), loss = 0.0408159
I0916 21:18:57.692729 13570 solver.cpp:336]     Train net output #0: loss = 0.0408159 (* 1 = 0.0408159 loss)
I0916 21:18:57.692734 13570 sgd_solver.cpp:136] Iteration 15300, lr = 0.01, m = 0.9
I0916 21:19:10.737325 13576 data_reader.cpp:305] Starting prefetch of epoch 13
I0916 21:19:17.161605 13570 solver.cpp:314] Iteration 15400 (5.13654 iter/s, 19.4684s/100 iter), loss = 0.0722576
I0916 21:19:17.161631 13570 solver.cpp:336]     Train net output #0: loss = 0.0722576 (* 1 = 0.0722576 loss)
I0916 21:19:17.161638 13570 sgd_solver.cpp:136] Iteration 15400, lr = 0.01, m = 0.9
I0916 21:19:36.473999 13570 solver.cpp:314] Iteration 15500 (5.17817 iter/s, 19.3119s/100 iter), loss = 0.0671703
I0916 21:19:36.474023 13570 solver.cpp:336]     Train net output #0: loss = 0.0671703 (* 1 = 0.0671703 loss)
I0916 21:19:36.474027 13570 sgd_solver.cpp:136] Iteration 15500, lr = 0.01, m = 0.9
I0916 21:19:42.916306 13544 data_reader.cpp:305] Starting prefetch of epoch 13
I0916 21:19:55.653688 13570 solver.cpp:314] Iteration 15600 (5.21399 iter/s, 19.1792s/100 iter), loss = 0.0885722
I0916 21:19:55.653713 13570 solver.cpp:336]     Train net output #0: loss = 0.0885721 (* 1 = 0.0885721 loss)
I0916 21:19:55.653719 13570 sgd_solver.cpp:136] Iteration 15600, lr = 0.01, m = 0.9
I0916 21:20:14.553793 13570 solver.cpp:314] Iteration 15700 (5.29112 iter/s, 18.8996s/100 iter), loss = 0.085968
I0916 21:20:14.553845 13570 solver.cpp:336]     Train net output #0: loss = 0.0859679 (* 1 = 0.0859679 loss)
I0916 21:20:14.553851 13570 sgd_solver.cpp:136] Iteration 15700, lr = 0.01, m = 0.9
I0916 21:20:33.524229 13570 solver.cpp:314] Iteration 15800 (5.27151 iter/s, 18.9699s/100 iter), loss = 0.0992197
I0916 21:20:33.524253 13570 solver.cpp:336]     Train net output #0: loss = 0.0992196 (* 1 = 0.0992196 loss)
I0916 21:20:33.524260 13570 sgd_solver.cpp:136] Iteration 15800, lr = 0.01, m = 0.9
I0916 21:20:45.567572 13574 data_reader.cpp:305] Starting prefetch of epoch 11
I0916 21:20:52.608018 13570 solver.cpp:314] Iteration 15900 (5.24019 iter/s, 19.0833s/100 iter), loss = 0.0543638
I0916 21:20:52.608043 13570 solver.cpp:336]     Train net output #0: loss = 0.0543637 (* 1 = 0.0543637 loss)
I0916 21:20:52.608049 13570 sgd_solver.cpp:136] Iteration 15900, lr = 0.01, m = 0.9
I0916 21:21:11.617687 13570 solver.cpp:368] Sparsity after update:
I0916 21:21:11.623600 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 21:21:11.623610 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 21:21:11.623617 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 21:21:11.623620 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 21:21:11.623621 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 21:21:11.623623 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 21:21:11.623625 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 21:21:11.623627 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 21:21:11.623630 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 21:21:11.623631 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 21:21:11.623636 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 21:21:11.623637 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 21:21:11.623641 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 21:21:11.623643 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 21:21:11.623646 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 21:21:11.623649 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 21:21:11.623652 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 21:21:11.623656 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 21:21:11.623659 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 21:21:11.623672 13570 solver.cpp:563] Iteration 16000, Testing net (#0)
I0916 21:21:28.668387 13604 data_reader.cpp:305] Starting prefetch of epoch 3
I0916 21:21:29.043851 13570 solver.cpp:655]     Test net output #0: accuracy/top1 = 0.945807
I0916 21:21:29.043869 13570 solver.cpp:655]     Test net output #1: accuracy/top5 = 1
I0916 21:21:29.043874 13570 solver.cpp:655]     Test net output #2: loss = 0.155071 (* 1 = 0.155071 loss)
I0916 21:21:29.043933 13570 solver.cpp:265] [MultiGPU] Tests completed in 17.4198s
I0916 21:21:29.242599 13570 solver.cpp:314] Iteration 16000 (2.72974 iter/s, 36.6336s/100 iter), loss = 0.082639
I0916 21:21:29.242625 13570 solver.cpp:336]     Train net output #0: loss = 0.0826389 (* 1 = 0.0826389 loss)
I0916 21:21:29.242630 13570 sgd_solver.cpp:136] Iteration 16000, lr = 0.01, m = 0.9
I0916 21:21:34.772779 13573 data_reader.cpp:305] Starting prefetch of epoch 8
I0916 21:21:48.625243 13570 solver.cpp:314] Iteration 16100 (5.1594 iter/s, 19.3821s/100 iter), loss = 0.0735314
I0916 21:21:48.625268 13570 solver.cpp:336]     Train net output #0: loss = 0.0735314 (* 1 = 0.0735314 loss)
I0916 21:21:48.625273 13570 sgd_solver.cpp:136] Iteration 16100, lr = 0.01, m = 0.9
I0916 21:22:08.140035 13570 solver.cpp:314] Iteration 16200 (5.12446 iter/s, 19.5142s/100 iter), loss = 0.104279
I0916 21:22:08.140082 13570 solver.cpp:336]     Train net output #0: loss = 0.104279 (* 1 = 0.104279 loss)
I0916 21:22:08.140089 13570 sgd_solver.cpp:136] Iteration 16200, lr = 0.01, m = 0.9
I0916 21:22:27.890805 13570 solver.cpp:314] Iteration 16300 (5.06324 iter/s, 19.7502s/100 iter), loss = 0.0691547
I0916 21:22:27.890830 13570 solver.cpp:336]     Train net output #0: loss = 0.0691546 (* 1 = 0.0691546 loss)
I0916 21:22:27.890836 13570 sgd_solver.cpp:136] Iteration 16300, lr = 0.01, m = 0.9
I0916 21:22:39.545912 13573 data_reader.cpp:305] Starting prefetch of epoch 9
I0916 21:22:47.539193 13570 solver.cpp:314] Iteration 16400 (5.08962 iter/s, 19.6478s/100 iter), loss = 0.0597028
I0916 21:22:47.539216 13570 solver.cpp:336]     Train net output #0: loss = 0.0597027 (* 1 = 0.0597027 loss)
I0916 21:22:47.539222 13570 sgd_solver.cpp:136] Iteration 16400, lr = 0.01, m = 0.9
I0916 21:23:06.896610 13570 solver.cpp:314] Iteration 16500 (5.16612 iter/s, 19.3569s/100 iter), loss = 0.060235
I0916 21:23:06.896637 13570 solver.cpp:336]     Train net output #0: loss = 0.0602349 (* 1 = 0.0602349 loss)
I0916 21:23:06.896644 13570 sgd_solver.cpp:136] Iteration 16500, lr = 0.01, m = 0.9
I0916 21:23:26.121656 13570 solver.cpp:314] Iteration 16600 (5.20169 iter/s, 19.2245s/100 iter), loss = 0.0709967
I0916 21:23:26.121738 13570 solver.cpp:336]     Train net output #0: loss = 0.0709967 (* 1 = 0.0709967 loss)
I0916 21:23:26.121747 13570 sgd_solver.cpp:136] Iteration 16600, lr = 0.01, m = 0.9
I0916 21:23:43.341223 13576 data_reader.cpp:305] Starting prefetch of epoch 14
I0916 21:23:45.266979 13570 solver.cpp:314] Iteration 16700 (5.22335 iter/s, 19.1448s/100 iter), loss = 0.0940214
I0916 21:23:45.267007 13570 solver.cpp:336]     Train net output #0: loss = 0.0940214 (* 1 = 0.0940214 loss)
I0916 21:23:45.267014 13570 sgd_solver.cpp:136] Iteration 16700, lr = 0.01, m = 0.9
I0916 21:24:04.671581 13570 solver.cpp:314] Iteration 16800 (5.15356 iter/s, 19.4041s/100 iter), loss = 0.0855164
I0916 21:24:04.671660 13570 solver.cpp:336]     Train net output #0: loss = 0.0855163 (* 1 = 0.0855163 loss)
I0916 21:24:04.671669 13570 sgd_solver.cpp:136] Iteration 16800, lr = 0.01, m = 0.9
I0916 21:24:15.223477 13544 data_reader.cpp:305] Starting prefetch of epoch 14
I0916 21:24:23.819828 13570 solver.cpp:314] Iteration 16900 (5.22256 iter/s, 19.1477s/100 iter), loss = 0.0649248
I0916 21:24:23.819854 13570 solver.cpp:336]     Train net output #0: loss = 0.0649248 (* 1 = 0.0649248 loss)
I0916 21:24:23.819859 13570 sgd_solver.cpp:136] Iteration 16900, lr = 0.01, m = 0.9
I0916 21:24:42.569404 13570 solver.cpp:368] Sparsity after update:
I0916 21:24:42.584130 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 21:24:42.584146 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 21:24:42.584152 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 21:24:42.584154 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 21:24:42.584156 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 21:24:42.584158 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 21:24:42.584161 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 21:24:42.584162 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 21:24:42.584164 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 21:24:42.584167 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 21:24:42.584172 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 21:24:42.584174 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 21:24:42.584177 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 21:24:42.584180 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 21:24:42.584193 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 21:24:42.584198 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 21:24:42.584203 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 21:24:42.584208 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 21:24:42.584213 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 21:24:42.754024 13570 solver.cpp:314] Iteration 17000 (5.2816 iter/s, 18.9337s/100 iter), loss = 0.144899
I0916 21:24:42.754046 13570 solver.cpp:336]     Train net output #0: loss = 0.144898 (* 1 = 0.144898 loss)
I0916 21:24:42.754052 13570 sgd_solver.cpp:136] Iteration 17000, lr = 0.01, m = 0.9
I0916 21:25:01.762704 13570 solver.cpp:314] Iteration 17100 (5.2609 iter/s, 19.0082s/100 iter), loss = 0.0826382
I0916 21:25:01.762727 13570 solver.cpp:336]     Train net output #0: loss = 0.0826381 (* 1 = 0.0826381 loss)
I0916 21:25:01.762732 13570 sgd_solver.cpp:136] Iteration 17100, lr = 0.01, m = 0.9
I0916 21:25:18.104800 13576 data_reader.cpp:305] Starting prefetch of epoch 15
I0916 21:25:20.989204 13570 solver.cpp:314] Iteration 17200 (5.2013 iter/s, 19.226s/100 iter), loss = 0.0878394
I0916 21:25:20.989229 13570 solver.cpp:336]     Train net output #0: loss = 0.0878393 (* 1 = 0.0878393 loss)
I0916 21:25:20.989235 13570 sgd_solver.cpp:136] Iteration 17200, lr = 0.01, m = 0.9
I0916 21:25:40.132177 13570 solver.cpp:314] Iteration 17300 (5.22399 iter/s, 19.1424s/100 iter), loss = 0.0560185
I0916 21:25:40.132201 13570 solver.cpp:336]     Train net output #0: loss = 0.0560184 (* 1 = 0.0560184 loss)
I0916 21:25:40.132208 13570 sgd_solver.cpp:136] Iteration 17300, lr = 0.01, m = 0.9
I0916 21:25:59.125465 13570 solver.cpp:314] Iteration 17400 (5.26516 iter/s, 18.9928s/100 iter), loss = 0.0525275
I0916 21:25:59.125519 13570 solver.cpp:336]     Train net output #0: loss = 0.0525274 (* 1 = 0.0525274 loss)
I0916 21:25:59.125524 13570 sgd_solver.cpp:136] Iteration 17400, lr = 0.01, m = 0.9
I0916 21:26:18.140785 13570 solver.cpp:314] Iteration 17500 (5.25906 iter/s, 19.0148s/100 iter), loss = 0.0486733
I0916 21:26:18.140807 13570 solver.cpp:336]     Train net output #0: loss = 0.0486732 (* 1 = 0.0486732 loss)
I0916 21:26:18.140811 13570 sgd_solver.cpp:136] Iteration 17500, lr = 0.01, m = 0.9
I0916 21:26:21.212550 13578 data_reader.cpp:305] Starting prefetch of epoch 8
I0916 21:26:37.301163 13570 solver.cpp:314] Iteration 17600 (5.21925 iter/s, 19.1598s/100 iter), loss = 0.081201
I0916 21:26:37.301249 13570 solver.cpp:336]     Train net output #0: loss = 0.0812009 (* 1 = 0.0812009 loss)
I0916 21:26:37.301256 13570 sgd_solver.cpp:136] Iteration 17600, lr = 0.01, m = 0.9
I0916 21:26:53.088529 13573 data_reader.cpp:305] Starting prefetch of epoch 10
I0916 21:26:56.729493 13570 solver.cpp:314] Iteration 17700 (5.14726 iter/s, 19.4278s/100 iter), loss = 0.0691581
I0916 21:26:56.729518 13570 solver.cpp:336]     Train net output #0: loss = 0.069158 (* 1 = 0.069158 loss)
I0916 21:26:56.729524 13570 sgd_solver.cpp:136] Iteration 17700, lr = 0.01, m = 0.9
I0916 21:27:16.184383 13570 solver.cpp:314] Iteration 17800 (5.14024 iter/s, 19.4543s/100 iter), loss = 0.0624589
I0916 21:27:16.184473 13570 solver.cpp:336]     Train net output #0: loss = 0.0624588 (* 1 = 0.0624588 loss)
I0916 21:27:16.184480 13570 sgd_solver.cpp:136] Iteration 17800, lr = 0.01, m = 0.9
I0916 21:27:35.619781 13570 solver.cpp:314] Iteration 17900 (5.14539 iter/s, 19.4349s/100 iter), loss = 0.0814559
I0916 21:27:35.619858 13570 solver.cpp:336]     Train net output #0: loss = 0.0814558 (* 1 = 0.0814558 loss)
I0916 21:27:35.619877 13570 sgd_solver.cpp:136] Iteration 17900, lr = 0.01, m = 0.9
I0916 21:27:54.752755 13570 solver.cpp:368] Sparsity after update:
I0916 21:27:54.762650 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 21:27:54.762703 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 21:27:54.762722 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 21:27:54.762734 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 21:27:54.762747 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 21:27:54.762758 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 21:27:54.762770 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 21:27:54.762783 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 21:27:54.762794 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 21:27:54.762806 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 21:27:54.762818 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 21:27:54.762830 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 21:27:54.762842 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 21:27:54.762854 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 21:27:54.762866 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 21:27:54.762878 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 21:27:54.762889 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 21:27:54.762902 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 21:27:54.762913 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 21:27:54.762936 13570 solver.cpp:563] Iteration 18000, Testing net (#0)
I0916 21:28:03.911329 13600 data_reader.cpp:305] Starting prefetch of epoch 1
I0916 21:28:07.836987 13570 solver.cpp:655]     Test net output #0: accuracy/top1 = 0.94925
I0916 21:28:07.837003 13570 solver.cpp:655]     Test net output #1: accuracy/top5 = 1
I0916 21:28:07.837008 13570 solver.cpp:655]     Test net output #2: loss = 0.165083 (* 1 = 0.165083 loss)
I0916 21:28:07.837035 13570 solver.cpp:265] [MultiGPU] Tests completed in 13.0737s
I0916 21:28:08.043509 13570 solver.cpp:314] Iteration 18000 (3.08425 iter/s, 32.4228s/100 iter), loss = 0.0665594
I0916 21:28:08.043530 13570 solver.cpp:336]     Train net output #0: loss = 0.0665593 (* 1 = 0.0665593 loss)
I0916 21:28:08.043534 13570 sgd_solver.cpp:136] Iteration 18000, lr = 0.01, m = 0.9
I0916 21:28:27.199630 13570 solver.cpp:314] Iteration 18100 (5.22041 iter/s, 19.1556s/100 iter), loss = 0.0658514
I0916 21:28:27.199684 13570 solver.cpp:336]     Train net output #0: loss = 0.0658513 (* 1 = 0.0658513 loss)
I0916 21:28:27.199690 13570 sgd_solver.cpp:136] Iteration 18100, lr = 0.01, m = 0.9
I0916 21:28:42.081938 13576 data_reader.cpp:305] Starting prefetch of epoch 16
I0916 21:28:46.473343 13570 solver.cpp:314] Iteration 18200 (5.18856 iter/s, 19.2732s/100 iter), loss = 0.0640518
I0916 21:28:46.473366 13570 solver.cpp:336]     Train net output #0: loss = 0.0640517 (* 1 = 0.0640517 loss)
I0916 21:28:46.473371 13570 sgd_solver.cpp:136] Iteration 18200, lr = 0.01, m = 0.9
I0916 21:29:05.772264 13570 solver.cpp:314] Iteration 18300 (5.18178 iter/s, 19.2984s/100 iter), loss = 0.0819666
I0916 21:29:05.772348 13570 solver.cpp:336]     Train net output #0: loss = 0.0819664 (* 1 = 0.0819664 loss)
I0916 21:29:05.772356 13570 sgd_solver.cpp:136] Iteration 18300, lr = 0.01, m = 0.9
I0916 21:29:13.939270 13574 data_reader.cpp:305] Starting prefetch of epoch 12
I0916 21:29:25.045374 13570 solver.cpp:314] Iteration 18400 (5.18872 iter/s, 19.2726s/100 iter), loss = 0.0614444
I0916 21:29:25.045397 13570 solver.cpp:336]     Train net output #0: loss = 0.0614442 (* 1 = 0.0614442 loss)
I0916 21:29:25.045403 13570 sgd_solver.cpp:136] Iteration 18400, lr = 0.01, m = 0.9
I0916 21:29:44.175652 13570 solver.cpp:314] Iteration 18500 (5.22746 iter/s, 19.1297s/100 iter), loss = 0.0910969
I0916 21:29:44.175724 13570 solver.cpp:336]     Train net output #0: loss = 0.0910968 (* 1 = 0.0910968 loss)
I0916 21:29:44.175731 13570 sgd_solver.cpp:136] Iteration 18500, lr = 0.01, m = 0.9
I0916 21:30:03.370318 13570 solver.cpp:314] Iteration 18600 (5.20992 iter/s, 19.1941s/100 iter), loss = 0.0834138
I0916 21:30:03.370347 13570 solver.cpp:336]     Train net output #0: loss = 0.0834137 (* 1 = 0.0834137 loss)
I0916 21:30:03.370352 13570 sgd_solver.cpp:136] Iteration 18600, lr = 0.01, m = 0.9
I0916 21:30:17.215520 13574 data_reader.cpp:305] Starting prefetch of epoch 13
I0916 21:30:22.266240 13570 solver.cpp:314] Iteration 18700 (5.29229 iter/s, 18.8954s/100 iter), loss = 0.0461382
I0916 21:30:22.266265 13570 solver.cpp:336]     Train net output #0: loss = 0.0461381 (* 1 = 0.0461381 loss)
I0916 21:30:22.266271 13570 sgd_solver.cpp:136] Iteration 18700, lr = 0.01, m = 0.9
I0916 21:30:41.509156 13570 solver.cpp:314] Iteration 18800 (5.19686 iter/s, 19.2424s/100 iter), loss = 0.186511
I0916 21:30:41.509186 13570 solver.cpp:336]     Train net output #0: loss = 0.186511 (* 1 = 0.186511 loss)
I0916 21:30:41.509191 13570 sgd_solver.cpp:136] Iteration 18800, lr = 0.01, m = 0.9
I0916 21:31:00.729890 13570 solver.cpp:314] Iteration 18900 (5.20286 iter/s, 19.2202s/100 iter), loss = 0.0713475
I0916 21:31:00.729946 13570 solver.cpp:336]     Train net output #0: loss = 0.0713473 (* 1 = 0.0713473 loss)
I0916 21:31:00.729953 13570 sgd_solver.cpp:136] Iteration 18900, lr = 0.01, m = 0.9
I0916 21:31:19.476356 13570 solver.cpp:368] Sparsity after update:
I0916 21:31:19.497900 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 21:31:19.497920 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 21:31:19.497928 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 21:31:19.497931 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 21:31:19.497934 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 21:31:19.497937 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 21:31:19.497948 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 21:31:19.498088 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 21:31:19.498092 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 21:31:19.498095 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 21:31:19.498106 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 21:31:19.498119 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 21:31:19.498124 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 21:31:19.498128 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 21:31:19.498138 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 21:31:19.498143 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 21:31:19.498147 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 21:31:19.498157 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 21:31:19.498162 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 21:31:19.667888 13570 solver.cpp:314] Iteration 19000 (5.28053 iter/s, 18.9375s/100 iter), loss = 0.130745
I0916 21:31:19.667913 13570 solver.cpp:336]     Train net output #0: loss = 0.130745 (* 1 = 0.130745 loss)
I0916 21:31:19.667918 13570 sgd_solver.cpp:136] Iteration 19000, lr = 0.01, m = 0.9
I0916 21:31:20.259241 13578 data_reader.cpp:305] Starting prefetch of epoch 9
I0916 21:31:38.842332 13570 solver.cpp:314] Iteration 19100 (5.21542 iter/s, 19.1739s/100 iter), loss = 0.0577735
I0916 21:31:38.842430 13570 solver.cpp:336]     Train net output #0: loss = 0.0577733 (* 1 = 0.0577733 loss)
I0916 21:31:38.842440 13570 sgd_solver.cpp:136] Iteration 19100, lr = 0.01, m = 0.9
I0916 21:31:52.132647 13574 data_reader.cpp:305] Starting prefetch of epoch 14
I0916 21:31:58.047056 13570 solver.cpp:314] Iteration 19200 (5.2072 iter/s, 19.2042s/100 iter), loss = 0.101265
I0916 21:31:58.047078 13570 solver.cpp:336]     Train net output #0: loss = 0.101264 (* 1 = 0.101264 loss)
I0916 21:31:58.047082 13570 sgd_solver.cpp:136] Iteration 19200, lr = 0.01, m = 0.9
I0916 21:32:17.266243 13570 solver.cpp:314] Iteration 19300 (5.20328 iter/s, 19.2187s/100 iter), loss = 0.0882747
I0916 21:32:17.266299 13570 solver.cpp:336]     Train net output #0: loss = 0.0882745 (* 1 = 0.0882745 loss)
I0916 21:32:17.266304 13570 sgd_solver.cpp:136] Iteration 19300, lr = 0.01, m = 0.9
I0916 21:32:36.505601 13570 solver.cpp:314] Iteration 19400 (5.19782 iter/s, 19.2388s/100 iter), loss = 0.109641
I0916 21:32:36.505625 13570 solver.cpp:336]     Train net output #0: loss = 0.109641 (* 1 = 0.109641 loss)
I0916 21:32:36.505630 13570 sgd_solver.cpp:136] Iteration 19400, lr = 0.01, m = 0.9
I0916 21:32:55.393311 13546 data_reader.cpp:305] Starting prefetch of epoch 6
I0916 21:32:55.560228 13570 solver.cpp:314] Iteration 19500 (5.24821 iter/s, 19.0541s/100 iter), loss = 0.0734464
I0916 21:32:55.560256 13570 solver.cpp:336]     Train net output #0: loss = 0.0734463 (* 1 = 0.0734463 loss)
I0916 21:32:55.560262 13570 sgd_solver.cpp:136] Iteration 19500, lr = 0.01, m = 0.9
I0916 21:33:14.916766 13570 solver.cpp:314] Iteration 19600 (5.16636 iter/s, 19.356s/100 iter), loss = 0.0778947
I0916 21:33:14.916790 13570 solver.cpp:336]     Train net output #0: loss = 0.0778946 (* 1 = 0.0778946 loss)
I0916 21:33:14.916796 13570 sgd_solver.cpp:136] Iteration 19600, lr = 0.01, m = 0.9
I0916 21:33:34.336952 13570 solver.cpp:314] Iteration 19700 (5.14942 iter/s, 19.4196s/100 iter), loss = 0.387243
I0916 21:33:34.337055 13570 solver.cpp:336]     Train net output #0: loss = 0.387243 (* 1 = 0.387243 loss)
I0916 21:33:34.337062 13570 sgd_solver.cpp:136] Iteration 19700, lr = 0.01, m = 0.9
I0916 21:33:53.717381 13570 solver.cpp:314] Iteration 19800 (5.15999 iter/s, 19.3799s/100 iter), loss = 0.0835243
I0916 21:33:53.717413 13570 solver.cpp:336]     Train net output #0: loss = 0.0835242 (* 1 = 0.0835242 loss)
I0916 21:33:53.717419 13570 sgd_solver.cpp:136] Iteration 19800, lr = 0.01, m = 0.9
I0916 21:33:59.541226 13578 data_reader.cpp:305] Starting prefetch of epoch 10
I0916 21:34:12.745961 13570 solver.cpp:314] Iteration 19900 (5.2554 iter/s, 19.0281s/100 iter), loss = 0.0680409
I0916 21:34:12.746011 13570 solver.cpp:336]     Train net output #0: loss = 0.0680408 (* 1 = 0.0680408 loss)
I0916 21:34:12.746017 13570 sgd_solver.cpp:136] Iteration 19900, lr = 0.01, m = 0.9
I0916 21:34:30.968166 13573 data_reader.cpp:305] Starting prefetch of epoch 11
I0916 21:34:31.704252 13570 solver.cpp:825] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/cityscapes5_jsegnet21v2_iter_20000.caffemodel
I0916 21:34:31.845266 13570 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/cityscapes5_jsegnet21v2_iter_20000.solverstate
I0916 21:34:31.860409 13570 solver.cpp:368] Sparsity after update:
I0916 21:34:31.865726 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 21:34:31.865738 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 21:34:31.865747 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 21:34:31.865751 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 21:34:31.865754 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 21:34:31.865767 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 21:34:31.865772 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 21:34:31.865777 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 21:34:31.865782 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 21:34:31.865787 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 21:34:31.865792 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 21:34:31.865797 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 21:34:31.865799 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 21:34:31.865803 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 21:34:31.865804 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 21:34:31.865808 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 21:34:31.865811 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 21:34:31.865816 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 21:34:31.865820 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 21:34:31.865833 13570 solver.cpp:563] Iteration 20000, Testing net (#0)
I0916 21:34:42.913796 13596 data_reader.cpp:305] Starting prefetch of epoch 5
I0916 21:34:43.198057 13570 solver.cpp:655]     Test net output #0: accuracy/top1 = 0.952837
I0916 21:34:43.198087 13570 solver.cpp:655]     Test net output #1: accuracy/top5 = 1
I0916 21:34:43.198093 13570 solver.cpp:655]     Test net output #2: loss = 0.136725 (* 1 = 0.136725 loss)
I0916 21:34:43.198119 13570 solver.cpp:265] [MultiGPU] Tests completed in 11.332s
I0916 21:34:43.416617 13570 solver.cpp:314] Iteration 20000 (3.26054 iter/s, 30.6698s/100 iter), loss = 0.0504049
I0916 21:34:43.416638 13570 solver.cpp:336]     Train net output #0: loss = 0.0504048 (* 1 = 0.0504048 loss)
I0916 21:34:43.416643 13570 sgd_solver.cpp:136] Iteration 20000, lr = 0.01, m = 0.9
I0916 21:35:02.599643 13570 solver.cpp:314] Iteration 20100 (5.21309 iter/s, 19.1825s/100 iter), loss = 0.0493404
I0916 21:35:02.599675 13570 solver.cpp:336]     Train net output #0: loss = 0.0493403 (* 1 = 0.0493403 loss)
I0916 21:35:02.599684 13570 sgd_solver.cpp:136] Iteration 20100, lr = 0.01, m = 0.9
I0916 21:35:21.690340 13570 solver.cpp:314] Iteration 20200 (5.2383 iter/s, 19.0902s/100 iter), loss = 0.0894915
I0916 21:35:21.690388 13570 solver.cpp:336]     Train net output #0: loss = 0.0894914 (* 1 = 0.0894914 loss)
I0916 21:35:21.690394 13570 sgd_solver.cpp:136] Iteration 20200, lr = 0.01, m = 0.9
I0916 21:35:40.663727 13570 solver.cpp:314] Iteration 20300 (5.27069 iter/s, 18.9729s/100 iter), loss = 0.065241
I0916 21:35:40.663753 13570 solver.cpp:336]     Train net output #0: loss = 0.065241 (* 1 = 0.065241 loss)
I0916 21:35:40.663756 13570 sgd_solver.cpp:136] Iteration 20300, lr = 0.01, m = 0.9
I0916 21:35:45.589361 13578 data_reader.cpp:305] Starting prefetch of epoch 11
I0916 21:35:59.836444 13570 solver.cpp:314] Iteration 20400 (5.21589 iter/s, 19.1722s/100 iter), loss = 0.0652461
I0916 21:35:59.836500 13570 solver.cpp:336]     Train net output #0: loss = 0.065246 (* 1 = 0.065246 loss)
I0916 21:35:59.836505 13570 sgd_solver.cpp:136] Iteration 20400, lr = 0.01, m = 0.9
I0916 21:36:17.032059 13544 data_reader.cpp:305] Starting prefetch of epoch 15
I0916 21:36:18.705744 13570 solver.cpp:314] Iteration 20500 (5.29976 iter/s, 18.8688s/100 iter), loss = 0.0628669
I0916 21:36:18.705768 13570 solver.cpp:336]     Train net output #0: loss = 0.0628668 (* 1 = 0.0628668 loss)
I0916 21:36:18.705775 13570 sgd_solver.cpp:136] Iteration 20500, lr = 0.01, m = 0.9
I0916 21:36:37.971015 13570 solver.cpp:314] Iteration 20600 (5.19083 iter/s, 19.2647s/100 iter), loss = 0.0573044
I0916 21:36:37.971122 13570 solver.cpp:336]     Train net output #0: loss = 0.0573044 (* 1 = 0.0573044 loss)
I0916 21:36:37.971128 13570 sgd_solver.cpp:136] Iteration 20600, lr = 0.01, m = 0.9
I0916 21:36:57.210741 13570 solver.cpp:314] Iteration 20700 (5.19772 iter/s, 19.2392s/100 iter), loss = 0.0561842
I0916 21:36:57.210769 13570 solver.cpp:336]     Train net output #0: loss = 0.0561841 (* 1 = 0.0561841 loss)
I0916 21:36:57.210777 13570 sgd_solver.cpp:136] Iteration 20700, lr = 0.01, m = 0.9
I0916 21:37:16.229621 13570 solver.cpp:314] Iteration 20800 (5.25808 iter/s, 19.0184s/100 iter), loss = 0.0440697
I0916 21:37:16.229673 13570 solver.cpp:336]     Train net output #0: loss = 0.0440697 (* 1 = 0.0440697 loss)
I0916 21:37:16.229681 13570 sgd_solver.cpp:136] Iteration 20800, lr = 0.01, m = 0.9
I0916 21:37:20.249635 13573 data_reader.cpp:305] Starting prefetch of epoch 12
I0916 21:37:35.115010 13570 solver.cpp:314] Iteration 20900 (5.29525 iter/s, 18.8849s/100 iter), loss = 0.0538882
I0916 21:37:35.115031 13570 solver.cpp:336]     Train net output #0: loss = 0.0538881 (* 1 = 0.0538881 loss)
I0916 21:37:35.115036 13570 sgd_solver.cpp:136] Iteration 20900, lr = 0.01, m = 0.9
I0916 21:37:54.361335 13570 solver.cpp:368] Sparsity after update:
I0916 21:37:54.370065 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 21:37:54.370079 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 21:37:54.370085 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 21:37:54.370087 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 21:37:54.370088 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 21:37:54.370090 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 21:37:54.370092 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 21:37:54.370095 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 21:37:54.370096 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 21:37:54.370098 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 21:37:54.370100 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 21:37:54.370102 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 21:37:54.370105 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 21:37:54.370105 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 21:37:54.370107 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 21:37:54.370110 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 21:37:54.370111 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 21:37:54.370113 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 21:37:54.370115 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 21:37:54.542327 13570 solver.cpp:314] Iteration 21000 (5.14753 iter/s, 19.4268s/100 iter), loss = 0.0918613
I0916 21:37:54.542353 13570 solver.cpp:336]     Train net output #0: loss = 0.0918613 (* 1 = 0.0918613 loss)
I0916 21:37:54.542359 13570 sgd_solver.cpp:136] Iteration 21000, lr = 0.01, m = 0.9
I0916 21:38:13.843861 13570 solver.cpp:314] Iteration 21100 (5.18108 iter/s, 19.301s/100 iter), loss = 0.0568948
I0916 21:38:13.843886 13570 solver.cpp:336]     Train net output #0: loss = 0.0568948 (* 1 = 0.0568948 loss)
I0916 21:38:13.843892 13570 sgd_solver.cpp:136] Iteration 21100, lr = 0.01, m = 0.9
I0916 21:38:23.852211 13578 data_reader.cpp:305] Starting prefetch of epoch 12
I0916 21:38:33.041299 13570 solver.cpp:314] Iteration 21200 (5.20917 iter/s, 19.1969s/100 iter), loss = 0.0946441
I0916 21:38:33.041381 13570 solver.cpp:336]     Train net output #0: loss = 0.0946441 (* 1 = 0.0946441 loss)
I0916 21:38:33.041388 13570 sgd_solver.cpp:136] Iteration 21200, lr = 0.01, m = 0.9
I0916 21:38:52.214087 13570 solver.cpp:314] Iteration 21300 (5.21587 iter/s, 19.1723s/100 iter), loss = 0.0806111
I0916 21:38:52.214110 13570 solver.cpp:336]     Train net output #0: loss = 0.080611 (* 1 = 0.080611 loss)
I0916 21:38:52.214114 13570 sgd_solver.cpp:136] Iteration 21300, lr = 0.01, m = 0.9
I0916 21:38:55.613209 13574 data_reader.cpp:305] Starting prefetch of epoch 15
I0916 21:39:11.492522 13570 solver.cpp:314] Iteration 21400 (5.18729 iter/s, 19.2779s/100 iter), loss = 0.0549161
I0916 21:39:11.492583 13570 solver.cpp:336]     Train net output #0: loss = 0.0549161 (* 1 = 0.0549161 loss)
I0916 21:39:11.492588 13570 sgd_solver.cpp:136] Iteration 21400, lr = 0.01, m = 0.9
I0916 21:39:30.673200 13570 solver.cpp:314] Iteration 21500 (5.21372 iter/s, 19.1801s/100 iter), loss = 0.0562319
I0916 21:39:30.673224 13570 solver.cpp:336]     Train net output #0: loss = 0.0562318 (* 1 = 0.0562318 loss)
I0916 21:39:30.673233 13570 sgd_solver.cpp:136] Iteration 21500, lr = 0.01, m = 0.9
I0916 21:39:49.763222 13570 solver.cpp:314] Iteration 21600 (5.23848 iter/s, 19.0895s/100 iter), loss = 0.0873113
I0916 21:39:49.763300 13570 solver.cpp:336]     Train net output #0: loss = 0.0873112 (* 1 = 0.0873112 loss)
I0916 21:39:49.763309 13570 sgd_solver.cpp:136] Iteration 21600, lr = 0.01, m = 0.9
I0916 21:39:58.999922 13544 data_reader.cpp:305] Starting prefetch of epoch 16
I0916 21:40:08.927502 13570 solver.cpp:314] Iteration 21700 (5.21819 iter/s, 19.1637s/100 iter), loss = 0.0702146
I0916 21:40:08.927527 13570 solver.cpp:336]     Train net output #0: loss = 0.0702145 (* 1 = 0.0702145 loss)
I0916 21:40:08.927533 13570 sgd_solver.cpp:136] Iteration 21700, lr = 0.01, m = 0.9
I0916 21:40:28.158970 13570 solver.cpp:314] Iteration 21800 (5.19996 iter/s, 19.2309s/100 iter), loss = 0.0755476
I0916 21:40:28.167224 13570 solver.cpp:336]     Train net output #0: loss = 0.0755475 (* 1 = 0.0755475 loss)
I0916 21:40:28.167264 13570 sgd_solver.cpp:136] Iteration 21800, lr = 0.01, m = 0.9
I0916 21:40:47.475782 13570 solver.cpp:314] Iteration 21900 (5.17698 iter/s, 19.3163s/100 iter), loss = 0.0616839
I0916 21:40:47.475811 13570 solver.cpp:336]     Train net output #0: loss = 0.0616838 (* 1 = 0.0616838 loss)
I0916 21:40:47.475818 13570 sgd_solver.cpp:136] Iteration 21900, lr = 0.01, m = 0.9
I0916 21:41:02.415021 13546 data_reader.cpp:305] Starting prefetch of epoch 7
I0916 21:41:06.370694 13570 solver.cpp:368] Sparsity after update:
I0916 21:41:06.377806 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 21:41:06.377817 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 21:41:06.377826 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 21:41:06.377830 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 21:41:06.377842 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 21:41:06.377851 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 21:41:06.377859 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 21:41:06.377872 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 21:41:06.377882 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 21:41:06.377892 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 21:41:06.377897 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 21:41:06.377907 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 21:41:06.377918 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 21:41:06.377923 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 21:41:06.377926 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 21:41:06.377934 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 21:41:06.377943 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 21:41:06.377948 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 21:41:06.377956 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 21:41:06.377970 13570 solver.cpp:563] Iteration 22000, Testing net (#0)
I0916 21:41:24.071929 13596 data_reader.cpp:305] Starting prefetch of epoch 6
I0916 21:41:30.516649 13570 solver.cpp:655]     Test net output #0: accuracy/top1 = 0.953003
I0916 21:41:30.516687 13570 solver.cpp:655]     Test net output #1: accuracy/top5 = 1
I0916 21:41:30.516692 13570 solver.cpp:655]     Test net output #2: loss = 0.14585 (* 1 = 0.14585 loss)
I0916 21:41:30.516719 13570 solver.cpp:265] [MultiGPU] Tests completed in 24.1381s
I0916 21:41:30.710103 13570 solver.cpp:314] Iteration 22000 (2.31304 iter/s, 43.2331s/100 iter), loss = 0.0924981
I0916 21:41:30.710132 13570 solver.cpp:336]     Train net output #0: loss = 0.092498 (* 1 = 0.092498 loss)
I0916 21:41:30.710139 13570 sgd_solver.cpp:136] Iteration 22000, lr = 0.01, m = 0.9
I0916 21:41:49.751557 13570 solver.cpp:314] Iteration 22100 (5.25185 iter/s, 19.0409s/100 iter), loss = 0.211635
I0916 21:41:49.751607 13570 solver.cpp:336]     Train net output #0: loss = 0.211635 (* 1 = 0.211635 loss)
I0916 21:41:49.751616 13570 sgd_solver.cpp:136] Iteration 22100, lr = 0.01, m = 0.9
I0916 21:42:08.817240 13570 solver.cpp:314] Iteration 22200 (5.24517 iter/s, 19.0651s/100 iter), loss = 0.0637696
I0916 21:42:08.817271 13570 solver.cpp:336]     Train net output #0: loss = 0.0637695 (* 1 = 0.0637695 loss)
I0916 21:42:08.817278 13570 sgd_solver.cpp:136] Iteration 22200, lr = 0.01, m = 0.9
I0916 21:42:28.049919 13570 solver.cpp:314] Iteration 22300 (5.19963 iter/s, 19.2321s/100 iter), loss = 0.0520868
I0916 21:42:28.049988 13570 solver.cpp:336]     Train net output #0: loss = 0.0520867 (* 1 = 0.0520867 loss)
I0916 21:42:28.049994 13570 sgd_solver.cpp:136] Iteration 22300, lr = 0.01, m = 0.9
I0916 21:42:29.888368 13546 data_reader.cpp:305] Starting prefetch of epoch 8
I0916 21:42:47.424401 13570 solver.cpp:314] Iteration 22400 (5.16157 iter/s, 19.3739s/100 iter), loss = 0.0849994
I0916 21:42:47.424430 13570 solver.cpp:336]     Train net output #0: loss = 0.0849994 (* 1 = 0.0849994 loss)
I0916 21:42:47.424437 13570 sgd_solver.cpp:136] Iteration 22400, lr = 0.01, m = 0.9
I0916 21:43:06.407027 13570 solver.cpp:314] Iteration 22500 (5.26812 iter/s, 18.9821s/100 iter), loss = 0.131864
I0916 21:43:06.407084 13570 solver.cpp:336]     Train net output #0: loss = 0.131864 (* 1 = 0.131864 loss)
I0916 21:43:06.407089 13570 sgd_solver.cpp:136] Iteration 22500, lr = 0.01, m = 0.9
I0916 21:43:25.668467 13570 solver.cpp:314] Iteration 22600 (5.19186 iter/s, 19.2609s/100 iter), loss = 0.0603681
I0916 21:43:25.668490 13570 solver.cpp:336]     Train net output #0: loss = 0.060368 (* 1 = 0.060368 loss)
I0916 21:43:25.668495 13570 sgd_solver.cpp:136] Iteration 22600, lr = 0.01, m = 0.9
I0916 21:43:33.326316 13576 data_reader.cpp:305] Starting prefetch of epoch 17
I0916 21:43:44.834463 13570 solver.cpp:314] Iteration 22700 (5.21772 iter/s, 19.1655s/100 iter), loss = 0.0834207
I0916 21:43:44.834518 13570 solver.cpp:336]     Train net output #0: loss = 0.0834206 (* 1 = 0.0834206 loss)
I0916 21:43:44.834523 13570 sgd_solver.cpp:136] Iteration 22700, lr = 0.01, m = 0.9
I0916 21:44:03.909570 13570 solver.cpp:314] Iteration 22800 (5.24258 iter/s, 19.0746s/100 iter), loss = 0.0445609
I0916 21:44:03.909596 13570 solver.cpp:336]     Train net output #0: loss = 0.0445608 (* 1 = 0.0445608 loss)
I0916 21:44:03.909602 13570 sgd_solver.cpp:136] Iteration 22800, lr = 0.01, m = 0.9
I0916 21:44:04.941170 13544 data_reader.cpp:305] Starting prefetch of epoch 17
I0916 21:44:23.108906 13570 solver.cpp:314] Iteration 22900 (5.20866 iter/s, 19.1988s/100 iter), loss = 0.0653097
I0916 21:44:23.108990 13570 solver.cpp:336]     Train net output #0: loss = 0.0653096 (* 1 = 0.0653096 loss)
I0916 21:44:23.108999 13570 sgd_solver.cpp:136] Iteration 22900, lr = 0.01, m = 0.9
I0916 21:44:41.995472 13570 solver.cpp:368] Sparsity after update:
I0916 21:44:42.013306 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 21:44:42.013330 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 21:44:42.013340 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 21:44:42.013344 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 21:44:42.013346 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 21:44:42.013368 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 21:44:42.013382 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 21:44:42.013392 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 21:44:42.013401 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 21:44:42.013408 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 21:44:42.013417 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 21:44:42.013422 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 21:44:42.013425 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 21:44:42.013433 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 21:44:42.013444 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 21:44:42.013449 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 21:44:42.013453 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 21:44:42.013461 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 21:44:42.013470 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 21:44:42.185396 13570 solver.cpp:314] Iteration 23000 (5.2422 iter/s, 19.076s/100 iter), loss = 0.0919014
I0916 21:44:42.185425 13570 solver.cpp:336]     Train net output #0: loss = 0.0919013 (* 1 = 0.0919013 loss)
I0916 21:44:42.185432 13570 sgd_solver.cpp:136] Iteration 23000, lr = 0.01, m = 0.9
I0916 21:45:01.338862 13570 solver.cpp:314] Iteration 23100 (5.22113 iter/s, 19.1529s/100 iter), loss = 0.0721674
I0916 21:45:01.338935 13570 solver.cpp:336]     Train net output #0: loss = 0.0721672 (* 1 = 0.0721672 loss)
I0916 21:45:01.338943 13570 sgd_solver.cpp:136] Iteration 23100, lr = 0.01, m = 0.9
I0916 21:45:08.221801 13574 data_reader.cpp:305] Starting prefetch of epoch 16
I0916 21:45:20.845337 13570 solver.cpp:314] Iteration 23200 (5.12665 iter/s, 19.5059s/100 iter), loss = 0.05273
I0916 21:45:20.845366 13570 solver.cpp:336]     Train net output #0: loss = 0.0527298 (* 1 = 0.0527298 loss)
I0916 21:45:20.845373 13570 sgd_solver.cpp:136] Iteration 23200, lr = 0.01, m = 0.9
I0916 21:45:40.041214 13570 solver.cpp:314] Iteration 23300 (5.2096 iter/s, 19.1953s/100 iter), loss = 0.0457525
I0916 21:45:40.041265 13570 solver.cpp:336]     Train net output #0: loss = 0.0457524 (* 1 = 0.0457524 loss)
I0916 21:45:40.041271 13570 sgd_solver.cpp:136] Iteration 23300, lr = 0.01, m = 0.9
I0916 21:45:59.289955 13570 solver.cpp:314] Iteration 23400 (5.19529 iter/s, 19.2482s/100 iter), loss = 0.0547238
I0916 21:45:59.289981 13570 solver.cpp:336]     Train net output #0: loss = 0.0547237 (* 1 = 0.0547237 loss)
I0916 21:45:59.289988 13570 sgd_solver.cpp:136] Iteration 23400, lr = 0.01, m = 0.9
I0916 21:46:11.885146 13578 data_reader.cpp:305] Starting prefetch of epoch 13
I0916 21:46:18.370645 13570 solver.cpp:314] Iteration 23500 (5.24105 iter/s, 19.0802s/100 iter), loss = 0.224558
I0916 21:46:18.370668 13570 solver.cpp:336]     Train net output #0: loss = 0.224558 (* 1 = 0.224558 loss)
I0916 21:46:18.370674 13570 sgd_solver.cpp:136] Iteration 23500, lr = 0.01, m = 0.9
I0916 21:46:37.618578 13570 solver.cpp:314] Iteration 23600 (5.19551 iter/s, 19.2474s/100 iter), loss = 0.0491775
I0916 21:46:37.618600 13570 solver.cpp:336]     Train net output #0: loss = 0.0491774 (* 1 = 0.0491774 loss)
I0916 21:46:37.618604 13570 sgd_solver.cpp:136] Iteration 23600, lr = 0.01, m = 0.9
I0916 21:46:43.604910 13574 data_reader.cpp:305] Starting prefetch of epoch 17
I0916 21:46:56.746809 13570 solver.cpp:314] Iteration 23700 (5.22802 iter/s, 19.1277s/100 iter), loss = 0.0639243
I0916 21:46:56.746836 13570 solver.cpp:336]     Train net output #0: loss = 0.0639242 (* 1 = 0.0639242 loss)
I0916 21:46:56.746843 13570 sgd_solver.cpp:136] Iteration 23700, lr = 0.01, m = 0.9
I0916 21:47:15.655366 13570 solver.cpp:314] Iteration 23800 (5.28876 iter/s, 18.908s/100 iter), loss = 0.106715
I0916 21:47:15.655419 13570 solver.cpp:336]     Train net output #0: loss = 0.106714 (* 1 = 0.106714 loss)
I0916 21:47:15.655423 13570 sgd_solver.cpp:136] Iteration 23800, lr = 0.01, m = 0.9
I0916 21:47:34.789366 13570 solver.cpp:314] Iteration 23900 (5.22644 iter/s, 19.1335s/100 iter), loss = 0.0650351
I0916 21:47:34.789392 13570 solver.cpp:336]     Train net output #0: loss = 0.0650351 (* 1 = 0.0650351 loss)
I0916 21:47:34.789398 13570 sgd_solver.cpp:136] Iteration 23900, lr = 0.01, m = 0.9
I0916 21:47:46.732476 13573 data_reader.cpp:305] Starting prefetch of epoch 13
I0916 21:47:53.727329 13570 solver.cpp:368] Sparsity after update:
I0916 21:47:53.733700 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 21:47:53.733710 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 21:47:53.733718 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 21:47:53.733722 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 21:47:53.733734 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 21:47:53.733748 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 21:47:53.733754 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 21:47:53.733758 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 21:47:53.733767 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 21:47:53.733777 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 21:47:53.733781 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 21:47:53.733785 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 21:47:53.733793 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 21:47:53.733798 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 21:47:53.733803 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 21:47:53.733805 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 21:47:53.733809 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 21:47:53.733817 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 21:47:53.733822 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 21:47:53.733839 13570 solver.cpp:563] Iteration 24000, Testing net (#0)
I0916 21:48:12.211954 13594 data_reader.cpp:305] Starting prefetch of epoch 1
I0916 21:48:12.554628 13570 solver.cpp:655]     Test net output #0: accuracy/top1 = 0.951081
I0916 21:48:12.554647 13570 solver.cpp:655]     Test net output #1: accuracy/top5 = 1
I0916 21:48:12.554652 13570 solver.cpp:655]     Test net output #2: loss = 0.142625 (* 1 = 0.142625 loss)
I0916 21:48:12.554687 13570 solver.cpp:265] [MultiGPU] Tests completed in 18.8203s
I0916 21:48:12.762351 13570 solver.cpp:314] Iteration 24000 (2.63352 iter/s, 37.9719s/100 iter), loss = 0.0680527
I0916 21:48:12.762382 13570 solver.cpp:336]     Train net output #0: loss = 0.0680526 (* 1 = 0.0680526 loss)
I0916 21:48:12.762388 13570 sgd_solver.cpp:136] Iteration 24000, lr = 0.01, m = 0.9
I0916 21:48:31.795590 13570 solver.cpp:314] Iteration 24100 (5.25411 iter/s, 19.0327s/100 iter), loss = 0.0882601
I0916 21:48:31.795691 13570 solver.cpp:336]     Train net output #0: loss = 0.08826 (* 1 = 0.08826 loss)
I0916 21:48:31.795703 13570 sgd_solver.cpp:136] Iteration 24100, lr = 0.01, m = 0.9
I0916 21:48:50.904335 13570 solver.cpp:314] Iteration 24200 (5.23335 iter/s, 19.1082s/100 iter), loss = 0.208525
I0916 21:48:50.904371 13570 solver.cpp:336]     Train net output #0: loss = 0.208525 (* 1 = 0.208525 loss)
I0916 21:48:50.904376 13570 sgd_solver.cpp:136] Iteration 24200, lr = 0.01, m = 0.9
I0916 21:49:08.606747 13546 data_reader.cpp:305] Starting prefetch of epoch 9
I0916 21:49:10.130388 13570 solver.cpp:314] Iteration 24300 (5.20142 iter/s, 19.2255s/100 iter), loss = 0.053348
I0916 21:49:10.130414 13570 solver.cpp:336]     Train net output #0: loss = 0.0533479 (* 1 = 0.0533479 loss)
I0916 21:49:10.130419 13570 sgd_solver.cpp:136] Iteration 24300, lr = 0.01, m = 0.9
I0916 21:49:29.229533 13570 solver.cpp:314] Iteration 24400 (5.23598 iter/s, 19.0986s/100 iter), loss = 0.0725523
I0916 21:49:29.229562 13570 solver.cpp:336]     Train net output #0: loss = 0.0725522 (* 1 = 0.0725522 loss)
I0916 21:49:29.229569 13570 sgd_solver.cpp:136] Iteration 24400, lr = 0.01, m = 0.9
I0916 21:49:40.523046 13573 data_reader.cpp:305] Starting prefetch of epoch 14
I0916 21:49:48.425725 13570 solver.cpp:314] Iteration 24500 (5.20951 iter/s, 19.1957s/100 iter), loss = 0.0479609
I0916 21:49:48.425747 13570 solver.cpp:336]     Train net output #0: loss = 0.0479607 (* 1 = 0.0479607 loss)
I0916 21:49:48.425751 13570 sgd_solver.cpp:136] Iteration 24500, lr = 0.01, m = 0.9
I0916 21:50:07.323480 13570 solver.cpp:314] Iteration 24600 (5.29178 iter/s, 18.8972s/100 iter), loss = 0.0959472
I0916 21:50:07.323509 13570 solver.cpp:336]     Train net output #0: loss = 0.0959471 (* 1 = 0.0959471 loss)
I0916 21:50:07.323516 13570 sgd_solver.cpp:136] Iteration 24600, lr = 0.01, m = 0.9
I0916 21:50:26.363586 13570 solver.cpp:314] Iteration 24700 (5.25222 iter/s, 19.0396s/100 iter), loss = 0.0558342
I0916 21:50:26.363644 13570 solver.cpp:336]     Train net output #0: loss = 0.055834 (* 1 = 0.055834 loss)
I0916 21:50:26.363651 13570 sgd_solver.cpp:136] Iteration 24700, lr = 0.01, m = 0.9
I0916 21:50:43.159019 13573 data_reader.cpp:305] Starting prefetch of epoch 15
I0916 21:50:45.467906 13570 solver.cpp:314] Iteration 24800 (5.23456 iter/s, 19.1038s/100 iter), loss = 0.0738396
I0916 21:50:45.467931 13570 solver.cpp:336]     Train net output #0: loss = 0.0738394 (* 1 = 0.0738394 loss)
I0916 21:50:45.467937 13570 sgd_solver.cpp:136] Iteration 24800, lr = 0.01, m = 0.9
I0916 21:51:04.645314 13570 solver.cpp:314] Iteration 24900 (5.21461 iter/s, 19.1769s/100 iter), loss = 0.0624406
I0916 21:51:04.645390 13570 solver.cpp:336]     Train net output #0: loss = 0.0624404 (* 1 = 0.0624404 loss)
I0916 21:51:04.645397 13570 sgd_solver.cpp:136] Iteration 24900, lr = 0.01, m = 0.9
I0916 21:51:23.746367 13570 solver.cpp:368] Sparsity after update:
I0916 21:51:23.771594 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 21:51:23.771608 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 21:51:23.771615 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 21:51:23.771616 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 21:51:23.771618 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 21:51:23.771620 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 21:51:23.771622 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 21:51:23.771625 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 21:51:23.771625 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 21:51:23.771627 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 21:51:23.771630 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 21:51:23.771631 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 21:51:23.771633 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 21:51:23.771636 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 21:51:23.771637 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 21:51:23.771639 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 21:51:23.771641 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 21:51:23.771643 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 21:51:23.771644 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 21:51:23.942112 13570 solver.cpp:314] Iteration 25000 (5.18235 iter/s, 19.2963s/100 iter), loss = 0.0798817
I0916 21:51:23.942139 13570 solver.cpp:336]     Train net output #0: loss = 0.0798816 (* 1 = 0.0798816 loss)
I0916 21:51:23.942144 13570 sgd_solver.cpp:136] Iteration 25000, lr = 0.01, m = 0.9
I0916 21:51:43.215754 13570 solver.cpp:314] Iteration 25100 (5.18858 iter/s, 19.2731s/100 iter), loss = 0.0627536
I0916 21:51:43.215839 13570 solver.cpp:336]     Train net output #0: loss = 0.0627535 (* 1 = 0.0627535 loss)
I0916 21:51:43.215847 13570 sgd_solver.cpp:136] Iteration 25100, lr = 0.01, m = 0.9
I0916 21:51:46.921615 13576 data_reader.cpp:305] Starting prefetch of epoch 18
I0916 21:52:02.643704 13570 solver.cpp:314] Iteration 25200 (5.14737 iter/s, 19.4274s/100 iter), loss = 0.080083
I0916 21:52:02.643728 13570 solver.cpp:336]     Train net output #0: loss = 0.0800828 (* 1 = 0.0800828 loss)
I0916 21:52:02.643733 13570 sgd_solver.cpp:136] Iteration 25200, lr = 0.01, m = 0.9
I0916 21:52:18.585271 13544 data_reader.cpp:305] Starting prefetch of epoch 18
I0916 21:52:21.577838 13570 solver.cpp:314] Iteration 25300 (5.28161 iter/s, 18.9336s/100 iter), loss = 0.0655013
I0916 21:52:21.577888 13570 solver.cpp:336]     Train net output #0: loss = 0.0655012 (* 1 = 0.0655012 loss)
I0916 21:52:21.577901 13570 sgd_solver.cpp:136] Iteration 25300, lr = 0.01, m = 0.9
I0916 21:52:40.883590 13570 solver.cpp:314] Iteration 25400 (5.17995 iter/s, 19.3052s/100 iter), loss = 0.0809528
I0916 21:52:40.883618 13570 solver.cpp:336]     Train net output #0: loss = 0.0809527 (* 1 = 0.0809527 loss)
I0916 21:52:40.883625 13570 sgd_solver.cpp:136] Iteration 25400, lr = 0.01, m = 0.9
I0916 21:53:00.165405 13570 solver.cpp:314] Iteration 25500 (5.18638 iter/s, 19.2813s/100 iter), loss = 0.0560368
I0916 21:53:00.165460 13570 solver.cpp:336]     Train net output #0: loss = 0.0560367 (* 1 = 0.0560367 loss)
I0916 21:53:00.165467 13570 sgd_solver.cpp:136] Iteration 25500, lr = 0.01, m = 0.9
I0916 21:53:19.388522 13570 solver.cpp:314] Iteration 25600 (5.20221 iter/s, 19.2226s/100 iter), loss = 0.0708832
I0916 21:53:19.388551 13570 solver.cpp:336]     Train net output #0: loss = 0.0708831 (* 1 = 0.0708831 loss)
I0916 21:53:19.388558 13570 sgd_solver.cpp:136] Iteration 25600, lr = 0.01, m = 0.9
I0916 21:53:22.241274 13573 data_reader.cpp:305] Starting prefetch of epoch 16
I0916 21:53:38.468493 13570 solver.cpp:314] Iteration 25700 (5.24124 iter/s, 19.0794s/100 iter), loss = 0.0466765
I0916 21:53:38.468605 13570 solver.cpp:336]     Train net output #0: loss = 0.0466764 (* 1 = 0.0466764 loss)
I0916 21:53:38.468612 13570 sgd_solver.cpp:136] Iteration 25700, lr = 0.01, m = 0.9
I0916 21:53:57.634166 13570 solver.cpp:314] Iteration 25800 (5.21781 iter/s, 19.1651s/100 iter), loss = 0.0648972
I0916 21:53:57.634189 13570 solver.cpp:336]     Train net output #0: loss = 0.0648971 (* 1 = 0.0648971 loss)
I0916 21:53:57.634194 13570 sgd_solver.cpp:136] Iteration 25800, lr = 0.01, m = 0.9
I0916 21:54:16.839838 13570 solver.cpp:314] Iteration 25900 (5.20694 iter/s, 19.2051s/100 iter), loss = 0.0485559
I0916 21:54:16.839887 13570 solver.cpp:336]     Train net output #0: loss = 0.0485558 (* 1 = 0.0485558 loss)
I0916 21:54:16.839893 13570 sgd_solver.cpp:136] Iteration 25900, lr = 0.01, m = 0.9
I0916 21:54:25.490916 13576 data_reader.cpp:305] Starting prefetch of epoch 19
I0916 21:54:35.947676 13570 solver.cpp:368] Sparsity after update:
I0916 21:54:35.953603 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 21:54:35.953613 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 21:54:35.953621 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 21:54:35.953634 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 21:54:35.953642 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 21:54:35.953651 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 21:54:35.953661 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 21:54:35.953671 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 21:54:35.953682 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 21:54:35.953688 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 21:54:35.953696 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 21:54:35.953707 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 21:54:35.953712 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 21:54:35.953716 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 21:54:35.953723 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 21:54:35.953732 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 21:54:35.953742 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 21:54:35.953749 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 21:54:35.953758 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 21:54:35.953771 13570 solver.cpp:563] Iteration 26000, Testing net (#0)
I0916 21:54:47.971915 13566 data_reader.cpp:305] Starting prefetch of epoch 2
I0916 21:54:51.984083 13570 solver.cpp:655]     Test net output #0: accuracy/top1 = 0.954274
I0916 21:54:51.984104 13570 solver.cpp:655]     Test net output #1: accuracy/top5 = 1
I0916 21:54:51.984109 13570 solver.cpp:655]     Test net output #2: loss = 0.157197 (* 1 = 0.157197 loss)
I0916 21:54:51.984128 13570 solver.cpp:265] [MultiGPU] Tests completed in 16.0299s
I0916 21:54:52.199852 13570 solver.cpp:314] Iteration 26000 (2.82813 iter/s, 35.359s/100 iter), loss = 0.0837243
I0916 21:54:52.199874 13570 solver.cpp:336]     Train net output #0: loss = 0.0837242 (* 1 = 0.0837242 loss)
I0916 21:54:52.199880 13570 sgd_solver.cpp:136] Iteration 26000, lr = 0.01, m = 0.9
I0916 21:55:11.005833 13570 solver.cpp:314] Iteration 26100 (5.31761 iter/s, 18.8055s/100 iter), loss = 0.0490528
I0916 21:55:11.005859 13570 solver.cpp:336]     Train net output #0: loss = 0.0490527 (* 1 = 0.0490527 loss)
I0916 21:55:11.005866 13570 sgd_solver.cpp:136] Iteration 26100, lr = 0.01, m = 0.9
I0916 21:55:30.242089 13570 solver.cpp:314] Iteration 26200 (5.19866 iter/s, 19.2357s/100 iter), loss = 0.086788
I0916 21:55:30.242147 13570 solver.cpp:336]     Train net output #0: loss = 0.0867879 (* 1 = 0.0867879 loss)
I0916 21:55:30.242152 13570 sgd_solver.cpp:136] Iteration 26200, lr = 0.01, m = 0.9
I0916 21:55:44.931910 13574 data_reader.cpp:305] Starting prefetch of epoch 18
I0916 21:55:49.457084 13570 solver.cpp:314] Iteration 26300 (5.20441 iter/s, 19.2145s/100 iter), loss = 0.0636526
I0916 21:55:49.457108 13570 solver.cpp:336]     Train net output #0: loss = 0.0636525 (* 1 = 0.0636525 loss)
I0916 21:55:49.457111 13570 sgd_solver.cpp:136] Iteration 26300, lr = 0.01, m = 0.9
I0916 21:56:08.643487 13570 solver.cpp:314] Iteration 26400 (5.21217 iter/s, 19.1859s/100 iter), loss = 0.0878349
I0916 21:56:08.643566 13570 solver.cpp:336]     Train net output #0: loss = 0.0878348 (* 1 = 0.0878348 loss)
I0916 21:56:08.643573 13570 sgd_solver.cpp:136] Iteration 26400, lr = 0.01, m = 0.9
I0916 21:56:27.849926 13570 solver.cpp:314] Iteration 26500 (5.20673 iter/s, 19.2059s/100 iter), loss = 0.0516267
I0916 21:56:27.849956 13570 solver.cpp:336]     Train net output #0: loss = 0.0516265 (* 1 = 0.0516265 loss)
I0916 21:56:27.849961 13570 sgd_solver.cpp:136] Iteration 26500, lr = 0.01, m = 0.9
I0916 21:56:46.870296 13570 solver.cpp:314] Iteration 26600 (5.25767 iter/s, 19.0198s/100 iter), loss = 0.0767204
I0916 21:56:46.870355 13570 solver.cpp:336]     Train net output #0: loss = 0.0767203 (* 1 = 0.0767203 loss)
I0916 21:56:46.870362 13570 sgd_solver.cpp:136] Iteration 26600, lr = 0.01, m = 0.9
I0916 21:56:48.040565 13578 data_reader.cpp:305] Starting prefetch of epoch 14
I0916 21:57:05.723798 13570 solver.cpp:314] Iteration 26700 (5.3042 iter/s, 18.853s/100 iter), loss = 0.0551436
I0916 21:57:05.723822 13570 solver.cpp:336]     Train net output #0: loss = 0.0551435 (* 1 = 0.0551435 loss)
I0916 21:57:05.723827 13570 sgd_solver.cpp:136] Iteration 26700, lr = 0.01, m = 0.9
I0916 21:57:19.131091 13573 data_reader.cpp:305] Starting prefetch of epoch 17
I0916 21:57:24.689271 13570 solver.cpp:314] Iteration 26800 (5.27289 iter/s, 18.9649s/100 iter), loss = 0.0956385
I0916 21:57:24.689298 13570 solver.cpp:336]     Train net output #0: loss = 0.0956383 (* 1 = 0.0956383 loss)
I0916 21:57:24.689304 13570 sgd_solver.cpp:136] Iteration 26800, lr = 0.01, m = 0.9
I0916 21:57:43.904880 13570 solver.cpp:314] Iteration 26900 (5.20425 iter/s, 19.2151s/100 iter), loss = 0.0544619
I0916 21:57:43.904902 13570 solver.cpp:336]     Train net output #0: loss = 0.0544617 (* 1 = 0.0544617 loss)
I0916 21:57:43.904906 13570 sgd_solver.cpp:136] Iteration 26900, lr = 0.01, m = 0.9
I0916 21:58:02.909230 13570 solver.cpp:368] Sparsity after update:
I0916 21:58:02.928885 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 21:58:02.928903 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 21:58:02.928913 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 21:58:02.928916 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 21:58:02.928920 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 21:58:02.928922 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 21:58:02.928926 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 21:58:02.928930 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 21:58:02.928935 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 21:58:02.928938 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 21:58:02.928942 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 21:58:02.928946 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 21:58:02.928949 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 21:58:02.928963 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 21:58:02.928967 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 21:58:02.928970 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 21:58:02.928973 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 21:58:02.928977 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 21:58:02.928979 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 21:58:03.101550 13570 solver.cpp:314] Iteration 27000 (5.20938 iter/s, 19.1961s/100 iter), loss = 0.127998
I0916 21:58:03.101572 13570 solver.cpp:336]     Train net output #0: loss = 0.127998 (* 1 = 0.127998 loss)
I0916 21:58:03.101578 13570 sgd_solver.cpp:136] Iteration 27000, lr = 0.01, m = 0.9
I0916 21:58:22.198768 13570 solver.cpp:314] Iteration 27100 (5.23651 iter/s, 19.0967s/100 iter), loss = 0.0706217
I0916 21:58:22.198792 13570 solver.cpp:336]     Train net output #0: loss = 0.0706216 (* 1 = 0.0706216 loss)
I0916 21:58:22.198796 13570 sgd_solver.cpp:136] Iteration 27100, lr = 0.01, m = 0.9
I0916 21:58:22.610453 13574 data_reader.cpp:305] Starting prefetch of epoch 19
I0916 21:58:41.060063 13570 solver.cpp:314] Iteration 27200 (5.30201 iter/s, 18.8608s/100 iter), loss = 0.0427336
I0916 21:58:41.060138 13570 solver.cpp:336]     Train net output #0: loss = 0.0427335 (* 1 = 0.0427335 loss)
I0916 21:58:41.060144 13570 sgd_solver.cpp:136] Iteration 27200, lr = 0.01, m = 0.9
I0916 21:59:00.267905 13570 solver.cpp:314] Iteration 27300 (5.20636 iter/s, 19.2073s/100 iter), loss = 0.0638019
I0916 21:59:00.267958 13570 solver.cpp:336]     Train net output #0: loss = 0.0638018 (* 1 = 0.0638018 loss)
I0916 21:59:00.267973 13570 sgd_solver.cpp:136] Iteration 27300, lr = 0.01, m = 0.9
I0916 21:59:19.436167 13570 solver.cpp:314] Iteration 27400 (5.2171 iter/s, 19.1677s/100 iter), loss = 0.138298
I0916 21:59:19.436269 13570 solver.cpp:336]     Train net output #0: loss = 0.138298 (* 1 = 0.138298 loss)
I0916 21:59:19.436276 13570 sgd_solver.cpp:136] Iteration 27400, lr = 0.01, m = 0.9
I0916 21:59:25.692271 13578 data_reader.cpp:305] Starting prefetch of epoch 15
I0916 21:59:38.637157 13570 solver.cpp:314] Iteration 27500 (5.20821 iter/s, 19.2005s/100 iter), loss = 0.0588674
I0916 21:59:38.637182 13570 solver.cpp:336]     Train net output #0: loss = 0.0588672 (* 1 = 0.0588672 loss)
I0916 21:59:38.637187 13570 sgd_solver.cpp:136] Iteration 27500, lr = 0.01, m = 0.9
I0916 21:59:57.576212 13544 data_reader.cpp:305] Starting prefetch of epoch 19
I0916 21:59:57.907987 13570 solver.cpp:314] Iteration 27600 (5.18933 iter/s, 19.2703s/100 iter), loss = 0.0782802
I0916 21:59:57.908012 13570 solver.cpp:336]     Train net output #0: loss = 0.0782801 (* 1 = 0.0782801 loss)
I0916 21:59:57.908018 13570 sgd_solver.cpp:136] Iteration 27600, lr = 0.01, m = 0.9
I0916 22:00:16.948281 13570 solver.cpp:314] Iteration 27700 (5.25217 iter/s, 19.0398s/100 iter), loss = 0.0938956
I0916 22:00:16.948307 13570 solver.cpp:336]     Train net output #0: loss = 0.0938955 (* 1 = 0.0938955 loss)
I0916 22:00:16.948314 13570 sgd_solver.cpp:136] Iteration 27700, lr = 0.01, m = 0.9
I0916 22:00:36.203696 13570 solver.cpp:314] Iteration 27800 (5.19349 iter/s, 19.2549s/100 iter), loss = 0.042848
I0916 22:00:36.203749 13570 solver.cpp:336]     Train net output #0: loss = 0.0428479 (* 1 = 0.0428479 loss)
I0916 22:00:36.203754 13570 sgd_solver.cpp:136] Iteration 27800, lr = 0.01, m = 0.9
I0916 22:00:55.266743 13570 solver.cpp:314] Iteration 27900 (5.2459 iter/s, 19.0625s/100 iter), loss = 0.066426
I0916 22:00:55.266768 13570 solver.cpp:336]     Train net output #0: loss = 0.0664259 (* 1 = 0.0664259 loss)
I0916 22:00:55.266775 13570 sgd_solver.cpp:136] Iteration 27900, lr = 0.01, m = 0.9
I0916 22:01:00.658792 13573 data_reader.cpp:305] Starting prefetch of epoch 18
I0916 22:01:14.298081 13570 solver.cpp:368] Sparsity after update:
I0916 22:01:14.305264 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 22:01:14.305275 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 22:01:14.305284 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 22:01:14.305292 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 22:01:14.305296 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 22:01:14.305300 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 22:01:14.305304 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 22:01:14.305306 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 22:01:14.305310 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 22:01:14.305312 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 22:01:14.305315 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 22:01:14.305320 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 22:01:14.305322 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 22:01:14.305325 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 22:01:14.305328 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 22:01:14.305331 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 22:01:14.305335 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 22:01:14.305337 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 22:01:14.305341 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 22:01:14.305351 13570 solver.cpp:563] Iteration 28000, Testing net (#0)
I0916 22:01:30.308620 13566 data_reader.cpp:305] Starting prefetch of epoch 3
I0916 22:01:30.700562 13570 solver.cpp:655]     Test net output #0: accuracy/top1 = 0.947207
I0916 22:01:30.700584 13570 solver.cpp:655]     Test net output #1: accuracy/top5 = 1
I0916 22:01:30.700589 13570 solver.cpp:655]     Test net output #2: loss = 0.154768 (* 1 = 0.154768 loss)
I0916 22:01:30.700620 13570 solver.cpp:265] [MultiGPU] Tests completed in 16.3948s
I0916 22:01:30.901274 13570 solver.cpp:314] Iteration 28000 (2.80634 iter/s, 35.6336s/100 iter), loss = 0.0797404
I0916 22:01:30.901300 13570 solver.cpp:336]     Train net output #0: loss = 0.0797403 (* 1 = 0.0797403 loss)
I0916 22:01:30.901305 13570 sgd_solver.cpp:136] Iteration 28000, lr = 0.01, m = 0.9
I0916 22:01:48.713807 13544 data_reader.cpp:305] Starting prefetch of epoch 20
I0916 22:01:49.859357 13570 solver.cpp:314] Iteration 28100 (5.27494 iter/s, 18.9576s/100 iter), loss = 0.0824409
I0916 22:01:49.859381 13570 solver.cpp:336]     Train net output #0: loss = 0.0824408 (* 1 = 0.0824408 loss)
I0916 22:01:49.859385 13570 sgd_solver.cpp:136] Iteration 28100, lr = 0.01, m = 0.9
I0916 22:02:08.909940 13570 solver.cpp:314] Iteration 28200 (5.24933 iter/s, 19.0501s/100 iter), loss = 0.0833704
I0916 22:02:08.909965 13570 solver.cpp:336]     Train net output #0: loss = 0.0833703 (* 1 = 0.0833703 loss)
I0916 22:02:08.909970 13570 sgd_solver.cpp:136] Iteration 28200, lr = 0.01, m = 0.9
I0916 22:02:28.048269 13570 solver.cpp:314] Iteration 28300 (5.22526 iter/s, 19.1378s/100 iter), loss = 0.0572312
I0916 22:02:28.048321 13570 solver.cpp:336]     Train net output #0: loss = 0.0572311 (* 1 = 0.0572311 loss)
I0916 22:02:28.048326 13570 sgd_solver.cpp:136] Iteration 28300, lr = 0.01, m = 0.9
I0916 22:02:47.368734 13570 solver.cpp:314] Iteration 28400 (5.176 iter/s, 19.3199s/100 iter), loss = 0.0768875
I0916 22:02:47.368760 13570 solver.cpp:336]     Train net output #0: loss = 0.0768874 (* 1 = 0.0768874 loss)
I0916 22:02:47.368767 13570 sgd_solver.cpp:136] Iteration 28400, lr = 0.01, m = 0.9
I0916 22:02:52.001020 13573 data_reader.cpp:305] Starting prefetch of epoch 19
I0916 22:03:06.417215 13570 solver.cpp:314] Iteration 28500 (5.24991 iter/s, 19.048s/100 iter), loss = 0.079399
I0916 22:03:06.417268 13570 solver.cpp:336]     Train net output #0: loss = 0.0793989 (* 1 = 0.0793989 loss)
I0916 22:03:06.417273 13570 sgd_solver.cpp:136] Iteration 28500, lr = 0.01, m = 0.9
I0916 22:03:25.746719 13570 solver.cpp:314] Iteration 28600 (5.17358 iter/s, 19.329s/100 iter), loss = 0.132582
I0916 22:03:25.746749 13570 solver.cpp:336]     Train net output #0: loss = 0.132582 (* 1 = 0.132582 loss)
I0916 22:03:25.746757 13570 sgd_solver.cpp:136] Iteration 28600, lr = 0.01, m = 0.9
I0916 22:03:45.076239 13570 solver.cpp:314] Iteration 28700 (5.17358 iter/s, 19.329s/100 iter), loss = 0.101226
I0916 22:03:45.076297 13570 solver.cpp:336]     Train net output #0: loss = 0.101226 (* 1 = 0.101226 loss)
I0916 22:03:45.076303 13570 sgd_solver.cpp:136] Iteration 28700, lr = 0.01, m = 0.9
I0916 22:03:55.520680 13578 data_reader.cpp:305] Starting prefetch of epoch 16
I0916 22:04:04.081390 13570 solver.cpp:314] Iteration 28800 (5.26188 iter/s, 19.0046s/100 iter), loss = 0.0604189
I0916 22:04:04.081411 13570 solver.cpp:336]     Train net output #0: loss = 0.0604188 (* 1 = 0.0604188 loss)
I0916 22:04:04.081415 13570 sgd_solver.cpp:136] Iteration 28800, lr = 0.01, m = 0.9
I0916 22:04:23.037956 13570 solver.cpp:314] Iteration 28900 (5.27536 iter/s, 18.956s/100 iter), loss = 0.139569
I0916 22:04:23.038008 13570 solver.cpp:336]     Train net output #0: loss = 0.139569 (* 1 = 0.139569 loss)
I0916 22:04:23.038012 13570 sgd_solver.cpp:136] Iteration 28900, lr = 0.01, m = 0.9
I0916 22:04:42.263219 13570 solver.cpp:368] Sparsity after update:
I0916 22:04:42.277962 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 22:04:42.277981 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 22:04:42.277988 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 22:04:42.277992 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 22:04:42.277994 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 22:04:42.277998 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 22:04:42.278002 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 22:04:42.278004 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 22:04:42.278007 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 22:04:42.278012 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 22:04:42.278017 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 22:04:42.278019 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 22:04:42.278023 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 22:04:42.278025 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 22:04:42.278028 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 22:04:42.278031 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 22:04:42.278034 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 22:04:42.278038 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 22:04:42.278041 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 22:04:42.458483 13570 solver.cpp:314] Iteration 29000 (5.14933 iter/s, 19.42s/100 iter), loss = 0.0658374
I0916 22:04:42.458505 13570 solver.cpp:336]     Train net output #0: loss = 0.0658373 (* 1 = 0.0658373 loss)
I0916 22:04:42.458509 13570 sgd_solver.cpp:136] Iteration 29000, lr = 0.01, m = 0.9
I0916 22:04:58.792340 13546 data_reader.cpp:305] Starting prefetch of epoch 10
I0916 22:05:01.537931 13570 solver.cpp:314] Iteration 29100 (5.24139 iter/s, 19.0789s/100 iter), loss = 0.0727181
I0916 22:05:01.537955 13570 solver.cpp:336]     Train net output #0: loss = 0.072718 (* 1 = 0.072718 loss)
I0916 22:05:01.537959 13570 sgd_solver.cpp:136] Iteration 29100, lr = 0.01, m = 0.9
I0916 22:05:20.775977 13570 solver.cpp:314] Iteration 29200 (5.19818 iter/s, 19.2375s/100 iter), loss = 0.0537239
I0916 22:05:20.776001 13570 solver.cpp:336]     Train net output #0: loss = 0.0537238 (* 1 = 0.0537238 loss)
I0916 22:05:20.776007 13570 sgd_solver.cpp:136] Iteration 29200, lr = 0.01, m = 0.9
I0916 22:05:30.432510 13573 data_reader.cpp:305] Starting prefetch of epoch 20
I0916 22:05:39.850898 13570 solver.cpp:314] Iteration 29300 (5.24263 iter/s, 19.0744s/100 iter), loss = 0.0524465
I0916 22:05:39.850922 13570 solver.cpp:336]     Train net output #0: loss = 0.0524464 (* 1 = 0.0524464 loss)
I0916 22:05:39.850926 13570 sgd_solver.cpp:136] Iteration 29300, lr = 0.01, m = 0.9
I0916 22:05:58.899166 13570 solver.cpp:314] Iteration 29400 (5.24997 iter/s, 19.0477s/100 iter), loss = 0.0575208
I0916 22:05:58.899194 13570 solver.cpp:336]     Train net output #0: loss = 0.0575208 (* 1 = 0.0575208 loss)
I0916 22:05:58.899199 13570 sgd_solver.cpp:136] Iteration 29400, lr = 0.01, m = 0.9
I0916 22:06:18.334082 13570 solver.cpp:314] Iteration 29500 (5.14552 iter/s, 19.4344s/100 iter), loss = 0.0959105
I0916 22:06:18.334139 13570 solver.cpp:336]     Train net output #0: loss = 0.0959104 (* 1 = 0.0959104 loss)
I0916 22:06:18.334146 13570 sgd_solver.cpp:136] Iteration 29500, lr = 0.01, m = 0.9
I0916 22:06:34.062237 13546 data_reader.cpp:305] Starting prefetch of epoch 11
I0916 22:06:37.621903 13570 solver.cpp:314] Iteration 29600 (5.18476 iter/s, 19.2873s/100 iter), loss = 0.06616
I0916 22:06:37.621925 13570 solver.cpp:336]     Train net output #0: loss = 0.0661599 (* 1 = 0.0661599 loss)
I0916 22:06:37.621930 13570 sgd_solver.cpp:136] Iteration 29600, lr = 0.01, m = 0.9
I0916 22:06:56.672991 13570 solver.cpp:314] Iteration 29700 (5.24919 iter/s, 19.0506s/100 iter), loss = 0.0592061
I0916 22:06:56.673040 13570 solver.cpp:336]     Train net output #0: loss = 0.059206 (* 1 = 0.059206 loss)
I0916 22:06:56.673044 13570 sgd_solver.cpp:136] Iteration 29700, lr = 0.01, m = 0.9
I0916 22:07:16.073288 13570 solver.cpp:314] Iteration 29800 (5.1547 iter/s, 19.3998s/100 iter), loss = 0.121634
I0916 22:07:16.073309 13570 solver.cpp:336]     Train net output #0: loss = 0.121634 (* 1 = 0.121634 loss)
I0916 22:07:16.073313 13570 sgd_solver.cpp:136] Iteration 29800, lr = 0.01, m = 0.9
I0916 22:07:35.531066 13570 solver.cpp:314] Iteration 29900 (5.13948 iter/s, 19.4572s/100 iter), loss = 0.0760968
I0916 22:07:35.531143 13570 solver.cpp:336]     Train net output #0: loss = 0.0760968 (* 1 = 0.0760968 loss)
I0916 22:07:35.531150 13570 sgd_solver.cpp:136] Iteration 29900, lr = 0.01, m = 0.9
I0916 22:07:37.832670 13578 data_reader.cpp:305] Starting prefetch of epoch 17
I0916 22:07:54.517983 13570 solver.cpp:825] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/cityscapes5_jsegnet21v2_iter_30000.caffemodel
I0916 22:07:54.669529 13570 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/cityscapes5_jsegnet21v2_iter_30000.solverstate
I0916 22:07:54.679026 13570 solver.cpp:368] Sparsity after update:
I0916 22:07:54.686975 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 22:07:54.687021 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 22:07:54.687039 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 22:07:54.687052 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 22:07:54.687062 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 22:07:54.687073 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 22:07:54.687084 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 22:07:54.687095 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 22:07:54.687106 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 22:07:54.687117 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 22:07:54.687129 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 22:07:54.687140 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 22:07:54.687151 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 22:07:54.687162 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 22:07:54.687173 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 22:07:54.687185 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 22:07:54.687196 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 22:07:54.687206 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 22:07:54.687217 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 22:07:54.687238 13570 solver.cpp:563] Iteration 30000, Testing net (#0)
I0916 22:08:01.274153 13568 data_reader.cpp:305] Starting prefetch of epoch 1
I0916 22:08:06.306603 13570 solver.cpp:655]     Test net output #0: accuracy/top1 = 0.952079
I0916 22:08:06.306705 13570 solver.cpp:655]     Test net output #1: accuracy/top5 = 1
I0916 22:08:06.306715 13570 solver.cpp:655]     Test net output #2: loss = 0.158065 (* 1 = 0.158065 loss)
I0916 22:08:06.306735 13570 solver.cpp:265] [MultiGPU] Tests completed in 11.6192s
I0916 22:08:06.422168 13625 sgd_solver.cpp:48] MultiStep Status: Iteration 30000, step = 1
I0916 22:08:06.422168 13623 sgd_solver.cpp:48] MultiStep Status: Iteration 30000, step = 1
I0916 22:08:06.422258 13624 sgd_solver.cpp:48] MultiStep Status: Iteration 30000, step = 1
I0916 22:08:06.516161 13570 solver.cpp:314] Iteration 30000 (3.22745 iter/s, 30.9842s/100 iter), loss = 0.109009
I0916 22:08:06.516189 13570 solver.cpp:336]     Train net output #0: loss = 0.109009 (* 1 = 0.109009 loss)
I0916 22:08:06.516193 13570 sgd_solver.cpp:136] Iteration 30000, lr = 0.001, m = 0.9
I0916 22:08:25.553102 13570 solver.cpp:314] Iteration 30100 (5.25309 iter/s, 19.0364s/100 iter), loss = 0.0573109
I0916 22:08:25.553135 13570 solver.cpp:336]     Train net output #0: loss = 0.0573108 (* 1 = 0.0573108 loss)
I0916 22:08:25.553141 13570 sgd_solver.cpp:136] Iteration 30100, lr = 0.001, m = 0.9
I0916 22:08:44.690786 13570 solver.cpp:314] Iteration 30200 (5.22544 iter/s, 19.1372s/100 iter), loss = 0.0645365
I0916 22:08:44.690865 13570 solver.cpp:336]     Train net output #0: loss = 0.0645365 (* 1 = 0.0645365 loss)
I0916 22:08:44.690872 13570 sgd_solver.cpp:136] Iteration 30200, lr = 0.001, m = 0.9
I0916 22:08:52.740566 13576 data_reader.cpp:305] Starting prefetch of epoch 20
I0916 22:09:04.000334 13570 solver.cpp:314] Iteration 30300 (5.17893 iter/s, 19.309s/100 iter), loss = 0.0777636
I0916 22:09:04.000357 13570 solver.cpp:336]     Train net output #0: loss = 0.0777635 (* 1 = 0.0777635 loss)
I0916 22:09:04.000362 13570 sgd_solver.cpp:136] Iteration 30300, lr = 0.001, m = 0.9
I0916 22:09:23.138027 13570 solver.cpp:314] Iteration 30400 (5.22544 iter/s, 19.1372s/100 iter), loss = 0.0703033
I0916 22:09:23.138140 13570 solver.cpp:336]     Train net output #0: loss = 0.0703032 (* 1 = 0.0703032 loss)
I0916 22:09:23.138147 13570 sgd_solver.cpp:136] Iteration 30400, lr = 0.001, m = 0.9
I0916 22:09:42.477707 13570 solver.cpp:314] Iteration 30500 (5.17086 iter/s, 19.3391s/100 iter), loss = 0.0921916
I0916 22:09:42.477738 13570 solver.cpp:336]     Train net output #0: loss = 0.0921915 (* 1 = 0.0921915 loss)
I0916 22:09:42.477744 13570 sgd_solver.cpp:136] Iteration 30500, lr = 0.001, m = 0.9
I0916 22:09:56.635658 13576 data_reader.cpp:305] Starting prefetch of epoch 21
I0916 22:10:01.787298 13570 solver.cpp:314] Iteration 30600 (5.17892 iter/s, 19.3091s/100 iter), loss = 0.0527453
I0916 22:10:01.787322 13570 solver.cpp:336]     Train net output #0: loss = 0.0527452 (* 1 = 0.0527452 loss)
I0916 22:10:01.787328 13570 sgd_solver.cpp:136] Iteration 30600, lr = 0.001, m = 0.9
I0916 22:10:21.262970 13570 solver.cpp:314] Iteration 30700 (5.13475 iter/s, 19.4751s/100 iter), loss = 0.156749
I0916 22:10:21.262994 13570 solver.cpp:336]     Train net output #0: loss = 0.156749 (* 1 = 0.156749 loss)
I0916 22:10:21.262997 13570 sgd_solver.cpp:136] Iteration 30700, lr = 0.001, m = 0.9
I0916 22:10:28.815296 13574 data_reader.cpp:305] Starting prefetch of epoch 20
I0916 22:10:40.777866 13570 solver.cpp:314] Iteration 30800 (5.12443 iter/s, 19.5144s/100 iter), loss = 0.0637417
I0916 22:10:40.777890 13570 solver.cpp:336]     Train net output #0: loss = 0.0637416 (* 1 = 0.0637416 loss)
I0916 22:10:40.777897 13570 sgd_solver.cpp:136] Iteration 30800, lr = 0.001, m = 0.9
I0916 22:11:00.301539 13570 solver.cpp:314] Iteration 30900 (5.12213 iter/s, 19.5231s/100 iter), loss = 0.0821883
I0916 22:11:00.301617 13570 solver.cpp:336]     Train net output #0: loss = 0.0821882 (* 1 = 0.0821882 loss)
I0916 22:11:00.301622 13570 sgd_solver.cpp:136] Iteration 30900, lr = 0.001, m = 0.9
I0916 22:11:19.292029 13570 solver.cpp:368] Sparsity after update:
I0916 22:11:19.307998 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 22:11:19.308019 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 22:11:19.308027 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 22:11:19.308032 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 22:11:19.308034 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 22:11:19.308059 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 22:11:19.308070 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 22:11:19.308079 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 22:11:19.308087 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 22:11:19.308096 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 22:11:19.308105 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 22:11:19.308113 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 22:11:19.308122 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 22:11:19.308130 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 22:11:19.308140 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 22:11:19.308147 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 22:11:19.308156 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 22:11:19.308164 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 22:11:19.308173 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 22:11:19.481791 13570 solver.cpp:314] Iteration 31000 (5.21384 iter/s, 19.1797s/100 iter), loss = 0.0538004
I0916 22:11:19.481817 13570 solver.cpp:336]     Train net output #0: loss = 0.0538003 (* 1 = 0.0538003 loss)
I0916 22:11:19.481824 13570 sgd_solver.cpp:136] Iteration 31000, lr = 0.001, m = 0.9
I0916 22:11:32.843065 13578 data_reader.cpp:305] Starting prefetch of epoch 18
I0916 22:11:38.839906 13570 solver.cpp:314] Iteration 31100 (5.16593 iter/s, 19.3576s/100 iter), loss = 0.0570479
I0916 22:11:38.839934 13570 solver.cpp:336]     Train net output #0: loss = 0.0570479 (* 1 = 0.0570479 loss)
I0916 22:11:38.839941 13570 sgd_solver.cpp:136] Iteration 31100, lr = 0.001, m = 0.9
I0916 22:11:58.223625 13570 solver.cpp:314] Iteration 31200 (5.15911 iter/s, 19.3832s/100 iter), loss = 0.124594
I0916 22:11:58.223647 13570 solver.cpp:336]     Train net output #0: loss = 0.124594 (* 1 = 0.124594 loss)
I0916 22:11:58.223652 13570 sgd_solver.cpp:136] Iteration 31200, lr = 0.001, m = 0.9
I0916 22:12:17.683639 13570 solver.cpp:314] Iteration 31300 (5.13889 iter/s, 19.4595s/100 iter), loss = 0.0962442
I0916 22:12:17.683753 13570 solver.cpp:336]     Train net output #0: loss = 0.0962442 (* 1 = 0.0962442 loss)
I0916 22:12:17.683761 13570 sgd_solver.cpp:136] Iteration 31300, lr = 0.001, m = 0.9
I0916 22:12:36.837079 13546 data_reader.cpp:305] Starting prefetch of epoch 12
I0916 22:12:37.010493 13570 solver.cpp:314] Iteration 31400 (5.17429 iter/s, 19.3263s/100 iter), loss = 0.0577421
I0916 22:12:37.010520 13570 solver.cpp:336]     Train net output #0: loss = 0.0577421 (* 1 = 0.0577421 loss)
I0916 22:12:37.010527 13570 sgd_solver.cpp:136] Iteration 31400, lr = 0.001, m = 0.9
I0916 22:12:56.582424 13570 solver.cpp:314] Iteration 31500 (5.1095 iter/s, 19.5714s/100 iter), loss = 0.0569322
I0916 22:12:56.582470 13570 solver.cpp:336]     Train net output #0: loss = 0.0569321 (* 1 = 0.0569321 loss)
I0916 22:12:56.582475 13570 sgd_solver.cpp:136] Iteration 31500, lr = 0.001, m = 0.9
I0916 22:13:15.951261 13570 solver.cpp:314] Iteration 31600 (5.16308 iter/s, 19.3683s/100 iter), loss = 0.204615
I0916 22:13:15.951287 13570 solver.cpp:336]     Train net output #0: loss = 0.204615 (* 1 = 0.204615 loss)
I0916 22:13:15.951294 13570 sgd_solver.cpp:136] Iteration 31600, lr = 0.001, m = 0.9
I0916 22:13:35.415802 13570 solver.cpp:314] Iteration 31700 (5.13769 iter/s, 19.464s/100 iter), loss = 0.0687517
I0916 22:13:35.415884 13570 solver.cpp:336]     Train net output #0: loss = 0.0687517 (* 1 = 0.0687517 loss)
I0916 22:13:35.415899 13570 sgd_solver.cpp:136] Iteration 31700, lr = 0.001, m = 0.9
I0916 22:13:41.267868 13578 data_reader.cpp:305] Starting prefetch of epoch 19
I0916 22:13:55.100805 13570 solver.cpp:314] Iteration 31800 (5.08015 iter/s, 19.6845s/100 iter), loss = 0.0714063
I0916 22:13:55.101091 13570 solver.cpp:336]     Train net output #0: loss = 0.0714062 (* 1 = 0.0714062 loss)
I0916 22:13:55.101234 13570 sgd_solver.cpp:136] Iteration 31800, lr = 0.001, m = 0.9
I0916 22:14:13.689108 13574 data_reader.cpp:305] Starting prefetch of epoch 21
I0916 22:14:14.581593 13570 solver.cpp:314] Iteration 31900 (5.13341 iter/s, 19.4802s/100 iter), loss = 0.0434854
I0916 22:14:14.581621 13570 solver.cpp:336]     Train net output #0: loss = 0.0434853 (* 1 = 0.0434853 loss)
I0916 22:14:14.581629 13570 sgd_solver.cpp:136] Iteration 31900, lr = 0.001, m = 0.9
I0916 22:14:33.629469 13570 solver.cpp:368] Sparsity after update:
I0916 22:14:33.634842 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 22:14:33.634850 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 22:14:33.634860 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 22:14:33.634865 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 22:14:33.634868 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 22:14:33.634872 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 22:14:33.634876 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 22:14:33.634881 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 22:14:33.634886 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 22:14:33.634889 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 22:14:33.634893 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 22:14:33.634897 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 22:14:33.634902 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 22:14:33.634905 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 22:14:33.634909 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 22:14:33.634912 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 22:14:33.634915 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 22:14:33.634919 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 22:14:33.634923 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 22:14:33.634934 13570 solver.cpp:563] Iteration 32000, Testing net (#0)
I0916 22:14:54.022379 13596 data_reader.cpp:305] Starting prefetch of epoch 7
I0916 22:14:54.300158 13570 solver.cpp:655]     Test net output #0: accuracy/top1 = 0.949979
I0916 22:14:54.300176 13570 solver.cpp:655]     Test net output #1: accuracy/top5 = 1
I0916 22:14:54.300181 13570 solver.cpp:655]     Test net output #2: loss = 0.146839 (* 1 = 0.146839 loss)
I0916 22:14:54.300227 13570 solver.cpp:265] [MultiGPU] Tests completed in 20.6647s
I0916 22:14:54.509891 13570 solver.cpp:314] Iteration 32000 (2.50456 iter/s, 39.9272s/100 iter), loss = 0.0629731
I0916 22:14:54.509917 13570 solver.cpp:336]     Train net output #0: loss = 0.062973 (* 1 = 0.062973 loss)
I0916 22:14:54.509922 13570 sgd_solver.cpp:136] Iteration 32000, lr = 0.001, m = 0.9
I0916 22:15:13.475605 13570 solver.cpp:314] Iteration 32100 (5.27282 iter/s, 18.9652s/100 iter), loss = 0.088553
I0916 22:15:13.475633 13570 solver.cpp:336]     Train net output #0: loss = 0.0885529 (* 1 = 0.0885529 loss)
I0916 22:15:13.475641 13570 sgd_solver.cpp:136] Iteration 32100, lr = 0.001, m = 0.9
I0916 22:15:32.728597 13570 solver.cpp:314] Iteration 32200 (5.19414 iter/s, 19.2525s/100 iter), loss = 0.0626736
I0916 22:15:32.728653 13570 solver.cpp:336]     Train net output #0: loss = 0.0626736 (* 1 = 0.0626736 loss)
I0916 22:15:32.728659 13570 sgd_solver.cpp:136] Iteration 32200, lr = 0.001, m = 0.9
I0916 22:15:37.683387 13578 data_reader.cpp:305] Starting prefetch of epoch 20
I0916 22:15:51.862829 13570 solver.cpp:314] Iteration 32300 (5.22638 iter/s, 19.1337s/100 iter), loss = 0.0716631
I0916 22:15:51.862854 13570 solver.cpp:336]     Train net output #0: loss = 0.0716631 (* 1 = 0.0716631 loss)
I0916 22:15:51.862860 13570 sgd_solver.cpp:136] Iteration 32300, lr = 0.001, m = 0.9
I0916 22:16:09.333096 13573 data_reader.cpp:305] Starting prefetch of epoch 21
I0916 22:16:11.021448 13570 solver.cpp:314] Iteration 32400 (5.21973 iter/s, 19.1581s/100 iter), loss = 0.0603324
I0916 22:16:11.021473 13570 solver.cpp:336]     Train net output #0: loss = 0.0603324 (* 1 = 0.0603324 loss)
I0916 22:16:11.021479 13570 sgd_solver.cpp:136] Iteration 32400, lr = 0.001, m = 0.9
I0916 22:16:30.326645 13570 solver.cpp:314] Iteration 32500 (5.1801 iter/s, 19.3047s/100 iter), loss = 0.0622925
I0916 22:16:30.326665 13570 solver.cpp:336]     Train net output #0: loss = 0.0622924 (* 1 = 0.0622924 loss)
I0916 22:16:30.326669 13570 sgd_solver.cpp:136] Iteration 32500, lr = 0.001, m = 0.9
I0916 22:16:49.586149 13570 solver.cpp:314] Iteration 32600 (5.19239 iter/s, 19.259s/100 iter), loss = 0.0509195
I0916 22:16:49.586200 13570 solver.cpp:336]     Train net output #0: loss = 0.0509195 (* 1 = 0.0509195 loss)
I0916 22:16:49.586207 13570 sgd_solver.cpp:136] Iteration 32600, lr = 0.001, m = 0.9
I0916 22:17:08.849230 13570 solver.cpp:314] Iteration 32700 (5.19142 iter/s, 19.2625s/100 iter), loss = 0.0661896
I0916 22:17:08.849256 13570 solver.cpp:336]     Train net output #0: loss = 0.0661896 (* 1 = 0.0661896 loss)
I0916 22:17:08.849261 13570 sgd_solver.cpp:136] Iteration 32700, lr = 0.001, m = 0.9
I0916 22:17:12.930404 13576 data_reader.cpp:305] Starting prefetch of epoch 22
I0916 22:17:27.984524 13570 solver.cpp:314] Iteration 32800 (5.22609 iter/s, 19.1348s/100 iter), loss = 0.0752387
I0916 22:17:27.984577 13570 solver.cpp:336]     Train net output #0: loss = 0.0752387 (* 1 = 0.0752387 loss)
I0916 22:17:27.984585 13570 sgd_solver.cpp:136] Iteration 32800, lr = 0.001, m = 0.9
I0916 22:17:47.213838 13570 solver.cpp:314] Iteration 32900 (5.20054 iter/s, 19.2288s/100 iter), loss = 0.084101
I0916 22:17:47.213865 13570 solver.cpp:336]     Train net output #0: loss = 0.084101 (* 1 = 0.084101 loss)
I0916 22:17:47.213870 13570 sgd_solver.cpp:136] Iteration 32900, lr = 0.001, m = 0.9
I0916 22:18:06.254482 13570 solver.cpp:368] Sparsity after update:
I0916 22:18:06.261256 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 22:18:06.261265 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 22:18:06.261271 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 22:18:06.261273 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 22:18:06.261276 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 22:18:06.261277 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 22:18:06.261279 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 22:18:06.261281 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 22:18:06.261283 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 22:18:06.261286 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 22:18:06.261286 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 22:18:06.261288 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 22:18:06.261291 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 22:18:06.261292 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 22:18:06.261294 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 22:18:06.261296 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 22:18:06.261298 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 22:18:06.261301 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 22:18:06.261302 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 22:18:06.432219 13570 solver.cpp:314] Iteration 33000 (5.2035 iter/s, 19.2178s/100 iter), loss = 0.0690769
I0916 22:18:06.432240 13570 solver.cpp:336]     Train net output #0: loss = 0.0690769 (* 1 = 0.0690769 loss)
I0916 22:18:06.432245 13570 sgd_solver.cpp:136] Iteration 33000, lr = 0.001, m = 0.9
I0916 22:18:16.482007 13546 data_reader.cpp:305] Starting prefetch of epoch 13
I0916 22:18:25.831450 13570 solver.cpp:314] Iteration 33100 (5.15499 iter/s, 19.3987s/100 iter), loss = 0.0722062
I0916 22:18:25.831475 13570 solver.cpp:336]     Train net output #0: loss = 0.0722061 (* 1 = 0.0722061 loss)
I0916 22:18:25.831478 13570 sgd_solver.cpp:136] Iteration 33100, lr = 0.001, m = 0.9
I0916 22:18:45.340806 13570 solver.cpp:314] Iteration 33200 (5.12589 iter/s, 19.5088s/100 iter), loss = 0.0824262
I0916 22:18:45.340859 13570 solver.cpp:336]     Train net output #0: loss = 0.0824261 (* 1 = 0.0824261 loss)
I0916 22:18:45.340867 13570 sgd_solver.cpp:136] Iteration 33200, lr = 0.001, m = 0.9
I0916 22:18:48.800302 13544 data_reader.cpp:305] Starting prefetch of epoch 21
I0916 22:19:05.195701 13570 solver.cpp:314] Iteration 33300 (5.03668 iter/s, 19.8543s/100 iter), loss = 0.0741907
I0916 22:19:05.195726 13570 solver.cpp:336]     Train net output #0: loss = 0.0741907 (* 1 = 0.0741907 loss)
I0916 22:19:05.195732 13570 sgd_solver.cpp:136] Iteration 33300, lr = 0.001, m = 0.9
I0916 22:19:25.024016 13570 solver.cpp:314] Iteration 33400 (5.04343 iter/s, 19.8278s/100 iter), loss = 0.0555021
I0916 22:19:25.024068 13570 solver.cpp:336]     Train net output #0: loss = 0.055502 (* 1 = 0.055502 loss)
I0916 22:19:25.024075 13570 sgd_solver.cpp:136] Iteration 33400, lr = 0.001, m = 0.9
I0916 22:19:44.655642 13570 solver.cpp:314] Iteration 33500 (5.09396 iter/s, 19.6311s/100 iter), loss = 0.0722521
I0916 22:19:44.655664 13570 solver.cpp:336]     Train net output #0: loss = 0.072252 (* 1 = 0.072252 loss)
I0916 22:19:44.655668 13570 sgd_solver.cpp:136] Iteration 33500, lr = 0.001, m = 0.9
I0916 22:19:54.071991 13574 data_reader.cpp:305] Starting prefetch of epoch 22
I0916 22:20:04.397706 13570 solver.cpp:314] Iteration 33600 (5.06547 iter/s, 19.7415s/100 iter), loss = 0.0637757
I0916 22:20:04.397794 13570 solver.cpp:336]     Train net output #0: loss = 0.0637756 (* 1 = 0.0637756 loss)
I0916 22:20:04.397801 13570 sgd_solver.cpp:136] Iteration 33600, lr = 0.001, m = 0.9
I0916 22:20:24.050545 13570 solver.cpp:314] Iteration 33700 (5.08846 iter/s, 19.6523s/100 iter), loss = 0.113522
I0916 22:20:24.050565 13570 solver.cpp:336]     Train net output #0: loss = 0.113522 (* 1 = 0.113522 loss)
I0916 22:20:24.050570 13570 sgd_solver.cpp:136] Iteration 33700, lr = 0.001, m = 0.9
I0916 22:20:43.553414 13570 solver.cpp:314] Iteration 33800 (5.12759 iter/s, 19.5023s/100 iter), loss = 0.0569467
I0916 22:20:43.553565 13570 solver.cpp:336]     Train net output #0: loss = 0.0569466 (* 1 = 0.0569466 loss)
I0916 22:20:43.553581 13570 sgd_solver.cpp:136] Iteration 33800, lr = 0.001, m = 0.9
I0916 22:20:58.973057 13546 data_reader.cpp:305] Starting prefetch of epoch 14
I0916 22:21:03.328374 13570 solver.cpp:314] Iteration 33900 (5.05704 iter/s, 19.7744s/100 iter), loss = 0.0548456
I0916 22:21:03.328402 13570 solver.cpp:336]     Train net output #0: loss = 0.0548455 (* 1 = 0.0548455 loss)
I0916 22:21:03.328408 13570 sgd_solver.cpp:136] Iteration 33900, lr = 0.001, m = 0.9
I0916 22:21:22.918221 13570 solver.cpp:368] Sparsity after update:
I0916 22:21:22.926574 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 22:21:22.926589 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 22:21:22.926596 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 22:21:22.926599 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 22:21:22.926604 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 22:21:22.926668 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 22:21:22.926674 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 22:21:22.926677 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 22:21:22.926681 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 22:21:22.926692 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 22:21:22.926697 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 22:21:22.926702 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 22:21:22.926712 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 22:21:22.926717 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 22:21:22.926719 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 22:21:22.926729 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 22:21:22.926734 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 22:21:22.926744 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 22:21:22.926748 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 22:21:22.926765 13570 solver.cpp:563] Iteration 34000, Testing net (#0)
I0916 22:21:34.292677 13568 data_reader.cpp:305] Starting prefetch of epoch 2
I0916 22:21:39.257408 13570 solver.cpp:655]     Test net output #0: accuracy/top1 = 0.954052
I0916 22:21:39.257437 13570 solver.cpp:655]     Test net output #1: accuracy/top5 = 1
I0916 22:21:39.257444 13570 solver.cpp:655]     Test net output #2: loss = 0.158289 (* 1 = 0.158289 loss)
I0916 22:21:39.257519 13570 solver.cpp:265] [MultiGPU] Tests completed in 16.3303s
I0916 22:21:39.466295 13570 solver.cpp:314] Iteration 34000 (2.76725 iter/s, 36.1369s/100 iter), loss = 0.158046
I0916 22:21:39.466325 13570 solver.cpp:336]     Train net output #0: loss = 0.158046 (* 1 = 0.158046 loss)
I0916 22:21:39.466331 13570 sgd_solver.cpp:136] Iteration 34000, lr = 0.001, m = 0.9
I0916 22:21:58.783859 13570 solver.cpp:314] Iteration 34100 (5.17678 iter/s, 19.317s/100 iter), loss = 0.055249
I0916 22:21:58.783951 13570 solver.cpp:336]     Train net output #0: loss = 0.0552489 (* 1 = 0.0552489 loss)
I0916 22:21:58.783959 13570 sgd_solver.cpp:136] Iteration 34100, lr = 0.001, m = 0.9
I0916 22:22:18.356523 13570 solver.cpp:314] Iteration 34200 (5.10931 iter/s, 19.5721s/100 iter), loss = 0.0500774
I0916 22:22:18.356550 13570 solver.cpp:336]     Train net output #0: loss = 0.0500773 (* 1 = 0.0500773 loss)
I0916 22:22:18.356557 13570 sgd_solver.cpp:136] Iteration 34200, lr = 0.001, m = 0.9
I0916 22:22:20.134627 13573 data_reader.cpp:305] Starting prefetch of epoch 22
I0916 22:22:38.243322 13570 solver.cpp:314] Iteration 34300 (5.0286 iter/s, 19.8862s/100 iter), loss = 0.0626318
I0916 22:22:38.243418 13570 solver.cpp:336]     Train net output #0: loss = 0.0626317 (* 1 = 0.0626317 loss)
I0916 22:22:38.243423 13570 sgd_solver.cpp:136] Iteration 34300, lr = 0.001, m = 0.9
I0916 22:22:57.666724 13570 solver.cpp:314] Iteration 34400 (5.14857 iter/s, 19.4229s/100 iter), loss = 0.0629678
I0916 22:22:57.666781 13570 solver.cpp:336]     Train net output #0: loss = 0.0629677 (* 1 = 0.0629677 loss)
I0916 22:22:57.666795 13570 sgd_solver.cpp:136] Iteration 34400, lr = 0.001, m = 0.9
I0916 22:23:17.347065 13570 solver.cpp:314] Iteration 34500 (5.08135 iter/s, 19.6798s/100 iter), loss = 0.172233
I0916 22:23:17.347146 13570 solver.cpp:336]     Train net output #0: loss = 0.172233 (* 1 = 0.172233 loss)
I0916 22:23:17.347153 13570 sgd_solver.cpp:136] Iteration 34500, lr = 0.001, m = 0.9
I0916 22:23:25.295020 13544 data_reader.cpp:305] Starting prefetch of epoch 22
I0916 22:23:37.085125 13570 solver.cpp:314] Iteration 34600 (5.06649 iter/s, 19.7375s/100 iter), loss = 0.0885575
I0916 22:23:37.085150 13570 solver.cpp:336]     Train net output #0: loss = 0.0885573 (* 1 = 0.0885573 loss)
I0916 22:23:37.085155 13570 sgd_solver.cpp:136] Iteration 34600, lr = 0.001, m = 0.9
I0916 22:23:56.693447 13570 solver.cpp:314] Iteration 34700 (5.10002 iter/s, 19.6078s/100 iter), loss = 0.0492809
I0916 22:23:56.693558 13570 solver.cpp:336]     Train net output #0: loss = 0.0492808 (* 1 = 0.0492808 loss)
I0916 22:23:56.693565 13570 sgd_solver.cpp:136] Iteration 34700, lr = 0.001, m = 0.9
I0916 22:23:57.688139 13544 data_reader.cpp:305] Starting prefetch of epoch 23
I0916 22:24:15.970335 13570 solver.cpp:314] Iteration 34800 (5.18771 iter/s, 19.2763s/100 iter), loss = 0.0660576
I0916 22:24:15.970376 13570 solver.cpp:336]     Train net output #0: loss = 0.0660575 (* 1 = 0.0660575 loss)
I0916 22:24:15.970388 13570 sgd_solver.cpp:136] Iteration 34800, lr = 0.001, m = 0.9
I0916 22:24:35.380807 13570 solver.cpp:314] Iteration 34900 (5.152 iter/s, 19.4099s/100 iter), loss = 0.07826
I0916 22:24:35.380861 13570 solver.cpp:336]     Train net output #0: loss = 0.0782599 (* 1 = 0.0782599 loss)
I0916 22:24:35.380867 13570 sgd_solver.cpp:136] Iteration 34900, lr = 0.001, m = 0.9
I0916 22:24:54.774186 13570 solver.cpp:368] Sparsity after update:
I0916 22:24:54.792433 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 22:24:54.792450 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 22:24:54.792456 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 22:24:54.792459 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 22:24:54.792461 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 22:24:54.792462 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 22:24:54.792464 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 22:24:54.792466 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 22:24:54.792469 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 22:24:54.792470 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 22:24:54.792472 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 22:24:54.792474 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 22:24:54.792476 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 22:24:54.792477 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 22:24:54.792479 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 22:24:54.792481 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 22:24:54.792484 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 22:24:54.792485 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 22:24:54.792487 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 22:24:54.964529 13570 solver.cpp:314] Iteration 35000 (5.10642 iter/s, 19.5832s/100 iter), loss = 0.0653202
I0916 22:24:54.964551 13570 solver.cpp:336]     Train net output #0: loss = 0.0653201 (* 1 = 0.0653201 loss)
I0916 22:24:54.964555 13570 sgd_solver.cpp:136] Iteration 35000, lr = 0.001, m = 0.9
I0916 22:25:01.721504 13574 data_reader.cpp:305] Starting prefetch of epoch 23
I0916 22:25:14.286095 13570 solver.cpp:314] Iteration 35100 (5.17571 iter/s, 19.321s/100 iter), loss = 0.0662967
I0916 22:25:14.286214 13570 solver.cpp:336]     Train net output #0: loss = 0.0662966 (* 1 = 0.0662966 loss)
I0916 22:25:14.286233 13570 sgd_solver.cpp:136] Iteration 35100, lr = 0.001, m = 0.9
I0916 22:25:33.901113 13570 solver.cpp:314] Iteration 35200 (5.09828 iter/s, 19.6145s/100 iter), loss = 0.0539222
I0916 22:25:33.901142 13570 solver.cpp:336]     Train net output #0: loss = 0.0539221 (* 1 = 0.0539221 loss)
I0916 22:25:33.901149 13570 sgd_solver.cpp:136] Iteration 35200, lr = 0.001, m = 0.9
I0916 22:25:53.056244 13570 solver.cpp:314] Iteration 35300 (5.22068 iter/s, 19.1546s/100 iter), loss = 0.0842631
I0916 22:25:53.062131 13570 solver.cpp:336]     Train net output #0: loss = 0.084263 (* 1 = 0.084263 loss)
I0916 22:25:53.062160 13570 sgd_solver.cpp:136] Iteration 35300, lr = 0.001, m = 0.9
I0916 22:26:05.812997 13576 data_reader.cpp:305] Starting prefetch of epoch 23
I0916 22:26:12.443032 13570 solver.cpp:314] Iteration 35400 (5.15829 iter/s, 19.3863s/100 iter), loss = 0.0734963
I0916 22:26:12.443060 13570 solver.cpp:336]     Train net output #0: loss = 0.0734962 (* 1 = 0.0734962 loss)
I0916 22:26:12.443068 13570 sgd_solver.cpp:136] Iteration 35400, lr = 0.001, m = 0.9
I0916 22:26:31.924829 13570 solver.cpp:314] Iteration 35500 (5.13314 iter/s, 19.4813s/100 iter), loss = 0.0419888
I0916 22:26:31.924890 13570 solver.cpp:336]     Train net output #0: loss = 0.0419887 (* 1 = 0.0419887 loss)
I0916 22:26:31.924896 13570 sgd_solver.cpp:136] Iteration 35500, lr = 0.001, m = 0.9
I0916 22:26:51.385061 13570 solver.cpp:314] Iteration 35600 (5.13883 iter/s, 19.4597s/100 iter), loss = 0.0701667
I0916 22:26:51.385084 13570 solver.cpp:336]     Train net output #0: loss = 0.0701666 (* 1 = 0.0701666 loss)
I0916 22:26:51.385089 13570 sgd_solver.cpp:136] Iteration 35600, lr = 0.001, m = 0.9
I0916 22:27:09.908689 13576 data_reader.cpp:305] Starting prefetch of epoch 24
I0916 22:27:10.622586 13570 solver.cpp:314] Iteration 35700 (5.19832 iter/s, 19.237s/100 iter), loss = 0.0549248
I0916 22:27:10.622611 13570 solver.cpp:336]     Train net output #0: loss = 0.0549247 (* 1 = 0.0549247 loss)
I0916 22:27:10.622615 13570 sgd_solver.cpp:136] Iteration 35700, lr = 0.001, m = 0.9
I0916 22:27:30.167976 13570 solver.cpp:314] Iteration 35800 (5.11644 iter/s, 19.5449s/100 iter), loss = 0.0596856
I0916 22:27:30.167999 13570 solver.cpp:336]     Train net output #0: loss = 0.0596854 (* 1 = 0.0596854 loss)
I0916 22:27:30.168004 13570 sgd_solver.cpp:136] Iteration 35800, lr = 0.001, m = 0.9
I0916 22:27:41.948842 13574 data_reader.cpp:305] Starting prefetch of epoch 24
I0916 22:27:49.164741 13570 solver.cpp:314] Iteration 35900 (5.2642 iter/s, 18.9962s/100 iter), loss = 0.070015
I0916 22:27:49.164765 13570 solver.cpp:336]     Train net output #0: loss = 0.0700149 (* 1 = 0.0700149 loss)
I0916 22:27:49.164770 13570 sgd_solver.cpp:136] Iteration 35900, lr = 0.001, m = 0.9
I0916 22:28:08.357271 13570 solver.cpp:368] Sparsity after update:
I0916 22:28:08.362357 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 22:28:08.362367 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 22:28:08.362376 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 22:28:08.362388 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 22:28:08.362397 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 22:28:08.362414 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 22:28:08.362424 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 22:28:08.362433 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 22:28:08.362443 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 22:28:08.362453 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 22:28:08.362463 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 22:28:08.362469 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 22:28:08.362473 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 22:28:08.362480 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 22:28:08.362489 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 22:28:08.362498 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 22:28:08.362504 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 22:28:08.362507 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 22:28:08.362510 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 22:28:08.362527 13570 solver.cpp:563] Iteration 36000, Testing net (#0)
I0916 22:28:18.584825 13604 data_reader.cpp:305] Starting prefetch of epoch 4
I0916 22:28:18.926913 13570 solver.cpp:655]     Test net output #0: accuracy/top1 = 0.951293
I0916 22:28:18.926934 13570 solver.cpp:655]     Test net output #1: accuracy/top5 = 1
I0916 22:28:18.926940 13570 solver.cpp:655]     Test net output #2: loss = 0.147453 (* 1 = 0.147453 loss)
I0916 22:28:18.926964 13570 solver.cpp:265] [MultiGPU] Tests completed in 10.5641s
I0916 22:28:19.122109 13570 solver.cpp:314] Iteration 36000 (3.33817 iter/s, 29.9565s/100 iter), loss = 0.0788283
I0916 22:28:19.122133 13570 solver.cpp:336]     Train net output #0: loss = 0.0788282 (* 1 = 0.0788282 loss)
I0916 22:28:19.122138 13570 sgd_solver.cpp:136] Iteration 36000, lr = 0.001, m = 0.9
I0916 22:28:38.677208 13570 solver.cpp:314] Iteration 36100 (5.1139 iter/s, 19.5546s/100 iter), loss = 0.178148
I0916 22:28:38.677233 13570 solver.cpp:336]     Train net output #0: loss = 0.178148 (* 1 = 0.178148 loss)
I0916 22:28:38.677239 13570 sgd_solver.cpp:136] Iteration 36100, lr = 0.001, m = 0.9
I0916 22:28:56.537634 13578 data_reader.cpp:305] Starting prefetch of epoch 21
I0916 22:28:58.085917 13570 solver.cpp:314] Iteration 36200 (5.15247 iter/s, 19.4082s/100 iter), loss = 0.0482803
I0916 22:28:58.086030 13570 solver.cpp:336]     Train net output #0: loss = 0.0482802 (* 1 = 0.0482802 loss)
I0916 22:28:58.086055 13570 sgd_solver.cpp:136] Iteration 36200, lr = 0.001, m = 0.9
I0916 22:29:17.680061 13570 solver.cpp:314] Iteration 36300 (5.10371 iter/s, 19.5936s/100 iter), loss = 0.0549617
I0916 22:29:17.680083 13570 solver.cpp:336]     Train net output #0: loss = 0.0549615 (* 1 = 0.0549615 loss)
I0916 22:29:17.680088 13570 sgd_solver.cpp:136] Iteration 36300, lr = 0.001, m = 0.9
I0916 22:29:29.057924 13573 data_reader.cpp:305] Starting prefetch of epoch 23
I0916 22:29:37.220197 13570 solver.cpp:314] Iteration 36400 (5.11782 iter/s, 19.5396s/100 iter), loss = 0.0643352
I0916 22:29:37.220227 13570 solver.cpp:336]     Train net output #0: loss = 0.064335 (* 1 = 0.064335 loss)
I0916 22:29:37.220234 13570 sgd_solver.cpp:136] Iteration 36400, lr = 0.001, m = 0.9
I0916 22:29:57.008008 13570 solver.cpp:314] Iteration 36500 (5.05376 iter/s, 19.7873s/100 iter), loss = 0.0866666
I0916 22:29:57.008033 13570 solver.cpp:336]     Train net output #0: loss = 0.0866664 (* 1 = 0.0866664 loss)
I0916 22:29:57.008038 13570 sgd_solver.cpp:136] Iteration 36500, lr = 0.001, m = 0.9
I0916 22:30:16.482964 13570 solver.cpp:314] Iteration 36600 (5.13494 iter/s, 19.4744s/100 iter), loss = 0.0663875
I0916 22:30:16.483018 13570 solver.cpp:336]     Train net output #0: loss = 0.0663874 (* 1 = 0.0663874 loss)
I0916 22:30:16.483023 13570 sgd_solver.cpp:136] Iteration 36600, lr = 0.001, m = 0.9
I0916 22:30:33.801353 13546 data_reader.cpp:305] Starting prefetch of epoch 15
I0916 22:30:36.140790 13570 solver.cpp:314] Iteration 36700 (5.08717 iter/s, 19.6573s/100 iter), loss = 0.0687121
I0916 22:30:36.140816 13570 solver.cpp:336]     Train net output #0: loss = 0.068712 (* 1 = 0.068712 loss)
I0916 22:30:36.140823 13570 sgd_solver.cpp:136] Iteration 36700, lr = 0.001, m = 0.9
I0916 22:30:55.734346 13570 solver.cpp:314] Iteration 36800 (5.10386 iter/s, 19.593s/100 iter), loss = 0.0568472
I0916 22:30:55.734428 13570 solver.cpp:336]     Train net output #0: loss = 0.0568471 (* 1 = 0.0568471 loss)
I0916 22:30:55.734436 13570 sgd_solver.cpp:136] Iteration 36800, lr = 0.001, m = 0.9
I0916 22:31:15.374603 13570 solver.cpp:314] Iteration 36900 (5.09172 iter/s, 19.6397s/100 iter), loss = 0.0671254
I0916 22:31:15.374626 13570 solver.cpp:336]     Train net output #0: loss = 0.0671253 (* 1 = 0.0671253 loss)
I0916 22:31:15.374634 13570 sgd_solver.cpp:136] Iteration 36900, lr = 0.001, m = 0.9
I0916 22:31:35.136399 13570 solver.cpp:368] Sparsity after update:
I0916 22:31:35.159327 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 22:31:35.159346 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 22:31:35.159355 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 22:31:35.159358 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 22:31:35.159361 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 22:31:35.159363 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 22:31:35.159366 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 22:31:35.159379 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 22:31:35.159387 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 22:31:35.159396 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 22:31:35.159404 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 22:31:35.159412 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 22:31:35.159420 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 22:31:35.159428 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 22:31:35.159437 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 22:31:35.159446 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 22:31:35.159461 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 22:31:35.159471 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 22:31:35.159481 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 22:31:35.331732 13570 solver.cpp:314] Iteration 37000 (5.01088 iter/s, 19.9566s/100 iter), loss = 0.0589063
I0916 22:31:35.331753 13570 solver.cpp:336]     Train net output #0: loss = 0.0589062 (* 1 = 0.0589062 loss)
I0916 22:31:35.331759 13570 sgd_solver.cpp:136] Iteration 37000, lr = 0.001, m = 0.9
I0916 22:31:39.048897 13578 data_reader.cpp:305] Starting prefetch of epoch 22
I0916 22:31:54.903327 13570 solver.cpp:314] Iteration 37100 (5.10959 iter/s, 19.571s/100 iter), loss = 0.0813027
I0916 22:31:54.903354 13570 solver.cpp:336]     Train net output #0: loss = 0.0813026 (* 1 = 0.0813026 loss)
I0916 22:31:54.903358 13570 sgd_solver.cpp:136] Iteration 37100, lr = 0.001, m = 0.9
I0916 22:32:14.282011 13570 solver.cpp:314] Iteration 37200 (5.16045 iter/s, 19.3781s/100 iter), loss = 0.0560242
I0916 22:32:14.282109 13570 solver.cpp:336]     Train net output #0: loss = 0.0560241 (* 1 = 0.0560241 loss)
I0916 22:32:14.282130 13570 sgd_solver.cpp:136] Iteration 37200, lr = 0.001, m = 0.9
I0916 22:32:34.007910 13570 solver.cpp:314] Iteration 37300 (5.06962 iter/s, 19.7254s/100 iter), loss = 0.0646096
I0916 22:32:34.007932 13570 solver.cpp:336]     Train net output #0: loss = 0.0646095 (* 1 = 0.0646095 loss)
I0916 22:32:34.007936 13570 sgd_solver.cpp:136] Iteration 37300, lr = 0.001, m = 0.9
I0916 22:32:43.756772 13576 data_reader.cpp:305] Starting prefetch of epoch 25
I0916 22:32:53.912307 13570 solver.cpp:314] Iteration 37400 (5.02416 iter/s, 19.9038s/100 iter), loss = 0.093204
I0916 22:32:53.912729 13570 solver.cpp:336]     Train net output #0: loss = 0.0932038 (* 1 = 0.0932038 loss)
I0916 22:32:53.912735 13570 sgd_solver.cpp:136] Iteration 37400, lr = 0.001, m = 0.9
I0916 22:33:13.760723 13570 solver.cpp:314] Iteration 37500 (5.03833 iter/s, 19.8479s/100 iter), loss = 0.0494654
I0916 22:33:13.760761 13570 solver.cpp:336]     Train net output #0: loss = 0.0494653 (* 1 = 0.0494653 loss)
I0916 22:33:13.760767 13570 sgd_solver.cpp:136] Iteration 37500, lr = 0.001, m = 0.9
I0916 22:33:16.818363 13573 data_reader.cpp:305] Starting prefetch of epoch 24
I0916 22:33:33.639519 13570 solver.cpp:314] Iteration 37600 (5.03062 iter/s, 19.8782s/100 iter), loss = 0.0400998
I0916 22:33:33.639577 13570 solver.cpp:336]     Train net output #0: loss = 0.0400997 (* 1 = 0.0400997 loss)
I0916 22:33:33.639585 13570 sgd_solver.cpp:136] Iteration 37600, lr = 0.001, m = 0.9
I0916 22:33:53.481724 13570 solver.cpp:314] Iteration 37700 (5.0399 iter/s, 19.8417s/100 iter), loss = 0.0686033
I0916 22:33:53.481752 13570 solver.cpp:336]     Train net output #0: loss = 0.0686031 (* 1 = 0.0686031 loss)
I0916 22:33:53.481758 13570 sgd_solver.cpp:136] Iteration 37700, lr = 0.001, m = 0.9
I0916 22:34:13.681721 13570 solver.cpp:314] Iteration 37800 (4.95063 iter/s, 20.1994s/100 iter), loss = 0.0549695
I0916 22:34:13.681787 13570 solver.cpp:336]     Train net output #0: loss = 0.0549693 (* 1 = 0.0549693 loss)
I0916 22:34:13.681793 13570 sgd_solver.cpp:136] Iteration 37800, lr = 0.001, m = 0.9
I0916 22:34:22.506886 13574 data_reader.cpp:305] Starting prefetch of epoch 25
I0916 22:34:33.315883 13570 solver.cpp:314] Iteration 37900 (5.0933 iter/s, 19.6336s/100 iter), loss = 0.0788437
I0916 22:34:33.315907 13570 solver.cpp:336]     Train net output #0: loss = 0.0788435 (* 1 = 0.0788435 loss)
I0916 22:34:33.315912 13570 sgd_solver.cpp:136] Iteration 37900, lr = 0.001, m = 0.9
I0916 22:34:52.855890 13570 solver.cpp:368] Sparsity after update:
I0916 22:34:52.864219 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 22:34:52.864291 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 22:34:52.864306 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 22:34:52.864315 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 22:34:52.864323 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 22:34:52.864331 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 22:34:52.864339 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 22:34:52.864347 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 22:34:52.864356 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 22:34:52.864363 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 22:34:52.864372 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 22:34:52.864379 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 22:34:52.864387 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 22:34:52.864395 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 22:34:52.864403 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 22:34:52.864411 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 22:34:52.864418 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 22:34:52.864426 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 22:34:52.864434 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 22:34:52.864454 13570 solver.cpp:563] Iteration 38000, Testing net (#0)
I0916 22:35:04.543647 13570 solver.cpp:655]     Test net output #0: accuracy/top1 = 0.95415
I0916 22:35:04.543671 13570 solver.cpp:655]     Test net output #1: accuracy/top5 = 1
I0916 22:35:04.543679 13570 solver.cpp:655]     Test net output #2: loss = 0.161011 (* 1 = 0.161011 loss)
I0916 22:35:04.543699 13570 solver.cpp:265] [MultiGPU] Tests completed in 11.6789s
I0916 22:35:04.730865 13570 solver.cpp:314] Iteration 38000 (3.18328 iter/s, 31.4141s/100 iter), loss = 0.0915211
I0916 22:35:04.730895 13570 solver.cpp:336]     Train net output #0: loss = 0.091521 (* 1 = 0.091521 loss)
I0916 22:35:04.730901 13570 sgd_solver.cpp:136] Iteration 38000, lr = 0.001, m = 0.9
I0916 22:35:06.735465 13576 data_reader.cpp:305] Starting prefetch of epoch 26
I0916 22:35:24.192911 13570 solver.cpp:314] Iteration 38100 (5.13835 iter/s, 19.4615s/100 iter), loss = 0.0797218
I0916 22:35:24.192963 13570 solver.cpp:336]     Train net output #0: loss = 0.0797217 (* 1 = 0.0797217 loss)
I0916 22:35:24.192968 13570 sgd_solver.cpp:136] Iteration 38100, lr = 0.001, m = 0.9
I0916 22:35:39.099952 13573 data_reader.cpp:305] Starting prefetch of epoch 25
I0916 22:35:43.682725 13570 solver.cpp:314] Iteration 38200 (5.13103 iter/s, 19.4893s/100 iter), loss = 0.0878149
I0916 22:35:43.682763 13570 solver.cpp:336]     Train net output #0: loss = 0.0878147 (* 1 = 0.0878147 loss)
I0916 22:35:43.682771 13570 sgd_solver.cpp:136] Iteration 38200, lr = 0.001, m = 0.9
I0916 22:36:03.262764 13570 solver.cpp:314] Iteration 38300 (5.10738 iter/s, 19.5795s/100 iter), loss = 0.0659254
I0916 22:36:03.262820 13570 solver.cpp:336]     Train net output #0: loss = 0.0659252 (* 1 = 0.0659252 loss)
I0916 22:36:03.262827 13570 sgd_solver.cpp:136] Iteration 38300, lr = 0.001, m = 0.9
I0916 22:36:23.044409 13570 solver.cpp:314] Iteration 38400 (5.05533 iter/s, 19.7811s/100 iter), loss = 0.0620498
I0916 22:36:23.044430 13570 solver.cpp:336]     Train net output #0: loss = 0.0620496 (* 1 = 0.0620496 loss)
I0916 22:36:23.044433 13570 sgd_solver.cpp:136] Iteration 38400, lr = 0.001, m = 0.9
I0916 22:36:42.782371 13570 solver.cpp:314] Iteration 38500 (5.06652 iter/s, 19.7374s/100 iter), loss = 0.0568931
I0916 22:36:42.782445 13570 solver.cpp:336]     Train net output #0: loss = 0.056893 (* 1 = 0.056893 loss)
I0916 22:36:42.782451 13570 sgd_solver.cpp:136] Iteration 38500, lr = 0.001, m = 0.9
I0916 22:36:44.052026 13544 data_reader.cpp:305] Starting prefetch of epoch 24
I0916 22:37:02.301138 13570 solver.cpp:314] Iteration 38600 (5.12342 iter/s, 19.5182s/100 iter), loss = 0.0548144
I0916 22:37:02.301167 13570 solver.cpp:336]     Train net output #0: loss = 0.0548142 (* 1 = 0.0548142 loss)
I0916 22:37:02.301172 13570 sgd_solver.cpp:136] Iteration 38600, lr = 0.001, m = 0.9
I0916 22:37:22.186328 13570 solver.cpp:314] Iteration 38700 (5.02901 iter/s, 19.8846s/100 iter), loss = 0.0818416
I0916 22:37:22.186414 13570 solver.cpp:336]     Train net output #0: loss = 0.0818415 (* 1 = 0.0818415 loss)
I0916 22:37:22.186421 13570 sgd_solver.cpp:136] Iteration 38700, lr = 0.001, m = 0.9
I0916 22:37:41.929927 13570 solver.cpp:314] Iteration 38800 (5.06507 iter/s, 19.743s/100 iter), loss = 0.0676718
I0916 22:37:41.929955 13570 solver.cpp:336]     Train net output #0: loss = 0.0676717 (* 1 = 0.0676717 loss)
I0916 22:37:41.929961 13570 sgd_solver.cpp:136] Iteration 38800, lr = 0.001, m = 0.9
I0916 22:37:49.333583 13576 data_reader.cpp:305] Starting prefetch of epoch 27
I0916 22:38:01.898811 13570 solver.cpp:314] Iteration 38900 (5.00793 iter/s, 19.9683s/100 iter), loss = 0.145101
I0916 22:38:01.898859 13570 solver.cpp:336]     Train net output #0: loss = 0.1451 (* 1 = 0.1451 loss)
I0916 22:38:01.898864 13570 sgd_solver.cpp:136] Iteration 38900, lr = 0.001, m = 0.9
I0916 22:38:21.411255 13570 solver.cpp:368] Sparsity after update:
I0916 22:38:21.416158 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 22:38:21.416330 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 22:38:21.416405 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 22:38:21.416438 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 22:38:21.416471 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 22:38:21.416503 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 22:38:21.416538 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 22:38:21.416570 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 22:38:21.416602 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 22:38:21.416636 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 22:38:21.416666 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 22:38:21.416697 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 22:38:21.416730 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 22:38:21.416762 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 22:38:21.416798 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 22:38:21.416829 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 22:38:21.416863 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 22:38:21.416893 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 22:38:21.416926 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 22:38:21.601426 13570 solver.cpp:314] Iteration 39000 (5.07561 iter/s, 19.7021s/100 iter), loss = 0.0644456
I0916 22:38:21.601451 13570 solver.cpp:336]     Train net output #0: loss = 0.0644454 (* 1 = 0.0644454 loss)
I0916 22:38:21.601456 13570 sgd_solver.cpp:136] Iteration 39000, lr = 0.001, m = 0.9
I0916 22:38:22.020211 13544 data_reader.cpp:305] Starting prefetch of epoch 25
I0916 22:38:41.511312 13570 solver.cpp:314] Iteration 39100 (5.02277 iter/s, 19.9093s/100 iter), loss = 0.0328501
I0916 22:38:41.511392 13570 solver.cpp:336]     Train net output #0: loss = 0.03285 (* 1 = 0.03285 loss)
I0916 22:38:41.511400 13570 sgd_solver.cpp:136] Iteration 39100, lr = 0.001, m = 0.9
I0916 22:39:00.852854 13570 solver.cpp:314] Iteration 39200 (5.17036 iter/s, 19.341s/100 iter), loss = 0.0839714
I0916 22:39:00.852893 13570 solver.cpp:336]     Train net output #0: loss = 0.0839713 (* 1 = 0.0839713 loss)
I0916 22:39:00.852900 13570 sgd_solver.cpp:136] Iteration 39200, lr = 0.001, m = 0.9
I0916 22:39:20.493901 13570 solver.cpp:314] Iteration 39300 (5.09152 iter/s, 19.6405s/100 iter), loss = 0.0632892
I0916 22:39:20.493976 13570 solver.cpp:336]     Train net output #0: loss = 0.0632891 (* 1 = 0.0632891 loss)
I0916 22:39:20.493983 13570 sgd_solver.cpp:136] Iteration 39300, lr = 0.001, m = 0.9
I0916 22:39:26.942786 13574 data_reader.cpp:305] Starting prefetch of epoch 26
I0916 22:39:39.910593 13570 solver.cpp:314] Iteration 39400 (5.15035 iter/s, 19.4162s/100 iter), loss = 0.0569796
I0916 22:39:39.910614 13570 solver.cpp:336]     Train net output #0: loss = 0.0569795 (* 1 = 0.0569795 loss)
I0916 22:39:39.910617 13570 sgd_solver.cpp:136] Iteration 39400, lr = 0.001, m = 0.9
I0916 22:39:59.526844 13570 solver.cpp:314] Iteration 39500 (5.09796 iter/s, 19.6157s/100 iter), loss = 0.0567595
I0916 22:39:59.526898 13570 solver.cpp:336]     Train net output #0: loss = 0.0567594 (* 1 = 0.0567594 loss)
I0916 22:39:59.526906 13570 sgd_solver.cpp:136] Iteration 39500, lr = 0.001, m = 0.9
I0916 22:40:19.062733 13570 solver.cpp:314] Iteration 39600 (5.11893 iter/s, 19.5353s/100 iter), loss = 0.072267
I0916 22:40:19.062760 13570 solver.cpp:336]     Train net output #0: loss = 0.0722669 (* 1 = 0.0722669 loss)
I0916 22:40:19.062764 13570 sgd_solver.cpp:136] Iteration 39600, lr = 0.001, m = 0.9
I0916 22:40:31.193922 13578 data_reader.cpp:305] Starting prefetch of epoch 23
I0916 22:40:38.317818 13570 solver.cpp:314] Iteration 39700 (5.19358 iter/s, 19.2546s/100 iter), loss = 0.0702501
I0916 22:40:38.317843 13570 solver.cpp:336]     Train net output #0: loss = 0.07025 (* 1 = 0.07025 loss)
I0916 22:40:38.317847 13570 sgd_solver.cpp:136] Iteration 39700, lr = 0.001, m = 0.9
I0916 22:40:57.647900 13570 solver.cpp:314] Iteration 39800 (5.17343 iter/s, 19.3295s/100 iter), loss = 0.057859
I0916 22:40:57.647935 13570 solver.cpp:336]     Train net output #0: loss = 0.0578589 (* 1 = 0.0578589 loss)
I0916 22:40:57.647941 13570 sgd_solver.cpp:136] Iteration 39800, lr = 0.001, m = 0.9
I0916 22:41:03.136008 13544 data_reader.cpp:305] Starting prefetch of epoch 26
I0916 22:41:17.397245 13570 solver.cpp:314] Iteration 39900 (5.0636 iter/s, 19.7488s/100 iter), loss = 0.0695099
I0916 22:41:17.397274 13570 solver.cpp:336]     Train net output #0: loss = 0.0695097 (* 1 = 0.0695097 loss)
I0916 22:41:17.397280 13570 sgd_solver.cpp:136] Iteration 39900, lr = 0.001, m = 0.9
I0916 22:41:36.921422 13570 solver.cpp:825] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/cityscapes5_jsegnet21v2_iter_40000.caffemodel
I0916 22:41:36.989200 13570 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/cityscapes5_jsegnet21v2_iter_40000.solverstate
I0916 22:41:36.996978 13570 solver.cpp:368] Sparsity after update:
I0916 22:41:36.998790 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 22:41:36.998800 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 22:41:36.998805 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 22:41:36.998809 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 22:41:36.998812 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 22:41:36.998816 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 22:41:36.998818 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 22:41:36.998822 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 22:41:36.998826 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 22:41:36.998828 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 22:41:36.998831 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 22:41:36.998834 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 22:41:36.998843 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 22:41:36.998847 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 22:41:36.998849 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 22:41:36.998852 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 22:41:36.998855 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 22:41:36.998858 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 22:41:36.998862 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 22:41:36.998873 13570 solver.cpp:563] Iteration 40000, Testing net (#0)
I0916 22:41:40.111858 13596 data_reader.cpp:305] Starting prefetch of epoch 8
I0916 22:41:48.228368 13570 solver.cpp:655]     Test net output #0: accuracy/top1 = 0.952269
I0916 22:41:48.228386 13570 solver.cpp:655]     Test net output #1: accuracy/top5 = 1
I0916 22:41:48.228391 13570 solver.cpp:655]     Test net output #2: loss = 0.142443 (* 1 = 0.142443 loss)
I0916 22:41:48.228420 13570 solver.cpp:265] [MultiGPU] Tests completed in 11.2292s
I0916 22:41:48.438877 13570 solver.cpp:314] Iteration 40000 (3.22157 iter/s, 31.0408s/100 iter), loss = 0.0781242
I0916 22:41:48.438905 13570 solver.cpp:336]     Train net output #0: loss = 0.0781241 (* 1 = 0.0781241 loss)
I0916 22:41:48.438912 13570 sgd_solver.cpp:136] Iteration 40000, lr = 0.001, m = 0.9
I0916 22:42:07.413648 13570 solver.cpp:314] Iteration 40100 (5.2703 iter/s, 18.9742s/100 iter), loss = 0.062461
I0916 22:42:07.437260 13570 solver.cpp:336]     Train net output #0: loss = 0.0624608 (* 1 = 0.0624608 loss)
I0916 22:42:07.437286 13570 sgd_solver.cpp:136] Iteration 40100, lr = 0.001, m = 0.9
I0916 22:42:18.783710 13546 data_reader.cpp:305] Starting prefetch of epoch 16
I0916 22:42:26.679811 13570 solver.cpp:314] Iteration 40200 (5.19059 iter/s, 19.2656s/100 iter), loss = 0.0452116
I0916 22:42:26.679833 13570 solver.cpp:336]     Train net output #0: loss = 0.0452115 (* 1 = 0.0452115 loss)
I0916 22:42:26.679837 13570 sgd_solver.cpp:136] Iteration 40200, lr = 0.001, m = 0.9
I0916 22:42:45.943516 13570 solver.cpp:314] Iteration 40300 (5.19125 iter/s, 19.2632s/100 iter), loss = 0.0781107
I0916 22:42:45.943568 13570 solver.cpp:336]     Train net output #0: loss = 0.0781106 (* 1 = 0.0781106 loss)
I0916 22:42:45.943573 13570 sgd_solver.cpp:136] Iteration 40300, lr = 0.001, m = 0.9
I0916 22:42:50.653384 13573 data_reader.cpp:305] Starting prefetch of epoch 26
I0916 22:43:05.583230 13570 solver.cpp:314] Iteration 40400 (5.09187 iter/s, 19.6392s/100 iter), loss = 0.0803965
I0916 22:43:05.583256 13570 solver.cpp:336]     Train net output #0: loss = 0.0803963 (* 1 = 0.0803963 loss)
I0916 22:43:05.583261 13570 sgd_solver.cpp:136] Iteration 40400, lr = 0.001, m = 0.9
I0916 22:43:24.887627 13570 solver.cpp:314] Iteration 40500 (5.18031 iter/s, 19.3039s/100 iter), loss = 0.0725264
I0916 22:43:24.902150 13570 solver.cpp:336]     Train net output #0: loss = 0.0725263 (* 1 = 0.0725263 loss)
I0916 22:43:24.902330 13570 sgd_solver.cpp:136] Iteration 40500, lr = 0.001, m = 0.9
I0916 22:43:44.530555 13570 solver.cpp:314] Iteration 40600 (5.09103 iter/s, 19.6424s/100 iter), loss = 0.072069
I0916 22:43:44.530580 13570 solver.cpp:336]     Train net output #0: loss = 0.0720688 (* 1 = 0.0720688 loss)
I0916 22:43:44.530586 13570 sgd_solver.cpp:136] Iteration 40600, lr = 0.001, m = 0.9
I0916 22:43:55.207365 13546 data_reader.cpp:305] Starting prefetch of epoch 17
I0916 22:44:03.863397 13570 solver.cpp:314] Iteration 40700 (5.17269 iter/s, 19.3323s/100 iter), loss = 0.0743773
I0916 22:44:03.863427 13570 solver.cpp:336]     Train net output #0: loss = 0.0743771 (* 1 = 0.0743771 loss)
I0916 22:44:03.863435 13570 sgd_solver.cpp:136] Iteration 40700, lr = 0.001, m = 0.9
I0916 22:44:23.704002 13570 solver.cpp:314] Iteration 40800 (5.04031 iter/s, 19.8401s/100 iter), loss = 0.11675
I0916 22:44:23.704027 13570 solver.cpp:336]     Train net output #0: loss = 0.11675 (* 1 = 0.11675 loss)
I0916 22:44:23.704033 13570 sgd_solver.cpp:136] Iteration 40800, lr = 0.001, m = 0.9
I0916 22:44:43.701822 13570 solver.cpp:314] Iteration 40900 (5.00068 iter/s, 19.9973s/100 iter), loss = 0.0648626
I0916 22:44:43.718503 13570 solver.cpp:336]     Train net output #0: loss = 0.0648624 (* 1 = 0.0648624 loss)
I0916 22:44:43.718703 13570 sgd_solver.cpp:136] Iteration 40900, lr = 0.001, m = 0.9
I0916 22:45:00.677501 13576 data_reader.cpp:305] Starting prefetch of epoch 28
I0916 22:45:03.389329 13570 solver.cpp:368] Sparsity after update:
I0916 22:45:03.397192 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 22:45:03.397204 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 22:45:03.397213 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 22:45:03.397217 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 22:45:03.397222 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 22:45:03.397225 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 22:45:03.397229 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 22:45:03.397233 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 22:45:03.397236 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 22:45:03.397239 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 22:45:03.397251 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 22:45:03.397255 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 22:45:03.397259 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 22:45:03.397263 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 22:45:03.397265 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 22:45:03.397269 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 22:45:03.397274 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 22:45:03.397277 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 22:45:03.397280 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 22:45:03.570483 13570 solver.cpp:314] Iteration 41000 (5.03319 iter/s, 19.8681s/100 iter), loss = 0.0866997
I0916 22:45:03.570507 13570 solver.cpp:336]     Train net output #0: loss = 0.0866996 (* 1 = 0.0866996 loss)
I0916 22:45:03.570510 13570 sgd_solver.cpp:136] Iteration 41000, lr = 0.001, m = 0.9
I0916 22:45:22.974095 13570 solver.cpp:314] Iteration 41100 (5.15382 iter/s, 19.4031s/100 iter), loss = 0.0539991
I0916 22:45:22.974565 13570 solver.cpp:336]     Train net output #0: loss = 0.053999 (* 1 = 0.053999 loss)
I0916 22:45:22.974575 13570 sgd_solver.cpp:136] Iteration 41100, lr = 0.001, m = 0.9
I0916 22:45:42.808414 13570 solver.cpp:314] Iteration 41200 (5.04191 iter/s, 19.8338s/100 iter), loss = 0.0547939
I0916 22:45:42.808439 13570 solver.cpp:336]     Train net output #0: loss = 0.0547938 (* 1 = 0.0547938 loss)
I0916 22:45:42.808444 13570 sgd_solver.cpp:136] Iteration 41200, lr = 0.001, m = 0.9
I0916 22:46:02.381341 13570 solver.cpp:314] Iteration 41300 (5.10924 iter/s, 19.5724s/100 iter), loss = 0.0330257
I0916 22:46:02.381443 13570 solver.cpp:336]     Train net output #0: loss = 0.0330256 (* 1 = 0.0330256 loss)
I0916 22:46:02.381456 13570 sgd_solver.cpp:136] Iteration 41300, lr = 0.001, m = 0.9
I0916 22:46:05.672762 13578 data_reader.cpp:305] Starting prefetch of epoch 24
I0916 22:46:22.138162 13570 solver.cpp:314] Iteration 41400 (5.06168 iter/s, 19.7563s/100 iter), loss = 0.0955008
I0916 22:46:22.138185 13570 solver.cpp:336]     Train net output #0: loss = 0.0955007 (* 1 = 0.0955007 loss)
I0916 22:46:22.138191 13570 sgd_solver.cpp:136] Iteration 41400, lr = 0.001, m = 0.9
I0916 22:46:37.922792 13574 data_reader.cpp:305] Starting prefetch of epoch 27
I0916 22:46:41.680539 13570 solver.cpp:314] Iteration 41500 (5.11723 iter/s, 19.5418s/100 iter), loss = 0.0816391
I0916 22:46:41.680624 13570 solver.cpp:336]     Train net output #0: loss = 0.081639 (* 1 = 0.081639 loss)
I0916 22:46:41.680645 13570 sgd_solver.cpp:136] Iteration 41500, lr = 0.001, m = 0.9
I0916 22:47:01.457968 13570 solver.cpp:314] Iteration 41600 (5.05641 iter/s, 19.7769s/100 iter), loss = 0.0454548
I0916 22:47:01.457998 13570 solver.cpp:336]     Train net output #0: loss = 0.0454546 (* 1 = 0.0454546 loss)
I0916 22:47:01.458003 13570 sgd_solver.cpp:136] Iteration 41600, lr = 0.001, m = 0.9
I0916 22:47:21.110638 13570 solver.cpp:314] Iteration 41700 (5.08851 iter/s, 19.6521s/100 iter), loss = 0.0940945
I0916 22:47:21.110711 13570 solver.cpp:336]     Train net output #0: loss = 0.0940943 (* 1 = 0.0940943 loss)
I0916 22:47:21.110716 13570 sgd_solver.cpp:136] Iteration 41700, lr = 0.001, m = 0.9
I0916 22:47:40.552846 13570 solver.cpp:314] Iteration 41800 (5.14359 iter/s, 19.4417s/100 iter), loss = 0.0820077
I0916 22:47:40.552870 13570 solver.cpp:336]     Train net output #0: loss = 0.0820076 (* 1 = 0.0820076 loss)
I0916 22:47:40.552873 13570 sgd_solver.cpp:136] Iteration 41800, lr = 0.001, m = 0.9
I0916 22:47:42.812830 13576 data_reader.cpp:305] Starting prefetch of epoch 29
I0916 22:48:00.029337 13570 solver.cpp:314] Iteration 41900 (5.13454 iter/s, 19.4759s/100 iter), loss = 0.0767126
I0916 22:48:00.030187 13570 solver.cpp:336]     Train net output #0: loss = 0.0767125 (* 1 = 0.0767125 loss)
I0916 22:48:00.030205 13570 sgd_solver.cpp:136] Iteration 41900, lr = 0.001, m = 0.9
I0916 22:48:19.654906 13570 solver.cpp:368] Sparsity after update:
I0916 22:48:19.663115 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 22:48:19.663133 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 22:48:19.663142 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 22:48:19.663146 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 22:48:19.663148 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 22:48:19.663152 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 22:48:19.663172 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 22:48:19.663182 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 22:48:19.663189 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 22:48:19.663198 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 22:48:19.663206 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 22:48:19.663218 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 22:48:19.663228 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 22:48:19.663239 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 22:48:19.663244 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 22:48:19.663247 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 22:48:19.663255 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 22:48:19.663264 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 22:48:19.663269 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 22:48:19.663285 13570 solver.cpp:563] Iteration 42000, Testing net (#0)
I0916 22:48:27.008527 13600 data_reader.cpp:305] Starting prefetch of epoch 2
I0916 22:48:31.419229 13570 solver.cpp:655]     Test net output #0: accuracy/top1 = 0.954198
I0916 22:48:31.419289 13570 solver.cpp:655]     Test net output #1: accuracy/top5 = 1
I0916 22:48:31.419297 13570 solver.cpp:655]     Test net output #2: loss = 0.159733 (* 1 = 0.159733 loss)
I0916 22:48:31.419329 13570 solver.cpp:265] [MultiGPU] Tests completed in 11.7557s
I0916 22:48:31.649610 13570 solver.cpp:314] Iteration 42000 (3.16262 iter/s, 31.6194s/100 iter), loss = 0.0516812
I0916 22:48:31.649663 13570 solver.cpp:336]     Train net output #0: loss = 0.0516811 (* 1 = 0.0516811 loss)
I0916 22:48:31.649682 13570 sgd_solver.cpp:136] Iteration 42000, lr = 0.001, m = 0.9
I0916 22:48:50.899164 13570 solver.cpp:314] Iteration 42100 (5.19507 iter/s, 19.249s/100 iter), loss = 0.0565575
I0916 22:48:50.899188 13570 solver.cpp:336]     Train net output #0: loss = 0.0565574 (* 1 = 0.0565574 loss)
I0916 22:48:50.899193 13570 sgd_solver.cpp:136] Iteration 42100, lr = 0.001, m = 0.9
I0916 22:48:58.976613 13573 data_reader.cpp:305] Starting prefetch of epoch 27
I0916 22:49:10.321295 13570 solver.cpp:314] Iteration 42200 (5.14891 iter/s, 19.4216s/100 iter), loss = 0.0592744
I0916 22:49:10.321350 13570 solver.cpp:336]     Train net output #0: loss = 0.0592742 (* 1 = 0.0592742 loss)
I0916 22:49:10.321357 13570 sgd_solver.cpp:136] Iteration 42200, lr = 0.001, m = 0.9
I0916 22:49:29.957427 13570 solver.cpp:314] Iteration 42300 (5.09279 iter/s, 19.6356s/100 iter), loss = 0.0802638
I0916 22:49:29.957449 13570 solver.cpp:336]     Train net output #0: loss = 0.0802637 (* 1 = 0.0802637 loss)
I0916 22:49:29.957453 13570 sgd_solver.cpp:136] Iteration 42300, lr = 0.001, m = 0.9
I0916 22:49:49.541329 13570 solver.cpp:314] Iteration 42400 (5.10638 iter/s, 19.5834s/100 iter), loss = 0.0598143
I0916 22:49:49.541401 13570 solver.cpp:336]     Train net output #0: loss = 0.0598142 (* 1 = 0.0598142 loss)
I0916 22:49:49.541409 13570 sgd_solver.cpp:136] Iteration 42400, lr = 0.001, m = 0.9
I0916 22:50:03.840003 13546 data_reader.cpp:305] Starting prefetch of epoch 18
I0916 22:50:09.081315 13570 solver.cpp:314] Iteration 42500 (5.11785 iter/s, 19.5394s/100 iter), loss = 0.0492221
I0916 22:50:09.081339 13570 solver.cpp:336]     Train net output #0: loss = 0.0492219 (* 1 = 0.0492219 loss)
I0916 22:50:09.081344 13570 sgd_solver.cpp:136] Iteration 42500, lr = 0.001, m = 0.9
I0916 22:50:28.526091 13570 solver.cpp:314] Iteration 42600 (5.14292 iter/s, 19.4442s/100 iter), loss = 0.084746
I0916 22:50:28.526149 13570 solver.cpp:336]     Train net output #0: loss = 0.0847459 (* 1 = 0.0847459 loss)
I0916 22:50:28.526167 13570 sgd_solver.cpp:136] Iteration 42600, lr = 0.001, m = 0.9
I0916 22:50:48.327483 13570 solver.cpp:314] Iteration 42700 (5.05029 iter/s, 19.8008s/100 iter), loss = 0.0593378
I0916 22:50:48.327507 13570 solver.cpp:336]     Train net output #0: loss = 0.0593377 (* 1 = 0.0593377 loss)
I0916 22:50:48.327510 13570 sgd_solver.cpp:136] Iteration 42700, lr = 0.001, m = 0.9
I0916 22:51:08.133787 13570 solver.cpp:314] Iteration 42800 (5.04904 iter/s, 19.8058s/100 iter), loss = 0.0551964
I0916 22:51:08.133839 13570 solver.cpp:336]     Train net output #0: loss = 0.0551962 (* 1 = 0.0551962 loss)
I0916 22:51:08.133846 13570 sgd_solver.cpp:136] Iteration 42800, lr = 0.001, m = 0.9
I0916 22:51:08.725590 13578 data_reader.cpp:305] Starting prefetch of epoch 25
I0916 22:51:27.407416 13570 solver.cpp:314] Iteration 42900 (5.18858 iter/s, 19.2731s/100 iter), loss = 0.0494708
I0916 22:51:27.407443 13570 solver.cpp:336]     Train net output #0: loss = 0.0494707 (* 1 = 0.0494707 loss)
I0916 22:51:27.407449 13570 sgd_solver.cpp:136] Iteration 42900, lr = 0.001, m = 0.9
I0916 22:51:41.091259 13574 data_reader.cpp:305] Starting prefetch of epoch 28
I0916 22:51:46.942561 13570 solver.cpp:368] Sparsity after update:
I0916 22:51:46.958097 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 22:51:46.958149 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 22:51:46.958174 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 22:51:46.958187 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 22:51:46.958200 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 22:51:46.958214 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 22:51:46.958227 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 22:51:46.958240 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 22:51:46.958251 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 22:51:46.958263 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 22:51:46.958274 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 22:51:46.958287 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 22:51:46.958298 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 22:51:46.958310 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 22:51:46.958323 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 22:51:46.958338 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 22:51:46.958351 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 22:51:46.958364 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 22:51:46.958376 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 22:51:47.128099 13570 solver.cpp:314] Iteration 43000 (5.07096 iter/s, 19.7201s/100 iter), loss = 0.0503758
I0916 22:51:47.128124 13570 solver.cpp:336]     Train net output #0: loss = 0.0503757 (* 1 = 0.0503757 loss)
I0916 22:51:47.128130 13570 sgd_solver.cpp:136] Iteration 43000, lr = 0.001, m = 0.9
I0916 22:52:06.897240 13570 solver.cpp:314] Iteration 43100 (5.05853 iter/s, 19.7686s/100 iter), loss = 0.0962716
I0916 22:52:06.897265 13570 solver.cpp:336]     Train net output #0: loss = 0.0962714 (* 1 = 0.0962714 loss)
I0916 22:52:06.897271 13570 sgd_solver.cpp:136] Iteration 43100, lr = 0.001, m = 0.9
I0916 22:52:26.165985 13570 solver.cpp:314] Iteration 43200 (5.1899 iter/s, 19.2682s/100 iter), loss = 0.0877511
I0916 22:52:26.166059 13570 solver.cpp:336]     Train net output #0: loss = 0.087751 (* 1 = 0.087751 loss)
I0916 22:52:26.166065 13570 sgd_solver.cpp:136] Iteration 43200, lr = 0.001, m = 0.9
I0916 22:52:45.267606 13546 data_reader.cpp:305] Starting prefetch of epoch 19
I0916 22:52:45.422863 13570 solver.cpp:314] Iteration 43300 (5.19309 iter/s, 19.2563s/100 iter), loss = 0.049812
I0916 22:52:45.422888 13570 solver.cpp:336]     Train net output #0: loss = 0.0498119 (* 1 = 0.0498119 loss)
I0916 22:52:45.422894 13570 sgd_solver.cpp:136] Iteration 43300, lr = 0.001, m = 0.9
I0916 22:53:04.809875 13570 solver.cpp:314] Iteration 43400 (5.15824 iter/s, 19.3865s/100 iter), loss = 0.078804
I0916 22:53:04.809938 13570 solver.cpp:336]     Train net output #0: loss = 0.0788039 (* 1 = 0.0788039 loss)
I0916 22:53:04.809947 13570 sgd_solver.cpp:136] Iteration 43400, lr = 0.001, m = 0.9
I0916 22:53:24.358940 13570 solver.cpp:314] Iteration 43500 (5.11548 iter/s, 19.5485s/100 iter), loss = 0.0882238
I0916 22:53:24.358964 13570 solver.cpp:336]     Train net output #0: loss = 0.0882236 (* 1 = 0.0882236 loss)
I0916 22:53:24.358969 13570 sgd_solver.cpp:136] Iteration 43500, lr = 0.001, m = 0.9
I0916 22:53:43.846901 13570 solver.cpp:314] Iteration 43600 (5.13152 iter/s, 19.4874s/100 iter), loss = 0.0624362
I0916 22:53:43.846961 13570 solver.cpp:336]     Train net output #0: loss = 0.0624361 (* 1 = 0.0624361 loss)
I0916 22:53:43.846966 13570 sgd_solver.cpp:136] Iteration 43600, lr = 0.001, m = 0.9
I0916 22:53:49.587294 13546 data_reader.cpp:305] Starting prefetch of epoch 20
I0916 22:54:03.088802 13570 solver.cpp:314] Iteration 43700 (5.19714 iter/s, 19.2414s/100 iter), loss = 0.101973
I0916 22:54:03.088824 13570 solver.cpp:336]     Train net output #0: loss = 0.101973 (* 1 = 0.101973 loss)
I0916 22:54:03.088829 13570 sgd_solver.cpp:136] Iteration 43700, lr = 0.001, m = 0.9
I0916 22:54:21.680814 13573 data_reader.cpp:305] Starting prefetch of epoch 28
I0916 22:54:22.651444 13570 solver.cpp:314] Iteration 43800 (5.11193 iter/s, 19.5621s/100 iter), loss = 0.0381728
I0916 22:54:22.651468 13570 solver.cpp:336]     Train net output #0: loss = 0.0381726 (* 1 = 0.0381726 loss)
I0916 22:54:22.651473 13570 sgd_solver.cpp:136] Iteration 43800, lr = 0.001, m = 0.9
I0916 22:54:42.130373 13570 solver.cpp:314] Iteration 43900 (5.13389 iter/s, 19.4784s/100 iter), loss = 0.048848
I0916 22:54:42.130398 13570 solver.cpp:336]     Train net output #0: loss = 0.0488478 (* 1 = 0.0488478 loss)
I0916 22:54:42.130403 13570 sgd_solver.cpp:136] Iteration 43900, lr = 0.001, m = 0.9
I0916 22:55:01.356218 13570 solver.cpp:368] Sparsity after update:
I0916 22:55:01.365273 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 22:55:01.365288 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 22:55:01.365294 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 22:55:01.365298 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 22:55:01.365299 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 22:55:01.365300 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 22:55:01.365303 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 22:55:01.365304 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 22:55:01.365306 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 22:55:01.365309 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 22:55:01.365310 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 22:55:01.365312 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 22:55:01.365314 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 22:55:01.365316 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 22:55:01.365319 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 22:55:01.365320 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 22:55:01.365322 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 22:55:01.365324 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 22:55:01.365325 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 22:55:01.365334 13570 solver.cpp:563] Iteration 44000, Testing net (#0)
I0916 22:55:04.870558 13568 data_reader.cpp:305] Starting prefetch of epoch 3
I0916 22:55:12.785444 13570 solver.cpp:655]     Test net output #0: accuracy/top1 = 0.951627
I0916 22:55:12.785470 13570 solver.cpp:655]     Test net output #1: accuracy/top5 = 1
I0916 22:55:12.785475 13570 solver.cpp:655]     Test net output #2: loss = 0.148007 (* 1 = 0.148007 loss)
I0916 22:55:12.785508 13570 solver.cpp:265] [MultiGPU] Tests completed in 11.4199s
I0916 22:55:13.008673 13570 solver.cpp:314] Iteration 44000 (3.23861 iter/s, 30.8774s/100 iter), loss = 0.0918156
I0916 22:55:13.008698 13570 solver.cpp:336]     Train net output #0: loss = 0.0918154 (* 1 = 0.0918154 loss)
I0916 22:55:13.008704 13570 sgd_solver.cpp:136] Iteration 44000, lr = 0.001, m = 0.9
I0916 22:55:32.392392 13570 solver.cpp:314] Iteration 44100 (5.15911 iter/s, 19.3832s/100 iter), loss = 0.0734994
I0916 22:55:32.392529 13570 solver.cpp:336]     Train net output #0: loss = 0.0734993 (* 1 = 0.0734993 loss)
I0916 22:55:32.392536 13570 sgd_solver.cpp:136] Iteration 44100, lr = 0.001, m = 0.9
I0916 22:55:37.508002 13576 data_reader.cpp:305] Starting prefetch of epoch 30
I0916 22:55:51.602455 13570 solver.cpp:314] Iteration 44200 (5.20575 iter/s, 19.2095s/100 iter), loss = 0.0611329
I0916 22:55:51.602479 13570 solver.cpp:336]     Train net output #0: loss = 0.0611328 (* 1 = 0.0611328 loss)
I0916 22:55:51.602483 13570 sgd_solver.cpp:136] Iteration 44200, lr = 0.001, m = 0.9
I0916 22:56:09.303495 13573 data_reader.cpp:305] Starting prefetch of epoch 29
I0916 22:56:10.980659 13570 solver.cpp:314] Iteration 44300 (5.16058 iter/s, 19.3777s/100 iter), loss = 0.0430401
I0916 22:56:10.980679 13570 solver.cpp:336]     Train net output #0: loss = 0.0430399 (* 1 = 0.0430399 loss)
I0916 22:56:10.980684 13570 sgd_solver.cpp:136] Iteration 44300, lr = 0.001, m = 0.9
I0916 22:56:30.650393 13570 solver.cpp:314] Iteration 44400 (5.08409 iter/s, 19.6692s/100 iter), loss = 0.0543779
I0916 22:56:30.650413 13570 solver.cpp:336]     Train net output #0: loss = 0.0543778 (* 1 = 0.0543778 loss)
I0916 22:56:30.650419 13570 sgd_solver.cpp:136] Iteration 44400, lr = 0.001, m = 0.9
I0916 22:56:50.098057 13570 solver.cpp:314] Iteration 44500 (5.14215 iter/s, 19.4471s/100 iter), loss = 0.0567879
I0916 22:56:50.098112 13570 solver.cpp:336]     Train net output #0: loss = 0.0567878 (* 1 = 0.0567878 loss)
I0916 22:56:50.098119 13570 sgd_solver.cpp:136] Iteration 44500, lr = 0.001, m = 0.9
I0916 22:57:09.195767 13570 solver.cpp:314] Iteration 44600 (5.23638 iter/s, 19.0972s/100 iter), loss = 0.0703127
I0916 22:57:09.195793 13570 solver.cpp:336]     Train net output #0: loss = 0.0703126 (* 1 = 0.0703126 loss)
I0916 22:57:09.195799 13570 sgd_solver.cpp:136] Iteration 44600, lr = 0.001, m = 0.9
I0916 22:57:13.405563 13578 data_reader.cpp:305] Starting prefetch of epoch 26
I0916 22:57:28.657826 13570 solver.cpp:314] Iteration 44700 (5.13835 iter/s, 19.4615s/100 iter), loss = 0.0627093
I0916 22:57:28.657907 13570 solver.cpp:336]     Train net output #0: loss = 0.0627092 (* 1 = 0.0627092 loss)
I0916 22:57:28.657912 13570 sgd_solver.cpp:136] Iteration 44700, lr = 0.001, m = 0.9
I0916 22:57:48.126631 13570 solver.cpp:314] Iteration 44800 (5.13656 iter/s, 19.4683s/100 iter), loss = 0.0855177
I0916 22:57:48.126658 13570 solver.cpp:336]     Train net output #0: loss = 0.0855176 (* 1 = 0.0855176 loss)
I0916 22:57:48.126663 13570 sgd_solver.cpp:136] Iteration 44800, lr = 0.001, m = 0.9
I0916 22:58:07.719213 13570 solver.cpp:314] Iteration 44900 (5.10412 iter/s, 19.592s/100 iter), loss = 0.0528402
I0916 22:58:07.719298 13570 solver.cpp:336]     Train net output #0: loss = 0.0528401 (* 1 = 0.0528401 loss)
I0916 22:58:07.719313 13570 sgd_solver.cpp:136] Iteration 44900, lr = 0.001, m = 0.9
I0916 22:58:17.941295 13576 data_reader.cpp:305] Starting prefetch of epoch 31
I0916 22:58:26.843730 13570 solver.cpp:368] Sparsity after update:
I0916 22:58:26.851810 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 22:58:26.851828 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 22:58:26.851836 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 22:58:26.851841 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 22:58:26.851845 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 22:58:26.851847 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 22:58:26.851850 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 22:58:26.851855 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 22:58:26.851857 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 22:58:26.851861 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 22:58:26.851864 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 22:58:26.851867 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 22:58:26.851871 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 22:58:26.851879 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 22:58:26.851886 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 22:58:26.851891 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 22:58:26.851896 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 22:58:26.851900 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 22:58:26.851903 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 22:58:26.938170 13623 sgd_solver.cpp:48] MultiStep Status: Iteration 45000, step = 2
I0916 22:58:26.938171 13624 sgd_solver.cpp:48] MultiStep Status: Iteration 45000, step = 2
I0916 22:58:26.938172 13625 sgd_solver.cpp:48] MultiStep Status: Iteration 45000, step = 2
I0916 22:58:27.026938 13570 solver.cpp:314] Iteration 45000 (5.17942 iter/s, 19.3072s/100 iter), loss = 0.078834
I0916 22:58:27.026963 13570 solver.cpp:336]     Train net output #0: loss = 0.0788338 (* 1 = 0.0788338 loss)
I0916 22:58:27.026968 13570 sgd_solver.cpp:136] Iteration 45000, lr = 0.0001, m = 0.9
I0916 22:58:46.492411 13570 solver.cpp:314] Iteration 45100 (5.13744 iter/s, 19.4649s/100 iter), loss = 0.102599
I0916 22:58:46.492486 13570 solver.cpp:336]     Train net output #0: loss = 0.102599 (* 1 = 0.102599 loss)
I0916 22:58:46.492491 13570 sgd_solver.cpp:136] Iteration 45100, lr = 0.0001, m = 0.9
I0916 22:58:49.800247 13544 data_reader.cpp:305] Starting prefetch of epoch 27
I0916 22:59:05.749977 13570 solver.cpp:314] Iteration 45200 (5.19291 iter/s, 19.257s/100 iter), loss = 0.0652478
I0916 22:59:05.750003 13570 solver.cpp:336]     Train net output #0: loss = 0.0652476 (* 1 = 0.0652476 loss)
I0916 22:59:05.750010 13570 sgd_solver.cpp:136] Iteration 45200, lr = 0.0001, m = 0.9
I0916 22:59:25.336900 13570 solver.cpp:314] Iteration 45300 (5.10559 iter/s, 19.5864s/100 iter), loss = 0.0730647
I0916 22:59:25.336997 13570 solver.cpp:336]     Train net output #0: loss = 0.0730645 (* 1 = 0.0730645 loss)
I0916 22:59:25.337014 13570 sgd_solver.cpp:136] Iteration 45300, lr = 0.0001, m = 0.9
I0916 22:59:45.102131 13570 solver.cpp:314] Iteration 45400 (5.05953 iter/s, 19.7647s/100 iter), loss = 0.065586
I0916 22:59:45.102341 13570 solver.cpp:336]     Train net output #0: loss = 0.0655859 (* 1 = 0.0655859 loss)
I0916 22:59:45.102398 13570 sgd_solver.cpp:136] Iteration 45400, lr = 0.0001, m = 0.9
I0916 22:59:54.487807 13578 data_reader.cpp:305] Starting prefetch of epoch 27
I0916 23:00:04.648216 13570 solver.cpp:314] Iteration 45500 (5.11625 iter/s, 19.5456s/100 iter), loss = 0.0766896
I0916 23:00:04.648432 13570 solver.cpp:336]     Train net output #0: loss = 0.0766894 (* 1 = 0.0766894 loss)
I0916 23:00:04.648439 13570 sgd_solver.cpp:136] Iteration 45500, lr = 0.0001, m = 0.9
I0916 23:00:24.368736 13570 solver.cpp:314] Iteration 45600 (5.071 iter/s, 19.72s/100 iter), loss = 0.0966067
I0916 23:00:24.368760 13570 solver.cpp:336]     Train net output #0: loss = 0.0966066 (* 1 = 0.0966066 loss)
I0916 23:00:24.368767 13570 sgd_solver.cpp:136] Iteration 45600, lr = 0.0001, m = 0.9
I0916 23:00:44.123468 13570 solver.cpp:314] Iteration 45700 (5.06222 iter/s, 19.7542s/100 iter), loss = 0.0405651
I0916 23:00:44.123540 13570 solver.cpp:336]     Train net output #0: loss = 0.040565 (* 1 = 0.040565 loss)
I0916 23:00:44.123548 13570 sgd_solver.cpp:136] Iteration 45700, lr = 0.0001, m = 0.9
I0916 23:00:59.373009 13578 data_reader.cpp:305] Starting prefetch of epoch 28
I0916 23:01:03.660224 13570 solver.cpp:314] Iteration 45800 (5.1187 iter/s, 19.5362s/100 iter), loss = 0.1034
I0916 23:01:03.660248 13570 solver.cpp:336]     Train net output #0: loss = 0.103399 (* 1 = 0.103399 loss)
I0916 23:01:03.660254 13570 sgd_solver.cpp:136] Iteration 45800, lr = 0.0001, m = 0.9
I0916 23:01:23.210969 13570 solver.cpp:314] Iteration 45900 (5.11504 iter/s, 19.5502s/100 iter), loss = 0.0451688
I0916 23:01:23.217470 13570 solver.cpp:336]     Train net output #0: loss = 0.0451686 (* 1 = 0.0451686 loss)
I0916 23:01:23.217505 13570 sgd_solver.cpp:136] Iteration 45900, lr = 0.0001, m = 0.9
I0916 23:01:31.964310 13574 data_reader.cpp:305] Starting prefetch of epoch 29
I0916 23:01:42.889168 13570 solver.cpp:368] Sparsity after update:
I0916 23:01:42.894881 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 23:01:42.894896 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 23:01:42.894906 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 23:01:42.894908 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 23:01:42.894912 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 23:01:42.894917 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 23:01:42.894919 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 23:01:42.894922 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 23:01:42.894925 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 23:01:42.894928 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 23:01:42.894932 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 23:01:42.894937 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 23:01:42.894939 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 23:01:42.894943 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 23:01:42.894948 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 23:01:42.894951 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 23:01:42.894955 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 23:01:42.894959 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 23:01:42.894963 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 23:01:42.894976 13570 solver.cpp:563] Iteration 46000, Testing net (#0)
I0916 23:01:54.388907 13570 solver.cpp:655]     Test net output #0: accuracy/top1 = 0.954587
I0916 23:01:54.388998 13570 solver.cpp:655]     Test net output #1: accuracy/top5 = 1
I0916 23:01:54.389006 13570 solver.cpp:655]     Test net output #2: loss = 0.161903 (* 1 = 0.161903 loss)
I0916 23:01:54.389034 13570 solver.cpp:265] [MultiGPU] Tests completed in 11.4937s
I0916 23:01:54.593240 13570 solver.cpp:314] Iteration 46000 (3.1866 iter/s, 31.3814s/100 iter), loss = 0.0616718
I0916 23:01:54.593266 13570 solver.cpp:336]     Train net output #0: loss = 0.0616716 (* 1 = 0.0616716 loss)
I0916 23:01:54.593271 13570 sgd_solver.cpp:136] Iteration 46000, lr = 0.0001, m = 0.9
I0916 23:02:14.068815 13570 solver.cpp:314] Iteration 46100 (5.13478 iter/s, 19.475s/100 iter), loss = 0.0465749
I0916 23:02:14.068836 13570 solver.cpp:336]     Train net output #0: loss = 0.0465748 (* 1 = 0.0465748 loss)
I0916 23:02:14.068841 13570 sgd_solver.cpp:136] Iteration 46100, lr = 0.0001, m = 0.9
I0916 23:02:15.894227 13546 data_reader.cpp:305] Starting prefetch of epoch 21
I0916 23:02:33.664311 13570 solver.cpp:314] Iteration 46200 (5.10336 iter/s, 19.5949s/100 iter), loss = 0.0416221
I0916 23:02:33.664376 13570 solver.cpp:336]     Train net output #0: loss = 0.0416218 (* 1 = 0.0416218 loss)
I0916 23:02:33.664381 13570 sgd_solver.cpp:136] Iteration 46200, lr = 0.0001, m = 0.9
I0916 23:02:53.357906 13570 solver.cpp:314] Iteration 46300 (5.07794 iter/s, 19.693s/100 iter), loss = 0.0599367
I0916 23:02:53.357996 13570 solver.cpp:336]     Train net output #0: loss = 0.0599365 (* 1 = 0.0599365 loss)
I0916 23:02:53.358031 13570 sgd_solver.cpp:136] Iteration 46300, lr = 0.0001, m = 0.9
I0916 23:03:12.994031 13570 solver.cpp:314] Iteration 46400 (5.0928 iter/s, 19.6356s/100 iter), loss = 0.0679505
I0916 23:03:12.994102 13570 solver.cpp:336]     Train net output #0: loss = 0.0679503 (* 1 = 0.0679503 loss)
I0916 23:03:12.994107 13570 sgd_solver.cpp:136] Iteration 46400, lr = 0.0001, m = 0.9
I0916 23:03:20.941064 13578 data_reader.cpp:305] Starting prefetch of epoch 29
I0916 23:03:32.515727 13570 solver.cpp:314] Iteration 46500 (5.12265 iter/s, 19.5212s/100 iter), loss = 0.0923736
I0916 23:03:32.515756 13570 solver.cpp:336]     Train net output #0: loss = 0.0923734 (* 1 = 0.0923734 loss)
I0916 23:03:32.515763 13570 sgd_solver.cpp:136] Iteration 46500, lr = 0.0001, m = 0.9
I0916 23:03:52.313277 13570 solver.cpp:314] Iteration 46600 (5.05127 iter/s, 19.797s/100 iter), loss = 0.040538
I0916 23:03:52.313330 13570 solver.cpp:336]     Train net output #0: loss = 0.0405378 (* 1 = 0.0405378 loss)
I0916 23:03:52.313335 13570 sgd_solver.cpp:136] Iteration 46600, lr = 0.0001, m = 0.9
I0916 23:03:53.382633 13573 data_reader.cpp:305] Starting prefetch of epoch 30
I0916 23:04:11.977183 13570 solver.cpp:314] Iteration 46700 (5.0856 iter/s, 19.6634s/100 iter), loss = 0.0771628
I0916 23:04:11.977212 13570 solver.cpp:336]     Train net output #0: loss = 0.0771626 (* 1 = 0.0771626 loss)
I0916 23:04:11.977219 13570 sgd_solver.cpp:136] Iteration 46700, lr = 0.0001, m = 0.9
I0916 23:04:31.718873 13570 solver.cpp:314] Iteration 46800 (5.06556 iter/s, 19.7411s/100 iter), loss = 0.0840155
I0916 23:04:31.718932 13570 solver.cpp:336]     Train net output #0: loss = 0.0840153 (* 1 = 0.0840153 loss)
I0916 23:04:31.718937 13570 sgd_solver.cpp:136] Iteration 46800, lr = 0.0001, m = 0.9
I0916 23:04:51.263238 13570 solver.cpp:314] Iteration 46900 (5.11671 iter/s, 19.5438s/100 iter), loss = 0.108085
I0916 23:04:51.263265 13570 solver.cpp:336]     Train net output #0: loss = 0.108084 (* 1 = 0.108084 loss)
I0916 23:04:51.263272 13570 sgd_solver.cpp:136] Iteration 46900, lr = 0.0001, m = 0.9
I0916 23:04:58.283468 13576 data_reader.cpp:305] Starting prefetch of epoch 32
I0916 23:05:11.095844 13570 solver.cpp:368] Sparsity after update:
I0916 23:05:11.102035 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 23:05:11.102046 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 23:05:11.102052 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 23:05:11.102056 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 23:05:11.102059 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 23:05:11.102063 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 23:05:11.102066 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 23:05:11.102069 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 23:05:11.102078 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 23:05:11.102082 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 23:05:11.102085 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 23:05:11.102093 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 23:05:11.102097 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 23:05:11.102100 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 23:05:11.102103 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 23:05:11.102107 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 23:05:11.102110 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 23:05:11.102113 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 23:05:11.102116 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 23:05:11.271620 13570 solver.cpp:314] Iteration 47000 (4.99804 iter/s, 20.0078s/100 iter), loss = 0.044046
I0916 23:05:11.271666 13570 solver.cpp:336]     Train net output #0: loss = 0.0440458 (* 1 = 0.0440458 loss)
I0916 23:05:11.271677 13570 sgd_solver.cpp:136] Iteration 47000, lr = 0.0001, m = 0.9
I0916 23:05:31.018438 13570 solver.cpp:314] Iteration 47100 (5.06425 iter/s, 19.7463s/100 iter), loss = 0.0639589
I0916 23:05:31.018465 13570 solver.cpp:336]     Train net output #0: loss = 0.0639587 (* 1 = 0.0639587 loss)
I0916 23:05:31.018472 13570 sgd_solver.cpp:136] Iteration 47100, lr = 0.0001, m = 0.9
I0916 23:05:51.124106 13570 solver.cpp:314] Iteration 47200 (4.97386 iter/s, 20.1051s/100 iter), loss = 0.0573039
I0916 23:05:51.124186 13570 solver.cpp:336]     Train net output #0: loss = 0.0573037 (* 1 = 0.0573037 loss)
I0916 23:05:51.124194 13570 sgd_solver.cpp:136] Iteration 47200, lr = 0.0001, m = 0.9
I0916 23:06:04.267562 13576 data_reader.cpp:305] Starting prefetch of epoch 33
I0916 23:06:11.073324 13570 solver.cpp:314] Iteration 47300 (5.01287 iter/s, 19.9487s/100 iter), loss = 0.183567
I0916 23:06:11.073348 13570 solver.cpp:336]     Train net output #0: loss = 0.183567 (* 1 = 0.183567 loss)
I0916 23:06:11.073354 13570 sgd_solver.cpp:136] Iteration 47300, lr = 0.0001, m = 0.9
I0916 23:06:30.985306 13570 solver.cpp:314] Iteration 47400 (5.02224 iter/s, 19.9114s/100 iter), loss = 0.0413993
I0916 23:06:30.985384 13570 solver.cpp:336]     Train net output #0: loss = 0.0413991 (* 1 = 0.0413991 loss)
I0916 23:06:30.985393 13570 sgd_solver.cpp:136] Iteration 47400, lr = 0.0001, m = 0.9
I0916 23:06:37.222122 13544 data_reader.cpp:305] Starting prefetch of epoch 28
I0916 23:06:50.784062 13570 solver.cpp:314] Iteration 47500 (5.05096 iter/s, 19.7982s/100 iter), loss = 0.0486552
I0916 23:06:50.784086 13570 solver.cpp:336]     Train net output #0: loss = 0.048655 (* 1 = 0.048655 loss)
I0916 23:06:50.784093 13570 sgd_solver.cpp:136] Iteration 47500, lr = 0.0001, m = 0.9
I0916 23:07:10.250844 13570 solver.cpp:314] Iteration 47600 (5.1371 iter/s, 19.4662s/100 iter), loss = 0.0829392
I0916 23:07:10.254142 13570 solver.cpp:336]     Train net output #0: loss = 0.0829389 (* 1 = 0.0829389 loss)
I0916 23:07:10.254163 13570 sgd_solver.cpp:136] Iteration 47600, lr = 0.0001, m = 0.9
I0916 23:07:30.057180 13570 solver.cpp:314] Iteration 47700 (5.04903 iter/s, 19.8058s/100 iter), loss = 0.0675972
I0916 23:07:30.057205 13570 solver.cpp:336]     Train net output #0: loss = 0.067597 (* 1 = 0.067597 loss)
I0916 23:07:30.057210 13570 sgd_solver.cpp:136] Iteration 47700, lr = 0.0001, m = 0.9
I0916 23:07:42.585299 13546 data_reader.cpp:305] Starting prefetch of epoch 22
I0916 23:07:50.109248 13570 solver.cpp:314] Iteration 47800 (4.98716 iter/s, 20.0515s/100 iter), loss = 0.0875142
I0916 23:07:50.109292 13570 solver.cpp:336]     Train net output #0: loss = 0.087514 (* 1 = 0.087514 loss)
I0916 23:07:50.109300 13570 sgd_solver.cpp:136] Iteration 47800, lr = 0.0001, m = 0.9
I0916 23:08:09.836457 13570 solver.cpp:314] Iteration 47900 (5.06928 iter/s, 19.7267s/100 iter), loss = 0.0686748
I0916 23:08:09.836482 13570 solver.cpp:336]     Train net output #0: loss = 0.0686746 (* 1 = 0.0686746 loss)
I0916 23:08:09.836485 13570 sgd_solver.cpp:136] Iteration 47900, lr = 0.0001, m = 0.9
I0916 23:08:29.165701 13570 solver.cpp:368] Sparsity after update:
I0916 23:08:29.170028 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 23:08:29.170038 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 23:08:29.170047 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 23:08:29.170051 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 23:08:29.170055 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 23:08:29.170058 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 23:08:29.170063 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 23:08:29.170066 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 23:08:29.170070 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 23:08:29.170079 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 23:08:29.170083 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 23:08:29.170087 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 23:08:29.170091 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 23:08:29.170095 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 23:08:29.170099 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 23:08:29.170102 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 23:08:29.170107 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 23:08:29.170110 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 23:08:29.170114 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 23:08:29.170126 13570 solver.cpp:563] Iteration 48000, Testing net (#0)
I0916 23:08:32.575701 13600 data_reader.cpp:305] Starting prefetch of epoch 3
I0916 23:08:40.473817 13568 data_reader.cpp:305] Starting prefetch of epoch 4
I0916 23:08:40.830111 13570 solver.cpp:655]     Test net output #0: accuracy/top1 = 0.951723
I0916 23:08:40.830137 13570 solver.cpp:655]     Test net output #1: accuracy/top5 = 1
I0916 23:08:40.830142 13570 solver.cpp:655]     Test net output #2: loss = 0.148438 (* 1 = 0.148438 loss)
I0916 23:08:40.830174 13570 solver.cpp:265] [MultiGPU] Tests completed in 11.6597s
I0916 23:08:41.047111 13570 solver.cpp:314] Iteration 48000 (3.20412 iter/s, 31.2098s/100 iter), loss = 0.167298
I0916 23:08:41.047134 13570 solver.cpp:336]     Train net output #0: loss = 0.167297 (* 1 = 0.167297 loss)
I0916 23:08:41.047140 13570 sgd_solver.cpp:136] Iteration 48000, lr = 0.0001, m = 0.9
I0916 23:09:00.545398 13570 solver.cpp:314] Iteration 48100 (5.1288 iter/s, 19.4977s/100 iter), loss = 0.0403825
I0916 23:09:00.545506 13570 solver.cpp:336]     Train net output #0: loss = 0.0403823 (* 1 = 0.0403823 loss)
I0916 23:09:00.545513 13570 sgd_solver.cpp:136] Iteration 48100, lr = 0.0001, m = 0.9
I0916 23:09:20.007033 13570 solver.cpp:314] Iteration 48200 (5.13846 iter/s, 19.4611s/100 iter), loss = 0.0581615
I0916 23:09:20.007061 13570 solver.cpp:336]     Train net output #0: loss = 0.0581613 (* 1 = 0.0581613 loss)
I0916 23:09:20.007066 13570 sgd_solver.cpp:136] Iteration 48200, lr = 0.0001, m = 0.9
I0916 23:09:31.376377 13574 data_reader.cpp:305] Starting prefetch of epoch 30
I0916 23:09:39.615453 13570 solver.cpp:314] Iteration 48300 (5.09999 iter/s, 19.6079s/100 iter), loss = 0.0396142
I0916 23:09:39.615475 13570 solver.cpp:336]     Train net output #0: loss = 0.039614 (* 1 = 0.039614 loss)
I0916 23:09:39.615481 13570 sgd_solver.cpp:136] Iteration 48300, lr = 0.0001, m = 0.9
I0916 23:09:59.044149 13570 solver.cpp:314] Iteration 48400 (5.14717 iter/s, 19.4282s/100 iter), loss = 0.0550134
I0916 23:09:59.044173 13570 solver.cpp:336]     Train net output #0: loss = 0.0550132 (* 1 = 0.0550132 loss)
I0916 23:09:59.044178 13570 sgd_solver.cpp:136] Iteration 48400, lr = 0.0001, m = 0.9
I0916 23:10:18.462633 13570 solver.cpp:314] Iteration 48500 (5.14988 iter/s, 19.4179s/100 iter), loss = 0.0494246
I0916 23:10:18.462688 13570 solver.cpp:336]     Train net output #0: loss = 0.0494244 (* 1 = 0.0494244 loss)
I0916 23:10:18.462693 13570 sgd_solver.cpp:136] Iteration 48500, lr = 0.0001, m = 0.9
I0916 23:10:35.382484 13578 data_reader.cpp:305] Starting prefetch of epoch 30
I0916 23:10:37.709789 13570 solver.cpp:314] Iteration 48600 (5.19572 iter/s, 19.2466s/100 iter), loss = 0.0486367
I0916 23:10:37.709813 13570 solver.cpp:336]     Train net output #0: loss = 0.0486365 (* 1 = 0.0486365 loss)
I0916 23:10:37.709817 13570 sgd_solver.cpp:136] Iteration 48600, lr = 0.0001, m = 0.9
I0916 23:10:57.235476 13570 solver.cpp:314] Iteration 48700 (5.1216 iter/s, 19.5251s/100 iter), loss = 0.0513759
I0916 23:10:57.235545 13570 solver.cpp:336]     Train net output #0: loss = 0.0513757 (* 1 = 0.0513757 loss)
I0916 23:10:57.235554 13570 sgd_solver.cpp:136] Iteration 48700, lr = 0.0001, m = 0.9
I0916 23:11:07.572787 13544 data_reader.cpp:305] Starting prefetch of epoch 29
I0916 23:11:16.763175 13570 solver.cpp:314] Iteration 48800 (5.12107 iter/s, 19.5272s/100 iter), loss = 0.0914929
I0916 23:11:16.763200 13570 solver.cpp:336]     Train net output #0: loss = 0.0914926 (* 1 = 0.0914926 loss)
I0916 23:11:16.763206 13570 sgd_solver.cpp:136] Iteration 48800, lr = 0.0001, m = 0.9
I0916 23:11:36.442697 13570 solver.cpp:314] Iteration 48900 (5.08157 iter/s, 19.679s/100 iter), loss = 0.0566771
I0916 23:11:36.442996 13570 solver.cpp:336]     Train net output #0: loss = 0.0566769 (* 1 = 0.0566769 loss)
I0916 23:11:36.443004 13570 sgd_solver.cpp:136] Iteration 48900, lr = 0.0001, m = 0.9
I0916 23:11:55.719204 13570 solver.cpp:368] Sparsity after update:
I0916 23:11:55.726717 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 23:11:55.726727 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 23:11:55.726735 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 23:11:55.726738 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 23:11:55.726742 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 23:11:55.726744 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 23:11:55.726747 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 23:11:55.726752 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 23:11:55.726754 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 23:11:55.726758 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 23:11:55.726760 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 23:11:55.726763 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 23:11:55.726773 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 23:11:55.726778 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 23:11:55.726783 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 23:11:55.726789 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 23:11:55.726795 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 23:11:55.726801 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 23:11:55.726806 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 23:11:55.897008 13570 solver.cpp:314] Iteration 49000 (5.14039 iter/s, 19.4538s/100 iter), loss = 0.0785737
I0916 23:11:55.897044 13570 solver.cpp:336]     Train net output #0: loss = 0.0785734 (* 1 = 0.0785734 loss)
I0916 23:11:55.897050 13570 sgd_solver.cpp:136] Iteration 49000, lr = 0.0001, m = 0.9
I0916 23:12:12.282450 13573 data_reader.cpp:305] Starting prefetch of epoch 31
I0916 23:12:15.379884 13570 solver.cpp:314] Iteration 49100 (5.13286 iter/s, 19.4823s/100 iter), loss = 0.0629482
I0916 23:12:15.379910 13570 solver.cpp:336]     Train net output #0: loss = 0.062948 (* 1 = 0.062948 loss)
I0916 23:12:15.379915 13570 sgd_solver.cpp:136] Iteration 49100, lr = 0.0001, m = 0.9
I0916 23:12:34.906216 13570 solver.cpp:314] Iteration 49200 (5.12143 iter/s, 19.5258s/100 iter), loss = 0.0652519
I0916 23:12:34.906241 13570 solver.cpp:336]     Train net output #0: loss = 0.0652517 (* 1 = 0.0652517 loss)
I0916 23:12:34.906249 13570 sgd_solver.cpp:136] Iteration 49200, lr = 0.0001, m = 0.9
I0916 23:12:54.302448 13570 solver.cpp:314] Iteration 49300 (5.15579 iter/s, 19.3957s/100 iter), loss = 0.0820813
I0916 23:12:54.302505 13570 solver.cpp:336]     Train net output #0: loss = 0.0820811 (* 1 = 0.0820811 loss)
I0916 23:12:54.302510 13570 sgd_solver.cpp:136] Iteration 49300, lr = 0.0001, m = 0.9
I0916 23:13:13.610623 13570 solver.cpp:314] Iteration 49400 (5.1793 iter/s, 19.3076s/100 iter), loss = 0.0663275
I0916 23:13:13.610647 13570 solver.cpp:336]     Train net output #0: loss = 0.0663273 (* 1 = 0.0663273 loss)
I0916 23:13:13.610653 13570 sgd_solver.cpp:136] Iteration 49400, lr = 0.0001, m = 0.9
I0916 23:13:16.499266 13573 data_reader.cpp:305] Starting prefetch of epoch 32
I0916 23:13:33.137508 13570 solver.cpp:314] Iteration 49500 (5.12129 iter/s, 19.5263s/100 iter), loss = 0.0541801
I0916 23:13:33.150156 13570 solver.cpp:336]     Train net output #0: loss = 0.0541799 (* 1 = 0.0541799 loss)
I0916 23:13:33.150194 13570 sgd_solver.cpp:136] Iteration 49500, lr = 0.0001, m = 0.9
I0916 23:13:52.759220 13570 solver.cpp:314] Iteration 49600 (5.09654 iter/s, 19.6212s/100 iter), loss = 0.0557167
I0916 23:13:52.759246 13570 solver.cpp:336]     Train net output #0: loss = 0.0557165 (* 1 = 0.0557165 loss)
I0916 23:13:52.759253 13570 sgd_solver.cpp:136] Iteration 49600, lr = 0.0001, m = 0.9
I0916 23:14:12.276197 13570 solver.cpp:314] Iteration 49700 (5.12389 iter/s, 19.5164s/100 iter), loss = 0.0681595
I0916 23:14:12.276252 13570 solver.cpp:336]     Train net output #0: loss = 0.0681593 (* 1 = 0.0681593 loss)
I0916 23:14:12.276260 13570 sgd_solver.cpp:136] Iteration 49700, lr = 0.0001, m = 0.9
I0916 23:14:21.078595 13576 data_reader.cpp:305] Starting prefetch of epoch 34
I0916 23:14:31.795933 13570 solver.cpp:314] Iteration 49800 (5.12316 iter/s, 19.5192s/100 iter), loss = 0.0772982
I0916 23:14:31.795963 13570 solver.cpp:336]     Train net output #0: loss = 0.077298 (* 1 = 0.077298 loss)
I0916 23:14:31.795969 13570 sgd_solver.cpp:136] Iteration 49800, lr = 0.0001, m = 0.9
I0916 23:14:51.565454 13570 solver.cpp:314] Iteration 49900 (5.05843 iter/s, 19.769s/100 iter), loss = 0.0449232
I0916 23:14:51.565521 13570 solver.cpp:336]     Train net output #0: loss = 0.0449229 (* 1 = 0.0449229 loss)
I0916 23:14:51.565526 13570 sgd_solver.cpp:136] Iteration 49900, lr = 0.0001, m = 0.9
I0916 23:14:53.557945 13544 data_reader.cpp:305] Starting prefetch of epoch 30
I0916 23:15:10.905803 13570 solver.cpp:825] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/cityscapes5_jsegnet21v2_iter_50000.caffemodel
I0916 23:15:11.081578 13570 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/cityscapes5_jsegnet21v2_iter_50000.solverstate
I0916 23:15:11.092876 13570 solver.cpp:368] Sparsity after update:
I0916 23:15:11.097492 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 23:15:11.097554 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 23:15:11.097584 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 23:15:11.097600 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 23:15:11.097615 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 23:15:11.097630 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 23:15:11.097645 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 23:15:11.097661 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 23:15:11.097676 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 23:15:11.097690 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 23:15:11.097707 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 23:15:11.097721 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 23:15:11.097735 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 23:15:11.097751 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 23:15:11.097765 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 23:15:11.097781 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 23:15:11.097796 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 23:15:11.097811 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 23:15:11.097826 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 23:15:11.097853 13570 solver.cpp:563] Iteration 50000, Testing net (#0)
I0916 23:15:21.944682 13570 solver.cpp:655]     Test net output #0: accuracy/top1 = 0.954443
I0916 23:15:21.944780 13570 solver.cpp:655]     Test net output #1: accuracy/top5 = 1
I0916 23:15:21.944789 13570 solver.cpp:655]     Test net output #2: loss = 0.161965 (* 1 = 0.161965 loss)
I0916 23:15:21.944808 13570 solver.cpp:265] [MultiGPU] Tests completed in 10.8467s
I0916 23:15:22.147406 13570 solver.cpp:314] Iteration 50000 (3.26999 iter/s, 30.5811s/100 iter), loss = 0.0657246
I0916 23:15:22.147440 13570 solver.cpp:336]     Train net output #0: loss = 0.0657243 (* 1 = 0.0657243 loss)
I0916 23:15:22.147444 13570 sgd_solver.cpp:136] Iteration 50000, lr = 0.0001, m = 0.9
I0916 23:15:37.047281 13574 data_reader.cpp:305] Starting prefetch of epoch 31
I0916 23:15:41.935648 13570 solver.cpp:314] Iteration 50100 (5.05365 iter/s, 19.7877s/100 iter), loss = 0.0752785
I0916 23:15:41.935678 13570 solver.cpp:336]     Train net output #0: loss = 0.0752783 (* 1 = 0.0752783 loss)
I0916 23:15:41.935685 13570 sgd_solver.cpp:136] Iteration 50100, lr = 0.0001, m = 0.9
I0916 23:16:01.617537 13570 solver.cpp:314] Iteration 50200 (5.08096 iter/s, 19.6813s/100 iter), loss = 0.0778879
I0916 23:16:01.617583 13570 solver.cpp:336]     Train net output #0: loss = 0.0778877 (* 1 = 0.0778877 loss)
I0916 23:16:01.617586 13570 sgd_solver.cpp:136] Iteration 50200, lr = 0.0001, m = 0.9
I0916 23:16:21.540587 13570 solver.cpp:314] Iteration 50300 (5.01945 iter/s, 19.9225s/100 iter), loss = 0.0407254
I0916 23:16:21.540616 13570 solver.cpp:336]     Train net output #0: loss = 0.0407252 (* 1 = 0.0407252 loss)
I0916 23:16:21.540619 13570 sgd_solver.cpp:136] Iteration 50300, lr = 0.0001, m = 0.9
I0916 23:16:41.208789 13570 solver.cpp:314] Iteration 50400 (5.08449 iter/s, 19.6676s/100 iter), loss = 0.0531592
I0916 23:16:41.208940 13570 solver.cpp:336]     Train net output #0: loss = 0.053159 (* 1 = 0.053159 loss)
I0916 23:16:41.208961 13570 sgd_solver.cpp:136] Iteration 50400, lr = 0.0001, m = 0.9
I0916 23:16:42.418776 13574 data_reader.cpp:305] Starting prefetch of epoch 32
I0916 23:17:00.890491 13570 solver.cpp:314] Iteration 50500 (5.081 iter/s, 19.6812s/100 iter), loss = 0.073773
I0916 23:17:00.890517 13570 solver.cpp:336]     Train net output #0: loss = 0.0737728 (* 1 = 0.0737728 loss)
I0916 23:17:00.890522 13570 sgd_solver.cpp:136] Iteration 50500, lr = 0.0001, m = 0.9
I0916 23:17:20.560623 13570 solver.cpp:314] Iteration 50600 (5.08399 iter/s, 19.6696s/100 iter), loss = 0.0679126
I0916 23:17:20.560688 13570 solver.cpp:336]     Train net output #0: loss = 0.0679124 (* 1 = 0.0679124 loss)
I0916 23:17:20.560695 13570 sgd_solver.cpp:136] Iteration 50600, lr = 0.0001, m = 0.9
I0916 23:17:40.067713 13570 solver.cpp:314] Iteration 50700 (5.12648 iter/s, 19.5065s/100 iter), loss = 0.0465317
I0916 23:17:40.067735 13570 solver.cpp:336]     Train net output #0: loss = 0.0465315 (* 1 = 0.0465315 loss)
I0916 23:17:40.067741 13570 sgd_solver.cpp:136] Iteration 50700, lr = 0.0001, m = 0.9
I0916 23:17:47.345473 13578 data_reader.cpp:305] Starting prefetch of epoch 31
I0916 23:17:59.768790 13570 solver.cpp:314] Iteration 50800 (5.07601 iter/s, 19.7005s/100 iter), loss = 0.0954666
I0916 23:17:59.768846 13570 solver.cpp:336]     Train net output #0: loss = 0.0954664 (* 1 = 0.0954664 loss)
I0916 23:17:59.768853 13570 sgd_solver.cpp:136] Iteration 50800, lr = 0.0001, m = 0.9
I0916 23:18:19.484334 13570 solver.cpp:314] Iteration 50900 (5.07228 iter/s, 19.715s/100 iter), loss = 0.0642493
I0916 23:18:19.484369 13570 solver.cpp:336]     Train net output #0: loss = 0.0642491 (* 1 = 0.0642491 loss)
I0916 23:18:19.484377 13570 sgd_solver.cpp:136] Iteration 50900, lr = 0.0001, m = 0.9
I0916 23:18:19.951032 13573 data_reader.cpp:305] Starting prefetch of epoch 33
I0916 23:18:39.156882 13570 solver.cpp:368] Sparsity after update:
I0916 23:18:39.161609 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 23:18:39.161702 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 23:18:39.161733 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 23:18:39.161747 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 23:18:39.161759 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 23:18:39.161772 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 23:18:39.161782 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 23:18:39.161794 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 23:18:39.161806 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 23:18:39.161818 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 23:18:39.161830 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 23:18:39.161841 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 23:18:39.161854 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 23:18:39.161865 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 23:18:39.161877 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 23:18:39.161888 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 23:18:39.161900 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 23:18:39.161916 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 23:18:39.161928 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 23:18:39.350407 13570 solver.cpp:314] Iteration 51000 (5.03385 iter/s, 19.8655s/100 iter), loss = 0.0254339
I0916 23:18:39.350471 13570 solver.cpp:336]     Train net output #0: loss = 0.0254337 (* 1 = 0.0254337 loss)
I0916 23:18:39.350488 13570 sgd_solver.cpp:136] Iteration 51000, lr = 0.0001, m = 0.9
I0916 23:18:58.812929 13570 solver.cpp:314] Iteration 51100 (5.13822 iter/s, 19.462s/100 iter), loss = 0.0775203
I0916 23:18:58.812957 13570 solver.cpp:336]     Train net output #0: loss = 0.0775201 (* 1 = 0.0775201 loss)
I0916 23:18:58.812963 13570 sgd_solver.cpp:136] Iteration 51100, lr = 0.0001, m = 0.9
I0916 23:19:18.717762 13570 solver.cpp:314] Iteration 51200 (5.02405 iter/s, 19.9043s/100 iter), loss = 0.0628552
I0916 23:19:18.717829 13570 solver.cpp:336]     Train net output #0: loss = 0.062855 (* 1 = 0.062855 loss)
I0916 23:19:18.717836 13570 sgd_solver.cpp:136] Iteration 51200, lr = 0.0001, m = 0.9
I0916 23:19:25.179272 13574 data_reader.cpp:305] Starting prefetch of epoch 33
I0916 23:19:38.640662 13570 solver.cpp:314] Iteration 51300 (5.01949 iter/s, 19.9223s/100 iter), loss = 0.0858505
I0916 23:19:38.640687 13570 solver.cpp:336]     Train net output #0: loss = 0.0858504 (* 1 = 0.0858504 loss)
I0916 23:19:38.640694 13570 sgd_solver.cpp:136] Iteration 51300, lr = 0.0001, m = 0.9
I0916 23:19:58.380908 13570 solver.cpp:314] Iteration 51400 (5.06593 iter/s, 19.7397s/100 iter), loss = 0.0662465
I0916 23:19:58.380959 13570 solver.cpp:336]     Train net output #0: loss = 0.0662463 (* 1 = 0.0662463 loss)
I0916 23:19:58.380964 13570 sgd_solver.cpp:136] Iteration 51400, lr = 0.0001, m = 0.9
I0916 23:20:18.494060 13570 solver.cpp:314] Iteration 51500 (4.97201 iter/s, 20.1126s/100 iter), loss = 0.0435035
I0916 23:20:18.494104 13570 solver.cpp:336]     Train net output #0: loss = 0.0435033 (* 1 = 0.0435033 loss)
I0916 23:20:18.494112 13570 sgd_solver.cpp:136] Iteration 51500, lr = 0.0001, m = 0.9
I0916 23:20:30.957211 13546 data_reader.cpp:305] Starting prefetch of epoch 23
I0916 23:20:38.390995 13570 solver.cpp:314] Iteration 51600 (5.02604 iter/s, 19.8964s/100 iter), loss = 0.0658213
I0916 23:20:38.391019 13570 solver.cpp:336]     Train net output #0: loss = 0.0658211 (* 1 = 0.0658211 loss)
I0916 23:20:38.391024 13570 sgd_solver.cpp:136] Iteration 51600, lr = 0.0001, m = 0.9
I0916 23:20:57.935497 13570 solver.cpp:314] Iteration 51700 (5.11667 iter/s, 19.544s/100 iter), loss = 0.0633818
I0916 23:20:57.935570 13570 solver.cpp:336]     Train net output #0: loss = 0.0633816 (* 1 = 0.0633816 loss)
I0916 23:20:57.935587 13570 sgd_solver.cpp:136] Iteration 51700, lr = 0.0001, m = 0.9
I0916 23:21:03.521184 13544 data_reader.cpp:305] Starting prefetch of epoch 31
I0916 23:21:17.717602 13570 solver.cpp:314] Iteration 51800 (5.05521 iter/s, 19.7816s/100 iter), loss = 0.0672557
I0916 23:21:17.717625 13570 solver.cpp:336]     Train net output #0: loss = 0.0672556 (* 1 = 0.0672556 loss)
I0916 23:21:17.717631 13570 sgd_solver.cpp:136] Iteration 51800, lr = 0.0001, m = 0.9
I0916 23:21:36.991909 13570 solver.cpp:314] Iteration 51900 (5.1884 iter/s, 19.2738s/100 iter), loss = 0.0854909
I0916 23:21:36.991966 13570 solver.cpp:336]     Train net output #0: loss = 0.0854907 (* 1 = 0.0854907 loss)
I0916 23:21:36.991973 13570 sgd_solver.cpp:136] Iteration 51900, lr = 0.0001, m = 0.9
I0916 23:21:56.403803 13570 solver.cpp:368] Sparsity after update:
I0916 23:21:56.409811 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 23:21:56.409826 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 23:21:56.409834 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 23:21:56.409838 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 23:21:56.409842 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 23:21:56.409845 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 23:21:56.409849 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 23:21:56.409853 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 23:21:56.409857 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 23:21:56.409860 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 23:21:56.409864 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 23:21:56.409868 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 23:21:56.409870 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 23:21:56.409873 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 23:21:56.409876 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 23:21:56.409879 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 23:21:56.409883 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 23:21:56.409885 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 23:21:56.409888 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 23:21:56.409899 13570 solver.cpp:563] Iteration 52000, Testing net (#0)
I0916 23:22:00.038476 13594 data_reader.cpp:305] Starting prefetch of epoch 2
I0916 23:22:08.191260 13570 solver.cpp:655]     Test net output #0: accuracy/top1 = 0.951679
I0916 23:22:08.192102 13570 solver.cpp:655]     Test net output #1: accuracy/top5 = 1
I0916 23:22:08.192112 13570 solver.cpp:655]     Test net output #2: loss = 0.148512 (* 1 = 0.148512 loss)
I0916 23:22:08.192142 13570 solver.cpp:265] [MultiGPU] Tests completed in 11.7819s
I0916 23:22:08.398875 13570 solver.cpp:314] Iteration 52000 (3.1841 iter/s, 31.4061s/100 iter), loss = 0.0566424
I0916 23:22:08.398900 13570 solver.cpp:336]     Train net output #0: loss = 0.0566423 (* 1 = 0.0566423 loss)
I0916 23:22:08.398905 13570 sgd_solver.cpp:136] Iteration 52000, lr = 0.0001, m = 0.9
I0916 23:22:19.972374 13578 data_reader.cpp:305] Starting prefetch of epoch 32
I0916 23:22:28.007223 13570 solver.cpp:314] Iteration 52100 (5.10001 iter/s, 19.6078s/100 iter), loss = 0.0336577
I0916 23:22:28.007247 13570 solver.cpp:336]     Train net output #0: loss = 0.0336576 (* 1 = 0.0336576 loss)
I0916 23:22:28.007251 13570 sgd_solver.cpp:136] Iteration 52100, lr = 0.0001, m = 0.9
I0916 23:22:47.796671 13570 solver.cpp:314] Iteration 52200 (5.05334 iter/s, 19.7889s/100 iter), loss = 0.0623757
I0916 23:22:47.796739 13570 solver.cpp:336]     Train net output #0: loss = 0.0623756 (* 1 = 0.0623756 loss)
I0916 23:22:47.796746 13570 sgd_solver.cpp:136] Iteration 52200, lr = 0.0001, m = 0.9
I0916 23:22:52.506954 13544 data_reader.cpp:305] Starting prefetch of epoch 32
I0916 23:23:07.529757 13570 solver.cpp:314] Iteration 52300 (5.06777 iter/s, 19.7325s/100 iter), loss = 0.0887764
I0916 23:23:07.529780 13570 solver.cpp:336]     Train net output #0: loss = 0.0887762 (* 1 = 0.0887762 loss)
I0916 23:23:07.529784 13570 sgd_solver.cpp:136] Iteration 52300, lr = 0.0001, m = 0.9
I0916 23:23:27.206051 13570 solver.cpp:314] Iteration 52400 (5.0824 iter/s, 19.6757s/100 iter), loss = 0.0562745
I0916 23:23:27.206120 13570 solver.cpp:336]     Train net output #0: loss = 0.0562743 (* 1 = 0.0562743 loss)
I0916 23:23:27.206126 13570 sgd_solver.cpp:136] Iteration 52400, lr = 0.0001, m = 0.9
I0916 23:23:46.933117 13570 solver.cpp:314] Iteration 52500 (5.06932 iter/s, 19.7265s/100 iter), loss = 0.0738185
I0916 23:23:46.933143 13570 solver.cpp:336]     Train net output #0: loss = 0.0738183 (* 1 = 0.0738183 loss)
I0916 23:23:46.933151 13570 sgd_solver.cpp:136] Iteration 52500, lr = 0.0001, m = 0.9
I0916 23:23:57.683063 13573 data_reader.cpp:305] Starting prefetch of epoch 34
I0916 23:24:06.408330 13570 solver.cpp:314] Iteration 52600 (5.13488 iter/s, 19.4747s/100 iter), loss = 0.0765972
I0916 23:24:06.408362 13570 solver.cpp:336]     Train net output #0: loss = 0.0765971 (* 1 = 0.0765971 loss)
I0916 23:24:06.408368 13570 sgd_solver.cpp:136] Iteration 52600, lr = 0.0001, m = 0.9
I0916 23:24:25.928385 13570 solver.cpp:314] Iteration 52700 (5.12308 iter/s, 19.5195s/100 iter), loss = 0.120942
I0916 23:24:25.928407 13570 solver.cpp:336]     Train net output #0: loss = 0.120942 (* 1 = 0.120942 loss)
I0916 23:24:25.928411 13570 sgd_solver.cpp:136] Iteration 52700, lr = 0.0001, m = 0.9
I0916 23:24:45.386718 13570 solver.cpp:314] Iteration 52800 (5.13933 iter/s, 19.4578s/100 iter), loss = 0.0744007
I0916 23:24:45.386787 13570 solver.cpp:336]     Train net output #0: loss = 0.0744006 (* 1 = 0.0744006 loss)
I0916 23:24:45.386796 13570 sgd_solver.cpp:136] Iteration 52800, lr = 0.0001, m = 0.9
I0916 23:25:01.952246 13576 data_reader.cpp:305] Starting prefetch of epoch 35
I0916 23:25:04.884891 13570 solver.cpp:314] Iteration 52900 (5.12883 iter/s, 19.4976s/100 iter), loss = 0.0575038
I0916 23:25:04.884914 13570 solver.cpp:336]     Train net output #0: loss = 0.0575037 (* 1 = 0.0575037 loss)
I0916 23:25:04.884918 13570 sgd_solver.cpp:136] Iteration 52900, lr = 0.0001, m = 0.9
I0916 23:25:24.362637 13570 solver.cpp:368] Sparsity after update:
I0916 23:25:24.365430 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 23:25:24.365486 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 23:25:24.365515 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 23:25:24.365525 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 23:25:24.365532 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 23:25:24.365540 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 23:25:24.365548 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 23:25:24.365556 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 23:25:24.365564 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 23:25:24.365572 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 23:25:24.365581 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 23:25:24.365589 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 23:25:24.365597 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 23:25:24.365605 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 23:25:24.365613 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 23:25:24.365622 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 23:25:24.365630 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 23:25:24.365639 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 23:25:24.365648 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 23:25:24.551681 13570 solver.cpp:314] Iteration 53000 (5.08486 iter/s, 19.6662s/100 iter), loss = 0.0500414
I0916 23:25:24.551708 13570 solver.cpp:336]     Train net output #0: loss = 0.0500412 (* 1 = 0.0500412 loss)
I0916 23:25:24.551715 13570 sgd_solver.cpp:136] Iteration 53000, lr = 0.0001, m = 0.9
I0916 23:25:34.518801 13574 data_reader.cpp:305] Starting prefetch of epoch 34
I0916 23:25:43.958899 13570 solver.cpp:314] Iteration 53100 (5.15287 iter/s, 19.4067s/100 iter), loss = 0.0604412
I0916 23:25:43.958923 13570 solver.cpp:336]     Train net output #0: loss = 0.0604411 (* 1 = 0.0604411 loss)
I0916 23:25:43.958927 13570 sgd_solver.cpp:136] Iteration 53100, lr = 0.0001, m = 0.9
I0916 23:26:03.614876 13570 solver.cpp:314] Iteration 53200 (5.08765 iter/s, 19.6554s/100 iter), loss = 0.0505332
I0916 23:26:03.614962 13570 solver.cpp:336]     Train net output #0: loss = 0.0505331 (* 1 = 0.0505331 loss)
I0916 23:26:03.614969 13570 sgd_solver.cpp:136] Iteration 53200, lr = 0.0001, m = 0.9
I0916 23:26:23.051179 13570 solver.cpp:314] Iteration 53300 (5.14515 iter/s, 19.4358s/100 iter), loss = 0.0616834
I0916 23:26:23.051200 13570 solver.cpp:336]     Train net output #0: loss = 0.0616833 (* 1 = 0.0616833 loss)
I0916 23:26:23.051204 13570 sgd_solver.cpp:136] Iteration 53300, lr = 0.0001, m = 0.9
I0916 23:26:38.807606 13573 data_reader.cpp:305] Starting prefetch of epoch 35
I0916 23:26:42.484577 13570 solver.cpp:314] Iteration 53400 (5.14592 iter/s, 19.4329s/100 iter), loss = 0.0658367
I0916 23:26:42.484598 13570 solver.cpp:336]     Train net output #0: loss = 0.0658366 (* 1 = 0.0658366 loss)
I0916 23:26:42.484602 13570 sgd_solver.cpp:136] Iteration 53400, lr = 0.0001, m = 0.9
I0916 23:27:02.139518 13570 solver.cpp:314] Iteration 53500 (5.08792 iter/s, 19.6544s/100 iter), loss = 0.0482583
I0916 23:27:02.139544 13570 solver.cpp:336]     Train net output #0: loss = 0.0482582 (* 1 = 0.0482582 loss)
I0916 23:27:02.139549 13570 sgd_solver.cpp:136] Iteration 53500, lr = 0.0001, m = 0.9
I0916 23:27:21.714706 13570 solver.cpp:314] Iteration 53600 (5.10865 iter/s, 19.5746s/100 iter), loss = 0.0949052
I0916 23:27:21.714818 13570 solver.cpp:336]     Train net output #0: loss = 0.0949051 (* 1 = 0.0949051 loss)
I0916 23:27:21.714828 13570 sgd_solver.cpp:136] Iteration 53600, lr = 0.0001, m = 0.9
I0916 23:27:41.183176 13570 solver.cpp:314] Iteration 53700 (5.13665 iter/s, 19.4679s/100 iter), loss = 0.0619753
I0916 23:27:41.183197 13570 solver.cpp:336]     Train net output #0: loss = 0.0619751 (* 1 = 0.0619751 loss)
I0916 23:27:41.183202 13570 sgd_solver.cpp:136] Iteration 53700, lr = 0.0001, m = 0.9
I0916 23:27:43.454995 13578 data_reader.cpp:305] Starting prefetch of epoch 33
I0916 23:28:00.439756 13570 solver.cpp:314] Iteration 53800 (5.19317 iter/s, 19.256s/100 iter), loss = 0.0583221
I0916 23:28:00.439844 13570 solver.cpp:336]     Train net output #0: loss = 0.0583219 (* 1 = 0.0583219 loss)
I0916 23:28:00.439851 13570 sgd_solver.cpp:136] Iteration 53800, lr = 0.0001, m = 0.9
I0916 23:28:15.476945 13544 data_reader.cpp:305] Starting prefetch of epoch 33
I0916 23:28:20.052345 13570 solver.cpp:314] Iteration 53900 (5.09891 iter/s, 19.612s/100 iter), loss = 0.0544164
I0916 23:28:20.052364 13570 solver.cpp:336]     Train net output #0: loss = 0.0544163 (* 1 = 0.0544163 loss)
I0916 23:28:20.052369 13570 sgd_solver.cpp:136] Iteration 53900, lr = 0.0001, m = 0.9
I0916 23:28:39.232985 13570 solver.cpp:368] Sparsity after update:
I0916 23:28:39.237515 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 23:28:39.237524 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 23:28:39.237530 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 23:28:39.237532 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 23:28:39.237534 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 23:28:39.237536 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 23:28:39.237538 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 23:28:39.237540 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 23:28:39.237542 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 23:28:39.237546 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 23:28:39.237550 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 23:28:39.237553 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 23:28:39.237556 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 23:28:39.237560 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 23:28:39.237562 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 23:28:39.237566 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 23:28:39.237570 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 23:28:39.237573 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 23:28:39.237576 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 23:28:39.237587 13570 solver.cpp:563] Iteration 54000, Testing net (#0)
I0916 23:28:49.964313 13570 solver.cpp:655]     Test net output #0: accuracy/top1 = 0.954399
I0916 23:28:49.964334 13570 solver.cpp:655]     Test net output #1: accuracy/top5 = 1
I0916 23:28:49.964341 13570 solver.cpp:655]     Test net output #2: loss = 0.161996 (* 1 = 0.161996 loss)
I0916 23:28:49.964408 13570 solver.cpp:265] [MultiGPU] Tests completed in 10.7265s
I0916 23:28:50.166337 13570 solver.cpp:314] Iteration 54000 (3.32081 iter/s, 30.1132s/100 iter), loss = 0.0641261
I0916 23:28:50.166359 13570 solver.cpp:336]     Train net output #0: loss = 0.0641259 (* 1 = 0.0641259 loss)
I0916 23:28:50.166365 13570 sgd_solver.cpp:136] Iteration 54000, lr = 0.0001, m = 0.9
I0916 23:28:58.141427 13544 data_reader.cpp:305] Starting prefetch of epoch 34
I0916 23:29:09.013941 13570 solver.cpp:314] Iteration 54100 (5.30586 iter/s, 18.8471s/100 iter), loss = 0.056865
I0916 23:29:09.014009 13570 solver.cpp:336]     Train net output #0: loss = 0.0568649 (* 1 = 0.0568649 loss)
I0916 23:29:09.014032 13570 sgd_solver.cpp:136] Iteration 54100, lr = 0.0001, m = 0.9
I0916 23:29:28.555018 13570 solver.cpp:314] Iteration 54200 (5.11757 iter/s, 19.5405s/100 iter), loss = 0.0701729
I0916 23:29:28.555101 13570 solver.cpp:336]     Train net output #0: loss = 0.0701728 (* 1 = 0.0701728 loss)
I0916 23:29:28.555109 13570 sgd_solver.cpp:136] Iteration 54200, lr = 0.0001, m = 0.9
I0916 23:29:48.540256 13570 solver.cpp:314] Iteration 54300 (5.00383 iter/s, 19.9847s/100 iter), loss = 0.0860544
I0916 23:29:48.540280 13570 solver.cpp:336]     Train net output #0: loss = 0.0860543 (* 1 = 0.0860543 loss)
I0916 23:29:48.540284 13570 sgd_solver.cpp:136] Iteration 54300, lr = 0.0001, m = 0.9
I0916 23:30:03.192376 13578 data_reader.cpp:305] Starting prefetch of epoch 34
I0916 23:30:08.362494 13570 solver.cpp:314] Iteration 54400 (5.04498 iter/s, 19.8217s/100 iter), loss = 0.0426577
I0916 23:30:08.362516 13570 solver.cpp:336]     Train net output #0: loss = 0.0426576 (* 1 = 0.0426576 loss)
I0916 23:30:08.362522 13570 sgd_solver.cpp:136] Iteration 54400, lr = 0.0001, m = 0.9
I0916 23:30:27.957435 13570 solver.cpp:314] Iteration 54500 (5.1035 iter/s, 19.5944s/100 iter), loss = 0.141335
I0916 23:30:27.957459 13570 solver.cpp:336]     Train net output #0: loss = 0.141335 (* 1 = 0.141335 loss)
I0916 23:30:27.957466 13570 sgd_solver.cpp:136] Iteration 54500, lr = 0.0001, m = 0.9
I0916 23:30:35.565091 13573 data_reader.cpp:305] Starting prefetch of epoch 36
I0916 23:30:47.922407 13570 solver.cpp:314] Iteration 54600 (5.00891 iter/s, 19.9644s/100 iter), loss = 0.0588434
I0916 23:30:47.922435 13570 solver.cpp:336]     Train net output #0: loss = 0.0588432 (* 1 = 0.0588432 loss)
I0916 23:30:47.922441 13570 sgd_solver.cpp:136] Iteration 54600, lr = 0.0001, m = 0.9
I0916 23:31:07.353412 13570 solver.cpp:314] Iteration 54700 (5.14656 iter/s, 19.4305s/100 iter), loss = 0.0716686
I0916 23:31:07.353490 13570 solver.cpp:336]     Train net output #0: loss = 0.0716684 (* 1 = 0.0716684 loss)
I0916 23:31:07.353497 13570 sgd_solver.cpp:136] Iteration 54700, lr = 0.0001, m = 0.9
I0916 23:31:26.726977 13570 solver.cpp:314] Iteration 54800 (5.16182 iter/s, 19.373s/100 iter), loss = 0.0567633
I0916 23:31:26.727001 13570 solver.cpp:336]     Train net output #0: loss = 0.0567632 (* 1 = 0.0567632 loss)
I0916 23:31:26.727006 13570 sgd_solver.cpp:136] Iteration 54800, lr = 0.0001, m = 0.9
I0916 23:31:40.175973 13574 data_reader.cpp:305] Starting prefetch of epoch 35
I0916 23:31:46.267349 13570 solver.cpp:314] Iteration 54900 (5.11775 iter/s, 19.5398s/100 iter), loss = 0.061319
I0916 23:31:46.267374 13570 solver.cpp:336]     Train net output #0: loss = 0.0613188 (* 1 = 0.0613188 loss)
I0916 23:31:46.267379 13570 sgd_solver.cpp:136] Iteration 54900, lr = 0.0001, m = 0.9
I0916 23:32:05.787163 13570 solver.cpp:368] Sparsity after update:
I0916 23:32:05.808279 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 23:32:05.808297 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 23:32:05.808306 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 23:32:05.808310 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 23:32:05.808312 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 23:32:05.808315 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 23:32:05.808318 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 23:32:05.808328 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 23:32:05.808333 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 23:32:05.808338 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 23:32:05.808343 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 23:32:05.808348 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 23:32:05.808353 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 23:32:05.808357 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 23:32:05.808360 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 23:32:05.808363 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 23:32:05.808367 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 23:32:05.808369 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 23:32:05.808373 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 23:32:05.978837 13570 solver.cpp:314] Iteration 55000 (5.07332 iter/s, 19.7109s/100 iter), loss = 0.106456
I0916 23:32:05.978860 13570 solver.cpp:336]     Train net output #0: loss = 0.106456 (* 1 = 0.106456 loss)
I0916 23:32:05.978865 13570 sgd_solver.cpp:136] Iteration 55000, lr = 0.0001, m = 0.9
I0916 23:32:25.612728 13570 solver.cpp:314] Iteration 55100 (5.09338 iter/s, 19.6333s/100 iter), loss = 0.0884568
I0916 23:32:25.612830 13570 solver.cpp:336]     Train net output #0: loss = 0.0884567 (* 1 = 0.0884567 loss)
I0916 23:32:25.612838 13570 sgd_solver.cpp:136] Iteration 55100, lr = 0.0001, m = 0.9
I0916 23:32:45.353472 13576 data_reader.cpp:305] Starting prefetch of epoch 36
I0916 23:32:45.541504 13570 solver.cpp:314] Iteration 55200 (5.01801 iter/s, 19.9282s/100 iter), loss = 0.0488045
I0916 23:32:45.541527 13570 solver.cpp:336]     Train net output #0: loss = 0.0488044 (* 1 = 0.0488044 loss)
I0916 23:32:45.541532 13570 sgd_solver.cpp:136] Iteration 55200, lr = 0.0001, m = 0.9
I0916 23:33:05.556730 13570 solver.cpp:314] Iteration 55300 (4.99634 iter/s, 20.0147s/100 iter), loss = 0.0790197
I0916 23:33:05.556816 13570 solver.cpp:336]     Train net output #0: loss = 0.0790196 (* 1 = 0.0790196 loss)
I0916 23:33:05.556826 13570 sgd_solver.cpp:136] Iteration 55300, lr = 0.0001, m = 0.9
I0916 23:33:18.454634 13578 data_reader.cpp:305] Starting prefetch of epoch 35
I0916 23:33:25.411347 13570 solver.cpp:314] Iteration 55400 (5.03675 iter/s, 19.8541s/100 iter), loss = 0.0455171
I0916 23:33:25.411629 13570 solver.cpp:336]     Train net output #0: loss = 0.045517 (* 1 = 0.045517 loss)
I0916 23:33:25.411744 13570 sgd_solver.cpp:136] Iteration 55400, lr = 0.0001, m = 0.9
I0916 23:33:45.160996 13570 solver.cpp:314] Iteration 55500 (5.06352 iter/s, 19.7491s/100 iter), loss = 0.0560366
I0916 23:33:45.161099 13570 solver.cpp:336]     Train net output #0: loss = 0.0560364 (* 1 = 0.0560364 loss)
I0916 23:33:45.161105 13570 sgd_solver.cpp:136] Iteration 55500, lr = 0.0001, m = 0.9
I0916 23:34:04.747025 13570 solver.cpp:314] Iteration 55600 (5.10582 iter/s, 19.5855s/100 iter), loss = 0.0691788
I0916 23:34:04.747062 13570 solver.cpp:336]     Train net output #0: loss = 0.0691787 (* 1 = 0.0691787 loss)
I0916 23:34:04.747069 13570 sgd_solver.cpp:136] Iteration 55600, lr = 0.0001, m = 0.9
I0916 23:34:23.517343 13573 data_reader.cpp:305] Starting prefetch of epoch 37
I0916 23:34:24.502827 13570 solver.cpp:314] Iteration 55700 (5.06194 iter/s, 19.7553s/100 iter), loss = 0.0388919
I0916 23:34:24.502943 13570 solver.cpp:336]     Train net output #0: loss = 0.0388918 (* 1 = 0.0388918 loss)
I0916 23:34:24.502964 13570 sgd_solver.cpp:136] Iteration 55700, lr = 0.0001, m = 0.9
I0916 23:34:44.236162 13570 solver.cpp:314] Iteration 55800 (5.06771 iter/s, 19.7328s/100 iter), loss = 0.0611894
I0916 23:34:44.236186 13570 solver.cpp:336]     Train net output #0: loss = 0.0611892 (* 1 = 0.0611892 loss)
I0916 23:34:44.236191 13570 sgd_solver.cpp:136] Iteration 55800, lr = 0.0001, m = 0.9
I0916 23:35:04.062387 13570 solver.cpp:314] Iteration 55900 (5.04396 iter/s, 19.8257s/100 iter), loss = 0.101901
I0916 23:35:04.062441 13570 solver.cpp:336]     Train net output #0: loss = 0.101901 (* 1 = 0.101901 loss)
I0916 23:35:04.062448 13570 sgd_solver.cpp:136] Iteration 55900, lr = 0.0001, m = 0.9
I0916 23:35:23.503945 13570 solver.cpp:368] Sparsity after update:
I0916 23:35:23.505771 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 23:35:23.505789 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 23:35:23.505800 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 23:35:23.505805 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 23:35:23.505810 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 23:35:23.505813 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 23:35:23.505817 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 23:35:23.505820 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 23:35:23.505828 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 23:35:23.505832 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 23:35:23.505836 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 23:35:23.505839 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 23:35:23.505843 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 23:35:23.505847 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 23:35:23.505851 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 23:35:23.505853 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 23:35:23.505857 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 23:35:23.505861 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 23:35:23.505864 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 23:35:23.505879 13570 solver.cpp:563] Iteration 56000, Testing net (#0)
I0916 23:35:26.967032 13594 data_reader.cpp:305] Starting prefetch of epoch 3
I0916 23:35:34.805594 13596 data_reader.cpp:305] Starting prefetch of epoch 9
I0916 23:35:35.115731 13570 solver.cpp:655]     Test net output #0: accuracy/top1 = 0.95168
I0916 23:35:35.115752 13570 solver.cpp:655]     Test net output #1: accuracy/top5 = 1
I0916 23:35:35.115759 13570 solver.cpp:655]     Test net output #2: loss = 0.148033 (* 1 = 0.148033 loss)
I0916 23:35:35.115787 13570 solver.cpp:265] [MultiGPU] Tests completed in 11.6096s
I0916 23:35:35.311899 13570 solver.cpp:314] Iteration 56000 (3.20014 iter/s, 31.2486s/100 iter), loss = 0.0671015
I0916 23:35:35.311923 13570 solver.cpp:336]     Train net output #0: loss = 0.0671013 (* 1 = 0.0671013 loss)
I0916 23:35:35.311928 13570 sgd_solver.cpp:136] Iteration 56000, lr = 0.0001, m = 0.9
I0916 23:35:54.721922 13570 solver.cpp:314] Iteration 56100 (5.15212 iter/s, 19.4095s/100 iter), loss = 0.0841791
I0916 23:35:54.721946 13570 solver.cpp:336]     Train net output #0: loss = 0.0841789 (* 1 = 0.0841789 loss)
I0916 23:35:54.721952 13570 sgd_solver.cpp:136] Iteration 56100, lr = 0.0001, m = 0.9
I0916 23:36:12.372169 13573 data_reader.cpp:305] Starting prefetch of epoch 38
I0916 23:36:14.177337 13570 solver.cpp:314] Iteration 56200 (5.1401 iter/s, 19.4549s/100 iter), loss = 0.0633555
I0916 23:36:14.177366 13570 solver.cpp:336]     Train net output #0: loss = 0.0633553 (* 1 = 0.0633553 loss)
I0916 23:36:14.177371 13570 sgd_solver.cpp:136] Iteration 56200, lr = 0.0001, m = 0.9
I0916 23:36:33.714704 13570 solver.cpp:314] Iteration 56300 (5.11854 iter/s, 19.5368s/100 iter), loss = 0.0594908
I0916 23:36:33.714758 13570 solver.cpp:336]     Train net output #0: loss = 0.0594906 (* 1 = 0.0594906 loss)
I0916 23:36:33.714895 13570 sgd_solver.cpp:136] Iteration 56300, lr = 0.0001, m = 0.9
I0916 23:36:53.438668 13570 solver.cpp:314] Iteration 56400 (5.07012 iter/s, 19.7234s/100 iter), loss = 0.0596627
I0916 23:36:53.438732 13570 solver.cpp:336]     Train net output #0: loss = 0.0596625 (* 1 = 0.0596625 loss)
I0916 23:36:53.438740 13570 sgd_solver.cpp:136] Iteration 56400, lr = 0.0001, m = 0.9
I0916 23:37:13.016640 13570 solver.cpp:314] Iteration 56500 (5.10792 iter/s, 19.5774s/100 iter), loss = 0.0726347
I0916 23:37:13.016666 13570 solver.cpp:336]     Train net output #0: loss = 0.0726346 (* 1 = 0.0726346 loss)
I0916 23:37:13.016671 13570 sgd_solver.cpp:136] Iteration 56500, lr = 0.0001, m = 0.9
I0916 23:37:17.002900 13546 data_reader.cpp:305] Starting prefetch of epoch 24
I0916 23:37:32.613926 13570 solver.cpp:314] Iteration 56600 (5.10289 iter/s, 19.5967s/100 iter), loss = 0.0619634
I0916 23:37:32.618101 13570 solver.cpp:336]     Train net output #0: loss = 0.0619633 (* 1 = 0.0619633 loss)
I0916 23:37:32.618109 13570 sgd_solver.cpp:136] Iteration 56600, lr = 0.0001, m = 0.9
I0916 23:37:50.019104 13573 data_reader.cpp:305] Starting prefetch of epoch 39
I0916 23:37:52.557190 13570 solver.cpp:314] Iteration 56700 (5.01436 iter/s, 19.9427s/100 iter), loss = 0.0875407
I0916 23:37:52.557219 13570 solver.cpp:336]     Train net output #0: loss = 0.0875405 (* 1 = 0.0875405 loss)
I0916 23:37:52.557224 13570 sgd_solver.cpp:136] Iteration 56700, lr = 0.0001, m = 0.9
I0916 23:38:12.509064 13570 solver.cpp:314] Iteration 56800 (5.0122 iter/s, 19.9513s/100 iter), loss = 0.0719758
I0916 23:38:12.509147 13570 solver.cpp:336]     Train net output #0: loss = 0.0719756 (* 1 = 0.0719756 loss)
I0916 23:38:12.509156 13570 sgd_solver.cpp:136] Iteration 56800, lr = 0.0001, m = 0.9
I0916 23:38:31.884866 13570 solver.cpp:314] Iteration 56900 (5.16122 iter/s, 19.3753s/100 iter), loss = 0.0522817
I0916 23:38:31.884898 13570 solver.cpp:336]     Train net output #0: loss = 0.0522816 (* 1 = 0.0522816 loss)
I0916 23:38:31.884907 13570 sgd_solver.cpp:136] Iteration 56900, lr = 0.0001, m = 0.9
I0916 23:38:51.340003 13570 solver.cpp:368] Sparsity after update:
I0916 23:38:51.344872 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 23:38:51.344976 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 23:38:51.345002 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 23:38:51.345011 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 23:38:51.345021 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 23:38:51.345031 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 23:38:51.345039 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 23:38:51.345048 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 23:38:51.345057 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 23:38:51.345067 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 23:38:51.345075 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 23:38:51.345083 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 23:38:51.345090 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 23:38:51.345099 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 23:38:51.345108 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 23:38:51.345116 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 23:38:51.345125 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 23:38:51.345134 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 23:38:51.345144 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 23:38:51.533615 13570 solver.cpp:314] Iteration 57000 (5.08952 iter/s, 19.6482s/100 iter), loss = 0.0796724
I0916 23:38:51.533643 13570 solver.cpp:336]     Train net output #0: loss = 0.0796722 (* 1 = 0.0796722 loss)
I0916 23:38:51.533649 13570 sgd_solver.cpp:136] Iteration 57000, lr = 0.0001, m = 0.9
I0916 23:38:54.865566 13574 data_reader.cpp:305] Starting prefetch of epoch 36
I0916 23:39:11.154013 13570 solver.cpp:314] Iteration 57100 (5.09688 iter/s, 19.6199s/100 iter), loss = 0.0805147
I0916 23:39:11.154038 13570 solver.cpp:336]     Train net output #0: loss = 0.0805146 (* 1 = 0.0805146 loss)
I0916 23:39:11.154088 13570 sgd_solver.cpp:136] Iteration 57100, lr = 0.0001, m = 0.9
I0916 23:39:30.616487 13570 solver.cpp:314] Iteration 57200 (5.13824 iter/s, 19.4619s/100 iter), loss = 0.0523767
I0916 23:39:30.616544 13570 solver.cpp:336]     Train net output #0: loss = 0.0523765 (* 1 = 0.0523765 loss)
I0916 23:39:30.616549 13570 sgd_solver.cpp:136] Iteration 57200, lr = 0.0001, m = 0.9
I0916 23:39:50.087532 13570 solver.cpp:314] Iteration 57300 (5.13597 iter/s, 19.4705s/100 iter), loss = 0.0869511
I0916 23:39:50.087554 13570 solver.cpp:336]     Train net output #0: loss = 0.086951 (* 1 = 0.086951 loss)
I0916 23:39:50.087558 13570 sgd_solver.cpp:136] Iteration 57300, lr = 0.0001, m = 0.9
I0916 23:39:59.369529 13576 data_reader.cpp:305] Starting prefetch of epoch 37
I0916 23:40:09.484946 13570 solver.cpp:314] Iteration 57400 (5.15547 iter/s, 19.3969s/100 iter), loss = 0.0556181
I0916 23:40:09.485026 13570 solver.cpp:336]     Train net output #0: loss = 0.055618 (* 1 = 0.055618 loss)
I0916 23:40:09.485034 13570 sgd_solver.cpp:136] Iteration 57400, lr = 0.0001, m = 0.9
I0916 23:40:28.883551 13570 solver.cpp:314] Iteration 57500 (5.15515 iter/s, 19.3981s/100 iter), loss = 0.10784
I0916 23:40:28.883577 13570 solver.cpp:336]     Train net output #0: loss = 0.10784 (* 1 = 0.10784 loss)
I0916 23:40:28.883582 13570 sgd_solver.cpp:136] Iteration 57500, lr = 0.0001, m = 0.9
I0916 23:40:48.230381 13570 solver.cpp:314] Iteration 57600 (5.16895 iter/s, 19.3463s/100 iter), loss = 0.0467867
I0916 23:40:48.230437 13570 solver.cpp:336]     Train net output #0: loss = 0.0467866 (* 1 = 0.0467866 loss)
I0916 23:40:48.230443 13570 sgd_solver.cpp:136] Iteration 57600, lr = 0.0001, m = 0.9
I0916 23:41:03.266942 13576 data_reader.cpp:305] Starting prefetch of epoch 38
I0916 23:41:07.517379 13570 solver.cpp:314] Iteration 57700 (5.18498 iter/s, 19.2865s/100 iter), loss = 0.0838724
I0916 23:41:07.517406 13570 solver.cpp:336]     Train net output #0: loss = 0.0838723 (* 1 = 0.0838723 loss)
I0916 23:41:07.517413 13570 sgd_solver.cpp:136] Iteration 57700, lr = 0.0001, m = 0.9
I0916 23:41:27.397353 13570 solver.cpp:314] Iteration 57800 (5.03033 iter/s, 19.8794s/100 iter), loss = 0.0409428
I0916 23:41:27.397462 13570 solver.cpp:336]     Train net output #0: loss = 0.0409427 (* 1 = 0.0409427 loss)
I0916 23:41:27.397469 13570 sgd_solver.cpp:136] Iteration 57800, lr = 0.0001, m = 0.9
I0916 23:41:36.111444 13544 data_reader.cpp:305] Starting prefetch of epoch 35
I0916 23:41:46.904932 13570 solver.cpp:314] Iteration 57900 (5.12636 iter/s, 19.507s/100 iter), loss = 0.0427548
I0916 23:41:46.904956 13570 solver.cpp:336]     Train net output #0: loss = 0.0427547 (* 1 = 0.0427547 loss)
I0916 23:41:46.904964 13570 sgd_solver.cpp:136] Iteration 57900, lr = 0.0001, m = 0.9
I0916 23:42:06.045259 13570 solver.cpp:368] Sparsity after update:
I0916 23:42:06.049607 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 23:42:06.049634 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 23:42:06.049644 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 23:42:06.049649 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 23:42:06.049652 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 23:42:06.049655 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 23:42:06.049660 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 23:42:06.049664 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 23:42:06.049667 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 23:42:06.049670 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 23:42:06.049674 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 23:42:06.049676 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 23:42:06.049679 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 23:42:06.049682 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 23:42:06.049685 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 23:42:06.049688 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 23:42:06.049691 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 23:42:06.049695 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 23:42:06.049697 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 23:42:06.049711 13570 solver.cpp:563] Iteration 58000, Testing net (#0)
I0916 23:42:17.341150 13570 solver.cpp:655]     Test net output #0: accuracy/top1 = 0.95427
I0916 23:42:17.341168 13570 solver.cpp:655]     Test net output #1: accuracy/top5 = 1
I0916 23:42:17.341173 13570 solver.cpp:655]     Test net output #2: loss = 0.161728 (* 1 = 0.161728 loss)
I0916 23:42:17.341233 13570 solver.cpp:265] [MultiGPU] Tests completed in 11.2912s
I0916 23:42:17.554136 13570 solver.cpp:314] Iteration 58000 (3.26282 iter/s, 30.6483s/100 iter), loss = 0.0397973
I0916 23:42:17.554260 13570 solver.cpp:336]     Train net output #0: loss = 0.0397972 (* 1 = 0.0397972 loss)
I0916 23:42:17.554280 13570 sgd_solver.cpp:136] Iteration 58000, lr = 0.0001, m = 0.9
I0916 23:42:19.298676 13576 data_reader.cpp:305] Starting prefetch of epoch 39
I0916 23:42:37.110721 13570 solver.cpp:314] Iteration 58100 (5.11351 iter/s, 19.556s/100 iter), loss = 0.0668931
I0916 23:42:37.110796 13570 solver.cpp:336]     Train net output #0: loss = 0.066893 (* 1 = 0.066893 loss)
I0916 23:42:37.110811 13570 sgd_solver.cpp:136] Iteration 58100, lr = 0.0001, m = 0.9
I0916 23:42:56.360340 13570 solver.cpp:314] Iteration 58200 (5.19505 iter/s, 19.2491s/100 iter), loss = 0.0758625
I0916 23:42:56.360365 13570 solver.cpp:336]     Train net output #0: loss = 0.0758624 (* 1 = 0.0758624 loss)
I0916 23:42:56.360369 13570 sgd_solver.cpp:136] Iteration 58200, lr = 0.0001, m = 0.9
I0916 23:43:15.652881 13570 solver.cpp:314] Iteration 58300 (5.18349 iter/s, 19.292s/100 iter), loss = 0.0353228
I0916 23:43:15.652932 13570 solver.cpp:336]     Train net output #0: loss = 0.0353227 (* 1 = 0.0353227 loss)
I0916 23:43:15.652937 13570 sgd_solver.cpp:136] Iteration 58300, lr = 0.0001, m = 0.9
I0916 23:43:23.399142 13574 data_reader.cpp:305] Starting prefetch of epoch 37
I0916 23:43:35.019021 13570 solver.cpp:314] Iteration 58400 (5.16379 iter/s, 19.3656s/100 iter), loss = 0.0945347
I0916 23:43:35.019048 13570 solver.cpp:336]     Train net output #0: loss = 0.0945346 (* 1 = 0.0945346 loss)
I0916 23:43:35.019053 13570 sgd_solver.cpp:136] Iteration 58400, lr = 0.0001, m = 0.9
I0916 23:43:54.589736 13570 solver.cpp:314] Iteration 58500 (5.10982 iter/s, 19.5702s/100 iter), loss = 0.0347936
I0916 23:43:54.589844 13570 solver.cpp:336]     Train net output #0: loss = 0.0347935 (* 1 = 0.0347935 loss)
I0916 23:43:54.589854 13570 sgd_solver.cpp:136] Iteration 58500, lr = 0.0001, m = 0.9
I0916 23:43:55.593966 13574 data_reader.cpp:305] Starting prefetch of epoch 38
I0916 23:44:14.194973 13570 solver.cpp:314] Iteration 58600 (5.10082 iter/s, 19.6047s/100 iter), loss = 0.0838758
I0916 23:44:14.195001 13570 solver.cpp:336]     Train net output #0: loss = 0.0838757 (* 1 = 0.0838757 loss)
I0916 23:44:14.195008 13570 sgd_solver.cpp:136] Iteration 58600, lr = 0.0001, m = 0.9
I0916 23:44:33.514062 13570 solver.cpp:314] Iteration 58700 (5.17637 iter/s, 19.3185s/100 iter), loss = 0.10089
I0916 23:44:33.514124 13570 solver.cpp:336]     Train net output #0: loss = 0.10089 (* 1 = 0.10089 loss)
I0916 23:44:33.514129 13570 sgd_solver.cpp:136] Iteration 58700, lr = 0.0001, m = 0.9
I0916 23:44:53.203232 13570 solver.cpp:314] Iteration 58800 (5.07908 iter/s, 19.6886s/100 iter), loss = 0.0786024
I0916 23:44:53.203259 13570 solver.cpp:336]     Train net output #0: loss = 0.0786023 (* 1 = 0.0786023 loss)
I0916 23:44:53.203264 13570 sgd_solver.cpp:136] Iteration 58800, lr = 0.0001, m = 0.9
I0916 23:45:00.139621 13546 data_reader.cpp:305] Starting prefetch of epoch 25
I0916 23:45:12.848071 13570 solver.cpp:314] Iteration 58900 (5.09054 iter/s, 19.6443s/100 iter), loss = 0.0637009
I0916 23:45:12.848129 13570 solver.cpp:336]     Train net output #0: loss = 0.0637008 (* 1 = 0.0637008 loss)
I0916 23:45:12.848134 13570 sgd_solver.cpp:136] Iteration 58900, lr = 0.0001, m = 0.9
I0916 23:45:31.912201 13570 solver.cpp:368] Sparsity after update:
I0916 23:45:31.934833 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 23:45:31.934849 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 23:45:31.934855 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 23:45:31.934859 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 23:45:31.934860 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 23:45:31.934862 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 23:45:31.934864 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 23:45:31.934866 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 23:45:31.934869 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 23:45:31.934870 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 23:45:31.934872 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 23:45:31.934881 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 23:45:31.934883 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 23:45:31.934885 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 23:45:31.934887 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 23:45:31.934891 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 23:45:31.934893 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 23:45:31.934895 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 23:45:31.934900 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 23:45:32.107666 13570 solver.cpp:314] Iteration 59000 (5.19236 iter/s, 19.2591s/100 iter), loss = 0.0507515
I0916 23:45:32.107692 13570 solver.cpp:336]     Train net output #0: loss = 0.0507514 (* 1 = 0.0507514 loss)
I0916 23:45:32.107699 13570 sgd_solver.cpp:136] Iteration 59000, lr = 0.0001, m = 0.9
I0916 23:45:51.735883 13570 solver.cpp:314] Iteration 59100 (5.09485 iter/s, 19.6277s/100 iter), loss = 0.0537824
I0916 23:45:51.735944 13570 solver.cpp:336]     Train net output #0: loss = 0.0537823 (* 1 = 0.0537823 loss)
I0916 23:45:51.735949 13570 sgd_solver.cpp:136] Iteration 59100, lr = 0.0001, m = 0.9
I0916 23:46:04.661067 13576 data_reader.cpp:305] Starting prefetch of epoch 40
I0916 23:46:11.462121 13570 solver.cpp:314] Iteration 59200 (5.06953 iter/s, 19.7257s/100 iter), loss = 0.0603915
I0916 23:46:11.462144 13570 solver.cpp:336]     Train net output #0: loss = 0.0603914 (* 1 = 0.0603914 loss)
I0916 23:46:11.462149 13570 sgd_solver.cpp:136] Iteration 59200, lr = 0.0001, m = 0.9
I0916 23:46:31.078999 13570 solver.cpp:314] Iteration 59300 (5.09779 iter/s, 19.6163s/100 iter), loss = 0.0427293
I0916 23:46:31.079105 13570 solver.cpp:336]     Train net output #0: loss = 0.0427292 (* 1 = 0.0427292 loss)
I0916 23:46:31.079113 13570 sgd_solver.cpp:136] Iteration 59300, lr = 0.0001, m = 0.9
I0916 23:46:37.202098 13544 data_reader.cpp:305] Starting prefetch of epoch 36
I0916 23:46:50.704800 13570 solver.cpp:314] Iteration 59400 (5.09548 iter/s, 19.6253s/100 iter), loss = 0.0593776
I0916 23:46:50.704828 13570 solver.cpp:336]     Train net output #0: loss = 0.0593775 (* 1 = 0.0593775 loss)
I0916 23:46:50.704833 13570 sgd_solver.cpp:136] Iteration 59400, lr = 0.0001, m = 0.9
I0916 23:47:10.479602 13570 solver.cpp:314] Iteration 59500 (5.05708 iter/s, 19.7743s/100 iter), loss = 0.0603493
I0916 23:47:10.511178 13570 solver.cpp:336]     Train net output #0: loss = 0.0603492 (* 1 = 0.0603492 loss)
I0916 23:47:10.511212 13570 sgd_solver.cpp:136] Iteration 59500, lr = 0.0001, m = 0.9
I0916 23:47:30.112824 13570 solver.cpp:314] Iteration 59600 (5.09355 iter/s, 19.6327s/100 iter), loss = 0.0487224
I0916 23:47:30.112850 13570 solver.cpp:336]     Train net output #0: loss = 0.0487223 (* 1 = 0.0487223 loss)
I0916 23:47:30.112854 13570 sgd_solver.cpp:136] Iteration 59600, lr = 0.0001, m = 0.9
I0916 23:47:42.220775 13546 data_reader.cpp:305] Starting prefetch of epoch 26
I0916 23:47:49.468168 13570 solver.cpp:314] Iteration 59700 (5.16668 iter/s, 19.3548s/100 iter), loss = 0.0812413
I0916 23:47:49.468190 13570 solver.cpp:336]     Train net output #0: loss = 0.0812412 (* 1 = 0.0812412 loss)
I0916 23:47:49.468196 13570 sgd_solver.cpp:136] Iteration 59700, lr = 0.0001, m = 0.9
I0916 23:48:09.442443 13570 solver.cpp:314] Iteration 59800 (5.00658 iter/s, 19.9737s/100 iter), loss = 0.102624
I0916 23:48:09.442550 13570 solver.cpp:336]     Train net output #0: loss = 0.102624 (* 1 = 0.102624 loss)
I0916 23:48:09.442571 13570 sgd_solver.cpp:136] Iteration 59800, lr = 0.0001, m = 0.9
I0916 23:48:28.925127 13570 solver.cpp:314] Iteration 59900 (5.13291 iter/s, 19.4821s/100 iter), loss = 0.0796353
I0916 23:48:28.925178 13570 solver.cpp:336]     Train net output #0: loss = 0.0796352 (* 1 = 0.0796352 loss)
I0916 23:48:28.925184 13570 sgd_solver.cpp:136] Iteration 59900, lr = 0.0001, m = 0.9
I0916 23:48:47.285440 13546 data_reader.cpp:305] Starting prefetch of epoch 27
I0916 23:48:48.618367 13570 solver.cpp:314] Iteration 59999 (5.02725 iter/s, 19.6927s/99 iter), loss = 0.0596415
I0916 23:48:48.618391 13570 solver.cpp:336]     Train net output #0: loss = 0.0596414 (* 1 = 0.0596414 loss)
I0916 23:48:48.618397 13570 solver.cpp:825] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/cityscapes5_jsegnet21v2_iter_60000.caffemodel
I0916 23:48:48.632429 13570 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-09-16_10-06-43/sparse/cityscapes5_jsegnet21v2_iter_60000.solverstate
I0916 23:48:48.640441 13570 solver.cpp:368] Sparsity after update:
I0916 23:48:48.642267 13570 net.cpp:2293] Num Params(17), Sparsity (zero_weights/count): 
I0916 23:48:48.642277 13570 net.cpp:2304] conv1a_param_0(0.326) 
I0916 23:48:48.642285 13570 net.cpp:2304] conv1b_param_0(0.733) 
I0916 23:48:48.642289 13570 net.cpp:2304] ctx_conv1_param_0(0.807) 
I0916 23:48:48.642292 13570 net.cpp:2304] ctx_conv2_param_0(0.808) 
I0916 23:48:48.642295 13570 net.cpp:2304] ctx_conv3_param_0(0.808) 
I0916 23:48:48.642298 13570 net.cpp:2304] ctx_conv4_param_0(0.808) 
I0916 23:48:48.642302 13570 net.cpp:2304] ctx_final_param_0(0.253) 
I0916 23:48:48.642304 13570 net.cpp:2304] out3a_param_0(0.809) 
I0916 23:48:48.642307 13570 net.cpp:2304] out5a_param_0(0.81) 
I0916 23:48:48.642312 13570 net.cpp:2304] res2a_branch2a_param_0(0.801) 
I0916 23:48:48.642314 13570 net.cpp:2304] res2a_branch2b_param_0(0.702) 
I0916 23:48:48.642318 13570 net.cpp:2304] res3a_branch2a_param_0(0.805) 
I0916 23:48:48.642320 13570 net.cpp:2304] res3a_branch2b_param_0(0.784) 
I0916 23:48:48.642324 13570 net.cpp:2304] res4a_branch2a_param_0(0.81) 
I0916 23:48:48.642328 13570 net.cpp:2304] res4a_branch2b_param_0(0.803) 
I0916 23:48:48.642333 13570 net.cpp:2304] res5a_branch2a_param_0(0.808) 
I0916 23:48:48.642336 13570 net.cpp:2304] res5a_branch2b_param_0(0.81) 
I0916 23:48:48.642339 13570 net.cpp:2308] Total Sparsity (zero_weights/count) =  (2.16983e+06/2.69117e+06) 0.806
I0916 23:48:48.737375 13570 solver.cpp:538] Iteration 60000, loss = 0.0525384
I0916 23:48:48.737648 13570 solver.cpp:563] Iteration 60000, Testing net (#0)
I0916 23:48:52.371510 13568 data_reader.cpp:305] Starting prefetch of epoch 5
I0916 23:49:01.092190 13570 solver.cpp:655]     Test net output #0: accuracy/top1 = 0.951396
I0916 23:49:01.092320 13570 solver.cpp:655]     Test net output #1: accuracy/top5 = 1
I0916 23:49:01.092330 13570 solver.cpp:655]     Test net output #2: loss = 0.14866 (* 1 = 0.14866 loss)
I0916 23:49:01.188536 13475 parallel.cpp:71] Root Solver performance on device 0: 4.934 * 6 = 29.6 img/sec (60000 itr in 1.216e+04 sec)
I0916 23:49:01.188556 13475 parallel.cpp:76]      Solver performance on device 1: 4.934 * 6 = 29.6 img/sec (60000 itr in 1.216e+04 sec)
I0916 23:49:01.188561 13475 parallel.cpp:76]      Solver performance on device 2: 4.934 * 6 = 29.6 img/sec (60000 itr in 1.216e+04 sec)
I0916 23:49:01.188563 13475 parallel.cpp:79] Overall multi-GPU performance: 88.8128 img/sec
I0916 23:49:02.822319 13475 caffe.cpp:253] Optimization Done in 3h 23m 0s
