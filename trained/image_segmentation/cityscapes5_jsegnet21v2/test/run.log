I0816 00:30:44.308667  9762 caffe.cpp:608] This is NVCaffe 0.16.3 started at Wed Aug 16 00:30:43 2017
I0816 00:30:44.309717  9762 caffe.cpp:611] CuDNN version: 6021
I0816 00:30:44.309721  9762 caffe.cpp:612] CuBLAS version: 8000
I0816 00:30:44.309723  9762 caffe.cpp:613] CUDA version: 8000
I0816 00:30:44.309725  9762 caffe.cpp:614] CUDA driver version: 8000
I0816 00:30:44.309731  9762 caffe.cpp:263] Not using GPU #2 for single-GPU function
I0816 00:30:44.309733  9762 caffe.cpp:263] Not using GPU #1 for single-GPU function
I0816 00:30:44.310281  9762 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0816 00:30:44.310819  9762 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I0816 00:30:44.310824  9762 caffe.cpp:275] Use GPU with device ID 0
I0816 00:30:44.311146  9762 caffe.cpp:279] GPU device name: GeForce GTX 1080
I0816 00:30:44.328624  9762 net.cpp:72] Initializing net from parameters: 
name: "jsegnet21v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/val-image-lmdb"
    label_list_path: "data/val-label-lmdb"
    batch_size: 4
    threads: 1
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
  accuracy_param {
    ignore_label: 255
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
    ignore_label: 255
  }
}
I0816 00:30:44.328812  9762 net.cpp:104] Using FLOAT as default forward math type
I0816 00:30:44.328829  9762 net.cpp:110] Using FLOAT as default backward math type
I0816 00:30:44.328835  9762 layer_factory.hpp:136] Creating layer 'data' of type 'ImageLabelData'
I0816 00:30:44.328840  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.328858  9762 net.cpp:184] Created Layer data (0)
I0816 00:30:44.328869  9762 net.cpp:530] data -> data
I0816 00:30:44.328886  9762 net.cpp:530] data -> label
I0816 00:30:44.341277  9762 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 4
I0816 00:30:44.341292  9762 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0816 00:30:44.357152  9811 db_lmdb.cpp:24] Opened lmdb data/val-image-lmdb
I0816 00:30:44.359326  9762 data_layer.cpp:185] (0) ReshapePrefetch 4, 3, 640, 640
I0816 00:30:44.359377  9762 data_layer.cpp:209] (0) Output data size: 4, 3, 640, 640
I0816 00:30:44.359383  9762 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0816 00:30:44.359457  9762 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 4
I0816 00:30:44.359468  9762 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0816 00:30:44.360188  9812 data_layer.cpp:97] (0) Parser threads: 1
I0816 00:30:44.360198  9812 data_layer.cpp:99] (0) Transformer threads: 1
I0816 00:30:44.364596  9813 db_lmdb.cpp:24] Opened lmdb data/val-label-lmdb
I0816 00:30:44.365926  9762 data_layer.cpp:185] (0) ReshapePrefetch 4, 1, 640, 640
I0816 00:30:44.365962  9762 data_layer.cpp:209] (0) Output data size: 4, 1, 640, 640
I0816 00:30:44.365970  9762 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0816 00:30:44.366199  9762 net.cpp:245] Setting up data
I0816 00:30:44.366221  9762 net.cpp:252] TEST Top shape for layer 0 'data' 4 3 640 640 (4915200)
I0816 00:30:44.366237  9762 net.cpp:252] TEST Top shape for layer 0 'data' 4 1 640 640 (1638400)
I0816 00:30:44.366250  9762 layer_factory.hpp:136] Creating layer 'label_data_1_split' of type 'Split'
I0816 00:30:44.366263  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.366291  9762 net.cpp:184] Created Layer label_data_1_split (1)
I0816 00:30:44.366302  9762 net.cpp:561] label_data_1_split <- label
I0816 00:30:44.366322  9762 net.cpp:530] label_data_1_split -> label_data_1_split_0
I0816 00:30:44.366333  9762 net.cpp:530] label_data_1_split -> label_data_1_split_1
I0816 00:30:44.366338  9762 net.cpp:530] label_data_1_split -> label_data_1_split_2
I0816 00:30:44.366399  9762 net.cpp:245] Setting up label_data_1_split
I0816 00:30:44.366405  9762 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 4 1 640 640 (1638400)
I0816 00:30:44.366410  9762 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 4 1 640 640 (1638400)
I0816 00:30:44.366415  9762 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 4 1 640 640 (1638400)
I0816 00:30:44.366420  9762 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0816 00:30:44.366425  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.366446  9762 net.cpp:184] Created Layer data/bias (2)
I0816 00:30:44.366451  9762 net.cpp:561] data/bias <- data
I0816 00:30:44.366456  9762 net.cpp:530] data/bias -> data/bias
I0816 00:30:44.367733  9814 data_layer.cpp:97] (0) Parser threads: 1
I0816 00:30:44.367755  9814 data_layer.cpp:99] (0) Transformer threads: 1
I0816 00:30:44.370901  9762 net.cpp:245] Setting up data/bias
I0816 00:30:44.370976  9762 net.cpp:252] TEST Top shape for layer 2 'data/bias' 4 3 640 640 (4915200)
I0816 00:30:44.371001  9762 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0816 00:30:44.371014  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.371049  9762 net.cpp:184] Created Layer conv1a (3)
I0816 00:30:44.371068  9762 net.cpp:561] conv1a <- data/bias
I0816 00:30:44.371076  9762 net.cpp:530] conv1a -> conv1a
I0816 00:30:44.941376  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.01G/1 1  (limit 8.06G, req 0G)
I0816 00:30:44.941398  9762 net.cpp:245] Setting up conv1a
I0816 00:30:44.941406  9762 net.cpp:252] TEST Top shape for layer 3 'conv1a' 4 32 320 320 (13107200)
I0816 00:30:44.941417  9762 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0816 00:30:44.941423  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.941437  9762 net.cpp:184] Created Layer conv1a/bn (4)
I0816 00:30:44.941442  9762 net.cpp:561] conv1a/bn <- conv1a
I0816 00:30:44.941447  9762 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0816 00:30:44.942003  9762 net.cpp:245] Setting up conv1a/bn
I0816 00:30:44.942013  9762 net.cpp:252] TEST Top shape for layer 4 'conv1a/bn' 4 32 320 320 (13107200)
I0816 00:30:44.942023  9762 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0816 00:30:44.942028  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.942039  9762 net.cpp:184] Created Layer conv1a/relu (5)
I0816 00:30:44.942042  9762 net.cpp:561] conv1a/relu <- conv1a
I0816 00:30:44.942046  9762 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0816 00:30:44.942059  9762 net.cpp:245] Setting up conv1a/relu
I0816 00:30:44.942065  9762 net.cpp:252] TEST Top shape for layer 5 'conv1a/relu' 4 32 320 320 (13107200)
I0816 00:30:44.942068  9762 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0816 00:30:44.942072  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.942082  9762 net.cpp:184] Created Layer conv1b (6)
I0816 00:30:44.942086  9762 net.cpp:561] conv1b <- conv1a
I0816 00:30:44.942091  9762 net.cpp:530] conv1b -> conv1b
I0816 00:30:44.959354  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 8G, req 0G)
I0816 00:30:44.959365  9762 net.cpp:245] Setting up conv1b
I0816 00:30:44.959370  9762 net.cpp:252] TEST Top shape for layer 6 'conv1b' 4 32 320 320 (13107200)
I0816 00:30:44.959378  9762 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0816 00:30:44.959383  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.959389  9762 net.cpp:184] Created Layer conv1b/bn (7)
I0816 00:30:44.959393  9762 net.cpp:561] conv1b/bn <- conv1b
I0816 00:30:44.959398  9762 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0816 00:30:44.959930  9762 net.cpp:245] Setting up conv1b/bn
I0816 00:30:44.959939  9762 net.cpp:252] TEST Top shape for layer 7 'conv1b/bn' 4 32 320 320 (13107200)
I0816 00:30:44.959949  9762 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0816 00:30:44.959951  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.959956  9762 net.cpp:184] Created Layer conv1b/relu (8)
I0816 00:30:44.959959  9762 net.cpp:561] conv1b/relu <- conv1b
I0816 00:30:44.959964  9762 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0816 00:30:44.959969  9762 net.cpp:245] Setting up conv1b/relu
I0816 00:30:44.959972  9762 net.cpp:252] TEST Top shape for layer 8 'conv1b/relu' 4 32 320 320 (13107200)
I0816 00:30:44.959976  9762 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0816 00:30:44.959980  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.959986  9762 net.cpp:184] Created Layer pool1 (9)
I0816 00:30:44.959990  9762 net.cpp:561] pool1 <- conv1b
I0816 00:30:44.959995  9762 net.cpp:530] pool1 -> pool1
I0816 00:30:44.960045  9762 net.cpp:245] Setting up pool1
I0816 00:30:44.960052  9762 net.cpp:252] TEST Top shape for layer 9 'pool1' 4 32 160 160 (3276800)
I0816 00:30:44.960055  9762 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0816 00:30:44.960060  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.960080  9762 net.cpp:184] Created Layer res2a_branch2a (10)
I0816 00:30:44.960085  9762 net.cpp:561] res2a_branch2a <- pool1
I0816 00:30:44.960090  9762 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0816 00:30:44.972374  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.95G, req 0G)
I0816 00:30:44.972386  9762 net.cpp:245] Setting up res2a_branch2a
I0816 00:30:44.972391  9762 net.cpp:252] TEST Top shape for layer 10 'res2a_branch2a' 4 64 160 160 (6553600)
I0816 00:30:44.972399  9762 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0816 00:30:44.972404  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.972410  9762 net.cpp:184] Created Layer res2a_branch2a/bn (11)
I0816 00:30:44.972414  9762 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0816 00:30:44.972419  9762 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0816 00:30:44.972949  9762 net.cpp:245] Setting up res2a_branch2a/bn
I0816 00:30:44.972957  9762 net.cpp:252] TEST Top shape for layer 11 'res2a_branch2a/bn' 4 64 160 160 (6553600)
I0816 00:30:44.972965  9762 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0816 00:30:44.972970  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.972973  9762 net.cpp:184] Created Layer res2a_branch2a/relu (12)
I0816 00:30:44.972977  9762 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0816 00:30:44.972980  9762 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0816 00:30:44.972986  9762 net.cpp:245] Setting up res2a_branch2a/relu
I0816 00:30:44.972990  9762 net.cpp:252] TEST Top shape for layer 12 'res2a_branch2a/relu' 4 64 160 160 (6553600)
I0816 00:30:44.972995  9762 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0816 00:30:44.972997  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.973006  9762 net.cpp:184] Created Layer res2a_branch2b (13)
I0816 00:30:44.973011  9762 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0816 00:30:44.973014  9762 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0816 00:30:44.981643  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.92G, req 0G)
I0816 00:30:44.981662  9762 net.cpp:245] Setting up res2a_branch2b
I0816 00:30:44.981667  9762 net.cpp:252] TEST Top shape for layer 13 'res2a_branch2b' 4 64 160 160 (6553600)
I0816 00:30:44.981674  9762 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0816 00:30:44.981679  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.981688  9762 net.cpp:184] Created Layer res2a_branch2b/bn (14)
I0816 00:30:44.981693  9762 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0816 00:30:44.981696  9762 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0816 00:30:44.982255  9762 net.cpp:245] Setting up res2a_branch2b/bn
I0816 00:30:44.982264  9762 net.cpp:252] TEST Top shape for layer 14 'res2a_branch2b/bn' 4 64 160 160 (6553600)
I0816 00:30:44.982273  9762 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0816 00:30:44.982277  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.982282  9762 net.cpp:184] Created Layer res2a_branch2b/relu (15)
I0816 00:30:44.982285  9762 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0816 00:30:44.982290  9762 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0816 00:30:44.982295  9762 net.cpp:245] Setting up res2a_branch2b/relu
I0816 00:30:44.982300  9762 net.cpp:252] TEST Top shape for layer 15 'res2a_branch2b/relu' 4 64 160 160 (6553600)
I0816 00:30:44.982303  9762 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0816 00:30:44.982306  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.982326  9762 net.cpp:184] Created Layer pool2 (16)
I0816 00:30:44.982329  9762 net.cpp:561] pool2 <- res2a_branch2b
I0816 00:30:44.982332  9762 net.cpp:530] pool2 -> pool2
I0816 00:30:44.982374  9762 net.cpp:245] Setting up pool2
I0816 00:30:44.982380  9762 net.cpp:252] TEST Top shape for layer 16 'pool2' 4 64 80 80 (1638400)
I0816 00:30:44.982384  9762 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0816 00:30:44.982388  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.982396  9762 net.cpp:184] Created Layer res3a_branch2a (17)
I0816 00:30:44.982399  9762 net.cpp:561] res3a_branch2a <- pool2
I0816 00:30:44.982403  9762 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0816 00:30:44.989929  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.9G, req 0G)
I0816 00:30:44.989940  9762 net.cpp:245] Setting up res3a_branch2a
I0816 00:30:44.989946  9762 net.cpp:252] TEST Top shape for layer 17 'res3a_branch2a' 4 128 80 80 (3276800)
I0816 00:30:44.989953  9762 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0816 00:30:44.989956  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.989962  9762 net.cpp:184] Created Layer res3a_branch2a/bn (18)
I0816 00:30:44.989965  9762 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0816 00:30:44.989969  9762 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0816 00:30:44.990906  9762 net.cpp:245] Setting up res3a_branch2a/bn
I0816 00:30:44.990916  9762 net.cpp:252] TEST Top shape for layer 18 'res3a_branch2a/bn' 4 128 80 80 (3276800)
I0816 00:30:44.990926  9762 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0816 00:30:44.990929  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.990934  9762 net.cpp:184] Created Layer res3a_branch2a/relu (19)
I0816 00:30:44.990937  9762 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0816 00:30:44.990942  9762 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0816 00:30:44.990948  9762 net.cpp:245] Setting up res3a_branch2a/relu
I0816 00:30:44.990952  9762 net.cpp:252] TEST Top shape for layer 19 'res3a_branch2a/relu' 4 128 80 80 (3276800)
I0816 00:30:44.990957  9762 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0816 00:30:44.990960  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.990968  9762 net.cpp:184] Created Layer res3a_branch2b (20)
I0816 00:30:44.990972  9762 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0816 00:30:44.990977  9762 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0816 00:30:44.996207  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.88G, req 0G)
I0816 00:30:44.996218  9762 net.cpp:245] Setting up res3a_branch2b
I0816 00:30:44.996225  9762 net.cpp:252] TEST Top shape for layer 20 'res3a_branch2b' 4 128 80 80 (3276800)
I0816 00:30:44.996232  9762 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0816 00:30:44.996237  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.996243  9762 net.cpp:184] Created Layer res3a_branch2b/bn (21)
I0816 00:30:44.996248  9762 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0816 00:30:44.996253  9762 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0816 00:30:44.996757  9762 net.cpp:245] Setting up res3a_branch2b/bn
I0816 00:30:44.996765  9762 net.cpp:252] TEST Top shape for layer 21 'res3a_branch2b/bn' 4 128 80 80 (3276800)
I0816 00:30:44.996773  9762 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0816 00:30:44.996778  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.996783  9762 net.cpp:184] Created Layer res3a_branch2b/relu (22)
I0816 00:30:44.996794  9762 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0816 00:30:44.996800  9762 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0816 00:30:44.996805  9762 net.cpp:245] Setting up res3a_branch2b/relu
I0816 00:30:44.996809  9762 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b/relu' 4 128 80 80 (3276800)
I0816 00:30:44.996824  9762 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0816 00:30:44.996829  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.996834  9762 net.cpp:184] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (23)
I0816 00:30:44.996837  9762 net.cpp:561] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0816 00:30:44.996842  9762 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0816 00:30:44.996847  9762 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0816 00:30:44.996877  9762 net.cpp:245] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0816 00:30:44.996884  9762 net.cpp:252] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 4 128 80 80 (3276800)
I0816 00:30:44.996889  9762 net.cpp:252] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 4 128 80 80 (3276800)
I0816 00:30:44.996893  9762 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0816 00:30:44.996897  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.996902  9762 net.cpp:184] Created Layer pool3 (24)
I0816 00:30:44.996906  9762 net.cpp:561] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0816 00:30:44.996912  9762 net.cpp:530] pool3 -> pool3
I0816 00:30:44.996960  9762 net.cpp:245] Setting up pool3
I0816 00:30:44.996968  9762 net.cpp:252] TEST Top shape for layer 24 'pool3' 4 128 40 40 (819200)
I0816 00:30:44.996971  9762 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0816 00:30:44.996975  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:44.996984  9762 net.cpp:184] Created Layer res4a_branch2a (25)
I0816 00:30:44.996987  9762 net.cpp:561] res4a_branch2a <- pool3
I0816 00:30:44.996992  9762 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0816 00:30:45.011598  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.87G, req 0G)
I0816 00:30:45.011616  9762 net.cpp:245] Setting up res4a_branch2a
I0816 00:30:45.011623  9762 net.cpp:252] TEST Top shape for layer 25 'res4a_branch2a' 4 256 40 40 (1638400)
I0816 00:30:45.011631  9762 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0816 00:30:45.011636  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.011646  9762 net.cpp:184] Created Layer res4a_branch2a/bn (26)
I0816 00:30:45.011651  9762 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0816 00:30:45.011656  9762 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0816 00:30:45.012204  9762 net.cpp:245] Setting up res4a_branch2a/bn
I0816 00:30:45.012213  9762 net.cpp:252] TEST Top shape for layer 26 'res4a_branch2a/bn' 4 256 40 40 (1638400)
I0816 00:30:45.012223  9762 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0816 00:30:45.012226  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.012230  9762 net.cpp:184] Created Layer res4a_branch2a/relu (27)
I0816 00:30:45.012234  9762 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0816 00:30:45.012238  9762 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0816 00:30:45.012243  9762 net.cpp:245] Setting up res4a_branch2a/relu
I0816 00:30:45.012248  9762 net.cpp:252] TEST Top shape for layer 27 'res4a_branch2a/relu' 4 256 40 40 (1638400)
I0816 00:30:45.012253  9762 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0816 00:30:45.012266  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.012275  9762 net.cpp:184] Created Layer res4a_branch2b (28)
I0816 00:30:45.012279  9762 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0816 00:30:45.012284  9762 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0816 00:30:45.019093  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.86G, req 0G)
I0816 00:30:45.019104  9762 net.cpp:245] Setting up res4a_branch2b
I0816 00:30:45.019109  9762 net.cpp:252] TEST Top shape for layer 28 'res4a_branch2b' 4 256 40 40 (1638400)
I0816 00:30:45.019114  9762 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0816 00:30:45.019117  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.019124  9762 net.cpp:184] Created Layer res4a_branch2b/bn (29)
I0816 00:30:45.019127  9762 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0816 00:30:45.019131  9762 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0816 00:30:45.019640  9762 net.cpp:245] Setting up res4a_branch2b/bn
I0816 00:30:45.019649  9762 net.cpp:252] TEST Top shape for layer 29 'res4a_branch2b/bn' 4 256 40 40 (1638400)
I0816 00:30:45.019656  9762 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0816 00:30:45.019660  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.019665  9762 net.cpp:184] Created Layer res4a_branch2b/relu (30)
I0816 00:30:45.019667  9762 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0816 00:30:45.019671  9762 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0816 00:30:45.019676  9762 net.cpp:245] Setting up res4a_branch2b/relu
I0816 00:30:45.019680  9762 net.cpp:252] TEST Top shape for layer 30 'res4a_branch2b/relu' 4 256 40 40 (1638400)
I0816 00:30:45.019685  9762 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0816 00:30:45.019688  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.019697  9762 net.cpp:184] Created Layer pool4 (31)
I0816 00:30:45.019701  9762 net.cpp:561] pool4 <- res4a_branch2b
I0816 00:30:45.019706  9762 net.cpp:530] pool4 -> pool4
I0816 00:30:45.019745  9762 net.cpp:245] Setting up pool4
I0816 00:30:45.019752  9762 net.cpp:252] TEST Top shape for layer 31 'pool4' 4 256 40 40 (1638400)
I0816 00:30:45.019755  9762 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0816 00:30:45.019759  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.019773  9762 net.cpp:184] Created Layer res5a_branch2a (32)
I0816 00:30:45.019776  9762 net.cpp:561] res5a_branch2a <- pool4
I0816 00:30:45.019779  9762 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0816 00:30:45.052844  9762 net.cpp:245] Setting up res5a_branch2a
I0816 00:30:45.052865  9762 net.cpp:252] TEST Top shape for layer 32 'res5a_branch2a' 4 512 40 40 (3276800)
I0816 00:30:45.052873  9762 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0816 00:30:45.052878  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.052887  9762 net.cpp:184] Created Layer res5a_branch2a/bn (33)
I0816 00:30:45.052891  9762 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0816 00:30:45.052896  9762 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0816 00:30:45.053966  9762 net.cpp:245] Setting up res5a_branch2a/bn
I0816 00:30:45.053975  9762 net.cpp:252] TEST Top shape for layer 33 'res5a_branch2a/bn' 4 512 40 40 (3276800)
I0816 00:30:45.053984  9762 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0816 00:30:45.053988  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.053993  9762 net.cpp:184] Created Layer res5a_branch2a/relu (34)
I0816 00:30:45.054008  9762 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0816 00:30:45.054011  9762 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0816 00:30:45.054018  9762 net.cpp:245] Setting up res5a_branch2a/relu
I0816 00:30:45.054023  9762 net.cpp:252] TEST Top shape for layer 34 'res5a_branch2a/relu' 4 512 40 40 (3276800)
I0816 00:30:45.054026  9762 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0816 00:30:45.054030  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.054039  9762 net.cpp:184] Created Layer res5a_branch2b (35)
I0816 00:30:45.054044  9762 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0816 00:30:45.054046  9762 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0816 00:30:45.070345  9762 net.cpp:245] Setting up res5a_branch2b
I0816 00:30:45.070358  9762 net.cpp:252] TEST Top shape for layer 35 'res5a_branch2b' 4 512 40 40 (3276800)
I0816 00:30:45.070376  9762 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0816 00:30:45.070381  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.070387  9762 net.cpp:184] Created Layer res5a_branch2b/bn (36)
I0816 00:30:45.070390  9762 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0816 00:30:45.070395  9762 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0816 00:30:45.070911  9762 net.cpp:245] Setting up res5a_branch2b/bn
I0816 00:30:45.070920  9762 net.cpp:252] TEST Top shape for layer 36 'res5a_branch2b/bn' 4 512 40 40 (3276800)
I0816 00:30:45.070929  9762 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0816 00:30:45.070933  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.070938  9762 net.cpp:184] Created Layer res5a_branch2b/relu (37)
I0816 00:30:45.070942  9762 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0816 00:30:45.070947  9762 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0816 00:30:45.070952  9762 net.cpp:245] Setting up res5a_branch2b/relu
I0816 00:30:45.070957  9762 net.cpp:252] TEST Top shape for layer 37 'res5a_branch2b/relu' 4 512 40 40 (3276800)
I0816 00:30:45.070961  9762 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I0816 00:30:45.070966  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.070974  9762 net.cpp:184] Created Layer out5a (38)
I0816 00:30:45.070978  9762 net.cpp:561] out5a <- res5a_branch2b
I0816 00:30:45.070983  9762 net.cpp:530] out5a -> out5a
I0816 00:30:45.075768  9762 net.cpp:245] Setting up out5a
I0816 00:30:45.075779  9762 net.cpp:252] TEST Top shape for layer 38 'out5a' 4 64 40 40 (409600)
I0816 00:30:45.075786  9762 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I0816 00:30:45.075790  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.075798  9762 net.cpp:184] Created Layer out5a/bn (39)
I0816 00:30:45.075801  9762 net.cpp:561] out5a/bn <- out5a
I0816 00:30:45.075806  9762 net.cpp:513] out5a/bn -> out5a (in-place)
I0816 00:30:45.076351  9762 net.cpp:245] Setting up out5a/bn
I0816 00:30:45.076360  9762 net.cpp:252] TEST Top shape for layer 39 'out5a/bn' 4 64 40 40 (409600)
I0816 00:30:45.076370  9762 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I0816 00:30:45.076373  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.076380  9762 net.cpp:184] Created Layer out5a/relu (40)
I0816 00:30:45.076383  9762 net.cpp:561] out5a/relu <- out5a
I0816 00:30:45.076387  9762 net.cpp:513] out5a/relu -> out5a (in-place)
I0816 00:30:45.076392  9762 net.cpp:245] Setting up out5a/relu
I0816 00:30:45.076397  9762 net.cpp:252] TEST Top shape for layer 40 'out5a/relu' 4 64 40 40 (409600)
I0816 00:30:45.076401  9762 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I0816 00:30:45.076414  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.076428  9762 net.cpp:184] Created Layer out5a_up2 (41)
I0816 00:30:45.076432  9762 net.cpp:561] out5a_up2 <- out5a
I0816 00:30:45.076436  9762 net.cpp:530] out5a_up2 -> out5a_up2
I0816 00:30:45.076643  9762 net.cpp:245] Setting up out5a_up2
I0816 00:30:45.076650  9762 net.cpp:252] TEST Top shape for layer 41 'out5a_up2' 4 64 80 80 (1638400)
I0816 00:30:45.076656  9762 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I0816 00:30:45.076660  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.076673  9762 net.cpp:184] Created Layer out3a (42)
I0816 00:30:45.076678  9762 net.cpp:561] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0816 00:30:45.076683  9762 net.cpp:530] out3a -> out3a
I0816 00:30:45.081637  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 7.84G, req 0G)
I0816 00:30:45.081648  9762 net.cpp:245] Setting up out3a
I0816 00:30:45.081655  9762 net.cpp:252] TEST Top shape for layer 42 'out3a' 4 64 80 80 (1638400)
I0816 00:30:45.081660  9762 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I0816 00:30:45.081665  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.081670  9762 net.cpp:184] Created Layer out3a/bn (43)
I0816 00:30:45.081673  9762 net.cpp:561] out3a/bn <- out3a
I0816 00:30:45.081677  9762 net.cpp:513] out3a/bn -> out3a (in-place)
I0816 00:30:45.082237  9762 net.cpp:245] Setting up out3a/bn
I0816 00:30:45.082247  9762 net.cpp:252] TEST Top shape for layer 43 'out3a/bn' 4 64 80 80 (1638400)
I0816 00:30:45.082254  9762 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I0816 00:30:45.082258  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.082262  9762 net.cpp:184] Created Layer out3a/relu (44)
I0816 00:30:45.082265  9762 net.cpp:561] out3a/relu <- out3a
I0816 00:30:45.082269  9762 net.cpp:513] out3a/relu -> out3a (in-place)
I0816 00:30:45.082274  9762 net.cpp:245] Setting up out3a/relu
I0816 00:30:45.082278  9762 net.cpp:252] TEST Top shape for layer 44 'out3a/relu' 4 64 80 80 (1638400)
I0816 00:30:45.082283  9762 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I0816 00:30:45.082286  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.082720  9762 net.cpp:184] Created Layer out3_out5_combined (45)
I0816 00:30:45.082726  9762 net.cpp:561] out3_out5_combined <- out5a_up2
I0816 00:30:45.082729  9762 net.cpp:561] out3_out5_combined <- out3a
I0816 00:30:45.082732  9762 net.cpp:530] out3_out5_combined -> out3_out5_combined
I0816 00:30:45.083721  9762 net.cpp:245] Setting up out3_out5_combined
I0816 00:30:45.083731  9762 net.cpp:252] TEST Top shape for layer 45 'out3_out5_combined' 4 64 80 80 (1638400)
I0816 00:30:45.083735  9762 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I0816 00:30:45.083739  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.083747  9762 net.cpp:184] Created Layer ctx_conv1 (46)
I0816 00:30:45.083751  9762 net.cpp:561] ctx_conv1 <- out3_out5_combined
I0816 00:30:45.083755  9762 net.cpp:530] ctx_conv1 -> ctx_conv1
I0816 00:30:45.088973  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 7.81G, req 0G)
I0816 00:30:45.088984  9762 net.cpp:245] Setting up ctx_conv1
I0816 00:30:45.088989  9762 net.cpp:252] TEST Top shape for layer 46 'ctx_conv1' 4 64 80 80 (1638400)
I0816 00:30:45.088995  9762 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I0816 00:30:45.088999  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.089013  9762 net.cpp:184] Created Layer ctx_conv1/bn (47)
I0816 00:30:45.089025  9762 net.cpp:561] ctx_conv1/bn <- ctx_conv1
I0816 00:30:45.089030  9762 net.cpp:513] ctx_conv1/bn -> ctx_conv1 (in-place)
I0816 00:30:45.089584  9762 net.cpp:245] Setting up ctx_conv1/bn
I0816 00:30:45.089593  9762 net.cpp:252] TEST Top shape for layer 47 'ctx_conv1/bn' 4 64 80 80 (1638400)
I0816 00:30:45.089601  9762 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I0816 00:30:45.089604  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.089608  9762 net.cpp:184] Created Layer ctx_conv1/relu (48)
I0816 00:30:45.089612  9762 net.cpp:561] ctx_conv1/relu <- ctx_conv1
I0816 00:30:45.089615  9762 net.cpp:513] ctx_conv1/relu -> ctx_conv1 (in-place)
I0816 00:30:45.089620  9762 net.cpp:245] Setting up ctx_conv1/relu
I0816 00:30:45.089624  9762 net.cpp:252] TEST Top shape for layer 48 'ctx_conv1/relu' 4 64 80 80 (1638400)
I0816 00:30:45.089628  9762 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I0816 00:30:45.089632  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.089645  9762 net.cpp:184] Created Layer ctx_conv2 (49)
I0816 00:30:45.089649  9762 net.cpp:561] ctx_conv2 <- ctx_conv1
I0816 00:30:45.089653  9762 net.cpp:530] ctx_conv2 -> ctx_conv2
I0816 00:30:45.090852  9762 net.cpp:245] Setting up ctx_conv2
I0816 00:30:45.090860  9762 net.cpp:252] TEST Top shape for layer 49 'ctx_conv2' 4 64 80 80 (1638400)
I0816 00:30:45.090867  9762 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I0816 00:30:45.090870  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.090875  9762 net.cpp:184] Created Layer ctx_conv2/bn (50)
I0816 00:30:45.090879  9762 net.cpp:561] ctx_conv2/bn <- ctx_conv2
I0816 00:30:45.090883  9762 net.cpp:513] ctx_conv2/bn -> ctx_conv2 (in-place)
I0816 00:30:45.091421  9762 net.cpp:245] Setting up ctx_conv2/bn
I0816 00:30:45.091429  9762 net.cpp:252] TEST Top shape for layer 50 'ctx_conv2/bn' 4 64 80 80 (1638400)
I0816 00:30:45.091444  9762 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I0816 00:30:45.091447  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.091451  9762 net.cpp:184] Created Layer ctx_conv2/relu (51)
I0816 00:30:45.091455  9762 net.cpp:561] ctx_conv2/relu <- ctx_conv2
I0816 00:30:45.091459  9762 net.cpp:513] ctx_conv2/relu -> ctx_conv2 (in-place)
I0816 00:30:45.091464  9762 net.cpp:245] Setting up ctx_conv2/relu
I0816 00:30:45.091469  9762 net.cpp:252] TEST Top shape for layer 51 'ctx_conv2/relu' 4 64 80 80 (1638400)
I0816 00:30:45.091472  9762 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I0816 00:30:45.091476  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.091483  9762 net.cpp:184] Created Layer ctx_conv3 (52)
I0816 00:30:45.091487  9762 net.cpp:561] ctx_conv3 <- ctx_conv2
I0816 00:30:45.091492  9762 net.cpp:530] ctx_conv3 -> ctx_conv3
I0816 00:30:45.092700  9762 net.cpp:245] Setting up ctx_conv3
I0816 00:30:45.092710  9762 net.cpp:252] TEST Top shape for layer 52 'ctx_conv3' 4 64 80 80 (1638400)
I0816 00:30:45.092716  9762 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I0816 00:30:45.092720  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.092726  9762 net.cpp:184] Created Layer ctx_conv3/bn (53)
I0816 00:30:45.092730  9762 net.cpp:561] ctx_conv3/bn <- ctx_conv3
I0816 00:30:45.092734  9762 net.cpp:513] ctx_conv3/bn -> ctx_conv3 (in-place)
I0816 00:30:45.093274  9762 net.cpp:245] Setting up ctx_conv3/bn
I0816 00:30:45.093282  9762 net.cpp:252] TEST Top shape for layer 53 'ctx_conv3/bn' 4 64 80 80 (1638400)
I0816 00:30:45.093291  9762 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I0816 00:30:45.093296  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.093307  9762 net.cpp:184] Created Layer ctx_conv3/relu (54)
I0816 00:30:45.093312  9762 net.cpp:561] ctx_conv3/relu <- ctx_conv3
I0816 00:30:45.093315  9762 net.cpp:513] ctx_conv3/relu -> ctx_conv3 (in-place)
I0816 00:30:45.093320  9762 net.cpp:245] Setting up ctx_conv3/relu
I0816 00:30:45.093325  9762 net.cpp:252] TEST Top shape for layer 54 'ctx_conv3/relu' 4 64 80 80 (1638400)
I0816 00:30:45.093329  9762 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I0816 00:30:45.093333  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.093346  9762 net.cpp:184] Created Layer ctx_conv4 (55)
I0816 00:30:45.093350  9762 net.cpp:561] ctx_conv4 <- ctx_conv3
I0816 00:30:45.093354  9762 net.cpp:530] ctx_conv4 -> ctx_conv4
I0816 00:30:45.094545  9762 net.cpp:245] Setting up ctx_conv4
I0816 00:30:45.094553  9762 net.cpp:252] TEST Top shape for layer 55 'ctx_conv4' 4 64 80 80 (1638400)
I0816 00:30:45.094559  9762 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I0816 00:30:45.094564  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.094570  9762 net.cpp:184] Created Layer ctx_conv4/bn (56)
I0816 00:30:45.094574  9762 net.cpp:561] ctx_conv4/bn <- ctx_conv4
I0816 00:30:45.094578  9762 net.cpp:513] ctx_conv4/bn -> ctx_conv4 (in-place)
I0816 00:30:45.095108  9762 net.cpp:245] Setting up ctx_conv4/bn
I0816 00:30:45.095115  9762 net.cpp:252] TEST Top shape for layer 56 'ctx_conv4/bn' 4 64 80 80 (1638400)
I0816 00:30:45.095124  9762 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I0816 00:30:45.095127  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.095132  9762 net.cpp:184] Created Layer ctx_conv4/relu (57)
I0816 00:30:45.095136  9762 net.cpp:561] ctx_conv4/relu <- ctx_conv4
I0816 00:30:45.095141  9762 net.cpp:513] ctx_conv4/relu -> ctx_conv4 (in-place)
I0816 00:30:45.095146  9762 net.cpp:245] Setting up ctx_conv4/relu
I0816 00:30:45.095151  9762 net.cpp:252] TEST Top shape for layer 57 'ctx_conv4/relu' 4 64 80 80 (1638400)
I0816 00:30:45.095155  9762 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I0816 00:30:45.095158  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.095172  9762 net.cpp:184] Created Layer ctx_final (58)
I0816 00:30:45.095176  9762 net.cpp:561] ctx_final <- ctx_conv4
I0816 00:30:45.095181  9762 net.cpp:530] ctx_final -> ctx_final
I0816 00:30:45.100643  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 7.8G, req 0G)
I0816 00:30:45.100654  9762 net.cpp:245] Setting up ctx_final
I0816 00:30:45.100659  9762 net.cpp:252] TEST Top shape for layer 58 'ctx_final' 4 8 80 80 (204800)
I0816 00:30:45.100666  9762 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I0816 00:30:45.100669  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.100673  9762 net.cpp:184] Created Layer ctx_final/relu (59)
I0816 00:30:45.100677  9762 net.cpp:561] ctx_final/relu <- ctx_final
I0816 00:30:45.100682  9762 net.cpp:513] ctx_final/relu -> ctx_final (in-place)
I0816 00:30:45.100687  9762 net.cpp:245] Setting up ctx_final/relu
I0816 00:30:45.100692  9762 net.cpp:252] TEST Top shape for layer 59 'ctx_final/relu' 4 8 80 80 (204800)
I0816 00:30:45.100694  9762 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I0816 00:30:45.100698  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.100705  9762 net.cpp:184] Created Layer out_deconv_final_up2 (60)
I0816 00:30:45.100709  9762 net.cpp:561] out_deconv_final_up2 <- ctx_final
I0816 00:30:45.100713  9762 net.cpp:530] out_deconv_final_up2 -> out_deconv_final_up2
I0816 00:30:45.100898  9762 net.cpp:245] Setting up out_deconv_final_up2
I0816 00:30:45.100905  9762 net.cpp:252] TEST Top shape for layer 60 'out_deconv_final_up2' 4 8 160 160 (819200)
I0816 00:30:45.100916  9762 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I0816 00:30:45.100920  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.100926  9762 net.cpp:184] Created Layer out_deconv_final_up4 (61)
I0816 00:30:45.100929  9762 net.cpp:561] out_deconv_final_up4 <- out_deconv_final_up2
I0816 00:30:45.100934  9762 net.cpp:530] out_deconv_final_up4 -> out_deconv_final_up4
I0816 00:30:45.101096  9762 net.cpp:245] Setting up out_deconv_final_up4
I0816 00:30:45.101104  9762 net.cpp:252] TEST Top shape for layer 61 'out_deconv_final_up4' 4 8 320 320 (3276800)
I0816 00:30:45.101109  9762 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I0816 00:30:45.101111  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.101125  9762 net.cpp:184] Created Layer out_deconv_final_up8 (62)
I0816 00:30:45.101130  9762 net.cpp:561] out_deconv_final_up8 <- out_deconv_final_up4
I0816 00:30:45.101132  9762 net.cpp:530] out_deconv_final_up8 -> out_deconv_final_up8
I0816 00:30:45.101296  9762 net.cpp:245] Setting up out_deconv_final_up8
I0816 00:30:45.101302  9762 net.cpp:252] TEST Top shape for layer 62 'out_deconv_final_up8' 4 8 640 640 (13107200)
I0816 00:30:45.101307  9762 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8_out_deconv_final_up8_0_split' of type 'Split'
I0816 00:30:45.101310  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.101315  9762 net.cpp:184] Created Layer out_deconv_final_up8_out_deconv_final_up8_0_split (63)
I0816 00:30:45.101318  9762 net.cpp:561] out_deconv_final_up8_out_deconv_final_up8_0_split <- out_deconv_final_up8
I0816 00:30:45.101322  9762 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0816 00:30:45.101326  9762 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0816 00:30:45.101330  9762 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0816 00:30:45.101379  9762 net.cpp:245] Setting up out_deconv_final_up8_out_deconv_final_up8_0_split
I0816 00:30:45.101385  9762 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 4 8 640 640 (13107200)
I0816 00:30:45.101389  9762 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 4 8 640 640 (13107200)
I0816 00:30:45.101393  9762 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 4 8 640 640 (13107200)
I0816 00:30:45.101397  9762 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0816 00:30:45.101400  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.101414  9762 net.cpp:184] Created Layer loss (64)
I0816 00:30:45.101418  9762 net.cpp:561] loss <- out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0816 00:30:45.101421  9762 net.cpp:561] loss <- label_data_1_split_0
I0816 00:30:45.101426  9762 net.cpp:530] loss -> loss
I0816 00:30:45.102613  9762 net.cpp:245] Setting up loss
I0816 00:30:45.102623  9762 net.cpp:252] TEST Top shape for layer 64 'loss' (1)
I0816 00:30:45.102627  9762 net.cpp:256]     with loss weight 1
I0816 00:30:45.102632  9762 layer_factory.hpp:136] Creating layer 'accuracy/top1' of type 'Accuracy'
I0816 00:30:45.102636  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.102643  9762 net.cpp:184] Created Layer accuracy/top1 (65)
I0816 00:30:45.102648  9762 net.cpp:561] accuracy/top1 <- out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0816 00:30:45.102651  9762 net.cpp:561] accuracy/top1 <- label_data_1_split_1
I0816 00:30:45.102656  9762 net.cpp:530] accuracy/top1 -> accuracy/top1
I0816 00:30:45.102675  9762 net.cpp:245] Setting up accuracy/top1
I0816 00:30:45.102680  9762 net.cpp:252] TEST Top shape for layer 65 'accuracy/top1' (1)
I0816 00:30:45.102684  9762 layer_factory.hpp:136] Creating layer 'accuracy/top5' of type 'Accuracy'
I0816 00:30:45.102686  9762 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 00:30:45.102691  9762 net.cpp:184] Created Layer accuracy/top5 (66)
I0816 00:30:45.102694  9762 net.cpp:561] accuracy/top5 <- out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0816 00:30:45.102699  9762 net.cpp:561] accuracy/top5 <- label_data_1_split_2
I0816 00:30:45.102701  9762 net.cpp:530] accuracy/top5 -> accuracy/top5
I0816 00:30:45.102706  9762 net.cpp:245] Setting up accuracy/top5
I0816 00:30:45.102710  9762 net.cpp:252] TEST Top shape for layer 66 'accuracy/top5' (1)
I0816 00:30:45.102715  9762 net.cpp:325] accuracy/top5 does not need backward computation.
I0816 00:30:45.102718  9762 net.cpp:325] accuracy/top1 does not need backward computation.
I0816 00:30:45.102721  9762 net.cpp:323] loss needs backward computation.
I0816 00:30:45.102726  9762 net.cpp:323] out_deconv_final_up8_out_deconv_final_up8_0_split needs backward computation.
I0816 00:30:45.102730  9762 net.cpp:323] out_deconv_final_up8 needs backward computation.
I0816 00:30:45.102732  9762 net.cpp:323] out_deconv_final_up4 needs backward computation.
I0816 00:30:45.102736  9762 net.cpp:323] out_deconv_final_up2 needs backward computation.
I0816 00:30:45.102741  9762 net.cpp:323] ctx_final/relu needs backward computation.
I0816 00:30:45.102743  9762 net.cpp:323] ctx_final needs backward computation.
I0816 00:30:45.102747  9762 net.cpp:323] ctx_conv4/relu needs backward computation.
I0816 00:30:45.102751  9762 net.cpp:323] ctx_conv4/bn needs backward computation.
I0816 00:30:45.102753  9762 net.cpp:323] ctx_conv4 needs backward computation.
I0816 00:30:45.102757  9762 net.cpp:323] ctx_conv3/relu needs backward computation.
I0816 00:30:45.102761  9762 net.cpp:323] ctx_conv3/bn needs backward computation.
I0816 00:30:45.102764  9762 net.cpp:323] ctx_conv3 needs backward computation.
I0816 00:30:45.102767  9762 net.cpp:323] ctx_conv2/relu needs backward computation.
I0816 00:30:45.102771  9762 net.cpp:323] ctx_conv2/bn needs backward computation.
I0816 00:30:45.102776  9762 net.cpp:323] ctx_conv2 needs backward computation.
I0816 00:30:45.102778  9762 net.cpp:323] ctx_conv1/relu needs backward computation.
I0816 00:30:45.102782  9762 net.cpp:323] ctx_conv1/bn needs backward computation.
I0816 00:30:45.102785  9762 net.cpp:323] ctx_conv1 needs backward computation.
I0816 00:30:45.102789  9762 net.cpp:323] out3_out5_combined needs backward computation.
I0816 00:30:45.102793  9762 net.cpp:323] out3a/relu needs backward computation.
I0816 00:30:45.102797  9762 net.cpp:323] out3a/bn needs backward computation.
I0816 00:30:45.102802  9762 net.cpp:323] out3a needs backward computation.
I0816 00:30:45.102805  9762 net.cpp:323] out5a_up2 needs backward computation.
I0816 00:30:45.102808  9762 net.cpp:323] out5a/relu needs backward computation.
I0816 00:30:45.102813  9762 net.cpp:323] out5a/bn needs backward computation.
I0816 00:30:45.102816  9762 net.cpp:323] out5a needs backward computation.
I0816 00:30:45.102825  9762 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0816 00:30:45.102829  9762 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0816 00:30:45.102833  9762 net.cpp:323] res5a_branch2b needs backward computation.
I0816 00:30:45.102836  9762 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0816 00:30:45.102840  9762 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0816 00:30:45.102844  9762 net.cpp:323] res5a_branch2a needs backward computation.
I0816 00:30:45.102847  9762 net.cpp:323] pool4 needs backward computation.
I0816 00:30:45.102852  9762 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0816 00:30:45.102855  9762 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0816 00:30:45.102865  9762 net.cpp:323] res4a_branch2b needs backward computation.
I0816 00:30:45.102869  9762 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0816 00:30:45.102874  9762 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0816 00:30:45.102877  9762 net.cpp:323] res4a_branch2a needs backward computation.
I0816 00:30:45.102881  9762 net.cpp:323] pool3 needs backward computation.
I0816 00:30:45.102885  9762 net.cpp:323] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I0816 00:30:45.102890  9762 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0816 00:30:45.102893  9762 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0816 00:30:45.102897  9762 net.cpp:323] res3a_branch2b needs backward computation.
I0816 00:30:45.102900  9762 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0816 00:30:45.102905  9762 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0816 00:30:45.102908  9762 net.cpp:323] res3a_branch2a needs backward computation.
I0816 00:30:45.102912  9762 net.cpp:323] pool2 needs backward computation.
I0816 00:30:45.102916  9762 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0816 00:30:45.102921  9762 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0816 00:30:45.102923  9762 net.cpp:323] res2a_branch2b needs backward computation.
I0816 00:30:45.102927  9762 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0816 00:30:45.102931  9762 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0816 00:30:45.102936  9762 net.cpp:323] res2a_branch2a needs backward computation.
I0816 00:30:45.102939  9762 net.cpp:323] pool1 needs backward computation.
I0816 00:30:45.102942  9762 net.cpp:323] conv1b/relu needs backward computation.
I0816 00:30:45.102946  9762 net.cpp:323] conv1b/bn needs backward computation.
I0816 00:30:45.102951  9762 net.cpp:323] conv1b needs backward computation.
I0816 00:30:45.102954  9762 net.cpp:323] conv1a/relu needs backward computation.
I0816 00:30:45.102958  9762 net.cpp:323] conv1a/bn needs backward computation.
I0816 00:30:45.102962  9762 net.cpp:323] conv1a needs backward computation.
I0816 00:30:45.102967  9762 net.cpp:325] data/bias does not need backward computation.
I0816 00:30:45.102977  9762 net.cpp:325] label_data_1_split does not need backward computation.
I0816 00:30:45.102987  9762 net.cpp:325] data does not need backward computation.
I0816 00:30:45.102989  9762 net.cpp:367] This network produces output accuracy/top1
I0816 00:30:45.102993  9762 net.cpp:367] This network produces output accuracy/top5
I0816 00:30:45.102998  9762 net.cpp:367] This network produces output loss
I0816 00:30:45.103054  9762 net.cpp:389] Top memory (TEST) required for data: 637337600 diff: 8
I0816 00:30:45.103058  9762 net.cpp:392] Bottom memory (TEST) required for data: 637337600 diff: 637337600
I0816 00:30:45.103062  9762 net.cpp:395] Shared (in-place) memory (TEST) by data: 420249600 diff: 420249600
I0816 00:30:45.103065  9762 net.cpp:398] Parameters memory (TEST) required for data: 2692608 diff: 2692608
I0816 00:30:45.103070  9762 net.cpp:401] Parameters shared memory (TEST) by data: 0 diff: 0
I0816 00:30:45.103073  9762 net.cpp:407] Network initialization done.
I0816 00:30:45.108525  9762 net.cpp:1095] Copying source layer data Type:ImageLabelData #blobs=0
I0816 00:30:45.108547  9762 net.cpp:1095] Copying source layer data/bias Type:Bias #blobs=1
I0816 00:30:45.108584  9762 net.cpp:1095] Copying source layer conv1a Type:Convolution #blobs=2
I0816 00:30:45.108599  9762 net.cpp:1095] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0816 00:30:45.108907  9762 net.cpp:1095] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0816 00:30:45.108914  9762 net.cpp:1095] Copying source layer conv1b Type:Convolution #blobs=2
I0816 00:30:45.108927  9762 net.cpp:1095] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0816 00:30:45.109166  9762 net.cpp:1095] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0816 00:30:45.109174  9762 net.cpp:1095] Copying source layer pool1 Type:Pooling #blobs=0
I0816 00:30:45.109189  9762 net.cpp:1095] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0816 00:30:45.109210  9762 net.cpp:1095] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0816 00:30:45.109447  9762 net.cpp:1095] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0816 00:30:45.109452  9762 net.cpp:1095] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0816 00:30:45.109467  9762 net.cpp:1095] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0816 00:30:45.109697  9762 net.cpp:1095] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0816 00:30:45.109704  9762 net.cpp:1095] Copying source layer pool2 Type:Pooling #blobs=0
I0816 00:30:45.109707  9762 net.cpp:1095] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0816 00:30:45.109747  9762 net.cpp:1095] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0816 00:30:45.109961  9762 net.cpp:1095] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0816 00:30:45.109968  9762 net.cpp:1095] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0816 00:30:45.109997  9762 net.cpp:1095] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0816 00:30:45.110224  9762 net.cpp:1095] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0816 00:30:45.110230  9762 net.cpp:1095] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I0816 00:30:45.110234  9762 net.cpp:1095] Copying source layer pool3 Type:Pooling #blobs=0
I0816 00:30:45.110237  9762 net.cpp:1095] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0816 00:30:45.110360  9762 net.cpp:1095] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0816 00:30:45.110571  9762 net.cpp:1095] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0816 00:30:45.110579  9762 net.cpp:1095] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0816 00:30:45.110643  9762 net.cpp:1095] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0816 00:30:45.110837  9762 net.cpp:1095] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0816 00:30:45.110843  9762 net.cpp:1095] Copying source layer pool4 Type:Pooling #blobs=0
I0816 00:30:45.110847  9762 net.cpp:1095] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0816 00:30:45.111238  9762 net.cpp:1095] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0816 00:30:45.111448  9762 net.cpp:1095] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0816 00:30:45.111454  9762 net.cpp:1095] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0816 00:30:45.111620  9762 net.cpp:1095] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0816 00:30:45.111825  9762 net.cpp:1095] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0816 00:30:45.111832  9762 net.cpp:1095] Copying source layer out5a Type:Convolution #blobs=2
I0816 00:30:45.111886  9762 net.cpp:1095] Copying source layer out5a/bn Type:BatchNorm #blobs=5
I0816 00:30:45.111995  9762 net.cpp:1095] Copying source layer out5a/relu Type:ReLU #blobs=0
I0816 00:30:45.112001  9762 net.cpp:1095] Copying source layer out5a_up2 Type:Deconvolution #blobs=1
I0816 00:30:45.112009  9762 net.cpp:1095] Copying source layer out3a Type:Convolution #blobs=2
I0816 00:30:45.112028  9762 net.cpp:1095] Copying source layer out3a/bn Type:BatchNorm #blobs=5
I0816 00:30:45.112150  9762 net.cpp:1095] Copying source layer out3a/relu Type:ReLU #blobs=0
I0816 00:30:45.112157  9762 net.cpp:1095] Copying source layer out3_out5_combined Type:Eltwise #blobs=0
I0816 00:30:45.112161  9762 net.cpp:1095] Copying source layer ctx_conv1 Type:Convolution #blobs=2
I0816 00:30:45.112195  9762 net.cpp:1095] Copying source layer ctx_conv1/bn Type:BatchNorm #blobs=5
I0816 00:30:45.112306  9762 net.cpp:1095] Copying source layer ctx_conv1/relu Type:ReLU #blobs=0
I0816 00:30:45.112313  9762 net.cpp:1095] Copying source layer ctx_conv2 Type:Convolution #blobs=2
I0816 00:30:45.112340  9762 net.cpp:1095] Copying source layer ctx_conv2/bn Type:BatchNorm #blobs=5
I0816 00:30:45.112455  9762 net.cpp:1095] Copying source layer ctx_conv2/relu Type:ReLU #blobs=0
I0816 00:30:45.112462  9762 net.cpp:1095] Copying source layer ctx_conv3 Type:Convolution #blobs=2
I0816 00:30:45.112484  9762 net.cpp:1095] Copying source layer ctx_conv3/bn Type:BatchNorm #blobs=5
I0816 00:30:45.112601  9762 net.cpp:1095] Copying source layer ctx_conv3/relu Type:ReLU #blobs=0
I0816 00:30:45.112607  9762 net.cpp:1095] Copying source layer ctx_conv4 Type:Convolution #blobs=2
I0816 00:30:45.112627  9762 net.cpp:1095] Copying source layer ctx_conv4/bn Type:BatchNorm #blobs=5
I0816 00:30:45.112736  9762 net.cpp:1095] Copying source layer ctx_conv4/relu Type:ReLU #blobs=0
I0816 00:30:45.112742  9762 net.cpp:1095] Copying source layer ctx_final Type:Convolution #blobs=2
I0816 00:30:45.112754  9762 net.cpp:1095] Copying source layer ctx_final/relu Type:ReLU #blobs=0
I0816 00:30:45.112758  9762 net.cpp:1095] Copying source layer out_deconv_final_up2 Type:Deconvolution #blobs=1
I0816 00:30:45.112772  9762 net.cpp:1095] Copying source layer out_deconv_final_up4 Type:Deconvolution #blobs=1
I0816 00:30:45.112779  9762 net.cpp:1095] Copying source layer out_deconv_final_up8 Type:Deconvolution #blobs=1
I0816 00:30:45.112787  9762 net.cpp:1095] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0816 00:30:45.112874  9762 caffe.cpp:290] Running for 50 iterations.
I0816 00:30:45.118573  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.72G, req 0G)
I0816 00:30:45.138870  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.62G, req 0G)
I0816 00:30:45.155766  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.5G, req 0G)
I0816 00:30:45.169842  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.44G, req 0G)
I0816 00:30:45.177834  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.38G, req 0G)
I0816 00:30:45.183218  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.35G, req 0G)
I0816 00:30:45.190201  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.33G, req 0G)
I0816 00:30:45.193990  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.32G, req 0G)
I0816 00:30:45.218603  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 7.09G, req 0G)
I0816 00:30:45.223830  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 7.07G, req 0G)
I0816 00:30:45.240011  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.94G, req 0G)
I0816 00:30:45.396050  9762 caffe.cpp:313] Batch 0, accuracy/top1 = 0.930066
I0816 00:30:45.396071  9762 caffe.cpp:313] Batch 0, accuracy/top5 = 1
I0816 00:30:45.396075  9762 caffe.cpp:313] Batch 0, loss = 0.220756
I0816 00:30:45.402719  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 1.22G/1 1  (limit 5.49G, req 0G)
I0816 00:30:45.426745  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 2.44G/2 6  (limit 4.27G, req 0G)
I0816 00:30:45.472440  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 2.44G/1 6  (limit 4.27G, req 0G)
I0816 00:30:45.488504  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 2.44G/2 6  (limit 4.27G, req 0G)
I0816 00:30:45.524411  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 2.44G/1 6  (limit 4.27G, req 0G)
I0816 00:30:45.533339  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 2.44G/2 6  (limit 4.27G, req 0G)
I0816 00:30:45.554831  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 2.44G/1 6  (limit 4.27G, req 0G)
I0816 00:30:45.560751  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 2.44G/2 6  (limit 4.27G, req 0G)
I0816 00:30:45.585239  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'out3a' with space 2.44G/2 6  (limit 4.27G, req 0G)
I0816 00:30:45.605265  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_conv1' with space 2.44G/1 6  (limit 4.27G, req 0G)
I0816 00:30:45.617367  9762 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_final' with space 2.44G/1 6  (limit 4.27G, req 0G)
I0816 00:30:45.751566  9762 caffe.cpp:313] Batch 1, accuracy/top1 = 0.955384
I0816 00:30:45.751590  9762 caffe.cpp:313] Batch 1, accuracy/top5 = 1
I0816 00:30:45.751592  9762 caffe.cpp:313] Batch 1, loss = 0.141199
I0816 00:30:45.915011  9762 caffe.cpp:313] Batch 2, accuracy/top1 = 0.959618
I0816 00:30:45.915033  9762 caffe.cpp:313] Batch 2, accuracy/top5 = 1
I0816 00:30:45.915036  9762 caffe.cpp:313] Batch 2, loss = 0.112921
I0816 00:30:46.080250  9762 caffe.cpp:313] Batch 3, accuracy/top1 = 0.972609
I0816 00:30:46.080269  9762 caffe.cpp:313] Batch 3, accuracy/top5 = 0.999996
I0816 00:30:46.080272  9762 caffe.cpp:313] Batch 3, loss = 0.0760811
I0816 00:30:46.243175  9762 caffe.cpp:313] Batch 4, accuracy/top1 = 0.962518
I0816 00:30:46.243196  9762 caffe.cpp:313] Batch 4, accuracy/top5 = 0.999993
I0816 00:30:46.243199  9762 caffe.cpp:313] Batch 4, loss = 0.120001
I0816 00:30:46.407047  9762 caffe.cpp:313] Batch 5, accuracy/top1 = 0.813128
I0816 00:30:46.407068  9762 caffe.cpp:313] Batch 5, accuracy/top5 = 1
I0816 00:30:46.407071  9762 caffe.cpp:313] Batch 5, loss = 1.02897
I0816 00:30:46.572259  9762 caffe.cpp:313] Batch 6, accuracy/top1 = 0.962231
I0816 00:30:46.572278  9762 caffe.cpp:313] Batch 6, accuracy/top5 = 1
I0816 00:30:46.572280  9762 caffe.cpp:313] Batch 6, loss = 0.0979562
I0816 00:30:46.738240  9762 caffe.cpp:313] Batch 7, accuracy/top1 = 0.962217
I0816 00:30:46.738261  9762 caffe.cpp:313] Batch 7, accuracy/top5 = 1
I0816 00:30:46.738265  9762 caffe.cpp:313] Batch 7, loss = 0.0962906
I0816 00:30:46.900099  9762 caffe.cpp:313] Batch 8, accuracy/top1 = 0.974926
I0816 00:30:46.900121  9762 caffe.cpp:313] Batch 8, accuracy/top5 = 1
I0816 00:30:46.900125  9762 caffe.cpp:313] Batch 8, loss = 0.0665642
I0816 00:30:47.065760  9762 caffe.cpp:313] Batch 9, accuracy/top1 = 0.982213
I0816 00:30:47.065778  9762 caffe.cpp:313] Batch 9, accuracy/top5 = 1
I0816 00:30:47.065781  9762 caffe.cpp:313] Batch 9, loss = 0.0482233
I0816 00:30:47.231524  9762 caffe.cpp:313] Batch 10, accuracy/top1 = 0.895045
I0816 00:30:47.231549  9762 caffe.cpp:313] Batch 10, accuracy/top5 = 1
I0816 00:30:47.231552  9762 caffe.cpp:313] Batch 10, loss = 0.286693
I0816 00:30:47.399586  9762 caffe.cpp:313] Batch 11, accuracy/top1 = 0.97725
I0816 00:30:47.399605  9762 caffe.cpp:313] Batch 11, accuracy/top5 = 1
I0816 00:30:47.399608  9762 caffe.cpp:313] Batch 11, loss = 0.0629691
I0816 00:30:47.565316  9762 caffe.cpp:313] Batch 12, accuracy/top1 = 0.965007
I0816 00:30:47.565335  9762 caffe.cpp:313] Batch 12, accuracy/top5 = 1
I0816 00:30:47.565337  9762 caffe.cpp:313] Batch 12, loss = 0.0938991
I0816 00:30:47.729823  9762 caffe.cpp:313] Batch 13, accuracy/top1 = 0.979467
I0816 00:30:47.729843  9762 caffe.cpp:313] Batch 13, accuracy/top5 = 1
I0816 00:30:47.729846  9762 caffe.cpp:313] Batch 13, loss = 0.0551973
I0816 00:30:47.894198  9762 caffe.cpp:313] Batch 14, accuracy/top1 = 0.977653
I0816 00:30:47.894222  9762 caffe.cpp:313] Batch 14, accuracy/top5 = 1
I0816 00:30:47.894224  9762 caffe.cpp:313] Batch 14, loss = 0.0572141
I0816 00:30:48.059252  9762 caffe.cpp:313] Batch 15, accuracy/top1 = 0.962654
I0816 00:30:48.059270  9762 caffe.cpp:313] Batch 15, accuracy/top5 = 1
I0816 00:30:48.059274  9762 caffe.cpp:313] Batch 15, loss = 0.10314
I0816 00:30:48.223881  9762 caffe.cpp:313] Batch 16, accuracy/top1 = 0.899707
I0816 00:30:48.223902  9762 caffe.cpp:313] Batch 16, accuracy/top5 = 1
I0816 00:30:48.223906  9762 caffe.cpp:313] Batch 16, loss = 0.388575
I0816 00:30:48.387871  9762 caffe.cpp:313] Batch 17, accuracy/top1 = 0.87246
I0816 00:30:48.387892  9762 caffe.cpp:313] Batch 17, accuracy/top5 = 1
I0816 00:30:48.387907  9762 caffe.cpp:313] Batch 17, loss = 0.601822
I0816 00:30:48.554101  9762 caffe.cpp:313] Batch 18, accuracy/top1 = 0.982779
I0816 00:30:48.554119  9762 caffe.cpp:313] Batch 18, accuracy/top5 = 0.99999
I0816 00:30:48.554122  9762 caffe.cpp:313] Batch 18, loss = 0.0439325
I0816 00:30:48.719720  9762 caffe.cpp:313] Batch 19, accuracy/top1 = 0.982094
I0816 00:30:48.719743  9762 caffe.cpp:313] Batch 19, accuracy/top5 = 1
I0816 00:30:48.719745  9762 caffe.cpp:313] Batch 19, loss = 0.051014
I0816 00:30:48.885210  9762 caffe.cpp:313] Batch 20, accuracy/top1 = 0.975827
I0816 00:30:48.885231  9762 caffe.cpp:313] Batch 20, accuracy/top5 = 1
I0816 00:30:48.885233  9762 caffe.cpp:313] Batch 20, loss = 0.0692319
I0816 00:30:49.048382  9762 caffe.cpp:313] Batch 21, accuracy/top1 = 0.891715
I0816 00:30:49.048399  9762 caffe.cpp:313] Batch 21, accuracy/top5 = 0.9999
I0816 00:30:49.048403  9762 caffe.cpp:313] Batch 21, loss = 0.604256
I0816 00:30:49.213877  9762 caffe.cpp:313] Batch 22, accuracy/top1 = 0.967383
I0816 00:30:49.213901  9762 caffe.cpp:313] Batch 22, accuracy/top5 = 1
I0816 00:30:49.213904  9762 caffe.cpp:313] Batch 22, loss = 0.088235
I0816 00:30:49.379048  9762 caffe.cpp:313] Batch 23, accuracy/top1 = 0.978001
I0816 00:30:49.379065  9762 caffe.cpp:313] Batch 23, accuracy/top5 = 1
I0816 00:30:49.379068  9762 caffe.cpp:313] Batch 23, loss = 0.0590779
I0816 00:30:49.545467  9762 caffe.cpp:313] Batch 24, accuracy/top1 = 0.950794
I0816 00:30:49.545485  9762 caffe.cpp:313] Batch 24, accuracy/top5 = 1
I0816 00:30:49.545488  9762 caffe.cpp:313] Batch 24, loss = 0.128335
I0816 00:30:49.713721  9762 caffe.cpp:313] Batch 25, accuracy/top1 = 0.97273
I0816 00:30:49.713744  9762 caffe.cpp:313] Batch 25, accuracy/top5 = 1
I0816 00:30:49.713747  9762 caffe.cpp:313] Batch 25, loss = 0.074232
I0816 00:30:49.876585  9762 caffe.cpp:313] Batch 26, accuracy/top1 = 0.952759
I0816 00:30:49.876607  9762 caffe.cpp:313] Batch 26, accuracy/top5 = 1
I0816 00:30:49.876610  9762 caffe.cpp:313] Batch 26, loss = 0.118792
I0816 00:30:50.041847  9762 caffe.cpp:313] Batch 27, accuracy/top1 = 0.966694
I0816 00:30:50.041867  9762 caffe.cpp:313] Batch 27, accuracy/top5 = 1
I0816 00:30:50.041869  9762 caffe.cpp:313] Batch 27, loss = 0.0944889
I0816 00:30:50.206145  9762 caffe.cpp:313] Batch 28, accuracy/top1 = 0.953026
I0816 00:30:50.206167  9762 caffe.cpp:313] Batch 28, accuracy/top5 = 1
I0816 00:30:50.206171  9762 caffe.cpp:313] Batch 28, loss = 0.125202
I0816 00:30:50.371675  9762 caffe.cpp:313] Batch 29, accuracy/top1 = 0.965533
I0816 00:30:50.371698  9762 caffe.cpp:313] Batch 29, accuracy/top5 = 1
I0816 00:30:50.371701  9762 caffe.cpp:313] Batch 29, loss = 0.104905
I0816 00:30:50.536644  9762 caffe.cpp:313] Batch 30, accuracy/top1 = 0.861784
I0816 00:30:50.536665  9762 caffe.cpp:313] Batch 30, accuracy/top5 = 1
I0816 00:30:50.536669  9762 caffe.cpp:313] Batch 30, loss = 0.644569
I0816 00:30:50.700757  9762 caffe.cpp:313] Batch 31, accuracy/top1 = 0.968184
I0816 00:30:50.700778  9762 caffe.cpp:313] Batch 31, accuracy/top5 = 1
I0816 00:30:50.700781  9762 caffe.cpp:313] Batch 31, loss = 0.0884397
I0816 00:30:50.864923  9762 caffe.cpp:313] Batch 32, accuracy/top1 = 0.950449
I0816 00:30:50.864944  9762 caffe.cpp:313] Batch 32, accuracy/top5 = 1
I0816 00:30:50.864948  9762 caffe.cpp:313] Batch 32, loss = 0.134578
I0816 00:30:51.030355  9762 caffe.cpp:313] Batch 33, accuracy/top1 = 0.967974
I0816 00:30:51.030374  9762 caffe.cpp:313] Batch 33, accuracy/top5 = 1
I0816 00:30:51.030376  9762 caffe.cpp:313] Batch 33, loss = 0.0850275
I0816 00:30:51.197466  9762 caffe.cpp:313] Batch 34, accuracy/top1 = 0.97725
I0816 00:30:51.197491  9762 caffe.cpp:313] Batch 34, accuracy/top5 = 1
I0816 00:30:51.197494  9762 caffe.cpp:313] Batch 34, loss = 0.0645503
I0816 00:30:51.360913  9762 caffe.cpp:313] Batch 35, accuracy/top1 = 0.975526
I0816 00:30:51.360935  9762 caffe.cpp:313] Batch 35, accuracy/top5 = 1
I0816 00:30:51.360939  9762 caffe.cpp:313] Batch 35, loss = 0.0668076
I0816 00:30:51.525363  9762 caffe.cpp:313] Batch 36, accuracy/top1 = 0.963138
I0816 00:30:51.525393  9762 caffe.cpp:313] Batch 36, accuracy/top5 = 1
I0816 00:30:51.525398  9762 caffe.cpp:313] Batch 36, loss = 0.104859
I0816 00:30:51.688918  9762 caffe.cpp:313] Batch 37, accuracy/top1 = 0.962077
I0816 00:30:51.688940  9762 caffe.cpp:313] Batch 37, accuracy/top5 = 1
I0816 00:30:51.688942  9762 caffe.cpp:313] Batch 37, loss = 0.113325
I0816 00:30:51.853919  9762 caffe.cpp:313] Batch 38, accuracy/top1 = 0.941712
I0816 00:30:51.853942  9762 caffe.cpp:313] Batch 38, accuracy/top5 = 1
I0816 00:30:51.853945  9762 caffe.cpp:313] Batch 38, loss = 0.174112
I0816 00:30:52.015498  9762 caffe.cpp:313] Batch 39, accuracy/top1 = 0.916295
I0816 00:30:52.015521  9762 caffe.cpp:313] Batch 39, accuracy/top5 = 1
I0816 00:30:52.015523  9762 caffe.cpp:313] Batch 39, loss = 0.230812
I0816 00:30:52.182013  9762 caffe.cpp:313] Batch 40, accuracy/top1 = 0.980853
I0816 00:30:52.182041  9762 caffe.cpp:313] Batch 40, accuracy/top5 = 1
I0816 00:30:52.182045  9762 caffe.cpp:313] Batch 40, loss = 0.0588717
I0816 00:30:52.347806  9762 caffe.cpp:313] Batch 41, accuracy/top1 = 0.976998
I0816 00:30:52.347829  9762 caffe.cpp:313] Batch 41, accuracy/top5 = 1
I0816 00:30:52.347832  9762 caffe.cpp:313] Batch 41, loss = 0.0659184
I0816 00:30:52.514281  9762 caffe.cpp:313] Batch 42, accuracy/top1 = 0.972377
I0816 00:30:52.514299  9762 caffe.cpp:313] Batch 42, accuracy/top5 = 1
I0816 00:30:52.514302  9762 caffe.cpp:313] Batch 42, loss = 0.0748789
I0816 00:30:52.680268  9762 caffe.cpp:313] Batch 43, accuracy/top1 = 0.977115
I0816 00:30:52.680289  9762 caffe.cpp:313] Batch 43, accuracy/top5 = 1
I0816 00:30:52.680291  9762 caffe.cpp:313] Batch 43, loss = 0.0663094
I0816 00:30:52.846160  9762 caffe.cpp:313] Batch 44, accuracy/top1 = 0.958332
I0816 00:30:52.846184  9762 caffe.cpp:313] Batch 44, accuracy/top5 = 1
I0816 00:30:52.846186  9762 caffe.cpp:313] Batch 44, loss = 0.11734
I0816 00:30:53.011114  9762 caffe.cpp:313] Batch 45, accuracy/top1 = 0.976915
I0816 00:30:53.011137  9762 caffe.cpp:313] Batch 45, accuracy/top5 = 1
I0816 00:30:53.011139  9762 caffe.cpp:313] Batch 45, loss = 0.0723509
I0816 00:30:53.177412  9762 caffe.cpp:313] Batch 46, accuracy/top1 = 0.972087
I0816 00:30:53.177431  9762 caffe.cpp:313] Batch 46, accuracy/top5 = 1
I0816 00:30:53.177434  9762 caffe.cpp:313] Batch 46, loss = 0.0759381
I0816 00:30:53.344467  9762 caffe.cpp:313] Batch 47, accuracy/top1 = 0.967969
I0816 00:30:53.344491  9762 caffe.cpp:313] Batch 47, accuracy/top5 = 1
I0816 00:30:53.344493  9762 caffe.cpp:313] Batch 47, loss = 0.118059
I0816 00:30:53.508762  9762 caffe.cpp:313] Batch 48, accuracy/top1 = 0.876619
I0816 00:30:53.508780  9762 caffe.cpp:313] Batch 48, accuracy/top5 = 1
I0816 00:30:53.508782  9762 caffe.cpp:313] Batch 48, loss = 0.465836
I0816 00:30:53.670256  9762 caffe.cpp:313] Batch 49, accuracy/top1 = 0.950034
I0816 00:30:53.670277  9762 caffe.cpp:313] Batch 49, accuracy/top5 = 1
I0816 00:30:53.670280  9762 caffe.cpp:313] Batch 49, loss = 0.135004
I0816 00:30:53.670284  9762 caffe.cpp:318] Loss: 0.163555
I0816 00:30:53.670290  9762 caffe.cpp:330] accuracy/top1 = 0.952743
I0816 00:30:53.670294  9762 caffe.cpp:330] accuracy/top5 = 0.999998
I0816 00:30:53.670298  9762 caffe.cpp:330] loss = 0.163555 (* 1 = 0.163555 loss)
