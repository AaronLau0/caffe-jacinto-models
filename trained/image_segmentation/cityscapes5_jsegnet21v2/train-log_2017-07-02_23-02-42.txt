Logging output to training/cityscapes5_jsegnet21v2_2017-07-02_23-02-42/train-log_2017-07-02_23-02-42.txt
Using pretrained model training/imagenet_jacintonet11_v2_bn_iter_160000.caffemodel
training/cityscapes5_jsegnet21v2_2017-07-02_23-02-42/initial
I0702 23:02:42.982724 13779 caffe.cpp:209] Using GPUs 0, 1
I0702 23:02:42.985044 13779 caffe.cpp:214] GPU 0: GeForce GTX 1080
I0702 23:02:42.985390 13779 caffe.cpp:214] GPU 1: GeForce GTX 1080
I0702 23:02:43.375926 13779 solver.cpp:48] Initializing solver from parameters: 
train_net: "training/cityscapes5_jsegnet21v2_2017-07-02_23-02-42/initial/train.prototxt"
test_net: "training/cityscapes5_jsegnet21v2_2017-07-02_23-02-42/initial/test.prototxt"
test_iter: 125
test_interval: 2000
base_lr: 0.0001
display: 100
max_iter: 32000
lr_policy: "multistep"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 0.0001
snapshot: 10000
snapshot_prefix: "training/cityscapes5_jsegnet21v2_2017-07-02_23-02-42/initial/cityscapes5_jsegnet21v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
test_initialization: false
stepvalue: 24000
iter_size: 1
type: "Adam"
I0702 23:02:43.376019 13779 solver.cpp:82] Creating training net from train_net file: training/cityscapes5_jsegnet21v2_2017-07-02_23-02-42/initial/train.prototxt
I0702 23:02:43.376726 13779 net.cpp:327] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0702 23:02:43.376732 13779 net.cpp:327] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
W0702 23:02:43.376744 13779 parallel.cpp:400] Batch size must be divisible by the number of solvers (GPUs)
I0702 23:02:43.377065 13779 net.cpp:56] Initializing net from parameters: 
name: "jsegnet21v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/train-image-lmdb"
    label_list_path: "data/train-label-lmdb"
    batch_size: 8
    threads: 4
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b/bn"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a/bn"
  top: "out5a/bn"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a/bn"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b/bn"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a/bn"
  top: "out3a/bn"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a/bn"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv1/bn"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv2/bn"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv3/bn"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4/bn"
  top: "ctx_conv4/bn"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4/bn"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
I0702 23:02:43.377203 13779 layer_factory.hpp:77] Creating layer data
I0702 23:02:43.377215 13779 net.cpp:98] Creating Layer data
I0702 23:02:43.377220 13779 net.cpp:413] data -> data
I0702 23:02:43.377238 13779 net.cpp:413] data -> label
I0702 23:02:43.378918 13794 db_lmdb.cpp:35] Opened lmdb data/train-image-lmdb
I0702 23:02:43.378918 13799 db_lmdb.cpp:35] Opened lmdb data/train-label-lmdb
I0702 23:02:43.382324 13779 data_layer.cpp:78] ReshapePrefetch 8, 3, 640, 640
I0702 23:02:43.382370 13779 data_layer.cpp:83] output data size: 8,3,640,640
I0702 23:02:43.422585 13779 data_layer.cpp:78] ReshapePrefetch 8, 1, 640, 640
I0702 23:02:43.422629 13779 data_layer.cpp:83] output data size: 8,1,640,640
I0702 23:02:43.431246 13815 blocking_queue.cpp:50] Waiting for data
I0702 23:02:43.440179 13779 net.cpp:148] Setting up data
I0702 23:02:43.440198 13779 net.cpp:155] Top shape: 8 3 640 640 (9830400)
I0702 23:02:43.440201 13779 net.cpp:155] Top shape: 8 1 640 640 (3276800)
I0702 23:02:43.440203 13779 net.cpp:163] Memory required for data: 52428800
I0702 23:02:43.440212 13779 layer_factory.hpp:77] Creating layer data/bias
I0702 23:02:43.440219 13779 net.cpp:98] Creating Layer data/bias
I0702 23:02:43.440223 13779 net.cpp:439] data/bias <- data
I0702 23:02:43.440230 13779 net.cpp:413] data/bias -> data/bias
I0702 23:02:43.441210 13779 net.cpp:148] Setting up data/bias
I0702 23:02:43.441218 13779 net.cpp:155] Top shape: 8 3 640 640 (9830400)
I0702 23:02:43.441220 13779 net.cpp:163] Memory required for data: 91750400
I0702 23:02:43.441229 13779 layer_factory.hpp:77] Creating layer conv1a
I0702 23:02:43.441239 13779 net.cpp:98] Creating Layer conv1a
I0702 23:02:43.441242 13779 net.cpp:439] conv1a <- data/bias
I0702 23:02:43.441246 13779 net.cpp:413] conv1a -> conv1a
I0702 23:02:43.443279 13779 net.cpp:148] Setting up conv1a
I0702 23:02:43.443294 13779 net.cpp:155] Top shape: 8 32 320 320 (26214400)
I0702 23:02:43.443296 13779 net.cpp:163] Memory required for data: 196608000
I0702 23:02:43.443302 13779 layer_factory.hpp:77] Creating layer conv1a/bn
I0702 23:02:43.443308 13779 net.cpp:98] Creating Layer conv1a/bn
I0702 23:02:43.443310 13779 net.cpp:439] conv1a/bn <- conv1a
I0702 23:02:43.443315 13779 net.cpp:413] conv1a/bn -> conv1a/bn
I0702 23:02:43.444597 13779 net.cpp:148] Setting up conv1a/bn
I0702 23:02:43.444607 13779 net.cpp:155] Top shape: 8 32 320 320 (26214400)
I0702 23:02:43.444609 13779 net.cpp:163] Memory required for data: 301465600
I0702 23:02:43.444615 13779 layer_factory.hpp:77] Creating layer conv1a/relu
I0702 23:02:43.444620 13779 net.cpp:98] Creating Layer conv1a/relu
I0702 23:02:43.444622 13779 net.cpp:439] conv1a/relu <- conv1a/bn
I0702 23:02:43.444625 13779 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0702 23:02:43.444638 13779 net.cpp:148] Setting up conv1a/relu
I0702 23:02:43.444640 13779 net.cpp:155] Top shape: 8 32 320 320 (26214400)
I0702 23:02:43.444643 13779 net.cpp:163] Memory required for data: 406323200
I0702 23:02:43.444645 13779 layer_factory.hpp:77] Creating layer conv1b
I0702 23:02:43.444651 13779 net.cpp:98] Creating Layer conv1b
I0702 23:02:43.444653 13779 net.cpp:439] conv1b <- conv1a/bn
I0702 23:02:43.444656 13779 net.cpp:413] conv1b -> conv1b
I0702 23:02:43.444949 13779 net.cpp:148] Setting up conv1b
I0702 23:02:43.444955 13779 net.cpp:155] Top shape: 8 32 320 320 (26214400)
I0702 23:02:43.444957 13779 net.cpp:163] Memory required for data: 511180800
I0702 23:02:43.444962 13779 layer_factory.hpp:77] Creating layer conv1b/bn
I0702 23:02:43.444965 13779 net.cpp:98] Creating Layer conv1b/bn
I0702 23:02:43.444967 13779 net.cpp:439] conv1b/bn <- conv1b
I0702 23:02:43.444970 13779 net.cpp:413] conv1b/bn -> conv1b/bn
I0702 23:02:43.445454 13779 net.cpp:148] Setting up conv1b/bn
I0702 23:02:43.445461 13779 net.cpp:155] Top shape: 8 32 320 320 (26214400)
I0702 23:02:43.445462 13779 net.cpp:163] Memory required for data: 616038400
I0702 23:02:43.445467 13779 layer_factory.hpp:77] Creating layer conv1b/relu
I0702 23:02:43.445469 13779 net.cpp:98] Creating Layer conv1b/relu
I0702 23:02:43.445472 13779 net.cpp:439] conv1b/relu <- conv1b/bn
I0702 23:02:43.445474 13779 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0702 23:02:43.445477 13779 net.cpp:148] Setting up conv1b/relu
I0702 23:02:43.445479 13779 net.cpp:155] Top shape: 8 32 320 320 (26214400)
I0702 23:02:43.445482 13779 net.cpp:163] Memory required for data: 720896000
I0702 23:02:43.445483 13779 layer_factory.hpp:77] Creating layer pool1
I0702 23:02:43.445488 13779 net.cpp:98] Creating Layer pool1
I0702 23:02:43.445490 13779 net.cpp:439] pool1 <- conv1b/bn
I0702 23:02:43.445493 13779 net.cpp:413] pool1 -> pool1
I0702 23:02:43.445523 13779 net.cpp:148] Setting up pool1
I0702 23:02:43.445528 13779 net.cpp:155] Top shape: 8 32 160 160 (6553600)
I0702 23:02:43.445529 13779 net.cpp:163] Memory required for data: 747110400
I0702 23:02:43.445531 13779 layer_factory.hpp:77] Creating layer res2a_branch2a
I0702 23:02:43.445535 13779 net.cpp:98] Creating Layer res2a_branch2a
I0702 23:02:43.445538 13779 net.cpp:439] res2a_branch2a <- pool1
I0702 23:02:43.445540 13779 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0702 23:02:43.446800 13779 net.cpp:148] Setting up res2a_branch2a
I0702 23:02:43.446810 13779 net.cpp:155] Top shape: 8 64 160 160 (13107200)
I0702 23:02:43.446811 13779 net.cpp:163] Memory required for data: 799539200
I0702 23:02:43.446816 13779 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0702 23:02:43.446820 13779 net.cpp:98] Creating Layer res2a_branch2a/bn
I0702 23:02:43.446822 13779 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0702 23:02:43.446825 13779 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0702 23:02:43.447300 13779 net.cpp:148] Setting up res2a_branch2a/bn
I0702 23:02:43.447305 13779 net.cpp:155] Top shape: 8 64 160 160 (13107200)
I0702 23:02:43.447307 13779 net.cpp:163] Memory required for data: 851968000
I0702 23:02:43.447312 13779 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0702 23:02:43.447314 13779 net.cpp:98] Creating Layer res2a_branch2a/relu
I0702 23:02:43.447316 13779 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0702 23:02:43.447319 13779 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0702 23:02:43.447322 13779 net.cpp:148] Setting up res2a_branch2a/relu
I0702 23:02:43.447324 13779 net.cpp:155] Top shape: 8 64 160 160 (13107200)
I0702 23:02:43.447326 13779 net.cpp:163] Memory required for data: 904396800
I0702 23:02:43.447329 13779 layer_factory.hpp:77] Creating layer res2a_branch2b
I0702 23:02:43.447335 13779 net.cpp:98] Creating Layer res2a_branch2b
I0702 23:02:43.447336 13779 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0702 23:02:43.447340 13779 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0702 23:02:43.448400 13779 net.cpp:148] Setting up res2a_branch2b
I0702 23:02:43.448407 13779 net.cpp:155] Top shape: 8 64 160 160 (13107200)
I0702 23:02:43.448410 13779 net.cpp:163] Memory required for data: 956825600
I0702 23:02:43.448413 13779 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0702 23:02:43.448417 13779 net.cpp:98] Creating Layer res2a_branch2b/bn
I0702 23:02:43.448420 13779 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0702 23:02:43.448422 13779 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0702 23:02:43.448909 13779 net.cpp:148] Setting up res2a_branch2b/bn
I0702 23:02:43.448915 13779 net.cpp:155] Top shape: 8 64 160 160 (13107200)
I0702 23:02:43.448917 13779 net.cpp:163] Memory required for data: 1009254400
I0702 23:02:43.448922 13779 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0702 23:02:43.448925 13779 net.cpp:98] Creating Layer res2a_branch2b/relu
I0702 23:02:43.448927 13779 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0702 23:02:43.448930 13779 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0702 23:02:43.448933 13779 net.cpp:148] Setting up res2a_branch2b/relu
I0702 23:02:43.448935 13779 net.cpp:155] Top shape: 8 64 160 160 (13107200)
I0702 23:02:43.448937 13779 net.cpp:163] Memory required for data: 1061683200
I0702 23:02:43.448940 13779 layer_factory.hpp:77] Creating layer pool2
I0702 23:02:43.448942 13779 net.cpp:98] Creating Layer pool2
I0702 23:02:43.448945 13779 net.cpp:439] pool2 <- res2a_branch2b/bn
I0702 23:02:43.448946 13779 net.cpp:413] pool2 -> pool2
I0702 23:02:43.448971 13779 net.cpp:148] Setting up pool2
I0702 23:02:43.448976 13779 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0702 23:02:43.448977 13779 net.cpp:163] Memory required for data: 1074790400
I0702 23:02:43.448979 13779 layer_factory.hpp:77] Creating layer res3a_branch2a
I0702 23:02:43.448983 13779 net.cpp:98] Creating Layer res3a_branch2a
I0702 23:02:43.448985 13779 net.cpp:439] res3a_branch2a <- pool2
I0702 23:02:43.448988 13779 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0702 23:02:43.450639 13779 net.cpp:148] Setting up res3a_branch2a
I0702 23:02:43.450645 13779 net.cpp:155] Top shape: 8 128 80 80 (6553600)
I0702 23:02:43.450647 13779 net.cpp:163] Memory required for data: 1101004800
I0702 23:02:43.450650 13779 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0702 23:02:43.450654 13779 net.cpp:98] Creating Layer res3a_branch2a/bn
I0702 23:02:43.450655 13779 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0702 23:02:43.450659 13779 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0702 23:02:43.451082 13779 net.cpp:148] Setting up res3a_branch2a/bn
I0702 23:02:43.451087 13779 net.cpp:155] Top shape: 8 128 80 80 (6553600)
I0702 23:02:43.451089 13779 net.cpp:163] Memory required for data: 1127219200
I0702 23:02:43.451097 13779 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0702 23:02:43.451098 13779 net.cpp:98] Creating Layer res3a_branch2a/relu
I0702 23:02:43.451100 13779 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0702 23:02:43.451103 13779 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0702 23:02:43.451107 13779 net.cpp:148] Setting up res3a_branch2a/relu
I0702 23:02:43.451110 13779 net.cpp:155] Top shape: 8 128 80 80 (6553600)
I0702 23:02:43.451112 13779 net.cpp:163] Memory required for data: 1153433600
I0702 23:02:43.451113 13779 layer_factory.hpp:77] Creating layer res3a_branch2b
I0702 23:02:43.451118 13779 net.cpp:98] Creating Layer res3a_branch2b
I0702 23:02:43.451120 13779 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0702 23:02:43.451124 13779 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0702 23:02:43.452039 13779 net.cpp:148] Setting up res3a_branch2b
I0702 23:02:43.452044 13779 net.cpp:155] Top shape: 8 128 80 80 (6553600)
I0702 23:02:43.452046 13779 net.cpp:163] Memory required for data: 1179648000
I0702 23:02:43.452049 13779 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0702 23:02:43.452052 13779 net.cpp:98] Creating Layer res3a_branch2b/bn
I0702 23:02:43.452054 13779 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0702 23:02:43.452056 13779 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0702 23:02:43.452476 13779 net.cpp:148] Setting up res3a_branch2b/bn
I0702 23:02:43.452482 13779 net.cpp:155] Top shape: 8 128 80 80 (6553600)
I0702 23:02:43.452484 13779 net.cpp:163] Memory required for data: 1205862400
I0702 23:02:43.452488 13779 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0702 23:02:43.452491 13779 net.cpp:98] Creating Layer res3a_branch2b/relu
I0702 23:02:43.452498 13779 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0702 23:02:43.452502 13779 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0702 23:02:43.452505 13779 net.cpp:148] Setting up res3a_branch2b/relu
I0702 23:02:43.452507 13779 net.cpp:155] Top shape: 8 128 80 80 (6553600)
I0702 23:02:43.452509 13779 net.cpp:163] Memory required for data: 1232076800
I0702 23:02:43.452510 13779 layer_factory.hpp:77] Creating layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0702 23:02:43.452513 13779 net.cpp:98] Creating Layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0702 23:02:43.452515 13779 net.cpp:439] res3a_branch2b/bn_res3a_branch2b/relu_0_split <- res3a_branch2b/bn
I0702 23:02:43.452517 13779 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0702 23:02:43.452520 13779 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0702 23:02:43.452545 13779 net.cpp:148] Setting up res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0702 23:02:43.452548 13779 net.cpp:155] Top shape: 8 128 80 80 (6553600)
I0702 23:02:43.452551 13779 net.cpp:155] Top shape: 8 128 80 80 (6553600)
I0702 23:02:43.452553 13779 net.cpp:163] Memory required for data: 1284505600
I0702 23:02:43.452554 13779 layer_factory.hpp:77] Creating layer pool3
I0702 23:02:43.452558 13779 net.cpp:98] Creating Layer pool3
I0702 23:02:43.452559 13779 net.cpp:439] pool3 <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0702 23:02:43.452561 13779 net.cpp:413] pool3 -> pool3
I0702 23:02:43.452585 13779 net.cpp:148] Setting up pool3
I0702 23:02:43.452589 13779 net.cpp:155] Top shape: 8 128 40 40 (1638400)
I0702 23:02:43.452590 13779 net.cpp:163] Memory required for data: 1291059200
I0702 23:02:43.452592 13779 layer_factory.hpp:77] Creating layer res4a_branch2a
I0702 23:02:43.452597 13779 net.cpp:98] Creating Layer res4a_branch2a
I0702 23:02:43.452600 13779 net.cpp:439] res4a_branch2a <- pool3
I0702 23:02:43.452602 13779 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0702 23:02:43.459278 13779 net.cpp:148] Setting up res4a_branch2a
I0702 23:02:43.459286 13779 net.cpp:155] Top shape: 8 256 40 40 (3276800)
I0702 23:02:43.459288 13779 net.cpp:163] Memory required for data: 1304166400
I0702 23:02:43.459292 13779 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0702 23:02:43.459296 13779 net.cpp:98] Creating Layer res4a_branch2a/bn
I0702 23:02:43.459298 13779 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0702 23:02:43.459302 13779 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0702 23:02:43.459734 13779 net.cpp:148] Setting up res4a_branch2a/bn
I0702 23:02:43.459739 13779 net.cpp:155] Top shape: 8 256 40 40 (3276800)
I0702 23:02:43.459741 13779 net.cpp:163] Memory required for data: 1317273600
I0702 23:02:43.459746 13779 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0702 23:02:43.459749 13779 net.cpp:98] Creating Layer res4a_branch2a/relu
I0702 23:02:43.459751 13779 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0702 23:02:43.459753 13779 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0702 23:02:43.459758 13779 net.cpp:148] Setting up res4a_branch2a/relu
I0702 23:02:43.459759 13779 net.cpp:155] Top shape: 8 256 40 40 (3276800)
I0702 23:02:43.459761 13779 net.cpp:163] Memory required for data: 1330380800
I0702 23:02:43.459763 13779 layer_factory.hpp:77] Creating layer res4a_branch2b
I0702 23:02:43.459766 13779 net.cpp:98] Creating Layer res4a_branch2b
I0702 23:02:43.459769 13779 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0702 23:02:43.459771 13779 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0702 23:02:43.462853 13779 net.cpp:148] Setting up res4a_branch2b
I0702 23:02:43.462859 13779 net.cpp:155] Top shape: 8 256 40 40 (3276800)
I0702 23:02:43.462862 13779 net.cpp:163] Memory required for data: 1343488000
I0702 23:02:43.462864 13779 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0702 23:02:43.462868 13779 net.cpp:98] Creating Layer res4a_branch2b/bn
I0702 23:02:43.462877 13779 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0702 23:02:43.462879 13779 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0702 23:02:43.463330 13779 net.cpp:148] Setting up res4a_branch2b/bn
I0702 23:02:43.463336 13779 net.cpp:155] Top shape: 8 256 40 40 (3276800)
I0702 23:02:43.463338 13779 net.cpp:163] Memory required for data: 1356595200
I0702 23:02:43.463343 13779 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0702 23:02:43.463346 13779 net.cpp:98] Creating Layer res4a_branch2b/relu
I0702 23:02:43.463348 13779 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0702 23:02:43.463351 13779 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0702 23:02:43.463354 13779 net.cpp:148] Setting up res4a_branch2b/relu
I0702 23:02:43.463356 13779 net.cpp:155] Top shape: 8 256 40 40 (3276800)
I0702 23:02:43.463358 13779 net.cpp:163] Memory required for data: 1369702400
I0702 23:02:43.463361 13779 layer_factory.hpp:77] Creating layer pool4
I0702 23:02:43.463363 13779 net.cpp:98] Creating Layer pool4
I0702 23:02:43.463366 13779 net.cpp:439] pool4 <- res4a_branch2b/bn
I0702 23:02:43.463367 13779 net.cpp:413] pool4 -> pool4
I0702 23:02:43.463392 13779 net.cpp:148] Setting up pool4
I0702 23:02:43.463397 13779 net.cpp:155] Top shape: 8 256 40 40 (3276800)
I0702 23:02:43.463398 13779 net.cpp:163] Memory required for data: 1382809600
I0702 23:02:43.463400 13779 layer_factory.hpp:77] Creating layer res5a_branch2a
I0702 23:02:43.463404 13779 net.cpp:98] Creating Layer res5a_branch2a
I0702 23:02:43.463407 13779 net.cpp:439] res5a_branch2a <- pool4
I0702 23:02:43.463408 13779 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0702 23:02:43.488729 13779 net.cpp:148] Setting up res5a_branch2a
I0702 23:02:43.488747 13779 net.cpp:155] Top shape: 8 512 40 40 (6553600)
I0702 23:02:43.488749 13779 net.cpp:163] Memory required for data: 1409024000
I0702 23:02:43.488755 13779 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0702 23:02:43.488764 13779 net.cpp:98] Creating Layer res5a_branch2a/bn
I0702 23:02:43.488766 13779 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0702 23:02:43.488770 13779 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0702 23:02:43.489228 13779 net.cpp:148] Setting up res5a_branch2a/bn
I0702 23:02:43.489234 13779 net.cpp:155] Top shape: 8 512 40 40 (6553600)
I0702 23:02:43.489236 13779 net.cpp:163] Memory required for data: 1435238400
I0702 23:02:43.489241 13779 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0702 23:02:43.489244 13779 net.cpp:98] Creating Layer res5a_branch2a/relu
I0702 23:02:43.489248 13779 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0702 23:02:43.489249 13779 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0702 23:02:43.489253 13779 net.cpp:148] Setting up res5a_branch2a/relu
I0702 23:02:43.489255 13779 net.cpp:155] Top shape: 8 512 40 40 (6553600)
I0702 23:02:43.489258 13779 net.cpp:163] Memory required for data: 1461452800
I0702 23:02:43.489259 13779 layer_factory.hpp:77] Creating layer res5a_branch2b
I0702 23:02:43.489264 13779 net.cpp:98] Creating Layer res5a_branch2b
I0702 23:02:43.489266 13779 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0702 23:02:43.489269 13779 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0702 23:02:43.501873 13779 net.cpp:148] Setting up res5a_branch2b
I0702 23:02:43.501894 13779 net.cpp:155] Top shape: 8 512 40 40 (6553600)
I0702 23:02:43.501896 13779 net.cpp:163] Memory required for data: 1487667200
I0702 23:02:43.501906 13779 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0702 23:02:43.501914 13779 net.cpp:98] Creating Layer res5a_branch2b/bn
I0702 23:02:43.501916 13779 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0702 23:02:43.501920 13779 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0702 23:02:43.502379 13779 net.cpp:148] Setting up res5a_branch2b/bn
I0702 23:02:43.502390 13779 net.cpp:155] Top shape: 8 512 40 40 (6553600)
I0702 23:02:43.502394 13779 net.cpp:163] Memory required for data: 1513881600
I0702 23:02:43.502401 13779 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0702 23:02:43.502413 13779 net.cpp:98] Creating Layer res5a_branch2b/relu
I0702 23:02:43.502416 13779 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0702 23:02:43.502418 13779 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0702 23:02:43.502424 13779 net.cpp:148] Setting up res5a_branch2b/relu
I0702 23:02:43.502427 13779 net.cpp:155] Top shape: 8 512 40 40 (6553600)
I0702 23:02:43.502429 13779 net.cpp:163] Memory required for data: 1540096000
I0702 23:02:43.502431 13779 layer_factory.hpp:77] Creating layer out5a
I0702 23:02:43.502437 13779 net.cpp:98] Creating Layer out5a
I0702 23:02:43.502439 13779 net.cpp:439] out5a <- res5a_branch2b/bn
I0702 23:02:43.502442 13779 net.cpp:413] out5a -> out5a
I0702 23:02:43.506292 13779 net.cpp:148] Setting up out5a
I0702 23:02:43.506300 13779 net.cpp:155] Top shape: 8 64 40 40 (819200)
I0702 23:02:43.506302 13779 net.cpp:163] Memory required for data: 1543372800
I0702 23:02:43.506306 13779 layer_factory.hpp:77] Creating layer out5a/bn
I0702 23:02:43.506310 13779 net.cpp:98] Creating Layer out5a/bn
I0702 23:02:43.506312 13779 net.cpp:439] out5a/bn <- out5a
I0702 23:02:43.506315 13779 net.cpp:413] out5a/bn -> out5a/bn
I0702 23:02:43.506817 13779 net.cpp:148] Setting up out5a/bn
I0702 23:02:43.506824 13779 net.cpp:155] Top shape: 8 64 40 40 (819200)
I0702 23:02:43.506825 13779 net.cpp:163] Memory required for data: 1546649600
I0702 23:02:43.506830 13779 layer_factory.hpp:77] Creating layer out5a/relu
I0702 23:02:43.506834 13779 net.cpp:98] Creating Layer out5a/relu
I0702 23:02:43.506836 13779 net.cpp:439] out5a/relu <- out5a/bn
I0702 23:02:43.506839 13779 net.cpp:400] out5a/relu -> out5a/bn (in-place)
I0702 23:02:43.506841 13779 net.cpp:148] Setting up out5a/relu
I0702 23:02:43.506844 13779 net.cpp:155] Top shape: 8 64 40 40 (819200)
I0702 23:02:43.506845 13779 net.cpp:163] Memory required for data: 1549926400
I0702 23:02:43.506847 13779 layer_factory.hpp:77] Creating layer out5a_up2
I0702 23:02:43.506855 13779 net.cpp:98] Creating Layer out5a_up2
I0702 23:02:43.506858 13779 net.cpp:439] out5a_up2 <- out5a/bn
I0702 23:02:43.506861 13779 net.cpp:413] out5a_up2 -> out5a_up2
I0702 23:02:43.507040 13779 net.cpp:148] Setting up out5a_up2
I0702 23:02:43.507045 13779 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0702 23:02:43.507046 13779 net.cpp:163] Memory required for data: 1563033600
I0702 23:02:43.507050 13779 layer_factory.hpp:77] Creating layer out3a
I0702 23:02:43.507055 13779 net.cpp:98] Creating Layer out3a
I0702 23:02:43.507058 13779 net.cpp:439] out3a <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0702 23:02:43.507062 13779 net.cpp:413] out3a -> out3a
I0702 23:02:43.508002 13779 net.cpp:148] Setting up out3a
I0702 23:02:43.508008 13779 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0702 23:02:43.508009 13779 net.cpp:163] Memory required for data: 1576140800
I0702 23:02:43.508013 13779 layer_factory.hpp:77] Creating layer out3a/bn
I0702 23:02:43.508015 13779 net.cpp:98] Creating Layer out3a/bn
I0702 23:02:43.508018 13779 net.cpp:439] out3a/bn <- out3a
I0702 23:02:43.508020 13779 net.cpp:413] out3a/bn -> out3a/bn
I0702 23:02:43.508508 13779 net.cpp:148] Setting up out3a/bn
I0702 23:02:43.508513 13779 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0702 23:02:43.508515 13779 net.cpp:163] Memory required for data: 1589248000
I0702 23:02:43.508520 13779 layer_factory.hpp:77] Creating layer out3a/relu
I0702 23:02:43.508522 13779 net.cpp:98] Creating Layer out3a/relu
I0702 23:02:43.508525 13779 net.cpp:439] out3a/relu <- out3a/bn
I0702 23:02:43.508527 13779 net.cpp:400] out3a/relu -> out3a/bn (in-place)
I0702 23:02:43.508530 13779 net.cpp:148] Setting up out3a/relu
I0702 23:02:43.508533 13779 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0702 23:02:43.508534 13779 net.cpp:163] Memory required for data: 1602355200
I0702 23:02:43.508536 13779 layer_factory.hpp:77] Creating layer out3_out5_combined
I0702 23:02:43.508541 13779 net.cpp:98] Creating Layer out3_out5_combined
I0702 23:02:43.508543 13779 net.cpp:439] out3_out5_combined <- out5a_up2
I0702 23:02:43.508553 13779 net.cpp:439] out3_out5_combined <- out3a/bn
I0702 23:02:43.508556 13779 net.cpp:413] out3_out5_combined -> out3_out5_combined
I0702 23:02:43.508574 13779 net.cpp:148] Setting up out3_out5_combined
I0702 23:02:43.508579 13779 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0702 23:02:43.508580 13779 net.cpp:163] Memory required for data: 1615462400
I0702 23:02:43.508582 13779 layer_factory.hpp:77] Creating layer ctx_conv1
I0702 23:02:43.508586 13779 net.cpp:98] Creating Layer ctx_conv1
I0702 23:02:43.508589 13779 net.cpp:439] ctx_conv1 <- out3_out5_combined
I0702 23:02:43.508590 13779 net.cpp:413] ctx_conv1 -> ctx_conv1
I0702 23:02:43.509518 13779 net.cpp:148] Setting up ctx_conv1
I0702 23:02:43.509523 13779 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0702 23:02:43.509526 13779 net.cpp:163] Memory required for data: 1628569600
I0702 23:02:43.509528 13779 layer_factory.hpp:77] Creating layer ctx_conv1/bn
I0702 23:02:43.509531 13779 net.cpp:98] Creating Layer ctx_conv1/bn
I0702 23:02:43.509533 13779 net.cpp:439] ctx_conv1/bn <- ctx_conv1
I0702 23:02:43.509536 13779 net.cpp:413] ctx_conv1/bn -> ctx_conv1/bn
I0702 23:02:43.510020 13779 net.cpp:148] Setting up ctx_conv1/bn
I0702 23:02:43.510025 13779 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0702 23:02:43.510026 13779 net.cpp:163] Memory required for data: 1641676800
I0702 23:02:43.510031 13779 layer_factory.hpp:77] Creating layer ctx_conv1/relu
I0702 23:02:43.510035 13779 net.cpp:98] Creating Layer ctx_conv1/relu
I0702 23:02:43.510036 13779 net.cpp:439] ctx_conv1/relu <- ctx_conv1/bn
I0702 23:02:43.510040 13779 net.cpp:400] ctx_conv1/relu -> ctx_conv1/bn (in-place)
I0702 23:02:43.510042 13779 net.cpp:148] Setting up ctx_conv1/relu
I0702 23:02:43.510044 13779 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0702 23:02:43.510046 13779 net.cpp:163] Memory required for data: 1654784000
I0702 23:02:43.510048 13779 layer_factory.hpp:77] Creating layer ctx_conv2
I0702 23:02:43.510051 13779 net.cpp:98] Creating Layer ctx_conv2
I0702 23:02:43.510054 13779 net.cpp:439] ctx_conv2 <- ctx_conv1/bn
I0702 23:02:43.510056 13779 net.cpp:413] ctx_conv2 -> ctx_conv2
I0702 23:02:43.510993 13779 net.cpp:148] Setting up ctx_conv2
I0702 23:02:43.510998 13779 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0702 23:02:43.511000 13779 net.cpp:163] Memory required for data: 1667891200
I0702 23:02:43.511003 13779 layer_factory.hpp:77] Creating layer ctx_conv2/bn
I0702 23:02:43.511006 13779 net.cpp:98] Creating Layer ctx_conv2/bn
I0702 23:02:43.511008 13779 net.cpp:439] ctx_conv2/bn <- ctx_conv2
I0702 23:02:43.511011 13779 net.cpp:413] ctx_conv2/bn -> ctx_conv2/bn
I0702 23:02:43.511502 13779 net.cpp:148] Setting up ctx_conv2/bn
I0702 23:02:43.511507 13779 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0702 23:02:43.511509 13779 net.cpp:163] Memory required for data: 1680998400
I0702 23:02:43.511514 13779 layer_factory.hpp:77] Creating layer ctx_conv2/relu
I0702 23:02:43.511518 13779 net.cpp:98] Creating Layer ctx_conv2/relu
I0702 23:02:43.511519 13779 net.cpp:439] ctx_conv2/relu <- ctx_conv2/bn
I0702 23:02:43.511523 13779 net.cpp:400] ctx_conv2/relu -> ctx_conv2/bn (in-place)
I0702 23:02:43.511526 13779 net.cpp:148] Setting up ctx_conv2/relu
I0702 23:02:43.511528 13779 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0702 23:02:43.511530 13779 net.cpp:163] Memory required for data: 1694105600
I0702 23:02:43.511533 13779 layer_factory.hpp:77] Creating layer ctx_conv3
I0702 23:02:43.511535 13779 net.cpp:98] Creating Layer ctx_conv3
I0702 23:02:43.511538 13779 net.cpp:439] ctx_conv3 <- ctx_conv2/bn
I0702 23:02:43.511540 13779 net.cpp:413] ctx_conv3 -> ctx_conv3
I0702 23:02:43.512468 13779 net.cpp:148] Setting up ctx_conv3
I0702 23:02:43.512472 13779 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0702 23:02:43.512475 13779 net.cpp:163] Memory required for data: 1707212800
I0702 23:02:43.512478 13779 layer_factory.hpp:77] Creating layer ctx_conv3/bn
I0702 23:02:43.512481 13779 net.cpp:98] Creating Layer ctx_conv3/bn
I0702 23:02:43.512483 13779 net.cpp:439] ctx_conv3/bn <- ctx_conv3
I0702 23:02:43.512491 13779 net.cpp:413] ctx_conv3/bn -> ctx_conv3/bn
I0702 23:02:43.512979 13779 net.cpp:148] Setting up ctx_conv3/bn
I0702 23:02:43.512984 13779 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0702 23:02:43.512985 13779 net.cpp:163] Memory required for data: 1720320000
I0702 23:02:43.512990 13779 layer_factory.hpp:77] Creating layer ctx_conv3/relu
I0702 23:02:43.512992 13779 net.cpp:98] Creating Layer ctx_conv3/relu
I0702 23:02:43.512995 13779 net.cpp:439] ctx_conv3/relu <- ctx_conv3/bn
I0702 23:02:43.512997 13779 net.cpp:400] ctx_conv3/relu -> ctx_conv3/bn (in-place)
I0702 23:02:43.513000 13779 net.cpp:148] Setting up ctx_conv3/relu
I0702 23:02:43.513002 13779 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0702 23:02:43.513005 13779 net.cpp:163] Memory required for data: 1733427200
I0702 23:02:43.513005 13779 layer_factory.hpp:77] Creating layer ctx_conv4
I0702 23:02:43.513010 13779 net.cpp:98] Creating Layer ctx_conv4
I0702 23:02:43.513011 13779 net.cpp:439] ctx_conv4 <- ctx_conv3/bn
I0702 23:02:43.513013 13779 net.cpp:413] ctx_conv4 -> ctx_conv4
I0702 23:02:43.513944 13779 net.cpp:148] Setting up ctx_conv4
I0702 23:02:43.513948 13779 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0702 23:02:43.513950 13779 net.cpp:163] Memory required for data: 1746534400
I0702 23:02:43.513953 13779 layer_factory.hpp:77] Creating layer ctx_conv4/bn
I0702 23:02:43.513957 13779 net.cpp:98] Creating Layer ctx_conv4/bn
I0702 23:02:43.513959 13779 net.cpp:439] ctx_conv4/bn <- ctx_conv4
I0702 23:02:43.513962 13779 net.cpp:413] ctx_conv4/bn -> ctx_conv4/bn
I0702 23:02:43.514461 13779 net.cpp:148] Setting up ctx_conv4/bn
I0702 23:02:43.514467 13779 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0702 23:02:43.514468 13779 net.cpp:163] Memory required for data: 1759641600
I0702 23:02:43.514473 13779 layer_factory.hpp:77] Creating layer ctx_conv4/relu
I0702 23:02:43.514475 13779 net.cpp:98] Creating Layer ctx_conv4/relu
I0702 23:02:43.514477 13779 net.cpp:439] ctx_conv4/relu <- ctx_conv4/bn
I0702 23:02:43.514480 13779 net.cpp:400] ctx_conv4/relu -> ctx_conv4/bn (in-place)
I0702 23:02:43.514483 13779 net.cpp:148] Setting up ctx_conv4/relu
I0702 23:02:43.514485 13779 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0702 23:02:43.514487 13779 net.cpp:163] Memory required for data: 1772748800
I0702 23:02:43.514489 13779 layer_factory.hpp:77] Creating layer ctx_final
I0702 23:02:43.514492 13779 net.cpp:98] Creating Layer ctx_final
I0702 23:02:43.514494 13779 net.cpp:439] ctx_final <- ctx_conv4/bn
I0702 23:02:43.514497 13779 net.cpp:413] ctx_final -> ctx_final
I0702 23:02:43.514791 13779 net.cpp:148] Setting up ctx_final
I0702 23:02:43.514796 13779 net.cpp:155] Top shape: 8 8 80 80 (409600)
I0702 23:02:43.514797 13779 net.cpp:163] Memory required for data: 1774387200
I0702 23:02:43.514801 13779 layer_factory.hpp:77] Creating layer ctx_final/relu
I0702 23:02:43.514802 13779 net.cpp:98] Creating Layer ctx_final/relu
I0702 23:02:43.514804 13779 net.cpp:439] ctx_final/relu <- ctx_final
I0702 23:02:43.514806 13779 net.cpp:400] ctx_final/relu -> ctx_final (in-place)
I0702 23:02:43.514809 13779 net.cpp:148] Setting up ctx_final/relu
I0702 23:02:43.514812 13779 net.cpp:155] Top shape: 8 8 80 80 (409600)
I0702 23:02:43.514813 13779 net.cpp:163] Memory required for data: 1776025600
I0702 23:02:43.514816 13779 layer_factory.hpp:77] Creating layer out_deconv_final_up2
I0702 23:02:43.514818 13779 net.cpp:98] Creating Layer out_deconv_final_up2
I0702 23:02:43.514820 13779 net.cpp:439] out_deconv_final_up2 <- ctx_final
I0702 23:02:43.514822 13779 net.cpp:413] out_deconv_final_up2 -> out_deconv_final_up2
I0702 23:02:43.514978 13779 net.cpp:148] Setting up out_deconv_final_up2
I0702 23:02:43.514982 13779 net.cpp:155] Top shape: 8 8 160 160 (1638400)
I0702 23:02:43.514984 13779 net.cpp:163] Memory required for data: 1782579200
I0702 23:02:43.514987 13779 layer_factory.hpp:77] Creating layer out_deconv_final_up4
I0702 23:02:43.514991 13779 net.cpp:98] Creating Layer out_deconv_final_up4
I0702 23:02:43.514992 13779 net.cpp:439] out_deconv_final_up4 <- out_deconv_final_up2
I0702 23:02:43.515000 13779 net.cpp:413] out_deconv_final_up4 -> out_deconv_final_up4
I0702 23:02:43.515154 13779 net.cpp:148] Setting up out_deconv_final_up4
I0702 23:02:43.515159 13779 net.cpp:155] Top shape: 8 8 320 320 (6553600)
I0702 23:02:43.515161 13779 net.cpp:163] Memory required for data: 1808793600
I0702 23:02:43.515163 13779 layer_factory.hpp:77] Creating layer out_deconv_final_up8
I0702 23:02:43.515166 13779 net.cpp:98] Creating Layer out_deconv_final_up8
I0702 23:02:43.515168 13779 net.cpp:439] out_deconv_final_up8 <- out_deconv_final_up4
I0702 23:02:43.515171 13779 net.cpp:413] out_deconv_final_up8 -> out_deconv_final_up8
I0702 23:02:43.515324 13779 net.cpp:148] Setting up out_deconv_final_up8
I0702 23:02:43.515328 13779 net.cpp:155] Top shape: 8 8 640 640 (26214400)
I0702 23:02:43.515331 13779 net.cpp:163] Memory required for data: 1913651200
I0702 23:02:43.515332 13779 layer_factory.hpp:77] Creating layer loss
I0702 23:02:43.515338 13779 net.cpp:98] Creating Layer loss
I0702 23:02:43.515341 13779 net.cpp:439] loss <- out_deconv_final_up8
I0702 23:02:43.515342 13779 net.cpp:439] loss <- label
I0702 23:02:43.515347 13779 net.cpp:413] loss -> loss
I0702 23:02:43.515352 13779 layer_factory.hpp:77] Creating layer loss
I0702 23:02:43.546562 13779 net.cpp:148] Setting up loss
I0702 23:02:43.546586 13779 net.cpp:155] Top shape: (1)
I0702 23:02:43.546587 13779 net.cpp:158]     with loss weight 1
I0702 23:02:43.546602 13779 net.cpp:163] Memory required for data: 1913651204
I0702 23:02:43.546604 13779 net.cpp:224] loss needs backward computation.
I0702 23:02:43.546608 13779 net.cpp:224] out_deconv_final_up8 needs backward computation.
I0702 23:02:43.546612 13779 net.cpp:224] out_deconv_final_up4 needs backward computation.
I0702 23:02:43.546613 13779 net.cpp:224] out_deconv_final_up2 needs backward computation.
I0702 23:02:43.546614 13779 net.cpp:224] ctx_final/relu needs backward computation.
I0702 23:02:43.546617 13779 net.cpp:224] ctx_final needs backward computation.
I0702 23:02:43.546618 13779 net.cpp:224] ctx_conv4/relu needs backward computation.
I0702 23:02:43.546620 13779 net.cpp:224] ctx_conv4/bn needs backward computation.
I0702 23:02:43.546623 13779 net.cpp:224] ctx_conv4 needs backward computation.
I0702 23:02:43.546625 13779 net.cpp:224] ctx_conv3/relu needs backward computation.
I0702 23:02:43.546628 13779 net.cpp:224] ctx_conv3/bn needs backward computation.
I0702 23:02:43.546630 13779 net.cpp:224] ctx_conv3 needs backward computation.
I0702 23:02:43.546633 13779 net.cpp:224] ctx_conv2/relu needs backward computation.
I0702 23:02:43.546634 13779 net.cpp:224] ctx_conv2/bn needs backward computation.
I0702 23:02:43.546636 13779 net.cpp:224] ctx_conv2 needs backward computation.
I0702 23:02:43.546638 13779 net.cpp:224] ctx_conv1/relu needs backward computation.
I0702 23:02:43.546641 13779 net.cpp:224] ctx_conv1/bn needs backward computation.
I0702 23:02:43.546643 13779 net.cpp:224] ctx_conv1 needs backward computation.
I0702 23:02:43.546645 13779 net.cpp:224] out3_out5_combined needs backward computation.
I0702 23:02:43.546648 13779 net.cpp:224] out3a/relu needs backward computation.
I0702 23:02:43.546650 13779 net.cpp:224] out3a/bn needs backward computation.
I0702 23:02:43.546653 13779 net.cpp:224] out3a needs backward computation.
I0702 23:02:43.546656 13779 net.cpp:224] out5a_up2 needs backward computation.
I0702 23:02:43.546659 13779 net.cpp:224] out5a/relu needs backward computation.
I0702 23:02:43.546661 13779 net.cpp:224] out5a/bn needs backward computation.
I0702 23:02:43.546664 13779 net.cpp:224] out5a needs backward computation.
I0702 23:02:43.546667 13779 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0702 23:02:43.546669 13779 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0702 23:02:43.546671 13779 net.cpp:224] res5a_branch2b needs backward computation.
I0702 23:02:43.546674 13779 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0702 23:02:43.546676 13779 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0702 23:02:43.546689 13779 net.cpp:224] res5a_branch2a needs backward computation.
I0702 23:02:43.546692 13779 net.cpp:224] pool4 needs backward computation.
I0702 23:02:43.546694 13779 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0702 23:02:43.546697 13779 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0702 23:02:43.546700 13779 net.cpp:224] res4a_branch2b needs backward computation.
I0702 23:02:43.546702 13779 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0702 23:02:43.546705 13779 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0702 23:02:43.546707 13779 net.cpp:224] res4a_branch2a needs backward computation.
I0702 23:02:43.546710 13779 net.cpp:224] pool3 needs backward computation.
I0702 23:02:43.546712 13779 net.cpp:224] res3a_branch2b/bn_res3a_branch2b/relu_0_split needs backward computation.
I0702 23:02:43.546715 13779 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0702 23:02:43.546718 13779 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0702 23:02:43.546720 13779 net.cpp:224] res3a_branch2b needs backward computation.
I0702 23:02:43.546723 13779 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0702 23:02:43.546725 13779 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0702 23:02:43.546727 13779 net.cpp:224] res3a_branch2a needs backward computation.
I0702 23:02:43.546730 13779 net.cpp:224] pool2 needs backward computation.
I0702 23:02:43.546732 13779 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0702 23:02:43.546736 13779 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0702 23:02:43.546737 13779 net.cpp:224] res2a_branch2b needs backward computation.
I0702 23:02:43.546739 13779 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0702 23:02:43.546741 13779 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0702 23:02:43.546744 13779 net.cpp:224] res2a_branch2a needs backward computation.
I0702 23:02:43.546747 13779 net.cpp:224] pool1 needs backward computation.
I0702 23:02:43.546749 13779 net.cpp:224] conv1b/relu needs backward computation.
I0702 23:02:43.546751 13779 net.cpp:224] conv1b/bn needs backward computation.
I0702 23:02:43.546754 13779 net.cpp:224] conv1b needs backward computation.
I0702 23:02:43.546756 13779 net.cpp:224] conv1a/relu needs backward computation.
I0702 23:02:43.546758 13779 net.cpp:224] conv1a/bn needs backward computation.
I0702 23:02:43.546761 13779 net.cpp:224] conv1a needs backward computation.
I0702 23:02:43.546763 13779 net.cpp:226] data/bias does not need backward computation.
I0702 23:02:43.546766 13779 net.cpp:226] data does not need backward computation.
I0702 23:02:43.546768 13779 net.cpp:268] This network produces output loss
I0702 23:02:43.546797 13779 net.cpp:288] Network initialization done.
I0702 23:02:43.547479 13779 solver.cpp:182] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-07-02_23-02-42/initial/test.prototxt
I0702 23:02:43.547762 13779 net.cpp:56] Initializing net from parameters: 
name: "jsegnet21v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/val-image-lmdb"
    label_list_path: "data/val-label-lmdb"
    batch_size: 4
    threads: 4
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b/bn"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a/bn"
  top: "out5a/bn"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a/bn"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b/bn"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a/bn"
  top: "out3a/bn"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a/bn"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv1/bn"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv2/bn"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv3/bn"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4/bn"
  top: "ctx_conv4/bn"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4/bn"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
  accuracy_param {
    ignore_label: 255
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
    ignore_label: 255
  }
}
I0702 23:02:43.547879 13779 layer_factory.hpp:77] Creating layer data
I0702 23:02:43.547886 13779 net.cpp:98] Creating Layer data
I0702 23:02:43.547890 13779 net.cpp:413] data -> data
I0702 23:02:43.547894 13779 net.cpp:413] data -> label
I0702 23:02:43.549422 13817 db_lmdb.cpp:35] Opened lmdb data/val-image-lmdb
I0702 23:02:43.549422 13822 db_lmdb.cpp:35] Opened lmdb data/val-label-lmdb
I0702 23:02:43.553217 13779 data_layer.cpp:78] ReshapePrefetch 4, 3, 640, 640
I0702 23:02:43.553303 13779 data_layer.cpp:83] output data size: 4,3,640,640
I0702 23:02:43.578783 13779 data_layer.cpp:78] ReshapePrefetch 4, 1, 640, 640
I0702 23:02:43.578850 13779 data_layer.cpp:83] output data size: 4,1,640,640
I0702 23:02:43.591603 13779 net.cpp:148] Setting up data
I0702 23:02:43.591652 13779 net.cpp:155] Top shape: 4 3 640 640 (4915200)
I0702 23:02:43.591658 13779 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0702 23:02:43.591661 13779 net.cpp:163] Memory required for data: 26214400
I0702 23:02:43.591670 13779 layer_factory.hpp:77] Creating layer label_data_1_split
I0702 23:02:43.591691 13779 net.cpp:98] Creating Layer label_data_1_split
I0702 23:02:43.591723 13779 net.cpp:439] label_data_1_split <- label
I0702 23:02:43.591733 13779 net.cpp:413] label_data_1_split -> label_data_1_split_0
I0702 23:02:43.591744 13779 net.cpp:413] label_data_1_split -> label_data_1_split_1
I0702 23:02:43.591750 13779 net.cpp:413] label_data_1_split -> label_data_1_split_2
I0702 23:02:43.591907 13779 net.cpp:148] Setting up label_data_1_split
I0702 23:02:43.591912 13779 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0702 23:02:43.591917 13779 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0702 23:02:43.591919 13779 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0702 23:02:43.591928 13779 net.cpp:163] Memory required for data: 45875200
I0702 23:02:43.591931 13779 layer_factory.hpp:77] Creating layer data/bias
I0702 23:02:43.591944 13779 net.cpp:98] Creating Layer data/bias
I0702 23:02:43.591948 13779 net.cpp:439] data/bias <- data
I0702 23:02:43.591953 13779 net.cpp:413] data/bias -> data/bias
I0702 23:02:43.593540 13779 net.cpp:148] Setting up data/bias
I0702 23:02:43.593571 13779 net.cpp:155] Top shape: 4 3 640 640 (4915200)
I0702 23:02:43.593575 13779 net.cpp:163] Memory required for data: 65536000
I0702 23:02:43.593590 13779 layer_factory.hpp:77] Creating layer conv1a
I0702 23:02:43.593607 13779 net.cpp:98] Creating Layer conv1a
I0702 23:02:43.593611 13779 net.cpp:439] conv1a <- data/bias
I0702 23:02:43.593617 13779 net.cpp:413] conv1a -> conv1a
I0702 23:02:43.594122 13779 net.cpp:148] Setting up conv1a
I0702 23:02:43.594131 13779 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0702 23:02:43.594135 13779 net.cpp:163] Memory required for data: 117964800
I0702 23:02:43.594142 13779 layer_factory.hpp:77] Creating layer conv1a/bn
I0702 23:02:43.594151 13779 net.cpp:98] Creating Layer conv1a/bn
I0702 23:02:43.594154 13779 net.cpp:439] conv1a/bn <- conv1a
I0702 23:02:43.594161 13779 net.cpp:413] conv1a/bn -> conv1a/bn
I0702 23:02:43.594952 13779 net.cpp:148] Setting up conv1a/bn
I0702 23:02:43.594961 13779 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0702 23:02:43.594964 13779 net.cpp:163] Memory required for data: 170393600
I0702 23:02:43.594974 13779 layer_factory.hpp:77] Creating layer conv1a/relu
I0702 23:02:43.594979 13779 net.cpp:98] Creating Layer conv1a/relu
I0702 23:02:43.594983 13779 net.cpp:439] conv1a/relu <- conv1a/bn
I0702 23:02:43.594987 13779 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0702 23:02:43.594995 13779 net.cpp:148] Setting up conv1a/relu
I0702 23:02:43.595000 13779 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0702 23:02:43.595002 13779 net.cpp:163] Memory required for data: 222822400
I0702 23:02:43.595006 13779 layer_factory.hpp:77] Creating layer conv1b
I0702 23:02:43.595015 13779 net.cpp:98] Creating Layer conv1b
I0702 23:02:43.595017 13779 net.cpp:439] conv1b <- conv1a/bn
I0702 23:02:43.595021 13779 net.cpp:413] conv1b -> conv1b
I0702 23:02:43.595425 13779 net.cpp:148] Setting up conv1b
I0702 23:02:43.595432 13779 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0702 23:02:43.595437 13779 net.cpp:163] Memory required for data: 275251200
I0702 23:02:43.595444 13779 layer_factory.hpp:77] Creating layer conv1b/bn
I0702 23:02:43.595449 13779 net.cpp:98] Creating Layer conv1b/bn
I0702 23:02:43.595453 13779 net.cpp:439] conv1b/bn <- conv1b
I0702 23:02:43.595458 13779 net.cpp:413] conv1b/bn -> conv1b/bn
I0702 23:02:43.596349 13779 net.cpp:148] Setting up conv1b/bn
I0702 23:02:43.596357 13779 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0702 23:02:43.596360 13779 net.cpp:163] Memory required for data: 327680000
I0702 23:02:43.596369 13779 layer_factory.hpp:77] Creating layer conv1b/relu
I0702 23:02:43.596376 13779 net.cpp:98] Creating Layer conv1b/relu
I0702 23:02:43.596380 13779 net.cpp:439] conv1b/relu <- conv1b/bn
I0702 23:02:43.596385 13779 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0702 23:02:43.596390 13779 net.cpp:148] Setting up conv1b/relu
I0702 23:02:43.596395 13779 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0702 23:02:43.596398 13779 net.cpp:163] Memory required for data: 380108800
I0702 23:02:43.596401 13779 layer_factory.hpp:77] Creating layer pool1
I0702 23:02:43.596421 13779 net.cpp:98] Creating Layer pool1
I0702 23:02:43.596423 13779 net.cpp:439] pool1 <- conv1b/bn
I0702 23:02:43.596428 13779 net.cpp:413] pool1 -> pool1
I0702 23:02:43.596462 13779 net.cpp:148] Setting up pool1
I0702 23:02:43.596467 13779 net.cpp:155] Top shape: 4 32 160 160 (3276800)
I0702 23:02:43.596470 13779 net.cpp:163] Memory required for data: 393216000
I0702 23:02:43.596473 13779 layer_factory.hpp:77] Creating layer res2a_branch2a
I0702 23:02:43.596482 13779 net.cpp:98] Creating Layer res2a_branch2a
I0702 23:02:43.596484 13779 net.cpp:439] res2a_branch2a <- pool1
I0702 23:02:43.596489 13779 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0702 23:02:43.597143 13779 net.cpp:148] Setting up res2a_branch2a
I0702 23:02:43.597151 13779 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0702 23:02:43.597153 13779 net.cpp:163] Memory required for data: 419430400
I0702 23:02:43.597162 13779 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0702 23:02:43.597169 13779 net.cpp:98] Creating Layer res2a_branch2a/bn
I0702 23:02:43.597172 13779 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0702 23:02:43.597178 13779 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0702 23:02:43.601799 13779 net.cpp:148] Setting up res2a_branch2a/bn
I0702 23:02:43.601842 13779 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0702 23:02:43.601864 13779 net.cpp:163] Memory required for data: 445644800
I0702 23:02:43.601898 13779 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0702 23:02:43.601917 13779 net.cpp:98] Creating Layer res2a_branch2a/relu
I0702 23:02:43.601928 13779 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0702 23:02:43.601943 13779 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0702 23:02:43.601958 13779 net.cpp:148] Setting up res2a_branch2a/relu
I0702 23:02:43.601974 13779 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0702 23:02:43.601985 13779 net.cpp:163] Memory required for data: 471859200
I0702 23:02:43.601997 13779 layer_factory.hpp:77] Creating layer res2a_branch2b
I0702 23:02:43.602015 13779 net.cpp:98] Creating Layer res2a_branch2b
I0702 23:02:43.602030 13779 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0702 23:02:43.602043 13779 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0702 23:02:43.602808 13779 net.cpp:148] Setting up res2a_branch2b
I0702 23:02:43.602821 13779 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0702 23:02:43.602825 13779 net.cpp:163] Memory required for data: 498073600
I0702 23:02:43.602833 13779 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0702 23:02:43.602843 13779 net.cpp:98] Creating Layer res2a_branch2b/bn
I0702 23:02:43.602847 13779 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0702 23:02:43.602855 13779 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0702 23:02:43.606034 13779 net.cpp:148] Setting up res2a_branch2b/bn
I0702 23:02:43.606045 13779 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0702 23:02:43.606047 13779 net.cpp:163] Memory required for data: 524288000
I0702 23:02:43.606058 13779 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0702 23:02:43.606066 13779 net.cpp:98] Creating Layer res2a_branch2b/relu
I0702 23:02:43.606070 13779 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0702 23:02:43.606075 13779 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0702 23:02:43.606082 13779 net.cpp:148] Setting up res2a_branch2b/relu
I0702 23:02:43.606086 13779 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0702 23:02:43.606091 13779 net.cpp:163] Memory required for data: 550502400
I0702 23:02:43.606094 13779 layer_factory.hpp:77] Creating layer pool2
I0702 23:02:43.606106 13779 net.cpp:98] Creating Layer pool2
I0702 23:02:43.606109 13779 net.cpp:439] pool2 <- res2a_branch2b/bn
I0702 23:02:43.606117 13779 net.cpp:413] pool2 -> pool2
I0702 23:02:43.606166 13779 net.cpp:148] Setting up pool2
I0702 23:02:43.606173 13779 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0702 23:02:43.606175 13779 net.cpp:163] Memory required for data: 557056000
I0702 23:02:43.606200 13779 layer_factory.hpp:77] Creating layer res3a_branch2a
I0702 23:02:43.606211 13779 net.cpp:98] Creating Layer res3a_branch2a
I0702 23:02:43.606215 13779 net.cpp:439] res3a_branch2a <- pool2
I0702 23:02:43.606220 13779 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0702 23:02:43.608044 13779 net.cpp:148] Setting up res3a_branch2a
I0702 23:02:43.608055 13779 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0702 23:02:43.608058 13779 net.cpp:163] Memory required for data: 570163200
I0702 23:02:43.608065 13779 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0702 23:02:43.608074 13779 net.cpp:98] Creating Layer res3a_branch2a/bn
I0702 23:02:43.608078 13779 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0702 23:02:43.608083 13779 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0702 23:02:43.608610 13779 net.cpp:148] Setting up res3a_branch2a/bn
I0702 23:02:43.608618 13779 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0702 23:02:43.608621 13779 net.cpp:163] Memory required for data: 583270400
I0702 23:02:43.608633 13779 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0702 23:02:43.608639 13779 net.cpp:98] Creating Layer res3a_branch2a/relu
I0702 23:02:43.608644 13779 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0702 23:02:43.608647 13779 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0702 23:02:43.608654 13779 net.cpp:148] Setting up res3a_branch2a/relu
I0702 23:02:43.608659 13779 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0702 23:02:43.608660 13779 net.cpp:163] Memory required for data: 596377600
I0702 23:02:43.608664 13779 layer_factory.hpp:77] Creating layer res3a_branch2b
I0702 23:02:43.608675 13779 net.cpp:98] Creating Layer res3a_branch2b
I0702 23:02:43.608677 13779 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0702 23:02:43.608682 13779 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0702 23:02:43.609699 13779 net.cpp:148] Setting up res3a_branch2b
I0702 23:02:43.609707 13779 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0702 23:02:43.609710 13779 net.cpp:163] Memory required for data: 609484800
I0702 23:02:43.609716 13779 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0702 23:02:43.609724 13779 net.cpp:98] Creating Layer res3a_branch2b/bn
I0702 23:02:43.609728 13779 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0702 23:02:43.609733 13779 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0702 23:02:43.610414 13779 net.cpp:148] Setting up res3a_branch2b/bn
I0702 23:02:43.610445 13779 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0702 23:02:43.610455 13779 net.cpp:163] Memory required for data: 622592000
I0702 23:02:43.610474 13779 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0702 23:02:43.610487 13779 net.cpp:98] Creating Layer res3a_branch2b/relu
I0702 23:02:43.610497 13779 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0702 23:02:43.610507 13779 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0702 23:02:43.610519 13779 net.cpp:148] Setting up res3a_branch2b/relu
I0702 23:02:43.610529 13779 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0702 23:02:43.610538 13779 net.cpp:163] Memory required for data: 635699200
I0702 23:02:43.610546 13779 layer_factory.hpp:77] Creating layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0702 23:02:43.610560 13779 net.cpp:98] Creating Layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0702 23:02:43.610570 13779 net.cpp:439] res3a_branch2b/bn_res3a_branch2b/relu_0_split <- res3a_branch2b/bn
I0702 23:02:43.610582 13779 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0702 23:02:43.610594 13779 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0702 23:02:43.610651 13779 net.cpp:148] Setting up res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0702 23:02:43.610663 13779 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0702 23:02:43.610672 13779 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0702 23:02:43.610688 13779 net.cpp:163] Memory required for data: 661913600
I0702 23:02:43.610715 13779 layer_factory.hpp:77] Creating layer pool3
I0702 23:02:43.610729 13779 net.cpp:98] Creating Layer pool3
I0702 23:02:43.610738 13779 net.cpp:439] pool3 <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0702 23:02:43.610750 13779 net.cpp:413] pool3 -> pool3
I0702 23:02:43.610810 13779 net.cpp:148] Setting up pool3
I0702 23:02:43.610822 13779 net.cpp:155] Top shape: 4 128 40 40 (819200)
I0702 23:02:43.610831 13779 net.cpp:163] Memory required for data: 665190400
I0702 23:02:43.610841 13779 layer_factory.hpp:77] Creating layer res4a_branch2a
I0702 23:02:43.611178 13779 net.cpp:98] Creating Layer res4a_branch2a
I0702 23:02:43.611196 13779 net.cpp:439] res4a_branch2a <- pool3
I0702 23:02:43.611207 13779 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0702 23:02:43.621978 13779 net.cpp:148] Setting up res4a_branch2a
I0702 23:02:43.622012 13779 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0702 23:02:43.622016 13779 net.cpp:163] Memory required for data: 671744000
I0702 23:02:43.622027 13779 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0702 23:02:43.622045 13779 net.cpp:98] Creating Layer res4a_branch2a/bn
I0702 23:02:43.622050 13779 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0702 23:02:43.622057 13779 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0702 23:02:43.625102 13779 net.cpp:148] Setting up res4a_branch2a/bn
I0702 23:02:43.625145 13779 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0702 23:02:43.625149 13779 net.cpp:163] Memory required for data: 678297600
I0702 23:02:43.625164 13779 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0702 23:02:43.625185 13779 net.cpp:98] Creating Layer res4a_branch2a/relu
I0702 23:02:43.625196 13779 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0702 23:02:43.625203 13779 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0702 23:02:43.625218 13779 net.cpp:148] Setting up res4a_branch2a/relu
I0702 23:02:43.625228 13779 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0702 23:02:43.625234 13779 net.cpp:163] Memory required for data: 684851200
I0702 23:02:43.625239 13779 layer_factory.hpp:77] Creating layer res4a_branch2b
I0702 23:02:43.625262 13779 net.cpp:98] Creating Layer res4a_branch2b
I0702 23:02:43.625272 13779 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0702 23:02:43.625283 13779 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0702 23:02:43.630349 13779 net.cpp:148] Setting up res4a_branch2b
I0702 23:02:43.630362 13779 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0702 23:02:43.630367 13779 net.cpp:163] Memory required for data: 691404800
I0702 23:02:43.630373 13779 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0702 23:02:43.630398 13779 net.cpp:98] Creating Layer res4a_branch2b/bn
I0702 23:02:43.630403 13779 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0702 23:02:43.630409 13779 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0702 23:02:43.631119 13779 net.cpp:148] Setting up res4a_branch2b/bn
I0702 23:02:43.631129 13779 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0702 23:02:43.631131 13779 net.cpp:163] Memory required for data: 697958400
I0702 23:02:43.631139 13779 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0702 23:02:43.631145 13779 net.cpp:98] Creating Layer res4a_branch2b/relu
I0702 23:02:43.631147 13779 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0702 23:02:43.631152 13779 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0702 23:02:43.631157 13779 net.cpp:148] Setting up res4a_branch2b/relu
I0702 23:02:43.631161 13779 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0702 23:02:43.631165 13779 net.cpp:163] Memory required for data: 704512000
I0702 23:02:43.631168 13779 layer_factory.hpp:77] Creating layer pool4
I0702 23:02:43.631176 13779 net.cpp:98] Creating Layer pool4
I0702 23:02:43.631180 13779 net.cpp:439] pool4 <- res4a_branch2b/bn
I0702 23:02:43.631183 13779 net.cpp:413] pool4 -> pool4
I0702 23:02:43.631224 13779 net.cpp:148] Setting up pool4
I0702 23:02:43.631229 13779 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0702 23:02:43.631247 13779 net.cpp:163] Memory required for data: 711065600
I0702 23:02:43.631252 13779 layer_factory.hpp:77] Creating layer res5a_branch2a
I0702 23:02:43.631261 13779 net.cpp:98] Creating Layer res5a_branch2a
I0702 23:02:43.631265 13779 net.cpp:439] res5a_branch2a <- pool4
I0702 23:02:43.631269 13779 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0702 23:02:43.659397 13779 net.cpp:148] Setting up res5a_branch2a
I0702 23:02:43.659420 13779 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0702 23:02:43.659421 13779 net.cpp:163] Memory required for data: 724172800
I0702 23:02:43.659427 13779 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0702 23:02:43.659435 13779 net.cpp:98] Creating Layer res5a_branch2a/bn
I0702 23:02:43.659437 13779 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0702 23:02:43.659440 13779 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0702 23:02:43.659955 13779 net.cpp:148] Setting up res5a_branch2a/bn
I0702 23:02:43.659962 13779 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0702 23:02:43.659965 13779 net.cpp:163] Memory required for data: 737280000
I0702 23:02:43.659970 13779 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0702 23:02:43.659973 13779 net.cpp:98] Creating Layer res5a_branch2a/relu
I0702 23:02:43.659976 13779 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0702 23:02:43.659977 13779 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0702 23:02:43.659981 13779 net.cpp:148] Setting up res5a_branch2a/relu
I0702 23:02:43.659984 13779 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0702 23:02:43.659986 13779 net.cpp:163] Memory required for data: 750387200
I0702 23:02:43.659987 13779 layer_factory.hpp:77] Creating layer res5a_branch2b
I0702 23:02:43.659992 13779 net.cpp:98] Creating Layer res5a_branch2b
I0702 23:02:43.659994 13779 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0702 23:02:43.659998 13779 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0702 23:02:43.673339 13779 net.cpp:148] Setting up res5a_branch2b
I0702 23:02:43.673348 13779 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0702 23:02:43.673351 13779 net.cpp:163] Memory required for data: 763494400
I0702 23:02:43.673357 13779 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0702 23:02:43.673362 13779 net.cpp:98] Creating Layer res5a_branch2b/bn
I0702 23:02:43.673363 13779 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0702 23:02:43.673367 13779 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0702 23:02:43.673861 13779 net.cpp:148] Setting up res5a_branch2b/bn
I0702 23:02:43.673866 13779 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0702 23:02:43.673868 13779 net.cpp:163] Memory required for data: 776601600
I0702 23:02:43.673873 13779 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0702 23:02:43.673877 13779 net.cpp:98] Creating Layer res5a_branch2b/relu
I0702 23:02:43.673878 13779 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0702 23:02:43.673882 13779 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0702 23:02:43.673884 13779 net.cpp:148] Setting up res5a_branch2b/relu
I0702 23:02:43.673887 13779 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0702 23:02:43.673889 13779 net.cpp:163] Memory required for data: 789708800
I0702 23:02:43.673892 13779 layer_factory.hpp:77] Creating layer out5a
I0702 23:02:43.673895 13779 net.cpp:98] Creating Layer out5a
I0702 23:02:43.673897 13779 net.cpp:439] out5a <- res5a_branch2b/bn
I0702 23:02:43.673900 13779 net.cpp:413] out5a -> out5a
I0702 23:02:43.678200 13779 net.cpp:148] Setting up out5a
I0702 23:02:43.678218 13779 net.cpp:155] Top shape: 4 64 40 40 (409600)
I0702 23:02:43.678220 13779 net.cpp:163] Memory required for data: 791347200
I0702 23:02:43.678226 13779 layer_factory.hpp:77] Creating layer out5a/bn
I0702 23:02:43.678233 13779 net.cpp:98] Creating Layer out5a/bn
I0702 23:02:43.678236 13779 net.cpp:439] out5a/bn <- out5a
I0702 23:02:43.678241 13779 net.cpp:413] out5a/bn -> out5a/bn
I0702 23:02:43.678848 13779 net.cpp:148] Setting up out5a/bn
I0702 23:02:43.678864 13779 net.cpp:155] Top shape: 4 64 40 40 (409600)
I0702 23:02:43.678867 13779 net.cpp:163] Memory required for data: 792985600
I0702 23:02:43.678875 13779 layer_factory.hpp:77] Creating layer out5a/relu
I0702 23:02:43.678879 13779 net.cpp:98] Creating Layer out5a/relu
I0702 23:02:43.678881 13779 net.cpp:439] out5a/relu <- out5a/bn
I0702 23:02:43.678884 13779 net.cpp:400] out5a/relu -> out5a/bn (in-place)
I0702 23:02:43.678890 13779 net.cpp:148] Setting up out5a/relu
I0702 23:02:43.678894 13779 net.cpp:155] Top shape: 4 64 40 40 (409600)
I0702 23:02:43.678894 13779 net.cpp:163] Memory required for data: 794624000
I0702 23:02:43.678896 13779 layer_factory.hpp:77] Creating layer out5a_up2
I0702 23:02:43.678903 13779 net.cpp:98] Creating Layer out5a_up2
I0702 23:02:43.678905 13779 net.cpp:439] out5a_up2 <- out5a/bn
I0702 23:02:43.678911 13779 net.cpp:413] out5a_up2 -> out5a_up2
I0702 23:02:43.679111 13779 net.cpp:148] Setting up out5a_up2
I0702 23:02:43.679116 13779 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0702 23:02:43.679117 13779 net.cpp:163] Memory required for data: 801177600
I0702 23:02:43.679121 13779 layer_factory.hpp:77] Creating layer out3a
I0702 23:02:43.679124 13779 net.cpp:98] Creating Layer out3a
I0702 23:02:43.679127 13779 net.cpp:439] out3a <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0702 23:02:43.679131 13779 net.cpp:413] out3a -> out3a
I0702 23:02:43.680892 13779 net.cpp:148] Setting up out3a
I0702 23:02:43.680902 13779 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0702 23:02:43.680904 13779 net.cpp:163] Memory required for data: 807731200
I0702 23:02:43.680907 13779 layer_factory.hpp:77] Creating layer out3a/bn
I0702 23:02:43.680912 13779 net.cpp:98] Creating Layer out3a/bn
I0702 23:02:43.680914 13779 net.cpp:439] out3a/bn <- out3a
I0702 23:02:43.680917 13779 net.cpp:413] out3a/bn -> out3a/bn
I0702 23:02:43.681466 13779 net.cpp:148] Setting up out3a/bn
I0702 23:02:43.681473 13779 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0702 23:02:43.681474 13779 net.cpp:163] Memory required for data: 814284800
I0702 23:02:43.681479 13779 layer_factory.hpp:77] Creating layer out3a/relu
I0702 23:02:43.681483 13779 net.cpp:98] Creating Layer out3a/relu
I0702 23:02:43.681485 13779 net.cpp:439] out3a/relu <- out3a/bn
I0702 23:02:43.681488 13779 net.cpp:400] out3a/relu -> out3a/bn (in-place)
I0702 23:02:43.681491 13779 net.cpp:148] Setting up out3a/relu
I0702 23:02:43.681493 13779 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0702 23:02:43.681495 13779 net.cpp:163] Memory required for data: 820838400
I0702 23:02:43.681498 13779 layer_factory.hpp:77] Creating layer out3_out5_combined
I0702 23:02:43.681500 13779 net.cpp:98] Creating Layer out3_out5_combined
I0702 23:02:43.681502 13779 net.cpp:439] out3_out5_combined <- out5a_up2
I0702 23:02:43.681504 13779 net.cpp:439] out3_out5_combined <- out3a/bn
I0702 23:02:43.681509 13779 net.cpp:413] out3_out5_combined -> out3_out5_combined
I0702 23:02:43.681529 13779 net.cpp:148] Setting up out3_out5_combined
I0702 23:02:43.681532 13779 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0702 23:02:43.681535 13779 net.cpp:163] Memory required for data: 827392000
I0702 23:02:43.681536 13779 layer_factory.hpp:77] Creating layer ctx_conv1
I0702 23:02:43.681540 13779 net.cpp:98] Creating Layer ctx_conv1
I0702 23:02:43.681542 13779 net.cpp:439] ctx_conv1 <- out3_out5_combined
I0702 23:02:43.681545 13779 net.cpp:413] ctx_conv1 -> ctx_conv1
I0702 23:02:43.682533 13779 net.cpp:148] Setting up ctx_conv1
I0702 23:02:43.682541 13779 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0702 23:02:43.682545 13779 net.cpp:163] Memory required for data: 833945600
I0702 23:02:43.682548 13779 layer_factory.hpp:77] Creating layer ctx_conv1/bn
I0702 23:02:43.682552 13779 net.cpp:98] Creating Layer ctx_conv1/bn
I0702 23:02:43.682554 13779 net.cpp:439] ctx_conv1/bn <- ctx_conv1
I0702 23:02:43.682557 13779 net.cpp:413] ctx_conv1/bn -> ctx_conv1/bn
I0702 23:02:43.683099 13779 net.cpp:148] Setting up ctx_conv1/bn
I0702 23:02:43.683104 13779 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0702 23:02:43.683113 13779 net.cpp:163] Memory required for data: 840499200
I0702 23:02:43.683120 13779 layer_factory.hpp:77] Creating layer ctx_conv1/relu
I0702 23:02:43.683121 13779 net.cpp:98] Creating Layer ctx_conv1/relu
I0702 23:02:43.683125 13779 net.cpp:439] ctx_conv1/relu <- ctx_conv1/bn
I0702 23:02:43.683126 13779 net.cpp:400] ctx_conv1/relu -> ctx_conv1/bn (in-place)
I0702 23:02:43.683130 13779 net.cpp:148] Setting up ctx_conv1/relu
I0702 23:02:43.683132 13779 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0702 23:02:43.683135 13779 net.cpp:163] Memory required for data: 847052800
I0702 23:02:43.683136 13779 layer_factory.hpp:77] Creating layer ctx_conv2
I0702 23:02:43.683140 13779 net.cpp:98] Creating Layer ctx_conv2
I0702 23:02:43.683142 13779 net.cpp:439] ctx_conv2 <- ctx_conv1/bn
I0702 23:02:43.683145 13779 net.cpp:413] ctx_conv2 -> ctx_conv2
I0702 23:02:43.684139 13779 net.cpp:148] Setting up ctx_conv2
I0702 23:02:43.684144 13779 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0702 23:02:43.684146 13779 net.cpp:163] Memory required for data: 853606400
I0702 23:02:43.684149 13779 layer_factory.hpp:77] Creating layer ctx_conv2/bn
I0702 23:02:43.684152 13779 net.cpp:98] Creating Layer ctx_conv2/bn
I0702 23:02:43.684154 13779 net.cpp:439] ctx_conv2/bn <- ctx_conv2
I0702 23:02:43.684157 13779 net.cpp:413] ctx_conv2/bn -> ctx_conv2/bn
I0702 23:02:43.684690 13779 net.cpp:148] Setting up ctx_conv2/bn
I0702 23:02:43.684695 13779 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0702 23:02:43.684697 13779 net.cpp:163] Memory required for data: 860160000
I0702 23:02:43.684701 13779 layer_factory.hpp:77] Creating layer ctx_conv2/relu
I0702 23:02:43.684705 13779 net.cpp:98] Creating Layer ctx_conv2/relu
I0702 23:02:43.684707 13779 net.cpp:439] ctx_conv2/relu <- ctx_conv2/bn
I0702 23:02:43.684710 13779 net.cpp:400] ctx_conv2/relu -> ctx_conv2/bn (in-place)
I0702 23:02:43.684712 13779 net.cpp:148] Setting up ctx_conv2/relu
I0702 23:02:43.684715 13779 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0702 23:02:43.684716 13779 net.cpp:163] Memory required for data: 866713600
I0702 23:02:43.684718 13779 layer_factory.hpp:77] Creating layer ctx_conv3
I0702 23:02:43.684722 13779 net.cpp:98] Creating Layer ctx_conv3
I0702 23:02:43.684725 13779 net.cpp:439] ctx_conv3 <- ctx_conv2/bn
I0702 23:02:43.684727 13779 net.cpp:413] ctx_conv3 -> ctx_conv3
I0702 23:02:43.685719 13779 net.cpp:148] Setting up ctx_conv3
I0702 23:02:43.685724 13779 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0702 23:02:43.685726 13779 net.cpp:163] Memory required for data: 873267200
I0702 23:02:43.685729 13779 layer_factory.hpp:77] Creating layer ctx_conv3/bn
I0702 23:02:43.685734 13779 net.cpp:98] Creating Layer ctx_conv3/bn
I0702 23:02:43.685735 13779 net.cpp:439] ctx_conv3/bn <- ctx_conv3
I0702 23:02:43.685739 13779 net.cpp:413] ctx_conv3/bn -> ctx_conv3/bn
I0702 23:02:43.686264 13779 net.cpp:148] Setting up ctx_conv3/bn
I0702 23:02:43.686269 13779 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0702 23:02:43.686270 13779 net.cpp:163] Memory required for data: 879820800
I0702 23:02:43.686275 13779 layer_factory.hpp:77] Creating layer ctx_conv3/relu
I0702 23:02:43.686277 13779 net.cpp:98] Creating Layer ctx_conv3/relu
I0702 23:02:43.686280 13779 net.cpp:439] ctx_conv3/relu <- ctx_conv3/bn
I0702 23:02:43.686285 13779 net.cpp:400] ctx_conv3/relu -> ctx_conv3/bn (in-place)
I0702 23:02:43.686287 13779 net.cpp:148] Setting up ctx_conv3/relu
I0702 23:02:43.686290 13779 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0702 23:02:43.686291 13779 net.cpp:163] Memory required for data: 886374400
I0702 23:02:43.686293 13779 layer_factory.hpp:77] Creating layer ctx_conv4
I0702 23:02:43.686297 13779 net.cpp:98] Creating Layer ctx_conv4
I0702 23:02:43.686300 13779 net.cpp:439] ctx_conv4 <- ctx_conv3/bn
I0702 23:02:43.686301 13779 net.cpp:413] ctx_conv4 -> ctx_conv4
I0702 23:02:43.687295 13779 net.cpp:148] Setting up ctx_conv4
I0702 23:02:43.687300 13779 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0702 23:02:43.687302 13779 net.cpp:163] Memory required for data: 892928000
I0702 23:02:43.687310 13779 layer_factory.hpp:77] Creating layer ctx_conv4/bn
I0702 23:02:43.687314 13779 net.cpp:98] Creating Layer ctx_conv4/bn
I0702 23:02:43.687316 13779 net.cpp:439] ctx_conv4/bn <- ctx_conv4
I0702 23:02:43.687319 13779 net.cpp:413] ctx_conv4/bn -> ctx_conv4/bn
I0702 23:02:43.687840 13779 net.cpp:148] Setting up ctx_conv4/bn
I0702 23:02:43.687845 13779 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0702 23:02:43.687847 13779 net.cpp:163] Memory required for data: 899481600
I0702 23:02:43.687851 13779 layer_factory.hpp:77] Creating layer ctx_conv4/relu
I0702 23:02:43.687855 13779 net.cpp:98] Creating Layer ctx_conv4/relu
I0702 23:02:43.687856 13779 net.cpp:439] ctx_conv4/relu <- ctx_conv4/bn
I0702 23:02:43.687858 13779 net.cpp:400] ctx_conv4/relu -> ctx_conv4/bn (in-place)
I0702 23:02:43.687861 13779 net.cpp:148] Setting up ctx_conv4/relu
I0702 23:02:43.687863 13779 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0702 23:02:43.687865 13779 net.cpp:163] Memory required for data: 906035200
I0702 23:02:43.687867 13779 layer_factory.hpp:77] Creating layer ctx_final
I0702 23:02:43.687871 13779 net.cpp:98] Creating Layer ctx_final
I0702 23:02:43.687872 13779 net.cpp:439] ctx_final <- ctx_conv4/bn
I0702 23:02:43.687875 13779 net.cpp:413] ctx_final -> ctx_final
I0702 23:02:43.688194 13779 net.cpp:148] Setting up ctx_final
I0702 23:02:43.688199 13779 net.cpp:155] Top shape: 4 8 80 80 (204800)
I0702 23:02:43.688200 13779 net.cpp:163] Memory required for data: 906854400
I0702 23:02:43.688204 13779 layer_factory.hpp:77] Creating layer ctx_final/relu
I0702 23:02:43.688205 13779 net.cpp:98] Creating Layer ctx_final/relu
I0702 23:02:43.688207 13779 net.cpp:439] ctx_final/relu <- ctx_final
I0702 23:02:43.688210 13779 net.cpp:400] ctx_final/relu -> ctx_final (in-place)
I0702 23:02:43.688212 13779 net.cpp:148] Setting up ctx_final/relu
I0702 23:02:43.688215 13779 net.cpp:155] Top shape: 4 8 80 80 (204800)
I0702 23:02:43.688216 13779 net.cpp:163] Memory required for data: 907673600
I0702 23:02:43.688218 13779 layer_factory.hpp:77] Creating layer out_deconv_final_up2
I0702 23:02:43.688221 13779 net.cpp:98] Creating Layer out_deconv_final_up2
I0702 23:02:43.688223 13779 net.cpp:439] out_deconv_final_up2 <- ctx_final
I0702 23:02:43.688225 13779 net.cpp:413] out_deconv_final_up2 -> out_deconv_final_up2
I0702 23:02:43.688393 13779 net.cpp:148] Setting up out_deconv_final_up2
I0702 23:02:43.688397 13779 net.cpp:155] Top shape: 4 8 160 160 (819200)
I0702 23:02:43.688400 13779 net.cpp:163] Memory required for data: 910950400
I0702 23:02:43.688402 13779 layer_factory.hpp:77] Creating layer out_deconv_final_up4
I0702 23:02:43.688405 13779 net.cpp:98] Creating Layer out_deconv_final_up4
I0702 23:02:43.688406 13779 net.cpp:439] out_deconv_final_up4 <- out_deconv_final_up2
I0702 23:02:43.688410 13779 net.cpp:413] out_deconv_final_up4 -> out_deconv_final_up4
I0702 23:02:43.688575 13779 net.cpp:148] Setting up out_deconv_final_up4
I0702 23:02:43.688578 13779 net.cpp:155] Top shape: 4 8 320 320 (3276800)
I0702 23:02:43.688580 13779 net.cpp:163] Memory required for data: 924057600
I0702 23:02:43.688583 13779 layer_factory.hpp:77] Creating layer out_deconv_final_up8
I0702 23:02:43.688585 13779 net.cpp:98] Creating Layer out_deconv_final_up8
I0702 23:02:43.688587 13779 net.cpp:439] out_deconv_final_up8 <- out_deconv_final_up4
I0702 23:02:43.688591 13779 net.cpp:413] out_deconv_final_up8 -> out_deconv_final_up8
I0702 23:02:43.688752 13779 net.cpp:148] Setting up out_deconv_final_up8
I0702 23:02:43.688757 13779 net.cpp:155] Top shape: 4 8 640 640 (13107200)
I0702 23:02:43.688758 13779 net.cpp:163] Memory required for data: 976486400
I0702 23:02:43.688761 13779 layer_factory.hpp:77] Creating layer out_deconv_final_up8_out_deconv_final_up8_0_split
I0702 23:02:43.688765 13779 net.cpp:98] Creating Layer out_deconv_final_up8_out_deconv_final_up8_0_split
I0702 23:02:43.688766 13779 net.cpp:439] out_deconv_final_up8_out_deconv_final_up8_0_split <- out_deconv_final_up8
I0702 23:02:43.688768 13779 net.cpp:413] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0702 23:02:43.688776 13779 net.cpp:413] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0702 23:02:43.688779 13779 net.cpp:413] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0702 23:02:43.688819 13779 net.cpp:148] Setting up out_deconv_final_up8_out_deconv_final_up8_0_split
I0702 23:02:43.688823 13779 net.cpp:155] Top shape: 4 8 640 640 (13107200)
I0702 23:02:43.688825 13779 net.cpp:155] Top shape: 4 8 640 640 (13107200)
I0702 23:02:43.688827 13779 net.cpp:155] Top shape: 4 8 640 640 (13107200)
I0702 23:02:43.688829 13779 net.cpp:163] Memory required for data: 1133772800
I0702 23:02:43.688832 13779 layer_factory.hpp:77] Creating layer loss
I0702 23:02:43.688835 13779 net.cpp:98] Creating Layer loss
I0702 23:02:43.688838 13779 net.cpp:439] loss <- out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0702 23:02:43.688839 13779 net.cpp:439] loss <- label_data_1_split_0
I0702 23:02:43.688843 13779 net.cpp:413] loss -> loss
I0702 23:02:43.688846 13779 layer_factory.hpp:77] Creating layer loss
I0702 23:02:43.704440 13779 net.cpp:148] Setting up loss
I0702 23:02:43.704463 13779 net.cpp:155] Top shape: (1)
I0702 23:02:43.704465 13779 net.cpp:158]     with loss weight 1
I0702 23:02:43.704473 13779 net.cpp:163] Memory required for data: 1133772804
I0702 23:02:43.704476 13779 layer_factory.hpp:77] Creating layer accuracy/top1
I0702 23:02:43.704485 13779 net.cpp:98] Creating Layer accuracy/top1
I0702 23:02:43.704489 13779 net.cpp:439] accuracy/top1 <- out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0702 23:02:43.704494 13779 net.cpp:439] accuracy/top1 <- label_data_1_split_1
I0702 23:02:43.704499 13779 net.cpp:413] accuracy/top1 -> accuracy/top1
I0702 23:02:43.704506 13779 net.cpp:148] Setting up accuracy/top1
I0702 23:02:43.704509 13779 net.cpp:155] Top shape: (1)
I0702 23:02:43.704511 13779 net.cpp:163] Memory required for data: 1133772808
I0702 23:02:43.704512 13779 layer_factory.hpp:77] Creating layer accuracy/top5
I0702 23:02:43.704516 13779 net.cpp:98] Creating Layer accuracy/top5
I0702 23:02:43.704519 13779 net.cpp:439] accuracy/top5 <- out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0702 23:02:43.704520 13779 net.cpp:439] accuracy/top5 <- label_data_1_split_2
I0702 23:02:43.704524 13779 net.cpp:413] accuracy/top5 -> accuracy/top5
I0702 23:02:43.704527 13779 net.cpp:148] Setting up accuracy/top5
I0702 23:02:43.704530 13779 net.cpp:155] Top shape: (1)
I0702 23:02:43.704532 13779 net.cpp:163] Memory required for data: 1133772812
I0702 23:02:43.704535 13779 net.cpp:226] accuracy/top5 does not need backward computation.
I0702 23:02:43.704537 13779 net.cpp:226] accuracy/top1 does not need backward computation.
I0702 23:02:43.704540 13779 net.cpp:224] loss needs backward computation.
I0702 23:02:43.704542 13779 net.cpp:224] out_deconv_final_up8_out_deconv_final_up8_0_split needs backward computation.
I0702 23:02:43.704545 13779 net.cpp:224] out_deconv_final_up8 needs backward computation.
I0702 23:02:43.704546 13779 net.cpp:224] out_deconv_final_up4 needs backward computation.
I0702 23:02:43.704550 13779 net.cpp:224] out_deconv_final_up2 needs backward computation.
I0702 23:02:43.704551 13779 net.cpp:224] ctx_final/relu needs backward computation.
I0702 23:02:43.704553 13779 net.cpp:224] ctx_final needs backward computation.
I0702 23:02:43.704555 13779 net.cpp:224] ctx_conv4/relu needs backward computation.
I0702 23:02:43.704557 13779 net.cpp:224] ctx_conv4/bn needs backward computation.
I0702 23:02:43.704560 13779 net.cpp:224] ctx_conv4 needs backward computation.
I0702 23:02:43.704562 13779 net.cpp:224] ctx_conv3/relu needs backward computation.
I0702 23:02:43.704565 13779 net.cpp:224] ctx_conv3/bn needs backward computation.
I0702 23:02:43.704567 13779 net.cpp:224] ctx_conv3 needs backward computation.
I0702 23:02:43.704569 13779 net.cpp:224] ctx_conv2/relu needs backward computation.
I0702 23:02:43.704572 13779 net.cpp:224] ctx_conv2/bn needs backward computation.
I0702 23:02:43.704586 13779 net.cpp:224] ctx_conv2 needs backward computation.
I0702 23:02:43.704587 13779 net.cpp:224] ctx_conv1/relu needs backward computation.
I0702 23:02:43.704589 13779 net.cpp:224] ctx_conv1/bn needs backward computation.
I0702 23:02:43.704591 13779 net.cpp:224] ctx_conv1 needs backward computation.
I0702 23:02:43.704596 13779 net.cpp:224] out3_out5_combined needs backward computation.
I0702 23:02:43.704598 13779 net.cpp:224] out3a/relu needs backward computation.
I0702 23:02:43.704601 13779 net.cpp:224] out3a/bn needs backward computation.
I0702 23:02:43.704603 13779 net.cpp:224] out3a needs backward computation.
I0702 23:02:43.704605 13779 net.cpp:224] out5a_up2 needs backward computation.
I0702 23:02:43.704608 13779 net.cpp:224] out5a/relu needs backward computation.
I0702 23:02:43.704610 13779 net.cpp:224] out5a/bn needs backward computation.
I0702 23:02:43.704612 13779 net.cpp:224] out5a needs backward computation.
I0702 23:02:43.704615 13779 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0702 23:02:43.704617 13779 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0702 23:02:43.704619 13779 net.cpp:224] res5a_branch2b needs backward computation.
I0702 23:02:43.704622 13779 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0702 23:02:43.704624 13779 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0702 23:02:43.704627 13779 net.cpp:224] res5a_branch2a needs backward computation.
I0702 23:02:43.704628 13779 net.cpp:224] pool4 needs backward computation.
I0702 23:02:43.704632 13779 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0702 23:02:43.704633 13779 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0702 23:02:43.704635 13779 net.cpp:224] res4a_branch2b needs backward computation.
I0702 23:02:43.704638 13779 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0702 23:02:43.704640 13779 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0702 23:02:43.704643 13779 net.cpp:224] res4a_branch2a needs backward computation.
I0702 23:02:43.704645 13779 net.cpp:224] pool3 needs backward computation.
I0702 23:02:43.704648 13779 net.cpp:224] res3a_branch2b/bn_res3a_branch2b/relu_0_split needs backward computation.
I0702 23:02:43.704650 13779 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0702 23:02:43.704653 13779 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0702 23:02:43.704655 13779 net.cpp:224] res3a_branch2b needs backward computation.
I0702 23:02:43.704658 13779 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0702 23:02:43.704660 13779 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0702 23:02:43.704663 13779 net.cpp:224] res3a_branch2a needs backward computation.
I0702 23:02:43.704665 13779 net.cpp:224] pool2 needs backward computation.
I0702 23:02:43.704668 13779 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0702 23:02:43.704670 13779 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0702 23:02:43.704672 13779 net.cpp:224] res2a_branch2b needs backward computation.
I0702 23:02:43.704675 13779 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0702 23:02:43.704677 13779 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0702 23:02:43.704679 13779 net.cpp:224] res2a_branch2a needs backward computation.
I0702 23:02:43.704682 13779 net.cpp:224] pool1 needs backward computation.
I0702 23:02:43.704684 13779 net.cpp:224] conv1b/relu needs backward computation.
I0702 23:02:43.704687 13779 net.cpp:224] conv1b/bn needs backward computation.
I0702 23:02:43.704689 13779 net.cpp:224] conv1b needs backward computation.
I0702 23:02:43.704692 13779 net.cpp:224] conv1a/relu needs backward computation.
I0702 23:02:43.704694 13779 net.cpp:224] conv1a/bn needs backward computation.
I0702 23:02:43.704697 13779 net.cpp:224] conv1a needs backward computation.
I0702 23:02:43.704699 13779 net.cpp:226] data/bias does not need backward computation.
I0702 23:02:43.704702 13779 net.cpp:226] label_data_1_split does not need backward computation.
I0702 23:02:43.704708 13779 net.cpp:226] data does not need backward computation.
I0702 23:02:43.704711 13779 net.cpp:268] This network produces output accuracy/top1
I0702 23:02:43.704713 13779 net.cpp:268] This network produces output accuracy/top5
I0702 23:02:43.704716 13779 net.cpp:268] This network produces output loss
I0702 23:02:43.704744 13779 net.cpp:288] Network initialization done.
I0702 23:02:43.704839 13779 solver.cpp:60] Solver scaffolding done.
I0702 23:02:43.710650 13779 caffe.cpp:145] Finetuning from training/imagenet_jacintonet11_v2_bn_iter_160000.caffemodel
I0702 23:02:43.806730 13779 net.cpp:804] Ignoring source layer input
I0702 23:02:43.811050 13779 net.cpp:804] Ignoring source layer pool5
I0702 23:02:43.811079 13779 net.cpp:804] Ignoring source layer fc1000
I0702 23:02:43.811094 13779 net.cpp:804] Ignoring source layer fc1000_fc1000_0_split
I0702 23:02:43.811110 13779 net.cpp:804] Ignoring source layer prob
I0702 23:02:43.811131 13779 net.cpp:804] Ignoring source layer argMaxOut
I0702 23:02:43.836127 13779 net.cpp:804] Ignoring source layer input
I0702 23:02:43.837726 13779 net.cpp:804] Ignoring source layer pool5
I0702 23:02:43.837736 13779 net.cpp:804] Ignoring source layer fc1000
I0702 23:02:43.837741 13779 net.cpp:804] Ignoring source layer fc1000_fc1000_0_split
I0702 23:02:43.837746 13779 net.cpp:804] Ignoring source layer prob
I0702 23:02:43.837752 13779 net.cpp:804] Ignoring source layer argMaxOut
W0702 23:02:43.849113 13779 parallel.cpp:400] Batch size must be divisible by the number of solvers (GPUs)
I0702 23:02:43.899621 13779 data_layer.cpp:78] ReshapePrefetch 8, 3, 640, 640
I0702 23:02:43.899670 13779 data_layer.cpp:83] output data size: 8,3,640,640
I0702 23:02:43.961908 13779 data_layer.cpp:78] ReshapePrefetch 8, 1, 640, 640
I0702 23:02:43.961966 13779 data_layer.cpp:83] output data size: 8,1,640,640
I0702 23:02:44.529709 13779 parallel.cpp:334] Starting Optimization
I0702 23:02:44.529754 13779 solver.cpp:415] Solving jsegnet21v2_train
I0702 23:02:44.529759 13779 solver.cpp:416] Learning Rate Policy: multistep
I0702 23:02:44.533639 13779 blocking_queue.cpp:50] Data layer prefetch queue empty
I0702 23:02:44.533640 13869 blocking_queue.cpp:50] Data layer prefetch queue empty
I0702 23:02:44.943545 13779 solver.cpp:290] Iteration 0 (0 iter/s, 0.413752s/100 iter), loss = 1.78075
I0702 23:02:44.943572 13779 solver.cpp:309]     Train net output #0: loss = 1.78075 (* 1 = 1.78075 loss)
I0702 23:02:44.943578 13779 sgd_solver.cpp:106] Iteration 0, lr = 0.0001
I0702 23:03:39.091298 13815 blocking_queue.cpp:50] Waiting for data
I0702 23:04:02.724092 13779 solver.cpp:290] Iteration 100 (1.2857 iter/s, 77.7785s/100 iter), loss = 0.433657
I0702 23:04:02.724118 13779 solver.cpp:309]     Train net output #0: loss = 0.433657 (* 1 = 0.433657 loss)
I0702 23:04:02.724125 13779 sgd_solver.cpp:106] Iteration 100, lr = 0.0001
I0702 23:04:33.724577 13815 blocking_queue.cpp:50] Waiting for data
I0702 23:05:10.147992 13779 solver.cpp:290] Iteration 200 (1.48319 iter/s, 67.422s/100 iter), loss = 0.121768
I0702 23:05:10.148046 13779 solver.cpp:309]     Train net output #0: loss = 0.121768 (* 1 = 0.121768 loss)
I0702 23:05:10.148054 13779 sgd_solver.cpp:106] Iteration 200, lr = 0.0001
I0702 23:05:33.480031 13779 solver.cpp:290] Iteration 300 (4.28608 iter/s, 23.3313s/100 iter), loss = 0.107143
I0702 23:05:33.480056 13779 solver.cpp:309]     Train net output #0: loss = 0.107142 (* 1 = 0.107142 loss)
I0702 23:05:33.480064 13779 sgd_solver.cpp:106] Iteration 300, lr = 0.0001
I0702 23:05:57.799996 13779 solver.cpp:290] Iteration 400 (4.11197 iter/s, 24.3193s/100 iter), loss = 0.173569
I0702 23:05:57.800052 13779 solver.cpp:309]     Train net output #0: loss = 0.173569 (* 1 = 0.173569 loss)
I0702 23:05:57.800060 13779 sgd_solver.cpp:106] Iteration 400, lr = 0.0001
I0702 23:06:22.299057 13779 solver.cpp:290] Iteration 500 (4.08191 iter/s, 24.4983s/100 iter), loss = 0.0841622
I0702 23:06:22.299083 13779 solver.cpp:309]     Train net output #0: loss = 0.0841622 (* 1 = 0.0841622 loss)
I0702 23:06:22.299090 13779 sgd_solver.cpp:106] Iteration 500, lr = 0.0001
I0702 23:06:46.850190 13779 solver.cpp:290] Iteration 600 (4.07325 iter/s, 24.5504s/100 iter), loss = 0.0529981
I0702 23:06:46.850266 13779 solver.cpp:309]     Train net output #0: loss = 0.052998 (* 1 = 0.052998 loss)
I0702 23:06:46.850276 13779 sgd_solver.cpp:106] Iteration 600, lr = 0.0001
I0702 23:07:11.366199 13779 solver.cpp:290] Iteration 700 (4.07909 iter/s, 24.5153s/100 iter), loss = 0.117636
I0702 23:07:11.366225 13779 solver.cpp:309]     Train net output #0: loss = 0.117635 (* 1 = 0.117635 loss)
I0702 23:07:11.366233 13779 sgd_solver.cpp:106] Iteration 700, lr = 0.0001
I0702 23:07:35.850114 13779 solver.cpp:290] Iteration 800 (4.08443 iter/s, 24.4832s/100 iter), loss = 0.148181
I0702 23:07:35.850162 13779 solver.cpp:309]     Train net output #0: loss = 0.148181 (* 1 = 0.148181 loss)
I0702 23:07:35.850170 13779 sgd_solver.cpp:106] Iteration 800, lr = 0.0001
I0702 23:08:00.314208 13779 solver.cpp:290] Iteration 900 (4.08774 iter/s, 24.4634s/100 iter), loss = 0.132403
I0702 23:08:00.314232 13779 solver.cpp:309]     Train net output #0: loss = 0.132403 (* 1 = 0.132403 loss)
I0702 23:08:00.314240 13779 sgd_solver.cpp:106] Iteration 900, lr = 0.0001
I0702 23:08:24.790918 13779 solver.cpp:290] Iteration 1000 (4.08563 iter/s, 24.476s/100 iter), loss = 0.0407398
I0702 23:08:24.791031 13779 solver.cpp:309]     Train net output #0: loss = 0.0407398 (* 1 = 0.0407398 loss)
I0702 23:08:24.791043 13779 sgd_solver.cpp:106] Iteration 1000, lr = 0.0001
I0702 23:08:49.325865 13779 solver.cpp:290] Iteration 1100 (4.07595 iter/s, 24.5342s/100 iter), loss = 0.0817075
I0702 23:08:49.325889 13779 solver.cpp:309]     Train net output #0: loss = 0.0817074 (* 1 = 0.0817074 loss)
I0702 23:08:49.325897 13779 sgd_solver.cpp:106] Iteration 1100, lr = 0.0001
I0702 23:09:13.830158 13779 solver.cpp:290] Iteration 1200 (4.08103 iter/s, 24.5036s/100 iter), loss = 0.118135
I0702 23:09:13.830265 13779 solver.cpp:309]     Train net output #0: loss = 0.118134 (* 1 = 0.118134 loss)
I0702 23:09:13.830274 13779 sgd_solver.cpp:106] Iteration 1200, lr = 0.0001
I0702 23:09:38.323781 13779 solver.cpp:290] Iteration 1300 (4.08282 iter/s, 24.4929s/100 iter), loss = 0.0586465
I0702 23:09:38.323803 13779 solver.cpp:309]     Train net output #0: loss = 0.0586464 (* 1 = 0.0586464 loss)
I0702 23:09:38.323810 13779 sgd_solver.cpp:106] Iteration 1300, lr = 0.0001
I0702 23:10:02.779587 13779 solver.cpp:290] Iteration 1400 (4.08912 iter/s, 24.4551s/100 iter), loss = 0.0652974
I0702 23:10:02.779676 13779 solver.cpp:309]     Train net output #0: loss = 0.0652973 (* 1 = 0.0652973 loss)
I0702 23:10:02.779686 13779 sgd_solver.cpp:106] Iteration 1400, lr = 0.0001
I0702 23:10:27.316624 13779 solver.cpp:290] Iteration 1500 (4.0756 iter/s, 24.5363s/100 iter), loss = 0.0771398
I0702 23:10:27.316649 13779 solver.cpp:309]     Train net output #0: loss = 0.0771397 (* 1 = 0.0771397 loss)
I0702 23:10:27.316656 13779 sgd_solver.cpp:106] Iteration 1500, lr = 0.0001
I0702 23:10:51.807673 13779 solver.cpp:290] Iteration 1600 (4.08324 iter/s, 24.4904s/100 iter), loss = 0.0556691
I0702 23:10:51.807749 13779 solver.cpp:309]     Train net output #0: loss = 0.055669 (* 1 = 0.055669 loss)
I0702 23:10:51.807761 13779 sgd_solver.cpp:106] Iteration 1600, lr = 0.0001
I0702 23:11:16.288795 13779 solver.cpp:290] Iteration 1700 (4.0849 iter/s, 24.4804s/100 iter), loss = 0.0555077
I0702 23:11:16.288817 13779 solver.cpp:309]     Train net output #0: loss = 0.0555077 (* 1 = 0.0555077 loss)
I0702 23:11:16.288825 13779 sgd_solver.cpp:106] Iteration 1700, lr = 0.0001
I0702 23:11:40.745383 13779 solver.cpp:290] Iteration 1800 (4.08899 iter/s, 24.4559s/100 iter), loss = 0.0703586
I0702 23:11:40.745493 13779 solver.cpp:309]     Train net output #0: loss = 0.0703586 (* 1 = 0.0703586 loss)
I0702 23:11:40.745503 13779 sgd_solver.cpp:106] Iteration 1800, lr = 0.0001
I0702 23:12:05.293540 13779 solver.cpp:290] Iteration 1900 (4.07375 iter/s, 24.5474s/100 iter), loss = 0.0579196
I0702 23:12:05.293561 13779 solver.cpp:309]     Train net output #0: loss = 0.0579196 (* 1 = 0.0579196 loss)
I0702 23:12:05.293570 13779 sgd_solver.cpp:106] Iteration 1900, lr = 0.0001
I0702 23:12:29.500013 13779 solver.cpp:473] Iteration 2000, Testing net (#0)
I0702 23:13:17.071158 13779 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.940226
I0702 23:13:17.072762 13779 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.999967
I0702 23:13:17.072772 13779 solver.cpp:546]     Test net output #2: loss = 0.100519 (* 1 = 0.100519 loss)
I0702 23:13:17.333117 13779 solver.cpp:290] Iteration 2000 (1.38816 iter/s, 72.0376s/100 iter), loss = 0.0535656
I0702 23:13:17.333140 13779 solver.cpp:309]     Train net output #0: loss = 0.0535656 (* 1 = 0.0535656 loss)
I0702 23:13:17.333148 13779 sgd_solver.cpp:106] Iteration 2000, lr = 0.0001
I0702 23:13:48.621125 13815 blocking_queue.cpp:50] Waiting for data
I0702 23:14:08.732981 13779 solver.cpp:290] Iteration 2100 (1.94558 iter/s, 51.3985s/100 iter), loss = 0.0642405
I0702 23:14:08.733011 13779 solver.cpp:309]     Train net output #0: loss = 0.0642405 (* 1 = 0.0642405 loss)
I0702 23:14:08.733021 13779 sgd_solver.cpp:106] Iteration 2100, lr = 0.0001
I0702 23:14:41.051951 13815 blocking_queue.cpp:50] Waiting for data
I0702 23:15:19.806202 13779 solver.cpp:290] Iteration 2200 (1.40704 iter/s, 71.0713s/100 iter), loss = 0.0406064
I0702 23:15:19.806258 13779 solver.cpp:309]     Train net output #0: loss = 0.0406064 (* 1 = 0.0406064 loss)
I0702 23:15:19.806267 13779 sgd_solver.cpp:106] Iteration 2200, lr = 0.0001
I0702 23:15:25.845926 13840 blocking_queue.cpp:50] Waiting for data
I0702 23:15:48.533037 13779 solver.cpp:290] Iteration 2300 (3.48117 iter/s, 28.726s/100 iter), loss = 0.0320029
I0702 23:15:48.533061 13779 solver.cpp:309]     Train net output #0: loss = 0.0320029 (* 1 = 0.0320029 loss)
I0702 23:15:48.533071 13779 sgd_solver.cpp:106] Iteration 2300, lr = 0.0001
I0702 23:16:12.623971 13779 solver.cpp:290] Iteration 2400 (4.15105 iter/s, 24.0903s/100 iter), loss = 0.0533104
I0702 23:16:12.624075 13779 solver.cpp:309]     Train net output #0: loss = 0.0533103 (* 1 = 0.0533103 loss)
I0702 23:16:12.624085 13779 sgd_solver.cpp:106] Iteration 2400, lr = 0.0001
I0702 23:16:36.787916 13779 solver.cpp:290] Iteration 2500 (4.13853 iter/s, 24.1632s/100 iter), loss = 0.0906536
I0702 23:16:36.787941 13779 solver.cpp:309]     Train net output #0: loss = 0.0906536 (* 1 = 0.0906536 loss)
I0702 23:16:36.787950 13779 sgd_solver.cpp:106] Iteration 2500, lr = 0.0001
I0702 23:17:00.920524 13779 solver.cpp:290] Iteration 2600 (4.14389 iter/s, 24.1319s/100 iter), loss = 0.0656374
I0702 23:17:00.920626 13779 solver.cpp:309]     Train net output #0: loss = 0.0656374 (* 1 = 0.0656374 loss)
I0702 23:17:00.920637 13779 sgd_solver.cpp:106] Iteration 2600, lr = 0.0001
I0702 23:17:25.088104 13779 solver.cpp:290] Iteration 2700 (4.1379 iter/s, 24.1668s/100 iter), loss = 0.038099
I0702 23:17:25.088127 13779 solver.cpp:309]     Train net output #0: loss = 0.038099 (* 1 = 0.038099 loss)
I0702 23:17:25.088135 13779 sgd_solver.cpp:106] Iteration 2700, lr = 0.0001
I0702 23:17:49.238930 13779 solver.cpp:290] Iteration 2800 (4.14076 iter/s, 24.1502s/100 iter), loss = 0.089529
I0702 23:17:49.239040 13779 solver.cpp:309]     Train net output #0: loss = 0.089529 (* 1 = 0.089529 loss)
I0702 23:17:49.239050 13779 sgd_solver.cpp:106] Iteration 2800, lr = 0.0001
I0702 23:18:13.389513 13779 solver.cpp:290] Iteration 2900 (4.14082 iter/s, 24.1498s/100 iter), loss = 0.0538159
I0702 23:18:13.389539 13779 solver.cpp:309]     Train net output #0: loss = 0.0538159 (* 1 = 0.0538159 loss)
I0702 23:18:13.389545 13779 sgd_solver.cpp:106] Iteration 2900, lr = 0.0001
I0702 23:18:37.538589 13779 solver.cpp:290] Iteration 3000 (4.14106 iter/s, 24.1484s/100 iter), loss = 0.0712822
I0702 23:18:37.538700 13779 solver.cpp:309]     Train net output #0: loss = 0.0712822 (* 1 = 0.0712822 loss)
I0702 23:18:37.538710 13779 sgd_solver.cpp:106] Iteration 3000, lr = 0.0001
I0702 23:19:01.660603 13779 solver.cpp:290] Iteration 3100 (4.14572 iter/s, 24.1213s/100 iter), loss = 0.0577893
I0702 23:19:01.660624 13779 solver.cpp:309]     Train net output #0: loss = 0.0577893 (* 1 = 0.0577893 loss)
I0702 23:19:01.660632 13779 sgd_solver.cpp:106] Iteration 3100, lr = 0.0001
I0702 23:19:25.817291 13779 solver.cpp:290] Iteration 3200 (4.13975 iter/s, 24.156s/100 iter), loss = 0.0364773
I0702 23:19:25.817414 13779 solver.cpp:309]     Train net output #0: loss = 0.0364774 (* 1 = 0.0364774 loss)
I0702 23:19:25.817423 13779 sgd_solver.cpp:106] Iteration 3200, lr = 0.0001
I0702 23:19:49.919956 13779 solver.cpp:290] Iteration 3300 (4.14905 iter/s, 24.1019s/100 iter), loss = 0.145375
I0702 23:19:49.919978 13779 solver.cpp:309]     Train net output #0: loss = 0.145375 (* 1 = 0.145375 loss)
I0702 23:19:49.919986 13779 sgd_solver.cpp:106] Iteration 3300, lr = 0.0001
I0702 23:20:14.035094 13779 solver.cpp:290] Iteration 3400 (4.14689 iter/s, 24.1145s/100 iter), loss = 0.0460306
I0702 23:20:14.035195 13779 solver.cpp:309]     Train net output #0: loss = 0.0460306 (* 1 = 0.0460306 loss)
I0702 23:20:14.035205 13779 sgd_solver.cpp:106] Iteration 3400, lr = 0.0001
I0702 23:20:38.201944 13779 solver.cpp:290] Iteration 3500 (4.13803 iter/s, 24.1661s/100 iter), loss = 0.0635301
I0702 23:20:38.201967 13779 solver.cpp:309]     Train net output #0: loss = 0.0635301 (* 1 = 0.0635301 loss)
I0702 23:20:38.201975 13779 sgd_solver.cpp:106] Iteration 3500, lr = 0.0001
I0702 23:21:02.311497 13779 solver.cpp:290] Iteration 3600 (4.14785 iter/s, 24.1089s/100 iter), loss = 0.122967
I0702 23:21:02.311609 13779 solver.cpp:309]     Train net output #0: loss = 0.122967 (* 1 = 0.122967 loss)
I0702 23:21:02.311619 13779 sgd_solver.cpp:106] Iteration 3600, lr = 0.0001
I0702 23:21:26.444140 13779 solver.cpp:290] Iteration 3700 (4.14389 iter/s, 24.1319s/100 iter), loss = 0.0399725
I0702 23:21:26.444164 13779 solver.cpp:309]     Train net output #0: loss = 0.0399725 (* 1 = 0.0399725 loss)
I0702 23:21:26.444171 13779 sgd_solver.cpp:106] Iteration 3700, lr = 0.0001
I0702 23:21:50.571995 13779 solver.cpp:290] Iteration 3800 (4.1447 iter/s, 24.1272s/100 iter), loss = 0.054645
I0702 23:21:50.572109 13779 solver.cpp:309]     Train net output #0: loss = 0.054645 (* 1 = 0.054645 loss)
I0702 23:21:50.572120 13779 sgd_solver.cpp:106] Iteration 3800, lr = 0.0001
I0702 23:22:14.685928 13779 solver.cpp:290] Iteration 3900 (4.14711 iter/s, 24.1132s/100 iter), loss = 0.0306287
I0702 23:22:14.685951 13779 solver.cpp:309]     Train net output #0: loss = 0.0306287 (* 1 = 0.0306287 loss)
I0702 23:22:14.685958 13779 sgd_solver.cpp:106] Iteration 3900, lr = 0.0001
I0702 23:22:38.614172 13779 solver.cpp:473] Iteration 4000, Testing net (#0)
I0702 23:23:26.106452 13779 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.943062
I0702 23:23:26.106536 13779 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.999989
I0702 23:23:26.106544 13779 solver.cpp:546]     Test net output #2: loss = 0.104184 (* 1 = 0.104184 loss)
I0702 23:23:26.360438 13779 solver.cpp:290] Iteration 4000 (1.39523 iter/s, 71.6726s/100 iter), loss = 0.0475856
I0702 23:23:26.360463 13779 solver.cpp:309]     Train net output #0: loss = 0.0475857 (* 1 = 0.0475857 loss)
I0702 23:23:26.360471 13779 sgd_solver.cpp:106] Iteration 4000, lr = 0.0001
I0702 23:23:49.864450 13779 solver.cpp:290] Iteration 4100 (4.25471 iter/s, 23.5034s/100 iter), loss = 0.0656268
I0702 23:23:49.864476 13779 solver.cpp:309]     Train net output #0: loss = 0.0656269 (* 1 = 0.0656269 loss)
I0702 23:23:49.864483 13779 sgd_solver.cpp:106] Iteration 4100, lr = 0.0001
I0702 23:24:34.009748 13815 blocking_queue.cpp:50] Waiting for data
I0702 23:24:44.424557 13779 solver.cpp:290] Iteration 4200 (1.83289 iter/s, 54.5586s/100 iter), loss = 0.0583775
I0702 23:24:44.424584 13779 solver.cpp:309]     Train net output #0: loss = 0.0583776 (* 1 = 0.0583776 loss)
I0702 23:24:44.424592 13779 sgd_solver.cpp:106] Iteration 4200, lr = 0.0001
I0702 23:25:08.567893 13779 solver.cpp:290] Iteration 4300 (4.14205 iter/s, 24.1427s/100 iter), loss = 0.0717098
I0702 23:25:08.568022 13779 solver.cpp:309]     Train net output #0: loss = 0.0717099 (* 1 = 0.0717099 loss)
I0702 23:25:08.568032 13779 sgd_solver.cpp:106] Iteration 4300, lr = 0.0001
I0702 23:25:32.771695 13779 solver.cpp:290] Iteration 4400 (4.13171 iter/s, 24.203s/100 iter), loss = 0.0704586
I0702 23:25:32.771719 13779 solver.cpp:309]     Train net output #0: loss = 0.0704587 (* 1 = 0.0704587 loss)
I0702 23:25:32.771728 13779 sgd_solver.cpp:106] Iteration 4400, lr = 0.0001
I0702 23:25:56.963681 13779 solver.cpp:290] Iteration 4500 (4.13371 iter/s, 24.1913s/100 iter), loss = 0.0605963
I0702 23:25:56.963734 13779 solver.cpp:309]     Train net output #0: loss = 0.0605963 (* 1 = 0.0605963 loss)
I0702 23:25:56.963745 13779 sgd_solver.cpp:106] Iteration 4500, lr = 0.0001
I0702 23:26:21.103955 13779 solver.cpp:290] Iteration 4600 (4.14257 iter/s, 24.1396s/100 iter), loss = 0.0364135
I0702 23:26:21.103978 13779 solver.cpp:309]     Train net output #0: loss = 0.0364135 (* 1 = 0.0364135 loss)
I0702 23:26:21.103986 13779 sgd_solver.cpp:106] Iteration 4600, lr = 0.0001
I0702 23:26:45.292218 13779 solver.cpp:290] Iteration 4700 (4.13435 iter/s, 24.1876s/100 iter), loss = 0.0435726
I0702 23:26:45.292349 13779 solver.cpp:309]     Train net output #0: loss = 0.0435726 (* 1 = 0.0435726 loss)
I0702 23:26:45.292364 13779 sgd_solver.cpp:106] Iteration 4700, lr = 0.0001
I0702 23:27:09.479082 13779 solver.cpp:290] Iteration 4800 (4.13461 iter/s, 24.1861s/100 iter), loss = 0.0533334
I0702 23:27:09.479109 13779 solver.cpp:309]     Train net output #0: loss = 0.0533333 (* 1 = 0.0533333 loss)
I0702 23:27:09.479115 13779 sgd_solver.cpp:106] Iteration 4800, lr = 0.0001
I0702 23:27:33.644706 13779 solver.cpp:290] Iteration 4900 (4.13822 iter/s, 24.165s/100 iter), loss = 0.0472902
I0702 23:27:33.644815 13779 solver.cpp:309]     Train net output #0: loss = 0.0472902 (* 1 = 0.0472902 loss)
I0702 23:27:33.644830 13779 sgd_solver.cpp:106] Iteration 4900, lr = 0.0001
I0702 23:27:57.845963 13779 solver.cpp:290] Iteration 5000 (4.13214 iter/s, 24.2005s/100 iter), loss = 0.0468225
I0702 23:27:57.845986 13779 solver.cpp:309]     Train net output #0: loss = 0.0468225 (* 1 = 0.0468225 loss)
I0702 23:27:57.845994 13779 sgd_solver.cpp:106] Iteration 5000, lr = 0.0001
I0702 23:28:22.003594 13779 solver.cpp:290] Iteration 5100 (4.13959 iter/s, 24.157s/100 iter), loss = 0.0397288
I0702 23:28:22.003690 13779 solver.cpp:309]     Train net output #0: loss = 0.0397288 (* 1 = 0.0397288 loss)
I0702 23:28:22.003705 13779 sgd_solver.cpp:106] Iteration 5100, lr = 0.0001
I0702 23:28:46.205267 13779 solver.cpp:290] Iteration 5200 (4.13207 iter/s, 24.2009s/100 iter), loss = 0.0421866
I0702 23:28:46.205291 13779 solver.cpp:309]     Train net output #0: loss = 0.0421866 (* 1 = 0.0421866 loss)
I0702 23:28:46.205298 13779 sgd_solver.cpp:106] Iteration 5200, lr = 0.0001
I0702 23:29:10.404304 13779 solver.cpp:290] Iteration 5300 (4.13251 iter/s, 24.1984s/100 iter), loss = 0.0547123
I0702 23:29:10.404415 13779 solver.cpp:309]     Train net output #0: loss = 0.0547123 (* 1 = 0.0547123 loss)
I0702 23:29:10.404429 13779 sgd_solver.cpp:106] Iteration 5300, lr = 0.0001
I0702 23:29:34.580229 13779 solver.cpp:290] Iteration 5400 (4.13647 iter/s, 24.1752s/100 iter), loss = 0.0548556
I0702 23:29:34.580252 13779 solver.cpp:309]     Train net output #0: loss = 0.0548555 (* 1 = 0.0548555 loss)
I0702 23:29:34.580260 13779 sgd_solver.cpp:106] Iteration 5400, lr = 0.0001
I0702 23:29:58.717377 13779 solver.cpp:290] Iteration 5500 (4.14311 iter/s, 24.1365s/100 iter), loss = 0.0436614
I0702 23:29:58.717494 13779 solver.cpp:309]     Train net output #0: loss = 0.0436614 (* 1 = 0.0436614 loss)
I0702 23:29:58.717505 13779 sgd_solver.cpp:106] Iteration 5500, lr = 0.0001
I0702 23:30:22.881502 13779 solver.cpp:290] Iteration 5600 (4.13849 iter/s, 24.1634s/100 iter), loss = 0.047574
I0702 23:30:22.881526 13779 solver.cpp:309]     Train net output #0: loss = 0.0475739 (* 1 = 0.0475739 loss)
I0702 23:30:22.881533 13779 sgd_solver.cpp:106] Iteration 5600, lr = 0.0001
I0702 23:30:47.064916 13779 solver.cpp:290] Iteration 5700 (4.13518 iter/s, 24.1828s/100 iter), loss = 0.0563022
I0702 23:30:47.065011 13779 solver.cpp:309]     Train net output #0: loss = 0.0563022 (* 1 = 0.0563022 loss)
I0702 23:30:47.065024 13779 sgd_solver.cpp:106] Iteration 5700, lr = 0.0001
I0702 23:31:11.233201 13779 solver.cpp:290] Iteration 5800 (4.13778 iter/s, 24.1676s/100 iter), loss = 0.181538
I0702 23:31:11.233224 13779 solver.cpp:309]     Train net output #0: loss = 0.181538 (* 1 = 0.181538 loss)
I0702 23:31:11.233232 13779 sgd_solver.cpp:106] Iteration 5800, lr = 0.0001
I0702 23:31:35.430907 13779 solver.cpp:290] Iteration 5900 (4.13274 iter/s, 24.197s/100 iter), loss = 0.041485
I0702 23:31:35.430961 13779 solver.cpp:309]     Train net output #0: loss = 0.0414849 (* 1 = 0.0414849 loss)
I0702 23:31:35.430970 13779 sgd_solver.cpp:106] Iteration 5900, lr = 0.0001
I0702 23:31:59.371944 13779 solver.cpp:473] Iteration 6000, Testing net (#0)
I0702 23:32:46.702782 13779 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.945313
I0702 23:32:46.702870 13779 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.999513
I0702 23:32:46.702878 13779 solver.cpp:546]     Test net output #2: loss = 0.146017 (* 1 = 0.146017 loss)
I0702 23:32:46.950688 13779 solver.cpp:290] Iteration 6000 (1.39825 iter/s, 71.5179s/100 iter), loss = 0.0521701
I0702 23:32:46.950713 13779 solver.cpp:309]     Train net output #0: loss = 0.0521701 (* 1 = 0.0521701 loss)
I0702 23:32:46.950721 13779 sgd_solver.cpp:106] Iteration 6000, lr = 0.0001
I0702 23:33:10.415491 13779 solver.cpp:290] Iteration 6100 (4.26182 iter/s, 23.4642s/100 iter), loss = 0.0467612
I0702 23:33:10.415515 13779 solver.cpp:309]     Train net output #0: loss = 0.0467612 (* 1 = 0.0467612 loss)
I0702 23:33:10.415524 13779 sgd_solver.cpp:106] Iteration 6100, lr = 0.0001
I0702 23:33:43.641671 13779 solver.cpp:290] Iteration 6200 (3.00976 iter/s, 33.2253s/100 iter), loss = 0.0750015
I0702 23:33:43.641942 13779 solver.cpp:309]     Train net output #0: loss = 0.0750015 (* 1 = 0.0750015 loss)
I0702 23:33:43.641953 13779 sgd_solver.cpp:106] Iteration 6200, lr = 0.0001
I0702 23:34:07.774092 13779 solver.cpp:290] Iteration 6300 (4.14396 iter/s, 24.1315s/100 iter), loss = 0.046345
I0702 23:34:07.774114 13779 solver.cpp:309]     Train net output #0: loss = 0.046345 (* 1 = 0.046345 loss)
I0702 23:34:07.774122 13779 sgd_solver.cpp:106] Iteration 6300, lr = 0.0001
I0702 23:34:31.944403 13779 solver.cpp:290] Iteration 6400 (4.13742 iter/s, 24.1696s/100 iter), loss = 0.119092
I0702 23:34:31.944454 13779 solver.cpp:309]     Train net output #0: loss = 0.119092 (* 1 = 0.119092 loss)
I0702 23:34:31.944463 13779 sgd_solver.cpp:106] Iteration 6400, lr = 0.0001
I0702 23:34:56.150727 13779 solver.cpp:290] Iteration 6500 (4.13127 iter/s, 24.2056s/100 iter), loss = 0.0401036
I0702 23:34:56.150751 13779 solver.cpp:309]     Train net output #0: loss = 0.0401036 (* 1 = 0.0401036 loss)
I0702 23:34:56.150758 13779 sgd_solver.cpp:106] Iteration 6500, lr = 0.0001
I0702 23:35:20.286473 13779 solver.cpp:290] Iteration 6600 (4.14335 iter/s, 24.1351s/100 iter), loss = 0.0371597
I0702 23:35:20.286556 13779 solver.cpp:309]     Train net output #0: loss = 0.0371597 (* 1 = 0.0371597 loss)
I0702 23:35:20.286566 13779 sgd_solver.cpp:106] Iteration 6600, lr = 0.0001
I0702 23:35:44.415343 13779 solver.cpp:290] Iteration 6700 (4.14454 iter/s, 24.1282s/100 iter), loss = 0.130172
I0702 23:35:44.415366 13779 solver.cpp:309]     Train net output #0: loss = 0.130172 (* 1 = 0.130172 loss)
I0702 23:35:44.415374 13779 sgd_solver.cpp:106] Iteration 6700, lr = 0.0001
I0702 23:36:08.596772 13779 solver.cpp:290] Iteration 6800 (4.13552 iter/s, 24.1808s/100 iter), loss = 0.0313324
I0702 23:36:08.596823 13779 solver.cpp:309]     Train net output #0: loss = 0.0313324 (* 1 = 0.0313324 loss)
I0702 23:36:08.596832 13779 sgd_solver.cpp:106] Iteration 6800, lr = 0.0001
I0702 23:36:32.771428 13779 solver.cpp:290] Iteration 6900 (4.13668 iter/s, 24.174s/100 iter), loss = 0.0406241
I0702 23:36:32.771455 13779 solver.cpp:309]     Train net output #0: loss = 0.040624 (* 1 = 0.040624 loss)
I0702 23:36:32.771461 13779 sgd_solver.cpp:106] Iteration 6900, lr = 0.0001
I0702 23:36:56.960052 13779 solver.cpp:290] Iteration 7000 (4.13429 iter/s, 24.188s/100 iter), loss = 0.0438908
I0702 23:36:56.960170 13779 solver.cpp:309]     Train net output #0: loss = 0.0438908 (* 1 = 0.0438908 loss)
I0702 23:36:56.960180 13779 sgd_solver.cpp:106] Iteration 7000, lr = 0.0001
I0702 23:37:21.111639 13779 solver.cpp:290] Iteration 7100 (4.14064 iter/s, 24.1508s/100 iter), loss = 0.127329
I0702 23:37:21.111660 13779 solver.cpp:309]     Train net output #0: loss = 0.127329 (* 1 = 0.127329 loss)
I0702 23:37:21.111667 13779 sgd_solver.cpp:106] Iteration 7100, lr = 0.0001
I0702 23:37:45.280558 13779 solver.cpp:290] Iteration 7200 (4.13766 iter/s, 24.1683s/100 iter), loss = 0.039989
I0702 23:37:45.280675 13779 solver.cpp:309]     Train net output #0: loss = 0.039989 (* 1 = 0.039989 loss)
I0702 23:37:45.280685 13779 sgd_solver.cpp:106] Iteration 7200, lr = 0.0001
I0702 23:38:09.422340 13779 solver.cpp:290] Iteration 7300 (4.14232 iter/s, 24.141s/100 iter), loss = 0.0392657
I0702 23:38:09.422365 13779 solver.cpp:309]     Train net output #0: loss = 0.0392657 (* 1 = 0.0392657 loss)
I0702 23:38:09.422372 13779 sgd_solver.cpp:106] Iteration 7300, lr = 0.0001
I0702 23:38:33.599251 13779 solver.cpp:290] Iteration 7400 (4.13629 iter/s, 24.1763s/100 iter), loss = 0.0326937
I0702 23:38:33.599361 13779 solver.cpp:309]     Train net output #0: loss = 0.0326937 (* 1 = 0.0326937 loss)
I0702 23:38:33.599372 13779 sgd_solver.cpp:106] Iteration 7400, lr = 0.0001
I0702 23:38:57.766278 13779 solver.cpp:290] Iteration 7500 (4.138 iter/s, 24.1663s/100 iter), loss = 0.0541119
I0702 23:38:57.766302 13779 solver.cpp:309]     Train net output #0: loss = 0.0541118 (* 1 = 0.0541118 loss)
I0702 23:38:57.766310 13779 sgd_solver.cpp:106] Iteration 7500, lr = 0.0001
I0702 23:39:21.939450 13779 solver.cpp:290] Iteration 7600 (4.13693 iter/s, 24.1725s/100 iter), loss = 0.0249413
I0702 23:39:21.939563 13779 solver.cpp:309]     Train net output #0: loss = 0.0249413 (* 1 = 0.0249413 loss)
I0702 23:39:21.939574 13779 sgd_solver.cpp:106] Iteration 7600, lr = 0.0001
I0702 23:39:46.146004 13779 solver.cpp:290] Iteration 7700 (4.13124 iter/s, 24.2058s/100 iter), loss = 0.0442923
I0702 23:39:46.146028 13779 solver.cpp:309]     Train net output #0: loss = 0.0442923 (* 1 = 0.0442923 loss)
I0702 23:39:46.146036 13779 sgd_solver.cpp:106] Iteration 7700, lr = 0.0001
I0702 23:40:10.292623 13779 solver.cpp:290] Iteration 7800 (4.14148 iter/s, 24.146s/100 iter), loss = 0.0432903
I0702 23:40:10.292724 13779 solver.cpp:309]     Train net output #0: loss = 0.0432902 (* 1 = 0.0432902 loss)
I0702 23:40:10.292734 13779 sgd_solver.cpp:106] Iteration 7800, lr = 0.0001
I0702 23:40:34.430179 13779 solver.cpp:290] Iteration 7900 (4.14305 iter/s, 24.1368s/100 iter), loss = 0.0395089
I0702 23:40:34.430208 13779 solver.cpp:309]     Train net output #0: loss = 0.0395088 (* 1 = 0.0395088 loss)
I0702 23:40:34.430218 13779 sgd_solver.cpp:106] Iteration 7900, lr = 0.0001
I0702 23:40:58.344388 13779 solver.cpp:473] Iteration 8000, Testing net (#0)
I0702 23:41:45.310365 13779 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.950123
I0702 23:41:45.310526 13779 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.99978
I0702 23:41:45.310539 13779 solver.cpp:546]     Test net output #2: loss = 0.115558 (* 1 = 0.115558 loss)
I0702 23:41:45.561110 13779 solver.cpp:290] Iteration 8000 (1.4059 iter/s, 71.129s/100 iter), loss = 0.0322065
I0702 23:41:45.561133 13779 solver.cpp:309]     Train net output #0: loss = 0.0322065 (* 1 = 0.0322065 loss)
I0702 23:41:45.561141 13779 sgd_solver.cpp:106] Iteration 8000, lr = 0.0001
I0702 23:42:09.056849 13779 solver.cpp:290] Iteration 8100 (4.25621 iter/s, 23.4951s/100 iter), loss = 0.0243085
I0702 23:42:09.056872 13779 solver.cpp:309]     Train net output #0: loss = 0.0243084 (* 1 = 0.0243084 loss)
I0702 23:42:09.056879 13779 sgd_solver.cpp:106] Iteration 8100, lr = 0.0001
I0702 23:42:33.199275 13779 solver.cpp:290] Iteration 8200 (4.1422 iter/s, 24.1418s/100 iter), loss = 0.0330328
I0702 23:42:33.199403 13779 solver.cpp:309]     Train net output #0: loss = 0.0330327 (* 1 = 0.0330327 loss)
I0702 23:42:33.199414 13779 sgd_solver.cpp:106] Iteration 8200, lr = 0.0001
I0702 23:42:57.379966 13779 solver.cpp:290] Iteration 8300 (4.13566 iter/s, 24.1799s/100 iter), loss = 0.0312198
I0702 23:42:57.379989 13779 solver.cpp:309]     Train net output #0: loss = 0.0312198 (* 1 = 0.0312198 loss)
I0702 23:42:57.379997 13779 sgd_solver.cpp:106] Iteration 8300, lr = 0.0001
I0702 23:43:21.543992 13779 solver.cpp:290] Iteration 8400 (4.1385 iter/s, 24.1634s/100 iter), loss = 0.0321853
I0702 23:43:21.544087 13779 solver.cpp:309]     Train net output #0: loss = 0.0321853 (* 1 = 0.0321853 loss)
I0702 23:43:21.544098 13779 sgd_solver.cpp:106] Iteration 8400, lr = 0.0001
I0702 23:43:45.720531 13779 solver.cpp:290] Iteration 8500 (4.13637 iter/s, 24.1758s/100 iter), loss = 0.0243596
I0702 23:43:45.720553 13779 solver.cpp:309]     Train net output #0: loss = 0.0243596 (* 1 = 0.0243596 loss)
I0702 23:43:45.720561 13779 sgd_solver.cpp:106] Iteration 8500, lr = 0.0001
I0702 23:44:09.862990 13779 solver.cpp:290] Iteration 8600 (4.14219 iter/s, 24.1418s/100 iter), loss = 0.0541097
I0702 23:44:09.863093 13779 solver.cpp:309]     Train net output #0: loss = 0.0541097 (* 1 = 0.0541097 loss)
I0702 23:44:09.863103 13779 sgd_solver.cpp:106] Iteration 8600, lr = 0.0001
I0702 23:44:34.008254 13779 solver.cpp:290] Iteration 8700 (4.14173 iter/s, 24.1445s/100 iter), loss = 0.0372143
I0702 23:44:34.008277 13779 solver.cpp:309]     Train net output #0: loss = 0.0372142 (* 1 = 0.0372142 loss)
I0702 23:44:34.008285 13779 sgd_solver.cpp:106] Iteration 8700, lr = 0.0001
I0702 23:44:58.157186 13779 solver.cpp:290] Iteration 8800 (4.14108 iter/s, 24.1483s/100 iter), loss = 0.0421834
I0702 23:44:58.157300 13779 solver.cpp:309]     Train net output #0: loss = 0.0421834 (* 1 = 0.0421834 loss)
I0702 23:44:58.157310 13779 sgd_solver.cpp:106] Iteration 8800, lr = 0.0001
I0702 23:45:22.345590 13779 solver.cpp:290] Iteration 8900 (4.13434 iter/s, 24.1877s/100 iter), loss = 0.0486177
I0702 23:45:22.345615 13779 solver.cpp:309]     Train net output #0: loss = 0.0486177 (* 1 = 0.0486177 loss)
I0702 23:45:22.345623 13779 sgd_solver.cpp:106] Iteration 8900, lr = 0.0001
I0702 23:45:46.537865 13779 solver.cpp:290] Iteration 9000 (4.13366 iter/s, 24.1916s/100 iter), loss = 0.0286085
I0702 23:45:46.537978 13779 solver.cpp:309]     Train net output #0: loss = 0.0286085 (* 1 = 0.0286085 loss)
I0702 23:45:46.537992 13779 sgd_solver.cpp:106] Iteration 9000, lr = 0.0001
I0702 23:46:10.699127 13779 solver.cpp:290] Iteration 9100 (4.13898 iter/s, 24.1605s/100 iter), loss = 0.0296295
I0702 23:46:10.699149 13779 solver.cpp:309]     Train net output #0: loss = 0.0296295 (* 1 = 0.0296295 loss)
I0702 23:46:10.699156 13779 sgd_solver.cpp:106] Iteration 9100, lr = 0.0001
I0702 23:46:34.866216 13779 solver.cpp:290] Iteration 9200 (4.13797 iter/s, 24.1664s/100 iter), loss = 0.026876
I0702 23:46:34.866323 13779 solver.cpp:309]     Train net output #0: loss = 0.0268759 (* 1 = 0.0268759 loss)
I0702 23:46:34.866333 13779 sgd_solver.cpp:106] Iteration 9200, lr = 0.0001
I0702 23:46:59.038342 13779 solver.cpp:290] Iteration 9300 (4.13712 iter/s, 24.1714s/100 iter), loss = 0.0392999
I0702 23:46:59.038365 13779 solver.cpp:309]     Train net output #0: loss = 0.0392998 (* 1 = 0.0392998 loss)
I0702 23:46:59.038373 13779 sgd_solver.cpp:106] Iteration 9300, lr = 0.0001
I0702 23:47:23.205626 13779 solver.cpp:290] Iteration 9400 (4.13794 iter/s, 24.1666s/100 iter), loss = 0.0461395
I0702 23:47:23.205734 13779 solver.cpp:309]     Train net output #0: loss = 0.0461395 (* 1 = 0.0461395 loss)
I0702 23:47:23.205744 13779 sgd_solver.cpp:106] Iteration 9400, lr = 0.0001
I0702 23:47:47.404332 13779 solver.cpp:290] Iteration 9500 (4.13258 iter/s, 24.198s/100 iter), loss = 0.0318986
I0702 23:47:47.404356 13779 solver.cpp:309]     Train net output #0: loss = 0.0318985 (* 1 = 0.0318985 loss)
I0702 23:47:47.404362 13779 sgd_solver.cpp:106] Iteration 9500, lr = 0.0001
I0702 23:48:11.553963 13779 solver.cpp:290] Iteration 9600 (4.14096 iter/s, 24.149s/100 iter), loss = 0.0481308
I0702 23:48:11.554097 13779 solver.cpp:309]     Train net output #0: loss = 0.0481308 (* 1 = 0.0481308 loss)
I0702 23:48:11.554107 13779 sgd_solver.cpp:106] Iteration 9600, lr = 0.0001
I0702 23:48:35.724839 13779 solver.cpp:290] Iteration 9700 (4.13734 iter/s, 24.1701s/100 iter), loss = 0.0276291
I0702 23:48:35.724864 13779 solver.cpp:309]     Train net output #0: loss = 0.027629 (* 1 = 0.027629 loss)
I0702 23:48:35.724874 13779 sgd_solver.cpp:106] Iteration 9700, lr = 0.0001
I0702 23:48:59.899077 13779 solver.cpp:290] Iteration 9800 (4.13675 iter/s, 24.1736s/100 iter), loss = 0.0448184
I0702 23:48:59.899183 13779 solver.cpp:309]     Train net output #0: loss = 0.0448184 (* 1 = 0.0448184 loss)
I0702 23:48:59.899194 13779 sgd_solver.cpp:106] Iteration 9800, lr = 0.0001
I0702 23:49:24.082242 13779 solver.cpp:290] Iteration 9900 (4.13523 iter/s, 24.1824s/100 iter), loss = 0.0520499
I0702 23:49:24.082265 13779 solver.cpp:309]     Train net output #0: loss = 0.0520499 (* 1 = 0.0520499 loss)
I0702 23:49:24.082273 13779 sgd_solver.cpp:106] Iteration 9900, lr = 0.0001
I0702 23:49:48.043668 13779 solver.cpp:600] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-02_23-02-42/initial/cityscapes5_jsegnet21v2_iter_10000.caffemodel
I0702 23:49:48.173094 13779 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-02_23-02-42/initial/cityscapes5_jsegnet21v2_iter_10000.solverstate
I0702 23:49:48.190333 13779 solver.cpp:473] Iteration 10000, Testing net (#0)
I0702 23:50:35.296077 13779 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.947628
I0702 23:50:35.296162 13779 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.999722
I0702 23:50:35.296172 13779 solver.cpp:546]     Test net output #2: loss = 0.136167 (* 1 = 0.136167 loss)
I0702 23:50:35.552404 13779 solver.cpp:290] Iteration 10000 (1.39922 iter/s, 71.4683s/100 iter), loss = 0.0255164
I0702 23:50:35.552428 13779 solver.cpp:309]     Train net output #0: loss = 0.0255164 (* 1 = 0.0255164 loss)
I0702 23:50:35.552435 13779 sgd_solver.cpp:106] Iteration 10000, lr = 0.0001
I0702 23:50:59.726033 13779 solver.cpp:290] Iteration 10100 (4.13685 iter/s, 24.173s/100 iter), loss = 0.0330942
I0702 23:50:59.726058 13779 solver.cpp:309]     Train net output #0: loss = 0.0330942 (* 1 = 0.0330942 loss)
I0702 23:50:59.726065 13779 sgd_solver.cpp:106] Iteration 10100, lr = 0.0001
I0702 23:51:23.834576 13779 solver.cpp:290] Iteration 10200 (4.14802 iter/s, 24.1079s/100 iter), loss = 0.0256059
I0702 23:51:23.834677 13779 solver.cpp:309]     Train net output #0: loss = 0.0256059 (* 1 = 0.0256059 loss)
I0702 23:51:23.834687 13779 sgd_solver.cpp:106] Iteration 10200, lr = 0.0001
I0702 23:51:48.014266 13779 solver.cpp:290] Iteration 10300 (4.13583 iter/s, 24.179s/100 iter), loss = 0.111669
I0702 23:51:48.014295 13779 solver.cpp:309]     Train net output #0: loss = 0.111669 (* 1 = 0.111669 loss)
I0702 23:51:48.014302 13779 sgd_solver.cpp:106] Iteration 10300, lr = 0.0001
I0702 23:52:12.207329 13779 solver.cpp:290] Iteration 10400 (4.13353 iter/s, 24.1924s/100 iter), loss = 0.0322882
I0702 23:52:12.207437 13779 solver.cpp:309]     Train net output #0: loss = 0.0322881 (* 1 = 0.0322881 loss)
I0702 23:52:12.207448 13779 sgd_solver.cpp:106] Iteration 10400, lr = 0.0001
I0702 23:52:36.419632 13779 solver.cpp:290] Iteration 10500 (4.13026 iter/s, 24.2116s/100 iter), loss = 0.038472
I0702 23:52:36.419658 13779 solver.cpp:309]     Train net output #0: loss = 0.038472 (* 1 = 0.038472 loss)
I0702 23:52:36.419665 13779 sgd_solver.cpp:106] Iteration 10500, lr = 0.0001
I0702 23:53:00.585813 13779 solver.cpp:290] Iteration 10600 (4.13813 iter/s, 24.1655s/100 iter), loss = 0.0482803
I0702 23:53:00.585928 13779 solver.cpp:309]     Train net output #0: loss = 0.0482802 (* 1 = 0.0482802 loss)
I0702 23:53:00.585939 13779 sgd_solver.cpp:106] Iteration 10600, lr = 0.0001
I0702 23:53:24.761651 13779 solver.cpp:290] Iteration 10700 (4.13649 iter/s, 24.1751s/100 iter), loss = 0.0469127
I0702 23:53:24.761675 13779 solver.cpp:309]     Train net output #0: loss = 0.0469126 (* 1 = 0.0469126 loss)
I0702 23:53:24.761682 13779 sgd_solver.cpp:106] Iteration 10700, lr = 0.0001
I0702 23:53:48.912576 13779 solver.cpp:290] Iteration 10800 (4.14074 iter/s, 24.1503s/100 iter), loss = 0.0305312
I0702 23:53:48.912708 13779 solver.cpp:309]     Train net output #0: loss = 0.0305312 (* 1 = 0.0305312 loss)
I0702 23:53:48.912719 13779 sgd_solver.cpp:106] Iteration 10800, lr = 0.0001
I0702 23:54:13.082952 13779 solver.cpp:290] Iteration 10900 (4.13743 iter/s, 24.1696s/100 iter), loss = 0.0229825
I0702 23:54:13.082974 13779 solver.cpp:309]     Train net output #0: loss = 0.0229824 (* 1 = 0.0229824 loss)
I0702 23:54:13.082981 13779 sgd_solver.cpp:106] Iteration 10900, lr = 0.0001
I0702 23:54:37.210264 13779 solver.cpp:290] Iteration 11000 (4.14479 iter/s, 24.1267s/100 iter), loss = 0.0522795
I0702 23:54:37.210304 13779 solver.cpp:309]     Train net output #0: loss = 0.0522794 (* 1 = 0.0522794 loss)
I0702 23:54:37.210311 13779 sgd_solver.cpp:106] Iteration 11000, lr = 0.0001
I0702 23:55:01.405537 13779 solver.cpp:290] Iteration 11100 (4.13315 iter/s, 24.1946s/100 iter), loss = 0.0261999
I0702 23:55:01.405560 13779 solver.cpp:309]     Train net output #0: loss = 0.0261998 (* 1 = 0.0261998 loss)
I0702 23:55:01.405566 13779 sgd_solver.cpp:106] Iteration 11100, lr = 0.0001
I0702 23:55:25.567909 13779 solver.cpp:290] Iteration 11200 (4.13878 iter/s, 24.1617s/100 iter), loss = 0.0289556
I0702 23:55:25.567965 13779 solver.cpp:309]     Train net output #0: loss = 0.0289555 (* 1 = 0.0289555 loss)
I0702 23:55:25.567976 13779 sgd_solver.cpp:106] Iteration 11200, lr = 0.0001
I0702 23:55:49.754271 13779 solver.cpp:290] Iteration 11300 (4.13468 iter/s, 24.1857s/100 iter), loss = 0.0483918
I0702 23:55:49.754297 13779 solver.cpp:309]     Train net output #0: loss = 0.0483917 (* 1 = 0.0483917 loss)
I0702 23:55:49.754303 13779 sgd_solver.cpp:106] Iteration 11300, lr = 0.0001
I0702 23:56:13.905783 13779 solver.cpp:290] Iteration 11400 (4.14064 iter/s, 24.1509s/100 iter), loss = 0.0387307
I0702 23:56:13.905822 13779 solver.cpp:309]     Train net output #0: loss = 0.0387306 (* 1 = 0.0387306 loss)
I0702 23:56:13.905829 13779 sgd_solver.cpp:106] Iteration 11400, lr = 0.0001
I0702 23:56:38.070417 13779 solver.cpp:290] Iteration 11500 (4.13839 iter/s, 24.164s/100 iter), loss = 0.0282366
I0702 23:56:38.070441 13779 solver.cpp:309]     Train net output #0: loss = 0.0282365 (* 1 = 0.0282365 loss)
I0702 23:56:38.070448 13779 sgd_solver.cpp:106] Iteration 11500, lr = 0.0001
I0702 23:57:02.346323 13779 solver.cpp:290] Iteration 11600 (4.11942 iter/s, 24.2752s/100 iter), loss = 0.0353232
I0702 23:57:02.346369 13779 solver.cpp:309]     Train net output #0: loss = 0.0353231 (* 1 = 0.0353231 loss)
I0702 23:57:02.346376 13779 sgd_solver.cpp:106] Iteration 11600, lr = 0.0001
I0702 23:57:26.534102 13779 solver.cpp:290] Iteration 11700 (4.13444 iter/s, 24.1871s/100 iter), loss = 0.0306663
I0702 23:57:26.534126 13779 solver.cpp:309]     Train net output #0: loss = 0.0306662 (* 1 = 0.0306662 loss)
I0702 23:57:26.534132 13779 sgd_solver.cpp:106] Iteration 11700, lr = 0.0001
I0702 23:57:50.660737 13779 solver.cpp:290] Iteration 11800 (4.14491 iter/s, 24.126s/100 iter), loss = 0.0485839
I0702 23:57:50.660845 13779 solver.cpp:309]     Train net output #0: loss = 0.0485838 (* 1 = 0.0485838 loss)
I0702 23:57:50.660856 13779 sgd_solver.cpp:106] Iteration 11800, lr = 0.0001
I0702 23:58:14.823622 13779 solver.cpp:290] Iteration 11900 (4.13871 iter/s, 24.1621s/100 iter), loss = 0.0403773
I0702 23:58:14.823647 13779 solver.cpp:309]     Train net output #0: loss = 0.0403772 (* 1 = 0.0403772 loss)
I0702 23:58:14.823653 13779 sgd_solver.cpp:106] Iteration 11900, lr = 0.0001
I0702 23:58:38.782969 13779 solver.cpp:473] Iteration 12000, Testing net (#0)
I0702 23:59:25.460856 13779 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.944467
I0702 23:59:25.461017 13779 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.999621
I0702 23:59:25.461028 13779 solver.cpp:546]     Test net output #2: loss = 0.142106 (* 1 = 0.142106 loss)
I0702 23:59:25.717615 13779 solver.cpp:290] Iteration 12000 (1.41059 iter/s, 70.8921s/100 iter), loss = 0.0769032
I0702 23:59:25.717638 13779 solver.cpp:309]     Train net output #0: loss = 0.0769032 (* 1 = 0.0769032 loss)
I0702 23:59:25.717644 13779 sgd_solver.cpp:106] Iteration 12000, lr = 0.0001
I0702 23:59:49.850721 13779 solver.cpp:290] Iteration 12100 (4.1438 iter/s, 24.1324s/100 iter), loss = 0.0490073
I0702 23:59:49.850746 13779 solver.cpp:309]     Train net output #0: loss = 0.0490072 (* 1 = 0.0490072 loss)
I0702 23:59:49.850754 13779 sgd_solver.cpp:106] Iteration 12100, lr = 0.0001
I0703 00:00:14.038502 13779 solver.cpp:290] Iteration 12200 (4.13443 iter/s, 24.1871s/100 iter), loss = 0.0444688
I0703 00:00:14.038553 13779 solver.cpp:309]     Train net output #0: loss = 0.0444687 (* 1 = 0.0444687 loss)
I0703 00:00:14.038561 13779 sgd_solver.cpp:106] Iteration 12200, lr = 0.0001
I0703 00:00:38.206866 13779 solver.cpp:290] Iteration 12300 (4.13776 iter/s, 24.1677s/100 iter), loss = 0.0466989
I0703 00:00:38.206889 13779 solver.cpp:309]     Train net output #0: loss = 0.0466988 (* 1 = 0.0466988 loss)
I0703 00:00:38.206897 13779 sgd_solver.cpp:106] Iteration 12300, lr = 0.0001
I0703 00:01:02.379874 13779 solver.cpp:290] Iteration 12400 (4.13696 iter/s, 24.1723s/100 iter), loss = 0.023381
I0703 00:01:02.379917 13779 solver.cpp:309]     Train net output #0: loss = 0.0233809 (* 1 = 0.0233809 loss)
I0703 00:01:02.379925 13779 sgd_solver.cpp:106] Iteration 12400, lr = 0.0001
I0703 00:01:26.618628 13779 solver.cpp:290] Iteration 12500 (4.12574 iter/s, 24.2381s/100 iter), loss = 0.023281
I0703 00:01:26.618654 13779 solver.cpp:309]     Train net output #0: loss = 0.0232809 (* 1 = 0.0232809 loss)
I0703 00:01:26.618661 13779 sgd_solver.cpp:106] Iteration 12500, lr = 0.0001
I0703 00:01:50.834470 13779 solver.cpp:290] Iteration 12600 (4.12964 iter/s, 24.2152s/100 iter), loss = 0.0420162
I0703 00:01:50.834523 13779 solver.cpp:309]     Train net output #0: loss = 0.0420161 (* 1 = 0.0420161 loss)
I0703 00:01:50.834532 13779 sgd_solver.cpp:106] Iteration 12600, lr = 0.0001
I0703 00:02:15.018177 13779 solver.cpp:290] Iteration 12700 (4.13513 iter/s, 24.183s/100 iter), loss = 0.0418404
I0703 00:02:15.018199 13779 solver.cpp:309]     Train net output #0: loss = 0.0418403 (* 1 = 0.0418403 loss)
I0703 00:02:15.018206 13779 sgd_solver.cpp:106] Iteration 12700, lr = 0.0001
I0703 00:02:39.211203 13779 solver.cpp:290] Iteration 12800 (4.13354 iter/s, 24.1924s/100 iter), loss = 0.0402074
I0703 00:02:39.211247 13779 solver.cpp:309]     Train net output #0: loss = 0.0402073 (* 1 = 0.0402073 loss)
I0703 00:02:39.211254 13779 sgd_solver.cpp:106] Iteration 12800, lr = 0.0001
I0703 00:03:03.439355 13779 solver.cpp:290] Iteration 12900 (4.12755 iter/s, 24.2275s/100 iter), loss = 0.0258465
I0703 00:03:03.439383 13779 solver.cpp:309]     Train net output #0: loss = 0.0258464 (* 1 = 0.0258464 loss)
I0703 00:03:03.439389 13779 sgd_solver.cpp:106] Iteration 12900, lr = 0.0001
I0703 00:03:27.613148 13779 solver.cpp:290] Iteration 13000 (4.13683 iter/s, 24.1731s/100 iter), loss = 0.0379678
I0703 00:03:27.613247 13779 solver.cpp:309]     Train net output #0: loss = 0.0379677 (* 1 = 0.0379677 loss)
I0703 00:03:27.613258 13779 sgd_solver.cpp:106] Iteration 13000, lr = 0.0001
I0703 00:03:51.834902 13779 solver.cpp:290] Iteration 13100 (4.12865 iter/s, 24.221s/100 iter), loss = 0.0450732
I0703 00:03:51.834924 13779 solver.cpp:309]     Train net output #0: loss = 0.0450732 (* 1 = 0.0450732 loss)
I0703 00:03:51.834933 13779 sgd_solver.cpp:106] Iteration 13100, lr = 0.0001
I0703 00:04:16.076299 13779 solver.cpp:290] Iteration 13200 (4.12529 iter/s, 24.2407s/100 iter), loss = 0.0360968
I0703 00:04:16.076504 13779 solver.cpp:309]     Train net output #0: loss = 0.0360968 (* 1 = 0.0360968 loss)
I0703 00:04:16.076514 13779 sgd_solver.cpp:106] Iteration 13200, lr = 0.0001
I0703 00:04:40.394857 13779 solver.cpp:290] Iteration 13300 (4.11223 iter/s, 24.3177s/100 iter), loss = 0.038389
I0703 00:04:40.394881 13779 solver.cpp:309]     Train net output #0: loss = 0.038389 (* 1 = 0.038389 loss)
I0703 00:04:40.394887 13779 sgd_solver.cpp:106] Iteration 13300, lr = 0.0001
I0703 00:05:04.713466 13779 solver.cpp:290] Iteration 13400 (4.11219 iter/s, 24.3179s/100 iter), loss = 0.0346904
I0703 00:05:04.713593 13779 solver.cpp:309]     Train net output #0: loss = 0.0346903 (* 1 = 0.0346903 loss)
I0703 00:05:04.713603 13779 sgd_solver.cpp:106] Iteration 13400, lr = 0.0001
I0703 00:05:28.909299 13779 solver.cpp:290] Iteration 13500 (4.13307 iter/s, 24.1951s/100 iter), loss = 0.0310236
I0703 00:05:28.909323 13779 solver.cpp:309]     Train net output #0: loss = 0.0310236 (* 1 = 0.0310236 loss)
I0703 00:05:28.909332 13779 sgd_solver.cpp:106] Iteration 13500, lr = 0.0001
I0703 00:05:53.223808 13779 solver.cpp:290] Iteration 13600 (4.11288 iter/s, 24.3138s/100 iter), loss = 0.0296147
I0703 00:05:53.223913 13779 solver.cpp:309]     Train net output #0: loss = 0.0296147 (* 1 = 0.0296147 loss)
I0703 00:05:53.223924 13779 sgd_solver.cpp:106] Iteration 13600, lr = 0.0001
I0703 00:06:17.509582 13779 solver.cpp:290] Iteration 13700 (4.11776 iter/s, 24.285s/100 iter), loss = 0.0336856
I0703 00:06:17.509609 13779 solver.cpp:309]     Train net output #0: loss = 0.0336856 (* 1 = 0.0336856 loss)
I0703 00:06:17.509616 13779 sgd_solver.cpp:106] Iteration 13700, lr = 0.0001
I0703 00:06:41.695549 13779 solver.cpp:290] Iteration 13800 (4.13474 iter/s, 24.1853s/100 iter), loss = 0.0332787
I0703 00:06:41.695657 13779 solver.cpp:309]     Train net output #0: loss = 0.0332787 (* 1 = 0.0332787 loss)
I0703 00:06:41.695668 13779 sgd_solver.cpp:106] Iteration 13800, lr = 0.0001
I0703 00:07:05.841073 13779 solver.cpp:290] Iteration 13900 (4.14168 iter/s, 24.1448s/100 iter), loss = 0.0260395
I0703 00:07:05.841096 13779 solver.cpp:309]     Train net output #0: loss = 0.0260394 (* 1 = 0.0260394 loss)
I0703 00:07:05.841104 13779 sgd_solver.cpp:106] Iteration 13900, lr = 0.0001
I0703 00:07:29.776346 13779 solver.cpp:473] Iteration 14000, Testing net (#0)
I0703 00:08:16.360644 13779 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.948359
I0703 00:08:16.360718 13779 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.999994
I0703 00:08:16.360724 13779 solver.cpp:546]     Test net output #2: loss = 0.116941 (* 1 = 0.116941 loss)
I0703 00:08:16.625695 13779 solver.cpp:290] Iteration 14000 (1.41277 iter/s, 70.7827s/100 iter), loss = 0.0521057
I0703 00:08:16.625721 13779 solver.cpp:309]     Train net output #0: loss = 0.0521056 (* 1 = 0.0521056 loss)
I0703 00:08:16.625728 13779 sgd_solver.cpp:106] Iteration 14000, lr = 0.0001
I0703 00:08:40.789089 13779 solver.cpp:290] Iteration 14100 (4.13861 iter/s, 24.1627s/100 iter), loss = 0.0261743
I0703 00:08:40.789113 13779 solver.cpp:309]     Train net output #0: loss = 0.0261743 (* 1 = 0.0261743 loss)
I0703 00:08:40.789120 13779 sgd_solver.cpp:106] Iteration 14100, lr = 0.0001
I0703 00:09:04.935784 13779 solver.cpp:290] Iteration 14200 (4.14147 iter/s, 24.146s/100 iter), loss = 0.0203192
I0703 00:09:04.935837 13779 solver.cpp:309]     Train net output #0: loss = 0.0203192 (* 1 = 0.0203192 loss)
I0703 00:09:04.935847 13779 sgd_solver.cpp:106] Iteration 14200, lr = 0.0001
I0703 00:09:29.099527 13779 solver.cpp:290] Iteration 14300 (4.13855 iter/s, 24.163s/100 iter), loss = 0.0242572
I0703 00:09:29.099550 13779 solver.cpp:309]     Train net output #0: loss = 0.0242572 (* 1 = 0.0242572 loss)
I0703 00:09:29.099558 13779 sgd_solver.cpp:106] Iteration 14300, lr = 0.0001
I0703 00:09:53.280936 13779 solver.cpp:290] Iteration 14400 (4.13552 iter/s, 24.1807s/100 iter), loss = 0.0345069
I0703 00:09:53.281044 13779 solver.cpp:309]     Train net output #0: loss = 0.0345069 (* 1 = 0.0345069 loss)
I0703 00:09:53.281055 13779 sgd_solver.cpp:106] Iteration 14400, lr = 0.0001
I0703 00:10:17.430297 13779 solver.cpp:290] Iteration 14500 (4.14103 iter/s, 24.1486s/100 iter), loss = 0.0332651
I0703 00:10:17.430320 13779 solver.cpp:309]     Train net output #0: loss = 0.0332651 (* 1 = 0.0332651 loss)
I0703 00:10:17.430327 13779 sgd_solver.cpp:106] Iteration 14500, lr = 0.0001
I0703 00:10:41.597764 13779 solver.cpp:290] Iteration 14600 (4.13791 iter/s, 24.1668s/100 iter), loss = 0.0480925
I0703 00:10:41.597892 13779 solver.cpp:309]     Train net output #0: loss = 0.0480924 (* 1 = 0.0480924 loss)
I0703 00:10:41.597901 13779 sgd_solver.cpp:106] Iteration 14600, lr = 0.0001
I0703 00:11:05.771225 13779 solver.cpp:290] Iteration 14700 (4.1369 iter/s, 24.1727s/100 iter), loss = 0.0454142
I0703 00:11:05.771247 13779 solver.cpp:309]     Train net output #0: loss = 0.0454142 (* 1 = 0.0454142 loss)
I0703 00:11:05.771255 13779 sgd_solver.cpp:106] Iteration 14700, lr = 0.0001
I0703 00:11:29.926257 13779 solver.cpp:290] Iteration 14800 (4.14004 iter/s, 24.1544s/100 iter), loss = 0.031058
I0703 00:11:29.926308 13779 solver.cpp:309]     Train net output #0: loss = 0.031058 (* 1 = 0.031058 loss)
I0703 00:11:29.926317 13779 sgd_solver.cpp:106] Iteration 14800, lr = 0.0001
I0703 00:11:54.088945 13779 solver.cpp:290] Iteration 14900 (4.13873 iter/s, 24.162s/100 iter), loss = 0.0365733
I0703 00:11:54.088968 13779 solver.cpp:309]     Train net output #0: loss = 0.0365733 (* 1 = 0.0365733 loss)
I0703 00:11:54.088975 13779 sgd_solver.cpp:106] Iteration 14900, lr = 0.0001
I0703 00:12:18.242923 13779 solver.cpp:290] Iteration 15000 (4.14022 iter/s, 24.1533s/100 iter), loss = 0.0305932
I0703 00:12:18.243029 13779 solver.cpp:309]     Train net output #0: loss = 0.0305932 (* 1 = 0.0305932 loss)
I0703 00:12:18.243039 13779 sgd_solver.cpp:106] Iteration 15000, lr = 0.0001
I0703 00:12:42.395382 13779 solver.cpp:290] Iteration 15100 (4.14049 iter/s, 24.1517s/100 iter), loss = 0.0198778
I0703 00:12:42.395408 13779 solver.cpp:309]     Train net output #0: loss = 0.0198778 (* 1 = 0.0198778 loss)
I0703 00:12:42.395417 13779 sgd_solver.cpp:106] Iteration 15100, lr = 0.0001
I0703 00:13:06.563217 13779 solver.cpp:290] Iteration 15200 (4.13785 iter/s, 24.1672s/100 iter), loss = 0.0376205
I0703 00:13:06.563329 13779 solver.cpp:309]     Train net output #0: loss = 0.0376205 (* 1 = 0.0376205 loss)
I0703 00:13:06.563340 13779 sgd_solver.cpp:106] Iteration 15200, lr = 0.0001
I0703 00:13:30.820339 13779 solver.cpp:290] Iteration 15300 (4.12263 iter/s, 24.2564s/100 iter), loss = 0.0525517
I0703 00:13:30.820363 13779 solver.cpp:309]     Train net output #0: loss = 0.0525517 (* 1 = 0.0525517 loss)
I0703 00:13:30.820370 13779 sgd_solver.cpp:106] Iteration 15300, lr = 0.0001
I0703 00:13:55.004906 13779 solver.cpp:290] Iteration 15400 (4.13498 iter/s, 24.1839s/100 iter), loss = 0.0291003
I0703 00:13:55.004987 13779 solver.cpp:309]     Train net output #0: loss = 0.0291003 (* 1 = 0.0291003 loss)
I0703 00:13:55.004995 13779 sgd_solver.cpp:106] Iteration 15400, lr = 0.0001
I0703 00:14:19.202602 13779 solver.cpp:290] Iteration 15500 (4.13275 iter/s, 24.197s/100 iter), loss = 0.0577418
I0703 00:14:19.202626 13779 solver.cpp:309]     Train net output #0: loss = 0.0577418 (* 1 = 0.0577418 loss)
I0703 00:14:19.202633 13779 sgd_solver.cpp:106] Iteration 15500, lr = 0.0001
I0703 00:14:43.395948 13779 solver.cpp:290] Iteration 15600 (4.13348 iter/s, 24.1927s/100 iter), loss = 0.0311261
I0703 00:14:43.395999 13779 solver.cpp:309]     Train net output #0: loss = 0.0311261 (* 1 = 0.0311261 loss)
I0703 00:14:43.396008 13779 sgd_solver.cpp:106] Iteration 15600, lr = 0.0001
I0703 00:15:07.551295 13779 solver.cpp:290] Iteration 15700 (4.13999 iter/s, 24.1546s/100 iter), loss = 0.0314561
I0703 00:15:07.551318 13779 solver.cpp:309]     Train net output #0: loss = 0.0314561 (* 1 = 0.0314561 loss)
I0703 00:15:07.551326 13779 sgd_solver.cpp:106] Iteration 15700, lr = 0.0001
I0703 00:15:31.700750 13779 solver.cpp:290] Iteration 15800 (4.141 iter/s, 24.1488s/100 iter), loss = 0.029358
I0703 00:15:31.700853 13779 solver.cpp:309]     Train net output #0: loss = 0.029358 (* 1 = 0.029358 loss)
I0703 00:15:31.700865 13779 sgd_solver.cpp:106] Iteration 15800, lr = 0.0001
I0703 00:15:55.938738 13779 solver.cpp:290] Iteration 15900 (4.12588 iter/s, 24.2372s/100 iter), loss = 0.0260849
I0703 00:15:55.938761 13779 solver.cpp:309]     Train net output #0: loss = 0.0260849 (* 1 = 0.0260849 loss)
I0703 00:15:55.938768 13779 sgd_solver.cpp:106] Iteration 15900, lr = 0.0001
I0703 00:16:19.877607 13779 solver.cpp:473] Iteration 16000, Testing net (#0)
I0703 00:17:06.523891 13779 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.951288
I0703 00:17:06.523946 13779 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.999139
I0703 00:17:06.523952 13779 solver.cpp:546]     Test net output #2: loss = 0.130729 (* 1 = 0.130729 loss)
I0703 00:17:06.767738 13779 solver.cpp:290] Iteration 16000 (1.41189 iter/s, 70.8271s/100 iter), loss = 0.0412796
I0703 00:17:06.767765 13779 solver.cpp:309]     Train net output #0: loss = 0.0412796 (* 1 = 0.0412796 loss)
I0703 00:17:06.767771 13779 sgd_solver.cpp:106] Iteration 16000, lr = 0.0001
I0703 00:17:30.867949 13779 solver.cpp:290] Iteration 16100 (4.14946 iter/s, 24.0995s/100 iter), loss = 0.0411171
I0703 00:17:30.867974 13779 solver.cpp:309]     Train net output #0: loss = 0.0411171 (* 1 = 0.0411171 loss)
I0703 00:17:30.867980 13779 sgd_solver.cpp:106] Iteration 16100, lr = 0.0001
I0703 00:17:55.068640 13779 solver.cpp:290] Iteration 16200 (4.13223 iter/s, 24.2s/100 iter), loss = 0.0368303
I0703 00:17:55.068750 13779 solver.cpp:309]     Train net output #0: loss = 0.0368303 (* 1 = 0.0368303 loss)
I0703 00:17:55.068761 13779 sgd_solver.cpp:106] Iteration 16200, lr = 0.0001
I0703 00:18:19.258518 13779 solver.cpp:290] Iteration 16300 (4.13409 iter/s, 24.1891s/100 iter), loss = 0.0297767
I0703 00:18:19.258541 13779 solver.cpp:309]     Train net output #0: loss = 0.0297767 (* 1 = 0.0297767 loss)
I0703 00:18:19.258548 13779 sgd_solver.cpp:106] Iteration 16300, lr = 0.0001
I0703 00:18:43.435556 13779 solver.cpp:290] Iteration 16400 (4.13627 iter/s, 24.1764s/100 iter), loss = 0.0763416
I0703 00:18:43.435637 13779 solver.cpp:309]     Train net output #0: loss = 0.0763416 (* 1 = 0.0763416 loss)
I0703 00:18:43.435648 13779 sgd_solver.cpp:106] Iteration 16400, lr = 0.0001
I0703 00:19:07.610242 13779 solver.cpp:290] Iteration 16500 (4.13668 iter/s, 24.1739s/100 iter), loss = 0.0153301
I0703 00:19:07.610266 13779 solver.cpp:309]     Train net output #0: loss = 0.0153301 (* 1 = 0.0153301 loss)
I0703 00:19:07.610275 13779 sgd_solver.cpp:106] Iteration 16500, lr = 0.0001
I0703 00:19:31.765655 13779 solver.cpp:290] Iteration 16600 (4.13998 iter/s, 24.1547s/100 iter), loss = 0.0253713
I0703 00:19:31.765758 13779 solver.cpp:309]     Train net output #0: loss = 0.0253713 (* 1 = 0.0253713 loss)
I0703 00:19:31.765769 13779 sgd_solver.cpp:106] Iteration 16600, lr = 0.0001
I0703 00:19:55.943950 13779 solver.cpp:290] Iteration 16700 (4.13607 iter/s, 24.1775s/100 iter), loss = 0.0532802
I0703 00:19:55.943975 13779 solver.cpp:309]     Train net output #0: loss = 0.0532802 (* 1 = 0.0532802 loss)
I0703 00:19:55.943982 13779 sgd_solver.cpp:106] Iteration 16700, lr = 0.0001
I0703 00:20:20.095468 13779 solver.cpp:290] Iteration 16800 (4.14064 iter/s, 24.1508s/100 iter), loss = 0.0241321
I0703 00:20:20.095546 13779 solver.cpp:309]     Train net output #0: loss = 0.0241321 (* 1 = 0.0241321 loss)
I0703 00:20:20.095554 13779 sgd_solver.cpp:106] Iteration 16800, lr = 0.0001
I0703 00:20:44.290313 13779 solver.cpp:290] Iteration 16900 (4.13324 iter/s, 24.1941s/100 iter), loss = 0.0425849
I0703 00:20:44.290339 13779 solver.cpp:309]     Train net output #0: loss = 0.0425849 (* 1 = 0.0425849 loss)
I0703 00:20:44.290346 13779 sgd_solver.cpp:106] Iteration 16900, lr = 0.0001
I0703 00:21:08.472865 13779 solver.cpp:290] Iteration 17000 (4.13533 iter/s, 24.1819s/100 iter), loss = 0.0290122
I0703 00:21:08.472980 13779 solver.cpp:309]     Train net output #0: loss = 0.0290121 (* 1 = 0.0290121 loss)
I0703 00:21:08.472990 13779 sgd_solver.cpp:106] Iteration 17000, lr = 0.0001
I0703 00:21:32.623298 13779 solver.cpp:290] Iteration 17100 (4.14084 iter/s, 24.1497s/100 iter), loss = 0.0234619
I0703 00:21:32.623322 13779 solver.cpp:309]     Train net output #0: loss = 0.0234619 (* 1 = 0.0234619 loss)
I0703 00:21:32.623330 13779 sgd_solver.cpp:106] Iteration 17100, lr = 0.0001
I0703 00:21:56.775589 13779 solver.cpp:290] Iteration 17200 (4.14051 iter/s, 24.1516s/100 iter), loss = 0.0274234
I0703 00:21:56.775708 13779 solver.cpp:309]     Train net output #0: loss = 0.0274234 (* 1 = 0.0274234 loss)
I0703 00:21:56.775719 13779 sgd_solver.cpp:106] Iteration 17200, lr = 0.0001
I0703 00:22:20.995120 13779 solver.cpp:290] Iteration 17300 (4.12903 iter/s, 24.2188s/100 iter), loss = 0.0311331
I0703 00:22:20.995146 13779 solver.cpp:309]     Train net output #0: loss = 0.0311331 (* 1 = 0.0311331 loss)
I0703 00:22:20.995152 13779 sgd_solver.cpp:106] Iteration 17300, lr = 0.0001
I0703 00:22:45.212882 13779 solver.cpp:290] Iteration 17400 (4.12932 iter/s, 24.2171s/100 iter), loss = 0.0370052
I0703 00:22:45.212990 13779 solver.cpp:309]     Train net output #0: loss = 0.0370051 (* 1 = 0.0370051 loss)
I0703 00:22:45.213001 13779 sgd_solver.cpp:106] Iteration 17400, lr = 0.0001
I0703 00:23:09.344712 13779 solver.cpp:290] Iteration 17500 (4.14403 iter/s, 24.1311s/100 iter), loss = 0.0253961
I0703 00:23:09.344739 13779 solver.cpp:309]     Train net output #0: loss = 0.0253961 (* 1 = 0.0253961 loss)
I0703 00:23:09.344746 13779 sgd_solver.cpp:106] Iteration 17500, lr = 0.0001
I0703 00:23:33.515699 13779 solver.cpp:290] Iteration 17600 (4.13731 iter/s, 24.1703s/100 iter), loss = 0.0593604
I0703 00:23:33.515806 13779 solver.cpp:309]     Train net output #0: loss = 0.0593603 (* 1 = 0.0593603 loss)
I0703 00:23:33.515817 13779 sgd_solver.cpp:106] Iteration 17600, lr = 0.0001
I0703 00:23:57.725368 13779 solver.cpp:290] Iteration 17700 (4.13071 iter/s, 24.2089s/100 iter), loss = 0.0293399
I0703 00:23:57.725394 13779 solver.cpp:309]     Train net output #0: loss = 0.0293399 (* 1 = 0.0293399 loss)
I0703 00:23:57.725400 13779 sgd_solver.cpp:106] Iteration 17700, lr = 0.0001
I0703 00:24:21.924835 13779 solver.cpp:290] Iteration 17800 (4.13244 iter/s, 24.1988s/100 iter), loss = 0.0264623
I0703 00:24:21.924944 13779 solver.cpp:309]     Train net output #0: loss = 0.0264623 (* 1 = 0.0264623 loss)
I0703 00:24:21.924955 13779 sgd_solver.cpp:106] Iteration 17800, lr = 0.0001
I0703 00:24:46.146306 13779 solver.cpp:290] Iteration 17900 (4.1287 iter/s, 24.2207s/100 iter), loss = 0.0386057
I0703 00:24:46.146332 13779 solver.cpp:309]     Train net output #0: loss = 0.0386057 (* 1 = 0.0386057 loss)
I0703 00:24:46.146342 13779 sgd_solver.cpp:106] Iteration 17900, lr = 0.0001
I0703 00:25:10.070552 13779 solver.cpp:473] Iteration 18000, Testing net (#0)
I0703 00:25:56.701376 13779 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.948156
I0703 00:25:56.701450 13779 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.999957
I0703 00:25:56.701458 13779 solver.cpp:546]     Test net output #2: loss = 0.142204 (* 1 = 0.142204 loss)
I0703 00:25:56.955337 13779 solver.cpp:290] Iteration 18000 (1.41229 iter/s, 70.8071s/100 iter), loss = 0.0340801
I0703 00:25:56.955361 13779 solver.cpp:309]     Train net output #0: loss = 0.0340801 (* 1 = 0.0340801 loss)
I0703 00:25:56.955371 13779 sgd_solver.cpp:106] Iteration 18000, lr = 0.0001
I0703 00:26:21.091219 13779 solver.cpp:290] Iteration 18100 (4.14333 iter/s, 24.1352s/100 iter), loss = 0.0327507
I0703 00:26:21.091245 13779 solver.cpp:309]     Train net output #0: loss = 0.0327507 (* 1 = 0.0327507 loss)
I0703 00:26:21.091253 13779 sgd_solver.cpp:106] Iteration 18100, lr = 0.0001
I0703 00:26:45.297258 13779 solver.cpp:290] Iteration 18200 (4.13132 iter/s, 24.2054s/100 iter), loss = 0.0430454
I0703 00:26:45.297369 13779 solver.cpp:309]     Train net output #0: loss = 0.0430453 (* 1 = 0.0430453 loss)
I0703 00:26:45.297379 13779 sgd_solver.cpp:106] Iteration 18200, lr = 0.0001
I0703 00:27:09.473340 13779 solver.cpp:290] Iteration 18300 (4.13645 iter/s, 24.1753s/100 iter), loss = 0.0215262
I0703 00:27:09.473363 13779 solver.cpp:309]     Train net output #0: loss = 0.0215262 (* 1 = 0.0215262 loss)
I0703 00:27:09.473371 13779 sgd_solver.cpp:106] Iteration 18300, lr = 0.0001
I0703 00:27:33.667819 13779 solver.cpp:290] Iteration 18400 (4.13329 iter/s, 24.1938s/100 iter), loss = 0.025691
I0703 00:27:33.667938 13779 solver.cpp:309]     Train net output #0: loss = 0.025691 (* 1 = 0.025691 loss)
I0703 00:27:33.667949 13779 sgd_solver.cpp:106] Iteration 18400, lr = 0.0001
I0703 00:27:57.834281 13779 solver.cpp:290] Iteration 18500 (4.1381 iter/s, 24.1657s/100 iter), loss = 0.0221497
I0703 00:27:57.834308 13779 solver.cpp:309]     Train net output #0: loss = 0.0221497 (* 1 = 0.0221497 loss)
I0703 00:27:57.834318 13779 sgd_solver.cpp:106] Iteration 18500, lr = 0.0001
I0703 00:28:22.046712 13779 solver.cpp:290] Iteration 18600 (4.13023 iter/s, 24.2118s/100 iter), loss = 0.0284573
I0703 00:28:22.046825 13779 solver.cpp:309]     Train net output #0: loss = 0.0284573 (* 1 = 0.0284573 loss)
I0703 00:28:22.046835 13779 sgd_solver.cpp:106] Iteration 18600, lr = 0.0001
I0703 00:28:46.206428 13779 solver.cpp:290] Iteration 18700 (4.13925 iter/s, 24.159s/100 iter), loss = 0.0290043
I0703 00:28:46.206451 13779 solver.cpp:309]     Train net output #0: loss = 0.0290043 (* 1 = 0.0290043 loss)
I0703 00:28:46.206459 13779 sgd_solver.cpp:106] Iteration 18700, lr = 0.0001
I0703 00:29:10.366572 13779 solver.cpp:290] Iteration 18800 (4.13916 iter/s, 24.1595s/100 iter), loss = 0.042997
I0703 00:29:10.366677 13779 solver.cpp:309]     Train net output #0: loss = 0.042997 (* 1 = 0.042997 loss)
I0703 00:29:10.366688 13779 sgd_solver.cpp:106] Iteration 18800, lr = 0.0001
I0703 00:29:34.508698 13779 solver.cpp:290] Iteration 18900 (4.14227 iter/s, 24.1414s/100 iter), loss = 0.0257624
I0703 00:29:34.508724 13779 solver.cpp:309]     Train net output #0: loss = 0.0257624 (* 1 = 0.0257624 loss)
I0703 00:29:34.508731 13779 sgd_solver.cpp:106] Iteration 18900, lr = 0.0001
I0703 00:29:58.658452 13779 solver.cpp:290] Iteration 19000 (4.14094 iter/s, 24.1491s/100 iter), loss = 0.0307679
I0703 00:29:58.658495 13779 solver.cpp:309]     Train net output #0: loss = 0.0307679 (* 1 = 0.0307679 loss)
I0703 00:29:58.658504 13779 sgd_solver.cpp:106] Iteration 19000, lr = 0.0001
I0703 00:30:22.838259 13779 solver.cpp:290] Iteration 19100 (4.1358 iter/s, 24.1791s/100 iter), loss = 0.0304391
I0703 00:30:22.838284 13779 solver.cpp:309]     Train net output #0: loss = 0.0304391 (* 1 = 0.0304391 loss)
I0703 00:30:22.838292 13779 sgd_solver.cpp:106] Iteration 19100, lr = 0.0001
I0703 00:30:47.026108 13779 solver.cpp:290] Iteration 19200 (4.13442 iter/s, 24.1872s/100 iter), loss = 0.0348779
I0703 00:30:47.026160 13779 solver.cpp:309]     Train net output #0: loss = 0.0348779 (* 1 = 0.0348779 loss)
I0703 00:30:47.026168 13779 sgd_solver.cpp:106] Iteration 19200, lr = 0.0001
I0703 00:31:11.182829 13779 solver.cpp:290] Iteration 19300 (4.13975 iter/s, 24.156s/100 iter), loss = 0.0326574
I0703 00:31:11.182853 13779 solver.cpp:309]     Train net output #0: loss = 0.0326574 (* 1 = 0.0326574 loss)
I0703 00:31:11.182860 13779 sgd_solver.cpp:106] Iteration 19300, lr = 0.0001
I0703 00:31:35.447942 13779 solver.cpp:290] Iteration 19400 (4.12126 iter/s, 24.2644s/100 iter), loss = 0.0418831
I0703 00:31:35.448002 13779 solver.cpp:309]     Train net output #0: loss = 0.0418831 (* 1 = 0.0418831 loss)
I0703 00:31:35.448010 13779 sgd_solver.cpp:106] Iteration 19400, lr = 0.0001
I0703 00:31:59.649606 13779 solver.cpp:290] Iteration 19500 (4.13207 iter/s, 24.201s/100 iter), loss = 0.027058
I0703 00:31:59.649631 13779 solver.cpp:309]     Train net output #0: loss = 0.027058 (* 1 = 0.027058 loss)
I0703 00:31:59.649637 13779 sgd_solver.cpp:106] Iteration 19500, lr = 0.0001
I0703 00:32:23.842020 13779 solver.cpp:290] Iteration 19600 (4.13364 iter/s, 24.1917s/100 iter), loss = 0.0306203
I0703 00:32:23.842130 13779 solver.cpp:309]     Train net output #0: loss = 0.0306203 (* 1 = 0.0306203 loss)
I0703 00:32:23.842145 13779 sgd_solver.cpp:106] Iteration 19600, lr = 0.0001
I0703 00:32:48.042476 13779 solver.cpp:290] Iteration 19700 (4.13228 iter/s, 24.1997s/100 iter), loss = 0.0408374
I0703 00:32:48.042500 13779 solver.cpp:309]     Train net output #0: loss = 0.0408374 (* 1 = 0.0408374 loss)
I0703 00:32:48.042506 13779 sgd_solver.cpp:106] Iteration 19700, lr = 0.0001
I0703 00:33:12.181731 13779 solver.cpp:290] Iteration 19800 (4.14275 iter/s, 24.1386s/100 iter), loss = 0.0385623
I0703 00:33:12.181856 13779 solver.cpp:309]     Train net output #0: loss = 0.0385623 (* 1 = 0.0385623 loss)
I0703 00:33:12.181866 13779 sgd_solver.cpp:106] Iteration 19800, lr = 0.0001
I0703 00:33:36.312343 13779 solver.cpp:290] Iteration 19900 (4.14425 iter/s, 24.1298s/100 iter), loss = 0.0218966
I0703 00:33:36.312366 13779 solver.cpp:309]     Train net output #0: loss = 0.0218966 (* 1 = 0.0218966 loss)
I0703 00:33:36.312372 13779 sgd_solver.cpp:106] Iteration 19900, lr = 0.0001
I0703 00:34:00.247042 13779 solver.cpp:600] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-02_23-02-42/initial/cityscapes5_jsegnet21v2_iter_20000.caffemodel
I0703 00:34:00.311905 13779 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-02_23-02-42/initial/cityscapes5_jsegnet21v2_iter_20000.solverstate
I0703 00:34:00.329949 13779 solver.cpp:473] Iteration 20000, Testing net (#0)
I0703 00:34:46.883056 13779 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.951865
I0703 00:34:46.883139 13779 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.999673
I0703 00:34:46.883147 13779 solver.cpp:546]     Test net output #2: loss = 0.146016 (* 1 = 0.146016 loss)
I0703 00:34:47.137537 13779 solver.cpp:290] Iteration 20000 (1.41197 iter/s, 70.8233s/100 iter), loss = 0.0128292
I0703 00:34:47.137560 13779 solver.cpp:309]     Train net output #0: loss = 0.0128292 (* 1 = 0.0128292 loss)
I0703 00:34:47.137567 13779 sgd_solver.cpp:106] Iteration 20000, lr = 0.0001
I0703 00:35:11.217018 13779 solver.cpp:290] Iteration 20100 (4.15303 iter/s, 24.0788s/100 iter), loss = 0.0392077
I0703 00:35:11.217044 13779 solver.cpp:309]     Train net output #0: loss = 0.0392078 (* 1 = 0.0392078 loss)
I0703 00:35:11.217051 13779 sgd_solver.cpp:106] Iteration 20100, lr = 0.0001
I0703 00:35:35.390445 13779 solver.cpp:290] Iteration 20200 (4.13689 iter/s, 24.1727s/100 iter), loss = 0.0210008
I0703 00:35:35.390550 13779 solver.cpp:309]     Train net output #0: loss = 0.0210008 (* 1 = 0.0210008 loss)
I0703 00:35:35.390560 13779 sgd_solver.cpp:106] Iteration 20200, lr = 0.0001
I0703 00:35:59.597362 13779 solver.cpp:290] Iteration 20300 (4.13118 iter/s, 24.2062s/100 iter), loss = 0.0121182
I0703 00:35:59.597398 13779 solver.cpp:309]     Train net output #0: loss = 0.0121182 (* 1 = 0.0121182 loss)
I0703 00:35:59.597406 13779 sgd_solver.cpp:106] Iteration 20300, lr = 0.0001
I0703 00:36:23.900501 13779 solver.cpp:290] Iteration 20400 (4.11481 iter/s, 24.3024s/100 iter), loss = 0.0443118
I0703 00:36:23.900611 13779 solver.cpp:309]     Train net output #0: loss = 0.0443118 (* 1 = 0.0443118 loss)
I0703 00:36:23.900621 13779 sgd_solver.cpp:106] Iteration 20400, lr = 0.0001
I0703 00:36:48.290158 13779 solver.cpp:290] Iteration 20500 (4.10023 iter/s, 24.3889s/100 iter), loss = 0.0293567
I0703 00:36:48.290181 13779 solver.cpp:309]     Train net output #0: loss = 0.0293567 (* 1 = 0.0293567 loss)
I0703 00:36:48.290189 13779 sgd_solver.cpp:106] Iteration 20500, lr = 0.0001
I0703 00:37:12.545316 13779 solver.cpp:290] Iteration 20600 (4.12295 iter/s, 24.2545s/100 iter), loss = 0.0246662
I0703 00:37:12.545362 13779 solver.cpp:309]     Train net output #0: loss = 0.0246663 (* 1 = 0.0246663 loss)
I0703 00:37:12.545370 13779 sgd_solver.cpp:106] Iteration 20600, lr = 0.0001
I0703 00:37:36.745662 13779 solver.cpp:290] Iteration 20700 (4.13229 iter/s, 24.1996s/100 iter), loss = 0.0251468
I0703 00:37:36.745687 13779 solver.cpp:309]     Train net output #0: loss = 0.0251468 (* 1 = 0.0251468 loss)
I0703 00:37:36.745693 13779 sgd_solver.cpp:106] Iteration 20700, lr = 0.0001
I0703 00:38:00.895920 13779 solver.cpp:290] Iteration 20800 (4.14086 iter/s, 24.1496s/100 iter), loss = 0.0323147
I0703 00:38:00.896126 13779 solver.cpp:309]     Train net output #0: loss = 0.0323147 (* 1 = 0.0323147 loss)
I0703 00:38:00.896140 13779 sgd_solver.cpp:106] Iteration 20800, lr = 0.0001
I0703 00:38:25.073227 13779 solver.cpp:290] Iteration 20900 (4.13626 iter/s, 24.1765s/100 iter), loss = 0.0313347
I0703 00:38:25.073251 13779 solver.cpp:309]     Train net output #0: loss = 0.0313348 (* 1 = 0.0313348 loss)
I0703 00:38:25.073257 13779 sgd_solver.cpp:106] Iteration 20900, lr = 0.0001
I0703 00:38:49.243969 13779 solver.cpp:290] Iteration 21000 (4.13735 iter/s, 24.1701s/100 iter), loss = 0.0250645
I0703 00:38:49.244045 13779 solver.cpp:309]     Train net output #0: loss = 0.0250645 (* 1 = 0.0250645 loss)
I0703 00:38:49.244055 13779 sgd_solver.cpp:106] Iteration 21000, lr = 0.0001
I0703 00:39:13.411789 13779 solver.cpp:290] Iteration 21100 (4.13786 iter/s, 24.1671s/100 iter), loss = 0.021332
I0703 00:39:13.411816 13779 solver.cpp:309]     Train net output #0: loss = 0.021332 (* 1 = 0.021332 loss)
I0703 00:39:13.411826 13779 sgd_solver.cpp:106] Iteration 21100, lr = 0.0001
I0703 00:39:37.587838 13779 solver.cpp:290] Iteration 21200 (4.13644 iter/s, 24.1754s/100 iter), loss = 0.0233636
I0703 00:39:37.587947 13779 solver.cpp:309]     Train net output #0: loss = 0.0233636 (* 1 = 0.0233636 loss)
I0703 00:39:37.587957 13779 sgd_solver.cpp:106] Iteration 21200, lr = 0.0001
I0703 00:40:01.756080 13779 solver.cpp:290] Iteration 21300 (4.13779 iter/s, 24.1675s/100 iter), loss = 0.0310525
I0703 00:40:01.756103 13779 solver.cpp:309]     Train net output #0: loss = 0.0310526 (* 1 = 0.0310526 loss)
I0703 00:40:01.756109 13779 sgd_solver.cpp:106] Iteration 21300, lr = 0.0001
I0703 00:40:25.918697 13779 solver.cpp:290] Iteration 21400 (4.13874 iter/s, 24.1619s/100 iter), loss = 0.0254362
I0703 00:40:25.918809 13779 solver.cpp:309]     Train net output #0: loss = 0.0254362 (* 1 = 0.0254362 loss)
I0703 00:40:25.918819 13779 sgd_solver.cpp:106] Iteration 21400, lr = 0.0001
I0703 00:40:50.114789 13779 solver.cpp:290] Iteration 21500 (4.13303 iter/s, 24.1953s/100 iter), loss = 0.032104
I0703 00:40:50.114812 13779 solver.cpp:309]     Train net output #0: loss = 0.0321041 (* 1 = 0.0321041 loss)
I0703 00:40:50.114820 13779 sgd_solver.cpp:106] Iteration 21500, lr = 0.0001
I0703 00:41:14.291661 13779 solver.cpp:290] Iteration 21600 (4.1363 iter/s, 24.1762s/100 iter), loss = 0.0377092
I0703 00:41:14.291703 13779 solver.cpp:309]     Train net output #0: loss = 0.0377092 (* 1 = 0.0377092 loss)
I0703 00:41:14.291712 13779 sgd_solver.cpp:106] Iteration 21600, lr = 0.0001
I0703 00:41:38.460515 13779 solver.cpp:290] Iteration 21700 (4.13768 iter/s, 24.1682s/100 iter), loss = 0.0438542
I0703 00:41:38.460538 13779 solver.cpp:309]     Train net output #0: loss = 0.0438542 (* 1 = 0.0438542 loss)
I0703 00:41:38.460546 13779 sgd_solver.cpp:106] Iteration 21700, lr = 0.0001
I0703 00:42:02.632253 13779 solver.cpp:290] Iteration 21800 (4.13718 iter/s, 24.1711s/100 iter), loss = 0.0550301
I0703 00:42:02.632295 13779 solver.cpp:309]     Train net output #0: loss = 0.0550301 (* 1 = 0.0550301 loss)
I0703 00:42:02.632302 13779 sgd_solver.cpp:106] Iteration 21800, lr = 0.0001
I0703 00:42:26.771292 13779 solver.cpp:290] Iteration 21900 (4.14279 iter/s, 24.1383s/100 iter), loss = 0.0235263
I0703 00:42:26.771314 13779 solver.cpp:309]     Train net output #0: loss = 0.0235263 (* 1 = 0.0235263 loss)
I0703 00:42:26.771322 13779 sgd_solver.cpp:106] Iteration 21900, lr = 0.0001
I0703 00:42:50.687418 13779 solver.cpp:473] Iteration 22000, Testing net (#0)
I0703 00:43:37.313529 13779 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.95402
I0703 00:43:37.313572 13779 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.999995
I0703 00:43:37.313580 13779 solver.cpp:546]     Test net output #2: loss = 0.114859 (* 1 = 0.114859 loss)
I0703 00:43:37.588171 13779 solver.cpp:290] Iteration 22000 (1.41213 iter/s, 70.815s/100 iter), loss = 0.0302485
I0703 00:43:37.588196 13779 solver.cpp:309]     Train net output #0: loss = 0.0302485 (* 1 = 0.0302485 loss)
I0703 00:43:37.588202 13779 sgd_solver.cpp:106] Iteration 22000, lr = 0.0001
I0703 00:44:01.712959 13779 solver.cpp:290] Iteration 22100 (4.14523 iter/s, 24.1241s/100 iter), loss = 0.0302076
I0703 00:44:01.712982 13779 solver.cpp:309]     Train net output #0: loss = 0.0302076 (* 1 = 0.0302076 loss)
I0703 00:44:01.712990 13779 sgd_solver.cpp:106] Iteration 22100, lr = 0.0001
I0703 00:44:25.880208 13779 solver.cpp:290] Iteration 22200 (4.13795 iter/s, 24.1666s/100 iter), loss = 0.0277547
I0703 00:44:25.880281 13779 solver.cpp:309]     Train net output #0: loss = 0.0277548 (* 1 = 0.0277548 loss)
I0703 00:44:25.880296 13779 sgd_solver.cpp:106] Iteration 22200, lr = 0.0001
I0703 00:44:50.038023 13779 solver.cpp:290] Iteration 22300 (4.13957 iter/s, 24.1571s/100 iter), loss = 0.0215179
I0703 00:44:50.038046 13779 solver.cpp:309]     Train net output #0: loss = 0.0215179 (* 1 = 0.0215179 loss)
I0703 00:44:50.038053 13779 sgd_solver.cpp:106] Iteration 22300, lr = 0.0001
I0703 00:45:14.191702 13779 solver.cpp:290] Iteration 22400 (4.14027 iter/s, 24.153s/100 iter), loss = 0.0312096
I0703 00:45:14.191810 13779 solver.cpp:309]     Train net output #0: loss = 0.0312097 (* 1 = 0.0312097 loss)
I0703 00:45:14.191820 13779 sgd_solver.cpp:106] Iteration 22400, lr = 0.0001
I0703 00:45:38.394645 13779 solver.cpp:290] Iteration 22500 (4.13186 iter/s, 24.2022s/100 iter), loss = 0.0256189
I0703 00:45:38.394670 13779 solver.cpp:309]     Train net output #0: loss = 0.0256189 (* 1 = 0.0256189 loss)
I0703 00:45:38.394677 13779 sgd_solver.cpp:106] Iteration 22500, lr = 0.0001
I0703 00:46:02.560600 13779 solver.cpp:290] Iteration 22600 (4.13817 iter/s, 24.1653s/100 iter), loss = 0.0241531
I0703 00:46:02.560695 13779 solver.cpp:309]     Train net output #0: loss = 0.0241532 (* 1 = 0.0241532 loss)
I0703 00:46:02.560704 13779 sgd_solver.cpp:106] Iteration 22600, lr = 0.0001
I0703 00:46:26.731762 13779 solver.cpp:290] Iteration 22700 (4.13729 iter/s, 24.1704s/100 iter), loss = 0.0240193
I0703 00:46:26.731787 13779 solver.cpp:309]     Train net output #0: loss = 0.0240193 (* 1 = 0.0240193 loss)
I0703 00:46:26.731796 13779 sgd_solver.cpp:106] Iteration 22700, lr = 0.0001
I0703 00:46:51.028792 13779 solver.cpp:290] Iteration 22800 (4.11584 iter/s, 24.2964s/100 iter), loss = 0.0237118
I0703 00:46:51.028901 13779 solver.cpp:309]     Train net output #0: loss = 0.0237119 (* 1 = 0.0237119 loss)
I0703 00:46:51.028913 13779 sgd_solver.cpp:106] Iteration 22800, lr = 0.0001
I0703 00:47:15.192592 13779 solver.cpp:290] Iteration 22900 (4.13855 iter/s, 24.163s/100 iter), loss = 0.0393145
I0703 00:47:15.192615 13779 solver.cpp:309]     Train net output #0: loss = 0.0393145 (* 1 = 0.0393145 loss)
I0703 00:47:15.192623 13779 sgd_solver.cpp:106] Iteration 22900, lr = 0.0001
I0703 00:47:39.348197 13779 solver.cpp:290] Iteration 23000 (4.13994 iter/s, 24.1549s/100 iter), loss = 0.019752
I0703 00:47:39.348310 13779 solver.cpp:309]     Train net output #0: loss = 0.019752 (* 1 = 0.019752 loss)
I0703 00:47:39.348325 13779 sgd_solver.cpp:106] Iteration 23000, lr = 0.0001
I0703 00:48:03.523097 13779 solver.cpp:290] Iteration 23100 (4.13665 iter/s, 24.1741s/100 iter), loss = 0.0385317
I0703 00:48:03.523121 13779 solver.cpp:309]     Train net output #0: loss = 0.0385318 (* 1 = 0.0385318 loss)
I0703 00:48:03.523129 13779 sgd_solver.cpp:106] Iteration 23100, lr = 0.0001
I0703 00:48:27.702047 13779 solver.cpp:290] Iteration 23200 (4.13594 iter/s, 24.1783s/100 iter), loss = 0.0318874
I0703 00:48:27.702157 13779 solver.cpp:309]     Train net output #0: loss = 0.0318874 (* 1 = 0.0318874 loss)
I0703 00:48:27.702170 13779 sgd_solver.cpp:106] Iteration 23200, lr = 0.0001
I0703 00:48:51.863256 13779 solver.cpp:290] Iteration 23300 (4.13899 iter/s, 24.1605s/100 iter), loss = 0.0293132
I0703 00:48:51.863281 13779 solver.cpp:309]     Train net output #0: loss = 0.0293133 (* 1 = 0.0293133 loss)
I0703 00:48:51.863287 13779 sgd_solver.cpp:106] Iteration 23300, lr = 0.0001
I0703 00:49:16.072332 13779 solver.cpp:290] Iteration 23400 (4.1308 iter/s, 24.2084s/100 iter), loss = 0.0173989
I0703 00:49:16.072381 13779 solver.cpp:309]     Train net output #0: loss = 0.017399 (* 1 = 0.017399 loss)
I0703 00:49:16.072388 13779 sgd_solver.cpp:106] Iteration 23400, lr = 0.0001
I0703 00:49:40.226848 13779 solver.cpp:290] Iteration 23500 (4.14013 iter/s, 24.1538s/100 iter), loss = 0.0297203
I0703 00:49:40.226872 13779 solver.cpp:309]     Train net output #0: loss = 0.0297203 (* 1 = 0.0297203 loss)
I0703 00:49:40.226879 13779 sgd_solver.cpp:106] Iteration 23500, lr = 0.0001
I0703 00:50:04.383360 13779 solver.cpp:290] Iteration 23600 (4.13979 iter/s, 24.1558s/100 iter), loss = 0.031779
I0703 00:50:04.383484 13779 solver.cpp:309]     Train net output #0: loss = 0.031779 (* 1 = 0.031779 loss)
I0703 00:50:04.383497 13779 sgd_solver.cpp:106] Iteration 23600, lr = 0.0001
I0703 00:50:28.658576 13779 solver.cpp:290] Iteration 23700 (4.11956 iter/s, 24.2744s/100 iter), loss = 0.023689
I0703 00:50:28.658598 13779 solver.cpp:309]     Train net output #0: loss = 0.023689 (* 1 = 0.023689 loss)
I0703 00:50:28.658605 13779 sgd_solver.cpp:106] Iteration 23700, lr = 0.0001
I0703 00:50:52.904894 13779 solver.cpp:290] Iteration 23800 (4.12445 iter/s, 24.2456s/100 iter), loss = 0.0241522
I0703 00:50:52.905007 13779 solver.cpp:309]     Train net output #0: loss = 0.0241522 (* 1 = 0.0241522 loss)
I0703 00:50:52.905017 13779 sgd_solver.cpp:106] Iteration 23800, lr = 0.0001
I0703 00:51:17.045632 13779 solver.cpp:290] Iteration 23900 (4.1425 iter/s, 24.14s/100 iter), loss = 0.0395136
I0703 00:51:17.045655 13779 solver.cpp:309]     Train net output #0: loss = 0.0395136 (* 1 = 0.0395136 loss)
I0703 00:51:17.045662 13779 sgd_solver.cpp:106] Iteration 23900, lr = 0.0001
I0703 00:51:40.984131 13779 solver.cpp:473] Iteration 24000, Testing net (#0)
I0703 00:52:27.592610 13779 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.953694
I0703 00:52:27.592653 13779 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.999965
I0703 00:52:27.592659 13779 solver.cpp:546]     Test net output #2: loss = 0.124169 (* 1 = 0.124169 loss)
I0703 00:52:27.854789 13779 solver.cpp:290] Iteration 24000 (1.41228 iter/s, 70.8073s/100 iter), loss = 0.0264137
I0703 00:52:27.854801 13869 sgd_solver.cpp:46] MultiStep Status: Iteration 24000, step = 1
I0703 00:52:27.854813 13779 solver.cpp:309]     Train net output #0: loss = 0.0264137 (* 1 = 0.0264137 loss)
I0703 00:52:27.854820 13779 sgd_solver.cpp:46] MultiStep Status: Iteration 24000, step = 1
I0703 00:52:27.854823 13779 sgd_solver.cpp:106] Iteration 24000, lr = 1e-05
I0703 00:52:51.955360 13779 solver.cpp:290] Iteration 24100 (4.1494 iter/s, 24.0999s/100 iter), loss = 0.0166557
I0703 00:52:51.955384 13779 solver.cpp:309]     Train net output #0: loss = 0.0166557 (* 1 = 0.0166557 loss)
I0703 00:52:51.955391 13779 sgd_solver.cpp:106] Iteration 24100, lr = 1e-05
I0703 00:53:16.099674 13779 solver.cpp:290] Iteration 24200 (4.14188 iter/s, 24.1436s/100 iter), loss = 0.025538
I0703 00:53:16.099786 13779 solver.cpp:309]     Train net output #0: loss = 0.025538 (* 1 = 0.025538 loss)
I0703 00:53:16.099797 13779 sgd_solver.cpp:106] Iteration 24200, lr = 1e-05
I0703 00:53:40.249230 13779 solver.cpp:290] Iteration 24300 (4.14099 iter/s, 24.1488s/100 iter), loss = 0.0219485
I0703 00:53:40.249254 13779 solver.cpp:309]     Train net output #0: loss = 0.0219486 (* 1 = 0.0219486 loss)
I0703 00:53:40.249261 13779 sgd_solver.cpp:106] Iteration 24300, lr = 1e-05
I0703 00:54:04.514518 13779 solver.cpp:290] Iteration 24400 (4.12123 iter/s, 24.2646s/100 iter), loss = 0.018248
I0703 00:54:04.514633 13779 solver.cpp:309]     Train net output #0: loss = 0.018248 (* 1 = 0.018248 loss)
I0703 00:54:04.514643 13779 sgd_solver.cpp:106] Iteration 24400, lr = 1e-05
I0703 00:54:28.795617 13779 solver.cpp:290] Iteration 24500 (4.11856 iter/s, 24.2803s/100 iter), loss = 0.0319882
I0703 00:54:28.795642 13779 solver.cpp:309]     Train net output #0: loss = 0.0319882 (* 1 = 0.0319882 loss)
I0703 00:54:28.795650 13779 sgd_solver.cpp:106] Iteration 24500, lr = 1e-05
I0703 00:54:52.974011 13779 solver.cpp:290] Iteration 24600 (4.13604 iter/s, 24.1777s/100 iter), loss = 0.0235675
I0703 00:54:52.974123 13779 solver.cpp:309]     Train net output #0: loss = 0.0235675 (* 1 = 0.0235675 loss)
I0703 00:54:52.974133 13779 sgd_solver.cpp:106] Iteration 24600, lr = 1e-05
I0703 00:55:17.145627 13779 solver.cpp:290] Iteration 24700 (4.13721 iter/s, 24.1709s/100 iter), loss = 0.0308665
I0703 00:55:17.145649 13779 solver.cpp:309]     Train net output #0: loss = 0.0308665 (* 1 = 0.0308665 loss)
I0703 00:55:17.145656 13779 sgd_solver.cpp:106] Iteration 24700, lr = 1e-05
I0703 00:55:41.371948 13779 solver.cpp:290] Iteration 24800 (4.12786 iter/s, 24.2256s/100 iter), loss = 0.016625
I0703 00:55:41.372017 13779 solver.cpp:309]     Train net output #0: loss = 0.0166251 (* 1 = 0.0166251 loss)
I0703 00:55:41.372025 13779 sgd_solver.cpp:106] Iteration 24800, lr = 1e-05
I0703 00:56:05.521744 13779 solver.cpp:290] Iteration 24900 (4.14094 iter/s, 24.1491s/100 iter), loss = 0.0222772
I0703 00:56:05.521769 13779 solver.cpp:309]     Train net output #0: loss = 0.0222772 (* 1 = 0.0222772 loss)
I0703 00:56:05.521775 13779 sgd_solver.cpp:106] Iteration 24900, lr = 1e-05
I0703 00:56:29.671002 13779 solver.cpp:290] Iteration 25000 (4.14103 iter/s, 24.1486s/100 iter), loss = 0.0346808
I0703 00:56:29.671114 13779 solver.cpp:309]     Train net output #0: loss = 0.0346809 (* 1 = 0.0346809 loss)
I0703 00:56:29.671125 13779 sgd_solver.cpp:106] Iteration 25000, lr = 1e-05
I0703 00:56:53.839869 13779 solver.cpp:290] Iteration 25100 (4.13768 iter/s, 24.1681s/100 iter), loss = 0.0170978
I0703 00:56:53.839892 13779 solver.cpp:309]     Train net output #0: loss = 0.0170978 (* 1 = 0.0170978 loss)
I0703 00:56:53.839900 13779 sgd_solver.cpp:106] Iteration 25100, lr = 1e-05
I0703 00:57:18.009886 13779 solver.cpp:290] Iteration 25200 (4.13747 iter/s, 24.1694s/100 iter), loss = 0.0265607
I0703 00:57:18.009986 13779 solver.cpp:309]     Train net output #0: loss = 0.0265608 (* 1 = 0.0265608 loss)
I0703 00:57:18.009995 13779 sgd_solver.cpp:106] Iteration 25200, lr = 1e-05
I0703 00:57:42.279796 13779 solver.cpp:290] Iteration 25300 (4.12046 iter/s, 24.2692s/100 iter), loss = 0.04381
I0703 00:57:42.279820 13779 solver.cpp:309]     Train net output #0: loss = 0.04381 (* 1 = 0.04381 loss)
I0703 00:57:42.279827 13779 sgd_solver.cpp:106] Iteration 25300, lr = 1e-05
I0703 00:58:06.411377 13779 solver.cpp:290] Iteration 25400 (4.14406 iter/s, 24.1309s/100 iter), loss = 0.0175286
I0703 00:58:06.411432 13779 solver.cpp:309]     Train net output #0: loss = 0.0175286 (* 1 = 0.0175286 loss)
I0703 00:58:06.411440 13779 sgd_solver.cpp:106] Iteration 25400, lr = 1e-05
I0703 00:58:30.595194 13779 solver.cpp:290] Iteration 25500 (4.13512 iter/s, 24.1831s/100 iter), loss = 0.0307658
I0703 00:58:30.595218 13779 solver.cpp:309]     Train net output #0: loss = 0.0307658 (* 1 = 0.0307658 loss)
I0703 00:58:30.595226 13779 sgd_solver.cpp:106] Iteration 25500, lr = 1e-05
I0703 00:58:54.738919 13779 solver.cpp:290] Iteration 25600 (4.14198 iter/s, 24.1431s/100 iter), loss = 0.0323086
I0703 00:58:54.738977 13779 solver.cpp:309]     Train net output #0: loss = 0.0323086 (* 1 = 0.0323086 loss)
I0703 00:58:54.738986 13779 sgd_solver.cpp:106] Iteration 25600, lr = 1e-05
I0703 00:59:18.928082 13779 solver.cpp:290] Iteration 25700 (4.1342 iter/s, 24.1885s/100 iter), loss = 0.0330083
I0703 00:59:18.928107 13779 solver.cpp:309]     Train net output #0: loss = 0.0330083 (* 1 = 0.0330083 loss)
I0703 00:59:18.928114 13779 sgd_solver.cpp:106] Iteration 25700, lr = 1e-05
I0703 00:59:43.173612 13779 solver.cpp:290] Iteration 25800 (4.12459 iter/s, 24.2449s/100 iter), loss = 0.0307969
I0703 00:59:43.173717 13779 solver.cpp:309]     Train net output #0: loss = 0.0307969 (* 1 = 0.0307969 loss)
I0703 00:59:43.173727 13779 sgd_solver.cpp:106] Iteration 25800, lr = 1e-05
I0703 01:00:07.334487 13779 solver.cpp:290] Iteration 25900 (4.13905 iter/s, 24.1601s/100 iter), loss = 0.023846
I0703 01:00:07.334511 13779 solver.cpp:309]     Train net output #0: loss = 0.023846 (* 1 = 0.023846 loss)
I0703 01:00:07.334518 13779 sgd_solver.cpp:106] Iteration 25900, lr = 1e-05
I0703 01:00:31.264948 13779 solver.cpp:473] Iteration 26000, Testing net (#0)
I0703 01:01:20.933001 13779 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.954159
I0703 01:01:20.933184 13779 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.999975
I0703 01:01:20.933194 13779 solver.cpp:546]     Test net output #2: loss = 0.140118 (* 1 = 0.140118 loss)
I0703 01:01:21.188012 13779 solver.cpp:290] Iteration 26000 (1.35407 iter/s, 73.8515s/100 iter), loss = 0.023295
I0703 01:01:21.188064 13779 solver.cpp:309]     Train net output #0: loss = 0.023295 (* 1 = 0.023295 loss)
I0703 01:01:21.188087 13779 sgd_solver.cpp:106] Iteration 26000, lr = 1e-05
I0703 01:01:45.583776 13779 solver.cpp:290] Iteration 26100 (4.09919 iter/s, 24.3951s/100 iter), loss = 0.0169904
I0703 01:01:45.583799 13779 solver.cpp:309]     Train net output #0: loss = 0.0169904 (* 1 = 0.0169904 loss)
I0703 01:01:45.583806 13779 sgd_solver.cpp:106] Iteration 26100, lr = 1e-05
I0703 01:02:09.804961 13779 solver.cpp:290] Iteration 26200 (4.12873 iter/s, 24.2205s/100 iter), loss = 0.0262295
I0703 01:02:09.805073 13779 solver.cpp:309]     Train net output #0: loss = 0.0262296 (* 1 = 0.0262296 loss)
I0703 01:02:09.805083 13779 sgd_solver.cpp:106] Iteration 26200, lr = 1e-05
I0703 01:02:34.074272 13779 solver.cpp:290] Iteration 26300 (4.12056 iter/s, 24.2685s/100 iter), loss = 0.0335413
I0703 01:02:34.074295 13779 solver.cpp:309]     Train net output #0: loss = 0.0335413 (* 1 = 0.0335413 loss)
I0703 01:02:34.074302 13779 sgd_solver.cpp:106] Iteration 26300, lr = 1e-05
I0703 01:02:58.255046 13779 solver.cpp:290] Iteration 26400 (4.13563 iter/s, 24.1801s/100 iter), loss = 0.0183117
I0703 01:02:58.255156 13779 solver.cpp:309]     Train net output #0: loss = 0.0183117 (* 1 = 0.0183117 loss)
I0703 01:02:58.255167 13779 sgd_solver.cpp:106] Iteration 26400, lr = 1e-05
I0703 01:03:22.402809 13779 solver.cpp:290] Iteration 26500 (4.1413 iter/s, 24.147s/100 iter), loss = 0.025803
I0703 01:03:22.402837 13779 solver.cpp:309]     Train net output #0: loss = 0.025803 (* 1 = 0.025803 loss)
I0703 01:03:22.402846 13779 sgd_solver.cpp:106] Iteration 26500, lr = 1e-05
I0703 01:03:46.617785 13779 solver.cpp:290] Iteration 26600 (4.12979 iter/s, 24.2143s/100 iter), loss = 0.0400401
I0703 01:03:46.617869 13779 solver.cpp:309]     Train net output #0: loss = 0.0400401 (* 1 = 0.0400401 loss)
I0703 01:03:46.617879 13779 sgd_solver.cpp:106] Iteration 26600, lr = 1e-05
I0703 01:04:10.831933 13779 solver.cpp:290] Iteration 26700 (4.12994 iter/s, 24.2134s/100 iter), loss = 0.015901
I0703 01:04:10.831957 13779 solver.cpp:309]     Train net output #0: loss = 0.015901 (* 1 = 0.015901 loss)
I0703 01:04:10.831964 13779 sgd_solver.cpp:106] Iteration 26700, lr = 1e-05
I0703 01:04:34.998608 13779 solver.cpp:290] Iteration 26800 (4.13804 iter/s, 24.166s/100 iter), loss = 0.0292571
I0703 01:04:34.998661 13779 solver.cpp:309]     Train net output #0: loss = 0.0292571 (* 1 = 0.0292571 loss)
I0703 01:04:34.998669 13779 sgd_solver.cpp:106] Iteration 26800, lr = 1e-05
I0703 01:04:59.161015 13779 solver.cpp:290] Iteration 26900 (4.13878 iter/s, 24.1617s/100 iter), loss = 0.0303091
I0703 01:04:59.161039 13779 solver.cpp:309]     Train net output #0: loss = 0.0303091 (* 1 = 0.0303091 loss)
I0703 01:04:59.161046 13779 sgd_solver.cpp:106] Iteration 26900, lr = 1e-05
I0703 01:05:23.367312 13779 solver.cpp:290] Iteration 27000 (4.13127 iter/s, 24.2056s/100 iter), loss = 0.0264759
I0703 01:05:23.367422 13779 solver.cpp:309]     Train net output #0: loss = 0.0264759 (* 1 = 0.0264759 loss)
I0703 01:05:23.367432 13779 sgd_solver.cpp:106] Iteration 27000, lr = 1e-05
I0703 01:05:47.634969 13779 solver.cpp:290] Iteration 27100 (4.12084 iter/s, 24.2669s/100 iter), loss = 0.0245516
I0703 01:05:47.634997 13779 solver.cpp:309]     Train net output #0: loss = 0.0245516 (* 1 = 0.0245516 loss)
I0703 01:05:47.635004 13779 sgd_solver.cpp:106] Iteration 27100, lr = 1e-05
I0703 01:06:11.914808 13779 solver.cpp:290] Iteration 27200 (4.11876 iter/s, 24.2792s/100 iter), loss = 0.0314758
I0703 01:06:11.914919 13779 solver.cpp:309]     Train net output #0: loss = 0.0314758 (* 1 = 0.0314758 loss)
I0703 01:06:11.914930 13779 sgd_solver.cpp:106] Iteration 27200, lr = 1e-05
I0703 01:06:36.144637 13779 solver.cpp:290] Iteration 27300 (4.12727 iter/s, 24.2291s/100 iter), loss = 0.0211298
I0703 01:06:36.144661 13779 solver.cpp:309]     Train net output #0: loss = 0.0211298 (* 1 = 0.0211298 loss)
I0703 01:06:36.144668 13779 sgd_solver.cpp:106] Iteration 27300, lr = 1e-05
I0703 01:07:00.309785 13779 solver.cpp:290] Iteration 27400 (4.13831 iter/s, 24.1645s/100 iter), loss = 0.0255148
I0703 01:07:00.309914 13779 solver.cpp:309]     Train net output #0: loss = 0.0255148 (* 1 = 0.0255148 loss)
I0703 01:07:00.309924 13779 sgd_solver.cpp:106] Iteration 27400, lr = 1e-05
I0703 01:07:24.481238 13779 solver.cpp:290] Iteration 27500 (4.13724 iter/s, 24.1707s/100 iter), loss = 0.0331433
I0703 01:07:24.481261 13779 solver.cpp:309]     Train net output #0: loss = 0.0331434 (* 1 = 0.0331434 loss)
I0703 01:07:24.481268 13779 sgd_solver.cpp:106] Iteration 27500, lr = 1e-05
I0703 01:07:48.625279 13779 solver.cpp:290] Iteration 27600 (4.14193 iter/s, 24.1433s/100 iter), loss = 0.0282266
I0703 01:07:48.625329 13779 solver.cpp:309]     Train net output #0: loss = 0.0282267 (* 1 = 0.0282267 loss)
I0703 01:07:48.625339 13779 sgd_solver.cpp:106] Iteration 27600, lr = 1e-05
I0703 01:08:12.762171 13779 solver.cpp:290] Iteration 27700 (4.14316 iter/s, 24.1362s/100 iter), loss = 0.0249148
I0703 01:08:12.762193 13779 solver.cpp:309]     Train net output #0: loss = 0.0249148 (* 1 = 0.0249148 loss)
I0703 01:08:12.762200 13779 sgd_solver.cpp:106] Iteration 27700, lr = 1e-05
I0703 01:08:36.951421 13779 solver.cpp:290] Iteration 27800 (4.13419 iter/s, 24.1885s/100 iter), loss = 0.0229094
I0703 01:08:36.951489 13779 solver.cpp:309]     Train net output #0: loss = 0.0229094 (* 1 = 0.0229094 loss)
I0703 01:08:36.951498 13779 sgd_solver.cpp:106] Iteration 27800, lr = 1e-05
I0703 01:09:01.114390 13779 solver.cpp:290] Iteration 27900 (4.13869 iter/s, 24.1622s/100 iter), loss = 0.025317
I0703 01:09:01.114413 13779 solver.cpp:309]     Train net output #0: loss = 0.025317 (* 1 = 0.025317 loss)
I0703 01:09:01.114421 13779 sgd_solver.cpp:106] Iteration 27900, lr = 1e-05
I0703 01:09:25.057199 13779 solver.cpp:473] Iteration 28000, Testing net (#0)
I0703 01:10:11.582721 13779 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.954415
I0703 01:10:11.582805 13779 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.999785
I0703 01:10:11.582813 13779 solver.cpp:546]     Test net output #2: loss = 0.148606 (* 1 = 0.148606 loss)
I0703 01:10:11.833220 13779 solver.cpp:290] Iteration 28000 (1.41409 iter/s, 70.7168s/100 iter), loss = 0.0267997
I0703 01:10:11.833241 13779 solver.cpp:309]     Train net output #0: loss = 0.0267997 (* 1 = 0.0267997 loss)
I0703 01:10:11.833248 13779 sgd_solver.cpp:106] Iteration 28000, lr = 1e-05
I0703 01:10:35.319494 13779 solver.cpp:290] Iteration 28100 (4.25793 iter/s, 23.4856s/100 iter), loss = 0.0259186
I0703 01:10:35.319521 13779 solver.cpp:309]     Train net output #0: loss = 0.0259186 (* 1 = 0.0259186 loss)
I0703 01:10:35.319528 13779 sgd_solver.cpp:106] Iteration 28100, lr = 1e-05
I0703 01:10:59.505043 13779 solver.cpp:290] Iteration 28200 (4.13482 iter/s, 24.1848s/100 iter), loss = 0.0279785
I0703 01:10:59.505151 13779 solver.cpp:309]     Train net output #0: loss = 0.0279785 (* 1 = 0.0279785 loss)
I0703 01:10:59.505162 13779 sgd_solver.cpp:106] Iteration 28200, lr = 1e-05
I0703 01:11:23.743326 13779 solver.cpp:290] Iteration 28300 (4.12584 iter/s, 24.2375s/100 iter), loss = 0.0231631
I0703 01:11:23.743352 13779 solver.cpp:309]     Train net output #0: loss = 0.0231631 (* 1 = 0.0231631 loss)
I0703 01:11:23.743360 13779 sgd_solver.cpp:106] Iteration 28300, lr = 1e-05
I0703 01:11:47.998965 13779 solver.cpp:290] Iteration 28400 (4.12288 iter/s, 24.2549s/100 iter), loss = 0.0194882
I0703 01:11:47.999053 13779 solver.cpp:309]     Train net output #0: loss = 0.0194882 (* 1 = 0.0194882 loss)
I0703 01:11:47.999074 13779 sgd_solver.cpp:106] Iteration 28400, lr = 1e-05
I0703 01:12:12.329766 13779 solver.cpp:290] Iteration 28500 (4.11015 iter/s, 24.33s/100 iter), loss = 0.0267676
I0703 01:12:12.329789 13779 solver.cpp:309]     Train net output #0: loss = 0.0267676 (* 1 = 0.0267676 loss)
I0703 01:12:12.329797 13779 sgd_solver.cpp:106] Iteration 28500, lr = 1e-05
I0703 01:12:36.550221 13779 solver.cpp:290] Iteration 28600 (4.12886 iter/s, 24.2197s/100 iter), loss = 0.0166107
I0703 01:12:36.550294 13779 solver.cpp:309]     Train net output #0: loss = 0.0166107 (* 1 = 0.0166107 loss)
I0703 01:12:36.550303 13779 sgd_solver.cpp:106] Iteration 28600, lr = 1e-05
I0703 01:13:00.796172 13779 solver.cpp:290] Iteration 28700 (4.12453 iter/s, 24.2452s/100 iter), loss = 0.0298468
I0703 01:13:00.796196 13779 solver.cpp:309]     Train net output #0: loss = 0.0298468 (* 1 = 0.0298468 loss)
I0703 01:13:00.796203 13779 sgd_solver.cpp:106] Iteration 28700, lr = 1e-05
I0703 01:13:25.012809 13779 solver.cpp:290] Iteration 28800 (4.12951 iter/s, 24.2159s/100 iter), loss = 0.0199554
I0703 01:13:25.012955 13779 solver.cpp:309]     Train net output #0: loss = 0.0199554 (* 1 = 0.0199554 loss)
I0703 01:13:25.012976 13779 sgd_solver.cpp:106] Iteration 28800, lr = 1e-05
I0703 01:13:49.228834 13779 solver.cpp:290] Iteration 28900 (4.12964 iter/s, 24.2152s/100 iter), loss = 0.020952
I0703 01:13:49.228857 13779 solver.cpp:309]     Train net output #0: loss = 0.020952 (* 1 = 0.020952 loss)
I0703 01:13:49.228864 13779 sgd_solver.cpp:106] Iteration 28900, lr = 1e-05
I0703 01:14:13.412087 13779 solver.cpp:290] Iteration 29000 (4.13521 iter/s, 24.1826s/100 iter), loss = 0.0200087
I0703 01:14:13.412194 13779 solver.cpp:309]     Train net output #0: loss = 0.0200087 (* 1 = 0.0200087 loss)
I0703 01:14:13.412204 13779 sgd_solver.cpp:106] Iteration 29000, lr = 1e-05
I0703 01:14:37.563202 13779 solver.cpp:290] Iteration 29100 (4.14073 iter/s, 24.1503s/100 iter), loss = 0.0165588
I0703 01:14:37.563227 13779 solver.cpp:309]     Train net output #0: loss = 0.0165588 (* 1 = 0.0165588 loss)
I0703 01:14:37.563235 13779 sgd_solver.cpp:106] Iteration 29100, lr = 1e-05
I0703 01:15:01.743239 13779 solver.cpp:290] Iteration 29200 (4.13576 iter/s, 24.1793s/100 iter), loss = 0.0347305
I0703 01:15:01.743342 13779 solver.cpp:309]     Train net output #0: loss = 0.0347305 (* 1 = 0.0347305 loss)
I0703 01:15:01.743357 13779 sgd_solver.cpp:106] Iteration 29200, lr = 1e-05
I0703 01:15:25.900540 13779 solver.cpp:290] Iteration 29300 (4.13967 iter/s, 24.1565s/100 iter), loss = 0.0355822
I0703 01:15:25.900564 13779 solver.cpp:309]     Train net output #0: loss = 0.0355822 (* 1 = 0.0355822 loss)
I0703 01:15:25.900571 13779 sgd_solver.cpp:106] Iteration 29300, lr = 1e-05
I0703 01:15:50.065194 13779 solver.cpp:290] Iteration 29400 (4.13839 iter/s, 24.164s/100 iter), loss = 0.0254183
I0703 01:15:50.065312 13779 solver.cpp:309]     Train net output #0: loss = 0.0254183 (* 1 = 0.0254183 loss)
I0703 01:15:50.065326 13779 sgd_solver.cpp:106] Iteration 29400, lr = 1e-05
I0703 01:16:14.289355 13779 solver.cpp:290] Iteration 29500 (4.12825 iter/s, 24.2234s/100 iter), loss = 0.0274391
I0703 01:16:14.289399 13779 solver.cpp:309]     Train net output #0: loss = 0.0274391 (* 1 = 0.0274391 loss)
I0703 01:16:14.289415 13779 sgd_solver.cpp:106] Iteration 29500, lr = 1e-05
I0703 01:16:38.668822 13779 solver.cpp:290] Iteration 29600 (4.10193 iter/s, 24.3787s/100 iter), loss = 0.0219041
I0703 01:16:38.668931 13779 solver.cpp:309]     Train net output #0: loss = 0.0219041 (* 1 = 0.0219041 loss)
I0703 01:16:38.668942 13779 sgd_solver.cpp:106] Iteration 29600, lr = 1e-05
I0703 01:17:02.981750 13779 solver.cpp:290] Iteration 29700 (4.11317 iter/s, 24.3121s/100 iter), loss = 0.0288469
I0703 01:17:02.981775 13779 solver.cpp:309]     Train net output #0: loss = 0.0288469 (* 1 = 0.0288469 loss)
I0703 01:17:02.981781 13779 sgd_solver.cpp:106] Iteration 29700, lr = 1e-05
I0703 01:17:27.147462 13779 solver.cpp:290] Iteration 29800 (4.13821 iter/s, 24.165s/100 iter), loss = 0.0273427
I0703 01:17:27.147570 13779 solver.cpp:309]     Train net output #0: loss = 0.0273427 (* 1 = 0.0273427 loss)
I0703 01:17:27.147584 13779 sgd_solver.cpp:106] Iteration 29800, lr = 1e-05
I0703 01:17:51.373028 13779 solver.cpp:290] Iteration 29900 (4.128 iter/s, 24.2248s/100 iter), loss = 0.0245696
I0703 01:17:51.373051 13779 solver.cpp:309]     Train net output #0: loss = 0.0245696 (* 1 = 0.0245696 loss)
I0703 01:17:51.373059 13779 sgd_solver.cpp:106] Iteration 29900, lr = 1e-05
I0703 01:18:15.271648 13779 solver.cpp:600] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-02_23-02-42/initial/cityscapes5_jsegnet21v2_iter_30000.caffemodel
I0703 01:18:15.299340 13779 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-02_23-02-42/initial/cityscapes5_jsegnet21v2_iter_30000.solverstate
I0703 01:18:15.316315 13779 solver.cpp:473] Iteration 30000, Testing net (#0)
I0703 01:19:01.967605 13779 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.954793
I0703 01:19:01.967695 13779 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.999793
I0703 01:19:01.967706 13779 solver.cpp:546]     Test net output #2: loss = 0.146515 (* 1 = 0.146515 loss)
I0703 01:19:02.219898 13779 solver.cpp:290] Iteration 30000 (1.41153 iter/s, 70.8449s/100 iter), loss = 0.0254025
I0703 01:19:02.219925 13779 solver.cpp:309]     Train net output #0: loss = 0.0254025 (* 1 = 0.0254025 loss)
I0703 01:19:02.219933 13779 sgd_solver.cpp:106] Iteration 30000, lr = 1e-05
I0703 01:19:25.698051 13779 solver.cpp:290] Iteration 30100 (4.2594 iter/s, 23.4775s/100 iter), loss = 0.0261522
I0703 01:19:25.698074 13779 solver.cpp:309]     Train net output #0: loss = 0.0261522 (* 1 = 0.0261522 loss)
I0703 01:19:25.698082 13779 sgd_solver.cpp:106] Iteration 30100, lr = 1e-05
I0703 01:19:49.889587 13779 solver.cpp:290] Iteration 30200 (4.1338 iter/s, 24.1908s/100 iter), loss = 0.0267954
I0703 01:19:49.889695 13779 solver.cpp:309]     Train net output #0: loss = 0.0267954 (* 1 = 0.0267954 loss)
I0703 01:19:49.889706 13779 sgd_solver.cpp:106] Iteration 30200, lr = 1e-05
I0703 01:20:14.066532 13779 solver.cpp:290] Iteration 30300 (4.13631 iter/s, 24.1762s/100 iter), loss = 0.0285043
I0703 01:20:14.066560 13779 solver.cpp:309]     Train net output #0: loss = 0.0285043 (* 1 = 0.0285043 loss)
I0703 01:20:14.066567 13779 sgd_solver.cpp:106] Iteration 30300, lr = 1e-05
I0703 01:20:38.212982 13779 solver.cpp:290] Iteration 30400 (4.14152 iter/s, 24.1457s/100 iter), loss = 0.0350156
I0703 01:20:38.213083 13779 solver.cpp:309]     Train net output #0: loss = 0.0350156 (* 1 = 0.0350156 loss)
I0703 01:20:38.213093 13779 sgd_solver.cpp:106] Iteration 30400, lr = 1e-05
I0703 01:21:02.402736 13779 solver.cpp:290] Iteration 30500 (4.13411 iter/s, 24.189s/100 iter), loss = 0.0293875
I0703 01:21:02.402760 13779 solver.cpp:309]     Train net output #0: loss = 0.0293875 (* 1 = 0.0293875 loss)
I0703 01:21:02.402766 13779 sgd_solver.cpp:106] Iteration 30500, lr = 1e-05
I0703 01:21:26.568475 13779 solver.cpp:290] Iteration 30600 (4.13821 iter/s, 24.165s/100 iter), loss = 0.0210223
I0703 01:21:26.568514 13779 solver.cpp:309]     Train net output #0: loss = 0.0210223 (* 1 = 0.0210223 loss)
I0703 01:21:26.568522 13779 sgd_solver.cpp:106] Iteration 30600, lr = 1e-05
I0703 01:21:50.728703 13779 solver.cpp:290] Iteration 30700 (4.13915 iter/s, 24.1595s/100 iter), loss = 0.0304843
I0703 01:21:50.728729 13779 solver.cpp:309]     Train net output #0: loss = 0.0304843 (* 1 = 0.0304843 loss)
I0703 01:21:50.728737 13779 sgd_solver.cpp:106] Iteration 30700, lr = 1e-05
I0703 01:22:14.917178 13779 solver.cpp:290] Iteration 30800 (4.13432 iter/s, 24.1878s/100 iter), loss = 0.0233428
I0703 01:22:14.917291 13779 solver.cpp:309]     Train net output #0: loss = 0.0233428 (* 1 = 0.0233428 loss)
I0703 01:22:14.917302 13779 sgd_solver.cpp:106] Iteration 30800, lr = 1e-05
I0703 01:22:39.075938 13779 solver.cpp:290] Iteration 30900 (4.13942 iter/s, 24.158s/100 iter), loss = 0.0420963
I0703 01:22:39.075961 13779 solver.cpp:309]     Train net output #0: loss = 0.0420963 (* 1 = 0.0420963 loss)
I0703 01:22:39.075968 13779 sgd_solver.cpp:106] Iteration 30900, lr = 1e-05
I0703 01:23:03.208891 13779 solver.cpp:290] Iteration 31000 (4.14383 iter/s, 24.1323s/100 iter), loss = 0.0348484
I0703 01:23:03.209002 13779 solver.cpp:309]     Train net output #0: loss = 0.0348484 (* 1 = 0.0348484 loss)
I0703 01:23:03.209012 13779 sgd_solver.cpp:106] Iteration 31000, lr = 1e-05
I0703 01:23:27.361438 13779 solver.cpp:290] Iteration 31100 (4.14048 iter/s, 24.1518s/100 iter), loss = 0.0247979
I0703 01:23:27.361462 13779 solver.cpp:309]     Train net output #0: loss = 0.0247979 (* 1 = 0.0247979 loss)
I0703 01:23:27.361469 13779 sgd_solver.cpp:106] Iteration 31100, lr = 1e-05
I0703 01:23:51.520144 13779 solver.cpp:290] Iteration 31200 (4.13941 iter/s, 24.158s/100 iter), loss = 0.0189387
I0703 01:23:51.530369 13779 solver.cpp:309]     Train net output #0: loss = 0.0189387 (* 1 = 0.0189387 loss)
I0703 01:23:51.530416 13779 sgd_solver.cpp:106] Iteration 31200, lr = 1e-05
I0703 01:24:15.676286 13779 solver.cpp:290] Iteration 31300 (4.14159 iter/s, 24.1453s/100 iter), loss = 0.0274857
I0703 01:24:15.676309 13779 solver.cpp:309]     Train net output #0: loss = 0.0274857 (* 1 = 0.0274857 loss)
I0703 01:24:15.676317 13779 sgd_solver.cpp:106] Iteration 31300, lr = 1e-05
I0703 01:24:39.892921 13779 solver.cpp:290] Iteration 31400 (4.12951 iter/s, 24.216s/100 iter), loss = 0.0190852
I0703 01:24:39.892966 13779 solver.cpp:309]     Train net output #0: loss = 0.0190852 (* 1 = 0.0190852 loss)
I0703 01:24:39.892973 13779 sgd_solver.cpp:106] Iteration 31400, lr = 1e-05
I0703 01:25:04.046023 13779 solver.cpp:290] Iteration 31500 (4.14038 iter/s, 24.1524s/100 iter), loss = 0.0159122
I0703 01:25:04.046046 13779 solver.cpp:309]     Train net output #0: loss = 0.0159122 (* 1 = 0.0159122 loss)
I0703 01:25:04.046054 13779 sgd_solver.cpp:106] Iteration 31500, lr = 1e-05
I0703 01:25:28.214152 13779 solver.cpp:290] Iteration 31600 (4.1378 iter/s, 24.1674s/100 iter), loss = 0.0258262
I0703 01:25:28.214258 13779 solver.cpp:309]     Train net output #0: loss = 0.0258262 (* 1 = 0.0258262 loss)
I0703 01:25:28.214268 13779 sgd_solver.cpp:106] Iteration 31600, lr = 1e-05
I0703 01:25:52.359244 13779 solver.cpp:290] Iteration 31700 (4.14176 iter/s, 24.1443s/100 iter), loss = 0.0250027
I0703 01:25:52.359271 13779 solver.cpp:309]     Train net output #0: loss = 0.0250027 (* 1 = 0.0250027 loss)
I0703 01:25:52.359279 13779 sgd_solver.cpp:106] Iteration 31700, lr = 1e-05
I0703 01:26:16.509021 13779 solver.cpp:290] Iteration 31800 (4.14094 iter/s, 24.1491s/100 iter), loss = 0.02098
I0703 01:26:16.509145 13779 solver.cpp:309]     Train net output #0: loss = 0.02098 (* 1 = 0.02098 loss)
I0703 01:26:16.509160 13779 sgd_solver.cpp:106] Iteration 31800, lr = 1e-05
I0703 01:26:40.711241 13779 solver.cpp:290] Iteration 31900 (4.13199 iter/s, 24.2014s/100 iter), loss = 0.0164478
I0703 01:26:40.711263 13779 solver.cpp:309]     Train net output #0: loss = 0.0164478 (* 1 = 0.0164478 loss)
I0703 01:26:40.711271 13779 sgd_solver.cpp:106] Iteration 31900, lr = 1e-05
I0703 01:27:04.672876 13779 solver.cpp:600] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-02_23-02-42/initial/cityscapes5_jsegnet21v2_iter_32000.caffemodel
I0703 01:27:04.697863 13779 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-02_23-02-42/initial/cityscapes5_jsegnet21v2_iter_32000.solverstate
I0703 01:27:04.783983 13779 solver.cpp:453] Iteration 32000, loss = 0.0203892
I0703 01:27:04.784005 13779 solver.cpp:473] Iteration 32000, Testing net (#0)
I0703 01:27:51.328760 13779 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.955501
I0703 01:27:51.328833 13779 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.999708
I0703 01:27:51.328840 13779 solver.cpp:546]     Test net output #2: loss = 0.141078 (* 1 = 0.141078 loss)
I0703 01:27:51.328843 13779 solver.cpp:458] Optimization Done.
I0703 01:27:51.446749 13779 caffe.cpp:246] Optimization Done.
training/cityscapes5_jsegnet21v2_2017-07-02_23-02-42/sparse
I0703 01:28:03.574080 31050 caffe.cpp:209] Using GPUs 0, 1
I0703 01:28:03.576458 31050 caffe.cpp:214] GPU 0: GeForce GTX 1080
I0703 01:28:03.576812 31050 caffe.cpp:214] GPU 1: GeForce GTX 1080
I0703 01:28:04.459381 31050 solver.cpp:48] Initializing solver from parameters: 
train_net: "training/cityscapes5_jsegnet21v2_2017-07-02_23-02-42/sparse/train.prototxt"
test_net: "training/cityscapes5_jsegnet21v2_2017-07-02_23-02-42/sparse/test.prototxt"
test_iter: 125
test_interval: 2000
base_lr: 1e-05
display: 100
max_iter: 32000
lr_policy: "multistep"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 1e-05
snapshot: 10000
snapshot_prefix: "training/cityscapes5_jsegnet21v2_2017-07-02_23-02-42/sparse/cityscapes5_jsegnet21v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
regularization_type: "L1"
test_initialization: false
stepvalue: 24000
iter_size: 1
type: "Adam"
display_sparsity: 1000
sparse_mode: SPARSE_UPDATE
sparsity_target: 0.8
sparsity_step_factor: 0.05
sparsity_step_iter: 1000
sparsity_start_iter: 4000
sparsity_start_factor: 0
I0703 01:28:04.482028 31050 solver.cpp:82] Creating training net from train_net file: training/cityscapes5_jsegnet21v2_2017-07-02_23-02-42/sparse/train.prototxt
I0703 01:28:04.490907 31050 net.cpp:327] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0703 01:28:04.490917 31050 net.cpp:327] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
W0703 01:28:04.490933 31050 parallel.cpp:400] Batch size must be divisible by the number of solvers (GPUs)
I0703 01:28:04.491334 31050 net.cpp:56] Initializing net from parameters: 
name: "jsegnet21v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/train-image-lmdb"
    label_list_path: "data/train-label-lmdb"
    batch_size: 8
    threads: 4
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b/bn"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a/bn"
  top: "out5a/bn"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a/bn"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b/bn"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a/bn"
  top: "out3a/bn"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a/bn"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv1/bn"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv2/bn"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv3/bn"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4/bn"
  top: "ctx_conv4/bn"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4/bn"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
I0703 01:28:04.494552 31050 layer_factory.hpp:77] Creating layer data
I0703 01:28:04.494566 31050 net.cpp:98] Creating Layer data
I0703 01:28:04.494571 31050 net.cpp:413] data -> data
I0703 01:28:04.494590 31050 net.cpp:413] data -> label
I0703 01:28:04.529078 31118 db_lmdb.cpp:35] Opened lmdb data/train-image-lmdb
I0703 01:28:04.529698 31123 db_lmdb.cpp:35] Opened lmdb data/train-label-lmdb
I0703 01:28:04.543244 31050 data_layer.cpp:78] ReshapePrefetch 8, 3, 640, 640
I0703 01:28:04.543464 31050 data_layer.cpp:83] output data size: 8,3,640,640
I0703 01:28:04.603737 31050 data_layer.cpp:78] ReshapePrefetch 8, 1, 640, 640
I0703 01:28:04.603785 31050 data_layer.cpp:83] output data size: 8,1,640,640
I0703 01:28:04.612304 31128 blocking_queue.cpp:50] Waiting for data
I0703 01:28:04.620362 31050 net.cpp:148] Setting up data
I0703 01:28:04.620383 31050 net.cpp:155] Top shape: 8 3 640 640 (9830400)
I0703 01:28:04.620386 31050 net.cpp:155] Top shape: 8 1 640 640 (3276800)
I0703 01:28:04.620388 31050 net.cpp:163] Memory required for data: 52428800
I0703 01:28:04.620396 31050 layer_factory.hpp:77] Creating layer data/bias
I0703 01:28:04.620407 31050 net.cpp:98] Creating Layer data/bias
I0703 01:28:04.620411 31050 net.cpp:439] data/bias <- data
I0703 01:28:04.620420 31050 net.cpp:413] data/bias -> data/bias
I0703 01:28:04.621345 31050 net.cpp:148] Setting up data/bias
I0703 01:28:04.621353 31050 net.cpp:155] Top shape: 8 3 640 640 (9830400)
I0703 01:28:04.621356 31050 net.cpp:163] Memory required for data: 91750400
I0703 01:28:04.621363 31050 layer_factory.hpp:77] Creating layer conv1a
I0703 01:28:04.621372 31050 net.cpp:98] Creating Layer conv1a
I0703 01:28:04.621376 31050 net.cpp:439] conv1a <- data/bias
I0703 01:28:04.621378 31050 net.cpp:413] conv1a -> conv1a
I0703 01:28:04.622454 31050 net.cpp:148] Setting up conv1a
I0703 01:28:04.622465 31050 net.cpp:155] Top shape: 8 32 320 320 (26214400)
I0703 01:28:04.622467 31050 net.cpp:163] Memory required for data: 196608000
I0703 01:28:04.622473 31050 layer_factory.hpp:77] Creating layer conv1a/bn
I0703 01:28:04.622510 31050 net.cpp:98] Creating Layer conv1a/bn
I0703 01:28:04.622514 31050 net.cpp:439] conv1a/bn <- conv1a
I0703 01:28:04.622519 31050 net.cpp:413] conv1a/bn -> conv1a/bn
I0703 01:28:04.624825 31050 net.cpp:148] Setting up conv1a/bn
I0703 01:28:04.624836 31050 net.cpp:155] Top shape: 8 32 320 320 (26214400)
I0703 01:28:04.624840 31050 net.cpp:163] Memory required for data: 301465600
I0703 01:28:04.624846 31050 layer_factory.hpp:77] Creating layer conv1a/relu
I0703 01:28:04.624852 31050 net.cpp:98] Creating Layer conv1a/relu
I0703 01:28:04.624855 31050 net.cpp:439] conv1a/relu <- conv1a/bn
I0703 01:28:04.624860 31050 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0703 01:28:04.624871 31050 net.cpp:148] Setting up conv1a/relu
I0703 01:28:04.624876 31050 net.cpp:155] Top shape: 8 32 320 320 (26214400)
I0703 01:28:04.624877 31050 net.cpp:163] Memory required for data: 406323200
I0703 01:28:04.624879 31050 layer_factory.hpp:77] Creating layer conv1b
I0703 01:28:04.624894 31050 net.cpp:98] Creating Layer conv1b
I0703 01:28:04.624898 31050 net.cpp:439] conv1b <- conv1a/bn
I0703 01:28:04.624902 31050 net.cpp:413] conv1b -> conv1b
I0703 01:28:04.625190 31050 net.cpp:148] Setting up conv1b
I0703 01:28:04.625196 31050 net.cpp:155] Top shape: 8 32 320 320 (26214400)
I0703 01:28:04.625198 31050 net.cpp:163] Memory required for data: 511180800
I0703 01:28:04.625203 31050 layer_factory.hpp:77] Creating layer conv1b/bn
I0703 01:28:04.625208 31050 net.cpp:98] Creating Layer conv1b/bn
I0703 01:28:04.625211 31050 net.cpp:439] conv1b/bn <- conv1b
I0703 01:28:04.625213 31050 net.cpp:413] conv1b/bn -> conv1b/bn
I0703 01:28:04.625720 31050 net.cpp:148] Setting up conv1b/bn
I0703 01:28:04.625725 31050 net.cpp:155] Top shape: 8 32 320 320 (26214400)
I0703 01:28:04.625726 31050 net.cpp:163] Memory required for data: 616038400
I0703 01:28:04.625731 31050 layer_factory.hpp:77] Creating layer conv1b/relu
I0703 01:28:04.625735 31050 net.cpp:98] Creating Layer conv1b/relu
I0703 01:28:04.625737 31050 net.cpp:439] conv1b/relu <- conv1b/bn
I0703 01:28:04.625741 31050 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0703 01:28:04.625743 31050 net.cpp:148] Setting up conv1b/relu
I0703 01:28:04.625746 31050 net.cpp:155] Top shape: 8 32 320 320 (26214400)
I0703 01:28:04.625747 31050 net.cpp:163] Memory required for data: 720896000
I0703 01:28:04.625751 31050 layer_factory.hpp:77] Creating layer pool1
I0703 01:28:04.625761 31050 net.cpp:98] Creating Layer pool1
I0703 01:28:04.625762 31050 net.cpp:439] pool1 <- conv1b/bn
I0703 01:28:04.625766 31050 net.cpp:413] pool1 -> pool1
I0703 01:28:04.626039 31050 net.cpp:148] Setting up pool1
I0703 01:28:04.626045 31050 net.cpp:155] Top shape: 8 32 160 160 (6553600)
I0703 01:28:04.626049 31050 net.cpp:163] Memory required for data: 747110400
I0703 01:28:04.626050 31050 layer_factory.hpp:77] Creating layer res2a_branch2a
I0703 01:28:04.626055 31050 net.cpp:98] Creating Layer res2a_branch2a
I0703 01:28:04.626058 31050 net.cpp:439] res2a_branch2a <- pool1
I0703 01:28:04.626061 31050 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0703 01:28:04.627339 31050 net.cpp:148] Setting up res2a_branch2a
I0703 01:28:04.627348 31050 net.cpp:155] Top shape: 8 64 160 160 (13107200)
I0703 01:28:04.627351 31050 net.cpp:163] Memory required for data: 799539200
I0703 01:28:04.627355 31050 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0703 01:28:04.627360 31050 net.cpp:98] Creating Layer res2a_branch2a/bn
I0703 01:28:04.627363 31050 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0703 01:28:04.627367 31050 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0703 01:28:04.627845 31050 net.cpp:148] Setting up res2a_branch2a/bn
I0703 01:28:04.627851 31050 net.cpp:155] Top shape: 8 64 160 160 (13107200)
I0703 01:28:04.627852 31050 net.cpp:163] Memory required for data: 851968000
I0703 01:28:04.627857 31050 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0703 01:28:04.627861 31050 net.cpp:98] Creating Layer res2a_branch2a/relu
I0703 01:28:04.627863 31050 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0703 01:28:04.627866 31050 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0703 01:28:04.627871 31050 net.cpp:148] Setting up res2a_branch2a/relu
I0703 01:28:04.627873 31050 net.cpp:155] Top shape: 8 64 160 160 (13107200)
I0703 01:28:04.627876 31050 net.cpp:163] Memory required for data: 904396800
I0703 01:28:04.627877 31050 layer_factory.hpp:77] Creating layer res2a_branch2b
I0703 01:28:04.627882 31050 net.cpp:98] Creating Layer res2a_branch2b
I0703 01:28:04.627884 31050 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0703 01:28:04.627887 31050 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0703 01:28:04.628957 31050 net.cpp:148] Setting up res2a_branch2b
I0703 01:28:04.628967 31050 net.cpp:155] Top shape: 8 64 160 160 (13107200)
I0703 01:28:04.628968 31050 net.cpp:163] Memory required for data: 956825600
I0703 01:28:04.628973 31050 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0703 01:28:04.628985 31050 net.cpp:98] Creating Layer res2a_branch2b/bn
I0703 01:28:04.628988 31050 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0703 01:28:04.628991 31050 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0703 01:28:04.629472 31050 net.cpp:148] Setting up res2a_branch2b/bn
I0703 01:28:04.629477 31050 net.cpp:155] Top shape: 8 64 160 160 (13107200)
I0703 01:28:04.629479 31050 net.cpp:163] Memory required for data: 1009254400
I0703 01:28:04.629485 31050 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0703 01:28:04.629488 31050 net.cpp:98] Creating Layer res2a_branch2b/relu
I0703 01:28:04.629492 31050 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0703 01:28:04.629494 31050 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0703 01:28:04.629498 31050 net.cpp:148] Setting up res2a_branch2b/relu
I0703 01:28:04.629501 31050 net.cpp:155] Top shape: 8 64 160 160 (13107200)
I0703 01:28:04.629503 31050 net.cpp:163] Memory required for data: 1061683200
I0703 01:28:04.629505 31050 layer_factory.hpp:77] Creating layer pool2
I0703 01:28:04.629508 31050 net.cpp:98] Creating Layer pool2
I0703 01:28:04.629511 31050 net.cpp:439] pool2 <- res2a_branch2b/bn
I0703 01:28:04.629513 31050 net.cpp:413] pool2 -> pool2
I0703 01:28:04.629540 31050 net.cpp:148] Setting up pool2
I0703 01:28:04.629544 31050 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0703 01:28:04.629546 31050 net.cpp:163] Memory required for data: 1074790400
I0703 01:28:04.629549 31050 layer_factory.hpp:77] Creating layer res3a_branch2a
I0703 01:28:04.629554 31050 net.cpp:98] Creating Layer res3a_branch2a
I0703 01:28:04.629556 31050 net.cpp:439] res3a_branch2a <- pool2
I0703 01:28:04.629559 31050 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0703 01:28:04.631212 31050 net.cpp:148] Setting up res3a_branch2a
I0703 01:28:04.631218 31050 net.cpp:155] Top shape: 8 128 80 80 (6553600)
I0703 01:28:04.631220 31050 net.cpp:163] Memory required for data: 1101004800
I0703 01:28:04.631224 31050 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0703 01:28:04.631227 31050 net.cpp:98] Creating Layer res3a_branch2a/bn
I0703 01:28:04.631230 31050 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0703 01:28:04.631234 31050 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0703 01:28:04.631695 31050 net.cpp:148] Setting up res3a_branch2a/bn
I0703 01:28:04.631705 31050 net.cpp:155] Top shape: 8 128 80 80 (6553600)
I0703 01:28:04.631707 31050 net.cpp:163] Memory required for data: 1127219200
I0703 01:28:04.631719 31050 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0703 01:28:04.631724 31050 net.cpp:98] Creating Layer res3a_branch2a/relu
I0703 01:28:04.631727 31050 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0703 01:28:04.631731 31050 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0703 01:28:04.631739 31050 net.cpp:148] Setting up res3a_branch2a/relu
I0703 01:28:04.631743 31050 net.cpp:155] Top shape: 8 128 80 80 (6553600)
I0703 01:28:04.631747 31050 net.cpp:163] Memory required for data: 1153433600
I0703 01:28:04.631749 31050 layer_factory.hpp:77] Creating layer res3a_branch2b
I0703 01:28:04.631755 31050 net.cpp:98] Creating Layer res3a_branch2b
I0703 01:28:04.631759 31050 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0703 01:28:04.631767 31050 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0703 01:28:04.632817 31050 net.cpp:148] Setting up res3a_branch2b
I0703 01:28:04.632824 31050 net.cpp:155] Top shape: 8 128 80 80 (6553600)
I0703 01:28:04.632827 31050 net.cpp:163] Memory required for data: 1179648000
I0703 01:28:04.632832 31050 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0703 01:28:04.632835 31050 net.cpp:98] Creating Layer res3a_branch2b/bn
I0703 01:28:04.632838 31050 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0703 01:28:04.632843 31050 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0703 01:28:04.633273 31050 net.cpp:148] Setting up res3a_branch2b/bn
I0703 01:28:04.633280 31050 net.cpp:155] Top shape: 8 128 80 80 (6553600)
I0703 01:28:04.633281 31050 net.cpp:163] Memory required for data: 1205862400
I0703 01:28:04.633292 31050 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0703 01:28:04.633296 31050 net.cpp:98] Creating Layer res3a_branch2b/relu
I0703 01:28:04.633299 31050 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0703 01:28:04.633302 31050 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0703 01:28:04.633306 31050 net.cpp:148] Setting up res3a_branch2b/relu
I0703 01:28:04.633309 31050 net.cpp:155] Top shape: 8 128 80 80 (6553600)
I0703 01:28:04.633311 31050 net.cpp:163] Memory required for data: 1232076800
I0703 01:28:04.633313 31050 layer_factory.hpp:77] Creating layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0703 01:28:04.633317 31050 net.cpp:98] Creating Layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0703 01:28:04.633319 31050 net.cpp:439] res3a_branch2b/bn_res3a_branch2b/relu_0_split <- res3a_branch2b/bn
I0703 01:28:04.633322 31050 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0703 01:28:04.633324 31050 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0703 01:28:04.633350 31050 net.cpp:148] Setting up res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0703 01:28:04.633354 31050 net.cpp:155] Top shape: 8 128 80 80 (6553600)
I0703 01:28:04.633357 31050 net.cpp:155] Top shape: 8 128 80 80 (6553600)
I0703 01:28:04.633359 31050 net.cpp:163] Memory required for data: 1284505600
I0703 01:28:04.633361 31050 layer_factory.hpp:77] Creating layer pool3
I0703 01:28:04.633364 31050 net.cpp:98] Creating Layer pool3
I0703 01:28:04.633368 31050 net.cpp:439] pool3 <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0703 01:28:04.633370 31050 net.cpp:413] pool3 -> pool3
I0703 01:28:04.633396 31050 net.cpp:148] Setting up pool3
I0703 01:28:04.633400 31050 net.cpp:155] Top shape: 8 128 40 40 (1638400)
I0703 01:28:04.633402 31050 net.cpp:163] Memory required for data: 1291059200
I0703 01:28:04.633405 31050 layer_factory.hpp:77] Creating layer res4a_branch2a
I0703 01:28:04.633410 31050 net.cpp:98] Creating Layer res4a_branch2a
I0703 01:28:04.633412 31050 net.cpp:439] res4a_branch2a <- pool3
I0703 01:28:04.633416 31050 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0703 01:28:04.640305 31050 net.cpp:148] Setting up res4a_branch2a
I0703 01:28:04.640316 31050 net.cpp:155] Top shape: 8 256 40 40 (3276800)
I0703 01:28:04.640317 31050 net.cpp:163] Memory required for data: 1304166400
I0703 01:28:04.640321 31050 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0703 01:28:04.640326 31050 net.cpp:98] Creating Layer res4a_branch2a/bn
I0703 01:28:04.640329 31050 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0703 01:28:04.640333 31050 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0703 01:28:04.640777 31050 net.cpp:148] Setting up res4a_branch2a/bn
I0703 01:28:04.640782 31050 net.cpp:155] Top shape: 8 256 40 40 (3276800)
I0703 01:28:04.640785 31050 net.cpp:163] Memory required for data: 1317273600
I0703 01:28:04.640790 31050 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0703 01:28:04.640794 31050 net.cpp:98] Creating Layer res4a_branch2a/relu
I0703 01:28:04.640796 31050 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0703 01:28:04.640799 31050 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0703 01:28:04.640802 31050 net.cpp:148] Setting up res4a_branch2a/relu
I0703 01:28:04.640805 31050 net.cpp:155] Top shape: 8 256 40 40 (3276800)
I0703 01:28:04.640807 31050 net.cpp:163] Memory required for data: 1330380800
I0703 01:28:04.640810 31050 layer_factory.hpp:77] Creating layer res4a_branch2b
I0703 01:28:04.640815 31050 net.cpp:98] Creating Layer res4a_branch2b
I0703 01:28:04.640816 31050 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0703 01:28:04.640820 31050 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0703 01:28:04.643913 31050 net.cpp:148] Setting up res4a_branch2b
I0703 01:28:04.643918 31050 net.cpp:155] Top shape: 8 256 40 40 (3276800)
I0703 01:28:04.643920 31050 net.cpp:163] Memory required for data: 1343488000
I0703 01:28:04.643930 31050 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0703 01:28:04.643935 31050 net.cpp:98] Creating Layer res4a_branch2b/bn
I0703 01:28:04.643939 31050 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0703 01:28:04.643942 31050 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0703 01:28:04.644376 31050 net.cpp:148] Setting up res4a_branch2b/bn
I0703 01:28:04.644381 31050 net.cpp:155] Top shape: 8 256 40 40 (3276800)
I0703 01:28:04.644384 31050 net.cpp:163] Memory required for data: 1356595200
I0703 01:28:04.644389 31050 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0703 01:28:04.644392 31050 net.cpp:98] Creating Layer res4a_branch2b/relu
I0703 01:28:04.644395 31050 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0703 01:28:04.644398 31050 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0703 01:28:04.644402 31050 net.cpp:148] Setting up res4a_branch2b/relu
I0703 01:28:04.644404 31050 net.cpp:155] Top shape: 8 256 40 40 (3276800)
I0703 01:28:04.644407 31050 net.cpp:163] Memory required for data: 1369702400
I0703 01:28:04.644408 31050 layer_factory.hpp:77] Creating layer pool4
I0703 01:28:04.644412 31050 net.cpp:98] Creating Layer pool4
I0703 01:28:04.644414 31050 net.cpp:439] pool4 <- res4a_branch2b/bn
I0703 01:28:04.644418 31050 net.cpp:413] pool4 -> pool4
I0703 01:28:04.644443 31050 net.cpp:148] Setting up pool4
I0703 01:28:04.644448 31050 net.cpp:155] Top shape: 8 256 40 40 (3276800)
I0703 01:28:04.644449 31050 net.cpp:163] Memory required for data: 1382809600
I0703 01:28:04.644453 31050 layer_factory.hpp:77] Creating layer res5a_branch2a
I0703 01:28:04.644456 31050 net.cpp:98] Creating Layer res5a_branch2a
I0703 01:28:04.644459 31050 net.cpp:439] res5a_branch2a <- pool4
I0703 01:28:04.644462 31050 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0703 01:28:04.669868 31050 net.cpp:148] Setting up res5a_branch2a
I0703 01:28:04.669888 31050 net.cpp:155] Top shape: 8 512 40 40 (6553600)
I0703 01:28:04.669891 31050 net.cpp:163] Memory required for data: 1409024000
I0703 01:28:04.669896 31050 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0703 01:28:04.669906 31050 net.cpp:98] Creating Layer res5a_branch2a/bn
I0703 01:28:04.669909 31050 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0703 01:28:04.669914 31050 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0703 01:28:04.670379 31050 net.cpp:148] Setting up res5a_branch2a/bn
I0703 01:28:04.670392 31050 net.cpp:155] Top shape: 8 512 40 40 (6553600)
I0703 01:28:04.670395 31050 net.cpp:163] Memory required for data: 1435238400
I0703 01:28:04.670400 31050 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0703 01:28:04.670404 31050 net.cpp:98] Creating Layer res5a_branch2a/relu
I0703 01:28:04.670406 31050 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0703 01:28:04.670409 31050 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0703 01:28:04.670413 31050 net.cpp:148] Setting up res5a_branch2a/relu
I0703 01:28:04.670415 31050 net.cpp:155] Top shape: 8 512 40 40 (6553600)
I0703 01:28:04.670418 31050 net.cpp:163] Memory required for data: 1461452800
I0703 01:28:04.670419 31050 layer_factory.hpp:77] Creating layer res5a_branch2b
I0703 01:28:04.670424 31050 net.cpp:98] Creating Layer res5a_branch2b
I0703 01:28:04.670426 31050 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0703 01:28:04.670428 31050 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0703 01:28:04.683050 31050 net.cpp:148] Setting up res5a_branch2b
I0703 01:28:04.683059 31050 net.cpp:155] Top shape: 8 512 40 40 (6553600)
I0703 01:28:04.683063 31050 net.cpp:163] Memory required for data: 1487667200
I0703 01:28:04.683069 31050 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0703 01:28:04.683074 31050 net.cpp:98] Creating Layer res5a_branch2b/bn
I0703 01:28:04.683076 31050 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0703 01:28:04.683079 31050 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0703 01:28:04.683534 31050 net.cpp:148] Setting up res5a_branch2b/bn
I0703 01:28:04.683550 31050 net.cpp:155] Top shape: 8 512 40 40 (6553600)
I0703 01:28:04.683552 31050 net.cpp:163] Memory required for data: 1513881600
I0703 01:28:04.683557 31050 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0703 01:28:04.683562 31050 net.cpp:98] Creating Layer res5a_branch2b/relu
I0703 01:28:04.683563 31050 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0703 01:28:04.683565 31050 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0703 01:28:04.683569 31050 net.cpp:148] Setting up res5a_branch2b/relu
I0703 01:28:04.683571 31050 net.cpp:155] Top shape: 8 512 40 40 (6553600)
I0703 01:28:04.683573 31050 net.cpp:163] Memory required for data: 1540096000
I0703 01:28:04.683575 31050 layer_factory.hpp:77] Creating layer out5a
I0703 01:28:04.683579 31050 net.cpp:98] Creating Layer out5a
I0703 01:28:04.683581 31050 net.cpp:439] out5a <- res5a_branch2b/bn
I0703 01:28:04.683584 31050 net.cpp:413] out5a -> out5a
I0703 01:28:04.687404 31050 net.cpp:148] Setting up out5a
I0703 01:28:04.687412 31050 net.cpp:155] Top shape: 8 64 40 40 (819200)
I0703 01:28:04.687414 31050 net.cpp:163] Memory required for data: 1543372800
I0703 01:28:04.687418 31050 layer_factory.hpp:77] Creating layer out5a/bn
I0703 01:28:04.687422 31050 net.cpp:98] Creating Layer out5a/bn
I0703 01:28:04.687425 31050 net.cpp:439] out5a/bn <- out5a
I0703 01:28:04.687428 31050 net.cpp:413] out5a/bn -> out5a/bn
I0703 01:28:04.687930 31050 net.cpp:148] Setting up out5a/bn
I0703 01:28:04.687937 31050 net.cpp:155] Top shape: 8 64 40 40 (819200)
I0703 01:28:04.687939 31050 net.cpp:163] Memory required for data: 1546649600
I0703 01:28:04.687944 31050 layer_factory.hpp:77] Creating layer out5a/relu
I0703 01:28:04.687947 31050 net.cpp:98] Creating Layer out5a/relu
I0703 01:28:04.687950 31050 net.cpp:439] out5a/relu <- out5a/bn
I0703 01:28:04.687952 31050 net.cpp:400] out5a/relu -> out5a/bn (in-place)
I0703 01:28:04.687958 31050 net.cpp:148] Setting up out5a/relu
I0703 01:28:04.687960 31050 net.cpp:155] Top shape: 8 64 40 40 (819200)
I0703 01:28:04.687963 31050 net.cpp:163] Memory required for data: 1549926400
I0703 01:28:04.687964 31050 layer_factory.hpp:77] Creating layer out5a_up2
I0703 01:28:04.687973 31050 net.cpp:98] Creating Layer out5a_up2
I0703 01:28:04.687975 31050 net.cpp:439] out5a_up2 <- out5a/bn
I0703 01:28:04.687978 31050 net.cpp:413] out5a_up2 -> out5a_up2
I0703 01:28:04.688159 31050 net.cpp:148] Setting up out5a_up2
I0703 01:28:04.688163 31050 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0703 01:28:04.688165 31050 net.cpp:163] Memory required for data: 1563033600
I0703 01:28:04.688169 31050 layer_factory.hpp:77] Creating layer out3a
I0703 01:28:04.688172 31050 net.cpp:98] Creating Layer out3a
I0703 01:28:04.688175 31050 net.cpp:439] out3a <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0703 01:28:04.688177 31050 net.cpp:413] out3a -> out3a
I0703 01:28:04.689134 31050 net.cpp:148] Setting up out3a
I0703 01:28:04.689141 31050 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0703 01:28:04.689143 31050 net.cpp:163] Memory required for data: 1576140800
I0703 01:28:04.689146 31050 layer_factory.hpp:77] Creating layer out3a/bn
I0703 01:28:04.689151 31050 net.cpp:98] Creating Layer out3a/bn
I0703 01:28:04.689153 31050 net.cpp:439] out3a/bn <- out3a
I0703 01:28:04.689155 31050 net.cpp:413] out3a/bn -> out3a/bn
I0703 01:28:04.689707 31050 net.cpp:148] Setting up out3a/bn
I0703 01:28:04.689713 31050 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0703 01:28:04.689715 31050 net.cpp:163] Memory required for data: 1589248000
I0703 01:28:04.689720 31050 layer_factory.hpp:77] Creating layer out3a/relu
I0703 01:28:04.689723 31050 net.cpp:98] Creating Layer out3a/relu
I0703 01:28:04.689725 31050 net.cpp:439] out3a/relu <- out3a/bn
I0703 01:28:04.689728 31050 net.cpp:400] out3a/relu -> out3a/bn (in-place)
I0703 01:28:04.689731 31050 net.cpp:148] Setting up out3a/relu
I0703 01:28:04.689733 31050 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0703 01:28:04.689735 31050 net.cpp:163] Memory required for data: 1602355200
I0703 01:28:04.689738 31050 layer_factory.hpp:77] Creating layer out3_out5_combined
I0703 01:28:04.689750 31050 net.cpp:98] Creating Layer out3_out5_combined
I0703 01:28:04.689754 31050 net.cpp:439] out3_out5_combined <- out5a_up2
I0703 01:28:04.689755 31050 net.cpp:439] out3_out5_combined <- out3a/bn
I0703 01:28:04.689757 31050 net.cpp:413] out3_out5_combined -> out3_out5_combined
I0703 01:28:04.689775 31050 net.cpp:148] Setting up out3_out5_combined
I0703 01:28:04.689779 31050 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0703 01:28:04.689779 31050 net.cpp:163] Memory required for data: 1615462400
I0703 01:28:04.689781 31050 layer_factory.hpp:77] Creating layer ctx_conv1
I0703 01:28:04.689785 31050 net.cpp:98] Creating Layer ctx_conv1
I0703 01:28:04.689787 31050 net.cpp:439] ctx_conv1 <- out3_out5_combined
I0703 01:28:04.689791 31050 net.cpp:413] ctx_conv1 -> ctx_conv1
I0703 01:28:04.690776 31050 net.cpp:148] Setting up ctx_conv1
I0703 01:28:04.690783 31050 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0703 01:28:04.690784 31050 net.cpp:163] Memory required for data: 1628569600
I0703 01:28:04.690788 31050 layer_factory.hpp:77] Creating layer ctx_conv1/bn
I0703 01:28:04.690791 31050 net.cpp:98] Creating Layer ctx_conv1/bn
I0703 01:28:04.690793 31050 net.cpp:439] ctx_conv1/bn <- ctx_conv1
I0703 01:28:04.690796 31050 net.cpp:413] ctx_conv1/bn -> ctx_conv1/bn
I0703 01:28:04.691311 31050 net.cpp:148] Setting up ctx_conv1/bn
I0703 01:28:04.691318 31050 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0703 01:28:04.691319 31050 net.cpp:163] Memory required for data: 1641676800
I0703 01:28:04.691324 31050 layer_factory.hpp:77] Creating layer ctx_conv1/relu
I0703 01:28:04.691328 31050 net.cpp:98] Creating Layer ctx_conv1/relu
I0703 01:28:04.691330 31050 net.cpp:439] ctx_conv1/relu <- ctx_conv1/bn
I0703 01:28:04.691332 31050 net.cpp:400] ctx_conv1/relu -> ctx_conv1/bn (in-place)
I0703 01:28:04.691335 31050 net.cpp:148] Setting up ctx_conv1/relu
I0703 01:28:04.691339 31050 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0703 01:28:04.691339 31050 net.cpp:163] Memory required for data: 1654784000
I0703 01:28:04.691341 31050 layer_factory.hpp:77] Creating layer ctx_conv2
I0703 01:28:04.691345 31050 net.cpp:98] Creating Layer ctx_conv2
I0703 01:28:04.691349 31050 net.cpp:439] ctx_conv2 <- ctx_conv1/bn
I0703 01:28:04.691350 31050 net.cpp:413] ctx_conv2 -> ctx_conv2
I0703 01:28:04.692312 31050 net.cpp:148] Setting up ctx_conv2
I0703 01:28:04.692317 31050 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0703 01:28:04.692319 31050 net.cpp:163] Memory required for data: 1667891200
I0703 01:28:04.692323 31050 layer_factory.hpp:77] Creating layer ctx_conv2/bn
I0703 01:28:04.692327 31050 net.cpp:98] Creating Layer ctx_conv2/bn
I0703 01:28:04.692328 31050 net.cpp:439] ctx_conv2/bn <- ctx_conv2
I0703 01:28:04.692332 31050 net.cpp:413] ctx_conv2/bn -> ctx_conv2/bn
I0703 01:28:04.692922 31050 net.cpp:148] Setting up ctx_conv2/bn
I0703 01:28:04.692929 31050 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0703 01:28:04.692931 31050 net.cpp:163] Memory required for data: 1680998400
I0703 01:28:04.692936 31050 layer_factory.hpp:77] Creating layer ctx_conv2/relu
I0703 01:28:04.692939 31050 net.cpp:98] Creating Layer ctx_conv2/relu
I0703 01:28:04.692940 31050 net.cpp:439] ctx_conv2/relu <- ctx_conv2/bn
I0703 01:28:04.692945 31050 net.cpp:400] ctx_conv2/relu -> ctx_conv2/bn (in-place)
I0703 01:28:04.692948 31050 net.cpp:148] Setting up ctx_conv2/relu
I0703 01:28:04.692950 31050 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0703 01:28:04.692952 31050 net.cpp:163] Memory required for data: 1694105600
I0703 01:28:04.692955 31050 layer_factory.hpp:77] Creating layer ctx_conv3
I0703 01:28:04.692958 31050 net.cpp:98] Creating Layer ctx_conv3
I0703 01:28:04.692960 31050 net.cpp:439] ctx_conv3 <- ctx_conv2/bn
I0703 01:28:04.692962 31050 net.cpp:413] ctx_conv3 -> ctx_conv3
I0703 01:28:04.693976 31050 net.cpp:148] Setting up ctx_conv3
I0703 01:28:04.693982 31050 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0703 01:28:04.693984 31050 net.cpp:163] Memory required for data: 1707212800
I0703 01:28:04.693994 31050 layer_factory.hpp:77] Creating layer ctx_conv3/bn
I0703 01:28:04.693997 31050 net.cpp:98] Creating Layer ctx_conv3/bn
I0703 01:28:04.694000 31050 net.cpp:439] ctx_conv3/bn <- ctx_conv3
I0703 01:28:04.694002 31050 net.cpp:413] ctx_conv3/bn -> ctx_conv3/bn
I0703 01:28:04.694576 31050 net.cpp:148] Setting up ctx_conv3/bn
I0703 01:28:04.694583 31050 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0703 01:28:04.694586 31050 net.cpp:163] Memory required for data: 1720320000
I0703 01:28:04.694591 31050 layer_factory.hpp:77] Creating layer ctx_conv3/relu
I0703 01:28:04.694593 31050 net.cpp:98] Creating Layer ctx_conv3/relu
I0703 01:28:04.694597 31050 net.cpp:439] ctx_conv3/relu <- ctx_conv3/bn
I0703 01:28:04.694598 31050 net.cpp:400] ctx_conv3/relu -> ctx_conv3/bn (in-place)
I0703 01:28:04.694602 31050 net.cpp:148] Setting up ctx_conv3/relu
I0703 01:28:04.694604 31050 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0703 01:28:04.694607 31050 net.cpp:163] Memory required for data: 1733427200
I0703 01:28:04.694608 31050 layer_factory.hpp:77] Creating layer ctx_conv4
I0703 01:28:04.694612 31050 net.cpp:98] Creating Layer ctx_conv4
I0703 01:28:04.694613 31050 net.cpp:439] ctx_conv4 <- ctx_conv3/bn
I0703 01:28:04.694617 31050 net.cpp:413] ctx_conv4 -> ctx_conv4
I0703 01:28:04.695638 31050 net.cpp:148] Setting up ctx_conv4
I0703 01:28:04.695644 31050 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0703 01:28:04.695647 31050 net.cpp:163] Memory required for data: 1746534400
I0703 01:28:04.695649 31050 layer_factory.hpp:77] Creating layer ctx_conv4/bn
I0703 01:28:04.695654 31050 net.cpp:98] Creating Layer ctx_conv4/bn
I0703 01:28:04.695657 31050 net.cpp:439] ctx_conv4/bn <- ctx_conv4
I0703 01:28:04.695660 31050 net.cpp:413] ctx_conv4/bn -> ctx_conv4/bn
I0703 01:28:04.696224 31050 net.cpp:148] Setting up ctx_conv4/bn
I0703 01:28:04.696230 31050 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0703 01:28:04.696233 31050 net.cpp:163] Memory required for data: 1759641600
I0703 01:28:04.696238 31050 layer_factory.hpp:77] Creating layer ctx_conv4/relu
I0703 01:28:04.696240 31050 net.cpp:98] Creating Layer ctx_conv4/relu
I0703 01:28:04.696243 31050 net.cpp:439] ctx_conv4/relu <- ctx_conv4/bn
I0703 01:28:04.696245 31050 net.cpp:400] ctx_conv4/relu -> ctx_conv4/bn (in-place)
I0703 01:28:04.696249 31050 net.cpp:148] Setting up ctx_conv4/relu
I0703 01:28:04.696250 31050 net.cpp:155] Top shape: 8 64 80 80 (3276800)
I0703 01:28:04.696252 31050 net.cpp:163] Memory required for data: 1772748800
I0703 01:28:04.696254 31050 layer_factory.hpp:77] Creating layer ctx_final
I0703 01:28:04.696259 31050 net.cpp:98] Creating Layer ctx_final
I0703 01:28:04.696260 31050 net.cpp:439] ctx_final <- ctx_conv4/bn
I0703 01:28:04.696262 31050 net.cpp:413] ctx_final -> ctx_final
I0703 01:28:04.696655 31050 net.cpp:148] Setting up ctx_final
I0703 01:28:04.696661 31050 net.cpp:155] Top shape: 8 8 80 80 (409600)
I0703 01:28:04.696663 31050 net.cpp:163] Memory required for data: 1774387200
I0703 01:28:04.696666 31050 layer_factory.hpp:77] Creating layer ctx_final/relu
I0703 01:28:04.696669 31050 net.cpp:98] Creating Layer ctx_final/relu
I0703 01:28:04.696671 31050 net.cpp:439] ctx_final/relu <- ctx_final
I0703 01:28:04.696674 31050 net.cpp:400] ctx_final/relu -> ctx_final (in-place)
I0703 01:28:04.696677 31050 net.cpp:148] Setting up ctx_final/relu
I0703 01:28:04.696681 31050 net.cpp:155] Top shape: 8 8 80 80 (409600)
I0703 01:28:04.696681 31050 net.cpp:163] Memory required for data: 1776025600
I0703 01:28:04.696683 31050 layer_factory.hpp:77] Creating layer out_deconv_final_up2
I0703 01:28:04.696686 31050 net.cpp:98] Creating Layer out_deconv_final_up2
I0703 01:28:04.696688 31050 net.cpp:439] out_deconv_final_up2 <- ctx_final
I0703 01:28:04.696691 31050 net.cpp:413] out_deconv_final_up2 -> out_deconv_final_up2
I0703 01:28:04.696899 31050 net.cpp:148] Setting up out_deconv_final_up2
I0703 01:28:04.696907 31050 net.cpp:155] Top shape: 8 8 160 160 (1638400)
I0703 01:28:04.696910 31050 net.cpp:163] Memory required for data: 1782579200
I0703 01:28:04.696915 31050 layer_factory.hpp:77] Creating layer out_deconv_final_up4
I0703 01:28:04.696929 31050 net.cpp:98] Creating Layer out_deconv_final_up4
I0703 01:28:04.696933 31050 net.cpp:439] out_deconv_final_up4 <- out_deconv_final_up2
I0703 01:28:04.696938 31050 net.cpp:413] out_deconv_final_up4 -> out_deconv_final_up4
I0703 01:28:04.697160 31050 net.cpp:148] Setting up out_deconv_final_up4
I0703 01:28:04.697167 31050 net.cpp:155] Top shape: 8 8 320 320 (6553600)
I0703 01:28:04.697170 31050 net.cpp:163] Memory required for data: 1808793600
I0703 01:28:04.697175 31050 layer_factory.hpp:77] Creating layer out_deconv_final_up8
I0703 01:28:04.697180 31050 net.cpp:98] Creating Layer out_deconv_final_up8
I0703 01:28:04.697182 31050 net.cpp:439] out_deconv_final_up8 <- out_deconv_final_up4
I0703 01:28:04.697187 31050 net.cpp:413] out_deconv_final_up8 -> out_deconv_final_up8
I0703 01:28:04.697401 31050 net.cpp:148] Setting up out_deconv_final_up8
I0703 01:28:04.697407 31050 net.cpp:155] Top shape: 8 8 640 640 (26214400)
I0703 01:28:04.697410 31050 net.cpp:163] Memory required for data: 1913651200
I0703 01:28:04.697414 31050 layer_factory.hpp:77] Creating layer loss
I0703 01:28:04.697419 31050 net.cpp:98] Creating Layer loss
I0703 01:28:04.697422 31050 net.cpp:439] loss <- out_deconv_final_up8
I0703 01:28:04.697427 31050 net.cpp:439] loss <- label
I0703 01:28:04.697432 31050 net.cpp:413] loss -> loss
I0703 01:28:04.697443 31050 layer_factory.hpp:77] Creating layer loss
I0703 01:28:04.729910 31050 net.cpp:148] Setting up loss
I0703 01:28:04.729934 31050 net.cpp:155] Top shape: (1)
I0703 01:28:04.729936 31050 net.cpp:158]     with loss weight 1
I0703 01:28:04.729949 31050 net.cpp:163] Memory required for data: 1913651204
I0703 01:28:04.729954 31050 net.cpp:224] loss needs backward computation.
I0703 01:28:04.729956 31050 net.cpp:224] out_deconv_final_up8 needs backward computation.
I0703 01:28:04.729959 31050 net.cpp:224] out_deconv_final_up4 needs backward computation.
I0703 01:28:04.729960 31050 net.cpp:224] out_deconv_final_up2 needs backward computation.
I0703 01:28:04.729962 31050 net.cpp:224] ctx_final/relu needs backward computation.
I0703 01:28:04.729964 31050 net.cpp:224] ctx_final needs backward computation.
I0703 01:28:04.729966 31050 net.cpp:224] ctx_conv4/relu needs backward computation.
I0703 01:28:04.729969 31050 net.cpp:224] ctx_conv4/bn needs backward computation.
I0703 01:28:04.729970 31050 net.cpp:224] ctx_conv4 needs backward computation.
I0703 01:28:04.729972 31050 net.cpp:224] ctx_conv3/relu needs backward computation.
I0703 01:28:04.729974 31050 net.cpp:224] ctx_conv3/bn needs backward computation.
I0703 01:28:04.729977 31050 net.cpp:224] ctx_conv3 needs backward computation.
I0703 01:28:04.729980 31050 net.cpp:224] ctx_conv2/relu needs backward computation.
I0703 01:28:04.729982 31050 net.cpp:224] ctx_conv2/bn needs backward computation.
I0703 01:28:04.729984 31050 net.cpp:224] ctx_conv2 needs backward computation.
I0703 01:28:04.729986 31050 net.cpp:224] ctx_conv1/relu needs backward computation.
I0703 01:28:04.729988 31050 net.cpp:224] ctx_conv1/bn needs backward computation.
I0703 01:28:04.729990 31050 net.cpp:224] ctx_conv1 needs backward computation.
I0703 01:28:04.729993 31050 net.cpp:224] out3_out5_combined needs backward computation.
I0703 01:28:04.729995 31050 net.cpp:224] out3a/relu needs backward computation.
I0703 01:28:04.729997 31050 net.cpp:224] out3a/bn needs backward computation.
I0703 01:28:04.730000 31050 net.cpp:224] out3a needs backward computation.
I0703 01:28:04.730002 31050 net.cpp:224] out5a_up2 needs backward computation.
I0703 01:28:04.730005 31050 net.cpp:224] out5a/relu needs backward computation.
I0703 01:28:04.730007 31050 net.cpp:224] out5a/bn needs backward computation.
I0703 01:28:04.730010 31050 net.cpp:224] out5a needs backward computation.
I0703 01:28:04.730012 31050 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0703 01:28:04.730015 31050 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0703 01:28:04.730016 31050 net.cpp:224] res5a_branch2b needs backward computation.
I0703 01:28:04.730031 31050 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0703 01:28:04.730032 31050 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0703 01:28:04.730034 31050 net.cpp:224] res5a_branch2a needs backward computation.
I0703 01:28:04.730036 31050 net.cpp:224] pool4 needs backward computation.
I0703 01:28:04.730039 31050 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0703 01:28:04.730041 31050 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0703 01:28:04.730043 31050 net.cpp:224] res4a_branch2b needs backward computation.
I0703 01:28:04.730046 31050 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0703 01:28:04.730049 31050 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0703 01:28:04.730051 31050 net.cpp:224] res4a_branch2a needs backward computation.
I0703 01:28:04.730053 31050 net.cpp:224] pool3 needs backward computation.
I0703 01:28:04.730056 31050 net.cpp:224] res3a_branch2b/bn_res3a_branch2b/relu_0_split needs backward computation.
I0703 01:28:04.730058 31050 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0703 01:28:04.730068 31050 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0703 01:28:04.730070 31050 net.cpp:224] res3a_branch2b needs backward computation.
I0703 01:28:04.730073 31050 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0703 01:28:04.730077 31050 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0703 01:28:04.730079 31050 net.cpp:224] res3a_branch2a needs backward computation.
I0703 01:28:04.730082 31050 net.cpp:224] pool2 needs backward computation.
I0703 01:28:04.730085 31050 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0703 01:28:04.730088 31050 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0703 01:28:04.730090 31050 net.cpp:224] res2a_branch2b needs backward computation.
I0703 01:28:04.730093 31050 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0703 01:28:04.730095 31050 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0703 01:28:04.730098 31050 net.cpp:224] res2a_branch2a needs backward computation.
I0703 01:28:04.730099 31050 net.cpp:224] pool1 needs backward computation.
I0703 01:28:04.730103 31050 net.cpp:224] conv1b/relu needs backward computation.
I0703 01:28:04.730105 31050 net.cpp:224] conv1b/bn needs backward computation.
I0703 01:28:04.730108 31050 net.cpp:224] conv1b needs backward computation.
I0703 01:28:04.730110 31050 net.cpp:224] conv1a/relu needs backward computation.
I0703 01:28:04.730111 31050 net.cpp:224] conv1a/bn needs backward computation.
I0703 01:28:04.730113 31050 net.cpp:224] conv1a needs backward computation.
I0703 01:28:04.730116 31050 net.cpp:226] data/bias does not need backward computation.
I0703 01:28:04.730119 31050 net.cpp:226] data does not need backward computation.
I0703 01:28:04.730121 31050 net.cpp:268] This network produces output loss
I0703 01:28:04.730151 31050 net.cpp:288] Network initialization done.
I0703 01:28:04.730891 31050 solver.cpp:182] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-07-02_23-02-42/sparse/test.prototxt
I0703 01:28:04.731187 31050 net.cpp:56] Initializing net from parameters: 
name: "jsegnet21v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/val-image-lmdb"
    label_list_path: "data/val-label-lmdb"
    batch_size: 4
    threads: 4
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b/bn"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a/bn"
  top: "out5a/bn"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a/bn"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b/bn"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a/bn"
  top: "out3a/bn"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a/bn"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv1/bn"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv2/bn"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv3/bn"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4/bn"
  top: "ctx_conv4/bn"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4/bn"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
  accuracy_param {
    ignore_label: 255
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
    ignore_label: 255
  }
}
I0703 01:28:04.731325 31050 layer_factory.hpp:77] Creating layer data
I0703 01:28:04.731335 31050 net.cpp:98] Creating Layer data
I0703 01:28:04.731340 31050 net.cpp:413] data -> data
I0703 01:28:04.731348 31050 net.cpp:413] data -> label
I0703 01:28:04.732861 31135 db_lmdb.cpp:35] Opened lmdb data/val-label-lmdb
I0703 01:28:04.761632 31130 db_lmdb.cpp:35] Opened lmdb data/val-image-lmdb
I0703 01:28:04.771430 31050 data_layer.cpp:78] ReshapePrefetch 4, 3, 640, 640
I0703 01:28:04.771621 31050 data_layer.cpp:83] output data size: 4,3,640,640
I0703 01:28:04.803767 31050 data_layer.cpp:78] ReshapePrefetch 4, 1, 640, 640
I0703 01:28:04.803853 31050 data_layer.cpp:83] output data size: 4,1,640,640
I0703 01:28:04.816761 31050 net.cpp:148] Setting up data
I0703 01:28:04.816793 31050 net.cpp:155] Top shape: 4 3 640 640 (4915200)
I0703 01:28:04.816798 31050 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0703 01:28:04.816802 31050 net.cpp:163] Memory required for data: 26214400
I0703 01:28:04.816835 31050 layer_factory.hpp:77] Creating layer label_data_1_split
I0703 01:28:04.816850 31050 net.cpp:98] Creating Layer label_data_1_split
I0703 01:28:04.816855 31050 net.cpp:439] label_data_1_split <- label
I0703 01:28:04.816862 31050 net.cpp:413] label_data_1_split -> label_data_1_split_0
I0703 01:28:04.816870 31050 net.cpp:413] label_data_1_split -> label_data_1_split_1
I0703 01:28:04.816874 31050 net.cpp:413] label_data_1_split -> label_data_1_split_2
I0703 01:28:04.817032 31050 net.cpp:148] Setting up label_data_1_split
I0703 01:28:04.817041 31050 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0703 01:28:04.817045 31050 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0703 01:28:04.817049 31050 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0703 01:28:04.817052 31050 net.cpp:163] Memory required for data: 45875200
I0703 01:28:04.817055 31050 layer_factory.hpp:77] Creating layer data/bias
I0703 01:28:04.817065 31050 net.cpp:98] Creating Layer data/bias
I0703 01:28:04.817068 31050 net.cpp:439] data/bias <- data
I0703 01:28:04.817075 31050 net.cpp:413] data/bias -> data/bias
I0703 01:28:04.818848 31050 net.cpp:148] Setting up data/bias
I0703 01:28:04.818877 31050 net.cpp:155] Top shape: 4 3 640 640 (4915200)
I0703 01:28:04.818881 31050 net.cpp:163] Memory required for data: 65536000
I0703 01:28:04.818894 31050 layer_factory.hpp:77] Creating layer conv1a
I0703 01:28:04.818914 31050 net.cpp:98] Creating Layer conv1a
I0703 01:28:04.818919 31050 net.cpp:439] conv1a <- data/bias
I0703 01:28:04.818925 31050 net.cpp:413] conv1a -> conv1a
I0703 01:28:04.819674 31050 net.cpp:148] Setting up conv1a
I0703 01:28:04.819689 31050 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0703 01:28:04.819691 31050 net.cpp:163] Memory required for data: 117964800
I0703 01:28:04.819700 31050 layer_factory.hpp:77] Creating layer conv1a/bn
I0703 01:28:04.819713 31050 net.cpp:98] Creating Layer conv1a/bn
I0703 01:28:04.819716 31050 net.cpp:439] conv1a/bn <- conv1a
I0703 01:28:04.819726 31050 net.cpp:413] conv1a/bn -> conv1a/bn
I0703 01:28:04.821094 31050 net.cpp:148] Setting up conv1a/bn
I0703 01:28:04.821117 31050 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0703 01:28:04.821120 31050 net.cpp:163] Memory required for data: 170393600
I0703 01:28:04.821133 31050 layer_factory.hpp:77] Creating layer conv1a/relu
I0703 01:28:04.821153 31050 net.cpp:98] Creating Layer conv1a/relu
I0703 01:28:04.821175 31050 net.cpp:439] conv1a/relu <- conv1a/bn
I0703 01:28:04.821192 31050 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0703 01:28:04.821210 31050 net.cpp:148] Setting up conv1a/relu
I0703 01:28:04.821221 31050 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0703 01:28:04.821230 31050 net.cpp:163] Memory required for data: 222822400
I0703 01:28:04.821240 31050 layer_factory.hpp:77] Creating layer conv1b
I0703 01:28:04.821260 31050 net.cpp:98] Creating Layer conv1b
I0703 01:28:04.821269 31050 net.cpp:439] conv1b <- conv1a/bn
I0703 01:28:04.821288 31050 net.cpp:413] conv1b -> conv1b
I0703 01:28:04.821844 31050 net.cpp:148] Setting up conv1b
I0703 01:28:04.821852 31050 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0703 01:28:04.821854 31050 net.cpp:163] Memory required for data: 275251200
I0703 01:28:04.821861 31050 layer_factory.hpp:77] Creating layer conv1b/bn
I0703 01:28:04.821872 31050 net.cpp:98] Creating Layer conv1b/bn
I0703 01:28:04.821877 31050 net.cpp:439] conv1b/bn <- conv1b
I0703 01:28:04.821882 31050 net.cpp:413] conv1b/bn -> conv1b/bn
I0703 01:28:04.822856 31050 net.cpp:148] Setting up conv1b/bn
I0703 01:28:04.822880 31050 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0703 01:28:04.822885 31050 net.cpp:163] Memory required for data: 327680000
I0703 01:28:04.822896 31050 layer_factory.hpp:77] Creating layer conv1b/relu
I0703 01:28:04.822906 31050 net.cpp:98] Creating Layer conv1b/relu
I0703 01:28:04.822911 31050 net.cpp:439] conv1b/relu <- conv1b/bn
I0703 01:28:04.822916 31050 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0703 01:28:04.822922 31050 net.cpp:148] Setting up conv1b/relu
I0703 01:28:04.822948 31050 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0703 01:28:04.822952 31050 net.cpp:163] Memory required for data: 380108800
I0703 01:28:04.822957 31050 layer_factory.hpp:77] Creating layer pool1
I0703 01:28:04.822966 31050 net.cpp:98] Creating Layer pool1
I0703 01:28:04.822970 31050 net.cpp:439] pool1 <- conv1b/bn
I0703 01:28:04.822976 31050 net.cpp:413] pool1 -> pool1
I0703 01:28:04.823024 31050 net.cpp:148] Setting up pool1
I0703 01:28:04.823030 31050 net.cpp:155] Top shape: 4 32 160 160 (3276800)
I0703 01:28:04.823035 31050 net.cpp:163] Memory required for data: 393216000
I0703 01:28:04.823038 31050 layer_factory.hpp:77] Creating layer res2a_branch2a
I0703 01:28:04.823047 31050 net.cpp:98] Creating Layer res2a_branch2a
I0703 01:28:04.823051 31050 net.cpp:439] res2a_branch2a <- pool1
I0703 01:28:04.823057 31050 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0703 01:28:04.824012 31050 net.cpp:148] Setting up res2a_branch2a
I0703 01:28:04.824026 31050 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0703 01:28:04.824031 31050 net.cpp:163] Memory required for data: 419430400
I0703 01:28:04.824041 31050 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0703 01:28:04.824049 31050 net.cpp:98] Creating Layer res2a_branch2a/bn
I0703 01:28:04.824054 31050 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0703 01:28:04.824060 31050 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0703 01:28:04.824805 31050 net.cpp:148] Setting up res2a_branch2a/bn
I0703 01:28:04.824813 31050 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0703 01:28:04.824817 31050 net.cpp:163] Memory required for data: 445644800
I0703 01:28:04.824832 31050 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0703 01:28:04.824839 31050 net.cpp:98] Creating Layer res2a_branch2a/relu
I0703 01:28:04.824844 31050 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0703 01:28:04.824849 31050 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0703 01:28:04.824856 31050 net.cpp:148] Setting up res2a_branch2a/relu
I0703 01:28:04.824862 31050 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0703 01:28:04.824867 31050 net.cpp:163] Memory required for data: 471859200
I0703 01:28:04.824870 31050 layer_factory.hpp:77] Creating layer res2a_branch2b
I0703 01:28:04.824878 31050 net.cpp:98] Creating Layer res2a_branch2b
I0703 01:28:04.824882 31050 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0703 01:28:04.824888 31050 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0703 01:28:04.825492 31050 net.cpp:148] Setting up res2a_branch2b
I0703 01:28:04.825501 31050 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0703 01:28:04.825505 31050 net.cpp:163] Memory required for data: 498073600
I0703 01:28:04.825510 31050 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0703 01:28:04.825517 31050 net.cpp:98] Creating Layer res2a_branch2b/bn
I0703 01:28:04.825522 31050 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0703 01:28:04.825526 31050 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0703 01:28:04.826239 31050 net.cpp:148] Setting up res2a_branch2b/bn
I0703 01:28:04.826246 31050 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0703 01:28:04.826249 31050 net.cpp:163] Memory required for data: 524288000
I0703 01:28:04.826256 31050 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0703 01:28:04.826264 31050 net.cpp:98] Creating Layer res2a_branch2b/relu
I0703 01:28:04.826268 31050 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0703 01:28:04.826274 31050 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0703 01:28:04.826280 31050 net.cpp:148] Setting up res2a_branch2b/relu
I0703 01:28:04.826285 31050 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0703 01:28:04.826289 31050 net.cpp:163] Memory required for data: 550502400
I0703 01:28:04.826293 31050 layer_factory.hpp:77] Creating layer pool2
I0703 01:28:04.826300 31050 net.cpp:98] Creating Layer pool2
I0703 01:28:04.826304 31050 net.cpp:439] pool2 <- res2a_branch2b/bn
I0703 01:28:04.826309 31050 net.cpp:413] pool2 -> pool2
I0703 01:28:04.826361 31050 net.cpp:148] Setting up pool2
I0703 01:28:04.826369 31050 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 01:28:04.826372 31050 net.cpp:163] Memory required for data: 557056000
I0703 01:28:04.826375 31050 layer_factory.hpp:77] Creating layer res3a_branch2a
I0703 01:28:04.826382 31050 net.cpp:98] Creating Layer res3a_branch2a
I0703 01:28:04.826390 31050 net.cpp:439] res3a_branch2a <- pool2
I0703 01:28:04.826396 31050 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0703 01:28:04.829180 31050 net.cpp:148] Setting up res3a_branch2a
I0703 01:28:04.829195 31050 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0703 01:28:04.829197 31050 net.cpp:163] Memory required for data: 570163200
I0703 01:28:04.829205 31050 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0703 01:28:04.829213 31050 net.cpp:98] Creating Layer res3a_branch2a/bn
I0703 01:28:04.829221 31050 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0703 01:28:04.829228 31050 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0703 01:28:04.830379 31050 net.cpp:148] Setting up res3a_branch2a/bn
I0703 01:28:04.830410 31050 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0703 01:28:04.830416 31050 net.cpp:163] Memory required for data: 583270400
I0703 01:28:04.830430 31050 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0703 01:28:04.830438 31050 net.cpp:98] Creating Layer res3a_branch2a/relu
I0703 01:28:04.830443 31050 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0703 01:28:04.830449 31050 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0703 01:28:04.830456 31050 net.cpp:148] Setting up res3a_branch2a/relu
I0703 01:28:04.830461 31050 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0703 01:28:04.830463 31050 net.cpp:163] Memory required for data: 596377600
I0703 01:28:04.830467 31050 layer_factory.hpp:77] Creating layer res3a_branch2b
I0703 01:28:04.830476 31050 net.cpp:98] Creating Layer res3a_branch2b
I0703 01:28:04.830480 31050 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0703 01:28:04.830484 31050 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0703 01:28:04.831899 31050 net.cpp:148] Setting up res3a_branch2b
I0703 01:28:04.831910 31050 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0703 01:28:04.831913 31050 net.cpp:163] Memory required for data: 609484800
I0703 01:28:04.831919 31050 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0703 01:28:04.831925 31050 net.cpp:98] Creating Layer res3a_branch2b/bn
I0703 01:28:04.831928 31050 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0703 01:28:04.831933 31050 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0703 01:28:04.832545 31050 net.cpp:148] Setting up res3a_branch2b/bn
I0703 01:28:04.832551 31050 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0703 01:28:04.832553 31050 net.cpp:163] Memory required for data: 622592000
I0703 01:28:04.832558 31050 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0703 01:28:04.832561 31050 net.cpp:98] Creating Layer res3a_branch2b/relu
I0703 01:28:04.832563 31050 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0703 01:28:04.832566 31050 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0703 01:28:04.832569 31050 net.cpp:148] Setting up res3a_branch2b/relu
I0703 01:28:04.832571 31050 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0703 01:28:04.832573 31050 net.cpp:163] Memory required for data: 635699200
I0703 01:28:04.832576 31050 layer_factory.hpp:77] Creating layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0703 01:28:04.832579 31050 net.cpp:98] Creating Layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0703 01:28:04.832581 31050 net.cpp:439] res3a_branch2b/bn_res3a_branch2b/relu_0_split <- res3a_branch2b/bn
I0703 01:28:04.832583 31050 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0703 01:28:04.832587 31050 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0703 01:28:04.832624 31050 net.cpp:148] Setting up res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0703 01:28:04.832644 31050 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0703 01:28:04.832648 31050 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0703 01:28:04.832648 31050 net.cpp:163] Memory required for data: 661913600
I0703 01:28:04.832650 31050 layer_factory.hpp:77] Creating layer pool3
I0703 01:28:04.832656 31050 net.cpp:98] Creating Layer pool3
I0703 01:28:04.832659 31050 net.cpp:439] pool3 <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0703 01:28:04.832664 31050 net.cpp:413] pool3 -> pool3
I0703 01:28:04.832698 31050 net.cpp:148] Setting up pool3
I0703 01:28:04.832717 31050 net.cpp:155] Top shape: 4 128 40 40 (819200)
I0703 01:28:04.832722 31050 net.cpp:163] Memory required for data: 665190400
I0703 01:28:04.832726 31050 layer_factory.hpp:77] Creating layer res4a_branch2a
I0703 01:28:04.832736 31050 net.cpp:98] Creating Layer res4a_branch2a
I0703 01:28:04.832741 31050 net.cpp:439] res4a_branch2a <- pool3
I0703 01:28:04.832746 31050 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0703 01:28:04.841529 31050 net.cpp:148] Setting up res4a_branch2a
I0703 01:28:04.841632 31050 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0703 01:28:04.841646 31050 net.cpp:163] Memory required for data: 671744000
I0703 01:28:04.841665 31050 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0703 01:28:04.841686 31050 net.cpp:98] Creating Layer res4a_branch2a/bn
I0703 01:28:04.841696 31050 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0703 01:28:04.841709 31050 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0703 01:28:04.842458 31050 net.cpp:148] Setting up res4a_branch2a/bn
I0703 01:28:04.842479 31050 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0703 01:28:04.842488 31050 net.cpp:163] Memory required for data: 678297600
I0703 01:28:04.842499 31050 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0703 01:28:04.842509 31050 net.cpp:98] Creating Layer res4a_branch2a/relu
I0703 01:28:04.842516 31050 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0703 01:28:04.842525 31050 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0703 01:28:04.842536 31050 net.cpp:148] Setting up res4a_branch2a/relu
I0703 01:28:04.842545 31050 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0703 01:28:04.842551 31050 net.cpp:163] Memory required for data: 684851200
I0703 01:28:04.842558 31050 layer_factory.hpp:77] Creating layer res4a_branch2b
I0703 01:28:04.842571 31050 net.cpp:98] Creating Layer res4a_branch2b
I0703 01:28:04.842577 31050 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0703 01:28:04.842586 31050 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0703 01:28:04.846402 31050 net.cpp:148] Setting up res4a_branch2b
I0703 01:28:04.846436 31050 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0703 01:28:04.846439 31050 net.cpp:163] Memory required for data: 691404800
I0703 01:28:04.846448 31050 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0703 01:28:04.846463 31050 net.cpp:98] Creating Layer res4a_branch2b/bn
I0703 01:28:04.846468 31050 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0703 01:28:04.846478 31050 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0703 01:28:04.847033 31050 net.cpp:148] Setting up res4a_branch2b/bn
I0703 01:28:04.847040 31050 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0703 01:28:04.847043 31050 net.cpp:163] Memory required for data: 697958400
I0703 01:28:04.847051 31050 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0703 01:28:04.847056 31050 net.cpp:98] Creating Layer res4a_branch2b/relu
I0703 01:28:04.847060 31050 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0703 01:28:04.847065 31050 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0703 01:28:04.847071 31050 net.cpp:148] Setting up res4a_branch2b/relu
I0703 01:28:04.847076 31050 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0703 01:28:04.847079 31050 net.cpp:163] Memory required for data: 704512000
I0703 01:28:04.847084 31050 layer_factory.hpp:77] Creating layer pool4
I0703 01:28:04.847091 31050 net.cpp:98] Creating Layer pool4
I0703 01:28:04.847095 31050 net.cpp:439] pool4 <- res4a_branch2b/bn
I0703 01:28:04.847112 31050 net.cpp:413] pool4 -> pool4
I0703 01:28:04.847151 31050 net.cpp:148] Setting up pool4
I0703 01:28:04.847157 31050 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0703 01:28:04.847160 31050 net.cpp:163] Memory required for data: 711065600
I0703 01:28:04.847164 31050 layer_factory.hpp:77] Creating layer res5a_branch2a
I0703 01:28:04.847174 31050 net.cpp:98] Creating Layer res5a_branch2a
I0703 01:28:04.847185 31050 net.cpp:439] res5a_branch2a <- pool4
I0703 01:28:04.847192 31050 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0703 01:28:04.874207 31050 net.cpp:148] Setting up res5a_branch2a
I0703 01:28:04.874224 31050 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0703 01:28:04.874228 31050 net.cpp:163] Memory required for data: 724172800
I0703 01:28:04.874234 31050 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0703 01:28:04.874244 31050 net.cpp:98] Creating Layer res5a_branch2a/bn
I0703 01:28:04.874248 31050 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0703 01:28:04.874254 31050 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0703 01:28:04.874765 31050 net.cpp:148] Setting up res5a_branch2a/bn
I0703 01:28:04.874773 31050 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0703 01:28:04.874776 31050 net.cpp:163] Memory required for data: 737280000
I0703 01:28:04.874784 31050 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0703 01:28:04.874789 31050 net.cpp:98] Creating Layer res5a_branch2a/relu
I0703 01:28:04.874794 31050 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0703 01:28:04.874799 31050 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0703 01:28:04.874805 31050 net.cpp:148] Setting up res5a_branch2a/relu
I0703 01:28:04.874811 31050 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0703 01:28:04.874814 31050 net.cpp:163] Memory required for data: 750387200
I0703 01:28:04.874816 31050 layer_factory.hpp:77] Creating layer res5a_branch2b
I0703 01:28:04.874821 31050 net.cpp:98] Creating Layer res5a_branch2b
I0703 01:28:04.874824 31050 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0703 01:28:04.874830 31050 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0703 01:28:04.887445 31050 net.cpp:148] Setting up res5a_branch2b
I0703 01:28:04.887456 31050 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0703 01:28:04.887459 31050 net.cpp:163] Memory required for data: 763494400
I0703 01:28:04.887465 31050 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0703 01:28:04.887470 31050 net.cpp:98] Creating Layer res5a_branch2b/bn
I0703 01:28:04.887473 31050 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0703 01:28:04.887476 31050 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0703 01:28:04.887969 31050 net.cpp:148] Setting up res5a_branch2b/bn
I0703 01:28:04.887974 31050 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0703 01:28:04.887975 31050 net.cpp:163] Memory required for data: 776601600
I0703 01:28:04.887980 31050 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0703 01:28:04.887984 31050 net.cpp:98] Creating Layer res5a_branch2b/relu
I0703 01:28:04.887986 31050 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0703 01:28:04.887989 31050 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0703 01:28:04.887992 31050 net.cpp:148] Setting up res5a_branch2b/relu
I0703 01:28:04.887995 31050 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0703 01:28:04.887996 31050 net.cpp:163] Memory required for data: 789708800
I0703 01:28:04.887998 31050 layer_factory.hpp:77] Creating layer out5a
I0703 01:28:04.888002 31050 net.cpp:98] Creating Layer out5a
I0703 01:28:04.888005 31050 net.cpp:439] out5a <- res5a_branch2b/bn
I0703 01:28:04.888007 31050 net.cpp:413] out5a -> out5a
I0703 01:28:04.891876 31050 net.cpp:148] Setting up out5a
I0703 01:28:04.891885 31050 net.cpp:155] Top shape: 4 64 40 40 (409600)
I0703 01:28:04.891887 31050 net.cpp:163] Memory required for data: 791347200
I0703 01:28:04.891891 31050 layer_factory.hpp:77] Creating layer out5a/bn
I0703 01:28:04.891896 31050 net.cpp:98] Creating Layer out5a/bn
I0703 01:28:04.891907 31050 net.cpp:439] out5a/bn <- out5a
I0703 01:28:04.891911 31050 net.cpp:413] out5a/bn -> out5a/bn
I0703 01:28:04.892448 31050 net.cpp:148] Setting up out5a/bn
I0703 01:28:04.892453 31050 net.cpp:155] Top shape: 4 64 40 40 (409600)
I0703 01:28:04.892457 31050 net.cpp:163] Memory required for data: 792985600
I0703 01:28:04.892463 31050 layer_factory.hpp:77] Creating layer out5a/relu
I0703 01:28:04.892467 31050 net.cpp:98] Creating Layer out5a/relu
I0703 01:28:04.892468 31050 net.cpp:439] out5a/relu <- out5a/bn
I0703 01:28:04.892472 31050 net.cpp:400] out5a/relu -> out5a/bn (in-place)
I0703 01:28:04.892474 31050 net.cpp:148] Setting up out5a/relu
I0703 01:28:04.892477 31050 net.cpp:155] Top shape: 4 64 40 40 (409600)
I0703 01:28:04.892478 31050 net.cpp:163] Memory required for data: 794624000
I0703 01:28:04.892480 31050 layer_factory.hpp:77] Creating layer out5a_up2
I0703 01:28:04.892484 31050 net.cpp:98] Creating Layer out5a_up2
I0703 01:28:04.892487 31050 net.cpp:439] out5a_up2 <- out5a/bn
I0703 01:28:04.892488 31050 net.cpp:413] out5a_up2 -> out5a_up2
I0703 01:28:04.892678 31050 net.cpp:148] Setting up out5a_up2
I0703 01:28:04.892681 31050 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 01:28:04.892684 31050 net.cpp:163] Memory required for data: 801177600
I0703 01:28:04.892688 31050 layer_factory.hpp:77] Creating layer out3a
I0703 01:28:04.892691 31050 net.cpp:98] Creating Layer out3a
I0703 01:28:04.892693 31050 net.cpp:439] out3a <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0703 01:28:04.892699 31050 net.cpp:413] out3a -> out3a
I0703 01:28:04.894356 31050 net.cpp:148] Setting up out3a
I0703 01:28:04.894364 31050 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 01:28:04.894366 31050 net.cpp:163] Memory required for data: 807731200
I0703 01:28:04.894371 31050 layer_factory.hpp:77] Creating layer out3a/bn
I0703 01:28:04.894374 31050 net.cpp:98] Creating Layer out3a/bn
I0703 01:28:04.894377 31050 net.cpp:439] out3a/bn <- out3a
I0703 01:28:04.894381 31050 net.cpp:413] out3a/bn -> out3a/bn
I0703 01:28:04.894922 31050 net.cpp:148] Setting up out3a/bn
I0703 01:28:04.894927 31050 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 01:28:04.894929 31050 net.cpp:163] Memory required for data: 814284800
I0703 01:28:04.894934 31050 layer_factory.hpp:77] Creating layer out3a/relu
I0703 01:28:04.894937 31050 net.cpp:98] Creating Layer out3a/relu
I0703 01:28:04.894938 31050 net.cpp:439] out3a/relu <- out3a/bn
I0703 01:28:04.894942 31050 net.cpp:400] out3a/relu -> out3a/bn (in-place)
I0703 01:28:04.894944 31050 net.cpp:148] Setting up out3a/relu
I0703 01:28:04.894946 31050 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 01:28:04.894948 31050 net.cpp:163] Memory required for data: 820838400
I0703 01:28:04.894950 31050 layer_factory.hpp:77] Creating layer out3_out5_combined
I0703 01:28:04.894953 31050 net.cpp:98] Creating Layer out3_out5_combined
I0703 01:28:04.894955 31050 net.cpp:439] out3_out5_combined <- out5a_up2
I0703 01:28:04.894958 31050 net.cpp:439] out3_out5_combined <- out3a/bn
I0703 01:28:04.894959 31050 net.cpp:413] out3_out5_combined -> out3_out5_combined
I0703 01:28:04.894979 31050 net.cpp:148] Setting up out3_out5_combined
I0703 01:28:04.894984 31050 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 01:28:04.894985 31050 net.cpp:163] Memory required for data: 827392000
I0703 01:28:04.894987 31050 layer_factory.hpp:77] Creating layer ctx_conv1
I0703 01:28:04.894990 31050 net.cpp:98] Creating Layer ctx_conv1
I0703 01:28:04.894992 31050 net.cpp:439] ctx_conv1 <- out3_out5_combined
I0703 01:28:04.894995 31050 net.cpp:413] ctx_conv1 -> ctx_conv1
I0703 01:28:04.895939 31050 net.cpp:148] Setting up ctx_conv1
I0703 01:28:04.895944 31050 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 01:28:04.895946 31050 net.cpp:163] Memory required for data: 833945600
I0703 01:28:04.895949 31050 layer_factory.hpp:77] Creating layer ctx_conv1/bn
I0703 01:28:04.895953 31050 net.cpp:98] Creating Layer ctx_conv1/bn
I0703 01:28:04.895956 31050 net.cpp:439] ctx_conv1/bn <- ctx_conv1
I0703 01:28:04.895964 31050 net.cpp:413] ctx_conv1/bn -> ctx_conv1/bn
I0703 01:28:04.896492 31050 net.cpp:148] Setting up ctx_conv1/bn
I0703 01:28:04.896498 31050 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 01:28:04.896500 31050 net.cpp:163] Memory required for data: 840499200
I0703 01:28:04.896504 31050 layer_factory.hpp:77] Creating layer ctx_conv1/relu
I0703 01:28:04.896507 31050 net.cpp:98] Creating Layer ctx_conv1/relu
I0703 01:28:04.896509 31050 net.cpp:439] ctx_conv1/relu <- ctx_conv1/bn
I0703 01:28:04.896512 31050 net.cpp:400] ctx_conv1/relu -> ctx_conv1/bn (in-place)
I0703 01:28:04.896515 31050 net.cpp:148] Setting up ctx_conv1/relu
I0703 01:28:04.896517 31050 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 01:28:04.896519 31050 net.cpp:163] Memory required for data: 847052800
I0703 01:28:04.896522 31050 layer_factory.hpp:77] Creating layer ctx_conv2
I0703 01:28:04.896524 31050 net.cpp:98] Creating Layer ctx_conv2
I0703 01:28:04.896526 31050 net.cpp:439] ctx_conv2 <- ctx_conv1/bn
I0703 01:28:04.896529 31050 net.cpp:413] ctx_conv2 -> ctx_conv2
I0703 01:28:04.897475 31050 net.cpp:148] Setting up ctx_conv2
I0703 01:28:04.897480 31050 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 01:28:04.897481 31050 net.cpp:163] Memory required for data: 853606400
I0703 01:28:04.897485 31050 layer_factory.hpp:77] Creating layer ctx_conv2/bn
I0703 01:28:04.897488 31050 net.cpp:98] Creating Layer ctx_conv2/bn
I0703 01:28:04.897490 31050 net.cpp:439] ctx_conv2/bn <- ctx_conv2
I0703 01:28:04.897493 31050 net.cpp:413] ctx_conv2/bn -> ctx_conv2/bn
I0703 01:28:04.898030 31050 net.cpp:148] Setting up ctx_conv2/bn
I0703 01:28:04.898035 31050 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 01:28:04.898036 31050 net.cpp:163] Memory required for data: 860160000
I0703 01:28:04.898041 31050 layer_factory.hpp:77] Creating layer ctx_conv2/relu
I0703 01:28:04.898046 31050 net.cpp:98] Creating Layer ctx_conv2/relu
I0703 01:28:04.898047 31050 net.cpp:439] ctx_conv2/relu <- ctx_conv2/bn
I0703 01:28:04.898049 31050 net.cpp:400] ctx_conv2/relu -> ctx_conv2/bn (in-place)
I0703 01:28:04.898053 31050 net.cpp:148] Setting up ctx_conv2/relu
I0703 01:28:04.898056 31050 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 01:28:04.898056 31050 net.cpp:163] Memory required for data: 866713600
I0703 01:28:04.898058 31050 layer_factory.hpp:77] Creating layer ctx_conv3
I0703 01:28:04.898062 31050 net.cpp:98] Creating Layer ctx_conv3
I0703 01:28:04.898064 31050 net.cpp:439] ctx_conv3 <- ctx_conv2/bn
I0703 01:28:04.898067 31050 net.cpp:413] ctx_conv3 -> ctx_conv3
I0703 01:28:04.899019 31050 net.cpp:148] Setting up ctx_conv3
I0703 01:28:04.899024 31050 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 01:28:04.899026 31050 net.cpp:163] Memory required for data: 873267200
I0703 01:28:04.899029 31050 layer_factory.hpp:77] Creating layer ctx_conv3/bn
I0703 01:28:04.899034 31050 net.cpp:98] Creating Layer ctx_conv3/bn
I0703 01:28:04.899035 31050 net.cpp:439] ctx_conv3/bn <- ctx_conv3
I0703 01:28:04.899039 31050 net.cpp:413] ctx_conv3/bn -> ctx_conv3/bn
I0703 01:28:04.899581 31050 net.cpp:148] Setting up ctx_conv3/bn
I0703 01:28:04.899586 31050 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 01:28:04.899588 31050 net.cpp:163] Memory required for data: 879820800
I0703 01:28:04.899593 31050 layer_factory.hpp:77] Creating layer ctx_conv3/relu
I0703 01:28:04.899596 31050 net.cpp:98] Creating Layer ctx_conv3/relu
I0703 01:28:04.899598 31050 net.cpp:439] ctx_conv3/relu <- ctx_conv3/bn
I0703 01:28:04.899603 31050 net.cpp:400] ctx_conv3/relu -> ctx_conv3/bn (in-place)
I0703 01:28:04.899607 31050 net.cpp:148] Setting up ctx_conv3/relu
I0703 01:28:04.899610 31050 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 01:28:04.899611 31050 net.cpp:163] Memory required for data: 886374400
I0703 01:28:04.899613 31050 layer_factory.hpp:77] Creating layer ctx_conv4
I0703 01:28:04.899616 31050 net.cpp:98] Creating Layer ctx_conv4
I0703 01:28:04.899619 31050 net.cpp:439] ctx_conv4 <- ctx_conv3/bn
I0703 01:28:04.899622 31050 net.cpp:413] ctx_conv4 -> ctx_conv4
I0703 01:28:04.900574 31050 net.cpp:148] Setting up ctx_conv4
I0703 01:28:04.900585 31050 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 01:28:04.900588 31050 net.cpp:163] Memory required for data: 892928000
I0703 01:28:04.900591 31050 layer_factory.hpp:77] Creating layer ctx_conv4/bn
I0703 01:28:04.900595 31050 net.cpp:98] Creating Layer ctx_conv4/bn
I0703 01:28:04.900598 31050 net.cpp:439] ctx_conv4/bn <- ctx_conv4
I0703 01:28:04.900600 31050 net.cpp:413] ctx_conv4/bn -> ctx_conv4/bn
I0703 01:28:04.901127 31050 net.cpp:148] Setting up ctx_conv4/bn
I0703 01:28:04.901132 31050 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 01:28:04.901134 31050 net.cpp:163] Memory required for data: 899481600
I0703 01:28:04.901139 31050 layer_factory.hpp:77] Creating layer ctx_conv4/relu
I0703 01:28:04.901142 31050 net.cpp:98] Creating Layer ctx_conv4/relu
I0703 01:28:04.901144 31050 net.cpp:439] ctx_conv4/relu <- ctx_conv4/bn
I0703 01:28:04.901146 31050 net.cpp:400] ctx_conv4/relu -> ctx_conv4/bn (in-place)
I0703 01:28:04.901149 31050 net.cpp:148] Setting up ctx_conv4/relu
I0703 01:28:04.901152 31050 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 01:28:04.901154 31050 net.cpp:163] Memory required for data: 906035200
I0703 01:28:04.901155 31050 layer_factory.hpp:77] Creating layer ctx_final
I0703 01:28:04.901160 31050 net.cpp:98] Creating Layer ctx_final
I0703 01:28:04.901161 31050 net.cpp:439] ctx_final <- ctx_conv4/bn
I0703 01:28:04.901163 31050 net.cpp:413] ctx_final -> ctx_final
I0703 01:28:04.901484 31050 net.cpp:148] Setting up ctx_final
I0703 01:28:04.901489 31050 net.cpp:155] Top shape: 4 8 80 80 (204800)
I0703 01:28:04.901491 31050 net.cpp:163] Memory required for data: 906854400
I0703 01:28:04.901495 31050 layer_factory.hpp:77] Creating layer ctx_final/relu
I0703 01:28:04.901497 31050 net.cpp:98] Creating Layer ctx_final/relu
I0703 01:28:04.901499 31050 net.cpp:439] ctx_final/relu <- ctx_final
I0703 01:28:04.901502 31050 net.cpp:400] ctx_final/relu -> ctx_final (in-place)
I0703 01:28:04.901504 31050 net.cpp:148] Setting up ctx_final/relu
I0703 01:28:04.901507 31050 net.cpp:155] Top shape: 4 8 80 80 (204800)
I0703 01:28:04.901509 31050 net.cpp:163] Memory required for data: 907673600
I0703 01:28:04.901510 31050 layer_factory.hpp:77] Creating layer out_deconv_final_up2
I0703 01:28:04.901515 31050 net.cpp:98] Creating Layer out_deconv_final_up2
I0703 01:28:04.901515 31050 net.cpp:439] out_deconv_final_up2 <- ctx_final
I0703 01:28:04.901518 31050 net.cpp:413] out_deconv_final_up2 -> out_deconv_final_up2
I0703 01:28:04.901687 31050 net.cpp:148] Setting up out_deconv_final_up2
I0703 01:28:04.901692 31050 net.cpp:155] Top shape: 4 8 160 160 (819200)
I0703 01:28:04.901693 31050 net.cpp:163] Memory required for data: 910950400
I0703 01:28:04.901696 31050 layer_factory.hpp:77] Creating layer out_deconv_final_up4
I0703 01:28:04.901700 31050 net.cpp:98] Creating Layer out_deconv_final_up4
I0703 01:28:04.901701 31050 net.cpp:439] out_deconv_final_up4 <- out_deconv_final_up2
I0703 01:28:04.901705 31050 net.cpp:413] out_deconv_final_up4 -> out_deconv_final_up4
I0703 01:28:04.901871 31050 net.cpp:148] Setting up out_deconv_final_up4
I0703 01:28:04.901875 31050 net.cpp:155] Top shape: 4 8 320 320 (3276800)
I0703 01:28:04.901877 31050 net.cpp:163] Memory required for data: 924057600
I0703 01:28:04.901880 31050 layer_factory.hpp:77] Creating layer out_deconv_final_up8
I0703 01:28:04.901882 31050 net.cpp:98] Creating Layer out_deconv_final_up8
I0703 01:28:04.901885 31050 net.cpp:439] out_deconv_final_up8 <- out_deconv_final_up4
I0703 01:28:04.901887 31050 net.cpp:413] out_deconv_final_up8 -> out_deconv_final_up8
I0703 01:28:04.902055 31050 net.cpp:148] Setting up out_deconv_final_up8
I0703 01:28:04.902058 31050 net.cpp:155] Top shape: 4 8 640 640 (13107200)
I0703 01:28:04.902060 31050 net.cpp:163] Memory required for data: 976486400
I0703 01:28:04.902063 31050 layer_factory.hpp:77] Creating layer out_deconv_final_up8_out_deconv_final_up8_0_split
I0703 01:28:04.902066 31050 net.cpp:98] Creating Layer out_deconv_final_up8_out_deconv_final_up8_0_split
I0703 01:28:04.902073 31050 net.cpp:439] out_deconv_final_up8_out_deconv_final_up8_0_split <- out_deconv_final_up8
I0703 01:28:04.902076 31050 net.cpp:413] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0703 01:28:04.902079 31050 net.cpp:413] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0703 01:28:04.902082 31050 net.cpp:413] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0703 01:28:04.902122 31050 net.cpp:148] Setting up out_deconv_final_up8_out_deconv_final_up8_0_split
I0703 01:28:04.902127 31050 net.cpp:155] Top shape: 4 8 640 640 (13107200)
I0703 01:28:04.902128 31050 net.cpp:155] Top shape: 4 8 640 640 (13107200)
I0703 01:28:04.902130 31050 net.cpp:155] Top shape: 4 8 640 640 (13107200)
I0703 01:28:04.902132 31050 net.cpp:163] Memory required for data: 1133772800
I0703 01:28:04.902134 31050 layer_factory.hpp:77] Creating layer loss
I0703 01:28:04.902138 31050 net.cpp:98] Creating Layer loss
I0703 01:28:04.902140 31050 net.cpp:439] loss <- out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0703 01:28:04.902143 31050 net.cpp:439] loss <- label_data_1_split_0
I0703 01:28:04.902145 31050 net.cpp:413] loss -> loss
I0703 01:28:04.902150 31050 layer_factory.hpp:77] Creating layer loss
I0703 01:28:04.917938 31050 net.cpp:148] Setting up loss
I0703 01:28:04.917961 31050 net.cpp:155] Top shape: (1)
I0703 01:28:04.917963 31050 net.cpp:158]     with loss weight 1
I0703 01:28:04.917970 31050 net.cpp:163] Memory required for data: 1133772804
I0703 01:28:04.917974 31050 layer_factory.hpp:77] Creating layer accuracy/top1
I0703 01:28:04.917982 31050 net.cpp:98] Creating Layer accuracy/top1
I0703 01:28:04.917985 31050 net.cpp:439] accuracy/top1 <- out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0703 01:28:04.917990 31050 net.cpp:439] accuracy/top1 <- label_data_1_split_1
I0703 01:28:04.917994 31050 net.cpp:413] accuracy/top1 -> accuracy/top1
I0703 01:28:04.918001 31050 net.cpp:148] Setting up accuracy/top1
I0703 01:28:04.918004 31050 net.cpp:155] Top shape: (1)
I0703 01:28:04.918007 31050 net.cpp:163] Memory required for data: 1133772808
I0703 01:28:04.918009 31050 layer_factory.hpp:77] Creating layer accuracy/top5
I0703 01:28:04.918014 31050 net.cpp:98] Creating Layer accuracy/top5
I0703 01:28:04.918015 31050 net.cpp:439] accuracy/top5 <- out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0703 01:28:04.918018 31050 net.cpp:439] accuracy/top5 <- label_data_1_split_2
I0703 01:28:04.918021 31050 net.cpp:413] accuracy/top5 -> accuracy/top5
I0703 01:28:04.918025 31050 net.cpp:148] Setting up accuracy/top5
I0703 01:28:04.918028 31050 net.cpp:155] Top shape: (1)
I0703 01:28:04.918032 31050 net.cpp:163] Memory required for data: 1133772812
I0703 01:28:04.918036 31050 net.cpp:226] accuracy/top5 does not need backward computation.
I0703 01:28:04.918038 31050 net.cpp:226] accuracy/top1 does not need backward computation.
I0703 01:28:04.918041 31050 net.cpp:224] loss needs backward computation.
I0703 01:28:04.918046 31050 net.cpp:224] out_deconv_final_up8_out_deconv_final_up8_0_split needs backward computation.
I0703 01:28:04.918051 31050 net.cpp:224] out_deconv_final_up8 needs backward computation.
I0703 01:28:04.918053 31050 net.cpp:224] out_deconv_final_up4 needs backward computation.
I0703 01:28:04.918056 31050 net.cpp:224] out_deconv_final_up2 needs backward computation.
I0703 01:28:04.918061 31050 net.cpp:224] ctx_final/relu needs backward computation.
I0703 01:28:04.918064 31050 net.cpp:224] ctx_final needs backward computation.
I0703 01:28:04.918067 31050 net.cpp:224] ctx_conv4/relu needs backward computation.
I0703 01:28:04.918071 31050 net.cpp:224] ctx_conv4/bn needs backward computation.
I0703 01:28:04.918073 31050 net.cpp:224] ctx_conv4 needs backward computation.
I0703 01:28:04.918078 31050 net.cpp:224] ctx_conv3/relu needs backward computation.
I0703 01:28:04.918081 31050 net.cpp:224] ctx_conv3/bn needs backward computation.
I0703 01:28:04.918085 31050 net.cpp:224] ctx_conv3 needs backward computation.
I0703 01:28:04.918098 31050 net.cpp:224] ctx_conv2/relu needs backward computation.
I0703 01:28:04.918102 31050 net.cpp:224] ctx_conv2/bn needs backward computation.
I0703 01:28:04.918104 31050 net.cpp:224] ctx_conv2 needs backward computation.
I0703 01:28:04.918107 31050 net.cpp:224] ctx_conv1/relu needs backward computation.
I0703 01:28:04.918108 31050 net.cpp:224] ctx_conv1/bn needs backward computation.
I0703 01:28:04.918112 31050 net.cpp:224] ctx_conv1 needs backward computation.
I0703 01:28:04.918114 31050 net.cpp:224] out3_out5_combined needs backward computation.
I0703 01:28:04.918118 31050 net.cpp:224] out3a/relu needs backward computation.
I0703 01:28:04.918120 31050 net.cpp:224] out3a/bn needs backward computation.
I0703 01:28:04.918123 31050 net.cpp:224] out3a needs backward computation.
I0703 01:28:04.918126 31050 net.cpp:224] out5a_up2 needs backward computation.
I0703 01:28:04.918130 31050 net.cpp:224] out5a/relu needs backward computation.
I0703 01:28:04.918133 31050 net.cpp:224] out5a/bn needs backward computation.
I0703 01:28:04.918136 31050 net.cpp:224] out5a needs backward computation.
I0703 01:28:04.918141 31050 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0703 01:28:04.918144 31050 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0703 01:28:04.918148 31050 net.cpp:224] res5a_branch2b needs backward computation.
I0703 01:28:04.918151 31050 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0703 01:28:04.918155 31050 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0703 01:28:04.918157 31050 net.cpp:224] res5a_branch2a needs backward computation.
I0703 01:28:04.918161 31050 net.cpp:224] pool4 needs backward computation.
I0703 01:28:04.918165 31050 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0703 01:28:04.918169 31050 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0703 01:28:04.918172 31050 net.cpp:224] res4a_branch2b needs backward computation.
I0703 01:28:04.918176 31050 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0703 01:28:04.918179 31050 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0703 01:28:04.918184 31050 net.cpp:224] res4a_branch2a needs backward computation.
I0703 01:28:04.918187 31050 net.cpp:224] pool3 needs backward computation.
I0703 01:28:04.918191 31050 net.cpp:224] res3a_branch2b/bn_res3a_branch2b/relu_0_split needs backward computation.
I0703 01:28:04.918195 31050 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0703 01:28:04.918198 31050 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0703 01:28:04.918201 31050 net.cpp:224] res3a_branch2b needs backward computation.
I0703 01:28:04.918205 31050 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0703 01:28:04.918208 31050 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0703 01:28:04.918212 31050 net.cpp:224] res3a_branch2a needs backward computation.
I0703 01:28:04.918216 31050 net.cpp:224] pool2 needs backward computation.
I0703 01:28:04.918220 31050 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0703 01:28:04.918223 31050 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0703 01:28:04.918227 31050 net.cpp:224] res2a_branch2b needs backward computation.
I0703 01:28:04.918231 31050 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0703 01:28:04.918234 31050 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0703 01:28:04.918238 31050 net.cpp:224] res2a_branch2a needs backward computation.
I0703 01:28:04.918242 31050 net.cpp:224] pool1 needs backward computation.
I0703 01:28:04.918246 31050 net.cpp:224] conv1b/relu needs backward computation.
I0703 01:28:04.918249 31050 net.cpp:224] conv1b/bn needs backward computation.
I0703 01:28:04.918253 31050 net.cpp:224] conv1b needs backward computation.
I0703 01:28:04.918257 31050 net.cpp:224] conv1a/relu needs backward computation.
I0703 01:28:04.918261 31050 net.cpp:224] conv1a/bn needs backward computation.
I0703 01:28:04.918264 31050 net.cpp:224] conv1a needs backward computation.
I0703 01:28:04.918272 31050 net.cpp:226] data/bias does not need backward computation.
I0703 01:28:04.918277 31050 net.cpp:226] label_data_1_split does not need backward computation.
I0703 01:28:04.918280 31050 net.cpp:226] data does not need backward computation.
I0703 01:28:04.918283 31050 net.cpp:268] This network produces output accuracy/top1
I0703 01:28:04.918287 31050 net.cpp:268] This network produces output accuracy/top5
I0703 01:28:04.918290 31050 net.cpp:268] This network produces output loss
I0703 01:28:04.918321 31050 net.cpp:288] Network initialization done.
I0703 01:28:04.918424 31050 solver.cpp:60] Solver scaffolding done.
I0703 01:28:04.923869 31050 caffe.cpp:145] Finetuning from training/cityscapes5_jsegnet21v2_2017-07-02_23-02-42/initial/cityscapes5_jsegnet21v2_iter_32000.caffemodel
W0703 01:28:04.952996 31050 parallel.cpp:400] Batch size must be divisible by the number of solvers (GPUs)
I0703 01:28:04.959111 31050 data_layer.cpp:78] ReshapePrefetch 8, 3, 640, 640
I0703 01:28:04.959216 31050 data_layer.cpp:83] output data size: 8,3,640,640
I0703 01:28:05.032171 31050 data_layer.cpp:78] ReshapePrefetch 8, 1, 640, 640
I0703 01:28:05.032281 31050 data_layer.cpp:83] output data size: 8,1,640,640
I0703 01:28:05.811269 31050 parallel.cpp:334] Starting Optimization
I0703 01:28:05.811312 31050 net.cpp:1824] All zero weights of convolution layers are frozen
I0703 01:28:05.819553 31050 solver.cpp:415] Solving jsegnet21v2_train
I0703 01:28:05.819568 31050 solver.cpp:416] Learning Rate Policy: multistep
I0703 01:28:06.270140 31050 solver.cpp:290] Iteration 0 (0 iter/s, 0.450541s/100 iter), loss = 0.022395
I0703 01:28:06.270164 31050 solver.cpp:309]     Train net output #0: loss = 0.022395 (* 1 = 0.022395 loss)
I0703 01:28:06.270171 31050 sgd_solver.cpp:106] Iteration 0, lr = 1e-05
I0703 01:28:26.186594 31166 blocking_queue.cpp:50] Data layer prefetch queue empty
I0703 01:28:30.763746 31050 solver.cpp:290] Iteration 100 (4.08282 iter/s, 24.4929s/100 iter), loss = 0.0334281
I0703 01:28:30.763768 31050 solver.cpp:309]     Train net output #0: loss = 0.0334281 (* 1 = 0.0334281 loss)
I0703 01:28:30.763775 31050 sgd_solver.cpp:106] Iteration 100, lr = 1e-05
I0703 01:28:55.264039 31050 solver.cpp:290] Iteration 200 (4.0817 iter/s, 24.4996s/100 iter), loss = 0.0173251
I0703 01:28:55.264284 31050 solver.cpp:309]     Train net output #0: loss = 0.0173251 (* 1 = 0.0173251 loss)
I0703 01:28:55.264295 31050 sgd_solver.cpp:106] Iteration 200, lr = 1e-05
I0703 01:29:19.749325 31050 solver.cpp:290] Iteration 300 (4.08424 iter/s, 24.4844s/100 iter), loss = 0.0198397
I0703 01:29:19.749351 31050 solver.cpp:309]     Train net output #0: loss = 0.0198397 (* 1 = 0.0198397 loss)
I0703 01:29:19.749359 31050 sgd_solver.cpp:106] Iteration 300, lr = 1e-05
I0703 01:29:44.247344 31050 solver.cpp:290] Iteration 400 (4.08208 iter/s, 24.4973s/100 iter), loss = 0.0171196
I0703 01:29:44.247448 31050 solver.cpp:309]     Train net output #0: loss = 0.0171196 (* 1 = 0.0171196 loss)
I0703 01:29:44.247459 31050 sgd_solver.cpp:106] Iteration 400, lr = 1e-05
I0703 01:30:08.755560 31050 solver.cpp:290] Iteration 500 (4.08039 iter/s, 24.5074s/100 iter), loss = 0.0194807
I0703 01:30:08.755585 31050 solver.cpp:309]     Train net output #0: loss = 0.0194807 (* 1 = 0.0194807 loss)
I0703 01:30:08.755592 31050 sgd_solver.cpp:106] Iteration 500, lr = 1e-05
I0703 01:30:33.219681 31050 solver.cpp:290] Iteration 600 (4.08773 iter/s, 24.4634s/100 iter), loss = 0.0119504
I0703 01:30:33.219728 31050 solver.cpp:309]     Train net output #0: loss = 0.0119504 (* 1 = 0.0119504 loss)
I0703 01:30:33.219738 31050 sgd_solver.cpp:106] Iteration 600, lr = 1e-05
I0703 01:30:57.760022 31050 solver.cpp:290] Iteration 700 (4.07504 iter/s, 24.5396s/100 iter), loss = 0.0248599
I0703 01:30:57.760044 31050 solver.cpp:309]     Train net output #0: loss = 0.0248599 (* 1 = 0.0248599 loss)
I0703 01:30:57.760052 31050 sgd_solver.cpp:106] Iteration 700, lr = 1e-05
I0703 01:31:22.242445 31050 solver.cpp:290] Iteration 800 (4.08468 iter/s, 24.4817s/100 iter), loss = 0.0217321
I0703 01:31:22.242568 31050 solver.cpp:309]     Train net output #0: loss = 0.0217321 (* 1 = 0.0217321 loss)
I0703 01:31:22.242584 31050 sgd_solver.cpp:106] Iteration 800, lr = 1e-05
I0703 01:31:46.759042 31050 solver.cpp:290] Iteration 900 (4.079 iter/s, 24.5158s/100 iter), loss = 0.0282669
I0703 01:31:46.759064 31050 solver.cpp:309]     Train net output #0: loss = 0.0282669 (* 1 = 0.0282669 loss)
I0703 01:31:46.759071 31050 sgd_solver.cpp:106] Iteration 900, lr = 1e-05
I0703 01:32:11.013408 31050 solver.cpp:354] Sparsity after update:
I0703 01:32:11.064505 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 01:32:11.064522 31050 net.cpp:1851] conv1a_param_0(0) 
I0703 01:32:11.064534 31050 net.cpp:1851] conv1b_param_0(0) 
I0703 01:32:11.064537 31050 net.cpp:1851] ctx_conv1_param_0(0) 
I0703 01:32:11.064538 31050 net.cpp:1851] ctx_conv2_param_0(0) 
I0703 01:32:11.064540 31050 net.cpp:1851] ctx_conv3_param_0(0) 
I0703 01:32:11.064543 31050 net.cpp:1851] ctx_conv4_param_0(0) 
I0703 01:32:11.064544 31050 net.cpp:1851] ctx_final_param_0(0) 
I0703 01:32:11.064545 31050 net.cpp:1851] out3a_param_0(0) 
I0703 01:32:11.064548 31050 net.cpp:1851] out5a_param_0(0) 
I0703 01:32:11.064550 31050 net.cpp:1851] res2a_branch2a_param_0(0) 
I0703 01:32:11.064553 31050 net.cpp:1851] res2a_branch2b_param_0(0) 
I0703 01:32:11.064554 31050 net.cpp:1851] res3a_branch2a_param_0(0) 
I0703 01:32:11.064556 31050 net.cpp:1851] res3a_branch2b_param_0(0) 
I0703 01:32:11.064558 31050 net.cpp:1851] res4a_branch2a_param_0(0) 
I0703 01:32:11.064560 31050 net.cpp:1851] res4a_branch2b_param_0(0) 
I0703 01:32:11.064561 31050 net.cpp:1851] res5a_branch2a_param_0(0) 
I0703 01:32:11.064563 31050 net.cpp:1851] res5a_branch2b_param_0(0) 
I0703 01:32:11.064565 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0703 01:32:11.297063 31050 solver.cpp:290] Iteration 1000 (4.07542 iter/s, 24.5373s/100 iter), loss = 0.0128293
I0703 01:32:11.297089 31050 solver.cpp:309]     Train net output #0: loss = 0.0128293 (* 1 = 0.0128293 loss)
I0703 01:32:11.297096 31050 sgd_solver.cpp:106] Iteration 1000, lr = 1e-05
I0703 01:32:35.792799 31050 solver.cpp:290] Iteration 1100 (4.08246 iter/s, 24.495s/100 iter), loss = 0.0340053
I0703 01:32:35.792822 31050 solver.cpp:309]     Train net output #0: loss = 0.0340053 (* 1 = 0.0340053 loss)
I0703 01:32:35.792830 31050 sgd_solver.cpp:106] Iteration 1100, lr = 1e-05
I0703 01:33:00.284731 31050 solver.cpp:290] Iteration 1200 (4.08309 iter/s, 24.4912s/100 iter), loss = 0.0291324
I0703 01:33:00.284842 31050 solver.cpp:309]     Train net output #0: loss = 0.0291324 (* 1 = 0.0291324 loss)
I0703 01:33:00.284852 31050 sgd_solver.cpp:106] Iteration 1200, lr = 1e-05
I0703 01:33:24.776051 31050 solver.cpp:290] Iteration 1300 (4.08321 iter/s, 24.4906s/100 iter), loss = 0.0219733
I0703 01:33:24.776073 31050 solver.cpp:309]     Train net output #0: loss = 0.0219733 (* 1 = 0.0219733 loss)
I0703 01:33:24.776080 31050 sgd_solver.cpp:106] Iteration 1300, lr = 1e-05
I0703 01:33:49.277182 31050 solver.cpp:290] Iteration 1400 (4.08156 iter/s, 24.5004s/100 iter), loss = 0.0245267
I0703 01:33:49.277289 31050 solver.cpp:309]     Train net output #0: loss = 0.0245267 (* 1 = 0.0245267 loss)
I0703 01:33:49.277299 31050 sgd_solver.cpp:106] Iteration 1400, lr = 1e-05
I0703 01:34:13.745435 31050 solver.cpp:290] Iteration 1500 (4.08706 iter/s, 24.4675s/100 iter), loss = 0.0298389
I0703 01:34:13.745457 31050 solver.cpp:309]     Train net output #0: loss = 0.0298389 (* 1 = 0.0298389 loss)
I0703 01:34:13.745465 31050 sgd_solver.cpp:106] Iteration 1500, lr = 1e-05
I0703 01:34:38.260201 31050 solver.cpp:290] Iteration 1600 (4.07929 iter/s, 24.5141s/100 iter), loss = 0.0261295
I0703 01:34:38.260313 31050 solver.cpp:309]     Train net output #0: loss = 0.0261295 (* 1 = 0.0261295 loss)
I0703 01:34:38.260324 31050 sgd_solver.cpp:106] Iteration 1600, lr = 1e-05
I0703 01:35:02.752092 31050 solver.cpp:290] Iteration 1700 (4.08311 iter/s, 24.4911s/100 iter), loss = 0.0210975
I0703 01:35:02.752116 31050 solver.cpp:309]     Train net output #0: loss = 0.0210975 (* 1 = 0.0210975 loss)
I0703 01:35:02.752123 31050 sgd_solver.cpp:106] Iteration 1700, lr = 1e-05
I0703 01:35:27.264531 31050 solver.cpp:290] Iteration 1800 (4.07968 iter/s, 24.5118s/100 iter), loss = 0.0328745
I0703 01:35:27.264660 31050 solver.cpp:309]     Train net output #0: loss = 0.0328745 (* 1 = 0.0328745 loss)
I0703 01:35:27.264670 31050 sgd_solver.cpp:106] Iteration 1800, lr = 1e-05
I0703 01:35:51.778623 31050 solver.cpp:290] Iteration 1900 (4.07942 iter/s, 24.5133s/100 iter), loss = 0.0226709
I0703 01:35:51.778650 31050 solver.cpp:309]     Train net output #0: loss = 0.0226709 (* 1 = 0.0226709 loss)
I0703 01:35:51.778657 31050 sgd_solver.cpp:106] Iteration 1900, lr = 1e-05
I0703 01:36:16.027067 31050 solver.cpp:354] Sparsity after update:
I0703 01:36:16.028832 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 01:36:16.028839 31050 net.cpp:1851] conv1a_param_0(0) 
I0703 01:36:16.028846 31050 net.cpp:1851] conv1b_param_0(0) 
I0703 01:36:16.028848 31050 net.cpp:1851] ctx_conv1_param_0(0) 
I0703 01:36:16.028851 31050 net.cpp:1851] ctx_conv2_param_0(0) 
I0703 01:36:16.028852 31050 net.cpp:1851] ctx_conv3_param_0(0) 
I0703 01:36:16.028854 31050 net.cpp:1851] ctx_conv4_param_0(0) 
I0703 01:36:16.028856 31050 net.cpp:1851] ctx_final_param_0(0) 
I0703 01:36:16.028858 31050 net.cpp:1851] out3a_param_0(0) 
I0703 01:36:16.028861 31050 net.cpp:1851] out5a_param_0(0) 
I0703 01:36:16.028862 31050 net.cpp:1851] res2a_branch2a_param_0(0) 
I0703 01:36:16.028863 31050 net.cpp:1851] res2a_branch2b_param_0(0) 
I0703 01:36:16.028865 31050 net.cpp:1851] res3a_branch2a_param_0(0) 
I0703 01:36:16.028868 31050 net.cpp:1851] res3a_branch2b_param_0(0) 
I0703 01:36:16.028869 31050 net.cpp:1851] res4a_branch2a_param_0(0) 
I0703 01:36:16.028872 31050 net.cpp:1851] res4a_branch2b_param_0(0) 
I0703 01:36:16.028873 31050 net.cpp:1851] res5a_branch2a_param_0(0) 
I0703 01:36:16.028875 31050 net.cpp:1851] res5a_branch2b_param_0(0) 
I0703 01:36:16.028878 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0703 01:36:16.029022 31050 solver.cpp:473] Iteration 2000, Testing net (#0)
I0703 01:37:05.767746 31050 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.954833
I0703 01:37:05.767829 31050 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.999852
I0703 01:37:05.767838 31050 solver.cpp:546]     Test net output #2: loss = 0.141555 (* 1 = 0.141555 loss)
I0703 01:37:06.017689 31050 solver.cpp:290] Iteration 2000 (1.34704 iter/s, 74.237s/100 iter), loss = 0.0238302
I0703 01:37:06.017717 31050 solver.cpp:309]     Train net output #0: loss = 0.0238302 (* 1 = 0.0238302 loss)
I0703 01:37:06.017724 31050 sgd_solver.cpp:106] Iteration 2000, lr = 1e-05
I0703 01:37:29.540304 31050 solver.cpp:290] Iteration 2100 (4.25135 iter/s, 23.5219s/100 iter), loss = 0.0234076
I0703 01:37:29.540329 31050 solver.cpp:309]     Train net output #0: loss = 0.0234076 (* 1 = 0.0234076 loss)
I0703 01:37:29.540336 31050 sgd_solver.cpp:106] Iteration 2100, lr = 1e-05
I0703 01:37:55.113137 31050 solver.cpp:290] Iteration 2200 (3.91051 iter/s, 25.5721s/100 iter), loss = 0.0172971
I0703 01:37:55.113183 31050 solver.cpp:309]     Train net output #0: loss = 0.017297 (* 1 = 0.017297 loss)
I0703 01:37:55.113193 31050 sgd_solver.cpp:106] Iteration 2200, lr = 1e-05
I0703 01:38:19.269459 31050 solver.cpp:290] Iteration 2300 (4.13982 iter/s, 24.1556s/100 iter), loss = 0.0182444
I0703 01:38:19.269482 31050 solver.cpp:309]     Train net output #0: loss = 0.0182443 (* 1 = 0.0182443 loss)
I0703 01:38:19.269490 31050 sgd_solver.cpp:106] Iteration 2300, lr = 1e-05
I0703 01:38:43.435806 31050 solver.cpp:290] Iteration 2400 (4.1381 iter/s, 24.1657s/100 iter), loss = 0.0199583
I0703 01:38:43.435922 31050 solver.cpp:309]     Train net output #0: loss = 0.0199583 (* 1 = 0.0199583 loss)
I0703 01:38:43.435932 31050 sgd_solver.cpp:106] Iteration 2400, lr = 1e-05
I0703 01:39:07.571527 31050 solver.cpp:290] Iteration 2500 (4.14337 iter/s, 24.135s/100 iter), loss = 0.0260882
I0703 01:39:07.571552 31050 solver.cpp:309]     Train net output #0: loss = 0.0260882 (* 1 = 0.0260882 loss)
I0703 01:39:07.571558 31050 sgd_solver.cpp:106] Iteration 2500, lr = 1e-05
I0703 01:39:31.793802 31050 solver.cpp:290] Iteration 2600 (4.12855 iter/s, 24.2216s/100 iter), loss = 0.0204204
I0703 01:39:31.793931 31050 solver.cpp:309]     Train net output #0: loss = 0.0204204 (* 1 = 0.0204204 loss)
I0703 01:39:31.793941 31050 sgd_solver.cpp:106] Iteration 2600, lr = 1e-05
I0703 01:39:55.946151 31050 solver.cpp:290] Iteration 2700 (4.14052 iter/s, 24.1516s/100 iter), loss = 0.014554
I0703 01:39:55.946202 31050 solver.cpp:309]     Train net output #0: loss = 0.014554 (* 1 = 0.014554 loss)
I0703 01:39:55.946213 31050 sgd_solver.cpp:106] Iteration 2700, lr = 1e-05
I0703 01:40:20.138648 31050 solver.cpp:290] Iteration 2800 (4.13363 iter/s, 24.1918s/100 iter), loss = 0.0493881
I0703 01:40:20.138756 31050 solver.cpp:309]     Train net output #0: loss = 0.0493881 (* 1 = 0.0493881 loss)
I0703 01:40:20.138767 31050 sgd_solver.cpp:106] Iteration 2800, lr = 1e-05
I0703 01:40:44.319144 31050 solver.cpp:290] Iteration 2900 (4.13569 iter/s, 24.1797s/100 iter), loss = 0.0251712
I0703 01:40:44.319167 31050 solver.cpp:309]     Train net output #0: loss = 0.0251712 (* 1 = 0.0251712 loss)
I0703 01:40:44.319175 31050 sgd_solver.cpp:106] Iteration 2900, lr = 1e-05
I0703 01:41:08.241856 31050 solver.cpp:354] Sparsity after update:
I0703 01:41:08.282577 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 01:41:08.282594 31050 net.cpp:1851] conv1a_param_0(0) 
I0703 01:41:08.282603 31050 net.cpp:1851] conv1b_param_0(0) 
I0703 01:41:08.282604 31050 net.cpp:1851] ctx_conv1_param_0(0) 
I0703 01:41:08.282606 31050 net.cpp:1851] ctx_conv2_param_0(0) 
I0703 01:41:08.282608 31050 net.cpp:1851] ctx_conv3_param_0(0) 
I0703 01:41:08.282610 31050 net.cpp:1851] ctx_conv4_param_0(0) 
I0703 01:41:08.282613 31050 net.cpp:1851] ctx_final_param_0(0) 
I0703 01:41:08.282614 31050 net.cpp:1851] out3a_param_0(0) 
I0703 01:41:08.282616 31050 net.cpp:1851] out5a_param_0(0) 
I0703 01:41:08.282618 31050 net.cpp:1851] res2a_branch2a_param_0(0) 
I0703 01:41:08.282620 31050 net.cpp:1851] res2a_branch2b_param_0(0) 
I0703 01:41:08.282622 31050 net.cpp:1851] res3a_branch2a_param_0(0) 
I0703 01:41:08.282624 31050 net.cpp:1851] res3a_branch2b_param_0(0) 
I0703 01:41:08.282626 31050 net.cpp:1851] res4a_branch2a_param_0(0) 
I0703 01:41:08.282627 31050 net.cpp:1851] res4a_branch2b_param_0(0) 
I0703 01:41:08.282629 31050 net.cpp:1851] res5a_branch2a_param_0(0) 
I0703 01:41:08.282631 31050 net.cpp:1851] res5a_branch2b_param_0(0) 
I0703 01:41:08.282634 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0703 01:41:08.513631 31050 solver.cpp:290] Iteration 3000 (4.13329 iter/s, 24.1938s/100 iter), loss = 0.0305134
I0703 01:41:08.513655 31050 solver.cpp:309]     Train net output #0: loss = 0.0305134 (* 1 = 0.0305134 loss)
I0703 01:41:08.513662 31050 sgd_solver.cpp:106] Iteration 3000, lr = 1e-05
I0703 01:41:32.649389 31050 solver.cpp:290] Iteration 3100 (4.14335 iter/s, 24.135s/100 iter), loss = 0.0275533
I0703 01:41:32.649413 31050 solver.cpp:309]     Train net output #0: loss = 0.0275533 (* 1 = 0.0275533 loss)
I0703 01:41:32.649420 31050 sgd_solver.cpp:106] Iteration 3100, lr = 1e-05
I0703 01:41:56.824509 31050 solver.cpp:290] Iteration 3200 (4.13661 iter/s, 24.1744s/100 iter), loss = 0.0196518
I0703 01:41:56.824568 31050 solver.cpp:309]     Train net output #0: loss = 0.0196518 (* 1 = 0.0196518 loss)
I0703 01:41:56.824575 31050 sgd_solver.cpp:106] Iteration 3200, lr = 1e-05
I0703 01:42:20.954430 31050 solver.cpp:290] Iteration 3300 (4.14436 iter/s, 24.1291s/100 iter), loss = 0.0285065
I0703 01:42:20.954454 31050 solver.cpp:309]     Train net output #0: loss = 0.0285065 (* 1 = 0.0285065 loss)
I0703 01:42:20.954461 31050 sgd_solver.cpp:106] Iteration 3300, lr = 1e-05
I0703 01:42:45.131654 31050 solver.cpp:290] Iteration 3400 (4.13625 iter/s, 24.1765s/100 iter), loss = 0.0164425
I0703 01:42:45.131780 31050 solver.cpp:309]     Train net output #0: loss = 0.0164425 (* 1 = 0.0164425 loss)
I0703 01:42:45.131790 31050 sgd_solver.cpp:106] Iteration 3400, lr = 1e-05
I0703 01:43:09.340595 31050 solver.cpp:290] Iteration 3500 (4.13085 iter/s, 24.2081s/100 iter), loss = 0.0205454
I0703 01:43:09.340620 31050 solver.cpp:309]     Train net output #0: loss = 0.0205454 (* 1 = 0.0205454 loss)
I0703 01:43:09.340627 31050 sgd_solver.cpp:106] Iteration 3500, lr = 1e-05
I0703 01:43:33.482087 31050 solver.cpp:290] Iteration 3600 (4.14237 iter/s, 24.1408s/100 iter), loss = 0.0269406
I0703 01:43:33.482179 31050 solver.cpp:309]     Train net output #0: loss = 0.0269406 (* 1 = 0.0269406 loss)
I0703 01:43:33.482190 31050 sgd_solver.cpp:106] Iteration 3600, lr = 1e-05
I0703 01:43:57.653404 31050 solver.cpp:290] Iteration 3700 (4.13727 iter/s, 24.1705s/100 iter), loss = 0.0221964
I0703 01:43:57.653430 31050 solver.cpp:309]     Train net output #0: loss = 0.0221964 (* 1 = 0.0221964 loss)
I0703 01:43:57.653440 31050 sgd_solver.cpp:106] Iteration 3700, lr = 1e-05
I0703 01:44:21.818341 31050 solver.cpp:290] Iteration 3800 (4.13835 iter/s, 24.1642s/100 iter), loss = 0.0284568
I0703 01:44:21.818378 31050 solver.cpp:309]     Train net output #0: loss = 0.0284568 (* 1 = 0.0284568 loss)
I0703 01:44:21.818390 31050 sgd_solver.cpp:106] Iteration 3800, lr = 1e-05
I0703 01:44:45.951690 31050 solver.cpp:290] Iteration 3900 (4.14377 iter/s, 24.1326s/100 iter), loss = 0.0155338
I0703 01:44:45.951714 31050 solver.cpp:309]     Train net output #0: loss = 0.0155338 (* 1 = 0.0155338 loss)
I0703 01:44:45.951721 31050 sgd_solver.cpp:106] Iteration 3900, lr = 1e-05
I0703 01:45:09.869546 31050 solver.cpp:354] Sparsity after update:
I0703 01:45:09.871435 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 01:45:09.871444 31050 net.cpp:1851] conv1a_param_0(0) 
I0703 01:45:09.871451 31050 net.cpp:1851] conv1b_param_0(0) 
I0703 01:45:09.871454 31050 net.cpp:1851] ctx_conv1_param_0(0) 
I0703 01:45:09.871456 31050 net.cpp:1851] ctx_conv2_param_0(0) 
I0703 01:45:09.871459 31050 net.cpp:1851] ctx_conv3_param_0(0) 
I0703 01:45:09.871461 31050 net.cpp:1851] ctx_conv4_param_0(0) 
I0703 01:45:09.871464 31050 net.cpp:1851] ctx_final_param_0(0) 
I0703 01:45:09.871465 31050 net.cpp:1851] out3a_param_0(0) 
I0703 01:45:09.871467 31050 net.cpp:1851] out5a_param_0(0) 
I0703 01:45:09.871469 31050 net.cpp:1851] res2a_branch2a_param_0(0) 
I0703 01:45:09.871471 31050 net.cpp:1851] res2a_branch2b_param_0(0) 
I0703 01:45:09.871474 31050 net.cpp:1851] res3a_branch2a_param_0(0) 
I0703 01:45:09.871476 31050 net.cpp:1851] res3a_branch2b_param_0(0) 
I0703 01:45:09.871479 31050 net.cpp:1851] res4a_branch2a_param_0(0) 
I0703 01:45:09.871480 31050 net.cpp:1851] res4a_branch2b_param_0(0) 
I0703 01:45:09.871482 31050 net.cpp:1851] res5a_branch2a_param_0(0) 
I0703 01:45:09.871484 31050 net.cpp:1851] res5a_branch2b_param_0(0) 
I0703 01:45:09.871487 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I0703 01:45:09.871696 31050 solver.cpp:473] Iteration 4000, Testing net (#0)
I0703 01:45:58.852124 31050 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.954527
I0703 01:45:58.852211 31050 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.999733
I0703 01:45:58.852218 31050 solver.cpp:546]     Test net output #2: loss = 0.148122 (* 1 = 0.148122 loss)
I0703 01:45:59.114356 31050 solver.cpp:290] Iteration 4000 (1.36686 iter/s, 73.1605s/100 iter), loss = 0.0221232
I0703 01:45:59.114390 31050 solver.cpp:309]     Train net output #0: loss = 0.0221232 (* 1 = 0.0221232 loss)
I0703 01:45:59.114400 31050 sgd_solver.cpp:106] Iteration 4000, lr = 1e-05
I0703 01:45:59.115792 31050 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.05
I0703 01:45:59.323696 31050 net.cpp:1824] All zero weights of convolution layers are frozen
I0703 01:46:23.006377 31050 solver.cpp:290] Iteration 4100 (4.18563 iter/s, 23.8913s/100 iter), loss = 0.0215287
I0703 01:46:23.006409 31050 solver.cpp:309]     Train net output #0: loss = 0.0215286 (* 1 = 0.0215286 loss)
I0703 01:46:23.006417 31050 sgd_solver.cpp:106] Iteration 4100, lr = 1e-05
I0703 01:46:37.299903 31129 blocking_queue.cpp:50] Waiting for data
I0703 01:47:14.330363 31050 solver.cpp:290] Iteration 4200 (1.94847 iter/s, 51.3224s/100 iter), loss = 0.0265287
I0703 01:47:14.330484 31050 solver.cpp:309]     Train net output #0: loss = 0.0265287 (* 1 = 0.0265287 loss)
I0703 01:47:14.330494 31050 sgd_solver.cpp:106] Iteration 4200, lr = 1e-05
I0703 01:47:38.611388 31050 solver.cpp:290] Iteration 4300 (4.11859 iter/s, 24.2802s/100 iter), loss = 0.0211112
I0703 01:47:38.611410 31050 solver.cpp:309]     Train net output #0: loss = 0.0211111 (* 1 = 0.0211111 loss)
I0703 01:47:38.611418 31050 sgd_solver.cpp:106] Iteration 4300, lr = 1e-05
I0703 01:48:02.806324 31050 solver.cpp:290] Iteration 4400 (4.13322 iter/s, 24.1942s/100 iter), loss = 0.0233011
I0703 01:48:02.806432 31050 solver.cpp:309]     Train net output #0: loss = 0.0233011 (* 1 = 0.0233011 loss)
I0703 01:48:02.806447 31050 sgd_solver.cpp:106] Iteration 4400, lr = 1e-05
I0703 01:48:26.954427 31050 solver.cpp:290] Iteration 4500 (4.14125 iter/s, 24.1473s/100 iter), loss = 0.0272907
I0703 01:48:26.954453 31050 solver.cpp:309]     Train net output #0: loss = 0.0272906 (* 1 = 0.0272906 loss)
I0703 01:48:26.954460 31050 sgd_solver.cpp:106] Iteration 4500, lr = 1e-05
I0703 01:48:51.127960 31050 solver.cpp:290] Iteration 4600 (4.13688 iter/s, 24.1728s/100 iter), loss = 0.0179839
I0703 01:48:51.128069 31050 solver.cpp:309]     Train net output #0: loss = 0.0179839 (* 1 = 0.0179839 loss)
I0703 01:48:51.128084 31050 sgd_solver.cpp:106] Iteration 4600, lr = 1e-05
I0703 01:49:15.340709 31050 solver.cpp:290] Iteration 4700 (4.13019 iter/s, 24.2119s/100 iter), loss = 0.0204298
I0703 01:49:15.340734 31050 solver.cpp:309]     Train net output #0: loss = 0.0204298 (* 1 = 0.0204298 loss)
I0703 01:49:15.340740 31050 sgd_solver.cpp:106] Iteration 4700, lr = 1e-05
I0703 01:49:39.475651 31050 solver.cpp:290] Iteration 4800 (4.1435 iter/s, 24.1342s/100 iter), loss = 0.0262297
I0703 01:49:39.475692 31050 solver.cpp:309]     Train net output #0: loss = 0.0262296 (* 1 = 0.0262296 loss)
I0703 01:49:39.475702 31050 sgd_solver.cpp:106] Iteration 4800, lr = 1e-05
I0703 01:50:03.673775 31050 solver.cpp:290] Iteration 4900 (4.13268 iter/s, 24.1974s/100 iter), loss = 0.0203736
I0703 01:50:03.673796 31050 solver.cpp:309]     Train net output #0: loss = 0.0203735 (* 1 = 0.0203735 loss)
I0703 01:50:03.673804 31050 sgd_solver.cpp:106] Iteration 4900, lr = 1e-05
I0703 01:50:27.617161 31050 solver.cpp:354] Sparsity after update:
I0703 01:50:27.659281 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 01:50:27.659297 31050 net.cpp:1851] conv1a_param_0(0) 
I0703 01:50:27.659306 31050 net.cpp:1851] conv1b_param_0(0.0499) 
I0703 01:50:27.659308 31050 net.cpp:1851] ctx_conv1_param_0(0.05) 
I0703 01:50:27.659310 31050 net.cpp:1851] ctx_conv2_param_0(0.05) 
I0703 01:50:27.659312 31050 net.cpp:1851] ctx_conv3_param_0(0.05) 
I0703 01:50:27.659314 31050 net.cpp:1851] ctx_conv4_param_0(0.05) 
I0703 01:50:27.659317 31050 net.cpp:1851] ctx_final_param_0(0.00195) 
I0703 01:50:27.659322 31050 net.cpp:1851] out3a_param_0(0.05) 
I0703 01:50:27.659329 31050 net.cpp:1851] out5a_param_0(0.0499) 
I0703 01:50:27.659334 31050 net.cpp:1851] res2a_branch2a_param_0(0.05) 
I0703 01:50:27.659340 31050 net.cpp:1851] res2a_branch2b_param_0(0.0499) 
I0703 01:50:27.659346 31050 net.cpp:1851] res3a_branch2a_param_0(0.05) 
I0703 01:50:27.659350 31050 net.cpp:1851] res3a_branch2b_param_0(0.05) 
I0703 01:50:27.659355 31050 net.cpp:1851] res4a_branch2a_param_0(0.05) 
I0703 01:50:27.659359 31050 net.cpp:1851] res4a_branch2b_param_0(0.05) 
I0703 01:50:27.659363 31050 net.cpp:1851] res5a_branch2a_param_0(0.0498) 
I0703 01:50:27.659368 31050 net.cpp:1851] res5a_branch2b_param_0(0.0499) 
I0703 01:50:27.659370 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (133917/2.69117e+06) 0.0498
I0703 01:50:27.889353 31050 solver.cpp:290] Iteration 5000 (4.1297 iter/s, 24.2149s/100 iter), loss = 0.0279282
I0703 01:50:27.889374 31050 solver.cpp:309]     Train net output #0: loss = 0.0279282 (* 1 = 0.0279282 loss)
I0703 01:50:27.889381 31050 sgd_solver.cpp:106] Iteration 5000, lr = 1e-05
I0703 01:50:27.890333 31050 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.1
I0703 01:50:28.134716 31050 net.cpp:1824] All zero weights of convolution layers are frozen
I0703 01:50:52.342419 31050 solver.cpp:290] Iteration 5100 (4.08959 iter/s, 24.4523s/100 iter), loss = 0.0217704
I0703 01:50:52.342445 31050 solver.cpp:309]     Train net output #0: loss = 0.0217704 (* 1 = 0.0217704 loss)
I0703 01:50:52.342453 31050 sgd_solver.cpp:106] Iteration 5100, lr = 1e-05
I0703 01:51:16.504767 31050 solver.cpp:290] Iteration 5200 (4.13879 iter/s, 24.1616s/100 iter), loss = 0.0368335
I0703 01:51:16.504890 31050 solver.cpp:309]     Train net output #0: loss = 0.0368335 (* 1 = 0.0368335 loss)
I0703 01:51:16.504900 31050 sgd_solver.cpp:106] Iteration 5200, lr = 1e-05
I0703 01:51:40.687316 31050 solver.cpp:290] Iteration 5300 (4.13535 iter/s, 24.1817s/100 iter), loss = 0.0176255
I0703 01:51:40.687340 31050 solver.cpp:309]     Train net output #0: loss = 0.0176254 (* 1 = 0.0176254 loss)
I0703 01:51:40.687347 31050 sgd_solver.cpp:106] Iteration 5300, lr = 1e-05
I0703 01:52:04.847888 31050 solver.cpp:290] Iteration 5400 (4.1391 iter/s, 24.1599s/100 iter), loss = 0.0177825
I0703 01:52:04.848000 31050 solver.cpp:309]     Train net output #0: loss = 0.0177825 (* 1 = 0.0177825 loss)
I0703 01:52:04.848009 31050 sgd_solver.cpp:106] Iteration 5400, lr = 1e-05
I0703 01:52:29.013370 31050 solver.cpp:290] Iteration 5500 (4.13827 iter/s, 24.1647s/100 iter), loss = 0.0255562
I0703 01:52:29.013393 31050 solver.cpp:309]     Train net output #0: loss = 0.0255562 (* 1 = 0.0255562 loss)
I0703 01:52:29.013401 31050 sgd_solver.cpp:106] Iteration 5500, lr = 1e-05
I0703 01:52:53.178630 31050 solver.cpp:290] Iteration 5600 (4.13829 iter/s, 24.1645s/100 iter), loss = 0.0267434
I0703 01:52:53.178759 31050 solver.cpp:309]     Train net output #0: loss = 0.0267433 (* 1 = 0.0267433 loss)
I0703 01:52:53.178771 31050 sgd_solver.cpp:106] Iteration 5600, lr = 1e-05
I0703 01:53:17.351961 31050 solver.cpp:290] Iteration 5700 (4.13693 iter/s, 24.1725s/100 iter), loss = 0.0290653
I0703 01:53:17.351986 31050 solver.cpp:309]     Train net output #0: loss = 0.0290652 (* 1 = 0.0290652 loss)
I0703 01:53:17.351994 31050 sgd_solver.cpp:106] Iteration 5700, lr = 1e-05
I0703 01:53:41.529356 31050 solver.cpp:290] Iteration 5800 (4.13622 iter/s, 24.1767s/100 iter), loss = 0.0345044
I0703 01:53:41.529469 31050 solver.cpp:309]     Train net output #0: loss = 0.0345044 (* 1 = 0.0345044 loss)
I0703 01:53:41.529479 31050 sgd_solver.cpp:106] Iteration 5800, lr = 1e-05
I0703 01:54:05.654340 31050 solver.cpp:290] Iteration 5900 (4.14522 iter/s, 24.1242s/100 iter), loss = 0.0220204
I0703 01:54:05.654362 31050 solver.cpp:309]     Train net output #0: loss = 0.0220203 (* 1 = 0.0220203 loss)
I0703 01:54:05.654369 31050 sgd_solver.cpp:106] Iteration 5900, lr = 1e-05
I0703 01:54:29.564985 31050 solver.cpp:354] Sparsity after update:
I0703 01:54:29.566843 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 01:54:29.566853 31050 net.cpp:1851] conv1a_param_0(0.05) 
I0703 01:54:29.566862 31050 net.cpp:1851] conv1b_param_0(0.0998) 
I0703 01:54:29.566867 31050 net.cpp:1851] ctx_conv1_param_0(0.1) 
I0703 01:54:29.566871 31050 net.cpp:1851] ctx_conv2_param_0(0.1) 
I0703 01:54:29.566875 31050 net.cpp:1851] ctx_conv3_param_0(0.1) 
I0703 01:54:29.566879 31050 net.cpp:1851] ctx_conv4_param_0(0.1) 
I0703 01:54:29.566884 31050 net.cpp:1851] ctx_final_param_0(0.00629) 
I0703 01:54:29.566887 31050 net.cpp:1851] out3a_param_0(0.1) 
I0703 01:54:29.566891 31050 net.cpp:1851] out5a_param_0(0.1) 
I0703 01:54:29.566895 31050 net.cpp:1851] res2a_branch2a_param_0(0.1) 
I0703 01:54:29.566900 31050 net.cpp:1851] res2a_branch2b_param_0(0.0999) 
I0703 01:54:29.566902 31050 net.cpp:1851] res3a_branch2a_param_0(0.1) 
I0703 01:54:29.566906 31050 net.cpp:1851] res3a_branch2b_param_0(0.1) 
I0703 01:54:29.566911 31050 net.cpp:1851] res4a_branch2a_param_0(0.1) 
I0703 01:54:29.566915 31050 net.cpp:1851] res4a_branch2b_param_0(0.1) 
I0703 01:54:29.566918 31050 net.cpp:1851] res5a_branch2a_param_0(0.0995) 
I0703 01:54:29.566921 31050 net.cpp:1851] res5a_branch2b_param_0(0.0993) 
I0703 01:54:29.566926 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (267512/2.69117e+06) 0.0994
I0703 01:54:29.567070 31050 solver.cpp:473] Iteration 6000, Testing net (#0)
I0703 01:55:17.595726 31050 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.954825
I0703 01:55:17.595831 31050 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.999647
I0703 01:55:17.595839 31050 solver.cpp:546]     Test net output #2: loss = 0.151565 (* 1 = 0.151565 loss)
I0703 01:55:17.857275 31050 solver.cpp:290] Iteration 6000 (1.38502 iter/s, 72.2009s/100 iter), loss = 0.030773
I0703 01:55:17.857298 31050 solver.cpp:309]     Train net output #0: loss = 0.030773 (* 1 = 0.030773 loss)
I0703 01:55:17.857306 31050 sgd_solver.cpp:106] Iteration 6000, lr = 1e-05
I0703 01:55:17.858268 31050 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.15
I0703 01:55:18.160099 31050 net.cpp:1824] All zero weights of convolution layers are frozen
I0703 01:55:41.734575 31050 solver.cpp:290] Iteration 6100 (4.1882 iter/s, 23.8766s/100 iter), loss = 0.0239002
I0703 01:55:41.734599 31050 solver.cpp:309]     Train net output #0: loss = 0.0239002 (* 1 = 0.0239002 loss)
I0703 01:55:41.734606 31050 sgd_solver.cpp:106] Iteration 6100, lr = 1e-05
I0703 01:56:05.903573 31050 solver.cpp:290] Iteration 6200 (4.13765 iter/s, 24.1683s/100 iter), loss = 0.0312084
I0703 01:56:05.903682 31050 solver.cpp:309]     Train net output #0: loss = 0.0312084 (* 1 = 0.0312084 loss)
I0703 01:56:05.903693 31050 sgd_solver.cpp:106] Iteration 6200, lr = 1e-05
I0703 01:56:30.131775 31050 solver.cpp:290] Iteration 6300 (4.12756 iter/s, 24.2274s/100 iter), loss = 0.0308336
I0703 01:56:30.131799 31050 solver.cpp:309]     Train net output #0: loss = 0.0308336 (* 1 = 0.0308336 loss)
I0703 01:56:30.131806 31050 sgd_solver.cpp:106] Iteration 6300, lr = 1e-05
I0703 01:56:54.291769 31050 solver.cpp:290] Iteration 6400 (4.1392 iter/s, 24.1593s/100 iter), loss = 0.0155741
I0703 01:56:54.291811 31050 solver.cpp:309]     Train net output #0: loss = 0.0155741 (* 1 = 0.0155741 loss)
I0703 01:56:54.291820 31050 sgd_solver.cpp:106] Iteration 6400, lr = 1e-05
I0703 01:57:18.442163 31050 solver.cpp:290] Iteration 6500 (4.14084 iter/s, 24.1497s/100 iter), loss = 0.0216166
I0703 01:57:18.442188 31050 solver.cpp:309]     Train net output #0: loss = 0.0216165 (* 1 = 0.0216165 loss)
I0703 01:57:18.442194 31050 sgd_solver.cpp:106] Iteration 6500, lr = 1e-05
I0703 01:57:42.615197 31050 solver.cpp:290] Iteration 6600 (4.13696 iter/s, 24.1723s/100 iter), loss = 0.0208513
I0703 01:57:42.615305 31050 solver.cpp:309]     Train net output #0: loss = 0.0208513 (* 1 = 0.0208513 loss)
I0703 01:57:42.615314 31050 sgd_solver.cpp:106] Iteration 6600, lr = 1e-05
I0703 01:58:06.784607 31050 solver.cpp:290] Iteration 6700 (4.1376 iter/s, 24.1686s/100 iter), loss = 0.0315002
I0703 01:58:06.784629 31050 solver.cpp:309]     Train net output #0: loss = 0.0315002 (* 1 = 0.0315002 loss)
I0703 01:58:06.784636 31050 sgd_solver.cpp:106] Iteration 6700, lr = 1e-05
I0703 01:58:30.923121 31050 solver.cpp:290] Iteration 6800 (4.14288 iter/s, 24.1378s/100 iter), loss = 0.0171216
I0703 01:58:30.923162 31050 solver.cpp:309]     Train net output #0: loss = 0.0171216 (* 1 = 0.0171216 loss)
I0703 01:58:30.923171 31050 sgd_solver.cpp:106] Iteration 6800, lr = 1e-05
I0703 01:58:55.071374 31050 solver.cpp:290] Iteration 6900 (4.14121 iter/s, 24.1475s/100 iter), loss = 0.0244313
I0703 01:58:55.071398 31050 solver.cpp:309]     Train net output #0: loss = 0.0244313 (* 1 = 0.0244313 loss)
I0703 01:58:55.071404 31050 sgd_solver.cpp:106] Iteration 6900, lr = 1e-05
I0703 01:59:18.978595 31050 solver.cpp:354] Sparsity after update:
I0703 01:59:19.022222 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 01:59:19.022238 31050 net.cpp:1851] conv1a_param_0(0.075) 
I0703 01:59:19.022244 31050 net.cpp:1851] conv1b_param_0(0.15) 
I0703 01:59:19.022248 31050 net.cpp:1851] ctx_conv1_param_0(0.15) 
I0703 01:59:19.022249 31050 net.cpp:1851] ctx_conv2_param_0(0.15) 
I0703 01:59:19.022250 31050 net.cpp:1851] ctx_conv3_param_0(0.15) 
I0703 01:59:19.022253 31050 net.cpp:1851] ctx_conv4_param_0(0.15) 
I0703 01:59:19.022254 31050 net.cpp:1851] ctx_final_param_0(0.00152) 
I0703 01:59:19.022256 31050 net.cpp:1851] out3a_param_0(0.15) 
I0703 01:59:19.022258 31050 net.cpp:1851] out5a_param_0(0.15) 
I0703 01:59:19.022260 31050 net.cpp:1851] res2a_branch2a_param_0(0.15) 
I0703 01:59:19.022263 31050 net.cpp:1851] res2a_branch2b_param_0(0.15) 
I0703 01:59:19.022264 31050 net.cpp:1851] res3a_branch2a_param_0(0.15) 
I0703 01:59:19.022266 31050 net.cpp:1851] res3a_branch2b_param_0(0.15) 
I0703 01:59:19.022269 31050 net.cpp:1851] res4a_branch2a_param_0(0.15) 
I0703 01:59:19.022270 31050 net.cpp:1851] res4a_branch2b_param_0(0.15) 
I0703 01:59:19.022274 31050 net.cpp:1851] res5a_branch2a_param_0(0.149) 
I0703 01:59:19.022276 31050 net.cpp:1851] res5a_branch2b_param_0(0.15) 
I0703 01:59:19.022279 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (401925/2.69117e+06) 0.149
I0703 01:59:19.253029 31050 solver.cpp:290] Iteration 7000 (4.13549 iter/s, 24.181s/100 iter), loss = 0.0218743
I0703 01:59:19.253054 31050 solver.cpp:309]     Train net output #0: loss = 0.0218743 (* 1 = 0.0218743 loss)
I0703 01:59:19.253062 31050 sgd_solver.cpp:106] Iteration 7000, lr = 1e-05
I0703 01:59:19.254082 31050 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.2
I0703 01:59:19.572204 31050 net.cpp:1824] All zero weights of convolution layers are frozen
I0703 01:59:43.718422 31050 solver.cpp:290] Iteration 7100 (4.08752 iter/s, 24.4647s/100 iter), loss = 0.0270131
I0703 01:59:43.718448 31050 solver.cpp:309]     Train net output #0: loss = 0.0270131 (* 1 = 0.0270131 loss)
I0703 01:59:43.718456 31050 sgd_solver.cpp:106] Iteration 7100, lr = 1e-05
I0703 02:00:07.864836 31050 solver.cpp:290] Iteration 7200 (4.14152 iter/s, 24.1457s/100 iter), loss = 0.0240564
I0703 02:00:07.864946 31050 solver.cpp:309]     Train net output #0: loss = 0.0240564 (* 1 = 0.0240564 loss)
I0703 02:00:07.864958 31050 sgd_solver.cpp:106] Iteration 7200, lr = 1e-05
I0703 02:00:32.047348 31050 solver.cpp:290] Iteration 7300 (4.13535 iter/s, 24.1817s/100 iter), loss = 0.0215663
I0703 02:00:32.047369 31050 solver.cpp:309]     Train net output #0: loss = 0.0215662 (* 1 = 0.0215662 loss)
I0703 02:00:32.047376 31050 sgd_solver.cpp:106] Iteration 7300, lr = 1e-05
I0703 02:00:56.247280 31050 solver.cpp:290] Iteration 7400 (4.13236 iter/s, 24.1992s/100 iter), loss = 0.0214787
I0703 02:00:56.247385 31050 solver.cpp:309]     Train net output #0: loss = 0.0214787 (* 1 = 0.0214787 loss)
I0703 02:00:56.247396 31050 sgd_solver.cpp:106] Iteration 7400, lr = 1e-05
I0703 02:01:20.384719 31050 solver.cpp:290] Iteration 7500 (4.14307 iter/s, 24.1367s/100 iter), loss = 0.0434618
I0703 02:01:20.384744 31050 solver.cpp:309]     Train net output #0: loss = 0.0434618 (* 1 = 0.0434618 loss)
I0703 02:01:20.384754 31050 sgd_solver.cpp:106] Iteration 7500, lr = 1e-05
I0703 02:01:44.550561 31050 solver.cpp:290] Iteration 7600 (4.13819 iter/s, 24.1651s/100 iter), loss = 0.0159459
I0703 02:01:44.550603 31050 solver.cpp:309]     Train net output #0: loss = 0.0159458 (* 1 = 0.0159458 loss)
I0703 02:01:44.550611 31050 sgd_solver.cpp:106] Iteration 7600, lr = 1e-05
I0703 02:02:08.678195 31050 solver.cpp:290] Iteration 7700 (4.14475 iter/s, 24.1269s/100 iter), loss = 0.028682
I0703 02:02:08.678217 31050 solver.cpp:309]     Train net output #0: loss = 0.0286819 (* 1 = 0.0286819 loss)
I0703 02:02:08.678225 31050 sgd_solver.cpp:106] Iteration 7700, lr = 1e-05
I0703 02:02:32.839287 31050 solver.cpp:290] Iteration 7800 (4.139 iter/s, 24.1604s/100 iter), loss = 0.029975
I0703 02:02:32.839344 31050 solver.cpp:309]     Train net output #0: loss = 0.029975 (* 1 = 0.029975 loss)
I0703 02:02:32.839354 31050 sgd_solver.cpp:106] Iteration 7800, lr = 1e-05
I0703 02:02:57.051514 31050 solver.cpp:290] Iteration 7900 (4.13027 iter/s, 24.2115s/100 iter), loss = 0.0293056
I0703 02:02:57.051538 31050 solver.cpp:309]     Train net output #0: loss = 0.0293055 (* 1 = 0.0293055 loss)
I0703 02:02:57.051545 31050 sgd_solver.cpp:106] Iteration 7900, lr = 1e-05
I0703 02:03:20.959692 31050 solver.cpp:354] Sparsity after update:
I0703 02:03:20.961550 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 02:03:20.961558 31050 net.cpp:1851] conv1a_param_0(0.1) 
I0703 02:03:20.961566 31050 net.cpp:1851] conv1b_param_0(0.2) 
I0703 02:03:20.961570 31050 net.cpp:1851] ctx_conv1_param_0(0.2) 
I0703 02:03:20.961571 31050 net.cpp:1851] ctx_conv2_param_0(0.2) 
I0703 02:03:20.961575 31050 net.cpp:1851] ctx_conv3_param_0(0.2) 
I0703 02:03:20.961576 31050 net.cpp:1851] ctx_conv4_param_0(0.2) 
I0703 02:03:20.961580 31050 net.cpp:1851] ctx_final_param_0(0.000217) 
I0703 02:03:20.961581 31050 net.cpp:1851] out3a_param_0(0.2) 
I0703 02:03:20.961583 31050 net.cpp:1851] out5a_param_0(0.2) 
I0703 02:03:20.961586 31050 net.cpp:1851] res2a_branch2a_param_0(0.2) 
I0703 02:03:20.961588 31050 net.cpp:1851] res2a_branch2b_param_0(0.2) 
I0703 02:03:20.961591 31050 net.cpp:1851] res3a_branch2a_param_0(0.2) 
I0703 02:03:20.961594 31050 net.cpp:1851] res3a_branch2b_param_0(0.2) 
I0703 02:03:20.961598 31050 net.cpp:1851] res4a_branch2a_param_0(0.2) 
I0703 02:03:20.961601 31050 net.cpp:1851] res4a_branch2b_param_0(0.2) 
I0703 02:03:20.961604 31050 net.cpp:1851] res5a_branch2a_param_0(0.2) 
I0703 02:03:20.961608 31050 net.cpp:1851] res5a_branch2b_param_0(0.2) 
I0703 02:03:20.961611 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (536886/2.69117e+06) 0.199
I0703 02:03:20.961746 31050 solver.cpp:473] Iteration 8000, Testing net (#0)
I0703 02:04:08.928622 31050 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.954519
I0703 02:04:08.928717 31050 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.999689
I0703 02:04:08.928725 31050 solver.cpp:546]     Test net output #2: loss = 0.152622 (* 1 = 0.152622 loss)
I0703 02:04:09.179185 31050 solver.cpp:290] Iteration 8000 (1.38647 iter/s, 72.1257s/100 iter), loss = 0.0221487
I0703 02:04:09.179209 31050 solver.cpp:309]     Train net output #0: loss = 0.0221487 (* 1 = 0.0221487 loss)
I0703 02:04:09.179216 31050 sgd_solver.cpp:106] Iteration 8000, lr = 1e-05
I0703 02:04:09.180209 31050 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.25
I0703 02:04:09.558549 31050 net.cpp:1824] All zero weights of convolution layers are frozen
I0703 02:04:33.058699 31050 solver.cpp:290] Iteration 8100 (4.18781 iter/s, 23.8788s/100 iter), loss = 0.0280045
I0703 02:04:33.058722 31050 solver.cpp:309]     Train net output #0: loss = 0.0280045 (* 1 = 0.0280045 loss)
I0703 02:04:33.058729 31050 sgd_solver.cpp:106] Iteration 8100, lr = 1e-05
I0703 02:04:57.196285 31050 solver.cpp:290] Iteration 8200 (4.14304 iter/s, 24.1369s/100 iter), loss = 0.0237745
I0703 02:04:57.196393 31050 solver.cpp:309]     Train net output #0: loss = 0.0237745 (* 1 = 0.0237745 loss)
I0703 02:04:57.196405 31050 sgd_solver.cpp:106] Iteration 8200, lr = 1e-05
I0703 02:05:21.375666 31050 solver.cpp:290] Iteration 8300 (4.13589 iter/s, 24.1786s/100 iter), loss = 0.0198021
I0703 02:05:21.375690 31050 solver.cpp:309]     Train net output #0: loss = 0.0198021 (* 1 = 0.0198021 loss)
I0703 02:05:21.375699 31050 sgd_solver.cpp:106] Iteration 8300, lr = 1e-05
I0703 02:05:45.541051 31050 solver.cpp:290] Iteration 8400 (4.13827 iter/s, 24.1647s/100 iter), loss = 0.0246437
I0703 02:05:45.541133 31050 solver.cpp:309]     Train net output #0: loss = 0.0246437 (* 1 = 0.0246437 loss)
I0703 02:05:45.541146 31050 sgd_solver.cpp:106] Iteration 8400, lr = 1e-05
I0703 02:06:09.709440 31050 solver.cpp:290] Iteration 8500 (4.13776 iter/s, 24.1676s/100 iter), loss = 0.0164813
I0703 02:06:09.709465 31050 solver.cpp:309]     Train net output #0: loss = 0.0164813 (* 1 = 0.0164813 loss)
I0703 02:06:09.709472 31050 sgd_solver.cpp:106] Iteration 8500, lr = 1e-05
I0703 02:06:33.883728 31050 solver.cpp:290] Iteration 8600 (4.13674 iter/s, 24.1736s/100 iter), loss = 0.039042
I0703 02:06:33.883848 31050 solver.cpp:309]     Train net output #0: loss = 0.039042 (* 1 = 0.039042 loss)
I0703 02:06:33.883859 31050 sgd_solver.cpp:106] Iteration 8600, lr = 1e-05
I0703 02:06:58.040222 31050 solver.cpp:290] Iteration 8700 (4.13981 iter/s, 24.1557s/100 iter), loss = 0.0260481
I0703 02:06:58.040243 31050 solver.cpp:309]     Train net output #0: loss = 0.0260481 (* 1 = 0.0260481 loss)
I0703 02:06:58.040251 31050 sgd_solver.cpp:106] Iteration 8700, lr = 1e-05
I0703 02:07:22.200534 31050 solver.cpp:290] Iteration 8800 (4.13914 iter/s, 24.1596s/100 iter), loss = 0.0231128
I0703 02:07:22.200644 31050 solver.cpp:309]     Train net output #0: loss = 0.0231128 (* 1 = 0.0231128 loss)
I0703 02:07:22.200659 31050 sgd_solver.cpp:106] Iteration 8800, lr = 1e-05
I0703 02:07:46.355247 31050 solver.cpp:290] Iteration 8900 (4.14011 iter/s, 24.1539s/100 iter), loss = 0.0351508
I0703 02:07:46.355269 31050 solver.cpp:309]     Train net output #0: loss = 0.0351508 (* 1 = 0.0351508 loss)
I0703 02:07:46.355276 31050 sgd_solver.cpp:106] Iteration 8900, lr = 1e-05
I0703 02:08:10.248133 31050 solver.cpp:354] Sparsity after update:
I0703 02:08:10.293740 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 02:08:10.293756 31050 net.cpp:1851] conv1a_param_0(0.125) 
I0703 02:08:10.293767 31050 net.cpp:1851] conv1b_param_0(0.25) 
I0703 02:08:10.293771 31050 net.cpp:1851] ctx_conv1_param_0(0.25) 
I0703 02:08:10.293773 31050 net.cpp:1851] ctx_conv2_param_0(0.25) 
I0703 02:08:10.293778 31050 net.cpp:1851] ctx_conv3_param_0(0.25) 
I0703 02:08:10.293783 31050 net.cpp:1851] ctx_conv4_param_0(0.25) 
I0703 02:08:10.293787 31050 net.cpp:1851] ctx_final_param_0(0.0013) 
I0703 02:08:10.293792 31050 net.cpp:1851] out3a_param_0(0.25) 
I0703 02:08:10.293797 31050 net.cpp:1851] out5a_param_0(0.25) 
I0703 02:08:10.293800 31050 net.cpp:1851] res2a_branch2a_param_0(0.25) 
I0703 02:08:10.293803 31050 net.cpp:1851] res2a_branch2b_param_0(0.25) 
I0703 02:08:10.293808 31050 net.cpp:1851] res3a_branch2a_param_0(0.25) 
I0703 02:08:10.293812 31050 net.cpp:1851] res3a_branch2b_param_0(0.25) 
I0703 02:08:10.293817 31050 net.cpp:1851] res4a_branch2a_param_0(0.25) 
I0703 02:08:10.293820 31050 net.cpp:1851] res4a_branch2b_param_0(0.25) 
I0703 02:08:10.293824 31050 net.cpp:1851] res5a_branch2a_param_0(0.25) 
I0703 02:08:10.293829 31050 net.cpp:1851] res5a_branch2b_param_0(0.249) 
I0703 02:08:10.293833 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (670847/2.69117e+06) 0.249
I0703 02:08:10.525452 31050 solver.cpp:290] Iteration 9000 (4.13744 iter/s, 24.1695s/100 iter), loss = 0.0226108
I0703 02:08:10.525478 31050 solver.cpp:309]     Train net output #0: loss = 0.0226108 (* 1 = 0.0226108 loss)
I0703 02:08:10.525487 31050 sgd_solver.cpp:106] Iteration 9000, lr = 1e-05
I0703 02:08:10.526499 31050 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.3
I0703 02:08:10.949002 31050 net.cpp:1824] All zero weights of convolution layers are frozen
I0703 02:08:35.075460 31050 solver.cpp:290] Iteration 9100 (4.07343 iter/s, 24.5493s/100 iter), loss = 0.0216661
I0703 02:08:35.075482 31050 solver.cpp:309]     Train net output #0: loss = 0.0216661 (* 1 = 0.0216661 loss)
I0703 02:08:35.075489 31050 sgd_solver.cpp:106] Iteration 9100, lr = 1e-05
I0703 02:08:59.313623 31050 solver.cpp:290] Iteration 9200 (4.12584 iter/s, 24.2375s/100 iter), loss = 0.0187891
I0703 02:08:59.313732 31050 solver.cpp:309]     Train net output #0: loss = 0.018789 (* 1 = 0.018789 loss)
I0703 02:08:59.313745 31050 sgd_solver.cpp:106] Iteration 9200, lr = 1e-05
I0703 02:09:23.485093 31050 solver.cpp:290] Iteration 9300 (4.13724 iter/s, 24.1707s/100 iter), loss = 0.0219785
I0703 02:09:23.485118 31050 solver.cpp:309]     Train net output #0: loss = 0.0219784 (* 1 = 0.0219784 loss)
I0703 02:09:23.485126 31050 sgd_solver.cpp:106] Iteration 9300, lr = 1e-05
I0703 02:09:47.653913 31050 solver.cpp:290] Iteration 9400 (4.13768 iter/s, 24.1681s/100 iter), loss = 0.0302146
I0703 02:09:47.653988 31050 solver.cpp:309]     Train net output #0: loss = 0.0302146 (* 1 = 0.0302146 loss)
I0703 02:09:47.653997 31050 sgd_solver.cpp:106] Iteration 9400, lr = 1e-05
I0703 02:10:11.813563 31050 solver.cpp:290] Iteration 9500 (4.13926 iter/s, 24.1589s/100 iter), loss = 0.0197126
I0703 02:10:11.813587 31050 solver.cpp:309]     Train net output #0: loss = 0.0197125 (* 1 = 0.0197125 loss)
I0703 02:10:11.813594 31050 sgd_solver.cpp:106] Iteration 9500, lr = 1e-05
I0703 02:10:35.964177 31050 solver.cpp:290] Iteration 9600 (4.1408 iter/s, 24.1499s/100 iter), loss = 0.02523
I0703 02:10:35.964272 31050 solver.cpp:309]     Train net output #0: loss = 0.02523 (* 1 = 0.02523 loss)
I0703 02:10:35.964283 31050 sgd_solver.cpp:106] Iteration 9600, lr = 1e-05
I0703 02:11:00.139766 31050 solver.cpp:290] Iteration 9700 (4.13653 iter/s, 24.1748s/100 iter), loss = 0.0196739
I0703 02:11:00.139791 31050 solver.cpp:309]     Train net output #0: loss = 0.0196739 (* 1 = 0.0196739 loss)
I0703 02:11:00.139797 31050 sgd_solver.cpp:106] Iteration 9700, lr = 1e-05
I0703 02:11:24.363668 31050 solver.cpp:290] Iteration 9800 (4.12827 iter/s, 24.2232s/100 iter), loss = 0.0207201
I0703 02:11:24.363747 31050 solver.cpp:309]     Train net output #0: loss = 0.02072 (* 1 = 0.02072 loss)
I0703 02:11:24.363756 31050 sgd_solver.cpp:106] Iteration 9800, lr = 1e-05
I0703 02:11:48.509763 31050 solver.cpp:290] Iteration 9900 (4.14158 iter/s, 24.1454s/100 iter), loss = 0.037172
I0703 02:11:48.509788 31050 solver.cpp:309]     Train net output #0: loss = 0.0371719 (* 1 = 0.0371719 loss)
I0703 02:11:48.509794 31050 sgd_solver.cpp:106] Iteration 9900, lr = 1e-05
I0703 02:12:12.436151 31050 solver.cpp:600] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-02_23-02-42/sparse/cityscapes5_jsegnet21v2_iter_10000.caffemodel
I0703 02:12:12.505095 31050 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-02_23-02-42/sparse/cityscapes5_jsegnet21v2_iter_10000.solverstate
I0703 02:12:12.521217 31050 solver.cpp:354] Sparsity after update:
I0703 02:12:12.522421 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 02:12:12.522429 31050 net.cpp:1851] conv1a_param_0(0.15) 
I0703 02:12:12.522438 31050 net.cpp:1851] conv1b_param_0(0.3) 
I0703 02:12:12.522439 31050 net.cpp:1851] ctx_conv1_param_0(0.3) 
I0703 02:12:12.522441 31050 net.cpp:1851] ctx_conv2_param_0(0.3) 
I0703 02:12:12.522444 31050 net.cpp:1851] ctx_conv3_param_0(0.3) 
I0703 02:12:12.522445 31050 net.cpp:1851] ctx_conv4_param_0(0.3) 
I0703 02:12:12.522447 31050 net.cpp:1851] ctx_final_param_0(0.0076) 
I0703 02:12:12.522449 31050 net.cpp:1851] out3a_param_0(0.3) 
I0703 02:12:12.522452 31050 net.cpp:1851] out5a_param_0(0.3) 
I0703 02:12:12.522454 31050 net.cpp:1851] res2a_branch2a_param_0(0.3) 
I0703 02:12:12.522455 31050 net.cpp:1851] res2a_branch2b_param_0(0.3) 
I0703 02:12:12.522457 31050 net.cpp:1851] res3a_branch2a_param_0(0.3) 
I0703 02:12:12.522459 31050 net.cpp:1851] res3a_branch2b_param_0(0.3) 
I0703 02:12:12.522461 31050 net.cpp:1851] res4a_branch2a_param_0(0.3) 
I0703 02:12:12.522464 31050 net.cpp:1851] res4a_branch2b_param_0(0.3) 
I0703 02:12:12.522465 31050 net.cpp:1851] res5a_branch2a_param_0(0.3) 
I0703 02:12:12.522467 31050 net.cpp:1851] res5a_branch2b_param_0(0.3) 
I0703 02:12:12.522469 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (805328/2.69117e+06) 0.299
I0703 02:12:12.522617 31050 solver.cpp:473] Iteration 10000, Testing net (#0)
I0703 02:13:00.543434 31050 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.954991
I0703 02:13:00.543525 31050 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.999878
I0703 02:13:00.543534 31050 solver.cpp:546]     Test net output #2: loss = 0.139855 (* 1 = 0.139855 loss)
I0703 02:13:00.789273 31050 solver.cpp:290] Iteration 10000 (1.38356 iter/s, 72.2775s/100 iter), loss = 0.0175528
I0703 02:13:00.789296 31050 solver.cpp:309]     Train net output #0: loss = 0.0175528 (* 1 = 0.0175528 loss)
I0703 02:13:00.789302 31050 sgd_solver.cpp:106] Iteration 10000, lr = 1e-05
I0703 02:13:00.790309 31050 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.35
I0703 02:13:01.270591 31050 net.cpp:1824] All zero weights of convolution layers are frozen
I0703 02:13:24.788717 31050 solver.cpp:290] Iteration 10100 (4.16688 iter/s, 23.9988s/100 iter), loss = 0.0189557
I0703 02:13:24.788739 31050 solver.cpp:309]     Train net output #0: loss = 0.0189557 (* 1 = 0.0189557 loss)
I0703 02:13:24.788746 31050 sgd_solver.cpp:106] Iteration 10100, lr = 1e-05
I0703 02:13:48.932126 31050 solver.cpp:290] Iteration 10200 (4.14204 iter/s, 24.1427s/100 iter), loss = 0.0189588
I0703 02:13:48.932199 31050 solver.cpp:309]     Train net output #0: loss = 0.0189587 (* 1 = 0.0189587 loss)
I0703 02:13:48.932209 31050 sgd_solver.cpp:106] Iteration 10200, lr = 1e-05
I0703 02:14:13.128303 31050 solver.cpp:290] Iteration 10300 (4.13302 iter/s, 24.1954s/100 iter), loss = 0.026732
I0703 02:14:13.128325 31050 solver.cpp:309]     Train net output #0: loss = 0.0267319 (* 1 = 0.0267319 loss)
I0703 02:14:13.128334 31050 sgd_solver.cpp:106] Iteration 10300, lr = 1e-05
I0703 02:14:37.309788 31050 solver.cpp:290] Iteration 10400 (4.13552 iter/s, 24.1808s/100 iter), loss = 0.0248204
I0703 02:14:37.309839 31050 solver.cpp:309]     Train net output #0: loss = 0.0248203 (* 1 = 0.0248203 loss)
I0703 02:14:37.309850 31050 sgd_solver.cpp:106] Iteration 10400, lr = 1e-05
I0703 02:15:01.457960 31050 solver.cpp:290] Iteration 10500 (4.14123 iter/s, 24.1474s/100 iter), loss = 0.0284437
I0703 02:15:01.457988 31050 solver.cpp:309]     Train net output #0: loss = 0.0284437 (* 1 = 0.0284437 loss)
I0703 02:15:01.457994 31050 sgd_solver.cpp:106] Iteration 10500, lr = 1e-05
I0703 02:15:25.811691 31050 solver.cpp:290] Iteration 10600 (4.10627 iter/s, 24.353s/100 iter), loss = 0.0247938
I0703 02:15:25.811805 31050 solver.cpp:309]     Train net output #0: loss = 0.0247938 (* 1 = 0.0247938 loss)
I0703 02:15:25.811815 31050 sgd_solver.cpp:106] Iteration 10600, lr = 1e-05
I0703 02:15:50.002423 31050 solver.cpp:290] Iteration 10700 (4.13395 iter/s, 24.1899s/100 iter), loss = 0.0184781
I0703 02:15:50.002447 31050 solver.cpp:309]     Train net output #0: loss = 0.0184781 (* 1 = 0.0184781 loss)
I0703 02:15:50.002454 31050 sgd_solver.cpp:106] Iteration 10700, lr = 1e-05
I0703 02:16:14.138563 31050 solver.cpp:290] Iteration 10800 (4.14329 iter/s, 24.1354s/100 iter), loss = 0.0174261
I0703 02:16:14.138697 31050 solver.cpp:309]     Train net output #0: loss = 0.0174261 (* 1 = 0.0174261 loss)
I0703 02:16:14.138708 31050 sgd_solver.cpp:106] Iteration 10800, lr = 1e-05
I0703 02:16:38.310183 31050 solver.cpp:290] Iteration 10900 (4.13722 iter/s, 24.1708s/100 iter), loss = 0.0166005
I0703 02:16:38.310206 31050 solver.cpp:309]     Train net output #0: loss = 0.0166005 (* 1 = 0.0166005 loss)
I0703 02:16:38.310214 31050 sgd_solver.cpp:106] Iteration 10900, lr = 1e-05
I0703 02:17:02.257072 31050 solver.cpp:354] Sparsity after update:
I0703 02:17:02.297730 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 02:17:02.297747 31050 net.cpp:1851] conv1a_param_0(0.175) 
I0703 02:17:02.297755 31050 net.cpp:1851] conv1b_param_0(0.35) 
I0703 02:17:02.297757 31050 net.cpp:1851] ctx_conv1_param_0(0.35) 
I0703 02:17:02.297760 31050 net.cpp:1851] ctx_conv2_param_0(0.35) 
I0703 02:17:02.297761 31050 net.cpp:1851] ctx_conv3_param_0(0.35) 
I0703 02:17:02.297763 31050 net.cpp:1851] ctx_conv4_param_0(0.35) 
I0703 02:17:02.297765 31050 net.cpp:1851] ctx_final_param_0(0.00195) 
I0703 02:17:02.297767 31050 net.cpp:1851] out3a_param_0(0.35) 
I0703 02:17:02.297770 31050 net.cpp:1851] out5a_param_0(0.35) 
I0703 02:17:02.297771 31050 net.cpp:1851] res2a_branch2a_param_0(0.35) 
I0703 02:17:02.297773 31050 net.cpp:1851] res2a_branch2b_param_0(0.35) 
I0703 02:17:02.297775 31050 net.cpp:1851] res3a_branch2a_param_0(0.35) 
I0703 02:17:02.297777 31050 net.cpp:1851] res3a_branch2b_param_0(0.35) 
I0703 02:17:02.297780 31050 net.cpp:1851] res4a_branch2a_param_0(0.35) 
I0703 02:17:02.297781 31050 net.cpp:1851] res4a_branch2b_param_0(0.35) 
I0703 02:17:02.297782 31050 net.cpp:1851] res5a_branch2a_param_0(0.35) 
I0703 02:17:02.297785 31050 net.cpp:1851] res5a_branch2b_param_0(0.35) 
I0703 02:17:02.297786 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (939608/2.69117e+06) 0.349
I0703 02:17:02.529042 31050 solver.cpp:290] Iteration 11000 (4.12913 iter/s, 24.2182s/100 iter), loss = 0.0339202
I0703 02:17:02.529067 31050 solver.cpp:309]     Train net output #0: loss = 0.0339202 (* 1 = 0.0339202 loss)
I0703 02:17:02.529074 31050 sgd_solver.cpp:106] Iteration 11000, lr = 1e-05
I0703 02:17:02.530058 31050 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.4
I0703 02:17:03.073974 31050 net.cpp:1824] All zero weights of convolution layers are frozen
I0703 02:17:27.208683 31050 solver.cpp:290] Iteration 11100 (4.05204 iter/s, 24.6789s/100 iter), loss = 0.0179118
I0703 02:17:27.208709 31050 solver.cpp:309]     Train net output #0: loss = 0.0179118 (* 1 = 0.0179118 loss)
I0703 02:17:27.208719 31050 sgd_solver.cpp:106] Iteration 11100, lr = 1e-05
I0703 02:17:51.393795 31050 solver.cpp:290] Iteration 11200 (4.13489 iter/s, 24.1844s/100 iter), loss = 0.0203856
I0703 02:17:51.393924 31050 solver.cpp:309]     Train net output #0: loss = 0.0203856 (* 1 = 0.0203856 loss)
I0703 02:17:51.393934 31050 sgd_solver.cpp:106] Iteration 11200, lr = 1e-05
I0703 02:18:15.527884 31050 solver.cpp:290] Iteration 11300 (4.14365 iter/s, 24.1333s/100 iter), loss = 0.0294114
I0703 02:18:15.527918 31050 solver.cpp:309]     Train net output #0: loss = 0.0294114 (* 1 = 0.0294114 loss)
I0703 02:18:15.527930 31050 sgd_solver.cpp:106] Iteration 11300, lr = 1e-05
I0703 02:18:39.670099 31050 solver.cpp:290] Iteration 11400 (4.14224 iter/s, 24.1415s/100 iter), loss = 0.0262624
I0703 02:18:39.670202 31050 solver.cpp:309]     Train net output #0: loss = 0.0262624 (* 1 = 0.0262624 loss)
I0703 02:18:39.670212 31050 sgd_solver.cpp:106] Iteration 11400, lr = 1e-05
I0703 02:19:04.067530 31050 solver.cpp:290] Iteration 11500 (4.09892 iter/s, 24.3967s/100 iter), loss = 0.0246447
I0703 02:19:04.067553 31050 solver.cpp:309]     Train net output #0: loss = 0.0246447 (* 1 = 0.0246447 loss)
I0703 02:19:04.067562 31050 sgd_solver.cpp:106] Iteration 11500, lr = 1e-05
I0703 02:19:28.179029 31050 solver.cpp:290] Iteration 11600 (4.14752 iter/s, 24.1108s/100 iter), loss = 0.0248632
I0703 02:19:28.179132 31050 solver.cpp:309]     Train net output #0: loss = 0.0248632 (* 1 = 0.0248632 loss)
I0703 02:19:28.179144 31050 sgd_solver.cpp:106] Iteration 11600, lr = 1e-05
I0703 02:19:52.361197 31050 solver.cpp:290] Iteration 11700 (4.13541 iter/s, 24.1814s/100 iter), loss = 0.0228802
I0703 02:19:52.361223 31050 solver.cpp:309]     Train net output #0: loss = 0.0228801 (* 1 = 0.0228801 loss)
I0703 02:19:52.361229 31050 sgd_solver.cpp:106] Iteration 11700, lr = 1e-05
I0703 02:20:16.528949 31050 solver.cpp:290] Iteration 11800 (4.13786 iter/s, 24.1671s/100 iter), loss = 0.025833
I0703 02:20:16.529058 31050 solver.cpp:309]     Train net output #0: loss = 0.0258329 (* 1 = 0.0258329 loss)
I0703 02:20:16.529068 31050 sgd_solver.cpp:106] Iteration 11800, lr = 1e-05
I0703 02:20:40.706142 31050 solver.cpp:290] Iteration 11900 (4.13626 iter/s, 24.1764s/100 iter), loss = 0.0259833
I0703 02:20:40.706166 31050 solver.cpp:309]     Train net output #0: loss = 0.0259833 (* 1 = 0.0259833 loss)
I0703 02:20:40.706174 31050 sgd_solver.cpp:106] Iteration 11900, lr = 1e-05
I0703 02:21:04.658290 31050 solver.cpp:354] Sparsity after update:
I0703 02:21:04.660158 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 02:21:04.660167 31050 net.cpp:1851] conv1a_param_0(0.2) 
I0703 02:21:04.660177 31050 net.cpp:1851] conv1b_param_0(0.4) 
I0703 02:21:04.660182 31050 net.cpp:1851] ctx_conv1_param_0(0.4) 
I0703 02:21:04.660187 31050 net.cpp:1851] ctx_conv2_param_0(0.4) 
I0703 02:21:04.660190 31050 net.cpp:1851] ctx_conv3_param_0(0.4) 
I0703 02:21:04.660194 31050 net.cpp:1851] ctx_conv4_param_0(0.4) 
I0703 02:21:04.660199 31050 net.cpp:1851] ctx_final_param_0(0.00608) 
I0703 02:21:04.660204 31050 net.cpp:1851] out3a_param_0(0.4) 
I0703 02:21:04.660207 31050 net.cpp:1851] out5a_param_0(0.4) 
I0703 02:21:04.660210 31050 net.cpp:1851] res2a_branch2a_param_0(0.4) 
I0703 02:21:04.660213 31050 net.cpp:1851] res2a_branch2b_param_0(0.4) 
I0703 02:21:04.660218 31050 net.cpp:1851] res3a_branch2a_param_0(0.4) 
I0703 02:21:04.660220 31050 net.cpp:1851] res3a_branch2b_param_0(0.4) 
I0703 02:21:04.660223 31050 net.cpp:1851] res4a_branch2a_param_0(0.4) 
I0703 02:21:04.660231 31050 net.cpp:1851] res4a_branch2b_param_0(0.4) 
I0703 02:21:04.660235 31050 net.cpp:1851] res5a_branch2a_param_0(0.4) 
I0703 02:21:04.660239 31050 net.cpp:1851] res5a_branch2b_param_0(0.4) 
I0703 02:21:04.660244 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.07404e+06/2.69117e+06) 0.399
I0703 02:21:04.660384 31050 solver.cpp:473] Iteration 12000, Testing net (#0)
I0703 02:21:52.565165 31050 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.954547
I0703 02:21:52.565268 31050 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.999663
I0703 02:21:52.565274 31050 solver.cpp:546]     Test net output #2: loss = 0.147363 (* 1 = 0.147363 loss)
I0703 02:21:52.837323 31050 solver.cpp:290] Iteration 12000 (1.3864 iter/s, 72.1292s/100 iter), loss = 0.0207261
I0703 02:21:52.837347 31050 solver.cpp:309]     Train net output #0: loss = 0.0207261 (* 1 = 0.0207261 loss)
I0703 02:21:52.837353 31050 sgd_solver.cpp:106] Iteration 12000, lr = 1e-05
I0703 02:21:52.838320 31050 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.45
I0703 02:21:53.441296 31050 net.cpp:1824] All zero weights of convolution layers are frozen
I0703 02:22:16.925705 31050 solver.cpp:290] Iteration 12100 (4.1515 iter/s, 24.0877s/100 iter), loss = 0.0194228
I0703 02:22:16.925731 31050 solver.cpp:309]     Train net output #0: loss = 0.0194228 (* 1 = 0.0194228 loss)
I0703 02:22:16.925739 31050 sgd_solver.cpp:106] Iteration 12100, lr = 1e-05
I0703 02:22:41.285823 31050 solver.cpp:290] Iteration 12200 (4.10519 iter/s, 24.3594s/100 iter), loss = 0.0399094
I0703 02:22:41.285933 31050 solver.cpp:309]     Train net output #0: loss = 0.0399093 (* 1 = 0.0399093 loss)
I0703 02:22:41.285944 31050 sgd_solver.cpp:106] Iteration 12200, lr = 1e-05
I0703 02:23:05.422039 31050 solver.cpp:290] Iteration 12300 (4.14328 iter/s, 24.1354s/100 iter), loss = 0.0278212
I0703 02:23:05.422063 31050 solver.cpp:309]     Train net output #0: loss = 0.0278212 (* 1 = 0.0278212 loss)
I0703 02:23:05.422070 31050 sgd_solver.cpp:106] Iteration 12300, lr = 1e-05
I0703 02:23:29.535526 31050 solver.cpp:290] Iteration 12400 (4.14717 iter/s, 24.1128s/100 iter), loss = 0.0174581
I0703 02:23:29.535637 31050 solver.cpp:309]     Train net output #0: loss = 0.0174581 (* 1 = 0.0174581 loss)
I0703 02:23:29.535653 31050 sgd_solver.cpp:106] Iteration 12400, lr = 1e-05
I0703 02:23:53.697687 31050 solver.cpp:290] Iteration 12500 (4.13883 iter/s, 24.1614s/100 iter), loss = 0.0175629
I0703 02:23:53.697710 31050 solver.cpp:309]     Train net output #0: loss = 0.0175629 (* 1 = 0.0175629 loss)
I0703 02:23:53.697717 31050 sgd_solver.cpp:106] Iteration 12500, lr = 1e-05
I0703 02:24:17.845249 31050 solver.cpp:290] Iteration 12600 (4.14132 iter/s, 24.1469s/100 iter), loss = 0.0321011
I0703 02:24:17.845345 31050 solver.cpp:309]     Train net output #0: loss = 0.0321011 (* 1 = 0.0321011 loss)
I0703 02:24:17.845355 31050 sgd_solver.cpp:106] Iteration 12600, lr = 1e-05
I0703 02:24:42.023074 31050 solver.cpp:290] Iteration 12700 (4.13615 iter/s, 24.1771s/100 iter), loss = 0.0348097
I0703 02:24:42.023098 31050 solver.cpp:309]     Train net output #0: loss = 0.0348097 (* 1 = 0.0348097 loss)
I0703 02:24:42.023105 31050 sgd_solver.cpp:106] Iteration 12700, lr = 1e-05
I0703 02:25:06.206933 31050 solver.cpp:290] Iteration 12800 (4.13511 iter/s, 24.1832s/100 iter), loss = 0.0296377
I0703 02:25:06.206989 31050 solver.cpp:309]     Train net output #0: loss = 0.0296377 (* 1 = 0.0296377 loss)
I0703 02:25:06.206997 31050 sgd_solver.cpp:106] Iteration 12800, lr = 1e-05
I0703 02:25:30.393307 31050 solver.cpp:290] Iteration 12900 (4.13468 iter/s, 24.1857s/100 iter), loss = 0.0210725
I0703 02:25:30.393329 31050 solver.cpp:309]     Train net output #0: loss = 0.0210725 (* 1 = 0.0210725 loss)
I0703 02:25:30.393337 31050 sgd_solver.cpp:106] Iteration 12900, lr = 1e-05
I0703 02:25:54.325242 31050 solver.cpp:354] Sparsity after update:
I0703 02:25:54.370759 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 02:25:54.370774 31050 net.cpp:1851] conv1a_param_0(0.225) 
I0703 02:25:54.370784 31050 net.cpp:1851] conv1b_param_0(0.45) 
I0703 02:25:54.370789 31050 net.cpp:1851] ctx_conv1_param_0(0.45) 
I0703 02:25:54.370791 31050 net.cpp:1851] ctx_conv2_param_0(0.45) 
I0703 02:25:54.370795 31050 net.cpp:1851] ctx_conv3_param_0(0.45) 
I0703 02:25:54.370800 31050 net.cpp:1851] ctx_conv4_param_0(0.45) 
I0703 02:25:54.370805 31050 net.cpp:1851] ctx_final_param_0(0.00174) 
I0703 02:25:54.370810 31050 net.cpp:1851] out3a_param_0(0.45) 
I0703 02:25:54.370812 31050 net.cpp:1851] out5a_param_0(0.45) 
I0703 02:25:54.370816 31050 net.cpp:1851] res2a_branch2a_param_0(0.45) 
I0703 02:25:54.370821 31050 net.cpp:1851] res2a_branch2b_param_0(0.45) 
I0703 02:25:54.370824 31050 net.cpp:1851] res3a_branch2a_param_0(0.45) 
I0703 02:25:54.370828 31050 net.cpp:1851] res3a_branch2b_param_0(0.45) 
I0703 02:25:54.370833 31050 net.cpp:1851] res4a_branch2a_param_0(0.45) 
I0703 02:25:54.370837 31050 net.cpp:1851] res4a_branch2b_param_0(0.45) 
I0703 02:25:54.370841 31050 net.cpp:1851] res5a_branch2a_param_0(0.45) 
I0703 02:25:54.370843 31050 net.cpp:1851] res5a_branch2b_param_0(0.45) 
I0703 02:25:54.370846 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.2082e+06/2.69117e+06) 0.449
I0703 02:25:54.600826 31050 solver.cpp:290] Iteration 13000 (4.13106 iter/s, 24.2068s/100 iter), loss = 0.025335
I0703 02:25:54.600852 31050 solver.cpp:309]     Train net output #0: loss = 0.025335 (* 1 = 0.025335 loss)
I0703 02:25:54.600860 31050 sgd_solver.cpp:106] Iteration 13000, lr = 1e-05
I0703 02:25:54.601835 31050 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.5
I0703 02:25:55.263425 31050 net.cpp:1824] All zero weights of convolution layers are frozen
I0703 02:26:19.736353 31050 solver.cpp:290] Iteration 13100 (3.97854 iter/s, 25.1348s/100 iter), loss = 0.0343361
I0703 02:26:19.736378 31050 solver.cpp:309]     Train net output #0: loss = 0.034336 (* 1 = 0.034336 loss)
I0703 02:26:19.736384 31050 sgd_solver.cpp:106] Iteration 13100, lr = 1e-05
I0703 02:26:43.920722 31050 solver.cpp:290] Iteration 13200 (4.13502 iter/s, 24.1837s/100 iter), loss = 0.0268732
I0703 02:26:43.920837 31050 solver.cpp:309]     Train net output #0: loss = 0.0268732 (* 1 = 0.0268732 loss)
I0703 02:26:43.920848 31050 sgd_solver.cpp:106] Iteration 13200, lr = 1e-05
I0703 02:27:08.064492 31050 solver.cpp:290] Iteration 13300 (4.14199 iter/s, 24.143s/100 iter), loss = 0.0308546
I0703 02:27:08.064515 31050 solver.cpp:309]     Train net output #0: loss = 0.0308546 (* 1 = 0.0308546 loss)
I0703 02:27:08.064522 31050 sgd_solver.cpp:106] Iteration 13300, lr = 1e-05
I0703 02:27:32.215654 31050 solver.cpp:290] Iteration 13400 (4.1407 iter/s, 24.1505s/100 iter), loss = 0.026775
I0703 02:27:32.215770 31050 solver.cpp:309]     Train net output #0: loss = 0.0267749 (* 1 = 0.0267749 loss)
I0703 02:27:32.215780 31050 sgd_solver.cpp:106] Iteration 13400, lr = 1e-05
I0703 02:27:56.443253 31050 solver.cpp:290] Iteration 13500 (4.12765 iter/s, 24.2268s/100 iter), loss = 0.0266327
I0703 02:27:56.443275 31050 solver.cpp:309]     Train net output #0: loss = 0.0266327 (* 1 = 0.0266327 loss)
I0703 02:27:56.443282 31050 sgd_solver.cpp:106] Iteration 13500, lr = 1e-05
I0703 02:28:20.589872 31050 solver.cpp:290] Iteration 13600 (4.14148 iter/s, 24.1459s/100 iter), loss = 0.0251182
I0703 02:28:20.589969 31050 solver.cpp:309]     Train net output #0: loss = 0.0251182 (* 1 = 0.0251182 loss)
I0703 02:28:20.589977 31050 sgd_solver.cpp:106] Iteration 13600, lr = 1e-05
I0703 02:28:44.765090 31050 solver.cpp:290] Iteration 13700 (4.1366 iter/s, 24.1745s/100 iter), loss = 0.0243018
I0703 02:28:44.765116 31050 solver.cpp:309]     Train net output #0: loss = 0.0243018 (* 1 = 0.0243018 loss)
I0703 02:28:44.765125 31050 sgd_solver.cpp:106] Iteration 13700, lr = 1e-05
I0703 02:29:08.932721 31050 solver.cpp:290] Iteration 13800 (4.13788 iter/s, 24.167s/100 iter), loss = 0.0240143
I0703 02:29:08.932854 31050 solver.cpp:309]     Train net output #0: loss = 0.0240142 (* 1 = 0.0240142 loss)
I0703 02:29:08.932864 31050 sgd_solver.cpp:106] Iteration 13800, lr = 1e-05
I0703 02:29:33.099200 31050 solver.cpp:290] Iteration 13900 (4.1381 iter/s, 24.1657s/100 iter), loss = 0.0246344
I0703 02:29:33.099222 31050 solver.cpp:309]     Train net output #0: loss = 0.0246344 (* 1 = 0.0246344 loss)
I0703 02:29:33.099230 31050 sgd_solver.cpp:106] Iteration 13900, lr = 1e-05
I0703 02:29:57.056411 31050 solver.cpp:354] Sparsity after update:
I0703 02:29:57.058238 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 02:29:57.058245 31050 net.cpp:1851] conv1a_param_0(0.25) 
I0703 02:29:57.058256 31050 net.cpp:1851] conv1b_param_0(0.5) 
I0703 02:29:57.058261 31050 net.cpp:1851] ctx_conv1_param_0(0.5) 
I0703 02:29:57.058265 31050 net.cpp:1851] ctx_conv2_param_0(0.5) 
I0703 02:29:57.058269 31050 net.cpp:1851] ctx_conv3_param_0(0.5) 
I0703 02:29:57.058274 31050 net.cpp:1851] ctx_conv4_param_0(0.5) 
I0703 02:29:57.058277 31050 net.cpp:1851] ctx_final_param_0(0.00738) 
I0703 02:29:57.058281 31050 net.cpp:1851] out3a_param_0(0.5) 
I0703 02:29:57.058285 31050 net.cpp:1851] out5a_param_0(0.5) 
I0703 02:29:57.058290 31050 net.cpp:1851] res2a_branch2a_param_0(0.5) 
I0703 02:29:57.058293 31050 net.cpp:1851] res2a_branch2b_param_0(0.5) 
I0703 02:29:57.058297 31050 net.cpp:1851] res3a_branch2a_param_0(0.5) 
I0703 02:29:57.058301 31050 net.cpp:1851] res3a_branch2b_param_0(0.5) 
I0703 02:29:57.058305 31050 net.cpp:1851] res4a_branch2a_param_0(0.5) 
I0703 02:29:57.058310 31050 net.cpp:1851] res4a_branch2b_param_0(0.5) 
I0703 02:29:57.058313 31050 net.cpp:1851] res5a_branch2a_param_0(0.5) 
I0703 02:29:57.058318 31050 net.cpp:1851] res5a_branch2b_param_0(0.5) 
I0703 02:29:57.058322 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.34268e+06/2.69117e+06) 0.499
I0703 02:29:57.058467 31050 solver.cpp:473] Iteration 14000, Testing net (#0)
I0703 02:30:46.914063 31050 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.95353
I0703 02:30:46.914218 31050 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.999878
I0703 02:30:46.914229 31050 solver.cpp:546]     Test net output #2: loss = 0.146663 (* 1 = 0.146663 loss)
I0703 02:30:47.155061 31050 solver.cpp:290] Iteration 14000 (1.35037 iter/s, 74.0538s/100 iter), loss = 0.0329122
I0703 02:30:47.155086 31050 solver.cpp:309]     Train net output #0: loss = 0.0329121 (* 1 = 0.0329121 loss)
I0703 02:30:47.155092 31050 sgd_solver.cpp:106] Iteration 14000, lr = 1e-05
I0703 02:30:47.156056 31050 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.55
I0703 02:30:47.892984 31050 net.cpp:1824] All zero weights of convolution layers are frozen
I0703 02:31:11.381537 31050 solver.cpp:290] Iteration 14100 (4.12783 iter/s, 24.2258s/100 iter), loss = 0.0249478
I0703 02:31:11.381561 31050 solver.cpp:309]     Train net output #0: loss = 0.0249478 (* 1 = 0.0249478 loss)
I0703 02:31:11.381568 31050 sgd_solver.cpp:106] Iteration 14100, lr = 1e-05
I0703 02:31:35.580016 31050 solver.cpp:290] Iteration 14200 (4.13261 iter/s, 24.1978s/100 iter), loss = 0.0196924
I0703 02:31:35.580124 31050 solver.cpp:309]     Train net output #0: loss = 0.0196924 (* 1 = 0.0196924 loss)
I0703 02:31:35.580135 31050 sgd_solver.cpp:106] Iteration 14200, lr = 1e-05
I0703 02:31:59.732265 31050 solver.cpp:290] Iteration 14300 (4.14053 iter/s, 24.1515s/100 iter), loss = 0.0206623
I0703 02:31:59.732291 31050 solver.cpp:309]     Train net output #0: loss = 0.0206623 (* 1 = 0.0206623 loss)
I0703 02:31:59.732300 31050 sgd_solver.cpp:106] Iteration 14300, lr = 1e-05
I0703 02:32:23.918612 31050 solver.cpp:290] Iteration 14400 (4.13468 iter/s, 24.1857s/100 iter), loss = 0.0355147
I0703 02:32:23.918679 31050 solver.cpp:309]     Train net output #0: loss = 0.0355147 (* 1 = 0.0355147 loss)
I0703 02:32:23.918687 31050 sgd_solver.cpp:106] Iteration 14400, lr = 1e-05
I0703 02:32:48.104125 31050 solver.cpp:290] Iteration 14500 (4.13483 iter/s, 24.1848s/100 iter), loss = 0.0300684
I0703 02:32:48.104149 31050 solver.cpp:309]     Train net output #0: loss = 0.0300684 (* 1 = 0.0300684 loss)
I0703 02:32:48.104159 31050 sgd_solver.cpp:106] Iteration 14500, lr = 1e-05
I0703 02:33:12.258070 31050 solver.cpp:290] Iteration 14600 (4.14023 iter/s, 24.1533s/100 iter), loss = 0.0400281
I0703 02:33:12.258184 31050 solver.cpp:309]     Train net output #0: loss = 0.040028 (* 1 = 0.040028 loss)
I0703 02:33:12.258194 31050 sgd_solver.cpp:106] Iteration 14600, lr = 1e-05
I0703 02:33:36.426980 31050 solver.cpp:290] Iteration 14700 (4.13768 iter/s, 24.1681s/100 iter), loss = 0.0428297
I0703 02:33:36.427006 31050 solver.cpp:309]     Train net output #0: loss = 0.0428296 (* 1 = 0.0428296 loss)
I0703 02:33:36.427013 31050 sgd_solver.cpp:106] Iteration 14700, lr = 1e-05
I0703 02:34:00.610229 31050 solver.cpp:290] Iteration 14800 (4.13521 iter/s, 24.1826s/100 iter), loss = 0.0263512
I0703 02:34:00.610339 31050 solver.cpp:309]     Train net output #0: loss = 0.0263511 (* 1 = 0.0263511 loss)
I0703 02:34:00.610349 31050 sgd_solver.cpp:106] Iteration 14800, lr = 1e-05
I0703 02:34:24.791357 31050 solver.cpp:290] Iteration 14900 (4.13559 iter/s, 24.1804s/100 iter), loss = 0.0300247
I0703 02:34:24.791380 31050 solver.cpp:309]     Train net output #0: loss = 0.0300247 (* 1 = 0.0300247 loss)
I0703 02:34:24.791388 31050 sgd_solver.cpp:106] Iteration 14900, lr = 1e-05
I0703 02:34:48.745493 31050 solver.cpp:354] Sparsity after update:
I0703 02:34:48.791347 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 02:34:48.791363 31050 net.cpp:1851] conv1a_param_0(0.275) 
I0703 02:34:48.791370 31050 net.cpp:1851] conv1b_param_0(0.55) 
I0703 02:34:48.791373 31050 net.cpp:1851] ctx_conv1_param_0(0.55) 
I0703 02:34:48.791375 31050 net.cpp:1851] ctx_conv2_param_0(0.55) 
I0703 02:34:48.791376 31050 net.cpp:1851] ctx_conv3_param_0(0.55) 
I0703 02:34:48.791378 31050 net.cpp:1851] ctx_conv4_param_0(0.55) 
I0703 02:34:48.791380 31050 net.cpp:1851] ctx_final_param_0(0.00694) 
I0703 02:34:48.791383 31050 net.cpp:1851] out3a_param_0(0.55) 
I0703 02:34:48.791384 31050 net.cpp:1851] out5a_param_0(0.55) 
I0703 02:34:48.791386 31050 net.cpp:1851] res2a_branch2a_param_0(0.55) 
I0703 02:34:48.791388 31050 net.cpp:1851] res2a_branch2b_param_0(0.55) 
I0703 02:34:48.791391 31050 net.cpp:1851] res3a_branch2a_param_0(0.55) 
I0703 02:34:48.791393 31050 net.cpp:1851] res3a_branch2b_param_0(0.55) 
I0703 02:34:48.791395 31050 net.cpp:1851] res4a_branch2a_param_0(0.55) 
I0703 02:34:48.791396 31050 net.cpp:1851] res4a_branch2b_param_0(0.55) 
I0703 02:34:48.791399 31050 net.cpp:1851] res5a_branch2a_param_0(0.55) 
I0703 02:34:48.791400 31050 net.cpp:1851] res5a_branch2b_param_0(0.55) 
I0703 02:34:48.791402 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.47692e+06/2.69117e+06) 0.549
I0703 02:34:49.022296 31050 solver.cpp:290] Iteration 15000 (4.12707 iter/s, 24.2303s/100 iter), loss = 0.0291666
I0703 02:34:49.022320 31050 solver.cpp:309]     Train net output #0: loss = 0.0291666 (* 1 = 0.0291666 loss)
I0703 02:34:49.022326 31050 sgd_solver.cpp:106] Iteration 15000, lr = 1e-05
I0703 02:34:49.023340 31050 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.6
I0703 02:34:49.838785 31050 net.cpp:1824] All zero weights of convolution layers are frozen
I0703 02:35:14.098301 31050 solver.cpp:290] Iteration 15100 (3.98799 iter/s, 25.0753s/100 iter), loss = 0.020981
I0703 02:35:14.098325 31050 solver.cpp:309]     Train net output #0: loss = 0.020981 (* 1 = 0.020981 loss)
I0703 02:35:14.098332 31050 sgd_solver.cpp:106] Iteration 15100, lr = 1e-05
I0703 02:35:38.599553 31050 solver.cpp:290] Iteration 15200 (4.08154 iter/s, 24.5006s/100 iter), loss = 0.0314504
I0703 02:35:38.600082 31050 solver.cpp:309]     Train net output #0: loss = 0.0314504 (* 1 = 0.0314504 loss)
I0703 02:35:38.600093 31050 sgd_solver.cpp:106] Iteration 15200, lr = 1e-05
I0703 02:36:02.790343 31050 solver.cpp:290] Iteration 15300 (4.13401 iter/s, 24.1896s/100 iter), loss = 0.0267294
I0703 02:36:02.790366 31050 solver.cpp:309]     Train net output #0: loss = 0.0267294 (* 1 = 0.0267294 loss)
I0703 02:36:02.790374 31050 sgd_solver.cpp:106] Iteration 15300, lr = 1e-05
I0703 02:36:26.976207 31050 solver.cpp:290] Iteration 15400 (4.13476 iter/s, 24.1852s/100 iter), loss = 0.0267781
I0703 02:36:26.976281 31050 solver.cpp:309]     Train net output #0: loss = 0.0267781 (* 1 = 0.0267781 loss)
I0703 02:36:26.976290 31050 sgd_solver.cpp:106] Iteration 15400, lr = 1e-05
I0703 02:36:51.139251 31050 solver.cpp:290] Iteration 15500 (4.13867 iter/s, 24.1623s/100 iter), loss = 0.0420833
I0703 02:36:51.139273 31050 solver.cpp:309]     Train net output #0: loss = 0.0420833 (* 1 = 0.0420833 loss)
I0703 02:36:51.139281 31050 sgd_solver.cpp:106] Iteration 15500, lr = 1e-05
I0703 02:37:15.348136 31050 solver.cpp:290] Iteration 15600 (4.13083 iter/s, 24.2082s/100 iter), loss = 0.0235285
I0703 02:37:15.348248 31050 solver.cpp:309]     Train net output #0: loss = 0.0235285 (* 1 = 0.0235285 loss)
I0703 02:37:15.348258 31050 sgd_solver.cpp:106] Iteration 15600, lr = 1e-05
I0703 02:37:39.518260 31050 solver.cpp:290] Iteration 15700 (4.13747 iter/s, 24.1694s/100 iter), loss = 0.036093
I0703 02:37:39.518285 31050 solver.cpp:309]     Train net output #0: loss = 0.036093 (* 1 = 0.036093 loss)
I0703 02:37:39.518295 31050 sgd_solver.cpp:106] Iteration 15700, lr = 1e-05
I0703 02:38:03.695063 31050 solver.cpp:290] Iteration 15800 (4.13631 iter/s, 24.1761s/100 iter), loss = 0.0295454
I0703 02:38:03.695171 31050 solver.cpp:309]     Train net output #0: loss = 0.0295454 (* 1 = 0.0295454 loss)
I0703 02:38:03.695181 31050 sgd_solver.cpp:106] Iteration 15800, lr = 1e-05
I0703 02:38:27.866125 31050 solver.cpp:290] Iteration 15900 (4.13731 iter/s, 24.1703s/100 iter), loss = 0.0255017
I0703 02:38:27.866149 31050 solver.cpp:309]     Train net output #0: loss = 0.0255016 (* 1 = 0.0255016 loss)
I0703 02:38:27.866156 31050 sgd_solver.cpp:106] Iteration 15900, lr = 1e-05
I0703 02:38:51.905207 31050 solver.cpp:354] Sparsity after update:
I0703 02:38:51.907065 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 02:38:51.907073 31050 net.cpp:1851] conv1a_param_0(0.3) 
I0703 02:38:51.907080 31050 net.cpp:1851] conv1b_param_0(0.6) 
I0703 02:38:51.907083 31050 net.cpp:1851] ctx_conv1_param_0(0.6) 
I0703 02:38:51.907085 31050 net.cpp:1851] ctx_conv2_param_0(0.6) 
I0703 02:38:51.907088 31050 net.cpp:1851] ctx_conv3_param_0(0.6) 
I0703 02:38:51.907088 31050 net.cpp:1851] ctx_conv4_param_0(0.6) 
I0703 02:38:51.907090 31050 net.cpp:1851] ctx_final_param_0(0.00109) 
I0703 02:38:51.907093 31050 net.cpp:1851] out3a_param_0(0.6) 
I0703 02:38:51.907094 31050 net.cpp:1851] out5a_param_0(0.6) 
I0703 02:38:51.907096 31050 net.cpp:1851] res2a_branch2a_param_0(0.6) 
I0703 02:38:51.907099 31050 net.cpp:1851] res2a_branch2b_param_0(0.6) 
I0703 02:38:51.907101 31050 net.cpp:1851] res3a_branch2a_param_0(0.6) 
I0703 02:38:51.907104 31050 net.cpp:1851] res3a_branch2b_param_0(0.6) 
I0703 02:38:51.907104 31050 net.cpp:1851] res4a_branch2a_param_0(0.6) 
I0703 02:38:51.907106 31050 net.cpp:1851] res4a_branch2b_param_0(0.6) 
I0703 02:38:51.907109 31050 net.cpp:1851] res5a_branch2a_param_0(0.6) 
I0703 02:38:51.907111 31050 net.cpp:1851] res5a_branch2b_param_0(0.6) 
I0703 02:38:51.907114 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.6112e+06/2.69117e+06) 0.599
I0703 02:38:51.907248 31050 solver.cpp:473] Iteration 16000, Testing net (#0)
I0703 02:39:40.007951 31050 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.952019
I0703 02:39:40.008052 31050 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.99993
I0703 02:39:40.008059 31050 solver.cpp:546]     Test net output #2: loss = 0.144492 (* 1 = 0.144492 loss)
I0703 02:39:40.261543 31050 solver.cpp:290] Iteration 16000 (1.38134 iter/s, 72.3935s/100 iter), loss = 0.0405432
I0703 02:39:40.261566 31050 solver.cpp:309]     Train net output #0: loss = 0.0405432 (* 1 = 0.0405432 loss)
I0703 02:39:40.261574 31050 sgd_solver.cpp:106] Iteration 16000, lr = 1e-05
I0703 02:39:40.262557 31050 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.65
I0703 02:39:41.146764 31050 net.cpp:1824] All zero weights of convolution layers are frozen
I0703 02:40:04.634600 31050 solver.cpp:290] Iteration 16100 (4.10301 iter/s, 24.3724s/100 iter), loss = 0.0284019
I0703 02:40:04.634626 31050 solver.cpp:309]     Train net output #0: loss = 0.0284019 (* 1 = 0.0284019 loss)
I0703 02:40:04.634634 31050 sgd_solver.cpp:106] Iteration 16100, lr = 1e-05
I0703 02:40:28.755959 31050 solver.cpp:290] Iteration 16200 (4.14582 iter/s, 24.1207s/100 iter), loss = 0.0298297
I0703 02:40:28.756089 31050 solver.cpp:309]     Train net output #0: loss = 0.0298297 (* 1 = 0.0298297 loss)
I0703 02:40:28.756099 31050 sgd_solver.cpp:106] Iteration 16200, lr = 1e-05
I0703 02:40:52.947152 31050 solver.cpp:290] Iteration 16300 (4.13387 iter/s, 24.1904s/100 iter), loss = 0.0281382
I0703 02:40:52.947176 31050 solver.cpp:309]     Train net output #0: loss = 0.0281382 (* 1 = 0.0281382 loss)
I0703 02:40:52.947183 31050 sgd_solver.cpp:106] Iteration 16300, lr = 1e-05
I0703 02:41:17.140566 31050 solver.cpp:290] Iteration 16400 (4.13347 iter/s, 24.1927s/100 iter), loss = 0.0383234
I0703 02:41:17.140678 31050 solver.cpp:309]     Train net output #0: loss = 0.0383234 (* 1 = 0.0383234 loss)
I0703 02:41:17.140688 31050 sgd_solver.cpp:106] Iteration 16400, lr = 1e-05
I0703 02:41:41.323052 31050 solver.cpp:290] Iteration 16500 (4.13535 iter/s, 24.1817s/100 iter), loss = 0.017105
I0703 02:41:41.323077 31050 solver.cpp:309]     Train net output #0: loss = 0.017105 (* 1 = 0.017105 loss)
I0703 02:41:41.323084 31050 sgd_solver.cpp:106] Iteration 16500, lr = 1e-05
I0703 02:42:05.444658 31050 solver.cpp:290] Iteration 16600 (4.14578 iter/s, 24.1209s/100 iter), loss = 0.0304796
I0703 02:42:05.444764 31050 solver.cpp:309]     Train net output #0: loss = 0.0304796 (* 1 = 0.0304796 loss)
I0703 02:42:05.444777 31050 sgd_solver.cpp:106] Iteration 16600, lr = 1e-05
I0703 02:42:29.856042 31050 solver.cpp:290] Iteration 16700 (4.09658 iter/s, 24.4106s/100 iter), loss = 0.0474211
I0703 02:42:29.856094 31050 solver.cpp:309]     Train net output #0: loss = 0.0474211 (* 1 = 0.0474211 loss)
I0703 02:42:29.856109 31050 sgd_solver.cpp:106] Iteration 16700, lr = 1e-05
I0703 02:42:54.567157 31050 solver.cpp:290] Iteration 16800 (4.04688 iter/s, 24.7104s/100 iter), loss = 0.0270917
I0703 02:42:54.567268 31050 solver.cpp:309]     Train net output #0: loss = 0.0270917 (* 1 = 0.0270917 loss)
I0703 02:42:54.567278 31050 sgd_solver.cpp:106] Iteration 16800, lr = 1e-05
I0703 02:43:18.737538 31050 solver.cpp:290] Iteration 16900 (4.13742 iter/s, 24.1696s/100 iter), loss = 0.0314104
I0703 02:43:18.737561 31050 solver.cpp:309]     Train net output #0: loss = 0.0314104 (* 1 = 0.0314104 loss)
I0703 02:43:18.737568 31050 sgd_solver.cpp:106] Iteration 16900, lr = 1e-05
I0703 02:43:42.657943 31050 solver.cpp:354] Sparsity after update:
I0703 02:43:42.664575 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 02:43:42.664608 31050 net.cpp:1851] conv1a_param_0(0.325) 
I0703 02:43:42.664626 31050 net.cpp:1851] conv1b_param_0(0.65) 
I0703 02:43:42.664628 31050 net.cpp:1851] ctx_conv1_param_0(0.65) 
I0703 02:43:42.664630 31050 net.cpp:1851] ctx_conv2_param_0(0.65) 
I0703 02:43:42.664633 31050 net.cpp:1851] ctx_conv3_param_0(0.65) 
I0703 02:43:42.664634 31050 net.cpp:1851] ctx_conv4_param_0(0.65) 
I0703 02:43:42.664636 31050 net.cpp:1851] ctx_final_param_0(0) 
I0703 02:43:42.664638 31050 net.cpp:1851] out3a_param_0(0.65) 
I0703 02:43:42.664640 31050 net.cpp:1851] out5a_param_0(0.65) 
I0703 02:43:42.664643 31050 net.cpp:1851] res2a_branch2a_param_0(0.65) 
I0703 02:43:42.664644 31050 net.cpp:1851] res2a_branch2b_param_0(0.65) 
I0703 02:43:42.664646 31050 net.cpp:1851] res3a_branch2a_param_0(0.65) 
I0703 02:43:42.664649 31050 net.cpp:1851] res3a_branch2b_param_0(0.65) 
I0703 02:43:42.664650 31050 net.cpp:1851] res4a_branch2a_param_0(0.65) 
I0703 02:43:42.664651 31050 net.cpp:1851] res4a_branch2b_param_0(0.65) 
I0703 02:43:42.664654 31050 net.cpp:1851] res5a_branch2a_param_0(0.65) 
I0703 02:43:42.664655 31050 net.cpp:1851] res5a_branch2b_param_0(0.65) 
I0703 02:43:42.664657 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.74543e+06/2.69117e+06) 0.649
I0703 02:43:42.914911 31050 solver.cpp:290] Iteration 17000 (4.13621 iter/s, 24.1767s/100 iter), loss = 0.0254071
I0703 02:43:42.914934 31050 solver.cpp:309]     Train net output #0: loss = 0.0254071 (* 1 = 0.0254071 loss)
I0703 02:43:42.914942 31050 sgd_solver.cpp:106] Iteration 17000, lr = 1e-05
I0703 02:43:42.915923 31050 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.7
I0703 02:43:43.920914 31050 net.cpp:1824] All zero weights of convolution layers are frozen
I0703 02:44:08.054103 31050 solver.cpp:290] Iteration 17100 (3.97796 iter/s, 25.1385s/100 iter), loss = 0.0268872
I0703 02:44:08.054126 31050 solver.cpp:309]     Train net output #0: loss = 0.0268872 (* 1 = 0.0268872 loss)
I0703 02:44:08.054133 31050 sgd_solver.cpp:106] Iteration 17100, lr = 1e-05
I0703 02:44:32.246744 31050 solver.cpp:290] Iteration 17200 (4.1336 iter/s, 24.192s/100 iter), loss = 0.0592871
I0703 02:44:32.246821 31050 solver.cpp:309]     Train net output #0: loss = 0.0592871 (* 1 = 0.0592871 loss)
I0703 02:44:32.246830 31050 sgd_solver.cpp:106] Iteration 17200, lr = 1e-05
I0703 02:44:56.421237 31050 solver.cpp:290] Iteration 17300 (4.13671 iter/s, 24.1738s/100 iter), loss = 0.0399308
I0703 02:44:56.421262 31050 solver.cpp:309]     Train net output #0: loss = 0.0399307 (* 1 = 0.0399307 loss)
I0703 02:44:56.421268 31050 sgd_solver.cpp:106] Iteration 17300, lr = 1e-05
I0703 02:45:20.583539 31050 solver.cpp:290] Iteration 17400 (4.13879 iter/s, 24.1616s/100 iter), loss = 0.0461443
I0703 02:45:20.583649 31050 solver.cpp:309]     Train net output #0: loss = 0.0461443 (* 1 = 0.0461443 loss)
I0703 02:45:20.583659 31050 sgd_solver.cpp:106] Iteration 17400, lr = 1e-05
I0703 02:45:44.759505 31050 solver.cpp:290] Iteration 17500 (4.13647 iter/s, 24.1752s/100 iter), loss = 0.0331534
I0703 02:45:44.759527 31050 solver.cpp:309]     Train net output #0: loss = 0.0331534 (* 1 = 0.0331534 loss)
I0703 02:45:44.759536 31050 sgd_solver.cpp:106] Iteration 17500, lr = 1e-05
I0703 02:46:09.281416 31050 solver.cpp:290] Iteration 17600 (4.0781 iter/s, 24.5212s/100 iter), loss = 0.0531705
I0703 02:46:09.281529 31050 solver.cpp:309]     Train net output #0: loss = 0.0531705 (* 1 = 0.0531705 loss)
I0703 02:46:09.281543 31050 sgd_solver.cpp:106] Iteration 17600, lr = 1e-05
I0703 02:46:33.956682 31050 solver.cpp:290] Iteration 17700 (4.05277 iter/s, 24.6745s/100 iter), loss = 0.0343455
I0703 02:46:33.956707 31050 solver.cpp:309]     Train net output #0: loss = 0.0343454 (* 1 = 0.0343454 loss)
I0703 02:46:33.956713 31050 sgd_solver.cpp:106] Iteration 17700, lr = 1e-05
I0703 02:46:58.142078 31050 solver.cpp:290] Iteration 17800 (4.13484 iter/s, 24.1847s/100 iter), loss = 0.0347701
I0703 02:46:58.142110 31050 solver.cpp:309]     Train net output #0: loss = 0.0347701 (* 1 = 0.0347701 loss)
I0703 02:46:58.142117 31050 sgd_solver.cpp:106] Iteration 17800, lr = 1e-05
I0703 02:47:22.283491 31050 solver.cpp:290] Iteration 17900 (4.14238 iter/s, 24.1407s/100 iter), loss = 0.0465073
I0703 02:47:22.283516 31050 solver.cpp:309]     Train net output #0: loss = 0.0465073 (* 1 = 0.0465073 loss)
I0703 02:47:22.283524 31050 sgd_solver.cpp:106] Iteration 17900, lr = 1e-05
I0703 02:47:46.221604 31050 solver.cpp:354] Sparsity after update:
I0703 02:47:46.223436 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 02:47:46.223444 31050 net.cpp:1851] conv1a_param_0(0.35) 
I0703 02:47:46.223451 31050 net.cpp:1851] conv1b_param_0(0.7) 
I0703 02:47:46.223453 31050 net.cpp:1851] ctx_conv1_param_0(0.7) 
I0703 02:47:46.223455 31050 net.cpp:1851] ctx_conv2_param_0(0.7) 
I0703 02:47:46.223457 31050 net.cpp:1851] ctx_conv3_param_0(0.7) 
I0703 02:47:46.223459 31050 net.cpp:1851] ctx_conv4_param_0(0.7) 
I0703 02:47:46.223461 31050 net.cpp:1851] ctx_final_param_0(0.000217) 
I0703 02:47:46.223464 31050 net.cpp:1851] out3a_param_0(0.7) 
I0703 02:47:46.223465 31050 net.cpp:1851] out5a_param_0(0.7) 
I0703 02:47:46.223467 31050 net.cpp:1851] res2a_branch2a_param_0(0.7) 
I0703 02:47:46.223469 31050 net.cpp:1851] res2a_branch2b_param_0(0.7) 
I0703 02:47:46.223471 31050 net.cpp:1851] res3a_branch2a_param_0(0.7) 
I0703 02:47:46.223474 31050 net.cpp:1851] res3a_branch2b_param_0(0.7) 
I0703 02:47:46.223474 31050 net.cpp:1851] res4a_branch2a_param_0(0.7) 
I0703 02:47:46.223476 31050 net.cpp:1851] res4a_branch2b_param_0(0.7) 
I0703 02:47:46.223479 31050 net.cpp:1851] res5a_branch2a_param_0(0.7) 
I0703 02:47:46.223480 31050 net.cpp:1851] res5a_branch2b_param_0(0.7) 
I0703 02:47:46.223482 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.87972e+06/2.69117e+06) 0.698
I0703 02:47:46.223618 31050 solver.cpp:473] Iteration 18000, Testing net (#0)
I0703 02:48:33.917950 31050 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.9489
I0703 02:48:33.918004 31050 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.999852
I0703 02:48:33.918010 31050 solver.cpp:546]     Test net output #2: loss = 0.14286 (* 1 = 0.14286 loss)
I0703 02:48:34.198074 31050 solver.cpp:290] Iteration 18000 (1.39058 iter/s, 71.9126s/100 iter), loss = 0.0315998
I0703 02:48:34.198102 31050 solver.cpp:309]     Train net output #0: loss = 0.0315998 (* 1 = 0.0315998 loss)
I0703 02:48:34.198110 31050 sgd_solver.cpp:106] Iteration 18000, lr = 1e-05
I0703 02:48:34.199120 31050 solver.cpp:377] Finding and applying thresholds. Target sparsity = 0.75
I0703 02:48:35.321730 31050 net.cpp:1824] All zero weights of convolution layers are frozen
I0703 02:48:58.816195 31050 solver.cpp:290] Iteration 18100 (4.06216 iter/s, 24.6174s/100 iter), loss = 0.0873555
I0703 02:48:58.816220 31050 solver.cpp:309]     Train net output #0: loss = 0.0873555 (* 1 = 0.0873555 loss)
I0703 02:48:58.816227 31050 sgd_solver.cpp:106] Iteration 18100, lr = 1e-05
I0703 02:49:23.049517 31050 solver.cpp:290] Iteration 18200 (4.12667 iter/s, 24.2326s/100 iter), loss = 0.0468204
I0703 02:49:23.049610 31050 solver.cpp:309]     Train net output #0: loss = 0.0468204 (* 1 = 0.0468204 loss)
I0703 02:49:23.049630 31050 sgd_solver.cpp:106] Iteration 18200, lr = 1e-05
I0703 02:49:47.553936 31050 solver.cpp:290] Iteration 18300 (4.08102 iter/s, 24.5037s/100 iter), loss = 0.0574592
I0703 02:49:47.553961 31050 solver.cpp:309]     Train net output #0: loss = 0.0574591 (* 1 = 0.0574591 loss)
I0703 02:49:47.553967 31050 sgd_solver.cpp:106] Iteration 18300, lr = 1e-05
I0703 02:50:12.206954 31050 solver.cpp:290] Iteration 18400 (4.05641 iter/s, 24.6523s/100 iter), loss = 0.0398709
I0703 02:50:12.207036 31050 solver.cpp:309]     Train net output #0: loss = 0.0398708 (* 1 = 0.0398708 loss)
I0703 02:50:12.207056 31050 sgd_solver.cpp:106] Iteration 18400, lr = 1e-05
I0703 02:50:36.797056 31050 solver.cpp:290] Iteration 18500 (4.0668 iter/s, 24.5894s/100 iter), loss = 0.034691
I0703 02:50:36.797080 31050 solver.cpp:309]     Train net output #0: loss = 0.0346909 (* 1 = 0.0346909 loss)
I0703 02:50:36.797086 31050 sgd_solver.cpp:106] Iteration 18500, lr = 1e-05
I0703 02:51:01.567848 31050 solver.cpp:290] Iteration 18600 (4.03713 iter/s, 24.7701s/100 iter), loss = 0.0543521
I0703 02:51:01.567899 31050 solver.cpp:309]     Train net output #0: loss = 0.0543521 (* 1 = 0.0543521 loss)
I0703 02:51:01.567908 31050 sgd_solver.cpp:106] Iteration 18600, lr = 1e-05
I0703 02:51:26.330118 31050 solver.cpp:290] Iteration 18700 (4.03852 iter/s, 24.7615s/100 iter), loss = 0.0418341
I0703 02:51:26.330152 31050 solver.cpp:309]     Train net output #0: loss = 0.041834 (* 1 = 0.041834 loss)
I0703 02:51:26.330160 31050 sgd_solver.cpp:106] Iteration 18700, lr = 1e-05
I0703 02:51:50.957901 31050 solver.cpp:290] Iteration 18800 (4.06057 iter/s, 24.6271s/100 iter), loss = 0.0536919
I0703 02:51:50.957968 31050 solver.cpp:309]     Train net output #0: loss = 0.0536919 (* 1 = 0.0536919 loss)
I0703 02:51:50.957979 31050 sgd_solver.cpp:106] Iteration 18800, lr = 1e-05
I0703 02:52:15.701690 31050 solver.cpp:290] Iteration 18900 (4.04154 iter/s, 24.743s/100 iter), loss = 0.0301538
I0703 02:52:15.701738 31050 solver.cpp:309]     Train net output #0: loss = 0.0301538 (* 1 = 0.0301538 loss)
I0703 02:52:15.701755 31050 sgd_solver.cpp:106] Iteration 18900, lr = 1e-05
I0703 02:52:40.216657 31050 solver.cpp:354] Sparsity after update:
I0703 02:52:40.260915 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 02:52:40.260932 31050 net.cpp:1851] conv1a_param_0(0.375) 
I0703 02:52:40.260939 31050 net.cpp:1851] conv1b_param_0(0.75) 
I0703 02:52:40.260942 31050 net.cpp:1851] ctx_conv1_param_0(0.75) 
I0703 02:52:40.260944 31050 net.cpp:1851] ctx_conv2_param_0(0.75) 
I0703 02:52:40.260946 31050 net.cpp:1851] ctx_conv3_param_0(0.75) 
I0703 02:52:40.260948 31050 net.cpp:1851] ctx_conv4_param_0(0.75) 
I0703 02:52:40.260951 31050 net.cpp:1851] ctx_final_param_0(0.00977) 
I0703 02:52:40.260952 31050 net.cpp:1851] out3a_param_0(0.75) 
I0703 02:52:40.260954 31050 net.cpp:1851] out5a_param_0(0.75) 
I0703 02:52:40.260956 31050 net.cpp:1851] res2a_branch2a_param_0(0.75) 
I0703 02:52:40.260958 31050 net.cpp:1851] res2a_branch2b_param_0(0.75) 
I0703 02:52:40.260960 31050 net.cpp:1851] res3a_branch2a_param_0(0.75) 
I0703 02:52:40.260962 31050 net.cpp:1851] res3a_branch2b_param_0(0.75) 
I0703 02:52:40.260964 31050 net.cpp:1851] res4a_branch2a_param_0(0.75) 
I0703 02:52:40.260967 31050 net.cpp:1851] res4a_branch2b_param_0(0.75) 
I0703 02:52:40.260968 31050 net.cpp:1851] res5a_branch2a_param_0(0.75) 
I0703 02:52:40.260970 31050 net.cpp:1851] res5a_branch2b_param_0(0.75) 
I0703 02:52:40.260972 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.01404e+06/2.69117e+06) 0.748
I0703 02:52:40.490803 31050 solver.cpp:290] Iteration 19000 (4.03415 iter/s, 24.7884s/100 iter), loss = 0.0562804
I0703 02:52:40.490826 31050 solver.cpp:309]     Train net output #0: loss = 0.0562804 (* 1 = 0.0562804 loss)
I0703 02:52:40.490833 31050 sgd_solver.cpp:106] Iteration 19000, lr = 1e-05
I0703 02:53:05.113171 31050 solver.cpp:290] Iteration 19100 (4.06146 iter/s, 24.6217s/100 iter), loss = 0.043082
I0703 02:53:05.113198 31050 solver.cpp:309]     Train net output #0: loss = 0.043082 (* 1 = 0.043082 loss)
I0703 02:53:05.113206 31050 sgd_solver.cpp:106] Iteration 19100, lr = 1e-05
I0703 02:53:29.763869 31050 solver.cpp:290] Iteration 19200 (4.0568 iter/s, 24.65s/100 iter), loss = 0.039303
I0703 02:53:29.764020 31050 solver.cpp:309]     Train net output #0: loss = 0.039303 (* 1 = 0.039303 loss)
I0703 02:53:29.764044 31050 sgd_solver.cpp:106] Iteration 19200, lr = 1e-05
I0703 02:53:54.463573 31050 solver.cpp:290] Iteration 19300 (4.04876 iter/s, 24.6989s/100 iter), loss = 0.0388101
I0703 02:53:54.463616 31050 solver.cpp:309]     Train net output #0: loss = 0.0388101 (* 1 = 0.0388101 loss)
I0703 02:53:54.463631 31050 sgd_solver.cpp:106] Iteration 19300, lr = 1e-05
I0703 02:54:19.055598 31050 solver.cpp:290] Iteration 19400 (4.06648 iter/s, 24.5913s/100 iter), loss = 0.0489562
I0703 02:54:19.055650 31050 solver.cpp:309]     Train net output #0: loss = 0.0489562 (* 1 = 0.0489562 loss)
I0703 02:54:19.055658 31050 sgd_solver.cpp:106] Iteration 19400, lr = 1e-05
I0703 02:54:43.209043 31050 solver.cpp:290] Iteration 19500 (4.14032 iter/s, 24.1527s/100 iter), loss = 0.0328271
I0703 02:54:43.209069 31050 solver.cpp:309]     Train net output #0: loss = 0.032827 (* 1 = 0.032827 loss)
I0703 02:54:43.209075 31050 sgd_solver.cpp:106] Iteration 19500, lr = 1e-05
I0703 02:55:07.347430 31050 solver.cpp:290] Iteration 19600 (4.14289 iter/s, 24.1377s/100 iter), loss = 0.0351995
I0703 02:55:07.347534 31050 solver.cpp:309]     Train net output #0: loss = 0.0351995 (* 1 = 0.0351995 loss)
I0703 02:55:07.347544 31050 sgd_solver.cpp:106] Iteration 19600, lr = 1e-05
I0703 02:55:31.513504 31050 solver.cpp:290] Iteration 19700 (4.13816 iter/s, 24.1653s/100 iter), loss = 0.0530345
I0703 02:55:31.513528 31050 solver.cpp:309]     Train net output #0: loss = 0.0530345 (* 1 = 0.0530345 loss)
I0703 02:55:31.513538 31050 sgd_solver.cpp:106] Iteration 19700, lr = 1e-05
I0703 02:55:55.688936 31050 solver.cpp:290] Iteration 19800 (4.13655 iter/s, 24.1748s/100 iter), loss = 0.0353925
I0703 02:55:55.689070 31050 solver.cpp:309]     Train net output #0: loss = 0.0353925 (* 1 = 0.0353925 loss)
I0703 02:55:55.689080 31050 sgd_solver.cpp:106] Iteration 19800, lr = 1e-05
I0703 02:56:19.848155 31050 solver.cpp:290] Iteration 19900 (4.13934 iter/s, 24.1584s/100 iter), loss = 0.0244907
I0703 02:56:19.848181 31050 solver.cpp:309]     Train net output #0: loss = 0.0244906 (* 1 = 0.0244906 loss)
I0703 02:56:19.848189 31050 sgd_solver.cpp:106] Iteration 19900, lr = 1e-05
I0703 02:56:43.761096 31050 solver.cpp:600] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-02_23-02-42/sparse/cityscapes5_jsegnet21v2_iter_20000.caffemodel
I0703 02:56:43.786293 31050 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-02_23-02-42/sparse/cityscapes5_jsegnet21v2_iter_20000.solverstate
I0703 02:56:43.802760 31050 solver.cpp:354] Sparsity after update:
I0703 02:56:43.803973 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 02:56:43.803982 31050 net.cpp:1851] conv1a_param_0(0.375) 
I0703 02:56:43.803992 31050 net.cpp:1851] conv1b_param_0(0.75) 
I0703 02:56:43.803997 31050 net.cpp:1851] ctx_conv1_param_0(0.75) 
I0703 02:56:43.804002 31050 net.cpp:1851] ctx_conv2_param_0(0.75) 
I0703 02:56:43.804005 31050 net.cpp:1851] ctx_conv3_param_0(0.75) 
I0703 02:56:43.804009 31050 net.cpp:1851] ctx_conv4_param_0(0.75) 
I0703 02:56:43.804013 31050 net.cpp:1851] ctx_final_param_0(0.00977) 
I0703 02:56:43.804018 31050 net.cpp:1851] out3a_param_0(0.75) 
I0703 02:56:43.804023 31050 net.cpp:1851] out5a_param_0(0.75) 
I0703 02:56:43.804025 31050 net.cpp:1851] res2a_branch2a_param_0(0.75) 
I0703 02:56:43.804029 31050 net.cpp:1851] res2a_branch2b_param_0(0.75) 
I0703 02:56:43.804033 31050 net.cpp:1851] res3a_branch2a_param_0(0.75) 
I0703 02:56:43.804038 31050 net.cpp:1851] res3a_branch2b_param_0(0.75) 
I0703 02:56:43.804041 31050 net.cpp:1851] res4a_branch2a_param_0(0.75) 
I0703 02:56:43.804046 31050 net.cpp:1851] res4a_branch2b_param_0(0.75) 
I0703 02:56:43.804050 31050 net.cpp:1851] res5a_branch2a_param_0(0.75) 
I0703 02:56:43.804054 31050 net.cpp:1851] res5a_branch2b_param_0(0.75) 
I0703 02:56:43.804059 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.01404e+06/2.69117e+06) 0.748
I0703 02:56:43.804211 31050 solver.cpp:473] Iteration 20000, Testing net (#0)
I0703 02:57:31.249930 31050 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.948467
I0703 02:57:31.250025 31050 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.999931
I0703 02:57:31.250032 31050 solver.cpp:546]     Test net output #2: loss = 0.126963 (* 1 = 0.126963 loss)
I0703 02:57:31.497972 31050 solver.cpp:290] Iteration 20000 (1.39571 iter/s, 71.6479s/100 iter), loss = 0.0165047
I0703 02:57:31.497995 31050 solver.cpp:309]     Train net output #0: loss = 0.0165046 (* 1 = 0.0165046 loss)
I0703 02:57:31.498003 31050 sgd_solver.cpp:106] Iteration 20000, lr = 1e-05
I0703 02:57:55.024128 31050 solver.cpp:290] Iteration 20100 (4.25071 iter/s, 23.5255s/100 iter), loss = 0.0452961
I0703 02:57:55.024152 31050 solver.cpp:309]     Train net output #0: loss = 0.045296 (* 1 = 0.045296 loss)
I0703 02:57:55.024159 31050 sgd_solver.cpp:106] Iteration 20100, lr = 1e-05
I0703 02:58:19.173615 31050 solver.cpp:290] Iteration 20200 (4.14099 iter/s, 24.1488s/100 iter), loss = 0.0228615
I0703 02:58:19.173728 31050 solver.cpp:309]     Train net output #0: loss = 0.0228615 (* 1 = 0.0228615 loss)
I0703 02:58:19.173738 31050 sgd_solver.cpp:106] Iteration 20200, lr = 1e-05
I0703 02:58:43.357830 31050 solver.cpp:290] Iteration 20300 (4.13506 iter/s, 24.1834s/100 iter), loss = 0.0144135
I0703 02:58:43.357856 31050 solver.cpp:309]     Train net output #0: loss = 0.0144134 (* 1 = 0.0144134 loss)
I0703 02:58:43.357862 31050 sgd_solver.cpp:106] Iteration 20300, lr = 1e-05
I0703 02:59:07.542026 31050 solver.cpp:290] Iteration 20400 (4.13505 iter/s, 24.1835s/100 iter), loss = 0.0410269
I0703 02:59:07.542153 31050 solver.cpp:309]     Train net output #0: loss = 0.0410269 (* 1 = 0.0410269 loss)
I0703 02:59:07.542165 31050 sgd_solver.cpp:106] Iteration 20400, lr = 1e-05
I0703 02:59:31.695586 31050 solver.cpp:290] Iteration 20500 (4.14031 iter/s, 24.1528s/100 iter), loss = 0.0543041
I0703 02:59:31.695610 31050 solver.cpp:309]     Train net output #0: loss = 0.054304 (* 1 = 0.054304 loss)
I0703 02:59:31.695616 31050 sgd_solver.cpp:106] Iteration 20500, lr = 1e-05
I0703 02:59:55.878149 31050 solver.cpp:290] Iteration 20600 (4.13533 iter/s, 24.1819s/100 iter), loss = 0.043004
I0703 02:59:55.878197 31050 solver.cpp:309]     Train net output #0: loss = 0.0430039 (* 1 = 0.0430039 loss)
I0703 02:59:55.878206 31050 sgd_solver.cpp:106] Iteration 20600, lr = 1e-05
I0703 03:00:20.048941 31050 solver.cpp:290] Iteration 20700 (4.13734 iter/s, 24.1701s/100 iter), loss = 0.0338639
I0703 03:00:20.048966 31050 solver.cpp:309]     Train net output #0: loss = 0.0338639 (* 1 = 0.0338639 loss)
I0703 03:00:20.048974 31050 sgd_solver.cpp:106] Iteration 20700, lr = 1e-05
I0703 03:00:44.215281 31050 solver.cpp:290] Iteration 20800 (4.1381 iter/s, 24.1657s/100 iter), loss = 0.0347515
I0703 03:00:44.215389 31050 solver.cpp:309]     Train net output #0: loss = 0.0347514 (* 1 = 0.0347514 loss)
I0703 03:00:44.215399 31050 sgd_solver.cpp:106] Iteration 20800, lr = 1e-05
I0703 03:01:08.489589 31050 solver.cpp:290] Iteration 20900 (4.11971 iter/s, 24.2735s/100 iter), loss = 0.0370786
I0703 03:01:08.489614 31050 solver.cpp:309]     Train net output #0: loss = 0.0370786 (* 1 = 0.0370786 loss)
I0703 03:01:08.489620 31050 sgd_solver.cpp:106] Iteration 20900, lr = 1e-05
I0703 03:01:32.464588 31050 solver.cpp:354] Sparsity after update:
I0703 03:01:32.479277 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 03:01:32.479307 31050 net.cpp:1851] conv1a_param_0(0.375) 
I0703 03:01:32.479324 31050 net.cpp:1851] conv1b_param_0(0.75) 
I0703 03:01:32.479327 31050 net.cpp:1851] ctx_conv1_param_0(0.75) 
I0703 03:01:32.479331 31050 net.cpp:1851] ctx_conv2_param_0(0.75) 
I0703 03:01:32.479333 31050 net.cpp:1851] ctx_conv3_param_0(0.75) 
I0703 03:01:32.479336 31050 net.cpp:1851] ctx_conv4_param_0(0.75) 
I0703 03:01:32.479339 31050 net.cpp:1851] ctx_final_param_0(0.00977) 
I0703 03:01:32.479342 31050 net.cpp:1851] out3a_param_0(0.75) 
I0703 03:01:32.479349 31050 net.cpp:1851] out5a_param_0(0.75) 
I0703 03:01:32.479352 31050 net.cpp:1851] res2a_branch2a_param_0(0.75) 
I0703 03:01:32.479356 31050 net.cpp:1851] res2a_branch2b_param_0(0.75) 
I0703 03:01:32.479359 31050 net.cpp:1851] res3a_branch2a_param_0(0.75) 
I0703 03:01:32.479362 31050 net.cpp:1851] res3a_branch2b_param_0(0.75) 
I0703 03:01:32.479365 31050 net.cpp:1851] res4a_branch2a_param_0(0.75) 
I0703 03:01:32.479368 31050 net.cpp:1851] res4a_branch2b_param_0(0.75) 
I0703 03:01:32.479372 31050 net.cpp:1851] res5a_branch2a_param_0(0.75) 
I0703 03:01:32.479374 31050 net.cpp:1851] res5a_branch2b_param_0(0.75) 
I0703 03:01:32.479378 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.01404e+06/2.69117e+06) 0.748
I0703 03:01:32.710271 31050 solver.cpp:290] Iteration 21000 (4.12882 iter/s, 24.22s/100 iter), loss = 0.0307623
I0703 03:01:32.710299 31050 solver.cpp:309]     Train net output #0: loss = 0.0307622 (* 1 = 0.0307622 loss)
I0703 03:01:32.710309 31050 sgd_solver.cpp:106] Iteration 21000, lr = 1e-05
I0703 03:01:56.875646 31050 solver.cpp:290] Iteration 21100 (4.13827 iter/s, 24.1647s/100 iter), loss = 0.0303104
I0703 03:01:56.875670 31050 solver.cpp:309]     Train net output #0: loss = 0.0303103 (* 1 = 0.0303103 loss)
I0703 03:01:56.875677 31050 sgd_solver.cpp:106] Iteration 21100, lr = 1e-05
I0703 03:02:21.030570 31050 solver.cpp:290] Iteration 21200 (4.14006 iter/s, 24.1543s/100 iter), loss = 0.0303785
I0703 03:02:21.030607 31050 solver.cpp:309]     Train net output #0: loss = 0.0303784 (* 1 = 0.0303784 loss)
I0703 03:02:21.030616 31050 sgd_solver.cpp:106] Iteration 21200, lr = 1e-05
I0703 03:02:45.202353 31050 solver.cpp:290] Iteration 21300 (4.13717 iter/s, 24.1711s/100 iter), loss = 0.0540421
I0703 03:02:45.202376 31050 solver.cpp:309]     Train net output #0: loss = 0.0540421 (* 1 = 0.0540421 loss)
I0703 03:02:45.202389 31050 sgd_solver.cpp:106] Iteration 21300, lr = 1e-05
I0703 03:03:09.355053 31050 solver.cpp:290] Iteration 21400 (4.14044 iter/s, 24.152s/100 iter), loss = 0.0288539
I0703 03:03:09.355166 31050 solver.cpp:309]     Train net output #0: loss = 0.0288539 (* 1 = 0.0288539 loss)
I0703 03:03:09.355175 31050 sgd_solver.cpp:106] Iteration 21400, lr = 1e-05
I0703 03:03:33.503053 31050 solver.cpp:290] Iteration 21500 (4.14126 iter/s, 24.1472s/100 iter), loss = 0.0497019
I0703 03:03:33.503079 31050 solver.cpp:309]     Train net output #0: loss = 0.0497018 (* 1 = 0.0497018 loss)
I0703 03:03:33.503087 31050 sgd_solver.cpp:106] Iteration 21500, lr = 1e-05
I0703 03:03:57.658751 31050 solver.cpp:290] Iteration 21600 (4.13992 iter/s, 24.155s/100 iter), loss = 0.0396138
I0703 03:03:57.658859 31050 solver.cpp:309]     Train net output #0: loss = 0.0396137 (* 1 = 0.0396137 loss)
I0703 03:03:57.658869 31050 sgd_solver.cpp:106] Iteration 21600, lr = 1e-05
I0703 03:04:21.848273 31050 solver.cpp:290] Iteration 21700 (4.13415 iter/s, 24.1888s/100 iter), loss = 0.0507819
I0703 03:04:21.848296 31050 solver.cpp:309]     Train net output #0: loss = 0.0507818 (* 1 = 0.0507818 loss)
I0703 03:04:21.848304 31050 sgd_solver.cpp:106] Iteration 21700, lr = 1e-05
I0703 03:04:46.034953 31050 solver.cpp:290] Iteration 21800 (4.13462 iter/s, 24.186s/100 iter), loss = 0.0599834
I0703 03:04:46.034994 31050 solver.cpp:309]     Train net output #0: loss = 0.0599833 (* 1 = 0.0599833 loss)
I0703 03:04:46.035006 31050 sgd_solver.cpp:106] Iteration 21800, lr = 1e-05
I0703 03:05:10.233281 31050 solver.cpp:290] Iteration 21900 (4.13263 iter/s, 24.1976s/100 iter), loss = 0.0238899
I0703 03:05:10.233306 31050 solver.cpp:309]     Train net output #0: loss = 0.0238898 (* 1 = 0.0238898 loss)
I0703 03:05:10.233314 31050 sgd_solver.cpp:106] Iteration 21900, lr = 1e-05
I0703 03:05:34.143856 31050 solver.cpp:354] Sparsity after update:
I0703 03:05:34.145685 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 03:05:34.145694 31050 net.cpp:1851] conv1a_param_0(0.375) 
I0703 03:05:34.145704 31050 net.cpp:1851] conv1b_param_0(0.75) 
I0703 03:05:34.145709 31050 net.cpp:1851] ctx_conv1_param_0(0.75) 
I0703 03:05:34.145711 31050 net.cpp:1851] ctx_conv2_param_0(0.75) 
I0703 03:05:34.145714 31050 net.cpp:1851] ctx_conv3_param_0(0.75) 
I0703 03:05:34.145714 31050 net.cpp:1851] ctx_conv4_param_0(0.75) 
I0703 03:05:34.145716 31050 net.cpp:1851] ctx_final_param_0(0.00977) 
I0703 03:05:34.145719 31050 net.cpp:1851] out3a_param_0(0.75) 
I0703 03:05:34.145720 31050 net.cpp:1851] out5a_param_0(0.75) 
I0703 03:05:34.145722 31050 net.cpp:1851] res2a_branch2a_param_0(0.75) 
I0703 03:05:34.145725 31050 net.cpp:1851] res2a_branch2b_param_0(0.75) 
I0703 03:05:34.145726 31050 net.cpp:1851] res3a_branch2a_param_0(0.75) 
I0703 03:05:34.145728 31050 net.cpp:1851] res3a_branch2b_param_0(0.75) 
I0703 03:05:34.145730 31050 net.cpp:1851] res4a_branch2a_param_0(0.75) 
I0703 03:05:34.145732 31050 net.cpp:1851] res4a_branch2b_param_0(0.75) 
I0703 03:05:34.145735 31050 net.cpp:1851] res5a_branch2a_param_0(0.75) 
I0703 03:05:34.145736 31050 net.cpp:1851] res5a_branch2b_param_0(0.75) 
I0703 03:05:34.145738 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.01404e+06/2.69117e+06) 0.748
I0703 03:05:34.145884 31050 solver.cpp:473] Iteration 22000, Testing net (#0)
I0703 03:06:23.633872 31050 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.949011
I0703 03:06:23.633961 31050 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.999913
I0703 03:06:23.633968 31050 solver.cpp:546]     Test net output #2: loss = 0.131509 (* 1 = 0.131509 loss)
I0703 03:06:23.877291 31050 solver.cpp:290] Iteration 22000 (1.35792 iter/s, 73.642s/100 iter), loss = 0.0275534
I0703 03:06:23.877316 31050 solver.cpp:309]     Train net output #0: loss = 0.0275533 (* 1 = 0.0275533 loss)
I0703 03:06:23.877323 31050 sgd_solver.cpp:106] Iteration 22000, lr = 1e-05
I0703 03:06:47.378621 31050 solver.cpp:290] Iteration 22100 (4.2552 iter/s, 23.5007s/100 iter), loss = 0.0423177
I0703 03:06:47.378644 31050 solver.cpp:309]     Train net output #0: loss = 0.0423176 (* 1 = 0.0423176 loss)
I0703 03:06:47.378651 31050 sgd_solver.cpp:106] Iteration 22100, lr = 1e-05
I0703 03:07:11.563503 31050 solver.cpp:290] Iteration 22200 (4.13493 iter/s, 24.1842s/100 iter), loss = 0.0474546
I0703 03:07:11.563611 31050 solver.cpp:309]     Train net output #0: loss = 0.0474546 (* 1 = 0.0474546 loss)
I0703 03:07:11.563621 31050 sgd_solver.cpp:106] Iteration 22200, lr = 1e-05
I0703 03:07:35.713687 31050 solver.cpp:290] Iteration 22300 (4.14089 iter/s, 24.1494s/100 iter), loss = 0.0264324
I0703 03:07:35.713711 31050 solver.cpp:309]     Train net output #0: loss = 0.0264324 (* 1 = 0.0264324 loss)
I0703 03:07:35.713717 31050 sgd_solver.cpp:106] Iteration 22300, lr = 1e-05
I0703 03:07:59.903123 31050 solver.cpp:290] Iteration 22400 (4.13415 iter/s, 24.1888s/100 iter), loss = 0.0331791
I0703 03:07:59.903236 31050 solver.cpp:309]     Train net output #0: loss = 0.033179 (* 1 = 0.033179 loss)
I0703 03:07:59.903251 31050 sgd_solver.cpp:106] Iteration 22400, lr = 1e-05
I0703 03:08:24.090335 31050 solver.cpp:290] Iteration 22500 (4.13455 iter/s, 24.1865s/100 iter), loss = 0.0348935
I0703 03:08:24.090358 31050 solver.cpp:309]     Train net output #0: loss = 0.0348934 (* 1 = 0.0348934 loss)
I0703 03:08:24.090365 31050 sgd_solver.cpp:106] Iteration 22500, lr = 1e-05
I0703 03:08:48.293485 31050 solver.cpp:290] Iteration 22600 (4.13181 iter/s, 24.2025s/100 iter), loss = 0.0357021
I0703 03:08:48.293596 31050 solver.cpp:309]     Train net output #0: loss = 0.035702 (* 1 = 0.035702 loss)
I0703 03:08:48.293611 31050 sgd_solver.cpp:106] Iteration 22600, lr = 1e-05
I0703 03:09:12.462152 31050 solver.cpp:290] Iteration 22700 (4.13772 iter/s, 24.1679s/100 iter), loss = 0.0304754
I0703 03:09:12.462173 31050 solver.cpp:309]     Train net output #0: loss = 0.0304753 (* 1 = 0.0304753 loss)
I0703 03:09:12.462180 31050 sgd_solver.cpp:106] Iteration 22700, lr = 1e-05
I0703 03:09:36.969413 31050 solver.cpp:290] Iteration 22800 (4.08054 iter/s, 24.5066s/100 iter), loss = 0.0259729
I0703 03:09:36.969542 31050 solver.cpp:309]     Train net output #0: loss = 0.0259728 (* 1 = 0.0259728 loss)
I0703 03:09:36.969552 31050 sgd_solver.cpp:106] Iteration 22800, lr = 1e-05
I0703 03:10:01.162405 31050 solver.cpp:290] Iteration 22900 (4.13356 iter/s, 24.1922s/100 iter), loss = 0.0547392
I0703 03:10:01.162428 31050 solver.cpp:309]     Train net output #0: loss = 0.0547391 (* 1 = 0.0547391 loss)
I0703 03:10:01.162436 31050 sgd_solver.cpp:106] Iteration 22900, lr = 1e-05
I0703 03:10:25.118348 31050 solver.cpp:354] Sparsity after update:
I0703 03:10:25.165635 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 03:10:25.165652 31050 net.cpp:1851] conv1a_param_0(0.375) 
I0703 03:10:25.165660 31050 net.cpp:1851] conv1b_param_0(0.75) 
I0703 03:10:25.165663 31050 net.cpp:1851] ctx_conv1_param_0(0.75) 
I0703 03:10:25.165664 31050 net.cpp:1851] ctx_conv2_param_0(0.75) 
I0703 03:10:25.165666 31050 net.cpp:1851] ctx_conv3_param_0(0.75) 
I0703 03:10:25.165668 31050 net.cpp:1851] ctx_conv4_param_0(0.75) 
I0703 03:10:25.165670 31050 net.cpp:1851] ctx_final_param_0(0.00977) 
I0703 03:10:25.165673 31050 net.cpp:1851] out3a_param_0(0.75) 
I0703 03:10:25.165674 31050 net.cpp:1851] out5a_param_0(0.75) 
I0703 03:10:25.165676 31050 net.cpp:1851] res2a_branch2a_param_0(0.75) 
I0703 03:10:25.165678 31050 net.cpp:1851] res2a_branch2b_param_0(0.75) 
I0703 03:10:25.165680 31050 net.cpp:1851] res3a_branch2a_param_0(0.75) 
I0703 03:10:25.165683 31050 net.cpp:1851] res3a_branch2b_param_0(0.75) 
I0703 03:10:25.165684 31050 net.cpp:1851] res4a_branch2a_param_0(0.75) 
I0703 03:10:25.165686 31050 net.cpp:1851] res4a_branch2b_param_0(0.75) 
I0703 03:10:25.165688 31050 net.cpp:1851] res5a_branch2a_param_0(0.75) 
I0703 03:10:25.165689 31050 net.cpp:1851] res5a_branch2b_param_0(0.75) 
I0703 03:10:25.165691 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.01404e+06/2.69117e+06) 0.748
I0703 03:10:25.396705 31050 solver.cpp:290] Iteration 23000 (4.1265 iter/s, 24.2336s/100 iter), loss = 0.0220846
I0703 03:10:25.396729 31050 solver.cpp:309]     Train net output #0: loss = 0.0220845 (* 1 = 0.0220845 loss)
I0703 03:10:25.396736 31050 sgd_solver.cpp:106] Iteration 23000, lr = 1e-05
I0703 03:10:49.569432 31050 solver.cpp:290] Iteration 23100 (4.13701 iter/s, 24.1721s/100 iter), loss = 0.0300033
I0703 03:10:49.569454 31050 solver.cpp:309]     Train net output #0: loss = 0.0300033 (* 1 = 0.0300033 loss)
I0703 03:10:49.569461 31050 sgd_solver.cpp:106] Iteration 23100, lr = 1e-05
I0703 03:11:13.732787 31050 solver.cpp:290] Iteration 23200 (4.13861 iter/s, 24.1627s/100 iter), loss = 0.0402656
I0703 03:11:13.732903 31050 solver.cpp:309]     Train net output #0: loss = 0.0402655 (* 1 = 0.0402655 loss)
I0703 03:11:13.732911 31050 sgd_solver.cpp:106] Iteration 23200, lr = 1e-05
I0703 03:11:37.999940 31050 solver.cpp:290] Iteration 23300 (4.12093 iter/s, 24.2664s/100 iter), loss = 0.0337778
I0703 03:11:37.999965 31050 solver.cpp:309]     Train net output #0: loss = 0.0337777 (* 1 = 0.0337777 loss)
I0703 03:11:37.999974 31050 sgd_solver.cpp:106] Iteration 23300, lr = 1e-05
I0703 03:12:02.238126 31050 solver.cpp:290] Iteration 23400 (4.12584 iter/s, 24.2375s/100 iter), loss = 0.0217381
I0703 03:12:02.238241 31050 solver.cpp:309]     Train net output #0: loss = 0.021738 (* 1 = 0.021738 loss)
I0703 03:12:02.238251 31050 sgd_solver.cpp:106] Iteration 23400, lr = 1e-05
I0703 03:12:26.412237 31050 solver.cpp:290] Iteration 23500 (4.13679 iter/s, 24.1734s/100 iter), loss = 0.0441397
I0703 03:12:26.412261 31050 solver.cpp:309]     Train net output #0: loss = 0.0441397 (* 1 = 0.0441397 loss)
I0703 03:12:26.412268 31050 sgd_solver.cpp:106] Iteration 23500, lr = 1e-05
I0703 03:12:50.573282 31050 solver.cpp:290] Iteration 23600 (4.13901 iter/s, 24.1604s/100 iter), loss = 0.026131
I0703 03:12:50.573333 31050 solver.cpp:309]     Train net output #0: loss = 0.0261309 (* 1 = 0.0261309 loss)
I0703 03:12:50.573343 31050 sgd_solver.cpp:106] Iteration 23600, lr = 1e-05
I0703 03:13:14.922617 31050 solver.cpp:290] Iteration 23700 (4.10701 iter/s, 24.3486s/100 iter), loss = 0.0239946
I0703 03:13:14.922641 31050 solver.cpp:309]     Train net output #0: loss = 0.0239945 (* 1 = 0.0239945 loss)
I0703 03:13:14.922648 31050 sgd_solver.cpp:106] Iteration 23700, lr = 1e-05
I0703 03:13:39.105244 31050 solver.cpp:290] Iteration 23800 (4.13532 iter/s, 24.182s/100 iter), loss = 0.0279383
I0703 03:13:39.105351 31050 solver.cpp:309]     Train net output #0: loss = 0.0279382 (* 1 = 0.0279382 loss)
I0703 03:13:39.105361 31050 sgd_solver.cpp:106] Iteration 23800, lr = 1e-05
I0703 03:14:03.277520 31050 solver.cpp:290] Iteration 23900 (4.1371 iter/s, 24.1715s/100 iter), loss = 0.0501442
I0703 03:14:03.277546 31050 solver.cpp:309]     Train net output #0: loss = 0.0501442 (* 1 = 0.0501442 loss)
I0703 03:14:03.277554 31050 sgd_solver.cpp:106] Iteration 23900, lr = 1e-05
I0703 03:14:27.246575 31050 solver.cpp:354] Sparsity after update:
I0703 03:14:27.248405 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 03:14:27.248414 31050 net.cpp:1851] conv1a_param_0(0.375) 
I0703 03:14:27.248420 31050 net.cpp:1851] conv1b_param_0(0.75) 
I0703 03:14:27.248422 31050 net.cpp:1851] ctx_conv1_param_0(0.75) 
I0703 03:14:27.248425 31050 net.cpp:1851] ctx_conv2_param_0(0.75) 
I0703 03:14:27.248426 31050 net.cpp:1851] ctx_conv3_param_0(0.75) 
I0703 03:14:27.248428 31050 net.cpp:1851] ctx_conv4_param_0(0.75) 
I0703 03:14:27.248430 31050 net.cpp:1851] ctx_final_param_0(0.00977) 
I0703 03:14:27.248432 31050 net.cpp:1851] out3a_param_0(0.75) 
I0703 03:14:27.248435 31050 net.cpp:1851] out5a_param_0(0.75) 
I0703 03:14:27.248436 31050 net.cpp:1851] res2a_branch2a_param_0(0.75) 
I0703 03:14:27.248438 31050 net.cpp:1851] res2a_branch2b_param_0(0.75) 
I0703 03:14:27.248440 31050 net.cpp:1851] res3a_branch2a_param_0(0.75) 
I0703 03:14:27.248442 31050 net.cpp:1851] res3a_branch2b_param_0(0.75) 
I0703 03:14:27.248445 31050 net.cpp:1851] res4a_branch2a_param_0(0.75) 
I0703 03:14:27.248445 31050 net.cpp:1851] res4a_branch2b_param_0(0.75) 
I0703 03:14:27.248447 31050 net.cpp:1851] res5a_branch2a_param_0(0.75) 
I0703 03:14:27.248450 31050 net.cpp:1851] res5a_branch2b_param_0(0.75) 
I0703 03:14:27.248451 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.01404e+06/2.69117e+06) 0.748
I0703 03:14:27.248589 31050 solver.cpp:473] Iteration 24000, Testing net (#0)
I0703 03:15:14.734251 31050 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.949682
I0703 03:15:14.734346 31050 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.999885
I0703 03:15:14.734352 31050 solver.cpp:546]     Test net output #2: loss = 0.132505 (* 1 = 0.132505 loss)
I0703 03:15:15.004395 31050 solver.cpp:290] Iteration 24000 (1.39422 iter/s, 71.7249s/100 iter), loss = 0.0391522
I0703 03:15:15.004407 31166 sgd_solver.cpp:46] MultiStep Status: Iteration 24000, step = 1
I0703 03:15:15.004416 31050 solver.cpp:309]     Train net output #0: loss = 0.0391521 (* 1 = 0.0391521 loss)
I0703 03:15:15.004422 31050 sgd_solver.cpp:46] MultiStep Status: Iteration 24000, step = 1
I0703 03:15:15.004426 31050 sgd_solver.cpp:106] Iteration 24000, lr = 1e-06
I0703 03:15:38.463296 31050 solver.cpp:290] Iteration 24100 (4.26289 iter/s, 23.4582s/100 iter), loss = 0.0239242
I0703 03:15:38.463323 31050 solver.cpp:309]     Train net output #0: loss = 0.0239241 (* 1 = 0.0239241 loss)
I0703 03:15:38.463333 31050 sgd_solver.cpp:106] Iteration 24100, lr = 1e-06
I0703 03:16:02.611448 31050 solver.cpp:290] Iteration 24200 (4.14122 iter/s, 24.1475s/100 iter), loss = 0.0363424
I0703 03:16:02.611500 31050 solver.cpp:309]     Train net output #0: loss = 0.0363424 (* 1 = 0.0363424 loss)
I0703 03:16:02.611508 31050 sgd_solver.cpp:106] Iteration 24200, lr = 1e-06
I0703 03:16:26.858849 31050 solver.cpp:290] Iteration 24300 (4.12427 iter/s, 24.2467s/100 iter), loss = 0.0253505
I0703 03:16:26.858875 31050 solver.cpp:309]     Train net output #0: loss = 0.0253504 (* 1 = 0.0253504 loss)
I0703 03:16:26.858881 31050 sgd_solver.cpp:106] Iteration 24300, lr = 1e-06
I0703 03:16:51.550264 31050 solver.cpp:290] Iteration 24400 (4.0501 iter/s, 24.6907s/100 iter), loss = 0.0240232
I0703 03:16:51.550303 31050 solver.cpp:309]     Train net output #0: loss = 0.0240231 (* 1 = 0.0240231 loss)
I0703 03:16:51.550310 31050 sgd_solver.cpp:106] Iteration 24400, lr = 1e-06
I0703 03:17:15.719720 31050 solver.cpp:290] Iteration 24500 (4.13757 iter/s, 24.1688s/100 iter), loss = 0.0346604
I0703 03:17:15.719745 31050 solver.cpp:309]     Train net output #0: loss = 0.0346603 (* 1 = 0.0346603 loss)
I0703 03:17:15.719753 31050 sgd_solver.cpp:106] Iteration 24500, lr = 1e-06
I0703 03:17:39.937722 31050 solver.cpp:290] Iteration 24600 (4.12927 iter/s, 24.2173s/100 iter), loss = 0.0306895
I0703 03:17:39.937831 31050 solver.cpp:309]     Train net output #0: loss = 0.0306894 (* 1 = 0.0306894 loss)
I0703 03:17:39.937842 31050 sgd_solver.cpp:106] Iteration 24600, lr = 1e-06
I0703 03:18:04.156302 31050 solver.cpp:290] Iteration 24700 (4.12919 iter/s, 24.2178s/100 iter), loss = 0.0417974
I0703 03:18:04.156327 31050 solver.cpp:309]     Train net output #0: loss = 0.0417973 (* 1 = 0.0417973 loss)
I0703 03:18:04.156332 31050 sgd_solver.cpp:106] Iteration 24700, lr = 1e-06
I0703 03:18:28.319452 31050 solver.cpp:290] Iteration 24800 (4.13865 iter/s, 24.1625s/100 iter), loss = 0.0247898
I0703 03:18:28.319504 31050 solver.cpp:309]     Train net output #0: loss = 0.0247897 (* 1 = 0.0247897 loss)
I0703 03:18:28.319511 31050 sgd_solver.cpp:106] Iteration 24800, lr = 1e-06
I0703 03:18:52.498226 31050 solver.cpp:290] Iteration 24900 (4.13598 iter/s, 24.1781s/100 iter), loss = 0.0319622
I0703 03:18:52.498251 31050 solver.cpp:309]     Train net output #0: loss = 0.0319621 (* 1 = 0.0319621 loss)
I0703 03:18:52.498261 31050 sgd_solver.cpp:106] Iteration 24900, lr = 1e-06
I0703 03:19:16.449127 31050 solver.cpp:354] Sparsity after update:
I0703 03:19:16.496312 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 03:19:16.496330 31050 net.cpp:1851] conv1a_param_0(0.375) 
I0703 03:19:16.496337 31050 net.cpp:1851] conv1b_param_0(0.75) 
I0703 03:19:16.496340 31050 net.cpp:1851] ctx_conv1_param_0(0.75) 
I0703 03:19:16.496342 31050 net.cpp:1851] ctx_conv2_param_0(0.75) 
I0703 03:19:16.496345 31050 net.cpp:1851] ctx_conv3_param_0(0.75) 
I0703 03:19:16.496346 31050 net.cpp:1851] ctx_conv4_param_0(0.75) 
I0703 03:19:16.496347 31050 net.cpp:1851] ctx_final_param_0(0.00977) 
I0703 03:19:16.496350 31050 net.cpp:1851] out3a_param_0(0.75) 
I0703 03:19:16.496351 31050 net.cpp:1851] out5a_param_0(0.75) 
I0703 03:19:16.496353 31050 net.cpp:1851] res2a_branch2a_param_0(0.75) 
I0703 03:19:16.496356 31050 net.cpp:1851] res2a_branch2b_param_0(0.75) 
I0703 03:19:16.496357 31050 net.cpp:1851] res3a_branch2a_param_0(0.75) 
I0703 03:19:16.496359 31050 net.cpp:1851] res3a_branch2b_param_0(0.75) 
I0703 03:19:16.496361 31050 net.cpp:1851] res4a_branch2a_param_0(0.75) 
I0703 03:19:16.496363 31050 net.cpp:1851] res4a_branch2b_param_0(0.75) 
I0703 03:19:16.496366 31050 net.cpp:1851] res5a_branch2a_param_0(0.75) 
I0703 03:19:16.496367 31050 net.cpp:1851] res5a_branch2b_param_0(0.75) 
I0703 03:19:16.496369 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.01404e+06/2.69117e+06) 0.748
I0703 03:19:16.727283 31050 solver.cpp:290] Iteration 25000 (4.12739 iter/s, 24.2284s/100 iter), loss = 0.052638
I0703 03:19:16.727305 31050 solver.cpp:309]     Train net output #0: loss = 0.0526379 (* 1 = 0.0526379 loss)
I0703 03:19:16.727313 31050 sgd_solver.cpp:106] Iteration 25000, lr = 1e-06
I0703 03:19:40.910704 31050 solver.cpp:290] Iteration 25100 (4.13518 iter/s, 24.1828s/100 iter), loss = 0.017421
I0703 03:19:40.910728 31050 solver.cpp:309]     Train net output #0: loss = 0.017421 (* 1 = 0.017421 loss)
I0703 03:19:40.910735 31050 sgd_solver.cpp:106] Iteration 25100, lr = 1e-06
I0703 03:20:05.057654 31050 solver.cpp:290] Iteration 25200 (4.14142 iter/s, 24.1463s/100 iter), loss = 0.0360861
I0703 03:20:05.057761 31050 solver.cpp:309]     Train net output #0: loss = 0.036086 (* 1 = 0.036086 loss)
I0703 03:20:05.057771 31050 sgd_solver.cpp:106] Iteration 25200, lr = 1e-06
I0703 03:20:29.270552 31050 solver.cpp:290] Iteration 25300 (4.13016 iter/s, 24.2121s/100 iter), loss = 0.0467008
I0703 03:20:29.270575 31050 solver.cpp:309]     Train net output #0: loss = 0.0467008 (* 1 = 0.0467008 loss)
I0703 03:20:29.270581 31050 sgd_solver.cpp:106] Iteration 25300, lr = 1e-06
I0703 03:20:53.506413 31050 solver.cpp:290] Iteration 25400 (4.12623 iter/s, 24.2352s/100 iter), loss = 0.0185537
I0703 03:20:53.506525 31050 solver.cpp:309]     Train net output #0: loss = 0.0185536 (* 1 = 0.0185536 loss)
I0703 03:20:53.506536 31050 sgd_solver.cpp:106] Iteration 25400, lr = 1e-06
I0703 03:21:17.685900 31050 solver.cpp:290] Iteration 25500 (4.13587 iter/s, 24.1787s/100 iter), loss = 0.0366022
I0703 03:21:17.685923 31050 solver.cpp:309]     Train net output #0: loss = 0.0366021 (* 1 = 0.0366021 loss)
I0703 03:21:17.685930 31050 sgd_solver.cpp:106] Iteration 25500, lr = 1e-06
I0703 03:21:41.901273 31050 solver.cpp:290] Iteration 25600 (4.12972 iter/s, 24.2147s/100 iter), loss = 0.0324306
I0703 03:21:41.901309 31050 solver.cpp:309]     Train net output #0: loss = 0.0324305 (* 1 = 0.0324305 loss)
I0703 03:21:41.901316 31050 sgd_solver.cpp:106] Iteration 25600, lr = 1e-06
I0703 03:22:06.074038 31050 solver.cpp:290] Iteration 25700 (4.137 iter/s, 24.1721s/100 iter), loss = 0.0406754
I0703 03:22:06.074062 31050 solver.cpp:309]     Train net output #0: loss = 0.0406753 (* 1 = 0.0406753 loss)
I0703 03:22:06.074069 31050 sgd_solver.cpp:106] Iteration 25700, lr = 1e-06
I0703 03:22:30.278983 31050 solver.cpp:290] Iteration 25800 (4.1315 iter/s, 24.2043s/100 iter), loss = 0.0403722
I0703 03:22:30.279037 31050 solver.cpp:309]     Train net output #0: loss = 0.0403721 (* 1 = 0.0403721 loss)
I0703 03:22:30.279047 31050 sgd_solver.cpp:106] Iteration 25800, lr = 1e-06
I0703 03:22:54.439364 31050 solver.cpp:290] Iteration 25900 (4.13913 iter/s, 24.1597s/100 iter), loss = 0.0327967
I0703 03:22:54.439388 31050 solver.cpp:309]     Train net output #0: loss = 0.0327966 (* 1 = 0.0327966 loss)
I0703 03:22:54.439399 31050 sgd_solver.cpp:106] Iteration 25900, lr = 1e-06
I0703 03:23:18.367319 31050 solver.cpp:354] Sparsity after update:
I0703 03:23:18.369148 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 03:23:18.369155 31050 net.cpp:1851] conv1a_param_0(0.375) 
I0703 03:23:18.369163 31050 net.cpp:1851] conv1b_param_0(0.75) 
I0703 03:23:18.369165 31050 net.cpp:1851] ctx_conv1_param_0(0.75) 
I0703 03:23:18.369168 31050 net.cpp:1851] ctx_conv2_param_0(0.75) 
I0703 03:23:18.369169 31050 net.cpp:1851] ctx_conv3_param_0(0.75) 
I0703 03:23:18.369171 31050 net.cpp:1851] ctx_conv4_param_0(0.75) 
I0703 03:23:18.369174 31050 net.cpp:1851] ctx_final_param_0(0.00977) 
I0703 03:23:18.369177 31050 net.cpp:1851] out3a_param_0(0.75) 
I0703 03:23:18.369180 31050 net.cpp:1851] out5a_param_0(0.75) 
I0703 03:23:18.369184 31050 net.cpp:1851] res2a_branch2a_param_0(0.75) 
I0703 03:23:18.369187 31050 net.cpp:1851] res2a_branch2b_param_0(0.75) 
I0703 03:23:18.369191 31050 net.cpp:1851] res3a_branch2a_param_0(0.75) 
I0703 03:23:18.369194 31050 net.cpp:1851] res3a_branch2b_param_0(0.75) 
I0703 03:23:18.369197 31050 net.cpp:1851] res4a_branch2a_param_0(0.75) 
I0703 03:23:18.369201 31050 net.cpp:1851] res4a_branch2b_param_0(0.75) 
I0703 03:23:18.369204 31050 net.cpp:1851] res5a_branch2a_param_0(0.75) 
I0703 03:23:18.369207 31050 net.cpp:1851] res5a_branch2b_param_0(0.75) 
I0703 03:23:18.369211 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.01404e+06/2.69117e+06) 0.748
I0703 03:23:18.369352 31050 solver.cpp:473] Iteration 26000, Testing net (#0)
I0703 03:24:05.880182 31050 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.949922
I0703 03:24:05.880262 31050 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.999886
I0703 03:24:05.880270 31050 solver.cpp:546]     Test net output #2: loss = 0.134294 (* 1 = 0.134294 loss)
I0703 03:24:06.124701 31050 solver.cpp:290] Iteration 26000 (1.39502 iter/s, 71.6834s/100 iter), loss = 0.0258299
I0703 03:24:06.124724 31050 solver.cpp:309]     Train net output #0: loss = 0.0258298 (* 1 = 0.0258298 loss)
I0703 03:24:06.124732 31050 sgd_solver.cpp:106] Iteration 26000, lr = 1e-06
I0703 03:24:29.681589 31050 solver.cpp:290] Iteration 26100 (4.24516 iter/s, 23.5562s/100 iter), loss = 0.0220823
I0703 03:24:29.681613 31050 solver.cpp:309]     Train net output #0: loss = 0.0220822 (* 1 = 0.0220822 loss)
I0703 03:24:29.681619 31050 sgd_solver.cpp:106] Iteration 26100, lr = 1e-06
I0703 03:24:53.836769 31050 solver.cpp:290] Iteration 26200 (4.14001 iter/s, 24.1545s/100 iter), loss = 0.0351768
I0703 03:24:53.836875 31050 solver.cpp:309]     Train net output #0: loss = 0.0351767 (* 1 = 0.0351767 loss)
I0703 03:24:53.836885 31050 sgd_solver.cpp:106] Iteration 26200, lr = 1e-06
I0703 03:25:18.015760 31050 solver.cpp:290] Iteration 26300 (4.13595 iter/s, 24.1782s/100 iter), loss = 0.0512115
I0703 03:25:18.015787 31050 solver.cpp:309]     Train net output #0: loss = 0.0512114 (* 1 = 0.0512114 loss)
I0703 03:25:18.015797 31050 sgd_solver.cpp:106] Iteration 26300, lr = 1e-06
I0703 03:25:42.228476 31050 solver.cpp:290] Iteration 26400 (4.13018 iter/s, 24.212s/100 iter), loss = 0.0236228
I0703 03:25:42.228582 31050 solver.cpp:309]     Train net output #0: loss = 0.0236227 (* 1 = 0.0236227 loss)
I0703 03:25:42.228592 31050 sgd_solver.cpp:106] Iteration 26400, lr = 1e-06
I0703 03:26:06.927119 31050 solver.cpp:290] Iteration 26500 (4.04893 iter/s, 24.6979s/100 iter), loss = 0.0396236
I0703 03:26:06.927201 31050 solver.cpp:309]     Train net output #0: loss = 0.0396235 (* 1 = 0.0396235 loss)
I0703 03:26:06.927224 31050 sgd_solver.cpp:106] Iteration 26500, lr = 1e-06
I0703 03:26:31.345096 31050 solver.cpp:290] Iteration 26600 (4.09547 iter/s, 24.4172s/100 iter), loss = 0.0393066
I0703 03:26:31.345213 31050 solver.cpp:309]     Train net output #0: loss = 0.0393065 (* 1 = 0.0393065 loss)
I0703 03:26:31.345223 31050 sgd_solver.cpp:106] Iteration 26600, lr = 1e-06
I0703 03:26:55.522601 31050 solver.cpp:290] Iteration 26700 (4.13621 iter/s, 24.1767s/100 iter), loss = 0.0229388
I0703 03:26:55.522625 31050 solver.cpp:309]     Train net output #0: loss = 0.0229387 (* 1 = 0.0229387 loss)
I0703 03:26:55.522632 31050 sgd_solver.cpp:106] Iteration 26700, lr = 1e-06
I0703 03:27:19.771209 31050 solver.cpp:290] Iteration 26800 (4.12406 iter/s, 24.2479s/100 iter), loss = 0.040493
I0703 03:27:19.771329 31050 solver.cpp:309]     Train net output #0: loss = 0.0404929 (* 1 = 0.0404929 loss)
I0703 03:27:19.771339 31050 sgd_solver.cpp:106] Iteration 26800, lr = 1e-06
I0703 03:27:43.948935 31050 solver.cpp:290] Iteration 26900 (4.13617 iter/s, 24.177s/100 iter), loss = 0.0385041
I0703 03:27:43.948958 31050 solver.cpp:309]     Train net output #0: loss = 0.038504 (* 1 = 0.038504 loss)
I0703 03:27:43.948966 31050 sgd_solver.cpp:106] Iteration 26900, lr = 1e-06
I0703 03:28:07.880100 31050 solver.cpp:354] Sparsity after update:
I0703 03:28:07.933230 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 03:28:07.933246 31050 net.cpp:1851] conv1a_param_0(0.375) 
I0703 03:28:07.933254 31050 net.cpp:1851] conv1b_param_0(0.75) 
I0703 03:28:07.933256 31050 net.cpp:1851] ctx_conv1_param_0(0.75) 
I0703 03:28:07.933259 31050 net.cpp:1851] ctx_conv2_param_0(0.75) 
I0703 03:28:07.933260 31050 net.cpp:1851] ctx_conv3_param_0(0.75) 
I0703 03:28:07.933262 31050 net.cpp:1851] ctx_conv4_param_0(0.75) 
I0703 03:28:07.933264 31050 net.cpp:1851] ctx_final_param_0(0.00977) 
I0703 03:28:07.933266 31050 net.cpp:1851] out3a_param_0(0.75) 
I0703 03:28:07.933269 31050 net.cpp:1851] out5a_param_0(0.75) 
I0703 03:28:07.933270 31050 net.cpp:1851] res2a_branch2a_param_0(0.75) 
I0703 03:28:07.933272 31050 net.cpp:1851] res2a_branch2b_param_0(0.75) 
I0703 03:28:07.933274 31050 net.cpp:1851] res3a_branch2a_param_0(0.75) 
I0703 03:28:07.933276 31050 net.cpp:1851] res3a_branch2b_param_0(0.75) 
I0703 03:28:07.933279 31050 net.cpp:1851] res4a_branch2a_param_0(0.75) 
I0703 03:28:07.933280 31050 net.cpp:1851] res4a_branch2b_param_0(0.75) 
I0703 03:28:07.933284 31050 net.cpp:1851] res5a_branch2a_param_0(0.75) 
I0703 03:28:07.933286 31050 net.cpp:1851] res5a_branch2b_param_0(0.75) 
I0703 03:28:07.933290 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.01404e+06/2.69117e+06) 0.748
I0703 03:28:08.165176 31050 solver.cpp:290] Iteration 27000 (4.12957 iter/s, 24.2156s/100 iter), loss = 0.0343487
I0703 03:28:08.165204 31050 solver.cpp:309]     Train net output #0: loss = 0.0343486 (* 1 = 0.0343486 loss)
I0703 03:28:08.165210 31050 sgd_solver.cpp:106] Iteration 27000, lr = 1e-06
I0703 03:28:32.328593 31050 solver.cpp:290] Iteration 27100 (4.1386 iter/s, 24.1627s/100 iter), loss = 0.0350823
I0703 03:28:32.328618 31050 solver.cpp:309]     Train net output #0: loss = 0.0350822 (* 1 = 0.0350822 loss)
I0703 03:28:32.328624 31050 sgd_solver.cpp:106] Iteration 27100, lr = 1e-06
I0703 03:28:56.512454 31050 solver.cpp:290] Iteration 27200 (4.1351 iter/s, 24.1832s/100 iter), loss = 0.0316266
I0703 03:28:56.512567 31050 solver.cpp:309]     Train net output #0: loss = 0.0316265 (* 1 = 0.0316265 loss)
I0703 03:28:56.512576 31050 sgd_solver.cpp:106] Iteration 27200, lr = 1e-06
I0703 03:29:20.784421 31050 solver.cpp:290] Iteration 27300 (4.12011 iter/s, 24.2712s/100 iter), loss = 0.0281622
I0703 03:29:20.784443 31050 solver.cpp:309]     Train net output #0: loss = 0.0281621 (* 1 = 0.0281621 loss)
I0703 03:29:20.784451 31050 sgd_solver.cpp:106] Iteration 27300, lr = 1e-06
I0703 03:29:45.447849 31050 solver.cpp:290] Iteration 27400 (4.0547 iter/s, 24.6627s/100 iter), loss = 0.0337269
I0703 03:29:45.448129 31050 solver.cpp:309]     Train net output #0: loss = 0.0337268 (* 1 = 0.0337268 loss)
I0703 03:29:45.448139 31050 sgd_solver.cpp:106] Iteration 27400, lr = 1e-06
I0703 03:30:09.667512 31050 solver.cpp:290] Iteration 27500 (4.12903 iter/s, 24.2187s/100 iter), loss = 0.0464873
I0703 03:30:09.667536 31050 solver.cpp:309]     Train net output #0: loss = 0.0464872 (* 1 = 0.0464872 loss)
I0703 03:30:09.667543 31050 sgd_solver.cpp:106] Iteration 27500, lr = 1e-06
I0703 03:30:33.862308 31050 solver.cpp:290] Iteration 27600 (4.13323 iter/s, 24.1941s/100 iter), loss = 0.0331539
I0703 03:30:33.862426 31050 solver.cpp:309]     Train net output #0: loss = 0.0331538 (* 1 = 0.0331538 loss)
I0703 03:30:33.862437 31050 sgd_solver.cpp:106] Iteration 27600, lr = 1e-06
I0703 03:30:58.025126 31050 solver.cpp:290] Iteration 27700 (4.13872 iter/s, 24.1621s/100 iter), loss = 0.0314248
I0703 03:30:58.025149 31050 solver.cpp:309]     Train net output #0: loss = 0.0314247 (* 1 = 0.0314247 loss)
I0703 03:30:58.025156 31050 sgd_solver.cpp:106] Iteration 27700, lr = 1e-06
I0703 03:31:22.208746 31050 solver.cpp:290] Iteration 27800 (4.13514 iter/s, 24.183s/100 iter), loss = 0.029132
I0703 03:31:22.208854 31050 solver.cpp:309]     Train net output #0: loss = 0.0291318 (* 1 = 0.0291318 loss)
I0703 03:31:22.208865 31050 sgd_solver.cpp:106] Iteration 27800, lr = 1e-06
I0703 03:31:46.463614 31050 solver.cpp:290] Iteration 27900 (4.12301 iter/s, 24.2541s/100 iter), loss = 0.0386961
I0703 03:31:46.463639 31050 solver.cpp:309]     Train net output #0: loss = 0.0386959 (* 1 = 0.0386959 loss)
I0703 03:31:46.463646 31050 sgd_solver.cpp:106] Iteration 27900, lr = 1e-06
I0703 03:32:10.388208 31050 solver.cpp:354] Sparsity after update:
I0703 03:32:10.390059 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 03:32:10.390066 31050 net.cpp:1851] conv1a_param_0(0.375) 
I0703 03:32:10.390074 31050 net.cpp:1851] conv1b_param_0(0.75) 
I0703 03:32:10.390075 31050 net.cpp:1851] ctx_conv1_param_0(0.75) 
I0703 03:32:10.390077 31050 net.cpp:1851] ctx_conv2_param_0(0.75) 
I0703 03:32:10.390079 31050 net.cpp:1851] ctx_conv3_param_0(0.75) 
I0703 03:32:10.390081 31050 net.cpp:1851] ctx_conv4_param_0(0.75) 
I0703 03:32:10.390084 31050 net.cpp:1851] ctx_final_param_0(0.00977) 
I0703 03:32:10.390085 31050 net.cpp:1851] out3a_param_0(0.75) 
I0703 03:32:10.390087 31050 net.cpp:1851] out5a_param_0(0.75) 
I0703 03:32:10.390089 31050 net.cpp:1851] res2a_branch2a_param_0(0.75) 
I0703 03:32:10.390091 31050 net.cpp:1851] res2a_branch2b_param_0(0.75) 
I0703 03:32:10.390094 31050 net.cpp:1851] res3a_branch2a_param_0(0.75) 
I0703 03:32:10.390095 31050 net.cpp:1851] res3a_branch2b_param_0(0.75) 
I0703 03:32:10.390097 31050 net.cpp:1851] res4a_branch2a_param_0(0.75) 
I0703 03:32:10.390100 31050 net.cpp:1851] res4a_branch2b_param_0(0.75) 
I0703 03:32:10.390102 31050 net.cpp:1851] res5a_branch2a_param_0(0.75) 
I0703 03:32:10.390105 31050 net.cpp:1851] res5a_branch2b_param_0(0.75) 
I0703 03:32:10.390107 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.01404e+06/2.69117e+06) 0.748
I0703 03:32:10.390259 31050 solver.cpp:473] Iteration 28000, Testing net (#0)
I0703 03:32:57.916357 31050 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.950002
I0703 03:32:57.916452 31050 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.999899
I0703 03:32:57.916461 31050 solver.cpp:546]     Test net output #2: loss = 0.132145 (* 1 = 0.132145 loss)
I0703 03:32:58.173029 31050 solver.cpp:290] Iteration 28000 (1.39455 iter/s, 71.7075s/100 iter), loss = 0.0417616
I0703 03:32:58.173056 31050 solver.cpp:309]     Train net output #0: loss = 0.0417615 (* 1 = 0.0417615 loss)
I0703 03:32:58.173066 31050 sgd_solver.cpp:106] Iteration 28000, lr = 1e-06
I0703 03:33:21.691663 31050 solver.cpp:290] Iteration 28100 (4.25207 iter/s, 23.518s/100 iter), loss = 0.041887
I0703 03:33:21.691685 31050 solver.cpp:309]     Train net output #0: loss = 0.0418869 (* 1 = 0.0418869 loss)
I0703 03:33:21.691692 31050 sgd_solver.cpp:106] Iteration 28100, lr = 1e-06
I0703 03:33:45.887434 31050 solver.cpp:290] Iteration 28200 (4.13307 iter/s, 24.1951s/100 iter), loss = 0.0404967
I0703 03:33:45.887543 31050 solver.cpp:309]     Train net output #0: loss = 0.0404966 (* 1 = 0.0404966 loss)
I0703 03:33:45.887553 31050 sgd_solver.cpp:106] Iteration 28200, lr = 1e-06
I0703 03:34:10.591101 31050 solver.cpp:290] Iteration 28300 (4.04811 iter/s, 24.7029s/100 iter), loss = 0.0355308
I0703 03:34:10.591130 31050 solver.cpp:309]     Train net output #0: loss = 0.0355307 (* 1 = 0.0355307 loss)
I0703 03:34:10.591136 31050 sgd_solver.cpp:106] Iteration 28300, lr = 1e-06
I0703 03:34:35.317490 31050 solver.cpp:290] Iteration 28400 (4.04438 iter/s, 24.7257s/100 iter), loss = 0.0295667
I0703 03:34:35.317646 31050 solver.cpp:309]     Train net output #0: loss = 0.0295665 (* 1 = 0.0295665 loss)
I0703 03:34:35.317675 31050 sgd_solver.cpp:106] Iteration 28400, lr = 1e-06
I0703 03:34:59.902120 31050 solver.cpp:290] Iteration 28500 (4.06772 iter/s, 24.5838s/100 iter), loss = 0.0491516
I0703 03:34:59.902144 31050 solver.cpp:309]     Train net output #0: loss = 0.0491514 (* 1 = 0.0491514 loss)
I0703 03:34:59.902151 31050 sgd_solver.cpp:106] Iteration 28500, lr = 1e-06
I0703 03:35:24.252833 31050 solver.cpp:290] Iteration 28600 (4.10677 iter/s, 24.35s/100 iter), loss = 0.0273414
I0703 03:35:24.252878 31050 solver.cpp:309]     Train net output #0: loss = 0.0273413 (* 1 = 0.0273413 loss)
I0703 03:35:24.252885 31050 sgd_solver.cpp:106] Iteration 28600, lr = 1e-06
I0703 03:35:48.427219 31050 solver.cpp:290] Iteration 28700 (4.13673 iter/s, 24.1737s/100 iter), loss = 0.0343721
I0703 03:35:48.427242 31050 solver.cpp:309]     Train net output #0: loss = 0.034372 (* 1 = 0.034372 loss)
I0703 03:35:48.427248 31050 sgd_solver.cpp:106] Iteration 28700, lr = 1e-06
I0703 03:36:12.605129 31050 solver.cpp:290] Iteration 28800 (4.13612 iter/s, 24.1772s/100 iter), loss = 0.028265
I0703 03:36:12.605180 31050 solver.cpp:309]     Train net output #0: loss = 0.0282649 (* 1 = 0.0282649 loss)
I0703 03:36:12.605187 31050 sgd_solver.cpp:106] Iteration 28800, lr = 1e-06
I0703 03:36:36.774639 31050 solver.cpp:290] Iteration 28900 (4.13757 iter/s, 24.1688s/100 iter), loss = 0.0483962
I0703 03:36:36.774662 31050 solver.cpp:309]     Train net output #0: loss = 0.0483961 (* 1 = 0.0483961 loss)
I0703 03:36:36.774668 31050 sgd_solver.cpp:106] Iteration 28900, lr = 1e-06
I0703 03:37:00.702466 31050 solver.cpp:354] Sparsity after update:
I0703 03:37:00.710651 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 03:37:00.710680 31050 net.cpp:1851] conv1a_param_0(0.375) 
I0703 03:37:00.710697 31050 net.cpp:1851] conv1b_param_0(0.75) 
I0703 03:37:00.710701 31050 net.cpp:1851] ctx_conv1_param_0(0.75) 
I0703 03:37:00.710705 31050 net.cpp:1851] ctx_conv2_param_0(0.75) 
I0703 03:37:00.710707 31050 net.cpp:1851] ctx_conv3_param_0(0.75) 
I0703 03:37:00.710711 31050 net.cpp:1851] ctx_conv4_param_0(0.75) 
I0703 03:37:00.710713 31050 net.cpp:1851] ctx_final_param_0(0.00977) 
I0703 03:37:00.710719 31050 net.cpp:1851] out3a_param_0(0.75) 
I0703 03:37:00.710722 31050 net.cpp:1851] out5a_param_0(0.75) 
I0703 03:37:00.710726 31050 net.cpp:1851] res2a_branch2a_param_0(0.75) 
I0703 03:37:00.710728 31050 net.cpp:1851] res2a_branch2b_param_0(0.75) 
I0703 03:37:00.710731 31050 net.cpp:1851] res3a_branch2a_param_0(0.75) 
I0703 03:37:00.710734 31050 net.cpp:1851] res3a_branch2b_param_0(0.75) 
I0703 03:37:00.710736 31050 net.cpp:1851] res4a_branch2a_param_0(0.75) 
I0703 03:37:00.710739 31050 net.cpp:1851] res4a_branch2b_param_0(0.75) 
I0703 03:37:00.710742 31050 net.cpp:1851] res5a_branch2a_param_0(0.75) 
I0703 03:37:00.710746 31050 net.cpp:1851] res5a_branch2b_param_0(0.75) 
I0703 03:37:00.710748 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.01404e+06/2.69117e+06) 0.748
I0703 03:37:00.955595 31050 solver.cpp:290] Iteration 29000 (4.1356 iter/s, 24.1803s/100 iter), loss = 0.023284
I0703 03:37:00.955621 31050 solver.cpp:309]     Train net output #0: loss = 0.0232838 (* 1 = 0.0232838 loss)
I0703 03:37:00.955631 31050 sgd_solver.cpp:106] Iteration 29000, lr = 1e-06
I0703 03:37:25.122290 31050 solver.cpp:290] Iteration 29100 (4.13804 iter/s, 24.166s/100 iter), loss = 0.0296128
I0703 03:37:25.122313 31050 solver.cpp:309]     Train net output #0: loss = 0.0296126 (* 1 = 0.0296126 loss)
I0703 03:37:25.122320 31050 sgd_solver.cpp:106] Iteration 29100, lr = 1e-06
I0703 03:37:49.314083 31050 solver.cpp:290] Iteration 29200 (4.13375 iter/s, 24.1911s/100 iter), loss = 0.0554765
I0703 03:37:49.314159 31050 solver.cpp:309]     Train net output #0: loss = 0.0554764 (* 1 = 0.0554764 loss)
I0703 03:37:49.314170 31050 sgd_solver.cpp:106] Iteration 29200, lr = 1e-06
I0703 03:38:13.573786 31050 solver.cpp:290] Iteration 29300 (4.12219 iter/s, 24.259s/100 iter), loss = 0.0473359
I0703 03:38:13.573812 31050 solver.cpp:309]     Train net output #0: loss = 0.0473358 (* 1 = 0.0473358 loss)
I0703 03:38:13.573818 31050 sgd_solver.cpp:106] Iteration 29300, lr = 1e-06
I0703 03:38:37.973940 31050 solver.cpp:290] Iteration 29400 (4.09845 iter/s, 24.3995s/100 iter), loss = 0.0299759
I0703 03:38:37.974017 31050 solver.cpp:309]     Train net output #0: loss = 0.0299758 (* 1 = 0.0299758 loss)
I0703 03:38:37.974025 31050 sgd_solver.cpp:106] Iteration 29400, lr = 1e-06
I0703 03:39:02.217223 31050 solver.cpp:290] Iteration 29500 (4.12498 iter/s, 24.2426s/100 iter), loss = 0.0348946
I0703 03:39:02.217247 31050 solver.cpp:309]     Train net output #0: loss = 0.0348945 (* 1 = 0.0348945 loss)
I0703 03:39:02.217254 31050 sgd_solver.cpp:106] Iteration 29500, lr = 1e-06
I0703 03:39:26.375445 31050 solver.cpp:290] Iteration 29600 (4.13949 iter/s, 24.1575s/100 iter), loss = 0.0284152
I0703 03:39:26.375552 31050 solver.cpp:309]     Train net output #0: loss = 0.0284151 (* 1 = 0.0284151 loss)
I0703 03:39:26.375562 31050 sgd_solver.cpp:106] Iteration 29600, lr = 1e-06
I0703 03:39:50.575717 31050 solver.cpp:290] Iteration 29700 (4.13231 iter/s, 24.1995s/100 iter), loss = 0.0269243
I0703 03:39:50.575742 31050 solver.cpp:309]     Train net output #0: loss = 0.0269242 (* 1 = 0.0269242 loss)
I0703 03:39:50.575749 31050 sgd_solver.cpp:106] Iteration 29700, lr = 1e-06
I0703 03:40:14.771692 31050 solver.cpp:290] Iteration 29800 (4.13303 iter/s, 24.1953s/100 iter), loss = 0.0367221
I0703 03:40:14.771795 31050 solver.cpp:309]     Train net output #0: loss = 0.036722 (* 1 = 0.036722 loss)
I0703 03:40:14.771806 31050 sgd_solver.cpp:106] Iteration 29800, lr = 1e-06
I0703 03:40:38.986148 31050 solver.cpp:290] Iteration 29900 (4.12989 iter/s, 24.2137s/100 iter), loss = 0.0321402
I0703 03:40:38.986172 31050 solver.cpp:309]     Train net output #0: loss = 0.0321401 (* 1 = 0.0321401 loss)
I0703 03:40:38.986178 31050 sgd_solver.cpp:106] Iteration 29900, lr = 1e-06
I0703 03:41:02.889514 31050 solver.cpp:600] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-02_23-02-42/sparse/cityscapes5_jsegnet21v2_iter_30000.caffemodel
I0703 03:41:02.915658 31050 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-02_23-02-42/sparse/cityscapes5_jsegnet21v2_iter_30000.solverstate
I0703 03:41:02.935480 31050 solver.cpp:354] Sparsity after update:
I0703 03:41:02.936686 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 03:41:02.936695 31050 net.cpp:1851] conv1a_param_0(0.375) 
I0703 03:41:02.936702 31050 net.cpp:1851] conv1b_param_0(0.75) 
I0703 03:41:02.936704 31050 net.cpp:1851] ctx_conv1_param_0(0.75) 
I0703 03:41:02.936707 31050 net.cpp:1851] ctx_conv2_param_0(0.75) 
I0703 03:41:02.936708 31050 net.cpp:1851] ctx_conv3_param_0(0.75) 
I0703 03:41:02.936710 31050 net.cpp:1851] ctx_conv4_param_0(0.75) 
I0703 03:41:02.936712 31050 net.cpp:1851] ctx_final_param_0(0.00977) 
I0703 03:41:02.936714 31050 net.cpp:1851] out3a_param_0(0.75) 
I0703 03:41:02.936717 31050 net.cpp:1851] out5a_param_0(0.75) 
I0703 03:41:02.936718 31050 net.cpp:1851] res2a_branch2a_param_0(0.75) 
I0703 03:41:02.936720 31050 net.cpp:1851] res2a_branch2b_param_0(0.75) 
I0703 03:41:02.936722 31050 net.cpp:1851] res3a_branch2a_param_0(0.75) 
I0703 03:41:02.936724 31050 net.cpp:1851] res3a_branch2b_param_0(0.75) 
I0703 03:41:02.936727 31050 net.cpp:1851] res4a_branch2a_param_0(0.75) 
I0703 03:41:02.936728 31050 net.cpp:1851] res4a_branch2b_param_0(0.75) 
I0703 03:41:02.936730 31050 net.cpp:1851] res5a_branch2a_param_0(0.75) 
I0703 03:41:02.936731 31050 net.cpp:1851] res5a_branch2b_param_0(0.75) 
I0703 03:41:02.936735 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.01404e+06/2.69117e+06) 0.748
I0703 03:41:02.936933 31050 solver.cpp:473] Iteration 30000, Testing net (#0)
I0703 03:41:50.404796 31050 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.949785
I0703 03:41:50.404914 31050 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.999895
I0703 03:41:50.404923 31050 solver.cpp:546]     Test net output #2: loss = 0.135386 (* 1 = 0.135386 loss)
I0703 03:41:50.648829 31050 solver.cpp:290] Iteration 30000 (1.39546 iter/s, 71.6607s/100 iter), loss = 0.0329481
I0703 03:41:50.648850 31050 solver.cpp:309]     Train net output #0: loss = 0.032948 (* 1 = 0.032948 loss)
I0703 03:41:50.648857 31050 sgd_solver.cpp:106] Iteration 30000, lr = 1e-06
I0703 03:42:14.129989 31050 solver.cpp:290] Iteration 30100 (4.25885 iter/s, 23.4805s/100 iter), loss = 0.0334141
I0703 03:42:14.130012 31050 solver.cpp:309]     Train net output #0: loss = 0.033414 (* 1 = 0.033414 loss)
I0703 03:42:14.130020 31050 sgd_solver.cpp:106] Iteration 30100, lr = 1e-06
I0703 03:42:38.296923 31050 solver.cpp:290] Iteration 30200 (4.138 iter/s, 24.1662s/100 iter), loss = 0.0540834
I0703 03:42:38.296975 31050 solver.cpp:309]     Train net output #0: loss = 0.0540833 (* 1 = 0.0540833 loss)
I0703 03:42:38.296984 31050 sgd_solver.cpp:106] Iteration 30200, lr = 1e-06
I0703 03:43:02.531020 31050 solver.cpp:290] Iteration 30300 (4.12654 iter/s, 24.2334s/100 iter), loss = 0.0427723
I0703 03:43:02.531044 31050 solver.cpp:309]     Train net output #0: loss = 0.0427722 (* 1 = 0.0427722 loss)
I0703 03:43:02.531051 31050 sgd_solver.cpp:106] Iteration 30300, lr = 1e-06
I0703 03:43:26.748347 31050 solver.cpp:290] Iteration 30400 (4.12939 iter/s, 24.2166s/100 iter), loss = 0.0438154
I0703 03:43:26.748618 31050 solver.cpp:309]     Train net output #0: loss = 0.0438153 (* 1 = 0.0438153 loss)
I0703 03:43:26.748628 31050 sgd_solver.cpp:106] Iteration 30400, lr = 1e-06
I0703 03:43:50.956190 31050 solver.cpp:290] Iteration 30500 (4.13105 iter/s, 24.2069s/100 iter), loss = 0.0802295
I0703 03:43:50.956212 31050 solver.cpp:309]     Train net output #0: loss = 0.0802294 (* 1 = 0.0802294 loss)
I0703 03:43:50.956219 31050 sgd_solver.cpp:106] Iteration 30500, lr = 1e-06
I0703 03:44:15.092629 31050 solver.cpp:290] Iteration 30600 (4.14323 iter/s, 24.1358s/100 iter), loss = 0.0346827
I0703 03:44:15.092736 31050 solver.cpp:309]     Train net output #0: loss = 0.0346826 (* 1 = 0.0346826 loss)
I0703 03:44:15.092746 31050 sgd_solver.cpp:106] Iteration 30600, lr = 1e-06
I0703 03:44:39.367669 31050 solver.cpp:290] Iteration 30700 (4.11959 iter/s, 24.2743s/100 iter), loss = 0.0552618
I0703 03:44:39.367692 31050 solver.cpp:309]     Train net output #0: loss = 0.0552617 (* 1 = 0.0552617 loss)
I0703 03:44:39.367698 31050 sgd_solver.cpp:106] Iteration 30700, lr = 1e-06
I0703 03:45:03.556989 31050 solver.cpp:290] Iteration 30800 (4.13417 iter/s, 24.1886s/100 iter), loss = 0.0326711
I0703 03:45:03.557101 31050 solver.cpp:309]     Train net output #0: loss = 0.032671 (* 1 = 0.032671 loss)
I0703 03:45:03.557111 31050 sgd_solver.cpp:106] Iteration 30800, lr = 1e-06
I0703 03:45:27.752732 31050 solver.cpp:290] Iteration 30900 (4.13309 iter/s, 24.195s/100 iter), loss = 0.0389502
I0703 03:45:27.752756 31050 solver.cpp:309]     Train net output #0: loss = 0.0389501 (* 1 = 0.0389501 loss)
I0703 03:45:27.752763 31050 sgd_solver.cpp:106] Iteration 30900, lr = 1e-06
I0703 03:45:51.965190 31050 solver.cpp:354] Sparsity after update:
I0703 03:45:52.014014 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 03:45:52.014124 31050 net.cpp:1851] conv1a_param_0(0.375) 
I0703 03:45:52.014223 31050 net.cpp:1851] conv1b_param_0(0.75) 
I0703 03:45:52.014264 31050 net.cpp:1851] ctx_conv1_param_0(0.75) 
I0703 03:45:52.014276 31050 net.cpp:1851] ctx_conv2_param_0(0.75) 
I0703 03:45:52.014323 31050 net.cpp:1851] ctx_conv3_param_0(0.75) 
I0703 03:45:52.014364 31050 net.cpp:1851] ctx_conv4_param_0(0.75) 
I0703 03:45:52.014418 31050 net.cpp:1851] ctx_final_param_0(0.00977) 
I0703 03:45:52.014462 31050 net.cpp:1851] out3a_param_0(0.75) 
I0703 03:45:52.014506 31050 net.cpp:1851] out5a_param_0(0.75) 
I0703 03:45:52.014557 31050 net.cpp:1851] res2a_branch2a_param_0(0.75) 
I0703 03:45:52.014595 31050 net.cpp:1851] res2a_branch2b_param_0(0.75) 
I0703 03:45:52.014632 31050 net.cpp:1851] res3a_branch2a_param_0(0.75) 
I0703 03:45:52.014683 31050 net.cpp:1851] res3a_branch2b_param_0(0.75) 
I0703 03:45:52.014724 31050 net.cpp:1851] res4a_branch2a_param_0(0.75) 
I0703 03:45:52.014765 31050 net.cpp:1851] res4a_branch2b_param_0(0.75) 
I0703 03:45:52.014794 31050 net.cpp:1851] res5a_branch2a_param_0(0.75) 
I0703 03:45:52.014837 31050 net.cpp:1851] res5a_branch2b_param_0(0.75) 
I0703 03:45:52.014852 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.01404e+06/2.69117e+06) 0.748
I0703 03:45:52.251433 31050 solver.cpp:290] Iteration 31000 (4.08196 iter/s, 24.498s/100 iter), loss = 0.0451159
I0703 03:45:52.251463 31050 solver.cpp:309]     Train net output #0: loss = 0.0451158 (* 1 = 0.0451158 loss)
I0703 03:45:52.251474 31050 sgd_solver.cpp:106] Iteration 31000, lr = 1e-06
I0703 03:46:17.142480 31050 solver.cpp:290] Iteration 31100 (4.01762 iter/s, 24.8903s/100 iter), loss = 0.050652
I0703 03:46:17.142536 31050 solver.cpp:309]     Train net output #0: loss = 0.0506519 (* 1 = 0.0506519 loss)
I0703 03:46:17.142558 31050 sgd_solver.cpp:106] Iteration 31100, lr = 1e-06
I0703 03:46:41.972113 31050 solver.cpp:290] Iteration 31200 (4.02756 iter/s, 24.8289s/100 iter), loss = 0.0332733
I0703 03:46:41.972240 31050 solver.cpp:309]     Train net output #0: loss = 0.0332732 (* 1 = 0.0332732 loss)
I0703 03:46:41.972250 31050 sgd_solver.cpp:106] Iteration 31200, lr = 1e-06
I0703 03:47:06.836203 31050 solver.cpp:290] Iteration 31300 (4.02199 iter/s, 24.8633s/100 iter), loss = 0.0357578
I0703 03:47:06.836249 31050 solver.cpp:309]     Train net output #0: loss = 0.0357576 (* 1 = 0.0357576 loss)
I0703 03:47:06.836263 31050 sgd_solver.cpp:106] Iteration 31300, lr = 1e-06
I0703 03:47:31.668491 31050 solver.cpp:290] Iteration 31400 (4.02713 iter/s, 24.8316s/100 iter), loss = 0.0269662
I0703 03:47:31.668547 31050 solver.cpp:309]     Train net output #0: loss = 0.026966 (* 1 = 0.026966 loss)
I0703 03:47:31.668555 31050 sgd_solver.cpp:106] Iteration 31400, lr = 1e-06
I0703 03:47:56.226133 31050 solver.cpp:290] Iteration 31500 (4.07217 iter/s, 24.5569s/100 iter), loss = 0.0236665
I0703 03:47:56.226166 31050 solver.cpp:309]     Train net output #0: loss = 0.0236664 (* 1 = 0.0236664 loss)
I0703 03:47:56.226173 31050 sgd_solver.cpp:106] Iteration 31500, lr = 1e-06
I0703 03:48:21.010291 31050 solver.cpp:290] Iteration 31600 (4.03495 iter/s, 24.7835s/100 iter), loss = 0.0387683
I0703 03:48:21.010381 31050 solver.cpp:309]     Train net output #0: loss = 0.0387682 (* 1 = 0.0387682 loss)
I0703 03:48:21.010401 31050 sgd_solver.cpp:106] Iteration 31600, lr = 1e-06
I0703 03:48:46.033171 31050 solver.cpp:290] Iteration 31700 (3.99646 iter/s, 25.0221s/100 iter), loss = 0.0333781
I0703 03:48:46.033195 31050 solver.cpp:309]     Train net output #0: loss = 0.033378 (* 1 = 0.033378 loss)
I0703 03:48:46.033201 31050 sgd_solver.cpp:106] Iteration 31700, lr = 1e-06
I0703 03:49:10.592350 31050 solver.cpp:290] Iteration 31800 (4.07191 iter/s, 24.5585s/100 iter), loss = 0.0268927
I0703 03:49:10.592403 31050 solver.cpp:309]     Train net output #0: loss = 0.0268926 (* 1 = 0.0268926 loss)
I0703 03:49:10.592414 31050 sgd_solver.cpp:106] Iteration 31800, lr = 1e-06
I0703 03:49:34.753767 31050 solver.cpp:290] Iteration 31900 (4.13895 iter/s, 24.1607s/100 iter), loss = 0.0230631
I0703 03:49:34.753792 31050 solver.cpp:309]     Train net output #0: loss = 0.023063 (* 1 = 0.023063 loss)
I0703 03:49:34.753798 31050 sgd_solver.cpp:106] Iteration 31900, lr = 1e-06
I0703 03:49:58.667731 31050 solver.cpp:354] Sparsity after update:
I0703 03:49:58.669435 31050 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0703 03:49:58.669442 31050 net.cpp:1851] conv1a_param_0(0.375) 
I0703 03:49:58.669450 31050 net.cpp:1851] conv1b_param_0(0.75) 
I0703 03:49:58.669451 31050 net.cpp:1851] ctx_conv1_param_0(0.75) 
I0703 03:49:58.669453 31050 net.cpp:1851] ctx_conv2_param_0(0.75) 
I0703 03:49:58.669456 31050 net.cpp:1851] ctx_conv3_param_0(0.75) 
I0703 03:49:58.669457 31050 net.cpp:1851] ctx_conv4_param_0(0.75) 
I0703 03:49:58.669459 31050 net.cpp:1851] ctx_final_param_0(0.00977) 
I0703 03:49:58.669461 31050 net.cpp:1851] out3a_param_0(0.75) 
I0703 03:49:58.669463 31050 net.cpp:1851] out5a_param_0(0.75) 
I0703 03:49:58.669466 31050 net.cpp:1851] res2a_branch2a_param_0(0.75) 
I0703 03:49:58.669467 31050 net.cpp:1851] res2a_branch2b_param_0(0.75) 
I0703 03:49:58.669469 31050 net.cpp:1851] res3a_branch2a_param_0(0.75) 
I0703 03:49:58.669471 31050 net.cpp:1851] res3a_branch2b_param_0(0.75) 
I0703 03:49:58.669473 31050 net.cpp:1851] res4a_branch2a_param_0(0.75) 
I0703 03:49:58.669476 31050 net.cpp:1851] res4a_branch2b_param_0(0.75) 
I0703 03:49:58.669477 31050 net.cpp:1851] res5a_branch2a_param_0(0.75) 
I0703 03:49:58.669479 31050 net.cpp:1851] res5a_branch2b_param_0(0.75) 
I0703 03:49:58.669481 31050 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.01404e+06/2.69117e+06) 0.748
I0703 03:49:58.669490 31050 solver.cpp:600] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-02_23-02-42/sparse/cityscapes5_jsegnet21v2_iter_32000.caffemodel
I0703 03:49:58.694672 31050 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-02_23-02-42/sparse/cityscapes5_jsegnet21v2_iter_32000.solverstate
I0703 03:49:58.781277 31050 solver.cpp:453] Iteration 32000, loss = 0.0732625
I0703 03:49:58.781298 31050 solver.cpp:473] Iteration 32000, Testing net (#0)
I0703 03:50:46.871357 31050 solver.cpp:546]     Test net output #0: accuracy/top1 = 0.95027
I0703 03:50:46.871454 31050 solver.cpp:546]     Test net output #1: accuracy/top5 = 0.99989
I0703 03:50:46.871462 31050 solver.cpp:546]     Test net output #2: loss = 0.133052 (* 1 = 0.133052 loss)
I0703 03:50:46.871465 31050 solver.cpp:458] Optimization Done.
I0703 03:50:47.025221 31050 caffe.cpp:246] Optimization Done.
training/cityscapes5_jsegnet21v2_2017-07-02_23-02-42/test
I0703 03:50:58.084902 13473 caffe.cpp:264] Not using GPU #1 for single-GPU function
I0703 03:50:59.138892 13473 caffe.cpp:273] Use GPU with device ID 0
I0703 03:50:59.139295 13473 caffe.cpp:277] GPU device name: GeForce GTX 1080
I0703 03:51:00.053241 13473 net.cpp:56] Initializing net from parameters: 
name: "jsegnet21v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/val-image-lmdb"
    label_list_path: "data/val-label-lmdb"
    batch_size: 4
    threads: 4
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b/bn"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a/bn"
  top: "out5a/bn"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a/bn"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b/bn"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a/bn"
  top: "out3a/bn"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a/bn"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv1/bn"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv2/bn"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv3/bn"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4/bn"
  top: "ctx_conv4/bn"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4/bn"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
  accuracy_param {
    ignore_label: 255
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
    ignore_label: 255
  }
}
I0703 03:51:00.149360 13473 layer_factory.hpp:77] Creating layer data
I0703 03:51:00.149436 13473 net.cpp:98] Creating Layer data
I0703 03:51:00.149461 13473 net.cpp:413] data -> data
I0703 03:51:00.149519 13473 net.cpp:413] data -> label
I0703 03:51:00.194471 13540 db_lmdb.cpp:35] Opened lmdb data/val-label-lmdb
I0703 03:51:00.211949 13535 db_lmdb.cpp:35] Opened lmdb data/val-image-lmdb
I0703 03:51:00.223580 13473 data_layer.cpp:78] ReshapePrefetch 4, 3, 640, 640
I0703 03:51:00.223628 13473 data_layer.cpp:83] output data size: 4,3,640,640
I0703 03:51:00.246518 13473 data_layer.cpp:78] ReshapePrefetch 4, 1, 640, 640
I0703 03:51:00.246562 13473 data_layer.cpp:83] output data size: 4,1,640,640
I0703 03:51:00.258579 13473 net.cpp:148] Setting up data
I0703 03:51:00.258683 13473 net.cpp:155] Top shape: 4 3 640 640 (4915200)
I0703 03:51:00.258710 13473 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0703 03:51:00.258721 13473 net.cpp:163] Memory required for data: 26214400
I0703 03:51:00.258751 13473 layer_factory.hpp:77] Creating layer label_data_1_split
I0703 03:51:00.258774 13473 net.cpp:98] Creating Layer label_data_1_split
I0703 03:51:00.258785 13473 net.cpp:439] label_data_1_split <- label
I0703 03:51:00.258805 13473 net.cpp:413] label_data_1_split -> label_data_1_split_0
I0703 03:51:00.258819 13473 net.cpp:413] label_data_1_split -> label_data_1_split_1
I0703 03:51:00.258829 13473 net.cpp:413] label_data_1_split -> label_data_1_split_2
I0703 03:51:00.258895 13473 net.cpp:148] Setting up label_data_1_split
I0703 03:51:00.258908 13473 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0703 03:51:00.258915 13473 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0703 03:51:00.258924 13473 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0703 03:51:00.258930 13473 net.cpp:163] Memory required for data: 45875200
I0703 03:51:00.258937 13473 layer_factory.hpp:77] Creating layer data/bias
I0703 03:51:00.258955 13473 net.cpp:98] Creating Layer data/bias
I0703 03:51:00.258963 13473 net.cpp:439] data/bias <- data
I0703 03:51:00.258971 13473 net.cpp:413] data/bias -> data/bias
I0703 03:51:00.260449 13473 net.cpp:148] Setting up data/bias
I0703 03:51:00.262441 13473 net.cpp:155] Top shape: 4 3 640 640 (4915200)
I0703 03:51:00.262456 13473 net.cpp:163] Memory required for data: 65536000
I0703 03:51:00.262491 13473 layer_factory.hpp:77] Creating layer conv1a
I0703 03:51:00.262637 13473 net.cpp:98] Creating Layer conv1a
I0703 03:51:00.262645 13473 net.cpp:439] conv1a <- data/bias
I0703 03:51:00.262657 13473 net.cpp:413] conv1a -> conv1a
I0703 03:51:00.264344 13473 net.cpp:148] Setting up conv1a
I0703 03:51:00.264364 13473 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0703 03:51:00.264369 13473 net.cpp:163] Memory required for data: 117964800
I0703 03:51:00.264379 13473 layer_factory.hpp:77] Creating layer conv1a/bn
I0703 03:51:00.264394 13473 net.cpp:98] Creating Layer conv1a/bn
I0703 03:51:00.264400 13473 net.cpp:439] conv1a/bn <- conv1a
I0703 03:51:00.264406 13473 net.cpp:413] conv1a/bn -> conv1a/bn
I0703 03:51:00.265760 13473 net.cpp:148] Setting up conv1a/bn
I0703 03:51:00.265777 13473 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0703 03:51:00.265781 13473 net.cpp:163] Memory required for data: 170393600
I0703 03:51:00.265794 13473 layer_factory.hpp:77] Creating layer conv1a/relu
I0703 03:51:00.265805 13473 net.cpp:98] Creating Layer conv1a/relu
I0703 03:51:00.265810 13473 net.cpp:439] conv1a/relu <- conv1a/bn
I0703 03:51:00.265816 13473 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0703 03:51:00.265836 13473 net.cpp:148] Setting up conv1a/relu
I0703 03:51:00.265841 13473 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0703 03:51:00.265843 13473 net.cpp:163] Memory required for data: 222822400
I0703 03:51:00.265846 13473 layer_factory.hpp:77] Creating layer conv1b
I0703 03:51:00.265858 13473 net.cpp:98] Creating Layer conv1b
I0703 03:51:00.265863 13473 net.cpp:439] conv1b <- conv1a/bn
I0703 03:51:00.265869 13473 net.cpp:413] conv1b -> conv1b
I0703 03:51:00.266203 13473 net.cpp:148] Setting up conv1b
I0703 03:51:00.266211 13473 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0703 03:51:00.266216 13473 net.cpp:163] Memory required for data: 275251200
I0703 03:51:00.266223 13473 layer_factory.hpp:77] Creating layer conv1b/bn
I0703 03:51:00.266232 13473 net.cpp:98] Creating Layer conv1b/bn
I0703 03:51:00.266235 13473 net.cpp:439] conv1b/bn <- conv1b
I0703 03:51:00.266242 13473 net.cpp:413] conv1b/bn -> conv1b/bn
I0703 03:51:00.266664 13473 net.cpp:148] Setting up conv1b/bn
I0703 03:51:00.266674 13473 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0703 03:51:00.266676 13473 net.cpp:163] Memory required for data: 327680000
I0703 03:51:00.266685 13473 layer_factory.hpp:77] Creating layer conv1b/relu
I0703 03:51:00.266691 13473 net.cpp:98] Creating Layer conv1b/relu
I0703 03:51:00.266695 13473 net.cpp:439] conv1b/relu <- conv1b/bn
I0703 03:51:00.266701 13473 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0703 03:51:00.266707 13473 net.cpp:148] Setting up conv1b/relu
I0703 03:51:00.266712 13473 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0703 03:51:00.266716 13473 net.cpp:163] Memory required for data: 380108800
I0703 03:51:00.266721 13473 layer_factory.hpp:77] Creating layer pool1
I0703 03:51:00.266729 13473 net.cpp:98] Creating Layer pool1
I0703 03:51:00.266734 13473 net.cpp:439] pool1 <- conv1b/bn
I0703 03:51:00.266739 13473 net.cpp:413] pool1 -> pool1
I0703 03:51:00.267161 13473 net.cpp:148] Setting up pool1
I0703 03:51:00.267179 13473 net.cpp:155] Top shape: 4 32 160 160 (3276800)
I0703 03:51:00.267185 13473 net.cpp:163] Memory required for data: 393216000
I0703 03:51:00.267191 13473 layer_factory.hpp:77] Creating layer res2a_branch2a
I0703 03:51:00.267201 13473 net.cpp:98] Creating Layer res2a_branch2a
I0703 03:51:00.267207 13473 net.cpp:439] res2a_branch2a <- pool1
I0703 03:51:00.267215 13473 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0703 03:51:00.270045 13473 net.cpp:148] Setting up res2a_branch2a
I0703 03:51:00.270056 13473 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0703 03:51:00.270058 13473 net.cpp:163] Memory required for data: 419430400
I0703 03:51:00.270064 13473 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0703 03:51:00.270071 13473 net.cpp:98] Creating Layer res2a_branch2a/bn
I0703 03:51:00.270072 13473 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0703 03:51:00.270076 13473 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0703 03:51:00.271061 13473 net.cpp:148] Setting up res2a_branch2a/bn
I0703 03:51:00.271086 13473 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0703 03:51:00.271096 13473 net.cpp:163] Memory required for data: 445644800
I0703 03:51:00.271119 13473 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0703 03:51:00.271136 13473 net.cpp:98] Creating Layer res2a_branch2a/relu
I0703 03:51:00.271143 13473 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0703 03:51:00.271154 13473 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0703 03:51:00.271169 13473 net.cpp:148] Setting up res2a_branch2a/relu
I0703 03:51:00.271179 13473 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0703 03:51:00.271188 13473 net.cpp:163] Memory required for data: 471859200
I0703 03:51:00.271195 13473 layer_factory.hpp:77] Creating layer res2a_branch2b
I0703 03:51:00.271222 13473 net.cpp:98] Creating Layer res2a_branch2b
I0703 03:51:00.271231 13473 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0703 03:51:00.271244 13473 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0703 03:51:00.275645 13473 net.cpp:148] Setting up res2a_branch2b
I0703 03:51:00.275749 13473 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0703 03:51:00.275777 13473 net.cpp:163] Memory required for data: 498073600
I0703 03:51:00.275810 13473 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0703 03:51:00.275854 13473 net.cpp:98] Creating Layer res2a_branch2b/bn
I0703 03:51:00.275880 13473 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0703 03:51:00.275931 13473 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0703 03:51:00.276547 13473 net.cpp:148] Setting up res2a_branch2b/bn
I0703 03:51:00.276563 13473 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0703 03:51:00.276572 13473 net.cpp:163] Memory required for data: 524288000
I0703 03:51:00.276587 13473 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0703 03:51:00.276598 13473 net.cpp:98] Creating Layer res2a_branch2b/relu
I0703 03:51:00.276607 13473 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0703 03:51:00.276617 13473 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0703 03:51:00.276629 13473 net.cpp:148] Setting up res2a_branch2b/relu
I0703 03:51:00.276639 13473 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0703 03:51:00.276664 13473 net.cpp:163] Memory required for data: 550502400
I0703 03:51:00.276680 13473 layer_factory.hpp:77] Creating layer pool2
I0703 03:51:00.276700 13473 net.cpp:98] Creating Layer pool2
I0703 03:51:00.276707 13473 net.cpp:439] pool2 <- res2a_branch2b/bn
I0703 03:51:00.276721 13473 net.cpp:413] pool2 -> pool2
I0703 03:51:00.276768 13473 net.cpp:148] Setting up pool2
I0703 03:51:00.276779 13473 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 03:51:00.276787 13473 net.cpp:163] Memory required for data: 557056000
I0703 03:51:00.276796 13473 layer_factory.hpp:77] Creating layer res3a_branch2a
I0703 03:51:00.276813 13473 net.cpp:98] Creating Layer res3a_branch2a
I0703 03:51:00.276820 13473 net.cpp:439] res3a_branch2a <- pool2
I0703 03:51:00.276830 13473 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0703 03:51:00.286577 13473 net.cpp:148] Setting up res3a_branch2a
I0703 03:51:00.286682 13473 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0703 03:51:00.286695 13473 net.cpp:163] Memory required for data: 570163200
I0703 03:51:00.286715 13473 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0703 03:51:00.286741 13473 net.cpp:98] Creating Layer res3a_branch2a/bn
I0703 03:51:00.286753 13473 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0703 03:51:00.286768 13473 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0703 03:51:00.287569 13473 net.cpp:148] Setting up res3a_branch2a/bn
I0703 03:51:00.287578 13473 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0703 03:51:00.287581 13473 net.cpp:163] Memory required for data: 583270400
I0703 03:51:00.287592 13473 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0703 03:51:00.287598 13473 net.cpp:98] Creating Layer res3a_branch2a/relu
I0703 03:51:00.287602 13473 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0703 03:51:00.287607 13473 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0703 03:51:00.287621 13473 net.cpp:148] Setting up res3a_branch2a/relu
I0703 03:51:00.287624 13473 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0703 03:51:00.287627 13473 net.cpp:163] Memory required for data: 596377600
I0703 03:51:00.287631 13473 layer_factory.hpp:77] Creating layer res3a_branch2b
I0703 03:51:00.287639 13473 net.cpp:98] Creating Layer res3a_branch2b
I0703 03:51:00.287643 13473 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0703 03:51:00.287647 13473 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0703 03:51:00.288902 13473 net.cpp:148] Setting up res3a_branch2b
I0703 03:51:00.288915 13473 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0703 03:51:00.288920 13473 net.cpp:163] Memory required for data: 609484800
I0703 03:51:00.288926 13473 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0703 03:51:00.288938 13473 net.cpp:98] Creating Layer res3a_branch2b/bn
I0703 03:51:00.288941 13473 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0703 03:51:00.288956 13473 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0703 03:51:00.290772 13473 net.cpp:148] Setting up res3a_branch2b/bn
I0703 03:51:00.290779 13473 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0703 03:51:00.290781 13473 net.cpp:163] Memory required for data: 622592000
I0703 03:51:00.290787 13473 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0703 03:51:00.290791 13473 net.cpp:98] Creating Layer res3a_branch2b/relu
I0703 03:51:00.290792 13473 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0703 03:51:00.290807 13473 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0703 03:51:00.290810 13473 net.cpp:148] Setting up res3a_branch2b/relu
I0703 03:51:00.290812 13473 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0703 03:51:00.290814 13473 net.cpp:163] Memory required for data: 635699200
I0703 03:51:00.290817 13473 layer_factory.hpp:77] Creating layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0703 03:51:00.290822 13473 net.cpp:98] Creating Layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0703 03:51:00.290824 13473 net.cpp:439] res3a_branch2b/bn_res3a_branch2b/relu_0_split <- res3a_branch2b/bn
I0703 03:51:00.290827 13473 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0703 03:51:00.290830 13473 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0703 03:51:00.290849 13473 net.cpp:148] Setting up res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0703 03:51:00.290853 13473 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0703 03:51:00.290855 13473 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0703 03:51:00.290858 13473 net.cpp:163] Memory required for data: 661913600
I0703 03:51:00.290859 13473 layer_factory.hpp:77] Creating layer pool3
I0703 03:51:00.290864 13473 net.cpp:98] Creating Layer pool3
I0703 03:51:00.290866 13473 net.cpp:439] pool3 <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0703 03:51:00.290869 13473 net.cpp:413] pool3 -> pool3
I0703 03:51:00.290889 13473 net.cpp:148] Setting up pool3
I0703 03:51:00.290892 13473 net.cpp:155] Top shape: 4 128 40 40 (819200)
I0703 03:51:00.290894 13473 net.cpp:163] Memory required for data: 665190400
I0703 03:51:00.290896 13473 layer_factory.hpp:77] Creating layer res4a_branch2a
I0703 03:51:00.290901 13473 net.cpp:98] Creating Layer res4a_branch2a
I0703 03:51:00.290904 13473 net.cpp:439] res4a_branch2a <- pool3
I0703 03:51:00.290907 13473 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0703 03:51:00.299072 13473 net.cpp:148] Setting up res4a_branch2a
I0703 03:51:00.299098 13473 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0703 03:51:00.299100 13473 net.cpp:163] Memory required for data: 671744000
I0703 03:51:00.299109 13473 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0703 03:51:00.299121 13473 net.cpp:98] Creating Layer res4a_branch2a/bn
I0703 03:51:00.299132 13473 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0703 03:51:00.299141 13473 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0703 03:51:00.299533 13473 net.cpp:148] Setting up res4a_branch2a/bn
I0703 03:51:00.299541 13473 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0703 03:51:00.299545 13473 net.cpp:163] Memory required for data: 678297600
I0703 03:51:00.299551 13473 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0703 03:51:00.299557 13473 net.cpp:98] Creating Layer res4a_branch2a/relu
I0703 03:51:00.299561 13473 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0703 03:51:00.299564 13473 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0703 03:51:00.299571 13473 net.cpp:148] Setting up res4a_branch2a/relu
I0703 03:51:00.299574 13473 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0703 03:51:00.299578 13473 net.cpp:163] Memory required for data: 684851200
I0703 03:51:00.299582 13473 layer_factory.hpp:77] Creating layer res4a_branch2b
I0703 03:51:00.299589 13473 net.cpp:98] Creating Layer res4a_branch2b
I0703 03:51:00.299592 13473 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0703 03:51:00.299598 13473 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0703 03:51:00.303418 13473 net.cpp:148] Setting up res4a_branch2b
I0703 03:51:00.303428 13473 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0703 03:51:00.303431 13473 net.cpp:163] Memory required for data: 691404800
I0703 03:51:00.303434 13473 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0703 03:51:00.303439 13473 net.cpp:98] Creating Layer res4a_branch2b/bn
I0703 03:51:00.303442 13473 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0703 03:51:00.303453 13473 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0703 03:51:00.303745 13473 net.cpp:148] Setting up res4a_branch2b/bn
I0703 03:51:00.303752 13473 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0703 03:51:00.303755 13473 net.cpp:163] Memory required for data: 697958400
I0703 03:51:00.303761 13473 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0703 03:51:00.303764 13473 net.cpp:98] Creating Layer res4a_branch2b/relu
I0703 03:51:00.303767 13473 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0703 03:51:00.303771 13473 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0703 03:51:00.303774 13473 net.cpp:148] Setting up res4a_branch2b/relu
I0703 03:51:00.303777 13473 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0703 03:51:00.303781 13473 net.cpp:163] Memory required for data: 704512000
I0703 03:51:00.303791 13473 layer_factory.hpp:77] Creating layer pool4
I0703 03:51:00.303795 13473 net.cpp:98] Creating Layer pool4
I0703 03:51:00.303798 13473 net.cpp:439] pool4 <- res4a_branch2b/bn
I0703 03:51:00.303800 13473 net.cpp:413] pool4 -> pool4
I0703 03:51:00.303820 13473 net.cpp:148] Setting up pool4
I0703 03:51:00.303824 13473 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0703 03:51:00.303827 13473 net.cpp:163] Memory required for data: 711065600
I0703 03:51:00.303828 13473 layer_factory.hpp:77] Creating layer res5a_branch2a
I0703 03:51:00.303834 13473 net.cpp:98] Creating Layer res5a_branch2a
I0703 03:51:00.303838 13473 net.cpp:439] res5a_branch2a <- pool4
I0703 03:51:00.303841 13473 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0703 03:51:00.331609 13473 net.cpp:148] Setting up res5a_branch2a
I0703 03:51:00.331629 13473 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0703 03:51:00.331634 13473 net.cpp:163] Memory required for data: 724172800
I0703 03:51:00.331641 13473 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0703 03:51:00.331652 13473 net.cpp:98] Creating Layer res5a_branch2a/bn
I0703 03:51:00.331658 13473 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0703 03:51:00.331665 13473 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0703 03:51:00.332065 13473 net.cpp:148] Setting up res5a_branch2a/bn
I0703 03:51:00.332073 13473 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0703 03:51:00.332077 13473 net.cpp:163] Memory required for data: 737280000
I0703 03:51:00.332084 13473 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0703 03:51:00.332089 13473 net.cpp:98] Creating Layer res5a_branch2a/relu
I0703 03:51:00.332094 13473 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0703 03:51:00.332098 13473 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0703 03:51:00.332105 13473 net.cpp:148] Setting up res5a_branch2a/relu
I0703 03:51:00.332110 13473 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0703 03:51:00.332113 13473 net.cpp:163] Memory required for data: 750387200
I0703 03:51:00.332118 13473 layer_factory.hpp:77] Creating layer res5a_branch2b
I0703 03:51:00.332129 13473 net.cpp:98] Creating Layer res5a_branch2b
I0703 03:51:00.332132 13473 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0703 03:51:00.332139 13473 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0703 03:51:00.345060 13473 net.cpp:148] Setting up res5a_branch2b
I0703 03:51:00.345089 13473 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0703 03:51:00.345093 13473 net.cpp:163] Memory required for data: 763494400
I0703 03:51:00.345103 13473 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0703 03:51:00.345113 13473 net.cpp:98] Creating Layer res5a_branch2b/bn
I0703 03:51:00.345116 13473 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0703 03:51:00.345120 13473 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0703 03:51:00.345451 13473 net.cpp:148] Setting up res5a_branch2b/bn
I0703 03:51:00.345459 13473 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0703 03:51:00.345463 13473 net.cpp:163] Memory required for data: 776601600
I0703 03:51:00.345470 13473 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0703 03:51:00.345475 13473 net.cpp:98] Creating Layer res5a_branch2b/relu
I0703 03:51:00.345491 13473 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0703 03:51:00.345499 13473 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0703 03:51:00.345510 13473 net.cpp:148] Setting up res5a_branch2b/relu
I0703 03:51:00.345515 13473 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0703 03:51:00.345520 13473 net.cpp:163] Memory required for data: 789708800
I0703 03:51:00.345525 13473 layer_factory.hpp:77] Creating layer out5a
I0703 03:51:00.345536 13473 net.cpp:98] Creating Layer out5a
I0703 03:51:00.345542 13473 net.cpp:439] out5a <- res5a_branch2b/bn
I0703 03:51:00.345549 13473 net.cpp:413] out5a -> out5a
I0703 03:51:00.350811 13473 net.cpp:148] Setting up out5a
I0703 03:51:00.350837 13473 net.cpp:155] Top shape: 4 64 40 40 (409600)
I0703 03:51:00.350841 13473 net.cpp:163] Memory required for data: 791347200
I0703 03:51:00.350849 13473 layer_factory.hpp:77] Creating layer out5a/bn
I0703 03:51:00.350859 13473 net.cpp:98] Creating Layer out5a/bn
I0703 03:51:00.350864 13473 net.cpp:439] out5a/bn <- out5a
I0703 03:51:00.350870 13473 net.cpp:413] out5a/bn -> out5a/bn
I0703 03:51:00.351330 13473 net.cpp:148] Setting up out5a/bn
I0703 03:51:00.351338 13473 net.cpp:155] Top shape: 4 64 40 40 (409600)
I0703 03:51:00.351341 13473 net.cpp:163] Memory required for data: 792985600
I0703 03:51:00.351351 13473 layer_factory.hpp:77] Creating layer out5a/relu
I0703 03:51:00.351356 13473 net.cpp:98] Creating Layer out5a/relu
I0703 03:51:00.351358 13473 net.cpp:439] out5a/relu <- out5a/bn
I0703 03:51:00.351362 13473 net.cpp:400] out5a/relu -> out5a/bn (in-place)
I0703 03:51:00.351368 13473 net.cpp:148] Setting up out5a/relu
I0703 03:51:00.351373 13473 net.cpp:155] Top shape: 4 64 40 40 (409600)
I0703 03:51:00.351377 13473 net.cpp:163] Memory required for data: 794624000
I0703 03:51:00.351379 13473 layer_factory.hpp:77] Creating layer out5a_up2
I0703 03:51:00.351385 13473 net.cpp:98] Creating Layer out5a_up2
I0703 03:51:00.351388 13473 net.cpp:439] out5a_up2 <- out5a/bn
I0703 03:51:00.351393 13473 net.cpp:413] out5a_up2 -> out5a_up2
I0703 03:51:00.351578 13473 net.cpp:148] Setting up out5a_up2
I0703 03:51:00.351585 13473 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 03:51:00.351588 13473 net.cpp:163] Memory required for data: 801177600
I0703 03:51:00.351593 13473 layer_factory.hpp:77] Creating layer out3a
I0703 03:51:00.351598 13473 net.cpp:98] Creating Layer out3a
I0703 03:51:00.351603 13473 net.cpp:439] out3a <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0703 03:51:00.351606 13473 net.cpp:413] out3a -> out3a
I0703 03:51:00.352761 13473 net.cpp:148] Setting up out3a
I0703 03:51:00.352768 13473 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 03:51:00.352771 13473 net.cpp:163] Memory required for data: 807731200
I0703 03:51:00.352774 13473 layer_factory.hpp:77] Creating layer out3a/bn
I0703 03:51:00.352778 13473 net.cpp:98] Creating Layer out3a/bn
I0703 03:51:00.352782 13473 net.cpp:439] out3a/bn <- out3a
I0703 03:51:00.352784 13473 net.cpp:413] out3a/bn -> out3a/bn
I0703 03:51:00.353082 13473 net.cpp:148] Setting up out3a/bn
I0703 03:51:00.353087 13473 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 03:51:00.353090 13473 net.cpp:163] Memory required for data: 814284800
I0703 03:51:00.353094 13473 layer_factory.hpp:77] Creating layer out3a/relu
I0703 03:51:00.353097 13473 net.cpp:98] Creating Layer out3a/relu
I0703 03:51:00.353101 13473 net.cpp:439] out3a/relu <- out3a/bn
I0703 03:51:00.353102 13473 net.cpp:400] out3a/relu -> out3a/bn (in-place)
I0703 03:51:00.353106 13473 net.cpp:148] Setting up out3a/relu
I0703 03:51:00.353108 13473 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 03:51:00.353109 13473 net.cpp:163] Memory required for data: 820838400
I0703 03:51:00.353111 13473 layer_factory.hpp:77] Creating layer out3_out5_combined
I0703 03:51:00.353119 13473 net.cpp:98] Creating Layer out3_out5_combined
I0703 03:51:00.353122 13473 net.cpp:439] out3_out5_combined <- out5a_up2
I0703 03:51:00.353126 13473 net.cpp:439] out3_out5_combined <- out3a/bn
I0703 03:51:00.353137 13473 net.cpp:413] out3_out5_combined -> out3_out5_combined
I0703 03:51:00.353149 13473 net.cpp:148] Setting up out3_out5_combined
I0703 03:51:00.353152 13473 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 03:51:00.353154 13473 net.cpp:163] Memory required for data: 827392000
I0703 03:51:00.353157 13473 layer_factory.hpp:77] Creating layer ctx_conv1
I0703 03:51:00.353160 13473 net.cpp:98] Creating Layer ctx_conv1
I0703 03:51:00.353163 13473 net.cpp:439] ctx_conv1 <- out3_out5_combined
I0703 03:51:00.353165 13473 net.cpp:413] ctx_conv1 -> ctx_conv1
I0703 03:51:00.354020 13473 net.cpp:148] Setting up ctx_conv1
I0703 03:51:00.354025 13473 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 03:51:00.354027 13473 net.cpp:163] Memory required for data: 833945600
I0703 03:51:00.354030 13473 layer_factory.hpp:77] Creating layer ctx_conv1/bn
I0703 03:51:00.354034 13473 net.cpp:98] Creating Layer ctx_conv1/bn
I0703 03:51:00.354037 13473 net.cpp:439] ctx_conv1/bn <- ctx_conv1
I0703 03:51:00.354041 13473 net.cpp:413] ctx_conv1/bn -> ctx_conv1/bn
I0703 03:51:00.354333 13473 net.cpp:148] Setting up ctx_conv1/bn
I0703 03:51:00.354338 13473 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 03:51:00.354341 13473 net.cpp:163] Memory required for data: 840499200
I0703 03:51:00.354346 13473 layer_factory.hpp:77] Creating layer ctx_conv1/relu
I0703 03:51:00.354349 13473 net.cpp:98] Creating Layer ctx_conv1/relu
I0703 03:51:00.354352 13473 net.cpp:439] ctx_conv1/relu <- ctx_conv1/bn
I0703 03:51:00.354356 13473 net.cpp:400] ctx_conv1/relu -> ctx_conv1/bn (in-place)
I0703 03:51:00.354359 13473 net.cpp:148] Setting up ctx_conv1/relu
I0703 03:51:00.354362 13473 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 03:51:00.354364 13473 net.cpp:163] Memory required for data: 847052800
I0703 03:51:00.354367 13473 layer_factory.hpp:77] Creating layer ctx_conv2
I0703 03:51:00.354370 13473 net.cpp:98] Creating Layer ctx_conv2
I0703 03:51:00.354372 13473 net.cpp:439] ctx_conv2 <- ctx_conv1/bn
I0703 03:51:00.354375 13473 net.cpp:413] ctx_conv2 -> ctx_conv2
I0703 03:51:00.355245 13473 net.cpp:148] Setting up ctx_conv2
I0703 03:51:00.355250 13473 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 03:51:00.355252 13473 net.cpp:163] Memory required for data: 853606400
I0703 03:51:00.355257 13473 layer_factory.hpp:77] Creating layer ctx_conv2/bn
I0703 03:51:00.355260 13473 net.cpp:98] Creating Layer ctx_conv2/bn
I0703 03:51:00.355263 13473 net.cpp:439] ctx_conv2/bn <- ctx_conv2
I0703 03:51:00.355267 13473 net.cpp:413] ctx_conv2/bn -> ctx_conv2/bn
I0703 03:51:00.355556 13473 net.cpp:148] Setting up ctx_conv2/bn
I0703 03:51:00.355561 13473 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 03:51:00.355564 13473 net.cpp:163] Memory required for data: 860160000
I0703 03:51:00.355569 13473 layer_factory.hpp:77] Creating layer ctx_conv2/relu
I0703 03:51:00.355572 13473 net.cpp:98] Creating Layer ctx_conv2/relu
I0703 03:51:00.355576 13473 net.cpp:439] ctx_conv2/relu <- ctx_conv2/bn
I0703 03:51:00.355578 13473 net.cpp:400] ctx_conv2/relu -> ctx_conv2/bn (in-place)
I0703 03:51:00.355581 13473 net.cpp:148] Setting up ctx_conv2/relu
I0703 03:51:00.355584 13473 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 03:51:00.355587 13473 net.cpp:163] Memory required for data: 866713600
I0703 03:51:00.355589 13473 layer_factory.hpp:77] Creating layer ctx_conv3
I0703 03:51:00.355595 13473 net.cpp:98] Creating Layer ctx_conv3
I0703 03:51:00.355597 13473 net.cpp:439] ctx_conv3 <- ctx_conv2/bn
I0703 03:51:00.355600 13473 net.cpp:413] ctx_conv3 -> ctx_conv3
I0703 03:51:00.356457 13473 net.cpp:148] Setting up ctx_conv3
I0703 03:51:00.356462 13473 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 03:51:00.356464 13473 net.cpp:163] Memory required for data: 873267200
I0703 03:51:00.356468 13473 layer_factory.hpp:77] Creating layer ctx_conv3/bn
I0703 03:51:00.356472 13473 net.cpp:98] Creating Layer ctx_conv3/bn
I0703 03:51:00.356474 13473 net.cpp:439] ctx_conv3/bn <- ctx_conv3
I0703 03:51:00.356477 13473 net.cpp:413] ctx_conv3/bn -> ctx_conv3/bn
I0703 03:51:00.356775 13473 net.cpp:148] Setting up ctx_conv3/bn
I0703 03:51:00.356779 13473 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 03:51:00.356781 13473 net.cpp:163] Memory required for data: 879820800
I0703 03:51:00.356786 13473 layer_factory.hpp:77] Creating layer ctx_conv3/relu
I0703 03:51:00.356789 13473 net.cpp:98] Creating Layer ctx_conv3/relu
I0703 03:51:00.356792 13473 net.cpp:439] ctx_conv3/relu <- ctx_conv3/bn
I0703 03:51:00.356796 13473 net.cpp:400] ctx_conv3/relu -> ctx_conv3/bn (in-place)
I0703 03:51:00.356798 13473 net.cpp:148] Setting up ctx_conv3/relu
I0703 03:51:00.356801 13473 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 03:51:00.356804 13473 net.cpp:163] Memory required for data: 886374400
I0703 03:51:00.356806 13473 layer_factory.hpp:77] Creating layer ctx_conv4
I0703 03:51:00.356811 13473 net.cpp:98] Creating Layer ctx_conv4
I0703 03:51:00.356812 13473 net.cpp:439] ctx_conv4 <- ctx_conv3/bn
I0703 03:51:00.356817 13473 net.cpp:413] ctx_conv4 -> ctx_conv4
I0703 03:51:00.357671 13473 net.cpp:148] Setting up ctx_conv4
I0703 03:51:00.357676 13473 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 03:51:00.357678 13473 net.cpp:163] Memory required for data: 892928000
I0703 03:51:00.357681 13473 layer_factory.hpp:77] Creating layer ctx_conv4/bn
I0703 03:51:00.357686 13473 net.cpp:98] Creating Layer ctx_conv4/bn
I0703 03:51:00.357688 13473 net.cpp:439] ctx_conv4/bn <- ctx_conv4
I0703 03:51:00.357692 13473 net.cpp:413] ctx_conv4/bn -> ctx_conv4/bn
I0703 03:51:00.357988 13473 net.cpp:148] Setting up ctx_conv4/bn
I0703 03:51:00.357993 13473 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 03:51:00.357996 13473 net.cpp:163] Memory required for data: 899481600
I0703 03:51:00.358001 13473 layer_factory.hpp:77] Creating layer ctx_conv4/relu
I0703 03:51:00.358005 13473 net.cpp:98] Creating Layer ctx_conv4/relu
I0703 03:51:00.358007 13473 net.cpp:439] ctx_conv4/relu <- ctx_conv4/bn
I0703 03:51:00.358011 13473 net.cpp:400] ctx_conv4/relu -> ctx_conv4/bn (in-place)
I0703 03:51:00.358013 13473 net.cpp:148] Setting up ctx_conv4/relu
I0703 03:51:00.358016 13473 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0703 03:51:00.358018 13473 net.cpp:163] Memory required for data: 906035200
I0703 03:51:00.358021 13473 layer_factory.hpp:77] Creating layer ctx_final
I0703 03:51:00.358024 13473 net.cpp:98] Creating Layer ctx_final
I0703 03:51:00.358027 13473 net.cpp:439] ctx_final <- ctx_conv4/bn
I0703 03:51:00.358031 13473 net.cpp:413] ctx_final -> ctx_final
I0703 03:51:00.358258 13473 net.cpp:148] Setting up ctx_final
I0703 03:51:00.358263 13473 net.cpp:155] Top shape: 4 8 80 80 (204800)
I0703 03:51:00.358264 13473 net.cpp:163] Memory required for data: 906854400
I0703 03:51:00.358268 13473 layer_factory.hpp:77] Creating layer ctx_final/relu
I0703 03:51:00.358270 13473 net.cpp:98] Creating Layer ctx_final/relu
I0703 03:51:00.358273 13473 net.cpp:439] ctx_final/relu <- ctx_final
I0703 03:51:00.358276 13473 net.cpp:400] ctx_final/relu -> ctx_final (in-place)
I0703 03:51:00.358279 13473 net.cpp:148] Setting up ctx_final/relu
I0703 03:51:00.358283 13473 net.cpp:155] Top shape: 4 8 80 80 (204800)
I0703 03:51:00.358284 13473 net.cpp:163] Memory required for data: 907673600
I0703 03:51:00.358288 13473 layer_factory.hpp:77] Creating layer out_deconv_final_up2
I0703 03:51:00.358291 13473 net.cpp:98] Creating Layer out_deconv_final_up2
I0703 03:51:00.358294 13473 net.cpp:439] out_deconv_final_up2 <- ctx_final
I0703 03:51:00.358296 13473 net.cpp:413] out_deconv_final_up2 -> out_deconv_final_up2
I0703 03:51:00.358397 13473 net.cpp:148] Setting up out_deconv_final_up2
I0703 03:51:00.358402 13473 net.cpp:155] Top shape: 4 8 160 160 (819200)
I0703 03:51:00.358403 13473 net.cpp:163] Memory required for data: 910950400
I0703 03:51:00.358407 13473 layer_factory.hpp:77] Creating layer out_deconv_final_up4
I0703 03:51:00.358410 13473 net.cpp:98] Creating Layer out_deconv_final_up4
I0703 03:51:00.358413 13473 net.cpp:439] out_deconv_final_up4 <- out_deconv_final_up2
I0703 03:51:00.358417 13473 net.cpp:413] out_deconv_final_up4 -> out_deconv_final_up4
I0703 03:51:00.358517 13473 net.cpp:148] Setting up out_deconv_final_up4
I0703 03:51:00.358521 13473 net.cpp:155] Top shape: 4 8 320 320 (3276800)
I0703 03:51:00.358525 13473 net.cpp:163] Memory required for data: 924057600
I0703 03:51:00.358527 13473 layer_factory.hpp:77] Creating layer out_deconv_final_up8
I0703 03:51:00.358530 13473 net.cpp:98] Creating Layer out_deconv_final_up8
I0703 03:51:00.358533 13473 net.cpp:439] out_deconv_final_up8 <- out_deconv_final_up4
I0703 03:51:00.358536 13473 net.cpp:413] out_deconv_final_up8 -> out_deconv_final_up8
I0703 03:51:00.358629 13473 net.cpp:148] Setting up out_deconv_final_up8
I0703 03:51:00.358633 13473 net.cpp:155] Top shape: 4 8 640 640 (13107200)
I0703 03:51:00.358635 13473 net.cpp:163] Memory required for data: 976486400
I0703 03:51:00.358639 13473 layer_factory.hpp:77] Creating layer out_deconv_final_up8_out_deconv_final_up8_0_split
I0703 03:51:00.358641 13473 net.cpp:98] Creating Layer out_deconv_final_up8_out_deconv_final_up8_0_split
I0703 03:51:00.358644 13473 net.cpp:439] out_deconv_final_up8_out_deconv_final_up8_0_split <- out_deconv_final_up8
I0703 03:51:00.358647 13473 net.cpp:413] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0703 03:51:00.358650 13473 net.cpp:413] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0703 03:51:00.358654 13473 net.cpp:413] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0703 03:51:00.358677 13473 net.cpp:148] Setting up out_deconv_final_up8_out_deconv_final_up8_0_split
I0703 03:51:00.358681 13473 net.cpp:155] Top shape: 4 8 640 640 (13107200)
I0703 03:51:00.358683 13473 net.cpp:155] Top shape: 4 8 640 640 (13107200)
I0703 03:51:00.358686 13473 net.cpp:155] Top shape: 4 8 640 640 (13107200)
I0703 03:51:00.358688 13473 net.cpp:163] Memory required for data: 1133772800
I0703 03:51:00.358690 13473 layer_factory.hpp:77] Creating layer loss
I0703 03:51:00.358698 13473 net.cpp:98] Creating Layer loss
I0703 03:51:00.358701 13473 net.cpp:439] loss <- out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0703 03:51:00.358705 13473 net.cpp:439] loss <- label_data_1_split_0
I0703 03:51:00.358707 13473 net.cpp:413] loss -> loss
I0703 03:51:00.358716 13473 layer_factory.hpp:77] Creating layer loss
I0703 03:51:00.374143 13473 net.cpp:148] Setting up loss
I0703 03:51:00.374166 13473 net.cpp:155] Top shape: (1)
I0703 03:51:00.374168 13473 net.cpp:158]     with loss weight 1
I0703 03:51:00.374181 13473 net.cpp:163] Memory required for data: 1133772804
I0703 03:51:00.374186 13473 layer_factory.hpp:77] Creating layer accuracy/top1
I0703 03:51:00.374199 13473 net.cpp:98] Creating Layer accuracy/top1
I0703 03:51:00.374203 13473 net.cpp:439] accuracy/top1 <- out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0703 03:51:00.374208 13473 net.cpp:439] accuracy/top1 <- label_data_1_split_1
I0703 03:51:00.374212 13473 net.cpp:413] accuracy/top1 -> accuracy/top1
I0703 03:51:00.374225 13473 net.cpp:148] Setting up accuracy/top1
I0703 03:51:00.374229 13473 net.cpp:155] Top shape: (1)
I0703 03:51:00.374231 13473 net.cpp:163] Memory required for data: 1133772808
I0703 03:51:00.374233 13473 layer_factory.hpp:77] Creating layer accuracy/top5
I0703 03:51:00.374238 13473 net.cpp:98] Creating Layer accuracy/top5
I0703 03:51:00.374241 13473 net.cpp:439] accuracy/top5 <- out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0703 03:51:00.374245 13473 net.cpp:439] accuracy/top5 <- label_data_1_split_2
I0703 03:51:00.374250 13473 net.cpp:413] accuracy/top5 -> accuracy/top5
I0703 03:51:00.374255 13473 net.cpp:148] Setting up accuracy/top5
I0703 03:51:00.374260 13473 net.cpp:155] Top shape: (1)
I0703 03:51:00.374264 13473 net.cpp:163] Memory required for data: 1133772812
I0703 03:51:00.374267 13473 net.cpp:226] accuracy/top5 does not need backward computation.
I0703 03:51:00.374271 13473 net.cpp:226] accuracy/top1 does not need backward computation.
I0703 03:51:00.374275 13473 net.cpp:224] loss needs backward computation.
I0703 03:51:00.374289 13473 net.cpp:224] out_deconv_final_up8_out_deconv_final_up8_0_split needs backward computation.
I0703 03:51:00.374294 13473 net.cpp:224] out_deconv_final_up8 needs backward computation.
I0703 03:51:00.374299 13473 net.cpp:224] out_deconv_final_up4 needs backward computation.
I0703 03:51:00.374303 13473 net.cpp:224] out_deconv_final_up2 needs backward computation.
I0703 03:51:00.374307 13473 net.cpp:224] ctx_final/relu needs backward computation.
I0703 03:51:00.374311 13473 net.cpp:224] ctx_final needs backward computation.
I0703 03:51:00.374316 13473 net.cpp:224] ctx_conv4/relu needs backward computation.
I0703 03:51:00.374320 13473 net.cpp:224] ctx_conv4/bn needs backward computation.
I0703 03:51:00.374325 13473 net.cpp:224] ctx_conv4 needs backward computation.
I0703 03:51:00.374328 13473 net.cpp:224] ctx_conv3/relu needs backward computation.
I0703 03:51:00.374332 13473 net.cpp:224] ctx_conv3/bn needs backward computation.
I0703 03:51:00.374337 13473 net.cpp:224] ctx_conv3 needs backward computation.
I0703 03:51:00.374341 13473 net.cpp:224] ctx_conv2/relu needs backward computation.
I0703 03:51:00.374346 13473 net.cpp:224] ctx_conv2/bn needs backward computation.
I0703 03:51:00.374349 13473 net.cpp:224] ctx_conv2 needs backward computation.
I0703 03:51:00.374353 13473 net.cpp:224] ctx_conv1/relu needs backward computation.
I0703 03:51:00.374357 13473 net.cpp:224] ctx_conv1/bn needs backward computation.
I0703 03:51:00.374361 13473 net.cpp:224] ctx_conv1 needs backward computation.
I0703 03:51:00.374366 13473 net.cpp:224] out3_out5_combined needs backward computation.
I0703 03:51:00.374370 13473 net.cpp:224] out3a/relu needs backward computation.
I0703 03:51:00.374375 13473 net.cpp:224] out3a/bn needs backward computation.
I0703 03:51:00.374379 13473 net.cpp:224] out3a needs backward computation.
I0703 03:51:00.374390 13473 net.cpp:224] out5a_up2 needs backward computation.
I0703 03:51:00.374394 13473 net.cpp:224] out5a/relu needs backward computation.
I0703 03:51:00.374398 13473 net.cpp:224] out5a/bn needs backward computation.
I0703 03:51:00.374403 13473 net.cpp:224] out5a needs backward computation.
I0703 03:51:00.374408 13473 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0703 03:51:00.374411 13473 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0703 03:51:00.374416 13473 net.cpp:224] res5a_branch2b needs backward computation.
I0703 03:51:00.374420 13473 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0703 03:51:00.374424 13473 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0703 03:51:00.374428 13473 net.cpp:224] res5a_branch2a needs backward computation.
I0703 03:51:00.374433 13473 net.cpp:224] pool4 needs backward computation.
I0703 03:51:00.374438 13473 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0703 03:51:00.374441 13473 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0703 03:51:00.374445 13473 net.cpp:224] res4a_branch2b needs backward computation.
I0703 03:51:00.374449 13473 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0703 03:51:00.374454 13473 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0703 03:51:00.374459 13473 net.cpp:224] res4a_branch2a needs backward computation.
I0703 03:51:00.374462 13473 net.cpp:224] pool3 needs backward computation.
I0703 03:51:00.374466 13473 net.cpp:224] res3a_branch2b/bn_res3a_branch2b/relu_0_split needs backward computation.
I0703 03:51:00.374471 13473 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0703 03:51:00.374475 13473 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0703 03:51:00.374480 13473 net.cpp:224] res3a_branch2b needs backward computation.
I0703 03:51:00.374483 13473 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0703 03:51:00.374488 13473 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0703 03:51:00.374492 13473 net.cpp:224] res3a_branch2a needs backward computation.
I0703 03:51:00.374496 13473 net.cpp:224] pool2 needs backward computation.
I0703 03:51:00.374505 13473 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0703 03:51:00.374511 13473 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0703 03:51:00.374514 13473 net.cpp:224] res2a_branch2b needs backward computation.
I0703 03:51:00.374519 13473 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0703 03:51:00.374523 13473 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0703 03:51:00.374527 13473 net.cpp:224] res2a_branch2a needs backward computation.
I0703 03:51:00.374531 13473 net.cpp:224] pool1 needs backward computation.
I0703 03:51:00.374536 13473 net.cpp:224] conv1b/relu needs backward computation.
I0703 03:51:00.374537 13473 net.cpp:224] conv1b/bn needs backward computation.
I0703 03:51:00.374539 13473 net.cpp:224] conv1b needs backward computation.
I0703 03:51:00.374542 13473 net.cpp:224] conv1a/relu needs backward computation.
I0703 03:51:00.374544 13473 net.cpp:224] conv1a/bn needs backward computation.
I0703 03:51:00.374547 13473 net.cpp:224] conv1a needs backward computation.
I0703 03:51:00.374549 13473 net.cpp:226] data/bias does not need backward computation.
I0703 03:51:00.374552 13473 net.cpp:226] label_data_1_split does not need backward computation.
I0703 03:51:00.374555 13473 net.cpp:226] data does not need backward computation.
I0703 03:51:00.374557 13473 net.cpp:268] This network produces output accuracy/top1
I0703 03:51:00.374560 13473 net.cpp:268] This network produces output accuracy/top5
I0703 03:51:00.374562 13473 net.cpp:268] This network produces output loss
I0703 03:51:00.374593 13473 net.cpp:288] Network initialization done.
I0703 03:51:00.385548 13473 caffe.cpp:289] Running for 50 iterations.
I0703 03:51:00.804659 13473 caffe.cpp:312] Batch 0, accuracy/top1 = 0.951501
I0703 03:51:00.804688 13473 caffe.cpp:312] Batch 0, accuracy/top5 = 1
I0703 03:51:00.804692 13473 caffe.cpp:312] Batch 0, loss = 0.0870306
I0703 03:51:01.174103 13473 caffe.cpp:312] Batch 1, accuracy/top1 = 0.939247
I0703 03:51:01.174125 13473 caffe.cpp:312] Batch 1, accuracy/top5 = 1
I0703 03:51:01.174129 13473 caffe.cpp:312] Batch 1, loss = 0.141391
I0703 03:51:01.543675 13473 caffe.cpp:312] Batch 2, accuracy/top1 = 0.949209
I0703 03:51:01.543700 13473 caffe.cpp:312] Batch 2, accuracy/top5 = 1
I0703 03:51:01.543704 13473 caffe.cpp:312] Batch 2, loss = 0.0885885
I0703 03:51:01.915647 13473 caffe.cpp:312] Batch 3, accuracy/top1 = 0.968707
I0703 03:51:01.915670 13473 caffe.cpp:312] Batch 3, accuracy/top5 = 0.999996
I0703 03:51:01.915673 13473 caffe.cpp:312] Batch 3, loss = 0.047841
I0703 03:51:02.286765 13473 caffe.cpp:312] Batch 4, accuracy/top1 = 0.965018
I0703 03:51:02.286789 13473 caffe.cpp:312] Batch 4, accuracy/top5 = 1
I0703 03:51:02.286793 13473 caffe.cpp:312] Batch 4, loss = 0.075163
I0703 03:51:02.657438 13473 caffe.cpp:312] Batch 5, accuracy/top1 = 0.827768
I0703 03:51:02.657459 13473 caffe.cpp:312] Batch 5, accuracy/top5 = 1
I0703 03:51:02.657464 13473 caffe.cpp:312] Batch 5, loss = 0.827146
I0703 03:51:03.028360 13473 caffe.cpp:312] Batch 6, accuracy/top1 = 0.961419
I0703 03:51:03.028381 13473 caffe.cpp:312] Batch 6, accuracy/top5 = 1
I0703 03:51:03.028385 13473 caffe.cpp:312] Batch 6, loss = 0.0657953
I0703 03:51:03.405283 13473 caffe.cpp:312] Batch 7, accuracy/top1 = 0.960613
I0703 03:51:03.405306 13473 caffe.cpp:312] Batch 7, accuracy/top5 = 1
I0703 03:51:03.405309 13473 caffe.cpp:312] Batch 7, loss = 0.0625012
I0703 03:51:03.774417 13473 caffe.cpp:312] Batch 8, accuracy/top1 = 0.972123
I0703 03:51:03.774442 13473 caffe.cpp:312] Batch 8, accuracy/top5 = 1
I0703 03:51:03.774446 13473 caffe.cpp:312] Batch 8, loss = 0.0434747
I0703 03:51:04.146528 13473 caffe.cpp:312] Batch 9, accuracy/top1 = 0.979886
I0703 03:51:04.146551 13473 caffe.cpp:312] Batch 9, accuracy/top5 = 1
I0703 03:51:04.146554 13473 caffe.cpp:312] Batch 9, loss = 0.0241364
I0703 03:51:04.519664 13473 caffe.cpp:312] Batch 10, accuracy/top1 = 0.967446
I0703 03:51:04.519687 13473 caffe.cpp:312] Batch 10, accuracy/top5 = 1
I0703 03:51:04.519690 13473 caffe.cpp:312] Batch 10, loss = 0.0464532
I0703 03:51:04.895093 13473 caffe.cpp:312] Batch 11, accuracy/top1 = 0.96884
I0703 03:51:04.895120 13473 caffe.cpp:312] Batch 11, accuracy/top5 = 1
I0703 03:51:04.895123 13473 caffe.cpp:312] Batch 11, loss = 0.0375922
I0703 03:51:05.265064 13473 caffe.cpp:312] Batch 12, accuracy/top1 = 0.965812
I0703 03:51:05.265089 13473 caffe.cpp:312] Batch 12, accuracy/top5 = 0.999998
I0703 03:51:05.265092 13473 caffe.cpp:312] Batch 12, loss = 0.0504749
I0703 03:51:05.639103 13473 caffe.cpp:312] Batch 13, accuracy/top1 = 0.96687
I0703 03:51:05.639124 13473 caffe.cpp:312] Batch 13, accuracy/top5 = 1
I0703 03:51:05.639128 13473 caffe.cpp:312] Batch 13, loss = 0.0580351
I0703 03:51:06.011339 13473 caffe.cpp:312] Batch 14, accuracy/top1 = 0.985928
I0703 03:51:06.011361 13473 caffe.cpp:312] Batch 14, accuracy/top5 = 1
I0703 03:51:06.011364 13473 caffe.cpp:312] Batch 14, loss = 0.0177087
I0703 03:51:06.383927 13473 caffe.cpp:312] Batch 15, accuracy/top1 = 0.961201
I0703 03:51:06.383949 13473 caffe.cpp:312] Batch 15, accuracy/top5 = 1
I0703 03:51:06.383952 13473 caffe.cpp:312] Batch 15, loss = 0.0578632
I0703 03:51:06.756619 13473 caffe.cpp:312] Batch 16, accuracy/top1 = 0.914202
I0703 03:51:06.756644 13473 caffe.cpp:312] Batch 16, accuracy/top5 = 1
I0703 03:51:06.756647 13473 caffe.cpp:312] Batch 16, loss = 0.16939
I0703 03:51:07.126302 13473 caffe.cpp:312] Batch 17, accuracy/top1 = 0.866948
I0703 03:51:07.126327 13473 caffe.cpp:312] Batch 17, accuracy/top5 = 1
I0703 03:51:07.126329 13473 caffe.cpp:312] Batch 17, loss = 0.598607
I0703 03:51:07.500197 13473 caffe.cpp:312] Batch 18, accuracy/top1 = 0.982352
I0703 03:51:07.500221 13473 caffe.cpp:312] Batch 18, accuracy/top5 = 0.99999
I0703 03:51:07.500224 13473 caffe.cpp:312] Batch 18, loss = 0.0207294
I0703 03:51:07.872256 13473 caffe.cpp:312] Batch 19, accuracy/top1 = 0.979822
I0703 03:51:07.872280 13473 caffe.cpp:312] Batch 19, accuracy/top5 = 1
I0703 03:51:07.872283 13473 caffe.cpp:312] Batch 19, loss = 0.0283093
I0703 03:51:08.243729 13473 caffe.cpp:312] Batch 20, accuracy/top1 = 0.973278
I0703 03:51:08.243751 13473 caffe.cpp:312] Batch 20, accuracy/top5 = 1
I0703 03:51:08.243753 13473 caffe.cpp:312] Batch 20, loss = 0.0407257
I0703 03:51:08.613893 13473 caffe.cpp:312] Batch 21, accuracy/top1 = 0.898394
I0703 03:51:08.613916 13473 caffe.cpp:312] Batch 21, accuracy/top5 = 0.999966
I0703 03:51:08.613919 13473 caffe.cpp:312] Batch 21, loss = 0.345943
I0703 03:51:08.985296 13473 caffe.cpp:312] Batch 22, accuracy/top1 = 0.964748
I0703 03:51:08.985321 13473 caffe.cpp:312] Batch 22, accuracy/top5 = 1
I0703 03:51:08.985323 13473 caffe.cpp:312] Batch 22, loss = 0.0568156
I0703 03:51:09.357311 13473 caffe.cpp:312] Batch 23, accuracy/top1 = 0.977983
I0703 03:51:09.357328 13473 caffe.cpp:312] Batch 23, accuracy/top5 = 1
I0703 03:51:09.357331 13473 caffe.cpp:312] Batch 23, loss = 0.0357582
I0703 03:51:09.730298 13473 caffe.cpp:312] Batch 24, accuracy/top1 = 0.949901
I0703 03:51:09.730321 13473 caffe.cpp:312] Batch 24, accuracy/top5 = 1
I0703 03:51:09.730324 13473 caffe.cpp:312] Batch 24, loss = 0.0879112
I0703 03:51:10.103260 13473 caffe.cpp:312] Batch 25, accuracy/top1 = 0.976265
I0703 03:51:10.103284 13473 caffe.cpp:312] Batch 25, accuracy/top5 = 1
I0703 03:51:10.103288 13473 caffe.cpp:312] Batch 25, loss = 0.0386749
I0703 03:51:10.476161 13473 caffe.cpp:312] Batch 26, accuracy/top1 = 0.951013
I0703 03:51:10.476183 13473 caffe.cpp:312] Batch 26, accuracy/top5 = 1
I0703 03:51:10.476186 13473 caffe.cpp:312] Batch 26, loss = 0.0691205
I0703 03:51:10.849323 13473 caffe.cpp:312] Batch 27, accuracy/top1 = 0.960701
I0703 03:51:10.849349 13473 caffe.cpp:312] Batch 27, accuracy/top5 = 1
I0703 03:51:10.849352 13473 caffe.cpp:312] Batch 27, loss = 0.10557
I0703 03:51:11.221597 13473 caffe.cpp:312] Batch 28, accuracy/top1 = 0.951159
I0703 03:51:11.221623 13473 caffe.cpp:312] Batch 28, accuracy/top5 = 1
I0703 03:51:11.221626 13473 caffe.cpp:312] Batch 28, loss = 0.0721942
I0703 03:51:11.596388 13473 caffe.cpp:312] Batch 29, accuracy/top1 = 0.965482
I0703 03:51:11.596408 13473 caffe.cpp:312] Batch 29, accuracy/top5 = 0.99998
I0703 03:51:11.596428 13473 caffe.cpp:312] Batch 29, loss = 0.0793494
I0703 03:51:11.969554 13473 caffe.cpp:312] Batch 30, accuracy/top1 = 0.87939
I0703 03:51:11.969579 13473 caffe.cpp:312] Batch 30, accuracy/top5 = 1
I0703 03:51:11.969583 13473 caffe.cpp:312] Batch 30, loss = 0.466501
I0703 03:51:12.340210 13473 caffe.cpp:312] Batch 31, accuracy/top1 = 0.952919
I0703 03:51:12.340231 13473 caffe.cpp:312] Batch 31, accuracy/top5 = 1
I0703 03:51:12.340235 13473 caffe.cpp:312] Batch 31, loss = 0.104198
I0703 03:51:12.712584 13473 caffe.cpp:312] Batch 32, accuracy/top1 = 0.963385
I0703 03:51:12.712606 13473 caffe.cpp:312] Batch 32, accuracy/top5 = 1
I0703 03:51:12.712610 13473 caffe.cpp:312] Batch 32, loss = 0.0499671
I0703 03:51:13.083149 13473 caffe.cpp:312] Batch 33, accuracy/top1 = 0.960217
I0703 03:51:13.083173 13473 caffe.cpp:312] Batch 33, accuracy/top5 = 1
I0703 03:51:13.083176 13473 caffe.cpp:312] Batch 33, loss = 0.0596579
I0703 03:51:13.457259 13473 caffe.cpp:312] Batch 34, accuracy/top1 = 0.978842
I0703 03:51:13.457283 13473 caffe.cpp:312] Batch 34, accuracy/top5 = 1
I0703 03:51:13.457285 13473 caffe.cpp:312] Batch 34, loss = 0.0414947
I0703 03:51:13.828137 13473 caffe.cpp:312] Batch 35, accuracy/top1 = 0.973028
I0703 03:51:13.828161 13473 caffe.cpp:312] Batch 35, accuracy/top5 = 1
I0703 03:51:13.828166 13473 caffe.cpp:312] Batch 35, loss = 0.0401477
I0703 03:51:14.199316 13473 caffe.cpp:312] Batch 36, accuracy/top1 = 0.963409
I0703 03:51:14.199338 13473 caffe.cpp:312] Batch 36, accuracy/top5 = 1
I0703 03:51:14.199342 13473 caffe.cpp:312] Batch 36, loss = 0.0572327
I0703 03:51:14.573904 13473 caffe.cpp:312] Batch 37, accuracy/top1 = 0.968069
I0703 03:51:14.573926 13473 caffe.cpp:312] Batch 37, accuracy/top5 = 1
I0703 03:51:14.573930 13473 caffe.cpp:312] Batch 37, loss = 0.0521592
I0703 03:51:14.944985 13473 caffe.cpp:312] Batch 38, accuracy/top1 = 0.955095
I0703 03:51:14.945011 13473 caffe.cpp:312] Batch 38, accuracy/top5 = 1
I0703 03:51:14.945014 13473 caffe.cpp:312] Batch 38, loss = 0.0721796
I0703 03:51:15.315963 13473 caffe.cpp:312] Batch 39, accuracy/top1 = 0.922526
I0703 03:51:15.315986 13473 caffe.cpp:312] Batch 39, accuracy/top5 = 1
I0703 03:51:15.315990 13473 caffe.cpp:312] Batch 39, loss = 0.220986
I0703 03:51:15.688567 13473 caffe.cpp:312] Batch 40, accuracy/top1 = 0.980414
I0703 03:51:15.688586 13473 caffe.cpp:312] Batch 40, accuracy/top5 = 1
I0703 03:51:15.688590 13473 caffe.cpp:312] Batch 40, loss = 0.0351581
I0703 03:51:16.059435 13473 caffe.cpp:312] Batch 41, accuracy/top1 = 0.977156
I0703 03:51:16.059458 13473 caffe.cpp:312] Batch 41, accuracy/top5 = 1
I0703 03:51:16.059460 13473 caffe.cpp:312] Batch 41, loss = 0.0312979
I0703 03:51:16.435132 13473 caffe.cpp:312] Batch 42, accuracy/top1 = 0.9642
I0703 03:51:16.435154 13473 caffe.cpp:312] Batch 42, accuracy/top5 = 1
I0703 03:51:16.435158 13473 caffe.cpp:312] Batch 42, loss = 0.0711777
I0703 03:51:16.808315 13473 caffe.cpp:312] Batch 43, accuracy/top1 = 0.974263
I0703 03:51:16.808336 13473 caffe.cpp:312] Batch 43, accuracy/top5 = 1
I0703 03:51:16.808341 13473 caffe.cpp:312] Batch 43, loss = 0.0366498
I0703 03:51:17.181905 13473 caffe.cpp:312] Batch 44, accuracy/top1 = 0.962454
I0703 03:51:17.181931 13473 caffe.cpp:312] Batch 44, accuracy/top5 = 1
I0703 03:51:17.181933 13473 caffe.cpp:312] Batch 44, loss = 0.0683088
I0703 03:51:17.555447 13473 caffe.cpp:312] Batch 45, accuracy/top1 = 0.97335
I0703 03:51:17.555471 13473 caffe.cpp:312] Batch 45, accuracy/top5 = 1
I0703 03:51:17.555474 13473 caffe.cpp:312] Batch 45, loss = 0.0567541
I0703 03:51:17.929519 13473 caffe.cpp:312] Batch 46, accuracy/top1 = 0.970272
I0703 03:51:17.929544 13473 caffe.cpp:312] Batch 46, accuracy/top5 = 1
I0703 03:51:17.929548 13473 caffe.cpp:312] Batch 46, loss = 0.0578503
I0703 03:51:18.303273 13473 caffe.cpp:312] Batch 47, accuracy/top1 = 0.961523
I0703 03:51:18.303299 13473 caffe.cpp:312] Batch 47, accuracy/top5 = 1
I0703 03:51:18.303303 13473 caffe.cpp:312] Batch 47, loss = 0.0984894
I0703 03:51:18.674923 13473 caffe.cpp:312] Batch 48, accuracy/top1 = 0.870038
I0703 03:51:18.674959 13473 caffe.cpp:312] Batch 48, accuracy/top5 = 1
I0703 03:51:18.674962 13473 caffe.cpp:312] Batch 48, loss = 0.362553
I0703 03:51:19.045553 13473 caffe.cpp:312] Batch 49, accuracy/top1 = 0.948657
I0703 03:51:19.045573 13473 caffe.cpp:312] Batch 49, accuracy/top5 = 1
I0703 03:51:19.045578 13473 caffe.cpp:312] Batch 49, loss = 0.0842121
I0703 03:51:19.045579 13473 caffe.cpp:317] Loss: 0.110945
I0703 03:51:19.045586 13473 caffe.cpp:329] accuracy/top1 = 0.954061
I0703 03:51:19.045590 13473 caffe.cpp:329] accuracy/top5 = 0.999999
I0703 03:51:19.045595 13473 caffe.cpp:329] loss = 0.110945 (* 1 = 0.110945 loss)
