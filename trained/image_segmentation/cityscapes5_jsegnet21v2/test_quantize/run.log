I0916 23:49:23.819829  4612 caffe.cpp:807] This is NVCaffe 0.16.4 started at Sat Sep 16 23:49:23 2017
I0916 23:49:23.819926  4612 caffe.cpp:810] CuDNN version: 6021
I0916 23:49:23.819931  4612 caffe.cpp:811] CuBLAS version: 8000
I0916 23:49:23.819931  4612 caffe.cpp:812] CUDA version: 8000
I0916 23:49:23.819933  4612 caffe.cpp:813] CUDA driver version: 8000
I0916 23:49:23.819937  4612 caffe.cpp:269] Not using GPU #2 for single-GPU function
I0916 23:49:23.819939  4612 caffe.cpp:269] Not using GPU #1 for single-GPU function
I0916 23:49:23.837633  4612 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0916 23:49:23.838244  4612 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I0916 23:49:23.838250  4612 caffe.cpp:281] Use GPU with device ID 0
I0916 23:49:23.838618  4612 caffe.cpp:285] GPU device name: GeForce GTX 1080
I0916 23:49:23.841143  4612 net.cpp:72] Initializing net from parameters: 
name: "jsegnet21v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/val-image-lmdb"
    label_list_path: "data/val-label-lmdb"
    batch_size: 4
    threads: 1
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
  accuracy_param {
    ignore_label: 255
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
    ignore_label: 255
  }
}
quantize: true
I0916 23:49:23.841348  4612 net.cpp:104] Using FLOAT as default forward math type
I0916 23:49:23.841356  4612 net.cpp:110] Using FLOAT as default backward math type
I0916 23:49:23.841359  4612 layer_factory.hpp:136] Creating layer 'data' of type 'ImageLabelData'
I0916 23:49:23.841363  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:23.841377  4612 net.cpp:184] Created Layer data (0)
I0916 23:49:23.841382  4612 net.cpp:530] data -> data
I0916 23:49:23.841397  4612 net.cpp:530] data -> label
I0916 23:49:23.841809  4612 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 4
I0916 23:49:23.841823  4612 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0916 23:49:23.849161  4646 db_lmdb.cpp:24] Opened lmdb data/val-image-lmdb
I0916 23:49:23.851449  4612 data_layer.cpp:187] (0) ReshapePrefetch 4, 3, 640, 640
I0916 23:49:23.851505  4612 data_layer.cpp:211] (0) Output data size: 4, 3, 640, 640
I0916 23:49:23.851513  4612 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0916 23:49:23.851579  4612 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 4
I0916 23:49:23.851594  4612 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0916 23:49:23.852391  4647 data_layer.cpp:101] (0) Parser threads: 1
I0916 23:49:23.852403  4647 data_layer.cpp:103] (0) Transformer threads: 1
I0916 23:49:23.856673  4648 db_lmdb.cpp:24] Opened lmdb data/val-label-lmdb
I0916 23:49:23.857803  4612 data_layer.cpp:187] (0) ReshapePrefetch 4, 1, 640, 640
I0916 23:49:23.857838  4612 data_layer.cpp:211] (0) Output data size: 4, 1, 640, 640
I0916 23:49:23.857846  4612 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0916 23:49:23.858032  4612 net.cpp:245] Setting up data
I0916 23:49:23.858069  4612 net.cpp:252] TEST Top shape for layer 0 'data' 4 3 640 640 (4915200)
I0916 23:49:23.858098  4612 net.cpp:252] TEST Top shape for layer 0 'data' 4 1 640 640 (1638400)
I0916 23:49:23.858109  4612 layer_factory.hpp:136] Creating layer 'label_data_1_split' of type 'Split'
I0916 23:49:23.858117  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:23.858137  4612 net.cpp:184] Created Layer label_data_1_split (1)
I0916 23:49:23.858144  4612 net.cpp:561] label_data_1_split <- label
I0916 23:49:23.858155  4612 net.cpp:530] label_data_1_split -> label_data_1_split_0
I0916 23:49:23.858165  4612 net.cpp:530] label_data_1_split -> label_data_1_split_1
I0916 23:49:23.858170  4612 net.cpp:530] label_data_1_split -> label_data_1_split_2
I0916 23:49:23.858232  4612 net.cpp:245] Setting up label_data_1_split
I0916 23:49:23.858240  4612 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 4 1 640 640 (1638400)
I0916 23:49:23.858247  4612 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 4 1 640 640 (1638400)
I0916 23:49:23.858250  4612 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 4 1 640 640 (1638400)
I0916 23:49:23.858255  4612 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0916 23:49:23.858261  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:23.858280  4612 net.cpp:184] Created Layer data/bias (2)
I0916 23:49:23.858285  4612 net.cpp:561] data/bias <- data
I0916 23:49:23.858292  4612 net.cpp:530] data/bias -> data/bias
I0916 23:49:23.859598  4649 data_layer.cpp:101] (0) Parser threads: 1
I0916 23:49:23.859627  4649 data_layer.cpp:103] (0) Transformer threads: 1
I0916 23:49:23.862989  4612 net.cpp:245] Setting up data/bias
I0916 23:49:23.863036  4612 net.cpp:252] TEST Top shape for layer 2 'data/bias' 4 3 640 640 (4915200)
I0916 23:49:23.863062  4612 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0916 23:49:23.863070  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:23.863099  4612 net.cpp:184] Created Layer conv1a (3)
I0916 23:49:23.863104  4612 net.cpp:561] conv1a <- data/bias
I0916 23:49:23.863123  4612 net.cpp:530] conv1a -> conv1a
I0916 23:49:24.163347  4612 net.cpp:245] Setting up conv1a
I0916 23:49:24.163369  4612 net.cpp:252] TEST Top shape for layer 3 'conv1a' 4 32 320 320 (13107200)
I0916 23:49:24.163381  4612 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0916 23:49:24.163385  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.163398  4612 net.cpp:184] Created Layer conv1a/bn (4)
I0916 23:49:24.163400  4612 net.cpp:561] conv1a/bn <- conv1a
I0916 23:49:24.163404  4612 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0916 23:49:24.163902  4612 net.cpp:245] Setting up conv1a/bn
I0916 23:49:24.163911  4612 net.cpp:252] TEST Top shape for layer 4 'conv1a/bn' 4 32 320 320 (13107200)
I0916 23:49:24.163918  4612 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0916 23:49:24.163921  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.163925  4612 net.cpp:184] Created Layer conv1a/relu (5)
I0916 23:49:24.163928  4612 net.cpp:561] conv1a/relu <- conv1a
I0916 23:49:24.163930  4612 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0916 23:49:24.163938  4612 net.cpp:245] Setting up conv1a/relu
I0916 23:49:24.163941  4612 net.cpp:252] TEST Top shape for layer 5 'conv1a/relu' 4 32 320 320 (13107200)
I0916 23:49:24.163944  4612 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0916 23:49:24.163945  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.163956  4612 net.cpp:184] Created Layer conv1b (6)
I0916 23:49:24.163959  4612 net.cpp:561] conv1b <- conv1a
I0916 23:49:24.163962  4612 net.cpp:530] conv1b -> conv1b
I0916 23:49:24.165208  4612 net.cpp:245] Setting up conv1b
I0916 23:49:24.165217  4612 net.cpp:252] TEST Top shape for layer 6 'conv1b' 4 32 320 320 (13107200)
I0916 23:49:24.165223  4612 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0916 23:49:24.165226  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.165231  4612 net.cpp:184] Created Layer conv1b/bn (7)
I0916 23:49:24.165233  4612 net.cpp:561] conv1b/bn <- conv1b
I0916 23:49:24.165236  4612 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0916 23:49:24.165621  4612 net.cpp:245] Setting up conv1b/bn
I0916 23:49:24.165628  4612 net.cpp:252] TEST Top shape for layer 7 'conv1b/bn' 4 32 320 320 (13107200)
I0916 23:49:24.165633  4612 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0916 23:49:24.165637  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.165639  4612 net.cpp:184] Created Layer conv1b/relu (8)
I0916 23:49:24.165642  4612 net.cpp:561] conv1b/relu <- conv1b
I0916 23:49:24.165643  4612 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0916 23:49:24.165647  4612 net.cpp:245] Setting up conv1b/relu
I0916 23:49:24.165649  4612 net.cpp:252] TEST Top shape for layer 8 'conv1b/relu' 4 32 320 320 (13107200)
I0916 23:49:24.165652  4612 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0916 23:49:24.165654  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.165662  4612 net.cpp:184] Created Layer pool1 (9)
I0916 23:49:24.165664  4612 net.cpp:561] pool1 <- conv1b
I0916 23:49:24.165668  4612 net.cpp:530] pool1 -> pool1
I0916 23:49:24.165714  4612 net.cpp:245] Setting up pool1
I0916 23:49:24.165721  4612 net.cpp:252] TEST Top shape for layer 9 'pool1' 4 32 160 160 (3276800)
I0916 23:49:24.165725  4612 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0916 23:49:24.165729  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.165740  4612 net.cpp:184] Created Layer res2a_branch2a (10)
I0916 23:49:24.165745  4612 net.cpp:561] res2a_branch2a <- pool1
I0916 23:49:24.165748  4612 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0916 23:49:24.167438  4612 net.cpp:245] Setting up res2a_branch2a
I0916 23:49:24.167446  4612 net.cpp:252] TEST Top shape for layer 10 'res2a_branch2a' 4 64 160 160 (6553600)
I0916 23:49:24.167454  4612 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0916 23:49:24.167456  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.167461  4612 net.cpp:184] Created Layer res2a_branch2a/bn (11)
I0916 23:49:24.167464  4612 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0916 23:49:24.167469  4612 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0916 23:49:24.167914  4612 net.cpp:245] Setting up res2a_branch2a/bn
I0916 23:49:24.167922  4612 net.cpp:252] TEST Top shape for layer 11 'res2a_branch2a/bn' 4 64 160 160 (6553600)
I0916 23:49:24.167928  4612 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0916 23:49:24.167932  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.167935  4612 net.cpp:184] Created Layer res2a_branch2a/relu (12)
I0916 23:49:24.167938  4612 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0916 23:49:24.167942  4612 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0916 23:49:24.167945  4612 net.cpp:245] Setting up res2a_branch2a/relu
I0916 23:49:24.167949  4612 net.cpp:252] TEST Top shape for layer 12 'res2a_branch2a/relu' 4 64 160 160 (6553600)
I0916 23:49:24.167951  4612 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0916 23:49:24.167954  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.167963  4612 net.cpp:184] Created Layer res2a_branch2b (13)
I0916 23:49:24.167968  4612 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0916 23:49:24.167973  4612 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0916 23:49:24.169011  4612 net.cpp:245] Setting up res2a_branch2b
I0916 23:49:24.169020  4612 net.cpp:252] TEST Top shape for layer 13 'res2a_branch2b' 4 64 160 160 (6553600)
I0916 23:49:24.169025  4612 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0916 23:49:24.169029  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.169034  4612 net.cpp:184] Created Layer res2a_branch2b/bn (14)
I0916 23:49:24.169037  4612 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0916 23:49:24.169040  4612 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0916 23:49:24.169493  4612 net.cpp:245] Setting up res2a_branch2b/bn
I0916 23:49:24.169502  4612 net.cpp:252] TEST Top shape for layer 14 'res2a_branch2b/bn' 4 64 160 160 (6553600)
I0916 23:49:24.169508  4612 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0916 23:49:24.169512  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.169515  4612 net.cpp:184] Created Layer res2a_branch2b/relu (15)
I0916 23:49:24.169518  4612 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0916 23:49:24.169522  4612 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0916 23:49:24.169525  4612 net.cpp:245] Setting up res2a_branch2b/relu
I0916 23:49:24.169528  4612 net.cpp:252] TEST Top shape for layer 15 'res2a_branch2b/relu' 4 64 160 160 (6553600)
I0916 23:49:24.169531  4612 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0916 23:49:24.169534  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.169539  4612 net.cpp:184] Created Layer pool2 (16)
I0916 23:49:24.169544  4612 net.cpp:561] pool2 <- res2a_branch2b
I0916 23:49:24.169548  4612 net.cpp:530] pool2 -> pool2
I0916 23:49:24.169589  4612 net.cpp:245] Setting up pool2
I0916 23:49:24.169595  4612 net.cpp:252] TEST Top shape for layer 16 'pool2' 4 64 80 80 (1638400)
I0916 23:49:24.169600  4612 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0916 23:49:24.169605  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.169622  4612 net.cpp:184] Created Layer res3a_branch2a (17)
I0916 23:49:24.169627  4612 net.cpp:561] res3a_branch2a <- pool2
I0916 23:49:24.169632  4612 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0916 23:49:24.171386  4612 net.cpp:245] Setting up res3a_branch2a
I0916 23:49:24.171394  4612 net.cpp:252] TEST Top shape for layer 17 'res3a_branch2a' 4 128 80 80 (3276800)
I0916 23:49:24.171399  4612 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0916 23:49:24.171402  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.171407  4612 net.cpp:184] Created Layer res3a_branch2a/bn (18)
I0916 23:49:24.171411  4612 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0916 23:49:24.171413  4612 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0916 23:49:24.172243  4612 net.cpp:245] Setting up res3a_branch2a/bn
I0916 23:49:24.172251  4612 net.cpp:252] TEST Top shape for layer 18 'res3a_branch2a/bn' 4 128 80 80 (3276800)
I0916 23:49:24.172260  4612 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0916 23:49:24.172263  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.172267  4612 net.cpp:184] Created Layer res3a_branch2a/relu (19)
I0916 23:49:24.172271  4612 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0916 23:49:24.172273  4612 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0916 23:49:24.172277  4612 net.cpp:245] Setting up res3a_branch2a/relu
I0916 23:49:24.172281  4612 net.cpp:252] TEST Top shape for layer 19 'res3a_branch2a/relu' 4 128 80 80 (3276800)
I0916 23:49:24.172283  4612 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0916 23:49:24.172287  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.172300  4612 net.cpp:184] Created Layer res3a_branch2b (20)
I0916 23:49:24.172305  4612 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0916 23:49:24.172309  4612 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0916 23:49:24.173300  4612 net.cpp:245] Setting up res3a_branch2b
I0916 23:49:24.173307  4612 net.cpp:252] TEST Top shape for layer 20 'res3a_branch2b' 4 128 80 80 (3276800)
I0916 23:49:24.173312  4612 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0916 23:49:24.173316  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.173321  4612 net.cpp:184] Created Layer res3a_branch2b/bn (21)
I0916 23:49:24.173323  4612 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0916 23:49:24.173326  4612 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0916 23:49:24.173754  4612 net.cpp:245] Setting up res3a_branch2b/bn
I0916 23:49:24.173763  4612 net.cpp:252] TEST Top shape for layer 21 'res3a_branch2b/bn' 4 128 80 80 (3276800)
I0916 23:49:24.173768  4612 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0916 23:49:24.173771  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.173775  4612 net.cpp:184] Created Layer res3a_branch2b/relu (22)
I0916 23:49:24.173779  4612 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0916 23:49:24.173781  4612 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0916 23:49:24.173785  4612 net.cpp:245] Setting up res3a_branch2b/relu
I0916 23:49:24.173789  4612 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b/relu' 4 128 80 80 (3276800)
I0916 23:49:24.173792  4612 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0916 23:49:24.173796  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.173802  4612 net.cpp:184] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (23)
I0916 23:49:24.173806  4612 net.cpp:561] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0916 23:49:24.173818  4612 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0916 23:49:24.173825  4612 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0916 23:49:24.173856  4612 net.cpp:245] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0916 23:49:24.173861  4612 net.cpp:252] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 4 128 80 80 (3276800)
I0916 23:49:24.173867  4612 net.cpp:252] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 4 128 80 80 (3276800)
I0916 23:49:24.173871  4612 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0916 23:49:24.173877  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.173882  4612 net.cpp:184] Created Layer pool3 (24)
I0916 23:49:24.173887  4612 net.cpp:561] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0916 23:49:24.173890  4612 net.cpp:530] pool3 -> pool3
I0916 23:49:24.173931  4612 net.cpp:245] Setting up pool3
I0916 23:49:24.173938  4612 net.cpp:252] TEST Top shape for layer 24 'pool3' 4 128 40 40 (819200)
I0916 23:49:24.173943  4612 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0916 23:49:24.173948  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.173956  4612 net.cpp:184] Created Layer res4a_branch2a (25)
I0916 23:49:24.173961  4612 net.cpp:561] res4a_branch2a <- pool3
I0916 23:49:24.173965  4612 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0916 23:49:24.181043  4612 net.cpp:245] Setting up res4a_branch2a
I0916 23:49:24.181054  4612 net.cpp:252] TEST Top shape for layer 25 'res4a_branch2a' 4 256 40 40 (1638400)
I0916 23:49:24.181059  4612 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0916 23:49:24.181064  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.181072  4612 net.cpp:184] Created Layer res4a_branch2a/bn (26)
I0916 23:49:24.181077  4612 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0916 23:49:24.181082  4612 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0916 23:49:24.181522  4612 net.cpp:245] Setting up res4a_branch2a/bn
I0916 23:49:24.181530  4612 net.cpp:252] TEST Top shape for layer 26 'res4a_branch2a/bn' 4 256 40 40 (1638400)
I0916 23:49:24.181538  4612 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0916 23:49:24.181543  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.181550  4612 net.cpp:184] Created Layer res4a_branch2a/relu (27)
I0916 23:49:24.181555  4612 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0916 23:49:24.181558  4612 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0916 23:49:24.181565  4612 net.cpp:245] Setting up res4a_branch2a/relu
I0916 23:49:24.181571  4612 net.cpp:252] TEST Top shape for layer 27 'res4a_branch2a/relu' 4 256 40 40 (1638400)
I0916 23:49:24.181576  4612 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0916 23:49:24.181579  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.181588  4612 net.cpp:184] Created Layer res4a_branch2b (28)
I0916 23:49:24.181592  4612 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0916 23:49:24.181596  4612 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0916 23:49:24.184746  4612 net.cpp:245] Setting up res4a_branch2b
I0916 23:49:24.184754  4612 net.cpp:252] TEST Top shape for layer 28 'res4a_branch2b' 4 256 40 40 (1638400)
I0916 23:49:24.184761  4612 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0916 23:49:24.184764  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.184772  4612 net.cpp:184] Created Layer res4a_branch2b/bn (29)
I0916 23:49:24.184777  4612 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0916 23:49:24.184790  4612 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0916 23:49:24.185238  4612 net.cpp:245] Setting up res4a_branch2b/bn
I0916 23:49:24.185245  4612 net.cpp:252] TEST Top shape for layer 29 'res4a_branch2b/bn' 4 256 40 40 (1638400)
I0916 23:49:24.185253  4612 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0916 23:49:24.185258  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.185264  4612 net.cpp:184] Created Layer res4a_branch2b/relu (30)
I0916 23:49:24.185269  4612 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0916 23:49:24.185274  4612 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0916 23:49:24.185281  4612 net.cpp:245] Setting up res4a_branch2b/relu
I0916 23:49:24.185286  4612 net.cpp:252] TEST Top shape for layer 30 'res4a_branch2b/relu' 4 256 40 40 (1638400)
I0916 23:49:24.185290  4612 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0916 23:49:24.185294  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.185302  4612 net.cpp:184] Created Layer pool4 (31)
I0916 23:49:24.185305  4612 net.cpp:561] pool4 <- res4a_branch2b
I0916 23:49:24.185310  4612 net.cpp:530] pool4 -> pool4
I0916 23:49:24.185351  4612 net.cpp:245] Setting up pool4
I0916 23:49:24.185359  4612 net.cpp:252] TEST Top shape for layer 31 'pool4' 4 256 40 40 (1638400)
I0916 23:49:24.185362  4612 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0916 23:49:24.185367  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.185379  4612 net.cpp:184] Created Layer res5a_branch2a (32)
I0916 23:49:24.185384  4612 net.cpp:561] res5a_branch2a <- pool4
I0916 23:49:24.185387  4612 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0916 23:49:24.210741  4612 net.cpp:245] Setting up res5a_branch2a
I0916 23:49:24.210759  4612 net.cpp:252] TEST Top shape for layer 32 'res5a_branch2a' 4 512 40 40 (3276800)
I0916 23:49:24.210767  4612 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0916 23:49:24.210772  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.210778  4612 net.cpp:184] Created Layer res5a_branch2a/bn (33)
I0916 23:49:24.210781  4612 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0916 23:49:24.210785  4612 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0916 23:49:24.211252  4612 net.cpp:245] Setting up res5a_branch2a/bn
I0916 23:49:24.211261  4612 net.cpp:252] TEST Top shape for layer 33 'res5a_branch2a/bn' 4 512 40 40 (3276800)
I0916 23:49:24.211267  4612 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0916 23:49:24.211271  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.211273  4612 net.cpp:184] Created Layer res5a_branch2a/relu (34)
I0916 23:49:24.211277  4612 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0916 23:49:24.211278  4612 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0916 23:49:24.211282  4612 net.cpp:245] Setting up res5a_branch2a/relu
I0916 23:49:24.211285  4612 net.cpp:252] TEST Top shape for layer 34 'res5a_branch2a/relu' 4 512 40 40 (3276800)
I0916 23:49:24.211287  4612 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0916 23:49:24.211289  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.211295  4612 net.cpp:184] Created Layer res5a_branch2b (35)
I0916 23:49:24.211299  4612 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0916 23:49:24.211303  4612 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0916 23:49:24.223875  4612 net.cpp:245] Setting up res5a_branch2b
I0916 23:49:24.223896  4612 net.cpp:252] TEST Top shape for layer 35 'res5a_branch2b' 4 512 40 40 (3276800)
I0916 23:49:24.223906  4612 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0916 23:49:24.223924  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.223933  4612 net.cpp:184] Created Layer res5a_branch2b/bn (36)
I0916 23:49:24.223937  4612 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0916 23:49:24.223940  4612 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0916 23:49:24.224409  4612 net.cpp:245] Setting up res5a_branch2b/bn
I0916 23:49:24.224417  4612 net.cpp:252] TEST Top shape for layer 36 'res5a_branch2b/bn' 4 512 40 40 (3276800)
I0916 23:49:24.224423  4612 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0916 23:49:24.224426  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.224431  4612 net.cpp:184] Created Layer res5a_branch2b/relu (37)
I0916 23:49:24.224432  4612 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0916 23:49:24.224434  4612 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0916 23:49:24.224439  4612 net.cpp:245] Setting up res5a_branch2b/relu
I0916 23:49:24.224442  4612 net.cpp:252] TEST Top shape for layer 37 'res5a_branch2b/relu' 4 512 40 40 (3276800)
I0916 23:49:24.224443  4612 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I0916 23:49:24.224447  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.224452  4612 net.cpp:184] Created Layer out5a (38)
I0916 23:49:24.224454  4612 net.cpp:561] out5a <- res5a_branch2b
I0916 23:49:24.224457  4612 net.cpp:530] out5a -> out5a
I0916 23:49:24.228299  4612 net.cpp:245] Setting up out5a
I0916 23:49:24.228309  4612 net.cpp:252] TEST Top shape for layer 38 'out5a' 4 64 40 40 (409600)
I0916 23:49:24.228314  4612 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I0916 23:49:24.228317  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.228322  4612 net.cpp:184] Created Layer out5a/bn (39)
I0916 23:49:24.228323  4612 net.cpp:561] out5a/bn <- out5a
I0916 23:49:24.228327  4612 net.cpp:513] out5a/bn -> out5a (in-place)
I0916 23:49:24.228778  4612 net.cpp:245] Setting up out5a/bn
I0916 23:49:24.228786  4612 net.cpp:252] TEST Top shape for layer 39 'out5a/bn' 4 64 40 40 (409600)
I0916 23:49:24.228791  4612 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I0916 23:49:24.228794  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.228797  4612 net.cpp:184] Created Layer out5a/relu (40)
I0916 23:49:24.228799  4612 net.cpp:561] out5a/relu <- out5a
I0916 23:49:24.228801  4612 net.cpp:513] out5a/relu -> out5a (in-place)
I0916 23:49:24.228806  4612 net.cpp:245] Setting up out5a/relu
I0916 23:49:24.228807  4612 net.cpp:252] TEST Top shape for layer 40 'out5a/relu' 4 64 40 40 (409600)
I0916 23:49:24.228809  4612 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I0916 23:49:24.228812  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.228823  4612 net.cpp:184] Created Layer out5a_up2 (41)
I0916 23:49:24.228826  4612 net.cpp:561] out5a_up2 <- out5a
I0916 23:49:24.228829  4612 net.cpp:530] out5a_up2 -> out5a_up2
I0916 23:49:24.228991  4612 net.cpp:245] Setting up out5a_up2
I0916 23:49:24.229001  4612 net.cpp:252] TEST Top shape for layer 41 'out5a_up2' 4 64 80 80 (1638400)
I0916 23:49:24.229005  4612 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I0916 23:49:24.229009  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.229018  4612 net.cpp:184] Created Layer out3a (42)
I0916 23:49:24.229022  4612 net.cpp:561] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0916 23:49:24.229027  4612 net.cpp:530] out3a -> out3a
I0916 23:49:24.230062  4612 net.cpp:245] Setting up out3a
I0916 23:49:24.230070  4612 net.cpp:252] TEST Top shape for layer 42 'out3a' 4 64 80 80 (1638400)
I0916 23:49:24.230087  4612 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I0916 23:49:24.230090  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.230095  4612 net.cpp:184] Created Layer out3a/bn (43)
I0916 23:49:24.230098  4612 net.cpp:561] out3a/bn <- out3a
I0916 23:49:24.230100  4612 net.cpp:513] out3a/bn -> out3a (in-place)
I0916 23:49:24.230556  4612 net.cpp:245] Setting up out3a/bn
I0916 23:49:24.230564  4612 net.cpp:252] TEST Top shape for layer 43 'out3a/bn' 4 64 80 80 (1638400)
I0916 23:49:24.230571  4612 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I0916 23:49:24.230572  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.230576  4612 net.cpp:184] Created Layer out3a/relu (44)
I0916 23:49:24.230578  4612 net.cpp:561] out3a/relu <- out3a
I0916 23:49:24.230581  4612 net.cpp:513] out3a/relu -> out3a (in-place)
I0916 23:49:24.230584  4612 net.cpp:245] Setting up out3a/relu
I0916 23:49:24.230587  4612 net.cpp:252] TEST Top shape for layer 44 'out3a/relu' 4 64 80 80 (1638400)
I0916 23:49:24.230588  4612 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I0916 23:49:24.230592  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.230598  4612 net.cpp:184] Created Layer out3_out5_combined (45)
I0916 23:49:24.230602  4612 net.cpp:561] out3_out5_combined <- out5a_up2
I0916 23:49:24.230604  4612 net.cpp:561] out3_out5_combined <- out3a
I0916 23:49:24.230607  4612 net.cpp:530] out3_out5_combined -> out3_out5_combined
I0916 23:49:24.230623  4612 net.cpp:245] Setting up out3_out5_combined
I0916 23:49:24.230626  4612 net.cpp:252] TEST Top shape for layer 45 'out3_out5_combined' 4 64 80 80 (1638400)
I0916 23:49:24.230628  4612 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I0916 23:49:24.230630  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.230640  4612 net.cpp:184] Created Layer ctx_conv1 (46)
I0916 23:49:24.230644  4612 net.cpp:561] ctx_conv1 <- out3_out5_combined
I0916 23:49:24.230649  4612 net.cpp:530] ctx_conv1 -> ctx_conv1
I0916 23:49:24.231606  4612 net.cpp:245] Setting up ctx_conv1
I0916 23:49:24.231613  4612 net.cpp:252] TEST Top shape for layer 46 'ctx_conv1' 4 64 80 80 (1638400)
I0916 23:49:24.231617  4612 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I0916 23:49:24.231621  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.231624  4612 net.cpp:184] Created Layer ctx_conv1/bn (47)
I0916 23:49:24.231626  4612 net.cpp:561] ctx_conv1/bn <- ctx_conv1
I0916 23:49:24.231629  4612 net.cpp:513] ctx_conv1/bn -> ctx_conv1 (in-place)
I0916 23:49:24.232045  4612 net.cpp:245] Setting up ctx_conv1/bn
I0916 23:49:24.232053  4612 net.cpp:252] TEST Top shape for layer 47 'ctx_conv1/bn' 4 64 80 80 (1638400)
I0916 23:49:24.232059  4612 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I0916 23:49:24.232061  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.232064  4612 net.cpp:184] Created Layer ctx_conv1/relu (48)
I0916 23:49:24.232066  4612 net.cpp:561] ctx_conv1/relu <- ctx_conv1
I0916 23:49:24.232069  4612 net.cpp:513] ctx_conv1/relu -> ctx_conv1 (in-place)
I0916 23:49:24.232072  4612 net.cpp:245] Setting up ctx_conv1/relu
I0916 23:49:24.232075  4612 net.cpp:252] TEST Top shape for layer 48 'ctx_conv1/relu' 4 64 80 80 (1638400)
I0916 23:49:24.232076  4612 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I0916 23:49:24.232079  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.232084  4612 net.cpp:184] Created Layer ctx_conv2 (49)
I0916 23:49:24.232086  4612 net.cpp:561] ctx_conv2 <- ctx_conv1
I0916 23:49:24.232089  4612 net.cpp:530] ctx_conv2 -> ctx_conv2
I0916 23:49:24.233036  4612 net.cpp:245] Setting up ctx_conv2
I0916 23:49:24.233043  4612 net.cpp:252] TEST Top shape for layer 49 'ctx_conv2' 4 64 80 80 (1638400)
I0916 23:49:24.233047  4612 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I0916 23:49:24.233050  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.233054  4612 net.cpp:184] Created Layer ctx_conv2/bn (50)
I0916 23:49:24.233057  4612 net.cpp:561] ctx_conv2/bn <- ctx_conv2
I0916 23:49:24.233058  4612 net.cpp:513] ctx_conv2/bn -> ctx_conv2 (in-place)
I0916 23:49:24.233541  4612 net.cpp:245] Setting up ctx_conv2/bn
I0916 23:49:24.233548  4612 net.cpp:252] TEST Top shape for layer 50 'ctx_conv2/bn' 4 64 80 80 (1638400)
I0916 23:49:24.233553  4612 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I0916 23:49:24.233556  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.233559  4612 net.cpp:184] Created Layer ctx_conv2/relu (51)
I0916 23:49:24.233562  4612 net.cpp:561] ctx_conv2/relu <- ctx_conv2
I0916 23:49:24.233564  4612 net.cpp:513] ctx_conv2/relu -> ctx_conv2 (in-place)
I0916 23:49:24.233568  4612 net.cpp:245] Setting up ctx_conv2/relu
I0916 23:49:24.233570  4612 net.cpp:252] TEST Top shape for layer 51 'ctx_conv2/relu' 4 64 80 80 (1638400)
I0916 23:49:24.233572  4612 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I0916 23:49:24.233574  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.233582  4612 net.cpp:184] Created Layer ctx_conv3 (52)
I0916 23:49:24.233583  4612 net.cpp:561] ctx_conv3 <- ctx_conv2
I0916 23:49:24.233587  4612 net.cpp:530] ctx_conv3 -> ctx_conv3
I0916 23:49:24.234589  4612 net.cpp:245] Setting up ctx_conv3
I0916 23:49:24.234597  4612 net.cpp:252] TEST Top shape for layer 52 'ctx_conv3' 4 64 80 80 (1638400)
I0916 23:49:24.234601  4612 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I0916 23:49:24.234604  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.234608  4612 net.cpp:184] Created Layer ctx_conv3/bn (53)
I0916 23:49:24.234611  4612 net.cpp:561] ctx_conv3/bn <- ctx_conv3
I0916 23:49:24.234613  4612 net.cpp:513] ctx_conv3/bn -> ctx_conv3 (in-place)
I0916 23:49:24.235077  4612 net.cpp:245] Setting up ctx_conv3/bn
I0916 23:49:24.235085  4612 net.cpp:252] TEST Top shape for layer 53 'ctx_conv3/bn' 4 64 80 80 (1638400)
I0916 23:49:24.235091  4612 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I0916 23:49:24.235093  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.235097  4612 net.cpp:184] Created Layer ctx_conv3/relu (54)
I0916 23:49:24.235100  4612 net.cpp:561] ctx_conv3/relu <- ctx_conv3
I0916 23:49:24.235102  4612 net.cpp:513] ctx_conv3/relu -> ctx_conv3 (in-place)
I0916 23:49:24.235106  4612 net.cpp:245] Setting up ctx_conv3/relu
I0916 23:49:24.235110  4612 net.cpp:252] TEST Top shape for layer 54 'ctx_conv3/relu' 4 64 80 80 (1638400)
I0916 23:49:24.235111  4612 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I0916 23:49:24.235113  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.235121  4612 net.cpp:184] Created Layer ctx_conv4 (55)
I0916 23:49:24.235122  4612 net.cpp:561] ctx_conv4 <- ctx_conv3
I0916 23:49:24.235126  4612 net.cpp:530] ctx_conv4 -> ctx_conv4
I0916 23:49:24.236124  4612 net.cpp:245] Setting up ctx_conv4
I0916 23:49:24.236131  4612 net.cpp:252] TEST Top shape for layer 55 'ctx_conv4' 4 64 80 80 (1638400)
I0916 23:49:24.236136  4612 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I0916 23:49:24.236140  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.236145  4612 net.cpp:184] Created Layer ctx_conv4/bn (56)
I0916 23:49:24.236155  4612 net.cpp:561] ctx_conv4/bn <- ctx_conv4
I0916 23:49:24.236157  4612 net.cpp:513] ctx_conv4/bn -> ctx_conv4 (in-place)
I0916 23:49:24.236630  4612 net.cpp:245] Setting up ctx_conv4/bn
I0916 23:49:24.236639  4612 net.cpp:252] TEST Top shape for layer 56 'ctx_conv4/bn' 4 64 80 80 (1638400)
I0916 23:49:24.236644  4612 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I0916 23:49:24.236647  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.236650  4612 net.cpp:184] Created Layer ctx_conv4/relu (57)
I0916 23:49:24.236654  4612 net.cpp:561] ctx_conv4/relu <- ctx_conv4
I0916 23:49:24.236656  4612 net.cpp:513] ctx_conv4/relu -> ctx_conv4 (in-place)
I0916 23:49:24.236660  4612 net.cpp:245] Setting up ctx_conv4/relu
I0916 23:49:24.236663  4612 net.cpp:252] TEST Top shape for layer 57 'ctx_conv4/relu' 4 64 80 80 (1638400)
I0916 23:49:24.236665  4612 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I0916 23:49:24.236668  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.236680  4612 net.cpp:184] Created Layer ctx_final (58)
I0916 23:49:24.236685  4612 net.cpp:561] ctx_final <- ctx_conv4
I0916 23:49:24.236688  4612 net.cpp:530] ctx_final -> ctx_final
I0916 23:49:24.237036  4612 net.cpp:245] Setting up ctx_final
I0916 23:49:24.237045  4612 net.cpp:252] TEST Top shape for layer 58 'ctx_final' 4 8 80 80 (204800)
I0916 23:49:24.237049  4612 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I0916 23:49:24.237052  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.237056  4612 net.cpp:184] Created Layer ctx_final/relu (59)
I0916 23:49:24.237058  4612 net.cpp:561] ctx_final/relu <- ctx_final
I0916 23:49:24.237062  4612 net.cpp:513] ctx_final/relu -> ctx_final (in-place)
I0916 23:49:24.237069  4612 net.cpp:245] Setting up ctx_final/relu
I0916 23:49:24.237074  4612 net.cpp:252] TEST Top shape for layer 59 'ctx_final/relu' 4 8 80 80 (204800)
I0916 23:49:24.237079  4612 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I0916 23:49:24.237083  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.237092  4612 net.cpp:184] Created Layer out_deconv_final_up2 (60)
I0916 23:49:24.237097  4612 net.cpp:561] out_deconv_final_up2 <- ctx_final
I0916 23:49:24.237102  4612 net.cpp:530] out_deconv_final_up2 -> out_deconv_final_up2
I0916 23:49:24.237234  4612 net.cpp:245] Setting up out_deconv_final_up2
I0916 23:49:24.237242  4612 net.cpp:252] TEST Top shape for layer 60 'out_deconv_final_up2' 4 8 160 160 (819200)
I0916 23:49:24.237248  4612 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I0916 23:49:24.237253  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.237262  4612 net.cpp:184] Created Layer out_deconv_final_up4 (61)
I0916 23:49:24.237265  4612 net.cpp:561] out_deconv_final_up4 <- out_deconv_final_up2
I0916 23:49:24.237270  4612 net.cpp:530] out_deconv_final_up4 -> out_deconv_final_up4
I0916 23:49:24.237435  4612 net.cpp:245] Setting up out_deconv_final_up4
I0916 23:49:24.237442  4612 net.cpp:252] TEST Top shape for layer 61 'out_deconv_final_up4' 4 8 320 320 (3276800)
I0916 23:49:24.237447  4612 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I0916 23:49:24.237452  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.237459  4612 net.cpp:184] Created Layer out_deconv_final_up8 (62)
I0916 23:49:24.237463  4612 net.cpp:561] out_deconv_final_up8 <- out_deconv_final_up4
I0916 23:49:24.237468  4612 net.cpp:530] out_deconv_final_up8 -> out_deconv_final_up8
I0916 23:49:24.237596  4612 net.cpp:245] Setting up out_deconv_final_up8
I0916 23:49:24.237603  4612 net.cpp:252] TEST Top shape for layer 62 'out_deconv_final_up8' 4 8 640 640 (13107200)
I0916 23:49:24.237615  4612 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8_out_deconv_final_up8_0_split' of type 'Split'
I0916 23:49:24.237620  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.237627  4612 net.cpp:184] Created Layer out_deconv_final_up8_out_deconv_final_up8_0_split (63)
I0916 23:49:24.237632  4612 net.cpp:561] out_deconv_final_up8_out_deconv_final_up8_0_split <- out_deconv_final_up8
I0916 23:49:24.237635  4612 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0916 23:49:24.237642  4612 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0916 23:49:24.237646  4612 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0916 23:49:24.237687  4612 net.cpp:245] Setting up out_deconv_final_up8_out_deconv_final_up8_0_split
I0916 23:49:24.237694  4612 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 4 8 640 640 (13107200)
I0916 23:49:24.237699  4612 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 4 8 640 640 (13107200)
I0916 23:49:24.237704  4612 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 4 8 640 640 (13107200)
I0916 23:49:24.237709  4612 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0916 23:49:24.237712  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.237726  4612 net.cpp:184] Created Layer loss (64)
I0916 23:49:24.237731  4612 net.cpp:561] loss <- out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0916 23:49:24.237737  4612 net.cpp:561] loss <- label_data_1_split_0
I0916 23:49:24.237742  4612 net.cpp:530] loss -> loss
I0916 23:49:24.238890  4612 net.cpp:245] Setting up loss
I0916 23:49:24.238900  4612 net.cpp:252] TEST Top shape for layer 64 'loss' (1)
I0916 23:49:24.238905  4612 net.cpp:256]     with loss weight 1
I0916 23:49:24.238921  4612 layer_factory.hpp:136] Creating layer 'accuracy/top1' of type 'Accuracy'
I0916 23:49:24.238926  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.238935  4612 net.cpp:184] Created Layer accuracy/top1 (65)
I0916 23:49:24.238940  4612 net.cpp:561] accuracy/top1 <- out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0916 23:49:24.238946  4612 net.cpp:561] accuracy/top1 <- label_data_1_split_1
I0916 23:49:24.238951  4612 net.cpp:530] accuracy/top1 -> accuracy/top1
I0916 23:49:24.238960  4612 net.cpp:245] Setting up accuracy/top1
I0916 23:49:24.238965  4612 net.cpp:252] TEST Top shape for layer 65 'accuracy/top1' (1)
I0916 23:49:24.238970  4612 layer_factory.hpp:136] Creating layer 'accuracy/top5' of type 'Accuracy'
I0916 23:49:24.238976  4612 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0916 23:49:24.238981  4612 net.cpp:184] Created Layer accuracy/top5 (66)
I0916 23:49:24.238986  4612 net.cpp:561] accuracy/top5 <- out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0916 23:49:24.238989  4612 net.cpp:561] accuracy/top5 <- label_data_1_split_2
I0916 23:49:24.238994  4612 net.cpp:530] accuracy/top5 -> accuracy/top5
I0916 23:49:24.239001  4612 net.cpp:245] Setting up accuracy/top5
I0916 23:49:24.239006  4612 net.cpp:252] TEST Top shape for layer 66 'accuracy/top5' (1)
I0916 23:49:24.239007  4612 net.cpp:325] accuracy/top5 does not need backward computation.
I0916 23:49:24.239011  4612 net.cpp:325] accuracy/top1 does not need backward computation.
I0916 23:49:24.239012  4612 net.cpp:323] loss needs backward computation.
I0916 23:49:24.239015  4612 net.cpp:323] out_deconv_final_up8_out_deconv_final_up8_0_split needs backward computation.
I0916 23:49:24.239018  4612 net.cpp:323] out_deconv_final_up8 needs backward computation.
I0916 23:49:24.239020  4612 net.cpp:323] out_deconv_final_up4 needs backward computation.
I0916 23:49:24.239028  4612 net.cpp:323] out_deconv_final_up2 needs backward computation.
I0916 23:49:24.239032  4612 net.cpp:323] ctx_final/relu needs backward computation.
I0916 23:49:24.239033  4612 net.cpp:323] ctx_final needs backward computation.
I0916 23:49:24.239035  4612 net.cpp:323] ctx_conv4/relu needs backward computation.
I0916 23:49:24.239037  4612 net.cpp:323] ctx_conv4/bn needs backward computation.
I0916 23:49:24.239039  4612 net.cpp:323] ctx_conv4 needs backward computation.
I0916 23:49:24.239042  4612 net.cpp:323] ctx_conv3/relu needs backward computation.
I0916 23:49:24.239043  4612 net.cpp:323] ctx_conv3/bn needs backward computation.
I0916 23:49:24.239045  4612 net.cpp:323] ctx_conv3 needs backward computation.
I0916 23:49:24.239048  4612 net.cpp:323] ctx_conv2/relu needs backward computation.
I0916 23:49:24.239050  4612 net.cpp:323] ctx_conv2/bn needs backward computation.
I0916 23:49:24.239051  4612 net.cpp:323] ctx_conv2 needs backward computation.
I0916 23:49:24.239053  4612 net.cpp:323] ctx_conv1/relu needs backward computation.
I0916 23:49:24.239056  4612 net.cpp:323] ctx_conv1/bn needs backward computation.
I0916 23:49:24.239058  4612 net.cpp:323] ctx_conv1 needs backward computation.
I0916 23:49:24.239060  4612 net.cpp:323] out3_out5_combined needs backward computation.
I0916 23:49:24.239063  4612 net.cpp:323] out3a/relu needs backward computation.
I0916 23:49:24.239065  4612 net.cpp:323] out3a/bn needs backward computation.
I0916 23:49:24.239068  4612 net.cpp:323] out3a needs backward computation.
I0916 23:49:24.239070  4612 net.cpp:323] out5a_up2 needs backward computation.
I0916 23:49:24.239073  4612 net.cpp:323] out5a/relu needs backward computation.
I0916 23:49:24.239075  4612 net.cpp:323] out5a/bn needs backward computation.
I0916 23:49:24.239078  4612 net.cpp:323] out5a needs backward computation.
I0916 23:49:24.239079  4612 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0916 23:49:24.239084  4612 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0916 23:49:24.239087  4612 net.cpp:323] res5a_branch2b needs backward computation.
I0916 23:49:24.239091  4612 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0916 23:49:24.239096  4612 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0916 23:49:24.239100  4612 net.cpp:323] res5a_branch2a needs backward computation.
I0916 23:49:24.239105  4612 net.cpp:323] pool4 needs backward computation.
I0916 23:49:24.239109  4612 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0916 23:49:24.239114  4612 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0916 23:49:24.239117  4612 net.cpp:323] res4a_branch2b needs backward computation.
I0916 23:49:24.239121  4612 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0916 23:49:24.239125  4612 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0916 23:49:24.239128  4612 net.cpp:323] res4a_branch2a needs backward computation.
I0916 23:49:24.239132  4612 net.cpp:323] pool3 needs backward computation.
I0916 23:49:24.239137  4612 net.cpp:323] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I0916 23:49:24.239142  4612 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0916 23:49:24.239146  4612 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0916 23:49:24.239150  4612 net.cpp:323] res3a_branch2b needs backward computation.
I0916 23:49:24.239154  4612 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0916 23:49:24.239158  4612 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0916 23:49:24.239162  4612 net.cpp:323] res3a_branch2a needs backward computation.
I0916 23:49:24.239166  4612 net.cpp:323] pool2 needs backward computation.
I0916 23:49:24.239171  4612 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0916 23:49:24.239174  4612 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0916 23:49:24.239178  4612 net.cpp:323] res2a_branch2b needs backward computation.
I0916 23:49:24.239182  4612 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0916 23:49:24.239189  4612 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0916 23:49:24.239194  4612 net.cpp:323] res2a_branch2a needs backward computation.
I0916 23:49:24.239198  4612 net.cpp:323] pool1 needs backward computation.
I0916 23:49:24.239202  4612 net.cpp:323] conv1b/relu needs backward computation.
I0916 23:49:24.239207  4612 net.cpp:323] conv1b/bn needs backward computation.
I0916 23:49:24.239210  4612 net.cpp:323] conv1b needs backward computation.
I0916 23:49:24.239214  4612 net.cpp:323] conv1a/relu needs backward computation.
I0916 23:49:24.239219  4612 net.cpp:323] conv1a/bn needs backward computation.
I0916 23:49:24.239223  4612 net.cpp:323] conv1a needs backward computation.
I0916 23:49:24.239228  4612 net.cpp:325] data/bias does not need backward computation.
I0916 23:49:24.239233  4612 net.cpp:325] label_data_1_split does not need backward computation.
I0916 23:49:24.239238  4612 net.cpp:325] data does not need backward computation.
I0916 23:49:24.239240  4612 net.cpp:367] This network produces output accuracy/top1
I0916 23:49:24.239244  4612 net.cpp:367] This network produces output accuracy/top5
I0916 23:49:24.239248  4612 net.cpp:367] This network produces output loss
I0916 23:49:24.239307  4612 net.cpp:389] Top memory (TEST) required for data: 1133772824 diff: 1133772824
I0916 23:49:24.239313  4612 net.cpp:392] Bottom memory (TEST) required for data: 1133772800 diff: 1133772800
I0916 23:49:24.239316  4612 net.cpp:395] Shared (in-place) memory (TEST) by data: 515276800 diff: 515276800
I0916 23:49:24.239320  4612 net.cpp:398] Parameters memory (TEST) required for data: 10817840 diff: 10817840
I0916 23:49:24.239323  4612 net.cpp:401] Parameters shared memory (TEST) by data: 0 diff: 0
I0916 23:49:24.239327  4612 net.cpp:407] Network initialization done.
I0916 23:49:24.244036  4612 net.cpp:1094] Copying source layer data Type:ImageLabelData #blobs=0
I0916 23:49:24.244058  4612 net.cpp:1094] Copying source layer data/bias Type:Bias #blobs=1
I0916 23:49:24.244096  4612 net.cpp:1094] Copying source layer conv1a Type:Convolution #blobs=2
I0916 23:49:24.244113  4612 net.cpp:1094] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0916 23:49:24.244285  4612 net.cpp:1094] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0916 23:49:24.244292  4612 net.cpp:1094] Copying source layer conv1b Type:Convolution #blobs=2
I0916 23:49:24.244302  4612 net.cpp:1094] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0916 23:49:24.244405  4612 net.cpp:1094] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0916 23:49:24.244412  4612 net.cpp:1094] Copying source layer pool1 Type:Pooling #blobs=0
I0916 23:49:24.244416  4612 net.cpp:1094] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0916 23:49:24.244436  4612 net.cpp:1094] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0916 23:49:24.244554  4612 net.cpp:1094] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0916 23:49:24.244560  4612 net.cpp:1094] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0916 23:49:24.244575  4612 net.cpp:1094] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0916 23:49:24.244678  4612 net.cpp:1094] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0916 23:49:24.244684  4612 net.cpp:1094] Copying source layer pool2 Type:Pooling #blobs=0
I0916 23:49:24.244688  4612 net.cpp:1094] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0916 23:49:24.244729  4612 net.cpp:1094] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0916 23:49:24.244834  4612 net.cpp:1094] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0916 23:49:24.244841  4612 net.cpp:1094] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0916 23:49:24.244868  4612 net.cpp:1094] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0916 23:49:24.244967  4612 net.cpp:1094] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0916 23:49:24.244974  4612 net.cpp:1094] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I0916 23:49:24.244989  4612 net.cpp:1094] Copying source layer pool3 Type:Pooling #blobs=0
I0916 23:49:24.244992  4612 net.cpp:1094] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0916 23:49:24.245113  4612 net.cpp:1094] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0916 23:49:24.245218  4612 net.cpp:1094] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0916 23:49:24.245223  4612 net.cpp:1094] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0916 23:49:24.245280  4612 net.cpp:1094] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0916 23:49:24.245383  4612 net.cpp:1094] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0916 23:49:24.245390  4612 net.cpp:1094] Copying source layer pool4 Type:Pooling #blobs=0
I0916 23:49:24.245393  4612 net.cpp:1094] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0916 23:49:24.245750  4612 net.cpp:1094] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0916 23:49:24.245846  4612 net.cpp:1094] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0916 23:49:24.245851  4612 net.cpp:1094] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0916 23:49:24.246017  4612 net.cpp:1094] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0916 23:49:24.246117  4612 net.cpp:1094] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0916 23:49:24.246122  4612 net.cpp:1094] Copying source layer out5a Type:Convolution #blobs=2
I0916 23:49:24.246162  4612 net.cpp:1094] Copying source layer out5a/bn Type:BatchNorm #blobs=5
I0916 23:49:24.246263  4612 net.cpp:1094] Copying source layer out5a/relu Type:ReLU #blobs=0
I0916 23:49:24.246268  4612 net.cpp:1094] Copying source layer out5a_up2 Type:Deconvolution #blobs=1
I0916 23:49:24.246274  4612 net.cpp:1094] Copying source layer out3a Type:Convolution #blobs=2
I0916 23:49:24.246289  4612 net.cpp:1094] Copying source layer out3a/bn Type:BatchNorm #blobs=5
I0916 23:49:24.246397  4612 net.cpp:1094] Copying source layer out3a/relu Type:ReLU #blobs=0
I0916 23:49:24.246404  4612 net.cpp:1094] Copying source layer out3_out5_combined Type:Eltwise #blobs=0
I0916 23:49:24.246408  4612 net.cpp:1094] Copying source layer ctx_conv1 Type:Convolution #blobs=2
I0916 23:49:24.246430  4612 net.cpp:1094] Copying source layer ctx_conv1/bn Type:BatchNorm #blobs=5
I0916 23:49:24.246543  4612 net.cpp:1094] Copying source layer ctx_conv1/relu Type:ReLU #blobs=0
I0916 23:49:24.246551  4612 net.cpp:1094] Copying source layer ctx_conv2 Type:Convolution #blobs=2
I0916 23:49:24.246570  4612 net.cpp:1094] Copying source layer ctx_conv2/bn Type:BatchNorm #blobs=5
I0916 23:49:24.246685  4612 net.cpp:1094] Copying source layer ctx_conv2/relu Type:ReLU #blobs=0
I0916 23:49:24.246691  4612 net.cpp:1094] Copying source layer ctx_conv3 Type:Convolution #blobs=2
I0916 23:49:24.246716  4612 net.cpp:1094] Copying source layer ctx_conv3/bn Type:BatchNorm #blobs=5
I0916 23:49:24.246829  4612 net.cpp:1094] Copying source layer ctx_conv3/relu Type:ReLU #blobs=0
I0916 23:49:24.246836  4612 net.cpp:1094] Copying source layer ctx_conv4 Type:Convolution #blobs=2
I0916 23:49:24.246857  4612 net.cpp:1094] Copying source layer ctx_conv4/bn Type:BatchNorm #blobs=5
I0916 23:49:24.246968  4612 net.cpp:1094] Copying source layer ctx_conv4/relu Type:ReLU #blobs=0
I0916 23:49:24.246973  4612 net.cpp:1094] Copying source layer ctx_final Type:Convolution #blobs=2
I0916 23:49:24.246980  4612 net.cpp:1094] Copying source layer ctx_final/relu Type:ReLU #blobs=0
I0916 23:49:24.246982  4612 net.cpp:1094] Copying source layer out_deconv_final_up2 Type:Deconvolution #blobs=1
I0916 23:49:24.246986  4612 net.cpp:1094] Copying source layer out_deconv_final_up4 Type:Deconvolution #blobs=1
I0916 23:49:24.246991  4612 net.cpp:1094] Copying source layer out_deconv_final_up8 Type:Deconvolution #blobs=1
I0916 23:49:24.246996  4612 net.cpp:1094] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0916 23:49:24.247104  4612 caffe.cpp:296] Running for 50 iterations.
I0916 23:49:24.506422  4612 caffe.cpp:319] Batch 0, accuracy/top1 = 0.907202
I0916 23:49:24.506443  4612 caffe.cpp:319] Batch 0, accuracy/top5 = 1
I0916 23:49:24.506446  4612 caffe.cpp:319] Batch 0, loss = 0.403046
I0916 23:49:24.506449  4612 net.cpp:1597] Adding quantization params at infer/iter index: 1
I0916 23:49:24.705137  4612 caffe.cpp:319] Batch 1, accuracy/top1 = 0.928734
I0916 23:49:24.705157  4612 caffe.cpp:319] Batch 1, accuracy/top5 = 1
I0916 23:49:24.705160  4612 caffe.cpp:319] Batch 1, loss = 0.187661
I0916 23:49:24.710960  4612 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'conv1a' with space 0G 3/1 1 	(avail 6.69G, req 0G)	t: 0
I0916 23:49:24.721171  4612 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'conv1b' with space 0G 32/4 6 	(avail 6.69G, req 0G)	t: 0
I0916 23:49:24.738921  4612 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res2a_branch2a' with space 0G 32/1 6 	(avail 6.69G, req 0G)	t: 0
I0916 23:49:24.744678  4612 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res2a_branch2b' with space 0G 64/4 6 	(avail 6.69G, req 0G)	t: 0
I0916 23:49:24.753726  4612 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res3a_branch2a' with space 0G 64/1 6 	(avail 6.69G, req 0G)	t: 0
I0916 23:49:24.757376  4612 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res3a_branch2b' with space 0G 128/4 6 	(avail 6.69G, req 0G)	t: 0
I0916 23:49:24.764387  4612 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res4a_branch2a' with space 0G 128/1 1 	(avail 6.69G, req 0G)	t: 0
I0916 23:49:24.767644  4612 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'res4a_branch2b' with space 0G 256/4 6 	(avail 6.69G, req 0G)	t: 0
I0916 23:49:24.783002  4612 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'out3a' with space 0G 128/2 6 	(avail 6.69G, req 0G)	t: 0
I0916 23:49:24.788745  4612 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'ctx_conv1' with space 0G 64/1 6 	(avail 6.69G, req 0G)	t: 0
I0916 23:49:24.796236  4612 cudnn_conv_layer.cpp:872] (0) Conv Algo (F): 'ctx_final' with space 0G 64/1 6 	(avail 6.69G, req 0G)	t: 0
I0916 23:49:24.962800  4612 caffe.cpp:319] Batch 2, accuracy/top1 = 0.947719
I0916 23:49:24.962823  4612 caffe.cpp:319] Batch 2, accuracy/top5 = 1
I0916 23:49:24.962826  4612 caffe.cpp:319] Batch 2, loss = 0.133824
I0916 23:49:25.163494  4612 caffe.cpp:319] Batch 3, accuracy/top1 = 0.974428
I0916 23:49:25.163512  4612 caffe.cpp:319] Batch 3, accuracy/top5 = 1
I0916 23:49:25.163516  4612 caffe.cpp:319] Batch 3, loss = 0.0738617
I0916 23:49:25.360767  4612 caffe.cpp:319] Batch 4, accuracy/top1 = 0.970425
I0916 23:49:25.360786  4612 caffe.cpp:319] Batch 4, accuracy/top5 = 1
I0916 23:49:25.360790  4612 caffe.cpp:319] Batch 4, loss = 0.110343
I0916 23:49:25.559651  4612 caffe.cpp:319] Batch 5, accuracy/top1 = 0.859474
I0916 23:49:25.559674  4612 caffe.cpp:319] Batch 5, accuracy/top5 = 1
I0916 23:49:25.559677  4612 caffe.cpp:319] Batch 5, loss = 0.61336
I0916 23:49:25.759739  4612 caffe.cpp:319] Batch 6, accuracy/top1 = 0.9604
I0916 23:49:25.759759  4612 caffe.cpp:319] Batch 6, accuracy/top5 = 1
I0916 23:49:25.759763  4612 caffe.cpp:319] Batch 6, loss = 0.10643
I0916 23:49:25.956859  4612 caffe.cpp:319] Batch 7, accuracy/top1 = 0.903278
I0916 23:49:25.956881  4612 caffe.cpp:319] Batch 7, accuracy/top5 = 1
I0916 23:49:25.956884  4612 caffe.cpp:319] Batch 7, loss = 0.246364
I0916 23:49:26.154314  4612 caffe.cpp:319] Batch 8, accuracy/top1 = 0.973939
I0916 23:49:26.154335  4612 caffe.cpp:319] Batch 8, accuracy/top5 = 1
I0916 23:49:26.154338  4612 caffe.cpp:319] Batch 8, loss = 0.0681054
I0916 23:49:26.351908  4612 caffe.cpp:319] Batch 9, accuracy/top1 = 0.984796
I0916 23:49:26.351927  4612 caffe.cpp:319] Batch 9, accuracy/top5 = 1
I0916 23:49:26.351929  4612 caffe.cpp:319] Batch 9, loss = 0.0424931
I0916 23:49:26.550637  4612 caffe.cpp:319] Batch 10, accuracy/top1 = 0.962515
I0916 23:49:26.550657  4612 caffe.cpp:319] Batch 10, accuracy/top5 = 1
I0916 23:49:26.550662  4612 caffe.cpp:319] Batch 10, loss = 0.109119
I0916 23:49:26.747540  4612 caffe.cpp:319] Batch 11, accuracy/top1 = 0.978657
I0916 23:49:26.747576  4612 caffe.cpp:319] Batch 11, accuracy/top5 = 1
I0916 23:49:26.747581  4612 caffe.cpp:319] Batch 11, loss = 0.0598643
I0916 23:49:26.945935  4612 caffe.cpp:319] Batch 12, accuracy/top1 = 0.969737
I0916 23:49:26.945958  4612 caffe.cpp:319] Batch 12, accuracy/top5 = 1
I0916 23:49:26.945961  4612 caffe.cpp:319] Batch 12, loss = 0.0890483
I0916 23:49:27.144628  4612 caffe.cpp:319] Batch 13, accuracy/top1 = 0.968335
I0916 23:49:27.144645  4612 caffe.cpp:319] Batch 13, accuracy/top5 = 1
I0916 23:49:27.144649  4612 caffe.cpp:319] Batch 13, loss = 0.0903139
I0916 23:49:27.346705  4612 caffe.cpp:319] Batch 14, accuracy/top1 = 0.985795
I0916 23:49:27.346724  4612 caffe.cpp:319] Batch 14, accuracy/top5 = 1
I0916 23:49:27.346727  4612 caffe.cpp:319] Batch 14, loss = 0.0439784
I0916 23:49:27.545505  4612 caffe.cpp:319] Batch 15, accuracy/top1 = 0.969001
I0916 23:49:27.545526  4612 caffe.cpp:319] Batch 15, accuracy/top5 = 1
I0916 23:49:27.545528  4612 caffe.cpp:319] Batch 15, loss = 0.088619
I0916 23:49:27.746603  4612 caffe.cpp:319] Batch 16, accuracy/top1 = 0.952099
I0916 23:49:27.746624  4612 caffe.cpp:319] Batch 16, accuracy/top5 = 1
I0916 23:49:27.746628  4612 caffe.cpp:319] Batch 16, loss = 0.155637
I0916 23:49:27.946168  4612 caffe.cpp:319] Batch 17, accuracy/top1 = 0.884619
I0916 23:49:27.946189  4612 caffe.cpp:319] Batch 17, accuracy/top5 = 1
I0916 23:49:27.946192  4612 caffe.cpp:319] Batch 17, loss = 0.578388
I0916 23:49:28.145225  4612 caffe.cpp:319] Batch 18, accuracy/top1 = 0.981697
I0916 23:49:28.145246  4612 caffe.cpp:319] Batch 18, accuracy/top5 = 1
I0916 23:49:28.145248  4612 caffe.cpp:319] Batch 18, loss = 0.0446604
I0916 23:49:28.345790  4612 caffe.cpp:319] Batch 19, accuracy/top1 = 0.984482
I0916 23:49:28.345808  4612 caffe.cpp:319] Batch 19, accuracy/top5 = 1
I0916 23:49:28.345811  4612 caffe.cpp:319] Batch 19, loss = 0.0419858
I0916 23:49:28.543668  4612 caffe.cpp:319] Batch 20, accuracy/top1 = 0.968995
I0916 23:49:28.543690  4612 caffe.cpp:319] Batch 20, accuracy/top5 = 1
I0916 23:49:28.543694  4612 caffe.cpp:319] Batch 20, loss = 0.085666
I0916 23:49:28.740341  4612 caffe.cpp:319] Batch 21, accuracy/top1 = 0.892113
I0916 23:49:28.740365  4612 caffe.cpp:319] Batch 21, accuracy/top5 = 1
I0916 23:49:28.740367  4612 caffe.cpp:319] Batch 21, loss = 0.558648
I0916 23:49:28.938748  4612 caffe.cpp:319] Batch 22, accuracy/top1 = 0.961987
I0916 23:49:28.938771  4612 caffe.cpp:319] Batch 22, accuracy/top5 = 1
I0916 23:49:28.938774  4612 caffe.cpp:319] Batch 22, loss = 0.0968617
I0916 23:49:29.138007  4612 caffe.cpp:319] Batch 23, accuracy/top1 = 0.979724
I0916 23:49:29.138026  4612 caffe.cpp:319] Batch 23, accuracy/top5 = 1
I0916 23:49:29.138029  4612 caffe.cpp:319] Batch 23, loss = 0.0542293
I0916 23:49:29.334923  4612 caffe.cpp:319] Batch 24, accuracy/top1 = 0.953143
I0916 23:49:29.334945  4612 caffe.cpp:319] Batch 24, accuracy/top5 = 1
I0916 23:49:29.334949  4612 caffe.cpp:319] Batch 24, loss = 0.134185
I0916 23:49:29.533252  4612 caffe.cpp:319] Batch 25, accuracy/top1 = 0.975838
I0916 23:49:29.533274  4612 caffe.cpp:319] Batch 25, accuracy/top5 = 1
I0916 23:49:29.533277  4612 caffe.cpp:319] Batch 25, loss = 0.0667558
I0916 23:49:29.731993  4612 caffe.cpp:319] Batch 26, accuracy/top1 = 0.953564
I0916 23:49:29.732017  4612 caffe.cpp:319] Batch 26, accuracy/top5 = 1
I0916 23:49:29.732020  4612 caffe.cpp:319] Batch 26, loss = 0.117112
I0916 23:49:29.929107  4612 caffe.cpp:319] Batch 27, accuracy/top1 = 0.966469
I0916 23:49:29.929131  4612 caffe.cpp:319] Batch 27, accuracy/top5 = 1
I0916 23:49:29.929134  4612 caffe.cpp:319] Batch 27, loss = 0.0905478
I0916 23:49:30.127028  4612 caffe.cpp:319] Batch 28, accuracy/top1 = 0.9615
I0916 23:49:30.127049  4612 caffe.cpp:319] Batch 28, accuracy/top5 = 1
I0916 23:49:30.127053  4612 caffe.cpp:319] Batch 28, loss = 0.101391
I0916 23:49:30.323457  4612 caffe.cpp:319] Batch 29, accuracy/top1 = 0.965552
I0916 23:49:30.323488  4612 caffe.cpp:319] Batch 29, accuracy/top5 = 1
I0916 23:49:30.323493  4612 caffe.cpp:319] Batch 29, loss = 0.115583
I0916 23:49:30.520426  4612 caffe.cpp:319] Batch 30, accuracy/top1 = 0.900292
I0916 23:49:30.520449  4612 caffe.cpp:319] Batch 30, accuracy/top5 = 1
I0916 23:49:30.520453  4612 caffe.cpp:319] Batch 30, loss = 0.241456
I0916 23:49:30.719368  4612 caffe.cpp:319] Batch 31, accuracy/top1 = 0.965754
I0916 23:49:30.719391  4612 caffe.cpp:319] Batch 31, accuracy/top5 = 1
I0916 23:49:30.719395  4612 caffe.cpp:319] Batch 31, loss = 0.100226
I0916 23:49:30.917676  4612 caffe.cpp:319] Batch 32, accuracy/top1 = 0.959601
I0916 23:49:30.917701  4612 caffe.cpp:319] Batch 32, accuracy/top5 = 1
I0916 23:49:30.917704  4612 caffe.cpp:319] Batch 32, loss = 0.110013
I0916 23:49:31.115180  4612 caffe.cpp:319] Batch 33, accuracy/top1 = 0.956376
I0916 23:49:31.115200  4612 caffe.cpp:319] Batch 33, accuracy/top5 = 1
I0916 23:49:31.115206  4612 caffe.cpp:319] Batch 33, loss = 0.118162
I0916 23:49:31.312853  4612 caffe.cpp:319] Batch 34, accuracy/top1 = 0.979011
I0916 23:49:31.312873  4612 caffe.cpp:319] Batch 34, accuracy/top5 = 1
I0916 23:49:31.312877  4612 caffe.cpp:319] Batch 34, loss = 0.0641107
I0916 23:49:31.511052  4612 caffe.cpp:319] Batch 35, accuracy/top1 = 0.94135
I0916 23:49:31.511075  4612 caffe.cpp:319] Batch 35, accuracy/top5 = 1
I0916 23:49:31.511080  4612 caffe.cpp:319] Batch 35, loss = 0.133327
I0916 23:49:31.708426  4612 caffe.cpp:319] Batch 36, accuracy/top1 = 0.965212
I0916 23:49:31.708448  4612 caffe.cpp:319] Batch 36, accuracy/top5 = 1
I0916 23:49:31.708453  4612 caffe.cpp:319] Batch 36, loss = 0.101406
I0916 23:49:31.904709  4612 caffe.cpp:319] Batch 37, accuracy/top1 = 0.963795
I0916 23:49:31.904731  4612 caffe.cpp:319] Batch 37, accuracy/top5 = 1
I0916 23:49:31.904736  4612 caffe.cpp:319] Batch 37, loss = 0.0946629
I0916 23:49:32.100903  4612 caffe.cpp:319] Batch 38, accuracy/top1 = 0.931799
I0916 23:49:32.100922  4612 caffe.cpp:319] Batch 38, accuracy/top5 = 1
I0916 23:49:32.100926  4612 caffe.cpp:319] Batch 38, loss = 0.162876
I0916 23:49:32.300437  4612 caffe.cpp:319] Batch 39, accuracy/top1 = 0.940802
I0916 23:49:32.300459  4612 caffe.cpp:319] Batch 39, accuracy/top5 = 1
I0916 23:49:32.300463  4612 caffe.cpp:319] Batch 39, loss = 0.161824
I0916 23:49:32.496829  4612 caffe.cpp:319] Batch 40, accuracy/top1 = 0.981793
I0916 23:49:32.496851  4612 caffe.cpp:319] Batch 40, accuracy/top5 = 1
I0916 23:49:32.496856  4612 caffe.cpp:319] Batch 40, loss = 0.0575013
I0916 23:49:32.694810  4612 caffe.cpp:319] Batch 41, accuracy/top1 = 0.978013
I0916 23:49:32.694831  4612 caffe.cpp:319] Batch 41, accuracy/top5 = 1
I0916 23:49:32.694836  4612 caffe.cpp:319] Batch 41, loss = 0.0635415
I0916 23:49:32.891273  4612 caffe.cpp:319] Batch 42, accuracy/top1 = 0.978923
I0916 23:49:32.891296  4612 caffe.cpp:319] Batch 42, accuracy/top5 = 1
I0916 23:49:32.891300  4612 caffe.cpp:319] Batch 42, loss = 0.0605742
I0916 23:49:33.088814  4612 caffe.cpp:319] Batch 43, accuracy/top1 = 0.976422
I0916 23:49:33.088834  4612 caffe.cpp:319] Batch 43, accuracy/top5 = 1
I0916 23:49:33.088837  4612 caffe.cpp:319] Batch 43, loss = 0.0635508
I0916 23:49:33.285763  4612 caffe.cpp:319] Batch 44, accuracy/top1 = 0.966977
I0916 23:49:33.285781  4612 caffe.cpp:319] Batch 44, accuracy/top5 = 1
I0916 23:49:33.285785  4612 caffe.cpp:319] Batch 44, loss = 0.0959525
I0916 23:49:33.483897  4612 caffe.cpp:319] Batch 45, accuracy/top1 = 0.977894
I0916 23:49:33.483918  4612 caffe.cpp:319] Batch 45, accuracy/top5 = 1
I0916 23:49:33.483923  4612 caffe.cpp:319] Batch 45, loss = 0.0683861
I0916 23:49:33.682404  4612 caffe.cpp:319] Batch 46, accuracy/top1 = 0.974327
I0916 23:49:33.682425  4612 caffe.cpp:319] Batch 46, accuracy/top5 = 1
I0916 23:49:33.682428  4612 caffe.cpp:319] Batch 46, loss = 0.0686439
I0916 23:49:33.878865  4612 caffe.cpp:319] Batch 47, accuracy/top1 = 0.962194
I0916 23:49:33.878887  4612 caffe.cpp:319] Batch 47, accuracy/top5 = 1
I0916 23:49:33.878891  4612 caffe.cpp:319] Batch 47, loss = 0.129681
I0916 23:49:34.076534  4612 caffe.cpp:319] Batch 48, accuracy/top1 = 0.904674
I0916 23:49:34.076553  4612 caffe.cpp:319] Batch 48, accuracy/top5 = 1
I0916 23:49:34.076557  4612 caffe.cpp:319] Batch 48, loss = 0.358865
I0916 23:49:34.275909  4612 caffe.cpp:319] Batch 49, accuracy/top1 = 0.951839
I0916 23:49:34.275929  4612 caffe.cpp:319] Batch 49, accuracy/top5 = 1
I0916 23:49:34.275933  4612 caffe.cpp:319] Batch 49, loss = 0.128622
I0916 23:49:34.275938  4612 caffe.cpp:324] Loss: 0.141829
I0916 23:49:34.275941  4612 caffe.cpp:336] accuracy/top1 = 0.955665
I0916 23:49:34.275944  4612 caffe.cpp:336] accuracy/top5 = 1
I0916 23:49:34.275952  4612 caffe.cpp:336] loss = 0.141829 (* 1 = 0.141829 loss)
