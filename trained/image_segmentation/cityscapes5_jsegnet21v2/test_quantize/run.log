I0816 11:05:51.789499  3971 caffe.cpp:608] This is NVCaffe 0.16.3 started at Wed Aug 16 11:05:51 2017
I0816 11:05:51.789613  3971 caffe.cpp:611] CuDNN version: 6021
I0816 11:05:51.789618  3971 caffe.cpp:612] CuBLAS version: 8000
I0816 11:05:51.789621  3971 caffe.cpp:613] CUDA version: 8000
I0816 11:05:51.789624  3971 caffe.cpp:614] CUDA driver version: 8000
I0816 11:05:51.789633  3971 caffe.cpp:263] Not using GPU #2 for single-GPU function
I0816 11:05:51.789636  3971 caffe.cpp:263] Not using GPU #1 for single-GPU function
I0816 11:05:51.790222  3971 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0816 11:05:51.790805  3971 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I0816 11:05:51.790812  3971 caffe.cpp:275] Use GPU with device ID 0
I0816 11:05:51.791174  3971 caffe.cpp:279] GPU device name: GeForce GTX 1080
I0816 11:05:51.792683  3971 net.cpp:72] Initializing net from parameters: 
name: "jsegnet21v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/val-image-lmdb"
    label_list_path: "data/val-label-lmdb"
    batch_size: 4
    threads: 1
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
  accuracy_param {
    ignore_label: 255
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
    ignore_label: 255
  }
}
quantize: true
I0816 11:05:51.792874  3971 net.cpp:104] Using FLOAT as default forward math type
I0816 11:05:51.792881  3971 net.cpp:110] Using FLOAT as default backward math type
I0816 11:05:51.792884  3971 layer_factory.hpp:136] Creating layer 'data' of type 'ImageLabelData'
I0816 11:05:51.792888  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:51.792901  3971 net.cpp:184] Created Layer data (0)
I0816 11:05:51.792906  3971 net.cpp:530] data -> data
I0816 11:05:51.792920  3971 net.cpp:530] data -> label
I0816 11:05:51.793289  3971 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 4
I0816 11:05:51.793308  3971 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0816 11:05:51.800173  4007 db_lmdb.cpp:24] Opened lmdb data/val-image-lmdb
I0816 11:05:51.802662  3971 data_layer.cpp:185] (0) ReshapePrefetch 4, 3, 640, 640
I0816 11:05:51.802768  3971 data_layer.cpp:209] (0) Output data size: 4, 3, 640, 640
I0816 11:05:51.802790  3971 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0816 11:05:51.802858  3971 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 4
I0816 11:05:51.802884  3971 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0816 11:05:51.804008  4009 data_layer.cpp:97] (0) Parser threads: 1
I0816 11:05:51.804028  4009 data_layer.cpp:99] (0) Transformer threads: 1
I0816 11:05:51.808675  4010 db_lmdb.cpp:24] Opened lmdb data/val-label-lmdb
I0816 11:05:51.809854  3971 data_layer.cpp:185] (0) ReshapePrefetch 4, 1, 640, 640
I0816 11:05:51.809979  3971 data_layer.cpp:209] (0) Output data size: 4, 1, 640, 640
I0816 11:05:51.810011  3971 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0816 11:05:51.810142  3971 net.cpp:245] Setting up data
I0816 11:05:51.810175  3971 net.cpp:252] TEST Top shape for layer 0 'data' 4 3 640 640 (4915200)
I0816 11:05:51.810197  3971 net.cpp:252] TEST Top shape for layer 0 'data' 4 1 640 640 (1638400)
I0816 11:05:51.810222  3971 layer_factory.hpp:136] Creating layer 'label_data_1_split' of type 'Split'
I0816 11:05:51.810245  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:51.810282  3971 net.cpp:184] Created Layer label_data_1_split (1)
I0816 11:05:51.810302  3971 net.cpp:561] label_data_1_split <- label
I0816 11:05:51.810331  3971 net.cpp:530] label_data_1_split -> label_data_1_split_0
I0816 11:05:51.810353  3971 net.cpp:530] label_data_1_split -> label_data_1_split_1
I0816 11:05:51.810365  3971 net.cpp:530] label_data_1_split -> label_data_1_split_2
I0816 11:05:51.810433  3971 net.cpp:245] Setting up label_data_1_split
I0816 11:05:51.810448  3971 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 4 1 640 640 (1638400)
I0816 11:05:51.810462  3971 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 4 1 640 640 (1638400)
I0816 11:05:51.810475  3971 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 4 1 640 640 (1638400)
I0816 11:05:51.810490  3971 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0816 11:05:51.810504  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:51.810539  3971 net.cpp:184] Created Layer data/bias (2)
I0816 11:05:51.810550  3971 net.cpp:561] data/bias <- data
I0816 11:05:51.810564  3971 net.cpp:530] data/bias -> data/bias
I0816 11:05:51.811954  4012 data_layer.cpp:97] (0) Parser threads: 1
I0816 11:05:51.811998  4012 data_layer.cpp:99] (0) Transformer threads: 1
I0816 11:05:51.815906  3971 net.cpp:245] Setting up data/bias
I0816 11:05:51.815996  3971 net.cpp:252] TEST Top shape for layer 2 'data/bias' 4 3 640 640 (4915200)
I0816 11:05:51.816037  3971 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0816 11:05:51.816057  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:51.816109  3971 net.cpp:184] Created Layer conv1a (3)
I0816 11:05:51.816159  3971 net.cpp:561] conv1a <- data/bias
I0816 11:05:51.816180  3971 net.cpp:530] conv1a -> conv1a
I0816 11:05:52.111963  3971 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.01G/1 1  (limit 8.06G, req 0G)
I0816 11:05:52.111984  3971 net.cpp:245] Setting up conv1a
I0816 11:05:52.111989  3971 net.cpp:252] TEST Top shape for layer 3 'conv1a' 4 32 320 320 (13107200)
I0816 11:05:52.111997  3971 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0816 11:05:52.112001  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.112012  3971 net.cpp:184] Created Layer conv1a/bn (4)
I0816 11:05:52.112016  3971 net.cpp:561] conv1a/bn <- conv1a
I0816 11:05:52.112020  3971 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0816 11:05:52.112469  3971 net.cpp:245] Setting up conv1a/bn
I0816 11:05:52.112478  3971 net.cpp:252] TEST Top shape for layer 4 'conv1a/bn' 4 32 320 320 (13107200)
I0816 11:05:52.112484  3971 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0816 11:05:52.112486  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.112491  3971 net.cpp:184] Created Layer conv1a/relu (5)
I0816 11:05:52.112493  3971 net.cpp:561] conv1a/relu <- conv1a
I0816 11:05:52.112496  3971 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0816 11:05:52.112505  3971 net.cpp:245] Setting up conv1a/relu
I0816 11:05:52.112509  3971 net.cpp:252] TEST Top shape for layer 5 'conv1a/relu' 4 32 320 320 (13107200)
I0816 11:05:52.112510  3971 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0816 11:05:52.112512  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.112519  3971 net.cpp:184] Created Layer conv1b (6)
I0816 11:05:52.112522  3971 net.cpp:561] conv1b <- conv1a
I0816 11:05:52.112524  3971 net.cpp:530] conv1b -> conv1b
I0816 11:05:52.129370  3971 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 8G, req 0G)
I0816 11:05:52.129380  3971 net.cpp:245] Setting up conv1b
I0816 11:05:52.129384  3971 net.cpp:252] TEST Top shape for layer 6 'conv1b' 4 32 320 320 (13107200)
I0816 11:05:52.129390  3971 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0816 11:05:52.129392  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.129397  3971 net.cpp:184] Created Layer conv1b/bn (7)
I0816 11:05:52.129400  3971 net.cpp:561] conv1b/bn <- conv1b
I0816 11:05:52.129401  3971 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0816 11:05:52.129809  3971 net.cpp:245] Setting up conv1b/bn
I0816 11:05:52.129817  3971 net.cpp:252] TEST Top shape for layer 7 'conv1b/bn' 4 32 320 320 (13107200)
I0816 11:05:52.129822  3971 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0816 11:05:52.129824  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.129828  3971 net.cpp:184] Created Layer conv1b/relu (8)
I0816 11:05:52.129830  3971 net.cpp:561] conv1b/relu <- conv1b
I0816 11:05:52.129832  3971 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0816 11:05:52.129835  3971 net.cpp:245] Setting up conv1b/relu
I0816 11:05:52.129837  3971 net.cpp:252] TEST Top shape for layer 8 'conv1b/relu' 4 32 320 320 (13107200)
I0816 11:05:52.129839  3971 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0816 11:05:52.129842  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.129848  3971 net.cpp:184] Created Layer pool1 (9)
I0816 11:05:52.129849  3971 net.cpp:561] pool1 <- conv1b
I0816 11:05:52.129851  3971 net.cpp:530] pool1 -> pool1
I0816 11:05:52.129889  3971 net.cpp:245] Setting up pool1
I0816 11:05:52.129894  3971 net.cpp:252] TEST Top shape for layer 9 'pool1' 4 32 160 160 (3276800)
I0816 11:05:52.129895  3971 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0816 11:05:52.129899  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.129915  3971 net.cpp:184] Created Layer res2a_branch2a (10)
I0816 11:05:52.129918  3971 net.cpp:561] res2a_branch2a <- pool1
I0816 11:05:52.129920  3971 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0816 11:05:52.141575  3971 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.95G, req 0G)
I0816 11:05:52.141592  3971 net.cpp:245] Setting up res2a_branch2a
I0816 11:05:52.141597  3971 net.cpp:252] TEST Top shape for layer 10 'res2a_branch2a' 4 64 160 160 (6553600)
I0816 11:05:52.141605  3971 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0816 11:05:52.141608  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.141618  3971 net.cpp:184] Created Layer res2a_branch2a/bn (11)
I0816 11:05:52.141620  3971 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0816 11:05:52.141623  3971 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0816 11:05:52.142063  3971 net.cpp:245] Setting up res2a_branch2a/bn
I0816 11:05:52.142071  3971 net.cpp:252] TEST Top shape for layer 11 'res2a_branch2a/bn' 4 64 160 160 (6553600)
I0816 11:05:52.142076  3971 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0816 11:05:52.142079  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.142083  3971 net.cpp:184] Created Layer res2a_branch2a/relu (12)
I0816 11:05:52.142086  3971 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0816 11:05:52.142087  3971 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0816 11:05:52.142091  3971 net.cpp:245] Setting up res2a_branch2a/relu
I0816 11:05:52.142094  3971 net.cpp:252] TEST Top shape for layer 12 'res2a_branch2a/relu' 4 64 160 160 (6553600)
I0816 11:05:52.142096  3971 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0816 11:05:52.142098  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.142107  3971 net.cpp:184] Created Layer res2a_branch2b (13)
I0816 11:05:52.142108  3971 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0816 11:05:52.142110  3971 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0816 11:05:52.150202  3971 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.92G, req 0G)
I0816 11:05:52.150215  3971 net.cpp:245] Setting up res2a_branch2b
I0816 11:05:52.150219  3971 net.cpp:252] TEST Top shape for layer 13 'res2a_branch2b' 4 64 160 160 (6553600)
I0816 11:05:52.150224  3971 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0816 11:05:52.150228  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.150233  3971 net.cpp:184] Created Layer res2a_branch2b/bn (14)
I0816 11:05:52.150235  3971 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0816 11:05:52.150238  3971 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0816 11:05:52.150655  3971 net.cpp:245] Setting up res2a_branch2b/bn
I0816 11:05:52.150661  3971 net.cpp:252] TEST Top shape for layer 14 'res2a_branch2b/bn' 4 64 160 160 (6553600)
I0816 11:05:52.150667  3971 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0816 11:05:52.150669  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.150673  3971 net.cpp:184] Created Layer res2a_branch2b/relu (15)
I0816 11:05:52.150676  3971 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0816 11:05:52.150677  3971 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0816 11:05:52.150681  3971 net.cpp:245] Setting up res2a_branch2b/relu
I0816 11:05:52.150684  3971 net.cpp:252] TEST Top shape for layer 15 'res2a_branch2b/relu' 4 64 160 160 (6553600)
I0816 11:05:52.150686  3971 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0816 11:05:52.150688  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.150705  3971 net.cpp:184] Created Layer pool2 (16)
I0816 11:05:52.150708  3971 net.cpp:561] pool2 <- res2a_branch2b
I0816 11:05:52.150712  3971 net.cpp:530] pool2 -> pool2
I0816 11:05:52.150741  3971 net.cpp:245] Setting up pool2
I0816 11:05:52.150744  3971 net.cpp:252] TEST Top shape for layer 16 'pool2' 4 64 80 80 (1638400)
I0816 11:05:52.150746  3971 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0816 11:05:52.150748  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.150754  3971 net.cpp:184] Created Layer res3a_branch2a (17)
I0816 11:05:52.150756  3971 net.cpp:561] res3a_branch2a <- pool2
I0816 11:05:52.150759  3971 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0816 11:05:52.157469  3971 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.9G, req 0G)
I0816 11:05:52.157479  3971 net.cpp:245] Setting up res3a_branch2a
I0816 11:05:52.157482  3971 net.cpp:252] TEST Top shape for layer 17 'res3a_branch2a' 4 128 80 80 (3276800)
I0816 11:05:52.157487  3971 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0816 11:05:52.157490  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.157495  3971 net.cpp:184] Created Layer res3a_branch2a/bn (18)
I0816 11:05:52.157496  3971 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0816 11:05:52.157498  3971 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0816 11:05:52.158345  3971 net.cpp:245] Setting up res3a_branch2a/bn
I0816 11:05:52.158354  3971 net.cpp:252] TEST Top shape for layer 18 'res3a_branch2a/bn' 4 128 80 80 (3276800)
I0816 11:05:52.158362  3971 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0816 11:05:52.158365  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.158368  3971 net.cpp:184] Created Layer res3a_branch2a/relu (19)
I0816 11:05:52.158370  3971 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0816 11:05:52.158372  3971 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0816 11:05:52.158376  3971 net.cpp:245] Setting up res3a_branch2a/relu
I0816 11:05:52.158378  3971 net.cpp:252] TEST Top shape for layer 19 'res3a_branch2a/relu' 4 128 80 80 (3276800)
I0816 11:05:52.158380  3971 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0816 11:05:52.158382  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.158390  3971 net.cpp:184] Created Layer res3a_branch2b (20)
I0816 11:05:52.158392  3971 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0816 11:05:52.158396  3971 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0816 11:05:52.163224  3971 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.88G, req 0G)
I0816 11:05:52.163234  3971 net.cpp:245] Setting up res3a_branch2b
I0816 11:05:52.163239  3971 net.cpp:252] TEST Top shape for layer 20 'res3a_branch2b' 4 128 80 80 (3276800)
I0816 11:05:52.163242  3971 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0816 11:05:52.163245  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.163249  3971 net.cpp:184] Created Layer res3a_branch2b/bn (21)
I0816 11:05:52.163252  3971 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0816 11:05:52.163254  3971 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0816 11:05:52.163664  3971 net.cpp:245] Setting up res3a_branch2b/bn
I0816 11:05:52.163671  3971 net.cpp:252] TEST Top shape for layer 21 'res3a_branch2b/bn' 4 128 80 80 (3276800)
I0816 11:05:52.163676  3971 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0816 11:05:52.163678  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.163681  3971 net.cpp:184] Created Layer res3a_branch2b/relu (22)
I0816 11:05:52.163691  3971 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0816 11:05:52.163693  3971 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0816 11:05:52.163697  3971 net.cpp:245] Setting up res3a_branch2b/relu
I0816 11:05:52.163699  3971 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b/relu' 4 128 80 80 (3276800)
I0816 11:05:52.163702  3971 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0816 11:05:52.163704  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.163707  3971 net.cpp:184] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (23)
I0816 11:05:52.163709  3971 net.cpp:561] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0816 11:05:52.163712  3971 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0816 11:05:52.163713  3971 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0816 11:05:52.163735  3971 net.cpp:245] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0816 11:05:52.163739  3971 net.cpp:252] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 4 128 80 80 (3276800)
I0816 11:05:52.163741  3971 net.cpp:252] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 4 128 80 80 (3276800)
I0816 11:05:52.163743  3971 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0816 11:05:52.163745  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.163749  3971 net.cpp:184] Created Layer pool3 (24)
I0816 11:05:52.163751  3971 net.cpp:561] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0816 11:05:52.163753  3971 net.cpp:530] pool3 -> pool3
I0816 11:05:52.163785  3971 net.cpp:245] Setting up pool3
I0816 11:05:52.163789  3971 net.cpp:252] TEST Top shape for layer 24 'pool3' 4 128 40 40 (819200)
I0816 11:05:52.163791  3971 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0816 11:05:52.163794  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.163800  3971 net.cpp:184] Created Layer res4a_branch2a (25)
I0816 11:05:52.163802  3971 net.cpp:561] res4a_branch2a <- pool3
I0816 11:05:52.163805  3971 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0816 11:05:52.175834  3971 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.87G, req 0G)
I0816 11:05:52.175851  3971 net.cpp:245] Setting up res4a_branch2a
I0816 11:05:52.175856  3971 net.cpp:252] TEST Top shape for layer 25 'res4a_branch2a' 4 256 40 40 (1638400)
I0816 11:05:52.175863  3971 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0816 11:05:52.175866  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.175873  3971 net.cpp:184] Created Layer res4a_branch2a/bn (26)
I0816 11:05:52.175876  3971 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0816 11:05:52.175879  3971 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0816 11:05:52.176342  3971 net.cpp:245] Setting up res4a_branch2a/bn
I0816 11:05:52.176349  3971 net.cpp:252] TEST Top shape for layer 26 'res4a_branch2a/bn' 4 256 40 40 (1638400)
I0816 11:05:52.176355  3971 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0816 11:05:52.176358  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.176362  3971 net.cpp:184] Created Layer res4a_branch2a/relu (27)
I0816 11:05:52.176364  3971 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0816 11:05:52.176367  3971 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0816 11:05:52.176370  3971 net.cpp:245] Setting up res4a_branch2a/relu
I0816 11:05:52.176373  3971 net.cpp:252] TEST Top shape for layer 27 'res4a_branch2a/relu' 4 256 40 40 (1638400)
I0816 11:05:52.176374  3971 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0816 11:05:52.176388  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.176395  3971 net.cpp:184] Created Layer res4a_branch2b (28)
I0816 11:05:52.176398  3971 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0816 11:05:52.176400  3971 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0816 11:05:52.182107  3971 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.86G, req 0G)
I0816 11:05:52.182118  3971 net.cpp:245] Setting up res4a_branch2b
I0816 11:05:52.182122  3971 net.cpp:252] TEST Top shape for layer 28 'res4a_branch2b' 4 256 40 40 (1638400)
I0816 11:05:52.182126  3971 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0816 11:05:52.182129  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.182134  3971 net.cpp:184] Created Layer res4a_branch2b/bn (29)
I0816 11:05:52.182137  3971 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0816 11:05:52.182139  3971 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0816 11:05:52.182559  3971 net.cpp:245] Setting up res4a_branch2b/bn
I0816 11:05:52.182566  3971 net.cpp:252] TEST Top shape for layer 29 'res4a_branch2b/bn' 4 256 40 40 (1638400)
I0816 11:05:52.182572  3971 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0816 11:05:52.182574  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.182577  3971 net.cpp:184] Created Layer res4a_branch2b/relu (30)
I0816 11:05:52.182580  3971 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0816 11:05:52.182582  3971 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0816 11:05:52.182585  3971 net.cpp:245] Setting up res4a_branch2b/relu
I0816 11:05:52.182590  3971 net.cpp:252] TEST Top shape for layer 30 'res4a_branch2b/relu' 4 256 40 40 (1638400)
I0816 11:05:52.182591  3971 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0816 11:05:52.182593  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.182602  3971 net.cpp:184] Created Layer pool4 (31)
I0816 11:05:52.182605  3971 net.cpp:561] pool4 <- res4a_branch2b
I0816 11:05:52.182608  3971 net.cpp:530] pool4 -> pool4
I0816 11:05:52.182638  3971 net.cpp:245] Setting up pool4
I0816 11:05:52.182642  3971 net.cpp:252] TEST Top shape for layer 31 'pool4' 4 256 40 40 (1638400)
I0816 11:05:52.182644  3971 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0816 11:05:52.182647  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.182658  3971 net.cpp:184] Created Layer res5a_branch2a (32)
I0816 11:05:52.182662  3971 net.cpp:561] res5a_branch2a <- pool4
I0816 11:05:52.182663  3971 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0816 11:05:52.210053  3971 net.cpp:245] Setting up res5a_branch2a
I0816 11:05:52.210073  3971 net.cpp:252] TEST Top shape for layer 32 'res5a_branch2a' 4 512 40 40 (3276800)
I0816 11:05:52.210079  3971 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0816 11:05:52.210084  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.210091  3971 net.cpp:184] Created Layer res5a_branch2a/bn (33)
I0816 11:05:52.210094  3971 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0816 11:05:52.210098  3971 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0816 11:05:52.210517  3971 net.cpp:245] Setting up res5a_branch2a/bn
I0816 11:05:52.210525  3971 net.cpp:252] TEST Top shape for layer 33 'res5a_branch2a/bn' 4 512 40 40 (3276800)
I0816 11:05:52.210530  3971 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0816 11:05:52.210532  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.210536  3971 net.cpp:184] Created Layer res5a_branch2a/relu (34)
I0816 11:05:52.210548  3971 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0816 11:05:52.210551  3971 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0816 11:05:52.210556  3971 net.cpp:245] Setting up res5a_branch2a/relu
I0816 11:05:52.210558  3971 net.cpp:252] TEST Top shape for layer 34 'res5a_branch2a/relu' 4 512 40 40 (3276800)
I0816 11:05:52.210561  3971 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0816 11:05:52.210563  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.210571  3971 net.cpp:184] Created Layer res5a_branch2b (35)
I0816 11:05:52.210573  3971 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0816 11:05:52.210577  3971 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0816 11:05:52.227638  3971 net.cpp:245] Setting up res5a_branch2b
I0816 11:05:52.227653  3971 net.cpp:252] TEST Top shape for layer 35 'res5a_branch2b' 4 512 40 40 (3276800)
I0816 11:05:52.227669  3971 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0816 11:05:52.227672  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.227679  3971 net.cpp:184] Created Layer res5a_branch2b/bn (36)
I0816 11:05:52.227681  3971 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0816 11:05:52.227684  3971 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0816 11:05:52.228091  3971 net.cpp:245] Setting up res5a_branch2b/bn
I0816 11:05:52.228098  3971 net.cpp:252] TEST Top shape for layer 36 'res5a_branch2b/bn' 4 512 40 40 (3276800)
I0816 11:05:52.228104  3971 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0816 11:05:52.228106  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.228111  3971 net.cpp:184] Created Layer res5a_branch2b/relu (37)
I0816 11:05:52.228112  3971 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0816 11:05:52.228114  3971 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0816 11:05:52.228118  3971 net.cpp:245] Setting up res5a_branch2b/relu
I0816 11:05:52.228121  3971 net.cpp:252] TEST Top shape for layer 37 'res5a_branch2b/relu' 4 512 40 40 (3276800)
I0816 11:05:52.228122  3971 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I0816 11:05:52.228126  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.228133  3971 net.cpp:184] Created Layer out5a (38)
I0816 11:05:52.228137  3971 net.cpp:561] out5a <- res5a_branch2b
I0816 11:05:52.228142  3971 net.cpp:530] out5a -> out5a
I0816 11:05:52.231792  3971 net.cpp:245] Setting up out5a
I0816 11:05:52.231804  3971 net.cpp:252] TEST Top shape for layer 38 'out5a' 4 64 40 40 (409600)
I0816 11:05:52.231809  3971 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I0816 11:05:52.231812  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.231817  3971 net.cpp:184] Created Layer out5a/bn (39)
I0816 11:05:52.231820  3971 net.cpp:561] out5a/bn <- out5a
I0816 11:05:52.231822  3971 net.cpp:513] out5a/bn -> out5a (in-place)
I0816 11:05:52.232273  3971 net.cpp:245] Setting up out5a/bn
I0816 11:05:52.232281  3971 net.cpp:252] TEST Top shape for layer 39 'out5a/bn' 4 64 40 40 (409600)
I0816 11:05:52.232287  3971 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I0816 11:05:52.232290  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.232295  3971 net.cpp:184] Created Layer out5a/relu (40)
I0816 11:05:52.232296  3971 net.cpp:561] out5a/relu <- out5a
I0816 11:05:52.232298  3971 net.cpp:513] out5a/relu -> out5a (in-place)
I0816 11:05:52.232302  3971 net.cpp:245] Setting up out5a/relu
I0816 11:05:52.232306  3971 net.cpp:252] TEST Top shape for layer 40 'out5a/relu' 4 64 40 40 (409600)
I0816 11:05:52.232307  3971 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I0816 11:05:52.232338  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.232353  3971 net.cpp:184] Created Layer out5a_up2 (41)
I0816 11:05:52.232357  3971 net.cpp:561] out5a_up2 <- out5a
I0816 11:05:52.232359  3971 net.cpp:530] out5a_up2 -> out5a_up2
I0816 11:05:52.232511  3971 net.cpp:245] Setting up out5a_up2
I0816 11:05:52.232517  3971 net.cpp:252] TEST Top shape for layer 41 'out5a_up2' 4 64 80 80 (1638400)
I0816 11:05:52.232520  3971 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I0816 11:05:52.232522  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.232532  3971 net.cpp:184] Created Layer out3a (42)
I0816 11:05:52.232533  3971 net.cpp:561] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0816 11:05:52.232537  3971 net.cpp:530] out3a -> out3a
I0816 11:05:52.236932  3971 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 7.84G, req 0G)
I0816 11:05:52.236943  3971 net.cpp:245] Setting up out3a
I0816 11:05:52.236946  3971 net.cpp:252] TEST Top shape for layer 42 'out3a' 4 64 80 80 (1638400)
I0816 11:05:52.236951  3971 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I0816 11:05:52.236954  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.236965  3971 net.cpp:184] Created Layer out3a/bn (43)
I0816 11:05:52.236969  3971 net.cpp:561] out3a/bn <- out3a
I0816 11:05:52.236971  3971 net.cpp:513] out3a/bn -> out3a (in-place)
I0816 11:05:52.237421  3971 net.cpp:245] Setting up out3a/bn
I0816 11:05:52.237427  3971 net.cpp:252] TEST Top shape for layer 43 'out3a/bn' 4 64 80 80 (1638400)
I0816 11:05:52.237433  3971 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I0816 11:05:52.237437  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.237440  3971 net.cpp:184] Created Layer out3a/relu (44)
I0816 11:05:52.237442  3971 net.cpp:561] out3a/relu <- out3a
I0816 11:05:52.237445  3971 net.cpp:513] out3a/relu -> out3a (in-place)
I0816 11:05:52.237448  3971 net.cpp:245] Setting up out3a/relu
I0816 11:05:52.237452  3971 net.cpp:252] TEST Top shape for layer 44 'out3a/relu' 4 64 80 80 (1638400)
I0816 11:05:52.237453  3971 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I0816 11:05:52.237457  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.237468  3971 net.cpp:184] Created Layer out3_out5_combined (45)
I0816 11:05:52.237470  3971 net.cpp:561] out3_out5_combined <- out5a_up2
I0816 11:05:52.237473  3971 net.cpp:561] out3_out5_combined <- out3a
I0816 11:05:52.237475  3971 net.cpp:530] out3_out5_combined -> out3_out5_combined
I0816 11:05:52.238296  3971 net.cpp:245] Setting up out3_out5_combined
I0816 11:05:52.238304  3971 net.cpp:252] TEST Top shape for layer 45 'out3_out5_combined' 4 64 80 80 (1638400)
I0816 11:05:52.238308  3971 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I0816 11:05:52.238310  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.238318  3971 net.cpp:184] Created Layer ctx_conv1 (46)
I0816 11:05:52.238322  3971 net.cpp:561] ctx_conv1 <- out3_out5_combined
I0816 11:05:52.238324  3971 net.cpp:530] ctx_conv1 -> ctx_conv1
I0816 11:05:52.243129  3971 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 7.81G, req 0G)
I0816 11:05:52.243140  3971 net.cpp:245] Setting up ctx_conv1
I0816 11:05:52.243144  3971 net.cpp:252] TEST Top shape for layer 46 'ctx_conv1' 4 64 80 80 (1638400)
I0816 11:05:52.243149  3971 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I0816 11:05:52.243151  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.243162  3971 net.cpp:184] Created Layer ctx_conv1/bn (47)
I0816 11:05:52.243173  3971 net.cpp:561] ctx_conv1/bn <- ctx_conv1
I0816 11:05:52.243176  3971 net.cpp:513] ctx_conv1/bn -> ctx_conv1 (in-place)
I0816 11:05:52.243621  3971 net.cpp:245] Setting up ctx_conv1/bn
I0816 11:05:52.243628  3971 net.cpp:252] TEST Top shape for layer 47 'ctx_conv1/bn' 4 64 80 80 (1638400)
I0816 11:05:52.243634  3971 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I0816 11:05:52.243636  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.243639  3971 net.cpp:184] Created Layer ctx_conv1/relu (48)
I0816 11:05:52.243641  3971 net.cpp:561] ctx_conv1/relu <- ctx_conv1
I0816 11:05:52.243644  3971 net.cpp:513] ctx_conv1/relu -> ctx_conv1 (in-place)
I0816 11:05:52.243647  3971 net.cpp:245] Setting up ctx_conv1/relu
I0816 11:05:52.243650  3971 net.cpp:252] TEST Top shape for layer 48 'ctx_conv1/relu' 4 64 80 80 (1638400)
I0816 11:05:52.243652  3971 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I0816 11:05:52.243654  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.243665  3971 net.cpp:184] Created Layer ctx_conv2 (49)
I0816 11:05:52.243669  3971 net.cpp:561] ctx_conv2 <- ctx_conv1
I0816 11:05:52.243670  3971 net.cpp:530] ctx_conv2 -> ctx_conv2
I0816 11:05:52.244626  3971 net.cpp:245] Setting up ctx_conv2
I0816 11:05:52.244633  3971 net.cpp:252] TEST Top shape for layer 49 'ctx_conv2' 4 64 80 80 (1638400)
I0816 11:05:52.244637  3971 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I0816 11:05:52.244640  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.244644  3971 net.cpp:184] Created Layer ctx_conv2/bn (50)
I0816 11:05:52.244647  3971 net.cpp:561] ctx_conv2/bn <- ctx_conv2
I0816 11:05:52.244648  3971 net.cpp:513] ctx_conv2/bn -> ctx_conv2 (in-place)
I0816 11:05:52.245064  3971 net.cpp:245] Setting up ctx_conv2/bn
I0816 11:05:52.245069  3971 net.cpp:252] TEST Top shape for layer 50 'ctx_conv2/bn' 4 64 80 80 (1638400)
I0816 11:05:52.245079  3971 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I0816 11:05:52.245081  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.245085  3971 net.cpp:184] Created Layer ctx_conv2/relu (51)
I0816 11:05:52.245087  3971 net.cpp:561] ctx_conv2/relu <- ctx_conv2
I0816 11:05:52.245090  3971 net.cpp:513] ctx_conv2/relu -> ctx_conv2 (in-place)
I0816 11:05:52.245092  3971 net.cpp:245] Setting up ctx_conv2/relu
I0816 11:05:52.245095  3971 net.cpp:252] TEST Top shape for layer 51 'ctx_conv2/relu' 4 64 80 80 (1638400)
I0816 11:05:52.245096  3971 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I0816 11:05:52.245098  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.245103  3971 net.cpp:184] Created Layer ctx_conv3 (52)
I0816 11:05:52.245105  3971 net.cpp:561] ctx_conv3 <- ctx_conv2
I0816 11:05:52.245107  3971 net.cpp:530] ctx_conv3 -> ctx_conv3
I0816 11:05:52.246007  3971 net.cpp:245] Setting up ctx_conv3
I0816 11:05:52.246014  3971 net.cpp:252] TEST Top shape for layer 52 'ctx_conv3' 4 64 80 80 (1638400)
I0816 11:05:52.246018  3971 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I0816 11:05:52.246021  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.246024  3971 net.cpp:184] Created Layer ctx_conv3/bn (53)
I0816 11:05:52.246026  3971 net.cpp:561] ctx_conv3/bn <- ctx_conv3
I0816 11:05:52.246029  3971 net.cpp:513] ctx_conv3/bn -> ctx_conv3 (in-place)
I0816 11:05:52.246434  3971 net.cpp:245] Setting up ctx_conv3/bn
I0816 11:05:52.246440  3971 net.cpp:252] TEST Top shape for layer 53 'ctx_conv3/bn' 4 64 80 80 (1638400)
I0816 11:05:52.246445  3971 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I0816 11:05:52.246448  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.246457  3971 net.cpp:184] Created Layer ctx_conv3/relu (54)
I0816 11:05:52.246459  3971 net.cpp:561] ctx_conv3/relu <- ctx_conv3
I0816 11:05:52.246461  3971 net.cpp:513] ctx_conv3/relu -> ctx_conv3 (in-place)
I0816 11:05:52.246464  3971 net.cpp:245] Setting up ctx_conv3/relu
I0816 11:05:52.246467  3971 net.cpp:252] TEST Top shape for layer 54 'ctx_conv3/relu' 4 64 80 80 (1638400)
I0816 11:05:52.246469  3971 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I0816 11:05:52.246471  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.246480  3971 net.cpp:184] Created Layer ctx_conv4 (55)
I0816 11:05:52.246484  3971 net.cpp:561] ctx_conv4 <- ctx_conv3
I0816 11:05:52.246485  3971 net.cpp:530] ctx_conv4 -> ctx_conv4
I0816 11:05:52.247385  3971 net.cpp:245] Setting up ctx_conv4
I0816 11:05:52.247390  3971 net.cpp:252] TEST Top shape for layer 55 'ctx_conv4' 4 64 80 80 (1638400)
I0816 11:05:52.247395  3971 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I0816 11:05:52.247396  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.247400  3971 net.cpp:184] Created Layer ctx_conv4/bn (56)
I0816 11:05:52.247402  3971 net.cpp:561] ctx_conv4/bn <- ctx_conv4
I0816 11:05:52.247404  3971 net.cpp:513] ctx_conv4/bn -> ctx_conv4 (in-place)
I0816 11:05:52.247807  3971 net.cpp:245] Setting up ctx_conv4/bn
I0816 11:05:52.247813  3971 net.cpp:252] TEST Top shape for layer 56 'ctx_conv4/bn' 4 64 80 80 (1638400)
I0816 11:05:52.247818  3971 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I0816 11:05:52.247822  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.247824  3971 net.cpp:184] Created Layer ctx_conv4/relu (57)
I0816 11:05:52.247826  3971 net.cpp:561] ctx_conv4/relu <- ctx_conv4
I0816 11:05:52.247828  3971 net.cpp:513] ctx_conv4/relu -> ctx_conv4 (in-place)
I0816 11:05:52.247831  3971 net.cpp:245] Setting up ctx_conv4/relu
I0816 11:05:52.247833  3971 net.cpp:252] TEST Top shape for layer 57 'ctx_conv4/relu' 4 64 80 80 (1638400)
I0816 11:05:52.247835  3971 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I0816 11:05:52.247838  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.247848  3971 net.cpp:184] Created Layer ctx_final (58)
I0816 11:05:52.247849  3971 net.cpp:561] ctx_final <- ctx_conv4
I0816 11:05:52.247853  3971 net.cpp:530] ctx_final -> ctx_final
I0816 11:05:52.253058  3971 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 7.8G, req 0G)
I0816 11:05:52.253068  3971 net.cpp:245] Setting up ctx_final
I0816 11:05:52.253072  3971 net.cpp:252] TEST Top shape for layer 58 'ctx_final' 4 8 80 80 (204800)
I0816 11:05:52.253077  3971 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I0816 11:05:52.253079  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.253082  3971 net.cpp:184] Created Layer ctx_final/relu (59)
I0816 11:05:52.253085  3971 net.cpp:561] ctx_final/relu <- ctx_final
I0816 11:05:52.253087  3971 net.cpp:513] ctx_final/relu -> ctx_final (in-place)
I0816 11:05:52.253093  3971 net.cpp:245] Setting up ctx_final/relu
I0816 11:05:52.253094  3971 net.cpp:252] TEST Top shape for layer 59 'ctx_final/relu' 4 8 80 80 (204800)
I0816 11:05:52.253098  3971 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I0816 11:05:52.253099  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.253105  3971 net.cpp:184] Created Layer out_deconv_final_up2 (60)
I0816 11:05:52.253108  3971 net.cpp:561] out_deconv_final_up2 <- ctx_final
I0816 11:05:52.253110  3971 net.cpp:530] out_deconv_final_up2 -> out_deconv_final_up2
I0816 11:05:52.253252  3971 net.cpp:245] Setting up out_deconv_final_up2
I0816 11:05:52.253262  3971 net.cpp:252] TEST Top shape for layer 60 'out_deconv_final_up2' 4 8 160 160 (819200)
I0816 11:05:52.253267  3971 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I0816 11:05:52.253268  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.253273  3971 net.cpp:184] Created Layer out_deconv_final_up4 (61)
I0816 11:05:52.253275  3971 net.cpp:561] out_deconv_final_up4 <- out_deconv_final_up2
I0816 11:05:52.253278  3971 net.cpp:530] out_deconv_final_up4 -> out_deconv_final_up4
I0816 11:05:52.253396  3971 net.cpp:245] Setting up out_deconv_final_up4
I0816 11:05:52.253401  3971 net.cpp:252] TEST Top shape for layer 61 'out_deconv_final_up4' 4 8 320 320 (3276800)
I0816 11:05:52.253403  3971 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I0816 11:05:52.253406  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.253418  3971 net.cpp:184] Created Layer out_deconv_final_up8 (62)
I0816 11:05:52.253420  3971 net.cpp:561] out_deconv_final_up8 <- out_deconv_final_up4
I0816 11:05:52.253422  3971 net.cpp:530] out_deconv_final_up8 -> out_deconv_final_up8
I0816 11:05:52.253537  3971 net.cpp:245] Setting up out_deconv_final_up8
I0816 11:05:52.253541  3971 net.cpp:252] TEST Top shape for layer 62 'out_deconv_final_up8' 4 8 640 640 (13107200)
I0816 11:05:52.253545  3971 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8_out_deconv_final_up8_0_split' of type 'Split'
I0816 11:05:52.253547  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.253551  3971 net.cpp:184] Created Layer out_deconv_final_up8_out_deconv_final_up8_0_split (63)
I0816 11:05:52.253552  3971 net.cpp:561] out_deconv_final_up8_out_deconv_final_up8_0_split <- out_deconv_final_up8
I0816 11:05:52.253554  3971 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0816 11:05:52.253557  3971 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0816 11:05:52.253559  3971 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0816 11:05:52.253587  3971 net.cpp:245] Setting up out_deconv_final_up8_out_deconv_final_up8_0_split
I0816 11:05:52.253590  3971 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 4 8 640 640 (13107200)
I0816 11:05:52.253592  3971 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 4 8 640 640 (13107200)
I0816 11:05:52.253595  3971 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 4 8 640 640 (13107200)
I0816 11:05:52.253597  3971 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0816 11:05:52.253599  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.253609  3971 net.cpp:184] Created Layer loss (64)
I0816 11:05:52.253612  3971 net.cpp:561] loss <- out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0816 11:05:52.253614  3971 net.cpp:561] loss <- label_data_1_split_0
I0816 11:05:52.253618  3971 net.cpp:530] loss -> loss
I0816 11:05:52.254565  3971 net.cpp:245] Setting up loss
I0816 11:05:52.254573  3971 net.cpp:252] TEST Top shape for layer 64 'loss' (1)
I0816 11:05:52.254575  3971 net.cpp:256]     with loss weight 1
I0816 11:05:52.254580  3971 layer_factory.hpp:136] Creating layer 'accuracy/top1' of type 'Accuracy'
I0816 11:05:52.254581  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.254590  3971 net.cpp:184] Created Layer accuracy/top1 (65)
I0816 11:05:52.254592  3971 net.cpp:561] accuracy/top1 <- out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0816 11:05:52.254595  3971 net.cpp:561] accuracy/top1 <- label_data_1_split_1
I0816 11:05:52.254603  3971 net.cpp:530] accuracy/top1 -> accuracy/top1
I0816 11:05:52.254612  3971 net.cpp:245] Setting up accuracy/top1
I0816 11:05:52.254616  3971 net.cpp:252] TEST Top shape for layer 65 'accuracy/top1' (1)
I0816 11:05:52.254618  3971 layer_factory.hpp:136] Creating layer 'accuracy/top5' of type 'Accuracy'
I0816 11:05:52.254621  3971 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 11:05:52.254623  3971 net.cpp:184] Created Layer accuracy/top5 (66)
I0816 11:05:52.254626  3971 net.cpp:561] accuracy/top5 <- out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0816 11:05:52.254629  3971 net.cpp:561] accuracy/top5 <- label_data_1_split_2
I0816 11:05:52.254631  3971 net.cpp:530] accuracy/top5 -> accuracy/top5
I0816 11:05:52.254636  3971 net.cpp:245] Setting up accuracy/top5
I0816 11:05:52.254638  3971 net.cpp:252] TEST Top shape for layer 66 'accuracy/top5' (1)
I0816 11:05:52.254640  3971 net.cpp:325] accuracy/top5 does not need backward computation.
I0816 11:05:52.254642  3971 net.cpp:325] accuracy/top1 does not need backward computation.
I0816 11:05:52.254644  3971 net.cpp:323] loss needs backward computation.
I0816 11:05:52.254647  3971 net.cpp:323] out_deconv_final_up8_out_deconv_final_up8_0_split needs backward computation.
I0816 11:05:52.254649  3971 net.cpp:323] out_deconv_final_up8 needs backward computation.
I0816 11:05:52.254652  3971 net.cpp:323] out_deconv_final_up4 needs backward computation.
I0816 11:05:52.254653  3971 net.cpp:323] out_deconv_final_up2 needs backward computation.
I0816 11:05:52.254657  3971 net.cpp:323] ctx_final/relu needs backward computation.
I0816 11:05:52.254657  3971 net.cpp:323] ctx_final needs backward computation.
I0816 11:05:52.254660  3971 net.cpp:323] ctx_conv4/relu needs backward computation.
I0816 11:05:52.254662  3971 net.cpp:323] ctx_conv4/bn needs backward computation.
I0816 11:05:52.254663  3971 net.cpp:323] ctx_conv4 needs backward computation.
I0816 11:05:52.254667  3971 net.cpp:323] ctx_conv3/relu needs backward computation.
I0816 11:05:52.254668  3971 net.cpp:323] ctx_conv3/bn needs backward computation.
I0816 11:05:52.254669  3971 net.cpp:323] ctx_conv3 needs backward computation.
I0816 11:05:52.254672  3971 net.cpp:323] ctx_conv2/relu needs backward computation.
I0816 11:05:52.254673  3971 net.cpp:323] ctx_conv2/bn needs backward computation.
I0816 11:05:52.254674  3971 net.cpp:323] ctx_conv2 needs backward computation.
I0816 11:05:52.254676  3971 net.cpp:323] ctx_conv1/relu needs backward computation.
I0816 11:05:52.254678  3971 net.cpp:323] ctx_conv1/bn needs backward computation.
I0816 11:05:52.254680  3971 net.cpp:323] ctx_conv1 needs backward computation.
I0816 11:05:52.254683  3971 net.cpp:323] out3_out5_combined needs backward computation.
I0816 11:05:52.254684  3971 net.cpp:323] out3a/relu needs backward computation.
I0816 11:05:52.254686  3971 net.cpp:323] out3a/bn needs backward computation.
I0816 11:05:52.254688  3971 net.cpp:323] out3a needs backward computation.
I0816 11:05:52.254690  3971 net.cpp:323] out5a_up2 needs backward computation.
I0816 11:05:52.254693  3971 net.cpp:323] out5a/relu needs backward computation.
I0816 11:05:52.254694  3971 net.cpp:323] out5a/bn needs backward computation.
I0816 11:05:52.254695  3971 net.cpp:323] out5a needs backward computation.
I0816 11:05:52.254701  3971 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0816 11:05:52.254704  3971 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0816 11:05:52.254706  3971 net.cpp:323] res5a_branch2b needs backward computation.
I0816 11:05:52.254709  3971 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0816 11:05:52.254710  3971 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0816 11:05:52.254712  3971 net.cpp:323] res5a_branch2a needs backward computation.
I0816 11:05:52.254714  3971 net.cpp:323] pool4 needs backward computation.
I0816 11:05:52.254716  3971 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0816 11:05:52.254719  3971 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0816 11:05:52.254724  3971 net.cpp:323] res4a_branch2b needs backward computation.
I0816 11:05:52.254726  3971 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0816 11:05:52.254729  3971 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0816 11:05:52.254731  3971 net.cpp:323] res4a_branch2a needs backward computation.
I0816 11:05:52.254734  3971 net.cpp:323] pool3 needs backward computation.
I0816 11:05:52.254735  3971 net.cpp:323] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I0816 11:05:52.254737  3971 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0816 11:05:52.254739  3971 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0816 11:05:52.254741  3971 net.cpp:323] res3a_branch2b needs backward computation.
I0816 11:05:52.254743  3971 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0816 11:05:52.254745  3971 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0816 11:05:52.254747  3971 net.cpp:323] res3a_branch2a needs backward computation.
I0816 11:05:52.254750  3971 net.cpp:323] pool2 needs backward computation.
I0816 11:05:52.254751  3971 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0816 11:05:52.254753  3971 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0816 11:05:52.254755  3971 net.cpp:323] res2a_branch2b needs backward computation.
I0816 11:05:52.254757  3971 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0816 11:05:52.254760  3971 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0816 11:05:52.254761  3971 net.cpp:323] res2a_branch2a needs backward computation.
I0816 11:05:52.254763  3971 net.cpp:323] pool1 needs backward computation.
I0816 11:05:52.254765  3971 net.cpp:323] conv1b/relu needs backward computation.
I0816 11:05:52.254767  3971 net.cpp:323] conv1b/bn needs backward computation.
I0816 11:05:52.254770  3971 net.cpp:323] conv1b needs backward computation.
I0816 11:05:52.254771  3971 net.cpp:323] conv1a/relu needs backward computation.
I0816 11:05:52.254773  3971 net.cpp:323] conv1a/bn needs backward computation.
I0816 11:05:52.254776  3971 net.cpp:323] conv1a needs backward computation.
I0816 11:05:52.254777  3971 net.cpp:325] data/bias does not need backward computation.
I0816 11:05:52.254781  3971 net.cpp:325] label_data_1_split does not need backward computation.
I0816 11:05:52.254782  3971 net.cpp:325] data does not need backward computation.
I0816 11:05:52.254784  3971 net.cpp:367] This network produces output accuracy/top1
I0816 11:05:52.254786  3971 net.cpp:367] This network produces output accuracy/top5
I0816 11:05:52.254788  3971 net.cpp:367] This network produces output loss
I0816 11:05:52.254828  3971 net.cpp:389] Top memory (TEST) required for data: 637337600 diff: 8
I0816 11:05:52.254832  3971 net.cpp:392] Bottom memory (TEST) required for data: 637337600 diff: 637337600
I0816 11:05:52.254833  3971 net.cpp:395] Shared (in-place) memory (TEST) by data: 420249600 diff: 420249600
I0816 11:05:52.254835  3971 net.cpp:398] Parameters memory (TEST) required for data: 2692608 diff: 2692608
I0816 11:05:52.254837  3971 net.cpp:401] Parameters shared memory (TEST) by data: 0 diff: 0
I0816 11:05:52.254839  3971 net.cpp:407] Network initialization done.
I0816 11:05:52.259629  3971 net.cpp:1095] Copying source layer data Type:ImageLabelData #blobs=0
I0816 11:05:52.259647  3971 net.cpp:1095] Copying source layer data/bias Type:Bias #blobs=1
I0816 11:05:52.259680  3971 net.cpp:1095] Copying source layer conv1a Type:Convolution #blobs=2
I0816 11:05:52.259691  3971 net.cpp:1095] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0816 11:05:52.259938  3971 net.cpp:1095] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0816 11:05:52.259943  3971 net.cpp:1095] Copying source layer conv1b Type:Convolution #blobs=2
I0816 11:05:52.259953  3971 net.cpp:1095] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0816 11:05:52.260126  3971 net.cpp:1095] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0816 11:05:52.260143  3971 net.cpp:1095] Copying source layer pool1 Type:Pooling #blobs=0
I0816 11:05:52.260159  3971 net.cpp:1095] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0816 11:05:52.260176  3971 net.cpp:1095] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0816 11:05:52.260351  3971 net.cpp:1095] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0816 11:05:52.260356  3971 net.cpp:1095] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0816 11:05:52.260367  3971 net.cpp:1095] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0816 11:05:52.260536  3971 net.cpp:1095] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0816 11:05:52.260541  3971 net.cpp:1095] Copying source layer pool2 Type:Pooling #blobs=0
I0816 11:05:52.260543  3971 net.cpp:1095] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0816 11:05:52.260581  3971 net.cpp:1095] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0816 11:05:52.260746  3971 net.cpp:1095] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0816 11:05:52.260751  3971 net.cpp:1095] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0816 11:05:52.260772  3971 net.cpp:1095] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0816 11:05:52.260929  3971 net.cpp:1095] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0816 11:05:52.260936  3971 net.cpp:1095] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I0816 11:05:52.260938  3971 net.cpp:1095] Copying source layer pool3 Type:Pooling #blobs=0
I0816 11:05:52.260941  3971 net.cpp:1095] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0816 11:05:52.261066  3971 net.cpp:1095] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0816 11:05:52.261226  3971 net.cpp:1095] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0816 11:05:52.261230  3971 net.cpp:1095] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0816 11:05:52.261289  3971 net.cpp:1095] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0816 11:05:52.261448  3971 net.cpp:1095] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0816 11:05:52.261453  3971 net.cpp:1095] Copying source layer pool4 Type:Pooling #blobs=0
I0816 11:05:52.261456  3971 net.cpp:1095] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0816 11:05:52.261929  3971 net.cpp:1095] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0816 11:05:52.262109  3971 net.cpp:1095] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0816 11:05:52.262115  3971 net.cpp:1095] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0816 11:05:52.262480  3971 net.cpp:1095] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0816 11:05:52.262657  3971 net.cpp:1095] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0816 11:05:52.262663  3971 net.cpp:1095] Copying source layer out5a Type:Convolution #blobs=2
I0816 11:05:52.262773  3971 net.cpp:1095] Copying source layer out5a/bn Type:BatchNorm #blobs=5
I0816 11:05:52.262874  3971 net.cpp:1095] Copying source layer out5a/relu Type:ReLU #blobs=0
I0816 11:05:52.262881  3971 net.cpp:1095] Copying source layer out5a_up2 Type:Deconvolution #blobs=1
I0816 11:05:52.262889  3971 net.cpp:1095] Copying source layer out3a Type:Convolution #blobs=2
I0816 11:05:52.262923  3971 net.cpp:1095] Copying source layer out3a/bn Type:BatchNorm #blobs=5
I0816 11:05:52.263022  3971 net.cpp:1095] Copying source layer out3a/relu Type:ReLU #blobs=0
I0816 11:05:52.263027  3971 net.cpp:1095] Copying source layer out3_out5_combined Type:Eltwise #blobs=0
I0816 11:05:52.263031  3971 net.cpp:1095] Copying source layer ctx_conv1 Type:Convolution #blobs=2
I0816 11:05:52.263068  3971 net.cpp:1095] Copying source layer ctx_conv1/bn Type:BatchNorm #blobs=5
I0816 11:05:52.263160  3971 net.cpp:1095] Copying source layer ctx_conv1/relu Type:ReLU #blobs=0
I0816 11:05:52.263165  3971 net.cpp:1095] Copying source layer ctx_conv2 Type:Convolution #blobs=2
I0816 11:05:52.263212  3971 net.cpp:1095] Copying source layer ctx_conv2/bn Type:BatchNorm #blobs=5
I0816 11:05:52.263311  3971 net.cpp:1095] Copying source layer ctx_conv2/relu Type:ReLU #blobs=0
I0816 11:05:52.263317  3971 net.cpp:1095] Copying source layer ctx_conv3 Type:Convolution #blobs=2
I0816 11:05:52.263355  3971 net.cpp:1095] Copying source layer ctx_conv3/bn Type:BatchNorm #blobs=5
I0816 11:05:52.263445  3971 net.cpp:1095] Copying source layer ctx_conv3/relu Type:ReLU #blobs=0
I0816 11:05:52.263450  3971 net.cpp:1095] Copying source layer ctx_conv4 Type:Convolution #blobs=2
I0816 11:05:52.263473  3971 net.cpp:1095] Copying source layer ctx_conv4/bn Type:BatchNorm #blobs=5
I0816 11:05:52.263566  3971 net.cpp:1095] Copying source layer ctx_conv4/relu Type:ReLU #blobs=0
I0816 11:05:52.263571  3971 net.cpp:1095] Copying source layer ctx_final Type:Convolution #blobs=2
I0816 11:05:52.263584  3971 net.cpp:1095] Copying source layer ctx_final/relu Type:ReLU #blobs=0
I0816 11:05:52.263588  3971 net.cpp:1095] Copying source layer out_deconv_final_up2 Type:Deconvolution #blobs=1
I0816 11:05:52.263597  3971 net.cpp:1095] Copying source layer out_deconv_final_up4 Type:Deconvolution #blobs=1
I0816 11:05:52.263604  3971 net.cpp:1095] Copying source layer out_deconv_final_up8 Type:Deconvolution #blobs=1
I0816 11:05:52.263612  3971 net.cpp:1095] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0816 11:05:52.263697  3971 caffe.cpp:290] Running for 50 iterations.
I0816 11:05:52.269306  3971 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.72G, req 0G)
I0816 11:05:52.289592  3971 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.62G, req 0G)
I0816 11:05:52.305897  3971 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.5G, req 0G)
I0816 11:05:52.315423  3971 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.44G, req 0G)
I0816 11:05:52.323294  3971 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.38G, req 0G)
I0816 11:05:52.328651  3971 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.35G, req 0G)
I0816 11:05:52.335531  3971 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.33G, req 0G)
I0816 11:05:52.339160  3971 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.32G, req 0G)
I0816 11:05:52.362452  3971 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 7.09G, req 0G)
I0816 11:05:52.367488  3971 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 7.07G, req 0G)
I0816 11:05:52.381685  3971 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.94G, req 0G)
I0816 11:05:52.573099  3971 caffe.cpp:313] Batch 0, accuracy/top1 = 0.930066
I0816 11:05:52.573119  3971 caffe.cpp:313] Batch 0, accuracy/top5 = 1
I0816 11:05:52.573122  3971 caffe.cpp:313] Batch 0, loss = 0.220756
I0816 11:05:52.573125  3971 net.cpp:1620] Adding quantization params at infer/iter index: 1
I0816 11:05:52.579664  3971 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 1.22G/1 1  (limit 5.47G, req 0G)
I0816 11:05:52.604856  3971 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 2.44G/2 6  (limit 4.25G, req 0G)
I0816 11:05:52.650084  3971 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 2.44G/1 6  (limit 4.25G, req 0G)
I0816 11:05:52.665791  3971 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 2.44G/2 6  (limit 4.25G, req 0G)
I0816 11:05:52.701179  3971 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 2.44G/1 6  (limit 4.25G, req 0G)
I0816 11:05:52.710367  3971 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 2.44G/2 6  (limit 4.25G, req 0G)
I0816 11:05:52.731895  3971 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 2.44G/1 6  (limit 4.25G, req 0G)
I0816 11:05:52.737519  3971 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 2.44G/2 6  (limit 4.25G, req 0G)
I0816 11:05:52.765954  3971 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'out3a' with space 2.44G/2 6  (limit 4.25G, req 0G)
I0816 11:05:52.785763  3971 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_conv1' with space 2.44G/1 6  (limit 4.25G, req 0G)
I0816 11:05:52.798235  3971 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_final' with space 2.44G/1 6  (limit 4.25G, req 0G)
I0816 11:05:52.970999  3971 caffe.cpp:313] Batch 1, accuracy/top1 = 0.950159
I0816 11:05:52.971021  3971 caffe.cpp:313] Batch 1, accuracy/top5 = 1
I0816 11:05:52.971024  3971 caffe.cpp:313] Batch 1, loss = 0.165596
I0816 11:05:53.181766  3971 caffe.cpp:313] Batch 2, accuracy/top1 = 0.960748
I0816 11:05:53.181787  3971 caffe.cpp:313] Batch 2, accuracy/top5 = 1
I0816 11:05:53.181790  3971 caffe.cpp:313] Batch 2, loss = 0.120175
I0816 11:05:53.393471  3971 caffe.cpp:313] Batch 3, accuracy/top1 = 0.970944
I0816 11:05:53.393493  3971 caffe.cpp:313] Batch 3, accuracy/top5 = 1
I0816 11:05:53.393496  3971 caffe.cpp:313] Batch 3, loss = 0.0790503
I0816 11:05:53.603499  3971 caffe.cpp:313] Batch 4, accuracy/top1 = 0.960007
I0816 11:05:53.603520  3971 caffe.cpp:313] Batch 4, accuracy/top5 = 0.999985
I0816 11:05:53.603523  3971 caffe.cpp:313] Batch 4, loss = 0.154211
I0816 11:05:53.815008  3971 caffe.cpp:313] Batch 5, accuracy/top1 = 0.798518
I0816 11:05:53.815026  3971 caffe.cpp:313] Batch 5, accuracy/top5 = 1
I0816 11:05:53.815028  3971 caffe.cpp:313] Batch 5, loss = 0.998597
I0816 11:05:54.026167  3971 caffe.cpp:313] Batch 6, accuracy/top1 = 0.962305
I0816 11:05:54.026187  3971 caffe.cpp:313] Batch 6, accuracy/top5 = 1
I0816 11:05:54.026190  3971 caffe.cpp:313] Batch 6, loss = 0.105262
I0816 11:05:54.240540  3971 caffe.cpp:313] Batch 7, accuracy/top1 = 0.962383
I0816 11:05:54.240556  3971 caffe.cpp:313] Batch 7, accuracy/top5 = 1
I0816 11:05:54.240559  3971 caffe.cpp:313] Batch 7, loss = 0.0918731
I0816 11:05:54.450732  3971 caffe.cpp:313] Batch 8, accuracy/top1 = 0.974322
I0816 11:05:54.450754  3971 caffe.cpp:313] Batch 8, accuracy/top5 = 1
I0816 11:05:54.450757  3971 caffe.cpp:313] Batch 8, loss = 0.0685061
I0816 11:05:54.662571  3971 caffe.cpp:313] Batch 9, accuracy/top1 = 0.982076
I0816 11:05:54.662591  3971 caffe.cpp:313] Batch 9, accuracy/top5 = 1
I0816 11:05:54.662595  3971 caffe.cpp:313] Batch 9, loss = 0.0498371
I0816 11:05:54.875692  3971 caffe.cpp:313] Batch 10, accuracy/top1 = 0.879669
I0816 11:05:54.875712  3971 caffe.cpp:313] Batch 10, accuracy/top5 = 1
I0816 11:05:54.875715  3971 caffe.cpp:313] Batch 10, loss = 0.418494
I0816 11:05:55.087929  3971 caffe.cpp:313] Batch 11, accuracy/top1 = 0.972764
I0816 11:05:55.087946  3971 caffe.cpp:313] Batch 11, accuracy/top5 = 1
I0816 11:05:55.087949  3971 caffe.cpp:313] Batch 11, loss = 0.0733404
I0816 11:05:55.299371  3971 caffe.cpp:313] Batch 12, accuracy/top1 = 0.966473
I0816 11:05:55.299393  3971 caffe.cpp:313] Batch 12, accuracy/top5 = 1
I0816 11:05:55.299396  3971 caffe.cpp:313] Batch 12, loss = 0.0918751
I0816 11:05:55.511545  3971 caffe.cpp:313] Batch 13, accuracy/top1 = 0.979729
I0816 11:05:55.511566  3971 caffe.cpp:313] Batch 13, accuracy/top5 = 1
I0816 11:05:55.511569  3971 caffe.cpp:313] Batch 13, loss = 0.0545078
I0816 11:05:55.723857  3971 caffe.cpp:313] Batch 14, accuracy/top1 = 0.983853
I0816 11:05:55.723875  3971 caffe.cpp:313] Batch 14, accuracy/top5 = 1
I0816 11:05:55.723877  3971 caffe.cpp:313] Batch 14, loss = 0.0444868
I0816 11:05:55.935639  3971 caffe.cpp:313] Batch 15, accuracy/top1 = 0.963123
I0816 11:05:55.935660  3971 caffe.cpp:313] Batch 15, accuracy/top5 = 1
I0816 11:05:55.935663  3971 caffe.cpp:313] Batch 15, loss = 0.104048
I0816 11:05:56.147586  3971 caffe.cpp:313] Batch 16, accuracy/top1 = 0.883975
I0816 11:05:56.147606  3971 caffe.cpp:313] Batch 16, accuracy/top5 = 1
I0816 11:05:56.147608  3971 caffe.cpp:313] Batch 16, loss = 0.513625
I0816 11:05:56.358124  3971 caffe.cpp:313] Batch 17, accuracy/top1 = 0.866594
I0816 11:05:56.358141  3971 caffe.cpp:313] Batch 17, accuracy/top5 = 1
I0816 11:05:56.358144  3971 caffe.cpp:313] Batch 17, loss = 0.668199
I0816 11:05:56.572443  3971 caffe.cpp:313] Batch 18, accuracy/top1 = 0.983167
I0816 11:05:56.572464  3971 caffe.cpp:313] Batch 18, accuracy/top5 = 0.99999
I0816 11:05:56.572468  3971 caffe.cpp:313] Batch 18, loss = 0.0433731
I0816 11:05:56.784574  3971 caffe.cpp:313] Batch 19, accuracy/top1 = 0.982637
I0816 11:05:56.784591  3971 caffe.cpp:313] Batch 19, accuracy/top5 = 1
I0816 11:05:56.784595  3971 caffe.cpp:313] Batch 19, loss = 0.0495629
I0816 11:05:56.996191  3971 caffe.cpp:313] Batch 20, accuracy/top1 = 0.975195
I0816 11:05:56.996212  3971 caffe.cpp:313] Batch 20, accuracy/top5 = 1
I0816 11:05:56.996215  3971 caffe.cpp:313] Batch 20, loss = 0.0716905
I0816 11:05:57.206673  3971 caffe.cpp:313] Batch 21, accuracy/top1 = 0.892168
I0816 11:05:57.206696  3971 caffe.cpp:313] Batch 21, accuracy/top5 = 0.999998
I0816 11:05:57.206697  3971 caffe.cpp:313] Batch 21, loss = 0.601368
I0816 11:05:57.418679  3971 caffe.cpp:313] Batch 22, accuracy/top1 = 0.968012
I0816 11:05:57.418701  3971 caffe.cpp:313] Batch 22, accuracy/top5 = 1
I0816 11:05:57.418704  3971 caffe.cpp:313] Batch 22, loss = 0.0857519
I0816 11:05:57.631160  3971 caffe.cpp:313] Batch 23, accuracy/top1 = 0.978132
I0816 11:05:57.631182  3971 caffe.cpp:313] Batch 23, accuracy/top5 = 1
I0816 11:05:57.631186  3971 caffe.cpp:313] Batch 23, loss = 0.0592591
I0816 11:05:57.846052  3971 caffe.cpp:313] Batch 24, accuracy/top1 = 0.950978
I0816 11:05:57.846071  3971 caffe.cpp:313] Batch 24, accuracy/top5 = 1
I0816 11:05:57.846076  3971 caffe.cpp:313] Batch 24, loss = 0.129788
I0816 11:05:58.059958  3971 caffe.cpp:313] Batch 25, accuracy/top1 = 0.972678
I0816 11:05:58.059978  3971 caffe.cpp:313] Batch 25, accuracy/top5 = 1
I0816 11:05:58.059980  3971 caffe.cpp:313] Batch 25, loss = 0.0759051
I0816 11:05:58.269929  3971 caffe.cpp:313] Batch 26, accuracy/top1 = 0.953137
I0816 11:05:58.269949  3971 caffe.cpp:313] Batch 26, accuracy/top5 = 1
I0816 11:05:58.269953  3971 caffe.cpp:313] Batch 26, loss = 0.122646
I0816 11:05:58.482113  3971 caffe.cpp:313] Batch 27, accuracy/top1 = 0.963725
I0816 11:05:58.482134  3971 caffe.cpp:313] Batch 27, accuracy/top5 = 1
I0816 11:05:58.482137  3971 caffe.cpp:313] Batch 27, loss = 0.110574
I0816 11:05:58.693307  3971 caffe.cpp:313] Batch 28, accuracy/top1 = 0.952549
I0816 11:05:58.693330  3971 caffe.cpp:313] Batch 28, accuracy/top5 = 1
I0816 11:05:58.693332  3971 caffe.cpp:313] Batch 28, loss = 0.128334
I0816 11:05:58.906098  3971 caffe.cpp:313] Batch 29, accuracy/top1 = 0.963799
I0816 11:05:58.906118  3971 caffe.cpp:313] Batch 29, accuracy/top5 = 1
I0816 11:05:58.906121  3971 caffe.cpp:313] Batch 29, loss = 0.112243
I0816 11:05:59.117725  3971 caffe.cpp:313] Batch 30, accuracy/top1 = 0.840544
I0816 11:05:59.117744  3971 caffe.cpp:313] Batch 30, accuracy/top5 = 1
I0816 11:05:59.117748  3971 caffe.cpp:313] Batch 30, loss = 0.664692
I0816 11:05:59.328480  3971 caffe.cpp:313] Batch 31, accuracy/top1 = 0.968706
I0816 11:05:59.328501  3971 caffe.cpp:313] Batch 31, accuracy/top5 = 1
I0816 11:05:59.328505  3971 caffe.cpp:313] Batch 31, loss = 0.0845477
I0816 11:05:59.538887  3971 caffe.cpp:313] Batch 32, accuracy/top1 = 0.946506
I0816 11:05:59.538908  3971 caffe.cpp:313] Batch 32, accuracy/top5 = 1
I0816 11:05:59.538911  3971 caffe.cpp:313] Batch 32, loss = 0.143015
I0816 11:05:59.750058  3971 caffe.cpp:313] Batch 33, accuracy/top1 = 0.967017
I0816 11:05:59.750075  3971 caffe.cpp:313] Batch 33, accuracy/top5 = 1
I0816 11:05:59.750078  3971 caffe.cpp:313] Batch 33, loss = 0.0880318
I0816 11:05:59.963929  3971 caffe.cpp:313] Batch 34, accuracy/top1 = 0.977803
I0816 11:05:59.963949  3971 caffe.cpp:313] Batch 34, accuracy/top5 = 1
I0816 11:05:59.963953  3971 caffe.cpp:313] Batch 34, loss = 0.0658997
I0816 11:06:00.174700  3971 caffe.cpp:313] Batch 35, accuracy/top1 = 0.975479
I0816 11:06:00.174721  3971 caffe.cpp:313] Batch 35, accuracy/top5 = 1
I0816 11:06:00.174738  3971 caffe.cpp:313] Batch 35, loss = 0.0614217
I0816 11:06:00.385669  3971 caffe.cpp:313] Batch 36, accuracy/top1 = 0.961253
I0816 11:06:00.385689  3971 caffe.cpp:313] Batch 36, accuracy/top5 = 1
I0816 11:06:00.385691  3971 caffe.cpp:313] Batch 36, loss = 0.116513
I0816 11:06:00.595527  3971 caffe.cpp:313] Batch 37, accuracy/top1 = 0.960462
I0816 11:06:00.595549  3971 caffe.cpp:313] Batch 37, accuracy/top5 = 1
I0816 11:06:00.595552  3971 caffe.cpp:313] Batch 37, loss = 0.130968
I0816 11:06:00.806789  3971 caffe.cpp:313] Batch 38, accuracy/top1 = 0.938473
I0816 11:06:00.806807  3971 caffe.cpp:313] Batch 38, accuracy/top5 = 1
I0816 11:06:00.806810  3971 caffe.cpp:313] Batch 38, loss = 0.164148
I0816 11:06:01.014179  3971 caffe.cpp:313] Batch 39, accuracy/top1 = 0.926797
I0816 11:06:01.014199  3971 caffe.cpp:313] Batch 39, accuracy/top5 = 1
I0816 11:06:01.014202  3971 caffe.cpp:313] Batch 39, loss = 0.213931
I0816 11:06:01.225769  3971 caffe.cpp:313] Batch 40, accuracy/top1 = 0.981233
I0816 11:06:01.225790  3971 caffe.cpp:313] Batch 40, accuracy/top5 = 1
I0816 11:06:01.225795  3971 caffe.cpp:313] Batch 40, loss = 0.0586615
I0816 11:06:01.438674  3971 caffe.cpp:313] Batch 41, accuracy/top1 = 0.976845
I0816 11:06:01.438694  3971 caffe.cpp:313] Batch 41, accuracy/top5 = 1
I0816 11:06:01.438696  3971 caffe.cpp:313] Batch 41, loss = 0.0681765
I0816 11:06:01.650933  3971 caffe.cpp:313] Batch 42, accuracy/top1 = 0.96581
I0816 11:06:01.650956  3971 caffe.cpp:313] Batch 42, accuracy/top5 = 1
I0816 11:06:01.650959  3971 caffe.cpp:313] Batch 42, loss = 0.110996
I0816 11:06:01.864599  3971 caffe.cpp:313] Batch 43, accuracy/top1 = 0.973115
I0816 11:06:01.864619  3971 caffe.cpp:313] Batch 43, accuracy/top5 = 1
I0816 11:06:01.864622  3971 caffe.cpp:313] Batch 43, loss = 0.073785
I0816 11:06:02.077468  3971 caffe.cpp:313] Batch 44, accuracy/top1 = 0.957501
I0816 11:06:02.077486  3971 caffe.cpp:313] Batch 44, accuracy/top5 = 1
I0816 11:06:02.077489  3971 caffe.cpp:313] Batch 44, loss = 0.124757
I0816 11:06:02.289104  3971 caffe.cpp:313] Batch 45, accuracy/top1 = 0.97663
I0816 11:06:02.289125  3971 caffe.cpp:313] Batch 45, accuracy/top5 = 1
I0816 11:06:02.289129  3971 caffe.cpp:313] Batch 45, loss = 0.0764358
I0816 11:06:02.502238  3971 caffe.cpp:313] Batch 46, accuracy/top1 = 0.971944
I0816 11:06:02.502257  3971 caffe.cpp:313] Batch 46, accuracy/top5 = 1
I0816 11:06:02.502260  3971 caffe.cpp:313] Batch 46, loss = 0.0742174
I0816 11:06:02.715572  3971 caffe.cpp:313] Batch 47, accuracy/top1 = 0.966643
I0816 11:06:02.715610  3971 caffe.cpp:313] Batch 47, accuracy/top5 = 1
I0816 11:06:02.715615  3971 caffe.cpp:313] Batch 47, loss = 0.12234
I0816 11:06:02.926201  3971 caffe.cpp:313] Batch 48, accuracy/top1 = 0.875361
I0816 11:06:02.926223  3971 caffe.cpp:313] Batch 48, accuracy/top5 = 1
I0816 11:06:02.926228  3971 caffe.cpp:313] Batch 48, loss = 0.47943
I0816 11:06:03.134582  3971 caffe.cpp:313] Batch 49, accuracy/top1 = 0.948827
I0816 11:06:03.134603  3971 caffe.cpp:313] Batch 49, accuracy/top5 = 1
I0816 11:06:03.134606  3971 caffe.cpp:313] Batch 49, loss = 0.137649
I0816 11:06:03.134610  3971 caffe.cpp:318] Loss: 0.173451
I0816 11:06:03.134620  3971 caffe.cpp:330] accuracy/top1 = 0.950817
I0816 11:06:03.134626  3971 caffe.cpp:330] accuracy/top5 = 0.999999
I0816 11:06:03.134634  3971 caffe.cpp:330] loss = 0.173451 (* 1 = 0.173451 loss)
