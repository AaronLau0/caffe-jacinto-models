Logging output to training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/train-log_2017-07-11_18-09-28.txt
Using pretrained model training/imagenet_jacintonet11v2_iter_320000.caffemodel
training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/initial
I0711 18:09:29.790408  7290 caffe.cpp:209] Using GPUs 0, 1, 2
I0711 18:09:29.790868  7290 caffe.cpp:214] GPU 0: GeForce GTX 1080
I0711 18:09:29.791204  7290 caffe.cpp:214] GPU 1: GeForce GTX 1080
I0711 18:09:29.791538  7290 caffe.cpp:214] GPU 2: GeForce GTX 1080
I0711 18:09:31.132441  7290 solver.cpp:48] Initializing solver from parameters: 
train_net: "training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/initial/train.prototxt"
test_net: "training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/initial/test.prototxt"
test_iter: 125
test_interval: 2000
base_lr: 0.0001
display: 100
max_iter: 32000
lr_policy: "multistep"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 0.0001
snapshot: 10000
snapshot_prefix: "training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/initial/cityscapes5_jsegnet21v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
test_initialization: false
stepvalue: 24000
iter_size: 1
type: "Adam"
I0711 18:09:31.132540  7290 solver.cpp:82] Creating training net from train_net file: training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/initial/train.prototxt
I0711 18:09:31.133242  7290 net.cpp:327] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0711 18:09:31.133249  7290 net.cpp:327] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
I0711 18:09:31.133507  7290 net.cpp:56] Initializing net from parameters: 
name: "jsegnet21v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/train-image-lmdb"
    label_list_path: "data/train-label-lmdb"
    batch_size: 5
    shuffle: false
    threads: 4
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b/bn"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a/bn"
  top: "out5a/bn"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a/bn"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b/bn"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a/bn"
  top: "out3a/bn"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a/bn"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv1/bn"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv2/bn"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv3/bn"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4/bn"
  top: "ctx_conv4/bn"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4/bn"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
I0711 18:09:31.133642  7290 layer_factory.hpp:77] Creating layer data
I0711 18:09:31.133656  7290 net.cpp:98] Creating Layer data
I0711 18:09:31.133659  7290 net.cpp:413] data -> data
I0711 18:09:31.133675  7290 net.cpp:413] data -> label
I0711 18:09:31.135936  7342 db_lmdb.cpp:35] Opened lmdb data/train-image-lmdb
I0711 18:09:31.165035  7347 db_lmdb.cpp:35] Opened lmdb data/train-label-lmdb
I0711 18:09:31.225064  7290 data_layer.cpp:78] ReshapePrefetch 5, 3, 640, 640
I0711 18:09:31.225127  7290 data_layer.cpp:83] output data size: 5,3,640,640
I0711 18:09:31.253641  7290 data_layer.cpp:78] ReshapePrefetch 5, 1, 640, 640
I0711 18:09:31.253711  7290 data_layer.cpp:83] output data size: 5,1,640,640
I0711 18:09:31.263092  7352 blocking_queue.cpp:50] Waiting for data
I0711 18:09:31.294282  7290 net.cpp:148] Setting up data
I0711 18:09:31.294306  7290 net.cpp:155] Top shape: 5 3 640 640 (6144000)
I0711 18:09:31.294308  7290 net.cpp:155] Top shape: 5 1 640 640 (2048000)
I0711 18:09:31.294311  7290 net.cpp:163] Memory required for data: 32768000
I0711 18:09:31.294317  7290 layer_factory.hpp:77] Creating layer data/bias
I0711 18:09:31.294332  7290 net.cpp:98] Creating Layer data/bias
I0711 18:09:31.294337  7290 net.cpp:439] data/bias <- data
I0711 18:09:31.294347  7290 net.cpp:413] data/bias -> data/bias
I0711 18:09:31.295812  7290 net.cpp:148] Setting up data/bias
I0711 18:09:31.295826  7290 net.cpp:155] Top shape: 5 3 640 640 (6144000)
I0711 18:09:31.295828  7290 net.cpp:163] Memory required for data: 57344000
I0711 18:09:31.295840  7290 layer_factory.hpp:77] Creating layer conv1a
I0711 18:09:31.295855  7290 net.cpp:98] Creating Layer conv1a
I0711 18:09:31.295866  7290 net.cpp:439] conv1a <- data/bias
I0711 18:09:31.295872  7290 net.cpp:413] conv1a -> conv1a
I0711 18:09:31.297780  7290 net.cpp:148] Setting up conv1a
I0711 18:09:31.297801  7290 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0711 18:09:31.297804  7290 net.cpp:163] Memory required for data: 122880000
I0711 18:09:31.297812  7290 layer_factory.hpp:77] Creating layer conv1a/bn
I0711 18:09:31.297821  7290 net.cpp:98] Creating Layer conv1a/bn
I0711 18:09:31.297827  7290 net.cpp:439] conv1a/bn <- conv1a
I0711 18:09:31.297832  7290 net.cpp:413] conv1a/bn -> conv1a/bn
I0711 18:09:31.299615  7290 net.cpp:148] Setting up conv1a/bn
I0711 18:09:31.299628  7290 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0711 18:09:31.299631  7290 net.cpp:163] Memory required for data: 188416000
I0711 18:09:31.299638  7290 layer_factory.hpp:77] Creating layer conv1a/relu
I0711 18:09:31.299643  7290 net.cpp:98] Creating Layer conv1a/relu
I0711 18:09:31.299645  7290 net.cpp:439] conv1a/relu <- conv1a/bn
I0711 18:09:31.299649  7290 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0711 18:09:31.299662  7290 net.cpp:148] Setting up conv1a/relu
I0711 18:09:31.299665  7290 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0711 18:09:31.299679  7290 net.cpp:163] Memory required for data: 253952000
I0711 18:09:31.299681  7290 layer_factory.hpp:77] Creating layer conv1b
I0711 18:09:31.299690  7290 net.cpp:98] Creating Layer conv1b
I0711 18:09:31.299695  7290 net.cpp:439] conv1b <- conv1a/bn
I0711 18:09:31.299700  7290 net.cpp:413] conv1b -> conv1b
I0711 18:09:31.300189  7290 net.cpp:148] Setting up conv1b
I0711 18:09:31.300195  7290 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0711 18:09:31.300199  7290 net.cpp:163] Memory required for data: 319488000
I0711 18:09:31.300202  7290 layer_factory.hpp:77] Creating layer conv1b/bn
I0711 18:09:31.300206  7290 net.cpp:98] Creating Layer conv1b/bn
I0711 18:09:31.300209  7290 net.cpp:439] conv1b/bn <- conv1b
I0711 18:09:31.300211  7290 net.cpp:413] conv1b/bn -> conv1b/bn
I0711 18:09:31.301023  7290 net.cpp:148] Setting up conv1b/bn
I0711 18:09:31.301030  7290 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0711 18:09:31.301033  7290 net.cpp:163] Memory required for data: 385024000
I0711 18:09:31.301038  7290 layer_factory.hpp:77] Creating layer conv1b/relu
I0711 18:09:31.301040  7290 net.cpp:98] Creating Layer conv1b/relu
I0711 18:09:31.301043  7290 net.cpp:439] conv1b/relu <- conv1b/bn
I0711 18:09:31.301045  7290 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0711 18:09:31.301049  7290 net.cpp:148] Setting up conv1b/relu
I0711 18:09:31.301051  7290 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0711 18:09:31.301054  7290 net.cpp:163] Memory required for data: 450560000
I0711 18:09:31.301055  7290 layer_factory.hpp:77] Creating layer pool1
I0711 18:09:31.301060  7290 net.cpp:98] Creating Layer pool1
I0711 18:09:31.301062  7290 net.cpp:439] pool1 <- conv1b/bn
I0711 18:09:31.301064  7290 net.cpp:413] pool1 -> pool1
I0711 18:09:31.301126  7290 net.cpp:148] Setting up pool1
I0711 18:09:31.301133  7290 net.cpp:155] Top shape: 5 32 160 160 (4096000)
I0711 18:09:31.301136  7290 net.cpp:163] Memory required for data: 466944000
I0711 18:09:31.301139  7290 layer_factory.hpp:77] Creating layer res2a_branch2a
I0711 18:09:31.301146  7290 net.cpp:98] Creating Layer res2a_branch2a
I0711 18:09:31.301149  7290 net.cpp:439] res2a_branch2a <- pool1
I0711 18:09:31.301154  7290 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0711 18:09:31.302872  7290 net.cpp:148] Setting up res2a_branch2a
I0711 18:09:31.302883  7290 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0711 18:09:31.302886  7290 net.cpp:163] Memory required for data: 499712000
I0711 18:09:31.302891  7290 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0711 18:09:31.302896  7290 net.cpp:98] Creating Layer res2a_branch2a/bn
I0711 18:09:31.302898  7290 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0711 18:09:31.302902  7290 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0711 18:09:31.303670  7290 net.cpp:148] Setting up res2a_branch2a/bn
I0711 18:09:31.303678  7290 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0711 18:09:31.303680  7290 net.cpp:163] Memory required for data: 532480000
I0711 18:09:31.303688  7290 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0711 18:09:31.303692  7290 net.cpp:98] Creating Layer res2a_branch2a/relu
I0711 18:09:31.303696  7290 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0711 18:09:31.303701  7290 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0711 18:09:31.303707  7290 net.cpp:148] Setting up res2a_branch2a/relu
I0711 18:09:31.303712  7290 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0711 18:09:31.303715  7290 net.cpp:163] Memory required for data: 565248000
I0711 18:09:31.303719  7290 layer_factory.hpp:77] Creating layer res2a_branch2b
I0711 18:09:31.303725  7290 net.cpp:98] Creating Layer res2a_branch2b
I0711 18:09:31.303730  7290 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0711 18:09:31.303735  7290 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0711 18:09:31.305222  7290 net.cpp:148] Setting up res2a_branch2b
I0711 18:09:31.305232  7290 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0711 18:09:31.305234  7290 net.cpp:163] Memory required for data: 598016000
I0711 18:09:31.305240  7290 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0711 18:09:31.305248  7290 net.cpp:98] Creating Layer res2a_branch2b/bn
I0711 18:09:31.305253  7290 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0711 18:09:31.305258  7290 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0711 18:09:31.306046  7290 net.cpp:148] Setting up res2a_branch2b/bn
I0711 18:09:31.306053  7290 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0711 18:09:31.306056  7290 net.cpp:163] Memory required for data: 630784000
I0711 18:09:31.306063  7290 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0711 18:09:31.306069  7290 net.cpp:98] Creating Layer res2a_branch2b/relu
I0711 18:09:31.306073  7290 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0711 18:09:31.306078  7290 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0711 18:09:31.306084  7290 net.cpp:148] Setting up res2a_branch2b/relu
I0711 18:09:31.306089  7290 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0711 18:09:31.306093  7290 net.cpp:163] Memory required for data: 663552000
I0711 18:09:31.306097  7290 layer_factory.hpp:77] Creating layer pool2
I0711 18:09:31.306102  7290 net.cpp:98] Creating Layer pool2
I0711 18:09:31.306105  7290 net.cpp:439] pool2 <- res2a_branch2b/bn
I0711 18:09:31.306109  7290 net.cpp:413] pool2 -> pool2
I0711 18:09:31.306165  7290 net.cpp:148] Setting up pool2
I0711 18:09:31.306172  7290 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 18:09:31.306176  7290 net.cpp:163] Memory required for data: 671744000
I0711 18:09:31.306180  7290 layer_factory.hpp:77] Creating layer res3a_branch2a
I0711 18:09:31.306187  7290 net.cpp:98] Creating Layer res3a_branch2a
I0711 18:09:31.306191  7290 net.cpp:439] res3a_branch2a <- pool2
I0711 18:09:31.306197  7290 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0711 18:09:31.308032  7290 net.cpp:148] Setting up res3a_branch2a
I0711 18:09:31.308039  7290 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0711 18:09:31.308042  7290 net.cpp:163] Memory required for data: 688128000
I0711 18:09:31.308046  7290 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0711 18:09:31.308053  7290 net.cpp:98] Creating Layer res3a_branch2a/bn
I0711 18:09:31.308056  7290 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0711 18:09:31.308061  7290 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0711 18:09:31.308746  7290 net.cpp:148] Setting up res3a_branch2a/bn
I0711 18:09:31.308753  7290 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0711 18:09:31.308756  7290 net.cpp:163] Memory required for data: 704512000
I0711 18:09:31.308765  7290 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0711 18:09:31.308771  7290 net.cpp:98] Creating Layer res3a_branch2a/relu
I0711 18:09:31.308775  7290 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0711 18:09:31.308779  7290 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0711 18:09:31.308785  7290 net.cpp:148] Setting up res3a_branch2a/relu
I0711 18:09:31.308790  7290 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0711 18:09:31.308794  7290 net.cpp:163] Memory required for data: 720896000
I0711 18:09:31.308799  7290 layer_factory.hpp:77] Creating layer res3a_branch2b
I0711 18:09:31.308805  7290 net.cpp:98] Creating Layer res3a_branch2b
I0711 18:09:31.308809  7290 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0711 18:09:31.308817  7290 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0711 18:09:31.309854  7290 net.cpp:148] Setting up res3a_branch2b
I0711 18:09:31.309860  7290 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0711 18:09:31.309864  7290 net.cpp:163] Memory required for data: 737280000
I0711 18:09:31.309867  7290 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0711 18:09:31.309875  7290 net.cpp:98] Creating Layer res3a_branch2b/bn
I0711 18:09:31.309880  7290 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0711 18:09:31.309885  7290 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0711 18:09:31.310531  7290 net.cpp:148] Setting up res3a_branch2b/bn
I0711 18:09:31.310539  7290 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0711 18:09:31.310541  7290 net.cpp:163] Memory required for data: 753664000
I0711 18:09:31.310549  7290 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0711 18:09:31.310554  7290 net.cpp:98] Creating Layer res3a_branch2b/relu
I0711 18:09:31.310557  7290 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0711 18:09:31.310571  7290 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0711 18:09:31.310578  7290 net.cpp:148] Setting up res3a_branch2b/relu
I0711 18:09:31.310582  7290 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0711 18:09:31.310586  7290 net.cpp:163] Memory required for data: 770048000
I0711 18:09:31.310590  7290 layer_factory.hpp:77] Creating layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0711 18:09:31.310595  7290 net.cpp:98] Creating Layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0711 18:09:31.310600  7290 net.cpp:439] res3a_branch2b/bn_res3a_branch2b/relu_0_split <- res3a_branch2b/bn
I0711 18:09:31.310605  7290 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0711 18:09:31.310609  7290 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0711 18:09:31.310659  7290 net.cpp:148] Setting up res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0711 18:09:31.310667  7290 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0711 18:09:31.310670  7290 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0711 18:09:31.310674  7290 net.cpp:163] Memory required for data: 802816000
I0711 18:09:31.310678  7290 layer_factory.hpp:77] Creating layer pool3
I0711 18:09:31.310683  7290 net.cpp:98] Creating Layer pool3
I0711 18:09:31.310688  7290 net.cpp:439] pool3 <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0711 18:09:31.310691  7290 net.cpp:413] pool3 -> pool3
I0711 18:09:31.310744  7290 net.cpp:148] Setting up pool3
I0711 18:09:31.310750  7290 net.cpp:155] Top shape: 5 128 40 40 (1024000)
I0711 18:09:31.310753  7290 net.cpp:163] Memory required for data: 806912000
I0711 18:09:31.310756  7290 layer_factory.hpp:77] Creating layer res4a_branch2a
I0711 18:09:31.310763  7290 net.cpp:98] Creating Layer res4a_branch2a
I0711 18:09:31.310767  7290 net.cpp:439] res4a_branch2a <- pool3
I0711 18:09:31.310772  7290 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0711 18:09:31.317945  7290 net.cpp:148] Setting up res4a_branch2a
I0711 18:09:31.317966  7290 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0711 18:09:31.317970  7290 net.cpp:163] Memory required for data: 815104000
I0711 18:09:31.317975  7290 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0711 18:09:31.317986  7290 net.cpp:98] Creating Layer res4a_branch2a/bn
I0711 18:09:31.317992  7290 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0711 18:09:31.317998  7290 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0711 18:09:31.318740  7290 net.cpp:148] Setting up res4a_branch2a/bn
I0711 18:09:31.318748  7290 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0711 18:09:31.318750  7290 net.cpp:163] Memory required for data: 823296000
I0711 18:09:31.318756  7290 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0711 18:09:31.318763  7290 net.cpp:98] Creating Layer res4a_branch2a/relu
I0711 18:09:31.318765  7290 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0711 18:09:31.318769  7290 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0711 18:09:31.318776  7290 net.cpp:148] Setting up res4a_branch2a/relu
I0711 18:09:31.318781  7290 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0711 18:09:31.318784  7290 net.cpp:163] Memory required for data: 831488000
I0711 18:09:31.318789  7290 layer_factory.hpp:77] Creating layer res4a_branch2b
I0711 18:09:31.318797  7290 net.cpp:98] Creating Layer res4a_branch2b
I0711 18:09:31.318801  7290 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0711 18:09:31.318806  7290 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0711 18:09:31.322146  7290 net.cpp:148] Setting up res4a_branch2b
I0711 18:09:31.322154  7290 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0711 18:09:31.322156  7290 net.cpp:163] Memory required for data: 839680000
I0711 18:09:31.322160  7290 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0711 18:09:31.322165  7290 net.cpp:98] Creating Layer res4a_branch2b/bn
I0711 18:09:31.322168  7290 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0711 18:09:31.322186  7290 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0711 18:09:31.322883  7290 net.cpp:148] Setting up res4a_branch2b/bn
I0711 18:09:31.322890  7290 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0711 18:09:31.322892  7290 net.cpp:163] Memory required for data: 847872000
I0711 18:09:31.322898  7290 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0711 18:09:31.322903  7290 net.cpp:98] Creating Layer res4a_branch2b/relu
I0711 18:09:31.322907  7290 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0711 18:09:31.322912  7290 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0711 18:09:31.322918  7290 net.cpp:148] Setting up res4a_branch2b/relu
I0711 18:09:31.322923  7290 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0711 18:09:31.322926  7290 net.cpp:163] Memory required for data: 856064000
I0711 18:09:31.322931  7290 layer_factory.hpp:77] Creating layer pool4
I0711 18:09:31.322937  7290 net.cpp:98] Creating Layer pool4
I0711 18:09:31.322940  7290 net.cpp:439] pool4 <- res4a_branch2b/bn
I0711 18:09:31.322944  7290 net.cpp:413] pool4 -> pool4
I0711 18:09:31.322995  7290 net.cpp:148] Setting up pool4
I0711 18:09:31.323001  7290 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0711 18:09:31.323005  7290 net.cpp:163] Memory required for data: 864256000
I0711 18:09:31.323009  7290 layer_factory.hpp:77] Creating layer res5a_branch2a
I0711 18:09:31.323016  7290 net.cpp:98] Creating Layer res5a_branch2a
I0711 18:09:31.323021  7290 net.cpp:439] res5a_branch2a <- pool4
I0711 18:09:31.323026  7290 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0711 18:09:31.349087  7290 net.cpp:148] Setting up res5a_branch2a
I0711 18:09:31.349112  7290 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0711 18:09:31.349114  7290 net.cpp:163] Memory required for data: 880640000
I0711 18:09:31.349122  7290 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0711 18:09:31.349135  7290 net.cpp:98] Creating Layer res5a_branch2a/bn
I0711 18:09:31.349151  7290 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0711 18:09:31.349162  7290 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0711 18:09:31.349946  7290 net.cpp:148] Setting up res5a_branch2a/bn
I0711 18:09:31.349956  7290 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0711 18:09:31.349958  7290 net.cpp:163] Memory required for data: 897024000
I0711 18:09:31.349964  7290 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0711 18:09:31.349970  7290 net.cpp:98] Creating Layer res5a_branch2a/relu
I0711 18:09:31.349975  7290 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0711 18:09:31.349979  7290 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0711 18:09:31.349985  7290 net.cpp:148] Setting up res5a_branch2a/relu
I0711 18:09:31.349990  7290 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0711 18:09:31.349994  7290 net.cpp:163] Memory required for data: 913408000
I0711 18:09:31.349997  7290 layer_factory.hpp:77] Creating layer res5a_branch2b
I0711 18:09:31.350008  7290 net.cpp:98] Creating Layer res5a_branch2b
I0711 18:09:31.350011  7290 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0711 18:09:31.350016  7290 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0711 18:09:31.363626  7290 net.cpp:148] Setting up res5a_branch2b
I0711 18:09:31.363646  7290 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0711 18:09:31.363649  7290 net.cpp:163] Memory required for data: 929792000
I0711 18:09:31.363661  7290 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0711 18:09:31.363672  7290 net.cpp:98] Creating Layer res5a_branch2b/bn
I0711 18:09:31.363677  7290 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0711 18:09:31.363682  7290 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0711 18:09:31.364440  7290 net.cpp:148] Setting up res5a_branch2b/bn
I0711 18:09:31.364449  7290 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0711 18:09:31.364451  7290 net.cpp:163] Memory required for data: 946176000
I0711 18:09:31.364456  7290 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0711 18:09:31.364470  7290 net.cpp:98] Creating Layer res5a_branch2b/relu
I0711 18:09:31.364472  7290 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0711 18:09:31.364475  7290 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0711 18:09:31.364480  7290 net.cpp:148] Setting up res5a_branch2b/relu
I0711 18:09:31.364482  7290 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0711 18:09:31.364485  7290 net.cpp:163] Memory required for data: 962560000
I0711 18:09:31.364487  7290 layer_factory.hpp:77] Creating layer out5a
I0711 18:09:31.364496  7290 net.cpp:98] Creating Layer out5a
I0711 18:09:31.364500  7290 net.cpp:439] out5a <- res5a_branch2b/bn
I0711 18:09:31.364506  7290 net.cpp:413] out5a -> out5a
I0711 18:09:31.368798  7290 net.cpp:148] Setting up out5a
I0711 18:09:31.368808  7290 net.cpp:155] Top shape: 5 64 40 40 (512000)
I0711 18:09:31.368811  7290 net.cpp:163] Memory required for data: 964608000
I0711 18:09:31.368821  7290 layer_factory.hpp:77] Creating layer out5a/bn
I0711 18:09:31.368830  7290 net.cpp:98] Creating Layer out5a/bn
I0711 18:09:31.368835  7290 net.cpp:439] out5a/bn <- out5a
I0711 18:09:31.368841  7290 net.cpp:413] out5a/bn -> out5a/bn
I0711 18:09:31.369621  7290 net.cpp:148] Setting up out5a/bn
I0711 18:09:31.369629  7290 net.cpp:155] Top shape: 5 64 40 40 (512000)
I0711 18:09:31.369632  7290 net.cpp:163] Memory required for data: 966656000
I0711 18:09:31.369637  7290 layer_factory.hpp:77] Creating layer out5a/relu
I0711 18:09:31.369640  7290 net.cpp:98] Creating Layer out5a/relu
I0711 18:09:31.369642  7290 net.cpp:439] out5a/relu <- out5a/bn
I0711 18:09:31.369647  7290 net.cpp:400] out5a/relu -> out5a/bn (in-place)
I0711 18:09:31.369650  7290 net.cpp:148] Setting up out5a/relu
I0711 18:09:31.369653  7290 net.cpp:155] Top shape: 5 64 40 40 (512000)
I0711 18:09:31.369657  7290 net.cpp:163] Memory required for data: 968704000
I0711 18:09:31.369658  7290 layer_factory.hpp:77] Creating layer out5a_up2
I0711 18:09:31.369668  7290 net.cpp:98] Creating Layer out5a_up2
I0711 18:09:31.369673  7290 net.cpp:439] out5a_up2 <- out5a/bn
I0711 18:09:31.369680  7290 net.cpp:413] out5a_up2 -> out5a_up2
I0711 18:09:31.370033  7290 net.cpp:148] Setting up out5a_up2
I0711 18:09:31.370039  7290 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 18:09:31.370041  7290 net.cpp:163] Memory required for data: 976896000
I0711 18:09:31.370045  7290 layer_factory.hpp:77] Creating layer out3a
I0711 18:09:31.370050  7290 net.cpp:98] Creating Layer out3a
I0711 18:09:31.370054  7290 net.cpp:439] out3a <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0711 18:09:31.370056  7290 net.cpp:413] out3a -> out3a
I0711 18:09:31.371141  7290 net.cpp:148] Setting up out3a
I0711 18:09:31.371150  7290 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 18:09:31.371151  7290 net.cpp:163] Memory required for data: 985088000
I0711 18:09:31.371155  7290 layer_factory.hpp:77] Creating layer out3a/bn
I0711 18:09:31.371175  7290 net.cpp:98] Creating Layer out3a/bn
I0711 18:09:31.371181  7290 net.cpp:439] out3a/bn <- out3a
I0711 18:09:31.371187  7290 net.cpp:413] out3a/bn -> out3a/bn
I0711 18:09:31.371954  7290 net.cpp:148] Setting up out3a/bn
I0711 18:09:31.371961  7290 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 18:09:31.371964  7290 net.cpp:163] Memory required for data: 993280000
I0711 18:09:31.371971  7290 layer_factory.hpp:77] Creating layer out3a/relu
I0711 18:09:31.371976  7290 net.cpp:98] Creating Layer out3a/relu
I0711 18:09:31.371980  7290 net.cpp:439] out3a/relu <- out3a/bn
I0711 18:09:31.371985  7290 net.cpp:400] out3a/relu -> out3a/bn (in-place)
I0711 18:09:31.371991  7290 net.cpp:148] Setting up out3a/relu
I0711 18:09:31.371996  7290 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 18:09:31.372000  7290 net.cpp:163] Memory required for data: 1001472000
I0711 18:09:31.372004  7290 layer_factory.hpp:77] Creating layer out3_out5_combined
I0711 18:09:31.372014  7290 net.cpp:98] Creating Layer out3_out5_combined
I0711 18:09:31.372017  7290 net.cpp:439] out3_out5_combined <- out5a_up2
I0711 18:09:31.372021  7290 net.cpp:439] out3_out5_combined <- out3a/bn
I0711 18:09:31.372035  7290 net.cpp:413] out3_out5_combined -> out3_out5_combined
I0711 18:09:31.372067  7290 net.cpp:148] Setting up out3_out5_combined
I0711 18:09:31.372074  7290 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 18:09:31.372077  7290 net.cpp:163] Memory required for data: 1009664000
I0711 18:09:31.372081  7290 layer_factory.hpp:77] Creating layer ctx_conv1
I0711 18:09:31.372087  7290 net.cpp:98] Creating Layer ctx_conv1
I0711 18:09:31.372092  7290 net.cpp:439] ctx_conv1 <- out3_out5_combined
I0711 18:09:31.372098  7290 net.cpp:413] ctx_conv1 -> ctx_conv1
I0711 18:09:31.373149  7290 net.cpp:148] Setting up ctx_conv1
I0711 18:09:31.373157  7290 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 18:09:31.373158  7290 net.cpp:163] Memory required for data: 1017856000
I0711 18:09:31.373162  7290 layer_factory.hpp:77] Creating layer ctx_conv1/bn
I0711 18:09:31.373167  7290 net.cpp:98] Creating Layer ctx_conv1/bn
I0711 18:09:31.373170  7290 net.cpp:439] ctx_conv1/bn <- ctx_conv1
I0711 18:09:31.373176  7290 net.cpp:413] ctx_conv1/bn -> ctx_conv1/bn
I0711 18:09:31.373924  7290 net.cpp:148] Setting up ctx_conv1/bn
I0711 18:09:31.373932  7290 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 18:09:31.373935  7290 net.cpp:163] Memory required for data: 1026048000
I0711 18:09:31.373944  7290 layer_factory.hpp:77] Creating layer ctx_conv1/relu
I0711 18:09:31.373958  7290 net.cpp:98] Creating Layer ctx_conv1/relu
I0711 18:09:31.373965  7290 net.cpp:439] ctx_conv1/relu <- ctx_conv1/bn
I0711 18:09:31.373968  7290 net.cpp:400] ctx_conv1/relu -> ctx_conv1/bn (in-place)
I0711 18:09:31.373977  7290 net.cpp:148] Setting up ctx_conv1/relu
I0711 18:09:31.373982  7290 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 18:09:31.373987  7290 net.cpp:163] Memory required for data: 1034240000
I0711 18:09:31.373991  7290 layer_factory.hpp:77] Creating layer ctx_conv2
I0711 18:09:31.374001  7290 net.cpp:98] Creating Layer ctx_conv2
I0711 18:09:31.374004  7290 net.cpp:439] ctx_conv2 <- ctx_conv1/bn
I0711 18:09:31.374008  7290 net.cpp:413] ctx_conv2 -> ctx_conv2
I0711 18:09:31.375102  7290 net.cpp:148] Setting up ctx_conv2
I0711 18:09:31.375110  7290 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 18:09:31.375113  7290 net.cpp:163] Memory required for data: 1042432000
I0711 18:09:31.375118  7290 layer_factory.hpp:77] Creating layer ctx_conv2/bn
I0711 18:09:31.375133  7290 net.cpp:98] Creating Layer ctx_conv2/bn
I0711 18:09:31.375139  7290 net.cpp:439] ctx_conv2/bn <- ctx_conv2
I0711 18:09:31.375144  7290 net.cpp:413] ctx_conv2/bn -> ctx_conv2/bn
I0711 18:09:31.375896  7290 net.cpp:148] Setting up ctx_conv2/bn
I0711 18:09:31.375902  7290 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 18:09:31.375905  7290 net.cpp:163] Memory required for data: 1050624000
I0711 18:09:31.375912  7290 layer_factory.hpp:77] Creating layer ctx_conv2/relu
I0711 18:09:31.375926  7290 net.cpp:98] Creating Layer ctx_conv2/relu
I0711 18:09:31.375929  7290 net.cpp:439] ctx_conv2/relu <- ctx_conv2/bn
I0711 18:09:31.375932  7290 net.cpp:400] ctx_conv2/relu -> ctx_conv2/bn (in-place)
I0711 18:09:31.375941  7290 net.cpp:148] Setting up ctx_conv2/relu
I0711 18:09:31.375949  7290 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 18:09:31.375954  7290 net.cpp:163] Memory required for data: 1058816000
I0711 18:09:31.375957  7290 layer_factory.hpp:77] Creating layer ctx_conv3
I0711 18:09:31.375964  7290 net.cpp:98] Creating Layer ctx_conv3
I0711 18:09:31.375969  7290 net.cpp:439] ctx_conv3 <- ctx_conv2/bn
I0711 18:09:31.375974  7290 net.cpp:413] ctx_conv3 -> ctx_conv3
I0711 18:09:31.377029  7290 net.cpp:148] Setting up ctx_conv3
I0711 18:09:31.377037  7290 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 18:09:31.377038  7290 net.cpp:163] Memory required for data: 1067008000
I0711 18:09:31.377043  7290 layer_factory.hpp:77] Creating layer ctx_conv3/bn
I0711 18:09:31.377050  7290 net.cpp:98] Creating Layer ctx_conv3/bn
I0711 18:09:31.377054  7290 net.cpp:439] ctx_conv3/bn <- ctx_conv3
I0711 18:09:31.377060  7290 net.cpp:413] ctx_conv3/bn -> ctx_conv3/bn
I0711 18:09:31.377812  7290 net.cpp:148] Setting up ctx_conv3/bn
I0711 18:09:31.377820  7290 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 18:09:31.377822  7290 net.cpp:163] Memory required for data: 1075200000
I0711 18:09:31.377830  7290 layer_factory.hpp:77] Creating layer ctx_conv3/relu
I0711 18:09:31.377836  7290 net.cpp:98] Creating Layer ctx_conv3/relu
I0711 18:09:31.377840  7290 net.cpp:439] ctx_conv3/relu <- ctx_conv3/bn
I0711 18:09:31.377846  7290 net.cpp:400] ctx_conv3/relu -> ctx_conv3/bn (in-place)
I0711 18:09:31.377853  7290 net.cpp:148] Setting up ctx_conv3/relu
I0711 18:09:31.377858  7290 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 18:09:31.377861  7290 net.cpp:163] Memory required for data: 1083392000
I0711 18:09:31.377866  7290 layer_factory.hpp:77] Creating layer ctx_conv4
I0711 18:09:31.377873  7290 net.cpp:98] Creating Layer ctx_conv4
I0711 18:09:31.377877  7290 net.cpp:439] ctx_conv4 <- ctx_conv3/bn
I0711 18:09:31.377882  7290 net.cpp:413] ctx_conv4 -> ctx_conv4
I0711 18:09:31.378926  7290 net.cpp:148] Setting up ctx_conv4
I0711 18:09:31.378931  7290 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 18:09:31.378933  7290 net.cpp:163] Memory required for data: 1091584000
I0711 18:09:31.378938  7290 layer_factory.hpp:77] Creating layer ctx_conv4/bn
I0711 18:09:31.378944  7290 net.cpp:98] Creating Layer ctx_conv4/bn
I0711 18:09:31.378948  7290 net.cpp:439] ctx_conv4/bn <- ctx_conv4
I0711 18:09:31.378954  7290 net.cpp:413] ctx_conv4/bn -> ctx_conv4/bn
I0711 18:09:31.379688  7290 net.cpp:148] Setting up ctx_conv4/bn
I0711 18:09:31.379694  7290 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 18:09:31.379698  7290 net.cpp:163] Memory required for data: 1099776000
I0711 18:09:31.379704  7290 layer_factory.hpp:77] Creating layer ctx_conv4/relu
I0711 18:09:31.379710  7290 net.cpp:98] Creating Layer ctx_conv4/relu
I0711 18:09:31.379714  7290 net.cpp:439] ctx_conv4/relu <- ctx_conv4/bn
I0711 18:09:31.379719  7290 net.cpp:400] ctx_conv4/relu -> ctx_conv4/bn (in-place)
I0711 18:09:31.379726  7290 net.cpp:148] Setting up ctx_conv4/relu
I0711 18:09:31.379731  7290 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 18:09:31.379735  7290 net.cpp:163] Memory required for data: 1107968000
I0711 18:09:31.379739  7290 layer_factory.hpp:77] Creating layer ctx_final
I0711 18:09:31.379745  7290 net.cpp:98] Creating Layer ctx_final
I0711 18:09:31.379750  7290 net.cpp:439] ctx_final <- ctx_conv4/bn
I0711 18:09:31.379753  7290 net.cpp:413] ctx_final -> ctx_final
I0711 18:09:31.380157  7290 net.cpp:148] Setting up ctx_final
I0711 18:09:31.380163  7290 net.cpp:155] Top shape: 5 8 80 80 (256000)
I0711 18:09:31.380165  7290 net.cpp:163] Memory required for data: 1108992000
I0711 18:09:31.380170  7290 layer_factory.hpp:77] Creating layer ctx_final/relu
I0711 18:09:31.380177  7290 net.cpp:98] Creating Layer ctx_final/relu
I0711 18:09:31.380179  7290 net.cpp:439] ctx_final/relu <- ctx_final
I0711 18:09:31.380182  7290 net.cpp:400] ctx_final/relu -> ctx_final (in-place)
I0711 18:09:31.380185  7290 net.cpp:148] Setting up ctx_final/relu
I0711 18:09:31.380187  7290 net.cpp:155] Top shape: 5 8 80 80 (256000)
I0711 18:09:31.380189  7290 net.cpp:163] Memory required for data: 1110016000
I0711 18:09:31.380192  7290 layer_factory.hpp:77] Creating layer out_deconv_final_up2
I0711 18:09:31.380197  7290 net.cpp:98] Creating Layer out_deconv_final_up2
I0711 18:09:31.380198  7290 net.cpp:439] out_deconv_final_up2 <- ctx_final
I0711 18:09:31.380201  7290 net.cpp:413] out_deconv_final_up2 -> out_deconv_final_up2
I0711 18:09:31.380440  7290 net.cpp:148] Setting up out_deconv_final_up2
I0711 18:09:31.380446  7290 net.cpp:155] Top shape: 5 8 160 160 (1024000)
I0711 18:09:31.380448  7290 net.cpp:163] Memory required for data: 1114112000
I0711 18:09:31.380453  7290 layer_factory.hpp:77] Creating layer out_deconv_final_up4
I0711 18:09:31.380466  7290 net.cpp:98] Creating Layer out_deconv_final_up4
I0711 18:09:31.380470  7290 net.cpp:439] out_deconv_final_up4 <- out_deconv_final_up2
I0711 18:09:31.380475  7290 net.cpp:413] out_deconv_final_up4 -> out_deconv_final_up4
I0711 18:09:31.380718  7290 net.cpp:148] Setting up out_deconv_final_up4
I0711 18:09:31.380724  7290 net.cpp:155] Top shape: 5 8 320 320 (4096000)
I0711 18:09:31.380728  7290 net.cpp:163] Memory required for data: 1130496000
I0711 18:09:31.380733  7290 layer_factory.hpp:77] Creating layer out_deconv_final_up8
I0711 18:09:31.380738  7290 net.cpp:98] Creating Layer out_deconv_final_up8
I0711 18:09:31.380743  7290 net.cpp:439] out_deconv_final_up8 <- out_deconv_final_up4
I0711 18:09:31.380746  7290 net.cpp:413] out_deconv_final_up8 -> out_deconv_final_up8
I0711 18:09:31.380987  7290 net.cpp:148] Setting up out_deconv_final_up8
I0711 18:09:31.380995  7290 net.cpp:155] Top shape: 5 8 640 640 (16384000)
I0711 18:09:31.380997  7290 net.cpp:163] Memory required for data: 1196032000
I0711 18:09:31.381002  7290 layer_factory.hpp:77] Creating layer loss
I0711 18:09:31.381007  7290 net.cpp:98] Creating Layer loss
I0711 18:09:31.381011  7290 net.cpp:439] loss <- out_deconv_final_up8
I0711 18:09:31.381014  7290 net.cpp:439] loss <- label
I0711 18:09:31.381021  7290 net.cpp:413] loss -> loss
I0711 18:09:31.381031  7290 layer_factory.hpp:77] Creating layer loss
I0711 18:09:31.408262  7290 net.cpp:148] Setting up loss
I0711 18:09:31.408299  7290 net.cpp:155] Top shape: (1)
I0711 18:09:31.408303  7290 net.cpp:158]     with loss weight 1
I0711 18:09:31.408332  7290 net.cpp:163] Memory required for data: 1196032004
I0711 18:09:31.408354  7290 net.cpp:224] loss needs backward computation.
I0711 18:09:31.408363  7290 net.cpp:224] out_deconv_final_up8 needs backward computation.
I0711 18:09:31.408370  7290 net.cpp:224] out_deconv_final_up4 needs backward computation.
I0711 18:09:31.408378  7290 net.cpp:224] out_deconv_final_up2 needs backward computation.
I0711 18:09:31.408388  7290 net.cpp:224] ctx_final/relu needs backward computation.
I0711 18:09:31.408393  7290 net.cpp:224] ctx_final needs backward computation.
I0711 18:09:31.408401  7290 net.cpp:224] ctx_conv4/relu needs backward computation.
I0711 18:09:31.408406  7290 net.cpp:224] ctx_conv4/bn needs backward computation.
I0711 18:09:31.408418  7290 net.cpp:224] ctx_conv4 needs backward computation.
I0711 18:09:31.408422  7290 net.cpp:224] ctx_conv3/relu needs backward computation.
I0711 18:09:31.408429  7290 net.cpp:224] ctx_conv3/bn needs backward computation.
I0711 18:09:31.408434  7290 net.cpp:224] ctx_conv3 needs backward computation.
I0711 18:09:31.408445  7290 net.cpp:224] ctx_conv2/relu needs backward computation.
I0711 18:09:31.408449  7290 net.cpp:224] ctx_conv2/bn needs backward computation.
I0711 18:09:31.408457  7290 net.cpp:224] ctx_conv2 needs backward computation.
I0711 18:09:31.408462  7290 net.cpp:224] ctx_conv1/relu needs backward computation.
I0711 18:09:31.408471  7290 net.cpp:224] ctx_conv1/bn needs backward computation.
I0711 18:09:31.408475  7290 net.cpp:224] ctx_conv1 needs backward computation.
I0711 18:09:31.408483  7290 net.cpp:224] out3_out5_combined needs backward computation.
I0711 18:09:31.408488  7290 net.cpp:224] out3a/relu needs backward computation.
I0711 18:09:31.408499  7290 net.cpp:224] out3a/bn needs backward computation.
I0711 18:09:31.408504  7290 net.cpp:224] out3a needs backward computation.
I0711 18:09:31.408510  7290 net.cpp:224] out5a_up2 needs backward computation.
I0711 18:09:31.408516  7290 net.cpp:224] out5a/relu needs backward computation.
I0711 18:09:31.408526  7290 net.cpp:224] out5a/bn needs backward computation.
I0711 18:09:31.408530  7290 net.cpp:224] out5a needs backward computation.
I0711 18:09:31.408537  7290 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0711 18:09:31.408542  7290 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0711 18:09:31.408555  7290 net.cpp:224] res5a_branch2b needs backward computation.
I0711 18:09:31.408558  7290 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0711 18:09:31.408565  7290 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0711 18:09:31.408579  7290 net.cpp:224] res5a_branch2a needs backward computation.
I0711 18:09:31.408589  7290 net.cpp:224] pool4 needs backward computation.
I0711 18:09:31.408601  7290 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0711 18:09:31.408604  7290 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0711 18:09:31.408612  7290 net.cpp:224] res4a_branch2b needs backward computation.
I0711 18:09:31.408618  7290 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0711 18:09:31.408628  7290 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0711 18:09:31.408632  7290 net.cpp:224] res4a_branch2a needs backward computation.
I0711 18:09:31.408640  7290 net.cpp:224] pool3 needs backward computation.
I0711 18:09:31.408645  7290 net.cpp:224] res3a_branch2b/bn_res3a_branch2b/relu_0_split needs backward computation.
I0711 18:09:31.408656  7290 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0711 18:09:31.408659  7290 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0711 18:09:31.408668  7290 net.cpp:224] res3a_branch2b needs backward computation.
I0711 18:09:31.408671  7290 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0711 18:09:31.408679  7290 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0711 18:09:31.408682  7290 net.cpp:224] res3a_branch2a needs backward computation.
I0711 18:09:31.408690  7290 net.cpp:224] pool2 needs backward computation.
I0711 18:09:31.408694  7290 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0711 18:09:31.408701  7290 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0711 18:09:31.408705  7290 net.cpp:224] res2a_branch2b needs backward computation.
I0711 18:09:31.408713  7290 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0711 18:09:31.408716  7290 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0711 18:09:31.408725  7290 net.cpp:224] res2a_branch2a needs backward computation.
I0711 18:09:31.408728  7290 net.cpp:224] pool1 needs backward computation.
I0711 18:09:31.408735  7290 net.cpp:224] conv1b/relu needs backward computation.
I0711 18:09:31.408740  7290 net.cpp:224] conv1b/bn needs backward computation.
I0711 18:09:31.408747  7290 net.cpp:224] conv1b needs backward computation.
I0711 18:09:31.408751  7290 net.cpp:224] conv1a/relu needs backward computation.
I0711 18:09:31.408758  7290 net.cpp:224] conv1a/bn needs backward computation.
I0711 18:09:31.408762  7290 net.cpp:224] conv1a needs backward computation.
I0711 18:09:31.408771  7290 net.cpp:226] data/bias does not need backward computation.
I0711 18:09:31.408774  7290 net.cpp:226] data does not need backward computation.
I0711 18:09:31.408782  7290 net.cpp:268] This network produces output loss
I0711 18:09:31.408825  7290 net.cpp:288] Network initialization done.
I0711 18:09:31.409572  7290 solver.cpp:182] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/initial/test.prototxt
I0711 18:09:31.409873  7290 net.cpp:56] Initializing net from parameters: 
name: "jsegnet21v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/val-image-lmdb"
    label_list_path: "data/val-label-lmdb"
    batch_size: 4
    threads: 4
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b/bn"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a/bn"
  top: "out5a/bn"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a/bn"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b/bn"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a/bn"
  top: "out3a/bn"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a/bn"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv1/bn"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv2/bn"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv3/bn"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4/bn"
  top: "ctx_conv4/bn"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4/bn"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
  accuracy_param {
    ignore_label: 255
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
    ignore_label: 255
  }
}
I0711 18:09:31.410055  7290 layer_factory.hpp:77] Creating layer data
I0711 18:09:31.410071  7290 net.cpp:98] Creating Layer data
I0711 18:09:31.410081  7290 net.cpp:413] data -> data
I0711 18:09:31.410092  7290 net.cpp:413] data -> label
I0711 18:09:31.411417  7360 db_lmdb.cpp:35] Opened lmdb data/val-image-lmdb
I0711 18:09:31.413859  7365 db_lmdb.cpp:35] Opened lmdb data/val-label-lmdb
I0711 18:09:31.446334  7290 data_layer.cpp:78] ReshapePrefetch 4, 3, 640, 640
I0711 18:09:31.446413  7290 data_layer.cpp:83] output data size: 4,3,640,640
I0711 18:09:31.489640  7290 data_layer.cpp:78] ReshapePrefetch 4, 1, 640, 640
I0711 18:09:31.489715  7290 data_layer.cpp:83] output data size: 4,1,640,640
I0711 18:09:31.804074  7290 net.cpp:148] Setting up data
I0711 18:09:31.804158  7290 net.cpp:155] Top shape: 4 3 640 640 (4915200)
I0711 18:09:31.804168  7290 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0711 18:09:31.804173  7290 net.cpp:163] Memory required for data: 26214400
I0711 18:09:31.804183  7290 layer_factory.hpp:77] Creating layer label_data_1_split
I0711 18:09:31.804210  7290 net.cpp:98] Creating Layer label_data_1_split
I0711 18:09:31.804222  7290 net.cpp:439] label_data_1_split <- label
I0711 18:09:31.804285  7290 net.cpp:413] label_data_1_split -> label_data_1_split_0
I0711 18:09:31.804302  7290 net.cpp:413] label_data_1_split -> label_data_1_split_1
I0711 18:09:31.804311  7290 net.cpp:413] label_data_1_split -> label_data_1_split_2
I0711 18:09:31.804744  7290 net.cpp:148] Setting up label_data_1_split
I0711 18:09:31.804759  7290 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0711 18:09:31.804766  7290 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0711 18:09:31.804774  7290 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0711 18:09:31.804778  7290 net.cpp:163] Memory required for data: 45875200
I0711 18:09:31.804783  7290 layer_factory.hpp:77] Creating layer data/bias
I0711 18:09:31.804813  7290 net.cpp:98] Creating Layer data/bias
I0711 18:09:31.804837  7290 net.cpp:439] data/bias <- data
I0711 18:09:31.804847  7290 net.cpp:413] data/bias -> data/bias
I0711 18:09:31.807446  7290 net.cpp:148] Setting up data/bias
I0711 18:09:31.807462  7290 net.cpp:155] Top shape: 4 3 640 640 (4915200)
I0711 18:09:31.807464  7290 net.cpp:163] Memory required for data: 65536000
I0711 18:09:31.807472  7290 layer_factory.hpp:77] Creating layer conv1a
I0711 18:09:31.807483  7290 net.cpp:98] Creating Layer conv1a
I0711 18:09:31.807487  7290 net.cpp:439] conv1a <- data/bias
I0711 18:09:31.807490  7290 net.cpp:413] conv1a -> conv1a
I0711 18:09:31.808028  7290 net.cpp:148] Setting up conv1a
I0711 18:09:31.808034  7290 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0711 18:09:31.808037  7290 net.cpp:163] Memory required for data: 117964800
I0711 18:09:31.808042  7290 layer_factory.hpp:77] Creating layer conv1a/bn
I0711 18:09:31.808048  7290 net.cpp:98] Creating Layer conv1a/bn
I0711 18:09:31.808050  7290 net.cpp:439] conv1a/bn <- conv1a
I0711 18:09:31.808053  7290 net.cpp:413] conv1a/bn -> conv1a/bn
I0711 18:09:31.809293  7290 net.cpp:148] Setting up conv1a/bn
I0711 18:09:31.809310  7290 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0711 18:09:31.809319  7290 net.cpp:163] Memory required for data: 170393600
I0711 18:09:31.809334  7290 layer_factory.hpp:77] Creating layer conv1a/relu
I0711 18:09:31.809342  7290 net.cpp:98] Creating Layer conv1a/relu
I0711 18:09:31.809346  7290 net.cpp:439] conv1a/relu <- conv1a/bn
I0711 18:09:31.809350  7290 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0711 18:09:31.809357  7290 net.cpp:148] Setting up conv1a/relu
I0711 18:09:31.809362  7290 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0711 18:09:31.809365  7290 net.cpp:163] Memory required for data: 222822400
I0711 18:09:31.809368  7290 layer_factory.hpp:77] Creating layer conv1b
I0711 18:09:31.809377  7290 net.cpp:98] Creating Layer conv1b
I0711 18:09:31.809381  7290 net.cpp:439] conv1b <- conv1a/bn
I0711 18:09:31.809386  7290 net.cpp:413] conv1b -> conv1b
I0711 18:09:31.810032  7290 net.cpp:148] Setting up conv1b
I0711 18:09:31.810044  7290 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0711 18:09:31.810047  7290 net.cpp:163] Memory required for data: 275251200
I0711 18:09:31.810055  7290 layer_factory.hpp:77] Creating layer conv1b/bn
I0711 18:09:31.810060  7290 net.cpp:98] Creating Layer conv1b/bn
I0711 18:09:31.810065  7290 net.cpp:439] conv1b/bn <- conv1b
I0711 18:09:31.810070  7290 net.cpp:413] conv1b/bn -> conv1b/bn
I0711 18:09:31.811287  7290 net.cpp:148] Setting up conv1b/bn
I0711 18:09:31.811298  7290 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0711 18:09:31.811305  7290 net.cpp:163] Memory required for data: 327680000
I0711 18:09:31.811312  7290 layer_factory.hpp:77] Creating layer conv1b/relu
I0711 18:09:31.811318  7290 net.cpp:98] Creating Layer conv1b/relu
I0711 18:09:31.811322  7290 net.cpp:439] conv1b/relu <- conv1b/bn
I0711 18:09:31.811327  7290 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0711 18:09:31.811333  7290 net.cpp:148] Setting up conv1b/relu
I0711 18:09:31.811338  7290 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0711 18:09:31.811342  7290 net.cpp:163] Memory required for data: 380108800
I0711 18:09:31.811345  7290 layer_factory.hpp:77] Creating layer pool1
I0711 18:09:31.811352  7290 net.cpp:98] Creating Layer pool1
I0711 18:09:31.811370  7290 net.cpp:439] pool1 <- conv1b/bn
I0711 18:09:31.811377  7290 net.cpp:413] pool1 -> pool1
I0711 18:09:31.811439  7290 net.cpp:148] Setting up pool1
I0711 18:09:31.811445  7290 net.cpp:155] Top shape: 4 32 160 160 (3276800)
I0711 18:09:31.811450  7290 net.cpp:163] Memory required for data: 393216000
I0711 18:09:31.811453  7290 layer_factory.hpp:77] Creating layer res2a_branch2a
I0711 18:09:31.811461  7290 net.cpp:98] Creating Layer res2a_branch2a
I0711 18:09:31.811466  7290 net.cpp:439] res2a_branch2a <- pool1
I0711 18:09:31.811471  7290 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0711 18:09:31.812367  7290 net.cpp:148] Setting up res2a_branch2a
I0711 18:09:31.812376  7290 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0711 18:09:31.812378  7290 net.cpp:163] Memory required for data: 419430400
I0711 18:09:31.812388  7290 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0711 18:09:31.812396  7290 net.cpp:98] Creating Layer res2a_branch2a/bn
I0711 18:09:31.812400  7290 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0711 18:09:31.812407  7290 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0711 18:09:31.814070  7290 net.cpp:148] Setting up res2a_branch2a/bn
I0711 18:09:31.814081  7290 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0711 18:09:31.814083  7290 net.cpp:163] Memory required for data: 445644800
I0711 18:09:31.814090  7290 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0711 18:09:31.814095  7290 net.cpp:98] Creating Layer res2a_branch2a/relu
I0711 18:09:31.814098  7290 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0711 18:09:31.814101  7290 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0711 18:09:31.814105  7290 net.cpp:148] Setting up res2a_branch2a/relu
I0711 18:09:31.814110  7290 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0711 18:09:31.814110  7290 net.cpp:163] Memory required for data: 471859200
I0711 18:09:31.814112  7290 layer_factory.hpp:77] Creating layer res2a_branch2b
I0711 18:09:31.814118  7290 net.cpp:98] Creating Layer res2a_branch2b
I0711 18:09:31.814121  7290 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0711 18:09:31.814123  7290 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0711 18:09:31.816365  7290 net.cpp:148] Setting up res2a_branch2b
I0711 18:09:31.816382  7290 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0711 18:09:31.816386  7290 net.cpp:163] Memory required for data: 498073600
I0711 18:09:31.816396  7290 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0711 18:09:31.816411  7290 net.cpp:98] Creating Layer res2a_branch2b/bn
I0711 18:09:31.816416  7290 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0711 18:09:31.816426  7290 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0711 18:09:31.841977  7290 net.cpp:148] Setting up res2a_branch2b/bn
I0711 18:09:31.842005  7290 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0711 18:09:31.842008  7290 net.cpp:163] Memory required for data: 524288000
I0711 18:09:31.842022  7290 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0711 18:09:31.842032  7290 net.cpp:98] Creating Layer res2a_branch2b/relu
I0711 18:09:31.842038  7290 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0711 18:09:31.842044  7290 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0711 18:09:31.842056  7290 net.cpp:148] Setting up res2a_branch2b/relu
I0711 18:09:31.842062  7290 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0711 18:09:31.842066  7290 net.cpp:163] Memory required for data: 550502400
I0711 18:09:31.842068  7290 layer_factory.hpp:77] Creating layer pool2
I0711 18:09:31.842077  7290 net.cpp:98] Creating Layer pool2
I0711 18:09:31.842082  7290 net.cpp:439] pool2 <- res2a_branch2b/bn
I0711 18:09:31.842085  7290 net.cpp:413] pool2 -> pool2
I0711 18:09:31.842160  7290 net.cpp:148] Setting up pool2
I0711 18:09:31.842166  7290 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 18:09:31.842170  7290 net.cpp:163] Memory required for data: 557056000
I0711 18:09:31.842172  7290 layer_factory.hpp:77] Creating layer res3a_branch2a
I0711 18:09:31.842193  7290 net.cpp:98] Creating Layer res3a_branch2a
I0711 18:09:31.842198  7290 net.cpp:439] res3a_branch2a <- pool2
I0711 18:09:31.842202  7290 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0711 18:09:31.845340  7290 net.cpp:148] Setting up res3a_branch2a
I0711 18:09:31.845357  7290 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 18:09:31.845360  7290 net.cpp:163] Memory required for data: 570163200
I0711 18:09:31.845367  7290 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0711 18:09:31.845376  7290 net.cpp:98] Creating Layer res3a_branch2a/bn
I0711 18:09:31.845381  7290 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0711 18:09:31.845386  7290 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0711 18:09:31.846417  7290 net.cpp:148] Setting up res3a_branch2a/bn
I0711 18:09:31.846427  7290 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 18:09:31.846431  7290 net.cpp:163] Memory required for data: 583270400
I0711 18:09:31.846439  7290 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0711 18:09:31.846444  7290 net.cpp:98] Creating Layer res3a_branch2a/relu
I0711 18:09:31.846448  7290 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0711 18:09:31.846452  7290 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0711 18:09:31.846458  7290 net.cpp:148] Setting up res3a_branch2a/relu
I0711 18:09:31.846462  7290 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 18:09:31.846465  7290 net.cpp:163] Memory required for data: 596377600
I0711 18:09:31.846469  7290 layer_factory.hpp:77] Creating layer res3a_branch2b
I0711 18:09:31.846477  7290 net.cpp:98] Creating Layer res3a_branch2b
I0711 18:09:31.846480  7290 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0711 18:09:31.846485  7290 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0711 18:09:31.848008  7290 net.cpp:148] Setting up res3a_branch2b
I0711 18:09:31.848016  7290 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 18:09:31.848019  7290 net.cpp:163] Memory required for data: 609484800
I0711 18:09:31.848023  7290 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0711 18:09:31.848029  7290 net.cpp:98] Creating Layer res3a_branch2b/bn
I0711 18:09:31.848033  7290 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0711 18:09:31.848039  7290 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0711 18:09:31.848960  7290 net.cpp:148] Setting up res3a_branch2b/bn
I0711 18:09:31.848968  7290 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 18:09:31.848970  7290 net.cpp:163] Memory required for data: 622592000
I0711 18:09:31.848978  7290 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0711 18:09:31.848984  7290 net.cpp:98] Creating Layer res3a_branch2b/relu
I0711 18:09:31.848987  7290 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0711 18:09:31.848992  7290 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0711 18:09:31.848999  7290 net.cpp:148] Setting up res3a_branch2b/relu
I0711 18:09:31.849004  7290 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 18:09:31.849005  7290 net.cpp:163] Memory required for data: 635699200
I0711 18:09:31.849009  7290 layer_factory.hpp:77] Creating layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0711 18:09:31.849014  7290 net.cpp:98] Creating Layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0711 18:09:31.849020  7290 net.cpp:439] res3a_branch2b/bn_res3a_branch2b/relu_0_split <- res3a_branch2b/bn
I0711 18:09:31.849023  7290 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0711 18:09:31.849028  7290 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0711 18:09:31.849088  7290 net.cpp:148] Setting up res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0711 18:09:31.849094  7290 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 18:09:31.849097  7290 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 18:09:31.849102  7290 net.cpp:163] Memory required for data: 661913600
I0711 18:09:31.849118  7290 layer_factory.hpp:77] Creating layer pool3
I0711 18:09:31.849128  7290 net.cpp:98] Creating Layer pool3
I0711 18:09:31.849133  7290 net.cpp:439] pool3 <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0711 18:09:31.849139  7290 net.cpp:413] pool3 -> pool3
I0711 18:09:31.849200  7290 net.cpp:148] Setting up pool3
I0711 18:09:31.849206  7290 net.cpp:155] Top shape: 4 128 40 40 (819200)
I0711 18:09:31.849210  7290 net.cpp:163] Memory required for data: 665190400
I0711 18:09:31.849212  7290 layer_factory.hpp:77] Creating layer res4a_branch2a
I0711 18:09:31.849222  7290 net.cpp:98] Creating Layer res4a_branch2a
I0711 18:09:31.849226  7290 net.cpp:439] res4a_branch2a <- pool3
I0711 18:09:31.849231  7290 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0711 18:09:31.886525  7290 net.cpp:148] Setting up res4a_branch2a
I0711 18:09:31.886564  7290 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0711 18:09:31.886567  7290 net.cpp:163] Memory required for data: 671744000
I0711 18:09:31.886579  7290 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0711 18:09:31.886593  7290 net.cpp:98] Creating Layer res4a_branch2a/bn
I0711 18:09:31.886598  7290 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0711 18:09:31.886605  7290 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0711 18:09:31.887536  7290 net.cpp:148] Setting up res4a_branch2a/bn
I0711 18:09:31.887545  7290 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0711 18:09:31.887547  7290 net.cpp:163] Memory required for data: 678297600
I0711 18:09:31.887554  7290 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0711 18:09:31.887560  7290 net.cpp:98] Creating Layer res4a_branch2a/relu
I0711 18:09:31.887563  7290 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0711 18:09:31.887565  7290 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0711 18:09:31.887570  7290 net.cpp:148] Setting up res4a_branch2a/relu
I0711 18:09:31.887573  7290 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0711 18:09:31.887575  7290 net.cpp:163] Memory required for data: 684851200
I0711 18:09:31.887578  7290 layer_factory.hpp:77] Creating layer res4a_branch2b
I0711 18:09:31.887585  7290 net.cpp:98] Creating Layer res4a_branch2b
I0711 18:09:31.887588  7290 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0711 18:09:31.887590  7290 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0711 18:09:31.898526  7290 net.cpp:148] Setting up res4a_branch2b
I0711 18:09:31.898561  7290 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0711 18:09:31.898567  7290 net.cpp:163] Memory required for data: 691404800
I0711 18:09:31.898583  7290 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0711 18:09:31.898602  7290 net.cpp:98] Creating Layer res4a_branch2b/bn
I0711 18:09:31.898610  7290 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0711 18:09:31.898620  7290 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0711 18:09:31.901593  7290 net.cpp:148] Setting up res4a_branch2b/bn
I0711 18:09:31.901643  7290 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0711 18:09:31.901650  7290 net.cpp:163] Memory required for data: 697958400
I0711 18:09:31.901669  7290 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0711 18:09:31.901693  7290 net.cpp:98] Creating Layer res4a_branch2b/relu
I0711 18:09:31.901705  7290 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0711 18:09:31.901715  7290 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0711 18:09:31.901736  7290 net.cpp:148] Setting up res4a_branch2b/relu
I0711 18:09:31.901741  7290 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0711 18:09:31.901744  7290 net.cpp:163] Memory required for data: 704512000
I0711 18:09:31.901748  7290 layer_factory.hpp:77] Creating layer pool4
I0711 18:09:31.901759  7290 net.cpp:98] Creating Layer pool4
I0711 18:09:31.901764  7290 net.cpp:439] pool4 <- res4a_branch2b/bn
I0711 18:09:31.901772  7290 net.cpp:413] pool4 -> pool4
I0711 18:09:31.901862  7290 net.cpp:148] Setting up pool4
I0711 18:09:31.901871  7290 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0711 18:09:31.901875  7290 net.cpp:163] Memory required for data: 711065600
I0711 18:09:31.901901  7290 layer_factory.hpp:77] Creating layer res5a_branch2a
I0711 18:09:31.901916  7290 net.cpp:98] Creating Layer res5a_branch2a
I0711 18:09:31.901923  7290 net.cpp:439] res5a_branch2a <- pool4
I0711 18:09:31.901929  7290 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0711 18:09:31.938400  7290 net.cpp:148] Setting up res5a_branch2a
I0711 18:09:31.938436  7290 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0711 18:09:31.938439  7290 net.cpp:163] Memory required for data: 724172800
I0711 18:09:31.938449  7290 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0711 18:09:31.938462  7290 net.cpp:98] Creating Layer res5a_branch2a/bn
I0711 18:09:31.938468  7290 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0711 18:09:31.938474  7290 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0711 18:09:31.939641  7290 net.cpp:148] Setting up res5a_branch2a/bn
I0711 18:09:31.939651  7290 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0711 18:09:31.939653  7290 net.cpp:163] Memory required for data: 737280000
I0711 18:09:31.939666  7290 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0711 18:09:31.939671  7290 net.cpp:98] Creating Layer res5a_branch2a/relu
I0711 18:09:31.939677  7290 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0711 18:09:31.939680  7290 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0711 18:09:31.939687  7290 net.cpp:148] Setting up res5a_branch2a/relu
I0711 18:09:31.939692  7290 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0711 18:09:31.939695  7290 net.cpp:163] Memory required for data: 750387200
I0711 18:09:31.939699  7290 layer_factory.hpp:77] Creating layer res5a_branch2b
I0711 18:09:31.939707  7290 net.cpp:98] Creating Layer res5a_branch2b
I0711 18:09:31.939710  7290 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0711 18:09:31.939714  7290 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0711 18:09:31.959228  7290 net.cpp:148] Setting up res5a_branch2b
I0711 18:09:31.959251  7290 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0711 18:09:31.959254  7290 net.cpp:163] Memory required for data: 763494400
I0711 18:09:31.959264  7290 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0711 18:09:31.959271  7290 net.cpp:98] Creating Layer res5a_branch2b/bn
I0711 18:09:31.959276  7290 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0711 18:09:31.959280  7290 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0711 18:09:31.960026  7290 net.cpp:148] Setting up res5a_branch2b/bn
I0711 18:09:31.960032  7290 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0711 18:09:31.960034  7290 net.cpp:163] Memory required for data: 776601600
I0711 18:09:31.960039  7290 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0711 18:09:31.960043  7290 net.cpp:98] Creating Layer res5a_branch2b/relu
I0711 18:09:31.960052  7290 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0711 18:09:31.960054  7290 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0711 18:09:31.960058  7290 net.cpp:148] Setting up res5a_branch2b/relu
I0711 18:09:31.960062  7290 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0711 18:09:31.960063  7290 net.cpp:163] Memory required for data: 789708800
I0711 18:09:31.960065  7290 layer_factory.hpp:77] Creating layer out5a
I0711 18:09:31.960072  7290 net.cpp:98] Creating Layer out5a
I0711 18:09:31.960074  7290 net.cpp:439] out5a <- res5a_branch2b/bn
I0711 18:09:31.960078  7290 net.cpp:413] out5a -> out5a
I0711 18:09:31.964403  7290 net.cpp:148] Setting up out5a
I0711 18:09:31.964421  7290 net.cpp:155] Top shape: 4 64 40 40 (409600)
I0711 18:09:31.964423  7290 net.cpp:163] Memory required for data: 791347200
I0711 18:09:31.964428  7290 layer_factory.hpp:77] Creating layer out5a/bn
I0711 18:09:31.964443  7290 net.cpp:98] Creating Layer out5a/bn
I0711 18:09:31.964447  7290 net.cpp:439] out5a/bn <- out5a
I0711 18:09:31.964450  7290 net.cpp:413] out5a/bn -> out5a/bn
I0711 18:09:31.965260  7290 net.cpp:148] Setting up out5a/bn
I0711 18:09:31.965267  7290 net.cpp:155] Top shape: 4 64 40 40 (409600)
I0711 18:09:31.965279  7290 net.cpp:163] Memory required for data: 792985600
I0711 18:09:31.965284  7290 layer_factory.hpp:77] Creating layer out5a/relu
I0711 18:09:31.965288  7290 net.cpp:98] Creating Layer out5a/relu
I0711 18:09:31.965291  7290 net.cpp:439] out5a/relu <- out5a/bn
I0711 18:09:31.965293  7290 net.cpp:400] out5a/relu -> out5a/bn (in-place)
I0711 18:09:31.965297  7290 net.cpp:148] Setting up out5a/relu
I0711 18:09:31.965299  7290 net.cpp:155] Top shape: 4 64 40 40 (409600)
I0711 18:09:31.965301  7290 net.cpp:163] Memory required for data: 794624000
I0711 18:09:31.965303  7290 layer_factory.hpp:77] Creating layer out5a_up2
I0711 18:09:31.965307  7290 net.cpp:98] Creating Layer out5a_up2
I0711 18:09:31.965309  7290 net.cpp:439] out5a_up2 <- out5a/bn
I0711 18:09:31.965312  7290 net.cpp:413] out5a_up2 -> out5a_up2
I0711 18:09:31.965600  7290 net.cpp:148] Setting up out5a_up2
I0711 18:09:31.965605  7290 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 18:09:31.965606  7290 net.cpp:163] Memory required for data: 801177600
I0711 18:09:31.965610  7290 layer_factory.hpp:77] Creating layer out3a
I0711 18:09:31.965613  7290 net.cpp:98] Creating Layer out3a
I0711 18:09:31.965616  7290 net.cpp:439] out3a <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0711 18:09:31.965620  7290 net.cpp:413] out3a -> out3a
I0711 18:09:31.967731  7290 net.cpp:148] Setting up out3a
I0711 18:09:31.967742  7290 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 18:09:31.967744  7290 net.cpp:163] Memory required for data: 807731200
I0711 18:09:31.967749  7290 layer_factory.hpp:77] Creating layer out3a/bn
I0711 18:09:31.967754  7290 net.cpp:98] Creating Layer out3a/bn
I0711 18:09:31.967756  7290 net.cpp:439] out3a/bn <- out3a
I0711 18:09:31.967761  7290 net.cpp:413] out3a/bn -> out3a/bn
I0711 18:09:31.968570  7290 net.cpp:148] Setting up out3a/bn
I0711 18:09:31.968577  7290 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 18:09:31.968580  7290 net.cpp:163] Memory required for data: 814284800
I0711 18:09:31.968585  7290 layer_factory.hpp:77] Creating layer out3a/relu
I0711 18:09:31.968587  7290 net.cpp:98] Creating Layer out3a/relu
I0711 18:09:31.968590  7290 net.cpp:439] out3a/relu <- out3a/bn
I0711 18:09:31.968592  7290 net.cpp:400] out3a/relu -> out3a/bn (in-place)
I0711 18:09:31.968596  7290 net.cpp:148] Setting up out3a/relu
I0711 18:09:31.968598  7290 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 18:09:31.968600  7290 net.cpp:163] Memory required for data: 820838400
I0711 18:09:31.968602  7290 layer_factory.hpp:77] Creating layer out3_out5_combined
I0711 18:09:31.968606  7290 net.cpp:98] Creating Layer out3_out5_combined
I0711 18:09:31.968608  7290 net.cpp:439] out3_out5_combined <- out5a_up2
I0711 18:09:31.968611  7290 net.cpp:439] out3_out5_combined <- out3a/bn
I0711 18:09:31.968612  7290 net.cpp:413] out3_out5_combined -> out3_out5_combined
I0711 18:09:31.968637  7290 net.cpp:148] Setting up out3_out5_combined
I0711 18:09:31.968641  7290 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 18:09:31.968643  7290 net.cpp:163] Memory required for data: 827392000
I0711 18:09:31.968646  7290 layer_factory.hpp:77] Creating layer ctx_conv1
I0711 18:09:31.968650  7290 net.cpp:98] Creating Layer ctx_conv1
I0711 18:09:31.968652  7290 net.cpp:439] ctx_conv1 <- out3_out5_combined
I0711 18:09:31.968657  7290 net.cpp:413] ctx_conv1 -> ctx_conv1
I0711 18:09:31.969722  7290 net.cpp:148] Setting up ctx_conv1
I0711 18:09:31.969727  7290 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 18:09:31.969729  7290 net.cpp:163] Memory required for data: 833945600
I0711 18:09:31.969732  7290 layer_factory.hpp:77] Creating layer ctx_conv1/bn
I0711 18:09:31.969735  7290 net.cpp:98] Creating Layer ctx_conv1/bn
I0711 18:09:31.969738  7290 net.cpp:439] ctx_conv1/bn <- ctx_conv1
I0711 18:09:31.969740  7290 net.cpp:413] ctx_conv1/bn -> ctx_conv1/bn
I0711 18:09:31.970549  7290 net.cpp:148] Setting up ctx_conv1/bn
I0711 18:09:31.970554  7290 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 18:09:31.970556  7290 net.cpp:163] Memory required for data: 840499200
I0711 18:09:31.970569  7290 layer_factory.hpp:77] Creating layer ctx_conv1/relu
I0711 18:09:31.970572  7290 net.cpp:98] Creating Layer ctx_conv1/relu
I0711 18:09:31.970576  7290 net.cpp:439] ctx_conv1/relu <- ctx_conv1/bn
I0711 18:09:31.970577  7290 net.cpp:400] ctx_conv1/relu -> ctx_conv1/bn (in-place)
I0711 18:09:31.970580  7290 net.cpp:148] Setting up ctx_conv1/relu
I0711 18:09:31.970583  7290 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 18:09:31.970585  7290 net.cpp:163] Memory required for data: 847052800
I0711 18:09:31.970587  7290 layer_factory.hpp:77] Creating layer ctx_conv2
I0711 18:09:31.970592  7290 net.cpp:98] Creating Layer ctx_conv2
I0711 18:09:31.970595  7290 net.cpp:439] ctx_conv2 <- ctx_conv1/bn
I0711 18:09:31.970598  7290 net.cpp:413] ctx_conv2 -> ctx_conv2
I0711 18:09:31.971653  7290 net.cpp:148] Setting up ctx_conv2
I0711 18:09:31.971658  7290 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 18:09:31.971660  7290 net.cpp:163] Memory required for data: 853606400
I0711 18:09:31.971663  7290 layer_factory.hpp:77] Creating layer ctx_conv2/bn
I0711 18:09:31.971667  7290 net.cpp:98] Creating Layer ctx_conv2/bn
I0711 18:09:31.971668  7290 net.cpp:439] ctx_conv2/bn <- ctx_conv2
I0711 18:09:31.971671  7290 net.cpp:413] ctx_conv2/bn -> ctx_conv2/bn
I0711 18:09:31.972483  7290 net.cpp:148] Setting up ctx_conv2/bn
I0711 18:09:31.972491  7290 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 18:09:31.972492  7290 net.cpp:163] Memory required for data: 860160000
I0711 18:09:31.972496  7290 layer_factory.hpp:77] Creating layer ctx_conv2/relu
I0711 18:09:31.972499  7290 net.cpp:98] Creating Layer ctx_conv2/relu
I0711 18:09:31.972501  7290 net.cpp:439] ctx_conv2/relu <- ctx_conv2/bn
I0711 18:09:31.972503  7290 net.cpp:400] ctx_conv2/relu -> ctx_conv2/bn (in-place)
I0711 18:09:31.972507  7290 net.cpp:148] Setting up ctx_conv2/relu
I0711 18:09:31.972509  7290 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 18:09:31.972512  7290 net.cpp:163] Memory required for data: 866713600
I0711 18:09:31.972513  7290 layer_factory.hpp:77] Creating layer ctx_conv3
I0711 18:09:31.972517  7290 net.cpp:98] Creating Layer ctx_conv3
I0711 18:09:31.972519  7290 net.cpp:439] ctx_conv3 <- ctx_conv2/bn
I0711 18:09:31.972522  7290 net.cpp:413] ctx_conv3 -> ctx_conv3
I0711 18:09:31.973588  7290 net.cpp:148] Setting up ctx_conv3
I0711 18:09:31.973594  7290 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 18:09:31.973597  7290 net.cpp:163] Memory required for data: 873267200
I0711 18:09:31.973599  7290 layer_factory.hpp:77] Creating layer ctx_conv3/bn
I0711 18:09:31.973603  7290 net.cpp:98] Creating Layer ctx_conv3/bn
I0711 18:09:31.973605  7290 net.cpp:439] ctx_conv3/bn <- ctx_conv3
I0711 18:09:31.973608  7290 net.cpp:413] ctx_conv3/bn -> ctx_conv3/bn
I0711 18:09:31.974416  7290 net.cpp:148] Setting up ctx_conv3/bn
I0711 18:09:31.974421  7290 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 18:09:31.974423  7290 net.cpp:163] Memory required for data: 879820800
I0711 18:09:31.974428  7290 layer_factory.hpp:77] Creating layer ctx_conv3/relu
I0711 18:09:31.974432  7290 net.cpp:98] Creating Layer ctx_conv3/relu
I0711 18:09:31.974436  7290 net.cpp:439] ctx_conv3/relu <- ctx_conv3/bn
I0711 18:09:31.974437  7290 net.cpp:400] ctx_conv3/relu -> ctx_conv3/bn (in-place)
I0711 18:09:31.974440  7290 net.cpp:148] Setting up ctx_conv3/relu
I0711 18:09:31.974443  7290 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 18:09:31.974445  7290 net.cpp:163] Memory required for data: 886374400
I0711 18:09:31.974449  7290 layer_factory.hpp:77] Creating layer ctx_conv4
I0711 18:09:31.974452  7290 net.cpp:98] Creating Layer ctx_conv4
I0711 18:09:31.974457  7290 net.cpp:439] ctx_conv4 <- ctx_conv3/bn
I0711 18:09:31.974459  7290 net.cpp:413] ctx_conv4 -> ctx_conv4
I0711 18:09:31.975518  7290 net.cpp:148] Setting up ctx_conv4
I0711 18:09:31.975524  7290 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 18:09:31.975527  7290 net.cpp:163] Memory required for data: 892928000
I0711 18:09:31.975529  7290 layer_factory.hpp:77] Creating layer ctx_conv4/bn
I0711 18:09:31.975539  7290 net.cpp:98] Creating Layer ctx_conv4/bn
I0711 18:09:31.975543  7290 net.cpp:439] ctx_conv4/bn <- ctx_conv4
I0711 18:09:31.975545  7290 net.cpp:413] ctx_conv4/bn -> ctx_conv4/bn
I0711 18:09:31.976346  7290 net.cpp:148] Setting up ctx_conv4/bn
I0711 18:09:31.976351  7290 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 18:09:31.976353  7290 net.cpp:163] Memory required for data: 899481600
I0711 18:09:31.976357  7290 layer_factory.hpp:77] Creating layer ctx_conv4/relu
I0711 18:09:31.976361  7290 net.cpp:98] Creating Layer ctx_conv4/relu
I0711 18:09:31.976363  7290 net.cpp:439] ctx_conv4/relu <- ctx_conv4/bn
I0711 18:09:31.976366  7290 net.cpp:400] ctx_conv4/relu -> ctx_conv4/bn (in-place)
I0711 18:09:31.976369  7290 net.cpp:148] Setting up ctx_conv4/relu
I0711 18:09:31.976372  7290 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 18:09:31.976375  7290 net.cpp:163] Memory required for data: 906035200
I0711 18:09:31.976377  7290 layer_factory.hpp:77] Creating layer ctx_final
I0711 18:09:31.976380  7290 net.cpp:98] Creating Layer ctx_final
I0711 18:09:31.976383  7290 net.cpp:439] ctx_final <- ctx_conv4/bn
I0711 18:09:31.976385  7290 net.cpp:413] ctx_final -> ctx_final
I0711 18:09:31.976821  7290 net.cpp:148] Setting up ctx_final
I0711 18:09:31.976827  7290 net.cpp:155] Top shape: 4 8 80 80 (204800)
I0711 18:09:31.976830  7290 net.cpp:163] Memory required for data: 906854400
I0711 18:09:31.976832  7290 layer_factory.hpp:77] Creating layer ctx_final/relu
I0711 18:09:31.976835  7290 net.cpp:98] Creating Layer ctx_final/relu
I0711 18:09:31.976837  7290 net.cpp:439] ctx_final/relu <- ctx_final
I0711 18:09:31.976840  7290 net.cpp:400] ctx_final/relu -> ctx_final (in-place)
I0711 18:09:31.976843  7290 net.cpp:148] Setting up ctx_final/relu
I0711 18:09:31.976846  7290 net.cpp:155] Top shape: 4 8 80 80 (204800)
I0711 18:09:31.976848  7290 net.cpp:163] Memory required for data: 907673600
I0711 18:09:31.976850  7290 layer_factory.hpp:77] Creating layer out_deconv_final_up2
I0711 18:09:31.976853  7290 net.cpp:98] Creating Layer out_deconv_final_up2
I0711 18:09:31.976856  7290 net.cpp:439] out_deconv_final_up2 <- ctx_final
I0711 18:09:31.976860  7290 net.cpp:413] out_deconv_final_up2 -> out_deconv_final_up2
I0711 18:09:31.977118  7290 net.cpp:148] Setting up out_deconv_final_up2
I0711 18:09:31.977123  7290 net.cpp:155] Top shape: 4 8 160 160 (819200)
I0711 18:09:31.977124  7290 net.cpp:163] Memory required for data: 910950400
I0711 18:09:31.977128  7290 layer_factory.hpp:77] Creating layer out_deconv_final_up4
I0711 18:09:31.977130  7290 net.cpp:98] Creating Layer out_deconv_final_up4
I0711 18:09:31.977133  7290 net.cpp:439] out_deconv_final_up4 <- out_deconv_final_up2
I0711 18:09:31.977136  7290 net.cpp:413] out_deconv_final_up4 -> out_deconv_final_up4
I0711 18:09:31.977394  7290 net.cpp:148] Setting up out_deconv_final_up4
I0711 18:09:31.977399  7290 net.cpp:155] Top shape: 4 8 320 320 (3276800)
I0711 18:09:31.977401  7290 net.cpp:163] Memory required for data: 924057600
I0711 18:09:31.977404  7290 layer_factory.hpp:77] Creating layer out_deconv_final_up8
I0711 18:09:31.977407  7290 net.cpp:98] Creating Layer out_deconv_final_up8
I0711 18:09:31.977411  7290 net.cpp:439] out_deconv_final_up8 <- out_deconv_final_up4
I0711 18:09:31.977413  7290 net.cpp:413] out_deconv_final_up8 -> out_deconv_final_up8
I0711 18:09:31.977661  7290 net.cpp:148] Setting up out_deconv_final_up8
I0711 18:09:31.977666  7290 net.cpp:155] Top shape: 4 8 640 640 (13107200)
I0711 18:09:31.977669  7290 net.cpp:163] Memory required for data: 976486400
I0711 18:09:31.977671  7290 layer_factory.hpp:77] Creating layer out_deconv_final_up8_out_deconv_final_up8_0_split
I0711 18:09:31.977675  7290 net.cpp:98] Creating Layer out_deconv_final_up8_out_deconv_final_up8_0_split
I0711 18:09:31.977677  7290 net.cpp:439] out_deconv_final_up8_out_deconv_final_up8_0_split <- out_deconv_final_up8
I0711 18:09:31.977680  7290 net.cpp:413] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0711 18:09:31.977689  7290 net.cpp:413] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0711 18:09:31.977694  7290 net.cpp:413] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0711 18:09:31.977752  7290 net.cpp:148] Setting up out_deconv_final_up8_out_deconv_final_up8_0_split
I0711 18:09:31.977757  7290 net.cpp:155] Top shape: 4 8 640 640 (13107200)
I0711 18:09:31.977759  7290 net.cpp:155] Top shape: 4 8 640 640 (13107200)
I0711 18:09:31.977762  7290 net.cpp:155] Top shape: 4 8 640 640 (13107200)
I0711 18:09:31.977764  7290 net.cpp:163] Memory required for data: 1133772800
I0711 18:09:31.977766  7290 layer_factory.hpp:77] Creating layer loss
I0711 18:09:31.977771  7290 net.cpp:98] Creating Layer loss
I0711 18:09:31.977774  7290 net.cpp:439] loss <- out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0711 18:09:31.977777  7290 net.cpp:439] loss <- label_data_1_split_0
I0711 18:09:31.977779  7290 net.cpp:413] loss -> loss
I0711 18:09:31.977784  7290 layer_factory.hpp:77] Creating layer loss
I0711 18:09:31.996263  7290 net.cpp:148] Setting up loss
I0711 18:09:31.996284  7290 net.cpp:155] Top shape: (1)
I0711 18:09:31.996287  7290 net.cpp:158]     with loss weight 1
I0711 18:09:31.996295  7290 net.cpp:163] Memory required for data: 1133772804
I0711 18:09:31.996299  7290 layer_factory.hpp:77] Creating layer accuracy/top1
I0711 18:09:31.996306  7290 net.cpp:98] Creating Layer accuracy/top1
I0711 18:09:31.996311  7290 net.cpp:439] accuracy/top1 <- out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0711 18:09:31.996316  7290 net.cpp:439] accuracy/top1 <- label_data_1_split_1
I0711 18:09:31.996320  7290 net.cpp:413] accuracy/top1 -> accuracy/top1
I0711 18:09:31.996328  7290 net.cpp:148] Setting up accuracy/top1
I0711 18:09:31.996331  7290 net.cpp:155] Top shape: (1)
I0711 18:09:31.996333  7290 net.cpp:163] Memory required for data: 1133772808
I0711 18:09:31.996335  7290 layer_factory.hpp:77] Creating layer accuracy/top5
I0711 18:09:31.996338  7290 net.cpp:98] Creating Layer accuracy/top5
I0711 18:09:31.996341  7290 net.cpp:439] accuracy/top5 <- out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0711 18:09:31.996343  7290 net.cpp:439] accuracy/top5 <- label_data_1_split_2
I0711 18:09:31.996347  7290 net.cpp:413] accuracy/top5 -> accuracy/top5
I0711 18:09:31.996351  7290 net.cpp:148] Setting up accuracy/top5
I0711 18:09:31.996353  7290 net.cpp:155] Top shape: (1)
I0711 18:09:31.996356  7290 net.cpp:163] Memory required for data: 1133772812
I0711 18:09:31.996357  7290 net.cpp:226] accuracy/top5 does not need backward computation.
I0711 18:09:31.996359  7290 net.cpp:226] accuracy/top1 does not need backward computation.
I0711 18:09:31.996362  7290 net.cpp:224] loss needs backward computation.
I0711 18:09:31.996366  7290 net.cpp:224] out_deconv_final_up8_out_deconv_final_up8_0_split needs backward computation.
I0711 18:09:31.996367  7290 net.cpp:224] out_deconv_final_up8 needs backward computation.
I0711 18:09:31.996369  7290 net.cpp:224] out_deconv_final_up4 needs backward computation.
I0711 18:09:31.996371  7290 net.cpp:224] out_deconv_final_up2 needs backward computation.
I0711 18:09:31.996374  7290 net.cpp:224] ctx_final/relu needs backward computation.
I0711 18:09:31.996377  7290 net.cpp:224] ctx_final needs backward computation.
I0711 18:09:31.996379  7290 net.cpp:224] ctx_conv4/relu needs backward computation.
I0711 18:09:31.996381  7290 net.cpp:224] ctx_conv4/bn needs backward computation.
I0711 18:09:31.996383  7290 net.cpp:224] ctx_conv4 needs backward computation.
I0711 18:09:31.996386  7290 net.cpp:224] ctx_conv3/relu needs backward computation.
I0711 18:09:31.996387  7290 net.cpp:224] ctx_conv3/bn needs backward computation.
I0711 18:09:31.996389  7290 net.cpp:224] ctx_conv3 needs backward computation.
I0711 18:09:31.996392  7290 net.cpp:224] ctx_conv2/relu needs backward computation.
I0711 18:09:31.996394  7290 net.cpp:224] ctx_conv2/bn needs backward computation.
I0711 18:09:31.996397  7290 net.cpp:224] ctx_conv2 needs backward computation.
I0711 18:09:31.996408  7290 net.cpp:224] ctx_conv1/relu needs backward computation.
I0711 18:09:31.996410  7290 net.cpp:224] ctx_conv1/bn needs backward computation.
I0711 18:09:31.996413  7290 net.cpp:224] ctx_conv1 needs backward computation.
I0711 18:09:31.996417  7290 net.cpp:224] out3_out5_combined needs backward computation.
I0711 18:09:31.996418  7290 net.cpp:224] out3a/relu needs backward computation.
I0711 18:09:31.996420  7290 net.cpp:224] out3a/bn needs backward computation.
I0711 18:09:31.996423  7290 net.cpp:224] out3a needs backward computation.
I0711 18:09:31.996425  7290 net.cpp:224] out5a_up2 needs backward computation.
I0711 18:09:31.996428  7290 net.cpp:224] out5a/relu needs backward computation.
I0711 18:09:31.996430  7290 net.cpp:224] out5a/bn needs backward computation.
I0711 18:09:31.996433  7290 net.cpp:224] out5a needs backward computation.
I0711 18:09:31.996435  7290 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0711 18:09:31.996438  7290 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0711 18:09:31.996440  7290 net.cpp:224] res5a_branch2b needs backward computation.
I0711 18:09:31.996443  7290 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0711 18:09:31.996445  7290 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0711 18:09:31.996448  7290 net.cpp:224] res5a_branch2a needs backward computation.
I0711 18:09:31.996450  7290 net.cpp:224] pool4 needs backward computation.
I0711 18:09:31.996454  7290 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0711 18:09:31.996455  7290 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0711 18:09:31.996457  7290 net.cpp:224] res4a_branch2b needs backward computation.
I0711 18:09:31.996460  7290 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0711 18:09:31.996462  7290 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0711 18:09:31.996464  7290 net.cpp:224] res4a_branch2a needs backward computation.
I0711 18:09:31.996467  7290 net.cpp:224] pool3 needs backward computation.
I0711 18:09:31.996469  7290 net.cpp:224] res3a_branch2b/bn_res3a_branch2b/relu_0_split needs backward computation.
I0711 18:09:31.996472  7290 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0711 18:09:31.996474  7290 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0711 18:09:31.996476  7290 net.cpp:224] res3a_branch2b needs backward computation.
I0711 18:09:31.996479  7290 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0711 18:09:31.996482  7290 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0711 18:09:31.996484  7290 net.cpp:224] res3a_branch2a needs backward computation.
I0711 18:09:31.996486  7290 net.cpp:224] pool2 needs backward computation.
I0711 18:09:31.996490  7290 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0711 18:09:31.996492  7290 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0711 18:09:31.996495  7290 net.cpp:224] res2a_branch2b needs backward computation.
I0711 18:09:31.996497  7290 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0711 18:09:31.996500  7290 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0711 18:09:31.996502  7290 net.cpp:224] res2a_branch2a needs backward computation.
I0711 18:09:31.996505  7290 net.cpp:224] pool1 needs backward computation.
I0711 18:09:31.996506  7290 net.cpp:224] conv1b/relu needs backward computation.
I0711 18:09:31.996508  7290 net.cpp:224] conv1b/bn needs backward computation.
I0711 18:09:31.996511  7290 net.cpp:224] conv1b needs backward computation.
I0711 18:09:31.996513  7290 net.cpp:224] conv1a/relu needs backward computation.
I0711 18:09:31.996515  7290 net.cpp:224] conv1a/bn needs backward computation.
I0711 18:09:31.996517  7290 net.cpp:224] conv1a needs backward computation.
I0711 18:09:31.996520  7290 net.cpp:226] data/bias does not need backward computation.
I0711 18:09:31.996523  7290 net.cpp:226] label_data_1_split does not need backward computation.
I0711 18:09:31.996526  7290 net.cpp:226] data does not need backward computation.
I0711 18:09:31.996531  7290 net.cpp:268] This network produces output accuracy/top1
I0711 18:09:31.996534  7290 net.cpp:268] This network produces output accuracy/top5
I0711 18:09:31.996536  7290 net.cpp:268] This network produces output loss
I0711 18:09:31.996569  7290 net.cpp:288] Network initialization done.
I0711 18:09:31.996666  7290 solver.cpp:60] Solver scaffolding done.
I0711 18:09:32.004822  7290 caffe.cpp:145] Finetuning from training/imagenet_jacintonet11v2_iter_320000.caffemodel
I0711 18:09:32.116971  7290 net.cpp:804] Ignoring source layer pool5
I0711 18:09:32.116991  7290 net.cpp:804] Ignoring source layer fc1000
I0711 18:09:32.127862  7290 net.cpp:804] Ignoring source layer pool5
I0711 18:09:32.127882  7290 net.cpp:804] Ignoring source layer fc1000
I0711 18:09:32.141258  7290 data_layer.cpp:78] ReshapePrefetch 5, 3, 640, 640
I0711 18:09:32.141343  7290 data_layer.cpp:83] output data size: 5,3,640,640
I0711 18:09:32.177129  7290 data_layer.cpp:78] ReshapePrefetch 5, 1, 640, 640
I0711 18:09:32.177563  7290 data_layer.cpp:83] output data size: 5,1,640,640
I0711 18:09:34.819324  7290 data_layer.cpp:78] ReshapePrefetch 5, 3, 640, 640
I0711 18:09:34.819582  7290 data_layer.cpp:83] output data size: 5,3,640,640
I0711 18:09:35.451122  7290 data_layer.cpp:78] ReshapePrefetch 5, 1, 640, 640
I0711 18:09:35.454493  7290 data_layer.cpp:83] output data size: 5,1,640,640
I0711 18:09:36.430651  7290 parallel.cpp:334] Starting Optimization
I0711 18:09:36.430721  7290 solver.cpp:409] Solving jsegnet21v2_train
I0711 18:09:36.430729  7290 solver.cpp:410] Learning Rate Policy: multistep
I0711 18:09:36.984307  7290 solver.cpp:290] Iteration 0 (0 iter/s, 0.553501s/100 iter), loss = 1.91868
I0711 18:09:36.984338  7290 solver.cpp:309]     Train net output #0: loss = 1.91868 (* 1 = 1.91868 loss)
I0711 18:09:36.984366  7290 sgd_solver.cpp:106] Iteration 0, lr = 0.0001
I0711 18:09:50.582197  7500 blocking_queue.cpp:50] Data layer prefetch queue empty
I0711 18:09:56.994362  7290 solver.cpp:290] Iteration 100 (4.99763 iter/s, 20.0095s/100 iter), loss = 0.667329
I0711 18:09:56.994395  7290 solver.cpp:309]     Train net output #0: loss = 0.667329 (* 1 = 0.667329 loss)
I0711 18:09:56.994407  7290 sgd_solver.cpp:106] Iteration 100, lr = 0.0001
I0711 18:11:45.395501  7352 blocking_queue.cpp:50] Waiting for data
I0711 18:12:06.712467  7290 solver.cpp:290] Iteration 200 (0.770924 iter/s, 129.715s/100 iter), loss = 0.167559
I0711 18:12:06.712489  7290 solver.cpp:309]     Train net output #0: loss = 0.167559 (* 1 = 0.167559 loss)
I0711 18:12:06.712496  7290 sgd_solver.cpp:106] Iteration 200, lr = 0.0001
I0711 18:12:23.744797  7290 solver.cpp:290] Iteration 300 (5.87136 iter/s, 17.0318s/100 iter), loss = 0.133885
I0711 18:12:23.744843  7290 solver.cpp:309]     Train net output #0: loss = 0.133885 (* 1 = 0.133885 loss)
I0711 18:12:23.744850  7290 sgd_solver.cpp:106] Iteration 300, lr = 0.0001
I0711 18:12:40.786767  7290 solver.cpp:290] Iteration 400 (5.86804 iter/s, 17.0415s/100 iter), loss = 0.0947322
I0711 18:12:40.786789  7290 solver.cpp:309]     Train net output #0: loss = 0.0947322 (* 1 = 0.0947322 loss)
I0711 18:12:40.786797  7290 sgd_solver.cpp:106] Iteration 400, lr = 0.0001
I0711 18:12:58.029858  7290 solver.cpp:290] Iteration 500 (5.79959 iter/s, 17.2426s/100 iter), loss = 0.132188
I0711 18:12:58.029949  7290 solver.cpp:309]     Train net output #0: loss = 0.132188 (* 1 = 0.132188 loss)
I0711 18:12:58.029960  7290 sgd_solver.cpp:106] Iteration 500, lr = 0.0001
I0711 18:13:15.236356  7290 solver.cpp:290] Iteration 600 (5.81195 iter/s, 17.2059s/100 iter), loss = 0.0519722
I0711 18:13:15.236379  7290 solver.cpp:309]     Train net output #0: loss = 0.0519722 (* 1 = 0.0519722 loss)
I0711 18:13:15.236387  7290 sgd_solver.cpp:106] Iteration 600, lr = 0.0001
I0711 18:13:32.419400  7290 solver.cpp:290] Iteration 700 (5.81986 iter/s, 17.1825s/100 iter), loss = 0.0685836
I0711 18:13:32.419478  7290 solver.cpp:309]     Train net output #0: loss = 0.0685836 (* 1 = 0.0685836 loss)
I0711 18:13:32.419487  7290 sgd_solver.cpp:106] Iteration 700, lr = 0.0001
I0711 18:13:49.525527  7290 solver.cpp:290] Iteration 800 (5.84605 iter/s, 17.1056s/100 iter), loss = 0.185176
I0711 18:13:49.525549  7290 solver.cpp:309]     Train net output #0: loss = 0.185176 (* 1 = 0.185176 loss)
I0711 18:13:49.525557  7290 sgd_solver.cpp:106] Iteration 800, lr = 0.0001
I0711 18:14:06.586501  7290 solver.cpp:290] Iteration 900 (5.8615 iter/s, 17.0605s/100 iter), loss = 0.041229
I0711 18:14:06.586621  7290 solver.cpp:309]     Train net output #0: loss = 0.041229 (* 1 = 0.041229 loss)
I0711 18:14:06.586629  7290 sgd_solver.cpp:106] Iteration 900, lr = 0.0001
I0711 18:14:23.656035  7290 solver.cpp:290] Iteration 1000 (5.85859 iter/s, 17.0689s/100 iter), loss = 0.160046
I0711 18:14:23.656059  7290 solver.cpp:309]     Train net output #0: loss = 0.160046 (* 1 = 0.160046 loss)
I0711 18:14:23.656066  7290 sgd_solver.cpp:106] Iteration 1000, lr = 0.0001
I0711 18:14:40.917798  7290 solver.cpp:290] Iteration 1100 (5.79332 iter/s, 17.2613s/100 iter), loss = 0.0953071
I0711 18:14:40.917850  7290 solver.cpp:309]     Train net output #0: loss = 0.0953072 (* 1 = 0.0953072 loss)
I0711 18:14:40.917858  7290 sgd_solver.cpp:106] Iteration 1100, lr = 0.0001
I0711 18:14:57.918982  7290 solver.cpp:290] Iteration 1200 (5.88212 iter/s, 17.0007s/100 iter), loss = 0.0970717
I0711 18:14:57.919004  7290 solver.cpp:309]     Train net output #0: loss = 0.0970718 (* 1 = 0.0970718 loss)
I0711 18:14:57.919011  7290 sgd_solver.cpp:106] Iteration 1200, lr = 0.0001
I0711 18:15:14.954910  7290 solver.cpp:290] Iteration 1300 (5.87012 iter/s, 17.0354s/100 iter), loss = 0.0564315
I0711 18:15:14.954957  7290 solver.cpp:309]     Train net output #0: loss = 0.0564316 (* 1 = 0.0564316 loss)
I0711 18:15:14.954965  7290 sgd_solver.cpp:106] Iteration 1300, lr = 0.0001
I0711 18:15:32.167434  7290 solver.cpp:290] Iteration 1400 (5.8099 iter/s, 17.212s/100 iter), loss = 0.0944538
I0711 18:15:32.167460  7290 solver.cpp:309]     Train net output #0: loss = 0.0944539 (* 1 = 0.0944539 loss)
I0711 18:15:32.167469  7290 sgd_solver.cpp:106] Iteration 1400, lr = 0.0001
I0711 18:15:49.564564  7290 solver.cpp:290] Iteration 1500 (5.74824 iter/s, 17.3966s/100 iter), loss = 0.0403869
I0711 18:15:49.564662  7290 solver.cpp:309]     Train net output #0: loss = 0.040387 (* 1 = 0.040387 loss)
I0711 18:15:49.564671  7290 sgd_solver.cpp:106] Iteration 1500, lr = 0.0001
I0711 18:16:06.592124  7290 solver.cpp:290] Iteration 1600 (5.87303 iter/s, 17.027s/100 iter), loss = 0.100196
I0711 18:16:06.592154  7290 solver.cpp:309]     Train net output #0: loss = 0.100196 (* 1 = 0.100196 loss)
I0711 18:16:06.592164  7290 sgd_solver.cpp:106] Iteration 1600, lr = 0.0001
I0711 18:16:23.894589  7290 solver.cpp:290] Iteration 1700 (5.77969 iter/s, 17.302s/100 iter), loss = 0.0670149
I0711 18:16:23.894644  7290 solver.cpp:309]     Train net output #0: loss = 0.0670151 (* 1 = 0.0670151 loss)
I0711 18:16:23.894654  7290 sgd_solver.cpp:106] Iteration 1700, lr = 0.0001
I0711 18:16:41.085741  7290 solver.cpp:290] Iteration 1800 (5.81712 iter/s, 17.1906s/100 iter), loss = 0.0896805
I0711 18:16:41.085769  7290 solver.cpp:309]     Train net output #0: loss = 0.0896807 (* 1 = 0.0896807 loss)
I0711 18:16:41.085779  7290 sgd_solver.cpp:106] Iteration 1800, lr = 0.0001
I0711 18:16:58.132289  7290 solver.cpp:290] Iteration 1900 (5.86646 iter/s, 17.0461s/100 iter), loss = 0.0702805
I0711 18:16:58.132331  7290 solver.cpp:309]     Train net output #0: loss = 0.0702806 (* 1 = 0.0702806 loss)
I0711 18:16:58.132339  7290 sgd_solver.cpp:106] Iteration 1900, lr = 0.0001
I0711 18:17:14.998767  7290 solver.cpp:467] Iteration 2000, Testing net (#0)
I0711 18:18:02.568859  7290 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.9376
I0711 18:18:02.568938  7290 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999886
I0711 18:18:02.568945  7290 solver.cpp:540]     Test net output #2: loss = 0.0980816 (* 1 = 0.0980816 loss)
I0711 18:18:02.766706  7290 solver.cpp:290] Iteration 2000 (1.54721 iter/s, 64.6326s/100 iter), loss = 0.0261143
I0711 18:18:02.766736  7290 solver.cpp:309]     Train net output #0: loss = 0.0261144 (* 1 = 0.0261144 loss)
I0711 18:18:02.766744  7290 sgd_solver.cpp:106] Iteration 2000, lr = 0.0001
I0711 18:18:23.314285  7386 blocking_queue.cpp:50] Waiting for data
I0711 18:18:53.397811  7290 solver.cpp:290] Iteration 2100 (1.97513 iter/s, 50.6297s/100 iter), loss = 0.0491348
I0711 18:18:53.397930  7290 solver.cpp:309]     Train net output #0: loss = 0.0491348 (* 1 = 0.0491348 loss)
I0711 18:18:53.397940  7290 sgd_solver.cpp:106] Iteration 2100, lr = 0.0001
I0711 18:20:01.529809  7483 blocking_queue.cpp:50] Waiting for data
I0711 18:20:25.158123  7290 solver.cpp:290] Iteration 2200 (1.08983 iter/s, 91.7577s/100 iter), loss = 0.0617981
I0711 18:20:25.158149  7290 solver.cpp:309]     Train net output #0: loss = 0.0617981 (* 1 = 0.0617981 loss)
I0711 18:20:25.158157  7290 sgd_solver.cpp:106] Iteration 2200, lr = 0.0001
I0711 18:20:49.393815  7290 solver.cpp:290] Iteration 2300 (4.12627 iter/s, 24.235s/100 iter), loss = 0.0827685
I0711 18:20:49.393924  7290 solver.cpp:309]     Train net output #0: loss = 0.0827686 (* 1 = 0.0827686 loss)
I0711 18:20:49.393935  7290 sgd_solver.cpp:106] Iteration 2300, lr = 0.0001
I0711 18:21:06.378211  7290 solver.cpp:290] Iteration 2400 (5.88796 iter/s, 16.9838s/100 iter), loss = 0.0362968
I0711 18:21:06.378234  7290 solver.cpp:309]     Train net output #0: loss = 0.0362968 (* 1 = 0.0362968 loss)
I0711 18:21:06.378242  7290 sgd_solver.cpp:106] Iteration 2400, lr = 0.0001
I0711 18:21:23.457068  7290 solver.cpp:290] Iteration 2500 (5.85536 iter/s, 17.0784s/100 iter), loss = 0.042775
I0711 18:21:23.457128  7290 solver.cpp:309]     Train net output #0: loss = 0.042775 (* 1 = 0.042775 loss)
I0711 18:21:23.457135  7290 sgd_solver.cpp:106] Iteration 2500, lr = 0.0001
I0711 18:21:40.594348  7290 solver.cpp:290] Iteration 2600 (5.83541 iter/s, 17.1367s/100 iter), loss = 0.0448009
I0711 18:21:40.594372  7290 solver.cpp:309]     Train net output #0: loss = 0.0448009 (* 1 = 0.0448009 loss)
I0711 18:21:40.594380  7290 sgd_solver.cpp:106] Iteration 2600, lr = 0.0001
I0711 18:21:57.593452  7290 solver.cpp:290] Iteration 2700 (5.88283 iter/s, 16.9986s/100 iter), loss = 0.0378237
I0711 18:21:57.593492  7290 solver.cpp:309]     Train net output #0: loss = 0.0378237 (* 1 = 0.0378237 loss)
I0711 18:21:57.593502  7290 sgd_solver.cpp:106] Iteration 2700, lr = 0.0001
I0711 18:22:14.553247  7290 solver.cpp:290] Iteration 2800 (5.89648 iter/s, 16.9593s/100 iter), loss = 0.0821368
I0711 18:22:14.553277  7290 solver.cpp:309]     Train net output #0: loss = 0.0821368 (* 1 = 0.0821368 loss)
I0711 18:22:14.553287  7290 sgd_solver.cpp:106] Iteration 2800, lr = 0.0001
I0711 18:22:31.437991  7290 solver.cpp:290] Iteration 2900 (5.92268 iter/s, 16.8842s/100 iter), loss = 0.054734
I0711 18:22:31.438076  7290 solver.cpp:309]     Train net output #0: loss = 0.054734 (* 1 = 0.054734 loss)
I0711 18:22:31.438086  7290 sgd_solver.cpp:106] Iteration 2900, lr = 0.0001
I0711 18:22:48.349581  7290 solver.cpp:290] Iteration 3000 (5.9133 iter/s, 16.911s/100 iter), loss = 0.0696464
I0711 18:22:48.349602  7290 solver.cpp:309]     Train net output #0: loss = 0.0696464 (* 1 = 0.0696464 loss)
I0711 18:22:48.349609  7290 sgd_solver.cpp:106] Iteration 3000, lr = 0.0001
I0711 18:23:05.442375  7290 solver.cpp:290] Iteration 3100 (5.85059 iter/s, 17.0923s/100 iter), loss = 0.0494853
I0711 18:23:05.442452  7290 solver.cpp:309]     Train net output #0: loss = 0.0494854 (* 1 = 0.0494854 loss)
I0711 18:23:05.442463  7290 sgd_solver.cpp:106] Iteration 3100, lr = 0.0001
I0711 18:23:22.444973  7290 solver.cpp:290] Iteration 3200 (5.88164 iter/s, 17.0021s/100 iter), loss = 0.0547966
I0711 18:23:22.444998  7290 solver.cpp:309]     Train net output #0: loss = 0.0547966 (* 1 = 0.0547966 loss)
I0711 18:23:22.445006  7290 sgd_solver.cpp:106] Iteration 3200, lr = 0.0001
I0711 18:23:39.496029  7290 solver.cpp:290] Iteration 3300 (5.86491 iter/s, 17.0506s/100 iter), loss = 0.0462428
I0711 18:23:39.496098  7290 solver.cpp:309]     Train net output #0: loss = 0.0462429 (* 1 = 0.0462429 loss)
I0711 18:23:39.496105  7290 sgd_solver.cpp:106] Iteration 3300, lr = 0.0001
I0711 18:23:56.443899  7290 solver.cpp:290] Iteration 3400 (5.90063 iter/s, 16.9473s/100 iter), loss = 0.0499624
I0711 18:23:56.443925  7290 solver.cpp:309]     Train net output #0: loss = 0.0499624 (* 1 = 0.0499624 loss)
I0711 18:23:56.443935  7290 sgd_solver.cpp:106] Iteration 3400, lr = 0.0001
I0711 18:24:13.482775  7290 solver.cpp:290] Iteration 3500 (5.8691 iter/s, 17.0384s/100 iter), loss = 0.0651987
I0711 18:24:13.482830  7290 solver.cpp:309]     Train net output #0: loss = 0.0651987 (* 1 = 0.0651987 loss)
I0711 18:24:13.482839  7290 sgd_solver.cpp:106] Iteration 3500, lr = 0.0001
I0711 18:24:30.267144  7290 solver.cpp:290] Iteration 3600 (5.95811 iter/s, 16.7839s/100 iter), loss = 0.0420858
I0711 18:24:30.267171  7290 solver.cpp:309]     Train net output #0: loss = 0.0420858 (* 1 = 0.0420858 loss)
I0711 18:24:30.267180  7290 sgd_solver.cpp:106] Iteration 3600, lr = 0.0001
I0711 18:24:47.357051  7290 solver.cpp:290] Iteration 3700 (5.85158 iter/s, 17.0894s/100 iter), loss = 0.0730352
I0711 18:24:47.357157  7290 solver.cpp:309]     Train net output #0: loss = 0.0730353 (* 1 = 0.0730353 loss)
I0711 18:24:47.357167  7290 sgd_solver.cpp:106] Iteration 3700, lr = 0.0001
I0711 18:25:04.249699  7290 solver.cpp:290] Iteration 3800 (5.91994 iter/s, 16.8921s/100 iter), loss = 0.0418119
I0711 18:25:04.249727  7290 solver.cpp:309]     Train net output #0: loss = 0.041812 (* 1 = 0.041812 loss)
I0711 18:25:04.249735  7290 sgd_solver.cpp:106] Iteration 3800, lr = 0.0001
I0711 18:25:21.217327  7290 solver.cpp:290] Iteration 3900 (5.89375 iter/s, 16.9671s/100 iter), loss = 0.0537464
I0711 18:25:21.217376  7290 solver.cpp:309]     Train net output #0: loss = 0.0537465 (* 1 = 0.0537465 loss)
I0711 18:25:21.217384  7290 sgd_solver.cpp:106] Iteration 3900, lr = 0.0001
I0711 18:25:38.108829  7290 solver.cpp:467] Iteration 4000, Testing net (#0)
I0711 18:26:25.500630  7290 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.942848
I0711 18:26:25.500720  7290 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999983
I0711 18:26:25.500727  7290 solver.cpp:540]     Test net output #2: loss = 0.0978816 (* 1 = 0.0978816 loss)
I0711 18:26:25.675846  7290 solver.cpp:290] Iteration 4000 (1.55143 iter/s, 64.4567s/100 iter), loss = 0.0529157
I0711 18:26:25.675879  7290 solver.cpp:309]     Train net output #0: loss = 0.0529158 (* 1 = 0.0529158 loss)
I0711 18:26:25.675887  7290 sgd_solver.cpp:106] Iteration 4000, lr = 0.0001
I0711 18:26:34.755595  7386 blocking_queue.cpp:50] Waiting for data
I0711 18:27:41.172003  7290 solver.cpp:290] Iteration 4100 (1.32461 iter/s, 75.494s/100 iter), loss = 0.114303
I0711 18:27:41.172109  7290 solver.cpp:309]     Train net output #0: loss = 0.114303 (* 1 = 0.114303 loss)
I0711 18:27:41.172121  7290 sgd_solver.cpp:106] Iteration 4100, lr = 0.0001
I0711 18:28:02.294813  7485 blocking_queue.cpp:50] Waiting for data
I0711 18:28:34.424893  7290 blocking_queue.cpp:50] Data layer prefetch queue empty
I0711 18:29:11.113041  7290 solver.cpp:290] Iteration 4200 (1.11187 iter/s, 89.9385s/100 iter), loss = 0.0317651
I0711 18:29:11.113091  7290 solver.cpp:309]     Train net output #0: loss = 0.0317651 (* 1 = 0.0317651 loss)
I0711 18:29:11.113099  7290 sgd_solver.cpp:106] Iteration 4200, lr = 0.0001
I0711 18:29:29.062860  7290 solver.cpp:290] Iteration 4300 (5.57126 iter/s, 17.9493s/100 iter), loss = 0.0681308
I0711 18:29:29.062882  7290 solver.cpp:309]     Train net output #0: loss = 0.0681309 (* 1 = 0.0681309 loss)
I0711 18:29:29.062889  7290 sgd_solver.cpp:106] Iteration 4300, lr = 0.0001
I0711 18:29:46.217173  7290 solver.cpp:290] Iteration 4400 (5.82961 iter/s, 17.1538s/100 iter), loss = 0.0887211
I0711 18:29:46.217228  7290 solver.cpp:309]     Train net output #0: loss = 0.0887212 (* 1 = 0.0887212 loss)
I0711 18:29:46.217239  7290 sgd_solver.cpp:106] Iteration 4400, lr = 0.0001
I0711 18:30:03.004484  7290 solver.cpp:290] Iteration 4500 (5.95707 iter/s, 16.7868s/100 iter), loss = 0.0351928
I0711 18:30:03.004506  7290 solver.cpp:309]     Train net output #0: loss = 0.0351929 (* 1 = 0.0351929 loss)
I0711 18:30:03.004513  7290 sgd_solver.cpp:106] Iteration 4500, lr = 0.0001
I0711 18:30:20.159389  7290 solver.cpp:290] Iteration 4600 (5.82941 iter/s, 17.1544s/100 iter), loss = 0.0689037
I0711 18:30:20.159494  7290 solver.cpp:309]     Train net output #0: loss = 0.0689038 (* 1 = 0.0689038 loss)
I0711 18:30:20.159507  7290 sgd_solver.cpp:106] Iteration 4600, lr = 0.0001
I0711 18:30:37.247740  7290 solver.cpp:290] Iteration 4700 (5.85214 iter/s, 17.0878s/100 iter), loss = 0.0781765
I0711 18:30:37.247764  7290 solver.cpp:309]     Train net output #0: loss = 0.0781766 (* 1 = 0.0781766 loss)
I0711 18:30:37.247773  7290 sgd_solver.cpp:106] Iteration 4700, lr = 0.0001
I0711 18:30:54.261812  7290 solver.cpp:290] Iteration 4800 (5.87766 iter/s, 17.0136s/100 iter), loss = 0.140073
I0711 18:30:54.261862  7290 solver.cpp:309]     Train net output #0: loss = 0.140073 (* 1 = 0.140073 loss)
I0711 18:30:54.261870  7290 sgd_solver.cpp:106] Iteration 4800, lr = 0.0001
I0711 18:31:11.128482  7290 solver.cpp:290] Iteration 4900 (5.92904 iter/s, 16.8661s/100 iter), loss = 0.10359
I0711 18:31:11.128504  7290 solver.cpp:309]     Train net output #0: loss = 0.10359 (* 1 = 0.10359 loss)
I0711 18:31:11.128512  7290 sgd_solver.cpp:106] Iteration 4900, lr = 0.0001
I0711 18:31:28.064632  7290 solver.cpp:290] Iteration 5000 (5.9047 iter/s, 16.9357s/100 iter), loss = 0.0245194
I0711 18:31:28.064673  7290 solver.cpp:309]     Train net output #0: loss = 0.0245194 (* 1 = 0.0245194 loss)
I0711 18:31:28.064682  7290 sgd_solver.cpp:106] Iteration 5000, lr = 0.0001
I0711 18:31:44.919827  7290 solver.cpp:290] Iteration 5100 (5.93307 iter/s, 16.8547s/100 iter), loss = 0.0978219
I0711 18:31:44.919850  7290 solver.cpp:309]     Train net output #0: loss = 0.0978219 (* 1 = 0.0978219 loss)
I0711 18:31:44.919857  7290 sgd_solver.cpp:106] Iteration 5100, lr = 0.0001
I0711 18:32:02.108726  7290 solver.cpp:290] Iteration 5200 (5.81788 iter/s, 17.1884s/100 iter), loss = 0.0318916
I0711 18:32:02.108770  7290 solver.cpp:309]     Train net output #0: loss = 0.0318916 (* 1 = 0.0318916 loss)
I0711 18:32:02.108778  7290 sgd_solver.cpp:106] Iteration 5200, lr = 0.0001
I0711 18:32:19.036031  7290 solver.cpp:290] Iteration 5300 (5.90779 iter/s, 16.9268s/100 iter), loss = 0.0365637
I0711 18:32:19.036053  7290 solver.cpp:309]     Train net output #0: loss = 0.0365638 (* 1 = 0.0365638 loss)
I0711 18:32:19.036061  7290 sgd_solver.cpp:106] Iteration 5300, lr = 0.0001
I0711 18:32:36.120342  7290 solver.cpp:290] Iteration 5400 (5.85349 iter/s, 17.0838s/100 iter), loss = 0.0258145
I0711 18:32:36.120388  7290 solver.cpp:309]     Train net output #0: loss = 0.0258145 (* 1 = 0.0258145 loss)
I0711 18:32:36.120395  7290 sgd_solver.cpp:106] Iteration 5400, lr = 0.0001
I0711 18:32:53.280110  7290 solver.cpp:290] Iteration 5500 (5.82776 iter/s, 17.1593s/100 iter), loss = 0.0561454
I0711 18:32:53.280133  7290 solver.cpp:309]     Train net output #0: loss = 0.0561455 (* 1 = 0.0561455 loss)
I0711 18:32:53.280140  7290 sgd_solver.cpp:106] Iteration 5500, lr = 0.0001
I0711 18:33:10.289558  7290 solver.cpp:290] Iteration 5600 (5.87926 iter/s, 17.009s/100 iter), loss = 0.0635741
I0711 18:33:10.289675  7290 solver.cpp:309]     Train net output #0: loss = 0.0635742 (* 1 = 0.0635742 loss)
I0711 18:33:10.289690  7290 sgd_solver.cpp:106] Iteration 5600, lr = 0.0001
I0711 18:33:27.239049  7290 solver.cpp:290] Iteration 5700 (5.90008 iter/s, 16.9489s/100 iter), loss = 0.0391532
I0711 18:33:27.239073  7290 solver.cpp:309]     Train net output #0: loss = 0.0391533 (* 1 = 0.0391533 loss)
I0711 18:33:27.239079  7290 sgd_solver.cpp:106] Iteration 5700, lr = 0.0001
I0711 18:33:44.280241  7290 solver.cpp:290] Iteration 5800 (5.8683 iter/s, 17.0407s/100 iter), loss = 0.0799887
I0711 18:33:44.280292  7290 solver.cpp:309]     Train net output #0: loss = 0.0799888 (* 1 = 0.0799888 loss)
I0711 18:33:44.280299  7290 sgd_solver.cpp:106] Iteration 5800, lr = 0.0001
I0711 18:34:01.172812  7290 solver.cpp:290] Iteration 5900 (5.91994 iter/s, 16.8921s/100 iter), loss = 0.214317
I0711 18:34:01.172847  7290 solver.cpp:309]     Train net output #0: loss = 0.214317 (* 1 = 0.214317 loss)
I0711 18:34:01.172854  7290 sgd_solver.cpp:106] Iteration 5900, lr = 0.0001
I0711 18:34:17.938062  7290 solver.cpp:467] Iteration 6000, Testing net (#0)
I0711 18:35:05.549047  7290 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.944847
I0711 18:35:05.549222  7290 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999993
I0711 18:35:05.549232  7290 solver.cpp:540]     Test net output #2: loss = 0.108163 (* 1 = 0.108163 loss)
I0711 18:35:05.747937  7290 solver.cpp:290] Iteration 6000 (1.54863 iter/s, 64.5733s/100 iter), loss = 0.0779569
I0711 18:35:05.747962  7290 solver.cpp:309]     Train net output #0: loss = 0.077957 (* 1 = 0.077957 loss)
I0711 18:35:05.747969  7290 sgd_solver.cpp:106] Iteration 6000, lr = 0.0001
I0711 18:35:11.915148  7386 blocking_queue.cpp:50] Waiting for data
I0711 18:36:29.768244  7290 solver.cpp:290] Iteration 6100 (1.19022 iter/s, 84.018s/100 iter), loss = 0.0386538
I0711 18:36:29.768349  7290 solver.cpp:309]     Train net output #0: loss = 0.0386538 (* 1 = 0.0386538 loss)
I0711 18:36:29.768360  7290 sgd_solver.cpp:106] Iteration 6100, lr = 0.0001
I0711 18:36:30.341492  7352 blocking_queue.cpp:50] Waiting for data
I0711 18:37:31.806093  7386 blocking_queue.cpp:50] Waiting for data
I0711 18:38:29.554322  7290 solver.cpp:290] Iteration 6200 (0.834845 iter/s, 119.783s/100 iter), loss = 0.0413135
I0711 18:38:29.554482  7290 solver.cpp:309]     Train net output #0: loss = 0.0413135 (* 1 = 0.0413135 loss)
I0711 18:38:29.554496  7290 sgd_solver.cpp:106] Iteration 6200, lr = 0.0001
I0711 18:38:55.955407  7483 blocking_queue.cpp:50] Waiting for data
I0711 18:39:11.683820  7290 solver.cpp:290] Iteration 6300 (2.37371 iter/s, 42.1282s/100 iter), loss = 0.112893
I0711 18:39:11.683868  7290 solver.cpp:309]     Train net output #0: loss = 0.112893 (* 1 = 0.112893 loss)
I0711 18:39:11.683879  7290 sgd_solver.cpp:106] Iteration 6300, lr = 0.0001
I0711 18:39:28.657593  7290 solver.cpp:290] Iteration 6400 (5.89162 iter/s, 16.9733s/100 iter), loss = 0.0554843
I0711 18:39:28.657615  7290 solver.cpp:309]     Train net output #0: loss = 0.0554843 (* 1 = 0.0554843 loss)
I0711 18:39:28.657624  7290 sgd_solver.cpp:106] Iteration 6400, lr = 0.0001
I0711 18:39:45.569495  7290 solver.cpp:290] Iteration 6500 (5.91317 iter/s, 16.9114s/100 iter), loss = 0.0666582
I0711 18:39:45.569581  7290 solver.cpp:309]     Train net output #0: loss = 0.0666583 (* 1 = 0.0666583 loss)
I0711 18:39:45.569589  7290 sgd_solver.cpp:106] Iteration 6500, lr = 0.0001
I0711 18:40:02.475806  7290 solver.cpp:290] Iteration 6600 (5.91514 iter/s, 16.9058s/100 iter), loss = 0.0294992
I0711 18:40:02.475831  7290 solver.cpp:309]     Train net output #0: loss = 0.0294992 (* 1 = 0.0294992 loss)
I0711 18:40:02.475837  7290 sgd_solver.cpp:106] Iteration 6600, lr = 0.0001
I0711 18:40:19.459666  7290 solver.cpp:290] Iteration 6700 (5.88812 iter/s, 16.9834s/100 iter), loss = 0.0326719
I0711 18:40:19.459756  7290 solver.cpp:309]     Train net output #0: loss = 0.0326719 (* 1 = 0.0326719 loss)
I0711 18:40:19.459767  7290 sgd_solver.cpp:106] Iteration 6700, lr = 0.0001
I0711 18:40:36.476174  7290 solver.cpp:290] Iteration 6800 (5.87684 iter/s, 17.0159s/100 iter), loss = 0.0317981
I0711 18:40:36.476202  7290 solver.cpp:309]     Train net output #0: loss = 0.0317981 (* 1 = 0.0317981 loss)
I0711 18:40:36.476209  7290 sgd_solver.cpp:106] Iteration 6800, lr = 0.0001
I0711 18:40:53.448981  7290 solver.cpp:290] Iteration 6900 (5.89195 iter/s, 16.9723s/100 iter), loss = 0.0472939
I0711 18:40:53.449028  7290 solver.cpp:309]     Train net output #0: loss = 0.047294 (* 1 = 0.047294 loss)
I0711 18:40:53.449036  7290 sgd_solver.cpp:106] Iteration 6900, lr = 0.0001
I0711 18:41:10.555889  7290 solver.cpp:290] Iteration 7000 (5.84577 iter/s, 17.1064s/100 iter), loss = 0.0335818
I0711 18:41:10.555912  7290 solver.cpp:309]     Train net output #0: loss = 0.0335818 (* 1 = 0.0335818 loss)
I0711 18:41:10.555919  7290 sgd_solver.cpp:106] Iteration 7000, lr = 0.0001
I0711 18:41:27.511945  7290 solver.cpp:290] Iteration 7100 (5.89777 iter/s, 16.9556s/100 iter), loss = 0.053921
I0711 18:41:27.512018  7290 solver.cpp:309]     Train net output #0: loss = 0.053921 (* 1 = 0.053921 loss)
I0711 18:41:27.512027  7290 sgd_solver.cpp:106] Iteration 7100, lr = 0.0001
I0711 18:41:44.455235  7290 solver.cpp:290] Iteration 7200 (5.90223 iter/s, 16.9427s/100 iter), loss = 0.0283452
I0711 18:41:44.455257  7290 solver.cpp:309]     Train net output #0: loss = 0.0283453 (* 1 = 0.0283453 loss)
I0711 18:41:44.455265  7290 sgd_solver.cpp:106] Iteration 7200, lr = 0.0001
I0711 18:42:01.452533  7290 solver.cpp:290] Iteration 7300 (5.88346 iter/s, 16.9968s/100 iter), loss = 0.0512436
I0711 18:42:01.452589  7290 solver.cpp:309]     Train net output #0: loss = 0.0512437 (* 1 = 0.0512437 loss)
I0711 18:42:01.452596  7290 sgd_solver.cpp:106] Iteration 7300, lr = 0.0001
I0711 18:42:18.427023  7290 solver.cpp:290] Iteration 7400 (5.89138 iter/s, 16.974s/100 iter), loss = 0.0561066
I0711 18:42:18.427048  7290 solver.cpp:309]     Train net output #0: loss = 0.0561067 (* 1 = 0.0561067 loss)
I0711 18:42:18.427055  7290 sgd_solver.cpp:106] Iteration 7400, lr = 0.0001
I0711 18:42:35.504536  7290 solver.cpp:290] Iteration 7500 (5.85582 iter/s, 17.077s/100 iter), loss = 0.0699209
I0711 18:42:35.504622  7290 solver.cpp:309]     Train net output #0: loss = 0.069921 (* 1 = 0.069921 loss)
I0711 18:42:35.504633  7290 sgd_solver.cpp:106] Iteration 7500, lr = 0.0001
I0711 18:42:52.580734  7290 solver.cpp:290] Iteration 7600 (5.8563 iter/s, 17.0756s/100 iter), loss = 0.0475304
I0711 18:42:52.580759  7290 solver.cpp:309]     Train net output #0: loss = 0.0475305 (* 1 = 0.0475305 loss)
I0711 18:42:52.580765  7290 sgd_solver.cpp:106] Iteration 7600, lr = 0.0001
I0711 18:43:09.450033  7290 solver.cpp:290] Iteration 7700 (5.9281 iter/s, 16.8688s/100 iter), loss = 0.0303552
I0711 18:43:09.450131  7290 solver.cpp:309]     Train net output #0: loss = 0.0303552 (* 1 = 0.0303552 loss)
I0711 18:43:09.450142  7290 sgd_solver.cpp:106] Iteration 7700, lr = 0.0001
I0711 18:43:26.474130  7290 solver.cpp:290] Iteration 7800 (5.87422 iter/s, 17.0235s/100 iter), loss = 0.0236738
I0711 18:43:26.474155  7290 solver.cpp:309]     Train net output #0: loss = 0.0236738 (* 1 = 0.0236738 loss)
I0711 18:43:26.474164  7290 sgd_solver.cpp:106] Iteration 7800, lr = 0.0001
I0711 18:43:43.414438  7290 solver.cpp:290] Iteration 7900 (5.90325 iter/s, 16.9398s/100 iter), loss = 0.0648853
I0711 18:43:43.414491  7290 solver.cpp:309]     Train net output #0: loss = 0.0648853 (* 1 = 0.0648853 loss)
I0711 18:43:43.414502  7290 sgd_solver.cpp:106] Iteration 7900, lr = 0.0001
I0711 18:44:00.305789  7290 solver.cpp:467] Iteration 8000, Testing net (#0)
I0711 18:44:47.584740  7290 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.943554
I0711 18:44:47.584822  7290 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999988
I0711 18:44:47.584831  7290 solver.cpp:540]     Test net output #2: loss = 0.125779 (* 1 = 0.125779 loss)
I0711 18:44:47.772753  7290 solver.cpp:290] Iteration 8000 (1.55384 iter/s, 64.3565s/100 iter), loss = 0.0304079
I0711 18:44:47.772775  7290 solver.cpp:309]     Train net output #0: loss = 0.0304079 (* 1 = 0.0304079 loss)
I0711 18:44:47.772783  7290 sgd_solver.cpp:106] Iteration 8000, lr = 0.0001
I0711 18:45:04.611014  7290 solver.cpp:290] Iteration 8100 (5.93904 iter/s, 16.8377s/100 iter), loss = 0.0566438
I0711 18:45:04.611040  7290 solver.cpp:309]     Train net output #0: loss = 0.0566437 (* 1 = 0.0566437 loss)
I0711 18:45:04.611050  7290 sgd_solver.cpp:106] Iteration 8100, lr = 0.0001
I0711 18:46:11.175535  7386 blocking_queue.cpp:50] Waiting for data
I0711 18:46:58.808794  7290 solver.cpp:290] Iteration 8200 (0.875698 iter/s, 114.195s/100 iter), loss = 0.0456689
I0711 18:46:58.808909  7290 solver.cpp:309]     Train net output #0: loss = 0.0456688 (* 1 = 0.0456688 loss)
I0711 18:46:58.808919  7290 sgd_solver.cpp:106] Iteration 8200, lr = 0.0001
I0711 18:47:18.937459  7500 blocking_queue.cpp:50] Data layer prefetch queue empty
I0711 18:47:22.143247  7483 blocking_queue.cpp:50] Waiting for data
I0711 18:47:32.561826  7290 solver.cpp:290] Iteration 8300 (2.96279 iter/s, 33.752s/100 iter), loss = 0.044689
I0711 18:47:32.561909  7290 solver.cpp:309]     Train net output #0: loss = 0.044689 (* 1 = 0.044689 loss)
I0711 18:47:32.561918  7290 sgd_solver.cpp:106] Iteration 8300, lr = 0.0001
I0711 18:48:02.594121  7290 solver.cpp:290] Iteration 8400 (3.32985 iter/s, 30.0314s/100 iter), loss = 0.0327974
I0711 18:48:02.594204  7290 solver.cpp:309]     Train net output #0: loss = 0.0327974 (* 1 = 0.0327974 loss)
I0711 18:48:02.594216  7290 sgd_solver.cpp:106] Iteration 8400, lr = 0.0001
I0711 18:48:19.599619  7290 solver.cpp:290] Iteration 8500 (5.88065 iter/s, 17.0049s/100 iter), loss = 0.0686541
I0711 18:48:19.599645  7290 solver.cpp:309]     Train net output #0: loss = 0.0686541 (* 1 = 0.0686541 loss)
I0711 18:48:19.599654  7290 sgd_solver.cpp:106] Iteration 8500, lr = 0.0001
I0711 18:48:36.708510  7290 solver.cpp:290] Iteration 8600 (5.84509 iter/s, 17.1084s/100 iter), loss = 0.0541645
I0711 18:48:36.708590  7290 solver.cpp:309]     Train net output #0: loss = 0.0541645 (* 1 = 0.0541645 loss)
I0711 18:48:36.708598  7290 sgd_solver.cpp:106] Iteration 8600, lr = 0.0001
I0711 18:48:53.695282  7290 solver.cpp:290] Iteration 8700 (5.88713 iter/s, 16.9862s/100 iter), loss = 0.0362202
I0711 18:48:53.695307  7290 solver.cpp:309]     Train net output #0: loss = 0.0362202 (* 1 = 0.0362202 loss)
I0711 18:48:53.695314  7290 sgd_solver.cpp:106] Iteration 8700, lr = 0.0001
I0711 18:49:10.646250  7290 solver.cpp:290] Iteration 8800 (5.89954 iter/s, 16.9505s/100 iter), loss = 0.0544917
I0711 18:49:10.646297  7290 solver.cpp:309]     Train net output #0: loss = 0.0544917 (* 1 = 0.0544917 loss)
I0711 18:49:10.646304  7290 sgd_solver.cpp:106] Iteration 8800, lr = 0.0001
I0711 18:49:27.501307  7290 solver.cpp:290] Iteration 8900 (5.93312 iter/s, 16.8545s/100 iter), loss = 0.0355642
I0711 18:49:27.501332  7290 solver.cpp:309]     Train net output #0: loss = 0.0355642 (* 1 = 0.0355642 loss)
I0711 18:49:27.501340  7290 sgd_solver.cpp:106] Iteration 8900, lr = 0.0001
I0711 18:49:44.494953  7290 solver.cpp:290] Iteration 9000 (5.88473 iter/s, 16.9931s/100 iter), loss = 0.044753
I0711 18:49:44.495002  7290 solver.cpp:309]     Train net output #0: loss = 0.0447529 (* 1 = 0.0447529 loss)
I0711 18:49:44.495008  7290 sgd_solver.cpp:106] Iteration 9000, lr = 0.0001
I0711 18:50:01.441182  7290 solver.cpp:290] Iteration 9100 (5.9012 iter/s, 16.9457s/100 iter), loss = 0.0780392
I0711 18:50:01.441210  7290 solver.cpp:309]     Train net output #0: loss = 0.0780391 (* 1 = 0.0780391 loss)
I0711 18:50:01.441223  7290 sgd_solver.cpp:106] Iteration 9100, lr = 0.0001
I0711 18:50:18.506731  7290 solver.cpp:290] Iteration 9200 (5.85993 iter/s, 17.065s/100 iter), loss = 0.0463734
I0711 18:50:18.506784  7290 solver.cpp:309]     Train net output #0: loss = 0.0463733 (* 1 = 0.0463733 loss)
I0711 18:50:18.506794  7290 sgd_solver.cpp:106] Iteration 9200, lr = 0.0001
I0711 18:50:35.500071  7290 solver.cpp:290] Iteration 9300 (5.88484 iter/s, 16.9928s/100 iter), loss = 0.0507376
I0711 18:50:35.500094  7290 solver.cpp:309]     Train net output #0: loss = 0.0507376 (* 1 = 0.0507376 loss)
I0711 18:50:35.500102  7290 sgd_solver.cpp:106] Iteration 9300, lr = 0.0001
I0711 18:50:52.413705  7290 solver.cpp:290] Iteration 9400 (5.91256 iter/s, 16.9131s/100 iter), loss = 0.0234703
I0711 18:50:52.413748  7290 solver.cpp:309]     Train net output #0: loss = 0.0234702 (* 1 = 0.0234702 loss)
I0711 18:50:52.413755  7290 sgd_solver.cpp:106] Iteration 9400, lr = 0.0001
I0711 18:51:09.377997  7290 solver.cpp:290] Iteration 9500 (5.89491 iter/s, 16.9638s/100 iter), loss = 0.0452537
I0711 18:51:09.378020  7290 solver.cpp:309]     Train net output #0: loss = 0.0452536 (* 1 = 0.0452536 loss)
I0711 18:51:09.378027  7290 sgd_solver.cpp:106] Iteration 9500, lr = 0.0001
I0711 18:51:26.420608  7290 solver.cpp:290] Iteration 9600 (5.86782 iter/s, 17.0421s/100 iter), loss = 0.0357792
I0711 18:51:26.420702  7290 solver.cpp:309]     Train net output #0: loss = 0.0357791 (* 1 = 0.0357791 loss)
I0711 18:51:26.420711  7290 sgd_solver.cpp:106] Iteration 9600, lr = 0.0001
I0711 18:51:43.368444  7290 solver.cpp:290] Iteration 9700 (5.90066 iter/s, 16.9473s/100 iter), loss = 0.0367929
I0711 18:51:43.368468  7290 solver.cpp:309]     Train net output #0: loss = 0.0367928 (* 1 = 0.0367928 loss)
I0711 18:51:43.368476  7290 sgd_solver.cpp:106] Iteration 9700, lr = 0.0001
I0711 18:52:00.429651  7290 solver.cpp:290] Iteration 9800 (5.86142 iter/s, 17.0607s/100 iter), loss = 0.0436817
I0711 18:52:00.429759  7290 solver.cpp:309]     Train net output #0: loss = 0.0436816 (* 1 = 0.0436816 loss)
I0711 18:52:00.429774  7290 sgd_solver.cpp:106] Iteration 9800, lr = 0.0001
I0711 18:52:17.483789  7290 solver.cpp:290] Iteration 9900 (5.86388 iter/s, 17.0536s/100 iter), loss = 0.0265108
I0711 18:52:17.483814  7290 solver.cpp:309]     Train net output #0: loss = 0.0265107 (* 1 = 0.0265107 loss)
I0711 18:52:17.483820  7290 sgd_solver.cpp:106] Iteration 9900, lr = 0.0001
I0711 18:52:34.475661  7290 solver.cpp:594] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/initial/cityscapes5_jsegnet21v2_iter_10000.caffemodel
I0711 18:52:34.588274  7290 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/initial/cityscapes5_jsegnet21v2_iter_10000.solverstate
I0711 18:52:34.605295  7290 solver.cpp:467] Iteration 10000, Testing net (#0)
I0711 18:53:22.090937  7290 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.942409
I0711 18:53:22.091028  7290 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999882
I0711 18:53:22.091251  7290 solver.cpp:540]     Test net output #2: loss = 0.130371 (* 1 = 0.130371 loss)
I0711 18:53:22.290850  7290 solver.cpp:290] Iteration 10000 (1.54309 iter/s, 64.8052s/100 iter), loss = 0.0235543
I0711 18:53:22.290873  7290 solver.cpp:309]     Train net output #0: loss = 0.0235542 (* 1 = 0.0235542 loss)
I0711 18:53:22.290879  7290 sgd_solver.cpp:106] Iteration 10000, lr = 0.0001
I0711 18:53:27.004294  7352 blocking_queue.cpp:50] Waiting for data
I0711 18:53:59.253542  7290 solver.cpp:290] Iteration 10100 (2.70551 iter/s, 36.9616s/100 iter), loss = 0.0429007
I0711 18:53:59.253594  7290 solver.cpp:309]     Train net output #0: loss = 0.0429006 (* 1 = 0.0429006 loss)
I0711 18:53:59.253602  7290 sgd_solver.cpp:106] Iteration 10100, lr = 0.0001
I0711 18:55:05.272697  7290 solver.cpp:290] Iteration 10200 (1.51476 iter/s, 66.0173s/100 iter), loss = 0.0252992
I0711 18:55:05.272781  7290 solver.cpp:309]     Train net output #0: loss = 0.0252991 (* 1 = 0.0252991 loss)
I0711 18:55:05.272792  7290 sgd_solver.cpp:106] Iteration 10200, lr = 0.0001
I0711 18:55:24.860239  7352 blocking_queue.cpp:50] Waiting for data
I0711 18:55:43.813117  7290 solver.cpp:290] Iteration 10300 (2.59476 iter/s, 38.5393s/100 iter), loss = 0.0324675
I0711 18:55:43.813179  7290 solver.cpp:309]     Train net output #0: loss = 0.0324674 (* 1 = 0.0324674 loss)
I0711 18:55:43.813189  7290 sgd_solver.cpp:106] Iteration 10300, lr = 0.0001
I0711 18:56:00.670248  7290 solver.cpp:290] Iteration 10400 (5.9324 iter/s, 16.8566s/100 iter), loss = 0.0603028
I0711 18:56:00.670271  7290 solver.cpp:309]     Train net output #0: loss = 0.0603027 (* 1 = 0.0603027 loss)
I0711 18:56:00.670279  7290 sgd_solver.cpp:106] Iteration 10400, lr = 0.0001
I0711 18:56:17.605761  7290 solver.cpp:290] Iteration 10500 (5.90493 iter/s, 16.935s/100 iter), loss = 0.0141168
I0711 18:56:17.605855  7290 solver.cpp:309]     Train net output #0: loss = 0.0141167 (* 1 = 0.0141167 loss)
I0711 18:56:17.605866  7290 sgd_solver.cpp:106] Iteration 10500, lr = 0.0001
I0711 18:56:34.632040  7290 solver.cpp:290] Iteration 10600 (5.87347 iter/s, 17.0257s/100 iter), loss = 0.0363722
I0711 18:56:34.632063  7290 solver.cpp:309]     Train net output #0: loss = 0.0363721 (* 1 = 0.0363721 loss)
I0711 18:56:34.632069  7290 sgd_solver.cpp:106] Iteration 10600, lr = 0.0001
I0711 18:56:51.594962  7290 solver.cpp:290] Iteration 10700 (5.89539 iter/s, 16.9624s/100 iter), loss = 0.0425913
I0711 18:56:51.595026  7290 solver.cpp:309]     Train net output #0: loss = 0.0425912 (* 1 = 0.0425912 loss)
I0711 18:56:51.595034  7290 sgd_solver.cpp:106] Iteration 10700, lr = 0.0001
I0711 18:57:08.538167  7290 solver.cpp:290] Iteration 10800 (5.90226 iter/s, 16.9427s/100 iter), loss = 0.0332228
I0711 18:57:08.538189  7290 solver.cpp:309]     Train net output #0: loss = 0.0332227 (* 1 = 0.0332227 loss)
I0711 18:57:08.538198  7290 sgd_solver.cpp:106] Iteration 10800, lr = 0.0001
I0711 18:57:25.583931  7290 solver.cpp:290] Iteration 10900 (5.86673 iter/s, 17.0453s/100 iter), loss = 0.0313951
I0711 18:57:25.584004  7290 solver.cpp:309]     Train net output #0: loss = 0.031395 (* 1 = 0.031395 loss)
I0711 18:57:25.584012  7290 sgd_solver.cpp:106] Iteration 10900, lr = 0.0001
I0711 18:57:42.390424  7290 solver.cpp:290] Iteration 11000 (5.95028 iter/s, 16.8059s/100 iter), loss = 0.0361319
I0711 18:57:42.390450  7290 solver.cpp:309]     Train net output #0: loss = 0.0361318 (* 1 = 0.0361318 loss)
I0711 18:57:42.390460  7290 sgd_solver.cpp:106] Iteration 11000, lr = 0.0001
I0711 18:57:59.271373  7290 solver.cpp:290] Iteration 11100 (5.92401 iter/s, 16.8804s/100 iter), loss = 0.0214206
I0711 18:57:59.271455  7290 solver.cpp:309]     Train net output #0: loss = 0.0214205 (* 1 = 0.0214205 loss)
I0711 18:57:59.271466  7290 sgd_solver.cpp:106] Iteration 11100, lr = 0.0001
I0711 18:58:16.262204  7290 solver.cpp:290] Iteration 11200 (5.88572 iter/s, 16.9903s/100 iter), loss = 0.0429011
I0711 18:58:16.262228  7290 solver.cpp:309]     Train net output #0: loss = 0.042901 (* 1 = 0.042901 loss)
I0711 18:58:16.262235  7290 sgd_solver.cpp:106] Iteration 11200, lr = 0.0001
I0711 18:58:33.408268  7290 solver.cpp:290] Iteration 11300 (5.83241 iter/s, 17.1456s/100 iter), loss = 0.0315334
I0711 18:58:33.408321  7290 solver.cpp:309]     Train net output #0: loss = 0.0315333 (* 1 = 0.0315333 loss)
I0711 18:58:33.408332  7290 sgd_solver.cpp:106] Iteration 11300, lr = 0.0001
I0711 18:58:50.253156  7290 solver.cpp:290] Iteration 11400 (5.9367 iter/s, 16.8444s/100 iter), loss = 0.0293833
I0711 18:58:50.253183  7290 solver.cpp:309]     Train net output #0: loss = 0.0293832 (* 1 = 0.0293832 loss)
I0711 18:58:50.253192  7290 sgd_solver.cpp:106] Iteration 11400, lr = 0.0001
I0711 18:59:07.335412  7290 solver.cpp:290] Iteration 11500 (5.8542 iter/s, 17.0818s/100 iter), loss = 0.0335265
I0711 18:59:07.335463  7290 solver.cpp:309]     Train net output #0: loss = 0.0335264 (* 1 = 0.0335264 loss)
I0711 18:59:07.335471  7290 sgd_solver.cpp:106] Iteration 11500, lr = 0.0001
I0711 18:59:24.418406  7290 solver.cpp:290] Iteration 11600 (5.85396 iter/s, 17.0825s/100 iter), loss = 0.0417331
I0711 18:59:24.418434  7290 solver.cpp:309]     Train net output #0: loss = 0.041733 (* 1 = 0.041733 loss)
I0711 18:59:24.418444  7290 sgd_solver.cpp:106] Iteration 11600, lr = 0.0001
I0711 18:59:41.589856  7290 solver.cpp:290] Iteration 11700 (5.82379 iter/s, 17.1709s/100 iter), loss = 0.0253136
I0711 18:59:41.589929  7290 solver.cpp:309]     Train net output #0: loss = 0.0253135 (* 1 = 0.0253135 loss)
I0711 18:59:41.589938  7290 sgd_solver.cpp:106] Iteration 11700, lr = 0.0001
I0711 18:59:58.756314  7290 solver.cpp:290] Iteration 11800 (5.8255 iter/s, 17.1659s/100 iter), loss = 0.0228712
I0711 18:59:58.756337  7290 solver.cpp:309]     Train net output #0: loss = 0.0228711 (* 1 = 0.0228711 loss)
I0711 18:59:58.756345  7290 sgd_solver.cpp:106] Iteration 11800, lr = 0.0001
I0711 19:00:15.961663  7290 solver.cpp:290] Iteration 11900 (5.81232 iter/s, 17.2048s/100 iter), loss = 0.0338853
I0711 19:00:15.961751  7290 solver.cpp:309]     Train net output #0: loss = 0.0338852 (* 1 = 0.0338852 loss)
I0711 19:00:15.961760  7290 sgd_solver.cpp:106] Iteration 11900, lr = 0.0001
I0711 19:00:32.983909  7290 solver.cpp:467] Iteration 12000, Testing net (#0)
I0711 19:01:20.673041  7290 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.950261
I0711 19:01:20.673152  7290 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999462
I0711 19:01:20.673161  7290 solver.cpp:540]     Test net output #2: loss = 0.1332 (* 1 = 0.1332 loss)
I0711 19:01:20.851073  7290 solver.cpp:290] Iteration 12000 (1.54113 iter/s, 64.8875s/100 iter), loss = 0.0305318
I0711 19:01:20.851097  7290 solver.cpp:309]     Train net output #0: loss = 0.0305317 (* 1 = 0.0305317 loss)
I0711 19:01:20.851104  7290 sgd_solver.cpp:106] Iteration 12000, lr = 0.0001
I0711 19:01:48.844015  7290 solver.cpp:290] Iteration 12100 (3.57243 iter/s, 27.9921s/100 iter), loss = 0.0338263
I0711 19:01:48.844039  7290 solver.cpp:309]     Train net output #0: loss = 0.0338262 (* 1 = 0.0338262 loss)
I0711 19:01:48.844046  7290 sgd_solver.cpp:106] Iteration 12100, lr = 0.0001
I0711 19:02:14.409940  7352 blocking_queue.cpp:50] Waiting for data
I0711 19:02:39.786128  7290 solver.cpp:290] Iteration 12200 (1.96307 iter/s, 50.9407s/100 iter), loss = 0.0226129
I0711 19:02:39.786159  7290 solver.cpp:309]     Train net output #0: loss = 0.0226128 (* 1 = 0.0226128 loss)
I0711 19:02:39.786166  7290 sgd_solver.cpp:106] Iteration 12200, lr = 0.0001
I0711 19:02:58.149039  7386 blocking_queue.cpp:50] Waiting for data
I0711 19:03:05.875540  7290 solver.cpp:290] Iteration 12300 (3.83308 iter/s, 26.0886s/100 iter), loss = 0.0626612
I0711 19:03:05.875562  7290 solver.cpp:309]     Train net output #0: loss = 0.0626611 (* 1 = 0.0626611 loss)
I0711 19:03:05.875569  7290 sgd_solver.cpp:106] Iteration 12300, lr = 0.0001
I0711 19:03:22.723865  7290 solver.cpp:290] Iteration 12400 (5.93548 iter/s, 16.8478s/100 iter), loss = 0.043537
I0711 19:03:22.723888  7290 solver.cpp:309]     Train net output #0: loss = 0.0435369 (* 1 = 0.0435369 loss)
I0711 19:03:22.723894  7290 sgd_solver.cpp:106] Iteration 12400, lr = 0.0001
I0711 19:03:39.719332  7290 solver.cpp:290] Iteration 12500 (5.8841 iter/s, 16.995s/100 iter), loss = 0.025833
I0711 19:03:39.719405  7290 solver.cpp:309]     Train net output #0: loss = 0.0258329 (* 1 = 0.0258329 loss)
I0711 19:03:39.719413  7290 sgd_solver.cpp:106] Iteration 12500, lr = 0.0001
I0711 19:03:56.822793  7290 solver.cpp:290] Iteration 12600 (5.84696 iter/s, 17.1029s/100 iter), loss = 0.0276784
I0711 19:03:56.822815  7290 solver.cpp:309]     Train net output #0: loss = 0.0276783 (* 1 = 0.0276783 loss)
I0711 19:03:56.822823  7290 sgd_solver.cpp:106] Iteration 12600, lr = 0.0001
I0711 19:04:13.992703  7290 solver.cpp:290] Iteration 12700 (5.82431 iter/s, 17.1694s/100 iter), loss = 0.0371779
I0711 19:04:13.992754  7290 solver.cpp:309]     Train net output #0: loss = 0.0371778 (* 1 = 0.0371778 loss)
I0711 19:04:13.992763  7290 sgd_solver.cpp:106] Iteration 12700, lr = 0.0001
I0711 19:04:31.153731  7290 solver.cpp:290] Iteration 12800 (5.82734 iter/s, 17.1605s/100 iter), loss = 0.0233727
I0711 19:04:31.153755  7290 solver.cpp:309]     Train net output #0: loss = 0.0233726 (* 1 = 0.0233726 loss)
I0711 19:04:31.153761  7290 sgd_solver.cpp:106] Iteration 12800, lr = 0.0001
I0711 19:04:48.066968  7290 solver.cpp:290] Iteration 12900 (5.9127 iter/s, 16.9127s/100 iter), loss = 0.0360042
I0711 19:04:48.067052  7290 solver.cpp:309]     Train net output #0: loss = 0.0360041 (* 1 = 0.0360041 loss)
I0711 19:04:48.067062  7290 sgd_solver.cpp:106] Iteration 12900, lr = 0.0001
I0711 19:05:05.114923  7290 solver.cpp:290] Iteration 13000 (5.866 iter/s, 17.0474s/100 iter), loss = 0.0422174
I0711 19:05:05.114946  7290 solver.cpp:309]     Train net output #0: loss = 0.0422173 (* 1 = 0.0422173 loss)
I0711 19:05:05.114953  7290 sgd_solver.cpp:106] Iteration 13000, lr = 0.0001
I0711 19:05:22.182996  7290 solver.cpp:290] Iteration 13100 (5.85907 iter/s, 17.0676s/100 iter), loss = 0.0287268
I0711 19:05:22.183049  7290 solver.cpp:309]     Train net output #0: loss = 0.0287267 (* 1 = 0.0287267 loss)
I0711 19:05:22.183063  7290 sgd_solver.cpp:106] Iteration 13100, lr = 0.0001
I0711 19:05:39.401485  7290 solver.cpp:290] Iteration 13200 (5.80789 iter/s, 17.218s/100 iter), loss = 0.0301324
I0711 19:05:39.401512  7290 solver.cpp:309]     Train net output #0: loss = 0.0301323 (* 1 = 0.0301323 loss)
I0711 19:05:39.401520  7290 sgd_solver.cpp:106] Iteration 13200, lr = 0.0001
I0711 19:05:56.508700  7290 solver.cpp:290] Iteration 13300 (5.84566 iter/s, 17.1067s/100 iter), loss = 0.0452536
I0711 19:05:56.508796  7290 solver.cpp:309]     Train net output #0: loss = 0.0452535 (* 1 = 0.0452535 loss)
I0711 19:05:56.508805  7290 sgd_solver.cpp:106] Iteration 13300, lr = 0.0001
I0711 19:06:13.485415  7290 solver.cpp:290] Iteration 13400 (5.89062 iter/s, 16.9761s/100 iter), loss = 0.035885
I0711 19:06:13.485440  7290 solver.cpp:309]     Train net output #0: loss = 0.0358849 (* 1 = 0.0358849 loss)
I0711 19:06:13.485445  7290 sgd_solver.cpp:106] Iteration 13400, lr = 0.0001
I0711 19:06:30.596299  7290 solver.cpp:290] Iteration 13500 (5.84441 iter/s, 17.1104s/100 iter), loss = 0.0337028
I0711 19:06:30.596379  7290 solver.cpp:309]     Train net output #0: loss = 0.0337027 (* 1 = 0.0337027 loss)
I0711 19:06:30.596390  7290 sgd_solver.cpp:106] Iteration 13500, lr = 0.0001
I0711 19:06:47.456398  7290 solver.cpp:290] Iteration 13600 (5.93136 iter/s, 16.8595s/100 iter), loss = 0.0252978
I0711 19:06:47.456421  7290 solver.cpp:309]     Train net output #0: loss = 0.0252977 (* 1 = 0.0252977 loss)
I0711 19:06:47.456435  7290 sgd_solver.cpp:106] Iteration 13600, lr = 0.0001
I0711 19:07:04.400682  7290 solver.cpp:290] Iteration 13700 (5.90187 iter/s, 16.9438s/100 iter), loss = 0.0398415
I0711 19:07:04.400775  7290 solver.cpp:309]     Train net output #0: loss = 0.0398414 (* 1 = 0.0398414 loss)
I0711 19:07:04.400784  7290 sgd_solver.cpp:106] Iteration 13700, lr = 0.0001
I0711 19:07:21.365521  7290 solver.cpp:290] Iteration 13800 (5.89474 iter/s, 16.9643s/100 iter), loss = 0.0289731
I0711 19:07:21.365548  7290 solver.cpp:309]     Train net output #0: loss = 0.028973 (* 1 = 0.028973 loss)
I0711 19:07:21.365557  7290 sgd_solver.cpp:106] Iteration 13800, lr = 0.0001
I0711 19:07:38.513283  7290 solver.cpp:290] Iteration 13900 (5.83184 iter/s, 17.1472s/100 iter), loss = 0.0289237
I0711 19:07:38.513380  7290 solver.cpp:309]     Train net output #0: loss = 0.0289236 (* 1 = 0.0289236 loss)
I0711 19:07:38.513391  7290 sgd_solver.cpp:106] Iteration 13900, lr = 0.0001
I0711 19:07:55.247089  7290 solver.cpp:467] Iteration 14000, Testing net (#0)
I0711 19:08:42.540029  7290 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.951701
I0711 19:08:42.540122  7290 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999927
I0711 19:08:42.540128  7290 solver.cpp:540]     Test net output #2: loss = 0.110516 (* 1 = 0.110516 loss)
I0711 19:08:42.734617  7290 solver.cpp:290] Iteration 14000 (1.55716 iter/s, 64.2194s/100 iter), loss = 0.030904
I0711 19:08:42.734647  7290 solver.cpp:309]     Train net output #0: loss = 0.0309039 (* 1 = 0.0309039 loss)
I0711 19:08:42.734655  7290 sgd_solver.cpp:106] Iteration 14000, lr = 0.0001
I0711 19:09:06.193305  7290 solver.cpp:290] Iteration 14100 (4.26294 iter/s, 23.458s/100 iter), loss = 0.0250245
I0711 19:09:06.193330  7290 solver.cpp:309]     Train net output #0: loss = 0.0250244 (* 1 = 0.0250244 loss)
I0711 19:09:06.193336  7290 sgd_solver.cpp:106] Iteration 14100, lr = 0.0001
I0711 19:09:27.841598  7290 solver.cpp:290] Iteration 14200 (4.61944 iter/s, 21.6477s/100 iter), loss = 0.0280597
I0711 19:09:27.841645  7290 solver.cpp:309]     Train net output #0: loss = 0.0280596 (* 1 = 0.0280596 loss)
I0711 19:09:27.841652  7290 sgd_solver.cpp:106] Iteration 14200, lr = 0.0001
I0711 19:09:44.851824  7290 solver.cpp:290] Iteration 14300 (5.879 iter/s, 17.0097s/100 iter), loss = 0.0383565
I0711 19:09:44.851846  7290 solver.cpp:309]     Train net output #0: loss = 0.0383564 (* 1 = 0.0383564 loss)
I0711 19:09:44.851853  7290 sgd_solver.cpp:106] Iteration 14300, lr = 0.0001
I0711 19:10:01.820013  7290 solver.cpp:290] Iteration 14400 (5.89356 iter/s, 16.9677s/100 iter), loss = 0.0206324
I0711 19:10:01.820086  7290 solver.cpp:309]     Train net output #0: loss = 0.0206323 (* 1 = 0.0206323 loss)
I0711 19:10:01.820094  7290 sgd_solver.cpp:106] Iteration 14400, lr = 0.0001
I0711 19:10:18.828466  7290 solver.cpp:290] Iteration 14500 (5.87962 iter/s, 17.0079s/100 iter), loss = 0.0442858
I0711 19:10:18.828490  7290 solver.cpp:309]     Train net output #0: loss = 0.0442858 (* 1 = 0.0442858 loss)
I0711 19:10:18.828496  7290 sgd_solver.cpp:106] Iteration 14500, lr = 0.0001
I0711 19:10:35.779430  7290 solver.cpp:290] Iteration 14600 (5.89955 iter/s, 16.9505s/100 iter), loss = 0.0305562
I0711 19:10:35.779537  7290 solver.cpp:309]     Train net output #0: loss = 0.0305561 (* 1 = 0.0305561 loss)
I0711 19:10:35.779548  7290 sgd_solver.cpp:106] Iteration 14600, lr = 0.0001
I0711 19:10:52.750440  7290 solver.cpp:290] Iteration 14700 (5.8926 iter/s, 16.9704s/100 iter), loss = 0.0911707
I0711 19:10:52.750468  7290 solver.cpp:309]     Train net output #0: loss = 0.0911706 (* 1 = 0.0911706 loss)
I0711 19:10:52.750476  7290 sgd_solver.cpp:106] Iteration 14700, lr = 0.0001
I0711 19:11:09.754436  7290 solver.cpp:290] Iteration 14800 (5.88115 iter/s, 17.0035s/100 iter), loss = 0.0420324
I0711 19:11:09.754518  7290 solver.cpp:309]     Train net output #0: loss = 0.0420323 (* 1 = 0.0420323 loss)
I0711 19:11:09.754528  7290 sgd_solver.cpp:106] Iteration 14800, lr = 0.0001
I0711 19:11:26.805960  7290 solver.cpp:290] Iteration 14900 (5.86477 iter/s, 17.051s/100 iter), loss = 0.0508641
I0711 19:11:26.805984  7290 solver.cpp:309]     Train net output #0: loss = 0.050864 (* 1 = 0.050864 loss)
I0711 19:11:26.805991  7290 sgd_solver.cpp:106] Iteration 14900, lr = 0.0001
I0711 19:11:43.800273  7290 solver.cpp:290] Iteration 15000 (5.8845 iter/s, 16.9938s/100 iter), loss = 0.0404286
I0711 19:11:43.800351  7290 solver.cpp:309]     Train net output #0: loss = 0.0404285 (* 1 = 0.0404285 loss)
I0711 19:11:43.800359  7290 sgd_solver.cpp:106] Iteration 15000, lr = 0.0001
I0711 19:12:00.844322  7290 solver.cpp:290] Iteration 15100 (5.86734 iter/s, 17.0435s/100 iter), loss = 0.0198565
I0711 19:12:00.844348  7290 solver.cpp:309]     Train net output #0: loss = 0.0198564 (* 1 = 0.0198564 loss)
I0711 19:12:00.844355  7290 sgd_solver.cpp:106] Iteration 15100, lr = 0.0001
I0711 19:12:17.885655  7290 solver.cpp:290] Iteration 15200 (5.86826 iter/s, 17.0408s/100 iter), loss = 0.0236007
I0711 19:12:17.885728  7290 solver.cpp:309]     Train net output #0: loss = 0.0236006 (* 1 = 0.0236006 loss)
I0711 19:12:17.885735  7290 sgd_solver.cpp:106] Iteration 15200, lr = 0.0001
I0711 19:12:34.967686  7290 solver.cpp:290] Iteration 15300 (5.85429 iter/s, 17.0815s/100 iter), loss = 0.0426724
I0711 19:12:34.967711  7290 solver.cpp:309]     Train net output #0: loss = 0.0426723 (* 1 = 0.0426723 loss)
I0711 19:12:34.967717  7290 sgd_solver.cpp:106] Iteration 15300, lr = 0.0001
I0711 19:12:51.919492  7290 solver.cpp:290] Iteration 15400 (5.89925 iter/s, 16.9513s/100 iter), loss = 0.0388065
I0711 19:12:51.919539  7290 solver.cpp:309]     Train net output #0: loss = 0.0388064 (* 1 = 0.0388064 loss)
I0711 19:12:51.919548  7290 sgd_solver.cpp:106] Iteration 15400, lr = 0.0001
I0711 19:13:08.772083  7290 solver.cpp:290] Iteration 15500 (5.93399 iter/s, 16.8521s/100 iter), loss = 0.0311759
I0711 19:13:08.772104  7290 solver.cpp:309]     Train net output #0: loss = 0.0311758 (* 1 = 0.0311758 loss)
I0711 19:13:08.772111  7290 sgd_solver.cpp:106] Iteration 15500, lr = 0.0001
I0711 19:13:25.811730  7290 solver.cpp:290] Iteration 15600 (5.86884 iter/s, 17.0391s/100 iter), loss = 0.0308214
I0711 19:13:25.811843  7290 solver.cpp:309]     Train net output #0: loss = 0.0308213 (* 1 = 0.0308213 loss)
I0711 19:13:25.811853  7290 sgd_solver.cpp:106] Iteration 15600, lr = 0.0001
I0711 19:13:42.790235  7290 solver.cpp:290] Iteration 15700 (5.89 iter/s, 16.9779s/100 iter), loss = 0.0315022
I0711 19:13:42.790261  7290 solver.cpp:309]     Train net output #0: loss = 0.0315021 (* 1 = 0.0315021 loss)
I0711 19:13:42.790267  7290 sgd_solver.cpp:106] Iteration 15700, lr = 0.0001
I0711 19:13:59.628919  7290 solver.cpp:290] Iteration 15800 (5.93888 iter/s, 16.8382s/100 iter), loss = 0.0517453
I0711 19:13:59.628990  7290 solver.cpp:309]     Train net output #0: loss = 0.0517452 (* 1 = 0.0517452 loss)
I0711 19:13:59.628998  7290 sgd_solver.cpp:106] Iteration 15800, lr = 0.0001
I0711 19:14:16.452039  7290 solver.cpp:290] Iteration 15900 (5.94439 iter/s, 16.8226s/100 iter), loss = 0.0326285
I0711 19:14:16.452061  7290 solver.cpp:309]     Train net output #0: loss = 0.0326284 (* 1 = 0.0326284 loss)
I0711 19:14:16.452069  7290 sgd_solver.cpp:106] Iteration 15900, lr = 0.0001
I0711 19:14:33.211323  7290 solver.cpp:467] Iteration 16000, Testing net (#0)
I0711 19:15:20.313905  7290 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.949388
I0711 19:15:20.313982  7290 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999545
I0711 19:15:20.313989  7290 solver.cpp:540]     Test net output #2: loss = 0.13676 (* 1 = 0.13676 loss)
I0711 19:15:20.501101  7290 solver.cpp:290] Iteration 16000 (1.56135 iter/s, 64.0473s/100 iter), loss = 0.0377887
I0711 19:15:20.501129  7290 solver.cpp:309]     Train net output #0: loss = 0.0377886 (* 1 = 0.0377886 loss)
I0711 19:15:20.501138  7290 sgd_solver.cpp:106] Iteration 16000, lr = 0.0001
I0711 19:15:37.522467  7290 solver.cpp:290] Iteration 16100 (5.87514 iter/s, 17.0209s/100 iter), loss = 0.0250812
I0711 19:15:37.522491  7290 solver.cpp:309]     Train net output #0: loss = 0.0250811 (* 1 = 0.0250811 loss)
I0711 19:15:37.522498  7290 sgd_solver.cpp:106] Iteration 16100, lr = 0.0001
I0711 19:16:02.536870  7290 solver.cpp:290] Iteration 16200 (3.99781 iter/s, 25.0137s/100 iter), loss = 0.0516085
I0711 19:16:02.536976  7290 solver.cpp:309]     Train net output #0: loss = 0.0516084 (* 1 = 0.0516084 loss)
I0711 19:16:02.536986  7290 sgd_solver.cpp:106] Iteration 16200, lr = 0.0001
I0711 19:16:21.384667  7290 solver.cpp:290] Iteration 16300 (5.30584 iter/s, 18.8472s/100 iter), loss = 0.0515404
I0711 19:16:21.384688  7290 solver.cpp:309]     Train net output #0: loss = 0.0515403 (* 1 = 0.0515403 loss)
I0711 19:16:21.384696  7290 sgd_solver.cpp:106] Iteration 16300, lr = 0.0001
I0711 19:16:38.688524  7290 solver.cpp:290] Iteration 16400 (5.77923 iter/s, 17.3033s/100 iter), loss = 0.0296192
I0711 19:16:38.688565  7290 solver.cpp:309]     Train net output #0: loss = 0.0296191 (* 1 = 0.0296191 loss)
I0711 19:16:38.688573  7290 sgd_solver.cpp:106] Iteration 16400, lr = 0.0001
I0711 19:16:55.850051  7290 solver.cpp:290] Iteration 16500 (5.82717 iter/s, 17.161s/100 iter), loss = 0.020527
I0711 19:16:55.850081  7290 solver.cpp:309]     Train net output #0: loss = 0.0205269 (* 1 = 0.0205269 loss)
I0711 19:16:55.850090  7290 sgd_solver.cpp:106] Iteration 16500, lr = 0.0001
I0711 19:17:12.997292  7290 solver.cpp:290] Iteration 16600 (5.83202 iter/s, 17.1467s/100 iter), loss = 0.0348751
I0711 19:17:12.997336  7290 solver.cpp:309]     Train net output #0: loss = 0.034875 (* 1 = 0.034875 loss)
I0711 19:17:12.997344  7290 sgd_solver.cpp:106] Iteration 16600, lr = 0.0001
I0711 19:17:30.327029  7290 solver.cpp:290] Iteration 16700 (5.7706 iter/s, 17.3292s/100 iter), loss = 0.0870045
I0711 19:17:30.327054  7290 solver.cpp:309]     Train net output #0: loss = 0.0870044 (* 1 = 0.0870044 loss)
I0711 19:17:30.327061  7290 sgd_solver.cpp:106] Iteration 16700, lr = 0.0001
I0711 19:17:47.779891  7290 solver.cpp:290] Iteration 16800 (5.72989 iter/s, 17.4523s/100 iter), loss = 0.0556285
I0711 19:17:47.779971  7290 solver.cpp:309]     Train net output #0: loss = 0.0556284 (* 1 = 0.0556284 loss)
I0711 19:17:47.779983  7290 sgd_solver.cpp:106] Iteration 16800, lr = 0.0001
I0711 19:18:05.085136  7290 solver.cpp:290] Iteration 16900 (5.77878 iter/s, 17.3047s/100 iter), loss = 0.0249942
I0711 19:18:05.085163  7290 solver.cpp:309]     Train net output #0: loss = 0.0249941 (* 1 = 0.0249941 loss)
I0711 19:18:05.085173  7290 sgd_solver.cpp:106] Iteration 16900, lr = 0.0001
I0711 19:18:22.229784  7290 solver.cpp:290] Iteration 17000 (5.8329 iter/s, 17.1441s/100 iter), loss = 0.0162487
I0711 19:18:22.242244  7290 solver.cpp:309]     Train net output #0: loss = 0.0162485 (* 1 = 0.0162485 loss)
I0711 19:18:22.242283  7290 sgd_solver.cpp:106] Iteration 17000, lr = 0.0001
I0711 19:18:39.726971  7290 solver.cpp:290] Iteration 17100 (5.71943 iter/s, 17.4843s/100 iter), loss = 0.0227745
I0711 19:18:39.727002  7290 solver.cpp:309]     Train net output #0: loss = 0.0227743 (* 1 = 0.0227743 loss)
I0711 19:18:39.727011  7290 sgd_solver.cpp:106] Iteration 17100, lr = 0.0001
I0711 19:18:57.253412  7290 solver.cpp:290] Iteration 17200 (5.70583 iter/s, 17.5259s/100 iter), loss = 0.0266461
I0711 19:18:57.253479  7290 solver.cpp:309]     Train net output #0: loss = 0.026646 (* 1 = 0.026646 loss)
I0711 19:18:57.253487  7290 sgd_solver.cpp:106] Iteration 17200, lr = 0.0001
I0711 19:19:14.610429  7290 solver.cpp:290] Iteration 17300 (5.76154 iter/s, 17.3565s/100 iter), loss = 0.0239469
I0711 19:19:14.610453  7290 solver.cpp:309]     Train net output #0: loss = 0.0239468 (* 1 = 0.0239468 loss)
I0711 19:19:14.610460  7290 sgd_solver.cpp:106] Iteration 17300, lr = 0.0001
I0711 19:19:31.998571  7290 solver.cpp:290] Iteration 17400 (5.75122 iter/s, 17.3876s/100 iter), loss = 0.0250904
I0711 19:19:31.998661  7290 solver.cpp:309]     Train net output #0: loss = 0.0250903 (* 1 = 0.0250903 loss)
I0711 19:19:31.998680  7290 sgd_solver.cpp:106] Iteration 17400, lr = 0.0001
I0711 19:19:49.397598  7290 solver.cpp:290] Iteration 17500 (5.74764 iter/s, 17.3985s/100 iter), loss = 0.118535
I0711 19:19:49.397621  7290 solver.cpp:309]     Train net output #0: loss = 0.118535 (* 1 = 0.118535 loss)
I0711 19:19:49.397629  7290 sgd_solver.cpp:106] Iteration 17500, lr = 0.0001
I0711 19:20:06.372879  7290 solver.cpp:290] Iteration 17600 (5.89109 iter/s, 16.9748s/100 iter), loss = 0.0171973
I0711 19:20:06.372927  7290 solver.cpp:309]     Train net output #0: loss = 0.0171972 (* 1 = 0.0171972 loss)
I0711 19:20:06.372934  7290 sgd_solver.cpp:106] Iteration 17600, lr = 0.0001
I0711 19:20:23.480384  7290 solver.cpp:290] Iteration 17700 (5.84557 iter/s, 17.107s/100 iter), loss = 0.0311948
I0711 19:20:23.480409  7290 solver.cpp:309]     Train net output #0: loss = 0.0311947 (* 1 = 0.0311947 loss)
I0711 19:20:23.480417  7290 sgd_solver.cpp:106] Iteration 17700, lr = 0.0001
I0711 19:20:40.422580  7290 solver.cpp:290] Iteration 17800 (5.9026 iter/s, 16.9417s/100 iter), loss = 0.109196
I0711 19:20:40.422680  7290 solver.cpp:309]     Train net output #0: loss = 0.109196 (* 1 = 0.109196 loss)
I0711 19:20:40.422691  7290 sgd_solver.cpp:106] Iteration 17800, lr = 0.0001
I0711 19:20:57.339088  7290 solver.cpp:290] Iteration 17900 (5.91158 iter/s, 16.9159s/100 iter), loss = 0.114935
I0711 19:20:57.339112  7290 solver.cpp:309]     Train net output #0: loss = 0.114935 (* 1 = 0.114935 loss)
I0711 19:20:57.339118  7290 sgd_solver.cpp:106] Iteration 17900, lr = 0.0001
I0711 19:21:14.168315  7290 solver.cpp:467] Iteration 18000, Testing net (#0)
I0711 19:22:01.098083  7290 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.951948
I0711 19:22:01.098155  7290 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999994
I0711 19:22:01.098160  7290 solver.cpp:540]     Test net output #2: loss = 0.111072 (* 1 = 0.111072 loss)
I0711 19:22:01.298126  7290 solver.cpp:290] Iteration 18000 (1.56354 iter/s, 63.9572s/100 iter), loss = 0.0251778
I0711 19:22:01.298149  7290 solver.cpp:309]     Train net output #0: loss = 0.0251777 (* 1 = 0.0251777 loss)
I0711 19:22:01.298156  7290 sgd_solver.cpp:106] Iteration 18000, lr = 0.0001
I0711 19:22:18.206068  7290 solver.cpp:290] Iteration 18100 (5.91456 iter/s, 16.9074s/100 iter), loss = 0.0361842
I0711 19:22:18.206100  7290 solver.cpp:309]     Train net output #0: loss = 0.0361841 (* 1 = 0.0361841 loss)
I0711 19:22:18.206111  7290 sgd_solver.cpp:106] Iteration 18100, lr = 0.0001
I0711 19:22:35.224308  7290 solver.cpp:290] Iteration 18200 (5.87623 iter/s, 17.0177s/100 iter), loss = 0.0305811
I0711 19:22:35.224380  7290 solver.cpp:309]     Train net output #0: loss = 0.0305811 (* 1 = 0.0305811 loss)
I0711 19:22:35.224388  7290 sgd_solver.cpp:106] Iteration 18200, lr = 0.0001
I0711 19:22:52.202952  7290 solver.cpp:290] Iteration 18300 (5.88994 iter/s, 16.9781s/100 iter), loss = 0.0312113
I0711 19:22:52.202976  7290 solver.cpp:309]     Train net output #0: loss = 0.0312112 (* 1 = 0.0312112 loss)
I0711 19:22:52.202983  7290 sgd_solver.cpp:106] Iteration 18300, lr = 0.0001
I0711 19:23:09.178663  7290 solver.cpp:290] Iteration 18400 (5.89094 iter/s, 16.9752s/100 iter), loss = 0.0362004
I0711 19:23:09.178773  7290 solver.cpp:309]     Train net output #0: loss = 0.0362003 (* 1 = 0.0362003 loss)
I0711 19:23:09.178786  7290 sgd_solver.cpp:106] Iteration 18400, lr = 0.0001
I0711 19:23:26.077397  7290 solver.cpp:290] Iteration 18500 (5.91781 iter/s, 16.8982s/100 iter), loss = 0.0612979
I0711 19:23:26.077419  7290 solver.cpp:309]     Train net output #0: loss = 0.0612979 (* 1 = 0.0612979 loss)
I0711 19:23:26.077426  7290 sgd_solver.cpp:106] Iteration 18500, lr = 0.0001
I0711 19:23:43.056598  7290 solver.cpp:290] Iteration 18600 (5.88973 iter/s, 16.9787s/100 iter), loss = 0.0182906
I0711 19:23:43.056648  7290 solver.cpp:309]     Train net output #0: loss = 0.0182905 (* 1 = 0.0182905 loss)
I0711 19:23:43.056655  7290 sgd_solver.cpp:106] Iteration 18600, lr = 0.0001
I0711 19:24:00.093200  7290 solver.cpp:290] Iteration 18700 (5.8699 iter/s, 17.0361s/100 iter), loss = 0.0482359
I0711 19:24:00.093225  7290 solver.cpp:309]     Train net output #0: loss = 0.0482358 (* 1 = 0.0482358 loss)
I0711 19:24:00.093235  7290 sgd_solver.cpp:106] Iteration 18700, lr = 0.0001
I0711 19:24:17.083343  7290 solver.cpp:290] Iteration 18800 (5.88594 iter/s, 16.9896s/100 iter), loss = 0.020993
I0711 19:24:17.083410  7290 solver.cpp:309]     Train net output #0: loss = 0.0209929 (* 1 = 0.0209929 loss)
I0711 19:24:17.083420  7290 sgd_solver.cpp:106] Iteration 18800, lr = 0.0001
I0711 19:24:34.226048  7290 solver.cpp:290] Iteration 18900 (5.83357 iter/s, 17.1422s/100 iter), loss = 0.028795
I0711 19:24:34.226073  7290 solver.cpp:309]     Train net output #0: loss = 0.0287949 (* 1 = 0.0287949 loss)
I0711 19:24:34.226079  7290 sgd_solver.cpp:106] Iteration 18900, lr = 0.0001
I0711 19:24:51.320755  7290 solver.cpp:290] Iteration 19000 (5.84994 iter/s, 17.0942s/100 iter), loss = 0.0338881
I0711 19:24:51.320838  7290 solver.cpp:309]     Train net output #0: loss = 0.033888 (* 1 = 0.033888 loss)
I0711 19:24:51.320847  7290 sgd_solver.cpp:106] Iteration 19000, lr = 0.0001
I0711 19:25:08.314604  7290 solver.cpp:290] Iteration 19100 (5.88468 iter/s, 16.9933s/100 iter), loss = 0.0310824
I0711 19:25:08.314627  7290 solver.cpp:309]     Train net output #0: loss = 0.0310823 (* 1 = 0.0310823 loss)
I0711 19:25:08.314635  7290 sgd_solver.cpp:106] Iteration 19100, lr = 0.0001
I0711 19:25:25.256896  7290 solver.cpp:290] Iteration 19200 (5.90256 iter/s, 16.9418s/100 iter), loss = 0.0210726
I0711 19:25:25.256948  7290 solver.cpp:309]     Train net output #0: loss = 0.0210725 (* 1 = 0.0210725 loss)
I0711 19:25:25.256956  7290 sgd_solver.cpp:106] Iteration 19200, lr = 0.0001
I0711 19:25:42.256666  7290 solver.cpp:290] Iteration 19300 (5.88261 iter/s, 16.9992s/100 iter), loss = 0.0643411
I0711 19:25:42.256690  7290 solver.cpp:309]     Train net output #0: loss = 0.064341 (* 1 = 0.064341 loss)
I0711 19:25:42.256696  7290 sgd_solver.cpp:106] Iteration 19300, lr = 0.0001
I0711 19:25:59.182636  7290 solver.cpp:290] Iteration 19400 (5.90826 iter/s, 16.9255s/100 iter), loss = 0.0264929
I0711 19:25:59.182683  7290 solver.cpp:309]     Train net output #0: loss = 0.0264928 (* 1 = 0.0264928 loss)
I0711 19:25:59.182692  7290 sgd_solver.cpp:106] Iteration 19400, lr = 0.0001
I0711 19:26:16.226498  7290 solver.cpp:290] Iteration 19500 (5.86739 iter/s, 17.0433s/100 iter), loss = 0.0307169
I0711 19:26:16.226522  7290 solver.cpp:309]     Train net output #0: loss = 0.0307168 (* 1 = 0.0307168 loss)
I0711 19:26:16.226529  7290 sgd_solver.cpp:106] Iteration 19500, lr = 0.0001
I0711 19:26:33.146615  7290 solver.cpp:290] Iteration 19600 (5.9103 iter/s, 16.9196s/100 iter), loss = 0.0170138
I0711 19:26:33.146657  7290 solver.cpp:309]     Train net output #0: loss = 0.0170137 (* 1 = 0.0170137 loss)
I0711 19:26:33.146666  7290 sgd_solver.cpp:106] Iteration 19600, lr = 0.0001
I0711 19:26:50.268736  7290 solver.cpp:290] Iteration 19700 (5.84057 iter/s, 17.1216s/100 iter), loss = 0.0205243
I0711 19:26:50.268760  7290 solver.cpp:309]     Train net output #0: loss = 0.0205242 (* 1 = 0.0205242 loss)
I0711 19:26:50.268767  7290 sgd_solver.cpp:106] Iteration 19700, lr = 0.0001
I0711 19:27:07.209236  7290 solver.cpp:290] Iteration 19800 (5.90319 iter/s, 16.94s/100 iter), loss = 0.0225709
I0711 19:27:07.209295  7290 solver.cpp:309]     Train net output #0: loss = 0.0225708 (* 1 = 0.0225708 loss)
I0711 19:27:07.209303  7290 sgd_solver.cpp:106] Iteration 19800, lr = 0.0001
I0711 19:27:24.384630  7290 solver.cpp:290] Iteration 19900 (5.82246 iter/s, 17.1749s/100 iter), loss = 0.0584642
I0711 19:27:24.384656  7290 solver.cpp:309]     Train net output #0: loss = 0.058464 (* 1 = 0.058464 loss)
I0711 19:27:24.384665  7290 sgd_solver.cpp:106] Iteration 19900, lr = 0.0001
I0711 19:27:41.572367  7290 solver.cpp:594] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/initial/cityscapes5_jsegnet21v2_iter_20000.caffemodel
I0711 19:27:41.701989  7290 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/initial/cityscapes5_jsegnet21v2_iter_20000.solverstate
I0711 19:27:41.724115  7290 solver.cpp:467] Iteration 20000, Testing net (#0)
I0711 19:28:31.487493  7290 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.949127
I0711 19:28:31.487530  7290 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999912
I0711 19:28:31.487536  7290 solver.cpp:540]     Test net output #2: loss = 0.121776 (* 1 = 0.121776 loss)
I0711 19:28:31.680284  7290 solver.cpp:290] Iteration 20000 (1.48602 iter/s, 67.2938s/100 iter), loss = 0.0231253
I0711 19:28:31.680317  7290 solver.cpp:309]     Train net output #0: loss = 0.0231251 (* 1 = 0.0231251 loss)
I0711 19:28:31.680328  7290 sgd_solver.cpp:106] Iteration 20000, lr = 0.0001
I0711 19:28:49.030557  7290 solver.cpp:290] Iteration 20100 (5.76377 iter/s, 17.3498s/100 iter), loss = 0.0380613
I0711 19:28:49.030581  7290 solver.cpp:309]     Train net output #0: loss = 0.0380611 (* 1 = 0.0380611 loss)
I0711 19:28:49.030588  7290 sgd_solver.cpp:106] Iteration 20100, lr = 0.0001
I0711 19:29:06.458914  7290 solver.cpp:290] Iteration 20200 (5.73795 iter/s, 17.4278s/100 iter), loss = 0.0207219
I0711 19:29:06.459056  7290 solver.cpp:309]     Train net output #0: loss = 0.0207217 (* 1 = 0.0207217 loss)
I0711 19:29:06.459111  7290 sgd_solver.cpp:106] Iteration 20200, lr = 0.0001
I0711 19:29:24.028281  7290 solver.cpp:290] Iteration 20300 (5.69193 iter/s, 17.5687s/100 iter), loss = 0.0203983
I0711 19:29:24.028306  7290 solver.cpp:309]     Train net output #0: loss = 0.0203981 (* 1 = 0.0203981 loss)
I0711 19:29:24.028314  7290 sgd_solver.cpp:106] Iteration 20300, lr = 0.0001
I0711 19:29:41.779180  7290 solver.cpp:290] Iteration 20400 (5.63368 iter/s, 17.7504s/100 iter), loss = 0.0602204
I0711 19:29:41.779253  7290 solver.cpp:309]     Train net output #0: loss = 0.0602202 (* 1 = 0.0602202 loss)
I0711 19:29:41.779260  7290 sgd_solver.cpp:106] Iteration 20400, lr = 0.0001
I0711 19:29:59.057377  7290 solver.cpp:290] Iteration 20500 (5.78783 iter/s, 17.2776s/100 iter), loss = 0.039767
I0711 19:29:59.057421  7290 solver.cpp:309]     Train net output #0: loss = 0.0397668 (* 1 = 0.0397668 loss)
I0711 19:29:59.057435  7290 sgd_solver.cpp:106] Iteration 20500, lr = 0.0001
I0711 19:30:16.339249  7290 solver.cpp:290] Iteration 20600 (5.78659 iter/s, 17.2813s/100 iter), loss = 0.0270132
I0711 19:30:16.339504  7290 solver.cpp:309]     Train net output #0: loss = 0.027013 (* 1 = 0.027013 loss)
I0711 19:30:16.339592  7290 sgd_solver.cpp:106] Iteration 20600, lr = 0.0001
I0711 19:30:33.825769  7290 solver.cpp:290] Iteration 20700 (5.71893 iter/s, 17.4858s/100 iter), loss = 0.0428762
I0711 19:30:33.825795  7290 solver.cpp:309]     Train net output #0: loss = 0.042876 (* 1 = 0.042876 loss)
I0711 19:30:33.825804  7290 sgd_solver.cpp:106] Iteration 20700, lr = 0.0001
I0711 19:30:51.238736  7290 solver.cpp:290] Iteration 20800 (5.74302 iter/s, 17.4125s/100 iter), loss = 0.0271103
I0711 19:30:51.238833  7290 solver.cpp:309]     Train net output #0: loss = 0.0271101 (* 1 = 0.0271101 loss)
I0711 19:30:51.238842  7290 sgd_solver.cpp:106] Iteration 20800, lr = 0.0001
I0711 19:31:08.478463  7290 solver.cpp:290] Iteration 20900 (5.80075 iter/s, 17.2391s/100 iter), loss = 0.0215658
I0711 19:31:08.478489  7290 solver.cpp:309]     Train net output #0: loss = 0.0215656 (* 1 = 0.0215656 loss)
I0711 19:31:08.478495  7290 sgd_solver.cpp:106] Iteration 20900, lr = 0.0001
I0711 19:31:25.727005  7290 solver.cpp:290] Iteration 21000 (5.79776 iter/s, 17.248s/100 iter), loss = 0.0434636
I0711 19:31:25.727082  7290 solver.cpp:309]     Train net output #0: loss = 0.0434634 (* 1 = 0.0434634 loss)
I0711 19:31:25.727092  7290 sgd_solver.cpp:106] Iteration 21000, lr = 0.0001
I0711 19:31:43.073173  7290 solver.cpp:290] Iteration 21100 (5.76515 iter/s, 17.3456s/100 iter), loss = 0.0290455
I0711 19:31:43.073197  7290 solver.cpp:309]     Train net output #0: loss = 0.0290453 (* 1 = 0.0290453 loss)
I0711 19:31:43.073206  7290 sgd_solver.cpp:106] Iteration 21100, lr = 0.0001
I0711 19:32:00.467160  7290 solver.cpp:290] Iteration 21200 (5.74928 iter/s, 17.3935s/100 iter), loss = 0.0519431
I0711 19:32:00.467222  7290 solver.cpp:309]     Train net output #0: loss = 0.051943 (* 1 = 0.051943 loss)
I0711 19:32:00.467233  7290 sgd_solver.cpp:106] Iteration 21200, lr = 0.0001
I0711 19:32:18.022115  7290 solver.cpp:290] Iteration 21300 (5.69657 iter/s, 17.5544s/100 iter), loss = 0.0219607
I0711 19:32:18.022141  7290 solver.cpp:309]     Train net output #0: loss = 0.0219605 (* 1 = 0.0219605 loss)
I0711 19:32:18.022151  7290 sgd_solver.cpp:106] Iteration 21300, lr = 0.0001
I0711 19:32:35.372797  7290 solver.cpp:290] Iteration 21400 (5.76363 iter/s, 17.3502s/100 iter), loss = 0.0295505
I0711 19:32:35.372886  7290 solver.cpp:309]     Train net output #0: loss = 0.0295503 (* 1 = 0.0295503 loss)
I0711 19:32:35.372898  7290 sgd_solver.cpp:106] Iteration 21400, lr = 0.0001
I0711 19:32:52.856266  7290 solver.cpp:290] Iteration 21500 (5.71988 iter/s, 17.4829s/100 iter), loss = 0.0381487
I0711 19:32:52.856293  7290 solver.cpp:309]     Train net output #0: loss = 0.0381486 (* 1 = 0.0381486 loss)
I0711 19:32:52.856300  7290 sgd_solver.cpp:106] Iteration 21500, lr = 0.0001
I0711 19:33:10.520392  7290 solver.cpp:290] Iteration 21600 (5.66136 iter/s, 17.6636s/100 iter), loss = 0.0367602
I0711 19:33:10.520498  7290 solver.cpp:309]     Train net output #0: loss = 0.0367601 (* 1 = 0.0367601 loss)
I0711 19:33:10.520510  7290 sgd_solver.cpp:106] Iteration 21600, lr = 0.0001
I0711 19:33:27.939492  7290 solver.cpp:290] Iteration 21700 (5.74102 iter/s, 17.4185s/100 iter), loss = 0.0346573
I0711 19:33:27.939520  7290 solver.cpp:309]     Train net output #0: loss = 0.0346572 (* 1 = 0.0346572 loss)
I0711 19:33:27.939529  7290 sgd_solver.cpp:106] Iteration 21700, lr = 0.0001
I0711 19:33:45.166438  7290 solver.cpp:290] Iteration 21800 (5.80503 iter/s, 17.2264s/100 iter), loss = 0.0177993
I0711 19:33:45.166486  7290 solver.cpp:309]     Train net output #0: loss = 0.0177992 (* 1 = 0.0177992 loss)
I0711 19:33:45.166496  7290 sgd_solver.cpp:106] Iteration 21800, lr = 0.0001
I0711 19:34:02.270303  7290 solver.cpp:290] Iteration 21900 (5.84681 iter/s, 17.1033s/100 iter), loss = 0.018152
I0711 19:34:02.270325  7290 solver.cpp:309]     Train net output #0: loss = 0.0181519 (* 1 = 0.0181519 loss)
I0711 19:34:02.270332  7290 sgd_solver.cpp:106] Iteration 21900, lr = 0.0001
I0711 19:34:19.073936  7290 solver.cpp:467] Iteration 22000, Testing net (#0)
I0711 19:35:06.012209  7290 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.94955
I0711 19:35:06.012300  7290 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999716
I0711 19:35:06.012307  7290 solver.cpp:540]     Test net output #2: loss = 0.127833 (* 1 = 0.127833 loss)
I0711 19:35:06.201530  7290 solver.cpp:290] Iteration 22000 (1.56422 iter/s, 63.9294s/100 iter), loss = 0.0499741
I0711 19:35:06.201555  7290 solver.cpp:309]     Train net output #0: loss = 0.0499739 (* 1 = 0.0499739 loss)
I0711 19:35:06.201562  7290 sgd_solver.cpp:106] Iteration 22000, lr = 0.0001
I0711 19:35:23.312695  7290 solver.cpp:290] Iteration 22100 (5.84431 iter/s, 17.1107s/100 iter), loss = 0.0225639
I0711 19:35:23.312721  7290 solver.cpp:309]     Train net output #0: loss = 0.0225637 (* 1 = 0.0225637 loss)
I0711 19:35:23.312731  7290 sgd_solver.cpp:106] Iteration 22100, lr = 0.0001
I0711 19:35:40.268235  7290 solver.cpp:290] Iteration 22200 (5.89795 iter/s, 16.955s/100 iter), loss = 0.0239346
I0711 19:35:40.268309  7290 solver.cpp:309]     Train net output #0: loss = 0.0239344 (* 1 = 0.0239344 loss)
I0711 19:35:40.268317  7290 sgd_solver.cpp:106] Iteration 22200, lr = 0.0001
I0711 19:35:57.170966  7290 solver.cpp:290] Iteration 22300 (5.9164 iter/s, 16.9022s/100 iter), loss = 0.0333297
I0711 19:35:57.170994  7290 solver.cpp:309]     Train net output #0: loss = 0.0333296 (* 1 = 0.0333296 loss)
I0711 19:35:57.171002  7290 sgd_solver.cpp:106] Iteration 22300, lr = 0.0001
I0711 19:36:14.226254  7290 solver.cpp:290] Iteration 22400 (5.86346 iter/s, 17.0548s/100 iter), loss = 0.024157
I0711 19:36:14.226310  7290 solver.cpp:309]     Train net output #0: loss = 0.0241568 (* 1 = 0.0241568 loss)
I0711 19:36:14.226317  7290 sgd_solver.cpp:106] Iteration 22400, lr = 0.0001
I0711 19:36:31.664942  7290 solver.cpp:290] Iteration 22500 (5.73455 iter/s, 17.4381s/100 iter), loss = 0.0324011
I0711 19:36:31.664964  7290 solver.cpp:309]     Train net output #0: loss = 0.032401 (* 1 = 0.032401 loss)
I0711 19:36:31.664971  7290 sgd_solver.cpp:106] Iteration 22500, lr = 0.0001
I0711 19:36:48.875891  7290 solver.cpp:290] Iteration 22600 (5.81043 iter/s, 17.2104s/100 iter), loss = 0.0364833
I0711 19:36:48.875975  7290 solver.cpp:309]     Train net output #0: loss = 0.0364831 (* 1 = 0.0364831 loss)
I0711 19:36:48.875986  7290 sgd_solver.cpp:106] Iteration 22600, lr = 0.0001
I0711 19:37:06.199823  7290 solver.cpp:290] Iteration 22700 (5.77255 iter/s, 17.3234s/100 iter), loss = 0.0228351
I0711 19:37:06.199851  7290 solver.cpp:309]     Train net output #0: loss = 0.022835 (* 1 = 0.022835 loss)
I0711 19:37:06.199859  7290 sgd_solver.cpp:106] Iteration 22700, lr = 0.0001
I0711 19:37:23.373271  7290 solver.cpp:290] Iteration 22800 (5.82312 iter/s, 17.1729s/100 iter), loss = 0.0260692
I0711 19:37:23.373373  7290 solver.cpp:309]     Train net output #0: loss = 0.026069 (* 1 = 0.026069 loss)
I0711 19:37:23.373399  7290 sgd_solver.cpp:106] Iteration 22800, lr = 0.0001
I0711 19:37:40.744695  7290 solver.cpp:290] Iteration 22900 (5.75677 iter/s, 17.3708s/100 iter), loss = 0.0187004
I0711 19:37:40.744719  7290 solver.cpp:309]     Train net output #0: loss = 0.0187002 (* 1 = 0.0187002 loss)
I0711 19:37:40.744725  7290 sgd_solver.cpp:106] Iteration 22900, lr = 0.0001
I0711 19:37:57.884246  7290 solver.cpp:290] Iteration 23000 (5.83463 iter/s, 17.139s/100 iter), loss = 0.0313216
I0711 19:37:57.884523  7290 solver.cpp:309]     Train net output #0: loss = 0.0313214 (* 1 = 0.0313214 loss)
I0711 19:37:57.884537  7290 sgd_solver.cpp:106] Iteration 23000, lr = 0.0001
I0711 19:38:15.260576  7290 solver.cpp:290] Iteration 23100 (5.75521 iter/s, 17.3756s/100 iter), loss = 0.0274884
I0711 19:38:15.260598  7290 solver.cpp:309]     Train net output #0: loss = 0.0274882 (* 1 = 0.0274882 loss)
I0711 19:38:15.260607  7290 sgd_solver.cpp:106] Iteration 23100, lr = 0.0001
I0711 19:38:32.470405  7290 solver.cpp:290] Iteration 23200 (5.8108 iter/s, 17.2093s/100 iter), loss = 0.0173784
I0711 19:38:32.470502  7290 solver.cpp:309]     Train net output #0: loss = 0.0173783 (* 1 = 0.0173783 loss)
I0711 19:38:32.470523  7290 sgd_solver.cpp:106] Iteration 23200, lr = 0.0001
I0711 19:38:49.655822  7290 solver.cpp:290] Iteration 23300 (5.81908 iter/s, 17.1848s/100 iter), loss = 0.0183244
I0711 19:38:49.655848  7290 solver.cpp:309]     Train net output #0: loss = 0.0183242 (* 1 = 0.0183242 loss)
I0711 19:38:49.655858  7290 sgd_solver.cpp:106] Iteration 23300, lr = 0.0001
I0711 19:39:07.105202  7290 solver.cpp:290] Iteration 23400 (5.73103 iter/s, 17.4489s/100 iter), loss = 0.0314142
I0711 19:39:07.105294  7290 solver.cpp:309]     Train net output #0: loss = 0.0314141 (* 1 = 0.0314141 loss)
I0711 19:39:07.105304  7290 sgd_solver.cpp:106] Iteration 23400, lr = 0.0001
I0711 19:39:24.437666  7290 solver.cpp:290] Iteration 23500 (5.76971 iter/s, 17.3319s/100 iter), loss = 0.0239255
I0711 19:39:24.437690  7290 solver.cpp:309]     Train net output #0: loss = 0.0239253 (* 1 = 0.0239253 loss)
I0711 19:39:24.437696  7290 sgd_solver.cpp:106] Iteration 23500, lr = 0.0001
I0711 19:39:41.527186  7290 solver.cpp:290] Iteration 23600 (5.85171 iter/s, 17.089s/100 iter), loss = 0.207712
I0711 19:39:41.527282  7290 solver.cpp:309]     Train net output #0: loss = 0.207712 (* 1 = 0.207712 loss)
I0711 19:39:41.527293  7290 sgd_solver.cpp:106] Iteration 23600, lr = 0.0001
I0711 19:39:58.883056  7290 solver.cpp:290] Iteration 23700 (5.76193 iter/s, 17.3553s/100 iter), loss = 0.029736
I0711 19:39:58.883080  7290 solver.cpp:309]     Train net output #0: loss = 0.0297358 (* 1 = 0.0297358 loss)
I0711 19:39:58.883086  7290 sgd_solver.cpp:106] Iteration 23700, lr = 0.0001
I0711 19:40:16.365200  7290 solver.cpp:290] Iteration 23800 (5.72029 iter/s, 17.4816s/100 iter), loss = 0.0355305
I0711 19:40:16.365285  7290 solver.cpp:309]     Train net output #0: loss = 0.0355303 (* 1 = 0.0355303 loss)
I0711 19:40:16.365299  7290 sgd_solver.cpp:106] Iteration 23800, lr = 0.0001
I0711 19:40:33.869624  7290 solver.cpp:290] Iteration 23900 (5.71303 iter/s, 17.5039s/100 iter), loss = 0.0231232
I0711 19:40:33.869650  7290 solver.cpp:309]     Train net output #0: loss = 0.023123 (* 1 = 0.023123 loss)
I0711 19:40:33.869658  7290 sgd_solver.cpp:106] Iteration 23900, lr = 0.0001
I0711 19:40:50.805266  7290 solver.cpp:467] Iteration 24000, Testing net (#0)
I0711 19:41:43.372508  7290 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.952738
I0711 19:41:43.372592  7290 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999935
I0711 19:41:43.372599  7290 solver.cpp:540]     Test net output #2: loss = 0.127557 (* 1 = 0.127557 loss)
I0711 19:41:43.561893  7290 solver.cpp:290] Iteration 24000 (1.43492 iter/s, 69.6903s/100 iter), loss = 0.0288483
I0711 19:41:43.561916  7290 solver.cpp:309]     Train net output #0: loss = 0.0288481 (* 1 = 0.0288481 loss)
I0711 19:41:43.561923  7500 sgd_solver.cpp:46] MultiStep Status: Iteration 24000, step = 1
I0711 19:41:43.561928  7501 sgd_solver.cpp:46] MultiStep Status: Iteration 24000, step = 1
I0711 19:41:43.561923  7290 sgd_solver.cpp:46] MultiStep Status: Iteration 24000, step = 1
I0711 19:41:43.561939  7290 sgd_solver.cpp:106] Iteration 24000, lr = 1e-05
I0711 19:42:00.759773  7290 solver.cpp:290] Iteration 24100 (5.81484 iter/s, 17.1974s/100 iter), loss = 0.0192892
I0711 19:42:00.759799  7290 solver.cpp:309]     Train net output #0: loss = 0.0192891 (* 1 = 0.0192891 loss)
I0711 19:42:00.759809  7290 sgd_solver.cpp:106] Iteration 24100, lr = 1e-05
I0711 19:42:18.460419  7290 solver.cpp:290] Iteration 24200 (5.64968 iter/s, 17.7001s/100 iter), loss = 0.0470148
I0711 19:42:18.460458  7290 solver.cpp:309]     Train net output #0: loss = 0.0470146 (* 1 = 0.0470146 loss)
I0711 19:42:18.460465  7290 sgd_solver.cpp:106] Iteration 24200, lr = 1e-05
I0711 19:42:35.967129  7290 solver.cpp:290] Iteration 24300 (5.71227 iter/s, 17.5062s/100 iter), loss = 0.0294049
I0711 19:42:35.967156  7290 solver.cpp:309]     Train net output #0: loss = 0.0294048 (* 1 = 0.0294048 loss)
I0711 19:42:35.967165  7290 sgd_solver.cpp:106] Iteration 24300, lr = 1e-05
I0711 19:42:53.296622  7290 solver.cpp:290] Iteration 24400 (5.77068 iter/s, 17.329s/100 iter), loss = 0.0174914
I0711 19:42:53.296669  7290 solver.cpp:309]     Train net output #0: loss = 0.0174912 (* 1 = 0.0174912 loss)
I0711 19:42:53.296679  7290 sgd_solver.cpp:106] Iteration 24400, lr = 1e-05
I0711 19:43:10.440153  7290 solver.cpp:290] Iteration 24500 (5.83328 iter/s, 17.143s/100 iter), loss = 0.0237801
I0711 19:43:10.440176  7290 solver.cpp:309]     Train net output #0: loss = 0.0237799 (* 1 = 0.0237799 loss)
I0711 19:43:10.440184  7290 sgd_solver.cpp:106] Iteration 24500, lr = 1e-05
I0711 19:43:27.358557  7290 solver.cpp:290] Iteration 24600 (5.91089 iter/s, 16.9179s/100 iter), loss = 0.0303642
I0711 19:43:27.358629  7290 solver.cpp:309]     Train net output #0: loss = 0.030364 (* 1 = 0.030364 loss)
I0711 19:43:27.358638  7290 sgd_solver.cpp:106] Iteration 24600, lr = 1e-05
I0711 19:43:44.396653  7290 solver.cpp:290] Iteration 24700 (5.86939 iter/s, 17.0376s/100 iter), loss = 0.0200329
I0711 19:43:44.396675  7290 solver.cpp:309]     Train net output #0: loss = 0.0200327 (* 1 = 0.0200327 loss)
I0711 19:43:44.396683  7290 sgd_solver.cpp:106] Iteration 24700, lr = 1e-05
I0711 19:44:01.413066  7290 solver.cpp:290] Iteration 24800 (5.87685 iter/s, 17.0159s/100 iter), loss = 0.0199048
I0711 19:44:01.413147  7290 solver.cpp:309]     Train net output #0: loss = 0.0199047 (* 1 = 0.0199047 loss)
I0711 19:44:01.413159  7290 sgd_solver.cpp:106] Iteration 24800, lr = 1e-05
I0711 19:44:18.519798  7290 solver.cpp:290] Iteration 24900 (5.84584 iter/s, 17.1062s/100 iter), loss = 0.0383152
I0711 19:44:18.519827  7290 solver.cpp:309]     Train net output #0: loss = 0.038315 (* 1 = 0.038315 loss)
I0711 19:44:18.519836  7290 sgd_solver.cpp:106] Iteration 24900, lr = 1e-05
I0711 19:44:35.580029  7290 solver.cpp:290] Iteration 25000 (5.86176 iter/s, 17.0597s/100 iter), loss = 0.0270356
I0711 19:44:35.580101  7290 solver.cpp:309]     Train net output #0: loss = 0.0270355 (* 1 = 0.0270355 loss)
I0711 19:44:35.580111  7290 sgd_solver.cpp:106] Iteration 25000, lr = 1e-05
I0711 19:44:52.499197  7290 solver.cpp:290] Iteration 25100 (5.91064 iter/s, 16.9186s/100 iter), loss = 0.022068
I0711 19:44:52.499220  7290 solver.cpp:309]     Train net output #0: loss = 0.0220679 (* 1 = 0.0220679 loss)
I0711 19:44:52.499227  7290 sgd_solver.cpp:106] Iteration 25100, lr = 1e-05
I0711 19:45:09.356052  7290 solver.cpp:290] Iteration 25200 (5.93248 iter/s, 16.8564s/100 iter), loss = 0.0549388
I0711 19:45:09.356098  7290 solver.cpp:309]     Train net output #0: loss = 0.0549386 (* 1 = 0.0549386 loss)
I0711 19:45:09.356107  7290 sgd_solver.cpp:106] Iteration 25200, lr = 1e-05
I0711 19:45:26.216164  7290 solver.cpp:290] Iteration 25300 (5.93134 iter/s, 16.8596s/100 iter), loss = 0.0219621
I0711 19:45:26.216188  7290 solver.cpp:309]     Train net output #0: loss = 0.0219619 (* 1 = 0.0219619 loss)
I0711 19:45:26.216197  7290 sgd_solver.cpp:106] Iteration 25300, lr = 1e-05
I0711 19:45:43.615453  7290 solver.cpp:290] Iteration 25400 (5.74753 iter/s, 17.3988s/100 iter), loss = 0.0228791
I0711 19:45:43.615559  7290 solver.cpp:309]     Train net output #0: loss = 0.0228789 (* 1 = 0.0228789 loss)
I0711 19:45:43.615586  7290 sgd_solver.cpp:106] Iteration 25400, lr = 1e-05
I0711 19:46:01.032178  7290 solver.cpp:290] Iteration 25500 (5.7418 iter/s, 17.4161s/100 iter), loss = 0.0234289
I0711 19:46:01.032223  7290 solver.cpp:309]     Train net output #0: loss = 0.0234287 (* 1 = 0.0234287 loss)
I0711 19:46:01.032248  7290 sgd_solver.cpp:106] Iteration 25500, lr = 1e-05
I0711 19:46:18.520123  7290 solver.cpp:290] Iteration 25600 (5.7184 iter/s, 17.4874s/100 iter), loss = 0.0325431
I0711 19:46:18.520207  7290 solver.cpp:309]     Train net output #0: loss = 0.032543 (* 1 = 0.032543 loss)
I0711 19:46:18.520233  7290 sgd_solver.cpp:106] Iteration 25600, lr = 1e-05
I0711 19:46:35.837332  7290 solver.cpp:290] Iteration 25700 (5.77479 iter/s, 17.3167s/100 iter), loss = 0.0203291
I0711 19:46:35.837357  7290 solver.cpp:309]     Train net output #0: loss = 0.0203289 (* 1 = 0.0203289 loss)
I0711 19:46:35.837366  7290 sgd_solver.cpp:106] Iteration 25700, lr = 1e-05
I0711 19:46:53.218660  7290 solver.cpp:290] Iteration 25800 (5.75347 iter/s, 17.3808s/100 iter), loss = 0.0349839
I0711 19:46:53.218777  7290 solver.cpp:309]     Train net output #0: loss = 0.0349837 (* 1 = 0.0349837 loss)
I0711 19:46:53.218794  7290 sgd_solver.cpp:106] Iteration 25800, lr = 1e-05
I0711 19:47:10.728687  7290 solver.cpp:290] Iteration 25900 (5.71121 iter/s, 17.5094s/100 iter), loss = 0.018033
I0711 19:47:10.728718  7290 solver.cpp:309]     Train net output #0: loss = 0.0180328 (* 1 = 0.0180328 loss)
I0711 19:47:10.728724  7290 sgd_solver.cpp:106] Iteration 25900, lr = 1e-05
I0711 19:47:28.111424  7290 solver.cpp:467] Iteration 26000, Testing net (#0)
I0711 19:48:19.762604  7290 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.952551
I0711 19:48:19.762665  7290 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999744
I0711 19:48:19.762673  7290 solver.cpp:540]     Test net output #2: loss = 0.140247 (* 1 = 0.140247 loss)
I0711 19:48:19.964718  7290 solver.cpp:290] Iteration 26000 (1.44437 iter/s, 69.2341s/100 iter), loss = 0.0231964
I0711 19:48:19.964743  7290 solver.cpp:309]     Train net output #0: loss = 0.0231963 (* 1 = 0.0231963 loss)
I0711 19:48:19.964761  7290 sgd_solver.cpp:106] Iteration 26000, lr = 1e-05
I0711 19:48:37.329900  7290 solver.cpp:290] Iteration 26100 (5.75882 iter/s, 17.3647s/100 iter), loss = 0.0202824
I0711 19:48:37.329924  7290 solver.cpp:309]     Train net output #0: loss = 0.0202822 (* 1 = 0.0202822 loss)
I0711 19:48:37.329931  7290 sgd_solver.cpp:106] Iteration 26100, lr = 1e-05
I0711 19:48:54.680801  7290 solver.cpp:290] Iteration 26200 (5.76356 iter/s, 17.3504s/100 iter), loss = 0.0286325
I0711 19:48:54.680843  7290 solver.cpp:309]     Train net output #0: loss = 0.0286323 (* 1 = 0.0286323 loss)
I0711 19:48:54.680852  7290 sgd_solver.cpp:106] Iteration 26200, lr = 1e-05
I0711 19:49:12.069919  7290 solver.cpp:290] Iteration 26300 (5.7509 iter/s, 17.3886s/100 iter), loss = 0.023574
I0711 19:49:12.069942  7290 solver.cpp:309]     Train net output #0: loss = 0.0235738 (* 1 = 0.0235738 loss)
I0711 19:49:12.069949  7290 sgd_solver.cpp:106] Iteration 26300, lr = 1e-05
I0711 19:49:29.667327  7290 solver.cpp:290] Iteration 26400 (5.68282 iter/s, 17.5969s/100 iter), loss = 0.0225688
I0711 19:49:29.667430  7290 solver.cpp:309]     Train net output #0: loss = 0.0225686 (* 1 = 0.0225686 loss)
I0711 19:49:29.667454  7290 sgd_solver.cpp:106] Iteration 26400, lr = 1e-05
I0711 19:49:47.191818  7290 solver.cpp:290] Iteration 26500 (5.70649 iter/s, 17.5239s/100 iter), loss = 0.0325137
I0711 19:49:47.191840  7290 solver.cpp:309]     Train net output #0: loss = 0.0325135 (* 1 = 0.0325135 loss)
I0711 19:49:47.191848  7290 sgd_solver.cpp:106] Iteration 26500, lr = 1e-05
I0711 19:50:04.452292  7290 solver.cpp:290] Iteration 26600 (5.79375 iter/s, 17.26s/100 iter), loss = 0.0385882
I0711 19:50:04.452407  7290 solver.cpp:309]     Train net output #0: loss = 0.0385881 (* 1 = 0.0385881 loss)
I0711 19:50:04.452419  7290 sgd_solver.cpp:106] Iteration 26600, lr = 1e-05
I0711 19:50:21.786654  7290 solver.cpp:290] Iteration 26700 (5.76909 iter/s, 17.3338s/100 iter), loss = 0.0205319
I0711 19:50:21.786705  7290 solver.cpp:309]     Train net output #0: loss = 0.0205318 (* 1 = 0.0205318 loss)
I0711 19:50:21.786725  7290 sgd_solver.cpp:106] Iteration 26700, lr = 1e-05
I0711 19:50:39.051020  7290 solver.cpp:290] Iteration 26800 (5.79245 iter/s, 17.2638s/100 iter), loss = 0.0331906
I0711 19:50:39.051120  7290 solver.cpp:309]     Train net output #0: loss = 0.0331904 (* 1 = 0.0331904 loss)
I0711 19:50:39.051142  7290 sgd_solver.cpp:106] Iteration 26800, lr = 1e-05
I0711 19:50:56.463695  7290 solver.cpp:290] Iteration 26900 (5.74313 iter/s, 17.4121s/100 iter), loss = 0.0300695
I0711 19:50:56.463717  7290 solver.cpp:309]     Train net output #0: loss = 0.0300693 (* 1 = 0.0300693 loss)
I0711 19:50:56.463724  7290 sgd_solver.cpp:106] Iteration 26900, lr = 1e-05
I0711 19:51:13.900424  7290 solver.cpp:290] Iteration 27000 (5.73518 iter/s, 17.4362s/100 iter), loss = 0.0170511
I0711 19:51:13.900466  7290 solver.cpp:309]     Train net output #0: loss = 0.017051 (* 1 = 0.017051 loss)
I0711 19:51:13.900475  7290 sgd_solver.cpp:106] Iteration 27000, lr = 1e-05
I0711 19:51:31.208564  7290 solver.cpp:290] Iteration 27100 (5.7778 iter/s, 17.3076s/100 iter), loss = 0.0170417
I0711 19:51:31.208588  7290 solver.cpp:309]     Train net output #0: loss = 0.0170415 (* 1 = 0.0170415 loss)
I0711 19:51:31.208595  7290 sgd_solver.cpp:106] Iteration 27100, lr = 1e-05
I0711 19:51:48.410617  7290 solver.cpp:290] Iteration 27200 (5.81343 iter/s, 17.2016s/100 iter), loss = 0.0219093
I0711 19:51:48.410742  7290 solver.cpp:309]     Train net output #0: loss = 0.0219092 (* 1 = 0.0219092 loss)
I0711 19:51:48.410753  7290 sgd_solver.cpp:106] Iteration 27200, lr = 1e-05
I0711 19:52:05.743335  7290 solver.cpp:290] Iteration 27300 (5.76964 iter/s, 17.3321s/100 iter), loss = 0.0336758
I0711 19:52:05.743413  7290 solver.cpp:309]     Train net output #0: loss = 0.0336756 (* 1 = 0.0336756 loss)
I0711 19:52:05.743433  7290 sgd_solver.cpp:106] Iteration 27300, lr = 1e-05
I0711 19:52:22.879385  7290 solver.cpp:290] Iteration 27400 (5.83584 iter/s, 17.1355s/100 iter), loss = 0.0193283
I0711 19:52:22.879436  7290 solver.cpp:309]     Train net output #0: loss = 0.0193282 (* 1 = 0.0193282 loss)
I0711 19:52:22.879443  7290 sgd_solver.cpp:106] Iteration 27400, lr = 1e-05
I0711 19:52:40.217723  7290 solver.cpp:290] Iteration 27500 (5.76774 iter/s, 17.3378s/100 iter), loss = 0.0213278
I0711 19:52:40.217746  7290 solver.cpp:309]     Train net output #0: loss = 0.0213277 (* 1 = 0.0213277 loss)
I0711 19:52:40.217752  7290 sgd_solver.cpp:106] Iteration 27500, lr = 1e-05
I0711 19:52:57.580207  7290 solver.cpp:290] Iteration 27600 (5.75971 iter/s, 17.362s/100 iter), loss = 0.0253669
I0711 19:52:57.580286  7290 solver.cpp:309]     Train net output #0: loss = 0.0253667 (* 1 = 0.0253667 loss)
I0711 19:52:57.580297  7290 sgd_solver.cpp:106] Iteration 27600, lr = 1e-05
I0711 19:53:14.943109  7290 solver.cpp:290] Iteration 27700 (5.75959 iter/s, 17.3623s/100 iter), loss = 0.0391308
I0711 19:53:14.943146  7290 solver.cpp:309]     Train net output #0: loss = 0.0391307 (* 1 = 0.0391307 loss)
I0711 19:53:14.943156  7290 sgd_solver.cpp:106] Iteration 27700, lr = 1e-05
I0711 19:53:32.197372  7290 solver.cpp:290] Iteration 27800 (5.79584 iter/s, 17.2537s/100 iter), loss = 0.0331955
I0711 19:53:32.197458  7290 solver.cpp:309]     Train net output #0: loss = 0.0331953 (* 1 = 0.0331953 loss)
I0711 19:53:32.197485  7290 sgd_solver.cpp:106] Iteration 27800, lr = 1e-05
I0711 19:53:49.376638  7290 solver.cpp:290] Iteration 27900 (5.82116 iter/s, 17.1787s/100 iter), loss = 0.0396285
I0711 19:53:49.376662  7290 solver.cpp:309]     Train net output #0: loss = 0.0396283 (* 1 = 0.0396283 loss)
I0711 19:53:49.376668  7290 sgd_solver.cpp:106] Iteration 27900, lr = 1e-05
I0711 19:54:06.647390  7290 solver.cpp:467] Iteration 28000, Testing net (#0)
I0711 19:54:56.690570  7290 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.95381
I0711 19:54:56.690656  7290 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999792
I0711 19:54:56.690663  7290 solver.cpp:540]     Test net output #2: loss = 0.139063 (* 1 = 0.139063 loss)
I0711 19:54:56.882596  7290 solver.cpp:290] Iteration 28000 (1.48139 iter/s, 67.5041s/100 iter), loss = 0.0203176
I0711 19:54:56.882621  7290 solver.cpp:309]     Train net output #0: loss = 0.0203174 (* 1 = 0.0203174 loss)
I0711 19:54:56.882627  7290 sgd_solver.cpp:106] Iteration 28000, lr = 1e-05
I0711 19:55:14.095139  7290 solver.cpp:290] Iteration 28100 (5.80989 iter/s, 17.212s/100 iter), loss = 0.0412571
I0711 19:55:14.095170  7290 solver.cpp:309]     Train net output #0: loss = 0.041257 (* 1 = 0.041257 loss)
I0711 19:55:14.095180  7290 sgd_solver.cpp:106] Iteration 28100, lr = 1e-05
I0711 19:55:31.608945  7290 solver.cpp:290] Iteration 28200 (5.70995 iter/s, 17.5133s/100 iter), loss = 0.0258932
I0711 19:55:31.609027  7290 solver.cpp:309]     Train net output #0: loss = 0.025893 (* 1 = 0.025893 loss)
I0711 19:55:31.609037  7290 sgd_solver.cpp:106] Iteration 28200, lr = 1e-05
I0711 19:55:48.789770  7290 solver.cpp:290] Iteration 28300 (5.82063 iter/s, 17.1803s/100 iter), loss = 0.0240568
I0711 19:55:48.789795  7290 solver.cpp:309]     Train net output #0: loss = 0.0240567 (* 1 = 0.0240567 loss)
I0711 19:55:48.789804  7290 sgd_solver.cpp:106] Iteration 28300, lr = 1e-05
I0711 19:56:05.817935  7290 solver.cpp:290] Iteration 28400 (5.87279 iter/s, 17.0277s/100 iter), loss = 0.0193142
I0711 19:56:05.818006  7290 solver.cpp:309]     Train net output #0: loss = 0.019314 (* 1 = 0.019314 loss)
I0711 19:56:05.818013  7290 sgd_solver.cpp:106] Iteration 28400, lr = 1e-05
I0711 19:56:23.206086  7290 solver.cpp:290] Iteration 28500 (5.75122 iter/s, 17.3876s/100 iter), loss = 0.0204719
I0711 19:56:23.206110  7290 solver.cpp:309]     Train net output #0: loss = 0.0204717 (* 1 = 0.0204717 loss)
I0711 19:56:23.206118  7290 sgd_solver.cpp:106] Iteration 28500, lr = 1e-05
I0711 19:56:40.123849  7290 solver.cpp:290] Iteration 28600 (5.91112 iter/s, 16.9173s/100 iter), loss = 0.0233298
I0711 19:56:40.123893  7290 solver.cpp:309]     Train net output #0: loss = 0.0233296 (* 1 = 0.0233296 loss)
I0711 19:56:40.123900  7290 sgd_solver.cpp:106] Iteration 28600, lr = 1e-05
I0711 19:56:57.295857  7290 solver.cpp:290] Iteration 28700 (5.82361 iter/s, 17.1715s/100 iter), loss = 0.0295925
I0711 19:56:57.295882  7290 solver.cpp:309]     Train net output #0: loss = 0.0295923 (* 1 = 0.0295923 loss)
I0711 19:56:57.295889  7290 sgd_solver.cpp:106] Iteration 28700, lr = 1e-05
I0711 19:57:14.397913  7290 solver.cpp:290] Iteration 28800 (5.84742 iter/s, 17.1016s/100 iter), loss = 0.0153141
I0711 19:57:14.398020  7290 solver.cpp:309]     Train net output #0: loss = 0.015314 (* 1 = 0.015314 loss)
I0711 19:57:14.398028  7290 sgd_solver.cpp:106] Iteration 28800, lr = 1e-05
I0711 19:57:31.482259  7290 solver.cpp:290] Iteration 28900 (5.85351 iter/s, 17.0838s/100 iter), loss = 0.0234288
I0711 19:57:31.482282  7290 solver.cpp:309]     Train net output #0: loss = 0.0234286 (* 1 = 0.0234286 loss)
I0711 19:57:31.482290  7290 sgd_solver.cpp:106] Iteration 28900, lr = 1e-05
I0711 19:57:48.407840  7290 solver.cpp:290] Iteration 29000 (5.90839 iter/s, 16.9251s/100 iter), loss = 0.022881
I0711 19:57:48.407918  7290 solver.cpp:309]     Train net output #0: loss = 0.0228809 (* 1 = 0.0228809 loss)
I0711 19:57:48.407928  7290 sgd_solver.cpp:106] Iteration 29000, lr = 1e-05
I0711 19:58:05.428635  7290 solver.cpp:290] Iteration 29100 (5.87535 iter/s, 17.0203s/100 iter), loss = 0.0163629
I0711 19:58:05.428659  7290 solver.cpp:309]     Train net output #0: loss = 0.0163628 (* 1 = 0.0163628 loss)
I0711 19:58:05.428666  7290 sgd_solver.cpp:106] Iteration 29100, lr = 1e-05
I0711 19:58:22.598071  7290 solver.cpp:290] Iteration 29200 (5.82447 iter/s, 17.1689s/100 iter), loss = 0.0145334
I0711 19:58:22.598152  7290 solver.cpp:309]     Train net output #0: loss = 0.0145332 (* 1 = 0.0145332 loss)
I0711 19:58:22.598163  7290 sgd_solver.cpp:106] Iteration 29200, lr = 1e-05
I0711 19:58:39.551285  7290 solver.cpp:290] Iteration 29300 (5.89878 iter/s, 16.9527s/100 iter), loss = 0.0389174
I0711 19:58:39.551312  7290 solver.cpp:309]     Train net output #0: loss = 0.0389173 (* 1 = 0.0389173 loss)
I0711 19:58:39.551321  7290 sgd_solver.cpp:106] Iteration 29300, lr = 1e-05
I0711 19:58:56.494639  7290 solver.cpp:290] Iteration 29400 (5.90219 iter/s, 16.9429s/100 iter), loss = 0.0316219
I0711 19:58:56.494719  7290 solver.cpp:309]     Train net output #0: loss = 0.0316218 (* 1 = 0.0316218 loss)
I0711 19:58:56.494730  7290 sgd_solver.cpp:106] Iteration 29400, lr = 1e-05
I0711 19:59:13.515385  7290 solver.cpp:290] Iteration 29500 (5.87537 iter/s, 17.0202s/100 iter), loss = 0.0215387
I0711 19:59:13.515411  7290 solver.cpp:309]     Train net output #0: loss = 0.0215386 (* 1 = 0.0215386 loss)
I0711 19:59:13.515419  7290 sgd_solver.cpp:106] Iteration 29500, lr = 1e-05
I0711 19:59:30.494977  7290 solver.cpp:290] Iteration 29600 (5.88959 iter/s, 16.9791s/100 iter), loss = 0.0430063
I0711 19:59:30.495046  7290 solver.cpp:309]     Train net output #0: loss = 0.0430062 (* 1 = 0.0430062 loss)
I0711 19:59:30.495059  7290 sgd_solver.cpp:106] Iteration 29600, lr = 1e-05
I0711 19:59:48.083304  7290 solver.cpp:290] Iteration 29700 (5.68577 iter/s, 17.5878s/100 iter), loss = 0.0363303
I0711 19:59:48.083328  7290 solver.cpp:309]     Train net output #0: loss = 0.0363302 (* 1 = 0.0363302 loss)
I0711 19:59:48.083333  7290 sgd_solver.cpp:106] Iteration 29700, lr = 1e-05
I0711 20:00:05.494956  7290 solver.cpp:290] Iteration 29800 (5.74345 iter/s, 17.4111s/100 iter), loss = 0.0204105
I0711 20:00:05.495033  7290 solver.cpp:309]     Train net output #0: loss = 0.0204103 (* 1 = 0.0204103 loss)
I0711 20:00:05.495043  7290 sgd_solver.cpp:106] Iteration 29800, lr = 1e-05
I0711 20:00:22.537140  7290 solver.cpp:290] Iteration 29900 (5.86798 iter/s, 17.0416s/100 iter), loss = 0.0248608
I0711 20:00:22.537164  7290 solver.cpp:309]     Train net output #0: loss = 0.0248606 (* 1 = 0.0248606 loss)
I0711 20:00:22.537170  7290 sgd_solver.cpp:106] Iteration 29900, lr = 1e-05
I0711 20:00:39.348532  7290 solver.cpp:594] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/initial/cityscapes5_jsegnet21v2_iter_30000.caffemodel
I0711 20:00:39.400977  7290 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/initial/cityscapes5_jsegnet21v2_iter_30000.solverstate
I0711 20:00:39.419348  7290 solver.cpp:467] Iteration 30000, Testing net (#0)
I0711 20:01:26.286561  7290 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.954205
I0711 20:01:26.286628  7290 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999786
I0711 20:01:26.286636  7290 solver.cpp:540]     Test net output #2: loss = 0.137828 (* 1 = 0.137828 loss)
I0711 20:01:26.473192  7290 solver.cpp:290] Iteration 30000 (1.56411 iter/s, 63.9343s/100 iter), loss = 0.0173026
I0711 20:01:26.473217  7290 solver.cpp:309]     Train net output #0: loss = 0.0173025 (* 1 = 0.0173025 loss)
I0711 20:01:26.473225  7290 sgd_solver.cpp:106] Iteration 30000, lr = 1e-05
I0711 20:01:43.510619  7290 solver.cpp:290] Iteration 30100 (5.8696 iter/s, 17.0369s/100 iter), loss = 0.0291802
I0711 20:01:43.510643  7290 solver.cpp:309]     Train net output #0: loss = 0.02918 (* 1 = 0.02918 loss)
I0711 20:01:43.510649  7290 sgd_solver.cpp:106] Iteration 30100, lr = 1e-05
I0711 20:02:00.428903  7290 solver.cpp:290] Iteration 30200 (5.91094 iter/s, 16.9178s/100 iter), loss = 0.0445827
I0711 20:02:00.428982  7290 solver.cpp:309]     Train net output #0: loss = 0.0445825 (* 1 = 0.0445825 loss)
I0711 20:02:00.429003  7290 sgd_solver.cpp:106] Iteration 30200, lr = 1e-05
I0711 20:02:17.457499  7290 solver.cpp:290] Iteration 30300 (5.87266 iter/s, 17.028s/100 iter), loss = 0.0357215
I0711 20:02:17.457526  7290 solver.cpp:309]     Train net output #0: loss = 0.0357213 (* 1 = 0.0357213 loss)
I0711 20:02:17.457535  7290 sgd_solver.cpp:106] Iteration 30300, lr = 1e-05
I0711 20:02:34.478695  7290 solver.cpp:290] Iteration 30400 (5.8752 iter/s, 17.0207s/100 iter), loss = 0.0223276
I0711 20:02:34.478759  7290 solver.cpp:309]     Train net output #0: loss = 0.0223274 (* 1 = 0.0223274 loss)
I0711 20:02:34.478768  7290 sgd_solver.cpp:106] Iteration 30400, lr = 1e-05
I0711 20:02:51.379216  7290 solver.cpp:290] Iteration 30500 (5.91716 iter/s, 16.9s/100 iter), loss = 0.0154606
I0711 20:02:51.379242  7290 solver.cpp:309]     Train net output #0: loss = 0.0154605 (* 1 = 0.0154605 loss)
I0711 20:02:51.379251  7290 sgd_solver.cpp:106] Iteration 30500, lr = 1e-05
I0711 20:03:08.576297  7290 solver.cpp:290] Iteration 30600 (5.81511 iter/s, 17.1966s/100 iter), loss = 0.018006
I0711 20:03:08.576400  7290 solver.cpp:309]     Train net output #0: loss = 0.0180059 (* 1 = 0.0180059 loss)
I0711 20:03:08.576417  7290 sgd_solver.cpp:106] Iteration 30600, lr = 1e-05
I0711 20:03:25.873880  7290 solver.cpp:290] Iteration 30700 (5.78135 iter/s, 17.297s/100 iter), loss = 0.0245963
I0711 20:03:25.873916  7290 solver.cpp:309]     Train net output #0: loss = 0.0245962 (* 1 = 0.0245962 loss)
I0711 20:03:25.873926  7290 sgd_solver.cpp:106] Iteration 30700, lr = 1e-05
I0711 20:03:43.252645  7290 solver.cpp:290] Iteration 30800 (5.75432 iter/s, 17.3782s/100 iter), loss = 0.02128
I0711 20:03:43.252748  7290 solver.cpp:309]     Train net output #0: loss = 0.0212798 (* 1 = 0.0212798 loss)
I0711 20:03:43.252771  7290 sgd_solver.cpp:106] Iteration 30800, lr = 1e-05
I0711 20:04:00.628490  7290 solver.cpp:290] Iteration 30900 (5.75531 iter/s, 17.3753s/100 iter), loss = 0.0425611
I0711 20:04:00.628512  7290 solver.cpp:309]     Train net output #0: loss = 0.042561 (* 1 = 0.042561 loss)
I0711 20:04:00.628520  7290 sgd_solver.cpp:106] Iteration 30900, lr = 1e-05
I0711 20:04:18.005465  7290 solver.cpp:290] Iteration 31000 (5.75491 iter/s, 17.3765s/100 iter), loss = 0.0197839
I0711 20:04:18.005525  7290 solver.cpp:309]     Train net output #0: loss = 0.0197838 (* 1 = 0.0197838 loss)
I0711 20:04:18.005534  7290 sgd_solver.cpp:106] Iteration 31000, lr = 1e-05
I0711 20:04:35.465118  7290 solver.cpp:290] Iteration 31100 (5.72767 iter/s, 17.4591s/100 iter), loss = 0.0257725
I0711 20:04:35.465143  7290 solver.cpp:309]     Train net output #0: loss = 0.0257723 (* 1 = 0.0257723 loss)
I0711 20:04:35.465152  7290 sgd_solver.cpp:106] Iteration 31100, lr = 1e-05
I0711 20:04:52.845741  7290 solver.cpp:290] Iteration 31200 (5.7537 iter/s, 17.3801s/100 iter), loss = 0.0260645
I0711 20:04:52.845819  7290 solver.cpp:309]     Train net output #0: loss = 0.0260643 (* 1 = 0.0260643 loss)
I0711 20:04:52.845830  7290 sgd_solver.cpp:106] Iteration 31200, lr = 1e-05
I0711 20:05:10.154433  7290 solver.cpp:290] Iteration 31300 (5.77763 iter/s, 17.3081s/100 iter), loss = 0.0350436
I0711 20:05:10.154458  7290 solver.cpp:309]     Train net output #0: loss = 0.0350435 (* 1 = 0.0350435 loss)
I0711 20:05:10.154464  7290 sgd_solver.cpp:106] Iteration 31300, lr = 1e-05
I0711 20:05:27.318169  7290 solver.cpp:290] Iteration 31400 (5.82641 iter/s, 17.1632s/100 iter), loss = 0.0255884
I0711 20:05:27.318248  7290 solver.cpp:309]     Train net output #0: loss = 0.0255883 (* 1 = 0.0255883 loss)
I0711 20:05:27.318259  7290 sgd_solver.cpp:106] Iteration 31400, lr = 1e-05
I0711 20:05:44.326303  7290 solver.cpp:290] Iteration 31500 (5.87973 iter/s, 17.0076s/100 iter), loss = 0.0145094
I0711 20:05:44.326326  7290 solver.cpp:309]     Train net output #0: loss = 0.0145093 (* 1 = 0.0145093 loss)
I0711 20:05:44.326335  7290 sgd_solver.cpp:106] Iteration 31500, lr = 1e-05
I0711 20:06:01.374552  7290 solver.cpp:290] Iteration 31600 (5.86587 iter/s, 17.0478s/100 iter), loss = 0.0288533
I0711 20:06:01.374630  7290 solver.cpp:309]     Train net output #0: loss = 0.0288531 (* 1 = 0.0288531 loss)
I0711 20:06:01.374639  7290 sgd_solver.cpp:106] Iteration 31600, lr = 1e-05
I0711 20:06:18.279189  7290 solver.cpp:290] Iteration 31700 (5.91573 iter/s, 16.9041s/100 iter), loss = 0.0407668
I0711 20:06:18.279212  7290 solver.cpp:309]     Train net output #0: loss = 0.0407666 (* 1 = 0.0407666 loss)
I0711 20:06:18.279219  7290 sgd_solver.cpp:106] Iteration 31700, lr = 1e-05
I0711 20:06:35.315052  7290 solver.cpp:290] Iteration 31800 (5.87014 iter/s, 17.0354s/100 iter), loss = 0.0192384
I0711 20:06:35.315140  7290 solver.cpp:309]     Train net output #0: loss = 0.0192383 (* 1 = 0.0192383 loss)
I0711 20:06:35.315153  7290 sgd_solver.cpp:106] Iteration 31800, lr = 1e-05
I0711 20:06:52.410508  7290 solver.cpp:290] Iteration 31900 (5.8497 iter/s, 17.0949s/100 iter), loss = 0.0342516
I0711 20:06:52.410531  7290 solver.cpp:309]     Train net output #0: loss = 0.0342515 (* 1 = 0.0342515 loss)
I0711 20:06:52.410537  7290 sgd_solver.cpp:106] Iteration 31900, lr = 1e-05
I0711 20:07:09.320436  7290 solver.cpp:594] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/initial/cityscapes5_jsegnet21v2_iter_32000.caffemodel
I0711 20:07:09.346453  7290 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/initial/cityscapes5_jsegnet21v2_iter_32000.solverstate
I0711 20:07:09.408466  7290 solver.cpp:447] Iteration 32000, loss = 0.0258329
I0711 20:07:09.408486  7290 solver.cpp:467] Iteration 32000, Testing net (#0)
I0711 20:07:58.729935  7290 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.955783
I0711 20:07:58.730022  7290 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999669
I0711 20:07:58.730031  7290 solver.cpp:540]     Test net output #2: loss = 0.138874 (* 1 = 0.138874 loss)
I0711 20:07:58.730036  7290 solver.cpp:452] Optimization Done.
I0711 20:07:59.013834  7290 caffe.cpp:246] Optimization Done.
training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/l1reg
I0711 20:08:11.738207 25943 caffe.cpp:209] Using GPUs 0, 1, 2
I0711 20:08:11.740319 25943 caffe.cpp:214] GPU 0: GeForce GTX 1080
I0711 20:08:11.740659 25943 caffe.cpp:214] GPU 1: GeForce GTX 1080
I0711 20:08:11.741004 25943 caffe.cpp:214] GPU 2: GeForce GTX 1080
I0711 20:08:12.614281 25943 solver.cpp:48] Initializing solver from parameters: 
train_net: "training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/l1reg/train.prototxt"
test_net: "training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/l1reg/test.prototxt"
test_iter: 125
test_interval: 2000
base_lr: 1e-05
display: 100
max_iter: 32000
lr_policy: "multistep"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 1e-05
snapshot: 10000
snapshot_prefix: "training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/l1reg/cityscapes5_jsegnet21v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
regularization_type: "L1"
test_initialization: false
stepvalue: 24000
iter_size: 1
type: "Adam"
I0711 20:08:12.632733 25943 solver.cpp:82] Creating training net from train_net file: training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/l1reg/train.prototxt
I0711 20:08:12.635576 25943 net.cpp:327] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0711 20:08:12.635584 25943 net.cpp:327] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
I0711 20:08:12.635821 25943 net.cpp:56] Initializing net from parameters: 
name: "jsegnet21v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/train-image-lmdb"
    label_list_path: "data/train-label-lmdb"
    batch_size: 5
    shuffle: false
    threads: 4
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b/bn"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a/bn"
  top: "out5a/bn"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a/bn"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b/bn"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a/bn"
  top: "out3a/bn"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a/bn"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv1/bn"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv2/bn"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv3/bn"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4/bn"
  top: "ctx_conv4/bn"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4/bn"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
I0711 20:08:12.636162 25943 layer_factory.hpp:77] Creating layer data
I0711 20:08:12.636183 25943 net.cpp:98] Creating Layer data
I0711 20:08:12.636188 25943 net.cpp:413] data -> data
I0711 20:08:12.636207 25943 net.cpp:413] data -> label
I0711 20:08:12.663363 26012 db_lmdb.cpp:35] Opened lmdb data/train-image-lmdb
I0711 20:08:12.663627 26017 db_lmdb.cpp:35] Opened lmdb data/train-label-lmdb
I0711 20:08:12.668056 25943 data_layer.cpp:78] ReshapePrefetch 5, 3, 640, 640
I0711 20:08:12.668126 25943 data_layer.cpp:83] output data size: 5,3,640,640
I0711 20:08:12.696697 25943 data_layer.cpp:78] ReshapePrefetch 5, 1, 640, 640
I0711 20:08:12.696758 25943 data_layer.cpp:83] output data size: 5,1,640,640
I0711 20:08:12.705622 26022 blocking_queue.cpp:50] Waiting for data
I0711 20:08:12.709240 25943 net.cpp:148] Setting up data
I0711 20:08:12.709262 25943 net.cpp:155] Top shape: 5 3 640 640 (6144000)
I0711 20:08:12.709265 25943 net.cpp:155] Top shape: 5 1 640 640 (2048000)
I0711 20:08:12.709267 25943 net.cpp:163] Memory required for data: 32768000
I0711 20:08:12.709275 25943 layer_factory.hpp:77] Creating layer data/bias
I0711 20:08:12.709282 25943 net.cpp:98] Creating Layer data/bias
I0711 20:08:12.709285 25943 net.cpp:439] data/bias <- data
I0711 20:08:12.709295 25943 net.cpp:413] data/bias -> data/bias
I0711 20:08:12.710577 25943 net.cpp:148] Setting up data/bias
I0711 20:08:12.710587 25943 net.cpp:155] Top shape: 5 3 640 640 (6144000)
I0711 20:08:12.710590 25943 net.cpp:163] Memory required for data: 57344000
I0711 20:08:12.710599 25943 layer_factory.hpp:77] Creating layer conv1a
I0711 20:08:12.710609 25943 net.cpp:98] Creating Layer conv1a
I0711 20:08:12.710611 25943 net.cpp:439] conv1a <- data/bias
I0711 20:08:12.710615 25943 net.cpp:413] conv1a -> conv1a
I0711 20:08:12.712355 25943 net.cpp:148] Setting up conv1a
I0711 20:08:12.712370 25943 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0711 20:08:12.712373 25943 net.cpp:163] Memory required for data: 122880000
I0711 20:08:12.712379 25943 layer_factory.hpp:77] Creating layer conv1a/bn
I0711 20:08:12.712390 25943 net.cpp:98] Creating Layer conv1a/bn
I0711 20:08:12.712393 25943 net.cpp:439] conv1a/bn <- conv1a
I0711 20:08:12.712399 25943 net.cpp:413] conv1a/bn -> conv1a/bn
I0711 20:08:12.714072 25943 net.cpp:148] Setting up conv1a/bn
I0711 20:08:12.714082 25943 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0711 20:08:12.714085 25943 net.cpp:163] Memory required for data: 188416000
I0711 20:08:12.714092 25943 layer_factory.hpp:77] Creating layer conv1a/relu
I0711 20:08:12.714097 25943 net.cpp:98] Creating Layer conv1a/relu
I0711 20:08:12.714098 25943 net.cpp:439] conv1a/relu <- conv1a/bn
I0711 20:08:12.714102 25943 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0711 20:08:12.714109 25943 net.cpp:148] Setting up conv1a/relu
I0711 20:08:12.714112 25943 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0711 20:08:12.714113 25943 net.cpp:163] Memory required for data: 253952000
I0711 20:08:12.714115 25943 layer_factory.hpp:77] Creating layer conv1b
I0711 20:08:12.714120 25943 net.cpp:98] Creating Layer conv1b
I0711 20:08:12.714123 25943 net.cpp:439] conv1b <- conv1a/bn
I0711 20:08:12.714125 25943 net.cpp:413] conv1b -> conv1b
I0711 20:08:12.714498 25943 net.cpp:148] Setting up conv1b
I0711 20:08:12.714504 25943 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0711 20:08:12.714505 25943 net.cpp:163] Memory required for data: 319488000
I0711 20:08:12.714509 25943 layer_factory.hpp:77] Creating layer conv1b/bn
I0711 20:08:12.714514 25943 net.cpp:98] Creating Layer conv1b/bn
I0711 20:08:12.714515 25943 net.cpp:439] conv1b/bn <- conv1b
I0711 20:08:12.714519 25943 net.cpp:413] conv1b/bn -> conv1b/bn
I0711 20:08:12.715205 25943 net.cpp:148] Setting up conv1b/bn
I0711 20:08:12.715211 25943 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0711 20:08:12.715214 25943 net.cpp:163] Memory required for data: 385024000
I0711 20:08:12.715219 25943 layer_factory.hpp:77] Creating layer conv1b/relu
I0711 20:08:12.715221 25943 net.cpp:98] Creating Layer conv1b/relu
I0711 20:08:12.715224 25943 net.cpp:439] conv1b/relu <- conv1b/bn
I0711 20:08:12.715225 25943 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0711 20:08:12.715229 25943 net.cpp:148] Setting up conv1b/relu
I0711 20:08:12.715231 25943 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0711 20:08:12.715232 25943 net.cpp:163] Memory required for data: 450560000
I0711 20:08:12.715234 25943 layer_factory.hpp:77] Creating layer pool1
I0711 20:08:12.715240 25943 net.cpp:98] Creating Layer pool1
I0711 20:08:12.715241 25943 net.cpp:439] pool1 <- conv1b/bn
I0711 20:08:12.715243 25943 net.cpp:413] pool1 -> pool1
I0711 20:08:12.729033 25943 net.cpp:148] Setting up pool1
I0711 20:08:12.729044 25943 net.cpp:155] Top shape: 5 32 160 160 (4096000)
I0711 20:08:12.729046 25943 net.cpp:163] Memory required for data: 466944000
I0711 20:08:12.729049 25943 layer_factory.hpp:77] Creating layer res2a_branch2a
I0711 20:08:12.729054 25943 net.cpp:98] Creating Layer res2a_branch2a
I0711 20:08:12.729058 25943 net.cpp:439] res2a_branch2a <- pool1
I0711 20:08:12.729060 25943 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0711 20:08:12.730670 25943 net.cpp:148] Setting up res2a_branch2a
I0711 20:08:12.730679 25943 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0711 20:08:12.730681 25943 net.cpp:163] Memory required for data: 499712000
I0711 20:08:12.730686 25943 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0711 20:08:12.730691 25943 net.cpp:98] Creating Layer res2a_branch2a/bn
I0711 20:08:12.730695 25943 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0711 20:08:12.730696 25943 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0711 20:08:12.731365 25943 net.cpp:148] Setting up res2a_branch2a/bn
I0711 20:08:12.731371 25943 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0711 20:08:12.731374 25943 net.cpp:163] Memory required for data: 532480000
I0711 20:08:12.731379 25943 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0711 20:08:12.731381 25943 net.cpp:98] Creating Layer res2a_branch2a/relu
I0711 20:08:12.731384 25943 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0711 20:08:12.731385 25943 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0711 20:08:12.731389 25943 net.cpp:148] Setting up res2a_branch2a/relu
I0711 20:08:12.731392 25943 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0711 20:08:12.731393 25943 net.cpp:163] Memory required for data: 565248000
I0711 20:08:12.731395 25943 layer_factory.hpp:77] Creating layer res2a_branch2b
I0711 20:08:12.731400 25943 net.cpp:98] Creating Layer res2a_branch2b
I0711 20:08:12.731401 25943 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0711 20:08:12.731403 25943 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0711 20:08:12.732748 25943 net.cpp:148] Setting up res2a_branch2b
I0711 20:08:12.732756 25943 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0711 20:08:12.732758 25943 net.cpp:163] Memory required for data: 598016000
I0711 20:08:12.732762 25943 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0711 20:08:12.732766 25943 net.cpp:98] Creating Layer res2a_branch2b/bn
I0711 20:08:12.732769 25943 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0711 20:08:12.732771 25943 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0711 20:08:12.733477 25943 net.cpp:148] Setting up res2a_branch2b/bn
I0711 20:08:12.733484 25943 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0711 20:08:12.733485 25943 net.cpp:163] Memory required for data: 630784000
I0711 20:08:12.733490 25943 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0711 20:08:12.733494 25943 net.cpp:98] Creating Layer res2a_branch2b/relu
I0711 20:08:12.733495 25943 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0711 20:08:12.733499 25943 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0711 20:08:12.733503 25943 net.cpp:148] Setting up res2a_branch2b/relu
I0711 20:08:12.733506 25943 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0711 20:08:12.733507 25943 net.cpp:163] Memory required for data: 663552000
I0711 20:08:12.733510 25943 layer_factory.hpp:77] Creating layer pool2
I0711 20:08:12.733513 25943 net.cpp:98] Creating Layer pool2
I0711 20:08:12.733515 25943 net.cpp:439] pool2 <- res2a_branch2b/bn
I0711 20:08:12.733517 25943 net.cpp:413] pool2 -> pool2
I0711 20:08:12.733553 25943 net.cpp:148] Setting up pool2
I0711 20:08:12.733557 25943 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 20:08:12.733559 25943 net.cpp:163] Memory required for data: 671744000
I0711 20:08:12.733561 25943 layer_factory.hpp:77] Creating layer res3a_branch2a
I0711 20:08:12.733566 25943 net.cpp:98] Creating Layer res3a_branch2a
I0711 20:08:12.733568 25943 net.cpp:439] res3a_branch2a <- pool2
I0711 20:08:12.733570 25943 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0711 20:08:12.735291 25943 net.cpp:148] Setting up res3a_branch2a
I0711 20:08:12.735296 25943 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0711 20:08:12.735298 25943 net.cpp:163] Memory required for data: 688128000
I0711 20:08:12.735302 25943 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0711 20:08:12.735306 25943 net.cpp:98] Creating Layer res3a_branch2a/bn
I0711 20:08:12.735307 25943 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0711 20:08:12.735309 25943 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0711 20:08:12.735908 25943 net.cpp:148] Setting up res3a_branch2a/bn
I0711 20:08:12.735914 25943 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0711 20:08:12.735916 25943 net.cpp:163] Memory required for data: 704512000
I0711 20:08:12.735921 25943 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0711 20:08:12.735924 25943 net.cpp:98] Creating Layer res3a_branch2a/relu
I0711 20:08:12.735926 25943 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0711 20:08:12.735929 25943 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0711 20:08:12.735931 25943 net.cpp:148] Setting up res3a_branch2a/relu
I0711 20:08:12.735934 25943 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0711 20:08:12.735935 25943 net.cpp:163] Memory required for data: 720896000
I0711 20:08:12.735937 25943 layer_factory.hpp:77] Creating layer res3a_branch2b
I0711 20:08:12.735941 25943 net.cpp:98] Creating Layer res3a_branch2b
I0711 20:08:12.735944 25943 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0711 20:08:12.735946 25943 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0711 20:08:12.736943 25943 net.cpp:148] Setting up res3a_branch2b
I0711 20:08:12.736948 25943 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0711 20:08:12.736949 25943 net.cpp:163] Memory required for data: 737280000
I0711 20:08:12.736953 25943 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0711 20:08:12.736956 25943 net.cpp:98] Creating Layer res3a_branch2b/bn
I0711 20:08:12.736958 25943 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0711 20:08:12.736961 25943 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0711 20:08:12.737566 25943 net.cpp:148] Setting up res3a_branch2b/bn
I0711 20:08:12.737572 25943 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0711 20:08:12.737574 25943 net.cpp:163] Memory required for data: 753664000
I0711 20:08:12.737578 25943 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0711 20:08:12.737581 25943 net.cpp:98] Creating Layer res3a_branch2b/relu
I0711 20:08:12.737583 25943 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0711 20:08:12.737592 25943 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0711 20:08:12.737596 25943 net.cpp:148] Setting up res3a_branch2b/relu
I0711 20:08:12.737598 25943 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0711 20:08:12.737599 25943 net.cpp:163] Memory required for data: 770048000
I0711 20:08:12.737601 25943 layer_factory.hpp:77] Creating layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0711 20:08:12.737604 25943 net.cpp:98] Creating Layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0711 20:08:12.737607 25943 net.cpp:439] res3a_branch2b/bn_res3a_branch2b/relu_0_split <- res3a_branch2b/bn
I0711 20:08:12.737608 25943 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0711 20:08:12.737612 25943 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0711 20:08:12.737650 25943 net.cpp:148] Setting up res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0711 20:08:12.737658 25943 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0711 20:08:12.737659 25943 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0711 20:08:12.737661 25943 net.cpp:163] Memory required for data: 802816000
I0711 20:08:12.737663 25943 layer_factory.hpp:77] Creating layer pool3
I0711 20:08:12.737666 25943 net.cpp:98] Creating Layer pool3
I0711 20:08:12.737668 25943 net.cpp:439] pool3 <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0711 20:08:12.737670 25943 net.cpp:413] pool3 -> pool3
I0711 20:08:12.737709 25943 net.cpp:148] Setting up pool3
I0711 20:08:12.737713 25943 net.cpp:155] Top shape: 5 128 40 40 (1024000)
I0711 20:08:12.737715 25943 net.cpp:163] Memory required for data: 806912000
I0711 20:08:12.737717 25943 layer_factory.hpp:77] Creating layer res4a_branch2a
I0711 20:08:12.737720 25943 net.cpp:98] Creating Layer res4a_branch2a
I0711 20:08:12.737723 25943 net.cpp:439] res4a_branch2a <- pool3
I0711 20:08:12.737725 25943 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0711 20:08:12.744794 25943 net.cpp:148] Setting up res4a_branch2a
I0711 20:08:12.744804 25943 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0711 20:08:12.744807 25943 net.cpp:163] Memory required for data: 815104000
I0711 20:08:12.744810 25943 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0711 20:08:12.744818 25943 net.cpp:98] Creating Layer res4a_branch2a/bn
I0711 20:08:12.744822 25943 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0711 20:08:12.744824 25943 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0711 20:08:12.745501 25943 net.cpp:148] Setting up res4a_branch2a/bn
I0711 20:08:12.745509 25943 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0711 20:08:12.745512 25943 net.cpp:163] Memory required for data: 823296000
I0711 20:08:12.745517 25943 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0711 20:08:12.745519 25943 net.cpp:98] Creating Layer res4a_branch2a/relu
I0711 20:08:12.745522 25943 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0711 20:08:12.745523 25943 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0711 20:08:12.745527 25943 net.cpp:148] Setting up res4a_branch2a/relu
I0711 20:08:12.745529 25943 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0711 20:08:12.745532 25943 net.cpp:163] Memory required for data: 831488000
I0711 20:08:12.745533 25943 layer_factory.hpp:77] Creating layer res4a_branch2b
I0711 20:08:12.745537 25943 net.cpp:98] Creating Layer res4a_branch2b
I0711 20:08:12.745540 25943 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0711 20:08:12.745543 25943 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0711 20:08:12.748811 25943 net.cpp:148] Setting up res4a_branch2b
I0711 20:08:12.748821 25943 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0711 20:08:12.748822 25943 net.cpp:163] Memory required for data: 839680000
I0711 20:08:12.748826 25943 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0711 20:08:12.748829 25943 net.cpp:98] Creating Layer res4a_branch2b/bn
I0711 20:08:12.748831 25943 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0711 20:08:12.748842 25943 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0711 20:08:12.749518 25943 net.cpp:148] Setting up res4a_branch2b/bn
I0711 20:08:12.749526 25943 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0711 20:08:12.749527 25943 net.cpp:163] Memory required for data: 847872000
I0711 20:08:12.749532 25943 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0711 20:08:12.749536 25943 net.cpp:98] Creating Layer res4a_branch2b/relu
I0711 20:08:12.749537 25943 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0711 20:08:12.749539 25943 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0711 20:08:12.749543 25943 net.cpp:148] Setting up res4a_branch2b/relu
I0711 20:08:12.749546 25943 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0711 20:08:12.749547 25943 net.cpp:163] Memory required for data: 856064000
I0711 20:08:12.749549 25943 layer_factory.hpp:77] Creating layer pool4
I0711 20:08:12.749552 25943 net.cpp:98] Creating Layer pool4
I0711 20:08:12.749554 25943 net.cpp:439] pool4 <- res4a_branch2b/bn
I0711 20:08:12.749557 25943 net.cpp:413] pool4 -> pool4
I0711 20:08:12.749593 25943 net.cpp:148] Setting up pool4
I0711 20:08:12.749598 25943 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0711 20:08:12.749599 25943 net.cpp:163] Memory required for data: 864256000
I0711 20:08:12.749601 25943 layer_factory.hpp:77] Creating layer res5a_branch2a
I0711 20:08:12.749605 25943 net.cpp:98] Creating Layer res5a_branch2a
I0711 20:08:12.749608 25943 net.cpp:439] res5a_branch2a <- pool4
I0711 20:08:12.749613 25943 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0711 20:08:12.780974 25943 net.cpp:148] Setting up res5a_branch2a
I0711 20:08:12.780992 25943 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0711 20:08:12.780994 25943 net.cpp:163] Memory required for data: 880640000
I0711 20:08:12.781000 25943 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0711 20:08:12.781010 25943 net.cpp:98] Creating Layer res5a_branch2a/bn
I0711 20:08:12.781013 25943 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0711 20:08:12.781018 25943 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0711 20:08:12.781738 25943 net.cpp:148] Setting up res5a_branch2a/bn
I0711 20:08:12.781745 25943 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0711 20:08:12.781747 25943 net.cpp:163] Memory required for data: 897024000
I0711 20:08:12.781752 25943 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0711 20:08:12.781756 25943 net.cpp:98] Creating Layer res5a_branch2a/relu
I0711 20:08:12.781759 25943 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0711 20:08:12.781761 25943 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0711 20:08:12.781765 25943 net.cpp:148] Setting up res5a_branch2a/relu
I0711 20:08:12.781767 25943 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0711 20:08:12.781769 25943 net.cpp:163] Memory required for data: 913408000
I0711 20:08:12.781771 25943 layer_factory.hpp:77] Creating layer res5a_branch2b
I0711 20:08:12.781776 25943 net.cpp:98] Creating Layer res5a_branch2b
I0711 20:08:12.781777 25943 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0711 20:08:12.781780 25943 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0711 20:08:12.799232 25943 net.cpp:148] Setting up res5a_branch2b
I0711 20:08:12.799249 25943 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0711 20:08:12.799252 25943 net.cpp:163] Memory required for data: 929792000
I0711 20:08:12.799260 25943 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0711 20:08:12.799268 25943 net.cpp:98] Creating Layer res5a_branch2b/bn
I0711 20:08:12.799270 25943 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0711 20:08:12.799273 25943 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0711 20:08:12.799937 25943 net.cpp:148] Setting up res5a_branch2b/bn
I0711 20:08:12.799942 25943 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0711 20:08:12.799944 25943 net.cpp:163] Memory required for data: 946176000
I0711 20:08:12.799949 25943 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0711 20:08:12.799962 25943 net.cpp:98] Creating Layer res5a_branch2b/relu
I0711 20:08:12.799965 25943 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0711 20:08:12.799968 25943 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0711 20:08:12.799973 25943 net.cpp:148] Setting up res5a_branch2b/relu
I0711 20:08:12.799974 25943 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0711 20:08:12.799976 25943 net.cpp:163] Memory required for data: 962560000
I0711 20:08:12.799978 25943 layer_factory.hpp:77] Creating layer out5a
I0711 20:08:12.799983 25943 net.cpp:98] Creating Layer out5a
I0711 20:08:12.799985 25943 net.cpp:439] out5a <- res5a_branch2b/bn
I0711 20:08:12.799991 25943 net.cpp:413] out5a -> out5a
I0711 20:08:12.804133 25943 net.cpp:148] Setting up out5a
I0711 20:08:12.804142 25943 net.cpp:155] Top shape: 5 64 40 40 (512000)
I0711 20:08:12.804145 25943 net.cpp:163] Memory required for data: 964608000
I0711 20:08:12.804148 25943 layer_factory.hpp:77] Creating layer out5a/bn
I0711 20:08:12.804152 25943 net.cpp:98] Creating Layer out5a/bn
I0711 20:08:12.804154 25943 net.cpp:439] out5a/bn <- out5a
I0711 20:08:12.804157 25943 net.cpp:413] out5a/bn -> out5a/bn
I0711 20:08:12.804870 25943 net.cpp:148] Setting up out5a/bn
I0711 20:08:12.804877 25943 net.cpp:155] Top shape: 5 64 40 40 (512000)
I0711 20:08:12.804879 25943 net.cpp:163] Memory required for data: 966656000
I0711 20:08:12.804884 25943 layer_factory.hpp:77] Creating layer out5a/relu
I0711 20:08:12.804888 25943 net.cpp:98] Creating Layer out5a/relu
I0711 20:08:12.804889 25943 net.cpp:439] out5a/relu <- out5a/bn
I0711 20:08:12.804891 25943 net.cpp:400] out5a/relu -> out5a/bn (in-place)
I0711 20:08:12.804895 25943 net.cpp:148] Setting up out5a/relu
I0711 20:08:12.804898 25943 net.cpp:155] Top shape: 5 64 40 40 (512000)
I0711 20:08:12.804899 25943 net.cpp:163] Memory required for data: 968704000
I0711 20:08:12.804901 25943 layer_factory.hpp:77] Creating layer out5a_up2
I0711 20:08:12.804909 25943 net.cpp:98] Creating Layer out5a_up2
I0711 20:08:12.804913 25943 net.cpp:439] out5a_up2 <- out5a/bn
I0711 20:08:12.804915 25943 net.cpp:413] out5a_up2 -> out5a_up2
I0711 20:08:12.805163 25943 net.cpp:148] Setting up out5a_up2
I0711 20:08:12.805168 25943 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 20:08:12.805169 25943 net.cpp:163] Memory required for data: 976896000
I0711 20:08:12.805172 25943 layer_factory.hpp:77] Creating layer out3a
I0711 20:08:12.805176 25943 net.cpp:98] Creating Layer out3a
I0711 20:08:12.805178 25943 net.cpp:439] out3a <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0711 20:08:12.805181 25943 net.cpp:413] out3a -> out3a
I0711 20:08:12.806435 25943 net.cpp:148] Setting up out3a
I0711 20:08:12.806447 25943 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 20:08:12.806450 25943 net.cpp:163] Memory required for data: 985088000
I0711 20:08:12.806455 25943 layer_factory.hpp:77] Creating layer out3a/bn
I0711 20:08:12.806464 25943 net.cpp:98] Creating Layer out3a/bn
I0711 20:08:12.806468 25943 net.cpp:439] out3a/bn <- out3a
I0711 20:08:12.806473 25943 net.cpp:413] out3a/bn -> out3a/bn
I0711 20:08:12.807474 25943 net.cpp:148] Setting up out3a/bn
I0711 20:08:12.807482 25943 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 20:08:12.807485 25943 net.cpp:163] Memory required for data: 993280000
I0711 20:08:12.807492 25943 layer_factory.hpp:77] Creating layer out3a/relu
I0711 20:08:12.807497 25943 net.cpp:98] Creating Layer out3a/relu
I0711 20:08:12.807500 25943 net.cpp:439] out3a/relu <- out3a/bn
I0711 20:08:12.807503 25943 net.cpp:400] out3a/relu -> out3a/bn (in-place)
I0711 20:08:12.807509 25943 net.cpp:148] Setting up out3a/relu
I0711 20:08:12.807512 25943 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 20:08:12.807516 25943 net.cpp:163] Memory required for data: 1001472000
I0711 20:08:12.807520 25943 layer_factory.hpp:77] Creating layer out3_out5_combined
I0711 20:08:12.807525 25943 net.cpp:98] Creating Layer out3_out5_combined
I0711 20:08:12.807529 25943 net.cpp:439] out3_out5_combined <- out5a_up2
I0711 20:08:12.807533 25943 net.cpp:439] out3_out5_combined <- out3a/bn
I0711 20:08:12.807549 25943 net.cpp:413] out3_out5_combined -> out3_out5_combined
I0711 20:08:12.807585 25943 net.cpp:148] Setting up out3_out5_combined
I0711 20:08:12.807590 25943 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 20:08:12.807592 25943 net.cpp:163] Memory required for data: 1009664000
I0711 20:08:12.807596 25943 layer_factory.hpp:77] Creating layer ctx_conv1
I0711 20:08:12.807601 25943 net.cpp:98] Creating Layer ctx_conv1
I0711 20:08:12.807605 25943 net.cpp:439] ctx_conv1 <- out3_out5_combined
I0711 20:08:12.807610 25943 net.cpp:413] ctx_conv1 -> ctx_conv1
I0711 20:08:12.809015 25943 net.cpp:148] Setting up ctx_conv1
I0711 20:08:12.809025 25943 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 20:08:12.809027 25943 net.cpp:163] Memory required for data: 1017856000
I0711 20:08:12.809032 25943 layer_factory.hpp:77] Creating layer ctx_conv1/bn
I0711 20:08:12.809037 25943 net.cpp:98] Creating Layer ctx_conv1/bn
I0711 20:08:12.809041 25943 net.cpp:439] ctx_conv1/bn <- ctx_conv1
I0711 20:08:12.809044 25943 net.cpp:413] ctx_conv1/bn -> ctx_conv1/bn
I0711 20:08:12.810044 25943 net.cpp:148] Setting up ctx_conv1/bn
I0711 20:08:12.810051 25943 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 20:08:12.810055 25943 net.cpp:163] Memory required for data: 1026048000
I0711 20:08:12.810061 25943 layer_factory.hpp:77] Creating layer ctx_conv1/relu
I0711 20:08:12.810065 25943 net.cpp:98] Creating Layer ctx_conv1/relu
I0711 20:08:12.810070 25943 net.cpp:439] ctx_conv1/relu <- ctx_conv1/bn
I0711 20:08:12.810072 25943 net.cpp:400] ctx_conv1/relu -> ctx_conv1/bn (in-place)
I0711 20:08:12.810077 25943 net.cpp:148] Setting up ctx_conv1/relu
I0711 20:08:12.810081 25943 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 20:08:12.810083 25943 net.cpp:163] Memory required for data: 1034240000
I0711 20:08:12.810087 25943 layer_factory.hpp:77] Creating layer ctx_conv2
I0711 20:08:12.810096 25943 net.cpp:98] Creating Layer ctx_conv2
I0711 20:08:12.810098 25943 net.cpp:439] ctx_conv2 <- ctx_conv1/bn
I0711 20:08:12.810102 25943 net.cpp:413] ctx_conv2 -> ctx_conv2
I0711 20:08:12.811172 25943 net.cpp:148] Setting up ctx_conv2
I0711 20:08:12.811179 25943 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 20:08:12.811182 25943 net.cpp:163] Memory required for data: 1042432000
I0711 20:08:12.811184 25943 layer_factory.hpp:77] Creating layer ctx_conv2/bn
I0711 20:08:12.811188 25943 net.cpp:98] Creating Layer ctx_conv2/bn
I0711 20:08:12.811190 25943 net.cpp:439] ctx_conv2/bn <- ctx_conv2
I0711 20:08:12.811192 25943 net.cpp:413] ctx_conv2/bn -> ctx_conv2/bn
I0711 20:08:12.811923 25943 net.cpp:148] Setting up ctx_conv2/bn
I0711 20:08:12.811930 25943 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 20:08:12.811933 25943 net.cpp:163] Memory required for data: 1050624000
I0711 20:08:12.811938 25943 layer_factory.hpp:77] Creating layer ctx_conv2/relu
I0711 20:08:12.811941 25943 net.cpp:98] Creating Layer ctx_conv2/relu
I0711 20:08:12.811944 25943 net.cpp:439] ctx_conv2/relu <- ctx_conv2/bn
I0711 20:08:12.811946 25943 net.cpp:400] ctx_conv2/relu -> ctx_conv2/bn (in-place)
I0711 20:08:12.811949 25943 net.cpp:148] Setting up ctx_conv2/relu
I0711 20:08:12.811952 25943 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 20:08:12.811954 25943 net.cpp:163] Memory required for data: 1058816000
I0711 20:08:12.811955 25943 layer_factory.hpp:77] Creating layer ctx_conv3
I0711 20:08:12.811959 25943 net.cpp:98] Creating Layer ctx_conv3
I0711 20:08:12.811961 25943 net.cpp:439] ctx_conv3 <- ctx_conv2/bn
I0711 20:08:12.811964 25943 net.cpp:413] ctx_conv3 -> ctx_conv3
I0711 20:08:12.813072 25943 net.cpp:148] Setting up ctx_conv3
I0711 20:08:12.813079 25943 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 20:08:12.813081 25943 net.cpp:163] Memory required for data: 1067008000
I0711 20:08:12.813084 25943 layer_factory.hpp:77] Creating layer ctx_conv3/bn
I0711 20:08:12.813087 25943 net.cpp:98] Creating Layer ctx_conv3/bn
I0711 20:08:12.813091 25943 net.cpp:439] ctx_conv3/bn <- ctx_conv3
I0711 20:08:12.813092 25943 net.cpp:413] ctx_conv3/bn -> ctx_conv3/bn
I0711 20:08:12.813861 25943 net.cpp:148] Setting up ctx_conv3/bn
I0711 20:08:12.813869 25943 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 20:08:12.813871 25943 net.cpp:163] Memory required for data: 1075200000
I0711 20:08:12.813876 25943 layer_factory.hpp:77] Creating layer ctx_conv3/relu
I0711 20:08:12.813879 25943 net.cpp:98] Creating Layer ctx_conv3/relu
I0711 20:08:12.813881 25943 net.cpp:439] ctx_conv3/relu <- ctx_conv3/bn
I0711 20:08:12.813884 25943 net.cpp:400] ctx_conv3/relu -> ctx_conv3/bn (in-place)
I0711 20:08:12.813887 25943 net.cpp:148] Setting up ctx_conv3/relu
I0711 20:08:12.813890 25943 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 20:08:12.813891 25943 net.cpp:163] Memory required for data: 1083392000
I0711 20:08:12.813894 25943 layer_factory.hpp:77] Creating layer ctx_conv4
I0711 20:08:12.813897 25943 net.cpp:98] Creating Layer ctx_conv4
I0711 20:08:12.813899 25943 net.cpp:439] ctx_conv4 <- ctx_conv3/bn
I0711 20:08:12.813901 25943 net.cpp:413] ctx_conv4 -> ctx_conv4
I0711 20:08:12.815003 25943 net.cpp:148] Setting up ctx_conv4
I0711 20:08:12.815011 25943 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 20:08:12.815012 25943 net.cpp:163] Memory required for data: 1091584000
I0711 20:08:12.815016 25943 layer_factory.hpp:77] Creating layer ctx_conv4/bn
I0711 20:08:12.815019 25943 net.cpp:98] Creating Layer ctx_conv4/bn
I0711 20:08:12.815021 25943 net.cpp:439] ctx_conv4/bn <- ctx_conv4
I0711 20:08:12.815023 25943 net.cpp:413] ctx_conv4/bn -> ctx_conv4/bn
I0711 20:08:12.815778 25943 net.cpp:148] Setting up ctx_conv4/bn
I0711 20:08:12.815784 25943 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 20:08:12.815786 25943 net.cpp:163] Memory required for data: 1099776000
I0711 20:08:12.815791 25943 layer_factory.hpp:77] Creating layer ctx_conv4/relu
I0711 20:08:12.815794 25943 net.cpp:98] Creating Layer ctx_conv4/relu
I0711 20:08:12.815796 25943 net.cpp:439] ctx_conv4/relu <- ctx_conv4/bn
I0711 20:08:12.815798 25943 net.cpp:400] ctx_conv4/relu -> ctx_conv4/bn (in-place)
I0711 20:08:12.815801 25943 net.cpp:148] Setting up ctx_conv4/relu
I0711 20:08:12.815804 25943 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 20:08:12.815805 25943 net.cpp:163] Memory required for data: 1107968000
I0711 20:08:12.815807 25943 layer_factory.hpp:77] Creating layer ctx_final
I0711 20:08:12.815811 25943 net.cpp:98] Creating Layer ctx_final
I0711 20:08:12.815814 25943 net.cpp:439] ctx_final <- ctx_conv4/bn
I0711 20:08:12.815815 25943 net.cpp:413] ctx_final -> ctx_final
I0711 20:08:12.816287 25943 net.cpp:148] Setting up ctx_final
I0711 20:08:12.816293 25943 net.cpp:155] Top shape: 5 8 80 80 (256000)
I0711 20:08:12.816295 25943 net.cpp:163] Memory required for data: 1108992000
I0711 20:08:12.816299 25943 layer_factory.hpp:77] Creating layer ctx_final/relu
I0711 20:08:12.816301 25943 net.cpp:98] Creating Layer ctx_final/relu
I0711 20:08:12.816303 25943 net.cpp:439] ctx_final/relu <- ctx_final
I0711 20:08:12.816306 25943 net.cpp:400] ctx_final/relu -> ctx_final (in-place)
I0711 20:08:12.816309 25943 net.cpp:148] Setting up ctx_final/relu
I0711 20:08:12.816313 25943 net.cpp:155] Top shape: 5 8 80 80 (256000)
I0711 20:08:12.816313 25943 net.cpp:163] Memory required for data: 1110016000
I0711 20:08:12.816315 25943 layer_factory.hpp:77] Creating layer out_deconv_final_up2
I0711 20:08:12.816319 25943 net.cpp:98] Creating Layer out_deconv_final_up2
I0711 20:08:12.816321 25943 net.cpp:439] out_deconv_final_up2 <- ctx_final
I0711 20:08:12.816323 25943 net.cpp:413] out_deconv_final_up2 -> out_deconv_final_up2
I0711 20:08:12.816617 25943 net.cpp:148] Setting up out_deconv_final_up2
I0711 20:08:12.816625 25943 net.cpp:155] Top shape: 5 8 160 160 (1024000)
I0711 20:08:12.816628 25943 net.cpp:163] Memory required for data: 1114112000
I0711 20:08:12.816633 25943 layer_factory.hpp:77] Creating layer out_deconv_final_up4
I0711 20:08:12.816638 25943 net.cpp:98] Creating Layer out_deconv_final_up4
I0711 20:08:12.816642 25943 net.cpp:439] out_deconv_final_up4 <- out_deconv_final_up2
I0711 20:08:12.816646 25943 net.cpp:413] out_deconv_final_up4 -> out_deconv_final_up4
I0711 20:08:12.816958 25943 net.cpp:148] Setting up out_deconv_final_up4
I0711 20:08:12.816964 25943 net.cpp:155] Top shape: 5 8 320 320 (4096000)
I0711 20:08:12.816967 25943 net.cpp:163] Memory required for data: 1130496000
I0711 20:08:12.816969 25943 layer_factory.hpp:77] Creating layer out_deconv_final_up8
I0711 20:08:12.816972 25943 net.cpp:98] Creating Layer out_deconv_final_up8
I0711 20:08:12.816974 25943 net.cpp:439] out_deconv_final_up8 <- out_deconv_final_up4
I0711 20:08:12.816977 25943 net.cpp:413] out_deconv_final_up8 -> out_deconv_final_up8
I0711 20:08:12.817252 25943 net.cpp:148] Setting up out_deconv_final_up8
I0711 20:08:12.817258 25943 net.cpp:155] Top shape: 5 8 640 640 (16384000)
I0711 20:08:12.817260 25943 net.cpp:163] Memory required for data: 1196032000
I0711 20:08:12.817263 25943 layer_factory.hpp:77] Creating layer loss
I0711 20:08:12.817270 25943 net.cpp:98] Creating Layer loss
I0711 20:08:12.817272 25943 net.cpp:439] loss <- out_deconv_final_up8
I0711 20:08:12.817275 25943 net.cpp:439] loss <- label
I0711 20:08:12.817278 25943 net.cpp:413] loss -> loss
I0711 20:08:12.817286 25943 layer_factory.hpp:77] Creating layer loss
I0711 20:08:12.838810 25943 net.cpp:148] Setting up loss
I0711 20:08:12.838831 25943 net.cpp:155] Top shape: (1)
I0711 20:08:12.838834 25943 net.cpp:158]     with loss weight 1
I0711 20:08:12.838846 25943 net.cpp:163] Memory required for data: 1196032004
I0711 20:08:12.838850 25943 net.cpp:224] loss needs backward computation.
I0711 20:08:12.838852 25943 net.cpp:224] out_deconv_final_up8 needs backward computation.
I0711 20:08:12.838855 25943 net.cpp:224] out_deconv_final_up4 needs backward computation.
I0711 20:08:12.838856 25943 net.cpp:224] out_deconv_final_up2 needs backward computation.
I0711 20:08:12.838858 25943 net.cpp:224] ctx_final/relu needs backward computation.
I0711 20:08:12.838860 25943 net.cpp:224] ctx_final needs backward computation.
I0711 20:08:12.838862 25943 net.cpp:224] ctx_conv4/relu needs backward computation.
I0711 20:08:12.838863 25943 net.cpp:224] ctx_conv4/bn needs backward computation.
I0711 20:08:12.838865 25943 net.cpp:224] ctx_conv4 needs backward computation.
I0711 20:08:12.838867 25943 net.cpp:224] ctx_conv3/relu needs backward computation.
I0711 20:08:12.838870 25943 net.cpp:224] ctx_conv3/bn needs backward computation.
I0711 20:08:12.838871 25943 net.cpp:224] ctx_conv3 needs backward computation.
I0711 20:08:12.838873 25943 net.cpp:224] ctx_conv2/relu needs backward computation.
I0711 20:08:12.838876 25943 net.cpp:224] ctx_conv2/bn needs backward computation.
I0711 20:08:12.838877 25943 net.cpp:224] ctx_conv2 needs backward computation.
I0711 20:08:12.838881 25943 net.cpp:224] ctx_conv1/relu needs backward computation.
I0711 20:08:12.838882 25943 net.cpp:224] ctx_conv1/bn needs backward computation.
I0711 20:08:12.838886 25943 net.cpp:224] ctx_conv1 needs backward computation.
I0711 20:08:12.838887 25943 net.cpp:224] out3_out5_combined needs backward computation.
I0711 20:08:12.838891 25943 net.cpp:224] out3a/relu needs backward computation.
I0711 20:08:12.838892 25943 net.cpp:224] out3a/bn needs backward computation.
I0711 20:08:12.838894 25943 net.cpp:224] out3a needs backward computation.
I0711 20:08:12.838897 25943 net.cpp:224] out5a_up2 needs backward computation.
I0711 20:08:12.838899 25943 net.cpp:224] out5a/relu needs backward computation.
I0711 20:08:12.838901 25943 net.cpp:224] out5a/bn needs backward computation.
I0711 20:08:12.838903 25943 net.cpp:224] out5a needs backward computation.
I0711 20:08:12.838907 25943 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0711 20:08:12.838909 25943 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0711 20:08:12.838912 25943 net.cpp:224] res5a_branch2b needs backward computation.
I0711 20:08:12.838913 25943 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0711 20:08:12.838915 25943 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0711 20:08:12.838917 25943 net.cpp:224] res5a_branch2a needs backward computation.
I0711 20:08:12.838932 25943 net.cpp:224] pool4 needs backward computation.
I0711 20:08:12.838935 25943 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0711 20:08:12.838939 25943 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0711 20:08:12.838943 25943 net.cpp:224] res4a_branch2b needs backward computation.
I0711 20:08:12.838946 25943 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0711 20:08:12.838949 25943 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0711 20:08:12.838953 25943 net.cpp:224] res4a_branch2a needs backward computation.
I0711 20:08:12.838958 25943 net.cpp:224] pool3 needs backward computation.
I0711 20:08:12.838961 25943 net.cpp:224] res3a_branch2b/bn_res3a_branch2b/relu_0_split needs backward computation.
I0711 20:08:12.838965 25943 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0711 20:08:12.838969 25943 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0711 20:08:12.838973 25943 net.cpp:224] res3a_branch2b needs backward computation.
I0711 20:08:12.838977 25943 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0711 20:08:12.838981 25943 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0711 20:08:12.838985 25943 net.cpp:224] res3a_branch2a needs backward computation.
I0711 20:08:12.838989 25943 net.cpp:224] pool2 needs backward computation.
I0711 20:08:12.838994 25943 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0711 20:08:12.838999 25943 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0711 20:08:12.839002 25943 net.cpp:224] res2a_branch2b needs backward computation.
I0711 20:08:12.839006 25943 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0711 20:08:12.839010 25943 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0711 20:08:12.839015 25943 net.cpp:224] res2a_branch2a needs backward computation.
I0711 20:08:12.839018 25943 net.cpp:224] pool1 needs backward computation.
I0711 20:08:12.839022 25943 net.cpp:224] conv1b/relu needs backward computation.
I0711 20:08:12.839026 25943 net.cpp:224] conv1b/bn needs backward computation.
I0711 20:08:12.839030 25943 net.cpp:224] conv1b needs backward computation.
I0711 20:08:12.839035 25943 net.cpp:224] conv1a/relu needs backward computation.
I0711 20:08:12.839037 25943 net.cpp:224] conv1a/bn needs backward computation.
I0711 20:08:12.839041 25943 net.cpp:224] conv1a needs backward computation.
I0711 20:08:12.839046 25943 net.cpp:226] data/bias does not need backward computation.
I0711 20:08:12.839051 25943 net.cpp:226] data does not need backward computation.
I0711 20:08:12.839054 25943 net.cpp:268] This network produces output loss
I0711 20:08:12.839097 25943 net.cpp:288] Network initialization done.
I0711 20:08:12.839880 25943 solver.cpp:182] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/l1reg/test.prototxt
I0711 20:08:12.840175 25943 net.cpp:56] Initializing net from parameters: 
name: "jsegnet21v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/val-image-lmdb"
    label_list_path: "data/val-label-lmdb"
    batch_size: 4
    threads: 4
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b/bn"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a/bn"
  top: "out5a/bn"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a/bn"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b/bn"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a/bn"
  top: "out3a/bn"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a/bn"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv1/bn"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv2/bn"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv3/bn"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4/bn"
  top: "ctx_conv4/bn"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4/bn"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
  accuracy_param {
    ignore_label: 255
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
    ignore_label: 255
  }
}
I0711 20:08:12.840304 25943 layer_factory.hpp:77] Creating layer data
I0711 20:08:12.840314 25943 net.cpp:98] Creating Layer data
I0711 20:08:12.840319 25943 net.cpp:413] data -> data
I0711 20:08:12.840327 25943 net.cpp:413] data -> label
I0711 20:08:12.855726 26027 db_lmdb.cpp:35] Opened lmdb data/val-image-lmdb
I0711 20:08:12.859331 25943 data_layer.cpp:78] ReshapePrefetch 4, 3, 640, 640
I0711 20:08:12.859444 25943 data_layer.cpp:83] output data size: 4,3,640,640
I0711 20:08:12.869568 26032 db_lmdb.cpp:35] Opened lmdb data/val-label-lmdb
I0711 20:08:12.886693 25943 data_layer.cpp:78] ReshapePrefetch 4, 1, 640, 640
I0711 20:08:12.886765 25943 data_layer.cpp:83] output data size: 4,1,640,640
I0711 20:08:12.903172 25943 net.cpp:148] Setting up data
I0711 20:08:12.903203 25943 net.cpp:155] Top shape: 4 3 640 640 (4915200)
I0711 20:08:12.903208 25943 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0711 20:08:12.903209 25943 net.cpp:163] Memory required for data: 26214400
I0711 20:08:12.903214 25943 layer_factory.hpp:77] Creating layer label_data_1_split
I0711 20:08:12.903228 25943 net.cpp:98] Creating Layer label_data_1_split
I0711 20:08:12.903230 25943 net.cpp:439] label_data_1_split <- label
I0711 20:08:12.903275 25943 net.cpp:413] label_data_1_split -> label_data_1_split_0
I0711 20:08:12.903296 25943 net.cpp:413] label_data_1_split -> label_data_1_split_1
I0711 20:08:12.903301 25943 net.cpp:413] label_data_1_split -> label_data_1_split_2
I0711 20:08:12.904938 25943 net.cpp:148] Setting up label_data_1_split
I0711 20:08:12.904945 25943 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0711 20:08:12.904948 25943 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0711 20:08:12.904950 25943 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0711 20:08:12.904953 25943 net.cpp:163] Memory required for data: 45875200
I0711 20:08:12.904954 25943 layer_factory.hpp:77] Creating layer data/bias
I0711 20:08:12.904960 25943 net.cpp:98] Creating Layer data/bias
I0711 20:08:12.904963 25943 net.cpp:439] data/bias <- data
I0711 20:08:12.904965 25943 net.cpp:413] data/bias -> data/bias
I0711 20:08:12.906335 25943 net.cpp:148] Setting up data/bias
I0711 20:08:12.906345 25943 net.cpp:155] Top shape: 4 3 640 640 (4915200)
I0711 20:08:12.906347 25943 net.cpp:163] Memory required for data: 65536000
I0711 20:08:12.906353 25943 layer_factory.hpp:77] Creating layer conv1a
I0711 20:08:12.906360 25943 net.cpp:98] Creating Layer conv1a
I0711 20:08:12.906363 25943 net.cpp:439] conv1a <- data/bias
I0711 20:08:12.906366 25943 net.cpp:413] conv1a -> conv1a
I0711 20:08:12.908598 25943 net.cpp:148] Setting up conv1a
I0711 20:08:12.908608 25943 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0711 20:08:12.908612 25943 net.cpp:163] Memory required for data: 117964800
I0711 20:08:12.908617 25943 layer_factory.hpp:77] Creating layer conv1a/bn
I0711 20:08:12.908627 25943 net.cpp:98] Creating Layer conv1a/bn
I0711 20:08:12.908629 25943 net.cpp:439] conv1a/bn <- conv1a
I0711 20:08:12.908633 25943 net.cpp:413] conv1a/bn -> conv1a/bn
I0711 20:08:12.916867 25943 net.cpp:148] Setting up conv1a/bn
I0711 20:08:12.916929 25943 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0711 20:08:12.916931 25943 net.cpp:163] Memory required for data: 170393600
I0711 20:08:12.916957 25943 layer_factory.hpp:77] Creating layer conv1a/relu
I0711 20:08:12.916973 25943 net.cpp:98] Creating Layer conv1a/relu
I0711 20:08:12.916981 25943 net.cpp:439] conv1a/relu <- conv1a/bn
I0711 20:08:12.916988 25943 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0711 20:08:12.917217 25943 net.cpp:148] Setting up conv1a/relu
I0711 20:08:12.917227 25943 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0711 20:08:12.917230 25943 net.cpp:163] Memory required for data: 222822400
I0711 20:08:12.917235 25943 layer_factory.hpp:77] Creating layer conv1b
I0711 20:08:12.917251 25943 net.cpp:98] Creating Layer conv1b
I0711 20:08:12.917256 25943 net.cpp:439] conv1b <- conv1a/bn
I0711 20:08:12.917263 25943 net.cpp:413] conv1b -> conv1b
I0711 20:08:12.918009 25943 net.cpp:148] Setting up conv1b
I0711 20:08:12.918020 25943 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0711 20:08:12.918021 25943 net.cpp:163] Memory required for data: 275251200
I0711 20:08:12.918027 25943 layer_factory.hpp:77] Creating layer conv1b/bn
I0711 20:08:12.918040 25943 net.cpp:98] Creating Layer conv1b/bn
I0711 20:08:12.918046 25943 net.cpp:439] conv1b/bn <- conv1b
I0711 20:08:12.918053 25943 net.cpp:413] conv1b/bn -> conv1b/bn
I0711 20:08:12.921620 25943 net.cpp:148] Setting up conv1b/bn
I0711 20:08:12.921747 25943 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0711 20:08:12.921761 25943 net.cpp:163] Memory required for data: 327680000
I0711 20:08:12.921864 25943 layer_factory.hpp:77] Creating layer conv1b/relu
I0711 20:08:12.921875 25943 net.cpp:98] Creating Layer conv1b/relu
I0711 20:08:12.921880 25943 net.cpp:439] conv1b/relu <- conv1b/bn
I0711 20:08:12.921886 25943 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0711 20:08:12.921984 25943 net.cpp:148] Setting up conv1b/relu
I0711 20:08:12.921993 25943 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0711 20:08:12.921995 25943 net.cpp:163] Memory required for data: 380108800
I0711 20:08:12.921999 25943 layer_factory.hpp:77] Creating layer pool1
I0711 20:08:12.922127 25943 net.cpp:98] Creating Layer pool1
I0711 20:08:12.922135 25943 net.cpp:439] pool1 <- conv1b/bn
I0711 20:08:12.922142 25943 net.cpp:413] pool1 -> pool1
I0711 20:08:12.922281 25943 net.cpp:148] Setting up pool1
I0711 20:08:12.922289 25943 net.cpp:155] Top shape: 4 32 160 160 (3276800)
I0711 20:08:12.922293 25943 net.cpp:163] Memory required for data: 393216000
I0711 20:08:12.922353 25943 layer_factory.hpp:77] Creating layer res2a_branch2a
I0711 20:08:12.922410 25943 net.cpp:98] Creating Layer res2a_branch2a
I0711 20:08:12.922449 25943 net.cpp:439] res2a_branch2a <- pool1
I0711 20:08:12.922489 25943 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0711 20:08:12.923413 25943 net.cpp:148] Setting up res2a_branch2a
I0711 20:08:12.923421 25943 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0711 20:08:12.923424 25943 net.cpp:163] Memory required for data: 419430400
I0711 20:08:12.923434 25943 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0711 20:08:12.923549 25943 net.cpp:98] Creating Layer res2a_branch2a/bn
I0711 20:08:12.923555 25943 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0711 20:08:12.923560 25943 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0711 20:08:12.926616 25943 net.cpp:148] Setting up res2a_branch2a/bn
I0711 20:08:12.926628 25943 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0711 20:08:12.926630 25943 net.cpp:163] Memory required for data: 445644800
I0711 20:08:12.926636 25943 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0711 20:08:12.926641 25943 net.cpp:98] Creating Layer res2a_branch2a/relu
I0711 20:08:12.926643 25943 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0711 20:08:12.926646 25943 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0711 20:08:12.926651 25943 net.cpp:148] Setting up res2a_branch2a/relu
I0711 20:08:12.926653 25943 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0711 20:08:12.926656 25943 net.cpp:163] Memory required for data: 471859200
I0711 20:08:12.926657 25943 layer_factory.hpp:77] Creating layer res2a_branch2b
I0711 20:08:12.926664 25943 net.cpp:98] Creating Layer res2a_branch2b
I0711 20:08:12.926857 25943 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0711 20:08:12.926865 25943 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0711 20:08:12.928807 25943 net.cpp:148] Setting up res2a_branch2b
I0711 20:08:12.928860 25943 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0711 20:08:12.928872 25943 net.cpp:163] Memory required for data: 498073600
I0711 20:08:12.928889 25943 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0711 20:08:12.928910 25943 net.cpp:98] Creating Layer res2a_branch2b/bn
I0711 20:08:12.928923 25943 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0711 20:08:12.928938 25943 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0711 20:08:12.933954 25943 net.cpp:148] Setting up res2a_branch2b/bn
I0711 20:08:12.933986 25943 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0711 20:08:12.933990 25943 net.cpp:163] Memory required for data: 524288000
I0711 20:08:12.934002 25943 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0711 20:08:12.934015 25943 net.cpp:98] Creating Layer res2a_branch2b/relu
I0711 20:08:12.934023 25943 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0711 20:08:12.934029 25943 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0711 20:08:12.934042 25943 net.cpp:148] Setting up res2a_branch2b/relu
I0711 20:08:12.934048 25943 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0711 20:08:12.934051 25943 net.cpp:163] Memory required for data: 550502400
I0711 20:08:12.934056 25943 layer_factory.hpp:77] Creating layer pool2
I0711 20:08:12.934067 25943 net.cpp:98] Creating Layer pool2
I0711 20:08:12.934072 25943 net.cpp:439] pool2 <- res2a_branch2b/bn
I0711 20:08:12.934077 25943 net.cpp:413] pool2 -> pool2
I0711 20:08:12.934150 25943 net.cpp:148] Setting up pool2
I0711 20:08:12.934156 25943 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 20:08:12.934159 25943 net.cpp:163] Memory required for data: 557056000
I0711 20:08:12.934164 25943 layer_factory.hpp:77] Creating layer res3a_branch2a
I0711 20:08:12.934202 25943 net.cpp:98] Creating Layer res3a_branch2a
I0711 20:08:12.934207 25943 net.cpp:439] res3a_branch2a <- pool2
I0711 20:08:12.934213 25943 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0711 20:08:12.936187 25943 net.cpp:148] Setting up res3a_branch2a
I0711 20:08:12.936197 25943 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 20:08:12.936200 25943 net.cpp:163] Memory required for data: 570163200
I0711 20:08:12.936205 25943 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0711 20:08:12.936218 25943 net.cpp:98] Creating Layer res3a_branch2a/bn
I0711 20:08:12.936223 25943 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0711 20:08:12.936228 25943 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0711 20:08:12.937463 25943 net.cpp:148] Setting up res3a_branch2a/bn
I0711 20:08:12.937477 25943 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 20:08:12.937480 25943 net.cpp:163] Memory required for data: 583270400
I0711 20:08:12.937496 25943 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0711 20:08:12.937503 25943 net.cpp:98] Creating Layer res3a_branch2a/relu
I0711 20:08:12.937506 25943 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0711 20:08:12.937511 25943 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0711 20:08:12.937517 25943 net.cpp:148] Setting up res3a_branch2a/relu
I0711 20:08:12.937521 25943 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 20:08:12.937525 25943 net.cpp:163] Memory required for data: 596377600
I0711 20:08:12.937527 25943 layer_factory.hpp:77] Creating layer res3a_branch2b
I0711 20:08:12.937535 25943 net.cpp:98] Creating Layer res3a_branch2b
I0711 20:08:12.937537 25943 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0711 20:08:12.937542 25943 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0711 20:08:12.939002 25943 net.cpp:148] Setting up res3a_branch2b
I0711 20:08:12.939015 25943 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 20:08:12.939018 25943 net.cpp:163] Memory required for data: 609484800
I0711 20:08:12.939026 25943 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0711 20:08:12.939033 25943 net.cpp:98] Creating Layer res3a_branch2b/bn
I0711 20:08:12.939038 25943 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0711 20:08:12.939043 25943 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0711 20:08:12.940212 25943 net.cpp:148] Setting up res3a_branch2b/bn
I0711 20:08:12.940225 25943 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 20:08:12.940229 25943 net.cpp:163] Memory required for data: 622592000
I0711 20:08:12.940238 25943 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0711 20:08:12.940244 25943 net.cpp:98] Creating Layer res3a_branch2b/relu
I0711 20:08:12.940248 25943 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0711 20:08:12.940253 25943 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0711 20:08:12.940258 25943 net.cpp:148] Setting up res3a_branch2b/relu
I0711 20:08:12.940263 25943 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 20:08:12.940279 25943 net.cpp:163] Memory required for data: 635699200
I0711 20:08:12.940282 25943 layer_factory.hpp:77] Creating layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0711 20:08:12.940291 25943 net.cpp:98] Creating Layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0711 20:08:12.940295 25943 net.cpp:439] res3a_branch2b/bn_res3a_branch2b/relu_0_split <- res3a_branch2b/bn
I0711 20:08:12.940299 25943 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0711 20:08:12.940307 25943 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0711 20:08:12.940373 25943 net.cpp:148] Setting up res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0711 20:08:12.940381 25943 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 20:08:12.940385 25943 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 20:08:12.940388 25943 net.cpp:163] Memory required for data: 661913600
I0711 20:08:12.940409 25943 layer_factory.hpp:77] Creating layer pool3
I0711 20:08:12.940415 25943 net.cpp:98] Creating Layer pool3
I0711 20:08:12.940420 25943 net.cpp:439] pool3 <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0711 20:08:12.940425 25943 net.cpp:413] pool3 -> pool3
I0711 20:08:12.940500 25943 net.cpp:148] Setting up pool3
I0711 20:08:12.940505 25943 net.cpp:155] Top shape: 4 128 40 40 (819200)
I0711 20:08:12.940510 25943 net.cpp:163] Memory required for data: 665190400
I0711 20:08:12.940513 25943 layer_factory.hpp:77] Creating layer res4a_branch2a
I0711 20:08:12.940522 25943 net.cpp:98] Creating Layer res4a_branch2a
I0711 20:08:12.940526 25943 net.cpp:439] res4a_branch2a <- pool3
I0711 20:08:12.940532 25943 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0711 20:08:12.948571 25943 net.cpp:148] Setting up res4a_branch2a
I0711 20:08:12.948582 25943 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0711 20:08:12.948585 25943 net.cpp:163] Memory required for data: 671744000
I0711 20:08:12.948588 25943 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0711 20:08:12.948595 25943 net.cpp:98] Creating Layer res4a_branch2a/bn
I0711 20:08:12.948597 25943 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0711 20:08:12.948602 25943 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0711 20:08:12.949304 25943 net.cpp:148] Setting up res4a_branch2a/bn
I0711 20:08:12.949311 25943 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0711 20:08:12.949312 25943 net.cpp:163] Memory required for data: 678297600
I0711 20:08:12.949317 25943 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0711 20:08:12.949321 25943 net.cpp:98] Creating Layer res4a_branch2a/relu
I0711 20:08:12.949323 25943 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0711 20:08:12.949326 25943 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0711 20:08:12.949331 25943 net.cpp:148] Setting up res4a_branch2a/relu
I0711 20:08:12.949334 25943 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0711 20:08:12.949337 25943 net.cpp:163] Memory required for data: 684851200
I0711 20:08:12.949338 25943 layer_factory.hpp:77] Creating layer res4a_branch2b
I0711 20:08:12.949342 25943 net.cpp:98] Creating Layer res4a_branch2b
I0711 20:08:12.949345 25943 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0711 20:08:12.949348 25943 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0711 20:08:12.952553 25943 net.cpp:148] Setting up res4a_branch2b
I0711 20:08:12.952558 25943 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0711 20:08:12.952560 25943 net.cpp:163] Memory required for data: 691404800
I0711 20:08:12.952564 25943 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0711 20:08:12.952567 25943 net.cpp:98] Creating Layer res4a_branch2b/bn
I0711 20:08:12.952570 25943 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0711 20:08:12.952574 25943 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0711 20:08:12.953260 25943 net.cpp:148] Setting up res4a_branch2b/bn
I0711 20:08:12.953265 25943 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0711 20:08:12.953268 25943 net.cpp:163] Memory required for data: 697958400
I0711 20:08:12.953272 25943 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0711 20:08:12.953275 25943 net.cpp:98] Creating Layer res4a_branch2b/relu
I0711 20:08:12.953277 25943 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0711 20:08:12.953279 25943 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0711 20:08:12.953284 25943 net.cpp:148] Setting up res4a_branch2b/relu
I0711 20:08:12.953285 25943 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0711 20:08:12.953286 25943 net.cpp:163] Memory required for data: 704512000
I0711 20:08:12.953289 25943 layer_factory.hpp:77] Creating layer pool4
I0711 20:08:12.953292 25943 net.cpp:98] Creating Layer pool4
I0711 20:08:12.953294 25943 net.cpp:439] pool4 <- res4a_branch2b/bn
I0711 20:08:12.953295 25943 net.cpp:413] pool4 -> pool4
I0711 20:08:12.953336 25943 net.cpp:148] Setting up pool4
I0711 20:08:12.953339 25943 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0711 20:08:12.953349 25943 net.cpp:163] Memory required for data: 711065600
I0711 20:08:12.953351 25943 layer_factory.hpp:77] Creating layer res5a_branch2a
I0711 20:08:12.953356 25943 net.cpp:98] Creating Layer res5a_branch2a
I0711 20:08:12.953358 25943 net.cpp:439] res5a_branch2a <- pool4
I0711 20:08:12.953361 25943 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0711 20:08:12.978256 25943 net.cpp:148] Setting up res5a_branch2a
I0711 20:08:12.978272 25943 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0711 20:08:12.978274 25943 net.cpp:163] Memory required for data: 724172800
I0711 20:08:12.978279 25943 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0711 20:08:12.978288 25943 net.cpp:98] Creating Layer res5a_branch2a/bn
I0711 20:08:12.978291 25943 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0711 20:08:12.978294 25943 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0711 20:08:12.978998 25943 net.cpp:148] Setting up res5a_branch2a/bn
I0711 20:08:12.979004 25943 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0711 20:08:12.979007 25943 net.cpp:163] Memory required for data: 737280000
I0711 20:08:12.979012 25943 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0711 20:08:12.979014 25943 net.cpp:98] Creating Layer res5a_branch2a/relu
I0711 20:08:12.979017 25943 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0711 20:08:12.979019 25943 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0711 20:08:12.979023 25943 net.cpp:148] Setting up res5a_branch2a/relu
I0711 20:08:12.979025 25943 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0711 20:08:12.979027 25943 net.cpp:163] Memory required for data: 750387200
I0711 20:08:12.979029 25943 layer_factory.hpp:77] Creating layer res5a_branch2b
I0711 20:08:12.979033 25943 net.cpp:98] Creating Layer res5a_branch2b
I0711 20:08:12.979035 25943 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0711 20:08:12.979038 25943 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0711 20:08:12.992012 25943 net.cpp:148] Setting up res5a_branch2b
I0711 20:08:12.992033 25943 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0711 20:08:12.992034 25943 net.cpp:163] Memory required for data: 763494400
I0711 20:08:12.992043 25943 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0711 20:08:12.992049 25943 net.cpp:98] Creating Layer res5a_branch2b/bn
I0711 20:08:12.992053 25943 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0711 20:08:12.992058 25943 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0711 20:08:12.992774 25943 net.cpp:148] Setting up res5a_branch2b/bn
I0711 20:08:12.992780 25943 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0711 20:08:12.992782 25943 net.cpp:163] Memory required for data: 776601600
I0711 20:08:12.992787 25943 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0711 20:08:12.992790 25943 net.cpp:98] Creating Layer res5a_branch2b/relu
I0711 20:08:12.992794 25943 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0711 20:08:12.992795 25943 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0711 20:08:12.992799 25943 net.cpp:148] Setting up res5a_branch2b/relu
I0711 20:08:12.992801 25943 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0711 20:08:12.992804 25943 net.cpp:163] Memory required for data: 789708800
I0711 20:08:12.992805 25943 layer_factory.hpp:77] Creating layer out5a
I0711 20:08:12.992810 25943 net.cpp:98] Creating Layer out5a
I0711 20:08:12.992812 25943 net.cpp:439] out5a <- res5a_branch2b/bn
I0711 20:08:12.992818 25943 net.cpp:413] out5a -> out5a
I0711 20:08:12.997078 25943 net.cpp:148] Setting up out5a
I0711 20:08:12.997087 25943 net.cpp:155] Top shape: 4 64 40 40 (409600)
I0711 20:08:12.997089 25943 net.cpp:163] Memory required for data: 791347200
I0711 20:08:12.997093 25943 layer_factory.hpp:77] Creating layer out5a/bn
I0711 20:08:12.997098 25943 net.cpp:98] Creating Layer out5a/bn
I0711 20:08:12.997100 25943 net.cpp:439] out5a/bn <- out5a
I0711 20:08:12.997103 25943 net.cpp:413] out5a/bn -> out5a/bn
I0711 20:08:12.997874 25943 net.cpp:148] Setting up out5a/bn
I0711 20:08:12.997880 25943 net.cpp:155] Top shape: 4 64 40 40 (409600)
I0711 20:08:12.997890 25943 net.cpp:163] Memory required for data: 792985600
I0711 20:08:12.997895 25943 layer_factory.hpp:77] Creating layer out5a/relu
I0711 20:08:12.997900 25943 net.cpp:98] Creating Layer out5a/relu
I0711 20:08:12.997901 25943 net.cpp:439] out5a/relu <- out5a/bn
I0711 20:08:12.997903 25943 net.cpp:400] out5a/relu -> out5a/bn (in-place)
I0711 20:08:12.997907 25943 net.cpp:148] Setting up out5a/relu
I0711 20:08:12.997910 25943 net.cpp:155] Top shape: 4 64 40 40 (409600)
I0711 20:08:12.997911 25943 net.cpp:163] Memory required for data: 794624000
I0711 20:08:12.997913 25943 layer_factory.hpp:77] Creating layer out5a_up2
I0711 20:08:12.997917 25943 net.cpp:98] Creating Layer out5a_up2
I0711 20:08:12.997920 25943 net.cpp:439] out5a_up2 <- out5a/bn
I0711 20:08:12.997922 25943 net.cpp:413] out5a_up2 -> out5a_up2
I0711 20:08:12.998190 25943 net.cpp:148] Setting up out5a_up2
I0711 20:08:12.998195 25943 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 20:08:12.998196 25943 net.cpp:163] Memory required for data: 801177600
I0711 20:08:12.998199 25943 layer_factory.hpp:77] Creating layer out3a
I0711 20:08:12.998203 25943 net.cpp:98] Creating Layer out3a
I0711 20:08:12.998205 25943 net.cpp:439] out3a <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0711 20:08:12.998209 25943 net.cpp:413] out3a -> out3a
I0711 20:08:13.000190 25943 net.cpp:148] Setting up out3a
I0711 20:08:13.000197 25943 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 20:08:13.000200 25943 net.cpp:163] Memory required for data: 807731200
I0711 20:08:13.000203 25943 layer_factory.hpp:77] Creating layer out3a/bn
I0711 20:08:13.000207 25943 net.cpp:98] Creating Layer out3a/bn
I0711 20:08:13.000210 25943 net.cpp:439] out3a/bn <- out3a
I0711 20:08:13.000212 25943 net.cpp:413] out3a/bn -> out3a/bn
I0711 20:08:13.001003 25943 net.cpp:148] Setting up out3a/bn
I0711 20:08:13.001009 25943 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 20:08:13.001011 25943 net.cpp:163] Memory required for data: 814284800
I0711 20:08:13.001016 25943 layer_factory.hpp:77] Creating layer out3a/relu
I0711 20:08:13.001019 25943 net.cpp:98] Creating Layer out3a/relu
I0711 20:08:13.001021 25943 net.cpp:439] out3a/relu <- out3a/bn
I0711 20:08:13.001024 25943 net.cpp:400] out3a/relu -> out3a/bn (in-place)
I0711 20:08:13.001027 25943 net.cpp:148] Setting up out3a/relu
I0711 20:08:13.001029 25943 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 20:08:13.001030 25943 net.cpp:163] Memory required for data: 820838400
I0711 20:08:13.001032 25943 layer_factory.hpp:77] Creating layer out3_out5_combined
I0711 20:08:13.001035 25943 net.cpp:98] Creating Layer out3_out5_combined
I0711 20:08:13.001037 25943 net.cpp:439] out3_out5_combined <- out5a_up2
I0711 20:08:13.001039 25943 net.cpp:439] out3_out5_combined <- out3a/bn
I0711 20:08:13.001042 25943 net.cpp:413] out3_out5_combined -> out3_out5_combined
I0711 20:08:13.001066 25943 net.cpp:148] Setting up out3_out5_combined
I0711 20:08:13.001070 25943 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 20:08:13.001071 25943 net.cpp:163] Memory required for data: 827392000
I0711 20:08:13.001073 25943 layer_factory.hpp:77] Creating layer ctx_conv1
I0711 20:08:13.001077 25943 net.cpp:98] Creating Layer ctx_conv1
I0711 20:08:13.001080 25943 net.cpp:439] ctx_conv1 <- out3_out5_combined
I0711 20:08:13.001082 25943 net.cpp:413] ctx_conv1 -> ctx_conv1
I0711 20:08:13.002125 25943 net.cpp:148] Setting up ctx_conv1
I0711 20:08:13.002130 25943 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 20:08:13.002131 25943 net.cpp:163] Memory required for data: 833945600
I0711 20:08:13.002135 25943 layer_factory.hpp:77] Creating layer ctx_conv1/bn
I0711 20:08:13.002137 25943 net.cpp:98] Creating Layer ctx_conv1/bn
I0711 20:08:13.002140 25943 net.cpp:439] ctx_conv1/bn <- ctx_conv1
I0711 20:08:13.002142 25943 net.cpp:413] ctx_conv1/bn -> ctx_conv1/bn
I0711 20:08:13.002915 25943 net.cpp:148] Setting up ctx_conv1/bn
I0711 20:08:13.002920 25943 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 20:08:13.002923 25943 net.cpp:163] Memory required for data: 840499200
I0711 20:08:13.002934 25943 layer_factory.hpp:77] Creating layer ctx_conv1/relu
I0711 20:08:13.002938 25943 net.cpp:98] Creating Layer ctx_conv1/relu
I0711 20:08:13.002939 25943 net.cpp:439] ctx_conv1/relu <- ctx_conv1/bn
I0711 20:08:13.002941 25943 net.cpp:400] ctx_conv1/relu -> ctx_conv1/bn (in-place)
I0711 20:08:13.002945 25943 net.cpp:148] Setting up ctx_conv1/relu
I0711 20:08:13.002948 25943 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 20:08:13.002949 25943 net.cpp:163] Memory required for data: 847052800
I0711 20:08:13.002951 25943 layer_factory.hpp:77] Creating layer ctx_conv2
I0711 20:08:13.002956 25943 net.cpp:98] Creating Layer ctx_conv2
I0711 20:08:13.002959 25943 net.cpp:439] ctx_conv2 <- ctx_conv1/bn
I0711 20:08:13.002961 25943 net.cpp:413] ctx_conv2 -> ctx_conv2
I0711 20:08:13.004005 25943 net.cpp:148] Setting up ctx_conv2
I0711 20:08:13.004010 25943 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 20:08:13.004012 25943 net.cpp:163] Memory required for data: 853606400
I0711 20:08:13.004015 25943 layer_factory.hpp:77] Creating layer ctx_conv2/bn
I0711 20:08:13.004019 25943 net.cpp:98] Creating Layer ctx_conv2/bn
I0711 20:08:13.004020 25943 net.cpp:439] ctx_conv2/bn <- ctx_conv2
I0711 20:08:13.004024 25943 net.cpp:413] ctx_conv2/bn -> ctx_conv2/bn
I0711 20:08:13.004806 25943 net.cpp:148] Setting up ctx_conv2/bn
I0711 20:08:13.004811 25943 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 20:08:13.004812 25943 net.cpp:163] Memory required for data: 860160000
I0711 20:08:13.004820 25943 layer_factory.hpp:77] Creating layer ctx_conv2/relu
I0711 20:08:13.004823 25943 net.cpp:98] Creating Layer ctx_conv2/relu
I0711 20:08:13.004827 25943 net.cpp:439] ctx_conv2/relu <- ctx_conv2/bn
I0711 20:08:13.004828 25943 net.cpp:400] ctx_conv2/relu -> ctx_conv2/bn (in-place)
I0711 20:08:13.004832 25943 net.cpp:148] Setting up ctx_conv2/relu
I0711 20:08:13.004834 25943 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 20:08:13.004835 25943 net.cpp:163] Memory required for data: 866713600
I0711 20:08:13.004837 25943 layer_factory.hpp:77] Creating layer ctx_conv3
I0711 20:08:13.004842 25943 net.cpp:98] Creating Layer ctx_conv3
I0711 20:08:13.004843 25943 net.cpp:439] ctx_conv3 <- ctx_conv2/bn
I0711 20:08:13.004847 25943 net.cpp:413] ctx_conv3 -> ctx_conv3
I0711 20:08:13.005889 25943 net.cpp:148] Setting up ctx_conv3
I0711 20:08:13.005894 25943 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 20:08:13.005895 25943 net.cpp:163] Memory required for data: 873267200
I0711 20:08:13.005898 25943 layer_factory.hpp:77] Creating layer ctx_conv3/bn
I0711 20:08:13.005903 25943 net.cpp:98] Creating Layer ctx_conv3/bn
I0711 20:08:13.005904 25943 net.cpp:439] ctx_conv3/bn <- ctx_conv3
I0711 20:08:13.005908 25943 net.cpp:413] ctx_conv3/bn -> ctx_conv3/bn
I0711 20:08:13.006677 25943 net.cpp:148] Setting up ctx_conv3/bn
I0711 20:08:13.006682 25943 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 20:08:13.006685 25943 net.cpp:163] Memory required for data: 879820800
I0711 20:08:13.006690 25943 layer_factory.hpp:77] Creating layer ctx_conv3/relu
I0711 20:08:13.006692 25943 net.cpp:98] Creating Layer ctx_conv3/relu
I0711 20:08:13.006695 25943 net.cpp:439] ctx_conv3/relu <- ctx_conv3/bn
I0711 20:08:13.006696 25943 net.cpp:400] ctx_conv3/relu -> ctx_conv3/bn (in-place)
I0711 20:08:13.006700 25943 net.cpp:148] Setting up ctx_conv3/relu
I0711 20:08:13.006702 25943 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 20:08:13.006703 25943 net.cpp:163] Memory required for data: 886374400
I0711 20:08:13.006705 25943 layer_factory.hpp:77] Creating layer ctx_conv4
I0711 20:08:13.006709 25943 net.cpp:98] Creating Layer ctx_conv4
I0711 20:08:13.006711 25943 net.cpp:439] ctx_conv4 <- ctx_conv3/bn
I0711 20:08:13.006713 25943 net.cpp:413] ctx_conv4 -> ctx_conv4
I0711 20:08:13.007755 25943 net.cpp:148] Setting up ctx_conv4
I0711 20:08:13.007760 25943 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 20:08:13.007761 25943 net.cpp:163] Memory required for data: 892928000
I0711 20:08:13.007764 25943 layer_factory.hpp:77] Creating layer ctx_conv4/bn
I0711 20:08:13.007772 25943 net.cpp:98] Creating Layer ctx_conv4/bn
I0711 20:08:13.007774 25943 net.cpp:439] ctx_conv4/bn <- ctx_conv4
I0711 20:08:13.007777 25943 net.cpp:413] ctx_conv4/bn -> ctx_conv4/bn
I0711 20:08:13.008548 25943 net.cpp:148] Setting up ctx_conv4/bn
I0711 20:08:13.008553 25943 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 20:08:13.008555 25943 net.cpp:163] Memory required for data: 899481600
I0711 20:08:13.008559 25943 layer_factory.hpp:77] Creating layer ctx_conv4/relu
I0711 20:08:13.008563 25943 net.cpp:98] Creating Layer ctx_conv4/relu
I0711 20:08:13.008564 25943 net.cpp:439] ctx_conv4/relu <- ctx_conv4/bn
I0711 20:08:13.008566 25943 net.cpp:400] ctx_conv4/relu -> ctx_conv4/bn (in-place)
I0711 20:08:13.008569 25943 net.cpp:148] Setting up ctx_conv4/relu
I0711 20:08:13.008572 25943 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 20:08:13.008574 25943 net.cpp:163] Memory required for data: 906035200
I0711 20:08:13.008575 25943 layer_factory.hpp:77] Creating layer ctx_final
I0711 20:08:13.008579 25943 net.cpp:98] Creating Layer ctx_final
I0711 20:08:13.008580 25943 net.cpp:439] ctx_final <- ctx_conv4/bn
I0711 20:08:13.008582 25943 net.cpp:413] ctx_final -> ctx_final
I0711 20:08:13.008999 25943 net.cpp:148] Setting up ctx_final
I0711 20:08:13.009006 25943 net.cpp:155] Top shape: 4 8 80 80 (204800)
I0711 20:08:13.009007 25943 net.cpp:163] Memory required for data: 906854400
I0711 20:08:13.009011 25943 layer_factory.hpp:77] Creating layer ctx_final/relu
I0711 20:08:13.009012 25943 net.cpp:98] Creating Layer ctx_final/relu
I0711 20:08:13.009014 25943 net.cpp:439] ctx_final/relu <- ctx_final
I0711 20:08:13.009016 25943 net.cpp:400] ctx_final/relu -> ctx_final (in-place)
I0711 20:08:13.009019 25943 net.cpp:148] Setting up ctx_final/relu
I0711 20:08:13.009022 25943 net.cpp:155] Top shape: 4 8 80 80 (204800)
I0711 20:08:13.009023 25943 net.cpp:163] Memory required for data: 907673600
I0711 20:08:13.009026 25943 layer_factory.hpp:77] Creating layer out_deconv_final_up2
I0711 20:08:13.009028 25943 net.cpp:98] Creating Layer out_deconv_final_up2
I0711 20:08:13.009030 25943 net.cpp:439] out_deconv_final_up2 <- ctx_final
I0711 20:08:13.009032 25943 net.cpp:413] out_deconv_final_up2 -> out_deconv_final_up2
I0711 20:08:13.009279 25943 net.cpp:148] Setting up out_deconv_final_up2
I0711 20:08:13.009284 25943 net.cpp:155] Top shape: 4 8 160 160 (819200)
I0711 20:08:13.009285 25943 net.cpp:163] Memory required for data: 910950400
I0711 20:08:13.009287 25943 layer_factory.hpp:77] Creating layer out_deconv_final_up4
I0711 20:08:13.009290 25943 net.cpp:98] Creating Layer out_deconv_final_up4
I0711 20:08:13.009292 25943 net.cpp:439] out_deconv_final_up4 <- out_deconv_final_up2
I0711 20:08:13.009295 25943 net.cpp:413] out_deconv_final_up4 -> out_deconv_final_up4
I0711 20:08:13.009536 25943 net.cpp:148] Setting up out_deconv_final_up4
I0711 20:08:13.009541 25943 net.cpp:155] Top shape: 4 8 320 320 (3276800)
I0711 20:08:13.009542 25943 net.cpp:163] Memory required for data: 924057600
I0711 20:08:13.009546 25943 layer_factory.hpp:77] Creating layer out_deconv_final_up8
I0711 20:08:13.009547 25943 net.cpp:98] Creating Layer out_deconv_final_up8
I0711 20:08:13.009549 25943 net.cpp:439] out_deconv_final_up8 <- out_deconv_final_up4
I0711 20:08:13.009552 25943 net.cpp:413] out_deconv_final_up8 -> out_deconv_final_up8
I0711 20:08:13.009790 25943 net.cpp:148] Setting up out_deconv_final_up8
I0711 20:08:13.009794 25943 net.cpp:155] Top shape: 4 8 640 640 (13107200)
I0711 20:08:13.009796 25943 net.cpp:163] Memory required for data: 976486400
I0711 20:08:13.009799 25943 layer_factory.hpp:77] Creating layer out_deconv_final_up8_out_deconv_final_up8_0_split
I0711 20:08:13.009802 25943 net.cpp:98] Creating Layer out_deconv_final_up8_out_deconv_final_up8_0_split
I0711 20:08:13.009804 25943 net.cpp:439] out_deconv_final_up8_out_deconv_final_up8_0_split <- out_deconv_final_up8
I0711 20:08:13.009806 25943 net.cpp:413] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0711 20:08:13.009814 25943 net.cpp:413] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0711 20:08:13.009817 25943 net.cpp:413] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0711 20:08:13.009876 25943 net.cpp:148] Setting up out_deconv_final_up8_out_deconv_final_up8_0_split
I0711 20:08:13.009881 25943 net.cpp:155] Top shape: 4 8 640 640 (13107200)
I0711 20:08:13.009882 25943 net.cpp:155] Top shape: 4 8 640 640 (13107200)
I0711 20:08:13.009884 25943 net.cpp:155] Top shape: 4 8 640 640 (13107200)
I0711 20:08:13.009886 25943 net.cpp:163] Memory required for data: 1133772800
I0711 20:08:13.009888 25943 layer_factory.hpp:77] Creating layer loss
I0711 20:08:13.009891 25943 net.cpp:98] Creating Layer loss
I0711 20:08:13.009894 25943 net.cpp:439] loss <- out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0711 20:08:13.009896 25943 net.cpp:439] loss <- label_data_1_split_0
I0711 20:08:13.009899 25943 net.cpp:413] loss -> loss
I0711 20:08:13.009903 25943 layer_factory.hpp:77] Creating layer loss
I0711 20:08:13.027357 25943 net.cpp:148] Setting up loss
I0711 20:08:13.027376 25943 net.cpp:155] Top shape: (1)
I0711 20:08:13.027379 25943 net.cpp:158]     with loss weight 1
I0711 20:08:13.027386 25943 net.cpp:163] Memory required for data: 1133772804
I0711 20:08:13.027390 25943 layer_factory.hpp:77] Creating layer accuracy/top1
I0711 20:08:13.027397 25943 net.cpp:98] Creating Layer accuracy/top1
I0711 20:08:13.027400 25943 net.cpp:439] accuracy/top1 <- out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0711 20:08:13.027405 25943 net.cpp:439] accuracy/top1 <- label_data_1_split_1
I0711 20:08:13.027408 25943 net.cpp:413] accuracy/top1 -> accuracy/top1
I0711 20:08:13.027416 25943 net.cpp:148] Setting up accuracy/top1
I0711 20:08:13.027420 25943 net.cpp:155] Top shape: (1)
I0711 20:08:13.027422 25943 net.cpp:163] Memory required for data: 1133772808
I0711 20:08:13.027425 25943 layer_factory.hpp:77] Creating layer accuracy/top5
I0711 20:08:13.027428 25943 net.cpp:98] Creating Layer accuracy/top5
I0711 20:08:13.027431 25943 net.cpp:439] accuracy/top5 <- out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0711 20:08:13.027433 25943 net.cpp:439] accuracy/top5 <- label_data_1_split_2
I0711 20:08:13.027436 25943 net.cpp:413] accuracy/top5 -> accuracy/top5
I0711 20:08:13.027441 25943 net.cpp:148] Setting up accuracy/top5
I0711 20:08:13.027443 25943 net.cpp:155] Top shape: (1)
I0711 20:08:13.027444 25943 net.cpp:163] Memory required for data: 1133772812
I0711 20:08:13.027446 25943 net.cpp:226] accuracy/top5 does not need backward computation.
I0711 20:08:13.027449 25943 net.cpp:226] accuracy/top1 does not need backward computation.
I0711 20:08:13.027451 25943 net.cpp:224] loss needs backward computation.
I0711 20:08:13.027454 25943 net.cpp:224] out_deconv_final_up8_out_deconv_final_up8_0_split needs backward computation.
I0711 20:08:13.027456 25943 net.cpp:224] out_deconv_final_up8 needs backward computation.
I0711 20:08:13.027459 25943 net.cpp:224] out_deconv_final_up4 needs backward computation.
I0711 20:08:13.027462 25943 net.cpp:224] out_deconv_final_up2 needs backward computation.
I0711 20:08:13.027463 25943 net.cpp:224] ctx_final/relu needs backward computation.
I0711 20:08:13.027465 25943 net.cpp:224] ctx_final needs backward computation.
I0711 20:08:13.027467 25943 net.cpp:224] ctx_conv4/relu needs backward computation.
I0711 20:08:13.027469 25943 net.cpp:224] ctx_conv4/bn needs backward computation.
I0711 20:08:13.027472 25943 net.cpp:224] ctx_conv4 needs backward computation.
I0711 20:08:13.027474 25943 net.cpp:224] ctx_conv3/relu needs backward computation.
I0711 20:08:13.027477 25943 net.cpp:224] ctx_conv3/bn needs backward computation.
I0711 20:08:13.027479 25943 net.cpp:224] ctx_conv3 needs backward computation.
I0711 20:08:13.027482 25943 net.cpp:224] ctx_conv2/relu needs backward computation.
I0711 20:08:13.027484 25943 net.cpp:224] ctx_conv2/bn needs backward computation.
I0711 20:08:13.027496 25943 net.cpp:224] ctx_conv2 needs backward computation.
I0711 20:08:13.027498 25943 net.cpp:224] ctx_conv1/relu needs backward computation.
I0711 20:08:13.027500 25943 net.cpp:224] ctx_conv1/bn needs backward computation.
I0711 20:08:13.027503 25943 net.cpp:224] ctx_conv1 needs backward computation.
I0711 20:08:13.027504 25943 net.cpp:224] out3_out5_combined needs backward computation.
I0711 20:08:13.027508 25943 net.cpp:224] out3a/relu needs backward computation.
I0711 20:08:13.027509 25943 net.cpp:224] out3a/bn needs backward computation.
I0711 20:08:13.027511 25943 net.cpp:224] out3a needs backward computation.
I0711 20:08:13.027513 25943 net.cpp:224] out5a_up2 needs backward computation.
I0711 20:08:13.027515 25943 net.cpp:224] out5a/relu needs backward computation.
I0711 20:08:13.027518 25943 net.cpp:224] out5a/bn needs backward computation.
I0711 20:08:13.027519 25943 net.cpp:224] out5a needs backward computation.
I0711 20:08:13.027521 25943 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0711 20:08:13.027523 25943 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0711 20:08:13.027525 25943 net.cpp:224] res5a_branch2b needs backward computation.
I0711 20:08:13.027529 25943 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0711 20:08:13.027529 25943 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0711 20:08:13.027531 25943 net.cpp:224] res5a_branch2a needs backward computation.
I0711 20:08:13.027534 25943 net.cpp:224] pool4 needs backward computation.
I0711 20:08:13.027537 25943 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0711 20:08:13.027540 25943 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0711 20:08:13.027542 25943 net.cpp:224] res4a_branch2b needs backward computation.
I0711 20:08:13.027545 25943 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0711 20:08:13.027547 25943 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0711 20:08:13.027549 25943 net.cpp:224] res4a_branch2a needs backward computation.
I0711 20:08:13.027552 25943 net.cpp:224] pool3 needs backward computation.
I0711 20:08:13.027554 25943 net.cpp:224] res3a_branch2b/bn_res3a_branch2b/relu_0_split needs backward computation.
I0711 20:08:13.027557 25943 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0711 20:08:13.027559 25943 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0711 20:08:13.027562 25943 net.cpp:224] res3a_branch2b needs backward computation.
I0711 20:08:13.027565 25943 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0711 20:08:13.027567 25943 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0711 20:08:13.027570 25943 net.cpp:224] res3a_branch2a needs backward computation.
I0711 20:08:13.027572 25943 net.cpp:224] pool2 needs backward computation.
I0711 20:08:13.027575 25943 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0711 20:08:13.027577 25943 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0711 20:08:13.027580 25943 net.cpp:224] res2a_branch2b needs backward computation.
I0711 20:08:13.027582 25943 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0711 20:08:13.027585 25943 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0711 20:08:13.027587 25943 net.cpp:224] res2a_branch2a needs backward computation.
I0711 20:08:13.027590 25943 net.cpp:224] pool1 needs backward computation.
I0711 20:08:13.027591 25943 net.cpp:224] conv1b/relu needs backward computation.
I0711 20:08:13.027595 25943 net.cpp:224] conv1b/bn needs backward computation.
I0711 20:08:13.027596 25943 net.cpp:224] conv1b needs backward computation.
I0711 20:08:13.027600 25943 net.cpp:224] conv1a/relu needs backward computation.
I0711 20:08:13.027601 25943 net.cpp:224] conv1a/bn needs backward computation.
I0711 20:08:13.027603 25943 net.cpp:224] conv1a needs backward computation.
I0711 20:08:13.027606 25943 net.cpp:226] data/bias does not need backward computation.
I0711 20:08:13.027609 25943 net.cpp:226] label_data_1_split does not need backward computation.
I0711 20:08:13.027614 25943 net.cpp:226] data does not need backward computation.
I0711 20:08:13.027616 25943 net.cpp:268] This network produces output accuracy/top1
I0711 20:08:13.027619 25943 net.cpp:268] This network produces output accuracy/top5
I0711 20:08:13.027621 25943 net.cpp:268] This network produces output loss
I0711 20:08:13.027652 25943 net.cpp:288] Network initialization done.
I0711 20:08:13.027743 25943 solver.cpp:60] Solver scaffolding done.
I0711 20:08:13.035576 25943 caffe.cpp:145] Finetuning from training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/initial/cityscapes5_jsegnet21v2_iter_32000.caffemodel
I0711 20:08:13.067530 25943 data_layer.cpp:78] ReshapePrefetch 5, 3, 640, 640
I0711 20:08:13.067596 25943 data_layer.cpp:83] output data size: 5,3,640,640
I0711 20:08:13.103935 25943 data_layer.cpp:78] ReshapePrefetch 5, 1, 640, 640
I0711 20:08:13.104014 25943 data_layer.cpp:83] output data size: 5,1,640,640
I0711 20:08:13.636772 25943 data_layer.cpp:78] ReshapePrefetch 5, 3, 640, 640
I0711 20:08:13.637950 25943 data_layer.cpp:83] output data size: 5,3,640,640
I0711 20:08:13.739501 25943 data_layer.cpp:78] ReshapePrefetch 5, 1, 640, 640
I0711 20:08:13.739666 25943 data_layer.cpp:83] output data size: 5,1,640,640
I0711 20:08:14.755390 25943 parallel.cpp:334] Starting Optimization
I0711 20:08:14.755450 25943 solver.cpp:409] Solving jsegnet21v2_train
I0711 20:08:14.755453 25943 solver.cpp:410] Learning Rate Policy: multistep
I0711 20:08:15.206455 25943 solver.cpp:290] Iteration 0 (0 iter/s, 0.450967s/100 iter), loss = 0.0168362
I0711 20:08:15.206477 25943 solver.cpp:309]     Train net output #0: loss = 0.0168362 (* 1 = 0.0168362 loss)
I0711 20:08:15.206485 25943 sgd_solver.cpp:106] Iteration 0, lr = 1e-05
I0711 20:08:32.432417 25943 solver.cpp:290] Iteration 100 (5.80536 iter/s, 17.2255s/100 iter), loss = 0.0228012
I0711 20:08:32.432443 25943 solver.cpp:309]     Train net output #0: loss = 0.0228012 (* 1 = 0.0228012 loss)
I0711 20:08:32.432451 25943 sgd_solver.cpp:106] Iteration 100, lr = 1e-05
I0711 20:08:46.582504 26105 blocking_queue.cpp:50] Data layer prefetch queue empty
I0711 20:09:05.486091 25943 solver.cpp:290] Iteration 200 (3.02547 iter/s, 33.0527s/100 iter), loss = 0.0217583
I0711 20:09:05.486119 25943 solver.cpp:309]     Train net output #0: loss = 0.0217583 (* 1 = 0.0217583 loss)
I0711 20:09:05.486129 25943 sgd_solver.cpp:106] Iteration 200, lr = 1e-05
I0711 20:09:31.778726 25943 solver.cpp:290] Iteration 300 (3.80345 iter/s, 26.2919s/100 iter), loss = 0.0295088
I0711 20:09:31.778801 25943 solver.cpp:309]     Train net output #0: loss = 0.0295088 (* 1 = 0.0295088 loss)
I0711 20:09:31.778808 25943 sgd_solver.cpp:106] Iteration 300, lr = 1e-05
I0711 20:09:54.513065 25943 solver.cpp:290] Iteration 400 (4.39877 iter/s, 22.7336s/100 iter), loss = 0.0187619
I0711 20:09:54.513092 25943 solver.cpp:309]     Train net output #0: loss = 0.0187619 (* 1 = 0.0187619 loss)
I0711 20:09:54.513101 25943 sgd_solver.cpp:106] Iteration 400, lr = 1e-05
I0711 20:10:11.678979 25943 solver.cpp:290] Iteration 500 (5.82567 iter/s, 17.1654s/100 iter), loss = 0.0248106
I0711 20:10:11.679062 25943 solver.cpp:309]     Train net output #0: loss = 0.0248106 (* 1 = 0.0248106 loss)
I0711 20:10:11.679074 25943 sgd_solver.cpp:106] Iteration 500, lr = 1e-05
I0711 20:10:28.960304 25943 solver.cpp:290] Iteration 600 (5.78678 iter/s, 17.2808s/100 iter), loss = 0.016377
I0711 20:10:28.960327 25943 solver.cpp:309]     Train net output #0: loss = 0.016377 (* 1 = 0.016377 loss)
I0711 20:10:28.960335 25943 sgd_solver.cpp:106] Iteration 600, lr = 1e-05
I0711 20:10:46.109261 25943 solver.cpp:290] Iteration 700 (5.83143 iter/s, 17.1485s/100 iter), loss = 0.0246561
I0711 20:10:46.109308 25943 solver.cpp:309]     Train net output #0: loss = 0.0246561 (* 1 = 0.0246561 loss)
I0711 20:10:46.109314 25943 sgd_solver.cpp:106] Iteration 700, lr = 1e-05
I0711 20:11:03.247766 25943 solver.cpp:290] Iteration 800 (5.83499 iter/s, 17.138s/100 iter), loss = 0.0199883
I0711 20:11:03.247790 25943 solver.cpp:309]     Train net output #0: loss = 0.0199883 (* 1 = 0.0199883 loss)
I0711 20:11:03.247797 25943 sgd_solver.cpp:106] Iteration 800, lr = 1e-05
I0711 20:11:20.375475 25943 solver.cpp:290] Iteration 900 (5.83866 iter/s, 17.1272s/100 iter), loss = 0.0125264
I0711 20:11:20.375602 25943 solver.cpp:309]     Train net output #0: loss = 0.0125264 (* 1 = 0.0125264 loss)
I0711 20:11:20.375612 25943 sgd_solver.cpp:106] Iteration 900, lr = 1e-05
I0711 20:11:37.540657 25943 solver.cpp:290] Iteration 1000 (5.82595 iter/s, 17.1646s/100 iter), loss = 0.0273993
I0711 20:11:37.540680 25943 solver.cpp:309]     Train net output #0: loss = 0.0273993 (* 1 = 0.0273993 loss)
I0711 20:11:37.540688 25943 sgd_solver.cpp:106] Iteration 1000, lr = 1e-05
I0711 20:11:54.742923 25943 solver.cpp:290] Iteration 1100 (5.81336 iter/s, 17.2018s/100 iter), loss = 0.0271729
I0711 20:11:54.742976 25943 solver.cpp:309]     Train net output #0: loss = 0.0271729 (* 1 = 0.0271729 loss)
I0711 20:11:54.742983 25943 sgd_solver.cpp:106] Iteration 1100, lr = 1e-05
I0711 20:12:11.894206 25943 solver.cpp:290] Iteration 1200 (5.83065 iter/s, 17.1508s/100 iter), loss = 0.0293711
I0711 20:12:11.894229 25943 solver.cpp:309]     Train net output #0: loss = 0.0293711 (* 1 = 0.0293711 loss)
I0711 20:12:11.894237 25943 sgd_solver.cpp:106] Iteration 1200, lr = 1e-05
I0711 20:12:29.084630 25943 solver.cpp:290] Iteration 1300 (5.81736 iter/s, 17.1899s/100 iter), loss = 0.0162563
I0711 20:12:29.084677 25943 solver.cpp:309]     Train net output #0: loss = 0.0162563 (* 1 = 0.0162563 loss)
I0711 20:12:29.084684 25943 sgd_solver.cpp:106] Iteration 1300, lr = 1e-05
I0711 20:12:46.339589 25943 solver.cpp:290] Iteration 1400 (5.79561 iter/s, 17.2544s/100 iter), loss = 0.0423282
I0711 20:12:46.339613 25943 solver.cpp:309]     Train net output #0: loss = 0.0423282 (* 1 = 0.0423282 loss)
I0711 20:12:46.339620 25943 sgd_solver.cpp:106] Iteration 1400, lr = 1e-05
I0711 20:13:03.597748 25943 solver.cpp:290] Iteration 1500 (5.79453 iter/s, 17.2577s/100 iter), loss = 0.0172704
I0711 20:13:03.597838 25943 solver.cpp:309]     Train net output #0: loss = 0.0172704 (* 1 = 0.0172704 loss)
I0711 20:13:03.597848 25943 sgd_solver.cpp:106] Iteration 1500, lr = 1e-05
I0711 20:13:20.801759 25943 solver.cpp:290] Iteration 1600 (5.81279 iter/s, 17.2034s/100 iter), loss = 0.0226167
I0711 20:13:20.801782 25943 solver.cpp:309]     Train net output #0: loss = 0.0226167 (* 1 = 0.0226167 loss)
I0711 20:13:20.801790 25943 sgd_solver.cpp:106] Iteration 1600, lr = 1e-05
I0711 20:13:37.971395 25943 solver.cpp:290] Iteration 1700 (5.8244 iter/s, 17.1691s/100 iter), loss = 0.0162588
I0711 20:13:37.971489 25943 solver.cpp:309]     Train net output #0: loss = 0.0162588 (* 1 = 0.0162588 loss)
I0711 20:13:37.971498 25943 sgd_solver.cpp:106] Iteration 1700, lr = 1e-05
I0711 20:13:55.079634 25943 solver.cpp:290] Iteration 1800 (5.84533 iter/s, 17.1077s/100 iter), loss = 0.0327368
I0711 20:13:55.079658 25943 solver.cpp:309]     Train net output #0: loss = 0.0327368 (* 1 = 0.0327368 loss)
I0711 20:13:55.079665 25943 sgd_solver.cpp:106] Iteration 1800, lr = 1e-05
I0711 20:14:12.228041 25943 solver.cpp:290] Iteration 1900 (5.83161 iter/s, 17.1479s/100 iter), loss = 0.0276456
I0711 20:14:12.228458 25943 solver.cpp:309]     Train net output #0: loss = 0.0276455 (* 1 = 0.0276455 loss)
I0711 20:14:12.228469 25943 sgd_solver.cpp:106] Iteration 1900, lr = 1e-05
I0711 20:14:29.334561 25943 solver.cpp:467] Iteration 2000, Testing net (#0)
I0711 20:15:15.848827 25943 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.954646
I0711 20:15:15.848927 25943 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999677
I0711 20:15:15.848934 25943 solver.cpp:540]     Test net output #2: loss = 0.145338 (* 1 = 0.145338 loss)
I0711 20:15:16.036445 25943 solver.cpp:290] Iteration 2000 (1.56724 iter/s, 63.8063s/100 iter), loss = 0.0139777
I0711 20:15:16.036470 25943 solver.cpp:309]     Train net output #0: loss = 0.0139777 (* 1 = 0.0139777 loss)
I0711 20:15:16.036478 25943 sgd_solver.cpp:106] Iteration 2000, lr = 1e-05
I0711 20:15:31.814342 26061 blocking_queue.cpp:50] Waiting for data
I0711 20:15:44.022194 25943 solver.cpp:290] Iteration 2100 (3.57335 iter/s, 27.985s/100 iter), loss = 0.0228779
I0711 20:15:44.022229 25943 solver.cpp:309]     Train net output #0: loss = 0.0228779 (* 1 = 0.0228779 loss)
I0711 20:15:44.022238 25943 sgd_solver.cpp:106] Iteration 2100, lr = 1e-05
I0711 20:16:19.163923 26061 blocking_queue.cpp:50] Waiting for data
I0711 20:16:51.417625 25943 solver.cpp:290] Iteration 2200 (1.48382 iter/s, 67.3935s/100 iter), loss = 0.0235983
I0711 20:16:51.417732 25943 solver.cpp:309]     Train net output #0: loss = 0.0235983 (* 1 = 0.0235983 loss)
I0711 20:16:51.417739 25943 sgd_solver.cpp:106] Iteration 2200, lr = 1e-05
I0711 20:17:09.192875 25943 solver.cpp:290] Iteration 2300 (5.62599 iter/s, 17.7747s/100 iter), loss = 0.0269009
I0711 20:17:09.192903 25943 solver.cpp:309]     Train net output #0: loss = 0.0269009 (* 1 = 0.0269009 loss)
I0711 20:17:09.192911 25943 sgd_solver.cpp:106] Iteration 2300, lr = 1e-05
I0711 20:17:26.193935 25943 solver.cpp:290] Iteration 2400 (5.88216 iter/s, 17.0006s/100 iter), loss = 0.0172797
I0711 20:17:26.194012 25943 solver.cpp:309]     Train net output #0: loss = 0.0172797 (* 1 = 0.0172797 loss)
I0711 20:17:26.194020 25943 sgd_solver.cpp:106] Iteration 2400, lr = 1e-05
I0711 20:17:43.161309 25943 solver.cpp:290] Iteration 2500 (5.89385 iter/s, 16.9668s/100 iter), loss = 0.0167326
I0711 20:17:43.161336 25943 solver.cpp:309]     Train net output #0: loss = 0.0167326 (* 1 = 0.0167326 loss)
I0711 20:17:43.161345 25943 sgd_solver.cpp:106] Iteration 2500, lr = 1e-05
I0711 20:18:00.285012 25943 solver.cpp:290] Iteration 2600 (5.84003 iter/s, 17.1232s/100 iter), loss = 0.0283394
I0711 20:18:00.285092 25943 solver.cpp:309]     Train net output #0: loss = 0.0283394 (* 1 = 0.0283394 loss)
I0711 20:18:00.285099 25943 sgd_solver.cpp:106] Iteration 2600, lr = 1e-05
I0711 20:18:17.217105 25943 solver.cpp:290] Iteration 2700 (5.90613 iter/s, 16.9315s/100 iter), loss = 0.013547
I0711 20:18:17.217129 25943 solver.cpp:309]     Train net output #0: loss = 0.013547 (* 1 = 0.013547 loss)
I0711 20:18:17.217136 25943 sgd_solver.cpp:106] Iteration 2700, lr = 1e-05
I0711 20:18:34.297808 25943 solver.cpp:290] Iteration 2800 (5.85473 iter/s, 17.0802s/100 iter), loss = 0.0404717
I0711 20:18:34.297895 25943 solver.cpp:309]     Train net output #0: loss = 0.0404717 (* 1 = 0.0404717 loss)
I0711 20:18:34.297906 25943 sgd_solver.cpp:106] Iteration 2800, lr = 1e-05
I0711 20:18:51.259418 25943 solver.cpp:290] Iteration 2900 (5.89586 iter/s, 16.9611s/100 iter), loss = 0.0251611
I0711 20:18:51.259447 25943 solver.cpp:309]     Train net output #0: loss = 0.0251611 (* 1 = 0.0251611 loss)
I0711 20:18:51.259457 25943 sgd_solver.cpp:106] Iteration 2900, lr = 1e-05
I0711 20:19:08.177590 25943 solver.cpp:290] Iteration 3000 (5.91098 iter/s, 16.9177s/100 iter), loss = 0.0372716
I0711 20:19:08.177629 25943 solver.cpp:309]     Train net output #0: loss = 0.0372716 (* 1 = 0.0372716 loss)
I0711 20:19:08.177637 25943 sgd_solver.cpp:106] Iteration 3000, lr = 1e-05
I0711 20:19:25.251134 25943 solver.cpp:290] Iteration 3100 (5.85719 iter/s, 17.073s/100 iter), loss = 0.0252866
I0711 20:19:25.251158 25943 solver.cpp:309]     Train net output #0: loss = 0.0252866 (* 1 = 0.0252866 loss)
I0711 20:19:25.251165 25943 sgd_solver.cpp:106] Iteration 3100, lr = 1e-05
I0711 20:19:42.238703 25943 solver.cpp:290] Iteration 3200 (5.88683 iter/s, 16.9871s/100 iter), loss = 0.0307177
I0711 20:19:42.238775 25943 solver.cpp:309]     Train net output #0: loss = 0.0307176 (* 1 = 0.0307176 loss)
I0711 20:19:42.238785 25943 sgd_solver.cpp:106] Iteration 3200, lr = 1e-05
I0711 20:19:59.269680 25943 solver.cpp:290] Iteration 3300 (5.87184 iter/s, 17.0304s/100 iter), loss = 0.0221793
I0711 20:19:59.269703 25943 solver.cpp:309]     Train net output #0: loss = 0.0221793 (* 1 = 0.0221793 loss)
I0711 20:19:59.269711 25943 sgd_solver.cpp:106] Iteration 3300, lr = 1e-05
I0711 20:20:16.243834 25943 solver.cpp:290] Iteration 3400 (5.89148 iter/s, 16.9737s/100 iter), loss = 0.019686
I0711 20:20:16.243921 25943 solver.cpp:309]     Train net output #0: loss = 0.019686 (* 1 = 0.019686 loss)
I0711 20:20:16.243932 25943 sgd_solver.cpp:106] Iteration 3400, lr = 1e-05
I0711 20:20:33.349153 25943 solver.cpp:290] Iteration 3500 (5.84632 iter/s, 17.1048s/100 iter), loss = 0.0301555
I0711 20:20:33.349177 25943 solver.cpp:309]     Train net output #0: loss = 0.0301555 (* 1 = 0.0301555 loss)
I0711 20:20:33.349184 25943 sgd_solver.cpp:106] Iteration 3500, lr = 1e-05
I0711 20:20:50.405519 25943 solver.cpp:290] Iteration 3600 (5.86308 iter/s, 17.0559s/100 iter), loss = 0.0178519
I0711 20:20:50.405611 25943 solver.cpp:309]     Train net output #0: loss = 0.0178519 (* 1 = 0.0178519 loss)
I0711 20:20:50.405622 25943 sgd_solver.cpp:106] Iteration 3600, lr = 1e-05
I0711 20:21:07.323068 25943 solver.cpp:290] Iteration 3700 (5.91121 iter/s, 16.917s/100 iter), loss = 0.0162452
I0711 20:21:07.323096 25943 solver.cpp:309]     Train net output #0: loss = 0.0162451 (* 1 = 0.0162451 loss)
I0711 20:21:07.323102 25943 sgd_solver.cpp:106] Iteration 3700, lr = 1e-05
I0711 20:21:24.394871 25943 solver.cpp:290] Iteration 3800 (5.85778 iter/s, 17.0713s/100 iter), loss = 0.0253355
I0711 20:21:24.394933 25943 solver.cpp:309]     Train net output #0: loss = 0.0253355 (* 1 = 0.0253355 loss)
I0711 20:21:24.394943 25943 sgd_solver.cpp:106] Iteration 3800, lr = 1e-05
I0711 20:21:41.360647 25943 solver.cpp:290] Iteration 3900 (5.8944 iter/s, 16.9653s/100 iter), loss = 0.0282103
I0711 20:21:41.360676 25943 solver.cpp:309]     Train net output #0: loss = 0.0282103 (* 1 = 0.0282103 loss)
I0711 20:21:41.360684 25943 sgd_solver.cpp:106] Iteration 3900, lr = 1e-05
I0711 20:21:58.341869 25943 solver.cpp:467] Iteration 4000, Testing net (#0)
I0711 20:22:45.104122 25943 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.955371
I0711 20:22:45.104204 25943 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999722
I0711 20:22:45.104212 25943 solver.cpp:540]     Test net output #2: loss = 0.138928 (* 1 = 0.138928 loss)
I0711 20:22:45.293092 25943 solver.cpp:290] Iteration 4000 (1.56419 iter/s, 63.9307s/100 iter), loss = 0.0181555
I0711 20:22:45.293118 25943 solver.cpp:309]     Train net output #0: loss = 0.0181555 (* 1 = 0.0181555 loss)
I0711 20:22:45.293125 25943 sgd_solver.cpp:106] Iteration 4000, lr = 1e-05
I0711 20:23:22.401427 26061 blocking_queue.cpp:50] Waiting for data
I0711 20:23:41.246528 25943 solver.cpp:290] Iteration 4100 (1.78725 iter/s, 55.9519s/100 iter), loss = 0.0489419
I0711 20:23:41.246556 25943 solver.cpp:309]     Train net output #0: loss = 0.0489419 (* 1 = 0.0489419 loss)
I0711 20:23:41.246562 25943 sgd_solver.cpp:106] Iteration 4100, lr = 1e-05
I0711 20:23:56.382218 26080 blocking_queue.cpp:50] Waiting for data
I0711 20:24:49.433560 25943 solver.cpp:290] Iteration 4200 (1.4666 iter/s, 68.1851s/100 iter), loss = 0.0226377
I0711 20:24:49.433670 25943 solver.cpp:309]     Train net output #0: loss = 0.0226377 (* 1 = 0.0226377 loss)
I0711 20:24:49.433681 25943 sgd_solver.cpp:106] Iteration 4200, lr = 1e-05
I0711 20:25:06.369508 25943 solver.cpp:290] Iteration 4300 (5.9048 iter/s, 16.9354s/100 iter), loss = 0.0327388
I0711 20:25:06.369535 25943 solver.cpp:309]     Train net output #0: loss = 0.0327388 (* 1 = 0.0327388 loss)
I0711 20:25:06.369544 25943 sgd_solver.cpp:106] Iteration 4300, lr = 1e-05
I0711 20:25:23.344054 25943 solver.cpp:290] Iteration 4400 (5.89135 iter/s, 16.9741s/100 iter), loss = 0.0308824
I0711 20:25:23.344112 25943 solver.cpp:309]     Train net output #0: loss = 0.0308823 (* 1 = 0.0308823 loss)
I0711 20:25:23.344120 25943 sgd_solver.cpp:106] Iteration 4400, lr = 1e-05
I0711 20:25:40.355330 25943 solver.cpp:290] Iteration 4500 (5.87864 iter/s, 17.0107s/100 iter), loss = 0.0182774
I0711 20:25:40.355358 25943 solver.cpp:309]     Train net output #0: loss = 0.0182774 (* 1 = 0.0182774 loss)
I0711 20:25:40.355367 25943 sgd_solver.cpp:106] Iteration 4500, lr = 1e-05
I0711 20:25:57.378783 25943 solver.cpp:290] Iteration 4600 (5.87442 iter/s, 17.023s/100 iter), loss = 0.0211551
I0711 20:25:57.378916 25943 solver.cpp:309]     Train net output #0: loss = 0.0211551 (* 1 = 0.0211551 loss)
I0711 20:25:57.378926 25943 sgd_solver.cpp:106] Iteration 4600, lr = 1e-05
I0711 20:26:14.349987 25943 solver.cpp:290] Iteration 4700 (5.89254 iter/s, 16.9706s/100 iter), loss = 0.0281167
I0711 20:26:14.350010 25943 solver.cpp:309]     Train net output #0: loss = 0.0281166 (* 1 = 0.0281166 loss)
I0711 20:26:14.350018 25943 sgd_solver.cpp:106] Iteration 4700, lr = 1e-05
I0711 20:26:31.381795 25943 solver.cpp:290] Iteration 4800 (5.87154 iter/s, 17.0313s/100 iter), loss = 0.0404244
I0711 20:26:31.381870 25943 solver.cpp:309]     Train net output #0: loss = 0.0404243 (* 1 = 0.0404243 loss)
I0711 20:26:31.381880 25943 sgd_solver.cpp:106] Iteration 4800, lr = 1e-05
I0711 20:26:48.553793 25943 solver.cpp:290] Iteration 4900 (5.82362 iter/s, 17.1715s/100 iter), loss = 0.0244342
I0711 20:26:48.553817 25943 solver.cpp:309]     Train net output #0: loss = 0.0244342 (* 1 = 0.0244342 loss)
I0711 20:26:48.553823 25943 sgd_solver.cpp:106] Iteration 4900, lr = 1e-05
I0711 20:27:05.611884 25943 solver.cpp:290] Iteration 5000 (5.86249 iter/s, 17.0576s/100 iter), loss = 0.0123206
I0711 20:27:05.611937 25943 solver.cpp:309]     Train net output #0: loss = 0.0123206 (* 1 = 0.0123206 loss)
I0711 20:27:05.611945 25943 sgd_solver.cpp:106] Iteration 5000, lr = 1e-05
I0711 20:27:22.815099 25943 solver.cpp:290] Iteration 5100 (5.81304 iter/s, 17.2027s/100 iter), loss = 0.0145455
I0711 20:27:22.815122 25943 solver.cpp:309]     Train net output #0: loss = 0.0145454 (* 1 = 0.0145454 loss)
I0711 20:27:22.815129 25943 sgd_solver.cpp:106] Iteration 5100, lr = 1e-05
I0711 20:27:39.819196 25943 solver.cpp:290] Iteration 5200 (5.8811 iter/s, 17.0036s/100 iter), loss = 0.0150289
I0711 20:27:39.819249 25943 solver.cpp:309]     Train net output #0: loss = 0.0150288 (* 1 = 0.0150288 loss)
I0711 20:27:39.819257 25943 sgd_solver.cpp:106] Iteration 5200, lr = 1e-05
I0711 20:27:56.853770 25943 solver.cpp:290] Iteration 5300 (5.87059 iter/s, 17.0341s/100 iter), loss = 0.0201549
I0711 20:27:56.853793 25943 solver.cpp:309]     Train net output #0: loss = 0.0201549 (* 1 = 0.0201549 loss)
I0711 20:27:56.853801 25943 sgd_solver.cpp:106] Iteration 5300, lr = 1e-05
I0711 20:28:13.738481 25943 solver.cpp:290] Iteration 5400 (5.92269 iter/s, 16.8842s/100 iter), loss = 0.0134872
I0711 20:28:13.738570 25943 solver.cpp:309]     Train net output #0: loss = 0.0134872 (* 1 = 0.0134872 loss)
I0711 20:28:13.738580 25943 sgd_solver.cpp:106] Iteration 5400, lr = 1e-05
I0711 20:28:30.877629 25943 solver.cpp:290] Iteration 5500 (5.83478 iter/s, 17.1386s/100 iter), loss = 0.0224444
I0711 20:28:30.877655 25943 solver.cpp:309]     Train net output #0: loss = 0.0224443 (* 1 = 0.0224443 loss)
I0711 20:28:30.877661 25943 sgd_solver.cpp:106] Iteration 5500, lr = 1e-05
I0711 20:28:47.712836 25943 solver.cpp:290] Iteration 5600 (5.9401 iter/s, 16.8347s/100 iter), loss = 0.0258914
I0711 20:28:47.712910 25943 solver.cpp:309]     Train net output #0: loss = 0.0258913 (* 1 = 0.0258913 loss)
I0711 20:28:47.712918 25943 sgd_solver.cpp:106] Iteration 5600, lr = 1e-05
I0711 20:29:04.752789 25943 solver.cpp:290] Iteration 5700 (5.86875 iter/s, 17.0394s/100 iter), loss = 0.0215494
I0711 20:29:04.752812 25943 solver.cpp:309]     Train net output #0: loss = 0.0215493 (* 1 = 0.0215493 loss)
I0711 20:29:04.752823 25943 sgd_solver.cpp:106] Iteration 5700, lr = 1e-05
I0711 20:29:21.863195 25943 solver.cpp:290] Iteration 5800 (5.84456 iter/s, 17.1099s/100 iter), loss = 0.0419161
I0711 20:29:21.863296 25943 solver.cpp:309]     Train net output #0: loss = 0.041916 (* 1 = 0.041916 loss)
I0711 20:29:21.863308 25943 sgd_solver.cpp:106] Iteration 5800, lr = 1e-05
I0711 20:29:38.841120 25943 solver.cpp:290] Iteration 5900 (5.8902 iter/s, 16.9774s/100 iter), loss = 0.0279855
I0711 20:29:38.841146 25943 solver.cpp:309]     Train net output #0: loss = 0.0279854 (* 1 = 0.0279854 loss)
I0711 20:29:38.841152 25943 sgd_solver.cpp:106] Iteration 5900, lr = 1e-05
I0711 20:29:55.766366 25943 solver.cpp:467] Iteration 6000, Testing net (#0)
I0711 20:30:43.436558 25943 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.953748
I0711 20:30:43.436776 25943 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999672
I0711 20:30:43.436784 25943 solver.cpp:540]     Test net output #2: loss = 0.152524 (* 1 = 0.152524 loss)
I0711 20:30:43.627130 25943 solver.cpp:290] Iteration 6000 (1.54359 iter/s, 64.7842s/100 iter), loss = 0.0268469
I0711 20:30:43.627156 25943 solver.cpp:309]     Train net output #0: loss = 0.0268469 (* 1 = 0.0268469 loss)
I0711 20:30:43.627162 25943 sgd_solver.cpp:106] Iteration 6000, lr = 1e-05
I0711 20:31:00.362442 25943 solver.cpp:290] Iteration 6100 (5.97556 iter/s, 16.7348s/100 iter), loss = 0.0241744
I0711 20:31:00.362468 25943 solver.cpp:309]     Train net output #0: loss = 0.0241743 (* 1 = 0.0241743 loss)
I0711 20:31:00.362476 25943 sgd_solver.cpp:106] Iteration 6100, lr = 1e-05
I0711 20:31:23.528726 26022 blocking_queue.cpp:50] Waiting for data
I0711 20:31:25.911176 26106 blocking_queue.cpp:50] Data layer prefetch queue empty
I0711 20:31:40.596856 25943 solver.cpp:290] Iteration 6200 (2.4855 iter/s, 40.2333s/100 iter), loss = 0.0269419
I0711 20:31:40.596961 25943 solver.cpp:309]     Train net output #0: loss = 0.0269418 (* 1 = 0.0269418 loss)
I0711 20:31:40.597002 25943 sgd_solver.cpp:106] Iteration 6200, lr = 1e-05
I0711 20:31:58.739675 25943 solver.cpp:290] Iteration 6300 (5.512 iter/s, 18.1422s/100 iter), loss = 0.0258023
I0711 20:31:58.739725 25943 solver.cpp:309]     Train net output #0: loss = 0.0258022 (* 1 = 0.0258022 loss)
I0711 20:31:58.739737 25943 sgd_solver.cpp:106] Iteration 6300, lr = 1e-05
I0711 20:32:15.766396 25943 solver.cpp:290] Iteration 6400 (5.8733 iter/s, 17.0262s/100 iter), loss = 0.0290447
I0711 20:32:15.766420 25943 solver.cpp:309]     Train net output #0: loss = 0.0290447 (* 1 = 0.0290447 loss)
I0711 20:32:15.766427 25943 sgd_solver.cpp:106] Iteration 6400, lr = 1e-05
I0711 20:32:32.675943 25943 solver.cpp:290] Iteration 6500 (5.91399 iter/s, 16.9091s/100 iter), loss = 0.0342241
I0711 20:32:32.675992 25943 solver.cpp:309]     Train net output #0: loss = 0.034224 (* 1 = 0.034224 loss)
I0711 20:32:32.675999 25943 sgd_solver.cpp:106] Iteration 6500, lr = 1e-05
I0711 20:32:49.586468 25943 solver.cpp:290] Iteration 6600 (5.91365 iter/s, 16.91s/100 iter), loss = 0.0179108
I0711 20:32:49.586493 25943 solver.cpp:309]     Train net output #0: loss = 0.0179108 (* 1 = 0.0179108 loss)
I0711 20:32:49.586499 25943 sgd_solver.cpp:106] Iteration 6600, lr = 1e-05
I0711 20:33:06.643301 25943 solver.cpp:290] Iteration 6700 (5.86292 iter/s, 17.0563s/100 iter), loss = 0.0154283
I0711 20:33:06.643357 25943 solver.cpp:309]     Train net output #0: loss = 0.0154282 (* 1 = 0.0154282 loss)
I0711 20:33:06.643365 25943 sgd_solver.cpp:106] Iteration 6700, lr = 1e-05
I0711 20:33:23.681645 25943 solver.cpp:290] Iteration 6800 (5.8693 iter/s, 17.0378s/100 iter), loss = 0.0218045
I0711 20:33:23.681668 25943 solver.cpp:309]     Train net output #0: loss = 0.0218044 (* 1 = 0.0218044 loss)
I0711 20:33:23.681675 25943 sgd_solver.cpp:106] Iteration 6800, lr = 1e-05
I0711 20:33:40.835078 25943 solver.cpp:290] Iteration 6900 (5.8299 iter/s, 17.1529s/100 iter), loss = 0.0257398
I0711 20:33:40.835144 25943 solver.cpp:309]     Train net output #0: loss = 0.0257398 (* 1 = 0.0257398 loss)
I0711 20:33:40.835155 25943 sgd_solver.cpp:106] Iteration 6900, lr = 1e-05
I0711 20:33:57.872064 25943 solver.cpp:290] Iteration 7000 (5.86976 iter/s, 17.0365s/100 iter), loss = 0.018077
I0711 20:33:57.872086 25943 solver.cpp:309]     Train net output #0: loss = 0.0180769 (* 1 = 0.0180769 loss)
I0711 20:33:57.872092 25943 sgd_solver.cpp:106] Iteration 7000, lr = 1e-05
I0711 20:34:14.982858 25943 solver.cpp:290] Iteration 7100 (5.84443 iter/s, 17.1103s/100 iter), loss = 0.0449575
I0711 20:34:14.982970 25943 solver.cpp:309]     Train net output #0: loss = 0.0449574 (* 1 = 0.0449574 loss)
I0711 20:34:14.982980 25943 sgd_solver.cpp:106] Iteration 7100, lr = 1e-05
I0711 20:34:32.027902 25943 solver.cpp:290] Iteration 7200 (5.867 iter/s, 17.0445s/100 iter), loss = 0.0160386
I0711 20:34:32.027928 25943 solver.cpp:309]     Train net output #0: loss = 0.0160386 (* 1 = 0.0160386 loss)
I0711 20:34:32.027936 25943 sgd_solver.cpp:106] Iteration 7200, lr = 1e-05
I0711 20:34:49.151813 25943 solver.cpp:290] Iteration 7300 (5.83996 iter/s, 17.1234s/100 iter), loss = 0.0323531
I0711 20:34:49.151943 25943 solver.cpp:309]     Train net output #0: loss = 0.032353 (* 1 = 0.032353 loss)
I0711 20:34:49.151954 25943 sgd_solver.cpp:106] Iteration 7300, lr = 1e-05
I0711 20:35:06.062863 25943 solver.cpp:290] Iteration 7400 (5.9135 iter/s, 16.9105s/100 iter), loss = 0.0325191
I0711 20:35:06.062889 25943 solver.cpp:309]     Train net output #0: loss = 0.032519 (* 1 = 0.032519 loss)
I0711 20:35:06.062898 25943 sgd_solver.cpp:106] Iteration 7400, lr = 1e-05
I0711 20:35:23.019309 25943 solver.cpp:290] Iteration 7500 (5.89763 iter/s, 16.956s/100 iter), loss = 0.0353778
I0711 20:35:23.019382 25943 solver.cpp:309]     Train net output #0: loss = 0.0353777 (* 1 = 0.0353777 loss)
I0711 20:35:23.019389 25943 sgd_solver.cpp:106] Iteration 7500, lr = 1e-05
I0711 20:35:39.918733 25943 solver.cpp:290] Iteration 7600 (5.91755 iter/s, 16.8989s/100 iter), loss = 0.021995
I0711 20:35:39.918756 25943 solver.cpp:309]     Train net output #0: loss = 0.021995 (* 1 = 0.021995 loss)
I0711 20:35:39.918763 25943 sgd_solver.cpp:106] Iteration 7600, lr = 1e-05
I0711 20:35:57.068563 25943 solver.cpp:290] Iteration 7700 (5.83113 iter/s, 17.1493s/100 iter), loss = 0.0245847
I0711 20:35:57.068640 25943 solver.cpp:309]     Train net output #0: loss = 0.0245847 (* 1 = 0.0245847 loss)
I0711 20:35:57.068647 25943 sgd_solver.cpp:106] Iteration 7700, lr = 1e-05
I0711 20:36:14.286115 25943 solver.cpp:290] Iteration 7800 (5.80821 iter/s, 17.217s/100 iter), loss = 0.0166087
I0711 20:36:14.286144 25943 solver.cpp:309]     Train net output #0: loss = 0.0166087 (* 1 = 0.0166087 loss)
I0711 20:36:14.286152 25943 sgd_solver.cpp:106] Iteration 7800, lr = 1e-05
I0711 20:36:31.441951 25943 solver.cpp:290] Iteration 7900 (5.82909 iter/s, 17.1553s/100 iter), loss = 0.0361252
I0711 20:36:31.442005 25943 solver.cpp:309]     Train net output #0: loss = 0.0361251 (* 1 = 0.0361251 loss)
I0711 20:36:31.442013 25943 sgd_solver.cpp:106] Iteration 7900, lr = 1e-05
I0711 20:36:48.332142 25943 solver.cpp:467] Iteration 8000, Testing net (#0)
I0711 20:37:35.173249 25943 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.954554
I0711 20:37:35.173328 25943 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999809
I0711 20:37:35.173336 25943 solver.cpp:540]     Test net output #2: loss = 0.144035 (* 1 = 0.144035 loss)
I0711 20:37:35.357697 25943 solver.cpp:290] Iteration 8000 (1.5646 iter/s, 63.914s/100 iter), loss = 0.017464
I0711 20:37:35.357724 25943 solver.cpp:309]     Train net output #0: loss = 0.017464 (* 1 = 0.017464 loss)
I0711 20:37:35.357734 25943 sgd_solver.cpp:106] Iteration 8000, lr = 1e-05
I0711 20:37:53.251627 25943 solver.cpp:290] Iteration 8100 (5.58865 iter/s, 17.8934s/100 iter), loss = 0.0271712
I0711 20:37:53.251652 25943 solver.cpp:309]     Train net output #0: loss = 0.0271711 (* 1 = 0.0271711 loss)
I0711 20:37:53.251659 25943 sgd_solver.cpp:106] Iteration 8100, lr = 1e-05
I0711 20:38:27.107126 25943 solver.cpp:290] Iteration 8200 (2.95381 iter/s, 33.8545s/100 iter), loss = 0.0220642
I0711 20:38:27.107214 25943 solver.cpp:309]     Train net output #0: loss = 0.0220641 (* 1 = 0.0220641 loss)
I0711 20:38:27.107226 25943 sgd_solver.cpp:106] Iteration 8200, lr = 1e-05
I0711 20:38:44.216553 25943 solver.cpp:290] Iteration 8300 (5.84492 iter/s, 17.1089s/100 iter), loss = 0.0339385
I0711 20:38:44.216576 25943 solver.cpp:309]     Train net output #0: loss = 0.0339385 (* 1 = 0.0339385 loss)
I0711 20:38:44.216583 25943 sgd_solver.cpp:106] Iteration 8300, lr = 1e-05
I0711 20:39:01.188951 25943 solver.cpp:290] Iteration 8400 (5.89209 iter/s, 16.9719s/100 iter), loss = 0.0183916
I0711 20:39:01.188998 25943 solver.cpp:309]     Train net output #0: loss = 0.0183916 (* 1 = 0.0183916 loss)
I0711 20:39:01.189005 25943 sgd_solver.cpp:106] Iteration 8400, lr = 1e-05
I0711 20:39:18.201339 25943 solver.cpp:290] Iteration 8500 (5.87825 iter/s, 17.0119s/100 iter), loss = 0.0418022
I0711 20:39:18.201361 25943 solver.cpp:309]     Train net output #0: loss = 0.0418021 (* 1 = 0.0418021 loss)
I0711 20:39:18.201369 25943 sgd_solver.cpp:106] Iteration 8500, lr = 1e-05
I0711 20:39:35.411183 25943 solver.cpp:290] Iteration 8600 (5.8108 iter/s, 17.2093s/100 iter), loss = 0.0348546
I0711 20:39:35.411295 25943 solver.cpp:309]     Train net output #0: loss = 0.0348546 (* 1 = 0.0348546 loss)
I0711 20:39:35.411306 25943 sgd_solver.cpp:106] Iteration 8600, lr = 1e-05
I0711 20:39:52.382225 25943 solver.cpp:290] Iteration 8700 (5.89259 iter/s, 16.9705s/100 iter), loss = 0.0253528
I0711 20:39:52.382247 25943 solver.cpp:309]     Train net output #0: loss = 0.0253527 (* 1 = 0.0253527 loss)
I0711 20:39:52.382254 25943 sgd_solver.cpp:106] Iteration 8700, lr = 1e-05
I0711 20:40:09.457489 25943 solver.cpp:290] Iteration 8800 (5.85659 iter/s, 17.0748s/100 iter), loss = 0.0250733
I0711 20:40:09.457542 25943 solver.cpp:309]     Train net output #0: loss = 0.0250732 (* 1 = 0.0250732 loss)
I0711 20:40:09.457552 25943 sgd_solver.cpp:106] Iteration 8800, lr = 1e-05
I0711 20:40:26.475515 25943 solver.cpp:290] Iteration 8900 (5.8763 iter/s, 17.0175s/100 iter), loss = 0.0237738
I0711 20:40:26.475541 25943 solver.cpp:309]     Train net output #0: loss = 0.0237737 (* 1 = 0.0237737 loss)
I0711 20:40:26.475551 25943 sgd_solver.cpp:106] Iteration 8900, lr = 1e-05
I0711 20:40:43.406347 25943 solver.cpp:290] Iteration 9000 (5.90655 iter/s, 16.9303s/100 iter), loss = 0.0258604
I0711 20:40:43.406396 25943 solver.cpp:309]     Train net output #0: loss = 0.0258604 (* 1 = 0.0258604 loss)
I0711 20:40:43.406404 25943 sgd_solver.cpp:106] Iteration 9000, lr = 1e-05
I0711 20:41:00.372617 25943 solver.cpp:290] Iteration 9100 (5.89423 iter/s, 16.9658s/100 iter), loss = 0.0429117
I0711 20:41:00.372651 25943 solver.cpp:309]     Train net output #0: loss = 0.0429116 (* 1 = 0.0429116 loss)
I0711 20:41:00.372663 25943 sgd_solver.cpp:106] Iteration 9100, lr = 1e-05
I0711 20:41:17.446990 25943 solver.cpp:290] Iteration 9200 (5.8569 iter/s, 17.0739s/100 iter), loss = 0.0309905
I0711 20:41:17.447103 25943 solver.cpp:309]     Train net output #0: loss = 0.0309904 (* 1 = 0.0309904 loss)
I0711 20:41:17.447113 25943 sgd_solver.cpp:106] Iteration 9200, lr = 1e-05
I0711 20:41:34.467952 25943 solver.cpp:290] Iteration 9300 (5.87531 iter/s, 17.0204s/100 iter), loss = 0.0323133
I0711 20:41:34.467978 25943 solver.cpp:309]     Train net output #0: loss = 0.0323132 (* 1 = 0.0323132 loss)
I0711 20:41:34.467986 25943 sgd_solver.cpp:106] Iteration 9300, lr = 1e-05
I0711 20:41:51.434613 25943 solver.cpp:290] Iteration 9400 (5.89408 iter/s, 16.9662s/100 iter), loss = 0.019191
I0711 20:41:51.434660 25943 solver.cpp:309]     Train net output #0: loss = 0.0191909 (* 1 = 0.0191909 loss)
I0711 20:41:51.434669 25943 sgd_solver.cpp:106] Iteration 9400, lr = 1e-05
I0711 20:42:08.490960 25943 solver.cpp:290] Iteration 9500 (5.8631 iter/s, 17.0558s/100 iter), loss = 0.0303161
I0711 20:42:08.490983 25943 solver.cpp:309]     Train net output #0: loss = 0.030316 (* 1 = 0.030316 loss)
I0711 20:42:08.490989 25943 sgd_solver.cpp:106] Iteration 9500, lr = 1e-05
I0711 20:42:25.623143 25943 solver.cpp:290] Iteration 9600 (5.83713 iter/s, 17.1317s/100 iter), loss = 0.0267962
I0711 20:42:25.623198 25943 solver.cpp:309]     Train net output #0: loss = 0.0267961 (* 1 = 0.0267961 loss)
I0711 20:42:25.623206 25943 sgd_solver.cpp:106] Iteration 9600, lr = 1e-05
I0711 20:42:42.713452 25943 solver.cpp:290] Iteration 9700 (5.85145 iter/s, 17.0898s/100 iter), loss = 0.0233109
I0711 20:42:42.713476 25943 solver.cpp:309]     Train net output #0: loss = 0.0233108 (* 1 = 0.0233108 loss)
I0711 20:42:42.713484 25943 sgd_solver.cpp:106] Iteration 9700, lr = 1e-05
I0711 20:42:59.812858 25943 solver.cpp:290] Iteration 9800 (5.84832 iter/s, 17.0989s/100 iter), loss = 0.0192291
I0711 20:42:59.812991 25943 solver.cpp:309]     Train net output #0: loss = 0.019229 (* 1 = 0.019229 loss)
I0711 20:42:59.813001 25943 sgd_solver.cpp:106] Iteration 9800, lr = 1e-05
I0711 20:43:16.959740 25943 solver.cpp:290] Iteration 9900 (5.83217 iter/s, 17.1463s/100 iter), loss = 0.0167835
I0711 20:43:16.959764 25943 solver.cpp:309]     Train net output #0: loss = 0.0167834 (* 1 = 0.0167834 loss)
I0711 20:43:16.959772 25943 sgd_solver.cpp:106] Iteration 9900, lr = 1e-05
I0711 20:43:33.887284 25943 solver.cpp:594] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/l1reg/cityscapes5_jsegnet21v2_iter_10000.caffemodel
I0711 20:43:34.085353 25943 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/l1reg/cityscapes5_jsegnet21v2_iter_10000.solverstate
I0711 20:43:34.102954 25943 solver.cpp:467] Iteration 10000, Testing net (#0)
I0711 20:44:21.142468 25943 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.954543
I0711 20:44:21.142565 25943 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999591
I0711 20:44:21.142575 25943 solver.cpp:540]     Test net output #2: loss = 0.150033 (* 1 = 0.150033 loss)
I0711 20:44:21.331518 25943 solver.cpp:290] Iteration 10000 (1.55352 iter/s, 64.37s/100 iter), loss = 0.0188189
I0711 20:44:21.331544 25943 solver.cpp:309]     Train net output #0: loss = 0.0188188 (* 1 = 0.0188188 loss)
I0711 20:44:21.331552 25943 sgd_solver.cpp:106] Iteration 10000, lr = 1e-05
I0711 20:44:45.866601 25943 solver.cpp:290] Iteration 10100 (4.07591 iter/s, 24.5344s/100 iter), loss = 0.0239857
I0711 20:44:45.866637 25943 solver.cpp:309]     Train net output #0: loss = 0.0239856 (* 1 = 0.0239856 loss)
I0711 20:44:45.866647 25943 sgd_solver.cpp:106] Iteration 10100, lr = 1e-05
I0711 20:45:10.418066 26061 blocking_queue.cpp:50] Waiting for data
I0711 20:45:10.925334 25943 solver.cpp:290] Iteration 10200 (3.99074 iter/s, 25.058s/100 iter), loss = 0.0187356
I0711 20:45:10.925364 25943 solver.cpp:309]     Train net output #0: loss = 0.0187356 (* 1 = 0.0187356 loss)
I0711 20:45:10.925372 25943 sgd_solver.cpp:106] Iteration 10200, lr = 1e-05
I0711 20:45:29.385540 25943 solver.cpp:290] Iteration 10300 (5.41722 iter/s, 18.4597s/100 iter), loss = 0.0215167
I0711 20:45:29.385571 25943 solver.cpp:309]     Train net output #0: loss = 0.0215167 (* 1 = 0.0215167 loss)
I0711 20:45:29.385577 25943 sgd_solver.cpp:106] Iteration 10300, lr = 1e-05
I0711 20:45:46.492204 25943 solver.cpp:290] Iteration 10400 (5.84585 iter/s, 17.1062s/100 iter), loss = 0.0402743
I0711 20:45:46.492274 25943 solver.cpp:309]     Train net output #0: loss = 0.0402743 (* 1 = 0.0402743 loss)
I0711 20:45:46.492446 25943 sgd_solver.cpp:106] Iteration 10400, lr = 1e-05
I0711 20:46:03.628031 25943 solver.cpp:290] Iteration 10500 (5.83591 iter/s, 17.1353s/100 iter), loss = 0.00722957
I0711 20:46:03.628051 25943 solver.cpp:309]     Train net output #0: loss = 0.00722949 (* 1 = 0.00722949 loss)
I0711 20:46:03.628058 25943 sgd_solver.cpp:106] Iteration 10500, lr = 1e-05
I0711 20:46:20.663421 25943 solver.cpp:290] Iteration 10600 (5.8703 iter/s, 17.0349s/100 iter), loss = 0.0173038
I0711 20:46:20.663466 25943 solver.cpp:309]     Train net output #0: loss = 0.0173037 (* 1 = 0.0173037 loss)
I0711 20:46:20.663475 25943 sgd_solver.cpp:106] Iteration 10600, lr = 1e-05
I0711 20:46:37.743257 25943 solver.cpp:290] Iteration 10700 (5.85503 iter/s, 17.0793s/100 iter), loss = 0.0278885
I0711 20:46:37.743279 25943 solver.cpp:309]     Train net output #0: loss = 0.0278884 (* 1 = 0.0278884 loss)
I0711 20:46:37.743288 25943 sgd_solver.cpp:106] Iteration 10700, lr = 1e-05
I0711 20:46:54.806211 25943 solver.cpp:290] Iteration 10800 (5.86082 iter/s, 17.0625s/100 iter), loss = 0.0215059
I0711 20:46:54.806262 25943 solver.cpp:309]     Train net output #0: loss = 0.0215059 (* 1 = 0.0215059 loss)
I0711 20:46:54.806269 25943 sgd_solver.cpp:106] Iteration 10800, lr = 1e-05
I0711 20:47:11.786801 25943 solver.cpp:290] Iteration 10900 (5.88926 iter/s, 16.9801s/100 iter), loss = 0.0195726
I0711 20:47:11.786823 25943 solver.cpp:309]     Train net output #0: loss = 0.0195725 (* 1 = 0.0195725 loss)
I0711 20:47:11.786829 25943 sgd_solver.cpp:106] Iteration 10900, lr = 1e-05
I0711 20:47:28.886574 25943 solver.cpp:290] Iteration 11000 (5.8482 iter/s, 17.0993s/100 iter), loss = 0.018587
I0711 20:47:28.886694 25943 solver.cpp:309]     Train net output #0: loss = 0.0185869 (* 1 = 0.0185869 loss)
I0711 20:47:28.886705 25943 sgd_solver.cpp:106] Iteration 11000, lr = 1e-05
I0711 20:47:45.765758 25943 solver.cpp:290] Iteration 11100 (5.92466 iter/s, 16.8786s/100 iter), loss = 0.0181337
I0711 20:47:45.765780 25943 solver.cpp:309]     Train net output #0: loss = 0.0181336 (* 1 = 0.0181336 loss)
I0711 20:47:45.765786 25943 sgd_solver.cpp:106] Iteration 11100, lr = 1e-05
I0711 20:48:02.836508 25943 solver.cpp:290] Iteration 11200 (5.85814 iter/s, 17.0703s/100 iter), loss = 0.0267851
I0711 20:48:02.836554 25943 solver.cpp:309]     Train net output #0: loss = 0.026785 (* 1 = 0.026785 loss)
I0711 20:48:02.836563 25943 sgd_solver.cpp:106] Iteration 11200, lr = 1e-05
I0711 20:48:19.910207 25943 solver.cpp:290] Iteration 11300 (5.85714 iter/s, 17.0732s/100 iter), loss = 0.0266452
I0711 20:48:19.910231 25943 solver.cpp:309]     Train net output #0: loss = 0.0266451 (* 1 = 0.0266451 loss)
I0711 20:48:19.910238 25943 sgd_solver.cpp:106] Iteration 11300, lr = 1e-05
I0711 20:48:36.960624 25943 solver.cpp:290] Iteration 11400 (5.86513 iter/s, 17.0499s/100 iter), loss = 0.0165269
I0711 20:48:36.960686 25943 solver.cpp:309]     Train net output #0: loss = 0.0165268 (* 1 = 0.0165268 loss)
I0711 20:48:36.960697 25943 sgd_solver.cpp:106] Iteration 11400, lr = 1e-05
I0711 20:48:53.910198 25943 solver.cpp:290] Iteration 11500 (5.90003 iter/s, 16.9491s/100 iter), loss = 0.0201895
I0711 20:48:53.910220 25943 solver.cpp:309]     Train net output #0: loss = 0.0201895 (* 1 = 0.0201895 loss)
I0711 20:48:53.910228 25943 sgd_solver.cpp:106] Iteration 11500, lr = 1e-05
I0711 20:49:10.877393 25943 solver.cpp:290] Iteration 11600 (5.89389 iter/s, 16.9667s/100 iter), loss = 0.0214503
I0711 20:49:10.877437 25943 solver.cpp:309]     Train net output #0: loss = 0.0214502 (* 1 = 0.0214502 loss)
I0711 20:49:10.877444 25943 sgd_solver.cpp:106] Iteration 11600, lr = 1e-05
I0711 20:49:28.115718 25943 solver.cpp:290] Iteration 11700 (5.8012 iter/s, 17.2378s/100 iter), loss = 0.0217999
I0711 20:49:28.115742 25943 solver.cpp:309]     Train net output #0: loss = 0.0217998 (* 1 = 0.0217998 loss)
I0711 20:49:28.115748 25943 sgd_solver.cpp:106] Iteration 11700, lr = 1e-05
I0711 20:49:45.135345 25943 solver.cpp:290] Iteration 11800 (5.87574 iter/s, 17.0191s/100 iter), loss = 0.0142061
I0711 20:49:45.135396 25943 solver.cpp:309]     Train net output #0: loss = 0.0142061 (* 1 = 0.0142061 loss)
I0711 20:49:45.135403 25943 sgd_solver.cpp:106] Iteration 11800, lr = 1e-05
I0711 20:50:02.123481 25943 solver.cpp:290] Iteration 11900 (5.88664 iter/s, 16.9876s/100 iter), loss = 0.0256931
I0711 20:50:02.123504 25943 solver.cpp:309]     Train net output #0: loss = 0.025693 (* 1 = 0.025693 loss)
I0711 20:50:02.123512 25943 sgd_solver.cpp:106] Iteration 11900, lr = 1e-05
I0711 20:50:18.855257 25943 solver.cpp:467] Iteration 12000, Testing net (#0)
I0711 20:51:05.536178 25943 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.954518
I0711 20:51:05.536275 25943 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999654
I0711 20:51:05.536283 25943 solver.cpp:540]     Test net output #2: loss = 0.15673 (* 1 = 0.15673 loss)
I0711 20:51:05.723222 25943 solver.cpp:290] Iteration 12000 (1.57238 iter/s, 63.598s/100 iter), loss = 0.0208287
I0711 20:51:05.723248 25943 solver.cpp:309]     Train net output #0: loss = 0.0208286 (* 1 = 0.0208286 loss)
I0711 20:51:05.723253 25943 sgd_solver.cpp:106] Iteration 12000, lr = 1e-05
I0711 20:51:22.350327 25943 solver.cpp:290] Iteration 12100 (6.01445 iter/s, 16.6266s/100 iter), loss = 0.0238717
I0711 20:51:22.350350 25943 solver.cpp:309]     Train net output #0: loss = 0.0238716 (* 1 = 0.0238716 loss)
I0711 20:51:22.350356 25943 sgd_solver.cpp:106] Iteration 12100, lr = 1e-05
I0711 20:51:43.115567 25943 solver.cpp:290] Iteration 12200 (4.81588 iter/s, 20.7646s/100 iter), loss = 0.0163873
I0711 20:51:43.115633 25943 solver.cpp:309]     Train net output #0: loss = 0.0163872 (* 1 = 0.0163872 loss)
I0711 20:51:43.115641 25943 sgd_solver.cpp:106] Iteration 12200, lr = 1e-05
I0711 20:52:00.748456 25943 solver.cpp:290] Iteration 12300 (5.6714 iter/s, 17.6323s/100 iter), loss = 0.0366571
I0711 20:52:00.748484 25943 solver.cpp:309]     Train net output #0: loss = 0.036657 (* 1 = 0.036657 loss)
I0711 20:52:00.748493 25943 sgd_solver.cpp:106] Iteration 12300, lr = 1e-05
I0711 20:52:17.744170 25943 solver.cpp:290] Iteration 12400 (5.88401 iter/s, 16.9952s/100 iter), loss = 0.0424687
I0711 20:52:17.744246 25943 solver.cpp:309]     Train net output #0: loss = 0.0424687 (* 1 = 0.0424687 loss)
I0711 20:52:17.744253 25943 sgd_solver.cpp:106] Iteration 12400, lr = 1e-05
I0711 20:52:34.796059 25943 solver.cpp:290] Iteration 12500 (5.86464 iter/s, 17.0513s/100 iter), loss = 0.0165205
I0711 20:52:34.796084 25943 solver.cpp:309]     Train net output #0: loss = 0.0165204 (* 1 = 0.0165204 loss)
I0711 20:52:34.796090 25943 sgd_solver.cpp:106] Iteration 12500, lr = 1e-05
I0711 20:52:51.705153 25943 solver.cpp:290] Iteration 12600 (5.91415 iter/s, 16.9086s/100 iter), loss = 0.0186354
I0711 20:52:51.705241 25943 solver.cpp:309]     Train net output #0: loss = 0.0186353 (* 1 = 0.0186353 loss)
I0711 20:52:51.705252 25943 sgd_solver.cpp:106] Iteration 12600, lr = 1e-05
I0711 20:53:08.725724 25943 solver.cpp:290] Iteration 12700 (5.87543 iter/s, 17.02s/100 iter), loss = 0.0203239
I0711 20:53:08.725749 25943 solver.cpp:309]     Train net output #0: loss = 0.0203238 (* 1 = 0.0203238 loss)
I0711 20:53:08.725756 25943 sgd_solver.cpp:106] Iteration 12700, lr = 1e-05
I0711 20:53:25.784169 25943 solver.cpp:290] Iteration 12800 (5.86237 iter/s, 17.058s/100 iter), loss = 0.0167286
I0711 20:53:25.784224 25943 solver.cpp:309]     Train net output #0: loss = 0.0167285 (* 1 = 0.0167285 loss)
I0711 20:53:25.784231 25943 sgd_solver.cpp:106] Iteration 12800, lr = 1e-05
I0711 20:53:42.803282 25943 solver.cpp:290] Iteration 12900 (5.87593 iter/s, 17.0186s/100 iter), loss = 0.0277644
I0711 20:53:42.803310 25943 solver.cpp:309]     Train net output #0: loss = 0.0277643 (* 1 = 0.0277643 loss)
I0711 20:53:42.803319 25943 sgd_solver.cpp:106] Iteration 12900, lr = 1e-05
I0711 20:53:59.951207 25943 solver.cpp:290] Iteration 13000 (5.83178 iter/s, 17.1474s/100 iter), loss = 0.0339986
I0711 20:53:59.951283 25943 solver.cpp:309]     Train net output #0: loss = 0.0339985 (* 1 = 0.0339985 loss)
I0711 20:53:59.951292 25943 sgd_solver.cpp:106] Iteration 13000, lr = 1e-05
I0711 20:54:16.925348 25943 solver.cpp:290] Iteration 13100 (5.8915 iter/s, 16.9736s/100 iter), loss = 0.0155797
I0711 20:54:16.925377 25943 solver.cpp:309]     Train net output #0: loss = 0.0155796 (* 1 = 0.0155796 loss)
I0711 20:54:16.925387 25943 sgd_solver.cpp:106] Iteration 13100, lr = 1e-05
I0711 20:54:34.038751 25943 solver.cpp:290] Iteration 13200 (5.84354 iter/s, 17.1129s/100 iter), loss = 0.0210055
I0711 20:54:34.038803 25943 solver.cpp:309]     Train net output #0: loss = 0.0210054 (* 1 = 0.0210054 loss)
I0711 20:54:34.038811 25943 sgd_solver.cpp:106] Iteration 13200, lr = 1e-05
I0711 20:54:51.078543 25943 solver.cpp:290] Iteration 13300 (5.86879 iter/s, 17.0393s/100 iter), loss = 0.032986
I0711 20:54:51.078567 25943 solver.cpp:309]     Train net output #0: loss = 0.0329859 (* 1 = 0.0329859 loss)
I0711 20:54:51.078573 25943 sgd_solver.cpp:106] Iteration 13300, lr = 1e-05
I0711 20:55:08.148159 25943 solver.cpp:290] Iteration 13400 (5.85853 iter/s, 17.0691s/100 iter), loss = 0.0205173
I0711 20:55:08.148218 25943 solver.cpp:309]     Train net output #0: loss = 0.0205172 (* 1 = 0.0205172 loss)
I0711 20:55:08.148226 25943 sgd_solver.cpp:106] Iteration 13400, lr = 1e-05
I0711 20:55:25.240427 25943 solver.cpp:290] Iteration 13500 (5.85078 iter/s, 17.0917s/100 iter), loss = 0.0281079
I0711 20:55:25.240455 25943 solver.cpp:309]     Train net output #0: loss = 0.0281078 (* 1 = 0.0281078 loss)
I0711 20:55:25.240463 25943 sgd_solver.cpp:106] Iteration 13500, lr = 1e-05
I0711 20:55:42.287737 25943 solver.cpp:290] Iteration 13600 (5.8662 iter/s, 17.0468s/100 iter), loss = 0.0176351
I0711 20:55:42.287809 25943 solver.cpp:309]     Train net output #0: loss = 0.017635 (* 1 = 0.017635 loss)
I0711 20:55:42.287818 25943 sgd_solver.cpp:106] Iteration 13600, lr = 1e-05
I0711 20:55:59.322574 25943 solver.cpp:290] Iteration 13700 (5.87051 iter/s, 17.0343s/100 iter), loss = 0.0327023
I0711 20:55:59.322598 25943 solver.cpp:309]     Train net output #0: loss = 0.0327022 (* 1 = 0.0327022 loss)
I0711 20:55:59.322609 25943 sgd_solver.cpp:106] Iteration 13700, lr = 1e-05
I0711 20:56:16.282027 25943 solver.cpp:290] Iteration 13800 (5.89659 iter/s, 16.959s/100 iter), loss = 0.0190158
I0711 20:56:16.282107 25943 solver.cpp:309]     Train net output #0: loss = 0.0190157 (* 1 = 0.0190157 loss)
I0711 20:56:16.282119 25943 sgd_solver.cpp:106] Iteration 13800, lr = 1e-05
I0711 20:56:33.297926 25943 solver.cpp:290] Iteration 13900 (5.87704 iter/s, 17.0154s/100 iter), loss = 0.0194357
I0711 20:56:33.297951 25943 solver.cpp:309]     Train net output #0: loss = 0.0194356 (* 1 = 0.0194356 loss)
I0711 20:56:33.297958 25943 sgd_solver.cpp:106] Iteration 13900, lr = 1e-05
I0711 20:56:50.250952 25943 solver.cpp:467] Iteration 14000, Testing net (#0)
I0711 20:57:37.141640 25943 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.953702
I0711 20:57:37.141729 25943 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999395
I0711 20:57:37.141736 25943 solver.cpp:540]     Test net output #2: loss = 0.16174 (* 1 = 0.16174 loss)
I0711 20:57:37.326373 25943 solver.cpp:290] Iteration 14000 (1.56185 iter/s, 64.0267s/100 iter), loss = 0.0212054
I0711 20:57:37.326397 25943 solver.cpp:309]     Train net output #0: loss = 0.0212053 (* 1 = 0.0212053 loss)
I0711 20:57:37.326403 25943 sgd_solver.cpp:106] Iteration 14000, lr = 1e-05
I0711 20:57:54.110023 25943 solver.cpp:290] Iteration 14100 (5.95836 iter/s, 16.7832s/100 iter), loss = 0.016821
I0711 20:57:54.110054 25943 solver.cpp:309]     Train net output #0: loss = 0.016821 (* 1 = 0.016821 loss)
I0711 20:57:54.110064 25943 sgd_solver.cpp:106] Iteration 14100, lr = 1e-05
I0711 20:58:11.240679 25943 solver.cpp:290] Iteration 14200 (5.83766 iter/s, 17.1302s/100 iter), loss = 0.0191788
I0711 20:58:11.240782 25943 solver.cpp:309]     Train net output #0: loss = 0.0191787 (* 1 = 0.0191787 loss)
I0711 20:58:11.240792 25943 sgd_solver.cpp:106] Iteration 14200, lr = 1e-05
I0711 20:58:28.242133 25943 solver.cpp:290] Iteration 14300 (5.88205 iter/s, 17.0009s/100 iter), loss = 0.0124078
I0711 20:58:28.242161 25943 solver.cpp:309]     Train net output #0: loss = 0.0124077 (* 1 = 0.0124077 loss)
I0711 20:58:28.242171 25943 sgd_solver.cpp:106] Iteration 14300, lr = 1e-05
I0711 20:58:45.283980 25943 solver.cpp:290] Iteration 14400 (5.86808 iter/s, 17.0414s/100 iter), loss = 0.0181368
I0711 20:58:45.284039 25943 solver.cpp:309]     Train net output #0: loss = 0.0181367 (* 1 = 0.0181367 loss)
I0711 20:58:45.284046 25943 sgd_solver.cpp:106] Iteration 14400, lr = 1e-05
I0711 20:59:02.212841 25943 solver.cpp:290] Iteration 14500 (5.90725 iter/s, 16.9283s/100 iter), loss = 0.0361383
I0711 20:59:02.212865 25943 solver.cpp:309]     Train net output #0: loss = 0.0361383 (* 1 = 0.0361383 loss)
I0711 20:59:02.212872 25943 sgd_solver.cpp:106] Iteration 14500, lr = 1e-05
I0711 20:59:19.188208 25943 solver.cpp:290] Iteration 14600 (5.89106 iter/s, 16.9749s/100 iter), loss = 0.0198914
I0711 20:59:19.188254 25943 solver.cpp:309]     Train net output #0: loss = 0.0198913 (* 1 = 0.0198913 loss)
I0711 20:59:19.188261 25943 sgd_solver.cpp:106] Iteration 14600, lr = 1e-05
I0711 20:59:36.048640 25943 solver.cpp:290] Iteration 14700 (5.93122 iter/s, 16.8599s/100 iter), loss = 0.0575131
I0711 20:59:36.048661 25943 solver.cpp:309]     Train net output #0: loss = 0.057513 (* 1 = 0.057513 loss)
I0711 20:59:36.048667 25943 sgd_solver.cpp:106] Iteration 14700, lr = 1e-05
I0711 20:59:53.077683 25943 solver.cpp:290] Iteration 14800 (5.87249 iter/s, 17.0286s/100 iter), loss = 0.0293133
I0711 20:59:53.077759 25943 solver.cpp:309]     Train net output #0: loss = 0.0293132 (* 1 = 0.0293132 loss)
I0711 20:59:53.077766 25943 sgd_solver.cpp:106] Iteration 14800, lr = 1e-05
I0711 21:00:10.064061 25943 solver.cpp:290] Iteration 14900 (5.88726 iter/s, 16.9858s/100 iter), loss = 0.0360433
I0711 21:00:10.064086 25943 solver.cpp:309]     Train net output #0: loss = 0.0360432 (* 1 = 0.0360432 loss)
I0711 21:00:10.064095 25943 sgd_solver.cpp:106] Iteration 14900, lr = 1e-05
I0711 21:00:27.068130 25943 solver.cpp:290] Iteration 15000 (5.88111 iter/s, 17.0036s/100 iter), loss = 0.0372117
I0711 21:00:27.068181 25943 solver.cpp:309]     Train net output #0: loss = 0.0372116 (* 1 = 0.0372116 loss)
I0711 21:00:27.068189 25943 sgd_solver.cpp:106] Iteration 15000, lr = 1e-05
I0711 21:00:44.059842 25943 solver.cpp:290] Iteration 15100 (5.8854 iter/s, 16.9912s/100 iter), loss = 0.0167423
I0711 21:00:44.059867 25943 solver.cpp:309]     Train net output #0: loss = 0.0167422 (* 1 = 0.0167422 loss)
I0711 21:00:44.059876 25943 sgd_solver.cpp:106] Iteration 15100, lr = 1e-05
I0711 21:01:01.063447 25943 solver.cpp:290] Iteration 15200 (5.88128 iter/s, 17.0031s/100 iter), loss = 0.01946
I0711 21:01:01.063503 25943 solver.cpp:309]     Train net output #0: loss = 0.01946 (* 1 = 0.01946 loss)
I0711 21:01:01.063511 25943 sgd_solver.cpp:106] Iteration 15200, lr = 1e-05
I0711 21:01:18.007068 25943 solver.cpp:290] Iteration 15300 (5.9021 iter/s, 16.9431s/100 iter), loss = 0.0238182
I0711 21:01:18.007097 25943 solver.cpp:309]     Train net output #0: loss = 0.0238181 (* 1 = 0.0238181 loss)
I0711 21:01:18.007105 25943 sgd_solver.cpp:106] Iteration 15300, lr = 1e-05
I0711 21:01:35.013906 25943 solver.cpp:290] Iteration 15400 (5.88016 iter/s, 17.0063s/100 iter), loss = 0.0267364
I0711 21:01:35.013993 25943 solver.cpp:309]     Train net output #0: loss = 0.0267363 (* 1 = 0.0267363 loss)
I0711 21:01:35.014006 25943 sgd_solver.cpp:106] Iteration 15400, lr = 1e-05
I0711 21:01:51.904183 25943 solver.cpp:290] Iteration 15500 (5.92076 iter/s, 16.8897s/100 iter), loss = 0.0236173
I0711 21:01:51.904211 25943 solver.cpp:309]     Train net output #0: loss = 0.0236172 (* 1 = 0.0236172 loss)
I0711 21:01:51.904219 25943 sgd_solver.cpp:106] Iteration 15500, lr = 1e-05
I0711 21:02:09.020572 25943 solver.cpp:290] Iteration 15600 (5.84252 iter/s, 17.1159s/100 iter), loss = 0.0274421
I0711 21:02:09.020632 25943 solver.cpp:309]     Train net output #0: loss = 0.027442 (* 1 = 0.027442 loss)
I0711 21:02:09.020642 25943 sgd_solver.cpp:106] Iteration 15600, lr = 1e-05
I0711 21:02:26.040443 25943 solver.cpp:290] Iteration 15700 (5.87567 iter/s, 17.0194s/100 iter), loss = 0.0243506
I0711 21:02:26.040464 25943 solver.cpp:309]     Train net output #0: loss = 0.0243505 (* 1 = 0.0243505 loss)
I0711 21:02:26.040472 25943 sgd_solver.cpp:106] Iteration 15700, lr = 1e-05
I0711 21:02:43.154892 25943 solver.cpp:290] Iteration 15800 (5.84318 iter/s, 17.114s/100 iter), loss = 0.0318157
I0711 21:02:43.154966 25943 solver.cpp:309]     Train net output #0: loss = 0.0318157 (* 1 = 0.0318157 loss)
I0711 21:02:43.154975 25943 sgd_solver.cpp:106] Iteration 15800, lr = 1e-05
I0711 21:03:00.227533 25943 solver.cpp:290] Iteration 15900 (5.85751 iter/s, 17.0721s/100 iter), loss = 0.0310758
I0711 21:03:00.227556 25943 solver.cpp:309]     Train net output #0: loss = 0.0310757 (* 1 = 0.0310757 loss)
I0711 21:03:00.227563 25943 sgd_solver.cpp:106] Iteration 15900, lr = 1e-05
I0711 21:03:16.962541 25943 solver.cpp:467] Iteration 16000, Testing net (#0)
I0711 21:04:03.534816 25943 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.953378
I0711 21:04:03.534900 25943 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999586
I0711 21:04:03.534909 25943 solver.cpp:540]     Test net output #2: loss = 0.168213 (* 1 = 0.168213 loss)
I0711 21:04:03.710425 25943 solver.cpp:290] Iteration 16000 (1.57527 iter/s, 63.4811s/100 iter), loss = 0.0281878
I0711 21:04:03.710449 25943 solver.cpp:309]     Train net output #0: loss = 0.0281877 (* 1 = 0.0281877 loss)
I0711 21:04:03.710456 25943 sgd_solver.cpp:106] Iteration 16000, lr = 1e-05
I0711 21:04:20.546418 25943 solver.cpp:290] Iteration 16100 (5.93983 iter/s, 16.8355s/100 iter), loss = 0.0181807
I0711 21:04:20.546458 25943 solver.cpp:309]     Train net output #0: loss = 0.0181807 (* 1 = 0.0181807 loss)
I0711 21:04:20.546468 25943 sgd_solver.cpp:106] Iteration 16100, lr = 1e-05
I0711 21:04:37.678592 25943 solver.cpp:290] Iteration 16200 (5.83714 iter/s, 17.1317s/100 iter), loss = 0.0348326
I0711 21:04:37.684973 25943 solver.cpp:309]     Train net output #0: loss = 0.0348325 (* 1 = 0.0348325 loss)
I0711 21:04:37.685027 25943 sgd_solver.cpp:106] Iteration 16200, lr = 1e-05
I0711 21:04:54.831936 25943 solver.cpp:290] Iteration 16300 (5.83209 iter/s, 17.1465s/100 iter), loss = 0.0334909
I0711 21:04:54.831959 25943 solver.cpp:309]     Train net output #0: loss = 0.0334909 (* 1 = 0.0334909 loss)
I0711 21:04:54.831966 25943 sgd_solver.cpp:106] Iteration 16300, lr = 1e-05
I0711 21:05:11.806557 25943 solver.cpp:290] Iteration 16400 (5.89132 iter/s, 16.9741s/100 iter), loss = 0.0238516
I0711 21:05:11.806669 25943 solver.cpp:309]     Train net output #0: loss = 0.0238516 (* 1 = 0.0238516 loss)
I0711 21:05:11.806679 25943 sgd_solver.cpp:106] Iteration 16400, lr = 1e-05
I0711 21:05:28.677492 25943 solver.cpp:290] Iteration 16500 (5.92756 iter/s, 16.8704s/100 iter), loss = 0.0145279
I0711 21:05:28.677515 25943 solver.cpp:309]     Train net output #0: loss = 0.0145278 (* 1 = 0.0145278 loss)
I0711 21:05:28.677522 25943 sgd_solver.cpp:106] Iteration 16500, lr = 1e-05
I0711 21:05:45.598369 25943 solver.cpp:290] Iteration 16600 (5.91003 iter/s, 16.9204s/100 iter), loss = 0.0275113
I0711 21:05:45.598445 25943 solver.cpp:309]     Train net output #0: loss = 0.0275113 (* 1 = 0.0275113 loss)
I0711 21:05:45.598453 25943 sgd_solver.cpp:106] Iteration 16600, lr = 1e-05
I0711 21:06:02.505666 25943 solver.cpp:290] Iteration 16700 (5.91479 iter/s, 16.9068s/100 iter), loss = 0.0346397
I0711 21:06:02.505690 25943 solver.cpp:309]     Train net output #0: loss = 0.0346396 (* 1 = 0.0346396 loss)
I0711 21:06:02.505699 25943 sgd_solver.cpp:106] Iteration 16700, lr = 1e-05
I0711 21:06:19.654618 25943 solver.cpp:290] Iteration 16800 (5.83143 iter/s, 17.1485s/100 iter), loss = 0.0487344
I0711 21:06:19.654682 25943 solver.cpp:309]     Train net output #0: loss = 0.0487344 (* 1 = 0.0487344 loss)
I0711 21:06:19.654693 25943 sgd_solver.cpp:106] Iteration 16800, lr = 1e-05
I0711 21:06:36.734182 25943 solver.cpp:290] Iteration 16900 (5.85513 iter/s, 17.079s/100 iter), loss = 0.0191461
I0711 21:06:36.734208 25943 solver.cpp:309]     Train net output #0: loss = 0.019146 (* 1 = 0.019146 loss)
I0711 21:06:36.734217 25943 sgd_solver.cpp:106] Iteration 16900, lr = 1e-05
I0711 21:06:53.742188 25943 solver.cpp:290] Iteration 17000 (5.87975 iter/s, 17.0075s/100 iter), loss = 0.0166016
I0711 21:06:53.742246 25943 solver.cpp:309]     Train net output #0: loss = 0.0166016 (* 1 = 0.0166016 loss)
I0711 21:06:53.742257 25943 sgd_solver.cpp:106] Iteration 17000, lr = 1e-05
I0711 21:07:10.777400 25943 solver.cpp:290] Iteration 17100 (5.87037 iter/s, 17.0347s/100 iter), loss = 0.0133025
I0711 21:07:10.777423 25943 solver.cpp:309]     Train net output #0: loss = 0.0133024 (* 1 = 0.0133024 loss)
I0711 21:07:10.777431 25943 sgd_solver.cpp:106] Iteration 17100, lr = 1e-05
I0711 21:07:27.779268 25943 solver.cpp:290] Iteration 17200 (5.88188 iter/s, 17.0014s/100 iter), loss = 0.0230211
I0711 21:07:27.779320 25943 solver.cpp:309]     Train net output #0: loss = 0.023021 (* 1 = 0.023021 loss)
I0711 21:07:27.779328 25943 sgd_solver.cpp:106] Iteration 17200, lr = 1e-05
I0711 21:07:44.741129 25943 solver.cpp:290] Iteration 17300 (5.89576 iter/s, 16.9613s/100 iter), loss = 0.0189019
I0711 21:07:44.741153 25943 solver.cpp:309]     Train net output #0: loss = 0.0189019 (* 1 = 0.0189019 loss)
I0711 21:07:44.741160 25943 sgd_solver.cpp:106] Iteration 17300, lr = 1e-05
I0711 21:08:01.804019 25943 solver.cpp:290] Iteration 17400 (5.86084 iter/s, 17.0624s/100 iter), loss = 0.0201729
I0711 21:08:01.805253 25943 solver.cpp:309]     Train net output #0: loss = 0.0201729 (* 1 = 0.0201729 loss)
I0711 21:08:01.805265 25943 sgd_solver.cpp:106] Iteration 17400, lr = 1e-05
I0711 21:08:19.116080 25943 solver.cpp:290] Iteration 17500 (5.77689 iter/s, 17.3104s/100 iter), loss = 0.0503401
I0711 21:08:19.116104 25943 solver.cpp:309]     Train net output #0: loss = 0.05034 (* 1 = 0.05034 loss)
I0711 21:08:19.116111 25943 sgd_solver.cpp:106] Iteration 17500, lr = 1e-05
I0711 21:08:36.098492 25943 solver.cpp:290] Iteration 17600 (5.88861 iter/s, 16.9819s/100 iter), loss = 0.0163527
I0711 21:08:36.098570 25943 solver.cpp:309]     Train net output #0: loss = 0.0163526 (* 1 = 0.0163526 loss)
I0711 21:08:36.098578 25943 sgd_solver.cpp:106] Iteration 17600, lr = 1e-05
I0711 21:08:52.982316 25943 solver.cpp:290] Iteration 17700 (5.92302 iter/s, 16.8833s/100 iter), loss = 0.026588
I0711 21:08:52.982342 25943 solver.cpp:309]     Train net output #0: loss = 0.026588 (* 1 = 0.026588 loss)
I0711 21:08:52.982352 25943 sgd_solver.cpp:106] Iteration 17700, lr = 1e-05
I0711 21:09:10.041040 25943 solver.cpp:290] Iteration 17800 (5.86227 iter/s, 17.0582s/100 iter), loss = 0.0565402
I0711 21:09:10.041168 25943 solver.cpp:309]     Train net output #0: loss = 0.0565402 (* 1 = 0.0565402 loss)
I0711 21:09:10.041179 25943 sgd_solver.cpp:106] Iteration 17800, lr = 1e-05
I0711 21:09:27.107363 25943 solver.cpp:290] Iteration 17900 (5.8597 iter/s, 17.0657s/100 iter), loss = 0.0300097
I0711 21:09:27.107393 25943 solver.cpp:309]     Train net output #0: loss = 0.0300097 (* 1 = 0.0300097 loss)
I0711 21:09:27.107401 25943 sgd_solver.cpp:106] Iteration 17900, lr = 1e-05
I0711 21:09:44.140404 25943 solver.cpp:467] Iteration 18000, Testing net (#0)
I0711 21:10:30.983666 25943 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.951366
I0711 21:10:30.983767 25943 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999734
I0711 21:10:30.983774 25943 solver.cpp:540]     Test net output #2: loss = 0.174061 (* 1 = 0.174061 loss)
I0711 21:10:31.178889 25943 solver.cpp:290] Iteration 18000 (1.5608 iter/s, 64.0698s/100 iter), loss = 0.020442
I0711 21:10:31.178917 25943 solver.cpp:309]     Train net output #0: loss = 0.0204419 (* 1 = 0.0204419 loss)
I0711 21:10:31.178923 25943 sgd_solver.cpp:106] Iteration 18000, lr = 1e-05
I0711 21:10:48.097239 25943 solver.cpp:290] Iteration 18100 (5.91092 iter/s, 16.9179s/100 iter), loss = 0.0305118
I0711 21:10:48.097263 25943 solver.cpp:309]     Train net output #0: loss = 0.0305117 (* 1 = 0.0305117 loss)
I0711 21:10:48.097270 25943 sgd_solver.cpp:106] Iteration 18100, lr = 1e-05
I0711 21:11:05.259099 25943 solver.cpp:290] Iteration 18200 (5.82704 iter/s, 17.1614s/100 iter), loss = 0.0251192
I0711 21:11:05.259177 25943 solver.cpp:309]     Train net output #0: loss = 0.0251191 (* 1 = 0.0251191 loss)
I0711 21:11:05.259187 25943 sgd_solver.cpp:106] Iteration 18200, lr = 1e-05
I0711 21:11:22.332021 25943 solver.cpp:290] Iteration 18300 (5.85742 iter/s, 17.0724s/100 iter), loss = 0.026263
I0711 21:11:22.332046 25943 solver.cpp:309]     Train net output #0: loss = 0.0262629 (* 1 = 0.0262629 loss)
I0711 21:11:22.332051 25943 sgd_solver.cpp:106] Iteration 18300, lr = 1e-05
I0711 21:11:39.459152 25943 solver.cpp:290] Iteration 18400 (5.83886 iter/s, 17.1266s/100 iter), loss = 0.0255635
I0711 21:11:39.459237 25943 solver.cpp:309]     Train net output #0: loss = 0.0255634 (* 1 = 0.0255634 loss)
I0711 21:11:39.459249 25943 sgd_solver.cpp:106] Iteration 18400, lr = 1e-05
I0711 21:11:56.419777 25943 solver.cpp:290] Iteration 18500 (5.8962 iter/s, 16.9601s/100 iter), loss = 0.041522
I0711 21:11:56.419806 25943 solver.cpp:309]     Train net output #0: loss = 0.041522 (* 1 = 0.041522 loss)
I0711 21:11:56.419811 25943 sgd_solver.cpp:106] Iteration 18500, lr = 1e-05
I0711 21:12:13.589259 25943 solver.cpp:290] Iteration 18600 (5.82446 iter/s, 17.169s/100 iter), loss = 0.0156392
I0711 21:12:13.589314 25943 solver.cpp:309]     Train net output #0: loss = 0.0156392 (* 1 = 0.0156392 loss)
I0711 21:12:13.589329 25943 sgd_solver.cpp:106] Iteration 18600, lr = 1e-05
I0711 21:12:30.528435 25943 solver.cpp:290] Iteration 18700 (5.90366 iter/s, 16.9387s/100 iter), loss = 0.0258387
I0711 21:12:30.528463 25943 solver.cpp:309]     Train net output #0: loss = 0.0258386 (* 1 = 0.0258386 loss)
I0711 21:12:30.528470 25943 sgd_solver.cpp:106] Iteration 18700, lr = 1e-05
I0711 21:12:47.397866 25943 solver.cpp:290] Iteration 18800 (5.92805 iter/s, 16.8689s/100 iter), loss = 0.0195042
I0711 21:12:47.397948 25943 solver.cpp:309]     Train net output #0: loss = 0.0195042 (* 1 = 0.0195042 loss)
I0711 21:12:47.397959 25943 sgd_solver.cpp:106] Iteration 18800, lr = 1e-05
I0711 21:13:04.524559 25943 solver.cpp:290] Iteration 18900 (5.83903 iter/s, 17.1261s/100 iter), loss = 0.0267715
I0711 21:13:04.524585 25943 solver.cpp:309]     Train net output #0: loss = 0.0267715 (* 1 = 0.0267715 loss)
I0711 21:13:04.524592 25943 sgd_solver.cpp:106] Iteration 18900, lr = 1e-05
I0711 21:13:21.560497 25943 solver.cpp:290] Iteration 19000 (5.87011 iter/s, 17.0354s/100 iter), loss = 0.0296351
I0711 21:13:21.560561 25943 solver.cpp:309]     Train net output #0: loss = 0.029635 (* 1 = 0.029635 loss)
I0711 21:13:21.560573 25943 sgd_solver.cpp:106] Iteration 19000, lr = 1e-05
I0711 21:13:38.664460 25943 solver.cpp:290] Iteration 19100 (5.84678 iter/s, 17.1034s/100 iter), loss = 0.0197047
I0711 21:13:38.664482 25943 solver.cpp:309]     Train net output #0: loss = 0.0197046 (* 1 = 0.0197046 loss)
I0711 21:13:38.664489 25943 sgd_solver.cpp:106] Iteration 19100, lr = 1e-05
I0711 21:13:55.893720 25943 solver.cpp:290] Iteration 19200 (5.80425 iter/s, 17.2288s/100 iter), loss = 0.0193212
I0711 21:13:55.893800 25943 solver.cpp:309]     Train net output #0: loss = 0.0193212 (* 1 = 0.0193212 loss)
I0711 21:13:55.893810 25943 sgd_solver.cpp:106] Iteration 19200, lr = 1e-05
I0711 21:14:12.954246 25943 solver.cpp:290] Iteration 19300 (5.86167 iter/s, 17.06s/100 iter), loss = 0.0433611
I0711 21:14:12.954274 25943 solver.cpp:309]     Train net output #0: loss = 0.0433611 (* 1 = 0.0433611 loss)
I0711 21:14:12.954283 25943 sgd_solver.cpp:106] Iteration 19300, lr = 1e-05
I0711 21:14:29.899994 25943 solver.cpp:290] Iteration 19400 (5.90136 iter/s, 16.9453s/100 iter), loss = 0.0257819
I0711 21:14:29.900102 25943 solver.cpp:309]     Train net output #0: loss = 0.0257819 (* 1 = 0.0257819 loss)
I0711 21:14:29.900112 25943 sgd_solver.cpp:106] Iteration 19400, lr = 1e-05
I0711 21:14:46.924943 25943 solver.cpp:290] Iteration 19500 (5.87393 iter/s, 17.0244s/100 iter), loss = 0.0315655
I0711 21:14:46.924967 25943 solver.cpp:309]     Train net output #0: loss = 0.0315654 (* 1 = 0.0315654 loss)
I0711 21:14:46.924973 25943 sgd_solver.cpp:106] Iteration 19500, lr = 1e-05
I0711 21:15:03.976450 25943 solver.cpp:290] Iteration 19600 (5.86475 iter/s, 17.051s/100 iter), loss = 0.0135738
I0711 21:15:03.976534 25943 solver.cpp:309]     Train net output #0: loss = 0.0135737 (* 1 = 0.0135737 loss)
I0711 21:15:03.976544 25943 sgd_solver.cpp:106] Iteration 19600, lr = 1e-05
I0711 21:15:21.171227 25943 solver.cpp:290] Iteration 19700 (5.81591 iter/s, 17.1942s/100 iter), loss = 0.017043
I0711 21:15:21.171255 25943 solver.cpp:309]     Train net output #0: loss = 0.017043 (* 1 = 0.017043 loss)
I0711 21:15:21.171264 25943 sgd_solver.cpp:106] Iteration 19700, lr = 1e-05
I0711 21:15:38.421727 25943 solver.cpp:290] Iteration 19800 (5.7971 iter/s, 17.25s/100 iter), loss = 0.0165171
I0711 21:15:38.421808 25943 solver.cpp:309]     Train net output #0: loss = 0.0165171 (* 1 = 0.0165171 loss)
I0711 21:15:38.421818 25943 sgd_solver.cpp:106] Iteration 19800, lr = 1e-05
I0711 21:15:55.528056 25943 solver.cpp:290] Iteration 19900 (5.84598 iter/s, 17.1058s/100 iter), loss = 0.0414882
I0711 21:15:55.528080 25943 solver.cpp:309]     Train net output #0: loss = 0.0414881 (* 1 = 0.0414881 loss)
I0711 21:15:55.528087 25943 sgd_solver.cpp:106] Iteration 19900, lr = 1e-05
I0711 21:16:12.315646 25943 solver.cpp:594] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/l1reg/cityscapes5_jsegnet21v2_iter_20000.caffemodel
I0711 21:16:12.368604 25943 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/l1reg/cityscapes5_jsegnet21v2_iter_20000.solverstate
I0711 21:16:12.385255 25943 solver.cpp:467] Iteration 20000, Testing net (#0)
I0711 21:16:58.867396 25943 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.953757
I0711 21:16:58.867462 25943 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999702
I0711 21:16:58.867470 25943 solver.cpp:540]     Test net output #2: loss = 0.162254 (* 1 = 0.162254 loss)
I0711 21:16:59.058522 25943 solver.cpp:290] Iteration 20000 (1.57409 iter/s, 63.5287s/100 iter), loss = 0.0195255
I0711 21:16:59.058545 25943 solver.cpp:309]     Train net output #0: loss = 0.0195255 (* 1 = 0.0195255 loss)
I0711 21:16:59.058552 25943 sgd_solver.cpp:106] Iteration 20000, lr = 1e-05
I0711 21:17:15.857130 25943 solver.cpp:290] Iteration 20100 (5.95305 iter/s, 16.7981s/100 iter), loss = 0.0316774
I0711 21:17:15.857156 25943 solver.cpp:309]     Train net output #0: loss = 0.0316773 (* 1 = 0.0316773 loss)
I0711 21:17:15.857164 25943 sgd_solver.cpp:106] Iteration 20100, lr = 1e-05
I0711 21:17:32.909037 25943 solver.cpp:290] Iteration 20200 (5.86462 iter/s, 17.0514s/100 iter), loss = 0.0167925
I0711 21:17:32.909087 25943 solver.cpp:309]     Train net output #0: loss = 0.0167925 (* 1 = 0.0167925 loss)
I0711 21:17:32.909096 25943 sgd_solver.cpp:106] Iteration 20200, lr = 1e-05
I0711 21:17:49.940822 25943 solver.cpp:290] Iteration 20300 (5.87156 iter/s, 17.0313s/100 iter), loss = 0.0180656
I0711 21:17:49.940845 25943 solver.cpp:309]     Train net output #0: loss = 0.0180656 (* 1 = 0.0180656 loss)
I0711 21:17:49.940850 25943 sgd_solver.cpp:106] Iteration 20300, lr = 1e-05
I0711 21:18:07.374302 25943 solver.cpp:290] Iteration 20400 (5.73625 iter/s, 17.433s/100 iter), loss = 0.0462751
I0711 21:18:07.374382 25943 solver.cpp:309]     Train net output #0: loss = 0.046275 (* 1 = 0.046275 loss)
I0711 21:18:07.374389 25943 sgd_solver.cpp:106] Iteration 20400, lr = 1e-05
I0711 21:18:24.634522 25943 solver.cpp:290] Iteration 20500 (5.79385 iter/s, 17.2597s/100 iter), loss = 0.0311642
I0711 21:18:24.634546 25943 solver.cpp:309]     Train net output #0: loss = 0.0311642 (* 1 = 0.0311642 loss)
I0711 21:18:24.634552 25943 sgd_solver.cpp:106] Iteration 20500, lr = 1e-05
I0711 21:18:41.608544 25943 solver.cpp:290] Iteration 20600 (5.89153 iter/s, 16.9735s/100 iter), loss = 0.0198744
I0711 21:18:41.608594 25943 solver.cpp:309]     Train net output #0: loss = 0.0198743 (* 1 = 0.0198743 loss)
I0711 21:18:41.608603 25943 sgd_solver.cpp:106] Iteration 20600, lr = 1e-05
I0711 21:18:58.576478 25943 solver.cpp:290] Iteration 20700 (5.89365 iter/s, 16.9674s/100 iter), loss = 0.0329889
I0711 21:18:58.576500 25943 solver.cpp:309]     Train net output #0: loss = 0.0329888 (* 1 = 0.0329888 loss)
I0711 21:18:58.576508 25943 sgd_solver.cpp:106] Iteration 20700, lr = 1e-05
I0711 21:19:15.656713 25943 solver.cpp:290] Iteration 20800 (5.85489 iter/s, 17.0797s/100 iter), loss = 0.0208864
I0711 21:19:15.656764 25943 solver.cpp:309]     Train net output #0: loss = 0.0208863 (* 1 = 0.0208863 loss)
I0711 21:19:15.656771 25943 sgd_solver.cpp:106] Iteration 20800, lr = 1e-05
I0711 21:19:32.508682 25943 solver.cpp:290] Iteration 20900 (5.9342 iter/s, 16.8515s/100 iter), loss = 0.019833
I0711 21:19:32.508707 25943 solver.cpp:309]     Train net output #0: loss = 0.0198329 (* 1 = 0.0198329 loss)
I0711 21:19:32.508715 25943 sgd_solver.cpp:106] Iteration 20900, lr = 1e-05
I0711 21:19:49.734633 25943 solver.cpp:290] Iteration 21000 (5.80536 iter/s, 17.2255s/100 iter), loss = 0.0355665
I0711 21:19:49.734711 25943 solver.cpp:309]     Train net output #0: loss = 0.0355665 (* 1 = 0.0355665 loss)
I0711 21:19:49.734719 25943 sgd_solver.cpp:106] Iteration 21000, lr = 1e-05
I0711 21:20:06.816458 25943 solver.cpp:290] Iteration 21100 (5.85436 iter/s, 17.0813s/100 iter), loss = 0.0238393
I0711 21:20:06.816488 25943 solver.cpp:309]     Train net output #0: loss = 0.0238392 (* 1 = 0.0238392 loss)
I0711 21:20:06.816496 25943 sgd_solver.cpp:106] Iteration 21100, lr = 1e-05
I0711 21:20:24.113580 25943 solver.cpp:290] Iteration 21200 (5.78148 iter/s, 17.2966s/100 iter), loss = 0.0384313
I0711 21:20:24.113653 25943 solver.cpp:309]     Train net output #0: loss = 0.0384312 (* 1 = 0.0384312 loss)
I0711 21:20:24.113662 25943 sgd_solver.cpp:106] Iteration 21200, lr = 1e-05
I0711 21:20:41.090298 25943 solver.cpp:290] Iteration 21300 (5.89061 iter/s, 16.9762s/100 iter), loss = 0.0180406
I0711 21:20:41.090325 25943 solver.cpp:309]     Train net output #0: loss = 0.0180406 (* 1 = 0.0180406 loss)
I0711 21:20:41.090335 25943 sgd_solver.cpp:106] Iteration 21300, lr = 1e-05
I0711 21:20:58.058152 25943 solver.cpp:290] Iteration 21400 (5.89367 iter/s, 16.9674s/100 iter), loss = 0.0244459
I0711 21:20:58.058204 25943 solver.cpp:309]     Train net output #0: loss = 0.0244458 (* 1 = 0.0244458 loss)
I0711 21:20:58.058212 25943 sgd_solver.cpp:106] Iteration 21400, lr = 1e-05
I0711 21:21:15.314074 25943 solver.cpp:290] Iteration 21500 (5.79529 iter/s, 17.2554s/100 iter), loss = 0.0341594
I0711 21:21:15.314100 25943 solver.cpp:309]     Train net output #0: loss = 0.0341594 (* 1 = 0.0341594 loss)
I0711 21:21:15.314106 25943 sgd_solver.cpp:106] Iteration 21500, lr = 1e-05
I0711 21:21:32.558256 25943 solver.cpp:290] Iteration 21600 (5.79922 iter/s, 17.2437s/100 iter), loss = 0.0317659
I0711 21:21:32.558354 25943 solver.cpp:309]     Train net output #0: loss = 0.0317659 (* 1 = 0.0317659 loss)
I0711 21:21:32.558365 25943 sgd_solver.cpp:106] Iteration 21600, lr = 1e-05
I0711 21:21:49.630949 25943 solver.cpp:290] Iteration 21700 (5.8575 iter/s, 17.0721s/100 iter), loss = 0.0304673
I0711 21:21:49.630978 25943 solver.cpp:309]     Train net output #0: loss = 0.0304672 (* 1 = 0.0304672 loss)
I0711 21:21:49.630987 25943 sgd_solver.cpp:106] Iteration 21700, lr = 1e-05
I0711 21:22:06.759912 25943 solver.cpp:290] Iteration 21800 (5.83823 iter/s, 17.1285s/100 iter), loss = 0.0177323
I0711 21:22:06.759984 25943 solver.cpp:309]     Train net output #0: loss = 0.0177323 (* 1 = 0.0177323 loss)
I0711 21:22:06.759991 25943 sgd_solver.cpp:106] Iteration 21800, lr = 1e-05
I0711 21:22:23.825608 25943 solver.cpp:290] Iteration 21900 (5.85989 iter/s, 17.0652s/100 iter), loss = 0.0130767
I0711 21:22:23.825630 25943 solver.cpp:309]     Train net output #0: loss = 0.0130767 (* 1 = 0.0130767 loss)
I0711 21:22:23.825637 25943 sgd_solver.cpp:106] Iteration 21900, lr = 1e-05
I0711 21:22:40.810356 25943 solver.cpp:467] Iteration 22000, Testing net (#0)
I0711 21:23:27.818897 25943 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.952638
I0711 21:23:27.818994 25943 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999636
I0711 21:23:27.819002 25943 solver.cpp:540]     Test net output #2: loss = 0.168657 (* 1 = 0.168657 loss)
I0711 21:23:28.012353 25943 solver.cpp:290] Iteration 22000 (1.558 iter/s, 64.185s/100 iter), loss = 0.0289806
I0711 21:23:28.012382 25943 solver.cpp:309]     Train net output #0: loss = 0.0289805 (* 1 = 0.0289805 loss)
I0711 21:23:28.012387 25943 sgd_solver.cpp:106] Iteration 22000, lr = 1e-05
I0711 21:23:47.468971 25943 solver.cpp:290] Iteration 22100 (5.13979 iter/s, 19.4561s/100 iter), loss = 0.0226487
I0711 21:23:47.468992 25943 solver.cpp:309]     Train net output #0: loss = 0.0226487 (* 1 = 0.0226487 loss)
I0711 21:23:47.468999 25943 sgd_solver.cpp:106] Iteration 22100, lr = 1e-05
I0711 21:24:04.388950 25943 solver.cpp:290] Iteration 22200 (5.91034 iter/s, 16.9195s/100 iter), loss = 0.0218852
I0711 21:24:04.388996 25943 solver.cpp:309]     Train net output #0: loss = 0.0218851 (* 1 = 0.0218851 loss)
I0711 21:24:04.389004 25943 sgd_solver.cpp:106] Iteration 22200, lr = 1e-05
I0711 21:24:21.485424 25943 solver.cpp:290] Iteration 22300 (5.84934 iter/s, 17.096s/100 iter), loss = 0.0272808
I0711 21:24:21.485451 25943 solver.cpp:309]     Train net output #0: loss = 0.0272808 (* 1 = 0.0272808 loss)
I0711 21:24:21.485460 25943 sgd_solver.cpp:106] Iteration 22300, lr = 1e-05
I0711 21:24:38.561431 25943 solver.cpp:290] Iteration 22400 (5.85634 iter/s, 17.0755s/100 iter), loss = 0.0215519
I0711 21:24:38.561513 25943 solver.cpp:309]     Train net output #0: loss = 0.0215518 (* 1 = 0.0215518 loss)
I0711 21:24:38.561522 25943 sgd_solver.cpp:106] Iteration 22400, lr = 1e-05
I0711 21:24:55.650635 25943 solver.cpp:290] Iteration 22500 (5.85184 iter/s, 17.0887s/100 iter), loss = 0.0259647
I0711 21:24:55.650657 25943 solver.cpp:309]     Train net output #0: loss = 0.0259646 (* 1 = 0.0259646 loss)
I0711 21:24:55.650665 25943 sgd_solver.cpp:106] Iteration 22500, lr = 1e-05
I0711 21:25:12.816769 25943 solver.cpp:290] Iteration 22600 (5.82559 iter/s, 17.1656s/100 iter), loss = 0.0325643
I0711 21:25:12.816876 25943 solver.cpp:309]     Train net output #0: loss = 0.0325643 (* 1 = 0.0325643 loss)
I0711 21:25:12.816884 25943 sgd_solver.cpp:106] Iteration 22600, lr = 1e-05
I0711 21:25:30.122490 25943 solver.cpp:290] Iteration 22700 (5.77863 iter/s, 17.3051s/100 iter), loss = 0.0191631
I0711 21:25:30.122519 25943 solver.cpp:309]     Train net output #0: loss = 0.0191631 (* 1 = 0.0191631 loss)
I0711 21:25:30.122525 25943 sgd_solver.cpp:106] Iteration 22700, lr = 1e-05
I0711 21:25:47.406225 25943 solver.cpp:290] Iteration 22800 (5.78595 iter/s, 17.2832s/100 iter), loss = 0.0215036
I0711 21:25:47.406288 25943 solver.cpp:309]     Train net output #0: loss = 0.0215036 (* 1 = 0.0215036 loss)
I0711 21:25:47.406296 25943 sgd_solver.cpp:106] Iteration 22800, lr = 1e-05
I0711 21:26:04.651540 25943 solver.cpp:290] Iteration 22900 (5.79886 iter/s, 17.2448s/100 iter), loss = 0.0181543
I0711 21:26:04.651567 25943 solver.cpp:309]     Train net output #0: loss = 0.0181543 (* 1 = 0.0181543 loss)
I0711 21:26:04.651574 25943 sgd_solver.cpp:106] Iteration 22900, lr = 1e-05
I0711 21:26:21.904258 25943 solver.cpp:290] Iteration 23000 (5.79636 iter/s, 17.2522s/100 iter), loss = 0.0244345
I0711 21:26:21.904350 25943 solver.cpp:309]     Train net output #0: loss = 0.0244344 (* 1 = 0.0244344 loss)
I0711 21:26:21.904361 25943 sgd_solver.cpp:106] Iteration 23000, lr = 1e-05
I0711 21:26:39.047128 25943 solver.cpp:290] Iteration 23100 (5.83352 iter/s, 17.1423s/100 iter), loss = 0.0236133
I0711 21:26:39.047150 25943 solver.cpp:309]     Train net output #0: loss = 0.0236133 (* 1 = 0.0236133 loss)
I0711 21:26:39.047158 25943 sgd_solver.cpp:106] Iteration 23100, lr = 1e-05
I0711 21:26:56.133622 25943 solver.cpp:290] Iteration 23200 (5.85274 iter/s, 17.086s/100 iter), loss = 0.0201395
I0711 21:26:56.133673 25943 solver.cpp:309]     Train net output #0: loss = 0.0201394 (* 1 = 0.0201394 loss)
I0711 21:26:56.133683 25943 sgd_solver.cpp:106] Iteration 23200, lr = 1e-05
I0711 21:27:13.310495 25943 solver.cpp:290] Iteration 23300 (5.82195 iter/s, 17.1764s/100 iter), loss = 0.0170816
I0711 21:27:13.310518 25943 solver.cpp:309]     Train net output #0: loss = 0.0170815 (* 1 = 0.0170815 loss)
I0711 21:27:13.310524 25943 sgd_solver.cpp:106] Iteration 23300, lr = 1e-05
I0711 21:27:30.372288 25943 solver.cpp:290] Iteration 23400 (5.86121 iter/s, 17.0613s/100 iter), loss = 0.0288858
I0711 21:27:30.372349 25943 solver.cpp:309]     Train net output #0: loss = 0.0288857 (* 1 = 0.0288857 loss)
I0711 21:27:30.372357 25943 sgd_solver.cpp:106] Iteration 23400, lr = 1e-05
I0711 21:27:47.410856 25943 solver.cpp:290] Iteration 23500 (5.86922 iter/s, 17.0381s/100 iter), loss = 0.0211866
I0711 21:27:47.410882 25943 solver.cpp:309]     Train net output #0: loss = 0.0211865 (* 1 = 0.0211865 loss)
I0711 21:27:47.410889 25943 sgd_solver.cpp:106] Iteration 23500, lr = 1e-05
I0711 21:28:04.365897 25943 solver.cpp:290] Iteration 23600 (5.89812 iter/s, 16.9546s/100 iter), loss = 0.02923
I0711 21:28:04.365977 25943 solver.cpp:309]     Train net output #0: loss = 0.02923 (* 1 = 0.02923 loss)
I0711 21:28:04.365983 25943 sgd_solver.cpp:106] Iteration 23600, lr = 1e-05
I0711 21:28:21.517388 25943 solver.cpp:290] Iteration 23700 (5.83058 iter/s, 17.1509s/100 iter), loss = 0.0228848
I0711 21:28:21.517413 25943 solver.cpp:309]     Train net output #0: loss = 0.0228848 (* 1 = 0.0228848 loss)
I0711 21:28:21.517421 25943 sgd_solver.cpp:106] Iteration 23700, lr = 1e-05
I0711 21:28:38.679882 25943 solver.cpp:290] Iteration 23800 (5.82682 iter/s, 17.162s/100 iter), loss = 0.0297706
I0711 21:28:38.679996 25943 solver.cpp:309]     Train net output #0: loss = 0.0297706 (* 1 = 0.0297706 loss)
I0711 21:28:38.680022 25943 sgd_solver.cpp:106] Iteration 23800, lr = 1e-05
I0711 21:28:55.734840 25943 solver.cpp:290] Iteration 23900 (5.86359 iter/s, 17.0544s/100 iter), loss = 0.0186841
I0711 21:28:55.734866 25943 solver.cpp:309]     Train net output #0: loss = 0.0186841 (* 1 = 0.0186841 loss)
I0711 21:28:55.734875 25943 sgd_solver.cpp:106] Iteration 23900, lr = 1e-05
I0711 21:29:12.533357 25943 solver.cpp:467] Iteration 24000, Testing net (#0)
I0711 21:29:59.716284 25943 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.95235
I0711 21:29:59.716329 25943 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999729
I0711 21:29:59.716336 25943 solver.cpp:540]     Test net output #2: loss = 0.166549 (* 1 = 0.166549 loss)
I0711 21:29:59.905439 26106 sgd_solver.cpp:46] MultiStep Status: Iteration 24000, step = 1
I0711 21:29:59.905433 25943 solver.cpp:290] Iteration 24000 (1.55839 iter/s, 64.1689s/100 iter), loss = 0.0213653
I0711 21:29:59.905439 26105 sgd_solver.cpp:46] MultiStep Status: Iteration 24000, step = 1
I0711 21:29:59.905470 25943 solver.cpp:309]     Train net output #0: loss = 0.0213652 (* 1 = 0.0213652 loss)
I0711 21:29:59.905481 25943 sgd_solver.cpp:46] MultiStep Status: Iteration 24000, step = 1
I0711 21:29:59.905486 25943 sgd_solver.cpp:106] Iteration 24000, lr = 1e-06
I0711 21:30:16.680512 25943 solver.cpp:290] Iteration 24100 (5.9614 iter/s, 16.7746s/100 iter), loss = 0.0192085
I0711 21:30:16.680536 25943 solver.cpp:309]     Train net output #0: loss = 0.0192085 (* 1 = 0.0192085 loss)
I0711 21:30:16.680543 25943 sgd_solver.cpp:106] Iteration 24100, lr = 1e-06
I0711 21:30:33.829064 25943 solver.cpp:290] Iteration 24200 (5.83156 iter/s, 17.1481s/100 iter), loss = 0.0318975
I0711 21:30:33.829712 25943 solver.cpp:309]     Train net output #0: loss = 0.0318975 (* 1 = 0.0318975 loss)
I0711 21:30:33.829736 25943 sgd_solver.cpp:106] Iteration 24200, lr = 1e-06
I0711 21:30:50.966624 25943 solver.cpp:290] Iteration 24300 (5.83551 iter/s, 17.1364s/100 iter), loss = 0.0290704
I0711 21:30:50.966653 25943 solver.cpp:309]     Train net output #0: loss = 0.0290704 (* 1 = 0.0290704 loss)
I0711 21:30:50.966662 25943 sgd_solver.cpp:106] Iteration 24300, lr = 1e-06
I0711 21:31:08.186177 25943 solver.cpp:290] Iteration 24400 (5.80752 iter/s, 17.2191s/100 iter), loss = 0.0149766
I0711 21:31:08.186228 25943 solver.cpp:309]     Train net output #0: loss = 0.0149766 (* 1 = 0.0149766 loss)
I0711 21:31:08.186235 25943 sgd_solver.cpp:106] Iteration 24400, lr = 1e-06
I0711 21:31:25.302429 25943 solver.cpp:290] Iteration 24500 (5.84258 iter/s, 17.1157s/100 iter), loss = 0.0211232
I0711 21:31:25.302456 25943 solver.cpp:309]     Train net output #0: loss = 0.0211232 (* 1 = 0.0211232 loss)
I0711 21:31:25.302462 25943 sgd_solver.cpp:106] Iteration 24500, lr = 1e-06
I0711 21:31:42.741037 25943 solver.cpp:290] Iteration 24600 (5.73457 iter/s, 17.4381s/100 iter), loss = 0.0329412
I0711 21:31:42.741117 25943 solver.cpp:309]     Train net output #0: loss = 0.0329412 (* 1 = 0.0329412 loss)
I0711 21:31:42.741125 25943 sgd_solver.cpp:106] Iteration 24600, lr = 1e-06
I0711 21:31:59.768709 25943 solver.cpp:290] Iteration 24700 (5.87298 iter/s, 17.0271s/100 iter), loss = 0.0214737
I0711 21:31:59.768734 25943 solver.cpp:309]     Train net output #0: loss = 0.0214736 (* 1 = 0.0214736 loss)
I0711 21:31:59.768741 25943 sgd_solver.cpp:106] Iteration 24700, lr = 1e-06
I0711 21:32:16.897677 25943 solver.cpp:290] Iteration 24800 (5.83823 iter/s, 17.1285s/100 iter), loss = 0.0198339
I0711 21:32:16.897774 25943 solver.cpp:309]     Train net output #0: loss = 0.0198339 (* 1 = 0.0198339 loss)
I0711 21:32:16.897783 25943 sgd_solver.cpp:106] Iteration 24800, lr = 1e-06
I0711 21:32:33.944886 25943 solver.cpp:290] Iteration 24900 (5.86625 iter/s, 17.0467s/100 iter), loss = 0.0461802
I0711 21:32:33.944911 25943 solver.cpp:309]     Train net output #0: loss = 0.0461802 (* 1 = 0.0461802 loss)
I0711 21:32:33.944917 25943 sgd_solver.cpp:106] Iteration 24900, lr = 1e-06
I0711 21:32:50.902076 25943 solver.cpp:290] Iteration 25000 (5.89737 iter/s, 16.9567s/100 iter), loss = 0.0267457
I0711 21:32:50.902204 25943 solver.cpp:309]     Train net output #0: loss = 0.0267457 (* 1 = 0.0267457 loss)
I0711 21:32:50.902214 25943 sgd_solver.cpp:106] Iteration 25000, lr = 1e-06
I0711 21:33:08.086933 25943 solver.cpp:290] Iteration 25100 (5.81928 iter/s, 17.1843s/100 iter), loss = 0.0227588
I0711 21:33:08.086958 25943 solver.cpp:309]     Train net output #0: loss = 0.0227588 (* 1 = 0.0227588 loss)
I0711 21:33:08.086966 25943 sgd_solver.cpp:106] Iteration 25100, lr = 1e-06
I0711 21:33:25.116335 25943 solver.cpp:290] Iteration 25200 (5.87236 iter/s, 17.0289s/100 iter), loss = 0.0601136
I0711 21:33:25.116389 25943 solver.cpp:309]     Train net output #0: loss = 0.0601136 (* 1 = 0.0601136 loss)
I0711 21:33:25.116400 25943 sgd_solver.cpp:106] Iteration 25200, lr = 1e-06
I0711 21:33:42.289834 25943 solver.cpp:290] Iteration 25300 (5.8231 iter/s, 17.173s/100 iter), loss = 0.0201969
I0711 21:33:42.289860 25943 solver.cpp:309]     Train net output #0: loss = 0.0201969 (* 1 = 0.0201969 loss)
I0711 21:33:42.289867 25943 sgd_solver.cpp:106] Iteration 25300, lr = 1e-06
I0711 21:33:59.397464 25943 solver.cpp:290] Iteration 25400 (5.84551 iter/s, 17.1071s/100 iter), loss = 0.0243403
I0711 21:33:59.397543 25943 solver.cpp:309]     Train net output #0: loss = 0.0243403 (* 1 = 0.0243403 loss)
I0711 21:33:59.397554 25943 sgd_solver.cpp:106] Iteration 25400, lr = 1e-06
I0711 21:34:16.499430 25943 solver.cpp:290] Iteration 25500 (5.84746 iter/s, 17.1014s/100 iter), loss = 0.0197478
I0711 21:34:16.499455 25943 solver.cpp:309]     Train net output #0: loss = 0.0197478 (* 1 = 0.0197478 loss)
I0711 21:34:16.499461 25943 sgd_solver.cpp:106] Iteration 25500, lr = 1e-06
I0711 21:34:33.613507 25943 solver.cpp:290] Iteration 25600 (5.84331 iter/s, 17.1136s/100 iter), loss = 0.0307461
I0711 21:34:33.613579 25943 solver.cpp:309]     Train net output #0: loss = 0.0307461 (* 1 = 0.0307461 loss)
I0711 21:34:33.613587 25943 sgd_solver.cpp:106] Iteration 25600, lr = 1e-06
I0711 21:34:51.061378 25943 solver.cpp:290] Iteration 25700 (5.73154 iter/s, 17.4473s/100 iter), loss = 0.0211181
I0711 21:34:51.061403 25943 solver.cpp:309]     Train net output #0: loss = 0.0211181 (* 1 = 0.0211181 loss)
I0711 21:34:51.061408 25943 sgd_solver.cpp:106] Iteration 25700, lr = 1e-06
I0711 21:35:08.198822 25943 solver.cpp:290] Iteration 25800 (5.83534 iter/s, 17.137s/100 iter), loss = 0.030157
I0711 21:35:08.198875 25943 solver.cpp:309]     Train net output #0: loss = 0.030157 (* 1 = 0.030157 loss)
I0711 21:35:08.198885 25943 sgd_solver.cpp:106] Iteration 25800, lr = 1e-06
I0711 21:35:25.393620 25943 solver.cpp:290] Iteration 25900 (5.81589 iter/s, 17.1943s/100 iter), loss = 0.0165308
I0711 21:35:25.393647 25943 solver.cpp:309]     Train net output #0: loss = 0.0165308 (* 1 = 0.0165308 loss)
I0711 21:35:25.393656 25943 sgd_solver.cpp:106] Iteration 25900, lr = 1e-06
I0711 21:35:42.346119 25943 solver.cpp:467] Iteration 26000, Testing net (#0)
I0711 21:36:28.713353 25943 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.953353
I0711 21:36:28.713435 25943 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999623
I0711 21:36:28.713443 25943 solver.cpp:540]     Test net output #2: loss = 0.165946 (* 1 = 0.165946 loss)
I0711 21:36:28.894820 25943 solver.cpp:290] Iteration 26000 (1.57482 iter/s, 63.4995s/100 iter), loss = 0.020636
I0711 21:36:28.894843 25943 solver.cpp:309]     Train net output #0: loss = 0.020636 (* 1 = 0.020636 loss)
I0711 21:36:28.894850 25943 sgd_solver.cpp:106] Iteration 26000, lr = 1e-06
I0711 21:36:45.676606 25943 solver.cpp:290] Iteration 26100 (5.95901 iter/s, 16.7813s/100 iter), loss = 0.0201982
I0711 21:36:45.676631 25943 solver.cpp:309]     Train net output #0: loss = 0.0201982 (* 1 = 0.0201982 loss)
I0711 21:36:45.676637 25943 sgd_solver.cpp:106] Iteration 26100, lr = 1e-06
I0711 21:37:02.683965 25943 solver.cpp:290] Iteration 26200 (5.87998 iter/s, 17.0069s/100 iter), loss = 0.0258706
I0711 21:37:02.684077 25943 solver.cpp:309]     Train net output #0: loss = 0.0258706 (* 1 = 0.0258706 loss)
I0711 21:37:02.684088 25943 sgd_solver.cpp:106] Iteration 26200, lr = 1e-06
I0711 21:37:19.588196 25943 solver.cpp:290] Iteration 26300 (5.91588 iter/s, 16.9037s/100 iter), loss = 0.0208248
I0711 21:37:19.588224 25943 solver.cpp:309]     Train net output #0: loss = 0.0208248 (* 1 = 0.0208248 loss)
I0711 21:37:19.588233 25943 sgd_solver.cpp:106] Iteration 26300, lr = 1e-06
I0711 21:37:36.680995 25943 solver.cpp:290] Iteration 26400 (5.85059 iter/s, 17.0923s/100 iter), loss = 0.0190843
I0711 21:37:36.681104 25943 solver.cpp:309]     Train net output #0: loss = 0.0190843 (* 1 = 0.0190843 loss)
I0711 21:37:36.681114 25943 sgd_solver.cpp:106] Iteration 26400, lr = 1e-06
I0711 21:37:53.794028 25943 solver.cpp:290] Iteration 26500 (5.8437 iter/s, 17.1125s/100 iter), loss = 0.0260022
I0711 21:37:53.794052 25943 solver.cpp:309]     Train net output #0: loss = 0.0260022 (* 1 = 0.0260022 loss)
I0711 21:37:53.794059 25943 sgd_solver.cpp:106] Iteration 26500, lr = 1e-06
I0711 21:38:10.790796 25943 solver.cpp:290] Iteration 26600 (5.88364 iter/s, 16.9963s/100 iter), loss = 0.036152
I0711 21:38:10.790853 25943 solver.cpp:309]     Train net output #0: loss = 0.036152 (* 1 = 0.036152 loss)
I0711 21:38:10.790863 25943 sgd_solver.cpp:106] Iteration 26600, lr = 1e-06
I0711 21:38:28.402905 25943 solver.cpp:290] Iteration 26700 (5.67808 iter/s, 17.6116s/100 iter), loss = 0.0186075
I0711 21:38:28.402938 25943 solver.cpp:309]     Train net output #0: loss = 0.0186075 (* 1 = 0.0186075 loss)
I0711 21:38:28.402947 25943 sgd_solver.cpp:106] Iteration 26700, lr = 1e-06
I0711 21:38:46.076254 25943 solver.cpp:290] Iteration 26800 (5.6584 iter/s, 17.6728s/100 iter), loss = 0.036938
I0711 21:38:46.076335 25943 solver.cpp:309]     Train net output #0: loss = 0.036938 (* 1 = 0.036938 loss)
I0711 21:38:46.076345 25943 sgd_solver.cpp:106] Iteration 26800, lr = 1e-06
I0711 21:39:03.626986 25943 solver.cpp:290] Iteration 26900 (5.69795 iter/s, 17.5502s/100 iter), loss = 0.0278506
I0711 21:39:03.627013 25943 solver.cpp:309]     Train net output #0: loss = 0.0278506 (* 1 = 0.0278506 loss)
I0711 21:39:03.627022 25943 sgd_solver.cpp:106] Iteration 26900, lr = 1e-06
I0711 21:39:20.909793 25943 solver.cpp:290] Iteration 27000 (5.78626 iter/s, 17.2823s/100 iter), loss = 0.0172372
I0711 21:39:20.909862 25943 solver.cpp:309]     Train net output #0: loss = 0.0172372 (* 1 = 0.0172372 loss)
I0711 21:39:20.909871 25943 sgd_solver.cpp:106] Iteration 27000, lr = 1e-06
I0711 21:39:38.461256 25943 solver.cpp:290] Iteration 27100 (5.69771 iter/s, 17.5509s/100 iter), loss = 0.0162051
I0711 21:39:38.461284 25943 solver.cpp:309]     Train net output #0: loss = 0.0162051 (* 1 = 0.0162051 loss)
I0711 21:39:38.461293 25943 sgd_solver.cpp:106] Iteration 27100, lr = 1e-06
I0711 21:39:56.000830 25943 solver.cpp:290] Iteration 27200 (5.70156 iter/s, 17.5391s/100 iter), loss = 0.0246519
I0711 21:39:56.000869 25943 solver.cpp:309]     Train net output #0: loss = 0.0246519 (* 1 = 0.0246519 loss)
I0711 21:39:56.000876 25943 sgd_solver.cpp:106] Iteration 27200, lr = 1e-06
I0711 21:40:13.459985 25943 solver.cpp:290] Iteration 27300 (5.72782 iter/s, 17.4586s/100 iter), loss = 0.0281194
I0711 21:40:13.460011 25943 solver.cpp:309]     Train net output #0: loss = 0.0281194 (* 1 = 0.0281194 loss)
I0711 21:40:13.460018 25943 sgd_solver.cpp:106] Iteration 27300, lr = 1e-06
I0711 21:40:30.977088 25943 solver.cpp:290] Iteration 27400 (5.70887 iter/s, 17.5166s/100 iter), loss = 0.0182657
I0711 21:40:30.977197 25943 solver.cpp:309]     Train net output #0: loss = 0.0182657 (* 1 = 0.0182657 loss)
I0711 21:40:30.977206 25943 sgd_solver.cpp:106] Iteration 27400, lr = 1e-06
I0711 21:40:48.362385 25943 solver.cpp:290] Iteration 27500 (5.75218 iter/s, 17.3847s/100 iter), loss = 0.0198907
I0711 21:40:48.362409 25943 solver.cpp:309]     Train net output #0: loss = 0.0198907 (* 1 = 0.0198907 loss)
I0711 21:40:48.362416 25943 sgd_solver.cpp:106] Iteration 27500, lr = 1e-06
I0711 21:41:05.745446 25943 solver.cpp:290] Iteration 27600 (5.75289 iter/s, 17.3826s/100 iter), loss = 0.0210297
I0711 21:41:05.745571 25943 solver.cpp:309]     Train net output #0: loss = 0.0210297 (* 1 = 0.0210297 loss)
I0711 21:41:05.745581 25943 sgd_solver.cpp:106] Iteration 27600, lr = 1e-06
I0711 21:41:23.190008 25943 solver.cpp:290] Iteration 27700 (5.73264 iter/s, 17.444s/100 iter), loss = 0.0386934
I0711 21:41:23.190033 25943 solver.cpp:309]     Train net output #0: loss = 0.0386934 (* 1 = 0.0386934 loss)
I0711 21:41:23.190042 25943 sgd_solver.cpp:106] Iteration 27700, lr = 1e-06
I0711 21:41:40.591486 25943 solver.cpp:290] Iteration 27800 (5.7468 iter/s, 17.401s/100 iter), loss = 0.0340361
I0711 21:41:40.591588 25943 solver.cpp:309]     Train net output #0: loss = 0.0340361 (* 1 = 0.0340361 loss)
I0711 21:41:40.591600 25943 sgd_solver.cpp:106] Iteration 27800, lr = 1e-06
I0711 21:41:58.073849 25943 solver.cpp:290] Iteration 27900 (5.72024 iter/s, 17.4818s/100 iter), loss = 0.0388233
I0711 21:41:58.073873 25943 solver.cpp:309]     Train net output #0: loss = 0.0388233 (* 1 = 0.0388233 loss)
I0711 21:41:58.073879 25943 sgd_solver.cpp:106] Iteration 27900, lr = 1e-06
I0711 21:42:15.356613 25943 solver.cpp:467] Iteration 28000, Testing net (#0)
I0711 21:43:05.083967 25943 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.953367
I0711 21:43:05.084040 25943 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999601
I0711 21:43:05.084049 25943 solver.cpp:540]     Test net output #2: loss = 0.167421 (* 1 = 0.167421 loss)
I0711 21:43:05.286123 25943 solver.cpp:290] Iteration 28000 (1.48786 iter/s, 67.2104s/100 iter), loss = 0.0209917
I0711 21:43:05.286146 25943 solver.cpp:309]     Train net output #0: loss = 0.0209917 (* 1 = 0.0209917 loss)
I0711 21:43:05.286152 25943 sgd_solver.cpp:106] Iteration 28000, lr = 1e-06
I0711 21:43:22.751329 25943 solver.cpp:290] Iteration 28100 (5.72584 iter/s, 17.4647s/100 iter), loss = 0.0392563
I0711 21:43:22.751381 25943 solver.cpp:309]     Train net output #0: loss = 0.0392563 (* 1 = 0.0392563 loss)
I0711 21:43:22.751395 25943 sgd_solver.cpp:106] Iteration 28100, lr = 1e-06
I0711 21:43:40.278662 25943 solver.cpp:290] Iteration 28200 (5.70555 iter/s, 17.5268s/100 iter), loss = 0.0238705
I0711 21:43:40.278710 25943 solver.cpp:309]     Train net output #0: loss = 0.0238705 (* 1 = 0.0238705 loss)
I0711 21:43:40.278720 25943 sgd_solver.cpp:106] Iteration 28200, lr = 1e-06
I0711 21:43:57.328510 25943 solver.cpp:290] Iteration 28300 (5.86533 iter/s, 17.0493s/100 iter), loss = 0.0234409
I0711 21:43:57.328538 25943 solver.cpp:309]     Train net output #0: loss = 0.0234409 (* 1 = 0.0234409 loss)
I0711 21:43:57.328547 25943 sgd_solver.cpp:106] Iteration 28300, lr = 1e-06
I0711 21:44:14.852725 25943 solver.cpp:290] Iteration 28400 (5.70656 iter/s, 17.5237s/100 iter), loss = 0.0176735
I0711 21:44:14.852787 25943 solver.cpp:309]     Train net output #0: loss = 0.0176735 (* 1 = 0.0176735 loss)
I0711 21:44:14.852798 25943 sgd_solver.cpp:106] Iteration 28400, lr = 1e-06
I0711 21:44:32.207509 25943 solver.cpp:290] Iteration 28500 (5.76228 iter/s, 17.3543s/100 iter), loss = 0.0183483
I0711 21:44:32.207536 25943 solver.cpp:309]     Train net output #0: loss = 0.0183482 (* 1 = 0.0183482 loss)
I0711 21:44:32.207545 25943 sgd_solver.cpp:106] Iteration 28500, lr = 1e-06
I0711 21:44:49.703330 25943 solver.cpp:290] Iteration 28600 (5.71582 iter/s, 17.4953s/100 iter), loss = 0.0224893
I0711 21:44:49.703702 25943 solver.cpp:309]     Train net output #0: loss = 0.0224893 (* 1 = 0.0224893 loss)
I0711 21:44:49.703709 25943 sgd_solver.cpp:106] Iteration 28600, lr = 1e-06
I0711 21:45:06.898773 25943 solver.cpp:290] Iteration 28700 (5.81578 iter/s, 17.1946s/100 iter), loss = 0.0246763
I0711 21:45:06.898815 25943 solver.cpp:309]     Train net output #0: loss = 0.0246763 (* 1 = 0.0246763 loss)
I0711 21:45:06.898828 25943 sgd_solver.cpp:106] Iteration 28700, lr = 1e-06
I0711 21:45:24.236251 25943 solver.cpp:290] Iteration 28800 (5.76802 iter/s, 17.337s/100 iter), loss = 0.01676
I0711 21:45:24.236359 25943 solver.cpp:309]     Train net output #0: loss = 0.01676 (* 1 = 0.01676 loss)
I0711 21:45:24.236371 25943 sgd_solver.cpp:106] Iteration 28800, lr = 1e-06
I0711 21:45:41.705723 25943 solver.cpp:290] Iteration 28900 (5.72446 iter/s, 17.4689s/100 iter), loss = 0.0236212
I0711 21:45:41.705746 25943 solver.cpp:309]     Train net output #0: loss = 0.0236212 (* 1 = 0.0236212 loss)
I0711 21:45:41.705752 25943 sgd_solver.cpp:106] Iteration 28900, lr = 1e-06
I0711 21:45:59.040514 25943 solver.cpp:290] Iteration 29000 (5.76891 iter/s, 17.3343s/100 iter), loss = 0.0189185
I0711 21:45:59.040594 25943 solver.cpp:309]     Train net output #0: loss = 0.0189185 (* 1 = 0.0189185 loss)
I0711 21:45:59.040602 25943 sgd_solver.cpp:106] Iteration 29000, lr = 1e-06
I0711 21:46:16.488613 25943 solver.cpp:290] Iteration 29100 (5.73146 iter/s, 17.4475s/100 iter), loss = 0.018771
I0711 21:46:16.488637 25943 solver.cpp:309]     Train net output #0: loss = 0.018771 (* 1 = 0.018771 loss)
I0711 21:46:16.488644 25943 sgd_solver.cpp:106] Iteration 29100, lr = 1e-06
I0711 21:46:34.117612 25943 solver.cpp:290] Iteration 29200 (5.67263 iter/s, 17.6285s/100 iter), loss = 0.01425
I0711 21:46:34.117689 25943 solver.cpp:309]     Train net output #0: loss = 0.01425 (* 1 = 0.01425 loss)
I0711 21:46:34.117697 25943 sgd_solver.cpp:106] Iteration 29200, lr = 1e-06
I0711 21:46:51.525640 25943 solver.cpp:290] Iteration 29300 (5.74466 iter/s, 17.4075s/100 iter), loss = 0.0384129
I0711 21:46:51.525671 25943 solver.cpp:309]     Train net output #0: loss = 0.0384129 (* 1 = 0.0384129 loss)
I0711 21:46:51.525679 25943 sgd_solver.cpp:106] Iteration 29300, lr = 1e-06
I0711 21:47:08.910213 25943 solver.cpp:290] Iteration 29400 (5.75239 iter/s, 17.3841s/100 iter), loss = 0.0276954
I0711 21:47:08.910303 25943 solver.cpp:309]     Train net output #0: loss = 0.0276954 (* 1 = 0.0276954 loss)
I0711 21:47:08.910315 25943 sgd_solver.cpp:106] Iteration 29400, lr = 1e-06
I0711 21:47:26.214078 25943 solver.cpp:290] Iteration 29500 (5.77924 iter/s, 17.3033s/100 iter), loss = 0.0209448
I0711 21:47:26.214100 25943 solver.cpp:309]     Train net output #0: loss = 0.0209448 (* 1 = 0.0209448 loss)
I0711 21:47:26.214107 25943 sgd_solver.cpp:106] Iteration 29500, lr = 1e-06
I0711 21:47:43.621053 25943 solver.cpp:290] Iteration 29600 (5.74499 iter/s, 17.4065s/100 iter), loss = 0.0419535
I0711 21:47:43.621109 25943 solver.cpp:309]     Train net output #0: loss = 0.0419535 (* 1 = 0.0419535 loss)
I0711 21:47:43.621116 25943 sgd_solver.cpp:106] Iteration 29600, lr = 1e-06
I0711 21:48:00.959818 25943 solver.cpp:290] Iteration 29700 (5.7676 iter/s, 17.3382s/100 iter), loss = 0.033433
I0711 21:48:00.959843 25943 solver.cpp:309]     Train net output #0: loss = 0.033433 (* 1 = 0.033433 loss)
I0711 21:48:00.959851 25943 sgd_solver.cpp:106] Iteration 29700, lr = 1e-06
I0711 21:48:18.601085 25943 solver.cpp:290] Iteration 29800 (5.66869 iter/s, 17.6408s/100 iter), loss = 0.0203752
I0711 21:48:18.601205 25943 solver.cpp:309]     Train net output #0: loss = 0.0203752 (* 1 = 0.0203752 loss)
I0711 21:48:18.601240 25943 sgd_solver.cpp:106] Iteration 29800, lr = 1e-06
I0711 21:48:35.931627 25943 solver.cpp:290] Iteration 29900 (5.77036 iter/s, 17.33s/100 iter), loss = 0.0252933
I0711 21:48:35.931654 25943 solver.cpp:309]     Train net output #0: loss = 0.0252933 (* 1 = 0.0252933 loss)
I0711 21:48:35.931663 25943 sgd_solver.cpp:106] Iteration 29900, lr = 1e-06
I0711 21:48:53.380795 25943 solver.cpp:594] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/l1reg/cityscapes5_jsegnet21v2_iter_30000.caffemodel
I0711 21:48:53.464057 25943 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/l1reg/cityscapes5_jsegnet21v2_iter_30000.solverstate
I0711 21:48:53.497918 25943 solver.cpp:467] Iteration 30000, Testing net (#0)
I0711 21:49:44.728150 25943 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.953073
I0711 21:49:44.728221 25943 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999642
I0711 21:49:44.728229 25943 solver.cpp:540]     Test net output #2: loss = 0.167014 (* 1 = 0.167014 loss)
I0711 21:49:44.918792 25943 solver.cpp:290] Iteration 30000 (1.44958 iter/s, 68.9853s/100 iter), loss = 0.0174868
I0711 21:49:44.918817 25943 solver.cpp:309]     Train net output #0: loss = 0.0174868 (* 1 = 0.0174868 loss)
I0711 21:49:44.918823 25943 sgd_solver.cpp:106] Iteration 30000, lr = 1e-06
I0711 21:50:02.162859 25943 solver.cpp:290] Iteration 30100 (5.79926 iter/s, 17.2436s/100 iter), loss = 0.0282022
I0711 21:50:02.162933 25943 solver.cpp:309]     Train net output #0: loss = 0.0282022 (* 1 = 0.0282022 loss)
I0711 21:50:02.162952 25943 sgd_solver.cpp:106] Iteration 30100, lr = 1e-06
I0711 21:50:19.784894 25943 solver.cpp:290] Iteration 30200 (5.67489 iter/s, 17.6215s/100 iter), loss = 0.0409297
I0711 21:50:19.784989 25943 solver.cpp:309]     Train net output #0: loss = 0.0409297 (* 1 = 0.0409297 loss)
I0711 21:50:19.785001 25943 sgd_solver.cpp:106] Iteration 30200, lr = 1e-06
I0711 21:50:37.266784 25943 solver.cpp:290] Iteration 30300 (5.72039 iter/s, 17.4813s/100 iter), loss = 0.0354923
I0711 21:50:37.266808 25943 solver.cpp:309]     Train net output #0: loss = 0.0354923 (* 1 = 0.0354923 loss)
I0711 21:50:37.266813 25943 sgd_solver.cpp:106] Iteration 30300, lr = 1e-06
I0711 21:50:54.205816 25943 solver.cpp:290] Iteration 30400 (5.9037 iter/s, 16.9385s/100 iter), loss = 0.020506
I0711 21:50:54.206663 25943 solver.cpp:309]     Train net output #0: loss = 0.020506 (* 1 = 0.020506 loss)
I0711 21:50:54.206679 25943 sgd_solver.cpp:106] Iteration 30400, lr = 1e-06
I0711 21:51:11.236852 25943 solver.cpp:290] Iteration 30500 (5.87208 iter/s, 17.0297s/100 iter), loss = 0.0163799
I0711 21:51:11.236876 25943 solver.cpp:309]     Train net output #0: loss = 0.0163799 (* 1 = 0.0163799 loss)
I0711 21:51:11.236886 25943 sgd_solver.cpp:106] Iteration 30500, lr = 1e-06
I0711 21:51:28.416508 25943 solver.cpp:290] Iteration 30600 (5.821 iter/s, 17.1792s/100 iter), loss = 0.0177153
I0711 21:51:28.416594 25943 solver.cpp:309]     Train net output #0: loss = 0.0177153 (* 1 = 0.0177153 loss)
I0711 21:51:28.416605 25943 sgd_solver.cpp:106] Iteration 30600, lr = 1e-06
I0711 21:51:45.430645 25943 solver.cpp:290] Iteration 30700 (5.87765 iter/s, 17.0136s/100 iter), loss = 0.0227873
I0711 21:51:45.430670 25943 solver.cpp:309]     Train net output #0: loss = 0.0227873 (* 1 = 0.0227873 loss)
I0711 21:51:45.430676 25943 sgd_solver.cpp:106] Iteration 30700, lr = 1e-06
I0711 21:52:02.395417 25943 solver.cpp:290] Iteration 30800 (5.89474 iter/s, 16.9643s/100 iter), loss = 0.0213565
I0711 21:52:02.395473 25943 solver.cpp:309]     Train net output #0: loss = 0.0213564 (* 1 = 0.0213564 loss)
I0711 21:52:02.395481 25943 sgd_solver.cpp:106] Iteration 30800, lr = 1e-06
I0711 21:52:19.436425 25943 solver.cpp:290] Iteration 30900 (5.86838 iter/s, 17.0405s/100 iter), loss = 0.0441528
I0711 21:52:19.436451 25943 solver.cpp:309]     Train net output #0: loss = 0.0441528 (* 1 = 0.0441528 loss)
I0711 21:52:19.436458 25943 sgd_solver.cpp:106] Iteration 30900, lr = 1e-06
I0711 21:52:36.382746 25943 solver.cpp:290] Iteration 31000 (5.90116 iter/s, 16.9458s/100 iter), loss = 0.0198541
I0711 21:52:36.382800 25943 solver.cpp:309]     Train net output #0: loss = 0.0198541 (* 1 = 0.0198541 loss)
I0711 21:52:36.382808 25943 sgd_solver.cpp:106] Iteration 31000, lr = 1e-06
I0711 21:52:53.287277 25943 solver.cpp:290] Iteration 31100 (5.91575 iter/s, 16.904s/100 iter), loss = 0.0268283
I0711 21:52:53.287302 25943 solver.cpp:309]     Train net output #0: loss = 0.0268283 (* 1 = 0.0268283 loss)
I0711 21:52:53.287308 25943 sgd_solver.cpp:106] Iteration 31100, lr = 1e-06
I0711 21:53:10.507638 25943 solver.cpp:290] Iteration 31200 (5.80724 iter/s, 17.2199s/100 iter), loss = 0.025954
I0711 21:53:10.507709 25943 solver.cpp:309]     Train net output #0: loss = 0.025954 (* 1 = 0.025954 loss)
I0711 21:53:10.507716 25943 sgd_solver.cpp:106] Iteration 31200, lr = 1e-06
I0711 21:53:27.581179 25943 solver.cpp:290] Iteration 31300 (5.8572 iter/s, 17.073s/100 iter), loss = 0.0397632
I0711 21:53:27.581208 25943 solver.cpp:309]     Train net output #0: loss = 0.0397631 (* 1 = 0.0397631 loss)
I0711 21:53:27.581218 25943 sgd_solver.cpp:106] Iteration 31300, lr = 1e-06
I0711 21:53:44.498623 25943 solver.cpp:290] Iteration 31400 (5.91123 iter/s, 16.917s/100 iter), loss = 0.0259564
I0711 21:53:44.498714 25943 solver.cpp:309]     Train net output #0: loss = 0.0259564 (* 1 = 0.0259564 loss)
I0711 21:53:44.498723 25943 sgd_solver.cpp:106] Iteration 31400, lr = 1e-06
I0711 21:54:01.403022 25943 solver.cpp:290] Iteration 31500 (5.91581 iter/s, 16.9039s/100 iter), loss = 0.0163565
I0711 21:54:01.403046 25943 solver.cpp:309]     Train net output #0: loss = 0.0163564 (* 1 = 0.0163564 loss)
I0711 21:54:01.403053 25943 sgd_solver.cpp:106] Iteration 31500, lr = 1e-06
I0711 21:54:18.570349 25943 solver.cpp:290] Iteration 31600 (5.82518 iter/s, 17.1668s/100 iter), loss = 0.0293581
I0711 21:54:18.570399 25943 solver.cpp:309]     Train net output #0: loss = 0.029358 (* 1 = 0.029358 loss)
I0711 21:54:18.570406 25943 sgd_solver.cpp:106] Iteration 31600, lr = 1e-06
I0711 21:54:35.520968 25943 solver.cpp:290] Iteration 31700 (5.89967 iter/s, 16.9501s/100 iter), loss = 0.0335596
I0711 21:54:35.520992 25943 solver.cpp:309]     Train net output #0: loss = 0.0335596 (* 1 = 0.0335596 loss)
I0711 21:54:35.520998 25943 sgd_solver.cpp:106] Iteration 31700, lr = 1e-06
I0711 21:54:52.613523 25943 solver.cpp:290] Iteration 31800 (5.85067 iter/s, 17.0921s/100 iter), loss = 0.0180552
I0711 21:54:52.613569 25943 solver.cpp:309]     Train net output #0: loss = 0.0180552 (* 1 = 0.0180552 loss)
I0711 21:54:52.613575 25943 sgd_solver.cpp:106] Iteration 31800, lr = 1e-06
I0711 21:55:09.732168 25943 solver.cpp:290] Iteration 31900 (5.84176 iter/s, 17.1181s/100 iter), loss = 0.0356738
I0711 21:55:09.732192 25943 solver.cpp:309]     Train net output #0: loss = 0.0356738 (* 1 = 0.0356738 loss)
I0711 21:55:09.732198 25943 sgd_solver.cpp:106] Iteration 31900, lr = 1e-06
I0711 21:55:26.506573 25943 solver.cpp:594] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/l1reg/cityscapes5_jsegnet21v2_iter_32000.caffemodel
I0711 21:55:26.532850 25943 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/l1reg/cityscapes5_jsegnet21v2_iter_32000.solverstate
I0711 21:55:26.598388 25943 solver.cpp:447] Iteration 32000, loss = 0.0255415
I0711 21:55:26.598410 25943 solver.cpp:467] Iteration 32000, Testing net (#0)
I0711 21:56:12.927333 25943 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.95353
I0711 21:56:12.927438 25943 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999614
I0711 21:56:12.927445 25943 solver.cpp:540]     Test net output #2: loss = 0.165123 (* 1 = 0.165123 loss)
I0711 21:56:12.927449 25943 solver.cpp:452] Optimization Done.
I0711 21:56:13.280091 25943 caffe.cpp:246] Optimization Done.
training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/sparse
I0711 21:56:25.143950 13090 caffe.cpp:209] Using GPUs 0, 1, 2
I0711 21:56:25.144436 13090 caffe.cpp:214] GPU 0: GeForce GTX 1080
I0711 21:56:25.144773 13090 caffe.cpp:214] GPU 1: GeForce GTX 1080
I0711 21:56:25.145126 13090 caffe.cpp:214] GPU 2: GeForce GTX 1080
I0711 21:56:26.118041 13090 solver.cpp:48] Initializing solver from parameters: 
train_net: "training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/sparse/train.prototxt"
test_net: "training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/sparse/test.prototxt"
test_iter: 125
test_interval: 2000
base_lr: 1e-05
display: 100
max_iter: 32000
lr_policy: "multistep"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 1e-05
snapshot: 10000
snapshot_prefix: "training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/sparse/cityscapes5_jsegnet21v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
regularization_type: "L1"
test_initialization: false
stepvalue: 24000
iter_size: 1
type: "Adam"
display_sparsity: 1000
sparse_mode: SPARSE_UPDATE
sparsity_target: 0.8
sparsity_step_factor: 0.01
sparsity_step_iter: 1000
sparsity_start_iter: 0
sparsity_start_factor: 0.8
I0711 21:56:26.118132 13090 solver.cpp:82] Creating training net from train_net file: training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/sparse/train.prototxt
I0711 21:56:26.133301 13090 net.cpp:327] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0711 21:56:26.133332 13090 net.cpp:327] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
I0711 21:56:26.134222 13090 net.cpp:56] Initializing net from parameters: 
name: "jsegnet21v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/train-image-lmdb"
    label_list_path: "data/train-label-lmdb"
    batch_size: 5
    shuffle: false
    threads: 4
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b/bn"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a/bn"
  top: "out5a/bn"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a/bn"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b/bn"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a/bn"
  top: "out3a/bn"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a/bn"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv1/bn"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv2/bn"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv3/bn"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4/bn"
  top: "ctx_conv4/bn"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4/bn"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
I0711 21:56:26.142734 13090 layer_factory.hpp:77] Creating layer data
I0711 21:56:26.142781 13090 net.cpp:98] Creating Layer data
I0711 21:56:26.142802 13090 net.cpp:413] data -> data
I0711 21:56:26.142856 13090 net.cpp:413] data -> label
I0711 21:56:26.161072 13154 db_lmdb.cpp:35] Opened lmdb data/train-image-lmdb
I0711 21:56:26.161672 13159 db_lmdb.cpp:35] Opened lmdb data/train-label-lmdb
I0711 21:56:26.169936 13090 data_layer.cpp:78] ReshapePrefetch 5, 3, 640, 640
I0711 21:56:26.170123 13090 data_layer.cpp:83] output data size: 5,3,640,640
I0711 21:56:26.212430 13090 data_layer.cpp:78] ReshapePrefetch 5, 1, 640, 640
I0711 21:56:26.212502 13090 data_layer.cpp:83] output data size: 5,1,640,640
I0711 21:56:26.219228 13164 blocking_queue.cpp:50] Waiting for data
I0711 21:56:26.225298 13090 net.cpp:148] Setting up data
I0711 21:56:26.225318 13090 net.cpp:155] Top shape: 5 3 640 640 (6144000)
I0711 21:56:26.225322 13090 net.cpp:155] Top shape: 5 1 640 640 (2048000)
I0711 21:56:26.225323 13090 net.cpp:163] Memory required for data: 32768000
I0711 21:56:26.225329 13090 layer_factory.hpp:77] Creating layer data/bias
I0711 21:56:26.225337 13090 net.cpp:98] Creating Layer data/bias
I0711 21:56:26.225342 13090 net.cpp:439] data/bias <- data
I0711 21:56:26.225349 13090 net.cpp:413] data/bias -> data/bias
I0711 21:56:26.226560 13090 net.cpp:148] Setting up data/bias
I0711 21:56:26.226572 13090 net.cpp:155] Top shape: 5 3 640 640 (6144000)
I0711 21:56:26.226573 13090 net.cpp:163] Memory required for data: 57344000
I0711 21:56:26.226583 13090 layer_factory.hpp:77] Creating layer conv1a
I0711 21:56:26.226598 13090 net.cpp:98] Creating Layer conv1a
I0711 21:56:26.226600 13090 net.cpp:439] conv1a <- data/bias
I0711 21:56:26.226604 13090 net.cpp:413] conv1a -> conv1a
I0711 21:56:26.229665 13090 net.cpp:148] Setting up conv1a
I0711 21:56:26.229681 13090 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0711 21:56:26.229684 13090 net.cpp:163] Memory required for data: 122880000
I0711 21:56:26.229691 13090 layer_factory.hpp:77] Creating layer conv1a/bn
I0711 21:56:26.229696 13090 net.cpp:98] Creating Layer conv1a/bn
I0711 21:56:26.229699 13090 net.cpp:439] conv1a/bn <- conv1a
I0711 21:56:26.229702 13090 net.cpp:413] conv1a/bn -> conv1a/bn
I0711 21:56:26.231371 13090 net.cpp:148] Setting up conv1a/bn
I0711 21:56:26.231380 13090 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0711 21:56:26.231384 13090 net.cpp:163] Memory required for data: 188416000
I0711 21:56:26.231389 13090 layer_factory.hpp:77] Creating layer conv1a/relu
I0711 21:56:26.242259 13090 net.cpp:98] Creating Layer conv1a/relu
I0711 21:56:26.242269 13090 net.cpp:439] conv1a/relu <- conv1a/bn
I0711 21:56:26.242271 13090 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0711 21:56:26.242280 13090 net.cpp:148] Setting up conv1a/relu
I0711 21:56:26.242283 13090 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0711 21:56:26.242285 13090 net.cpp:163] Memory required for data: 253952000
I0711 21:56:26.242287 13090 layer_factory.hpp:77] Creating layer conv1b
I0711 21:56:26.242303 13090 net.cpp:98] Creating Layer conv1b
I0711 21:56:26.242306 13090 net.cpp:439] conv1b <- conv1a/bn
I0711 21:56:26.242308 13090 net.cpp:413] conv1b -> conv1b
I0711 21:56:26.242678 13090 net.cpp:148] Setting up conv1b
I0711 21:56:26.242684 13090 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0711 21:56:26.242686 13090 net.cpp:163] Memory required for data: 319488000
I0711 21:56:26.242691 13090 layer_factory.hpp:77] Creating layer conv1b/bn
I0711 21:56:26.242696 13090 net.cpp:98] Creating Layer conv1b/bn
I0711 21:56:26.242697 13090 net.cpp:439] conv1b/bn <- conv1b
I0711 21:56:26.242702 13090 net.cpp:413] conv1b/bn -> conv1b/bn
I0711 21:56:26.243401 13090 net.cpp:148] Setting up conv1b/bn
I0711 21:56:26.243407 13090 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0711 21:56:26.243409 13090 net.cpp:163] Memory required for data: 385024000
I0711 21:56:26.243413 13090 layer_factory.hpp:77] Creating layer conv1b/relu
I0711 21:56:26.243417 13090 net.cpp:98] Creating Layer conv1b/relu
I0711 21:56:26.243418 13090 net.cpp:439] conv1b/relu <- conv1b/bn
I0711 21:56:26.243420 13090 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0711 21:56:26.243424 13090 net.cpp:148] Setting up conv1b/relu
I0711 21:56:26.243427 13090 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0711 21:56:26.243428 13090 net.cpp:163] Memory required for data: 450560000
I0711 21:56:26.243430 13090 layer_factory.hpp:77] Creating layer pool1
I0711 21:56:26.243438 13090 net.cpp:98] Creating Layer pool1
I0711 21:56:26.243440 13090 net.cpp:439] pool1 <- conv1b/bn
I0711 21:56:26.243443 13090 net.cpp:413] pool1 -> pool1
I0711 21:56:26.246986 13090 net.cpp:148] Setting up pool1
I0711 21:56:26.247036 13090 net.cpp:155] Top shape: 5 32 160 160 (4096000)
I0711 21:56:26.247050 13090 net.cpp:163] Memory required for data: 466944000
I0711 21:56:26.247061 13090 layer_factory.hpp:77] Creating layer res2a_branch2a
I0711 21:56:26.247090 13090 net.cpp:98] Creating Layer res2a_branch2a
I0711 21:56:26.247102 13090 net.cpp:439] res2a_branch2a <- pool1
I0711 21:56:26.247123 13090 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0711 21:56:26.253763 13090 net.cpp:148] Setting up res2a_branch2a
I0711 21:56:26.253808 13090 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0711 21:56:26.253819 13090 net.cpp:163] Memory required for data: 499712000
I0711 21:56:26.253844 13090 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0711 21:56:26.253864 13090 net.cpp:98] Creating Layer res2a_branch2a/bn
I0711 21:56:26.253875 13090 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0711 21:56:26.253901 13090 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0711 21:56:26.256338 13090 net.cpp:148] Setting up res2a_branch2a/bn
I0711 21:56:26.256359 13090 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0711 21:56:26.256367 13090 net.cpp:163] Memory required for data: 532480000
I0711 21:56:26.256383 13090 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0711 21:56:26.256393 13090 net.cpp:98] Creating Layer res2a_branch2a/relu
I0711 21:56:26.256402 13090 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0711 21:56:26.256409 13090 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0711 21:56:26.256423 13090 net.cpp:148] Setting up res2a_branch2a/relu
I0711 21:56:26.256433 13090 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0711 21:56:26.256439 13090 net.cpp:163] Memory required for data: 565248000
I0711 21:56:26.256446 13090 layer_factory.hpp:77] Creating layer res2a_branch2b
I0711 21:56:26.256458 13090 net.cpp:98] Creating Layer res2a_branch2b
I0711 21:56:26.256467 13090 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0711 21:56:26.256476 13090 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0711 21:56:26.260087 13090 net.cpp:148] Setting up res2a_branch2b
I0711 21:56:26.260108 13090 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0711 21:56:26.260114 13090 net.cpp:163] Memory required for data: 598016000
I0711 21:56:26.260124 13090 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0711 21:56:26.260133 13090 net.cpp:98] Creating Layer res2a_branch2b/bn
I0711 21:56:26.260157 13090 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0711 21:56:26.260166 13090 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0711 21:56:26.261951 13090 net.cpp:148] Setting up res2a_branch2b/bn
I0711 21:56:26.261968 13090 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0711 21:56:26.261975 13090 net.cpp:163] Memory required for data: 630784000
I0711 21:56:26.261986 13090 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0711 21:56:26.261994 13090 net.cpp:98] Creating Layer res2a_branch2b/relu
I0711 21:56:26.262001 13090 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0711 21:56:26.262006 13090 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0711 21:56:26.262020 13090 net.cpp:148] Setting up res2a_branch2b/relu
I0711 21:56:26.262029 13090 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0711 21:56:26.262033 13090 net.cpp:163] Memory required for data: 663552000
I0711 21:56:26.262039 13090 layer_factory.hpp:77] Creating layer pool2
I0711 21:56:26.262046 13090 net.cpp:98] Creating Layer pool2
I0711 21:56:26.262053 13090 net.cpp:439] pool2 <- res2a_branch2b/bn
I0711 21:56:26.262059 13090 net.cpp:413] pool2 -> pool2
I0711 21:56:26.262152 13090 net.cpp:148] Setting up pool2
I0711 21:56:26.262162 13090 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 21:56:26.262167 13090 net.cpp:163] Memory required for data: 671744000
I0711 21:56:26.262172 13090 layer_factory.hpp:77] Creating layer res3a_branch2a
I0711 21:56:26.262182 13090 net.cpp:98] Creating Layer res3a_branch2a
I0711 21:56:26.262188 13090 net.cpp:439] res3a_branch2a <- pool2
I0711 21:56:26.262195 13090 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0711 21:56:26.266203 13090 net.cpp:148] Setting up res3a_branch2a
I0711 21:56:26.266216 13090 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0711 21:56:26.266219 13090 net.cpp:163] Memory required for data: 688128000
I0711 21:56:26.266225 13090 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0711 21:56:26.266232 13090 net.cpp:98] Creating Layer res3a_branch2a/bn
I0711 21:56:26.266237 13090 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0711 21:56:26.266242 13090 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0711 21:56:26.267498 13090 net.cpp:148] Setting up res3a_branch2a/bn
I0711 21:56:26.267508 13090 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0711 21:56:26.267513 13090 net.cpp:163] Memory required for data: 704512000
I0711 21:56:26.267524 13090 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0711 21:56:26.267531 13090 net.cpp:98] Creating Layer res3a_branch2a/relu
I0711 21:56:26.267535 13090 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0711 21:56:26.267540 13090 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0711 21:56:26.267549 13090 net.cpp:148] Setting up res3a_branch2a/relu
I0711 21:56:26.267555 13090 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0711 21:56:26.267558 13090 net.cpp:163] Memory required for data: 720896000
I0711 21:56:26.267563 13090 layer_factory.hpp:77] Creating layer res3a_branch2b
I0711 21:56:26.267571 13090 net.cpp:98] Creating Layer res3a_branch2b
I0711 21:56:26.267576 13090 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0711 21:56:26.267582 13090 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0711 21:56:26.269656 13090 net.cpp:148] Setting up res3a_branch2b
I0711 21:56:26.269670 13090 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0711 21:56:26.269673 13090 net.cpp:163] Memory required for data: 737280000
I0711 21:56:26.269678 13090 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0711 21:56:26.269685 13090 net.cpp:98] Creating Layer res3a_branch2b/bn
I0711 21:56:26.269688 13090 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0711 21:56:26.269695 13090 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0711 21:56:26.270802 13090 net.cpp:148] Setting up res3a_branch2b/bn
I0711 21:56:26.270814 13090 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0711 21:56:26.270818 13090 net.cpp:163] Memory required for data: 753664000
I0711 21:56:26.270838 13090 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0711 21:56:26.270843 13090 net.cpp:98] Creating Layer res3a_branch2b/relu
I0711 21:56:26.270846 13090 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0711 21:56:26.270850 13090 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0711 21:56:26.270856 13090 net.cpp:148] Setting up res3a_branch2b/relu
I0711 21:56:26.270862 13090 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0711 21:56:26.270865 13090 net.cpp:163] Memory required for data: 770048000
I0711 21:56:26.270869 13090 layer_factory.hpp:77] Creating layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0711 21:56:26.270874 13090 net.cpp:98] Creating Layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0711 21:56:26.270879 13090 net.cpp:439] res3a_branch2b/bn_res3a_branch2b/relu_0_split <- res3a_branch2b/bn
I0711 21:56:26.270882 13090 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0711 21:56:26.270889 13090 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0711 21:56:26.270967 13090 net.cpp:148] Setting up res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0711 21:56:26.270978 13090 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0711 21:56:26.270987 13090 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0711 21:56:26.270992 13090 net.cpp:163] Memory required for data: 802816000
I0711 21:56:26.270999 13090 layer_factory.hpp:77] Creating layer pool3
I0711 21:56:26.271008 13090 net.cpp:98] Creating Layer pool3
I0711 21:56:26.271014 13090 net.cpp:439] pool3 <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0711 21:56:26.271023 13090 net.cpp:413] pool3 -> pool3
I0711 21:56:26.271107 13090 net.cpp:148] Setting up pool3
I0711 21:56:26.271117 13090 net.cpp:155] Top shape: 5 128 40 40 (1024000)
I0711 21:56:26.271124 13090 net.cpp:163] Memory required for data: 806912000
I0711 21:56:26.271131 13090 layer_factory.hpp:77] Creating layer res4a_branch2a
I0711 21:56:26.271142 13090 net.cpp:98] Creating Layer res4a_branch2a
I0711 21:56:26.271149 13090 net.cpp:439] res4a_branch2a <- pool3
I0711 21:56:26.271157 13090 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0711 21:56:26.281518 13090 net.cpp:148] Setting up res4a_branch2a
I0711 21:56:26.281529 13090 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0711 21:56:26.281533 13090 net.cpp:163] Memory required for data: 815104000
I0711 21:56:26.281538 13090 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0711 21:56:26.281546 13090 net.cpp:98] Creating Layer res4a_branch2a/bn
I0711 21:56:26.281550 13090 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0711 21:56:26.281555 13090 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0711 21:56:26.282407 13090 net.cpp:148] Setting up res4a_branch2a/bn
I0711 21:56:26.282415 13090 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0711 21:56:26.282418 13090 net.cpp:163] Memory required for data: 823296000
I0711 21:56:26.282425 13090 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0711 21:56:26.282430 13090 net.cpp:98] Creating Layer res4a_branch2a/relu
I0711 21:56:26.282434 13090 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0711 21:56:26.282438 13090 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0711 21:56:26.282444 13090 net.cpp:148] Setting up res4a_branch2a/relu
I0711 21:56:26.282447 13090 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0711 21:56:26.282450 13090 net.cpp:163] Memory required for data: 831488000
I0711 21:56:26.282454 13090 layer_factory.hpp:77] Creating layer res4a_branch2b
I0711 21:56:26.282461 13090 net.cpp:98] Creating Layer res4a_branch2b
I0711 21:56:26.282465 13090 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0711 21:56:26.282469 13090 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0711 21:56:26.286404 13090 net.cpp:148] Setting up res4a_branch2b
I0711 21:56:26.286412 13090 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0711 21:56:26.286415 13090 net.cpp:163] Memory required for data: 839680000
I0711 21:56:26.286419 13090 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0711 21:56:26.286432 13090 net.cpp:98] Creating Layer res4a_branch2b/bn
I0711 21:56:26.286437 13090 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0711 21:56:26.286440 13090 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0711 21:56:26.287204 13090 net.cpp:148] Setting up res4a_branch2b/bn
I0711 21:56:26.287214 13090 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0711 21:56:26.287216 13090 net.cpp:163] Memory required for data: 847872000
I0711 21:56:26.287222 13090 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0711 21:56:26.287226 13090 net.cpp:98] Creating Layer res4a_branch2b/relu
I0711 21:56:26.287230 13090 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0711 21:56:26.287232 13090 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0711 21:56:26.287237 13090 net.cpp:148] Setting up res4a_branch2b/relu
I0711 21:56:26.287241 13090 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0711 21:56:26.287243 13090 net.cpp:163] Memory required for data: 856064000
I0711 21:56:26.287245 13090 layer_factory.hpp:77] Creating layer pool4
I0711 21:56:26.287250 13090 net.cpp:98] Creating Layer pool4
I0711 21:56:26.287252 13090 net.cpp:439] pool4 <- res4a_branch2b/bn
I0711 21:56:26.287256 13090 net.cpp:413] pool4 -> pool4
I0711 21:56:26.287302 13090 net.cpp:148] Setting up pool4
I0711 21:56:26.287308 13090 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0711 21:56:26.287313 13090 net.cpp:163] Memory required for data: 864256000
I0711 21:56:26.287317 13090 layer_factory.hpp:77] Creating layer res5a_branch2a
I0711 21:56:26.287325 13090 net.cpp:98] Creating Layer res5a_branch2a
I0711 21:56:26.287330 13090 net.cpp:439] res5a_branch2a <- pool4
I0711 21:56:26.287339 13090 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0711 21:56:26.313550 13090 net.cpp:148] Setting up res5a_branch2a
I0711 21:56:26.313565 13090 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0711 21:56:26.313568 13090 net.cpp:163] Memory required for data: 880640000
I0711 21:56:26.313573 13090 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0711 21:56:26.313581 13090 net.cpp:98] Creating Layer res5a_branch2a/bn
I0711 21:56:26.313585 13090 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0711 21:56:26.313590 13090 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0711 21:56:26.314270 13090 net.cpp:148] Setting up res5a_branch2a/bn
I0711 21:56:26.314276 13090 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0711 21:56:26.314278 13090 net.cpp:163] Memory required for data: 897024000
I0711 21:56:26.314283 13090 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0711 21:56:26.314287 13090 net.cpp:98] Creating Layer res5a_branch2a/relu
I0711 21:56:26.314290 13090 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0711 21:56:26.314291 13090 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0711 21:56:26.314296 13090 net.cpp:148] Setting up res5a_branch2a/relu
I0711 21:56:26.314298 13090 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0711 21:56:26.314299 13090 net.cpp:163] Memory required for data: 913408000
I0711 21:56:26.314301 13090 layer_factory.hpp:77] Creating layer res5a_branch2b
I0711 21:56:26.314306 13090 net.cpp:98] Creating Layer res5a_branch2b
I0711 21:56:26.314308 13090 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0711 21:56:26.314312 13090 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0711 21:56:26.327107 13090 net.cpp:148] Setting up res5a_branch2b
I0711 21:56:26.327116 13090 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0711 21:56:26.327118 13090 net.cpp:163] Memory required for data: 929792000
I0711 21:56:26.327124 13090 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0711 21:56:26.327128 13090 net.cpp:98] Creating Layer res5a_branch2b/bn
I0711 21:56:26.327131 13090 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0711 21:56:26.327133 13090 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0711 21:56:26.327795 13090 net.cpp:148] Setting up res5a_branch2b/bn
I0711 21:56:26.327802 13090 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0711 21:56:26.327813 13090 net.cpp:163] Memory required for data: 946176000
I0711 21:56:26.327819 13090 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0711 21:56:26.327822 13090 net.cpp:98] Creating Layer res5a_branch2b/relu
I0711 21:56:26.327824 13090 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0711 21:56:26.327826 13090 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0711 21:56:26.327831 13090 net.cpp:148] Setting up res5a_branch2b/relu
I0711 21:56:26.327832 13090 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0711 21:56:26.327834 13090 net.cpp:163] Memory required for data: 962560000
I0711 21:56:26.327836 13090 layer_factory.hpp:77] Creating layer out5a
I0711 21:56:26.327841 13090 net.cpp:98] Creating Layer out5a
I0711 21:56:26.327843 13090 net.cpp:439] out5a <- res5a_branch2b/bn
I0711 21:56:26.327847 13090 net.cpp:413] out5a -> out5a
I0711 21:56:26.331962 13090 net.cpp:148] Setting up out5a
I0711 21:56:26.331971 13090 net.cpp:155] Top shape: 5 64 40 40 (512000)
I0711 21:56:26.331974 13090 net.cpp:163] Memory required for data: 964608000
I0711 21:56:26.331977 13090 layer_factory.hpp:77] Creating layer out5a/bn
I0711 21:56:26.331981 13090 net.cpp:98] Creating Layer out5a/bn
I0711 21:56:26.331984 13090 net.cpp:439] out5a/bn <- out5a
I0711 21:56:26.331986 13090 net.cpp:413] out5a/bn -> out5a/bn
I0711 21:56:26.332720 13090 net.cpp:148] Setting up out5a/bn
I0711 21:56:26.332727 13090 net.cpp:155] Top shape: 5 64 40 40 (512000)
I0711 21:56:26.332729 13090 net.cpp:163] Memory required for data: 966656000
I0711 21:56:26.332734 13090 layer_factory.hpp:77] Creating layer out5a/relu
I0711 21:56:26.332737 13090 net.cpp:98] Creating Layer out5a/relu
I0711 21:56:26.332739 13090 net.cpp:439] out5a/relu <- out5a/bn
I0711 21:56:26.332741 13090 net.cpp:400] out5a/relu -> out5a/bn (in-place)
I0711 21:56:26.332746 13090 net.cpp:148] Setting up out5a/relu
I0711 21:56:26.332747 13090 net.cpp:155] Top shape: 5 64 40 40 (512000)
I0711 21:56:26.332749 13090 net.cpp:163] Memory required for data: 968704000
I0711 21:56:26.332751 13090 layer_factory.hpp:77] Creating layer out5a_up2
I0711 21:56:26.332759 13090 net.cpp:98] Creating Layer out5a_up2
I0711 21:56:26.332762 13090 net.cpp:439] out5a_up2 <- out5a/bn
I0711 21:56:26.332767 13090 net.cpp:413] out5a_up2 -> out5a_up2
I0711 21:56:26.333055 13090 net.cpp:148] Setting up out5a_up2
I0711 21:56:26.333061 13090 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 21:56:26.333063 13090 net.cpp:163] Memory required for data: 976896000
I0711 21:56:26.333066 13090 layer_factory.hpp:77] Creating layer out3a
I0711 21:56:26.333070 13090 net.cpp:98] Creating Layer out3a
I0711 21:56:26.333072 13090 net.cpp:439] out3a <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0711 21:56:26.333076 13090 net.cpp:413] out3a -> out3a
I0711 21:56:26.334121 13090 net.cpp:148] Setting up out3a
I0711 21:56:26.334128 13090 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 21:56:26.334131 13090 net.cpp:163] Memory required for data: 985088000
I0711 21:56:26.334133 13090 layer_factory.hpp:77] Creating layer out3a/bn
I0711 21:56:26.334137 13090 net.cpp:98] Creating Layer out3a/bn
I0711 21:56:26.334141 13090 net.cpp:439] out3a/bn <- out3a
I0711 21:56:26.334143 13090 net.cpp:413] out3a/bn -> out3a/bn
I0711 21:56:26.334877 13090 net.cpp:148] Setting up out3a/bn
I0711 21:56:26.334884 13090 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 21:56:26.334887 13090 net.cpp:163] Memory required for data: 993280000
I0711 21:56:26.334892 13090 layer_factory.hpp:77] Creating layer out3a/relu
I0711 21:56:26.334893 13090 net.cpp:98] Creating Layer out3a/relu
I0711 21:56:26.334895 13090 net.cpp:439] out3a/relu <- out3a/bn
I0711 21:56:26.334898 13090 net.cpp:400] out3a/relu -> out3a/bn (in-place)
I0711 21:56:26.334902 13090 net.cpp:148] Setting up out3a/relu
I0711 21:56:26.334904 13090 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 21:56:26.334905 13090 net.cpp:163] Memory required for data: 1001472000
I0711 21:56:26.334908 13090 layer_factory.hpp:77] Creating layer out3_out5_combined
I0711 21:56:26.334919 13090 net.cpp:98] Creating Layer out3_out5_combined
I0711 21:56:26.334921 13090 net.cpp:439] out3_out5_combined <- out5a_up2
I0711 21:56:26.334925 13090 net.cpp:439] out3_out5_combined <- out3a/bn
I0711 21:56:26.334928 13090 net.cpp:413] out3_out5_combined -> out3_out5_combined
I0711 21:56:26.334954 13090 net.cpp:148] Setting up out3_out5_combined
I0711 21:56:26.334957 13090 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 21:56:26.334959 13090 net.cpp:163] Memory required for data: 1009664000
I0711 21:56:26.334960 13090 layer_factory.hpp:77] Creating layer ctx_conv1
I0711 21:56:26.334964 13090 net.cpp:98] Creating Layer ctx_conv1
I0711 21:56:26.334966 13090 net.cpp:439] ctx_conv1 <- out3_out5_combined
I0711 21:56:26.334969 13090 net.cpp:413] ctx_conv1 -> ctx_conv1
I0711 21:56:26.336050 13090 net.cpp:148] Setting up ctx_conv1
I0711 21:56:26.336056 13090 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 21:56:26.336058 13090 net.cpp:163] Memory required for data: 1017856000
I0711 21:56:26.336062 13090 layer_factory.hpp:77] Creating layer ctx_conv1/bn
I0711 21:56:26.336066 13090 net.cpp:98] Creating Layer ctx_conv1/bn
I0711 21:56:26.336067 13090 net.cpp:439] ctx_conv1/bn <- ctx_conv1
I0711 21:56:26.336071 13090 net.cpp:413] ctx_conv1/bn -> ctx_conv1/bn
I0711 21:56:26.336805 13090 net.cpp:148] Setting up ctx_conv1/bn
I0711 21:56:26.336812 13090 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 21:56:26.336813 13090 net.cpp:163] Memory required for data: 1026048000
I0711 21:56:26.336822 13090 layer_factory.hpp:77] Creating layer ctx_conv1/relu
I0711 21:56:26.336825 13090 net.cpp:98] Creating Layer ctx_conv1/relu
I0711 21:56:26.336827 13090 net.cpp:439] ctx_conv1/relu <- ctx_conv1/bn
I0711 21:56:26.336830 13090 net.cpp:400] ctx_conv1/relu -> ctx_conv1/bn (in-place)
I0711 21:56:26.336833 13090 net.cpp:148] Setting up ctx_conv1/relu
I0711 21:56:26.336836 13090 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 21:56:26.336838 13090 net.cpp:163] Memory required for data: 1034240000
I0711 21:56:26.336839 13090 layer_factory.hpp:77] Creating layer ctx_conv2
I0711 21:56:26.336843 13090 net.cpp:98] Creating Layer ctx_conv2
I0711 21:56:26.336846 13090 net.cpp:439] ctx_conv2 <- ctx_conv1/bn
I0711 21:56:26.336848 13090 net.cpp:413] ctx_conv2 -> ctx_conv2
I0711 21:56:26.337903 13090 net.cpp:148] Setting up ctx_conv2
I0711 21:56:26.337908 13090 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 21:56:26.337910 13090 net.cpp:163] Memory required for data: 1042432000
I0711 21:56:26.337914 13090 layer_factory.hpp:77] Creating layer ctx_conv2/bn
I0711 21:56:26.337918 13090 net.cpp:98] Creating Layer ctx_conv2/bn
I0711 21:56:26.337919 13090 net.cpp:439] ctx_conv2/bn <- ctx_conv2
I0711 21:56:26.337923 13090 net.cpp:413] ctx_conv2/bn -> ctx_conv2/bn
I0711 21:56:26.338667 13090 net.cpp:148] Setting up ctx_conv2/bn
I0711 21:56:26.338675 13090 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 21:56:26.338676 13090 net.cpp:163] Memory required for data: 1050624000
I0711 21:56:26.338681 13090 layer_factory.hpp:77] Creating layer ctx_conv2/relu
I0711 21:56:26.338685 13090 net.cpp:98] Creating Layer ctx_conv2/relu
I0711 21:56:26.338687 13090 net.cpp:439] ctx_conv2/relu <- ctx_conv2/bn
I0711 21:56:26.338690 13090 net.cpp:400] ctx_conv2/relu -> ctx_conv2/bn (in-place)
I0711 21:56:26.338693 13090 net.cpp:148] Setting up ctx_conv2/relu
I0711 21:56:26.338696 13090 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 21:56:26.338697 13090 net.cpp:163] Memory required for data: 1058816000
I0711 21:56:26.338698 13090 layer_factory.hpp:77] Creating layer ctx_conv3
I0711 21:56:26.338702 13090 net.cpp:98] Creating Layer ctx_conv3
I0711 21:56:26.338704 13090 net.cpp:439] ctx_conv3 <- ctx_conv2/bn
I0711 21:56:26.338706 13090 net.cpp:413] ctx_conv3 -> ctx_conv3
I0711 21:56:26.339768 13090 net.cpp:148] Setting up ctx_conv3
I0711 21:56:26.339774 13090 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 21:56:26.339777 13090 net.cpp:163] Memory required for data: 1067008000
I0711 21:56:26.339781 13090 layer_factory.hpp:77] Creating layer ctx_conv3/bn
I0711 21:56:26.339789 13090 net.cpp:98] Creating Layer ctx_conv3/bn
I0711 21:56:26.339792 13090 net.cpp:439] ctx_conv3/bn <- ctx_conv3
I0711 21:56:26.339794 13090 net.cpp:413] ctx_conv3/bn -> ctx_conv3/bn
I0711 21:56:26.340534 13090 net.cpp:148] Setting up ctx_conv3/bn
I0711 21:56:26.340540 13090 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 21:56:26.340543 13090 net.cpp:163] Memory required for data: 1075200000
I0711 21:56:26.340548 13090 layer_factory.hpp:77] Creating layer ctx_conv3/relu
I0711 21:56:26.340550 13090 net.cpp:98] Creating Layer ctx_conv3/relu
I0711 21:56:26.340553 13090 net.cpp:439] ctx_conv3/relu <- ctx_conv3/bn
I0711 21:56:26.340555 13090 net.cpp:400] ctx_conv3/relu -> ctx_conv3/bn (in-place)
I0711 21:56:26.340560 13090 net.cpp:148] Setting up ctx_conv3/relu
I0711 21:56:26.340564 13090 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 21:56:26.340564 13090 net.cpp:163] Memory required for data: 1083392000
I0711 21:56:26.340566 13090 layer_factory.hpp:77] Creating layer ctx_conv4
I0711 21:56:26.340570 13090 net.cpp:98] Creating Layer ctx_conv4
I0711 21:56:26.340572 13090 net.cpp:439] ctx_conv4 <- ctx_conv3/bn
I0711 21:56:26.340575 13090 net.cpp:413] ctx_conv4 -> ctx_conv4
I0711 21:56:26.341640 13090 net.cpp:148] Setting up ctx_conv4
I0711 21:56:26.341647 13090 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 21:56:26.341650 13090 net.cpp:163] Memory required for data: 1091584000
I0711 21:56:26.341652 13090 layer_factory.hpp:77] Creating layer ctx_conv4/bn
I0711 21:56:26.341655 13090 net.cpp:98] Creating Layer ctx_conv4/bn
I0711 21:56:26.341657 13090 net.cpp:439] ctx_conv4/bn <- ctx_conv4
I0711 21:56:26.341660 13090 net.cpp:413] ctx_conv4/bn -> ctx_conv4/bn
I0711 21:56:26.342406 13090 net.cpp:148] Setting up ctx_conv4/bn
I0711 21:56:26.342411 13090 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 21:56:26.342414 13090 net.cpp:163] Memory required for data: 1099776000
I0711 21:56:26.342418 13090 layer_factory.hpp:77] Creating layer ctx_conv4/relu
I0711 21:56:26.342422 13090 net.cpp:98] Creating Layer ctx_conv4/relu
I0711 21:56:26.342424 13090 net.cpp:439] ctx_conv4/relu <- ctx_conv4/bn
I0711 21:56:26.342427 13090 net.cpp:400] ctx_conv4/relu -> ctx_conv4/bn (in-place)
I0711 21:56:26.342429 13090 net.cpp:148] Setting up ctx_conv4/relu
I0711 21:56:26.342432 13090 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0711 21:56:26.342433 13090 net.cpp:163] Memory required for data: 1107968000
I0711 21:56:26.342435 13090 layer_factory.hpp:77] Creating layer ctx_final
I0711 21:56:26.342439 13090 net.cpp:98] Creating Layer ctx_final
I0711 21:56:26.342442 13090 net.cpp:439] ctx_final <- ctx_conv4/bn
I0711 21:56:26.342444 13090 net.cpp:413] ctx_final -> ctx_final
I0711 21:56:26.342871 13090 net.cpp:148] Setting up ctx_final
I0711 21:56:26.342877 13090 net.cpp:155] Top shape: 5 8 80 80 (256000)
I0711 21:56:26.342880 13090 net.cpp:163] Memory required for data: 1108992000
I0711 21:56:26.342882 13090 layer_factory.hpp:77] Creating layer ctx_final/relu
I0711 21:56:26.342885 13090 net.cpp:98] Creating Layer ctx_final/relu
I0711 21:56:26.342887 13090 net.cpp:439] ctx_final/relu <- ctx_final
I0711 21:56:26.342890 13090 net.cpp:400] ctx_final/relu -> ctx_final (in-place)
I0711 21:56:26.342893 13090 net.cpp:148] Setting up ctx_final/relu
I0711 21:56:26.342895 13090 net.cpp:155] Top shape: 5 8 80 80 (256000)
I0711 21:56:26.342896 13090 net.cpp:163] Memory required for data: 1110016000
I0711 21:56:26.342898 13090 layer_factory.hpp:77] Creating layer out_deconv_final_up2
I0711 21:56:26.342901 13090 net.cpp:98] Creating Layer out_deconv_final_up2
I0711 21:56:26.342905 13090 net.cpp:439] out_deconv_final_up2 <- ctx_final
I0711 21:56:26.342906 13090 net.cpp:413] out_deconv_final_up2 -> out_deconv_final_up2
I0711 21:56:26.343173 13090 net.cpp:148] Setting up out_deconv_final_up2
I0711 21:56:26.343178 13090 net.cpp:155] Top shape: 5 8 160 160 (1024000)
I0711 21:56:26.343180 13090 net.cpp:163] Memory required for data: 1114112000
I0711 21:56:26.343183 13090 layer_factory.hpp:77] Creating layer out_deconv_final_up4
I0711 21:56:26.343192 13090 net.cpp:98] Creating Layer out_deconv_final_up4
I0711 21:56:26.343194 13090 net.cpp:439] out_deconv_final_up4 <- out_deconv_final_up2
I0711 21:56:26.343197 13090 net.cpp:413] out_deconv_final_up4 -> out_deconv_final_up4
I0711 21:56:26.343446 13090 net.cpp:148] Setting up out_deconv_final_up4
I0711 21:56:26.343452 13090 net.cpp:155] Top shape: 5 8 320 320 (4096000)
I0711 21:56:26.343454 13090 net.cpp:163] Memory required for data: 1130496000
I0711 21:56:26.343456 13090 layer_factory.hpp:77] Creating layer out_deconv_final_up8
I0711 21:56:26.343459 13090 net.cpp:98] Creating Layer out_deconv_final_up8
I0711 21:56:26.343462 13090 net.cpp:439] out_deconv_final_up8 <- out_deconv_final_up4
I0711 21:56:26.343464 13090 net.cpp:413] out_deconv_final_up8 -> out_deconv_final_up8
I0711 21:56:26.343710 13090 net.cpp:148] Setting up out_deconv_final_up8
I0711 21:56:26.343716 13090 net.cpp:155] Top shape: 5 8 640 640 (16384000)
I0711 21:56:26.343719 13090 net.cpp:163] Memory required for data: 1196032000
I0711 21:56:26.343720 13090 layer_factory.hpp:77] Creating layer loss
I0711 21:56:26.343726 13090 net.cpp:98] Creating Layer loss
I0711 21:56:26.343729 13090 net.cpp:439] loss <- out_deconv_final_up8
I0711 21:56:26.343731 13090 net.cpp:439] loss <- label
I0711 21:56:26.343734 13090 net.cpp:413] loss -> loss
I0711 21:56:26.343741 13090 layer_factory.hpp:77] Creating layer loss
I0711 21:56:26.364989 13090 net.cpp:148] Setting up loss
I0711 21:56:26.365010 13090 net.cpp:155] Top shape: (1)
I0711 21:56:26.365012 13090 net.cpp:158]     with loss weight 1
I0711 21:56:26.365026 13090 net.cpp:163] Memory required for data: 1196032004
I0711 21:56:26.365031 13090 net.cpp:224] loss needs backward computation.
I0711 21:56:26.365033 13090 net.cpp:224] out_deconv_final_up8 needs backward computation.
I0711 21:56:26.365036 13090 net.cpp:224] out_deconv_final_up4 needs backward computation.
I0711 21:56:26.365037 13090 net.cpp:224] out_deconv_final_up2 needs backward computation.
I0711 21:56:26.365039 13090 net.cpp:224] ctx_final/relu needs backward computation.
I0711 21:56:26.365041 13090 net.cpp:224] ctx_final needs backward computation.
I0711 21:56:26.365043 13090 net.cpp:224] ctx_conv4/relu needs backward computation.
I0711 21:56:26.365046 13090 net.cpp:224] ctx_conv4/bn needs backward computation.
I0711 21:56:26.365047 13090 net.cpp:224] ctx_conv4 needs backward computation.
I0711 21:56:26.365049 13090 net.cpp:224] ctx_conv3/relu needs backward computation.
I0711 21:56:26.365051 13090 net.cpp:224] ctx_conv3/bn needs backward computation.
I0711 21:56:26.365054 13090 net.cpp:224] ctx_conv3 needs backward computation.
I0711 21:56:26.365056 13090 net.cpp:224] ctx_conv2/relu needs backward computation.
I0711 21:56:26.365059 13090 net.cpp:224] ctx_conv2/bn needs backward computation.
I0711 21:56:26.365061 13090 net.cpp:224] ctx_conv2 needs backward computation.
I0711 21:56:26.365063 13090 net.cpp:224] ctx_conv1/relu needs backward computation.
I0711 21:56:26.365067 13090 net.cpp:224] ctx_conv1/bn needs backward computation.
I0711 21:56:26.365068 13090 net.cpp:224] ctx_conv1 needs backward computation.
I0711 21:56:26.365072 13090 net.cpp:224] out3_out5_combined needs backward computation.
I0711 21:56:26.365073 13090 net.cpp:224] out3a/relu needs backward computation.
I0711 21:56:26.365077 13090 net.cpp:224] out3a/bn needs backward computation.
I0711 21:56:26.365079 13090 net.cpp:224] out3a needs backward computation.
I0711 21:56:26.365082 13090 net.cpp:224] out5a_up2 needs backward computation.
I0711 21:56:26.365084 13090 net.cpp:224] out5a/relu needs backward computation.
I0711 21:56:26.365087 13090 net.cpp:224] out5a/bn needs backward computation.
I0711 21:56:26.365088 13090 net.cpp:224] out5a needs backward computation.
I0711 21:56:26.365092 13090 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0711 21:56:26.365093 13090 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0711 21:56:26.365095 13090 net.cpp:224] res5a_branch2b needs backward computation.
I0711 21:56:26.365098 13090 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0711 21:56:26.365109 13090 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0711 21:56:26.365113 13090 net.cpp:224] res5a_branch2a needs backward computation.
I0711 21:56:26.365118 13090 net.cpp:224] pool4 needs backward computation.
I0711 21:56:26.365121 13090 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0711 21:56:26.365124 13090 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0711 21:56:26.365128 13090 net.cpp:224] res4a_branch2b needs backward computation.
I0711 21:56:26.365131 13090 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0711 21:56:26.365135 13090 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0711 21:56:26.365139 13090 net.cpp:224] res4a_branch2a needs backward computation.
I0711 21:56:26.365142 13090 net.cpp:224] pool3 needs backward computation.
I0711 21:56:26.365146 13090 net.cpp:224] res3a_branch2b/bn_res3a_branch2b/relu_0_split needs backward computation.
I0711 21:56:26.365150 13090 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0711 21:56:26.365154 13090 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0711 21:56:26.365159 13090 net.cpp:224] res3a_branch2b needs backward computation.
I0711 21:56:26.365162 13090 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0711 21:56:26.365166 13090 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0711 21:56:26.365170 13090 net.cpp:224] res3a_branch2a needs backward computation.
I0711 21:56:26.365175 13090 net.cpp:224] pool2 needs backward computation.
I0711 21:56:26.365178 13090 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0711 21:56:26.365182 13090 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0711 21:56:26.365186 13090 net.cpp:224] res2a_branch2b needs backward computation.
I0711 21:56:26.365190 13090 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0711 21:56:26.365195 13090 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0711 21:56:26.365198 13090 net.cpp:224] res2a_branch2a needs backward computation.
I0711 21:56:26.365202 13090 net.cpp:224] pool1 needs backward computation.
I0711 21:56:26.365206 13090 net.cpp:224] conv1b/relu needs backward computation.
I0711 21:56:26.365211 13090 net.cpp:224] conv1b/bn needs backward computation.
I0711 21:56:26.365216 13090 net.cpp:224] conv1b needs backward computation.
I0711 21:56:26.365219 13090 net.cpp:224] conv1a/relu needs backward computation.
I0711 21:56:26.365222 13090 net.cpp:224] conv1a/bn needs backward computation.
I0711 21:56:26.365226 13090 net.cpp:224] conv1a needs backward computation.
I0711 21:56:26.365231 13090 net.cpp:226] data/bias does not need backward computation.
I0711 21:56:26.365236 13090 net.cpp:226] data does not need backward computation.
I0711 21:56:26.365239 13090 net.cpp:268] This network produces output loss
I0711 21:56:26.365283 13090 net.cpp:288] Network initialization done.
I0711 21:56:26.366222 13090 solver.cpp:182] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/sparse/test.prototxt
I0711 21:56:26.366515 13090 net.cpp:56] Initializing net from parameters: 
name: "jsegnet21v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/val-image-lmdb"
    label_list_path: "data/val-label-lmdb"
    batch_size: 4
    threads: 4
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b/bn"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a/bn"
  top: "out5a/bn"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a/bn"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b/bn"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a/bn"
  top: "out3a/bn"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a/bn"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv1/bn"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv2/bn"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv3/bn"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4/bn"
  top: "ctx_conv4/bn"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4/bn"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
  accuracy_param {
    ignore_label: 255
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
    ignore_label: 255
  }
}
I0711 21:56:26.366688 13090 layer_factory.hpp:77] Creating layer data
I0711 21:56:26.366696 13090 net.cpp:98] Creating Layer data
I0711 21:56:26.366700 13090 net.cpp:413] data -> data
I0711 21:56:26.366704 13090 net.cpp:413] data -> label
I0711 21:56:26.383443 13166 db_lmdb.cpp:35] Opened lmdb data/val-image-lmdb
I0711 21:56:26.386066 13090 data_layer.cpp:78] ReshapePrefetch 4, 3, 640, 640
I0711 21:56:26.386175 13090 data_layer.cpp:83] output data size: 4,3,640,640
I0711 21:56:26.396155 13171 db_lmdb.cpp:35] Opened lmdb data/val-label-lmdb
I0711 21:56:26.413851 13090 data_layer.cpp:78] ReshapePrefetch 4, 1, 640, 640
I0711 21:56:26.413929 13090 data_layer.cpp:83] output data size: 4,1,640,640
I0711 21:56:26.427109 13090 net.cpp:148] Setting up data
I0711 21:56:26.427201 13090 net.cpp:155] Top shape: 4 3 640 640 (4915200)
I0711 21:56:26.427220 13090 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0711 21:56:26.427242 13090 net.cpp:163] Memory required for data: 26214400
I0711 21:56:26.427266 13090 layer_factory.hpp:77] Creating layer label_data_1_split
I0711 21:56:26.427304 13090 net.cpp:98] Creating Layer label_data_1_split
I0711 21:56:26.427314 13090 net.cpp:439] label_data_1_split <- label
I0711 21:56:26.427327 13090 net.cpp:413] label_data_1_split -> label_data_1_split_0
I0711 21:56:26.427340 13090 net.cpp:413] label_data_1_split -> label_data_1_split_1
I0711 21:56:26.427350 13090 net.cpp:413] label_data_1_split -> label_data_1_split_2
I0711 21:56:26.427520 13090 net.cpp:148] Setting up label_data_1_split
I0711 21:56:26.427532 13090 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0711 21:56:26.427541 13090 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0711 21:56:26.427549 13090 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0711 21:56:26.427556 13090 net.cpp:163] Memory required for data: 45875200
I0711 21:56:26.427563 13090 layer_factory.hpp:77] Creating layer data/bias
I0711 21:56:26.427575 13090 net.cpp:98] Creating Layer data/bias
I0711 21:56:26.427583 13090 net.cpp:439] data/bias <- data
I0711 21:56:26.427592 13090 net.cpp:413] data/bias -> data/bias
I0711 21:56:26.429461 13090 net.cpp:148] Setting up data/bias
I0711 21:56:26.429524 13090 net.cpp:155] Top shape: 4 3 640 640 (4915200)
I0711 21:56:26.429538 13090 net.cpp:163] Memory required for data: 65536000
I0711 21:56:26.429558 13090 layer_factory.hpp:77] Creating layer conv1a
I0711 21:56:26.429595 13090 net.cpp:98] Creating Layer conv1a
I0711 21:56:26.429607 13090 net.cpp:439] conv1a <- data/bias
I0711 21:56:26.429618 13090 net.cpp:413] conv1a -> conv1a
I0711 21:56:26.430541 13090 net.cpp:148] Setting up conv1a
I0711 21:56:26.430554 13090 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0711 21:56:26.430557 13090 net.cpp:163] Memory required for data: 117964800
I0711 21:56:26.430565 13090 layer_factory.hpp:77] Creating layer conv1a/bn
I0711 21:56:26.430580 13090 net.cpp:98] Creating Layer conv1a/bn
I0711 21:56:26.430595 13090 net.cpp:439] conv1a/bn <- conv1a
I0711 21:56:26.430608 13090 net.cpp:413] conv1a/bn -> conv1a/bn
I0711 21:56:26.434708 13090 net.cpp:148] Setting up conv1a/bn
I0711 21:56:26.434737 13090 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0711 21:56:26.434741 13090 net.cpp:163] Memory required for data: 170393600
I0711 21:56:26.434763 13090 layer_factory.hpp:77] Creating layer conv1a/relu
I0711 21:56:26.434775 13090 net.cpp:98] Creating Layer conv1a/relu
I0711 21:56:26.434782 13090 net.cpp:439] conv1a/relu <- conv1a/bn
I0711 21:56:26.434795 13090 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0711 21:56:26.434828 13090 net.cpp:148] Setting up conv1a/relu
I0711 21:56:26.434835 13090 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0711 21:56:26.434840 13090 net.cpp:163] Memory required for data: 222822400
I0711 21:56:26.434844 13090 layer_factory.hpp:77] Creating layer conv1b
I0711 21:56:26.434857 13090 net.cpp:98] Creating Layer conv1b
I0711 21:56:26.434861 13090 net.cpp:439] conv1b <- conv1a/bn
I0711 21:56:26.434867 13090 net.cpp:413] conv1b -> conv1b
I0711 21:56:26.435618 13090 net.cpp:148] Setting up conv1b
I0711 21:56:26.435628 13090 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0711 21:56:26.435631 13090 net.cpp:163] Memory required for data: 275251200
I0711 21:56:26.435638 13090 layer_factory.hpp:77] Creating layer conv1b/bn
I0711 21:56:26.435647 13090 net.cpp:98] Creating Layer conv1b/bn
I0711 21:56:26.435649 13090 net.cpp:439] conv1b/bn <- conv1b
I0711 21:56:26.435654 13090 net.cpp:413] conv1b/bn -> conv1b/bn
I0711 21:56:26.436786 13090 net.cpp:148] Setting up conv1b/bn
I0711 21:56:26.436795 13090 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0711 21:56:26.436797 13090 net.cpp:163] Memory required for data: 327680000
I0711 21:56:26.436803 13090 layer_factory.hpp:77] Creating layer conv1b/relu
I0711 21:56:26.436812 13090 net.cpp:98] Creating Layer conv1b/relu
I0711 21:56:26.436836 13090 net.cpp:439] conv1b/relu <- conv1b/bn
I0711 21:56:26.436841 13090 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0711 21:56:26.436847 13090 net.cpp:148] Setting up conv1b/relu
I0711 21:56:26.436852 13090 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0711 21:56:26.436877 13090 net.cpp:163] Memory required for data: 380108800
I0711 21:56:26.436882 13090 layer_factory.hpp:77] Creating layer pool1
I0711 21:56:26.436893 13090 net.cpp:98] Creating Layer pool1
I0711 21:56:26.436897 13090 net.cpp:439] pool1 <- conv1b/bn
I0711 21:56:26.436902 13090 net.cpp:413] pool1 -> pool1
I0711 21:56:26.436957 13090 net.cpp:148] Setting up pool1
I0711 21:56:26.436962 13090 net.cpp:155] Top shape: 4 32 160 160 (3276800)
I0711 21:56:26.436966 13090 net.cpp:163] Memory required for data: 393216000
I0711 21:56:26.436970 13090 layer_factory.hpp:77] Creating layer res2a_branch2a
I0711 21:56:26.436980 13090 net.cpp:98] Creating Layer res2a_branch2a
I0711 21:56:26.436983 13090 net.cpp:439] res2a_branch2a <- pool1
I0711 21:56:26.436990 13090 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0711 21:56:26.440515 13090 net.cpp:148] Setting up res2a_branch2a
I0711 21:56:26.440567 13090 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0711 21:56:26.440589 13090 net.cpp:163] Memory required for data: 419430400
I0711 21:56:26.440631 13090 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0711 21:56:26.440670 13090 net.cpp:98] Creating Layer res2a_branch2a/bn
I0711 21:56:26.440699 13090 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0711 21:56:26.440732 13090 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0711 21:56:26.443466 13090 net.cpp:148] Setting up res2a_branch2a/bn
I0711 21:56:26.443506 13090 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0711 21:56:26.443521 13090 net.cpp:163] Memory required for data: 445644800
I0711 21:56:26.443547 13090 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0711 21:56:26.443567 13090 net.cpp:98] Creating Layer res2a_branch2a/relu
I0711 21:56:26.443583 13090 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0711 21:56:26.443600 13090 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0711 21:56:26.443620 13090 net.cpp:148] Setting up res2a_branch2a/relu
I0711 21:56:26.443637 13090 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0711 21:56:26.443650 13090 net.cpp:163] Memory required for data: 471859200
I0711 21:56:26.443666 13090 layer_factory.hpp:77] Creating layer res2a_branch2b
I0711 21:56:26.443692 13090 net.cpp:98] Creating Layer res2a_branch2b
I0711 21:56:26.443706 13090 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0711 21:56:26.443722 13090 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0711 21:56:26.444669 13090 net.cpp:148] Setting up res2a_branch2b
I0711 21:56:26.444689 13090 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0711 21:56:26.444699 13090 net.cpp:163] Memory required for data: 498073600
I0711 21:56:26.444710 13090 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0711 21:56:26.444722 13090 net.cpp:98] Creating Layer res2a_branch2b/bn
I0711 21:56:26.444731 13090 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0711 21:56:26.444741 13090 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0711 21:56:26.446015 13090 net.cpp:148] Setting up res2a_branch2b/bn
I0711 21:56:26.446082 13090 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0711 21:56:26.446094 13090 net.cpp:163] Memory required for data: 524288000
I0711 21:56:26.446110 13090 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0711 21:56:26.446125 13090 net.cpp:98] Creating Layer res2a_branch2b/relu
I0711 21:56:26.446135 13090 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0711 21:56:26.446146 13090 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0711 21:56:26.446159 13090 net.cpp:148] Setting up res2a_branch2b/relu
I0711 21:56:26.446168 13090 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0711 21:56:26.446177 13090 net.cpp:163] Memory required for data: 550502400
I0711 21:56:26.446184 13090 layer_factory.hpp:77] Creating layer pool2
I0711 21:56:26.446197 13090 net.cpp:98] Creating Layer pool2
I0711 21:56:26.446205 13090 net.cpp:439] pool2 <- res2a_branch2b/bn
I0711 21:56:26.446214 13090 net.cpp:413] pool2 -> pool2
I0711 21:56:26.446313 13090 net.cpp:148] Setting up pool2
I0711 21:56:26.446352 13090 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 21:56:26.446359 13090 net.cpp:163] Memory required for data: 557056000
I0711 21:56:26.446367 13090 layer_factory.hpp:77] Creating layer res3a_branch2a
I0711 21:56:26.446386 13090 net.cpp:98] Creating Layer res3a_branch2a
I0711 21:56:26.446395 13090 net.cpp:439] res3a_branch2a <- pool2
I0711 21:56:26.446405 13090 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0711 21:56:26.448885 13090 net.cpp:148] Setting up res3a_branch2a
I0711 21:56:26.448900 13090 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 21:56:26.448904 13090 net.cpp:163] Memory required for data: 570163200
I0711 21:56:26.448909 13090 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0711 21:56:26.448920 13090 net.cpp:98] Creating Layer res3a_branch2a/bn
I0711 21:56:26.448923 13090 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0711 21:56:26.448931 13090 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0711 21:56:26.449734 13090 net.cpp:148] Setting up res3a_branch2a/bn
I0711 21:56:26.449743 13090 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 21:56:26.449745 13090 net.cpp:163] Memory required for data: 583270400
I0711 21:56:26.449759 13090 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0711 21:56:26.449765 13090 net.cpp:98] Creating Layer res3a_branch2a/relu
I0711 21:56:26.449769 13090 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0711 21:56:26.449779 13090 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0711 21:56:26.449784 13090 net.cpp:148] Setting up res3a_branch2a/relu
I0711 21:56:26.449790 13090 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 21:56:26.449792 13090 net.cpp:163] Memory required for data: 596377600
I0711 21:56:26.449796 13090 layer_factory.hpp:77] Creating layer res3a_branch2b
I0711 21:56:26.449805 13090 net.cpp:98] Creating Layer res3a_branch2b
I0711 21:56:26.449808 13090 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0711 21:56:26.449813 13090 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0711 21:56:26.450959 13090 net.cpp:148] Setting up res3a_branch2b
I0711 21:56:26.450968 13090 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 21:56:26.450971 13090 net.cpp:163] Memory required for data: 609484800
I0711 21:56:26.450978 13090 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0711 21:56:26.450995 13090 net.cpp:98] Creating Layer res3a_branch2b/bn
I0711 21:56:26.451001 13090 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0711 21:56:26.451012 13090 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0711 21:56:26.454614 13090 net.cpp:148] Setting up res3a_branch2b/bn
I0711 21:56:26.454668 13090 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 21:56:26.454689 13090 net.cpp:163] Memory required for data: 622592000
I0711 21:56:26.454715 13090 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0711 21:56:26.454747 13090 net.cpp:98] Creating Layer res3a_branch2b/relu
I0711 21:56:26.454769 13090 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0711 21:56:26.454789 13090 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0711 21:56:26.454813 13090 net.cpp:148] Setting up res3a_branch2b/relu
I0711 21:56:26.454834 13090 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 21:56:26.454851 13090 net.cpp:163] Memory required for data: 635699200
I0711 21:56:26.454869 13090 layer_factory.hpp:77] Creating layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0711 21:56:26.454895 13090 net.cpp:98] Creating Layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0711 21:56:26.454915 13090 net.cpp:439] res3a_branch2b/bn_res3a_branch2b/relu_0_split <- res3a_branch2b/bn
I0711 21:56:26.454936 13090 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0711 21:56:26.454957 13090 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0711 21:56:26.455052 13090 net.cpp:148] Setting up res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0711 21:56:26.455081 13090 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 21:56:26.455121 13090 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 21:56:26.455139 13090 net.cpp:163] Memory required for data: 661913600
I0711 21:56:26.455154 13090 layer_factory.hpp:77] Creating layer pool3
I0711 21:56:26.455175 13090 net.cpp:98] Creating Layer pool3
I0711 21:56:26.455193 13090 net.cpp:439] pool3 <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0711 21:56:26.455209 13090 net.cpp:413] pool3 -> pool3
I0711 21:56:26.455313 13090 net.cpp:148] Setting up pool3
I0711 21:56:26.455332 13090 net.cpp:155] Top shape: 4 128 40 40 (819200)
I0711 21:56:26.455350 13090 net.cpp:163] Memory required for data: 665190400
I0711 21:56:26.455368 13090 layer_factory.hpp:77] Creating layer res4a_branch2a
I0711 21:56:26.455394 13090 net.cpp:98] Creating Layer res4a_branch2a
I0711 21:56:26.455411 13090 net.cpp:439] res4a_branch2a <- pool3
I0711 21:56:26.455436 13090 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0711 21:56:26.465858 13090 net.cpp:148] Setting up res4a_branch2a
I0711 21:56:26.465960 13090 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0711 21:56:26.465978 13090 net.cpp:163] Memory required for data: 671744000
I0711 21:56:26.465997 13090 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0711 21:56:26.466019 13090 net.cpp:98] Creating Layer res4a_branch2a/bn
I0711 21:56:26.466034 13090 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0711 21:56:26.466054 13090 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0711 21:56:26.467051 13090 net.cpp:148] Setting up res4a_branch2a/bn
I0711 21:56:26.467061 13090 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0711 21:56:26.467063 13090 net.cpp:163] Memory required for data: 678297600
I0711 21:56:26.467072 13090 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0711 21:56:26.467077 13090 net.cpp:98] Creating Layer res4a_branch2a/relu
I0711 21:56:26.467079 13090 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0711 21:56:26.467083 13090 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0711 21:56:26.467090 13090 net.cpp:148] Setting up res4a_branch2a/relu
I0711 21:56:26.467094 13090 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0711 21:56:26.467097 13090 net.cpp:163] Memory required for data: 684851200
I0711 21:56:26.467102 13090 layer_factory.hpp:77] Creating layer res4a_branch2b
I0711 21:56:26.467109 13090 net.cpp:98] Creating Layer res4a_branch2b
I0711 21:56:26.467113 13090 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0711 21:56:26.467116 13090 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0711 21:56:26.470491 13090 net.cpp:148] Setting up res4a_branch2b
I0711 21:56:26.470499 13090 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0711 21:56:26.470500 13090 net.cpp:163] Memory required for data: 691404800
I0711 21:56:26.470504 13090 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0711 21:56:26.470507 13090 net.cpp:98] Creating Layer res4a_branch2b/bn
I0711 21:56:26.470510 13090 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0711 21:56:26.470512 13090 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0711 21:56:26.471202 13090 net.cpp:148] Setting up res4a_branch2b/bn
I0711 21:56:26.471209 13090 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0711 21:56:26.471210 13090 net.cpp:163] Memory required for data: 697958400
I0711 21:56:26.471215 13090 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0711 21:56:26.471217 13090 net.cpp:98] Creating Layer res4a_branch2b/relu
I0711 21:56:26.471220 13090 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0711 21:56:26.471222 13090 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0711 21:56:26.471225 13090 net.cpp:148] Setting up res4a_branch2b/relu
I0711 21:56:26.471227 13090 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0711 21:56:26.471230 13090 net.cpp:163] Memory required for data: 704512000
I0711 21:56:26.471231 13090 layer_factory.hpp:77] Creating layer pool4
I0711 21:56:26.471235 13090 net.cpp:98] Creating Layer pool4
I0711 21:56:26.471237 13090 net.cpp:439] pool4 <- res4a_branch2b/bn
I0711 21:56:26.471251 13090 net.cpp:413] pool4 -> pool4
I0711 21:56:26.471293 13090 net.cpp:148] Setting up pool4
I0711 21:56:26.471297 13090 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0711 21:56:26.471299 13090 net.cpp:163] Memory required for data: 711065600
I0711 21:56:26.471302 13090 layer_factory.hpp:77] Creating layer res5a_branch2a
I0711 21:56:26.471307 13090 net.cpp:98] Creating Layer res5a_branch2a
I0711 21:56:26.471310 13090 net.cpp:439] res5a_branch2a <- pool4
I0711 21:56:26.471313 13090 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0711 21:56:26.496119 13090 net.cpp:148] Setting up res5a_branch2a
I0711 21:56:26.496131 13090 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0711 21:56:26.496134 13090 net.cpp:163] Memory required for data: 724172800
I0711 21:56:26.496139 13090 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0711 21:56:26.496146 13090 net.cpp:98] Creating Layer res5a_branch2a/bn
I0711 21:56:26.496150 13090 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0711 21:56:26.496152 13090 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0711 21:56:26.496858 13090 net.cpp:148] Setting up res5a_branch2a/bn
I0711 21:56:26.496865 13090 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0711 21:56:26.496867 13090 net.cpp:163] Memory required for data: 737280000
I0711 21:56:26.496872 13090 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0711 21:56:26.496876 13090 net.cpp:98] Creating Layer res5a_branch2a/relu
I0711 21:56:26.496877 13090 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0711 21:56:26.496881 13090 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0711 21:56:26.496884 13090 net.cpp:148] Setting up res5a_branch2a/relu
I0711 21:56:26.496886 13090 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0711 21:56:26.496888 13090 net.cpp:163] Memory required for data: 750387200
I0711 21:56:26.496891 13090 layer_factory.hpp:77] Creating layer res5a_branch2b
I0711 21:56:26.496894 13090 net.cpp:98] Creating Layer res5a_branch2b
I0711 21:56:26.496896 13090 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0711 21:56:26.496899 13090 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0711 21:56:26.509726 13090 net.cpp:148] Setting up res5a_branch2b
I0711 21:56:26.509734 13090 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0711 21:56:26.509737 13090 net.cpp:163] Memory required for data: 763494400
I0711 21:56:26.509743 13090 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0711 21:56:26.509747 13090 net.cpp:98] Creating Layer res5a_branch2b/bn
I0711 21:56:26.509749 13090 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0711 21:56:26.509752 13090 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0711 21:56:26.510453 13090 net.cpp:148] Setting up res5a_branch2b/bn
I0711 21:56:26.510459 13090 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0711 21:56:26.510462 13090 net.cpp:163] Memory required for data: 776601600
I0711 21:56:26.510466 13090 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0711 21:56:26.510469 13090 net.cpp:98] Creating Layer res5a_branch2b/relu
I0711 21:56:26.510471 13090 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0711 21:56:26.510473 13090 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0711 21:56:26.510478 13090 net.cpp:148] Setting up res5a_branch2b/relu
I0711 21:56:26.510479 13090 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0711 21:56:26.510481 13090 net.cpp:163] Memory required for data: 789708800
I0711 21:56:26.510483 13090 layer_factory.hpp:77] Creating layer out5a
I0711 21:56:26.510488 13090 net.cpp:98] Creating Layer out5a
I0711 21:56:26.510489 13090 net.cpp:439] out5a <- res5a_branch2b/bn
I0711 21:56:26.510491 13090 net.cpp:413] out5a -> out5a
I0711 21:56:26.514597 13090 net.cpp:148] Setting up out5a
I0711 21:56:26.514606 13090 net.cpp:155] Top shape: 4 64 40 40 (409600)
I0711 21:56:26.514608 13090 net.cpp:163] Memory required for data: 791347200
I0711 21:56:26.514612 13090 layer_factory.hpp:77] Creating layer out5a/bn
I0711 21:56:26.514617 13090 net.cpp:98] Creating Layer out5a/bn
I0711 21:56:26.514621 13090 net.cpp:439] out5a/bn <- out5a
I0711 21:56:26.514632 13090 net.cpp:413] out5a/bn -> out5a/bn
I0711 21:56:26.515403 13090 net.cpp:148] Setting up out5a/bn
I0711 21:56:26.515408 13090 net.cpp:155] Top shape: 4 64 40 40 (409600)
I0711 21:56:26.515410 13090 net.cpp:163] Memory required for data: 792985600
I0711 21:56:26.515415 13090 layer_factory.hpp:77] Creating layer out5a/relu
I0711 21:56:26.515419 13090 net.cpp:98] Creating Layer out5a/relu
I0711 21:56:26.515420 13090 net.cpp:439] out5a/relu <- out5a/bn
I0711 21:56:26.515424 13090 net.cpp:400] out5a/relu -> out5a/bn (in-place)
I0711 21:56:26.515426 13090 net.cpp:148] Setting up out5a/relu
I0711 21:56:26.515429 13090 net.cpp:155] Top shape: 4 64 40 40 (409600)
I0711 21:56:26.515430 13090 net.cpp:163] Memory required for data: 794624000
I0711 21:56:26.515432 13090 layer_factory.hpp:77] Creating layer out5a_up2
I0711 21:56:26.515436 13090 net.cpp:98] Creating Layer out5a_up2
I0711 21:56:26.515439 13090 net.cpp:439] out5a_up2 <- out5a/bn
I0711 21:56:26.515440 13090 net.cpp:413] out5a_up2 -> out5a_up2
I0711 21:56:26.515705 13090 net.cpp:148] Setting up out5a_up2
I0711 21:56:26.515710 13090 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 21:56:26.515712 13090 net.cpp:163] Memory required for data: 801177600
I0711 21:56:26.515715 13090 layer_factory.hpp:77] Creating layer out3a
I0711 21:56:26.515719 13090 net.cpp:98] Creating Layer out3a
I0711 21:56:26.515722 13090 net.cpp:439] out3a <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0711 21:56:26.515724 13090 net.cpp:413] out3a -> out3a
I0711 21:56:26.517662 13090 net.cpp:148] Setting up out3a
I0711 21:56:26.517669 13090 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 21:56:26.517673 13090 net.cpp:163] Memory required for data: 807731200
I0711 21:56:26.517676 13090 layer_factory.hpp:77] Creating layer out3a/bn
I0711 21:56:26.517679 13090 net.cpp:98] Creating Layer out3a/bn
I0711 21:56:26.517683 13090 net.cpp:439] out3a/bn <- out3a
I0711 21:56:26.517685 13090 net.cpp:413] out3a/bn -> out3a/bn
I0711 21:56:26.518471 13090 net.cpp:148] Setting up out3a/bn
I0711 21:56:26.518476 13090 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 21:56:26.518478 13090 net.cpp:163] Memory required for data: 814284800
I0711 21:56:26.518483 13090 layer_factory.hpp:77] Creating layer out3a/relu
I0711 21:56:26.518486 13090 net.cpp:98] Creating Layer out3a/relu
I0711 21:56:26.518488 13090 net.cpp:439] out3a/relu <- out3a/bn
I0711 21:56:26.518491 13090 net.cpp:400] out3a/relu -> out3a/bn (in-place)
I0711 21:56:26.518494 13090 net.cpp:148] Setting up out3a/relu
I0711 21:56:26.518496 13090 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 21:56:26.518498 13090 net.cpp:163] Memory required for data: 820838400
I0711 21:56:26.518501 13090 layer_factory.hpp:77] Creating layer out3_out5_combined
I0711 21:56:26.518502 13090 net.cpp:98] Creating Layer out3_out5_combined
I0711 21:56:26.518504 13090 net.cpp:439] out3_out5_combined <- out5a_up2
I0711 21:56:26.518507 13090 net.cpp:439] out3_out5_combined <- out3a/bn
I0711 21:56:26.518509 13090 net.cpp:413] out3_out5_combined -> out3_out5_combined
I0711 21:56:26.518532 13090 net.cpp:148] Setting up out3_out5_combined
I0711 21:56:26.518537 13090 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 21:56:26.518537 13090 net.cpp:163] Memory required for data: 827392000
I0711 21:56:26.518539 13090 layer_factory.hpp:77] Creating layer ctx_conv1
I0711 21:56:26.518543 13090 net.cpp:98] Creating Layer ctx_conv1
I0711 21:56:26.518545 13090 net.cpp:439] ctx_conv1 <- out3_out5_combined
I0711 21:56:26.518548 13090 net.cpp:413] ctx_conv1 -> ctx_conv1
I0711 21:56:26.519593 13090 net.cpp:148] Setting up ctx_conv1
I0711 21:56:26.519598 13090 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 21:56:26.519600 13090 net.cpp:163] Memory required for data: 833945600
I0711 21:56:26.519604 13090 layer_factory.hpp:77] Creating layer ctx_conv1/bn
I0711 21:56:26.519608 13090 net.cpp:98] Creating Layer ctx_conv1/bn
I0711 21:56:26.519609 13090 net.cpp:439] ctx_conv1/bn <- ctx_conv1
I0711 21:56:26.519611 13090 net.cpp:413] ctx_conv1/bn -> ctx_conv1/bn
I0711 21:56:26.520397 13090 net.cpp:148] Setting up ctx_conv1/bn
I0711 21:56:26.520403 13090 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 21:56:26.520406 13090 net.cpp:163] Memory required for data: 840499200
I0711 21:56:26.520411 13090 layer_factory.hpp:77] Creating layer ctx_conv1/relu
I0711 21:56:26.520413 13090 net.cpp:98] Creating Layer ctx_conv1/relu
I0711 21:56:26.520416 13090 net.cpp:439] ctx_conv1/relu <- ctx_conv1/bn
I0711 21:56:26.520417 13090 net.cpp:400] ctx_conv1/relu -> ctx_conv1/bn (in-place)
I0711 21:56:26.520421 13090 net.cpp:148] Setting up ctx_conv1/relu
I0711 21:56:26.520423 13090 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 21:56:26.520426 13090 net.cpp:163] Memory required for data: 847052800
I0711 21:56:26.520426 13090 layer_factory.hpp:77] Creating layer ctx_conv2
I0711 21:56:26.520432 13090 net.cpp:98] Creating Layer ctx_conv2
I0711 21:56:26.520434 13090 net.cpp:439] ctx_conv2 <- ctx_conv1/bn
I0711 21:56:26.520437 13090 net.cpp:413] ctx_conv2 -> ctx_conv2
I0711 21:56:26.521483 13090 net.cpp:148] Setting up ctx_conv2
I0711 21:56:26.521489 13090 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 21:56:26.521492 13090 net.cpp:163] Memory required for data: 853606400
I0711 21:56:26.521494 13090 layer_factory.hpp:77] Creating layer ctx_conv2/bn
I0711 21:56:26.521497 13090 net.cpp:98] Creating Layer ctx_conv2/bn
I0711 21:56:26.521499 13090 net.cpp:439] ctx_conv2/bn <- ctx_conv2
I0711 21:56:26.521502 13090 net.cpp:413] ctx_conv2/bn -> ctx_conv2/bn
I0711 21:56:26.522287 13090 net.cpp:148] Setting up ctx_conv2/bn
I0711 21:56:26.522292 13090 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 21:56:26.522294 13090 net.cpp:163] Memory required for data: 860160000
I0711 21:56:26.522299 13090 layer_factory.hpp:77] Creating layer ctx_conv2/relu
I0711 21:56:26.522301 13090 net.cpp:98] Creating Layer ctx_conv2/relu
I0711 21:56:26.522303 13090 net.cpp:439] ctx_conv2/relu <- ctx_conv2/bn
I0711 21:56:26.522305 13090 net.cpp:400] ctx_conv2/relu -> ctx_conv2/bn (in-place)
I0711 21:56:26.522310 13090 net.cpp:148] Setting up ctx_conv2/relu
I0711 21:56:26.522311 13090 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 21:56:26.522312 13090 net.cpp:163] Memory required for data: 866713600
I0711 21:56:26.522315 13090 layer_factory.hpp:77] Creating layer ctx_conv3
I0711 21:56:26.522318 13090 net.cpp:98] Creating Layer ctx_conv3
I0711 21:56:26.522320 13090 net.cpp:439] ctx_conv3 <- ctx_conv2/bn
I0711 21:56:26.522323 13090 net.cpp:413] ctx_conv3 -> ctx_conv3
I0711 21:56:26.523368 13090 net.cpp:148] Setting up ctx_conv3
I0711 21:56:26.523373 13090 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 21:56:26.523375 13090 net.cpp:163] Memory required for data: 873267200
I0711 21:56:26.523378 13090 layer_factory.hpp:77] Creating layer ctx_conv3/bn
I0711 21:56:26.523381 13090 net.cpp:98] Creating Layer ctx_conv3/bn
I0711 21:56:26.523383 13090 net.cpp:439] ctx_conv3/bn <- ctx_conv3
I0711 21:56:26.523386 13090 net.cpp:413] ctx_conv3/bn -> ctx_conv3/bn
I0711 21:56:26.524163 13090 net.cpp:148] Setting up ctx_conv3/bn
I0711 21:56:26.524168 13090 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 21:56:26.524169 13090 net.cpp:163] Memory required for data: 879820800
I0711 21:56:26.524174 13090 layer_factory.hpp:77] Creating layer ctx_conv3/relu
I0711 21:56:26.524178 13090 net.cpp:98] Creating Layer ctx_conv3/relu
I0711 21:56:26.524179 13090 net.cpp:439] ctx_conv3/relu <- ctx_conv3/bn
I0711 21:56:26.524183 13090 net.cpp:400] ctx_conv3/relu -> ctx_conv3/bn (in-place)
I0711 21:56:26.524185 13090 net.cpp:148] Setting up ctx_conv3/relu
I0711 21:56:26.524188 13090 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 21:56:26.524189 13090 net.cpp:163] Memory required for data: 886374400
I0711 21:56:26.524191 13090 layer_factory.hpp:77] Creating layer ctx_conv4
I0711 21:56:26.524194 13090 net.cpp:98] Creating Layer ctx_conv4
I0711 21:56:26.524196 13090 net.cpp:439] ctx_conv4 <- ctx_conv3/bn
I0711 21:56:26.524199 13090 net.cpp:413] ctx_conv4 -> ctx_conv4
I0711 21:56:26.525246 13090 net.cpp:148] Setting up ctx_conv4
I0711 21:56:26.525257 13090 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 21:56:26.525259 13090 net.cpp:163] Memory required for data: 892928000
I0711 21:56:26.525264 13090 layer_factory.hpp:77] Creating layer ctx_conv4/bn
I0711 21:56:26.525266 13090 net.cpp:98] Creating Layer ctx_conv4/bn
I0711 21:56:26.525269 13090 net.cpp:439] ctx_conv4/bn <- ctx_conv4
I0711 21:56:26.525271 13090 net.cpp:413] ctx_conv4/bn -> ctx_conv4/bn
I0711 21:56:26.526054 13090 net.cpp:148] Setting up ctx_conv4/bn
I0711 21:56:26.526059 13090 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 21:56:26.526062 13090 net.cpp:163] Memory required for data: 899481600
I0711 21:56:26.526067 13090 layer_factory.hpp:77] Creating layer ctx_conv4/relu
I0711 21:56:26.526069 13090 net.cpp:98] Creating Layer ctx_conv4/relu
I0711 21:56:26.526072 13090 net.cpp:439] ctx_conv4/relu <- ctx_conv4/bn
I0711 21:56:26.526073 13090 net.cpp:400] ctx_conv4/relu -> ctx_conv4/bn (in-place)
I0711 21:56:26.526077 13090 net.cpp:148] Setting up ctx_conv4/relu
I0711 21:56:26.526078 13090 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 21:56:26.526080 13090 net.cpp:163] Memory required for data: 906035200
I0711 21:56:26.526082 13090 layer_factory.hpp:77] Creating layer ctx_final
I0711 21:56:26.526085 13090 net.cpp:98] Creating Layer ctx_final
I0711 21:56:26.526087 13090 net.cpp:439] ctx_final <- ctx_conv4/bn
I0711 21:56:26.526089 13090 net.cpp:413] ctx_final -> ctx_final
I0711 21:56:26.526509 13090 net.cpp:148] Setting up ctx_final
I0711 21:56:26.526513 13090 net.cpp:155] Top shape: 4 8 80 80 (204800)
I0711 21:56:26.526515 13090 net.cpp:163] Memory required for data: 906854400
I0711 21:56:26.526518 13090 layer_factory.hpp:77] Creating layer ctx_final/relu
I0711 21:56:26.526521 13090 net.cpp:98] Creating Layer ctx_final/relu
I0711 21:56:26.526523 13090 net.cpp:439] ctx_final/relu <- ctx_final
I0711 21:56:26.526525 13090 net.cpp:400] ctx_final/relu -> ctx_final (in-place)
I0711 21:56:26.526528 13090 net.cpp:148] Setting up ctx_final/relu
I0711 21:56:26.526531 13090 net.cpp:155] Top shape: 4 8 80 80 (204800)
I0711 21:56:26.526532 13090 net.cpp:163] Memory required for data: 907673600
I0711 21:56:26.526535 13090 layer_factory.hpp:77] Creating layer out_deconv_final_up2
I0711 21:56:26.526537 13090 net.cpp:98] Creating Layer out_deconv_final_up2
I0711 21:56:26.526540 13090 net.cpp:439] out_deconv_final_up2 <- ctx_final
I0711 21:56:26.526541 13090 net.cpp:413] out_deconv_final_up2 -> out_deconv_final_up2
I0711 21:56:26.526792 13090 net.cpp:148] Setting up out_deconv_final_up2
I0711 21:56:26.526796 13090 net.cpp:155] Top shape: 4 8 160 160 (819200)
I0711 21:56:26.526798 13090 net.cpp:163] Memory required for data: 910950400
I0711 21:56:26.526800 13090 layer_factory.hpp:77] Creating layer out_deconv_final_up4
I0711 21:56:26.526803 13090 net.cpp:98] Creating Layer out_deconv_final_up4
I0711 21:56:26.526805 13090 net.cpp:439] out_deconv_final_up4 <- out_deconv_final_up2
I0711 21:56:26.526808 13090 net.cpp:413] out_deconv_final_up4 -> out_deconv_final_up4
I0711 21:56:26.527055 13090 net.cpp:148] Setting up out_deconv_final_up4
I0711 21:56:26.527060 13090 net.cpp:155] Top shape: 4 8 320 320 (3276800)
I0711 21:56:26.527061 13090 net.cpp:163] Memory required for data: 924057600
I0711 21:56:26.527063 13090 layer_factory.hpp:77] Creating layer out_deconv_final_up8
I0711 21:56:26.527066 13090 net.cpp:98] Creating Layer out_deconv_final_up8
I0711 21:56:26.527068 13090 net.cpp:439] out_deconv_final_up8 <- out_deconv_final_up4
I0711 21:56:26.527070 13090 net.cpp:413] out_deconv_final_up8 -> out_deconv_final_up8
I0711 21:56:26.527313 13090 net.cpp:148] Setting up out_deconv_final_up8
I0711 21:56:26.527318 13090 net.cpp:155] Top shape: 4 8 640 640 (13107200)
I0711 21:56:26.527319 13090 net.cpp:163] Memory required for data: 976486400
I0711 21:56:26.527321 13090 layer_factory.hpp:77] Creating layer out_deconv_final_up8_out_deconv_final_up8_0_split
I0711 21:56:26.527324 13090 net.cpp:98] Creating Layer out_deconv_final_up8_out_deconv_final_up8_0_split
I0711 21:56:26.527328 13090 net.cpp:439] out_deconv_final_up8_out_deconv_final_up8_0_split <- out_deconv_final_up8
I0711 21:56:26.527334 13090 net.cpp:413] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0711 21:56:26.527338 13090 net.cpp:413] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0711 21:56:26.527340 13090 net.cpp:413] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0711 21:56:26.527397 13090 net.cpp:148] Setting up out_deconv_final_up8_out_deconv_final_up8_0_split
I0711 21:56:26.527401 13090 net.cpp:155] Top shape: 4 8 640 640 (13107200)
I0711 21:56:26.527405 13090 net.cpp:155] Top shape: 4 8 640 640 (13107200)
I0711 21:56:26.527406 13090 net.cpp:155] Top shape: 4 8 640 640 (13107200)
I0711 21:56:26.527407 13090 net.cpp:163] Memory required for data: 1133772800
I0711 21:56:26.527410 13090 layer_factory.hpp:77] Creating layer loss
I0711 21:56:26.527413 13090 net.cpp:98] Creating Layer loss
I0711 21:56:26.527416 13090 net.cpp:439] loss <- out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0711 21:56:26.527418 13090 net.cpp:439] loss <- label_data_1_split_0
I0711 21:56:26.527421 13090 net.cpp:413] loss -> loss
I0711 21:56:26.527426 13090 layer_factory.hpp:77] Creating layer loss
I0711 21:56:26.544015 13090 net.cpp:148] Setting up loss
I0711 21:56:26.544039 13090 net.cpp:155] Top shape: (1)
I0711 21:56:26.544040 13090 net.cpp:158]     with loss weight 1
I0711 21:56:26.544049 13090 net.cpp:163] Memory required for data: 1133772804
I0711 21:56:26.544051 13090 layer_factory.hpp:77] Creating layer accuracy/top1
I0711 21:56:26.544059 13090 net.cpp:98] Creating Layer accuracy/top1
I0711 21:56:26.544062 13090 net.cpp:439] accuracy/top1 <- out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0711 21:56:26.544066 13090 net.cpp:439] accuracy/top1 <- label_data_1_split_1
I0711 21:56:26.544070 13090 net.cpp:413] accuracy/top1 -> accuracy/top1
I0711 21:56:26.544078 13090 net.cpp:148] Setting up accuracy/top1
I0711 21:56:26.544080 13090 net.cpp:155] Top shape: (1)
I0711 21:56:26.544082 13090 net.cpp:163] Memory required for data: 1133772808
I0711 21:56:26.544085 13090 layer_factory.hpp:77] Creating layer accuracy/top5
I0711 21:56:26.544087 13090 net.cpp:98] Creating Layer accuracy/top5
I0711 21:56:26.544090 13090 net.cpp:439] accuracy/top5 <- out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0711 21:56:26.544092 13090 net.cpp:439] accuracy/top5 <- label_data_1_split_2
I0711 21:56:26.544095 13090 net.cpp:413] accuracy/top5 -> accuracy/top5
I0711 21:56:26.544100 13090 net.cpp:148] Setting up accuracy/top5
I0711 21:56:26.544102 13090 net.cpp:155] Top shape: (1)
I0711 21:56:26.544104 13090 net.cpp:163] Memory required for data: 1133772812
I0711 21:56:26.544106 13090 net.cpp:226] accuracy/top5 does not need backward computation.
I0711 21:56:26.544108 13090 net.cpp:226] accuracy/top1 does not need backward computation.
I0711 21:56:26.544111 13090 net.cpp:224] loss needs backward computation.
I0711 21:56:26.544113 13090 net.cpp:224] out_deconv_final_up8_out_deconv_final_up8_0_split needs backward computation.
I0711 21:56:26.544116 13090 net.cpp:224] out_deconv_final_up8 needs backward computation.
I0711 21:56:26.544118 13090 net.cpp:224] out_deconv_final_up4 needs backward computation.
I0711 21:56:26.544121 13090 net.cpp:224] out_deconv_final_up2 needs backward computation.
I0711 21:56:26.544123 13090 net.cpp:224] ctx_final/relu needs backward computation.
I0711 21:56:26.544126 13090 net.cpp:224] ctx_final needs backward computation.
I0711 21:56:26.544128 13090 net.cpp:224] ctx_conv4/relu needs backward computation.
I0711 21:56:26.544131 13090 net.cpp:224] ctx_conv4/bn needs backward computation.
I0711 21:56:26.544133 13090 net.cpp:224] ctx_conv4 needs backward computation.
I0711 21:56:26.544136 13090 net.cpp:224] ctx_conv3/relu needs backward computation.
I0711 21:56:26.544137 13090 net.cpp:224] ctx_conv3/bn needs backward computation.
I0711 21:56:26.544139 13090 net.cpp:224] ctx_conv3 needs backward computation.
I0711 21:56:26.544152 13090 net.cpp:224] ctx_conv2/relu needs backward computation.
I0711 21:56:26.544155 13090 net.cpp:224] ctx_conv2/bn needs backward computation.
I0711 21:56:26.544157 13090 net.cpp:224] ctx_conv2 needs backward computation.
I0711 21:56:26.544158 13090 net.cpp:224] ctx_conv1/relu needs backward computation.
I0711 21:56:26.544162 13090 net.cpp:224] ctx_conv1/bn needs backward computation.
I0711 21:56:26.544163 13090 net.cpp:224] ctx_conv1 needs backward computation.
I0711 21:56:26.544167 13090 net.cpp:224] out3_out5_combined needs backward computation.
I0711 21:56:26.544170 13090 net.cpp:224] out3a/relu needs backward computation.
I0711 21:56:26.544173 13090 net.cpp:224] out3a/bn needs backward computation.
I0711 21:56:26.544175 13090 net.cpp:224] out3a needs backward computation.
I0711 21:56:26.544178 13090 net.cpp:224] out5a_up2 needs backward computation.
I0711 21:56:26.544181 13090 net.cpp:224] out5a/relu needs backward computation.
I0711 21:56:26.544184 13090 net.cpp:224] out5a/bn needs backward computation.
I0711 21:56:26.544186 13090 net.cpp:224] out5a needs backward computation.
I0711 21:56:26.544189 13090 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0711 21:56:26.544193 13090 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0711 21:56:26.544194 13090 net.cpp:224] res5a_branch2b needs backward computation.
I0711 21:56:26.544196 13090 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0711 21:56:26.544199 13090 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0711 21:56:26.544201 13090 net.cpp:224] res5a_branch2a needs backward computation.
I0711 21:56:26.544204 13090 net.cpp:224] pool4 needs backward computation.
I0711 21:56:26.544206 13090 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0711 21:56:26.544209 13090 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0711 21:56:26.544211 13090 net.cpp:224] res4a_branch2b needs backward computation.
I0711 21:56:26.544214 13090 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0711 21:56:26.544215 13090 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0711 21:56:26.544217 13090 net.cpp:224] res4a_branch2a needs backward computation.
I0711 21:56:26.544220 13090 net.cpp:224] pool3 needs backward computation.
I0711 21:56:26.544224 13090 net.cpp:224] res3a_branch2b/bn_res3a_branch2b/relu_0_split needs backward computation.
I0711 21:56:26.544225 13090 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0711 21:56:26.544227 13090 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0711 21:56:26.544230 13090 net.cpp:224] res3a_branch2b needs backward computation.
I0711 21:56:26.544232 13090 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0711 21:56:26.544234 13090 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0711 21:56:26.544237 13090 net.cpp:224] res3a_branch2a needs backward computation.
I0711 21:56:26.544239 13090 net.cpp:224] pool2 needs backward computation.
I0711 21:56:26.544241 13090 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0711 21:56:26.544245 13090 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0711 21:56:26.544246 13090 net.cpp:224] res2a_branch2b needs backward computation.
I0711 21:56:26.544250 13090 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0711 21:56:26.544251 13090 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0711 21:56:26.544253 13090 net.cpp:224] res2a_branch2a needs backward computation.
I0711 21:56:26.544255 13090 net.cpp:224] pool1 needs backward computation.
I0711 21:56:26.544257 13090 net.cpp:224] conv1b/relu needs backward computation.
I0711 21:56:26.544260 13090 net.cpp:224] conv1b/bn needs backward computation.
I0711 21:56:26.544263 13090 net.cpp:224] conv1b needs backward computation.
I0711 21:56:26.544265 13090 net.cpp:224] conv1a/relu needs backward computation.
I0711 21:56:26.544267 13090 net.cpp:224] conv1a/bn needs backward computation.
I0711 21:56:26.544270 13090 net.cpp:224] conv1a needs backward computation.
I0711 21:56:26.544275 13090 net.cpp:226] data/bias does not need backward computation.
I0711 21:56:26.544278 13090 net.cpp:226] label_data_1_split does not need backward computation.
I0711 21:56:26.544281 13090 net.cpp:226] data does not need backward computation.
I0711 21:56:26.544283 13090 net.cpp:268] This network produces output accuracy/top1
I0711 21:56:26.544286 13090 net.cpp:268] This network produces output accuracy/top5
I0711 21:56:26.544288 13090 net.cpp:268] This network produces output loss
I0711 21:56:26.544322 13090 net.cpp:288] Network initialization done.
I0711 21:56:26.544412 13090 solver.cpp:60] Solver scaffolding done.
I0711 21:56:26.552425 13090 caffe.cpp:145] Finetuning from training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/l1reg/cityscapes5_jsegnet21v2_iter_32000.caffemodel
I0711 21:56:26.584442 13090 data_layer.cpp:78] ReshapePrefetch 5, 3, 640, 640
I0711 21:56:26.584507 13090 data_layer.cpp:83] output data size: 5,3,640,640
I0711 21:56:26.624512 13090 data_layer.cpp:78] ReshapePrefetch 5, 1, 640, 640
I0711 21:56:26.624583 13090 data_layer.cpp:83] output data size: 5,1,640,640
I0711 21:56:27.137807 13090 data_layer.cpp:78] ReshapePrefetch 5, 3, 640, 640
I0711 21:56:27.138151 13090 data_layer.cpp:83] output data size: 5,3,640,640
I0711 21:56:27.216341 13090 data_layer.cpp:78] ReshapePrefetch 5, 1, 640, 640
I0711 21:56:27.218072 13090 data_layer.cpp:83] output data size: 5,1,640,640
I0711 21:56:29.592989 13090 parallel.cpp:334] Starting Optimization
I0711 21:56:29.593044 13090 net.cpp:1907] All zero weights of convolution layers are frozen
I0711 21:56:29.604212 13090 solver.cpp:409] Solving jsegnet21v2_train
I0711 21:56:29.604228 13090 solver.cpp:410] Learning Rate Policy: multistep
I0711 21:56:29.999721 13090 solver.cpp:290] Iteration 0 (0 iter/s, 0.395462s/100 iter), loss = 0.0155229
I0711 21:56:29.999743 13090 solver.cpp:309]     Train net output #0: loss = 0.0155229 (* 1 = 0.0155229 loss)
I0711 21:56:29.999752 13090 sgd_solver.cpp:106] Iteration 0, lr = 1e-05
I0711 21:56:30.048492 13090 solver.cpp:376] Finding and applying sparsity: 0.8
I0711 21:57:38.113512 13090 net.cpp:1907] All zero weights of convolution layers are frozen
I0711 21:57:45.196447 13090 blocking_queue.cpp:50] Data layer prefetch queue empty
I0711 21:58:14.992141 13090 solver.cpp:290] Iteration 100 (0.952476 iter/s, 104.99s/100 iter), loss = 0.096618
I0711 21:58:14.992214 13090 solver.cpp:309]     Train net output #0: loss = 0.0966179 (* 1 = 0.0966179 loss)
I0711 21:58:14.992223 13090 sgd_solver.cpp:106] Iteration 100, lr = 1e-05
I0711 21:58:40.420359 13227 blocking_queue.cpp:50] Waiting for data
I0711 21:59:00.279531 13090 solver.cpp:290] Iteration 200 (2.20818 iter/s, 45.2861s/100 iter), loss = 0.0521534
I0711 21:59:00.279633 13090 solver.cpp:309]     Train net output #0: loss = 0.0521534 (* 1 = 0.0521534 loss)
I0711 21:59:00.279644 13090 sgd_solver.cpp:106] Iteration 200, lr = 1e-05
I0711 21:59:24.184201 13090 solver.cpp:290] Iteration 300 (4.18342 iter/s, 23.9039s/100 iter), loss = 0.130307
I0711 21:59:24.184224 13090 solver.cpp:309]     Train net output #0: loss = 0.130307 (* 1 = 0.130307 loss)
I0711 21:59:24.184231 13090 sgd_solver.cpp:106] Iteration 300, lr = 1e-05
I0711 21:59:41.302057 13090 solver.cpp:290] Iteration 400 (5.84202 iter/s, 17.1174s/100 iter), loss = 0.0622129
I0711 21:59:41.302145 13090 solver.cpp:309]     Train net output #0: loss = 0.0622129 (* 1 = 0.0622129 loss)
I0711 21:59:41.302156 13090 sgd_solver.cpp:106] Iteration 400, lr = 1e-05
I0711 21:59:58.426431 13090 solver.cpp:290] Iteration 500 (5.83982 iter/s, 17.1238s/100 iter), loss = 0.0362475
I0711 21:59:58.426455 13090 solver.cpp:309]     Train net output #0: loss = 0.0362475 (* 1 = 0.0362475 loss)
I0711 21:59:58.426462 13090 sgd_solver.cpp:106] Iteration 500, lr = 1e-05
I0711 22:00:15.763095 13090 solver.cpp:290] Iteration 600 (5.76829 iter/s, 17.3362s/100 iter), loss = 0.0217607
I0711 22:00:15.763162 13090 solver.cpp:309]     Train net output #0: loss = 0.0217607 (* 1 = 0.0217607 loss)
I0711 22:00:15.763173 13090 sgd_solver.cpp:106] Iteration 600, lr = 1e-05
I0711 22:00:32.991034 13090 solver.cpp:290] Iteration 700 (5.80471 iter/s, 17.2274s/100 iter), loss = 0.0590531
I0711 22:00:32.991057 13090 solver.cpp:309]     Train net output #0: loss = 0.0590531 (* 1 = 0.0590531 loss)
I0711 22:00:32.991065 13090 sgd_solver.cpp:106] Iteration 700, lr = 1e-05
I0711 22:00:50.041280 13090 solver.cpp:290] Iteration 800 (5.86519 iter/s, 17.0498s/100 iter), loss = 0.030436
I0711 22:00:50.041357 13090 solver.cpp:309]     Train net output #0: loss = 0.0304361 (* 1 = 0.0304361 loss)
I0711 22:00:50.041364 13090 sgd_solver.cpp:106] Iteration 800, lr = 1e-05
I0711 22:01:07.099623 13090 solver.cpp:290] Iteration 900 (5.86242 iter/s, 17.0578s/100 iter), loss = 0.0380209
I0711 22:01:07.099647 13090 solver.cpp:309]     Train net output #0: loss = 0.038021 (* 1 = 0.038021 loss)
I0711 22:01:07.099653 13090 sgd_solver.cpp:106] Iteration 900, lr = 1e-05
I0711 22:01:24.015879 13090 solver.cpp:354] Sparsity after update:
I0711 22:01:24.071175 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 22:01:24.071190 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 22:01:24.071202 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 22:01:24.071205 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 22:01:24.071208 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 22:01:24.071208 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 22:01:24.071210 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 22:01:24.071213 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 22:01:24.071214 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 22:01:24.071216 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 22:01:24.071218 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 22:01:24.071220 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 22:01:24.071223 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 22:01:24.071224 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 22:01:24.071226 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 22:01:24.071228 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 22:01:24.071230 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 22:01:24.071234 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 22:01:24.071238 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 22:01:24.228116 13090 solver.cpp:290] Iteration 1000 (5.83839 iter/s, 17.128s/100 iter), loss = 0.0826203
I0711 22:01:24.228144 13090 solver.cpp:309]     Train net output #0: loss = 0.0826204 (* 1 = 0.0826204 loss)
I0711 22:01:24.228153 13090 sgd_solver.cpp:106] Iteration 1000, lr = 1e-05
I0711 22:01:41.458732 13090 solver.cpp:290] Iteration 1100 (5.80379 iter/s, 17.2301s/100 iter), loss = 0.0616464
I0711 22:01:41.458766 13090 solver.cpp:309]     Train net output #0: loss = 0.0616465 (* 1 = 0.0616465 loss)
I0711 22:01:41.458777 13090 sgd_solver.cpp:106] Iteration 1100, lr = 1e-05
I0711 22:01:58.489960 13090 solver.cpp:290] Iteration 1200 (5.87174 iter/s, 17.0307s/100 iter), loss = 0.0614037
I0711 22:01:58.490039 13090 solver.cpp:309]     Train net output #0: loss = 0.0614038 (* 1 = 0.0614038 loss)
I0711 22:01:58.490051 13090 sgd_solver.cpp:106] Iteration 1200, lr = 1e-05
I0711 22:02:15.693430 13090 solver.cpp:290] Iteration 1300 (5.81297 iter/s, 17.2029s/100 iter), loss = 0.045411
I0711 22:02:15.693454 13090 solver.cpp:309]     Train net output #0: loss = 0.0454111 (* 1 = 0.0454111 loss)
I0711 22:02:15.693460 13090 sgd_solver.cpp:106] Iteration 1300, lr = 1e-05
I0711 22:02:33.036651 13090 solver.cpp:290] Iteration 1400 (5.76611 iter/s, 17.3427s/100 iter), loss = 0.0956192
I0711 22:02:33.036762 13090 solver.cpp:309]     Train net output #0: loss = 0.0956194 (* 1 = 0.0956194 loss)
I0711 22:02:33.036774 13090 sgd_solver.cpp:106] Iteration 1400, lr = 1e-05
I0711 22:02:50.139547 13090 solver.cpp:290] Iteration 1500 (5.84716 iter/s, 17.1023s/100 iter), loss = 0.0223211
I0711 22:02:50.139593 13090 solver.cpp:309]     Train net output #0: loss = 0.0223212 (* 1 = 0.0223212 loss)
I0711 22:02:50.139607 13090 sgd_solver.cpp:106] Iteration 1500, lr = 1e-05
I0711 22:03:08.143749 13090 solver.cpp:290] Iteration 1600 (5.55442 iter/s, 18.0037s/100 iter), loss = 0.0694799
I0711 22:03:08.143843 13090 solver.cpp:309]     Train net output #0: loss = 0.06948 (* 1 = 0.06948 loss)
I0711 22:03:08.143851 13090 sgd_solver.cpp:106] Iteration 1600, lr = 1e-05
I0711 22:03:25.346989 13090 solver.cpp:290] Iteration 1700 (5.81305 iter/s, 17.2027s/100 iter), loss = 0.036184
I0711 22:03:25.347013 13090 solver.cpp:309]     Train net output #0: loss = 0.0361841 (* 1 = 0.0361841 loss)
I0711 22:03:25.347020 13090 sgd_solver.cpp:106] Iteration 1700, lr = 1e-05
I0711 22:03:42.439709 13090 solver.cpp:290] Iteration 1800 (5.85061 iter/s, 17.0922s/100 iter), loss = 0.0989872
I0711 22:03:42.439800 13090 solver.cpp:309]     Train net output #0: loss = 0.0989873 (* 1 = 0.0989873 loss)
I0711 22:03:42.439810 13090 sgd_solver.cpp:106] Iteration 1800, lr = 1e-05
I0711 22:03:59.657243 13090 solver.cpp:290] Iteration 1900 (5.80822 iter/s, 17.217s/100 iter), loss = 0.0762458
I0711 22:03:59.657270 13090 solver.cpp:309]     Train net output #0: loss = 0.076246 (* 1 = 0.076246 loss)
I0711 22:03:59.657279 13090 sgd_solver.cpp:106] Iteration 1900, lr = 1e-05
I0711 22:04:16.571547 13090 solver.cpp:354] Sparsity after update:
I0711 22:04:16.573709 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 22:04:16.573717 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 22:04:16.573724 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 22:04:16.573726 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 22:04:16.573729 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 22:04:16.573730 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 22:04:16.573732 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 22:04:16.573734 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 22:04:16.573736 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 22:04:16.573738 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 22:04:16.573740 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 22:04:16.573742 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 22:04:16.573745 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 22:04:16.573746 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 22:04:16.573748 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 22:04:16.573750 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 22:04:16.573752 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 22:04:16.573755 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 22:04:16.573757 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 22:04:16.573973 13090 solver.cpp:467] Iteration 2000, Testing net (#0)
I0711 22:05:04.685932 13090 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.944677
I0711 22:05:04.686033 13090 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999859
I0711 22:05:04.686040 13090 solver.cpp:540]     Test net output #2: loss = 0.136176 (* 1 = 0.136176 loss)
I0711 22:05:04.861604 13090 solver.cpp:290] Iteration 2000 (1.53368 iter/s, 65.2026s/100 iter), loss = 0.0226161
I0711 22:05:04.861627 13090 solver.cpp:309]     Train net output #0: loss = 0.0226163 (* 1 = 0.0226163 loss)
I0711 22:05:04.861634 13090 sgd_solver.cpp:106] Iteration 2000, lr = 1e-05
I0711 22:05:27.375010 13090 solver.cpp:290] Iteration 2100 (4.44193 iter/s, 22.5128s/100 iter), loss = 0.0300786
I0711 22:05:27.375036 13090 solver.cpp:309]     Train net output #0: loss = 0.0300788 (* 1 = 0.0300788 loss)
I0711 22:05:27.375042 13090 sgd_solver.cpp:106] Iteration 2100, lr = 1e-05
I0711 22:05:40.109591 13227 blocking_queue.cpp:50] Waiting for data
I0711 22:06:04.296638 13090 solver.cpp:290] Iteration 2200 (2.70852 iter/s, 36.9206s/100 iter), loss = 0.0509574
I0711 22:06:04.296663 13090 solver.cpp:309]     Train net output #0: loss = 0.0509576 (* 1 = 0.0509576 loss)
I0711 22:06:04.296671 13090 sgd_solver.cpp:106] Iteration 2200, lr = 1e-05
I0711 22:06:21.282992 13090 solver.cpp:290] Iteration 2300 (5.88725 iter/s, 16.9859s/100 iter), loss = 0.0609007
I0711 22:06:21.283089 13090 solver.cpp:309]     Train net output #0: loss = 0.0609009 (* 1 = 0.0609009 loss)
I0711 22:06:21.283100 13090 sgd_solver.cpp:106] Iteration 2300, lr = 1e-05
I0711 22:06:38.321895 13090 solver.cpp:290] Iteration 2400 (5.86912 iter/s, 17.0383s/100 iter), loss = 0.0347756
I0711 22:06:38.321921 13090 solver.cpp:309]     Train net output #0: loss = 0.0347758 (* 1 = 0.0347758 loss)
I0711 22:06:38.321930 13090 sgd_solver.cpp:106] Iteration 2400, lr = 1e-05
I0711 22:06:55.754662 13090 solver.cpp:290] Iteration 2500 (5.7365 iter/s, 17.4322s/100 iter), loss = 0.0267127
I0711 22:06:55.754865 13090 solver.cpp:309]     Train net output #0: loss = 0.0267129 (* 1 = 0.0267129 loss)
I0711 22:06:55.754904 13090 sgd_solver.cpp:106] Iteration 2500, lr = 1e-05
I0711 22:07:13.010782 13090 solver.cpp:290] Iteration 2600 (5.79526 iter/s, 17.2555s/100 iter), loss = 0.0383124
I0711 22:07:13.010807 13090 solver.cpp:309]     Train net output #0: loss = 0.0383126 (* 1 = 0.0383126 loss)
I0711 22:07:13.010813 13090 sgd_solver.cpp:106] Iteration 2600, lr = 1e-05
I0711 22:07:30.119976 13090 solver.cpp:290] Iteration 2700 (5.84498 iter/s, 17.1087s/100 iter), loss = 0.022857
I0711 22:07:30.120034 13090 solver.cpp:309]     Train net output #0: loss = 0.0228572 (* 1 = 0.0228572 loss)
I0711 22:07:30.120045 13090 sgd_solver.cpp:106] Iteration 2700, lr = 1e-05
I0711 22:07:47.061837 13090 solver.cpp:290] Iteration 2800 (5.90272 iter/s, 16.9413s/100 iter), loss = 0.0561066
I0711 22:07:47.061864 13090 solver.cpp:309]     Train net output #0: loss = 0.0561068 (* 1 = 0.0561068 loss)
I0711 22:07:47.061880 13090 sgd_solver.cpp:106] Iteration 2800, lr = 1e-05
I0711 22:08:04.014853 13090 solver.cpp:290] Iteration 2900 (5.89883 iter/s, 16.9525s/100 iter), loss = 0.0330383
I0711 22:08:04.014909 13090 solver.cpp:309]     Train net output #0: loss = 0.0330384 (* 1 = 0.0330384 loss)
I0711 22:08:04.014917 13090 sgd_solver.cpp:106] Iteration 2900, lr = 1e-05
I0711 22:08:20.660935 13090 solver.cpp:354] Sparsity after update:
I0711 22:08:20.716143 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 22:08:20.716162 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 22:08:20.716174 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 22:08:20.716178 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 22:08:20.716182 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 22:08:20.716187 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 22:08:20.716190 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 22:08:20.716194 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 22:08:20.716198 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 22:08:20.716202 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 22:08:20.716207 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 22:08:20.716212 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 22:08:20.716217 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 22:08:20.716222 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 22:08:20.716225 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 22:08:20.716229 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 22:08:20.716233 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 22:08:20.716243 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 22:08:20.716246 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 22:08:20.865573 13090 solver.cpp:290] Iteration 3000 (5.93465 iter/s, 16.8502s/100 iter), loss = 0.0514176
I0711 22:08:20.865602 13090 solver.cpp:309]     Train net output #0: loss = 0.0514177 (* 1 = 0.0514177 loss)
I0711 22:08:20.865609 13090 sgd_solver.cpp:106] Iteration 3000, lr = 1e-05
I0711 22:08:37.865110 13090 solver.cpp:290] Iteration 3100 (5.88268 iter/s, 16.999s/100 iter), loss = 0.0384769
I0711 22:08:37.865164 13090 solver.cpp:309]     Train net output #0: loss = 0.0384771 (* 1 = 0.0384771 loss)
I0711 22:08:37.865173 13090 sgd_solver.cpp:106] Iteration 3100, lr = 1e-05
I0711 22:08:54.854112 13090 solver.cpp:290] Iteration 3200 (5.88634 iter/s, 16.9885s/100 iter), loss = 0.0411953
I0711 22:08:54.854136 13090 solver.cpp:309]     Train net output #0: loss = 0.0411955 (* 1 = 0.0411955 loss)
I0711 22:08:54.854143 13090 sgd_solver.cpp:106] Iteration 3200, lr = 1e-05
I0711 22:09:11.738801 13090 solver.cpp:290] Iteration 3300 (5.9227 iter/s, 16.8842s/100 iter), loss = 0.0281996
I0711 22:09:11.738880 13090 solver.cpp:309]     Train net output #0: loss = 0.0281998 (* 1 = 0.0281998 loss)
I0711 22:09:11.738899 13090 sgd_solver.cpp:106] Iteration 3300, lr = 1e-05
I0711 22:09:28.680209 13090 solver.cpp:290] Iteration 3400 (5.90288 iter/s, 16.9409s/100 iter), loss = 0.0293484
I0711 22:09:28.680232 13090 solver.cpp:309]     Train net output #0: loss = 0.0293486 (* 1 = 0.0293486 loss)
I0711 22:09:28.680238 13090 sgd_solver.cpp:106] Iteration 3400, lr = 1e-05
I0711 22:09:45.742753 13090 solver.cpp:290] Iteration 3500 (5.86096 iter/s, 17.0621s/100 iter), loss = 0.0522294
I0711 22:09:45.742859 13090 solver.cpp:309]     Train net output #0: loss = 0.0522296 (* 1 = 0.0522296 loss)
I0711 22:09:45.742869 13090 sgd_solver.cpp:106] Iteration 3500, lr = 1e-05
I0711 22:10:02.793740 13090 solver.cpp:290] Iteration 3600 (5.86496 iter/s, 17.0504s/100 iter), loss = 0.0223704
I0711 22:10:02.793763 13090 solver.cpp:309]     Train net output #0: loss = 0.0223706 (* 1 = 0.0223706 loss)
I0711 22:10:02.793771 13090 sgd_solver.cpp:106] Iteration 3600, lr = 1e-05
I0711 22:10:19.745550 13090 solver.cpp:290] Iteration 3700 (5.89924 iter/s, 16.9513s/100 iter), loss = 0.0241995
I0711 22:10:19.745597 13090 solver.cpp:309]     Train net output #0: loss = 0.0241997 (* 1 = 0.0241997 loss)
I0711 22:10:19.745604 13090 sgd_solver.cpp:106] Iteration 3700, lr = 1e-05
I0711 22:10:36.913959 13090 solver.cpp:290] Iteration 3800 (5.82483 iter/s, 17.1679s/100 iter), loss = 0.0321409
I0711 22:10:36.913981 13090 solver.cpp:309]     Train net output #0: loss = 0.0321411 (* 1 = 0.0321411 loss)
I0711 22:10:36.913990 13090 sgd_solver.cpp:106] Iteration 3800, lr = 1e-05
I0711 22:10:53.852423 13090 solver.cpp:290] Iteration 3900 (5.90389 iter/s, 16.938s/100 iter), loss = 0.0388618
I0711 22:10:53.852493 13090 solver.cpp:309]     Train net output #0: loss = 0.038862 (* 1 = 0.038862 loss)
I0711 22:10:53.852500 13090 sgd_solver.cpp:106] Iteration 3900, lr = 1e-05
I0711 22:11:10.703076 13090 solver.cpp:354] Sparsity after update:
I0711 22:11:10.705353 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 22:11:10.705361 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 22:11:10.705370 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 22:11:10.705375 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 22:11:10.705380 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 22:11:10.705384 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 22:11:10.705387 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 22:11:10.705392 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 22:11:10.705396 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 22:11:10.705400 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 22:11:10.705404 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 22:11:10.705409 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 22:11:10.705412 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 22:11:10.705416 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 22:11:10.705420 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 22:11:10.705425 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 22:11:10.705428 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 22:11:10.705433 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 22:11:10.705437 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 22:11:10.705577 13090 solver.cpp:467] Iteration 4000, Testing net (#0)
I0711 22:11:57.570858 13090 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.946874
I0711 22:11:57.570976 13090 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999814
I0711 22:11:57.570983 13090 solver.cpp:540]     Test net output #2: loss = 0.13827 (* 1 = 0.13827 loss)
I0711 22:11:57.771574 13090 solver.cpp:290] Iteration 4000 (1.56452 iter/s, 63.9173s/100 iter), loss = 0.0226881
I0711 22:11:57.771600 13090 solver.cpp:309]     Train net output #0: loss = 0.0226883 (* 1 = 0.0226883 loss)
I0711 22:11:57.771613 13090 sgd_solver.cpp:106] Iteration 4000, lr = 1e-05
I0711 22:12:23.962608 13090 solver.cpp:290] Iteration 4100 (3.81821 iter/s, 26.1903s/100 iter), loss = 0.0608795
I0711 22:12:23.962631 13090 solver.cpp:309]     Train net output #0: loss = 0.0608797 (* 1 = 0.0608797 loss)
I0711 22:12:23.962638 13090 sgd_solver.cpp:106] Iteration 4100, lr = 1e-05
I0711 22:12:27.619036 13199 blocking_queue.cpp:50] Waiting for data
I0711 22:13:20.395987 13090 solver.cpp:290] Iteration 4200 (1.77205 iter/s, 56.4318s/100 iter), loss = 0.0321732
I0711 22:13:20.396040 13090 solver.cpp:309]     Train net output #0: loss = 0.0321733 (* 1 = 0.0321733 loss)
I0711 22:13:20.396049 13090 sgd_solver.cpp:106] Iteration 4200, lr = 1e-05
I0711 22:13:39.091428 13090 solver.cpp:290] Iteration 4300 (5.34906 iter/s, 18.6949s/100 iter), loss = 0.0460296
I0711 22:13:39.091452 13090 solver.cpp:309]     Train net output #0: loss = 0.0460298 (* 1 = 0.0460298 loss)
I0711 22:13:39.091459 13090 sgd_solver.cpp:106] Iteration 4300, lr = 1e-05
I0711 22:13:56.035197 13090 solver.cpp:290] Iteration 4400 (5.90205 iter/s, 16.9433s/100 iter), loss = 0.0748695
I0711 22:13:56.035248 13090 solver.cpp:309]     Train net output #0: loss = 0.0748696 (* 1 = 0.0748696 loss)
I0711 22:13:56.035256 13090 sgd_solver.cpp:106] Iteration 4400, lr = 1e-05
I0711 22:14:12.984359 13090 solver.cpp:290] Iteration 4500 (5.90018 iter/s, 16.9486s/100 iter), loss = 0.0299632
I0711 22:14:12.984383 13090 solver.cpp:309]     Train net output #0: loss = 0.0299634 (* 1 = 0.0299634 loss)
I0711 22:14:12.984392 13090 sgd_solver.cpp:106] Iteration 4500, lr = 1e-05
I0711 22:14:30.049432 13090 solver.cpp:290] Iteration 4600 (5.86009 iter/s, 17.0646s/100 iter), loss = 0.0309133
I0711 22:14:30.049479 13090 solver.cpp:309]     Train net output #0: loss = 0.0309134 (* 1 = 0.0309134 loss)
I0711 22:14:30.049487 13090 sgd_solver.cpp:106] Iteration 4600, lr = 1e-05
I0711 22:14:47.066073 13090 solver.cpp:290] Iteration 4700 (5.87678 iter/s, 17.0161s/100 iter), loss = 0.0956195
I0711 22:14:47.066097 13090 solver.cpp:309]     Train net output #0: loss = 0.0956197 (* 1 = 0.0956197 loss)
I0711 22:14:47.066103 13090 sgd_solver.cpp:106] Iteration 4700, lr = 1e-05
I0711 22:15:04.170584 13090 solver.cpp:290] Iteration 4800 (5.84658 iter/s, 17.104s/100 iter), loss = 0.0599974
I0711 22:15:04.170636 13090 solver.cpp:309]     Train net output #0: loss = 0.0599976 (* 1 = 0.0599976 loss)
I0711 22:15:04.170644 13090 sgd_solver.cpp:106] Iteration 4800, lr = 1e-05
I0711 22:15:21.184660 13090 solver.cpp:290] Iteration 4900 (5.87767 iter/s, 17.0136s/100 iter), loss = 0.0397377
I0711 22:15:21.184687 13090 solver.cpp:309]     Train net output #0: loss = 0.0397379 (* 1 = 0.0397379 loss)
I0711 22:15:21.184696 13090 sgd_solver.cpp:106] Iteration 4900, lr = 1e-05
I0711 22:15:38.024374 13090 solver.cpp:354] Sparsity after update:
I0711 22:15:38.076504 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 22:15:38.076520 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 22:15:38.076529 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 22:15:38.076530 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 22:15:38.076532 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 22:15:38.076534 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 22:15:38.076536 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 22:15:38.076539 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 22:15:38.076540 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 22:15:38.076542 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 22:15:38.076545 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 22:15:38.076547 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 22:15:38.076550 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 22:15:38.076550 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 22:15:38.076552 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 22:15:38.076555 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 22:15:38.076556 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 22:15:38.076558 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 22:15:38.076560 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 22:15:38.227149 13090 solver.cpp:290] Iteration 5000 (5.86786 iter/s, 17.042s/100 iter), loss = 0.021821
I0711 22:15:38.227171 13090 solver.cpp:309]     Train net output #0: loss = 0.0218212 (* 1 = 0.0218212 loss)
I0711 22:15:38.227179 13090 sgd_solver.cpp:106] Iteration 5000, lr = 1e-05
I0711 22:15:55.180563 13090 solver.cpp:290] Iteration 5100 (5.89869 iter/s, 16.9529s/100 iter), loss = 0.0410414
I0711 22:15:55.180588 13090 solver.cpp:309]     Train net output #0: loss = 0.0410416 (* 1 = 0.0410416 loss)
I0711 22:15:55.180594 13090 sgd_solver.cpp:106] Iteration 5100, lr = 1e-05
I0711 22:16:12.060828 13090 solver.cpp:290] Iteration 5200 (5.92425 iter/s, 16.8798s/100 iter), loss = 0.0207933
I0711 22:16:12.060899 13090 solver.cpp:309]     Train net output #0: loss = 0.0207935 (* 1 = 0.0207935 loss)
I0711 22:16:12.060909 13090 sgd_solver.cpp:106] Iteration 5200, lr = 1e-05
I0711 22:16:29.034729 13090 solver.cpp:290] Iteration 5300 (5.89158 iter/s, 16.9734s/100 iter), loss = 0.0301292
I0711 22:16:29.034752 13090 solver.cpp:309]     Train net output #0: loss = 0.0301294 (* 1 = 0.0301294 loss)
I0711 22:16:29.034759 13090 sgd_solver.cpp:106] Iteration 5300, lr = 1e-05
I0711 22:16:46.002617 13090 solver.cpp:290] Iteration 5400 (5.89365 iter/s, 16.9674s/100 iter), loss = 0.0206035
I0711 22:16:46.002709 13090 solver.cpp:309]     Train net output #0: loss = 0.0206037 (* 1 = 0.0206037 loss)
I0711 22:16:46.002722 13090 sgd_solver.cpp:106] Iteration 5400, lr = 1e-05
I0711 22:17:03.035387 13090 solver.cpp:290] Iteration 5500 (5.87123 iter/s, 17.0322s/100 iter), loss = 0.0284124
I0711 22:17:03.035409 13090 solver.cpp:309]     Train net output #0: loss = 0.0284126 (* 1 = 0.0284126 loss)
I0711 22:17:03.035416 13090 sgd_solver.cpp:106] Iteration 5500, lr = 1e-05
I0711 22:17:20.002794 13090 solver.cpp:290] Iteration 5600 (5.89382 iter/s, 16.9669s/100 iter), loss = 0.0523131
I0711 22:17:20.002887 13090 solver.cpp:309]     Train net output #0: loss = 0.0523133 (* 1 = 0.0523133 loss)
I0711 22:17:20.002898 13090 sgd_solver.cpp:106] Iteration 5600, lr = 1e-05
I0711 22:17:37.129174 13090 solver.cpp:290] Iteration 5700 (5.83914 iter/s, 17.1258s/100 iter), loss = 0.0325364
I0711 22:17:37.129197 13090 solver.cpp:309]     Train net output #0: loss = 0.0325366 (* 1 = 0.0325366 loss)
I0711 22:17:37.129204 13090 sgd_solver.cpp:106] Iteration 5700, lr = 1e-05
I0711 22:17:54.291031 13090 solver.cpp:290] Iteration 5800 (5.82704 iter/s, 17.1614s/100 iter), loss = 0.0557424
I0711 22:17:54.291076 13090 solver.cpp:309]     Train net output #0: loss = 0.0557427 (* 1 = 0.0557427 loss)
I0711 22:17:54.291085 13090 sgd_solver.cpp:106] Iteration 5800, lr = 1e-05
I0711 22:18:11.377801 13090 solver.cpp:290] Iteration 5900 (5.85266 iter/s, 17.0863s/100 iter), loss = 0.037098
I0711 22:18:11.377825 13090 solver.cpp:309]     Train net output #0: loss = 0.0370982 (* 1 = 0.0370982 loss)
I0711 22:18:11.377832 13090 sgd_solver.cpp:106] Iteration 5900, lr = 1e-05
I0711 22:18:28.220989 13090 solver.cpp:354] Sparsity after update:
I0711 22:18:28.222954 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 22:18:28.222961 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 22:18:28.222970 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 22:18:28.222971 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 22:18:28.222973 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 22:18:28.222975 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 22:18:28.222977 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 22:18:28.222980 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 22:18:28.222980 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 22:18:28.222982 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 22:18:28.222985 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 22:18:28.222986 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 22:18:28.222990 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 22:18:28.222990 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 22:18:28.222992 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 22:18:28.222995 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 22:18:28.222996 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 22:18:28.222998 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 22:18:28.223001 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 22:18:28.223136 13090 solver.cpp:467] Iteration 6000, Testing net (#0)
I0711 22:18:32.728492 13176 blocking_queue.cpp:50] Waiting for data
I0711 22:19:18.094111 13090 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.947429
I0711 22:19:18.094260 13090 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999626
I0711 22:19:18.094270 13090 solver.cpp:540]     Test net output #2: loss = 0.146325 (* 1 = 0.146325 loss)
I0711 22:19:18.279904 13090 solver.cpp:290] Iteration 6000 (1.49476 iter/s, 66.9003s/100 iter), loss = 0.0333571
I0711 22:19:18.279927 13090 solver.cpp:309]     Train net output #0: loss = 0.0333574 (* 1 = 0.0333574 loss)
I0711 22:19:18.279934 13090 sgd_solver.cpp:106] Iteration 6000, lr = 1e-05
I0711 22:19:36.883790 13090 solver.cpp:290] Iteration 6100 (5.37538 iter/s, 18.6033s/100 iter), loss = 0.0347468
I0711 22:19:36.883813 13090 solver.cpp:309]     Train net output #0: loss = 0.0347471 (* 1 = 0.0347471 loss)
I0711 22:19:36.883819 13090 sgd_solver.cpp:106] Iteration 6100, lr = 1e-05
I0711 22:20:00.619102 13090 solver.cpp:290] Iteration 6200 (4.21325 iter/s, 23.7346s/100 iter), loss = 0.0352523
I0711 22:20:00.619182 13090 solver.cpp:309]     Train net output #0: loss = 0.0352526 (* 1 = 0.0352526 loss)
I0711 22:20:00.619190 13090 sgd_solver.cpp:106] Iteration 6200, lr = 1e-05
I0711 22:20:17.665707 13090 solver.cpp:290] Iteration 6300 (5.86646 iter/s, 17.0461s/100 iter), loss = 0.0860286
I0711 22:20:17.665730 13090 solver.cpp:309]     Train net output #0: loss = 0.0860288 (* 1 = 0.0860288 loss)
I0711 22:20:17.665737 13090 sgd_solver.cpp:106] Iteration 6300, lr = 1e-05
I0711 22:20:34.569309 13090 solver.cpp:290] Iteration 6400 (5.91607 iter/s, 16.9031s/100 iter), loss = 0.0417691
I0711 22:20:34.569363 13090 solver.cpp:309]     Train net output #0: loss = 0.0417694 (* 1 = 0.0417694 loss)
I0711 22:20:34.569371 13090 sgd_solver.cpp:106] Iteration 6400, lr = 1e-05
I0711 22:20:51.372922 13090 solver.cpp:290] Iteration 6500 (5.95128 iter/s, 16.8031s/100 iter), loss = 0.0667606
I0711 22:20:51.372953 13090 solver.cpp:309]     Train net output #0: loss = 0.0667608 (* 1 = 0.0667608 loss)
I0711 22:20:51.372961 13090 sgd_solver.cpp:106] Iteration 6500, lr = 1e-05
I0711 22:21:08.260952 13090 solver.cpp:290] Iteration 6600 (5.92153 iter/s, 16.8875s/100 iter), loss = 0.0244727
I0711 22:21:08.261036 13090 solver.cpp:309]     Train net output #0: loss = 0.0244729 (* 1 = 0.0244729 loss)
I0711 22:21:08.261044 13090 sgd_solver.cpp:106] Iteration 6600, lr = 1e-05
I0711 22:21:25.198943 13090 solver.cpp:290] Iteration 6700 (5.90408 iter/s, 16.9374s/100 iter), loss = 0.018781
I0711 22:21:25.198968 13090 solver.cpp:309]     Train net output #0: loss = 0.0187813 (* 1 = 0.0187813 loss)
I0711 22:21:25.198976 13090 sgd_solver.cpp:106] Iteration 6700, lr = 1e-05
I0711 22:21:42.311765 13090 solver.cpp:290] Iteration 6800 (5.84374 iter/s, 17.1123s/100 iter), loss = 0.0331572
I0711 22:21:42.311862 13090 solver.cpp:309]     Train net output #0: loss = 0.0331574 (* 1 = 0.0331574 loss)
I0711 22:21:42.311873 13090 sgd_solver.cpp:106] Iteration 6800, lr = 1e-05
I0711 22:21:59.478801 13090 solver.cpp:290] Iteration 6900 (5.82531 iter/s, 17.1665s/100 iter), loss = 0.0452859
I0711 22:21:59.478857 13090 solver.cpp:309]     Train net output #0: loss = 0.0452861 (* 1 = 0.0452861 loss)
I0711 22:21:59.478876 13090 sgd_solver.cpp:106] Iteration 6900, lr = 1e-05
I0711 22:22:17.830461 13090 solver.cpp:354] Sparsity after update:
I0711 22:22:17.881878 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 22:22:17.881893 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 22:22:17.881901 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 22:22:17.881903 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 22:22:17.881906 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 22:22:17.881907 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 22:22:17.881909 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 22:22:17.881911 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 22:22:17.881913 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 22:22:17.881916 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 22:22:17.881918 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 22:22:17.881922 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 22:22:17.881923 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 22:22:17.881925 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 22:22:17.881927 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 22:22:17.881929 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 22:22:17.881932 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 22:22:17.881934 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 22:22:17.881937 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 22:22:18.032176 13090 solver.cpp:290] Iteration 7000 (5.39002 iter/s, 18.5528s/100 iter), loss = 0.0239244
I0711 22:22:18.032202 13090 solver.cpp:309]     Train net output #0: loss = 0.0239246 (* 1 = 0.0239246 loss)
I0711 22:22:18.032208 13090 sgd_solver.cpp:106] Iteration 7000, lr = 1e-05
I0711 22:22:35.348201 13090 solver.cpp:290] Iteration 7100 (5.77516 iter/s, 17.3155s/100 iter), loss = 0.0490725
I0711 22:22:35.348227 13090 solver.cpp:309]     Train net output #0: loss = 0.0490727 (* 1 = 0.0490727 loss)
I0711 22:22:35.348232 13090 sgd_solver.cpp:106] Iteration 7100, lr = 1e-05
I0711 22:22:52.558437 13090 solver.cpp:290] Iteration 7200 (5.81066 iter/s, 17.2097s/100 iter), loss = 0.0219998
I0711 22:22:52.558502 13090 solver.cpp:309]     Train net output #0: loss = 0.022 (* 1 = 0.022 loss)
I0711 22:22:52.558513 13090 sgd_solver.cpp:106] Iteration 7200, lr = 1e-05
I0711 22:23:09.711829 13090 solver.cpp:290] Iteration 7300 (5.82993 iter/s, 17.1529s/100 iter), loss = 0.0394309
I0711 22:23:09.711858 13090 solver.cpp:309]     Train net output #0: loss = 0.0394312 (* 1 = 0.0394312 loss)
I0711 22:23:09.711868 13090 sgd_solver.cpp:106] Iteration 7300, lr = 1e-05
I0711 22:23:26.944391 13090 solver.cpp:290] Iteration 7400 (5.80313 iter/s, 17.2321s/100 iter), loss = 0.0497859
I0711 22:23:26.944432 13090 solver.cpp:309]     Train net output #0: loss = 0.0497862 (* 1 = 0.0497862 loss)
I0711 22:23:26.944440 13090 sgd_solver.cpp:106] Iteration 7400, lr = 1e-05
I0711 22:23:44.069950 13090 solver.cpp:290] Iteration 7500 (5.8394 iter/s, 17.1251s/100 iter), loss = 0.0447849
I0711 22:23:44.069973 13090 solver.cpp:309]     Train net output #0: loss = 0.0447852 (* 1 = 0.0447852 loss)
I0711 22:23:44.069980 13090 sgd_solver.cpp:106] Iteration 7500, lr = 1e-05
I0711 22:24:01.151614 13090 solver.cpp:290] Iteration 7600 (5.8544 iter/s, 17.0812s/100 iter), loss = 0.0385858
I0711 22:24:01.151695 13090 solver.cpp:309]     Train net output #0: loss = 0.038586 (* 1 = 0.038586 loss)
I0711 22:24:01.151701 13090 sgd_solver.cpp:106] Iteration 7600, lr = 1e-05
I0711 22:24:18.389837 13090 solver.cpp:290] Iteration 7700 (5.80125 iter/s, 17.2377s/100 iter), loss = 0.0270684
I0711 22:24:18.389866 13090 solver.cpp:309]     Train net output #0: loss = 0.0270686 (* 1 = 0.0270686 loss)
I0711 22:24:18.389874 13090 sgd_solver.cpp:106] Iteration 7700, lr = 1e-05
I0711 22:24:35.562696 13090 solver.cpp:290] Iteration 7800 (5.82331 iter/s, 17.1724s/100 iter), loss = 0.0191226
I0711 22:24:35.562832 13090 solver.cpp:309]     Train net output #0: loss = 0.0191229 (* 1 = 0.0191229 loss)
I0711 22:24:35.562841 13090 sgd_solver.cpp:106] Iteration 7800, lr = 1e-05
I0711 22:24:52.823011 13090 solver.cpp:290] Iteration 7900 (5.79384 iter/s, 17.2597s/100 iter), loss = 0.0418846
I0711 22:24:52.823043 13090 solver.cpp:309]     Train net output #0: loss = 0.0418849 (* 1 = 0.0418849 loss)
I0711 22:24:52.823053 13090 sgd_solver.cpp:106] Iteration 7900, lr = 1e-05
I0711 22:25:09.832254 13090 solver.cpp:354] Sparsity after update:
I0711 22:25:09.834468 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 22:25:09.834477 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 22:25:09.834484 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 22:25:09.834486 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 22:25:09.834488 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 22:25:09.834491 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 22:25:09.834492 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 22:25:09.834494 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 22:25:09.834496 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 22:25:09.834498 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 22:25:09.834501 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 22:25:09.834502 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 22:25:09.834504 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 22:25:09.834506 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 22:25:09.834508 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 22:25:09.834511 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 22:25:09.834512 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 22:25:09.834514 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 22:25:09.834517 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 22:25:09.834656 13090 solver.cpp:467] Iteration 8000, Testing net (#0)
I0711 22:25:59.378186 13090 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.948537
I0711 22:25:59.378284 13090 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.99976
I0711 22:25:59.378291 13090 solver.cpp:540]     Test net output #2: loss = 0.141758 (* 1 = 0.141758 loss)
I0711 22:25:59.574710 13090 solver.cpp:290] Iteration 8000 (1.49813 iter/s, 66.7499s/100 iter), loss = 0.0480269
I0711 22:25:59.574738 13090 solver.cpp:309]     Train net output #0: loss = 0.0480271 (* 1 = 0.0480271 loss)
I0711 22:25:59.574744 13090 sgd_solver.cpp:106] Iteration 8000, lr = 1e-05
I0711 22:26:21.698289 13090 solver.cpp:290] Iteration 8100 (4.52019 iter/s, 22.1229s/100 iter), loss = 0.0378518
I0711 22:26:21.698318 13090 solver.cpp:309]     Train net output #0: loss = 0.037852 (* 1 = 0.037852 loss)
I0711 22:26:21.698324 13090 sgd_solver.cpp:106] Iteration 8100, lr = 1e-05
I0711 22:27:03.250861 13227 blocking_queue.cpp:50] Waiting for data
I0711 22:27:12.591315 13090 solver.cpp:290] Iteration 8200 (1.96496 iter/s, 50.8916s/100 iter), loss = 0.0369764
I0711 22:27:12.591344 13090 solver.cpp:309]     Train net output #0: loss = 0.0369767 (* 1 = 0.0369767 loss)
I0711 22:27:12.591351 13090 sgd_solver.cpp:106] Iteration 8200, lr = 1e-05
I0711 22:27:29.834616 13090 solver.cpp:290] Iteration 8300 (5.79952 iter/s, 17.2428s/100 iter), loss = 0.0347742
I0711 22:27:29.834642 13090 solver.cpp:309]     Train net output #0: loss = 0.0347744 (* 1 = 0.0347744 loss)
I0711 22:27:29.834650 13090 sgd_solver.cpp:106] Iteration 8300, lr = 1e-05
I0711 22:27:47.079068 13090 solver.cpp:290] Iteration 8400 (5.79914 iter/s, 17.2439s/100 iter), loss = 0.0219216
I0711 22:27:47.079157 13090 solver.cpp:309]     Train net output #0: loss = 0.0219219 (* 1 = 0.0219219 loss)
I0711 22:27:47.079169 13090 sgd_solver.cpp:106] Iteration 8400, lr = 1e-05
I0711 22:28:04.040101 13090 solver.cpp:290] Iteration 8500 (5.89606 iter/s, 16.9605s/100 iter), loss = 0.0627958
I0711 22:28:04.040127 13090 solver.cpp:309]     Train net output #0: loss = 0.062796 (* 1 = 0.062796 loss)
I0711 22:28:04.040135 13090 sgd_solver.cpp:106] Iteration 8500, lr = 1e-05
I0711 22:28:21.343745 13090 solver.cpp:290] Iteration 8600 (5.7793 iter/s, 17.3031s/100 iter), loss = 0.0721623
I0711 22:28:21.343817 13090 solver.cpp:309]     Train net output #0: loss = 0.0721625 (* 1 = 0.0721625 loss)
I0711 22:28:21.343825 13090 sgd_solver.cpp:106] Iteration 8600, lr = 1e-05
I0711 22:28:38.650297 13090 solver.cpp:290] Iteration 8700 (5.77834 iter/s, 17.306s/100 iter), loss = 0.0345917
I0711 22:28:38.650321 13090 solver.cpp:309]     Train net output #0: loss = 0.034592 (* 1 = 0.034592 loss)
I0711 22:28:38.650328 13090 sgd_solver.cpp:106] Iteration 8700, lr = 1e-05
I0711 22:28:55.805294 13090 solver.cpp:290] Iteration 8800 (5.82938 iter/s, 17.1545s/100 iter), loss = 0.0299143
I0711 22:28:55.805377 13090 solver.cpp:309]     Train net output #0: loss = 0.0299146 (* 1 = 0.0299146 loss)
I0711 22:28:55.805390 13090 sgd_solver.cpp:106] Iteration 8800, lr = 1e-05
I0711 22:29:12.938206 13090 solver.cpp:290] Iteration 8900 (5.83691 iter/s, 17.1324s/100 iter), loss = 0.0379329
I0711 22:29:12.938230 13090 solver.cpp:309]     Train net output #0: loss = 0.0379331 (* 1 = 0.0379331 loss)
I0711 22:29:12.938237 13090 sgd_solver.cpp:106] Iteration 8900, lr = 1e-05
I0711 22:29:29.881036 13090 solver.cpp:354] Sparsity after update:
I0711 22:29:29.936976 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 22:29:29.936992 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 22:29:29.937000 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 22:29:29.937001 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 22:29:29.937003 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 22:29:29.937005 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 22:29:29.937007 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 22:29:29.937010 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 22:29:29.937011 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 22:29:29.937013 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 22:29:29.937016 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 22:29:29.937017 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 22:29:29.937019 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 22:29:29.937021 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 22:29:29.937023 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 22:29:29.937026 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 22:29:29.937027 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 22:29:29.937029 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 22:29:29.937031 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 22:29:30.086614 13090 solver.cpp:290] Iteration 9000 (5.83161 iter/s, 17.1479s/100 iter), loss = 0.0320511
I0711 22:29:30.086638 13090 solver.cpp:309]     Train net output #0: loss = 0.0320513 (* 1 = 0.0320513 loss)
I0711 22:29:30.086645 13090 sgd_solver.cpp:106] Iteration 9000, lr = 1e-05
I0711 22:29:47.300457 13090 solver.cpp:290] Iteration 9100 (5.80945 iter/s, 17.2133s/100 iter), loss = 0.0479693
I0711 22:29:47.300479 13090 solver.cpp:309]     Train net output #0: loss = 0.0479695 (* 1 = 0.0479695 loss)
I0711 22:29:47.300487 13090 sgd_solver.cpp:106] Iteration 9100, lr = 1e-05
I0711 22:30:04.405527 13090 solver.cpp:290] Iteration 9200 (5.84639 iter/s, 17.1046s/100 iter), loss = 0.0392525
I0711 22:30:04.405576 13090 solver.cpp:309]     Train net output #0: loss = 0.0392527 (* 1 = 0.0392527 loss)
I0711 22:30:04.405583 13090 sgd_solver.cpp:106] Iteration 9200, lr = 1e-05
I0711 22:30:21.415416 13090 solver.cpp:290] Iteration 9300 (5.87911 iter/s, 17.0094s/100 iter), loss = 0.0433125
I0711 22:30:21.415441 13090 solver.cpp:309]     Train net output #0: loss = 0.0433127 (* 1 = 0.0433127 loss)
I0711 22:30:21.415447 13090 sgd_solver.cpp:106] Iteration 9300, lr = 1e-05
I0711 22:30:38.517249 13090 solver.cpp:290] Iteration 9400 (5.8475 iter/s, 17.1013s/100 iter), loss = 0.0221018
I0711 22:30:38.517343 13090 solver.cpp:309]     Train net output #0: loss = 0.0221021 (* 1 = 0.0221021 loss)
I0711 22:30:38.517351 13090 sgd_solver.cpp:106] Iteration 9400, lr = 1e-05
I0711 22:30:55.558162 13090 solver.cpp:290] Iteration 9500 (5.86842 iter/s, 17.0404s/100 iter), loss = 0.0413281
I0711 22:30:55.558187 13090 solver.cpp:309]     Train net output #0: loss = 0.0413283 (* 1 = 0.0413283 loss)
I0711 22:30:55.558192 13090 sgd_solver.cpp:106] Iteration 9500, lr = 1e-05
I0711 22:31:12.597360 13090 solver.cpp:290] Iteration 9600 (5.86899 iter/s, 17.0387s/100 iter), loss = 0.0340849
I0711 22:31:12.597488 13090 solver.cpp:309]     Train net output #0: loss = 0.0340851 (* 1 = 0.0340851 loss)
I0711 22:31:12.597498 13090 sgd_solver.cpp:106] Iteration 9600, lr = 1e-05
I0711 22:31:29.751783 13090 solver.cpp:290] Iteration 9700 (5.8296 iter/s, 17.1538s/100 iter), loss = 0.0332697
I0711 22:31:29.751806 13090 solver.cpp:309]     Train net output #0: loss = 0.0332699 (* 1 = 0.0332699 loss)
I0711 22:31:29.751813 13090 sgd_solver.cpp:106] Iteration 9700, lr = 1e-05
I0711 22:31:46.865011 13090 solver.cpp:290] Iteration 9800 (5.8436 iter/s, 17.1127s/100 iter), loss = 0.0252254
I0711 22:31:46.865068 13090 solver.cpp:309]     Train net output #0: loss = 0.0252256 (* 1 = 0.0252256 loss)
I0711 22:31:46.865079 13090 sgd_solver.cpp:106] Iteration 9800, lr = 1e-05
I0711 22:32:04.034013 13090 solver.cpp:290] Iteration 9900 (5.82463 iter/s, 17.1685s/100 iter), loss = 0.0238339
I0711 22:32:04.034039 13090 solver.cpp:309]     Train net output #0: loss = 0.0238341 (* 1 = 0.0238341 loss)
I0711 22:32:04.034047 13090 sgd_solver.cpp:106] Iteration 9900, lr = 1e-05
I0711 22:32:20.982661 13090 solver.cpp:594] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/sparse/cityscapes5_jsegnet21v2_iter_10000.caffemodel
I0711 22:32:21.130033 13090 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/sparse/cityscapes5_jsegnet21v2_iter_10000.solverstate
I0711 22:32:21.147176 13090 solver.cpp:354] Sparsity after update:
I0711 22:32:21.148767 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 22:32:21.148775 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 22:32:21.148782 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 22:32:21.148784 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 22:32:21.148787 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 22:32:21.148788 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 22:32:21.148790 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 22:32:21.148792 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 22:32:21.148794 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 22:32:21.148797 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 22:32:21.148798 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 22:32:21.148800 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 22:32:21.148802 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 22:32:21.148804 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 22:32:21.148805 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 22:32:21.148808 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 22:32:21.148809 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 22:32:21.148813 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 22:32:21.148826 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 22:32:21.148972 13090 solver.cpp:467] Iteration 10000, Testing net (#0)
I0711 22:33:10.022608 13090 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.949002
I0711 22:33:10.023175 13090 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999482
I0711 22:33:10.023187 13090 solver.cpp:540]     Test net output #2: loss = 0.150472 (* 1 = 0.150472 loss)
I0711 22:33:10.208782 13090 solver.cpp:290] Iteration 10000 (1.51119 iter/s, 66.1729s/100 iter), loss = 0.044427
I0711 22:33:10.208806 13090 solver.cpp:309]     Train net output #0: loss = 0.0444272 (* 1 = 0.0444272 loss)
I0711 22:33:10.208812 13090 sgd_solver.cpp:106] Iteration 10000, lr = 1e-05
I0711 22:33:27.442476 13090 solver.cpp:290] Iteration 10100 (5.80276 iter/s, 17.2332s/100 iter), loss = 0.035033
I0711 22:33:27.442505 13090 solver.cpp:309]     Train net output #0: loss = 0.0350332 (* 1 = 0.0350332 loss)
I0711 22:33:27.442515 13090 sgd_solver.cpp:106] Iteration 10100, lr = 1e-05
I0711 22:33:44.759806 13090 solver.cpp:290] Iteration 10200 (5.77473 iter/s, 17.3168s/100 iter), loss = 0.0205332
I0711 22:33:44.759907 13090 solver.cpp:309]     Train net output #0: loss = 0.0205334 (* 1 = 0.0205334 loss)
I0711 22:33:44.759914 13090 sgd_solver.cpp:106] Iteration 10200, lr = 1e-05
I0711 22:34:02.009871 13090 solver.cpp:290] Iteration 10300 (5.79727 iter/s, 17.2495s/100 iter), loss = 0.0360116
I0711 22:34:02.009899 13090 solver.cpp:309]     Train net output #0: loss = 0.0360118 (* 1 = 0.0360118 loss)
I0711 22:34:02.009917 13090 sgd_solver.cpp:106] Iteration 10300, lr = 1e-05
I0711 22:34:19.376343 13090 solver.cpp:290] Iteration 10400 (5.75839 iter/s, 17.366s/100 iter), loss = 0.0505369
I0711 22:34:19.376446 13090 solver.cpp:309]     Train net output #0: loss = 0.050537 (* 1 = 0.050537 loss)
I0711 22:34:19.376453 13090 sgd_solver.cpp:106] Iteration 10400, lr = 1e-05
I0711 22:34:36.584698 13090 solver.cpp:290] Iteration 10500 (5.81133 iter/s, 17.2078s/100 iter), loss = 0.010596
I0711 22:34:36.584728 13090 solver.cpp:309]     Train net output #0: loss = 0.0105962 (* 1 = 0.0105962 loss)
I0711 22:34:36.584738 13090 sgd_solver.cpp:106] Iteration 10500, lr = 1e-05
I0711 22:34:53.763732 13090 solver.cpp:290] Iteration 10600 (5.82122 iter/s, 17.1785s/100 iter), loss = 0.0299925
I0711 22:34:53.763811 13090 solver.cpp:309]     Train net output #0: loss = 0.0299927 (* 1 = 0.0299927 loss)
I0711 22:34:53.763819 13090 sgd_solver.cpp:106] Iteration 10600, lr = 1e-05
I0711 22:35:10.791610 13090 solver.cpp:290] Iteration 10700 (5.87291 iter/s, 17.0273s/100 iter), loss = 0.0292596
I0711 22:35:10.791633 13090 solver.cpp:309]     Train net output #0: loss = 0.0292598 (* 1 = 0.0292598 loss)
I0711 22:35:10.791640 13090 sgd_solver.cpp:106] Iteration 10700, lr = 1e-05
I0711 22:35:27.820221 13090 solver.cpp:290] Iteration 10800 (5.87264 iter/s, 17.0281s/100 iter), loss = 0.0262356
I0711 22:35:27.820315 13090 solver.cpp:309]     Train net output #0: loss = 0.0262358 (* 1 = 0.0262358 loss)
I0711 22:35:27.820328 13090 sgd_solver.cpp:106] Iteration 10800, lr = 1e-05
I0711 22:35:44.790834 13090 solver.cpp:290] Iteration 10900 (5.89273 iter/s, 16.9701s/100 iter), loss = 0.0247454
I0711 22:35:44.790858 13090 solver.cpp:309]     Train net output #0: loss = 0.0247456 (* 1 = 0.0247456 loss)
I0711 22:35:44.790866 13090 sgd_solver.cpp:106] Iteration 10900, lr = 1e-05
I0711 22:36:01.632617 13090 solver.cpp:354] Sparsity after update:
I0711 22:36:01.694007 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 22:36:01.694025 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 22:36:01.694032 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 22:36:01.694034 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 22:36:01.694036 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 22:36:01.694038 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 22:36:01.694041 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 22:36:01.694042 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 22:36:01.694044 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 22:36:01.694046 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 22:36:01.694048 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 22:36:01.694051 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 22:36:01.694053 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 22:36:01.694056 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 22:36:01.694057 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 22:36:01.694059 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 22:36:01.694061 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 22:36:01.694062 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 22:36:01.694064 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 22:36:01.844369 13090 solver.cpp:290] Iteration 11000 (5.86406 iter/s, 17.053s/100 iter), loss = 0.023616
I0711 22:36:01.844393 13090 solver.cpp:309]     Train net output #0: loss = 0.0236162 (* 1 = 0.0236162 loss)
I0711 22:36:01.844399 13090 sgd_solver.cpp:106] Iteration 11000, lr = 1e-05
I0711 22:36:19.004046 13090 solver.cpp:290] Iteration 11100 (5.82778 iter/s, 17.1592s/100 iter), loss = 0.0207858
I0711 22:36:19.004068 13090 solver.cpp:309]     Train net output #0: loss = 0.0207859 (* 1 = 0.0207859 loss)
I0711 22:36:19.004076 13090 sgd_solver.cpp:106] Iteration 11100, lr = 1e-05
I0711 22:36:36.180647 13090 solver.cpp:290] Iteration 11200 (5.82204 iter/s, 17.1761s/100 iter), loss = 0.0366037
I0711 22:36:36.180718 13090 solver.cpp:309]     Train net output #0: loss = 0.0366039 (* 1 = 0.0366039 loss)
I0711 22:36:36.180727 13090 sgd_solver.cpp:106] Iteration 11200, lr = 1e-05
I0711 22:36:53.199452 13090 solver.cpp:290] Iteration 11300 (5.87603 iter/s, 17.0183s/100 iter), loss = 0.0284623
I0711 22:36:53.199477 13090 solver.cpp:309]     Train net output #0: loss = 0.0284625 (* 1 = 0.0284625 loss)
I0711 22:36:53.199483 13090 sgd_solver.cpp:106] Iteration 11300, lr = 1e-05
I0711 22:37:10.153646 13090 solver.cpp:290] Iteration 11400 (5.89841 iter/s, 16.9537s/100 iter), loss = 0.0199253
I0711 22:37:10.153703 13090 solver.cpp:309]     Train net output #0: loss = 0.0199254 (* 1 = 0.0199254 loss)
I0711 22:37:10.153712 13090 sgd_solver.cpp:106] Iteration 11400, lr = 1e-05
I0711 22:37:27.125118 13090 solver.cpp:290] Iteration 11500 (5.89242 iter/s, 16.971s/100 iter), loss = 0.0245555
I0711 22:37:27.125145 13090 solver.cpp:309]     Train net output #0: loss = 0.0245557 (* 1 = 0.0245557 loss)
I0711 22:37:27.125155 13090 sgd_solver.cpp:106] Iteration 11500, lr = 1e-05
I0711 22:37:44.172281 13090 solver.cpp:290] Iteration 11600 (5.86624 iter/s, 17.0467s/100 iter), loss = 0.0351137
I0711 22:37:44.172333 13090 solver.cpp:309]     Train net output #0: loss = 0.0351138 (* 1 = 0.0351138 loss)
I0711 22:37:44.172341 13090 sgd_solver.cpp:106] Iteration 11600, lr = 1e-05
I0711 22:38:01.096693 13090 solver.cpp:290] Iteration 11700 (5.9088 iter/s, 16.9239s/100 iter), loss = 0.0247386
I0711 22:38:01.096716 13090 solver.cpp:309]     Train net output #0: loss = 0.0247388 (* 1 = 0.0247388 loss)
I0711 22:38:01.096724 13090 sgd_solver.cpp:106] Iteration 11700, lr = 1e-05
I0711 22:38:18.158308 13090 solver.cpp:290] Iteration 11800 (5.86128 iter/s, 17.0611s/100 iter), loss = 0.0191639
I0711 22:38:18.158368 13090 solver.cpp:309]     Train net output #0: loss = 0.0191641 (* 1 = 0.0191641 loss)
I0711 22:38:18.158380 13090 sgd_solver.cpp:106] Iteration 11800, lr = 1e-05
I0711 22:38:35.239903 13090 solver.cpp:290] Iteration 11900 (5.85443 iter/s, 17.0811s/100 iter), loss = 0.0354869
I0711 22:38:35.239929 13090 solver.cpp:309]     Train net output #0: loss = 0.0354871 (* 1 = 0.0354871 loss)
I0711 22:38:35.239936 13090 sgd_solver.cpp:106] Iteration 11900, lr = 1e-05
I0711 22:38:52.158083 13090 solver.cpp:354] Sparsity after update:
I0711 22:38:52.160271 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 22:38:52.160279 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 22:38:52.160286 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 22:38:52.160289 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 22:38:52.160290 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 22:38:52.160292 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 22:38:52.160295 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 22:38:52.160296 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 22:38:52.160298 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 22:38:52.160300 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 22:38:52.160301 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 22:38:52.160303 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 22:38:52.160305 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 22:38:52.160307 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 22:38:52.160310 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 22:38:52.160311 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 22:38:52.160313 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 22:38:52.160316 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 22:38:52.160320 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 22:38:52.160514 13090 solver.cpp:467] Iteration 12000, Testing net (#0)
I0711 22:39:38.487416 13090 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.949087
I0711 22:39:38.487506 13090 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999251
I0711 22:39:38.487514 13090 solver.cpp:540]     Test net output #2: loss = 0.161603 (* 1 = 0.161603 loss)
I0711 22:39:38.683210 13090 solver.cpp:290] Iteration 12000 (1.57625 iter/s, 63.4416s/100 iter), loss = 0.0305345
I0711 22:39:38.683238 13090 solver.cpp:309]     Train net output #0: loss = 0.0305347 (* 1 = 0.0305347 loss)
I0711 22:39:38.683246 13090 sgd_solver.cpp:106] Iteration 12000, lr = 1e-05
I0711 22:39:55.511530 13090 solver.cpp:290] Iteration 12100 (5.94254 iter/s, 16.8278s/100 iter), loss = 0.0309101
I0711 22:39:55.511554 13090 solver.cpp:309]     Train net output #0: loss = 0.0309102 (* 1 = 0.0309102 loss)
I0711 22:39:55.511560 13090 sgd_solver.cpp:106] Iteration 12100, lr = 1e-05
I0711 22:40:12.473546 13090 solver.cpp:290] Iteration 12200 (5.89569 iter/s, 16.9615s/100 iter), loss = 0.0195374
I0711 22:40:12.473599 13090 solver.cpp:309]     Train net output #0: loss = 0.0195376 (* 1 = 0.0195376 loss)
I0711 22:40:12.473606 13090 sgd_solver.cpp:106] Iteration 12200, lr = 1e-05
I0711 22:40:29.371021 13090 solver.cpp:290] Iteration 12300 (5.91822 iter/s, 16.897s/100 iter), loss = 0.0575116
I0711 22:40:29.371043 13090 solver.cpp:309]     Train net output #0: loss = 0.0575118 (* 1 = 0.0575118 loss)
I0711 22:40:29.371050 13090 sgd_solver.cpp:106] Iteration 12300, lr = 1e-05
I0711 22:40:46.310914 13090 solver.cpp:290] Iteration 12400 (5.90339 iter/s, 16.9394s/100 iter), loss = 0.045838
I0711 22:40:46.311003 13090 solver.cpp:309]     Train net output #0: loss = 0.0458382 (* 1 = 0.0458382 loss)
I0711 22:40:46.311010 13090 sgd_solver.cpp:106] Iteration 12400, lr = 1e-05
I0711 22:41:03.331689 13090 solver.cpp:290] Iteration 12500 (5.87536 iter/s, 17.0202s/100 iter), loss = 0.0204423
I0711 22:41:03.331717 13090 solver.cpp:309]     Train net output #0: loss = 0.0204425 (* 1 = 0.0204425 loss)
I0711 22:41:03.331727 13090 sgd_solver.cpp:106] Iteration 12500, lr = 1e-05
I0711 22:41:20.356171 13090 solver.cpp:290] Iteration 12600 (5.87406 iter/s, 17.024s/100 iter), loss = 0.0247841
I0711 22:41:20.356271 13090 solver.cpp:309]     Train net output #0: loss = 0.0247843 (* 1 = 0.0247843 loss)
I0711 22:41:20.356282 13090 sgd_solver.cpp:106] Iteration 12600, lr = 1e-05
I0711 22:41:37.361292 13090 solver.cpp:290] Iteration 12700 (5.88077 iter/s, 17.0046s/100 iter), loss = 0.0254945
I0711 22:41:37.361315 13090 solver.cpp:309]     Train net output #0: loss = 0.0254947 (* 1 = 0.0254947 loss)
I0711 22:41:37.361322 13090 sgd_solver.cpp:106] Iteration 12700, lr = 1e-05
I0711 22:41:54.363878 13090 solver.cpp:290] Iteration 12800 (5.88163 iter/s, 17.0021s/100 iter), loss = 0.0207313
I0711 22:41:54.363927 13090 solver.cpp:309]     Train net output #0: loss = 0.0207314 (* 1 = 0.0207314 loss)
I0711 22:41:54.363935 13090 sgd_solver.cpp:106] Iteration 12800, lr = 1e-05
I0711 22:42:11.325501 13090 solver.cpp:290] Iteration 12900 (5.89584 iter/s, 16.9611s/100 iter), loss = 0.034211
I0711 22:42:11.325523 13090 solver.cpp:309]     Train net output #0: loss = 0.0342111 (* 1 = 0.0342111 loss)
I0711 22:42:11.325531 13090 sgd_solver.cpp:106] Iteration 12900, lr = 1e-05
I0711 22:42:28.285490 13090 solver.cpp:354] Sparsity after update:
I0711 22:42:28.336429 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 22:42:28.336444 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 22:42:28.336454 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 22:42:28.336457 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 22:42:28.336460 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 22:42:28.336465 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 22:42:28.336469 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 22:42:28.336473 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 22:42:28.336477 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 22:42:28.336480 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 22:42:28.336483 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 22:42:28.336488 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 22:42:28.336493 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 22:42:28.336496 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 22:42:28.336501 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 22:42:28.336504 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 22:42:28.336508 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 22:42:28.336513 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 22:42:28.336516 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 22:42:28.486263 13090 solver.cpp:290] Iteration 13000 (5.82741 iter/s, 17.1603s/100 iter), loss = 0.033472
I0711 22:42:28.486289 13090 solver.cpp:309]     Train net output #0: loss = 0.0334722 (* 1 = 0.0334722 loss)
I0711 22:42:28.486299 13090 sgd_solver.cpp:106] Iteration 13000, lr = 1e-05
I0711 22:42:45.462153 13090 solver.cpp:290] Iteration 13100 (5.89087 iter/s, 16.9754s/100 iter), loss = 0.0207863
I0711 22:42:45.462177 13090 solver.cpp:309]     Train net output #0: loss = 0.0207865 (* 1 = 0.0207865 loss)
I0711 22:42:45.462183 13090 sgd_solver.cpp:106] Iteration 13100, lr = 1e-05
I0711 22:43:02.509974 13090 solver.cpp:290] Iteration 13200 (5.86602 iter/s, 17.0473s/100 iter), loss = 0.0258876
I0711 22:43:02.510041 13090 solver.cpp:309]     Train net output #0: loss = 0.0258877 (* 1 = 0.0258877 loss)
I0711 22:43:02.510049 13090 sgd_solver.cpp:106] Iteration 13200, lr = 1e-05
I0711 22:43:19.598533 13090 solver.cpp:290] Iteration 13300 (5.85205 iter/s, 17.088s/100 iter), loss = 0.0621381
I0711 22:43:19.598556 13090 solver.cpp:309]     Train net output #0: loss = 0.0621383 (* 1 = 0.0621383 loss)
I0711 22:43:19.598563 13090 sgd_solver.cpp:106] Iteration 13300, lr = 1e-05
I0711 22:43:36.682808 13090 solver.cpp:290] Iteration 13400 (5.8535 iter/s, 17.0838s/100 iter), loss = 0.027458
I0711 22:43:36.682883 13090 solver.cpp:309]     Train net output #0: loss = 0.0274581 (* 1 = 0.0274581 loss)
I0711 22:43:36.682890 13090 sgd_solver.cpp:106] Iteration 13400, lr = 1e-05
I0711 22:43:53.699093 13090 solver.cpp:290] Iteration 13500 (5.87691 iter/s, 17.0158s/100 iter), loss = 0.0377032
I0711 22:43:53.699117 13090 solver.cpp:309]     Train net output #0: loss = 0.0377033 (* 1 = 0.0377033 loss)
I0711 22:43:53.699124 13090 sgd_solver.cpp:106] Iteration 13500, lr = 1e-05
I0711 22:44:10.636605 13090 solver.cpp:290] Iteration 13600 (5.90422 iter/s, 16.937s/100 iter), loss = 0.0264189
I0711 22:44:10.636682 13090 solver.cpp:309]     Train net output #0: loss = 0.0264191 (* 1 = 0.0264191 loss)
I0711 22:44:10.636689 13090 sgd_solver.cpp:106] Iteration 13600, lr = 1e-05
I0711 22:44:27.587496 13090 solver.cpp:290] Iteration 13700 (5.89958 iter/s, 16.9504s/100 iter), loss = 0.041518
I0711 22:44:27.587523 13090 solver.cpp:309]     Train net output #0: loss = 0.0415182 (* 1 = 0.0415182 loss)
I0711 22:44:27.587532 13090 sgd_solver.cpp:106] Iteration 13700, lr = 1e-05
I0711 22:44:44.565479 13090 solver.cpp:290] Iteration 13800 (5.89015 iter/s, 16.9775s/100 iter), loss = 0.0238753
I0711 22:44:44.565536 13090 solver.cpp:309]     Train net output #0: loss = 0.0238754 (* 1 = 0.0238754 loss)
I0711 22:44:44.565547 13090 sgd_solver.cpp:106] Iteration 13800, lr = 1e-05
I0711 22:45:01.616145 13090 solver.cpp:290] Iteration 13900 (5.86505 iter/s, 17.0502s/100 iter), loss = 0.0330012
I0711 22:45:01.616170 13090 solver.cpp:309]     Train net output #0: loss = 0.0330014 (* 1 = 0.0330014 loss)
I0711 22:45:01.616179 13090 sgd_solver.cpp:106] Iteration 13900, lr = 1e-05
I0711 22:45:18.477501 13090 solver.cpp:354] Sparsity after update:
I0711 22:45:18.479681 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 22:45:18.479689 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 22:45:18.479696 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 22:45:18.479699 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 22:45:18.479701 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 22:45:18.479703 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 22:45:18.479706 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 22:45:18.479707 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 22:45:18.479709 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 22:45:18.479712 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 22:45:18.479712 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 22:45:18.479714 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 22:45:18.479717 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 22:45:18.479719 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 22:45:18.479722 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 22:45:18.479722 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 22:45:18.479724 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 22:45:18.479727 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 22:45:18.479728 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 22:45:18.479867 13090 solver.cpp:467] Iteration 14000, Testing net (#0)
I0711 22:46:04.984632 13090 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.948772
I0711 22:46:04.984705 13090 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999299
I0711 22:46:04.984712 13090 solver.cpp:540]     Test net output #2: loss = 0.164605 (* 1 = 0.164605 loss)
I0711 22:46:05.168081 13090 solver.cpp:290] Iteration 14000 (1.57356 iter/s, 63.5502s/100 iter), loss = 0.0259977
I0711 22:46:05.168107 13090 solver.cpp:309]     Train net output #0: loss = 0.0259979 (* 1 = 0.0259979 loss)
I0711 22:46:05.168112 13090 sgd_solver.cpp:106] Iteration 14000, lr = 1e-05
I0711 22:46:21.965914 13090 solver.cpp:290] Iteration 14100 (5.95332 iter/s, 16.7974s/100 iter), loss = 0.0199034
I0711 22:46:21.965940 13090 solver.cpp:309]     Train net output #0: loss = 0.0199036 (* 1 = 0.0199036 loss)
I0711 22:46:21.965947 13090 sgd_solver.cpp:106] Iteration 14100, lr = 1e-05
I0711 22:46:39.457710 13090 solver.cpp:290] Iteration 14200 (5.71713 iter/s, 17.4913s/100 iter), loss = 0.021262
I0711 22:46:39.457806 13090 solver.cpp:309]     Train net output #0: loss = 0.0212622 (* 1 = 0.0212622 loss)
I0711 22:46:39.457818 13090 sgd_solver.cpp:106] Iteration 14200, lr = 1e-05
I0711 22:46:58.293897 13090 solver.cpp:290] Iteration 14300 (5.3091 iter/s, 18.8356s/100 iter), loss = 0.0226287
I0711 22:46:58.293941 13090 solver.cpp:309]     Train net output #0: loss = 0.0226289 (* 1 = 0.0226289 loss)
I0711 22:46:58.293956 13090 sgd_solver.cpp:106] Iteration 14300, lr = 1e-05
I0711 22:47:16.213651 13090 solver.cpp:290] Iteration 14400 (5.5806 iter/s, 17.9192s/100 iter), loss = 0.0208534
I0711 22:47:16.213728 13090 solver.cpp:309]     Train net output #0: loss = 0.0208535 (* 1 = 0.0208535 loss)
I0711 22:47:16.213737 13090 sgd_solver.cpp:106] Iteration 14400, lr = 1e-05
I0711 22:47:34.119474 13090 solver.cpp:290] Iteration 14500 (5.58495 iter/s, 17.9053s/100 iter), loss = 0.0438681
I0711 22:47:34.119498 13090 solver.cpp:309]     Train net output #0: loss = 0.0438683 (* 1 = 0.0438683 loss)
I0711 22:47:34.119505 13090 sgd_solver.cpp:106] Iteration 14500, lr = 1e-05
I0711 22:47:51.643415 13090 solver.cpp:290] Iteration 14600 (5.70664 iter/s, 17.5234s/100 iter), loss = 0.0225341
I0711 22:47:51.643486 13090 solver.cpp:309]     Train net output #0: loss = 0.0225343 (* 1 = 0.0225343 loss)
I0711 22:47:51.643501 13090 sgd_solver.cpp:106] Iteration 14600, lr = 1e-05
I0711 22:48:09.113994 13090 solver.cpp:290] Iteration 14700 (5.72409 iter/s, 17.47s/100 iter), loss = 0.0631895
I0711 22:48:09.114022 13090 solver.cpp:309]     Train net output #0: loss = 0.0631896 (* 1 = 0.0631896 loss)
I0711 22:48:09.114030 13090 sgd_solver.cpp:106] Iteration 14700, lr = 1e-05
I0711 22:48:26.511963 13090 solver.cpp:290] Iteration 14800 (5.74796 iter/s, 17.3975s/100 iter), loss = 0.0356657
I0711 22:48:26.512055 13090 solver.cpp:309]     Train net output #0: loss = 0.0356659 (* 1 = 0.0356659 loss)
I0711 22:48:26.512065 13090 sgd_solver.cpp:106] Iteration 14800, lr = 1e-05
I0711 22:48:43.523710 13090 solver.cpp:290] Iteration 14900 (5.87848 iter/s, 17.0112s/100 iter), loss = 0.0406048
I0711 22:48:43.523731 13090 solver.cpp:309]     Train net output #0: loss = 0.0406049 (* 1 = 0.0406049 loss)
I0711 22:48:43.523738 13090 sgd_solver.cpp:106] Iteration 14900, lr = 1e-05
I0711 22:49:00.420719 13090 solver.cpp:354] Sparsity after update:
I0711 22:49:00.485332 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 22:49:00.485350 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 22:49:00.485358 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 22:49:00.485360 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 22:49:00.485363 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 22:49:00.485364 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 22:49:00.485366 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 22:49:00.485368 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 22:49:00.485370 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 22:49:00.485373 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 22:49:00.485374 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 22:49:00.485376 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 22:49:00.485378 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 22:49:00.485380 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 22:49:00.485383 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 22:49:00.485384 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 22:49:00.485386 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 22:49:00.485388 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 22:49:00.485390 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 22:49:00.635735 13090 solver.cpp:290] Iteration 15000 (5.84401 iter/s, 17.1115s/100 iter), loss = 0.0401788
I0711 22:49:00.635764 13090 solver.cpp:309]     Train net output #0: loss = 0.040179 (* 1 = 0.040179 loss)
I0711 22:49:00.635773 13090 sgd_solver.cpp:106] Iteration 15000, lr = 1e-05
I0711 22:49:17.682315 13090 solver.cpp:290] Iteration 15100 (5.86645 iter/s, 17.0461s/100 iter), loss = 0.0208675
I0711 22:49:17.682338 13090 solver.cpp:309]     Train net output #0: loss = 0.0208677 (* 1 = 0.0208677 loss)
I0711 22:49:17.682345 13090 sgd_solver.cpp:106] Iteration 15100, lr = 1e-05
I0711 22:49:35.951843 13090 solver.cpp:290] Iteration 15200 (5.47375 iter/s, 18.269s/100 iter), loss = 0.024015
I0711 22:49:35.951920 13090 solver.cpp:309]     Train net output #0: loss = 0.0240151 (* 1 = 0.0240151 loss)
I0711 22:49:35.951928 13090 sgd_solver.cpp:106] Iteration 15200, lr = 1e-05
I0711 22:49:53.491266 13090 solver.cpp:290] Iteration 15300 (5.70162 iter/s, 17.5389s/100 iter), loss = 0.0279693
I0711 22:49:53.491322 13090 solver.cpp:309]     Train net output #0: loss = 0.0279695 (* 1 = 0.0279695 loss)
I0711 22:49:53.491345 13090 sgd_solver.cpp:106] Iteration 15300, lr = 1e-05
I0711 22:50:10.765221 13090 solver.cpp:290] Iteration 15400 (5.78923 iter/s, 17.2734s/100 iter), loss = 0.0312528
I0711 22:50:10.765278 13090 solver.cpp:309]     Train net output #0: loss = 0.031253 (* 1 = 0.031253 loss)
I0711 22:50:10.765290 13090 sgd_solver.cpp:106] Iteration 15400, lr = 1e-05
I0711 22:50:27.915601 13090 solver.cpp:290] Iteration 15500 (5.83095 iter/s, 17.1499s/100 iter), loss = 0.0266507
I0711 22:50:27.915624 13090 solver.cpp:309]     Train net output #0: loss = 0.0266509 (* 1 = 0.0266509 loss)
I0711 22:50:27.915632 13090 sgd_solver.cpp:106] Iteration 15500, lr = 1e-05
I0711 22:50:45.042088 13090 solver.cpp:290] Iteration 15600 (5.83907 iter/s, 17.126s/100 iter), loss = 0.0320553
I0711 22:50:45.042146 13090 solver.cpp:309]     Train net output #0: loss = 0.0320555 (* 1 = 0.0320555 loss)
I0711 22:50:45.042161 13090 sgd_solver.cpp:106] Iteration 15600, lr = 1e-05
I0711 22:51:02.077728 13090 solver.cpp:290] Iteration 15700 (5.87022 iter/s, 17.0351s/100 iter), loss = 0.0273836
I0711 22:51:02.077754 13090 solver.cpp:309]     Train net output #0: loss = 0.0273838 (* 1 = 0.0273838 loss)
I0711 22:51:02.077764 13090 sgd_solver.cpp:106] Iteration 15700, lr = 1e-05
I0711 22:51:19.085429 13090 solver.cpp:290] Iteration 15800 (5.87986 iter/s, 17.0072s/100 iter), loss = 0.040705
I0711 22:51:19.085528 13090 solver.cpp:309]     Train net output #0: loss = 0.0407051 (* 1 = 0.0407051 loss)
I0711 22:51:19.085536 13090 sgd_solver.cpp:106] Iteration 15800, lr = 1e-05
I0711 22:51:36.117259 13090 solver.cpp:290] Iteration 15900 (5.87155 iter/s, 17.0313s/100 iter), loss = 0.0357982
I0711 22:51:36.117282 13090 solver.cpp:309]     Train net output #0: loss = 0.0357983 (* 1 = 0.0357983 loss)
I0711 22:51:36.117290 13090 sgd_solver.cpp:106] Iteration 15900, lr = 1e-05
I0711 22:51:53.005621 13090 solver.cpp:354] Sparsity after update:
I0711 22:51:53.007807 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 22:51:53.007814 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 22:51:53.007824 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 22:51:53.007829 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 22:51:53.007833 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 22:51:53.007838 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 22:51:53.007841 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 22:51:53.007846 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 22:51:53.007849 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 22:51:53.007854 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 22:51:53.007858 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 22:51:53.007861 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 22:51:53.007865 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 22:51:53.007869 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 22:51:53.007874 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 22:51:53.007877 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 22:51:53.007882 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 22:51:53.007886 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 22:51:53.007890 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 22:51:53.008028 13090 solver.cpp:467] Iteration 16000, Testing net (#0)
I0711 22:52:39.336042 13090 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.949381
I0711 22:52:39.336122 13090 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999031
I0711 22:52:39.336132 13090 solver.cpp:540]     Test net output #2: loss = 0.171221 (* 1 = 0.171221 loss)
I0711 22:52:39.522639 13090 solver.cpp:290] Iteration 16000 (1.5772 iter/s, 63.4037s/100 iter), loss = 0.0365465
I0711 22:52:39.522663 13090 solver.cpp:309]     Train net output #0: loss = 0.0365466 (* 1 = 0.0365466 loss)
I0711 22:52:39.522672 13090 sgd_solver.cpp:106] Iteration 16000, lr = 1e-05
I0711 22:52:57.076520 13090 solver.cpp:290] Iteration 16100 (5.69691 iter/s, 17.5534s/100 iter), loss = 0.0346034
I0711 22:52:57.076566 13090 solver.cpp:309]     Train net output #0: loss = 0.0346035 (* 1 = 0.0346035 loss)
I0711 22:52:57.076581 13090 sgd_solver.cpp:106] Iteration 16100, lr = 1e-05
I0711 22:53:15.458091 13090 solver.cpp:290] Iteration 16200 (5.44039 iter/s, 18.381s/100 iter), loss = 0.0542311
I0711 22:53:15.458178 13090 solver.cpp:309]     Train net output #0: loss = 0.0542312 (* 1 = 0.0542312 loss)
I0711 22:53:15.458189 13090 sgd_solver.cpp:106] Iteration 16200, lr = 1e-05
I0711 22:53:34.016510 13090 solver.cpp:290] Iteration 16300 (5.38856 iter/s, 18.5578s/100 iter), loss = 0.0410569
I0711 22:53:34.016649 13090 solver.cpp:309]     Train net output #0: loss = 0.041057 (* 1 = 0.041057 loss)
I0711 22:53:34.016712 13090 sgd_solver.cpp:106] Iteration 16300, lr = 1e-05
I0711 22:53:52.088068 13090 solver.cpp:290] Iteration 16400 (5.53374 iter/s, 18.0709s/100 iter), loss = 0.0282318
I0711 22:53:52.088390 13090 solver.cpp:309]     Train net output #0: loss = 0.0282319 (* 1 = 0.0282319 loss)
I0711 22:53:52.088505 13090 sgd_solver.cpp:106] Iteration 16400, lr = 1e-05
I0711 22:54:09.877806 13090 solver.cpp:290] Iteration 16500 (5.62147 iter/s, 17.7889s/100 iter), loss = 0.0180467
I0711 22:54:09.877933 13090 solver.cpp:309]     Train net output #0: loss = 0.0180469 (* 1 = 0.0180469 loss)
I0711 22:54:09.877976 13090 sgd_solver.cpp:106] Iteration 16500, lr = 1e-05
I0711 22:54:27.137202 13090 solver.cpp:290] Iteration 16600 (5.79414 iter/s, 17.2588s/100 iter), loss = 0.0307938
I0711 22:54:27.137316 13090 solver.cpp:309]     Train net output #0: loss = 0.030794 (* 1 = 0.030794 loss)
I0711 22:54:27.137331 13090 sgd_solver.cpp:106] Iteration 16600, lr = 1e-05
I0711 22:54:44.307500 13090 solver.cpp:290] Iteration 16700 (5.8242 iter/s, 17.1697s/100 iter), loss = 0.0660511
I0711 22:54:44.307526 13090 solver.cpp:309]     Train net output #0: loss = 0.0660513 (* 1 = 0.0660513 loss)
I0711 22:54:44.307533 13090 sgd_solver.cpp:106] Iteration 16700, lr = 1e-05
I0711 22:55:01.233558 13090 solver.cpp:290] Iteration 16800 (5.90822 iter/s, 16.9256s/100 iter), loss = 0.0720282
I0711 22:55:01.233669 13090 solver.cpp:309]     Train net output #0: loss = 0.0720283 (* 1 = 0.0720283 loss)
I0711 22:55:01.233677 13090 sgd_solver.cpp:106] Iteration 16800, lr = 1e-05
I0711 22:55:18.245760 13090 solver.cpp:290] Iteration 16900 (5.87833 iter/s, 17.0116s/100 iter), loss = 0.0233099
I0711 22:55:18.245784 13090 solver.cpp:309]     Train net output #0: loss = 0.0233101 (* 1 = 0.0233101 loss)
I0711 22:55:18.245790 13090 sgd_solver.cpp:106] Iteration 16900, lr = 1e-05
I0711 22:55:35.016396 13090 solver.cpp:354] Sparsity after update:
I0711 22:55:35.082880 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 22:55:35.082895 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 22:55:35.082901 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 22:55:35.082903 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 22:55:35.082906 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 22:55:35.082907 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 22:55:35.082909 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 22:55:35.082911 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 22:55:35.082913 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 22:55:35.082916 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 22:55:35.082917 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 22:55:35.082919 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 22:55:35.082921 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 22:55:35.082923 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 22:55:35.082926 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 22:55:35.082927 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 22:55:35.082929 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 22:55:35.082931 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 22:55:35.082933 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 22:55:35.234064 13090 solver.cpp:290] Iteration 17000 (5.88657 iter/s, 16.9878s/100 iter), loss = 0.017767
I0711 22:55:35.234091 13090 solver.cpp:309]     Train net output #0: loss = 0.0177671 (* 1 = 0.0177671 loss)
I0711 22:55:35.234097 13090 sgd_solver.cpp:106] Iteration 17000, lr = 1e-05
I0711 22:55:52.315641 13090 solver.cpp:290] Iteration 17100 (5.85443 iter/s, 17.0811s/100 iter), loss = 0.0181443
I0711 22:55:52.315665 13090 solver.cpp:309]     Train net output #0: loss = 0.0181444 (* 1 = 0.0181444 loss)
I0711 22:55:52.315671 13090 sgd_solver.cpp:106] Iteration 17100, lr = 1e-05
I0711 22:56:09.405721 13090 solver.cpp:290] Iteration 17200 (5.85151 iter/s, 17.0896s/100 iter), loss = 0.0250677
I0711 22:56:09.405817 13090 solver.cpp:309]     Train net output #0: loss = 0.0250679 (* 1 = 0.0250679 loss)
I0711 22:56:09.405827 13090 sgd_solver.cpp:106] Iteration 17200, lr = 1e-05
I0711 22:56:26.454802 13090 solver.cpp:290] Iteration 17300 (5.86561 iter/s, 17.0485s/100 iter), loss = 0.0226854
I0711 22:56:26.454830 13090 solver.cpp:309]     Train net output #0: loss = 0.0226855 (* 1 = 0.0226855 loss)
I0711 22:56:26.454839 13090 sgd_solver.cpp:106] Iteration 17300, lr = 1e-05
I0711 22:56:43.442368 13090 solver.cpp:290] Iteration 17400 (5.88683 iter/s, 16.9871s/100 iter), loss = 0.0245536
I0711 22:56:43.442437 13090 solver.cpp:309]     Train net output #0: loss = 0.0245537 (* 1 = 0.0245537 loss)
I0711 22:56:43.442445 13090 sgd_solver.cpp:106] Iteration 17400, lr = 1e-05
I0711 22:57:00.605675 13090 solver.cpp:290] Iteration 17500 (5.82656 iter/s, 17.1628s/100 iter), loss = 0.0506717
I0711 22:57:00.605702 13090 solver.cpp:309]     Train net output #0: loss = 0.0506718 (* 1 = 0.0506718 loss)
I0711 22:57:00.605711 13090 sgd_solver.cpp:106] Iteration 17500, lr = 1e-05
I0711 22:57:17.613247 13090 solver.cpp:290] Iteration 17600 (5.8799 iter/s, 17.0071s/100 iter), loss = 0.0194577
I0711 22:57:17.613348 13090 solver.cpp:309]     Train net output #0: loss = 0.0194578 (* 1 = 0.0194578 loss)
I0711 22:57:17.613356 13090 sgd_solver.cpp:106] Iteration 17600, lr = 1e-05
I0711 22:57:34.638491 13090 solver.cpp:290] Iteration 17700 (5.87382 iter/s, 17.0247s/100 iter), loss = 0.0383852
I0711 22:57:34.638517 13090 solver.cpp:309]     Train net output #0: loss = 0.0383854 (* 1 = 0.0383854 loss)
I0711 22:57:34.638525 13090 sgd_solver.cpp:106] Iteration 17700, lr = 1e-05
I0711 22:57:51.645143 13090 solver.cpp:290] Iteration 17800 (5.88022 iter/s, 17.0062s/100 iter), loss = 0.121997
I0711 22:57:51.645192 13090 solver.cpp:309]     Train net output #0: loss = 0.121997 (* 1 = 0.121997 loss)
I0711 22:57:51.645200 13090 sgd_solver.cpp:106] Iteration 17800, lr = 1e-05
I0711 22:58:08.658669 13090 solver.cpp:290] Iteration 17900 (5.87785 iter/s, 17.013s/100 iter), loss = 0.0378805
I0711 22:58:08.658694 13090 solver.cpp:309]     Train net output #0: loss = 0.0378807 (* 1 = 0.0378807 loss)
I0711 22:58:08.658701 13090 sgd_solver.cpp:106] Iteration 17900, lr = 1e-05
I0711 22:58:25.496639 13090 solver.cpp:354] Sparsity after update:
I0711 22:58:25.498575 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 22:58:25.498584 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 22:58:25.498590 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 22:58:25.498594 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 22:58:25.498594 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 22:58:25.498596 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 22:58:25.498598 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 22:58:25.498600 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 22:58:25.498602 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 22:58:25.498605 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 22:58:25.498606 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 22:58:25.498608 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 22:58:25.498610 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 22:58:25.498612 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 22:58:25.498615 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 22:58:25.498616 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 22:58:25.498618 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 22:58:25.498621 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 22:58:25.498625 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 22:58:25.498757 13090 solver.cpp:467] Iteration 18000, Testing net (#0)
I0711 22:59:18.816855 13090 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.949302
I0711 22:59:18.816928 13090 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.99915
I0711 22:59:18.816936 13090 solver.cpp:540]     Test net output #2: loss = 0.175334 (* 1 = 0.175334 loss)
I0711 22:59:19.006059 13090 solver.cpp:290] Iteration 18000 (1.42156 iter/s, 70.3455s/100 iter), loss = 0.027849
I0711 22:59:19.006083 13090 solver.cpp:309]     Train net output #0: loss = 0.0278491 (* 1 = 0.0278491 loss)
I0711 22:59:19.006089 13090 sgd_solver.cpp:106] Iteration 18000, lr = 1e-05
I0711 22:59:35.817966 13090 solver.cpp:290] Iteration 18100 (5.94834 iter/s, 16.8114s/100 iter), loss = 0.0374286
I0711 22:59:35.817991 13090 solver.cpp:309]     Train net output #0: loss = 0.0374287 (* 1 = 0.0374287 loss)
I0711 22:59:35.817999 13090 sgd_solver.cpp:106] Iteration 18100, lr = 1e-05
I0711 22:59:52.746487 13090 solver.cpp:290] Iteration 18200 (5.90736 iter/s, 16.928s/100 iter), loss = 0.0305814
I0711 22:59:52.746574 13090 solver.cpp:309]     Train net output #0: loss = 0.0305815 (* 1 = 0.0305815 loss)
I0711 22:59:52.746587 13090 sgd_solver.cpp:106] Iteration 18200, lr = 1e-05
I0711 23:00:09.785383 13090 solver.cpp:290] Iteration 18300 (5.86911 iter/s, 17.0383s/100 iter), loss = 0.0303097
I0711 23:00:09.785406 13090 solver.cpp:309]     Train net output #0: loss = 0.0303098 (* 1 = 0.0303098 loss)
I0711 23:00:09.785413 13090 sgd_solver.cpp:106] Iteration 18300, lr = 1e-05
I0711 23:00:26.856071 13090 solver.cpp:290] Iteration 18400 (5.85816 iter/s, 17.0702s/100 iter), loss = 0.0278692
I0711 23:00:26.856153 13090 solver.cpp:309]     Train net output #0: loss = 0.0278693 (* 1 = 0.0278693 loss)
I0711 23:00:26.856160 13090 sgd_solver.cpp:106] Iteration 18400, lr = 1e-05
I0711 23:00:44.009124 13090 solver.cpp:290] Iteration 18500 (5.83005 iter/s, 17.1525s/100 iter), loss = 0.0392567
I0711 23:00:44.009147 13090 solver.cpp:309]     Train net output #0: loss = 0.0392568 (* 1 = 0.0392568 loss)
I0711 23:00:44.009155 13090 sgd_solver.cpp:106] Iteration 18500, lr = 1e-05
I0711 23:01:01.275208 13090 solver.cpp:290] Iteration 18600 (5.79187 iter/s, 17.2656s/100 iter), loss = 0.0218392
I0711 23:01:01.275259 13090 solver.cpp:309]     Train net output #0: loss = 0.0218394 (* 1 = 0.0218394 loss)
I0711 23:01:01.275266 13090 sgd_solver.cpp:106] Iteration 18600, lr = 1e-05
I0711 23:01:18.279551 13090 solver.cpp:290] Iteration 18700 (5.88103 iter/s, 17.0038s/100 iter), loss = 0.0304103
I0711 23:01:18.279606 13090 solver.cpp:309]     Train net output #0: loss = 0.0304104 (* 1 = 0.0304104 loss)
I0711 23:01:18.279623 13090 sgd_solver.cpp:106] Iteration 18700, lr = 1e-05
I0711 23:01:35.372611 13090 solver.cpp:290] Iteration 18800 (5.8505 iter/s, 17.0925s/100 iter), loss = 0.0253967
I0711 23:01:35.372655 13090 solver.cpp:309]     Train net output #0: loss = 0.0253968 (* 1 = 0.0253968 loss)
I0711 23:01:35.372663 13090 sgd_solver.cpp:106] Iteration 18800, lr = 1e-05
I0711 23:01:52.502574 13090 solver.cpp:290] Iteration 18900 (5.8379 iter/s, 17.1295s/100 iter), loss = 0.0326898
I0711 23:01:52.502602 13090 solver.cpp:309]     Train net output #0: loss = 0.0326899 (* 1 = 0.0326899 loss)
I0711 23:01:52.502611 13090 sgd_solver.cpp:106] Iteration 18900, lr = 1e-05
I0711 23:02:09.450700 13090 solver.cpp:354] Sparsity after update:
I0711 23:02:09.508078 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 23:02:09.508103 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 23:02:09.508116 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 23:02:09.508121 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 23:02:09.508127 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 23:02:09.508132 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 23:02:09.508138 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 23:02:09.508144 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 23:02:09.508149 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 23:02:09.508155 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 23:02:09.508160 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 23:02:09.508164 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 23:02:09.508169 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 23:02:09.508173 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 23:02:09.508178 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 23:02:09.508183 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 23:02:09.508188 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 23:02:09.508191 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 23:02:09.508196 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 23:02:09.660467 13090 solver.cpp:290] Iteration 19000 (5.82839 iter/s, 17.1574s/100 iter), loss = 0.0354918
I0711 23:02:09.660497 13090 solver.cpp:309]     Train net output #0: loss = 0.035492 (* 1 = 0.035492 loss)
I0711 23:02:09.660504 13090 sgd_solver.cpp:106] Iteration 19000, lr = 1e-05
I0711 23:02:28.087865 13090 solver.cpp:290] Iteration 19100 (5.42686 iter/s, 18.4269s/100 iter), loss = 0.0274867
I0711 23:02:28.087890 13090 solver.cpp:309]     Train net output #0: loss = 0.0274868 (* 1 = 0.0274868 loss)
I0711 23:02:28.087898 13090 sgd_solver.cpp:106] Iteration 19100, lr = 1e-05
I0711 23:02:46.305212 13090 solver.cpp:290] Iteration 19200 (5.48943 iter/s, 18.2168s/100 iter), loss = 0.0236527
I0711 23:02:46.305338 13090 solver.cpp:309]     Train net output #0: loss = 0.0236529 (* 1 = 0.0236529 loss)
I0711 23:02:46.305450 13090 sgd_solver.cpp:106] Iteration 19200, lr = 1e-05
I0711 23:03:04.231107 13090 solver.cpp:290] Iteration 19300 (5.57871 iter/s, 17.9253s/100 iter), loss = 0.0497121
I0711 23:03:04.231130 13090 solver.cpp:309]     Train net output #0: loss = 0.0497122 (* 1 = 0.0497122 loss)
I0711 23:03:04.231137 13090 sgd_solver.cpp:106] Iteration 19300, lr = 1e-05
I0711 23:03:21.875320 13090 solver.cpp:290] Iteration 19400 (5.66774 iter/s, 17.6437s/100 iter), loss = 0.0342339
I0711 23:03:21.875412 13090 solver.cpp:309]     Train net output #0: loss = 0.0342341 (* 1 = 0.0342341 loss)
I0711 23:03:21.875430 13090 sgd_solver.cpp:106] Iteration 19400, lr = 1e-05
I0711 23:03:40.174504 13090 solver.cpp:290] Iteration 19500 (5.4649 iter/s, 18.2986s/100 iter), loss = 0.037036
I0711 23:03:40.174526 13090 solver.cpp:309]     Train net output #0: loss = 0.0370362 (* 1 = 0.0370362 loss)
I0711 23:03:40.174533 13090 sgd_solver.cpp:106] Iteration 19500, lr = 1e-05
I0711 23:03:58.789693 13090 solver.cpp:290] Iteration 19600 (5.37211 iter/s, 18.6147s/100 iter), loss = 0.0163714
I0711 23:03:58.789777 13090 solver.cpp:309]     Train net output #0: loss = 0.0163716 (* 1 = 0.0163716 loss)
I0711 23:03:58.789795 13090 sgd_solver.cpp:106] Iteration 19600, lr = 1e-05
I0711 23:04:16.865332 13090 solver.cpp:290] Iteration 19700 (5.53248 iter/s, 18.0751s/100 iter), loss = 0.0211029
I0711 23:04:16.865356 13090 solver.cpp:309]     Train net output #0: loss = 0.0211031 (* 1 = 0.0211031 loss)
I0711 23:04:16.865362 13090 sgd_solver.cpp:106] Iteration 19700, lr = 1e-05
I0711 23:04:34.555263 13090 solver.cpp:290] Iteration 19800 (5.65309 iter/s, 17.6894s/100 iter), loss = 0.0204564
I0711 23:04:34.555311 13090 solver.cpp:309]     Train net output #0: loss = 0.0204565 (* 1 = 0.0204565 loss)
I0711 23:04:34.555320 13090 sgd_solver.cpp:106] Iteration 19800, lr = 1e-05
I0711 23:04:52.722084 13090 solver.cpp:290] Iteration 19900 (5.5047 iter/s, 18.1663s/100 iter), loss = 0.035623
I0711 23:04:52.722108 13090 solver.cpp:309]     Train net output #0: loss = 0.0356232 (* 1 = 0.0356232 loss)
I0711 23:04:52.722115 13090 sgd_solver.cpp:106] Iteration 19900, lr = 1e-05
I0711 23:05:10.616458 13090 solver.cpp:594] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/sparse/cityscapes5_jsegnet21v2_iter_20000.caffemodel
I0711 23:05:10.658710 13090 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/sparse/cityscapes5_jsegnet21v2_iter_20000.solverstate
I0711 23:05:10.675606 13090 solver.cpp:354] Sparsity after update:
I0711 23:05:10.677319 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 23:05:10.677330 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 23:05:10.677340 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 23:05:10.677345 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 23:05:10.677350 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 23:05:10.677355 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 23:05:10.677358 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 23:05:10.677362 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 23:05:10.677366 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 23:05:10.677371 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 23:05:10.677374 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 23:05:10.677378 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 23:05:10.677382 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 23:05:10.677386 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 23:05:10.677389 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 23:05:10.677393 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 23:05:10.677397 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 23:05:10.677402 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 23:05:10.677405 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 23:05:10.677561 13090 solver.cpp:467] Iteration 20000, Testing net (#0)
I0711 23:06:14.682797 13090 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.950321
I0711 23:06:14.682927 13090 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999324
I0711 23:06:14.682937 13090 solver.cpp:540]     Test net output #2: loss = 0.1677 (* 1 = 0.1677 loss)
I0711 23:06:14.880645 13090 solver.cpp:290] Iteration 20000 (1.21719 iter/s, 82.1563s/100 iter), loss = 0.037898
I0711 23:06:14.880692 13090 solver.cpp:309]     Train net output #0: loss = 0.0378981 (* 1 = 0.0378981 loss)
I0711 23:06:14.880703 13090 sgd_solver.cpp:106] Iteration 20000, lr = 1e-05
I0711 23:06:32.951225 13090 solver.cpp:290] Iteration 20100 (5.53402 iter/s, 18.07s/100 iter), loss = 0.0398808
I0711 23:06:32.951267 13090 solver.cpp:309]     Train net output #0: loss = 0.039881 (* 1 = 0.039881 loss)
I0711 23:06:32.951277 13090 sgd_solver.cpp:106] Iteration 20100, lr = 1e-05
I0711 23:06:50.827599 13090 solver.cpp:290] Iteration 20200 (5.59414 iter/s, 17.8758s/100 iter), loss = 0.0194085
I0711 23:06:50.827678 13090 solver.cpp:309]     Train net output #0: loss = 0.0194087 (* 1 = 0.0194087 loss)
I0711 23:06:50.827687 13090 sgd_solver.cpp:106] Iteration 20200, lr = 1e-05
I0711 23:07:08.134421 13090 solver.cpp:290] Iteration 20300 (5.77825 iter/s, 17.3063s/100 iter), loss = 0.0191318
I0711 23:07:08.134443 13090 solver.cpp:309]     Train net output #0: loss = 0.019132 (* 1 = 0.019132 loss)
I0711 23:07:08.134450 13090 sgd_solver.cpp:106] Iteration 20300, lr = 1e-05
I0711 23:07:25.344804 13090 solver.cpp:290] Iteration 20400 (5.81061 iter/s, 17.2099s/100 iter), loss = 0.0533764
I0711 23:07:25.344897 13090 solver.cpp:309]     Train net output #0: loss = 0.0533765 (* 1 = 0.0533765 loss)
I0711 23:07:25.344907 13090 sgd_solver.cpp:106] Iteration 20400, lr = 1e-05
I0711 23:07:42.323738 13090 solver.cpp:290] Iteration 20500 (5.88984 iter/s, 16.9784s/100 iter), loss = 0.0484974
I0711 23:07:42.323765 13090 solver.cpp:309]     Train net output #0: loss = 0.0484975 (* 1 = 0.0484975 loss)
I0711 23:07:42.323774 13090 sgd_solver.cpp:106] Iteration 20500, lr = 1e-05
I0711 23:07:59.350783 13090 solver.cpp:290] Iteration 20600 (5.87318 iter/s, 17.0266s/100 iter), loss = 0.0279792
I0711 23:07:59.350842 13090 solver.cpp:309]     Train net output #0: loss = 0.0279793 (* 1 = 0.0279793 loss)
I0711 23:07:59.350853 13090 sgd_solver.cpp:106] Iteration 20600, lr = 1e-05
I0711 23:08:16.512347 13090 solver.cpp:290] Iteration 20700 (5.82715 iter/s, 17.161s/100 iter), loss = 0.0386273
I0711 23:08:16.512377 13090 solver.cpp:309]     Train net output #0: loss = 0.0386275 (* 1 = 0.0386275 loss)
I0711 23:08:16.512385 13090 sgd_solver.cpp:106] Iteration 20700, lr = 1e-05
I0711 23:08:33.437607 13090 solver.cpp:290] Iteration 20800 (5.9085 iter/s, 16.9248s/100 iter), loss = 0.0216204
I0711 23:08:33.437660 13090 solver.cpp:309]     Train net output #0: loss = 0.0216206 (* 1 = 0.0216206 loss)
I0711 23:08:33.437667 13090 sgd_solver.cpp:106] Iteration 20800, lr = 1e-05
I0711 23:08:50.463656 13090 solver.cpp:290] Iteration 20900 (5.87353 iter/s, 17.0255s/100 iter), loss = 0.0218961
I0711 23:08:50.463682 13090 solver.cpp:309]     Train net output #0: loss = 0.0218962 (* 1 = 0.0218962 loss)
I0711 23:08:50.463691 13090 sgd_solver.cpp:106] Iteration 20900, lr = 1e-05
I0711 23:09:07.311870 13090 solver.cpp:354] Sparsity after update:
I0711 23:09:07.362685 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 23:09:07.362701 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 23:09:07.362709 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 23:09:07.362711 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 23:09:07.362713 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 23:09:07.362715 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 23:09:07.362716 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 23:09:07.362718 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 23:09:07.362720 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 23:09:07.362722 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 23:09:07.362725 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 23:09:07.362728 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 23:09:07.362730 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 23:09:07.362733 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 23:09:07.362738 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 23:09:07.362742 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 23:09:07.362746 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 23:09:07.362751 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 23:09:07.362754 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 23:09:07.513600 13090 solver.cpp:290] Iteration 21000 (5.86529 iter/s, 17.0495s/100 iter), loss = 0.0455522
I0711 23:09:07.513623 13090 solver.cpp:309]     Train net output #0: loss = 0.0455523 (* 1 = 0.0455523 loss)
I0711 23:09:07.513630 13090 sgd_solver.cpp:106] Iteration 21000, lr = 1e-05
I0711 23:09:24.592250 13090 solver.cpp:290] Iteration 21100 (5.85543 iter/s, 17.0782s/100 iter), loss = 0.0243547
I0711 23:09:24.592275 13090 solver.cpp:309]     Train net output #0: loss = 0.0243548 (* 1 = 0.0243548 loss)
I0711 23:09:24.592283 13090 sgd_solver.cpp:106] Iteration 21100, lr = 1e-05
I0711 23:09:41.469256 13090 solver.cpp:290] Iteration 21200 (5.92539 iter/s, 16.8765s/100 iter), loss = 0.0411851
I0711 23:09:41.469307 13090 solver.cpp:309]     Train net output #0: loss = 0.0411852 (* 1 = 0.0411852 loss)
I0711 23:09:41.469316 13090 sgd_solver.cpp:106] Iteration 21200, lr = 1e-05
I0711 23:09:58.532106 13090 solver.cpp:290] Iteration 21300 (5.86086 iter/s, 17.0623s/100 iter), loss = 0.0237201
I0711 23:09:58.532130 13090 solver.cpp:309]     Train net output #0: loss = 0.0237203 (* 1 = 0.0237203 loss)
I0711 23:09:58.532137 13090 sgd_solver.cpp:106] Iteration 21300, lr = 1e-05
I0711 23:10:15.476426 13090 solver.cpp:290] Iteration 21400 (5.90185 iter/s, 16.9438s/100 iter), loss = 0.0320165
I0711 23:10:15.476475 13090 solver.cpp:309]     Train net output #0: loss = 0.0320166 (* 1 = 0.0320166 loss)
I0711 23:10:15.476485 13090 sgd_solver.cpp:106] Iteration 21400, lr = 1e-05
I0711 23:10:33.527720 13090 solver.cpp:290] Iteration 21500 (5.53993 iter/s, 18.0508s/100 iter), loss = 0.0316272
I0711 23:10:33.527765 13090 solver.cpp:309]     Train net output #0: loss = 0.0316274 (* 1 = 0.0316274 loss)
I0711 23:10:33.527791 13090 sgd_solver.cpp:106] Iteration 21500, lr = 1e-05
I0711 23:10:50.987340 13090 solver.cpp:290] Iteration 21600 (5.72767 iter/s, 17.4591s/100 iter), loss = 0.0360489
I0711 23:10:50.987391 13090 solver.cpp:309]     Train net output #0: loss = 0.0360491 (* 1 = 0.0360491 loss)
I0711 23:10:50.987399 13090 sgd_solver.cpp:106] Iteration 21600, lr = 1e-05
I0711 23:11:08.018837 13090 solver.cpp:290] Iteration 21700 (5.87165 iter/s, 17.031s/100 iter), loss = 0.0355756
I0711 23:11:08.018867 13090 solver.cpp:309]     Train net output #0: loss = 0.0355757 (* 1 = 0.0355757 loss)
I0711 23:11:08.018878 13090 sgd_solver.cpp:106] Iteration 21700, lr = 1e-05
I0711 23:11:25.096484 13090 solver.cpp:290] Iteration 21800 (5.85578 iter/s, 17.0772s/100 iter), loss = 0.0229224
I0711 23:11:25.096541 13090 solver.cpp:309]     Train net output #0: loss = 0.0229226 (* 1 = 0.0229226 loss)
I0711 23:11:25.096552 13090 sgd_solver.cpp:106] Iteration 21800, lr = 1e-05
I0711 23:11:42.257591 13090 solver.cpp:290] Iteration 21900 (5.82731 iter/s, 17.1606s/100 iter), loss = 0.020808
I0711 23:11:42.257616 13090 solver.cpp:309]     Train net output #0: loss = 0.0208081 (* 1 = 0.0208081 loss)
I0711 23:11:42.257625 13090 sgd_solver.cpp:106] Iteration 21900, lr = 1e-05
I0711 23:11:58.985618 13090 solver.cpp:354] Sparsity after update:
I0711 23:11:58.988215 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 23:11:58.988231 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 23:11:58.988242 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 23:11:58.988247 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 23:11:58.988251 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 23:11:58.988256 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 23:11:58.988260 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 23:11:58.988265 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 23:11:58.988268 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 23:11:58.988272 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 23:11:58.988276 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 23:11:58.988281 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 23:11:58.988286 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 23:11:58.988289 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 23:11:58.988293 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 23:11:58.988297 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 23:11:58.988301 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 23:11:58.988304 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 23:11:58.988307 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 23:11:58.988507 13090 solver.cpp:467] Iteration 22000, Testing net (#0)
I0711 23:12:45.359262 13090 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.950097
I0711 23:12:45.359356 13090 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999519
I0711 23:12:45.359364 13090 solver.cpp:540]     Test net output #2: loss = 0.162282 (* 1 = 0.162282 loss)
I0711 23:12:45.543015 13090 solver.cpp:290] Iteration 22000 (1.58019 iter/s, 63.2837s/100 iter), loss = 0.0368986
I0711 23:12:45.543041 13090 solver.cpp:309]     Train net output #0: loss = 0.0368987 (* 1 = 0.0368987 loss)
I0711 23:12:45.543048 13090 sgd_solver.cpp:106] Iteration 22000, lr = 1e-05
I0711 23:13:03.499867 13090 solver.cpp:290] Iteration 22100 (5.56907 iter/s, 17.9563s/100 iter), loss = 0.0260617
I0711 23:13:03.499923 13090 solver.cpp:309]     Train net output #0: loss = 0.0260618 (* 1 = 0.0260618 loss)
I0711 23:13:03.499948 13090 sgd_solver.cpp:106] Iteration 22100, lr = 1e-05
I0711 23:13:21.806524 13090 solver.cpp:290] Iteration 22200 (5.46266 iter/s, 18.3061s/100 iter), loss = 0.0320386
I0711 23:13:21.806612 13090 solver.cpp:309]     Train net output #0: loss = 0.0320387 (* 1 = 0.0320387 loss)
I0711 23:13:21.806628 13090 sgd_solver.cpp:106] Iteration 22200, lr = 1e-05
I0711 23:13:40.094108 13090 solver.cpp:290] Iteration 22300 (5.46836 iter/s, 18.287s/100 iter), loss = 0.0317626
I0711 23:13:40.094154 13090 solver.cpp:309]     Train net output #0: loss = 0.0317627 (* 1 = 0.0317627 loss)
I0711 23:13:40.094179 13090 sgd_solver.cpp:106] Iteration 22300, lr = 1e-05
I0711 23:13:57.354511 13090 solver.cpp:290] Iteration 22400 (5.79378 iter/s, 17.2599s/100 iter), loss = 0.0256372
I0711 23:13:57.354554 13090 solver.cpp:309]     Train net output #0: loss = 0.0256373 (* 1 = 0.0256373 loss)
I0711 23:13:57.354564 13090 sgd_solver.cpp:106] Iteration 22400, lr = 1e-05
I0711 23:14:14.436930 13090 solver.cpp:290] Iteration 22500 (5.85415 iter/s, 17.0819s/100 iter), loss = 0.0295089
I0711 23:14:14.436954 13090 solver.cpp:309]     Train net output #0: loss = 0.029509 (* 1 = 0.029509 loss)
I0711 23:14:14.436960 13090 sgd_solver.cpp:106] Iteration 22500, lr = 1e-05
I0711 23:14:31.509920 13090 solver.cpp:290] Iteration 22600 (5.85737 iter/s, 17.0725s/100 iter), loss = 0.0378212
I0711 23:14:31.510026 13090 solver.cpp:309]     Train net output #0: loss = 0.0378213 (* 1 = 0.0378213 loss)
I0711 23:14:31.510036 13090 sgd_solver.cpp:106] Iteration 22600, lr = 1e-05
I0711 23:14:48.512086 13090 solver.cpp:290] Iteration 22700 (5.8818 iter/s, 17.0016s/100 iter), loss = 0.0250482
I0711 23:14:48.512117 13090 solver.cpp:309]     Train net output #0: loss = 0.0250483 (* 1 = 0.0250483 loss)
I0711 23:14:48.512127 13090 sgd_solver.cpp:106] Iteration 22700, lr = 1e-05
I0711 23:15:05.606510 13090 solver.cpp:290] Iteration 22800 (5.85003 iter/s, 17.0939s/100 iter), loss = 0.0296388
I0711 23:15:05.606621 13090 solver.cpp:309]     Train net output #0: loss = 0.0296389 (* 1 = 0.0296389 loss)
I0711 23:15:05.606631 13090 sgd_solver.cpp:106] Iteration 22800, lr = 1e-05
I0711 23:15:22.676869 13090 solver.cpp:290] Iteration 22900 (5.8583 iter/s, 17.0698s/100 iter), loss = 0.0215033
I0711 23:15:22.676893 13090 solver.cpp:309]     Train net output #0: loss = 0.0215034 (* 1 = 0.0215034 loss)
I0711 23:15:22.676900 13090 sgd_solver.cpp:106] Iteration 22900, lr = 1e-05
I0711 23:15:39.435776 13090 solver.cpp:354] Sparsity after update:
I0711 23:15:39.497758 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 23:15:39.497773 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 23:15:39.497781 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 23:15:39.497783 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 23:15:39.497786 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 23:15:39.497787 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 23:15:39.497789 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 23:15:39.497792 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 23:15:39.497793 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 23:15:39.497797 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 23:15:39.497798 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 23:15:39.497800 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 23:15:39.497803 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 23:15:39.497805 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 23:15:39.497807 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 23:15:39.497809 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 23:15:39.497812 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 23:15:39.497814 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 23:15:39.497817 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 23:15:39.649562 13090 solver.cpp:290] Iteration 23000 (5.89198 iter/s, 16.9722s/100 iter), loss = 0.0291
I0711 23:15:39.649588 13090 solver.cpp:309]     Train net output #0: loss = 0.0291001 (* 1 = 0.0291001 loss)
I0711 23:15:39.649595 13090 sgd_solver.cpp:106] Iteration 23000, lr = 1e-05
I0711 23:15:56.861435 13090 solver.cpp:290] Iteration 23100 (5.81011 iter/s, 17.2114s/100 iter), loss = 0.0297656
I0711 23:15:56.861461 13090 solver.cpp:309]     Train net output #0: loss = 0.0297658 (* 1 = 0.0297658 loss)
I0711 23:15:56.861470 13090 sgd_solver.cpp:106] Iteration 23100, lr = 1e-05
I0711 23:16:13.843976 13090 solver.cpp:290] Iteration 23200 (5.88857 iter/s, 16.9821s/100 iter), loss = 0.0218417
I0711 23:16:13.844053 13090 solver.cpp:309]     Train net output #0: loss = 0.0218418 (* 1 = 0.0218418 loss)
I0711 23:16:13.844061 13090 sgd_solver.cpp:106] Iteration 23200, lr = 1e-05
I0711 23:16:31.496510 13090 solver.cpp:290] Iteration 23300 (5.66509 iter/s, 17.652s/100 iter), loss = 0.0204261
I0711 23:16:31.496554 13090 solver.cpp:309]     Train net output #0: loss = 0.0204263 (* 1 = 0.0204263 loss)
I0711 23:16:31.496579 13090 sgd_solver.cpp:106] Iteration 23300, lr = 1e-05
I0711 23:16:48.598891 13090 solver.cpp:290] Iteration 23400 (5.84731 iter/s, 17.1019s/100 iter), loss = 0.0331327
I0711 23:16:48.598942 13090 solver.cpp:309]     Train net output #0: loss = 0.0331328 (* 1 = 0.0331328 loss)
I0711 23:16:48.598948 13090 sgd_solver.cpp:106] Iteration 23400, lr = 1e-05
I0711 23:17:05.621903 13090 solver.cpp:290] Iteration 23500 (5.87458 iter/s, 17.0225s/100 iter), loss = 0.0242694
I0711 23:17:05.621928 13090 solver.cpp:309]     Train net output #0: loss = 0.0242695 (* 1 = 0.0242695 loss)
I0711 23:17:05.621935 13090 sgd_solver.cpp:106] Iteration 23500, lr = 1e-05
I0711 23:17:22.564499 13090 solver.cpp:290] Iteration 23600 (5.90245 iter/s, 16.9421s/100 iter), loss = 0.0370463
I0711 23:17:22.564574 13090 solver.cpp:309]     Train net output #0: loss = 0.0370464 (* 1 = 0.0370464 loss)
I0711 23:17:22.564585 13090 sgd_solver.cpp:106] Iteration 23600, lr = 1e-05
I0711 23:17:39.618728 13090 solver.cpp:290] Iteration 23700 (5.86383 iter/s, 17.0537s/100 iter), loss = 0.0310658
I0711 23:17:39.618752 13090 solver.cpp:309]     Train net output #0: loss = 0.0310659 (* 1 = 0.0310659 loss)
I0711 23:17:39.618758 13090 sgd_solver.cpp:106] Iteration 23700, lr = 1e-05
I0711 23:17:56.691367 13090 solver.cpp:290] Iteration 23800 (5.85749 iter/s, 17.0722s/100 iter), loss = 0.0329062
I0711 23:17:56.691419 13090 solver.cpp:309]     Train net output #0: loss = 0.0329063 (* 1 = 0.0329063 loss)
I0711 23:17:56.691428 13090 sgd_solver.cpp:106] Iteration 23800, lr = 1e-05
I0711 23:18:13.681408 13090 solver.cpp:290] Iteration 23900 (5.88598 iter/s, 16.9895s/100 iter), loss = 0.0215785
I0711 23:18:13.681432 13090 solver.cpp:309]     Train net output #0: loss = 0.0215786 (* 1 = 0.0215786 loss)
I0711 23:18:13.681439 13090 sgd_solver.cpp:106] Iteration 23900, lr = 1e-05
I0711 23:18:30.499063 13090 solver.cpp:354] Sparsity after update:
I0711 23:18:30.501225 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 23:18:30.501235 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 23:18:30.501240 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 23:18:30.501242 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 23:18:30.501245 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 23:18:30.501246 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 23:18:30.501248 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 23:18:30.501250 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 23:18:30.501252 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 23:18:30.501255 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 23:18:30.501256 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 23:18:30.501258 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 23:18:30.501260 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 23:18:30.501262 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 23:18:30.501263 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 23:18:30.501266 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 23:18:30.501267 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 23:18:30.501269 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 23:18:30.501271 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 23:18:30.501405 13090 solver.cpp:467] Iteration 24000, Testing net (#0)
I0711 23:19:17.032886 13090 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.949969
I0711 23:19:17.032956 13090 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999499
I0711 23:19:17.032964 13090 solver.cpp:540]     Test net output #2: loss = 0.163111 (* 1 = 0.163111 loss)
I0711 23:19:17.232530 13090 solver.cpp:290] Iteration 24000 (1.57358 iter/s, 63.5494s/100 iter), loss = 0.0264291
I0711 23:19:17.232558 13090 solver.cpp:309]     Train net output #0: loss = 0.0264292 (* 1 = 0.0264292 loss)
I0711 23:19:17.232560 13271 sgd_solver.cpp:46] MultiStep Status: Iteration 24000, step = 1
I0711 23:19:17.232560 13270 sgd_solver.cpp:46] MultiStep Status: Iteration 24000, step = 1
I0711 23:19:17.232568 13090 sgd_solver.cpp:46] MultiStep Status: Iteration 24000, step = 1
I0711 23:19:17.232573 13090 sgd_solver.cpp:106] Iteration 24000, lr = 1e-06
I0711 23:19:34.317178 13090 solver.cpp:290] Iteration 24100 (5.85338 iter/s, 17.0842s/100 iter), loss = 0.0218588
I0711 23:19:34.317206 13090 solver.cpp:309]     Train net output #0: loss = 0.0218589 (* 1 = 0.0218589 loss)
I0711 23:19:34.317215 13090 sgd_solver.cpp:106] Iteration 24100, lr = 1e-06
I0711 23:19:51.253504 13090 solver.cpp:290] Iteration 24200 (5.90464 iter/s, 16.9358s/100 iter), loss = 0.0458334
I0711 23:19:51.253612 13090 solver.cpp:309]     Train net output #0: loss = 0.0458335 (* 1 = 0.0458335 loss)
I0711 23:19:51.253624 13090 sgd_solver.cpp:106] Iteration 24200, lr = 1e-06
I0711 23:20:09.305176 13090 solver.cpp:290] Iteration 24300 (5.53984 iter/s, 18.0511s/100 iter), loss = 0.0334719
I0711 23:20:09.305222 13090 solver.cpp:309]     Train net output #0: loss = 0.033472 (* 1 = 0.033472 loss)
I0711 23:20:09.305234 13090 sgd_solver.cpp:106] Iteration 24300, lr = 1e-06
I0711 23:20:27.510360 13090 solver.cpp:290] Iteration 24400 (5.4931 iter/s, 18.2046s/100 iter), loss = 0.0139647
I0711 23:20:27.510491 13090 solver.cpp:309]     Train net output #0: loss = 0.0139649 (* 1 = 0.0139649 loss)
I0711 23:20:27.510501 13090 sgd_solver.cpp:106] Iteration 24400, lr = 1e-06
I0711 23:20:45.625725 13090 solver.cpp:290] Iteration 24500 (5.52036 iter/s, 18.1147s/100 iter), loss = 0.0282061
I0711 23:20:45.625752 13090 solver.cpp:309]     Train net output #0: loss = 0.0282062 (* 1 = 0.0282062 loss)
I0711 23:20:45.625761 13090 sgd_solver.cpp:106] Iteration 24500, lr = 1e-06
I0711 23:21:03.028956 13090 solver.cpp:290] Iteration 24600 (5.74623 iter/s, 17.4027s/100 iter), loss = 0.034823
I0711 23:21:03.029048 13090 solver.cpp:309]     Train net output #0: loss = 0.0348231 (* 1 = 0.0348231 loss)
I0711 23:21:03.029078 13090 sgd_solver.cpp:106] Iteration 24600, lr = 1e-06
I0711 23:21:21.306694 13090 solver.cpp:290] Iteration 24700 (5.47131 iter/s, 18.2772s/100 iter), loss = 0.0367567
I0711 23:21:21.306738 13090 solver.cpp:309]     Train net output #0: loss = 0.0367568 (* 1 = 0.0367568 loss)
I0711 23:21:21.306751 13090 sgd_solver.cpp:106] Iteration 24700, lr = 1e-06
I0711 23:21:39.351783 13090 solver.cpp:290] Iteration 24800 (5.54184 iter/s, 18.0446s/100 iter), loss = 0.0218172
I0711 23:21:39.372876 13090 solver.cpp:309]     Train net output #0: loss = 0.0218173 (* 1 = 0.0218173 loss)
I0711 23:21:39.372927 13090 sgd_solver.cpp:106] Iteration 24800, lr = 1e-06
I0711 23:21:57.258693 13090 solver.cpp:290] Iteration 24900 (5.59117 iter/s, 17.8854s/100 iter), loss = 0.0390552
I0711 23:21:57.258723 13090 solver.cpp:309]     Train net output #0: loss = 0.0390553 (* 1 = 0.0390553 loss)
I0711 23:21:57.258733 13090 sgd_solver.cpp:106] Iteration 24900, lr = 1e-06
I0711 23:22:14.543519 13090 solver.cpp:354] Sparsity after update:
I0711 23:22:14.566345 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 23:22:14.566458 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 23:22:14.566498 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 23:22:14.566506 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 23:22:14.566515 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 23:22:14.566522 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 23:22:14.566530 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 23:22:14.566539 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 23:22:14.566546 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 23:22:14.566555 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 23:22:14.566562 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 23:22:14.566570 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 23:22:14.566578 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 23:22:14.566586 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 23:22:14.566593 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 23:22:14.566601 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 23:22:14.566608 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 23:22:14.566617 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 23:22:14.566624 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 23:22:14.727476 13090 solver.cpp:290] Iteration 25000 (5.72467 iter/s, 17.4683s/100 iter), loss = 0.0303252
I0711 23:22:14.727605 13090 solver.cpp:309]     Train net output #0: loss = 0.0303253 (* 1 = 0.0303253 loss)
I0711 23:22:14.727680 13090 sgd_solver.cpp:106] Iteration 25000, lr = 1e-06
I0711 23:22:32.863739 13090 solver.cpp:290] Iteration 25100 (5.514 iter/s, 18.1357s/100 iter), loss = 0.0271073
I0711 23:22:32.863796 13090 solver.cpp:309]     Train net output #0: loss = 0.0271074 (* 1 = 0.0271074 loss)
I0711 23:22:32.863821 13090 sgd_solver.cpp:106] Iteration 25100, lr = 1e-06
I0711 23:22:51.097621 13090 solver.cpp:290] Iteration 25200 (5.48447 iter/s, 18.2333s/100 iter), loss = 0.0681001
I0711 23:22:51.097911 13090 solver.cpp:309]     Train net output #0: loss = 0.0681002 (* 1 = 0.0681002 loss)
I0711 23:22:51.097951 13090 sgd_solver.cpp:106] Iteration 25200, lr = 1e-06
I0711 23:23:09.067803 13090 solver.cpp:290] Iteration 25300 (5.56501 iter/s, 17.9694s/100 iter), loss = 0.0246263
I0711 23:23:09.067826 13090 solver.cpp:309]     Train net output #0: loss = 0.0246265 (* 1 = 0.0246265 loss)
I0711 23:23:09.067833 13090 sgd_solver.cpp:106] Iteration 25300, lr = 1e-06
I0711 23:23:26.473212 13090 solver.cpp:290] Iteration 25400 (5.7455 iter/s, 17.4049s/100 iter), loss = 0.0358568
I0711 23:23:26.473266 13090 solver.cpp:309]     Train net output #0: loss = 0.0358569 (* 1 = 0.0358569 loss)
I0711 23:23:26.473274 13090 sgd_solver.cpp:106] Iteration 25400, lr = 1e-06
I0711 23:23:44.578817 13090 solver.cpp:290] Iteration 25500 (5.52332 iter/s, 18.1051s/100 iter), loss = 0.0235712
I0711 23:23:44.578842 13090 solver.cpp:309]     Train net output #0: loss = 0.0235713 (* 1 = 0.0235713 loss)
I0711 23:23:44.578850 13090 sgd_solver.cpp:106] Iteration 25500, lr = 1e-06
I0711 23:24:02.645995 13090 solver.cpp:290] Iteration 25600 (5.53506 iter/s, 18.0667s/100 iter), loss = 0.072245
I0711 23:24:02.646100 13090 solver.cpp:309]     Train net output #0: loss = 0.0722451 (* 1 = 0.0722451 loss)
I0711 23:24:02.646109 13090 sgd_solver.cpp:106] Iteration 25600, lr = 1e-06
I0711 23:24:20.906924 13090 solver.cpp:290] Iteration 25700 (5.47635 iter/s, 18.2603s/100 iter), loss = 0.032931
I0711 23:24:20.906949 13090 solver.cpp:309]     Train net output #0: loss = 0.0329311 (* 1 = 0.0329311 loss)
I0711 23:24:20.906958 13090 sgd_solver.cpp:106] Iteration 25700, lr = 1e-06
I0711 23:24:38.227826 13090 solver.cpp:290] Iteration 25800 (5.77354 iter/s, 17.3204s/100 iter), loss = 0.0377947
I0711 23:24:38.227993 13090 solver.cpp:309]     Train net output #0: loss = 0.0377949 (* 1 = 0.0377949 loss)
I0711 23:24:38.228006 13090 sgd_solver.cpp:106] Iteration 25800, lr = 1e-06
I0711 23:24:56.209313 13090 solver.cpp:290] Iteration 25900 (5.56148 iter/s, 17.9808s/100 iter), loss = 0.0207783
I0711 23:24:56.209339 13090 solver.cpp:309]     Train net output #0: loss = 0.0207784 (* 1 = 0.0207784 loss)
I0711 23:24:56.209347 13090 sgd_solver.cpp:106] Iteration 25900, lr = 1e-06
I0711 23:25:14.464498 13090 solver.cpp:354] Sparsity after update:
I0711 23:25:14.466426 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 23:25:14.466435 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 23:25:14.466442 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 23:25:14.466444 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 23:25:14.466446 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 23:25:14.466449 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 23:25:14.466450 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 23:25:14.466452 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 23:25:14.466454 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 23:25:14.466456 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 23:25:14.466459 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 23:25:14.466460 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 23:25:14.466462 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 23:25:14.466464 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 23:25:14.466466 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 23:25:14.466469 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 23:25:14.466470 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 23:25:14.466473 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 23:25:14.466476 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 23:25:14.466609 13090 solver.cpp:467] Iteration 26000, Testing net (#0)
I0711 23:26:12.136562 13090 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.950884
I0711 23:26:12.136708 13090 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999384
I0711 23:26:12.136729 13090 solver.cpp:540]     Test net output #2: loss = 0.161358 (* 1 = 0.161358 loss)
I0711 23:26:12.344475 13090 solver.cpp:290] Iteration 26000 (1.31349 iter/s, 76.1331s/100 iter), loss = 0.0304259
I0711 23:26:12.344519 13090 solver.cpp:309]     Train net output #0: loss = 0.030426 (* 1 = 0.030426 loss)
I0711 23:26:12.344532 13090 sgd_solver.cpp:106] Iteration 26000, lr = 1e-06
I0711 23:26:30.052742 13090 solver.cpp:290] Iteration 26100 (5.64725 iter/s, 17.7077s/100 iter), loss = 0.0306903
I0711 23:26:30.052821 13090 solver.cpp:309]     Train net output #0: loss = 0.0306904 (* 1 = 0.0306904 loss)
I0711 23:26:30.052851 13090 sgd_solver.cpp:106] Iteration 26100, lr = 1e-06
I0711 23:26:47.372018 13090 solver.cpp:290] Iteration 26200 (5.7741 iter/s, 17.3187s/100 iter), loss = 0.0294115
I0711 23:26:47.372069 13090 solver.cpp:309]     Train net output #0: loss = 0.0294116 (* 1 = 0.0294116 loss)
I0711 23:26:47.372076 13090 sgd_solver.cpp:106] Iteration 26200, lr = 1e-06
I0711 23:27:04.488898 13090 solver.cpp:290] Iteration 26300 (5.84237 iter/s, 17.1164s/100 iter), loss = 0.0224892
I0711 23:27:04.488922 13090 solver.cpp:309]     Train net output #0: loss = 0.0224893 (* 1 = 0.0224893 loss)
I0711 23:27:04.488931 13090 sgd_solver.cpp:106] Iteration 26300, lr = 1e-06
I0711 23:27:21.485622 13090 solver.cpp:290] Iteration 26400 (5.88366 iter/s, 16.9962s/100 iter), loss = 0.0214195
I0711 23:27:21.485690 13090 solver.cpp:309]     Train net output #0: loss = 0.0214196 (* 1 = 0.0214196 loss)
I0711 23:27:21.485700 13090 sgd_solver.cpp:106] Iteration 26400, lr = 1e-06
I0711 23:27:38.480819 13090 solver.cpp:290] Iteration 26500 (5.8842 iter/s, 16.9947s/100 iter), loss = 0.0374695
I0711 23:27:38.480842 13090 solver.cpp:309]     Train net output #0: loss = 0.0374697 (* 1 = 0.0374697 loss)
I0711 23:27:38.480849 13090 sgd_solver.cpp:106] Iteration 26500, lr = 1e-06
I0711 23:27:55.538967 13090 solver.cpp:290] Iteration 26600 (5.86247 iter/s, 17.0577s/100 iter), loss = 0.040984
I0711 23:27:55.539041 13090 solver.cpp:309]     Train net output #0: loss = 0.0409841 (* 1 = 0.0409841 loss)
I0711 23:27:55.539052 13090 sgd_solver.cpp:106] Iteration 26600, lr = 1e-06
I0711 23:28:12.483398 13090 solver.cpp:290] Iteration 26700 (5.90183 iter/s, 16.9439s/100 iter), loss = 0.0229501
I0711 23:28:12.483423 13090 solver.cpp:309]     Train net output #0: loss = 0.0229503 (* 1 = 0.0229503 loss)
I0711 23:28:12.483430 13090 sgd_solver.cpp:106] Iteration 26700, lr = 1e-06
I0711 23:28:29.413085 13090 solver.cpp:290] Iteration 26800 (5.90695 iter/s, 16.9292s/100 iter), loss = 0.0390385
I0711 23:28:29.413192 13090 solver.cpp:309]     Train net output #0: loss = 0.0390386 (* 1 = 0.0390386 loss)
I0711 23:28:29.413202 13090 sgd_solver.cpp:106] Iteration 26800, lr = 1e-06
I0711 23:28:46.338415 13090 solver.cpp:290] Iteration 26900 (5.9085 iter/s, 16.9248s/100 iter), loss = 0.0335077
I0711 23:28:46.338439 13090 solver.cpp:309]     Train net output #0: loss = 0.0335078 (* 1 = 0.0335078 loss)
I0711 23:28:46.338446 13090 sgd_solver.cpp:106] Iteration 26900, lr = 1e-06
I0711 23:29:03.212093 13090 solver.cpp:354] Sparsity after update:
I0711 23:29:03.273775 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 23:29:03.273789 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 23:29:03.273799 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 23:29:03.273802 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 23:29:03.273807 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 23:29:03.273810 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 23:29:03.273814 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 23:29:03.273819 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 23:29:03.273823 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 23:29:03.273828 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 23:29:03.273833 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 23:29:03.273836 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 23:29:03.273840 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 23:29:03.273844 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 23:29:03.273846 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 23:29:03.273849 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 23:29:03.273854 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 23:29:03.273857 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 23:29:03.273861 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 23:29:03.423825 13090 solver.cpp:290] Iteration 27000 (5.85312 iter/s, 17.0849s/100 iter), loss = 0.0199302
I0711 23:29:03.423856 13090 solver.cpp:309]     Train net output #0: loss = 0.0199303 (* 1 = 0.0199303 loss)
I0711 23:29:03.423864 13090 sgd_solver.cpp:106] Iteration 27000, lr = 1e-06
I0711 23:29:21.092767 13090 solver.cpp:290] Iteration 27100 (5.65981 iter/s, 17.6684s/100 iter), loss = 0.0183119
I0711 23:29:21.092795 13090 solver.cpp:309]     Train net output #0: loss = 0.018312 (* 1 = 0.018312 loss)
I0711 23:29:21.092802 13090 sgd_solver.cpp:106] Iteration 27100, lr = 1e-06
I0711 23:29:39.824089 13090 solver.cpp:290] Iteration 27200 (5.33881 iter/s, 18.7308s/100 iter), loss = 0.0318305
I0711 23:29:39.824158 13090 solver.cpp:309]     Train net output #0: loss = 0.0318306 (* 1 = 0.0318306 loss)
I0711 23:29:39.824167 13090 sgd_solver.cpp:106] Iteration 27200, lr = 1e-06
I0711 23:29:57.856497 13090 solver.cpp:290] Iteration 27300 (5.54574 iter/s, 18.0318s/100 iter), loss = 0.0384612
I0711 23:29:57.856520 13090 solver.cpp:309]     Train net output #0: loss = 0.0384613 (* 1 = 0.0384613 loss)
I0711 23:29:57.856528 13090 sgd_solver.cpp:106] Iteration 27300, lr = 1e-06
I0711 23:30:15.923835 13090 solver.cpp:290] Iteration 27400 (5.53501 iter/s, 18.0668s/100 iter), loss = 0.0214975
I0711 23:30:15.923892 13090 solver.cpp:309]     Train net output #0: loss = 0.0214976 (* 1 = 0.0214976 loss)
I0711 23:30:15.923899 13090 sgd_solver.cpp:106] Iteration 27400, lr = 1e-06
I0711 23:30:33.736333 13090 solver.cpp:290] Iteration 27500 (5.61421 iter/s, 17.8119s/100 iter), loss = 0.0224518
I0711 23:30:33.736378 13090 solver.cpp:309]     Train net output #0: loss = 0.0224519 (* 1 = 0.0224519 loss)
I0711 23:30:33.736394 13090 sgd_solver.cpp:106] Iteration 27500, lr = 1e-06
I0711 23:30:52.483155 13090 solver.cpp:290] Iteration 27600 (5.3344 iter/s, 18.7463s/100 iter), loss = 0.0275236
I0711 23:30:52.483230 13090 solver.cpp:309]     Train net output #0: loss = 0.0275237 (* 1 = 0.0275237 loss)
I0711 23:30:52.483248 13090 sgd_solver.cpp:106] Iteration 27600, lr = 1e-06
I0711 23:31:11.064462 13090 solver.cpp:290] Iteration 27700 (5.38193 iter/s, 18.5807s/100 iter), loss = 0.0416203
I0711 23:31:11.064589 13090 solver.cpp:309]     Train net output #0: loss = 0.0416204 (* 1 = 0.0416204 loss)
I0711 23:31:11.064632 13090 sgd_solver.cpp:106] Iteration 27700, lr = 1e-06
I0711 23:31:28.975713 13090 solver.cpp:290] Iteration 27800 (5.58327 iter/s, 17.9107s/100 iter), loss = 0.0380873
I0711 23:31:28.975801 13090 solver.cpp:309]     Train net output #0: loss = 0.0380874 (* 1 = 0.0380874 loss)
I0711 23:31:28.975812 13090 sgd_solver.cpp:106] Iteration 27800, lr = 1e-06
I0711 23:31:47.009313 13090 solver.cpp:290] Iteration 27900 (5.54538 iter/s, 18.033s/100 iter), loss = 0.0493147
I0711 23:31:47.009362 13090 solver.cpp:309]     Train net output #0: loss = 0.0493148 (* 1 = 0.0493148 loss)
I0711 23:31:47.009376 13090 sgd_solver.cpp:106] Iteration 27900, lr = 1e-06
I0711 23:32:05.078130 13090 solver.cpp:354] Sparsity after update:
I0711 23:32:05.081531 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 23:32:05.081549 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 23:32:05.081565 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 23:32:05.081570 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 23:32:05.081575 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 23:32:05.081580 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 23:32:05.081584 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 23:32:05.081589 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 23:32:05.081594 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 23:32:05.081600 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 23:32:05.081604 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 23:32:05.081610 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 23:32:05.081615 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 23:32:05.081620 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 23:32:05.081622 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 23:32:05.081626 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 23:32:05.081631 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 23:32:05.081636 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 23:32:05.081642 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 23:32:05.081957 13090 solver.cpp:467] Iteration 28000, Testing net (#0)
I0711 23:33:11.183135 13090 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.95093
I0711 23:33:11.183228 13090 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999355
I0711 23:33:11.183235 13090 solver.cpp:540]     Test net output #2: loss = 0.163173 (* 1 = 0.163173 loss)
I0711 23:33:11.373442 13090 solver.cpp:290] Iteration 28000 (1.18537 iter/s, 84.3618s/100 iter), loss = 0.0250614
I0711 23:33:11.373486 13090 solver.cpp:309]     Train net output #0: loss = 0.0250615 (* 1 = 0.0250615 loss)
I0711 23:33:11.373498 13090 sgd_solver.cpp:106] Iteration 28000, lr = 1e-06
I0711 23:33:29.406929 13090 solver.cpp:290] Iteration 28100 (5.54541 iter/s, 18.0329s/100 iter), loss = 0.046102
I0711 23:33:29.406960 13090 solver.cpp:309]     Train net output #0: loss = 0.0461021 (* 1 = 0.0461021 loss)
I0711 23:33:29.406967 13090 sgd_solver.cpp:106] Iteration 28100, lr = 1e-06
I0711 23:33:47.270596 13090 solver.cpp:290] Iteration 28200 (5.59812 iter/s, 17.8631s/100 iter), loss = 0.0366167
I0711 23:33:47.270653 13090 solver.cpp:309]     Train net output #0: loss = 0.0366168 (* 1 = 0.0366168 loss)
I0711 23:33:47.270659 13090 sgd_solver.cpp:106] Iteration 28200, lr = 1e-06
I0711 23:34:05.010177 13090 solver.cpp:290] Iteration 28300 (5.63729 iter/s, 17.739s/100 iter), loss = 0.0262924
I0711 23:34:05.010226 13090 solver.cpp:309]     Train net output #0: loss = 0.0262925 (* 1 = 0.0262925 loss)
I0711 23:34:05.010249 13090 sgd_solver.cpp:106] Iteration 28300, lr = 1e-06
I0711 23:34:23.441498 13090 solver.cpp:290] Iteration 28400 (5.42571 iter/s, 18.4308s/100 iter), loss = 0.0207001
I0711 23:34:23.441577 13090 solver.cpp:309]     Train net output #0: loss = 0.0207002 (* 1 = 0.0207002 loss)
I0711 23:34:23.441592 13090 sgd_solver.cpp:106] Iteration 28400, lr = 1e-06
I0711 23:34:41.792889 13090 solver.cpp:290] Iteration 28500 (5.44935 iter/s, 18.3508s/100 iter), loss = 0.0250474
I0711 23:34:41.792913 13090 solver.cpp:309]     Train net output #0: loss = 0.0250475 (* 1 = 0.0250475 loss)
I0711 23:34:41.792922 13090 sgd_solver.cpp:106] Iteration 28500, lr = 1e-06
I0711 23:34:59.817782 13090 solver.cpp:290] Iteration 28600 (5.54804 iter/s, 18.0244s/100 iter), loss = 0.0287275
I0711 23:34:59.817828 13090 solver.cpp:309]     Train net output #0: loss = 0.0287276 (* 1 = 0.0287276 loss)
I0711 23:34:59.817837 13090 sgd_solver.cpp:106] Iteration 28600, lr = 1e-06
I0711 23:35:17.614001 13090 solver.cpp:290] Iteration 28700 (5.61934 iter/s, 17.7957s/100 iter), loss = 0.031977
I0711 23:35:17.614027 13090 solver.cpp:309]     Train net output #0: loss = 0.0319771 (* 1 = 0.0319771 loss)
I0711 23:35:17.614033 13090 sgd_solver.cpp:106] Iteration 28700, lr = 1e-06
I0711 23:35:35.922763 13090 solver.cpp:290] Iteration 28800 (5.46202 iter/s, 18.3082s/100 iter), loss = 0.0210419
I0711 23:35:35.922845 13090 solver.cpp:309]     Train net output #0: loss = 0.021042 (* 1 = 0.021042 loss)
I0711 23:35:35.922855 13090 sgd_solver.cpp:106] Iteration 28800, lr = 1e-06
I0711 23:35:54.302749 13090 solver.cpp:290] Iteration 28900 (5.44087 iter/s, 18.3794s/100 iter), loss = 0.0232541
I0711 23:35:54.302775 13090 solver.cpp:309]     Train net output #0: loss = 0.0232542 (* 1 = 0.0232542 loss)
I0711 23:35:54.302783 13090 sgd_solver.cpp:106] Iteration 28900, lr = 1e-06
I0711 23:36:11.926832 13090 solver.cpp:354] Sparsity after update:
I0711 23:36:11.990162 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 23:36:11.990188 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 23:36:11.990203 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 23:36:11.990209 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 23:36:11.990213 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 23:36:11.990218 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 23:36:11.990222 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 23:36:11.990226 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 23:36:11.990236 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 23:36:11.990242 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 23:36:11.990247 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 23:36:11.990254 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 23:36:11.990259 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 23:36:11.990267 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 23:36:11.990270 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 23:36:11.990276 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 23:36:11.990281 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 23:36:11.990288 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 23:36:11.990293 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 23:36:12.148118 13090 solver.cpp:290] Iteration 29000 (5.60386 iter/s, 17.8448s/100 iter), loss = 0.0248232
I0711 23:36:12.148241 13090 solver.cpp:309]     Train net output #0: loss = 0.0248232 (* 1 = 0.0248232 loss)
I0711 23:36:12.148353 13090 sgd_solver.cpp:106] Iteration 29000, lr = 1e-06
I0711 23:36:29.751746 13090 solver.cpp:290] Iteration 29100 (5.68084 iter/s, 17.603s/100 iter), loss = 0.0271547
I0711 23:36:29.751801 13090 solver.cpp:309]     Train net output #0: loss = 0.0271548 (* 1 = 0.0271548 loss)
I0711 23:36:29.751822 13090 sgd_solver.cpp:106] Iteration 29100, lr = 1e-06
I0711 23:36:48.223091 13090 solver.cpp:290] Iteration 29200 (5.41396 iter/s, 18.4708s/100 iter), loss = 0.0141449
I0711 23:36:48.223234 13090 solver.cpp:309]     Train net output #0: loss = 0.014145 (* 1 = 0.014145 loss)
I0711 23:36:48.223259 13090 sgd_solver.cpp:106] Iteration 29200, lr = 1e-06
I0711 23:37:06.246634 13090 solver.cpp:290] Iteration 29300 (5.5485 iter/s, 18.0229s/100 iter), loss = 0.0472151
I0711 23:37:06.246693 13090 solver.cpp:309]     Train net output #0: loss = 0.0472152 (* 1 = 0.0472152 loss)
I0711 23:37:06.246717 13090 sgd_solver.cpp:106] Iteration 29300, lr = 1e-06
I0711 23:37:23.842552 13090 solver.cpp:290] Iteration 29400 (5.68332 iter/s, 17.5954s/100 iter), loss = 0.045624
I0711 23:37:23.842764 13090 solver.cpp:309]     Train net output #0: loss = 0.045624 (* 1 = 0.045624 loss)
I0711 23:37:23.842813 13090 sgd_solver.cpp:106] Iteration 29400, lr = 1e-06
I0711 23:37:41.552708 13090 solver.cpp:290] Iteration 29500 (5.64669 iter/s, 17.7095s/100 iter), loss = 0.0240131
I0711 23:37:41.552757 13090 solver.cpp:309]     Train net output #0: loss = 0.0240132 (* 1 = 0.0240132 loss)
I0711 23:37:41.552781 13090 sgd_solver.cpp:106] Iteration 29500, lr = 1e-06
I0711 23:37:59.778403 13090 solver.cpp:290] Iteration 29600 (5.48692 iter/s, 18.2252s/100 iter), loss = 0.0550521
I0711 23:37:59.778477 13090 solver.cpp:309]     Train net output #0: loss = 0.0550522 (* 1 = 0.0550522 loss)
I0711 23:37:59.778484 13090 sgd_solver.cpp:106] Iteration 29600, lr = 1e-06
I0711 23:38:18.012768 13090 solver.cpp:290] Iteration 29700 (5.48432 iter/s, 18.2338s/100 iter), loss = 0.0404446
I0711 23:38:18.012789 13090 solver.cpp:309]     Train net output #0: loss = 0.0404447 (* 1 = 0.0404447 loss)
I0711 23:38:18.012796 13090 sgd_solver.cpp:106] Iteration 29700, lr = 1e-06
I0711 23:38:36.559736 13090 solver.cpp:290] Iteration 29800 (5.39187 iter/s, 18.5464s/100 iter), loss = 0.0233942
I0711 23:38:36.559797 13090 solver.cpp:309]     Train net output #0: loss = 0.0233942 (* 1 = 0.0233942 loss)
I0711 23:38:36.559804 13090 sgd_solver.cpp:106] Iteration 29800, lr = 1e-06
I0711 23:38:54.476697 13090 solver.cpp:290] Iteration 29900 (5.58148 iter/s, 17.9164s/100 iter), loss = 0.0319873
I0711 23:38:54.476724 13090 solver.cpp:309]     Train net output #0: loss = 0.0319874 (* 1 = 0.0319874 loss)
I0711 23:38:54.476733 13090 sgd_solver.cpp:106] Iteration 29900, lr = 1e-06
I0711 23:39:11.356868 13090 solver.cpp:594] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/sparse/cityscapes5_jsegnet21v2_iter_30000.caffemodel
I0711 23:39:11.382587 13090 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/sparse/cityscapes5_jsegnet21v2_iter_30000.solverstate
I0711 23:39:11.400962 13090 solver.cpp:354] Sparsity after update:
I0711 23:39:11.402576 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 23:39:11.402583 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 23:39:11.402591 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 23:39:11.402593 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 23:39:11.402595 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 23:39:11.402597 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 23:39:11.402600 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 23:39:11.402601 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 23:39:11.402603 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 23:39:11.402606 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 23:39:11.402607 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 23:39:11.402608 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 23:39:11.402611 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 23:39:11.402612 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 23:39:11.402614 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 23:39:11.402616 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 23:39:11.402618 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 23:39:11.402621 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 23:39:11.402622 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 23:39:11.402767 13090 solver.cpp:467] Iteration 30000, Testing net (#0)
I0711 23:39:57.948081 13090 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.950868
I0711 23:39:57.948151 13090 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.999455
I0711 23:39:57.948158 13090 solver.cpp:540]     Test net output #2: loss = 0.160663 (* 1 = 0.160663 loss)
I0711 23:39:58.146100 13090 solver.cpp:290] Iteration 30000 (1.57066 iter/s, 63.6676s/100 iter), loss = 0.0203139
I0711 23:39:58.146122 13090 solver.cpp:309]     Train net output #0: loss = 0.0203139 (* 1 = 0.0203139 loss)
I0711 23:39:58.146129 13090 sgd_solver.cpp:106] Iteration 30000, lr = 1e-06
I0711 23:40:15.204337 13090 solver.cpp:290] Iteration 30100 (5.86244 iter/s, 17.0577s/100 iter), loss = 0.0353606
I0711 23:40:15.204363 13090 solver.cpp:309]     Train net output #0: loss = 0.0353607 (* 1 = 0.0353607 loss)
I0711 23:40:15.204372 13090 sgd_solver.cpp:106] Iteration 30100, lr = 1e-06
I0711 23:40:32.237046 13090 solver.cpp:290] Iteration 30200 (5.87123 iter/s, 17.0322s/100 iter), loss = 0.0456484
I0711 23:40:32.237154 13090 solver.cpp:309]     Train net output #0: loss = 0.0456484 (* 1 = 0.0456484 loss)
I0711 23:40:32.237165 13090 sgd_solver.cpp:106] Iteration 30200, lr = 1e-06
I0711 23:40:49.418305 13090 solver.cpp:290] Iteration 30300 (5.82049 iter/s, 17.1807s/100 iter), loss = 0.0527802
I0711 23:40:49.418336 13090 solver.cpp:309]     Train net output #0: loss = 0.0527803 (* 1 = 0.0527803 loss)
I0711 23:40:49.418345 13090 sgd_solver.cpp:106] Iteration 30300, lr = 1e-06
I0711 23:41:06.613790 13090 solver.cpp:290] Iteration 30400 (5.81565 iter/s, 17.195s/100 iter), loss = 0.0269117
I0711 23:41:06.613869 13090 solver.cpp:309]     Train net output #0: loss = 0.0269118 (* 1 = 0.0269118 loss)
I0711 23:41:06.613878 13090 sgd_solver.cpp:106] Iteration 30400, lr = 1e-06
I0711 23:41:23.730533 13090 solver.cpp:290] Iteration 30500 (5.84242 iter/s, 17.1162s/100 iter), loss = 0.016581
I0711 23:41:23.730561 13090 solver.cpp:309]     Train net output #0: loss = 0.0165811 (* 1 = 0.0165811 loss)
I0711 23:41:23.730571 13090 sgd_solver.cpp:106] Iteration 30500, lr = 1e-06
I0711 23:41:40.771013 13090 solver.cpp:290] Iteration 30600 (5.86855 iter/s, 17.04s/100 iter), loss = 0.0183129
I0711 23:41:40.771123 13090 solver.cpp:309]     Train net output #0: loss = 0.018313 (* 1 = 0.018313 loss)
I0711 23:41:40.771133 13090 sgd_solver.cpp:106] Iteration 30600, lr = 1e-06
I0711 23:41:57.770750 13090 solver.cpp:290] Iteration 30700 (5.88264 iter/s, 16.9992s/100 iter), loss = 0.0298751
I0711 23:41:57.770772 13090 solver.cpp:309]     Train net output #0: loss = 0.0298751 (* 1 = 0.0298751 loss)
I0711 23:41:57.770779 13090 sgd_solver.cpp:106] Iteration 30700, lr = 1e-06
I0711 23:42:14.869175 13090 solver.cpp:290] Iteration 30800 (5.84866 iter/s, 17.0979s/100 iter), loss = 0.0273925
I0711 23:42:14.869226 13090 solver.cpp:309]     Train net output #0: loss = 0.0273926 (* 1 = 0.0273926 loss)
I0711 23:42:14.869233 13090 sgd_solver.cpp:106] Iteration 30800, lr = 1e-06
I0711 23:42:32.720424 13090 solver.cpp:290] Iteration 30900 (5.60202 iter/s, 17.8507s/100 iter), loss = 0.0397752
I0711 23:42:32.720453 13090 solver.cpp:309]     Train net output #0: loss = 0.0397753 (* 1 = 0.0397753 loss)
I0711 23:42:32.720466 13090 sgd_solver.cpp:106] Iteration 30900, lr = 1e-06
I0711 23:42:50.134551 13090 solver.cpp:354] Sparsity after update:
I0711 23:42:50.203680 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 23:42:50.203698 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 23:42:50.203704 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 23:42:50.203707 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 23:42:50.203709 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 23:42:50.203711 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 23:42:50.203713 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 23:42:50.203716 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 23:42:50.203722 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 23:42:50.203727 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 23:42:50.203729 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 23:42:50.203732 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 23:42:50.203734 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 23:42:50.203737 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 23:42:50.203739 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 23:42:50.203742 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 23:42:50.203744 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 23:42:50.203747 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 23:42:50.203748 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 23:42:50.358139 13090 solver.cpp:290] Iteration 31000 (5.66983 iter/s, 17.6372s/100 iter), loss = 0.0201152
I0711 23:42:50.358165 13090 solver.cpp:309]     Train net output #0: loss = 0.0201153 (* 1 = 0.0201153 loss)
I0711 23:42:50.358170 13090 sgd_solver.cpp:106] Iteration 31000, lr = 1e-06
I0711 23:43:07.370579 13090 solver.cpp:290] Iteration 31100 (5.87822 iter/s, 17.0119s/100 iter), loss = 0.032176
I0711 23:43:07.370605 13090 solver.cpp:309]     Train net output #0: loss = 0.0321761 (* 1 = 0.0321761 loss)
I0711 23:43:07.370615 13090 sgd_solver.cpp:106] Iteration 31100, lr = 1e-06
I0711 23:43:24.326025 13090 solver.cpp:290] Iteration 31200 (5.89798 iter/s, 16.955s/100 iter), loss = 0.037313
I0711 23:43:24.326081 13090 solver.cpp:309]     Train net output #0: loss = 0.037313 (* 1 = 0.037313 loss)
I0711 23:43:24.326093 13090 sgd_solver.cpp:106] Iteration 31200, lr = 1e-06
I0711 23:43:41.221808 13090 solver.cpp:290] Iteration 31300 (5.91882 iter/s, 16.8953s/100 iter), loss = 0.0450762
I0711 23:43:41.221830 13090 solver.cpp:309]     Train net output #0: loss = 0.0450763 (* 1 = 0.0450763 loss)
I0711 23:43:41.221837 13090 sgd_solver.cpp:106] Iteration 31300, lr = 1e-06
I0711 23:43:58.296365 13090 solver.cpp:290] Iteration 31400 (5.85684 iter/s, 17.0741s/100 iter), loss = 0.027245
I0711 23:43:58.296440 13090 solver.cpp:309]     Train net output #0: loss = 0.027245 (* 1 = 0.027245 loss)
I0711 23:43:58.296449 13090 sgd_solver.cpp:106] Iteration 31400, lr = 1e-06
I0711 23:44:15.368424 13090 solver.cpp:290] Iteration 31500 (5.85771 iter/s, 17.0715s/100 iter), loss = 0.0222524
I0711 23:44:15.368446 13090 solver.cpp:309]     Train net output #0: loss = 0.0222524 (* 1 = 0.0222524 loss)
I0711 23:44:15.368453 13090 sgd_solver.cpp:106] Iteration 31500, lr = 1e-06
I0711 23:44:32.424859 13090 solver.cpp:290] Iteration 31600 (5.86306 iter/s, 17.0559s/100 iter), loss = 0.0344909
I0711 23:44:32.424937 13090 solver.cpp:309]     Train net output #0: loss = 0.034491 (* 1 = 0.034491 loss)
I0711 23:44:32.424947 13090 sgd_solver.cpp:106] Iteration 31600, lr = 1e-06
I0711 23:44:49.544421 13090 solver.cpp:290] Iteration 31700 (5.84145 iter/s, 17.119s/100 iter), loss = 0.0668461
I0711 23:44:49.544445 13090 solver.cpp:309]     Train net output #0: loss = 0.0668462 (* 1 = 0.0668462 loss)
I0711 23:44:49.544450 13090 sgd_solver.cpp:106] Iteration 31700, lr = 1e-06
I0711 23:45:06.451910 13090 solver.cpp:290] Iteration 31800 (5.91471 iter/s, 16.907s/100 iter), loss = 0.0189023
I0711 23:45:06.451983 13090 solver.cpp:309]     Train net output #0: loss = 0.0189023 (* 1 = 0.0189023 loss)
I0711 23:45:06.451992 13090 sgd_solver.cpp:106] Iteration 31800, lr = 1e-06
I0711 23:45:23.665891 13090 solver.cpp:290] Iteration 31900 (5.80941 iter/s, 17.2134s/100 iter), loss = 0.042641
I0711 23:45:23.665917 13090 solver.cpp:309]     Train net output #0: loss = 0.0426411 (* 1 = 0.0426411 loss)
I0711 23:45:23.665926 13090 sgd_solver.cpp:106] Iteration 31900, lr = 1e-06
I0711 23:45:40.588008 13090 solver.cpp:354] Sparsity after update:
I0711 23:45:40.589913 13090 net.cpp:1925] Num Params(17), Sparsity (zero_weights/count): 
I0711 23:45:40.589922 13090 net.cpp:1934] conv1a_param_0(0.322) 
I0711 23:45:40.589931 13090 net.cpp:1934] conv1b_param_0(0.632) 
I0711 23:45:40.589936 13090 net.cpp:1934] ctx_conv1_param_0(0.592) 
I0711 23:45:40.589939 13090 net.cpp:1934] ctx_conv2_param_0(0.617) 
I0711 23:45:40.589943 13090 net.cpp:1934] ctx_conv3_param_0(0.608) 
I0711 23:45:40.589948 13090 net.cpp:1934] ctx_conv4_param_0(0.629) 
I0711 23:45:40.589951 13090 net.cpp:1934] ctx_final_param_0(0.25) 
I0711 23:45:40.589956 13090 net.cpp:1934] out3a_param_0(0.71) 
I0711 23:45:40.589960 13090 net.cpp:1934] out5a_param_0(0.758) 
I0711 23:45:40.589964 13090 net.cpp:1934] res2a_branch2a_param_0(0.761) 
I0711 23:45:40.589968 13090 net.cpp:1934] res2a_branch2b_param_0(0.62) 
I0711 23:45:40.589972 13090 net.cpp:1934] res3a_branch2a_param_0(0.775) 
I0711 23:45:40.589977 13090 net.cpp:1934] res3a_branch2b_param_0(0.688) 
I0711 23:45:40.589982 13090 net.cpp:1934] res4a_branch2a_param_0(0.792) 
I0711 23:45:40.589985 13090 net.cpp:1934] res4a_branch2b_param_0(0.775) 
I0711 23:45:40.589990 13090 net.cpp:1934] res5a_branch2a_param_0(0.8) 
I0711 23:45:40.589994 13090 net.cpp:1934] res5a_branch2b_param_0(0.799) 
I0711 23:45:40.589998 13090 net.cpp:1936] Total Sparsity (zero_weights/count) =  (2.09653e+06/2.69117e+06) 0.779
I0711 23:45:40.590011 13090 solver.cpp:594] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/sparse/cityscapes5_jsegnet21v2_iter_32000.caffemodel
I0711 23:45:40.617529 13090 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/sparse/cityscapes5_jsegnet21v2_iter_32000.solverstate
I0711 23:45:40.678750 13090 solver.cpp:447] Iteration 32000, loss = 0.0343516
I0711 23:45:40.678772 13090 solver.cpp:467] Iteration 32000, Testing net (#0)
I0711 23:46:44.827744 13090 solver.cpp:540]     Test net output #0: accuracy/top1 = 0.951162
I0711 23:46:44.827855 13090 solver.cpp:540]     Test net output #1: accuracy/top5 = 0.99937
I0711 23:46:44.827863 13090 solver.cpp:540]     Test net output #2: loss = 0.161306 (* 1 = 0.161306 loss)
I0711 23:46:44.827867 13090 solver.cpp:452] Optimization Done.
I0711 23:46:45.112176 13090 caffe.cpp:246] Optimization Done.
training/cityscapes5_jsegnet21v2_2017-07-11_18-09-28/test
I0711 23:46:55.830932 18382 caffe.cpp:264] Not using GPU #2 for single-GPU function
I0711 23:46:55.831058 18382 caffe.cpp:264] Not using GPU #1 for single-GPU function
I0711 23:46:56.812490 18382 caffe.cpp:273] Use GPU with device ID 0
I0711 23:46:56.813412 18382 caffe.cpp:277] GPU device name: GeForce GTX 1080
I0711 23:46:57.762310 18382 net.cpp:56] Initializing net from parameters: 
name: "jsegnet21v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/val-image-lmdb"
    label_list_path: "data/val-label-lmdb"
    batch_size: 4
    threads: 4
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b/bn"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a/bn"
  top: "out5a/bn"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a/bn"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b/bn"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a/bn"
  top: "out3a/bn"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a/bn"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv1/bn"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv2/bn"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv3/bn"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4/bn"
  top: "ctx_conv4/bn"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4/bn"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
  accuracy_param {
    ignore_label: 255
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
    ignore_label: 255
  }
}
I0711 23:46:57.770180 18382 layer_factory.hpp:77] Creating layer data
I0711 23:46:57.770344 18382 net.cpp:98] Creating Layer data
I0711 23:46:57.770372 18382 net.cpp:413] data -> data
I0711 23:46:57.770453 18382 net.cpp:413] data -> label
I0711 23:46:57.800494 18439 db_lmdb.cpp:35] Opened lmdb data/val-image-lmdb
I0711 23:46:57.804539 18382 data_layer.cpp:78] ReshapePrefetch 4, 3, 640, 640
I0711 23:46:57.804584 18382 data_layer.cpp:83] output data size: 4,3,640,640
I0711 23:46:57.813127 18444 db_lmdb.cpp:35] Opened lmdb data/val-label-lmdb
I0711 23:46:57.829485 18382 data_layer.cpp:78] ReshapePrefetch 4, 1, 640, 640
I0711 23:46:57.829548 18382 data_layer.cpp:83] output data size: 4,1,640,640
I0711 23:46:57.843083 18382 net.cpp:148] Setting up data
I0711 23:46:57.843116 18382 net.cpp:155] Top shape: 4 3 640 640 (4915200)
I0711 23:46:57.843122 18382 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0711 23:46:57.843124 18382 net.cpp:163] Memory required for data: 26214400
I0711 23:46:57.843137 18382 layer_factory.hpp:77] Creating layer label_data_1_split
I0711 23:46:57.843152 18382 net.cpp:98] Creating Layer label_data_1_split
I0711 23:46:57.843158 18382 net.cpp:439] label_data_1_split <- label
I0711 23:46:57.843178 18382 net.cpp:413] label_data_1_split -> label_data_1_split_0
I0711 23:46:57.843189 18382 net.cpp:413] label_data_1_split -> label_data_1_split_1
I0711 23:46:57.843194 18382 net.cpp:413] label_data_1_split -> label_data_1_split_2
I0711 23:46:57.843272 18382 net.cpp:148] Setting up label_data_1_split
I0711 23:46:57.843303 18382 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0711 23:46:57.843317 18382 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0711 23:46:57.843329 18382 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0711 23:46:57.843339 18382 net.cpp:163] Memory required for data: 45875200
I0711 23:46:57.843351 18382 layer_factory.hpp:77] Creating layer data/bias
I0711 23:46:57.843376 18382 net.cpp:98] Creating Layer data/bias
I0711 23:46:57.843389 18382 net.cpp:439] data/bias <- data
I0711 23:46:57.843401 18382 net.cpp:413] data/bias -> data/bias
I0711 23:46:57.844588 18382 net.cpp:148] Setting up data/bias
I0711 23:46:57.844640 18382 net.cpp:155] Top shape: 4 3 640 640 (4915200)
I0711 23:46:57.844655 18382 net.cpp:163] Memory required for data: 65536000
I0711 23:46:57.844683 18382 layer_factory.hpp:77] Creating layer conv1a
I0711 23:46:57.844712 18382 net.cpp:98] Creating Layer conv1a
I0711 23:46:57.844727 18382 net.cpp:439] conv1a <- data/bias
I0711 23:46:57.844741 18382 net.cpp:413] conv1a -> conv1a
I0711 23:46:57.847559 18382 net.cpp:148] Setting up conv1a
I0711 23:46:57.847666 18382 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0711 23:46:57.847676 18382 net.cpp:163] Memory required for data: 117964800
I0711 23:46:57.847704 18382 layer_factory.hpp:77] Creating layer conv1a/bn
I0711 23:46:57.847795 18382 net.cpp:98] Creating Layer conv1a/bn
I0711 23:46:57.847815 18382 net.cpp:439] conv1a/bn <- conv1a
I0711 23:46:57.847836 18382 net.cpp:413] conv1a/bn -> conv1a/bn
I0711 23:46:57.851512 18382 net.cpp:148] Setting up conv1a/bn
I0711 23:46:57.851578 18382 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0711 23:46:57.851589 18382 net.cpp:163] Memory required for data: 170393600
I0711 23:46:57.851614 18382 layer_factory.hpp:77] Creating layer conv1a/relu
I0711 23:46:57.851635 18382 net.cpp:98] Creating Layer conv1a/relu
I0711 23:46:57.851658 18382 net.cpp:439] conv1a/relu <- conv1a/bn
I0711 23:46:57.851675 18382 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0711 23:46:57.851716 18382 net.cpp:148] Setting up conv1a/relu
I0711 23:46:57.851730 18382 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0711 23:46:57.851742 18382 net.cpp:163] Memory required for data: 222822400
I0711 23:46:57.851752 18382 layer_factory.hpp:77] Creating layer conv1b
I0711 23:46:57.851781 18382 net.cpp:98] Creating Layer conv1b
I0711 23:46:57.851805 18382 net.cpp:439] conv1b <- conv1a/bn
I0711 23:46:57.851840 18382 net.cpp:413] conv1b -> conv1b
I0711 23:46:57.852273 18382 net.cpp:148] Setting up conv1b
I0711 23:46:57.852283 18382 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0711 23:46:57.852286 18382 net.cpp:163] Memory required for data: 275251200
I0711 23:46:57.852293 18382 layer_factory.hpp:77] Creating layer conv1b/bn
I0711 23:46:57.852301 18382 net.cpp:98] Creating Layer conv1b/bn
I0711 23:46:57.852305 18382 net.cpp:439] conv1b/bn <- conv1b
I0711 23:46:57.852311 18382 net.cpp:413] conv1b/bn -> conv1b/bn
I0711 23:46:57.854918 18382 net.cpp:148] Setting up conv1b/bn
I0711 23:46:57.854938 18382 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0711 23:46:57.854945 18382 net.cpp:163] Memory required for data: 327680000
I0711 23:46:57.855026 18382 layer_factory.hpp:77] Creating layer conv1b/relu
I0711 23:46:57.855060 18382 net.cpp:98] Creating Layer conv1b/relu
I0711 23:46:57.855088 18382 net.cpp:439] conv1b/relu <- conv1b/bn
I0711 23:46:57.855119 18382 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0711 23:46:57.855151 18382 net.cpp:148] Setting up conv1b/relu
I0711 23:46:57.855181 18382 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0711 23:46:57.855207 18382 net.cpp:163] Memory required for data: 380108800
I0711 23:46:57.855234 18382 layer_factory.hpp:77] Creating layer pool1
I0711 23:46:57.855284 18382 net.cpp:98] Creating Layer pool1
I0711 23:46:57.855315 18382 net.cpp:439] pool1 <- conv1b/bn
I0711 23:46:57.855348 18382 net.cpp:413] pool1 -> pool1
I0711 23:46:57.855834 18382 net.cpp:148] Setting up pool1
I0711 23:46:57.855865 18382 net.cpp:155] Top shape: 4 32 160 160 (3276800)
I0711 23:46:57.855888 18382 net.cpp:163] Memory required for data: 393216000
I0711 23:46:57.855916 18382 layer_factory.hpp:77] Creating layer res2a_branch2a
I0711 23:46:57.855962 18382 net.cpp:98] Creating Layer res2a_branch2a
I0711 23:46:57.855988 18382 net.cpp:439] res2a_branch2a <- pool1
I0711 23:46:57.856022 18382 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0711 23:46:57.859598 18382 net.cpp:148] Setting up res2a_branch2a
I0711 23:46:57.859673 18382 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0711 23:46:57.859697 18382 net.cpp:163] Memory required for data: 419430400
I0711 23:46:57.859740 18382 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0711 23:46:57.859783 18382 net.cpp:98] Creating Layer res2a_branch2a/bn
I0711 23:46:57.859810 18382 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0711 23:46:57.859830 18382 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0711 23:46:57.860581 18382 net.cpp:148] Setting up res2a_branch2a/bn
I0711 23:46:57.860610 18382 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0711 23:46:57.860622 18382 net.cpp:163] Memory required for data: 445644800
I0711 23:46:57.860642 18382 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0711 23:46:57.860656 18382 net.cpp:98] Creating Layer res2a_branch2a/relu
I0711 23:46:57.860668 18382 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0711 23:46:57.860682 18382 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0711 23:46:57.860702 18382 net.cpp:148] Setting up res2a_branch2a/relu
I0711 23:46:57.860716 18382 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0711 23:46:57.860729 18382 net.cpp:163] Memory required for data: 471859200
I0711 23:46:57.860743 18382 layer_factory.hpp:77] Creating layer res2a_branch2b
I0711 23:46:57.860770 18382 net.cpp:98] Creating Layer res2a_branch2b
I0711 23:46:57.860785 18382 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0711 23:46:57.860803 18382 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0711 23:46:57.862709 18382 net.cpp:148] Setting up res2a_branch2b
I0711 23:46:57.862778 18382 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0711 23:46:57.862794 18382 net.cpp:163] Memory required for data: 498073600
I0711 23:46:57.862812 18382 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0711 23:46:57.862835 18382 net.cpp:98] Creating Layer res2a_branch2b/bn
I0711 23:46:57.862874 18382 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0711 23:46:57.862916 18382 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0711 23:46:57.863487 18382 net.cpp:148] Setting up res2a_branch2b/bn
I0711 23:46:57.863503 18382 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0711 23:46:57.863512 18382 net.cpp:163] Memory required for data: 524288000
I0711 23:46:57.863526 18382 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0711 23:46:57.863538 18382 net.cpp:98] Creating Layer res2a_branch2b/relu
I0711 23:46:57.863545 18382 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0711 23:46:57.863554 18382 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0711 23:46:57.863565 18382 net.cpp:148] Setting up res2a_branch2b/relu
I0711 23:46:57.863574 18382 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0711 23:46:57.863582 18382 net.cpp:163] Memory required for data: 550502400
I0711 23:46:57.863590 18382 layer_factory.hpp:77] Creating layer pool2
I0711 23:46:57.863601 18382 net.cpp:98] Creating Layer pool2
I0711 23:46:57.863610 18382 net.cpp:439] pool2 <- res2a_branch2b/bn
I0711 23:46:57.863618 18382 net.cpp:413] pool2 -> pool2
I0711 23:46:57.863656 18382 net.cpp:148] Setting up pool2
I0711 23:46:57.863667 18382 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 23:46:57.863675 18382 net.cpp:163] Memory required for data: 557056000
I0711 23:46:57.863683 18382 layer_factory.hpp:77] Creating layer res3a_branch2a
I0711 23:46:57.863694 18382 net.cpp:98] Creating Layer res3a_branch2a
I0711 23:46:57.863703 18382 net.cpp:439] res3a_branch2a <- pool2
I0711 23:46:57.863714 18382 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0711 23:46:57.865999 18382 net.cpp:148] Setting up res3a_branch2a
I0711 23:46:57.866024 18382 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 23:46:57.866051 18382 net.cpp:163] Memory required for data: 570163200
I0711 23:46:57.866063 18382 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0711 23:46:57.866077 18382 net.cpp:98] Creating Layer res3a_branch2a/bn
I0711 23:46:57.866086 18382 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0711 23:46:57.866096 18382 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0711 23:46:57.866467 18382 net.cpp:148] Setting up res3a_branch2a/bn
I0711 23:46:57.866483 18382 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 23:46:57.866490 18382 net.cpp:163] Memory required for data: 583270400
I0711 23:46:57.866506 18382 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0711 23:46:57.866518 18382 net.cpp:98] Creating Layer res3a_branch2a/relu
I0711 23:46:57.866526 18382 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0711 23:46:57.866535 18382 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0711 23:46:57.866545 18382 net.cpp:148] Setting up res3a_branch2a/relu
I0711 23:46:57.866554 18382 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 23:46:57.866562 18382 net.cpp:163] Memory required for data: 596377600
I0711 23:46:57.866570 18382 layer_factory.hpp:77] Creating layer res3a_branch2b
I0711 23:46:57.866583 18382 net.cpp:98] Creating Layer res3a_branch2b
I0711 23:46:57.866591 18382 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0711 23:46:57.866600 18382 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0711 23:46:57.868412 18382 net.cpp:148] Setting up res3a_branch2b
I0711 23:46:57.868451 18382 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 23:46:57.868465 18382 net.cpp:163] Memory required for data: 609484800
I0711 23:46:57.868481 18382 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0711 23:46:57.868502 18382 net.cpp:98] Creating Layer res3a_branch2b/bn
I0711 23:46:57.868516 18382 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0711 23:46:57.868546 18382 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0711 23:46:57.869007 18382 net.cpp:148] Setting up res3a_branch2b/bn
I0711 23:46:57.869030 18382 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 23:46:57.869043 18382 net.cpp:163] Memory required for data: 622592000
I0711 23:46:57.869062 18382 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0711 23:46:57.869094 18382 net.cpp:98] Creating Layer res3a_branch2b/relu
I0711 23:46:57.869132 18382 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0711 23:46:57.869145 18382 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0711 23:46:57.869160 18382 net.cpp:148] Setting up res3a_branch2b/relu
I0711 23:46:57.869174 18382 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 23:46:57.869184 18382 net.cpp:163] Memory required for data: 635699200
I0711 23:46:57.869201 18382 layer_factory.hpp:77] Creating layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0711 23:46:57.869217 18382 net.cpp:98] Creating Layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0711 23:46:57.869230 18382 net.cpp:439] res3a_branch2b/bn_res3a_branch2b/relu_0_split <- res3a_branch2b/bn
I0711 23:46:57.869243 18382 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0711 23:46:57.869256 18382 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0711 23:46:57.869304 18382 net.cpp:148] Setting up res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0711 23:46:57.869321 18382 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 23:46:57.869334 18382 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0711 23:46:57.869345 18382 net.cpp:163] Memory required for data: 661913600
I0711 23:46:57.869357 18382 layer_factory.hpp:77] Creating layer pool3
I0711 23:46:57.869374 18382 net.cpp:98] Creating Layer pool3
I0711 23:46:57.869386 18382 net.cpp:439] pool3 <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0711 23:46:57.869401 18382 net.cpp:413] pool3 -> pool3
I0711 23:46:57.869448 18382 net.cpp:148] Setting up pool3
I0711 23:46:57.869463 18382 net.cpp:155] Top shape: 4 128 40 40 (819200)
I0711 23:46:57.869475 18382 net.cpp:163] Memory required for data: 665190400
I0711 23:46:57.869488 18382 layer_factory.hpp:77] Creating layer res4a_branch2a
I0711 23:46:57.869506 18382 net.cpp:98] Creating Layer res4a_branch2a
I0711 23:46:57.869519 18382 net.cpp:439] res4a_branch2a <- pool3
I0711 23:46:57.869532 18382 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0711 23:46:57.878399 18382 net.cpp:148] Setting up res4a_branch2a
I0711 23:46:57.878444 18382 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0711 23:46:57.878454 18382 net.cpp:163] Memory required for data: 671744000
I0711 23:46:57.878473 18382 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0711 23:46:57.878500 18382 net.cpp:98] Creating Layer res4a_branch2a/bn
I0711 23:46:57.878535 18382 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0711 23:46:57.878573 18382 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0711 23:46:57.879570 18382 net.cpp:148] Setting up res4a_branch2a/bn
I0711 23:46:57.879585 18382 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0711 23:46:57.879590 18382 net.cpp:163] Memory required for data: 678297600
I0711 23:46:57.879604 18382 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0711 23:46:57.879617 18382 net.cpp:98] Creating Layer res4a_branch2a/relu
I0711 23:46:57.879626 18382 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0711 23:46:57.879634 18382 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0711 23:46:57.879647 18382 net.cpp:148] Setting up res4a_branch2a/relu
I0711 23:46:57.879652 18382 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0711 23:46:57.879655 18382 net.cpp:163] Memory required for data: 684851200
I0711 23:46:57.879659 18382 layer_factory.hpp:77] Creating layer res4a_branch2b
I0711 23:46:57.879675 18382 net.cpp:98] Creating Layer res4a_branch2b
I0711 23:46:57.879679 18382 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0711 23:46:57.879686 18382 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0711 23:46:57.884073 18382 net.cpp:148] Setting up res4a_branch2b
I0711 23:46:57.884141 18382 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0711 23:46:57.884152 18382 net.cpp:163] Memory required for data: 691404800
I0711 23:46:57.884181 18382 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0711 23:46:57.884202 18382 net.cpp:98] Creating Layer res4a_branch2b/bn
I0711 23:46:57.884224 18382 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0711 23:46:57.884235 18382 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0711 23:46:57.884632 18382 net.cpp:148] Setting up res4a_branch2b/bn
I0711 23:46:57.884647 18382 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0711 23:46:57.884655 18382 net.cpp:163] Memory required for data: 697958400
I0711 23:46:57.884667 18382 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0711 23:46:57.884676 18382 net.cpp:98] Creating Layer res4a_branch2b/relu
I0711 23:46:57.884685 18382 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0711 23:46:57.884692 18382 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0711 23:46:57.884702 18382 net.cpp:148] Setting up res4a_branch2b/relu
I0711 23:46:57.884711 18382 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0711 23:46:57.884717 18382 net.cpp:163] Memory required for data: 704512000
I0711 23:46:57.884724 18382 layer_factory.hpp:77] Creating layer pool4
I0711 23:46:57.884735 18382 net.cpp:98] Creating Layer pool4
I0711 23:46:57.884742 18382 net.cpp:439] pool4 <- res4a_branch2b/bn
I0711 23:46:57.884750 18382 net.cpp:413] pool4 -> pool4
I0711 23:46:57.884783 18382 net.cpp:148] Setting up pool4
I0711 23:46:57.884793 18382 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0711 23:46:57.884799 18382 net.cpp:163] Memory required for data: 711065600
I0711 23:46:57.884807 18382 layer_factory.hpp:77] Creating layer res5a_branch2a
I0711 23:46:57.884827 18382 net.cpp:98] Creating Layer res5a_branch2a
I0711 23:46:57.884836 18382 net.cpp:439] res5a_branch2a <- pool4
I0711 23:46:57.884845 18382 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0711 23:46:57.911284 18382 net.cpp:148] Setting up res5a_branch2a
I0711 23:46:57.911301 18382 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0711 23:46:57.911303 18382 net.cpp:163] Memory required for data: 724172800
I0711 23:46:57.911309 18382 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0711 23:46:57.911317 18382 net.cpp:98] Creating Layer res5a_branch2a/bn
I0711 23:46:57.911320 18382 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0711 23:46:57.911324 18382 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0711 23:46:57.911603 18382 net.cpp:148] Setting up res5a_branch2a/bn
I0711 23:46:57.911608 18382 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0711 23:46:57.911610 18382 net.cpp:163] Memory required for data: 737280000
I0711 23:46:57.911615 18382 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0711 23:46:57.911618 18382 net.cpp:98] Creating Layer res5a_branch2a/relu
I0711 23:46:57.911620 18382 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0711 23:46:57.911623 18382 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0711 23:46:57.911628 18382 net.cpp:148] Setting up res5a_branch2a/relu
I0711 23:46:57.911629 18382 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0711 23:46:57.911631 18382 net.cpp:163] Memory required for data: 750387200
I0711 23:46:57.911633 18382 layer_factory.hpp:77] Creating layer res5a_branch2b
I0711 23:46:57.911638 18382 net.cpp:98] Creating Layer res5a_branch2b
I0711 23:46:57.911640 18382 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0711 23:46:57.911643 18382 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0711 23:46:57.923951 18382 net.cpp:148] Setting up res5a_branch2b
I0711 23:46:57.923971 18382 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0711 23:46:57.923974 18382 net.cpp:163] Memory required for data: 763494400
I0711 23:46:57.923982 18382 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0711 23:46:57.923990 18382 net.cpp:98] Creating Layer res5a_branch2b/bn
I0711 23:46:57.923992 18382 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0711 23:46:57.923997 18382 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0711 23:46:57.924288 18382 net.cpp:148] Setting up res5a_branch2b/bn
I0711 23:46:57.924293 18382 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0711 23:46:57.924295 18382 net.cpp:163] Memory required for data: 776601600
I0711 23:46:57.924300 18382 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0711 23:46:57.924314 18382 net.cpp:98] Creating Layer res5a_branch2b/relu
I0711 23:46:57.924317 18382 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0711 23:46:57.924320 18382 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0711 23:46:57.924325 18382 net.cpp:148] Setting up res5a_branch2b/relu
I0711 23:46:57.924329 18382 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0711 23:46:57.924330 18382 net.cpp:163] Memory required for data: 789708800
I0711 23:46:57.924332 18382 layer_factory.hpp:77] Creating layer out5a
I0711 23:46:57.924337 18382 net.cpp:98] Creating Layer out5a
I0711 23:46:57.924340 18382 net.cpp:439] out5a <- res5a_branch2b/bn
I0711 23:46:57.924342 18382 net.cpp:413] out5a -> out5a
I0711 23:46:57.927986 18382 net.cpp:148] Setting up out5a
I0711 23:46:57.927994 18382 net.cpp:155] Top shape: 4 64 40 40 (409600)
I0711 23:46:57.927997 18382 net.cpp:163] Memory required for data: 791347200
I0711 23:46:57.928000 18382 layer_factory.hpp:77] Creating layer out5a/bn
I0711 23:46:57.928005 18382 net.cpp:98] Creating Layer out5a/bn
I0711 23:46:57.928007 18382 net.cpp:439] out5a/bn <- out5a
I0711 23:46:57.928010 18382 net.cpp:413] out5a/bn -> out5a/bn
I0711 23:46:57.928309 18382 net.cpp:148] Setting up out5a/bn
I0711 23:46:57.928314 18382 net.cpp:155] Top shape: 4 64 40 40 (409600)
I0711 23:46:57.928316 18382 net.cpp:163] Memory required for data: 792985600
I0711 23:46:57.928321 18382 layer_factory.hpp:77] Creating layer out5a/relu
I0711 23:46:57.928323 18382 net.cpp:98] Creating Layer out5a/relu
I0711 23:46:57.928325 18382 net.cpp:439] out5a/relu <- out5a/bn
I0711 23:46:57.928328 18382 net.cpp:400] out5a/relu -> out5a/bn (in-place)
I0711 23:46:57.928331 18382 net.cpp:148] Setting up out5a/relu
I0711 23:46:57.928334 18382 net.cpp:155] Top shape: 4 64 40 40 (409600)
I0711 23:46:57.928335 18382 net.cpp:163] Memory required for data: 794624000
I0711 23:46:57.928338 18382 layer_factory.hpp:77] Creating layer out5a_up2
I0711 23:46:57.928341 18382 net.cpp:98] Creating Layer out5a_up2
I0711 23:46:57.928344 18382 net.cpp:439] out5a_up2 <- out5a/bn
I0711 23:46:57.928345 18382 net.cpp:413] out5a_up2 -> out5a_up2
I0711 23:46:57.928467 18382 net.cpp:148] Setting up out5a_up2
I0711 23:46:57.928472 18382 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 23:46:57.928473 18382 net.cpp:163] Memory required for data: 801177600
I0711 23:46:57.928477 18382 layer_factory.hpp:77] Creating layer out3a
I0711 23:46:57.928480 18382 net.cpp:98] Creating Layer out3a
I0711 23:46:57.928483 18382 net.cpp:439] out3a <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0711 23:46:57.928485 18382 net.cpp:413] out3a -> out3a
I0711 23:46:57.929353 18382 net.cpp:148] Setting up out3a
I0711 23:46:57.929359 18382 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 23:46:57.929361 18382 net.cpp:163] Memory required for data: 807731200
I0711 23:46:57.929364 18382 layer_factory.hpp:77] Creating layer out3a/bn
I0711 23:46:57.929368 18382 net.cpp:98] Creating Layer out3a/bn
I0711 23:46:57.929370 18382 net.cpp:439] out3a/bn <- out3a
I0711 23:46:57.929373 18382 net.cpp:413] out3a/bn -> out3a/bn
I0711 23:46:57.929674 18382 net.cpp:148] Setting up out3a/bn
I0711 23:46:57.929679 18382 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 23:46:57.929682 18382 net.cpp:163] Memory required for data: 814284800
I0711 23:46:57.929687 18382 layer_factory.hpp:77] Creating layer out3a/relu
I0711 23:46:57.929689 18382 net.cpp:98] Creating Layer out3a/relu
I0711 23:46:57.929692 18382 net.cpp:439] out3a/relu <- out3a/bn
I0711 23:46:57.929694 18382 net.cpp:400] out3a/relu -> out3a/bn (in-place)
I0711 23:46:57.929697 18382 net.cpp:148] Setting up out3a/relu
I0711 23:46:57.929699 18382 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 23:46:57.929702 18382 net.cpp:163] Memory required for data: 820838400
I0711 23:46:57.929703 18382 layer_factory.hpp:77] Creating layer out3_out5_combined
I0711 23:46:57.929709 18382 net.cpp:98] Creating Layer out3_out5_combined
I0711 23:46:57.929711 18382 net.cpp:439] out3_out5_combined <- out5a_up2
I0711 23:46:57.929719 18382 net.cpp:439] out3_out5_combined <- out3a/bn
I0711 23:46:57.929724 18382 net.cpp:413] out3_out5_combined -> out3_out5_combined
I0711 23:46:57.929738 18382 net.cpp:148] Setting up out3_out5_combined
I0711 23:46:57.929741 18382 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 23:46:57.929744 18382 net.cpp:163] Memory required for data: 827392000
I0711 23:46:57.929745 18382 layer_factory.hpp:77] Creating layer ctx_conv1
I0711 23:46:57.929749 18382 net.cpp:98] Creating Layer ctx_conv1
I0711 23:46:57.929750 18382 net.cpp:439] ctx_conv1 <- out3_out5_combined
I0711 23:46:57.929754 18382 net.cpp:413] ctx_conv1 -> ctx_conv1
I0711 23:46:57.930611 18382 net.cpp:148] Setting up ctx_conv1
I0711 23:46:57.930616 18382 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 23:46:57.930619 18382 net.cpp:163] Memory required for data: 833945600
I0711 23:46:57.930621 18382 layer_factory.hpp:77] Creating layer ctx_conv1/bn
I0711 23:46:57.930624 18382 net.cpp:98] Creating Layer ctx_conv1/bn
I0711 23:46:57.930626 18382 net.cpp:439] ctx_conv1/bn <- ctx_conv1
I0711 23:46:57.930629 18382 net.cpp:413] ctx_conv1/bn -> ctx_conv1/bn
I0711 23:46:57.930927 18382 net.cpp:148] Setting up ctx_conv1/bn
I0711 23:46:57.930932 18382 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 23:46:57.930934 18382 net.cpp:163] Memory required for data: 840499200
I0711 23:46:57.930939 18382 layer_factory.hpp:77] Creating layer ctx_conv1/relu
I0711 23:46:57.930941 18382 net.cpp:98] Creating Layer ctx_conv1/relu
I0711 23:46:57.930943 18382 net.cpp:439] ctx_conv1/relu <- ctx_conv1/bn
I0711 23:46:57.930946 18382 net.cpp:400] ctx_conv1/relu -> ctx_conv1/bn (in-place)
I0711 23:46:57.930949 18382 net.cpp:148] Setting up ctx_conv1/relu
I0711 23:46:57.930951 18382 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 23:46:57.930953 18382 net.cpp:163] Memory required for data: 847052800
I0711 23:46:57.930954 18382 layer_factory.hpp:77] Creating layer ctx_conv2
I0711 23:46:57.930958 18382 net.cpp:98] Creating Layer ctx_conv2
I0711 23:46:57.930960 18382 net.cpp:439] ctx_conv2 <- ctx_conv1/bn
I0711 23:46:57.930963 18382 net.cpp:413] ctx_conv2 -> ctx_conv2
I0711 23:46:57.931816 18382 net.cpp:148] Setting up ctx_conv2
I0711 23:46:57.931820 18382 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 23:46:57.931823 18382 net.cpp:163] Memory required for data: 853606400
I0711 23:46:57.931825 18382 layer_factory.hpp:77] Creating layer ctx_conv2/bn
I0711 23:46:57.931828 18382 net.cpp:98] Creating Layer ctx_conv2/bn
I0711 23:46:57.931830 18382 net.cpp:439] ctx_conv2/bn <- ctx_conv2
I0711 23:46:57.931833 18382 net.cpp:413] ctx_conv2/bn -> ctx_conv2/bn
I0711 23:46:57.932132 18382 net.cpp:148] Setting up ctx_conv2/bn
I0711 23:46:57.932137 18382 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 23:46:57.932139 18382 net.cpp:163] Memory required for data: 860160000
I0711 23:46:57.932143 18382 layer_factory.hpp:77] Creating layer ctx_conv2/relu
I0711 23:46:57.932147 18382 net.cpp:98] Creating Layer ctx_conv2/relu
I0711 23:46:57.932148 18382 net.cpp:439] ctx_conv2/relu <- ctx_conv2/bn
I0711 23:46:57.932150 18382 net.cpp:400] ctx_conv2/relu -> ctx_conv2/bn (in-place)
I0711 23:46:57.932153 18382 net.cpp:148] Setting up ctx_conv2/relu
I0711 23:46:57.932157 18382 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 23:46:57.932157 18382 net.cpp:163] Memory required for data: 866713600
I0711 23:46:57.932159 18382 layer_factory.hpp:77] Creating layer ctx_conv3
I0711 23:46:57.932164 18382 net.cpp:98] Creating Layer ctx_conv3
I0711 23:46:57.932166 18382 net.cpp:439] ctx_conv3 <- ctx_conv2/bn
I0711 23:46:57.932168 18382 net.cpp:413] ctx_conv3 -> ctx_conv3
I0711 23:46:57.933027 18382 net.cpp:148] Setting up ctx_conv3
I0711 23:46:57.933032 18382 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 23:46:57.933034 18382 net.cpp:163] Memory required for data: 873267200
I0711 23:46:57.933037 18382 layer_factory.hpp:77] Creating layer ctx_conv3/bn
I0711 23:46:57.933040 18382 net.cpp:98] Creating Layer ctx_conv3/bn
I0711 23:46:57.933043 18382 net.cpp:439] ctx_conv3/bn <- ctx_conv3
I0711 23:46:57.933050 18382 net.cpp:413] ctx_conv3/bn -> ctx_conv3/bn
I0711 23:46:57.933348 18382 net.cpp:148] Setting up ctx_conv3/bn
I0711 23:46:57.933353 18382 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 23:46:57.933356 18382 net.cpp:163] Memory required for data: 879820800
I0711 23:46:57.933360 18382 layer_factory.hpp:77] Creating layer ctx_conv3/relu
I0711 23:46:57.933363 18382 net.cpp:98] Creating Layer ctx_conv3/relu
I0711 23:46:57.933365 18382 net.cpp:439] ctx_conv3/relu <- ctx_conv3/bn
I0711 23:46:57.933367 18382 net.cpp:400] ctx_conv3/relu -> ctx_conv3/bn (in-place)
I0711 23:46:57.933370 18382 net.cpp:148] Setting up ctx_conv3/relu
I0711 23:46:57.933372 18382 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 23:46:57.933374 18382 net.cpp:163] Memory required for data: 886374400
I0711 23:46:57.933377 18382 layer_factory.hpp:77] Creating layer ctx_conv4
I0711 23:46:57.933379 18382 net.cpp:98] Creating Layer ctx_conv4
I0711 23:46:57.933382 18382 net.cpp:439] ctx_conv4 <- ctx_conv3/bn
I0711 23:46:57.933383 18382 net.cpp:413] ctx_conv4 -> ctx_conv4
I0711 23:46:57.934243 18382 net.cpp:148] Setting up ctx_conv4
I0711 23:46:57.934248 18382 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 23:46:57.934250 18382 net.cpp:163] Memory required for data: 892928000
I0711 23:46:57.934253 18382 layer_factory.hpp:77] Creating layer ctx_conv4/bn
I0711 23:46:57.934255 18382 net.cpp:98] Creating Layer ctx_conv4/bn
I0711 23:46:57.934257 18382 net.cpp:439] ctx_conv4/bn <- ctx_conv4
I0711 23:46:57.934260 18382 net.cpp:413] ctx_conv4/bn -> ctx_conv4/bn
I0711 23:46:57.934566 18382 net.cpp:148] Setting up ctx_conv4/bn
I0711 23:46:57.934571 18382 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 23:46:57.934572 18382 net.cpp:163] Memory required for data: 899481600
I0711 23:46:57.934576 18382 layer_factory.hpp:77] Creating layer ctx_conv4/relu
I0711 23:46:57.934579 18382 net.cpp:98] Creating Layer ctx_conv4/relu
I0711 23:46:57.934581 18382 net.cpp:439] ctx_conv4/relu <- ctx_conv4/bn
I0711 23:46:57.934584 18382 net.cpp:400] ctx_conv4/relu -> ctx_conv4/bn (in-place)
I0711 23:46:57.934587 18382 net.cpp:148] Setting up ctx_conv4/relu
I0711 23:46:57.934589 18382 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0711 23:46:57.934592 18382 net.cpp:163] Memory required for data: 906035200
I0711 23:46:57.934593 18382 layer_factory.hpp:77] Creating layer ctx_final
I0711 23:46:57.934597 18382 net.cpp:98] Creating Layer ctx_final
I0711 23:46:57.934597 18382 net.cpp:439] ctx_final <- ctx_conv4/bn
I0711 23:46:57.934602 18382 net.cpp:413] ctx_final -> ctx_final
I0711 23:46:57.934828 18382 net.cpp:148] Setting up ctx_final
I0711 23:46:57.934833 18382 net.cpp:155] Top shape: 4 8 80 80 (204800)
I0711 23:46:57.934834 18382 net.cpp:163] Memory required for data: 906854400
I0711 23:46:57.934836 18382 layer_factory.hpp:77] Creating layer ctx_final/relu
I0711 23:46:57.934839 18382 net.cpp:98] Creating Layer ctx_final/relu
I0711 23:46:57.934841 18382 net.cpp:439] ctx_final/relu <- ctx_final
I0711 23:46:57.934844 18382 net.cpp:400] ctx_final/relu -> ctx_final (in-place)
I0711 23:46:57.934846 18382 net.cpp:148] Setting up ctx_final/relu
I0711 23:46:57.934849 18382 net.cpp:155] Top shape: 4 8 80 80 (204800)
I0711 23:46:57.934850 18382 net.cpp:163] Memory required for data: 907673600
I0711 23:46:57.934852 18382 layer_factory.hpp:77] Creating layer out_deconv_final_up2
I0711 23:46:57.934855 18382 net.cpp:98] Creating Layer out_deconv_final_up2
I0711 23:46:57.934857 18382 net.cpp:439] out_deconv_final_up2 <- ctx_final
I0711 23:46:57.934860 18382 net.cpp:413] out_deconv_final_up2 -> out_deconv_final_up2
I0711 23:46:57.934960 18382 net.cpp:148] Setting up out_deconv_final_up2
I0711 23:46:57.934964 18382 net.cpp:155] Top shape: 4 8 160 160 (819200)
I0711 23:46:57.934967 18382 net.cpp:163] Memory required for data: 910950400
I0711 23:46:57.934968 18382 layer_factory.hpp:77] Creating layer out_deconv_final_up4
I0711 23:46:57.934972 18382 net.cpp:98] Creating Layer out_deconv_final_up4
I0711 23:46:57.934973 18382 net.cpp:439] out_deconv_final_up4 <- out_deconv_final_up2
I0711 23:46:57.934980 18382 net.cpp:413] out_deconv_final_up4 -> out_deconv_final_up4
I0711 23:46:57.935079 18382 net.cpp:148] Setting up out_deconv_final_up4
I0711 23:46:57.935083 18382 net.cpp:155] Top shape: 4 8 320 320 (3276800)
I0711 23:46:57.935086 18382 net.cpp:163] Memory required for data: 924057600
I0711 23:46:57.935088 18382 layer_factory.hpp:77] Creating layer out_deconv_final_up8
I0711 23:46:57.935091 18382 net.cpp:98] Creating Layer out_deconv_final_up8
I0711 23:46:57.935092 18382 net.cpp:439] out_deconv_final_up8 <- out_deconv_final_up4
I0711 23:46:57.935096 18382 net.cpp:413] out_deconv_final_up8 -> out_deconv_final_up8
I0711 23:46:57.935191 18382 net.cpp:148] Setting up out_deconv_final_up8
I0711 23:46:57.935195 18382 net.cpp:155] Top shape: 4 8 640 640 (13107200)
I0711 23:46:57.935197 18382 net.cpp:163] Memory required for data: 976486400
I0711 23:46:57.935199 18382 layer_factory.hpp:77] Creating layer out_deconv_final_up8_out_deconv_final_up8_0_split
I0711 23:46:57.935202 18382 net.cpp:98] Creating Layer out_deconv_final_up8_out_deconv_final_up8_0_split
I0711 23:46:57.935204 18382 net.cpp:439] out_deconv_final_up8_out_deconv_final_up8_0_split <- out_deconv_final_up8
I0711 23:46:57.935206 18382 net.cpp:413] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0711 23:46:57.935209 18382 net.cpp:413] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0711 23:46:57.935212 18382 net.cpp:413] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0711 23:46:57.935236 18382 net.cpp:148] Setting up out_deconv_final_up8_out_deconv_final_up8_0_split
I0711 23:46:57.935240 18382 net.cpp:155] Top shape: 4 8 640 640 (13107200)
I0711 23:46:57.935241 18382 net.cpp:155] Top shape: 4 8 640 640 (13107200)
I0711 23:46:57.935245 18382 net.cpp:155] Top shape: 4 8 640 640 (13107200)
I0711 23:46:57.935245 18382 net.cpp:163] Memory required for data: 1133772800
I0711 23:46:57.935247 18382 layer_factory.hpp:77] Creating layer loss
I0711 23:46:57.935253 18382 net.cpp:98] Creating Layer loss
I0711 23:46:57.935256 18382 net.cpp:439] loss <- out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0711 23:46:57.935258 18382 net.cpp:439] loss <- label_data_1_split_0
I0711 23:46:57.935261 18382 net.cpp:413] loss -> loss
I0711 23:46:57.935268 18382 layer_factory.hpp:77] Creating layer loss
I0711 23:46:57.952126 18382 net.cpp:148] Setting up loss
I0711 23:46:57.952148 18382 net.cpp:155] Top shape: (1)
I0711 23:46:57.952150 18382 net.cpp:158]     with loss weight 1
I0711 23:46:57.952163 18382 net.cpp:163] Memory required for data: 1133772804
I0711 23:46:57.952167 18382 layer_factory.hpp:77] Creating layer accuracy/top1
I0711 23:46:57.952175 18382 net.cpp:98] Creating Layer accuracy/top1
I0711 23:46:57.952179 18382 net.cpp:439] accuracy/top1 <- out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0711 23:46:57.952184 18382 net.cpp:439] accuracy/top1 <- label_data_1_split_1
I0711 23:46:57.952188 18382 net.cpp:413] accuracy/top1 -> accuracy/top1
I0711 23:46:57.952203 18382 net.cpp:148] Setting up accuracy/top1
I0711 23:46:57.952208 18382 net.cpp:155] Top shape: (1)
I0711 23:46:57.952210 18382 net.cpp:163] Memory required for data: 1133772808
I0711 23:46:57.952214 18382 layer_factory.hpp:77] Creating layer accuracy/top5
I0711 23:46:57.952216 18382 net.cpp:98] Creating Layer accuracy/top5
I0711 23:46:57.952220 18382 net.cpp:439] accuracy/top5 <- out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0711 23:46:57.952224 18382 net.cpp:439] accuracy/top5 <- label_data_1_split_2
I0711 23:46:57.952229 18382 net.cpp:413] accuracy/top5 -> accuracy/top5
I0711 23:46:57.952242 18382 net.cpp:148] Setting up accuracy/top5
I0711 23:46:57.952246 18382 net.cpp:155] Top shape: (1)
I0711 23:46:57.952250 18382 net.cpp:163] Memory required for data: 1133772812
I0711 23:46:57.952253 18382 net.cpp:226] accuracy/top5 does not need backward computation.
I0711 23:46:57.952257 18382 net.cpp:226] accuracy/top1 does not need backward computation.
I0711 23:46:57.952270 18382 net.cpp:224] loss needs backward computation.
I0711 23:46:57.952275 18382 net.cpp:224] out_deconv_final_up8_out_deconv_final_up8_0_split needs backward computation.
I0711 23:46:57.952280 18382 net.cpp:224] out_deconv_final_up8 needs backward computation.
I0711 23:46:57.952284 18382 net.cpp:224] out_deconv_final_up4 needs backward computation.
I0711 23:46:57.952288 18382 net.cpp:224] out_deconv_final_up2 needs backward computation.
I0711 23:46:57.952291 18382 net.cpp:224] ctx_final/relu needs backward computation.
I0711 23:46:57.952296 18382 net.cpp:224] ctx_final needs backward computation.
I0711 23:46:57.952299 18382 net.cpp:224] ctx_conv4/relu needs backward computation.
I0711 23:46:57.952303 18382 net.cpp:224] ctx_conv4/bn needs backward computation.
I0711 23:46:57.952307 18382 net.cpp:224] ctx_conv4 needs backward computation.
I0711 23:46:57.952311 18382 net.cpp:224] ctx_conv3/relu needs backward computation.
I0711 23:46:57.952314 18382 net.cpp:224] ctx_conv3/bn needs backward computation.
I0711 23:46:57.952318 18382 net.cpp:224] ctx_conv3 needs backward computation.
I0711 23:46:57.952322 18382 net.cpp:224] ctx_conv2/relu needs backward computation.
I0711 23:46:57.952327 18382 net.cpp:224] ctx_conv2/bn needs backward computation.
I0711 23:46:57.952330 18382 net.cpp:224] ctx_conv2 needs backward computation.
I0711 23:46:57.952335 18382 net.cpp:224] ctx_conv1/relu needs backward computation.
I0711 23:46:57.952338 18382 net.cpp:224] ctx_conv1/bn needs backward computation.
I0711 23:46:57.952342 18382 net.cpp:224] ctx_conv1 needs backward computation.
I0711 23:46:57.952347 18382 net.cpp:224] out3_out5_combined needs backward computation.
I0711 23:46:57.952352 18382 net.cpp:224] out3a/relu needs backward computation.
I0711 23:46:57.952355 18382 net.cpp:224] out3a/bn needs backward computation.
I0711 23:46:57.952359 18382 net.cpp:224] out3a needs backward computation.
I0711 23:46:57.952363 18382 net.cpp:224] out5a_up2 needs backward computation.
I0711 23:46:57.952368 18382 net.cpp:224] out5a/relu needs backward computation.
I0711 23:46:57.952371 18382 net.cpp:224] out5a/bn needs backward computation.
I0711 23:46:57.952375 18382 net.cpp:224] out5a needs backward computation.
I0711 23:46:57.952379 18382 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0711 23:46:57.952383 18382 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0711 23:46:57.952388 18382 net.cpp:224] res5a_branch2b needs backward computation.
I0711 23:46:57.952392 18382 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0711 23:46:57.952395 18382 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0711 23:46:57.952399 18382 net.cpp:224] res5a_branch2a needs backward computation.
I0711 23:46:57.952404 18382 net.cpp:224] pool4 needs backward computation.
I0711 23:46:57.952407 18382 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0711 23:46:57.952411 18382 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0711 23:46:57.952415 18382 net.cpp:224] res4a_branch2b needs backward computation.
I0711 23:46:57.952419 18382 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0711 23:46:57.952422 18382 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0711 23:46:57.952426 18382 net.cpp:224] res4a_branch2a needs backward computation.
I0711 23:46:57.952430 18382 net.cpp:224] pool3 needs backward computation.
I0711 23:46:57.952435 18382 net.cpp:224] res3a_branch2b/bn_res3a_branch2b/relu_0_split needs backward computation.
I0711 23:46:57.952438 18382 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0711 23:46:57.952442 18382 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0711 23:46:57.952446 18382 net.cpp:224] res3a_branch2b needs backward computation.
I0711 23:46:57.952451 18382 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0711 23:46:57.952455 18382 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0711 23:46:57.952460 18382 net.cpp:224] res3a_branch2a needs backward computation.
I0711 23:46:57.952467 18382 net.cpp:224] pool2 needs backward computation.
I0711 23:46:57.952471 18382 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0711 23:46:57.952476 18382 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0711 23:46:57.952479 18382 net.cpp:224] res2a_branch2b needs backward computation.
I0711 23:46:57.952484 18382 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0711 23:46:57.952487 18382 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0711 23:46:57.952491 18382 net.cpp:224] res2a_branch2a needs backward computation.
I0711 23:46:57.952495 18382 net.cpp:224] pool1 needs backward computation.
I0711 23:46:57.952499 18382 net.cpp:224] conv1b/relu needs backward computation.
I0711 23:46:57.952503 18382 net.cpp:224] conv1b/bn needs backward computation.
I0711 23:46:57.952507 18382 net.cpp:224] conv1b needs backward computation.
I0711 23:46:57.952512 18382 net.cpp:224] conv1a/relu needs backward computation.
I0711 23:46:57.952514 18382 net.cpp:224] conv1a/bn needs backward computation.
I0711 23:46:57.952518 18382 net.cpp:224] conv1a needs backward computation.
I0711 23:46:57.952522 18382 net.cpp:226] data/bias does not need backward computation.
I0711 23:46:57.952527 18382 net.cpp:226] label_data_1_split does not need backward computation.
I0711 23:46:57.952533 18382 net.cpp:226] data does not need backward computation.
I0711 23:46:57.952535 18382 net.cpp:268] This network produces output accuracy/top1
I0711 23:46:57.952539 18382 net.cpp:268] This network produces output accuracy/top5
I0711 23:46:57.952543 18382 net.cpp:268] This network produces output loss
I0711 23:46:57.952574 18382 net.cpp:288] Network initialization done.
I0711 23:46:57.963294 18382 caffe.cpp:289] Running for 50 iterations.
I0711 23:46:58.379917 18382 caffe.cpp:312] Batch 0, accuracy/top1 = 0.927554
I0711 23:46:58.379941 18382 caffe.cpp:312] Batch 0, accuracy/top5 = 1
I0711 23:46:58.379945 18382 caffe.cpp:312] Batch 0, loss = 0.282908
I0711 23:46:58.752136 18382 caffe.cpp:312] Batch 1, accuracy/top1 = 0.949982
I0711 23:46:58.752161 18382 caffe.cpp:312] Batch 1, accuracy/top5 = 1
I0711 23:46:58.752166 18382 caffe.cpp:312] Batch 1, loss = 0.123466
I0711 23:46:59.124784 18382 caffe.cpp:312] Batch 2, accuracy/top1 = 0.963028
I0711 23:46:59.124806 18382 caffe.cpp:312] Batch 2, accuracy/top5 = 1
I0711 23:46:59.124810 18382 caffe.cpp:312] Batch 2, loss = 0.0684247
I0711 23:46:59.500566 18382 caffe.cpp:312] Batch 3, accuracy/top1 = 0.973134
I0711 23:46:59.500586 18382 caffe.cpp:312] Batch 3, accuracy/top5 = 0.999999
I0711 23:46:59.500591 18382 caffe.cpp:312] Batch 3, loss = 0.0385598
I0711 23:46:59.873206 18382 caffe.cpp:312] Batch 4, accuracy/top1 = 0.962885
I0711 23:46:59.873230 18382 caffe.cpp:312] Batch 4, accuracy/top5 = 1
I0711 23:46:59.873235 18382 caffe.cpp:312] Batch 4, loss = 0.0868134
I0711 23:47:00.246853 18382 caffe.cpp:312] Batch 5, accuracy/top1 = 0.844534
I0711 23:47:00.246877 18382 caffe.cpp:312] Batch 5, accuracy/top5 = 0.992622
I0711 23:47:00.246881 18382 caffe.cpp:312] Batch 5, loss = 1.14652
I0711 23:47:00.617650 18382 caffe.cpp:312] Batch 6, accuracy/top1 = 0.957197
I0711 23:47:00.617671 18382 caffe.cpp:312] Batch 6, accuracy/top5 = 1
I0711 23:47:00.617676 18382 caffe.cpp:312] Batch 6, loss = 0.0868579
I0711 23:47:00.991684 18382 caffe.cpp:312] Batch 7, accuracy/top1 = 0.967113
I0711 23:47:00.991709 18382 caffe.cpp:312] Batch 7, accuracy/top5 = 1
I0711 23:47:00.991714 18382 caffe.cpp:312] Batch 7, loss = 0.0451337
I0711 23:47:01.361793 18382 caffe.cpp:312] Batch 8, accuracy/top1 = 0.975588
I0711 23:47:01.361810 18382 caffe.cpp:312] Batch 8, accuracy/top5 = 1
I0711 23:47:01.361814 18382 caffe.cpp:312] Batch 8, loss = 0.0345734
I0711 23:47:01.732594 18382 caffe.cpp:312] Batch 9, accuracy/top1 = 0.982318
I0711 23:47:01.732616 18382 caffe.cpp:312] Batch 9, accuracy/top5 = 1
I0711 23:47:01.732620 18382 caffe.cpp:312] Batch 9, loss = 0.0229718
I0711 23:47:02.104158 18382 caffe.cpp:312] Batch 10, accuracy/top1 = 0.96643
I0711 23:47:02.104181 18382 caffe.cpp:312] Batch 10, accuracy/top5 = 0.999998
I0711 23:47:02.104198 18382 caffe.cpp:312] Batch 10, loss = 0.0502197
I0711 23:47:02.477880 18382 caffe.cpp:312] Batch 11, accuracy/top1 = 0.976868
I0711 23:47:02.477900 18382 caffe.cpp:312] Batch 11, accuracy/top5 = 1
I0711 23:47:02.477905 18382 caffe.cpp:312] Batch 11, loss = 0.0277814
I0711 23:47:02.848140 18382 caffe.cpp:312] Batch 12, accuracy/top1 = 0.966541
I0711 23:47:02.848163 18382 caffe.cpp:312] Batch 12, accuracy/top5 = 1
I0711 23:47:02.848166 18382 caffe.cpp:312] Batch 12, loss = 0.0493518
I0711 23:47:03.221058 18382 caffe.cpp:312] Batch 13, accuracy/top1 = 0.98172
I0711 23:47:03.221081 18382 caffe.cpp:312] Batch 13, accuracy/top5 = 1
I0711 23:47:03.221084 18382 caffe.cpp:312] Batch 13, loss = 0.0299722
I0711 23:47:03.591686 18382 caffe.cpp:312] Batch 14, accuracy/top1 = 0.978136
I0711 23:47:03.591709 18382 caffe.cpp:312] Batch 14, accuracy/top5 = 1
I0711 23:47:03.591712 18382 caffe.cpp:312] Batch 14, loss = 0.0219135
I0711 23:47:03.960091 18382 caffe.cpp:312] Batch 15, accuracy/top1 = 0.960172
I0711 23:47:03.960113 18382 caffe.cpp:312] Batch 15, accuracy/top5 = 1
I0711 23:47:03.960115 18382 caffe.cpp:312] Batch 15, loss = 0.0679205
I0711 23:47:04.330118 18382 caffe.cpp:312] Batch 16, accuracy/top1 = 0.93074
I0711 23:47:04.330137 18382 caffe.cpp:312] Batch 16, accuracy/top5 = 1
I0711 23:47:04.330140 18382 caffe.cpp:312] Batch 16, loss = 0.278489
I0711 23:47:04.699219 18382 caffe.cpp:312] Batch 17, accuracy/top1 = 0.861562
I0711 23:47:04.699241 18382 caffe.cpp:312] Batch 17, accuracy/top5 = 0.993992
I0711 23:47:04.699244 18382 caffe.cpp:312] Batch 17, loss = 0.865926
I0711 23:47:05.072139 18382 caffe.cpp:312] Batch 18, accuracy/top1 = 0.981113
I0711 23:47:05.072160 18382 caffe.cpp:312] Batch 18, accuracy/top5 = 1
I0711 23:47:05.072162 18382 caffe.cpp:312] Batch 18, loss = 0.0209369
I0711 23:47:05.443806 18382 caffe.cpp:312] Batch 19, accuracy/top1 = 0.983182
I0711 23:47:05.443827 18382 caffe.cpp:312] Batch 19, accuracy/top5 = 1
I0711 23:47:05.443830 18382 caffe.cpp:312] Batch 19, loss = 0.0221118
I0711 23:47:05.813516 18382 caffe.cpp:312] Batch 20, accuracy/top1 = 0.97559
I0711 23:47:05.813539 18382 caffe.cpp:312] Batch 20, accuracy/top5 = 1
I0711 23:47:05.813541 18382 caffe.cpp:312] Batch 20, loss = 0.0438306
I0711 23:47:06.182607 18382 caffe.cpp:312] Batch 21, accuracy/top1 = 0.878048
I0711 23:47:06.182629 18382 caffe.cpp:312] Batch 21, accuracy/top5 = 0.969558
I0711 23:47:06.182632 18382 caffe.cpp:312] Batch 21, loss = 1.21603
I0711 23:47:06.552987 18382 caffe.cpp:312] Batch 22, accuracy/top1 = 0.969815
I0711 23:47:06.553007 18382 caffe.cpp:312] Batch 22, accuracy/top5 = 1
I0711 23:47:06.553010 18382 caffe.cpp:312] Batch 22, loss = 0.0445187
I0711 23:47:06.926216 18382 caffe.cpp:312] Batch 23, accuracy/top1 = 0.977263
I0711 23:47:06.926239 18382 caffe.cpp:312] Batch 23, accuracy/top5 = 1
I0711 23:47:06.926242 18382 caffe.cpp:312] Batch 23, loss = 0.0314893
I0711 23:47:07.298467 18382 caffe.cpp:312] Batch 24, accuracy/top1 = 0.949731
I0711 23:47:07.298492 18382 caffe.cpp:312] Batch 24, accuracy/top5 = 1
I0711 23:47:07.298496 18382 caffe.cpp:312] Batch 24, loss = 0.0880889
I0711 23:47:07.674034 18382 caffe.cpp:312] Batch 25, accuracy/top1 = 0.971205
I0711 23:47:07.674058 18382 caffe.cpp:312] Batch 25, accuracy/top5 = 1
I0711 23:47:07.674062 18382 caffe.cpp:312] Batch 25, loss = 0.0444232
I0711 23:47:08.051230 18382 caffe.cpp:312] Batch 26, accuracy/top1 = 0.954687
I0711 23:47:08.051250 18382 caffe.cpp:312] Batch 26, accuracy/top5 = 1
I0711 23:47:08.051254 18382 caffe.cpp:312] Batch 26, loss = 0.0547882
I0711 23:47:08.422791 18382 caffe.cpp:312] Batch 27, accuracy/top1 = 0.974252
I0711 23:47:08.422811 18382 caffe.cpp:312] Batch 27, accuracy/top5 = 1
I0711 23:47:08.422813 18382 caffe.cpp:312] Batch 27, loss = 0.0350671
I0711 23:47:08.792474 18382 caffe.cpp:312] Batch 28, accuracy/top1 = 0.949492
I0711 23:47:08.792496 18382 caffe.cpp:312] Batch 28, accuracy/top5 = 1
I0711 23:47:08.792500 18382 caffe.cpp:312] Batch 28, loss = 0.0894208
I0711 23:47:09.164727 18382 caffe.cpp:312] Batch 29, accuracy/top1 = 0.96223
I0711 23:47:09.164764 18382 caffe.cpp:312] Batch 29, accuracy/top5 = 1
I0711 23:47:09.164768 18382 caffe.cpp:312] Batch 29, loss = 0.121297
I0711 23:47:09.535876 18382 caffe.cpp:312] Batch 30, accuracy/top1 = 0.82045
I0711 23:47:09.535899 18382 caffe.cpp:312] Batch 30, accuracy/top5 = 1
I0711 23:47:09.535903 18382 caffe.cpp:312] Batch 30, loss = 0.602959
I0711 23:47:09.909068 18382 caffe.cpp:312] Batch 31, accuracy/top1 = 0.974847
I0711 23:47:09.909091 18382 caffe.cpp:312] Batch 31, accuracy/top5 = 1
I0711 23:47:09.909095 18382 caffe.cpp:312] Batch 31, loss = 0.0442834
I0711 23:47:10.279696 18382 caffe.cpp:312] Batch 32, accuracy/top1 = 0.954242
I0711 23:47:10.279718 18382 caffe.cpp:312] Batch 32, accuracy/top5 = 1
I0711 23:47:10.279722 18382 caffe.cpp:312] Batch 32, loss = 0.0656886
I0711 23:47:10.653872 18382 caffe.cpp:312] Batch 33, accuracy/top1 = 0.968039
I0711 23:47:10.653895 18382 caffe.cpp:312] Batch 33, accuracy/top5 = 1
I0711 23:47:10.653898 18382 caffe.cpp:312] Batch 33, loss = 0.0509459
I0711 23:47:11.029175 18382 caffe.cpp:312] Batch 34, accuracy/top1 = 0.980037
I0711 23:47:11.029196 18382 caffe.cpp:312] Batch 34, accuracy/top5 = 1
I0711 23:47:11.029198 18382 caffe.cpp:312] Batch 34, loss = 0.0399487
I0711 23:47:11.401669 18382 caffe.cpp:312] Batch 35, accuracy/top1 = 0.973125
I0711 23:47:11.401690 18382 caffe.cpp:312] Batch 35, accuracy/top5 = 1
I0711 23:47:11.401692 18382 caffe.cpp:312] Batch 35, loss = 0.0402765
I0711 23:47:11.772080 18382 caffe.cpp:312] Batch 36, accuracy/top1 = 0.963826
I0711 23:47:11.772104 18382 caffe.cpp:312] Batch 36, accuracy/top5 = 1
I0711 23:47:11.772107 18382 caffe.cpp:312] Batch 36, loss = 0.0640399
I0711 23:47:12.142071 18382 caffe.cpp:312] Batch 37, accuracy/top1 = 0.969088
I0711 23:47:12.142091 18382 caffe.cpp:312] Batch 37, accuracy/top5 = 1
I0711 23:47:12.142094 18382 caffe.cpp:312] Batch 37, loss = 0.0462034
I0711 23:47:12.511448 18382 caffe.cpp:312] Batch 38, accuracy/top1 = 0.942105
I0711 23:47:12.511468 18382 caffe.cpp:312] Batch 38, accuracy/top5 = 1
I0711 23:47:12.511471 18382 caffe.cpp:312] Batch 38, loss = 0.117064
I0711 23:47:12.879875 18382 caffe.cpp:312] Batch 39, accuracy/top1 = 0.932063
I0711 23:47:12.879899 18382 caffe.cpp:312] Batch 39, accuracy/top5 = 1
I0711 23:47:12.879902 18382 caffe.cpp:312] Batch 39, loss = 0.167876
I0711 23:47:13.251303 18382 caffe.cpp:312] Batch 40, accuracy/top1 = 0.981528
I0711 23:47:13.251327 18382 caffe.cpp:312] Batch 40, accuracy/top5 = 1
I0711 23:47:13.251330 18382 caffe.cpp:312] Batch 40, loss = 0.0377991
I0711 23:47:13.622196 18382 caffe.cpp:312] Batch 41, accuracy/top1 = 0.967675
I0711 23:47:13.622220 18382 caffe.cpp:312] Batch 41, accuracy/top5 = 1
I0711 23:47:13.622222 18382 caffe.cpp:312] Batch 41, loss = 0.0401298
I0711 23:47:13.992837 18382 caffe.cpp:312] Batch 42, accuracy/top1 = 0.977274
I0711 23:47:13.992861 18382 caffe.cpp:312] Batch 42, accuracy/top5 = 1
I0711 23:47:13.992864 18382 caffe.cpp:312] Batch 42, loss = 0.0332917
I0711 23:47:14.364521 18382 caffe.cpp:312] Batch 43, accuracy/top1 = 0.97942
I0711 23:47:14.364540 18382 caffe.cpp:312] Batch 43, accuracy/top5 = 1
I0711 23:47:14.364543 18382 caffe.cpp:312] Batch 43, loss = 0.0287781
I0711 23:47:14.736171 18382 caffe.cpp:312] Batch 44, accuracy/top1 = 0.958374
I0711 23:47:14.736193 18382 caffe.cpp:312] Batch 44, accuracy/top5 = 1
I0711 23:47:14.736196 18382 caffe.cpp:312] Batch 44, loss = 0.0789716
I0711 23:47:15.108352 18382 caffe.cpp:312] Batch 45, accuracy/top1 = 0.977076
I0711 23:47:15.108372 18382 caffe.cpp:312] Batch 45, accuracy/top5 = 1
I0711 23:47:15.108376 18382 caffe.cpp:312] Batch 45, loss = 0.040257
I0711 23:47:15.481472 18382 caffe.cpp:312] Batch 46, accuracy/top1 = 0.971617
I0711 23:47:15.481493 18382 caffe.cpp:312] Batch 46, accuracy/top5 = 1
I0711 23:47:15.481497 18382 caffe.cpp:312] Batch 46, loss = 0.0431561
I0711 23:47:15.854609 18382 caffe.cpp:312] Batch 47, accuracy/top1 = 0.969081
I0711 23:47:15.854630 18382 caffe.cpp:312] Batch 47, accuracy/top5 = 0.999977
I0711 23:47:15.854634 18382 caffe.cpp:312] Batch 47, loss = 0.102682
I0711 23:47:16.227671 18382 caffe.cpp:312] Batch 48, accuracy/top1 = 0.866307
I0711 23:47:16.227695 18382 caffe.cpp:312] Batch 48, accuracy/top5 = 1
I0711 23:47:16.227699 18382 caffe.cpp:312] Batch 48, loss = 0.883984
I0711 23:47:16.598068 18382 caffe.cpp:312] Batch 49, accuracy/top1 = 0.949664
I0711 23:47:16.598088 18382 caffe.cpp:312] Batch 49, accuracy/top5 = 1
I0711 23:47:16.598091 18382 caffe.cpp:312] Batch 49, loss = 0.0820899
I0711 23:47:16.598093 18382 caffe.cpp:317] Loss: 0.155005
I0711 23:47:16.598100 18382 caffe.cpp:329] accuracy/top1 = 0.954559
I0711 23:47:16.598104 18382 caffe.cpp:329] accuracy/top5 = 0.999123
I0711 23:47:16.598109 18382 caffe.cpp:329] loss = 0.155005 (* 1 = 0.155005 loss)
