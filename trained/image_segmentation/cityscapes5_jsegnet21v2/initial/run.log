I0815 19:04:17.830703 20809 caffe.cpp:608] This is NVCaffe 0.16.3 started at Tue Aug 15 19:04:17 2017
I0815 19:04:17.830837 20809 caffe.cpp:611] CuDNN version: 6021
I0815 19:04:17.830839 20809 caffe.cpp:612] CuBLAS version: 8000
I0815 19:04:17.830842 20809 caffe.cpp:613] CUDA version: 8000
I0815 19:04:17.830843 20809 caffe.cpp:614] CUDA driver version: 8000
I0815 19:04:18.104919 20809 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0815 19:04:18.105489 20809 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I0815 19:04:18.106029 20809 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[1]: total=8508145664 free=8379236352
I0815 19:04:18.106546 20809 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[2]: total=8508145664 free=8379236352
I0815 19:04:18.106554 20809 caffe.cpp:208] Using GPUs 0, 1, 2
I0815 19:04:18.106875 20809 caffe.cpp:213] GPU 0: GeForce GTX 1080
I0815 19:04:18.107197 20809 caffe.cpp:213] GPU 1: GeForce GTX 1080
I0815 19:04:18.107518 20809 caffe.cpp:213] GPU 2: GeForce GTX 1080
I0815 19:04:18.112586 20809 solver.cpp:42] Solver data type: FLOAT
I0815 19:04:18.112623 20809 solver.cpp:45] Initializing solver from parameters: 
train_net: "training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/initial/train.prototxt"
test_net: "training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/initial/test.prototxt"
test_iter: 125
test_interval: 2000
base_lr: 0.0001
display: 100
max_iter: 32000
lr_policy: "multistep"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 0.0001
snapshot: 10000
snapshot_prefix: "training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/initial/cityscapes5_jsegnet21v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
test_initialization: false
stepvalue: 24000
iter_size: 1
type: "Adam"
I0815 19:04:18.119510 20809 solver.cpp:77] Creating training net from train_net file: training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/initial/train.prototxt
I0815 19:04:18.120100 20809 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0815 19:04:18.120107 20809 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
W0815 19:04:18.120146 20809 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0815 19:04:18.120394 20809 net.cpp:72] Initializing net from parameters: 
name: "jsegnet21v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/train-image-lmdb"
    label_list_path: "data/train-label-lmdb"
    batch_size: 6
    shuffle: false
    threads: 1
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
I0815 19:04:18.120579 20809 net.cpp:104] Using FLOAT as default forward math type
I0815 19:04:18.120585 20809 net.cpp:110] Using FLOAT as default backward math type
I0815 19:04:18.120589 20809 layer_factory.hpp:136] Creating layer 'data' of type 'ImageLabelData'
I0815 19:04:18.120594 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.125507 20809 net.cpp:184] Created Layer data (0)
I0815 19:04:18.125516 20809 net.cpp:530] data -> data
I0815 19:04:18.125532 20809 net.cpp:530] data -> label
I0815 19:04:18.125617 20809 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0815 19:04:18.125632 20809 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 19:04:18.175300 20830 db_lmdb.cpp:24] Opened lmdb data/train-image-lmdb
I0815 19:04:18.246284 20809 data_layer.cpp:185] [0] ReshapePrefetch 6, 3, 640, 640
I0815 19:04:18.246367 20809 data_layer.cpp:209] [0] Output data size: 6, 3, 640, 640
I0815 19:04:18.246382 20809 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 19:04:18.246474 20809 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0815 19:04:18.246501 20809 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 19:04:18.247313 20840 data_layer.cpp:97] [0] Parser threads: 1
I0815 19:04:18.247323 20840 data_layer.cpp:99] [0] Transformer threads: 1
I0815 19:04:18.257385 20840 blocking_queue.cpp:40] Waiting for datum
I0815 19:04:18.283761 20841 db_lmdb.cpp:24] Opened lmdb data/train-label-lmdb
I0815 19:04:18.429167 20809 data_layer.cpp:185] [0] ReshapePrefetch 6, 1, 640, 640
I0815 19:04:18.429215 20809 data_layer.cpp:209] [0] Output data size: 6, 1, 640, 640
I0815 19:04:18.429220 20809 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 19:04:18.429265 20809 net.cpp:245] Setting up data
I0815 19:04:18.429276 20809 net.cpp:252] TRAIN Top shape for layer 0 'data' 6 3 640 640 (7372800)
I0815 19:04:18.429288 20809 net.cpp:252] TRAIN Top shape for layer 0 'data' 6 1 640 640 (2457600)
I0815 19:04:18.429297 20809 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0815 19:04:18.429303 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.429324 20809 net.cpp:184] Created Layer data/bias (1)
I0815 19:04:18.429330 20809 net.cpp:561] data/bias <- data
I0815 19:04:18.429343 20809 net.cpp:530] data/bias -> data/bias
I0815 19:04:18.430322 20865 data_layer.cpp:97] [0] Parser threads: 1
I0815 19:04:18.430333 20865 data_layer.cpp:99] [0] Transformer threads: 1
I0815 19:04:18.433727 20809 net.cpp:245] Setting up data/bias
I0815 19:04:18.433750 20809 net.cpp:252] TRAIN Top shape for layer 1 'data/bias' 6 3 640 640 (7372800)
I0815 19:04:18.433765 20809 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0815 19:04:18.433773 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.433802 20809 net.cpp:184] Created Layer conv1a (2)
I0815 19:04:18.433807 20809 net.cpp:561] conv1a <- data/bias
I0815 19:04:18.433812 20809 net.cpp:530] conv1a -> conv1a
I0815 19:04:18.795943 20809 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 7.91G, req 0G)
I0815 19:04:18.795996 20809 net.cpp:245] Setting up conv1a
I0815 19:04:18.796005 20809 net.cpp:252] TRAIN Top shape for layer 2 'conv1a' 6 32 320 320 (19660800)
I0815 19:04:18.796021 20809 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0815 19:04:18.796027 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.796041 20809 net.cpp:184] Created Layer conv1a/bn (3)
I0815 19:04:18.796046 20809 net.cpp:561] conv1a/bn <- conv1a
I0815 19:04:18.796051 20809 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0815 19:04:18.796741 20809 net.cpp:245] Setting up conv1a/bn
I0815 19:04:18.796751 20809 net.cpp:252] TRAIN Top shape for layer 3 'conv1a/bn' 6 32 320 320 (19660800)
I0815 19:04:18.796759 20809 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0815 19:04:18.796764 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.796772 20809 net.cpp:184] Created Layer conv1a/relu (4)
I0815 19:04:18.796775 20809 net.cpp:561] conv1a/relu <- conv1a
I0815 19:04:18.796780 20809 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0815 19:04:18.796794 20809 net.cpp:245] Setting up conv1a/relu
I0815 19:04:18.796799 20809 net.cpp:252] TRAIN Top shape for layer 4 'conv1a/relu' 6 32 320 320 (19660800)
I0815 19:04:18.796803 20809 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0815 19:04:18.796808 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.796818 20809 net.cpp:184] Created Layer conv1b (5)
I0815 19:04:18.796823 20809 net.cpp:561] conv1b <- conv1a
I0815 19:04:18.796826 20809 net.cpp:530] conv1b -> conv1b
I0815 19:04:18.842250 20809 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.74G, req 0G)
I0815 19:04:18.842273 20809 net.cpp:245] Setting up conv1b
I0815 19:04:18.842280 20809 net.cpp:252] TRAIN Top shape for layer 5 'conv1b' 6 32 320 320 (19660800)
I0815 19:04:18.842293 20809 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0815 19:04:18.842298 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.842309 20809 net.cpp:184] Created Layer conv1b/bn (6)
I0815 19:04:18.842314 20809 net.cpp:561] conv1b/bn <- conv1b
I0815 19:04:18.842319 20809 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0815 19:04:18.843003 20809 net.cpp:245] Setting up conv1b/bn
I0815 19:04:18.843013 20809 net.cpp:252] TRAIN Top shape for layer 6 'conv1b/bn' 6 32 320 320 (19660800)
I0815 19:04:18.843020 20809 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0815 19:04:18.843024 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.843031 20809 net.cpp:184] Created Layer conv1b/relu (7)
I0815 19:04:18.843034 20809 net.cpp:561] conv1b/relu <- conv1b
I0815 19:04:18.843039 20809 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0815 19:04:18.843045 20809 net.cpp:245] Setting up conv1b/relu
I0815 19:04:18.843050 20809 net.cpp:252] TRAIN Top shape for layer 7 'conv1b/relu' 6 32 320 320 (19660800)
I0815 19:04:18.843053 20809 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0815 19:04:18.843058 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.843067 20809 net.cpp:184] Created Layer pool1 (8)
I0815 19:04:18.843071 20809 net.cpp:561] pool1 <- conv1b
I0815 19:04:18.843075 20809 net.cpp:530] pool1 -> pool1
I0815 19:04:18.843152 20809 net.cpp:245] Setting up pool1
I0815 19:04:18.843158 20809 net.cpp:252] TRAIN Top shape for layer 8 'pool1' 6 32 160 160 (4915200)
I0815 19:04:18.843171 20809 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0815 19:04:18.843176 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.843188 20809 net.cpp:184] Created Layer res2a_branch2a (9)
I0815 19:04:18.843190 20809 net.cpp:561] res2a_branch2a <- pool1
I0815 19:04:18.843195 20809 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0815 19:04:18.881155 20809 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.61G, req 0G)
I0815 19:04:18.881175 20809 net.cpp:245] Setting up res2a_branch2a
I0815 19:04:18.881182 20809 net.cpp:252] TRAIN Top shape for layer 9 'res2a_branch2a' 6 64 160 160 (9830400)
I0815 19:04:18.881194 20809 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0815 19:04:18.881199 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.881209 20809 net.cpp:184] Created Layer res2a_branch2a/bn (10)
I0815 19:04:18.881214 20809 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0815 19:04:18.881220 20809 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0815 19:04:18.882313 20809 net.cpp:245] Setting up res2a_branch2a/bn
I0815 19:04:18.882323 20809 net.cpp:252] TRAIN Top shape for layer 10 'res2a_branch2a/bn' 6 64 160 160 (9830400)
I0815 19:04:18.882331 20809 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0815 19:04:18.882335 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.882340 20809 net.cpp:184] Created Layer res2a_branch2a/relu (11)
I0815 19:04:18.882344 20809 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0815 19:04:18.882349 20809 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0815 19:04:18.882355 20809 net.cpp:245] Setting up res2a_branch2a/relu
I0815 19:04:18.882359 20809 net.cpp:252] TRAIN Top shape for layer 11 'res2a_branch2a/relu' 6 64 160 160 (9830400)
I0815 19:04:18.882364 20809 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0815 19:04:18.882367 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.882377 20809 net.cpp:184] Created Layer res2a_branch2b (12)
I0815 19:04:18.882380 20809 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0815 19:04:18.882385 20809 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0815 19:04:18.902343 20809 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.52G, req 0G)
I0815 19:04:18.902362 20809 net.cpp:245] Setting up res2a_branch2b
I0815 19:04:18.902369 20809 net.cpp:252] TRAIN Top shape for layer 12 'res2a_branch2b' 6 64 160 160 (9830400)
I0815 19:04:18.902379 20809 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0815 19:04:18.902384 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.902395 20809 net.cpp:184] Created Layer res2a_branch2b/bn (13)
I0815 19:04:18.902398 20809 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0815 19:04:18.902405 20809 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0815 19:04:18.903079 20809 net.cpp:245] Setting up res2a_branch2b/bn
I0815 19:04:18.903089 20809 net.cpp:252] TRAIN Top shape for layer 13 'res2a_branch2b/bn' 6 64 160 160 (9830400)
I0815 19:04:18.903097 20809 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0815 19:04:18.903101 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.903107 20809 net.cpp:184] Created Layer res2a_branch2b/relu (14)
I0815 19:04:18.903111 20809 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0815 19:04:18.903115 20809 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0815 19:04:18.903121 20809 net.cpp:245] Setting up res2a_branch2b/relu
I0815 19:04:18.903126 20809 net.cpp:252] TRAIN Top shape for layer 14 'res2a_branch2b/relu' 6 64 160 160 (9830400)
I0815 19:04:18.903138 20809 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0815 19:04:18.903143 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.903151 20809 net.cpp:184] Created Layer pool2 (15)
I0815 19:04:18.903154 20809 net.cpp:561] pool2 <- res2a_branch2b
I0815 19:04:18.903158 20809 net.cpp:530] pool2 -> pool2
I0815 19:04:18.903221 20809 net.cpp:245] Setting up pool2
I0815 19:04:18.903228 20809 net.cpp:252] TRAIN Top shape for layer 15 'pool2' 6 64 80 80 (2457600)
I0815 19:04:18.903231 20809 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0815 19:04:18.903236 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.903245 20809 net.cpp:184] Created Layer res3a_branch2a (16)
I0815 19:04:18.903249 20809 net.cpp:561] res3a_branch2a <- pool2
I0815 19:04:18.903254 20809 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0815 19:04:18.922541 20809 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 7.46G, req 0G)
I0815 19:04:18.922561 20809 net.cpp:245] Setting up res3a_branch2a
I0815 19:04:18.922569 20809 net.cpp:252] TRAIN Top shape for layer 16 'res3a_branch2a' 6 128 80 80 (4915200)
I0815 19:04:18.922580 20809 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0815 19:04:18.922585 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.922595 20809 net.cpp:184] Created Layer res3a_branch2a/bn (17)
I0815 19:04:18.922598 20809 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0815 19:04:18.922605 20809 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0815 19:04:18.923295 20809 net.cpp:245] Setting up res3a_branch2a/bn
I0815 19:04:18.923303 20809 net.cpp:252] TRAIN Top shape for layer 17 'res3a_branch2a/bn' 6 128 80 80 (4915200)
I0815 19:04:18.923316 20809 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0815 19:04:18.923321 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.923327 20809 net.cpp:184] Created Layer res3a_branch2a/relu (18)
I0815 19:04:18.923331 20809 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0815 19:04:18.923334 20809 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0815 19:04:18.923341 20809 net.cpp:245] Setting up res3a_branch2a/relu
I0815 19:04:18.923346 20809 net.cpp:252] TRAIN Top shape for layer 18 'res3a_branch2a/relu' 6 128 80 80 (4915200)
I0815 19:04:18.923349 20809 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0815 19:04:18.923353 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.923363 20809 net.cpp:184] Created Layer res3a_branch2b (19)
I0815 19:04:18.923367 20809 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0815 19:04:18.923370 20809 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0815 19:04:18.933624 20809 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.42G, req 0G)
I0815 19:04:18.933639 20809 net.cpp:245] Setting up res3a_branch2b
I0815 19:04:18.933645 20809 net.cpp:252] TRAIN Top shape for layer 19 'res3a_branch2b' 6 128 80 80 (4915200)
I0815 19:04:18.933652 20809 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0815 19:04:18.933656 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.933665 20809 net.cpp:184] Created Layer res3a_branch2b/bn (20)
I0815 19:04:18.933670 20809 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0815 19:04:18.933675 20809 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0815 19:04:18.934293 20809 net.cpp:245] Setting up res3a_branch2b/bn
I0815 19:04:18.934301 20809 net.cpp:252] TRAIN Top shape for layer 20 'res3a_branch2b/bn' 6 128 80 80 (4915200)
I0815 19:04:18.934317 20809 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0815 19:04:18.934321 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.934327 20809 net.cpp:184] Created Layer res3a_branch2b/relu (21)
I0815 19:04:18.934330 20809 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0815 19:04:18.934335 20809 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0815 19:04:18.934341 20809 net.cpp:245] Setting up res3a_branch2b/relu
I0815 19:04:18.934345 20809 net.cpp:252] TRAIN Top shape for layer 21 'res3a_branch2b/relu' 6 128 80 80 (4915200)
I0815 19:04:18.934350 20809 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0815 19:04:18.934355 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.934360 20809 net.cpp:184] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (22)
I0815 19:04:18.934363 20809 net.cpp:561] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0815 19:04:18.934367 20809 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0815 19:04:18.934372 20809 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0815 19:04:18.934417 20809 net.cpp:245] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0815 19:04:18.934422 20809 net.cpp:252] TRAIN Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 6 128 80 80 (4915200)
I0815 19:04:18.934427 20809 net.cpp:252] TRAIN Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 6 128 80 80 (4915200)
I0815 19:04:18.934432 20809 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0815 19:04:18.934435 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.934442 20809 net.cpp:184] Created Layer pool3 (23)
I0815 19:04:18.934445 20809 net.cpp:561] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0815 19:04:18.934449 20809 net.cpp:530] pool3 -> pool3
I0815 19:04:18.934514 20809 net.cpp:245] Setting up pool3
I0815 19:04:18.934518 20809 net.cpp:252] TRAIN Top shape for layer 23 'pool3' 6 128 40 40 (1228800)
I0815 19:04:18.934523 20809 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0815 19:04:18.934527 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.934536 20809 net.cpp:184] Created Layer res4a_branch2a (24)
I0815 19:04:18.934540 20809 net.cpp:561] res4a_branch2a <- pool3
I0815 19:04:18.934545 20809 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0815 19:04:18.958847 20809 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.38G, req 0G)
I0815 19:04:18.958868 20809 net.cpp:245] Setting up res4a_branch2a
I0815 19:04:18.958875 20809 net.cpp:252] TRAIN Top shape for layer 24 'res4a_branch2a' 6 256 40 40 (2457600)
I0815 19:04:18.958884 20809 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0815 19:04:18.958889 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.958899 20809 net.cpp:184] Created Layer res4a_branch2a/bn (25)
I0815 19:04:18.958904 20809 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0815 19:04:18.958909 20809 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0815 19:04:18.959615 20809 net.cpp:245] Setting up res4a_branch2a/bn
I0815 19:04:18.959623 20809 net.cpp:252] TRAIN Top shape for layer 25 'res4a_branch2a/bn' 6 256 40 40 (2457600)
I0815 19:04:18.959632 20809 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0815 19:04:18.959636 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.959642 20809 net.cpp:184] Created Layer res4a_branch2a/relu (26)
I0815 19:04:18.959646 20809 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0815 19:04:18.959659 20809 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0815 19:04:18.959666 20809 net.cpp:245] Setting up res4a_branch2a/relu
I0815 19:04:18.959671 20809 net.cpp:252] TRAIN Top shape for layer 26 'res4a_branch2a/relu' 6 256 40 40 (2457600)
I0815 19:04:18.959674 20809 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0815 19:04:18.959679 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.959692 20809 net.cpp:184] Created Layer res4a_branch2b (27)
I0815 19:04:18.959694 20809 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0815 19:04:18.959698 20809 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0815 19:04:18.968643 20809 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.36G, req 0G)
I0815 19:04:18.968657 20809 net.cpp:245] Setting up res4a_branch2b
I0815 19:04:18.968664 20809 net.cpp:252] TRAIN Top shape for layer 27 'res4a_branch2b' 6 256 40 40 (2457600)
I0815 19:04:18.968672 20809 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0815 19:04:18.968677 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.968683 20809 net.cpp:184] Created Layer res4a_branch2b/bn (28)
I0815 19:04:18.968688 20809 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0815 19:04:18.968693 20809 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0815 19:04:18.969334 20809 net.cpp:245] Setting up res4a_branch2b/bn
I0815 19:04:18.969343 20809 net.cpp:252] TRAIN Top shape for layer 28 'res4a_branch2b/bn' 6 256 40 40 (2457600)
I0815 19:04:18.969352 20809 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0815 19:04:18.969355 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.969360 20809 net.cpp:184] Created Layer res4a_branch2b/relu (29)
I0815 19:04:18.969364 20809 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0815 19:04:18.969368 20809 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0815 19:04:18.969375 20809 net.cpp:245] Setting up res4a_branch2b/relu
I0815 19:04:18.969379 20809 net.cpp:252] TRAIN Top shape for layer 29 'res4a_branch2b/relu' 6 256 40 40 (2457600)
I0815 19:04:18.969383 20809 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0815 19:04:18.969388 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.969393 20809 net.cpp:184] Created Layer pool4 (30)
I0815 19:04:18.969398 20809 net.cpp:561] pool4 <- res4a_branch2b
I0815 19:04:18.969403 20809 net.cpp:530] pool4 -> pool4
I0815 19:04:18.969471 20809 net.cpp:245] Setting up pool4
I0815 19:04:18.969476 20809 net.cpp:252] TRAIN Top shape for layer 30 'pool4' 6 256 40 40 (2457600)
I0815 19:04:18.969481 20809 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0815 19:04:18.969485 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.969497 20809 net.cpp:184] Created Layer res5a_branch2a (31)
I0815 19:04:18.969501 20809 net.cpp:561] res5a_branch2a <- pool4
I0815 19:04:18.969506 20809 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0815 19:04:18.995772 20809 net.cpp:245] Setting up res5a_branch2a
I0815 19:04:18.995793 20809 net.cpp:252] TRAIN Top shape for layer 31 'res5a_branch2a' 6 512 40 40 (4915200)
I0815 19:04:18.995803 20809 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0815 19:04:18.995810 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.995828 20809 net.cpp:184] Created Layer res5a_branch2a/bn (32)
I0815 19:04:18.995833 20809 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0815 19:04:18.995838 20809 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0815 19:04:18.996480 20809 net.cpp:245] Setting up res5a_branch2a/bn
I0815 19:04:18.996490 20809 net.cpp:252] TRAIN Top shape for layer 32 'res5a_branch2a/bn' 6 512 40 40 (4915200)
I0815 19:04:18.996507 20809 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0815 19:04:18.996512 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.996517 20809 net.cpp:184] Created Layer res5a_branch2a/relu (33)
I0815 19:04:18.996521 20809 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0815 19:04:18.996526 20809 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0815 19:04:18.996532 20809 net.cpp:245] Setting up res5a_branch2a/relu
I0815 19:04:18.996536 20809 net.cpp:252] TRAIN Top shape for layer 33 'res5a_branch2a/relu' 6 512 40 40 (4915200)
I0815 19:04:18.996541 20809 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0815 19:04:18.996546 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:18.996554 20809 net.cpp:184] Created Layer res5a_branch2b (34)
I0815 19:04:18.996558 20809 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0815 19:04:18.996562 20809 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0815 19:04:19.010489 20809 net.cpp:245] Setting up res5a_branch2b
I0815 19:04:19.010516 20809 net.cpp:252] TRAIN Top shape for layer 34 'res5a_branch2b' 6 512 40 40 (4915200)
I0815 19:04:19.010532 20809 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0815 19:04:19.010537 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.010547 20809 net.cpp:184] Created Layer res5a_branch2b/bn (35)
I0815 19:04:19.010551 20809 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0815 19:04:19.010556 20809 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0815 19:04:19.011183 20809 net.cpp:245] Setting up res5a_branch2b/bn
I0815 19:04:19.011191 20809 net.cpp:252] TRAIN Top shape for layer 35 'res5a_branch2b/bn' 6 512 40 40 (4915200)
I0815 19:04:19.011200 20809 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0815 19:04:19.011204 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.011209 20809 net.cpp:184] Created Layer res5a_branch2b/relu (36)
I0815 19:04:19.011222 20809 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0815 19:04:19.011227 20809 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0815 19:04:19.011234 20809 net.cpp:245] Setting up res5a_branch2b/relu
I0815 19:04:19.011240 20809 net.cpp:252] TRAIN Top shape for layer 36 'res5a_branch2b/relu' 6 512 40 40 (4915200)
I0815 19:04:19.011243 20809 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I0815 19:04:19.011248 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.011260 20809 net.cpp:184] Created Layer out5a (37)
I0815 19:04:19.011263 20809 net.cpp:561] out5a <- res5a_branch2b
I0815 19:04:19.011266 20809 net.cpp:530] out5a -> out5a
I0815 19:04:19.015564 20809 net.cpp:245] Setting up out5a
I0815 19:04:19.015585 20809 net.cpp:252] TRAIN Top shape for layer 37 'out5a' 6 64 40 40 (614400)
I0815 19:04:19.015594 20809 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I0815 19:04:19.015607 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.015616 20809 net.cpp:184] Created Layer out5a/bn (38)
I0815 19:04:19.015620 20809 net.cpp:561] out5a/bn <- out5a
I0815 19:04:19.015625 20809 net.cpp:513] out5a/bn -> out5a (in-place)
I0815 19:04:19.016278 20809 net.cpp:245] Setting up out5a/bn
I0815 19:04:19.016288 20809 net.cpp:252] TRAIN Top shape for layer 38 'out5a/bn' 6 64 40 40 (614400)
I0815 19:04:19.016295 20809 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I0815 19:04:19.016299 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.016305 20809 net.cpp:184] Created Layer out5a/relu (39)
I0815 19:04:19.016309 20809 net.cpp:561] out5a/relu <- out5a
I0815 19:04:19.016321 20809 net.cpp:513] out5a/relu -> out5a (in-place)
I0815 19:04:19.016329 20809 net.cpp:245] Setting up out5a/relu
I0815 19:04:19.016332 20809 net.cpp:252] TRAIN Top shape for layer 39 'out5a/relu' 6 64 40 40 (614400)
I0815 19:04:19.016337 20809 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I0815 19:04:19.016341 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.060957 20809 net.cpp:184] Created Layer out5a_up2 (40)
I0815 19:04:19.060968 20809 net.cpp:561] out5a_up2 <- out5a
I0815 19:04:19.060971 20809 net.cpp:530] out5a_up2 -> out5a_up2
I0815 19:04:19.061307 20809 net.cpp:245] Setting up out5a_up2
I0815 19:04:19.061316 20809 net.cpp:252] TRAIN Top shape for layer 40 'out5a_up2' 6 64 80 80 (2457600)
I0815 19:04:19.061319 20809 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I0815 19:04:19.061321 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.061329 20809 net.cpp:184] Created Layer out3a (41)
I0815 19:04:19.061331 20809 net.cpp:561] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0815 19:04:19.061334 20809 net.cpp:530] out3a -> out3a
I0815 19:04:19.072268 20809 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 1 3  (limit 7.3G, req 0G)
I0815 19:04:19.072280 20809 net.cpp:245] Setting up out3a
I0815 19:04:19.072284 20809 net.cpp:252] TRAIN Top shape for layer 41 'out3a' 6 64 80 80 (2457600)
I0815 19:04:19.072288 20809 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I0815 19:04:19.072291 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.072296 20809 net.cpp:184] Created Layer out3a/bn (42)
I0815 19:04:19.072299 20809 net.cpp:561] out3a/bn <- out3a
I0815 19:04:19.072301 20809 net.cpp:513] out3a/bn -> out3a (in-place)
I0815 19:04:19.072959 20809 net.cpp:245] Setting up out3a/bn
I0815 19:04:19.072966 20809 net.cpp:252] TRAIN Top shape for layer 42 'out3a/bn' 6 64 80 80 (2457600)
I0815 19:04:19.072973 20809 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I0815 19:04:19.072974 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.072978 20809 net.cpp:184] Created Layer out3a/relu (43)
I0815 19:04:19.072980 20809 net.cpp:561] out3a/relu <- out3a
I0815 19:04:19.072983 20809 net.cpp:513] out3a/relu -> out3a (in-place)
I0815 19:04:19.072986 20809 net.cpp:245] Setting up out3a/relu
I0815 19:04:19.072988 20809 net.cpp:252] TRAIN Top shape for layer 43 'out3a/relu' 6 64 80 80 (2457600)
I0815 19:04:19.072990 20809 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I0815 19:04:19.072993 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.142196 20809 net.cpp:184] Created Layer out3_out5_combined (44)
I0815 19:04:19.142206 20809 net.cpp:561] out3_out5_combined <- out5a_up2
I0815 19:04:19.142210 20809 net.cpp:561] out3_out5_combined <- out3a
I0815 19:04:19.142213 20809 net.cpp:530] out3_out5_combined -> out3_out5_combined
I0815 19:04:19.177711 20809 net.cpp:245] Setting up out3_out5_combined
I0815 19:04:19.177721 20809 net.cpp:252] TRAIN Top shape for layer 44 'out3_out5_combined' 6 64 80 80 (2457600)
I0815 19:04:19.177726 20809 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I0815 19:04:19.177728 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.177742 20809 net.cpp:184] Created Layer ctx_conv1 (45)
I0815 19:04:19.177745 20809 net.cpp:561] ctx_conv1 <- out3_out5_combined
I0815 19:04:19.177747 20809 net.cpp:530] ctx_conv1 -> ctx_conv1
I0815 19:04:19.190714 20809 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 7.24G, req 0G)
I0815 19:04:19.190733 20809 net.cpp:245] Setting up ctx_conv1
I0815 19:04:19.190739 20809 net.cpp:252] TRAIN Top shape for layer 45 'ctx_conv1' 6 64 80 80 (2457600)
I0815 19:04:19.190759 20809 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I0815 19:04:19.190764 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.190773 20809 net.cpp:184] Created Layer ctx_conv1/bn (46)
I0815 19:04:19.190778 20809 net.cpp:561] ctx_conv1/bn <- ctx_conv1
I0815 19:04:19.190783 20809 net.cpp:513] ctx_conv1/bn -> ctx_conv1 (in-place)
I0815 19:04:19.191746 20809 net.cpp:245] Setting up ctx_conv1/bn
I0815 19:04:19.191756 20809 net.cpp:252] TRAIN Top shape for layer 46 'ctx_conv1/bn' 6 64 80 80 (2457600)
I0815 19:04:19.191764 20809 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I0815 19:04:19.191769 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.191774 20809 net.cpp:184] Created Layer ctx_conv1/relu (47)
I0815 19:04:19.191777 20809 net.cpp:561] ctx_conv1/relu <- ctx_conv1
I0815 19:04:19.191781 20809 net.cpp:513] ctx_conv1/relu -> ctx_conv1 (in-place)
I0815 19:04:19.191787 20809 net.cpp:245] Setting up ctx_conv1/relu
I0815 19:04:19.191792 20809 net.cpp:252] TRAIN Top shape for layer 47 'ctx_conv1/relu' 6 64 80 80 (2457600)
I0815 19:04:19.191794 20809 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I0815 19:04:19.191798 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.191807 20809 net.cpp:184] Created Layer ctx_conv2 (48)
I0815 19:04:19.191810 20809 net.cpp:561] ctx_conv2 <- ctx_conv1
I0815 19:04:19.191814 20809 net.cpp:530] ctx_conv2 -> ctx_conv2
I0815 19:04:19.193331 20809 net.cpp:245] Setting up ctx_conv2
I0815 19:04:19.193341 20809 net.cpp:252] TRAIN Top shape for layer 48 'ctx_conv2' 6 64 80 80 (2457600)
I0815 19:04:19.193346 20809 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I0815 19:04:19.193351 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.193356 20809 net.cpp:184] Created Layer ctx_conv2/bn (49)
I0815 19:04:19.193361 20809 net.cpp:561] ctx_conv2/bn <- ctx_conv2
I0815 19:04:19.193364 20809 net.cpp:513] ctx_conv2/bn -> ctx_conv2 (in-place)
I0815 19:04:19.194257 20809 net.cpp:245] Setting up ctx_conv2/bn
I0815 19:04:19.194265 20809 net.cpp:252] TRAIN Top shape for layer 49 'ctx_conv2/bn' 6 64 80 80 (2457600)
I0815 19:04:19.194274 20809 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I0815 19:04:19.194278 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.194283 20809 net.cpp:184] Created Layer ctx_conv2/relu (50)
I0815 19:04:19.194286 20809 net.cpp:561] ctx_conv2/relu <- ctx_conv2
I0815 19:04:19.194290 20809 net.cpp:513] ctx_conv2/relu -> ctx_conv2 (in-place)
I0815 19:04:19.194295 20809 net.cpp:245] Setting up ctx_conv2/relu
I0815 19:04:19.194299 20809 net.cpp:252] TRAIN Top shape for layer 50 'ctx_conv2/relu' 6 64 80 80 (2457600)
I0815 19:04:19.194303 20809 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I0815 19:04:19.194308 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.194314 20809 net.cpp:184] Created Layer ctx_conv3 (51)
I0815 19:04:19.194317 20809 net.cpp:561] ctx_conv3 <- ctx_conv2
I0815 19:04:19.194321 20809 net.cpp:530] ctx_conv3 -> ctx_conv3
I0815 19:04:19.195812 20809 net.cpp:245] Setting up ctx_conv3
I0815 19:04:19.195822 20809 net.cpp:252] TRAIN Top shape for layer 51 'ctx_conv3' 6 64 80 80 (2457600)
I0815 19:04:19.195827 20809 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I0815 19:04:19.195832 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.195837 20809 net.cpp:184] Created Layer ctx_conv3/bn (52)
I0815 19:04:19.195840 20809 net.cpp:561] ctx_conv3/bn <- ctx_conv3
I0815 19:04:19.195843 20809 net.cpp:513] ctx_conv3/bn -> ctx_conv3 (in-place)
I0815 19:04:19.196735 20809 net.cpp:245] Setting up ctx_conv3/bn
I0815 19:04:19.196745 20809 net.cpp:252] TRAIN Top shape for layer 52 'ctx_conv3/bn' 6 64 80 80 (2457600)
I0815 19:04:19.196753 20809 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I0815 19:04:19.196756 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.196761 20809 net.cpp:184] Created Layer ctx_conv3/relu (53)
I0815 19:04:19.196764 20809 net.cpp:561] ctx_conv3/relu <- ctx_conv3
I0815 19:04:19.196768 20809 net.cpp:513] ctx_conv3/relu -> ctx_conv3 (in-place)
I0815 19:04:19.196772 20809 net.cpp:245] Setting up ctx_conv3/relu
I0815 19:04:19.196777 20809 net.cpp:252] TRAIN Top shape for layer 53 'ctx_conv3/relu' 6 64 80 80 (2457600)
I0815 19:04:19.196781 20809 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I0815 19:04:19.196784 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.196791 20809 net.cpp:184] Created Layer ctx_conv4 (54)
I0815 19:04:19.196795 20809 net.cpp:561] ctx_conv4 <- ctx_conv3
I0815 19:04:19.196799 20809 net.cpp:530] ctx_conv4 -> ctx_conv4
I0815 19:04:19.198465 20809 net.cpp:245] Setting up ctx_conv4
I0815 19:04:19.198478 20809 net.cpp:252] TRAIN Top shape for layer 54 'ctx_conv4' 6 64 80 80 (2457600)
I0815 19:04:19.198487 20809 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I0815 19:04:19.198492 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.198511 20809 net.cpp:184] Created Layer ctx_conv4/bn (55)
I0815 19:04:19.198518 20809 net.cpp:561] ctx_conv4/bn <- ctx_conv4
I0815 19:04:19.198523 20809 net.cpp:513] ctx_conv4/bn -> ctx_conv4 (in-place)
I0815 19:04:19.199522 20809 net.cpp:245] Setting up ctx_conv4/bn
I0815 19:04:19.199532 20809 net.cpp:252] TRAIN Top shape for layer 55 'ctx_conv4/bn' 6 64 80 80 (2457600)
I0815 19:04:19.199540 20809 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I0815 19:04:19.199543 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.199548 20809 net.cpp:184] Created Layer ctx_conv4/relu (56)
I0815 19:04:19.199553 20809 net.cpp:561] ctx_conv4/relu <- ctx_conv4
I0815 19:04:19.199556 20809 net.cpp:513] ctx_conv4/relu -> ctx_conv4 (in-place)
I0815 19:04:19.199561 20809 net.cpp:245] Setting up ctx_conv4/relu
I0815 19:04:19.199566 20809 net.cpp:252] TRAIN Top shape for layer 56 'ctx_conv4/relu' 6 64 80 80 (2457600)
I0815 19:04:19.199569 20809 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I0815 19:04:19.199573 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.199581 20809 net.cpp:184] Created Layer ctx_final (57)
I0815 19:04:19.199585 20809 net.cpp:561] ctx_final <- ctx_conv4
I0815 19:04:19.199590 20809 net.cpp:530] ctx_final -> ctx_final
I0815 19:04:19.212684 20809 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 7.22G, req 0G)
I0815 19:04:19.212702 20809 net.cpp:245] Setting up ctx_final
I0815 19:04:19.212709 20809 net.cpp:252] TRAIN Top shape for layer 57 'ctx_final' 6 8 80 80 (307200)
I0815 19:04:19.212716 20809 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I0815 19:04:19.212721 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.212728 20809 net.cpp:184] Created Layer ctx_final/relu (58)
I0815 19:04:19.212733 20809 net.cpp:561] ctx_final/relu <- ctx_final
I0815 19:04:19.212738 20809 net.cpp:513] ctx_final/relu -> ctx_final (in-place)
I0815 19:04:19.212743 20809 net.cpp:245] Setting up ctx_final/relu
I0815 19:04:19.212748 20809 net.cpp:252] TRAIN Top shape for layer 58 'ctx_final/relu' 6 8 80 80 (307200)
I0815 19:04:19.212749 20809 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I0815 19:04:19.212751 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.212769 20809 net.cpp:184] Created Layer out_deconv_final_up2 (59)
I0815 19:04:19.212772 20809 net.cpp:561] out_deconv_final_up2 <- ctx_final
I0815 19:04:19.212774 20809 net.cpp:530] out_deconv_final_up2 -> out_deconv_final_up2
I0815 19:04:19.213148 20809 net.cpp:245] Setting up out_deconv_final_up2
I0815 19:04:19.213155 20809 net.cpp:252] TRAIN Top shape for layer 59 'out_deconv_final_up2' 6 8 160 160 (1228800)
I0815 19:04:19.213160 20809 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I0815 19:04:19.213162 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.356034 20809 net.cpp:184] Created Layer out_deconv_final_up4 (60)
I0815 19:04:19.356055 20809 net.cpp:561] out_deconv_final_up4 <- out_deconv_final_up2
I0815 19:04:19.356061 20809 net.cpp:530] out_deconv_final_up4 -> out_deconv_final_up4
I0815 19:04:19.356451 20809 net.cpp:245] Setting up out_deconv_final_up4
I0815 19:04:19.356461 20809 net.cpp:252] TRAIN Top shape for layer 60 'out_deconv_final_up4' 6 8 320 320 (4915200)
I0815 19:04:19.356465 20809 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I0815 19:04:19.356468 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.356475 20809 net.cpp:184] Created Layer out_deconv_final_up8 (61)
I0815 19:04:19.356478 20809 net.cpp:561] out_deconv_final_up8 <- out_deconv_final_up4
I0815 19:04:19.356480 20809 net.cpp:530] out_deconv_final_up8 -> out_deconv_final_up8
I0815 19:04:19.356745 20809 net.cpp:245] Setting up out_deconv_final_up8
I0815 19:04:19.356750 20809 net.cpp:252] TRAIN Top shape for layer 61 'out_deconv_final_up8' 6 8 640 640 (19660800)
I0815 19:04:19.356753 20809 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0815 19:04:19.356756 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.356765 20809 net.cpp:184] Created Layer loss (62)
I0815 19:04:19.356768 20809 net.cpp:561] loss <- out_deconv_final_up8
I0815 19:04:19.356770 20809 net.cpp:561] loss <- label
I0815 19:04:19.356775 20809 net.cpp:530] loss -> loss
I0815 19:04:19.358168 20809 net.cpp:245] Setting up loss
I0815 19:04:19.358177 20809 net.cpp:252] TRAIN Top shape for layer 62 'loss' (1)
I0815 19:04:19.358180 20809 net.cpp:256]     with loss weight 1
I0815 19:04:19.358186 20809 net.cpp:323] loss needs backward computation.
I0815 19:04:19.358187 20809 net.cpp:323] out_deconv_final_up8 needs backward computation.
I0815 19:04:19.358189 20809 net.cpp:323] out_deconv_final_up4 needs backward computation.
I0815 19:04:19.358191 20809 net.cpp:323] out_deconv_final_up2 needs backward computation.
I0815 19:04:19.358193 20809 net.cpp:323] ctx_final/relu needs backward computation.
I0815 19:04:19.358196 20809 net.cpp:323] ctx_final needs backward computation.
I0815 19:04:19.358196 20809 net.cpp:323] ctx_conv4/relu needs backward computation.
I0815 19:04:19.358198 20809 net.cpp:323] ctx_conv4/bn needs backward computation.
I0815 19:04:19.358201 20809 net.cpp:323] ctx_conv4 needs backward computation.
I0815 19:04:19.358203 20809 net.cpp:323] ctx_conv3/relu needs backward computation.
I0815 19:04:19.358204 20809 net.cpp:323] ctx_conv3/bn needs backward computation.
I0815 19:04:19.358206 20809 net.cpp:323] ctx_conv3 needs backward computation.
I0815 19:04:19.358208 20809 net.cpp:323] ctx_conv2/relu needs backward computation.
I0815 19:04:19.358211 20809 net.cpp:323] ctx_conv2/bn needs backward computation.
I0815 19:04:19.358213 20809 net.cpp:323] ctx_conv2 needs backward computation.
I0815 19:04:19.358216 20809 net.cpp:323] ctx_conv1/relu needs backward computation.
I0815 19:04:19.358218 20809 net.cpp:323] ctx_conv1/bn needs backward computation.
I0815 19:04:19.358219 20809 net.cpp:323] ctx_conv1 needs backward computation.
I0815 19:04:19.358222 20809 net.cpp:323] out3_out5_combined needs backward computation.
I0815 19:04:19.358224 20809 net.cpp:323] out3a/relu needs backward computation.
I0815 19:04:19.358234 20809 net.cpp:323] out3a/bn needs backward computation.
I0815 19:04:19.358237 20809 net.cpp:323] out3a needs backward computation.
I0815 19:04:19.358238 20809 net.cpp:323] out5a_up2 needs backward computation.
I0815 19:04:19.358240 20809 net.cpp:323] out5a/relu needs backward computation.
I0815 19:04:19.358242 20809 net.cpp:323] out5a/bn needs backward computation.
I0815 19:04:19.358244 20809 net.cpp:323] out5a needs backward computation.
I0815 19:04:19.358247 20809 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0815 19:04:19.358248 20809 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0815 19:04:19.358249 20809 net.cpp:323] res5a_branch2b needs backward computation.
I0815 19:04:19.358252 20809 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0815 19:04:19.358253 20809 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0815 19:04:19.358254 20809 net.cpp:323] res5a_branch2a needs backward computation.
I0815 19:04:19.358256 20809 net.cpp:323] pool4 needs backward computation.
I0815 19:04:19.358258 20809 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0815 19:04:19.358260 20809 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0815 19:04:19.358263 20809 net.cpp:323] res4a_branch2b needs backward computation.
I0815 19:04:19.358264 20809 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0815 19:04:19.358266 20809 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0815 19:04:19.358268 20809 net.cpp:323] res4a_branch2a needs backward computation.
I0815 19:04:19.358269 20809 net.cpp:323] pool3 needs backward computation.
I0815 19:04:19.358271 20809 net.cpp:323] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I0815 19:04:19.358273 20809 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0815 19:04:19.358275 20809 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0815 19:04:19.358278 20809 net.cpp:323] res3a_branch2b needs backward computation.
I0815 19:04:19.358279 20809 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0815 19:04:19.358281 20809 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0815 19:04:19.358284 20809 net.cpp:323] res3a_branch2a needs backward computation.
I0815 19:04:19.358286 20809 net.cpp:323] pool2 needs backward computation.
I0815 19:04:19.358289 20809 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0815 19:04:19.358290 20809 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0815 19:04:19.358292 20809 net.cpp:323] res2a_branch2b needs backward computation.
I0815 19:04:19.358295 20809 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0815 19:04:19.358297 20809 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0815 19:04:19.358300 20809 net.cpp:323] res2a_branch2a needs backward computation.
I0815 19:04:19.358302 20809 net.cpp:323] pool1 needs backward computation.
I0815 19:04:19.358305 20809 net.cpp:323] conv1b/relu needs backward computation.
I0815 19:04:19.358309 20809 net.cpp:323] conv1b/bn needs backward computation.
I0815 19:04:19.358310 20809 net.cpp:323] conv1b needs backward computation.
I0815 19:04:19.358312 20809 net.cpp:323] conv1a/relu needs backward computation.
I0815 19:04:19.358314 20809 net.cpp:323] conv1a/bn needs backward computation.
I0815 19:04:19.358317 20809 net.cpp:323] conv1a needs backward computation.
I0815 19:04:19.358319 20809 net.cpp:325] data/bias does not need backward computation.
I0815 19:04:19.358324 20809 net.cpp:325] data does not need backward computation.
I0815 19:04:19.358325 20809 net.cpp:367] This network produces output loss
I0815 19:04:19.358376 20809 net.cpp:389] Top memory (TRAIN) required for data: 956006400 diff: 946176008
I0815 19:04:19.358379 20809 net.cpp:392] Bottom memory (TRAIN) required for data: 956006400 diff: 956006400
I0815 19:04:19.358381 20809 net.cpp:395] Shared (in-place) memory (TRAIN) by data: 630374400 diff: 630374400
I0815 19:04:19.358383 20809 net.cpp:398] Parameters memory (TRAIN) required for data: 2692608 diff: 2692608
I0815 19:04:19.358388 20809 net.cpp:401] Parameters shared memory (TRAIN) by data: 0 diff: 0
I0815 19:04:19.358392 20809 net.cpp:407] Network initialization done.
I0815 19:04:19.358944 20809 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/initial/test.prototxt
W0815 19:04:19.359001 20809 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0815 19:04:19.359176 20809 net.cpp:72] Initializing net from parameters: 
name: "jsegnet21v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/val-image-lmdb"
    label_list_path: "data/val-label-lmdb"
    batch_size: 2
    threads: 1
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
  accuracy_param {
    ignore_label: 255
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
    ignore_label: 255
  }
}
I0815 19:04:19.359328 20809 net.cpp:104] Using FLOAT as default forward math type
I0815 19:04:19.359333 20809 net.cpp:110] Using FLOAT as default backward math type
I0815 19:04:19.359335 20809 layer_factory.hpp:136] Creating layer 'data' of type 'ImageLabelData'
I0815 19:04:19.359342 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.359347 20809 net.cpp:184] Created Layer data (0)
I0815 19:04:19.359350 20809 net.cpp:530] data -> data
I0815 19:04:19.359354 20809 net.cpp:530] data -> label
I0815 19:04:19.359371 20809 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0815 19:04:19.359377 20809 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 19:04:19.440485 20869 db_lmdb.cpp:24] Opened lmdb data/val-image-lmdb
I0815 19:04:19.564152 20809 data_layer.cpp:185] (0) ReshapePrefetch 2, 3, 640, 640
I0815 19:04:19.564262 20809 data_layer.cpp:209] (0) Output data size: 2, 3, 640, 640
I0815 19:04:19.564270 20809 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 19:04:19.564309 20809 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0815 19:04:19.564321 20809 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 19:04:19.565315 20884 data_layer.cpp:97] (0) Parser threads: 1
I0815 19:04:19.565331 20884 data_layer.cpp:99] (0) Transformer threads: 1
I0815 19:04:19.623591 20885 db_lmdb.cpp:24] Opened lmdb data/val-label-lmdb
I0815 19:04:19.694633 20809 data_layer.cpp:185] (0) ReshapePrefetch 2, 1, 640, 640
I0815 19:04:19.694720 20809 data_layer.cpp:209] (0) Output data size: 2, 1, 640, 640
I0815 19:04:19.694728 20809 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 19:04:19.694775 20809 net.cpp:245] Setting up data
I0815 19:04:19.694785 20809 net.cpp:252] TEST Top shape for layer 0 'data' 2 3 640 640 (2457600)
I0815 19:04:19.694794 20809 net.cpp:252] TEST Top shape for layer 0 'data' 2 1 640 640 (819200)
I0815 19:04:19.694802 20809 layer_factory.hpp:136] Creating layer 'label_data_1_split' of type 'Split'
I0815 19:04:19.694809 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.694821 20809 net.cpp:184] Created Layer label_data_1_split (1)
I0815 19:04:19.694825 20809 net.cpp:561] label_data_1_split <- label
I0815 19:04:19.694831 20809 net.cpp:530] label_data_1_split -> label_data_1_split_0
I0815 19:04:19.694838 20809 net.cpp:530] label_data_1_split -> label_data_1_split_1
I0815 19:04:19.694842 20809 net.cpp:530] label_data_1_split -> label_data_1_split_2
I0815 19:04:19.695014 20809 net.cpp:245] Setting up label_data_1_split
I0815 19:04:19.695024 20809 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0815 19:04:19.695029 20809 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0815 19:04:19.695034 20809 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0815 19:04:19.695039 20809 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0815 19:04:19.695044 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.695052 20809 net.cpp:184] Created Layer data/bias (2)
I0815 19:04:19.695055 20809 net.cpp:561] data/bias <- data
I0815 19:04:19.695060 20809 net.cpp:530] data/bias -> data/bias
I0815 19:04:19.695915 20886 data_layer.cpp:97] (0) Parser threads: 1
I0815 19:04:19.695925 20886 data_layer.cpp:99] (0) Transformer threads: 1
I0815 19:04:19.697540 20809 net.cpp:245] Setting up data/bias
I0815 19:04:19.697552 20809 net.cpp:252] TEST Top shape for layer 2 'data/bias' 2 3 640 640 (2457600)
I0815 19:04:19.697561 20809 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0815 19:04:19.697567 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.697582 20809 net.cpp:184] Created Layer conv1a (3)
I0815 19:04:19.697585 20809 net.cpp:561] conv1a <- data/bias
I0815 19:04:19.697590 20809 net.cpp:530] conv1a -> conv1a
I0815 19:04:19.701495 20809 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.1G, req 0G)
I0815 19:04:19.701508 20809 net.cpp:245] Setting up conv1a
I0815 19:04:19.701514 20809 net.cpp:252] TEST Top shape for layer 3 'conv1a' 2 32 320 320 (6553600)
I0815 19:04:19.701531 20809 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0815 19:04:19.701536 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.701545 20809 net.cpp:184] Created Layer conv1a/bn (4)
I0815 19:04:19.701550 20809 net.cpp:561] conv1a/bn <- conv1a
I0815 19:04:19.701553 20809 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0815 19:04:19.702247 20809 net.cpp:245] Setting up conv1a/bn
I0815 19:04:19.702255 20809 net.cpp:252] TEST Top shape for layer 4 'conv1a/bn' 2 32 320 320 (6553600)
I0815 19:04:19.702265 20809 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0815 19:04:19.702270 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.702275 20809 net.cpp:184] Created Layer conv1a/relu (5)
I0815 19:04:19.702280 20809 net.cpp:561] conv1a/relu <- conv1a
I0815 19:04:19.702282 20809 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0815 19:04:19.702289 20809 net.cpp:245] Setting up conv1a/relu
I0815 19:04:19.702296 20809 net.cpp:252] TEST Top shape for layer 5 'conv1a/relu' 2 32 320 320 (6553600)
I0815 19:04:19.702298 20809 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0815 19:04:19.702301 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.702308 20809 net.cpp:184] Created Layer conv1b (6)
I0815 19:04:19.702312 20809 net.cpp:561] conv1b <- conv1a
I0815 19:04:19.702316 20809 net.cpp:530] conv1b -> conv1b
I0815 19:04:19.715555 20809 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.07G, req 0G)
I0815 19:04:19.715567 20809 net.cpp:245] Setting up conv1b
I0815 19:04:19.715571 20809 net.cpp:252] TEST Top shape for layer 6 'conv1b' 2 32 320 320 (6553600)
I0815 19:04:19.715577 20809 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0815 19:04:19.715580 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.715585 20809 net.cpp:184] Created Layer conv1b/bn (7)
I0815 19:04:19.715589 20809 net.cpp:561] conv1b/bn <- conv1b
I0815 19:04:19.715590 20809 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0815 19:04:19.716389 20809 net.cpp:245] Setting up conv1b/bn
I0815 19:04:19.716398 20809 net.cpp:252] TEST Top shape for layer 7 'conv1b/bn' 2 32 320 320 (6553600)
I0815 19:04:19.716403 20809 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0815 19:04:19.716408 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.716415 20809 net.cpp:184] Created Layer conv1b/relu (8)
I0815 19:04:19.716418 20809 net.cpp:561] conv1b/relu <- conv1b
I0815 19:04:19.716421 20809 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0815 19:04:19.716425 20809 net.cpp:245] Setting up conv1b/relu
I0815 19:04:19.716428 20809 net.cpp:252] TEST Top shape for layer 8 'conv1b/relu' 2 32 320 320 (6553600)
I0815 19:04:19.716430 20809 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0815 19:04:19.716434 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.716439 20809 net.cpp:184] Created Layer pool1 (9)
I0815 19:04:19.716444 20809 net.cpp:561] pool1 <- conv1b
I0815 19:04:19.716447 20809 net.cpp:530] pool1 -> pool1
I0815 19:04:19.716526 20809 net.cpp:245] Setting up pool1
I0815 19:04:19.716531 20809 net.cpp:252] TEST Top shape for layer 9 'pool1' 2 32 160 160 (1638400)
I0815 19:04:19.716533 20809 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0815 19:04:19.716537 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.716547 20809 net.cpp:184] Created Layer res2a_branch2a (10)
I0815 19:04:19.716550 20809 net.cpp:561] res2a_branch2a <- pool1
I0815 19:04:19.716553 20809 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0815 19:04:19.724673 20809 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.05G, req 0G)
I0815 19:04:19.724692 20809 net.cpp:245] Setting up res2a_branch2a
I0815 19:04:19.724697 20809 net.cpp:252] TEST Top shape for layer 10 'res2a_branch2a' 2 64 160 160 (3276800)
I0815 19:04:19.724705 20809 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0815 19:04:19.724710 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.724715 20809 net.cpp:184] Created Layer res2a_branch2a/bn (11)
I0815 19:04:19.724720 20809 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0815 19:04:19.724723 20809 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0815 19:04:19.725432 20809 net.cpp:245] Setting up res2a_branch2a/bn
I0815 19:04:19.725440 20809 net.cpp:252] TEST Top shape for layer 11 'res2a_branch2a/bn' 2 64 160 160 (3276800)
I0815 19:04:19.725445 20809 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0815 19:04:19.725448 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.725451 20809 net.cpp:184] Created Layer res2a_branch2a/relu (12)
I0815 19:04:19.725455 20809 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0815 19:04:19.725456 20809 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0815 19:04:19.725461 20809 net.cpp:245] Setting up res2a_branch2a/relu
I0815 19:04:19.725463 20809 net.cpp:252] TEST Top shape for layer 12 'res2a_branch2a/relu' 2 64 160 160 (3276800)
I0815 19:04:19.725466 20809 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0815 19:04:19.725468 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.725474 20809 net.cpp:184] Created Layer res2a_branch2b (13)
I0815 19:04:19.725479 20809 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0815 19:04:19.725482 20809 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0815 19:04:19.731919 20809 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.03G, req 0G)
I0815 19:04:19.731935 20809 net.cpp:245] Setting up res2a_branch2b
I0815 19:04:19.731940 20809 net.cpp:252] TEST Top shape for layer 13 'res2a_branch2b' 2 64 160 160 (3276800)
I0815 19:04:19.731945 20809 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0815 19:04:19.731952 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.731972 20809 net.cpp:184] Created Layer res2a_branch2b/bn (14)
I0815 19:04:19.731978 20809 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0815 19:04:19.731983 20809 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0815 19:04:19.732772 20809 net.cpp:245] Setting up res2a_branch2b/bn
I0815 19:04:19.732782 20809 net.cpp:252] TEST Top shape for layer 14 'res2a_branch2b/bn' 2 64 160 160 (3276800)
I0815 19:04:19.732789 20809 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0815 19:04:19.732794 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.732797 20809 net.cpp:184] Created Layer res2a_branch2b/relu (15)
I0815 19:04:19.732801 20809 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0815 19:04:19.732805 20809 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0815 19:04:19.732811 20809 net.cpp:245] Setting up res2a_branch2b/relu
I0815 19:04:19.732816 20809 net.cpp:252] TEST Top shape for layer 15 'res2a_branch2b/relu' 2 64 160 160 (3276800)
I0815 19:04:19.732827 20809 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0815 19:04:19.732832 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.732841 20809 net.cpp:184] Created Layer pool2 (16)
I0815 19:04:19.732846 20809 net.cpp:561] pool2 <- res2a_branch2b
I0815 19:04:19.732849 20809 net.cpp:530] pool2 -> pool2
I0815 19:04:19.732930 20809 net.cpp:245] Setting up pool2
I0815 19:04:19.732937 20809 net.cpp:252] TEST Top shape for layer 16 'pool2' 2 64 80 80 (819200)
I0815 19:04:19.732954 20809 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0815 19:04:19.732959 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.732969 20809 net.cpp:184] Created Layer res3a_branch2a (17)
I0815 19:04:19.732972 20809 net.cpp:561] res3a_branch2a <- pool2
I0815 19:04:19.732977 20809 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0815 19:04:19.738621 20809 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.02G, req 0G)
I0815 19:04:19.738634 20809 net.cpp:245] Setting up res3a_branch2a
I0815 19:04:19.738641 20809 net.cpp:252] TEST Top shape for layer 17 'res3a_branch2a' 2 128 80 80 (1638400)
I0815 19:04:19.738648 20809 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0815 19:04:19.738652 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.738661 20809 net.cpp:184] Created Layer res3a_branch2a/bn (18)
I0815 19:04:19.738664 20809 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0815 19:04:19.738668 20809 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0815 19:04:19.739379 20809 net.cpp:245] Setting up res3a_branch2a/bn
I0815 19:04:19.739388 20809 net.cpp:252] TEST Top shape for layer 18 'res3a_branch2a/bn' 2 128 80 80 (1638400)
I0815 19:04:19.739398 20809 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0815 19:04:19.739403 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.739408 20809 net.cpp:184] Created Layer res3a_branch2a/relu (19)
I0815 19:04:19.739411 20809 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0815 19:04:19.739415 20809 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0815 19:04:19.739421 20809 net.cpp:245] Setting up res3a_branch2a/relu
I0815 19:04:19.739426 20809 net.cpp:252] TEST Top shape for layer 19 'res3a_branch2a/relu' 2 128 80 80 (1638400)
I0815 19:04:19.739430 20809 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0815 19:04:19.739434 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.739454 20809 net.cpp:184] Created Layer res3a_branch2b (20)
I0815 19:04:19.739457 20809 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0815 19:04:19.739461 20809 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0815 19:04:19.745003 20809 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7G, req 0G)
I0815 19:04:19.745015 20809 net.cpp:245] Setting up res3a_branch2b
I0815 19:04:19.745021 20809 net.cpp:252] TEST Top shape for layer 20 'res3a_branch2b' 2 128 80 80 (1638400)
I0815 19:04:19.745028 20809 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0815 19:04:19.745033 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.745040 20809 net.cpp:184] Created Layer res3a_branch2b/bn (21)
I0815 19:04:19.745045 20809 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0815 19:04:19.745049 20809 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0815 19:04:19.745736 20809 net.cpp:245] Setting up res3a_branch2b/bn
I0815 19:04:19.745744 20809 net.cpp:252] TEST Top shape for layer 21 'res3a_branch2b/bn' 2 128 80 80 (1638400)
I0815 19:04:19.745754 20809 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0815 19:04:19.745757 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.745761 20809 net.cpp:184] Created Layer res3a_branch2b/relu (22)
I0815 19:04:19.745765 20809 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0815 19:04:19.745769 20809 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0815 19:04:19.745775 20809 net.cpp:245] Setting up res3a_branch2b/relu
I0815 19:04:19.745780 20809 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b/relu' 2 128 80 80 (1638400)
I0815 19:04:19.745790 20809 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0815 19:04:19.745795 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.745800 20809 net.cpp:184] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (23)
I0815 19:04:19.745805 20809 net.cpp:561] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0815 19:04:19.745810 20809 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0815 19:04:19.745815 20809 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0815 19:04:19.745862 20809 net.cpp:245] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0815 19:04:19.745868 20809 net.cpp:252] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 2 128 80 80 (1638400)
I0815 19:04:19.745873 20809 net.cpp:252] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 2 128 80 80 (1638400)
I0815 19:04:19.745877 20809 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0815 19:04:19.745882 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.745888 20809 net.cpp:184] Created Layer pool3 (24)
I0815 19:04:19.745893 20809 net.cpp:561] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0815 19:04:19.745896 20809 net.cpp:530] pool3 -> pool3
I0815 19:04:19.745968 20809 net.cpp:245] Setting up pool3
I0815 19:04:19.745975 20809 net.cpp:252] TEST Top shape for layer 24 'pool3' 2 128 40 40 (409600)
I0815 19:04:19.745978 20809 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0815 19:04:19.745982 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.745995 20809 net.cpp:184] Created Layer res4a_branch2a (25)
I0815 19:04:19.745999 20809 net.cpp:561] res4a_branch2a <- pool3
I0815 19:04:19.746003 20809 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0815 19:04:19.757181 20809 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.99G, req 0G)
I0815 19:04:19.757199 20809 net.cpp:245] Setting up res4a_branch2a
I0815 19:04:19.757205 20809 net.cpp:252] TEST Top shape for layer 25 'res4a_branch2a' 2 256 40 40 (819200)
I0815 19:04:19.757212 20809 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0815 19:04:19.757218 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.757290 20809 net.cpp:184] Created Layer res4a_branch2a/bn (26)
I0815 19:04:19.757297 20809 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0815 19:04:19.757302 20809 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0815 19:04:19.758059 20809 net.cpp:245] Setting up res4a_branch2a/bn
I0815 19:04:19.758067 20809 net.cpp:252] TEST Top shape for layer 26 'res4a_branch2a/bn' 2 256 40 40 (819200)
I0815 19:04:19.758076 20809 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0815 19:04:19.758081 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.758086 20809 net.cpp:184] Created Layer res4a_branch2a/relu (27)
I0815 19:04:19.758090 20809 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0815 19:04:19.758095 20809 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0815 19:04:19.758101 20809 net.cpp:245] Setting up res4a_branch2a/relu
I0815 19:04:19.758106 20809 net.cpp:252] TEST Top shape for layer 27 'res4a_branch2a/relu' 2 256 40 40 (819200)
I0815 19:04:19.758111 20809 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0815 19:04:19.758114 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.758124 20809 net.cpp:184] Created Layer res4a_branch2b (28)
I0815 19:04:19.758136 20809 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0815 19:04:19.758141 20809 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0815 19:04:19.764926 20809 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.99G, req 0G)
I0815 19:04:19.764940 20809 net.cpp:245] Setting up res4a_branch2b
I0815 19:04:19.764946 20809 net.cpp:252] TEST Top shape for layer 28 'res4a_branch2b' 2 256 40 40 (819200)
I0815 19:04:19.764953 20809 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0815 19:04:19.764960 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.764971 20809 net.cpp:184] Created Layer res4a_branch2b/bn (29)
I0815 19:04:19.764976 20809 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0815 19:04:19.764981 20809 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0815 19:04:19.765697 20809 net.cpp:245] Setting up res4a_branch2b/bn
I0815 19:04:19.765705 20809 net.cpp:252] TEST Top shape for layer 29 'res4a_branch2b/bn' 2 256 40 40 (819200)
I0815 19:04:19.765714 20809 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0815 19:04:19.765718 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.765723 20809 net.cpp:184] Created Layer res4a_branch2b/relu (30)
I0815 19:04:19.765727 20809 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0815 19:04:19.765732 20809 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0815 19:04:19.765738 20809 net.cpp:245] Setting up res4a_branch2b/relu
I0815 19:04:19.765743 20809 net.cpp:252] TEST Top shape for layer 30 'res4a_branch2b/relu' 2 256 40 40 (819200)
I0815 19:04:19.765746 20809 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0815 19:04:19.765751 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.765758 20809 net.cpp:184] Created Layer pool4 (31)
I0815 19:04:19.765761 20809 net.cpp:561] pool4 <- res4a_branch2b
I0815 19:04:19.765765 20809 net.cpp:530] pool4 -> pool4
I0815 19:04:19.765837 20809 net.cpp:245] Setting up pool4
I0815 19:04:19.765843 20809 net.cpp:252] TEST Top shape for layer 31 'pool4' 2 256 40 40 (819200)
I0815 19:04:19.765847 20809 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0815 19:04:19.765852 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.765867 20809 net.cpp:184] Created Layer res5a_branch2a (32)
I0815 19:04:19.765872 20809 net.cpp:561] res5a_branch2a <- pool4
I0815 19:04:19.765875 20809 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0815 19:04:19.791191 20809 net.cpp:245] Setting up res5a_branch2a
I0815 19:04:19.791216 20809 net.cpp:252] TEST Top shape for layer 32 'res5a_branch2a' 2 512 40 40 (1638400)
I0815 19:04:19.791225 20809 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0815 19:04:19.791230 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.791241 20809 net.cpp:184] Created Layer res5a_branch2a/bn (33)
I0815 19:04:19.791246 20809 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0815 19:04:19.791251 20809 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0815 19:04:19.791942 20809 net.cpp:245] Setting up res5a_branch2a/bn
I0815 19:04:19.791950 20809 net.cpp:252] TEST Top shape for layer 33 'res5a_branch2a/bn' 2 512 40 40 (1638400)
I0815 19:04:19.791960 20809 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0815 19:04:19.791965 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.791970 20809 net.cpp:184] Created Layer res5a_branch2a/relu (34)
I0815 19:04:19.791973 20809 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0815 19:04:19.791977 20809 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0815 19:04:19.791985 20809 net.cpp:245] Setting up res5a_branch2a/relu
I0815 19:04:19.791998 20809 net.cpp:252] TEST Top shape for layer 34 'res5a_branch2a/relu' 2 512 40 40 (1638400)
I0815 19:04:19.792002 20809 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0815 19:04:19.792006 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.792016 20809 net.cpp:184] Created Layer res5a_branch2b (35)
I0815 19:04:19.792019 20809 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0815 19:04:19.792023 20809 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0815 19:04:19.805243 20809 net.cpp:245] Setting up res5a_branch2b
I0815 19:04:19.805258 20809 net.cpp:252] TEST Top shape for layer 35 'res5a_branch2b' 2 512 40 40 (1638400)
I0815 19:04:19.805269 20809 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0815 19:04:19.805275 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.805282 20809 net.cpp:184] Created Layer res5a_branch2b/bn (36)
I0815 19:04:19.805287 20809 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0815 19:04:19.805291 20809 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0815 19:04:19.805970 20809 net.cpp:245] Setting up res5a_branch2b/bn
I0815 19:04:19.805979 20809 net.cpp:252] TEST Top shape for layer 36 'res5a_branch2b/bn' 2 512 40 40 (1638400)
I0815 19:04:19.805987 20809 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0815 19:04:19.805991 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.805997 20809 net.cpp:184] Created Layer res5a_branch2b/relu (37)
I0815 19:04:19.806001 20809 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0815 19:04:19.806005 20809 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0815 19:04:19.806011 20809 net.cpp:245] Setting up res5a_branch2b/relu
I0815 19:04:19.806016 20809 net.cpp:252] TEST Top shape for layer 37 'res5a_branch2b/relu' 2 512 40 40 (1638400)
I0815 19:04:19.806020 20809 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I0815 19:04:19.806025 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.806037 20809 net.cpp:184] Created Layer out5a (38)
I0815 19:04:19.806041 20809 net.cpp:561] out5a <- res5a_branch2b
I0815 19:04:19.806046 20809 net.cpp:530] out5a -> out5a
I0815 19:04:19.809345 20809 net.cpp:245] Setting up out5a
I0815 19:04:19.809353 20809 net.cpp:252] TEST Top shape for layer 38 'out5a' 2 64 40 40 (204800)
I0815 19:04:19.809360 20809 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I0815 19:04:19.809363 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.809370 20809 net.cpp:184] Created Layer out5a/bn (39)
I0815 19:04:19.809375 20809 net.cpp:561] out5a/bn <- out5a
I0815 19:04:19.809378 20809 net.cpp:513] out5a/bn -> out5a (in-place)
I0815 19:04:19.810066 20809 net.cpp:245] Setting up out5a/bn
I0815 19:04:19.810075 20809 net.cpp:252] TEST Top shape for layer 39 'out5a/bn' 2 64 40 40 (204800)
I0815 19:04:19.810082 20809 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I0815 19:04:19.810086 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.810091 20809 net.cpp:184] Created Layer out5a/relu (40)
I0815 19:04:19.810094 20809 net.cpp:561] out5a/relu <- out5a
I0815 19:04:19.810098 20809 net.cpp:513] out5a/relu -> out5a (in-place)
I0815 19:04:19.810103 20809 net.cpp:245] Setting up out5a/relu
I0815 19:04:19.810108 20809 net.cpp:252] TEST Top shape for layer 40 'out5a/relu' 2 64 40 40 (204800)
I0815 19:04:19.810112 20809 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I0815 19:04:19.810117 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.810128 20809 net.cpp:184] Created Layer out5a_up2 (41)
I0815 19:04:19.810132 20809 net.cpp:561] out5a_up2 <- out5a
I0815 19:04:19.810143 20809 net.cpp:530] out5a_up2 -> out5a_up2
I0815 19:04:19.810441 20809 net.cpp:245] Setting up out5a_up2
I0815 19:04:19.810446 20809 net.cpp:252] TEST Top shape for layer 41 'out5a_up2' 2 64 80 80 (819200)
I0815 19:04:19.810452 20809 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I0815 19:04:19.810456 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.810466 20809 net.cpp:184] Created Layer out3a (42)
I0815 19:04:19.810468 20809 net.cpp:561] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0815 19:04:19.810473 20809 net.cpp:530] out3a -> out3a
I0815 19:04:19.814461 20809 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.97G, req 0G)
I0815 19:04:19.814471 20809 net.cpp:245] Setting up out3a
I0815 19:04:19.814477 20809 net.cpp:252] TEST Top shape for layer 42 'out3a' 2 64 80 80 (819200)
I0815 19:04:19.814484 20809 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I0815 19:04:19.814488 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.814496 20809 net.cpp:184] Created Layer out3a/bn (43)
I0815 19:04:19.814499 20809 net.cpp:561] out3a/bn <- out3a
I0815 19:04:19.814504 20809 net.cpp:513] out3a/bn -> out3a (in-place)
I0815 19:04:19.815438 20809 net.cpp:245] Setting up out3a/bn
I0815 19:04:19.815448 20809 net.cpp:252] TEST Top shape for layer 43 'out3a/bn' 2 64 80 80 (819200)
I0815 19:04:19.815457 20809 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I0815 19:04:19.815461 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.815465 20809 net.cpp:184] Created Layer out3a/relu (44)
I0815 19:04:19.815469 20809 net.cpp:561] out3a/relu <- out3a
I0815 19:04:19.815472 20809 net.cpp:513] out3a/relu -> out3a (in-place)
I0815 19:04:19.815477 20809 net.cpp:245] Setting up out3a/relu
I0815 19:04:19.815481 20809 net.cpp:252] TEST Top shape for layer 44 'out3a/relu' 2 64 80 80 (819200)
I0815 19:04:19.815486 20809 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I0815 19:04:19.815490 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.815495 20809 net.cpp:184] Created Layer out3_out5_combined (45)
I0815 19:04:19.815497 20809 net.cpp:561] out3_out5_combined <- out5a_up2
I0815 19:04:19.815500 20809 net.cpp:561] out3_out5_combined <- out3a
I0815 19:04:19.815503 20809 net.cpp:530] out3_out5_combined -> out3_out5_combined
I0815 19:04:19.816606 20809 net.cpp:245] Setting up out3_out5_combined
I0815 19:04:19.816617 20809 net.cpp:252] TEST Top shape for layer 45 'out3_out5_combined' 2 64 80 80 (819200)
I0815 19:04:19.816622 20809 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I0815 19:04:19.816627 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.816637 20809 net.cpp:184] Created Layer ctx_conv1 (46)
I0815 19:04:19.816640 20809 net.cpp:561] ctx_conv1 <- out3_out5_combined
I0815 19:04:19.816645 20809 net.cpp:530] ctx_conv1 -> ctx_conv1
I0815 19:04:19.820621 20809 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.96G, req 0G)
I0815 19:04:19.820636 20809 net.cpp:245] Setting up ctx_conv1
I0815 19:04:19.820642 20809 net.cpp:252] TEST Top shape for layer 46 'ctx_conv1' 2 64 80 80 (819200)
I0815 19:04:19.820648 20809 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I0815 19:04:19.820653 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.820659 20809 net.cpp:184] Created Layer ctx_conv1/bn (47)
I0815 19:04:19.820664 20809 net.cpp:561] ctx_conv1/bn <- ctx_conv1
I0815 19:04:19.820669 20809 net.cpp:513] ctx_conv1/bn -> ctx_conv1 (in-place)
I0815 19:04:19.821651 20809 net.cpp:245] Setting up ctx_conv1/bn
I0815 19:04:19.821661 20809 net.cpp:252] TEST Top shape for layer 47 'ctx_conv1/bn' 2 64 80 80 (819200)
I0815 19:04:19.821681 20809 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I0815 19:04:19.821684 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.821689 20809 net.cpp:184] Created Layer ctx_conv1/relu (48)
I0815 19:04:19.821693 20809 net.cpp:561] ctx_conv1/relu <- ctx_conv1
I0815 19:04:19.821698 20809 net.cpp:513] ctx_conv1/relu -> ctx_conv1 (in-place)
I0815 19:04:19.821707 20809 net.cpp:245] Setting up ctx_conv1/relu
I0815 19:04:19.821712 20809 net.cpp:252] TEST Top shape for layer 48 'ctx_conv1/relu' 2 64 80 80 (819200)
I0815 19:04:19.821715 20809 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I0815 19:04:19.821720 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.821735 20809 net.cpp:184] Created Layer ctx_conv2 (49)
I0815 19:04:19.821739 20809 net.cpp:561] ctx_conv2 <- ctx_conv1
I0815 19:04:19.821744 20809 net.cpp:530] ctx_conv2 -> ctx_conv2
I0815 19:04:19.823282 20809 net.cpp:245] Setting up ctx_conv2
I0815 19:04:19.823292 20809 net.cpp:252] TEST Top shape for layer 49 'ctx_conv2' 2 64 80 80 (819200)
I0815 19:04:19.823299 20809 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I0815 19:04:19.823303 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.823309 20809 net.cpp:184] Created Layer ctx_conv2/bn (50)
I0815 19:04:19.823313 20809 net.cpp:561] ctx_conv2/bn <- ctx_conv2
I0815 19:04:19.823318 20809 net.cpp:513] ctx_conv2/bn -> ctx_conv2 (in-place)
I0815 19:04:19.824270 20809 net.cpp:245] Setting up ctx_conv2/bn
I0815 19:04:19.824280 20809 net.cpp:252] TEST Top shape for layer 50 'ctx_conv2/bn' 2 64 80 80 (819200)
I0815 19:04:19.824291 20809 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I0815 19:04:19.824295 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.824301 20809 net.cpp:184] Created Layer ctx_conv2/relu (51)
I0815 19:04:19.824304 20809 net.cpp:561] ctx_conv2/relu <- ctx_conv2
I0815 19:04:19.824307 20809 net.cpp:513] ctx_conv2/relu -> ctx_conv2 (in-place)
I0815 19:04:19.824313 20809 net.cpp:245] Setting up ctx_conv2/relu
I0815 19:04:19.824317 20809 net.cpp:252] TEST Top shape for layer 51 'ctx_conv2/relu' 2 64 80 80 (819200)
I0815 19:04:19.824321 20809 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I0815 19:04:19.824326 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.824334 20809 net.cpp:184] Created Layer ctx_conv3 (52)
I0815 19:04:19.824338 20809 net.cpp:561] ctx_conv3 <- ctx_conv2
I0815 19:04:19.824342 20809 net.cpp:530] ctx_conv3 -> ctx_conv3
I0815 19:04:19.825978 20809 net.cpp:245] Setting up ctx_conv3
I0815 19:04:19.825987 20809 net.cpp:252] TEST Top shape for layer 52 'ctx_conv3' 2 64 80 80 (819200)
I0815 19:04:19.825992 20809 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I0815 19:04:19.825995 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.825999 20809 net.cpp:184] Created Layer ctx_conv3/bn (53)
I0815 19:04:19.826002 20809 net.cpp:561] ctx_conv3/bn <- ctx_conv3
I0815 19:04:19.826004 20809 net.cpp:513] ctx_conv3/bn -> ctx_conv3 (in-place)
I0815 19:04:19.826690 20809 net.cpp:245] Setting up ctx_conv3/bn
I0815 19:04:19.826697 20809 net.cpp:252] TEST Top shape for layer 53 'ctx_conv3/bn' 2 64 80 80 (819200)
I0815 19:04:19.826704 20809 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I0815 19:04:19.826705 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.826709 20809 net.cpp:184] Created Layer ctx_conv3/relu (54)
I0815 19:04:19.826711 20809 net.cpp:561] ctx_conv3/relu <- ctx_conv3
I0815 19:04:19.826714 20809 net.cpp:513] ctx_conv3/relu -> ctx_conv3 (in-place)
I0815 19:04:19.826717 20809 net.cpp:245] Setting up ctx_conv3/relu
I0815 19:04:19.826727 20809 net.cpp:252] TEST Top shape for layer 54 'ctx_conv3/relu' 2 64 80 80 (819200)
I0815 19:04:19.826730 20809 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I0815 19:04:19.826732 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.826737 20809 net.cpp:184] Created Layer ctx_conv4 (55)
I0815 19:04:19.826740 20809 net.cpp:561] ctx_conv4 <- ctx_conv3
I0815 19:04:19.826742 20809 net.cpp:530] ctx_conv4 -> ctx_conv4
I0815 19:04:19.827847 20809 net.cpp:245] Setting up ctx_conv4
I0815 19:04:19.827853 20809 net.cpp:252] TEST Top shape for layer 55 'ctx_conv4' 2 64 80 80 (819200)
I0815 19:04:19.827858 20809 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I0815 19:04:19.827862 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.827864 20809 net.cpp:184] Created Layer ctx_conv4/bn (56)
I0815 19:04:19.827867 20809 net.cpp:561] ctx_conv4/bn <- ctx_conv4
I0815 19:04:19.827869 20809 net.cpp:513] ctx_conv4/bn -> ctx_conv4 (in-place)
I0815 19:04:19.828569 20809 net.cpp:245] Setting up ctx_conv4/bn
I0815 19:04:19.828575 20809 net.cpp:252] TEST Top shape for layer 56 'ctx_conv4/bn' 2 64 80 80 (819200)
I0815 19:04:19.828582 20809 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I0815 19:04:19.828584 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.828588 20809 net.cpp:184] Created Layer ctx_conv4/relu (57)
I0815 19:04:19.828589 20809 net.cpp:561] ctx_conv4/relu <- ctx_conv4
I0815 19:04:19.828593 20809 net.cpp:513] ctx_conv4/relu -> ctx_conv4 (in-place)
I0815 19:04:19.828595 20809 net.cpp:245] Setting up ctx_conv4/relu
I0815 19:04:19.828598 20809 net.cpp:252] TEST Top shape for layer 57 'ctx_conv4/relu' 2 64 80 80 (819200)
I0815 19:04:19.828601 20809 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I0815 19:04:19.828603 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.828608 20809 net.cpp:184] Created Layer ctx_final (58)
I0815 19:04:19.828611 20809 net.cpp:561] ctx_final <- ctx_conv4
I0815 19:04:19.828613 20809 net.cpp:530] ctx_final -> ctx_final
I0815 19:04:19.833222 20809 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.96G, req 0G)
I0815 19:04:19.833235 20809 net.cpp:245] Setting up ctx_final
I0815 19:04:19.833240 20809 net.cpp:252] TEST Top shape for layer 58 'ctx_final' 2 8 80 80 (102400)
I0815 19:04:19.833248 20809 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I0815 19:04:19.833252 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.833258 20809 net.cpp:184] Created Layer ctx_final/relu (59)
I0815 19:04:19.833263 20809 net.cpp:561] ctx_final/relu <- ctx_final
I0815 19:04:19.833267 20809 net.cpp:513] ctx_final/relu -> ctx_final (in-place)
I0815 19:04:19.833282 20809 net.cpp:245] Setting up ctx_final/relu
I0815 19:04:19.833287 20809 net.cpp:252] TEST Top shape for layer 59 'ctx_final/relu' 2 8 80 80 (102400)
I0815 19:04:19.833292 20809 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I0815 19:04:19.833297 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.833307 20809 net.cpp:184] Created Layer out_deconv_final_up2 (60)
I0815 19:04:19.833312 20809 net.cpp:561] out_deconv_final_up2 <- ctx_final
I0815 19:04:19.833315 20809 net.cpp:530] out_deconv_final_up2 -> out_deconv_final_up2
I0815 19:04:19.833632 20809 net.cpp:245] Setting up out_deconv_final_up2
I0815 19:04:19.833640 20809 net.cpp:252] TEST Top shape for layer 60 'out_deconv_final_up2' 2 8 160 160 (409600)
I0815 19:04:19.833645 20809 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I0815 19:04:19.833650 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.833664 20809 net.cpp:184] Created Layer out_deconv_final_up4 (61)
I0815 19:04:19.833668 20809 net.cpp:561] out_deconv_final_up4 <- out_deconv_final_up2
I0815 19:04:19.833673 20809 net.cpp:530] out_deconv_final_up4 -> out_deconv_final_up4
I0815 19:04:19.833966 20809 net.cpp:245] Setting up out_deconv_final_up4
I0815 19:04:19.833972 20809 net.cpp:252] TEST Top shape for layer 61 'out_deconv_final_up4' 2 8 320 320 (1638400)
I0815 19:04:19.833978 20809 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I0815 19:04:19.833982 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.833989 20809 net.cpp:184] Created Layer out_deconv_final_up8 (62)
I0815 19:04:19.833992 20809 net.cpp:561] out_deconv_final_up8 <- out_deconv_final_up4
I0815 19:04:19.833997 20809 net.cpp:530] out_deconv_final_up8 -> out_deconv_final_up8
I0815 19:04:19.834290 20809 net.cpp:245] Setting up out_deconv_final_up8
I0815 19:04:19.834296 20809 net.cpp:252] TEST Top shape for layer 62 'out_deconv_final_up8' 2 8 640 640 (6553600)
I0815 19:04:19.834302 20809 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8_out_deconv_final_up8_0_split' of type 'Split'
I0815 19:04:19.834307 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.834312 20809 net.cpp:184] Created Layer out_deconv_final_up8_out_deconv_final_up8_0_split (63)
I0815 19:04:19.834316 20809 net.cpp:561] out_deconv_final_up8_out_deconv_final_up8_0_split <- out_deconv_final_up8
I0815 19:04:19.834321 20809 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0815 19:04:19.834326 20809 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0815 19:04:19.834331 20809 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0815 19:04:19.834403 20809 net.cpp:245] Setting up out_deconv_final_up8_out_deconv_final_up8_0_split
I0815 19:04:19.834408 20809 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 8 640 640 (6553600)
I0815 19:04:19.834414 20809 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 8 640 640 (6553600)
I0815 19:04:19.834419 20809 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 8 640 640 (6553600)
I0815 19:04:19.834424 20809 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0815 19:04:19.834427 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.834435 20809 net.cpp:184] Created Layer loss (64)
I0815 19:04:19.834439 20809 net.cpp:561] loss <- out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0815 19:04:19.834444 20809 net.cpp:561] loss <- label_data_1_split_0
I0815 19:04:19.834447 20809 net.cpp:530] loss -> loss
I0815 19:04:19.835253 20809 net.cpp:245] Setting up loss
I0815 19:04:19.835263 20809 net.cpp:252] TEST Top shape for layer 64 'loss' (1)
I0815 19:04:19.835266 20809 net.cpp:256]     with loss weight 1
I0815 19:04:19.835273 20809 layer_factory.hpp:136] Creating layer 'accuracy/top1' of type 'Accuracy'
I0815 19:04:19.835278 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.835286 20809 net.cpp:184] Created Layer accuracy/top1 (65)
I0815 19:04:19.835290 20809 net.cpp:561] accuracy/top1 <- out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0815 19:04:19.835295 20809 net.cpp:561] accuracy/top1 <- label_data_1_split_1
I0815 19:04:19.835300 20809 net.cpp:530] accuracy/top1 -> accuracy/top1
I0815 19:04:19.835307 20809 net.cpp:245] Setting up accuracy/top1
I0815 19:04:19.835311 20809 net.cpp:252] TEST Top shape for layer 65 'accuracy/top1' (1)
I0815 19:04:19.835316 20809 layer_factory.hpp:136] Creating layer 'accuracy/top5' of type 'Accuracy'
I0815 19:04:19.835325 20809 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0815 19:04:19.835330 20809 net.cpp:184] Created Layer accuracy/top5 (66)
I0815 19:04:19.835335 20809 net.cpp:561] accuracy/top5 <- out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0815 19:04:19.835340 20809 net.cpp:561] accuracy/top5 <- label_data_1_split_2
I0815 19:04:19.835345 20809 net.cpp:530] accuracy/top5 -> accuracy/top5
I0815 19:04:19.835351 20809 net.cpp:245] Setting up accuracy/top5
I0815 19:04:19.835355 20809 net.cpp:252] TEST Top shape for layer 66 'accuracy/top5' (1)
I0815 19:04:19.835360 20809 net.cpp:325] accuracy/top5 does not need backward computation.
I0815 19:04:19.835364 20809 net.cpp:325] accuracy/top1 does not need backward computation.
I0815 19:04:19.835368 20809 net.cpp:323] loss needs backward computation.
I0815 19:04:19.835372 20809 net.cpp:323] out_deconv_final_up8_out_deconv_final_up8_0_split needs backward computation.
I0815 19:04:19.835376 20809 net.cpp:323] out_deconv_final_up8 needs backward computation.
I0815 19:04:19.835381 20809 net.cpp:323] out_deconv_final_up4 needs backward computation.
I0815 19:04:19.835384 20809 net.cpp:323] out_deconv_final_up2 needs backward computation.
I0815 19:04:19.835388 20809 net.cpp:323] ctx_final/relu needs backward computation.
I0815 19:04:19.835392 20809 net.cpp:323] ctx_final needs backward computation.
I0815 19:04:19.835397 20809 net.cpp:323] ctx_conv4/relu needs backward computation.
I0815 19:04:19.835399 20809 net.cpp:323] ctx_conv4/bn needs backward computation.
I0815 19:04:19.835403 20809 net.cpp:323] ctx_conv4 needs backward computation.
I0815 19:04:19.835407 20809 net.cpp:323] ctx_conv3/relu needs backward computation.
I0815 19:04:19.835410 20809 net.cpp:323] ctx_conv3/bn needs backward computation.
I0815 19:04:19.835414 20809 net.cpp:323] ctx_conv3 needs backward computation.
I0815 19:04:19.835417 20809 net.cpp:323] ctx_conv2/relu needs backward computation.
I0815 19:04:19.835422 20809 net.cpp:323] ctx_conv2/bn needs backward computation.
I0815 19:04:19.835425 20809 net.cpp:323] ctx_conv2 needs backward computation.
I0815 19:04:19.835428 20809 net.cpp:323] ctx_conv1/relu needs backward computation.
I0815 19:04:19.835433 20809 net.cpp:323] ctx_conv1/bn needs backward computation.
I0815 19:04:19.835436 20809 net.cpp:323] ctx_conv1 needs backward computation.
I0815 19:04:19.835440 20809 net.cpp:323] out3_out5_combined needs backward computation.
I0815 19:04:19.835444 20809 net.cpp:323] out3a/relu needs backward computation.
I0815 19:04:19.835448 20809 net.cpp:323] out3a/bn needs backward computation.
I0815 19:04:19.835451 20809 net.cpp:323] out3a needs backward computation.
I0815 19:04:19.835455 20809 net.cpp:323] out5a_up2 needs backward computation.
I0815 19:04:19.835460 20809 net.cpp:323] out5a/relu needs backward computation.
I0815 19:04:19.835464 20809 net.cpp:323] out5a/bn needs backward computation.
I0815 19:04:19.835467 20809 net.cpp:323] out5a needs backward computation.
I0815 19:04:19.835471 20809 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0815 19:04:19.835475 20809 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0815 19:04:19.835479 20809 net.cpp:323] res5a_branch2b needs backward computation.
I0815 19:04:19.835484 20809 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0815 19:04:19.835487 20809 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0815 19:04:19.835491 20809 net.cpp:323] res5a_branch2a needs backward computation.
I0815 19:04:19.835494 20809 net.cpp:323] pool4 needs backward computation.
I0815 19:04:19.835500 20809 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0815 19:04:19.835502 20809 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0815 19:04:19.835506 20809 net.cpp:323] res4a_branch2b needs backward computation.
I0815 19:04:19.835510 20809 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0815 19:04:19.835515 20809 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0815 19:04:19.835518 20809 net.cpp:323] res4a_branch2a needs backward computation.
I0815 19:04:19.835526 20809 net.cpp:323] pool3 needs backward computation.
I0815 19:04:19.835530 20809 net.cpp:323] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I0815 19:04:19.835536 20809 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0815 19:04:19.835539 20809 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0815 19:04:19.835542 20809 net.cpp:323] res3a_branch2b needs backward computation.
I0815 19:04:19.835547 20809 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0815 19:04:19.835551 20809 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0815 19:04:19.835554 20809 net.cpp:323] res3a_branch2a needs backward computation.
I0815 19:04:19.835558 20809 net.cpp:323] pool2 needs backward computation.
I0815 19:04:19.835562 20809 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0815 19:04:19.835566 20809 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0815 19:04:19.835571 20809 net.cpp:323] res2a_branch2b needs backward computation.
I0815 19:04:19.835574 20809 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0815 19:04:19.835578 20809 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0815 19:04:19.835582 20809 net.cpp:323] res2a_branch2a needs backward computation.
I0815 19:04:19.835587 20809 net.cpp:323] pool1 needs backward computation.
I0815 19:04:19.835590 20809 net.cpp:323] conv1b/relu needs backward computation.
I0815 19:04:19.835594 20809 net.cpp:323] conv1b/bn needs backward computation.
I0815 19:04:19.835598 20809 net.cpp:323] conv1b needs backward computation.
I0815 19:04:19.835602 20809 net.cpp:323] conv1a/relu needs backward computation.
I0815 19:04:19.835605 20809 net.cpp:323] conv1a/bn needs backward computation.
I0815 19:04:19.835609 20809 net.cpp:323] conv1a needs backward computation.
I0815 19:04:19.835613 20809 net.cpp:325] data/bias does not need backward computation.
I0815 19:04:19.835618 20809 net.cpp:325] label_data_1_split does not need backward computation.
I0815 19:04:19.835623 20809 net.cpp:325] data does not need backward computation.
I0815 19:04:19.835626 20809 net.cpp:367] This network produces output accuracy/top1
I0815 19:04:19.835630 20809 net.cpp:367] This network produces output accuracy/top5
I0815 19:04:19.835633 20809 net.cpp:367] This network produces output loss
I0815 19:04:19.835677 20809 net.cpp:389] Top memory (TEST) required for data: 318668800 diff: 8
I0815 19:04:19.835681 20809 net.cpp:392] Bottom memory (TEST) required for data: 318668800 diff: 318668800
I0815 19:04:19.835685 20809 net.cpp:395] Shared (in-place) memory (TEST) by data: 210124800 diff: 210124800
I0815 19:04:19.835688 20809 net.cpp:398] Parameters memory (TEST) required for data: 2692608 diff: 2692608
I0815 19:04:19.835691 20809 net.cpp:401] Parameters shared memory (TEST) by data: 0 diff: 0
I0815 19:04:19.835695 20809 net.cpp:407] Network initialization done.
I0815 19:04:19.835768 20809 solver.cpp:56] Solver scaffolding done.
I0815 19:04:19.845239 20809 caffe.cpp:137] Finetuning from training/imagenet_jacintonet11v2_iter_320000.caffemodel
I0815 19:04:20.107897 20809 net.cpp:1095] Copying source layer data Type:Data #blobs=0
I0815 19:04:20.107915 20809 net.cpp:1095] Copying source layer data/bias Type:Bias #blobs=1
I0815 19:04:20.107923 20809 net.cpp:1095] Copying source layer conv1a Type:Convolution #blobs=2
I0815 19:04:20.107960 20809 net.cpp:1095] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0815 19:04:20.108508 20809 net.cpp:1108] BN legacy DIGITS format detected ... 
I0815 19:04:20.108516 20809 net.cpp:1114] BN Transforming to new format completed.
I0815 19:04:20.108518 20809 net.cpp:1095] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0815 19:04:20.108520 20809 net.cpp:1095] Copying source layer conv1b Type:Convolution #blobs=2
I0815 19:04:20.108559 20809 net.cpp:1095] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0815 19:04:20.108923 20809 net.cpp:1108] BN legacy DIGITS format detected ... 
I0815 19:04:20.108929 20809 net.cpp:1114] BN Transforming to new format completed.
I0815 19:04:20.108939 20809 net.cpp:1095] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0815 19:04:20.108942 20809 net.cpp:1095] Copying source layer pool1 Type:Pooling #blobs=0
I0815 19:04:20.108943 20809 net.cpp:1095] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0815 19:04:20.109194 20809 net.cpp:1095] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0815 19:04:20.109568 20809 net.cpp:1108] BN legacy DIGITS format detected ... 
I0815 19:04:20.109575 20809 net.cpp:1114] BN Transforming to new format completed.
I0815 19:04:20.109576 20809 net.cpp:1095] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0815 19:04:20.109578 20809 net.cpp:1095] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0815 19:04:20.109706 20809 net.cpp:1095] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0815 19:04:20.110069 20809 net.cpp:1108] BN legacy DIGITS format detected ... 
I0815 19:04:20.110074 20809 net.cpp:1114] BN Transforming to new format completed.
I0815 19:04:20.110075 20809 net.cpp:1095] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0815 19:04:20.110077 20809 net.cpp:1095] Copying source layer pool2 Type:Pooling #blobs=0
I0815 19:04:20.110080 20809 net.cpp:1095] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0815 19:04:20.111053 20809 net.cpp:1095] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0815 19:04:20.111390 20809 net.cpp:1108] BN legacy DIGITS format detected ... 
I0815 19:04:20.111395 20809 net.cpp:1114] BN Transforming to new format completed.
I0815 19:04:20.111398 20809 net.cpp:1095] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0815 19:04:20.111400 20809 net.cpp:1095] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0815 19:04:20.111891 20809 net.cpp:1095] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0815 19:04:20.112227 20809 net.cpp:1108] BN legacy DIGITS format detected ... 
I0815 19:04:20.112233 20809 net.cpp:1114] BN Transforming to new format completed.
I0815 19:04:20.112236 20809 net.cpp:1095] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0815 19:04:20.112239 20809 net.cpp:1095] Copying source layer pool3 Type:Pooling #blobs=0
I0815 19:04:20.112243 20809 net.cpp:1095] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0815 19:04:20.116119 20809 net.cpp:1095] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0815 19:04:20.116466 20809 net.cpp:1108] BN legacy DIGITS format detected ... 
I0815 19:04:20.116472 20809 net.cpp:1114] BN Transforming to new format completed.
I0815 19:04:20.116473 20809 net.cpp:1095] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0815 19:04:20.116475 20809 net.cpp:1095] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0815 19:04:20.118419 20809 net.cpp:1095] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0815 19:04:20.118758 20809 net.cpp:1108] BN legacy DIGITS format detected ... 
I0815 19:04:20.118763 20809 net.cpp:1114] BN Transforming to new format completed.
I0815 19:04:20.118765 20809 net.cpp:1095] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0815 19:04:20.118768 20809 net.cpp:1095] Copying source layer pool4 Type:Pooling #blobs=0
I0815 19:04:20.118772 20809 net.cpp:1095] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0815 19:04:20.134233 20809 net.cpp:1095] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0815 19:04:20.134603 20809 net.cpp:1108] BN legacy DIGITS format detected ... 
I0815 19:04:20.134608 20809 net.cpp:1114] BN Transforming to new format completed.
I0815 19:04:20.134610 20809 net.cpp:1095] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0815 19:04:20.134613 20809 net.cpp:1095] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0815 19:04:20.142344 20809 net.cpp:1095] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0815 19:04:20.142698 20809 net.cpp:1108] BN legacy DIGITS format detected ... 
I0815 19:04:20.142709 20809 net.cpp:1114] BN Transforming to new format completed.
I0815 19:04:20.142712 20809 net.cpp:1095] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0815 19:04:20.142714 20809 net.cpp:1079] Ignoring source layer pool5
I0815 19:04:20.142716 20809 net.cpp:1079] Ignoring source layer fc1000
I0815 19:04:20.142719 20809 net.cpp:1095] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0815 19:04:20.151964 20809 net.cpp:1095] Copying source layer data Type:Data #blobs=0
I0815 19:04:20.151983 20809 net.cpp:1095] Copying source layer data/bias Type:Bias #blobs=1
I0815 19:04:20.151988 20809 net.cpp:1095] Copying source layer conv1a Type:Convolution #blobs=2
I0815 19:04:20.152024 20809 net.cpp:1095] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0815 19:04:20.152531 20809 net.cpp:1108] BN legacy DIGITS format detected ... 
I0815 19:04:20.152539 20809 net.cpp:1114] BN Transforming to new format completed.
I0815 19:04:20.152540 20809 net.cpp:1095] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0815 19:04:20.152542 20809 net.cpp:1095] Copying source layer conv1b Type:Convolution #blobs=2
I0815 19:04:20.152580 20809 net.cpp:1095] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0815 19:04:20.152945 20809 net.cpp:1108] BN legacy DIGITS format detected ... 
I0815 19:04:20.152951 20809 net.cpp:1114] BN Transforming to new format completed.
I0815 19:04:20.152953 20809 net.cpp:1095] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0815 19:04:20.152958 20809 net.cpp:1095] Copying source layer pool1 Type:Pooling #blobs=0
I0815 19:04:20.152962 20809 net.cpp:1095] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0815 19:04:20.153211 20809 net.cpp:1095] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0815 19:04:20.153587 20809 net.cpp:1108] BN legacy DIGITS format detected ... 
I0815 19:04:20.153594 20809 net.cpp:1114] BN Transforming to new format completed.
I0815 19:04:20.153595 20809 net.cpp:1095] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0815 19:04:20.153597 20809 net.cpp:1095] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0815 19:04:20.153725 20809 net.cpp:1095] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0815 19:04:20.154091 20809 net.cpp:1108] BN legacy DIGITS format detected ... 
I0815 19:04:20.154096 20809 net.cpp:1114] BN Transforming to new format completed.
I0815 19:04:20.154099 20809 net.cpp:1095] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0815 19:04:20.154101 20809 net.cpp:1095] Copying source layer pool2 Type:Pooling #blobs=0
I0815 19:04:20.154103 20809 net.cpp:1095] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0815 19:04:20.155076 20809 net.cpp:1095] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0815 19:04:20.155422 20809 net.cpp:1108] BN legacy DIGITS format detected ... 
I0815 19:04:20.155427 20809 net.cpp:1114] BN Transforming to new format completed.
I0815 19:04:20.155429 20809 net.cpp:1095] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0815 19:04:20.155431 20809 net.cpp:1095] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0815 19:04:20.155925 20809 net.cpp:1095] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0815 19:04:20.156266 20809 net.cpp:1108] BN legacy DIGITS format detected ... 
I0815 19:04:20.156271 20809 net.cpp:1114] BN Transforming to new format completed.
I0815 19:04:20.156273 20809 net.cpp:1095] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0815 19:04:20.156275 20809 net.cpp:1095] Copying source layer pool3 Type:Pooling #blobs=0
I0815 19:04:20.156277 20809 net.cpp:1095] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0815 19:04:20.160150 20809 net.cpp:1095] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0815 19:04:20.160502 20809 net.cpp:1108] BN legacy DIGITS format detected ... 
I0815 19:04:20.160507 20809 net.cpp:1114] BN Transforming to new format completed.
I0815 19:04:20.160509 20809 net.cpp:1095] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0815 19:04:20.160519 20809 net.cpp:1095] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0815 19:04:20.162474 20809 net.cpp:1095] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0815 19:04:20.162837 20809 net.cpp:1108] BN legacy DIGITS format detected ... 
I0815 19:04:20.162842 20809 net.cpp:1114] BN Transforming to new format completed.
I0815 19:04:20.162844 20809 net.cpp:1095] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0815 19:04:20.162847 20809 net.cpp:1095] Copying source layer pool4 Type:Pooling #blobs=0
I0815 19:04:20.162848 20809 net.cpp:1095] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0815 19:04:20.178328 20809 net.cpp:1095] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0815 19:04:20.178694 20809 net.cpp:1108] BN legacy DIGITS format detected ... 
I0815 19:04:20.178699 20809 net.cpp:1114] BN Transforming to new format completed.
I0815 19:04:20.178701 20809 net.cpp:1095] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0815 19:04:20.178704 20809 net.cpp:1095] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0815 19:04:20.186466 20809 net.cpp:1095] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0815 19:04:20.186894 20809 net.cpp:1108] BN legacy DIGITS format detected ... 
I0815 19:04:20.186900 20809 net.cpp:1114] BN Transforming to new format completed.
I0815 19:04:20.186903 20809 net.cpp:1095] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0815 19:04:20.186905 20809 net.cpp:1079] Ignoring source layer pool5
I0815 19:04:20.186908 20809 net.cpp:1079] Ignoring source layer fc1000
I0815 19:04:20.186909 20809 net.cpp:1095] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0815 19:04:20.187000 20809 parallel.cpp:106] [0 - 0] P2pSync adding callback
I0815 19:04:20.187005 20809 parallel.cpp:106] [1 - 1] P2pSync adding callback
I0815 19:04:20.187006 20809 parallel.cpp:106] [2 - 2] P2pSync adding callback
I0815 19:04:20.187010 20809 parallel.cpp:59] Starting Optimization
I0815 19:04:20.187013 20809 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0815 19:04:20.187047 20809 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 19:04:20.187072 20809 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 19:04:20.187785 20887 device_alternate.hpp:116] NVML initialized on thread 136059126073088
I0815 19:04:20.216938 20887 common.cpp:583] NVML succeeded to set CPU affinity on device 0
I0815 19:04:20.216989 20888 device_alternate.hpp:116] NVML initialized on thread 136059117680384
I0815 19:04:20.217978 20888 common.cpp:583] NVML succeeded to set CPU affinity on device 1
I0815 19:04:20.218019 20889 device_alternate.hpp:116] NVML initialized on thread 136059109287680
I0815 19:04:20.218729 20889 common.cpp:583] NVML succeeded to set CPU affinity on device 2
I0815 19:04:20.222512 20888 solver.cpp:42] Solver data type: FLOAT
W0815 19:04:20.223088 20888 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0815 19:04:20.223220 20888 net.cpp:104] Using FLOAT as default forward math type
I0815 19:04:20.223227 20888 net.cpp:110] Using FLOAT as default backward math type
I0815 19:04:20.223263 20888 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0815 19:04:20.223273 20888 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 19:04:20.226574 20889 solver.cpp:42] Solver data type: FLOAT
W0815 19:04:20.227164 20889 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0815 19:04:20.227274 20890 db_lmdb.cpp:24] Opened lmdb data/train-image-lmdb
I0815 19:04:20.227398 20889 net.cpp:104] Using FLOAT as default forward math type
I0815 19:04:20.227404 20889 net.cpp:110] Using FLOAT as default backward math type
I0815 19:04:20.227450 20889 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0815 19:04:20.227473 20889 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 19:04:20.230345 20891 db_lmdb.cpp:24] Opened lmdb data/train-image-lmdb
I0815 19:04:20.230478 20888 data_layer.cpp:185] [1] ReshapePrefetch 6, 3, 640, 640
I0815 19:04:20.230572 20888 data_layer.cpp:209] [1] Output data size: 6, 3, 640, 640
I0815 19:04:20.230587 20888 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 19:04:20.230648 20888 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0815 19:04:20.230664 20888 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 19:04:20.231775 20892 data_layer.cpp:97] [1] Parser threads: 1
I0815 19:04:20.231808 20892 data_layer.cpp:99] [1] Transformer threads: 1
I0815 19:04:20.237700 20893 db_lmdb.cpp:24] Opened lmdb data/train-label-lmdb
I0815 19:04:20.241207 20888 data_layer.cpp:185] [1] ReshapePrefetch 6, 1, 640, 640
I0815 19:04:20.241683 20889 data_layer.cpp:185] [2] ReshapePrefetch 6, 3, 640, 640
I0815 19:04:20.241838 20889 data_layer.cpp:209] [2] Output data size: 6, 3, 640, 640
I0815 19:04:20.241852 20889 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 19:04:20.241943 20889 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0815 19:04:20.241962 20889 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 19:04:20.242064 20888 data_layer.cpp:209] [1] Output data size: 6, 1, 640, 640
I0815 19:04:20.242074 20888 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 19:04:20.243764 20894 data_layer.cpp:97] [2] Parser threads: 1
I0815 19:04:20.243855 20894 data_layer.cpp:99] [2] Transformer threads: 1
I0815 19:04:20.249794 20895 db_lmdb.cpp:24] Opened lmdb data/train-label-lmdb
I0815 19:04:20.251224 20896 data_layer.cpp:97] [1] Parser threads: 1
I0815 19:04:20.251327 20896 data_layer.cpp:99] [1] Transformer threads: 1
I0815 19:04:20.259923 20889 data_layer.cpp:185] [2] ReshapePrefetch 6, 1, 640, 640
I0815 19:04:20.260023 20889 data_layer.cpp:209] [2] Output data size: 6, 1, 640, 640
I0815 19:04:20.260035 20889 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 19:04:20.268067 20897 data_layer.cpp:97] [2] Parser threads: 1
I0815 19:04:20.268101 20897 data_layer.cpp:99] [2] Transformer threads: 1
I0815 19:04:20.790302 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 7.99G, req 0G)
I0815 19:04:20.836558 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 0  (limit 7.99G, req 0G)
I0815 19:04:20.840016 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.82G, req 0G)
I0815 19:04:20.882266 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.7G, req 0G)
I0815 19:04:20.887094 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.83G, req 0G)
I0815 19:04:20.904455 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.62G, req 0G)
I0815 19:04:20.925640 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 7.56G, req 0G)
I0815 19:04:20.929401 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.7G, req 0G)
I0815 19:04:20.938050 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.51G, req 0G)
I0815 19:04:20.952937 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.62G, req 0G)
I0815 19:04:20.965083 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.48G, req 0G)
I0815 19:04:20.977811 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.46G, req 0G)
I0815 19:04:20.978339 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 7.56G, req 0G)
I0815 19:04:20.991750 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.52G, req 0G)
I0815 19:04:21.018362 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.48G, req 0G)
I0815 19:04:21.028920 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.46G, req 0G)
I0815 19:04:21.039433 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 7.39G, req 0G)
I0815 19:04:21.055289 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 7.34G, req 0G)
I0815 19:04:21.077791 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 7.31G, req 0G)
I0815 19:04:21.081231 20888 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/initial/test.prototxt
W0815 19:04:21.081298 20888 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0815 19:04:21.081432 20888 net.cpp:104] Using FLOAT as default forward math type
I0815 19:04:21.081439 20888 net.cpp:110] Using FLOAT as default backward math type
I0815 19:04:21.081467 20888 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0815 19:04:21.081473 20888 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 19:04:21.082231 20933 db_lmdb.cpp:24] Opened lmdb data/val-image-lmdb
I0815 19:04:21.083804 20888 data_layer.cpp:185] (1) ReshapePrefetch 2, 3, 640, 640
I0815 19:04:21.083900 20888 data_layer.cpp:209] (1) Output data size: 2, 3, 640, 640
I0815 19:04:21.083907 20888 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 19:04:21.083950 20888 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0815 19:04:21.083961 20888 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 19:04:21.084702 20934 data_layer.cpp:97] (1) Parser threads: 1
I0815 19:04:21.084715 20934 data_layer.cpp:99] (1) Transformer threads: 1
I0815 19:04:21.087347 20935 db_lmdb.cpp:24] Opened lmdb data/val-label-lmdb
I0815 19:04:21.088629 20888 data_layer.cpp:185] (1) ReshapePrefetch 2, 1, 640, 640
I0815 19:04:21.088796 20888 data_layer.cpp:209] (1) Output data size: 2, 1, 640, 640
I0815 19:04:21.088806 20888 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0815 19:04:21.089855 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 7.39G, req 0G)
I0815 19:04:21.090257 20936 data_layer.cpp:97] (1) Parser threads: 1
I0815 19:04:21.090266 20936 data_layer.cpp:99] (1) Transformer threads: 1
I0815 19:04:21.100538 20888 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.18G, req 0G)
I0815 19:04:21.112747 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 7.34G, req 0G)
I0815 19:04:21.116387 20888 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.15G, req 0G)
I0815 19:04:21.127382 20888 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.13G, req 0G)
I0815 19:04:21.139648 20888 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.11G, req 0G)
I0815 19:04:21.139892 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 7.32G, req 0G)
I0815 19:04:21.144456 20889 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/initial/test.prototxt
W0815 19:04:21.144558 20889 parallel.cpp:272] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0815 19:04:21.144740 20889 net.cpp:104] Using FLOAT as default forward math type
I0815 19:04:21.144755 20889 net.cpp:110] Using FLOAT as default backward math type
I0815 19:04:21.144788 20889 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0815 19:04:21.144805 20889 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 19:04:21.145902 20937 db_lmdb.cpp:24] Opened lmdb data/val-image-lmdb
I0815 19:04:21.147521 20889 data_layer.cpp:185] (2) ReshapePrefetch 2, 3, 640, 640
I0815 19:04:21.147680 20889 data_layer.cpp:209] (2) Output data size: 2, 3, 640, 640
I0815 19:04:21.147688 20889 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 19:04:21.147758 20889 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0815 19:04:21.147789 20889 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 19:04:21.148803 20888 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.1G, req 0G)
I0815 19:04:21.149058 20938 data_layer.cpp:97] (2) Parser threads: 1
I0815 19:04:21.149085 20938 data_layer.cpp:99] (2) Transformer threads: 1
I0815 19:04:21.152585 20939 db_lmdb.cpp:24] Opened lmdb data/val-label-lmdb
I0815 19:04:21.156201 20889 data_layer.cpp:185] (2) ReshapePrefetch 2, 1, 640, 640
I0815 19:04:21.156272 20889 data_layer.cpp:209] (2) Output data size: 2, 1, 640, 640
I0815 19:04:21.156278 20889 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0815 19:04:21.157889 20940 data_layer.cpp:97] (2) Parser threads: 1
I0815 19:04:21.157901 20940 data_layer.cpp:99] (2) Transformer threads: 1
I0815 19:04:21.164400 20888 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.09G, req 0G)
I0815 19:04:21.168901 20889 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.18G, req 0G)
I0815 19:04:21.178737 20888 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.08G, req 0G)
I0815 19:04:21.185384 20889 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.15G, req 0G)
I0815 19:04:21.188057 20888 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.08G, req 0G)
I0815 19:04:21.196280 20889 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.13G, req 0G)
I0815 19:04:21.204373 20889 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.12G, req 0G)
I0815 19:04:21.211268 20889 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.1G, req 0G)
I0815 19:04:21.217571 20889 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.09G, req 0G)
I0815 19:04:21.230587 20889 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.09G, req 0G)
I0815 19:04:21.238672 20889 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.08G, req 0G)
I0815 19:04:21.248504 20888 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 7.07G, req 0G)
I0815 19:04:21.256043 20888 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 7.05G, req 0G)
I0815 19:04:21.267993 20888 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 7.05G, req 0G)
I0815 19:04:21.270311 20888 solver.cpp:56] Solver scaffolding done.
I0815 19:04:21.291203 20889 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 7.07G, req 0G)
I0815 19:04:21.297050 20889 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 7.06G, req 0G)
I0815 19:04:21.308682 20889 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 7.05G, req 0G)
I0815 19:04:21.311141 20889 solver.cpp:56] Solver scaffolding done.
I0815 19:04:21.373580 20887 parallel.cpp:161] [0 - 0] P2pSync adding callback
I0815 19:04:21.373605 20888 parallel.cpp:161] [1 - 1] P2pSync adding callback
I0815 19:04:21.373606 20889 parallel.cpp:161] [2 - 2] P2pSync adding callback
I0815 19:04:21.563798 20888 solver.cpp:438] Solving jsegnet21v2_train
I0815 19:04:21.563798 20887 solver.cpp:438] Solving jsegnet21v2_train
I0815 19:04:21.563798 20889 solver.cpp:438] Solving jsegnet21v2_train
I0815 19:04:21.563817 20888 solver.cpp:439] Learning Rate Policy: multistep
I0815 19:04:21.563824 20889 solver.cpp:439] Learning Rate Policy: multistep
I0815 19:04:21.563822 20887 solver.cpp:439] Learning Rate Policy: multistep
I0815 19:04:21.577165 20888 solver.cpp:227] Starting Optimization on GPU 1
I0815 19:04:21.577172 20889 solver.cpp:227] Starting Optimization on GPU 2
I0815 19:04:21.577167 20887 solver.cpp:227] Starting Optimization on GPU 0
I0815 19:04:21.577354 20887 solver.cpp:509] Iteration 0, Testing net (#0)
I0815 19:04:21.577358 20955 device_alternate.hpp:116] NVML initialized on thread 128058949420800
I0815 19:04:21.577379 20955 common.cpp:583] NVML succeeded to set CPU affinity on device 2
I0815 19:04:21.577394 20956 device_alternate.hpp:116] NVML initialized on thread 128058957813504
I0815 19:04:21.577409 20956 common.cpp:583] NVML succeeded to set CPU affinity on device 1
I0815 19:04:21.577419 20957 device_alternate.hpp:116] NVML initialized on thread 128058941028096
I0815 19:04:21.577425 20957 common.cpp:583] NVML succeeded to set CPU affinity on device 0
I0815 19:04:21.592221 20888 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 6.95G, req 0G)
I0815 19:04:21.593780 20889 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 6.96G, req 0G)
I0815 19:04:21.608796 20887 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.01G/1 1  (limit 6.88G, req 0G)
I0815 19:04:21.612206 20888 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 6.9G, req 0G)
I0815 19:04:21.613306 20889 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 6.9G, req 0G)
I0815 19:04:21.625893 20888 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 6.84G, req 0G)
I0815 19:04:21.626796 20887 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 6.82G, req 0G)
I0815 19:04:21.627864 20889 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 6.84G, req 0G)
I0815 19:04:21.682943 20888 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 6.81G, req 0G)
I0815 19:04:21.683262 20889 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 6.81G, req 0G)
I0815 19:04:21.686522 20887 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 6.75G, req 0G)
I0815 19:04:21.691627 20888 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 6.77G, req 0G)
I0815 19:04:21.692080 20889 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 6.78G, req 0G)
I0815 19:04:21.696161 20887 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 6.72G, req 0G)
I0815 19:04:21.698230 20888 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 6.76G, req 0G)
I0815 19:04:21.698768 20889 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 6.76G, req 0G)
I0815 19:04:21.703778 20887 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 6.69G, req 0G)
I0815 19:04:21.706073 20888 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.74G, req 0G)
I0815 19:04:21.707621 20889 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.74G, req 0G)
I0815 19:04:21.709588 20887 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 6.67G, req 0G)
I0815 19:04:21.710105 20888 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.73G, req 0G)
I0815 19:04:21.712693 20889 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.74G, req 0G)
I0815 19:04:21.717785 20887 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.65G, req 0G)
I0815 19:04:21.722050 20887 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.65G, req 0G)
I0815 19:04:21.739228 20888 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.58G, req 0G)
I0815 19:04:21.744658 20888 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.57G, req 0G)
I0815 19:04:21.745573 20889 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.58G, req 0G)
I0815 19:04:21.747895 20887 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.49G, req 0G)
I0815 19:04:21.752460 20889 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.57G, req 0G)
I0815 19:04:21.753594 20887 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.48G, req 0G)
I0815 19:04:21.769798 20888 cudnn_conv_layer.cpp:1010] (1) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.48G, req 0G)
I0815 19:04:21.775382 20889 cudnn_conv_layer.cpp:1010] (2) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.48G, req 0G)
I0815 19:04:21.777070 20887 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.39G, req 0G)
I0815 19:04:21.925346 20887 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.0484509
I0815 19:04:21.925366 20887 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.590535
I0815 19:04:21.925372 20887 solver.cpp:594]     Test net output #2: loss = 83.105 (* 1 = 83.105 loss)
I0815 19:04:21.925377 20887 solver.cpp:254] [MultiGPU] Initial Test completed
I0815 19:04:21.998857 20887 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 6.19G, req 0G)
I0815 19:04:22.002629 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 6.27G, req 0G)
I0815 19:04:22.003283 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 6.28G, req 0G)
I0815 19:04:22.048925 20887 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 1  (limit 6.03G, req 0G)
I0815 19:04:22.054224 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 6.11G, req 0G)
I0815 19:04:22.054594 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 6.12G, req 0G)
I0815 19:04:22.090960 20887 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 1 4 3  (limit 5.85G, req 0G)
I0815 19:04:22.098762 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 5.93G, req 0G)
I0815 19:04:22.099243 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 5.94G, req 0G)
I0815 19:04:22.112012 20887 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 5.77G, req 0G)
I0815 19:04:22.122488 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 5.86G, req 0G)
I0815 19:04:22.123044 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 5.85G, req 0G)
I0815 19:04:22.133066 20887 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 5.68G, req 0G)
I0815 19:04:22.143565 20887 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 5.64G, req 0G)
I0815 19:04:22.147939 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 5.76G, req 0G)
I0815 19:04:22.148274 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 5.77G, req 0G)
I0815 19:04:22.161126 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 5.72G, req 0G)
I0815 19:04:22.161907 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 5.72G, req 0G)
I0815 19:04:22.166162 20887 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 5.61G, req 0G)
I0815 19:04:22.173475 20887 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 5.59G, req 0G)
I0815 19:04:22.180930 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 5.69G, req 0G)
I0815 19:04:22.182677 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 5.7G, req 0G)
I0815 19:04:22.189007 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 5.67G, req 0G)
I0815 19:04:22.191591 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 5.68G, req 0G)
I0815 19:04:22.217931 20887 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 5.32G, req 0G)
I0815 19:04:22.232102 20887 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 5.3G, req 0G)
I0815 19:04:22.239286 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 5.4G, req 0G)
I0815 19:04:22.241324 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 5.41G, req 0G)
I0815 19:04:22.255272 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 5.38G, req 0G)
I0815 19:04:22.256893 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 5.39G, req 0G)
I0815 19:04:22.259907 20887 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 5.14G, req 0G)
I0815 19:04:22.289665 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 5.23G, req 0G)
I0815 19:04:22.290809 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 5.23G, req 0G)
I0815 19:04:22.488346 20887 solver.cpp:317] Iteration 0 (0.562928 s), loss = 2.16281
I0815 19:04:22.488373 20887 solver.cpp:334]     Train net output #0: loss = 2.16281 (* 1 = 2.16281 loss)
I0815 19:04:22.488407 20887 sgd_solver.cpp:136] Iteration 0, lr = 0.0001, m = 0.9
I0815 19:04:22.695040 20887 solver.cpp:317] Iteration 1 (0.206681 s), loss = 2.1386
I0815 19:04:22.695050 20888 blocking_queue.cpp:40] Data layer prefetch queue empty
I0815 19:04:22.695065 20887 solver.cpp:334]     Train net output #0: loss = 2.1386 (* 1 = 2.1386 loss)
I0815 19:04:22.862552 20887 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1a' with space 1.29G/1 1 0 3  (limit 2.99G, req 0G)
I0815 19:04:22.867040 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1a' with space 1.29G/1 1 0 3  (limit 3.08G, req 0G)
I0815 19:04:22.868839 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1a' with space 1.29G/1 1 0 3  (limit 3.08G, req 0G)
I0815 19:04:22.920820 20887 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'conv1b' with space 2.57G/2 6 4 3  (limit 1.7G, req 0G)
I0815 19:04:22.929148 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'conv1b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0G)
I0815 19:04:22.929606 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'conv1b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0G)
I0815 19:04:23.046139 20887 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 2.57G/1 6 4 3  (limit 1.7G, req 0G)
I0815 19:04:23.056579 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 2.57G/1 6 4 3  (limit 1.79G, req 0G)
I0815 19:04:23.056988 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 2.57G/1 6 4 3  (limit 1.79G, req 0G)
I0815 19:04:23.084736 20887 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 2.57G/2 6 4 3  (limit 1.7G, req 0G)
I0815 19:04:23.095661 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0G)
I0815 19:04:23.096238 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0G)
I0815 19:04:23.177140 20887 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 2.57G/1 6 4 5  (limit 1.7G, req 0.07G)
I0815 19:04:23.189581 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 2.57G/1 6 4 5  (limit 1.79G, req 0.07G)
I0815 19:04:23.190361 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 2.57G/1 6 4 5  (limit 1.79G, req 0.07G)
I0815 19:04:23.198058 20887 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 2.57G/2 6 4 3  (limit 1.7G, req 0.07G)
I0815 19:04:23.211036 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0.07G)
I0815 19:04:23.211714 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0.07G)
I0815 19:04:23.262054 20887 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 2.57G/1 6 4 5  (limit 1.7G, req 0.07G)
I0815 19:04:23.274524 20887 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 2.57G/2 6 4 3  (limit 1.7G, req 0.07G)
I0815 19:04:23.278427 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 2.57G/1 6 4 5  (limit 1.79G, req 0.07G)
I0815 19:04:23.278817 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 2.57G/1 6 4 5  (limit 1.79G, req 0.07G)
I0815 19:04:23.290850 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0.07G)
I0815 19:04:23.291402 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0.07G)
I0815 19:04:23.324597 20887 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'out3a' with space 2.57G/2 6 4 3  (limit 1.7G, req 0.07G)
I0815 19:04:23.344372 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'out3a' with space 2.57G/2 6 4 3  (limit 1.79G, req 0.07G)
I0815 19:04:23.344472 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'out3a' with space 2.57G/2 6 4 3  (limit 1.79G, req 0.07G)
I0815 19:04:23.376847 20887 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 2.57G/1 6 4 3  (limit 1.7G, req 0.07G)
I0815 19:04:23.398620 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 2.57G/1 6 4 3  (limit 1.79G, req 0.07G)
I0815 19:04:23.407742 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 2.57G/1 6 4 3  (limit 1.79G, req 0.07G)
I0815 19:04:23.408517 20887 cudnn_conv_layer.cpp:1010] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 2.57G/1 6 1 5  (limit 1.7G, req 0.07G)
I0815 19:04:23.426144 20889 cudnn_conv_layer.cpp:1010] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 2.57G/1 6 1 5  (limit 1.79G, req 0.07G)
I0815 19:04:23.434356 20888 cudnn_conv_layer.cpp:1010] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 2.57G/1 6 1 5  (limit 1.79G, req 0.07G)
I0815 19:04:23.559273 20887 solver.cpp:317] Iteration 2 (0.864201 s), loss = 2.13893
I0815 19:04:23.559298 20887 solver.cpp:334]     Train net output #0: loss = 2.13893 (* 1 = 2.13893 loss)
I0815 19:04:23.559908 20889 cudnn_conv_layer.cpp:292] [2] Layer 'conv1a' reallocating workspace: 2.57G -> 0.14G
I0815 19:04:23.559934 20888 cudnn_conv_layer.cpp:292] [1] Layer 'conv1a' reallocating workspace: 2.57G -> 0.14G
I0815 19:04:23.559922 20887 cudnn_conv_layer.cpp:292] [0] Layer 'conv1a' reallocating workspace: 2.57G -> 0.14G
I0815 19:05:29.841306 20887 solver.cpp:312] Iteration 100 (1.47857 iter/s, 66.2802s/98 iter), loss = 0.568677
I0815 19:05:29.841363 20887 solver.cpp:334]     Train net output #0: loss = 0.568677 (* 1 = 0.568677 loss)
I0815 19:05:29.841368 20887 sgd_solver.cpp:136] Iteration 100, lr = 0.0001, m = 0.9
I0815 19:06:10.397675 20893 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 19:06:28.492179 20887 solver.cpp:312] Iteration 200 (1.70505 iter/s, 58.6492s/100 iter), loss = 0.508882
I0815 19:06:28.492208 20887 solver.cpp:334]     Train net output #0: loss = 0.508882 (* 1 = 0.508882 loss)
I0815 19:06:28.492214 20887 sgd_solver.cpp:136] Iteration 200, lr = 0.0001, m = 0.9
I0815 19:07:19.524049 20887 solver.cpp:312] Iteration 300 (1.95961 iter/s, 51.0304s/100 iter), loss = 0.342625
I0815 19:07:19.524093 20887 solver.cpp:334]     Train net output #0: loss = 0.342625 (* 1 = 0.342625 loss)
I0815 19:07:19.524099 20887 sgd_solver.cpp:136] Iteration 300, lr = 0.0001, m = 0.9
I0815 19:08:09.968456 20887 solver.cpp:312] Iteration 400 (1.98244 iter/s, 50.443s/100 iter), loss = 0.366578
I0815 19:08:09.968531 20887 solver.cpp:334]     Train net output #0: loss = 0.366578 (* 1 = 0.366578 loss)
I0815 19:08:09.968536 20887 sgd_solver.cpp:136] Iteration 400, lr = 0.0001, m = 0.9
I0815 19:08:58.183528 20895 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 19:09:00.517235 20887 solver.cpp:312] Iteration 500 (1.97834 iter/s, 50.5474s/100 iter), loss = 0.238404
I0815 19:09:00.517261 20887 solver.cpp:334]     Train net output #0: loss = 0.238404 (* 1 = 0.238404 loss)
I0815 19:09:00.517266 20887 sgd_solver.cpp:136] Iteration 500, lr = 0.0001, m = 0.9
I0815 19:09:13.128109 20894 blocking_queue.cpp:40] Waiting for datum
I0815 19:09:20.462153 20887 solver.cpp:312] Iteration 600 (5.01395 iter/s, 19.9444s/100 iter), loss = 0.256399
I0815 19:09:20.462184 20887 solver.cpp:334]     Train net output #0: loss = 0.256399 (* 1 = 0.256399 loss)
I0815 19:09:20.462190 20887 sgd_solver.cpp:136] Iteration 600, lr = 0.0001, m = 0.9
I0815 19:09:39.838171 20887 solver.cpp:312] Iteration 700 (5.16116 iter/s, 19.3755s/100 iter), loss = 0.334159
I0815 19:09:39.838220 20887 solver.cpp:334]     Train net output #0: loss = 0.334158 (* 1 = 0.334158 loss)
I0815 19:09:39.838225 20887 sgd_solver.cpp:136] Iteration 700, lr = 0.0001, m = 0.9
I0815 19:09:58.889179 20887 solver.cpp:312] Iteration 800 (5.24921 iter/s, 19.0505s/100 iter), loss = 0.282593
I0815 19:09:58.889200 20887 solver.cpp:334]     Train net output #0: loss = 0.282593 (* 1 = 0.282593 loss)
I0815 19:09:58.889206 20887 sgd_solver.cpp:136] Iteration 800, lr = 0.0001, m = 0.9
I0815 19:10:03.421311 20841 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 19:10:18.161305 20887 solver.cpp:312] Iteration 900 (5.18899 iter/s, 19.2716s/100 iter), loss = 0.213218
I0815 19:10:18.161360 20887 solver.cpp:334]     Train net output #0: loss = 0.213218 (* 1 = 0.213218 loss)
I0815 19:10:18.161366 20887 sgd_solver.cpp:136] Iteration 900, lr = 0.0001, m = 0.9
I0815 19:10:35.373358 20891 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 19:10:37.614287 20887 solver.cpp:312] Iteration 1000 (5.14074 iter/s, 19.4524s/100 iter), loss = 0.235387
I0815 19:10:37.614311 20887 solver.cpp:334]     Train net output #0: loss = 0.235387 (* 1 = 0.235387 loss)
I0815 19:10:37.614315 20887 sgd_solver.cpp:136] Iteration 1000, lr = 0.0001, m = 0.9
I0815 19:10:56.937587 20887 solver.cpp:312] Iteration 1100 (5.17524 iter/s, 19.3228s/100 iter), loss = 0.191233
I0815 19:10:56.937638 20887 solver.cpp:334]     Train net output #0: loss = 0.191233 (* 1 = 0.191233 loss)
I0815 19:10:56.937644 20887 sgd_solver.cpp:136] Iteration 1100, lr = 0.0001, m = 0.9
I0815 19:11:16.374416 20887 solver.cpp:312] Iteration 1200 (5.14501 iter/s, 19.4363s/100 iter), loss = 0.24677
I0815 19:11:16.374439 20887 solver.cpp:334]     Train net output #0: loss = 0.24677 (* 1 = 0.24677 loss)
I0815 19:11:16.374444 20887 sgd_solver.cpp:136] Iteration 1200, lr = 0.0001, m = 0.9
I0815 19:11:36.008695 20887 solver.cpp:312] Iteration 1300 (5.09327 iter/s, 19.6337s/100 iter), loss = 0.330945
I0815 19:11:36.008750 20887 solver.cpp:334]     Train net output #0: loss = 0.330945 (* 1 = 0.330945 loss)
I0815 19:11:36.008757 20887 sgd_solver.cpp:136] Iteration 1300, lr = 0.0001, m = 0.9
I0815 19:11:39.849097 20895 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 19:11:55.392154 20887 solver.cpp:312] Iteration 1400 (5.15918 iter/s, 19.3829s/100 iter), loss = 0.326585
I0815 19:11:55.392177 20887 solver.cpp:334]     Train net output #0: loss = 0.326585 (* 1 = 0.326585 loss)
I0815 19:11:55.392182 20887 sgd_solver.cpp:136] Iteration 1400, lr = 0.0001, m = 0.9
I0815 19:12:14.789826 20887 solver.cpp:312] Iteration 1500 (5.1554 iter/s, 19.3971s/100 iter), loss = 0.390338
I0815 19:12:14.789883 20887 solver.cpp:334]     Train net output #0: loss = 0.390337 (* 1 = 0.390337 loss)
I0815 19:12:14.789891 20887 sgd_solver.cpp:136] Iteration 1500, lr = 0.0001, m = 0.9
I0815 19:12:34.164957 20887 solver.cpp:312] Iteration 1600 (5.1614 iter/s, 19.3746s/100 iter), loss = 0.197186
I0815 19:12:34.164985 20887 solver.cpp:334]     Train net output #0: loss = 0.197185 (* 1 = 0.197185 loss)
I0815 19:12:34.164993 20887 sgd_solver.cpp:136] Iteration 1600, lr = 0.0001, m = 0.9
I0815 19:12:43.715723 20893 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 19:12:53.682375 20887 solver.cpp:312] Iteration 1700 (5.12377 iter/s, 19.5169s/100 iter), loss = 0.276403
I0815 19:12:53.682440 20887 solver.cpp:334]     Train net output #0: loss = 0.276403 (* 1 = 0.276403 loss)
I0815 19:12:53.682449 20887 sgd_solver.cpp:136] Iteration 1700, lr = 0.0001, m = 0.9
I0815 19:13:13.275924 20887 solver.cpp:312] Iteration 1800 (5.10386 iter/s, 19.593s/100 iter), loss = 0.169287
I0815 19:13:13.275949 20887 solver.cpp:334]     Train net output #0: loss = 0.169287 (* 1 = 0.169287 loss)
I0815 19:13:13.275955 20887 sgd_solver.cpp:136] Iteration 1800, lr = 0.0001, m = 0.9
I0815 19:13:16.211633 20830 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 19:13:32.503075 20887 solver.cpp:312] Iteration 1900 (5.20112 iter/s, 19.2266s/100 iter), loss = 0.139058
I0815 19:13:32.503129 20887 solver.cpp:334]     Train net output #0: loss = 0.139058 (* 1 = 0.139058 loss)
I0815 19:13:32.503135 20887 sgd_solver.cpp:136] Iteration 1900, lr = 0.0001, m = 0.9
I0815 19:13:51.872505 20887 solver.cpp:509] Iteration 2000, Testing net (#0)
I0815 19:14:09.870875 20885 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 19:14:17.948968 20887 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.930587
I0815 19:14:17.948993 20887 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999582
I0815 19:14:17.948998 20887 solver.cpp:594]     Test net output #2: loss = 0.20762 (* 1 = 0.20762 loss)
I0815 19:14:17.949105 20887 solver.cpp:264] [MultiGPU] Tests completed in 26.0759s
I0815 19:14:18.164482 20887 solver.cpp:312] Iteration 2000 (2.19009 iter/s, 45.6602s/100 iter), loss = 0.249104
I0815 19:14:18.164510 20887 solver.cpp:334]     Train net output #0: loss = 0.249104 (* 1 = 0.249104 loss)
I0815 19:14:18.164517 20887 sgd_solver.cpp:136] Iteration 2000, lr = 0.0001, m = 0.9
I0815 19:14:37.512840 20887 solver.cpp:312] Iteration 2100 (5.16854 iter/s, 19.3478s/100 iter), loss = 0.13502
I0815 19:14:37.512863 20887 solver.cpp:334]     Train net output #0: loss = 0.135019 (* 1 = 0.135019 loss)
I0815 19:14:37.512867 20887 sgd_solver.cpp:136] Iteration 2100, lr = 0.0001, m = 0.9
I0815 19:14:56.988699 20887 solver.cpp:312] Iteration 2200 (5.1347 iter/s, 19.4753s/100 iter), loss = 0.330887
I0815 19:14:56.988752 20887 solver.cpp:334]     Train net output #0: loss = 0.330887 (* 1 = 0.330887 loss)
I0815 19:14:56.988757 20887 sgd_solver.cpp:136] Iteration 2200, lr = 0.0001, m = 0.9
I0815 19:15:16.329325 20887 solver.cpp:312] Iteration 2300 (5.17061 iter/s, 19.3401s/100 iter), loss = 0.155582
I0815 19:15:16.329350 20887 solver.cpp:334]     Train net output #0: loss = 0.155582 (* 1 = 0.155582 loss)
I0815 19:15:16.329357 20887 sgd_solver.cpp:136] Iteration 2300, lr = 0.0001, m = 0.9
I0815 19:15:18.363301 20893 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 19:15:35.822829 20887 solver.cpp:312] Iteration 2400 (5.13006 iter/s, 19.493s/100 iter), loss = 0.138271
I0815 19:15:35.822903 20887 solver.cpp:334]     Train net output #0: loss = 0.138271 (* 1 = 0.138271 loss)
I0815 19:15:35.822911 20887 sgd_solver.cpp:136] Iteration 2400, lr = 0.0001, m = 0.9
I0815 19:15:50.778798 20890 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 19:15:55.319775 20887 solver.cpp:312] Iteration 2500 (5.12915 iter/s, 19.4964s/100 iter), loss = 0.189999
I0815 19:15:55.319798 20887 solver.cpp:334]     Train net output #0: loss = 0.189999 (* 1 = 0.189999 loss)
I0815 19:15:55.319805 20887 sgd_solver.cpp:136] Iteration 2500, lr = 0.0001, m = 0.9
I0815 19:16:14.678107 20887 solver.cpp:312] Iteration 2600 (5.16588 iter/s, 19.3578s/100 iter), loss = 0.162291
I0815 19:16:14.678156 20887 solver.cpp:334]     Train net output #0: loss = 0.162291 (* 1 = 0.162291 loss)
I0815 19:16:14.678162 20887 sgd_solver.cpp:136] Iteration 2600, lr = 0.0001, m = 0.9
I0815 19:16:33.979534 20887 solver.cpp:312] Iteration 2700 (5.18111 iter/s, 19.3009s/100 iter), loss = 0.168306
I0815 19:16:33.979558 20887 solver.cpp:334]     Train net output #0: loss = 0.168306 (* 1 = 0.168306 loss)
I0815 19:16:33.979562 20887 sgd_solver.cpp:136] Iteration 2700, lr = 0.0001, m = 0.9
I0815 19:16:53.367676 20887 solver.cpp:312] Iteration 2800 (5.15793 iter/s, 19.3876s/100 iter), loss = 0.185736
I0815 19:16:53.367758 20887 solver.cpp:334]     Train net output #0: loss = 0.185736 (* 1 = 0.185736 loss)
I0815 19:16:53.367766 20887 sgd_solver.cpp:136] Iteration 2800, lr = 0.0001, m = 0.9
I0815 19:16:54.527053 20830 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 19:17:12.623780 20887 solver.cpp:312] Iteration 2900 (5.1933 iter/s, 19.2556s/100 iter), loss = 0.174731
I0815 19:17:12.623805 20887 solver.cpp:334]     Train net output #0: loss = 0.174731 (* 1 = 0.174731 loss)
I0815 19:17:12.623808 20887 sgd_solver.cpp:136] Iteration 2900, lr = 0.0001, m = 0.9
I0815 19:17:31.827792 20887 solver.cpp:312] Iteration 3000 (5.20739 iter/s, 19.2035s/100 iter), loss = 0.176327
I0815 19:17:31.827846 20887 solver.cpp:334]     Train net output #0: loss = 0.176327 (* 1 = 0.176327 loss)
I0815 19:17:31.827853 20887 sgd_solver.cpp:136] Iteration 3000, lr = 0.0001, m = 0.9
I0815 19:17:51.295557 20887 solver.cpp:312] Iteration 3100 (5.13684 iter/s, 19.4672s/100 iter), loss = 0.0998377
I0815 19:17:51.295585 20887 solver.cpp:334]     Train net output #0: loss = 0.0998376 (* 1 = 0.0998376 loss)
I0815 19:17:51.295591 20887 sgd_solver.cpp:136] Iteration 3100, lr = 0.0001, m = 0.9
I0815 19:17:58.466398 20895 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 19:18:10.561971 20887 solver.cpp:312] Iteration 3200 (5.19052 iter/s, 19.2659s/100 iter), loss = 0.336115
I0815 19:18:10.562028 20887 solver.cpp:334]     Train net output #0: loss = 0.336115 (* 1 = 0.336115 loss)
I0815 19:18:10.562036 20887 sgd_solver.cpp:136] Iteration 3200, lr = 0.0001, m = 0.9
I0815 19:18:29.885367 20887 solver.cpp:312] Iteration 3300 (5.17522 iter/s, 19.3229s/100 iter), loss = 0.136027
I0815 19:18:29.885430 20887 solver.cpp:334]     Train net output #0: loss = 0.136027 (* 1 = 0.136027 loss)
I0815 19:18:29.885447 20887 sgd_solver.cpp:136] Iteration 3300, lr = 0.0001, m = 0.9
I0815 19:18:30.323766 20891 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 19:18:49.301692 20887 solver.cpp:312] Iteration 3400 (5.15045 iter/s, 19.4158s/100 iter), loss = 0.102566
I0815 19:18:49.301744 20887 solver.cpp:334]     Train net output #0: loss = 0.102566 (* 1 = 0.102566 loss)
I0815 19:18:49.301751 20887 sgd_solver.cpp:136] Iteration 3400, lr = 0.0001, m = 0.9
I0815 19:19:08.884848 20887 solver.cpp:312] Iteration 3500 (5.10657 iter/s, 19.5826s/100 iter), loss = 0.266654
I0815 19:19:08.884873 20887 solver.cpp:334]     Train net output #0: loss = 0.266654 (* 1 = 0.266654 loss)
I0815 19:19:08.884879 20887 sgd_solver.cpp:136] Iteration 3500, lr = 0.0001, m = 0.9
I0815 19:19:28.393863 20887 solver.cpp:312] Iteration 3600 (5.12598 iter/s, 19.5085s/100 iter), loss = 0.181055
I0815 19:19:28.393939 20887 solver.cpp:334]     Train net output #0: loss = 0.181055 (* 1 = 0.181055 loss)
I0815 19:19:28.393944 20887 sgd_solver.cpp:136] Iteration 3600, lr = 0.0001, m = 0.9
I0815 19:19:34.802449 20890 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 19:19:47.861315 20887 solver.cpp:312] Iteration 3700 (5.13692 iter/s, 19.4669s/100 iter), loss = 0.160068
I0815 19:19:47.861337 20887 solver.cpp:334]     Train net output #0: loss = 0.160068 (* 1 = 0.160068 loss)
I0815 19:19:47.861343 20887 sgd_solver.cpp:136] Iteration 3700, lr = 0.0001, m = 0.9
I0815 19:20:07.360322 20887 solver.cpp:312] Iteration 3800 (5.12861 iter/s, 19.4985s/100 iter), loss = 0.246731
I0815 19:20:07.360376 20887 solver.cpp:334]     Train net output #0: loss = 0.246731 (* 1 = 0.246731 loss)
I0815 19:20:07.360383 20887 sgd_solver.cpp:136] Iteration 3800, lr = 0.0001, m = 0.9
I0815 19:20:26.714295 20887 solver.cpp:312] Iteration 3900 (5.16704 iter/s, 19.3534s/100 iter), loss = 0.159033
I0815 19:20:26.714316 20887 solver.cpp:334]     Train net output #0: loss = 0.159033 (* 1 = 0.159033 loss)
I0815 19:20:26.714321 20887 sgd_solver.cpp:136] Iteration 3900, lr = 0.0001, m = 0.9
I0815 19:20:39.035265 20841 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 19:20:45.890650 20887 solver.cpp:509] Iteration 4000, Testing net (#0)
I0815 19:21:06.824028 20935 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 19:21:06.824028 20939 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 19:21:06.824028 20885 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 19:21:07.353682 20887 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.936879
I0815 19:21:07.353703 20887 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999938
I0815 19:21:07.353709 20887 solver.cpp:594]     Test net output #2: loss = 0.176712 (* 1 = 0.176712 loss)
I0815 19:21:07.353735 20887 solver.cpp:264] [MultiGPU] Tests completed in 21.4625s
I0815 19:21:07.596345 20887 solver.cpp:312] Iteration 4000 (2.44613 iter/s, 40.8809s/100 iter), loss = 0.100487
I0815 19:21:07.596375 20887 solver.cpp:334]     Train net output #0: loss = 0.100487 (* 1 = 0.100487 loss)
I0815 19:21:07.596381 20887 sgd_solver.cpp:136] Iteration 4000, lr = 0.0001, m = 0.9
I0815 19:21:27.010953 20887 solver.cpp:312] Iteration 4100 (5.15091 iter/s, 19.4141s/100 iter), loss = 0.173701
I0815 19:21:27.011056 20887 solver.cpp:334]     Train net output #0: loss = 0.173701 (* 1 = 0.173701 loss)
I0815 19:21:27.011065 20887 sgd_solver.cpp:136] Iteration 4100, lr = 0.0001, m = 0.9
I0815 19:21:46.538709 20887 solver.cpp:312] Iteration 4200 (5.12106 iter/s, 19.5272s/100 iter), loss = 0.189477
I0815 19:21:46.538733 20887 solver.cpp:334]     Train net output #0: loss = 0.189478 (* 1 = 0.189478 loss)
I0815 19:21:46.538739 20887 sgd_solver.cpp:136] Iteration 4200, lr = 0.0001, m = 0.9
I0815 19:22:05.175688 20893 data_reader.cpp:288] Starting prefetch of epoch 4
I0815 19:22:06.334301 20887 solver.cpp:312] Iteration 4300 (5.05177 iter/s, 19.795s/100 iter), loss = 0.185414
I0815 19:22:06.334324 20887 solver.cpp:334]     Train net output #0: loss = 0.185414 (* 1 = 0.185414 loss)
I0815 19:22:06.334328 20887 sgd_solver.cpp:136] Iteration 4300, lr = 0.0001, m = 0.9
I0815 19:22:25.539458 20887 solver.cpp:312] Iteration 4400 (5.20708 iter/s, 19.2046s/100 iter), loss = 0.116149
I0815 19:22:25.539481 20887 solver.cpp:334]     Train net output #0: loss = 0.116149 (* 1 = 0.116149 loss)
I0815 19:22:25.539487 20887 sgd_solver.cpp:136] Iteration 4400, lr = 0.0001, m = 0.9
I0815 19:22:36.911303 20891 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 19:22:44.745389 20887 solver.cpp:312] Iteration 4500 (5.20687 iter/s, 19.2054s/100 iter), loss = 0.0911353
I0815 19:22:44.745435 20887 solver.cpp:334]     Train net output #0: loss = 0.0911354 (* 1 = 0.0911354 loss)
I0815 19:22:44.745455 20887 sgd_solver.cpp:136] Iteration 4500, lr = 0.0001, m = 0.9
I0815 19:23:04.374128 20887 solver.cpp:312] Iteration 4600 (5.09471 iter/s, 19.6282s/100 iter), loss = 0.180316
I0815 19:23:04.374155 20887 solver.cpp:334]     Train net output #0: loss = 0.180316 (* 1 = 0.180316 loss)
I0815 19:23:04.374161 20887 sgd_solver.cpp:136] Iteration 4600, lr = 0.0001, m = 0.9
I0815 19:23:23.783146 20887 solver.cpp:312] Iteration 4700 (5.15239 iter/s, 19.4085s/100 iter), loss = 0.156884
I0815 19:23:23.784695 20887 solver.cpp:334]     Train net output #0: loss = 0.156884 (* 1 = 0.156884 loss)
I0815 19:23:23.784715 20887 sgd_solver.cpp:136] Iteration 4700, lr = 0.0001, m = 0.9
I0815 19:23:41.463778 20890 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 19:23:43.381151 20887 solver.cpp:312] Iteration 4800 (5.1027 iter/s, 19.5975s/100 iter), loss = 0.232845
I0815 19:23:43.381178 20887 solver.cpp:334]     Train net output #0: loss = 0.232845 (* 1 = 0.232845 loss)
I0815 19:23:43.381184 20887 sgd_solver.cpp:136] Iteration 4800, lr = 0.0001, m = 0.9
I0815 19:24:02.906836 20887 solver.cpp:312] Iteration 4900 (5.1216 iter/s, 19.5251s/100 iter), loss = 0.142292
I0815 19:24:02.906911 20887 solver.cpp:334]     Train net output #0: loss = 0.142292 (* 1 = 0.142292 loss)
I0815 19:24:02.906919 20887 sgd_solver.cpp:136] Iteration 4900, lr = 0.0001, m = 0.9
I0815 19:24:22.280521 20887 solver.cpp:312] Iteration 5000 (5.16178 iter/s, 19.3731s/100 iter), loss = 0.159069
I0815 19:24:22.280550 20887 solver.cpp:334]     Train net output #0: loss = 0.159069 (* 1 = 0.159069 loss)
I0815 19:24:22.280556 20887 sgd_solver.cpp:136] Iteration 5000, lr = 0.0001, m = 0.9
I0815 19:24:41.897524 20887 solver.cpp:312] Iteration 5100 (5.09776 iter/s, 19.6165s/100 iter), loss = 0.212673
I0815 19:24:41.897581 20887 solver.cpp:334]     Train net output #0: loss = 0.212673 (* 1 = 0.212673 loss)
I0815 19:24:41.897588 20887 sgd_solver.cpp:136] Iteration 5100, lr = 0.0001, m = 0.9
I0815 19:24:45.880569 20895 data_reader.cpp:288] Starting prefetch of epoch 4
I0815 19:25:01.499992 20887 solver.cpp:312] Iteration 5200 (5.10154 iter/s, 19.6019s/100 iter), loss = 0.202232
I0815 19:25:01.500015 20887 solver.cpp:334]     Train net output #0: loss = 0.202232 (* 1 = 0.202232 loss)
I0815 19:25:01.500020 20887 sgd_solver.cpp:136] Iteration 5200, lr = 0.0001, m = 0.9
I0815 19:25:18.153836 20891 data_reader.cpp:288] Starting prefetch of epoch 4
I0815 19:25:21.033697 20887 solver.cpp:312] Iteration 5300 (5.1195 iter/s, 19.5332s/100 iter), loss = 0.102887
I0815 19:25:21.033720 20887 solver.cpp:334]     Train net output #0: loss = 0.102887 (* 1 = 0.102887 loss)
I0815 19:25:21.033725 20887 sgd_solver.cpp:136] Iteration 5300, lr = 0.0001, m = 0.9
I0815 19:25:40.777055 20887 solver.cpp:312] Iteration 5400 (5.06513 iter/s, 19.7428s/100 iter), loss = 0.101848
I0815 19:25:40.777081 20887 solver.cpp:334]     Train net output #0: loss = 0.101848 (* 1 = 0.101848 loss)
I0815 19:25:40.777086 20887 sgd_solver.cpp:136] Iteration 5400, lr = 0.0001, m = 0.9
I0815 19:26:00.193652 20887 solver.cpp:312] Iteration 5500 (5.15037 iter/s, 19.4161s/100 iter), loss = 0.232078
I0815 19:26:00.193701 20887 solver.cpp:334]     Train net output #0: loss = 0.232078 (* 1 = 0.232078 loss)
I0815 19:26:00.193708 20887 sgd_solver.cpp:136] Iteration 5500, lr = 0.0001, m = 0.9
I0815 19:26:19.473047 20887 solver.cpp:312] Iteration 5600 (5.18703 iter/s, 19.2789s/100 iter), loss = 0.0812484
I0815 19:26:19.473073 20887 solver.cpp:334]     Train net output #0: loss = 0.0812484 (* 1 = 0.0812484 loss)
I0815 19:26:19.473076 20887 sgd_solver.cpp:136] Iteration 5600, lr = 0.0001, m = 0.9
I0815 19:26:22.597847 20893 data_reader.cpp:288] Starting prefetch of epoch 5
I0815 19:26:38.969365 20887 solver.cpp:312] Iteration 5700 (5.12931 iter/s, 19.4958s/100 iter), loss = 0.380505
I0815 19:26:38.969432 20887 solver.cpp:334]     Train net output #0: loss = 0.380505 (* 1 = 0.380505 loss)
I0815 19:26:38.969437 20887 sgd_solver.cpp:136] Iteration 5700, lr = 0.0001, m = 0.9
I0815 19:26:58.460011 20887 solver.cpp:312] Iteration 5800 (5.13081 iter/s, 19.4901s/100 iter), loss = 0.117095
I0815 19:26:58.460039 20887 solver.cpp:334]     Train net output #0: loss = 0.117095 (* 1 = 0.117095 loss)
I0815 19:26:58.460045 20887 sgd_solver.cpp:136] Iteration 5800, lr = 0.0001, m = 0.9
I0815 19:27:17.771387 20887 solver.cpp:312] Iteration 5900 (5.17844 iter/s, 19.3108s/100 iter), loss = 0.236365
I0815 19:27:17.771737 20887 solver.cpp:334]     Train net output #0: loss = 0.236365 (* 1 = 0.236365 loss)
I0815 19:27:17.771744 20887 sgd_solver.cpp:136] Iteration 5900, lr = 0.0001, m = 0.9
I0815 19:27:26.728127 20893 data_reader.cpp:288] Starting prefetch of epoch 6
I0815 19:27:37.020706 20887 solver.cpp:509] Iteration 6000, Testing net (#0)
I0815 19:27:44.551409 20935 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 19:27:48.944089 20887 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.938328
I0815 19:27:48.944151 20887 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999991
I0815 19:27:48.944159 20887 solver.cpp:594]     Test net output #2: loss = 0.175991 (* 1 = 0.175991 loss)
I0815 19:27:48.944178 20887 solver.cpp:264] [MultiGPU] Tests completed in 11.9231s
I0815 19:27:49.134445 20887 solver.cpp:312] Iteration 6000 (3.18855 iter/s, 31.3622s/100 iter), loss = 0.248493
I0815 19:27:49.134470 20887 solver.cpp:334]     Train net output #0: loss = 0.248493 (* 1 = 0.248493 loss)
I0815 19:27:49.134476 20887 sgd_solver.cpp:136] Iteration 6000, lr = 0.0001, m = 0.9
I0815 19:28:08.641875 20887 solver.cpp:312] Iteration 6100 (5.12639 iter/s, 19.5069s/100 iter), loss = 0.136336
I0815 19:28:08.641898 20887 solver.cpp:334]     Train net output #0: loss = 0.136336 (* 1 = 0.136336 loss)
I0815 19:28:08.641902 20887 sgd_solver.cpp:136] Iteration 6100, lr = 0.0001, m = 0.9
I0815 19:28:27.934284 20887 solver.cpp:312] Iteration 6200 (5.18353 iter/s, 19.2919s/100 iter), loss = 0.160164
I0815 19:28:27.934340 20887 solver.cpp:334]     Train net output #0: loss = 0.160164 (* 1 = 0.160164 loss)
I0815 19:28:27.934345 20887 sgd_solver.cpp:136] Iteration 6200, lr = 0.0001, m = 0.9
I0815 19:28:43.127050 20893 data_reader.cpp:288] Starting prefetch of epoch 7
I0815 19:28:47.550725 20887 solver.cpp:312] Iteration 6300 (5.09791 iter/s, 19.6159s/100 iter), loss = 0.136442
I0815 19:28:47.550750 20887 solver.cpp:334]     Train net output #0: loss = 0.136442 (* 1 = 0.136442 loss)
I0815 19:28:47.550753 20887 sgd_solver.cpp:136] Iteration 6300, lr = 0.0001, m = 0.9
I0815 19:29:07.030766 20887 solver.cpp:312] Iteration 6400 (5.1336 iter/s, 19.4795s/100 iter), loss = 0.16143
I0815 19:29:07.030813 20887 solver.cpp:334]     Train net output #0: loss = 0.16143 (* 1 = 0.16143 loss)
I0815 19:29:07.030818 20887 sgd_solver.cpp:136] Iteration 6400, lr = 0.0001, m = 0.9
I0815 19:29:26.554488 20887 solver.cpp:312] Iteration 6500 (5.12211 iter/s, 19.5232s/100 iter), loss = 0.142198
I0815 19:29:26.554512 20887 solver.cpp:334]     Train net output #0: loss = 0.142198 (* 1 = 0.142198 loss)
I0815 19:29:26.554515 20887 sgd_solver.cpp:136] Iteration 6500, lr = 0.0001, m = 0.9
I0815 19:29:46.035254 20887 solver.cpp:312] Iteration 6600 (5.13341 iter/s, 19.4802s/100 iter), loss = 0.224642
I0815 19:29:46.035298 20887 solver.cpp:334]     Train net output #0: loss = 0.224642 (* 1 = 0.224642 loss)
I0815 19:29:46.035305 20887 sgd_solver.cpp:136] Iteration 6600, lr = 0.0001, m = 0.9
I0815 19:29:47.598014 20841 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 19:30:05.281642 20887 solver.cpp:312] Iteration 6700 (5.19592 iter/s, 19.2459s/100 iter), loss = 0.182586
I0815 19:30:05.281666 20887 solver.cpp:334]     Train net output #0: loss = 0.182586 (* 1 = 0.182586 loss)
I0815 19:30:05.281672 20887 sgd_solver.cpp:136] Iteration 6700, lr = 0.0001, m = 0.9
I0815 19:30:19.666806 20890 data_reader.cpp:288] Starting prefetch of epoch 4
I0815 19:30:24.849052 20887 solver.cpp:312] Iteration 6800 (5.11068 iter/s, 19.5669s/100 iter), loss = 0.0855129
I0815 19:30:24.849082 20887 solver.cpp:334]     Train net output #0: loss = 0.0855128 (* 1 = 0.0855128 loss)
I0815 19:30:24.849089 20887 sgd_solver.cpp:136] Iteration 6800, lr = 0.0001, m = 0.9
I0815 19:30:44.217967 20887 solver.cpp:312] Iteration 6900 (5.16305 iter/s, 19.3684s/100 iter), loss = 0.168279
I0815 19:30:44.217991 20887 solver.cpp:334]     Train net output #0: loss = 0.168279 (* 1 = 0.168279 loss)
I0815 19:30:44.217996 20887 sgd_solver.cpp:136] Iteration 6900, lr = 0.0001, m = 0.9
I0815 19:31:03.787535 20887 solver.cpp:312] Iteration 7000 (5.11012 iter/s, 19.569s/100 iter), loss = 0.233495
I0815 19:31:03.787606 20887 solver.cpp:334]     Train net output #0: loss = 0.233495 (* 1 = 0.233495 loss)
I0815 19:31:03.787613 20887 sgd_solver.cpp:136] Iteration 7000, lr = 0.0001, m = 0.9
I0815 19:31:23.515705 20887 solver.cpp:312] Iteration 7100 (5.06903 iter/s, 19.7276s/100 iter), loss = 0.13819
I0815 19:31:23.515725 20887 solver.cpp:334]     Train net output #0: loss = 0.13819 (* 1 = 0.13819 loss)
I0815 19:31:23.515729 20887 sgd_solver.cpp:136] Iteration 7100, lr = 0.0001, m = 0.9
I0815 19:31:24.130620 20841 data_reader.cpp:288] Starting prefetch of epoch 4
I0815 19:31:42.798174 20887 solver.cpp:312] Iteration 7200 (5.1862 iter/s, 19.2819s/100 iter), loss = 0.142292
I0815 19:31:42.798228 20887 solver.cpp:334]     Train net output #0: loss = 0.142292 (* 1 = 0.142292 loss)
I0815 19:31:42.798234 20887 sgd_solver.cpp:136] Iteration 7200, lr = 0.0001, m = 0.9
I0815 19:32:02.208838 20887 solver.cpp:312] Iteration 7300 (5.15195 iter/s, 19.4101s/100 iter), loss = 0.110638
I0815 19:32:02.208864 20887 solver.cpp:334]     Train net output #0: loss = 0.110638 (* 1 = 0.110638 loss)
I0815 19:32:02.208870 20887 sgd_solver.cpp:136] Iteration 7300, lr = 0.0001, m = 0.9
I0815 19:32:21.843300 20887 solver.cpp:312] Iteration 7400 (5.09323 iter/s, 19.6339s/100 iter), loss = 0.211895
I0815 19:32:21.843353 20887 solver.cpp:334]     Train net output #0: loss = 0.211895 (* 1 = 0.211895 loss)
I0815 19:32:21.843358 20887 sgd_solver.cpp:136] Iteration 7400, lr = 0.0001, m = 0.9
I0815 19:32:28.391145 20841 data_reader.cpp:288] Starting prefetch of epoch 5
I0815 19:32:41.259456 20887 solver.cpp:312] Iteration 7500 (5.15049 iter/s, 19.4156s/100 iter), loss = 0.151964
I0815 19:32:41.259485 20887 solver.cpp:334]     Train net output #0: loss = 0.151964 (* 1 = 0.151964 loss)
I0815 19:32:41.259490 20887 sgd_solver.cpp:136] Iteration 7500, lr = 0.0001, m = 0.9
I0815 19:33:00.865417 20891 data_reader.cpp:288] Starting prefetch of epoch 5
I0815 19:33:01.016575 20887 solver.cpp:312] Iteration 7600 (5.0616 iter/s, 19.7566s/100 iter), loss = 0.0955082
I0815 19:33:01.016598 20887 solver.cpp:334]     Train net output #0: loss = 0.0955081 (* 1 = 0.0955081 loss)
I0815 19:33:01.016604 20887 sgd_solver.cpp:136] Iteration 7600, lr = 0.0001, m = 0.9
I0815 19:33:20.556568 20887 solver.cpp:312] Iteration 7700 (5.11785 iter/s, 19.5395s/100 iter), loss = 0.0835602
I0815 19:33:20.556587 20887 solver.cpp:334]     Train net output #0: loss = 0.0835601 (* 1 = 0.0835601 loss)
I0815 19:33:20.556591 20887 sgd_solver.cpp:136] Iteration 7700, lr = 0.0001, m = 0.9
I0815 19:33:40.040750 20887 solver.cpp:312] Iteration 7800 (5.13251 iter/s, 19.4837s/100 iter), loss = 0.243024
I0815 19:33:40.040801 20887 solver.cpp:334]     Train net output #0: loss = 0.243023 (* 1 = 0.243023 loss)
I0815 19:33:40.040807 20887 sgd_solver.cpp:136] Iteration 7800, lr = 0.0001, m = 0.9
I0815 19:33:59.611721 20887 solver.cpp:312] Iteration 7900 (5.10975 iter/s, 19.5704s/100 iter), loss = 0.117616
I0815 19:33:59.611743 20887 solver.cpp:334]     Train net output #0: loss = 0.117616 (* 1 = 0.117616 loss)
I0815 19:33:59.611750 20887 sgd_solver.cpp:136] Iteration 7900, lr = 0.0001, m = 0.9
I0815 19:34:05.489470 20830 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 19:34:18.691819 20887 solver.cpp:509] Iteration 8000, Testing net (#0)
I0815 19:34:29.672966 20869 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 19:34:30.631613 20887 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.941444
I0815 19:34:30.631631 20887 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999995
I0815 19:34:30.631636 20887 solver.cpp:594]     Test net output #2: loss = 0.159701 (* 1 = 0.159701 loss)
I0815 19:34:30.631673 20887 solver.cpp:264] [MultiGPU] Tests completed in 11.9395s
I0815 19:34:30.830366 20887 solver.cpp:312] Iteration 8000 (3.2033 iter/s, 31.2178s/100 iter), loss = 0.124213
I0815 19:34:30.830389 20887 solver.cpp:334]     Train net output #0: loss = 0.124213 (* 1 = 0.124213 loss)
I0815 19:34:30.830394 20887 sgd_solver.cpp:136] Iteration 8000, lr = 0.0001, m = 0.9
I0815 19:34:49.296908 20830 data_reader.cpp:288] Starting prefetch of epoch 4
I0815 19:34:50.246264 20887 solver.cpp:312] Iteration 8100 (5.15056 iter/s, 19.4154s/100 iter), loss = 0.135762
I0815 19:34:50.246289 20887 solver.cpp:334]     Train net output #0: loss = 0.135762 (* 1 = 0.135762 loss)
I0815 19:34:50.246295 20887 sgd_solver.cpp:136] Iteration 8100, lr = 0.0001, m = 0.9
I0815 19:35:09.830482 20887 solver.cpp:312] Iteration 8200 (5.10629 iter/s, 19.5837s/100 iter), loss = 0.098685
I0815 19:35:09.830510 20887 solver.cpp:334]     Train net output #0: loss = 0.0986848 (* 1 = 0.0986848 loss)
I0815 19:35:09.830514 20887 sgd_solver.cpp:136] Iteration 8200, lr = 0.0001, m = 0.9
I0815 19:35:29.041316 20887 solver.cpp:312] Iteration 8300 (5.20554 iter/s, 19.2103s/100 iter), loss = 0.29135
I0815 19:35:29.041363 20887 solver.cpp:334]     Train net output #0: loss = 0.29135 (* 1 = 0.29135 loss)
I0815 19:35:29.041368 20887 sgd_solver.cpp:136] Iteration 8300, lr = 0.0001, m = 0.9
I0815 19:35:48.271138 20887 solver.cpp:312] Iteration 8400 (5.2004 iter/s, 19.2293s/100 iter), loss = 0.117632
I0815 19:35:48.271165 20887 solver.cpp:334]     Train net output #0: loss = 0.117632 (* 1 = 0.117632 loss)
I0815 19:35:48.271172 20887 sgd_solver.cpp:136] Iteration 8400, lr = 0.0001, m = 0.9
I0815 19:35:53.328879 20830 data_reader.cpp:288] Starting prefetch of epoch 5
I0815 19:36:08.002846 20887 solver.cpp:312] Iteration 8500 (5.06812 iter/s, 19.7312s/100 iter), loss = 0.103444
I0815 19:36:08.002931 20887 solver.cpp:334]     Train net output #0: loss = 0.103444 (* 1 = 0.103444 loss)
I0815 19:36:08.002938 20887 sgd_solver.cpp:136] Iteration 8500, lr = 0.0001, m = 0.9
I0815 19:36:27.610750 20887 solver.cpp:312] Iteration 8600 (5.10012 iter/s, 19.6074s/100 iter), loss = 0.168835
I0815 19:36:27.610780 20887 solver.cpp:334]     Train net output #0: loss = 0.168835 (* 1 = 0.168835 loss)
I0815 19:36:27.610786 20887 sgd_solver.cpp:136] Iteration 8600, lr = 0.0001, m = 0.9
I0815 19:36:47.076860 20887 solver.cpp:312] Iteration 8700 (5.13728 iter/s, 19.4656s/100 iter), loss = 0.101042
I0815 19:36:47.076912 20887 solver.cpp:334]     Train net output #0: loss = 0.101042 (* 1 = 0.101042 loss)
I0815 19:36:47.076920 20887 sgd_solver.cpp:136] Iteration 8700, lr = 0.0001, m = 0.9
I0815 19:36:57.846571 20841 data_reader.cpp:288] Starting prefetch of epoch 6
I0815 19:37:06.374675 20887 solver.cpp:312] Iteration 8800 (5.18208 iter/s, 19.2973s/100 iter), loss = 0.0972986
I0815 19:37:06.374704 20887 solver.cpp:334]     Train net output #0: loss = 0.0972985 (* 1 = 0.0972985 loss)
I0815 19:37:06.374711 20887 sgd_solver.cpp:136] Iteration 8800, lr = 0.0001, m = 0.9
I0815 19:37:25.843616 20887 solver.cpp:312] Iteration 8900 (5.13653 iter/s, 19.4684s/100 iter), loss = 0.139747
I0815 19:37:25.843660 20887 solver.cpp:334]     Train net output #0: loss = 0.139747 (* 1 = 0.139747 loss)
I0815 19:37:25.843667 20887 sgd_solver.cpp:136] Iteration 8900, lr = 0.0001, m = 0.9
I0815 19:37:30.011557 20890 data_reader.cpp:288] Starting prefetch of epoch 5
I0815 19:37:45.297019 20887 solver.cpp:312] Iteration 9000 (5.14063 iter/s, 19.4529s/100 iter), loss = 0.0983153
I0815 19:37:45.297044 20887 solver.cpp:334]     Train net output #0: loss = 0.0983151 (* 1 = 0.0983151 loss)
I0815 19:37:45.297050 20887 sgd_solver.cpp:136] Iteration 9000, lr = 0.0001, m = 0.9
I0815 19:38:04.594187 20887 solver.cpp:312] Iteration 9100 (5.18225 iter/s, 19.2966s/100 iter), loss = 0.190395
I0815 19:38:04.594288 20887 solver.cpp:334]     Train net output #0: loss = 0.190395 (* 1 = 0.190395 loss)
I0815 19:38:04.594295 20887 sgd_solver.cpp:136] Iteration 9100, lr = 0.0001, m = 0.9
I0815 19:38:24.299403 20887 solver.cpp:312] Iteration 9200 (5.07494 iter/s, 19.7047s/100 iter), loss = 0.11624
I0815 19:38:24.299432 20887 solver.cpp:334]     Train net output #0: loss = 0.11624 (* 1 = 0.11624 loss)
I0815 19:38:24.299438 20887 sgd_solver.cpp:136] Iteration 9200, lr = 0.0001, m = 0.9
I0815 19:38:34.330744 20830 data_reader.cpp:288] Starting prefetch of epoch 6
I0815 19:38:43.509220 20887 solver.cpp:312] Iteration 9300 (5.20582 iter/s, 19.2093s/100 iter), loss = 0.126366
I0815 19:38:43.509275 20887 solver.cpp:334]     Train net output #0: loss = 0.126366 (* 1 = 0.126366 loss)
I0815 19:38:43.509280 20887 sgd_solver.cpp:136] Iteration 9300, lr = 0.0001, m = 0.9
I0815 19:39:02.916123 20887 solver.cpp:312] Iteration 9400 (5.15295 iter/s, 19.4064s/100 iter), loss = 0.144463
I0815 19:39:02.916157 20887 solver.cpp:334]     Train net output #0: loss = 0.144463 (* 1 = 0.144463 loss)
I0815 19:39:02.916164 20887 sgd_solver.cpp:136] Iteration 9400, lr = 0.0001, m = 0.9
I0815 19:39:22.391883 20887 solver.cpp:312] Iteration 9500 (5.13473 iter/s, 19.4752s/100 iter), loss = 0.115688
I0815 19:39:22.391937 20887 solver.cpp:334]     Train net output #0: loss = 0.115687 (* 1 = 0.115687 loss)
I0815 19:39:22.391943 20887 sgd_solver.cpp:136] Iteration 9500, lr = 0.0001, m = 0.9
I0815 19:39:38.502830 20841 data_reader.cpp:288] Starting prefetch of epoch 7
I0815 19:39:41.792395 20887 solver.cpp:312] Iteration 9600 (5.15464 iter/s, 19.4s/100 iter), loss = 0.114717
I0815 19:39:41.792423 20887 solver.cpp:334]     Train net output #0: loss = 0.114716 (* 1 = 0.114716 loss)
I0815 19:39:41.792429 20887 sgd_solver.cpp:136] Iteration 9600, lr = 0.0001, m = 0.9
I0815 19:40:01.307190 20887 solver.cpp:312] Iteration 9700 (5.12446 iter/s, 19.5143s/100 iter), loss = 0.104907
I0815 19:40:01.307272 20887 solver.cpp:334]     Train net output #0: loss = 0.104907 (* 1 = 0.104907 loss)
I0815 19:40:01.307277 20887 sgd_solver.cpp:136] Iteration 9700, lr = 0.0001, m = 0.9
I0815 19:40:10.660459 20890 data_reader.cpp:288] Starting prefetch of epoch 6
I0815 19:40:20.864078 20887 solver.cpp:312] Iteration 9800 (5.11343 iter/s, 19.5564s/100 iter), loss = 0.108421
I0815 19:40:20.864099 20887 solver.cpp:334]     Train net output #0: loss = 0.108421 (* 1 = 0.108421 loss)
I0815 19:40:20.864104 20887 sgd_solver.cpp:136] Iteration 9800, lr = 0.0001, m = 0.9
I0815 19:40:40.437625 20887 solver.cpp:312] Iteration 9900 (5.10908 iter/s, 19.573s/100 iter), loss = 0.183411
I0815 19:40:40.437707 20887 solver.cpp:334]     Train net output #0: loss = 0.183411 (* 1 = 0.183411 loss)
I0815 19:40:40.437714 20887 sgd_solver.cpp:136] Iteration 9900, lr = 0.0001, m = 0.9
I0815 19:40:59.863323 20887 solver.cpp:639] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/initial/cityscapes5_jsegnet21v2_iter_10000.caffemodel
I0815 19:41:00.159395 20887 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/initial/cityscapes5_jsegnet21v2_iter_10000.solverstate
I0815 19:41:00.167876 20887 solver.cpp:509] Iteration 10000, Testing net (#0)
I0815 19:41:18.551153 20937 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 19:41:27.401728 20887 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.946446
I0815 19:41:27.401753 20887 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999994
I0815 19:41:27.401760 20887 solver.cpp:594]     Test net output #2: loss = 0.155604 (* 1 = 0.155604 loss)
I0815 19:41:27.401788 20887 solver.cpp:264] [MultiGPU] Tests completed in 27.2331s
I0815 19:41:27.599385 20887 solver.cpp:312] Iteration 10000 (2.12042 iter/s, 47.1605s/100 iter), loss = 0.0919557
I0815 19:41:27.599413 20887 solver.cpp:334]     Train net output #0: loss = 0.0919555 (* 1 = 0.0919555 loss)
I0815 19:41:27.599419 20887 sgd_solver.cpp:136] Iteration 10000, lr = 0.0001, m = 0.9
I0815 19:41:46.959487 20887 solver.cpp:312] Iteration 10100 (5.16541 iter/s, 19.3596s/100 iter), loss = 0.0956914
I0815 19:41:46.959514 20887 solver.cpp:334]     Train net output #0: loss = 0.0956911 (* 1 = 0.0956911 loss)
I0815 19:41:46.959519 20887 sgd_solver.cpp:136] Iteration 10100, lr = 0.0001, m = 0.9
I0815 19:42:06.174270 20887 solver.cpp:312] Iteration 10200 (5.20447 iter/s, 19.2142s/100 iter), loss = 0.323771
I0815 19:42:06.174342 20887 solver.cpp:334]     Train net output #0: loss = 0.32377 (* 1 = 0.32377 loss)
I0815 19:42:06.174347 20887 sgd_solver.cpp:136] Iteration 10200, lr = 0.0001, m = 0.9
I0815 19:42:14.739534 20893 data_reader.cpp:288] Starting prefetch of epoch 8
I0815 19:42:25.655863 20887 solver.cpp:312] Iteration 10300 (5.13319 iter/s, 19.4811s/100 iter), loss = 0.0961498
I0815 19:42:25.655890 20887 solver.cpp:334]     Train net output #0: loss = 0.0961496 (* 1 = 0.0961496 loss)
I0815 19:42:25.655897 20887 sgd_solver.cpp:136] Iteration 10300, lr = 0.0001, m = 0.9
I0815 19:42:45.040076 20887 solver.cpp:312] Iteration 10400 (5.15898 iter/s, 19.3837s/100 iter), loss = 0.0783144
I0815 19:42:45.040120 20887 solver.cpp:334]     Train net output #0: loss = 0.0783141 (* 1 = 0.0783141 loss)
I0815 19:42:45.040127 20887 sgd_solver.cpp:136] Iteration 10400, lr = 0.0001, m = 0.9
I0815 19:43:04.531802 20887 solver.cpp:312] Iteration 10500 (5.13052 iter/s, 19.4912s/100 iter), loss = 0.0840846
I0815 19:43:04.531828 20887 solver.cpp:334]     Train net output #0: loss = 0.0840844 (* 1 = 0.0840844 loss)
I0815 19:43:04.531834 20887 sgd_solver.cpp:136] Iteration 10500, lr = 0.0001, m = 0.9
I0815 19:43:18.949287 20893 data_reader.cpp:288] Starting prefetch of epoch 9
I0815 19:43:24.022712 20887 solver.cpp:312] Iteration 10600 (5.13074 iter/s, 19.4904s/100 iter), loss = 0.14687
I0815 19:43:24.022734 20887 solver.cpp:334]     Train net output #0: loss = 0.146869 (* 1 = 0.146869 loss)
I0815 19:43:24.022740 20887 sgd_solver.cpp:136] Iteration 10600, lr = 0.0001, m = 0.9
I0815 19:43:43.455555 20887 solver.cpp:312] Iteration 10700 (5.14607 iter/s, 19.4323s/100 iter), loss = 0.088799
I0815 19:43:43.455580 20887 solver.cpp:334]     Train net output #0: loss = 0.0887987 (* 1 = 0.0887987 loss)
I0815 19:43:43.455585 20887 sgd_solver.cpp:136] Iteration 10700, lr = 0.0001, m = 0.9
I0815 19:43:51.192931 20841 data_reader.cpp:288] Starting prefetch of epoch 8
I0815 19:44:02.871054 20887 solver.cpp:312] Iteration 10800 (5.15067 iter/s, 19.415s/100 iter), loss = 0.166933
I0815 19:44:02.871083 20887 solver.cpp:334]     Train net output #0: loss = 0.166933 (* 1 = 0.166933 loss)
I0815 19:44:02.871089 20887 sgd_solver.cpp:136] Iteration 10800, lr = 0.0001, m = 0.9
I0815 19:44:22.268507 20887 solver.cpp:312] Iteration 10900 (5.15546 iter/s, 19.3969s/100 iter), loss = 0.0660203
I0815 19:44:22.268555 20887 solver.cpp:334]     Train net output #0: loss = 0.0660201 (* 1 = 0.0660201 loss)
I0815 19:44:22.268561 20887 sgd_solver.cpp:136] Iteration 10900, lr = 0.0001, m = 0.9
I0815 19:44:41.583034 20887 solver.cpp:312] Iteration 11000 (5.17759 iter/s, 19.314s/100 iter), loss = 0.107541
I0815 19:44:41.583060 20887 solver.cpp:334]     Train net output #0: loss = 0.107541 (* 1 = 0.107541 loss)
I0815 19:44:41.583063 20887 sgd_solver.cpp:136] Iteration 11000, lr = 0.0001, m = 0.9
I0815 19:44:55.176383 20890 data_reader.cpp:288] Starting prefetch of epoch 7
I0815 19:45:00.983646 20887 solver.cpp:312] Iteration 11100 (5.15462 iter/s, 19.4001s/100 iter), loss = 0.122738
I0815 19:45:00.983670 20887 solver.cpp:334]     Train net output #0: loss = 0.122738 (* 1 = 0.122738 loss)
I0815 19:45:00.983675 20887 sgd_solver.cpp:136] Iteration 11100, lr = 0.0001, m = 0.9
I0815 19:45:20.423815 20887 solver.cpp:312] Iteration 11200 (5.14413 iter/s, 19.4396s/100 iter), loss = 0.0945036
I0815 19:45:20.423836 20887 solver.cpp:334]     Train net output #0: loss = 0.0945033 (* 1 = 0.0945033 loss)
I0815 19:45:20.423841 20887 sgd_solver.cpp:136] Iteration 11200, lr = 0.0001, m = 0.9
I0815 19:45:39.773124 20887 solver.cpp:312] Iteration 11300 (5.16829 iter/s, 19.3488s/100 iter), loss = 0.108577
I0815 19:45:39.773196 20887 solver.cpp:334]     Train net output #0: loss = 0.108577 (* 1 = 0.108577 loss)
I0815 19:45:39.773203 20887 sgd_solver.cpp:136] Iteration 11300, lr = 0.0001, m = 0.9
I0815 19:45:59.489234 20887 solver.cpp:312] Iteration 11400 (5.07213 iter/s, 19.7156s/100 iter), loss = 0.0807952
I0815 19:45:59.489254 20887 solver.cpp:334]     Train net output #0: loss = 0.0807949 (* 1 = 0.0807949 loss)
I0815 19:45:59.489259 20887 sgd_solver.cpp:136] Iteration 11400, lr = 0.0001, m = 0.9
I0815 19:45:59.691185 20895 data_reader.cpp:288] Starting prefetch of epoch 5
I0815 19:46:18.877540 20887 solver.cpp:312] Iteration 11500 (5.15789 iter/s, 19.3878s/100 iter), loss = 0.141415
I0815 19:46:18.877622 20887 solver.cpp:334]     Train net output #0: loss = 0.141415 (* 1 = 0.141415 loss)
I0815 19:46:18.877629 20887 sgd_solver.cpp:136] Iteration 11500, lr = 0.0001, m = 0.9
I0815 19:46:31.891383 20830 data_reader.cpp:288] Starting prefetch of epoch 7
I0815 19:46:38.269542 20887 solver.cpp:312] Iteration 11600 (5.15691 iter/s, 19.3915s/100 iter), loss = 0.147734
I0815 19:46:38.269572 20887 solver.cpp:334]     Train net output #0: loss = 0.147733 (* 1 = 0.147733 loss)
I0815 19:46:38.269578 20887 sgd_solver.cpp:136] Iteration 11600, lr = 0.0001, m = 0.9
I0815 19:46:57.755529 20887 solver.cpp:312] Iteration 11700 (5.13203 iter/s, 19.4855s/100 iter), loss = 0.0574526
I0815 19:46:57.755609 20887 solver.cpp:334]     Train net output #0: loss = 0.0574523 (* 1 = 0.0574523 loss)
I0815 19:46:57.755622 20887 sgd_solver.cpp:136] Iteration 11700, lr = 0.0001, m = 0.9
I0815 19:47:17.143787 20887 solver.cpp:312] Iteration 11800 (5.1579 iter/s, 19.3877s/100 iter), loss = 0.136417
I0815 19:47:17.143813 20887 solver.cpp:334]     Train net output #0: loss = 0.136417 (* 1 = 0.136417 loss)
I0815 19:47:17.143821 20887 sgd_solver.cpp:136] Iteration 11800, lr = 0.0001, m = 0.9
I0815 19:47:35.829191 20891 data_reader.cpp:288] Starting prefetch of epoch 6
I0815 19:47:36.561664 20887 solver.cpp:312] Iteration 11900 (5.15004 iter/s, 19.4173s/100 iter), loss = 0.109749
I0815 19:47:36.561684 20887 solver.cpp:334]     Train net output #0: loss = 0.109748 (* 1 = 0.109748 loss)
I0815 19:47:36.561691 20887 sgd_solver.cpp:136] Iteration 11900, lr = 0.0001, m = 0.9
I0815 19:47:55.680375 20887 solver.cpp:509] Iteration 12000, Testing net (#0)
I0815 19:48:07.510062 20869 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 19:48:08.007380 20887 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.943812
I0815 19:48:08.007405 20887 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999996
I0815 19:48:08.007410 20887 solver.cpp:594]     Test net output #2: loss = 0.154984 (* 1 = 0.154984 loss)
I0815 19:48:08.007501 20887 solver.cpp:264] [MultiGPU] Tests completed in 12.3268s
I0815 19:48:08.208499 20887 solver.cpp:312] Iteration 12000 (3.15996 iter/s, 31.646s/100 iter), loss = 0.0608578
I0815 19:48:08.208526 20887 solver.cpp:334]     Train net output #0: loss = 0.0608576 (* 1 = 0.0608576 loss)
I0815 19:48:08.208533 20887 sgd_solver.cpp:136] Iteration 12000, lr = 0.0001, m = 0.9
I0815 19:48:20.365422 20830 data_reader.cpp:288] Starting prefetch of epoch 8
I0815 19:48:27.622198 20887 solver.cpp:312] Iteration 12100 (5.15114 iter/s, 19.4132s/100 iter), loss = 0.140818
I0815 19:48:27.622218 20887 solver.cpp:334]     Train net output #0: loss = 0.140818 (* 1 = 0.140818 loss)
I0815 19:48:27.622222 20887 sgd_solver.cpp:136] Iteration 12100, lr = 0.0001, m = 0.9
I0815 19:48:46.895294 20887 solver.cpp:312] Iteration 12200 (5.18872 iter/s, 19.2726s/100 iter), loss = 0.135253
I0815 19:48:46.895395 20887 solver.cpp:334]     Train net output #0: loss = 0.135253 (* 1 = 0.135253 loss)
I0815 19:48:46.895416 20887 sgd_solver.cpp:136] Iteration 12200, lr = 0.0001, m = 0.9
I0815 19:49:06.376462 20887 solver.cpp:312] Iteration 12300 (5.1333 iter/s, 19.4806s/100 iter), loss = 0.0997504
I0815 19:49:06.376695 20887 solver.cpp:334]     Train net output #0: loss = 0.0997502 (* 1 = 0.0997502 loss)
I0815 19:49:06.376824 20887 sgd_solver.cpp:136] Iteration 12300, lr = 0.0001, m = 0.9
I0815 19:49:24.417132 20890 data_reader.cpp:288] Starting prefetch of epoch 8
I0815 19:49:25.851806 20887 solver.cpp:312] Iteration 12400 (5.13484 iter/s, 19.4748s/100 iter), loss = 0.0787297
I0815 19:49:25.851830 20887 solver.cpp:334]     Train net output #0: loss = 0.0787295 (* 1 = 0.0787295 loss)
I0815 19:49:25.851833 20887 sgd_solver.cpp:136] Iteration 12400, lr = 0.0001, m = 0.9
I0815 19:49:45.183172 20887 solver.cpp:312] Iteration 12500 (5.17308 iter/s, 19.3308s/100 iter), loss = 0.0938057
I0815 19:49:45.183199 20887 solver.cpp:334]     Train net output #0: loss = 0.0938055 (* 1 = 0.0938055 loss)
I0815 19:49:45.183205 20887 sgd_solver.cpp:136] Iteration 12500, lr = 0.0001, m = 0.9
I0815 19:50:04.847785 20887 solver.cpp:312] Iteration 12600 (5.08542 iter/s, 19.6641s/100 iter), loss = 0.111992
I0815 19:50:04.847832 20887 solver.cpp:334]     Train net output #0: loss = 0.111992 (* 1 = 0.111992 loss)
I0815 19:50:04.847839 20887 sgd_solver.cpp:136] Iteration 12600, lr = 0.0001, m = 0.9
I0815 19:50:24.561409 20887 solver.cpp:312] Iteration 12700 (5.07277 iter/s, 19.7131s/100 iter), loss = 0.252726
I0815 19:50:24.561434 20887 solver.cpp:334]     Train net output #0: loss = 0.252726 (* 1 = 0.252726 loss)
I0815 19:50:24.561439 20887 sgd_solver.cpp:136] Iteration 12700, lr = 0.0001, m = 0.9
I0815 19:50:28.994598 20893 data_reader.cpp:288] Starting prefetch of epoch 10
I0815 19:50:44.092450 20887 solver.cpp:312] Iteration 12800 (5.1202 iter/s, 19.5305s/100 iter), loss = 0.0799723
I0815 19:50:44.092501 20887 solver.cpp:334]     Train net output #0: loss = 0.079972 (* 1 = 0.079972 loss)
I0815 19:50:44.092509 20887 sgd_solver.cpp:136] Iteration 12800, lr = 0.0001, m = 0.9
I0815 19:51:01.407833 20830 data_reader.cpp:288] Starting prefetch of epoch 9
I0815 19:51:03.696179 20887 solver.cpp:312] Iteration 12900 (5.10121 iter/s, 19.6032s/100 iter), loss = 0.10406
I0815 19:51:03.696200 20887 solver.cpp:334]     Train net output #0: loss = 0.10406 (* 1 = 0.10406 loss)
I0815 19:51:03.696207 20887 sgd_solver.cpp:136] Iteration 12900, lr = 0.0001, m = 0.9
I0815 19:51:23.191540 20887 solver.cpp:312] Iteration 13000 (5.12957 iter/s, 19.4948s/100 iter), loss = 0.0810247
I0815 19:51:23.191596 20887 solver.cpp:334]     Train net output #0: loss = 0.0810244 (* 1 = 0.0810244 loss)
I0815 19:51:23.191601 20887 sgd_solver.cpp:136] Iteration 13000, lr = 0.0001, m = 0.9
I0815 19:51:42.322966 20887 solver.cpp:312] Iteration 13100 (5.22714 iter/s, 19.1309s/100 iter), loss = 0.18332
I0815 19:51:42.322988 20887 solver.cpp:334]     Train net output #0: loss = 0.183319 (* 1 = 0.183319 loss)
I0815 19:51:42.322993 20887 sgd_solver.cpp:136] Iteration 13100, lr = 0.0001, m = 0.9
I0815 19:52:01.582149 20887 solver.cpp:312] Iteration 13200 (5.19247 iter/s, 19.2587s/100 iter), loss = 0.0812014
I0815 19:52:01.582201 20887 solver.cpp:334]     Train net output #0: loss = 0.0812012 (* 1 = 0.0812012 loss)
I0815 19:52:01.582206 20887 sgd_solver.cpp:136] Iteration 13200, lr = 0.0001, m = 0.9
I0815 19:52:05.272518 20891 data_reader.cpp:288] Starting prefetch of epoch 7
I0815 19:52:20.898071 20887 solver.cpp:312] Iteration 13300 (5.17722 iter/s, 19.3154s/100 iter), loss = 0.0869071
I0815 19:52:20.898092 20887 solver.cpp:334]     Train net output #0: loss = 0.0869069 (* 1 = 0.0869069 loss)
I0815 19:52:20.898095 20887 sgd_solver.cpp:136] Iteration 13300, lr = 0.0001, m = 0.9
I0815 19:52:40.443697 20887 solver.cpp:312] Iteration 13400 (5.11637 iter/s, 19.5451s/100 iter), loss = 0.0825462
I0815 19:52:40.443755 20887 solver.cpp:334]     Train net output #0: loss = 0.082546 (* 1 = 0.082546 loss)
I0815 19:52:40.443763 20887 sgd_solver.cpp:136] Iteration 13400, lr = 0.0001, m = 0.9
I0815 19:53:00.014734 20887 solver.cpp:312] Iteration 13500 (5.10973 iter/s, 19.5705s/100 iter), loss = 0.136757
I0815 19:53:00.014758 20887 solver.cpp:334]     Train net output #0: loss = 0.136757 (* 1 = 0.136757 loss)
I0815 19:53:00.014765 20887 sgd_solver.cpp:136] Iteration 13500, lr = 0.0001, m = 0.9
I0815 19:53:09.531096 20893 data_reader.cpp:288] Starting prefetch of epoch 11
I0815 19:53:19.399896 20887 solver.cpp:312] Iteration 13600 (5.15873 iter/s, 19.3846s/100 iter), loss = 0.112086
I0815 19:53:19.399973 20887 solver.cpp:334]     Train net output #0: loss = 0.112086 (* 1 = 0.112086 loss)
I0815 19:53:19.399981 20887 sgd_solver.cpp:136] Iteration 13600, lr = 0.0001, m = 0.9
I0815 19:53:38.700660 20887 solver.cpp:312] Iteration 13700 (5.18128 iter/s, 19.3002s/100 iter), loss = 0.0837837
I0815 19:53:38.700685 20887 solver.cpp:334]     Train net output #0: loss = 0.0837835 (* 1 = 0.0837835 loss)
I0815 19:53:38.700692 20887 sgd_solver.cpp:136] Iteration 13700, lr = 0.0001, m = 0.9
I0815 19:53:41.647369 20890 data_reader.cpp:288] Starting prefetch of epoch 9
I0815 19:53:57.985699 20887 solver.cpp:312] Iteration 13800 (5.18551 iter/s, 19.2845s/100 iter), loss = 0.104896
I0815 19:53:57.985746 20887 solver.cpp:334]     Train net output #0: loss = 0.104896 (* 1 = 0.104896 loss)
I0815 19:53:57.985752 20887 sgd_solver.cpp:136] Iteration 13800, lr = 0.0001, m = 0.9
I0815 19:54:17.465381 20887 solver.cpp:312] Iteration 13900 (5.13369 iter/s, 19.4791s/100 iter), loss = 0.110906
I0815 19:54:17.465407 20887 solver.cpp:334]     Train net output #0: loss = 0.110906 (* 1 = 0.110906 loss)
I0815 19:54:17.465414 20887 sgd_solver.cpp:136] Iteration 13900, lr = 0.0001, m = 0.9
I0815 19:54:36.827085 20887 solver.cpp:509] Iteration 14000, Testing net (#0)
I0815 19:54:44.533480 20933 data_reader.cpp:288] Starting prefetch of epoch 1
I0815 19:54:48.995455 20887 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.947668
I0815 19:54:48.995477 20887 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999995
I0815 19:54:48.995483 20887 solver.cpp:594]     Test net output #2: loss = 0.155081 (* 1 = 0.155081 loss)
I0815 19:54:48.995676 20887 solver.cpp:264] [MultiGPU] Tests completed in 12.1683s
I0815 19:54:49.206209 20887 solver.cpp:312] Iteration 14000 (3.1506 iter/s, 31.74s/100 iter), loss = 0.131852
I0815 19:54:49.206236 20887 solver.cpp:334]     Train net output #0: loss = 0.131851 (* 1 = 0.131851 loss)
I0815 19:54:49.206243 20887 sgd_solver.cpp:136] Iteration 14000, lr = 0.0001, m = 0.9
I0815 19:55:08.798252 20887 solver.cpp:312] Iteration 14100 (5.10425 iter/s, 19.5915s/100 iter), loss = 0.145497
I0815 19:55:08.798303 20887 solver.cpp:334]     Train net output #0: loss = 0.145497 (* 1 = 0.145497 loss)
I0815 19:55:08.798308 20887 sgd_solver.cpp:136] Iteration 14100, lr = 0.0001, m = 0.9
I0815 19:55:28.305280 20887 solver.cpp:312] Iteration 14200 (5.1265 iter/s, 19.5065s/100 iter), loss = 0.100638
I0815 19:55:28.305305 20887 solver.cpp:334]     Train net output #0: loss = 0.100637 (* 1 = 0.100637 loss)
I0815 19:55:28.305311 20887 sgd_solver.cpp:136] Iteration 14200, lr = 0.0001, m = 0.9
I0815 19:55:30.319392 20893 data_reader.cpp:288] Starting prefetch of epoch 12
I0815 19:55:48.038604 20887 solver.cpp:312] Iteration 14300 (5.06771 iter/s, 19.7328s/100 iter), loss = 0.162185
I0815 19:55:48.038657 20887 solver.cpp:334]     Train net output #0: loss = 0.162184 (* 1 = 0.162184 loss)
I0815 19:55:48.038663 20887 sgd_solver.cpp:136] Iteration 14300, lr = 0.0001, m = 0.9
I0815 19:56:02.806126 20891 data_reader.cpp:288] Starting prefetch of epoch 8
I0815 19:56:07.332414 20887 solver.cpp:312] Iteration 14400 (5.18315 iter/s, 19.2933s/100 iter), loss = 0.101411
I0815 19:56:07.332439 20887 solver.cpp:334]     Train net output #0: loss = 0.101411 (* 1 = 0.101411 loss)
I0815 19:56:07.332444 20887 sgd_solver.cpp:136] Iteration 14400, lr = 0.0001, m = 0.9
I0815 19:56:26.857587 20887 solver.cpp:312] Iteration 14500 (5.12174 iter/s, 19.5246s/100 iter), loss = 0.105859
I0815 19:56:26.857640 20887 solver.cpp:334]     Train net output #0: loss = 0.105858 (* 1 = 0.105858 loss)
I0815 19:56:26.857646 20887 sgd_solver.cpp:136] Iteration 14500, lr = 0.0001, m = 0.9
I0815 19:56:46.193773 20887 solver.cpp:312] Iteration 14600 (5.17179 iter/s, 19.3356s/100 iter), loss = 0.0974646
I0815 19:56:46.193794 20887 solver.cpp:334]     Train net output #0: loss = 0.0974644 (* 1 = 0.0974644 loss)
I0815 19:56:46.193800 20887 sgd_solver.cpp:136] Iteration 14600, lr = 0.0001, m = 0.9
I0815 19:57:05.698410 20887 solver.cpp:312] Iteration 14700 (5.12713 iter/s, 19.5041s/100 iter), loss = 0.175828
I0815 19:57:05.698475 20887 solver.cpp:334]     Train net output #0: loss = 0.175828 (* 1 = 0.175828 loss)
I0815 19:57:05.698482 20887 sgd_solver.cpp:136] Iteration 14700, lr = 0.0001, m = 0.9
I0815 19:57:06.852655 20895 data_reader.cpp:288] Starting prefetch of epoch 6
I0815 19:57:24.907562 20887 solver.cpp:312] Iteration 14800 (5.20599 iter/s, 19.2086s/100 iter), loss = 0.0992714
I0815 19:57:24.907586 20887 solver.cpp:334]     Train net output #0: loss = 0.0992712 (* 1 = 0.0992712 loss)
I0815 19:57:24.907593 20887 sgd_solver.cpp:136] Iteration 14800, lr = 0.0001, m = 0.9
I0815 19:57:44.334313 20887 solver.cpp:312] Iteration 14900 (5.14768 iter/s, 19.4262s/100 iter), loss = 0.178289
I0815 19:57:44.334365 20887 solver.cpp:334]     Train net output #0: loss = 0.178289 (* 1 = 0.178289 loss)
I0815 19:57:44.334374 20887 sgd_solver.cpp:136] Iteration 14900, lr = 0.0001, m = 0.9
I0815 19:58:04.063727 20887 solver.cpp:312] Iteration 15000 (5.06871 iter/s, 19.7289s/100 iter), loss = 0.0919131
I0815 19:58:04.063751 20887 solver.cpp:334]     Train net output #0: loss = 0.0919129 (* 1 = 0.0919129 loss)
I0815 19:58:04.063758 20887 sgd_solver.cpp:136] Iteration 15000, lr = 0.0001, m = 0.9
I0815 19:58:11.147490 20841 data_reader.cpp:288] Starting prefetch of epoch 9
I0815 19:58:23.408269 20887 solver.cpp:312] Iteration 15100 (5.16956 iter/s, 19.344s/100 iter), loss = 0.193135
I0815 19:58:23.408315 20887 solver.cpp:334]     Train net output #0: loss = 0.193134 (* 1 = 0.193134 loss)
I0815 19:58:23.408323 20887 sgd_solver.cpp:136] Iteration 15100, lr = 0.0001, m = 0.9
I0815 19:58:42.645319 20887 solver.cpp:312] Iteration 15200 (5.19845 iter/s, 19.2365s/100 iter), loss = 0.105367
I0815 19:58:42.645344 20887 solver.cpp:334]     Train net output #0: loss = 0.105367 (* 1 = 0.105367 loss)
I0815 19:58:42.645349 20887 sgd_solver.cpp:136] Iteration 15200, lr = 0.0001, m = 0.9
I0815 19:59:02.347156 20887 solver.cpp:312] Iteration 15300 (5.07581 iter/s, 19.7013s/100 iter), loss = 0.0494174
I0815 19:59:02.347208 20887 solver.cpp:334]     Train net output #0: loss = 0.0494172 (* 1 = 0.0494172 loss)
I0815 19:59:02.347216 20887 sgd_solver.cpp:136] Iteration 15300, lr = 0.0001, m = 0.9
I0815 19:59:15.591271 20893 data_reader.cpp:288] Starting prefetch of epoch 13
I0815 19:59:21.964874 20887 solver.cpp:312] Iteration 15400 (5.09757 iter/s, 19.6172s/100 iter), loss = 0.107777
I0815 19:59:21.964901 20887 solver.cpp:334]     Train net output #0: loss = 0.107777 (* 1 = 0.107777 loss)
I0815 19:59:21.964907 20887 sgd_solver.cpp:136] Iteration 15400, lr = 0.0001, m = 0.9
I0815 19:59:41.566017 20887 solver.cpp:312] Iteration 15500 (5.10188 iter/s, 19.6006s/100 iter), loss = 0.0936631
I0815 19:59:41.566061 20887 solver.cpp:334]     Train net output #0: loss = 0.093663 (* 1 = 0.093663 loss)
I0815 19:59:41.566068 20887 sgd_solver.cpp:136] Iteration 15500, lr = 0.0001, m = 0.9
I0815 19:59:48.088066 20830 data_reader.cpp:288] Starting prefetch of epoch 10
I0815 20:00:01.018204 20887 solver.cpp:312] Iteration 15600 (5.14095 iter/s, 19.4517s/100 iter), loss = 0.0937205
I0815 20:00:01.018225 20887 solver.cpp:334]     Train net output #0: loss = 0.0937203 (* 1 = 0.0937203 loss)
I0815 20:00:01.018230 20887 sgd_solver.cpp:136] Iteration 15600, lr = 0.0001, m = 0.9
I0815 20:00:20.601639 20887 solver.cpp:312] Iteration 15700 (5.1065 iter/s, 19.5829s/100 iter), loss = 0.119206
I0815 20:00:20.601686 20887 solver.cpp:334]     Train net output #0: loss = 0.119206 (* 1 = 0.119206 loss)
I0815 20:00:20.601693 20887 sgd_solver.cpp:136] Iteration 15700, lr = 0.0001, m = 0.9
I0815 20:00:40.243232 20887 solver.cpp:312] Iteration 15800 (5.09138 iter/s, 19.641s/100 iter), loss = 0.113153
I0815 20:00:40.243261 20887 solver.cpp:334]     Train net output #0: loss = 0.113152 (* 1 = 0.113152 loss)
I0815 20:00:40.243268 20887 sgd_solver.cpp:136] Iteration 15800, lr = 0.0001, m = 0.9
I0815 20:00:52.459815 20841 data_reader.cpp:288] Starting prefetch of epoch 10
I0815 20:00:59.581045 20887 solver.cpp:312] Iteration 15900 (5.17136 iter/s, 19.3373s/100 iter), loss = 0.0680901
I0815 20:00:59.581069 20887 solver.cpp:334]     Train net output #0: loss = 0.0680899 (* 1 = 0.0680899 loss)
I0815 20:00:59.581076 20887 sgd_solver.cpp:136] Iteration 15900, lr = 0.0001, m = 0.9
I0815 20:01:18.623725 20887 solver.cpp:509] Iteration 16000, Testing net (#0)
I0815 20:01:29.363675 20885 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 20:01:30.078336 20887 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.948013
I0815 20:01:30.078356 20887 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999998
I0815 20:01:30.078363 20887 solver.cpp:594]     Test net output #2: loss = 0.1515 (* 1 = 0.1515 loss)
I0815 20:01:30.078444 20887 solver.cpp:264] [MultiGPU] Tests completed in 11.4544s
I0815 20:01:30.280939 20887 solver.cpp:312] Iteration 16000 (3.25743 iter/s, 30.6991s/100 iter), loss = 0.112008
I0815 20:01:30.280964 20887 solver.cpp:334]     Train net output #0: loss = 0.112008 (* 1 = 0.112008 loss)
I0815 20:01:30.280971 20887 sgd_solver.cpp:136] Iteration 16000, lr = 0.0001, m = 0.9
I0815 20:01:35.665576 20830 data_reader.cpp:288] Starting prefetch of epoch 11
I0815 20:01:49.707916 20887 solver.cpp:312] Iteration 16100 (5.14762 iter/s, 19.4264s/100 iter), loss = 0.111454
I0815 20:01:49.707942 20887 solver.cpp:334]     Train net output #0: loss = 0.111453 (* 1 = 0.111453 loss)
I0815 20:01:49.707948 20887 sgd_solver.cpp:136] Iteration 16100, lr = 0.0001, m = 0.9
I0815 20:02:09.144367 20887 solver.cpp:312] Iteration 16200 (5.14511 iter/s, 19.4359s/100 iter), loss = 0.1521
I0815 20:02:09.144413 20887 solver.cpp:334]     Train net output #0: loss = 0.1521 (* 1 = 0.1521 loss)
I0815 20:02:09.144419 20887 sgd_solver.cpp:136] Iteration 16200, lr = 0.0001, m = 0.9
I0815 20:02:28.706775 20887 solver.cpp:312] Iteration 16300 (5.11199 iter/s, 19.5619s/100 iter), loss = 0.097242
I0815 20:02:28.706799 20887 solver.cpp:334]     Train net output #0: loss = 0.0972418 (* 1 = 0.0972418 loss)
I0815 20:02:28.706804 20887 sgd_solver.cpp:136] Iteration 16300, lr = 0.0001, m = 0.9
I0815 20:02:40.159482 20841 data_reader.cpp:288] Starting prefetch of epoch 11
I0815 20:02:48.101294 20887 solver.cpp:312] Iteration 16400 (5.15624 iter/s, 19.394s/100 iter), loss = 0.0689483
I0815 20:02:48.101315 20887 solver.cpp:334]     Train net output #0: loss = 0.0689482 (* 1 = 0.0689482 loss)
I0815 20:02:48.101320 20887 sgd_solver.cpp:136] Iteration 16400, lr = 0.0001, m = 0.9
I0815 20:03:07.524394 20887 solver.cpp:312] Iteration 16500 (5.14865 iter/s, 19.4226s/100 iter), loss = 0.0893815
I0815 20:03:07.524423 20887 solver.cpp:334]     Train net output #0: loss = 0.0893814 (* 1 = 0.0893814 loss)
I0815 20:03:07.524430 20887 sgd_solver.cpp:136] Iteration 16500, lr = 0.0001, m = 0.9
I0815 20:03:27.080693 20887 solver.cpp:312] Iteration 16600 (5.11358 iter/s, 19.5558s/100 iter), loss = 0.0923838
I0815 20:03:27.080737 20887 solver.cpp:334]     Train net output #0: loss = 0.0923837 (* 1 = 0.0923837 loss)
I0815 20:03:27.080744 20887 sgd_solver.cpp:136] Iteration 16600, lr = 0.0001, m = 0.9
I0815 20:03:44.686127 20893 data_reader.cpp:288] Starting prefetch of epoch 14
I0815 20:03:46.641327 20887 solver.cpp:312] Iteration 16700 (5.11245 iter/s, 19.5601s/100 iter), loss = 0.278854
I0815 20:03:46.641358 20887 solver.cpp:334]     Train net output #0: loss = 0.278854 (* 1 = 0.278854 loss)
I0815 20:03:46.641364 20887 sgd_solver.cpp:136] Iteration 16700, lr = 0.0001, m = 0.9
I0815 20:04:06.117625 20887 solver.cpp:312] Iteration 16800 (5.13459 iter/s, 19.4758s/100 iter), loss = 0.11335
I0815 20:04:06.117671 20887 solver.cpp:334]     Train net output #0: loss = 0.113349 (* 1 = 0.113349 loss)
I0815 20:04:06.117677 20887 sgd_solver.cpp:136] Iteration 16800, lr = 0.0001, m = 0.9
I0815 20:04:16.839162 20891 data_reader.cpp:288] Starting prefetch of epoch 9
I0815 20:04:25.353484 20887 solver.cpp:312] Iteration 16900 (5.19877 iter/s, 19.2353s/100 iter), loss = 0.097402
I0815 20:04:25.353513 20887 solver.cpp:334]     Train net output #0: loss = 0.0974018 (* 1 = 0.0974018 loss)
I0815 20:04:25.353518 20887 sgd_solver.cpp:136] Iteration 16900, lr = 0.0001, m = 0.9
I0815 20:04:44.864217 20887 solver.cpp:312] Iteration 17000 (5.12552 iter/s, 19.5102s/100 iter), loss = 0.148773
I0815 20:04:44.864281 20887 solver.cpp:334]     Train net output #0: loss = 0.148772 (* 1 = 0.148772 loss)
I0815 20:04:44.864286 20887 sgd_solver.cpp:136] Iteration 17000, lr = 0.0001, m = 0.9
I0815 20:05:04.353549 20887 solver.cpp:312] Iteration 17100 (5.13115 iter/s, 19.4888s/100 iter), loss = 0.121164
I0815 20:05:04.353572 20887 solver.cpp:334]     Train net output #0: loss = 0.121164 (* 1 = 0.121164 loss)
I0815 20:05:04.353579 20887 sgd_solver.cpp:136] Iteration 17100, lr = 0.0001, m = 0.9
I0815 20:05:21.132386 20890 data_reader.cpp:288] Starting prefetch of epoch 10
I0815 20:05:24.020932 20887 solver.cpp:312] Iteration 17200 (5.0847 iter/s, 19.6668s/100 iter), loss = 0.12643
I0815 20:05:24.020959 20887 solver.cpp:334]     Train net output #0: loss = 0.12643 (* 1 = 0.12643 loss)
I0815 20:05:24.020967 20887 sgd_solver.cpp:136] Iteration 17200, lr = 0.0001, m = 0.9
I0815 20:05:43.629972 20887 solver.cpp:312] Iteration 17300 (5.09983 iter/s, 19.6085s/100 iter), loss = 0.0779089
I0815 20:05:43.630002 20887 solver.cpp:334]     Train net output #0: loss = 0.0779087 (* 1 = 0.0779087 loss)
I0815 20:05:43.630007 20887 sgd_solver.cpp:136] Iteration 17300, lr = 0.0001, m = 0.9
I0815 20:06:03.305611 20887 solver.cpp:312] Iteration 17400 (5.08257 iter/s, 19.6751s/100 iter), loss = 0.0663688
I0815 20:06:03.305665 20887 solver.cpp:334]     Train net output #0: loss = 0.0663686 (* 1 = 0.0663686 loss)
I0815 20:06:03.305670 20887 sgd_solver.cpp:136] Iteration 17400, lr = 0.0001, m = 0.9
I0815 20:06:22.790623 20887 solver.cpp:312] Iteration 17500 (5.13229 iter/s, 19.4845s/100 iter), loss = 0.0645475
I0815 20:06:22.790645 20887 solver.cpp:334]     Train net output #0: loss = 0.0645473 (* 1 = 0.0645473 loss)
I0815 20:06:22.790649 20887 sgd_solver.cpp:136] Iteration 17500, lr = 0.0001, m = 0.9
I0815 20:06:25.947602 20841 data_reader.cpp:288] Starting prefetch of epoch 12
I0815 20:06:42.135861 20887 solver.cpp:312] Iteration 17600 (5.16937 iter/s, 19.3447s/100 iter), loss = 0.188118
I0815 20:06:42.135911 20887 solver.cpp:334]     Train net output #0: loss = 0.188117 (* 1 = 0.188117 loss)
I0815 20:06:42.135916 20887 sgd_solver.cpp:136] Iteration 17600, lr = 0.0001, m = 0.9
I0815 20:06:57.822511 20830 data_reader.cpp:288] Starting prefetch of epoch 12
I0815 20:07:01.445495 20887 solver.cpp:312] Iteration 17700 (5.1789 iter/s, 19.3091s/100 iter), loss = 0.0839225
I0815 20:07:01.445516 20887 solver.cpp:334]     Train net output #0: loss = 0.0839223 (* 1 = 0.0839223 loss)
I0815 20:07:01.445520 20887 sgd_solver.cpp:136] Iteration 17700, lr = 0.0001, m = 0.9
I0815 20:07:20.699311 20887 solver.cpp:312] Iteration 17800 (5.19392 iter/s, 19.2533s/100 iter), loss = 0.0822247
I0815 20:07:20.699369 20887 solver.cpp:334]     Train net output #0: loss = 0.0822245 (* 1 = 0.0822245 loss)
I0815 20:07:20.699378 20887 sgd_solver.cpp:136] Iteration 17800, lr = 0.0001, m = 0.9
I0815 20:07:40.284270 20887 solver.cpp:312] Iteration 17900 (5.1061 iter/s, 19.5844s/100 iter), loss = 0.120198
I0815 20:07:40.284299 20887 solver.cpp:334]     Train net output #0: loss = 0.120198 (* 1 = 0.120198 loss)
I0815 20:07:40.284303 20887 sgd_solver.cpp:136] Iteration 17900, lr = 0.0001, m = 0.9
I0815 20:07:59.368239 20887 solver.cpp:509] Iteration 18000, Testing net (#0)
I0815 20:08:06.848153 20937 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 20:08:11.124701 20887 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.945952
I0815 20:08:11.124724 20887 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999882
I0815 20:08:11.124730 20887 solver.cpp:594]     Test net output #2: loss = 0.174745 (* 1 = 0.174745 loss)
I0815 20:08:11.124822 20887 solver.cpp:264] [MultiGPU] Tests completed in 11.7563s
I0815 20:08:11.335078 20887 solver.cpp:312] Iteration 18000 (3.22062 iter/s, 31.05s/100 iter), loss = 0.0976373
I0815 20:08:11.335105 20887 solver.cpp:334]     Train net output #0: loss = 0.0976371 (* 1 = 0.0976371 loss)
I0815 20:08:11.335111 20887 sgd_solver.cpp:136] Iteration 18000, lr = 0.0001, m = 0.9
I0815 20:08:30.528764 20887 solver.cpp:312] Iteration 18100 (5.21019 iter/s, 19.1932s/100 iter), loss = 0.109155
I0815 20:08:30.530510 20887 solver.cpp:334]     Train net output #0: loss = 0.109155 (* 1 = 0.109155 loss)
I0815 20:08:30.530539 20887 sgd_solver.cpp:136] Iteration 18100, lr = 0.0001, m = 0.9
I0815 20:08:45.702811 20841 data_reader.cpp:288] Starting prefetch of epoch 13
I0815 20:08:50.216902 20887 solver.cpp:312] Iteration 18200 (5.07934 iter/s, 19.6876s/100 iter), loss = 0.121216
I0815 20:08:50.216925 20887 solver.cpp:334]     Train net output #0: loss = 0.121216 (* 1 = 0.121216 loss)
I0815 20:08:50.216930 20887 sgd_solver.cpp:136] Iteration 18200, lr = 0.0001, m = 0.9
I0815 20:09:09.819298 20887 solver.cpp:312] Iteration 18300 (5.10156 iter/s, 19.6019s/100 iter), loss = 0.142155
I0815 20:09:09.819372 20887 solver.cpp:334]     Train net output #0: loss = 0.142155 (* 1 = 0.142155 loss)
I0815 20:09:09.819380 20887 sgd_solver.cpp:136] Iteration 18300, lr = 0.0001, m = 0.9
I0815 20:09:18.141571 20891 data_reader.cpp:288] Starting prefetch of epoch 10
I0815 20:09:29.401952 20887 solver.cpp:312] Iteration 18400 (5.1067 iter/s, 19.5821s/100 iter), loss = 0.0906389
I0815 20:09:29.401976 20887 solver.cpp:334]     Train net output #0: loss = 0.0906388 (* 1 = 0.0906388 loss)
I0815 20:09:29.401980 20887 sgd_solver.cpp:136] Iteration 18400, lr = 0.0001, m = 0.9
I0815 20:09:49.022663 20887 solver.cpp:312] Iteration 18500 (5.0968 iter/s, 19.6202s/100 iter), loss = 0.167884
I0815 20:09:49.023308 20887 solver.cpp:334]     Train net output #0: loss = 0.167884 (* 1 = 0.167884 loss)
I0815 20:09:49.023327 20887 sgd_solver.cpp:136] Iteration 18500, lr = 0.0001, m = 0.9
I0815 20:10:08.511674 20887 solver.cpp:312] Iteration 18600 (5.13124 iter/s, 19.4885s/100 iter), loss = 0.119794
I0815 20:10:08.511703 20887 solver.cpp:334]     Train net output #0: loss = 0.119793 (* 1 = 0.119793 loss)
I0815 20:10:08.511706 20887 sgd_solver.cpp:136] Iteration 18600, lr = 0.0001, m = 0.9
I0815 20:10:22.866700 20890 data_reader.cpp:288] Starting prefetch of epoch 11
I0815 20:10:28.082607 20887 solver.cpp:312] Iteration 18700 (5.10976 iter/s, 19.5704s/100 iter), loss = 0.0585748
I0815 20:10:28.082628 20887 solver.cpp:334]     Train net output #0: loss = 0.0585746 (* 1 = 0.0585746 loss)
I0815 20:10:28.082633 20887 sgd_solver.cpp:136] Iteration 18700, lr = 0.0001, m = 0.9
I0815 20:10:47.436853 20887 solver.cpp:312] Iteration 18800 (5.16697 iter/s, 19.3537s/100 iter), loss = 0.25228
I0815 20:10:47.436878 20887 solver.cpp:334]     Train net output #0: loss = 0.252279 (* 1 = 0.252279 loss)
I0815 20:10:47.436884 20887 sgd_solver.cpp:136] Iteration 18800, lr = 0.0001, m = 0.9
I0815 20:11:06.876497 20887 solver.cpp:312] Iteration 18900 (5.14427 iter/s, 19.4391s/100 iter), loss = 0.117324
I0815 20:11:06.876577 20887 solver.cpp:334]     Train net output #0: loss = 0.117324 (* 1 = 0.117324 loss)
I0815 20:11:06.876585 20887 sgd_solver.cpp:136] Iteration 18900, lr = 0.0001, m = 0.9
I0815 20:11:26.258443 20887 solver.cpp:312] Iteration 19000 (5.15958 iter/s, 19.3814s/100 iter), loss = 0.214046
I0815 20:11:26.258462 20887 solver.cpp:334]     Train net output #0: loss = 0.214046 (* 1 = 0.214046 loss)
I0815 20:11:26.258467 20887 sgd_solver.cpp:136] Iteration 19000, lr = 0.0001, m = 0.9
I0815 20:11:26.861572 20890 data_reader.cpp:288] Starting prefetch of epoch 12
I0815 20:11:45.644176 20887 solver.cpp:312] Iteration 19100 (5.15858 iter/s, 19.3852s/100 iter), loss = 0.0713238
I0815 20:11:45.644225 20887 solver.cpp:334]     Train net output #0: loss = 0.0713236 (* 1 = 0.0713236 loss)
I0815 20:11:45.644230 20887 sgd_solver.cpp:136] Iteration 19100, lr = 0.0001, m = 0.9
I0815 20:11:59.076792 20891 data_reader.cpp:288] Starting prefetch of epoch 11
I0815 20:12:05.027456 20887 solver.cpp:312] Iteration 19200 (5.15923 iter/s, 19.3827s/100 iter), loss = 0.123743
I0815 20:12:05.027479 20887 solver.cpp:334]     Train net output #0: loss = 0.123743 (* 1 = 0.123743 loss)
I0815 20:12:05.027485 20887 sgd_solver.cpp:136] Iteration 19200, lr = 0.0001, m = 0.9
I0815 20:12:24.204676 20887 solver.cpp:312] Iteration 19300 (5.21466 iter/s, 19.1767s/100 iter), loss = 0.125956
I0815 20:12:24.204746 20887 solver.cpp:334]     Train net output #0: loss = 0.125955 (* 1 = 0.125955 loss)
I0815 20:12:24.204751 20887 sgd_solver.cpp:136] Iteration 19300, lr = 0.0001, m = 0.9
I0815 20:12:43.657680 20887 solver.cpp:312] Iteration 19400 (5.14074 iter/s, 19.4525s/100 iter), loss = 0.121584
I0815 20:12:43.657709 20887 solver.cpp:334]     Train net output #0: loss = 0.121584 (* 1 = 0.121584 loss)
I0815 20:12:43.657716 20887 sgd_solver.cpp:136] Iteration 19400, lr = 0.0001, m = 0.9
I0815 20:13:02.750362 20890 data_reader.cpp:288] Starting prefetch of epoch 13
I0815 20:13:02.914042 20887 solver.cpp:312] Iteration 19500 (5.19323 iter/s, 19.2558s/100 iter), loss = 0.0895744
I0815 20:13:02.914067 20887 solver.cpp:334]     Train net output #0: loss = 0.0895742 (* 1 = 0.0895742 loss)
I0815 20:13:02.914072 20887 sgd_solver.cpp:136] Iteration 19500, lr = 0.0001, m = 0.9
I0815 20:13:22.229951 20887 solver.cpp:312] Iteration 19600 (5.17722 iter/s, 19.3154s/100 iter), loss = 0.0977333
I0815 20:13:22.229979 20887 solver.cpp:334]     Train net output #0: loss = 0.0977331 (* 1 = 0.0977331 loss)
I0815 20:13:22.229986 20887 sgd_solver.cpp:136] Iteration 19600, lr = 0.0001, m = 0.9
I0815 20:13:41.852233 20887 solver.cpp:312] Iteration 19700 (5.09639 iter/s, 19.6217s/100 iter), loss = 0.816596
I0815 20:13:41.852298 20887 solver.cpp:334]     Train net output #0: loss = 0.816596 (* 1 = 0.816596 loss)
I0815 20:13:41.852304 20887 sgd_solver.cpp:136] Iteration 19700, lr = 0.0001, m = 0.9
I0815 20:14:01.161156 20887 solver.cpp:312] Iteration 19800 (5.1791 iter/s, 19.3084s/100 iter), loss = 0.0724904
I0815 20:14:01.161187 20887 solver.cpp:334]     Train net output #0: loss = 0.0724903 (* 1 = 0.0724903 loss)
I0815 20:14:01.161195 20887 sgd_solver.cpp:136] Iteration 19800, lr = 0.0001, m = 0.9
I0815 20:14:06.941290 20841 data_reader.cpp:288] Starting prefetch of epoch 14
I0815 20:14:20.620399 20887 solver.cpp:312] Iteration 19900 (5.13909 iter/s, 19.4587s/100 iter), loss = 0.0907687
I0815 20:14:20.620451 20887 solver.cpp:334]     Train net output #0: loss = 0.0907686 (* 1 = 0.0907686 loss)
I0815 20:14:20.620458 20887 sgd_solver.cpp:136] Iteration 19900, lr = 0.0001, m = 0.9
I0815 20:14:39.062800 20891 data_reader.cpp:288] Starting prefetch of epoch 12
I0815 20:14:39.822628 20887 solver.cpp:639] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/initial/cityscapes5_jsegnet21v2_iter_20000.caffemodel
I0815 20:14:39.914824 20887 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/initial/cityscapes5_jsegnet21v2_iter_20000.solverstate
I0815 20:14:39.922950 20887 solver.cpp:509] Iteration 20000, Testing net (#0)
I0815 20:14:51.099223 20933 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 20:14:51.579118 20887 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.948568
I0815 20:14:51.579141 20887 solver.cpp:594]     Test net output #1: accuracy/top5 = 1
I0815 20:14:51.579147 20887 solver.cpp:594]     Test net output #2: loss = 0.149101 (* 1 = 0.149101 loss)
I0815 20:14:51.579273 20887 solver.cpp:264] [MultiGPU] Tests completed in 11.656s
I0815 20:14:51.791076 20887 solver.cpp:312] Iteration 20000 (3.20823 iter/s, 31.1698s/100 iter), loss = 0.0764047
I0815 20:14:51.791103 20887 solver.cpp:334]     Train net output #0: loss = 0.0764045 (* 1 = 0.0764045 loss)
I0815 20:14:51.791107 20887 sgd_solver.cpp:136] Iteration 20000, lr = 0.0001, m = 0.9
I0815 20:15:11.250434 20887 solver.cpp:312] Iteration 20100 (5.13906 iter/s, 19.4588s/100 iter), loss = 0.0507235
I0815 20:15:11.250463 20887 solver.cpp:334]     Train net output #0: loss = 0.0507233 (* 1 = 0.0507233 loss)
I0815 20:15:11.250468 20887 sgd_solver.cpp:136] Iteration 20100, lr = 0.0001, m = 0.9
I0815 20:15:30.863703 20887 solver.cpp:312] Iteration 20200 (5.09873 iter/s, 19.6127s/100 iter), loss = 0.113502
I0815 20:15:30.863780 20887 solver.cpp:334]     Train net output #0: loss = 0.113502 (* 1 = 0.113502 loss)
I0815 20:15:30.863786 20887 sgd_solver.cpp:136] Iteration 20200, lr = 0.0001, m = 0.9
I0815 20:15:50.117161 20887 solver.cpp:312] Iteration 20300 (5.19402 iter/s, 19.2529s/100 iter), loss = 0.0818793
I0815 20:15:50.117185 20887 solver.cpp:334]     Train net output #0: loss = 0.0818792 (* 1 = 0.0818792 loss)
I0815 20:15:50.117192 20887 sgd_solver.cpp:136] Iteration 20300, lr = 0.0001, m = 0.9
I0815 20:15:55.258770 20890 data_reader.cpp:288] Starting prefetch of epoch 14
I0815 20:16:09.679267 20887 solver.cpp:312] Iteration 20400 (5.11207 iter/s, 19.5616s/100 iter), loss = 0.115193
I0815 20:16:09.679324 20887 solver.cpp:334]     Train net output #0: loss = 0.115193 (* 1 = 0.115193 loss)
I0815 20:16:09.679332 20887 sgd_solver.cpp:136] Iteration 20400, lr = 0.0001, m = 0.9
I0815 20:16:29.137717 20887 solver.cpp:312] Iteration 20500 (5.1393 iter/s, 19.4579s/100 iter), loss = 0.076128
I0815 20:16:29.137766 20887 solver.cpp:334]     Train net output #0: loss = 0.0761278 (* 1 = 0.0761278 loss)
I0815 20:16:29.137779 20887 sgd_solver.cpp:136] Iteration 20500, lr = 0.0001, m = 0.9
I0815 20:16:48.739567 20887 solver.cpp:312] Iteration 20600 (5.1017 iter/s, 19.6013s/100 iter), loss = 0.0848693
I0815 20:16:48.739657 20887 solver.cpp:334]     Train net output #0: loss = 0.0848692 (* 1 = 0.0848692 loss)
I0815 20:16:48.739678 20887 sgd_solver.cpp:136] Iteration 20600, lr = 0.0001, m = 0.9
I0815 20:16:59.641152 20895 data_reader.cpp:288] Starting prefetch of epoch 7
I0815 20:17:08.253819 20887 solver.cpp:312] Iteration 20700 (5.1246 iter/s, 19.5137s/100 iter), loss = 0.071831
I0815 20:17:08.253893 20887 solver.cpp:334]     Train net output #0: loss = 0.0718309 (* 1 = 0.0718309 loss)
I0815 20:17:08.253917 20887 sgd_solver.cpp:136] Iteration 20700, lr = 0.0001, m = 0.9
I0815 20:17:27.823114 20887 solver.cpp:312] Iteration 20800 (5.11019 iter/s, 19.5688s/100 iter), loss = 0.0770694
I0815 20:17:27.823160 20887 solver.cpp:334]     Train net output #0: loss = 0.0770692 (* 1 = 0.0770692 loss)
I0815 20:17:27.823168 20887 sgd_solver.cpp:136] Iteration 20800, lr = 0.0001, m = 0.9
I0815 20:17:31.977843 20830 data_reader.cpp:288] Starting prefetch of epoch 13
I0815 20:17:47.322957 20887 solver.cpp:312] Iteration 20900 (5.12839 iter/s, 19.4993s/100 iter), loss = 0.0606319
I0815 20:17:47.322980 20887 solver.cpp:334]     Train net output #0: loss = 0.0606317 (* 1 = 0.0606317 loss)
I0815 20:17:47.322984 20887 sgd_solver.cpp:136] Iteration 20900, lr = 0.0001, m = 0.9
I0815 20:18:06.822839 20887 solver.cpp:312] Iteration 21000 (5.12838 iter/s, 19.4993s/100 iter), loss = 0.154579
I0815 20:18:06.822888 20887 solver.cpp:334]     Train net output #0: loss = 0.154579 (* 1 = 0.154579 loss)
I0815 20:18:06.822896 20887 sgd_solver.cpp:136] Iteration 21000, lr = 0.0001, m = 0.9
I0815 20:18:26.223424 20887 solver.cpp:312] Iteration 21100 (5.15463 iter/s, 19.4s/100 iter), loss = 0.0927383
I0815 20:18:26.223453 20887 solver.cpp:334]     Train net output #0: loss = 0.0927381 (* 1 = 0.0927381 loss)
I0815 20:18:26.223459 20887 sgd_solver.cpp:136] Iteration 21100, lr = 0.0001, m = 0.9
I0815 20:18:36.209447 20841 data_reader.cpp:288] Starting prefetch of epoch 15
I0815 20:18:45.529616 20887 solver.cpp:312] Iteration 21200 (5.17983 iter/s, 19.3057s/100 iter), loss = 0.111547
I0815 20:18:45.529669 20887 solver.cpp:334]     Train net output #0: loss = 0.111546 (* 1 = 0.111546 loss)
I0815 20:18:45.529675 20887 sgd_solver.cpp:136] Iteration 21200, lr = 0.0001, m = 0.9
I0815 20:19:04.810256 20887 solver.cpp:312] Iteration 21300 (5.18669 iter/s, 19.2801s/100 iter), loss = 0.100896
I0815 20:19:04.810276 20887 solver.cpp:334]     Train net output #0: loss = 0.100896 (* 1 = 0.100896 loss)
I0815 20:19:04.810279 20887 sgd_solver.cpp:136] Iteration 21300, lr = 0.0001, m = 0.9
I0815 20:19:24.137325 20887 solver.cpp:312] Iteration 21400 (5.17423 iter/s, 19.3265s/100 iter), loss = 0.0804823
I0815 20:19:24.138638 20887 solver.cpp:334]     Train net output #0: loss = 0.0804822 (* 1 = 0.0804822 loss)
I0815 20:19:24.138646 20887 sgd_solver.cpp:136] Iteration 21400, lr = 0.0001, m = 0.9
I0815 20:19:40.171567 20895 data_reader.cpp:288] Starting prefetch of epoch 8
I0815 20:19:43.488165 20887 solver.cpp:312] Iteration 21500 (5.16788 iter/s, 19.3503s/100 iter), loss = 0.0966118
I0815 20:19:43.488191 20887 solver.cpp:334]     Train net output #0: loss = 0.0966117 (* 1 = 0.0966117 loss)
I0815 20:19:43.488195 20887 sgd_solver.cpp:136] Iteration 21500, lr = 0.0001, m = 0.9
I0815 20:20:02.958060 20887 solver.cpp:312] Iteration 21600 (5.13628 iter/s, 19.4694s/100 iter), loss = 0.117883
I0815 20:20:02.958106 20887 solver.cpp:334]     Train net output #0: loss = 0.117883 (* 1 = 0.117883 loss)
I0815 20:20:02.958111 20887 sgd_solver.cpp:136] Iteration 21600, lr = 0.0001, m = 0.9
I0815 20:20:12.291733 20830 data_reader.cpp:288] Starting prefetch of epoch 14
I0815 20:20:22.295228 20887 solver.cpp:312] Iteration 21700 (5.17153 iter/s, 19.3366s/100 iter), loss = 0.0889876
I0815 20:20:22.295256 20887 solver.cpp:334]     Train net output #0: loss = 0.0889875 (* 1 = 0.0889875 loss)
I0815 20:20:22.295262 20887 sgd_solver.cpp:136] Iteration 21700, lr = 0.0001, m = 0.9
I0815 20:20:41.622429 20887 solver.cpp:312] Iteration 21800 (5.1742 iter/s, 19.3267s/100 iter), loss = 0.0983334
I0815 20:20:41.622483 20887 solver.cpp:334]     Train net output #0: loss = 0.0983332 (* 1 = 0.0983332 loss)
I0815 20:20:41.622488 20887 sgd_solver.cpp:136] Iteration 21800, lr = 0.0001, m = 0.9
I0815 20:21:01.323627 20887 solver.cpp:312] Iteration 21900 (5.07597 iter/s, 19.7007s/100 iter), loss = 0.0905187
I0815 20:21:01.323652 20887 solver.cpp:334]     Train net output #0: loss = 0.0905185 (* 1 = 0.0905185 loss)
I0815 20:21:01.323657 20887 sgd_solver.cpp:136] Iteration 21900, lr = 0.0001, m = 0.9
I0815 20:21:16.550317 20891 data_reader.cpp:288] Starting prefetch of epoch 13
I0815 20:21:20.577914 20887 solver.cpp:509] Iteration 22000, Testing net (#0)
I0815 20:21:32.542434 20887 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.949965
I0815 20:21:32.542459 20887 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999914
I0815 20:21:32.542464 20887 solver.cpp:594]     Test net output #2: loss = 0.159075 (* 1 = 0.159075 loss)
I0815 20:21:32.542558 20887 solver.cpp:264] [MultiGPU] Tests completed in 11.9643s
I0815 20:21:32.761438 20887 solver.cpp:312] Iteration 22000 (3.18097 iter/s, 31.437s/100 iter), loss = 0.122632
I0815 20:21:32.761463 20887 solver.cpp:334]     Train net output #0: loss = 0.122632 (* 1 = 0.122632 loss)
I0815 20:21:32.761469 20887 sgd_solver.cpp:136] Iteration 22000, lr = 0.0001, m = 0.9
I0815 20:21:52.199527 20887 solver.cpp:312] Iteration 22100 (5.14468 iter/s, 19.4375s/100 iter), loss = 0.159587
I0815 20:21:52.199580 20887 solver.cpp:334]     Train net output #0: loss = 0.159587 (* 1 = 0.159587 loss)
I0815 20:21:52.199587 20887 sgd_solver.cpp:136] Iteration 22100, lr = 0.0001, m = 0.9
I0815 20:22:00.837425 20893 data_reader.cpp:288] Starting prefetch of epoch 15
I0815 20:22:11.575182 20887 solver.cpp:312] Iteration 22200 (5.16126 iter/s, 19.3751s/100 iter), loss = 0.120574
I0815 20:22:11.575242 20887 solver.cpp:334]     Train net output #0: loss = 0.120574 (* 1 = 0.120574 loss)
I0815 20:22:11.575263 20887 sgd_solver.cpp:136] Iteration 22200, lr = 0.0001, m = 0.9
I0815 20:22:31.077555 20887 solver.cpp:312] Iteration 22300 (5.12772 iter/s, 19.5018s/100 iter), loss = 0.0654961
I0815 20:22:31.077656 20887 solver.cpp:334]     Train net output #0: loss = 0.065496 (* 1 = 0.065496 loss)
I0815 20:22:31.077673 20887 sgd_solver.cpp:136] Iteration 22300, lr = 0.0001, m = 0.9
I0815 20:22:32.787011 20890 data_reader.cpp:288] Starting prefetch of epoch 15
I0815 20:22:50.249833 20887 solver.cpp:312] Iteration 22400 (5.21601 iter/s, 19.1717s/100 iter), loss = 0.0960178
I0815 20:22:50.249856 20887 solver.cpp:334]     Train net output #0: loss = 0.0960176 (* 1 = 0.0960176 loss)
I0815 20:22:50.249861 20887 sgd_solver.cpp:136] Iteration 22400, lr = 0.0001, m = 0.9
I0815 20:23:09.643633 20887 solver.cpp:312] Iteration 22500 (5.15643 iter/s, 19.3933s/100 iter), loss = 0.120441
I0815 20:23:09.643702 20887 solver.cpp:334]     Train net output #0: loss = 0.120441 (* 1 = 0.120441 loss)
I0815 20:23:09.643707 20887 sgd_solver.cpp:136] Iteration 22500, lr = 0.0001, m = 0.9
I0815 20:23:28.875548 20887 solver.cpp:312] Iteration 22600 (5.19983 iter/s, 19.2314s/100 iter), loss = 0.0817216
I0815 20:23:28.875572 20887 solver.cpp:334]     Train net output #0: loss = 0.0817215 (* 1 = 0.0817215 loss)
I0815 20:23:28.875576 20887 sgd_solver.cpp:136] Iteration 22600, lr = 0.0001, m = 0.9
I0815 20:23:36.673696 20893 data_reader.cpp:288] Starting prefetch of epoch 16
I0815 20:23:48.490190 20887 solver.cpp:312] Iteration 22700 (5.09837 iter/s, 19.6141s/100 iter), loss = 0.0928479
I0815 20:23:48.490275 20887 solver.cpp:334]     Train net output #0: loss = 0.0928478 (* 1 = 0.0928478 loss)
I0815 20:23:48.490283 20887 sgd_solver.cpp:136] Iteration 22700, lr = 0.0001, m = 0.9
I0815 20:24:07.895961 20887 solver.cpp:312] Iteration 22800 (5.15325 iter/s, 19.4052s/100 iter), loss = 0.0514077
I0815 20:24:07.895989 20887 solver.cpp:334]     Train net output #0: loss = 0.0514076 (* 1 = 0.0514076 loss)
I0815 20:24:07.895995 20887 sgd_solver.cpp:136] Iteration 22800, lr = 0.0001, m = 0.9
I0815 20:24:27.221199 20887 solver.cpp:312] Iteration 22900 (5.17472 iter/s, 19.3247s/100 iter), loss = 0.0990283
I0815 20:24:27.221299 20887 solver.cpp:334]     Train net output #0: loss = 0.0990282 (* 1 = 0.0990282 loss)
I0815 20:24:27.221307 20887 sgd_solver.cpp:136] Iteration 22900, lr = 0.0001, m = 0.9
I0815 20:24:40.971360 20893 data_reader.cpp:288] Starting prefetch of epoch 17
I0815 20:24:46.717125 20887 solver.cpp:312] Iteration 23000 (5.12942 iter/s, 19.4954s/100 iter), loss = 0.108347
I0815 20:24:46.717149 20887 solver.cpp:334]     Train net output #0: loss = 0.108347 (* 1 = 0.108347 loss)
I0815 20:24:46.717152 20887 sgd_solver.cpp:136] Iteration 23000, lr = 0.0001, m = 0.9
I0815 20:25:06.212220 20887 solver.cpp:312] Iteration 23100 (5.12964 iter/s, 19.4946s/100 iter), loss = 0.0831834
I0815 20:25:06.212267 20887 solver.cpp:334]     Train net output #0: loss = 0.0831833 (* 1 = 0.0831833 loss)
I0815 20:25:06.212272 20887 sgd_solver.cpp:136] Iteration 23100, lr = 0.0001, m = 0.9
I0815 20:25:13.096894 20891 data_reader.cpp:288] Starting prefetch of epoch 14
I0815 20:25:25.797545 20887 solver.cpp:312] Iteration 23200 (5.106 iter/s, 19.5848s/100 iter), loss = 0.0873131
I0815 20:25:25.797569 20887 solver.cpp:334]     Train net output #0: loss = 0.087313 (* 1 = 0.087313 loss)
I0815 20:25:25.797575 20887 sgd_solver.cpp:136] Iteration 23200, lr = 0.0001, m = 0.9
I0815 20:25:45.161249 20887 solver.cpp:312] Iteration 23300 (5.16444 iter/s, 19.3632s/100 iter), loss = 0.0548812
I0815 20:25:45.161301 20887 solver.cpp:334]     Train net output #0: loss = 0.0548812 (* 1 = 0.0548812 loss)
I0815 20:25:45.161306 20887 sgd_solver.cpp:136] Iteration 23300, lr = 0.0001, m = 0.9
I0815 20:26:04.677160 20887 solver.cpp:312] Iteration 23400 (5.12417 iter/s, 19.5154s/100 iter), loss = 0.0814971
I0815 20:26:04.677184 20887 solver.cpp:334]     Train net output #0: loss = 0.081497 (* 1 = 0.081497 loss)
I0815 20:26:04.677188 20887 sgd_solver.cpp:136] Iteration 23400, lr = 0.0001, m = 0.9
I0815 20:26:17.604169 20841 data_reader.cpp:288] Starting prefetch of epoch 16
I0815 20:26:24.107429 20887 solver.cpp:312] Iteration 23500 (5.14675 iter/s, 19.4297s/100 iter), loss = 0.279218
I0815 20:26:24.107455 20887 solver.cpp:334]     Train net output #0: loss = 0.279218 (* 1 = 0.279218 loss)
I0815 20:26:24.107460 20887 sgd_solver.cpp:136] Iteration 23500, lr = 0.0001, m = 0.9
I0815 20:26:43.346532 20887 solver.cpp:312] Iteration 23600 (5.19789 iter/s, 19.2386s/100 iter), loss = 0.0660515
I0815 20:26:43.346556 20887 solver.cpp:334]     Train net output #0: loss = 0.0660514 (* 1 = 0.0660514 loss)
I0815 20:26:43.346563 20887 sgd_solver.cpp:136] Iteration 23600, lr = 0.0001, m = 0.9
I0815 20:27:02.486438 20887 solver.cpp:312] Iteration 23700 (5.22483 iter/s, 19.1394s/100 iter), loss = 0.0793521
I0815 20:27:02.486508 20887 solver.cpp:334]     Train net output #0: loss = 0.079352 (* 1 = 0.079352 loss)
I0815 20:27:02.486513 20887 sgd_solver.cpp:136] Iteration 23700, lr = 0.0001, m = 0.9
I0815 20:27:21.002897 20893 data_reader.cpp:288] Starting prefetch of epoch 18
I0815 20:27:21.768245 20887 solver.cpp:312] Iteration 23800 (5.18638 iter/s, 19.2813s/100 iter), loss = 0.156531
I0815 20:27:21.768265 20887 solver.cpp:334]     Train net output #0: loss = 0.15653 (* 1 = 0.15653 loss)
I0815 20:27:21.768268 20887 sgd_solver.cpp:136] Iteration 23800, lr = 0.0001, m = 0.9
I0815 20:27:41.036820 20887 solver.cpp:312] Iteration 23900 (5.18994 iter/s, 19.2681s/100 iter), loss = 0.0700453
I0815 20:27:41.036866 20887 solver.cpp:334]     Train net output #0: loss = 0.0700452 (* 1 = 0.0700452 loss)
I0815 20:27:41.036873 20887 sgd_solver.cpp:136] Iteration 23900, lr = 0.0001, m = 0.9
I0815 20:27:53.175218 20891 data_reader.cpp:288] Starting prefetch of epoch 15
I0815 20:28:00.429504 20887 solver.cpp:509] Iteration 24000, Testing net (#0)
I0815 20:28:12.191305 20885 data_reader.cpp:288] Starting prefetch of epoch 4
I0815 20:28:12.731998 20887 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.947983
I0815 20:28:12.732018 20887 solver.cpp:594]     Test net output #1: accuracy/top5 = 1
I0815 20:28:12.732023 20887 solver.cpp:594]     Test net output #2: loss = 0.162361 (* 1 = 0.162361 loss)
I0815 20:28:12.732043 20887 solver.cpp:264] [MultiGPU] Tests completed in 12.3022s
I0815 20:28:12.833636 20956 sgd_solver.cpp:48] MultiStep Status: Iteration 24000, step = 1
I0815 20:28:12.833747 20955 sgd_solver.cpp:48] MultiStep Status: Iteration 24000, step = 1
I0815 20:28:12.833642 20957 sgd_solver.cpp:48] MultiStep Status: Iteration 24000, step = 1
I0815 20:28:12.936708 20887 solver.cpp:312] Iteration 24000 (3.13489 iter/s, 31.899s/100 iter), loss = 0.0949077
I0815 20:28:12.936738 20887 solver.cpp:334]     Train net output #0: loss = 0.0949076 (* 1 = 0.0949076 loss)
I0815 20:28:12.936745 20887 sgd_solver.cpp:136] Iteration 24000, lr = 1e-05, m = 0.9
I0815 20:28:32.367503 20887 solver.cpp:312] Iteration 24100 (5.14661 iter/s, 19.4303s/100 iter), loss = 0.108398
I0815 20:28:32.367527 20887 solver.cpp:334]     Train net output #0: loss = 0.108398 (* 1 = 0.108398 loss)
I0815 20:28:32.367532 20887 sgd_solver.cpp:136] Iteration 24100, lr = 1e-05, m = 0.9
I0815 20:28:51.744683 20887 solver.cpp:312] Iteration 24200 (5.16085 iter/s, 19.3766s/100 iter), loss = 0.194035
I0815 20:28:51.744730 20887 solver.cpp:334]     Train net output #0: loss = 0.194034 (* 1 = 0.194034 loss)
I0815 20:28:51.744735 20887 sgd_solver.cpp:136] Iteration 24200, lr = 1e-05, m = 0.9
I0815 20:29:09.580590 20895 data_reader.cpp:288] Starting prefetch of epoch 9
I0815 20:29:11.095831 20887 solver.cpp:312] Iteration 24300 (5.16779 iter/s, 19.3506s/100 iter), loss = 0.063287
I0815 20:29:11.095852 20887 solver.cpp:334]     Train net output #0: loss = 0.0632869 (* 1 = 0.0632869 loss)
I0815 20:29:11.095856 20887 sgd_solver.cpp:136] Iteration 24300, lr = 1e-05, m = 0.9
I0815 20:29:30.837043 20887 solver.cpp:312] Iteration 24400 (5.06569 iter/s, 19.7407s/100 iter), loss = 0.0860034
I0815 20:29:30.837137 20887 solver.cpp:334]     Train net output #0: loss = 0.0860033 (* 1 = 0.0860033 loss)
I0815 20:29:30.837155 20887 sgd_solver.cpp:136] Iteration 24400, lr = 1e-05, m = 0.9
I0815 20:29:42.047065 20891 data_reader.cpp:288] Starting prefetch of epoch 16
I0815 20:29:50.251183 20887 solver.cpp:312] Iteration 24500 (5.15103 iter/s, 19.4136s/100 iter), loss = 0.0670915
I0815 20:29:50.251206 20887 solver.cpp:334]     Train net output #0: loss = 0.0670914 (* 1 = 0.0670914 loss)
I0815 20:29:50.251210 20887 sgd_solver.cpp:136] Iteration 24500, lr = 1e-05, m = 0.9
I0815 20:30:09.855731 20887 solver.cpp:312] Iteration 24600 (5.101 iter/s, 19.604s/100 iter), loss = 0.109931
I0815 20:30:09.855782 20887 solver.cpp:334]     Train net output #0: loss = 0.109931 (* 1 = 0.109931 loss)
I0815 20:30:09.855788 20887 sgd_solver.cpp:136] Iteration 24600, lr = 1e-05, m = 0.9
I0815 20:30:29.281791 20887 solver.cpp:312] Iteration 24700 (5.14787 iter/s, 19.4255s/100 iter), loss = 0.0690158
I0815 20:30:29.281817 20887 solver.cpp:334]     Train net output #0: loss = 0.0690158 (* 1 = 0.0690158 loss)
I0815 20:30:29.281821 20887 sgd_solver.cpp:136] Iteration 24700, lr = 1e-05, m = 0.9
I0815 20:30:46.300076 20893 data_reader.cpp:288] Starting prefetch of epoch 19
I0815 20:30:48.592631 20887 solver.cpp:312] Iteration 24800 (5.17858 iter/s, 19.3103s/100 iter), loss = 0.111094
I0815 20:30:48.592658 20887 solver.cpp:334]     Train net output #0: loss = 0.111094 (* 1 = 0.111094 loss)
I0815 20:30:48.592665 20887 sgd_solver.cpp:136] Iteration 24800, lr = 1e-05, m = 0.9
I0815 20:31:07.895364 20887 solver.cpp:312] Iteration 24900 (5.18076 iter/s, 19.3022s/100 iter), loss = 0.0687769
I0815 20:31:07.895388 20887 solver.cpp:334]     Train net output #0: loss = 0.0687768 (* 1 = 0.0687768 loss)
I0815 20:31:07.895392 20887 sgd_solver.cpp:136] Iteration 24900, lr = 1e-05, m = 0.9
I0815 20:31:27.372643 20887 solver.cpp:312] Iteration 25000 (5.13433 iter/s, 19.4767s/100 iter), loss = 0.0898818
I0815 20:31:27.372692 20887 solver.cpp:334]     Train net output #0: loss = 0.0898817 (* 1 = 0.0898817 loss)
I0815 20:31:27.372699 20887 sgd_solver.cpp:136] Iteration 25000, lr = 1e-05, m = 0.9
I0815 20:31:46.785009 20887 solver.cpp:312] Iteration 25100 (5.1515 iter/s, 19.4118s/100 iter), loss = 0.082423
I0815 20:31:46.785033 20887 solver.cpp:334]     Train net output #0: loss = 0.0824229 (* 1 = 0.0824229 loss)
I0815 20:31:46.785037 20887 sgd_solver.cpp:136] Iteration 25100, lr = 1e-05, m = 0.9
I0815 20:31:50.517025 20830 data_reader.cpp:288] Starting prefetch of epoch 15
I0815 20:32:06.284994 20887 solver.cpp:312] Iteration 25200 (5.12835 iter/s, 19.4995s/100 iter), loss = 0.099597
I0815 20:32:06.285050 20887 solver.cpp:334]     Train net output #0: loss = 0.0995969 (* 1 = 0.0995969 loss)
I0815 20:32:06.285058 20887 sgd_solver.cpp:136] Iteration 25200, lr = 1e-05, m = 0.9
I0815 20:32:22.708191 20890 data_reader.cpp:288] Starting prefetch of epoch 16
I0815 20:32:25.755148 20887 solver.cpp:312] Iteration 25300 (5.13621 iter/s, 19.4696s/100 iter), loss = 0.0890651
I0815 20:32:25.755169 20887 solver.cpp:334]     Train net output #0: loss = 0.089065 (* 1 = 0.089065 loss)
I0815 20:32:25.755173 20887 sgd_solver.cpp:136] Iteration 25300, lr = 1e-05, m = 0.9
I0815 20:32:45.065402 20887 solver.cpp:312] Iteration 25400 (5.17874 iter/s, 19.3097s/100 iter), loss = 0.0951909
I0815 20:32:45.065452 20887 solver.cpp:334]     Train net output #0: loss = 0.0951909 (* 1 = 0.0951909 loss)
I0815 20:32:45.065457 20887 sgd_solver.cpp:136] Iteration 25400, lr = 1e-05, m = 0.9
I0815 20:33:04.481497 20887 solver.cpp:312] Iteration 25500 (5.15051 iter/s, 19.4156s/100 iter), loss = 0.0593201
I0815 20:33:04.481523 20887 solver.cpp:334]     Train net output #0: loss = 0.0593201 (* 1 = 0.0593201 loss)
I0815 20:33:04.481526 20887 sgd_solver.cpp:136] Iteration 25500, lr = 1e-05, m = 0.9
I0815 20:33:23.952814 20887 solver.cpp:312] Iteration 25600 (5.1359 iter/s, 19.4708s/100 iter), loss = 0.0830585
I0815 20:33:23.952865 20887 solver.cpp:334]     Train net output #0: loss = 0.0830585 (* 1 = 0.0830585 loss)
I0815 20:33:23.952869 20887 sgd_solver.cpp:136] Iteration 25600, lr = 1e-05, m = 0.9
I0815 20:33:26.963819 20890 data_reader.cpp:288] Starting prefetch of epoch 17
I0815 20:33:43.391345 20887 solver.cpp:312] Iteration 25700 (5.14456 iter/s, 19.438s/100 iter), loss = 0.0500879
I0815 20:33:43.391366 20887 solver.cpp:334]     Train net output #0: loss = 0.0500879 (* 1 = 0.0500879 loss)
I0815 20:33:43.391372 20887 sgd_solver.cpp:136] Iteration 25700, lr = 1e-05, m = 0.9
I0815 20:34:02.929395 20887 solver.cpp:312] Iteration 25800 (5.11836 iter/s, 19.5375s/100 iter), loss = 0.0783117
I0815 20:34:02.929440 20887 solver.cpp:334]     Train net output #0: loss = 0.0783116 (* 1 = 0.0783116 loss)
I0815 20:34:02.929446 20887 sgd_solver.cpp:136] Iteration 25800, lr = 1e-05, m = 0.9
I0815 20:34:22.479843 20887 solver.cpp:312] Iteration 25900 (5.11511 iter/s, 19.5499s/100 iter), loss = 0.0652774
I0815 20:34:22.479871 20887 solver.cpp:334]     Train net output #0: loss = 0.0652773 (* 1 = 0.0652773 loss)
I0815 20:34:22.479878 20887 sgd_solver.cpp:136] Iteration 25900, lr = 1e-05, m = 0.9
I0815 20:34:31.242030 20895 data_reader.cpp:288] Starting prefetch of epoch 10
I0815 20:34:41.662044 20887 solver.cpp:509] Iteration 26000, Testing net (#0)
I0815 20:34:49.173477 20939 data_reader.cpp:288] Starting prefetch of epoch 2
I0815 20:34:53.435894 20887 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.951068
I0815 20:34:53.435914 20887 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999756
I0815 20:34:53.435921 20887 solver.cpp:594]     Test net output #2: loss = 0.163875 (* 1 = 0.163875 loss)
I0815 20:34:53.435945 20887 solver.cpp:264] [MultiGPU] Tests completed in 11.7736s
I0815 20:34:53.643824 20887 solver.cpp:312] Iteration 26000 (3.20892 iter/s, 31.1631s/100 iter), loss = 0.0971869
I0815 20:34:53.643848 20887 solver.cpp:334]     Train net output #0: loss = 0.0971867 (* 1 = 0.0971867 loss)
I0815 20:34:53.643854 20887 sgd_solver.cpp:136] Iteration 26000, lr = 1e-05, m = 0.9
I0815 20:35:13.167471 20887 solver.cpp:312] Iteration 26100 (5.12214 iter/s, 19.5231s/100 iter), loss = 0.0523584
I0815 20:35:13.167520 20887 solver.cpp:334]     Train net output #0: loss = 0.0523583 (* 1 = 0.0523583 loss)
I0815 20:35:13.167527 20887 sgd_solver.cpp:136] Iteration 26100, lr = 1e-05, m = 0.9
I0815 20:35:32.511937 20887 solver.cpp:312] Iteration 26200 (5.16958 iter/s, 19.3439s/100 iter), loss = 0.111994
I0815 20:35:32.511963 20887 solver.cpp:334]     Train net output #0: loss = 0.111994 (* 1 = 0.111994 loss)
I0815 20:35:32.511970 20887 sgd_solver.cpp:136] Iteration 26200, lr = 1e-05, m = 0.9
I0815 20:35:47.310266 20893 data_reader.cpp:288] Starting prefetch of epoch 20
I0815 20:35:51.954131 20887 solver.cpp:312] Iteration 26300 (5.14359 iter/s, 19.4417s/100 iter), loss = 0.090723
I0815 20:35:51.954152 20887 solver.cpp:334]     Train net output #0: loss = 0.0907229 (* 1 = 0.0907229 loss)
I0815 20:35:51.954157 20887 sgd_solver.cpp:136] Iteration 26300, lr = 1e-05, m = 0.9
I0815 20:36:11.518757 20887 solver.cpp:312] Iteration 26400 (5.11141 iter/s, 19.5641s/100 iter), loss = 0.136346
I0815 20:36:11.518781 20887 solver.cpp:334]     Train net output #0: loss = 0.136346 (* 1 = 0.136346 loss)
I0815 20:36:11.518785 20887 sgd_solver.cpp:136] Iteration 26400, lr = 1e-05, m = 0.9
I0815 20:36:30.849249 20887 solver.cpp:312] Iteration 26500 (5.17332 iter/s, 19.33s/100 iter), loss = 0.0550273
I0815 20:36:30.849326 20887 solver.cpp:334]     Train net output #0: loss = 0.0550272 (* 1 = 0.0550272 loss)
I0815 20:36:30.849334 20887 sgd_solver.cpp:136] Iteration 26500, lr = 1e-05, m = 0.9
I0815 20:36:50.182215 20887 solver.cpp:312] Iteration 26600 (5.17265 iter/s, 19.3324s/100 iter), loss = 0.0842122
I0815 20:36:50.182238 20887 solver.cpp:334]     Train net output #0: loss = 0.0842121 (* 1 = 0.0842121 loss)
I0815 20:36:50.182242 20887 sgd_solver.cpp:136] Iteration 26600, lr = 1e-05, m = 0.9
I0815 20:36:51.320736 20895 data_reader.cpp:288] Starting prefetch of epoch 11
I0815 20:37:09.532579 20887 solver.cpp:312] Iteration 26700 (5.168 iter/s, 19.3498s/100 iter), loss = 0.0756016
I0815 20:37:09.532629 20887 solver.cpp:334]     Train net output #0: loss = 0.0756015 (* 1 = 0.0756015 loss)
I0815 20:37:09.532634 20887 sgd_solver.cpp:136] Iteration 26700, lr = 1e-05, m = 0.9
I0815 20:37:23.363185 20830 data_reader.cpp:288] Starting prefetch of epoch 16
I0815 20:37:28.818567 20887 solver.cpp:312] Iteration 26800 (5.18525 iter/s, 19.2855s/100 iter), loss = 0.0964386
I0815 20:37:28.818595 20887 solver.cpp:334]     Train net output #0: loss = 0.0964386 (* 1 = 0.0964386 loss)
I0815 20:37:28.818603 20887 sgd_solver.cpp:136] Iteration 26800, lr = 1e-05, m = 0.9
I0815 20:37:48.216562 20887 solver.cpp:312] Iteration 26900 (5.15531 iter/s, 19.3975s/100 iter), loss = 0.0595216
I0815 20:37:48.216619 20887 solver.cpp:334]     Train net output #0: loss = 0.0595215 (* 1 = 0.0595215 loss)
I0815 20:37:48.216625 20887 sgd_solver.cpp:136] Iteration 26900, lr = 1e-05, m = 0.9
I0815 20:38:07.594070 20887 solver.cpp:312] Iteration 27000 (5.16076 iter/s, 19.377s/100 iter), loss = 0.191666
I0815 20:38:07.594099 20887 solver.cpp:334]     Train net output #0: loss = 0.191666 (* 1 = 0.191666 loss)
I0815 20:38:07.594105 20887 sgd_solver.cpp:136] Iteration 27000, lr = 1e-05, m = 0.9
I0815 20:38:27.079282 20887 solver.cpp:312] Iteration 27100 (5.13224 iter/s, 19.4847s/100 iter), loss = 0.0757632
I0815 20:38:27.079813 20887 solver.cpp:334]     Train net output #0: loss = 0.0757631 (* 1 = 0.0757631 loss)
I0815 20:38:27.079834 20887 sgd_solver.cpp:136] Iteration 27100, lr = 1e-05, m = 0.9
I0815 20:38:27.495735 20895 data_reader.cpp:288] Starting prefetch of epoch 12
I0815 20:38:46.565253 20887 solver.cpp:312] Iteration 27200 (5.13204 iter/s, 19.4854s/100 iter), loss = 0.0459424
I0815 20:38:46.565275 20887 solver.cpp:334]     Train net output #0: loss = 0.0459424 (* 1 = 0.0459424 loss)
I0815 20:38:46.565279 20887 sgd_solver.cpp:136] Iteration 27200, lr = 1e-05, m = 0.9
I0815 20:39:06.064127 20887 solver.cpp:312] Iteration 27300 (5.12864 iter/s, 19.4983s/100 iter), loss = 0.0799937
I0815 20:39:06.064209 20887 solver.cpp:334]     Train net output #0: loss = 0.0799936 (* 1 = 0.0799936 loss)
I0815 20:39:06.064216 20887 sgd_solver.cpp:136] Iteration 27300, lr = 1e-05, m = 0.9
I0815 20:39:25.598397 20887 solver.cpp:312] Iteration 27400 (5.11935 iter/s, 19.5337s/100 iter), loss = 0.145281
I0815 20:39:25.598422 20887 solver.cpp:334]     Train net output #0: loss = 0.145281 (* 1 = 0.145281 loss)
I0815 20:39:25.598425 20887 sgd_solver.cpp:136] Iteration 27400, lr = 1e-05, m = 0.9
I0815 20:39:32.151238 20895 data_reader.cpp:288] Starting prefetch of epoch 13
I0815 20:39:45.149108 20887 solver.cpp:312] Iteration 27500 (5.11504 iter/s, 19.5502s/100 iter), loss = 0.0707827
I0815 20:39:45.149161 20887 solver.cpp:334]     Train net output #0: loss = 0.0707826 (* 1 = 0.0707826 loss)
I0815 20:39:45.149168 20887 sgd_solver.cpp:136] Iteration 27500, lr = 1e-05, m = 0.9
I0815 20:40:04.493883 20890 data_reader.cpp:288] Starting prefetch of epoch 18
I0815 20:40:04.855621 20887 solver.cpp:312] Iteration 27600 (5.0746 iter/s, 19.706s/100 iter), loss = 0.0942213
I0815 20:40:04.855643 20887 solver.cpp:334]     Train net output #0: loss = 0.0942212 (* 1 = 0.0942212 loss)
I0815 20:40:04.855646 20887 sgd_solver.cpp:136] Iteration 27600, lr = 1e-05, m = 0.9
I0815 20:40:24.264500 20887 solver.cpp:312] Iteration 27700 (5.15242 iter/s, 19.4083s/100 iter), loss = 0.0923788
I0815 20:40:24.264546 20887 solver.cpp:334]     Train net output #0: loss = 0.0923788 (* 1 = 0.0923788 loss)
I0815 20:40:24.264552 20887 sgd_solver.cpp:136] Iteration 27700, lr = 1e-05, m = 0.9
I0815 20:40:43.623736 20887 solver.cpp:312] Iteration 27800 (5.16563 iter/s, 19.3587s/100 iter), loss = 0.0489933
I0815 20:40:43.623759 20887 solver.cpp:334]     Train net output #0: loss = 0.0489932 (* 1 = 0.0489932 loss)
I0815 20:40:43.623764 20887 sgd_solver.cpp:136] Iteration 27800, lr = 1e-05, m = 0.9
I0815 20:41:03.234752 20887 solver.cpp:312] Iteration 27900 (5.09931 iter/s, 19.6105s/100 iter), loss = 0.0691467
I0815 20:41:03.234805 20887 solver.cpp:334]     Train net output #0: loss = 0.0691467 (* 1 = 0.0691467 loss)
I0815 20:41:03.234812 20887 sgd_solver.cpp:136] Iteration 27900, lr = 1e-05, m = 0.9
I0815 20:41:08.680934 20841 data_reader.cpp:288] Starting prefetch of epoch 17
I0815 20:41:22.383684 20887 solver.cpp:509] Iteration 28000, Testing net (#0)
I0815 20:41:33.205883 20885 data_reader.cpp:288] Starting prefetch of epoch 5
I0815 20:41:34.087718 20887 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.949315
I0815 20:41:34.087811 20887 solver.cpp:594]     Test net output #1: accuracy/top5 = 1
I0815 20:41:34.087821 20887 solver.cpp:594]     Test net output #2: loss = 0.152676 (* 1 = 0.152676 loss)
I0815 20:41:34.087841 20887 solver.cpp:264] [MultiGPU] Tests completed in 11.7038s
I0815 20:41:34.276509 20887 solver.cpp:312] Iteration 28000 (3.22156 iter/s, 31.0409s/100 iter), loss = 0.102328
I0815 20:41:34.276533 20887 solver.cpp:334]     Train net output #0: loss = 0.102328 (* 1 = 0.102328 loss)
I0815 20:41:34.276540 20887 sgd_solver.cpp:136] Iteration 28000, lr = 1e-05, m = 0.9
I0815 20:41:52.671653 20830 data_reader.cpp:288] Starting prefetch of epoch 17
I0815 20:41:53.763195 20887 solver.cpp:312] Iteration 28100 (5.13185 iter/s, 19.4861s/100 iter), loss = 0.0809231
I0815 20:41:53.763219 20887 solver.cpp:334]     Train net output #0: loss = 0.0809231 (* 1 = 0.0809231 loss)
I0815 20:41:53.763226 20887 sgd_solver.cpp:136] Iteration 28100, lr = 1e-05, m = 0.9
I0815 20:42:13.230830 20887 solver.cpp:312] Iteration 28200 (5.13687 iter/s, 19.4671s/100 iter), loss = 0.0834406
I0815 20:42:13.230907 20887 solver.cpp:334]     Train net output #0: loss = 0.0834405 (* 1 = 0.0834405 loss)
I0815 20:42:13.230914 20887 sgd_solver.cpp:136] Iteration 28200, lr = 1e-05, m = 0.9
I0815 20:42:32.578362 20887 solver.cpp:312] Iteration 28300 (5.16876 iter/s, 19.347s/100 iter), loss = 0.0536742
I0815 20:42:32.578388 20887 solver.cpp:334]     Train net output #0: loss = 0.0536741 (* 1 = 0.0536741 loss)
I0815 20:42:32.578395 20887 sgd_solver.cpp:136] Iteration 28300, lr = 1e-05, m = 0.9
I0815 20:42:52.168970 20887 solver.cpp:312] Iteration 28400 (5.10463 iter/s, 19.5901s/100 iter), loss = 0.0969053
I0815 20:42:52.169010 20887 solver.cpp:334]     Train net output #0: loss = 0.0969053 (* 1 = 0.0969053 loss)
I0815 20:42:52.169015 20887 sgd_solver.cpp:136] Iteration 28400, lr = 1e-05, m = 0.9
I0815 20:42:56.881093 20841 data_reader.cpp:288] Starting prefetch of epoch 18
I0815 20:43:11.747241 20887 solver.cpp:312] Iteration 28500 (5.10784 iter/s, 19.5777s/100 iter), loss = 0.0900104
I0815 20:43:11.747267 20887 solver.cpp:334]     Train net output #0: loss = 0.0900104 (* 1 = 0.0900104 loss)
I0815 20:43:11.747272 20887 sgd_solver.cpp:136] Iteration 28500, lr = 1e-05, m = 0.9
I0815 20:43:31.243201 20887 solver.cpp:312] Iteration 28600 (5.12941 iter/s, 19.4954s/100 iter), loss = 0.121283
I0815 20:43:31.243254 20887 solver.cpp:334]     Train net output #0: loss = 0.121283 (* 1 = 0.121283 loss)
I0815 20:43:31.243259 20887 sgd_solver.cpp:136] Iteration 28600, lr = 1e-05, m = 0.9
I0815 20:43:50.789892 20887 solver.cpp:312] Iteration 28700 (5.1161 iter/s, 19.5462s/100 iter), loss = 0.0993443
I0815 20:43:50.789916 20887 solver.cpp:334]     Train net output #0: loss = 0.0993442 (* 1 = 0.0993442 loss)
I0815 20:43:50.789922 20887 sgd_solver.cpp:136] Iteration 28700, lr = 1e-05, m = 0.9
I0815 20:44:01.392946 20895 data_reader.cpp:288] Starting prefetch of epoch 14
I0815 20:44:10.024425 20887 solver.cpp:312] Iteration 28800 (5.19913 iter/s, 19.234s/100 iter), loss = 0.0735802
I0815 20:44:10.024444 20887 solver.cpp:334]     Train net output #0: loss = 0.0735802 (* 1 = 0.0735802 loss)
I0815 20:44:10.024448 20887 sgd_solver.cpp:136] Iteration 28800, lr = 1e-05, m = 0.9
I0815 20:44:29.585264 20887 solver.cpp:312] Iteration 28900 (5.1124 iter/s, 19.5603s/100 iter), loss = 0.159104
I0815 20:44:29.585294 20887 solver.cpp:334]     Train net output #0: loss = 0.159104 (* 1 = 0.159104 loss)
I0815 20:44:29.585300 20887 sgd_solver.cpp:136] Iteration 28900, lr = 1e-05, m = 0.9
I0815 20:44:33.394704 20890 data_reader.cpp:288] Starting prefetch of epoch 19
I0815 20:44:48.766034 20887 solver.cpp:312] Iteration 29000 (5.2137 iter/s, 19.1802s/100 iter), loss = 0.0952514
I0815 20:44:48.766058 20887 solver.cpp:334]     Train net output #0: loss = 0.0952514 (* 1 = 0.0952514 loss)
I0815 20:44:48.766062 20887 sgd_solver.cpp:136] Iteration 29000, lr = 1e-05, m = 0.9
I0815 20:45:08.174700 20887 solver.cpp:312] Iteration 29100 (5.15248 iter/s, 19.4081s/100 iter), loss = 0.0913881
I0815 20:45:08.174751 20887 solver.cpp:334]     Train net output #0: loss = 0.091388 (* 1 = 0.091388 loss)
I0815 20:45:08.174758 20887 sgd_solver.cpp:136] Iteration 29100, lr = 1e-05, m = 0.9
I0815 20:45:27.375522 20887 solver.cpp:312] Iteration 29200 (5.20825 iter/s, 19.2003s/100 iter), loss = 0.0621565
I0815 20:45:27.375547 20887 solver.cpp:334]     Train net output #0: loss = 0.0621565 (* 1 = 0.0621565 loss)
I0815 20:45:27.375553 20887 sgd_solver.cpp:136] Iteration 29200, lr = 1e-05, m = 0.9
I0815 20:45:37.461752 20895 data_reader.cpp:288] Starting prefetch of epoch 15
I0815 20:45:47.107094 20887 solver.cpp:312] Iteration 29300 (5.06816 iter/s, 19.731s/100 iter), loss = 0.0605243
I0815 20:45:47.107162 20887 solver.cpp:334]     Train net output #0: loss = 0.0605243 (* 1 = 0.0605243 loss)
I0815 20:45:47.107167 20887 sgd_solver.cpp:136] Iteration 29300, lr = 1e-05, m = 0.9
I0815 20:46:06.560631 20887 solver.cpp:312] Iteration 29400 (5.14059 iter/s, 19.453s/100 iter), loss = 0.0602441
I0815 20:46:06.560657 20887 solver.cpp:334]     Train net output #0: loss = 0.0602441 (* 1 = 0.0602441 loss)
I0815 20:46:06.560662 20887 sgd_solver.cpp:136] Iteration 29400, lr = 1e-05, m = 0.9
I0815 20:46:26.061928 20887 solver.cpp:312] Iteration 29500 (5.128 iter/s, 19.5008s/100 iter), loss = 0.140698
I0815 20:46:26.062006 20887 solver.cpp:334]     Train net output #0: loss = 0.140698 (* 1 = 0.140698 loss)
I0815 20:46:26.062012 20887 sgd_solver.cpp:136] Iteration 29500, lr = 1e-05, m = 0.9
I0815 20:46:41.747416 20895 data_reader.cpp:288] Starting prefetch of epoch 16
I0815 20:46:45.438400 20887 solver.cpp:312] Iteration 29600 (5.16104 iter/s, 19.3759s/100 iter), loss = 0.0846207
I0815 20:46:45.438429 20887 solver.cpp:334]     Train net output #0: loss = 0.0846207 (* 1 = 0.0846207 loss)
I0815 20:46:45.438436 20887 sgd_solver.cpp:136] Iteration 29600, lr = 1e-05, m = 0.9
I0815 20:47:04.765357 20887 solver.cpp:312] Iteration 29700 (5.17426 iter/s, 19.3264s/100 iter), loss = 0.0693928
I0815 20:47:04.765406 20887 solver.cpp:334]     Train net output #0: loss = 0.0693928 (* 1 = 0.0693928 loss)
I0815 20:47:04.765411 20887 sgd_solver.cpp:136] Iteration 29700, lr = 1e-05, m = 0.9
I0815 20:47:13.730834 20890 data_reader.cpp:288] Starting prefetch of epoch 20
I0815 20:47:24.174249 20887 solver.cpp:312] Iteration 29800 (5.15242 iter/s, 19.4084s/100 iter), loss = 0.149759
I0815 20:47:24.174275 20887 solver.cpp:334]     Train net output #0: loss = 0.149759 (* 1 = 0.149759 loss)
I0815 20:47:24.174281 20887 sgd_solver.cpp:136] Iteration 29800, lr = 1e-05, m = 0.9
I0815 20:47:43.756636 20887 solver.cpp:312] Iteration 29900 (5.10677 iter/s, 19.5819s/100 iter), loss = 0.078146
I0815 20:47:43.772194 20887 solver.cpp:334]     Train net output #0: loss = 0.078146 (* 1 = 0.078146 loss)
I0815 20:47:43.772222 20887 sgd_solver.cpp:136] Iteration 29900, lr = 1e-05, m = 0.9
I0815 20:48:03.170014 20887 solver.cpp:639] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/initial/cityscapes5_jsegnet21v2_iter_30000.caffemodel
I0815 20:48:03.220566 20887 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/initial/cityscapes5_jsegnet21v2_iter_30000.solverstate
I0815 20:48:03.231389 20887 solver.cpp:509] Iteration 30000, Testing net (#0)
I0815 20:48:10.719717 20935 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 20:48:15.033571 20887 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.950035
I0815 20:48:15.033612 20887 solver.cpp:594]     Test net output #1: accuracy/top5 = 0.999546
I0815 20:48:15.033618 20887 solver.cpp:594]     Test net output #2: loss = 0.184462 (* 1 = 0.184462 loss)
I0815 20:48:15.033644 20887 solver.cpp:264] [MultiGPU] Tests completed in 11.8019s
I0815 20:48:15.241544 20887 solver.cpp:312] Iteration 30000 (3.17621 iter/s, 31.484s/100 iter), loss = 0.139326
I0815 20:48:15.241574 20887 solver.cpp:334]     Train net output #0: loss = 0.139326 (* 1 = 0.139326 loss)
I0815 20:48:15.241580 20887 sgd_solver.cpp:136] Iteration 30000, lr = 1e-05, m = 0.9
I0815 20:48:34.578555 20887 solver.cpp:312] Iteration 30100 (5.17157 iter/s, 19.3365s/100 iter), loss = 0.0796032
I0815 20:48:34.578584 20887 solver.cpp:334]     Train net output #0: loss = 0.0796032 (* 1 = 0.0796032 loss)
I0815 20:48:34.578590 20887 sgd_solver.cpp:136] Iteration 30100, lr = 1e-05, m = 0.9
I0815 20:48:54.076216 20887 solver.cpp:312] Iteration 30200 (5.12896 iter/s, 19.4971s/100 iter), loss = 0.0695413
I0815 20:48:54.076288 20887 solver.cpp:334]     Train net output #0: loss = 0.0695414 (* 1 = 0.0695414 loss)
I0815 20:48:54.076295 20887 sgd_solver.cpp:136] Iteration 30200, lr = 1e-05, m = 0.9
I0815 20:49:02.210515 20895 data_reader.cpp:288] Starting prefetch of epoch 17
I0815 20:49:13.357017 20887 solver.cpp:312] Iteration 30300 (5.18665 iter/s, 19.2803s/100 iter), loss = 0.0937344
I0815 20:49:13.357043 20887 solver.cpp:334]     Train net output #0: loss = 0.0937345 (* 1 = 0.0937345 loss)
I0815 20:49:13.357050 20887 sgd_solver.cpp:136] Iteration 30300, lr = 1e-05, m = 0.9
I0815 20:49:32.971503 20887 solver.cpp:312] Iteration 30400 (5.09841 iter/s, 19.6139s/100 iter), loss = 0.0831745
I0815 20:49:32.971550 20887 solver.cpp:334]     Train net output #0: loss = 0.0831745 (* 1 = 0.0831745 loss)
I0815 20:49:32.971556 20887 sgd_solver.cpp:136] Iteration 30400, lr = 1e-05, m = 0.9
I0815 20:49:34.565201 20895 data_reader.cpp:288] Starting prefetch of epoch 18
I0815 20:49:52.302301 20887 solver.cpp:312] Iteration 30500 (5.17324 iter/s, 19.3303s/100 iter), loss = 0.0899855
I0815 20:49:52.302325 20887 solver.cpp:334]     Train net output #0: loss = 0.0899855 (* 1 = 0.0899855 loss)
I0815 20:49:52.302330 20887 sgd_solver.cpp:136] Iteration 30500, lr = 1e-05, m = 0.9
I0815 20:50:11.622195 20887 solver.cpp:312] Iteration 30600 (5.17616 iter/s, 19.3194s/100 iter), loss = 0.0647404
I0815 20:50:11.622274 20887 solver.cpp:334]     Train net output #0: loss = 0.0647405 (* 1 = 0.0647405 loss)
I0815 20:50:11.622282 20887 sgd_solver.cpp:136] Iteration 30600, lr = 1e-05, m = 0.9
I0815 20:50:31.135249 20887 solver.cpp:312] Iteration 30700 (5.12491 iter/s, 19.5125s/100 iter), loss = 0.177439
I0815 20:50:31.135272 20887 solver.cpp:334]     Train net output #0: loss = 0.177439 (* 1 = 0.177439 loss)
I0815 20:50:31.135275 20887 sgd_solver.cpp:136] Iteration 30700, lr = 1e-05, m = 0.9
I0815 20:50:38.589594 20895 data_reader.cpp:288] Starting prefetch of epoch 19
I0815 20:50:50.498916 20887 solver.cpp:312] Iteration 30800 (5.16445 iter/s, 19.3631s/100 iter), loss = 0.0889117
I0815 20:50:50.499006 20887 solver.cpp:334]     Train net output #0: loss = 0.0889118 (* 1 = 0.0889118 loss)
I0815 20:50:50.499014 20887 sgd_solver.cpp:136] Iteration 30800, lr = 1e-05, m = 0.9
I0815 20:51:09.835805 20887 solver.cpp:312] Iteration 30900 (5.17161 iter/s, 19.3364s/100 iter), loss = 0.0795591
I0815 20:51:09.835835 20887 solver.cpp:334]     Train net output #0: loss = 0.0795591 (* 1 = 0.0795591 loss)
I0815 20:51:09.835842 20887 sgd_solver.cpp:136] Iteration 30900, lr = 1e-05, m = 0.9
I0815 20:51:29.254753 20887 solver.cpp:312] Iteration 31000 (5.14975 iter/s, 19.4184s/100 iter), loss = 0.0708222
I0815 20:51:29.268203 20887 solver.cpp:334]     Train net output #0: loss = 0.0708222 (* 1 = 0.0708222 loss)
I0815 20:51:29.268235 20887 sgd_solver.cpp:136] Iteration 31000, lr = 1e-05, m = 0.9
I0815 20:51:42.679879 20841 data_reader.cpp:288] Starting prefetch of epoch 19
I0815 20:51:48.782821 20887 solver.cpp:312] Iteration 31100 (5.12098 iter/s, 19.5275s/100 iter), loss = 0.0839641
I0815 20:51:48.782850 20887 solver.cpp:334]     Train net output #0: loss = 0.0839642 (* 1 = 0.0839642 loss)
I0815 20:51:48.782855 20887 sgd_solver.cpp:136] Iteration 31100, lr = 1e-05, m = 0.9
I0815 20:52:08.278045 20887 solver.cpp:312] Iteration 31200 (5.1296 iter/s, 19.4947s/100 iter), loss = 0.160095
I0815 20:52:08.278097 20887 solver.cpp:334]     Train net output #0: loss = 0.160095 (* 1 = 0.160095 loss)
I0815 20:52:08.278105 20887 sgd_solver.cpp:136] Iteration 31200, lr = 1e-05, m = 0.9
I0815 20:52:14.917326 20890 data_reader.cpp:288] Starting prefetch of epoch 21
I0815 20:52:27.736165 20887 solver.cpp:312] Iteration 31300 (5.13938 iter/s, 19.4576s/100 iter), loss = 0.110656
I0815 20:52:27.736186 20887 solver.cpp:334]     Train net output #0: loss = 0.110656 (* 1 = 0.110656 loss)
I0815 20:52:27.736191 20887 sgd_solver.cpp:136] Iteration 31300, lr = 1e-05, m = 0.9
I0815 20:52:47.278334 20887 solver.cpp:312] Iteration 31400 (5.11728 iter/s, 19.5416s/100 iter), loss = 0.0663427
I0815 20:52:47.278383 20887 solver.cpp:334]     Train net output #0: loss = 0.0663427 (* 1 = 0.0663427 loss)
I0815 20:52:47.278388 20887 sgd_solver.cpp:136] Iteration 31400, lr = 1e-05, m = 0.9
I0815 20:53:06.781877 20887 solver.cpp:312] Iteration 31500 (5.12741 iter/s, 19.503s/100 iter), loss = 0.0735966
I0815 20:53:06.781900 20887 solver.cpp:334]     Train net output #0: loss = 0.0735966 (* 1 = 0.0735966 loss)
I0815 20:53:06.781905 20887 sgd_solver.cpp:136] Iteration 31500, lr = 1e-05, m = 0.9
I0815 20:53:19.493676 20895 data_reader.cpp:288] Starting prefetch of epoch 20
I0815 20:53:26.275195 20887 solver.cpp:312] Iteration 31600 (5.1301 iter/s, 19.4928s/100 iter), loss = 0.205428
I0815 20:53:26.275219 20887 solver.cpp:334]     Train net output #0: loss = 0.205428 (* 1 = 0.205428 loss)
I0815 20:53:26.275224 20887 sgd_solver.cpp:136] Iteration 31600, lr = 1e-05, m = 0.9
I0815 20:53:45.822414 20887 solver.cpp:312] Iteration 31700 (5.11596 iter/s, 19.5467s/100 iter), loss = 0.0931227
I0815 20:53:45.822439 20887 solver.cpp:334]     Train net output #0: loss = 0.0931228 (* 1 = 0.0931228 loss)
I0815 20:53:45.822444 20887 sgd_solver.cpp:136] Iteration 31700, lr = 1e-05, m = 0.9
I0815 20:54:05.198626 20887 solver.cpp:312] Iteration 31800 (5.16111 iter/s, 19.3757s/100 iter), loss = 0.0812021
I0815 20:54:05.198683 20887 solver.cpp:334]     Train net output #0: loss = 0.0812021 (* 1 = 0.0812021 loss)
I0815 20:54:05.198690 20887 sgd_solver.cpp:136] Iteration 31800, lr = 1e-05, m = 0.9
I0815 20:54:23.522258 20893 data_reader.cpp:288] Starting prefetch of epoch 21
I0815 20:54:23.522258 20841 data_reader.cpp:288] Starting prefetch of epoch 20
I0815 20:54:24.464838 20887 solver.cpp:312] Iteration 31900 (5.19058 iter/s, 19.2657s/100 iter), loss = 0.0506355
I0815 20:54:24.464862 20887 solver.cpp:334]     Train net output #0: loss = 0.0506355 (* 1 = 0.0506355 loss)
I0815 20:54:24.464869 20887 sgd_solver.cpp:136] Iteration 31900, lr = 1e-05, m = 0.9
I0815 20:54:43.869547 20887 solver.cpp:312] Iteration 31999 (5.10199 iter/s, 19.4042s/99 iter), loss = 0.0637584
I0815 20:54:43.869639 20887 solver.cpp:334]     Train net output #0: loss = 0.0637585 (* 1 = 0.0637585 loss)
I0815 20:54:43.926149 20887 solver.cpp:639] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/initial/cityscapes5_jsegnet21v2_iter_32000.caffemodel
I0815 20:54:43.971223 20887 sgd_solver.cpp:345] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-08-15_19-04-07/initial/cityscapes5_jsegnet21v2_iter_32000.solverstate
I0815 20:54:44.040051 20887 solver.cpp:486] Iteration 32000, loss = 0.0706673
I0815 20:54:44.040077 20887 solver.cpp:509] Iteration 32000, Testing net (#0)
I0815 20:54:47.664839 20933 data_reader.cpp:288] Starting prefetch of epoch 3
I0815 20:54:55.930138 20887 solver.cpp:594]     Test net output #0: accuracy/top1 = 0.949219
I0815 20:54:55.930162 20887 solver.cpp:594]     Test net output #1: accuracy/top5 = 1
I0815 20:54:55.930169 20887 solver.cpp:594]     Test net output #2: loss = 0.154581 (* 1 = 0.154581 loss)
I0815 20:54:55.989426 20809 parallel.cpp:71] Root Solver performance on device 0: 4.833 * 6 = 29 img/sec (32000 itr in 6620 sec)
I0815 20:54:55.989445 20809 parallel.cpp:76]      Solver performance on device 1: 4.833 * 6 = 29 img/sec (32000 itr in 6620 sec)
I0815 20:54:55.989450 20809 parallel.cpp:76]      Solver performance on device 2: 4.833 * 6 = 29 img/sec (32000 itr in 6620 sec)
I0815 20:54:55.989451 20809 parallel.cpp:79] Overall multi-GPU performance: 87.0007 img/sec
I0815 20:54:57.321442 20809 caffe.cpp:247] Optimization Done in 1h 50m 40s
