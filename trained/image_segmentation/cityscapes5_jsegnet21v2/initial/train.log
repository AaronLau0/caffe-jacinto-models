I0731 18:11:05.542618 16855 caffe.cpp:608] This is NVCaffe 0.16.3 started at Mon Jul 31 18:11:05 2017
I0731 18:11:05.542742 16855 caffe.cpp:611] CuDNN version: 6021
I0731 18:11:05.542745 16855 caffe.cpp:612] CuBLAS version: 8000
I0731 18:11:05.542747 16855 caffe.cpp:613] CUDA version: 8000
I0731 18:11:05.542749 16855 caffe.cpp:614] CUDA driver version: 8000
I0731 18:11:05.832962 16855 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0731 18:11:05.833528 16855 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I0731 18:11:05.834048 16855 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[1]: total=8508145664 free=8379236352
I0731 18:11:05.834560 16855 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[2]: total=8508145664 free=8379236352
I0731 18:11:05.834568 16855 caffe.cpp:208] Using GPUs 0, 1, 2
I0731 18:11:05.834890 16855 caffe.cpp:213] GPU 0: GeForce GTX 1080
I0731 18:11:05.835211 16855 caffe.cpp:213] GPU 1: GeForce GTX 1080
I0731 18:11:05.835532 16855 caffe.cpp:213] GPU 2: GeForce GTX 1080
I0731 18:11:05.835566 16855 solver.cpp:42] Solver data type: FLOAT
I0731 18:11:05.835595 16855 solver.cpp:45] Initializing solver from parameters: 
train_net: "training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/initial/train.prototxt"
test_net: "training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/initial/test.prototxt"
test_iter: 125
test_interval: 2000
base_lr: 0.0001
display: 100
max_iter: 32000
lr_policy: "multistep"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 0.0001
snapshot: 10000
snapshot_prefix: "training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/initial/cityscapes5_jsegnet21v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
test_initialization: false
stepvalue: 24000
iter_size: 1
type: "Adam"
I0731 18:11:05.841791 16855 solver.cpp:77] Creating training net from train_net file: training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/initial/train.prototxt
I0731 18:11:05.842391 16855 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0731 18:11:05.842398 16855 net.cpp:443] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
W0731 18:11:05.842432 16855 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0731 18:11:05.842669 16855 net.cpp:72] Initializing net from parameters: 
name: "jsegnet21v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/train-image-lmdb"
    label_list_path: "data/train-label-lmdb"
    batch_size: 6
    shuffle: false
    threads: 1
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
I0731 18:11:05.842805 16855 net.cpp:104] Using FLOAT as default forward math type
I0731 18:11:05.842810 16855 net.cpp:110] Using FLOAT as default backward math type
I0731 18:11:05.842813 16855 layer_factory.hpp:136] Creating layer 'data' of type 'ImageLabelData'
I0731 18:11:05.842816 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:05.842828 16855 net.cpp:184] Created Layer data (0)
I0731 18:11:05.842831 16855 net.cpp:530] data -> data
I0731 18:11:05.842842 16855 net.cpp:530] data -> label
I0731 18:11:05.842906 16855 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0731 18:11:05.842918 16855 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 18:11:05.844923 16874 db_lmdb.cpp:35] Opened lmdb data/train-image-lmdb
I0731 18:11:05.847651 16855 data_layer.cpp:184] [0] ReshapePrefetch 6, 3, 640, 640
I0731 18:11:05.847714 16855 data_layer.cpp:208] [0] Output data size: 6, 3, 640, 640
I0731 18:11:05.847721 16855 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 18:11:05.847877 16855 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0731 18:11:05.847887 16855 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 18:11:05.848757 16876 db_lmdb.cpp:35] Opened lmdb data/train-label-lmdb
I0731 18:11:05.849496 16875 data_layer.cpp:97] [0] Parser threads: 1
I0731 18:11:05.849509 16875 data_layer.cpp:99] [0] Transformer threads: 1
I0731 18:11:05.855829 16855 data_layer.cpp:184] [0] ReshapePrefetch 6, 1, 640, 640
I0731 18:11:05.856003 16855 data_layer.cpp:208] [0] Output data size: 6, 1, 640, 640
I0731 18:11:05.856030 16855 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 18:11:05.856277 16855 net.cpp:245] Setting up data
I0731 18:11:05.856313 16855 net.cpp:252] TRAIN Top shape for layer 0 'data' 6 3 640 640 (7372800)
I0731 18:11:05.856333 16855 net.cpp:252] TRAIN Top shape for layer 0 'data' 6 1 640 640 (2457600)
I0731 18:11:05.856350 16855 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0731 18:11:05.856365 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:05.856407 16855 net.cpp:184] Created Layer data/bias (1)
I0731 18:11:05.856421 16855 net.cpp:561] data/bias <- data
I0731 18:11:05.856442 16855 net.cpp:530] data/bias -> data/bias
I0731 18:11:05.858108 16877 data_layer.cpp:97] [0] Parser threads: 1
I0731 18:11:05.858134 16877 data_layer.cpp:99] [0] Transformer threads: 1
I0731 18:11:05.862846 16855 net.cpp:245] Setting up data/bias
I0731 18:11:05.862987 16855 net.cpp:252] TRAIN Top shape for layer 1 'data/bias' 6 3 640 640 (7372800)
I0731 18:11:05.863034 16855 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0731 18:11:05.863051 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:05.863116 16855 net.cpp:184] Created Layer conv1a (2)
I0731 18:11:05.863123 16855 net.cpp:561] conv1a <- data/bias
I0731 18:11:05.863131 16855 net.cpp:530] conv1a -> conv1a
I0731 18:11:06.239864 16855 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 7.89G, req 0G)
I0731 18:11:06.239900 16855 net.cpp:245] Setting up conv1a
I0731 18:11:06.239914 16855 net.cpp:252] TRAIN Top shape for layer 2 'conv1a' 6 32 320 320 (19660800)
I0731 18:11:06.239936 16855 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0731 18:11:06.239946 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.239975 16855 net.cpp:184] Created Layer conv1a/bn (3)
I0731 18:11:06.239984 16855 net.cpp:561] conv1a/bn <- conv1a
I0731 18:11:06.239994 16855 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0731 18:11:06.241240 16855 net.cpp:245] Setting up conv1a/bn
I0731 18:11:06.241258 16855 net.cpp:252] TRAIN Top shape for layer 3 'conv1a/bn' 6 32 320 320 (19660800)
I0731 18:11:06.241279 16855 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0731 18:11:06.241287 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.241300 16855 net.cpp:184] Created Layer conv1a/relu (4)
I0731 18:11:06.241308 16855 net.cpp:561] conv1a/relu <- conv1a
I0731 18:11:06.241315 16855 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0731 18:11:06.241343 16855 net.cpp:245] Setting up conv1a/relu
I0731 18:11:06.241353 16855 net.cpp:252] TRAIN Top shape for layer 4 'conv1a/relu' 6 32 320 320 (19660800)
I0731 18:11:06.241361 16855 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0731 18:11:06.241370 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.241396 16855 net.cpp:184] Created Layer conv1b (5)
I0731 18:11:06.241405 16855 net.cpp:561] conv1b <- conv1a
I0731 18:11:06.241411 16855 net.cpp:530] conv1b -> conv1b
I0731 18:11:06.294466 16855 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.73G, req 0G)
I0731 18:11:06.294549 16855 net.cpp:245] Setting up conv1b
I0731 18:11:06.294585 16855 net.cpp:252] TRAIN Top shape for layer 5 'conv1b' 6 32 320 320 (19660800)
I0731 18:11:06.294636 16855 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0731 18:11:06.294661 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.294700 16855 net.cpp:184] Created Layer conv1b/bn (6)
I0731 18:11:06.294719 16855 net.cpp:561] conv1b/bn <- conv1b
I0731 18:11:06.294742 16855 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0731 18:11:06.297818 16855 net.cpp:245] Setting up conv1b/bn
I0731 18:11:06.297864 16855 net.cpp:252] TRAIN Top shape for layer 6 'conv1b/bn' 6 32 320 320 (19660800)
I0731 18:11:06.297914 16855 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0731 18:11:06.297935 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.297979 16855 net.cpp:184] Created Layer conv1b/relu (7)
I0731 18:11:06.297999 16855 net.cpp:561] conv1b/relu <- conv1b
I0731 18:11:06.298019 16855 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0731 18:11:06.298054 16855 net.cpp:245] Setting up conv1b/relu
I0731 18:11:06.298079 16855 net.cpp:252] TRAIN Top shape for layer 7 'conv1b/relu' 6 32 320 320 (19660800)
I0731 18:11:06.298101 16855 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0731 18:11:06.298125 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.298163 16855 net.cpp:184] Created Layer pool1 (8)
I0731 18:11:06.298182 16855 net.cpp:561] pool1 <- conv1b
I0731 18:11:06.298202 16855 net.cpp:530] pool1 -> pool1
I0731 18:11:06.298560 16855 net.cpp:245] Setting up pool1
I0731 18:11:06.298593 16855 net.cpp:252] TRAIN Top shape for layer 8 'pool1' 6 32 160 160 (4915200)
I0731 18:11:06.298616 16855 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0731 18:11:06.298671 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.298729 16855 net.cpp:184] Created Layer res2a_branch2a (9)
I0731 18:11:06.298749 16855 net.cpp:561] res2a_branch2a <- pool1
I0731 18:11:06.298770 16855 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0731 18:11:06.344637 16855 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.6G, req 0G)
I0731 18:11:06.344661 16855 net.cpp:245] Setting up res2a_branch2a
I0731 18:11:06.344669 16855 net.cpp:252] TRAIN Top shape for layer 9 'res2a_branch2a' 6 64 160 160 (9830400)
I0731 18:11:06.344681 16855 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0731 18:11:06.344687 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.344699 16855 net.cpp:184] Created Layer res2a_branch2a/bn (10)
I0731 18:11:06.344704 16855 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0731 18:11:06.344719 16855 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0731 18:11:06.345924 16855 net.cpp:245] Setting up res2a_branch2a/bn
I0731 18:11:06.345934 16855 net.cpp:252] TRAIN Top shape for layer 10 'res2a_branch2a/bn' 6 64 160 160 (9830400)
I0731 18:11:06.345945 16855 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0731 18:11:06.345950 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.345957 16855 net.cpp:184] Created Layer res2a_branch2a/relu (11)
I0731 18:11:06.345960 16855 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0731 18:11:06.345964 16855 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0731 18:11:06.345971 16855 net.cpp:245] Setting up res2a_branch2a/relu
I0731 18:11:06.345975 16855 net.cpp:252] TRAIN Top shape for layer 11 'res2a_branch2a/relu' 6 64 160 160 (9830400)
I0731 18:11:06.345980 16855 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0731 18:11:06.345984 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.345995 16855 net.cpp:184] Created Layer res2a_branch2b (12)
I0731 18:11:06.345999 16855 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0731 18:11:06.346002 16855 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0731 18:11:06.367775 16855 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.52G, req 0G)
I0731 18:11:06.367799 16855 net.cpp:245] Setting up res2a_branch2b
I0731 18:11:06.367805 16855 net.cpp:252] TRAIN Top shape for layer 12 'res2a_branch2b' 6 64 160 160 (9830400)
I0731 18:11:06.367813 16855 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0731 18:11:06.367820 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.367830 16855 net.cpp:184] Created Layer res2a_branch2b/bn (13)
I0731 18:11:06.367833 16855 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0731 18:11:06.367837 16855 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0731 18:11:06.368733 16855 net.cpp:245] Setting up res2a_branch2b/bn
I0731 18:11:06.368744 16855 net.cpp:252] TRAIN Top shape for layer 13 'res2a_branch2b/bn' 6 64 160 160 (9830400)
I0731 18:11:06.368752 16855 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0731 18:11:06.368757 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.368762 16855 net.cpp:184] Created Layer res2a_branch2b/relu (14)
I0731 18:11:06.368765 16855 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0731 18:11:06.368768 16855 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0731 18:11:06.368773 16855 net.cpp:245] Setting up res2a_branch2b/relu
I0731 18:11:06.368777 16855 net.cpp:252] TRAIN Top shape for layer 14 'res2a_branch2b/relu' 6 64 160 160 (9830400)
I0731 18:11:06.368796 16855 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0731 18:11:06.368803 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.368813 16855 net.cpp:184] Created Layer pool2 (15)
I0731 18:11:06.368834 16855 net.cpp:561] pool2 <- res2a_branch2b
I0731 18:11:06.368839 16855 net.cpp:530] pool2 -> pool2
I0731 18:11:06.368943 16855 net.cpp:245] Setting up pool2
I0731 18:11:06.368952 16855 net.cpp:252] TRAIN Top shape for layer 15 'pool2' 6 64 80 80 (2457600)
I0731 18:11:06.368957 16855 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0731 18:11:06.368963 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.368975 16855 net.cpp:184] Created Layer res3a_branch2a (16)
I0731 18:11:06.368980 16855 net.cpp:561] res3a_branch2a <- pool2
I0731 18:11:06.368986 16855 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0731 18:11:06.391413 16855 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 7.46G, req 0G)
I0731 18:11:06.391433 16855 net.cpp:245] Setting up res3a_branch2a
I0731 18:11:06.391441 16855 net.cpp:252] TRAIN Top shape for layer 16 'res3a_branch2a' 6 128 80 80 (4915200)
I0731 18:11:06.391454 16855 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0731 18:11:06.391461 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.391472 16855 net.cpp:184] Created Layer res3a_branch2a/bn (17)
I0731 18:11:06.391479 16855 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0731 18:11:06.391484 16855 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0731 18:11:06.392383 16855 net.cpp:245] Setting up res3a_branch2a/bn
I0731 18:11:06.392395 16855 net.cpp:252] TRAIN Top shape for layer 17 'res3a_branch2a/bn' 6 128 80 80 (4915200)
I0731 18:11:06.392410 16855 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0731 18:11:06.392416 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.392423 16855 net.cpp:184] Created Layer res3a_branch2a/relu (18)
I0731 18:11:06.392429 16855 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0731 18:11:06.392434 16855 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0731 18:11:06.392443 16855 net.cpp:245] Setting up res3a_branch2a/relu
I0731 18:11:06.392451 16855 net.cpp:252] TRAIN Top shape for layer 18 'res3a_branch2a/relu' 6 128 80 80 (4915200)
I0731 18:11:06.392457 16855 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0731 18:11:06.392462 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.392477 16855 net.cpp:184] Created Layer res3a_branch2b (19)
I0731 18:11:06.392482 16855 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0731 18:11:06.392488 16855 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0731 18:11:06.404358 16855 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.42G, req 0G)
I0731 18:11:06.404381 16855 net.cpp:245] Setting up res3a_branch2b
I0731 18:11:06.404391 16855 net.cpp:252] TRAIN Top shape for layer 19 'res3a_branch2b' 6 128 80 80 (4915200)
I0731 18:11:06.404403 16855 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0731 18:11:06.404410 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.404420 16855 net.cpp:184] Created Layer res3a_branch2b/bn (20)
I0731 18:11:06.404426 16855 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0731 18:11:06.404433 16855 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0731 18:11:06.405292 16855 net.cpp:245] Setting up res3a_branch2b/bn
I0731 18:11:06.405304 16855 net.cpp:252] TRAIN Top shape for layer 20 'res3a_branch2b/bn' 6 128 80 80 (4915200)
I0731 18:11:06.405316 16855 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0731 18:11:06.405334 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.405342 16855 net.cpp:184] Created Layer res3a_branch2b/relu (21)
I0731 18:11:06.405349 16855 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0731 18:11:06.405354 16855 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0731 18:11:06.405364 16855 net.cpp:245] Setting up res3a_branch2b/relu
I0731 18:11:06.405369 16855 net.cpp:252] TRAIN Top shape for layer 21 'res3a_branch2b/relu' 6 128 80 80 (4915200)
I0731 18:11:06.405375 16855 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0731 18:11:06.405381 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.405391 16855 net.cpp:184] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (22)
I0731 18:11:06.405396 16855 net.cpp:561] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0731 18:11:06.405401 16855 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0731 18:11:06.405408 16855 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0731 18:11:06.405472 16855 net.cpp:245] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0731 18:11:06.405479 16855 net.cpp:252] TRAIN Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 6 128 80 80 (4915200)
I0731 18:11:06.405488 16855 net.cpp:252] TRAIN Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 6 128 80 80 (4915200)
I0731 18:11:06.405493 16855 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0731 18:11:06.405499 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.405508 16855 net.cpp:184] Created Layer pool3 (23)
I0731 18:11:06.405514 16855 net.cpp:561] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0731 18:11:06.405520 16855 net.cpp:530] pool3 -> pool3
I0731 18:11:06.405604 16855 net.cpp:245] Setting up pool3
I0731 18:11:06.405611 16855 net.cpp:252] TRAIN Top shape for layer 23 'pool3' 6 128 40 40 (1228800)
I0731 18:11:06.405617 16855 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0731 18:11:06.405623 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.405637 16855 net.cpp:184] Created Layer res4a_branch2a (24)
I0731 18:11:06.405642 16855 net.cpp:561] res4a_branch2a <- pool3
I0731 18:11:06.405647 16855 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0731 18:11:06.433912 16855 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.38G, req 0G)
I0731 18:11:06.433936 16855 net.cpp:245] Setting up res4a_branch2a
I0731 18:11:06.433945 16855 net.cpp:252] TRAIN Top shape for layer 24 'res4a_branch2a' 6 256 40 40 (2457600)
I0731 18:11:06.433959 16855 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0731 18:11:06.433965 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.433982 16855 net.cpp:184] Created Layer res4a_branch2a/bn (25)
I0731 18:11:06.433989 16855 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0731 18:11:06.433996 16855 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0731 18:11:06.434900 16855 net.cpp:245] Setting up res4a_branch2a/bn
I0731 18:11:06.434914 16855 net.cpp:252] TRAIN Top shape for layer 25 'res4a_branch2a/bn' 6 256 40 40 (2457600)
I0731 18:11:06.434928 16855 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0731 18:11:06.434934 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.434942 16855 net.cpp:184] Created Layer res4a_branch2a/relu (26)
I0731 18:11:06.434947 16855 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0731 18:11:06.434952 16855 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0731 18:11:06.434973 16855 net.cpp:245] Setting up res4a_branch2a/relu
I0731 18:11:06.434980 16855 net.cpp:252] TRAIN Top shape for layer 26 'res4a_branch2a/relu' 6 256 40 40 (2457600)
I0731 18:11:06.434985 16855 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0731 18:11:06.434993 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.435005 16855 net.cpp:184] Created Layer res4a_branch2b (27)
I0731 18:11:06.435010 16855 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0731 18:11:06.435016 16855 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0731 18:11:06.447881 16855 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.36G, req 0G)
I0731 18:11:06.447901 16855 net.cpp:245] Setting up res4a_branch2b
I0731 18:11:06.447911 16855 net.cpp:252] TRAIN Top shape for layer 27 'res4a_branch2b' 6 256 40 40 (2457600)
I0731 18:11:06.447921 16855 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0731 18:11:06.447928 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.447937 16855 net.cpp:184] Created Layer res4a_branch2b/bn (28)
I0731 18:11:06.447943 16855 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0731 18:11:06.447948 16855 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0731 18:11:06.448832 16855 net.cpp:245] Setting up res4a_branch2b/bn
I0731 18:11:06.448844 16855 net.cpp:252] TRAIN Top shape for layer 28 'res4a_branch2b/bn' 6 256 40 40 (2457600)
I0731 18:11:06.448856 16855 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0731 18:11:06.448863 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.448870 16855 net.cpp:184] Created Layer res4a_branch2b/relu (29)
I0731 18:11:06.448876 16855 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0731 18:11:06.448881 16855 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0731 18:11:06.448890 16855 net.cpp:245] Setting up res4a_branch2b/relu
I0731 18:11:06.448897 16855 net.cpp:252] TRAIN Top shape for layer 29 'res4a_branch2b/relu' 6 256 40 40 (2457600)
I0731 18:11:06.448904 16855 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0731 18:11:06.448909 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.448918 16855 net.cpp:184] Created Layer pool4 (30)
I0731 18:11:06.448923 16855 net.cpp:561] pool4 <- res4a_branch2b
I0731 18:11:06.448930 16855 net.cpp:530] pool4 -> pool4
I0731 18:11:06.449025 16855 net.cpp:245] Setting up pool4
I0731 18:11:06.449033 16855 net.cpp:252] TRAIN Top shape for layer 30 'pool4' 6 256 40 40 (2457600)
I0731 18:11:06.449039 16855 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0731 18:11:06.449046 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.449065 16855 net.cpp:184] Created Layer res5a_branch2a (31)
I0731 18:11:06.449071 16855 net.cpp:561] res5a_branch2a <- pool4
I0731 18:11:06.449077 16855 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0731 18:11:06.479456 16855 net.cpp:245] Setting up res5a_branch2a
I0731 18:11:06.479477 16855 net.cpp:252] TRAIN Top shape for layer 31 'res5a_branch2a' 6 512 40 40 (4915200)
I0731 18:11:06.479486 16855 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0731 18:11:06.479491 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.479503 16855 net.cpp:184] Created Layer res5a_branch2a/bn (32)
I0731 18:11:06.479507 16855 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0731 18:11:06.479512 16855 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0731 18:11:06.480149 16855 net.cpp:245] Setting up res5a_branch2a/bn
I0731 18:11:06.480157 16855 net.cpp:252] TRAIN Top shape for layer 32 'res5a_branch2a/bn' 6 512 40 40 (4915200)
I0731 18:11:06.480175 16855 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0731 18:11:06.480180 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.480186 16855 net.cpp:184] Created Layer res5a_branch2a/relu (33)
I0731 18:11:06.480190 16855 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0731 18:11:06.480195 16855 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0731 18:11:06.480201 16855 net.cpp:245] Setting up res5a_branch2a/relu
I0731 18:11:06.480206 16855 net.cpp:252] TRAIN Top shape for layer 33 'res5a_branch2a/relu' 6 512 40 40 (4915200)
I0731 18:11:06.480211 16855 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0731 18:11:06.480214 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.480224 16855 net.cpp:184] Created Layer res5a_branch2b (34)
I0731 18:11:06.480227 16855 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0731 18:11:06.480232 16855 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0731 18:11:06.493242 16855 net.cpp:245] Setting up res5a_branch2b
I0731 18:11:06.493266 16855 net.cpp:252] TRAIN Top shape for layer 34 'res5a_branch2b' 6 512 40 40 (4915200)
I0731 18:11:06.493278 16855 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0731 18:11:06.493285 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.493294 16855 net.cpp:184] Created Layer res5a_branch2b/bn (35)
I0731 18:11:06.493299 16855 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0731 18:11:06.493305 16855 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0731 18:11:06.493952 16855 net.cpp:245] Setting up res5a_branch2b/bn
I0731 18:11:06.493959 16855 net.cpp:252] TRAIN Top shape for layer 35 'res5a_branch2b/bn' 6 512 40 40 (4915200)
I0731 18:11:06.493968 16855 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0731 18:11:06.493973 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.493978 16855 net.cpp:184] Created Layer res5a_branch2b/relu (36)
I0731 18:11:06.493981 16855 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0731 18:11:06.493986 16855 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0731 18:11:06.493993 16855 net.cpp:245] Setting up res5a_branch2b/relu
I0731 18:11:06.493998 16855 net.cpp:252] TRAIN Top shape for layer 36 'res5a_branch2b/relu' 6 512 40 40 (4915200)
I0731 18:11:06.494001 16855 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I0731 18:11:06.494006 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.494022 16855 net.cpp:184] Created Layer out5a (37)
I0731 18:11:06.494025 16855 net.cpp:561] out5a <- res5a_branch2b
I0731 18:11:06.494030 16855 net.cpp:530] out5a -> out5a
I0731 18:11:06.498297 16855 net.cpp:245] Setting up out5a
I0731 18:11:06.498307 16855 net.cpp:252] TRAIN Top shape for layer 37 'out5a' 6 64 40 40 (614400)
I0731 18:11:06.498314 16855 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I0731 18:11:06.498319 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.498330 16855 net.cpp:184] Created Layer out5a/bn (38)
I0731 18:11:06.498335 16855 net.cpp:561] out5a/bn <- out5a
I0731 18:11:06.498339 16855 net.cpp:513] out5a/bn -> out5a (in-place)
I0731 18:11:06.498994 16855 net.cpp:245] Setting up out5a/bn
I0731 18:11:06.499002 16855 net.cpp:252] TRAIN Top shape for layer 38 'out5a/bn' 6 64 40 40 (614400)
I0731 18:11:06.499011 16855 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I0731 18:11:06.499016 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.499020 16855 net.cpp:184] Created Layer out5a/relu (39)
I0731 18:11:06.499024 16855 net.cpp:561] out5a/relu <- out5a
I0731 18:11:06.499028 16855 net.cpp:513] out5a/relu -> out5a (in-place)
I0731 18:11:06.499042 16855 net.cpp:245] Setting up out5a/relu
I0731 18:11:06.499048 16855 net.cpp:252] TRAIN Top shape for layer 39 'out5a/relu' 6 64 40 40 (614400)
I0731 18:11:06.499053 16855 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I0731 18:11:06.499056 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.499071 16855 net.cpp:184] Created Layer out5a_up2 (40)
I0731 18:11:06.499075 16855 net.cpp:561] out5a_up2 <- out5a
I0731 18:11:06.499079 16855 net.cpp:530] out5a_up2 -> out5a_up2
I0731 18:11:06.499369 16855 net.cpp:245] Setting up out5a_up2
I0731 18:11:06.499377 16855 net.cpp:252] TRAIN Top shape for layer 40 'out5a_up2' 6 64 80 80 (2457600)
I0731 18:11:06.499382 16855 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I0731 18:11:06.499387 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.499402 16855 net.cpp:184] Created Layer out3a (41)
I0731 18:11:06.499406 16855 net.cpp:561] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0731 18:11:06.499411 16855 net.cpp:530] out3a -> out3a
I0731 18:11:06.512928 16855 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 7.3G, req 0G)
I0731 18:11:06.512941 16855 net.cpp:245] Setting up out3a
I0731 18:11:06.512949 16855 net.cpp:252] TRAIN Top shape for layer 41 'out3a' 6 64 80 80 (2457600)
I0731 18:11:06.512958 16855 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I0731 18:11:06.512961 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.512969 16855 net.cpp:184] Created Layer out3a/bn (42)
I0731 18:11:06.512974 16855 net.cpp:561] out3a/bn <- out3a
I0731 18:11:06.512977 16855 net.cpp:513] out3a/bn -> out3a (in-place)
I0731 18:11:06.513710 16855 net.cpp:245] Setting up out3a/bn
I0731 18:11:06.513720 16855 net.cpp:252] TRAIN Top shape for layer 42 'out3a/bn' 6 64 80 80 (2457600)
I0731 18:11:06.513728 16855 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I0731 18:11:06.513732 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.513737 16855 net.cpp:184] Created Layer out3a/relu (43)
I0731 18:11:06.513741 16855 net.cpp:561] out3a/relu <- out3a
I0731 18:11:06.513746 16855 net.cpp:513] out3a/relu -> out3a (in-place)
I0731 18:11:06.513751 16855 net.cpp:245] Setting up out3a/relu
I0731 18:11:06.513756 16855 net.cpp:252] TRAIN Top shape for layer 43 'out3a/relu' 6 64 80 80 (2457600)
I0731 18:11:06.513761 16855 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I0731 18:11:06.513764 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.513777 16855 net.cpp:184] Created Layer out3_out5_combined (44)
I0731 18:11:06.513782 16855 net.cpp:561] out3_out5_combined <- out5a_up2
I0731 18:11:06.513785 16855 net.cpp:561] out3_out5_combined <- out3a
I0731 18:11:06.513789 16855 net.cpp:530] out3_out5_combined -> out3_out5_combined
I0731 18:11:06.514809 16855 net.cpp:245] Setting up out3_out5_combined
I0731 18:11:06.514822 16855 net.cpp:252] TRAIN Top shape for layer 44 'out3_out5_combined' 6 64 80 80 (2457600)
I0731 18:11:06.514827 16855 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I0731 18:11:06.514832 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.514844 16855 net.cpp:184] Created Layer ctx_conv1 (45)
I0731 18:11:06.514848 16855 net.cpp:561] ctx_conv1 <- out3_out5_combined
I0731 18:11:06.514853 16855 net.cpp:530] ctx_conv1 -> ctx_conv1
I0731 18:11:06.529580 16855 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 7.24G, req 0G)
I0731 18:11:06.529599 16855 net.cpp:245] Setting up ctx_conv1
I0731 18:11:06.529606 16855 net.cpp:252] TRAIN Top shape for layer 45 'ctx_conv1' 6 64 80 80 (2457600)
I0731 18:11:06.529624 16855 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I0731 18:11:06.529629 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.529645 16855 net.cpp:184] Created Layer ctx_conv1/bn (46)
I0731 18:11:06.529649 16855 net.cpp:561] ctx_conv1/bn <- ctx_conv1
I0731 18:11:06.529654 16855 net.cpp:513] ctx_conv1/bn -> ctx_conv1 (in-place)
I0731 18:11:06.530382 16855 net.cpp:245] Setting up ctx_conv1/bn
I0731 18:11:06.530391 16855 net.cpp:252] TRAIN Top shape for layer 46 'ctx_conv1/bn' 6 64 80 80 (2457600)
I0731 18:11:06.530400 16855 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I0731 18:11:06.530405 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.530411 16855 net.cpp:184] Created Layer ctx_conv1/relu (47)
I0731 18:11:06.530414 16855 net.cpp:561] ctx_conv1/relu <- ctx_conv1
I0731 18:11:06.530418 16855 net.cpp:513] ctx_conv1/relu -> ctx_conv1 (in-place)
I0731 18:11:06.530424 16855 net.cpp:245] Setting up ctx_conv1/relu
I0731 18:11:06.530429 16855 net.cpp:252] TRAIN Top shape for layer 47 'ctx_conv1/relu' 6 64 80 80 (2457600)
I0731 18:11:06.530434 16855 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I0731 18:11:06.530438 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.530449 16855 net.cpp:184] Created Layer ctx_conv2 (48)
I0731 18:11:06.530452 16855 net.cpp:561] ctx_conv2 <- ctx_conv1
I0731 18:11:06.530457 16855 net.cpp:530] ctx_conv2 -> ctx_conv2
I0731 18:11:06.531564 16855 net.cpp:245] Setting up ctx_conv2
I0731 18:11:06.531572 16855 net.cpp:252] TRAIN Top shape for layer 48 'ctx_conv2' 6 64 80 80 (2457600)
I0731 18:11:06.531579 16855 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I0731 18:11:06.531584 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.531589 16855 net.cpp:184] Created Layer ctx_conv2/bn (49)
I0731 18:11:06.531594 16855 net.cpp:561] ctx_conv2/bn <- ctx_conv2
I0731 18:11:06.531597 16855 net.cpp:513] ctx_conv2/bn -> ctx_conv2 (in-place)
I0731 18:11:06.532261 16855 net.cpp:245] Setting up ctx_conv2/bn
I0731 18:11:06.532269 16855 net.cpp:252] TRAIN Top shape for layer 49 'ctx_conv2/bn' 6 64 80 80 (2457600)
I0731 18:11:06.532276 16855 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I0731 18:11:06.532280 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.532286 16855 net.cpp:184] Created Layer ctx_conv2/relu (50)
I0731 18:11:06.532290 16855 net.cpp:561] ctx_conv2/relu <- ctx_conv2
I0731 18:11:06.532294 16855 net.cpp:513] ctx_conv2/relu -> ctx_conv2 (in-place)
I0731 18:11:06.532300 16855 net.cpp:245] Setting up ctx_conv2/relu
I0731 18:11:06.532305 16855 net.cpp:252] TRAIN Top shape for layer 50 'ctx_conv2/relu' 6 64 80 80 (2457600)
I0731 18:11:06.532310 16855 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I0731 18:11:06.532313 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.532326 16855 net.cpp:184] Created Layer ctx_conv3 (51)
I0731 18:11:06.532330 16855 net.cpp:561] ctx_conv3 <- ctx_conv2
I0731 18:11:06.532335 16855 net.cpp:530] ctx_conv3 -> ctx_conv3
I0731 18:11:06.533432 16855 net.cpp:245] Setting up ctx_conv3
I0731 18:11:06.533440 16855 net.cpp:252] TRAIN Top shape for layer 51 'ctx_conv3' 6 64 80 80 (2457600)
I0731 18:11:06.533447 16855 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I0731 18:11:06.533450 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.533457 16855 net.cpp:184] Created Layer ctx_conv3/bn (52)
I0731 18:11:06.533462 16855 net.cpp:561] ctx_conv3/bn <- ctx_conv3
I0731 18:11:06.533465 16855 net.cpp:513] ctx_conv3/bn -> ctx_conv3 (in-place)
I0731 18:11:06.534131 16855 net.cpp:245] Setting up ctx_conv3/bn
I0731 18:11:06.534143 16855 net.cpp:252] TRAIN Top shape for layer 52 'ctx_conv3/bn' 6 64 80 80 (2457600)
I0731 18:11:06.534152 16855 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I0731 18:11:06.534157 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.534162 16855 net.cpp:184] Created Layer ctx_conv3/relu (53)
I0731 18:11:06.534166 16855 net.cpp:561] ctx_conv3/relu <- ctx_conv3
I0731 18:11:06.534170 16855 net.cpp:513] ctx_conv3/relu -> ctx_conv3 (in-place)
I0731 18:11:06.534176 16855 net.cpp:245] Setting up ctx_conv3/relu
I0731 18:11:06.534181 16855 net.cpp:252] TRAIN Top shape for layer 53 'ctx_conv3/relu' 6 64 80 80 (2457600)
I0731 18:11:06.534185 16855 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I0731 18:11:06.534189 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.534198 16855 net.cpp:184] Created Layer ctx_conv4 (54)
I0731 18:11:06.534201 16855 net.cpp:561] ctx_conv4 <- ctx_conv3
I0731 18:11:06.534205 16855 net.cpp:530] ctx_conv4 -> ctx_conv4
I0731 18:11:06.535293 16855 net.cpp:245] Setting up ctx_conv4
I0731 18:11:06.535301 16855 net.cpp:252] TRAIN Top shape for layer 54 'ctx_conv4' 6 64 80 80 (2457600)
I0731 18:11:06.535307 16855 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I0731 18:11:06.535311 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.535318 16855 net.cpp:184] Created Layer ctx_conv4/bn (55)
I0731 18:11:06.535321 16855 net.cpp:561] ctx_conv4/bn <- ctx_conv4
I0731 18:11:06.535326 16855 net.cpp:513] ctx_conv4/bn -> ctx_conv4 (in-place)
I0731 18:11:06.535982 16855 net.cpp:245] Setting up ctx_conv4/bn
I0731 18:11:06.535990 16855 net.cpp:252] TRAIN Top shape for layer 55 'ctx_conv4/bn' 6 64 80 80 (2457600)
I0731 18:11:06.535998 16855 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I0731 18:11:06.536002 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.536007 16855 net.cpp:184] Created Layer ctx_conv4/relu (56)
I0731 18:11:06.536011 16855 net.cpp:561] ctx_conv4/relu <- ctx_conv4
I0731 18:11:06.536015 16855 net.cpp:513] ctx_conv4/relu -> ctx_conv4 (in-place)
I0731 18:11:06.536021 16855 net.cpp:245] Setting up ctx_conv4/relu
I0731 18:11:06.536026 16855 net.cpp:252] TRAIN Top shape for layer 56 'ctx_conv4/relu' 6 64 80 80 (2457600)
I0731 18:11:06.536031 16855 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I0731 18:11:06.536034 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.536047 16855 net.cpp:184] Created Layer ctx_final (57)
I0731 18:11:06.536051 16855 net.cpp:561] ctx_final <- ctx_conv4
I0731 18:11:06.536054 16855 net.cpp:530] ctx_final -> ctx_final
I0731 18:11:06.549239 16855 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 7.22G, req 0G)
I0731 18:11:06.549259 16855 net.cpp:245] Setting up ctx_final
I0731 18:11:06.549268 16855 net.cpp:252] TRAIN Top shape for layer 57 'ctx_final' 6 8 80 80 (307200)
I0731 18:11:06.549278 16855 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I0731 18:11:06.549283 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.549290 16855 net.cpp:184] Created Layer ctx_final/relu (58)
I0731 18:11:06.549295 16855 net.cpp:561] ctx_final/relu <- ctx_final
I0731 18:11:06.549301 16855 net.cpp:513] ctx_final/relu -> ctx_final (in-place)
I0731 18:11:06.549310 16855 net.cpp:245] Setting up ctx_final/relu
I0731 18:11:06.549315 16855 net.cpp:252] TRAIN Top shape for layer 58 'ctx_final/relu' 6 8 80 80 (307200)
I0731 18:11:06.549319 16855 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I0731 18:11:06.549324 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.549345 16855 net.cpp:184] Created Layer out_deconv_final_up2 (59)
I0731 18:11:06.549348 16855 net.cpp:561] out_deconv_final_up2 <- ctx_final
I0731 18:11:06.549353 16855 net.cpp:530] out_deconv_final_up2 -> out_deconv_final_up2
I0731 18:11:06.549775 16855 net.cpp:245] Setting up out_deconv_final_up2
I0731 18:11:06.549783 16855 net.cpp:252] TRAIN Top shape for layer 59 'out_deconv_final_up2' 6 8 160 160 (1228800)
I0731 18:11:06.549789 16855 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I0731 18:11:06.549794 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.549801 16855 net.cpp:184] Created Layer out_deconv_final_up4 (60)
I0731 18:11:06.549805 16855 net.cpp:561] out_deconv_final_up4 <- out_deconv_final_up2
I0731 18:11:06.549809 16855 net.cpp:530] out_deconv_final_up4 -> out_deconv_final_up4
I0731 18:11:06.550091 16855 net.cpp:245] Setting up out_deconv_final_up4
I0731 18:11:06.550097 16855 net.cpp:252] TRAIN Top shape for layer 60 'out_deconv_final_up4' 6 8 320 320 (4915200)
I0731 18:11:06.550103 16855 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I0731 18:11:06.550107 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.550117 16855 net.cpp:184] Created Layer out_deconv_final_up8 (61)
I0731 18:11:06.550119 16855 net.cpp:561] out_deconv_final_up8 <- out_deconv_final_up4
I0731 18:11:06.550124 16855 net.cpp:530] out_deconv_final_up8 -> out_deconv_final_up8
I0731 18:11:06.550398 16855 net.cpp:245] Setting up out_deconv_final_up8
I0731 18:11:06.550405 16855 net.cpp:252] TRAIN Top shape for layer 61 'out_deconv_final_up8' 6 8 640 640 (19660800)
I0731 18:11:06.550411 16855 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0731 18:11:06.550415 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.550431 16855 net.cpp:184] Created Layer loss (62)
I0731 18:11:06.550434 16855 net.cpp:561] loss <- out_deconv_final_up8
I0731 18:11:06.550439 16855 net.cpp:561] loss <- label
I0731 18:11:06.550446 16855 net.cpp:530] loss -> loss
I0731 18:11:06.551834 16855 net.cpp:245] Setting up loss
I0731 18:11:06.551844 16855 net.cpp:252] TRAIN Top shape for layer 62 'loss' (1)
I0731 18:11:06.551848 16855 net.cpp:256]     with loss weight 1
I0731 18:11:06.551854 16855 net.cpp:323] loss needs backward computation.
I0731 18:11:06.551859 16855 net.cpp:323] out_deconv_final_up8 needs backward computation.
I0731 18:11:06.551863 16855 net.cpp:323] out_deconv_final_up4 needs backward computation.
I0731 18:11:06.551868 16855 net.cpp:323] out_deconv_final_up2 needs backward computation.
I0731 18:11:06.551872 16855 net.cpp:323] ctx_final/relu needs backward computation.
I0731 18:11:06.551877 16855 net.cpp:323] ctx_final needs backward computation.
I0731 18:11:06.551880 16855 net.cpp:323] ctx_conv4/relu needs backward computation.
I0731 18:11:06.551884 16855 net.cpp:323] ctx_conv4/bn needs backward computation.
I0731 18:11:06.551888 16855 net.cpp:323] ctx_conv4 needs backward computation.
I0731 18:11:06.551892 16855 net.cpp:323] ctx_conv3/relu needs backward computation.
I0731 18:11:06.551897 16855 net.cpp:323] ctx_conv3/bn needs backward computation.
I0731 18:11:06.551900 16855 net.cpp:323] ctx_conv3 needs backward computation.
I0731 18:11:06.551904 16855 net.cpp:323] ctx_conv2/relu needs backward computation.
I0731 18:11:06.551908 16855 net.cpp:323] ctx_conv2/bn needs backward computation.
I0731 18:11:06.551911 16855 net.cpp:323] ctx_conv2 needs backward computation.
I0731 18:11:06.551915 16855 net.cpp:323] ctx_conv1/relu needs backward computation.
I0731 18:11:06.551919 16855 net.cpp:323] ctx_conv1/bn needs backward computation.
I0731 18:11:06.551923 16855 net.cpp:323] ctx_conv1 needs backward computation.
I0731 18:11:06.551928 16855 net.cpp:323] out3_out5_combined needs backward computation.
I0731 18:11:06.551933 16855 net.cpp:323] out3a/relu needs backward computation.
I0731 18:11:06.551942 16855 net.cpp:323] out3a/bn needs backward computation.
I0731 18:11:06.551947 16855 net.cpp:323] out3a needs backward computation.
I0731 18:11:06.551951 16855 net.cpp:323] out5a_up2 needs backward computation.
I0731 18:11:06.551955 16855 net.cpp:323] out5a/relu needs backward computation.
I0731 18:11:06.551959 16855 net.cpp:323] out5a/bn needs backward computation.
I0731 18:11:06.551964 16855 net.cpp:323] out5a needs backward computation.
I0731 18:11:06.551967 16855 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0731 18:11:06.551971 16855 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0731 18:11:06.551975 16855 net.cpp:323] res5a_branch2b needs backward computation.
I0731 18:11:06.551980 16855 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0731 18:11:06.551983 16855 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0731 18:11:06.551987 16855 net.cpp:323] res5a_branch2a needs backward computation.
I0731 18:11:06.551991 16855 net.cpp:323] pool4 needs backward computation.
I0731 18:11:06.551996 16855 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0731 18:11:06.552000 16855 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0731 18:11:06.552004 16855 net.cpp:323] res4a_branch2b needs backward computation.
I0731 18:11:06.552008 16855 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0731 18:11:06.552012 16855 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0731 18:11:06.552016 16855 net.cpp:323] res4a_branch2a needs backward computation.
I0731 18:11:06.552021 16855 net.cpp:323] pool3 needs backward computation.
I0731 18:11:06.552026 16855 net.cpp:323] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I0731 18:11:06.552029 16855 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0731 18:11:06.552033 16855 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0731 18:11:06.552037 16855 net.cpp:323] res3a_branch2b needs backward computation.
I0731 18:11:06.552042 16855 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0731 18:11:06.552047 16855 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0731 18:11:06.552050 16855 net.cpp:323] res3a_branch2a needs backward computation.
I0731 18:11:06.552054 16855 net.cpp:323] pool2 needs backward computation.
I0731 18:11:06.552058 16855 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0731 18:11:06.552062 16855 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0731 18:11:06.552067 16855 net.cpp:323] res2a_branch2b needs backward computation.
I0731 18:11:06.552070 16855 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0731 18:11:06.552074 16855 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0731 18:11:06.552079 16855 net.cpp:323] res2a_branch2a needs backward computation.
I0731 18:11:06.552083 16855 net.cpp:323] pool1 needs backward computation.
I0731 18:11:06.552088 16855 net.cpp:323] conv1b/relu needs backward computation.
I0731 18:11:06.552091 16855 net.cpp:323] conv1b/bn needs backward computation.
I0731 18:11:06.552095 16855 net.cpp:323] conv1b needs backward computation.
I0731 18:11:06.552100 16855 net.cpp:323] conv1a/relu needs backward computation.
I0731 18:11:06.552103 16855 net.cpp:323] conv1a/bn needs backward computation.
I0731 18:11:06.552108 16855 net.cpp:323] conv1a needs backward computation.
I0731 18:11:06.552112 16855 net.cpp:325] data/bias does not need backward computation.
I0731 18:11:06.552117 16855 net.cpp:325] data does not need backward computation.
I0731 18:11:06.552121 16855 net.cpp:367] This network produces output loss
I0731 18:11:06.552170 16855 net.cpp:389] Top memory (TRAIN) required for data: 956006400 diff: 946176008
I0731 18:11:06.552175 16855 net.cpp:392] Bottom memory (TRAIN) required for data: 956006400 diff: 956006400
I0731 18:11:06.552178 16855 net.cpp:395] Shared (in-place) memory (TRAIN) by data: 630374400 diff: 630374400
I0731 18:11:06.552181 16855 net.cpp:398] Parameters memory (TRAIN) required for data: 2692608 diff: 2692608
I0731 18:11:06.552189 16855 net.cpp:401] Parameters shared memory (TRAIN) by data: 0 diff: 0
I0731 18:11:06.552193 16855 net.cpp:407] Network initialization done.
I0731 18:11:06.552717 16855 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/initial/test.prototxt
W0731 18:11:06.552783 16855 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0731 18:11:06.552968 16855 net.cpp:72] Initializing net from parameters: 
name: "jsegnet21v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/val-image-lmdb"
    label_list_path: "data/val-label-lmdb"
    batch_size: 2
    threads: 1
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
  accuracy_param {
    ignore_label: 255
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
    ignore_label: 255
  }
}
I0731 18:11:06.553118 16855 net.cpp:104] Using FLOAT as default forward math type
I0731 18:11:06.553123 16855 net.cpp:110] Using FLOAT as default backward math type
I0731 18:11:06.553127 16855 layer_factory.hpp:136] Creating layer 'data' of type 'ImageLabelData'
I0731 18:11:06.553130 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.553140 16855 net.cpp:184] Created Layer data (0)
I0731 18:11:06.553145 16855 net.cpp:530] data -> data
I0731 18:11:06.553150 16855 net.cpp:530] data -> label
I0731 18:11:06.553169 16855 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0731 18:11:06.553176 16855 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 18:11:06.554091 16891 db_lmdb.cpp:35] Opened lmdb data/val-image-lmdb
I0731 18:11:06.555550 16855 data_layer.cpp:184] (0) ReshapePrefetch 2, 3, 640, 640
I0731 18:11:06.555608 16855 data_layer.cpp:208] (0) Output data size: 2, 3, 640, 640
I0731 18:11:06.555614 16855 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 18:11:06.555655 16855 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0731 18:11:06.555662 16855 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 18:11:06.556402 16892 data_layer.cpp:97] (0) Parser threads: 1
I0731 18:11:06.556417 16892 data_layer.cpp:99] (0) Transformer threads: 1
I0731 18:11:06.559315 16893 db_lmdb.cpp:35] Opened lmdb data/val-label-lmdb
I0731 18:11:06.560286 16855 data_layer.cpp:184] (0) ReshapePrefetch 2, 1, 640, 640
I0731 18:11:06.560333 16855 data_layer.cpp:208] (0) Output data size: 2, 1, 640, 640
I0731 18:11:06.560340 16855 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 18:11:06.560379 16855 net.cpp:245] Setting up data
I0731 18:11:06.560387 16855 net.cpp:252] TEST Top shape for layer 0 'data' 2 3 640 640 (2457600)
I0731 18:11:06.560394 16855 net.cpp:252] TEST Top shape for layer 0 'data' 2 1 640 640 (819200)
I0731 18:11:06.560400 16855 layer_factory.hpp:136] Creating layer 'label_data_1_split' of type 'Split'
I0731 18:11:06.560406 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.560416 16855 net.cpp:184] Created Layer label_data_1_split (1)
I0731 18:11:06.560420 16855 net.cpp:561] label_data_1_split <- label
I0731 18:11:06.560425 16855 net.cpp:530] label_data_1_split -> label_data_1_split_0
I0731 18:11:06.560432 16855 net.cpp:530] label_data_1_split -> label_data_1_split_1
I0731 18:11:06.560436 16855 net.cpp:530] label_data_1_split -> label_data_1_split_2
I0731 18:11:06.560509 16855 net.cpp:245] Setting up label_data_1_split
I0731 18:11:06.560515 16855 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0731 18:11:06.560521 16855 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0731 18:11:06.560526 16855 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0731 18:11:06.560530 16855 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0731 18:11:06.560535 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.560544 16855 net.cpp:184] Created Layer data/bias (2)
I0731 18:11:06.560547 16855 net.cpp:561] data/bias <- data
I0731 18:11:06.560551 16855 net.cpp:530] data/bias -> data/bias
I0731 18:11:06.561625 16894 data_layer.cpp:97] (0) Parser threads: 1
I0731 18:11:06.561640 16894 data_layer.cpp:99] (0) Transformer threads: 1
I0731 18:11:06.563438 16855 net.cpp:245] Setting up data/bias
I0731 18:11:06.563455 16855 net.cpp:252] TEST Top shape for layer 2 'data/bias' 2 3 640 640 (2457600)
I0731 18:11:06.563469 16855 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0731 18:11:06.563477 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.563499 16855 net.cpp:184] Created Layer conv1a (3)
I0731 18:11:06.563504 16855 net.cpp:561] conv1a <- data/bias
I0731 18:11:06.563511 16855 net.cpp:530] conv1a -> conv1a
I0731 18:11:06.569077 16855 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.08G, req 0G)
I0731 18:11:06.569093 16855 net.cpp:245] Setting up conv1a
I0731 18:11:06.569100 16855 net.cpp:252] TEST Top shape for layer 3 'conv1a' 2 32 320 320 (6553600)
I0731 18:11:06.569113 16855 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0731 18:11:06.569125 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.569138 16855 net.cpp:184] Created Layer conv1a/bn (4)
I0731 18:11:06.569141 16855 net.cpp:561] conv1a/bn <- conv1a
I0731 18:11:06.569146 16855 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0731 18:11:06.569897 16855 net.cpp:245] Setting up conv1a/bn
I0731 18:11:06.569906 16855 net.cpp:252] TEST Top shape for layer 4 'conv1a/bn' 2 32 320 320 (6553600)
I0731 18:11:06.569916 16855 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0731 18:11:06.569921 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.569926 16855 net.cpp:184] Created Layer conv1a/relu (5)
I0731 18:11:06.569931 16855 net.cpp:561] conv1a/relu <- conv1a
I0731 18:11:06.569934 16855 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0731 18:11:06.569941 16855 net.cpp:245] Setting up conv1a/relu
I0731 18:11:06.569946 16855 net.cpp:252] TEST Top shape for layer 5 'conv1a/relu' 2 32 320 320 (6553600)
I0731 18:11:06.569950 16855 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0731 18:11:06.569954 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.569964 16855 net.cpp:184] Created Layer conv1b (6)
I0731 18:11:06.569967 16855 net.cpp:561] conv1b <- conv1a
I0731 18:11:06.569972 16855 net.cpp:530] conv1b -> conv1b
I0731 18:11:06.583525 16855 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 1  (limit 7.06G, req 0G)
I0731 18:11:06.583547 16855 net.cpp:245] Setting up conv1b
I0731 18:11:06.583555 16855 net.cpp:252] TEST Top shape for layer 6 'conv1b' 2 32 320 320 (6553600)
I0731 18:11:06.583569 16855 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0731 18:11:06.583573 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.583586 16855 net.cpp:184] Created Layer conv1b/bn (7)
I0731 18:11:06.583591 16855 net.cpp:561] conv1b/bn <- conv1b
I0731 18:11:06.583596 16855 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0731 18:11:06.584501 16855 net.cpp:245] Setting up conv1b/bn
I0731 18:11:06.584511 16855 net.cpp:252] TEST Top shape for layer 7 'conv1b/bn' 2 32 320 320 (6553600)
I0731 18:11:06.584520 16855 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0731 18:11:06.584524 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.584535 16855 net.cpp:184] Created Layer conv1b/relu (8)
I0731 18:11:06.584539 16855 net.cpp:561] conv1b/relu <- conv1b
I0731 18:11:06.584543 16855 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0731 18:11:06.584550 16855 net.cpp:245] Setting up conv1b/relu
I0731 18:11:06.584555 16855 net.cpp:252] TEST Top shape for layer 8 'conv1b/relu' 2 32 320 320 (6553600)
I0731 18:11:06.584560 16855 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0731 18:11:06.584564 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.584571 16855 net.cpp:184] Created Layer pool1 (9)
I0731 18:11:06.584574 16855 net.cpp:561] pool1 <- conv1b
I0731 18:11:06.584579 16855 net.cpp:530] pool1 -> pool1
I0731 18:11:06.584655 16855 net.cpp:245] Setting up pool1
I0731 18:11:06.584661 16855 net.cpp:252] TEST Top shape for layer 9 'pool1' 2 32 160 160 (1638400)
I0731 18:11:06.584666 16855 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0731 18:11:06.584671 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.584681 16855 net.cpp:184] Created Layer res2a_branch2a (10)
I0731 18:11:06.584686 16855 net.cpp:561] res2a_branch2a <- pool1
I0731 18:11:06.584689 16855 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0731 18:11:06.594310 16855 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 1  (limit 7.03G, req 0G)
I0731 18:11:06.594408 16855 net.cpp:245] Setting up res2a_branch2a
I0731 18:11:06.594441 16855 net.cpp:252] TEST Top shape for layer 10 'res2a_branch2a' 2 64 160 160 (3276800)
I0731 18:11:06.594487 16855 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0731 18:11:06.594512 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.594571 16855 net.cpp:184] Created Layer res2a_branch2a/bn (11)
I0731 18:11:06.594594 16855 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0731 18:11:06.594619 16855 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0731 18:11:06.598037 16855 net.cpp:245] Setting up res2a_branch2a/bn
I0731 18:11:06.598085 16855 net.cpp:252] TEST Top shape for layer 11 'res2a_branch2a/bn' 2 64 160 160 (3276800)
I0731 18:11:06.598131 16855 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0731 18:11:06.598152 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.598178 16855 net.cpp:184] Created Layer res2a_branch2a/relu (12)
I0731 18:11:06.598201 16855 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0731 18:11:06.598225 16855 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0731 18:11:06.598258 16855 net.cpp:245] Setting up res2a_branch2a/relu
I0731 18:11:06.598284 16855 net.cpp:252] TEST Top shape for layer 12 'res2a_branch2a/relu' 2 64 160 160 (3276800)
I0731 18:11:06.598305 16855 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0731 18:11:06.598330 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.598372 16855 net.cpp:184] Created Layer res2a_branch2b (13)
I0731 18:11:06.598392 16855 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0731 18:11:06.598412 16855 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0731 18:11:06.611882 16855 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 1  (limit 7.02G, req 0G)
I0731 18:11:06.611953 16855 net.cpp:245] Setting up res2a_branch2b
I0731 18:11:06.611992 16855 net.cpp:252] TEST Top shape for layer 13 'res2a_branch2b' 2 64 160 160 (3276800)
I0731 18:11:06.612032 16855 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0731 18:11:06.612061 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.612097 16855 net.cpp:184] Created Layer res2a_branch2b/bn (14)
I0731 18:11:06.612118 16855 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0731 18:11:06.612141 16855 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0731 18:11:06.615384 16855 net.cpp:245] Setting up res2a_branch2b/bn
I0731 18:11:06.615417 16855 net.cpp:252] TEST Top shape for layer 14 'res2a_branch2b/bn' 2 64 160 160 (3276800)
I0731 18:11:06.615447 16855 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0731 18:11:06.615463 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.615479 16855 net.cpp:184] Created Layer res2a_branch2b/relu (15)
I0731 18:11:06.615495 16855 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0731 18:11:06.615509 16855 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0731 18:11:06.615530 16855 net.cpp:245] Setting up res2a_branch2b/relu
I0731 18:11:06.615547 16855 net.cpp:252] TEST Top shape for layer 15 'res2a_branch2b/relu' 2 64 160 160 (3276800)
I0731 18:11:06.615562 16855 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0731 18:11:06.615576 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.615602 16855 net.cpp:184] Created Layer pool2 (16)
I0731 18:11:06.615614 16855 net.cpp:561] pool2 <- res2a_branch2b
I0731 18:11:06.615628 16855 net.cpp:530] pool2 -> pool2
I0731 18:11:06.615861 16855 net.cpp:245] Setting up pool2
I0731 18:11:06.615881 16855 net.cpp:252] TEST Top shape for layer 16 'pool2' 2 64 80 80 (819200)
I0731 18:11:06.615919 16855 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0731 18:11:06.615936 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.615985 16855 net.cpp:184] Created Layer res3a_branch2a (17)
I0731 18:11:06.615999 16855 net.cpp:561] res3a_branch2a <- pool2
I0731 18:11:06.616014 16855 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0731 18:11:06.627661 16855 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.01G, req 0G)
I0731 18:11:06.627687 16855 net.cpp:245] Setting up res3a_branch2a
I0731 18:11:06.627701 16855 net.cpp:252] TEST Top shape for layer 17 'res3a_branch2a' 2 128 80 80 (1638400)
I0731 18:11:06.627717 16855 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0731 18:11:06.627727 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.627741 16855 net.cpp:184] Created Layer res3a_branch2a/bn (18)
I0731 18:11:06.627749 16855 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0731 18:11:06.627758 16855 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0731 18:11:06.629189 16855 net.cpp:245] Setting up res3a_branch2a/bn
I0731 18:11:06.629209 16855 net.cpp:252] TEST Top shape for layer 18 'res3a_branch2a/bn' 2 128 80 80 (1638400)
I0731 18:11:06.629230 16855 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0731 18:11:06.629240 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.629251 16855 net.cpp:184] Created Layer res3a_branch2a/relu (19)
I0731 18:11:06.629259 16855 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0731 18:11:06.629267 16855 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0731 18:11:06.629281 16855 net.cpp:245] Setting up res3a_branch2a/relu
I0731 18:11:06.629292 16855 net.cpp:252] TEST Top shape for layer 19 'res3a_branch2a/relu' 2 128 80 80 (1638400)
I0731 18:11:06.629299 16855 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0731 18:11:06.629308 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.629328 16855 net.cpp:184] Created Layer res3a_branch2b (20)
I0731 18:11:06.629335 16855 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0731 18:11:06.629343 16855 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0731 18:11:06.636102 16855 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 1  (limit 6.99G, req 0G)
I0731 18:11:06.636128 16855 net.cpp:245] Setting up res3a_branch2b
I0731 18:11:06.636138 16855 net.cpp:252] TEST Top shape for layer 20 'res3a_branch2b' 2 128 80 80 (1638400)
I0731 18:11:06.636152 16855 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0731 18:11:06.636159 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.636173 16855 net.cpp:184] Created Layer res3a_branch2b/bn (21)
I0731 18:11:06.636179 16855 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0731 18:11:06.636188 16855 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0731 18:11:06.637305 16855 net.cpp:245] Setting up res3a_branch2b/bn
I0731 18:11:06.637320 16855 net.cpp:252] TEST Top shape for layer 21 'res3a_branch2b/bn' 2 128 80 80 (1638400)
I0731 18:11:06.637332 16855 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0731 18:11:06.637339 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.637348 16855 net.cpp:184] Created Layer res3a_branch2b/relu (22)
I0731 18:11:06.637354 16855 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0731 18:11:06.637362 16855 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0731 18:11:06.637372 16855 net.cpp:245] Setting up res3a_branch2b/relu
I0731 18:11:06.637378 16855 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b/relu' 2 128 80 80 (1638400)
I0731 18:11:06.637397 16855 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0731 18:11:06.637403 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.637411 16855 net.cpp:184] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (23)
I0731 18:11:06.637418 16855 net.cpp:561] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0731 18:11:06.637424 16855 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0731 18:11:06.637432 16855 net.cpp:530] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0731 18:11:06.637504 16855 net.cpp:245] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0731 18:11:06.637512 16855 net.cpp:252] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 2 128 80 80 (1638400)
I0731 18:11:06.637521 16855 net.cpp:252] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 2 128 80 80 (1638400)
I0731 18:11:06.637526 16855 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0731 18:11:06.637533 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.637554 16855 net.cpp:184] Created Layer pool3 (24)
I0731 18:11:06.637560 16855 net.cpp:561] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0731 18:11:06.637567 16855 net.cpp:530] pool3 -> pool3
I0731 18:11:06.637673 16855 net.cpp:245] Setting up pool3
I0731 18:11:06.637682 16855 net.cpp:252] TEST Top shape for layer 24 'pool3' 2 128 40 40 (409600)
I0731 18:11:06.637689 16855 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0731 18:11:06.637696 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.637711 16855 net.cpp:184] Created Layer res4a_branch2a (25)
I0731 18:11:06.637717 16855 net.cpp:561] res4a_branch2a <- pool3
I0731 18:11:06.637722 16855 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0731 18:11:06.651975 16855 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.99G, req 0G)
I0731 18:11:06.651989 16855 net.cpp:245] Setting up res4a_branch2a
I0731 18:11:06.651995 16855 net.cpp:252] TEST Top shape for layer 25 'res4a_branch2a' 2 256 40 40 (819200)
I0731 18:11:06.652005 16855 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0731 18:11:06.652010 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.652025 16855 net.cpp:184] Created Layer res4a_branch2a/bn (26)
I0731 18:11:06.652030 16855 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0731 18:11:06.652035 16855 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0731 18:11:06.652765 16855 net.cpp:245] Setting up res4a_branch2a/bn
I0731 18:11:06.652775 16855 net.cpp:252] TEST Top shape for layer 26 'res4a_branch2a/bn' 2 256 40 40 (819200)
I0731 18:11:06.652783 16855 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0731 18:11:06.652787 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.652793 16855 net.cpp:184] Created Layer res4a_branch2a/relu (27)
I0731 18:11:06.652797 16855 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0731 18:11:06.652801 16855 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0731 18:11:06.652822 16855 net.cpp:245] Setting up res4a_branch2a/relu
I0731 18:11:06.652828 16855 net.cpp:252] TEST Top shape for layer 27 'res4a_branch2a/relu' 2 256 40 40 (819200)
I0731 18:11:06.652833 16855 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0731 18:11:06.652838 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.652851 16855 net.cpp:184] Created Layer res4a_branch2b (28)
I0731 18:11:06.652855 16855 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0731 18:11:06.652869 16855 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0731 18:11:06.660167 16855 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.98G, req 0G)
I0731 18:11:06.660178 16855 net.cpp:245] Setting up res4a_branch2b
I0731 18:11:06.660184 16855 net.cpp:252] TEST Top shape for layer 28 'res4a_branch2b' 2 256 40 40 (819200)
I0731 18:11:06.660192 16855 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0731 18:11:06.660197 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.660204 16855 net.cpp:184] Created Layer res4a_branch2b/bn (29)
I0731 18:11:06.660209 16855 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0731 18:11:06.660213 16855 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0731 18:11:06.660923 16855 net.cpp:245] Setting up res4a_branch2b/bn
I0731 18:11:06.660930 16855 net.cpp:252] TEST Top shape for layer 29 'res4a_branch2b/bn' 2 256 40 40 (819200)
I0731 18:11:06.660939 16855 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0731 18:11:06.660943 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.660948 16855 net.cpp:184] Created Layer res4a_branch2b/relu (30)
I0731 18:11:06.660953 16855 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0731 18:11:06.660956 16855 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0731 18:11:06.660962 16855 net.cpp:245] Setting up res4a_branch2b/relu
I0731 18:11:06.660967 16855 net.cpp:252] TEST Top shape for layer 30 'res4a_branch2b/relu' 2 256 40 40 (819200)
I0731 18:11:06.660971 16855 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0731 18:11:06.660976 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.660982 16855 net.cpp:184] Created Layer pool4 (31)
I0731 18:11:06.660986 16855 net.cpp:561] pool4 <- res4a_branch2b
I0731 18:11:06.660991 16855 net.cpp:530] pool4 -> pool4
I0731 18:11:06.661069 16855 net.cpp:245] Setting up pool4
I0731 18:11:06.661075 16855 net.cpp:252] TEST Top shape for layer 31 'pool4' 2 256 40 40 (819200)
I0731 18:11:06.661080 16855 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0731 18:11:06.661084 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.661099 16855 net.cpp:184] Created Layer res5a_branch2a (32)
I0731 18:11:06.661103 16855 net.cpp:561] res5a_branch2a <- pool4
I0731 18:11:06.661108 16855 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0731 18:11:06.686245 16855 net.cpp:245] Setting up res5a_branch2a
I0731 18:11:06.686269 16855 net.cpp:252] TEST Top shape for layer 32 'res5a_branch2a' 2 512 40 40 (1638400)
I0731 18:11:06.686278 16855 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0731 18:11:06.686283 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.686295 16855 net.cpp:184] Created Layer res5a_branch2a/bn (33)
I0731 18:11:06.686300 16855 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0731 18:11:06.686305 16855 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0731 18:11:06.687000 16855 net.cpp:245] Setting up res5a_branch2a/bn
I0731 18:11:06.687007 16855 net.cpp:252] TEST Top shape for layer 33 'res5a_branch2a/bn' 2 512 40 40 (1638400)
I0731 18:11:06.687017 16855 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0731 18:11:06.687022 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.687028 16855 net.cpp:184] Created Layer res5a_branch2a/relu (34)
I0731 18:11:06.687032 16855 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0731 18:11:06.687036 16855 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0731 18:11:06.687043 16855 net.cpp:245] Setting up res5a_branch2a/relu
I0731 18:11:06.687048 16855 net.cpp:252] TEST Top shape for layer 34 'res5a_branch2a/relu' 2 512 40 40 (1638400)
I0731 18:11:06.687060 16855 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0731 18:11:06.687065 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.687074 16855 net.cpp:184] Created Layer res5a_branch2b (35)
I0731 18:11:06.687078 16855 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0731 18:11:06.687083 16855 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0731 18:11:06.700268 16855 net.cpp:245] Setting up res5a_branch2b
I0731 18:11:06.700292 16855 net.cpp:252] TEST Top shape for layer 35 'res5a_branch2b' 2 512 40 40 (1638400)
I0731 18:11:06.700314 16855 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0731 18:11:06.700320 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.700330 16855 net.cpp:184] Created Layer res5a_branch2b/bn (36)
I0731 18:11:06.700335 16855 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0731 18:11:06.700340 16855 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0731 18:11:06.701050 16855 net.cpp:245] Setting up res5a_branch2b/bn
I0731 18:11:06.701057 16855 net.cpp:252] TEST Top shape for layer 36 'res5a_branch2b/bn' 2 512 40 40 (1638400)
I0731 18:11:06.701066 16855 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0731 18:11:06.701071 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.701076 16855 net.cpp:184] Created Layer res5a_branch2b/relu (37)
I0731 18:11:06.701081 16855 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0731 18:11:06.701086 16855 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0731 18:11:06.701092 16855 net.cpp:245] Setting up res5a_branch2b/relu
I0731 18:11:06.701097 16855 net.cpp:252] TEST Top shape for layer 37 'res5a_branch2b/relu' 2 512 40 40 (1638400)
I0731 18:11:06.701100 16855 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I0731 18:11:06.701105 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.701118 16855 net.cpp:184] Created Layer out5a (38)
I0731 18:11:06.701122 16855 net.cpp:561] out5a <- res5a_branch2b
I0731 18:11:06.701128 16855 net.cpp:530] out5a -> out5a
I0731 18:11:06.704458 16855 net.cpp:245] Setting up out5a
I0731 18:11:06.704468 16855 net.cpp:252] TEST Top shape for layer 38 'out5a' 2 64 40 40 (204800)
I0731 18:11:06.704474 16855 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I0731 18:11:06.704479 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.704486 16855 net.cpp:184] Created Layer out5a/bn (39)
I0731 18:11:06.704490 16855 net.cpp:561] out5a/bn <- out5a
I0731 18:11:06.704494 16855 net.cpp:513] out5a/bn -> out5a (in-place)
I0731 18:11:06.705200 16855 net.cpp:245] Setting up out5a/bn
I0731 18:11:06.705209 16855 net.cpp:252] TEST Top shape for layer 39 'out5a/bn' 2 64 40 40 (204800)
I0731 18:11:06.705217 16855 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I0731 18:11:06.705221 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.705226 16855 net.cpp:184] Created Layer out5a/relu (40)
I0731 18:11:06.705231 16855 net.cpp:561] out5a/relu <- out5a
I0731 18:11:06.705235 16855 net.cpp:513] out5a/relu -> out5a (in-place)
I0731 18:11:06.705242 16855 net.cpp:245] Setting up out5a/relu
I0731 18:11:06.705246 16855 net.cpp:252] TEST Top shape for layer 40 'out5a/relu' 2 64 40 40 (204800)
I0731 18:11:06.705251 16855 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I0731 18:11:06.705255 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.705263 16855 net.cpp:184] Created Layer out5a_up2 (41)
I0731 18:11:06.705266 16855 net.cpp:561] out5a_up2 <- out5a
I0731 18:11:06.705271 16855 net.cpp:530] out5a_up2 -> out5a_up2
I0731 18:11:06.705657 16855 net.cpp:245] Setting up out5a_up2
I0731 18:11:06.705665 16855 net.cpp:252] TEST Top shape for layer 41 'out5a_up2' 2 64 80 80 (819200)
I0731 18:11:06.705672 16855 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I0731 18:11:06.705675 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.705683 16855 net.cpp:184] Created Layer out3a (42)
I0731 18:11:06.705688 16855 net.cpp:561] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0731 18:11:06.705693 16855 net.cpp:530] out3a -> out3a
I0731 18:11:06.710281 16855 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.97G, req 0G)
I0731 18:11:06.710294 16855 net.cpp:245] Setting up out3a
I0731 18:11:06.710300 16855 net.cpp:252] TEST Top shape for layer 42 'out3a' 2 64 80 80 (819200)
I0731 18:11:06.710309 16855 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I0731 18:11:06.710314 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.710320 16855 net.cpp:184] Created Layer out3a/bn (43)
I0731 18:11:06.710325 16855 net.cpp:561] out3a/bn <- out3a
I0731 18:11:06.710330 16855 net.cpp:513] out3a/bn -> out3a (in-place)
I0731 18:11:06.711061 16855 net.cpp:245] Setting up out3a/bn
I0731 18:11:06.711069 16855 net.cpp:252] TEST Top shape for layer 43 'out3a/bn' 2 64 80 80 (819200)
I0731 18:11:06.711078 16855 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I0731 18:11:06.711082 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.711087 16855 net.cpp:184] Created Layer out3a/relu (44)
I0731 18:11:06.711091 16855 net.cpp:561] out3a/relu <- out3a
I0731 18:11:06.711096 16855 net.cpp:513] out3a/relu -> out3a (in-place)
I0731 18:11:06.711102 16855 net.cpp:245] Setting up out3a/relu
I0731 18:11:06.711105 16855 net.cpp:252] TEST Top shape for layer 44 'out3a/relu' 2 64 80 80 (819200)
I0731 18:11:06.711110 16855 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I0731 18:11:06.711113 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.711119 16855 net.cpp:184] Created Layer out3_out5_combined (45)
I0731 18:11:06.711123 16855 net.cpp:561] out3_out5_combined <- out5a_up2
I0731 18:11:06.711127 16855 net.cpp:561] out3_out5_combined <- out3a
I0731 18:11:06.711133 16855 net.cpp:530] out3_out5_combined -> out3_out5_combined
I0731 18:11:06.712039 16855 net.cpp:245] Setting up out3_out5_combined
I0731 18:11:06.712049 16855 net.cpp:252] TEST Top shape for layer 45 'out3_out5_combined' 2 64 80 80 (819200)
I0731 18:11:06.712054 16855 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I0731 18:11:06.712059 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.712066 16855 net.cpp:184] Created Layer ctx_conv1 (46)
I0731 18:11:06.712070 16855 net.cpp:561] ctx_conv1 <- out3_out5_combined
I0731 18:11:06.712076 16855 net.cpp:530] ctx_conv1 -> ctx_conv1
I0731 18:11:06.716709 16855 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.96G, req 0G)
I0731 18:11:06.716722 16855 net.cpp:245] Setting up ctx_conv1
I0731 18:11:06.716727 16855 net.cpp:252] TEST Top shape for layer 46 'ctx_conv1' 2 64 80 80 (819200)
I0731 18:11:06.716734 16855 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I0731 18:11:06.716739 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.716751 16855 net.cpp:184] Created Layer ctx_conv1/bn (47)
I0731 18:11:06.716756 16855 net.cpp:561] ctx_conv1/bn <- ctx_conv1
I0731 18:11:06.716760 16855 net.cpp:513] ctx_conv1/bn -> ctx_conv1 (in-place)
I0731 18:11:06.717494 16855 net.cpp:245] Setting up ctx_conv1/bn
I0731 18:11:06.717502 16855 net.cpp:252] TEST Top shape for layer 47 'ctx_conv1/bn' 2 64 80 80 (819200)
I0731 18:11:06.717511 16855 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I0731 18:11:06.717523 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.717530 16855 net.cpp:184] Created Layer ctx_conv1/relu (48)
I0731 18:11:06.717535 16855 net.cpp:561] ctx_conv1/relu <- ctx_conv1
I0731 18:11:06.717540 16855 net.cpp:513] ctx_conv1/relu -> ctx_conv1 (in-place)
I0731 18:11:06.717545 16855 net.cpp:245] Setting up ctx_conv1/relu
I0731 18:11:06.717550 16855 net.cpp:252] TEST Top shape for layer 48 'ctx_conv1/relu' 2 64 80 80 (819200)
I0731 18:11:06.717555 16855 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I0731 18:11:06.717559 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.717567 16855 net.cpp:184] Created Layer ctx_conv2 (49)
I0731 18:11:06.717571 16855 net.cpp:561] ctx_conv2 <- ctx_conv1
I0731 18:11:06.717576 16855 net.cpp:530] ctx_conv2 -> ctx_conv2
I0731 18:11:06.718694 16855 net.cpp:245] Setting up ctx_conv2
I0731 18:11:06.718703 16855 net.cpp:252] TEST Top shape for layer 49 'ctx_conv2' 2 64 80 80 (819200)
I0731 18:11:06.718710 16855 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I0731 18:11:06.718714 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.718720 16855 net.cpp:184] Created Layer ctx_conv2/bn (50)
I0731 18:11:06.718724 16855 net.cpp:561] ctx_conv2/bn <- ctx_conv2
I0731 18:11:06.718729 16855 net.cpp:513] ctx_conv2/bn -> ctx_conv2 (in-place)
I0731 18:11:06.719440 16855 net.cpp:245] Setting up ctx_conv2/bn
I0731 18:11:06.719449 16855 net.cpp:252] TEST Top shape for layer 50 'ctx_conv2/bn' 2 64 80 80 (819200)
I0731 18:11:06.719458 16855 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I0731 18:11:06.719462 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.719467 16855 net.cpp:184] Created Layer ctx_conv2/relu (51)
I0731 18:11:06.719471 16855 net.cpp:561] ctx_conv2/relu <- ctx_conv2
I0731 18:11:06.719475 16855 net.cpp:513] ctx_conv2/relu -> ctx_conv2 (in-place)
I0731 18:11:06.719481 16855 net.cpp:245] Setting up ctx_conv2/relu
I0731 18:11:06.719486 16855 net.cpp:252] TEST Top shape for layer 51 'ctx_conv2/relu' 2 64 80 80 (819200)
I0731 18:11:06.719491 16855 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I0731 18:11:06.719494 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.719502 16855 net.cpp:184] Created Layer ctx_conv3 (52)
I0731 18:11:06.719506 16855 net.cpp:561] ctx_conv3 <- ctx_conv2
I0731 18:11:06.719511 16855 net.cpp:530] ctx_conv3 -> ctx_conv3
I0731 18:11:06.720630 16855 net.cpp:245] Setting up ctx_conv3
I0731 18:11:06.720638 16855 net.cpp:252] TEST Top shape for layer 52 'ctx_conv3' 2 64 80 80 (819200)
I0731 18:11:06.720644 16855 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I0731 18:11:06.720649 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.720654 16855 net.cpp:184] Created Layer ctx_conv3/bn (53)
I0731 18:11:06.720659 16855 net.cpp:561] ctx_conv3/bn <- ctx_conv3
I0731 18:11:06.720664 16855 net.cpp:513] ctx_conv3/bn -> ctx_conv3 (in-place)
I0731 18:11:06.721369 16855 net.cpp:245] Setting up ctx_conv3/bn
I0731 18:11:06.721376 16855 net.cpp:252] TEST Top shape for layer 53 'ctx_conv3/bn' 2 64 80 80 (819200)
I0731 18:11:06.721385 16855 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I0731 18:11:06.721388 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.721395 16855 net.cpp:184] Created Layer ctx_conv3/relu (54)
I0731 18:11:06.721398 16855 net.cpp:561] ctx_conv3/relu <- ctx_conv3
I0731 18:11:06.721402 16855 net.cpp:513] ctx_conv3/relu -> ctx_conv3 (in-place)
I0731 18:11:06.721408 16855 net.cpp:245] Setting up ctx_conv3/relu
I0731 18:11:06.721415 16855 net.cpp:252] TEST Top shape for layer 54 'ctx_conv3/relu' 2 64 80 80 (819200)
I0731 18:11:06.721423 16855 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I0731 18:11:06.721428 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.721441 16855 net.cpp:184] Created Layer ctx_conv4 (55)
I0731 18:11:06.721444 16855 net.cpp:561] ctx_conv4 <- ctx_conv3
I0731 18:11:06.721448 16855 net.cpp:530] ctx_conv4 -> ctx_conv4
I0731 18:11:06.722558 16855 net.cpp:245] Setting up ctx_conv4
I0731 18:11:06.722565 16855 net.cpp:252] TEST Top shape for layer 55 'ctx_conv4' 2 64 80 80 (819200)
I0731 18:11:06.722573 16855 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I0731 18:11:06.722578 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.722584 16855 net.cpp:184] Created Layer ctx_conv4/bn (56)
I0731 18:11:06.722589 16855 net.cpp:561] ctx_conv4/bn <- ctx_conv4
I0731 18:11:06.722592 16855 net.cpp:513] ctx_conv4/bn -> ctx_conv4 (in-place)
I0731 18:11:06.723363 16855 net.cpp:245] Setting up ctx_conv4/bn
I0731 18:11:06.723372 16855 net.cpp:252] TEST Top shape for layer 56 'ctx_conv4/bn' 2 64 80 80 (819200)
I0731 18:11:06.723381 16855 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I0731 18:11:06.723384 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.723389 16855 net.cpp:184] Created Layer ctx_conv4/relu (57)
I0731 18:11:06.723393 16855 net.cpp:561] ctx_conv4/relu <- ctx_conv4
I0731 18:11:06.723398 16855 net.cpp:513] ctx_conv4/relu -> ctx_conv4 (in-place)
I0731 18:11:06.723404 16855 net.cpp:245] Setting up ctx_conv4/relu
I0731 18:11:06.723409 16855 net.cpp:252] TEST Top shape for layer 57 'ctx_conv4/relu' 2 64 80 80 (819200)
I0731 18:11:06.723413 16855 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I0731 18:11:06.723417 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.723426 16855 net.cpp:184] Created Layer ctx_final (58)
I0731 18:11:06.723430 16855 net.cpp:561] ctx_final <- ctx_conv4
I0731 18:11:06.723434 16855 net.cpp:530] ctx_final -> ctx_final
I0731 18:11:06.727973 16855 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.95G, req 0G)
I0731 18:11:06.727993 16855 net.cpp:245] Setting up ctx_final
I0731 18:11:06.728001 16855 net.cpp:252] TEST Top shape for layer 58 'ctx_final' 2 8 80 80 (102400)
I0731 18:11:06.728011 16855 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I0731 18:11:06.728016 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.728025 16855 net.cpp:184] Created Layer ctx_final/relu (59)
I0731 18:11:06.728030 16855 net.cpp:561] ctx_final/relu <- ctx_final
I0731 18:11:06.728039 16855 net.cpp:513] ctx_final/relu -> ctx_final (in-place)
I0731 18:11:06.728057 16855 net.cpp:245] Setting up ctx_final/relu
I0731 18:11:06.728062 16855 net.cpp:252] TEST Top shape for layer 59 'ctx_final/relu' 2 8 80 80 (102400)
I0731 18:11:06.728066 16855 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I0731 18:11:06.728070 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.728080 16855 net.cpp:184] Created Layer out_deconv_final_up2 (60)
I0731 18:11:06.728085 16855 net.cpp:561] out_deconv_final_up2 <- ctx_final
I0731 18:11:06.728090 16855 net.cpp:530] out_deconv_final_up2 -> out_deconv_final_up2
I0731 18:11:06.728484 16855 net.cpp:245] Setting up out_deconv_final_up2
I0731 18:11:06.728492 16855 net.cpp:252] TEST Top shape for layer 60 'out_deconv_final_up2' 2 8 160 160 (409600)
I0731 18:11:06.728498 16855 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I0731 18:11:06.728502 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.728510 16855 net.cpp:184] Created Layer out_deconv_final_up4 (61)
I0731 18:11:06.728523 16855 net.cpp:561] out_deconv_final_up4 <- out_deconv_final_up2
I0731 18:11:06.728528 16855 net.cpp:530] out_deconv_final_up4 -> out_deconv_final_up4
I0731 18:11:06.728849 16855 net.cpp:245] Setting up out_deconv_final_up4
I0731 18:11:06.728857 16855 net.cpp:252] TEST Top shape for layer 61 'out_deconv_final_up4' 2 8 320 320 (1638400)
I0731 18:11:06.728863 16855 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I0731 18:11:06.728868 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.728881 16855 net.cpp:184] Created Layer out_deconv_final_up8 (62)
I0731 18:11:06.728885 16855 net.cpp:561] out_deconv_final_up8 <- out_deconv_final_up4
I0731 18:11:06.728890 16855 net.cpp:530] out_deconv_final_up8 -> out_deconv_final_up8
I0731 18:11:06.729189 16855 net.cpp:245] Setting up out_deconv_final_up8
I0731 18:11:06.729197 16855 net.cpp:252] TEST Top shape for layer 62 'out_deconv_final_up8' 2 8 640 640 (6553600)
I0731 18:11:06.729202 16855 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8_out_deconv_final_up8_0_split' of type 'Split'
I0731 18:11:06.729207 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.729212 16855 net.cpp:184] Created Layer out_deconv_final_up8_out_deconv_final_up8_0_split (63)
I0731 18:11:06.729216 16855 net.cpp:561] out_deconv_final_up8_out_deconv_final_up8_0_split <- out_deconv_final_up8
I0731 18:11:06.729220 16855 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0731 18:11:06.729226 16855 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0731 18:11:06.729231 16855 net.cpp:530] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0731 18:11:06.729298 16855 net.cpp:245] Setting up out_deconv_final_up8_out_deconv_final_up8_0_split
I0731 18:11:06.729303 16855 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 8 640 640 (6553600)
I0731 18:11:06.729310 16855 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 8 640 640 (6553600)
I0731 18:11:06.729315 16855 net.cpp:252] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 8 640 640 (6553600)
I0731 18:11:06.729318 16855 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0731 18:11:06.729322 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.729331 16855 net.cpp:184] Created Layer loss (64)
I0731 18:11:06.729336 16855 net.cpp:561] loss <- out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0731 18:11:06.729339 16855 net.cpp:561] loss <- label_data_1_split_0
I0731 18:11:06.729346 16855 net.cpp:530] loss -> loss
I0731 18:11:06.730235 16855 net.cpp:245] Setting up loss
I0731 18:11:06.730244 16855 net.cpp:252] TEST Top shape for layer 64 'loss' (1)
I0731 18:11:06.730248 16855 net.cpp:256]     with loss weight 1
I0731 18:11:06.730255 16855 layer_factory.hpp:136] Creating layer 'accuracy/top1' of type 'Accuracy'
I0731 18:11:06.730260 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.730270 16855 net.cpp:184] Created Layer accuracy/top1 (65)
I0731 18:11:06.730274 16855 net.cpp:561] accuracy/top1 <- out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0731 18:11:06.730279 16855 net.cpp:561] accuracy/top1 <- label_data_1_split_1
I0731 18:11:06.730284 16855 net.cpp:530] accuracy/top1 -> accuracy/top1
I0731 18:11:06.730293 16855 net.cpp:245] Setting up accuracy/top1
I0731 18:11:06.730296 16855 net.cpp:252] TEST Top shape for layer 65 'accuracy/top1' (1)
I0731 18:11:06.730301 16855 layer_factory.hpp:136] Creating layer 'accuracy/top5' of type 'Accuracy'
I0731 18:11:06.730305 16855 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0731 18:11:06.730317 16855 net.cpp:184] Created Layer accuracy/top5 (66)
I0731 18:11:06.730321 16855 net.cpp:561] accuracy/top5 <- out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0731 18:11:06.730326 16855 net.cpp:561] accuracy/top5 <- label_data_1_split_2
I0731 18:11:06.730331 16855 net.cpp:530] accuracy/top5 -> accuracy/top5
I0731 18:11:06.730339 16855 net.cpp:245] Setting up accuracy/top5
I0731 18:11:06.730342 16855 net.cpp:252] TEST Top shape for layer 66 'accuracy/top5' (1)
I0731 18:11:06.730347 16855 net.cpp:325] accuracy/top5 does not need backward computation.
I0731 18:11:06.730351 16855 net.cpp:325] accuracy/top1 does not need backward computation.
I0731 18:11:06.730355 16855 net.cpp:323] loss needs backward computation.
I0731 18:11:06.730360 16855 net.cpp:323] out_deconv_final_up8_out_deconv_final_up8_0_split needs backward computation.
I0731 18:11:06.730365 16855 net.cpp:323] out_deconv_final_up8 needs backward computation.
I0731 18:11:06.730368 16855 net.cpp:323] out_deconv_final_up4 needs backward computation.
I0731 18:11:06.730372 16855 net.cpp:323] out_deconv_final_up2 needs backward computation.
I0731 18:11:06.730377 16855 net.cpp:323] ctx_final/relu needs backward computation.
I0731 18:11:06.730381 16855 net.cpp:323] ctx_final needs backward computation.
I0731 18:11:06.730386 16855 net.cpp:323] ctx_conv4/relu needs backward computation.
I0731 18:11:06.730389 16855 net.cpp:323] ctx_conv4/bn needs backward computation.
I0731 18:11:06.730393 16855 net.cpp:323] ctx_conv4 needs backward computation.
I0731 18:11:06.730397 16855 net.cpp:323] ctx_conv3/relu needs backward computation.
I0731 18:11:06.730401 16855 net.cpp:323] ctx_conv3/bn needs backward computation.
I0731 18:11:06.730406 16855 net.cpp:323] ctx_conv3 needs backward computation.
I0731 18:11:06.730409 16855 net.cpp:323] ctx_conv2/relu needs backward computation.
I0731 18:11:06.730412 16855 net.cpp:323] ctx_conv2/bn needs backward computation.
I0731 18:11:06.730417 16855 net.cpp:323] ctx_conv2 needs backward computation.
I0731 18:11:06.730420 16855 net.cpp:323] ctx_conv1/relu needs backward computation.
I0731 18:11:06.730425 16855 net.cpp:323] ctx_conv1/bn needs backward computation.
I0731 18:11:06.730429 16855 net.cpp:323] ctx_conv1 needs backward computation.
I0731 18:11:06.730433 16855 net.cpp:323] out3_out5_combined needs backward computation.
I0731 18:11:06.730438 16855 net.cpp:323] out3a/relu needs backward computation.
I0731 18:11:06.730443 16855 net.cpp:323] out3a/bn needs backward computation.
I0731 18:11:06.730446 16855 net.cpp:323] out3a needs backward computation.
I0731 18:11:06.730450 16855 net.cpp:323] out5a_up2 needs backward computation.
I0731 18:11:06.730454 16855 net.cpp:323] out5a/relu needs backward computation.
I0731 18:11:06.730458 16855 net.cpp:323] out5a/bn needs backward computation.
I0731 18:11:06.730463 16855 net.cpp:323] out5a needs backward computation.
I0731 18:11:06.730468 16855 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0731 18:11:06.730471 16855 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0731 18:11:06.730474 16855 net.cpp:323] res5a_branch2b needs backward computation.
I0731 18:11:06.730479 16855 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0731 18:11:06.730482 16855 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0731 18:11:06.730486 16855 net.cpp:323] res5a_branch2a needs backward computation.
I0731 18:11:06.730490 16855 net.cpp:323] pool4 needs backward computation.
I0731 18:11:06.730495 16855 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0731 18:11:06.730499 16855 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0731 18:11:06.730502 16855 net.cpp:323] res4a_branch2b needs backward computation.
I0731 18:11:06.730506 16855 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0731 18:11:06.730510 16855 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0731 18:11:06.730515 16855 net.cpp:323] res4a_branch2a needs backward computation.
I0731 18:11:06.730518 16855 net.cpp:323] pool3 needs backward computation.
I0731 18:11:06.730527 16855 net.cpp:323] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I0731 18:11:06.730531 16855 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0731 18:11:06.730536 16855 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0731 18:11:06.730540 16855 net.cpp:323] res3a_branch2b needs backward computation.
I0731 18:11:06.730545 16855 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0731 18:11:06.730548 16855 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0731 18:11:06.730552 16855 net.cpp:323] res3a_branch2a needs backward computation.
I0731 18:11:06.730556 16855 net.cpp:323] pool2 needs backward computation.
I0731 18:11:06.730561 16855 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0731 18:11:06.730566 16855 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0731 18:11:06.730569 16855 net.cpp:323] res2a_branch2b needs backward computation.
I0731 18:11:06.730573 16855 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0731 18:11:06.730577 16855 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0731 18:11:06.730581 16855 net.cpp:323] res2a_branch2a needs backward computation.
I0731 18:11:06.730587 16855 net.cpp:323] pool1 needs backward computation.
I0731 18:11:06.730590 16855 net.cpp:323] conv1b/relu needs backward computation.
I0731 18:11:06.730594 16855 net.cpp:323] conv1b/bn needs backward computation.
I0731 18:11:06.730598 16855 net.cpp:323] conv1b needs backward computation.
I0731 18:11:06.730602 16855 net.cpp:323] conv1a/relu needs backward computation.
I0731 18:11:06.730607 16855 net.cpp:323] conv1a/bn needs backward computation.
I0731 18:11:06.730612 16855 net.cpp:323] conv1a needs backward computation.
I0731 18:11:06.730615 16855 net.cpp:325] data/bias does not need backward computation.
I0731 18:11:06.730620 16855 net.cpp:325] label_data_1_split does not need backward computation.
I0731 18:11:06.730625 16855 net.cpp:325] data does not need backward computation.
I0731 18:11:06.730629 16855 net.cpp:367] This network produces output accuracy/top1
I0731 18:11:06.730633 16855 net.cpp:367] This network produces output accuracy/top5
I0731 18:11:06.730638 16855 net.cpp:367] This network produces output loss
I0731 18:11:06.730685 16855 net.cpp:389] Top memory (TEST) required for data: 318668800 diff: 8
I0731 18:11:06.730690 16855 net.cpp:392] Bottom memory (TEST) required for data: 318668800 diff: 318668800
I0731 18:11:06.730692 16855 net.cpp:395] Shared (in-place) memory (TEST) by data: 210124800 diff: 210124800
I0731 18:11:06.730696 16855 net.cpp:398] Parameters memory (TEST) required for data: 2692608 diff: 2692608
I0731 18:11:06.730700 16855 net.cpp:401] Parameters shared memory (TEST) by data: 0 diff: 0
I0731 18:11:06.730705 16855 net.cpp:407] Network initialization done.
I0731 18:11:06.730787 16855 solver.cpp:56] Solver scaffolding done.
I0731 18:11:06.740223 16855 caffe.cpp:137] Finetuning from training/imagenet_jacintonet11v2_iter_320000.caffemodel
I0731 18:11:06.751122 16855 net.cpp:1089] Copying source layer data Type:Data #blobs=0
I0731 18:11:06.751142 16855 net.cpp:1089] Copying source layer data/bias Type:Bias #blobs=1
I0731 18:11:06.751150 16855 net.cpp:1089] Copying source layer conv1a Type:Convolution #blobs=2
I0731 18:11:06.751191 16855 net.cpp:1089] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0731 18:11:06.751718 16855 net.cpp:1102] BN legacy DIGITS format detected ... 
I0731 18:11:06.751725 16855 net.cpp:1108] BN Transforming to new format completed.
I0731 18:11:06.751729 16855 net.cpp:1089] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0731 18:11:06.751734 16855 net.cpp:1089] Copying source layer conv1b Type:Convolution #blobs=2
I0731 18:11:06.751773 16855 net.cpp:1089] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0731 18:11:06.752144 16855 net.cpp:1102] BN legacy DIGITS format detected ... 
I0731 18:11:06.752151 16855 net.cpp:1108] BN Transforming to new format completed.
I0731 18:11:06.752164 16855 net.cpp:1089] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0731 18:11:06.752169 16855 net.cpp:1089] Copying source layer pool1 Type:Pooling #blobs=0
I0731 18:11:06.752173 16855 net.cpp:1089] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0731 18:11:06.752426 16855 net.cpp:1089] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0731 18:11:06.752805 16855 net.cpp:1102] BN legacy DIGITS format detected ... 
I0731 18:11:06.752811 16855 net.cpp:1108] BN Transforming to new format completed.
I0731 18:11:06.752820 16855 net.cpp:1089] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0731 18:11:06.752822 16855 net.cpp:1089] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0731 18:11:06.752954 16855 net.cpp:1089] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0731 18:11:06.753329 16855 net.cpp:1102] BN legacy DIGITS format detected ... 
I0731 18:11:06.753335 16855 net.cpp:1108] BN Transforming to new format completed.
I0731 18:11:06.753338 16855 net.cpp:1089] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0731 18:11:06.753342 16855 net.cpp:1089] Copying source layer pool2 Type:Pooling #blobs=0
I0731 18:11:06.753346 16855 net.cpp:1089] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0731 18:11:06.754335 16855 net.cpp:1089] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0731 18:11:06.754683 16855 net.cpp:1102] BN legacy DIGITS format detected ... 
I0731 18:11:06.754688 16855 net.cpp:1108] BN Transforming to new format completed.
I0731 18:11:06.754693 16855 net.cpp:1089] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0731 18:11:06.754695 16855 net.cpp:1089] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0731 18:11:06.755192 16855 net.cpp:1089] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0731 18:11:06.755524 16855 net.cpp:1102] BN legacy DIGITS format detected ... 
I0731 18:11:06.755530 16855 net.cpp:1108] BN Transforming to new format completed.
I0731 18:11:06.755533 16855 net.cpp:1089] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0731 18:11:06.755537 16855 net.cpp:1089] Copying source layer pool3 Type:Pooling #blobs=0
I0731 18:11:06.755542 16855 net.cpp:1089] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0731 18:11:06.759474 16855 net.cpp:1089] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0731 18:11:06.759918 16855 net.cpp:1102] BN legacy DIGITS format detected ... 
I0731 18:11:06.759925 16855 net.cpp:1108] BN Transforming to new format completed.
I0731 18:11:06.759929 16855 net.cpp:1089] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0731 18:11:06.759933 16855 net.cpp:1089] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0731 18:11:06.761888 16855 net.cpp:1089] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0731 18:11:06.762243 16855 net.cpp:1102] BN legacy DIGITS format detected ... 
I0731 18:11:06.762249 16855 net.cpp:1108] BN Transforming to new format completed.
I0731 18:11:06.762253 16855 net.cpp:1089] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0731 18:11:06.762257 16855 net.cpp:1089] Copying source layer pool4 Type:Pooling #blobs=0
I0731 18:11:06.762262 16855 net.cpp:1089] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0731 18:11:06.777854 16855 net.cpp:1089] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0731 18:11:06.778228 16855 net.cpp:1102] BN legacy DIGITS format detected ... 
I0731 18:11:06.778234 16855 net.cpp:1108] BN Transforming to new format completed.
I0731 18:11:06.778237 16855 net.cpp:1089] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0731 18:11:06.778241 16855 net.cpp:1089] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0731 18:11:06.786031 16855 net.cpp:1089] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0731 18:11:06.786388 16855 net.cpp:1102] BN legacy DIGITS format detected ... 
I0731 18:11:06.786393 16855 net.cpp:1108] BN Transforming to new format completed.
I0731 18:11:06.786408 16855 net.cpp:1089] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0731 18:11:06.786413 16855 net.cpp:1073] Ignoring source layer pool5
I0731 18:11:06.786417 16855 net.cpp:1073] Ignoring source layer fc1000
I0731 18:11:06.786422 16855 net.cpp:1089] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0731 18:11:06.796341 16855 net.cpp:1089] Copying source layer data Type:Data #blobs=0
I0731 18:11:06.796360 16855 net.cpp:1089] Copying source layer data/bias Type:Bias #blobs=1
I0731 18:11:06.796367 16855 net.cpp:1089] Copying source layer conv1a Type:Convolution #blobs=2
I0731 18:11:06.796407 16855 net.cpp:1089] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0731 18:11:06.796942 16855 net.cpp:1102] BN legacy DIGITS format detected ... 
I0731 18:11:06.796949 16855 net.cpp:1108] BN Transforming to new format completed.
I0731 18:11:06.796952 16855 net.cpp:1089] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0731 18:11:06.796957 16855 net.cpp:1089] Copying source layer conv1b Type:Convolution #blobs=2
I0731 18:11:06.796994 16855 net.cpp:1089] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0731 18:11:06.797376 16855 net.cpp:1102] BN legacy DIGITS format detected ... 
I0731 18:11:06.797382 16855 net.cpp:1108] BN Transforming to new format completed.
I0731 18:11:06.797385 16855 net.cpp:1089] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0731 18:11:06.797389 16855 net.cpp:1089] Copying source layer pool1 Type:Pooling #blobs=0
I0731 18:11:06.797394 16855 net.cpp:1089] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0731 18:11:06.797646 16855 net.cpp:1089] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0731 18:11:06.798027 16855 net.cpp:1102] BN legacy DIGITS format detected ... 
I0731 18:11:06.798032 16855 net.cpp:1108] BN Transforming to new format completed.
I0731 18:11:06.798036 16855 net.cpp:1089] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0731 18:11:06.798039 16855 net.cpp:1089] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0731 18:11:06.798171 16855 net.cpp:1089] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0731 18:11:06.798545 16855 net.cpp:1102] BN legacy DIGITS format detected ... 
I0731 18:11:06.798552 16855 net.cpp:1108] BN Transforming to new format completed.
I0731 18:11:06.798555 16855 net.cpp:1089] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0731 18:11:06.798558 16855 net.cpp:1089] Copying source layer pool2 Type:Pooling #blobs=0
I0731 18:11:06.798563 16855 net.cpp:1089] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0731 18:11:06.799551 16855 net.cpp:1089] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0731 18:11:06.799901 16855 net.cpp:1102] BN legacy DIGITS format detected ... 
I0731 18:11:06.799907 16855 net.cpp:1108] BN Transforming to new format completed.
I0731 18:11:06.799911 16855 net.cpp:1089] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0731 18:11:06.799914 16855 net.cpp:1089] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0731 18:11:06.800415 16855 net.cpp:1089] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0731 18:11:06.800766 16855 net.cpp:1102] BN legacy DIGITS format detected ... 
I0731 18:11:06.800771 16855 net.cpp:1108] BN Transforming to new format completed.
I0731 18:11:06.800776 16855 net.cpp:1089] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0731 18:11:06.800779 16855 net.cpp:1089] Copying source layer pool3 Type:Pooling #blobs=0
I0731 18:11:06.800783 16855 net.cpp:1089] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0731 18:11:06.804702 16855 net.cpp:1089] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0731 18:11:06.805066 16855 net.cpp:1102] BN legacy DIGITS format detected ... 
I0731 18:11:06.805073 16855 net.cpp:1108] BN Transforming to new format completed.
I0731 18:11:06.805075 16855 net.cpp:1089] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0731 18:11:06.805088 16855 net.cpp:1089] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0731 18:11:06.807029 16855 net.cpp:1089] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0731 18:11:06.807382 16855 net.cpp:1102] BN legacy DIGITS format detected ... 
I0731 18:11:06.807387 16855 net.cpp:1108] BN Transforming to new format completed.
I0731 18:11:06.807391 16855 net.cpp:1089] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0731 18:11:06.807395 16855 net.cpp:1089] Copying source layer pool4 Type:Pooling #blobs=0
I0731 18:11:06.807399 16855 net.cpp:1089] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0731 18:11:06.823009 16855 net.cpp:1089] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0731 18:11:06.823470 16855 net.cpp:1102] BN legacy DIGITS format detected ... 
I0731 18:11:06.823477 16855 net.cpp:1108] BN Transforming to new format completed.
I0731 18:11:06.823482 16855 net.cpp:1089] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0731 18:11:06.823485 16855 net.cpp:1089] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0731 18:11:06.831284 16855 net.cpp:1089] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0731 18:11:06.831652 16855 net.cpp:1102] BN legacy DIGITS format detected ... 
I0731 18:11:06.831657 16855 net.cpp:1108] BN Transforming to new format completed.
I0731 18:11:06.831661 16855 net.cpp:1089] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0731 18:11:06.831665 16855 net.cpp:1073] Ignoring source layer pool5
I0731 18:11:06.831670 16855 net.cpp:1073] Ignoring source layer fc1000
I0731 18:11:06.831672 16855 net.cpp:1089] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0731 18:11:06.831761 16855 parallel.cpp:108] [0 - 0] P2pSync adding callback
I0731 18:11:06.831768 16855 parallel.cpp:108] [1 - 1] P2pSync adding callback
I0731 18:11:06.831773 16855 parallel.cpp:108] [2 - 2] P2pSync adding callback
I0731 18:11:06.831775 16855 parallel.cpp:61] Starting Optimization
I0731 18:11:06.831779 16855 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0731 18:11:06.831809 16855 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 18:11:06.831822 16855 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 18:11:06.832262 16895 device_alternate.hpp:116] NVML initialized on thread 135869095761664
I0731 18:11:06.844985 16895 common.cpp:583] NVML succeeded to set CPU affinity on device 0
I0731 18:11:06.845038 16897 device_alternate.hpp:116] NVML initialized on thread 135869078976256
I0731 18:11:06.846217 16897 common.cpp:583] NVML succeeded to set CPU affinity on device 2
I0731 18:11:06.846230 16896 device_alternate.hpp:116] NVML initialized on thread 135869087368960
I0731 18:11:06.846776 16896 common.cpp:583] NVML succeeded to set CPU affinity on device 1
I0731 18:11:06.851002 16897 solver.cpp:42] Solver data type: FLOAT
W0731 18:11:06.851804 16897 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0731 18:11:06.851963 16897 net.cpp:104] Using FLOAT as default forward math type
I0731 18:11:06.851971 16897 net.cpp:110] Using FLOAT as default backward math type
I0731 18:11:06.852011 16897 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0731 18:11:06.852021 16897 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 18:11:06.855191 16896 solver.cpp:42] Solver data type: FLOAT
W0731 18:11:06.855737 16896 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0731 18:11:06.855850 16896 net.cpp:104] Using FLOAT as default forward math type
I0731 18:11:06.855855 16896 net.cpp:110] Using FLOAT as default backward math type
I0731 18:11:06.855893 16896 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0731 18:11:06.855901 16896 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 18:11:06.855937 16898 db_lmdb.cpp:35] Opened lmdb data/train-image-lmdb
I0731 18:11:06.856840 16899 db_lmdb.cpp:35] Opened lmdb data/train-image-lmdb
I0731 18:11:06.859509 16897 data_layer.cpp:184] [2] ReshapePrefetch 6, 3, 640, 640
I0731 18:11:06.860684 16897 data_layer.cpp:208] [2] Output data size: 6, 3, 640, 640
I0731 18:11:06.860720 16897 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 18:11:06.860795 16897 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0731 18:11:06.860808 16897 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 18:11:06.861140 16896 data_layer.cpp:184] [1] ReshapePrefetch 6, 3, 640, 640
I0731 18:11:06.861230 16896 data_layer.cpp:208] [1] Output data size: 6, 3, 640, 640
I0731 18:11:06.861237 16896 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 18:11:06.861263 16896 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0731 18:11:06.861269 16896 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 18:11:06.861763 16900 data_layer.cpp:97] [2] Parser threads: 1
I0731 18:11:06.861768 16900 data_layer.cpp:99] [2] Transformer threads: 1
I0731 18:11:06.867401 16901 db_lmdb.cpp:35] Opened lmdb data/train-label-lmdb
I0731 18:11:06.868541 16902 data_layer.cpp:97] [1] Parser threads: 1
I0731 18:11:06.868577 16902 data_layer.cpp:99] [1] Transformer threads: 1
I0731 18:11:06.874500 16903 db_lmdb.cpp:35] Opened lmdb data/train-label-lmdb
I0731 18:11:06.874506 16897 data_layer.cpp:184] [2] ReshapePrefetch 6, 1, 640, 640
I0731 18:11:06.875553 16897 data_layer.cpp:208] [2] Output data size: 6, 1, 640, 640
I0731 18:11:06.875592 16897 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 18:11:06.877156 16896 data_layer.cpp:184] [1] ReshapePrefetch 6, 1, 640, 640
I0731 18:11:06.886016 16896 data_layer.cpp:208] [1] Output data size: 6, 1, 640, 640
I0731 18:11:06.886054 16896 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 18:11:06.886148 16904 data_layer.cpp:97] [2] Parser threads: 1
I0731 18:11:06.886157 16904 data_layer.cpp:99] [2] Transformer threads: 1
I0731 18:11:06.886183 16900 blocking_queue.cpp:40] Waiting for datum
I0731 18:11:06.894809 16905 data_layer.cpp:97] [1] Parser threads: 1
I0731 18:11:06.894873 16905 data_layer.cpp:99] [1] Transformer threads: 1
I0731 18:11:07.434801 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 7.99G, req 0G)
I0731 18:11:07.459107 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 7.99G, req 0G)
I0731 18:11:07.486738 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.83G, req 0G)
I0731 18:11:07.515368 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.83G, req 0G)
I0731 18:11:07.537058 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.7G, req 0G)
I0731 18:11:07.566838 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.7G, req 0G)
I0731 18:11:07.567260 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.62G, req 0G)
I0731 18:11:07.595706 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.62G, req 0G)
I0731 18:11:07.599392 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 7.56G, req 0G)
I0731 18:11:07.615047 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.52G, req 0G)
I0731 18:11:07.622081 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 7.56G, req 0G)
I0731 18:11:07.636879 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.52G, req 0G)
I0731 18:11:07.647147 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.48G, req 0G)
I0731 18:11:07.662684 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.46G, req 0G)
I0731 18:11:07.667522 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.48G, req 0G)
I0731 18:11:07.678479 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.46G, req 0G)
I0731 18:11:07.727372 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 7.39G, req 0G)
I0731 18:11:07.740222 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 7.39G, req 0G)
I0731 18:11:07.745641 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 7.34G, req 0G)
I0731 18:11:07.760699 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 7.34G, req 0G)
I0731 18:11:07.768312 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 7.32G, req 0G)
I0731 18:11:07.772894 16897 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/initial/test.prototxt
W0731 18:11:07.772974 16897 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0731 18:11:07.773118 16897 net.cpp:104] Using FLOAT as default forward math type
I0731 18:11:07.773123 16897 net.cpp:110] Using FLOAT as default backward math type
I0731 18:11:07.773149 16897 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0731 18:11:07.773154 16897 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 18:11:07.773926 16934 db_lmdb.cpp:35] Opened lmdb data/val-image-lmdb
I0731 18:11:07.775635 16897 data_layer.cpp:184] (2) ReshapePrefetch 2, 3, 640, 640
I0731 18:11:07.775709 16897 data_layer.cpp:208] (2) Output data size: 2, 3, 640, 640
I0731 18:11:07.775715 16897 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 18:11:07.775750 16897 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0731 18:11:07.775758 16897 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 18:11:07.777453 16936 db_lmdb.cpp:35] Opened lmdb data/val-label-lmdb
I0731 18:11:07.778421 16935 data_layer.cpp:97] (2) Parser threads: 1
I0731 18:11:07.778448 16935 data_layer.cpp:99] (2) Transformer threads: 1
I0731 18:11:07.781810 16897 data_layer.cpp:184] (2) ReshapePrefetch 2, 1, 640, 640
I0731 18:11:07.781919 16897 data_layer.cpp:208] (2) Output data size: 2, 1, 640, 640
I0731 18:11:07.781927 16897 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0731 18:11:07.783246 16937 data_layer.cpp:97] (2) Parser threads: 1
I0731 18:11:07.783255 16937 data_layer.cpp:99] (2) Transformer threads: 1
I0731 18:11:07.783886 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 7.32G, req 0G)
I0731 18:11:07.790647 16896 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/initial/test.prototxt
W0731 18:11:07.790725 16896 parallel.cpp:274] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0731 18:11:07.790853 16896 net.cpp:104] Using FLOAT as default forward math type
I0731 18:11:07.790858 16896 net.cpp:110] Using FLOAT as default backward math type
I0731 18:11:07.790880 16896 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0731 18:11:07.790885 16896 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 18:11:07.791796 16938 db_lmdb.cpp:35] Opened lmdb data/val-image-lmdb
I0731 18:11:07.793246 16896 data_layer.cpp:184] (1) ReshapePrefetch 2, 3, 640, 640
I0731 18:11:07.793371 16896 data_layer.cpp:208] (1) Output data size: 2, 3, 640, 640
I0731 18:11:07.793387 16896 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 18:11:07.793548 16896 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0731 18:11:07.793561 16896 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 18:11:07.794535 16897 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.18G, req 0G)
I0731 18:11:07.794718 16939 data_layer.cpp:97] (1) Parser threads: 1
I0731 18:11:07.794729 16939 data_layer.cpp:99] (1) Transformer threads: 1
I0731 18:11:07.797039 16940 db_lmdb.cpp:35] Opened lmdb data/val-label-lmdb
I0731 18:11:07.797806 16896 data_layer.cpp:184] (1) ReshapePrefetch 2, 1, 640, 640
I0731 18:11:07.797951 16896 data_layer.cpp:208] (1) Output data size: 2, 1, 640, 640
I0731 18:11:07.797961 16896 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0731 18:11:07.799612 16941 data_layer.cpp:97] (1) Parser threads: 1
I0731 18:11:07.799625 16941 data_layer.cpp:99] (1) Transformer threads: 1
I0731 18:11:07.810817 16896 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.18G, req 0G)
I0731 18:11:07.823050 16897 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.15G, req 0G)
I0731 18:11:07.827843 16896 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.15G, req 0G)
I0731 18:11:07.832900 16897 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.13G, req 0G)
I0731 18:11:07.837729 16896 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.13G, req 0G)
I0731 18:11:07.841496 16897 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.12G, req 0G)
I0731 18:11:07.846345 16896 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.12G, req 0G)
I0731 18:11:07.848808 16897 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.1G, req 0G)
I0731 18:11:07.854193 16896 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.1G, req 0G)
I0731 18:11:07.857270 16897 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.09G, req 0G)
I0731 18:11:07.860867 16896 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.09G, req 0G)
I0731 18:11:07.871522 16897 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.09G, req 0G)
I0731 18:11:07.874418 16896 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.09G, req 0G)
I0731 18:11:07.879724 16897 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.08G, req 0G)
I0731 18:11:07.882202 16896 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.08G, req 0G)
I0731 18:11:07.930487 16897 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 7.07G, req 0G)
I0731 18:11:07.934023 16896 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 7.07G, req 0G)
I0731 18:11:07.939203 16897 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 7.06G, req 0G)
I0731 18:11:07.941087 16896 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 7.06G, req 0G)
I0731 18:11:07.953794 16897 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 7.05G, req 0G)
I0731 18:11:07.956670 16896 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 7.05G, req 0G)
I0731 18:11:07.956921 16897 solver.cpp:56] Solver scaffolding done.
I0731 18:11:07.964550 16896 solver.cpp:56] Solver scaffolding done.
I0731 18:11:08.033459 16896 parallel.cpp:164] [1 - 1] P2pSync adding callback
I0731 18:11:08.033459 16895 parallel.cpp:164] [0 - 0] P2pSync adding callback
I0731 18:11:08.033459 16897 parallel.cpp:164] [2 - 2] P2pSync adding callback
I0731 18:11:08.254377 16897 solver.cpp:479] Solving jsegnet21v2_train
I0731 18:11:08.254395 16897 solver.cpp:480] Learning Rate Policy: multistep
I0731 18:11:08.254406 16895 solver.cpp:479] Solving jsegnet21v2_train
I0731 18:11:08.254413 16895 solver.cpp:480] Learning Rate Policy: multistep
I0731 18:11:08.254406 16896 solver.cpp:479] Solving jsegnet21v2_train
I0731 18:11:08.254422 16896 solver.cpp:480] Learning Rate Policy: multistep
I0731 18:11:08.268198 16896 solver.cpp:268] Starting Optimization on GPU 1
I0731 18:11:08.268199 16895 solver.cpp:268] Starting Optimization on GPU 0
I0731 18:11:08.268199 16897 solver.cpp:268] Starting Optimization on GPU 2
I0731 18:11:08.268409 16895 solver.cpp:550] Iteration 0, Testing net (#0)
I0731 18:11:08.268424 16958 device_alternate.hpp:116] NVML initialized on thread 127868910716672
I0731 18:11:08.268450 16958 common.cpp:583] NVML succeeded to set CPU affinity on device 2
I0731 18:11:08.269170 16959 device_alternate.hpp:116] NVML initialized on thread 127868919109376
I0731 18:11:08.269181 16959 common.cpp:583] NVML succeeded to set CPU affinity on device 0
I0731 18:11:08.269201 16960 device_alternate.hpp:116] NVML initialized on thread 127868927502080
I0731 18:11:08.269214 16960 common.cpp:583] NVML succeeded to set CPU affinity on device 1
I0731 18:11:08.282621 16897 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 6.96G, req 0G)
I0731 18:11:08.288322 16896 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 6.96G, req 0G)
I0731 18:11:08.304783 16897 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 6.9G, req 0G)
I0731 18:11:08.306720 16896 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 6.9G, req 0G)
I0731 18:11:08.320374 16897 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 6.84G, req 0G)
I0731 18:11:08.321465 16895 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1a' with space 0.01G/1 1  (limit 6.88G, req 0G)
I0731 18:11:08.325698 16896 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 6.84G, req 0G)
I0731 18:11:08.331784 16897 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 6.81G, req 0G)
I0731 18:11:08.339139 16896 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 6.81G, req 0G)
I0731 18:11:08.339395 16897 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 6.78G, req 0G)
I0731 18:11:08.344094 16895 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 6.82G, req 0G)
I0731 18:11:08.345793 16897 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 6.76G, req 0G)
I0731 18:11:08.347674 16896 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 6.78G, req 0G)
I0731 18:11:08.354610 16897 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.74G, req 0G)
I0731 18:11:08.354832 16896 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 6.76G, req 0G)
I0731 18:11:08.358342 16895 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 6.75G, req 0G)
I0731 18:11:08.359365 16897 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.74G, req 0G)
I0731 18:11:08.363245 16896 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.74G, req 0G)
I0731 18:11:08.367462 16896 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.74G, req 0G)
I0731 18:11:08.369386 16895 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 6.72G, req 0G)
I0731 18:11:08.378510 16895 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 6.69G, req 0G)
I0731 18:11:08.384770 16895 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 6.67G, req 0G)
I0731 18:11:08.387061 16897 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.58G, req 0G)
I0731 18:11:08.392993 16895 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 6.65G, req 0G)
I0731 18:11:08.393155 16897 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.57G, req 0G)
I0731 18:11:08.397418 16896 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.58G, req 0G)
I0731 18:11:08.397995 16895 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 6.65G, req 0G)
I0731 18:11:08.404460 16896 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.57G, req 0G)
I0731 18:11:08.421082 16897 cudnn_conv_layer.cpp:1011] (2) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.48G, req 0G)
I0731 18:11:08.426198 16895 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'out3a' with space 0.02G/2 6  (limit 6.49G, req 0G)
I0731 18:11:08.429194 16896 cudnn_conv_layer.cpp:1011] (1) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.48G, req 0G)
I0731 18:11:08.431536 16895 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'ctx_conv1' with space 0.02G/1 6  (limit 6.48G, req 0G)
I0731 18:11:08.460011 16895 cudnn_conv_layer.cpp:1011] (0) Conv Algo (F): 'ctx_final' with space 0.02G/1 6  (limit 6.39G, req 0G)
I0731 18:11:08.546785 16895 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.0484509
I0731 18:11:08.546823 16895 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.590535
I0731 18:11:08.546829 16895 solver.cpp:635]     Test net output #2: loss = 83.105 (* 1 = 83.105 loss)
I0731 18:11:08.546833 16895 solver.cpp:295] [MultiGPU] Initial Test completed
I0731 18:11:08.631512 16895 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 6.19G, req 0G)
I0731 18:11:08.634982 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 6.28G, req 0G)
I0731 18:11:08.642751 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 6.28G, req 0G)
I0731 18:11:08.683637 16895 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 6.03G, req 0G)
I0731 18:11:08.691367 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 6.12G, req 0G)
I0731 18:11:08.697335 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 6.12G, req 0G)
I0731 18:11:08.728037 16895 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 1 4 3  (limit 5.85G, req 0G)
I0731 18:11:08.741925 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 5.94G, req 0G)
I0731 18:11:08.745398 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 5.94G, req 0G)
I0731 18:11:08.750954 16895 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 5.77G, req 0G)
I0731 18:11:08.767264 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 5.86G, req 0G)
I0731 18:11:08.770375 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 5.86G, req 0G)
I0731 18:11:08.773150 16895 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 5.68G, req 0G)
I0731 18:11:08.786556 16895 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 5.64G, req 0G)
I0731 18:11:08.791337 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 5.77G, req 0G)
I0731 18:11:08.796172 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 5.77G, req 0G)
I0731 18:11:08.806862 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 5.72G, req 0G)
I0731 18:11:08.810760 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 5.72G, req 0G)
I0731 18:11:08.810948 16895 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 5.61G, req 0G)
I0731 18:11:08.819629 16895 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 1 4 3  (limit 5.59G, req 0G)
I0731 18:11:08.828474 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 5.7G, req 0G)
I0731 18:11:08.833760 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 5.7G, req 0G)
I0731 18:11:08.837771 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 5.68G, req 0G)
I0731 18:11:08.843185 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 1 3  (limit 5.68G, req 0G)
I0731 18:11:08.864863 16895 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 5.32G, req 0G)
I0731 18:11:08.879400 16895 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 5.3G, req 0G)
I0731 18:11:08.886741 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 5.41G, req 0G)
I0731 18:11:08.889319 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 5.41G, req 0G)
I0731 18:11:08.903465 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 5.39G, req 0G)
I0731 18:11:08.906509 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 5.39G, req 0G)
I0731 18:11:08.907308 16895 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 5.14G, req 0G)
I0731 18:11:08.937857 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 5.23G, req 0G)
I0731 18:11:08.941301 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 5.23G, req 0G)
I0731 18:11:09.123849 16895 solver.cpp:358] Iteration 0 (0.576975 s), loss = 2.16281
I0731 18:11:09.123868 16895 solver.cpp:375]     Train net output #0: loss = 2.16281 (* 1 = 2.16281 loss)
I0731 18:11:09.123873 16895 sgd_solver.cpp:136] Iteration 0, lr = 0.0001, m = 0.9
I0731 18:11:09.340369 16895 solver.cpp:358] Iteration 1 (0.216504 s), loss = 2.1386
I0731 18:11:09.340394 16895 solver.cpp:375]     Train net output #0: loss = 2.1386 (* 1 = 2.1386 loss)
I0731 18:11:09.443681 16895 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1a' with space 1.29G/1 1 0 3  (limit 2.99G, req 0G)
I0731 18:11:09.451325 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1a' with space 1.29G/1 1 0 3  (limit 3.08G, req 0G)
I0731 18:11:09.452286 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1a' with space 1.29G/1 1 0 3  (limit 3.08G, req 0G)
I0731 18:11:09.505254 16895 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'conv1b' with space 2.57G/2 6 4 3  (limit 1.7G, req 0G)
I0731 18:11:09.531277 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'conv1b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0G)
I0731 18:11:09.535879 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'conv1b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0G)
I0731 18:11:09.634876 16895 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 2.57G/1 6 4 3  (limit 1.7G, req 0G)
I0731 18:11:09.668767 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 2.57G/1 6 4 3  (limit 1.79G, req 0G)
I0731 18:11:09.675148 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 2.57G/1 6 4 3  (limit 1.79G, req 0G)
I0731 18:11:09.676532 16895 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 2.57G/2 6 4 3  (limit 1.7G, req 0G)
I0731 18:11:09.713724 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0G)
I0731 18:11:09.719440 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0G)
I0731 18:11:09.772716 16895 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 2.57G/1 6 4 5  (limit 1.7G, req 0.07G)
I0731 18:11:09.798176 16895 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 2.57G/2 6 1 3  (limit 1.7G, req 0.07G)
I0731 18:11:09.815923 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 2.57G/1 6 4 5  (limit 1.79G, req 0.07G)
I0731 18:11:09.822451 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 2.57G/1 6 4 5  (limit 1.79G, req 0.07G)
I0731 18:11:09.844193 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 2.57G/2 6 4 5  (limit 1.79G, req 0.07G)
I0731 18:11:09.848942 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0.07G)
I0731 18:11:09.864670 16895 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 2.57G/1 6 4 5  (limit 1.7G, req 0.07G)
I0731 18:11:09.878459 16895 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 2.57G/2 6 4 3  (limit 1.7G, req 0.07G)
I0731 18:11:09.915311 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 2.57G/1 6 4 5  (limit 1.79G, req 0.07G)
I0731 18:11:09.920207 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 2.57G/1 6 4 5  (limit 1.79G, req 0.07G)
I0731 18:11:09.929852 16895 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'out3a' with space 2.57G/2 6 4 3  (limit 1.7G, req 0.07G)
I0731 18:11:09.930867 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0.07G)
I0731 18:11:09.934341 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 2.57G/2 6 4 3  (limit 1.79G, req 0.07G)
I0731 18:11:09.983330 16895 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 2.57G/1 6 4 3  (limit 1.7G, req 0.07G)
I0731 18:11:09.987082 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'out3a' with space 2.57G/2 6 4 3  (limit 1.79G, req 0.07G)
I0731 18:11:09.990953 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'out3a' with space 2.57G/2 6 4 3  (limit 1.79G, req 0.07G)
I0731 18:11:10.011384 16895 cudnn_conv_layer.cpp:1011] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 2.57G/1 6 1 3  (limit 1.7G, req 0.07G)
I0731 18:11:10.043587 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 2.57G/1 6 4 3  (limit 1.79G, req 0.07G)
I0731 18:11:10.048048 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 2.57G/1 6 4 3  (limit 1.79G, req 0.07G)
I0731 18:11:10.071342 16897 cudnn_conv_layer.cpp:1011] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 2.57G/1 6 1 5  (limit 1.79G, req 0.07G)
I0731 18:11:10.075688 16896 cudnn_conv_layer.cpp:1011] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 2.57G/1 6 1 5  (limit 1.79G, req 0.07G)
I0731 18:11:10.204319 16895 solver.cpp:358] Iteration 2 (0.86392 s), loss = 2.13893
I0731 18:11:10.204342 16895 solver.cpp:375]     Train net output #0: loss = 2.13893 (* 1 = 2.13893 loss)
I0731 18:11:10.204941 16896 cudnn_conv_layer.cpp:292] [1] Layer 'conv1a' reallocating workspace: 2.57G -> 0.14G
I0731 18:11:10.204953 16897 cudnn_conv_layer.cpp:292] [2] Layer 'conv1a' reallocating workspace: 2.57G -> 0.14G
I0731 18:11:10.225599 16895 cudnn_conv_layer.cpp:292] [0] Layer 'conv1a' reallocating workspace: 2.57G -> 0.14G
I0731 18:11:28.474076 16895 solver.cpp:353] Iteration 100 (5.36421 iter/s, 18.2692s/98 iter), loss = 0.568728
I0731 18:11:28.474098 16895 solver.cpp:375]     Train net output #0: loss = 0.568728 (* 1 = 0.568728 loss)
I0731 18:11:28.474102 16895 sgd_solver.cpp:136] Iteration 100, lr = 0.0001, m = 0.9
I0731 18:11:39.958631 16903 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 18:11:46.961035 16895 solver.cpp:353] Iteration 200 (5.40937 iter/s, 18.4864s/100 iter), loss = 0.509982
I0731 18:11:46.961060 16895 solver.cpp:375]     Train net output #0: loss = 0.509982 (* 1 = 0.509982 loss)
I0731 18:11:46.961063 16895 sgd_solver.cpp:136] Iteration 200, lr = 0.0001, m = 0.9
I0731 18:12:05.369566 16895 solver.cpp:353] Iteration 300 (5.43242 iter/s, 18.408s/100 iter), loss = 0.342063
I0731 18:12:05.369595 16895 solver.cpp:375]     Train net output #0: loss = 0.342063 (* 1 = 0.342063 loss)
I0731 18:12:05.369601 16895 sgd_solver.cpp:136] Iteration 300, lr = 0.0001, m = 0.9
I0731 18:12:10.424635 16874 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 18:12:23.813911 16895 solver.cpp:353] Iteration 400 (5.42187 iter/s, 18.4438s/100 iter), loss = 0.366329
I0731 18:12:23.813932 16895 solver.cpp:375]     Train net output #0: loss = 0.366329 (* 1 = 0.366329 loss)
I0731 18:12:23.813937 16895 sgd_solver.cpp:136] Iteration 400, lr = 0.0001, m = 0.9
I0731 18:12:42.496090 16895 solver.cpp:353] Iteration 500 (5.35284 iter/s, 18.6817s/100 iter), loss = 0.23861
I0731 18:12:42.496150 16895 solver.cpp:375]     Train net output #0: loss = 0.238609 (* 1 = 0.238609 loss)
I0731 18:12:42.496157 16895 sgd_solver.cpp:136] Iteration 500, lr = 0.0001, m = 0.9
I0731 18:13:01.049793 16895 solver.cpp:353] Iteration 600 (5.38991 iter/s, 18.5532s/100 iter), loss = 0.256315
I0731 18:13:01.049818 16895 solver.cpp:375]     Train net output #0: loss = 0.256315 (* 1 = 0.256315 loss)
I0731 18:13:01.049823 16895 sgd_solver.cpp:136] Iteration 600, lr = 0.0001, m = 0.9
I0731 18:13:11.765561 16876 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 18:13:19.544248 16895 solver.cpp:353] Iteration 700 (5.40718 iter/s, 18.4939s/100 iter), loss = 0.334405
I0731 18:13:19.544299 16895 solver.cpp:375]     Train net output #0: loss = 0.334405 (* 1 = 0.334405 loss)
I0731 18:13:19.544304 16895 sgd_solver.cpp:136] Iteration 700, lr = 0.0001, m = 0.9
I0731 18:13:38.354162 16895 solver.cpp:353] Iteration 800 (5.31649 iter/s, 18.8094s/100 iter), loss = 0.282974
I0731 18:13:38.354190 16895 solver.cpp:375]     Train net output #0: loss = 0.282974 (* 1 = 0.282974 loss)
I0731 18:13:38.354197 16895 sgd_solver.cpp:136] Iteration 800, lr = 0.0001, m = 0.9
I0731 18:13:56.983738 16895 solver.cpp:353] Iteration 900 (5.36796 iter/s, 18.6291s/100 iter), loss = 0.213485
I0731 18:13:56.983816 16895 solver.cpp:375]     Train net output #0: loss = 0.213485 (* 1 = 0.213485 loss)
I0731 18:13:56.983821 16895 sgd_solver.cpp:136] Iteration 900, lr = 0.0001, m = 0.9
I0731 18:14:13.357496 16903 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 18:14:15.605506 16895 solver.cpp:353] Iteration 1000 (5.37021 iter/s, 18.6213s/100 iter), loss = 0.235646
I0731 18:14:15.605533 16895 solver.cpp:375]     Train net output #0: loss = 0.235645 (* 1 = 0.235645 loss)
I0731 18:14:15.605540 16895 sgd_solver.cpp:136] Iteration 1000, lr = 0.0001, m = 0.9
I0731 18:14:34.155038 16895 solver.cpp:353] Iteration 1100 (5.39112 iter/s, 18.549s/100 iter), loss = 0.191682
I0731 18:14:34.155097 16895 solver.cpp:375]     Train net output #0: loss = 0.191682 (* 1 = 0.191682 loss)
I0731 18:14:34.155104 16895 sgd_solver.cpp:136] Iteration 1100, lr = 0.0001, m = 0.9
I0731 18:14:43.980026 16899 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 18:14:52.629736 16895 solver.cpp:353] Iteration 1200 (5.41296 iter/s, 18.4742s/100 iter), loss = 0.246225
I0731 18:14:52.629762 16895 solver.cpp:375]     Train net output #0: loss = 0.246225 (* 1 = 0.246225 loss)
I0731 18:14:52.629766 16895 sgd_solver.cpp:136] Iteration 1200, lr = 0.0001, m = 0.9
I0731 18:15:11.125047 16895 solver.cpp:353] Iteration 1300 (5.40693 iter/s, 18.4948s/100 iter), loss = 0.329271
I0731 18:15:11.125321 16895 solver.cpp:375]     Train net output #0: loss = 0.329271 (* 1 = 0.329271 loss)
I0731 18:15:11.125329 16895 sgd_solver.cpp:136] Iteration 1300, lr = 0.0001, m = 0.9
I0731 18:15:29.649334 16895 solver.cpp:353] Iteration 1400 (5.39847 iter/s, 18.5238s/100 iter), loss = 0.325848
I0731 18:15:29.649358 16895 solver.cpp:375]     Train net output #0: loss = 0.325848 (* 1 = 0.325848 loss)
I0731 18:15:29.649361 16895 sgd_solver.cpp:136] Iteration 1400, lr = 0.0001, m = 0.9
I0731 18:15:45.260100 16901 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 18:15:48.259616 16895 solver.cpp:353] Iteration 1500 (5.37352 iter/s, 18.6098s/100 iter), loss = 0.389745
I0731 18:15:48.259644 16895 solver.cpp:375]     Train net output #0: loss = 0.389745 (* 1 = 0.389745 loss)
I0731 18:15:48.259649 16895 sgd_solver.cpp:136] Iteration 1500, lr = 0.0001, m = 0.9
I0731 18:16:06.875597 16895 solver.cpp:353] Iteration 1600 (5.37188 iter/s, 18.6155s/100 iter), loss = 0.196608
I0731 18:16:06.875622 16895 solver.cpp:375]     Train net output #0: loss = 0.196608 (* 1 = 0.196608 loss)
I0731 18:16:06.875627 16895 sgd_solver.cpp:136] Iteration 1600, lr = 0.0001, m = 0.9
I0731 18:16:25.395726 16895 solver.cpp:353] Iteration 1700 (5.39968 iter/s, 18.5196s/100 iter), loss = 0.277557
I0731 18:16:25.395776 16895 solver.cpp:375]     Train net output #0: loss = 0.277557 (* 1 = 0.277557 loss)
I0731 18:16:25.395781 16895 sgd_solver.cpp:136] Iteration 1700, lr = 0.0001, m = 0.9
I0731 18:16:44.026406 16895 solver.cpp:353] Iteration 1800 (5.36764 iter/s, 18.6302s/100 iter), loss = 0.168652
I0731 18:16:44.026433 16895 solver.cpp:375]     Train net output #0: loss = 0.168652 (* 1 = 0.168652 loss)
I0731 18:16:44.026439 16895 sgd_solver.cpp:136] Iteration 1800, lr = 0.0001, m = 0.9
I0731 18:16:46.825479 16903 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 18:17:02.564273 16895 solver.cpp:353] Iteration 1900 (5.39451 iter/s, 18.5374s/100 iter), loss = 0.13945
I0731 18:17:02.564388 16895 solver.cpp:375]     Train net output #0: loss = 0.13945 (* 1 = 0.13945 loss)
I0731 18:17:02.564394 16895 sgd_solver.cpp:136] Iteration 1900, lr = 0.0001, m = 0.9
I0731 18:17:17.443951 16898 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 18:17:20.914850 16895 solver.cpp:550] Iteration 2000, Testing net (#0)
I0731 18:17:21.184744 16895 blocking_queue.cpp:40] Data layer prefetch queue empty
I0731 18:17:53.604980 16895 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.930324
I0731 18:17:53.605078 16895 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999572
I0731 18:17:53.605087 16895 solver.cpp:635]     Test net output #2: loss = 0.208021 (* 1 = 0.208021 loss)
I0731 18:17:53.605118 16895 solver.cpp:305] [MultiGPU] Tests completed in 32.6894s
I0731 18:17:53.816975 16895 solver.cpp:353] Iteration 2000 (1.95117 iter/s, 51.2513s/100 iter), loss = 0.24678
I0731 18:17:53.817005 16895 solver.cpp:375]     Train net output #0: loss = 0.24678 (* 1 = 0.24678 loss)
I0731 18:17:53.817013 16895 sgd_solver.cpp:136] Iteration 2000, lr = 0.0001, m = 0.9
I0731 18:18:12.268326 16895 solver.cpp:353] Iteration 2100 (5.41981 iter/s, 18.4508s/100 iter), loss = 0.135249
I0731 18:18:12.268354 16895 solver.cpp:375]     Train net output #0: loss = 0.135249 (* 1 = 0.135249 loss)
I0731 18:18:12.268362 16895 sgd_solver.cpp:136] Iteration 2100, lr = 0.0001, m = 0.9
I0731 18:18:20.714885 16903 data_reader.cpp:264] Starting prefetch of epoch 4
I0731 18:18:30.821285 16895 solver.cpp:353] Iteration 2200 (5.39013 iter/s, 18.5524s/100 iter), loss = 0.332999
I0731 18:18:30.821336 16895 solver.cpp:375]     Train net output #0: loss = 0.332999 (* 1 = 0.332999 loss)
I0731 18:18:30.821341 16895 sgd_solver.cpp:136] Iteration 2200, lr = 0.0001, m = 0.9
I0731 18:18:49.256325 16895 solver.cpp:353] Iteration 2300 (5.4246 iter/s, 18.4345s/100 iter), loss = 0.157549
I0731 18:18:49.256347 16895 solver.cpp:375]     Train net output #0: loss = 0.157549 (* 1 = 0.157549 loss)
I0731 18:18:49.256352 16895 sgd_solver.cpp:136] Iteration 2300, lr = 0.0001, m = 0.9
I0731 18:19:07.777689 16895 solver.cpp:353] Iteration 2400 (5.39932 iter/s, 18.5208s/100 iter), loss = 0.136544
I0731 18:19:07.777806 16895 solver.cpp:375]     Train net output #0: loss = 0.136544 (* 1 = 0.136544 loss)
I0731 18:19:07.777812 16895 sgd_solver.cpp:136] Iteration 2400, lr = 0.0001, m = 0.9
I0731 18:19:21.819599 16876 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 18:19:26.283193 16895 solver.cpp:353] Iteration 2500 (5.40395 iter/s, 18.505s/100 iter), loss = 0.189543
I0731 18:19:26.283221 16895 solver.cpp:375]     Train net output #0: loss = 0.189543 (* 1 = 0.189543 loss)
I0731 18:19:26.283224 16895 sgd_solver.cpp:136] Iteration 2500, lr = 0.0001, m = 0.9
I0731 18:19:44.738615 16895 solver.cpp:353] Iteration 2600 (5.41861 iter/s, 18.4549s/100 iter), loss = 0.161889
I0731 18:19:44.738690 16895 solver.cpp:375]     Train net output #0: loss = 0.161889 (* 1 = 0.161889 loss)
I0731 18:19:44.738695 16895 sgd_solver.cpp:136] Iteration 2600, lr = 0.0001, m = 0.9
I0731 18:19:52.439607 16898 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 18:20:03.364154 16895 solver.cpp:353] Iteration 2700 (5.36912 iter/s, 18.625s/100 iter), loss = 0.170089
I0731 18:20:03.364181 16895 solver.cpp:375]     Train net output #0: loss = 0.170089 (* 1 = 0.170089 loss)
I0731 18:20:03.364184 16895 sgd_solver.cpp:136] Iteration 2700, lr = 0.0001, m = 0.9
I0731 18:20:21.886448 16895 solver.cpp:353] Iteration 2800 (5.39905 iter/s, 18.5218s/100 iter), loss = 0.18703
I0731 18:20:21.887114 16895 solver.cpp:375]     Train net output #0: loss = 0.18703 (* 1 = 0.18703 loss)
I0731 18:20:21.887133 16895 sgd_solver.cpp:136] Iteration 2800, lr = 0.0001, m = 0.9
I0731 18:20:40.329900 16895 solver.cpp:353] Iteration 2900 (5.42213 iter/s, 18.4429s/100 iter), loss = 0.178707
I0731 18:20:40.329924 16895 solver.cpp:375]     Train net output #0: loss = 0.178707 (* 1 = 0.178707 loss)
I0731 18:20:40.329928 16895 sgd_solver.cpp:136] Iteration 2900, lr = 0.0001, m = 0.9
I0731 18:20:53.570683 16899 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 18:20:58.882877 16895 solver.cpp:353] Iteration 3000 (5.39012 iter/s, 18.5525s/100 iter), loss = 0.176111
I0731 18:20:58.882899 16895 solver.cpp:375]     Train net output #0: loss = 0.176111 (* 1 = 0.176111 loss)
I0731 18:20:58.882903 16895 sgd_solver.cpp:136] Iteration 3000, lr = 0.0001, m = 0.9
I0731 18:21:17.448598 16895 solver.cpp:353] Iteration 3100 (5.38642 iter/s, 18.5652s/100 iter), loss = 0.0989209
I0731 18:21:17.448626 16895 solver.cpp:375]     Train net output #0: loss = 0.0989209 (* 1 = 0.0989209 loss)
I0731 18:21:17.448629 16895 sgd_solver.cpp:136] Iteration 3100, lr = 0.0001, m = 0.9
I0731 18:21:35.854506 16895 solver.cpp:353] Iteration 3200 (5.43319 iter/s, 18.4054s/100 iter), loss = 0.338542
I0731 18:21:35.854554 16895 solver.cpp:375]     Train net output #0: loss = 0.338542 (* 1 = 0.338542 loss)
I0731 18:21:35.854562 16895 sgd_solver.cpp:136] Iteration 3200, lr = 0.0001, m = 0.9
I0731 18:21:54.229046 16895 solver.cpp:353] Iteration 3300 (5.44246 iter/s, 18.374s/100 iter), loss = 0.134473
I0731 18:21:54.229068 16895 solver.cpp:375]     Train net output #0: loss = 0.134473 (* 1 = 0.134473 loss)
I0731 18:21:54.229073 16895 sgd_solver.cpp:136] Iteration 3300, lr = 0.0001, m = 0.9
I0731 18:21:54.629715 16876 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 18:22:12.843266 16895 solver.cpp:353] Iteration 3400 (5.37238 iter/s, 18.6137s/100 iter), loss = 0.102924
I0731 18:22:12.843497 16895 solver.cpp:375]     Train net output #0: loss = 0.102924 (* 1 = 0.102924 loss)
I0731 18:22:12.843504 16895 sgd_solver.cpp:136] Iteration 3400, lr = 0.0001, m = 0.9
I0731 18:22:25.323323 16899 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 18:22:31.383546 16895 solver.cpp:353] Iteration 3500 (5.39381 iter/s, 18.5398s/100 iter), loss = 0.264272
I0731 18:22:31.383570 16895 solver.cpp:375]     Train net output #0: loss = 0.264272 (* 1 = 0.264272 loss)
I0731 18:22:31.383574 16895 sgd_solver.cpp:136] Iteration 3500, lr = 0.0001, m = 0.9
I0731 18:22:49.967012 16895 solver.cpp:353] Iteration 3600 (5.38128 iter/s, 18.583s/100 iter), loss = 0.181135
I0731 18:22:49.967083 16895 solver.cpp:375]     Train net output #0: loss = 0.181134 (* 1 = 0.181134 loss)
I0731 18:22:49.967088 16895 sgd_solver.cpp:136] Iteration 3600, lr = 0.0001, m = 0.9
I0731 18:23:08.772717 16895 solver.cpp:353] Iteration 3700 (5.31768 iter/s, 18.8052s/100 iter), loss = 0.162281
I0731 18:23:08.772742 16895 solver.cpp:375]     Train net output #0: loss = 0.16228 (* 1 = 0.16228 loss)
I0731 18:23:08.772745 16895 sgd_solver.cpp:136] Iteration 3700, lr = 0.0001, m = 0.9
I0731 18:23:27.115873 16898 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 18:23:27.452574 16895 solver.cpp:353] Iteration 3800 (5.35351 iter/s, 18.6793s/100 iter), loss = 0.248596
I0731 18:23:27.452597 16895 solver.cpp:375]     Train net output #0: loss = 0.248596 (* 1 = 0.248596 loss)
I0731 18:23:27.452601 16895 sgd_solver.cpp:136] Iteration 3800, lr = 0.0001, m = 0.9
I0731 18:23:46.073104 16895 solver.cpp:353] Iteration 3900 (5.37056 iter/s, 18.62s/100 iter), loss = 0.158688
I0731 18:23:46.073127 16895 solver.cpp:375]     Train net output #0: loss = 0.158688 (* 1 = 0.158688 loss)
I0731 18:23:46.073132 16895 sgd_solver.cpp:136] Iteration 3900, lr = 0.0001, m = 0.9
I0731 18:24:04.558969 16895 solver.cpp:550] Iteration 4000, Testing net (#0)
I0731 18:24:08.317531 16891 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 18:24:17.219688 16938 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 18:24:17.564672 16895 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.93666
I0731 18:24:17.564700 16895 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999939
I0731 18:24:17.564707 16895 solver.cpp:635]     Test net output #2: loss = 0.176968 (* 1 = 0.176968 loss)
I0731 18:24:17.564788 16895 solver.cpp:305] [MultiGPU] Tests completed in 13.0055s
I0731 18:24:17.752861 16895 solver.cpp:353] Iteration 4000 (3.15668 iter/s, 31.6789s/100 iter), loss = 0.100464
I0731 18:24:17.752887 16895 solver.cpp:375]     Train net output #0: loss = 0.100464 (* 1 = 0.100464 loss)
I0731 18:24:17.752892 16895 sgd_solver.cpp:136] Iteration 4000, lr = 0.0001, m = 0.9
I0731 18:24:36.393286 16895 solver.cpp:353] Iteration 4100 (5.36483 iter/s, 18.6399s/100 iter), loss = 0.177553
I0731 18:24:36.393337 16895 solver.cpp:375]     Train net output #0: loss = 0.177552 (* 1 = 0.177552 loss)
I0731 18:24:36.393342 16895 sgd_solver.cpp:136] Iteration 4100, lr = 0.0001, m = 0.9
I0731 18:24:54.841101 16895 solver.cpp:353] Iteration 4200 (5.42085 iter/s, 18.4473s/100 iter), loss = 0.191574
I0731 18:24:54.841125 16895 solver.cpp:375]     Train net output #0: loss = 0.191574 (* 1 = 0.191574 loss)
I0731 18:24:54.841130 16895 sgd_solver.cpp:136] Iteration 4200, lr = 0.0001, m = 0.9
I0731 18:25:12.312049 16874 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 18:25:13.391573 16895 solver.cpp:353] Iteration 4300 (5.39085 iter/s, 18.55s/100 iter), loss = 0.186605
I0731 18:25:13.391598 16895 solver.cpp:375]     Train net output #0: loss = 0.186605 (* 1 = 0.186605 loss)
I0731 18:25:13.391604 16895 sgd_solver.cpp:136] Iteration 4300, lr = 0.0001, m = 0.9
I0731 18:25:31.915668 16895 solver.cpp:353] Iteration 4400 (5.39852 iter/s, 18.5236s/100 iter), loss = 0.114894
I0731 18:25:31.915693 16895 solver.cpp:375]     Train net output #0: loss = 0.114894 (* 1 = 0.114894 loss)
I0731 18:25:31.915697 16895 sgd_solver.cpp:136] Iteration 4400, lr = 0.0001, m = 0.9
I0731 18:25:50.386718 16895 solver.cpp:353] Iteration 4500 (5.41403 iter/s, 18.4705s/100 iter), loss = 0.090743
I0731 18:25:50.386828 16895 solver.cpp:375]     Train net output #0: loss = 0.0907429 (* 1 = 0.0907429 loss)
I0731 18:25:50.386835 16895 sgd_solver.cpp:136] Iteration 4500, lr = 0.0001, m = 0.9
I0731 18:26:08.817718 16895 solver.cpp:353] Iteration 4600 (5.42579 iter/s, 18.4305s/100 iter), loss = 0.179321
I0731 18:26:08.817742 16895 solver.cpp:375]     Train net output #0: loss = 0.179321 (* 1 = 0.179321 loss)
I0731 18:26:08.817746 16895 sgd_solver.cpp:136] Iteration 4600, lr = 0.0001, m = 0.9
I0731 18:26:13.298305 16876 data_reader.cpp:264] Starting prefetch of epoch 4
I0731 18:26:27.423954 16895 solver.cpp:353] Iteration 4700 (5.37469 iter/s, 18.6057s/100 iter), loss = 0.158534
I0731 18:26:27.424036 16895 solver.cpp:375]     Train net output #0: loss = 0.158534 (* 1 = 0.158534 loss)
I0731 18:26:27.424041 16895 sgd_solver.cpp:136] Iteration 4700, lr = 0.0001, m = 0.9
I0731 18:26:44.173993 16874 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 18:26:45.998708 16895 solver.cpp:353] Iteration 4800 (5.3838 iter/s, 18.5742s/100 iter), loss = 0.230809
I0731 18:26:45.998733 16895 solver.cpp:375]     Train net output #0: loss = 0.230809 (* 1 = 0.230809 loss)
I0731 18:26:45.998739 16895 sgd_solver.cpp:136] Iteration 4800, lr = 0.0001, m = 0.9
I0731 18:27:04.479656 16895 solver.cpp:353] Iteration 4900 (5.41113 iter/s, 18.4804s/100 iter), loss = 0.144173
I0731 18:27:04.479712 16895 solver.cpp:375]     Train net output #0: loss = 0.144173 (* 1 = 0.144173 loss)
I0731 18:27:04.479717 16895 sgd_solver.cpp:136] Iteration 4900, lr = 0.0001, m = 0.9
I0731 18:27:23.092249 16895 solver.cpp:353] Iteration 5000 (5.37285 iter/s, 18.6121s/100 iter), loss = 0.162114
I0731 18:27:23.092273 16895 solver.cpp:375]     Train net output #0: loss = 0.162114 (* 1 = 0.162114 loss)
I0731 18:27:23.092278 16895 sgd_solver.cpp:136] Iteration 5000, lr = 0.0001, m = 0.9
I0731 18:27:41.685920 16895 solver.cpp:353] Iteration 5100 (5.37832 iter/s, 18.5932s/100 iter), loss = 0.213413
I0731 18:27:41.694779 16895 solver.cpp:375]     Train net output #0: loss = 0.213413 (* 1 = 0.213413 loss)
I0731 18:27:41.694824 16895 sgd_solver.cpp:136] Iteration 5100, lr = 0.0001, m = 0.9
I0731 18:27:45.415237 16901 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 18:28:00.181833 16895 solver.cpp:353] Iteration 5200 (5.40675 iter/s, 18.4954s/100 iter), loss = 0.210293
I0731 18:28:00.181856 16895 solver.cpp:375]     Train net output #0: loss = 0.210293 (* 1 = 0.210293 loss)
I0731 18:28:00.181861 16895 sgd_solver.cpp:136] Iteration 5200, lr = 0.0001, m = 0.9
I0731 18:28:18.618372 16895 solver.cpp:353] Iteration 5300 (5.42416 iter/s, 18.436s/100 iter), loss = 0.102583
I0731 18:28:18.618424 16895 solver.cpp:375]     Train net output #0: loss = 0.102583 (* 1 = 0.102583 loss)
I0731 18:28:18.618429 16895 sgd_solver.cpp:136] Iteration 5300, lr = 0.0001, m = 0.9
I0731 18:28:37.144887 16895 solver.cpp:353] Iteration 5400 (5.39782 iter/s, 18.526s/100 iter), loss = 0.100171
I0731 18:28:37.144913 16895 solver.cpp:375]     Train net output #0: loss = 0.100171 (* 1 = 0.100171 loss)
I0731 18:28:37.144917 16895 sgd_solver.cpp:136] Iteration 5400, lr = 0.0001, m = 0.9
I0731 18:28:46.657218 16876 data_reader.cpp:264] Starting prefetch of epoch 5
I0731 18:28:55.770293 16895 solver.cpp:353] Iteration 5500 (5.36916 iter/s, 18.6249s/100 iter), loss = 0.236338
I0731 18:28:55.770344 16895 solver.cpp:375]     Train net output #0: loss = 0.236338 (* 1 = 0.236338 loss)
I0731 18:28:55.770349 16895 sgd_solver.cpp:136] Iteration 5500, lr = 0.0001, m = 0.9
I0731 18:29:14.314664 16895 solver.cpp:353] Iteration 5600 (5.39262 iter/s, 18.5439s/100 iter), loss = 0.0807359
I0731 18:29:14.314687 16895 solver.cpp:375]     Train net output #0: loss = 0.0807359 (* 1 = 0.0807359 loss)
I0731 18:29:14.314693 16895 sgd_solver.cpp:136] Iteration 5600, lr = 0.0001, m = 0.9
I0731 18:29:17.329725 16874 data_reader.cpp:264] Starting prefetch of epoch 4
I0731 18:29:33.040515 16895 solver.cpp:353] Iteration 5700 (5.34036 iter/s, 18.7253s/100 iter), loss = 0.374645
I0731 18:29:33.040612 16895 solver.cpp:375]     Train net output #0: loss = 0.374645 (* 1 = 0.374645 loss)
I0731 18:29:33.040619 16895 sgd_solver.cpp:136] Iteration 5700, lr = 0.0001, m = 0.9
I0731 18:29:51.729663 16895 solver.cpp:353] Iteration 5800 (5.35085 iter/s, 18.6886s/100 iter), loss = 0.117933
I0731 18:29:51.729691 16895 solver.cpp:375]     Train net output #0: loss = 0.117933 (* 1 = 0.117933 loss)
I0731 18:29:51.729698 16895 sgd_solver.cpp:136] Iteration 5800, lr = 0.0001, m = 0.9
I0731 18:30:10.318073 16895 solver.cpp:353] Iteration 5900 (5.37984 iter/s, 18.5879s/100 iter), loss = 0.234558
I0731 18:30:10.318172 16895 solver.cpp:375]     Train net output #0: loss = 0.234558 (* 1 = 0.234558 loss)
I0731 18:30:10.318179 16895 sgd_solver.cpp:136] Iteration 5900, lr = 0.0001, m = 0.9
I0731 18:30:19.006623 16899 data_reader.cpp:264] Starting prefetch of epoch 4
I0731 18:30:28.762199 16895 solver.cpp:550] Iteration 6000, Testing net (#0)
I0731 18:30:40.010843 16895 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.938279
I0731 18:30:40.010866 16895 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999993
I0731 18:30:40.010871 16895 solver.cpp:635]     Test net output #2: loss = 0.175419 (* 1 = 0.175419 loss)
I0731 18:30:40.010948 16895 solver.cpp:305] [MultiGPU] Tests completed in 11.2484s
I0731 18:30:40.204170 16895 solver.cpp:353] Iteration 6000 (3.34613 iter/s, 29.8853s/100 iter), loss = 0.244488
I0731 18:30:40.204198 16895 solver.cpp:375]     Train net output #0: loss = 0.244488 (* 1 = 0.244488 loss)
I0731 18:30:40.204205 16895 sgd_solver.cpp:136] Iteration 6000, lr = 0.0001, m = 0.9
I0731 18:30:58.992117 16895 solver.cpp:353] Iteration 6100 (5.32271 iter/s, 18.7874s/100 iter), loss = 0.131227
I0731 18:30:58.992236 16895 solver.cpp:375]     Train net output #0: loss = 0.131227 (* 1 = 0.131227 loss)
I0731 18:30:58.992244 16895 sgd_solver.cpp:136] Iteration 6100, lr = 0.0001, m = 0.9
I0731 18:31:01.280764 16903 data_reader.cpp:264] Starting prefetch of epoch 5
I0731 18:31:17.636564 16895 solver.cpp:353] Iteration 6200 (5.36367 iter/s, 18.6439s/100 iter), loss = 0.15832
I0731 18:31:17.636590 16895 solver.cpp:375]     Train net output #0: loss = 0.15832 (* 1 = 0.15832 loss)
I0731 18:31:17.636593 16895 sgd_solver.cpp:136] Iteration 6200, lr = 0.0001, m = 0.9
I0731 18:31:31.922065 16899 data_reader.cpp:264] Starting prefetch of epoch 5
I0731 18:31:36.112583 16895 solver.cpp:353] Iteration 6300 (5.41257 iter/s, 18.4755s/100 iter), loss = 0.135247
I0731 18:31:36.112607 16895 solver.cpp:375]     Train net output #0: loss = 0.135247 (* 1 = 0.135247 loss)
I0731 18:31:36.112612 16895 sgd_solver.cpp:136] Iteration 6300, lr = 0.0001, m = 0.9
I0731 18:31:54.748971 16895 solver.cpp:353] Iteration 6400 (5.36599 iter/s, 18.6359s/100 iter), loss = 0.165852
I0731 18:31:54.748996 16895 solver.cpp:375]     Train net output #0: loss = 0.165852 (* 1 = 0.165852 loss)
I0731 18:31:54.749001 16895 sgd_solver.cpp:136] Iteration 6400, lr = 0.0001, m = 0.9
I0731 18:32:13.334138 16895 solver.cpp:353] Iteration 6500 (5.38078 iter/s, 18.5847s/100 iter), loss = 0.14043
I0731 18:32:13.334192 16895 solver.cpp:375]     Train net output #0: loss = 0.14043 (* 1 = 0.14043 loss)
I0731 18:32:13.334197 16895 sgd_solver.cpp:136] Iteration 6500, lr = 0.0001, m = 0.9
I0731 18:32:31.812597 16895 solver.cpp:353] Iteration 6600 (5.41186 iter/s, 18.4779s/100 iter), loss = 0.229537
I0731 18:32:31.812624 16895 solver.cpp:375]     Train net output #0: loss = 0.229537 (* 1 = 0.229537 loss)
I0731 18:32:31.812628 16895 sgd_solver.cpp:136] Iteration 6600, lr = 0.0001, m = 0.9
I0731 18:32:33.302883 16876 data_reader.cpp:264] Starting prefetch of epoch 6
I0731 18:32:50.325315 16895 solver.cpp:353] Iteration 6700 (5.40184 iter/s, 18.5122s/100 iter), loss = 0.186273
I0731 18:32:50.325366 16895 solver.cpp:375]     Train net output #0: loss = 0.186273 (* 1 = 0.186273 loss)
I0731 18:32:50.325371 16895 sgd_solver.cpp:136] Iteration 6700, lr = 0.0001, m = 0.9
I0731 18:33:08.941462 16895 solver.cpp:353] Iteration 6800 (5.37183 iter/s, 18.6156s/100 iter), loss = 0.0856368
I0731 18:33:08.941484 16895 solver.cpp:375]     Train net output #0: loss = 0.0856368 (* 1 = 0.0856368 loss)
I0731 18:33:08.941489 16895 sgd_solver.cpp:136] Iteration 6800, lr = 0.0001, m = 0.9
I0731 18:33:27.711562 16895 solver.cpp:353] Iteration 6900 (5.32777 iter/s, 18.7696s/100 iter), loss = 0.166159
I0731 18:33:27.711625 16895 solver.cpp:375]     Train net output #0: loss = 0.166159 (* 1 = 0.166159 loss)
I0731 18:33:27.711632 16895 sgd_solver.cpp:136] Iteration 6900, lr = 0.0001, m = 0.9
I0731 18:33:34.753293 16876 data_reader.cpp:264] Starting prefetch of epoch 7
I0731 18:33:46.238759 16895 solver.cpp:353] Iteration 7000 (5.39762 iter/s, 18.5267s/100 iter), loss = 0.233967
I0731 18:33:46.238783 16895 solver.cpp:375]     Train net output #0: loss = 0.233967 (* 1 = 0.233967 loss)
I0731 18:33:46.238790 16895 sgd_solver.cpp:136] Iteration 7000, lr = 0.0001, m = 0.9
I0731 18:34:04.842321 16895 solver.cpp:353] Iteration 7100 (5.37546 iter/s, 18.6031s/100 iter), loss = 0.137743
I0731 18:34:04.842429 16895 solver.cpp:375]     Train net output #0: loss = 0.137743 (* 1 = 0.137743 loss)
I0731 18:34:04.842437 16895 sgd_solver.cpp:136] Iteration 7100, lr = 0.0001, m = 0.9
I0731 18:34:05.446573 16899 data_reader.cpp:264] Starting prefetch of epoch 6
I0731 18:34:23.344197 16895 solver.cpp:353] Iteration 7200 (5.40501 iter/s, 18.5014s/100 iter), loss = 0.144289
I0731 18:34:23.344223 16895 solver.cpp:375]     Train net output #0: loss = 0.144289 (* 1 = 0.144289 loss)
I0731 18:34:23.344226 16895 sgd_solver.cpp:136] Iteration 7200, lr = 0.0001, m = 0.9
I0731 18:34:41.824532 16895 solver.cpp:353] Iteration 7300 (5.41131 iter/s, 18.4798s/100 iter), loss = 0.113652
I0731 18:34:41.824582 16895 solver.cpp:375]     Train net output #0: loss = 0.113651 (* 1 = 0.113651 loss)
I0731 18:34:41.824589 16895 sgd_solver.cpp:136] Iteration 7300, lr = 0.0001, m = 0.9
I0731 18:35:00.477684 16895 solver.cpp:353] Iteration 7400 (5.36117 iter/s, 18.6526s/100 iter), loss = 0.211173
I0731 18:35:00.477710 16895 solver.cpp:375]     Train net output #0: loss = 0.211173 (* 1 = 0.211173 loss)
I0731 18:35:00.477713 16895 sgd_solver.cpp:136] Iteration 7400, lr = 0.0001, m = 0.9
I0731 18:35:06.754748 16874 data_reader.cpp:264] Starting prefetch of epoch 5
I0731 18:35:19.013409 16895 solver.cpp:353] Iteration 7500 (5.39513 iter/s, 18.5352s/100 iter), loss = 0.151271
I0731 18:35:19.013460 16895 solver.cpp:375]     Train net output #0: loss = 0.15127 (* 1 = 0.15127 loss)
I0731 18:35:19.013465 16895 sgd_solver.cpp:136] Iteration 7500, lr = 0.0001, m = 0.9
I0731 18:35:37.643838 16895 solver.cpp:353] Iteration 7600 (5.36771 iter/s, 18.6299s/100 iter), loss = 0.096497
I0731 18:35:37.643863 16895 solver.cpp:375]     Train net output #0: loss = 0.0964969 (* 1 = 0.0964969 loss)
I0731 18:35:37.643867 16895 sgd_solver.cpp:136] Iteration 7600, lr = 0.0001, m = 0.9
I0731 18:35:56.369447 16895 solver.cpp:353] Iteration 7700 (5.34043 iter/s, 18.7251s/100 iter), loss = 0.0841814
I0731 18:35:56.369500 16895 solver.cpp:375]     Train net output #0: loss = 0.0841812 (* 1 = 0.0841812 loss)
I0731 18:35:56.369505 16895 sgd_solver.cpp:136] Iteration 7700, lr = 0.0001, m = 0.9
I0731 18:36:08.421048 16903 data_reader.cpp:264] Starting prefetch of epoch 6
I0731 18:36:15.065824 16895 solver.cpp:353] Iteration 7800 (5.34878 iter/s, 18.6959s/100 iter), loss = 0.2411
I0731 18:36:15.065847 16895 solver.cpp:375]     Train net output #0: loss = 0.2411 (* 1 = 0.2411 loss)
I0731 18:36:15.065851 16895 sgd_solver.cpp:136] Iteration 7800, lr = 0.0001, m = 0.9
I0731 18:36:33.627349 16895 solver.cpp:353] Iteration 7900 (5.38764 iter/s, 18.561s/100 iter), loss = 0.12066
I0731 18:36:33.627434 16895 solver.cpp:375]     Train net output #0: loss = 0.12066 (* 1 = 0.12066 loss)
I0731 18:36:33.627440 16895 sgd_solver.cpp:136] Iteration 7900, lr = 0.0001, m = 0.9
I0731 18:36:39.258896 16874 data_reader.cpp:264] Starting prefetch of epoch 6
I0731 18:36:52.108695 16895 solver.cpp:550] Iteration 8000, Testing net (#0)
I0731 18:37:18.723600 16934 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 18:37:19.112588 16895 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.941687
I0731 18:37:19.112609 16895 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999995
I0731 18:37:19.112615 16895 solver.cpp:635]     Test net output #2: loss = 0.158836 (* 1 = 0.158836 loss)
I0731 18:37:19.112639 16895 solver.cpp:305] [MultiGPU] Tests completed in 27.0032s
I0731 18:37:19.314896 16895 solver.cpp:353] Iteration 8000 (2.18884 iter/s, 45.6863s/100 iter), loss = 0.124591
I0731 18:37:19.314924 16895 solver.cpp:375]     Train net output #0: loss = 0.124591 (* 1 = 0.124591 loss)
I0731 18:37:19.314930 16895 sgd_solver.cpp:136] Iteration 8000, lr = 0.0001, m = 0.9
I0731 18:37:37.936120 16895 solver.cpp:353] Iteration 8100 (5.37037 iter/s, 18.6207s/100 iter), loss = 0.135524
I0731 18:37:37.936141 16895 solver.cpp:375]     Train net output #0: loss = 0.135524 (* 1 = 0.135524 loss)
I0731 18:37:37.936146 16895 sgd_solver.cpp:136] Iteration 8100, lr = 0.0001, m = 0.9
I0731 18:37:56.505499 16895 solver.cpp:353] Iteration 8200 (5.38536 iter/s, 18.5689s/100 iter), loss = 0.0984445
I0731 18:37:56.505626 16895 solver.cpp:375]     Train net output #0: loss = 0.0984443 (* 1 = 0.0984443 loss)
I0731 18:37:56.505632 16895 sgd_solver.cpp:136] Iteration 8200, lr = 0.0001, m = 0.9
I0731 18:38:07.680297 16901 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 18:38:15.044185 16895 solver.cpp:353] Iteration 8300 (5.39428 iter/s, 18.5382s/100 iter), loss = 0.289801
I0731 18:38:15.044209 16895 solver.cpp:375]     Train net output #0: loss = 0.289801 (* 1 = 0.289801 loss)
I0731 18:38:15.044212 16895 sgd_solver.cpp:136] Iteration 8300, lr = 0.0001, m = 0.9
I0731 18:38:33.636068 16895 solver.cpp:353] Iteration 8400 (5.37884 iter/s, 18.5914s/100 iter), loss = 0.115228
I0731 18:38:33.636126 16895 solver.cpp:375]     Train net output #0: loss = 0.115227 (* 1 = 0.115227 loss)
I0731 18:38:33.636132 16895 sgd_solver.cpp:136] Iteration 8400, lr = 0.0001, m = 0.9
I0731 18:38:52.243659 16895 solver.cpp:353] Iteration 8500 (5.3743 iter/s, 18.6071s/100 iter), loss = 0.101134
I0731 18:38:52.243685 16895 solver.cpp:375]     Train net output #0: loss = 0.101134 (* 1 = 0.101134 loss)
I0731 18:38:52.243690 16895 sgd_solver.cpp:136] Iteration 8500, lr = 0.0001, m = 0.9
I0731 18:39:09.168807 16903 data_reader.cpp:264] Starting prefetch of epoch 7
I0731 18:39:10.830551 16895 solver.cpp:353] Iteration 8600 (5.38028 iter/s, 18.5864s/100 iter), loss = 0.170582
I0731 18:39:10.830576 16895 solver.cpp:375]     Train net output #0: loss = 0.170581 (* 1 = 0.170581 loss)
I0731 18:39:10.830580 16895 sgd_solver.cpp:136] Iteration 8600, lr = 0.0001, m = 0.9
I0731 18:39:29.348841 16895 solver.cpp:353] Iteration 8700 (5.40022 iter/s, 18.5178s/100 iter), loss = 0.101873
I0731 18:39:29.348865 16895 solver.cpp:375]     Train net output #0: loss = 0.101873 (* 1 = 0.101873 loss)
I0731 18:39:29.348870 16895 sgd_solver.cpp:136] Iteration 8700, lr = 0.0001, m = 0.9
I0731 18:39:39.760704 16874 data_reader.cpp:264] Starting prefetch of epoch 7
I0731 18:39:47.833278 16895 solver.cpp:353] Iteration 8800 (5.41011 iter/s, 18.4839s/100 iter), loss = 0.0978431
I0731 18:39:47.833304 16895 solver.cpp:375]     Train net output #0: loss = 0.0978429 (* 1 = 0.0978429 loss)
I0731 18:39:47.833308 16895 sgd_solver.cpp:136] Iteration 8800, lr = 0.0001, m = 0.9
I0731 18:40:06.491861 16895 solver.cpp:353] Iteration 8900 (5.35961 iter/s, 18.6581s/100 iter), loss = 0.134101
I0731 18:40:06.491884 16895 solver.cpp:375]     Train net output #0: loss = 0.134101 (* 1 = 0.134101 loss)
I0731 18:40:06.491888 16895 sgd_solver.cpp:136] Iteration 8900, lr = 0.0001, m = 0.9
I0731 18:40:25.197238 16895 solver.cpp:353] Iteration 9000 (5.3462 iter/s, 18.7049s/100 iter), loss = 0.0990635
I0731 18:40:25.197285 16895 solver.cpp:375]     Train net output #0: loss = 0.0990633 (* 1 = 0.0990633 loss)
I0731 18:40:25.197293 16895 sgd_solver.cpp:136] Iteration 9000, lr = 0.0001, m = 0.9
I0731 18:40:41.305459 16903 data_reader.cpp:264] Starting prefetch of epoch 8
I0731 18:40:43.687042 16895 solver.cpp:353] Iteration 9100 (5.40854 iter/s, 18.4893s/100 iter), loss = 0.191978
I0731 18:40:43.687068 16895 solver.cpp:375]     Train net output #0: loss = 0.191978 (* 1 = 0.191978 loss)
I0731 18:40:43.687074 16895 sgd_solver.cpp:136] Iteration 9100, lr = 0.0001, m = 0.9
I0731 18:41:02.346854 16895 solver.cpp:353] Iteration 9200 (5.35926 iter/s, 18.6593s/100 iter), loss = 0.115555
I0731 18:41:02.346930 16895 solver.cpp:375]     Train net output #0: loss = 0.115555 (* 1 = 0.115555 loss)
I0731 18:41:02.346935 16895 sgd_solver.cpp:136] Iteration 9200, lr = 0.0001, m = 0.9
I0731 18:41:20.873234 16895 solver.cpp:353] Iteration 9300 (5.39786 iter/s, 18.5259s/100 iter), loss = 0.127822
I0731 18:41:20.873260 16895 solver.cpp:375]     Train net output #0: loss = 0.127822 (* 1 = 0.127822 loss)
I0731 18:41:20.873265 16895 sgd_solver.cpp:136] Iteration 9300, lr = 0.0001, m = 0.9
I0731 18:41:39.542906 16895 solver.cpp:353] Iteration 9400 (5.35643 iter/s, 18.6692s/100 iter), loss = 0.142559
I0731 18:41:39.543009 16895 solver.cpp:375]     Train net output #0: loss = 0.142559 (* 1 = 0.142559 loss)
I0731 18:41:39.543015 16895 sgd_solver.cpp:136] Iteration 9400, lr = 0.0001, m = 0.9
I0731 18:41:42.746332 16901 data_reader.cpp:264] Starting prefetch of epoch 4
I0731 18:41:58.168244 16895 solver.cpp:353] Iteration 9500 (5.36918 iter/s, 18.6248s/100 iter), loss = 0.118447
I0731 18:41:58.168274 16895 solver.cpp:375]     Train net output #0: loss = 0.118447 (* 1 = 0.118447 loss)
I0731 18:41:58.168279 16895 sgd_solver.cpp:136] Iteration 9500, lr = 0.0001, m = 0.9
I0731 18:42:13.759500 16899 data_reader.cpp:264] Starting prefetch of epoch 7
I0731 18:42:16.864542 16895 solver.cpp:353] Iteration 9600 (5.3488 iter/s, 18.6958s/100 iter), loss = 0.111096
I0731 18:42:16.864567 16895 solver.cpp:375]     Train net output #0: loss = 0.111095 (* 1 = 0.111095 loss)
I0731 18:42:16.864573 16895 sgd_solver.cpp:136] Iteration 9600, lr = 0.0001, m = 0.9
I0731 18:42:35.438446 16895 solver.cpp:353] Iteration 9700 (5.38405 iter/s, 18.5734s/100 iter), loss = 0.106962
I0731 18:42:35.438472 16895 solver.cpp:375]     Train net output #0: loss = 0.106962 (* 1 = 0.106962 loss)
I0731 18:42:35.438477 16895 sgd_solver.cpp:136] Iteration 9700, lr = 0.0001, m = 0.9
I0731 18:42:53.994302 16895 solver.cpp:353] Iteration 9800 (5.38928 iter/s, 18.5553s/100 iter), loss = 0.109069
I0731 18:42:53.994359 16895 solver.cpp:375]     Train net output #0: loss = 0.109069 (* 1 = 0.109069 loss)
I0731 18:42:53.994364 16895 sgd_solver.cpp:136] Iteration 9800, lr = 0.0001, m = 0.9
I0731 18:43:12.514705 16895 solver.cpp:353] Iteration 9900 (5.3996 iter/s, 18.5199s/100 iter), loss = 0.199178
I0731 18:43:12.514729 16895 solver.cpp:375]     Train net output #0: loss = 0.199178 (* 1 = 0.199178 loss)
I0731 18:43:12.514734 16895 sgd_solver.cpp:136] Iteration 9900, lr = 0.0001, m = 0.9
I0731 18:43:15.013370 16874 data_reader.cpp:264] Starting prefetch of epoch 8
I0731 18:43:30.939188 16895 solver.cpp:680] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/initial/cityscapes5_jsegnet21v2_iter_10000.caffemodel
I0731 18:43:31.016254 16895 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/initial/cityscapes5_jsegnet21v2_iter_10000.solverstate
I0731 18:43:31.024690 16895 solver.cpp:550] Iteration 10000, Testing net (#0)
I0731 18:43:43.513289 16895 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.946047
I0731 18:43:43.513309 16895 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999994
I0731 18:43:43.513315 16895 solver.cpp:635]     Test net output #2: loss = 0.155756 (* 1 = 0.155756 loss)
I0731 18:43:43.513396 16895 solver.cpp:305] [MultiGPU] Tests completed in 12.4884s
I0731 18:43:43.717808 16895 solver.cpp:353] Iteration 10000 (3.2049 iter/s, 31.2022s/100 iter), loss = 0.0913581
I0731 18:43:43.717834 16895 solver.cpp:375]     Train net output #0: loss = 0.0913579 (* 1 = 0.0913579 loss)
I0731 18:43:43.717839 16895 sgd_solver.cpp:136] Iteration 10000, lr = 0.0001, m = 0.9
I0731 18:43:58.303889 16876 data_reader.cpp:264] Starting prefetch of epoch 8
I0731 18:44:02.418900 16895 solver.cpp:353] Iteration 10100 (5.34743 iter/s, 18.7006s/100 iter), loss = 0.0956916
I0731 18:44:02.418948 16895 solver.cpp:375]     Train net output #0: loss = 0.0956914 (* 1 = 0.0956914 loss)
I0731 18:44:02.418953 16895 sgd_solver.cpp:136] Iteration 10100, lr = 0.0001, m = 0.9
I0731 18:44:20.936651 16895 solver.cpp:353] Iteration 10200 (5.40037 iter/s, 18.5172s/100 iter), loss = 0.31475
I0731 18:44:20.936676 16895 solver.cpp:375]     Train net output #0: loss = 0.31475 (* 1 = 0.31475 loss)
I0731 18:44:20.936681 16895 sgd_solver.cpp:136] Iteration 10200, lr = 0.0001, m = 0.9
I0731 18:44:29.105712 16899 data_reader.cpp:264] Starting prefetch of epoch 8
I0731 18:44:39.578400 16895 solver.cpp:353] Iteration 10300 (5.36445 iter/s, 18.6412s/100 iter), loss = 0.0960819
I0731 18:44:39.578521 16895 solver.cpp:375]     Train net output #0: loss = 0.0960817 (* 1 = 0.0960817 loss)
I0731 18:44:39.578527 16895 sgd_solver.cpp:136] Iteration 10300, lr = 0.0001, m = 0.9
I0731 18:44:58.212108 16895 solver.cpp:353] Iteration 10400 (5.36677 iter/s, 18.6332s/100 iter), loss = 0.0793995
I0731 18:44:58.212132 16895 solver.cpp:375]     Train net output #0: loss = 0.0793993 (* 1 = 0.0793993 loss)
I0731 18:44:58.212136 16895 sgd_solver.cpp:136] Iteration 10400, lr = 0.0001, m = 0.9
I0731 18:45:17.003475 16895 solver.cpp:353] Iteration 10500 (5.32174 iter/s, 18.7908s/100 iter), loss = 0.0850744
I0731 18:45:17.003559 16895 solver.cpp:375]     Train net output #0: loss = 0.0850742 (* 1 = 0.0850742 loss)
I0731 18:45:17.003567 16895 sgd_solver.cpp:136] Iteration 10500, lr = 0.0001, m = 0.9
I0731 18:45:30.776718 16874 data_reader.cpp:264] Starting prefetch of epoch 9
I0731 18:45:35.592051 16895 solver.cpp:353] Iteration 10600 (5.3798 iter/s, 18.5881s/100 iter), loss = 0.148572
I0731 18:45:35.592073 16895 solver.cpp:375]     Train net output #0: loss = 0.148572 (* 1 = 0.148572 loss)
I0731 18:45:35.592077 16895 sgd_solver.cpp:136] Iteration 10600, lr = 0.0001, m = 0.9
I0731 18:45:54.076863 16895 solver.cpp:353] Iteration 10700 (5.41 iter/s, 18.4843s/100 iter), loss = 0.0883492
I0731 18:45:54.076911 16895 solver.cpp:375]     Train net output #0: loss = 0.088349 (* 1 = 0.088349 loss)
I0731 18:45:54.076916 16895 sgd_solver.cpp:136] Iteration 10700, lr = 0.0001, m = 0.9
I0731 18:46:12.548568 16895 solver.cpp:353] Iteration 10800 (5.41383 iter/s, 18.4712s/100 iter), loss = 0.165894
I0731 18:46:12.548591 16895 solver.cpp:375]     Train net output #0: loss = 0.165894 (* 1 = 0.165894 loss)
I0731 18:46:12.548595 16895 sgd_solver.cpp:136] Iteration 10800, lr = 0.0001, m = 0.9
I0731 18:46:31.179129 16895 solver.cpp:353] Iteration 10900 (5.36767 iter/s, 18.63s/100 iter), loss = 0.0659118
I0731 18:46:31.179553 16895 solver.cpp:375]     Train net output #0: loss = 0.0659116 (* 1 = 0.0659116 loss)
I0731 18:46:31.179561 16895 sgd_solver.cpp:136] Iteration 10900, lr = 0.0001, m = 0.9
I0731 18:46:32.119209 16903 data_reader.cpp:264] Starting prefetch of epoch 9
I0731 18:46:49.799329 16895 solver.cpp:353] Iteration 11000 (5.37066 iter/s, 18.6197s/100 iter), loss = 0.109232
I0731 18:46:49.799353 16895 solver.cpp:375]     Train net output #0: loss = 0.109232 (* 1 = 0.109232 loss)
I0731 18:46:49.799358 16895 sgd_solver.cpp:136] Iteration 11000, lr = 0.0001, m = 0.9
I0731 18:47:02.854945 16899 data_reader.cpp:264] Starting prefetch of epoch 9
I0731 18:47:08.358635 16895 solver.cpp:353] Iteration 11100 (5.38828 iter/s, 18.5588s/100 iter), loss = 0.122437
I0731 18:47:08.358659 16895 solver.cpp:375]     Train net output #0: loss = 0.122437 (* 1 = 0.122437 loss)
I0731 18:47:08.358664 16895 sgd_solver.cpp:136] Iteration 11100, lr = 0.0001, m = 0.9
I0731 18:47:27.013348 16895 solver.cpp:353] Iteration 11200 (5.36072 iter/s, 18.6542s/100 iter), loss = 0.0968579
I0731 18:47:27.013373 16895 solver.cpp:375]     Train net output #0: loss = 0.0968577 (* 1 = 0.0968577 loss)
I0731 18:47:27.013380 16895 sgd_solver.cpp:136] Iteration 11200, lr = 0.0001, m = 0.9
I0731 18:47:45.667250 16895 solver.cpp:353] Iteration 11300 (5.36096 iter/s, 18.6534s/100 iter), loss = 0.10304
I0731 18:47:45.667302 16895 solver.cpp:375]     Train net output #0: loss = 0.10304 (* 1 = 0.10304 loss)
I0731 18:47:45.667309 16895 sgd_solver.cpp:136] Iteration 11300, lr = 0.0001, m = 0.9
I0731 18:48:04.179816 16895 solver.cpp:353] Iteration 11400 (5.40189 iter/s, 18.5121s/100 iter), loss = 0.0817959
I0731 18:48:04.179841 16895 solver.cpp:375]     Train net output #0: loss = 0.0817956 (* 1 = 0.0817956 loss)
I0731 18:48:04.179847 16895 sgd_solver.cpp:136] Iteration 11400, lr = 0.0001, m = 0.9
I0731 18:48:04.391109 16898 data_reader.cpp:264] Starting prefetch of epoch 4
I0731 18:48:22.842214 16895 solver.cpp:353] Iteration 11500 (5.35852 iter/s, 18.6619s/100 iter), loss = 0.139584
I0731 18:48:22.842308 16895 solver.cpp:375]     Train net output #0: loss = 0.139584 (* 1 = 0.139584 loss)
I0731 18:48:22.842315 16895 sgd_solver.cpp:136] Iteration 11500, lr = 0.0001, m = 0.9
I0731 18:48:41.477908 16895 solver.cpp:353] Iteration 11600 (5.36619 iter/s, 18.6352s/100 iter), loss = 0.145391
I0731 18:48:41.477931 16895 solver.cpp:375]     Train net output #0: loss = 0.145391 (* 1 = 0.145391 loss)
I0731 18:48:41.477936 16895 sgd_solver.cpp:136] Iteration 11600, lr = 0.0001, m = 0.9
I0731 18:48:59.908741 16895 solver.cpp:353] Iteration 11700 (5.42584 iter/s, 18.4303s/100 iter), loss = 0.0575542
I0731 18:48:59.908849 16895 solver.cpp:375]     Train net output #0: loss = 0.0575541 (* 1 = 0.0575541 loss)
I0731 18:48:59.908856 16895 sgd_solver.cpp:136] Iteration 11700, lr = 0.0001, m = 0.9
I0731 18:49:05.654397 16876 data_reader.cpp:264] Starting prefetch of epoch 9
I0731 18:49:18.379835 16895 solver.cpp:353] Iteration 11800 (5.41401 iter/s, 18.4706s/100 iter), loss = 0.133147
I0731 18:49:18.379859 16895 solver.cpp:375]     Train net output #0: loss = 0.133147 (* 1 = 0.133147 loss)
I0731 18:49:18.379863 16895 sgd_solver.cpp:136] Iteration 11800, lr = 0.0001, m = 0.9
I0731 18:49:37.030122 16895 solver.cpp:353] Iteration 11900 (5.362 iter/s, 18.6498s/100 iter), loss = 0.108828
I0731 18:49:37.030227 16895 solver.cpp:375]     Train net output #0: loss = 0.108828 (* 1 = 0.108828 loss)
I0731 18:49:37.030234 16895 sgd_solver.cpp:136] Iteration 11900, lr = 0.0001, m = 0.9
I0731 18:49:55.382186 16895 solver.cpp:550] Iteration 12000, Testing net (#0)
I0731 18:49:59.247328 16893 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 18:50:11.079319 16934 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 18:50:11.079319 16938 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 18:50:11.559463 16895 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.94416
I0731 18:50:11.559484 16895 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999996
I0731 18:50:11.559490 16895 solver.cpp:635]     Test net output #2: loss = 0.153105 (* 1 = 0.153105 loss)
I0731 18:50:11.559520 16895 solver.cpp:305] [MultiGPU] Tests completed in 16.1769s
I0731 18:50:11.754133 16895 solver.cpp:353] Iteration 12000 (2.87993 iter/s, 34.723s/100 iter), loss = 0.0631996
I0731 18:50:11.754179 16895 solver.cpp:375]     Train net output #0: loss = 0.0631995 (* 1 = 0.0631995 loss)
I0731 18:50:11.754189 16895 sgd_solver.cpp:136] Iteration 12000, lr = 0.0001, m = 0.9
I0731 18:50:30.423099 16895 solver.cpp:353] Iteration 12100 (5.35663 iter/s, 18.6684s/100 iter), loss = 0.139045
I0731 18:50:30.423121 16895 solver.cpp:375]     Train net output #0: loss = 0.139045 (* 1 = 0.139045 loss)
I0731 18:50:30.423125 16895 sgd_solver.cpp:136] Iteration 12100, lr = 0.0001, m = 0.9
I0731 18:50:48.974345 16895 solver.cpp:353] Iteration 12200 (5.39062 iter/s, 18.5507s/100 iter), loss = 0.132657
I0731 18:50:48.974390 16895 solver.cpp:375]     Train net output #0: loss = 0.132657 (* 1 = 0.132657 loss)
I0731 18:50:48.974397 16895 sgd_solver.cpp:136] Iteration 12200, lr = 0.0001, m = 0.9
I0731 18:50:53.991716 16898 data_reader.cpp:264] Starting prefetch of epoch 5
I0731 18:51:07.577587 16895 solver.cpp:353] Iteration 12300 (5.37556 iter/s, 18.6027s/100 iter), loss = 0.09952
I0731 18:51:07.577613 16895 solver.cpp:375]     Train net output #0: loss = 0.0995199 (* 1 = 0.0995199 loss)
I0731 18:51:07.577618 16895 sgd_solver.cpp:136] Iteration 12300, lr = 0.0001, m = 0.9
I0731 18:51:26.004676 16895 solver.cpp:353] Iteration 12400 (5.42694 iter/s, 18.4266s/100 iter), loss = 0.0791331
I0731 18:51:26.004776 16895 solver.cpp:375]     Train net output #0: loss = 0.079133 (* 1 = 0.079133 loss)
I0731 18:51:26.004783 16895 sgd_solver.cpp:136] Iteration 12400, lr = 0.0001, m = 0.9
I0731 18:51:44.428256 16895 solver.cpp:353] Iteration 12500 (5.42798 iter/s, 18.4231s/100 iter), loss = 0.093474
I0731 18:51:44.428282 16895 solver.cpp:375]     Train net output #0: loss = 0.0934739 (* 1 = 0.0934739 loss)
I0731 18:51:44.428285 16895 sgd_solver.cpp:136] Iteration 12500, lr = 0.0001, m = 0.9
I0731 18:51:55.243103 16901 data_reader.cpp:264] Starting prefetch of epoch 5
I0731 18:52:03.042153 16895 solver.cpp:353] Iteration 12600 (5.37248 iter/s, 18.6134s/100 iter), loss = 0.119121
I0731 18:52:03.042242 16895 solver.cpp:375]     Train net output #0: loss = 0.119121 (* 1 = 0.119121 loss)
I0731 18:52:03.042250 16895 sgd_solver.cpp:136] Iteration 12600, lr = 0.0001, m = 0.9
I0731 18:52:21.614773 16895 solver.cpp:353] Iteration 12700 (5.38442 iter/s, 18.5721s/100 iter), loss = 0.247917
I0731 18:52:21.614796 16895 solver.cpp:375]     Train net output #0: loss = 0.247917 (* 1 = 0.247917 loss)
I0731 18:52:21.614800 16895 sgd_solver.cpp:136] Iteration 12700, lr = 0.0001, m = 0.9
I0731 18:52:25.965541 16898 data_reader.cpp:264] Starting prefetch of epoch 6
I0731 18:52:40.149981 16895 solver.cpp:353] Iteration 12800 (5.39529 iter/s, 18.5347s/100 iter), loss = 0.0774385
I0731 18:52:40.150027 16895 solver.cpp:375]     Train net output #0: loss = 0.0774383 (* 1 = 0.0774383 loss)
I0731 18:52:40.150033 16895 sgd_solver.cpp:136] Iteration 12800, lr = 0.0001, m = 0.9
I0731 18:52:58.714998 16895 solver.cpp:353] Iteration 12900 (5.38662 iter/s, 18.5645s/100 iter), loss = 0.103083
I0731 18:52:58.715023 16895 solver.cpp:375]     Train net output #0: loss = 0.103083 (* 1 = 0.103083 loss)
I0731 18:52:58.715026 16895 sgd_solver.cpp:136] Iteration 12900, lr = 0.0001, m = 0.9
I0731 18:53:17.233166 16895 solver.cpp:353] Iteration 13000 (5.40025 iter/s, 18.5177s/100 iter), loss = 0.0824777
I0731 18:53:17.233268 16895 solver.cpp:375]     Train net output #0: loss = 0.0824775 (* 1 = 0.0824775 loss)
I0731 18:53:17.233274 16895 sgd_solver.cpp:136] Iteration 13000, lr = 0.0001, m = 0.9
I0731 18:53:27.132706 16898 data_reader.cpp:264] Starting prefetch of epoch 7
I0731 18:53:35.848454 16895 solver.cpp:353] Iteration 13100 (5.37208 iter/s, 18.6148s/100 iter), loss = 0.185881
I0731 18:53:35.848479 16895 solver.cpp:375]     Train net output #0: loss = 0.18588 (* 1 = 0.18588 loss)
I0731 18:53:35.848484 16895 sgd_solver.cpp:136] Iteration 13100, lr = 0.0001, m = 0.9
I0731 18:53:54.376802 16895 solver.cpp:353] Iteration 13200 (5.39728 iter/s, 18.5278s/100 iter), loss = 0.0818811
I0731 18:53:54.376857 16895 solver.cpp:375]     Train net output #0: loss = 0.0818809 (* 1 = 0.0818809 loss)
I0731 18:53:54.376864 16895 sgd_solver.cpp:136] Iteration 13200, lr = 0.0001, m = 0.9
I0731 18:54:12.920202 16895 solver.cpp:353] Iteration 13300 (5.3929 iter/s, 18.5429s/100 iter), loss = 0.0860399
I0731 18:54:12.920228 16895 solver.cpp:375]     Train net output #0: loss = 0.0860397 (* 1 = 0.0860397 loss)
I0731 18:54:12.920231 16895 sgd_solver.cpp:136] Iteration 13300, lr = 0.0001, m = 0.9
I0731 18:54:28.543704 16901 data_reader.cpp:264] Starting prefetch of epoch 6
I0731 18:54:31.544867 16895 solver.cpp:353] Iteration 13400 (5.36937 iter/s, 18.6242s/100 iter), loss = 0.083717
I0731 18:54:31.544895 16895 solver.cpp:375]     Train net output #0: loss = 0.0837168 (* 1 = 0.0837168 loss)
I0731 18:54:31.544903 16895 sgd_solver.cpp:136] Iteration 13400, lr = 0.0001, m = 0.9
I0731 18:54:50.086377 16895 solver.cpp:353] Iteration 13500 (5.39345 iter/s, 18.541s/100 iter), loss = 0.133579
I0731 18:54:50.086428 16895 solver.cpp:375]     Train net output #0: loss = 0.133579 (* 1 = 0.133579 loss)
I0731 18:54:50.086441 16895 sgd_solver.cpp:136] Iteration 13500, lr = 0.0001, m = 0.9
I0731 18:54:59.167177 16898 data_reader.cpp:264] Starting prefetch of epoch 8
I0731 18:55:08.550746 16895 solver.cpp:353] Iteration 13600 (5.41598 iter/s, 18.4639s/100 iter), loss = 0.113525
I0731 18:55:08.550770 16895 solver.cpp:375]     Train net output #0: loss = 0.113525 (* 1 = 0.113525 loss)
I0731 18:55:08.550773 16895 sgd_solver.cpp:136] Iteration 13600, lr = 0.0001, m = 0.9
I0731 18:55:27.154137 16895 solver.cpp:353] Iteration 13700 (5.37551 iter/s, 18.6029s/100 iter), loss = 0.0830864
I0731 18:55:27.154161 16895 solver.cpp:375]     Train net output #0: loss = 0.0830862 (* 1 = 0.0830862 loss)
I0731 18:55:27.154166 16895 sgd_solver.cpp:136] Iteration 13700, lr = 0.0001, m = 0.9
I0731 18:55:45.673655 16895 solver.cpp:353] Iteration 13800 (5.39986 iter/s, 18.519s/100 iter), loss = 0.103585
I0731 18:55:45.673771 16895 solver.cpp:375]     Train net output #0: loss = 0.103585 (* 1 = 0.103585 loss)
I0731 18:55:45.673779 16895 sgd_solver.cpp:136] Iteration 13800, lr = 0.0001, m = 0.9
I0731 18:56:00.570564 16874 data_reader.cpp:264] Starting prefetch of epoch 10
I0731 18:56:04.204515 16895 solver.cpp:353] Iteration 13900 (5.39655 iter/s, 18.5304s/100 iter), loss = 0.109163
I0731 18:56:04.204540 16895 solver.cpp:375]     Train net output #0: loss = 0.109163 (* 1 = 0.109163 loss)
I0731 18:56:04.204545 16895 sgd_solver.cpp:136] Iteration 13900, lr = 0.0001, m = 0.9
I0731 18:56:22.544787 16895 solver.cpp:550] Iteration 14000, Testing net (#0)
I0731 18:56:33.837448 16895 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.947747
I0731 18:56:33.837472 16895 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999986
I0731 18:56:33.837477 16895 solver.cpp:635]     Test net output #2: loss = 0.155834 (* 1 = 0.155834 loss)
I0731 18:56:33.837581 16895 solver.cpp:305] [MultiGPU] Tests completed in 11.2925s
I0731 18:56:34.027556 16895 solver.cpp:353] Iteration 14000 (3.3532 iter/s, 29.8222s/100 iter), loss = 0.134754
I0731 18:56:34.027582 16895 solver.cpp:375]     Train net output #0: loss = 0.134754 (* 1 = 0.134754 loss)
I0731 18:56:34.027590 16895 sgd_solver.cpp:136] Iteration 14000, lr = 0.0001, m = 0.9
I0731 18:56:42.423758 16898 data_reader.cpp:264] Starting prefetch of epoch 9
I0731 18:56:52.683917 16895 solver.cpp:353] Iteration 14100 (5.36025 iter/s, 18.6558s/100 iter), loss = 0.146534
I0731 18:56:52.683969 16895 solver.cpp:375]     Train net output #0: loss = 0.146534 (* 1 = 0.146534 loss)
I0731 18:56:52.683975 16895 sgd_solver.cpp:136] Iteration 14100, lr = 0.0001, m = 0.9
I0731 18:57:11.207749 16895 solver.cpp:353] Iteration 14200 (5.3986 iter/s, 18.5233s/100 iter), loss = 0.0979127
I0731 18:57:11.207778 16895 solver.cpp:375]     Train net output #0: loss = 0.0979126 (* 1 = 0.0979126 loss)
I0731 18:57:11.207785 16895 sgd_solver.cpp:136] Iteration 14200, lr = 0.0001, m = 0.9
I0731 18:57:29.884889 16895 solver.cpp:353] Iteration 14300 (5.35429 iter/s, 18.6766s/100 iter), loss = 0.162341
I0731 18:57:29.884999 16895 solver.cpp:375]     Train net output #0: loss = 0.162341 (* 1 = 0.162341 loss)
I0731 18:57:29.885006 16895 sgd_solver.cpp:136] Iteration 14300, lr = 0.0001, m = 0.9
I0731 18:57:43.944159 16901 data_reader.cpp:264] Starting prefetch of epoch 7
I0731 18:57:48.456812 16895 solver.cpp:353] Iteration 14400 (5.38462 iter/s, 18.5714s/100 iter), loss = 0.103589
I0731 18:57:48.456843 16895 solver.cpp:375]     Train net output #0: loss = 0.103589 (* 1 = 0.103589 loss)
I0731 18:57:48.456847 16895 sgd_solver.cpp:136] Iteration 14400, lr = 0.0001, m = 0.9
I0731 18:58:07.050645 16895 solver.cpp:353] Iteration 14500 (5.37828 iter/s, 18.5933s/100 iter), loss = 0.106696
I0731 18:58:07.050693 16895 solver.cpp:375]     Train net output #0: loss = 0.106696 (* 1 = 0.106696 loss)
I0731 18:58:07.050698 16895 sgd_solver.cpp:136] Iteration 14500, lr = 0.0001, m = 0.9
I0731 18:58:25.612597 16895 solver.cpp:353] Iteration 14600 (5.38751 iter/s, 18.5614s/100 iter), loss = 0.0996438
I0731 18:58:25.612622 16895 solver.cpp:375]     Train net output #0: loss = 0.0996437 (* 1 = 0.0996437 loss)
I0731 18:58:25.612627 16895 sgd_solver.cpp:136] Iteration 14600, lr = 0.0001, m = 0.9
I0731 18:58:44.105226 16895 solver.cpp:353] Iteration 14700 (5.40771 iter/s, 18.4921s/100 iter), loss = 0.180351
I0731 18:58:44.105332 16895 solver.cpp:375]     Train net output #0: loss = 0.180351 (* 1 = 0.180351 loss)
I0731 18:58:44.105340 16895 sgd_solver.cpp:136] Iteration 14700, lr = 0.0001, m = 0.9
I0731 18:58:45.227049 16876 data_reader.cpp:264] Starting prefetch of epoch 10
I0731 18:59:02.767776 16895 solver.cpp:353] Iteration 14800 (5.35847 iter/s, 18.662s/100 iter), loss = 0.0995206
I0731 18:59:02.767801 16895 solver.cpp:375]     Train net output #0: loss = 0.0995205 (* 1 = 0.0995205 loss)
I0731 18:59:02.767805 16895 sgd_solver.cpp:136] Iteration 14800, lr = 0.0001, m = 0.9
I0731 18:59:15.868837 16874 data_reader.cpp:264] Starting prefetch of epoch 11
I0731 18:59:21.177620 16895 solver.cpp:353] Iteration 14900 (5.43203 iter/s, 18.4093s/100 iter), loss = 0.177936
I0731 18:59:21.177649 16895 solver.cpp:375]     Train net output #0: loss = 0.177936 (* 1 = 0.177936 loss)
I0731 18:59:21.177655 16895 sgd_solver.cpp:136] Iteration 14900, lr = 0.0001, m = 0.9
I0731 18:59:39.720043 16895 solver.cpp:353] Iteration 15000 (5.39319 iter/s, 18.5419s/100 iter), loss = 0.0922562
I0731 18:59:39.720072 16895 solver.cpp:375]     Train net output #0: loss = 0.0922561 (* 1 = 0.0922561 loss)
I0731 18:59:39.720078 16895 sgd_solver.cpp:136] Iteration 15000, lr = 0.0001, m = 0.9
I0731 18:59:58.491348 16895 solver.cpp:353] Iteration 15100 (5.32743 iter/s, 18.7708s/100 iter), loss = 0.191082
I0731 18:59:58.491427 16895 solver.cpp:375]     Train net output #0: loss = 0.191082 (* 1 = 0.191082 loss)
I0731 18:59:58.491435 16895 sgd_solver.cpp:136] Iteration 15100, lr = 0.0001, m = 0.9
I0731 19:00:17.120600 16895 solver.cpp:353] Iteration 15200 (5.36805 iter/s, 18.6287s/100 iter), loss = 0.106029
I0731 19:00:17.120628 16895 solver.cpp:375]     Train net output #0: loss = 0.106029 (* 1 = 0.106029 loss)
I0731 19:00:17.120633 16895 sgd_solver.cpp:136] Iteration 15200, lr = 0.0001, m = 0.9
I0731 19:00:17.545142 16901 data_reader.cpp:264] Starting prefetch of epoch 8
I0731 19:00:35.737596 16895 solver.cpp:353] Iteration 15300 (5.37158 iter/s, 18.6165s/100 iter), loss = 0.0486253
I0731 19:00:35.737645 16895 solver.cpp:375]     Train net output #0: loss = 0.0486251 (* 1 = 0.0486251 loss)
I0731 19:00:35.737650 16895 sgd_solver.cpp:136] Iteration 15300, lr = 0.0001, m = 0.9
I0731 19:00:54.333246 16895 solver.cpp:353] Iteration 15400 (5.37775 iter/s, 18.5951s/100 iter), loss = 0.108729
I0731 19:00:54.333272 16895 solver.cpp:375]     Train net output #0: loss = 0.108728 (* 1 = 0.108728 loss)
I0731 19:00:54.333276 16895 sgd_solver.cpp:136] Iteration 15400, lr = 0.0001, m = 0.9
I0731 19:01:12.878336 16895 solver.cpp:353] Iteration 15500 (5.39241 iter/s, 18.5446s/100 iter), loss = 0.0940228
I0731 19:01:12.878388 16895 solver.cpp:375]     Train net output #0: loss = 0.0940227 (* 1 = 0.0940227 loss)
I0731 19:01:12.878393 16895 sgd_solver.cpp:136] Iteration 15500, lr = 0.0001, m = 0.9
I0731 19:01:19.000459 16901 data_reader.cpp:264] Starting prefetch of epoch 9
I0731 19:01:31.452477 16895 solver.cpp:353] Iteration 15600 (5.38398 iter/s, 18.5736s/100 iter), loss = 0.0937304
I0731 19:01:31.452500 16895 solver.cpp:375]     Train net output #0: loss = 0.0937303 (* 1 = 0.0937303 loss)
I0731 19:01:31.452504 16895 sgd_solver.cpp:136] Iteration 15600, lr = 0.0001, m = 0.9
I0731 19:01:49.662520 16898 data_reader.cpp:264] Starting prefetch of epoch 10
I0731 19:01:49.983714 16895 solver.cpp:353] Iteration 15700 (5.39644 iter/s, 18.5307s/100 iter), loss = 0.120774
I0731 19:01:49.983737 16895 solver.cpp:375]     Train net output #0: loss = 0.120773 (* 1 = 0.120773 loss)
I0731 19:01:49.983741 16895 sgd_solver.cpp:136] Iteration 15700, lr = 0.0001, m = 0.9
I0731 19:02:08.564663 16895 solver.cpp:353] Iteration 15800 (5.382 iter/s, 18.5804s/100 iter), loss = 0.110289
I0731 19:02:08.564687 16895 solver.cpp:375]     Train net output #0: loss = 0.110289 (* 1 = 0.110289 loss)
I0731 19:02:08.564692 16895 sgd_solver.cpp:136] Iteration 15800, lr = 0.0001, m = 0.9
I0731 19:02:27.104354 16895 solver.cpp:353] Iteration 15900 (5.39398 iter/s, 18.5392s/100 iter), loss = 0.0698556
I0731 19:02:27.104406 16895 solver.cpp:375]     Train net output #0: loss = 0.0698554 (* 1 = 0.0698554 loss)
I0731 19:02:27.104413 16895 sgd_solver.cpp:136] Iteration 15900, lr = 0.0001, m = 0.9
I0731 19:02:45.498348 16895 solver.cpp:550] Iteration 16000, Testing net (#0)
I0731 19:02:48.844746 16891 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 19:02:56.564635 16895 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.948936
I0731 19:02:56.564658 16895 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999998
I0731 19:02:56.564663 16895 solver.cpp:635]     Test net output #2: loss = 0.149419 (* 1 = 0.149419 loss)
I0731 19:02:56.564744 16895 solver.cpp:305] [MultiGPU] Tests completed in 11.0661s
I0731 19:02:56.767482 16895 solver.cpp:353] Iteration 16000 (3.37128 iter/s, 29.6623s/100 iter), loss = 0.112194
I0731 19:02:56.767504 16895 solver.cpp:375]     Train net output #0: loss = 0.112193 (* 1 = 0.112193 loss)
I0731 19:02:56.767508 16895 sgd_solver.cpp:136] Iteration 16000, lr = 0.0001, m = 0.9
I0731 19:03:01.974112 16903 data_reader.cpp:264] Starting prefetch of epoch 10
I0731 19:03:15.306840 16895 solver.cpp:353] Iteration 16100 (5.39408 iter/s, 18.5388s/100 iter), loss = 0.112619
I0731 19:03:15.306864 16895 solver.cpp:375]     Train net output #0: loss = 0.112619 (* 1 = 0.112619 loss)
I0731 19:03:15.306869 16895 sgd_solver.cpp:136] Iteration 16100, lr = 0.0001, m = 0.9
I0731 19:03:32.799865 16899 data_reader.cpp:264] Starting prefetch of epoch 10
I0731 19:03:33.883098 16895 solver.cpp:353] Iteration 16200 (5.38336 iter/s, 18.5757s/100 iter), loss = 0.155786
I0731 19:03:33.883121 16895 solver.cpp:375]     Train net output #0: loss = 0.155786 (* 1 = 0.155786 loss)
I0731 19:03:33.883124 16895 sgd_solver.cpp:136] Iteration 16200, lr = 0.0001, m = 0.9
I0731 19:03:52.466433 16895 solver.cpp:353] Iteration 16300 (5.38132 iter/s, 18.5828s/100 iter), loss = 0.0932663
I0731 19:03:52.466456 16895 solver.cpp:375]     Train net output #0: loss = 0.0932661 (* 1 = 0.0932661 loss)
I0731 19:03:52.466464 16895 sgd_solver.cpp:136] Iteration 16300, lr = 0.0001, m = 0.9
I0731 19:04:11.143896 16895 solver.cpp:353] Iteration 16400 (5.35419 iter/s, 18.6769s/100 iter), loss = 0.068053
I0731 19:04:11.143946 16895 solver.cpp:375]     Train net output #0: loss = 0.0680528 (* 1 = 0.0680528 loss)
I0731 19:04:11.143954 16895 sgd_solver.cpp:136] Iteration 16400, lr = 0.0001, m = 0.9
I0731 19:04:29.609537 16895 solver.cpp:353] Iteration 16500 (5.41561 iter/s, 18.4651s/100 iter), loss = 0.0902472
I0731 19:04:29.609562 16895 solver.cpp:375]     Train net output #0: loss = 0.090247 (* 1 = 0.090247 loss)
I0731 19:04:29.609568 16895 sgd_solver.cpp:136] Iteration 16500, lr = 0.0001, m = 0.9
I0731 19:04:34.118156 16898 data_reader.cpp:264] Starting prefetch of epoch 11
I0731 19:04:48.239200 16895 solver.cpp:353] Iteration 16600 (5.36793 iter/s, 18.6291s/100 iter), loss = 0.0927396
I0731 19:04:48.239253 16895 solver.cpp:375]     Train net output #0: loss = 0.0927394 (* 1 = 0.0927394 loss)
I0731 19:04:48.239259 16895 sgd_solver.cpp:136] Iteration 16600, lr = 0.0001, m = 0.9
I0731 19:05:06.816862 16895 solver.cpp:353] Iteration 16700 (5.38296 iter/s, 18.5771s/100 iter), loss = 0.282002
I0731 19:05:06.816889 16895 solver.cpp:375]     Train net output #0: loss = 0.282001 (* 1 = 0.282001 loss)
I0731 19:05:06.816892 16895 sgd_solver.cpp:136] Iteration 16700, lr = 0.0001, m = 0.9
I0731 19:05:25.363725 16895 solver.cpp:353] Iteration 16800 (5.3919 iter/s, 18.5464s/100 iter), loss = 0.114452
I0731 19:05:25.363782 16895 solver.cpp:375]     Train net output #0: loss = 0.114451 (* 1 = 0.114451 loss)
I0731 19:05:25.363787 16895 sgd_solver.cpp:136] Iteration 16800, lr = 0.0001, m = 0.9
I0731 19:05:35.601124 16903 data_reader.cpp:264] Starting prefetch of epoch 11
I0731 19:05:43.998019 16895 solver.cpp:353] Iteration 16900 (5.3666 iter/s, 18.6338s/100 iter), loss = 0.097389
I0731 19:05:43.998042 16895 solver.cpp:375]     Train net output #0: loss = 0.0973887 (* 1 = 0.0973887 loss)
I0731 19:05:43.998046 16895 sgd_solver.cpp:136] Iteration 16900, lr = 0.0001, m = 0.9
I0731 19:06:02.658252 16895 solver.cpp:353] Iteration 17000 (5.35914 iter/s, 18.6597s/100 iter), loss = 0.149229
I0731 19:06:02.658316 16895 solver.cpp:375]     Train net output #0: loss = 0.149229 (* 1 = 0.149229 loss)
I0731 19:06:02.658323 16895 sgd_solver.cpp:136] Iteration 17000, lr = 0.0001, m = 0.9
I0731 19:06:06.378216 16898 data_reader.cpp:264] Starting prefetch of epoch 12
I0731 19:06:21.097846 16895 solver.cpp:353] Iteration 17100 (5.42326 iter/s, 18.4391s/100 iter), loss = 0.124452
I0731 19:06:21.097868 16895 solver.cpp:375]     Train net output #0: loss = 0.124452 (* 1 = 0.124452 loss)
I0731 19:06:21.097872 16895 sgd_solver.cpp:136] Iteration 17100, lr = 0.0001, m = 0.9
I0731 19:06:39.578052 16895 solver.cpp:353] Iteration 17200 (5.41135 iter/s, 18.4797s/100 iter), loss = 0.128592
I0731 19:06:39.578105 16895 solver.cpp:375]     Train net output #0: loss = 0.128592 (* 1 = 0.128592 loss)
I0731 19:06:39.578110 16895 sgd_solver.cpp:136] Iteration 17200, lr = 0.0001, m = 0.9
I0731 19:06:58.159934 16895 solver.cpp:353] Iteration 17300 (5.38174 iter/s, 18.5814s/100 iter), loss = 0.0762832
I0731 19:06:58.159958 16895 solver.cpp:375]     Train net output #0: loss = 0.0762829 (* 1 = 0.0762829 loss)
I0731 19:06:58.159965 16895 sgd_solver.cpp:136] Iteration 17300, lr = 0.0001, m = 0.9
I0731 19:07:07.704155 16899 data_reader.cpp:264] Starting prefetch of epoch 11
I0731 19:07:16.661224 16895 solver.cpp:353] Iteration 17400 (5.40518 iter/s, 18.5008s/100 iter), loss = 0.0661483
I0731 19:07:16.661301 16895 solver.cpp:375]     Train net output #0: loss = 0.066148 (* 1 = 0.066148 loss)
I0731 19:07:16.661306 16895 sgd_solver.cpp:136] Iteration 17400, lr = 0.0001, m = 0.9
I0731 19:07:35.313269 16895 solver.cpp:353] Iteration 17500 (5.36149 iter/s, 18.6515s/100 iter), loss = 0.0654078
I0731 19:07:35.313293 16895 solver.cpp:375]     Train net output #0: loss = 0.0654076 (* 1 = 0.0654076 loss)
I0731 19:07:35.313299 16895 sgd_solver.cpp:136] Iteration 17500, lr = 0.0001, m = 0.9
I0731 19:07:53.883342 16895 solver.cpp:353] Iteration 17600 (5.38516 iter/s, 18.5696s/100 iter), loss = 0.184322
I0731 19:07:53.883385 16895 solver.cpp:375]     Train net output #0: loss = 0.184322 (* 1 = 0.184322 loss)
I0731 19:07:53.883390 16895 sgd_solver.cpp:136] Iteration 17600, lr = 0.0001, m = 0.9
I0731 19:08:08.911833 16876 data_reader.cpp:264] Starting prefetch of epoch 11
I0731 19:08:12.406455 16895 solver.cpp:353] Iteration 17700 (5.39881 iter/s, 18.5226s/100 iter), loss = 0.0843731
I0731 19:08:12.406481 16895 solver.cpp:375]     Train net output #0: loss = 0.0843729 (* 1 = 0.0843729 loss)
I0731 19:08:12.406484 16895 sgd_solver.cpp:136] Iteration 17700, lr = 0.0001, m = 0.9
I0731 19:08:30.867286 16895 solver.cpp:353] Iteration 17800 (5.41702 iter/s, 18.4603s/100 iter), loss = 0.082174
I0731 19:08:30.867341 16895 solver.cpp:375]     Train net output #0: loss = 0.0821738 (* 1 = 0.0821738 loss)
I0731 19:08:30.867347 16895 sgd_solver.cpp:136] Iteration 17800, lr = 0.0001, m = 0.9
I0731 19:08:39.359382 16899 data_reader.cpp:264] Starting prefetch of epoch 12
I0731 19:08:49.358778 16895 solver.cpp:353] Iteration 17900 (5.40804 iter/s, 18.491s/100 iter), loss = 0.117314
I0731 19:08:49.358805 16895 solver.cpp:375]     Train net output #0: loss = 0.117314 (* 1 = 0.117314 loss)
I0731 19:08:49.358809 16895 sgd_solver.cpp:136] Iteration 17900, lr = 0.0001, m = 0.9
I0731 19:09:07.735949 16895 solver.cpp:550] Iteration 18000, Testing net (#0)
I0731 19:09:18.811198 16895 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.945321
I0731 19:09:18.811223 16895 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999869
I0731 19:09:18.811228 16895 solver.cpp:635]     Test net output #2: loss = 0.177661 (* 1 = 0.177661 loss)
I0731 19:09:18.811327 16895 solver.cpp:305] [MultiGPU] Tests completed in 11.0751s
I0731 19:09:19.001227 16895 solver.cpp:353] Iteration 18000 (3.37363 iter/s, 29.6416s/100 iter), loss = 0.104226
I0731 19:09:19.001251 16895 solver.cpp:375]     Train net output #0: loss = 0.104225 (* 1 = 0.104225 loss)
I0731 19:09:19.001255 16895 sgd_solver.cpp:136] Iteration 18000, lr = 0.0001, m = 0.9
I0731 19:09:21.276152 16876 data_reader.cpp:264] Starting prefetch of epoch 12
I0731 19:09:37.554942 16895 solver.cpp:353] Iteration 18100 (5.38991 iter/s, 18.5532s/100 iter), loss = 0.108922
I0731 19:09:37.554971 16895 solver.cpp:375]     Train net output #0: loss = 0.108922 (* 1 = 0.108922 loss)
I0731 19:09:37.554978 16895 sgd_solver.cpp:136] Iteration 18100, lr = 0.0001, m = 0.9
I0731 19:09:56.261030 16895 solver.cpp:353] Iteration 18200 (5.346 iter/s, 18.7056s/100 iter), loss = 0.116848
I0731 19:09:56.261096 16895 solver.cpp:375]     Train net output #0: loss = 0.116847 (* 1 = 0.116847 loss)
I0731 19:09:56.261102 16895 sgd_solver.cpp:136] Iteration 18200, lr = 0.0001, m = 0.9
I0731 19:10:14.871625 16895 solver.cpp:353] Iteration 18300 (5.37343 iter/s, 18.6101s/100 iter), loss = 0.137911
I0731 19:10:14.871649 16895 solver.cpp:375]     Train net output #0: loss = 0.137911 (* 1 = 0.137911 loss)
I0731 19:10:14.871652 16895 sgd_solver.cpp:136] Iteration 18300, lr = 0.0001, m = 0.9
I0731 19:10:22.715400 16876 data_reader.cpp:264] Starting prefetch of epoch 13
I0731 19:10:33.448885 16895 solver.cpp:353] Iteration 18400 (5.38307 iter/s, 18.5767s/100 iter), loss = 0.092573
I0731 19:10:33.448938 16895 solver.cpp:375]     Train net output #0: loss = 0.0925729 (* 1 = 0.0925729 loss)
I0731 19:10:33.448943 16895 sgd_solver.cpp:136] Iteration 18400, lr = 0.0001, m = 0.9
I0731 19:10:52.052911 16895 solver.cpp:353] Iteration 18500 (5.37533 iter/s, 18.6035s/100 iter), loss = 0.155035
I0731 19:10:52.052937 16895 solver.cpp:375]     Train net output #0: loss = 0.155035 (* 1 = 0.155035 loss)
I0731 19:10:52.052943 16895 sgd_solver.cpp:136] Iteration 18500, lr = 0.0001, m = 0.9
I0731 19:10:53.543772 16898 data_reader.cpp:264] Starting prefetch of epoch 13
I0731 19:11:10.615413 16895 solver.cpp:353] Iteration 18600 (5.38735 iter/s, 18.562s/100 iter), loss = 0.122815
I0731 19:11:10.615469 16895 solver.cpp:375]     Train net output #0: loss = 0.122815 (* 1 = 0.122815 loss)
I0731 19:11:10.615476 16895 sgd_solver.cpp:136] Iteration 18600, lr = 0.0001, m = 0.9
I0731 19:11:29.223745 16895 solver.cpp:353] Iteration 18700 (5.37408 iter/s, 18.6078s/100 iter), loss = 0.0587228
I0731 19:11:29.223770 16895 solver.cpp:375]     Train net output #0: loss = 0.0587227 (* 1 = 0.0587227 loss)
I0731 19:11:29.223774 16895 sgd_solver.cpp:136] Iteration 18700, lr = 0.0001, m = 0.9
I0731 19:11:47.965447 16895 solver.cpp:353] Iteration 18800 (5.33584 iter/s, 18.7412s/100 iter), loss = 0.245342
I0731 19:11:47.965531 16895 solver.cpp:375]     Train net output #0: loss = 0.245342 (* 1 = 0.245342 loss)
I0731 19:11:47.965538 16895 sgd_solver.cpp:136] Iteration 18800, lr = 0.0001, m = 0.9
I0731 19:11:55.067176 16874 data_reader.cpp:264] Starting prefetch of epoch 12
I0731 19:12:06.501562 16895 solver.cpp:353] Iteration 18900 (5.39502 iter/s, 18.5356s/100 iter), loss = 0.121225
I0731 19:12:06.501585 16895 solver.cpp:375]     Train net output #0: loss = 0.121225 (* 1 = 0.121225 loss)
I0731 19:12:06.501590 16895 sgd_solver.cpp:136] Iteration 18900, lr = 0.0001, m = 0.9
I0731 19:12:25.206038 16895 solver.cpp:353] Iteration 19000 (5.34646 iter/s, 18.704s/100 iter), loss = 0.211816
I0731 19:12:25.206090 16895 solver.cpp:375]     Train net output #0: loss = 0.211816 (* 1 = 0.211816 loss)
I0731 19:12:25.206096 16895 sgd_solver.cpp:136] Iteration 19000, lr = 0.0001, m = 0.9
I0731 19:12:43.848235 16895 solver.cpp:353] Iteration 19100 (5.36432 iter/s, 18.6417s/100 iter), loss = 0.0720573
I0731 19:12:43.848263 16895 solver.cpp:375]     Train net output #0: loss = 0.0720571 (* 1 = 0.0720571 loss)
I0731 19:12:43.848266 16895 sgd_solver.cpp:136] Iteration 19100, lr = 0.0001, m = 0.9
I0731 19:12:56.723909 16876 data_reader.cpp:264] Starting prefetch of epoch 14
I0731 19:13:02.511258 16895 solver.cpp:353] Iteration 19200 (5.35834 iter/s, 18.6625s/100 iter), loss = 0.123291
I0731 19:13:02.511281 16895 solver.cpp:375]     Train net output #0: loss = 0.123291 (* 1 = 0.123291 loss)
I0731 19:13:02.511286 16895 sgd_solver.cpp:136] Iteration 19200, lr = 0.0001, m = 0.9
I0731 19:13:21.123739 16895 solver.cpp:353] Iteration 19300 (5.37289 iter/s, 18.612s/100 iter), loss = 0.127959
I0731 19:13:21.123759 16895 solver.cpp:375]     Train net output #0: loss = 0.127959 (* 1 = 0.127959 loss)
I0731 19:13:21.123764 16895 sgd_solver.cpp:136] Iteration 19300, lr = 0.0001, m = 0.9
I0731 19:13:27.517976 16899 data_reader.cpp:264] Starting prefetch of epoch 13
I0731 19:13:39.634241 16895 solver.cpp:353] Iteration 19400 (5.40249 iter/s, 18.51s/100 iter), loss = 0.122667
I0731 19:13:39.634265 16895 solver.cpp:375]     Train net output #0: loss = 0.122667 (* 1 = 0.122667 loss)
I0731 19:13:39.634269 16895 sgd_solver.cpp:136] Iteration 19400, lr = 0.0001, m = 0.9
I0731 19:13:58.218338 16895 solver.cpp:353] Iteration 19500 (5.38109 iter/s, 18.5836s/100 iter), loss = 0.0873203
I0731 19:13:58.218390 16895 solver.cpp:375]     Train net output #0: loss = 0.0873201 (* 1 = 0.0873201 loss)
I0731 19:13:58.218395 16895 sgd_solver.cpp:136] Iteration 19500, lr = 0.0001, m = 0.9
I0731 19:14:16.805137 16895 solver.cpp:353] Iteration 19600 (5.38031 iter/s, 18.5863s/100 iter), loss = 0.0963952
I0731 19:14:16.805169 16895 solver.cpp:375]     Train net output #0: loss = 0.0963949 (* 1 = 0.0963949 loss)
I0731 19:14:16.805173 16895 sgd_solver.cpp:136] Iteration 19600, lr = 0.0001, m = 0.9
I0731 19:14:28.937301 16899 data_reader.cpp:264] Starting prefetch of epoch 14
I0731 19:14:35.381180 16895 solver.cpp:353] Iteration 19700 (5.38342 iter/s, 18.5755s/100 iter), loss = 0.810466
I0731 19:14:35.381204 16895 solver.cpp:375]     Train net output #0: loss = 0.810466 (* 1 = 0.810466 loss)
I0731 19:14:35.381208 16895 sgd_solver.cpp:136] Iteration 19700, lr = 0.0001, m = 0.9
I0731 19:14:53.953583 16895 solver.cpp:353] Iteration 19800 (5.38448 iter/s, 18.5719s/100 iter), loss = 0.0721171
I0731 19:14:53.953608 16895 solver.cpp:375]     Train net output #0: loss = 0.0721169 (* 1 = 0.0721169 loss)
I0731 19:14:53.953613 16895 sgd_solver.cpp:136] Iteration 19800, lr = 0.0001, m = 0.9
I0731 19:15:12.437906 16895 solver.cpp:353] Iteration 19900 (5.41014 iter/s, 18.4838s/100 iter), loss = 0.0925922
I0731 19:15:12.438002 16895 solver.cpp:375]     Train net output #0: loss = 0.092592 (* 1 = 0.092592 loss)
I0731 19:15:12.438009 16895 sgd_solver.cpp:136] Iteration 19900, lr = 0.0001, m = 0.9
I0731 19:15:30.096657 16899 data_reader.cpp:264] Starting prefetch of epoch 15
I0731 19:15:30.801620 16895 solver.cpp:680] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/initial/cityscapes5_jsegnet21v2_iter_20000.caffemodel
I0731 19:15:30.852090 16895 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/initial/cityscapes5_jsegnet21v2_iter_20000.solverstate
I0731 19:15:30.860527 16895 solver.cpp:550] Iteration 20000, Testing net (#0)
I0731 19:15:41.490460 16893 data_reader.cpp:264] Starting prefetch of epoch 2
I0731 19:15:42.031971 16895 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.949154
I0731 19:15:42.031994 16895 solver.cpp:635]     Test net output #1: accuracy/top5 = 1
I0731 19:15:42.032001 16895 solver.cpp:635]     Test net output #2: loss = 0.146284 (* 1 = 0.146284 loss)
I0731 19:15:42.032104 16895 solver.cpp:305] [MultiGPU] Tests completed in 11.1713s
I0731 19:15:42.235386 16895 solver.cpp:353] Iteration 20000 (3.35608 iter/s, 29.7967s/100 iter), loss = 0.0742942
I0731 19:15:42.235409 16895 solver.cpp:375]     Train net output #0: loss = 0.074294 (* 1 = 0.074294 loss)
I0731 19:15:42.235415 16895 sgd_solver.cpp:136] Iteration 20000, lr = 0.0001, m = 0.9
I0731 19:16:00.898090 16895 solver.cpp:353] Iteration 20100 (5.35843 iter/s, 18.6622s/100 iter), loss = 0.051696
I0731 19:16:00.898201 16895 solver.cpp:375]     Train net output #0: loss = 0.0516958 (* 1 = 0.0516958 loss)
I0731 19:16:00.898208 16895 sgd_solver.cpp:136] Iteration 20100, lr = 0.0001, m = 0.9
I0731 19:16:12.017257 16899 data_reader.cpp:264] Starting prefetch of epoch 16
I0731 19:16:19.427353 16895 solver.cpp:353] Iteration 20200 (5.39702 iter/s, 18.5287s/100 iter), loss = 0.113389
I0731 19:16:19.427376 16895 solver.cpp:375]     Train net output #0: loss = 0.113388 (* 1 = 0.113388 loss)
I0731 19:16:19.427381 16895 sgd_solver.cpp:136] Iteration 20200, lr = 0.0001, m = 0.9
I0731 19:16:37.983608 16895 solver.cpp:353] Iteration 20300 (5.38917 iter/s, 18.5557s/100 iter), loss = 0.0819916
I0731 19:16:37.983680 16895 solver.cpp:375]     Train net output #0: loss = 0.0819914 (* 1 = 0.0819914 loss)
I0731 19:16:37.983685 16895 sgd_solver.cpp:136] Iteration 20300, lr = 0.0001, m = 0.9
I0731 19:16:56.493852 16895 solver.cpp:353] Iteration 20400 (5.40256 iter/s, 18.5097s/100 iter), loss = 0.11921
I0731 19:16:56.493875 16895 solver.cpp:375]     Train net output #0: loss = 0.11921 (* 1 = 0.11921 loss)
I0731 19:16:56.493880 16895 sgd_solver.cpp:136] Iteration 20400, lr = 0.0001, m = 0.9
I0731 19:17:13.386955 16898 data_reader.cpp:264] Starting prefetch of epoch 14
I0731 19:17:15.112869 16895 solver.cpp:353] Iteration 20500 (5.37101 iter/s, 18.6185s/100 iter), loss = 0.0770927
I0731 19:17:15.112895 16895 solver.cpp:375]     Train net output #0: loss = 0.0770925 (* 1 = 0.0770925 loss)
I0731 19:17:15.112898 16895 sgd_solver.cpp:136] Iteration 20500, lr = 0.0001, m = 0.9
I0731 19:17:33.608599 16895 solver.cpp:353] Iteration 20600 (5.4068 iter/s, 18.4952s/100 iter), loss = 0.0820835
I0731 19:17:33.608626 16895 solver.cpp:375]     Train net output #0: loss = 0.0820832 (* 1 = 0.0820832 loss)
I0731 19:17:33.608631 16895 sgd_solver.cpp:136] Iteration 20600, lr = 0.0001, m = 0.9
I0731 19:17:52.128307 16895 solver.cpp:353] Iteration 20700 (5.3998 iter/s, 18.5192s/100 iter), loss = 0.0700263
I0731 19:17:52.128362 16895 solver.cpp:375]     Train net output #0: loss = 0.0700261 (* 1 = 0.0700261 loss)
I0731 19:17:52.128370 16895 sgd_solver.cpp:136] Iteration 20700, lr = 0.0001, m = 0.9
I0731 19:18:10.810611 16895 solver.cpp:353] Iteration 20800 (5.35281 iter/s, 18.6818s/100 iter), loss = 0.0713599
I0731 19:18:10.810633 16895 solver.cpp:375]     Train net output #0: loss = 0.0713597 (* 1 = 0.0713597 loss)
I0731 19:18:10.810638 16895 sgd_solver.cpp:136] Iteration 20800, lr = 0.0001, m = 0.9
I0731 19:18:14.749864 16903 data_reader.cpp:264] Starting prefetch of epoch 12
I0731 19:18:29.365914 16895 solver.cpp:353] Iteration 20900 (5.38944 iter/s, 18.5548s/100 iter), loss = 0.0615994
I0731 19:18:29.365988 16895 solver.cpp:375]     Train net output #0: loss = 0.0615992 (* 1 = 0.0615992 loss)
I0731 19:18:29.366003 16895 sgd_solver.cpp:136] Iteration 20900, lr = 0.0001, m = 0.9
I0731 19:18:47.961951 16895 solver.cpp:353] Iteration 21000 (5.37764 iter/s, 18.5955s/100 iter), loss = 0.149327
I0731 19:18:47.961975 16895 solver.cpp:375]     Train net output #0: loss = 0.149327 (* 1 = 0.149327 loss)
I0731 19:18:47.961982 16895 sgd_solver.cpp:136] Iteration 21000, lr = 0.0001, m = 0.9
I0731 19:19:06.446781 16895 solver.cpp:353] Iteration 21100 (5.40999 iter/s, 18.4843s/100 iter), loss = 0.0814454
I0731 19:19:06.446832 16895 solver.cpp:375]     Train net output #0: loss = 0.0814451 (* 1 = 0.0814451 loss)
I0731 19:19:06.446838 16895 sgd_solver.cpp:136] Iteration 21100, lr = 0.0001, m = 0.9
I0731 19:19:16.067337 16903 data_reader.cpp:264] Starting prefetch of epoch 13
I0731 19:19:25.053400 16895 solver.cpp:353] Iteration 21200 (5.37458 iter/s, 18.6061s/100 iter), loss = 0.115749
I0731 19:19:25.053423 16895 solver.cpp:375]     Train net output #0: loss = 0.115748 (* 1 = 0.115748 loss)
I0731 19:19:25.053427 16895 sgd_solver.cpp:136] Iteration 21200, lr = 0.0001, m = 0.9
I0731 19:19:43.518749 16895 solver.cpp:353] Iteration 21300 (5.4157 iter/s, 18.4648s/100 iter), loss = 0.109178
I0731 19:19:43.518856 16895 solver.cpp:375]     Train net output #0: loss = 0.109178 (* 1 = 0.109178 loss)
I0731 19:19:43.518864 16895 sgd_solver.cpp:136] Iteration 21300, lr = 0.0001, m = 0.9
I0731 19:19:46.760347 16874 data_reader.cpp:264] Starting prefetch of epoch 13
I0731 19:20:01.990077 16895 solver.cpp:353] Iteration 21400 (5.41395 iter/s, 18.4708s/100 iter), loss = 0.0761149
I0731 19:20:01.990098 16895 solver.cpp:375]     Train net output #0: loss = 0.0761147 (* 1 = 0.0761147 loss)
I0731 19:20:01.990103 16895 sgd_solver.cpp:136] Iteration 21400, lr = 0.0001, m = 0.9
I0731 19:20:20.671473 16895 solver.cpp:353] Iteration 21500 (5.35307 iter/s, 18.6809s/100 iter), loss = 0.102652
I0731 19:20:20.671582 16895 solver.cpp:375]     Train net output #0: loss = 0.102652 (* 1 = 0.102652 loss)
I0731 19:20:20.671589 16895 sgd_solver.cpp:136] Iteration 21500, lr = 0.0001, m = 0.9
I0731 19:20:39.238338 16895 solver.cpp:353] Iteration 21600 (5.38609 iter/s, 18.5663s/100 iter), loss = 0.117655
I0731 19:20:39.238363 16895 solver.cpp:375]     Train net output #0: loss = 0.117655 (* 1 = 0.117655 loss)
I0731 19:20:39.238366 16895 sgd_solver.cpp:136] Iteration 21600, lr = 0.0001, m = 0.9
I0731 19:20:48.227697 16901 data_reader.cpp:264] Starting prefetch of epoch 10
I0731 19:20:57.911337 16895 solver.cpp:353] Iteration 21700 (5.35547 iter/s, 18.6725s/100 iter), loss = 0.0830412
I0731 19:20:57.911442 16895 solver.cpp:375]     Train net output #0: loss = 0.083041 (* 1 = 0.083041 loss)
I0731 19:20:57.911448 16895 sgd_solver.cpp:136] Iteration 21700, lr = 0.0001, m = 0.9
I0731 19:21:16.528400 16895 solver.cpp:353] Iteration 21800 (5.37157 iter/s, 18.6165s/100 iter), loss = 0.0948835
I0731 19:21:16.528424 16895 solver.cpp:375]     Train net output #0: loss = 0.0948832 (* 1 = 0.0948832 loss)
I0731 19:21:16.528429 16895 sgd_solver.cpp:136] Iteration 21800, lr = 0.0001, m = 0.9
I0731 19:21:35.201375 16895 solver.cpp:353] Iteration 21900 (5.35548 iter/s, 18.6725s/100 iter), loss = 0.0923789
I0731 19:21:35.201448 16895 solver.cpp:375]     Train net output #0: loss = 0.0923787 (* 1 = 0.0923787 loss)
I0731 19:21:35.201454 16895 sgd_solver.cpp:136] Iteration 21900, lr = 0.0001, m = 0.9
I0731 19:21:49.576385 16876 data_reader.cpp:264] Starting prefetch of epoch 15
I0731 19:21:53.458272 16895 solver.cpp:550] Iteration 22000, Testing net (#0)
I0731 19:22:00.664882 16938 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 19:22:04.663736 16895 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.949959
I0731 19:22:04.663753 16895 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999922
I0731 19:22:04.663759 16895 solver.cpp:635]     Test net output #2: loss = 0.163894 (* 1 = 0.163894 loss)
I0731 19:22:04.663789 16895 solver.cpp:305] [MultiGPU] Tests completed in 11.2052s
I0731 19:22:04.859923 16895 solver.cpp:353] Iteration 22000 (3.3718 iter/s, 29.6577s/100 iter), loss = 0.121644
I0731 19:22:04.859952 16895 solver.cpp:375]     Train net output #0: loss = 0.121643 (* 1 = 0.121643 loss)
I0731 19:22:04.859956 16895 sgd_solver.cpp:136] Iteration 22000, lr = 0.0001, m = 0.9
I0731 19:22:23.461740 16895 solver.cpp:353] Iteration 22100 (5.37597 iter/s, 18.6013s/100 iter), loss = 0.141431
I0731 19:22:23.461853 16895 solver.cpp:375]     Train net output #0: loss = 0.141431 (* 1 = 0.141431 loss)
I0731 19:22:23.461860 16895 sgd_solver.cpp:136] Iteration 22100, lr = 0.0001, m = 0.9
I0731 19:22:41.928019 16895 solver.cpp:353] Iteration 22200 (5.41543 iter/s, 18.4658s/100 iter), loss = 0.122802
I0731 19:22:41.928043 16895 solver.cpp:375]     Train net output #0: loss = 0.122801 (* 1 = 0.122801 loss)
I0731 19:22:41.928047 16895 sgd_solver.cpp:136] Iteration 22200, lr = 0.0001, m = 0.9
I0731 19:23:00.421383 16895 solver.cpp:353] Iteration 22300 (5.4075 iter/s, 18.4928s/100 iter), loss = 0.0625515
I0731 19:23:00.421438 16895 solver.cpp:375]     Train net output #0: loss = 0.0625512 (* 1 = 0.0625512 loss)
I0731 19:23:00.421444 16895 sgd_solver.cpp:136] Iteration 22300, lr = 0.0001, m = 0.9
I0731 19:23:02.133085 16899 data_reader.cpp:264] Starting prefetch of epoch 17
I0731 19:23:18.916250 16895 solver.cpp:353] Iteration 22400 (5.40706 iter/s, 18.4944s/100 iter), loss = 0.097383
I0731 19:23:18.916275 16895 solver.cpp:375]     Train net output #0: loss = 0.0973828 (* 1 = 0.0973828 loss)
I0731 19:23:18.916280 16895 sgd_solver.cpp:136] Iteration 22400, lr = 0.0001, m = 0.9
I0731 19:23:37.433881 16895 solver.cpp:353] Iteration 22500 (5.40041 iter/s, 18.5171s/100 iter), loss = 0.126856
I0731 19:23:37.433959 16895 solver.cpp:375]     Train net output #0: loss = 0.126856 (* 1 = 0.126856 loss)
I0731 19:23:37.433964 16895 sgd_solver.cpp:136] Iteration 22500, lr = 0.0001, m = 0.9
I0731 19:23:56.018836 16895 solver.cpp:353] Iteration 22600 (5.38085 iter/s, 18.5844s/100 iter), loss = 0.0870659
I0731 19:23:56.018862 16895 solver.cpp:375]     Train net output #0: loss = 0.0870657 (* 1 = 0.0870657 loss)
I0731 19:23:56.018865 16895 sgd_solver.cpp:136] Iteration 22600, lr = 0.0001, m = 0.9
I0731 19:24:03.382838 16901 data_reader.cpp:264] Starting prefetch of epoch 11
I0731 19:24:14.542289 16895 solver.cpp:353] Iteration 22700 (5.39871 iter/s, 18.5229s/100 iter), loss = 0.0922053
I0731 19:24:14.542414 16895 solver.cpp:375]     Train net output #0: loss = 0.0922051 (* 1 = 0.0922051 loss)
I0731 19:24:14.542421 16895 sgd_solver.cpp:136] Iteration 22700, lr = 0.0001, m = 0.9
I0731 19:24:33.061033 16895 solver.cpp:353] Iteration 22800 (5.40008 iter/s, 18.5182s/100 iter), loss = 0.0500346
I0731 19:24:33.061055 16895 solver.cpp:375]     Train net output #0: loss = 0.0500344 (* 1 = 0.0500344 loss)
I0731 19:24:33.061059 16895 sgd_solver.cpp:136] Iteration 22800, lr = 0.0001, m = 0.9
I0731 19:24:51.683943 16895 solver.cpp:353] Iteration 22900 (5.36988 iter/s, 18.6224s/100 iter), loss = 0.10342
I0731 19:24:51.684026 16895 solver.cpp:375]     Train net output #0: loss = 0.10342 (* 1 = 0.10342 loss)
I0731 19:24:51.684031 16895 sgd_solver.cpp:136] Iteration 22900, lr = 0.0001, m = 0.9
I0731 19:25:04.737663 16901 data_reader.cpp:264] Starting prefetch of epoch 12
I0731 19:25:10.378610 16895 solver.cpp:353] Iteration 23000 (5.34927 iter/s, 18.6941s/100 iter), loss = 0.109147
I0731 19:25:10.378633 16895 solver.cpp:375]     Train net output #0: loss = 0.109146 (* 1 = 0.109146 loss)
I0731 19:25:10.378638 16895 sgd_solver.cpp:136] Iteration 23000, lr = 0.0001, m = 0.9
I0731 19:25:28.958138 16895 solver.cpp:353] Iteration 23100 (5.38242 iter/s, 18.579s/100 iter), loss = 0.0843774
I0731 19:25:28.958191 16895 solver.cpp:375]     Train net output #0: loss = 0.0843772 (* 1 = 0.0843772 loss)
I0731 19:25:28.958197 16895 sgd_solver.cpp:136] Iteration 23100, lr = 0.0001, m = 0.9
I0731 19:25:35.529309 16874 data_reader.cpp:264] Starting prefetch of epoch 14
I0731 19:25:47.502320 16895 solver.cpp:353] Iteration 23200 (5.39268 iter/s, 18.5437s/100 iter), loss = 0.0863819
I0731 19:25:47.502346 16895 solver.cpp:375]     Train net output #0: loss = 0.0863818 (* 1 = 0.0863818 loss)
I0731 19:25:47.502353 16895 sgd_solver.cpp:136] Iteration 23200, lr = 0.0001, m = 0.9
I0731 19:26:06.026197 16895 solver.cpp:353] Iteration 23300 (5.39859 iter/s, 18.5234s/100 iter), loss = 0.0535975
I0731 19:26:06.026284 16895 solver.cpp:375]     Train net output #0: loss = 0.0535973 (* 1 = 0.0535973 loss)
I0731 19:26:06.026293 16895 sgd_solver.cpp:136] Iteration 23300, lr = 0.0001, m = 0.9
I0731 19:26:24.660686 16895 solver.cpp:353] Iteration 23400 (5.36654 iter/s, 18.634s/100 iter), loss = 0.0841558
I0731 19:26:24.660711 16895 solver.cpp:375]     Train net output #0: loss = 0.0841557 (* 1 = 0.0841557 loss)
I0731 19:26:24.660714 16895 sgd_solver.cpp:136] Iteration 23400, lr = 0.0001, m = 0.9
I0731 19:26:36.949760 16903 data_reader.cpp:264] Starting prefetch of epoch 14
I0731 19:26:43.317206 16895 solver.cpp:353] Iteration 23500 (5.36021 iter/s, 18.656s/100 iter), loss = 0.282119
I0731 19:26:43.317235 16895 solver.cpp:375]     Train net output #0: loss = 0.282119 (* 1 = 0.282119 loss)
I0731 19:26:43.317242 16895 sgd_solver.cpp:136] Iteration 23500, lr = 0.0001, m = 0.9
I0731 19:27:01.847048 16895 solver.cpp:353] Iteration 23600 (5.39685 iter/s, 18.5293s/100 iter), loss = 0.0693958
I0731 19:27:01.847074 16895 solver.cpp:375]     Train net output #0: loss = 0.0693957 (* 1 = 0.0693957 loss)
I0731 19:27:01.847079 16895 sgd_solver.cpp:136] Iteration 23600, lr = 0.0001, m = 0.9
I0731 19:27:20.367046 16895 solver.cpp:353] Iteration 23700 (5.39972 iter/s, 18.5195s/100 iter), loss = 0.0777276
I0731 19:27:20.367147 16895 solver.cpp:375]     Train net output #0: loss = 0.0777275 (* 1 = 0.0777275 loss)
I0731 19:27:20.367156 16895 sgd_solver.cpp:136] Iteration 23700, lr = 0.0001, m = 0.9
I0731 19:27:38.187031 16903 data_reader.cpp:264] Starting prefetch of epoch 15
I0731 19:27:38.922068 16895 solver.cpp:353] Iteration 23800 (5.38953 iter/s, 18.5545s/100 iter), loss = 0.158977
I0731 19:27:38.922094 16895 solver.cpp:375]     Train net output #0: loss = 0.158976 (* 1 = 0.158976 loss)
I0731 19:27:38.922099 16895 sgd_solver.cpp:136] Iteration 23800, lr = 0.0001, m = 0.9
I0731 19:27:57.457180 16895 solver.cpp:353] Iteration 23900 (5.39531 iter/s, 18.5346s/100 iter), loss = 0.0708941
I0731 19:27:57.457278 16895 solver.cpp:375]     Train net output #0: loss = 0.070894 (* 1 = 0.070894 loss)
I0731 19:27:57.457285 16895 sgd_solver.cpp:136] Iteration 23900, lr = 0.0001, m = 0.9
I0731 19:28:08.908278 16874 data_reader.cpp:264] Starting prefetch of epoch 15
I0731 19:28:15.776485 16895 solver.cpp:550] Iteration 24000, Testing net (#0)
I0731 19:28:26.588626 16940 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 19:28:26.940583 16895 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.947568
I0731 19:28:26.940608 16895 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999995
I0731 19:28:26.940613 16895 solver.cpp:635]     Test net output #2: loss = 0.162854 (* 1 = 0.162854 loss)
I0731 19:28:26.940685 16895 solver.cpp:305] [MultiGPU] Tests completed in 11.1639s
I0731 19:28:27.032938 16958 sgd_solver.cpp:48] MultiStep Status: Iteration 24000, step = 1
I0731 19:28:27.032938 16960 sgd_solver.cpp:48] MultiStep Status: Iteration 24000, step = 1
I0731 19:28:27.032938 16959 sgd_solver.cpp:48] MultiStep Status: Iteration 24000, step = 1
I0731 19:28:27.133074 16895 solver.cpp:353] Iteration 24000 (3.36983 iter/s, 29.6751s/100 iter), loss = 0.0909775
I0731 19:28:27.133102 16895 solver.cpp:375]     Train net output #0: loss = 0.0909775 (* 1 = 0.0909775 loss)
I0731 19:28:27.133110 16895 sgd_solver.cpp:136] Iteration 24000, lr = 1e-05, m = 0.9
I0731 19:28:45.595423 16895 solver.cpp:353] Iteration 24100 (5.41658 iter/s, 18.4618s/100 iter), loss = 0.113347
I0731 19:28:45.595475 16895 solver.cpp:375]     Train net output #0: loss = 0.113346 (* 1 = 0.113346 loss)
I0731 19:28:45.595481 16895 sgd_solver.cpp:136] Iteration 24100, lr = 1e-05, m = 0.9
I0731 19:29:04.142102 16895 solver.cpp:353] Iteration 24200 (5.39195 iter/s, 18.5462s/100 iter), loss = 0.20701
I0731 19:29:04.142124 16895 solver.cpp:375]     Train net output #0: loss = 0.20701 (* 1 = 0.20701 loss)
I0731 19:29:04.142129 16895 sgd_solver.cpp:136] Iteration 24200, lr = 1e-05, m = 0.9
I0731 19:29:21.317585 16901 data_reader.cpp:264] Starting prefetch of epoch 13
I0731 19:29:22.816807 16895 solver.cpp:353] Iteration 24300 (5.35499 iter/s, 18.6742s/100 iter), loss = 0.0644075
I0731 19:29:22.816836 16895 solver.cpp:375]     Train net output #0: loss = 0.0644074 (* 1 = 0.0644074 loss)
I0731 19:29:22.816840 16895 sgd_solver.cpp:136] Iteration 24300, lr = 1e-05, m = 0.9
I0731 19:29:41.463171 16895 solver.cpp:353] Iteration 24400 (5.36312 iter/s, 18.6458s/100 iter), loss = 0.084307
I0731 19:29:41.463196 16895 solver.cpp:375]     Train net output #0: loss = 0.0843069 (* 1 = 0.0843069 loss)
I0731 19:29:41.463199 16895 sgd_solver.cpp:136] Iteration 24400, lr = 1e-05, m = 0.9
I0731 19:29:52.250021 16899 data_reader.cpp:264] Starting prefetch of epoch 18
I0731 19:29:59.952898 16895 solver.cpp:353] Iteration 24500 (5.40856 iter/s, 18.4892s/100 iter), loss = 0.0630685
I0731 19:29:59.952929 16895 solver.cpp:375]     Train net output #0: loss = 0.0630684 (* 1 = 0.0630684 loss)
I0731 19:29:59.952935 16895 sgd_solver.cpp:136] Iteration 24500, lr = 1e-05, m = 0.9
I0731 19:30:18.477191 16895 solver.cpp:353] Iteration 24600 (5.39847 iter/s, 18.5238s/100 iter), loss = 0.105378
I0731 19:30:18.477214 16895 solver.cpp:375]     Train net output #0: loss = 0.105378 (* 1 = 0.105378 loss)
I0731 19:30:18.477219 16895 sgd_solver.cpp:136] Iteration 24600, lr = 1e-05, m = 0.9
I0731 19:30:37.189606 16895 solver.cpp:353] Iteration 24700 (5.34419 iter/s, 18.7119s/100 iter), loss = 0.0685053
I0731 19:30:37.189658 16895 solver.cpp:375]     Train net output #0: loss = 0.0685051 (* 1 = 0.0685051 loss)
I0731 19:30:37.189663 16895 sgd_solver.cpp:136] Iteration 24700, lr = 1e-05, m = 0.9
I0731 19:30:53.540956 16903 data_reader.cpp:264] Starting prefetch of epoch 16
I0731 19:30:55.737457 16895 solver.cpp:353] Iteration 24800 (5.39161 iter/s, 18.5473s/100 iter), loss = 0.114455
I0731 19:30:55.737478 16895 solver.cpp:375]     Train net output #0: loss = 0.114455 (* 1 = 0.114455 loss)
I0731 19:30:55.737483 16895 sgd_solver.cpp:136] Iteration 24800, lr = 1e-05, m = 0.9
I0731 19:31:14.241164 16895 solver.cpp:353] Iteration 24900 (5.40447 iter/s, 18.5032s/100 iter), loss = 0.0686629
I0731 19:31:14.241261 16895 solver.cpp:375]     Train net output #0: loss = 0.0686627 (* 1 = 0.0686627 loss)
I0731 19:31:14.241266 16895 sgd_solver.cpp:136] Iteration 24900, lr = 1e-05, m = 0.9
I0731 19:31:32.854246 16895 solver.cpp:353] Iteration 25000 (5.37271 iter/s, 18.6126s/100 iter), loss = 0.0883391
I0731 19:31:32.854274 16895 solver.cpp:375]     Train net output #0: loss = 0.088339 (* 1 = 0.088339 loss)
I0731 19:31:32.854281 16895 sgd_solver.cpp:136] Iteration 25000, lr = 1e-05, m = 0.9
I0731 19:31:51.441371 16895 solver.cpp:353] Iteration 25100 (5.38022 iter/s, 18.5866s/100 iter), loss = 0.0869434
I0731 19:31:51.441440 16895 solver.cpp:375]     Train net output #0: loss = 0.0869432 (* 1 = 0.0869432 loss)
I0731 19:31:51.441447 16895 sgd_solver.cpp:136] Iteration 25100, lr = 1e-05, m = 0.9
I0731 19:31:54.953179 16874 data_reader.cpp:264] Starting prefetch of epoch 16
I0731 19:32:10.040729 16895 solver.cpp:353] Iteration 25200 (5.37668 iter/s, 18.5988s/100 iter), loss = 0.0992967
I0731 19:32:10.040755 16895 solver.cpp:375]     Train net output #0: loss = 0.0992966 (* 1 = 0.0992966 loss)
I0731 19:32:10.040758 16895 sgd_solver.cpp:136] Iteration 25200, lr = 1e-05, m = 0.9
I0731 19:32:28.667088 16895 solver.cpp:353] Iteration 25300 (5.36888 iter/s, 18.6258s/100 iter), loss = 0.0911182
I0731 19:32:28.667137 16895 solver.cpp:375]     Train net output #0: loss = 0.0911181 (* 1 = 0.0911181 loss)
I0731 19:32:28.667142 16895 sgd_solver.cpp:136] Iteration 25300, lr = 1e-05, m = 0.9
I0731 19:32:47.484758 16895 solver.cpp:353] Iteration 25400 (5.3143 iter/s, 18.8172s/100 iter), loss = 0.105268
I0731 19:32:47.484782 16895 solver.cpp:375]     Train net output #0: loss = 0.105268 (* 1 = 0.105268 loss)
I0731 19:32:47.484786 16895 sgd_solver.cpp:136] Iteration 25400, lr = 1e-05, m = 0.9
I0731 19:32:56.613703 16876 data_reader.cpp:264] Starting prefetch of epoch 16
I0731 19:33:06.068445 16895 solver.cpp:353] Iteration 25500 (5.38121 iter/s, 18.5832s/100 iter), loss = 0.0595335
I0731 19:33:06.068500 16895 solver.cpp:375]     Train net output #0: loss = 0.0595334 (* 1 = 0.0595334 loss)
I0731 19:33:06.068506 16895 sgd_solver.cpp:136] Iteration 25500, lr = 1e-05, m = 0.9
I0731 19:33:24.685256 16895 solver.cpp:353] Iteration 25600 (5.37164 iter/s, 18.6163s/100 iter), loss = 0.0798673
I0731 19:33:24.685286 16895 solver.cpp:375]     Train net output #0: loss = 0.0798671 (* 1 = 0.0798671 loss)
I0731 19:33:24.685292 16895 sgd_solver.cpp:136] Iteration 25600, lr = 1e-05, m = 0.9
I0731 19:33:27.457157 16899 data_reader.cpp:264] Starting prefetch of epoch 19
I0731 19:33:43.145362 16895 solver.cpp:353] Iteration 25700 (5.41724 iter/s, 18.4596s/100 iter), loss = 0.0510433
I0731 19:33:43.145416 16895 solver.cpp:375]     Train net output #0: loss = 0.0510431 (* 1 = 0.0510431 loss)
I0731 19:33:43.145421 16895 sgd_solver.cpp:136] Iteration 25700, lr = 1e-05, m = 0.9
I0731 19:34:01.641012 16895 solver.cpp:353] Iteration 25800 (5.40683 iter/s, 18.4951s/100 iter), loss = 0.0773511
I0731 19:34:01.641037 16895 solver.cpp:375]     Train net output #0: loss = 0.077351 (* 1 = 0.077351 loss)
I0731 19:34:01.641041 16895 sgd_solver.cpp:136] Iteration 25800, lr = 1e-05, m = 0.9
I0731 19:34:20.276801 16895 solver.cpp:353] Iteration 25900 (5.36617 iter/s, 18.6353s/100 iter), loss = 0.0652455
I0731 19:34:20.276882 16895 solver.cpp:375]     Train net output #0: loss = 0.0652454 (* 1 = 0.0652454 loss)
I0731 19:34:20.276890 16895 sgd_solver.cpp:136] Iteration 25900, lr = 1e-05, m = 0.9
I0731 19:34:28.694089 16874 data_reader.cpp:264] Starting prefetch of epoch 17
I0731 19:34:38.785473 16895 solver.cpp:550] Iteration 26000, Testing net (#0)
I0731 19:34:49.753564 16895 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.950595
I0731 19:34:49.753585 16895 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999689
I0731 19:34:49.753590 16895 solver.cpp:635]     Test net output #2: loss = 0.169406 (* 1 = 0.169406 loss)
I0731 19:34:49.753664 16895 solver.cpp:305] [MultiGPU] Tests completed in 10.9679s
I0731 19:34:49.948613 16895 solver.cpp:353] Iteration 26000 (3.3703 iter/s, 29.671s/100 iter), loss = 0.0948464
I0731 19:34:49.948639 16895 solver.cpp:375]     Train net output #0: loss = 0.0948463 (* 1 = 0.0948463 loss)
I0731 19:34:49.948645 16895 sgd_solver.cpp:136] Iteration 26000, lr = 1e-05, m = 0.9
I0731 19:35:08.526060 16895 solver.cpp:353] Iteration 26100 (5.38302 iter/s, 18.5769s/100 iter), loss = 0.0518993
I0731 19:35:08.526134 16895 solver.cpp:375]     Train net output #0: loss = 0.0518991 (* 1 = 0.0518991 loss)
I0731 19:35:08.526141 16895 sgd_solver.cpp:136] Iteration 26100, lr = 1e-05, m = 0.9
I0731 19:35:10.410540 16901 data_reader.cpp:264] Starting prefetch of epoch 14
I0731 19:35:27.081132 16895 solver.cpp:353] Iteration 26200 (5.38951 iter/s, 18.5546s/100 iter), loss = 0.112135
I0731 19:35:27.081154 16895 solver.cpp:375]     Train net output #0: loss = 0.112135 (* 1 = 0.112135 loss)
I0731 19:35:27.081158 16895 sgd_solver.cpp:136] Iteration 26200, lr = 1e-05, m = 0.9
I0731 19:35:45.583425 16895 solver.cpp:353] Iteration 26300 (5.40489 iter/s, 18.5018s/100 iter), loss = 0.0901977
I0731 19:35:45.583556 16895 solver.cpp:375]     Train net output #0: loss = 0.0901976 (* 1 = 0.0901976 loss)
I0731 19:35:45.583564 16895 sgd_solver.cpp:136] Iteration 26300, lr = 1e-05, m = 0.9
I0731 19:36:04.103682 16895 solver.cpp:353] Iteration 26400 (5.39964 iter/s, 18.5197s/100 iter), loss = 0.134318
I0731 19:36:04.103703 16895 solver.cpp:375]     Train net output #0: loss = 0.134318 (* 1 = 0.134318 loss)
I0731 19:36:04.103708 16895 sgd_solver.cpp:136] Iteration 26400, lr = 1e-05, m = 0.9
I0731 19:36:11.675438 16903 data_reader.cpp:264] Starting prefetch of epoch 17
I0731 19:36:22.605406 16895 solver.cpp:353] Iteration 26500 (5.40505 iter/s, 18.5012s/100 iter), loss = 0.0546557
I0731 19:36:22.605458 16895 solver.cpp:375]     Train net output #0: loss = 0.0546555 (* 1 = 0.0546555 loss)
I0731 19:36:22.605463 16895 sgd_solver.cpp:136] Iteration 26500, lr = 1e-05, m = 0.9
I0731 19:36:41.139250 16895 solver.cpp:353] Iteration 26600 (5.39568 iter/s, 18.5333s/100 iter), loss = 0.0870072
I0731 19:36:41.139272 16895 solver.cpp:375]     Train net output #0: loss = 0.087007 (* 1 = 0.087007 loss)
I0731 19:36:41.139278 16895 sgd_solver.cpp:136] Iteration 26600, lr = 1e-05, m = 0.9
I0731 19:36:42.279579 16899 data_reader.cpp:264] Starting prefetch of epoch 20
I0731 19:36:59.708160 16895 solver.cpp:353] Iteration 26700 (5.3855 iter/s, 18.5684s/100 iter), loss = 0.0764507
I0731 19:36:59.708269 16895 solver.cpp:375]     Train net output #0: loss = 0.0764506 (* 1 = 0.0764506 loss)
I0731 19:36:59.708276 16895 sgd_solver.cpp:136] Iteration 26700, lr = 1e-05, m = 0.9
I0731 19:37:18.320086 16895 solver.cpp:353] Iteration 26800 (5.37305 iter/s, 18.6114s/100 iter), loss = 0.0949733
I0731 19:37:18.320116 16895 solver.cpp:375]     Train net output #0: loss = 0.0949731 (* 1 = 0.0949731 loss)
I0731 19:37:18.320123 16895 sgd_solver.cpp:136] Iteration 26800, lr = 1e-05, m = 0.9
I0731 19:37:37.005497 16895 solver.cpp:353] Iteration 26900 (5.35192 iter/s, 18.6849s/100 iter), loss = 0.0591203
I0731 19:37:37.005549 16895 solver.cpp:375]     Train net output #0: loss = 0.0591202 (* 1 = 0.0591202 loss)
I0731 19:37:37.005554 16895 sgd_solver.cpp:136] Iteration 26900, lr = 1e-05, m = 0.9
I0731 19:37:43.954077 16901 data_reader.cpp:264] Starting prefetch of epoch 15
I0731 19:37:55.597563 16895 solver.cpp:353] Iteration 27000 (5.37879 iter/s, 18.5915s/100 iter), loss = 0.190562
I0731 19:37:55.597589 16895 solver.cpp:375]     Train net output #0: loss = 0.190562 (* 1 = 0.190562 loss)
I0731 19:37:55.597595 16895 sgd_solver.cpp:136] Iteration 27000, lr = 1e-05, m = 0.9
I0731 19:38:14.193681 16895 solver.cpp:353] Iteration 27100 (5.37761 iter/s, 18.5956s/100 iter), loss = 0.0775418
I0731 19:38:14.193768 16895 solver.cpp:375]     Train net output #0: loss = 0.0775416 (* 1 = 0.0775416 loss)
I0731 19:38:14.193775 16895 sgd_solver.cpp:136] Iteration 27100, lr = 1e-05, m = 0.9
I0731 19:38:32.673146 16895 solver.cpp:353] Iteration 27200 (5.41156 iter/s, 18.4789s/100 iter), loss = 0.0479062
I0731 19:38:32.673168 16895 solver.cpp:375]     Train net output #0: loss = 0.047906 (* 1 = 0.047906 loss)
I0731 19:38:32.673172 16895 sgd_solver.cpp:136] Iteration 27200, lr = 1e-05, m = 0.9
I0731 19:38:45.206573 16901 data_reader.cpp:264] Starting prefetch of epoch 16
I0731 19:38:51.344334 16895 solver.cpp:353] Iteration 27300 (5.35599 iter/s, 18.6707s/100 iter), loss = 0.0772896
I0731 19:38:51.344357 16895 solver.cpp:375]     Train net output #0: loss = 0.0772894 (* 1 = 0.0772894 loss)
I0731 19:38:51.344360 16895 sgd_solver.cpp:136] Iteration 27300, lr = 1e-05, m = 0.9
I0731 19:39:09.809181 16895 solver.cpp:353] Iteration 27400 (5.41585 iter/s, 18.4643s/100 iter), loss = 0.143495
I0731 19:39:09.809206 16895 solver.cpp:375]     Train net output #0: loss = 0.143495 (* 1 = 0.143495 loss)
I0731 19:39:09.809211 16895 sgd_solver.cpp:136] Iteration 27400, lr = 1e-05, m = 0.9
I0731 19:39:15.958884 16898 data_reader.cpp:264] Starting prefetch of epoch 15
I0731 19:39:28.399848 16895 solver.cpp:353] Iteration 27500 (5.37919 iter/s, 18.5902s/100 iter), loss = 0.0687978
I0731 19:39:28.399871 16895 solver.cpp:375]     Train net output #0: loss = 0.0687976 (* 1 = 0.0687976 loss)
I0731 19:39:28.399876 16895 sgd_solver.cpp:136] Iteration 27500, lr = 1e-05, m = 0.9
I0731 19:39:47.042378 16895 solver.cpp:353] Iteration 27600 (5.36423 iter/s, 18.642s/100 iter), loss = 0.0962823
I0731 19:39:47.042434 16895 solver.cpp:375]     Train net output #0: loss = 0.0962821 (* 1 = 0.0962821 loss)
I0731 19:39:47.042441 16895 sgd_solver.cpp:136] Iteration 27600, lr = 1e-05, m = 0.9
I0731 19:40:05.533861 16895 solver.cpp:353] Iteration 27700 (5.40804 iter/s, 18.491s/100 iter), loss = 0.0939745
I0731 19:40:05.533885 16895 solver.cpp:375]     Train net output #0: loss = 0.0939743 (* 1 = 0.0939743 loss)
I0731 19:40:05.533888 16895 sgd_solver.cpp:136] Iteration 27700, lr = 1e-05, m = 0.9
I0731 19:40:17.175045 16903 data_reader.cpp:264] Starting prefetch of epoch 18
I0731 19:40:23.991966 16895 solver.cpp:353] Iteration 27800 (5.41782 iter/s, 18.4576s/100 iter), loss = 0.0479142
I0731 19:40:23.991989 16895 solver.cpp:375]     Train net output #0: loss = 0.0479141 (* 1 = 0.0479141 loss)
I0731 19:40:23.991993 16895 sgd_solver.cpp:136] Iteration 27800, lr = 1e-05, m = 0.9
I0731 19:40:42.596488 16895 solver.cpp:353] Iteration 27900 (5.37519 iter/s, 18.604s/100 iter), loss = 0.0700293
I0731 19:40:42.596520 16895 solver.cpp:375]     Train net output #0: loss = 0.0700291 (* 1 = 0.0700291 loss)
I0731 19:40:42.596527 16895 sgd_solver.cpp:136] Iteration 27900, lr = 1e-05, m = 0.9
I0731 19:41:01.146373 16895 solver.cpp:550] Iteration 28000, Testing net (#0)
I0731 19:41:04.412425 16893 data_reader.cpp:264] Starting prefetch of epoch 3
I0731 19:41:12.301131 16936 data_reader.cpp:264] Starting prefetch of epoch 1
I0731 19:41:12.639309 16895 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.94887
I0731 19:41:12.639333 16895 solver.cpp:635]     Test net output #1: accuracy/top5 = 1
I0731 19:41:12.639338 16895 solver.cpp:635]     Test net output #2: loss = 0.158042 (* 1 = 0.158042 loss)
I0731 19:41:12.639415 16895 solver.cpp:305] [MultiGPU] Tests completed in 11.4927s
I0731 19:41:12.852612 16895 solver.cpp:353] Iteration 28000 (3.30521 iter/s, 30.2553s/100 iter), loss = 0.103962
I0731 19:41:12.852639 16895 solver.cpp:375]     Train net output #0: loss = 0.103962 (* 1 = 0.103962 loss)
I0731 19:41:12.852643 16895 sgd_solver.cpp:136] Iteration 28000, lr = 1e-05, m = 0.9
I0731 19:41:31.387063 16895 solver.cpp:353] Iteration 28100 (5.39551 iter/s, 18.5339s/100 iter), loss = 0.0813759
I0731 19:41:31.387131 16895 solver.cpp:375]     Train net output #0: loss = 0.0813757 (* 1 = 0.0813757 loss)
I0731 19:41:31.387138 16895 sgd_solver.cpp:136] Iteration 28100, lr = 1e-05, m = 0.9
I0731 19:41:50.028465 16895 solver.cpp:353] Iteration 28200 (5.36455 iter/s, 18.6409s/100 iter), loss = 0.0835746
I0731 19:41:50.028487 16895 solver.cpp:375]     Train net output #0: loss = 0.0835745 (* 1 = 0.0835745 loss)
I0731 19:41:50.028491 16895 sgd_solver.cpp:136] Iteration 28200, lr = 1e-05, m = 0.9
I0731 19:42:00.970288 16903 data_reader.cpp:264] Starting prefetch of epoch 19
I0731 19:42:08.583876 16895 solver.cpp:353] Iteration 28300 (5.38941 iter/s, 18.5549s/100 iter), loss = 0.055764
I0731 19:42:08.583927 16895 solver.cpp:375]     Train net output #0: loss = 0.0557639 (* 1 = 0.0557639 loss)
I0731 19:42:08.583932 16895 sgd_solver.cpp:136] Iteration 28300, lr = 1e-05, m = 0.9
I0731 19:42:27.114148 16895 solver.cpp:353] Iteration 28400 (5.39672 iter/s, 18.5298s/100 iter), loss = 0.0938832
I0731 19:42:27.114172 16895 solver.cpp:375]     Train net output #0: loss = 0.093883 (* 1 = 0.093883 loss)
I0731 19:42:27.114177 16895 sgd_solver.cpp:136] Iteration 28400, lr = 1e-05, m = 0.9
I0731 19:42:45.688261 16895 solver.cpp:353] Iteration 28500 (5.38399 iter/s, 18.5736s/100 iter), loss = 0.0914015
I0731 19:42:45.688315 16895 solver.cpp:375]     Train net output #0: loss = 0.0914013 (* 1 = 0.0914013 loss)
I0731 19:42:45.688321 16895 sgd_solver.cpp:136] Iteration 28500, lr = 1e-05, m = 0.9
I0731 19:43:02.320116 16901 data_reader.cpp:264] Starting prefetch of epoch 17
I0731 19:43:04.161609 16895 solver.cpp:353] Iteration 28600 (5.41335 iter/s, 18.4728s/100 iter), loss = 0.133157
I0731 19:43:04.161633 16895 solver.cpp:375]     Train net output #0: loss = 0.133157 (* 1 = 0.133157 loss)
I0731 19:43:04.161639 16895 sgd_solver.cpp:136] Iteration 28600, lr = 1e-05, m = 0.9
I0731 19:43:22.646903 16895 solver.cpp:353] Iteration 28700 (5.40986 iter/s, 18.4848s/100 iter), loss = 0.0990574
I0731 19:43:22.646955 16895 solver.cpp:375]     Train net output #0: loss = 0.0990573 (* 1 = 0.0990573 loss)
I0731 19:43:22.646958 16895 sgd_solver.cpp:136] Iteration 28700, lr = 1e-05, m = 0.9
I0731 19:43:32.911159 16874 data_reader.cpp:264] Starting prefetch of epoch 18
I0731 19:43:41.213423 16895 solver.cpp:353] Iteration 28800 (5.38619 iter/s, 18.566s/100 iter), loss = 0.0723923
I0731 19:43:41.213449 16895 solver.cpp:375]     Train net output #0: loss = 0.0723922 (* 1 = 0.0723922 loss)
I0731 19:43:41.213452 16895 sgd_solver.cpp:136] Iteration 28800, lr = 1e-05, m = 0.9
I0731 19:43:59.824240 16895 solver.cpp:353] Iteration 28900 (5.37337 iter/s, 18.6103s/100 iter), loss = 0.157444
I0731 19:43:59.824295 16895 solver.cpp:375]     Train net output #0: loss = 0.157444 (* 1 = 0.157444 loss)
I0731 19:43:59.824301 16895 sgd_solver.cpp:136] Iteration 28900, lr = 1e-05, m = 0.9
I0731 19:44:18.581553 16895 solver.cpp:353] Iteration 29000 (5.3314 iter/s, 18.7568s/100 iter), loss = 0.0946085
I0731 19:44:18.581578 16895 solver.cpp:375]     Train net output #0: loss = 0.0946084 (* 1 = 0.0946084 loss)
I0731 19:44:18.581583 16895 sgd_solver.cpp:136] Iteration 29000, lr = 1e-05, m = 0.9
I0731 19:44:34.451192 16876 data_reader.cpp:264] Starting prefetch of epoch 17
I0731 19:44:37.203186 16895 solver.cpp:353] Iteration 29100 (5.37025 iter/s, 18.6211s/100 iter), loss = 0.0919976
I0731 19:44:37.203208 16895 solver.cpp:375]     Train net output #0: loss = 0.0919975 (* 1 = 0.0919975 loss)
I0731 19:44:37.203212 16895 sgd_solver.cpp:136] Iteration 29100, lr = 1e-05, m = 0.9
I0731 19:44:55.611901 16895 solver.cpp:353] Iteration 29200 (5.43236 iter/s, 18.4082s/100 iter), loss = 0.0620114
I0731 19:44:55.611923 16895 solver.cpp:375]     Train net output #0: loss = 0.0620113 (* 1 = 0.0620113 loss)
I0731 19:44:55.611927 16895 sgd_solver.cpp:136] Iteration 29200, lr = 1e-05, m = 0.9
I0731 19:45:14.333528 16895 solver.cpp:353] Iteration 29300 (5.34156 iter/s, 18.7211s/100 iter), loss = 0.0598422
I0731 19:45:14.333657 16895 solver.cpp:375]     Train net output #0: loss = 0.0598421 (* 1 = 0.0598421 loss)
I0731 19:45:14.333663 16895 sgd_solver.cpp:136] Iteration 29300, lr = 1e-05, m = 0.9
I0731 19:45:32.897390 16895 solver.cpp:353] Iteration 29400 (5.38696 iter/s, 18.5634s/100 iter), loss = 0.0585891
I0731 19:45:32.897415 16895 solver.cpp:375]     Train net output #0: loss = 0.058589 (* 1 = 0.058589 loss)
I0731 19:45:32.897419 16895 sgd_solver.cpp:136] Iteration 29400, lr = 1e-05, m = 0.9
I0731 19:45:35.868558 16903 data_reader.cpp:264] Starting prefetch of epoch 20
I0731 19:45:51.468192 16895 solver.cpp:353] Iteration 29500 (5.38494 iter/s, 18.5703s/100 iter), loss = 0.138378
I0731 19:45:51.468245 16895 solver.cpp:375]     Train net output #0: loss = 0.138377 (* 1 = 0.138377 loss)
I0731 19:45:51.468251 16895 sgd_solver.cpp:136] Iteration 29500, lr = 1e-05, m = 0.9
I0731 19:46:06.484594 16874 data_reader.cpp:264] Starting prefetch of epoch 19
I0731 19:46:09.973763 16895 solver.cpp:353] Iteration 29600 (5.40392 iter/s, 18.5051s/100 iter), loss = 0.0859054
I0731 19:46:09.973788 16895 solver.cpp:375]     Train net output #0: loss = 0.0859052 (* 1 = 0.0859052 loss)
I0731 19:46:09.973791 16895 sgd_solver.cpp:136] Iteration 29600, lr = 1e-05, m = 0.9
I0731 19:46:28.535109 16895 solver.cpp:353] Iteration 29700 (5.38769 iter/s, 18.5608s/100 iter), loss = 0.0705883
I0731 19:46:28.535166 16895 solver.cpp:375]     Train net output #0: loss = 0.0705882 (* 1 = 0.0705882 loss)
I0731 19:46:28.535171 16895 sgd_solver.cpp:136] Iteration 29700, lr = 1e-05, m = 0.9
I0731 19:46:47.202584 16895 solver.cpp:353] Iteration 29800 (5.35706 iter/s, 18.667s/100 iter), loss = 0.148605
I0731 19:46:47.202610 16895 solver.cpp:375]     Train net output #0: loss = 0.148605 (* 1 = 0.148605 loss)
I0731 19:46:47.202615 16895 sgd_solver.cpp:136] Iteration 29800, lr = 1e-05, m = 0.9
I0731 19:47:05.637884 16895 solver.cpp:353] Iteration 29900 (5.42452 iter/s, 18.4348s/100 iter), loss = 0.0793571
I0731 19:47:05.637990 16895 solver.cpp:375]     Train net output #0: loss = 0.079357 (* 1 = 0.079357 loss)
I0731 19:47:05.637996 16895 sgd_solver.cpp:136] Iteration 29900, lr = 1e-05, m = 0.9
I0731 19:47:07.900349 16903 data_reader.cpp:264] Starting prefetch of epoch 21
I0731 19:47:24.050166 16895 solver.cpp:680] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/initial/cityscapes5_jsegnet21v2_iter_30000.caffemodel
I0731 19:47:24.159864 16895 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/initial/cityscapes5_jsegnet21v2_iter_30000.solverstate
I0731 19:47:24.170681 16895 solver.cpp:550] Iteration 30000, Testing net (#0)
I0731 19:47:35.102744 16895 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.949544
I0731 19:47:35.102764 16895 solver.cpp:635]     Test net output #1: accuracy/top5 = 0.999469
I0731 19:47:35.102771 16895 solver.cpp:635]     Test net output #2: loss = 0.191064 (* 1 = 0.191064 loss)
I0731 19:47:35.102787 16895 solver.cpp:305] [MultiGPU] Tests completed in 10.9318s
I0731 19:47:35.300060 16895 solver.cpp:353] Iteration 30000 (3.37139 iter/s, 29.6614s/100 iter), loss = 0.146011
I0731 19:47:35.300084 16895 solver.cpp:375]     Train net output #0: loss = 0.146011 (* 1 = 0.146011 loss)
I0731 19:47:35.300088 16895 sgd_solver.cpp:136] Iteration 30000, lr = 1e-05, m = 0.9
I0731 19:47:49.560660 16901 data_reader.cpp:264] Starting prefetch of epoch 18
I0731 19:47:53.792579 16895 solver.cpp:353] Iteration 30100 (5.40774 iter/s, 18.492s/100 iter), loss = 0.0781085
I0731 19:47:53.792604 16895 solver.cpp:375]     Train net output #0: loss = 0.0781084 (* 1 = 0.0781084 loss)
I0731 19:47:53.792608 16895 sgd_solver.cpp:136] Iteration 30100, lr = 1e-05, m = 0.9
I0731 19:48:12.269429 16895 solver.cpp:353] Iteration 30200 (5.41233 iter/s, 18.4763s/100 iter), loss = 0.0717178
I0731 19:48:12.269454 16895 solver.cpp:375]     Train net output #0: loss = 0.0717177 (* 1 = 0.0717177 loss)
I0731 19:48:12.269459 16895 sgd_solver.cpp:136] Iteration 30200, lr = 1e-05, m = 0.9
I0731 19:48:20.105785 16899 data_reader.cpp:264] Starting prefetch of epoch 21
I0731 19:48:30.881665 16895 solver.cpp:353] Iteration 30300 (5.37296 iter/s, 18.6117s/100 iter), loss = 0.0937092
I0731 19:48:30.881690 16895 solver.cpp:375]     Train net output #0: loss = 0.0937091 (* 1 = 0.0937091 loss)
I0731 19:48:30.881695 16895 sgd_solver.cpp:136] Iteration 30300, lr = 1e-05, m = 0.9
I0731 19:48:49.435701 16895 solver.cpp:353] Iteration 30400 (5.38981 iter/s, 18.5535s/100 iter), loss = 0.0825513
I0731 19:48:49.435724 16895 solver.cpp:375]     Train net output #0: loss = 0.0825512 (* 1 = 0.0825512 loss)
I0731 19:48:49.435727 16895 sgd_solver.cpp:136] Iteration 30400, lr = 1e-05, m = 0.9
I0731 19:49:08.125344 16895 solver.cpp:353] Iteration 30500 (5.3507 iter/s, 18.6891s/100 iter), loss = 0.0909085
I0731 19:49:08.125432 16895 solver.cpp:375]     Train net output #0: loss = 0.0909084 (* 1 = 0.0909084 loss)
I0731 19:49:08.125439 16895 sgd_solver.cpp:136] Iteration 30500, lr = 1e-05, m = 0.9
I0731 19:49:21.570225 16876 data_reader.cpp:264] Starting prefetch of epoch 18
I0731 19:49:26.601089 16895 solver.cpp:353] Iteration 30600 (5.41265 iter/s, 18.4752s/100 iter), loss = 0.0649177
I0731 19:49:26.601111 16895 solver.cpp:375]     Train net output #0: loss = 0.0649176 (* 1 = 0.0649176 loss)
I0731 19:49:26.601115 16895 sgd_solver.cpp:136] Iteration 30600, lr = 1e-05, m = 0.9
I0731 19:49:45.104059 16895 solver.cpp:353] Iteration 30700 (5.40469 iter/s, 18.5025s/100 iter), loss = 0.175896
I0731 19:49:45.104109 16895 solver.cpp:375]     Train net output #0: loss = 0.175896 (* 1 = 0.175896 loss)
I0731 19:49:45.104115 16895 sgd_solver.cpp:136] Iteration 30700, lr = 1e-05, m = 0.9
I0731 19:50:03.672487 16895 solver.cpp:353] Iteration 30800 (5.38563 iter/s, 18.5679s/100 iter), loss = 0.087321
I0731 19:50:03.672511 16895 solver.cpp:375]     Train net output #0: loss = 0.0873209 (* 1 = 0.0873209 loss)
I0731 19:50:03.672515 16895 sgd_solver.cpp:136] Iteration 30800, lr = 1e-05, m = 0.9
I0731 19:50:22.309486 16895 solver.cpp:353] Iteration 30900 (5.36582 iter/s, 18.6365s/100 iter), loss = 0.0820568
I0731 19:50:22.309535 16895 solver.cpp:375]     Train net output #0: loss = 0.0820567 (* 1 = 0.0820567 loss)
I0731 19:50:22.309540 16895 sgd_solver.cpp:136] Iteration 30900, lr = 1e-05, m = 0.9
I0731 19:50:22.870481 16901 data_reader.cpp:264] Starting prefetch of epoch 19
I0731 19:50:40.807093 16895 solver.cpp:353] Iteration 31000 (5.40626 iter/s, 18.4971s/100 iter), loss = 0.0724859
I0731 19:50:40.807121 16895 solver.cpp:375]     Train net output #0: loss = 0.0724858 (* 1 = 0.0724858 loss)
I0731 19:50:40.807126 16895 sgd_solver.cpp:136] Iteration 31000, lr = 1e-05, m = 0.9
I0731 19:50:53.653362 16898 data_reader.cpp:264] Starting prefetch of epoch 16
I0731 19:50:59.428081 16895 solver.cpp:353] Iteration 31100 (5.37043 iter/s, 18.6205s/100 iter), loss = 0.081388
I0731 19:50:59.428104 16895 solver.cpp:375]     Train net output #0: loss = 0.0813879 (* 1 = 0.0813879 loss)
I0731 19:50:59.428109 16895 sgd_solver.cpp:136] Iteration 31100, lr = 1e-05, m = 0.9
I0731 19:51:17.879245 16895 solver.cpp:353] Iteration 31200 (5.41986 iter/s, 18.4507s/100 iter), loss = 0.162828
I0731 19:51:17.879271 16895 solver.cpp:375]     Train net output #0: loss = 0.162828 (* 1 = 0.162828 loss)
I0731 19:51:17.879276 16895 sgd_solver.cpp:136] Iteration 31200, lr = 1e-05, m = 0.9
I0731 19:51:36.347036 16895 solver.cpp:353] Iteration 31300 (5.41498 iter/s, 18.4673s/100 iter), loss = 0.110033
I0731 19:51:36.347098 16895 solver.cpp:375]     Train net output #0: loss = 0.110033 (* 1 = 0.110033 loss)
I0731 19:51:36.347105 16895 sgd_solver.cpp:136] Iteration 31300, lr = 1e-05, m = 0.9
I0731 19:51:54.666635 16901 data_reader.cpp:264] Starting prefetch of epoch 20
I0731 19:51:54.847101 16895 solver.cpp:353] Iteration 31400 (5.40554 iter/s, 18.4996s/100 iter), loss = 0.0658627
I0731 19:51:54.847127 16895 solver.cpp:375]     Train net output #0: loss = 0.0658627 (* 1 = 0.0658627 loss)
I0731 19:51:54.847131 16895 sgd_solver.cpp:136] Iteration 31400, lr = 1e-05, m = 0.9
I0731 19:52:13.387361 16895 solver.cpp:353] Iteration 31500 (5.39382 iter/s, 18.5397s/100 iter), loss = 0.0709707
I0731 19:52:13.387467 16895 solver.cpp:375]     Train net output #0: loss = 0.0709706 (* 1 = 0.0709706 loss)
I0731 19:52:13.387475 16895 sgd_solver.cpp:136] Iteration 31500, lr = 1e-05, m = 0.9
I0731 19:52:31.985975 16895 solver.cpp:353] Iteration 31600 (5.37689 iter/s, 18.5981s/100 iter), loss = 0.196909
I0731 19:52:31.985999 16895 solver.cpp:375]     Train net output #0: loss = 0.196909 (* 1 = 0.196909 loss)
I0731 19:52:31.986003 16895 sgd_solver.cpp:136] Iteration 31600, lr = 1e-05, m = 0.9
I0731 19:52:50.521822 16895 solver.cpp:353] Iteration 31700 (5.3951 iter/s, 18.5353s/100 iter), loss = 0.0991279
I0731 19:52:50.521874 16895 solver.cpp:375]     Train net output #0: loss = 0.0991279 (* 1 = 0.0991279 loss)
I0731 19:52:50.521879 16895 sgd_solver.cpp:136] Iteration 31700, lr = 1e-05, m = 0.9
I0731 19:52:56.089323 16876 data_reader.cpp:264] Starting prefetch of epoch 19
I0731 19:53:08.965854 16895 solver.cpp:353] Iteration 31800 (5.42196 iter/s, 18.4435s/100 iter), loss = 0.0821226
I0731 19:53:08.965878 16895 solver.cpp:375]     Train net output #0: loss = 0.0821225 (* 1 = 0.0821225 loss)
I0731 19:53:08.965883 16895 sgd_solver.cpp:136] Iteration 31800, lr = 1e-05, m = 0.9
I0731 19:53:26.679898 16899 data_reader.cpp:264] Starting prefetch of epoch 22
I0731 19:53:27.554625 16895 solver.cpp:353] Iteration 31900 (5.37974 iter/s, 18.5883s/100 iter), loss = 0.051225
I0731 19:53:27.554648 16895 solver.cpp:375]     Train net output #0: loss = 0.051225 (* 1 = 0.051225 loss)
I0731 19:53:27.554653 16895 sgd_solver.cpp:136] Iteration 31900, lr = 1e-05, m = 0.9
I0731 19:53:45.906549 16895 solver.cpp:353] Iteration 31999 (5.39468 iter/s, 18.3514s/99 iter), loss = 0.0638738
I0731 19:53:45.906575 16895 solver.cpp:375]     Train net output #0: loss = 0.0638738 (* 1 = 0.0638738 loss)
I0731 19:53:45.941938 16895 solver.cpp:680] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/initial/cityscapes5_jsegnet21v2_iter_32000.caffemodel
I0731 19:53:45.990514 16895 sgd_solver.cpp:310] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2017-07-31_18-11-04/initial/cityscapes5_jsegnet21v2_iter_32000.solverstate
I0731 19:53:46.062991 16895 solver.cpp:527] Iteration 32000, loss = 0.06889
I0731 19:53:46.063015 16895 solver.cpp:550] Iteration 32000, Testing net (#0)
I0731 19:53:56.963451 16893 data_reader.cpp:264] Starting prefetch of epoch 4
I0731 19:53:57.428248 16895 solver.cpp:635]     Test net output #0: accuracy/top1 = 0.949015
I0731 19:53:57.428269 16895 solver.cpp:635]     Test net output #1: accuracy/top5 = 1
I0731 19:53:57.428277 16895 solver.cpp:635]     Test net output #2: loss = 0.159448 (* 1 = 0.159448 loss)
I0731 19:53:57.465728 16855 parallel.cpp:73] Root Solver performance on device 0: 5.198 * 6 = 31.19 img/sec (32000 itr in 6156 sec)
I0731 19:53:57.465749 16855 parallel.cpp:78]      Solver performance on device 1: 5.198 * 6 = 31.19 img/sec (32000 itr in 6156 sec)
I0731 19:53:57.465754 16855 parallel.cpp:78]      Solver performance on device 2: 5.198 * 6 = 31.19 img/sec (32000 itr in 6156 sec)
I0731 19:53:57.465755 16855 parallel.cpp:81] Overall multi-GPU performance: 93.5672 img/sec
I0731 19:53:58.702842 16855 caffe.cpp:247] Optimization Done in 1h 42m 53s
