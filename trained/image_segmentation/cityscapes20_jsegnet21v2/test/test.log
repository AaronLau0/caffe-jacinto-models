I0701 00:17:44.847193  9899 caffe.cpp:264] Not using GPU #2 for single-GPU function
I0701 00:17:44.847302  9899 caffe.cpp:264] Not using GPU #1 for single-GPU function
I0701 00:17:45.942719  9899 caffe.cpp:273] Use GPU with device ID 0
I0701 00:17:45.943646  9899 caffe.cpp:277] GPU device name: GeForce GTX 1080
I0701 00:17:46.755717  9899 net.cpp:56] Initializing net from parameters: 
name: "jsegnet21v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/val-image-lmdb"
    label_list_path: "data/val-label-lmdb"
    batch_size: 4
    threads: 4
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b/bn"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a/bn"
  top: "out5a/bn"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a/bn"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b/bn"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a/bn"
  top: "out3a/bn"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a/bn"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv1/bn"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv2/bn"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv3/bn"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4/bn"
  top: "ctx_conv4/bn"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4/bn"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 20
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 20
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 20
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
  accuracy_param {
    ignore_label: 255
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
    ignore_label: 255
  }
}
I0701 00:17:46.763599  9899 layer_factory.hpp:77] Creating layer data
I0701 00:17:46.763723  9899 net.cpp:98] Creating Layer data
I0701 00:17:46.763752  9899 net.cpp:413] data -> data
I0701 00:17:46.763815  9899 net.cpp:413] data -> label
I0701 00:17:46.786564  9952 db_lmdb.cpp:35] Opened lmdb data/val-image-lmdb
I0701 00:17:46.790571  9899 data_layer.cpp:78] ReshapePrefetch 4, 3, 640, 640
I0701 00:17:46.790612  9899 data_layer.cpp:83] output data size: 4,3,640,640
I0701 00:17:46.815685  9957 db_lmdb.cpp:35] Opened lmdb data/val-label-lmdb
I0701 00:17:46.817770  9899 data_layer.cpp:78] ReshapePrefetch 4, 1, 640, 640
I0701 00:17:46.817824  9899 data_layer.cpp:83] output data size: 4,1,640,640
I0701 00:17:46.832326  9899 net.cpp:148] Setting up data
I0701 00:17:46.832383  9899 net.cpp:155] Top shape: 4 3 640 640 (4915200)
I0701 00:17:46.832391  9899 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0701 00:17:46.832394  9899 net.cpp:163] Memory required for data: 26214400
I0701 00:17:46.832415  9899 layer_factory.hpp:77] Creating layer label_data_1_split
I0701 00:17:46.832439  9899 net.cpp:98] Creating Layer label_data_1_split
I0701 00:17:46.832448  9899 net.cpp:439] label_data_1_split <- label
I0701 00:17:46.832502  9899 net.cpp:413] label_data_1_split -> label_data_1_split_0
I0701 00:17:46.832515  9899 net.cpp:413] label_data_1_split -> label_data_1_split_1
I0701 00:17:46.832527  9899 net.cpp:413] label_data_1_split -> label_data_1_split_2
I0701 00:17:46.834403  9899 net.cpp:148] Setting up label_data_1_split
I0701 00:17:46.834411  9899 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0701 00:17:46.834414  9899 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0701 00:17:46.834415  9899 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0701 00:17:46.834417  9899 net.cpp:163] Memory required for data: 45875200
I0701 00:17:46.834420  9899 layer_factory.hpp:77] Creating layer data/bias
I0701 00:17:46.834426  9899 net.cpp:98] Creating Layer data/bias
I0701 00:17:46.834429  9899 net.cpp:439] data/bias <- data
I0701 00:17:46.834434  9899 net.cpp:413] data/bias -> data/bias
I0701 00:17:46.835453  9899 net.cpp:148] Setting up data/bias
I0701 00:17:46.835467  9899 net.cpp:155] Top shape: 4 3 640 640 (4915200)
I0701 00:17:46.835470  9899 net.cpp:163] Memory required for data: 65536000
I0701 00:17:46.835489  9899 layer_factory.hpp:77] Creating layer conv1a
I0701 00:17:46.835511  9899 net.cpp:98] Creating Layer conv1a
I0701 00:17:46.835517  9899 net.cpp:439] conv1a <- data/bias
I0701 00:17:46.835522  9899 net.cpp:413] conv1a -> conv1a
I0701 00:17:46.838974  9899 net.cpp:148] Setting up conv1a
I0701 00:17:46.839027  9899 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0701 00:17:46.839031  9899 net.cpp:163] Memory required for data: 117964800
I0701 00:17:46.839053  9899 layer_factory.hpp:77] Creating layer conv1a/bn
I0701 00:17:46.839076  9899 net.cpp:98] Creating Layer conv1a/bn
I0701 00:17:46.839083  9899 net.cpp:439] conv1a/bn <- conv1a
I0701 00:17:46.839095  9899 net.cpp:413] conv1a/bn -> conv1a/bn
I0701 00:17:46.841954  9899 net.cpp:148] Setting up conv1a/bn
I0701 00:17:46.842023  9899 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0701 00:17:46.842028  9899 net.cpp:163] Memory required for data: 170393600
I0701 00:17:46.842061  9899 layer_factory.hpp:77] Creating layer conv1a/relu
I0701 00:17:46.842097  9899 net.cpp:98] Creating Layer conv1a/relu
I0701 00:17:46.842110  9899 net.cpp:439] conv1a/relu <- conv1a/bn
I0701 00:17:46.842149  9899 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0701 00:17:46.842180  9899 net.cpp:148] Setting up conv1a/relu
I0701 00:17:46.842185  9899 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0701 00:17:46.842188  9899 net.cpp:163] Memory required for data: 222822400
I0701 00:17:46.842197  9899 layer_factory.hpp:77] Creating layer conv1b
I0701 00:17:46.842217  9899 net.cpp:98] Creating Layer conv1b
I0701 00:17:46.842226  9899 net.cpp:439] conv1b <- conv1a/bn
I0701 00:17:46.842254  9899 net.cpp:413] conv1b -> conv1b
I0701 00:17:46.842715  9899 net.cpp:148] Setting up conv1b
I0701 00:17:46.842726  9899 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0701 00:17:46.842730  9899 net.cpp:163] Memory required for data: 275251200
I0701 00:17:46.842737  9899 layer_factory.hpp:77] Creating layer conv1b/bn
I0701 00:17:46.842754  9899 net.cpp:98] Creating Layer conv1b/bn
I0701 00:17:46.842759  9899 net.cpp:439] conv1b/bn <- conv1b
I0701 00:17:46.842764  9899 net.cpp:413] conv1b/bn -> conv1b/bn
I0701 00:17:46.843300  9899 net.cpp:148] Setting up conv1b/bn
I0701 00:17:46.843308  9899 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0701 00:17:46.843312  9899 net.cpp:163] Memory required for data: 327680000
I0701 00:17:46.843327  9899 layer_factory.hpp:77] Creating layer conv1b/relu
I0701 00:17:46.843335  9899 net.cpp:98] Creating Layer conv1b/relu
I0701 00:17:46.843344  9899 net.cpp:439] conv1b/relu <- conv1b/bn
I0701 00:17:46.843350  9899 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0701 00:17:46.843361  9899 net.cpp:148] Setting up conv1b/relu
I0701 00:17:46.843366  9899 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0701 00:17:46.843369  9899 net.cpp:163] Memory required for data: 380108800
I0701 00:17:46.843376  9899 layer_factory.hpp:77] Creating layer pool1
I0701 00:17:46.843389  9899 net.cpp:98] Creating Layer pool1
I0701 00:17:46.843394  9899 net.cpp:439] pool1 <- conv1b/bn
I0701 00:17:46.843397  9899 net.cpp:413] pool1 -> pool1
I0701 00:17:46.843799  9899 net.cpp:148] Setting up pool1
I0701 00:17:46.843807  9899 net.cpp:155] Top shape: 4 32 160 160 (3276800)
I0701 00:17:46.843811  9899 net.cpp:163] Memory required for data: 393216000
I0701 00:17:46.843828  9899 layer_factory.hpp:77] Creating layer res2a_branch2a
I0701 00:17:46.843842  9899 net.cpp:98] Creating Layer res2a_branch2a
I0701 00:17:46.843845  9899 net.cpp:439] res2a_branch2a <- pool1
I0701 00:17:46.843859  9899 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0701 00:17:46.845479  9899 net.cpp:148] Setting up res2a_branch2a
I0701 00:17:46.845494  9899 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0701 00:17:46.845497  9899 net.cpp:163] Memory required for data: 419430400
I0701 00:17:46.845508  9899 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0701 00:17:46.845527  9899 net.cpp:98] Creating Layer res2a_branch2a/bn
I0701 00:17:46.845533  9899 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0701 00:17:46.845540  9899 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0701 00:17:46.845897  9899 net.cpp:148] Setting up res2a_branch2a/bn
I0701 00:17:46.845906  9899 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0701 00:17:46.845908  9899 net.cpp:163] Memory required for data: 445644800
I0701 00:17:46.845924  9899 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0701 00:17:46.845932  9899 net.cpp:98] Creating Layer res2a_branch2a/relu
I0701 00:17:46.845942  9899 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0701 00:17:46.845947  9899 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0701 00:17:46.845959  9899 net.cpp:148] Setting up res2a_branch2a/relu
I0701 00:17:46.845969  9899 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0701 00:17:46.845973  9899 net.cpp:163] Memory required for data: 471859200
I0701 00:17:46.845978  9899 layer_factory.hpp:77] Creating layer res2a_branch2b
I0701 00:17:46.845990  9899 net.cpp:98] Creating Layer res2a_branch2b
I0701 00:17:46.845994  9899 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0701 00:17:46.846004  9899 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0701 00:17:46.847256  9899 net.cpp:148] Setting up res2a_branch2b
I0701 00:17:46.847270  9899 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0701 00:17:46.847273  9899 net.cpp:163] Memory required for data: 498073600
I0701 00:17:46.847280  9899 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0701 00:17:46.847295  9899 net.cpp:98] Creating Layer res2a_branch2b/bn
I0701 00:17:46.847317  9899 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0701 00:17:46.847342  9899 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0701 00:17:46.847810  9899 net.cpp:148] Setting up res2a_branch2b/bn
I0701 00:17:46.847820  9899 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0701 00:17:46.847822  9899 net.cpp:163] Memory required for data: 524288000
I0701 00:17:46.847832  9899 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0701 00:17:46.847846  9899 net.cpp:98] Creating Layer res2a_branch2b/relu
I0701 00:17:46.847854  9899 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0701 00:17:46.847865  9899 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0701 00:17:46.847878  9899 net.cpp:148] Setting up res2a_branch2b/relu
I0701 00:17:46.847889  9899 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0701 00:17:46.847898  9899 net.cpp:163] Memory required for data: 550502400
I0701 00:17:46.847906  9899 layer_factory.hpp:77] Creating layer pool2
I0701 00:17:46.847921  9899 net.cpp:98] Creating Layer pool2
I0701 00:17:46.847931  9899 net.cpp:439] pool2 <- res2a_branch2b/bn
I0701 00:17:46.847942  9899 net.cpp:413] pool2 -> pool2
I0701 00:17:46.847986  9899 net.cpp:148] Setting up pool2
I0701 00:17:46.847998  9899 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0701 00:17:46.848007  9899 net.cpp:163] Memory required for data: 557056000
I0701 00:17:46.848016  9899 layer_factory.hpp:77] Creating layer res3a_branch2a
I0701 00:17:46.848028  9899 net.cpp:98] Creating Layer res3a_branch2a
I0701 00:17:46.848038  9899 net.cpp:439] res3a_branch2a <- pool2
I0701 00:17:46.848049  9899 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0701 00:17:46.849958  9899 net.cpp:148] Setting up res3a_branch2a
I0701 00:17:46.849973  9899 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0701 00:17:46.849979  9899 net.cpp:163] Memory required for data: 570163200
I0701 00:17:46.849989  9899 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0701 00:17:46.850002  9899 net.cpp:98] Creating Layer res3a_branch2a/bn
I0701 00:17:46.850008  9899 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0701 00:17:46.850018  9899 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0701 00:17:46.850664  9899 net.cpp:148] Setting up res3a_branch2a/bn
I0701 00:17:46.850679  9899 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0701 00:17:46.850683  9899 net.cpp:163] Memory required for data: 583270400
I0701 00:17:46.850699  9899 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0701 00:17:46.850708  9899 net.cpp:98] Creating Layer res3a_branch2a/relu
I0701 00:17:46.850713  9899 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0701 00:17:46.850720  9899 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0701 00:17:46.850728  9899 net.cpp:148] Setting up res3a_branch2a/relu
I0701 00:17:46.850750  9899 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0701 00:17:46.850755  9899 net.cpp:163] Memory required for data: 596377600
I0701 00:17:46.850760  9899 layer_factory.hpp:77] Creating layer res3a_branch2b
I0701 00:17:46.850774  9899 net.cpp:98] Creating Layer res3a_branch2b
I0701 00:17:46.850780  9899 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0701 00:17:46.850791  9899 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0701 00:17:46.852941  9899 net.cpp:148] Setting up res3a_branch2b
I0701 00:17:46.852995  9899 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0701 00:17:46.852999  9899 net.cpp:163] Memory required for data: 609484800
I0701 00:17:46.853006  9899 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0701 00:17:46.853016  9899 net.cpp:98] Creating Layer res3a_branch2b/bn
I0701 00:17:46.853020  9899 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0701 00:17:46.853037  9899 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0701 00:17:46.853368  9899 net.cpp:148] Setting up res3a_branch2b/bn
I0701 00:17:46.853374  9899 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0701 00:17:46.853379  9899 net.cpp:163] Memory required for data: 622592000
I0701 00:17:46.853387  9899 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0701 00:17:46.853396  9899 net.cpp:98] Creating Layer res3a_branch2b/relu
I0701 00:17:46.853415  9899 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0701 00:17:46.853420  9899 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0701 00:17:46.853426  9899 net.cpp:148] Setting up res3a_branch2b/relu
I0701 00:17:46.853430  9899 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0701 00:17:46.853433  9899 net.cpp:163] Memory required for data: 635699200
I0701 00:17:46.853438  9899 layer_factory.hpp:77] Creating layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0701 00:17:46.853446  9899 net.cpp:98] Creating Layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0701 00:17:46.853447  9899 net.cpp:439] res3a_branch2b/bn_res3a_branch2b/relu_0_split <- res3a_branch2b/bn
I0701 00:17:46.853451  9899 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0701 00:17:46.853457  9899 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0701 00:17:46.853485  9899 net.cpp:148] Setting up res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0701 00:17:46.853489  9899 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0701 00:17:46.853494  9899 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0701 00:17:46.853497  9899 net.cpp:163] Memory required for data: 661913600
I0701 00:17:46.853502  9899 layer_factory.hpp:77] Creating layer pool3
I0701 00:17:46.853509  9899 net.cpp:98] Creating Layer pool3
I0701 00:17:46.853513  9899 net.cpp:439] pool3 <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0701 00:17:46.853518  9899 net.cpp:413] pool3 -> pool3
I0701 00:17:46.853539  9899 net.cpp:148] Setting up pool3
I0701 00:17:46.853544  9899 net.cpp:155] Top shape: 4 128 40 40 (819200)
I0701 00:17:46.853548  9899 net.cpp:163] Memory required for data: 665190400
I0701 00:17:46.853552  9899 layer_factory.hpp:77] Creating layer res4a_branch2a
I0701 00:17:46.853560  9899 net.cpp:98] Creating Layer res4a_branch2a
I0701 00:17:46.853564  9899 net.cpp:439] res4a_branch2a <- pool3
I0701 00:17:46.853569  9899 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0701 00:17:46.861945  9899 net.cpp:148] Setting up res4a_branch2a
I0701 00:17:46.862040  9899 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0701 00:17:46.862062  9899 net.cpp:163] Memory required for data: 671744000
I0701 00:17:46.862085  9899 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0701 00:17:46.862115  9899 net.cpp:98] Creating Layer res4a_branch2a/bn
I0701 00:17:46.862134  9899 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0701 00:17:46.862155  9899 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0701 00:17:46.862702  9899 net.cpp:148] Setting up res4a_branch2a/bn
I0701 00:17:46.862736  9899 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0701 00:17:46.862751  9899 net.cpp:163] Memory required for data: 678297600
I0701 00:17:46.862771  9899 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0701 00:17:46.862790  9899 net.cpp:98] Creating Layer res4a_branch2a/relu
I0701 00:17:46.862802  9899 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0701 00:17:46.862817  9899 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0701 00:17:46.862833  9899 net.cpp:148] Setting up res4a_branch2a/relu
I0701 00:17:46.862848  9899 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0701 00:17:46.862859  9899 net.cpp:163] Memory required for data: 684851200
I0701 00:17:46.862870  9899 layer_factory.hpp:77] Creating layer res4a_branch2b
I0701 00:17:46.862890  9899 net.cpp:98] Creating Layer res4a_branch2b
I0701 00:17:46.862903  9899 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0701 00:17:46.862917  9899 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0701 00:17:46.867533  9899 net.cpp:148] Setting up res4a_branch2b
I0701 00:17:46.867673  9899 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0701 00:17:46.867707  9899 net.cpp:163] Memory required for data: 691404800
I0701 00:17:46.867738  9899 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0701 00:17:46.867789  9899 net.cpp:98] Creating Layer res4a_branch2b/bn
I0701 00:17:46.867836  9899 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0701 00:17:46.867866  9899 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0701 00:17:46.868358  9899 net.cpp:148] Setting up res4a_branch2b/bn
I0701 00:17:46.868379  9899 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0701 00:17:46.868391  9899 net.cpp:163] Memory required for data: 697958400
I0701 00:17:46.868409  9899 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0701 00:17:46.868427  9899 net.cpp:98] Creating Layer res4a_branch2b/relu
I0701 00:17:46.868436  9899 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0701 00:17:46.868448  9899 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0701 00:17:46.868463  9899 net.cpp:148] Setting up res4a_branch2b/relu
I0701 00:17:46.868475  9899 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0701 00:17:46.868485  9899 net.cpp:163] Memory required for data: 704512000
I0701 00:17:46.868495  9899 layer_factory.hpp:77] Creating layer pool4
I0701 00:17:46.868512  9899 net.cpp:98] Creating Layer pool4
I0701 00:17:46.868522  9899 net.cpp:439] pool4 <- res4a_branch2b/bn
I0701 00:17:46.868535  9899 net.cpp:413] pool4 -> pool4
I0701 00:17:46.868574  9899 net.cpp:148] Setting up pool4
I0701 00:17:46.868588  9899 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0701 00:17:46.868598  9899 net.cpp:163] Memory required for data: 711065600
I0701 00:17:46.868609  9899 layer_factory.hpp:77] Creating layer res5a_branch2a
I0701 00:17:46.868636  9899 net.cpp:98] Creating Layer res5a_branch2a
I0701 00:17:46.868649  9899 net.cpp:439] res5a_branch2a <- pool4
I0701 00:17:46.868660  9899 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0701 00:17:46.895473  9899 net.cpp:148] Setting up res5a_branch2a
I0701 00:17:46.895491  9899 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0701 00:17:46.895494  9899 net.cpp:163] Memory required for data: 724172800
I0701 00:17:46.895499  9899 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0701 00:17:46.895508  9899 net.cpp:98] Creating Layer res5a_branch2a/bn
I0701 00:17:46.895510  9899 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0701 00:17:46.895515  9899 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0701 00:17:46.895795  9899 net.cpp:148] Setting up res5a_branch2a/bn
I0701 00:17:46.895802  9899 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0701 00:17:46.895804  9899 net.cpp:163] Memory required for data: 737280000
I0701 00:17:46.895809  9899 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0701 00:17:46.895813  9899 net.cpp:98] Creating Layer res5a_branch2a/relu
I0701 00:17:46.895815  9899 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0701 00:17:46.895817  9899 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0701 00:17:46.895822  9899 net.cpp:148] Setting up res5a_branch2a/relu
I0701 00:17:46.895823  9899 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0701 00:17:46.895825  9899 net.cpp:163] Memory required for data: 750387200
I0701 00:17:46.895828  9899 layer_factory.hpp:77] Creating layer res5a_branch2b
I0701 00:17:46.895833  9899 net.cpp:98] Creating Layer res5a_branch2b
I0701 00:17:46.895835  9899 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0701 00:17:46.895838  9899 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0701 00:17:46.908035  9899 net.cpp:148] Setting up res5a_branch2b
I0701 00:17:46.908052  9899 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0701 00:17:46.908054  9899 net.cpp:163] Memory required for data: 763494400
I0701 00:17:46.908063  9899 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0701 00:17:46.908069  9899 net.cpp:98] Creating Layer res5a_branch2b/bn
I0701 00:17:46.908072  9899 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0701 00:17:46.908077  9899 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0701 00:17:46.908362  9899 net.cpp:148] Setting up res5a_branch2b/bn
I0701 00:17:46.908367  9899 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0701 00:17:46.908370  9899 net.cpp:163] Memory required for data: 776601600
I0701 00:17:46.908375  9899 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0701 00:17:46.908388  9899 net.cpp:98] Creating Layer res5a_branch2b/relu
I0701 00:17:46.908390  9899 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0701 00:17:46.908393  9899 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0701 00:17:46.908397  9899 net.cpp:148] Setting up res5a_branch2b/relu
I0701 00:17:46.908399  9899 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0701 00:17:46.908401  9899 net.cpp:163] Memory required for data: 789708800
I0701 00:17:46.908403  9899 layer_factory.hpp:77] Creating layer out5a
I0701 00:17:46.908409  9899 net.cpp:98] Creating Layer out5a
I0701 00:17:46.908411  9899 net.cpp:439] out5a <- res5a_branch2b/bn
I0701 00:17:46.908414  9899 net.cpp:413] out5a -> out5a
I0701 00:17:46.912003  9899 net.cpp:148] Setting up out5a
I0701 00:17:46.912012  9899 net.cpp:155] Top shape: 4 64 40 40 (409600)
I0701 00:17:46.912014  9899 net.cpp:163] Memory required for data: 791347200
I0701 00:17:46.912019  9899 layer_factory.hpp:77] Creating layer out5a/bn
I0701 00:17:46.912024  9899 net.cpp:98] Creating Layer out5a/bn
I0701 00:17:46.912026  9899 net.cpp:439] out5a/bn <- out5a
I0701 00:17:46.912030  9899 net.cpp:413] out5a/bn -> out5a/bn
I0701 00:17:46.912330  9899 net.cpp:148] Setting up out5a/bn
I0701 00:17:46.912335  9899 net.cpp:155] Top shape: 4 64 40 40 (409600)
I0701 00:17:46.912338  9899 net.cpp:163] Memory required for data: 792985600
I0701 00:17:46.912343  9899 layer_factory.hpp:77] Creating layer out5a/relu
I0701 00:17:46.912345  9899 net.cpp:98] Creating Layer out5a/relu
I0701 00:17:46.912348  9899 net.cpp:439] out5a/relu <- out5a/bn
I0701 00:17:46.912350  9899 net.cpp:400] out5a/relu -> out5a/bn (in-place)
I0701 00:17:46.912353  9899 net.cpp:148] Setting up out5a/relu
I0701 00:17:46.912356  9899 net.cpp:155] Top shape: 4 64 40 40 (409600)
I0701 00:17:46.912358  9899 net.cpp:163] Memory required for data: 794624000
I0701 00:17:46.912359  9899 layer_factory.hpp:77] Creating layer out5a_up2
I0701 00:17:46.912367  9899 net.cpp:98] Creating Layer out5a_up2
I0701 00:17:46.912370  9899 net.cpp:439] out5a_up2 <- out5a/bn
I0701 00:17:46.912374  9899 net.cpp:413] out5a_up2 -> out5a_up2
I0701 00:17:46.912492  9899 net.cpp:148] Setting up out5a_up2
I0701 00:17:46.912497  9899 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0701 00:17:46.912498  9899 net.cpp:163] Memory required for data: 801177600
I0701 00:17:46.912502  9899 layer_factory.hpp:77] Creating layer out3a
I0701 00:17:46.912505  9899 net.cpp:98] Creating Layer out3a
I0701 00:17:46.912508  9899 net.cpp:439] out3a <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0701 00:17:46.912511  9899 net.cpp:413] out3a -> out3a
I0701 00:17:46.913384  9899 net.cpp:148] Setting up out3a
I0701 00:17:46.913390  9899 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0701 00:17:46.913393  9899 net.cpp:163] Memory required for data: 807731200
I0701 00:17:46.913395  9899 layer_factory.hpp:77] Creating layer out3a/bn
I0701 00:17:46.913401  9899 net.cpp:98] Creating Layer out3a/bn
I0701 00:17:46.913404  9899 net.cpp:439] out3a/bn <- out3a
I0701 00:17:46.913408  9899 net.cpp:413] out3a/bn -> out3a/bn
I0701 00:17:46.913709  9899 net.cpp:148] Setting up out3a/bn
I0701 00:17:46.913714  9899 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0701 00:17:46.913717  9899 net.cpp:163] Memory required for data: 814284800
I0701 00:17:46.913722  9899 layer_factory.hpp:77] Creating layer out3a/relu
I0701 00:17:46.913727  9899 net.cpp:98] Creating Layer out3a/relu
I0701 00:17:46.913729  9899 net.cpp:439] out3a/relu <- out3a/bn
I0701 00:17:46.913731  9899 net.cpp:400] out3a/relu -> out3a/bn (in-place)
I0701 00:17:46.913735  9899 net.cpp:148] Setting up out3a/relu
I0701 00:17:46.913738  9899 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0701 00:17:46.913740  9899 net.cpp:163] Memory required for data: 820838400
I0701 00:17:46.913743  9899 layer_factory.hpp:77] Creating layer out3_out5_combined
I0701 00:17:46.913748  9899 net.cpp:98] Creating Layer out3_out5_combined
I0701 00:17:46.913750  9899 net.cpp:439] out3_out5_combined <- out5a_up2
I0701 00:17:46.913763  9899 net.cpp:439] out3_out5_combined <- out3a/bn
I0701 00:17:46.913767  9899 net.cpp:413] out3_out5_combined -> out3_out5_combined
I0701 00:17:46.913784  9899 net.cpp:148] Setting up out3_out5_combined
I0701 00:17:46.913789  9899 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0701 00:17:46.913791  9899 net.cpp:163] Memory required for data: 827392000
I0701 00:17:46.913794  9899 layer_factory.hpp:77] Creating layer ctx_conv1
I0701 00:17:46.913800  9899 net.cpp:98] Creating Layer ctx_conv1
I0701 00:17:46.913802  9899 net.cpp:439] ctx_conv1 <- out3_out5_combined
I0701 00:17:46.913806  9899 net.cpp:413] ctx_conv1 -> ctx_conv1
I0701 00:17:46.914702  9899 net.cpp:148] Setting up ctx_conv1
I0701 00:17:46.914710  9899 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0701 00:17:46.914712  9899 net.cpp:163] Memory required for data: 833945600
I0701 00:17:46.914716  9899 layer_factory.hpp:77] Creating layer ctx_conv1/bn
I0701 00:17:46.914719  9899 net.cpp:98] Creating Layer ctx_conv1/bn
I0701 00:17:46.914721  9899 net.cpp:439] ctx_conv1/bn <- ctx_conv1
I0701 00:17:46.914726  9899 net.cpp:413] ctx_conv1/bn -> ctx_conv1/bn
I0701 00:17:46.915021  9899 net.cpp:148] Setting up ctx_conv1/bn
I0701 00:17:46.915026  9899 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0701 00:17:46.915029  9899 net.cpp:163] Memory required for data: 840499200
I0701 00:17:46.915033  9899 layer_factory.hpp:77] Creating layer ctx_conv1/relu
I0701 00:17:46.915036  9899 net.cpp:98] Creating Layer ctx_conv1/relu
I0701 00:17:46.915038  9899 net.cpp:439] ctx_conv1/relu <- ctx_conv1/bn
I0701 00:17:46.915041  9899 net.cpp:400] ctx_conv1/relu -> ctx_conv1/bn (in-place)
I0701 00:17:46.915045  9899 net.cpp:148] Setting up ctx_conv1/relu
I0701 00:17:46.915046  9899 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0701 00:17:46.915048  9899 net.cpp:163] Memory required for data: 847052800
I0701 00:17:46.915050  9899 layer_factory.hpp:77] Creating layer ctx_conv2
I0701 00:17:46.915053  9899 net.cpp:98] Creating Layer ctx_conv2
I0701 00:17:46.915055  9899 net.cpp:439] ctx_conv2 <- ctx_conv1/bn
I0701 00:17:46.915057  9899 net.cpp:413] ctx_conv2 -> ctx_conv2
I0701 00:17:46.915908  9899 net.cpp:148] Setting up ctx_conv2
I0701 00:17:46.915913  9899 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0701 00:17:46.915915  9899 net.cpp:163] Memory required for data: 853606400
I0701 00:17:46.915918  9899 layer_factory.hpp:77] Creating layer ctx_conv2/bn
I0701 00:17:46.915921  9899 net.cpp:98] Creating Layer ctx_conv2/bn
I0701 00:17:46.915923  9899 net.cpp:439] ctx_conv2/bn <- ctx_conv2
I0701 00:17:46.915926  9899 net.cpp:413] ctx_conv2/bn -> ctx_conv2/bn
I0701 00:17:46.916218  9899 net.cpp:148] Setting up ctx_conv2/bn
I0701 00:17:46.916224  9899 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0701 00:17:46.916225  9899 net.cpp:163] Memory required for data: 860160000
I0701 00:17:46.916230  9899 layer_factory.hpp:77] Creating layer ctx_conv2/relu
I0701 00:17:46.916232  9899 net.cpp:98] Creating Layer ctx_conv2/relu
I0701 00:17:46.916234  9899 net.cpp:439] ctx_conv2/relu <- ctx_conv2/bn
I0701 00:17:46.916236  9899 net.cpp:400] ctx_conv2/relu -> ctx_conv2/bn (in-place)
I0701 00:17:46.916239  9899 net.cpp:148] Setting up ctx_conv2/relu
I0701 00:17:46.916242  9899 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0701 00:17:46.916244  9899 net.cpp:163] Memory required for data: 866713600
I0701 00:17:46.916245  9899 layer_factory.hpp:77] Creating layer ctx_conv3
I0701 00:17:46.916250  9899 net.cpp:98] Creating Layer ctx_conv3
I0701 00:17:46.916252  9899 net.cpp:439] ctx_conv3 <- ctx_conv2/bn
I0701 00:17:46.916255  9899 net.cpp:413] ctx_conv3 -> ctx_conv3
I0701 00:17:46.917116  9899 net.cpp:148] Setting up ctx_conv3
I0701 00:17:46.917121  9899 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0701 00:17:46.917124  9899 net.cpp:163] Memory required for data: 873267200
I0701 00:17:46.917126  9899 layer_factory.hpp:77] Creating layer ctx_conv3/bn
I0701 00:17:46.917130  9899 net.cpp:98] Creating Layer ctx_conv3/bn
I0701 00:17:46.917132  9899 net.cpp:439] ctx_conv3/bn <- ctx_conv3
I0701 00:17:46.917143  9899 net.cpp:413] ctx_conv3/bn -> ctx_conv3/bn
I0701 00:17:46.917438  9899 net.cpp:148] Setting up ctx_conv3/bn
I0701 00:17:46.917443  9899 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0701 00:17:46.917444  9899 net.cpp:163] Memory required for data: 879820800
I0701 00:17:46.917449  9899 layer_factory.hpp:77] Creating layer ctx_conv3/relu
I0701 00:17:46.917454  9899 net.cpp:98] Creating Layer ctx_conv3/relu
I0701 00:17:46.917457  9899 net.cpp:439] ctx_conv3/relu <- ctx_conv3/bn
I0701 00:17:46.917461  9899 net.cpp:400] ctx_conv3/relu -> ctx_conv3/bn (in-place)
I0701 00:17:46.917476  9899 net.cpp:148] Setting up ctx_conv3/relu
I0701 00:17:46.917482  9899 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0701 00:17:46.917485  9899 net.cpp:163] Memory required for data: 886374400
I0701 00:17:46.917488  9899 layer_factory.hpp:77] Creating layer ctx_conv4
I0701 00:17:46.917496  9899 net.cpp:98] Creating Layer ctx_conv4
I0701 00:17:46.917500  9899 net.cpp:439] ctx_conv4 <- ctx_conv3/bn
I0701 00:17:46.917505  9899 net.cpp:413] ctx_conv4 -> ctx_conv4
I0701 00:17:46.918362  9899 net.cpp:148] Setting up ctx_conv4
I0701 00:17:46.918368  9899 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0701 00:17:46.918371  9899 net.cpp:163] Memory required for data: 892928000
I0701 00:17:46.918376  9899 layer_factory.hpp:77] Creating layer ctx_conv4/bn
I0701 00:17:46.918382  9899 net.cpp:98] Creating Layer ctx_conv4/bn
I0701 00:17:46.918391  9899 net.cpp:439] ctx_conv4/bn <- ctx_conv4
I0701 00:17:46.918396  9899 net.cpp:413] ctx_conv4/bn -> ctx_conv4/bn
I0701 00:17:46.918701  9899 net.cpp:148] Setting up ctx_conv4/bn
I0701 00:17:46.918707  9899 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0701 00:17:46.918712  9899 net.cpp:163] Memory required for data: 899481600
I0701 00:17:46.918720  9899 layer_factory.hpp:77] Creating layer ctx_conv4/relu
I0701 00:17:46.918725  9899 net.cpp:98] Creating Layer ctx_conv4/relu
I0701 00:17:46.918728  9899 net.cpp:439] ctx_conv4/relu <- ctx_conv4/bn
I0701 00:17:46.918735  9899 net.cpp:400] ctx_conv4/relu -> ctx_conv4/bn (in-place)
I0701 00:17:46.918740  9899 net.cpp:148] Setting up ctx_conv4/relu
I0701 00:17:46.918745  9899 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0701 00:17:46.918747  9899 net.cpp:163] Memory required for data: 906035200
I0701 00:17:46.918751  9899 layer_factory.hpp:77] Creating layer ctx_final
I0701 00:17:46.918758  9899 net.cpp:98] Creating Layer ctx_final
I0701 00:17:46.918761  9899 net.cpp:439] ctx_final <- ctx_conv4/bn
I0701 00:17:46.918768  9899 net.cpp:413] ctx_final -> ctx_final
I0701 00:17:46.919133  9899 net.cpp:148] Setting up ctx_final
I0701 00:17:46.919139  9899 net.cpp:155] Top shape: 4 20 80 80 (512000)
I0701 00:17:46.919143  9899 net.cpp:163] Memory required for data: 908083200
I0701 00:17:46.919149  9899 layer_factory.hpp:77] Creating layer ctx_final/relu
I0701 00:17:46.919154  9899 net.cpp:98] Creating Layer ctx_final/relu
I0701 00:17:46.919158  9899 net.cpp:439] ctx_final/relu <- ctx_final
I0701 00:17:46.919162  9899 net.cpp:400] ctx_final/relu -> ctx_final (in-place)
I0701 00:17:46.919168  9899 net.cpp:148] Setting up ctx_final/relu
I0701 00:17:46.919173  9899 net.cpp:155] Top shape: 4 20 80 80 (512000)
I0701 00:17:46.919176  9899 net.cpp:163] Memory required for data: 910131200
I0701 00:17:46.919179  9899 layer_factory.hpp:77] Creating layer out_deconv_final_up2
I0701 00:17:46.919186  9899 net.cpp:98] Creating Layer out_deconv_final_up2
I0701 00:17:46.919190  9899 net.cpp:439] out_deconv_final_up2 <- ctx_final
I0701 00:17:46.919195  9899 net.cpp:413] out_deconv_final_up2 -> out_deconv_final_up2
I0701 00:17:46.919301  9899 net.cpp:148] Setting up out_deconv_final_up2
I0701 00:17:46.919306  9899 net.cpp:155] Top shape: 4 20 160 160 (2048000)
I0701 00:17:46.919311  9899 net.cpp:163] Memory required for data: 918323200
I0701 00:17:46.919315  9899 layer_factory.hpp:77] Creating layer out_deconv_final_up4
I0701 00:17:46.919322  9899 net.cpp:98] Creating Layer out_deconv_final_up4
I0701 00:17:46.919325  9899 net.cpp:439] out_deconv_final_up4 <- out_deconv_final_up2
I0701 00:17:46.919337  9899 net.cpp:413] out_deconv_final_up4 -> out_deconv_final_up4
I0701 00:17:46.919448  9899 net.cpp:148] Setting up out_deconv_final_up4
I0701 00:17:46.919454  9899 net.cpp:155] Top shape: 4 20 320 320 (8192000)
I0701 00:17:46.919458  9899 net.cpp:163] Memory required for data: 951091200
I0701 00:17:46.919463  9899 layer_factory.hpp:77] Creating layer out_deconv_final_up8
I0701 00:17:46.919469  9899 net.cpp:98] Creating Layer out_deconv_final_up8
I0701 00:17:46.919473  9899 net.cpp:439] out_deconv_final_up8 <- out_deconv_final_up4
I0701 00:17:46.919478  9899 net.cpp:413] out_deconv_final_up8 -> out_deconv_final_up8
I0701 00:17:46.919584  9899 net.cpp:148] Setting up out_deconv_final_up8
I0701 00:17:46.919589  9899 net.cpp:155] Top shape: 4 20 640 640 (32768000)
I0701 00:17:46.919594  9899 net.cpp:163] Memory required for data: 1082163200
I0701 00:17:46.919598  9899 layer_factory.hpp:77] Creating layer out_deconv_final_up8_out_deconv_final_up8_0_split
I0701 00:17:46.919605  9899 net.cpp:98] Creating Layer out_deconv_final_up8_out_deconv_final_up8_0_split
I0701 00:17:46.919608  9899 net.cpp:439] out_deconv_final_up8_out_deconv_final_up8_0_split <- out_deconv_final_up8
I0701 00:17:46.919612  9899 net.cpp:413] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0701 00:17:46.919618  9899 net.cpp:413] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0701 00:17:46.919625  9899 net.cpp:413] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0701 00:17:46.919652  9899 net.cpp:148] Setting up out_deconv_final_up8_out_deconv_final_up8_0_split
I0701 00:17:46.919656  9899 net.cpp:155] Top shape: 4 20 640 640 (32768000)
I0701 00:17:46.919661  9899 net.cpp:155] Top shape: 4 20 640 640 (32768000)
I0701 00:17:46.919665  9899 net.cpp:155] Top shape: 4 20 640 640 (32768000)
I0701 00:17:46.919669  9899 net.cpp:163] Memory required for data: 1475379200
I0701 00:17:46.919673  9899 layer_factory.hpp:77] Creating layer loss
I0701 00:17:46.919687  9899 net.cpp:98] Creating Layer loss
I0701 00:17:46.919689  9899 net.cpp:439] loss <- out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0701 00:17:46.919694  9899 net.cpp:439] loss <- label_data_1_split_0
I0701 00:17:46.919699  9899 net.cpp:413] loss -> loss
I0701 00:17:46.919711  9899 layer_factory.hpp:77] Creating layer loss
I0701 00:17:46.957788  9899 net.cpp:148] Setting up loss
I0701 00:17:46.957809  9899 net.cpp:155] Top shape: (1)
I0701 00:17:46.957813  9899 net.cpp:158]     with loss weight 1
I0701 00:17:46.957828  9899 net.cpp:163] Memory required for data: 1475379204
I0701 00:17:46.957834  9899 layer_factory.hpp:77] Creating layer accuracy/top1
I0701 00:17:46.957844  9899 net.cpp:98] Creating Layer accuracy/top1
I0701 00:17:46.957849  9899 net.cpp:439] accuracy/top1 <- out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0701 00:17:46.957855  9899 net.cpp:439] accuracy/top1 <- label_data_1_split_1
I0701 00:17:46.957861  9899 net.cpp:413] accuracy/top1 -> accuracy/top1
I0701 00:17:46.957877  9899 net.cpp:148] Setting up accuracy/top1
I0701 00:17:46.957881  9899 net.cpp:155] Top shape: (1)
I0701 00:17:46.957885  9899 net.cpp:163] Memory required for data: 1475379208
I0701 00:17:46.957888  9899 layer_factory.hpp:77] Creating layer accuracy/top5
I0701 00:17:46.957893  9899 net.cpp:98] Creating Layer accuracy/top5
I0701 00:17:46.957897  9899 net.cpp:439] accuracy/top5 <- out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0701 00:17:46.957902  9899 net.cpp:439] accuracy/top5 <- label_data_1_split_2
I0701 00:17:46.957907  9899 net.cpp:413] accuracy/top5 -> accuracy/top5
I0701 00:17:46.957913  9899 net.cpp:148] Setting up accuracy/top5
I0701 00:17:46.957917  9899 net.cpp:155] Top shape: (1)
I0701 00:17:46.957921  9899 net.cpp:163] Memory required for data: 1475379212
I0701 00:17:46.957924  9899 net.cpp:226] accuracy/top5 does not need backward computation.
I0701 00:17:46.957929  9899 net.cpp:226] accuracy/top1 does not need backward computation.
I0701 00:17:46.957942  9899 net.cpp:224] loss needs backward computation.
I0701 00:17:46.957947  9899 net.cpp:224] out_deconv_final_up8_out_deconv_final_up8_0_split needs backward computation.
I0701 00:17:46.957950  9899 net.cpp:224] out_deconv_final_up8 needs backward computation.
I0701 00:17:46.957954  9899 net.cpp:224] out_deconv_final_up4 needs backward computation.
I0701 00:17:46.957958  9899 net.cpp:224] out_deconv_final_up2 needs backward computation.
I0701 00:17:46.957962  9899 net.cpp:224] ctx_final/relu needs backward computation.
I0701 00:17:46.957967  9899 net.cpp:224] ctx_final needs backward computation.
I0701 00:17:46.957970  9899 net.cpp:224] ctx_conv4/relu needs backward computation.
I0701 00:17:46.957973  9899 net.cpp:224] ctx_conv4/bn needs backward computation.
I0701 00:17:46.957978  9899 net.cpp:224] ctx_conv4 needs backward computation.
I0701 00:17:46.957981  9899 net.cpp:224] ctx_conv3/relu needs backward computation.
I0701 00:17:46.957985  9899 net.cpp:224] ctx_conv3/bn needs backward computation.
I0701 00:17:46.957989  9899 net.cpp:224] ctx_conv3 needs backward computation.
I0701 00:17:46.957993  9899 net.cpp:224] ctx_conv2/relu needs backward computation.
I0701 00:17:46.957998  9899 net.cpp:224] ctx_conv2/bn needs backward computation.
I0701 00:17:46.958001  9899 net.cpp:224] ctx_conv2 needs backward computation.
I0701 00:17:46.958004  9899 net.cpp:224] ctx_conv1/relu needs backward computation.
I0701 00:17:46.958009  9899 net.cpp:224] ctx_conv1/bn needs backward computation.
I0701 00:17:46.958012  9899 net.cpp:224] ctx_conv1 needs backward computation.
I0701 00:17:46.958016  9899 net.cpp:224] out3_out5_combined needs backward computation.
I0701 00:17:46.958021  9899 net.cpp:224] out3a/relu needs backward computation.
I0701 00:17:46.958025  9899 net.cpp:224] out3a/bn needs backward computation.
I0701 00:17:46.958029  9899 net.cpp:224] out3a needs backward computation.
I0701 00:17:46.958034  9899 net.cpp:224] out5a_up2 needs backward computation.
I0701 00:17:46.958037  9899 net.cpp:224] out5a/relu needs backward computation.
I0701 00:17:46.958041  9899 net.cpp:224] out5a/bn needs backward computation.
I0701 00:17:46.958045  9899 net.cpp:224] out5a needs backward computation.
I0701 00:17:46.958050  9899 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0701 00:17:46.958053  9899 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0701 00:17:46.958057  9899 net.cpp:224] res5a_branch2b needs backward computation.
I0701 00:17:46.958061  9899 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0701 00:17:46.958065  9899 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0701 00:17:46.958068  9899 net.cpp:224] res5a_branch2a needs backward computation.
I0701 00:17:46.958072  9899 net.cpp:224] pool4 needs backward computation.
I0701 00:17:46.958077  9899 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0701 00:17:46.958081  9899 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0701 00:17:46.958086  9899 net.cpp:224] res4a_branch2b needs backward computation.
I0701 00:17:46.958089  9899 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0701 00:17:46.958092  9899 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0701 00:17:46.958096  9899 net.cpp:224] res4a_branch2a needs backward computation.
I0701 00:17:46.958101  9899 net.cpp:224] pool3 needs backward computation.
I0701 00:17:46.958106  9899 net.cpp:224] res3a_branch2b/bn_res3a_branch2b/relu_0_split needs backward computation.
I0701 00:17:46.958109  9899 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0701 00:17:46.958112  9899 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0701 00:17:46.958117  9899 net.cpp:224] res3a_branch2b needs backward computation.
I0701 00:17:46.958120  9899 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0701 00:17:46.958124  9899 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0701 00:17:46.958128  9899 net.cpp:224] res3a_branch2a needs backward computation.
I0701 00:17:46.958135  9899 net.cpp:224] pool2 needs backward computation.
I0701 00:17:46.958139  9899 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0701 00:17:46.958144  9899 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0701 00:17:46.958148  9899 net.cpp:224] res2a_branch2b needs backward computation.
I0701 00:17:46.958151  9899 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0701 00:17:46.958155  9899 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0701 00:17:46.958159  9899 net.cpp:224] res2a_branch2a needs backward computation.
I0701 00:17:46.958163  9899 net.cpp:224] pool1 needs backward computation.
I0701 00:17:46.958168  9899 net.cpp:224] conv1b/relu needs backward computation.
I0701 00:17:46.958171  9899 net.cpp:224] conv1b/bn needs backward computation.
I0701 00:17:46.958175  9899 net.cpp:224] conv1b needs backward computation.
I0701 00:17:46.958179  9899 net.cpp:224] conv1a/relu needs backward computation.
I0701 00:17:46.958184  9899 net.cpp:224] conv1a/bn needs backward computation.
I0701 00:17:46.958186  9899 net.cpp:224] conv1a needs backward computation.
I0701 00:17:46.958191  9899 net.cpp:226] data/bias does not need backward computation.
I0701 00:17:46.958196  9899 net.cpp:226] label_data_1_split does not need backward computation.
I0701 00:17:46.958200  9899 net.cpp:226] data does not need backward computation.
I0701 00:17:46.958204  9899 net.cpp:268] This network produces output accuracy/top1
I0701 00:17:46.958207  9899 net.cpp:268] This network produces output accuracy/top5
I0701 00:17:46.958211  9899 net.cpp:268] This network produces output loss
I0701 00:17:46.958242  9899 net.cpp:288] Network initialization done.
I0701 00:17:46.969144  9899 caffe.cpp:289] Running for 50 iterations.
I0701 00:17:47.782443  9899 caffe.cpp:312] Batch 0, accuracy/top1 = 0.918487
I0701 00:17:47.782471  9899 caffe.cpp:312] Batch 0, accuracy/top5 = 0.991621
I0701 00:17:47.782476  9899 caffe.cpp:312] Batch 0, loss = 0.161182
I0701 00:17:48.541229  9899 caffe.cpp:312] Batch 1, accuracy/top1 = 0.8981
I0701 00:17:48.541256  9899 caffe.cpp:312] Batch 1, accuracy/top5 = 0.990504
I0701 00:17:48.541260  9899 caffe.cpp:312] Batch 1, loss = 0.271098
I0701 00:17:49.321869  9899 caffe.cpp:312] Batch 2, accuracy/top1 = 0.883237
I0701 00:17:49.321897  9899 caffe.cpp:312] Batch 2, accuracy/top5 = 0.992708
I0701 00:17:49.321902  9899 caffe.cpp:312] Batch 2, loss = 0.203529
I0701 00:17:50.061295  9899 caffe.cpp:312] Batch 3, accuracy/top1 = 0.916812
I0701 00:17:50.061321  9899 caffe.cpp:312] Batch 3, accuracy/top5 = 0.992509
I0701 00:17:50.061326  9899 caffe.cpp:312] Batch 3, loss = 0.159322
I0701 00:17:50.811486  9899 caffe.cpp:312] Batch 4, accuracy/top1 = 0.911028
I0701 00:17:50.811513  9899 caffe.cpp:312] Batch 4, accuracy/top5 = 0.99175
I0701 00:17:50.811519  9899 caffe.cpp:312] Batch 4, loss = 0.220506
I0701 00:17:51.396680  9899 caffe.cpp:312] Batch 5, accuracy/top1 = 0.923971
I0701 00:17:51.396706  9899 caffe.cpp:312] Batch 5, accuracy/top5 = 0.99707
I0701 00:17:51.396710  9899 caffe.cpp:312] Batch 5, loss = 0.193563
I0701 00:17:52.165575  9899 caffe.cpp:312] Batch 6, accuracy/top1 = 0.926876
I0701 00:17:52.165596  9899 caffe.cpp:312] Batch 6, accuracy/top5 = 0.996671
I0701 00:17:52.165599  9899 caffe.cpp:312] Batch 6, loss = 0.130481
I0701 00:17:52.929172  9899 caffe.cpp:312] Batch 7, accuracy/top1 = 0.931263
I0701 00:17:52.929198  9899 caffe.cpp:312] Batch 7, accuracy/top5 = 0.997341
I0701 00:17:52.929201  9899 caffe.cpp:312] Batch 7, loss = 0.145955
I0701 00:17:53.694983  9899 caffe.cpp:312] Batch 8, accuracy/top1 = 0.943895
I0701 00:17:53.695008  9899 caffe.cpp:312] Batch 8, accuracy/top5 = 0.996064
I0701 00:17:53.695011  9899 caffe.cpp:312] Batch 8, loss = 0.101973
I0701 00:17:54.457870  9899 caffe.cpp:312] Batch 9, accuracy/top1 = 0.95246
I0701 00:17:54.457897  9899 caffe.cpp:312] Batch 9, accuracy/top5 = 0.997475
I0701 00:17:54.457902  9899 caffe.cpp:312] Batch 9, loss = 0.0891257
I0701 00:17:55.234784  9899 caffe.cpp:312] Batch 10, accuracy/top1 = 0.945588
I0701 00:17:55.234827  9899 caffe.cpp:312] Batch 10, accuracy/top5 = 0.994037
I0701 00:17:55.234832  9899 caffe.cpp:312] Batch 10, loss = 0.110514
I0701 00:17:56.005313  9899 caffe.cpp:312] Batch 11, accuracy/top1 = 0.941774
I0701 00:17:56.005338  9899 caffe.cpp:312] Batch 11, accuracy/top5 = 0.993535
I0701 00:17:56.005342  9899 caffe.cpp:312] Batch 11, loss = 0.118555
I0701 00:17:56.754138  9899 caffe.cpp:312] Batch 12, accuracy/top1 = 0.919249
I0701 00:17:56.754164  9899 caffe.cpp:312] Batch 12, accuracy/top5 = 0.991955
I0701 00:17:56.754168  9899 caffe.cpp:312] Batch 12, loss = 0.168413
I0701 00:17:57.514235  9899 caffe.cpp:312] Batch 13, accuracy/top1 = 0.945576
I0701 00:17:57.514256  9899 caffe.cpp:312] Batch 13, accuracy/top5 = 0.998279
I0701 00:17:57.514259  9899 caffe.cpp:312] Batch 13, loss = 0.092224
I0701 00:17:58.287880  9899 caffe.cpp:312] Batch 14, accuracy/top1 = 0.953765
I0701 00:17:58.287902  9899 caffe.cpp:312] Batch 14, accuracy/top5 = 0.997621
I0701 00:17:58.287906  9899 caffe.cpp:312] Batch 14, loss = 0.077303
I0701 00:17:59.049310  9899 caffe.cpp:312] Batch 15, accuracy/top1 = 0.912928
I0701 00:17:59.049334  9899 caffe.cpp:312] Batch 15, accuracy/top5 = 0.990188
I0701 00:17:59.049337  9899 caffe.cpp:312] Batch 15, loss = 0.185553
I0701 00:17:59.732883  9899 caffe.cpp:312] Batch 16, accuracy/top1 = 0.937204
I0701 00:17:59.732905  9899 caffe.cpp:312] Batch 16, accuracy/top5 = 0.994914
I0701 00:17:59.732909  9899 caffe.cpp:312] Batch 16, loss = 0.131999
I0701 00:18:00.376860  9899 caffe.cpp:312] Batch 17, accuracy/top1 = 0.943746
I0701 00:18:00.376885  9899 caffe.cpp:312] Batch 17, accuracy/top5 = 0.990367
I0701 00:18:00.376889  9899 caffe.cpp:312] Batch 17, loss = 0.125548
I0701 00:18:01.151095  9899 caffe.cpp:312] Batch 18, accuracy/top1 = 0.933723
I0701 00:18:01.151119  9899 caffe.cpp:312] Batch 18, accuracy/top5 = 0.996805
I0701 00:18:01.151124  9899 caffe.cpp:312] Batch 18, loss = 0.122954
I0701 00:18:01.921828  9899 caffe.cpp:312] Batch 19, accuracy/top1 = 0.955824
I0701 00:18:01.921854  9899 caffe.cpp:312] Batch 19, accuracy/top5 = 0.999114
I0701 00:18:01.921859  9899 caffe.cpp:312] Batch 19, loss = 0.0698131
I0701 00:18:02.697042  9899 caffe.cpp:312] Batch 20, accuracy/top1 = 0.913164
I0701 00:18:02.697067  9899 caffe.cpp:312] Batch 20, accuracy/top5 = 0.991645
I0701 00:18:02.697072  9899 caffe.cpp:312] Batch 20, loss = 0.172564
I0701 00:18:03.348562  9899 caffe.cpp:312] Batch 21, accuracy/top1 = 0.952254
I0701 00:18:03.348588  9899 caffe.cpp:312] Batch 21, accuracy/top5 = 0.992047
I0701 00:18:03.348593  9899 caffe.cpp:312] Batch 21, loss = 0.110627
I0701 00:18:04.123811  9899 caffe.cpp:312] Batch 22, accuracy/top1 = 0.942945
I0701 00:18:04.123834  9899 caffe.cpp:312] Batch 22, accuracy/top5 = 0.995784
I0701 00:18:04.123838  9899 caffe.cpp:312] Batch 22, loss = 0.113229
I0701 00:18:04.883800  9899 caffe.cpp:312] Batch 23, accuracy/top1 = 0.933434
I0701 00:18:04.883826  9899 caffe.cpp:312] Batch 23, accuracy/top5 = 0.995057
I0701 00:18:04.883831  9899 caffe.cpp:312] Batch 23, loss = 0.13276
I0701 00:18:05.639920  9899 caffe.cpp:312] Batch 24, accuracy/top1 = 0.919657
I0701 00:18:05.639943  9899 caffe.cpp:312] Batch 24, accuracy/top5 = 0.992948
I0701 00:18:05.639947  9899 caffe.cpp:312] Batch 24, loss = 0.166093
I0701 00:18:06.402531  9899 caffe.cpp:312] Batch 25, accuracy/top1 = 0.949731
I0701 00:18:06.402552  9899 caffe.cpp:312] Batch 25, accuracy/top5 = 0.994343
I0701 00:18:06.402556  9899 caffe.cpp:312] Batch 25, loss = 0.111027
I0701 00:18:07.155968  9899 caffe.cpp:312] Batch 26, accuracy/top1 = 0.896999
I0701 00:18:07.155993  9899 caffe.cpp:312] Batch 26, accuracy/top5 = 0.996817
I0701 00:18:07.155998  9899 caffe.cpp:312] Batch 26, loss = 0.199358
I0701 00:18:07.909714  9899 caffe.cpp:312] Batch 27, accuracy/top1 = 0.917776
I0701 00:18:07.909739  9899 caffe.cpp:312] Batch 27, accuracy/top5 = 0.987426
I0701 00:18:07.909744  9899 caffe.cpp:312] Batch 27, loss = 0.181024
I0701 00:18:08.679848  9899 caffe.cpp:312] Batch 28, accuracy/top1 = 0.897638
I0701 00:18:08.679872  9899 caffe.cpp:312] Batch 28, accuracy/top5 = 0.992359
I0701 00:18:08.679893  9899 caffe.cpp:312] Batch 28, loss = 0.232927
I0701 00:18:09.444214  9899 caffe.cpp:312] Batch 29, accuracy/top1 = 0.922988
I0701 00:18:09.444242  9899 caffe.cpp:312] Batch 29, accuracy/top5 = 0.985846
I0701 00:18:09.444245  9899 caffe.cpp:312] Batch 29, loss = 0.209912
I0701 00:18:10.130787  9899 caffe.cpp:312] Batch 30, accuracy/top1 = 0.93561
I0701 00:18:10.130812  9899 caffe.cpp:312] Batch 30, accuracy/top5 = 0.996984
I0701 00:18:10.130816  9899 caffe.cpp:312] Batch 30, loss = 0.118794
I0701 00:18:10.872257  9899 caffe.cpp:312] Batch 31, accuracy/top1 = 0.927028
I0701 00:18:10.872283  9899 caffe.cpp:312] Batch 31, accuracy/top5 = 0.991582
I0701 00:18:10.872288  9899 caffe.cpp:312] Batch 31, loss = 0.16961
I0701 00:18:11.628871  9899 caffe.cpp:312] Batch 32, accuracy/top1 = 0.926818
I0701 00:18:11.628893  9899 caffe.cpp:312] Batch 32, accuracy/top5 = 0.995769
I0701 00:18:11.628897  9899 caffe.cpp:312] Batch 32, loss = 0.129326
I0701 00:18:12.395488  9899 caffe.cpp:312] Batch 33, accuracy/top1 = 0.940952
I0701 00:18:12.395509  9899 caffe.cpp:312] Batch 33, accuracy/top5 = 0.995814
I0701 00:18:12.395512  9899 caffe.cpp:312] Batch 33, loss = 0.111134
I0701 00:18:13.169623  9899 caffe.cpp:312] Batch 34, accuracy/top1 = 0.939445
I0701 00:18:13.169649  9899 caffe.cpp:312] Batch 34, accuracy/top5 = 0.993273
I0701 00:18:13.169653  9899 caffe.cpp:312] Batch 34, loss = 0.144633
I0701 00:18:13.942783  9899 caffe.cpp:312] Batch 35, accuracy/top1 = 0.930186
I0701 00:18:13.942811  9899 caffe.cpp:312] Batch 35, accuracy/top5 = 0.996247
I0701 00:18:13.942814  9899 caffe.cpp:312] Batch 35, loss = 0.130125
I0701 00:18:14.679515  9899 caffe.cpp:312] Batch 36, accuracy/top1 = 0.904185
I0701 00:18:14.679540  9899 caffe.cpp:312] Batch 36, accuracy/top5 = 0.98706
I0701 00:18:14.679544  9899 caffe.cpp:312] Batch 36, loss = 0.258469
I0701 00:18:15.456668  9899 caffe.cpp:312] Batch 37, accuracy/top1 = 0.904443
I0701 00:18:15.456727  9899 caffe.cpp:312] Batch 37, accuracy/top5 = 0.993036
I0701 00:18:15.456732  9899 caffe.cpp:312] Batch 37, loss = 0.188421
I0701 00:18:16.219326  9899 caffe.cpp:312] Batch 38, accuracy/top1 = 0.918621
I0701 00:18:16.219352  9899 caffe.cpp:312] Batch 38, accuracy/top5 = 0.990972
I0701 00:18:16.219355  9899 caffe.cpp:312] Batch 38, loss = 0.198106
I0701 00:18:16.965270  9899 caffe.cpp:312] Batch 39, accuracy/top1 = 0.907998
I0701 00:18:16.965296  9899 caffe.cpp:312] Batch 39, accuracy/top5 = 0.991862
I0701 00:18:16.965299  9899 caffe.cpp:312] Batch 39, loss = 0.180723
I0701 00:18:17.728529  9899 caffe.cpp:312] Batch 40, accuracy/top1 = 0.927043
I0701 00:18:17.728555  9899 caffe.cpp:312] Batch 40, accuracy/top5 = 0.992216
I0701 00:18:17.728559  9899 caffe.cpp:312] Batch 40, loss = 0.152103
I0701 00:18:18.490648  9899 caffe.cpp:312] Batch 41, accuracy/top1 = 0.921441
I0701 00:18:18.490674  9899 caffe.cpp:312] Batch 41, accuracy/top5 = 0.995943
I0701 00:18:18.490679  9899 caffe.cpp:312] Batch 41, loss = 0.128036
I0701 00:18:19.272161  9899 caffe.cpp:312] Batch 42, accuracy/top1 = 0.93291
I0701 00:18:19.272188  9899 caffe.cpp:312] Batch 42, accuracy/top5 = 0.996992
I0701 00:18:19.272192  9899 caffe.cpp:312] Batch 42, loss = 0.119998
I0701 00:18:20.036847  9899 caffe.cpp:312] Batch 43, accuracy/top1 = 0.950054
I0701 00:18:20.036870  9899 caffe.cpp:312] Batch 43, accuracy/top5 = 0.997901
I0701 00:18:20.036875  9899 caffe.cpp:312] Batch 43, loss = 0.0774627
I0701 00:18:20.800648  9899 caffe.cpp:312] Batch 44, accuracy/top1 = 0.920683
I0701 00:18:20.800674  9899 caffe.cpp:312] Batch 44, accuracy/top5 = 0.986313
I0701 00:18:20.800678  9899 caffe.cpp:312] Batch 44, loss = 0.194521
I0701 00:18:21.554644  9899 caffe.cpp:312] Batch 45, accuracy/top1 = 0.935109
I0701 00:18:21.554669  9899 caffe.cpp:312] Batch 45, accuracy/top5 = 0.990977
I0701 00:18:21.554673  9899 caffe.cpp:312] Batch 45, loss = 0.149178
I0701 00:18:22.325197  9899 caffe.cpp:312] Batch 46, accuracy/top1 = 0.928223
I0701 00:18:22.325222  9899 caffe.cpp:312] Batch 46, accuracy/top5 = 0.996321
I0701 00:18:22.325227  9899 caffe.cpp:312] Batch 46, loss = 0.135202
I0701 00:18:23.094313  9899 caffe.cpp:312] Batch 47, accuracy/top1 = 0.947517
I0701 00:18:23.094336  9899 caffe.cpp:312] Batch 47, accuracy/top5 = 0.995586
I0701 00:18:23.094341  9899 caffe.cpp:312] Batch 47, loss = 0.104496
I0701 00:18:23.769681  9899 caffe.cpp:312] Batch 48, accuracy/top1 = 0.892747
I0701 00:18:23.769709  9899 caffe.cpp:312] Batch 48, accuracy/top5 = 0.997442
I0701 00:18:23.769713  9899 caffe.cpp:312] Batch 48, loss = 0.217535
I0701 00:18:24.540637  9899 caffe.cpp:312] Batch 49, accuracy/top1 = 0.88619
I0701 00:18:24.540663  9899 caffe.cpp:312] Batch 49, accuracy/top5 = 0.986018
I0701 00:18:24.540668  9899 caffe.cpp:312] Batch 49, loss = 0.255068
I0701 00:18:24.540670  9899 caffe.cpp:317] Loss: 0.153441
I0701 00:18:24.540679  9899 caffe.cpp:329] accuracy/top1 = 0.926386
I0701 00:18:24.540686  9899 caffe.cpp:329] accuracy/top5 = 0.993662
I0701 00:18:24.540694  9899 caffe.cpp:329] loss = 0.153441 (* 1 = 0.153441 loss)
