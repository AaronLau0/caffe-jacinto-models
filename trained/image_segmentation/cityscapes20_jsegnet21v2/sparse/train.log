I0630 22:02:31.435077  6183 caffe.cpp:209] Using GPUs 0, 1, 2
I0630 22:02:31.435549  6183 caffe.cpp:214] GPU 0: GeForce GTX 1080
I0630 22:02:31.435892  6183 caffe.cpp:214] GPU 1: GeForce GTX 1080
I0630 22:02:31.436233  6183 caffe.cpp:214] GPU 2: GeForce GTX 1080
I0630 22:02:32.264278  6183 solver.cpp:48] Initializing solver from parameters: 
train_net: "training/cityscapes20_jsegnet21v2_2017-06-30_19-26-17/sparse/train.prototxt"
test_net: "training/cityscapes20_jsegnet21v2_2017-06-30_19-26-17/sparse/test.prototxt"
test_iter: 125
test_interval: 2000
base_lr: 1e-05
display: 100
max_iter: 32000
lr_policy: "multistep"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 0.0001
snapshot: 10000
snapshot_prefix: "training/cityscapes20_jsegnet21v2_2017-06-30_19-26-17/sparse/cityscapes20_jsegnet21v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
test_initialization: false
stepvalue: 24000
iter_size: 1
type: "Adam"
display_sparsity: 1000
sparse_mode: SPARSE_UPDATE
sparsity_target: 0.8
sparsity_step_factor: 0.05
sparsity_step_iter: 1000
sparsity_start_iter: 0
sparsity_start_factor: 0
I0630 22:02:32.264370  6183 solver.cpp:82] Creating training net from train_net file: training/cityscapes20_jsegnet21v2_2017-06-30_19-26-17/sparse/train.prototxt
I0630 22:02:32.280292  6183 net.cpp:327] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0630 22:02:32.280336  6183 net.cpp:327] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
I0630 22:02:32.281893  6183 net.cpp:56] Initializing net from parameters: 
name: "jsegnet21v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/train-image-lmdb"
    label_list_path: "data/train-label-lmdb"
    batch_size: 5
    threads: 4
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b/bn"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a/bn"
  top: "out5a/bn"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a/bn"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b/bn"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a/bn"
  top: "out3a/bn"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a/bn"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv1/bn"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv2/bn"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv3/bn"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4/bn"
  top: "ctx_conv4/bn"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4/bn"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 20
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 20
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 20
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
I0630 22:02:32.289773  6183 layer_factory.hpp:77] Creating layer data
I0630 22:02:32.289852  6183 net.cpp:98] Creating Layer data
I0630 22:02:32.289880  6183 net.cpp:413] data -> data
I0630 22:02:32.289980  6183 net.cpp:413] data -> label
I0630 22:02:32.331529  6266 db_lmdb.cpp:35] Opened lmdb data/train-image-lmdb
I0630 22:02:32.341922  6271 db_lmdb.cpp:35] Opened lmdb data/train-label-lmdb
I0630 22:02:32.352357  6183 data_layer.cpp:78] ReshapePrefetch 5, 3, 640, 640
I0630 22:02:32.352638  6183 data_layer.cpp:83] output data size: 5,3,640,640
I0630 22:02:32.401856  6183 data_layer.cpp:78] ReshapePrefetch 5, 1, 640, 640
I0630 22:02:32.401929  6183 data_layer.cpp:83] output data size: 5,1,640,640
I0630 22:02:32.408550  6276 blocking_queue.cpp:50] Waiting for data
I0630 22:02:32.414536  6183 net.cpp:148] Setting up data
I0630 22:02:32.414557  6183 net.cpp:155] Top shape: 5 3 640 640 (6144000)
I0630 22:02:32.414561  6183 net.cpp:155] Top shape: 5 1 640 640 (2048000)
I0630 22:02:32.414562  6183 net.cpp:163] Memory required for data: 32768000
I0630 22:02:32.414569  6183 layer_factory.hpp:77] Creating layer data/bias
I0630 22:02:32.414578  6183 net.cpp:98] Creating Layer data/bias
I0630 22:02:32.414582  6183 net.cpp:439] data/bias <- data
I0630 22:02:32.414590  6183 net.cpp:413] data/bias -> data/bias
I0630 22:02:32.415817  6183 net.cpp:148] Setting up data/bias
I0630 22:02:32.415827  6183 net.cpp:155] Top shape: 5 3 640 640 (6144000)
I0630 22:02:32.415829  6183 net.cpp:163] Memory required for data: 57344000
I0630 22:02:32.415838  6183 layer_factory.hpp:77] Creating layer conv1a
I0630 22:02:32.415848  6183 net.cpp:98] Creating Layer conv1a
I0630 22:02:32.415851  6183 net.cpp:439] conv1a <- data/bias
I0630 22:02:32.415855  6183 net.cpp:413] conv1a -> conv1a
I0630 22:02:32.417227  6183 net.cpp:148] Setting up conv1a
I0630 22:02:32.417237  6183 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0630 22:02:32.417240  6183 net.cpp:163] Memory required for data: 122880000
I0630 22:02:32.417245  6183 layer_factory.hpp:77] Creating layer conv1a/bn
I0630 22:02:32.417255  6183 net.cpp:98] Creating Layer conv1a/bn
I0630 22:02:32.417258  6183 net.cpp:439] conv1a/bn <- conv1a
I0630 22:02:32.417263  6183 net.cpp:413] conv1a/bn -> conv1a/bn
I0630 22:02:32.419606  6183 net.cpp:148] Setting up conv1a/bn
I0630 22:02:32.419620  6183 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0630 22:02:32.419622  6183 net.cpp:163] Memory required for data: 188416000
I0630 22:02:32.419631  6183 layer_factory.hpp:77] Creating layer conv1a/relu
I0630 22:02:32.419636  6183 net.cpp:98] Creating Layer conv1a/relu
I0630 22:02:32.419638  6183 net.cpp:439] conv1a/relu <- conv1a/bn
I0630 22:02:32.419641  6183 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0630 22:02:32.419656  6183 net.cpp:148] Setting up conv1a/relu
I0630 22:02:32.419659  6183 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0630 22:02:32.419661  6183 net.cpp:163] Memory required for data: 253952000
I0630 22:02:32.419664  6183 layer_factory.hpp:77] Creating layer conv1b
I0630 22:02:32.419669  6183 net.cpp:98] Creating Layer conv1b
I0630 22:02:32.419682  6183 net.cpp:439] conv1b <- conv1a/bn
I0630 22:02:32.419685  6183 net.cpp:413] conv1b -> conv1b
I0630 22:02:32.420146  6183 net.cpp:148] Setting up conv1b
I0630 22:02:32.420153  6183 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0630 22:02:32.420156  6183 net.cpp:163] Memory required for data: 319488000
I0630 22:02:32.420161  6183 layer_factory.hpp:77] Creating layer conv1b/bn
I0630 22:02:32.420164  6183 net.cpp:98] Creating Layer conv1b/bn
I0630 22:02:32.420166  6183 net.cpp:439] conv1b/bn <- conv1b
I0630 22:02:32.420171  6183 net.cpp:413] conv1b/bn -> conv1b/bn
I0630 22:02:32.420917  6183 net.cpp:148] Setting up conv1b/bn
I0630 22:02:32.420924  6183 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0630 22:02:32.420928  6183 net.cpp:163] Memory required for data: 385024000
I0630 22:02:32.420933  6183 layer_factory.hpp:77] Creating layer conv1b/relu
I0630 22:02:32.420935  6183 net.cpp:98] Creating Layer conv1b/relu
I0630 22:02:32.420938  6183 net.cpp:439] conv1b/relu <- conv1b/bn
I0630 22:02:32.420939  6183 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0630 22:02:32.420943  6183 net.cpp:148] Setting up conv1b/relu
I0630 22:02:32.420945  6183 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0630 22:02:32.420948  6183 net.cpp:163] Memory required for data: 450560000
I0630 22:02:32.420949  6183 layer_factory.hpp:77] Creating layer pool1
I0630 22:02:32.420958  6183 net.cpp:98] Creating Layer pool1
I0630 22:02:32.420961  6183 net.cpp:439] pool1 <- conv1b/bn
I0630 22:02:32.420964  6183 net.cpp:413] pool1 -> pool1
I0630 22:02:32.421375  6183 net.cpp:148] Setting up pool1
I0630 22:02:32.421382  6183 net.cpp:155] Top shape: 5 32 160 160 (4096000)
I0630 22:02:32.421386  6183 net.cpp:163] Memory required for data: 466944000
I0630 22:02:32.421387  6183 layer_factory.hpp:77] Creating layer res2a_branch2a
I0630 22:02:32.421392  6183 net.cpp:98] Creating Layer res2a_branch2a
I0630 22:02:32.421394  6183 net.cpp:439] res2a_branch2a <- pool1
I0630 22:02:32.421397  6183 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0630 22:02:32.422894  6183 net.cpp:148] Setting up res2a_branch2a
I0630 22:02:32.422904  6183 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0630 22:02:32.422905  6183 net.cpp:163] Memory required for data: 499712000
I0630 22:02:32.422910  6183 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0630 22:02:32.422914  6183 net.cpp:98] Creating Layer res2a_branch2a/bn
I0630 22:02:32.422916  6183 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0630 22:02:32.422919  6183 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0630 22:02:32.423575  6183 net.cpp:148] Setting up res2a_branch2a/bn
I0630 22:02:32.423581  6183 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0630 22:02:32.423583  6183 net.cpp:163] Memory required for data: 532480000
I0630 22:02:32.423588  6183 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0630 22:02:32.423591  6183 net.cpp:98] Creating Layer res2a_branch2a/relu
I0630 22:02:32.423593  6183 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0630 22:02:32.423595  6183 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0630 22:02:32.423599  6183 net.cpp:148] Setting up res2a_branch2a/relu
I0630 22:02:32.423601  6183 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0630 22:02:32.423604  6183 net.cpp:163] Memory required for data: 565248000
I0630 22:02:32.423605  6183 layer_factory.hpp:77] Creating layer res2a_branch2b
I0630 22:02:32.423609  6183 net.cpp:98] Creating Layer res2a_branch2b
I0630 22:02:32.423610  6183 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0630 22:02:32.423614  6183 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0630 22:02:32.424897  6183 net.cpp:148] Setting up res2a_branch2b
I0630 22:02:32.424906  6183 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0630 22:02:32.424907  6183 net.cpp:163] Memory required for data: 598016000
I0630 22:02:32.424911  6183 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0630 22:02:32.424916  6183 net.cpp:98] Creating Layer res2a_branch2b/bn
I0630 22:02:32.424924  6183 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0630 22:02:32.424928  6183 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0630 22:02:32.425596  6183 net.cpp:148] Setting up res2a_branch2b/bn
I0630 22:02:32.425602  6183 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0630 22:02:32.425604  6183 net.cpp:163] Memory required for data: 630784000
I0630 22:02:32.425609  6183 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0630 22:02:32.425612  6183 net.cpp:98] Creating Layer res2a_branch2b/relu
I0630 22:02:32.425614  6183 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0630 22:02:32.425616  6183 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0630 22:02:32.425622  6183 net.cpp:148] Setting up res2a_branch2b/relu
I0630 22:02:32.425626  6183 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0630 22:02:32.425626  6183 net.cpp:163] Memory required for data: 663552000
I0630 22:02:32.425629  6183 layer_factory.hpp:77] Creating layer pool2
I0630 22:02:32.425632  6183 net.cpp:98] Creating Layer pool2
I0630 22:02:32.425634  6183 net.cpp:439] pool2 <- res2a_branch2b/bn
I0630 22:02:32.425637  6183 net.cpp:413] pool2 -> pool2
I0630 22:02:32.425670  6183 net.cpp:148] Setting up pool2
I0630 22:02:32.425674  6183 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 22:02:32.425676  6183 net.cpp:163] Memory required for data: 671744000
I0630 22:02:32.425678  6183 layer_factory.hpp:77] Creating layer res3a_branch2a
I0630 22:02:32.425683  6183 net.cpp:98] Creating Layer res3a_branch2a
I0630 22:02:32.425684  6183 net.cpp:439] res3a_branch2a <- pool2
I0630 22:02:32.425688  6183 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0630 22:02:32.427408  6183 net.cpp:148] Setting up res3a_branch2a
I0630 22:02:32.427414  6183 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0630 22:02:32.427417  6183 net.cpp:163] Memory required for data: 688128000
I0630 22:02:32.427419  6183 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0630 22:02:32.427423  6183 net.cpp:98] Creating Layer res3a_branch2a/bn
I0630 22:02:32.427426  6183 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0630 22:02:32.427428  6183 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0630 22:02:32.428021  6183 net.cpp:148] Setting up res3a_branch2a/bn
I0630 22:02:32.428026  6183 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0630 22:02:32.428028  6183 net.cpp:163] Memory required for data: 704512000
I0630 22:02:32.428035  6183 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0630 22:02:32.428037  6183 net.cpp:98] Creating Layer res3a_branch2a/relu
I0630 22:02:32.428040  6183 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0630 22:02:32.428042  6183 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0630 22:02:32.428045  6183 net.cpp:148] Setting up res3a_branch2a/relu
I0630 22:02:32.428047  6183 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0630 22:02:32.428050  6183 net.cpp:163] Memory required for data: 720896000
I0630 22:02:32.428051  6183 layer_factory.hpp:77] Creating layer res3a_branch2b
I0630 22:02:32.428056  6183 net.cpp:98] Creating Layer res3a_branch2b
I0630 22:02:32.428057  6183 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0630 22:02:32.428061  6183 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0630 22:02:32.429150  6183 net.cpp:148] Setting up res3a_branch2b
I0630 22:02:32.429159  6183 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0630 22:02:32.429162  6183 net.cpp:163] Memory required for data: 737280000
I0630 22:02:32.429168  6183 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0630 22:02:32.429174  6183 net.cpp:98] Creating Layer res3a_branch2b/bn
I0630 22:02:32.429177  6183 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0630 22:02:32.429181  6183 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0630 22:02:32.429862  6183 net.cpp:148] Setting up res3a_branch2b/bn
I0630 22:02:32.429869  6183 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0630 22:02:32.429872  6183 net.cpp:163] Memory required for data: 753664000
I0630 22:02:32.429877  6183 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0630 22:02:32.429885  6183 net.cpp:98] Creating Layer res3a_branch2b/relu
I0630 22:02:32.429888  6183 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0630 22:02:32.429890  6183 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0630 22:02:32.429894  6183 net.cpp:148] Setting up res3a_branch2b/relu
I0630 22:02:32.429896  6183 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0630 22:02:32.429898  6183 net.cpp:163] Memory required for data: 770048000
I0630 22:02:32.429900  6183 layer_factory.hpp:77] Creating layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0630 22:02:32.429903  6183 net.cpp:98] Creating Layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0630 22:02:32.429905  6183 net.cpp:439] res3a_branch2b/bn_res3a_branch2b/relu_0_split <- res3a_branch2b/bn
I0630 22:02:32.429908  6183 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0630 22:02:32.429911  6183 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0630 22:02:32.429945  6183 net.cpp:148] Setting up res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0630 22:02:32.429949  6183 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0630 22:02:32.429951  6183 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0630 22:02:32.429953  6183 net.cpp:163] Memory required for data: 802816000
I0630 22:02:32.429955  6183 layer_factory.hpp:77] Creating layer pool3
I0630 22:02:32.429958  6183 net.cpp:98] Creating Layer pool3
I0630 22:02:32.429960  6183 net.cpp:439] pool3 <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0630 22:02:32.429962  6183 net.cpp:413] pool3 -> pool3
I0630 22:02:32.429997  6183 net.cpp:148] Setting up pool3
I0630 22:02:32.430001  6183 net.cpp:155] Top shape: 5 128 40 40 (1024000)
I0630 22:02:32.430003  6183 net.cpp:163] Memory required for data: 806912000
I0630 22:02:32.430006  6183 layer_factory.hpp:77] Creating layer res4a_branch2a
I0630 22:02:32.430008  6183 net.cpp:98] Creating Layer res4a_branch2a
I0630 22:02:32.430011  6183 net.cpp:439] res4a_branch2a <- pool3
I0630 22:02:32.430013  6183 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0630 22:02:32.437062  6183 net.cpp:148] Setting up res4a_branch2a
I0630 22:02:32.437072  6183 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0630 22:02:32.437074  6183 net.cpp:163] Memory required for data: 815104000
I0630 22:02:32.437078  6183 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0630 22:02:32.437084  6183 net.cpp:98] Creating Layer res4a_branch2a/bn
I0630 22:02:32.437088  6183 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0630 22:02:32.437090  6183 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0630 22:02:32.437706  6183 net.cpp:148] Setting up res4a_branch2a/bn
I0630 22:02:32.437712  6183 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0630 22:02:32.437714  6183 net.cpp:163] Memory required for data: 823296000
I0630 22:02:32.437719  6183 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0630 22:02:32.437722  6183 net.cpp:98] Creating Layer res4a_branch2a/relu
I0630 22:02:32.437724  6183 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0630 22:02:32.437726  6183 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0630 22:02:32.437729  6183 net.cpp:148] Setting up res4a_branch2a/relu
I0630 22:02:32.437732  6183 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0630 22:02:32.437733  6183 net.cpp:163] Memory required for data: 831488000
I0630 22:02:32.437736  6183 layer_factory.hpp:77] Creating layer res4a_branch2b
I0630 22:02:32.437739  6183 net.cpp:98] Creating Layer res4a_branch2b
I0630 22:02:32.437741  6183 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0630 22:02:32.437744  6183 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0630 22:02:32.440902  6183 net.cpp:148] Setting up res4a_branch2b
I0630 22:02:32.440908  6183 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0630 22:02:32.440910  6183 net.cpp:163] Memory required for data: 839680000
I0630 22:02:32.440913  6183 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0630 22:02:32.440924  6183 net.cpp:98] Creating Layer res4a_branch2b/bn
I0630 22:02:32.440927  6183 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0630 22:02:32.440929  6183 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0630 22:02:32.441534  6183 net.cpp:148] Setting up res4a_branch2b/bn
I0630 22:02:32.441539  6183 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0630 22:02:32.441541  6183 net.cpp:163] Memory required for data: 847872000
I0630 22:02:32.441545  6183 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0630 22:02:32.441548  6183 net.cpp:98] Creating Layer res4a_branch2b/relu
I0630 22:02:32.441550  6183 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0630 22:02:32.441552  6183 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0630 22:02:32.441555  6183 net.cpp:148] Setting up res4a_branch2b/relu
I0630 22:02:32.441558  6183 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0630 22:02:32.441560  6183 net.cpp:163] Memory required for data: 856064000
I0630 22:02:32.441561  6183 layer_factory.hpp:77] Creating layer pool4
I0630 22:02:32.441565  6183 net.cpp:98] Creating Layer pool4
I0630 22:02:32.441566  6183 net.cpp:439] pool4 <- res4a_branch2b/bn
I0630 22:02:32.441570  6183 net.cpp:413] pool4 -> pool4
I0630 22:02:32.441603  6183 net.cpp:148] Setting up pool4
I0630 22:02:32.441607  6183 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0630 22:02:32.441609  6183 net.cpp:163] Memory required for data: 864256000
I0630 22:02:32.441612  6183 layer_factory.hpp:77] Creating layer res5a_branch2a
I0630 22:02:32.441617  6183 net.cpp:98] Creating Layer res5a_branch2a
I0630 22:02:32.441618  6183 net.cpp:439] res5a_branch2a <- pool4
I0630 22:02:32.441622  6183 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0630 22:02:32.467459  6183 net.cpp:148] Setting up res5a_branch2a
I0630 22:02:32.467479  6183 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0630 22:02:32.467481  6183 net.cpp:163] Memory required for data: 880640000
I0630 22:02:32.467488  6183 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0630 22:02:32.467499  6183 net.cpp:98] Creating Layer res5a_branch2a/bn
I0630 22:02:32.467501  6183 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0630 22:02:32.467505  6183 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0630 22:02:32.468149  6183 net.cpp:148] Setting up res5a_branch2a/bn
I0630 22:02:32.468155  6183 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0630 22:02:32.468158  6183 net.cpp:163] Memory required for data: 897024000
I0630 22:02:32.468163  6183 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0630 22:02:32.468166  6183 net.cpp:98] Creating Layer res5a_branch2a/relu
I0630 22:02:32.468168  6183 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0630 22:02:32.468170  6183 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0630 22:02:32.468174  6183 net.cpp:148] Setting up res5a_branch2a/relu
I0630 22:02:32.468178  6183 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0630 22:02:32.468179  6183 net.cpp:163] Memory required for data: 913408000
I0630 22:02:32.468180  6183 layer_factory.hpp:77] Creating layer res5a_branch2b
I0630 22:02:32.468185  6183 net.cpp:98] Creating Layer res5a_branch2b
I0630 22:02:32.468188  6183 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0630 22:02:32.468194  6183 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0630 22:02:32.481076  6183 net.cpp:148] Setting up res5a_branch2b
I0630 22:02:32.481086  6183 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0630 22:02:32.481088  6183 net.cpp:163] Memory required for data: 929792000
I0630 22:02:32.481094  6183 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0630 22:02:32.481099  6183 net.cpp:98] Creating Layer res5a_branch2b/bn
I0630 22:02:32.481101  6183 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0630 22:02:32.481104  6183 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0630 22:02:32.481736  6183 net.cpp:148] Setting up res5a_branch2b/bn
I0630 22:02:32.481741  6183 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0630 22:02:32.481752  6183 net.cpp:163] Memory required for data: 946176000
I0630 22:02:32.481758  6183 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0630 22:02:32.481761  6183 net.cpp:98] Creating Layer res5a_branch2b/relu
I0630 22:02:32.481763  6183 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0630 22:02:32.481765  6183 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0630 22:02:32.481770  6183 net.cpp:148] Setting up res5a_branch2b/relu
I0630 22:02:32.481772  6183 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0630 22:02:32.481773  6183 net.cpp:163] Memory required for data: 962560000
I0630 22:02:32.481775  6183 layer_factory.hpp:77] Creating layer out5a
I0630 22:02:32.481779  6183 net.cpp:98] Creating Layer out5a
I0630 22:02:32.481782  6183 net.cpp:439] out5a <- res5a_branch2b/bn
I0630 22:02:32.481786  6183 net.cpp:413] out5a -> out5a
I0630 22:02:32.485831  6183 net.cpp:148] Setting up out5a
I0630 22:02:32.485841  6183 net.cpp:155] Top shape: 5 64 40 40 (512000)
I0630 22:02:32.485842  6183 net.cpp:163] Memory required for data: 964608000
I0630 22:02:32.485846  6183 layer_factory.hpp:77] Creating layer out5a/bn
I0630 22:02:32.485851  6183 net.cpp:98] Creating Layer out5a/bn
I0630 22:02:32.485852  6183 net.cpp:439] out5a/bn <- out5a
I0630 22:02:32.485855  6183 net.cpp:413] out5a/bn -> out5a/bn
I0630 22:02:32.486553  6183 net.cpp:148] Setting up out5a/bn
I0630 22:02:32.486559  6183 net.cpp:155] Top shape: 5 64 40 40 (512000)
I0630 22:02:32.486562  6183 net.cpp:163] Memory required for data: 966656000
I0630 22:02:32.486567  6183 layer_factory.hpp:77] Creating layer out5a/relu
I0630 22:02:32.486570  6183 net.cpp:98] Creating Layer out5a/relu
I0630 22:02:32.486572  6183 net.cpp:439] out5a/relu <- out5a/bn
I0630 22:02:32.486574  6183 net.cpp:400] out5a/relu -> out5a/bn (in-place)
I0630 22:02:32.486578  6183 net.cpp:148] Setting up out5a/relu
I0630 22:02:32.486580  6183 net.cpp:155] Top shape: 5 64 40 40 (512000)
I0630 22:02:32.486582  6183 net.cpp:163] Memory required for data: 968704000
I0630 22:02:32.486584  6183 layer_factory.hpp:77] Creating layer out5a_up2
I0630 22:02:32.486593  6183 net.cpp:98] Creating Layer out5a_up2
I0630 22:02:32.486594  6183 net.cpp:439] out5a_up2 <- out5a/bn
I0630 22:02:32.486598  6183 net.cpp:413] out5a_up2 -> out5a_up2
I0630 22:02:32.486843  6183 net.cpp:148] Setting up out5a_up2
I0630 22:02:32.486850  6183 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 22:02:32.486851  6183 net.cpp:163] Memory required for data: 976896000
I0630 22:02:32.486853  6183 layer_factory.hpp:77] Creating layer out3a
I0630 22:02:32.486857  6183 net.cpp:98] Creating Layer out3a
I0630 22:02:32.486860  6183 net.cpp:439] out3a <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0630 22:02:32.486862  6183 net.cpp:413] out3a -> out3a
I0630 22:02:32.487880  6183 net.cpp:148] Setting up out3a
I0630 22:02:32.487886  6183 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 22:02:32.487888  6183 net.cpp:163] Memory required for data: 985088000
I0630 22:02:32.487891  6183 layer_factory.hpp:77] Creating layer out3a/bn
I0630 22:02:32.487895  6183 net.cpp:98] Creating Layer out3a/bn
I0630 22:02:32.487897  6183 net.cpp:439] out3a/bn <- out3a
I0630 22:02:32.487900  6183 net.cpp:413] out3a/bn -> out3a/bn
I0630 22:02:32.488595  6183 net.cpp:148] Setting up out3a/bn
I0630 22:02:32.488600  6183 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 22:02:32.488602  6183 net.cpp:163] Memory required for data: 993280000
I0630 22:02:32.488607  6183 layer_factory.hpp:77] Creating layer out3a/relu
I0630 22:02:32.488610  6183 net.cpp:98] Creating Layer out3a/relu
I0630 22:02:32.488612  6183 net.cpp:439] out3a/relu <- out3a/bn
I0630 22:02:32.488615  6183 net.cpp:400] out3a/relu -> out3a/bn (in-place)
I0630 22:02:32.488617  6183 net.cpp:148] Setting up out3a/relu
I0630 22:02:32.488620  6183 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 22:02:32.488621  6183 net.cpp:163] Memory required for data: 1001472000
I0630 22:02:32.488623  6183 layer_factory.hpp:77] Creating layer out3_out5_combined
I0630 22:02:32.488629  6183 net.cpp:98] Creating Layer out3_out5_combined
I0630 22:02:32.488641  6183 net.cpp:439] out3_out5_combined <- out5a_up2
I0630 22:02:32.488643  6183 net.cpp:439] out3_out5_combined <- out3a/bn
I0630 22:02:32.488646  6183 net.cpp:413] out3_out5_combined -> out3_out5_combined
I0630 22:02:32.488670  6183 net.cpp:148] Setting up out3_out5_combined
I0630 22:02:32.488674  6183 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 22:02:32.488677  6183 net.cpp:163] Memory required for data: 1009664000
I0630 22:02:32.488678  6183 layer_factory.hpp:77] Creating layer ctx_conv1
I0630 22:02:32.488682  6183 net.cpp:98] Creating Layer ctx_conv1
I0630 22:02:32.488683  6183 net.cpp:439] ctx_conv1 <- out3_out5_combined
I0630 22:02:32.488687  6183 net.cpp:413] ctx_conv1 -> ctx_conv1
I0630 22:02:32.489694  6183 net.cpp:148] Setting up ctx_conv1
I0630 22:02:32.489699  6183 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 22:02:32.489701  6183 net.cpp:163] Memory required for data: 1017856000
I0630 22:02:32.489704  6183 layer_factory.hpp:77] Creating layer ctx_conv1/bn
I0630 22:02:32.489707  6183 net.cpp:98] Creating Layer ctx_conv1/bn
I0630 22:02:32.489709  6183 net.cpp:439] ctx_conv1/bn <- ctx_conv1
I0630 22:02:32.489712  6183 net.cpp:413] ctx_conv1/bn -> ctx_conv1/bn
I0630 22:02:32.490404  6183 net.cpp:148] Setting up ctx_conv1/bn
I0630 22:02:32.490411  6183 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 22:02:32.490412  6183 net.cpp:163] Memory required for data: 1026048000
I0630 22:02:32.490417  6183 layer_factory.hpp:77] Creating layer ctx_conv1/relu
I0630 22:02:32.490420  6183 net.cpp:98] Creating Layer ctx_conv1/relu
I0630 22:02:32.490422  6183 net.cpp:439] ctx_conv1/relu <- ctx_conv1/bn
I0630 22:02:32.490425  6183 net.cpp:400] ctx_conv1/relu -> ctx_conv1/bn (in-place)
I0630 22:02:32.490428  6183 net.cpp:148] Setting up ctx_conv1/relu
I0630 22:02:32.490430  6183 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 22:02:32.490432  6183 net.cpp:163] Memory required for data: 1034240000
I0630 22:02:32.490434  6183 layer_factory.hpp:77] Creating layer ctx_conv2
I0630 22:02:32.490438  6183 net.cpp:98] Creating Layer ctx_conv2
I0630 22:02:32.490442  6183 net.cpp:439] ctx_conv2 <- ctx_conv1/bn
I0630 22:02:32.490443  6183 net.cpp:413] ctx_conv2 -> ctx_conv2
I0630 22:02:32.491449  6183 net.cpp:148] Setting up ctx_conv2
I0630 22:02:32.491454  6183 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 22:02:32.491456  6183 net.cpp:163] Memory required for data: 1042432000
I0630 22:02:32.491459  6183 layer_factory.hpp:77] Creating layer ctx_conv2/bn
I0630 22:02:32.491462  6183 net.cpp:98] Creating Layer ctx_conv2/bn
I0630 22:02:32.491466  6183 net.cpp:439] ctx_conv2/bn <- ctx_conv2
I0630 22:02:32.491467  6183 net.cpp:413] ctx_conv2/bn -> ctx_conv2/bn
I0630 22:02:32.492166  6183 net.cpp:148] Setting up ctx_conv2/bn
I0630 22:02:32.492171  6183 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 22:02:32.492172  6183 net.cpp:163] Memory required for data: 1050624000
I0630 22:02:32.492177  6183 layer_factory.hpp:77] Creating layer ctx_conv2/relu
I0630 22:02:32.492182  6183 net.cpp:98] Creating Layer ctx_conv2/relu
I0630 22:02:32.492183  6183 net.cpp:439] ctx_conv2/relu <- ctx_conv2/bn
I0630 22:02:32.492185  6183 net.cpp:400] ctx_conv2/relu -> ctx_conv2/bn (in-place)
I0630 22:02:32.492188  6183 net.cpp:148] Setting up ctx_conv2/relu
I0630 22:02:32.492192  6183 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 22:02:32.492192  6183 net.cpp:163] Memory required for data: 1058816000
I0630 22:02:32.492194  6183 layer_factory.hpp:77] Creating layer ctx_conv3
I0630 22:02:32.492197  6183 net.cpp:98] Creating Layer ctx_conv3
I0630 22:02:32.492200  6183 net.cpp:439] ctx_conv3 <- ctx_conv2/bn
I0630 22:02:32.492202  6183 net.cpp:413] ctx_conv3 -> ctx_conv3
I0630 22:02:32.493213  6183 net.cpp:148] Setting up ctx_conv3
I0630 22:02:32.493217  6183 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 22:02:32.493219  6183 net.cpp:163] Memory required for data: 1067008000
I0630 22:02:32.493223  6183 layer_factory.hpp:77] Creating layer ctx_conv3/bn
I0630 22:02:32.493227  6183 net.cpp:98] Creating Layer ctx_conv3/bn
I0630 22:02:32.493234  6183 net.cpp:439] ctx_conv3/bn <- ctx_conv3
I0630 22:02:32.493237  6183 net.cpp:413] ctx_conv3/bn -> ctx_conv3/bn
I0630 22:02:32.493947  6183 net.cpp:148] Setting up ctx_conv3/bn
I0630 22:02:32.493952  6183 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 22:02:32.493954  6183 net.cpp:163] Memory required for data: 1075200000
I0630 22:02:32.493959  6183 layer_factory.hpp:77] Creating layer ctx_conv3/relu
I0630 22:02:32.493963  6183 net.cpp:98] Creating Layer ctx_conv3/relu
I0630 22:02:32.493965  6183 net.cpp:439] ctx_conv3/relu <- ctx_conv3/bn
I0630 22:02:32.493968  6183 net.cpp:400] ctx_conv3/relu -> ctx_conv3/bn (in-place)
I0630 22:02:32.493973  6183 net.cpp:148] Setting up ctx_conv3/relu
I0630 22:02:32.493975  6183 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 22:02:32.493976  6183 net.cpp:163] Memory required for data: 1083392000
I0630 22:02:32.493978  6183 layer_factory.hpp:77] Creating layer ctx_conv4
I0630 22:02:32.493983  6183 net.cpp:98] Creating Layer ctx_conv4
I0630 22:02:32.493985  6183 net.cpp:439] ctx_conv4 <- ctx_conv3/bn
I0630 22:02:32.493988  6183 net.cpp:413] ctx_conv4 -> ctx_conv4
I0630 22:02:32.495029  6183 net.cpp:148] Setting up ctx_conv4
I0630 22:02:32.495036  6183 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 22:02:32.495038  6183 net.cpp:163] Memory required for data: 1091584000
I0630 22:02:32.495041  6183 layer_factory.hpp:77] Creating layer ctx_conv4/bn
I0630 22:02:32.495044  6183 net.cpp:98] Creating Layer ctx_conv4/bn
I0630 22:02:32.495048  6183 net.cpp:439] ctx_conv4/bn <- ctx_conv4
I0630 22:02:32.495049  6183 net.cpp:413] ctx_conv4/bn -> ctx_conv4/bn
I0630 22:02:32.495744  6183 net.cpp:148] Setting up ctx_conv4/bn
I0630 22:02:32.495750  6183 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 22:02:32.495753  6183 net.cpp:163] Memory required for data: 1099776000
I0630 22:02:32.495756  6183 layer_factory.hpp:77] Creating layer ctx_conv4/relu
I0630 22:02:32.495759  6183 net.cpp:98] Creating Layer ctx_conv4/relu
I0630 22:02:32.495761  6183 net.cpp:439] ctx_conv4/relu <- ctx_conv4/bn
I0630 22:02:32.495764  6183 net.cpp:400] ctx_conv4/relu -> ctx_conv4/bn (in-place)
I0630 22:02:32.495766  6183 net.cpp:148] Setting up ctx_conv4/relu
I0630 22:02:32.495769  6183 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 22:02:32.495770  6183 net.cpp:163] Memory required for data: 1107968000
I0630 22:02:32.495772  6183 layer_factory.hpp:77] Creating layer ctx_final
I0630 22:02:32.495776  6183 net.cpp:98] Creating Layer ctx_final
I0630 22:02:32.495777  6183 net.cpp:439] ctx_final <- ctx_conv4/bn
I0630 22:02:32.495780  6183 net.cpp:413] ctx_final -> ctx_final
I0630 22:02:32.496299  6183 net.cpp:148] Setting up ctx_final
I0630 22:02:32.496305  6183 net.cpp:155] Top shape: 5 20 80 80 (640000)
I0630 22:02:32.496307  6183 net.cpp:163] Memory required for data: 1110528000
I0630 22:02:32.496310  6183 layer_factory.hpp:77] Creating layer ctx_final/relu
I0630 22:02:32.496312  6183 net.cpp:98] Creating Layer ctx_final/relu
I0630 22:02:32.496315  6183 net.cpp:439] ctx_final/relu <- ctx_final
I0630 22:02:32.496316  6183 net.cpp:400] ctx_final/relu -> ctx_final (in-place)
I0630 22:02:32.496320  6183 net.cpp:148] Setting up ctx_final/relu
I0630 22:02:32.496322  6183 net.cpp:155] Top shape: 5 20 80 80 (640000)
I0630 22:02:32.496323  6183 net.cpp:163] Memory required for data: 1113088000
I0630 22:02:32.496325  6183 layer_factory.hpp:77] Creating layer out_deconv_final_up2
I0630 22:02:32.496328  6183 net.cpp:98] Creating Layer out_deconv_final_up2
I0630 22:02:32.496330  6183 net.cpp:439] out_deconv_final_up2 <- ctx_final
I0630 22:02:32.496333  6183 net.cpp:413] out_deconv_final_up2 -> out_deconv_final_up2
I0630 22:02:32.496562  6183 net.cpp:148] Setting up out_deconv_final_up2
I0630 22:02:32.496567  6183 net.cpp:155] Top shape: 5 20 160 160 (2560000)
I0630 22:02:32.496568  6183 net.cpp:163] Memory required for data: 1123328000
I0630 22:02:32.496572  6183 layer_factory.hpp:77] Creating layer out_deconv_final_up4
I0630 22:02:32.496574  6183 net.cpp:98] Creating Layer out_deconv_final_up4
I0630 22:02:32.496582  6183 net.cpp:439] out_deconv_final_up4 <- out_deconv_final_up2
I0630 22:02:32.496585  6183 net.cpp:413] out_deconv_final_up4 -> out_deconv_final_up4
I0630 22:02:32.496820  6183 net.cpp:148] Setting up out_deconv_final_up4
I0630 22:02:32.496825  6183 net.cpp:155] Top shape: 5 20 320 320 (10240000)
I0630 22:02:32.496826  6183 net.cpp:163] Memory required for data: 1164288000
I0630 22:02:32.496829  6183 layer_factory.hpp:77] Creating layer out_deconv_final_up8
I0630 22:02:32.496832  6183 net.cpp:98] Creating Layer out_deconv_final_up8
I0630 22:02:32.496834  6183 net.cpp:439] out_deconv_final_up8 <- out_deconv_final_up4
I0630 22:02:32.496837  6183 net.cpp:413] out_deconv_final_up8 -> out_deconv_final_up8
I0630 22:02:32.497062  6183 net.cpp:148] Setting up out_deconv_final_up8
I0630 22:02:32.497066  6183 net.cpp:155] Top shape: 5 20 640 640 (40960000)
I0630 22:02:32.497068  6183 net.cpp:163] Memory required for data: 1328128000
I0630 22:02:32.497071  6183 layer_factory.hpp:77] Creating layer loss
I0630 22:02:32.497074  6183 net.cpp:98] Creating Layer loss
I0630 22:02:32.497076  6183 net.cpp:439] loss <- out_deconv_final_up8
I0630 22:02:32.497078  6183 net.cpp:439] loss <- label
I0630 22:02:32.497083  6183 net.cpp:413] loss -> loss
I0630 22:02:32.497089  6183 layer_factory.hpp:77] Creating layer loss
I0630 22:02:32.547430  6183 net.cpp:148] Setting up loss
I0630 22:02:32.547451  6183 net.cpp:155] Top shape: (1)
I0630 22:02:32.547454  6183 net.cpp:158]     with loss weight 1
I0630 22:02:32.547466  6183 net.cpp:163] Memory required for data: 1328128004
I0630 22:02:32.547469  6183 net.cpp:224] loss needs backward computation.
I0630 22:02:32.547472  6183 net.cpp:224] out_deconv_final_up8 needs backward computation.
I0630 22:02:32.547474  6183 net.cpp:224] out_deconv_final_up4 needs backward computation.
I0630 22:02:32.547477  6183 net.cpp:224] out_deconv_final_up2 needs backward computation.
I0630 22:02:32.547479  6183 net.cpp:224] ctx_final/relu needs backward computation.
I0630 22:02:32.547482  6183 net.cpp:224] ctx_final needs backward computation.
I0630 22:02:32.547482  6183 net.cpp:224] ctx_conv4/relu needs backward computation.
I0630 22:02:32.547484  6183 net.cpp:224] ctx_conv4/bn needs backward computation.
I0630 22:02:32.547487  6183 net.cpp:224] ctx_conv4 needs backward computation.
I0630 22:02:32.547488  6183 net.cpp:224] ctx_conv3/relu needs backward computation.
I0630 22:02:32.547490  6183 net.cpp:224] ctx_conv3/bn needs backward computation.
I0630 22:02:32.547492  6183 net.cpp:224] ctx_conv3 needs backward computation.
I0630 22:02:32.547494  6183 net.cpp:224] ctx_conv2/relu needs backward computation.
I0630 22:02:32.547497  6183 net.cpp:224] ctx_conv2/bn needs backward computation.
I0630 22:02:32.547498  6183 net.cpp:224] ctx_conv2 needs backward computation.
I0630 22:02:32.547502  6183 net.cpp:224] ctx_conv1/relu needs backward computation.
I0630 22:02:32.547503  6183 net.cpp:224] ctx_conv1/bn needs backward computation.
I0630 22:02:32.547505  6183 net.cpp:224] ctx_conv1 needs backward computation.
I0630 22:02:32.547508  6183 net.cpp:224] out3_out5_combined needs backward computation.
I0630 22:02:32.547511  6183 net.cpp:224] out3a/relu needs backward computation.
I0630 22:02:32.547513  6183 net.cpp:224] out3a/bn needs backward computation.
I0630 22:02:32.547516  6183 net.cpp:224] out3a needs backward computation.
I0630 22:02:32.547519  6183 net.cpp:224] out5a_up2 needs backward computation.
I0630 22:02:32.547521  6183 net.cpp:224] out5a/relu needs backward computation.
I0630 22:02:32.547523  6183 net.cpp:224] out5a/bn needs backward computation.
I0630 22:02:32.547525  6183 net.cpp:224] out5a needs backward computation.
I0630 22:02:32.547528  6183 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0630 22:02:32.547529  6183 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0630 22:02:32.547531  6183 net.cpp:224] res5a_branch2b needs backward computation.
I0630 22:02:32.547534  6183 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0630 22:02:32.547547  6183 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0630 22:02:32.547549  6183 net.cpp:224] res5a_branch2a needs backward computation.
I0630 22:02:32.547552  6183 net.cpp:224] pool4 needs backward computation.
I0630 22:02:32.547554  6183 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0630 22:02:32.547556  6183 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0630 22:02:32.547559  6183 net.cpp:224] res4a_branch2b needs backward computation.
I0630 22:02:32.547560  6183 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0630 22:02:32.547562  6183 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0630 22:02:32.547564  6183 net.cpp:224] res4a_branch2a needs backward computation.
I0630 22:02:32.547566  6183 net.cpp:224] pool3 needs backward computation.
I0630 22:02:32.547569  6183 net.cpp:224] res3a_branch2b/bn_res3a_branch2b/relu_0_split needs backward computation.
I0630 22:02:32.547570  6183 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0630 22:02:32.547572  6183 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0630 22:02:32.547574  6183 net.cpp:224] res3a_branch2b needs backward computation.
I0630 22:02:32.547576  6183 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0630 22:02:32.547579  6183 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0630 22:02:32.547580  6183 net.cpp:224] res3a_branch2a needs backward computation.
I0630 22:02:32.547582  6183 net.cpp:224] pool2 needs backward computation.
I0630 22:02:32.547585  6183 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0630 22:02:32.547586  6183 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0630 22:02:32.547588  6183 net.cpp:224] res2a_branch2b needs backward computation.
I0630 22:02:32.547590  6183 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0630 22:02:32.547592  6183 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0630 22:02:32.547595  6183 net.cpp:224] res2a_branch2a needs backward computation.
I0630 22:02:32.547596  6183 net.cpp:224] pool1 needs backward computation.
I0630 22:02:32.547598  6183 net.cpp:224] conv1b/relu needs backward computation.
I0630 22:02:32.547601  6183 net.cpp:224] conv1b/bn needs backward computation.
I0630 22:02:32.547603  6183 net.cpp:224] conv1b needs backward computation.
I0630 22:02:32.547606  6183 net.cpp:224] conv1a/relu needs backward computation.
I0630 22:02:32.547608  6183 net.cpp:224] conv1a/bn needs backward computation.
I0630 22:02:32.547610  6183 net.cpp:224] conv1a needs backward computation.
I0630 22:02:32.547612  6183 net.cpp:226] data/bias does not need backward computation.
I0630 22:02:32.547616  6183 net.cpp:226] data does not need backward computation.
I0630 22:02:32.547617  6183 net.cpp:268] This network produces output loss
I0630 22:02:32.547649  6183 net.cpp:288] Network initialization done.
I0630 22:02:32.548369  6183 solver.cpp:182] Creating test net (#0) specified by test_net file: training/cityscapes20_jsegnet21v2_2017-06-30_19-26-17/sparse/test.prototxt
I0630 22:02:32.548646  6183 net.cpp:56] Initializing net from parameters: 
name: "jsegnet21v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/val-image-lmdb"
    label_list_path: "data/val-label-lmdb"
    batch_size: 4
    threads: 4
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b/bn"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a/bn"
  top: "out5a/bn"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a/bn"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b/bn"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a/bn"
  top: "out3a/bn"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a/bn"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv1/bn"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv2/bn"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv3/bn"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4/bn"
  top: "ctx_conv4/bn"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4/bn"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 20
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 20
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 20
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
  accuracy_param {
    ignore_label: 255
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
    ignore_label: 255
  }
}
I0630 22:02:32.548765  6183 layer_factory.hpp:77] Creating layer data
I0630 22:02:32.548773  6183 net.cpp:98] Creating Layer data
I0630 22:02:32.548775  6183 net.cpp:413] data -> data
I0630 22:02:32.548780  6183 net.cpp:413] data -> label
I0630 22:02:32.572214  6283 db_lmdb.cpp:35] Opened lmdb data/val-label-lmdb
I0630 22:02:32.593104  6278 db_lmdb.cpp:35] Opened lmdb data/val-image-lmdb
I0630 22:02:32.596525  6183 data_layer.cpp:78] ReshapePrefetch 4, 3, 640, 640
I0630 22:02:32.596609  6183 data_layer.cpp:83] output data size: 4,3,640,640
I0630 22:02:32.620383  6183 data_layer.cpp:78] ReshapePrefetch 4, 1, 640, 640
I0630 22:02:32.620451  6183 data_layer.cpp:83] output data size: 4,1,640,640
I0630 22:02:32.634029  6183 net.cpp:148] Setting up data
I0630 22:02:32.634061  6183 net.cpp:155] Top shape: 4 3 640 640 (4915200)
I0630 22:02:32.634066  6183 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0630 22:02:32.634068  6183 net.cpp:163] Memory required for data: 26214400
I0630 22:02:32.634074  6183 layer_factory.hpp:77] Creating layer label_data_1_split
I0630 22:02:32.634114  6183 net.cpp:98] Creating Layer label_data_1_split
I0630 22:02:32.634119  6183 net.cpp:439] label_data_1_split <- label
I0630 22:02:32.634125  6183 net.cpp:413] label_data_1_split -> label_data_1_split_0
I0630 22:02:32.634135  6183 net.cpp:413] label_data_1_split -> label_data_1_split_1
I0630 22:02:32.634152  6183 net.cpp:413] label_data_1_split -> label_data_1_split_2
I0630 22:02:32.634294  6183 net.cpp:148] Setting up label_data_1_split
I0630 22:02:32.634300  6183 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0630 22:02:32.634305  6183 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0630 22:02:32.634310  6183 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0630 22:02:32.634312  6183 net.cpp:163] Memory required for data: 45875200
I0630 22:02:32.634317  6183 layer_factory.hpp:77] Creating layer data/bias
I0630 22:02:32.634326  6183 net.cpp:98] Creating Layer data/bias
I0630 22:02:32.634331  6183 net.cpp:439] data/bias <- data
I0630 22:02:32.634336  6183 net.cpp:413] data/bias -> data/bias
I0630 22:02:32.635743  6183 net.cpp:148] Setting up data/bias
I0630 22:02:32.635761  6183 net.cpp:155] Top shape: 4 3 640 640 (4915200)
I0630 22:02:32.635764  6183 net.cpp:163] Memory required for data: 65536000
I0630 22:02:32.635776  6183 layer_factory.hpp:77] Creating layer conv1a
I0630 22:02:32.635799  6183 net.cpp:98] Creating Layer conv1a
I0630 22:02:32.635807  6183 net.cpp:439] conv1a <- data/bias
I0630 22:02:32.635812  6183 net.cpp:413] conv1a -> conv1a
I0630 22:02:32.636579  6183 net.cpp:148] Setting up conv1a
I0630 22:02:32.636589  6183 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0630 22:02:32.636591  6183 net.cpp:163] Memory required for data: 117964800
I0630 22:02:32.636602  6183 layer_factory.hpp:77] Creating layer conv1a/bn
I0630 22:02:32.636615  6183 net.cpp:98] Creating Layer conv1a/bn
I0630 22:02:32.636620  6183 net.cpp:439] conv1a/bn <- conv1a
I0630 22:02:32.636626  6183 net.cpp:413] conv1a/bn -> conv1a/bn
I0630 22:02:32.639363  6183 net.cpp:148] Setting up conv1a/bn
I0630 22:02:32.639380  6183 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0630 22:02:32.639384  6183 net.cpp:163] Memory required for data: 170393600
I0630 22:02:32.639396  6183 layer_factory.hpp:77] Creating layer conv1a/relu
I0630 22:02:32.639406  6183 net.cpp:98] Creating Layer conv1a/relu
I0630 22:02:32.639410  6183 net.cpp:439] conv1a/relu <- conv1a/bn
I0630 22:02:32.639418  6183 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0630 22:02:32.639430  6183 net.cpp:148] Setting up conv1a/relu
I0630 22:02:32.639437  6183 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0630 22:02:32.639441  6183 net.cpp:163] Memory required for data: 222822400
I0630 22:02:32.639446  6183 layer_factory.hpp:77] Creating layer conv1b
I0630 22:02:32.639458  6183 net.cpp:98] Creating Layer conv1b
I0630 22:02:32.639462  6183 net.cpp:439] conv1b <- conv1a/bn
I0630 22:02:32.639468  6183 net.cpp:413] conv1b -> conv1b
I0630 22:02:32.640049  6183 net.cpp:148] Setting up conv1b
I0630 22:02:32.640058  6183 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0630 22:02:32.640061  6183 net.cpp:163] Memory required for data: 275251200
I0630 22:02:32.640070  6183 layer_factory.hpp:77] Creating layer conv1b/bn
I0630 22:02:32.640079  6183 net.cpp:98] Creating Layer conv1b/bn
I0630 22:02:32.640084  6183 net.cpp:439] conv1b/bn <- conv1b
I0630 22:02:32.640091  6183 net.cpp:413] conv1b/bn -> conv1b/bn
I0630 22:02:32.641099  6183 net.cpp:148] Setting up conv1b/bn
I0630 22:02:32.641110  6183 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0630 22:02:32.641113  6183 net.cpp:163] Memory required for data: 327680000
I0630 22:02:32.641124  6183 layer_factory.hpp:77] Creating layer conv1b/relu
I0630 22:02:32.641134  6183 net.cpp:98] Creating Layer conv1b/relu
I0630 22:02:32.641137  6183 net.cpp:439] conv1b/relu <- conv1b/bn
I0630 22:02:32.641142  6183 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0630 22:02:32.641149  6183 net.cpp:148] Setting up conv1b/relu
I0630 22:02:32.641155  6183 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0630 22:02:32.641186  6183 net.cpp:163] Memory required for data: 380108800
I0630 22:02:32.641191  6183 layer_factory.hpp:77] Creating layer pool1
I0630 22:02:32.641201  6183 net.cpp:98] Creating Layer pool1
I0630 22:02:32.641204  6183 net.cpp:439] pool1 <- conv1b/bn
I0630 22:02:32.641209  6183 net.cpp:413] pool1 -> pool1
I0630 22:02:32.641273  6183 net.cpp:148] Setting up pool1
I0630 22:02:32.641280  6183 net.cpp:155] Top shape: 4 32 160 160 (3276800)
I0630 22:02:32.641283  6183 net.cpp:163] Memory required for data: 393216000
I0630 22:02:32.641288  6183 layer_factory.hpp:77] Creating layer res2a_branch2a
I0630 22:02:32.641297  6183 net.cpp:98] Creating Layer res2a_branch2a
I0630 22:02:32.641301  6183 net.cpp:439] res2a_branch2a <- pool1
I0630 22:02:32.641306  6183 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0630 22:02:32.642202  6183 net.cpp:148] Setting up res2a_branch2a
I0630 22:02:32.642212  6183 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0630 22:02:32.642215  6183 net.cpp:163] Memory required for data: 419430400
I0630 22:02:32.642225  6183 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0630 22:02:32.642235  6183 net.cpp:98] Creating Layer res2a_branch2a/bn
I0630 22:02:32.642238  6183 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0630 22:02:32.642244  6183 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0630 22:02:32.645519  6183 net.cpp:148] Setting up res2a_branch2a/bn
I0630 22:02:32.645532  6183 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0630 22:02:32.645535  6183 net.cpp:163] Memory required for data: 445644800
I0630 22:02:32.645545  6183 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0630 22:02:32.645638  6183 net.cpp:98] Creating Layer res2a_branch2a/relu
I0630 22:02:32.645644  6183 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0630 22:02:32.645648  6183 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0630 22:02:32.645658  6183 net.cpp:148] Setting up res2a_branch2a/relu
I0630 22:02:32.645738  6183 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0630 22:02:32.645743  6183 net.cpp:163] Memory required for data: 471859200
I0630 22:02:32.645747  6183 layer_factory.hpp:77] Creating layer res2a_branch2b
I0630 22:02:32.645757  6183 net.cpp:98] Creating Layer res2a_branch2b
I0630 22:02:32.645823  6183 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0630 22:02:32.645830  6183 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0630 22:02:32.646499  6183 net.cpp:148] Setting up res2a_branch2b
I0630 22:02:32.646508  6183 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0630 22:02:32.646512  6183 net.cpp:163] Memory required for data: 498073600
I0630 22:02:32.646519  6183 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0630 22:02:32.646540  6183 net.cpp:98] Creating Layer res2a_branch2b/bn
I0630 22:02:32.646546  6183 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0630 22:02:32.646551  6183 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0630 22:02:32.647469  6183 net.cpp:148] Setting up res2a_branch2b/bn
I0630 22:02:32.647480  6183 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0630 22:02:32.647485  6183 net.cpp:163] Memory required for data: 524288000
I0630 22:02:32.647493  6183 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0630 22:02:32.647500  6183 net.cpp:98] Creating Layer res2a_branch2b/relu
I0630 22:02:32.647505  6183 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0630 22:02:32.647508  6183 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0630 22:02:32.647514  6183 net.cpp:148] Setting up res2a_branch2b/relu
I0630 22:02:32.647519  6183 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0630 22:02:32.647522  6183 net.cpp:163] Memory required for data: 550502400
I0630 22:02:32.647526  6183 layer_factory.hpp:77] Creating layer pool2
I0630 22:02:32.647536  6183 net.cpp:98] Creating Layer pool2
I0630 22:02:32.647538  6183 net.cpp:439] pool2 <- res2a_branch2b/bn
I0630 22:02:32.647543  6183 net.cpp:413] pool2 -> pool2
I0630 22:02:32.647611  6183 net.cpp:148] Setting up pool2
I0630 22:02:32.647619  6183 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 22:02:32.647639  6183 net.cpp:163] Memory required for data: 557056000
I0630 22:02:32.647644  6183 layer_factory.hpp:77] Creating layer res3a_branch2a
I0630 22:02:32.647653  6183 net.cpp:98] Creating Layer res3a_branch2a
I0630 22:02:32.647657  6183 net.cpp:439] res3a_branch2a <- pool2
I0630 22:02:32.647662  6183 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0630 22:02:32.649709  6183 net.cpp:148] Setting up res3a_branch2a
I0630 22:02:32.649741  6183 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0630 22:02:32.649744  6183 net.cpp:163] Memory required for data: 570163200
I0630 22:02:32.649755  6183 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0630 22:02:32.649767  6183 net.cpp:98] Creating Layer res3a_branch2a/bn
I0630 22:02:32.649773  6183 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0630 22:02:32.649785  6183 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0630 22:02:32.650602  6183 net.cpp:148] Setting up res3a_branch2a/bn
I0630 22:02:32.650614  6183 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0630 22:02:32.650617  6183 net.cpp:163] Memory required for data: 583270400
I0630 22:02:32.650630  6183 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0630 22:02:32.650638  6183 net.cpp:98] Creating Layer res3a_branch2a/relu
I0630 22:02:32.650642  6183 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0630 22:02:32.650646  6183 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0630 22:02:32.650653  6183 net.cpp:148] Setting up res3a_branch2a/relu
I0630 22:02:32.650657  6183 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0630 22:02:32.650660  6183 net.cpp:163] Memory required for data: 596377600
I0630 22:02:32.650663  6183 layer_factory.hpp:77] Creating layer res3a_branch2b
I0630 22:02:32.650672  6183 net.cpp:98] Creating Layer res3a_branch2b
I0630 22:02:32.650676  6183 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0630 22:02:32.650679  6183 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0630 22:02:32.652637  6183 net.cpp:148] Setting up res3a_branch2b
I0630 22:02:32.652650  6183 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0630 22:02:32.652653  6183 net.cpp:163] Memory required for data: 609484800
I0630 22:02:32.652662  6183 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0630 22:02:32.652673  6183 net.cpp:98] Creating Layer res3a_branch2b/bn
I0630 22:02:32.652676  6183 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0630 22:02:32.652691  6183 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0630 22:02:32.653501  6183 net.cpp:148] Setting up res3a_branch2b/bn
I0630 22:02:32.653512  6183 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0630 22:02:32.653514  6183 net.cpp:163] Memory required for data: 622592000
I0630 22:02:32.653524  6183 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0630 22:02:32.653530  6183 net.cpp:98] Creating Layer res3a_branch2b/relu
I0630 22:02:32.653535  6183 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0630 22:02:32.653540  6183 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0630 22:02:32.653548  6183 net.cpp:148] Setting up res3a_branch2b/relu
I0630 22:02:32.653553  6183 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0630 22:02:32.653556  6183 net.cpp:163] Memory required for data: 635699200
I0630 22:02:32.653560  6183 layer_factory.hpp:77] Creating layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0630 22:02:32.653568  6183 net.cpp:98] Creating Layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0630 22:02:32.653573  6183 net.cpp:439] res3a_branch2b/bn_res3a_branch2b/relu_0_split <- res3a_branch2b/bn
I0630 22:02:32.653578  6183 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0630 22:02:32.653584  6183 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0630 22:02:32.653638  6183 net.cpp:148] Setting up res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0630 22:02:32.653643  6183 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0630 22:02:32.653663  6183 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0630 22:02:32.653668  6183 net.cpp:163] Memory required for data: 661913600
I0630 22:02:32.653672  6183 layer_factory.hpp:77] Creating layer pool3
I0630 22:02:32.653681  6183 net.cpp:98] Creating Layer pool3
I0630 22:02:32.653684  6183 net.cpp:439] pool3 <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0630 22:02:32.653689  6183 net.cpp:413] pool3 -> pool3
I0630 22:02:32.653739  6183 net.cpp:148] Setting up pool3
I0630 22:02:32.653744  6183 net.cpp:155] Top shape: 4 128 40 40 (819200)
I0630 22:02:32.653748  6183 net.cpp:163] Memory required for data: 665190400
I0630 22:02:32.653753  6183 layer_factory.hpp:77] Creating layer res4a_branch2a
I0630 22:02:32.653764  6183 net.cpp:98] Creating Layer res4a_branch2a
I0630 22:02:32.653767  6183 net.cpp:439] res4a_branch2a <- pool3
I0630 22:02:32.653774  6183 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0630 22:02:32.663594  6183 net.cpp:148] Setting up res4a_branch2a
I0630 22:02:32.663645  6183 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0630 22:02:32.663647  6183 net.cpp:163] Memory required for data: 671744000
I0630 22:02:32.663658  6183 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0630 22:02:32.663679  6183 net.cpp:98] Creating Layer res4a_branch2a/bn
I0630 22:02:32.663686  6183 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0630 22:02:32.663702  6183 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0630 22:02:32.664508  6183 net.cpp:148] Setting up res4a_branch2a/bn
I0630 22:02:32.664517  6183 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0630 22:02:32.664520  6183 net.cpp:163] Memory required for data: 678297600
I0630 22:02:32.664525  6183 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0630 22:02:32.664531  6183 net.cpp:98] Creating Layer res4a_branch2a/relu
I0630 22:02:32.664533  6183 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0630 22:02:32.664536  6183 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0630 22:02:32.664543  6183 net.cpp:148] Setting up res4a_branch2a/relu
I0630 22:02:32.664546  6183 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0630 22:02:32.664547  6183 net.cpp:163] Memory required for data: 684851200
I0630 22:02:32.664551  6183 layer_factory.hpp:77] Creating layer res4a_branch2b
I0630 22:02:32.664557  6183 net.cpp:98] Creating Layer res4a_branch2b
I0630 22:02:32.664561  6183 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0630 22:02:32.664563  6183 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0630 22:02:32.668623  6183 net.cpp:148] Setting up res4a_branch2b
I0630 22:02:32.668642  6183 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0630 22:02:32.668648  6183 net.cpp:163] Memory required for data: 691404800
I0630 22:02:32.668656  6183 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0630 22:02:32.668666  6183 net.cpp:98] Creating Layer res4a_branch2b/bn
I0630 22:02:32.668673  6183 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0630 22:02:32.668679  6183 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0630 22:02:32.670132  6183 net.cpp:148] Setting up res4a_branch2b/bn
I0630 22:02:32.670168  6183 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0630 22:02:32.670182  6183 net.cpp:163] Memory required for data: 697958400
I0630 22:02:32.670204  6183 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0630 22:02:32.670222  6183 net.cpp:98] Creating Layer res4a_branch2b/relu
I0630 22:02:32.670235  6183 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0630 22:02:32.670246  6183 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0630 22:02:32.670261  6183 net.cpp:148] Setting up res4a_branch2b/relu
I0630 22:02:32.670276  6183 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0630 22:02:32.670287  6183 net.cpp:163] Memory required for data: 704512000
I0630 22:02:32.670297  6183 layer_factory.hpp:77] Creating layer pool4
I0630 22:02:32.670313  6183 net.cpp:98] Creating Layer pool4
I0630 22:02:32.670322  6183 net.cpp:439] pool4 <- res4a_branch2b/bn
I0630 22:02:32.670339  6183 net.cpp:413] pool4 -> pool4
I0630 22:02:32.670423  6183 net.cpp:148] Setting up pool4
I0630 22:02:32.670435  6183 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0630 22:02:32.670442  6183 net.cpp:163] Memory required for data: 711065600
I0630 22:02:32.670450  6183 layer_factory.hpp:77] Creating layer res5a_branch2a
I0630 22:02:32.670464  6183 net.cpp:98] Creating Layer res5a_branch2a
I0630 22:02:32.670471  6183 net.cpp:439] res5a_branch2a <- pool4
I0630 22:02:32.670480  6183 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0630 22:02:32.696772  6183 net.cpp:148] Setting up res5a_branch2a
I0630 22:02:32.696799  6183 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0630 22:02:32.696801  6183 net.cpp:163] Memory required for data: 724172800
I0630 22:02:32.696808  6183 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0630 22:02:32.696818  6183 net.cpp:98] Creating Layer res5a_branch2a/bn
I0630 22:02:32.696822  6183 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0630 22:02:32.696826  6183 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0630 22:02:32.697567  6183 net.cpp:148] Setting up res5a_branch2a/bn
I0630 22:02:32.697573  6183 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0630 22:02:32.697576  6183 net.cpp:163] Memory required for data: 737280000
I0630 22:02:32.697582  6183 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0630 22:02:32.697584  6183 net.cpp:98] Creating Layer res5a_branch2a/relu
I0630 22:02:32.697587  6183 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0630 22:02:32.697589  6183 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0630 22:02:32.697593  6183 net.cpp:148] Setting up res5a_branch2a/relu
I0630 22:02:32.697595  6183 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0630 22:02:32.697597  6183 net.cpp:163] Memory required for data: 750387200
I0630 22:02:32.697599  6183 layer_factory.hpp:77] Creating layer res5a_branch2b
I0630 22:02:32.697604  6183 net.cpp:98] Creating Layer res5a_branch2b
I0630 22:02:32.697607  6183 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0630 22:02:32.697610  6183 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0630 22:02:32.710532  6183 net.cpp:148] Setting up res5a_branch2b
I0630 22:02:32.710546  6183 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0630 22:02:32.710549  6183 net.cpp:163] Memory required for data: 763494400
I0630 22:02:32.710557  6183 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0630 22:02:32.710563  6183 net.cpp:98] Creating Layer res5a_branch2b/bn
I0630 22:02:32.710566  6183 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0630 22:02:32.710569  6183 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0630 22:02:32.711264  6183 net.cpp:148] Setting up res5a_branch2b/bn
I0630 22:02:32.711271  6183 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0630 22:02:32.711272  6183 net.cpp:163] Memory required for data: 776601600
I0630 22:02:32.711277  6183 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0630 22:02:32.711280  6183 net.cpp:98] Creating Layer res5a_branch2b/relu
I0630 22:02:32.711283  6183 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0630 22:02:32.711285  6183 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0630 22:02:32.711289  6183 net.cpp:148] Setting up res5a_branch2b/relu
I0630 22:02:32.711292  6183 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0630 22:02:32.711293  6183 net.cpp:163] Memory required for data: 789708800
I0630 22:02:32.711295  6183 layer_factory.hpp:77] Creating layer out5a
I0630 22:02:32.711299  6183 net.cpp:98] Creating Layer out5a
I0630 22:02:32.711302  6183 net.cpp:439] out5a <- res5a_branch2b/bn
I0630 22:02:32.711304  6183 net.cpp:413] out5a -> out5a
I0630 22:02:32.715437  6183 net.cpp:148] Setting up out5a
I0630 22:02:32.715445  6183 net.cpp:155] Top shape: 4 64 40 40 (409600)
I0630 22:02:32.715447  6183 net.cpp:163] Memory required for data: 791347200
I0630 22:02:32.715451  6183 layer_factory.hpp:77] Creating layer out5a/bn
I0630 22:02:32.715456  6183 net.cpp:98] Creating Layer out5a/bn
I0630 22:02:32.715459  6183 net.cpp:439] out5a/bn <- out5a
I0630 22:02:32.715472  6183 net.cpp:413] out5a/bn -> out5a/bn
I0630 22:02:32.716223  6183 net.cpp:148] Setting up out5a/bn
I0630 22:02:32.716228  6183 net.cpp:155] Top shape: 4 64 40 40 (409600)
I0630 22:02:32.716229  6183 net.cpp:163] Memory required for data: 792985600
I0630 22:02:32.716234  6183 layer_factory.hpp:77] Creating layer out5a/relu
I0630 22:02:32.716238  6183 net.cpp:98] Creating Layer out5a/relu
I0630 22:02:32.716239  6183 net.cpp:439] out5a/relu <- out5a/bn
I0630 22:02:32.716243  6183 net.cpp:400] out5a/relu -> out5a/bn (in-place)
I0630 22:02:32.716245  6183 net.cpp:148] Setting up out5a/relu
I0630 22:02:32.716248  6183 net.cpp:155] Top shape: 4 64 40 40 (409600)
I0630 22:02:32.716249  6183 net.cpp:163] Memory required for data: 794624000
I0630 22:02:32.716251  6183 layer_factory.hpp:77] Creating layer out5a_up2
I0630 22:02:32.716255  6183 net.cpp:98] Creating Layer out5a_up2
I0630 22:02:32.716258  6183 net.cpp:439] out5a_up2 <- out5a/bn
I0630 22:02:32.716259  6183 net.cpp:413] out5a_up2 -> out5a_up2
I0630 22:02:32.716527  6183 net.cpp:148] Setting up out5a_up2
I0630 22:02:32.716532  6183 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 22:02:32.716534  6183 net.cpp:163] Memory required for data: 801177600
I0630 22:02:32.716537  6183 layer_factory.hpp:77] Creating layer out3a
I0630 22:02:32.716542  6183 net.cpp:98] Creating Layer out3a
I0630 22:02:32.716544  6183 net.cpp:439] out3a <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0630 22:02:32.716547  6183 net.cpp:413] out3a -> out3a
I0630 22:02:32.718616  6183 net.cpp:148] Setting up out3a
I0630 22:02:32.718626  6183 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 22:02:32.718628  6183 net.cpp:163] Memory required for data: 807731200
I0630 22:02:32.718632  6183 layer_factory.hpp:77] Creating layer out3a/bn
I0630 22:02:32.718637  6183 net.cpp:98] Creating Layer out3a/bn
I0630 22:02:32.718641  6183 net.cpp:439] out3a/bn <- out3a
I0630 22:02:32.718644  6183 net.cpp:413] out3a/bn -> out3a/bn
I0630 22:02:32.719415  6183 net.cpp:148] Setting up out3a/bn
I0630 22:02:32.719420  6183 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 22:02:32.719424  6183 net.cpp:163] Memory required for data: 814284800
I0630 22:02:32.719429  6183 layer_factory.hpp:77] Creating layer out3a/relu
I0630 22:02:32.719431  6183 net.cpp:98] Creating Layer out3a/relu
I0630 22:02:32.719434  6183 net.cpp:439] out3a/relu <- out3a/bn
I0630 22:02:32.719435  6183 net.cpp:400] out3a/relu -> out3a/bn (in-place)
I0630 22:02:32.719439  6183 net.cpp:148] Setting up out3a/relu
I0630 22:02:32.719441  6183 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 22:02:32.719444  6183 net.cpp:163] Memory required for data: 820838400
I0630 22:02:32.719445  6183 layer_factory.hpp:77] Creating layer out3_out5_combined
I0630 22:02:32.719449  6183 net.cpp:98] Creating Layer out3_out5_combined
I0630 22:02:32.719450  6183 net.cpp:439] out3_out5_combined <- out5a_up2
I0630 22:02:32.719452  6183 net.cpp:439] out3_out5_combined <- out3a/bn
I0630 22:02:32.719456  6183 net.cpp:413] out3_out5_combined -> out3_out5_combined
I0630 22:02:32.719481  6183 net.cpp:148] Setting up out3_out5_combined
I0630 22:02:32.719485  6183 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 22:02:32.719487  6183 net.cpp:163] Memory required for data: 827392000
I0630 22:02:32.719490  6183 layer_factory.hpp:77] Creating layer ctx_conv1
I0630 22:02:32.719493  6183 net.cpp:98] Creating Layer ctx_conv1
I0630 22:02:32.719496  6183 net.cpp:439] ctx_conv1 <- out3_out5_combined
I0630 22:02:32.719498  6183 net.cpp:413] ctx_conv1 -> ctx_conv1
I0630 22:02:32.720547  6183 net.cpp:148] Setting up ctx_conv1
I0630 22:02:32.720552  6183 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 22:02:32.720554  6183 net.cpp:163] Memory required for data: 833945600
I0630 22:02:32.720557  6183 layer_factory.hpp:77] Creating layer ctx_conv1/bn
I0630 22:02:32.720561  6183 net.cpp:98] Creating Layer ctx_conv1/bn
I0630 22:02:32.720563  6183 net.cpp:439] ctx_conv1/bn <- ctx_conv1
I0630 22:02:32.720566  6183 net.cpp:413] ctx_conv1/bn -> ctx_conv1/bn
I0630 22:02:32.721339  6183 net.cpp:148] Setting up ctx_conv1/bn
I0630 22:02:32.721345  6183 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 22:02:32.721348  6183 net.cpp:163] Memory required for data: 840499200
I0630 22:02:32.721351  6183 layer_factory.hpp:77] Creating layer ctx_conv1/relu
I0630 22:02:32.721354  6183 net.cpp:98] Creating Layer ctx_conv1/relu
I0630 22:02:32.721356  6183 net.cpp:439] ctx_conv1/relu <- ctx_conv1/bn
I0630 22:02:32.721359  6183 net.cpp:400] ctx_conv1/relu -> ctx_conv1/bn (in-place)
I0630 22:02:32.721362  6183 net.cpp:148] Setting up ctx_conv1/relu
I0630 22:02:32.721364  6183 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 22:02:32.721366  6183 net.cpp:163] Memory required for data: 847052800
I0630 22:02:32.721369  6183 layer_factory.hpp:77] Creating layer ctx_conv2
I0630 22:02:32.721375  6183 net.cpp:98] Creating Layer ctx_conv2
I0630 22:02:32.721379  6183 net.cpp:439] ctx_conv2 <- ctx_conv1/bn
I0630 22:02:32.721381  6183 net.cpp:413] ctx_conv2 -> ctx_conv2
I0630 22:02:32.722437  6183 net.cpp:148] Setting up ctx_conv2
I0630 22:02:32.722442  6183 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 22:02:32.722445  6183 net.cpp:163] Memory required for data: 853606400
I0630 22:02:32.722448  6183 layer_factory.hpp:77] Creating layer ctx_conv2/bn
I0630 22:02:32.722451  6183 net.cpp:98] Creating Layer ctx_conv2/bn
I0630 22:02:32.722453  6183 net.cpp:439] ctx_conv2/bn <- ctx_conv2
I0630 22:02:32.722456  6183 net.cpp:413] ctx_conv2/bn -> ctx_conv2/bn
I0630 22:02:32.723242  6183 net.cpp:148] Setting up ctx_conv2/bn
I0630 22:02:32.723248  6183 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 22:02:32.723249  6183 net.cpp:163] Memory required for data: 860160000
I0630 22:02:32.723254  6183 layer_factory.hpp:77] Creating layer ctx_conv2/relu
I0630 22:02:32.723258  6183 net.cpp:98] Creating Layer ctx_conv2/relu
I0630 22:02:32.723259  6183 net.cpp:439] ctx_conv2/relu <- ctx_conv2/bn
I0630 22:02:32.723261  6183 net.cpp:400] ctx_conv2/relu -> ctx_conv2/bn (in-place)
I0630 22:02:32.723264  6183 net.cpp:148] Setting up ctx_conv2/relu
I0630 22:02:32.723266  6183 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 22:02:32.723268  6183 net.cpp:163] Memory required for data: 866713600
I0630 22:02:32.723270  6183 layer_factory.hpp:77] Creating layer ctx_conv3
I0630 22:02:32.723274  6183 net.cpp:98] Creating Layer ctx_conv3
I0630 22:02:32.723276  6183 net.cpp:439] ctx_conv3 <- ctx_conv2/bn
I0630 22:02:32.723280  6183 net.cpp:413] ctx_conv3 -> ctx_conv3
I0630 22:02:32.724333  6183 net.cpp:148] Setting up ctx_conv3
I0630 22:02:32.724337  6183 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 22:02:32.724339  6183 net.cpp:163] Memory required for data: 873267200
I0630 22:02:32.724342  6183 layer_factory.hpp:77] Creating layer ctx_conv3/bn
I0630 22:02:32.724346  6183 net.cpp:98] Creating Layer ctx_conv3/bn
I0630 22:02:32.724349  6183 net.cpp:439] ctx_conv3/bn <- ctx_conv3
I0630 22:02:32.724350  6183 net.cpp:413] ctx_conv3/bn -> ctx_conv3/bn
I0630 22:02:32.725127  6183 net.cpp:148] Setting up ctx_conv3/bn
I0630 22:02:32.725132  6183 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 22:02:32.725134  6183 net.cpp:163] Memory required for data: 879820800
I0630 22:02:32.725139  6183 layer_factory.hpp:77] Creating layer ctx_conv3/relu
I0630 22:02:32.725143  6183 net.cpp:98] Creating Layer ctx_conv3/relu
I0630 22:02:32.725145  6183 net.cpp:439] ctx_conv3/relu <- ctx_conv3/bn
I0630 22:02:32.725148  6183 net.cpp:400] ctx_conv3/relu -> ctx_conv3/bn (in-place)
I0630 22:02:32.725152  6183 net.cpp:148] Setting up ctx_conv3/relu
I0630 22:02:32.725153  6183 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 22:02:32.725155  6183 net.cpp:163] Memory required for data: 886374400
I0630 22:02:32.725157  6183 layer_factory.hpp:77] Creating layer ctx_conv4
I0630 22:02:32.725162  6183 net.cpp:98] Creating Layer ctx_conv4
I0630 22:02:32.725163  6183 net.cpp:439] ctx_conv4 <- ctx_conv3/bn
I0630 22:02:32.725167  6183 net.cpp:413] ctx_conv4 -> ctx_conv4
I0630 22:02:32.726215  6183 net.cpp:148] Setting up ctx_conv4
I0630 22:02:32.726219  6183 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 22:02:32.726228  6183 net.cpp:163] Memory required for data: 892928000
I0630 22:02:32.726233  6183 layer_factory.hpp:77] Creating layer ctx_conv4/bn
I0630 22:02:32.726235  6183 net.cpp:98] Creating Layer ctx_conv4/bn
I0630 22:02:32.726238  6183 net.cpp:439] ctx_conv4/bn <- ctx_conv4
I0630 22:02:32.726240  6183 net.cpp:413] ctx_conv4/bn -> ctx_conv4/bn
I0630 22:02:32.727008  6183 net.cpp:148] Setting up ctx_conv4/bn
I0630 22:02:32.727015  6183 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 22:02:32.727016  6183 net.cpp:163] Memory required for data: 899481600
I0630 22:02:32.727021  6183 layer_factory.hpp:77] Creating layer ctx_conv4/relu
I0630 22:02:32.727023  6183 net.cpp:98] Creating Layer ctx_conv4/relu
I0630 22:02:32.727025  6183 net.cpp:439] ctx_conv4/relu <- ctx_conv4/bn
I0630 22:02:32.727028  6183 net.cpp:400] ctx_conv4/relu -> ctx_conv4/bn (in-place)
I0630 22:02:32.727031  6183 net.cpp:148] Setting up ctx_conv4/relu
I0630 22:02:32.727033  6183 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 22:02:32.727035  6183 net.cpp:163] Memory required for data: 906035200
I0630 22:02:32.727037  6183 layer_factory.hpp:77] Creating layer ctx_final
I0630 22:02:32.727041  6183 net.cpp:98] Creating Layer ctx_final
I0630 22:02:32.727042  6183 net.cpp:439] ctx_final <- ctx_conv4/bn
I0630 22:02:32.727046  6183 net.cpp:413] ctx_final -> ctx_final
I0630 22:02:32.727603  6183 net.cpp:148] Setting up ctx_final
I0630 22:02:32.727608  6183 net.cpp:155] Top shape: 4 20 80 80 (512000)
I0630 22:02:32.727610  6183 net.cpp:163] Memory required for data: 908083200
I0630 22:02:32.727613  6183 layer_factory.hpp:77] Creating layer ctx_final/relu
I0630 22:02:32.727617  6183 net.cpp:98] Creating Layer ctx_final/relu
I0630 22:02:32.727618  6183 net.cpp:439] ctx_final/relu <- ctx_final
I0630 22:02:32.727620  6183 net.cpp:400] ctx_final/relu -> ctx_final (in-place)
I0630 22:02:32.727623  6183 net.cpp:148] Setting up ctx_final/relu
I0630 22:02:32.727627  6183 net.cpp:155] Top shape: 4 20 80 80 (512000)
I0630 22:02:32.727627  6183 net.cpp:163] Memory required for data: 910131200
I0630 22:02:32.727629  6183 layer_factory.hpp:77] Creating layer out_deconv_final_up2
I0630 22:02:32.727633  6183 net.cpp:98] Creating Layer out_deconv_final_up2
I0630 22:02:32.727635  6183 net.cpp:439] out_deconv_final_up2 <- ctx_final
I0630 22:02:32.727638  6183 net.cpp:413] out_deconv_final_up2 -> out_deconv_final_up2
I0630 22:02:32.727898  6183 net.cpp:148] Setting up out_deconv_final_up2
I0630 22:02:32.727902  6183 net.cpp:155] Top shape: 4 20 160 160 (2048000)
I0630 22:02:32.727905  6183 net.cpp:163] Memory required for data: 918323200
I0630 22:02:32.727907  6183 layer_factory.hpp:77] Creating layer out_deconv_final_up4
I0630 22:02:32.727910  6183 net.cpp:98] Creating Layer out_deconv_final_up4
I0630 22:02:32.727912  6183 net.cpp:439] out_deconv_final_up4 <- out_deconv_final_up2
I0630 22:02:32.727916  6183 net.cpp:413] out_deconv_final_up4 -> out_deconv_final_up4
I0630 22:02:32.728173  6183 net.cpp:148] Setting up out_deconv_final_up4
I0630 22:02:32.728178  6183 net.cpp:155] Top shape: 4 20 320 320 (8192000)
I0630 22:02:32.728180  6183 net.cpp:163] Memory required for data: 951091200
I0630 22:02:32.728183  6183 layer_factory.hpp:77] Creating layer out_deconv_final_up8
I0630 22:02:32.728185  6183 net.cpp:98] Creating Layer out_deconv_final_up8
I0630 22:02:32.728188  6183 net.cpp:439] out_deconv_final_up8 <- out_deconv_final_up4
I0630 22:02:32.728190  6183 net.cpp:413] out_deconv_final_up8 -> out_deconv_final_up8
I0630 22:02:32.728442  6183 net.cpp:148] Setting up out_deconv_final_up8
I0630 22:02:32.728447  6183 net.cpp:155] Top shape: 4 20 640 640 (32768000)
I0630 22:02:32.728449  6183 net.cpp:163] Memory required for data: 1082163200
I0630 22:02:32.728452  6183 layer_factory.hpp:77] Creating layer out_deconv_final_up8_out_deconv_final_up8_0_split
I0630 22:02:32.728456  6183 net.cpp:98] Creating Layer out_deconv_final_up8_out_deconv_final_up8_0_split
I0630 22:02:32.728459  6183 net.cpp:439] out_deconv_final_up8_out_deconv_final_up8_0_split <- out_deconv_final_up8
I0630 22:02:32.728468  6183 net.cpp:413] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0630 22:02:32.728472  6183 net.cpp:413] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0630 22:02:32.728477  6183 net.cpp:413] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0630 22:02:32.728533  6183 net.cpp:148] Setting up out_deconv_final_up8_out_deconv_final_up8_0_split
I0630 22:02:32.728538  6183 net.cpp:155] Top shape: 4 20 640 640 (32768000)
I0630 22:02:32.728540  6183 net.cpp:155] Top shape: 4 20 640 640 (32768000)
I0630 22:02:32.728543  6183 net.cpp:155] Top shape: 4 20 640 640 (32768000)
I0630 22:02:32.728544  6183 net.cpp:163] Memory required for data: 1475379200
I0630 22:02:32.728546  6183 layer_factory.hpp:77] Creating layer loss
I0630 22:02:32.728550  6183 net.cpp:98] Creating Layer loss
I0630 22:02:32.728554  6183 net.cpp:439] loss <- out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0630 22:02:32.728555  6183 net.cpp:439] loss <- label_data_1_split_0
I0630 22:02:32.728559  6183 net.cpp:413] loss -> loss
I0630 22:02:32.728562  6183 layer_factory.hpp:77] Creating layer loss
I0630 22:02:32.768090  6183 net.cpp:148] Setting up loss
I0630 22:02:32.768111  6183 net.cpp:155] Top shape: (1)
I0630 22:02:32.768115  6183 net.cpp:158]     with loss weight 1
I0630 22:02:32.768121  6183 net.cpp:163] Memory required for data: 1475379204
I0630 22:02:32.768126  6183 layer_factory.hpp:77] Creating layer accuracy/top1
I0630 22:02:32.768132  6183 net.cpp:98] Creating Layer accuracy/top1
I0630 22:02:32.768137  6183 net.cpp:439] accuracy/top1 <- out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0630 22:02:32.768141  6183 net.cpp:439] accuracy/top1 <- label_data_1_split_1
I0630 22:02:32.768146  6183 net.cpp:413] accuracy/top1 -> accuracy/top1
I0630 22:02:32.768152  6183 net.cpp:148] Setting up accuracy/top1
I0630 22:02:32.768157  6183 net.cpp:155] Top shape: (1)
I0630 22:02:32.768157  6183 net.cpp:163] Memory required for data: 1475379208
I0630 22:02:32.768159  6183 layer_factory.hpp:77] Creating layer accuracy/top5
I0630 22:02:32.768163  6183 net.cpp:98] Creating Layer accuracy/top5
I0630 22:02:32.768165  6183 net.cpp:439] accuracy/top5 <- out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0630 22:02:32.768168  6183 net.cpp:439] accuracy/top5 <- label_data_1_split_2
I0630 22:02:32.768172  6183 net.cpp:413] accuracy/top5 -> accuracy/top5
I0630 22:02:32.768174  6183 net.cpp:148] Setting up accuracy/top5
I0630 22:02:32.768177  6183 net.cpp:155] Top shape: (1)
I0630 22:02:32.768178  6183 net.cpp:163] Memory required for data: 1475379212
I0630 22:02:32.768180  6183 net.cpp:226] accuracy/top5 does not need backward computation.
I0630 22:02:32.768184  6183 net.cpp:226] accuracy/top1 does not need backward computation.
I0630 22:02:32.768187  6183 net.cpp:224] loss needs backward computation.
I0630 22:02:32.768189  6183 net.cpp:224] out_deconv_final_up8_out_deconv_final_up8_0_split needs backward computation.
I0630 22:02:32.768193  6183 net.cpp:224] out_deconv_final_up8 needs backward computation.
I0630 22:02:32.768194  6183 net.cpp:224] out_deconv_final_up4 needs backward computation.
I0630 22:02:32.768198  6183 net.cpp:224] out_deconv_final_up2 needs backward computation.
I0630 22:02:32.768199  6183 net.cpp:224] ctx_final/relu needs backward computation.
I0630 22:02:32.768203  6183 net.cpp:224] ctx_final needs backward computation.
I0630 22:02:32.768204  6183 net.cpp:224] ctx_conv4/relu needs backward computation.
I0630 22:02:32.768206  6183 net.cpp:224] ctx_conv4/bn needs backward computation.
I0630 22:02:32.768208  6183 net.cpp:224] ctx_conv4 needs backward computation.
I0630 22:02:32.768211  6183 net.cpp:224] ctx_conv3/relu needs backward computation.
I0630 22:02:32.768213  6183 net.cpp:224] ctx_conv3/bn needs backward computation.
I0630 22:02:32.768216  6183 net.cpp:224] ctx_conv3 needs backward computation.
I0630 22:02:32.768227  6183 net.cpp:224] ctx_conv2/relu needs backward computation.
I0630 22:02:32.768230  6183 net.cpp:224] ctx_conv2/bn needs backward computation.
I0630 22:02:32.768232  6183 net.cpp:224] ctx_conv2 needs backward computation.
I0630 22:02:32.768234  6183 net.cpp:224] ctx_conv1/relu needs backward computation.
I0630 22:02:32.768235  6183 net.cpp:224] ctx_conv1/bn needs backward computation.
I0630 22:02:32.768239  6183 net.cpp:224] ctx_conv1 needs backward computation.
I0630 22:02:32.768241  6183 net.cpp:224] out3_out5_combined needs backward computation.
I0630 22:02:32.768244  6183 net.cpp:224] out3a/relu needs backward computation.
I0630 22:02:32.768246  6183 net.cpp:224] out3a/bn needs backward computation.
I0630 22:02:32.768249  6183 net.cpp:224] out3a needs backward computation.
I0630 22:02:32.768252  6183 net.cpp:224] out5a_up2 needs backward computation.
I0630 22:02:32.768255  6183 net.cpp:224] out5a/relu needs backward computation.
I0630 22:02:32.768257  6183 net.cpp:224] out5a/bn needs backward computation.
I0630 22:02:32.768260  6183 net.cpp:224] out5a needs backward computation.
I0630 22:02:32.768262  6183 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0630 22:02:32.768265  6183 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0630 22:02:32.768267  6183 net.cpp:224] res5a_branch2b needs backward computation.
I0630 22:02:32.768270  6183 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0630 22:02:32.768272  6183 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0630 22:02:32.768275  6183 net.cpp:224] res5a_branch2a needs backward computation.
I0630 22:02:32.768277  6183 net.cpp:224] pool4 needs backward computation.
I0630 22:02:32.768280  6183 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0630 22:02:32.768282  6183 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0630 22:02:32.768285  6183 net.cpp:224] res4a_branch2b needs backward computation.
I0630 22:02:32.768287  6183 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0630 22:02:32.768290  6183 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0630 22:02:32.768291  6183 net.cpp:224] res4a_branch2a needs backward computation.
I0630 22:02:32.768295  6183 net.cpp:224] pool3 needs backward computation.
I0630 22:02:32.768297  6183 net.cpp:224] res3a_branch2b/bn_res3a_branch2b/relu_0_split needs backward computation.
I0630 22:02:32.768299  6183 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0630 22:02:32.768302  6183 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0630 22:02:32.768304  6183 net.cpp:224] res3a_branch2b needs backward computation.
I0630 22:02:32.768306  6183 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0630 22:02:32.768308  6183 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0630 22:02:32.768311  6183 net.cpp:224] res3a_branch2a needs backward computation.
I0630 22:02:32.768313  6183 net.cpp:224] pool2 needs backward computation.
I0630 22:02:32.768316  6183 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0630 22:02:32.768319  6183 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0630 22:02:32.768321  6183 net.cpp:224] res2a_branch2b needs backward computation.
I0630 22:02:32.768323  6183 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0630 22:02:32.768326  6183 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0630 22:02:32.768328  6183 net.cpp:224] res2a_branch2a needs backward computation.
I0630 22:02:32.768331  6183 net.cpp:224] pool1 needs backward computation.
I0630 22:02:32.768333  6183 net.cpp:224] conv1b/relu needs backward computation.
I0630 22:02:32.768335  6183 net.cpp:224] conv1b/bn needs backward computation.
I0630 22:02:32.768337  6183 net.cpp:224] conv1b needs backward computation.
I0630 22:02:32.768340  6183 net.cpp:224] conv1a/relu needs backward computation.
I0630 22:02:32.768342  6183 net.cpp:224] conv1a/bn needs backward computation.
I0630 22:02:32.768344  6183 net.cpp:224] conv1a needs backward computation.
I0630 22:02:32.768347  6183 net.cpp:226] data/bias does not need backward computation.
I0630 22:02:32.768353  6183 net.cpp:226] label_data_1_split does not need backward computation.
I0630 22:02:32.768357  6183 net.cpp:226] data does not need backward computation.
I0630 22:02:32.768358  6183 net.cpp:268] This network produces output accuracy/top1
I0630 22:02:32.768360  6183 net.cpp:268] This network produces output accuracy/top5
I0630 22:02:32.768363  6183 net.cpp:268] This network produces output loss
I0630 22:02:32.768394  6183 net.cpp:288] Network initialization done.
I0630 22:02:32.768487  6183 solver.cpp:60] Solver scaffolding done.
I0630 22:02:32.776463  6183 caffe.cpp:145] Finetuning from training/cityscapes20_jsegnet21v2_2017-06-30_19-26-17/initial/cityscapes20_jsegnet21v2_iter_32000.caffemodel
I0630 22:02:32.830099  6183 data_layer.cpp:78] ReshapePrefetch 5, 3, 640, 640
I0630 22:02:32.830168  6183 data_layer.cpp:83] output data size: 5,3,640,640
I0630 22:02:32.861964  6183 data_layer.cpp:78] ReshapePrefetch 5, 1, 640, 640
I0630 22:02:32.862032  6183 data_layer.cpp:83] output data size: 5,1,640,640
I0630 22:02:33.401605  6183 data_layer.cpp:78] ReshapePrefetch 5, 3, 640, 640
I0630 22:02:33.401765  6183 data_layer.cpp:83] output data size: 5,3,640,640
I0630 22:02:33.460273  6183 data_layer.cpp:78] ReshapePrefetch 5, 1, 640, 640
I0630 22:02:33.460422  6183 data_layer.cpp:83] output data size: 5,1,640,640
I0630 22:02:34.159998  6183 parallel.cpp:334] Starting Optimization
I0630 22:02:34.160056  6183 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 22:02:34.171515  6183 solver.cpp:413] Solving jsegnet21v2_train
I0630 22:02:34.171531  6183 solver.cpp:414] Learning Rate Policy: multistep
I0630 22:02:34.684921  6183 solver.cpp:290] Iteration 0 (0 iter/s, 0.513357s/100 iter), loss = 0.080394
I0630 22:02:34.684947  6183 solver.cpp:309]     Train net output #0: loss = 0.080394 (* 1 = 0.080394 loss)
I0630 22:02:34.684953  6183 sgd_solver.cpp:106] Iteration 0, lr = 1e-05
I0630 22:02:34.716505  6183 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.05
I0630 22:02:35.044258  6183 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 22:02:54.560667  6183 solver.cpp:290] Iteration 100 (5.0314 iter/s, 19.8752s/100 iter), loss = 0.0607867
I0630 22:02:54.560694  6183 solver.cpp:309]     Train net output #0: loss = 0.0607867 (* 1 = 0.0607867 loss)
I0630 22:02:54.560703  6183 sgd_solver.cpp:106] Iteration 100, lr = 1e-05
I0630 22:02:57.948920  6344 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 22:03:16.939716  6183 solver.cpp:290] Iteration 200 (4.46859 iter/s, 22.3784s/100 iter), loss = 0.0427452
I0630 22:03:16.939774  6183 solver.cpp:309]     Train net output #0: loss = 0.0427452 (* 1 = 0.0427452 loss)
I0630 22:03:16.939781  6183 sgd_solver.cpp:106] Iteration 200, lr = 1e-05
I0630 22:03:36.283085  6183 solver.cpp:290] Iteration 300 (5.16989 iter/s, 19.3428s/100 iter), loss = 0.0713072
I0630 22:03:36.283113  6183 solver.cpp:309]     Train net output #0: loss = 0.0713072 (* 1 = 0.0713072 loss)
I0630 22:03:36.283121  6183 sgd_solver.cpp:106] Iteration 300, lr = 1e-05
I0630 22:03:55.776875  6183 solver.cpp:290] Iteration 400 (5.12999 iter/s, 19.4932s/100 iter), loss = 0.0662863
I0630 22:03:55.776928  6183 solver.cpp:309]     Train net output #0: loss = 0.0662863 (* 1 = 0.0662863 loss)
I0630 22:03:55.776937  6183 sgd_solver.cpp:106] Iteration 400, lr = 1e-05
I0630 22:04:15.206717  6183 solver.cpp:290] Iteration 500 (5.14688 iter/s, 19.4293s/100 iter), loss = 0.0780309
I0630 22:04:15.206744  6183 solver.cpp:309]     Train net output #0: loss = 0.0780309 (* 1 = 0.0780309 loss)
I0630 22:04:15.206753  6183 sgd_solver.cpp:106] Iteration 500, lr = 1e-05
I0630 22:04:34.583106  6183 solver.cpp:290] Iteration 600 (5.16107 iter/s, 19.3758s/100 iter), loss = 0.0402618
I0630 22:04:34.583192  6183 solver.cpp:309]     Train net output #0: loss = 0.0402618 (* 1 = 0.0402618 loss)
I0630 22:04:34.583204  6183 sgd_solver.cpp:106] Iteration 600, lr = 1e-05
I0630 22:04:53.964725  6183 solver.cpp:290] Iteration 700 (5.15969 iter/s, 19.381s/100 iter), loss = 0.0817724
I0630 22:04:53.964751  6183 solver.cpp:309]     Train net output #0: loss = 0.0817724 (* 1 = 0.0817724 loss)
I0630 22:04:53.964758  6183 sgd_solver.cpp:106] Iteration 700, lr = 1e-05
I0630 22:05:13.310587  6183 solver.cpp:290] Iteration 800 (5.16921 iter/s, 19.3453s/100 iter), loss = 0.0587701
I0630 22:05:13.310653  6183 solver.cpp:309]     Train net output #0: loss = 0.0587701 (* 1 = 0.0587701 loss)
I0630 22:05:13.310662  6183 sgd_solver.cpp:106] Iteration 800, lr = 1e-05
I0630 22:05:32.844583  6183 solver.cpp:290] Iteration 900 (5.11944 iter/s, 19.5334s/100 iter), loss = 0.0792876
I0630 22:05:32.844606  6183 solver.cpp:309]     Train net output #0: loss = 0.0792876 (* 1 = 0.0792876 loss)
I0630 22:05:32.844614  6183 sgd_solver.cpp:106] Iteration 900, lr = 1e-05
I0630 22:05:51.903400  6183 solver.cpp:354] Sparsity after update:
I0630 22:05:51.947928  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 22:05:51.947973  6183 net.cpp:1851] conv1a_param_0(0) 
I0630 22:05:51.947999  6183 net.cpp:1851] conv1b_param_0(0.0499) 
I0630 22:05:51.948014  6183 net.cpp:1851] ctx_conv1_param_0(0.05) 
I0630 22:05:51.948026  6183 net.cpp:1851] ctx_conv2_param_0(0.05) 
I0630 22:05:51.948038  6183 net.cpp:1851] ctx_conv3_param_0(0.05) 
I0630 22:05:51.948050  6183 net.cpp:1851] ctx_conv4_param_0(0.0499) 
I0630 22:05:51.948062  6183 net.cpp:1851] ctx_final_param_0(0.025) 
I0630 22:05:51.948073  6183 net.cpp:1851] out3a_param_0(0.05) 
I0630 22:05:51.948083  6183 net.cpp:1851] out5a_param_0(0.05) 
I0630 22:05:51.948093  6183 net.cpp:1851] res2a_branch2a_param_0(0.05) 
I0630 22:05:51.948106  6183 net.cpp:1851] res2a_branch2b_param_0(0.0499) 
I0630 22:05:51.948117  6183 net.cpp:1851] res3a_branch2a_param_0(0.05) 
I0630 22:05:51.948128  6183 net.cpp:1851] res3a_branch2b_param_0(0.05) 
I0630 22:05:51.948140  6183 net.cpp:1851] res4a_branch2a_param_0(0.05) 
I0630 22:05:51.948153  6183 net.cpp:1851] res4a_branch2b_param_0(0.05) 
I0630 22:05:51.948163  6183 net.cpp:1851] res5a_branch2a_param_0(0.05) 
I0630 22:05:51.948175  6183 net.cpp:1851] res5a_branch2b_param_0(0.05) 
I0630 22:05:51.948186  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (134479/2.69808e+06) 0.0498
I0630 22:05:52.129348  6183 solver.cpp:290] Iteration 1000 (5.18559 iter/s, 19.2842s/100 iter), loss = 0.0686194
I0630 22:05:52.129374  6183 solver.cpp:309]     Train net output #0: loss = 0.0686193 (* 1 = 0.0686193 loss)
I0630 22:05:52.129380  6183 sgd_solver.cpp:106] Iteration 1000, lr = 1e-05
I0630 22:05:52.130363  6183 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.1
I0630 22:05:52.549183  6183 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 22:06:11.802105  6183 solver.cpp:290] Iteration 1100 (5.08332 iter/s, 19.6722s/100 iter), loss = 0.0868463
I0630 22:06:11.802129  6183 solver.cpp:309]     Train net output #0: loss = 0.0868464 (* 1 = 0.0868464 loss)
I0630 22:06:11.802135  6183 sgd_solver.cpp:106] Iteration 1100, lr = 1e-05
I0630 22:06:31.617329  6183 solver.cpp:290] Iteration 1200 (5.04677 iter/s, 19.8147s/100 iter), loss = 0.0489149
I0630 22:06:31.617419  6183 solver.cpp:309]     Train net output #0: loss = 0.0489149 (* 1 = 0.0489149 loss)
I0630 22:06:31.617427  6183 sgd_solver.cpp:106] Iteration 1200, lr = 1e-05
I0630 22:06:51.121191  6183 solver.cpp:290] Iteration 1300 (5.12735 iter/s, 19.5032s/100 iter), loss = 0.0665028
I0630 22:06:51.121213  6183 solver.cpp:309]     Train net output #0: loss = 0.0665028 (* 1 = 0.0665028 loss)
I0630 22:06:51.121220  6183 sgd_solver.cpp:106] Iteration 1300, lr = 1e-05
I0630 22:07:10.655335  6183 solver.cpp:290] Iteration 1400 (5.11939 iter/s, 19.5336s/100 iter), loss = 0.165664
I0630 22:07:10.655401  6183 solver.cpp:309]     Train net output #0: loss = 0.165664 (* 1 = 0.165664 loss)
I0630 22:07:10.655413  6183 sgd_solver.cpp:106] Iteration 1400, lr = 1e-05
I0630 22:07:30.146548  6183 solver.cpp:290] Iteration 1500 (5.13067 iter/s, 19.4906s/100 iter), loss = 0.0458943
I0630 22:07:30.146574  6183 solver.cpp:309]     Train net output #0: loss = 0.0458943 (* 1 = 0.0458943 loss)
I0630 22:07:30.146584  6183 sgd_solver.cpp:106] Iteration 1500, lr = 1e-05
I0630 22:07:49.782768  6183 solver.cpp:290] Iteration 1600 (5.09277 iter/s, 19.6357s/100 iter), loss = 0.0545988
I0630 22:07:49.782850  6183 solver.cpp:309]     Train net output #0: loss = 0.0545988 (* 1 = 0.0545988 loss)
I0630 22:07:49.782858  6183 sgd_solver.cpp:106] Iteration 1600, lr = 1e-05
I0630 22:08:09.089679  6183 solver.cpp:290] Iteration 1700 (5.17965 iter/s, 19.3063s/100 iter), loss = 0.0504498
I0630 22:08:09.089701  6183 solver.cpp:309]     Train net output #0: loss = 0.0504498 (* 1 = 0.0504498 loss)
I0630 22:08:09.089709  6183 sgd_solver.cpp:106] Iteration 1700, lr = 1e-05
I0630 22:08:28.432071  6183 solver.cpp:290] Iteration 1800 (5.17014 iter/s, 19.3418s/100 iter), loss = 0.102232
I0630 22:08:28.432155  6183 solver.cpp:309]     Train net output #0: loss = 0.102232 (* 1 = 0.102232 loss)
I0630 22:08:28.432168  6183 sgd_solver.cpp:106] Iteration 1800, lr = 1e-05
I0630 22:08:47.920644  6183 solver.cpp:290] Iteration 1900 (5.13137 iter/s, 19.488s/100 iter), loss = 0.0813589
I0630 22:08:47.920666  6183 solver.cpp:309]     Train net output #0: loss = 0.0813589 (* 1 = 0.0813589 loss)
I0630 22:08:47.920673  6183 sgd_solver.cpp:106] Iteration 1900, lr = 1e-05
I0630 22:09:07.088467  6183 solver.cpp:354] Sparsity after update:
I0630 22:09:07.090335  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 22:09:07.090342  6183 net.cpp:1851] conv1a_param_0(0.05) 
I0630 22:09:07.090349  6183 net.cpp:1851] conv1b_param_0(0.0998) 
I0630 22:09:07.090353  6183 net.cpp:1851] ctx_conv1_param_0(0.1) 
I0630 22:09:07.090354  6183 net.cpp:1851] ctx_conv2_param_0(0.1) 
I0630 22:09:07.090356  6183 net.cpp:1851] ctx_conv3_param_0(0.1) 
I0630 22:09:07.090358  6183 net.cpp:1851] ctx_conv4_param_0(0.1) 
I0630 22:09:07.090360  6183 net.cpp:1851] ctx_final_param_0(0.05) 
I0630 22:09:07.090363  6183 net.cpp:1851] out3a_param_0(0.1) 
I0630 22:09:07.090364  6183 net.cpp:1851] out5a_param_0(0.1) 
I0630 22:09:07.090366  6183 net.cpp:1851] res2a_branch2a_param_0(0.1) 
I0630 22:09:07.090368  6183 net.cpp:1851] res2a_branch2b_param_0(0.0999) 
I0630 22:09:07.090370  6183 net.cpp:1851] res3a_branch2a_param_0(0.1) 
I0630 22:09:07.090371  6183 net.cpp:1851] res3a_branch2b_param_0(0.1) 
I0630 22:09:07.090374  6183 net.cpp:1851] res4a_branch2a_param_0(0.1) 
I0630 22:09:07.090376  6183 net.cpp:1851] res4a_branch2b_param_0(0.1) 
I0630 22:09:07.090378  6183 net.cpp:1851] res5a_branch2a_param_0(0.1) 
I0630 22:09:07.090380  6183 net.cpp:1851] res5a_branch2b_param_0(0.1) 
I0630 22:09:07.090382  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (269082/2.69808e+06) 0.0997
I0630 22:09:07.090525  6183 solver.cpp:471] Iteration 2000, Testing net (#0)
I0630 22:10:45.227181  6183 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.93917
I0630 22:10:45.227277  6183 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.995972
I0630 22:10:45.227283  6183 solver.cpp:544]     Test net output #2: loss = 0.129458 (* 1 = 0.129458 loss)
I0630 22:10:45.452996  6183 solver.cpp:290] Iteration 2000 (0.850852 iter/s, 117.529s/100 iter), loss = 0.0562635
I0630 22:10:45.453022  6183 solver.cpp:309]     Train net output #0: loss = 0.0562635 (* 1 = 0.0562635 loss)
I0630 22:10:45.453028  6183 sgd_solver.cpp:106] Iteration 2000, lr = 1e-05
I0630 22:10:45.454008  6183 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.15
I0630 22:10:45.954640  6183 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 22:11:16.851387  6183 solver.cpp:290] Iteration 2100 (3.18497 iter/s, 31.3975s/100 iter), loss = 0.0588866
I0630 22:11:16.851449  6183 solver.cpp:309]     Train net output #0: loss = 0.0588866 (* 1 = 0.0588866 loss)
I0630 22:11:16.851460  6183 sgd_solver.cpp:106] Iteration 2100, lr = 1e-05
I0630 22:13:12.130020  6276 blocking_queue.cpp:50] Waiting for data
I0630 22:13:14.141876  6183 solver.cpp:290] Iteration 2200 (0.852607 iter/s, 117.287s/100 iter), loss = 0.085147
I0630 22:13:14.141902  6183 solver.cpp:309]     Train net output #0: loss = 0.0851469 (* 1 = 0.0851469 loss)
I0630 22:13:14.141909  6183 sgd_solver.cpp:106] Iteration 2200, lr = 1e-05
I0630 22:13:59.089124  6183 solver.cpp:290] Iteration 2300 (2.22489 iter/s, 44.946s/100 iter), loss = 0.0516093
I0630 22:13:59.089203  6183 solver.cpp:309]     Train net output #0: loss = 0.0516093 (* 1 = 0.0516093 loss)
I0630 22:13:59.089215  6183 sgd_solver.cpp:106] Iteration 2300, lr = 1e-05
I0630 22:14:18.141672  6183 solver.cpp:290] Iteration 2400 (5.24881 iter/s, 19.052s/100 iter), loss = 0.0447182
I0630 22:14:18.141695  6183 solver.cpp:309]     Train net output #0: loss = 0.0447182 (* 1 = 0.0447182 loss)
I0630 22:14:18.141701  6183 sgd_solver.cpp:106] Iteration 2400, lr = 1e-05
I0630 22:14:37.263224  6183 solver.cpp:290] Iteration 2500 (5.22985 iter/s, 19.121s/100 iter), loss = 0.0727641
I0630 22:14:37.263309  6183 solver.cpp:309]     Train net output #0: loss = 0.0727641 (* 1 = 0.0727641 loss)
I0630 22:14:37.263321  6183 sgd_solver.cpp:106] Iteration 2500, lr = 1e-05
I0630 22:14:56.402509  6183 solver.cpp:290] Iteration 2600 (5.22502 iter/s, 19.1387s/100 iter), loss = 0.0735061
I0630 22:14:56.402532  6183 solver.cpp:309]     Train net output #0: loss = 0.0735061 (* 1 = 0.0735061 loss)
I0630 22:14:56.402539  6183 sgd_solver.cpp:106] Iteration 2600, lr = 1e-05
I0630 22:15:15.182616  6183 solver.cpp:290] Iteration 2700 (5.32493 iter/s, 18.7796s/100 iter), loss = 0.0590858
I0630 22:15:15.182660  6183 solver.cpp:309]     Train net output #0: loss = 0.0590858 (* 1 = 0.0590858 loss)
I0630 22:15:15.182668  6183 sgd_solver.cpp:106] Iteration 2700, lr = 1e-05
I0630 22:15:34.275899  6183 solver.cpp:290] Iteration 2800 (5.2376 iter/s, 19.0927s/100 iter), loss = 0.0676441
I0630 22:15:34.275921  6183 solver.cpp:309]     Train net output #0: loss = 0.067644 (* 1 = 0.067644 loss)
I0630 22:15:34.275928  6183 sgd_solver.cpp:106] Iteration 2800, lr = 1e-05
I0630 22:15:53.327910  6183 solver.cpp:290] Iteration 2900 (5.24894 iter/s, 19.0515s/100 iter), loss = 0.0456141
I0630 22:15:53.327961  6183 solver.cpp:309]     Train net output #0: loss = 0.0456141 (* 1 = 0.0456141 loss)
I0630 22:15:53.327970  6183 sgd_solver.cpp:106] Iteration 2900, lr = 1e-05
I0630 22:16:12.623028  6183 solver.cpp:354] Sparsity after update:
I0630 22:16:12.701439  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 22:16:12.701457  6183 net.cpp:1851] conv1a_param_0(0.075) 
I0630 22:16:12.701465  6183 net.cpp:1851] conv1b_param_0(0.15) 
I0630 22:16:12.701467  6183 net.cpp:1851] ctx_conv1_param_0(0.15) 
I0630 22:16:12.701469  6183 net.cpp:1851] ctx_conv2_param_0(0.15) 
I0630 22:16:12.701472  6183 net.cpp:1851] ctx_conv3_param_0(0.15) 
I0630 22:16:12.701473  6183 net.cpp:1851] ctx_conv4_param_0(0.15) 
I0630 22:16:12.701475  6183 net.cpp:1851] ctx_final_param_0(0.075) 
I0630 22:16:12.701478  6183 net.cpp:1851] out3a_param_0(0.15) 
I0630 22:16:12.701479  6183 net.cpp:1851] out5a_param_0(0.15) 
I0630 22:16:12.701481  6183 net.cpp:1851] res2a_branch2a_param_0(0.15) 
I0630 22:16:12.701483  6183 net.cpp:1851] res2a_branch2b_param_0(0.15) 
I0630 22:16:12.701485  6183 net.cpp:1851] res3a_branch2a_param_0(0.15) 
I0630 22:16:12.701488  6183 net.cpp:1851] res3a_branch2b_param_0(0.15) 
I0630 22:16:12.701490  6183 net.cpp:1851] res4a_branch2a_param_0(0.15) 
I0630 22:16:12.701493  6183 net.cpp:1851] res4a_branch2b_param_0(0.15) 
I0630 22:16:12.701493  6183 net.cpp:1851] res5a_branch2a_param_0(0.15) 
I0630 22:16:12.701495  6183 net.cpp:1851] res5a_branch2b_param_0(0.15) 
I0630 22:16:12.701498  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (403642/2.69808e+06) 0.15
I0630 22:16:12.874223  6183 solver.cpp:290] Iteration 3000 (5.11621 iter/s, 19.5457s/100 iter), loss = 0.0736833
I0630 22:16:12.874248  6183 solver.cpp:309]     Train net output #0: loss = 0.0736833 (* 1 = 0.0736833 loss)
I0630 22:16:12.874255  6183 sgd_solver.cpp:106] Iteration 3000, lr = 1e-05
I0630 22:16:12.875509  6183 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.2
I0630 22:16:13.467013  6183 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 22:16:32.586846  6183 solver.cpp:290] Iteration 3100 (5.07303 iter/s, 19.7121s/100 iter), loss = 0.0898425
I0630 22:16:32.586920  6183 solver.cpp:309]     Train net output #0: loss = 0.0898425 (* 1 = 0.0898425 loss)
I0630 22:16:32.586927  6183 sgd_solver.cpp:106] Iteration 3100, lr = 1e-05
I0630 22:16:51.757342  6183 solver.cpp:290] Iteration 3200 (5.21651 iter/s, 19.1699s/100 iter), loss = 0.0581685
I0630 22:16:51.757365  6183 solver.cpp:309]     Train net output #0: loss = 0.0581685 (* 1 = 0.0581685 loss)
I0630 22:16:51.757372  6183 sgd_solver.cpp:106] Iteration 3200, lr = 1e-05
I0630 22:17:10.864214  6183 solver.cpp:290] Iteration 3300 (5.23387 iter/s, 19.1063s/100 iter), loss = 0.0537112
I0630 22:17:10.864286  6183 solver.cpp:309]     Train net output #0: loss = 0.0537112 (* 1 = 0.0537112 loss)
I0630 22:17:10.864295  6183 sgd_solver.cpp:106] Iteration 3300, lr = 1e-05
I0630 22:17:29.757051  6183 solver.cpp:290] Iteration 3400 (5.29317 iter/s, 18.8923s/100 iter), loss = 0.0992056
I0630 22:17:29.757076  6183 solver.cpp:309]     Train net output #0: loss = 0.0992055 (* 1 = 0.0992055 loss)
I0630 22:17:29.757082  6183 sgd_solver.cpp:106] Iteration 3400, lr = 1e-05
I0630 22:17:48.817209  6183 solver.cpp:290] Iteration 3500 (5.24669 iter/s, 19.0596s/100 iter), loss = 0.0609058
I0630 22:17:48.817301  6183 solver.cpp:309]     Train net output #0: loss = 0.0609058 (* 1 = 0.0609058 loss)
I0630 22:17:48.817312  6183 sgd_solver.cpp:106] Iteration 3500, lr = 1e-05
I0630 22:18:07.746912  6183 solver.cpp:290] Iteration 3600 (5.28287 iter/s, 18.9291s/100 iter), loss = 0.0426924
I0630 22:18:07.746933  6183 solver.cpp:309]     Train net output #0: loss = 0.0426924 (* 1 = 0.0426924 loss)
I0630 22:18:07.746940  6183 sgd_solver.cpp:106] Iteration 3600, lr = 1e-05
I0630 22:18:26.834504  6183 solver.cpp:290] Iteration 3700 (5.23915 iter/s, 19.0871s/100 iter), loss = 0.089163
I0630 22:18:26.834581  6183 solver.cpp:309]     Train net output #0: loss = 0.089163 (* 1 = 0.089163 loss)
I0630 22:18:26.834592  6183 sgd_solver.cpp:106] Iteration 3700, lr = 1e-05
I0630 22:18:46.003108  6183 solver.cpp:290] Iteration 3800 (5.21702 iter/s, 19.168s/100 iter), loss = 0.0776446
I0630 22:18:46.003134  6183 solver.cpp:309]     Train net output #0: loss = 0.0776446 (* 1 = 0.0776446 loss)
I0630 22:18:46.003144  6183 sgd_solver.cpp:106] Iteration 3800, lr = 1e-05
I0630 22:19:05.273636  6183 solver.cpp:290] Iteration 3900 (5.18942 iter/s, 19.27s/100 iter), loss = 0.0738207
I0630 22:19:05.273696  6183 solver.cpp:309]     Train net output #0: loss = 0.0738206 (* 1 = 0.0738206 loss)
I0630 22:19:05.273708  6183 sgd_solver.cpp:106] Iteration 3900, lr = 1e-05
I0630 22:19:24.423934  6183 solver.cpp:354] Sparsity after update:
I0630 22:19:24.425864  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 22:19:24.425873  6183 net.cpp:1851] conv1a_param_0(0.1) 
I0630 22:19:24.425879  6183 net.cpp:1851] conv1b_param_0(0.2) 
I0630 22:19:24.425881  6183 net.cpp:1851] ctx_conv1_param_0(0.2) 
I0630 22:19:24.425884  6183 net.cpp:1851] ctx_conv2_param_0(0.2) 
I0630 22:19:24.425885  6183 net.cpp:1851] ctx_conv3_param_0(0.2) 
I0630 22:19:24.425887  6183 net.cpp:1851] ctx_conv4_param_0(0.2) 
I0630 22:19:24.425889  6183 net.cpp:1851] ctx_final_param_0(0.1) 
I0630 22:19:24.425891  6183 net.cpp:1851] out3a_param_0(0.2) 
I0630 22:19:24.425894  6183 net.cpp:1851] out5a_param_0(0.2) 
I0630 22:19:24.425895  6183 net.cpp:1851] res2a_branch2a_param_0(0.2) 
I0630 22:19:24.425896  6183 net.cpp:1851] res2a_branch2b_param_0(0.2) 
I0630 22:19:24.425899  6183 net.cpp:1851] res3a_branch2a_param_0(0.2) 
I0630 22:19:24.425900  6183 net.cpp:1851] res3a_branch2b_param_0(0.2) 
I0630 22:19:24.425902  6183 net.cpp:1851] res4a_branch2a_param_0(0.2) 
I0630 22:19:24.425904  6183 net.cpp:1851] res4a_branch2b_param_0(0.2) 
I0630 22:19:24.425907  6183 net.cpp:1851] res5a_branch2a_param_0(0.2) 
I0630 22:19:24.425911  6183 net.cpp:1851] res5a_branch2b_param_0(0.2) 
I0630 22:19:24.425918  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (538209/2.69808e+06) 0.199
I0630 22:19:24.426061  6183 solver.cpp:471] Iteration 4000, Testing net (#0)
I0630 22:21:04.024420  6183 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.939086
I0630 22:21:04.024549  6183 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.995829
I0630 22:21:04.024559  6183 solver.cpp:544]     Test net output #2: loss = 0.129876 (* 1 = 0.129876 loss)
I0630 22:21:04.238117  6183 solver.cpp:290] Iteration 4000 (0.84061 iter/s, 118.961s/100 iter), loss = 0.0646758
I0630 22:21:04.238143  6183 solver.cpp:309]     Train net output #0: loss = 0.0646758 (* 1 = 0.0646758 loss)
I0630 22:21:04.238152  6183 sgd_solver.cpp:106] Iteration 4000, lr = 1e-05
I0630 22:21:04.239123  6183 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.25
I0630 22:21:04.908326  6183 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 22:21:38.386248  6183 solver.cpp:290] Iteration 4100 (2.9285 iter/s, 34.1472s/100 iter), loss = 0.0769122
I0630 22:21:38.386291  6183 solver.cpp:309]     Train net output #0: loss = 0.0769122 (* 1 = 0.0769122 loss)
I0630 22:21:38.386299  6183 sgd_solver.cpp:106] Iteration 4100, lr = 1e-05
I0630 22:22:00.754904  6325 blocking_queue.cpp:50] Waiting for data
I0630 22:23:15.024941  6183 solver.cpp:290] Iteration 4200 (1.03481 iter/s, 96.636s/100 iter), loss = 0.0672835
I0630 22:23:15.025072  6183 solver.cpp:309]     Train net output #0: loss = 0.0672835 (* 1 = 0.0672835 loss)
I0630 22:23:15.025084  6183 sgd_solver.cpp:106] Iteration 4200, lr = 1e-05
I0630 22:23:37.178468  6183 solver.cpp:290] Iteration 4300 (4.5141 iter/s, 22.1528s/100 iter), loss = 0.0963314
I0630 22:23:37.178491  6183 solver.cpp:309]     Train net output #0: loss = 0.0963314 (* 1 = 0.0963314 loss)
I0630 22:23:37.178498  6183 sgd_solver.cpp:106] Iteration 4300, lr = 1e-05
I0630 22:23:56.230330  6183 solver.cpp:290] Iteration 4400 (5.24898 iter/s, 19.0513s/100 iter), loss = 0.0511291
I0630 22:23:56.230417  6183 solver.cpp:309]     Train net output #0: loss = 0.0511291 (* 1 = 0.0511291 loss)
I0630 22:23:56.230428  6183 sgd_solver.cpp:106] Iteration 4400, lr = 1e-05
I0630 22:24:15.322772  6183 solver.cpp:290] Iteration 4500 (5.23784 iter/s, 19.0918s/100 iter), loss = 0.0698602
I0630 22:24:15.322800  6183 solver.cpp:309]     Train net output #0: loss = 0.0698602 (* 1 = 0.0698602 loss)
I0630 22:24:15.322809  6183 sgd_solver.cpp:106] Iteration 4500, lr = 1e-05
I0630 22:24:34.340831  6183 solver.cpp:290] Iteration 4600 (5.25831 iter/s, 19.0175s/100 iter), loss = 0.104008
I0630 22:24:34.340898  6183 solver.cpp:309]     Train net output #0: loss = 0.104008 (* 1 = 0.104008 loss)
I0630 22:24:34.340906  6183 sgd_solver.cpp:106] Iteration 4600, lr = 1e-05
I0630 22:24:53.559495  6183 solver.cpp:290] Iteration 4700 (5.20343 iter/s, 19.2181s/100 iter), loss = 0.0522869
I0630 22:24:53.559520  6183 solver.cpp:309]     Train net output #0: loss = 0.0522869 (* 1 = 0.0522869 loss)
I0630 22:24:53.559526  6183 sgd_solver.cpp:106] Iteration 4700, lr = 1e-05
I0630 22:25:12.983961  6183 solver.cpp:290] Iteration 4800 (5.14829 iter/s, 19.4239s/100 iter), loss = 0.0666827
I0630 22:25:12.984007  6183 solver.cpp:309]     Train net output #0: loss = 0.0666826 (* 1 = 0.0666826 loss)
I0630 22:25:12.984015  6183 sgd_solver.cpp:106] Iteration 4800, lr = 1e-05
I0630 22:25:32.000849  6183 solver.cpp:290] Iteration 4900 (5.25864 iter/s, 19.0163s/100 iter), loss = 0.0480318
I0630 22:25:32.000869  6183 solver.cpp:309]     Train net output #0: loss = 0.0480318 (* 1 = 0.0480318 loss)
I0630 22:25:32.000876  6183 sgd_solver.cpp:106] Iteration 4900, lr = 1e-05
I0630 22:25:50.839964  6183 solver.cpp:354] Sparsity after update:
I0630 22:25:50.917209  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 22:25:50.917224  6183 net.cpp:1851] conv1a_param_0(0.125) 
I0630 22:25:50.917232  6183 net.cpp:1851] conv1b_param_0(0.25) 
I0630 22:25:50.917234  6183 net.cpp:1851] ctx_conv1_param_0(0.25) 
I0630 22:25:50.917237  6183 net.cpp:1851] ctx_conv2_param_0(0.25) 
I0630 22:25:50.917238  6183 net.cpp:1851] ctx_conv3_param_0(0.25) 
I0630 22:25:50.917240  6183 net.cpp:1851] ctx_conv4_param_0(0.25) 
I0630 22:25:50.917243  6183 net.cpp:1851] ctx_final_param_0(0.125) 
I0630 22:25:50.917244  6183 net.cpp:1851] out3a_param_0(0.25) 
I0630 22:25:50.917246  6183 net.cpp:1851] out5a_param_0(0.25) 
I0630 22:25:50.917248  6183 net.cpp:1851] res2a_branch2a_param_0(0.25) 
I0630 22:25:50.917250  6183 net.cpp:1851] res2a_branch2b_param_0(0.25) 
I0630 22:25:50.917253  6183 net.cpp:1851] res3a_branch2a_param_0(0.25) 
I0630 22:25:50.917254  6183 net.cpp:1851] res3a_branch2b_param_0(0.25) 
I0630 22:25:50.917256  6183 net.cpp:1851] res4a_branch2a_param_0(0.25) 
I0630 22:25:50.917258  6183 net.cpp:1851] res4a_branch2b_param_0(0.25) 
I0630 22:25:50.917260  6183 net.cpp:1851] res5a_branch2a_param_0(0.25) 
I0630 22:25:50.917263  6183 net.cpp:1851] res5a_branch2b_param_0(0.25) 
I0630 22:25:50.917264  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (672759/2.69808e+06) 0.249
I0630 22:25:51.094807  6183 solver.cpp:290] Iteration 5000 (5.23741 iter/s, 19.0934s/100 iter), loss = 0.0523088
I0630 22:25:51.094830  6183 solver.cpp:309]     Train net output #0: loss = 0.0523088 (* 1 = 0.0523088 loss)
I0630 22:25:51.094837  6183 sgd_solver.cpp:106] Iteration 5000, lr = 1e-05
I0630 22:25:51.095716  6183 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.3
I0630 22:25:51.840116  6183 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 22:26:11.024863  6183 solver.cpp:290] Iteration 5100 (5.01769 iter/s, 19.9295s/100 iter), loss = 0.0441847
I0630 22:26:11.024885  6183 solver.cpp:309]     Train net output #0: loss = 0.0441846 (* 1 = 0.0441846 loss)
I0630 22:26:11.024893  6183 sgd_solver.cpp:106] Iteration 5100, lr = 1e-05
I0630 22:26:29.995564  6183 solver.cpp:290] Iteration 5200 (5.27144 iter/s, 18.9702s/100 iter), loss = 0.0598185
I0630 22:26:29.995661  6183 solver.cpp:309]     Train net output #0: loss = 0.0598185 (* 1 = 0.0598185 loss)
I0630 22:26:29.995673  6183 sgd_solver.cpp:106] Iteration 5200, lr = 1e-05
I0630 22:26:48.930130  6183 solver.cpp:290] Iteration 5300 (5.28151 iter/s, 18.934s/100 iter), loss = 0.0691462
I0630 22:26:48.930155  6183 solver.cpp:309]     Train net output #0: loss = 0.0691462 (* 1 = 0.0691462 loss)
I0630 22:26:48.930160  6183 sgd_solver.cpp:106] Iteration 5300, lr = 1e-05
I0630 22:27:07.891180  6183 solver.cpp:290] Iteration 5400 (5.27412 iter/s, 18.9605s/100 iter), loss = 0.0597772
I0630 22:27:07.891273  6183 solver.cpp:309]     Train net output #0: loss = 0.0597772 (* 1 = 0.0597772 loss)
I0630 22:27:07.891283  6183 sgd_solver.cpp:106] Iteration 5400, lr = 1e-05
I0630 22:27:26.877667  6183 solver.cpp:290] Iteration 5500 (5.26707 iter/s, 18.9859s/100 iter), loss = 0.0618154
I0630 22:27:26.877693  6183 solver.cpp:309]     Train net output #0: loss = 0.0618153 (* 1 = 0.0618153 loss)
I0630 22:27:26.877701  6183 sgd_solver.cpp:106] Iteration 5500, lr = 1e-05
I0630 22:27:45.850256  6183 solver.cpp:290] Iteration 5600 (5.27091 iter/s, 18.9721s/100 iter), loss = 0.0759368
I0630 22:27:45.850311  6183 solver.cpp:309]     Train net output #0: loss = 0.0759368 (* 1 = 0.0759368 loss)
I0630 22:27:45.850319  6183 sgd_solver.cpp:106] Iteration 5600, lr = 1e-05
I0630 22:28:04.965492  6183 solver.cpp:290] Iteration 5700 (5.23158 iter/s, 19.1147s/100 iter), loss = 0.0541886
I0630 22:28:04.965514  6183 solver.cpp:309]     Train net output #0: loss = 0.0541885 (* 1 = 0.0541885 loss)
I0630 22:28:04.965520  6183 sgd_solver.cpp:106] Iteration 5700, lr = 1e-05
I0630 22:28:24.046955  6183 solver.cpp:290] Iteration 5800 (5.24084 iter/s, 19.0809s/100 iter), loss = 0.110019
I0630 22:28:24.047062  6183 solver.cpp:309]     Train net output #0: loss = 0.110019 (* 1 = 0.110019 loss)
I0630 22:28:24.047072  6183 sgd_solver.cpp:106] Iteration 5800, lr = 1e-05
I0630 22:28:43.195178  6183 solver.cpp:290] Iteration 5900 (5.22259 iter/s, 19.1476s/100 iter), loss = 0.11208
I0630 22:28:43.195200  6183 solver.cpp:309]     Train net output #0: loss = 0.11208 (* 1 = 0.11208 loss)
I0630 22:28:43.195210  6183 sgd_solver.cpp:106] Iteration 5900, lr = 1e-05
I0630 22:29:02.128131  6183 solver.cpp:354] Sparsity after update:
I0630 22:29:02.129823  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 22:29:02.129832  6183 net.cpp:1851] conv1a_param_0(0.15) 
I0630 22:29:02.129842  6183 net.cpp:1851] conv1b_param_0(0.3) 
I0630 22:29:02.129847  6183 net.cpp:1851] ctx_conv1_param_0(0.3) 
I0630 22:29:02.129851  6183 net.cpp:1851] ctx_conv2_param_0(0.3) 
I0630 22:29:02.129855  6183 net.cpp:1851] ctx_conv3_param_0(0.3) 
I0630 22:29:02.129860  6183 net.cpp:1851] ctx_conv4_param_0(0.3) 
I0630 22:29:02.129864  6183 net.cpp:1851] ctx_final_param_0(0.15) 
I0630 22:29:02.129870  6183 net.cpp:1851] out3a_param_0(0.3) 
I0630 22:29:02.129873  6183 net.cpp:1851] out5a_param_0(0.3) 
I0630 22:29:02.129878  6183 net.cpp:1851] res2a_branch2a_param_0(0.3) 
I0630 22:29:02.129881  6183 net.cpp:1851] res2a_branch2b_param_0(0.3) 
I0630 22:29:02.129885  6183 net.cpp:1851] res3a_branch2a_param_0(0.3) 
I0630 22:29:02.129890  6183 net.cpp:1851] res3a_branch2b_param_0(0.3) 
I0630 22:29:02.129894  6183 net.cpp:1851] res4a_branch2a_param_0(0.3) 
I0630 22:29:02.129897  6183 net.cpp:1851] res4a_branch2b_param_0(0.3) 
I0630 22:29:02.129902  6183 net.cpp:1851] res5a_branch2a_param_0(0.3) 
I0630 22:29:02.129906  6183 net.cpp:1851] res5a_branch2b_param_0(0.3) 
I0630 22:29:02.129911  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (807305/2.69808e+06) 0.299
I0630 22:29:02.130126  6183 solver.cpp:471] Iteration 6000, Testing net (#0)
I0630 22:30:42.730092  6183 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.938581
I0630 22:30:42.730283  6183 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.995838
I0630 22:30:42.730293  6183 solver.cpp:544]     Test net output #2: loss = 0.131583 (* 1 = 0.131583 loss)
I0630 22:30:42.935739  6183 solver.cpp:290] Iteration 6000 (0.835161 iter/s, 119.737s/100 iter), loss = 0.0696933
I0630 22:30:42.935762  6183 solver.cpp:309]     Train net output #0: loss = 0.0696932 (* 1 = 0.0696932 loss)
I0630 22:30:42.935770  6183 sgd_solver.cpp:106] Iteration 6000, lr = 1e-05
I0630 22:30:42.936740  6183 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.35
I0630 22:30:43.769938  6183 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 22:31:06.964707  6183 solver.cpp:290] Iteration 6100 (4.16176 iter/s, 24.0283s/100 iter), loss = 0.0722149
I0630 22:31:06.964736  6183 solver.cpp:309]     Train net output #0: loss = 0.0722148 (* 1 = 0.0722148 loss)
I0630 22:31:06.964745  6183 sgd_solver.cpp:106] Iteration 6100, lr = 1e-05
I0630 22:31:36.119367  6276 blocking_queue.cpp:50] Waiting for data
I0630 22:31:55.236284  6183 solver.cpp:290] Iteration 6200 (2.07167 iter/s, 48.2702s/100 iter), loss = 0.0656174
I0630 22:31:55.236310  6183 solver.cpp:309]     Train net output #0: loss = 0.0656173 (* 1 = 0.0656173 loss)
I0630 22:31:55.236316  6183 sgd_solver.cpp:106] Iteration 6200, lr = 1e-05
I0630 22:32:17.060219  6183 solver.cpp:290] Iteration 6300 (4.58225 iter/s, 21.8233s/100 iter), loss = 0.094609
I0630 22:32:17.060274  6183 solver.cpp:309]     Train net output #0: loss = 0.0946089 (* 1 = 0.0946089 loss)
I0630 22:32:17.060286  6183 sgd_solver.cpp:106] Iteration 6300, lr = 1e-05
I0630 22:32:36.187827  6183 solver.cpp:290] Iteration 6400 (5.2282 iter/s, 19.127s/100 iter), loss = 0.0957429
I0630 22:32:36.187850  6183 solver.cpp:309]     Train net output #0: loss = 0.0957429 (* 1 = 0.0957429 loss)
I0630 22:32:36.187857  6183 sgd_solver.cpp:106] Iteration 6400, lr = 1e-05
I0630 22:32:55.142065  6183 solver.cpp:290] Iteration 6500 (5.27602 iter/s, 18.9537s/100 iter), loss = 0.0973256
I0630 22:32:55.142120  6183 solver.cpp:309]     Train net output #0: loss = 0.0973255 (* 1 = 0.0973255 loss)
I0630 22:32:55.142132  6183 sgd_solver.cpp:106] Iteration 6500, lr = 1e-05
I0630 22:33:14.369446  6183 solver.cpp:290] Iteration 6600 (5.20107 iter/s, 19.2268s/100 iter), loss = 0.102898
I0630 22:33:14.369469  6183 solver.cpp:309]     Train net output #0: loss = 0.102898 (* 1 = 0.102898 loss)
I0630 22:33:14.369477  6183 sgd_solver.cpp:106] Iteration 6600, lr = 1e-05
I0630 22:33:33.496016  6183 solver.cpp:290] Iteration 6700 (5.22848 iter/s, 19.126s/100 iter), loss = 0.0521776
I0630 22:33:33.496088  6183 solver.cpp:309]     Train net output #0: loss = 0.0521775 (* 1 = 0.0521775 loss)
I0630 22:33:33.496095  6183 sgd_solver.cpp:106] Iteration 6700, lr = 1e-05
I0630 22:33:52.875982  6183 solver.cpp:290] Iteration 6800 (5.16013 iter/s, 19.3794s/100 iter), loss = 0.0513142
I0630 22:33:52.876009  6183 solver.cpp:309]     Train net output #0: loss = 0.0513142 (* 1 = 0.0513142 loss)
I0630 22:33:52.876019  6183 sgd_solver.cpp:106] Iteration 6800, lr = 1e-05
I0630 22:34:12.092814  6183 solver.cpp:290] Iteration 6900 (5.20392 iter/s, 19.2163s/100 iter), loss = 0.0461285
I0630 22:34:12.092871  6183 solver.cpp:309]     Train net output #0: loss = 0.0461284 (* 1 = 0.0461284 loss)
I0630 22:34:12.092880  6183 sgd_solver.cpp:106] Iteration 6900, lr = 1e-05
I0630 22:34:31.859544  6183 solver.cpp:354] Sparsity after update:
I0630 22:34:31.936928  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 22:34:31.936949  6183 net.cpp:1851] conv1a_param_0(0.175) 
I0630 22:34:31.936959  6183 net.cpp:1851] conv1b_param_0(0.35) 
I0630 22:34:31.936964  6183 net.cpp:1851] ctx_conv1_param_0(0.35) 
I0630 22:34:31.936967  6183 net.cpp:1851] ctx_conv2_param_0(0.35) 
I0630 22:34:31.936971  6183 net.cpp:1851] ctx_conv3_param_0(0.35) 
I0630 22:34:31.936975  6183 net.cpp:1851] ctx_conv4_param_0(0.35) 
I0630 22:34:31.936980  6183 net.cpp:1851] ctx_final_param_0(0.175) 
I0630 22:34:31.936983  6183 net.cpp:1851] out3a_param_0(0.35) 
I0630 22:34:31.936987  6183 net.cpp:1851] out5a_param_0(0.35) 
I0630 22:34:31.936991  6183 net.cpp:1851] res2a_branch2a_param_0(0.35) 
I0630 22:34:31.936995  6183 net.cpp:1851] res2a_branch2b_param_0(0.35) 
I0630 22:34:31.937000  6183 net.cpp:1851] res3a_branch2a_param_0(0.35) 
I0630 22:34:31.937003  6183 net.cpp:1851] res3a_branch2b_param_0(0.35) 
I0630 22:34:31.937007  6183 net.cpp:1851] res4a_branch2a_param_0(0.35) 
I0630 22:34:31.937011  6183 net.cpp:1851] res4a_branch2b_param_0(0.35) 
I0630 22:34:31.937016  6183 net.cpp:1851] res5a_branch2a_param_0(0.35) 
I0630 22:34:31.937019  6183 net.cpp:1851] res5a_branch2b_param_0(0.35) 
I0630 22:34:31.937023  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (941870/2.69808e+06) 0.349
I0630 22:34:32.107156  6183 solver.cpp:290] Iteration 7000 (4.99657 iter/s, 20.0137s/100 iter), loss = 0.083873
I0630 22:34:32.107182  6183 solver.cpp:309]     Train net output #0: loss = 0.0838729 (* 1 = 0.0838729 loss)
I0630 22:34:32.107190  6183 sgd_solver.cpp:106] Iteration 7000, lr = 1e-05
I0630 22:34:32.108445  6183 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.4
I0630 22:34:33.041461  6183 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 22:34:52.154606  6183 solver.cpp:290] Iteration 7100 (4.98831 iter/s, 20.0469s/100 iter), loss = 0.10051
I0630 22:34:52.154712  6183 solver.cpp:309]     Train net output #0: loss = 0.10051 (* 1 = 0.10051 loss)
I0630 22:34:52.154722  6183 sgd_solver.cpp:106] Iteration 7100, lr = 1e-05
I0630 22:35:11.051735  6183 solver.cpp:290] Iteration 7200 (5.29198 iter/s, 18.8965s/100 iter), loss = 0.0786183
I0630 22:35:11.051759  6183 solver.cpp:309]     Train net output #0: loss = 0.0786183 (* 1 = 0.0786183 loss)
I0630 22:35:11.051765  6183 sgd_solver.cpp:106] Iteration 7200, lr = 1e-05
I0630 22:35:30.349145  6183 solver.cpp:290] Iteration 7300 (5.18219 iter/s, 19.2969s/100 iter), loss = 0.0695157
I0630 22:35:30.349249  6183 solver.cpp:309]     Train net output #0: loss = 0.0695156 (* 1 = 0.0695156 loss)
I0630 22:35:30.349257  6183 sgd_solver.cpp:106] Iteration 7300, lr = 1e-05
I0630 22:35:49.523138  6183 solver.cpp:290] Iteration 7400 (5.21557 iter/s, 19.1734s/100 iter), loss = 0.105437
I0630 22:35:49.523165  6183 solver.cpp:309]     Train net output #0: loss = 0.105437 (* 1 = 0.105437 loss)
I0630 22:35:49.523175  6183 sgd_solver.cpp:106] Iteration 7400, lr = 1e-05
I0630 22:36:08.831759  6183 solver.cpp:290] Iteration 7500 (5.17918 iter/s, 19.3081s/100 iter), loss = 0.0892907
I0630 22:36:08.831822  6183 solver.cpp:309]     Train net output #0: loss = 0.0892906 (* 1 = 0.0892906 loss)
I0630 22:36:08.831830  6183 sgd_solver.cpp:106] Iteration 7500, lr = 1e-05
I0630 22:36:28.012908  6183 solver.cpp:290] Iteration 7600 (5.21361 iter/s, 19.1806s/100 iter), loss = 0.055229
I0630 22:36:28.012935  6183 solver.cpp:309]     Train net output #0: loss = 0.0552289 (* 1 = 0.0552289 loss)
I0630 22:36:28.012943  6183 sgd_solver.cpp:106] Iteration 7600, lr = 1e-05
I0630 22:36:47.129281  6183 solver.cpp:290] Iteration 7700 (5.23127 iter/s, 19.1158s/100 iter), loss = 0.0775177
I0630 22:36:47.129333  6183 solver.cpp:309]     Train net output #0: loss = 0.0775176 (* 1 = 0.0775176 loss)
I0630 22:36:47.129341  6183 sgd_solver.cpp:106] Iteration 7700, lr = 1e-05
I0630 22:37:06.331452  6183 solver.cpp:290] Iteration 7800 (5.2079 iter/s, 19.2016s/100 iter), loss = 0.0605085
I0630 22:37:06.331480  6183 solver.cpp:309]     Train net output #0: loss = 0.0605084 (* 1 = 0.0605084 loss)
I0630 22:37:06.331490  6183 sgd_solver.cpp:106] Iteration 7800, lr = 1e-05
I0630 22:37:25.430492  6183 solver.cpp:290] Iteration 7900 (5.23601 iter/s, 19.0985s/100 iter), loss = 0.0795523
I0630 22:37:25.430567  6183 solver.cpp:309]     Train net output #0: loss = 0.0795522 (* 1 = 0.0795522 loss)
I0630 22:37:25.430575  6183 sgd_solver.cpp:106] Iteration 7900, lr = 1e-05
I0630 22:37:44.397614  6183 solver.cpp:354] Sparsity after update:
I0630 22:37:44.399538  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 22:37:44.399545  6183 net.cpp:1851] conv1a_param_0(0.2) 
I0630 22:37:44.399552  6183 net.cpp:1851] conv1b_param_0(0.4) 
I0630 22:37:44.399554  6183 net.cpp:1851] ctx_conv1_param_0(0.4) 
I0630 22:37:44.399556  6183 net.cpp:1851] ctx_conv2_param_0(0.4) 
I0630 22:37:44.399559  6183 net.cpp:1851] ctx_conv3_param_0(0.4) 
I0630 22:37:44.399560  6183 net.cpp:1851] ctx_conv4_param_0(0.4) 
I0630 22:37:44.399562  6183 net.cpp:1851] ctx_final_param_0(0.2) 
I0630 22:37:44.399564  6183 net.cpp:1851] out3a_param_0(0.4) 
I0630 22:37:44.399566  6183 net.cpp:1851] out5a_param_0(0.4) 
I0630 22:37:44.399569  6183 net.cpp:1851] res2a_branch2a_param_0(0.4) 
I0630 22:37:44.399570  6183 net.cpp:1851] res2a_branch2b_param_0(0.4) 
I0630 22:37:44.399572  6183 net.cpp:1851] res3a_branch2a_param_0(0.4) 
I0630 22:37:44.399574  6183 net.cpp:1851] res3a_branch2b_param_0(0.4) 
I0630 22:37:44.399576  6183 net.cpp:1851] res4a_branch2a_param_0(0.4) 
I0630 22:37:44.399579  6183 net.cpp:1851] res4a_branch2b_param_0(0.4) 
I0630 22:37:44.399580  6183 net.cpp:1851] res5a_branch2a_param_0(0.4) 
I0630 22:37:44.399585  6183 net.cpp:1851] res5a_branch2b_param_0(0.4) 
I0630 22:37:44.399586  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.07641e+06/2.69808e+06) 0.399
I0630 22:37:44.399718  6183 solver.cpp:471] Iteration 8000, Testing net (#0)
I0630 22:39:23.385239  6183 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.937661
I0630 22:39:23.385326  6183 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.995667
I0630 22:39:23.385334  6183 solver.cpp:544]     Test net output #2: loss = 0.133156 (* 1 = 0.133156 loss)
I0630 22:39:23.593489  6183 solver.cpp:290] Iteration 8000 (0.846312 iter/s, 118.16s/100 iter), loss = 0.0734733
I0630 22:39:23.593513  6183 solver.cpp:309]     Train net output #0: loss = 0.0734732 (* 1 = 0.0734732 loss)
I0630 22:39:23.593520  6183 sgd_solver.cpp:106] Iteration 8000, lr = 1e-05
I0630 22:39:23.594512  6183 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.45
I0630 22:39:24.630766  6183 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 22:40:09.655014  6183 solver.cpp:290] Iteration 8100 (2.17107 iter/s, 46.0602s/100 iter), loss = 0.0971045
I0630 22:40:09.655102  6183 solver.cpp:309]     Train net output #0: loss = 0.0971044 (* 1 = 0.0971044 loss)
I0630 22:40:09.655109  6183 sgd_solver.cpp:106] Iteration 8100, lr = 1e-05
I0630 22:40:12.706514  6276 blocking_queue.cpp:50] Waiting for data
I0630 22:41:18.754971  6325 blocking_queue.cpp:50] Waiting for data
I0630 22:41:22.095422  6183 solver.cpp:290] Iteration 8200 (1.38048 iter/s, 72.4383s/100 iter), loss = 0.0934549
I0630 22:41:22.095448  6183 solver.cpp:309]     Train net output #0: loss = 0.0934548 (* 1 = 0.0934548 loss)
I0630 22:41:22.095458  6183 sgd_solver.cpp:106] Iteration 8200, lr = 1e-05
I0630 22:41:43.977263  6183 solver.cpp:290] Iteration 8300 (4.57013 iter/s, 21.8812s/100 iter), loss = 0.0621539
I0630 22:41:43.977288  6183 solver.cpp:309]     Train net output #0: loss = 0.0621538 (* 1 = 0.0621538 loss)
I0630 22:41:43.977293  6183 sgd_solver.cpp:106] Iteration 8300, lr = 1e-05
I0630 22:42:02.988121  6183 solver.cpp:290] Iteration 8400 (5.2603 iter/s, 19.0103s/100 iter), loss = 0.0449747
I0630 22:42:02.988221  6183 solver.cpp:309]     Train net output #0: loss = 0.0449746 (* 1 = 0.0449746 loss)
I0630 22:42:02.988229  6183 sgd_solver.cpp:106] Iteration 8400, lr = 1e-05
I0630 22:42:22.193148  6183 solver.cpp:290] Iteration 8500 (5.20714 iter/s, 19.2044s/100 iter), loss = 0.0992109
I0630 22:42:22.193171  6183 solver.cpp:309]     Train net output #0: loss = 0.0992109 (* 1 = 0.0992109 loss)
I0630 22:42:22.193178  6183 sgd_solver.cpp:106] Iteration 8500, lr = 1e-05
I0630 22:42:41.341902  6183 solver.cpp:290] Iteration 8600 (5.22242 iter/s, 19.1482s/100 iter), loss = 0.057223
I0630 22:42:41.341950  6183 solver.cpp:309]     Train net output #0: loss = 0.057223 (* 1 = 0.057223 loss)
I0630 22:42:41.341958  6183 sgd_solver.cpp:106] Iteration 8600, lr = 1e-05
I0630 22:43:00.417954  6183 solver.cpp:290] Iteration 8700 (5.24233 iter/s, 19.0755s/100 iter), loss = 0.0830471
I0630 22:43:00.417978  6183 solver.cpp:309]     Train net output #0: loss = 0.083047 (* 1 = 0.083047 loss)
I0630 22:43:00.417984  6183 sgd_solver.cpp:106] Iteration 8700, lr = 1e-05
I0630 22:43:19.378373  6183 solver.cpp:290] Iteration 8800 (5.2743 iter/s, 18.9599s/100 iter), loss = 0.0754851
I0630 22:43:19.378419  6183 solver.cpp:309]     Train net output #0: loss = 0.075485 (* 1 = 0.075485 loss)
I0630 22:43:19.378427  6183 sgd_solver.cpp:106] Iteration 8800, lr = 1e-05
I0630 22:43:38.477306  6183 solver.cpp:290] Iteration 8900 (5.23605 iter/s, 19.0984s/100 iter), loss = 0.0664115
I0630 22:43:38.477329  6183 solver.cpp:309]     Train net output #0: loss = 0.0664114 (* 1 = 0.0664114 loss)
I0630 22:43:38.477336  6183 sgd_solver.cpp:106] Iteration 8900, lr = 1e-05
I0630 22:43:57.364380  6183 solver.cpp:354] Sparsity after update:
I0630 22:43:57.439055  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 22:43:57.439072  6183 net.cpp:1851] conv1a_param_0(0.225) 
I0630 22:43:57.439080  6183 net.cpp:1851] conv1b_param_0(0.45) 
I0630 22:43:57.439082  6183 net.cpp:1851] ctx_conv1_param_0(0.45) 
I0630 22:43:57.439085  6183 net.cpp:1851] ctx_conv2_param_0(0.45) 
I0630 22:43:57.439086  6183 net.cpp:1851] ctx_conv3_param_0(0.45) 
I0630 22:43:57.439088  6183 net.cpp:1851] ctx_conv4_param_0(0.45) 
I0630 22:43:57.439090  6183 net.cpp:1851] ctx_final_param_0(0.225) 
I0630 22:43:57.439092  6183 net.cpp:1851] out3a_param_0(0.45) 
I0630 22:43:57.439095  6183 net.cpp:1851] out5a_param_0(0.45) 
I0630 22:43:57.439096  6183 net.cpp:1851] res2a_branch2a_param_0(0.45) 
I0630 22:43:57.439100  6183 net.cpp:1851] res2a_branch2b_param_0(0.45) 
I0630 22:43:57.439101  6183 net.cpp:1851] res3a_branch2a_param_0(0.45) 
I0630 22:43:57.439103  6183 net.cpp:1851] res3a_branch2b_param_0(0.45) 
I0630 22:43:57.439105  6183 net.cpp:1851] res4a_branch2a_param_0(0.45) 
I0630 22:43:57.439107  6183 net.cpp:1851] res4a_branch2b_param_0(0.45) 
I0630 22:43:57.439110  6183 net.cpp:1851] res5a_branch2a_param_0(0.45) 
I0630 22:43:57.439111  6183 net.cpp:1851] res5a_branch2b_param_0(0.45) 
I0630 22:43:57.439113  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.21097e+06/2.69808e+06) 0.449
I0630 22:43:57.609755  6183 solver.cpp:290] Iteration 9000 (5.22687 iter/s, 19.1319s/100 iter), loss = 0.0837013
I0630 22:43:57.609782  6183 solver.cpp:309]     Train net output #0: loss = 0.0837012 (* 1 = 0.0837012 loss)
I0630 22:43:57.609789  6183 sgd_solver.cpp:106] Iteration 9000, lr = 1e-05
I0630 22:43:57.611079  6183 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.5
I0630 22:43:58.731117  6183 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 22:44:17.824980  6183 solver.cpp:290] Iteration 9100 (4.94691 iter/s, 20.2146s/100 iter), loss = 0.100986
I0630 22:44:17.825006  6183 solver.cpp:309]     Train net output #0: loss = 0.100986 (* 1 = 0.100986 loss)
I0630 22:44:17.825012  6183 sgd_solver.cpp:106] Iteration 9100, lr = 1e-05
I0630 22:44:36.895167  6183 solver.cpp:290] Iteration 9200 (5.24394 iter/s, 19.0696s/100 iter), loss = 0.0863522
I0630 22:44:36.895244  6183 solver.cpp:309]     Train net output #0: loss = 0.0863522 (* 1 = 0.0863522 loss)
I0630 22:44:36.895253  6183 sgd_solver.cpp:106] Iteration 9200, lr = 1e-05
I0630 22:44:56.065157  6183 solver.cpp:290] Iteration 9300 (5.21665 iter/s, 19.1694s/100 iter), loss = 0.112267
I0630 22:44:56.065181  6183 solver.cpp:309]     Train net output #0: loss = 0.112267 (* 1 = 0.112267 loss)
I0630 22:44:56.065191  6183 sgd_solver.cpp:106] Iteration 9300, lr = 1e-05
I0630 22:45:15.302105  6183 solver.cpp:290] Iteration 9400 (5.19848 iter/s, 19.2364s/100 iter), loss = 0.0904237
I0630 22:45:15.302162  6183 solver.cpp:309]     Train net output #0: loss = 0.0904236 (* 1 = 0.0904236 loss)
I0630 22:45:15.302171  6183 sgd_solver.cpp:106] Iteration 9400, lr = 1e-05
I0630 22:45:34.449955  6183 solver.cpp:290] Iteration 9500 (5.22268 iter/s, 19.1473s/100 iter), loss = 0.0912011
I0630 22:45:34.449977  6183 solver.cpp:309]     Train net output #0: loss = 0.091201 (* 1 = 0.091201 loss)
I0630 22:45:34.449985  6183 sgd_solver.cpp:106] Iteration 9500, lr = 1e-05
I0630 22:45:53.458675  6183 solver.cpp:290] Iteration 9600 (5.26089 iter/s, 19.0082s/100 iter), loss = 0.0832914
I0630 22:45:53.458721  6183 solver.cpp:309]     Train net output #0: loss = 0.0832914 (* 1 = 0.0832914 loss)
I0630 22:45:53.458732  6183 sgd_solver.cpp:106] Iteration 9600, lr = 1e-05
I0630 22:46:12.608312  6183 solver.cpp:290] Iteration 9700 (5.22219 iter/s, 19.1491s/100 iter), loss = 0.0855816
I0630 22:46:12.608335  6183 solver.cpp:309]     Train net output #0: loss = 0.0855816 (* 1 = 0.0855816 loss)
I0630 22:46:12.608342  6183 sgd_solver.cpp:106] Iteration 9700, lr = 1e-05
I0630 22:46:31.593533  6183 solver.cpp:290] Iteration 9800 (5.26741 iter/s, 18.9847s/100 iter), loss = 0.0557295
I0630 22:46:31.593613  6183 solver.cpp:309]     Train net output #0: loss = 0.0557295 (* 1 = 0.0557295 loss)
I0630 22:46:31.593621  6183 sgd_solver.cpp:106] Iteration 9800, lr = 1e-05
I0630 22:46:50.636198  6183 solver.cpp:290] Iteration 9900 (5.25153 iter/s, 19.0421s/100 iter), loss = 0.14833
I0630 22:46:50.636225  6183 solver.cpp:309]     Train net output #0: loss = 0.14833 (* 1 = 0.14833 loss)
I0630 22:46:50.636234  6183 sgd_solver.cpp:106] Iteration 9900, lr = 1e-05
I0630 22:47:09.580633  6183 solver.cpp:598] Snapshotting to binary proto file training/cityscapes20_jsegnet21v2_2017-06-30_19-26-17/sparse/cityscapes20_jsegnet21v2_iter_10000.caffemodel
I0630 22:47:09.759845  6183 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cityscapes20_jsegnet21v2_2017-06-30_19-26-17/sparse/cityscapes20_jsegnet21v2_iter_10000.solverstate
I0630 22:47:09.776487  6183 solver.cpp:354] Sparsity after update:
I0630 22:47:09.777989  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 22:47:09.777997  6183 net.cpp:1851] conv1a_param_0(0.25) 
I0630 22:47:09.778005  6183 net.cpp:1851] conv1b_param_0(0.5) 
I0630 22:47:09.778007  6183 net.cpp:1851] ctx_conv1_param_0(0.5) 
I0630 22:47:09.778009  6183 net.cpp:1851] ctx_conv2_param_0(0.5) 
I0630 22:47:09.778012  6183 net.cpp:1851] ctx_conv3_param_0(0.5) 
I0630 22:47:09.778013  6183 net.cpp:1851] ctx_conv4_param_0(0.5) 
I0630 22:47:09.778015  6183 net.cpp:1851] ctx_final_param_0(0.25) 
I0630 22:47:09.778017  6183 net.cpp:1851] out3a_param_0(0.5) 
I0630 22:47:09.778019  6183 net.cpp:1851] out5a_param_0(0.5) 
I0630 22:47:09.778022  6183 net.cpp:1851] res2a_branch2a_param_0(0.5) 
I0630 22:47:09.778023  6183 net.cpp:1851] res2a_branch2b_param_0(0.5) 
I0630 22:47:09.778025  6183 net.cpp:1851] res3a_branch2a_param_0(0.5) 
I0630 22:47:09.778028  6183 net.cpp:1851] res3a_branch2b_param_0(0.5) 
I0630 22:47:09.778029  6183 net.cpp:1851] res4a_branch2a_param_0(0.5) 
I0630 22:47:09.778031  6183 net.cpp:1851] res4a_branch2b_param_0(0.5) 
I0630 22:47:09.778033  6183 net.cpp:1851] res5a_branch2a_param_0(0.5) 
I0630 22:47:09.778035  6183 net.cpp:1851] res5a_branch2b_param_0(0.5) 
I0630 22:47:09.778038  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.34554e+06/2.69808e+06) 0.499
I0630 22:47:09.778187  6183 solver.cpp:471] Iteration 10000, Testing net (#0)
I0630 22:48:47.409083  6183 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.93542
I0630 22:48:47.409209  6183 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.995143
I0630 22:48:47.409219  6183 solver.cpp:544]     Test net output #2: loss = 0.140101 (* 1 = 0.140101 loss)
I0630 22:48:47.619235  6183 solver.cpp:290] Iteration 10000 (0.854848 iter/s, 116.98s/100 iter), loss = 0.0528353
I0630 22:48:47.619259  6183 solver.cpp:309]     Train net output #0: loss = 0.0528352 (* 1 = 0.0528352 loss)
I0630 22:48:47.619709  6183 sgd_solver.cpp:106] Iteration 10000, lr = 1e-05
I0630 22:48:47.620530  6183 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.55
I0630 22:48:48.869014  6183 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 22:49:08.176195  6183 solver.cpp:290] Iteration 10100 (4.86467 iter/s, 20.5564s/100 iter), loss = 0.0880809
I0630 22:49:08.176220  6183 solver.cpp:309]     Train net output #0: loss = 0.0880809 (* 1 = 0.0880809 loss)
I0630 22:49:08.176228  6183 sgd_solver.cpp:106] Iteration 10100, lr = 1e-05
I0630 22:49:49.492882  6344 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 22:49:57.130920  6183 solver.cpp:290] Iteration 10200 (2.04276 iter/s, 48.9534s/100 iter), loss = 0.0830373
I0630 22:49:57.130944  6183 solver.cpp:309]     Train net output #0: loss = 0.0830373 (* 1 = 0.0830373 loss)
I0630 22:49:57.130951  6183 sgd_solver.cpp:106] Iteration 10200, lr = 1e-05
I0630 22:50:16.359863  6183 solver.cpp:290] Iteration 10300 (5.20064 iter/s, 19.2284s/100 iter), loss = 0.0882273
I0630 22:50:16.359886  6183 solver.cpp:309]     Train net output #0: loss = 0.0882272 (* 1 = 0.0882272 loss)
I0630 22:50:16.359894  6183 sgd_solver.cpp:106] Iteration 10300, lr = 1e-05
I0630 22:50:35.475009  6183 solver.cpp:290] Iteration 10400 (5.2316 iter/s, 19.1146s/100 iter), loss = 0.101522
I0630 22:50:35.475055  6183 solver.cpp:309]     Train net output #0: loss = 0.101522 (* 1 = 0.101522 loss)
I0630 22:50:35.475062  6183 sgd_solver.cpp:106] Iteration 10400, lr = 1e-05
I0630 22:50:54.395344  6183 solver.cpp:290] Iteration 10500 (5.28548 iter/s, 18.9198s/100 iter), loss = 0.0616472
I0630 22:50:54.395367  6183 solver.cpp:309]     Train net output #0: loss = 0.0616471 (* 1 = 0.0616471 loss)
I0630 22:50:54.395375  6183 sgd_solver.cpp:106] Iteration 10500, lr = 1e-05
I0630 22:51:13.494526  6183 solver.cpp:290] Iteration 10600 (5.23598 iter/s, 19.0986s/100 iter), loss = 0.0647788
I0630 22:51:13.494576  6183 solver.cpp:309]     Train net output #0: loss = 0.0647787 (* 1 = 0.0647787 loss)
I0630 22:51:13.494585  6183 sgd_solver.cpp:106] Iteration 10600, lr = 1e-05
I0630 22:51:32.504375  6183 solver.cpp:290] Iteration 10700 (5.26059 iter/s, 19.0093s/100 iter), loss = 0.131475
I0630 22:51:32.504396  6183 solver.cpp:309]     Train net output #0: loss = 0.131475 (* 1 = 0.131475 loss)
I0630 22:51:32.504403  6183 sgd_solver.cpp:106] Iteration 10700, lr = 1e-05
I0630 22:51:51.574931  6183 solver.cpp:290] Iteration 10800 (5.24384 iter/s, 19.07s/100 iter), loss = 0.0429831
I0630 22:51:51.575008  6183 solver.cpp:309]     Train net output #0: loss = 0.042983 (* 1 = 0.042983 loss)
I0630 22:51:51.575016  6183 sgd_solver.cpp:106] Iteration 10800, lr = 1e-05
I0630 22:52:10.979249  6183 solver.cpp:290] Iteration 10900 (5.15365 iter/s, 19.4037s/100 iter), loss = 0.0849775
I0630 22:52:10.979272  6183 solver.cpp:309]     Train net output #0: loss = 0.0849775 (* 1 = 0.0849775 loss)
I0630 22:52:10.979279  6183 sgd_solver.cpp:106] Iteration 10900, lr = 1e-05
I0630 22:52:30.005993  6183 solver.cpp:354] Sparsity after update:
I0630 22:52:30.073745  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 22:52:30.073767  6183 net.cpp:1851] conv1a_param_0(0.275) 
I0630 22:52:30.073778  6183 net.cpp:1851] conv1b_param_0(0.55) 
I0630 22:52:30.073783  6183 net.cpp:1851] ctx_conv1_param_0(0.55) 
I0630 22:52:30.073786  6183 net.cpp:1851] ctx_conv2_param_0(0.55) 
I0630 22:52:30.073791  6183 net.cpp:1851] ctx_conv3_param_0(0.55) 
I0630 22:52:30.073793  6183 net.cpp:1851] ctx_conv4_param_0(0.55) 
I0630 22:52:30.073796  6183 net.cpp:1851] ctx_final_param_0(0.275) 
I0630 22:52:30.073801  6183 net.cpp:1851] out3a_param_0(0.55) 
I0630 22:52:30.073803  6183 net.cpp:1851] out5a_param_0(0.55) 
I0630 22:52:30.073807  6183 net.cpp:1851] res2a_branch2a_param_0(0.55) 
I0630 22:52:30.073810  6183 net.cpp:1851] res2a_branch2b_param_0(0.55) 
I0630 22:52:30.073814  6183 net.cpp:1851] res3a_branch2a_param_0(0.55) 
I0630 22:52:30.073819  6183 net.cpp:1851] res3a_branch2b_param_0(0.55) 
I0630 22:52:30.073823  6183 net.cpp:1851] res4a_branch2a_param_0(0.55) 
I0630 22:52:30.073827  6183 net.cpp:1851] res4a_branch2b_param_0(0.55) 
I0630 22:52:30.073832  6183 net.cpp:1851] res5a_branch2a_param_0(0.55) 
I0630 22:52:30.073837  6183 net.cpp:1851] res5a_branch2b_param_0(0.55) 
I0630 22:52:30.073840  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.4801e+06/2.69808e+06) 0.549
I0630 22:52:30.245112  6183 solver.cpp:290] Iteration 11000 (5.19068 iter/s, 19.2653s/100 iter), loss = 0.0833842
I0630 22:52:30.245138  6183 solver.cpp:309]     Train net output #0: loss = 0.0833841 (* 1 = 0.0833841 loss)
I0630 22:52:30.245146  6183 sgd_solver.cpp:106] Iteration 11000, lr = 1e-05
I0630 22:52:30.246121  6183 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.6
I0630 22:52:31.634292  6183 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 22:52:50.685799  6183 solver.cpp:290] Iteration 11100 (4.89234 iter/s, 20.4401s/100 iter), loss = 0.129366
I0630 22:52:50.685822  6183 solver.cpp:309]     Train net output #0: loss = 0.129366 (* 1 = 0.129366 loss)
I0630 22:52:50.685829  6183 sgd_solver.cpp:106] Iteration 11100, lr = 1e-05
I0630 22:53:09.841634  6183 solver.cpp:290] Iteration 11200 (5.22049 iter/s, 19.1553s/100 iter), loss = 0.0898908
I0630 22:53:09.841686  6183 solver.cpp:309]     Train net output #0: loss = 0.0898907 (* 1 = 0.0898907 loss)
I0630 22:53:09.841694  6183 sgd_solver.cpp:106] Iteration 11200, lr = 1e-05
I0630 22:53:29.051672  6183 solver.cpp:290] Iteration 11300 (5.20577 iter/s, 19.2095s/100 iter), loss = 0.0685745
I0630 22:53:29.051693  6183 solver.cpp:309]     Train net output #0: loss = 0.0685745 (* 1 = 0.0685745 loss)
I0630 22:53:29.051700  6183 sgd_solver.cpp:106] Iteration 11300, lr = 1e-05
I0630 22:53:48.301026  6183 solver.cpp:290] Iteration 11400 (5.19513 iter/s, 19.2488s/100 iter), loss = 0.0716817
I0630 22:53:48.301105  6183 solver.cpp:309]     Train net output #0: loss = 0.0716817 (* 1 = 0.0716817 loss)
I0630 22:53:48.301115  6183 sgd_solver.cpp:106] Iteration 11400, lr = 1e-05
I0630 22:54:07.168418  6183 solver.cpp:290] Iteration 11500 (5.30032 iter/s, 18.8668s/100 iter), loss = 0.069271
I0630 22:54:07.168443  6183 solver.cpp:309]     Train net output #0: loss = 0.069271 (* 1 = 0.069271 loss)
I0630 22:54:07.168453  6183 sgd_solver.cpp:106] Iteration 11500, lr = 1e-05
I0630 22:54:26.428829  6183 solver.cpp:290] Iteration 11600 (5.19214 iter/s, 19.2599s/100 iter), loss = 0.0647485
I0630 22:54:26.428905  6183 solver.cpp:309]     Train net output #0: loss = 0.0647485 (* 1 = 0.0647485 loss)
I0630 22:54:26.428912  6183 sgd_solver.cpp:106] Iteration 11600, lr = 1e-05
I0630 22:54:45.604594  6183 solver.cpp:290] Iteration 11700 (5.21508 iter/s, 19.1752s/100 iter), loss = 0.152001
I0630 22:54:45.604619  6183 solver.cpp:309]     Train net output #0: loss = 0.152001 (* 1 = 0.152001 loss)
I0630 22:54:45.604625  6183 sgd_solver.cpp:106] Iteration 11700, lr = 1e-05
I0630 22:55:04.675210  6183 solver.cpp:290] Iteration 11800 (5.24382 iter/s, 19.0701s/100 iter), loss = 0.0705581
I0630 22:55:04.675276  6183 solver.cpp:309]     Train net output #0: loss = 0.070558 (* 1 = 0.070558 loss)
I0630 22:55:04.675284  6183 sgd_solver.cpp:106] Iteration 11800, lr = 1e-05
I0630 22:55:23.919462  6183 solver.cpp:290] Iteration 11900 (5.19652 iter/s, 19.2437s/100 iter), loss = 0.0919735
I0630 22:55:23.919486  6183 solver.cpp:309]     Train net output #0: loss = 0.0919734 (* 1 = 0.0919734 loss)
I0630 22:55:23.919492  6183 sgd_solver.cpp:106] Iteration 11900, lr = 1e-05
I0630 22:55:42.781095  6183 solver.cpp:354] Sparsity after update:
I0630 22:55:42.783196  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 22:55:42.783205  6183 net.cpp:1851] conv1a_param_0(0.3) 
I0630 22:55:42.783211  6183 net.cpp:1851] conv1b_param_0(0.6) 
I0630 22:55:42.783213  6183 net.cpp:1851] ctx_conv1_param_0(0.6) 
I0630 22:55:42.783215  6183 net.cpp:1851] ctx_conv2_param_0(0.6) 
I0630 22:55:42.783217  6183 net.cpp:1851] ctx_conv3_param_0(0.6) 
I0630 22:55:42.783219  6183 net.cpp:1851] ctx_conv4_param_0(0.6) 
I0630 22:55:42.783221  6183 net.cpp:1851] ctx_final_param_0(0.3) 
I0630 22:55:42.783223  6183 net.cpp:1851] out3a_param_0(0.6) 
I0630 22:55:42.783226  6183 net.cpp:1851] out5a_param_0(0.6) 
I0630 22:55:42.783227  6183 net.cpp:1851] res2a_branch2a_param_0(0.6) 
I0630 22:55:42.783229  6183 net.cpp:1851] res2a_branch2b_param_0(0.6) 
I0630 22:55:42.783231  6183 net.cpp:1851] res3a_branch2a_param_0(0.6) 
I0630 22:55:42.783233  6183 net.cpp:1851] res3a_branch2b_param_0(0.6) 
I0630 22:55:42.783236  6183 net.cpp:1851] res4a_branch2a_param_0(0.6) 
I0630 22:55:42.783237  6183 net.cpp:1851] res4a_branch2b_param_0(0.6) 
I0630 22:55:42.783239  6183 net.cpp:1851] res5a_branch2a_param_0(0.6) 
I0630 22:55:42.783242  6183 net.cpp:1851] res5a_branch2b_param_0(0.6) 
I0630 22:55:42.783246  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.61464e+06/2.69808e+06) 0.598
I0630 22:55:42.783377  6183 solver.cpp:471] Iteration 12000, Testing net (#0)
I0630 22:57:21.390043  6183 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.932652
I0630 22:57:21.390136  6183 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.994624
I0630 22:57:21.390143  6183 solver.cpp:544]     Test net output #2: loss = 0.145428 (* 1 = 0.145428 loss)
I0630 22:57:21.613607  6183 solver.cpp:290] Iteration 12000 (0.849683 iter/s, 117.691s/100 iter), loss = 0.0793091
I0630 22:57:21.613636  6183 solver.cpp:309]     Train net output #0: loss = 0.079309 (* 1 = 0.079309 loss)
I0630 22:57:21.613651  6183 sgd_solver.cpp:106] Iteration 12000, lr = 1e-05
I0630 22:57:21.614627  6183 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.65
I0630 22:57:23.104311  6183 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 22:57:42.258072  6183 solver.cpp:290] Iteration 12100 (4.84405 iter/s, 20.6439s/100 iter), loss = 0.0756494
I0630 22:57:42.258100  6183 solver.cpp:309]     Train net output #0: loss = 0.0756494 (* 1 = 0.0756494 loss)
I0630 22:57:42.258121  6183 sgd_solver.cpp:106] Iteration 12100, lr = 1e-05
I0630 22:58:01.524184  6183 solver.cpp:290] Iteration 12200 (5.19061 iter/s, 19.2656s/100 iter), loss = 0.0824985
I0630 22:58:01.524261  6183 solver.cpp:309]     Train net output #0: loss = 0.0824984 (* 1 = 0.0824984 loss)
I0630 22:58:01.524272  6183 sgd_solver.cpp:106] Iteration 12200, lr = 1e-05
I0630 22:58:20.717581  6183 solver.cpp:290] Iteration 12300 (5.21029 iter/s, 19.1928s/100 iter), loss = 0.0843738
I0630 22:58:20.717604  6183 solver.cpp:309]     Train net output #0: loss = 0.0843737 (* 1 = 0.0843737 loss)
I0630 22:58:20.717612  6183 sgd_solver.cpp:106] Iteration 12300, lr = 1e-05
I0630 22:58:39.920956  6183 solver.cpp:290] Iteration 12400 (5.20757 iter/s, 19.2028s/100 iter), loss = 0.0853084
I0630 22:58:39.921087  6183 solver.cpp:309]     Train net output #0: loss = 0.0853083 (* 1 = 0.0853083 loss)
I0630 22:58:39.921097  6183 sgd_solver.cpp:106] Iteration 12400, lr = 1e-05
I0630 22:58:59.050892  6183 solver.cpp:290] Iteration 12500 (5.22759 iter/s, 19.1293s/100 iter), loss = 0.0577654
I0630 22:58:59.050916  6183 solver.cpp:309]     Train net output #0: loss = 0.0577653 (* 1 = 0.0577653 loss)
I0630 22:58:59.050925  6183 sgd_solver.cpp:106] Iteration 12500, lr = 1e-05
I0630 22:59:18.325752  6183 solver.cpp:290] Iteration 12600 (5.18825 iter/s, 19.2743s/100 iter), loss = 0.0734227
I0630 22:59:18.325805  6183 solver.cpp:309]     Train net output #0: loss = 0.0734226 (* 1 = 0.0734226 loss)
I0630 22:59:18.325814  6183 sgd_solver.cpp:106] Iteration 12600, lr = 1e-05
I0630 22:59:37.523316  6183 solver.cpp:290] Iteration 12700 (5.20915 iter/s, 19.197s/100 iter), loss = 0.11181
I0630 22:59:37.523339  6183 solver.cpp:309]     Train net output #0: loss = 0.11181 (* 1 = 0.11181 loss)
I0630 22:59:37.523346  6183 sgd_solver.cpp:106] Iteration 12700, lr = 1e-05
I0630 22:59:56.642396  6183 solver.cpp:290] Iteration 12800 (5.23053 iter/s, 19.1185s/100 iter), loss = 0.0952862
I0630 22:59:56.642449  6183 solver.cpp:309]     Train net output #0: loss = 0.0952861 (* 1 = 0.0952861 loss)
I0630 22:59:56.642460  6183 sgd_solver.cpp:106] Iteration 12800, lr = 1e-05
I0630 23:00:15.843582  6183 solver.cpp:290] Iteration 12900 (5.20817 iter/s, 19.2006s/100 iter), loss = 0.0746608
I0630 23:00:15.843608  6183 solver.cpp:309]     Train net output #0: loss = 0.0746608 (* 1 = 0.0746608 loss)
I0630 23:00:15.843614  6183 sgd_solver.cpp:106] Iteration 12900, lr = 1e-05
I0630 23:00:34.795313  6183 solver.cpp:354] Sparsity after update:
I0630 23:00:34.871013  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 23:00:34.871033  6183 net.cpp:1851] conv1a_param_0(0.325) 
I0630 23:00:34.871045  6183 net.cpp:1851] conv1b_param_0(0.65) 
I0630 23:00:34.871049  6183 net.cpp:1851] ctx_conv1_param_0(0.626) 
I0630 23:00:34.871052  6183 net.cpp:1851] ctx_conv2_param_0(0.65) 
I0630 23:00:34.871057  6183 net.cpp:1851] ctx_conv3_param_0(0.619) 
I0630 23:00:34.871059  6183 net.cpp:1851] ctx_conv4_param_0(0.65) 
I0630 23:00:34.871062  6183 net.cpp:1851] ctx_final_param_0(0.325) 
I0630 23:00:34.871067  6183 net.cpp:1851] out3a_param_0(0.65) 
I0630 23:00:34.871069  6183 net.cpp:1851] out5a_param_0(0.65) 
I0630 23:00:34.871073  6183 net.cpp:1851] res2a_branch2a_param_0(0.65) 
I0630 23:00:34.871078  6183 net.cpp:1851] res2a_branch2b_param_0(0.65) 
I0630 23:00:34.871081  6183 net.cpp:1851] res3a_branch2a_param_0(0.65) 
I0630 23:00:34.871085  6183 net.cpp:1851] res3a_branch2b_param_0(0.65) 
I0630 23:00:34.871090  6183 net.cpp:1851] res4a_branch2a_param_0(0.65) 
I0630 23:00:34.871095  6183 net.cpp:1851] res4a_branch2b_param_0(0.65) 
I0630 23:00:34.871099  6183 net.cpp:1851] res5a_branch2a_param_0(0.65) 
I0630 23:00:34.871104  6183 net.cpp:1851] res5a_branch2b_param_0(0.65) 
I0630 23:00:34.871109  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.74715e+06/2.69808e+06) 0.648
I0630 23:00:35.041309  6183 solver.cpp:290] Iteration 13000 (5.2091 iter/s, 19.1972s/100 iter), loss = 0.0953069
I0630 23:00:35.041337  6183 solver.cpp:309]     Train net output #0: loss = 0.0953068 (* 1 = 0.0953068 loss)
I0630 23:00:35.041347  6183 sgd_solver.cpp:106] Iteration 13000, lr = 1e-05
I0630 23:00:35.042287  6183 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.7
I0630 23:00:36.637589  6183 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 23:00:55.888979  6183 solver.cpp:290] Iteration 13100 (4.79684 iter/s, 20.8471s/100 iter), loss = 0.241075
I0630 23:00:55.889005  6183 solver.cpp:309]     Train net output #0: loss = 0.241075 (* 1 = 0.241075 loss)
I0630 23:00:55.889014  6183 sgd_solver.cpp:106] Iteration 13100, lr = 1e-05
I0630 23:01:15.210599  6183 solver.cpp:290] Iteration 13200 (5.1757 iter/s, 19.3211s/100 iter), loss = 0.0915433
I0630 23:01:15.210654  6183 solver.cpp:309]     Train net output #0: loss = 0.0915433 (* 1 = 0.0915433 loss)
I0630 23:01:15.210661  6183 sgd_solver.cpp:106] Iteration 13200, lr = 1e-05
I0630 23:01:34.270993  6183 solver.cpp:290] Iteration 13300 (5.24664 iter/s, 19.0598s/100 iter), loss = 0.0844101
I0630 23:01:34.271018  6183 solver.cpp:309]     Train net output #0: loss = 0.08441 (* 1 = 0.08441 loss)
I0630 23:01:34.271024  6183 sgd_solver.cpp:106] Iteration 13300, lr = 1e-05
I0630 23:01:53.405287  6183 solver.cpp:290] Iteration 13400 (5.22637 iter/s, 19.1338s/100 iter), loss = 0.066569
I0630 23:01:53.405354  6183 solver.cpp:309]     Train net output #0: loss = 0.0665689 (* 1 = 0.0665689 loss)
I0630 23:01:53.405361  6183 sgd_solver.cpp:106] Iteration 13400, lr = 1e-05
I0630 23:02:12.447569  6183 solver.cpp:290] Iteration 13500 (5.25163 iter/s, 19.0417s/100 iter), loss = 0.141264
I0630 23:02:12.447592  6183 solver.cpp:309]     Train net output #0: loss = 0.141264 (* 1 = 0.141264 loss)
I0630 23:02:12.447599  6183 sgd_solver.cpp:106] Iteration 13500, lr = 1e-05
I0630 23:02:31.583592  6183 solver.cpp:290] Iteration 13600 (5.22589 iter/s, 19.1355s/100 iter), loss = 0.0843519
I0630 23:02:31.583652  6183 solver.cpp:309]     Train net output #0: loss = 0.0843518 (* 1 = 0.0843518 loss)
I0630 23:02:31.583663  6183 sgd_solver.cpp:106] Iteration 13600, lr = 1e-05
I0630 23:02:50.854539  6183 solver.cpp:290] Iteration 13700 (5.18931 iter/s, 19.2704s/100 iter), loss = 0.0950526
I0630 23:02:50.854563  6183 solver.cpp:309]     Train net output #0: loss = 0.0950525 (* 1 = 0.0950525 loss)
I0630 23:02:50.854570  6183 sgd_solver.cpp:106] Iteration 13700, lr = 1e-05
I0630 23:03:09.994174  6183 solver.cpp:290] Iteration 13800 (5.22491 iter/s, 19.1391s/100 iter), loss = 0.124694
I0630 23:03:09.994261  6183 solver.cpp:309]     Train net output #0: loss = 0.124694 (* 1 = 0.124694 loss)
I0630 23:03:09.994272  6183 sgd_solver.cpp:106] Iteration 13800, lr = 1e-05
I0630 23:03:29.086058  6183 solver.cpp:290] Iteration 13900 (5.23799 iter/s, 19.0913s/100 iter), loss = 0.0791134
I0630 23:03:29.086081  6183 solver.cpp:309]     Train net output #0: loss = 0.0791133 (* 1 = 0.0791133 loss)
I0630 23:03:29.086087  6183 sgd_solver.cpp:106] Iteration 13900, lr = 1e-05
I0630 23:03:48.111675  6183 solver.cpp:354] Sparsity after update:
I0630 23:03:48.113598  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 23:03:48.113605  6183 net.cpp:1851] conv1a_param_0(0.35) 
I0630 23:03:48.113613  6183 net.cpp:1851] conv1b_param_0(0.7) 
I0630 23:03:48.113616  6183 net.cpp:1851] ctx_conv1_param_0(0.627) 
I0630 23:03:48.113618  6183 net.cpp:1851] ctx_conv2_param_0(0.682) 
I0630 23:03:48.113621  6183 net.cpp:1851] ctx_conv3_param_0(0.621) 
I0630 23:03:48.113623  6183 net.cpp:1851] ctx_conv4_param_0(0.673) 
I0630 23:03:48.113626  6183 net.cpp:1851] ctx_final_param_0(0.35) 
I0630 23:03:48.113628  6183 net.cpp:1851] out3a_param_0(0.7) 
I0630 23:03:48.113631  6183 net.cpp:1851] out5a_param_0(0.7) 
I0630 23:03:48.113632  6183 net.cpp:1851] res2a_branch2a_param_0(0.7) 
I0630 23:03:48.113636  6183 net.cpp:1851] res2a_branch2b_param_0(0.7) 
I0630 23:03:48.113637  6183 net.cpp:1851] res3a_branch2a_param_0(0.7) 
I0630 23:03:48.113639  6183 net.cpp:1851] res3a_branch2b_param_0(0.7) 
I0630 23:03:48.113642  6183 net.cpp:1851] res4a_branch2a_param_0(0.7) 
I0630 23:03:48.113644  6183 net.cpp:1851] res4a_branch2b_param_0(0.7) 
I0630 23:03:48.113646  6183 net.cpp:1851] res5a_branch2a_param_0(0.7) 
I0630 23:03:48.113648  6183 net.cpp:1851] res5a_branch2b_param_0(0.7) 
I0630 23:03:48.113651  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.87654e+06/2.69808e+06) 0.696
I0630 23:03:48.113781  6183 solver.cpp:471] Iteration 14000, Testing net (#0)
I0630 23:05:27.144423  6183 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.926823
I0630 23:05:27.144511  6183 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.993667
I0630 23:05:27.144520  6183 solver.cpp:544]     Test net output #2: loss = 0.159123 (* 1 = 0.159123 loss)
I0630 23:05:27.358676  6183 solver.cpp:290] Iteration 14000 (0.845527 iter/s, 118.269s/100 iter), loss = 0.0806951
I0630 23:05:27.358701  6183 solver.cpp:309]     Train net output #0: loss = 0.080695 (* 1 = 0.080695 loss)
I0630 23:05:27.358708  6183 sgd_solver.cpp:106] Iteration 14000, lr = 1e-05
I0630 23:05:27.359622  6183 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.75
I0630 23:05:29.057809  6183 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 23:05:48.485626  6183 solver.cpp:290] Iteration 14100 (4.73342 iter/s, 21.1264s/100 iter), loss = 0.287912
I0630 23:05:48.485653  6183 solver.cpp:309]     Train net output #0: loss = 0.287912 (* 1 = 0.287912 loss)
I0630 23:05:48.485663  6183 sgd_solver.cpp:106] Iteration 14100, lr = 1e-05
I0630 23:06:07.717880  6183 solver.cpp:290] Iteration 14200 (5.19975 iter/s, 19.2317s/100 iter), loss = 0.0769771
I0630 23:06:07.717975  6183 solver.cpp:309]     Train net output #0: loss = 0.0769771 (* 1 = 0.0769771 loss)
I0630 23:06:07.717984  6183 sgd_solver.cpp:106] Iteration 14200, lr = 1e-05
I0630 23:06:27.584560  6183 solver.cpp:290] Iteration 14300 (5.03371 iter/s, 19.866s/100 iter), loss = 0.0950777
I0630 23:06:27.584585  6183 solver.cpp:309]     Train net output #0: loss = 0.0950777 (* 1 = 0.0950777 loss)
I0630 23:06:27.584592  6183 sgd_solver.cpp:106] Iteration 14300, lr = 1e-05
I0630 23:06:46.822844  6183 solver.cpp:290] Iteration 14400 (5.19812 iter/s, 19.2377s/100 iter), loss = 0.129255
I0630 23:06:46.822916  6183 solver.cpp:309]     Train net output #0: loss = 0.129255 (* 1 = 0.129255 loss)
I0630 23:06:46.822923  6183 sgd_solver.cpp:106] Iteration 14400, lr = 1e-05
I0630 23:07:06.100539  6183 solver.cpp:290] Iteration 14500 (5.1875 iter/s, 19.2771s/100 iter), loss = 0.167218
I0630 23:07:06.100566  6183 solver.cpp:309]     Train net output #0: loss = 0.167217 (* 1 = 0.167217 loss)
I0630 23:07:06.100574  6183 sgd_solver.cpp:106] Iteration 14500, lr = 1e-05
I0630 23:07:25.159857  6183 solver.cpp:290] Iteration 14600 (5.24693 iter/s, 19.0588s/100 iter), loss = 0.10043
I0630 23:07:25.159904  6183 solver.cpp:309]     Train net output #0: loss = 0.10043 (* 1 = 0.10043 loss)
I0630 23:07:25.159912  6183 sgd_solver.cpp:106] Iteration 14600, lr = 1e-05
I0630 23:07:44.294725  6183 solver.cpp:290] Iteration 14700 (5.22621 iter/s, 19.1343s/100 iter), loss = 0.152852
I0630 23:07:44.294750  6183 solver.cpp:309]     Train net output #0: loss = 0.152852 (* 1 = 0.152852 loss)
I0630 23:07:44.294757  6183 sgd_solver.cpp:106] Iteration 14700, lr = 1e-05
I0630 23:08:03.380723  6183 solver.cpp:290] Iteration 14800 (5.23959 iter/s, 19.0855s/100 iter), loss = 0.0837526
I0630 23:08:03.380772  6183 solver.cpp:309]     Train net output #0: loss = 0.0837525 (* 1 = 0.0837525 loss)
I0630 23:08:03.380779  6183 sgd_solver.cpp:106] Iteration 14800, lr = 1e-05
I0630 23:08:22.336107  6183 solver.cpp:290] Iteration 14900 (5.2757 iter/s, 18.9548s/100 iter), loss = 0.230844
I0630 23:08:22.336135  6183 solver.cpp:309]     Train net output #0: loss = 0.230844 (* 1 = 0.230844 loss)
I0630 23:08:22.336148  6183 sgd_solver.cpp:106] Iteration 14900, lr = 1e-05
I0630 23:08:41.251935  6183 solver.cpp:354] Sparsity after update:
I0630 23:08:41.316893  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 23:08:41.316911  6183 net.cpp:1851] conv1a_param_0(0.375) 
I0630 23:08:41.316920  6183 net.cpp:1851] conv1b_param_0(0.75) 
I0630 23:08:41.316922  6183 net.cpp:1851] ctx_conv1_param_0(0.629) 
I0630 23:08:41.316925  6183 net.cpp:1851] ctx_conv2_param_0(0.684) 
I0630 23:08:41.316926  6183 net.cpp:1851] ctx_conv3_param_0(0.624) 
I0630 23:08:41.316928  6183 net.cpp:1851] ctx_conv4_param_0(0.674) 
I0630 23:08:41.316931  6183 net.cpp:1851] ctx_final_param_0(0.375) 
I0630 23:08:41.316932  6183 net.cpp:1851] out3a_param_0(0.75) 
I0630 23:08:41.316934  6183 net.cpp:1851] out5a_param_0(0.75) 
I0630 23:08:41.316936  6183 net.cpp:1851] res2a_branch2a_param_0(0.75) 
I0630 23:08:41.316938  6183 net.cpp:1851] res2a_branch2b_param_0(0.75) 
I0630 23:08:41.316941  6183 net.cpp:1851] res3a_branch2a_param_0(0.75) 
I0630 23:08:41.316942  6183 net.cpp:1851] res3a_branch2b_param_0(0.75) 
I0630 23:08:41.316944  6183 net.cpp:1851] res4a_branch2a_param_0(0.75) 
I0630 23:08:41.316946  6183 net.cpp:1851] res4a_branch2b_param_0(0.75) 
I0630 23:08:41.316947  6183 net.cpp:1851] res5a_branch2a_param_0(0.75) 
I0630 23:08:41.316949  6183 net.cpp:1851] res5a_branch2b_param_0(0.75) 
I0630 23:08:41.316951  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.004e+06/2.69808e+06) 0.743
I0630 23:08:41.487812  6183 solver.cpp:290] Iteration 15000 (5.22162 iter/s, 19.1512s/100 iter), loss = 0.145521
I0630 23:08:41.487838  6183 solver.cpp:309]     Train net output #0: loss = 0.145521 (* 1 = 0.145521 loss)
I0630 23:08:41.487844  6183 sgd_solver.cpp:106] Iteration 15000, lr = 1e-05
I0630 23:08:41.488822  6183 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.8
I0630 23:08:43.273886  6183 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 23:09:02.161046  6183 solver.cpp:290] Iteration 15100 (4.83731 iter/s, 20.6727s/100 iter), loss = 0.153319
I0630 23:09:02.161067  6183 solver.cpp:309]     Train net output #0: loss = 0.153319 (* 1 = 0.153319 loss)
I0630 23:09:02.161074  6183 sgd_solver.cpp:106] Iteration 15100, lr = 1e-05
I0630 23:09:21.593365  6183 solver.cpp:290] Iteration 15200 (5.14621 iter/s, 19.4318s/100 iter), loss = 0.110469
I0630 23:09:21.593456  6183 solver.cpp:309]     Train net output #0: loss = 0.110469 (* 1 = 0.110469 loss)
I0630 23:09:21.593463  6183 sgd_solver.cpp:106] Iteration 15200, lr = 1e-05
I0630 23:09:40.680281  6183 solver.cpp:290] Iteration 15300 (5.23936 iter/s, 19.0863s/100 iter), loss = 0.182861
I0630 23:09:40.680310  6183 solver.cpp:309]     Train net output #0: loss = 0.182861 (* 1 = 0.182861 loss)
I0630 23:09:40.680318  6183 sgd_solver.cpp:106] Iteration 15300, lr = 1e-05
I0630 23:09:59.763525  6183 solver.cpp:290] Iteration 15400 (5.24035 iter/s, 19.0827s/100 iter), loss = 0.131533
I0630 23:09:59.763577  6183 solver.cpp:309]     Train net output #0: loss = 0.131533 (* 1 = 0.131533 loss)
I0630 23:09:59.763586  6183 sgd_solver.cpp:106] Iteration 15400, lr = 1e-05
I0630 23:10:18.887471  6183 solver.cpp:290] Iteration 15500 (5.2292 iter/s, 19.1234s/100 iter), loss = 0.0981063
I0630 23:10:18.887496  6183 solver.cpp:309]     Train net output #0: loss = 0.0981064 (* 1 = 0.0981064 loss)
I0630 23:10:18.887502  6183 sgd_solver.cpp:106] Iteration 15500, lr = 1e-05
I0630 23:10:38.025665  6183 solver.cpp:290] Iteration 15600 (5.2253 iter/s, 19.1377s/100 iter), loss = 0.228007
I0630 23:10:38.025774  6183 solver.cpp:309]     Train net output #0: loss = 0.228007 (* 1 = 0.228007 loss)
I0630 23:10:38.025784  6183 sgd_solver.cpp:106] Iteration 15600, lr = 1e-05
I0630 23:10:57.122635  6183 solver.cpp:290] Iteration 15700 (5.2366 iter/s, 19.0964s/100 iter), loss = 0.175053
I0630 23:10:57.122658  6183 solver.cpp:309]     Train net output #0: loss = 0.175053 (* 1 = 0.175053 loss)
I0630 23:10:57.122664  6183 sgd_solver.cpp:106] Iteration 15700, lr = 1e-05
I0630 23:11:16.154717  6183 solver.cpp:290] Iteration 15800 (5.25443 iter/s, 19.0316s/100 iter), loss = 0.169081
I0630 23:11:16.154811  6183 solver.cpp:309]     Train net output #0: loss = 0.169082 (* 1 = 0.169082 loss)
I0630 23:11:16.154822  6183 sgd_solver.cpp:106] Iteration 15800, lr = 1e-05
I0630 23:11:35.173915  6183 solver.cpp:290] Iteration 15900 (5.25801 iter/s, 19.0186s/100 iter), loss = 0.164171
I0630 23:11:35.173938  6183 solver.cpp:309]     Train net output #0: loss = 0.164172 (* 1 = 0.164172 loss)
I0630 23:11:35.173945  6183 sgd_solver.cpp:106] Iteration 15900, lr = 1e-05
I0630 23:11:54.105933  6183 solver.cpp:354] Sparsity after update:
I0630 23:11:54.107884  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 23:11:54.107893  6183 net.cpp:1851] conv1a_param_0(0.4) 
I0630 23:11:54.107900  6183 net.cpp:1851] conv1b_param_0(0.774) 
I0630 23:11:54.107903  6183 net.cpp:1851] ctx_conv1_param_0(0.63) 
I0630 23:11:54.107904  6183 net.cpp:1851] ctx_conv2_param_0(0.686) 
I0630 23:11:54.107906  6183 net.cpp:1851] ctx_conv3_param_0(0.626) 
I0630 23:11:54.107909  6183 net.cpp:1851] ctx_conv4_param_0(0.675) 
I0630 23:11:54.107911  6183 net.cpp:1851] ctx_final_param_0(0.4) 
I0630 23:11:54.107914  6183 net.cpp:1851] out3a_param_0(0.757) 
I0630 23:11:54.107916  6183 net.cpp:1851] out5a_param_0(0.784) 
I0630 23:11:54.107918  6183 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0630 23:11:54.107920  6183 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0630 23:11:54.107923  6183 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0630 23:11:54.107925  6183 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0630 23:11:54.107928  6183 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0630 23:11:54.107930  6183 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0630 23:11:54.107933  6183 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0630 23:11:54.107934  6183 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0630 23:11:54.107936  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.12736e+06/2.69808e+06) 0.788
I0630 23:11:54.108073  6183 solver.cpp:471] Iteration 16000, Testing net (#0)
I0630 23:13:32.552650  6183 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.918132
I0630 23:13:32.552769  6183 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.99229
I0630 23:13:32.552779  6183 solver.cpp:544]     Test net output #2: loss = 0.174071 (* 1 = 0.174071 loss)
I0630 23:13:32.772297  6183 solver.cpp:290] Iteration 16000 (0.850375 iter/s, 117.595s/100 iter), loss = 0.14701
I0630 23:13:32.772323  6183 solver.cpp:309]     Train net output #0: loss = 0.14701 (* 1 = 0.14701 loss)
I0630 23:13:32.772332  6183 sgd_solver.cpp:106] Iteration 16000, lr = 1e-05
I0630 23:13:51.760818  6183 solver.cpp:290] Iteration 16100 (5.26649 iter/s, 18.988s/100 iter), loss = 0.0842504
I0630 23:13:51.760843  6183 solver.cpp:309]     Train net output #0: loss = 0.0842506 (* 1 = 0.0842506 loss)
I0630 23:13:51.760852  6183 sgd_solver.cpp:106] Iteration 16100, lr = 1e-05
I0630 23:14:11.172372  6183 solver.cpp:290] Iteration 16200 (5.15172 iter/s, 19.411s/100 iter), loss = 0.168242
I0630 23:14:11.172456  6183 solver.cpp:309]     Train net output #0: loss = 0.168242 (* 1 = 0.168242 loss)
I0630 23:14:11.172466  6183 sgd_solver.cpp:106] Iteration 16200, lr = 1e-05
I0630 23:14:30.446300  6183 solver.cpp:290] Iteration 16300 (5.18852 iter/s, 19.2733s/100 iter), loss = 0.107081
I0630 23:14:30.446327  6183 solver.cpp:309]     Train net output #0: loss = 0.107081 (* 1 = 0.107081 loss)
I0630 23:14:30.446336  6183 sgd_solver.cpp:106] Iteration 16300, lr = 1e-05
I0630 23:14:49.333027  6183 solver.cpp:290] Iteration 16400 (5.29487 iter/s, 18.8862s/100 iter), loss = 0.13562
I0630 23:14:49.333079  6183 solver.cpp:309]     Train net output #0: loss = 0.13562 (* 1 = 0.13562 loss)
I0630 23:14:49.333086  6183 sgd_solver.cpp:106] Iteration 16400, lr = 1e-05
I0630 23:15:08.390040  6183 solver.cpp:290] Iteration 16500 (5.24757 iter/s, 19.0565s/100 iter), loss = 0.155575
I0630 23:15:08.390064  6183 solver.cpp:309]     Train net output #0: loss = 0.155576 (* 1 = 0.155576 loss)
I0630 23:15:08.390070  6183 sgd_solver.cpp:106] Iteration 16500, lr = 1e-05
I0630 23:15:27.448715  6183 solver.cpp:290] Iteration 16600 (5.2471 iter/s, 19.0581s/100 iter), loss = 0.12847
I0630 23:15:27.448755  6183 solver.cpp:309]     Train net output #0: loss = 0.12847 (* 1 = 0.12847 loss)
I0630 23:15:27.448761  6183 sgd_solver.cpp:106] Iteration 16600, lr = 1e-05
I0630 23:15:46.440413  6183 solver.cpp:290] Iteration 16700 (5.26561 iter/s, 18.9911s/100 iter), loss = 0.0858648
I0630 23:15:46.440440  6183 solver.cpp:309]     Train net output #0: loss = 0.085865 (* 1 = 0.085865 loss)
I0630 23:15:46.440449  6183 sgd_solver.cpp:106] Iteration 16700, lr = 1e-05
I0630 23:16:05.838793  6183 solver.cpp:290] Iteration 16800 (5.15521 iter/s, 19.3978s/100 iter), loss = 0.140188
I0630 23:16:05.838840  6183 solver.cpp:309]     Train net output #0: loss = 0.140189 (* 1 = 0.140189 loss)
I0630 23:16:05.838846  6183 sgd_solver.cpp:106] Iteration 16800, lr = 1e-05
I0630 23:16:24.967573  6183 solver.cpp:290] Iteration 16900 (5.22788 iter/s, 19.1282s/100 iter), loss = 0.137164
I0630 23:16:24.967597  6183 solver.cpp:309]     Train net output #0: loss = 0.137164 (* 1 = 0.137164 loss)
I0630 23:16:24.967604  6183 sgd_solver.cpp:106] Iteration 16900, lr = 1e-05
I0630 23:16:43.903265  6183 solver.cpp:354] Sparsity after update:
I0630 23:16:43.972296  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 23:16:43.972313  6183 net.cpp:1851] conv1a_param_0(0.4) 
I0630 23:16:43.972321  6183 net.cpp:1851] conv1b_param_0(0.774) 
I0630 23:16:43.972324  6183 net.cpp:1851] ctx_conv1_param_0(0.63) 
I0630 23:16:43.972326  6183 net.cpp:1851] ctx_conv2_param_0(0.686) 
I0630 23:16:43.972328  6183 net.cpp:1851] ctx_conv3_param_0(0.626) 
I0630 23:16:43.972331  6183 net.cpp:1851] ctx_conv4_param_0(0.675) 
I0630 23:16:43.972332  6183 net.cpp:1851] ctx_final_param_0(0.4) 
I0630 23:16:43.972334  6183 net.cpp:1851] out3a_param_0(0.757) 
I0630 23:16:43.972337  6183 net.cpp:1851] out5a_param_0(0.784) 
I0630 23:16:43.972337  6183 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0630 23:16:43.972340  6183 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0630 23:16:43.972342  6183 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0630 23:16:43.972343  6183 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0630 23:16:43.972347  6183 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0630 23:16:43.972349  6183 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0630 23:16:43.972352  6183 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0630 23:16:43.972355  6183 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0630 23:16:43.972359  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.12736e+06/2.69808e+06) 0.788
I0630 23:16:44.142774  6183 solver.cpp:290] Iteration 17000 (5.21522 iter/s, 19.1747s/100 iter), loss = 0.0791059
I0630 23:16:44.142798  6183 solver.cpp:309]     Train net output #0: loss = 0.079106 (* 1 = 0.079106 loss)
I0630 23:16:44.142805  6183 sgd_solver.cpp:106] Iteration 17000, lr = 1e-05
I0630 23:17:03.503875  6183 solver.cpp:290] Iteration 17100 (5.16514 iter/s, 19.3606s/100 iter), loss = 0.1106
I0630 23:17:03.503902  6183 solver.cpp:309]     Train net output #0: loss = 0.1106 (* 1 = 0.1106 loss)
I0630 23:17:03.503911  6183 sgd_solver.cpp:106] Iteration 17100, lr = 1e-05
I0630 23:17:22.898568  6183 solver.cpp:290] Iteration 17200 (5.15619 iter/s, 19.3941s/100 iter), loss = 0.116436
I0630 23:17:22.898627  6183 solver.cpp:309]     Train net output #0: loss = 0.116437 (* 1 = 0.116437 loss)
I0630 23:17:22.898636  6183 sgd_solver.cpp:106] Iteration 17200, lr = 1e-05
I0630 23:17:41.912798  6183 solver.cpp:290] Iteration 17300 (5.25937 iter/s, 19.0137s/100 iter), loss = 0.10652
I0630 23:17:41.912822  6183 solver.cpp:309]     Train net output #0: loss = 0.106521 (* 1 = 0.106521 loss)
I0630 23:17:41.912828  6183 sgd_solver.cpp:106] Iteration 17300, lr = 1e-05
I0630 23:18:01.223321  6183 solver.cpp:290] Iteration 17400 (5.17867 iter/s, 19.31s/100 iter), loss = 0.152148
I0630 23:18:01.223373  6183 solver.cpp:309]     Train net output #0: loss = 0.152148 (* 1 = 0.152148 loss)
I0630 23:18:01.223381  6183 sgd_solver.cpp:106] Iteration 17400, lr = 1e-05
I0630 23:18:20.265508  6183 solver.cpp:290] Iteration 17500 (5.25165 iter/s, 19.0416s/100 iter), loss = 0.101378
I0630 23:18:20.265560  6183 solver.cpp:309]     Train net output #0: loss = 0.101378 (* 1 = 0.101378 loss)
I0630 23:18:20.265573  6183 sgd_solver.cpp:106] Iteration 17500, lr = 1e-05
I0630 23:18:39.306488  6183 solver.cpp:290] Iteration 17600 (5.25198 iter/s, 19.0404s/100 iter), loss = 0.116941
I0630 23:18:39.306541  6183 solver.cpp:309]     Train net output #0: loss = 0.116941 (* 1 = 0.116941 loss)
I0630 23:18:39.306548  6183 sgd_solver.cpp:106] Iteration 17600, lr = 1e-05
I0630 23:18:58.376170  6183 solver.cpp:290] Iteration 17700 (5.24408 iter/s, 19.0691s/100 iter), loss = 0.188421
I0630 23:18:58.376199  6183 solver.cpp:309]     Train net output #0: loss = 0.188421 (* 1 = 0.188421 loss)
I0630 23:18:58.376206  6183 sgd_solver.cpp:106] Iteration 17700, lr = 1e-05
I0630 23:19:17.452409  6183 solver.cpp:290] Iteration 17800 (5.24227 iter/s, 19.0757s/100 iter), loss = 0.219503
I0630 23:19:17.452473  6183 solver.cpp:309]     Train net output #0: loss = 0.219503 (* 1 = 0.219503 loss)
I0630 23:19:17.452481  6183 sgd_solver.cpp:106] Iteration 17800, lr = 1e-05
I0630 23:19:36.536345  6183 solver.cpp:290] Iteration 17900 (5.24017 iter/s, 19.0834s/100 iter), loss = 0.162081
I0630 23:19:36.536368  6183 solver.cpp:309]     Train net output #0: loss = 0.162081 (* 1 = 0.162081 loss)
I0630 23:19:36.536375  6183 sgd_solver.cpp:106] Iteration 17900, lr = 1e-05
I0630 23:19:55.318792  6183 solver.cpp:354] Sparsity after update:
I0630 23:19:55.320878  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 23:19:55.320885  6183 net.cpp:1851] conv1a_param_0(0.4) 
I0630 23:19:55.320893  6183 net.cpp:1851] conv1b_param_0(0.774) 
I0630 23:19:55.320894  6183 net.cpp:1851] ctx_conv1_param_0(0.63) 
I0630 23:19:55.320896  6183 net.cpp:1851] ctx_conv2_param_0(0.686) 
I0630 23:19:55.320899  6183 net.cpp:1851] ctx_conv3_param_0(0.626) 
I0630 23:19:55.320900  6183 net.cpp:1851] ctx_conv4_param_0(0.675) 
I0630 23:19:55.320902  6183 net.cpp:1851] ctx_final_param_0(0.4) 
I0630 23:19:55.320904  6183 net.cpp:1851] out3a_param_0(0.757) 
I0630 23:19:55.320906  6183 net.cpp:1851] out5a_param_0(0.784) 
I0630 23:19:55.320909  6183 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0630 23:19:55.320910  6183 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0630 23:19:55.320912  6183 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0630 23:19:55.320914  6183 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0630 23:19:55.320916  6183 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0630 23:19:55.320919  6183 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0630 23:19:55.320920  6183 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0630 23:19:55.320922  6183 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0630 23:19:55.320924  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.12736e+06/2.69808e+06) 0.788
I0630 23:19:55.321056  6183 solver.cpp:471] Iteration 18000, Testing net (#0)
I0630 23:21:33.417742  6183 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.920677
I0630 23:21:33.417825  6183 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.99282
I0630 23:21:33.417832  6183 solver.cpp:544]     Test net output #2: loss = 0.166156 (* 1 = 0.166156 loss)
I0630 23:21:33.645488  6183 solver.cpp:290] Iteration 18000 (0.853927 iter/s, 117.106s/100 iter), loss = 0.118446
I0630 23:21:33.645516  6183 solver.cpp:309]     Train net output #0: loss = 0.118446 (* 1 = 0.118446 loss)
I0630 23:21:33.645525  6183 sgd_solver.cpp:106] Iteration 18000, lr = 1e-05
I0630 23:21:52.578542  6183 solver.cpp:290] Iteration 18100 (5.28192 iter/s, 18.9325s/100 iter), loss = 0.191272
I0630 23:21:52.578565  6183 solver.cpp:309]     Train net output #0: loss = 0.191272 (* 1 = 0.191272 loss)
I0630 23:21:52.578572  6183 sgd_solver.cpp:106] Iteration 18100, lr = 1e-05
I0630 23:22:11.629639  6183 solver.cpp:290] Iteration 18200 (5.24919 iter/s, 19.0506s/100 iter), loss = 0.0827249
I0630 23:22:11.629688  6183 solver.cpp:309]     Train net output #0: loss = 0.0827252 (* 1 = 0.0827252 loss)
I0630 23:22:11.629696  6183 sgd_solver.cpp:106] Iteration 18200, lr = 1e-05
I0630 23:22:30.631809  6183 solver.cpp:290] Iteration 18300 (5.26271 iter/s, 19.0016s/100 iter), loss = 0.15024
I0630 23:22:30.631832  6183 solver.cpp:309]     Train net output #0: loss = 0.15024 (* 1 = 0.15024 loss)
I0630 23:22:30.631839  6183 sgd_solver.cpp:106] Iteration 18300, lr = 1e-05
I0630 23:22:49.665839  6183 solver.cpp:290] Iteration 18400 (5.25389 iter/s, 19.0335s/100 iter), loss = 0.194017
I0630 23:22:49.665886  6183 solver.cpp:309]     Train net output #0: loss = 0.194018 (* 1 = 0.194018 loss)
I0630 23:22:49.665894  6183 sgd_solver.cpp:106] Iteration 18400, lr = 1e-05
I0630 23:23:08.715489  6183 solver.cpp:290] Iteration 18500 (5.24959 iter/s, 19.0491s/100 iter), loss = 0.153379
I0630 23:23:08.715512  6183 solver.cpp:309]     Train net output #0: loss = 0.15338 (* 1 = 0.15338 loss)
I0630 23:23:08.715519  6183 sgd_solver.cpp:106] Iteration 18500, lr = 1e-05
I0630 23:23:27.824878  6183 solver.cpp:290] Iteration 18600 (5.23318 iter/s, 19.1089s/100 iter), loss = 0.104525
I0630 23:23:27.825002  6183 solver.cpp:309]     Train net output #0: loss = 0.104525 (* 1 = 0.104525 loss)
I0630 23:23:27.825013  6183 sgd_solver.cpp:106] Iteration 18600, lr = 1e-05
I0630 23:23:46.910218  6183 solver.cpp:290] Iteration 18700 (5.2398 iter/s, 19.0847s/100 iter), loss = 0.14032
I0630 23:23:46.910240  6183 solver.cpp:309]     Train net output #0: loss = 0.14032 (* 1 = 0.14032 loss)
I0630 23:23:46.910246  6183 sgd_solver.cpp:106] Iteration 18700, lr = 1e-05
I0630 23:24:05.986688  6183 solver.cpp:290] Iteration 18800 (5.24221 iter/s, 19.0759s/100 iter), loss = 0.12578
I0630 23:24:05.986759  6183 solver.cpp:309]     Train net output #0: loss = 0.12578 (* 1 = 0.12578 loss)
I0630 23:24:05.986766  6183 sgd_solver.cpp:106] Iteration 18800, lr = 1e-05
I0630 23:24:24.984112  6183 solver.cpp:290] Iteration 18900 (5.26403 iter/s, 18.9968s/100 iter), loss = 0.116361
I0630 23:24:24.984133  6183 solver.cpp:309]     Train net output #0: loss = 0.116361 (* 1 = 0.116361 loss)
I0630 23:24:24.984140  6183 sgd_solver.cpp:106] Iteration 18900, lr = 1e-05
I0630 23:24:43.842931  6183 solver.cpp:354] Sparsity after update:
I0630 23:24:43.917086  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 23:24:43.917104  6183 net.cpp:1851] conv1a_param_0(0.4) 
I0630 23:24:43.917111  6183 net.cpp:1851] conv1b_param_0(0.774) 
I0630 23:24:43.917114  6183 net.cpp:1851] ctx_conv1_param_0(0.63) 
I0630 23:24:43.917116  6183 net.cpp:1851] ctx_conv2_param_0(0.686) 
I0630 23:24:43.917119  6183 net.cpp:1851] ctx_conv3_param_0(0.626) 
I0630 23:24:43.917120  6183 net.cpp:1851] ctx_conv4_param_0(0.675) 
I0630 23:24:43.917131  6183 net.cpp:1851] ctx_final_param_0(0.4) 
I0630 23:24:43.917138  6183 net.cpp:1851] out3a_param_0(0.757) 
I0630 23:24:43.917141  6183 net.cpp:1851] out5a_param_0(0.784) 
I0630 23:24:43.917145  6183 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0630 23:24:43.917148  6183 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0630 23:24:43.917151  6183 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0630 23:24:43.917153  6183 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0630 23:24:43.917157  6183 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0630 23:24:43.917160  6183 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0630 23:24:43.917163  6183 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0630 23:24:43.917167  6183 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0630 23:24:43.917171  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.12736e+06/2.69808e+06) 0.788
I0630 23:24:44.087924  6183 solver.cpp:290] Iteration 19000 (5.2347 iter/s, 19.1033s/100 iter), loss = 0.128935
I0630 23:24:44.087947  6183 solver.cpp:309]     Train net output #0: loss = 0.128935 (* 1 = 0.128935 loss)
I0630 23:24:44.087954  6183 sgd_solver.cpp:106] Iteration 19000, lr = 1e-05
I0630 23:25:03.177368  6183 solver.cpp:290] Iteration 19100 (5.23864 iter/s, 19.0889s/100 iter), loss = 0.215027
I0630 23:25:03.177392  6183 solver.cpp:309]     Train net output #0: loss = 0.215027 (* 1 = 0.215027 loss)
I0630 23:25:03.177398  6183 sgd_solver.cpp:106] Iteration 19100, lr = 1e-05
I0630 23:25:22.341112  6183 solver.cpp:290] Iteration 19200 (5.21833 iter/s, 19.1632s/100 iter), loss = 0.0862525
I0630 23:25:22.341150  6183 solver.cpp:309]     Train net output #0: loss = 0.0862527 (* 1 = 0.0862527 loss)
I0630 23:25:22.341157  6183 sgd_solver.cpp:106] Iteration 19200, lr = 1e-05
I0630 23:25:41.504566  6183 solver.cpp:290] Iteration 19300 (5.21842 iter/s, 19.1629s/100 iter), loss = 0.201211
I0630 23:25:41.504593  6183 solver.cpp:309]     Train net output #0: loss = 0.201211 (* 1 = 0.201211 loss)
I0630 23:25:41.504602  6183 sgd_solver.cpp:106] Iteration 19300, lr = 1e-05
I0630 23:26:00.667503  6183 solver.cpp:290] Iteration 19400 (5.21855 iter/s, 19.1624s/100 iter), loss = 0.162197
I0630 23:26:00.667582  6183 solver.cpp:309]     Train net output #0: loss = 0.162197 (* 1 = 0.162197 loss)
I0630 23:26:00.667589  6183 sgd_solver.cpp:106] Iteration 19400, lr = 1e-05
I0630 23:26:19.971879  6183 solver.cpp:290] Iteration 19500 (5.18033 iter/s, 19.3038s/100 iter), loss = 0.110757
I0630 23:26:19.971912  6183 solver.cpp:309]     Train net output #0: loss = 0.110757 (* 1 = 0.110757 loss)
I0630 23:26:19.971921  6183 sgd_solver.cpp:106] Iteration 19500, lr = 1e-05
I0630 23:26:39.304280  6183 solver.cpp:290] Iteration 19600 (5.17281 iter/s, 19.3319s/100 iter), loss = 0.117844
I0630 23:26:39.304380  6183 solver.cpp:309]     Train net output #0: loss = 0.117844 (* 1 = 0.117844 loss)
I0630 23:26:39.304396  6183 sgd_solver.cpp:106] Iteration 19600, lr = 1e-05
I0630 23:26:58.363487  6183 solver.cpp:290] Iteration 19700 (5.24697 iter/s, 19.0586s/100 iter), loss = 0.08412
I0630 23:26:58.363509  6183 solver.cpp:309]     Train net output #0: loss = 0.0841202 (* 1 = 0.0841202 loss)
I0630 23:26:58.363518  6183 sgd_solver.cpp:106] Iteration 19700, lr = 1e-05
I0630 23:27:17.641175  6183 solver.cpp:290] Iteration 19800 (5.18749 iter/s, 19.2772s/100 iter), loss = 0.105927
I0630 23:27:17.641263  6183 solver.cpp:309]     Train net output #0: loss = 0.105927 (* 1 = 0.105927 loss)
I0630 23:27:17.641275  6183 sgd_solver.cpp:106] Iteration 19800, lr = 1e-05
I0630 23:27:36.986711  6183 solver.cpp:290] Iteration 19900 (5.16931 iter/s, 19.3449s/100 iter), loss = 0.190298
I0630 23:27:36.986737  6183 solver.cpp:309]     Train net output #0: loss = 0.190298 (* 1 = 0.190298 loss)
I0630 23:27:36.986747  6183 sgd_solver.cpp:106] Iteration 19900, lr = 1e-05
I0630 23:27:55.880105  6183 solver.cpp:598] Snapshotting to binary proto file training/cityscapes20_jsegnet21v2_2017-06-30_19-26-17/sparse/cityscapes20_jsegnet21v2_iter_20000.caffemodel
I0630 23:27:55.944000  6183 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cityscapes20_jsegnet21v2_2017-06-30_19-26-17/sparse/cityscapes20_jsegnet21v2_iter_20000.solverstate
I0630 23:27:55.964663  6183 solver.cpp:354] Sparsity after update:
I0630 23:27:55.966459  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 23:27:55.966470  6183 net.cpp:1851] conv1a_param_0(0.4) 
I0630 23:27:55.966478  6183 net.cpp:1851] conv1b_param_0(0.774) 
I0630 23:27:55.966481  6183 net.cpp:1851] ctx_conv1_param_0(0.63) 
I0630 23:27:55.966483  6183 net.cpp:1851] ctx_conv2_param_0(0.686) 
I0630 23:27:55.966486  6183 net.cpp:1851] ctx_conv3_param_0(0.626) 
I0630 23:27:55.966488  6183 net.cpp:1851] ctx_conv4_param_0(0.675) 
I0630 23:27:55.966491  6183 net.cpp:1851] ctx_final_param_0(0.4) 
I0630 23:27:55.966493  6183 net.cpp:1851] out3a_param_0(0.757) 
I0630 23:27:55.966496  6183 net.cpp:1851] out5a_param_0(0.784) 
I0630 23:27:55.966500  6183 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0630 23:27:55.966501  6183 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0630 23:27:55.966505  6183 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0630 23:27:55.966506  6183 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0630 23:27:55.966511  6183 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0630 23:27:55.966513  6183 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0630 23:27:55.966516  6183 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0630 23:27:55.966521  6183 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0630 23:27:55.966523  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.12736e+06/2.69808e+06) 0.788
I0630 23:27:55.966786  6183 solver.cpp:471] Iteration 20000, Testing net (#0)
I0630 23:29:33.596391  6183 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.922686
I0630 23:29:33.596483  6183 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.993074
I0630 23:29:33.596490  6183 solver.cpp:544]     Test net output #2: loss = 0.162889 (* 1 = 0.162889 loss)
I0630 23:29:33.829605  6183 solver.cpp:290] Iteration 20000 (0.855873 iter/s, 116.84s/100 iter), loss = 0.106042
I0630 23:29:33.829632  6183 solver.cpp:309]     Train net output #0: loss = 0.106042 (* 1 = 0.106042 loss)
I0630 23:29:33.829640  6183 sgd_solver.cpp:106] Iteration 20000, lr = 1e-05
I0630 23:29:52.943260  6183 solver.cpp:290] Iteration 20100 (5.23201 iter/s, 19.1131s/100 iter), loss = 0.102775
I0630 23:29:52.943284  6183 solver.cpp:309]     Train net output #0: loss = 0.102775 (* 1 = 0.102775 loss)
I0630 23:29:52.943290  6183 sgd_solver.cpp:106] Iteration 20100, lr = 1e-05
I0630 23:30:12.060621  6183 solver.cpp:290] Iteration 20200 (5.23099 iter/s, 19.1168s/100 iter), loss = 0.254848
I0630 23:30:12.060712  6183 solver.cpp:309]     Train net output #0: loss = 0.254848 (* 1 = 0.254848 loss)
I0630 23:30:12.060724  6183 sgd_solver.cpp:106] Iteration 20200, lr = 1e-05
I0630 23:30:31.086813  6183 solver.cpp:290] Iteration 20300 (5.25608 iter/s, 19.0256s/100 iter), loss = 0.118165
I0630 23:30:31.086836  6183 solver.cpp:309]     Train net output #0: loss = 0.118166 (* 1 = 0.118166 loss)
I0630 23:30:31.086843  6183 sgd_solver.cpp:106] Iteration 20300, lr = 1e-05
I0630 23:30:50.362169  6183 solver.cpp:290] Iteration 20400 (5.18812 iter/s, 19.2748s/100 iter), loss = 0.154691
I0630 23:30:50.362247  6183 solver.cpp:309]     Train net output #0: loss = 0.154691 (* 1 = 0.154691 loss)
I0630 23:30:50.362256  6183 sgd_solver.cpp:106] Iteration 20400, lr = 1e-05
I0630 23:31:09.465854  6183 solver.cpp:290] Iteration 20500 (5.23475 iter/s, 19.1031s/100 iter), loss = 0.104798
I0630 23:31:09.465880  6183 solver.cpp:309]     Train net output #0: loss = 0.104798 (* 1 = 0.104798 loss)
I0630 23:31:09.465889  6183 sgd_solver.cpp:106] Iteration 20500, lr = 1e-05
I0630 23:31:28.571815  6183 solver.cpp:290] Iteration 20600 (5.23412 iter/s, 19.1054s/100 iter), loss = 0.152037
I0630 23:31:28.571904  6183 solver.cpp:309]     Train net output #0: loss = 0.152037 (* 1 = 0.152037 loss)
I0630 23:31:28.571914  6183 sgd_solver.cpp:106] Iteration 20600, lr = 1e-05
I0630 23:31:47.626900  6183 solver.cpp:290] Iteration 20700 (5.24811 iter/s, 19.0545s/100 iter), loss = 0.12621
I0630 23:31:47.626926  6183 solver.cpp:309]     Train net output #0: loss = 0.12621 (* 1 = 0.12621 loss)
I0630 23:31:47.626935  6183 sgd_solver.cpp:106] Iteration 20700, lr = 1e-05
I0630 23:32:06.528376  6183 solver.cpp:290] Iteration 20800 (5.29074 iter/s, 18.9009s/100 iter), loss = 0.105916
I0630 23:32:06.528455  6183 solver.cpp:309]     Train net output #0: loss = 0.105916 (* 1 = 0.105916 loss)
I0630 23:32:06.528465  6183 sgd_solver.cpp:106] Iteration 20800, lr = 1e-05
I0630 23:32:25.663877  6183 solver.cpp:290] Iteration 20900 (5.22605 iter/s, 19.1349s/100 iter), loss = 0.139631
I0630 23:32:25.663902  6183 solver.cpp:309]     Train net output #0: loss = 0.139631 (* 1 = 0.139631 loss)
I0630 23:32:25.663910  6183 sgd_solver.cpp:106] Iteration 20900, lr = 1e-05
I0630 23:32:44.537457  6183 solver.cpp:354] Sparsity after update:
I0630 23:32:44.609839  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 23:32:44.609856  6183 net.cpp:1851] conv1a_param_0(0.4) 
I0630 23:32:44.609865  6183 net.cpp:1851] conv1b_param_0(0.774) 
I0630 23:32:44.609868  6183 net.cpp:1851] ctx_conv1_param_0(0.63) 
I0630 23:32:44.609869  6183 net.cpp:1851] ctx_conv2_param_0(0.686) 
I0630 23:32:44.609871  6183 net.cpp:1851] ctx_conv3_param_0(0.626) 
I0630 23:32:44.609874  6183 net.cpp:1851] ctx_conv4_param_0(0.675) 
I0630 23:32:44.609875  6183 net.cpp:1851] ctx_final_param_0(0.4) 
I0630 23:32:44.609877  6183 net.cpp:1851] out3a_param_0(0.757) 
I0630 23:32:44.609879  6183 net.cpp:1851] out5a_param_0(0.784) 
I0630 23:32:44.609881  6183 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0630 23:32:44.609884  6183 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0630 23:32:44.609887  6183 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0630 23:32:44.609889  6183 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0630 23:32:44.609892  6183 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0630 23:32:44.609894  6183 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0630 23:32:44.609896  6183 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0630 23:32:44.609899  6183 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0630 23:32:44.609901  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.12736e+06/2.69808e+06) 0.788
I0630 23:32:44.781489  6183 solver.cpp:290] Iteration 21000 (5.23092 iter/s, 19.1171s/100 iter), loss = 0.173716
I0630 23:32:44.781513  6183 solver.cpp:309]     Train net output #0: loss = 0.173717 (* 1 = 0.173717 loss)
I0630 23:32:44.781522  6183 sgd_solver.cpp:106] Iteration 21000, lr = 1e-05
I0630 23:33:03.873744  6183 solver.cpp:290] Iteration 21100 (5.23787 iter/s, 19.0917s/100 iter), loss = 0.161753
I0630 23:33:03.873765  6183 solver.cpp:309]     Train net output #0: loss = 0.161753 (* 1 = 0.161753 loss)
I0630 23:33:03.873772  6183 sgd_solver.cpp:106] Iteration 21100, lr = 1e-05
I0630 23:33:23.115752  6183 solver.cpp:290] Iteration 21200 (5.19711 iter/s, 19.2415s/100 iter), loss = 0.107395
I0630 23:33:23.115823  6183 solver.cpp:309]     Train net output #0: loss = 0.107396 (* 1 = 0.107396 loss)
I0630 23:33:23.115831  6183 sgd_solver.cpp:106] Iteration 21200, lr = 1e-05
I0630 23:33:42.313567  6183 solver.cpp:290] Iteration 21300 (5.20908 iter/s, 19.1972s/100 iter), loss = 0.189351
I0630 23:33:42.313591  6183 solver.cpp:309]     Train net output #0: loss = 0.189352 (* 1 = 0.189352 loss)
I0630 23:33:42.313598  6183 sgd_solver.cpp:106] Iteration 21300, lr = 1e-05
I0630 23:34:01.530546  6183 solver.cpp:290] Iteration 21400 (5.20388 iter/s, 19.2164s/100 iter), loss = 0.133369
I0630 23:34:01.530622  6183 solver.cpp:309]     Train net output #0: loss = 0.133369 (* 1 = 0.133369 loss)
I0630 23:34:01.530630  6183 sgd_solver.cpp:106] Iteration 21400, lr = 1e-05
I0630 23:34:20.901052  6183 solver.cpp:290] Iteration 21500 (5.16264 iter/s, 19.3699s/100 iter), loss = 0.129442
I0630 23:34:20.901077  6183 solver.cpp:309]     Train net output #0: loss = 0.129442 (* 1 = 0.129442 loss)
I0630 23:34:20.901087  6183 sgd_solver.cpp:106] Iteration 21500, lr = 1e-05
I0630 23:34:39.918730  6183 solver.cpp:290] Iteration 21600 (5.25841 iter/s, 19.0172s/100 iter), loss = 0.163995
I0630 23:34:39.918784  6183 solver.cpp:309]     Train net output #0: loss = 0.163995 (* 1 = 0.163995 loss)
I0630 23:34:39.918793  6183 sgd_solver.cpp:106] Iteration 21600, lr = 1e-05
I0630 23:34:59.075197  6183 solver.cpp:290] Iteration 21700 (5.22032 iter/s, 19.1559s/100 iter), loss = 0.086914
I0630 23:34:59.075220  6183 solver.cpp:309]     Train net output #0: loss = 0.0869142 (* 1 = 0.0869142 loss)
I0630 23:34:59.075227  6183 sgd_solver.cpp:106] Iteration 21700, lr = 1e-05
I0630 23:35:18.108088  6183 solver.cpp:290] Iteration 21800 (5.25421 iter/s, 19.0324s/100 iter), loss = 0.110905
I0630 23:35:18.108170  6183 solver.cpp:309]     Train net output #0: loss = 0.110905 (* 1 = 0.110905 loss)
I0630 23:35:18.108180  6183 sgd_solver.cpp:106] Iteration 21800, lr = 1e-05
I0630 23:35:37.240037  6183 solver.cpp:290] Iteration 21900 (5.22702 iter/s, 19.1314s/100 iter), loss = 0.0675026
I0630 23:35:37.240061  6183 solver.cpp:309]     Train net output #0: loss = 0.0675028 (* 1 = 0.0675028 loss)
I0630 23:35:37.240067  6183 sgd_solver.cpp:106] Iteration 21900, lr = 1e-05
I0630 23:35:56.204010  6183 solver.cpp:354] Sparsity after update:
I0630 23:35:56.205930  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 23:35:56.205937  6183 net.cpp:1851] conv1a_param_0(0.4) 
I0630 23:35:56.205945  6183 net.cpp:1851] conv1b_param_0(0.774) 
I0630 23:35:56.205947  6183 net.cpp:1851] ctx_conv1_param_0(0.63) 
I0630 23:35:56.205950  6183 net.cpp:1851] ctx_conv2_param_0(0.686) 
I0630 23:35:56.205951  6183 net.cpp:1851] ctx_conv3_param_0(0.626) 
I0630 23:35:56.205953  6183 net.cpp:1851] ctx_conv4_param_0(0.675) 
I0630 23:35:56.205955  6183 net.cpp:1851] ctx_final_param_0(0.4) 
I0630 23:35:56.205957  6183 net.cpp:1851] out3a_param_0(0.757) 
I0630 23:35:56.205960  6183 net.cpp:1851] out5a_param_0(0.784) 
I0630 23:35:56.205961  6183 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0630 23:35:56.205963  6183 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0630 23:35:56.205965  6183 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0630 23:35:56.205967  6183 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0630 23:35:56.205970  6183 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0630 23:35:56.205971  6183 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0630 23:35:56.205973  6183 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0630 23:35:56.205976  6183 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0630 23:35:56.205977  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.12736e+06/2.69808e+06) 0.788
I0630 23:35:56.206113  6183 solver.cpp:471] Iteration 22000, Testing net (#0)
I0630 23:37:33.325731  6183 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.923743
I0630 23:37:33.325837  6183 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.993272
I0630 23:37:33.325845  6183 solver.cpp:544]     Test net output #2: loss = 0.158933 (* 1 = 0.158933 loss)
I0630 23:37:33.534974  6183 solver.cpp:290] Iteration 22000 (0.859905 iter/s, 116.292s/100 iter), loss = 0.119662
I0630 23:37:33.535002  6183 solver.cpp:309]     Train net output #0: loss = 0.119662 (* 1 = 0.119662 loss)
I0630 23:37:33.535012  6183 sgd_solver.cpp:106] Iteration 22000, lr = 1e-05
I0630 23:37:52.744016  6183 solver.cpp:290] Iteration 22100 (5.20603 iter/s, 19.2085s/100 iter), loss = 0.13355
I0630 23:37:52.744040  6183 solver.cpp:309]     Train net output #0: loss = 0.133551 (* 1 = 0.133551 loss)
I0630 23:37:52.744046  6183 sgd_solver.cpp:106] Iteration 22100, lr = 1e-05
I0630 23:38:11.963317  6183 solver.cpp:290] Iteration 22200 (5.20325 iter/s, 19.2188s/100 iter), loss = 0.139913
I0630 23:38:11.963402  6183 solver.cpp:309]     Train net output #0: loss = 0.139913 (* 1 = 0.139913 loss)
I0630 23:38:11.963413  6183 sgd_solver.cpp:106] Iteration 22200, lr = 1e-05
I0630 23:38:31.074694  6183 solver.cpp:290] Iteration 22300 (5.23265 iter/s, 19.1108s/100 iter), loss = 0.128327
I0630 23:38:31.074718  6183 solver.cpp:309]     Train net output #0: loss = 0.128327 (* 1 = 0.128327 loss)
I0630 23:38:31.074725  6183 sgd_solver.cpp:106] Iteration 22300, lr = 1e-05
I0630 23:38:50.082636  6183 solver.cpp:290] Iteration 22400 (5.26111 iter/s, 19.0074s/100 iter), loss = 0.113436
I0630 23:38:50.082715  6183 solver.cpp:309]     Train net output #0: loss = 0.113436 (* 1 = 0.113436 loss)
I0630 23:38:50.082721  6183 sgd_solver.cpp:106] Iteration 22400, lr = 1e-05
I0630 23:39:09.015527  6183 solver.cpp:290] Iteration 22500 (5.28198 iter/s, 18.9323s/100 iter), loss = 0.0798705
I0630 23:39:09.015550  6183 solver.cpp:309]     Train net output #0: loss = 0.0798707 (* 1 = 0.0798707 loss)
I0630 23:39:09.015558  6183 sgd_solver.cpp:106] Iteration 22500, lr = 1e-05
I0630 23:39:28.355583  6183 solver.cpp:290] Iteration 22600 (5.17076 iter/s, 19.3395s/100 iter), loss = 0.230824
I0630 23:39:28.355630  6183 solver.cpp:309]     Train net output #0: loss = 0.230824 (* 1 = 0.230824 loss)
I0630 23:39:28.355638  6183 sgd_solver.cpp:106] Iteration 22600, lr = 1e-05
I0630 23:39:47.376857  6183 solver.cpp:290] Iteration 22700 (5.25742 iter/s, 19.0207s/100 iter), loss = 0.0751275
I0630 23:39:47.376881  6183 solver.cpp:309]     Train net output #0: loss = 0.0751278 (* 1 = 0.0751278 loss)
I0630 23:39:47.376888  6183 sgd_solver.cpp:106] Iteration 22700, lr = 1e-05
I0630 23:40:06.535131  6183 solver.cpp:290] Iteration 22800 (5.21982 iter/s, 19.1577s/100 iter), loss = 0.169822
I0630 23:40:06.535218  6183 solver.cpp:309]     Train net output #0: loss = 0.169823 (* 1 = 0.169823 loss)
I0630 23:40:06.535228  6183 sgd_solver.cpp:106] Iteration 22800, lr = 1e-05
I0630 23:40:25.569236  6183 solver.cpp:290] Iteration 22900 (5.25389 iter/s, 19.0335s/100 iter), loss = 0.113699
I0630 23:40:25.569259  6183 solver.cpp:309]     Train net output #0: loss = 0.113699 (* 1 = 0.113699 loss)
I0630 23:40:25.569267  6183 sgd_solver.cpp:106] Iteration 22900, lr = 1e-05
I0630 23:40:44.545735  6183 solver.cpp:354] Sparsity after update:
I0630 23:40:44.615218  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 23:40:44.615236  6183 net.cpp:1851] conv1a_param_0(0.4) 
I0630 23:40:44.615243  6183 net.cpp:1851] conv1b_param_0(0.774) 
I0630 23:40:44.615245  6183 net.cpp:1851] ctx_conv1_param_0(0.63) 
I0630 23:40:44.615247  6183 net.cpp:1851] ctx_conv2_param_0(0.686) 
I0630 23:40:44.615249  6183 net.cpp:1851] ctx_conv3_param_0(0.626) 
I0630 23:40:44.615252  6183 net.cpp:1851] ctx_conv4_param_0(0.675) 
I0630 23:40:44.615253  6183 net.cpp:1851] ctx_final_param_0(0.4) 
I0630 23:40:44.615255  6183 net.cpp:1851] out3a_param_0(0.757) 
I0630 23:40:44.615257  6183 net.cpp:1851] out5a_param_0(0.784) 
I0630 23:40:44.615259  6183 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0630 23:40:44.615262  6183 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0630 23:40:44.615263  6183 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0630 23:40:44.615265  6183 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0630 23:40:44.615267  6183 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0630 23:40:44.615269  6183 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0630 23:40:44.615272  6183 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0630 23:40:44.615273  6183 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0630 23:40:44.615275  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.12736e+06/2.69808e+06) 0.788
I0630 23:40:44.790849  6183 solver.cpp:290] Iteration 23000 (5.20262 iter/s, 19.2211s/100 iter), loss = 0.213282
I0630 23:40:44.790874  6183 solver.cpp:309]     Train net output #0: loss = 0.213283 (* 1 = 0.213283 loss)
I0630 23:40:44.790880  6183 sgd_solver.cpp:106] Iteration 23000, lr = 1e-05
I0630 23:41:03.877699  6183 solver.cpp:290] Iteration 23100 (5.23936 iter/s, 19.0863s/100 iter), loss = 0.117535
I0630 23:41:03.877723  6183 solver.cpp:309]     Train net output #0: loss = 0.117536 (* 1 = 0.117536 loss)
I0630 23:41:03.877730  6183 sgd_solver.cpp:106] Iteration 23100, lr = 1e-05
I0630 23:41:22.912995  6183 solver.cpp:290] Iteration 23200 (5.25355 iter/s, 19.0348s/100 iter), loss = 0.126882
I0630 23:41:22.913092  6183 solver.cpp:309]     Train net output #0: loss = 0.126883 (* 1 = 0.126883 loss)
I0630 23:41:22.913100  6183 sgd_solver.cpp:106] Iteration 23200, lr = 1e-05
I0630 23:41:42.019321  6183 solver.cpp:290] Iteration 23300 (5.23404 iter/s, 19.1057s/100 iter), loss = 0.105304
I0630 23:41:42.019345  6183 solver.cpp:309]     Train net output #0: loss = 0.105304 (* 1 = 0.105304 loss)
I0630 23:41:42.019352  6183 sgd_solver.cpp:106] Iteration 23300, lr = 1e-05
I0630 23:42:01.200431  6183 solver.cpp:290] Iteration 23400 (5.21361 iter/s, 19.1806s/100 iter), loss = 0.0881742
I0630 23:42:01.200505  6183 solver.cpp:309]     Train net output #0: loss = 0.0881745 (* 1 = 0.0881745 loss)
I0630 23:42:01.200512  6183 sgd_solver.cpp:106] Iteration 23400, lr = 1e-05
I0630 23:42:20.523497  6183 solver.cpp:290] Iteration 23500 (5.17533 iter/s, 19.3225s/100 iter), loss = 0.174609
I0630 23:42:20.523519  6183 solver.cpp:309]     Train net output #0: loss = 0.174609 (* 1 = 0.174609 loss)
I0630 23:42:20.523525  6183 sgd_solver.cpp:106] Iteration 23500, lr = 1e-05
I0630 23:42:39.759374  6183 solver.cpp:290] Iteration 23600 (5.19877 iter/s, 19.2353s/100 iter), loss = 0.248752
I0630 23:42:39.759470  6183 solver.cpp:309]     Train net output #0: loss = 0.248752 (* 1 = 0.248752 loss)
I0630 23:42:39.759479  6183 sgd_solver.cpp:106] Iteration 23600, lr = 1e-05
I0630 23:42:58.829797  6183 solver.cpp:290] Iteration 23700 (5.24389 iter/s, 19.0698s/100 iter), loss = 0.0915703
I0630 23:42:58.829820  6183 solver.cpp:309]     Train net output #0: loss = 0.0915706 (* 1 = 0.0915706 loss)
I0630 23:42:58.829826  6183 sgd_solver.cpp:106] Iteration 23700, lr = 1e-05
I0630 23:43:17.956482  6183 solver.cpp:290] Iteration 23800 (5.22845 iter/s, 19.1261s/100 iter), loss = 0.106737
I0630 23:43:17.956531  6183 solver.cpp:309]     Train net output #0: loss = 0.106737 (* 1 = 0.106737 loss)
I0630 23:43:17.956538  6183 sgd_solver.cpp:106] Iteration 23800, lr = 1e-05
I0630 23:43:37.058265  6183 solver.cpp:290] Iteration 23900 (5.23527 iter/s, 19.1012s/100 iter), loss = 0.146452
I0630 23:43:37.058290  6183 solver.cpp:309]     Train net output #0: loss = 0.146452 (* 1 = 0.146452 loss)
I0630 23:43:37.058296  6183 sgd_solver.cpp:106] Iteration 23900, lr = 1e-05
I0630 23:43:56.146049  6183 solver.cpp:354] Sparsity after update:
I0630 23:43:56.147927  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 23:43:56.147933  6183 net.cpp:1851] conv1a_param_0(0.4) 
I0630 23:43:56.147941  6183 net.cpp:1851] conv1b_param_0(0.774) 
I0630 23:43:56.147944  6183 net.cpp:1851] ctx_conv1_param_0(0.63) 
I0630 23:43:56.147946  6183 net.cpp:1851] ctx_conv2_param_0(0.686) 
I0630 23:43:56.147948  6183 net.cpp:1851] ctx_conv3_param_0(0.626) 
I0630 23:43:56.147951  6183 net.cpp:1851] ctx_conv4_param_0(0.675) 
I0630 23:43:56.147953  6183 net.cpp:1851] ctx_final_param_0(0.4) 
I0630 23:43:56.147955  6183 net.cpp:1851] out3a_param_0(0.757) 
I0630 23:43:56.147958  6183 net.cpp:1851] out5a_param_0(0.784) 
I0630 23:43:56.147959  6183 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0630 23:43:56.147963  6183 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0630 23:43:56.147965  6183 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0630 23:43:56.147970  6183 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0630 23:43:56.147972  6183 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0630 23:43:56.147974  6183 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0630 23:43:56.147977  6183 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0630 23:43:56.147981  6183 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0630 23:43:56.147985  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.12736e+06/2.69808e+06) 0.788
I0630 23:43:56.148123  6183 solver.cpp:471] Iteration 24000, Testing net (#0)
I0630 23:45:33.859681  6183 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.924386
I0630 23:45:33.859793  6183 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.993536
I0630 23:45:33.859800  6183 solver.cpp:544]     Test net output #2: loss = 0.157914 (* 1 = 0.157914 loss)
I0630 23:45:34.064378  6344 sgd_solver.cpp:46] MultiStep Status: Iteration 24000, step = 1
I0630 23:45:34.064373  6183 solver.cpp:290] Iteration 24000 (0.85468 iter/s, 117.003s/100 iter), loss = 0.0797874
I0630 23:45:34.064404  6183 solver.cpp:309]     Train net output #0: loss = 0.0797877 (* 1 = 0.0797877 loss)
I0630 23:45:34.064410  6183 sgd_solver.cpp:46] MultiStep Status: Iteration 24000, step = 1
I0630 23:45:34.064414  6183 sgd_solver.cpp:106] Iteration 24000, lr = 1e-06
I0630 23:45:34.064383  6345 sgd_solver.cpp:46] MultiStep Status: Iteration 24000, step = 1
I0630 23:45:53.427959  6183 solver.cpp:290] Iteration 24100 (5.16448 iter/s, 19.363s/100 iter), loss = 0.116088
I0630 23:45:53.427985  6183 solver.cpp:309]     Train net output #0: loss = 0.116088 (* 1 = 0.116088 loss)
I0630 23:45:53.427994  6183 sgd_solver.cpp:106] Iteration 24100, lr = 1e-06
I0630 23:46:12.434206  6183 solver.cpp:290] Iteration 24200 (5.26158 iter/s, 19.0057s/100 iter), loss = 0.115731
I0630 23:46:12.434317  6183 solver.cpp:309]     Train net output #0: loss = 0.115731 (* 1 = 0.115731 loss)
I0630 23:46:12.434331  6183 sgd_solver.cpp:106] Iteration 24200, lr = 1e-06
I0630 23:46:31.513533  6183 solver.cpp:290] Iteration 24300 (5.24145 iter/s, 19.0787s/100 iter), loss = 0.0898752
I0630 23:46:31.513559  6183 solver.cpp:309]     Train net output #0: loss = 0.0898755 (* 1 = 0.0898755 loss)
I0630 23:46:31.513568  6183 sgd_solver.cpp:106] Iteration 24300, lr = 1e-06
I0630 23:46:50.574815  6183 solver.cpp:290] Iteration 24400 (5.24639 iter/s, 19.0607s/100 iter), loss = 0.081263
I0630 23:46:50.574868  6183 solver.cpp:309]     Train net output #0: loss = 0.0812633 (* 1 = 0.0812633 loss)
I0630 23:46:50.574877  6183 sgd_solver.cpp:106] Iteration 24400, lr = 1e-06
I0630 23:47:09.602005  6183 solver.cpp:290] Iteration 24500 (5.2558 iter/s, 19.0266s/100 iter), loss = 0.120052
I0630 23:47:09.602030  6183 solver.cpp:309]     Train net output #0: loss = 0.120052 (* 1 = 0.120052 loss)
I0630 23:47:09.602039  6183 sgd_solver.cpp:106] Iteration 24500, lr = 1e-06
I0630 23:47:28.520531  6183 solver.cpp:290] Iteration 24600 (5.28598 iter/s, 18.918s/100 iter), loss = 0.115887
I0630 23:47:28.520607  6183 solver.cpp:309]     Train net output #0: loss = 0.115887 (* 1 = 0.115887 loss)
I0630 23:47:28.520617  6183 sgd_solver.cpp:106] Iteration 24600, lr = 1e-06
I0630 23:47:47.580261  6183 solver.cpp:290] Iteration 24700 (5.24683 iter/s, 19.0591s/100 iter), loss = 0.146809
I0630 23:47:47.580284  6183 solver.cpp:309]     Train net output #0: loss = 0.146809 (* 1 = 0.146809 loss)
I0630 23:47:47.580291  6183 sgd_solver.cpp:106] Iteration 24700, lr = 1e-06
I0630 23:48:06.718706  6183 solver.cpp:290] Iteration 24800 (5.22524 iter/s, 19.1379s/100 iter), loss = 0.0606489
I0630 23:48:06.718803  6183 solver.cpp:309]     Train net output #0: loss = 0.0606492 (* 1 = 0.0606492 loss)
I0630 23:48:06.718816  6183 sgd_solver.cpp:106] Iteration 24800, lr = 1e-06
I0630 23:48:25.858268  6183 solver.cpp:290] Iteration 24900 (5.22495 iter/s, 19.1389s/100 iter), loss = 0.101544
I0630 23:48:25.858291  6183 solver.cpp:309]     Train net output #0: loss = 0.101544 (* 1 = 0.101544 loss)
I0630 23:48:25.858299  6183 sgd_solver.cpp:106] Iteration 24900, lr = 1e-06
I0630 23:48:44.832186  6183 solver.cpp:354] Sparsity after update:
I0630 23:48:44.921409  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 23:48:44.921427  6183 net.cpp:1851] conv1a_param_0(0.4) 
I0630 23:48:44.921434  6183 net.cpp:1851] conv1b_param_0(0.774) 
I0630 23:48:44.921437  6183 net.cpp:1851] ctx_conv1_param_0(0.63) 
I0630 23:48:44.921438  6183 net.cpp:1851] ctx_conv2_param_0(0.686) 
I0630 23:48:44.921440  6183 net.cpp:1851] ctx_conv3_param_0(0.626) 
I0630 23:48:44.921442  6183 net.cpp:1851] ctx_conv4_param_0(0.675) 
I0630 23:48:44.921444  6183 net.cpp:1851] ctx_final_param_0(0.4) 
I0630 23:48:44.921447  6183 net.cpp:1851] out3a_param_0(0.757) 
I0630 23:48:44.921448  6183 net.cpp:1851] out5a_param_0(0.784) 
I0630 23:48:44.921450  6183 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0630 23:48:44.921453  6183 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0630 23:48:44.921455  6183 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0630 23:48:44.921458  6183 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0630 23:48:44.921460  6183 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0630 23:48:44.921463  6183 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0630 23:48:44.921466  6183 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0630 23:48:44.921469  6183 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0630 23:48:44.921473  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.12736e+06/2.69808e+06) 0.788
I0630 23:48:45.096645  6183 solver.cpp:290] Iteration 25000 (5.19809 iter/s, 19.2378s/100 iter), loss = 0.174162
I0630 23:48:45.096668  6183 solver.cpp:309]     Train net output #0: loss = 0.174162 (* 1 = 0.174162 loss)
I0630 23:48:45.096674  6183 sgd_solver.cpp:106] Iteration 25000, lr = 1e-06
I0630 23:49:04.153375  6183 solver.cpp:290] Iteration 25100 (5.24764 iter/s, 19.0562s/100 iter), loss = 0.117994
I0630 23:49:04.153401  6183 solver.cpp:309]     Train net output #0: loss = 0.117994 (* 1 = 0.117994 loss)
I0630 23:49:04.153410  6183 sgd_solver.cpp:106] Iteration 25100, lr = 1e-06
I0630 23:49:23.429985  6183 solver.cpp:290] Iteration 25200 (5.18779 iter/s, 19.276s/100 iter), loss = 0.112656
I0630 23:49:23.430058  6183 solver.cpp:309]     Train net output #0: loss = 0.112656 (* 1 = 0.112656 loss)
I0630 23:49:23.430065  6183 sgd_solver.cpp:106] Iteration 25200, lr = 1e-06
I0630 23:49:42.602918  6183 solver.cpp:290] Iteration 25300 (5.21585 iter/s, 19.1723s/100 iter), loss = 0.108097
I0630 23:49:42.602941  6183 solver.cpp:309]     Train net output #0: loss = 0.108097 (* 1 = 0.108097 loss)
I0630 23:49:42.602948  6183 sgd_solver.cpp:106] Iteration 25300, lr = 1e-06
I0630 23:50:01.902642  6183 solver.cpp:290] Iteration 25400 (5.18157 iter/s, 19.2992s/100 iter), loss = 0.294087
I0630 23:50:01.902925  6183 solver.cpp:309]     Train net output #0: loss = 0.294087 (* 1 = 0.294087 loss)
I0630 23:50:01.902932  6183 sgd_solver.cpp:106] Iteration 25400, lr = 1e-06
I0630 23:50:21.143316  6183 solver.cpp:290] Iteration 25500 (5.19754 iter/s, 19.2399s/100 iter), loss = 0.17124
I0630 23:50:21.143337  6183 solver.cpp:309]     Train net output #0: loss = 0.17124 (* 1 = 0.17124 loss)
I0630 23:50:21.143343  6183 sgd_solver.cpp:106] Iteration 25500, lr = 1e-06
I0630 23:50:40.302605  6183 solver.cpp:290] Iteration 25600 (5.21955 iter/s, 19.1587s/100 iter), loss = 0.137686
I0630 23:50:40.302651  6183 solver.cpp:309]     Train net output #0: loss = 0.137686 (* 1 = 0.137686 loss)
I0630 23:50:40.302659  6183 sgd_solver.cpp:106] Iteration 25600, lr = 1e-06
I0630 23:50:59.523438  6183 solver.cpp:290] Iteration 25700 (5.20285 iter/s, 19.2203s/100 iter), loss = 0.098362
I0630 23:50:59.523464  6183 solver.cpp:309]     Train net output #0: loss = 0.0983623 (* 1 = 0.0983623 loss)
I0630 23:50:59.523471  6183 sgd_solver.cpp:106] Iteration 25700, lr = 1e-06
I0630 23:51:18.807512  6183 solver.cpp:290] Iteration 25800 (5.18578 iter/s, 19.2835s/100 iter), loss = 0.0822623
I0630 23:51:18.807634  6183 solver.cpp:309]     Train net output #0: loss = 0.0822625 (* 1 = 0.0822625 loss)
I0630 23:51:18.807643  6183 sgd_solver.cpp:106] Iteration 25800, lr = 1e-06
I0630 23:51:37.904783  6183 solver.cpp:290] Iteration 25900 (5.23653 iter/s, 19.0966s/100 iter), loss = 0.0717812
I0630 23:51:37.904806  6183 solver.cpp:309]     Train net output #0: loss = 0.0717814 (* 1 = 0.0717814 loss)
I0630 23:51:37.904814  6183 sgd_solver.cpp:106] Iteration 25900, lr = 1e-06
I0630 23:51:56.883004  6183 solver.cpp:354] Sparsity after update:
I0630 23:51:56.884631  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 23:51:56.884639  6183 net.cpp:1851] conv1a_param_0(0.4) 
I0630 23:51:56.884646  6183 net.cpp:1851] conv1b_param_0(0.774) 
I0630 23:51:56.884649  6183 net.cpp:1851] ctx_conv1_param_0(0.63) 
I0630 23:51:56.884650  6183 net.cpp:1851] ctx_conv2_param_0(0.686) 
I0630 23:51:56.884652  6183 net.cpp:1851] ctx_conv3_param_0(0.626) 
I0630 23:51:56.884654  6183 net.cpp:1851] ctx_conv4_param_0(0.675) 
I0630 23:51:56.884656  6183 net.cpp:1851] ctx_final_param_0(0.4) 
I0630 23:51:56.884658  6183 net.cpp:1851] out3a_param_0(0.757) 
I0630 23:51:56.884660  6183 net.cpp:1851] out5a_param_0(0.784) 
I0630 23:51:56.884662  6183 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0630 23:51:56.884665  6183 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0630 23:51:56.884666  6183 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0630 23:51:56.884668  6183 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0630 23:51:56.884670  6183 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0630 23:51:56.884672  6183 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0630 23:51:56.884675  6183 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0630 23:51:56.884678  6183 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0630 23:51:56.884680  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.12736e+06/2.69808e+06) 0.788
I0630 23:51:56.884811  6183 solver.cpp:471] Iteration 26000, Testing net (#0)
I0630 23:53:34.080459  6183 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.924609
I0630 23:53:34.080540  6183 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.993512
I0630 23:53:34.080546  6183 solver.cpp:544]     Test net output #2: loss = 0.157464 (* 1 = 0.157464 loss)
I0630 23:53:34.275108  6183 solver.cpp:290] Iteration 26000 (0.859349 iter/s, 116.367s/100 iter), loss = 0.112323
I0630 23:53:34.275133  6183 solver.cpp:309]     Train net output #0: loss = 0.112324 (* 1 = 0.112324 loss)
I0630 23:53:34.275143  6183 sgd_solver.cpp:106] Iteration 26000, lr = 1e-06
I0630 23:53:53.187489  6183 solver.cpp:290] Iteration 26100 (5.2877 iter/s, 18.9118s/100 iter), loss = 0.159115
I0630 23:53:53.187513  6183 solver.cpp:309]     Train net output #0: loss = 0.159115 (* 1 = 0.159115 loss)
I0630 23:53:53.187520  6183 sgd_solver.cpp:106] Iteration 26100, lr = 1e-06
I0630 23:54:12.305052  6183 solver.cpp:290] Iteration 26200 (5.23095 iter/s, 19.117s/100 iter), loss = 0.0516412
I0630 23:54:12.305162  6183 solver.cpp:309]     Train net output #0: loss = 0.0516414 (* 1 = 0.0516414 loss)
I0630 23:54:12.305172  6183 sgd_solver.cpp:106] Iteration 26200, lr = 1e-06
I0630 23:54:31.174593  6183 solver.cpp:290] Iteration 26300 (5.29973 iter/s, 18.8689s/100 iter), loss = 0.0973212
I0630 23:54:31.174619  6183 solver.cpp:309]     Train net output #0: loss = 0.0973214 (* 1 = 0.0973214 loss)
I0630 23:54:31.174633  6183 sgd_solver.cpp:106] Iteration 26300, lr = 1e-06
I0630 23:54:50.282600  6183 solver.cpp:290] Iteration 26400 (5.23356 iter/s, 19.1074s/100 iter), loss = 0.274427
I0630 23:54:50.282682  6183 solver.cpp:309]     Train net output #0: loss = 0.274427 (* 1 = 0.274427 loss)
I0630 23:54:50.282693  6183 sgd_solver.cpp:106] Iteration 26400, lr = 1e-06
I0630 23:55:09.339397  6183 solver.cpp:290] Iteration 26500 (5.24764 iter/s, 19.0562s/100 iter), loss = 0.110842
I0630 23:55:09.339421  6183 solver.cpp:309]     Train net output #0: loss = 0.110842 (* 1 = 0.110842 loss)
I0630 23:55:09.339427  6183 sgd_solver.cpp:106] Iteration 26500, lr = 1e-06
I0630 23:55:28.463243  6183 solver.cpp:290] Iteration 26600 (5.22923 iter/s, 19.1233s/100 iter), loss = 0.175785
I0630 23:55:28.463305  6183 solver.cpp:309]     Train net output #0: loss = 0.175785 (* 1 = 0.175785 loss)
I0630 23:55:28.463313  6183 sgd_solver.cpp:106] Iteration 26600, lr = 1e-06
I0630 23:55:47.623656  6183 solver.cpp:290] Iteration 26700 (5.21926 iter/s, 19.1598s/100 iter), loss = 0.113886
I0630 23:55:47.623677  6183 solver.cpp:309]     Train net output #0: loss = 0.113886 (* 1 = 0.113886 loss)
I0630 23:55:47.623683  6183 sgd_solver.cpp:106] Iteration 26700, lr = 1e-06
I0630 23:56:06.552568  6183 solver.cpp:290] Iteration 26800 (5.28308 iter/s, 18.9284s/100 iter), loss = 0.114191
I0630 23:56:06.552613  6183 solver.cpp:309]     Train net output #0: loss = 0.114191 (* 1 = 0.114191 loss)
I0630 23:56:06.552619  6183 sgd_solver.cpp:106] Iteration 26800, lr = 1e-06
I0630 23:56:25.799031  6183 solver.cpp:290] Iteration 26900 (5.19592 iter/s, 19.2459s/100 iter), loss = 0.156358
I0630 23:56:25.799053  6183 solver.cpp:309]     Train net output #0: loss = 0.156358 (* 1 = 0.156358 loss)
I0630 23:56:25.799060  6183 sgd_solver.cpp:106] Iteration 26900, lr = 1e-06
I0630 23:56:44.904948  6183 solver.cpp:354] Sparsity after update:
I0630 23:56:44.974923  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 23:56:44.974943  6183 net.cpp:1851] conv1a_param_0(0.4) 
I0630 23:56:44.974954  6183 net.cpp:1851] conv1b_param_0(0.774) 
I0630 23:56:44.974957  6183 net.cpp:1851] ctx_conv1_param_0(0.63) 
I0630 23:56:44.974959  6183 net.cpp:1851] ctx_conv2_param_0(0.686) 
I0630 23:56:44.974966  6183 net.cpp:1851] ctx_conv3_param_0(0.626) 
I0630 23:56:44.974969  6183 net.cpp:1851] ctx_conv4_param_0(0.675) 
I0630 23:56:44.974974  6183 net.cpp:1851] ctx_final_param_0(0.4) 
I0630 23:56:44.974978  6183 net.cpp:1851] out3a_param_0(0.757) 
I0630 23:56:44.974983  6183 net.cpp:1851] out5a_param_0(0.784) 
I0630 23:56:44.974987  6183 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0630 23:56:44.974992  6183 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0630 23:56:44.974995  6183 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0630 23:56:44.975000  6183 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0630 23:56:44.975003  6183 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0630 23:56:44.975008  6183 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0630 23:56:44.975011  6183 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0630 23:56:44.975016  6183 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0630 23:56:44.975020  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.12736e+06/2.69808e+06) 0.788
I0630 23:56:45.145750  6183 solver.cpp:290] Iteration 27000 (5.16898 iter/s, 19.3462s/100 iter), loss = 0.117241
I0630 23:56:45.145773  6183 solver.cpp:309]     Train net output #0: loss = 0.117241 (* 1 = 0.117241 loss)
I0630 23:56:45.145781  6183 sgd_solver.cpp:106] Iteration 27000, lr = 1e-06
I0630 23:57:04.003620  6183 solver.cpp:290] Iteration 27100 (5.30298 iter/s, 18.8573s/100 iter), loss = 0.0746588
I0630 23:57:04.003644  6183 solver.cpp:309]     Train net output #0: loss = 0.074659 (* 1 = 0.074659 loss)
I0630 23:57:04.003650  6183 sgd_solver.cpp:106] Iteration 27100, lr = 1e-06
I0630 23:57:23.303915  6183 solver.cpp:290] Iteration 27200 (5.18142 iter/s, 19.2997s/100 iter), loss = 0.141996
I0630 23:57:23.303969  6183 solver.cpp:309]     Train net output #0: loss = 0.141996 (* 1 = 0.141996 loss)
I0630 23:57:23.303977  6183 sgd_solver.cpp:106] Iteration 27200, lr = 1e-06
I0630 23:57:42.467144  6183 solver.cpp:290] Iteration 27300 (5.21849 iter/s, 19.1626s/100 iter), loss = 0.143237
I0630 23:57:42.467167  6183 solver.cpp:309]     Train net output #0: loss = 0.143237 (* 1 = 0.143237 loss)
I0630 23:57:42.467173  6183 sgd_solver.cpp:106] Iteration 27300, lr = 1e-06
I0630 23:58:01.726447  6183 solver.cpp:290] Iteration 27400 (5.19244 iter/s, 19.2588s/100 iter), loss = 0.0647739
I0630 23:58:01.726505  6183 solver.cpp:309]     Train net output #0: loss = 0.0647742 (* 1 = 0.0647742 loss)
I0630 23:58:01.726511  6183 sgd_solver.cpp:106] Iteration 27400, lr = 1e-06
I0630 23:58:20.936101  6183 solver.cpp:290] Iteration 27500 (5.20587 iter/s, 19.2091s/100 iter), loss = 0.115319
I0630 23:58:20.936125  6183 solver.cpp:309]     Train net output #0: loss = 0.11532 (* 1 = 0.11532 loss)
I0630 23:58:20.936131  6183 sgd_solver.cpp:106] Iteration 27500, lr = 1e-06
I0630 23:58:40.101933  6183 solver.cpp:290] Iteration 27600 (5.21777 iter/s, 19.1653s/100 iter), loss = 0.0751673
I0630 23:58:40.101986  6183 solver.cpp:309]     Train net output #0: loss = 0.0751675 (* 1 = 0.0751675 loss)
I0630 23:58:40.101994  6183 sgd_solver.cpp:106] Iteration 27600, lr = 1e-06
I0630 23:58:59.267094  6183 solver.cpp:290] Iteration 27700 (5.21796 iter/s, 19.1646s/100 iter), loss = 0.108228
I0630 23:58:59.267119  6183 solver.cpp:309]     Train net output #0: loss = 0.108228 (* 1 = 0.108228 loss)
I0630 23:58:59.267127  6183 sgd_solver.cpp:106] Iteration 27700, lr = 1e-06
I0630 23:59:18.079409  6183 solver.cpp:290] Iteration 27800 (5.31582 iter/s, 18.8118s/100 iter), loss = 0.0760967
I0630 23:59:18.079459  6183 solver.cpp:309]     Train net output #0: loss = 0.0760969 (* 1 = 0.0760969 loss)
I0630 23:59:18.079468  6183 sgd_solver.cpp:106] Iteration 27800, lr = 1e-06
I0630 23:59:37.127424  6183 solver.cpp:290] Iteration 27900 (5.25005 iter/s, 19.0474s/100 iter), loss = 0.114615
I0630 23:59:37.127446  6183 solver.cpp:309]     Train net output #0: loss = 0.114616 (* 1 = 0.114616 loss)
I0630 23:59:37.127452  6183 sgd_solver.cpp:106] Iteration 27900, lr = 1e-06
I0630 23:59:56.139376  6183 solver.cpp:354] Sparsity after update:
I0630 23:59:56.141288  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 23:59:56.141295  6183 net.cpp:1851] conv1a_param_0(0.4) 
I0630 23:59:56.141302  6183 net.cpp:1851] conv1b_param_0(0.774) 
I0630 23:59:56.141304  6183 net.cpp:1851] ctx_conv1_param_0(0.63) 
I0630 23:59:56.141306  6183 net.cpp:1851] ctx_conv2_param_0(0.686) 
I0630 23:59:56.141309  6183 net.cpp:1851] ctx_conv3_param_0(0.626) 
I0630 23:59:56.141310  6183 net.cpp:1851] ctx_conv4_param_0(0.675) 
I0630 23:59:56.141312  6183 net.cpp:1851] ctx_final_param_0(0.4) 
I0630 23:59:56.141314  6183 net.cpp:1851] out3a_param_0(0.757) 
I0630 23:59:56.141316  6183 net.cpp:1851] out5a_param_0(0.784) 
I0630 23:59:56.141317  6183 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0630 23:59:56.141319  6183 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0630 23:59:56.141321  6183 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0630 23:59:56.141324  6183 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0630 23:59:56.141325  6183 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0630 23:59:56.141327  6183 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0630 23:59:56.141330  6183 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0630 23:59:56.141332  6183 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0630 23:59:56.141335  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.12736e+06/2.69808e+06) 0.788
I0630 23:59:56.141471  6183 solver.cpp:471] Iteration 28000, Testing net (#0)
I0701 00:01:33.851636  6183 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.924717
I0701 00:01:33.851716  6183 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.993542
I0701 00:01:33.851723  6183 solver.cpp:544]     Test net output #2: loss = 0.156471 (* 1 = 0.156471 loss)
I0701 00:01:34.057849  6183 solver.cpp:290] Iteration 28000 (0.855234 iter/s, 116.927s/100 iter), loss = 0.0882793
I0701 00:01:34.057875  6183 solver.cpp:309]     Train net output #0: loss = 0.0882796 (* 1 = 0.0882796 loss)
I0701 00:01:34.057881  6183 sgd_solver.cpp:106] Iteration 28000, lr = 1e-06
I0701 00:01:53.311319  6183 solver.cpp:290] Iteration 28100 (5.19403 iter/s, 19.2529s/100 iter), loss = 0.128896
I0701 00:01:53.311347  6183 solver.cpp:309]     Train net output #0: loss = 0.128896 (* 1 = 0.128896 loss)
I0701 00:01:53.311355  6183 sgd_solver.cpp:106] Iteration 28100, lr = 1e-06
I0701 00:02:12.432482  6183 solver.cpp:290] Iteration 28200 (5.22996 iter/s, 19.1206s/100 iter), loss = 0.169995
I0701 00:02:12.432556  6183 solver.cpp:309]     Train net output #0: loss = 0.169996 (* 1 = 0.169996 loss)
I0701 00:02:12.432569  6183 sgd_solver.cpp:106] Iteration 28200, lr = 1e-06
I0701 00:02:31.496423  6183 solver.cpp:290] Iteration 28300 (5.24568 iter/s, 19.0633s/100 iter), loss = 0.162979
I0701 00:02:31.496445  6183 solver.cpp:309]     Train net output #0: loss = 0.162979 (* 1 = 0.162979 loss)
I0701 00:02:31.496453  6183 sgd_solver.cpp:106] Iteration 28300, lr = 1e-06
I0701 00:02:50.737573  6183 solver.cpp:290] Iteration 28400 (5.19735 iter/s, 19.2406s/100 iter), loss = 0.126353
I0701 00:02:50.737684  6183 solver.cpp:309]     Train net output #0: loss = 0.126354 (* 1 = 0.126354 loss)
I0701 00:02:50.737694  6183 sgd_solver.cpp:106] Iteration 28400, lr = 1e-06
I0701 00:03:09.767961  6183 solver.cpp:290] Iteration 28500 (5.25493 iter/s, 19.0297s/100 iter), loss = 0.0879228
I0701 00:03:09.767987  6183 solver.cpp:309]     Train net output #0: loss = 0.0879231 (* 1 = 0.0879231 loss)
I0701 00:03:09.767994  6183 sgd_solver.cpp:106] Iteration 28500, lr = 1e-06
I0701 00:03:29.006009  6183 solver.cpp:290] Iteration 28600 (5.19819 iter/s, 19.2375s/100 iter), loss = 0.167123
I0701 00:03:29.006418  6183 solver.cpp:309]     Train net output #0: loss = 0.167124 (* 1 = 0.167124 loss)
I0701 00:03:29.006448  6183 sgd_solver.cpp:106] Iteration 28600, lr = 1e-06
I0701 00:03:48.142246  6183 solver.cpp:290] Iteration 28700 (5.22595 iter/s, 19.1353s/100 iter), loss = 0.0678674
I0701 00:03:48.142271  6183 solver.cpp:309]     Train net output #0: loss = 0.0678676 (* 1 = 0.0678676 loss)
I0701 00:03:48.142294  6183 sgd_solver.cpp:106] Iteration 28700, lr = 1e-06
I0701 00:04:07.398535  6183 solver.cpp:290] Iteration 28800 (5.19326 iter/s, 19.2557s/100 iter), loss = 0.153917
I0701 00:04:07.398610  6183 solver.cpp:309]     Train net output #0: loss = 0.153918 (* 1 = 0.153918 loss)
I0701 00:04:07.398620  6183 sgd_solver.cpp:106] Iteration 28800, lr = 1e-06
I0701 00:04:26.651180  6183 solver.cpp:290] Iteration 28900 (5.19426 iter/s, 19.252s/100 iter), loss = 0.167103
I0701 00:04:26.651202  6183 solver.cpp:309]     Train net output #0: loss = 0.167104 (* 1 = 0.167104 loss)
I0701 00:04:26.651209  6183 sgd_solver.cpp:106] Iteration 28900, lr = 1e-06
I0701 00:04:45.886432  6183 solver.cpp:354] Sparsity after update:
I0701 00:04:45.960880  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0701 00:04:45.960896  6183 net.cpp:1851] conv1a_param_0(0.4) 
I0701 00:04:45.960906  6183 net.cpp:1851] conv1b_param_0(0.774) 
I0701 00:04:45.960907  6183 net.cpp:1851] ctx_conv1_param_0(0.63) 
I0701 00:04:45.960909  6183 net.cpp:1851] ctx_conv2_param_0(0.686) 
I0701 00:04:45.960911  6183 net.cpp:1851] ctx_conv3_param_0(0.626) 
I0701 00:04:45.960913  6183 net.cpp:1851] ctx_conv4_param_0(0.675) 
I0701 00:04:45.960923  6183 net.cpp:1851] ctx_final_param_0(0.4) 
I0701 00:04:45.960925  6183 net.cpp:1851] out3a_param_0(0.757) 
I0701 00:04:45.960928  6183 net.cpp:1851] out5a_param_0(0.784) 
I0701 00:04:45.960930  6183 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0701 00:04:45.960933  6183 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0701 00:04:45.960935  6183 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0701 00:04:45.960937  6183 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0701 00:04:45.960940  6183 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0701 00:04:45.960942  6183 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0701 00:04:45.960944  6183 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0701 00:04:45.960947  6183 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0701 00:04:45.960949  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.12736e+06/2.69808e+06) 0.788
I0701 00:04:46.132360  6183 solver.cpp:290] Iteration 29000 (5.13331 iter/s, 19.4806s/100 iter), loss = 0.144085
I0701 00:04:46.132382  6183 solver.cpp:309]     Train net output #0: loss = 0.144086 (* 1 = 0.144086 loss)
I0701 00:04:46.132390  6183 sgd_solver.cpp:106] Iteration 29000, lr = 1e-06
I0701 00:05:05.294777  6183 solver.cpp:290] Iteration 29100 (5.2187 iter/s, 19.1618s/100 iter), loss = 0.0822346
I0701 00:05:05.294806  6183 solver.cpp:309]     Train net output #0: loss = 0.0822349 (* 1 = 0.0822349 loss)
I0701 00:05:05.294816  6183 sgd_solver.cpp:106] Iteration 29100, lr = 1e-06
I0701 00:05:24.422474  6183 solver.cpp:290] Iteration 29200 (5.22818 iter/s, 19.1271s/100 iter), loss = 0.0920118
I0701 00:05:24.422580  6183 solver.cpp:309]     Train net output #0: loss = 0.0920121 (* 1 = 0.0920121 loss)
I0701 00:05:24.422591  6183 sgd_solver.cpp:106] Iteration 29200, lr = 1e-06
I0701 00:05:43.475329  6183 solver.cpp:290] Iteration 29300 (5.24873 iter/s, 19.0522s/100 iter), loss = 0.068017
I0701 00:05:43.475353  6183 solver.cpp:309]     Train net output #0: loss = 0.0680174 (* 1 = 0.0680174 loss)
I0701 00:05:43.475360  6183 sgd_solver.cpp:106] Iteration 29300, lr = 1e-06
I0701 00:06:02.674185  6183 solver.cpp:290] Iteration 29400 (5.2088 iter/s, 19.1983s/100 iter), loss = 0.105338
I0701 00:06:02.674234  6183 solver.cpp:309]     Train net output #0: loss = 0.105338 (* 1 = 0.105338 loss)
I0701 00:06:02.674244  6183 sgd_solver.cpp:106] Iteration 29400, lr = 1e-06
I0701 00:06:21.609387  6183 solver.cpp:290] Iteration 29500 (5.28133 iter/s, 18.9346s/100 iter), loss = 0.0891665
I0701 00:06:21.609411  6183 solver.cpp:309]     Train net output #0: loss = 0.0891668 (* 1 = 0.0891668 loss)
I0701 00:06:21.609417  6183 sgd_solver.cpp:106] Iteration 29500, lr = 1e-06
I0701 00:06:40.772693  6183 solver.cpp:290] Iteration 29600 (5.21846 iter/s, 19.1627s/100 iter), loss = 0.183283
I0701 00:06:40.772799  6183 solver.cpp:309]     Train net output #0: loss = 0.183283 (* 1 = 0.183283 loss)
I0701 00:06:40.772809  6183 sgd_solver.cpp:106] Iteration 29600, lr = 1e-06
I0701 00:07:00.094964  6183 solver.cpp:290] Iteration 29700 (5.17555 iter/s, 19.3216s/100 iter), loss = 0.175247
I0701 00:07:00.094986  6183 solver.cpp:309]     Train net output #0: loss = 0.175247 (* 1 = 0.175247 loss)
I0701 00:07:00.094993  6183 sgd_solver.cpp:106] Iteration 29700, lr = 1e-06
I0701 00:07:19.442138  6183 solver.cpp:290] Iteration 29800 (5.16887 iter/s, 19.3466s/100 iter), loss = 0.133367
I0701 00:07:19.442183  6183 solver.cpp:309]     Train net output #0: loss = 0.133367 (* 1 = 0.133367 loss)
I0701 00:07:19.442189  6183 sgd_solver.cpp:106] Iteration 29800, lr = 1e-06
I0701 00:07:38.578651  6183 solver.cpp:290] Iteration 29900 (5.22577 iter/s, 19.1359s/100 iter), loss = 0.111642
I0701 00:07:38.578675  6183 solver.cpp:309]     Train net output #0: loss = 0.111643 (* 1 = 0.111643 loss)
I0701 00:07:38.578680  6183 sgd_solver.cpp:106] Iteration 29900, lr = 1e-06
I0701 00:07:57.349455  6183 solver.cpp:598] Snapshotting to binary proto file training/cityscapes20_jsegnet21v2_2017-06-30_19-26-17/sparse/cityscapes20_jsegnet21v2_iter_30000.caffemodel
I0701 00:07:57.378254  6183 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cityscapes20_jsegnet21v2_2017-06-30_19-26-17/sparse/cityscapes20_jsegnet21v2_iter_30000.solverstate
I0701 00:07:57.394956  6183 solver.cpp:354] Sparsity after update:
I0701 00:07:57.396486  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0701 00:07:57.396493  6183 net.cpp:1851] conv1a_param_0(0.4) 
I0701 00:07:57.396502  6183 net.cpp:1851] conv1b_param_0(0.774) 
I0701 00:07:57.396504  6183 net.cpp:1851] ctx_conv1_param_0(0.63) 
I0701 00:07:57.396507  6183 net.cpp:1851] ctx_conv2_param_0(0.686) 
I0701 00:07:57.396508  6183 net.cpp:1851] ctx_conv3_param_0(0.626) 
I0701 00:07:57.396510  6183 net.cpp:1851] ctx_conv4_param_0(0.675) 
I0701 00:07:57.396512  6183 net.cpp:1851] ctx_final_param_0(0.4) 
I0701 00:07:57.396514  6183 net.cpp:1851] out3a_param_0(0.757) 
I0701 00:07:57.396517  6183 net.cpp:1851] out5a_param_0(0.784) 
I0701 00:07:57.396518  6183 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0701 00:07:57.396520  6183 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0701 00:07:57.396522  6183 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0701 00:07:57.396524  6183 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0701 00:07:57.396526  6183 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0701 00:07:57.396528  6183 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0701 00:07:57.396530  6183 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0701 00:07:57.396533  6183 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0701 00:07:57.396534  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.12736e+06/2.69808e+06) 0.788
I0701 00:07:57.396682  6183 solver.cpp:471] Iteration 30000, Testing net (#0)
I0701 00:09:35.236037  6183 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.925086
I0701 00:09:35.236143  6183 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.993577
I0701 00:09:35.236151  6183 solver.cpp:544]     Test net output #2: loss = 0.156085 (* 1 = 0.156085 loss)
I0701 00:09:35.450376  6183 solver.cpp:290] Iteration 30000 (0.855663 iter/s, 116.868s/100 iter), loss = 0.170123
I0701 00:09:35.450404  6183 solver.cpp:309]     Train net output #0: loss = 0.170124 (* 1 = 0.170124 loss)
I0701 00:09:35.450412  6183 sgd_solver.cpp:106] Iteration 30000, lr = 1e-06
I0701 00:09:54.716028  6183 solver.cpp:290] Iteration 30100 (5.19074 iter/s, 19.2651s/100 iter), loss = 0.106507
I0701 00:09:54.716054  6183 solver.cpp:309]     Train net output #0: loss = 0.106507 (* 1 = 0.106507 loss)
I0701 00:09:54.716060  6183 sgd_solver.cpp:106] Iteration 30100, lr = 1e-06
I0701 00:10:13.863876  6183 solver.cpp:290] Iteration 30200 (5.22267 iter/s, 19.1473s/100 iter), loss = 0.161513
I0701 00:10:13.863960  6183 solver.cpp:309]     Train net output #0: loss = 0.161513 (* 1 = 0.161513 loss)
I0701 00:10:13.863970  6183 sgd_solver.cpp:106] Iteration 30200, lr = 1e-06
I0701 00:10:32.991152  6183 solver.cpp:290] Iteration 30300 (5.22831 iter/s, 19.1267s/100 iter), loss = 0.144989
I0701 00:10:32.991174  6183 solver.cpp:309]     Train net output #0: loss = 0.144989 (* 1 = 0.144989 loss)
I0701 00:10:32.991181  6183 sgd_solver.cpp:106] Iteration 30300, lr = 1e-06
I0701 00:10:51.943864  6183 solver.cpp:290] Iteration 30400 (5.27645 iter/s, 18.9522s/100 iter), loss = 0.10105
I0701 00:10:51.943943  6183 solver.cpp:309]     Train net output #0: loss = 0.101051 (* 1 = 0.101051 loss)
I0701 00:10:51.943950  6183 sgd_solver.cpp:106] Iteration 30400, lr = 1e-06
I0701 00:11:11.162377  6183 solver.cpp:290] Iteration 30500 (5.20349 iter/s, 19.2179s/100 iter), loss = 0.129386
I0701 00:11:11.162408  6183 solver.cpp:309]     Train net output #0: loss = 0.129386 (* 1 = 0.129386 loss)
I0701 00:11:11.162417  6183 sgd_solver.cpp:106] Iteration 30500, lr = 1e-06
I0701 00:11:30.262593  6183 solver.cpp:290] Iteration 30600 (5.2357 iter/s, 19.0996s/100 iter), loss = 0.0875575
I0701 00:11:30.262679  6183 solver.cpp:309]     Train net output #0: loss = 0.0875578 (* 1 = 0.0875578 loss)
I0701 00:11:30.262691  6183 sgd_solver.cpp:106] Iteration 30600, lr = 1e-06
I0701 00:11:49.266680  6183 solver.cpp:290] Iteration 30700 (5.2622 iter/s, 19.0035s/100 iter), loss = 0.0917084
I0701 00:11:49.266703  6183 solver.cpp:309]     Train net output #0: loss = 0.0917087 (* 1 = 0.0917087 loss)
I0701 00:11:49.266710  6183 sgd_solver.cpp:106] Iteration 30700, lr = 1e-06
I0701 00:12:08.513634  6183 solver.cpp:290] Iteration 30800 (5.19578 iter/s, 19.2464s/100 iter), loss = 0.0952825
I0701 00:12:08.513689  6183 solver.cpp:309]     Train net output #0: loss = 0.0952828 (* 1 = 0.0952828 loss)
I0701 00:12:08.513697  6183 sgd_solver.cpp:106] Iteration 30800, lr = 1e-06
I0701 00:12:27.640071  6183 solver.cpp:290] Iteration 30900 (5.22853 iter/s, 19.1258s/100 iter), loss = 0.106022
I0701 00:12:27.640097  6183 solver.cpp:309]     Train net output #0: loss = 0.106022 (* 1 = 0.106022 loss)
I0701 00:12:27.640107  6183 sgd_solver.cpp:106] Iteration 30900, lr = 1e-06
I0701 00:12:46.510815  6183 solver.cpp:354] Sparsity after update:
I0701 00:12:46.579355  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0701 00:12:46.579372  6183 net.cpp:1851] conv1a_param_0(0.4) 
I0701 00:12:46.579380  6183 net.cpp:1851] conv1b_param_0(0.774) 
I0701 00:12:46.579382  6183 net.cpp:1851] ctx_conv1_param_0(0.63) 
I0701 00:12:46.579385  6183 net.cpp:1851] ctx_conv2_param_0(0.686) 
I0701 00:12:46.579386  6183 net.cpp:1851] ctx_conv3_param_0(0.626) 
I0701 00:12:46.579388  6183 net.cpp:1851] ctx_conv4_param_0(0.675) 
I0701 00:12:46.579391  6183 net.cpp:1851] ctx_final_param_0(0.4) 
I0701 00:12:46.579392  6183 net.cpp:1851] out3a_param_0(0.757) 
I0701 00:12:46.579394  6183 net.cpp:1851] out5a_param_0(0.784) 
I0701 00:12:46.579396  6183 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0701 00:12:46.579399  6183 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0701 00:12:46.579401  6183 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0701 00:12:46.579403  6183 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0701 00:12:46.579406  6183 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0701 00:12:46.579407  6183 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0701 00:12:46.579409  6183 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0701 00:12:46.579411  6183 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0701 00:12:46.579413  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.12736e+06/2.69808e+06) 0.788
I0701 00:12:46.750656  6183 solver.cpp:290] Iteration 31000 (5.23286 iter/s, 19.11s/100 iter), loss = 0.105885
I0701 00:12:46.750682  6183 solver.cpp:309]     Train net output #0: loss = 0.105885 (* 1 = 0.105885 loss)
I0701 00:12:46.750696  6183 sgd_solver.cpp:106] Iteration 31000, lr = 1e-06
I0701 00:13:05.914633  6183 solver.cpp:290] Iteration 31100 (5.21828 iter/s, 19.1634s/100 iter), loss = 0.0881869
I0701 00:13:05.914655  6183 solver.cpp:309]     Train net output #0: loss = 0.0881873 (* 1 = 0.0881873 loss)
I0701 00:13:05.914662  6183 sgd_solver.cpp:106] Iteration 31100, lr = 1e-06
I0701 00:13:25.056110  6183 solver.cpp:290] Iteration 31200 (5.22441 iter/s, 19.1409s/100 iter), loss = 0.177802
I0701 00:13:25.056180  6183 solver.cpp:309]     Train net output #0: loss = 0.177802 (* 1 = 0.177802 loss)
I0701 00:13:25.056191  6183 sgd_solver.cpp:106] Iteration 31200, lr = 1e-06
I0701 00:13:44.322732  6183 solver.cpp:290] Iteration 31300 (5.19049 iter/s, 19.266s/100 iter), loss = 0.138647
I0701 00:13:44.322757  6183 solver.cpp:309]     Train net output #0: loss = 0.138647 (* 1 = 0.138647 loss)
I0701 00:13:44.322763  6183 sgd_solver.cpp:106] Iteration 31300, lr = 1e-06
I0701 00:14:03.342833  6183 solver.cpp:290] Iteration 31400 (5.25775 iter/s, 19.0195s/100 iter), loss = 0.198583
I0701 00:14:03.342877  6183 solver.cpp:309]     Train net output #0: loss = 0.198584 (* 1 = 0.198584 loss)
I0701 00:14:03.342887  6183 sgd_solver.cpp:106] Iteration 31400, lr = 1e-06
I0701 00:14:22.394807  6183 solver.cpp:290] Iteration 31500 (5.24896 iter/s, 19.0514s/100 iter), loss = 0.0775808
I0701 00:14:22.394831  6183 solver.cpp:309]     Train net output #0: loss = 0.0775812 (* 1 = 0.0775812 loss)
I0701 00:14:22.394841  6183 sgd_solver.cpp:106] Iteration 31500, lr = 1e-06
I0701 00:14:41.294786  6183 solver.cpp:290] Iteration 31600 (5.29116 iter/s, 18.8994s/100 iter), loss = 0.111411
I0701 00:14:41.294863  6183 solver.cpp:309]     Train net output #0: loss = 0.111412 (* 1 = 0.111412 loss)
I0701 00:14:41.294873  6183 sgd_solver.cpp:106] Iteration 31600, lr = 1e-06
I0701 00:15:00.460767  6183 solver.cpp:290] Iteration 31700 (5.21775 iter/s, 19.1654s/100 iter), loss = 0.108201
I0701 00:15:00.460790  6183 solver.cpp:309]     Train net output #0: loss = 0.108202 (* 1 = 0.108202 loss)
I0701 00:15:00.460798  6183 sgd_solver.cpp:106] Iteration 31700, lr = 1e-06
I0701 00:15:19.652194  6183 solver.cpp:290] Iteration 31800 (5.21081 iter/s, 19.1909s/100 iter), loss = 0.0576294
I0701 00:15:19.652245  6183 solver.cpp:309]     Train net output #0: loss = 0.0576297 (* 1 = 0.0576297 loss)
I0701 00:15:19.652256  6183 sgd_solver.cpp:106] Iteration 31800, lr = 1e-06
I0701 00:15:38.995736  6183 solver.cpp:290] Iteration 31900 (5.16984 iter/s, 19.343s/100 iter), loss = 0.0822659
I0701 00:15:38.995760  6183 solver.cpp:309]     Train net output #0: loss = 0.0822662 (* 1 = 0.0822662 loss)
I0701 00:15:38.995767  6183 sgd_solver.cpp:106] Iteration 31900, lr = 1e-06
I0701 00:15:57.933223  6183 solver.cpp:354] Sparsity after update:
I0701 00:15:57.935084  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0701 00:15:57.935094  6183 net.cpp:1851] conv1a_param_0(0.4) 
I0701 00:15:57.935102  6183 net.cpp:1851] conv1b_param_0(0.774) 
I0701 00:15:57.935104  6183 net.cpp:1851] ctx_conv1_param_0(0.63) 
I0701 00:15:57.935106  6183 net.cpp:1851] ctx_conv2_param_0(0.686) 
I0701 00:15:57.935108  6183 net.cpp:1851] ctx_conv3_param_0(0.626) 
I0701 00:15:57.935111  6183 net.cpp:1851] ctx_conv4_param_0(0.675) 
I0701 00:15:57.935112  6183 net.cpp:1851] ctx_final_param_0(0.4) 
I0701 00:15:57.935114  6183 net.cpp:1851] out3a_param_0(0.757) 
I0701 00:15:57.935117  6183 net.cpp:1851] out5a_param_0(0.784) 
I0701 00:15:57.935118  6183 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0701 00:15:57.935120  6183 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0701 00:15:57.935122  6183 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0701 00:15:57.935124  6183 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0701 00:15:57.935127  6183 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0701 00:15:57.935128  6183 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0701 00:15:57.935130  6183 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0701 00:15:57.935132  6183 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0701 00:15:57.935135  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.12736e+06/2.69808e+06) 0.788
I0701 00:15:57.935145  6183 solver.cpp:598] Snapshotting to binary proto file training/cityscapes20_jsegnet21v2_2017-06-30_19-26-17/sparse/cityscapes20_jsegnet21v2_iter_32000.caffemodel
I0701 00:15:57.963176  6183 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cityscapes20_jsegnet21v2_2017-06-30_19-26-17/sparse/cityscapes20_jsegnet21v2_iter_32000.solverstate
I0701 00:15:58.037686  6183 solver.cpp:451] Iteration 32000, loss = 0.110268
I0701 00:15:58.037705  6183 solver.cpp:471] Iteration 32000, Testing net (#0)
I0701 00:17:34.066874  6183 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.924956
I0701 00:17:34.066947  6183 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.993579
I0701 00:17:34.066956  6183 solver.cpp:544]     Test net output #2: loss = 0.156318 (* 1 = 0.156318 loss)
I0701 00:17:34.066959  6183 solver.cpp:456] Optimization Done.
I0701 00:17:34.312887  6183 caffe.cpp:246] Optimization Done.
