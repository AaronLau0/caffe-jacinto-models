Logging output to training/cityscapes20_jsegnet21v2_2017-06-30_19-26-17/train-log_2017-06-30_19-26-17.txt
Using pretrained model training/imagenet_jacintonet11_v2_bn_iter_160000.caffemodel
training/cityscapes20_jsegnet21v2_2017-06-30_19-26-17/initial
I0630 19:26:26.753027  5429 caffe.cpp:209] Using GPUs 0, 1, 2
I0630 19:26:26.753481  5429 caffe.cpp:214] GPU 0: GeForce GTX 1080
I0630 19:26:26.753815  5429 caffe.cpp:214] GPU 1: GeForce GTX 1080
I0630 19:26:26.754145  5429 caffe.cpp:214] GPU 2: GeForce GTX 1080
I0630 19:26:27.687506  5429 solver.cpp:48] Initializing solver from parameters: 
train_net: "training/cityscapes20_jsegnet21v2_2017-06-30_19-26-17/initial/train.prototxt"
test_net: "training/cityscapes20_jsegnet21v2_2017-06-30_19-26-17/initial/test.prototxt"
test_iter: 125
test_interval: 2000
base_lr: 0.0001
display: 100
max_iter: 32000
lr_policy: "multistep"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 0.0001
snapshot: 10000
snapshot_prefix: "training/cityscapes20_jsegnet21v2_2017-06-30_19-26-17/initial/cityscapes20_jsegnet21v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
test_initialization: false
stepvalue: 24000
iter_size: 1
type: "Adam"
I0630 19:26:27.693665  5429 solver.cpp:82] Creating training net from train_net file: training/cityscapes20_jsegnet21v2_2017-06-30_19-26-17/initial/train.prototxt
I0630 19:26:27.698935  5429 net.cpp:327] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0630 19:26:27.698979  5429 net.cpp:327] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
I0630 19:26:27.700279  5429 net.cpp:56] Initializing net from parameters: 
name: "jsegnet21v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/train-image-lmdb"
    label_list_path: "data/train-label-lmdb"
    batch_size: 5
    threads: 4
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b/bn"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a/bn"
  top: "out5a/bn"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a/bn"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b/bn"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a/bn"
  top: "out3a/bn"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a/bn"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv1/bn"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv2/bn"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv3/bn"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4/bn"
  top: "ctx_conv4/bn"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4/bn"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 20
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 20
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 20
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
I0630 19:26:27.701280  5429 layer_factory.hpp:77] Creating layer data
I0630 19:26:27.701793  5429 net.cpp:98] Creating Layer data
I0630 19:26:27.701830  5429 net.cpp:413] data -> data
I0630 19:26:27.701905  5429 net.cpp:413] data -> label
I0630 19:26:27.756829  5505 db_lmdb.cpp:35] Opened lmdb data/train-image-lmdb
I0630 19:26:27.775555  5510 db_lmdb.cpp:35] Opened lmdb data/train-label-lmdb
I0630 19:26:28.232306  5429 data_layer.cpp:78] ReshapePrefetch 5, 3, 640, 640
I0630 19:26:28.232374  5429 data_layer.cpp:83] output data size: 5,3,640,640
I0630 19:26:28.257689  5429 data_layer.cpp:78] ReshapePrefetch 5, 1, 640, 640
I0630 19:26:28.257755  5429 data_layer.cpp:83] output data size: 5,1,640,640
I0630 19:26:28.264848  5518 blocking_queue.cpp:50] Waiting for data
I0630 19:26:28.270660  5429 net.cpp:148] Setting up data
I0630 19:26:28.270680  5429 net.cpp:155] Top shape: 5 3 640 640 (6144000)
I0630 19:26:28.270684  5429 net.cpp:155] Top shape: 5 1 640 640 (2048000)
I0630 19:26:28.270686  5429 net.cpp:163] Memory required for data: 32768000
I0630 19:26:28.270692  5429 layer_factory.hpp:77] Creating layer data/bias
I0630 19:26:28.270704  5429 net.cpp:98] Creating Layer data/bias
I0630 19:26:28.270707  5429 net.cpp:439] data/bias <- data
I0630 19:26:28.270717  5429 net.cpp:413] data/bias -> data/bias
I0630 19:26:28.271883  5429 net.cpp:148] Setting up data/bias
I0630 19:26:28.271893  5429 net.cpp:155] Top shape: 5 3 640 640 (6144000)
I0630 19:26:28.271894  5429 net.cpp:163] Memory required for data: 57344000
I0630 19:26:28.271905  5429 layer_factory.hpp:77] Creating layer conv1a
I0630 19:26:28.271915  5429 net.cpp:98] Creating Layer conv1a
I0630 19:26:28.271917  5429 net.cpp:439] conv1a <- data/bias
I0630 19:26:28.271920  5429 net.cpp:413] conv1a -> conv1a
I0630 19:26:28.274107  5429 net.cpp:148] Setting up conv1a
I0630 19:26:28.274121  5429 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0630 19:26:28.274122  5429 net.cpp:163] Memory required for data: 122880000
I0630 19:26:28.274128  5429 layer_factory.hpp:77] Creating layer conv1a/bn
I0630 19:26:28.274137  5429 net.cpp:98] Creating Layer conv1a/bn
I0630 19:26:28.274139  5429 net.cpp:439] conv1a/bn <- conv1a
I0630 19:26:28.274143  5429 net.cpp:413] conv1a/bn -> conv1a/bn
I0630 19:26:28.275977  5429 net.cpp:148] Setting up conv1a/bn
I0630 19:26:28.275987  5429 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0630 19:26:28.275990  5429 net.cpp:163] Memory required for data: 188416000
I0630 19:26:28.275995  5429 layer_factory.hpp:77] Creating layer conv1a/relu
I0630 19:26:28.276000  5429 net.cpp:98] Creating Layer conv1a/relu
I0630 19:26:28.276002  5429 net.cpp:439] conv1a/relu <- conv1a/bn
I0630 19:26:28.276005  5429 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0630 19:26:28.276012  5429 net.cpp:148] Setting up conv1a/relu
I0630 19:26:28.276015  5429 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0630 19:26:28.276017  5429 net.cpp:163] Memory required for data: 253952000
I0630 19:26:28.276018  5429 layer_factory.hpp:77] Creating layer conv1b
I0630 19:26:28.276023  5429 net.cpp:98] Creating Layer conv1b
I0630 19:26:28.276026  5429 net.cpp:439] conv1b <- conv1a/bn
I0630 19:26:28.276029  5429 net.cpp:413] conv1b -> conv1b
I0630 19:26:28.276399  5429 net.cpp:148] Setting up conv1b
I0630 19:26:28.276406  5429 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0630 19:26:28.276407  5429 net.cpp:163] Memory required for data: 319488000
I0630 19:26:28.276412  5429 layer_factory.hpp:77] Creating layer conv1b/bn
I0630 19:26:28.276415  5429 net.cpp:98] Creating Layer conv1b/bn
I0630 19:26:28.276417  5429 net.cpp:439] conv1b/bn <- conv1b
I0630 19:26:28.276422  5429 net.cpp:413] conv1b/bn -> conv1b/bn
I0630 19:26:28.277117  5429 net.cpp:148] Setting up conv1b/bn
I0630 19:26:28.277123  5429 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0630 19:26:28.277125  5429 net.cpp:163] Memory required for data: 385024000
I0630 19:26:28.277130  5429 layer_factory.hpp:77] Creating layer conv1b/relu
I0630 19:26:28.277133  5429 net.cpp:98] Creating Layer conv1b/relu
I0630 19:26:28.277135  5429 net.cpp:439] conv1b/relu <- conv1b/bn
I0630 19:26:28.277137  5429 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0630 19:26:28.277140  5429 net.cpp:148] Setting up conv1b/relu
I0630 19:26:28.277143  5429 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0630 19:26:28.277144  5429 net.cpp:163] Memory required for data: 450560000
I0630 19:26:28.277146  5429 layer_factory.hpp:77] Creating layer pool1
I0630 19:26:28.277153  5429 net.cpp:98] Creating Layer pool1
I0630 19:26:28.277155  5429 net.cpp:439] pool1 <- conv1b/bn
I0630 19:26:28.277159  5429 net.cpp:413] pool1 -> pool1
I0630 19:26:28.277590  5429 net.cpp:148] Setting up pool1
I0630 19:26:28.277596  5429 net.cpp:155] Top shape: 5 32 160 160 (4096000)
I0630 19:26:28.277598  5429 net.cpp:163] Memory required for data: 466944000
I0630 19:26:28.277601  5429 layer_factory.hpp:77] Creating layer res2a_branch2a
I0630 19:26:28.277606  5429 net.cpp:98] Creating Layer res2a_branch2a
I0630 19:26:28.277607  5429 net.cpp:439] res2a_branch2a <- pool1
I0630 19:26:28.277611  5429 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0630 19:26:28.279110  5429 net.cpp:148] Setting up res2a_branch2a
I0630 19:26:28.279119  5429 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0630 19:26:28.279121  5429 net.cpp:163] Memory required for data: 499712000
I0630 19:26:28.279126  5429 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0630 19:26:28.279130  5429 net.cpp:98] Creating Layer res2a_branch2a/bn
I0630 19:26:28.279132  5429 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0630 19:26:28.279135  5429 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0630 19:26:28.279806  5429 net.cpp:148] Setting up res2a_branch2a/bn
I0630 19:26:28.279813  5429 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0630 19:26:28.279814  5429 net.cpp:163] Memory required for data: 532480000
I0630 19:26:28.279819  5429 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0630 19:26:28.279822  5429 net.cpp:98] Creating Layer res2a_branch2a/relu
I0630 19:26:28.279824  5429 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0630 19:26:28.279826  5429 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0630 19:26:28.279830  5429 net.cpp:148] Setting up res2a_branch2a/relu
I0630 19:26:28.279832  5429 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0630 19:26:28.279834  5429 net.cpp:163] Memory required for data: 565248000
I0630 19:26:28.279835  5429 layer_factory.hpp:77] Creating layer res2a_branch2b
I0630 19:26:28.279839  5429 net.cpp:98] Creating Layer res2a_branch2b
I0630 19:26:28.279841  5429 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0630 19:26:28.279844  5429 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0630 19:26:28.281138  5429 net.cpp:148] Setting up res2a_branch2b
I0630 19:26:28.281147  5429 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0630 19:26:28.281148  5429 net.cpp:163] Memory required for data: 598016000
I0630 19:26:28.281152  5429 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0630 19:26:28.281157  5429 net.cpp:98] Creating Layer res2a_branch2b/bn
I0630 19:26:28.281158  5429 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0630 19:26:28.281162  5429 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0630 19:26:28.281860  5429 net.cpp:148] Setting up res2a_branch2b/bn
I0630 19:26:28.281867  5429 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0630 19:26:28.281868  5429 net.cpp:163] Memory required for data: 630784000
I0630 19:26:28.281873  5429 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0630 19:26:28.281877  5429 net.cpp:98] Creating Layer res2a_branch2b/relu
I0630 19:26:28.281878  5429 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0630 19:26:28.281880  5429 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0630 19:26:28.281884  5429 net.cpp:148] Setting up res2a_branch2b/relu
I0630 19:26:28.281886  5429 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0630 19:26:28.281888  5429 net.cpp:163] Memory required for data: 663552000
I0630 19:26:28.281890  5429 layer_factory.hpp:77] Creating layer pool2
I0630 19:26:28.281893  5429 net.cpp:98] Creating Layer pool2
I0630 19:26:28.281895  5429 net.cpp:439] pool2 <- res2a_branch2b/bn
I0630 19:26:28.281898  5429 net.cpp:413] pool2 -> pool2
I0630 19:26:28.281932  5429 net.cpp:148] Setting up pool2
I0630 19:26:28.281936  5429 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 19:26:28.281939  5429 net.cpp:163] Memory required for data: 671744000
I0630 19:26:28.281940  5429 layer_factory.hpp:77] Creating layer res3a_branch2a
I0630 19:26:28.281944  5429 net.cpp:98] Creating Layer res3a_branch2a
I0630 19:26:28.281946  5429 net.cpp:439] res3a_branch2a <- pool2
I0630 19:26:28.281949  5429 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0630 19:26:28.283677  5429 net.cpp:148] Setting up res3a_branch2a
I0630 19:26:28.283684  5429 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0630 19:26:28.283686  5429 net.cpp:163] Memory required for data: 688128000
I0630 19:26:28.283689  5429 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0630 19:26:28.283694  5429 net.cpp:98] Creating Layer res3a_branch2a/bn
I0630 19:26:28.283695  5429 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0630 19:26:28.283699  5429 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0630 19:26:28.284312  5429 net.cpp:148] Setting up res3a_branch2a/bn
I0630 19:26:28.284317  5429 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0630 19:26:28.284319  5429 net.cpp:163] Memory required for data: 704512000
I0630 19:26:28.284325  5429 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0630 19:26:28.284328  5429 net.cpp:98] Creating Layer res3a_branch2a/relu
I0630 19:26:28.284330  5429 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0630 19:26:28.284332  5429 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0630 19:26:28.284335  5429 net.cpp:148] Setting up res3a_branch2a/relu
I0630 19:26:28.284337  5429 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0630 19:26:28.284339  5429 net.cpp:163] Memory required for data: 720896000
I0630 19:26:28.284342  5429 layer_factory.hpp:77] Creating layer res3a_branch2b
I0630 19:26:28.284345  5429 net.cpp:98] Creating Layer res3a_branch2b
I0630 19:26:28.284348  5429 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0630 19:26:28.284350  5429 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0630 19:26:28.285348  5429 net.cpp:148] Setting up res3a_branch2b
I0630 19:26:28.285354  5429 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0630 19:26:28.285356  5429 net.cpp:163] Memory required for data: 737280000
I0630 19:26:28.285359  5429 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0630 19:26:28.285363  5429 net.cpp:98] Creating Layer res3a_branch2b/bn
I0630 19:26:28.285365  5429 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0630 19:26:28.285367  5429 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0630 19:26:28.285984  5429 net.cpp:148] Setting up res3a_branch2b/bn
I0630 19:26:28.285990  5429 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0630 19:26:28.285992  5429 net.cpp:163] Memory required for data: 753664000
I0630 19:26:28.285996  5429 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0630 19:26:28.286000  5429 net.cpp:98] Creating Layer res3a_branch2b/relu
I0630 19:26:28.286001  5429 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0630 19:26:28.286010  5429 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0630 19:26:28.286013  5429 net.cpp:148] Setting up res3a_branch2b/relu
I0630 19:26:28.286015  5429 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0630 19:26:28.286017  5429 net.cpp:163] Memory required for data: 770048000
I0630 19:26:28.286020  5429 layer_factory.hpp:77] Creating layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0630 19:26:28.286022  5429 net.cpp:98] Creating Layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0630 19:26:28.286023  5429 net.cpp:439] res3a_branch2b/bn_res3a_branch2b/relu_0_split <- res3a_branch2b/bn
I0630 19:26:28.286026  5429 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0630 19:26:28.286031  5429 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0630 19:26:28.286064  5429 net.cpp:148] Setting up res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0630 19:26:28.286069  5429 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0630 19:26:28.286072  5429 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0630 19:26:28.286073  5429 net.cpp:163] Memory required for data: 802816000
I0630 19:26:28.286074  5429 layer_factory.hpp:77] Creating layer pool3
I0630 19:26:28.286077  5429 net.cpp:98] Creating Layer pool3
I0630 19:26:28.286079  5429 net.cpp:439] pool3 <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0630 19:26:28.286082  5429 net.cpp:413] pool3 -> pool3
I0630 19:26:28.286118  5429 net.cpp:148] Setting up pool3
I0630 19:26:28.286121  5429 net.cpp:155] Top shape: 5 128 40 40 (1024000)
I0630 19:26:28.286123  5429 net.cpp:163] Memory required for data: 806912000
I0630 19:26:28.286125  5429 layer_factory.hpp:77] Creating layer res4a_branch2a
I0630 19:26:28.286129  5429 net.cpp:98] Creating Layer res4a_branch2a
I0630 19:26:28.286131  5429 net.cpp:439] res4a_branch2a <- pool3
I0630 19:26:28.286134  5429 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0630 19:26:28.293206  5429 net.cpp:148] Setting up res4a_branch2a
I0630 19:26:28.293226  5429 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0630 19:26:28.293228  5429 net.cpp:163] Memory required for data: 815104000
I0630 19:26:28.293234  5429 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0630 19:26:28.293243  5429 net.cpp:98] Creating Layer res4a_branch2a/bn
I0630 19:26:28.293246  5429 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0630 19:26:28.293251  5429 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0630 19:26:28.293905  5429 net.cpp:148] Setting up res4a_branch2a/bn
I0630 19:26:28.293910  5429 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0630 19:26:28.293913  5429 net.cpp:163] Memory required for data: 823296000
I0630 19:26:28.293918  5429 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0630 19:26:28.293921  5429 net.cpp:98] Creating Layer res4a_branch2a/relu
I0630 19:26:28.293923  5429 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0630 19:26:28.293926  5429 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0630 19:26:28.293929  5429 net.cpp:148] Setting up res4a_branch2a/relu
I0630 19:26:28.293932  5429 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0630 19:26:28.293933  5429 net.cpp:163] Memory required for data: 831488000
I0630 19:26:28.293936  5429 layer_factory.hpp:77] Creating layer res4a_branch2b
I0630 19:26:28.293941  5429 net.cpp:98] Creating Layer res4a_branch2b
I0630 19:26:28.293942  5429 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0630 19:26:28.293946  5429 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0630 19:26:28.297144  5429 net.cpp:148] Setting up res4a_branch2b
I0630 19:26:28.297150  5429 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0630 19:26:28.297152  5429 net.cpp:163] Memory required for data: 839680000
I0630 19:26:28.297155  5429 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0630 19:26:28.297159  5429 net.cpp:98] Creating Layer res4a_branch2b/bn
I0630 19:26:28.297161  5429 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0630 19:26:28.297175  5429 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0630 19:26:28.297802  5429 net.cpp:148] Setting up res4a_branch2b/bn
I0630 19:26:28.297808  5429 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0630 19:26:28.297811  5429 net.cpp:163] Memory required for data: 847872000
I0630 19:26:28.297814  5429 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0630 19:26:28.297817  5429 net.cpp:98] Creating Layer res4a_branch2b/relu
I0630 19:26:28.297819  5429 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0630 19:26:28.297821  5429 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0630 19:26:28.297825  5429 net.cpp:148] Setting up res4a_branch2b/relu
I0630 19:26:28.297827  5429 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0630 19:26:28.297828  5429 net.cpp:163] Memory required for data: 856064000
I0630 19:26:28.297830  5429 layer_factory.hpp:77] Creating layer pool4
I0630 19:26:28.297834  5429 net.cpp:98] Creating Layer pool4
I0630 19:26:28.297837  5429 net.cpp:439] pool4 <- res4a_branch2b/bn
I0630 19:26:28.297839  5429 net.cpp:413] pool4 -> pool4
I0630 19:26:28.297875  5429 net.cpp:148] Setting up pool4
I0630 19:26:28.297879  5429 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0630 19:26:28.297881  5429 net.cpp:163] Memory required for data: 864256000
I0630 19:26:28.297883  5429 layer_factory.hpp:77] Creating layer res5a_branch2a
I0630 19:26:28.297888  5429 net.cpp:98] Creating Layer res5a_branch2a
I0630 19:26:28.297889  5429 net.cpp:439] res5a_branch2a <- pool4
I0630 19:26:28.297893  5429 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0630 19:26:28.323674  5429 net.cpp:148] Setting up res5a_branch2a
I0630 19:26:28.323693  5429 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0630 19:26:28.323695  5429 net.cpp:163] Memory required for data: 880640000
I0630 19:26:28.323701  5429 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0630 19:26:28.323712  5429 net.cpp:98] Creating Layer res5a_branch2a/bn
I0630 19:26:28.323715  5429 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0630 19:26:28.323719  5429 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0630 19:26:28.324373  5429 net.cpp:148] Setting up res5a_branch2a/bn
I0630 19:26:28.324379  5429 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0630 19:26:28.324381  5429 net.cpp:163] Memory required for data: 897024000
I0630 19:26:28.324386  5429 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0630 19:26:28.324390  5429 net.cpp:98] Creating Layer res5a_branch2a/relu
I0630 19:26:28.324393  5429 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0630 19:26:28.324394  5429 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0630 19:26:28.324398  5429 net.cpp:148] Setting up res5a_branch2a/relu
I0630 19:26:28.324401  5429 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0630 19:26:28.324403  5429 net.cpp:163] Memory required for data: 913408000
I0630 19:26:28.324404  5429 layer_factory.hpp:77] Creating layer res5a_branch2b
I0630 19:26:28.324409  5429 net.cpp:98] Creating Layer res5a_branch2b
I0630 19:26:28.324411  5429 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0630 19:26:28.324414  5429 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0630 19:26:28.337128  5429 net.cpp:148] Setting up res5a_branch2b
I0630 19:26:28.337139  5429 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0630 19:26:28.337141  5429 net.cpp:163] Memory required for data: 929792000
I0630 19:26:28.337147  5429 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0630 19:26:28.337152  5429 net.cpp:98] Creating Layer res5a_branch2b/bn
I0630 19:26:28.337154  5429 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0630 19:26:28.337157  5429 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0630 19:26:28.337800  5429 net.cpp:148] Setting up res5a_branch2b/bn
I0630 19:26:28.337805  5429 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0630 19:26:28.337806  5429 net.cpp:163] Memory required for data: 946176000
I0630 19:26:28.337811  5429 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0630 19:26:28.337815  5429 net.cpp:98] Creating Layer res5a_branch2b/relu
I0630 19:26:28.337824  5429 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0630 19:26:28.337827  5429 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0630 19:26:28.337831  5429 net.cpp:148] Setting up res5a_branch2b/relu
I0630 19:26:28.337833  5429 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0630 19:26:28.337836  5429 net.cpp:163] Memory required for data: 962560000
I0630 19:26:28.337837  5429 layer_factory.hpp:77] Creating layer out5a
I0630 19:26:28.337841  5429 net.cpp:98] Creating Layer out5a
I0630 19:26:28.337843  5429 net.cpp:439] out5a <- res5a_branch2b/bn
I0630 19:26:28.337848  5429 net.cpp:413] out5a -> out5a
I0630 19:26:28.341904  5429 net.cpp:148] Setting up out5a
I0630 19:26:28.341913  5429 net.cpp:155] Top shape: 5 64 40 40 (512000)
I0630 19:26:28.341915  5429 net.cpp:163] Memory required for data: 964608000
I0630 19:26:28.341919  5429 layer_factory.hpp:77] Creating layer out5a/bn
I0630 19:26:28.341923  5429 net.cpp:98] Creating Layer out5a/bn
I0630 19:26:28.341925  5429 net.cpp:439] out5a/bn <- out5a
I0630 19:26:28.341928  5429 net.cpp:413] out5a/bn -> out5a/bn
I0630 19:26:28.342639  5429 net.cpp:148] Setting up out5a/bn
I0630 19:26:28.342645  5429 net.cpp:155] Top shape: 5 64 40 40 (512000)
I0630 19:26:28.342648  5429 net.cpp:163] Memory required for data: 966656000
I0630 19:26:28.342653  5429 layer_factory.hpp:77] Creating layer out5a/relu
I0630 19:26:28.342655  5429 net.cpp:98] Creating Layer out5a/relu
I0630 19:26:28.342658  5429 net.cpp:439] out5a/relu <- out5a/bn
I0630 19:26:28.342659  5429 net.cpp:400] out5a/relu -> out5a/bn (in-place)
I0630 19:26:28.342663  5429 net.cpp:148] Setting up out5a/relu
I0630 19:26:28.342665  5429 net.cpp:155] Top shape: 5 64 40 40 (512000)
I0630 19:26:28.342666  5429 net.cpp:163] Memory required for data: 968704000
I0630 19:26:28.342669  5429 layer_factory.hpp:77] Creating layer out5a_up2
I0630 19:26:28.342672  5429 net.cpp:98] Creating Layer out5a_up2
I0630 19:26:28.342674  5429 net.cpp:439] out5a_up2 <- out5a/bn
I0630 19:26:28.342676  5429 net.cpp:413] out5a_up2 -> out5a_up2
I0630 19:26:28.342924  5429 net.cpp:148] Setting up out5a_up2
I0630 19:26:28.342929  5429 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 19:26:28.342931  5429 net.cpp:163] Memory required for data: 976896000
I0630 19:26:28.342934  5429 layer_factory.hpp:77] Creating layer out3a
I0630 19:26:28.342938  5429 net.cpp:98] Creating Layer out3a
I0630 19:26:28.342941  5429 net.cpp:439] out3a <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0630 19:26:28.342943  5429 net.cpp:413] out3a -> out3a
I0630 19:26:28.343966  5429 net.cpp:148] Setting up out3a
I0630 19:26:28.343971  5429 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 19:26:28.343973  5429 net.cpp:163] Memory required for data: 985088000
I0630 19:26:28.343976  5429 layer_factory.hpp:77] Creating layer out3a/bn
I0630 19:26:28.343981  5429 net.cpp:98] Creating Layer out3a/bn
I0630 19:26:28.343983  5429 net.cpp:439] out3a/bn <- out3a
I0630 19:26:28.343986  5429 net.cpp:413] out3a/bn -> out3a/bn
I0630 19:26:28.344696  5429 net.cpp:148] Setting up out3a/bn
I0630 19:26:28.344702  5429 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 19:26:28.344703  5429 net.cpp:163] Memory required for data: 993280000
I0630 19:26:28.344707  5429 layer_factory.hpp:77] Creating layer out3a/relu
I0630 19:26:28.344710  5429 net.cpp:98] Creating Layer out3a/relu
I0630 19:26:28.344712  5429 net.cpp:439] out3a/relu <- out3a/bn
I0630 19:26:28.344714  5429 net.cpp:400] out3a/relu -> out3a/bn (in-place)
I0630 19:26:28.344718  5429 net.cpp:148] Setting up out3a/relu
I0630 19:26:28.344720  5429 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 19:26:28.344722  5429 net.cpp:163] Memory required for data: 1001472000
I0630 19:26:28.344723  5429 layer_factory.hpp:77] Creating layer out3_out5_combined
I0630 19:26:28.344729  5429 net.cpp:98] Creating Layer out3_out5_combined
I0630 19:26:28.344732  5429 net.cpp:439] out3_out5_combined <- out5a_up2
I0630 19:26:28.344733  5429 net.cpp:439] out3_out5_combined <- out3a/bn
I0630 19:26:28.344743  5429 net.cpp:413] out3_out5_combined -> out3_out5_combined
I0630 19:26:28.344765  5429 net.cpp:148] Setting up out3_out5_combined
I0630 19:26:28.344769  5429 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 19:26:28.344770  5429 net.cpp:163] Memory required for data: 1009664000
I0630 19:26:28.344772  5429 layer_factory.hpp:77] Creating layer ctx_conv1
I0630 19:26:28.344776  5429 net.cpp:98] Creating Layer ctx_conv1
I0630 19:26:28.344779  5429 net.cpp:439] ctx_conv1 <- out3_out5_combined
I0630 19:26:28.344780  5429 net.cpp:413] ctx_conv1 -> ctx_conv1
I0630 19:26:28.345795  5429 net.cpp:148] Setting up ctx_conv1
I0630 19:26:28.345800  5429 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 19:26:28.345803  5429 net.cpp:163] Memory required for data: 1017856000
I0630 19:26:28.345805  5429 layer_factory.hpp:77] Creating layer ctx_conv1/bn
I0630 19:26:28.345808  5429 net.cpp:98] Creating Layer ctx_conv1/bn
I0630 19:26:28.345810  5429 net.cpp:439] ctx_conv1/bn <- ctx_conv1
I0630 19:26:28.345813  5429 net.cpp:413] ctx_conv1/bn -> ctx_conv1/bn
I0630 19:26:28.346529  5429 net.cpp:148] Setting up ctx_conv1/bn
I0630 19:26:28.346535  5429 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 19:26:28.346537  5429 net.cpp:163] Memory required for data: 1026048000
I0630 19:26:28.346541  5429 layer_factory.hpp:77] Creating layer ctx_conv1/relu
I0630 19:26:28.346544  5429 net.cpp:98] Creating Layer ctx_conv1/relu
I0630 19:26:28.346546  5429 net.cpp:439] ctx_conv1/relu <- ctx_conv1/bn
I0630 19:26:28.346549  5429 net.cpp:400] ctx_conv1/relu -> ctx_conv1/bn (in-place)
I0630 19:26:28.346551  5429 net.cpp:148] Setting up ctx_conv1/relu
I0630 19:26:28.346554  5429 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 19:26:28.346555  5429 net.cpp:163] Memory required for data: 1034240000
I0630 19:26:28.346557  5429 layer_factory.hpp:77] Creating layer ctx_conv2
I0630 19:26:28.346561  5429 net.cpp:98] Creating Layer ctx_conv2
I0630 19:26:28.346563  5429 net.cpp:439] ctx_conv2 <- ctx_conv1/bn
I0630 19:26:28.346566  5429 net.cpp:413] ctx_conv2 -> ctx_conv2
I0630 19:26:28.347579  5429 net.cpp:148] Setting up ctx_conv2
I0630 19:26:28.347584  5429 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 19:26:28.347585  5429 net.cpp:163] Memory required for data: 1042432000
I0630 19:26:28.347589  5429 layer_factory.hpp:77] Creating layer ctx_conv2/bn
I0630 19:26:28.347591  5429 net.cpp:98] Creating Layer ctx_conv2/bn
I0630 19:26:28.347594  5429 net.cpp:439] ctx_conv2/bn <- ctx_conv2
I0630 19:26:28.347595  5429 net.cpp:413] ctx_conv2/bn -> ctx_conv2/bn
I0630 19:26:28.348312  5429 net.cpp:148] Setting up ctx_conv2/bn
I0630 19:26:28.348317  5429 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 19:26:28.348320  5429 net.cpp:163] Memory required for data: 1050624000
I0630 19:26:28.348325  5429 layer_factory.hpp:77] Creating layer ctx_conv2/relu
I0630 19:26:28.348327  5429 net.cpp:98] Creating Layer ctx_conv2/relu
I0630 19:26:28.348330  5429 net.cpp:439] ctx_conv2/relu <- ctx_conv2/bn
I0630 19:26:28.348331  5429 net.cpp:400] ctx_conv2/relu -> ctx_conv2/bn (in-place)
I0630 19:26:28.348335  5429 net.cpp:148] Setting up ctx_conv2/relu
I0630 19:26:28.348337  5429 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 19:26:28.348338  5429 net.cpp:163] Memory required for data: 1058816000
I0630 19:26:28.348340  5429 layer_factory.hpp:77] Creating layer ctx_conv3
I0630 19:26:28.348345  5429 net.cpp:98] Creating Layer ctx_conv3
I0630 19:26:28.348346  5429 net.cpp:439] ctx_conv3 <- ctx_conv2/bn
I0630 19:26:28.348348  5429 net.cpp:413] ctx_conv3 -> ctx_conv3
I0630 19:26:28.349366  5429 net.cpp:148] Setting up ctx_conv3
I0630 19:26:28.349371  5429 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 19:26:28.349373  5429 net.cpp:163] Memory required for data: 1067008000
I0630 19:26:28.349376  5429 layer_factory.hpp:77] Creating layer ctx_conv3/bn
I0630 19:26:28.349380  5429 net.cpp:98] Creating Layer ctx_conv3/bn
I0630 19:26:28.349381  5429 net.cpp:439] ctx_conv3/bn <- ctx_conv3
I0630 19:26:28.349383  5429 net.cpp:413] ctx_conv3/bn -> ctx_conv3/bn
I0630 19:26:28.350175  5429 net.cpp:148] Setting up ctx_conv3/bn
I0630 19:26:28.350186  5429 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 19:26:28.350190  5429 net.cpp:163] Memory required for data: 1075200000
I0630 19:26:28.350198  5429 layer_factory.hpp:77] Creating layer ctx_conv3/relu
I0630 19:26:28.350206  5429 net.cpp:98] Creating Layer ctx_conv3/relu
I0630 19:26:28.350211  5429 net.cpp:439] ctx_conv3/relu <- ctx_conv3/bn
I0630 19:26:28.350219  5429 net.cpp:400] ctx_conv3/relu -> ctx_conv3/bn (in-place)
I0630 19:26:28.350226  5429 net.cpp:148] Setting up ctx_conv3/relu
I0630 19:26:28.350232  5429 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 19:26:28.350235  5429 net.cpp:163] Memory required for data: 1083392000
I0630 19:26:28.350240  5429 layer_factory.hpp:77] Creating layer ctx_conv4
I0630 19:26:28.350247  5429 net.cpp:98] Creating Layer ctx_conv4
I0630 19:26:28.350250  5429 net.cpp:439] ctx_conv4 <- ctx_conv3/bn
I0630 19:26:28.350253  5429 net.cpp:413] ctx_conv4 -> ctx_conv4
I0630 19:26:28.351349  5429 net.cpp:148] Setting up ctx_conv4
I0630 19:26:28.351359  5429 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 19:26:28.351361  5429 net.cpp:163] Memory required for data: 1091584000
I0630 19:26:28.351364  5429 layer_factory.hpp:77] Creating layer ctx_conv4/bn
I0630 19:26:28.351369  5429 net.cpp:98] Creating Layer ctx_conv4/bn
I0630 19:26:28.351372  5429 net.cpp:439] ctx_conv4/bn <- ctx_conv4
I0630 19:26:28.351375  5429 net.cpp:413] ctx_conv4/bn -> ctx_conv4/bn
I0630 19:26:28.352110  5429 net.cpp:148] Setting up ctx_conv4/bn
I0630 19:26:28.352116  5429 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 19:26:28.352118  5429 net.cpp:163] Memory required for data: 1099776000
I0630 19:26:28.352123  5429 layer_factory.hpp:77] Creating layer ctx_conv4/relu
I0630 19:26:28.352128  5429 net.cpp:98] Creating Layer ctx_conv4/relu
I0630 19:26:28.352130  5429 net.cpp:439] ctx_conv4/relu <- ctx_conv4/bn
I0630 19:26:28.352133  5429 net.cpp:400] ctx_conv4/relu -> ctx_conv4/bn (in-place)
I0630 19:26:28.352136  5429 net.cpp:148] Setting up ctx_conv4/relu
I0630 19:26:28.352139  5429 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 19:26:28.352141  5429 net.cpp:163] Memory required for data: 1107968000
I0630 19:26:28.352144  5429 layer_factory.hpp:77] Creating layer ctx_final
I0630 19:26:28.352149  5429 net.cpp:98] Creating Layer ctx_final
I0630 19:26:28.352150  5429 net.cpp:439] ctx_final <- ctx_conv4/bn
I0630 19:26:28.352154  5429 net.cpp:413] ctx_final -> ctx_final
I0630 19:26:28.352687  5429 net.cpp:148] Setting up ctx_final
I0630 19:26:28.352692  5429 net.cpp:155] Top shape: 5 20 80 80 (640000)
I0630 19:26:28.352695  5429 net.cpp:163] Memory required for data: 1110528000
I0630 19:26:28.352699  5429 layer_factory.hpp:77] Creating layer ctx_final/relu
I0630 19:26:28.352701  5429 net.cpp:98] Creating Layer ctx_final/relu
I0630 19:26:28.352705  5429 net.cpp:439] ctx_final/relu <- ctx_final
I0630 19:26:28.352707  5429 net.cpp:400] ctx_final/relu -> ctx_final (in-place)
I0630 19:26:28.352711  5429 net.cpp:148] Setting up ctx_final/relu
I0630 19:26:28.352713  5429 net.cpp:155] Top shape: 5 20 80 80 (640000)
I0630 19:26:28.352716  5429 net.cpp:163] Memory required for data: 1113088000
I0630 19:26:28.352718  5429 layer_factory.hpp:77] Creating layer out_deconv_final_up2
I0630 19:26:28.352721  5429 net.cpp:98] Creating Layer out_deconv_final_up2
I0630 19:26:28.352725  5429 net.cpp:439] out_deconv_final_up2 <- ctx_final
I0630 19:26:28.352728  5429 net.cpp:413] out_deconv_final_up2 -> out_deconv_final_up2
I0630 19:26:28.352967  5429 net.cpp:148] Setting up out_deconv_final_up2
I0630 19:26:28.352972  5429 net.cpp:155] Top shape: 5 20 160 160 (2560000)
I0630 19:26:28.352973  5429 net.cpp:163] Memory required for data: 1123328000
I0630 19:26:28.352977  5429 layer_factory.hpp:77] Creating layer out_deconv_final_up4
I0630 19:26:28.352979  5429 net.cpp:98] Creating Layer out_deconv_final_up4
I0630 19:26:28.352982  5429 net.cpp:439] out_deconv_final_up4 <- out_deconv_final_up2
I0630 19:26:28.352985  5429 net.cpp:413] out_deconv_final_up4 -> out_deconv_final_up4
I0630 19:26:28.353235  5429 net.cpp:148] Setting up out_deconv_final_up4
I0630 19:26:28.353240  5429 net.cpp:155] Top shape: 5 20 320 320 (10240000)
I0630 19:26:28.353243  5429 net.cpp:163] Memory required for data: 1164288000
I0630 19:26:28.353245  5429 layer_factory.hpp:77] Creating layer out_deconv_final_up8
I0630 19:26:28.353250  5429 net.cpp:98] Creating Layer out_deconv_final_up8
I0630 19:26:28.353251  5429 net.cpp:439] out_deconv_final_up8 <- out_deconv_final_up4
I0630 19:26:28.353255  5429 net.cpp:413] out_deconv_final_up8 -> out_deconv_final_up8
I0630 19:26:28.353488  5429 net.cpp:148] Setting up out_deconv_final_up8
I0630 19:26:28.353493  5429 net.cpp:155] Top shape: 5 20 640 640 (40960000)
I0630 19:26:28.353495  5429 net.cpp:163] Memory required for data: 1328128000
I0630 19:26:28.353498  5429 layer_factory.hpp:77] Creating layer loss
I0630 19:26:28.353798  5429 net.cpp:98] Creating Layer loss
I0630 19:26:28.353803  5429 net.cpp:439] loss <- out_deconv_final_up8
I0630 19:26:28.353806  5429 net.cpp:439] loss <- label
I0630 19:26:28.353811  5429 net.cpp:413] loss -> loss
I0630 19:26:28.353819  5429 layer_factory.hpp:77] Creating layer loss
I0630 19:26:28.404989  5429 net.cpp:148] Setting up loss
I0630 19:26:28.405009  5429 net.cpp:155] Top shape: (1)
I0630 19:26:28.405012  5429 net.cpp:158]     with loss weight 1
I0630 19:26:28.405025  5429 net.cpp:163] Memory required for data: 1328128004
I0630 19:26:28.405028  5429 net.cpp:224] loss needs backward computation.
I0630 19:26:28.405031  5429 net.cpp:224] out_deconv_final_up8 needs backward computation.
I0630 19:26:28.405033  5429 net.cpp:224] out_deconv_final_up4 needs backward computation.
I0630 19:26:28.405035  5429 net.cpp:224] out_deconv_final_up2 needs backward computation.
I0630 19:26:28.405037  5429 net.cpp:224] ctx_final/relu needs backward computation.
I0630 19:26:28.405040  5429 net.cpp:224] ctx_final needs backward computation.
I0630 19:26:28.405041  5429 net.cpp:224] ctx_conv4/relu needs backward computation.
I0630 19:26:28.405043  5429 net.cpp:224] ctx_conv4/bn needs backward computation.
I0630 19:26:28.405045  5429 net.cpp:224] ctx_conv4 needs backward computation.
I0630 19:26:28.405047  5429 net.cpp:224] ctx_conv3/relu needs backward computation.
I0630 19:26:28.405050  5429 net.cpp:224] ctx_conv3/bn needs backward computation.
I0630 19:26:28.405051  5429 net.cpp:224] ctx_conv3 needs backward computation.
I0630 19:26:28.405053  5429 net.cpp:224] ctx_conv2/relu needs backward computation.
I0630 19:26:28.405056  5429 net.cpp:224] ctx_conv2/bn needs backward computation.
I0630 19:26:28.405058  5429 net.cpp:224] ctx_conv2 needs backward computation.
I0630 19:26:28.405061  5429 net.cpp:224] ctx_conv1/relu needs backward computation.
I0630 19:26:28.405062  5429 net.cpp:224] ctx_conv1/bn needs backward computation.
I0630 19:26:28.405064  5429 net.cpp:224] ctx_conv1 needs backward computation.
I0630 19:26:28.405067  5429 net.cpp:224] out3_out5_combined needs backward computation.
I0630 19:26:28.405069  5429 net.cpp:224] out3a/relu needs backward computation.
I0630 19:26:28.405071  5429 net.cpp:224] out3a/bn needs backward computation.
I0630 19:26:28.405074  5429 net.cpp:224] out3a needs backward computation.
I0630 19:26:28.405076  5429 net.cpp:224] out5a_up2 needs backward computation.
I0630 19:26:28.405078  5429 net.cpp:224] out5a/relu needs backward computation.
I0630 19:26:28.405081  5429 net.cpp:224] out5a/bn needs backward computation.
I0630 19:26:28.405083  5429 net.cpp:224] out5a needs backward computation.
I0630 19:26:28.405086  5429 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0630 19:26:28.405087  5429 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0630 19:26:28.405091  5429 net.cpp:224] res5a_branch2b needs backward computation.
I0630 19:26:28.405092  5429 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0630 19:26:28.405094  5429 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0630 19:26:28.405097  5429 net.cpp:224] res5a_branch2a needs backward computation.
I0630 19:26:28.405109  5429 net.cpp:224] pool4 needs backward computation.
I0630 19:26:28.405112  5429 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0630 19:26:28.405113  5429 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0630 19:26:28.405115  5429 net.cpp:224] res4a_branch2b needs backward computation.
I0630 19:26:28.405117  5429 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0630 19:26:28.405119  5429 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0630 19:26:28.405122  5429 net.cpp:224] res4a_branch2a needs backward computation.
I0630 19:26:28.405123  5429 net.cpp:224] pool3 needs backward computation.
I0630 19:26:28.405125  5429 net.cpp:224] res3a_branch2b/bn_res3a_branch2b/relu_0_split needs backward computation.
I0630 19:26:28.405128  5429 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0630 19:26:28.405129  5429 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0630 19:26:28.405131  5429 net.cpp:224] res3a_branch2b needs backward computation.
I0630 19:26:28.405133  5429 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0630 19:26:28.405135  5429 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0630 19:26:28.405138  5429 net.cpp:224] res3a_branch2a needs backward computation.
I0630 19:26:28.405139  5429 net.cpp:224] pool2 needs backward computation.
I0630 19:26:28.405141  5429 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0630 19:26:28.405143  5429 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0630 19:26:28.405145  5429 net.cpp:224] res2a_branch2b needs backward computation.
I0630 19:26:28.405148  5429 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0630 19:26:28.405149  5429 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0630 19:26:28.405151  5429 net.cpp:224] res2a_branch2a needs backward computation.
I0630 19:26:28.405153  5429 net.cpp:224] pool1 needs backward computation.
I0630 19:26:28.405155  5429 net.cpp:224] conv1b/relu needs backward computation.
I0630 19:26:28.405158  5429 net.cpp:224] conv1b/bn needs backward computation.
I0630 19:26:28.405160  5429 net.cpp:224] conv1b needs backward computation.
I0630 19:26:28.405163  5429 net.cpp:224] conv1a/relu needs backward computation.
I0630 19:26:28.405165  5429 net.cpp:224] conv1a/bn needs backward computation.
I0630 19:26:28.405167  5429 net.cpp:224] conv1a needs backward computation.
I0630 19:26:28.405170  5429 net.cpp:226] data/bias does not need backward computation.
I0630 19:26:28.405172  5429 net.cpp:226] data does not need backward computation.
I0630 19:26:28.405174  5429 net.cpp:268] This network produces output loss
I0630 19:26:28.405206  5429 net.cpp:288] Network initialization done.
I0630 19:26:28.405920  5429 solver.cpp:182] Creating test net (#0) specified by test_net file: training/cityscapes20_jsegnet21v2_2017-06-30_19-26-17/initial/test.prototxt
I0630 19:26:28.406195  5429 net.cpp:56] Initializing net from parameters: 
name: "jsegnet21v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/val-image-lmdb"
    label_list_path: "data/val-label-lmdb"
    batch_size: 4
    threads: 4
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b/bn"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a/bn"
  top: "out5a/bn"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a/bn"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b/bn"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a/bn"
  top: "out3a/bn"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a/bn"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv1/bn"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv2/bn"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv3/bn"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4/bn"
  top: "ctx_conv4/bn"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4/bn"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 20
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 20
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 20
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
  accuracy_param {
    ignore_label: 255
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
    ignore_label: 255
  }
}
I0630 19:26:28.406316  5429 layer_factory.hpp:77] Creating layer data
I0630 19:26:28.406322  5429 net.cpp:98] Creating Layer data
I0630 19:26:28.406325  5429 net.cpp:413] data -> data
I0630 19:26:28.406330  5429 net.cpp:413] data -> label
I0630 19:26:28.421135  5520 db_lmdb.cpp:35] Opened lmdb data/val-image-lmdb
I0630 19:26:28.439621  5525 db_lmdb.cpp:35] Opened lmdb data/val-label-lmdb
I0630 19:26:28.687757  5429 data_layer.cpp:78] ReshapePrefetch 4, 3, 640, 640
I0630 19:26:28.687839  5429 data_layer.cpp:83] output data size: 4,3,640,640
I0630 19:26:28.711022  5429 data_layer.cpp:78] ReshapePrefetch 4, 1, 640, 640
I0630 19:26:28.711097  5429 data_layer.cpp:83] output data size: 4,1,640,640
I0630 19:26:28.721737  5429 net.cpp:148] Setting up data
I0630 19:26:28.721755  5429 net.cpp:155] Top shape: 4 3 640 640 (4915200)
I0630 19:26:28.721758  5429 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0630 19:26:28.721760  5429 net.cpp:163] Memory required for data: 26214400
I0630 19:26:28.721765  5429 layer_factory.hpp:77] Creating layer label_data_1_split
I0630 19:26:28.721771  5429 net.cpp:98] Creating Layer label_data_1_split
I0630 19:26:28.721774  5429 net.cpp:439] label_data_1_split <- label
I0630 19:26:28.721792  5429 net.cpp:413] label_data_1_split -> label_data_1_split_0
I0630 19:26:28.721797  5429 net.cpp:413] label_data_1_split -> label_data_1_split_1
I0630 19:26:28.721801  5429 net.cpp:413] label_data_1_split -> label_data_1_split_2
I0630 19:26:28.721948  5429 net.cpp:148] Setting up label_data_1_split
I0630 19:26:28.721959  5429 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0630 19:26:28.721962  5429 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0630 19:26:28.721964  5429 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0630 19:26:28.721966  5429 net.cpp:163] Memory required for data: 45875200
I0630 19:26:28.721971  5429 layer_factory.hpp:77] Creating layer data/bias
I0630 19:26:28.721976  5429 net.cpp:98] Creating Layer data/bias
I0630 19:26:28.721978  5429 net.cpp:439] data/bias <- data
I0630 19:26:28.721982  5429 net.cpp:413] data/bias -> data/bias
I0630 19:26:28.723155  5429 net.cpp:148] Setting up data/bias
I0630 19:26:28.723163  5429 net.cpp:155] Top shape: 4 3 640 640 (4915200)
I0630 19:26:28.723165  5429 net.cpp:163] Memory required for data: 65536000
I0630 19:26:28.723171  5429 layer_factory.hpp:77] Creating layer conv1a
I0630 19:26:28.723181  5429 net.cpp:98] Creating Layer conv1a
I0630 19:26:28.723182  5429 net.cpp:439] conv1a <- data/bias
I0630 19:26:28.723186  5429 net.cpp:413] conv1a -> conv1a
I0630 19:26:28.724521  5429 net.cpp:148] Setting up conv1a
I0630 19:26:28.724540  5429 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0630 19:26:28.724545  5429 net.cpp:163] Memory required for data: 117964800
I0630 19:26:28.724556  5429 layer_factory.hpp:77] Creating layer conv1a/bn
I0630 19:26:28.724572  5429 net.cpp:98] Creating Layer conv1a/bn
I0630 19:26:28.724578  5429 net.cpp:439] conv1a/bn <- conv1a
I0630 19:26:28.724587  5429 net.cpp:413] conv1a/bn -> conv1a/bn
I0630 19:26:28.725872  5429 net.cpp:148] Setting up conv1a/bn
I0630 19:26:28.725881  5429 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0630 19:26:28.725884  5429 net.cpp:163] Memory required for data: 170393600
I0630 19:26:28.725893  5429 layer_factory.hpp:77] Creating layer conv1a/relu
I0630 19:26:28.725899  5429 net.cpp:98] Creating Layer conv1a/relu
I0630 19:26:28.725903  5429 net.cpp:439] conv1a/relu <- conv1a/bn
I0630 19:26:28.725905  5429 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0630 19:26:28.725911  5429 net.cpp:148] Setting up conv1a/relu
I0630 19:26:28.725914  5429 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0630 19:26:28.725927  5429 net.cpp:163] Memory required for data: 222822400
I0630 19:26:28.725931  5429 layer_factory.hpp:77] Creating layer conv1b
I0630 19:26:28.725939  5429 net.cpp:98] Creating Layer conv1b
I0630 19:26:28.725942  5429 net.cpp:439] conv1b <- conv1a/bn
I0630 19:26:28.725946  5429 net.cpp:413] conv1b -> conv1b
I0630 19:26:28.726514  5429 net.cpp:148] Setting up conv1b
I0630 19:26:28.726521  5429 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0630 19:26:28.726523  5429 net.cpp:163] Memory required for data: 275251200
I0630 19:26:28.726527  5429 layer_factory.hpp:77] Creating layer conv1b/bn
I0630 19:26:28.726531  5429 net.cpp:98] Creating Layer conv1b/bn
I0630 19:26:28.726533  5429 net.cpp:439] conv1b/bn <- conv1b
I0630 19:26:28.726537  5429 net.cpp:413] conv1b/bn -> conv1b/bn
I0630 19:26:28.727335  5429 net.cpp:148] Setting up conv1b/bn
I0630 19:26:28.727341  5429 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0630 19:26:28.727344  5429 net.cpp:163] Memory required for data: 327680000
I0630 19:26:28.727349  5429 layer_factory.hpp:77] Creating layer conv1b/relu
I0630 19:26:28.727351  5429 net.cpp:98] Creating Layer conv1b/relu
I0630 19:26:28.727354  5429 net.cpp:439] conv1b/relu <- conv1b/bn
I0630 19:26:28.727355  5429 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0630 19:26:28.727358  5429 net.cpp:148] Setting up conv1b/relu
I0630 19:26:28.727361  5429 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0630 19:26:28.727362  5429 net.cpp:163] Memory required for data: 380108800
I0630 19:26:28.727365  5429 layer_factory.hpp:77] Creating layer pool1
I0630 19:26:28.727378  5429 net.cpp:98] Creating Layer pool1
I0630 19:26:28.727381  5429 net.cpp:439] pool1 <- conv1b/bn
I0630 19:26:28.727383  5429 net.cpp:413] pool1 -> pool1
I0630 19:26:28.727422  5429 net.cpp:148] Setting up pool1
I0630 19:26:28.727427  5429 net.cpp:155] Top shape: 4 32 160 160 (3276800)
I0630 19:26:28.727428  5429 net.cpp:163] Memory required for data: 393216000
I0630 19:26:28.727430  5429 layer_factory.hpp:77] Creating layer res2a_branch2a
I0630 19:26:28.727433  5429 net.cpp:98] Creating Layer res2a_branch2a
I0630 19:26:28.727435  5429 net.cpp:439] res2a_branch2a <- pool1
I0630 19:26:28.727438  5429 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0630 19:26:28.728130  5429 net.cpp:148] Setting up res2a_branch2a
I0630 19:26:28.728135  5429 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0630 19:26:28.728137  5429 net.cpp:163] Memory required for data: 419430400
I0630 19:26:28.728142  5429 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0630 19:26:28.728145  5429 net.cpp:98] Creating Layer res2a_branch2a/bn
I0630 19:26:28.728147  5429 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0630 19:26:28.728149  5429 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0630 19:26:28.728896  5429 net.cpp:148] Setting up res2a_branch2a/bn
I0630 19:26:28.728901  5429 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0630 19:26:28.728904  5429 net.cpp:163] Memory required for data: 445644800
I0630 19:26:28.728909  5429 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0630 19:26:28.728911  5429 net.cpp:98] Creating Layer res2a_branch2a/relu
I0630 19:26:28.728914  5429 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0630 19:26:28.728915  5429 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0630 19:26:28.728919  5429 net.cpp:148] Setting up res2a_branch2a/relu
I0630 19:26:28.728920  5429 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0630 19:26:28.728922  5429 net.cpp:163] Memory required for data: 471859200
I0630 19:26:28.728924  5429 layer_factory.hpp:77] Creating layer res2a_branch2b
I0630 19:26:28.728927  5429 net.cpp:98] Creating Layer res2a_branch2b
I0630 19:26:28.728929  5429 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0630 19:26:28.728932  5429 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0630 19:26:28.729435  5429 net.cpp:148] Setting up res2a_branch2b
I0630 19:26:28.729440  5429 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0630 19:26:28.729442  5429 net.cpp:163] Memory required for data: 498073600
I0630 19:26:28.729445  5429 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0630 19:26:28.729449  5429 net.cpp:98] Creating Layer res2a_branch2b/bn
I0630 19:26:28.729451  5429 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0630 19:26:28.729454  5429 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0630 19:26:28.730197  5429 net.cpp:148] Setting up res2a_branch2b/bn
I0630 19:26:28.730202  5429 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0630 19:26:28.730204  5429 net.cpp:163] Memory required for data: 524288000
I0630 19:26:28.730209  5429 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0630 19:26:28.730212  5429 net.cpp:98] Creating Layer res2a_branch2b/relu
I0630 19:26:28.730214  5429 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0630 19:26:28.730216  5429 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0630 19:26:28.730219  5429 net.cpp:148] Setting up res2a_branch2b/relu
I0630 19:26:28.730222  5429 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0630 19:26:28.730223  5429 net.cpp:163] Memory required for data: 550502400
I0630 19:26:28.730226  5429 layer_factory.hpp:77] Creating layer pool2
I0630 19:26:28.730229  5429 net.cpp:98] Creating Layer pool2
I0630 19:26:28.730231  5429 net.cpp:439] pool2 <- res2a_branch2b/bn
I0630 19:26:28.730233  5429 net.cpp:413] pool2 -> pool2
I0630 19:26:28.730273  5429 net.cpp:148] Setting up pool2
I0630 19:26:28.730276  5429 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 19:26:28.730278  5429 net.cpp:163] Memory required for data: 557056000
I0630 19:26:28.730280  5429 layer_factory.hpp:77] Creating layer res3a_branch2a
I0630 19:26:28.730290  5429 net.cpp:98] Creating Layer res3a_branch2a
I0630 19:26:28.730293  5429 net.cpp:439] res3a_branch2a <- pool2
I0630 19:26:28.730296  5429 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0630 19:26:28.732085  5429 net.cpp:148] Setting up res3a_branch2a
I0630 19:26:28.732095  5429 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0630 19:26:28.732096  5429 net.cpp:163] Memory required for data: 570163200
I0630 19:26:28.732100  5429 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0630 19:26:28.732103  5429 net.cpp:98] Creating Layer res3a_branch2a/bn
I0630 19:26:28.732107  5429 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0630 19:26:28.732111  5429 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0630 19:26:28.732796  5429 net.cpp:148] Setting up res3a_branch2a/bn
I0630 19:26:28.732801  5429 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0630 19:26:28.732803  5429 net.cpp:163] Memory required for data: 583270400
I0630 19:26:28.732810  5429 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0630 19:26:28.732813  5429 net.cpp:98] Creating Layer res3a_branch2a/relu
I0630 19:26:28.732815  5429 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0630 19:26:28.732818  5429 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0630 19:26:28.732822  5429 net.cpp:148] Setting up res3a_branch2a/relu
I0630 19:26:28.732825  5429 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0630 19:26:28.732826  5429 net.cpp:163] Memory required for data: 596377600
I0630 19:26:28.732828  5429 layer_factory.hpp:77] Creating layer res3a_branch2b
I0630 19:26:28.732832  5429 net.cpp:98] Creating Layer res3a_branch2b
I0630 19:26:28.732836  5429 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0630 19:26:28.732838  5429 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0630 19:26:28.733870  5429 net.cpp:148] Setting up res3a_branch2b
I0630 19:26:28.733875  5429 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0630 19:26:28.733876  5429 net.cpp:163] Memory required for data: 609484800
I0630 19:26:28.733880  5429 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0630 19:26:28.733882  5429 net.cpp:98] Creating Layer res3a_branch2b/bn
I0630 19:26:28.733886  5429 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0630 19:26:28.733888  5429 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0630 19:26:28.734571  5429 net.cpp:148] Setting up res3a_branch2b/bn
I0630 19:26:28.734577  5429 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0630 19:26:28.734580  5429 net.cpp:163] Memory required for data: 622592000
I0630 19:26:28.734585  5429 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0630 19:26:28.734587  5429 net.cpp:98] Creating Layer res3a_branch2b/relu
I0630 19:26:28.734589  5429 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0630 19:26:28.734591  5429 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0630 19:26:28.734594  5429 net.cpp:148] Setting up res3a_branch2b/relu
I0630 19:26:28.734596  5429 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0630 19:26:28.734598  5429 net.cpp:163] Memory required for data: 635699200
I0630 19:26:28.734601  5429 layer_factory.hpp:77] Creating layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0630 19:26:28.734603  5429 net.cpp:98] Creating Layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0630 19:26:28.734606  5429 net.cpp:439] res3a_branch2b/bn_res3a_branch2b/relu_0_split <- res3a_branch2b/bn
I0630 19:26:28.734607  5429 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0630 19:26:28.734611  5429 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0630 19:26:28.734647  5429 net.cpp:148] Setting up res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0630 19:26:28.734652  5429 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0630 19:26:28.734653  5429 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0630 19:26:28.734655  5429 net.cpp:163] Memory required for data: 661913600
I0630 19:26:28.734664  5429 layer_factory.hpp:77] Creating layer pool3
I0630 19:26:28.734668  5429 net.cpp:98] Creating Layer pool3
I0630 19:26:28.734669  5429 net.cpp:439] pool3 <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0630 19:26:28.734673  5429 net.cpp:413] pool3 -> pool3
I0630 19:26:28.734714  5429 net.cpp:148] Setting up pool3
I0630 19:26:28.734719  5429 net.cpp:155] Top shape: 4 128 40 40 (819200)
I0630 19:26:28.734720  5429 net.cpp:163] Memory required for data: 665190400
I0630 19:26:28.734722  5429 layer_factory.hpp:77] Creating layer res4a_branch2a
I0630 19:26:28.734726  5429 net.cpp:98] Creating Layer res4a_branch2a
I0630 19:26:28.734728  5429 net.cpp:439] res4a_branch2a <- pool3
I0630 19:26:28.734731  5429 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0630 19:26:28.741770  5429 net.cpp:148] Setting up res4a_branch2a
I0630 19:26:28.741781  5429 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0630 19:26:28.741783  5429 net.cpp:163] Memory required for data: 671744000
I0630 19:26:28.741787  5429 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0630 19:26:28.741791  5429 net.cpp:98] Creating Layer res4a_branch2a/bn
I0630 19:26:28.741793  5429 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0630 19:26:28.741799  5429 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0630 19:26:28.742507  5429 net.cpp:148] Setting up res4a_branch2a/bn
I0630 19:26:28.742513  5429 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0630 19:26:28.742516  5429 net.cpp:163] Memory required for data: 678297600
I0630 19:26:28.742521  5429 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0630 19:26:28.742523  5429 net.cpp:98] Creating Layer res4a_branch2a/relu
I0630 19:26:28.742525  5429 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0630 19:26:28.742527  5429 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0630 19:26:28.742530  5429 net.cpp:148] Setting up res4a_branch2a/relu
I0630 19:26:28.742533  5429 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0630 19:26:28.742535  5429 net.cpp:163] Memory required for data: 684851200
I0630 19:26:28.742537  5429 layer_factory.hpp:77] Creating layer res4a_branch2b
I0630 19:26:28.742540  5429 net.cpp:98] Creating Layer res4a_branch2b
I0630 19:26:28.742542  5429 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0630 19:26:28.742545  5429 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0630 19:26:28.745743  5429 net.cpp:148] Setting up res4a_branch2b
I0630 19:26:28.745748  5429 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0630 19:26:28.745749  5429 net.cpp:163] Memory required for data: 691404800
I0630 19:26:28.745753  5429 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0630 19:26:28.745756  5429 net.cpp:98] Creating Layer res4a_branch2b/bn
I0630 19:26:28.745759  5429 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0630 19:26:28.745760  5429 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0630 19:26:28.746451  5429 net.cpp:148] Setting up res4a_branch2b/bn
I0630 19:26:28.746457  5429 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0630 19:26:28.746459  5429 net.cpp:163] Memory required for data: 697958400
I0630 19:26:28.746464  5429 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0630 19:26:28.746466  5429 net.cpp:98] Creating Layer res4a_branch2b/relu
I0630 19:26:28.746469  5429 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0630 19:26:28.746470  5429 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0630 19:26:28.746474  5429 net.cpp:148] Setting up res4a_branch2b/relu
I0630 19:26:28.746476  5429 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0630 19:26:28.746479  5429 net.cpp:163] Memory required for data: 704512000
I0630 19:26:28.746479  5429 layer_factory.hpp:77] Creating layer pool4
I0630 19:26:28.746482  5429 net.cpp:98] Creating Layer pool4
I0630 19:26:28.746484  5429 net.cpp:439] pool4 <- res4a_branch2b/bn
I0630 19:26:28.746486  5429 net.cpp:413] pool4 -> pool4
I0630 19:26:28.746525  5429 net.cpp:148] Setting up pool4
I0630 19:26:28.746528  5429 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0630 19:26:28.746537  5429 net.cpp:163] Memory required for data: 711065600
I0630 19:26:28.746539  5429 layer_factory.hpp:77] Creating layer res5a_branch2a
I0630 19:26:28.746544  5429 net.cpp:98] Creating Layer res5a_branch2a
I0630 19:26:28.746546  5429 net.cpp:439] res5a_branch2a <- pool4
I0630 19:26:28.746549  5429 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0630 19:26:28.771392  5429 net.cpp:148] Setting up res5a_branch2a
I0630 19:26:28.771411  5429 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0630 19:26:28.771414  5429 net.cpp:163] Memory required for data: 724172800
I0630 19:26:28.771420  5429 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0630 19:26:28.771426  5429 net.cpp:98] Creating Layer res5a_branch2a/bn
I0630 19:26:28.771430  5429 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0630 19:26:28.771435  5429 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0630 19:26:28.772148  5429 net.cpp:148] Setting up res5a_branch2a/bn
I0630 19:26:28.772155  5429 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0630 19:26:28.772156  5429 net.cpp:163] Memory required for data: 737280000
I0630 19:26:28.772162  5429 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0630 19:26:28.772166  5429 net.cpp:98] Creating Layer res5a_branch2a/relu
I0630 19:26:28.772167  5429 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0630 19:26:28.772171  5429 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0630 19:26:28.772173  5429 net.cpp:148] Setting up res5a_branch2a/relu
I0630 19:26:28.772176  5429 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0630 19:26:28.772177  5429 net.cpp:163] Memory required for data: 750387200
I0630 19:26:28.772179  5429 layer_factory.hpp:77] Creating layer res5a_branch2b
I0630 19:26:28.772184  5429 net.cpp:98] Creating Layer res5a_branch2b
I0630 19:26:28.772186  5429 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0630 19:26:28.772188  5429 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0630 19:26:28.785019  5429 net.cpp:148] Setting up res5a_branch2b
I0630 19:26:28.785028  5429 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0630 19:26:28.785030  5429 net.cpp:163] Memory required for data: 763494400
I0630 19:26:28.785037  5429 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0630 19:26:28.785042  5429 net.cpp:98] Creating Layer res5a_branch2b/bn
I0630 19:26:28.785043  5429 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0630 19:26:28.785046  5429 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0630 19:26:28.785739  5429 net.cpp:148] Setting up res5a_branch2b/bn
I0630 19:26:28.785744  5429 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0630 19:26:28.785747  5429 net.cpp:163] Memory required for data: 776601600
I0630 19:26:28.785751  5429 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0630 19:26:28.785754  5429 net.cpp:98] Creating Layer res5a_branch2b/relu
I0630 19:26:28.785756  5429 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0630 19:26:28.785759  5429 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0630 19:26:28.785763  5429 net.cpp:148] Setting up res5a_branch2b/relu
I0630 19:26:28.785764  5429 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0630 19:26:28.785766  5429 net.cpp:163] Memory required for data: 789708800
I0630 19:26:28.785768  5429 layer_factory.hpp:77] Creating layer out5a
I0630 19:26:28.785771  5429 net.cpp:98] Creating Layer out5a
I0630 19:26:28.785773  5429 net.cpp:439] out5a <- res5a_branch2b/bn
I0630 19:26:28.785776  5429 net.cpp:413] out5a -> out5a
I0630 19:26:28.789860  5429 net.cpp:148] Setting up out5a
I0630 19:26:28.789870  5429 net.cpp:155] Top shape: 4 64 40 40 (409600)
I0630 19:26:28.789872  5429 net.cpp:163] Memory required for data: 791347200
I0630 19:26:28.789876  5429 layer_factory.hpp:77] Creating layer out5a/bn
I0630 19:26:28.789882  5429 net.cpp:98] Creating Layer out5a/bn
I0630 19:26:28.789885  5429 net.cpp:439] out5a/bn <- out5a
I0630 19:26:28.789887  5429 net.cpp:413] out5a/bn -> out5a/bn
I0630 19:26:28.790657  5429 net.cpp:148] Setting up out5a/bn
I0630 19:26:28.790663  5429 net.cpp:155] Top shape: 4 64 40 40 (409600)
I0630 19:26:28.790673  5429 net.cpp:163] Memory required for data: 792985600
I0630 19:26:28.790679  5429 layer_factory.hpp:77] Creating layer out5a/relu
I0630 19:26:28.790683  5429 net.cpp:98] Creating Layer out5a/relu
I0630 19:26:28.790684  5429 net.cpp:439] out5a/relu <- out5a/bn
I0630 19:26:28.790686  5429 net.cpp:400] out5a/relu -> out5a/bn (in-place)
I0630 19:26:28.790690  5429 net.cpp:148] Setting up out5a/relu
I0630 19:26:28.790693  5429 net.cpp:155] Top shape: 4 64 40 40 (409600)
I0630 19:26:28.790694  5429 net.cpp:163] Memory required for data: 794624000
I0630 19:26:28.790696  5429 layer_factory.hpp:77] Creating layer out5a_up2
I0630 19:26:28.790700  5429 net.cpp:98] Creating Layer out5a_up2
I0630 19:26:28.790701  5429 net.cpp:439] out5a_up2 <- out5a/bn
I0630 19:26:28.790705  5429 net.cpp:413] out5a_up2 -> out5a_up2
I0630 19:26:28.790971  5429 net.cpp:148] Setting up out5a_up2
I0630 19:26:28.790977  5429 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 19:26:28.790978  5429 net.cpp:163] Memory required for data: 801177600
I0630 19:26:28.790980  5429 layer_factory.hpp:77] Creating layer out3a
I0630 19:26:28.790984  5429 net.cpp:98] Creating Layer out3a
I0630 19:26:28.790987  5429 net.cpp:439] out3a <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0630 19:26:28.790990  5429 net.cpp:413] out3a -> out3a
I0630 19:26:28.793148  5429 net.cpp:148] Setting up out3a
I0630 19:26:28.793165  5429 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 19:26:28.793167  5429 net.cpp:163] Memory required for data: 807731200
I0630 19:26:28.793174  5429 layer_factory.hpp:77] Creating layer out3a/bn
I0630 19:26:28.793180  5429 net.cpp:98] Creating Layer out3a/bn
I0630 19:26:28.793185  5429 net.cpp:439] out3a/bn <- out3a
I0630 19:26:28.793190  5429 net.cpp:413] out3a/bn -> out3a/bn
I0630 19:26:28.793987  5429 net.cpp:148] Setting up out3a/bn
I0630 19:26:28.793992  5429 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 19:26:28.793994  5429 net.cpp:163] Memory required for data: 814284800
I0630 19:26:28.793999  5429 layer_factory.hpp:77] Creating layer out3a/relu
I0630 19:26:28.794003  5429 net.cpp:98] Creating Layer out3a/relu
I0630 19:26:28.794005  5429 net.cpp:439] out3a/relu <- out3a/bn
I0630 19:26:28.794008  5429 net.cpp:400] out3a/relu -> out3a/bn (in-place)
I0630 19:26:28.794010  5429 net.cpp:148] Setting up out3a/relu
I0630 19:26:28.794013  5429 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 19:26:28.794014  5429 net.cpp:163] Memory required for data: 820838400
I0630 19:26:28.794016  5429 layer_factory.hpp:77] Creating layer out3_out5_combined
I0630 19:26:28.794019  5429 net.cpp:98] Creating Layer out3_out5_combined
I0630 19:26:28.794021  5429 net.cpp:439] out3_out5_combined <- out5a_up2
I0630 19:26:28.794023  5429 net.cpp:439] out3_out5_combined <- out3a/bn
I0630 19:26:28.794026  5429 net.cpp:413] out3_out5_combined -> out3_out5_combined
I0630 19:26:28.794049  5429 net.cpp:148] Setting up out3_out5_combined
I0630 19:26:28.794052  5429 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 19:26:28.794054  5429 net.cpp:163] Memory required for data: 827392000
I0630 19:26:28.794056  5429 layer_factory.hpp:77] Creating layer ctx_conv1
I0630 19:26:28.794061  5429 net.cpp:98] Creating Layer ctx_conv1
I0630 19:26:28.794064  5429 net.cpp:439] ctx_conv1 <- out3_out5_combined
I0630 19:26:28.794066  5429 net.cpp:413] ctx_conv1 -> ctx_conv1
I0630 19:26:28.795126  5429 net.cpp:148] Setting up ctx_conv1
I0630 19:26:28.795132  5429 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 19:26:28.795135  5429 net.cpp:163] Memory required for data: 833945600
I0630 19:26:28.795137  5429 layer_factory.hpp:77] Creating layer ctx_conv1/bn
I0630 19:26:28.795140  5429 net.cpp:98] Creating Layer ctx_conv1/bn
I0630 19:26:28.795142  5429 net.cpp:439] ctx_conv1/bn <- ctx_conv1
I0630 19:26:28.795145  5429 net.cpp:413] ctx_conv1/bn -> ctx_conv1/bn
I0630 19:26:28.795907  5429 net.cpp:148] Setting up ctx_conv1/bn
I0630 19:26:28.795912  5429 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 19:26:28.795914  5429 net.cpp:163] Memory required for data: 840499200
I0630 19:26:28.795928  5429 layer_factory.hpp:77] Creating layer ctx_conv1/relu
I0630 19:26:28.795930  5429 net.cpp:98] Creating Layer ctx_conv1/relu
I0630 19:26:28.795933  5429 net.cpp:439] ctx_conv1/relu <- ctx_conv1/bn
I0630 19:26:28.795934  5429 net.cpp:400] ctx_conv1/relu -> ctx_conv1/bn (in-place)
I0630 19:26:28.795938  5429 net.cpp:148] Setting up ctx_conv1/relu
I0630 19:26:28.795940  5429 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 19:26:28.795941  5429 net.cpp:163] Memory required for data: 847052800
I0630 19:26:28.795943  5429 layer_factory.hpp:77] Creating layer ctx_conv2
I0630 19:26:28.795951  5429 net.cpp:98] Creating Layer ctx_conv2
I0630 19:26:28.795954  5429 net.cpp:439] ctx_conv2 <- ctx_conv1/bn
I0630 19:26:28.795958  5429 net.cpp:413] ctx_conv2 -> ctx_conv2
I0630 19:26:28.797004  5429 net.cpp:148] Setting up ctx_conv2
I0630 19:26:28.797009  5429 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 19:26:28.797010  5429 net.cpp:163] Memory required for data: 853606400
I0630 19:26:28.797014  5429 layer_factory.hpp:77] Creating layer ctx_conv2/bn
I0630 19:26:28.797018  5429 net.cpp:98] Creating Layer ctx_conv2/bn
I0630 19:26:28.797019  5429 net.cpp:439] ctx_conv2/bn <- ctx_conv2
I0630 19:26:28.797022  5429 net.cpp:413] ctx_conv2/bn -> ctx_conv2/bn
I0630 19:26:28.797790  5429 net.cpp:148] Setting up ctx_conv2/bn
I0630 19:26:28.797794  5429 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 19:26:28.797796  5429 net.cpp:163] Memory required for data: 860160000
I0630 19:26:28.797801  5429 layer_factory.hpp:77] Creating layer ctx_conv2/relu
I0630 19:26:28.797803  5429 net.cpp:98] Creating Layer ctx_conv2/relu
I0630 19:26:28.797806  5429 net.cpp:439] ctx_conv2/relu <- ctx_conv2/bn
I0630 19:26:28.797808  5429 net.cpp:400] ctx_conv2/relu -> ctx_conv2/bn (in-place)
I0630 19:26:28.797811  5429 net.cpp:148] Setting up ctx_conv2/relu
I0630 19:26:28.797813  5429 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 19:26:28.797816  5429 net.cpp:163] Memory required for data: 866713600
I0630 19:26:28.797816  5429 layer_factory.hpp:77] Creating layer ctx_conv3
I0630 19:26:28.797821  5429 net.cpp:98] Creating Layer ctx_conv3
I0630 19:26:28.797823  5429 net.cpp:439] ctx_conv3 <- ctx_conv2/bn
I0630 19:26:28.797825  5429 net.cpp:413] ctx_conv3 -> ctx_conv3
I0630 19:26:28.798889  5429 net.cpp:148] Setting up ctx_conv3
I0630 19:26:28.798895  5429 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 19:26:28.798897  5429 net.cpp:163] Memory required for data: 873267200
I0630 19:26:28.798900  5429 layer_factory.hpp:77] Creating layer ctx_conv3/bn
I0630 19:26:28.798903  5429 net.cpp:98] Creating Layer ctx_conv3/bn
I0630 19:26:28.798905  5429 net.cpp:439] ctx_conv3/bn <- ctx_conv3
I0630 19:26:28.798908  5429 net.cpp:413] ctx_conv3/bn -> ctx_conv3/bn
I0630 19:26:28.799682  5429 net.cpp:148] Setting up ctx_conv3/bn
I0630 19:26:28.799687  5429 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 19:26:28.799690  5429 net.cpp:163] Memory required for data: 879820800
I0630 19:26:28.799695  5429 layer_factory.hpp:77] Creating layer ctx_conv3/relu
I0630 19:26:28.799698  5429 net.cpp:98] Creating Layer ctx_conv3/relu
I0630 19:26:28.799700  5429 net.cpp:439] ctx_conv3/relu <- ctx_conv3/bn
I0630 19:26:28.799703  5429 net.cpp:400] ctx_conv3/relu -> ctx_conv3/bn (in-place)
I0630 19:26:28.799706  5429 net.cpp:148] Setting up ctx_conv3/relu
I0630 19:26:28.799708  5429 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 19:26:28.799710  5429 net.cpp:163] Memory required for data: 886374400
I0630 19:26:28.799712  5429 layer_factory.hpp:77] Creating layer ctx_conv4
I0630 19:26:28.799715  5429 net.cpp:98] Creating Layer ctx_conv4
I0630 19:26:28.799717  5429 net.cpp:439] ctx_conv4 <- ctx_conv3/bn
I0630 19:26:28.799721  5429 net.cpp:413] ctx_conv4 -> ctx_conv4
I0630 19:26:28.800766  5429 net.cpp:148] Setting up ctx_conv4
I0630 19:26:28.800771  5429 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 19:26:28.800773  5429 net.cpp:163] Memory required for data: 892928000
I0630 19:26:28.800776  5429 layer_factory.hpp:77] Creating layer ctx_conv4/bn
I0630 19:26:28.800786  5429 net.cpp:98] Creating Layer ctx_conv4/bn
I0630 19:26:28.800788  5429 net.cpp:439] ctx_conv4/bn <- ctx_conv4
I0630 19:26:28.800791  5429 net.cpp:413] ctx_conv4/bn -> ctx_conv4/bn
I0630 19:26:28.801555  5429 net.cpp:148] Setting up ctx_conv4/bn
I0630 19:26:28.801560  5429 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 19:26:28.801563  5429 net.cpp:163] Memory required for data: 899481600
I0630 19:26:28.801568  5429 layer_factory.hpp:77] Creating layer ctx_conv4/relu
I0630 19:26:28.801569  5429 net.cpp:98] Creating Layer ctx_conv4/relu
I0630 19:26:28.801571  5429 net.cpp:439] ctx_conv4/relu <- ctx_conv4/bn
I0630 19:26:28.801574  5429 net.cpp:400] ctx_conv4/relu -> ctx_conv4/bn (in-place)
I0630 19:26:28.801578  5429 net.cpp:148] Setting up ctx_conv4/relu
I0630 19:26:28.801579  5429 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 19:26:28.801581  5429 net.cpp:163] Memory required for data: 906035200
I0630 19:26:28.801582  5429 layer_factory.hpp:77] Creating layer ctx_final
I0630 19:26:28.801586  5429 net.cpp:98] Creating Layer ctx_final
I0630 19:26:28.801589  5429 net.cpp:439] ctx_final <- ctx_conv4/bn
I0630 19:26:28.801590  5429 net.cpp:413] ctx_final -> ctx_final
I0630 19:26:28.802148  5429 net.cpp:148] Setting up ctx_final
I0630 19:26:28.802153  5429 net.cpp:155] Top shape: 4 20 80 80 (512000)
I0630 19:26:28.802155  5429 net.cpp:163] Memory required for data: 908083200
I0630 19:26:28.802158  5429 layer_factory.hpp:77] Creating layer ctx_final/relu
I0630 19:26:28.802161  5429 net.cpp:98] Creating Layer ctx_final/relu
I0630 19:26:28.802163  5429 net.cpp:439] ctx_final/relu <- ctx_final
I0630 19:26:28.802165  5429 net.cpp:400] ctx_final/relu -> ctx_final (in-place)
I0630 19:26:28.802168  5429 net.cpp:148] Setting up ctx_final/relu
I0630 19:26:28.802170  5429 net.cpp:155] Top shape: 4 20 80 80 (512000)
I0630 19:26:28.802172  5429 net.cpp:163] Memory required for data: 910131200
I0630 19:26:28.802173  5429 layer_factory.hpp:77] Creating layer out_deconv_final_up2
I0630 19:26:28.802177  5429 net.cpp:98] Creating Layer out_deconv_final_up2
I0630 19:26:28.802178  5429 net.cpp:439] out_deconv_final_up2 <- ctx_final
I0630 19:26:28.802181  5429 net.cpp:413] out_deconv_final_up2 -> out_deconv_final_up2
I0630 19:26:28.802443  5429 net.cpp:148] Setting up out_deconv_final_up2
I0630 19:26:28.802448  5429 net.cpp:155] Top shape: 4 20 160 160 (2048000)
I0630 19:26:28.802449  5429 net.cpp:163] Memory required for data: 918323200
I0630 19:26:28.802451  5429 layer_factory.hpp:77] Creating layer out_deconv_final_up4
I0630 19:26:28.802455  5429 net.cpp:98] Creating Layer out_deconv_final_up4
I0630 19:26:28.802458  5429 net.cpp:439] out_deconv_final_up4 <- out_deconv_final_up2
I0630 19:26:28.802459  5429 net.cpp:413] out_deconv_final_up4 -> out_deconv_final_up4
I0630 19:26:28.802714  5429 net.cpp:148] Setting up out_deconv_final_up4
I0630 19:26:28.802718  5429 net.cpp:155] Top shape: 4 20 320 320 (8192000)
I0630 19:26:28.802721  5429 net.cpp:163] Memory required for data: 951091200
I0630 19:26:28.802723  5429 layer_factory.hpp:77] Creating layer out_deconv_final_up8
I0630 19:26:28.802728  5429 net.cpp:98] Creating Layer out_deconv_final_up8
I0630 19:26:28.802731  5429 net.cpp:439] out_deconv_final_up8 <- out_deconv_final_up4
I0630 19:26:28.802733  5429 net.cpp:413] out_deconv_final_up8 -> out_deconv_final_up8
I0630 19:26:28.802985  5429 net.cpp:148] Setting up out_deconv_final_up8
I0630 19:26:28.802989  5429 net.cpp:155] Top shape: 4 20 640 640 (32768000)
I0630 19:26:28.802991  5429 net.cpp:163] Memory required for data: 1082163200
I0630 19:26:28.802994  5429 layer_factory.hpp:77] Creating layer out_deconv_final_up8_out_deconv_final_up8_0_split
I0630 19:26:28.802997  5429 net.cpp:98] Creating Layer out_deconv_final_up8_out_deconv_final_up8_0_split
I0630 19:26:28.802999  5429 net.cpp:439] out_deconv_final_up8_out_deconv_final_up8_0_split <- out_deconv_final_up8
I0630 19:26:28.803001  5429 net.cpp:413] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0630 19:26:28.803010  5429 net.cpp:413] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0630 19:26:28.803014  5429 net.cpp:413] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0630 19:26:28.803068  5429 net.cpp:148] Setting up out_deconv_final_up8_out_deconv_final_up8_0_split
I0630 19:26:28.803072  5429 net.cpp:155] Top shape: 4 20 640 640 (32768000)
I0630 19:26:28.803074  5429 net.cpp:155] Top shape: 4 20 640 640 (32768000)
I0630 19:26:28.803077  5429 net.cpp:155] Top shape: 4 20 640 640 (32768000)
I0630 19:26:28.803078  5429 net.cpp:163] Memory required for data: 1475379200
I0630 19:26:28.803081  5429 layer_factory.hpp:77] Creating layer loss
I0630 19:26:28.803086  5429 net.cpp:98] Creating Layer loss
I0630 19:26:28.803087  5429 net.cpp:439] loss <- out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0630 19:26:28.803089  5429 net.cpp:439] loss <- label_data_1_split_0
I0630 19:26:28.803092  5429 net.cpp:413] loss -> loss
I0630 19:26:28.803097  5429 layer_factory.hpp:77] Creating layer loss
I0630 19:26:28.843915  5429 net.cpp:148] Setting up loss
I0630 19:26:28.843938  5429 net.cpp:155] Top shape: (1)
I0630 19:26:28.843940  5429 net.cpp:158]     with loss weight 1
I0630 19:26:28.843947  5429 net.cpp:163] Memory required for data: 1475379204
I0630 19:26:28.843951  5429 layer_factory.hpp:77] Creating layer accuracy/top1
I0630 19:26:28.843963  5429 net.cpp:98] Creating Layer accuracy/top1
I0630 19:26:28.843967  5429 net.cpp:439] accuracy/top1 <- out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0630 19:26:28.843971  5429 net.cpp:439] accuracy/top1 <- label_data_1_split_1
I0630 19:26:28.843974  5429 net.cpp:413] accuracy/top1 -> accuracy/top1
I0630 19:26:28.843982  5429 net.cpp:148] Setting up accuracy/top1
I0630 19:26:28.843986  5429 net.cpp:155] Top shape: (1)
I0630 19:26:28.843987  5429 net.cpp:163] Memory required for data: 1475379208
I0630 19:26:28.843989  5429 layer_factory.hpp:77] Creating layer accuracy/top5
I0630 19:26:28.843993  5429 net.cpp:98] Creating Layer accuracy/top5
I0630 19:26:28.843996  5429 net.cpp:439] accuracy/top5 <- out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0630 19:26:28.843997  5429 net.cpp:439] accuracy/top5 <- label_data_1_split_2
I0630 19:26:28.844000  5429 net.cpp:413] accuracy/top5 -> accuracy/top5
I0630 19:26:28.844003  5429 net.cpp:148] Setting up accuracy/top5
I0630 19:26:28.844007  5429 net.cpp:155] Top shape: (1)
I0630 19:26:28.844008  5429 net.cpp:163] Memory required for data: 1475379212
I0630 19:26:28.844010  5429 net.cpp:226] accuracy/top5 does not need backward computation.
I0630 19:26:28.844013  5429 net.cpp:226] accuracy/top1 does not need backward computation.
I0630 19:26:28.844015  5429 net.cpp:224] loss needs backward computation.
I0630 19:26:28.844017  5429 net.cpp:224] out_deconv_final_up8_out_deconv_final_up8_0_split needs backward computation.
I0630 19:26:28.844020  5429 net.cpp:224] out_deconv_final_up8 needs backward computation.
I0630 19:26:28.844022  5429 net.cpp:224] out_deconv_final_up4 needs backward computation.
I0630 19:26:28.844025  5429 net.cpp:224] out_deconv_final_up2 needs backward computation.
I0630 19:26:28.844027  5429 net.cpp:224] ctx_final/relu needs backward computation.
I0630 19:26:28.844030  5429 net.cpp:224] ctx_final needs backward computation.
I0630 19:26:28.844033  5429 net.cpp:224] ctx_conv4/relu needs backward computation.
I0630 19:26:28.844034  5429 net.cpp:224] ctx_conv4/bn needs backward computation.
I0630 19:26:28.844036  5429 net.cpp:224] ctx_conv4 needs backward computation.
I0630 19:26:28.844039  5429 net.cpp:224] ctx_conv3/relu needs backward computation.
I0630 19:26:28.844043  5429 net.cpp:224] ctx_conv3/bn needs backward computation.
I0630 19:26:28.844044  5429 net.cpp:224] ctx_conv3 needs backward computation.
I0630 19:26:28.844046  5429 net.cpp:224] ctx_conv2/relu needs backward computation.
I0630 19:26:28.844048  5429 net.cpp:224] ctx_conv2/bn needs backward computation.
I0630 19:26:28.844059  5429 net.cpp:224] ctx_conv2 needs backward computation.
I0630 19:26:28.844063  5429 net.cpp:224] ctx_conv1/relu needs backward computation.
I0630 19:26:28.844063  5429 net.cpp:224] ctx_conv1/bn needs backward computation.
I0630 19:26:28.844066  5429 net.cpp:224] ctx_conv1 needs backward computation.
I0630 19:26:28.844069  5429 net.cpp:224] out3_out5_combined needs backward computation.
I0630 19:26:28.844072  5429 net.cpp:224] out3a/relu needs backward computation.
I0630 19:26:28.844074  5429 net.cpp:224] out3a/bn needs backward computation.
I0630 19:26:28.844076  5429 net.cpp:224] out3a needs backward computation.
I0630 19:26:28.844079  5429 net.cpp:224] out5a_up2 needs backward computation.
I0630 19:26:28.844082  5429 net.cpp:224] out5a/relu needs backward computation.
I0630 19:26:28.844084  5429 net.cpp:224] out5a/bn needs backward computation.
I0630 19:26:28.844087  5429 net.cpp:224] out5a needs backward computation.
I0630 19:26:28.844090  5429 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0630 19:26:28.844092  5429 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0630 19:26:28.844094  5429 net.cpp:224] res5a_branch2b needs backward computation.
I0630 19:26:28.844097  5429 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0630 19:26:28.844100  5429 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0630 19:26:28.844102  5429 net.cpp:224] res5a_branch2a needs backward computation.
I0630 19:26:28.844105  5429 net.cpp:224] pool4 needs backward computation.
I0630 19:26:28.844107  5429 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0630 19:26:28.844110  5429 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0630 19:26:28.844111  5429 net.cpp:224] res4a_branch2b needs backward computation.
I0630 19:26:28.844115  5429 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0630 19:26:28.844116  5429 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0630 19:26:28.844120  5429 net.cpp:224] res4a_branch2a needs backward computation.
I0630 19:26:28.844121  5429 net.cpp:224] pool3 needs backward computation.
I0630 19:26:28.844125  5429 net.cpp:224] res3a_branch2b/bn_res3a_branch2b/relu_0_split needs backward computation.
I0630 19:26:28.844127  5429 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0630 19:26:28.844130  5429 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0630 19:26:28.844131  5429 net.cpp:224] res3a_branch2b needs backward computation.
I0630 19:26:28.844135  5429 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0630 19:26:28.844136  5429 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0630 19:26:28.844139  5429 net.cpp:224] res3a_branch2a needs backward computation.
I0630 19:26:28.844141  5429 net.cpp:224] pool2 needs backward computation.
I0630 19:26:28.844144  5429 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0630 19:26:28.844147  5429 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0630 19:26:28.844149  5429 net.cpp:224] res2a_branch2b needs backward computation.
I0630 19:26:28.844152  5429 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0630 19:26:28.844156  5429 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0630 19:26:28.844157  5429 net.cpp:224] res2a_branch2a needs backward computation.
I0630 19:26:28.844159  5429 net.cpp:224] pool1 needs backward computation.
I0630 19:26:28.844162  5429 net.cpp:224] conv1b/relu needs backward computation.
I0630 19:26:28.844166  5429 net.cpp:224] conv1b/bn needs backward computation.
I0630 19:26:28.844167  5429 net.cpp:224] conv1b needs backward computation.
I0630 19:26:28.844171  5429 net.cpp:224] conv1a/relu needs backward computation.
I0630 19:26:28.844172  5429 net.cpp:224] conv1a/bn needs backward computation.
I0630 19:26:28.844175  5429 net.cpp:224] conv1a needs backward computation.
I0630 19:26:28.844177  5429 net.cpp:226] data/bias does not need backward computation.
I0630 19:26:28.844180  5429 net.cpp:226] label_data_1_split does not need backward computation.
I0630 19:26:28.844187  5429 net.cpp:226] data does not need backward computation.
I0630 19:26:28.844188  5429 net.cpp:268] This network produces output accuracy/top1
I0630 19:26:28.844190  5429 net.cpp:268] This network produces output accuracy/top5
I0630 19:26:28.844193  5429 net.cpp:268] This network produces output loss
I0630 19:26:28.844224  5429 net.cpp:288] Network initialization done.
I0630 19:26:28.844316  5429 solver.cpp:60] Solver scaffolding done.
I0630 19:26:28.852279  5429 caffe.cpp:145] Finetuning from training/imagenet_jacintonet11_v2_bn_iter_160000.caffemodel
I0630 19:26:33.119323  5429 net.cpp:804] Ignoring source layer input
I0630 19:26:33.123808  5429 net.cpp:804] Ignoring source layer pool5
I0630 19:26:33.123836  5429 net.cpp:804] Ignoring source layer fc1000
I0630 19:26:33.123847  5429 net.cpp:804] Ignoring source layer fc1000_fc1000_0_split
I0630 19:26:33.123868  5429 net.cpp:804] Ignoring source layer prob
I0630 19:26:33.123870  5429 net.cpp:804] Ignoring source layer argMaxOut
I0630 19:26:33.132794  5429 net.cpp:804] Ignoring source layer input
I0630 19:26:33.133968  5429 net.cpp:804] Ignoring source layer pool5
I0630 19:26:33.133975  5429 net.cpp:804] Ignoring source layer fc1000
I0630 19:26:33.133976  5429 net.cpp:804] Ignoring source layer fc1000_fc1000_0_split
I0630 19:26:33.133978  5429 net.cpp:804] Ignoring source layer prob
I0630 19:26:33.133980  5429 net.cpp:804] Ignoring source layer argMaxOut
I0630 19:26:33.830165  5429 data_layer.cpp:78] ReshapePrefetch 5, 3, 640, 640
I0630 19:26:33.830231  5429 data_layer.cpp:83] output data size: 5,3,640,640
I0630 19:26:33.868137  5429 data_layer.cpp:78] ReshapePrefetch 5, 1, 640, 640
I0630 19:26:33.870117  5429 data_layer.cpp:83] output data size: 5,1,640,640
I0630 19:26:35.364768  5429 data_layer.cpp:78] ReshapePrefetch 5, 3, 640, 640
I0630 19:26:35.364843  5429 data_layer.cpp:83] output data size: 5,3,640,640
I0630 19:26:35.402186  5429 data_layer.cpp:78] ReshapePrefetch 5, 1, 640, 640
I0630 19:26:35.410117  5429 data_layer.cpp:83] output data size: 5,1,640,640
I0630 19:26:35.960364  5429 parallel.cpp:334] Starting Optimization
I0630 19:26:35.960417  5429 solver.cpp:413] Solving jsegnet21v2_train
I0630 19:26:35.960423  5429 solver.cpp:414] Learning Rate Policy: multistep
I0630 19:26:36.010901  5713 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 19:26:41.192584  5429 solver.cpp:290] Iteration 0 (0 iter/s, 5.22933s/100 iter), loss = 2.74647
I0630 19:26:41.192607  5429 solver.cpp:309]     Train net output #0: loss = 2.74647 (* 1 = 2.74647 loss)
I0630 19:26:41.192616  5429 sgd_solver.cpp:106] Iteration 0, lr = 0.0001
I0630 19:31:25.981591  5660 blocking_queue.cpp:50] Waiting for data
I0630 19:32:45.899688  5429 solver.cpp:290] Iteration 100 (0.274206 iter/s, 364.689s/100 iter), loss = 0.687969
I0630 19:32:45.906404  5429 solver.cpp:309]     Train net output #0: loss = 0.687969 (* 1 = 0.687969 loss)
I0630 19:32:45.906415  5429 sgd_solver.cpp:106] Iteration 100, lr = 0.0001
I0630 19:35:09.554340  5694 blocking_queue.cpp:50] Waiting for data
I0630 19:36:18.150493  5429 solver.cpp:290] Iteration 200 (0.471144 iter/s, 212.249s/100 iter), loss = 0.855586
I0630 19:36:18.150537  5429 solver.cpp:309]     Train net output #0: loss = 0.855586 (* 1 = 0.855586 loss)
I0630 19:36:18.150545  5429 sgd_solver.cpp:106] Iteration 200, lr = 0.0001
I0630 19:36:36.818286  5429 solver.cpp:290] Iteration 300 (5.35681 iter/s, 18.6678s/100 iter), loss = 0.443862
I0630 19:36:36.818310  5429 solver.cpp:309]     Train net output #0: loss = 0.443862 (* 1 = 0.443862 loss)
I0630 19:36:36.818317  5429 sgd_solver.cpp:106] Iteration 300, lr = 0.0001
I0630 19:36:56.207587  5429 solver.cpp:290] Iteration 400 (5.15748 iter/s, 19.3893s/100 iter), loss = 0.355478
I0630 19:36:56.207641  5429 solver.cpp:309]     Train net output #0: loss = 0.355478 (* 1 = 0.355478 loss)
I0630 19:36:56.207651  5429 sgd_solver.cpp:106] Iteration 400, lr = 0.0001
I0630 19:37:15.715085  5429 solver.cpp:290] Iteration 500 (5.12625 iter/s, 19.5075s/100 iter), loss = 0.337974
I0630 19:37:15.715107  5429 solver.cpp:309]     Train net output #0: loss = 0.337974 (* 1 = 0.337974 loss)
I0630 19:37:15.715114  5429 sgd_solver.cpp:106] Iteration 500, lr = 0.0001
I0630 19:37:35.235857  5429 solver.cpp:290] Iteration 600 (5.12276 iter/s, 19.5207s/100 iter), loss = 0.153137
I0630 19:37:35.235939  5429 solver.cpp:309]     Train net output #0: loss = 0.153138 (* 1 = 0.153138 loss)
I0630 19:37:35.235949  5429 sgd_solver.cpp:106] Iteration 600, lr = 0.0001
I0630 19:37:54.481595  5429 solver.cpp:290] Iteration 700 (5.19599 iter/s, 19.2456s/100 iter), loss = 0.25378
I0630 19:37:54.481619  5429 solver.cpp:309]     Train net output #0: loss = 0.25378 (* 1 = 0.25378 loss)
I0630 19:37:54.481626  5429 sgd_solver.cpp:106] Iteration 700, lr = 0.0001
I0630 19:38:13.949225  5429 solver.cpp:290] Iteration 800 (5.13679 iter/s, 19.4674s/100 iter), loss = 0.713979
I0630 19:38:13.949306  5429 solver.cpp:309]     Train net output #0: loss = 0.713979 (* 1 = 0.713979 loss)
I0630 19:38:13.949313  5429 sgd_solver.cpp:106] Iteration 800, lr = 0.0001
I0630 19:38:33.364128  5429 solver.cpp:290] Iteration 900 (5.15083 iter/s, 19.4143s/100 iter), loss = 0.194988
I0630 19:38:33.364151  5429 solver.cpp:309]     Train net output #0: loss = 0.194988 (* 1 = 0.194988 loss)
I0630 19:38:33.364158  5429 sgd_solver.cpp:106] Iteration 900, lr = 0.0001
I0630 19:38:52.966876  5429 solver.cpp:290] Iteration 1000 (5.10146 iter/s, 19.6022s/100 iter), loss = 0.457464
I0630 19:38:52.968431  5429 solver.cpp:309]     Train net output #0: loss = 0.457464 (* 1 = 0.457464 loss)
I0630 19:38:52.968444  5429 sgd_solver.cpp:106] Iteration 1000, lr = 0.0001
I0630 19:39:12.345299  5429 solver.cpp:290] Iteration 1100 (5.16092 iter/s, 19.3764s/100 iter), loss = 0.325143
I0630 19:39:12.345329  5429 solver.cpp:309]     Train net output #0: loss = 0.325143 (* 1 = 0.325143 loss)
I0630 19:39:12.345337  5429 sgd_solver.cpp:106] Iteration 1100, lr = 0.0001
I0630 19:39:31.896270  5429 solver.cpp:290] Iteration 1200 (5.11497 iter/s, 19.5505s/100 iter), loss = 0.174273
I0630 19:39:31.896314  5429 solver.cpp:309]     Train net output #0: loss = 0.174273 (* 1 = 0.174273 loss)
I0630 19:39:31.896325  5429 sgd_solver.cpp:106] Iteration 1200, lr = 0.0001
I0630 19:39:51.467409  5429 solver.cpp:290] Iteration 1300 (5.1097 iter/s, 19.5706s/100 iter), loss = 0.209306
I0630 19:39:51.467437  5429 solver.cpp:309]     Train net output #0: loss = 0.209306 (* 1 = 0.209306 loss)
I0630 19:39:51.467447  5429 sgd_solver.cpp:106] Iteration 1300, lr = 0.0001
I0630 19:40:11.054872  5429 solver.cpp:290] Iteration 1400 (5.10544 iter/s, 19.587s/100 iter), loss = 0.700326
I0630 19:40:11.054940  5429 solver.cpp:309]     Train net output #0: loss = 0.700326 (* 1 = 0.700326 loss)
I0630 19:40:11.054947  5429 sgd_solver.cpp:106] Iteration 1400, lr = 0.0001
I0630 19:40:30.430704  5429 solver.cpp:290] Iteration 1500 (5.16121 iter/s, 19.3753s/100 iter), loss = 0.117938
I0630 19:40:30.430727  5429 solver.cpp:309]     Train net output #0: loss = 0.117938 (* 1 = 0.117938 loss)
I0630 19:40:30.430734  5429 sgd_solver.cpp:106] Iteration 1500, lr = 0.0001
I0630 19:40:49.827844  5429 solver.cpp:290] Iteration 1600 (5.15553 iter/s, 19.3966s/100 iter), loss = 0.236955
I0630 19:40:49.827898  5429 solver.cpp:309]     Train net output #0: loss = 0.236955 (* 1 = 0.236955 loss)
I0630 19:40:49.827913  5429 sgd_solver.cpp:106] Iteration 1600, lr = 0.0001
I0630 19:41:09.424984  5429 solver.cpp:290] Iteration 1700 (5.10292 iter/s, 19.5966s/100 iter), loss = 0.262355
I0630 19:41:09.425009  5429 solver.cpp:309]     Train net output #0: loss = 0.262355 (* 1 = 0.262355 loss)
I0630 19:41:09.425017  5429 sgd_solver.cpp:106] Iteration 1700, lr = 0.0001
I0630 19:41:28.597721  5429 solver.cpp:290] Iteration 1800 (5.21587 iter/s, 19.1723s/100 iter), loss = 0.250876
I0630 19:41:28.597822  5429 solver.cpp:309]     Train net output #0: loss = 0.250877 (* 1 = 0.250877 loss)
I0630 19:41:28.597832  5429 sgd_solver.cpp:106] Iteration 1800, lr = 0.0001
I0630 19:41:48.002044  5429 solver.cpp:290] Iteration 1900 (5.15364 iter/s, 19.4038s/100 iter), loss = 0.561186
I0630 19:41:48.002066  5429 solver.cpp:309]     Train net output #0: loss = 0.561186 (* 1 = 0.561186 loss)
I0630 19:41:48.002074  5429 sgd_solver.cpp:106] Iteration 1900, lr = 0.0001
I0630 19:42:07.350342  5429 solver.cpp:471] Iteration 2000, Testing net (#0)
I0630 19:43:43.168138  5429 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.901507
I0630 19:43:43.169898  5429 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.980604
I0630 19:43:43.169909  5429 solver.cpp:544]     Test net output #2: loss = 0.221404 (* 1 = 0.221404 loss)
I0630 19:43:43.394817  5429 solver.cpp:290] Iteration 2000 (0.866626 iter/s, 115.39s/100 iter), loss = 0.246151
I0630 19:43:43.394840  5429 solver.cpp:309]     Train net output #0: loss = 0.246151 (* 1 = 0.246151 loss)
I0630 19:43:43.394847  5429 sgd_solver.cpp:106] Iteration 2000, lr = 0.0001
I0630 19:44:01.495885  5429 solver.cpp:290] Iteration 2100 (5.52468 iter/s, 18.1006s/100 iter), loss = 0.120977
I0630 19:44:01.495913  5429 solver.cpp:309]     Train net output #0: loss = 0.120977 (* 1 = 0.120977 loss)
I0630 19:44:01.495921  5429 sgd_solver.cpp:106] Iteration 2100, lr = 0.0001
I0630 19:45:31.828320  5429 solver.cpp:290] Iteration 2200 (1.10705 iter/s, 90.3303s/100 iter), loss = 0.73541
I0630 19:45:31.828374  5429 solver.cpp:309]     Train net output #0: loss = 0.735411 (* 1 = 0.735411 loss)
I0630 19:45:31.828382  5429 sgd_solver.cpp:106] Iteration 2200, lr = 0.0001
I0630 19:45:54.022749  5429 solver.cpp:290] Iteration 2300 (4.50576 iter/s, 22.1938s/100 iter), loss = 0.16164
I0630 19:45:54.022773  5429 solver.cpp:309]     Train net output #0: loss = 0.16164 (* 1 = 0.16164 loss)
I0630 19:45:54.022781  5429 sgd_solver.cpp:106] Iteration 2300, lr = 0.0001
I0630 19:46:13.034775  5429 solver.cpp:290] Iteration 2400 (5.26001 iter/s, 19.0114s/100 iter), loss = 0.130325
I0630 19:46:13.034821  5429 solver.cpp:309]     Train net output #0: loss = 0.130325 (* 1 = 0.130325 loss)
I0630 19:46:13.034828  5429 sgd_solver.cpp:106] Iteration 2400, lr = 0.0001
I0630 19:46:32.064512  5429 solver.cpp:290] Iteration 2500 (5.25512 iter/s, 19.0291s/100 iter), loss = 0.175133
I0630 19:46:32.064535  5429 solver.cpp:309]     Train net output #0: loss = 0.175133 (* 1 = 0.175133 loss)
I0630 19:46:32.064543  5429 sgd_solver.cpp:106] Iteration 2500, lr = 0.0001
I0630 19:46:51.241194  5429 solver.cpp:290] Iteration 2600 (5.21484 iter/s, 19.176s/100 iter), loss = 0.224398
I0630 19:46:51.241230  5429 solver.cpp:309]     Train net output #0: loss = 0.224398 (* 1 = 0.224398 loss)
I0630 19:46:51.241238  5429 sgd_solver.cpp:106] Iteration 2600, lr = 0.0001
I0630 19:47:10.608156  5429 solver.cpp:290] Iteration 2700 (5.16361 iter/s, 19.3663s/100 iter), loss = 0.132893
I0630 19:47:10.608181  5429 solver.cpp:309]     Train net output #0: loss = 0.132893 (* 1 = 0.132893 loss)
I0630 19:47:10.608187  5429 sgd_solver.cpp:106] Iteration 2700, lr = 0.0001
I0630 19:47:29.957798  5429 solver.cpp:290] Iteration 2800 (5.16825 iter/s, 19.3489s/100 iter), loss = 0.226913
I0630 19:47:29.957876  5429 solver.cpp:309]     Train net output #0: loss = 0.226914 (* 1 = 0.226914 loss)
I0630 19:47:29.957886  5429 sgd_solver.cpp:106] Iteration 2800, lr = 0.0001
I0630 19:47:49.000182  5429 solver.cpp:290] Iteration 2900 (5.25165 iter/s, 19.0416s/100 iter), loss = 0.10714
I0630 19:47:49.000205  5429 solver.cpp:309]     Train net output #0: loss = 0.10714 (* 1 = 0.10714 loss)
I0630 19:47:49.000212  5429 sgd_solver.cpp:106] Iteration 2900, lr = 0.0001
I0630 19:48:07.972378  5429 solver.cpp:290] Iteration 3000 (5.27106 iter/s, 18.9715s/100 iter), loss = 0.16946
I0630 19:48:07.972424  5429 solver.cpp:309]     Train net output #0: loss = 0.16946 (* 1 = 0.16946 loss)
I0630 19:48:07.972432  5429 sgd_solver.cpp:106] Iteration 3000, lr = 0.0001
I0630 19:48:27.210813  5429 solver.cpp:290] Iteration 3100 (5.19812 iter/s, 19.2377s/100 iter), loss = 0.186968
I0630 19:48:27.210835  5429 solver.cpp:309]     Train net output #0: loss = 0.186968 (* 1 = 0.186968 loss)
I0630 19:48:27.210842  5429 sgd_solver.cpp:106] Iteration 3100, lr = 0.0001
I0630 19:48:46.441362  5429 solver.cpp:290] Iteration 3200 (5.20024 iter/s, 19.2299s/100 iter), loss = 0.127278
I0630 19:48:46.441453  5429 solver.cpp:309]     Train net output #0: loss = 0.127278 (* 1 = 0.127278 loss)
I0630 19:48:46.441462  5429 sgd_solver.cpp:106] Iteration 3200, lr = 0.0001
I0630 19:49:05.798693  5429 solver.cpp:290] Iteration 3300 (5.1662 iter/s, 19.3566s/100 iter), loss = 0.100765
I0630 19:49:05.798719  5429 solver.cpp:309]     Train net output #0: loss = 0.100765 (* 1 = 0.100765 loss)
I0630 19:49:05.798728  5429 sgd_solver.cpp:106] Iteration 3300, lr = 0.0001
I0630 19:49:24.896965  5429 solver.cpp:290] Iteration 3400 (5.23626 iter/s, 19.0976s/100 iter), loss = 0.35129
I0630 19:49:24.897014  5429 solver.cpp:309]     Train net output #0: loss = 0.35129 (* 1 = 0.35129 loss)
I0630 19:49:24.897022  5429 sgd_solver.cpp:106] Iteration 3400, lr = 0.0001
I0630 19:49:44.055533  5429 solver.cpp:290] Iteration 3500 (5.21978 iter/s, 19.1579s/100 iter), loss = 0.192884
I0630 19:49:44.055558  5429 solver.cpp:309]     Train net output #0: loss = 0.192884 (* 1 = 0.192884 loss)
I0630 19:49:44.055567  5429 sgd_solver.cpp:106] Iteration 3500, lr = 0.0001
I0630 19:50:03.324772  5429 solver.cpp:290] Iteration 3600 (5.18979 iter/s, 19.2686s/100 iter), loss = 0.0916764
I0630 19:50:03.324816  5429 solver.cpp:309]     Train net output #0: loss = 0.0916765 (* 1 = 0.0916765 loss)
I0630 19:50:03.324825  5429 sgd_solver.cpp:106] Iteration 3600, lr = 0.0001
I0630 19:50:22.476022  5429 solver.cpp:290] Iteration 3700 (5.22177 iter/s, 19.1506s/100 iter), loss = 0.324966
I0630 19:50:22.476078  5429 solver.cpp:309]     Train net output #0: loss = 0.324967 (* 1 = 0.324967 loss)
I0630 19:50:22.476089  5429 sgd_solver.cpp:106] Iteration 3700, lr = 0.0001
I0630 19:50:41.555795  5429 solver.cpp:290] Iteration 3800 (5.24133 iter/s, 19.0791s/100 iter), loss = 0.235144
I0630 19:50:41.555840  5429 solver.cpp:309]     Train net output #0: loss = 0.235145 (* 1 = 0.235145 loss)
I0630 19:50:41.555848  5429 sgd_solver.cpp:106] Iteration 3800, lr = 0.0001
I0630 19:51:00.810511  5429 solver.cpp:290] Iteration 3900 (5.19371 iter/s, 19.2541s/100 iter), loss = 0.11207
I0630 19:51:00.810534  5429 solver.cpp:309]     Train net output #0: loss = 0.11207 (* 1 = 0.11207 loss)
I0630 19:51:00.810541  5429 sgd_solver.cpp:106] Iteration 3900, lr = 0.0001
I0630 19:51:19.866116  5429 solver.cpp:471] Iteration 4000, Testing net (#0)
I0630 19:52:54.264072  5429 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.91504
I0630 19:52:54.264169  5429 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.99196
I0630 19:52:54.264176  5429 solver.cpp:544]     Test net output #2: loss = 0.173459 (* 1 = 0.173459 loss)
I0630 19:52:54.471616  5429 solver.cpp:290] Iteration 4000 (0.879835 iter/s, 113.658s/100 iter), loss = 0.11938
I0630 19:52:54.471642  5429 solver.cpp:309]     Train net output #0: loss = 0.11938 (* 1 = 0.11938 loss)
I0630 19:52:54.471649  5429 sgd_solver.cpp:106] Iteration 4000, lr = 0.0001
I0630 19:53:38.248164  5429 solver.cpp:290] Iteration 4100 (2.2844 iter/s, 43.7751s/100 iter), loss = 0.159242
I0630 19:53:38.248214  5429 solver.cpp:309]     Train net output #0: loss = 0.159242 (* 1 = 0.159242 loss)
I0630 19:53:38.248221  5429 sgd_solver.cpp:106] Iteration 4100, lr = 0.0001
I0630 19:54:09.020256  5694 blocking_queue.cpp:50] Waiting for data
I0630 19:55:08.621134  5429 solver.cpp:290] Iteration 4200 (1.10656 iter/s, 90.3699s/100 iter), loss = 0.118977
I0630 19:55:08.621212  5429 solver.cpp:309]     Train net output #0: loss = 0.118978 (* 1 = 0.118978 loss)
I0630 19:55:08.621220  5429 sgd_solver.cpp:106] Iteration 4200, lr = 0.0001
I0630 19:55:27.445088  5429 solver.cpp:290] Iteration 4300 (5.31257 iter/s, 18.8233s/100 iter), loss = 0.295999
I0630 19:55:27.445112  5429 solver.cpp:309]     Train net output #0: loss = 0.295999 (* 1 = 0.295999 loss)
I0630 19:55:27.445119  5429 sgd_solver.cpp:106] Iteration 4300, lr = 0.0001
I0630 19:55:46.599792  5429 solver.cpp:290] Iteration 4400 (5.22082 iter/s, 19.1541s/100 iter), loss = 0.105363
I0630 19:55:46.599895  5429 solver.cpp:309]     Train net output #0: loss = 0.105363 (* 1 = 0.105363 loss)
I0630 19:55:46.599903  5429 sgd_solver.cpp:106] Iteration 4400, lr = 0.0001
I0630 19:56:05.720142  5429 solver.cpp:290] Iteration 4500 (5.23022 iter/s, 19.1196s/100 iter), loss = 0.134886
I0630 19:56:05.720166  5429 solver.cpp:309]     Train net output #0: loss = 0.134886 (* 1 = 0.134886 loss)
I0630 19:56:05.720173  5429 sgd_solver.cpp:106] Iteration 4500, lr = 0.0001
I0630 19:56:24.755962  5429 solver.cpp:290] Iteration 4600 (5.25343 iter/s, 19.0352s/100 iter), loss = 0.169664
I0630 19:56:24.756059  5429 solver.cpp:309]     Train net output #0: loss = 0.169664 (* 1 = 0.169664 loss)
I0630 19:56:24.756070  5429 sgd_solver.cpp:106] Iteration 4600, lr = 0.0001
I0630 19:56:43.847908  5429 solver.cpp:290] Iteration 4700 (5.238 iter/s, 19.0913s/100 iter), loss = 0.140418
I0630 19:56:43.847935  5429 solver.cpp:309]     Train net output #0: loss = 0.140418 (* 1 = 0.140418 loss)
I0630 19:56:43.847944  5429 sgd_solver.cpp:106] Iteration 4700, lr = 0.0001
I0630 19:57:02.952869  5429 solver.cpp:290] Iteration 4800 (5.23441 iter/s, 19.1043s/100 iter), loss = 0.121721
I0630 19:57:02.952950  5429 solver.cpp:309]     Train net output #0: loss = 0.121721 (* 1 = 0.121721 loss)
I0630 19:57:02.952965  5429 sgd_solver.cpp:106] Iteration 4800, lr = 0.0001
I0630 19:57:22.069211  5429 solver.cpp:290] Iteration 4900 (5.23131 iter/s, 19.1157s/100 iter), loss = 0.165884
I0630 19:57:22.069233  5429 solver.cpp:309]     Train net output #0: loss = 0.165884 (* 1 = 0.165884 loss)
I0630 19:57:22.069241  5429 sgd_solver.cpp:106] Iteration 4900, lr = 0.0001
I0630 19:57:41.283125  5429 solver.cpp:290] Iteration 5000 (5.20473 iter/s, 19.2133s/100 iter), loss = 0.101935
I0630 19:57:41.283202  5429 solver.cpp:309]     Train net output #0: loss = 0.101935 (* 1 = 0.101935 loss)
I0630 19:57:41.283213  5429 sgd_solver.cpp:106] Iteration 5000, lr = 0.0001
I0630 19:58:00.537964  5429 solver.cpp:290] Iteration 5100 (5.19368 iter/s, 19.2542s/100 iter), loss = 0.0650554
I0630 19:58:00.537987  5429 solver.cpp:309]     Train net output #0: loss = 0.0650555 (* 1 = 0.0650555 loss)
I0630 19:58:00.537993  5429 sgd_solver.cpp:106] Iteration 5100, lr = 0.0001
I0630 19:58:19.603713  5429 solver.cpp:290] Iteration 5200 (5.24517 iter/s, 19.0652s/100 iter), loss = 0.0917206
I0630 19:58:19.603785  5429 solver.cpp:309]     Train net output #0: loss = 0.0917208 (* 1 = 0.0917208 loss)
I0630 19:58:19.603794  5429 sgd_solver.cpp:106] Iteration 5200, lr = 0.0001
I0630 19:58:38.669136  5429 solver.cpp:290] Iteration 5300 (5.24527 iter/s, 19.0648s/100 iter), loss = 0.128555
I0630 19:58:38.669162  5429 solver.cpp:309]     Train net output #0: loss = 0.128556 (* 1 = 0.128556 loss)
I0630 19:58:38.669172  5429 sgd_solver.cpp:106] Iteration 5300, lr = 0.0001
I0630 19:58:57.780012  5429 solver.cpp:290] Iteration 5400 (5.23278 iter/s, 19.1103s/100 iter), loss = 0.116487
I0630 19:58:57.780093  5429 solver.cpp:309]     Train net output #0: loss = 0.116487 (* 1 = 0.116487 loss)
I0630 19:58:57.780103  5429 sgd_solver.cpp:106] Iteration 5400, lr = 0.0001
I0630 19:59:16.932225  5429 solver.cpp:290] Iteration 5500 (5.2215 iter/s, 19.1516s/100 iter), loss = 0.106772
I0630 19:59:16.932250  5429 solver.cpp:309]     Train net output #0: loss = 0.106772 (* 1 = 0.106772 loss)
I0630 19:59:16.932257  5429 sgd_solver.cpp:106] Iteration 5500, lr = 0.0001
I0630 19:59:36.160895  5429 solver.cpp:290] Iteration 5600 (5.20073 iter/s, 19.2281s/100 iter), loss = 0.120108
I0630 19:59:36.166146  5429 solver.cpp:309]     Train net output #0: loss = 0.120108 (* 1 = 0.120108 loss)
I0630 19:59:36.166193  5429 sgd_solver.cpp:106] Iteration 5600, lr = 0.0001
I0630 19:59:55.279652  5429 solver.cpp:290] Iteration 5700 (5.23205 iter/s, 19.113s/100 iter), loss = 0.132335
I0630 19:59:55.279675  5429 solver.cpp:309]     Train net output #0: loss = 0.132335 (* 1 = 0.132335 loss)
I0630 19:59:55.279682  5429 sgd_solver.cpp:106] Iteration 5700, lr = 0.0001
I0630 20:00:14.371248  5429 solver.cpp:290] Iteration 5800 (5.23807 iter/s, 19.091s/100 iter), loss = 0.214675
I0630 20:00:14.371318  5429 solver.cpp:309]     Train net output #0: loss = 0.214675 (* 1 = 0.214675 loss)
I0630 20:00:14.371326  5429 sgd_solver.cpp:106] Iteration 5800, lr = 0.0001
I0630 20:00:33.592474  5429 solver.cpp:290] Iteration 5900 (5.20275 iter/s, 19.2206s/100 iter), loss = 0.239315
I0630 20:00:33.592499  5429 solver.cpp:309]     Train net output #0: loss = 0.239315 (* 1 = 0.239315 loss)
I0630 20:00:33.592507  5429 sgd_solver.cpp:106] Iteration 5900, lr = 0.0001
I0630 20:00:52.384157  5429 solver.cpp:471] Iteration 6000, Testing net (#0)
I0630 20:02:27.948573  5429 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.92355
I0630 20:02:27.948674  5429 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.993404
I0630 20:02:27.948683  5429 solver.cpp:544]     Test net output #2: loss = 0.152629 (* 1 = 0.152629 loss)
I0630 20:02:28.162382  5429 solver.cpp:290] Iteration 6000 (0.872854 iter/s, 114.567s/100 iter), loss = 0.12572
I0630 20:02:28.162411  5429 solver.cpp:309]     Train net output #0: loss = 0.12572 (* 1 = 0.12572 loss)
I0630 20:02:28.162418  5429 sgd_solver.cpp:106] Iteration 6000, lr = 0.0001
I0630 20:04:16.850893  5429 solver.cpp:290] Iteration 6100 (0.920086 iter/s, 108.686s/100 iter), loss = 0.172857
I0630 20:04:16.850946  5429 solver.cpp:309]     Train net output #0: loss = 0.172857 (* 1 = 0.172857 loss)
I0630 20:04:16.850955  5429 sgd_solver.cpp:106] Iteration 6100, lr = 0.0001
I0630 20:04:52.937256  5660 blocking_queue.cpp:50] Waiting for data
I0630 20:05:31.799526  5713 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 20:08:45.353124  5429 solver.cpp:290] Iteration 6200 (0.372446 iter/s, 268.495s/100 iter), loss = 0.151617
I0630 20:08:45.353197  5429 solver.cpp:309]     Train net output #0: loss = 0.151617 (* 1 = 0.151617 loss)
I0630 20:08:45.353205  5429 sgd_solver.cpp:106] Iteration 6200, lr = 0.0001
I0630 20:08:48.563202  5694 blocking_queue.cpp:50] Waiting for data
I0630 20:11:31.809408  5429 solver.cpp:290] Iteration 6300 (0.600774 iter/s, 166.452s/100 iter), loss = 0.124393
I0630 20:11:31.809509  5429 solver.cpp:309]     Train net output #0: loss = 0.124393 (* 1 = 0.124393 loss)
I0630 20:11:31.809516  5429 sgd_solver.cpp:106] Iteration 6300, lr = 0.0001
I0630 20:11:50.913897  5429 solver.cpp:290] Iteration 6400 (5.23453 iter/s, 19.1039s/100 iter), loss = 0.150577
I0630 20:11:50.913921  5429 solver.cpp:309]     Train net output #0: loss = 0.150577 (* 1 = 0.150577 loss)
I0630 20:11:50.913928  5429 sgd_solver.cpp:106] Iteration 6400, lr = 0.0001
I0630 20:12:09.800011  5429 solver.cpp:290] Iteration 6500 (5.29504 iter/s, 18.8856s/100 iter), loss = 0.218773
I0630 20:12:09.800081  5429 solver.cpp:309]     Train net output #0: loss = 0.218773 (* 1 = 0.218773 loss)
I0630 20:12:09.800091  5429 sgd_solver.cpp:106] Iteration 6500, lr = 0.0001
I0630 20:12:28.852404  5429 solver.cpp:290] Iteration 6600 (5.24884 iter/s, 19.0518s/100 iter), loss = 0.129673
I0630 20:12:28.852427  5429 solver.cpp:309]     Train net output #0: loss = 0.129673 (* 1 = 0.129673 loss)
I0630 20:12:28.852434  5429 sgd_solver.cpp:106] Iteration 6600, lr = 0.0001
I0630 20:12:47.901103  5429 solver.cpp:290] Iteration 6700 (5.24984 iter/s, 19.0482s/100 iter), loss = 0.0755845
I0630 20:12:47.901206  5429 solver.cpp:309]     Train net output #0: loss = 0.0755844 (* 1 = 0.0755844 loss)
I0630 20:12:47.901216  5429 sgd_solver.cpp:106] Iteration 6700, lr = 0.0001
I0630 20:13:07.012439  5429 solver.cpp:290] Iteration 6800 (5.23267 iter/s, 19.1107s/100 iter), loss = 0.130629
I0630 20:13:07.012460  5429 solver.cpp:309]     Train net output #0: loss = 0.130629 (* 1 = 0.130629 loss)
I0630 20:13:07.012467  5429 sgd_solver.cpp:106] Iteration 6800, lr = 0.0001
I0630 20:13:26.521286  5429 solver.cpp:290] Iteration 6900 (5.12604 iter/s, 19.5082s/100 iter), loss = 0.0725434
I0630 20:13:26.521332  5429 solver.cpp:309]     Train net output #0: loss = 0.0725432 (* 1 = 0.0725432 loss)
I0630 20:13:26.521339  5429 sgd_solver.cpp:106] Iteration 6900, lr = 0.0001
I0630 20:13:45.387913  5429 solver.cpp:290] Iteration 7000 (5.30054 iter/s, 18.866s/100 iter), loss = 0.115313
I0630 20:13:45.387936  5429 solver.cpp:309]     Train net output #0: loss = 0.115313 (* 1 = 0.115313 loss)
I0630 20:13:45.387944  5429 sgd_solver.cpp:106] Iteration 7000, lr = 0.0001
I0630 20:14:04.546496  5429 solver.cpp:290] Iteration 7100 (5.21974 iter/s, 19.158s/100 iter), loss = 0.122775
I0630 20:14:04.546572  5429 solver.cpp:309]     Train net output #0: loss = 0.122775 (* 1 = 0.122775 loss)
I0630 20:14:04.546581  5429 sgd_solver.cpp:106] Iteration 7100, lr = 0.0001
I0630 20:14:23.509938  5429 solver.cpp:290] Iteration 7200 (5.27347 iter/s, 18.9628s/100 iter), loss = 0.160498
I0630 20:14:23.509963  5429 solver.cpp:309]     Train net output #0: loss = 0.160498 (* 1 = 0.160498 loss)
I0630 20:14:23.509970  5429 sgd_solver.cpp:106] Iteration 7200, lr = 0.0001
I0630 20:14:42.599973  5429 solver.cpp:290] Iteration 7300 (5.23849 iter/s, 19.0895s/100 iter), loss = 0.126562
I0630 20:14:42.600028  5429 solver.cpp:309]     Train net output #0: loss = 0.126562 (* 1 = 0.126562 loss)
I0630 20:14:42.600036  5429 sgd_solver.cpp:106] Iteration 7300, lr = 0.0001
I0630 20:15:01.669277  5429 solver.cpp:290] Iteration 7400 (5.24419 iter/s, 19.0687s/100 iter), loss = 0.196361
I0630 20:15:01.669299  5429 solver.cpp:309]     Train net output #0: loss = 0.196361 (* 1 = 0.196361 loss)
I0630 20:15:01.669306  5429 sgd_solver.cpp:106] Iteration 7400, lr = 0.0001
I0630 20:15:20.713816  5429 solver.cpp:290] Iteration 7500 (5.251 iter/s, 19.044s/100 iter), loss = 0.15232
I0630 20:15:20.713856  5429 solver.cpp:309]     Train net output #0: loss = 0.15232 (* 1 = 0.15232 loss)
I0630 20:15:20.713865  5429 sgd_solver.cpp:106] Iteration 7500, lr = 0.0001
I0630 20:15:39.804003  5429 solver.cpp:290] Iteration 7600 (5.23845 iter/s, 19.0896s/100 iter), loss = 0.100301
I0630 20:15:39.804028  5429 solver.cpp:309]     Train net output #0: loss = 0.100301 (* 1 = 0.100301 loss)
I0630 20:15:39.804034  5429 sgd_solver.cpp:106] Iteration 7600, lr = 0.0001
I0630 20:15:58.828616  5429 solver.cpp:290] Iteration 7700 (5.2565 iter/s, 19.0241s/100 iter), loss = 0.0969102
I0630 20:15:58.828670  5429 solver.cpp:309]     Train net output #0: loss = 0.09691 (* 1 = 0.09691 loss)
I0630 20:15:58.828681  5429 sgd_solver.cpp:106] Iteration 7700, lr = 0.0001
I0630 20:16:18.065829  5429 solver.cpp:290] Iteration 7800 (5.19842 iter/s, 19.2366s/100 iter), loss = 0.128974
I0630 20:16:18.065851  5429 solver.cpp:309]     Train net output #0: loss = 0.128974 (* 1 = 0.128974 loss)
I0630 20:16:18.065858  5429 sgd_solver.cpp:106] Iteration 7800, lr = 0.0001
I0630 20:16:37.170838  5429 solver.cpp:290] Iteration 7900 (5.23438 iter/s, 19.1045s/100 iter), loss = 0.119556
I0630 20:16:37.170910  5429 solver.cpp:309]     Train net output #0: loss = 0.119555 (* 1 = 0.119555 loss)
I0630 20:16:37.170918  5429 sgd_solver.cpp:106] Iteration 7900, lr = 0.0001
I0630 20:16:56.079802  5429 solver.cpp:471] Iteration 8000, Testing net (#0)
I0630 20:18:30.923048  5429 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.926498
I0630 20:18:30.923143  5429 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.994144
I0630 20:18:30.923151  5429 solver.cpp:544]     Test net output #2: loss = 0.142364 (* 1 = 0.142364 loss)
I0630 20:18:31.140166  5429 solver.cpp:290] Iteration 8000 (0.877451 iter/s, 113.966s/100 iter), loss = 0.108447
I0630 20:18:31.140189  5429 solver.cpp:309]     Train net output #0: loss = 0.108447 (* 1 = 0.108447 loss)
I0630 20:18:31.140197  5429 sgd_solver.cpp:106] Iteration 8000, lr = 0.0001
I0630 20:18:49.472411  5429 solver.cpp:290] Iteration 8100 (5.45501 iter/s, 18.3318s/100 iter), loss = 0.152832
I0630 20:18:49.472434  5429 solver.cpp:309]     Train net output #0: loss = 0.152832 (* 1 = 0.152832 loss)
I0630 20:18:49.472441  5429 sgd_solver.cpp:106] Iteration 8100, lr = 0.0001
I0630 20:20:47.969199  5660 blocking_queue.cpp:50] Waiting for data
I0630 20:21:56.343435  5429 solver.cpp:290] Iteration 8200 (0.535142 iter/s, 186.866s/100 iter), loss = 0.1286
I0630 20:21:56.343538  5429 solver.cpp:309]     Train net output #0: loss = 0.1286 (* 1 = 0.1286 loss)
I0630 20:21:56.343547  5429 sgd_solver.cpp:106] Iteration 8200, lr = 0.0001
I0630 20:22:17.855624  5429 solver.cpp:290] Iteration 8300 (4.64867 iter/s, 21.5115s/100 iter), loss = 0.0948518
I0630 20:22:17.855653  5429 solver.cpp:309]     Train net output #0: loss = 0.0948516 (* 1 = 0.0948516 loss)
I0630 20:22:17.855662  5429 sgd_solver.cpp:106] Iteration 8300, lr = 0.0001
I0630 20:22:37.157785  5429 solver.cpp:290] Iteration 8400 (5.1809 iter/s, 19.3016s/100 iter), loss = 0.0844778
I0630 20:22:37.157869  5429 solver.cpp:309]     Train net output #0: loss = 0.0844776 (* 1 = 0.0844776 loss)
I0630 20:22:37.157879  5429 sgd_solver.cpp:106] Iteration 8400, lr = 0.0001
I0630 20:22:56.151300  5429 solver.cpp:290] Iteration 8500 (5.26511 iter/s, 18.993s/100 iter), loss = 0.177681
I0630 20:22:56.151325  5429 solver.cpp:309]     Train net output #0: loss = 0.177681 (* 1 = 0.177681 loss)
I0630 20:22:56.151332  5429 sgd_solver.cpp:106] Iteration 8500, lr = 0.0001
I0630 20:23:14.970640  5429 solver.cpp:290] Iteration 8600 (5.31382 iter/s, 18.8188s/100 iter), loss = 0.120142
I0630 20:23:14.970690  5429 solver.cpp:309]     Train net output #0: loss = 0.120142 (* 1 = 0.120142 loss)
I0630 20:23:14.970696  5429 sgd_solver.cpp:106] Iteration 8600, lr = 0.0001
I0630 20:23:33.931332  5429 solver.cpp:290] Iteration 8700 (5.27421 iter/s, 18.9602s/100 iter), loss = 0.165305
I0630 20:23:33.931357  5429 solver.cpp:309]     Train net output #0: loss = 0.165305 (* 1 = 0.165305 loss)
I0630 20:23:33.931365  5429 sgd_solver.cpp:106] Iteration 8700, lr = 0.0001
I0630 20:23:53.176887  5429 solver.cpp:290] Iteration 8800 (5.19614 iter/s, 19.245s/100 iter), loss = 0.106414
I0630 20:23:53.176975  5429 solver.cpp:309]     Train net output #0: loss = 0.106413 (* 1 = 0.106413 loss)
I0630 20:23:53.176986  5429 sgd_solver.cpp:106] Iteration 8800, lr = 0.0001
I0630 20:24:12.155138  5429 solver.cpp:290] Iteration 8900 (5.26935 iter/s, 18.9777s/100 iter), loss = 0.112602
I0630 20:24:12.155159  5429 solver.cpp:309]     Train net output #0: loss = 0.112602 (* 1 = 0.112602 loss)
I0630 20:24:12.155166  5429 sgd_solver.cpp:106] Iteration 8900, lr = 0.0001
I0630 20:24:31.106016  5429 solver.cpp:290] Iteration 9000 (5.27694 iter/s, 18.9504s/100 iter), loss = 0.104817
I0630 20:24:31.106096  5429 solver.cpp:309]     Train net output #0: loss = 0.104816 (* 1 = 0.104816 loss)
I0630 20:24:31.106106  5429 sgd_solver.cpp:106] Iteration 9000, lr = 0.0001
I0630 20:24:50.151245  5429 solver.cpp:290] Iteration 9100 (5.25081 iter/s, 19.0447s/100 iter), loss = 0.213925
I0630 20:24:50.151266  5429 solver.cpp:309]     Train net output #0: loss = 0.213925 (* 1 = 0.213925 loss)
I0630 20:24:50.151274  5429 sgd_solver.cpp:106] Iteration 9100, lr = 0.0001
I0630 20:25:09.061745  5429 solver.cpp:290] Iteration 9200 (5.28821 iter/s, 18.91s/100 iter), loss = 0.103541
I0630 20:25:09.061811  5429 solver.cpp:309]     Train net output #0: loss = 0.10354 (* 1 = 0.10354 loss)
I0630 20:25:09.061820  5429 sgd_solver.cpp:106] Iteration 9200, lr = 0.0001
I0630 20:25:28.333459  5429 solver.cpp:290] Iteration 9300 (5.1891 iter/s, 19.2712s/100 iter), loss = 0.196834
I0630 20:25:28.333482  5429 solver.cpp:309]     Train net output #0: loss = 0.196834 (* 1 = 0.196834 loss)
I0630 20:25:28.333489  5429 sgd_solver.cpp:106] Iteration 9300, lr = 0.0001
I0630 20:25:47.385866  5429 solver.cpp:290] Iteration 9400 (5.24882 iter/s, 19.0519s/100 iter), loss = 0.228286
I0630 20:25:47.385962  5429 solver.cpp:309]     Train net output #0: loss = 0.228286 (* 1 = 0.228286 loss)
I0630 20:25:47.385973  5429 sgd_solver.cpp:106] Iteration 9400, lr = 0.0001
I0630 20:26:06.563931  5429 solver.cpp:290] Iteration 9500 (5.21445 iter/s, 19.1775s/100 iter), loss = 0.123546
I0630 20:26:06.563958  5429 solver.cpp:309]     Train net output #0: loss = 0.123546 (* 1 = 0.123546 loss)
I0630 20:26:06.563967  5429 sgd_solver.cpp:106] Iteration 9500, lr = 0.0001
I0630 20:26:25.786288  5429 solver.cpp:290] Iteration 9600 (5.20241 iter/s, 19.2218s/100 iter), loss = 0.105034
I0630 20:26:25.786418  5429 solver.cpp:309]     Train net output #0: loss = 0.105033 (* 1 = 0.105033 loss)
I0630 20:26:25.786429  5429 sgd_solver.cpp:106] Iteration 9600, lr = 0.0001
I0630 20:26:44.746600  5429 solver.cpp:290] Iteration 9700 (5.27434 iter/s, 18.9597s/100 iter), loss = 0.108437
I0630 20:26:44.746624  5429 solver.cpp:309]     Train net output #0: loss = 0.108437 (* 1 = 0.108437 loss)
I0630 20:26:44.746631  5429 sgd_solver.cpp:106] Iteration 9700, lr = 0.0001
I0630 20:27:03.814149  5429 solver.cpp:290] Iteration 9800 (5.24465 iter/s, 19.0671s/100 iter), loss = 0.0694628
I0630 20:27:03.814230  5429 solver.cpp:309]     Train net output #0: loss = 0.0694625 (* 1 = 0.0694625 loss)
I0630 20:27:03.814241  5429 sgd_solver.cpp:106] Iteration 9800, lr = 0.0001
I0630 20:27:22.975335  5429 solver.cpp:290] Iteration 9900 (5.21904 iter/s, 19.1606s/100 iter), loss = 0.207476
I0630 20:27:22.975361  5429 solver.cpp:309]     Train net output #0: loss = 0.207476 (* 1 = 0.207476 loss)
I0630 20:27:22.975369  5429 sgd_solver.cpp:106] Iteration 9900, lr = 0.0001
I0630 20:27:41.927918  5429 solver.cpp:598] Snapshotting to binary proto file training/cityscapes20_jsegnet21v2_2017-06-30_19-26-17/initial/cityscapes20_jsegnet21v2_iter_10000.caffemodel
I0630 20:27:42.034955  5429 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cityscapes20_jsegnet21v2_2017-06-30_19-26-17/initial/cityscapes20_jsegnet21v2_iter_10000.solverstate
I0630 20:27:42.085322  5429 solver.cpp:471] Iteration 10000, Testing net (#0)
I0630 20:29:15.119088  5429 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.92671
I0630 20:29:15.119163  5429 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.994139
I0630 20:29:15.119173  5429 solver.cpp:544]     Test net output #2: loss = 0.143588 (* 1 = 0.143588 loss)
I0630 20:29:15.334506  5429 solver.cpp:290] Iteration 10000 (0.890025 iter/s, 112.356s/100 iter), loss = 0.0817711
I0630 20:29:15.334533  5429 solver.cpp:309]     Train net output #0: loss = 0.0817707 (* 1 = 0.0817707 loss)
I0630 20:29:15.334542  5429 sgd_solver.cpp:106] Iteration 10000, lr = 0.0001
I0630 20:30:09.410066  5518 blocking_queue.cpp:50] Waiting for data
I0630 20:30:40.497797  5429 solver.cpp:290] Iteration 10100 (1.17424 iter/s, 85.1612s/100 iter), loss = 0.130447
I0630 20:30:40.497843  5429 solver.cpp:309]     Train net output #0: loss = 0.130447 (* 1 = 0.130447 loss)
I0630 20:30:40.497854  5429 sgd_solver.cpp:106] Iteration 10100, lr = 0.0001
I0630 20:33:45.526274  5695 blocking_queue.cpp:50] Waiting for data
I0630 20:33:49.623913  5429 solver.cpp:290] Iteration 10200 (0.528761 iter/s, 189.121s/100 iter), loss = 0.104663
I0630 20:33:49.623935  5429 solver.cpp:309]     Train net output #0: loss = 0.104662 (* 1 = 0.104662 loss)
I0630 20:33:49.623942  5429 sgd_solver.cpp:106] Iteration 10200, lr = 0.0001
I0630 20:34:12.797773  5429 solver.cpp:290] Iteration 10300 (4.31532 iter/s, 23.1733s/100 iter), loss = 0.150428
I0630 20:34:12.797796  5429 solver.cpp:309]     Train net output #0: loss = 0.150427 (* 1 = 0.150427 loss)
I0630 20:34:12.797802  5429 sgd_solver.cpp:106] Iteration 10300, lr = 0.0001
I0630 20:34:31.853058  5429 solver.cpp:290] Iteration 10400 (5.24803 iter/s, 19.0548s/100 iter), loss = 0.121831
I0630 20:34:31.853123  5429 solver.cpp:309]     Train net output #0: loss = 0.121831 (* 1 = 0.121831 loss)
I0630 20:34:31.853133  5429 sgd_solver.cpp:106] Iteration 10400, lr = 0.0001
I0630 20:34:51.055763  5429 solver.cpp:290] Iteration 10500 (5.20775 iter/s, 19.2022s/100 iter), loss = 0.0907077
I0630 20:34:51.055789  5429 solver.cpp:309]     Train net output #0: loss = 0.0907074 (* 1 = 0.0907074 loss)
I0630 20:34:51.055799  5429 sgd_solver.cpp:106] Iteration 10500, lr = 0.0001
I0630 20:35:10.151765  5429 solver.cpp:290] Iteration 10600 (5.23684 iter/s, 19.0955s/100 iter), loss = 0.0915265
I0630 20:35:10.151818  5429 solver.cpp:309]     Train net output #0: loss = 0.0915262 (* 1 = 0.0915262 loss)
I0630 20:35:10.151829  5429 sgd_solver.cpp:106] Iteration 10600, lr = 0.0001
I0630 20:35:29.367455  5429 solver.cpp:290] Iteration 10700 (5.20422 iter/s, 19.2152s/100 iter), loss = 0.163533
I0630 20:35:29.367480  5429 solver.cpp:309]     Train net output #0: loss = 0.163532 (* 1 = 0.163532 loss)
I0630 20:35:29.367486  5429 sgd_solver.cpp:106] Iteration 10700, lr = 0.0001
I0630 20:35:48.569056  5429 solver.cpp:290] Iteration 10800 (5.20804 iter/s, 19.2011s/100 iter), loss = 0.0563943
I0630 20:35:48.569186  5429 solver.cpp:309]     Train net output #0: loss = 0.0563939 (* 1 = 0.0563939 loss)
I0630 20:35:48.569195  5429 sgd_solver.cpp:106] Iteration 10800, lr = 0.0001
I0630 20:36:07.903909  5429 solver.cpp:290] Iteration 10900 (5.17217 iter/s, 19.3342s/100 iter), loss = 0.137538
I0630 20:36:07.903934  5429 solver.cpp:309]     Train net output #0: loss = 0.137538 (* 1 = 0.137538 loss)
I0630 20:36:07.903944  5429 sgd_solver.cpp:106] Iteration 10900, lr = 0.0001
I0630 20:36:27.142815  5429 solver.cpp:290] Iteration 11000 (5.19794 iter/s, 19.2384s/100 iter), loss = 0.103417
I0630 20:36:27.142874  5429 solver.cpp:309]     Train net output #0: loss = 0.103417 (* 1 = 0.103417 loss)
I0630 20:36:27.142881  5429 sgd_solver.cpp:106] Iteration 11000, lr = 0.0001
I0630 20:36:46.333997  5429 solver.cpp:290] Iteration 11100 (5.21087 iter/s, 19.1906s/100 iter), loss = 0.120189
I0630 20:36:46.334022  5429 solver.cpp:309]     Train net output #0: loss = 0.120188 (* 1 = 0.120188 loss)
I0630 20:36:46.334029  5429 sgd_solver.cpp:106] Iteration 11100, lr = 0.0001
I0630 20:37:05.494940  5429 solver.cpp:290] Iteration 11200 (5.21909 iter/s, 19.1604s/100 iter), loss = 0.0960929
I0630 20:37:05.494992  5429 solver.cpp:309]     Train net output #0: loss = 0.0960925 (* 1 = 0.0960925 loss)
I0630 20:37:05.495005  5429 sgd_solver.cpp:106] Iteration 11200, lr = 0.0001
I0630 20:37:24.635135  5429 solver.cpp:290] Iteration 11300 (5.22475 iter/s, 19.1397s/100 iter), loss = 0.0716244
I0630 20:37:24.635157  5429 solver.cpp:309]     Train net output #0: loss = 0.071624 (* 1 = 0.071624 loss)
I0630 20:37:24.635164  5429 sgd_solver.cpp:106] Iteration 11300, lr = 0.0001
I0630 20:37:43.648597  5429 solver.cpp:290] Iteration 11400 (5.25957 iter/s, 19.013s/100 iter), loss = 0.0828364
I0630 20:37:43.648641  5429 solver.cpp:309]     Train net output #0: loss = 0.082836 (* 1 = 0.082836 loss)
I0630 20:37:43.648651  5429 sgd_solver.cpp:106] Iteration 11400, lr = 0.0001
I0630 20:38:02.804214  5429 solver.cpp:290] Iteration 11500 (5.22054 iter/s, 19.1551s/100 iter), loss = 0.077565
I0630 20:38:02.804239  5429 solver.cpp:309]     Train net output #0: loss = 0.0775646 (* 1 = 0.0775646 loss)
I0630 20:38:02.804245  5429 sgd_solver.cpp:106] Iteration 11500, lr = 0.0001
I0630 20:38:22.098858  5429 solver.cpp:290] Iteration 11600 (5.18292 iter/s, 19.2941s/100 iter), loss = 0.0622331
I0630 20:38:22.098938  5429 solver.cpp:309]     Train net output #0: loss = 0.0622327 (* 1 = 0.0622327 loss)
I0630 20:38:22.098951  5429 sgd_solver.cpp:106] Iteration 11600, lr = 0.0001
I0630 20:38:41.193608  5429 solver.cpp:290] Iteration 11700 (5.23719 iter/s, 19.0942s/100 iter), loss = 0.140836
I0630 20:38:41.193630  5429 solver.cpp:309]     Train net output #0: loss = 0.140835 (* 1 = 0.140835 loss)
I0630 20:38:41.193639  5429 sgd_solver.cpp:106] Iteration 11700, lr = 0.0001
I0630 20:39:00.499003  5429 solver.cpp:290] Iteration 11800 (5.18003 iter/s, 19.3049s/100 iter), loss = 0.08539
I0630 20:39:00.499104  5429 solver.cpp:309]     Train net output #0: loss = 0.0853896 (* 1 = 0.0853896 loss)
I0630 20:39:00.499115  5429 sgd_solver.cpp:106] Iteration 11800, lr = 0.0001
I0630 20:39:19.411805  5429 solver.cpp:290] Iteration 11900 (5.28758 iter/s, 18.9122s/100 iter), loss = 0.136616
I0630 20:39:19.411829  5429 solver.cpp:309]     Train net output #0: loss = 0.136615 (* 1 = 0.136615 loss)
I0630 20:39:19.411836  5429 sgd_solver.cpp:106] Iteration 11900, lr = 0.0001
I0630 20:39:38.245220  5429 solver.cpp:471] Iteration 12000, Testing net (#0)
I0630 20:41:14.245569  5429 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.928017
I0630 20:41:14.245683  5429 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.993707
I0630 20:41:14.245692  5429 solver.cpp:544]     Test net output #2: loss = 0.147323 (* 1 = 0.147323 loss)
I0630 20:41:14.472506  5429 solver.cpp:290] Iteration 12000 (0.869128 iter/s, 115.058s/100 iter), loss = 0.0837495
I0630 20:41:14.472528  5429 solver.cpp:309]     Train net output #0: loss = 0.0837492 (* 1 = 0.0837492 loss)
I0630 20:41:14.472537  5429 sgd_solver.cpp:106] Iteration 12000, lr = 0.0001
I0630 20:42:15.729230  5713 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 20:42:38.739426  5429 solver.cpp:290] Iteration 12100 (1.18673 iter/s, 84.2648s/100 iter), loss = 0.0901462
I0630 20:42:38.739451  5429 solver.cpp:309]     Train net output #0: loss = 0.0901458 (* 1 = 0.0901458 loss)
I0630 20:42:38.739459  5429 sgd_solver.cpp:106] Iteration 12100, lr = 0.0001
I0630 20:42:44.696269  5518 blocking_queue.cpp:50] Waiting for data
I0630 20:43:44.759868  5429 solver.cpp:290] Iteration 12200 (1.51472 iter/s, 66.0188s/100 iter), loss = 0.0899194
I0630 20:43:44.759917  5429 solver.cpp:309]     Train net output #0: loss = 0.089919 (* 1 = 0.089919 loss)
I0630 20:43:44.759924  5429 sgd_solver.cpp:106] Iteration 12200, lr = 0.0001
I0630 20:43:45.033756  5695 blocking_queue.cpp:50] Waiting for data
I0630 20:44:03.931874  5429 solver.cpp:290] Iteration 12300 (5.21608 iter/s, 19.1715s/100 iter), loss = 0.0900487
I0630 20:44:03.931895  5429 solver.cpp:309]     Train net output #0: loss = 0.0900484 (* 1 = 0.0900484 loss)
I0630 20:44:03.931903  5429 sgd_solver.cpp:106] Iteration 12300, lr = 0.0001
I0630 20:44:23.014855  5429 solver.cpp:290] Iteration 12400 (5.24041 iter/s, 19.0825s/100 iter), loss = 0.0795638
I0630 20:44:23.014925  5429 solver.cpp:309]     Train net output #0: loss = 0.0795635 (* 1 = 0.0795635 loss)
I0630 20:44:23.014932  5429 sgd_solver.cpp:106] Iteration 12400, lr = 0.0001
I0630 20:44:42.166259  5429 solver.cpp:290] Iteration 12500 (5.2217 iter/s, 19.1509s/100 iter), loss = 0.075455
I0630 20:44:42.166281  5429 solver.cpp:309]     Train net output #0: loss = 0.0754546 (* 1 = 0.0754546 loss)
I0630 20:44:42.166288  5429 sgd_solver.cpp:106] Iteration 12500, lr = 0.0001
I0630 20:45:01.236198  5429 solver.cpp:290] Iteration 12600 (5.24399 iter/s, 19.0694s/100 iter), loss = 0.0879664
I0630 20:45:01.236275  5429 solver.cpp:309]     Train net output #0: loss = 0.087966 (* 1 = 0.087966 loss)
I0630 20:45:01.236284  5429 sgd_solver.cpp:106] Iteration 12600, lr = 0.0001
I0630 20:45:20.340623  5429 solver.cpp:290] Iteration 12700 (5.23454 iter/s, 19.1039s/100 iter), loss = 0.100452
I0630 20:45:20.340649  5429 solver.cpp:309]     Train net output #0: loss = 0.100452 (* 1 = 0.100452 loss)
I0630 20:45:20.340658  5429 sgd_solver.cpp:106] Iteration 12700, lr = 0.0001
I0630 20:45:39.450958  5429 solver.cpp:290] Iteration 12800 (5.23291 iter/s, 19.1098s/100 iter), loss = 0.0786972
I0630 20:45:39.451040  5429 solver.cpp:309]     Train net output #0: loss = 0.0786969 (* 1 = 0.0786969 loss)
I0630 20:45:39.451050  5429 sgd_solver.cpp:106] Iteration 12800, lr = 0.0001
I0630 20:45:58.474997  5429 solver.cpp:290] Iteration 12900 (5.25666 iter/s, 19.0235s/100 iter), loss = 0.0620362
I0630 20:45:58.475019  5429 solver.cpp:309]     Train net output #0: loss = 0.0620358 (* 1 = 0.0620358 loss)
I0630 20:45:58.475026  5429 sgd_solver.cpp:106] Iteration 12900, lr = 0.0001
I0630 20:46:17.674113  5429 solver.cpp:290] Iteration 13000 (5.20871 iter/s, 19.1986s/100 iter), loss = 0.0829186
I0630 20:46:17.674186  5429 solver.cpp:309]     Train net output #0: loss = 0.0829183 (* 1 = 0.0829183 loss)
I0630 20:46:17.674195  5429 sgd_solver.cpp:106] Iteration 13000, lr = 0.0001
I0630 20:46:36.858489  5429 solver.cpp:290] Iteration 13100 (5.21272 iter/s, 19.1838s/100 iter), loss = 0.105977
I0630 20:46:36.858515  5429 solver.cpp:309]     Train net output #0: loss = 0.105977 (* 1 = 0.105977 loss)
I0630 20:46:36.858522  5429 sgd_solver.cpp:106] Iteration 13100, lr = 0.0001
I0630 20:46:56.089915  5429 solver.cpp:290] Iteration 13200 (5.19996 iter/s, 19.2309s/100 iter), loss = 0.0800845
I0630 20:46:56.089984  5429 solver.cpp:309]     Train net output #0: loss = 0.0800842 (* 1 = 0.0800842 loss)
I0630 20:46:56.089993  5429 sgd_solver.cpp:106] Iteration 13200, lr = 0.0001
I0630 20:47:15.072578  5429 solver.cpp:290] Iteration 13300 (5.26811 iter/s, 18.9821s/100 iter), loss = 0.0749383
I0630 20:47:15.072607  5429 solver.cpp:309]     Train net output #0: loss = 0.074938 (* 1 = 0.074938 loss)
I0630 20:47:15.072616  5429 sgd_solver.cpp:106] Iteration 13300, lr = 0.0001
I0630 20:47:34.029614  5429 solver.cpp:290] Iteration 13400 (5.27523 iter/s, 18.9565s/100 iter), loss = 0.0610113
I0630 20:47:34.029703  5429 solver.cpp:309]     Train net output #0: loss = 0.061011 (* 1 = 0.061011 loss)
I0630 20:47:34.029713  5429 sgd_solver.cpp:106] Iteration 13400, lr = 0.0001
I0630 20:47:53.073302  5429 solver.cpp:290] Iteration 13500 (5.25126 iter/s, 19.043s/100 iter), loss = 0.128607
I0630 20:47:53.073324  5429 solver.cpp:309]     Train net output #0: loss = 0.128607 (* 1 = 0.128607 loss)
I0630 20:47:53.073331  5429 sgd_solver.cpp:106] Iteration 13500, lr = 0.0001
I0630 20:48:12.078768  5429 solver.cpp:290] Iteration 13600 (5.2618 iter/s, 19.0049s/100 iter), loss = 0.0888473
I0630 20:48:12.078855  5429 solver.cpp:309]     Train net output #0: loss = 0.088847 (* 1 = 0.088847 loss)
I0630 20:48:12.078866  5429 sgd_solver.cpp:106] Iteration 13600, lr = 0.0001
I0630 20:48:31.069595  5429 solver.cpp:290] Iteration 13700 (5.26588 iter/s, 18.9902s/100 iter), loss = 0.0791066
I0630 20:48:31.069617  5429 solver.cpp:309]     Train net output #0: loss = 0.0791063 (* 1 = 0.0791063 loss)
I0630 20:48:31.069624  5429 sgd_solver.cpp:106] Iteration 13700, lr = 0.0001
I0630 20:48:50.340338  5429 solver.cpp:290] Iteration 13800 (5.18937 iter/s, 19.2702s/100 iter), loss = 0.115439
I0630 20:48:50.340387  5429 solver.cpp:309]     Train net output #0: loss = 0.115438 (* 1 = 0.115438 loss)
I0630 20:48:50.340395  5429 sgd_solver.cpp:106] Iteration 13800, lr = 0.0001
I0630 20:49:09.529795  5429 solver.cpp:290] Iteration 13900 (5.21136 iter/s, 19.1889s/100 iter), loss = 0.0785274
I0630 20:49:09.529819  5429 solver.cpp:309]     Train net output #0: loss = 0.0785272 (* 1 = 0.0785272 loss)
I0630 20:49:09.529824  5429 sgd_solver.cpp:106] Iteration 13900, lr = 0.0001
I0630 20:49:28.434123  5429 solver.cpp:471] Iteration 14000, Testing net (#0)
I0630 20:51:02.683445  5429 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.925848
I0630 20:51:02.683535  5429 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.993884
I0630 20:51:02.683542  5429 solver.cpp:544]     Test net output #2: loss = 0.142218 (* 1 = 0.142218 loss)
I0630 20:51:02.897466  5429 solver.cpp:290] Iteration 14000 (0.882111 iter/s, 113.364s/100 iter), loss = 0.0795643
I0630 20:51:02.897490  5429 solver.cpp:309]     Train net output #0: loss = 0.0795641 (* 1 = 0.0795641 loss)
I0630 20:51:02.897496  5429 sgd_solver.cpp:106] Iteration 14000, lr = 0.0001
I0630 20:51:21.053231  5429 solver.cpp:290] Iteration 14100 (5.50806 iter/s, 18.1552s/100 iter), loss = 0.187062
I0630 20:51:21.053254  5429 solver.cpp:309]     Train net output #0: loss = 0.187061 (* 1 = 0.187061 loss)
I0630 20:51:21.053261  5429 sgd_solver.cpp:106] Iteration 14100, lr = 0.0001
I0630 20:51:40.207504  5429 solver.cpp:290] Iteration 14200 (5.22092 iter/s, 19.1537s/100 iter), loss = 0.0575425
I0630 20:51:40.207574  5429 solver.cpp:309]     Train net output #0: loss = 0.0575423 (* 1 = 0.0575423 loss)
I0630 20:51:40.207586  5429 sgd_solver.cpp:106] Iteration 14200, lr = 0.0001
I0630 20:51:59.225464  5429 solver.cpp:290] Iteration 14300 (5.25836 iter/s, 19.0173s/100 iter), loss = 0.076787
I0630 20:51:59.225491  5429 solver.cpp:309]     Train net output #0: loss = 0.0767867 (* 1 = 0.0767867 loss)
I0630 20:51:59.225498  5429 sgd_solver.cpp:106] Iteration 14300, lr = 0.0001
I0630 20:52:18.398134  5429 solver.cpp:290] Iteration 14400 (5.21591 iter/s, 19.1721s/100 iter), loss = 0.0840433
I0630 20:52:18.398172  5429 solver.cpp:309]     Train net output #0: loss = 0.084043 (* 1 = 0.084043 loss)
I0630 20:52:18.398180  5429 sgd_solver.cpp:106] Iteration 14400, lr = 0.0001
I0630 20:52:37.572451  5429 solver.cpp:290] Iteration 14500 (5.21547 iter/s, 19.1737s/100 iter), loss = 0.152902
I0630 20:52:37.572474  5429 solver.cpp:309]     Train net output #0: loss = 0.152902 (* 1 = 0.152902 loss)
I0630 20:52:37.572480  5429 sgd_solver.cpp:106] Iteration 14500, lr = 0.0001
I0630 20:52:56.826803  5429 solver.cpp:290] Iteration 14600 (5.19378 iter/s, 19.2538s/100 iter), loss = 0.0803616
I0630 20:52:56.826870  5429 solver.cpp:309]     Train net output #0: loss = 0.0803613 (* 1 = 0.0803613 loss)
I0630 20:52:56.826879  5429 sgd_solver.cpp:106] Iteration 14600, lr = 0.0001
I0630 20:53:15.985292  5429 solver.cpp:290] Iteration 14700 (5.21978 iter/s, 19.1579s/100 iter), loss = 0.121181
I0630 20:53:15.985319  5429 solver.cpp:309]     Train net output #0: loss = 0.12118 (* 1 = 0.12118 loss)
I0630 20:53:15.985328  5429 sgd_solver.cpp:106] Iteration 14700, lr = 0.0001
I0630 20:53:34.983680  5429 solver.cpp:290] Iteration 14800 (5.26376 iter/s, 18.9978s/100 iter), loss = 0.0718724
I0630 20:53:34.983757  5429 solver.cpp:309]     Train net output #0: loss = 0.0718721 (* 1 = 0.0718721 loss)
I0630 20:53:34.983765  5429 sgd_solver.cpp:106] Iteration 14800, lr = 0.0001
I0630 20:53:54.089440  5429 solver.cpp:290] Iteration 14900 (5.23419 iter/s, 19.1051s/100 iter), loss = 0.23514
I0630 20:53:54.089462  5429 solver.cpp:309]     Train net output #0: loss = 0.23514 (* 1 = 0.23514 loss)
I0630 20:53:54.089469  5429 sgd_solver.cpp:106] Iteration 14900, lr = 0.0001
I0630 20:54:13.060606  5429 solver.cpp:290] Iteration 15000 (5.27131 iter/s, 18.9706s/100 iter), loss = 0.118394
I0630 20:54:13.060678  5429 solver.cpp:309]     Train net output #0: loss = 0.118394 (* 1 = 0.118394 loss)
I0630 20:54:13.060684  5429 sgd_solver.cpp:106] Iteration 15000, lr = 0.0001
I0630 20:54:32.271957  5429 solver.cpp:290] Iteration 15100 (5.20542 iter/s, 19.2107s/100 iter), loss = 0.139644
I0630 20:54:32.271982  5429 solver.cpp:309]     Train net output #0: loss = 0.139643 (* 1 = 0.139643 loss)
I0630 20:54:32.271991  5429 sgd_solver.cpp:106] Iteration 15100, lr = 0.0001
I0630 20:54:51.467821  5429 solver.cpp:290] Iteration 15200 (5.20961 iter/s, 19.1953s/100 iter), loss = 0.0825212
I0630 20:54:51.467895  5429 solver.cpp:309]     Train net output #0: loss = 0.0825208 (* 1 = 0.0825208 loss)
I0630 20:54:51.467902  5429 sgd_solver.cpp:106] Iteration 15200, lr = 0.0001
I0630 20:55:10.351411  5429 solver.cpp:290] Iteration 15300 (5.29577 iter/s, 18.883s/100 iter), loss = 0.0775852
I0630 20:55:10.351433  5429 solver.cpp:309]     Train net output #0: loss = 0.0775849 (* 1 = 0.0775849 loss)
I0630 20:55:10.351440  5429 sgd_solver.cpp:106] Iteration 15300, lr = 0.0001
I0630 20:55:29.521488  5429 solver.cpp:290] Iteration 15400 (5.21661 iter/s, 19.1695s/100 iter), loss = 0.0655288
I0630 20:55:29.521561  5429 solver.cpp:309]     Train net output #0: loss = 0.0655284 (* 1 = 0.0655284 loss)
I0630 20:55:29.521569  5429 sgd_solver.cpp:106] Iteration 15400, lr = 0.0001
I0630 20:55:48.514051  5429 solver.cpp:290] Iteration 15500 (5.26539 iter/s, 18.992s/100 iter), loss = 0.0769465
I0630 20:55:48.514073  5429 solver.cpp:309]     Train net output #0: loss = 0.0769461 (* 1 = 0.0769461 loss)
I0630 20:55:48.514081  5429 sgd_solver.cpp:106] Iteration 15500, lr = 0.0001
I0630 20:56:07.773617  5429 solver.cpp:290] Iteration 15600 (5.19238 iter/s, 19.259s/100 iter), loss = 0.077483
I0630 20:56:07.773694  5429 solver.cpp:309]     Train net output #0: loss = 0.0774827 (* 1 = 0.0774827 loss)
I0630 20:56:07.773705  5429 sgd_solver.cpp:106] Iteration 15600, lr = 0.0001
I0630 20:56:26.952617  5429 solver.cpp:290] Iteration 15700 (5.2142 iter/s, 19.1784s/100 iter), loss = 0.115442
I0630 20:56:26.952641  5429 solver.cpp:309]     Train net output #0: loss = 0.115441 (* 1 = 0.115441 loss)
I0630 20:56:26.952651  5429 sgd_solver.cpp:106] Iteration 15700, lr = 0.0001
I0630 20:56:46.195767  5429 solver.cpp:290] Iteration 15800 (5.1968 iter/s, 19.2426s/100 iter), loss = 0.0870652
I0630 20:56:46.195828  5429 solver.cpp:309]     Train net output #0: loss = 0.0870648 (* 1 = 0.0870648 loss)
I0630 20:56:46.195837  5429 sgd_solver.cpp:106] Iteration 15800, lr = 0.0001
I0630 20:57:05.261376  5429 solver.cpp:290] Iteration 15900 (5.24521 iter/s, 19.065s/100 iter), loss = 0.11519
I0630 20:57:05.261399  5429 solver.cpp:309]     Train net output #0: loss = 0.115189 (* 1 = 0.115189 loss)
I0630 20:57:05.261406  5429 sgd_solver.cpp:106] Iteration 15900, lr = 0.0001
I0630 20:57:24.333019  5429 solver.cpp:471] Iteration 16000, Testing net (#0)
I0630 20:58:58.069967  5429 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.930884
I0630 20:58:58.070143  5429 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.994987
I0630 20:58:58.070153  5429 solver.cpp:544]     Test net output #2: loss = 0.140637 (* 1 = 0.140637 loss)
I0630 20:58:58.296066  5429 solver.cpp:290] Iteration 16000 (0.884708 iter/s, 113.032s/100 iter), loss = 0.0831568
I0630 20:58:58.296089  5429 solver.cpp:309]     Train net output #0: loss = 0.0831564 (* 1 = 0.0831564 loss)
I0630 20:58:58.296097  5429 sgd_solver.cpp:106] Iteration 16000, lr = 0.0001
I0630 20:59:16.499593  5429 solver.cpp:290] Iteration 16100 (5.4936 iter/s, 18.203s/100 iter), loss = 0.0666988
I0630 20:59:16.499620  5429 solver.cpp:309]     Train net output #0: loss = 0.0666984 (* 1 = 0.0666984 loss)
I0630 20:59:16.499629  5429 sgd_solver.cpp:106] Iteration 16100, lr = 0.0001
I0630 20:59:35.614058  5429 solver.cpp:290] Iteration 16200 (5.23179 iter/s, 19.1139s/100 iter), loss = 0.0957872
I0630 20:59:35.614146  5429 solver.cpp:309]     Train net output #0: loss = 0.0957868 (* 1 = 0.0957868 loss)
I0630 20:59:35.614157  5429 sgd_solver.cpp:106] Iteration 16200, lr = 0.0001
I0630 20:59:54.612365  5429 solver.cpp:290] Iteration 16300 (5.2638 iter/s, 18.9977s/100 iter), loss = 0.0568139
I0630 20:59:54.612388  5429 solver.cpp:309]     Train net output #0: loss = 0.0568135 (* 1 = 0.0568135 loss)
I0630 20:59:54.612396  5429 sgd_solver.cpp:106] Iteration 16300, lr = 0.0001
I0630 21:00:13.884780  5429 solver.cpp:290] Iteration 16400 (5.18891 iter/s, 19.2719s/100 iter), loss = 0.0974117
I0630 21:00:13.884824  5429 solver.cpp:309]     Train net output #0: loss = 0.0974113 (* 1 = 0.0974113 loss)
I0630 21:00:13.884831  5429 sgd_solver.cpp:106] Iteration 16400, lr = 0.0001
I0630 21:00:33.085340  5429 solver.cpp:290] Iteration 16500 (5.20834 iter/s, 19.2s/100 iter), loss = 0.062476
I0630 21:00:33.085367  5429 solver.cpp:309]     Train net output #0: loss = 0.0624756 (* 1 = 0.0624756 loss)
I0630 21:00:33.085381  5429 sgd_solver.cpp:106] Iteration 16500, lr = 0.0001
I0630 21:00:52.149189  5429 solver.cpp:290] Iteration 16600 (5.24568 iter/s, 19.0633s/100 iter), loss = 0.0704622
I0630 21:00:52.149245  5429 solver.cpp:309]     Train net output #0: loss = 0.0704618 (* 1 = 0.0704618 loss)
I0630 21:00:52.149256  5429 sgd_solver.cpp:106] Iteration 16600, lr = 0.0001
I0630 21:01:11.479930  5429 solver.cpp:290] Iteration 16700 (5.17327 iter/s, 19.3301s/100 iter), loss = 0.0618501
I0630 21:01:11.479954  5429 solver.cpp:309]     Train net output #0: loss = 0.0618497 (* 1 = 0.0618497 loss)
I0630 21:01:11.479960  5429 sgd_solver.cpp:106] Iteration 16700, lr = 0.0001
I0630 21:01:30.542083  5429 solver.cpp:290] Iteration 16800 (5.24615 iter/s, 19.0616s/100 iter), loss = 0.129682
I0630 21:01:30.542193  5429 solver.cpp:309]     Train net output #0: loss = 0.129681 (* 1 = 0.129681 loss)
I0630 21:01:30.542203  5429 sgd_solver.cpp:106] Iteration 16800, lr = 0.0001
I0630 21:01:49.660769  5429 solver.cpp:290] Iteration 16900 (5.23066 iter/s, 19.118s/100 iter), loss = 0.0983927
I0630 21:01:49.660794  5429 solver.cpp:309]     Train net output #0: loss = 0.0983923 (* 1 = 0.0983923 loss)
I0630 21:01:49.660804  5429 sgd_solver.cpp:106] Iteration 16900, lr = 0.0001
I0630 21:02:09.013010  5429 solver.cpp:290] Iteration 17000 (5.16751 iter/s, 19.3517s/100 iter), loss = 0.0746811
I0630 21:02:09.013058  5429 solver.cpp:309]     Train net output #0: loss = 0.0746806 (* 1 = 0.0746806 loss)
I0630 21:02:09.013067  5429 sgd_solver.cpp:106] Iteration 17000, lr = 0.0001
I0630 21:02:28.166365  5429 solver.cpp:290] Iteration 17100 (5.22118 iter/s, 19.1528s/100 iter), loss = 0.0867968
I0630 21:02:28.166393  5429 solver.cpp:309]     Train net output #0: loss = 0.0867964 (* 1 = 0.0867964 loss)
I0630 21:02:28.166401  5429 sgd_solver.cpp:106] Iteration 17100, lr = 0.0001
I0630 21:02:47.265058  5429 solver.cpp:290] Iteration 17200 (5.23612 iter/s, 19.0981s/100 iter), loss = 0.0828645
I0630 21:02:47.265178  5429 solver.cpp:309]     Train net output #0: loss = 0.0828641 (* 1 = 0.0828641 loss)
I0630 21:02:47.265188  5429 sgd_solver.cpp:106] Iteration 17200, lr = 0.0001
I0630 21:03:06.266162  5429 solver.cpp:290] Iteration 17300 (5.26303 iter/s, 19.0005s/100 iter), loss = 0.0727931
I0630 21:03:06.266186  5429 solver.cpp:309]     Train net output #0: loss = 0.0727927 (* 1 = 0.0727927 loss)
I0630 21:03:06.266192  5429 sgd_solver.cpp:106] Iteration 17300, lr = 0.0001
I0630 21:03:25.307121  5429 solver.cpp:290] Iteration 17400 (5.25199 iter/s, 19.0404s/100 iter), loss = 0.0825397
I0630 21:03:25.307173  5429 solver.cpp:309]     Train net output #0: loss = 0.0825393 (* 1 = 0.0825393 loss)
I0630 21:03:25.307179  5429 sgd_solver.cpp:106] Iteration 17400, lr = 0.0001
I0630 21:03:44.537261  5429 solver.cpp:290] Iteration 17500 (5.20033 iter/s, 19.2295s/100 iter), loss = 0.0647001
I0630 21:03:44.537286  5429 solver.cpp:309]     Train net output #0: loss = 0.0646997 (* 1 = 0.0646997 loss)
I0630 21:03:44.537295  5429 sgd_solver.cpp:106] Iteration 17500, lr = 0.0001
I0630 21:04:03.771637  5429 solver.cpp:290] Iteration 17600 (5.19918 iter/s, 19.2338s/100 iter), loss = 0.0904891
I0630 21:04:03.772135  5429 solver.cpp:309]     Train net output #0: loss = 0.0904887 (* 1 = 0.0904887 loss)
I0630 21:04:03.772145  5429 sgd_solver.cpp:106] Iteration 17600, lr = 0.0001
I0630 21:04:22.804414  5429 solver.cpp:290] Iteration 17700 (5.25438 iter/s, 19.0317s/100 iter), loss = 0.138211
I0630 21:04:22.804438  5429 solver.cpp:309]     Train net output #0: loss = 0.138211 (* 1 = 0.138211 loss)
I0630 21:04:22.804447  5429 sgd_solver.cpp:106] Iteration 17700, lr = 0.0001
I0630 21:04:41.811810  5429 solver.cpp:290] Iteration 17800 (5.26126 iter/s, 19.0068s/100 iter), loss = 0.0687797
I0630 21:04:41.811863  5429 solver.cpp:309]     Train net output #0: loss = 0.0687793 (* 1 = 0.0687793 loss)
I0630 21:04:41.811870  5429 sgd_solver.cpp:106] Iteration 17800, lr = 0.0001
I0630 21:05:00.857939  5429 solver.cpp:290] Iteration 17900 (5.25057 iter/s, 19.0455s/100 iter), loss = 0.117813
I0630 21:05:00.857964  5429 solver.cpp:309]     Train net output #0: loss = 0.117813 (* 1 = 0.117813 loss)
I0630 21:05:00.857971  5429 sgd_solver.cpp:106] Iteration 17900, lr = 0.0001
I0630 21:05:19.872777  5429 solver.cpp:471] Iteration 18000, Testing net (#0)
I0630 21:06:53.585321  5429 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.93318
I0630 21:06:53.585400  5429 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.994845
I0630 21:06:53.585407  5429 solver.cpp:544]     Test net output #2: loss = 0.14004 (* 1 = 0.14004 loss)
I0630 21:06:53.802940  5429 solver.cpp:290] Iteration 18000 (0.885411 iter/s, 112.942s/100 iter), loss = 0.0896128
I0630 21:06:53.802964  5429 solver.cpp:309]     Train net output #0: loss = 0.0896124 (* 1 = 0.0896124 loss)
I0630 21:06:53.802970  5429 sgd_solver.cpp:106] Iteration 18000, lr = 0.0001
I0630 21:07:12.217141  5429 solver.cpp:290] Iteration 18100 (5.43075 iter/s, 18.4137s/100 iter), loss = 0.102937
I0630 21:07:12.217164  5429 solver.cpp:309]     Train net output #0: loss = 0.102936 (* 1 = 0.102936 loss)
I0630 21:07:12.217170  5429 sgd_solver.cpp:106] Iteration 18100, lr = 0.0001
I0630 21:07:31.352336  5429 solver.cpp:290] Iteration 18200 (5.22612 iter/s, 19.1346s/100 iter), loss = 0.069806
I0630 21:07:31.352445  5429 solver.cpp:309]     Train net output #0: loss = 0.0698056 (* 1 = 0.0698056 loss)
I0630 21:07:31.352455  5429 sgd_solver.cpp:106] Iteration 18200, lr = 0.0001
I0630 21:07:50.477869  5429 solver.cpp:290] Iteration 18300 (5.22879 iter/s, 19.1249s/100 iter), loss = 0.120183
I0630 21:07:50.477895  5429 solver.cpp:309]     Train net output #0: loss = 0.120183 (* 1 = 0.120183 loss)
I0630 21:07:50.477903  5429 sgd_solver.cpp:106] Iteration 18300, lr = 0.0001
I0630 21:08:09.492986  5429 solver.cpp:290] Iteration 18400 (5.25913 iter/s, 19.0146s/100 iter), loss = 0.157442
I0630 21:08:09.493082  5429 solver.cpp:309]     Train net output #0: loss = 0.157442 (* 1 = 0.157442 loss)
I0630 21:08:09.493090  5429 sgd_solver.cpp:106] Iteration 18400, lr = 0.0001
I0630 21:08:28.493546  5429 solver.cpp:290] Iteration 18500 (5.26318 iter/s, 18.9999s/100 iter), loss = 0.113923
I0630 21:08:28.493571  5429 solver.cpp:309]     Train net output #0: loss = 0.113922 (* 1 = 0.113922 loss)
I0630 21:08:28.493577  5429 sgd_solver.cpp:106] Iteration 18500, lr = 0.0001
I0630 21:08:47.395836  5429 solver.cpp:290] Iteration 18600 (5.29052 iter/s, 18.9017s/100 iter), loss = 0.0639297
I0630 21:08:47.395884  5429 solver.cpp:309]     Train net output #0: loss = 0.0639293 (* 1 = 0.0639293 loss)
I0630 21:08:47.395890  5429 sgd_solver.cpp:106] Iteration 18600, lr = 0.0001
I0630 21:09:06.519739  5429 solver.cpp:290] Iteration 18700 (5.22922 iter/s, 19.1233s/100 iter), loss = 0.109442
I0630 21:09:06.519762  5429 solver.cpp:309]     Train net output #0: loss = 0.109441 (* 1 = 0.109441 loss)
I0630 21:09:06.519768  5429 sgd_solver.cpp:106] Iteration 18700, lr = 0.0001
I0630 21:09:25.594880  5429 solver.cpp:290] Iteration 18800 (5.24258 iter/s, 19.0746s/100 iter), loss = 0.0801012
I0630 21:09:25.594936  5429 solver.cpp:309]     Train net output #0: loss = 0.0801008 (* 1 = 0.0801008 loss)
I0630 21:09:25.594944  5429 sgd_solver.cpp:106] Iteration 18800, lr = 0.0001
I0630 21:09:44.899235  5429 solver.cpp:290] Iteration 18900 (5.18034 iter/s, 19.3038s/100 iter), loss = 0.0820159
I0630 21:09:44.899260  5429 solver.cpp:309]     Train net output #0: loss = 0.0820155 (* 1 = 0.0820155 loss)
I0630 21:09:44.899267  5429 sgd_solver.cpp:106] Iteration 18900, lr = 0.0001
I0630 21:10:04.061269  5429 solver.cpp:290] Iteration 19000 (5.2188 iter/s, 19.1615s/100 iter), loss = 0.0876166
I0630 21:10:04.061321  5429 solver.cpp:309]     Train net output #0: loss = 0.0876162 (* 1 = 0.0876162 loss)
I0630 21:10:04.061328  5429 sgd_solver.cpp:106] Iteration 19000, lr = 0.0001
I0630 21:10:23.184610  5429 solver.cpp:290] Iteration 19100 (5.22937 iter/s, 19.1228s/100 iter), loss = 0.150419
I0630 21:10:23.184638  5429 solver.cpp:309]     Train net output #0: loss = 0.150418 (* 1 = 0.150418 loss)
I0630 21:10:23.184646  5429 sgd_solver.cpp:106] Iteration 19100, lr = 0.0001
I0630 21:10:42.244637  5429 solver.cpp:290] Iteration 19200 (5.24673 iter/s, 19.0595s/100 iter), loss = 0.0602512
I0630 21:10:42.244711  5429 solver.cpp:309]     Train net output #0: loss = 0.0602508 (* 1 = 0.0602508 loss)
I0630 21:10:42.244721  5429 sgd_solver.cpp:106] Iteration 19200, lr = 0.0001
I0630 21:11:01.597455  5429 solver.cpp:290] Iteration 19300 (5.16737 iter/s, 19.3522s/100 iter), loss = 0.126578
I0630 21:11:01.597479  5429 solver.cpp:309]     Train net output #0: loss = 0.126578 (* 1 = 0.126578 loss)
I0630 21:11:01.597486  5429 sgd_solver.cpp:106] Iteration 19300, lr = 0.0001
I0630 21:11:20.657999  5429 solver.cpp:290] Iteration 19400 (5.24659 iter/s, 19.06s/100 iter), loss = 0.102573
I0630 21:11:20.658046  5429 solver.cpp:309]     Train net output #0: loss = 0.102573 (* 1 = 0.102573 loss)
I0630 21:11:20.658053  5429 sgd_solver.cpp:106] Iteration 19400, lr = 0.0001
I0630 21:11:39.646646  5429 solver.cpp:290] Iteration 19500 (5.26646 iter/s, 18.9881s/100 iter), loss = 0.0783299
I0630 21:11:39.646672  5429 solver.cpp:309]     Train net output #0: loss = 0.0783295 (* 1 = 0.0783295 loss)
I0630 21:11:39.646682  5429 sgd_solver.cpp:106] Iteration 19500, lr = 0.0001
I0630 21:11:58.867343  5429 solver.cpp:290] Iteration 19600 (5.20288 iter/s, 19.2201s/100 iter), loss = 0.0756822
I0630 21:11:58.867403  5429 solver.cpp:309]     Train net output #0: loss = 0.0756818 (* 1 = 0.0756818 loss)
I0630 21:11:58.867413  5429 sgd_solver.cpp:106] Iteration 19600, lr = 0.0001
I0630 21:12:18.058748  5429 solver.cpp:290] Iteration 19700 (5.21082 iter/s, 19.1908s/100 iter), loss = 0.0587753
I0630 21:12:18.058769  5429 solver.cpp:309]     Train net output #0: loss = 0.0587749 (* 1 = 0.0587749 loss)
I0630 21:12:18.058776  5429 sgd_solver.cpp:106] Iteration 19700, lr = 0.0001
I0630 21:12:37.201058  5429 solver.cpp:290] Iteration 19800 (5.22418 iter/s, 19.1418s/100 iter), loss = 0.0797299
I0630 21:12:37.201129  5429 solver.cpp:309]     Train net output #0: loss = 0.0797296 (* 1 = 0.0797296 loss)
I0630 21:12:37.201138  5429 sgd_solver.cpp:106] Iteration 19800, lr = 0.0001
I0630 21:12:56.321316  5429 solver.cpp:290] Iteration 19900 (5.23022 iter/s, 19.1197s/100 iter), loss = 0.0787919
I0630 21:12:56.321342  5429 solver.cpp:309]     Train net output #0: loss = 0.0787915 (* 1 = 0.0787915 loss)
I0630 21:12:56.321349  5429 sgd_solver.cpp:106] Iteration 19900, lr = 0.0001
I0630 21:13:15.275111  5429 solver.cpp:598] Snapshotting to binary proto file training/cityscapes20_jsegnet21v2_2017-06-30_19-26-17/initial/cityscapes20_jsegnet21v2_iter_20000.caffemodel
I0630 21:13:15.348865  5429 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cityscapes20_jsegnet21v2_2017-06-30_19-26-17/initial/cityscapes20_jsegnet21v2_iter_20000.solverstate
I0630 21:13:15.369601  5429 solver.cpp:471] Iteration 20000, Testing net (#0)
I0630 21:14:48.981520  5429 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.932703
I0630 21:14:48.981600  5429 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.994701
I0630 21:14:48.981607  5429 solver.cpp:544]     Test net output #2: loss = 0.149304 (* 1 = 0.149304 loss)
I0630 21:14:49.193789  5429 solver.cpp:290] Iteration 20000 (0.88598 iter/s, 112.869s/100 iter), loss = 0.0781226
I0630 21:14:49.193815  5429 solver.cpp:309]     Train net output #0: loss = 0.0781222 (* 1 = 0.0781222 loss)
I0630 21:14:49.193825  5429 sgd_solver.cpp:106] Iteration 20000, lr = 0.0001
I0630 21:15:07.475752  5429 solver.cpp:290] Iteration 20100 (5.47003 iter/s, 18.2814s/100 iter), loss = 0.0698583
I0630 21:15:07.475778  5429 solver.cpp:309]     Train net output #0: loss = 0.0698579 (* 1 = 0.0698579 loss)
I0630 21:15:07.475785  5429 sgd_solver.cpp:106] Iteration 20100, lr = 0.0001
I0630 21:15:26.398960  5429 solver.cpp:290] Iteration 20200 (5.28467 iter/s, 18.9227s/100 iter), loss = 0.179333
I0630 21:15:26.399065  5429 solver.cpp:309]     Train net output #0: loss = 0.179333 (* 1 = 0.179333 loss)
I0630 21:15:26.399075  5429 sgd_solver.cpp:106] Iteration 20200, lr = 0.0001
I0630 21:15:45.440526  5429 solver.cpp:290] Iteration 20300 (5.25184 iter/s, 19.0409s/100 iter), loss = 0.0599853
I0630 21:15:45.440551  5429 solver.cpp:309]     Train net output #0: loss = 0.0599849 (* 1 = 0.0599849 loss)
I0630 21:15:45.440558  5429 sgd_solver.cpp:106] Iteration 20300, lr = 0.0001
I0630 21:16:04.718083  5429 solver.cpp:290] Iteration 20400 (5.18753 iter/s, 19.277s/100 iter), loss = 0.097101
I0630 21:16:04.718178  5429 solver.cpp:309]     Train net output #0: loss = 0.0971006 (* 1 = 0.0971006 loss)
I0630 21:16:04.718189  5429 sgd_solver.cpp:106] Iteration 20400, lr = 0.0001
I0630 21:16:23.705878  5429 solver.cpp:290] Iteration 20500 (5.26671 iter/s, 18.9872s/100 iter), loss = 0.0565909
I0630 21:16:23.705900  5429 solver.cpp:309]     Train net output #0: loss = 0.0565905 (* 1 = 0.0565905 loss)
I0630 21:16:23.705906  5429 sgd_solver.cpp:106] Iteration 20500, lr = 0.0001
I0630 21:16:43.021759  5429 solver.cpp:290] Iteration 20600 (5.17724 iter/s, 19.3153s/100 iter), loss = 0.101787
I0630 21:16:43.021811  5429 solver.cpp:309]     Train net output #0: loss = 0.101786 (* 1 = 0.101786 loss)
I0630 21:16:43.021821  5429 sgd_solver.cpp:106] Iteration 20600, lr = 0.0001
I0630 21:17:02.294733  5429 solver.cpp:290] Iteration 20700 (5.18877 iter/s, 19.2724s/100 iter), loss = 0.0896704
I0630 21:17:02.294759  5429 solver.cpp:309]     Train net output #0: loss = 0.08967 (* 1 = 0.08967 loss)
I0630 21:17:02.294767  5429 sgd_solver.cpp:106] Iteration 20700, lr = 0.0001
I0630 21:17:21.297395  5429 solver.cpp:290] Iteration 20800 (5.26257 iter/s, 19.0021s/100 iter), loss = 0.0852597
I0630 21:17:21.297459  5429 solver.cpp:309]     Train net output #0: loss = 0.0852593 (* 1 = 0.0852593 loss)
I0630 21:17:21.297467  5429 sgd_solver.cpp:106] Iteration 20800, lr = 0.0001
I0630 21:17:40.347623  5429 solver.cpp:290] Iteration 20900 (5.24944 iter/s, 19.0496s/100 iter), loss = 0.094642
I0630 21:17:40.347654  5429 solver.cpp:309]     Train net output #0: loss = 0.0946415 (* 1 = 0.0946415 loss)
I0630 21:17:40.347662  5429 sgd_solver.cpp:106] Iteration 20900, lr = 0.0001
I0630 21:17:59.483633  5429 solver.cpp:290] Iteration 21000 (5.2259 iter/s, 19.1355s/100 iter), loss = 0.132687
I0630 21:17:59.483687  5429 solver.cpp:309]     Train net output #0: loss = 0.132687 (* 1 = 0.132687 loss)
I0630 21:17:59.483695  5429 sgd_solver.cpp:106] Iteration 21000, lr = 0.0001
I0630 21:18:18.596251  5429 solver.cpp:290] Iteration 21100 (5.2323 iter/s, 19.112s/100 iter), loss = 0.118779
I0630 21:18:18.596274  5429 solver.cpp:309]     Train net output #0: loss = 0.118779 (* 1 = 0.118779 loss)
I0630 21:18:18.596283  5429 sgd_solver.cpp:106] Iteration 21100, lr = 0.0001
I0630 21:18:37.696333  5429 solver.cpp:290] Iteration 21200 (5.23573 iter/s, 19.0995s/100 iter), loss = 0.0883315
I0630 21:18:37.696441  5429 solver.cpp:309]     Train net output #0: loss = 0.088331 (* 1 = 0.088331 loss)
I0630 21:18:37.696451  5429 sgd_solver.cpp:106] Iteration 21200, lr = 0.0001
I0630 21:18:56.822224  5429 solver.cpp:290] Iteration 21300 (5.22869 iter/s, 19.1253s/100 iter), loss = 0.147963
I0630 21:18:56.822248  5429 solver.cpp:309]     Train net output #0: loss = 0.147963 (* 1 = 0.147963 loss)
I0630 21:18:56.822255  5429 sgd_solver.cpp:106] Iteration 21300, lr = 0.0001
I0630 21:19:16.109280  5429 solver.cpp:290] Iteration 21400 (5.18497 iter/s, 19.2865s/100 iter), loss = 0.107209
I0630 21:19:16.109354  5429 solver.cpp:309]     Train net output #0: loss = 0.107208 (* 1 = 0.107208 loss)
I0630 21:19:16.109362  5429 sgd_solver.cpp:106] Iteration 21400, lr = 0.0001
I0630 21:19:35.346940  5429 solver.cpp:290] Iteration 21500 (5.1983 iter/s, 19.2371s/100 iter), loss = 0.0778426
I0630 21:19:35.346963  5429 solver.cpp:309]     Train net output #0: loss = 0.0778422 (* 1 = 0.0778422 loss)
I0630 21:19:35.346971  5429 sgd_solver.cpp:106] Iteration 21500, lr = 0.0001
I0630 21:19:54.730202  5429 solver.cpp:290] Iteration 21600 (5.15924 iter/s, 19.3827s/100 iter), loss = 0.198641
I0630 21:19:54.730239  5429 solver.cpp:309]     Train net output #0: loss = 0.198641 (* 1 = 0.198641 loss)
I0630 21:19:54.730247  5429 sgd_solver.cpp:106] Iteration 21600, lr = 0.0001
I0630 21:20:13.804250  5429 solver.cpp:290] Iteration 21700 (5.24288 iter/s, 19.0735s/100 iter), loss = 0.0623842
I0630 21:20:13.804273  5429 solver.cpp:309]     Train net output #0: loss = 0.0623837 (* 1 = 0.0623837 loss)
I0630 21:20:13.804280  5429 sgd_solver.cpp:106] Iteration 21700, lr = 0.0001
I0630 21:20:32.937218  5429 solver.cpp:290] Iteration 21800 (5.22673 iter/s, 19.1324s/100 iter), loss = 0.0758716
I0630 21:20:32.937263  5429 solver.cpp:309]     Train net output #0: loss = 0.0758711 (* 1 = 0.0758711 loss)
I0630 21:20:32.937269  5429 sgd_solver.cpp:106] Iteration 21800, lr = 0.0001
I0630 21:20:52.078413  5429 solver.cpp:290] Iteration 21900 (5.22449 iter/s, 19.1406s/100 iter), loss = 0.031482
I0630 21:20:52.078480  5429 solver.cpp:309]     Train net output #0: loss = 0.0314815 (* 1 = 0.0314815 loss)
I0630 21:20:52.078505  5429 sgd_solver.cpp:106] Iteration 21900, lr = 0.0001
I0630 21:21:11.211187  5429 solver.cpp:471] Iteration 22000, Testing net (#0)
I0630 21:22:43.969454  5429 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.934486
I0630 21:22:43.969540  5429 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.994916
I0630 21:22:43.969549  5429 solver.cpp:544]     Test net output #2: loss = 0.14391 (* 1 = 0.14391 loss)
I0630 21:22:44.198990  5429 solver.cpp:290] Iteration 22000 (0.891921 iter/s, 112.118s/100 iter), loss = 0.0850116
I0630 21:22:44.199014  5429 solver.cpp:309]     Train net output #0: loss = 0.0850111 (* 1 = 0.0850111 loss)
I0630 21:22:44.199021  5429 sgd_solver.cpp:106] Iteration 22000, lr = 0.0001
I0630 21:23:02.602097  5429 solver.cpp:290] Iteration 22100 (5.43402 iter/s, 18.4026s/100 iter), loss = 0.088284
I0630 21:23:02.602123  5429 solver.cpp:309]     Train net output #0: loss = 0.0882835 (* 1 = 0.0882835 loss)
I0630 21:23:02.602131  5429 sgd_solver.cpp:106] Iteration 22100, lr = 0.0001
I0630 21:23:21.764163  5429 solver.cpp:290] Iteration 22200 (5.21879 iter/s, 19.1615s/100 iter), loss = 0.0859445
I0630 21:23:21.764225  5429 solver.cpp:309]     Train net output #0: loss = 0.085944 (* 1 = 0.085944 loss)
I0630 21:23:21.764233  5429 sgd_solver.cpp:106] Iteration 22200, lr = 0.0001
I0630 21:23:40.897013  5429 solver.cpp:290] Iteration 22300 (5.22677 iter/s, 19.1323s/100 iter), loss = 0.0884386
I0630 21:23:40.897035  5429 solver.cpp:309]     Train net output #0: loss = 0.0884381 (* 1 = 0.0884381 loss)
I0630 21:23:40.897042  5429 sgd_solver.cpp:106] Iteration 22300, lr = 0.0001
I0630 21:24:00.162318  5429 solver.cpp:290] Iteration 22400 (5.19082 iter/s, 19.2648s/100 iter), loss = 0.0822707
I0630 21:24:00.162410  5429 solver.cpp:309]     Train net output #0: loss = 0.0822702 (* 1 = 0.0822702 loss)
I0630 21:24:00.162420  5429 sgd_solver.cpp:106] Iteration 22400, lr = 0.0001
I0630 21:24:19.301054  5429 solver.cpp:290] Iteration 22500 (5.22517 iter/s, 19.1381s/100 iter), loss = 0.0628854
I0630 21:24:19.301076  5429 solver.cpp:309]     Train net output #0: loss = 0.0628849 (* 1 = 0.0628849 loss)
I0630 21:24:19.301084  5429 sgd_solver.cpp:106] Iteration 22500, lr = 0.0001
I0630 21:24:38.417419  5429 solver.cpp:290] Iteration 22600 (5.23127 iter/s, 19.1158s/100 iter), loss = 0.156419
I0630 21:24:38.417487  5429 solver.cpp:309]     Train net output #0: loss = 0.156418 (* 1 = 0.156418 loss)
I0630 21:24:38.417500  5429 sgd_solver.cpp:106] Iteration 22600, lr = 0.0001
I0630 21:24:57.727635  5429 solver.cpp:290] Iteration 22700 (5.17876 iter/s, 19.3096s/100 iter), loss = 0.070628
I0630 21:24:57.727658  5429 solver.cpp:309]     Train net output #0: loss = 0.0706275 (* 1 = 0.0706275 loss)
I0630 21:24:57.727665  5429 sgd_solver.cpp:106] Iteration 22700, lr = 0.0001
I0630 21:25:16.667598  5429 solver.cpp:290] Iteration 22800 (5.27999 iter/s, 18.9394s/100 iter), loss = 0.12594
I0630 21:25:16.667686  5429 solver.cpp:309]     Train net output #0: loss = 0.125939 (* 1 = 0.125939 loss)
I0630 21:25:16.667696  5429 sgd_solver.cpp:106] Iteration 22800, lr = 0.0001
I0630 21:25:35.820523  5429 solver.cpp:290] Iteration 22900 (5.2213 iter/s, 19.1523s/100 iter), loss = 0.0793839
I0630 21:25:35.820545  5429 solver.cpp:309]     Train net output #0: loss = 0.0793835 (* 1 = 0.0793835 loss)
I0630 21:25:35.820551  5429 sgd_solver.cpp:106] Iteration 22900, lr = 0.0001
I0630 21:25:54.925784  5429 solver.cpp:290] Iteration 23000 (5.23431 iter/s, 19.1047s/100 iter), loss = 0.0924845
I0630 21:25:54.925899  5429 solver.cpp:309]     Train net output #0: loss = 0.092484 (* 1 = 0.092484 loss)
I0630 21:25:54.925905  5429 sgd_solver.cpp:106] Iteration 23000, lr = 0.0001
I0630 21:26:14.111289  5429 solver.cpp:290] Iteration 23100 (5.21244 iter/s, 19.1849s/100 iter), loss = 0.0797142
I0630 21:26:14.111315  5429 solver.cpp:309]     Train net output #0: loss = 0.0797138 (* 1 = 0.0797138 loss)
I0630 21:26:14.111321  5429 sgd_solver.cpp:106] Iteration 23100, lr = 0.0001
I0630 21:26:33.278023  5429 solver.cpp:290] Iteration 23200 (5.21752 iter/s, 19.1662s/100 iter), loss = 0.0927219
I0630 21:26:33.278074  5429 solver.cpp:309]     Train net output #0: loss = 0.0927215 (* 1 = 0.0927215 loss)
I0630 21:26:33.278082  5429 sgd_solver.cpp:106] Iteration 23200, lr = 0.0001
I0630 21:26:52.478584  5429 solver.cpp:290] Iteration 23300 (5.20833 iter/s, 19.2s/100 iter), loss = 0.0898942
I0630 21:26:52.478607  5429 solver.cpp:309]     Train net output #0: loss = 0.0898937 (* 1 = 0.0898937 loss)
I0630 21:26:52.478615  5429 sgd_solver.cpp:106] Iteration 23300, lr = 0.0001
I0630 21:27:11.724040  5429 solver.cpp:290] Iteration 23400 (5.19618 iter/s, 19.2449s/100 iter), loss = 0.0598528
I0630 21:27:11.724124  5429 solver.cpp:309]     Train net output #0: loss = 0.0598523 (* 1 = 0.0598523 loss)
I0630 21:27:11.724135  5429 sgd_solver.cpp:106] Iteration 23400, lr = 0.0001
I0630 21:27:30.721218  5429 solver.cpp:290] Iteration 23500 (5.2641 iter/s, 18.9966s/100 iter), loss = 0.105229
I0630 21:27:30.721240  5429 solver.cpp:309]     Train net output #0: loss = 0.105228 (* 1 = 0.105228 loss)
I0630 21:27:30.721247  5429 sgd_solver.cpp:106] Iteration 23500, lr = 0.0001
I0630 21:27:49.690990  5429 solver.cpp:290] Iteration 23600 (5.27169 iter/s, 18.9692s/100 iter), loss = 0.127416
I0630 21:27:49.691036  5429 solver.cpp:309]     Train net output #0: loss = 0.127415 (* 1 = 0.127415 loss)
I0630 21:27:49.691045  5429 sgd_solver.cpp:106] Iteration 23600, lr = 0.0001
I0630 21:28:08.877295  5429 solver.cpp:290] Iteration 23700 (5.2122 iter/s, 19.1857s/100 iter), loss = 0.0560074
I0630 21:28:08.877318  5429 solver.cpp:309]     Train net output #0: loss = 0.0560069 (* 1 = 0.0560069 loss)
I0630 21:28:08.877326  5429 sgd_solver.cpp:106] Iteration 23700, lr = 0.0001
I0630 21:28:27.906369  5429 solver.cpp:290] Iteration 23800 (5.25526 iter/s, 19.0285s/100 iter), loss = 0.0920871
I0630 21:28:27.906443  5429 solver.cpp:309]     Train net output #0: loss = 0.0920866 (* 1 = 0.0920866 loss)
I0630 21:28:27.906451  5429 sgd_solver.cpp:106] Iteration 23800, lr = 0.0001
I0630 21:28:46.934528  5429 solver.cpp:290] Iteration 23900 (5.25553 iter/s, 19.0276s/100 iter), loss = 0.0790047
I0630 21:28:46.934556  5429 solver.cpp:309]     Train net output #0: loss = 0.0790042 (* 1 = 0.0790042 loss)
I0630 21:28:46.934564  5429 sgd_solver.cpp:106] Iteration 23900, lr = 0.0001
I0630 21:29:05.779527  5429 solver.cpp:471] Iteration 24000, Testing net (#0)
I0630 21:30:38.783382  5429 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.934933
I0630 21:30:38.783486  5429 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.995818
I0630 21:30:38.783493  5429 solver.cpp:544]     Test net output #2: loss = 0.12954 (* 1 = 0.12954 loss)
I0630 21:30:38.988459  5429 solver.cpp:290] Iteration 24000 (0.892451 iter/s, 112.051s/100 iter), loss = 0.059087
I0630 21:30:38.988483  5429 solver.cpp:309]     Train net output #0: loss = 0.0590865 (* 1 = 0.0590865 loss)
I0630 21:30:38.988490  5713 sgd_solver.cpp:46] MultiStep Status: Iteration 24000, step = 1
I0630 21:30:38.988494  5712 sgd_solver.cpp:46] MultiStep Status: Iteration 24000, step = 1
I0630 21:30:38.988490  5429 sgd_solver.cpp:46] MultiStep Status: Iteration 24000, step = 1
I0630 21:30:38.988507  5429 sgd_solver.cpp:106] Iteration 24000, lr = 1e-05
I0630 21:30:57.418548  5429 solver.cpp:290] Iteration 24100 (5.42606 iter/s, 18.4296s/100 iter), loss = 0.0799452
I0630 21:30:57.418570  5429 solver.cpp:309]     Train net output #0: loss = 0.0799447 (* 1 = 0.0799447 loss)
I0630 21:30:57.418577  5429 sgd_solver.cpp:106] Iteration 24100, lr = 1e-05
I0630 21:31:16.388422  5429 solver.cpp:290] Iteration 24200 (5.27166 iter/s, 18.9693s/100 iter), loss = 0.0971637
I0630 21:31:16.388496  5429 solver.cpp:309]     Train net output #0: loss = 0.0971632 (* 1 = 0.0971632 loss)
I0630 21:31:16.388505  5429 sgd_solver.cpp:106] Iteration 24200, lr = 1e-05
I0630 21:31:35.417790  5429 solver.cpp:290] Iteration 24300 (5.2552 iter/s, 19.0288s/100 iter), loss = 0.0529078
I0630 21:31:35.417814  5429 solver.cpp:309]     Train net output #0: loss = 0.0529073 (* 1 = 0.0529073 loss)
I0630 21:31:35.417820  5429 sgd_solver.cpp:106] Iteration 24300, lr = 1e-05
I0630 21:31:54.598496  5429 solver.cpp:290] Iteration 24400 (5.21372 iter/s, 19.1802s/100 iter), loss = 0.0726764
I0630 21:31:54.598552  5429 solver.cpp:309]     Train net output #0: loss = 0.0726759 (* 1 = 0.0726759 loss)
I0630 21:31:54.598561  5429 sgd_solver.cpp:106] Iteration 24400, lr = 1e-05
I0630 21:32:13.784790  5429 solver.cpp:290] Iteration 24500 (5.21221 iter/s, 19.1857s/100 iter), loss = 0.0760292
I0630 21:32:13.784812  5429 solver.cpp:309]     Train net output #0: loss = 0.0760287 (* 1 = 0.0760287 loss)
I0630 21:32:13.784819  5429 sgd_solver.cpp:106] Iteration 24500, lr = 1e-05
I0630 21:32:32.828572  5429 solver.cpp:290] Iteration 24600 (5.2512 iter/s, 19.0433s/100 iter), loss = 0.064486
I0630 21:32:32.828639  5429 solver.cpp:309]     Train net output #0: loss = 0.0644855 (* 1 = 0.0644855 loss)
I0630 21:32:32.828646  5429 sgd_solver.cpp:106] Iteration 24600, lr = 1e-05
I0630 21:32:51.693374  5429 solver.cpp:290] Iteration 24700 (5.30104 iter/s, 18.8642s/100 iter), loss = 0.111931
I0630 21:32:51.693397  5429 solver.cpp:309]     Train net output #0: loss = 0.111931 (* 1 = 0.111931 loss)
I0630 21:32:51.693404  5429 sgd_solver.cpp:106] Iteration 24700, lr = 1e-05
I0630 21:33:10.616140  5429 solver.cpp:290] Iteration 24800 (5.28479 iter/s, 18.9222s/100 iter), loss = 0.0375395
I0630 21:33:10.616183  5429 solver.cpp:309]     Train net output #0: loss = 0.037539 (* 1 = 0.037539 loss)
I0630 21:33:10.616189  5429 sgd_solver.cpp:106] Iteration 24800, lr = 1e-05
I0630 21:33:29.892663  5429 solver.cpp:290] Iteration 24900 (5.18781 iter/s, 19.276s/100 iter), loss = 0.0635059
I0630 21:33:29.892688  5429 solver.cpp:309]     Train net output #0: loss = 0.0635054 (* 1 = 0.0635054 loss)
I0630 21:33:29.892695  5429 sgd_solver.cpp:106] Iteration 24900, lr = 1e-05
I0630 21:33:49.280347  5429 solver.cpp:290] Iteration 25000 (5.15806 iter/s, 19.3871s/100 iter), loss = 0.107219
I0630 21:33:49.280412  5429 solver.cpp:309]     Train net output #0: loss = 0.107219 (* 1 = 0.107219 loss)
I0630 21:33:49.280423  5429 sgd_solver.cpp:106] Iteration 25000, lr = 1e-05
I0630 21:34:08.479781  5429 solver.cpp:290] Iteration 25100 (5.20864 iter/s, 19.1989s/100 iter), loss = 0.0658536
I0630 21:34:08.479807  5429 solver.cpp:309]     Train net output #0: loss = 0.0658531 (* 1 = 0.0658531 loss)
I0630 21:34:08.479816  5429 sgd_solver.cpp:106] Iteration 25100, lr = 1e-05
I0630 21:34:27.466636  5429 solver.cpp:290] Iteration 25200 (5.26695 iter/s, 18.9863s/100 iter), loss = 0.0849872
I0630 21:34:27.466694  5429 solver.cpp:309]     Train net output #0: loss = 0.0849866 (* 1 = 0.0849866 loss)
I0630 21:34:27.466702  5429 sgd_solver.cpp:106] Iteration 25200, lr = 1e-05
I0630 21:34:46.510463  5429 solver.cpp:290] Iteration 25300 (5.2512 iter/s, 19.0433s/100 iter), loss = 0.0630214
I0630 21:34:46.510486  5429 solver.cpp:309]     Train net output #0: loss = 0.0630208 (* 1 = 0.0630208 loss)
I0630 21:34:46.510494  5429 sgd_solver.cpp:106] Iteration 25300, lr = 1e-05
I0630 21:35:05.352573  5429 solver.cpp:290] Iteration 25400 (5.30741 iter/s, 18.8416s/100 iter), loss = 0.112971
I0630 21:35:05.352624  5429 solver.cpp:309]     Train net output #0: loss = 0.112971 (* 1 = 0.112971 loss)
I0630 21:35:05.352632  5429 sgd_solver.cpp:106] Iteration 25400, lr = 1e-05
I0630 21:35:24.536056  5429 solver.cpp:290] Iteration 25500 (5.21297 iter/s, 19.1829s/100 iter), loss = 0.100422
I0630 21:35:24.536080  5429 solver.cpp:309]     Train net output #0: loss = 0.100421 (* 1 = 0.100421 loss)
I0630 21:35:24.536087  5429 sgd_solver.cpp:106] Iteration 25500, lr = 1e-05
I0630 21:35:43.528214  5429 solver.cpp:290] Iteration 25600 (5.26548 iter/s, 18.9916s/100 iter), loss = 0.121658
I0630 21:35:43.528259  5429 solver.cpp:309]     Train net output #0: loss = 0.121657 (* 1 = 0.121657 loss)
I0630 21:35:43.528267  5429 sgd_solver.cpp:106] Iteration 25600, lr = 1e-05
I0630 21:36:02.634124  5429 solver.cpp:290] Iteration 25700 (5.23413 iter/s, 19.1054s/100 iter), loss = 0.0723484
I0630 21:36:02.634150  5429 solver.cpp:309]     Train net output #0: loss = 0.0723479 (* 1 = 0.0723479 loss)
I0630 21:36:02.634157  5429 sgd_solver.cpp:106] Iteration 25700, lr = 1e-05
I0630 21:36:21.767534  5429 solver.cpp:290] Iteration 25800 (5.2266 iter/s, 19.1329s/100 iter), loss = 0.0656031
I0630 21:36:21.767576  5429 solver.cpp:309]     Train net output #0: loss = 0.0656026 (* 1 = 0.0656026 loss)
I0630 21:36:21.767585  5429 sgd_solver.cpp:106] Iteration 25800, lr = 1e-05
I0630 21:36:40.761397  5429 solver.cpp:290] Iteration 25900 (5.26502 iter/s, 18.9933s/100 iter), loss = 0.0544191
I0630 21:36:40.761421  5429 solver.cpp:309]     Train net output #0: loss = 0.0544186 (* 1 = 0.0544186 loss)
I0630 21:36:40.761427  5429 sgd_solver.cpp:106] Iteration 25900, lr = 1e-05
I0630 21:36:59.733942  5429 solver.cpp:471] Iteration 26000, Testing net (#0)
I0630 21:38:32.759531  5429 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.93871
I0630 21:38:32.759609  5429 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.995902
I0630 21:38:32.759616  5429 solver.cpp:544]     Test net output #2: loss = 0.127114 (* 1 = 0.127114 loss)
I0630 21:38:32.972810  5429 solver.cpp:290] Iteration 26000 (0.8912 iter/s, 112.208s/100 iter), loss = 0.0745138
I0630 21:38:32.972833  5429 solver.cpp:309]     Train net output #0: loss = 0.0745133 (* 1 = 0.0745133 loss)
I0630 21:38:32.972841  5429 sgd_solver.cpp:106] Iteration 26000, lr = 1e-05
I0630 21:38:51.246098  5429 solver.cpp:290] Iteration 26100 (5.47263 iter/s, 18.2727s/100 iter), loss = 0.0783837
I0630 21:38:51.246124  5429 solver.cpp:309]     Train net output #0: loss = 0.0783832 (* 1 = 0.0783832 loss)
I0630 21:38:51.246132  5429 sgd_solver.cpp:106] Iteration 26100, lr = 1e-05
I0630 21:39:10.389374  5429 solver.cpp:290] Iteration 26200 (5.22392 iter/s, 19.1427s/100 iter), loss = 0.0408745
I0630 21:39:10.389461  5429 solver.cpp:309]     Train net output #0: loss = 0.040874 (* 1 = 0.040874 loss)
I0630 21:39:10.389472  5429 sgd_solver.cpp:106] Iteration 26200, lr = 1e-05
I0630 21:39:29.312986  5429 solver.cpp:290] Iteration 26300 (5.28458 iter/s, 18.923s/100 iter), loss = 0.0651741
I0630 21:39:29.313010  5429 solver.cpp:309]     Train net output #0: loss = 0.0651735 (* 1 = 0.0651735 loss)
I0630 21:39:29.313017  5429 sgd_solver.cpp:106] Iteration 26300, lr = 1e-05
I0630 21:39:48.400732  5429 solver.cpp:290] Iteration 26400 (5.23912 iter/s, 19.0872s/100 iter), loss = 0.0987975
I0630 21:39:48.400810  5429 solver.cpp:309]     Train net output #0: loss = 0.098797 (* 1 = 0.098797 loss)
I0630 21:39:48.400817  5429 sgd_solver.cpp:106] Iteration 26400, lr = 1e-05
I0630 21:40:07.409464  5429 solver.cpp:290] Iteration 26500 (5.26091 iter/s, 19.0081s/100 iter), loss = 0.0632124
I0630 21:40:07.409488  5429 solver.cpp:309]     Train net output #0: loss = 0.0632119 (* 1 = 0.0632119 loss)
I0630 21:40:07.409495  5429 sgd_solver.cpp:106] Iteration 26500, lr = 1e-05
I0630 21:40:26.774796  5429 solver.cpp:290] Iteration 26600 (5.16402 iter/s, 19.3648s/100 iter), loss = 0.120369
I0630 21:40:26.774842  5429 solver.cpp:309]     Train net output #0: loss = 0.120368 (* 1 = 0.120368 loss)
I0630 21:40:26.774849  5429 sgd_solver.cpp:106] Iteration 26600, lr = 1e-05
I0630 21:40:45.859048  5429 solver.cpp:290] Iteration 26700 (5.24008 iter/s, 19.0837s/100 iter), loss = 0.0711406
I0630 21:40:45.859072  5429 solver.cpp:309]     Train net output #0: loss = 0.0711401 (* 1 = 0.0711401 loss)
I0630 21:40:45.859079  5429 sgd_solver.cpp:106] Iteration 26700, lr = 1e-05
I0630 21:41:05.186846  5429 solver.cpp:290] Iteration 26800 (5.17405 iter/s, 19.3272s/100 iter), loss = 0.0877709
I0630 21:41:05.186928  5429 solver.cpp:309]     Train net output #0: loss = 0.0877704 (* 1 = 0.0877704 loss)
I0630 21:41:05.186939  5429 sgd_solver.cpp:106] Iteration 26800, lr = 1e-05
I0630 21:41:24.365043  5429 solver.cpp:290] Iteration 26900 (5.21442 iter/s, 19.1776s/100 iter), loss = 0.0935941
I0630 21:41:24.365067  5429 solver.cpp:309]     Train net output #0: loss = 0.0935936 (* 1 = 0.0935936 loss)
I0630 21:41:24.365073  5429 sgd_solver.cpp:106] Iteration 26900, lr = 1e-05
I0630 21:41:43.375725  5429 solver.cpp:290] Iteration 27000 (5.26036 iter/s, 19.0101s/100 iter), loss = 0.0762573
I0630 21:41:43.375854  5429 solver.cpp:309]     Train net output #0: loss = 0.0762568 (* 1 = 0.0762568 loss)
I0630 21:41:43.375864  5429 sgd_solver.cpp:106] Iteration 27000, lr = 1e-05
I0630 21:42:02.650494  5429 solver.cpp:290] Iteration 27100 (5.18831 iter/s, 19.2741s/100 iter), loss = 0.0445008
I0630 21:42:02.650516  5429 solver.cpp:309]     Train net output #0: loss = 0.0445003 (* 1 = 0.0445003 loss)
I0630 21:42:02.650523  5429 sgd_solver.cpp:106] Iteration 27100, lr = 1e-05
I0630 21:42:21.857062  5429 solver.cpp:290] Iteration 27200 (5.20671 iter/s, 19.206s/100 iter), loss = 0.0838681
I0630 21:42:21.857151  5429 solver.cpp:309]     Train net output #0: loss = 0.0838676 (* 1 = 0.0838676 loss)
I0630 21:42:21.857161  5429 sgd_solver.cpp:106] Iteration 27200, lr = 1e-05
I0630 21:42:40.804252  5429 solver.cpp:290] Iteration 27300 (5.278 iter/s, 18.9466s/100 iter), loss = 0.0828029
I0630 21:42:40.804275  5429 solver.cpp:309]     Train net output #0: loss = 0.0828024 (* 1 = 0.0828024 loss)
I0630 21:42:40.804282  5429 sgd_solver.cpp:106] Iteration 27300, lr = 1e-05
I0630 21:42:59.988241  5429 solver.cpp:290] Iteration 27400 (5.21283 iter/s, 19.1834s/100 iter), loss = 0.0435778
I0630 21:42:59.988286  5429 solver.cpp:309]     Train net output #0: loss = 0.0435773 (* 1 = 0.0435773 loss)
I0630 21:42:59.988294  5429 sgd_solver.cpp:106] Iteration 27400, lr = 1e-05
I0630 21:43:19.181269  5429 solver.cpp:290] Iteration 27500 (5.21038 iter/s, 19.1924s/100 iter), loss = 0.0604317
I0630 21:43:19.181294  5429 solver.cpp:309]     Train net output #0: loss = 0.0604312 (* 1 = 0.0604312 loss)
I0630 21:43:19.181300  5429 sgd_solver.cpp:106] Iteration 27500, lr = 1e-05
I0630 21:43:38.234700  5429 solver.cpp:290] Iteration 27600 (5.24855 iter/s, 19.0529s/100 iter), loss = 0.0519092
I0630 21:43:38.234750  5429 solver.cpp:309]     Train net output #0: loss = 0.0519087 (* 1 = 0.0519087 loss)
I0630 21:43:38.234757  5429 sgd_solver.cpp:106] Iteration 27600, lr = 1e-05
I0630 21:43:57.269340  5429 solver.cpp:290] Iteration 27700 (5.25374 iter/s, 19.0341s/100 iter), loss = 0.0753686
I0630 21:43:57.269366  5429 solver.cpp:309]     Train net output #0: loss = 0.0753681 (* 1 = 0.0753681 loss)
I0630 21:43:57.269376  5429 sgd_solver.cpp:106] Iteration 27700, lr = 1e-05
I0630 21:44:16.048914  5429 solver.cpp:290] Iteration 27800 (5.32509 iter/s, 18.779s/100 iter), loss = 0.0570071
I0630 21:44:16.048965  5429 solver.cpp:309]     Train net output #0: loss = 0.0570066 (* 1 = 0.0570066 loss)
I0630 21:44:16.048974  5429 sgd_solver.cpp:106] Iteration 27800, lr = 1e-05
I0630 21:44:35.230754  5429 solver.cpp:290] Iteration 27900 (5.21342 iter/s, 19.1812s/100 iter), loss = 0.0790409
I0630 21:44:35.230777  5429 solver.cpp:309]     Train net output #0: loss = 0.0790404 (* 1 = 0.0790404 loss)
I0630 21:44:35.230784  5429 sgd_solver.cpp:106] Iteration 27900, lr = 1e-05
I0630 21:44:54.082053  5429 solver.cpp:471] Iteration 28000, Testing net (#0)
I0630 21:46:27.078949  5429 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.93921
I0630 21:46:27.078989  5429 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.995771
I0630 21:46:27.078994  5429 solver.cpp:544]     Test net output #2: loss = 0.127924 (* 1 = 0.127924 loss)
I0630 21:46:27.298524  5429 solver.cpp:290] Iteration 28000 (0.892342 iter/s, 112.065s/100 iter), loss = 0.0631419
I0630 21:46:27.298547  5429 solver.cpp:309]     Train net output #0: loss = 0.0631414 (* 1 = 0.0631414 loss)
I0630 21:46:27.298553  5429 sgd_solver.cpp:106] Iteration 28000, lr = 1e-05
I0630 21:46:46.495591  5429 solver.cpp:290] Iteration 28100 (5.20928 iter/s, 19.1965s/100 iter), loss = 0.0701565
I0630 21:46:46.495615  5429 solver.cpp:309]     Train net output #0: loss = 0.070156 (* 1 = 0.070156 loss)
I0630 21:46:46.495621  5429 sgd_solver.cpp:106] Iteration 28100, lr = 1e-05
I0630 21:47:05.706905  5429 solver.cpp:290] Iteration 28200 (5.20542 iter/s, 19.2107s/100 iter), loss = 0.080405
I0630 21:47:05.706950  5429 solver.cpp:309]     Train net output #0: loss = 0.0804045 (* 1 = 0.0804045 loss)
I0630 21:47:05.706959  5429 sgd_solver.cpp:106] Iteration 28200, lr = 1e-05
I0630 21:47:24.771267  5429 solver.cpp:290] Iteration 28300 (5.24555 iter/s, 19.0638s/100 iter), loss = 0.0940452
I0630 21:47:24.771291  5429 solver.cpp:309]     Train net output #0: loss = 0.0940447 (* 1 = 0.0940447 loss)
I0630 21:47:24.771297  5429 sgd_solver.cpp:106] Iteration 28300, lr = 1e-05
I0630 21:47:43.864105  5429 solver.cpp:290] Iteration 28400 (5.23772 iter/s, 19.0923s/100 iter), loss = 0.0909215
I0630 21:47:43.864188  5429 solver.cpp:309]     Train net output #0: loss = 0.090921 (* 1 = 0.090921 loss)
I0630 21:47:43.864200  5429 sgd_solver.cpp:106] Iteration 28400, lr = 1e-05
I0630 21:48:02.923054  5429 solver.cpp:290] Iteration 28500 (5.24705 iter/s, 19.0583s/100 iter), loss = 0.0528347
I0630 21:48:02.923079  5429 solver.cpp:309]     Train net output #0: loss = 0.0528342 (* 1 = 0.0528342 loss)
I0630 21:48:02.923085  5429 sgd_solver.cpp:106] Iteration 28500, lr = 1e-05
I0630 21:48:22.033093  5429 solver.cpp:290] Iteration 28600 (5.23301 iter/s, 19.1095s/100 iter), loss = 0.0914814
I0630 21:48:22.033150  5429 solver.cpp:309]     Train net output #0: loss = 0.0914809 (* 1 = 0.0914809 loss)
I0630 21:48:22.033159  5429 sgd_solver.cpp:106] Iteration 28600, lr = 1e-05
I0630 21:48:41.161713  5429 solver.cpp:290] Iteration 28700 (5.22793 iter/s, 19.128s/100 iter), loss = 0.0458436
I0630 21:48:41.161742  5429 solver.cpp:309]     Train net output #0: loss = 0.0458431 (* 1 = 0.0458431 loss)
I0630 21:48:41.161752  5429 sgd_solver.cpp:106] Iteration 28700, lr = 1e-05
I0630 21:49:00.460749  5429 solver.cpp:290] Iteration 28800 (5.18176 iter/s, 19.2985s/100 iter), loss = 0.0860932
I0630 21:49:00.460795  5429 solver.cpp:309]     Train net output #0: loss = 0.0860927 (* 1 = 0.0860927 loss)
I0630 21:49:00.460804  5429 sgd_solver.cpp:106] Iteration 28800, lr = 1e-05
I0630 21:49:19.471457  5429 solver.cpp:290] Iteration 28900 (5.26036 iter/s, 19.0101s/100 iter), loss = 0.125325
I0630 21:49:19.471482  5429 solver.cpp:309]     Train net output #0: loss = 0.125325 (* 1 = 0.125325 loss)
I0630 21:49:19.471491  5429 sgd_solver.cpp:106] Iteration 28900, lr = 1e-05
I0630 21:49:38.457309  5429 solver.cpp:290] Iteration 29000 (5.26724 iter/s, 18.9853s/100 iter), loss = 0.0818155
I0630 21:49:38.457363  5429 solver.cpp:309]     Train net output #0: loss = 0.081815 (* 1 = 0.081815 loss)
I0630 21:49:38.457371  5429 sgd_solver.cpp:106] Iteration 29000, lr = 1e-05
I0630 21:49:57.596325  5429 solver.cpp:290] Iteration 29100 (5.22509 iter/s, 19.1384s/100 iter), loss = 0.0562806
I0630 21:49:57.596350  5429 solver.cpp:309]     Train net output #0: loss = 0.0562802 (* 1 = 0.0562802 loss)
I0630 21:49:57.596359  5429 sgd_solver.cpp:106] Iteration 29100, lr = 1e-05
I0630 21:50:16.839902  5429 solver.cpp:290] Iteration 29200 (5.19669 iter/s, 19.243s/100 iter), loss = 0.065218
I0630 21:50:16.840020  5429 solver.cpp:309]     Train net output #0: loss = 0.0652176 (* 1 = 0.0652176 loss)
I0630 21:50:16.840030  5429 sgd_solver.cpp:106] Iteration 29200, lr = 1e-05
I0630 21:50:35.895654  5429 solver.cpp:290] Iteration 29300 (5.24794 iter/s, 19.0551s/100 iter), loss = 0.0410787
I0630 21:50:35.895678  5429 solver.cpp:309]     Train net output #0: loss = 0.0410783 (* 1 = 0.0410783 loss)
I0630 21:50:35.895684  5429 sgd_solver.cpp:106] Iteration 29300, lr = 1e-05
I0630 21:50:54.870517  5429 solver.cpp:290] Iteration 29400 (5.27029 iter/s, 18.9743s/100 iter), loss = 0.058831
I0630 21:50:54.870626  5429 solver.cpp:309]     Train net output #0: loss = 0.0588305 (* 1 = 0.0588305 loss)
I0630 21:50:54.870636  5429 sgd_solver.cpp:106] Iteration 29400, lr = 1e-05
I0630 21:51:13.869984  5429 solver.cpp:290] Iteration 29500 (5.26348 iter/s, 18.9988s/100 iter), loss = 0.0466017
I0630 21:51:13.870009  5429 solver.cpp:309]     Train net output #0: loss = 0.0466012 (* 1 = 0.0466012 loss)
I0630 21:51:13.870016  5429 sgd_solver.cpp:106] Iteration 29500, lr = 1e-05
I0630 21:51:33.023686  5429 solver.cpp:290] Iteration 29600 (5.22108 iter/s, 19.1531s/100 iter), loss = 0.120055
I0630 21:51:33.023777  5429 solver.cpp:309]     Train net output #0: loss = 0.120054 (* 1 = 0.120054 loss)
I0630 21:51:33.023792  5429 sgd_solver.cpp:106] Iteration 29600, lr = 1e-05
I0630 21:51:52.064779  5429 solver.cpp:290] Iteration 29700 (5.25197 iter/s, 19.0405s/100 iter), loss = 0.110836
I0630 21:51:52.064800  5429 solver.cpp:309]     Train net output #0: loss = 0.110836 (* 1 = 0.110836 loss)
I0630 21:51:52.064808  5429 sgd_solver.cpp:106] Iteration 29700, lr = 1e-05
I0630 21:52:11.335633  5429 solver.cpp:290] Iteration 29800 (5.18934 iter/s, 19.2703s/100 iter), loss = 0.0871948
I0630 21:52:11.335690  5429 solver.cpp:309]     Train net output #0: loss = 0.0871944 (* 1 = 0.0871944 loss)
I0630 21:52:11.335700  5429 sgd_solver.cpp:106] Iteration 29800, lr = 1e-05
I0630 21:52:30.449986  5429 solver.cpp:290] Iteration 29900 (5.23183 iter/s, 19.1138s/100 iter), loss = 0.0763954
I0630 21:52:30.450014  5429 solver.cpp:309]     Train net output #0: loss = 0.076395 (* 1 = 0.076395 loss)
I0630 21:52:30.450021  5429 sgd_solver.cpp:106] Iteration 29900, lr = 1e-05
I0630 21:52:49.249289  5429 solver.cpp:598] Snapshotting to binary proto file training/cityscapes20_jsegnet21v2_2017-06-30_19-26-17/initial/cityscapes20_jsegnet21v2_iter_30000.caffemodel
I0630 21:52:49.274492  5429 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cityscapes20_jsegnet21v2_2017-06-30_19-26-17/initial/cityscapes20_jsegnet21v2_iter_30000.solverstate
I0630 21:52:49.295944  5429 solver.cpp:471] Iteration 30000, Testing net (#0)
I0630 21:54:22.752815  5429 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.939235
I0630 21:54:22.752889  5429 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.996004
I0630 21:54:22.752897  5429 solver.cpp:544]     Test net output #2: loss = 0.128261 (* 1 = 0.128261 loss)
I0630 21:54:22.994814  5429 solver.cpp:290] Iteration 30000 (0.88856 iter/s, 112.542s/100 iter), loss = 0.0817671
I0630 21:54:22.994839  5429 solver.cpp:309]     Train net output #0: loss = 0.0817667 (* 1 = 0.0817667 loss)
I0630 21:54:22.994848  5429 sgd_solver.cpp:106] Iteration 30000, lr = 1e-05
I0630 21:54:42.111484  5429 solver.cpp:290] Iteration 30100 (5.23119 iter/s, 19.1161s/100 iter), loss = 0.0534849
I0630 21:54:42.111505  5429 solver.cpp:309]     Train net output #0: loss = 0.0534845 (* 1 = 0.0534845 loss)
I0630 21:54:42.111512  5429 sgd_solver.cpp:106] Iteration 30100, lr = 1e-05
I0630 21:55:01.199739  5429 solver.cpp:290] Iteration 30200 (5.23898 iter/s, 19.0877s/100 iter), loss = 0.067205
I0630 21:55:01.199780  5429 solver.cpp:309]     Train net output #0: loss = 0.0672046 (* 1 = 0.0672046 loss)
I0630 21:55:01.199789  5429 sgd_solver.cpp:106] Iteration 30200, lr = 1e-05
I0630 21:55:20.340976  5429 solver.cpp:290] Iteration 30300 (5.22448 iter/s, 19.1407s/100 iter), loss = 0.0900274
I0630 21:55:20.341002  5429 solver.cpp:309]     Train net output #0: loss = 0.090027 (* 1 = 0.090027 loss)
I0630 21:55:20.341012  5429 sgd_solver.cpp:106] Iteration 30300, lr = 1e-05
I0630 21:55:39.439327  5429 solver.cpp:290] Iteration 30400 (5.23621 iter/s, 19.0978s/100 iter), loss = 0.0659831
I0630 21:55:39.439370  5429 solver.cpp:309]     Train net output #0: loss = 0.0659827 (* 1 = 0.0659827 loss)
I0630 21:55:39.439378  5429 sgd_solver.cpp:106] Iteration 30400, lr = 1e-05
I0630 21:55:58.598587  5429 solver.cpp:290] Iteration 30500 (5.21956 iter/s, 19.1587s/100 iter), loss = 0.0868088
I0630 21:55:58.598611  5429 solver.cpp:309]     Train net output #0: loss = 0.0868084 (* 1 = 0.0868084 loss)
I0630 21:55:58.598618  5429 sgd_solver.cpp:106] Iteration 30500, lr = 1e-05
I0630 21:56:17.640051  5429 solver.cpp:290] Iteration 30600 (5.25185 iter/s, 19.0409s/100 iter), loss = 0.058014
I0630 21:56:17.640089  5429 solver.cpp:309]     Train net output #0: loss = 0.0580136 (* 1 = 0.0580136 loss)
I0630 21:56:17.640096  5429 sgd_solver.cpp:106] Iteration 30600, lr = 1e-05
I0630 21:56:36.691301  5429 solver.cpp:290] Iteration 30700 (5.24915 iter/s, 19.0507s/100 iter), loss = 0.0655588
I0630 21:56:36.691324  5429 solver.cpp:309]     Train net output #0: loss = 0.0655584 (* 1 = 0.0655584 loss)
I0630 21:56:36.691331  5429 sgd_solver.cpp:106] Iteration 30700, lr = 1e-05
I0630 21:56:55.884627  5429 solver.cpp:290] Iteration 30800 (5.21029 iter/s, 19.1928s/100 iter), loss = 0.0790909
I0630 21:56:55.884675  5429 solver.cpp:309]     Train net output #0: loss = 0.0790905 (* 1 = 0.0790905 loss)
I0630 21:56:55.884683  5429 sgd_solver.cpp:106] Iteration 30800, lr = 1e-05
I0630 21:57:15.098325  5429 solver.cpp:290] Iteration 30900 (5.20477 iter/s, 19.2131s/100 iter), loss = 0.0604149
I0630 21:57:15.098346  5429 solver.cpp:309]     Train net output #0: loss = 0.0604145 (* 1 = 0.0604145 loss)
I0630 21:57:15.098353  5429 sgd_solver.cpp:106] Iteration 30900, lr = 1e-05
I0630 21:57:34.038301  5429 solver.cpp:290] Iteration 31000 (5.27999 iter/s, 18.9394s/100 iter), loss = 0.0658335
I0630 21:57:34.038368  5429 solver.cpp:309]     Train net output #0: loss = 0.0658331 (* 1 = 0.0658331 loss)
I0630 21:57:34.038377  5429 sgd_solver.cpp:106] Iteration 31000, lr = 1e-05
I0630 21:57:53.100184  5429 solver.cpp:290] Iteration 31100 (5.24623 iter/s, 19.0613s/100 iter), loss = 0.0545499
I0630 21:57:53.100210  5429 solver.cpp:309]     Train net output #0: loss = 0.0545495 (* 1 = 0.0545495 loss)
I0630 21:57:53.100219  5429 sgd_solver.cpp:106] Iteration 31100, lr = 1e-05
I0630 21:58:12.190367  5429 solver.cpp:290] Iteration 31200 (5.23844 iter/s, 19.0896s/100 iter), loss = 0.087293
I0630 21:58:12.190450  5429 solver.cpp:309]     Train net output #0: loss = 0.0872926 (* 1 = 0.0872926 loss)
I0630 21:58:12.190459  5429 sgd_solver.cpp:106] Iteration 31200, lr = 1e-05
I0630 21:58:31.088762  5429 solver.cpp:290] Iteration 31300 (5.29162 iter/s, 18.8978s/100 iter), loss = 0.0838009
I0630 21:58:31.088784  5429 solver.cpp:309]     Train net output #0: loss = 0.0838004 (* 1 = 0.0838004 loss)
I0630 21:58:31.088791  5429 sgd_solver.cpp:106] Iteration 31300, lr = 1e-05
I0630 21:58:50.242689  5429 solver.cpp:290] Iteration 31400 (5.22101 iter/s, 19.1534s/100 iter), loss = 0.0646171
I0630 21:58:50.242763  5429 solver.cpp:309]     Train net output #0: loss = 0.0646167 (* 1 = 0.0646167 loss)
I0630 21:58:50.242771  5429 sgd_solver.cpp:106] Iteration 31400, lr = 1e-05
I0630 21:59:09.394811  5429 solver.cpp:290] Iteration 31500 (5.22152 iter/s, 19.1515s/100 iter), loss = 0.0574569
I0630 21:59:09.394836  5429 solver.cpp:309]     Train net output #0: loss = 0.0574565 (* 1 = 0.0574565 loss)
I0630 21:59:09.394843  5429 sgd_solver.cpp:106] Iteration 31500, lr = 1e-05
I0630 21:59:28.293089  5429 solver.cpp:290] Iteration 31600 (5.29164 iter/s, 18.8977s/100 iter), loss = 0.0536601
I0630 21:59:28.293200  5429 solver.cpp:309]     Train net output #0: loss = 0.0536597 (* 1 = 0.0536597 loss)
I0630 21:59:28.293210  5429 sgd_solver.cpp:106] Iteration 31600, lr = 1e-05
I0630 21:59:47.308977  5429 solver.cpp:290] Iteration 31700 (5.25893 iter/s, 19.0153s/100 iter), loss = 0.0782927
I0630 21:59:47.309003  5429 solver.cpp:309]     Train net output #0: loss = 0.0782923 (* 1 = 0.0782923 loss)
I0630 21:59:47.309010  5429 sgd_solver.cpp:106] Iteration 31700, lr = 1e-05
I0630 22:00:06.367347  5429 solver.cpp:290] Iteration 31800 (5.24719 iter/s, 19.0578s/100 iter), loss = 0.0440781
I0630 22:00:06.367398  5429 solver.cpp:309]     Train net output #0: loss = 0.0440777 (* 1 = 0.0440777 loss)
I0630 22:00:06.367406  5429 sgd_solver.cpp:106] Iteration 31800, lr = 1e-05
I0630 22:00:25.501071  5429 solver.cpp:290] Iteration 31900 (5.22653 iter/s, 19.1332s/100 iter), loss = 0.060034
I0630 22:00:25.501098  5429 solver.cpp:309]     Train net output #0: loss = 0.0600336 (* 1 = 0.0600336 loss)
I0630 22:00:25.501106  5429 sgd_solver.cpp:106] Iteration 31900, lr = 1e-05
I0630 22:00:44.471187  5429 solver.cpp:598] Snapshotting to binary proto file training/cityscapes20_jsegnet21v2_2017-06-30_19-26-17/initial/cityscapes20_jsegnet21v2_iter_32000.caffemodel
I0630 22:00:44.497059  5429 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cityscapes20_jsegnet21v2_2017-06-30_19-26-17/initial/cityscapes20_jsegnet21v2_iter_32000.solverstate
I0630 22:00:44.570858  5429 solver.cpp:451] Iteration 32000, loss = 0.0610758
I0630 22:00:44.570879  5429 solver.cpp:471] Iteration 32000, Testing net (#0)
I0630 22:02:18.846148  5429 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.93853
I0630 22:02:18.846230  5429 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.995939
I0630 22:02:18.846237  5429 solver.cpp:544]     Test net output #2: loss = 0.130387 (* 1 = 0.130387 loss)
I0630 22:02:18.846240  5429 solver.cpp:456] Optimization Done.
I0630 22:02:19.092031  5429 caffe.cpp:246] Optimization Done.
training/cityscapes20_jsegnet21v2_2017-06-30_19-26-17/sparse
I0630 22:02:31.435077  6183 caffe.cpp:209] Using GPUs 0, 1, 2
I0630 22:02:31.435549  6183 caffe.cpp:214] GPU 0: GeForce GTX 1080
I0630 22:02:31.435892  6183 caffe.cpp:214] GPU 1: GeForce GTX 1080
I0630 22:02:31.436233  6183 caffe.cpp:214] GPU 2: GeForce GTX 1080
I0630 22:02:32.264278  6183 solver.cpp:48] Initializing solver from parameters: 
train_net: "training/cityscapes20_jsegnet21v2_2017-06-30_19-26-17/sparse/train.prototxt"
test_net: "training/cityscapes20_jsegnet21v2_2017-06-30_19-26-17/sparse/test.prototxt"
test_iter: 125
test_interval: 2000
base_lr: 1e-05
display: 100
max_iter: 32000
lr_policy: "multistep"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 0.0001
snapshot: 10000
snapshot_prefix: "training/cityscapes20_jsegnet21v2_2017-06-30_19-26-17/sparse/cityscapes20_jsegnet21v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
test_initialization: false
stepvalue: 24000
iter_size: 1
type: "Adam"
display_sparsity: 1000
sparse_mode: SPARSE_UPDATE
sparsity_target: 0.8
sparsity_step_factor: 0.05
sparsity_step_iter: 1000
sparsity_start_iter: 0
sparsity_start_factor: 0
I0630 22:02:32.264370  6183 solver.cpp:82] Creating training net from train_net file: training/cityscapes20_jsegnet21v2_2017-06-30_19-26-17/sparse/train.prototxt
I0630 22:02:32.280292  6183 net.cpp:327] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0630 22:02:32.280336  6183 net.cpp:327] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
I0630 22:02:32.281893  6183 net.cpp:56] Initializing net from parameters: 
name: "jsegnet21v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/train-image-lmdb"
    label_list_path: "data/train-label-lmdb"
    batch_size: 5
    threads: 4
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b/bn"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a/bn"
  top: "out5a/bn"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a/bn"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b/bn"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a/bn"
  top: "out3a/bn"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a/bn"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv1/bn"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv2/bn"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv3/bn"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4/bn"
  top: "ctx_conv4/bn"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4/bn"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 20
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 20
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 20
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
I0630 22:02:32.289773  6183 layer_factory.hpp:77] Creating layer data
I0630 22:02:32.289852  6183 net.cpp:98] Creating Layer data
I0630 22:02:32.289880  6183 net.cpp:413] data -> data
I0630 22:02:32.289980  6183 net.cpp:413] data -> label
I0630 22:02:32.331529  6266 db_lmdb.cpp:35] Opened lmdb data/train-image-lmdb
I0630 22:02:32.341922  6271 db_lmdb.cpp:35] Opened lmdb data/train-label-lmdb
I0630 22:02:32.352357  6183 data_layer.cpp:78] ReshapePrefetch 5, 3, 640, 640
I0630 22:02:32.352638  6183 data_layer.cpp:83] output data size: 5,3,640,640
I0630 22:02:32.401856  6183 data_layer.cpp:78] ReshapePrefetch 5, 1, 640, 640
I0630 22:02:32.401929  6183 data_layer.cpp:83] output data size: 5,1,640,640
I0630 22:02:32.408550  6276 blocking_queue.cpp:50] Waiting for data
I0630 22:02:32.414536  6183 net.cpp:148] Setting up data
I0630 22:02:32.414557  6183 net.cpp:155] Top shape: 5 3 640 640 (6144000)
I0630 22:02:32.414561  6183 net.cpp:155] Top shape: 5 1 640 640 (2048000)
I0630 22:02:32.414562  6183 net.cpp:163] Memory required for data: 32768000
I0630 22:02:32.414569  6183 layer_factory.hpp:77] Creating layer data/bias
I0630 22:02:32.414578  6183 net.cpp:98] Creating Layer data/bias
I0630 22:02:32.414582  6183 net.cpp:439] data/bias <- data
I0630 22:02:32.414590  6183 net.cpp:413] data/bias -> data/bias
I0630 22:02:32.415817  6183 net.cpp:148] Setting up data/bias
I0630 22:02:32.415827  6183 net.cpp:155] Top shape: 5 3 640 640 (6144000)
I0630 22:02:32.415829  6183 net.cpp:163] Memory required for data: 57344000
I0630 22:02:32.415838  6183 layer_factory.hpp:77] Creating layer conv1a
I0630 22:02:32.415848  6183 net.cpp:98] Creating Layer conv1a
I0630 22:02:32.415851  6183 net.cpp:439] conv1a <- data/bias
I0630 22:02:32.415855  6183 net.cpp:413] conv1a -> conv1a
I0630 22:02:32.417227  6183 net.cpp:148] Setting up conv1a
I0630 22:02:32.417237  6183 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0630 22:02:32.417240  6183 net.cpp:163] Memory required for data: 122880000
I0630 22:02:32.417245  6183 layer_factory.hpp:77] Creating layer conv1a/bn
I0630 22:02:32.417255  6183 net.cpp:98] Creating Layer conv1a/bn
I0630 22:02:32.417258  6183 net.cpp:439] conv1a/bn <- conv1a
I0630 22:02:32.417263  6183 net.cpp:413] conv1a/bn -> conv1a/bn
I0630 22:02:32.419606  6183 net.cpp:148] Setting up conv1a/bn
I0630 22:02:32.419620  6183 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0630 22:02:32.419622  6183 net.cpp:163] Memory required for data: 188416000
I0630 22:02:32.419631  6183 layer_factory.hpp:77] Creating layer conv1a/relu
I0630 22:02:32.419636  6183 net.cpp:98] Creating Layer conv1a/relu
I0630 22:02:32.419638  6183 net.cpp:439] conv1a/relu <- conv1a/bn
I0630 22:02:32.419641  6183 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0630 22:02:32.419656  6183 net.cpp:148] Setting up conv1a/relu
I0630 22:02:32.419659  6183 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0630 22:02:32.419661  6183 net.cpp:163] Memory required for data: 253952000
I0630 22:02:32.419664  6183 layer_factory.hpp:77] Creating layer conv1b
I0630 22:02:32.419669  6183 net.cpp:98] Creating Layer conv1b
I0630 22:02:32.419682  6183 net.cpp:439] conv1b <- conv1a/bn
I0630 22:02:32.419685  6183 net.cpp:413] conv1b -> conv1b
I0630 22:02:32.420146  6183 net.cpp:148] Setting up conv1b
I0630 22:02:32.420153  6183 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0630 22:02:32.420156  6183 net.cpp:163] Memory required for data: 319488000
I0630 22:02:32.420161  6183 layer_factory.hpp:77] Creating layer conv1b/bn
I0630 22:02:32.420164  6183 net.cpp:98] Creating Layer conv1b/bn
I0630 22:02:32.420166  6183 net.cpp:439] conv1b/bn <- conv1b
I0630 22:02:32.420171  6183 net.cpp:413] conv1b/bn -> conv1b/bn
I0630 22:02:32.420917  6183 net.cpp:148] Setting up conv1b/bn
I0630 22:02:32.420924  6183 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0630 22:02:32.420928  6183 net.cpp:163] Memory required for data: 385024000
I0630 22:02:32.420933  6183 layer_factory.hpp:77] Creating layer conv1b/relu
I0630 22:02:32.420935  6183 net.cpp:98] Creating Layer conv1b/relu
I0630 22:02:32.420938  6183 net.cpp:439] conv1b/relu <- conv1b/bn
I0630 22:02:32.420939  6183 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0630 22:02:32.420943  6183 net.cpp:148] Setting up conv1b/relu
I0630 22:02:32.420945  6183 net.cpp:155] Top shape: 5 32 320 320 (16384000)
I0630 22:02:32.420948  6183 net.cpp:163] Memory required for data: 450560000
I0630 22:02:32.420949  6183 layer_factory.hpp:77] Creating layer pool1
I0630 22:02:32.420958  6183 net.cpp:98] Creating Layer pool1
I0630 22:02:32.420961  6183 net.cpp:439] pool1 <- conv1b/bn
I0630 22:02:32.420964  6183 net.cpp:413] pool1 -> pool1
I0630 22:02:32.421375  6183 net.cpp:148] Setting up pool1
I0630 22:02:32.421382  6183 net.cpp:155] Top shape: 5 32 160 160 (4096000)
I0630 22:02:32.421386  6183 net.cpp:163] Memory required for data: 466944000
I0630 22:02:32.421387  6183 layer_factory.hpp:77] Creating layer res2a_branch2a
I0630 22:02:32.421392  6183 net.cpp:98] Creating Layer res2a_branch2a
I0630 22:02:32.421394  6183 net.cpp:439] res2a_branch2a <- pool1
I0630 22:02:32.421397  6183 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0630 22:02:32.422894  6183 net.cpp:148] Setting up res2a_branch2a
I0630 22:02:32.422904  6183 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0630 22:02:32.422905  6183 net.cpp:163] Memory required for data: 499712000
I0630 22:02:32.422910  6183 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0630 22:02:32.422914  6183 net.cpp:98] Creating Layer res2a_branch2a/bn
I0630 22:02:32.422916  6183 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0630 22:02:32.422919  6183 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0630 22:02:32.423575  6183 net.cpp:148] Setting up res2a_branch2a/bn
I0630 22:02:32.423581  6183 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0630 22:02:32.423583  6183 net.cpp:163] Memory required for data: 532480000
I0630 22:02:32.423588  6183 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0630 22:02:32.423591  6183 net.cpp:98] Creating Layer res2a_branch2a/relu
I0630 22:02:32.423593  6183 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0630 22:02:32.423595  6183 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0630 22:02:32.423599  6183 net.cpp:148] Setting up res2a_branch2a/relu
I0630 22:02:32.423601  6183 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0630 22:02:32.423604  6183 net.cpp:163] Memory required for data: 565248000
I0630 22:02:32.423605  6183 layer_factory.hpp:77] Creating layer res2a_branch2b
I0630 22:02:32.423609  6183 net.cpp:98] Creating Layer res2a_branch2b
I0630 22:02:32.423610  6183 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0630 22:02:32.423614  6183 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0630 22:02:32.424897  6183 net.cpp:148] Setting up res2a_branch2b
I0630 22:02:32.424906  6183 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0630 22:02:32.424907  6183 net.cpp:163] Memory required for data: 598016000
I0630 22:02:32.424911  6183 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0630 22:02:32.424916  6183 net.cpp:98] Creating Layer res2a_branch2b/bn
I0630 22:02:32.424924  6183 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0630 22:02:32.424928  6183 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0630 22:02:32.425596  6183 net.cpp:148] Setting up res2a_branch2b/bn
I0630 22:02:32.425602  6183 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0630 22:02:32.425604  6183 net.cpp:163] Memory required for data: 630784000
I0630 22:02:32.425609  6183 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0630 22:02:32.425612  6183 net.cpp:98] Creating Layer res2a_branch2b/relu
I0630 22:02:32.425614  6183 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0630 22:02:32.425616  6183 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0630 22:02:32.425622  6183 net.cpp:148] Setting up res2a_branch2b/relu
I0630 22:02:32.425626  6183 net.cpp:155] Top shape: 5 64 160 160 (8192000)
I0630 22:02:32.425626  6183 net.cpp:163] Memory required for data: 663552000
I0630 22:02:32.425629  6183 layer_factory.hpp:77] Creating layer pool2
I0630 22:02:32.425632  6183 net.cpp:98] Creating Layer pool2
I0630 22:02:32.425634  6183 net.cpp:439] pool2 <- res2a_branch2b/bn
I0630 22:02:32.425637  6183 net.cpp:413] pool2 -> pool2
I0630 22:02:32.425670  6183 net.cpp:148] Setting up pool2
I0630 22:02:32.425674  6183 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 22:02:32.425676  6183 net.cpp:163] Memory required for data: 671744000
I0630 22:02:32.425678  6183 layer_factory.hpp:77] Creating layer res3a_branch2a
I0630 22:02:32.425683  6183 net.cpp:98] Creating Layer res3a_branch2a
I0630 22:02:32.425684  6183 net.cpp:439] res3a_branch2a <- pool2
I0630 22:02:32.425688  6183 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0630 22:02:32.427408  6183 net.cpp:148] Setting up res3a_branch2a
I0630 22:02:32.427414  6183 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0630 22:02:32.427417  6183 net.cpp:163] Memory required for data: 688128000
I0630 22:02:32.427419  6183 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0630 22:02:32.427423  6183 net.cpp:98] Creating Layer res3a_branch2a/bn
I0630 22:02:32.427426  6183 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0630 22:02:32.427428  6183 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0630 22:02:32.428021  6183 net.cpp:148] Setting up res3a_branch2a/bn
I0630 22:02:32.428026  6183 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0630 22:02:32.428028  6183 net.cpp:163] Memory required for data: 704512000
I0630 22:02:32.428035  6183 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0630 22:02:32.428037  6183 net.cpp:98] Creating Layer res3a_branch2a/relu
I0630 22:02:32.428040  6183 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0630 22:02:32.428042  6183 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0630 22:02:32.428045  6183 net.cpp:148] Setting up res3a_branch2a/relu
I0630 22:02:32.428047  6183 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0630 22:02:32.428050  6183 net.cpp:163] Memory required for data: 720896000
I0630 22:02:32.428051  6183 layer_factory.hpp:77] Creating layer res3a_branch2b
I0630 22:02:32.428056  6183 net.cpp:98] Creating Layer res3a_branch2b
I0630 22:02:32.428057  6183 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0630 22:02:32.428061  6183 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0630 22:02:32.429150  6183 net.cpp:148] Setting up res3a_branch2b
I0630 22:02:32.429159  6183 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0630 22:02:32.429162  6183 net.cpp:163] Memory required for data: 737280000
I0630 22:02:32.429168  6183 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0630 22:02:32.429174  6183 net.cpp:98] Creating Layer res3a_branch2b/bn
I0630 22:02:32.429177  6183 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0630 22:02:32.429181  6183 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0630 22:02:32.429862  6183 net.cpp:148] Setting up res3a_branch2b/bn
I0630 22:02:32.429869  6183 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0630 22:02:32.429872  6183 net.cpp:163] Memory required for data: 753664000
I0630 22:02:32.429877  6183 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0630 22:02:32.429885  6183 net.cpp:98] Creating Layer res3a_branch2b/relu
I0630 22:02:32.429888  6183 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0630 22:02:32.429890  6183 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0630 22:02:32.429894  6183 net.cpp:148] Setting up res3a_branch2b/relu
I0630 22:02:32.429896  6183 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0630 22:02:32.429898  6183 net.cpp:163] Memory required for data: 770048000
I0630 22:02:32.429900  6183 layer_factory.hpp:77] Creating layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0630 22:02:32.429903  6183 net.cpp:98] Creating Layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0630 22:02:32.429905  6183 net.cpp:439] res3a_branch2b/bn_res3a_branch2b/relu_0_split <- res3a_branch2b/bn
I0630 22:02:32.429908  6183 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0630 22:02:32.429911  6183 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0630 22:02:32.429945  6183 net.cpp:148] Setting up res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0630 22:02:32.429949  6183 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0630 22:02:32.429951  6183 net.cpp:155] Top shape: 5 128 80 80 (4096000)
I0630 22:02:32.429953  6183 net.cpp:163] Memory required for data: 802816000
I0630 22:02:32.429955  6183 layer_factory.hpp:77] Creating layer pool3
I0630 22:02:32.429958  6183 net.cpp:98] Creating Layer pool3
I0630 22:02:32.429960  6183 net.cpp:439] pool3 <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0630 22:02:32.429962  6183 net.cpp:413] pool3 -> pool3
I0630 22:02:32.429997  6183 net.cpp:148] Setting up pool3
I0630 22:02:32.430001  6183 net.cpp:155] Top shape: 5 128 40 40 (1024000)
I0630 22:02:32.430003  6183 net.cpp:163] Memory required for data: 806912000
I0630 22:02:32.430006  6183 layer_factory.hpp:77] Creating layer res4a_branch2a
I0630 22:02:32.430008  6183 net.cpp:98] Creating Layer res4a_branch2a
I0630 22:02:32.430011  6183 net.cpp:439] res4a_branch2a <- pool3
I0630 22:02:32.430013  6183 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0630 22:02:32.437062  6183 net.cpp:148] Setting up res4a_branch2a
I0630 22:02:32.437072  6183 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0630 22:02:32.437074  6183 net.cpp:163] Memory required for data: 815104000
I0630 22:02:32.437078  6183 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0630 22:02:32.437084  6183 net.cpp:98] Creating Layer res4a_branch2a/bn
I0630 22:02:32.437088  6183 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0630 22:02:32.437090  6183 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0630 22:02:32.437706  6183 net.cpp:148] Setting up res4a_branch2a/bn
I0630 22:02:32.437712  6183 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0630 22:02:32.437714  6183 net.cpp:163] Memory required for data: 823296000
I0630 22:02:32.437719  6183 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0630 22:02:32.437722  6183 net.cpp:98] Creating Layer res4a_branch2a/relu
I0630 22:02:32.437724  6183 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0630 22:02:32.437726  6183 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0630 22:02:32.437729  6183 net.cpp:148] Setting up res4a_branch2a/relu
I0630 22:02:32.437732  6183 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0630 22:02:32.437733  6183 net.cpp:163] Memory required for data: 831488000
I0630 22:02:32.437736  6183 layer_factory.hpp:77] Creating layer res4a_branch2b
I0630 22:02:32.437739  6183 net.cpp:98] Creating Layer res4a_branch2b
I0630 22:02:32.437741  6183 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0630 22:02:32.437744  6183 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0630 22:02:32.440902  6183 net.cpp:148] Setting up res4a_branch2b
I0630 22:02:32.440908  6183 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0630 22:02:32.440910  6183 net.cpp:163] Memory required for data: 839680000
I0630 22:02:32.440913  6183 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0630 22:02:32.440924  6183 net.cpp:98] Creating Layer res4a_branch2b/bn
I0630 22:02:32.440927  6183 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0630 22:02:32.440929  6183 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0630 22:02:32.441534  6183 net.cpp:148] Setting up res4a_branch2b/bn
I0630 22:02:32.441539  6183 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0630 22:02:32.441541  6183 net.cpp:163] Memory required for data: 847872000
I0630 22:02:32.441545  6183 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0630 22:02:32.441548  6183 net.cpp:98] Creating Layer res4a_branch2b/relu
I0630 22:02:32.441550  6183 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0630 22:02:32.441552  6183 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0630 22:02:32.441555  6183 net.cpp:148] Setting up res4a_branch2b/relu
I0630 22:02:32.441558  6183 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0630 22:02:32.441560  6183 net.cpp:163] Memory required for data: 856064000
I0630 22:02:32.441561  6183 layer_factory.hpp:77] Creating layer pool4
I0630 22:02:32.441565  6183 net.cpp:98] Creating Layer pool4
I0630 22:02:32.441566  6183 net.cpp:439] pool4 <- res4a_branch2b/bn
I0630 22:02:32.441570  6183 net.cpp:413] pool4 -> pool4
I0630 22:02:32.441603  6183 net.cpp:148] Setting up pool4
I0630 22:02:32.441607  6183 net.cpp:155] Top shape: 5 256 40 40 (2048000)
I0630 22:02:32.441609  6183 net.cpp:163] Memory required for data: 864256000
I0630 22:02:32.441612  6183 layer_factory.hpp:77] Creating layer res5a_branch2a
I0630 22:02:32.441617  6183 net.cpp:98] Creating Layer res5a_branch2a
I0630 22:02:32.441618  6183 net.cpp:439] res5a_branch2a <- pool4
I0630 22:02:32.441622  6183 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0630 22:02:32.467459  6183 net.cpp:148] Setting up res5a_branch2a
I0630 22:02:32.467479  6183 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0630 22:02:32.467481  6183 net.cpp:163] Memory required for data: 880640000
I0630 22:02:32.467488  6183 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0630 22:02:32.467499  6183 net.cpp:98] Creating Layer res5a_branch2a/bn
I0630 22:02:32.467501  6183 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0630 22:02:32.467505  6183 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0630 22:02:32.468149  6183 net.cpp:148] Setting up res5a_branch2a/bn
I0630 22:02:32.468155  6183 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0630 22:02:32.468158  6183 net.cpp:163] Memory required for data: 897024000
I0630 22:02:32.468163  6183 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0630 22:02:32.468166  6183 net.cpp:98] Creating Layer res5a_branch2a/relu
I0630 22:02:32.468168  6183 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0630 22:02:32.468170  6183 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0630 22:02:32.468174  6183 net.cpp:148] Setting up res5a_branch2a/relu
I0630 22:02:32.468178  6183 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0630 22:02:32.468179  6183 net.cpp:163] Memory required for data: 913408000
I0630 22:02:32.468180  6183 layer_factory.hpp:77] Creating layer res5a_branch2b
I0630 22:02:32.468185  6183 net.cpp:98] Creating Layer res5a_branch2b
I0630 22:02:32.468188  6183 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0630 22:02:32.468194  6183 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0630 22:02:32.481076  6183 net.cpp:148] Setting up res5a_branch2b
I0630 22:02:32.481086  6183 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0630 22:02:32.481088  6183 net.cpp:163] Memory required for data: 929792000
I0630 22:02:32.481094  6183 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0630 22:02:32.481099  6183 net.cpp:98] Creating Layer res5a_branch2b/bn
I0630 22:02:32.481101  6183 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0630 22:02:32.481104  6183 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0630 22:02:32.481736  6183 net.cpp:148] Setting up res5a_branch2b/bn
I0630 22:02:32.481741  6183 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0630 22:02:32.481752  6183 net.cpp:163] Memory required for data: 946176000
I0630 22:02:32.481758  6183 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0630 22:02:32.481761  6183 net.cpp:98] Creating Layer res5a_branch2b/relu
I0630 22:02:32.481763  6183 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0630 22:02:32.481765  6183 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0630 22:02:32.481770  6183 net.cpp:148] Setting up res5a_branch2b/relu
I0630 22:02:32.481772  6183 net.cpp:155] Top shape: 5 512 40 40 (4096000)
I0630 22:02:32.481773  6183 net.cpp:163] Memory required for data: 962560000
I0630 22:02:32.481775  6183 layer_factory.hpp:77] Creating layer out5a
I0630 22:02:32.481779  6183 net.cpp:98] Creating Layer out5a
I0630 22:02:32.481782  6183 net.cpp:439] out5a <- res5a_branch2b/bn
I0630 22:02:32.481786  6183 net.cpp:413] out5a -> out5a
I0630 22:02:32.485831  6183 net.cpp:148] Setting up out5a
I0630 22:02:32.485841  6183 net.cpp:155] Top shape: 5 64 40 40 (512000)
I0630 22:02:32.485842  6183 net.cpp:163] Memory required for data: 964608000
I0630 22:02:32.485846  6183 layer_factory.hpp:77] Creating layer out5a/bn
I0630 22:02:32.485851  6183 net.cpp:98] Creating Layer out5a/bn
I0630 22:02:32.485852  6183 net.cpp:439] out5a/bn <- out5a
I0630 22:02:32.485855  6183 net.cpp:413] out5a/bn -> out5a/bn
I0630 22:02:32.486553  6183 net.cpp:148] Setting up out5a/bn
I0630 22:02:32.486559  6183 net.cpp:155] Top shape: 5 64 40 40 (512000)
I0630 22:02:32.486562  6183 net.cpp:163] Memory required for data: 966656000
I0630 22:02:32.486567  6183 layer_factory.hpp:77] Creating layer out5a/relu
I0630 22:02:32.486570  6183 net.cpp:98] Creating Layer out5a/relu
I0630 22:02:32.486572  6183 net.cpp:439] out5a/relu <- out5a/bn
I0630 22:02:32.486574  6183 net.cpp:400] out5a/relu -> out5a/bn (in-place)
I0630 22:02:32.486578  6183 net.cpp:148] Setting up out5a/relu
I0630 22:02:32.486580  6183 net.cpp:155] Top shape: 5 64 40 40 (512000)
I0630 22:02:32.486582  6183 net.cpp:163] Memory required for data: 968704000
I0630 22:02:32.486584  6183 layer_factory.hpp:77] Creating layer out5a_up2
I0630 22:02:32.486593  6183 net.cpp:98] Creating Layer out5a_up2
I0630 22:02:32.486594  6183 net.cpp:439] out5a_up2 <- out5a/bn
I0630 22:02:32.486598  6183 net.cpp:413] out5a_up2 -> out5a_up2
I0630 22:02:32.486843  6183 net.cpp:148] Setting up out5a_up2
I0630 22:02:32.486850  6183 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 22:02:32.486851  6183 net.cpp:163] Memory required for data: 976896000
I0630 22:02:32.486853  6183 layer_factory.hpp:77] Creating layer out3a
I0630 22:02:32.486857  6183 net.cpp:98] Creating Layer out3a
I0630 22:02:32.486860  6183 net.cpp:439] out3a <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0630 22:02:32.486862  6183 net.cpp:413] out3a -> out3a
I0630 22:02:32.487880  6183 net.cpp:148] Setting up out3a
I0630 22:02:32.487886  6183 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 22:02:32.487888  6183 net.cpp:163] Memory required for data: 985088000
I0630 22:02:32.487891  6183 layer_factory.hpp:77] Creating layer out3a/bn
I0630 22:02:32.487895  6183 net.cpp:98] Creating Layer out3a/bn
I0630 22:02:32.487897  6183 net.cpp:439] out3a/bn <- out3a
I0630 22:02:32.487900  6183 net.cpp:413] out3a/bn -> out3a/bn
I0630 22:02:32.488595  6183 net.cpp:148] Setting up out3a/bn
I0630 22:02:32.488600  6183 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 22:02:32.488602  6183 net.cpp:163] Memory required for data: 993280000
I0630 22:02:32.488607  6183 layer_factory.hpp:77] Creating layer out3a/relu
I0630 22:02:32.488610  6183 net.cpp:98] Creating Layer out3a/relu
I0630 22:02:32.488612  6183 net.cpp:439] out3a/relu <- out3a/bn
I0630 22:02:32.488615  6183 net.cpp:400] out3a/relu -> out3a/bn (in-place)
I0630 22:02:32.488617  6183 net.cpp:148] Setting up out3a/relu
I0630 22:02:32.488620  6183 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 22:02:32.488621  6183 net.cpp:163] Memory required for data: 1001472000
I0630 22:02:32.488623  6183 layer_factory.hpp:77] Creating layer out3_out5_combined
I0630 22:02:32.488629  6183 net.cpp:98] Creating Layer out3_out5_combined
I0630 22:02:32.488641  6183 net.cpp:439] out3_out5_combined <- out5a_up2
I0630 22:02:32.488643  6183 net.cpp:439] out3_out5_combined <- out3a/bn
I0630 22:02:32.488646  6183 net.cpp:413] out3_out5_combined -> out3_out5_combined
I0630 22:02:32.488670  6183 net.cpp:148] Setting up out3_out5_combined
I0630 22:02:32.488674  6183 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 22:02:32.488677  6183 net.cpp:163] Memory required for data: 1009664000
I0630 22:02:32.488678  6183 layer_factory.hpp:77] Creating layer ctx_conv1
I0630 22:02:32.488682  6183 net.cpp:98] Creating Layer ctx_conv1
I0630 22:02:32.488683  6183 net.cpp:439] ctx_conv1 <- out3_out5_combined
I0630 22:02:32.488687  6183 net.cpp:413] ctx_conv1 -> ctx_conv1
I0630 22:02:32.489694  6183 net.cpp:148] Setting up ctx_conv1
I0630 22:02:32.489699  6183 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 22:02:32.489701  6183 net.cpp:163] Memory required for data: 1017856000
I0630 22:02:32.489704  6183 layer_factory.hpp:77] Creating layer ctx_conv1/bn
I0630 22:02:32.489707  6183 net.cpp:98] Creating Layer ctx_conv1/bn
I0630 22:02:32.489709  6183 net.cpp:439] ctx_conv1/bn <- ctx_conv1
I0630 22:02:32.489712  6183 net.cpp:413] ctx_conv1/bn -> ctx_conv1/bn
I0630 22:02:32.490404  6183 net.cpp:148] Setting up ctx_conv1/bn
I0630 22:02:32.490411  6183 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 22:02:32.490412  6183 net.cpp:163] Memory required for data: 1026048000
I0630 22:02:32.490417  6183 layer_factory.hpp:77] Creating layer ctx_conv1/relu
I0630 22:02:32.490420  6183 net.cpp:98] Creating Layer ctx_conv1/relu
I0630 22:02:32.490422  6183 net.cpp:439] ctx_conv1/relu <- ctx_conv1/bn
I0630 22:02:32.490425  6183 net.cpp:400] ctx_conv1/relu -> ctx_conv1/bn (in-place)
I0630 22:02:32.490428  6183 net.cpp:148] Setting up ctx_conv1/relu
I0630 22:02:32.490430  6183 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 22:02:32.490432  6183 net.cpp:163] Memory required for data: 1034240000
I0630 22:02:32.490434  6183 layer_factory.hpp:77] Creating layer ctx_conv2
I0630 22:02:32.490438  6183 net.cpp:98] Creating Layer ctx_conv2
I0630 22:02:32.490442  6183 net.cpp:439] ctx_conv2 <- ctx_conv1/bn
I0630 22:02:32.490443  6183 net.cpp:413] ctx_conv2 -> ctx_conv2
I0630 22:02:32.491449  6183 net.cpp:148] Setting up ctx_conv2
I0630 22:02:32.491454  6183 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 22:02:32.491456  6183 net.cpp:163] Memory required for data: 1042432000
I0630 22:02:32.491459  6183 layer_factory.hpp:77] Creating layer ctx_conv2/bn
I0630 22:02:32.491462  6183 net.cpp:98] Creating Layer ctx_conv2/bn
I0630 22:02:32.491466  6183 net.cpp:439] ctx_conv2/bn <- ctx_conv2
I0630 22:02:32.491467  6183 net.cpp:413] ctx_conv2/bn -> ctx_conv2/bn
I0630 22:02:32.492166  6183 net.cpp:148] Setting up ctx_conv2/bn
I0630 22:02:32.492171  6183 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 22:02:32.492172  6183 net.cpp:163] Memory required for data: 1050624000
I0630 22:02:32.492177  6183 layer_factory.hpp:77] Creating layer ctx_conv2/relu
I0630 22:02:32.492182  6183 net.cpp:98] Creating Layer ctx_conv2/relu
I0630 22:02:32.492183  6183 net.cpp:439] ctx_conv2/relu <- ctx_conv2/bn
I0630 22:02:32.492185  6183 net.cpp:400] ctx_conv2/relu -> ctx_conv2/bn (in-place)
I0630 22:02:32.492188  6183 net.cpp:148] Setting up ctx_conv2/relu
I0630 22:02:32.492192  6183 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 22:02:32.492192  6183 net.cpp:163] Memory required for data: 1058816000
I0630 22:02:32.492194  6183 layer_factory.hpp:77] Creating layer ctx_conv3
I0630 22:02:32.492197  6183 net.cpp:98] Creating Layer ctx_conv3
I0630 22:02:32.492200  6183 net.cpp:439] ctx_conv3 <- ctx_conv2/bn
I0630 22:02:32.492202  6183 net.cpp:413] ctx_conv3 -> ctx_conv3
I0630 22:02:32.493213  6183 net.cpp:148] Setting up ctx_conv3
I0630 22:02:32.493217  6183 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 22:02:32.493219  6183 net.cpp:163] Memory required for data: 1067008000
I0630 22:02:32.493223  6183 layer_factory.hpp:77] Creating layer ctx_conv3/bn
I0630 22:02:32.493227  6183 net.cpp:98] Creating Layer ctx_conv3/bn
I0630 22:02:32.493234  6183 net.cpp:439] ctx_conv3/bn <- ctx_conv3
I0630 22:02:32.493237  6183 net.cpp:413] ctx_conv3/bn -> ctx_conv3/bn
I0630 22:02:32.493947  6183 net.cpp:148] Setting up ctx_conv3/bn
I0630 22:02:32.493952  6183 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 22:02:32.493954  6183 net.cpp:163] Memory required for data: 1075200000
I0630 22:02:32.493959  6183 layer_factory.hpp:77] Creating layer ctx_conv3/relu
I0630 22:02:32.493963  6183 net.cpp:98] Creating Layer ctx_conv3/relu
I0630 22:02:32.493965  6183 net.cpp:439] ctx_conv3/relu <- ctx_conv3/bn
I0630 22:02:32.493968  6183 net.cpp:400] ctx_conv3/relu -> ctx_conv3/bn (in-place)
I0630 22:02:32.493973  6183 net.cpp:148] Setting up ctx_conv3/relu
I0630 22:02:32.493975  6183 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 22:02:32.493976  6183 net.cpp:163] Memory required for data: 1083392000
I0630 22:02:32.493978  6183 layer_factory.hpp:77] Creating layer ctx_conv4
I0630 22:02:32.493983  6183 net.cpp:98] Creating Layer ctx_conv4
I0630 22:02:32.493985  6183 net.cpp:439] ctx_conv4 <- ctx_conv3/bn
I0630 22:02:32.493988  6183 net.cpp:413] ctx_conv4 -> ctx_conv4
I0630 22:02:32.495029  6183 net.cpp:148] Setting up ctx_conv4
I0630 22:02:32.495036  6183 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 22:02:32.495038  6183 net.cpp:163] Memory required for data: 1091584000
I0630 22:02:32.495041  6183 layer_factory.hpp:77] Creating layer ctx_conv4/bn
I0630 22:02:32.495044  6183 net.cpp:98] Creating Layer ctx_conv4/bn
I0630 22:02:32.495048  6183 net.cpp:439] ctx_conv4/bn <- ctx_conv4
I0630 22:02:32.495049  6183 net.cpp:413] ctx_conv4/bn -> ctx_conv4/bn
I0630 22:02:32.495744  6183 net.cpp:148] Setting up ctx_conv4/bn
I0630 22:02:32.495750  6183 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 22:02:32.495753  6183 net.cpp:163] Memory required for data: 1099776000
I0630 22:02:32.495756  6183 layer_factory.hpp:77] Creating layer ctx_conv4/relu
I0630 22:02:32.495759  6183 net.cpp:98] Creating Layer ctx_conv4/relu
I0630 22:02:32.495761  6183 net.cpp:439] ctx_conv4/relu <- ctx_conv4/bn
I0630 22:02:32.495764  6183 net.cpp:400] ctx_conv4/relu -> ctx_conv4/bn (in-place)
I0630 22:02:32.495766  6183 net.cpp:148] Setting up ctx_conv4/relu
I0630 22:02:32.495769  6183 net.cpp:155] Top shape: 5 64 80 80 (2048000)
I0630 22:02:32.495770  6183 net.cpp:163] Memory required for data: 1107968000
I0630 22:02:32.495772  6183 layer_factory.hpp:77] Creating layer ctx_final
I0630 22:02:32.495776  6183 net.cpp:98] Creating Layer ctx_final
I0630 22:02:32.495777  6183 net.cpp:439] ctx_final <- ctx_conv4/bn
I0630 22:02:32.495780  6183 net.cpp:413] ctx_final -> ctx_final
I0630 22:02:32.496299  6183 net.cpp:148] Setting up ctx_final
I0630 22:02:32.496305  6183 net.cpp:155] Top shape: 5 20 80 80 (640000)
I0630 22:02:32.496307  6183 net.cpp:163] Memory required for data: 1110528000
I0630 22:02:32.496310  6183 layer_factory.hpp:77] Creating layer ctx_final/relu
I0630 22:02:32.496312  6183 net.cpp:98] Creating Layer ctx_final/relu
I0630 22:02:32.496315  6183 net.cpp:439] ctx_final/relu <- ctx_final
I0630 22:02:32.496316  6183 net.cpp:400] ctx_final/relu -> ctx_final (in-place)
I0630 22:02:32.496320  6183 net.cpp:148] Setting up ctx_final/relu
I0630 22:02:32.496322  6183 net.cpp:155] Top shape: 5 20 80 80 (640000)
I0630 22:02:32.496323  6183 net.cpp:163] Memory required for data: 1113088000
I0630 22:02:32.496325  6183 layer_factory.hpp:77] Creating layer out_deconv_final_up2
I0630 22:02:32.496328  6183 net.cpp:98] Creating Layer out_deconv_final_up2
I0630 22:02:32.496330  6183 net.cpp:439] out_deconv_final_up2 <- ctx_final
I0630 22:02:32.496333  6183 net.cpp:413] out_deconv_final_up2 -> out_deconv_final_up2
I0630 22:02:32.496562  6183 net.cpp:148] Setting up out_deconv_final_up2
I0630 22:02:32.496567  6183 net.cpp:155] Top shape: 5 20 160 160 (2560000)
I0630 22:02:32.496568  6183 net.cpp:163] Memory required for data: 1123328000
I0630 22:02:32.496572  6183 layer_factory.hpp:77] Creating layer out_deconv_final_up4
I0630 22:02:32.496574  6183 net.cpp:98] Creating Layer out_deconv_final_up4
I0630 22:02:32.496582  6183 net.cpp:439] out_deconv_final_up4 <- out_deconv_final_up2
I0630 22:02:32.496585  6183 net.cpp:413] out_deconv_final_up4 -> out_deconv_final_up4
I0630 22:02:32.496820  6183 net.cpp:148] Setting up out_deconv_final_up4
I0630 22:02:32.496825  6183 net.cpp:155] Top shape: 5 20 320 320 (10240000)
I0630 22:02:32.496826  6183 net.cpp:163] Memory required for data: 1164288000
I0630 22:02:32.496829  6183 layer_factory.hpp:77] Creating layer out_deconv_final_up8
I0630 22:02:32.496832  6183 net.cpp:98] Creating Layer out_deconv_final_up8
I0630 22:02:32.496834  6183 net.cpp:439] out_deconv_final_up8 <- out_deconv_final_up4
I0630 22:02:32.496837  6183 net.cpp:413] out_deconv_final_up8 -> out_deconv_final_up8
I0630 22:02:32.497062  6183 net.cpp:148] Setting up out_deconv_final_up8
I0630 22:02:32.497066  6183 net.cpp:155] Top shape: 5 20 640 640 (40960000)
I0630 22:02:32.497068  6183 net.cpp:163] Memory required for data: 1328128000
I0630 22:02:32.497071  6183 layer_factory.hpp:77] Creating layer loss
I0630 22:02:32.497074  6183 net.cpp:98] Creating Layer loss
I0630 22:02:32.497076  6183 net.cpp:439] loss <- out_deconv_final_up8
I0630 22:02:32.497078  6183 net.cpp:439] loss <- label
I0630 22:02:32.497083  6183 net.cpp:413] loss -> loss
I0630 22:02:32.497089  6183 layer_factory.hpp:77] Creating layer loss
I0630 22:02:32.547430  6183 net.cpp:148] Setting up loss
I0630 22:02:32.547451  6183 net.cpp:155] Top shape: (1)
I0630 22:02:32.547454  6183 net.cpp:158]     with loss weight 1
I0630 22:02:32.547466  6183 net.cpp:163] Memory required for data: 1328128004
I0630 22:02:32.547469  6183 net.cpp:224] loss needs backward computation.
I0630 22:02:32.547472  6183 net.cpp:224] out_deconv_final_up8 needs backward computation.
I0630 22:02:32.547474  6183 net.cpp:224] out_deconv_final_up4 needs backward computation.
I0630 22:02:32.547477  6183 net.cpp:224] out_deconv_final_up2 needs backward computation.
I0630 22:02:32.547479  6183 net.cpp:224] ctx_final/relu needs backward computation.
I0630 22:02:32.547482  6183 net.cpp:224] ctx_final needs backward computation.
I0630 22:02:32.547482  6183 net.cpp:224] ctx_conv4/relu needs backward computation.
I0630 22:02:32.547484  6183 net.cpp:224] ctx_conv4/bn needs backward computation.
I0630 22:02:32.547487  6183 net.cpp:224] ctx_conv4 needs backward computation.
I0630 22:02:32.547488  6183 net.cpp:224] ctx_conv3/relu needs backward computation.
I0630 22:02:32.547490  6183 net.cpp:224] ctx_conv3/bn needs backward computation.
I0630 22:02:32.547492  6183 net.cpp:224] ctx_conv3 needs backward computation.
I0630 22:02:32.547494  6183 net.cpp:224] ctx_conv2/relu needs backward computation.
I0630 22:02:32.547497  6183 net.cpp:224] ctx_conv2/bn needs backward computation.
I0630 22:02:32.547498  6183 net.cpp:224] ctx_conv2 needs backward computation.
I0630 22:02:32.547502  6183 net.cpp:224] ctx_conv1/relu needs backward computation.
I0630 22:02:32.547503  6183 net.cpp:224] ctx_conv1/bn needs backward computation.
I0630 22:02:32.547505  6183 net.cpp:224] ctx_conv1 needs backward computation.
I0630 22:02:32.547508  6183 net.cpp:224] out3_out5_combined needs backward computation.
I0630 22:02:32.547511  6183 net.cpp:224] out3a/relu needs backward computation.
I0630 22:02:32.547513  6183 net.cpp:224] out3a/bn needs backward computation.
I0630 22:02:32.547516  6183 net.cpp:224] out3a needs backward computation.
I0630 22:02:32.547519  6183 net.cpp:224] out5a_up2 needs backward computation.
I0630 22:02:32.547521  6183 net.cpp:224] out5a/relu needs backward computation.
I0630 22:02:32.547523  6183 net.cpp:224] out5a/bn needs backward computation.
I0630 22:02:32.547525  6183 net.cpp:224] out5a needs backward computation.
I0630 22:02:32.547528  6183 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0630 22:02:32.547529  6183 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0630 22:02:32.547531  6183 net.cpp:224] res5a_branch2b needs backward computation.
I0630 22:02:32.547534  6183 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0630 22:02:32.547547  6183 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0630 22:02:32.547549  6183 net.cpp:224] res5a_branch2a needs backward computation.
I0630 22:02:32.547552  6183 net.cpp:224] pool4 needs backward computation.
I0630 22:02:32.547554  6183 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0630 22:02:32.547556  6183 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0630 22:02:32.547559  6183 net.cpp:224] res4a_branch2b needs backward computation.
I0630 22:02:32.547560  6183 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0630 22:02:32.547562  6183 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0630 22:02:32.547564  6183 net.cpp:224] res4a_branch2a needs backward computation.
I0630 22:02:32.547566  6183 net.cpp:224] pool3 needs backward computation.
I0630 22:02:32.547569  6183 net.cpp:224] res3a_branch2b/bn_res3a_branch2b/relu_0_split needs backward computation.
I0630 22:02:32.547570  6183 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0630 22:02:32.547572  6183 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0630 22:02:32.547574  6183 net.cpp:224] res3a_branch2b needs backward computation.
I0630 22:02:32.547576  6183 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0630 22:02:32.547579  6183 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0630 22:02:32.547580  6183 net.cpp:224] res3a_branch2a needs backward computation.
I0630 22:02:32.547582  6183 net.cpp:224] pool2 needs backward computation.
I0630 22:02:32.547585  6183 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0630 22:02:32.547586  6183 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0630 22:02:32.547588  6183 net.cpp:224] res2a_branch2b needs backward computation.
I0630 22:02:32.547590  6183 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0630 22:02:32.547592  6183 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0630 22:02:32.547595  6183 net.cpp:224] res2a_branch2a needs backward computation.
I0630 22:02:32.547596  6183 net.cpp:224] pool1 needs backward computation.
I0630 22:02:32.547598  6183 net.cpp:224] conv1b/relu needs backward computation.
I0630 22:02:32.547601  6183 net.cpp:224] conv1b/bn needs backward computation.
I0630 22:02:32.547603  6183 net.cpp:224] conv1b needs backward computation.
I0630 22:02:32.547606  6183 net.cpp:224] conv1a/relu needs backward computation.
I0630 22:02:32.547608  6183 net.cpp:224] conv1a/bn needs backward computation.
I0630 22:02:32.547610  6183 net.cpp:224] conv1a needs backward computation.
I0630 22:02:32.547612  6183 net.cpp:226] data/bias does not need backward computation.
I0630 22:02:32.547616  6183 net.cpp:226] data does not need backward computation.
I0630 22:02:32.547617  6183 net.cpp:268] This network produces output loss
I0630 22:02:32.547649  6183 net.cpp:288] Network initialization done.
I0630 22:02:32.548369  6183 solver.cpp:182] Creating test net (#0) specified by test_net file: training/cityscapes20_jsegnet21v2_2017-06-30_19-26-17/sparse/test.prototxt
I0630 22:02:32.548646  6183 net.cpp:56] Initializing net from parameters: 
name: "jsegnet21v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/val-image-lmdb"
    label_list_path: "data/val-label-lmdb"
    batch_size: 4
    threads: 4
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b/bn"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a/bn"
  top: "out5a/bn"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a/bn"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b/bn"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a/bn"
  top: "out3a/bn"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a/bn"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv1/bn"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv2/bn"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv3/bn"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4/bn"
  top: "ctx_conv4/bn"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4/bn"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 20
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 20
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 20
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
  accuracy_param {
    ignore_label: 255
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
    ignore_label: 255
  }
}
I0630 22:02:32.548765  6183 layer_factory.hpp:77] Creating layer data
I0630 22:02:32.548773  6183 net.cpp:98] Creating Layer data
I0630 22:02:32.548775  6183 net.cpp:413] data -> data
I0630 22:02:32.548780  6183 net.cpp:413] data -> label
I0630 22:02:32.572214  6283 db_lmdb.cpp:35] Opened lmdb data/val-label-lmdb
I0630 22:02:32.593104  6278 db_lmdb.cpp:35] Opened lmdb data/val-image-lmdb
I0630 22:02:32.596525  6183 data_layer.cpp:78] ReshapePrefetch 4, 3, 640, 640
I0630 22:02:32.596609  6183 data_layer.cpp:83] output data size: 4,3,640,640
I0630 22:02:32.620383  6183 data_layer.cpp:78] ReshapePrefetch 4, 1, 640, 640
I0630 22:02:32.620451  6183 data_layer.cpp:83] output data size: 4,1,640,640
I0630 22:02:32.634029  6183 net.cpp:148] Setting up data
I0630 22:02:32.634061  6183 net.cpp:155] Top shape: 4 3 640 640 (4915200)
I0630 22:02:32.634066  6183 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0630 22:02:32.634068  6183 net.cpp:163] Memory required for data: 26214400
I0630 22:02:32.634074  6183 layer_factory.hpp:77] Creating layer label_data_1_split
I0630 22:02:32.634114  6183 net.cpp:98] Creating Layer label_data_1_split
I0630 22:02:32.634119  6183 net.cpp:439] label_data_1_split <- label
I0630 22:02:32.634125  6183 net.cpp:413] label_data_1_split -> label_data_1_split_0
I0630 22:02:32.634135  6183 net.cpp:413] label_data_1_split -> label_data_1_split_1
I0630 22:02:32.634152  6183 net.cpp:413] label_data_1_split -> label_data_1_split_2
I0630 22:02:32.634294  6183 net.cpp:148] Setting up label_data_1_split
I0630 22:02:32.634300  6183 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0630 22:02:32.634305  6183 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0630 22:02:32.634310  6183 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0630 22:02:32.634312  6183 net.cpp:163] Memory required for data: 45875200
I0630 22:02:32.634317  6183 layer_factory.hpp:77] Creating layer data/bias
I0630 22:02:32.634326  6183 net.cpp:98] Creating Layer data/bias
I0630 22:02:32.634331  6183 net.cpp:439] data/bias <- data
I0630 22:02:32.634336  6183 net.cpp:413] data/bias -> data/bias
I0630 22:02:32.635743  6183 net.cpp:148] Setting up data/bias
I0630 22:02:32.635761  6183 net.cpp:155] Top shape: 4 3 640 640 (4915200)
I0630 22:02:32.635764  6183 net.cpp:163] Memory required for data: 65536000
I0630 22:02:32.635776  6183 layer_factory.hpp:77] Creating layer conv1a
I0630 22:02:32.635799  6183 net.cpp:98] Creating Layer conv1a
I0630 22:02:32.635807  6183 net.cpp:439] conv1a <- data/bias
I0630 22:02:32.635812  6183 net.cpp:413] conv1a -> conv1a
I0630 22:02:32.636579  6183 net.cpp:148] Setting up conv1a
I0630 22:02:32.636589  6183 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0630 22:02:32.636591  6183 net.cpp:163] Memory required for data: 117964800
I0630 22:02:32.636602  6183 layer_factory.hpp:77] Creating layer conv1a/bn
I0630 22:02:32.636615  6183 net.cpp:98] Creating Layer conv1a/bn
I0630 22:02:32.636620  6183 net.cpp:439] conv1a/bn <- conv1a
I0630 22:02:32.636626  6183 net.cpp:413] conv1a/bn -> conv1a/bn
I0630 22:02:32.639363  6183 net.cpp:148] Setting up conv1a/bn
I0630 22:02:32.639380  6183 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0630 22:02:32.639384  6183 net.cpp:163] Memory required for data: 170393600
I0630 22:02:32.639396  6183 layer_factory.hpp:77] Creating layer conv1a/relu
I0630 22:02:32.639406  6183 net.cpp:98] Creating Layer conv1a/relu
I0630 22:02:32.639410  6183 net.cpp:439] conv1a/relu <- conv1a/bn
I0630 22:02:32.639418  6183 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0630 22:02:32.639430  6183 net.cpp:148] Setting up conv1a/relu
I0630 22:02:32.639437  6183 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0630 22:02:32.639441  6183 net.cpp:163] Memory required for data: 222822400
I0630 22:02:32.639446  6183 layer_factory.hpp:77] Creating layer conv1b
I0630 22:02:32.639458  6183 net.cpp:98] Creating Layer conv1b
I0630 22:02:32.639462  6183 net.cpp:439] conv1b <- conv1a/bn
I0630 22:02:32.639468  6183 net.cpp:413] conv1b -> conv1b
I0630 22:02:32.640049  6183 net.cpp:148] Setting up conv1b
I0630 22:02:32.640058  6183 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0630 22:02:32.640061  6183 net.cpp:163] Memory required for data: 275251200
I0630 22:02:32.640070  6183 layer_factory.hpp:77] Creating layer conv1b/bn
I0630 22:02:32.640079  6183 net.cpp:98] Creating Layer conv1b/bn
I0630 22:02:32.640084  6183 net.cpp:439] conv1b/bn <- conv1b
I0630 22:02:32.640091  6183 net.cpp:413] conv1b/bn -> conv1b/bn
I0630 22:02:32.641099  6183 net.cpp:148] Setting up conv1b/bn
I0630 22:02:32.641110  6183 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0630 22:02:32.641113  6183 net.cpp:163] Memory required for data: 327680000
I0630 22:02:32.641124  6183 layer_factory.hpp:77] Creating layer conv1b/relu
I0630 22:02:32.641134  6183 net.cpp:98] Creating Layer conv1b/relu
I0630 22:02:32.641137  6183 net.cpp:439] conv1b/relu <- conv1b/bn
I0630 22:02:32.641142  6183 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0630 22:02:32.641149  6183 net.cpp:148] Setting up conv1b/relu
I0630 22:02:32.641155  6183 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0630 22:02:32.641186  6183 net.cpp:163] Memory required for data: 380108800
I0630 22:02:32.641191  6183 layer_factory.hpp:77] Creating layer pool1
I0630 22:02:32.641201  6183 net.cpp:98] Creating Layer pool1
I0630 22:02:32.641204  6183 net.cpp:439] pool1 <- conv1b/bn
I0630 22:02:32.641209  6183 net.cpp:413] pool1 -> pool1
I0630 22:02:32.641273  6183 net.cpp:148] Setting up pool1
I0630 22:02:32.641280  6183 net.cpp:155] Top shape: 4 32 160 160 (3276800)
I0630 22:02:32.641283  6183 net.cpp:163] Memory required for data: 393216000
I0630 22:02:32.641288  6183 layer_factory.hpp:77] Creating layer res2a_branch2a
I0630 22:02:32.641297  6183 net.cpp:98] Creating Layer res2a_branch2a
I0630 22:02:32.641301  6183 net.cpp:439] res2a_branch2a <- pool1
I0630 22:02:32.641306  6183 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0630 22:02:32.642202  6183 net.cpp:148] Setting up res2a_branch2a
I0630 22:02:32.642212  6183 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0630 22:02:32.642215  6183 net.cpp:163] Memory required for data: 419430400
I0630 22:02:32.642225  6183 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0630 22:02:32.642235  6183 net.cpp:98] Creating Layer res2a_branch2a/bn
I0630 22:02:32.642238  6183 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0630 22:02:32.642244  6183 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0630 22:02:32.645519  6183 net.cpp:148] Setting up res2a_branch2a/bn
I0630 22:02:32.645532  6183 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0630 22:02:32.645535  6183 net.cpp:163] Memory required for data: 445644800
I0630 22:02:32.645545  6183 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0630 22:02:32.645638  6183 net.cpp:98] Creating Layer res2a_branch2a/relu
I0630 22:02:32.645644  6183 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0630 22:02:32.645648  6183 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0630 22:02:32.645658  6183 net.cpp:148] Setting up res2a_branch2a/relu
I0630 22:02:32.645738  6183 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0630 22:02:32.645743  6183 net.cpp:163] Memory required for data: 471859200
I0630 22:02:32.645747  6183 layer_factory.hpp:77] Creating layer res2a_branch2b
I0630 22:02:32.645757  6183 net.cpp:98] Creating Layer res2a_branch2b
I0630 22:02:32.645823  6183 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0630 22:02:32.645830  6183 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0630 22:02:32.646499  6183 net.cpp:148] Setting up res2a_branch2b
I0630 22:02:32.646508  6183 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0630 22:02:32.646512  6183 net.cpp:163] Memory required for data: 498073600
I0630 22:02:32.646519  6183 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0630 22:02:32.646540  6183 net.cpp:98] Creating Layer res2a_branch2b/bn
I0630 22:02:32.646546  6183 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0630 22:02:32.646551  6183 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0630 22:02:32.647469  6183 net.cpp:148] Setting up res2a_branch2b/bn
I0630 22:02:32.647480  6183 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0630 22:02:32.647485  6183 net.cpp:163] Memory required for data: 524288000
I0630 22:02:32.647493  6183 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0630 22:02:32.647500  6183 net.cpp:98] Creating Layer res2a_branch2b/relu
I0630 22:02:32.647505  6183 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0630 22:02:32.647508  6183 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0630 22:02:32.647514  6183 net.cpp:148] Setting up res2a_branch2b/relu
I0630 22:02:32.647519  6183 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0630 22:02:32.647522  6183 net.cpp:163] Memory required for data: 550502400
I0630 22:02:32.647526  6183 layer_factory.hpp:77] Creating layer pool2
I0630 22:02:32.647536  6183 net.cpp:98] Creating Layer pool2
I0630 22:02:32.647538  6183 net.cpp:439] pool2 <- res2a_branch2b/bn
I0630 22:02:32.647543  6183 net.cpp:413] pool2 -> pool2
I0630 22:02:32.647611  6183 net.cpp:148] Setting up pool2
I0630 22:02:32.647619  6183 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 22:02:32.647639  6183 net.cpp:163] Memory required for data: 557056000
I0630 22:02:32.647644  6183 layer_factory.hpp:77] Creating layer res3a_branch2a
I0630 22:02:32.647653  6183 net.cpp:98] Creating Layer res3a_branch2a
I0630 22:02:32.647657  6183 net.cpp:439] res3a_branch2a <- pool2
I0630 22:02:32.647662  6183 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0630 22:02:32.649709  6183 net.cpp:148] Setting up res3a_branch2a
I0630 22:02:32.649741  6183 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0630 22:02:32.649744  6183 net.cpp:163] Memory required for data: 570163200
I0630 22:02:32.649755  6183 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0630 22:02:32.649767  6183 net.cpp:98] Creating Layer res3a_branch2a/bn
I0630 22:02:32.649773  6183 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0630 22:02:32.649785  6183 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0630 22:02:32.650602  6183 net.cpp:148] Setting up res3a_branch2a/bn
I0630 22:02:32.650614  6183 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0630 22:02:32.650617  6183 net.cpp:163] Memory required for data: 583270400
I0630 22:02:32.650630  6183 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0630 22:02:32.650638  6183 net.cpp:98] Creating Layer res3a_branch2a/relu
I0630 22:02:32.650642  6183 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0630 22:02:32.650646  6183 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0630 22:02:32.650653  6183 net.cpp:148] Setting up res3a_branch2a/relu
I0630 22:02:32.650657  6183 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0630 22:02:32.650660  6183 net.cpp:163] Memory required for data: 596377600
I0630 22:02:32.650663  6183 layer_factory.hpp:77] Creating layer res3a_branch2b
I0630 22:02:32.650672  6183 net.cpp:98] Creating Layer res3a_branch2b
I0630 22:02:32.650676  6183 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0630 22:02:32.650679  6183 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0630 22:02:32.652637  6183 net.cpp:148] Setting up res3a_branch2b
I0630 22:02:32.652650  6183 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0630 22:02:32.652653  6183 net.cpp:163] Memory required for data: 609484800
I0630 22:02:32.652662  6183 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0630 22:02:32.652673  6183 net.cpp:98] Creating Layer res3a_branch2b/bn
I0630 22:02:32.652676  6183 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0630 22:02:32.652691  6183 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0630 22:02:32.653501  6183 net.cpp:148] Setting up res3a_branch2b/bn
I0630 22:02:32.653512  6183 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0630 22:02:32.653514  6183 net.cpp:163] Memory required for data: 622592000
I0630 22:02:32.653524  6183 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0630 22:02:32.653530  6183 net.cpp:98] Creating Layer res3a_branch2b/relu
I0630 22:02:32.653535  6183 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0630 22:02:32.653540  6183 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0630 22:02:32.653548  6183 net.cpp:148] Setting up res3a_branch2b/relu
I0630 22:02:32.653553  6183 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0630 22:02:32.653556  6183 net.cpp:163] Memory required for data: 635699200
I0630 22:02:32.653560  6183 layer_factory.hpp:77] Creating layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0630 22:02:32.653568  6183 net.cpp:98] Creating Layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0630 22:02:32.653573  6183 net.cpp:439] res3a_branch2b/bn_res3a_branch2b/relu_0_split <- res3a_branch2b/bn
I0630 22:02:32.653578  6183 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0630 22:02:32.653584  6183 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0630 22:02:32.653638  6183 net.cpp:148] Setting up res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0630 22:02:32.653643  6183 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0630 22:02:32.653663  6183 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0630 22:02:32.653668  6183 net.cpp:163] Memory required for data: 661913600
I0630 22:02:32.653672  6183 layer_factory.hpp:77] Creating layer pool3
I0630 22:02:32.653681  6183 net.cpp:98] Creating Layer pool3
I0630 22:02:32.653684  6183 net.cpp:439] pool3 <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0630 22:02:32.653689  6183 net.cpp:413] pool3 -> pool3
I0630 22:02:32.653739  6183 net.cpp:148] Setting up pool3
I0630 22:02:32.653744  6183 net.cpp:155] Top shape: 4 128 40 40 (819200)
I0630 22:02:32.653748  6183 net.cpp:163] Memory required for data: 665190400
I0630 22:02:32.653753  6183 layer_factory.hpp:77] Creating layer res4a_branch2a
I0630 22:02:32.653764  6183 net.cpp:98] Creating Layer res4a_branch2a
I0630 22:02:32.653767  6183 net.cpp:439] res4a_branch2a <- pool3
I0630 22:02:32.653774  6183 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0630 22:02:32.663594  6183 net.cpp:148] Setting up res4a_branch2a
I0630 22:02:32.663645  6183 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0630 22:02:32.663647  6183 net.cpp:163] Memory required for data: 671744000
I0630 22:02:32.663658  6183 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0630 22:02:32.663679  6183 net.cpp:98] Creating Layer res4a_branch2a/bn
I0630 22:02:32.663686  6183 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0630 22:02:32.663702  6183 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0630 22:02:32.664508  6183 net.cpp:148] Setting up res4a_branch2a/bn
I0630 22:02:32.664517  6183 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0630 22:02:32.664520  6183 net.cpp:163] Memory required for data: 678297600
I0630 22:02:32.664525  6183 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0630 22:02:32.664531  6183 net.cpp:98] Creating Layer res4a_branch2a/relu
I0630 22:02:32.664533  6183 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0630 22:02:32.664536  6183 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0630 22:02:32.664543  6183 net.cpp:148] Setting up res4a_branch2a/relu
I0630 22:02:32.664546  6183 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0630 22:02:32.664547  6183 net.cpp:163] Memory required for data: 684851200
I0630 22:02:32.664551  6183 layer_factory.hpp:77] Creating layer res4a_branch2b
I0630 22:02:32.664557  6183 net.cpp:98] Creating Layer res4a_branch2b
I0630 22:02:32.664561  6183 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0630 22:02:32.664563  6183 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0630 22:02:32.668623  6183 net.cpp:148] Setting up res4a_branch2b
I0630 22:02:32.668642  6183 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0630 22:02:32.668648  6183 net.cpp:163] Memory required for data: 691404800
I0630 22:02:32.668656  6183 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0630 22:02:32.668666  6183 net.cpp:98] Creating Layer res4a_branch2b/bn
I0630 22:02:32.668673  6183 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0630 22:02:32.668679  6183 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0630 22:02:32.670132  6183 net.cpp:148] Setting up res4a_branch2b/bn
I0630 22:02:32.670168  6183 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0630 22:02:32.670182  6183 net.cpp:163] Memory required for data: 697958400
I0630 22:02:32.670204  6183 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0630 22:02:32.670222  6183 net.cpp:98] Creating Layer res4a_branch2b/relu
I0630 22:02:32.670235  6183 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0630 22:02:32.670246  6183 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0630 22:02:32.670261  6183 net.cpp:148] Setting up res4a_branch2b/relu
I0630 22:02:32.670276  6183 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0630 22:02:32.670287  6183 net.cpp:163] Memory required for data: 704512000
I0630 22:02:32.670297  6183 layer_factory.hpp:77] Creating layer pool4
I0630 22:02:32.670313  6183 net.cpp:98] Creating Layer pool4
I0630 22:02:32.670322  6183 net.cpp:439] pool4 <- res4a_branch2b/bn
I0630 22:02:32.670339  6183 net.cpp:413] pool4 -> pool4
I0630 22:02:32.670423  6183 net.cpp:148] Setting up pool4
I0630 22:02:32.670435  6183 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0630 22:02:32.670442  6183 net.cpp:163] Memory required for data: 711065600
I0630 22:02:32.670450  6183 layer_factory.hpp:77] Creating layer res5a_branch2a
I0630 22:02:32.670464  6183 net.cpp:98] Creating Layer res5a_branch2a
I0630 22:02:32.670471  6183 net.cpp:439] res5a_branch2a <- pool4
I0630 22:02:32.670480  6183 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0630 22:02:32.696772  6183 net.cpp:148] Setting up res5a_branch2a
I0630 22:02:32.696799  6183 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0630 22:02:32.696801  6183 net.cpp:163] Memory required for data: 724172800
I0630 22:02:32.696808  6183 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0630 22:02:32.696818  6183 net.cpp:98] Creating Layer res5a_branch2a/bn
I0630 22:02:32.696822  6183 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0630 22:02:32.696826  6183 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0630 22:02:32.697567  6183 net.cpp:148] Setting up res5a_branch2a/bn
I0630 22:02:32.697573  6183 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0630 22:02:32.697576  6183 net.cpp:163] Memory required for data: 737280000
I0630 22:02:32.697582  6183 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0630 22:02:32.697584  6183 net.cpp:98] Creating Layer res5a_branch2a/relu
I0630 22:02:32.697587  6183 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0630 22:02:32.697589  6183 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0630 22:02:32.697593  6183 net.cpp:148] Setting up res5a_branch2a/relu
I0630 22:02:32.697595  6183 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0630 22:02:32.697597  6183 net.cpp:163] Memory required for data: 750387200
I0630 22:02:32.697599  6183 layer_factory.hpp:77] Creating layer res5a_branch2b
I0630 22:02:32.697604  6183 net.cpp:98] Creating Layer res5a_branch2b
I0630 22:02:32.697607  6183 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0630 22:02:32.697610  6183 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0630 22:02:32.710532  6183 net.cpp:148] Setting up res5a_branch2b
I0630 22:02:32.710546  6183 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0630 22:02:32.710549  6183 net.cpp:163] Memory required for data: 763494400
I0630 22:02:32.710557  6183 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0630 22:02:32.710563  6183 net.cpp:98] Creating Layer res5a_branch2b/bn
I0630 22:02:32.710566  6183 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0630 22:02:32.710569  6183 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0630 22:02:32.711264  6183 net.cpp:148] Setting up res5a_branch2b/bn
I0630 22:02:32.711271  6183 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0630 22:02:32.711272  6183 net.cpp:163] Memory required for data: 776601600
I0630 22:02:32.711277  6183 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0630 22:02:32.711280  6183 net.cpp:98] Creating Layer res5a_branch2b/relu
I0630 22:02:32.711283  6183 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0630 22:02:32.711285  6183 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0630 22:02:32.711289  6183 net.cpp:148] Setting up res5a_branch2b/relu
I0630 22:02:32.711292  6183 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0630 22:02:32.711293  6183 net.cpp:163] Memory required for data: 789708800
I0630 22:02:32.711295  6183 layer_factory.hpp:77] Creating layer out5a
I0630 22:02:32.711299  6183 net.cpp:98] Creating Layer out5a
I0630 22:02:32.711302  6183 net.cpp:439] out5a <- res5a_branch2b/bn
I0630 22:02:32.711304  6183 net.cpp:413] out5a -> out5a
I0630 22:02:32.715437  6183 net.cpp:148] Setting up out5a
I0630 22:02:32.715445  6183 net.cpp:155] Top shape: 4 64 40 40 (409600)
I0630 22:02:32.715447  6183 net.cpp:163] Memory required for data: 791347200
I0630 22:02:32.715451  6183 layer_factory.hpp:77] Creating layer out5a/bn
I0630 22:02:32.715456  6183 net.cpp:98] Creating Layer out5a/bn
I0630 22:02:32.715459  6183 net.cpp:439] out5a/bn <- out5a
I0630 22:02:32.715472  6183 net.cpp:413] out5a/bn -> out5a/bn
I0630 22:02:32.716223  6183 net.cpp:148] Setting up out5a/bn
I0630 22:02:32.716228  6183 net.cpp:155] Top shape: 4 64 40 40 (409600)
I0630 22:02:32.716229  6183 net.cpp:163] Memory required for data: 792985600
I0630 22:02:32.716234  6183 layer_factory.hpp:77] Creating layer out5a/relu
I0630 22:02:32.716238  6183 net.cpp:98] Creating Layer out5a/relu
I0630 22:02:32.716239  6183 net.cpp:439] out5a/relu <- out5a/bn
I0630 22:02:32.716243  6183 net.cpp:400] out5a/relu -> out5a/bn (in-place)
I0630 22:02:32.716245  6183 net.cpp:148] Setting up out5a/relu
I0630 22:02:32.716248  6183 net.cpp:155] Top shape: 4 64 40 40 (409600)
I0630 22:02:32.716249  6183 net.cpp:163] Memory required for data: 794624000
I0630 22:02:32.716251  6183 layer_factory.hpp:77] Creating layer out5a_up2
I0630 22:02:32.716255  6183 net.cpp:98] Creating Layer out5a_up2
I0630 22:02:32.716258  6183 net.cpp:439] out5a_up2 <- out5a/bn
I0630 22:02:32.716259  6183 net.cpp:413] out5a_up2 -> out5a_up2
I0630 22:02:32.716527  6183 net.cpp:148] Setting up out5a_up2
I0630 22:02:32.716532  6183 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 22:02:32.716534  6183 net.cpp:163] Memory required for data: 801177600
I0630 22:02:32.716537  6183 layer_factory.hpp:77] Creating layer out3a
I0630 22:02:32.716542  6183 net.cpp:98] Creating Layer out3a
I0630 22:02:32.716544  6183 net.cpp:439] out3a <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0630 22:02:32.716547  6183 net.cpp:413] out3a -> out3a
I0630 22:02:32.718616  6183 net.cpp:148] Setting up out3a
I0630 22:02:32.718626  6183 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 22:02:32.718628  6183 net.cpp:163] Memory required for data: 807731200
I0630 22:02:32.718632  6183 layer_factory.hpp:77] Creating layer out3a/bn
I0630 22:02:32.718637  6183 net.cpp:98] Creating Layer out3a/bn
I0630 22:02:32.718641  6183 net.cpp:439] out3a/bn <- out3a
I0630 22:02:32.718644  6183 net.cpp:413] out3a/bn -> out3a/bn
I0630 22:02:32.719415  6183 net.cpp:148] Setting up out3a/bn
I0630 22:02:32.719420  6183 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 22:02:32.719424  6183 net.cpp:163] Memory required for data: 814284800
I0630 22:02:32.719429  6183 layer_factory.hpp:77] Creating layer out3a/relu
I0630 22:02:32.719431  6183 net.cpp:98] Creating Layer out3a/relu
I0630 22:02:32.719434  6183 net.cpp:439] out3a/relu <- out3a/bn
I0630 22:02:32.719435  6183 net.cpp:400] out3a/relu -> out3a/bn (in-place)
I0630 22:02:32.719439  6183 net.cpp:148] Setting up out3a/relu
I0630 22:02:32.719441  6183 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 22:02:32.719444  6183 net.cpp:163] Memory required for data: 820838400
I0630 22:02:32.719445  6183 layer_factory.hpp:77] Creating layer out3_out5_combined
I0630 22:02:32.719449  6183 net.cpp:98] Creating Layer out3_out5_combined
I0630 22:02:32.719450  6183 net.cpp:439] out3_out5_combined <- out5a_up2
I0630 22:02:32.719452  6183 net.cpp:439] out3_out5_combined <- out3a/bn
I0630 22:02:32.719456  6183 net.cpp:413] out3_out5_combined -> out3_out5_combined
I0630 22:02:32.719481  6183 net.cpp:148] Setting up out3_out5_combined
I0630 22:02:32.719485  6183 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 22:02:32.719487  6183 net.cpp:163] Memory required for data: 827392000
I0630 22:02:32.719490  6183 layer_factory.hpp:77] Creating layer ctx_conv1
I0630 22:02:32.719493  6183 net.cpp:98] Creating Layer ctx_conv1
I0630 22:02:32.719496  6183 net.cpp:439] ctx_conv1 <- out3_out5_combined
I0630 22:02:32.719498  6183 net.cpp:413] ctx_conv1 -> ctx_conv1
I0630 22:02:32.720547  6183 net.cpp:148] Setting up ctx_conv1
I0630 22:02:32.720552  6183 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 22:02:32.720554  6183 net.cpp:163] Memory required for data: 833945600
I0630 22:02:32.720557  6183 layer_factory.hpp:77] Creating layer ctx_conv1/bn
I0630 22:02:32.720561  6183 net.cpp:98] Creating Layer ctx_conv1/bn
I0630 22:02:32.720563  6183 net.cpp:439] ctx_conv1/bn <- ctx_conv1
I0630 22:02:32.720566  6183 net.cpp:413] ctx_conv1/bn -> ctx_conv1/bn
I0630 22:02:32.721339  6183 net.cpp:148] Setting up ctx_conv1/bn
I0630 22:02:32.721345  6183 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 22:02:32.721348  6183 net.cpp:163] Memory required for data: 840499200
I0630 22:02:32.721351  6183 layer_factory.hpp:77] Creating layer ctx_conv1/relu
I0630 22:02:32.721354  6183 net.cpp:98] Creating Layer ctx_conv1/relu
I0630 22:02:32.721356  6183 net.cpp:439] ctx_conv1/relu <- ctx_conv1/bn
I0630 22:02:32.721359  6183 net.cpp:400] ctx_conv1/relu -> ctx_conv1/bn (in-place)
I0630 22:02:32.721362  6183 net.cpp:148] Setting up ctx_conv1/relu
I0630 22:02:32.721364  6183 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 22:02:32.721366  6183 net.cpp:163] Memory required for data: 847052800
I0630 22:02:32.721369  6183 layer_factory.hpp:77] Creating layer ctx_conv2
I0630 22:02:32.721375  6183 net.cpp:98] Creating Layer ctx_conv2
I0630 22:02:32.721379  6183 net.cpp:439] ctx_conv2 <- ctx_conv1/bn
I0630 22:02:32.721381  6183 net.cpp:413] ctx_conv2 -> ctx_conv2
I0630 22:02:32.722437  6183 net.cpp:148] Setting up ctx_conv2
I0630 22:02:32.722442  6183 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 22:02:32.722445  6183 net.cpp:163] Memory required for data: 853606400
I0630 22:02:32.722448  6183 layer_factory.hpp:77] Creating layer ctx_conv2/bn
I0630 22:02:32.722451  6183 net.cpp:98] Creating Layer ctx_conv2/bn
I0630 22:02:32.722453  6183 net.cpp:439] ctx_conv2/bn <- ctx_conv2
I0630 22:02:32.722456  6183 net.cpp:413] ctx_conv2/bn -> ctx_conv2/bn
I0630 22:02:32.723242  6183 net.cpp:148] Setting up ctx_conv2/bn
I0630 22:02:32.723248  6183 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 22:02:32.723249  6183 net.cpp:163] Memory required for data: 860160000
I0630 22:02:32.723254  6183 layer_factory.hpp:77] Creating layer ctx_conv2/relu
I0630 22:02:32.723258  6183 net.cpp:98] Creating Layer ctx_conv2/relu
I0630 22:02:32.723259  6183 net.cpp:439] ctx_conv2/relu <- ctx_conv2/bn
I0630 22:02:32.723261  6183 net.cpp:400] ctx_conv2/relu -> ctx_conv2/bn (in-place)
I0630 22:02:32.723264  6183 net.cpp:148] Setting up ctx_conv2/relu
I0630 22:02:32.723266  6183 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 22:02:32.723268  6183 net.cpp:163] Memory required for data: 866713600
I0630 22:02:32.723270  6183 layer_factory.hpp:77] Creating layer ctx_conv3
I0630 22:02:32.723274  6183 net.cpp:98] Creating Layer ctx_conv3
I0630 22:02:32.723276  6183 net.cpp:439] ctx_conv3 <- ctx_conv2/bn
I0630 22:02:32.723280  6183 net.cpp:413] ctx_conv3 -> ctx_conv3
I0630 22:02:32.724333  6183 net.cpp:148] Setting up ctx_conv3
I0630 22:02:32.724337  6183 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 22:02:32.724339  6183 net.cpp:163] Memory required for data: 873267200
I0630 22:02:32.724342  6183 layer_factory.hpp:77] Creating layer ctx_conv3/bn
I0630 22:02:32.724346  6183 net.cpp:98] Creating Layer ctx_conv3/bn
I0630 22:02:32.724349  6183 net.cpp:439] ctx_conv3/bn <- ctx_conv3
I0630 22:02:32.724350  6183 net.cpp:413] ctx_conv3/bn -> ctx_conv3/bn
I0630 22:02:32.725127  6183 net.cpp:148] Setting up ctx_conv3/bn
I0630 22:02:32.725132  6183 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 22:02:32.725134  6183 net.cpp:163] Memory required for data: 879820800
I0630 22:02:32.725139  6183 layer_factory.hpp:77] Creating layer ctx_conv3/relu
I0630 22:02:32.725143  6183 net.cpp:98] Creating Layer ctx_conv3/relu
I0630 22:02:32.725145  6183 net.cpp:439] ctx_conv3/relu <- ctx_conv3/bn
I0630 22:02:32.725148  6183 net.cpp:400] ctx_conv3/relu -> ctx_conv3/bn (in-place)
I0630 22:02:32.725152  6183 net.cpp:148] Setting up ctx_conv3/relu
I0630 22:02:32.725153  6183 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 22:02:32.725155  6183 net.cpp:163] Memory required for data: 886374400
I0630 22:02:32.725157  6183 layer_factory.hpp:77] Creating layer ctx_conv4
I0630 22:02:32.725162  6183 net.cpp:98] Creating Layer ctx_conv4
I0630 22:02:32.725163  6183 net.cpp:439] ctx_conv4 <- ctx_conv3/bn
I0630 22:02:32.725167  6183 net.cpp:413] ctx_conv4 -> ctx_conv4
I0630 22:02:32.726215  6183 net.cpp:148] Setting up ctx_conv4
I0630 22:02:32.726219  6183 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 22:02:32.726228  6183 net.cpp:163] Memory required for data: 892928000
I0630 22:02:32.726233  6183 layer_factory.hpp:77] Creating layer ctx_conv4/bn
I0630 22:02:32.726235  6183 net.cpp:98] Creating Layer ctx_conv4/bn
I0630 22:02:32.726238  6183 net.cpp:439] ctx_conv4/bn <- ctx_conv4
I0630 22:02:32.726240  6183 net.cpp:413] ctx_conv4/bn -> ctx_conv4/bn
I0630 22:02:32.727008  6183 net.cpp:148] Setting up ctx_conv4/bn
I0630 22:02:32.727015  6183 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 22:02:32.727016  6183 net.cpp:163] Memory required for data: 899481600
I0630 22:02:32.727021  6183 layer_factory.hpp:77] Creating layer ctx_conv4/relu
I0630 22:02:32.727023  6183 net.cpp:98] Creating Layer ctx_conv4/relu
I0630 22:02:32.727025  6183 net.cpp:439] ctx_conv4/relu <- ctx_conv4/bn
I0630 22:02:32.727028  6183 net.cpp:400] ctx_conv4/relu -> ctx_conv4/bn (in-place)
I0630 22:02:32.727031  6183 net.cpp:148] Setting up ctx_conv4/relu
I0630 22:02:32.727033  6183 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0630 22:02:32.727035  6183 net.cpp:163] Memory required for data: 906035200
I0630 22:02:32.727037  6183 layer_factory.hpp:77] Creating layer ctx_final
I0630 22:02:32.727041  6183 net.cpp:98] Creating Layer ctx_final
I0630 22:02:32.727042  6183 net.cpp:439] ctx_final <- ctx_conv4/bn
I0630 22:02:32.727046  6183 net.cpp:413] ctx_final -> ctx_final
I0630 22:02:32.727603  6183 net.cpp:148] Setting up ctx_final
I0630 22:02:32.727608  6183 net.cpp:155] Top shape: 4 20 80 80 (512000)
I0630 22:02:32.727610  6183 net.cpp:163] Memory required for data: 908083200
I0630 22:02:32.727613  6183 layer_factory.hpp:77] Creating layer ctx_final/relu
I0630 22:02:32.727617  6183 net.cpp:98] Creating Layer ctx_final/relu
I0630 22:02:32.727618  6183 net.cpp:439] ctx_final/relu <- ctx_final
I0630 22:02:32.727620  6183 net.cpp:400] ctx_final/relu -> ctx_final (in-place)
I0630 22:02:32.727623  6183 net.cpp:148] Setting up ctx_final/relu
I0630 22:02:32.727627  6183 net.cpp:155] Top shape: 4 20 80 80 (512000)
I0630 22:02:32.727627  6183 net.cpp:163] Memory required for data: 910131200
I0630 22:02:32.727629  6183 layer_factory.hpp:77] Creating layer out_deconv_final_up2
I0630 22:02:32.727633  6183 net.cpp:98] Creating Layer out_deconv_final_up2
I0630 22:02:32.727635  6183 net.cpp:439] out_deconv_final_up2 <- ctx_final
I0630 22:02:32.727638  6183 net.cpp:413] out_deconv_final_up2 -> out_deconv_final_up2
I0630 22:02:32.727898  6183 net.cpp:148] Setting up out_deconv_final_up2
I0630 22:02:32.727902  6183 net.cpp:155] Top shape: 4 20 160 160 (2048000)
I0630 22:02:32.727905  6183 net.cpp:163] Memory required for data: 918323200
I0630 22:02:32.727907  6183 layer_factory.hpp:77] Creating layer out_deconv_final_up4
I0630 22:02:32.727910  6183 net.cpp:98] Creating Layer out_deconv_final_up4
I0630 22:02:32.727912  6183 net.cpp:439] out_deconv_final_up4 <- out_deconv_final_up2
I0630 22:02:32.727916  6183 net.cpp:413] out_deconv_final_up4 -> out_deconv_final_up4
I0630 22:02:32.728173  6183 net.cpp:148] Setting up out_deconv_final_up4
I0630 22:02:32.728178  6183 net.cpp:155] Top shape: 4 20 320 320 (8192000)
I0630 22:02:32.728180  6183 net.cpp:163] Memory required for data: 951091200
I0630 22:02:32.728183  6183 layer_factory.hpp:77] Creating layer out_deconv_final_up8
I0630 22:02:32.728185  6183 net.cpp:98] Creating Layer out_deconv_final_up8
I0630 22:02:32.728188  6183 net.cpp:439] out_deconv_final_up8 <- out_deconv_final_up4
I0630 22:02:32.728190  6183 net.cpp:413] out_deconv_final_up8 -> out_deconv_final_up8
I0630 22:02:32.728442  6183 net.cpp:148] Setting up out_deconv_final_up8
I0630 22:02:32.728447  6183 net.cpp:155] Top shape: 4 20 640 640 (32768000)
I0630 22:02:32.728449  6183 net.cpp:163] Memory required for data: 1082163200
I0630 22:02:32.728452  6183 layer_factory.hpp:77] Creating layer out_deconv_final_up8_out_deconv_final_up8_0_split
I0630 22:02:32.728456  6183 net.cpp:98] Creating Layer out_deconv_final_up8_out_deconv_final_up8_0_split
I0630 22:02:32.728459  6183 net.cpp:439] out_deconv_final_up8_out_deconv_final_up8_0_split <- out_deconv_final_up8
I0630 22:02:32.728468  6183 net.cpp:413] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0630 22:02:32.728472  6183 net.cpp:413] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0630 22:02:32.728477  6183 net.cpp:413] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0630 22:02:32.728533  6183 net.cpp:148] Setting up out_deconv_final_up8_out_deconv_final_up8_0_split
I0630 22:02:32.728538  6183 net.cpp:155] Top shape: 4 20 640 640 (32768000)
I0630 22:02:32.728540  6183 net.cpp:155] Top shape: 4 20 640 640 (32768000)
I0630 22:02:32.728543  6183 net.cpp:155] Top shape: 4 20 640 640 (32768000)
I0630 22:02:32.728544  6183 net.cpp:163] Memory required for data: 1475379200
I0630 22:02:32.728546  6183 layer_factory.hpp:77] Creating layer loss
I0630 22:02:32.728550  6183 net.cpp:98] Creating Layer loss
I0630 22:02:32.728554  6183 net.cpp:439] loss <- out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0630 22:02:32.728555  6183 net.cpp:439] loss <- label_data_1_split_0
I0630 22:02:32.728559  6183 net.cpp:413] loss -> loss
I0630 22:02:32.728562  6183 layer_factory.hpp:77] Creating layer loss
I0630 22:02:32.768090  6183 net.cpp:148] Setting up loss
I0630 22:02:32.768111  6183 net.cpp:155] Top shape: (1)
I0630 22:02:32.768115  6183 net.cpp:158]     with loss weight 1
I0630 22:02:32.768121  6183 net.cpp:163] Memory required for data: 1475379204
I0630 22:02:32.768126  6183 layer_factory.hpp:77] Creating layer accuracy/top1
I0630 22:02:32.768132  6183 net.cpp:98] Creating Layer accuracy/top1
I0630 22:02:32.768137  6183 net.cpp:439] accuracy/top1 <- out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0630 22:02:32.768141  6183 net.cpp:439] accuracy/top1 <- label_data_1_split_1
I0630 22:02:32.768146  6183 net.cpp:413] accuracy/top1 -> accuracy/top1
I0630 22:02:32.768152  6183 net.cpp:148] Setting up accuracy/top1
I0630 22:02:32.768157  6183 net.cpp:155] Top shape: (1)
I0630 22:02:32.768157  6183 net.cpp:163] Memory required for data: 1475379208
I0630 22:02:32.768159  6183 layer_factory.hpp:77] Creating layer accuracy/top5
I0630 22:02:32.768163  6183 net.cpp:98] Creating Layer accuracy/top5
I0630 22:02:32.768165  6183 net.cpp:439] accuracy/top5 <- out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0630 22:02:32.768168  6183 net.cpp:439] accuracy/top5 <- label_data_1_split_2
I0630 22:02:32.768172  6183 net.cpp:413] accuracy/top5 -> accuracy/top5
I0630 22:02:32.768174  6183 net.cpp:148] Setting up accuracy/top5
I0630 22:02:32.768177  6183 net.cpp:155] Top shape: (1)
I0630 22:02:32.768178  6183 net.cpp:163] Memory required for data: 1475379212
I0630 22:02:32.768180  6183 net.cpp:226] accuracy/top5 does not need backward computation.
I0630 22:02:32.768184  6183 net.cpp:226] accuracy/top1 does not need backward computation.
I0630 22:02:32.768187  6183 net.cpp:224] loss needs backward computation.
I0630 22:02:32.768189  6183 net.cpp:224] out_deconv_final_up8_out_deconv_final_up8_0_split needs backward computation.
I0630 22:02:32.768193  6183 net.cpp:224] out_deconv_final_up8 needs backward computation.
I0630 22:02:32.768194  6183 net.cpp:224] out_deconv_final_up4 needs backward computation.
I0630 22:02:32.768198  6183 net.cpp:224] out_deconv_final_up2 needs backward computation.
I0630 22:02:32.768199  6183 net.cpp:224] ctx_final/relu needs backward computation.
I0630 22:02:32.768203  6183 net.cpp:224] ctx_final needs backward computation.
I0630 22:02:32.768204  6183 net.cpp:224] ctx_conv4/relu needs backward computation.
I0630 22:02:32.768206  6183 net.cpp:224] ctx_conv4/bn needs backward computation.
I0630 22:02:32.768208  6183 net.cpp:224] ctx_conv4 needs backward computation.
I0630 22:02:32.768211  6183 net.cpp:224] ctx_conv3/relu needs backward computation.
I0630 22:02:32.768213  6183 net.cpp:224] ctx_conv3/bn needs backward computation.
I0630 22:02:32.768216  6183 net.cpp:224] ctx_conv3 needs backward computation.
I0630 22:02:32.768227  6183 net.cpp:224] ctx_conv2/relu needs backward computation.
I0630 22:02:32.768230  6183 net.cpp:224] ctx_conv2/bn needs backward computation.
I0630 22:02:32.768232  6183 net.cpp:224] ctx_conv2 needs backward computation.
I0630 22:02:32.768234  6183 net.cpp:224] ctx_conv1/relu needs backward computation.
I0630 22:02:32.768235  6183 net.cpp:224] ctx_conv1/bn needs backward computation.
I0630 22:02:32.768239  6183 net.cpp:224] ctx_conv1 needs backward computation.
I0630 22:02:32.768241  6183 net.cpp:224] out3_out5_combined needs backward computation.
I0630 22:02:32.768244  6183 net.cpp:224] out3a/relu needs backward computation.
I0630 22:02:32.768246  6183 net.cpp:224] out3a/bn needs backward computation.
I0630 22:02:32.768249  6183 net.cpp:224] out3a needs backward computation.
I0630 22:02:32.768252  6183 net.cpp:224] out5a_up2 needs backward computation.
I0630 22:02:32.768255  6183 net.cpp:224] out5a/relu needs backward computation.
I0630 22:02:32.768257  6183 net.cpp:224] out5a/bn needs backward computation.
I0630 22:02:32.768260  6183 net.cpp:224] out5a needs backward computation.
I0630 22:02:32.768262  6183 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0630 22:02:32.768265  6183 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0630 22:02:32.768267  6183 net.cpp:224] res5a_branch2b needs backward computation.
I0630 22:02:32.768270  6183 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0630 22:02:32.768272  6183 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0630 22:02:32.768275  6183 net.cpp:224] res5a_branch2a needs backward computation.
I0630 22:02:32.768277  6183 net.cpp:224] pool4 needs backward computation.
I0630 22:02:32.768280  6183 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0630 22:02:32.768282  6183 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0630 22:02:32.768285  6183 net.cpp:224] res4a_branch2b needs backward computation.
I0630 22:02:32.768287  6183 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0630 22:02:32.768290  6183 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0630 22:02:32.768291  6183 net.cpp:224] res4a_branch2a needs backward computation.
I0630 22:02:32.768295  6183 net.cpp:224] pool3 needs backward computation.
I0630 22:02:32.768297  6183 net.cpp:224] res3a_branch2b/bn_res3a_branch2b/relu_0_split needs backward computation.
I0630 22:02:32.768299  6183 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0630 22:02:32.768302  6183 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0630 22:02:32.768304  6183 net.cpp:224] res3a_branch2b needs backward computation.
I0630 22:02:32.768306  6183 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0630 22:02:32.768308  6183 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0630 22:02:32.768311  6183 net.cpp:224] res3a_branch2a needs backward computation.
I0630 22:02:32.768313  6183 net.cpp:224] pool2 needs backward computation.
I0630 22:02:32.768316  6183 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0630 22:02:32.768319  6183 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0630 22:02:32.768321  6183 net.cpp:224] res2a_branch2b needs backward computation.
I0630 22:02:32.768323  6183 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0630 22:02:32.768326  6183 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0630 22:02:32.768328  6183 net.cpp:224] res2a_branch2a needs backward computation.
I0630 22:02:32.768331  6183 net.cpp:224] pool1 needs backward computation.
I0630 22:02:32.768333  6183 net.cpp:224] conv1b/relu needs backward computation.
I0630 22:02:32.768335  6183 net.cpp:224] conv1b/bn needs backward computation.
I0630 22:02:32.768337  6183 net.cpp:224] conv1b needs backward computation.
I0630 22:02:32.768340  6183 net.cpp:224] conv1a/relu needs backward computation.
I0630 22:02:32.768342  6183 net.cpp:224] conv1a/bn needs backward computation.
I0630 22:02:32.768344  6183 net.cpp:224] conv1a needs backward computation.
I0630 22:02:32.768347  6183 net.cpp:226] data/bias does not need backward computation.
I0630 22:02:32.768353  6183 net.cpp:226] label_data_1_split does not need backward computation.
I0630 22:02:32.768357  6183 net.cpp:226] data does not need backward computation.
I0630 22:02:32.768358  6183 net.cpp:268] This network produces output accuracy/top1
I0630 22:02:32.768360  6183 net.cpp:268] This network produces output accuracy/top5
I0630 22:02:32.768363  6183 net.cpp:268] This network produces output loss
I0630 22:02:32.768394  6183 net.cpp:288] Network initialization done.
I0630 22:02:32.768487  6183 solver.cpp:60] Solver scaffolding done.
I0630 22:02:32.776463  6183 caffe.cpp:145] Finetuning from training/cityscapes20_jsegnet21v2_2017-06-30_19-26-17/initial/cityscapes20_jsegnet21v2_iter_32000.caffemodel
I0630 22:02:32.830099  6183 data_layer.cpp:78] ReshapePrefetch 5, 3, 640, 640
I0630 22:02:32.830168  6183 data_layer.cpp:83] output data size: 5,3,640,640
I0630 22:02:32.861964  6183 data_layer.cpp:78] ReshapePrefetch 5, 1, 640, 640
I0630 22:02:32.862032  6183 data_layer.cpp:83] output data size: 5,1,640,640
I0630 22:02:33.401605  6183 data_layer.cpp:78] ReshapePrefetch 5, 3, 640, 640
I0630 22:02:33.401765  6183 data_layer.cpp:83] output data size: 5,3,640,640
I0630 22:02:33.460273  6183 data_layer.cpp:78] ReshapePrefetch 5, 1, 640, 640
I0630 22:02:33.460422  6183 data_layer.cpp:83] output data size: 5,1,640,640
I0630 22:02:34.159998  6183 parallel.cpp:334] Starting Optimization
I0630 22:02:34.160056  6183 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 22:02:34.171515  6183 solver.cpp:413] Solving jsegnet21v2_train
I0630 22:02:34.171531  6183 solver.cpp:414] Learning Rate Policy: multistep
I0630 22:02:34.684921  6183 solver.cpp:290] Iteration 0 (0 iter/s, 0.513357s/100 iter), loss = 0.080394
I0630 22:02:34.684947  6183 solver.cpp:309]     Train net output #0: loss = 0.080394 (* 1 = 0.080394 loss)
I0630 22:02:34.684953  6183 sgd_solver.cpp:106] Iteration 0, lr = 1e-05
I0630 22:02:34.716505  6183 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.05
I0630 22:02:35.044258  6183 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 22:02:54.560667  6183 solver.cpp:290] Iteration 100 (5.0314 iter/s, 19.8752s/100 iter), loss = 0.0607867
I0630 22:02:54.560694  6183 solver.cpp:309]     Train net output #0: loss = 0.0607867 (* 1 = 0.0607867 loss)
I0630 22:02:54.560703  6183 sgd_solver.cpp:106] Iteration 100, lr = 1e-05
I0630 22:02:57.948920  6344 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 22:03:16.939716  6183 solver.cpp:290] Iteration 200 (4.46859 iter/s, 22.3784s/100 iter), loss = 0.0427452
I0630 22:03:16.939774  6183 solver.cpp:309]     Train net output #0: loss = 0.0427452 (* 1 = 0.0427452 loss)
I0630 22:03:16.939781  6183 sgd_solver.cpp:106] Iteration 200, lr = 1e-05
I0630 22:03:36.283085  6183 solver.cpp:290] Iteration 300 (5.16989 iter/s, 19.3428s/100 iter), loss = 0.0713072
I0630 22:03:36.283113  6183 solver.cpp:309]     Train net output #0: loss = 0.0713072 (* 1 = 0.0713072 loss)
I0630 22:03:36.283121  6183 sgd_solver.cpp:106] Iteration 300, lr = 1e-05
I0630 22:03:55.776875  6183 solver.cpp:290] Iteration 400 (5.12999 iter/s, 19.4932s/100 iter), loss = 0.0662863
I0630 22:03:55.776928  6183 solver.cpp:309]     Train net output #0: loss = 0.0662863 (* 1 = 0.0662863 loss)
I0630 22:03:55.776937  6183 sgd_solver.cpp:106] Iteration 400, lr = 1e-05
I0630 22:04:15.206717  6183 solver.cpp:290] Iteration 500 (5.14688 iter/s, 19.4293s/100 iter), loss = 0.0780309
I0630 22:04:15.206744  6183 solver.cpp:309]     Train net output #0: loss = 0.0780309 (* 1 = 0.0780309 loss)
I0630 22:04:15.206753  6183 sgd_solver.cpp:106] Iteration 500, lr = 1e-05
I0630 22:04:34.583106  6183 solver.cpp:290] Iteration 600 (5.16107 iter/s, 19.3758s/100 iter), loss = 0.0402618
I0630 22:04:34.583192  6183 solver.cpp:309]     Train net output #0: loss = 0.0402618 (* 1 = 0.0402618 loss)
I0630 22:04:34.583204  6183 sgd_solver.cpp:106] Iteration 600, lr = 1e-05
I0630 22:04:53.964725  6183 solver.cpp:290] Iteration 700 (5.15969 iter/s, 19.381s/100 iter), loss = 0.0817724
I0630 22:04:53.964751  6183 solver.cpp:309]     Train net output #0: loss = 0.0817724 (* 1 = 0.0817724 loss)
I0630 22:04:53.964758  6183 sgd_solver.cpp:106] Iteration 700, lr = 1e-05
I0630 22:05:13.310587  6183 solver.cpp:290] Iteration 800 (5.16921 iter/s, 19.3453s/100 iter), loss = 0.0587701
I0630 22:05:13.310653  6183 solver.cpp:309]     Train net output #0: loss = 0.0587701 (* 1 = 0.0587701 loss)
I0630 22:05:13.310662  6183 sgd_solver.cpp:106] Iteration 800, lr = 1e-05
I0630 22:05:32.844583  6183 solver.cpp:290] Iteration 900 (5.11944 iter/s, 19.5334s/100 iter), loss = 0.0792876
I0630 22:05:32.844606  6183 solver.cpp:309]     Train net output #0: loss = 0.0792876 (* 1 = 0.0792876 loss)
I0630 22:05:32.844614  6183 sgd_solver.cpp:106] Iteration 900, lr = 1e-05
I0630 22:05:51.903400  6183 solver.cpp:354] Sparsity after update:
I0630 22:05:51.947928  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 22:05:51.947973  6183 net.cpp:1851] conv1a_param_0(0) 
I0630 22:05:51.947999  6183 net.cpp:1851] conv1b_param_0(0.0499) 
I0630 22:05:51.948014  6183 net.cpp:1851] ctx_conv1_param_0(0.05) 
I0630 22:05:51.948026  6183 net.cpp:1851] ctx_conv2_param_0(0.05) 
I0630 22:05:51.948038  6183 net.cpp:1851] ctx_conv3_param_0(0.05) 
I0630 22:05:51.948050  6183 net.cpp:1851] ctx_conv4_param_0(0.0499) 
I0630 22:05:51.948062  6183 net.cpp:1851] ctx_final_param_0(0.025) 
I0630 22:05:51.948073  6183 net.cpp:1851] out3a_param_0(0.05) 
I0630 22:05:51.948083  6183 net.cpp:1851] out5a_param_0(0.05) 
I0630 22:05:51.948093  6183 net.cpp:1851] res2a_branch2a_param_0(0.05) 
I0630 22:05:51.948106  6183 net.cpp:1851] res2a_branch2b_param_0(0.0499) 
I0630 22:05:51.948117  6183 net.cpp:1851] res3a_branch2a_param_0(0.05) 
I0630 22:05:51.948128  6183 net.cpp:1851] res3a_branch2b_param_0(0.05) 
I0630 22:05:51.948140  6183 net.cpp:1851] res4a_branch2a_param_0(0.05) 
I0630 22:05:51.948153  6183 net.cpp:1851] res4a_branch2b_param_0(0.05) 
I0630 22:05:51.948163  6183 net.cpp:1851] res5a_branch2a_param_0(0.05) 
I0630 22:05:51.948175  6183 net.cpp:1851] res5a_branch2b_param_0(0.05) 
I0630 22:05:51.948186  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (134479/2.69808e+06) 0.0498
I0630 22:05:52.129348  6183 solver.cpp:290] Iteration 1000 (5.18559 iter/s, 19.2842s/100 iter), loss = 0.0686194
I0630 22:05:52.129374  6183 solver.cpp:309]     Train net output #0: loss = 0.0686193 (* 1 = 0.0686193 loss)
I0630 22:05:52.129380  6183 sgd_solver.cpp:106] Iteration 1000, lr = 1e-05
I0630 22:05:52.130363  6183 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.1
I0630 22:05:52.549183  6183 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 22:06:11.802105  6183 solver.cpp:290] Iteration 1100 (5.08332 iter/s, 19.6722s/100 iter), loss = 0.0868463
I0630 22:06:11.802129  6183 solver.cpp:309]     Train net output #0: loss = 0.0868464 (* 1 = 0.0868464 loss)
I0630 22:06:11.802135  6183 sgd_solver.cpp:106] Iteration 1100, lr = 1e-05
I0630 22:06:31.617329  6183 solver.cpp:290] Iteration 1200 (5.04677 iter/s, 19.8147s/100 iter), loss = 0.0489149
I0630 22:06:31.617419  6183 solver.cpp:309]     Train net output #0: loss = 0.0489149 (* 1 = 0.0489149 loss)
I0630 22:06:31.617427  6183 sgd_solver.cpp:106] Iteration 1200, lr = 1e-05
I0630 22:06:51.121191  6183 solver.cpp:290] Iteration 1300 (5.12735 iter/s, 19.5032s/100 iter), loss = 0.0665028
I0630 22:06:51.121213  6183 solver.cpp:309]     Train net output #0: loss = 0.0665028 (* 1 = 0.0665028 loss)
I0630 22:06:51.121220  6183 sgd_solver.cpp:106] Iteration 1300, lr = 1e-05
I0630 22:07:10.655335  6183 solver.cpp:290] Iteration 1400 (5.11939 iter/s, 19.5336s/100 iter), loss = 0.165664
I0630 22:07:10.655401  6183 solver.cpp:309]     Train net output #0: loss = 0.165664 (* 1 = 0.165664 loss)
I0630 22:07:10.655413  6183 sgd_solver.cpp:106] Iteration 1400, lr = 1e-05
I0630 22:07:30.146548  6183 solver.cpp:290] Iteration 1500 (5.13067 iter/s, 19.4906s/100 iter), loss = 0.0458943
I0630 22:07:30.146574  6183 solver.cpp:309]     Train net output #0: loss = 0.0458943 (* 1 = 0.0458943 loss)
I0630 22:07:30.146584  6183 sgd_solver.cpp:106] Iteration 1500, lr = 1e-05
I0630 22:07:49.782768  6183 solver.cpp:290] Iteration 1600 (5.09277 iter/s, 19.6357s/100 iter), loss = 0.0545988
I0630 22:07:49.782850  6183 solver.cpp:309]     Train net output #0: loss = 0.0545988 (* 1 = 0.0545988 loss)
I0630 22:07:49.782858  6183 sgd_solver.cpp:106] Iteration 1600, lr = 1e-05
I0630 22:08:09.089679  6183 solver.cpp:290] Iteration 1700 (5.17965 iter/s, 19.3063s/100 iter), loss = 0.0504498
I0630 22:08:09.089701  6183 solver.cpp:309]     Train net output #0: loss = 0.0504498 (* 1 = 0.0504498 loss)
I0630 22:08:09.089709  6183 sgd_solver.cpp:106] Iteration 1700, lr = 1e-05
I0630 22:08:28.432071  6183 solver.cpp:290] Iteration 1800 (5.17014 iter/s, 19.3418s/100 iter), loss = 0.102232
I0630 22:08:28.432155  6183 solver.cpp:309]     Train net output #0: loss = 0.102232 (* 1 = 0.102232 loss)
I0630 22:08:28.432168  6183 sgd_solver.cpp:106] Iteration 1800, lr = 1e-05
I0630 22:08:47.920644  6183 solver.cpp:290] Iteration 1900 (5.13137 iter/s, 19.488s/100 iter), loss = 0.0813589
I0630 22:08:47.920666  6183 solver.cpp:309]     Train net output #0: loss = 0.0813589 (* 1 = 0.0813589 loss)
I0630 22:08:47.920673  6183 sgd_solver.cpp:106] Iteration 1900, lr = 1e-05
I0630 22:09:07.088467  6183 solver.cpp:354] Sparsity after update:
I0630 22:09:07.090335  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 22:09:07.090342  6183 net.cpp:1851] conv1a_param_0(0.05) 
I0630 22:09:07.090349  6183 net.cpp:1851] conv1b_param_0(0.0998) 
I0630 22:09:07.090353  6183 net.cpp:1851] ctx_conv1_param_0(0.1) 
I0630 22:09:07.090354  6183 net.cpp:1851] ctx_conv2_param_0(0.1) 
I0630 22:09:07.090356  6183 net.cpp:1851] ctx_conv3_param_0(0.1) 
I0630 22:09:07.090358  6183 net.cpp:1851] ctx_conv4_param_0(0.1) 
I0630 22:09:07.090360  6183 net.cpp:1851] ctx_final_param_0(0.05) 
I0630 22:09:07.090363  6183 net.cpp:1851] out3a_param_0(0.1) 
I0630 22:09:07.090364  6183 net.cpp:1851] out5a_param_0(0.1) 
I0630 22:09:07.090366  6183 net.cpp:1851] res2a_branch2a_param_0(0.1) 
I0630 22:09:07.090368  6183 net.cpp:1851] res2a_branch2b_param_0(0.0999) 
I0630 22:09:07.090370  6183 net.cpp:1851] res3a_branch2a_param_0(0.1) 
I0630 22:09:07.090371  6183 net.cpp:1851] res3a_branch2b_param_0(0.1) 
I0630 22:09:07.090374  6183 net.cpp:1851] res4a_branch2a_param_0(0.1) 
I0630 22:09:07.090376  6183 net.cpp:1851] res4a_branch2b_param_0(0.1) 
I0630 22:09:07.090378  6183 net.cpp:1851] res5a_branch2a_param_0(0.1) 
I0630 22:09:07.090380  6183 net.cpp:1851] res5a_branch2b_param_0(0.1) 
I0630 22:09:07.090382  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (269082/2.69808e+06) 0.0997
I0630 22:09:07.090525  6183 solver.cpp:471] Iteration 2000, Testing net (#0)
I0630 22:10:45.227181  6183 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.93917
I0630 22:10:45.227277  6183 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.995972
I0630 22:10:45.227283  6183 solver.cpp:544]     Test net output #2: loss = 0.129458 (* 1 = 0.129458 loss)
I0630 22:10:45.452996  6183 solver.cpp:290] Iteration 2000 (0.850852 iter/s, 117.529s/100 iter), loss = 0.0562635
I0630 22:10:45.453022  6183 solver.cpp:309]     Train net output #0: loss = 0.0562635 (* 1 = 0.0562635 loss)
I0630 22:10:45.453028  6183 sgd_solver.cpp:106] Iteration 2000, lr = 1e-05
I0630 22:10:45.454008  6183 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.15
I0630 22:10:45.954640  6183 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 22:11:16.851387  6183 solver.cpp:290] Iteration 2100 (3.18497 iter/s, 31.3975s/100 iter), loss = 0.0588866
I0630 22:11:16.851449  6183 solver.cpp:309]     Train net output #0: loss = 0.0588866 (* 1 = 0.0588866 loss)
I0630 22:11:16.851460  6183 sgd_solver.cpp:106] Iteration 2100, lr = 1e-05
I0630 22:13:12.130020  6276 blocking_queue.cpp:50] Waiting for data
I0630 22:13:14.141876  6183 solver.cpp:290] Iteration 2200 (0.852607 iter/s, 117.287s/100 iter), loss = 0.085147
I0630 22:13:14.141902  6183 solver.cpp:309]     Train net output #0: loss = 0.0851469 (* 1 = 0.0851469 loss)
I0630 22:13:14.141909  6183 sgd_solver.cpp:106] Iteration 2200, lr = 1e-05
I0630 22:13:59.089124  6183 solver.cpp:290] Iteration 2300 (2.22489 iter/s, 44.946s/100 iter), loss = 0.0516093
I0630 22:13:59.089203  6183 solver.cpp:309]     Train net output #0: loss = 0.0516093 (* 1 = 0.0516093 loss)
I0630 22:13:59.089215  6183 sgd_solver.cpp:106] Iteration 2300, lr = 1e-05
I0630 22:14:18.141672  6183 solver.cpp:290] Iteration 2400 (5.24881 iter/s, 19.052s/100 iter), loss = 0.0447182
I0630 22:14:18.141695  6183 solver.cpp:309]     Train net output #0: loss = 0.0447182 (* 1 = 0.0447182 loss)
I0630 22:14:18.141701  6183 sgd_solver.cpp:106] Iteration 2400, lr = 1e-05
I0630 22:14:37.263224  6183 solver.cpp:290] Iteration 2500 (5.22985 iter/s, 19.121s/100 iter), loss = 0.0727641
I0630 22:14:37.263309  6183 solver.cpp:309]     Train net output #0: loss = 0.0727641 (* 1 = 0.0727641 loss)
I0630 22:14:37.263321  6183 sgd_solver.cpp:106] Iteration 2500, lr = 1e-05
I0630 22:14:56.402509  6183 solver.cpp:290] Iteration 2600 (5.22502 iter/s, 19.1387s/100 iter), loss = 0.0735061
I0630 22:14:56.402532  6183 solver.cpp:309]     Train net output #0: loss = 0.0735061 (* 1 = 0.0735061 loss)
I0630 22:14:56.402539  6183 sgd_solver.cpp:106] Iteration 2600, lr = 1e-05
I0630 22:15:15.182616  6183 solver.cpp:290] Iteration 2700 (5.32493 iter/s, 18.7796s/100 iter), loss = 0.0590858
I0630 22:15:15.182660  6183 solver.cpp:309]     Train net output #0: loss = 0.0590858 (* 1 = 0.0590858 loss)
I0630 22:15:15.182668  6183 sgd_solver.cpp:106] Iteration 2700, lr = 1e-05
I0630 22:15:34.275899  6183 solver.cpp:290] Iteration 2800 (5.2376 iter/s, 19.0927s/100 iter), loss = 0.0676441
I0630 22:15:34.275921  6183 solver.cpp:309]     Train net output #0: loss = 0.067644 (* 1 = 0.067644 loss)
I0630 22:15:34.275928  6183 sgd_solver.cpp:106] Iteration 2800, lr = 1e-05
I0630 22:15:53.327910  6183 solver.cpp:290] Iteration 2900 (5.24894 iter/s, 19.0515s/100 iter), loss = 0.0456141
I0630 22:15:53.327961  6183 solver.cpp:309]     Train net output #0: loss = 0.0456141 (* 1 = 0.0456141 loss)
I0630 22:15:53.327970  6183 sgd_solver.cpp:106] Iteration 2900, lr = 1e-05
I0630 22:16:12.623028  6183 solver.cpp:354] Sparsity after update:
I0630 22:16:12.701439  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 22:16:12.701457  6183 net.cpp:1851] conv1a_param_0(0.075) 
I0630 22:16:12.701465  6183 net.cpp:1851] conv1b_param_0(0.15) 
I0630 22:16:12.701467  6183 net.cpp:1851] ctx_conv1_param_0(0.15) 
I0630 22:16:12.701469  6183 net.cpp:1851] ctx_conv2_param_0(0.15) 
I0630 22:16:12.701472  6183 net.cpp:1851] ctx_conv3_param_0(0.15) 
I0630 22:16:12.701473  6183 net.cpp:1851] ctx_conv4_param_0(0.15) 
I0630 22:16:12.701475  6183 net.cpp:1851] ctx_final_param_0(0.075) 
I0630 22:16:12.701478  6183 net.cpp:1851] out3a_param_0(0.15) 
I0630 22:16:12.701479  6183 net.cpp:1851] out5a_param_0(0.15) 
I0630 22:16:12.701481  6183 net.cpp:1851] res2a_branch2a_param_0(0.15) 
I0630 22:16:12.701483  6183 net.cpp:1851] res2a_branch2b_param_0(0.15) 
I0630 22:16:12.701485  6183 net.cpp:1851] res3a_branch2a_param_0(0.15) 
I0630 22:16:12.701488  6183 net.cpp:1851] res3a_branch2b_param_0(0.15) 
I0630 22:16:12.701490  6183 net.cpp:1851] res4a_branch2a_param_0(0.15) 
I0630 22:16:12.701493  6183 net.cpp:1851] res4a_branch2b_param_0(0.15) 
I0630 22:16:12.701493  6183 net.cpp:1851] res5a_branch2a_param_0(0.15) 
I0630 22:16:12.701495  6183 net.cpp:1851] res5a_branch2b_param_0(0.15) 
I0630 22:16:12.701498  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (403642/2.69808e+06) 0.15
I0630 22:16:12.874223  6183 solver.cpp:290] Iteration 3000 (5.11621 iter/s, 19.5457s/100 iter), loss = 0.0736833
I0630 22:16:12.874248  6183 solver.cpp:309]     Train net output #0: loss = 0.0736833 (* 1 = 0.0736833 loss)
I0630 22:16:12.874255  6183 sgd_solver.cpp:106] Iteration 3000, lr = 1e-05
I0630 22:16:12.875509  6183 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.2
I0630 22:16:13.467013  6183 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 22:16:32.586846  6183 solver.cpp:290] Iteration 3100 (5.07303 iter/s, 19.7121s/100 iter), loss = 0.0898425
I0630 22:16:32.586920  6183 solver.cpp:309]     Train net output #0: loss = 0.0898425 (* 1 = 0.0898425 loss)
I0630 22:16:32.586927  6183 sgd_solver.cpp:106] Iteration 3100, lr = 1e-05
I0630 22:16:51.757342  6183 solver.cpp:290] Iteration 3200 (5.21651 iter/s, 19.1699s/100 iter), loss = 0.0581685
I0630 22:16:51.757365  6183 solver.cpp:309]     Train net output #0: loss = 0.0581685 (* 1 = 0.0581685 loss)
I0630 22:16:51.757372  6183 sgd_solver.cpp:106] Iteration 3200, lr = 1e-05
I0630 22:17:10.864214  6183 solver.cpp:290] Iteration 3300 (5.23387 iter/s, 19.1063s/100 iter), loss = 0.0537112
I0630 22:17:10.864286  6183 solver.cpp:309]     Train net output #0: loss = 0.0537112 (* 1 = 0.0537112 loss)
I0630 22:17:10.864295  6183 sgd_solver.cpp:106] Iteration 3300, lr = 1e-05
I0630 22:17:29.757051  6183 solver.cpp:290] Iteration 3400 (5.29317 iter/s, 18.8923s/100 iter), loss = 0.0992056
I0630 22:17:29.757076  6183 solver.cpp:309]     Train net output #0: loss = 0.0992055 (* 1 = 0.0992055 loss)
I0630 22:17:29.757082  6183 sgd_solver.cpp:106] Iteration 3400, lr = 1e-05
I0630 22:17:48.817209  6183 solver.cpp:290] Iteration 3500 (5.24669 iter/s, 19.0596s/100 iter), loss = 0.0609058
I0630 22:17:48.817301  6183 solver.cpp:309]     Train net output #0: loss = 0.0609058 (* 1 = 0.0609058 loss)
I0630 22:17:48.817312  6183 sgd_solver.cpp:106] Iteration 3500, lr = 1e-05
I0630 22:18:07.746912  6183 solver.cpp:290] Iteration 3600 (5.28287 iter/s, 18.9291s/100 iter), loss = 0.0426924
I0630 22:18:07.746933  6183 solver.cpp:309]     Train net output #0: loss = 0.0426924 (* 1 = 0.0426924 loss)
I0630 22:18:07.746940  6183 sgd_solver.cpp:106] Iteration 3600, lr = 1e-05
I0630 22:18:26.834504  6183 solver.cpp:290] Iteration 3700 (5.23915 iter/s, 19.0871s/100 iter), loss = 0.089163
I0630 22:18:26.834581  6183 solver.cpp:309]     Train net output #0: loss = 0.089163 (* 1 = 0.089163 loss)
I0630 22:18:26.834592  6183 sgd_solver.cpp:106] Iteration 3700, lr = 1e-05
I0630 22:18:46.003108  6183 solver.cpp:290] Iteration 3800 (5.21702 iter/s, 19.168s/100 iter), loss = 0.0776446
I0630 22:18:46.003134  6183 solver.cpp:309]     Train net output #0: loss = 0.0776446 (* 1 = 0.0776446 loss)
I0630 22:18:46.003144  6183 sgd_solver.cpp:106] Iteration 3800, lr = 1e-05
I0630 22:19:05.273636  6183 solver.cpp:290] Iteration 3900 (5.18942 iter/s, 19.27s/100 iter), loss = 0.0738207
I0630 22:19:05.273696  6183 solver.cpp:309]     Train net output #0: loss = 0.0738206 (* 1 = 0.0738206 loss)
I0630 22:19:05.273708  6183 sgd_solver.cpp:106] Iteration 3900, lr = 1e-05
I0630 22:19:24.423934  6183 solver.cpp:354] Sparsity after update:
I0630 22:19:24.425864  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 22:19:24.425873  6183 net.cpp:1851] conv1a_param_0(0.1) 
I0630 22:19:24.425879  6183 net.cpp:1851] conv1b_param_0(0.2) 
I0630 22:19:24.425881  6183 net.cpp:1851] ctx_conv1_param_0(0.2) 
I0630 22:19:24.425884  6183 net.cpp:1851] ctx_conv2_param_0(0.2) 
I0630 22:19:24.425885  6183 net.cpp:1851] ctx_conv3_param_0(0.2) 
I0630 22:19:24.425887  6183 net.cpp:1851] ctx_conv4_param_0(0.2) 
I0630 22:19:24.425889  6183 net.cpp:1851] ctx_final_param_0(0.1) 
I0630 22:19:24.425891  6183 net.cpp:1851] out3a_param_0(0.2) 
I0630 22:19:24.425894  6183 net.cpp:1851] out5a_param_0(0.2) 
I0630 22:19:24.425895  6183 net.cpp:1851] res2a_branch2a_param_0(0.2) 
I0630 22:19:24.425896  6183 net.cpp:1851] res2a_branch2b_param_0(0.2) 
I0630 22:19:24.425899  6183 net.cpp:1851] res3a_branch2a_param_0(0.2) 
I0630 22:19:24.425900  6183 net.cpp:1851] res3a_branch2b_param_0(0.2) 
I0630 22:19:24.425902  6183 net.cpp:1851] res4a_branch2a_param_0(0.2) 
I0630 22:19:24.425904  6183 net.cpp:1851] res4a_branch2b_param_0(0.2) 
I0630 22:19:24.425907  6183 net.cpp:1851] res5a_branch2a_param_0(0.2) 
I0630 22:19:24.425911  6183 net.cpp:1851] res5a_branch2b_param_0(0.2) 
I0630 22:19:24.425918  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (538209/2.69808e+06) 0.199
I0630 22:19:24.426061  6183 solver.cpp:471] Iteration 4000, Testing net (#0)
I0630 22:21:04.024420  6183 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.939086
I0630 22:21:04.024549  6183 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.995829
I0630 22:21:04.024559  6183 solver.cpp:544]     Test net output #2: loss = 0.129876 (* 1 = 0.129876 loss)
I0630 22:21:04.238117  6183 solver.cpp:290] Iteration 4000 (0.84061 iter/s, 118.961s/100 iter), loss = 0.0646758
I0630 22:21:04.238143  6183 solver.cpp:309]     Train net output #0: loss = 0.0646758 (* 1 = 0.0646758 loss)
I0630 22:21:04.238152  6183 sgd_solver.cpp:106] Iteration 4000, lr = 1e-05
I0630 22:21:04.239123  6183 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.25
I0630 22:21:04.908326  6183 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 22:21:38.386248  6183 solver.cpp:290] Iteration 4100 (2.9285 iter/s, 34.1472s/100 iter), loss = 0.0769122
I0630 22:21:38.386291  6183 solver.cpp:309]     Train net output #0: loss = 0.0769122 (* 1 = 0.0769122 loss)
I0630 22:21:38.386299  6183 sgd_solver.cpp:106] Iteration 4100, lr = 1e-05
I0630 22:22:00.754904  6325 blocking_queue.cpp:50] Waiting for data
I0630 22:23:15.024941  6183 solver.cpp:290] Iteration 4200 (1.03481 iter/s, 96.636s/100 iter), loss = 0.0672835
I0630 22:23:15.025072  6183 solver.cpp:309]     Train net output #0: loss = 0.0672835 (* 1 = 0.0672835 loss)
I0630 22:23:15.025084  6183 sgd_solver.cpp:106] Iteration 4200, lr = 1e-05
I0630 22:23:37.178468  6183 solver.cpp:290] Iteration 4300 (4.5141 iter/s, 22.1528s/100 iter), loss = 0.0963314
I0630 22:23:37.178491  6183 solver.cpp:309]     Train net output #0: loss = 0.0963314 (* 1 = 0.0963314 loss)
I0630 22:23:37.178498  6183 sgd_solver.cpp:106] Iteration 4300, lr = 1e-05
I0630 22:23:56.230330  6183 solver.cpp:290] Iteration 4400 (5.24898 iter/s, 19.0513s/100 iter), loss = 0.0511291
I0630 22:23:56.230417  6183 solver.cpp:309]     Train net output #0: loss = 0.0511291 (* 1 = 0.0511291 loss)
I0630 22:23:56.230428  6183 sgd_solver.cpp:106] Iteration 4400, lr = 1e-05
I0630 22:24:15.322772  6183 solver.cpp:290] Iteration 4500 (5.23784 iter/s, 19.0918s/100 iter), loss = 0.0698602
I0630 22:24:15.322800  6183 solver.cpp:309]     Train net output #0: loss = 0.0698602 (* 1 = 0.0698602 loss)
I0630 22:24:15.322809  6183 sgd_solver.cpp:106] Iteration 4500, lr = 1e-05
I0630 22:24:34.340831  6183 solver.cpp:290] Iteration 4600 (5.25831 iter/s, 19.0175s/100 iter), loss = 0.104008
I0630 22:24:34.340898  6183 solver.cpp:309]     Train net output #0: loss = 0.104008 (* 1 = 0.104008 loss)
I0630 22:24:34.340906  6183 sgd_solver.cpp:106] Iteration 4600, lr = 1e-05
I0630 22:24:53.559495  6183 solver.cpp:290] Iteration 4700 (5.20343 iter/s, 19.2181s/100 iter), loss = 0.0522869
I0630 22:24:53.559520  6183 solver.cpp:309]     Train net output #0: loss = 0.0522869 (* 1 = 0.0522869 loss)
I0630 22:24:53.559526  6183 sgd_solver.cpp:106] Iteration 4700, lr = 1e-05
I0630 22:25:12.983961  6183 solver.cpp:290] Iteration 4800 (5.14829 iter/s, 19.4239s/100 iter), loss = 0.0666827
I0630 22:25:12.984007  6183 solver.cpp:309]     Train net output #0: loss = 0.0666826 (* 1 = 0.0666826 loss)
I0630 22:25:12.984015  6183 sgd_solver.cpp:106] Iteration 4800, lr = 1e-05
I0630 22:25:32.000849  6183 solver.cpp:290] Iteration 4900 (5.25864 iter/s, 19.0163s/100 iter), loss = 0.0480318
I0630 22:25:32.000869  6183 solver.cpp:309]     Train net output #0: loss = 0.0480318 (* 1 = 0.0480318 loss)
I0630 22:25:32.000876  6183 sgd_solver.cpp:106] Iteration 4900, lr = 1e-05
I0630 22:25:50.839964  6183 solver.cpp:354] Sparsity after update:
I0630 22:25:50.917209  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 22:25:50.917224  6183 net.cpp:1851] conv1a_param_0(0.125) 
I0630 22:25:50.917232  6183 net.cpp:1851] conv1b_param_0(0.25) 
I0630 22:25:50.917234  6183 net.cpp:1851] ctx_conv1_param_0(0.25) 
I0630 22:25:50.917237  6183 net.cpp:1851] ctx_conv2_param_0(0.25) 
I0630 22:25:50.917238  6183 net.cpp:1851] ctx_conv3_param_0(0.25) 
I0630 22:25:50.917240  6183 net.cpp:1851] ctx_conv4_param_0(0.25) 
I0630 22:25:50.917243  6183 net.cpp:1851] ctx_final_param_0(0.125) 
I0630 22:25:50.917244  6183 net.cpp:1851] out3a_param_0(0.25) 
I0630 22:25:50.917246  6183 net.cpp:1851] out5a_param_0(0.25) 
I0630 22:25:50.917248  6183 net.cpp:1851] res2a_branch2a_param_0(0.25) 
I0630 22:25:50.917250  6183 net.cpp:1851] res2a_branch2b_param_0(0.25) 
I0630 22:25:50.917253  6183 net.cpp:1851] res3a_branch2a_param_0(0.25) 
I0630 22:25:50.917254  6183 net.cpp:1851] res3a_branch2b_param_0(0.25) 
I0630 22:25:50.917256  6183 net.cpp:1851] res4a_branch2a_param_0(0.25) 
I0630 22:25:50.917258  6183 net.cpp:1851] res4a_branch2b_param_0(0.25) 
I0630 22:25:50.917260  6183 net.cpp:1851] res5a_branch2a_param_0(0.25) 
I0630 22:25:50.917263  6183 net.cpp:1851] res5a_branch2b_param_0(0.25) 
I0630 22:25:50.917264  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (672759/2.69808e+06) 0.249
I0630 22:25:51.094807  6183 solver.cpp:290] Iteration 5000 (5.23741 iter/s, 19.0934s/100 iter), loss = 0.0523088
I0630 22:25:51.094830  6183 solver.cpp:309]     Train net output #0: loss = 0.0523088 (* 1 = 0.0523088 loss)
I0630 22:25:51.094837  6183 sgd_solver.cpp:106] Iteration 5000, lr = 1e-05
I0630 22:25:51.095716  6183 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.3
I0630 22:25:51.840116  6183 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 22:26:11.024863  6183 solver.cpp:290] Iteration 5100 (5.01769 iter/s, 19.9295s/100 iter), loss = 0.0441847
I0630 22:26:11.024885  6183 solver.cpp:309]     Train net output #0: loss = 0.0441846 (* 1 = 0.0441846 loss)
I0630 22:26:11.024893  6183 sgd_solver.cpp:106] Iteration 5100, lr = 1e-05
I0630 22:26:29.995564  6183 solver.cpp:290] Iteration 5200 (5.27144 iter/s, 18.9702s/100 iter), loss = 0.0598185
I0630 22:26:29.995661  6183 solver.cpp:309]     Train net output #0: loss = 0.0598185 (* 1 = 0.0598185 loss)
I0630 22:26:29.995673  6183 sgd_solver.cpp:106] Iteration 5200, lr = 1e-05
I0630 22:26:48.930130  6183 solver.cpp:290] Iteration 5300 (5.28151 iter/s, 18.934s/100 iter), loss = 0.0691462
I0630 22:26:48.930155  6183 solver.cpp:309]     Train net output #0: loss = 0.0691462 (* 1 = 0.0691462 loss)
I0630 22:26:48.930160  6183 sgd_solver.cpp:106] Iteration 5300, lr = 1e-05
I0630 22:27:07.891180  6183 solver.cpp:290] Iteration 5400 (5.27412 iter/s, 18.9605s/100 iter), loss = 0.0597772
I0630 22:27:07.891273  6183 solver.cpp:309]     Train net output #0: loss = 0.0597772 (* 1 = 0.0597772 loss)
I0630 22:27:07.891283  6183 sgd_solver.cpp:106] Iteration 5400, lr = 1e-05
I0630 22:27:26.877667  6183 solver.cpp:290] Iteration 5500 (5.26707 iter/s, 18.9859s/100 iter), loss = 0.0618154
I0630 22:27:26.877693  6183 solver.cpp:309]     Train net output #0: loss = 0.0618153 (* 1 = 0.0618153 loss)
I0630 22:27:26.877701  6183 sgd_solver.cpp:106] Iteration 5500, lr = 1e-05
I0630 22:27:45.850256  6183 solver.cpp:290] Iteration 5600 (5.27091 iter/s, 18.9721s/100 iter), loss = 0.0759368
I0630 22:27:45.850311  6183 solver.cpp:309]     Train net output #0: loss = 0.0759368 (* 1 = 0.0759368 loss)
I0630 22:27:45.850319  6183 sgd_solver.cpp:106] Iteration 5600, lr = 1e-05
I0630 22:28:04.965492  6183 solver.cpp:290] Iteration 5700 (5.23158 iter/s, 19.1147s/100 iter), loss = 0.0541886
I0630 22:28:04.965514  6183 solver.cpp:309]     Train net output #0: loss = 0.0541885 (* 1 = 0.0541885 loss)
I0630 22:28:04.965520  6183 sgd_solver.cpp:106] Iteration 5700, lr = 1e-05
I0630 22:28:24.046955  6183 solver.cpp:290] Iteration 5800 (5.24084 iter/s, 19.0809s/100 iter), loss = 0.110019
I0630 22:28:24.047062  6183 solver.cpp:309]     Train net output #0: loss = 0.110019 (* 1 = 0.110019 loss)
I0630 22:28:24.047072  6183 sgd_solver.cpp:106] Iteration 5800, lr = 1e-05
I0630 22:28:43.195178  6183 solver.cpp:290] Iteration 5900 (5.22259 iter/s, 19.1476s/100 iter), loss = 0.11208
I0630 22:28:43.195200  6183 solver.cpp:309]     Train net output #0: loss = 0.11208 (* 1 = 0.11208 loss)
I0630 22:28:43.195210  6183 sgd_solver.cpp:106] Iteration 5900, lr = 1e-05
I0630 22:29:02.128131  6183 solver.cpp:354] Sparsity after update:
I0630 22:29:02.129823  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 22:29:02.129832  6183 net.cpp:1851] conv1a_param_0(0.15) 
I0630 22:29:02.129842  6183 net.cpp:1851] conv1b_param_0(0.3) 
I0630 22:29:02.129847  6183 net.cpp:1851] ctx_conv1_param_0(0.3) 
I0630 22:29:02.129851  6183 net.cpp:1851] ctx_conv2_param_0(0.3) 
I0630 22:29:02.129855  6183 net.cpp:1851] ctx_conv3_param_0(0.3) 
I0630 22:29:02.129860  6183 net.cpp:1851] ctx_conv4_param_0(0.3) 
I0630 22:29:02.129864  6183 net.cpp:1851] ctx_final_param_0(0.15) 
I0630 22:29:02.129870  6183 net.cpp:1851] out3a_param_0(0.3) 
I0630 22:29:02.129873  6183 net.cpp:1851] out5a_param_0(0.3) 
I0630 22:29:02.129878  6183 net.cpp:1851] res2a_branch2a_param_0(0.3) 
I0630 22:29:02.129881  6183 net.cpp:1851] res2a_branch2b_param_0(0.3) 
I0630 22:29:02.129885  6183 net.cpp:1851] res3a_branch2a_param_0(0.3) 
I0630 22:29:02.129890  6183 net.cpp:1851] res3a_branch2b_param_0(0.3) 
I0630 22:29:02.129894  6183 net.cpp:1851] res4a_branch2a_param_0(0.3) 
I0630 22:29:02.129897  6183 net.cpp:1851] res4a_branch2b_param_0(0.3) 
I0630 22:29:02.129902  6183 net.cpp:1851] res5a_branch2a_param_0(0.3) 
I0630 22:29:02.129906  6183 net.cpp:1851] res5a_branch2b_param_0(0.3) 
I0630 22:29:02.129911  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (807305/2.69808e+06) 0.299
I0630 22:29:02.130126  6183 solver.cpp:471] Iteration 6000, Testing net (#0)
I0630 22:30:42.730092  6183 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.938581
I0630 22:30:42.730283  6183 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.995838
I0630 22:30:42.730293  6183 solver.cpp:544]     Test net output #2: loss = 0.131583 (* 1 = 0.131583 loss)
I0630 22:30:42.935739  6183 solver.cpp:290] Iteration 6000 (0.835161 iter/s, 119.737s/100 iter), loss = 0.0696933
I0630 22:30:42.935762  6183 solver.cpp:309]     Train net output #0: loss = 0.0696932 (* 1 = 0.0696932 loss)
I0630 22:30:42.935770  6183 sgd_solver.cpp:106] Iteration 6000, lr = 1e-05
I0630 22:30:42.936740  6183 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.35
I0630 22:30:43.769938  6183 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 22:31:06.964707  6183 solver.cpp:290] Iteration 6100 (4.16176 iter/s, 24.0283s/100 iter), loss = 0.0722149
I0630 22:31:06.964736  6183 solver.cpp:309]     Train net output #0: loss = 0.0722148 (* 1 = 0.0722148 loss)
I0630 22:31:06.964745  6183 sgd_solver.cpp:106] Iteration 6100, lr = 1e-05
I0630 22:31:36.119367  6276 blocking_queue.cpp:50] Waiting for data
I0630 22:31:55.236284  6183 solver.cpp:290] Iteration 6200 (2.07167 iter/s, 48.2702s/100 iter), loss = 0.0656174
I0630 22:31:55.236310  6183 solver.cpp:309]     Train net output #0: loss = 0.0656173 (* 1 = 0.0656173 loss)
I0630 22:31:55.236316  6183 sgd_solver.cpp:106] Iteration 6200, lr = 1e-05
I0630 22:32:17.060219  6183 solver.cpp:290] Iteration 6300 (4.58225 iter/s, 21.8233s/100 iter), loss = 0.094609
I0630 22:32:17.060274  6183 solver.cpp:309]     Train net output #0: loss = 0.0946089 (* 1 = 0.0946089 loss)
I0630 22:32:17.060286  6183 sgd_solver.cpp:106] Iteration 6300, lr = 1e-05
I0630 22:32:36.187827  6183 solver.cpp:290] Iteration 6400 (5.2282 iter/s, 19.127s/100 iter), loss = 0.0957429
I0630 22:32:36.187850  6183 solver.cpp:309]     Train net output #0: loss = 0.0957429 (* 1 = 0.0957429 loss)
I0630 22:32:36.187857  6183 sgd_solver.cpp:106] Iteration 6400, lr = 1e-05
I0630 22:32:55.142065  6183 solver.cpp:290] Iteration 6500 (5.27602 iter/s, 18.9537s/100 iter), loss = 0.0973256
I0630 22:32:55.142120  6183 solver.cpp:309]     Train net output #0: loss = 0.0973255 (* 1 = 0.0973255 loss)
I0630 22:32:55.142132  6183 sgd_solver.cpp:106] Iteration 6500, lr = 1e-05
I0630 22:33:14.369446  6183 solver.cpp:290] Iteration 6600 (5.20107 iter/s, 19.2268s/100 iter), loss = 0.102898
I0630 22:33:14.369469  6183 solver.cpp:309]     Train net output #0: loss = 0.102898 (* 1 = 0.102898 loss)
I0630 22:33:14.369477  6183 sgd_solver.cpp:106] Iteration 6600, lr = 1e-05
I0630 22:33:33.496016  6183 solver.cpp:290] Iteration 6700 (5.22848 iter/s, 19.126s/100 iter), loss = 0.0521776
I0630 22:33:33.496088  6183 solver.cpp:309]     Train net output #0: loss = 0.0521775 (* 1 = 0.0521775 loss)
I0630 22:33:33.496095  6183 sgd_solver.cpp:106] Iteration 6700, lr = 1e-05
I0630 22:33:52.875982  6183 solver.cpp:290] Iteration 6800 (5.16013 iter/s, 19.3794s/100 iter), loss = 0.0513142
I0630 22:33:52.876009  6183 solver.cpp:309]     Train net output #0: loss = 0.0513142 (* 1 = 0.0513142 loss)
I0630 22:33:52.876019  6183 sgd_solver.cpp:106] Iteration 6800, lr = 1e-05
I0630 22:34:12.092814  6183 solver.cpp:290] Iteration 6900 (5.20392 iter/s, 19.2163s/100 iter), loss = 0.0461285
I0630 22:34:12.092871  6183 solver.cpp:309]     Train net output #0: loss = 0.0461284 (* 1 = 0.0461284 loss)
I0630 22:34:12.092880  6183 sgd_solver.cpp:106] Iteration 6900, lr = 1e-05
I0630 22:34:31.859544  6183 solver.cpp:354] Sparsity after update:
I0630 22:34:31.936928  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 22:34:31.936949  6183 net.cpp:1851] conv1a_param_0(0.175) 
I0630 22:34:31.936959  6183 net.cpp:1851] conv1b_param_0(0.35) 
I0630 22:34:31.936964  6183 net.cpp:1851] ctx_conv1_param_0(0.35) 
I0630 22:34:31.936967  6183 net.cpp:1851] ctx_conv2_param_0(0.35) 
I0630 22:34:31.936971  6183 net.cpp:1851] ctx_conv3_param_0(0.35) 
I0630 22:34:31.936975  6183 net.cpp:1851] ctx_conv4_param_0(0.35) 
I0630 22:34:31.936980  6183 net.cpp:1851] ctx_final_param_0(0.175) 
I0630 22:34:31.936983  6183 net.cpp:1851] out3a_param_0(0.35) 
I0630 22:34:31.936987  6183 net.cpp:1851] out5a_param_0(0.35) 
I0630 22:34:31.936991  6183 net.cpp:1851] res2a_branch2a_param_0(0.35) 
I0630 22:34:31.936995  6183 net.cpp:1851] res2a_branch2b_param_0(0.35) 
I0630 22:34:31.937000  6183 net.cpp:1851] res3a_branch2a_param_0(0.35) 
I0630 22:34:31.937003  6183 net.cpp:1851] res3a_branch2b_param_0(0.35) 
I0630 22:34:31.937007  6183 net.cpp:1851] res4a_branch2a_param_0(0.35) 
I0630 22:34:31.937011  6183 net.cpp:1851] res4a_branch2b_param_0(0.35) 
I0630 22:34:31.937016  6183 net.cpp:1851] res5a_branch2a_param_0(0.35) 
I0630 22:34:31.937019  6183 net.cpp:1851] res5a_branch2b_param_0(0.35) 
I0630 22:34:31.937023  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (941870/2.69808e+06) 0.349
I0630 22:34:32.107156  6183 solver.cpp:290] Iteration 7000 (4.99657 iter/s, 20.0137s/100 iter), loss = 0.083873
I0630 22:34:32.107182  6183 solver.cpp:309]     Train net output #0: loss = 0.0838729 (* 1 = 0.0838729 loss)
I0630 22:34:32.107190  6183 sgd_solver.cpp:106] Iteration 7000, lr = 1e-05
I0630 22:34:32.108445  6183 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.4
I0630 22:34:33.041461  6183 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 22:34:52.154606  6183 solver.cpp:290] Iteration 7100 (4.98831 iter/s, 20.0469s/100 iter), loss = 0.10051
I0630 22:34:52.154712  6183 solver.cpp:309]     Train net output #0: loss = 0.10051 (* 1 = 0.10051 loss)
I0630 22:34:52.154722  6183 sgd_solver.cpp:106] Iteration 7100, lr = 1e-05
I0630 22:35:11.051735  6183 solver.cpp:290] Iteration 7200 (5.29198 iter/s, 18.8965s/100 iter), loss = 0.0786183
I0630 22:35:11.051759  6183 solver.cpp:309]     Train net output #0: loss = 0.0786183 (* 1 = 0.0786183 loss)
I0630 22:35:11.051765  6183 sgd_solver.cpp:106] Iteration 7200, lr = 1e-05
I0630 22:35:30.349145  6183 solver.cpp:290] Iteration 7300 (5.18219 iter/s, 19.2969s/100 iter), loss = 0.0695157
I0630 22:35:30.349249  6183 solver.cpp:309]     Train net output #0: loss = 0.0695156 (* 1 = 0.0695156 loss)
I0630 22:35:30.349257  6183 sgd_solver.cpp:106] Iteration 7300, lr = 1e-05
I0630 22:35:49.523138  6183 solver.cpp:290] Iteration 7400 (5.21557 iter/s, 19.1734s/100 iter), loss = 0.105437
I0630 22:35:49.523165  6183 solver.cpp:309]     Train net output #0: loss = 0.105437 (* 1 = 0.105437 loss)
I0630 22:35:49.523175  6183 sgd_solver.cpp:106] Iteration 7400, lr = 1e-05
I0630 22:36:08.831759  6183 solver.cpp:290] Iteration 7500 (5.17918 iter/s, 19.3081s/100 iter), loss = 0.0892907
I0630 22:36:08.831822  6183 solver.cpp:309]     Train net output #0: loss = 0.0892906 (* 1 = 0.0892906 loss)
I0630 22:36:08.831830  6183 sgd_solver.cpp:106] Iteration 7500, lr = 1e-05
I0630 22:36:28.012908  6183 solver.cpp:290] Iteration 7600 (5.21361 iter/s, 19.1806s/100 iter), loss = 0.055229
I0630 22:36:28.012935  6183 solver.cpp:309]     Train net output #0: loss = 0.0552289 (* 1 = 0.0552289 loss)
I0630 22:36:28.012943  6183 sgd_solver.cpp:106] Iteration 7600, lr = 1e-05
I0630 22:36:47.129281  6183 solver.cpp:290] Iteration 7700 (5.23127 iter/s, 19.1158s/100 iter), loss = 0.0775177
I0630 22:36:47.129333  6183 solver.cpp:309]     Train net output #0: loss = 0.0775176 (* 1 = 0.0775176 loss)
I0630 22:36:47.129341  6183 sgd_solver.cpp:106] Iteration 7700, lr = 1e-05
I0630 22:37:06.331452  6183 solver.cpp:290] Iteration 7800 (5.2079 iter/s, 19.2016s/100 iter), loss = 0.0605085
I0630 22:37:06.331480  6183 solver.cpp:309]     Train net output #0: loss = 0.0605084 (* 1 = 0.0605084 loss)
I0630 22:37:06.331490  6183 sgd_solver.cpp:106] Iteration 7800, lr = 1e-05
I0630 22:37:25.430492  6183 solver.cpp:290] Iteration 7900 (5.23601 iter/s, 19.0985s/100 iter), loss = 0.0795523
I0630 22:37:25.430567  6183 solver.cpp:309]     Train net output #0: loss = 0.0795522 (* 1 = 0.0795522 loss)
I0630 22:37:25.430575  6183 sgd_solver.cpp:106] Iteration 7900, lr = 1e-05
I0630 22:37:44.397614  6183 solver.cpp:354] Sparsity after update:
I0630 22:37:44.399538  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 22:37:44.399545  6183 net.cpp:1851] conv1a_param_0(0.2) 
I0630 22:37:44.399552  6183 net.cpp:1851] conv1b_param_0(0.4) 
I0630 22:37:44.399554  6183 net.cpp:1851] ctx_conv1_param_0(0.4) 
I0630 22:37:44.399556  6183 net.cpp:1851] ctx_conv2_param_0(0.4) 
I0630 22:37:44.399559  6183 net.cpp:1851] ctx_conv3_param_0(0.4) 
I0630 22:37:44.399560  6183 net.cpp:1851] ctx_conv4_param_0(0.4) 
I0630 22:37:44.399562  6183 net.cpp:1851] ctx_final_param_0(0.2) 
I0630 22:37:44.399564  6183 net.cpp:1851] out3a_param_0(0.4) 
I0630 22:37:44.399566  6183 net.cpp:1851] out5a_param_0(0.4) 
I0630 22:37:44.399569  6183 net.cpp:1851] res2a_branch2a_param_0(0.4) 
I0630 22:37:44.399570  6183 net.cpp:1851] res2a_branch2b_param_0(0.4) 
I0630 22:37:44.399572  6183 net.cpp:1851] res3a_branch2a_param_0(0.4) 
I0630 22:37:44.399574  6183 net.cpp:1851] res3a_branch2b_param_0(0.4) 
I0630 22:37:44.399576  6183 net.cpp:1851] res4a_branch2a_param_0(0.4) 
I0630 22:37:44.399579  6183 net.cpp:1851] res4a_branch2b_param_0(0.4) 
I0630 22:37:44.399580  6183 net.cpp:1851] res5a_branch2a_param_0(0.4) 
I0630 22:37:44.399585  6183 net.cpp:1851] res5a_branch2b_param_0(0.4) 
I0630 22:37:44.399586  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.07641e+06/2.69808e+06) 0.399
I0630 22:37:44.399718  6183 solver.cpp:471] Iteration 8000, Testing net (#0)
I0630 22:39:23.385239  6183 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.937661
I0630 22:39:23.385326  6183 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.995667
I0630 22:39:23.385334  6183 solver.cpp:544]     Test net output #2: loss = 0.133156 (* 1 = 0.133156 loss)
I0630 22:39:23.593489  6183 solver.cpp:290] Iteration 8000 (0.846312 iter/s, 118.16s/100 iter), loss = 0.0734733
I0630 22:39:23.593513  6183 solver.cpp:309]     Train net output #0: loss = 0.0734732 (* 1 = 0.0734732 loss)
I0630 22:39:23.593520  6183 sgd_solver.cpp:106] Iteration 8000, lr = 1e-05
I0630 22:39:23.594512  6183 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.45
I0630 22:39:24.630766  6183 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 22:40:09.655014  6183 solver.cpp:290] Iteration 8100 (2.17107 iter/s, 46.0602s/100 iter), loss = 0.0971045
I0630 22:40:09.655102  6183 solver.cpp:309]     Train net output #0: loss = 0.0971044 (* 1 = 0.0971044 loss)
I0630 22:40:09.655109  6183 sgd_solver.cpp:106] Iteration 8100, lr = 1e-05
I0630 22:40:12.706514  6276 blocking_queue.cpp:50] Waiting for data
I0630 22:41:18.754971  6325 blocking_queue.cpp:50] Waiting for data
I0630 22:41:22.095422  6183 solver.cpp:290] Iteration 8200 (1.38048 iter/s, 72.4383s/100 iter), loss = 0.0934549
I0630 22:41:22.095448  6183 solver.cpp:309]     Train net output #0: loss = 0.0934548 (* 1 = 0.0934548 loss)
I0630 22:41:22.095458  6183 sgd_solver.cpp:106] Iteration 8200, lr = 1e-05
I0630 22:41:43.977263  6183 solver.cpp:290] Iteration 8300 (4.57013 iter/s, 21.8812s/100 iter), loss = 0.0621539
I0630 22:41:43.977288  6183 solver.cpp:309]     Train net output #0: loss = 0.0621538 (* 1 = 0.0621538 loss)
I0630 22:41:43.977293  6183 sgd_solver.cpp:106] Iteration 8300, lr = 1e-05
I0630 22:42:02.988121  6183 solver.cpp:290] Iteration 8400 (5.2603 iter/s, 19.0103s/100 iter), loss = 0.0449747
I0630 22:42:02.988221  6183 solver.cpp:309]     Train net output #0: loss = 0.0449746 (* 1 = 0.0449746 loss)
I0630 22:42:02.988229  6183 sgd_solver.cpp:106] Iteration 8400, lr = 1e-05
I0630 22:42:22.193148  6183 solver.cpp:290] Iteration 8500 (5.20714 iter/s, 19.2044s/100 iter), loss = 0.0992109
I0630 22:42:22.193171  6183 solver.cpp:309]     Train net output #0: loss = 0.0992109 (* 1 = 0.0992109 loss)
I0630 22:42:22.193178  6183 sgd_solver.cpp:106] Iteration 8500, lr = 1e-05
I0630 22:42:41.341902  6183 solver.cpp:290] Iteration 8600 (5.22242 iter/s, 19.1482s/100 iter), loss = 0.057223
I0630 22:42:41.341950  6183 solver.cpp:309]     Train net output #0: loss = 0.057223 (* 1 = 0.057223 loss)
I0630 22:42:41.341958  6183 sgd_solver.cpp:106] Iteration 8600, lr = 1e-05
I0630 22:43:00.417954  6183 solver.cpp:290] Iteration 8700 (5.24233 iter/s, 19.0755s/100 iter), loss = 0.0830471
I0630 22:43:00.417978  6183 solver.cpp:309]     Train net output #0: loss = 0.083047 (* 1 = 0.083047 loss)
I0630 22:43:00.417984  6183 sgd_solver.cpp:106] Iteration 8700, lr = 1e-05
I0630 22:43:19.378373  6183 solver.cpp:290] Iteration 8800 (5.2743 iter/s, 18.9599s/100 iter), loss = 0.0754851
I0630 22:43:19.378419  6183 solver.cpp:309]     Train net output #0: loss = 0.075485 (* 1 = 0.075485 loss)
I0630 22:43:19.378427  6183 sgd_solver.cpp:106] Iteration 8800, lr = 1e-05
I0630 22:43:38.477306  6183 solver.cpp:290] Iteration 8900 (5.23605 iter/s, 19.0984s/100 iter), loss = 0.0664115
I0630 22:43:38.477329  6183 solver.cpp:309]     Train net output #0: loss = 0.0664114 (* 1 = 0.0664114 loss)
I0630 22:43:38.477336  6183 sgd_solver.cpp:106] Iteration 8900, lr = 1e-05
I0630 22:43:57.364380  6183 solver.cpp:354] Sparsity after update:
I0630 22:43:57.439055  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 22:43:57.439072  6183 net.cpp:1851] conv1a_param_0(0.225) 
I0630 22:43:57.439080  6183 net.cpp:1851] conv1b_param_0(0.45) 
I0630 22:43:57.439082  6183 net.cpp:1851] ctx_conv1_param_0(0.45) 
I0630 22:43:57.439085  6183 net.cpp:1851] ctx_conv2_param_0(0.45) 
I0630 22:43:57.439086  6183 net.cpp:1851] ctx_conv3_param_0(0.45) 
I0630 22:43:57.439088  6183 net.cpp:1851] ctx_conv4_param_0(0.45) 
I0630 22:43:57.439090  6183 net.cpp:1851] ctx_final_param_0(0.225) 
I0630 22:43:57.439092  6183 net.cpp:1851] out3a_param_0(0.45) 
I0630 22:43:57.439095  6183 net.cpp:1851] out5a_param_0(0.45) 
I0630 22:43:57.439096  6183 net.cpp:1851] res2a_branch2a_param_0(0.45) 
I0630 22:43:57.439100  6183 net.cpp:1851] res2a_branch2b_param_0(0.45) 
I0630 22:43:57.439101  6183 net.cpp:1851] res3a_branch2a_param_0(0.45) 
I0630 22:43:57.439103  6183 net.cpp:1851] res3a_branch2b_param_0(0.45) 
I0630 22:43:57.439105  6183 net.cpp:1851] res4a_branch2a_param_0(0.45) 
I0630 22:43:57.439107  6183 net.cpp:1851] res4a_branch2b_param_0(0.45) 
I0630 22:43:57.439110  6183 net.cpp:1851] res5a_branch2a_param_0(0.45) 
I0630 22:43:57.439111  6183 net.cpp:1851] res5a_branch2b_param_0(0.45) 
I0630 22:43:57.439113  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.21097e+06/2.69808e+06) 0.449
I0630 22:43:57.609755  6183 solver.cpp:290] Iteration 9000 (5.22687 iter/s, 19.1319s/100 iter), loss = 0.0837013
I0630 22:43:57.609782  6183 solver.cpp:309]     Train net output #0: loss = 0.0837012 (* 1 = 0.0837012 loss)
I0630 22:43:57.609789  6183 sgd_solver.cpp:106] Iteration 9000, lr = 1e-05
I0630 22:43:57.611079  6183 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.5
I0630 22:43:58.731117  6183 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 22:44:17.824980  6183 solver.cpp:290] Iteration 9100 (4.94691 iter/s, 20.2146s/100 iter), loss = 0.100986
I0630 22:44:17.825006  6183 solver.cpp:309]     Train net output #0: loss = 0.100986 (* 1 = 0.100986 loss)
I0630 22:44:17.825012  6183 sgd_solver.cpp:106] Iteration 9100, lr = 1e-05
I0630 22:44:36.895167  6183 solver.cpp:290] Iteration 9200 (5.24394 iter/s, 19.0696s/100 iter), loss = 0.0863522
I0630 22:44:36.895244  6183 solver.cpp:309]     Train net output #0: loss = 0.0863522 (* 1 = 0.0863522 loss)
I0630 22:44:36.895253  6183 sgd_solver.cpp:106] Iteration 9200, lr = 1e-05
I0630 22:44:56.065157  6183 solver.cpp:290] Iteration 9300 (5.21665 iter/s, 19.1694s/100 iter), loss = 0.112267
I0630 22:44:56.065181  6183 solver.cpp:309]     Train net output #0: loss = 0.112267 (* 1 = 0.112267 loss)
I0630 22:44:56.065191  6183 sgd_solver.cpp:106] Iteration 9300, lr = 1e-05
I0630 22:45:15.302105  6183 solver.cpp:290] Iteration 9400 (5.19848 iter/s, 19.2364s/100 iter), loss = 0.0904237
I0630 22:45:15.302162  6183 solver.cpp:309]     Train net output #0: loss = 0.0904236 (* 1 = 0.0904236 loss)
I0630 22:45:15.302171  6183 sgd_solver.cpp:106] Iteration 9400, lr = 1e-05
I0630 22:45:34.449955  6183 solver.cpp:290] Iteration 9500 (5.22268 iter/s, 19.1473s/100 iter), loss = 0.0912011
I0630 22:45:34.449977  6183 solver.cpp:309]     Train net output #0: loss = 0.091201 (* 1 = 0.091201 loss)
I0630 22:45:34.449985  6183 sgd_solver.cpp:106] Iteration 9500, lr = 1e-05
I0630 22:45:53.458675  6183 solver.cpp:290] Iteration 9600 (5.26089 iter/s, 19.0082s/100 iter), loss = 0.0832914
I0630 22:45:53.458721  6183 solver.cpp:309]     Train net output #0: loss = 0.0832914 (* 1 = 0.0832914 loss)
I0630 22:45:53.458732  6183 sgd_solver.cpp:106] Iteration 9600, lr = 1e-05
I0630 22:46:12.608312  6183 solver.cpp:290] Iteration 9700 (5.22219 iter/s, 19.1491s/100 iter), loss = 0.0855816
I0630 22:46:12.608335  6183 solver.cpp:309]     Train net output #0: loss = 0.0855816 (* 1 = 0.0855816 loss)
I0630 22:46:12.608342  6183 sgd_solver.cpp:106] Iteration 9700, lr = 1e-05
I0630 22:46:31.593533  6183 solver.cpp:290] Iteration 9800 (5.26741 iter/s, 18.9847s/100 iter), loss = 0.0557295
I0630 22:46:31.593613  6183 solver.cpp:309]     Train net output #0: loss = 0.0557295 (* 1 = 0.0557295 loss)
I0630 22:46:31.593621  6183 sgd_solver.cpp:106] Iteration 9800, lr = 1e-05
I0630 22:46:50.636198  6183 solver.cpp:290] Iteration 9900 (5.25153 iter/s, 19.0421s/100 iter), loss = 0.14833
I0630 22:46:50.636225  6183 solver.cpp:309]     Train net output #0: loss = 0.14833 (* 1 = 0.14833 loss)
I0630 22:46:50.636234  6183 sgd_solver.cpp:106] Iteration 9900, lr = 1e-05
I0630 22:47:09.580633  6183 solver.cpp:598] Snapshotting to binary proto file training/cityscapes20_jsegnet21v2_2017-06-30_19-26-17/sparse/cityscapes20_jsegnet21v2_iter_10000.caffemodel
I0630 22:47:09.759845  6183 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cityscapes20_jsegnet21v2_2017-06-30_19-26-17/sparse/cityscapes20_jsegnet21v2_iter_10000.solverstate
I0630 22:47:09.776487  6183 solver.cpp:354] Sparsity after update:
I0630 22:47:09.777989  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 22:47:09.777997  6183 net.cpp:1851] conv1a_param_0(0.25) 
I0630 22:47:09.778005  6183 net.cpp:1851] conv1b_param_0(0.5) 
I0630 22:47:09.778007  6183 net.cpp:1851] ctx_conv1_param_0(0.5) 
I0630 22:47:09.778009  6183 net.cpp:1851] ctx_conv2_param_0(0.5) 
I0630 22:47:09.778012  6183 net.cpp:1851] ctx_conv3_param_0(0.5) 
I0630 22:47:09.778013  6183 net.cpp:1851] ctx_conv4_param_0(0.5) 
I0630 22:47:09.778015  6183 net.cpp:1851] ctx_final_param_0(0.25) 
I0630 22:47:09.778017  6183 net.cpp:1851] out3a_param_0(0.5) 
I0630 22:47:09.778019  6183 net.cpp:1851] out5a_param_0(0.5) 
I0630 22:47:09.778022  6183 net.cpp:1851] res2a_branch2a_param_0(0.5) 
I0630 22:47:09.778023  6183 net.cpp:1851] res2a_branch2b_param_0(0.5) 
I0630 22:47:09.778025  6183 net.cpp:1851] res3a_branch2a_param_0(0.5) 
I0630 22:47:09.778028  6183 net.cpp:1851] res3a_branch2b_param_0(0.5) 
I0630 22:47:09.778029  6183 net.cpp:1851] res4a_branch2a_param_0(0.5) 
I0630 22:47:09.778031  6183 net.cpp:1851] res4a_branch2b_param_0(0.5) 
I0630 22:47:09.778033  6183 net.cpp:1851] res5a_branch2a_param_0(0.5) 
I0630 22:47:09.778035  6183 net.cpp:1851] res5a_branch2b_param_0(0.5) 
I0630 22:47:09.778038  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.34554e+06/2.69808e+06) 0.499
I0630 22:47:09.778187  6183 solver.cpp:471] Iteration 10000, Testing net (#0)
I0630 22:48:47.409083  6183 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.93542
I0630 22:48:47.409209  6183 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.995143
I0630 22:48:47.409219  6183 solver.cpp:544]     Test net output #2: loss = 0.140101 (* 1 = 0.140101 loss)
I0630 22:48:47.619235  6183 solver.cpp:290] Iteration 10000 (0.854848 iter/s, 116.98s/100 iter), loss = 0.0528353
I0630 22:48:47.619259  6183 solver.cpp:309]     Train net output #0: loss = 0.0528352 (* 1 = 0.0528352 loss)
I0630 22:48:47.619709  6183 sgd_solver.cpp:106] Iteration 10000, lr = 1e-05
I0630 22:48:47.620530  6183 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.55
I0630 22:48:48.869014  6183 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 22:49:08.176195  6183 solver.cpp:290] Iteration 10100 (4.86467 iter/s, 20.5564s/100 iter), loss = 0.0880809
I0630 22:49:08.176220  6183 solver.cpp:309]     Train net output #0: loss = 0.0880809 (* 1 = 0.0880809 loss)
I0630 22:49:08.176228  6183 sgd_solver.cpp:106] Iteration 10100, lr = 1e-05
I0630 22:49:49.492882  6344 blocking_queue.cpp:50] Data layer prefetch queue empty
I0630 22:49:57.130920  6183 solver.cpp:290] Iteration 10200 (2.04276 iter/s, 48.9534s/100 iter), loss = 0.0830373
I0630 22:49:57.130944  6183 solver.cpp:309]     Train net output #0: loss = 0.0830373 (* 1 = 0.0830373 loss)
I0630 22:49:57.130951  6183 sgd_solver.cpp:106] Iteration 10200, lr = 1e-05
I0630 22:50:16.359863  6183 solver.cpp:290] Iteration 10300 (5.20064 iter/s, 19.2284s/100 iter), loss = 0.0882273
I0630 22:50:16.359886  6183 solver.cpp:309]     Train net output #0: loss = 0.0882272 (* 1 = 0.0882272 loss)
I0630 22:50:16.359894  6183 sgd_solver.cpp:106] Iteration 10300, lr = 1e-05
I0630 22:50:35.475009  6183 solver.cpp:290] Iteration 10400 (5.2316 iter/s, 19.1146s/100 iter), loss = 0.101522
I0630 22:50:35.475055  6183 solver.cpp:309]     Train net output #0: loss = 0.101522 (* 1 = 0.101522 loss)
I0630 22:50:35.475062  6183 sgd_solver.cpp:106] Iteration 10400, lr = 1e-05
I0630 22:50:54.395344  6183 solver.cpp:290] Iteration 10500 (5.28548 iter/s, 18.9198s/100 iter), loss = 0.0616472
I0630 22:50:54.395367  6183 solver.cpp:309]     Train net output #0: loss = 0.0616471 (* 1 = 0.0616471 loss)
I0630 22:50:54.395375  6183 sgd_solver.cpp:106] Iteration 10500, lr = 1e-05
I0630 22:51:13.494526  6183 solver.cpp:290] Iteration 10600 (5.23598 iter/s, 19.0986s/100 iter), loss = 0.0647788
I0630 22:51:13.494576  6183 solver.cpp:309]     Train net output #0: loss = 0.0647787 (* 1 = 0.0647787 loss)
I0630 22:51:13.494585  6183 sgd_solver.cpp:106] Iteration 10600, lr = 1e-05
I0630 22:51:32.504375  6183 solver.cpp:290] Iteration 10700 (5.26059 iter/s, 19.0093s/100 iter), loss = 0.131475
I0630 22:51:32.504396  6183 solver.cpp:309]     Train net output #0: loss = 0.131475 (* 1 = 0.131475 loss)
I0630 22:51:32.504403  6183 sgd_solver.cpp:106] Iteration 10700, lr = 1e-05
I0630 22:51:51.574931  6183 solver.cpp:290] Iteration 10800 (5.24384 iter/s, 19.07s/100 iter), loss = 0.0429831
I0630 22:51:51.575008  6183 solver.cpp:309]     Train net output #0: loss = 0.042983 (* 1 = 0.042983 loss)
I0630 22:51:51.575016  6183 sgd_solver.cpp:106] Iteration 10800, lr = 1e-05
I0630 22:52:10.979249  6183 solver.cpp:290] Iteration 10900 (5.15365 iter/s, 19.4037s/100 iter), loss = 0.0849775
I0630 22:52:10.979272  6183 solver.cpp:309]     Train net output #0: loss = 0.0849775 (* 1 = 0.0849775 loss)
I0630 22:52:10.979279  6183 sgd_solver.cpp:106] Iteration 10900, lr = 1e-05
I0630 22:52:30.005993  6183 solver.cpp:354] Sparsity after update:
I0630 22:52:30.073745  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 22:52:30.073767  6183 net.cpp:1851] conv1a_param_0(0.275) 
I0630 22:52:30.073778  6183 net.cpp:1851] conv1b_param_0(0.55) 
I0630 22:52:30.073783  6183 net.cpp:1851] ctx_conv1_param_0(0.55) 
I0630 22:52:30.073786  6183 net.cpp:1851] ctx_conv2_param_0(0.55) 
I0630 22:52:30.073791  6183 net.cpp:1851] ctx_conv3_param_0(0.55) 
I0630 22:52:30.073793  6183 net.cpp:1851] ctx_conv4_param_0(0.55) 
I0630 22:52:30.073796  6183 net.cpp:1851] ctx_final_param_0(0.275) 
I0630 22:52:30.073801  6183 net.cpp:1851] out3a_param_0(0.55) 
I0630 22:52:30.073803  6183 net.cpp:1851] out5a_param_0(0.55) 
I0630 22:52:30.073807  6183 net.cpp:1851] res2a_branch2a_param_0(0.55) 
I0630 22:52:30.073810  6183 net.cpp:1851] res2a_branch2b_param_0(0.55) 
I0630 22:52:30.073814  6183 net.cpp:1851] res3a_branch2a_param_0(0.55) 
I0630 22:52:30.073819  6183 net.cpp:1851] res3a_branch2b_param_0(0.55) 
I0630 22:52:30.073823  6183 net.cpp:1851] res4a_branch2a_param_0(0.55) 
I0630 22:52:30.073827  6183 net.cpp:1851] res4a_branch2b_param_0(0.55) 
I0630 22:52:30.073832  6183 net.cpp:1851] res5a_branch2a_param_0(0.55) 
I0630 22:52:30.073837  6183 net.cpp:1851] res5a_branch2b_param_0(0.55) 
I0630 22:52:30.073840  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.4801e+06/2.69808e+06) 0.549
I0630 22:52:30.245112  6183 solver.cpp:290] Iteration 11000 (5.19068 iter/s, 19.2653s/100 iter), loss = 0.0833842
I0630 22:52:30.245138  6183 solver.cpp:309]     Train net output #0: loss = 0.0833841 (* 1 = 0.0833841 loss)
I0630 22:52:30.245146  6183 sgd_solver.cpp:106] Iteration 11000, lr = 1e-05
I0630 22:52:30.246121  6183 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.6
I0630 22:52:31.634292  6183 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 22:52:50.685799  6183 solver.cpp:290] Iteration 11100 (4.89234 iter/s, 20.4401s/100 iter), loss = 0.129366
I0630 22:52:50.685822  6183 solver.cpp:309]     Train net output #0: loss = 0.129366 (* 1 = 0.129366 loss)
I0630 22:52:50.685829  6183 sgd_solver.cpp:106] Iteration 11100, lr = 1e-05
I0630 22:53:09.841634  6183 solver.cpp:290] Iteration 11200 (5.22049 iter/s, 19.1553s/100 iter), loss = 0.0898908
I0630 22:53:09.841686  6183 solver.cpp:309]     Train net output #0: loss = 0.0898907 (* 1 = 0.0898907 loss)
I0630 22:53:09.841694  6183 sgd_solver.cpp:106] Iteration 11200, lr = 1e-05
I0630 22:53:29.051672  6183 solver.cpp:290] Iteration 11300 (5.20577 iter/s, 19.2095s/100 iter), loss = 0.0685745
I0630 22:53:29.051693  6183 solver.cpp:309]     Train net output #0: loss = 0.0685745 (* 1 = 0.0685745 loss)
I0630 22:53:29.051700  6183 sgd_solver.cpp:106] Iteration 11300, lr = 1e-05
I0630 22:53:48.301026  6183 solver.cpp:290] Iteration 11400 (5.19513 iter/s, 19.2488s/100 iter), loss = 0.0716817
I0630 22:53:48.301105  6183 solver.cpp:309]     Train net output #0: loss = 0.0716817 (* 1 = 0.0716817 loss)
I0630 22:53:48.301115  6183 sgd_solver.cpp:106] Iteration 11400, lr = 1e-05
I0630 22:54:07.168418  6183 solver.cpp:290] Iteration 11500 (5.30032 iter/s, 18.8668s/100 iter), loss = 0.069271
I0630 22:54:07.168443  6183 solver.cpp:309]     Train net output #0: loss = 0.069271 (* 1 = 0.069271 loss)
I0630 22:54:07.168453  6183 sgd_solver.cpp:106] Iteration 11500, lr = 1e-05
I0630 22:54:26.428829  6183 solver.cpp:290] Iteration 11600 (5.19214 iter/s, 19.2599s/100 iter), loss = 0.0647485
I0630 22:54:26.428905  6183 solver.cpp:309]     Train net output #0: loss = 0.0647485 (* 1 = 0.0647485 loss)
I0630 22:54:26.428912  6183 sgd_solver.cpp:106] Iteration 11600, lr = 1e-05
I0630 22:54:45.604594  6183 solver.cpp:290] Iteration 11700 (5.21508 iter/s, 19.1752s/100 iter), loss = 0.152001
I0630 22:54:45.604619  6183 solver.cpp:309]     Train net output #0: loss = 0.152001 (* 1 = 0.152001 loss)
I0630 22:54:45.604625  6183 sgd_solver.cpp:106] Iteration 11700, lr = 1e-05
I0630 22:55:04.675210  6183 solver.cpp:290] Iteration 11800 (5.24382 iter/s, 19.0701s/100 iter), loss = 0.0705581
I0630 22:55:04.675276  6183 solver.cpp:309]     Train net output #0: loss = 0.070558 (* 1 = 0.070558 loss)
I0630 22:55:04.675284  6183 sgd_solver.cpp:106] Iteration 11800, lr = 1e-05
I0630 22:55:23.919462  6183 solver.cpp:290] Iteration 11900 (5.19652 iter/s, 19.2437s/100 iter), loss = 0.0919735
I0630 22:55:23.919486  6183 solver.cpp:309]     Train net output #0: loss = 0.0919734 (* 1 = 0.0919734 loss)
I0630 22:55:23.919492  6183 sgd_solver.cpp:106] Iteration 11900, lr = 1e-05
I0630 22:55:42.781095  6183 solver.cpp:354] Sparsity after update:
I0630 22:55:42.783196  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 22:55:42.783205  6183 net.cpp:1851] conv1a_param_0(0.3) 
I0630 22:55:42.783211  6183 net.cpp:1851] conv1b_param_0(0.6) 
I0630 22:55:42.783213  6183 net.cpp:1851] ctx_conv1_param_0(0.6) 
I0630 22:55:42.783215  6183 net.cpp:1851] ctx_conv2_param_0(0.6) 
I0630 22:55:42.783217  6183 net.cpp:1851] ctx_conv3_param_0(0.6) 
I0630 22:55:42.783219  6183 net.cpp:1851] ctx_conv4_param_0(0.6) 
I0630 22:55:42.783221  6183 net.cpp:1851] ctx_final_param_0(0.3) 
I0630 22:55:42.783223  6183 net.cpp:1851] out3a_param_0(0.6) 
I0630 22:55:42.783226  6183 net.cpp:1851] out5a_param_0(0.6) 
I0630 22:55:42.783227  6183 net.cpp:1851] res2a_branch2a_param_0(0.6) 
I0630 22:55:42.783229  6183 net.cpp:1851] res2a_branch2b_param_0(0.6) 
I0630 22:55:42.783231  6183 net.cpp:1851] res3a_branch2a_param_0(0.6) 
I0630 22:55:42.783233  6183 net.cpp:1851] res3a_branch2b_param_0(0.6) 
I0630 22:55:42.783236  6183 net.cpp:1851] res4a_branch2a_param_0(0.6) 
I0630 22:55:42.783237  6183 net.cpp:1851] res4a_branch2b_param_0(0.6) 
I0630 22:55:42.783239  6183 net.cpp:1851] res5a_branch2a_param_0(0.6) 
I0630 22:55:42.783242  6183 net.cpp:1851] res5a_branch2b_param_0(0.6) 
I0630 22:55:42.783246  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.61464e+06/2.69808e+06) 0.598
I0630 22:55:42.783377  6183 solver.cpp:471] Iteration 12000, Testing net (#0)
I0630 22:57:21.390043  6183 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.932652
I0630 22:57:21.390136  6183 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.994624
I0630 22:57:21.390143  6183 solver.cpp:544]     Test net output #2: loss = 0.145428 (* 1 = 0.145428 loss)
I0630 22:57:21.613607  6183 solver.cpp:290] Iteration 12000 (0.849683 iter/s, 117.691s/100 iter), loss = 0.0793091
I0630 22:57:21.613636  6183 solver.cpp:309]     Train net output #0: loss = 0.079309 (* 1 = 0.079309 loss)
I0630 22:57:21.613651  6183 sgd_solver.cpp:106] Iteration 12000, lr = 1e-05
I0630 22:57:21.614627  6183 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.65
I0630 22:57:23.104311  6183 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 22:57:42.258072  6183 solver.cpp:290] Iteration 12100 (4.84405 iter/s, 20.6439s/100 iter), loss = 0.0756494
I0630 22:57:42.258100  6183 solver.cpp:309]     Train net output #0: loss = 0.0756494 (* 1 = 0.0756494 loss)
I0630 22:57:42.258121  6183 sgd_solver.cpp:106] Iteration 12100, lr = 1e-05
I0630 22:58:01.524184  6183 solver.cpp:290] Iteration 12200 (5.19061 iter/s, 19.2656s/100 iter), loss = 0.0824985
I0630 22:58:01.524261  6183 solver.cpp:309]     Train net output #0: loss = 0.0824984 (* 1 = 0.0824984 loss)
I0630 22:58:01.524272  6183 sgd_solver.cpp:106] Iteration 12200, lr = 1e-05
I0630 22:58:20.717581  6183 solver.cpp:290] Iteration 12300 (5.21029 iter/s, 19.1928s/100 iter), loss = 0.0843738
I0630 22:58:20.717604  6183 solver.cpp:309]     Train net output #0: loss = 0.0843737 (* 1 = 0.0843737 loss)
I0630 22:58:20.717612  6183 sgd_solver.cpp:106] Iteration 12300, lr = 1e-05
I0630 22:58:39.920956  6183 solver.cpp:290] Iteration 12400 (5.20757 iter/s, 19.2028s/100 iter), loss = 0.0853084
I0630 22:58:39.921087  6183 solver.cpp:309]     Train net output #0: loss = 0.0853083 (* 1 = 0.0853083 loss)
I0630 22:58:39.921097  6183 sgd_solver.cpp:106] Iteration 12400, lr = 1e-05
I0630 22:58:59.050892  6183 solver.cpp:290] Iteration 12500 (5.22759 iter/s, 19.1293s/100 iter), loss = 0.0577654
I0630 22:58:59.050916  6183 solver.cpp:309]     Train net output #0: loss = 0.0577653 (* 1 = 0.0577653 loss)
I0630 22:58:59.050925  6183 sgd_solver.cpp:106] Iteration 12500, lr = 1e-05
I0630 22:59:18.325752  6183 solver.cpp:290] Iteration 12600 (5.18825 iter/s, 19.2743s/100 iter), loss = 0.0734227
I0630 22:59:18.325805  6183 solver.cpp:309]     Train net output #0: loss = 0.0734226 (* 1 = 0.0734226 loss)
I0630 22:59:18.325814  6183 sgd_solver.cpp:106] Iteration 12600, lr = 1e-05
I0630 22:59:37.523316  6183 solver.cpp:290] Iteration 12700 (5.20915 iter/s, 19.197s/100 iter), loss = 0.11181
I0630 22:59:37.523339  6183 solver.cpp:309]     Train net output #0: loss = 0.11181 (* 1 = 0.11181 loss)
I0630 22:59:37.523346  6183 sgd_solver.cpp:106] Iteration 12700, lr = 1e-05
I0630 22:59:56.642396  6183 solver.cpp:290] Iteration 12800 (5.23053 iter/s, 19.1185s/100 iter), loss = 0.0952862
I0630 22:59:56.642449  6183 solver.cpp:309]     Train net output #0: loss = 0.0952861 (* 1 = 0.0952861 loss)
I0630 22:59:56.642460  6183 sgd_solver.cpp:106] Iteration 12800, lr = 1e-05
I0630 23:00:15.843582  6183 solver.cpp:290] Iteration 12900 (5.20817 iter/s, 19.2006s/100 iter), loss = 0.0746608
I0630 23:00:15.843608  6183 solver.cpp:309]     Train net output #0: loss = 0.0746608 (* 1 = 0.0746608 loss)
I0630 23:00:15.843614  6183 sgd_solver.cpp:106] Iteration 12900, lr = 1e-05
I0630 23:00:34.795313  6183 solver.cpp:354] Sparsity after update:
I0630 23:00:34.871013  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 23:00:34.871033  6183 net.cpp:1851] conv1a_param_0(0.325) 
I0630 23:00:34.871045  6183 net.cpp:1851] conv1b_param_0(0.65) 
I0630 23:00:34.871049  6183 net.cpp:1851] ctx_conv1_param_0(0.626) 
I0630 23:00:34.871052  6183 net.cpp:1851] ctx_conv2_param_0(0.65) 
I0630 23:00:34.871057  6183 net.cpp:1851] ctx_conv3_param_0(0.619) 
I0630 23:00:34.871059  6183 net.cpp:1851] ctx_conv4_param_0(0.65) 
I0630 23:00:34.871062  6183 net.cpp:1851] ctx_final_param_0(0.325) 
I0630 23:00:34.871067  6183 net.cpp:1851] out3a_param_0(0.65) 
I0630 23:00:34.871069  6183 net.cpp:1851] out5a_param_0(0.65) 
I0630 23:00:34.871073  6183 net.cpp:1851] res2a_branch2a_param_0(0.65) 
I0630 23:00:34.871078  6183 net.cpp:1851] res2a_branch2b_param_0(0.65) 
I0630 23:00:34.871081  6183 net.cpp:1851] res3a_branch2a_param_0(0.65) 
I0630 23:00:34.871085  6183 net.cpp:1851] res3a_branch2b_param_0(0.65) 
I0630 23:00:34.871090  6183 net.cpp:1851] res4a_branch2a_param_0(0.65) 
I0630 23:00:34.871095  6183 net.cpp:1851] res4a_branch2b_param_0(0.65) 
I0630 23:00:34.871099  6183 net.cpp:1851] res5a_branch2a_param_0(0.65) 
I0630 23:00:34.871104  6183 net.cpp:1851] res5a_branch2b_param_0(0.65) 
I0630 23:00:34.871109  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.74715e+06/2.69808e+06) 0.648
I0630 23:00:35.041309  6183 solver.cpp:290] Iteration 13000 (5.2091 iter/s, 19.1972s/100 iter), loss = 0.0953069
I0630 23:00:35.041337  6183 solver.cpp:309]     Train net output #0: loss = 0.0953068 (* 1 = 0.0953068 loss)
I0630 23:00:35.041347  6183 sgd_solver.cpp:106] Iteration 13000, lr = 1e-05
I0630 23:00:35.042287  6183 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.7
I0630 23:00:36.637589  6183 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 23:00:55.888979  6183 solver.cpp:290] Iteration 13100 (4.79684 iter/s, 20.8471s/100 iter), loss = 0.241075
I0630 23:00:55.889005  6183 solver.cpp:309]     Train net output #0: loss = 0.241075 (* 1 = 0.241075 loss)
I0630 23:00:55.889014  6183 sgd_solver.cpp:106] Iteration 13100, lr = 1e-05
I0630 23:01:15.210599  6183 solver.cpp:290] Iteration 13200 (5.1757 iter/s, 19.3211s/100 iter), loss = 0.0915433
I0630 23:01:15.210654  6183 solver.cpp:309]     Train net output #0: loss = 0.0915433 (* 1 = 0.0915433 loss)
I0630 23:01:15.210661  6183 sgd_solver.cpp:106] Iteration 13200, lr = 1e-05
I0630 23:01:34.270993  6183 solver.cpp:290] Iteration 13300 (5.24664 iter/s, 19.0598s/100 iter), loss = 0.0844101
I0630 23:01:34.271018  6183 solver.cpp:309]     Train net output #0: loss = 0.08441 (* 1 = 0.08441 loss)
I0630 23:01:34.271024  6183 sgd_solver.cpp:106] Iteration 13300, lr = 1e-05
I0630 23:01:53.405287  6183 solver.cpp:290] Iteration 13400 (5.22637 iter/s, 19.1338s/100 iter), loss = 0.066569
I0630 23:01:53.405354  6183 solver.cpp:309]     Train net output #0: loss = 0.0665689 (* 1 = 0.0665689 loss)
I0630 23:01:53.405361  6183 sgd_solver.cpp:106] Iteration 13400, lr = 1e-05
I0630 23:02:12.447569  6183 solver.cpp:290] Iteration 13500 (5.25163 iter/s, 19.0417s/100 iter), loss = 0.141264
I0630 23:02:12.447592  6183 solver.cpp:309]     Train net output #0: loss = 0.141264 (* 1 = 0.141264 loss)
I0630 23:02:12.447599  6183 sgd_solver.cpp:106] Iteration 13500, lr = 1e-05
I0630 23:02:31.583592  6183 solver.cpp:290] Iteration 13600 (5.22589 iter/s, 19.1355s/100 iter), loss = 0.0843519
I0630 23:02:31.583652  6183 solver.cpp:309]     Train net output #0: loss = 0.0843518 (* 1 = 0.0843518 loss)
I0630 23:02:31.583663  6183 sgd_solver.cpp:106] Iteration 13600, lr = 1e-05
I0630 23:02:50.854539  6183 solver.cpp:290] Iteration 13700 (5.18931 iter/s, 19.2704s/100 iter), loss = 0.0950526
I0630 23:02:50.854563  6183 solver.cpp:309]     Train net output #0: loss = 0.0950525 (* 1 = 0.0950525 loss)
I0630 23:02:50.854570  6183 sgd_solver.cpp:106] Iteration 13700, lr = 1e-05
I0630 23:03:09.994174  6183 solver.cpp:290] Iteration 13800 (5.22491 iter/s, 19.1391s/100 iter), loss = 0.124694
I0630 23:03:09.994261  6183 solver.cpp:309]     Train net output #0: loss = 0.124694 (* 1 = 0.124694 loss)
I0630 23:03:09.994272  6183 sgd_solver.cpp:106] Iteration 13800, lr = 1e-05
I0630 23:03:29.086058  6183 solver.cpp:290] Iteration 13900 (5.23799 iter/s, 19.0913s/100 iter), loss = 0.0791134
I0630 23:03:29.086081  6183 solver.cpp:309]     Train net output #0: loss = 0.0791133 (* 1 = 0.0791133 loss)
I0630 23:03:29.086087  6183 sgd_solver.cpp:106] Iteration 13900, lr = 1e-05
I0630 23:03:48.111675  6183 solver.cpp:354] Sparsity after update:
I0630 23:03:48.113598  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 23:03:48.113605  6183 net.cpp:1851] conv1a_param_0(0.35) 
I0630 23:03:48.113613  6183 net.cpp:1851] conv1b_param_0(0.7) 
I0630 23:03:48.113616  6183 net.cpp:1851] ctx_conv1_param_0(0.627) 
I0630 23:03:48.113618  6183 net.cpp:1851] ctx_conv2_param_0(0.682) 
I0630 23:03:48.113621  6183 net.cpp:1851] ctx_conv3_param_0(0.621) 
I0630 23:03:48.113623  6183 net.cpp:1851] ctx_conv4_param_0(0.673) 
I0630 23:03:48.113626  6183 net.cpp:1851] ctx_final_param_0(0.35) 
I0630 23:03:48.113628  6183 net.cpp:1851] out3a_param_0(0.7) 
I0630 23:03:48.113631  6183 net.cpp:1851] out5a_param_0(0.7) 
I0630 23:03:48.113632  6183 net.cpp:1851] res2a_branch2a_param_0(0.7) 
I0630 23:03:48.113636  6183 net.cpp:1851] res2a_branch2b_param_0(0.7) 
I0630 23:03:48.113637  6183 net.cpp:1851] res3a_branch2a_param_0(0.7) 
I0630 23:03:48.113639  6183 net.cpp:1851] res3a_branch2b_param_0(0.7) 
I0630 23:03:48.113642  6183 net.cpp:1851] res4a_branch2a_param_0(0.7) 
I0630 23:03:48.113644  6183 net.cpp:1851] res4a_branch2b_param_0(0.7) 
I0630 23:03:48.113646  6183 net.cpp:1851] res5a_branch2a_param_0(0.7) 
I0630 23:03:48.113648  6183 net.cpp:1851] res5a_branch2b_param_0(0.7) 
I0630 23:03:48.113651  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (1.87654e+06/2.69808e+06) 0.696
I0630 23:03:48.113781  6183 solver.cpp:471] Iteration 14000, Testing net (#0)
I0630 23:05:27.144423  6183 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.926823
I0630 23:05:27.144511  6183 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.993667
I0630 23:05:27.144520  6183 solver.cpp:544]     Test net output #2: loss = 0.159123 (* 1 = 0.159123 loss)
I0630 23:05:27.358676  6183 solver.cpp:290] Iteration 14000 (0.845527 iter/s, 118.269s/100 iter), loss = 0.0806951
I0630 23:05:27.358701  6183 solver.cpp:309]     Train net output #0: loss = 0.080695 (* 1 = 0.080695 loss)
I0630 23:05:27.358708  6183 sgd_solver.cpp:106] Iteration 14000, lr = 1e-05
I0630 23:05:27.359622  6183 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.75
I0630 23:05:29.057809  6183 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 23:05:48.485626  6183 solver.cpp:290] Iteration 14100 (4.73342 iter/s, 21.1264s/100 iter), loss = 0.287912
I0630 23:05:48.485653  6183 solver.cpp:309]     Train net output #0: loss = 0.287912 (* 1 = 0.287912 loss)
I0630 23:05:48.485663  6183 sgd_solver.cpp:106] Iteration 14100, lr = 1e-05
I0630 23:06:07.717880  6183 solver.cpp:290] Iteration 14200 (5.19975 iter/s, 19.2317s/100 iter), loss = 0.0769771
I0630 23:06:07.717975  6183 solver.cpp:309]     Train net output #0: loss = 0.0769771 (* 1 = 0.0769771 loss)
I0630 23:06:07.717984  6183 sgd_solver.cpp:106] Iteration 14200, lr = 1e-05
I0630 23:06:27.584560  6183 solver.cpp:290] Iteration 14300 (5.03371 iter/s, 19.866s/100 iter), loss = 0.0950777
I0630 23:06:27.584585  6183 solver.cpp:309]     Train net output #0: loss = 0.0950777 (* 1 = 0.0950777 loss)
I0630 23:06:27.584592  6183 sgd_solver.cpp:106] Iteration 14300, lr = 1e-05
I0630 23:06:46.822844  6183 solver.cpp:290] Iteration 14400 (5.19812 iter/s, 19.2377s/100 iter), loss = 0.129255
I0630 23:06:46.822916  6183 solver.cpp:309]     Train net output #0: loss = 0.129255 (* 1 = 0.129255 loss)
I0630 23:06:46.822923  6183 sgd_solver.cpp:106] Iteration 14400, lr = 1e-05
I0630 23:07:06.100539  6183 solver.cpp:290] Iteration 14500 (5.1875 iter/s, 19.2771s/100 iter), loss = 0.167218
I0630 23:07:06.100566  6183 solver.cpp:309]     Train net output #0: loss = 0.167217 (* 1 = 0.167217 loss)
I0630 23:07:06.100574  6183 sgd_solver.cpp:106] Iteration 14500, lr = 1e-05
I0630 23:07:25.159857  6183 solver.cpp:290] Iteration 14600 (5.24693 iter/s, 19.0588s/100 iter), loss = 0.10043
I0630 23:07:25.159904  6183 solver.cpp:309]     Train net output #0: loss = 0.10043 (* 1 = 0.10043 loss)
I0630 23:07:25.159912  6183 sgd_solver.cpp:106] Iteration 14600, lr = 1e-05
I0630 23:07:44.294725  6183 solver.cpp:290] Iteration 14700 (5.22621 iter/s, 19.1343s/100 iter), loss = 0.152852
I0630 23:07:44.294750  6183 solver.cpp:309]     Train net output #0: loss = 0.152852 (* 1 = 0.152852 loss)
I0630 23:07:44.294757  6183 sgd_solver.cpp:106] Iteration 14700, lr = 1e-05
I0630 23:08:03.380723  6183 solver.cpp:290] Iteration 14800 (5.23959 iter/s, 19.0855s/100 iter), loss = 0.0837526
I0630 23:08:03.380772  6183 solver.cpp:309]     Train net output #0: loss = 0.0837525 (* 1 = 0.0837525 loss)
I0630 23:08:03.380779  6183 sgd_solver.cpp:106] Iteration 14800, lr = 1e-05
I0630 23:08:22.336107  6183 solver.cpp:290] Iteration 14900 (5.2757 iter/s, 18.9548s/100 iter), loss = 0.230844
I0630 23:08:22.336135  6183 solver.cpp:309]     Train net output #0: loss = 0.230844 (* 1 = 0.230844 loss)
I0630 23:08:22.336148  6183 sgd_solver.cpp:106] Iteration 14900, lr = 1e-05
I0630 23:08:41.251935  6183 solver.cpp:354] Sparsity after update:
I0630 23:08:41.316893  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 23:08:41.316911  6183 net.cpp:1851] conv1a_param_0(0.375) 
I0630 23:08:41.316920  6183 net.cpp:1851] conv1b_param_0(0.75) 
I0630 23:08:41.316922  6183 net.cpp:1851] ctx_conv1_param_0(0.629) 
I0630 23:08:41.316925  6183 net.cpp:1851] ctx_conv2_param_0(0.684) 
I0630 23:08:41.316926  6183 net.cpp:1851] ctx_conv3_param_0(0.624) 
I0630 23:08:41.316928  6183 net.cpp:1851] ctx_conv4_param_0(0.674) 
I0630 23:08:41.316931  6183 net.cpp:1851] ctx_final_param_0(0.375) 
I0630 23:08:41.316932  6183 net.cpp:1851] out3a_param_0(0.75) 
I0630 23:08:41.316934  6183 net.cpp:1851] out5a_param_0(0.75) 
I0630 23:08:41.316936  6183 net.cpp:1851] res2a_branch2a_param_0(0.75) 
I0630 23:08:41.316938  6183 net.cpp:1851] res2a_branch2b_param_0(0.75) 
I0630 23:08:41.316941  6183 net.cpp:1851] res3a_branch2a_param_0(0.75) 
I0630 23:08:41.316942  6183 net.cpp:1851] res3a_branch2b_param_0(0.75) 
I0630 23:08:41.316944  6183 net.cpp:1851] res4a_branch2a_param_0(0.75) 
I0630 23:08:41.316946  6183 net.cpp:1851] res4a_branch2b_param_0(0.75) 
I0630 23:08:41.316947  6183 net.cpp:1851] res5a_branch2a_param_0(0.75) 
I0630 23:08:41.316949  6183 net.cpp:1851] res5a_branch2b_param_0(0.75) 
I0630 23:08:41.316951  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.004e+06/2.69808e+06) 0.743
I0630 23:08:41.487812  6183 solver.cpp:290] Iteration 15000 (5.22162 iter/s, 19.1512s/100 iter), loss = 0.145521
I0630 23:08:41.487838  6183 solver.cpp:309]     Train net output #0: loss = 0.145521 (* 1 = 0.145521 loss)
I0630 23:08:41.487844  6183 sgd_solver.cpp:106] Iteration 15000, lr = 1e-05
I0630 23:08:41.488822  6183 solver.cpp:376] Finding and applying thresholds. Target sparsity = 0.8
I0630 23:08:43.273886  6183 net.cpp:1824] All zero weights of convolution layers are frozen
I0630 23:09:02.161046  6183 solver.cpp:290] Iteration 15100 (4.83731 iter/s, 20.6727s/100 iter), loss = 0.153319
I0630 23:09:02.161067  6183 solver.cpp:309]     Train net output #0: loss = 0.153319 (* 1 = 0.153319 loss)
I0630 23:09:02.161074  6183 sgd_solver.cpp:106] Iteration 15100, lr = 1e-05
I0630 23:09:21.593365  6183 solver.cpp:290] Iteration 15200 (5.14621 iter/s, 19.4318s/100 iter), loss = 0.110469
I0630 23:09:21.593456  6183 solver.cpp:309]     Train net output #0: loss = 0.110469 (* 1 = 0.110469 loss)
I0630 23:09:21.593463  6183 sgd_solver.cpp:106] Iteration 15200, lr = 1e-05
I0630 23:09:40.680281  6183 solver.cpp:290] Iteration 15300 (5.23936 iter/s, 19.0863s/100 iter), loss = 0.182861
I0630 23:09:40.680310  6183 solver.cpp:309]     Train net output #0: loss = 0.182861 (* 1 = 0.182861 loss)
I0630 23:09:40.680318  6183 sgd_solver.cpp:106] Iteration 15300, lr = 1e-05
I0630 23:09:59.763525  6183 solver.cpp:290] Iteration 15400 (5.24035 iter/s, 19.0827s/100 iter), loss = 0.131533
I0630 23:09:59.763577  6183 solver.cpp:309]     Train net output #0: loss = 0.131533 (* 1 = 0.131533 loss)
I0630 23:09:59.763586  6183 sgd_solver.cpp:106] Iteration 15400, lr = 1e-05
I0630 23:10:18.887471  6183 solver.cpp:290] Iteration 15500 (5.2292 iter/s, 19.1234s/100 iter), loss = 0.0981063
I0630 23:10:18.887496  6183 solver.cpp:309]     Train net output #0: loss = 0.0981064 (* 1 = 0.0981064 loss)
I0630 23:10:18.887502  6183 sgd_solver.cpp:106] Iteration 15500, lr = 1e-05
I0630 23:10:38.025665  6183 solver.cpp:290] Iteration 15600 (5.2253 iter/s, 19.1377s/100 iter), loss = 0.228007
I0630 23:10:38.025774  6183 solver.cpp:309]     Train net output #0: loss = 0.228007 (* 1 = 0.228007 loss)
I0630 23:10:38.025784  6183 sgd_solver.cpp:106] Iteration 15600, lr = 1e-05
I0630 23:10:57.122635  6183 solver.cpp:290] Iteration 15700 (5.2366 iter/s, 19.0964s/100 iter), loss = 0.175053
I0630 23:10:57.122658  6183 solver.cpp:309]     Train net output #0: loss = 0.175053 (* 1 = 0.175053 loss)
I0630 23:10:57.122664  6183 sgd_solver.cpp:106] Iteration 15700, lr = 1e-05
I0630 23:11:16.154717  6183 solver.cpp:290] Iteration 15800 (5.25443 iter/s, 19.0316s/100 iter), loss = 0.169081
I0630 23:11:16.154811  6183 solver.cpp:309]     Train net output #0: loss = 0.169082 (* 1 = 0.169082 loss)
I0630 23:11:16.154822  6183 sgd_solver.cpp:106] Iteration 15800, lr = 1e-05
I0630 23:11:35.173915  6183 solver.cpp:290] Iteration 15900 (5.25801 iter/s, 19.0186s/100 iter), loss = 0.164171
I0630 23:11:35.173938  6183 solver.cpp:309]     Train net output #0: loss = 0.164172 (* 1 = 0.164172 loss)
I0630 23:11:35.173945  6183 sgd_solver.cpp:106] Iteration 15900, lr = 1e-05
I0630 23:11:54.105933  6183 solver.cpp:354] Sparsity after update:
I0630 23:11:54.107884  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 23:11:54.107893  6183 net.cpp:1851] conv1a_param_0(0.4) 
I0630 23:11:54.107900  6183 net.cpp:1851] conv1b_param_0(0.774) 
I0630 23:11:54.107903  6183 net.cpp:1851] ctx_conv1_param_0(0.63) 
I0630 23:11:54.107904  6183 net.cpp:1851] ctx_conv2_param_0(0.686) 
I0630 23:11:54.107906  6183 net.cpp:1851] ctx_conv3_param_0(0.626) 
I0630 23:11:54.107909  6183 net.cpp:1851] ctx_conv4_param_0(0.675) 
I0630 23:11:54.107911  6183 net.cpp:1851] ctx_final_param_0(0.4) 
I0630 23:11:54.107914  6183 net.cpp:1851] out3a_param_0(0.757) 
I0630 23:11:54.107916  6183 net.cpp:1851] out5a_param_0(0.784) 
I0630 23:11:54.107918  6183 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0630 23:11:54.107920  6183 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0630 23:11:54.107923  6183 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0630 23:11:54.107925  6183 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0630 23:11:54.107928  6183 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0630 23:11:54.107930  6183 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0630 23:11:54.107933  6183 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0630 23:11:54.107934  6183 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0630 23:11:54.107936  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.12736e+06/2.69808e+06) 0.788
I0630 23:11:54.108073  6183 solver.cpp:471] Iteration 16000, Testing net (#0)
I0630 23:13:32.552650  6183 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.918132
I0630 23:13:32.552769  6183 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.99229
I0630 23:13:32.552779  6183 solver.cpp:544]     Test net output #2: loss = 0.174071 (* 1 = 0.174071 loss)
I0630 23:13:32.772297  6183 solver.cpp:290] Iteration 16000 (0.850375 iter/s, 117.595s/100 iter), loss = 0.14701
I0630 23:13:32.772323  6183 solver.cpp:309]     Train net output #0: loss = 0.14701 (* 1 = 0.14701 loss)
I0630 23:13:32.772332  6183 sgd_solver.cpp:106] Iteration 16000, lr = 1e-05
I0630 23:13:51.760818  6183 solver.cpp:290] Iteration 16100 (5.26649 iter/s, 18.988s/100 iter), loss = 0.0842504
I0630 23:13:51.760843  6183 solver.cpp:309]     Train net output #0: loss = 0.0842506 (* 1 = 0.0842506 loss)
I0630 23:13:51.760852  6183 sgd_solver.cpp:106] Iteration 16100, lr = 1e-05
I0630 23:14:11.172372  6183 solver.cpp:290] Iteration 16200 (5.15172 iter/s, 19.411s/100 iter), loss = 0.168242
I0630 23:14:11.172456  6183 solver.cpp:309]     Train net output #0: loss = 0.168242 (* 1 = 0.168242 loss)
I0630 23:14:11.172466  6183 sgd_solver.cpp:106] Iteration 16200, lr = 1e-05
I0630 23:14:30.446300  6183 solver.cpp:290] Iteration 16300 (5.18852 iter/s, 19.2733s/100 iter), loss = 0.107081
I0630 23:14:30.446327  6183 solver.cpp:309]     Train net output #0: loss = 0.107081 (* 1 = 0.107081 loss)
I0630 23:14:30.446336  6183 sgd_solver.cpp:106] Iteration 16300, lr = 1e-05
I0630 23:14:49.333027  6183 solver.cpp:290] Iteration 16400 (5.29487 iter/s, 18.8862s/100 iter), loss = 0.13562
I0630 23:14:49.333079  6183 solver.cpp:309]     Train net output #0: loss = 0.13562 (* 1 = 0.13562 loss)
I0630 23:14:49.333086  6183 sgd_solver.cpp:106] Iteration 16400, lr = 1e-05
I0630 23:15:08.390040  6183 solver.cpp:290] Iteration 16500 (5.24757 iter/s, 19.0565s/100 iter), loss = 0.155575
I0630 23:15:08.390064  6183 solver.cpp:309]     Train net output #0: loss = 0.155576 (* 1 = 0.155576 loss)
I0630 23:15:08.390070  6183 sgd_solver.cpp:106] Iteration 16500, lr = 1e-05
I0630 23:15:27.448715  6183 solver.cpp:290] Iteration 16600 (5.2471 iter/s, 19.0581s/100 iter), loss = 0.12847
I0630 23:15:27.448755  6183 solver.cpp:309]     Train net output #0: loss = 0.12847 (* 1 = 0.12847 loss)
I0630 23:15:27.448761  6183 sgd_solver.cpp:106] Iteration 16600, lr = 1e-05
I0630 23:15:46.440413  6183 solver.cpp:290] Iteration 16700 (5.26561 iter/s, 18.9911s/100 iter), loss = 0.0858648
I0630 23:15:46.440440  6183 solver.cpp:309]     Train net output #0: loss = 0.085865 (* 1 = 0.085865 loss)
I0630 23:15:46.440449  6183 sgd_solver.cpp:106] Iteration 16700, lr = 1e-05
I0630 23:16:05.838793  6183 solver.cpp:290] Iteration 16800 (5.15521 iter/s, 19.3978s/100 iter), loss = 0.140188
I0630 23:16:05.838840  6183 solver.cpp:309]     Train net output #0: loss = 0.140189 (* 1 = 0.140189 loss)
I0630 23:16:05.838846  6183 sgd_solver.cpp:106] Iteration 16800, lr = 1e-05
I0630 23:16:24.967573  6183 solver.cpp:290] Iteration 16900 (5.22788 iter/s, 19.1282s/100 iter), loss = 0.137164
I0630 23:16:24.967597  6183 solver.cpp:309]     Train net output #0: loss = 0.137164 (* 1 = 0.137164 loss)
I0630 23:16:24.967604  6183 sgd_solver.cpp:106] Iteration 16900, lr = 1e-05
I0630 23:16:43.903265  6183 solver.cpp:354] Sparsity after update:
I0630 23:16:43.972296  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 23:16:43.972313  6183 net.cpp:1851] conv1a_param_0(0.4) 
I0630 23:16:43.972321  6183 net.cpp:1851] conv1b_param_0(0.774) 
I0630 23:16:43.972324  6183 net.cpp:1851] ctx_conv1_param_0(0.63) 
I0630 23:16:43.972326  6183 net.cpp:1851] ctx_conv2_param_0(0.686) 
I0630 23:16:43.972328  6183 net.cpp:1851] ctx_conv3_param_0(0.626) 
I0630 23:16:43.972331  6183 net.cpp:1851] ctx_conv4_param_0(0.675) 
I0630 23:16:43.972332  6183 net.cpp:1851] ctx_final_param_0(0.4) 
I0630 23:16:43.972334  6183 net.cpp:1851] out3a_param_0(0.757) 
I0630 23:16:43.972337  6183 net.cpp:1851] out5a_param_0(0.784) 
I0630 23:16:43.972337  6183 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0630 23:16:43.972340  6183 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0630 23:16:43.972342  6183 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0630 23:16:43.972343  6183 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0630 23:16:43.972347  6183 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0630 23:16:43.972349  6183 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0630 23:16:43.972352  6183 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0630 23:16:43.972355  6183 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0630 23:16:43.972359  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.12736e+06/2.69808e+06) 0.788
I0630 23:16:44.142774  6183 solver.cpp:290] Iteration 17000 (5.21522 iter/s, 19.1747s/100 iter), loss = 0.0791059
I0630 23:16:44.142798  6183 solver.cpp:309]     Train net output #0: loss = 0.079106 (* 1 = 0.079106 loss)
I0630 23:16:44.142805  6183 sgd_solver.cpp:106] Iteration 17000, lr = 1e-05
I0630 23:17:03.503875  6183 solver.cpp:290] Iteration 17100 (5.16514 iter/s, 19.3606s/100 iter), loss = 0.1106
I0630 23:17:03.503902  6183 solver.cpp:309]     Train net output #0: loss = 0.1106 (* 1 = 0.1106 loss)
I0630 23:17:03.503911  6183 sgd_solver.cpp:106] Iteration 17100, lr = 1e-05
I0630 23:17:22.898568  6183 solver.cpp:290] Iteration 17200 (5.15619 iter/s, 19.3941s/100 iter), loss = 0.116436
I0630 23:17:22.898627  6183 solver.cpp:309]     Train net output #0: loss = 0.116437 (* 1 = 0.116437 loss)
I0630 23:17:22.898636  6183 sgd_solver.cpp:106] Iteration 17200, lr = 1e-05
I0630 23:17:41.912798  6183 solver.cpp:290] Iteration 17300 (5.25937 iter/s, 19.0137s/100 iter), loss = 0.10652
I0630 23:17:41.912822  6183 solver.cpp:309]     Train net output #0: loss = 0.106521 (* 1 = 0.106521 loss)
I0630 23:17:41.912828  6183 sgd_solver.cpp:106] Iteration 17300, lr = 1e-05
I0630 23:18:01.223321  6183 solver.cpp:290] Iteration 17400 (5.17867 iter/s, 19.31s/100 iter), loss = 0.152148
I0630 23:18:01.223373  6183 solver.cpp:309]     Train net output #0: loss = 0.152148 (* 1 = 0.152148 loss)
I0630 23:18:01.223381  6183 sgd_solver.cpp:106] Iteration 17400, lr = 1e-05
I0630 23:18:20.265508  6183 solver.cpp:290] Iteration 17500 (5.25165 iter/s, 19.0416s/100 iter), loss = 0.101378
I0630 23:18:20.265560  6183 solver.cpp:309]     Train net output #0: loss = 0.101378 (* 1 = 0.101378 loss)
I0630 23:18:20.265573  6183 sgd_solver.cpp:106] Iteration 17500, lr = 1e-05
I0630 23:18:39.306488  6183 solver.cpp:290] Iteration 17600 (5.25198 iter/s, 19.0404s/100 iter), loss = 0.116941
I0630 23:18:39.306541  6183 solver.cpp:309]     Train net output #0: loss = 0.116941 (* 1 = 0.116941 loss)
I0630 23:18:39.306548  6183 sgd_solver.cpp:106] Iteration 17600, lr = 1e-05
I0630 23:18:58.376170  6183 solver.cpp:290] Iteration 17700 (5.24408 iter/s, 19.0691s/100 iter), loss = 0.188421
I0630 23:18:58.376199  6183 solver.cpp:309]     Train net output #0: loss = 0.188421 (* 1 = 0.188421 loss)
I0630 23:18:58.376206  6183 sgd_solver.cpp:106] Iteration 17700, lr = 1e-05
I0630 23:19:17.452409  6183 solver.cpp:290] Iteration 17800 (5.24227 iter/s, 19.0757s/100 iter), loss = 0.219503
I0630 23:19:17.452473  6183 solver.cpp:309]     Train net output #0: loss = 0.219503 (* 1 = 0.219503 loss)
I0630 23:19:17.452481  6183 sgd_solver.cpp:106] Iteration 17800, lr = 1e-05
I0630 23:19:36.536345  6183 solver.cpp:290] Iteration 17900 (5.24017 iter/s, 19.0834s/100 iter), loss = 0.162081
I0630 23:19:36.536368  6183 solver.cpp:309]     Train net output #0: loss = 0.162081 (* 1 = 0.162081 loss)
I0630 23:19:36.536375  6183 sgd_solver.cpp:106] Iteration 17900, lr = 1e-05
I0630 23:19:55.318792  6183 solver.cpp:354] Sparsity after update:
I0630 23:19:55.320878  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 23:19:55.320885  6183 net.cpp:1851] conv1a_param_0(0.4) 
I0630 23:19:55.320893  6183 net.cpp:1851] conv1b_param_0(0.774) 
I0630 23:19:55.320894  6183 net.cpp:1851] ctx_conv1_param_0(0.63) 
I0630 23:19:55.320896  6183 net.cpp:1851] ctx_conv2_param_0(0.686) 
I0630 23:19:55.320899  6183 net.cpp:1851] ctx_conv3_param_0(0.626) 
I0630 23:19:55.320900  6183 net.cpp:1851] ctx_conv4_param_0(0.675) 
I0630 23:19:55.320902  6183 net.cpp:1851] ctx_final_param_0(0.4) 
I0630 23:19:55.320904  6183 net.cpp:1851] out3a_param_0(0.757) 
I0630 23:19:55.320906  6183 net.cpp:1851] out5a_param_0(0.784) 
I0630 23:19:55.320909  6183 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0630 23:19:55.320910  6183 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0630 23:19:55.320912  6183 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0630 23:19:55.320914  6183 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0630 23:19:55.320916  6183 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0630 23:19:55.320919  6183 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0630 23:19:55.320920  6183 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0630 23:19:55.320922  6183 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0630 23:19:55.320924  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.12736e+06/2.69808e+06) 0.788
I0630 23:19:55.321056  6183 solver.cpp:471] Iteration 18000, Testing net (#0)
I0630 23:21:33.417742  6183 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.920677
I0630 23:21:33.417825  6183 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.99282
I0630 23:21:33.417832  6183 solver.cpp:544]     Test net output #2: loss = 0.166156 (* 1 = 0.166156 loss)
I0630 23:21:33.645488  6183 solver.cpp:290] Iteration 18000 (0.853927 iter/s, 117.106s/100 iter), loss = 0.118446
I0630 23:21:33.645516  6183 solver.cpp:309]     Train net output #0: loss = 0.118446 (* 1 = 0.118446 loss)
I0630 23:21:33.645525  6183 sgd_solver.cpp:106] Iteration 18000, lr = 1e-05
I0630 23:21:52.578542  6183 solver.cpp:290] Iteration 18100 (5.28192 iter/s, 18.9325s/100 iter), loss = 0.191272
I0630 23:21:52.578565  6183 solver.cpp:309]     Train net output #0: loss = 0.191272 (* 1 = 0.191272 loss)
I0630 23:21:52.578572  6183 sgd_solver.cpp:106] Iteration 18100, lr = 1e-05
I0630 23:22:11.629639  6183 solver.cpp:290] Iteration 18200 (5.24919 iter/s, 19.0506s/100 iter), loss = 0.0827249
I0630 23:22:11.629688  6183 solver.cpp:309]     Train net output #0: loss = 0.0827252 (* 1 = 0.0827252 loss)
I0630 23:22:11.629696  6183 sgd_solver.cpp:106] Iteration 18200, lr = 1e-05
I0630 23:22:30.631809  6183 solver.cpp:290] Iteration 18300 (5.26271 iter/s, 19.0016s/100 iter), loss = 0.15024
I0630 23:22:30.631832  6183 solver.cpp:309]     Train net output #0: loss = 0.15024 (* 1 = 0.15024 loss)
I0630 23:22:30.631839  6183 sgd_solver.cpp:106] Iteration 18300, lr = 1e-05
I0630 23:22:49.665839  6183 solver.cpp:290] Iteration 18400 (5.25389 iter/s, 19.0335s/100 iter), loss = 0.194017
I0630 23:22:49.665886  6183 solver.cpp:309]     Train net output #0: loss = 0.194018 (* 1 = 0.194018 loss)
I0630 23:22:49.665894  6183 sgd_solver.cpp:106] Iteration 18400, lr = 1e-05
I0630 23:23:08.715489  6183 solver.cpp:290] Iteration 18500 (5.24959 iter/s, 19.0491s/100 iter), loss = 0.153379
I0630 23:23:08.715512  6183 solver.cpp:309]     Train net output #0: loss = 0.15338 (* 1 = 0.15338 loss)
I0630 23:23:08.715519  6183 sgd_solver.cpp:106] Iteration 18500, lr = 1e-05
I0630 23:23:27.824878  6183 solver.cpp:290] Iteration 18600 (5.23318 iter/s, 19.1089s/100 iter), loss = 0.104525
I0630 23:23:27.825002  6183 solver.cpp:309]     Train net output #0: loss = 0.104525 (* 1 = 0.104525 loss)
I0630 23:23:27.825013  6183 sgd_solver.cpp:106] Iteration 18600, lr = 1e-05
I0630 23:23:46.910218  6183 solver.cpp:290] Iteration 18700 (5.2398 iter/s, 19.0847s/100 iter), loss = 0.14032
I0630 23:23:46.910240  6183 solver.cpp:309]     Train net output #0: loss = 0.14032 (* 1 = 0.14032 loss)
I0630 23:23:46.910246  6183 sgd_solver.cpp:106] Iteration 18700, lr = 1e-05
I0630 23:24:05.986688  6183 solver.cpp:290] Iteration 18800 (5.24221 iter/s, 19.0759s/100 iter), loss = 0.12578
I0630 23:24:05.986759  6183 solver.cpp:309]     Train net output #0: loss = 0.12578 (* 1 = 0.12578 loss)
I0630 23:24:05.986766  6183 sgd_solver.cpp:106] Iteration 18800, lr = 1e-05
I0630 23:24:24.984112  6183 solver.cpp:290] Iteration 18900 (5.26403 iter/s, 18.9968s/100 iter), loss = 0.116361
I0630 23:24:24.984133  6183 solver.cpp:309]     Train net output #0: loss = 0.116361 (* 1 = 0.116361 loss)
I0630 23:24:24.984140  6183 sgd_solver.cpp:106] Iteration 18900, lr = 1e-05
I0630 23:24:43.842931  6183 solver.cpp:354] Sparsity after update:
I0630 23:24:43.917086  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 23:24:43.917104  6183 net.cpp:1851] conv1a_param_0(0.4) 
I0630 23:24:43.917111  6183 net.cpp:1851] conv1b_param_0(0.774) 
I0630 23:24:43.917114  6183 net.cpp:1851] ctx_conv1_param_0(0.63) 
I0630 23:24:43.917116  6183 net.cpp:1851] ctx_conv2_param_0(0.686) 
I0630 23:24:43.917119  6183 net.cpp:1851] ctx_conv3_param_0(0.626) 
I0630 23:24:43.917120  6183 net.cpp:1851] ctx_conv4_param_0(0.675) 
I0630 23:24:43.917131  6183 net.cpp:1851] ctx_final_param_0(0.4) 
I0630 23:24:43.917138  6183 net.cpp:1851] out3a_param_0(0.757) 
I0630 23:24:43.917141  6183 net.cpp:1851] out5a_param_0(0.784) 
I0630 23:24:43.917145  6183 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0630 23:24:43.917148  6183 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0630 23:24:43.917151  6183 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0630 23:24:43.917153  6183 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0630 23:24:43.917157  6183 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0630 23:24:43.917160  6183 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0630 23:24:43.917163  6183 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0630 23:24:43.917167  6183 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0630 23:24:43.917171  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.12736e+06/2.69808e+06) 0.788
I0630 23:24:44.087924  6183 solver.cpp:290] Iteration 19000 (5.2347 iter/s, 19.1033s/100 iter), loss = 0.128935
I0630 23:24:44.087947  6183 solver.cpp:309]     Train net output #0: loss = 0.128935 (* 1 = 0.128935 loss)
I0630 23:24:44.087954  6183 sgd_solver.cpp:106] Iteration 19000, lr = 1e-05
I0630 23:25:03.177368  6183 solver.cpp:290] Iteration 19100 (5.23864 iter/s, 19.0889s/100 iter), loss = 0.215027
I0630 23:25:03.177392  6183 solver.cpp:309]     Train net output #0: loss = 0.215027 (* 1 = 0.215027 loss)
I0630 23:25:03.177398  6183 sgd_solver.cpp:106] Iteration 19100, lr = 1e-05
I0630 23:25:22.341112  6183 solver.cpp:290] Iteration 19200 (5.21833 iter/s, 19.1632s/100 iter), loss = 0.0862525
I0630 23:25:22.341150  6183 solver.cpp:309]     Train net output #0: loss = 0.0862527 (* 1 = 0.0862527 loss)
I0630 23:25:22.341157  6183 sgd_solver.cpp:106] Iteration 19200, lr = 1e-05
I0630 23:25:41.504566  6183 solver.cpp:290] Iteration 19300 (5.21842 iter/s, 19.1629s/100 iter), loss = 0.201211
I0630 23:25:41.504593  6183 solver.cpp:309]     Train net output #0: loss = 0.201211 (* 1 = 0.201211 loss)
I0630 23:25:41.504602  6183 sgd_solver.cpp:106] Iteration 19300, lr = 1e-05
I0630 23:26:00.667503  6183 solver.cpp:290] Iteration 19400 (5.21855 iter/s, 19.1624s/100 iter), loss = 0.162197
I0630 23:26:00.667582  6183 solver.cpp:309]     Train net output #0: loss = 0.162197 (* 1 = 0.162197 loss)
I0630 23:26:00.667589  6183 sgd_solver.cpp:106] Iteration 19400, lr = 1e-05
I0630 23:26:19.971879  6183 solver.cpp:290] Iteration 19500 (5.18033 iter/s, 19.3038s/100 iter), loss = 0.110757
I0630 23:26:19.971912  6183 solver.cpp:309]     Train net output #0: loss = 0.110757 (* 1 = 0.110757 loss)
I0630 23:26:19.971921  6183 sgd_solver.cpp:106] Iteration 19500, lr = 1e-05
I0630 23:26:39.304280  6183 solver.cpp:290] Iteration 19600 (5.17281 iter/s, 19.3319s/100 iter), loss = 0.117844
I0630 23:26:39.304380  6183 solver.cpp:309]     Train net output #0: loss = 0.117844 (* 1 = 0.117844 loss)
I0630 23:26:39.304396  6183 sgd_solver.cpp:106] Iteration 19600, lr = 1e-05
I0630 23:26:58.363487  6183 solver.cpp:290] Iteration 19700 (5.24697 iter/s, 19.0586s/100 iter), loss = 0.08412
I0630 23:26:58.363509  6183 solver.cpp:309]     Train net output #0: loss = 0.0841202 (* 1 = 0.0841202 loss)
I0630 23:26:58.363518  6183 sgd_solver.cpp:106] Iteration 19700, lr = 1e-05
I0630 23:27:17.641175  6183 solver.cpp:290] Iteration 19800 (5.18749 iter/s, 19.2772s/100 iter), loss = 0.105927
I0630 23:27:17.641263  6183 solver.cpp:309]     Train net output #0: loss = 0.105927 (* 1 = 0.105927 loss)
I0630 23:27:17.641275  6183 sgd_solver.cpp:106] Iteration 19800, lr = 1e-05
I0630 23:27:36.986711  6183 solver.cpp:290] Iteration 19900 (5.16931 iter/s, 19.3449s/100 iter), loss = 0.190298
I0630 23:27:36.986737  6183 solver.cpp:309]     Train net output #0: loss = 0.190298 (* 1 = 0.190298 loss)
I0630 23:27:36.986747  6183 sgd_solver.cpp:106] Iteration 19900, lr = 1e-05
I0630 23:27:55.880105  6183 solver.cpp:598] Snapshotting to binary proto file training/cityscapes20_jsegnet21v2_2017-06-30_19-26-17/sparse/cityscapes20_jsegnet21v2_iter_20000.caffemodel
I0630 23:27:55.944000  6183 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cityscapes20_jsegnet21v2_2017-06-30_19-26-17/sparse/cityscapes20_jsegnet21v2_iter_20000.solverstate
I0630 23:27:55.964663  6183 solver.cpp:354] Sparsity after update:
I0630 23:27:55.966459  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 23:27:55.966470  6183 net.cpp:1851] conv1a_param_0(0.4) 
I0630 23:27:55.966478  6183 net.cpp:1851] conv1b_param_0(0.774) 
I0630 23:27:55.966481  6183 net.cpp:1851] ctx_conv1_param_0(0.63) 
I0630 23:27:55.966483  6183 net.cpp:1851] ctx_conv2_param_0(0.686) 
I0630 23:27:55.966486  6183 net.cpp:1851] ctx_conv3_param_0(0.626) 
I0630 23:27:55.966488  6183 net.cpp:1851] ctx_conv4_param_0(0.675) 
I0630 23:27:55.966491  6183 net.cpp:1851] ctx_final_param_0(0.4) 
I0630 23:27:55.966493  6183 net.cpp:1851] out3a_param_0(0.757) 
I0630 23:27:55.966496  6183 net.cpp:1851] out5a_param_0(0.784) 
I0630 23:27:55.966500  6183 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0630 23:27:55.966501  6183 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0630 23:27:55.966505  6183 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0630 23:27:55.966506  6183 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0630 23:27:55.966511  6183 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0630 23:27:55.966513  6183 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0630 23:27:55.966516  6183 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0630 23:27:55.966521  6183 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0630 23:27:55.966523  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.12736e+06/2.69808e+06) 0.788
I0630 23:27:55.966786  6183 solver.cpp:471] Iteration 20000, Testing net (#0)
I0630 23:29:33.596391  6183 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.922686
I0630 23:29:33.596483  6183 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.993074
I0630 23:29:33.596490  6183 solver.cpp:544]     Test net output #2: loss = 0.162889 (* 1 = 0.162889 loss)
I0630 23:29:33.829605  6183 solver.cpp:290] Iteration 20000 (0.855873 iter/s, 116.84s/100 iter), loss = 0.106042
I0630 23:29:33.829632  6183 solver.cpp:309]     Train net output #0: loss = 0.106042 (* 1 = 0.106042 loss)
I0630 23:29:33.829640  6183 sgd_solver.cpp:106] Iteration 20000, lr = 1e-05
I0630 23:29:52.943260  6183 solver.cpp:290] Iteration 20100 (5.23201 iter/s, 19.1131s/100 iter), loss = 0.102775
I0630 23:29:52.943284  6183 solver.cpp:309]     Train net output #0: loss = 0.102775 (* 1 = 0.102775 loss)
I0630 23:29:52.943290  6183 sgd_solver.cpp:106] Iteration 20100, lr = 1e-05
I0630 23:30:12.060621  6183 solver.cpp:290] Iteration 20200 (5.23099 iter/s, 19.1168s/100 iter), loss = 0.254848
I0630 23:30:12.060712  6183 solver.cpp:309]     Train net output #0: loss = 0.254848 (* 1 = 0.254848 loss)
I0630 23:30:12.060724  6183 sgd_solver.cpp:106] Iteration 20200, lr = 1e-05
I0630 23:30:31.086813  6183 solver.cpp:290] Iteration 20300 (5.25608 iter/s, 19.0256s/100 iter), loss = 0.118165
I0630 23:30:31.086836  6183 solver.cpp:309]     Train net output #0: loss = 0.118166 (* 1 = 0.118166 loss)
I0630 23:30:31.086843  6183 sgd_solver.cpp:106] Iteration 20300, lr = 1e-05
I0630 23:30:50.362169  6183 solver.cpp:290] Iteration 20400 (5.18812 iter/s, 19.2748s/100 iter), loss = 0.154691
I0630 23:30:50.362247  6183 solver.cpp:309]     Train net output #0: loss = 0.154691 (* 1 = 0.154691 loss)
I0630 23:30:50.362256  6183 sgd_solver.cpp:106] Iteration 20400, lr = 1e-05
I0630 23:31:09.465854  6183 solver.cpp:290] Iteration 20500 (5.23475 iter/s, 19.1031s/100 iter), loss = 0.104798
I0630 23:31:09.465880  6183 solver.cpp:309]     Train net output #0: loss = 0.104798 (* 1 = 0.104798 loss)
I0630 23:31:09.465889  6183 sgd_solver.cpp:106] Iteration 20500, lr = 1e-05
I0630 23:31:28.571815  6183 solver.cpp:290] Iteration 20600 (5.23412 iter/s, 19.1054s/100 iter), loss = 0.152037
I0630 23:31:28.571904  6183 solver.cpp:309]     Train net output #0: loss = 0.152037 (* 1 = 0.152037 loss)
I0630 23:31:28.571914  6183 sgd_solver.cpp:106] Iteration 20600, lr = 1e-05
I0630 23:31:47.626900  6183 solver.cpp:290] Iteration 20700 (5.24811 iter/s, 19.0545s/100 iter), loss = 0.12621
I0630 23:31:47.626926  6183 solver.cpp:309]     Train net output #0: loss = 0.12621 (* 1 = 0.12621 loss)
I0630 23:31:47.626935  6183 sgd_solver.cpp:106] Iteration 20700, lr = 1e-05
I0630 23:32:06.528376  6183 solver.cpp:290] Iteration 20800 (5.29074 iter/s, 18.9009s/100 iter), loss = 0.105916
I0630 23:32:06.528455  6183 solver.cpp:309]     Train net output #0: loss = 0.105916 (* 1 = 0.105916 loss)
I0630 23:32:06.528465  6183 sgd_solver.cpp:106] Iteration 20800, lr = 1e-05
I0630 23:32:25.663877  6183 solver.cpp:290] Iteration 20900 (5.22605 iter/s, 19.1349s/100 iter), loss = 0.139631
I0630 23:32:25.663902  6183 solver.cpp:309]     Train net output #0: loss = 0.139631 (* 1 = 0.139631 loss)
I0630 23:32:25.663910  6183 sgd_solver.cpp:106] Iteration 20900, lr = 1e-05
I0630 23:32:44.537457  6183 solver.cpp:354] Sparsity after update:
I0630 23:32:44.609839  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 23:32:44.609856  6183 net.cpp:1851] conv1a_param_0(0.4) 
I0630 23:32:44.609865  6183 net.cpp:1851] conv1b_param_0(0.774) 
I0630 23:32:44.609868  6183 net.cpp:1851] ctx_conv1_param_0(0.63) 
I0630 23:32:44.609869  6183 net.cpp:1851] ctx_conv2_param_0(0.686) 
I0630 23:32:44.609871  6183 net.cpp:1851] ctx_conv3_param_0(0.626) 
I0630 23:32:44.609874  6183 net.cpp:1851] ctx_conv4_param_0(0.675) 
I0630 23:32:44.609875  6183 net.cpp:1851] ctx_final_param_0(0.4) 
I0630 23:32:44.609877  6183 net.cpp:1851] out3a_param_0(0.757) 
I0630 23:32:44.609879  6183 net.cpp:1851] out5a_param_0(0.784) 
I0630 23:32:44.609881  6183 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0630 23:32:44.609884  6183 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0630 23:32:44.609887  6183 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0630 23:32:44.609889  6183 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0630 23:32:44.609892  6183 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0630 23:32:44.609894  6183 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0630 23:32:44.609896  6183 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0630 23:32:44.609899  6183 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0630 23:32:44.609901  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.12736e+06/2.69808e+06) 0.788
I0630 23:32:44.781489  6183 solver.cpp:290] Iteration 21000 (5.23092 iter/s, 19.1171s/100 iter), loss = 0.173716
I0630 23:32:44.781513  6183 solver.cpp:309]     Train net output #0: loss = 0.173717 (* 1 = 0.173717 loss)
I0630 23:32:44.781522  6183 sgd_solver.cpp:106] Iteration 21000, lr = 1e-05
I0630 23:33:03.873744  6183 solver.cpp:290] Iteration 21100 (5.23787 iter/s, 19.0917s/100 iter), loss = 0.161753
I0630 23:33:03.873765  6183 solver.cpp:309]     Train net output #0: loss = 0.161753 (* 1 = 0.161753 loss)
I0630 23:33:03.873772  6183 sgd_solver.cpp:106] Iteration 21100, lr = 1e-05
I0630 23:33:23.115752  6183 solver.cpp:290] Iteration 21200 (5.19711 iter/s, 19.2415s/100 iter), loss = 0.107395
I0630 23:33:23.115823  6183 solver.cpp:309]     Train net output #0: loss = 0.107396 (* 1 = 0.107396 loss)
I0630 23:33:23.115831  6183 sgd_solver.cpp:106] Iteration 21200, lr = 1e-05
I0630 23:33:42.313567  6183 solver.cpp:290] Iteration 21300 (5.20908 iter/s, 19.1972s/100 iter), loss = 0.189351
I0630 23:33:42.313591  6183 solver.cpp:309]     Train net output #0: loss = 0.189352 (* 1 = 0.189352 loss)
I0630 23:33:42.313598  6183 sgd_solver.cpp:106] Iteration 21300, lr = 1e-05
I0630 23:34:01.530546  6183 solver.cpp:290] Iteration 21400 (5.20388 iter/s, 19.2164s/100 iter), loss = 0.133369
I0630 23:34:01.530622  6183 solver.cpp:309]     Train net output #0: loss = 0.133369 (* 1 = 0.133369 loss)
I0630 23:34:01.530630  6183 sgd_solver.cpp:106] Iteration 21400, lr = 1e-05
I0630 23:34:20.901052  6183 solver.cpp:290] Iteration 21500 (5.16264 iter/s, 19.3699s/100 iter), loss = 0.129442
I0630 23:34:20.901077  6183 solver.cpp:309]     Train net output #0: loss = 0.129442 (* 1 = 0.129442 loss)
I0630 23:34:20.901087  6183 sgd_solver.cpp:106] Iteration 21500, lr = 1e-05
I0630 23:34:39.918730  6183 solver.cpp:290] Iteration 21600 (5.25841 iter/s, 19.0172s/100 iter), loss = 0.163995
I0630 23:34:39.918784  6183 solver.cpp:309]     Train net output #0: loss = 0.163995 (* 1 = 0.163995 loss)
I0630 23:34:39.918793  6183 sgd_solver.cpp:106] Iteration 21600, lr = 1e-05
I0630 23:34:59.075197  6183 solver.cpp:290] Iteration 21700 (5.22032 iter/s, 19.1559s/100 iter), loss = 0.086914
I0630 23:34:59.075220  6183 solver.cpp:309]     Train net output #0: loss = 0.0869142 (* 1 = 0.0869142 loss)
I0630 23:34:59.075227  6183 sgd_solver.cpp:106] Iteration 21700, lr = 1e-05
I0630 23:35:18.108088  6183 solver.cpp:290] Iteration 21800 (5.25421 iter/s, 19.0324s/100 iter), loss = 0.110905
I0630 23:35:18.108170  6183 solver.cpp:309]     Train net output #0: loss = 0.110905 (* 1 = 0.110905 loss)
I0630 23:35:18.108180  6183 sgd_solver.cpp:106] Iteration 21800, lr = 1e-05
I0630 23:35:37.240037  6183 solver.cpp:290] Iteration 21900 (5.22702 iter/s, 19.1314s/100 iter), loss = 0.0675026
I0630 23:35:37.240061  6183 solver.cpp:309]     Train net output #0: loss = 0.0675028 (* 1 = 0.0675028 loss)
I0630 23:35:37.240067  6183 sgd_solver.cpp:106] Iteration 21900, lr = 1e-05
I0630 23:35:56.204010  6183 solver.cpp:354] Sparsity after update:
I0630 23:35:56.205930  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 23:35:56.205937  6183 net.cpp:1851] conv1a_param_0(0.4) 
I0630 23:35:56.205945  6183 net.cpp:1851] conv1b_param_0(0.774) 
I0630 23:35:56.205947  6183 net.cpp:1851] ctx_conv1_param_0(0.63) 
I0630 23:35:56.205950  6183 net.cpp:1851] ctx_conv2_param_0(0.686) 
I0630 23:35:56.205951  6183 net.cpp:1851] ctx_conv3_param_0(0.626) 
I0630 23:35:56.205953  6183 net.cpp:1851] ctx_conv4_param_0(0.675) 
I0630 23:35:56.205955  6183 net.cpp:1851] ctx_final_param_0(0.4) 
I0630 23:35:56.205957  6183 net.cpp:1851] out3a_param_0(0.757) 
I0630 23:35:56.205960  6183 net.cpp:1851] out5a_param_0(0.784) 
I0630 23:35:56.205961  6183 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0630 23:35:56.205963  6183 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0630 23:35:56.205965  6183 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0630 23:35:56.205967  6183 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0630 23:35:56.205970  6183 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0630 23:35:56.205971  6183 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0630 23:35:56.205973  6183 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0630 23:35:56.205976  6183 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0630 23:35:56.205977  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.12736e+06/2.69808e+06) 0.788
I0630 23:35:56.206113  6183 solver.cpp:471] Iteration 22000, Testing net (#0)
I0630 23:37:33.325731  6183 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.923743
I0630 23:37:33.325837  6183 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.993272
I0630 23:37:33.325845  6183 solver.cpp:544]     Test net output #2: loss = 0.158933 (* 1 = 0.158933 loss)
I0630 23:37:33.534974  6183 solver.cpp:290] Iteration 22000 (0.859905 iter/s, 116.292s/100 iter), loss = 0.119662
I0630 23:37:33.535002  6183 solver.cpp:309]     Train net output #0: loss = 0.119662 (* 1 = 0.119662 loss)
I0630 23:37:33.535012  6183 sgd_solver.cpp:106] Iteration 22000, lr = 1e-05
I0630 23:37:52.744016  6183 solver.cpp:290] Iteration 22100 (5.20603 iter/s, 19.2085s/100 iter), loss = 0.13355
I0630 23:37:52.744040  6183 solver.cpp:309]     Train net output #0: loss = 0.133551 (* 1 = 0.133551 loss)
I0630 23:37:52.744046  6183 sgd_solver.cpp:106] Iteration 22100, lr = 1e-05
I0630 23:38:11.963317  6183 solver.cpp:290] Iteration 22200 (5.20325 iter/s, 19.2188s/100 iter), loss = 0.139913
I0630 23:38:11.963402  6183 solver.cpp:309]     Train net output #0: loss = 0.139913 (* 1 = 0.139913 loss)
I0630 23:38:11.963413  6183 sgd_solver.cpp:106] Iteration 22200, lr = 1e-05
I0630 23:38:31.074694  6183 solver.cpp:290] Iteration 22300 (5.23265 iter/s, 19.1108s/100 iter), loss = 0.128327
I0630 23:38:31.074718  6183 solver.cpp:309]     Train net output #0: loss = 0.128327 (* 1 = 0.128327 loss)
I0630 23:38:31.074725  6183 sgd_solver.cpp:106] Iteration 22300, lr = 1e-05
I0630 23:38:50.082636  6183 solver.cpp:290] Iteration 22400 (5.26111 iter/s, 19.0074s/100 iter), loss = 0.113436
I0630 23:38:50.082715  6183 solver.cpp:309]     Train net output #0: loss = 0.113436 (* 1 = 0.113436 loss)
I0630 23:38:50.082721  6183 sgd_solver.cpp:106] Iteration 22400, lr = 1e-05
I0630 23:39:09.015527  6183 solver.cpp:290] Iteration 22500 (5.28198 iter/s, 18.9323s/100 iter), loss = 0.0798705
I0630 23:39:09.015550  6183 solver.cpp:309]     Train net output #0: loss = 0.0798707 (* 1 = 0.0798707 loss)
I0630 23:39:09.015558  6183 sgd_solver.cpp:106] Iteration 22500, lr = 1e-05
I0630 23:39:28.355583  6183 solver.cpp:290] Iteration 22600 (5.17076 iter/s, 19.3395s/100 iter), loss = 0.230824
I0630 23:39:28.355630  6183 solver.cpp:309]     Train net output #0: loss = 0.230824 (* 1 = 0.230824 loss)
I0630 23:39:28.355638  6183 sgd_solver.cpp:106] Iteration 22600, lr = 1e-05
I0630 23:39:47.376857  6183 solver.cpp:290] Iteration 22700 (5.25742 iter/s, 19.0207s/100 iter), loss = 0.0751275
I0630 23:39:47.376881  6183 solver.cpp:309]     Train net output #0: loss = 0.0751278 (* 1 = 0.0751278 loss)
I0630 23:39:47.376888  6183 sgd_solver.cpp:106] Iteration 22700, lr = 1e-05
I0630 23:40:06.535131  6183 solver.cpp:290] Iteration 22800 (5.21982 iter/s, 19.1577s/100 iter), loss = 0.169822
I0630 23:40:06.535218  6183 solver.cpp:309]     Train net output #0: loss = 0.169823 (* 1 = 0.169823 loss)
I0630 23:40:06.535228  6183 sgd_solver.cpp:106] Iteration 22800, lr = 1e-05
I0630 23:40:25.569236  6183 solver.cpp:290] Iteration 22900 (5.25389 iter/s, 19.0335s/100 iter), loss = 0.113699
I0630 23:40:25.569259  6183 solver.cpp:309]     Train net output #0: loss = 0.113699 (* 1 = 0.113699 loss)
I0630 23:40:25.569267  6183 sgd_solver.cpp:106] Iteration 22900, lr = 1e-05
I0630 23:40:44.545735  6183 solver.cpp:354] Sparsity after update:
I0630 23:40:44.615218  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 23:40:44.615236  6183 net.cpp:1851] conv1a_param_0(0.4) 
I0630 23:40:44.615243  6183 net.cpp:1851] conv1b_param_0(0.774) 
I0630 23:40:44.615245  6183 net.cpp:1851] ctx_conv1_param_0(0.63) 
I0630 23:40:44.615247  6183 net.cpp:1851] ctx_conv2_param_0(0.686) 
I0630 23:40:44.615249  6183 net.cpp:1851] ctx_conv3_param_0(0.626) 
I0630 23:40:44.615252  6183 net.cpp:1851] ctx_conv4_param_0(0.675) 
I0630 23:40:44.615253  6183 net.cpp:1851] ctx_final_param_0(0.4) 
I0630 23:40:44.615255  6183 net.cpp:1851] out3a_param_0(0.757) 
I0630 23:40:44.615257  6183 net.cpp:1851] out5a_param_0(0.784) 
I0630 23:40:44.615259  6183 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0630 23:40:44.615262  6183 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0630 23:40:44.615263  6183 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0630 23:40:44.615265  6183 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0630 23:40:44.615267  6183 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0630 23:40:44.615269  6183 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0630 23:40:44.615272  6183 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0630 23:40:44.615273  6183 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0630 23:40:44.615275  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.12736e+06/2.69808e+06) 0.788
I0630 23:40:44.790849  6183 solver.cpp:290] Iteration 23000 (5.20262 iter/s, 19.2211s/100 iter), loss = 0.213282
I0630 23:40:44.790874  6183 solver.cpp:309]     Train net output #0: loss = 0.213283 (* 1 = 0.213283 loss)
I0630 23:40:44.790880  6183 sgd_solver.cpp:106] Iteration 23000, lr = 1e-05
I0630 23:41:03.877699  6183 solver.cpp:290] Iteration 23100 (5.23936 iter/s, 19.0863s/100 iter), loss = 0.117535
I0630 23:41:03.877723  6183 solver.cpp:309]     Train net output #0: loss = 0.117536 (* 1 = 0.117536 loss)
I0630 23:41:03.877730  6183 sgd_solver.cpp:106] Iteration 23100, lr = 1e-05
I0630 23:41:22.912995  6183 solver.cpp:290] Iteration 23200 (5.25355 iter/s, 19.0348s/100 iter), loss = 0.126882
I0630 23:41:22.913092  6183 solver.cpp:309]     Train net output #0: loss = 0.126883 (* 1 = 0.126883 loss)
I0630 23:41:22.913100  6183 sgd_solver.cpp:106] Iteration 23200, lr = 1e-05
I0630 23:41:42.019321  6183 solver.cpp:290] Iteration 23300 (5.23404 iter/s, 19.1057s/100 iter), loss = 0.105304
I0630 23:41:42.019345  6183 solver.cpp:309]     Train net output #0: loss = 0.105304 (* 1 = 0.105304 loss)
I0630 23:41:42.019352  6183 sgd_solver.cpp:106] Iteration 23300, lr = 1e-05
I0630 23:42:01.200431  6183 solver.cpp:290] Iteration 23400 (5.21361 iter/s, 19.1806s/100 iter), loss = 0.0881742
I0630 23:42:01.200505  6183 solver.cpp:309]     Train net output #0: loss = 0.0881745 (* 1 = 0.0881745 loss)
I0630 23:42:01.200512  6183 sgd_solver.cpp:106] Iteration 23400, lr = 1e-05
I0630 23:42:20.523497  6183 solver.cpp:290] Iteration 23500 (5.17533 iter/s, 19.3225s/100 iter), loss = 0.174609
I0630 23:42:20.523519  6183 solver.cpp:309]     Train net output #0: loss = 0.174609 (* 1 = 0.174609 loss)
I0630 23:42:20.523525  6183 sgd_solver.cpp:106] Iteration 23500, lr = 1e-05
I0630 23:42:39.759374  6183 solver.cpp:290] Iteration 23600 (5.19877 iter/s, 19.2353s/100 iter), loss = 0.248752
I0630 23:42:39.759470  6183 solver.cpp:309]     Train net output #0: loss = 0.248752 (* 1 = 0.248752 loss)
I0630 23:42:39.759479  6183 sgd_solver.cpp:106] Iteration 23600, lr = 1e-05
I0630 23:42:58.829797  6183 solver.cpp:290] Iteration 23700 (5.24389 iter/s, 19.0698s/100 iter), loss = 0.0915703
I0630 23:42:58.829820  6183 solver.cpp:309]     Train net output #0: loss = 0.0915706 (* 1 = 0.0915706 loss)
I0630 23:42:58.829826  6183 sgd_solver.cpp:106] Iteration 23700, lr = 1e-05
I0630 23:43:17.956482  6183 solver.cpp:290] Iteration 23800 (5.22845 iter/s, 19.1261s/100 iter), loss = 0.106737
I0630 23:43:17.956531  6183 solver.cpp:309]     Train net output #0: loss = 0.106737 (* 1 = 0.106737 loss)
I0630 23:43:17.956538  6183 sgd_solver.cpp:106] Iteration 23800, lr = 1e-05
I0630 23:43:37.058265  6183 solver.cpp:290] Iteration 23900 (5.23527 iter/s, 19.1012s/100 iter), loss = 0.146452
I0630 23:43:37.058290  6183 solver.cpp:309]     Train net output #0: loss = 0.146452 (* 1 = 0.146452 loss)
I0630 23:43:37.058296  6183 sgd_solver.cpp:106] Iteration 23900, lr = 1e-05
I0630 23:43:56.146049  6183 solver.cpp:354] Sparsity after update:
I0630 23:43:56.147927  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 23:43:56.147933  6183 net.cpp:1851] conv1a_param_0(0.4) 
I0630 23:43:56.147941  6183 net.cpp:1851] conv1b_param_0(0.774) 
I0630 23:43:56.147944  6183 net.cpp:1851] ctx_conv1_param_0(0.63) 
I0630 23:43:56.147946  6183 net.cpp:1851] ctx_conv2_param_0(0.686) 
I0630 23:43:56.147948  6183 net.cpp:1851] ctx_conv3_param_0(0.626) 
I0630 23:43:56.147951  6183 net.cpp:1851] ctx_conv4_param_0(0.675) 
I0630 23:43:56.147953  6183 net.cpp:1851] ctx_final_param_0(0.4) 
I0630 23:43:56.147955  6183 net.cpp:1851] out3a_param_0(0.757) 
I0630 23:43:56.147958  6183 net.cpp:1851] out5a_param_0(0.784) 
I0630 23:43:56.147959  6183 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0630 23:43:56.147963  6183 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0630 23:43:56.147965  6183 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0630 23:43:56.147970  6183 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0630 23:43:56.147972  6183 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0630 23:43:56.147974  6183 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0630 23:43:56.147977  6183 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0630 23:43:56.147981  6183 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0630 23:43:56.147985  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.12736e+06/2.69808e+06) 0.788
I0630 23:43:56.148123  6183 solver.cpp:471] Iteration 24000, Testing net (#0)
I0630 23:45:33.859681  6183 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.924386
I0630 23:45:33.859793  6183 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.993536
I0630 23:45:33.859800  6183 solver.cpp:544]     Test net output #2: loss = 0.157914 (* 1 = 0.157914 loss)
I0630 23:45:34.064378  6344 sgd_solver.cpp:46] MultiStep Status: Iteration 24000, step = 1
I0630 23:45:34.064373  6183 solver.cpp:290] Iteration 24000 (0.85468 iter/s, 117.003s/100 iter), loss = 0.0797874
I0630 23:45:34.064404  6183 solver.cpp:309]     Train net output #0: loss = 0.0797877 (* 1 = 0.0797877 loss)
I0630 23:45:34.064410  6183 sgd_solver.cpp:46] MultiStep Status: Iteration 24000, step = 1
I0630 23:45:34.064414  6183 sgd_solver.cpp:106] Iteration 24000, lr = 1e-06
I0630 23:45:34.064383  6345 sgd_solver.cpp:46] MultiStep Status: Iteration 24000, step = 1
I0630 23:45:53.427959  6183 solver.cpp:290] Iteration 24100 (5.16448 iter/s, 19.363s/100 iter), loss = 0.116088
I0630 23:45:53.427985  6183 solver.cpp:309]     Train net output #0: loss = 0.116088 (* 1 = 0.116088 loss)
I0630 23:45:53.427994  6183 sgd_solver.cpp:106] Iteration 24100, lr = 1e-06
I0630 23:46:12.434206  6183 solver.cpp:290] Iteration 24200 (5.26158 iter/s, 19.0057s/100 iter), loss = 0.115731
I0630 23:46:12.434317  6183 solver.cpp:309]     Train net output #0: loss = 0.115731 (* 1 = 0.115731 loss)
I0630 23:46:12.434331  6183 sgd_solver.cpp:106] Iteration 24200, lr = 1e-06
I0630 23:46:31.513533  6183 solver.cpp:290] Iteration 24300 (5.24145 iter/s, 19.0787s/100 iter), loss = 0.0898752
I0630 23:46:31.513559  6183 solver.cpp:309]     Train net output #0: loss = 0.0898755 (* 1 = 0.0898755 loss)
I0630 23:46:31.513568  6183 sgd_solver.cpp:106] Iteration 24300, lr = 1e-06
I0630 23:46:50.574815  6183 solver.cpp:290] Iteration 24400 (5.24639 iter/s, 19.0607s/100 iter), loss = 0.081263
I0630 23:46:50.574868  6183 solver.cpp:309]     Train net output #0: loss = 0.0812633 (* 1 = 0.0812633 loss)
I0630 23:46:50.574877  6183 sgd_solver.cpp:106] Iteration 24400, lr = 1e-06
I0630 23:47:09.602005  6183 solver.cpp:290] Iteration 24500 (5.2558 iter/s, 19.0266s/100 iter), loss = 0.120052
I0630 23:47:09.602030  6183 solver.cpp:309]     Train net output #0: loss = 0.120052 (* 1 = 0.120052 loss)
I0630 23:47:09.602039  6183 sgd_solver.cpp:106] Iteration 24500, lr = 1e-06
I0630 23:47:28.520531  6183 solver.cpp:290] Iteration 24600 (5.28598 iter/s, 18.918s/100 iter), loss = 0.115887
I0630 23:47:28.520607  6183 solver.cpp:309]     Train net output #0: loss = 0.115887 (* 1 = 0.115887 loss)
I0630 23:47:28.520617  6183 sgd_solver.cpp:106] Iteration 24600, lr = 1e-06
I0630 23:47:47.580261  6183 solver.cpp:290] Iteration 24700 (5.24683 iter/s, 19.0591s/100 iter), loss = 0.146809
I0630 23:47:47.580284  6183 solver.cpp:309]     Train net output #0: loss = 0.146809 (* 1 = 0.146809 loss)
I0630 23:47:47.580291  6183 sgd_solver.cpp:106] Iteration 24700, lr = 1e-06
I0630 23:48:06.718706  6183 solver.cpp:290] Iteration 24800 (5.22524 iter/s, 19.1379s/100 iter), loss = 0.0606489
I0630 23:48:06.718803  6183 solver.cpp:309]     Train net output #0: loss = 0.0606492 (* 1 = 0.0606492 loss)
I0630 23:48:06.718816  6183 sgd_solver.cpp:106] Iteration 24800, lr = 1e-06
I0630 23:48:25.858268  6183 solver.cpp:290] Iteration 24900 (5.22495 iter/s, 19.1389s/100 iter), loss = 0.101544
I0630 23:48:25.858291  6183 solver.cpp:309]     Train net output #0: loss = 0.101544 (* 1 = 0.101544 loss)
I0630 23:48:25.858299  6183 sgd_solver.cpp:106] Iteration 24900, lr = 1e-06
I0630 23:48:44.832186  6183 solver.cpp:354] Sparsity after update:
I0630 23:48:44.921409  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 23:48:44.921427  6183 net.cpp:1851] conv1a_param_0(0.4) 
I0630 23:48:44.921434  6183 net.cpp:1851] conv1b_param_0(0.774) 
I0630 23:48:44.921437  6183 net.cpp:1851] ctx_conv1_param_0(0.63) 
I0630 23:48:44.921438  6183 net.cpp:1851] ctx_conv2_param_0(0.686) 
I0630 23:48:44.921440  6183 net.cpp:1851] ctx_conv3_param_0(0.626) 
I0630 23:48:44.921442  6183 net.cpp:1851] ctx_conv4_param_0(0.675) 
I0630 23:48:44.921444  6183 net.cpp:1851] ctx_final_param_0(0.4) 
I0630 23:48:44.921447  6183 net.cpp:1851] out3a_param_0(0.757) 
I0630 23:48:44.921448  6183 net.cpp:1851] out5a_param_0(0.784) 
I0630 23:48:44.921450  6183 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0630 23:48:44.921453  6183 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0630 23:48:44.921455  6183 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0630 23:48:44.921458  6183 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0630 23:48:44.921460  6183 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0630 23:48:44.921463  6183 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0630 23:48:44.921466  6183 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0630 23:48:44.921469  6183 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0630 23:48:44.921473  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.12736e+06/2.69808e+06) 0.788
I0630 23:48:45.096645  6183 solver.cpp:290] Iteration 25000 (5.19809 iter/s, 19.2378s/100 iter), loss = 0.174162
I0630 23:48:45.096668  6183 solver.cpp:309]     Train net output #0: loss = 0.174162 (* 1 = 0.174162 loss)
I0630 23:48:45.096674  6183 sgd_solver.cpp:106] Iteration 25000, lr = 1e-06
I0630 23:49:04.153375  6183 solver.cpp:290] Iteration 25100 (5.24764 iter/s, 19.0562s/100 iter), loss = 0.117994
I0630 23:49:04.153401  6183 solver.cpp:309]     Train net output #0: loss = 0.117994 (* 1 = 0.117994 loss)
I0630 23:49:04.153410  6183 sgd_solver.cpp:106] Iteration 25100, lr = 1e-06
I0630 23:49:23.429985  6183 solver.cpp:290] Iteration 25200 (5.18779 iter/s, 19.276s/100 iter), loss = 0.112656
I0630 23:49:23.430058  6183 solver.cpp:309]     Train net output #0: loss = 0.112656 (* 1 = 0.112656 loss)
I0630 23:49:23.430065  6183 sgd_solver.cpp:106] Iteration 25200, lr = 1e-06
I0630 23:49:42.602918  6183 solver.cpp:290] Iteration 25300 (5.21585 iter/s, 19.1723s/100 iter), loss = 0.108097
I0630 23:49:42.602941  6183 solver.cpp:309]     Train net output #0: loss = 0.108097 (* 1 = 0.108097 loss)
I0630 23:49:42.602948  6183 sgd_solver.cpp:106] Iteration 25300, lr = 1e-06
I0630 23:50:01.902642  6183 solver.cpp:290] Iteration 25400 (5.18157 iter/s, 19.2992s/100 iter), loss = 0.294087
I0630 23:50:01.902925  6183 solver.cpp:309]     Train net output #0: loss = 0.294087 (* 1 = 0.294087 loss)
I0630 23:50:01.902932  6183 sgd_solver.cpp:106] Iteration 25400, lr = 1e-06
I0630 23:50:21.143316  6183 solver.cpp:290] Iteration 25500 (5.19754 iter/s, 19.2399s/100 iter), loss = 0.17124
I0630 23:50:21.143337  6183 solver.cpp:309]     Train net output #0: loss = 0.17124 (* 1 = 0.17124 loss)
I0630 23:50:21.143343  6183 sgd_solver.cpp:106] Iteration 25500, lr = 1e-06
I0630 23:50:40.302605  6183 solver.cpp:290] Iteration 25600 (5.21955 iter/s, 19.1587s/100 iter), loss = 0.137686
I0630 23:50:40.302651  6183 solver.cpp:309]     Train net output #0: loss = 0.137686 (* 1 = 0.137686 loss)
I0630 23:50:40.302659  6183 sgd_solver.cpp:106] Iteration 25600, lr = 1e-06
I0630 23:50:59.523438  6183 solver.cpp:290] Iteration 25700 (5.20285 iter/s, 19.2203s/100 iter), loss = 0.098362
I0630 23:50:59.523464  6183 solver.cpp:309]     Train net output #0: loss = 0.0983623 (* 1 = 0.0983623 loss)
I0630 23:50:59.523471  6183 sgd_solver.cpp:106] Iteration 25700, lr = 1e-06
I0630 23:51:18.807512  6183 solver.cpp:290] Iteration 25800 (5.18578 iter/s, 19.2835s/100 iter), loss = 0.0822623
I0630 23:51:18.807634  6183 solver.cpp:309]     Train net output #0: loss = 0.0822625 (* 1 = 0.0822625 loss)
I0630 23:51:18.807643  6183 sgd_solver.cpp:106] Iteration 25800, lr = 1e-06
I0630 23:51:37.904783  6183 solver.cpp:290] Iteration 25900 (5.23653 iter/s, 19.0966s/100 iter), loss = 0.0717812
I0630 23:51:37.904806  6183 solver.cpp:309]     Train net output #0: loss = 0.0717814 (* 1 = 0.0717814 loss)
I0630 23:51:37.904814  6183 sgd_solver.cpp:106] Iteration 25900, lr = 1e-06
I0630 23:51:56.883004  6183 solver.cpp:354] Sparsity after update:
I0630 23:51:56.884631  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 23:51:56.884639  6183 net.cpp:1851] conv1a_param_0(0.4) 
I0630 23:51:56.884646  6183 net.cpp:1851] conv1b_param_0(0.774) 
I0630 23:51:56.884649  6183 net.cpp:1851] ctx_conv1_param_0(0.63) 
I0630 23:51:56.884650  6183 net.cpp:1851] ctx_conv2_param_0(0.686) 
I0630 23:51:56.884652  6183 net.cpp:1851] ctx_conv3_param_0(0.626) 
I0630 23:51:56.884654  6183 net.cpp:1851] ctx_conv4_param_0(0.675) 
I0630 23:51:56.884656  6183 net.cpp:1851] ctx_final_param_0(0.4) 
I0630 23:51:56.884658  6183 net.cpp:1851] out3a_param_0(0.757) 
I0630 23:51:56.884660  6183 net.cpp:1851] out5a_param_0(0.784) 
I0630 23:51:56.884662  6183 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0630 23:51:56.884665  6183 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0630 23:51:56.884666  6183 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0630 23:51:56.884668  6183 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0630 23:51:56.884670  6183 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0630 23:51:56.884672  6183 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0630 23:51:56.884675  6183 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0630 23:51:56.884678  6183 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0630 23:51:56.884680  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.12736e+06/2.69808e+06) 0.788
I0630 23:51:56.884811  6183 solver.cpp:471] Iteration 26000, Testing net (#0)
I0630 23:53:34.080459  6183 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.924609
I0630 23:53:34.080540  6183 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.993512
I0630 23:53:34.080546  6183 solver.cpp:544]     Test net output #2: loss = 0.157464 (* 1 = 0.157464 loss)
I0630 23:53:34.275108  6183 solver.cpp:290] Iteration 26000 (0.859349 iter/s, 116.367s/100 iter), loss = 0.112323
I0630 23:53:34.275133  6183 solver.cpp:309]     Train net output #0: loss = 0.112324 (* 1 = 0.112324 loss)
I0630 23:53:34.275143  6183 sgd_solver.cpp:106] Iteration 26000, lr = 1e-06
I0630 23:53:53.187489  6183 solver.cpp:290] Iteration 26100 (5.2877 iter/s, 18.9118s/100 iter), loss = 0.159115
I0630 23:53:53.187513  6183 solver.cpp:309]     Train net output #0: loss = 0.159115 (* 1 = 0.159115 loss)
I0630 23:53:53.187520  6183 sgd_solver.cpp:106] Iteration 26100, lr = 1e-06
I0630 23:54:12.305052  6183 solver.cpp:290] Iteration 26200 (5.23095 iter/s, 19.117s/100 iter), loss = 0.0516412
I0630 23:54:12.305162  6183 solver.cpp:309]     Train net output #0: loss = 0.0516414 (* 1 = 0.0516414 loss)
I0630 23:54:12.305172  6183 sgd_solver.cpp:106] Iteration 26200, lr = 1e-06
I0630 23:54:31.174593  6183 solver.cpp:290] Iteration 26300 (5.29973 iter/s, 18.8689s/100 iter), loss = 0.0973212
I0630 23:54:31.174619  6183 solver.cpp:309]     Train net output #0: loss = 0.0973214 (* 1 = 0.0973214 loss)
I0630 23:54:31.174633  6183 sgd_solver.cpp:106] Iteration 26300, lr = 1e-06
I0630 23:54:50.282600  6183 solver.cpp:290] Iteration 26400 (5.23356 iter/s, 19.1074s/100 iter), loss = 0.274427
I0630 23:54:50.282682  6183 solver.cpp:309]     Train net output #0: loss = 0.274427 (* 1 = 0.274427 loss)
I0630 23:54:50.282693  6183 sgd_solver.cpp:106] Iteration 26400, lr = 1e-06
I0630 23:55:09.339397  6183 solver.cpp:290] Iteration 26500 (5.24764 iter/s, 19.0562s/100 iter), loss = 0.110842
I0630 23:55:09.339421  6183 solver.cpp:309]     Train net output #0: loss = 0.110842 (* 1 = 0.110842 loss)
I0630 23:55:09.339427  6183 sgd_solver.cpp:106] Iteration 26500, lr = 1e-06
I0630 23:55:28.463243  6183 solver.cpp:290] Iteration 26600 (5.22923 iter/s, 19.1233s/100 iter), loss = 0.175785
I0630 23:55:28.463305  6183 solver.cpp:309]     Train net output #0: loss = 0.175785 (* 1 = 0.175785 loss)
I0630 23:55:28.463313  6183 sgd_solver.cpp:106] Iteration 26600, lr = 1e-06
I0630 23:55:47.623656  6183 solver.cpp:290] Iteration 26700 (5.21926 iter/s, 19.1598s/100 iter), loss = 0.113886
I0630 23:55:47.623677  6183 solver.cpp:309]     Train net output #0: loss = 0.113886 (* 1 = 0.113886 loss)
I0630 23:55:47.623683  6183 sgd_solver.cpp:106] Iteration 26700, lr = 1e-06
I0630 23:56:06.552568  6183 solver.cpp:290] Iteration 26800 (5.28308 iter/s, 18.9284s/100 iter), loss = 0.114191
I0630 23:56:06.552613  6183 solver.cpp:309]     Train net output #0: loss = 0.114191 (* 1 = 0.114191 loss)
I0630 23:56:06.552619  6183 sgd_solver.cpp:106] Iteration 26800, lr = 1e-06
I0630 23:56:25.799031  6183 solver.cpp:290] Iteration 26900 (5.19592 iter/s, 19.2459s/100 iter), loss = 0.156358
I0630 23:56:25.799053  6183 solver.cpp:309]     Train net output #0: loss = 0.156358 (* 1 = 0.156358 loss)
I0630 23:56:25.799060  6183 sgd_solver.cpp:106] Iteration 26900, lr = 1e-06
I0630 23:56:44.904948  6183 solver.cpp:354] Sparsity after update:
I0630 23:56:44.974923  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 23:56:44.974943  6183 net.cpp:1851] conv1a_param_0(0.4) 
I0630 23:56:44.974954  6183 net.cpp:1851] conv1b_param_0(0.774) 
I0630 23:56:44.974957  6183 net.cpp:1851] ctx_conv1_param_0(0.63) 
I0630 23:56:44.974959  6183 net.cpp:1851] ctx_conv2_param_0(0.686) 
I0630 23:56:44.974966  6183 net.cpp:1851] ctx_conv3_param_0(0.626) 
I0630 23:56:44.974969  6183 net.cpp:1851] ctx_conv4_param_0(0.675) 
I0630 23:56:44.974974  6183 net.cpp:1851] ctx_final_param_0(0.4) 
I0630 23:56:44.974978  6183 net.cpp:1851] out3a_param_0(0.757) 
I0630 23:56:44.974983  6183 net.cpp:1851] out5a_param_0(0.784) 
I0630 23:56:44.974987  6183 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0630 23:56:44.974992  6183 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0630 23:56:44.974995  6183 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0630 23:56:44.975000  6183 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0630 23:56:44.975003  6183 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0630 23:56:44.975008  6183 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0630 23:56:44.975011  6183 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0630 23:56:44.975016  6183 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0630 23:56:44.975020  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.12736e+06/2.69808e+06) 0.788
I0630 23:56:45.145750  6183 solver.cpp:290] Iteration 27000 (5.16898 iter/s, 19.3462s/100 iter), loss = 0.117241
I0630 23:56:45.145773  6183 solver.cpp:309]     Train net output #0: loss = 0.117241 (* 1 = 0.117241 loss)
I0630 23:56:45.145781  6183 sgd_solver.cpp:106] Iteration 27000, lr = 1e-06
I0630 23:57:04.003620  6183 solver.cpp:290] Iteration 27100 (5.30298 iter/s, 18.8573s/100 iter), loss = 0.0746588
I0630 23:57:04.003644  6183 solver.cpp:309]     Train net output #0: loss = 0.074659 (* 1 = 0.074659 loss)
I0630 23:57:04.003650  6183 sgd_solver.cpp:106] Iteration 27100, lr = 1e-06
I0630 23:57:23.303915  6183 solver.cpp:290] Iteration 27200 (5.18142 iter/s, 19.2997s/100 iter), loss = 0.141996
I0630 23:57:23.303969  6183 solver.cpp:309]     Train net output #0: loss = 0.141996 (* 1 = 0.141996 loss)
I0630 23:57:23.303977  6183 sgd_solver.cpp:106] Iteration 27200, lr = 1e-06
I0630 23:57:42.467144  6183 solver.cpp:290] Iteration 27300 (5.21849 iter/s, 19.1626s/100 iter), loss = 0.143237
I0630 23:57:42.467167  6183 solver.cpp:309]     Train net output #0: loss = 0.143237 (* 1 = 0.143237 loss)
I0630 23:57:42.467173  6183 sgd_solver.cpp:106] Iteration 27300, lr = 1e-06
I0630 23:58:01.726447  6183 solver.cpp:290] Iteration 27400 (5.19244 iter/s, 19.2588s/100 iter), loss = 0.0647739
I0630 23:58:01.726505  6183 solver.cpp:309]     Train net output #0: loss = 0.0647742 (* 1 = 0.0647742 loss)
I0630 23:58:01.726511  6183 sgd_solver.cpp:106] Iteration 27400, lr = 1e-06
I0630 23:58:20.936101  6183 solver.cpp:290] Iteration 27500 (5.20587 iter/s, 19.2091s/100 iter), loss = 0.115319
I0630 23:58:20.936125  6183 solver.cpp:309]     Train net output #0: loss = 0.11532 (* 1 = 0.11532 loss)
I0630 23:58:20.936131  6183 sgd_solver.cpp:106] Iteration 27500, lr = 1e-06
I0630 23:58:40.101933  6183 solver.cpp:290] Iteration 27600 (5.21777 iter/s, 19.1653s/100 iter), loss = 0.0751673
I0630 23:58:40.101986  6183 solver.cpp:309]     Train net output #0: loss = 0.0751675 (* 1 = 0.0751675 loss)
I0630 23:58:40.101994  6183 sgd_solver.cpp:106] Iteration 27600, lr = 1e-06
I0630 23:58:59.267094  6183 solver.cpp:290] Iteration 27700 (5.21796 iter/s, 19.1646s/100 iter), loss = 0.108228
I0630 23:58:59.267119  6183 solver.cpp:309]     Train net output #0: loss = 0.108228 (* 1 = 0.108228 loss)
I0630 23:58:59.267127  6183 sgd_solver.cpp:106] Iteration 27700, lr = 1e-06
I0630 23:59:18.079409  6183 solver.cpp:290] Iteration 27800 (5.31582 iter/s, 18.8118s/100 iter), loss = 0.0760967
I0630 23:59:18.079459  6183 solver.cpp:309]     Train net output #0: loss = 0.0760969 (* 1 = 0.0760969 loss)
I0630 23:59:18.079468  6183 sgd_solver.cpp:106] Iteration 27800, lr = 1e-06
I0630 23:59:37.127424  6183 solver.cpp:290] Iteration 27900 (5.25005 iter/s, 19.0474s/100 iter), loss = 0.114615
I0630 23:59:37.127446  6183 solver.cpp:309]     Train net output #0: loss = 0.114616 (* 1 = 0.114616 loss)
I0630 23:59:37.127452  6183 sgd_solver.cpp:106] Iteration 27900, lr = 1e-06
I0630 23:59:56.139376  6183 solver.cpp:354] Sparsity after update:
I0630 23:59:56.141288  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0630 23:59:56.141295  6183 net.cpp:1851] conv1a_param_0(0.4) 
I0630 23:59:56.141302  6183 net.cpp:1851] conv1b_param_0(0.774) 
I0630 23:59:56.141304  6183 net.cpp:1851] ctx_conv1_param_0(0.63) 
I0630 23:59:56.141306  6183 net.cpp:1851] ctx_conv2_param_0(0.686) 
I0630 23:59:56.141309  6183 net.cpp:1851] ctx_conv3_param_0(0.626) 
I0630 23:59:56.141310  6183 net.cpp:1851] ctx_conv4_param_0(0.675) 
I0630 23:59:56.141312  6183 net.cpp:1851] ctx_final_param_0(0.4) 
I0630 23:59:56.141314  6183 net.cpp:1851] out3a_param_0(0.757) 
I0630 23:59:56.141316  6183 net.cpp:1851] out5a_param_0(0.784) 
I0630 23:59:56.141317  6183 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0630 23:59:56.141319  6183 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0630 23:59:56.141321  6183 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0630 23:59:56.141324  6183 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0630 23:59:56.141325  6183 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0630 23:59:56.141327  6183 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0630 23:59:56.141330  6183 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0630 23:59:56.141332  6183 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0630 23:59:56.141335  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.12736e+06/2.69808e+06) 0.788
I0630 23:59:56.141471  6183 solver.cpp:471] Iteration 28000, Testing net (#0)
I0701 00:01:33.851636  6183 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.924717
I0701 00:01:33.851716  6183 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.993542
I0701 00:01:33.851723  6183 solver.cpp:544]     Test net output #2: loss = 0.156471 (* 1 = 0.156471 loss)
I0701 00:01:34.057849  6183 solver.cpp:290] Iteration 28000 (0.855234 iter/s, 116.927s/100 iter), loss = 0.0882793
I0701 00:01:34.057875  6183 solver.cpp:309]     Train net output #0: loss = 0.0882796 (* 1 = 0.0882796 loss)
I0701 00:01:34.057881  6183 sgd_solver.cpp:106] Iteration 28000, lr = 1e-06
I0701 00:01:53.311319  6183 solver.cpp:290] Iteration 28100 (5.19403 iter/s, 19.2529s/100 iter), loss = 0.128896
I0701 00:01:53.311347  6183 solver.cpp:309]     Train net output #0: loss = 0.128896 (* 1 = 0.128896 loss)
I0701 00:01:53.311355  6183 sgd_solver.cpp:106] Iteration 28100, lr = 1e-06
I0701 00:02:12.432482  6183 solver.cpp:290] Iteration 28200 (5.22996 iter/s, 19.1206s/100 iter), loss = 0.169995
I0701 00:02:12.432556  6183 solver.cpp:309]     Train net output #0: loss = 0.169996 (* 1 = 0.169996 loss)
I0701 00:02:12.432569  6183 sgd_solver.cpp:106] Iteration 28200, lr = 1e-06
I0701 00:02:31.496423  6183 solver.cpp:290] Iteration 28300 (5.24568 iter/s, 19.0633s/100 iter), loss = 0.162979
I0701 00:02:31.496445  6183 solver.cpp:309]     Train net output #0: loss = 0.162979 (* 1 = 0.162979 loss)
I0701 00:02:31.496453  6183 sgd_solver.cpp:106] Iteration 28300, lr = 1e-06
I0701 00:02:50.737573  6183 solver.cpp:290] Iteration 28400 (5.19735 iter/s, 19.2406s/100 iter), loss = 0.126353
I0701 00:02:50.737684  6183 solver.cpp:309]     Train net output #0: loss = 0.126354 (* 1 = 0.126354 loss)
I0701 00:02:50.737694  6183 sgd_solver.cpp:106] Iteration 28400, lr = 1e-06
I0701 00:03:09.767961  6183 solver.cpp:290] Iteration 28500 (5.25493 iter/s, 19.0297s/100 iter), loss = 0.0879228
I0701 00:03:09.767987  6183 solver.cpp:309]     Train net output #0: loss = 0.0879231 (* 1 = 0.0879231 loss)
I0701 00:03:09.767994  6183 sgd_solver.cpp:106] Iteration 28500, lr = 1e-06
I0701 00:03:29.006009  6183 solver.cpp:290] Iteration 28600 (5.19819 iter/s, 19.2375s/100 iter), loss = 0.167123
I0701 00:03:29.006418  6183 solver.cpp:309]     Train net output #0: loss = 0.167124 (* 1 = 0.167124 loss)
I0701 00:03:29.006448  6183 sgd_solver.cpp:106] Iteration 28600, lr = 1e-06
I0701 00:03:48.142246  6183 solver.cpp:290] Iteration 28700 (5.22595 iter/s, 19.1353s/100 iter), loss = 0.0678674
I0701 00:03:48.142271  6183 solver.cpp:309]     Train net output #0: loss = 0.0678676 (* 1 = 0.0678676 loss)
I0701 00:03:48.142294  6183 sgd_solver.cpp:106] Iteration 28700, lr = 1e-06
I0701 00:04:07.398535  6183 solver.cpp:290] Iteration 28800 (5.19326 iter/s, 19.2557s/100 iter), loss = 0.153917
I0701 00:04:07.398610  6183 solver.cpp:309]     Train net output #0: loss = 0.153918 (* 1 = 0.153918 loss)
I0701 00:04:07.398620  6183 sgd_solver.cpp:106] Iteration 28800, lr = 1e-06
I0701 00:04:26.651180  6183 solver.cpp:290] Iteration 28900 (5.19426 iter/s, 19.252s/100 iter), loss = 0.167103
I0701 00:04:26.651202  6183 solver.cpp:309]     Train net output #0: loss = 0.167104 (* 1 = 0.167104 loss)
I0701 00:04:26.651209  6183 sgd_solver.cpp:106] Iteration 28900, lr = 1e-06
I0701 00:04:45.886432  6183 solver.cpp:354] Sparsity after update:
I0701 00:04:45.960880  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0701 00:04:45.960896  6183 net.cpp:1851] conv1a_param_0(0.4) 
I0701 00:04:45.960906  6183 net.cpp:1851] conv1b_param_0(0.774) 
I0701 00:04:45.960907  6183 net.cpp:1851] ctx_conv1_param_0(0.63) 
I0701 00:04:45.960909  6183 net.cpp:1851] ctx_conv2_param_0(0.686) 
I0701 00:04:45.960911  6183 net.cpp:1851] ctx_conv3_param_0(0.626) 
I0701 00:04:45.960913  6183 net.cpp:1851] ctx_conv4_param_0(0.675) 
I0701 00:04:45.960923  6183 net.cpp:1851] ctx_final_param_0(0.4) 
I0701 00:04:45.960925  6183 net.cpp:1851] out3a_param_0(0.757) 
I0701 00:04:45.960928  6183 net.cpp:1851] out5a_param_0(0.784) 
I0701 00:04:45.960930  6183 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0701 00:04:45.960933  6183 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0701 00:04:45.960935  6183 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0701 00:04:45.960937  6183 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0701 00:04:45.960940  6183 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0701 00:04:45.960942  6183 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0701 00:04:45.960944  6183 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0701 00:04:45.960947  6183 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0701 00:04:45.960949  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.12736e+06/2.69808e+06) 0.788
I0701 00:04:46.132360  6183 solver.cpp:290] Iteration 29000 (5.13331 iter/s, 19.4806s/100 iter), loss = 0.144085
I0701 00:04:46.132382  6183 solver.cpp:309]     Train net output #0: loss = 0.144086 (* 1 = 0.144086 loss)
I0701 00:04:46.132390  6183 sgd_solver.cpp:106] Iteration 29000, lr = 1e-06
I0701 00:05:05.294777  6183 solver.cpp:290] Iteration 29100 (5.2187 iter/s, 19.1618s/100 iter), loss = 0.0822346
I0701 00:05:05.294806  6183 solver.cpp:309]     Train net output #0: loss = 0.0822349 (* 1 = 0.0822349 loss)
I0701 00:05:05.294816  6183 sgd_solver.cpp:106] Iteration 29100, lr = 1e-06
I0701 00:05:24.422474  6183 solver.cpp:290] Iteration 29200 (5.22818 iter/s, 19.1271s/100 iter), loss = 0.0920118
I0701 00:05:24.422580  6183 solver.cpp:309]     Train net output #0: loss = 0.0920121 (* 1 = 0.0920121 loss)
I0701 00:05:24.422591  6183 sgd_solver.cpp:106] Iteration 29200, lr = 1e-06
I0701 00:05:43.475329  6183 solver.cpp:290] Iteration 29300 (5.24873 iter/s, 19.0522s/100 iter), loss = 0.068017
I0701 00:05:43.475353  6183 solver.cpp:309]     Train net output #0: loss = 0.0680174 (* 1 = 0.0680174 loss)
I0701 00:05:43.475360  6183 sgd_solver.cpp:106] Iteration 29300, lr = 1e-06
I0701 00:06:02.674185  6183 solver.cpp:290] Iteration 29400 (5.2088 iter/s, 19.1983s/100 iter), loss = 0.105338
I0701 00:06:02.674234  6183 solver.cpp:309]     Train net output #0: loss = 0.105338 (* 1 = 0.105338 loss)
I0701 00:06:02.674244  6183 sgd_solver.cpp:106] Iteration 29400, lr = 1e-06
I0701 00:06:21.609387  6183 solver.cpp:290] Iteration 29500 (5.28133 iter/s, 18.9346s/100 iter), loss = 0.0891665
I0701 00:06:21.609411  6183 solver.cpp:309]     Train net output #0: loss = 0.0891668 (* 1 = 0.0891668 loss)
I0701 00:06:21.609417  6183 sgd_solver.cpp:106] Iteration 29500, lr = 1e-06
I0701 00:06:40.772693  6183 solver.cpp:290] Iteration 29600 (5.21846 iter/s, 19.1627s/100 iter), loss = 0.183283
I0701 00:06:40.772799  6183 solver.cpp:309]     Train net output #0: loss = 0.183283 (* 1 = 0.183283 loss)
I0701 00:06:40.772809  6183 sgd_solver.cpp:106] Iteration 29600, lr = 1e-06
I0701 00:07:00.094964  6183 solver.cpp:290] Iteration 29700 (5.17555 iter/s, 19.3216s/100 iter), loss = 0.175247
I0701 00:07:00.094986  6183 solver.cpp:309]     Train net output #0: loss = 0.175247 (* 1 = 0.175247 loss)
I0701 00:07:00.094993  6183 sgd_solver.cpp:106] Iteration 29700, lr = 1e-06
I0701 00:07:19.442138  6183 solver.cpp:290] Iteration 29800 (5.16887 iter/s, 19.3466s/100 iter), loss = 0.133367
I0701 00:07:19.442183  6183 solver.cpp:309]     Train net output #0: loss = 0.133367 (* 1 = 0.133367 loss)
I0701 00:07:19.442189  6183 sgd_solver.cpp:106] Iteration 29800, lr = 1e-06
I0701 00:07:38.578651  6183 solver.cpp:290] Iteration 29900 (5.22577 iter/s, 19.1359s/100 iter), loss = 0.111642
I0701 00:07:38.578675  6183 solver.cpp:309]     Train net output #0: loss = 0.111643 (* 1 = 0.111643 loss)
I0701 00:07:38.578680  6183 sgd_solver.cpp:106] Iteration 29900, lr = 1e-06
I0701 00:07:57.349455  6183 solver.cpp:598] Snapshotting to binary proto file training/cityscapes20_jsegnet21v2_2017-06-30_19-26-17/sparse/cityscapes20_jsegnet21v2_iter_30000.caffemodel
I0701 00:07:57.378254  6183 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cityscapes20_jsegnet21v2_2017-06-30_19-26-17/sparse/cityscapes20_jsegnet21v2_iter_30000.solverstate
I0701 00:07:57.394956  6183 solver.cpp:354] Sparsity after update:
I0701 00:07:57.396486  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0701 00:07:57.396493  6183 net.cpp:1851] conv1a_param_0(0.4) 
I0701 00:07:57.396502  6183 net.cpp:1851] conv1b_param_0(0.774) 
I0701 00:07:57.396504  6183 net.cpp:1851] ctx_conv1_param_0(0.63) 
I0701 00:07:57.396507  6183 net.cpp:1851] ctx_conv2_param_0(0.686) 
I0701 00:07:57.396508  6183 net.cpp:1851] ctx_conv3_param_0(0.626) 
I0701 00:07:57.396510  6183 net.cpp:1851] ctx_conv4_param_0(0.675) 
I0701 00:07:57.396512  6183 net.cpp:1851] ctx_final_param_0(0.4) 
I0701 00:07:57.396514  6183 net.cpp:1851] out3a_param_0(0.757) 
I0701 00:07:57.396517  6183 net.cpp:1851] out5a_param_0(0.784) 
I0701 00:07:57.396518  6183 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0701 00:07:57.396520  6183 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0701 00:07:57.396522  6183 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0701 00:07:57.396524  6183 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0701 00:07:57.396526  6183 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0701 00:07:57.396528  6183 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0701 00:07:57.396530  6183 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0701 00:07:57.396533  6183 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0701 00:07:57.396534  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.12736e+06/2.69808e+06) 0.788
I0701 00:07:57.396682  6183 solver.cpp:471] Iteration 30000, Testing net (#0)
I0701 00:09:35.236037  6183 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.925086
I0701 00:09:35.236143  6183 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.993577
I0701 00:09:35.236151  6183 solver.cpp:544]     Test net output #2: loss = 0.156085 (* 1 = 0.156085 loss)
I0701 00:09:35.450376  6183 solver.cpp:290] Iteration 30000 (0.855663 iter/s, 116.868s/100 iter), loss = 0.170123
I0701 00:09:35.450404  6183 solver.cpp:309]     Train net output #0: loss = 0.170124 (* 1 = 0.170124 loss)
I0701 00:09:35.450412  6183 sgd_solver.cpp:106] Iteration 30000, lr = 1e-06
I0701 00:09:54.716028  6183 solver.cpp:290] Iteration 30100 (5.19074 iter/s, 19.2651s/100 iter), loss = 0.106507
I0701 00:09:54.716054  6183 solver.cpp:309]     Train net output #0: loss = 0.106507 (* 1 = 0.106507 loss)
I0701 00:09:54.716060  6183 sgd_solver.cpp:106] Iteration 30100, lr = 1e-06
I0701 00:10:13.863876  6183 solver.cpp:290] Iteration 30200 (5.22267 iter/s, 19.1473s/100 iter), loss = 0.161513
I0701 00:10:13.863960  6183 solver.cpp:309]     Train net output #0: loss = 0.161513 (* 1 = 0.161513 loss)
I0701 00:10:13.863970  6183 sgd_solver.cpp:106] Iteration 30200, lr = 1e-06
I0701 00:10:32.991152  6183 solver.cpp:290] Iteration 30300 (5.22831 iter/s, 19.1267s/100 iter), loss = 0.144989
I0701 00:10:32.991174  6183 solver.cpp:309]     Train net output #0: loss = 0.144989 (* 1 = 0.144989 loss)
I0701 00:10:32.991181  6183 sgd_solver.cpp:106] Iteration 30300, lr = 1e-06
I0701 00:10:51.943864  6183 solver.cpp:290] Iteration 30400 (5.27645 iter/s, 18.9522s/100 iter), loss = 0.10105
I0701 00:10:51.943943  6183 solver.cpp:309]     Train net output #0: loss = 0.101051 (* 1 = 0.101051 loss)
I0701 00:10:51.943950  6183 sgd_solver.cpp:106] Iteration 30400, lr = 1e-06
I0701 00:11:11.162377  6183 solver.cpp:290] Iteration 30500 (5.20349 iter/s, 19.2179s/100 iter), loss = 0.129386
I0701 00:11:11.162408  6183 solver.cpp:309]     Train net output #0: loss = 0.129386 (* 1 = 0.129386 loss)
I0701 00:11:11.162417  6183 sgd_solver.cpp:106] Iteration 30500, lr = 1e-06
I0701 00:11:30.262593  6183 solver.cpp:290] Iteration 30600 (5.2357 iter/s, 19.0996s/100 iter), loss = 0.0875575
I0701 00:11:30.262679  6183 solver.cpp:309]     Train net output #0: loss = 0.0875578 (* 1 = 0.0875578 loss)
I0701 00:11:30.262691  6183 sgd_solver.cpp:106] Iteration 30600, lr = 1e-06
I0701 00:11:49.266680  6183 solver.cpp:290] Iteration 30700 (5.2622 iter/s, 19.0035s/100 iter), loss = 0.0917084
I0701 00:11:49.266703  6183 solver.cpp:309]     Train net output #0: loss = 0.0917087 (* 1 = 0.0917087 loss)
I0701 00:11:49.266710  6183 sgd_solver.cpp:106] Iteration 30700, lr = 1e-06
I0701 00:12:08.513634  6183 solver.cpp:290] Iteration 30800 (5.19578 iter/s, 19.2464s/100 iter), loss = 0.0952825
I0701 00:12:08.513689  6183 solver.cpp:309]     Train net output #0: loss = 0.0952828 (* 1 = 0.0952828 loss)
I0701 00:12:08.513697  6183 sgd_solver.cpp:106] Iteration 30800, lr = 1e-06
I0701 00:12:27.640071  6183 solver.cpp:290] Iteration 30900 (5.22853 iter/s, 19.1258s/100 iter), loss = 0.106022
I0701 00:12:27.640097  6183 solver.cpp:309]     Train net output #0: loss = 0.106022 (* 1 = 0.106022 loss)
I0701 00:12:27.640107  6183 sgd_solver.cpp:106] Iteration 30900, lr = 1e-06
I0701 00:12:46.510815  6183 solver.cpp:354] Sparsity after update:
I0701 00:12:46.579355  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0701 00:12:46.579372  6183 net.cpp:1851] conv1a_param_0(0.4) 
I0701 00:12:46.579380  6183 net.cpp:1851] conv1b_param_0(0.774) 
I0701 00:12:46.579382  6183 net.cpp:1851] ctx_conv1_param_0(0.63) 
I0701 00:12:46.579385  6183 net.cpp:1851] ctx_conv2_param_0(0.686) 
I0701 00:12:46.579386  6183 net.cpp:1851] ctx_conv3_param_0(0.626) 
I0701 00:12:46.579388  6183 net.cpp:1851] ctx_conv4_param_0(0.675) 
I0701 00:12:46.579391  6183 net.cpp:1851] ctx_final_param_0(0.4) 
I0701 00:12:46.579392  6183 net.cpp:1851] out3a_param_0(0.757) 
I0701 00:12:46.579394  6183 net.cpp:1851] out5a_param_0(0.784) 
I0701 00:12:46.579396  6183 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0701 00:12:46.579399  6183 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0701 00:12:46.579401  6183 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0701 00:12:46.579403  6183 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0701 00:12:46.579406  6183 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0701 00:12:46.579407  6183 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0701 00:12:46.579409  6183 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0701 00:12:46.579411  6183 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0701 00:12:46.579413  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.12736e+06/2.69808e+06) 0.788
I0701 00:12:46.750656  6183 solver.cpp:290] Iteration 31000 (5.23286 iter/s, 19.11s/100 iter), loss = 0.105885
I0701 00:12:46.750682  6183 solver.cpp:309]     Train net output #0: loss = 0.105885 (* 1 = 0.105885 loss)
I0701 00:12:46.750696  6183 sgd_solver.cpp:106] Iteration 31000, lr = 1e-06
I0701 00:13:05.914633  6183 solver.cpp:290] Iteration 31100 (5.21828 iter/s, 19.1634s/100 iter), loss = 0.0881869
I0701 00:13:05.914655  6183 solver.cpp:309]     Train net output #0: loss = 0.0881873 (* 1 = 0.0881873 loss)
I0701 00:13:05.914662  6183 sgd_solver.cpp:106] Iteration 31100, lr = 1e-06
I0701 00:13:25.056110  6183 solver.cpp:290] Iteration 31200 (5.22441 iter/s, 19.1409s/100 iter), loss = 0.177802
I0701 00:13:25.056180  6183 solver.cpp:309]     Train net output #0: loss = 0.177802 (* 1 = 0.177802 loss)
I0701 00:13:25.056191  6183 sgd_solver.cpp:106] Iteration 31200, lr = 1e-06
I0701 00:13:44.322732  6183 solver.cpp:290] Iteration 31300 (5.19049 iter/s, 19.266s/100 iter), loss = 0.138647
I0701 00:13:44.322757  6183 solver.cpp:309]     Train net output #0: loss = 0.138647 (* 1 = 0.138647 loss)
I0701 00:13:44.322763  6183 sgd_solver.cpp:106] Iteration 31300, lr = 1e-06
I0701 00:14:03.342833  6183 solver.cpp:290] Iteration 31400 (5.25775 iter/s, 19.0195s/100 iter), loss = 0.198583
I0701 00:14:03.342877  6183 solver.cpp:309]     Train net output #0: loss = 0.198584 (* 1 = 0.198584 loss)
I0701 00:14:03.342887  6183 sgd_solver.cpp:106] Iteration 31400, lr = 1e-06
I0701 00:14:22.394807  6183 solver.cpp:290] Iteration 31500 (5.24896 iter/s, 19.0514s/100 iter), loss = 0.0775808
I0701 00:14:22.394831  6183 solver.cpp:309]     Train net output #0: loss = 0.0775812 (* 1 = 0.0775812 loss)
I0701 00:14:22.394841  6183 sgd_solver.cpp:106] Iteration 31500, lr = 1e-06
I0701 00:14:41.294786  6183 solver.cpp:290] Iteration 31600 (5.29116 iter/s, 18.8994s/100 iter), loss = 0.111411
I0701 00:14:41.294863  6183 solver.cpp:309]     Train net output #0: loss = 0.111412 (* 1 = 0.111412 loss)
I0701 00:14:41.294873  6183 sgd_solver.cpp:106] Iteration 31600, lr = 1e-06
I0701 00:15:00.460767  6183 solver.cpp:290] Iteration 31700 (5.21775 iter/s, 19.1654s/100 iter), loss = 0.108201
I0701 00:15:00.460790  6183 solver.cpp:309]     Train net output #0: loss = 0.108202 (* 1 = 0.108202 loss)
I0701 00:15:00.460798  6183 sgd_solver.cpp:106] Iteration 31700, lr = 1e-06
I0701 00:15:19.652194  6183 solver.cpp:290] Iteration 31800 (5.21081 iter/s, 19.1909s/100 iter), loss = 0.0576294
I0701 00:15:19.652245  6183 solver.cpp:309]     Train net output #0: loss = 0.0576297 (* 1 = 0.0576297 loss)
I0701 00:15:19.652256  6183 sgd_solver.cpp:106] Iteration 31800, lr = 1e-06
I0701 00:15:38.995736  6183 solver.cpp:290] Iteration 31900 (5.16984 iter/s, 19.343s/100 iter), loss = 0.0822659
I0701 00:15:38.995760  6183 solver.cpp:309]     Train net output #0: loss = 0.0822662 (* 1 = 0.0822662 loss)
I0701 00:15:38.995767  6183 sgd_solver.cpp:106] Iteration 31900, lr = 1e-06
I0701 00:15:57.933223  6183 solver.cpp:354] Sparsity after update:
I0701 00:15:57.935084  6183 net.cpp:1842] Num Params(17), Sparsity (zero_weights/count): 
I0701 00:15:57.935094  6183 net.cpp:1851] conv1a_param_0(0.4) 
I0701 00:15:57.935102  6183 net.cpp:1851] conv1b_param_0(0.774) 
I0701 00:15:57.935104  6183 net.cpp:1851] ctx_conv1_param_0(0.63) 
I0701 00:15:57.935106  6183 net.cpp:1851] ctx_conv2_param_0(0.686) 
I0701 00:15:57.935108  6183 net.cpp:1851] ctx_conv3_param_0(0.626) 
I0701 00:15:57.935111  6183 net.cpp:1851] ctx_conv4_param_0(0.675) 
I0701 00:15:57.935112  6183 net.cpp:1851] ctx_final_param_0(0.4) 
I0701 00:15:57.935114  6183 net.cpp:1851] out3a_param_0(0.757) 
I0701 00:15:57.935117  6183 net.cpp:1851] out5a_param_0(0.784) 
I0701 00:15:57.935118  6183 net.cpp:1851] res2a_branch2a_param_0(0.8) 
I0701 00:15:57.935120  6183 net.cpp:1851] res2a_branch2b_param_0(0.8) 
I0701 00:15:57.935122  6183 net.cpp:1851] res3a_branch2a_param_0(0.8) 
I0701 00:15:57.935124  6183 net.cpp:1851] res3a_branch2b_param_0(0.8) 
I0701 00:15:57.935127  6183 net.cpp:1851] res4a_branch2a_param_0(0.8) 
I0701 00:15:57.935128  6183 net.cpp:1851] res4a_branch2b_param_0(0.8) 
I0701 00:15:57.935130  6183 net.cpp:1851] res5a_branch2a_param_0(0.8) 
I0701 00:15:57.935132  6183 net.cpp:1851] res5a_branch2b_param_0(0.8) 
I0701 00:15:57.935135  6183 net.cpp:1853] Total Sparsity (zero_weights/count) =  (2.12736e+06/2.69808e+06) 0.788
I0701 00:15:57.935145  6183 solver.cpp:598] Snapshotting to binary proto file training/cityscapes20_jsegnet21v2_2017-06-30_19-26-17/sparse/cityscapes20_jsegnet21v2_iter_32000.caffemodel
I0701 00:15:57.963176  6183 sgd_solver.cpp:273] Snapshotting solver state to binary proto file training/cityscapes20_jsegnet21v2_2017-06-30_19-26-17/sparse/cityscapes20_jsegnet21v2_iter_32000.solverstate
I0701 00:15:58.037686  6183 solver.cpp:451] Iteration 32000, loss = 0.110268
I0701 00:15:58.037705  6183 solver.cpp:471] Iteration 32000, Testing net (#0)
I0701 00:17:34.066874  6183 solver.cpp:544]     Test net output #0: accuracy/top1 = 0.924956
I0701 00:17:34.066947  6183 solver.cpp:544]     Test net output #1: accuracy/top5 = 0.993579
I0701 00:17:34.066956  6183 solver.cpp:544]     Test net output #2: loss = 0.156318 (* 1 = 0.156318 loss)
I0701 00:17:34.066959  6183 solver.cpp:456] Optimization Done.
I0701 00:17:34.312887  6183 caffe.cpp:246] Optimization Done.
training/cityscapes20_jsegnet21v2_2017-06-30_19-26-17/test
I0701 00:17:44.847193  9899 caffe.cpp:264] Not using GPU #2 for single-GPU function
I0701 00:17:44.847302  9899 caffe.cpp:264] Not using GPU #1 for single-GPU function
I0701 00:17:45.942719  9899 caffe.cpp:273] Use GPU with device ID 0
I0701 00:17:45.943646  9899 caffe.cpp:277] GPU device name: GeForce GTX 1080
I0701 00:17:46.755717  9899 net.cpp:56] Initializing net from parameters: 
name: "jsegnet21v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/val-image-lmdb"
    label_list_path: "data/val-label-lmdb"
    batch_size: 4
    threads: 4
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a/bn"
  top: "conv1a/bn"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a/bn"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b/bn"
  top: "conv1b/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2a/bn"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a/bn"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b/bn"
  top: "res2a_branch2b/bn"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b/bn"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2a/bn"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a/bn"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b/bn"
  top: "res3a_branch2b/bn"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b/bn"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2a/bn"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a/bn"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b/bn"
  top: "res4a_branch2b/bn"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b/bn"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2a/bn"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a/bn"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b/bn"
  top: "res5a_branch2b/bn"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b/bn"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a/bn"
  top: "out5a/bn"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a/bn"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b/bn"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a/bn"
  top: "out3a/bn"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a/bn"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv1/bn"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1/bn"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv2/bn"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2/bn"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv3/bn"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3/bn"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4/bn"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_filler {
      type: "constant"
      value: 1
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4/bn"
  top: "ctx_conv4/bn"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4/bn"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 20
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 20
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 20
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
  accuracy_param {
    ignore_label: 255
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
    ignore_label: 255
  }
}
I0701 00:17:46.763599  9899 layer_factory.hpp:77] Creating layer data
I0701 00:17:46.763723  9899 net.cpp:98] Creating Layer data
I0701 00:17:46.763752  9899 net.cpp:413] data -> data
I0701 00:17:46.763815  9899 net.cpp:413] data -> label
I0701 00:17:46.786564  9952 db_lmdb.cpp:35] Opened lmdb data/val-image-lmdb
I0701 00:17:46.790571  9899 data_layer.cpp:78] ReshapePrefetch 4, 3, 640, 640
I0701 00:17:46.790612  9899 data_layer.cpp:83] output data size: 4,3,640,640
I0701 00:17:46.815685  9957 db_lmdb.cpp:35] Opened lmdb data/val-label-lmdb
I0701 00:17:46.817770  9899 data_layer.cpp:78] ReshapePrefetch 4, 1, 640, 640
I0701 00:17:46.817824  9899 data_layer.cpp:83] output data size: 4,1,640,640
I0701 00:17:46.832326  9899 net.cpp:148] Setting up data
I0701 00:17:46.832383  9899 net.cpp:155] Top shape: 4 3 640 640 (4915200)
I0701 00:17:46.832391  9899 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0701 00:17:46.832394  9899 net.cpp:163] Memory required for data: 26214400
I0701 00:17:46.832415  9899 layer_factory.hpp:77] Creating layer label_data_1_split
I0701 00:17:46.832439  9899 net.cpp:98] Creating Layer label_data_1_split
I0701 00:17:46.832448  9899 net.cpp:439] label_data_1_split <- label
I0701 00:17:46.832502  9899 net.cpp:413] label_data_1_split -> label_data_1_split_0
I0701 00:17:46.832515  9899 net.cpp:413] label_data_1_split -> label_data_1_split_1
I0701 00:17:46.832527  9899 net.cpp:413] label_data_1_split -> label_data_1_split_2
I0701 00:17:46.834403  9899 net.cpp:148] Setting up label_data_1_split
I0701 00:17:46.834411  9899 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0701 00:17:46.834414  9899 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0701 00:17:46.834415  9899 net.cpp:155] Top shape: 4 1 640 640 (1638400)
I0701 00:17:46.834417  9899 net.cpp:163] Memory required for data: 45875200
I0701 00:17:46.834420  9899 layer_factory.hpp:77] Creating layer data/bias
I0701 00:17:46.834426  9899 net.cpp:98] Creating Layer data/bias
I0701 00:17:46.834429  9899 net.cpp:439] data/bias <- data
I0701 00:17:46.834434  9899 net.cpp:413] data/bias -> data/bias
I0701 00:17:46.835453  9899 net.cpp:148] Setting up data/bias
I0701 00:17:46.835467  9899 net.cpp:155] Top shape: 4 3 640 640 (4915200)
I0701 00:17:46.835470  9899 net.cpp:163] Memory required for data: 65536000
I0701 00:17:46.835489  9899 layer_factory.hpp:77] Creating layer conv1a
I0701 00:17:46.835511  9899 net.cpp:98] Creating Layer conv1a
I0701 00:17:46.835517  9899 net.cpp:439] conv1a <- data/bias
I0701 00:17:46.835522  9899 net.cpp:413] conv1a -> conv1a
I0701 00:17:46.838974  9899 net.cpp:148] Setting up conv1a
I0701 00:17:46.839027  9899 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0701 00:17:46.839031  9899 net.cpp:163] Memory required for data: 117964800
I0701 00:17:46.839053  9899 layer_factory.hpp:77] Creating layer conv1a/bn
I0701 00:17:46.839076  9899 net.cpp:98] Creating Layer conv1a/bn
I0701 00:17:46.839083  9899 net.cpp:439] conv1a/bn <- conv1a
I0701 00:17:46.839095  9899 net.cpp:413] conv1a/bn -> conv1a/bn
I0701 00:17:46.841954  9899 net.cpp:148] Setting up conv1a/bn
I0701 00:17:46.842023  9899 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0701 00:17:46.842028  9899 net.cpp:163] Memory required for data: 170393600
I0701 00:17:46.842061  9899 layer_factory.hpp:77] Creating layer conv1a/relu
I0701 00:17:46.842097  9899 net.cpp:98] Creating Layer conv1a/relu
I0701 00:17:46.842110  9899 net.cpp:439] conv1a/relu <- conv1a/bn
I0701 00:17:46.842149  9899 net.cpp:400] conv1a/relu -> conv1a/bn (in-place)
I0701 00:17:46.842180  9899 net.cpp:148] Setting up conv1a/relu
I0701 00:17:46.842185  9899 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0701 00:17:46.842188  9899 net.cpp:163] Memory required for data: 222822400
I0701 00:17:46.842197  9899 layer_factory.hpp:77] Creating layer conv1b
I0701 00:17:46.842217  9899 net.cpp:98] Creating Layer conv1b
I0701 00:17:46.842226  9899 net.cpp:439] conv1b <- conv1a/bn
I0701 00:17:46.842254  9899 net.cpp:413] conv1b -> conv1b
I0701 00:17:46.842715  9899 net.cpp:148] Setting up conv1b
I0701 00:17:46.842726  9899 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0701 00:17:46.842730  9899 net.cpp:163] Memory required for data: 275251200
I0701 00:17:46.842737  9899 layer_factory.hpp:77] Creating layer conv1b/bn
I0701 00:17:46.842754  9899 net.cpp:98] Creating Layer conv1b/bn
I0701 00:17:46.842759  9899 net.cpp:439] conv1b/bn <- conv1b
I0701 00:17:46.842764  9899 net.cpp:413] conv1b/bn -> conv1b/bn
I0701 00:17:46.843300  9899 net.cpp:148] Setting up conv1b/bn
I0701 00:17:46.843308  9899 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0701 00:17:46.843312  9899 net.cpp:163] Memory required for data: 327680000
I0701 00:17:46.843327  9899 layer_factory.hpp:77] Creating layer conv1b/relu
I0701 00:17:46.843335  9899 net.cpp:98] Creating Layer conv1b/relu
I0701 00:17:46.843344  9899 net.cpp:439] conv1b/relu <- conv1b/bn
I0701 00:17:46.843350  9899 net.cpp:400] conv1b/relu -> conv1b/bn (in-place)
I0701 00:17:46.843361  9899 net.cpp:148] Setting up conv1b/relu
I0701 00:17:46.843366  9899 net.cpp:155] Top shape: 4 32 320 320 (13107200)
I0701 00:17:46.843369  9899 net.cpp:163] Memory required for data: 380108800
I0701 00:17:46.843376  9899 layer_factory.hpp:77] Creating layer pool1
I0701 00:17:46.843389  9899 net.cpp:98] Creating Layer pool1
I0701 00:17:46.843394  9899 net.cpp:439] pool1 <- conv1b/bn
I0701 00:17:46.843397  9899 net.cpp:413] pool1 -> pool1
I0701 00:17:46.843799  9899 net.cpp:148] Setting up pool1
I0701 00:17:46.843807  9899 net.cpp:155] Top shape: 4 32 160 160 (3276800)
I0701 00:17:46.843811  9899 net.cpp:163] Memory required for data: 393216000
I0701 00:17:46.843828  9899 layer_factory.hpp:77] Creating layer res2a_branch2a
I0701 00:17:46.843842  9899 net.cpp:98] Creating Layer res2a_branch2a
I0701 00:17:46.843845  9899 net.cpp:439] res2a_branch2a <- pool1
I0701 00:17:46.843859  9899 net.cpp:413] res2a_branch2a -> res2a_branch2a
I0701 00:17:46.845479  9899 net.cpp:148] Setting up res2a_branch2a
I0701 00:17:46.845494  9899 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0701 00:17:46.845497  9899 net.cpp:163] Memory required for data: 419430400
I0701 00:17:46.845508  9899 layer_factory.hpp:77] Creating layer res2a_branch2a/bn
I0701 00:17:46.845527  9899 net.cpp:98] Creating Layer res2a_branch2a/bn
I0701 00:17:46.845533  9899 net.cpp:439] res2a_branch2a/bn <- res2a_branch2a
I0701 00:17:46.845540  9899 net.cpp:413] res2a_branch2a/bn -> res2a_branch2a/bn
I0701 00:17:46.845897  9899 net.cpp:148] Setting up res2a_branch2a/bn
I0701 00:17:46.845906  9899 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0701 00:17:46.845908  9899 net.cpp:163] Memory required for data: 445644800
I0701 00:17:46.845924  9899 layer_factory.hpp:77] Creating layer res2a_branch2a/relu
I0701 00:17:46.845932  9899 net.cpp:98] Creating Layer res2a_branch2a/relu
I0701 00:17:46.845942  9899 net.cpp:439] res2a_branch2a/relu <- res2a_branch2a/bn
I0701 00:17:46.845947  9899 net.cpp:400] res2a_branch2a/relu -> res2a_branch2a/bn (in-place)
I0701 00:17:46.845959  9899 net.cpp:148] Setting up res2a_branch2a/relu
I0701 00:17:46.845969  9899 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0701 00:17:46.845973  9899 net.cpp:163] Memory required for data: 471859200
I0701 00:17:46.845978  9899 layer_factory.hpp:77] Creating layer res2a_branch2b
I0701 00:17:46.845990  9899 net.cpp:98] Creating Layer res2a_branch2b
I0701 00:17:46.845994  9899 net.cpp:439] res2a_branch2b <- res2a_branch2a/bn
I0701 00:17:46.846004  9899 net.cpp:413] res2a_branch2b -> res2a_branch2b
I0701 00:17:46.847256  9899 net.cpp:148] Setting up res2a_branch2b
I0701 00:17:46.847270  9899 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0701 00:17:46.847273  9899 net.cpp:163] Memory required for data: 498073600
I0701 00:17:46.847280  9899 layer_factory.hpp:77] Creating layer res2a_branch2b/bn
I0701 00:17:46.847295  9899 net.cpp:98] Creating Layer res2a_branch2b/bn
I0701 00:17:46.847317  9899 net.cpp:439] res2a_branch2b/bn <- res2a_branch2b
I0701 00:17:46.847342  9899 net.cpp:413] res2a_branch2b/bn -> res2a_branch2b/bn
I0701 00:17:46.847810  9899 net.cpp:148] Setting up res2a_branch2b/bn
I0701 00:17:46.847820  9899 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0701 00:17:46.847822  9899 net.cpp:163] Memory required for data: 524288000
I0701 00:17:46.847832  9899 layer_factory.hpp:77] Creating layer res2a_branch2b/relu
I0701 00:17:46.847846  9899 net.cpp:98] Creating Layer res2a_branch2b/relu
I0701 00:17:46.847854  9899 net.cpp:439] res2a_branch2b/relu <- res2a_branch2b/bn
I0701 00:17:46.847865  9899 net.cpp:400] res2a_branch2b/relu -> res2a_branch2b/bn (in-place)
I0701 00:17:46.847878  9899 net.cpp:148] Setting up res2a_branch2b/relu
I0701 00:17:46.847889  9899 net.cpp:155] Top shape: 4 64 160 160 (6553600)
I0701 00:17:46.847898  9899 net.cpp:163] Memory required for data: 550502400
I0701 00:17:46.847906  9899 layer_factory.hpp:77] Creating layer pool2
I0701 00:17:46.847921  9899 net.cpp:98] Creating Layer pool2
I0701 00:17:46.847931  9899 net.cpp:439] pool2 <- res2a_branch2b/bn
I0701 00:17:46.847942  9899 net.cpp:413] pool2 -> pool2
I0701 00:17:46.847986  9899 net.cpp:148] Setting up pool2
I0701 00:17:46.847998  9899 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0701 00:17:46.848007  9899 net.cpp:163] Memory required for data: 557056000
I0701 00:17:46.848016  9899 layer_factory.hpp:77] Creating layer res3a_branch2a
I0701 00:17:46.848028  9899 net.cpp:98] Creating Layer res3a_branch2a
I0701 00:17:46.848038  9899 net.cpp:439] res3a_branch2a <- pool2
I0701 00:17:46.848049  9899 net.cpp:413] res3a_branch2a -> res3a_branch2a
I0701 00:17:46.849958  9899 net.cpp:148] Setting up res3a_branch2a
I0701 00:17:46.849973  9899 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0701 00:17:46.849979  9899 net.cpp:163] Memory required for data: 570163200
I0701 00:17:46.849989  9899 layer_factory.hpp:77] Creating layer res3a_branch2a/bn
I0701 00:17:46.850002  9899 net.cpp:98] Creating Layer res3a_branch2a/bn
I0701 00:17:46.850008  9899 net.cpp:439] res3a_branch2a/bn <- res3a_branch2a
I0701 00:17:46.850018  9899 net.cpp:413] res3a_branch2a/bn -> res3a_branch2a/bn
I0701 00:17:46.850664  9899 net.cpp:148] Setting up res3a_branch2a/bn
I0701 00:17:46.850679  9899 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0701 00:17:46.850683  9899 net.cpp:163] Memory required for data: 583270400
I0701 00:17:46.850699  9899 layer_factory.hpp:77] Creating layer res3a_branch2a/relu
I0701 00:17:46.850708  9899 net.cpp:98] Creating Layer res3a_branch2a/relu
I0701 00:17:46.850713  9899 net.cpp:439] res3a_branch2a/relu <- res3a_branch2a/bn
I0701 00:17:46.850720  9899 net.cpp:400] res3a_branch2a/relu -> res3a_branch2a/bn (in-place)
I0701 00:17:46.850728  9899 net.cpp:148] Setting up res3a_branch2a/relu
I0701 00:17:46.850750  9899 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0701 00:17:46.850755  9899 net.cpp:163] Memory required for data: 596377600
I0701 00:17:46.850760  9899 layer_factory.hpp:77] Creating layer res3a_branch2b
I0701 00:17:46.850774  9899 net.cpp:98] Creating Layer res3a_branch2b
I0701 00:17:46.850780  9899 net.cpp:439] res3a_branch2b <- res3a_branch2a/bn
I0701 00:17:46.850791  9899 net.cpp:413] res3a_branch2b -> res3a_branch2b
I0701 00:17:46.852941  9899 net.cpp:148] Setting up res3a_branch2b
I0701 00:17:46.852995  9899 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0701 00:17:46.852999  9899 net.cpp:163] Memory required for data: 609484800
I0701 00:17:46.853006  9899 layer_factory.hpp:77] Creating layer res3a_branch2b/bn
I0701 00:17:46.853016  9899 net.cpp:98] Creating Layer res3a_branch2b/bn
I0701 00:17:46.853020  9899 net.cpp:439] res3a_branch2b/bn <- res3a_branch2b
I0701 00:17:46.853037  9899 net.cpp:413] res3a_branch2b/bn -> res3a_branch2b/bn
I0701 00:17:46.853368  9899 net.cpp:148] Setting up res3a_branch2b/bn
I0701 00:17:46.853374  9899 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0701 00:17:46.853379  9899 net.cpp:163] Memory required for data: 622592000
I0701 00:17:46.853387  9899 layer_factory.hpp:77] Creating layer res3a_branch2b/relu
I0701 00:17:46.853396  9899 net.cpp:98] Creating Layer res3a_branch2b/relu
I0701 00:17:46.853415  9899 net.cpp:439] res3a_branch2b/relu <- res3a_branch2b/bn
I0701 00:17:46.853420  9899 net.cpp:400] res3a_branch2b/relu -> res3a_branch2b/bn (in-place)
I0701 00:17:46.853426  9899 net.cpp:148] Setting up res3a_branch2b/relu
I0701 00:17:46.853430  9899 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0701 00:17:46.853433  9899 net.cpp:163] Memory required for data: 635699200
I0701 00:17:46.853438  9899 layer_factory.hpp:77] Creating layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0701 00:17:46.853446  9899 net.cpp:98] Creating Layer res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0701 00:17:46.853447  9899 net.cpp:439] res3a_branch2b/bn_res3a_branch2b/relu_0_split <- res3a_branch2b/bn
I0701 00:17:46.853451  9899 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0701 00:17:46.853457  9899 net.cpp:413] res3a_branch2b/bn_res3a_branch2b/relu_0_split -> res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0701 00:17:46.853485  9899 net.cpp:148] Setting up res3a_branch2b/bn_res3a_branch2b/relu_0_split
I0701 00:17:46.853489  9899 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0701 00:17:46.853494  9899 net.cpp:155] Top shape: 4 128 80 80 (3276800)
I0701 00:17:46.853497  9899 net.cpp:163] Memory required for data: 661913600
I0701 00:17:46.853502  9899 layer_factory.hpp:77] Creating layer pool3
I0701 00:17:46.853509  9899 net.cpp:98] Creating Layer pool3
I0701 00:17:46.853513  9899 net.cpp:439] pool3 <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_0
I0701 00:17:46.853518  9899 net.cpp:413] pool3 -> pool3
I0701 00:17:46.853539  9899 net.cpp:148] Setting up pool3
I0701 00:17:46.853544  9899 net.cpp:155] Top shape: 4 128 40 40 (819200)
I0701 00:17:46.853548  9899 net.cpp:163] Memory required for data: 665190400
I0701 00:17:46.853552  9899 layer_factory.hpp:77] Creating layer res4a_branch2a
I0701 00:17:46.853560  9899 net.cpp:98] Creating Layer res4a_branch2a
I0701 00:17:46.853564  9899 net.cpp:439] res4a_branch2a <- pool3
I0701 00:17:46.853569  9899 net.cpp:413] res4a_branch2a -> res4a_branch2a
I0701 00:17:46.861945  9899 net.cpp:148] Setting up res4a_branch2a
I0701 00:17:46.862040  9899 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0701 00:17:46.862062  9899 net.cpp:163] Memory required for data: 671744000
I0701 00:17:46.862085  9899 layer_factory.hpp:77] Creating layer res4a_branch2a/bn
I0701 00:17:46.862115  9899 net.cpp:98] Creating Layer res4a_branch2a/bn
I0701 00:17:46.862134  9899 net.cpp:439] res4a_branch2a/bn <- res4a_branch2a
I0701 00:17:46.862155  9899 net.cpp:413] res4a_branch2a/bn -> res4a_branch2a/bn
I0701 00:17:46.862702  9899 net.cpp:148] Setting up res4a_branch2a/bn
I0701 00:17:46.862736  9899 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0701 00:17:46.862751  9899 net.cpp:163] Memory required for data: 678297600
I0701 00:17:46.862771  9899 layer_factory.hpp:77] Creating layer res4a_branch2a/relu
I0701 00:17:46.862790  9899 net.cpp:98] Creating Layer res4a_branch2a/relu
I0701 00:17:46.862802  9899 net.cpp:439] res4a_branch2a/relu <- res4a_branch2a/bn
I0701 00:17:46.862817  9899 net.cpp:400] res4a_branch2a/relu -> res4a_branch2a/bn (in-place)
I0701 00:17:46.862833  9899 net.cpp:148] Setting up res4a_branch2a/relu
I0701 00:17:46.862848  9899 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0701 00:17:46.862859  9899 net.cpp:163] Memory required for data: 684851200
I0701 00:17:46.862870  9899 layer_factory.hpp:77] Creating layer res4a_branch2b
I0701 00:17:46.862890  9899 net.cpp:98] Creating Layer res4a_branch2b
I0701 00:17:46.862903  9899 net.cpp:439] res4a_branch2b <- res4a_branch2a/bn
I0701 00:17:46.862917  9899 net.cpp:413] res4a_branch2b -> res4a_branch2b
I0701 00:17:46.867533  9899 net.cpp:148] Setting up res4a_branch2b
I0701 00:17:46.867673  9899 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0701 00:17:46.867707  9899 net.cpp:163] Memory required for data: 691404800
I0701 00:17:46.867738  9899 layer_factory.hpp:77] Creating layer res4a_branch2b/bn
I0701 00:17:46.867789  9899 net.cpp:98] Creating Layer res4a_branch2b/bn
I0701 00:17:46.867836  9899 net.cpp:439] res4a_branch2b/bn <- res4a_branch2b
I0701 00:17:46.867866  9899 net.cpp:413] res4a_branch2b/bn -> res4a_branch2b/bn
I0701 00:17:46.868358  9899 net.cpp:148] Setting up res4a_branch2b/bn
I0701 00:17:46.868379  9899 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0701 00:17:46.868391  9899 net.cpp:163] Memory required for data: 697958400
I0701 00:17:46.868409  9899 layer_factory.hpp:77] Creating layer res4a_branch2b/relu
I0701 00:17:46.868427  9899 net.cpp:98] Creating Layer res4a_branch2b/relu
I0701 00:17:46.868436  9899 net.cpp:439] res4a_branch2b/relu <- res4a_branch2b/bn
I0701 00:17:46.868448  9899 net.cpp:400] res4a_branch2b/relu -> res4a_branch2b/bn (in-place)
I0701 00:17:46.868463  9899 net.cpp:148] Setting up res4a_branch2b/relu
I0701 00:17:46.868475  9899 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0701 00:17:46.868485  9899 net.cpp:163] Memory required for data: 704512000
I0701 00:17:46.868495  9899 layer_factory.hpp:77] Creating layer pool4
I0701 00:17:46.868512  9899 net.cpp:98] Creating Layer pool4
I0701 00:17:46.868522  9899 net.cpp:439] pool4 <- res4a_branch2b/bn
I0701 00:17:46.868535  9899 net.cpp:413] pool4 -> pool4
I0701 00:17:46.868574  9899 net.cpp:148] Setting up pool4
I0701 00:17:46.868588  9899 net.cpp:155] Top shape: 4 256 40 40 (1638400)
I0701 00:17:46.868598  9899 net.cpp:163] Memory required for data: 711065600
I0701 00:17:46.868609  9899 layer_factory.hpp:77] Creating layer res5a_branch2a
I0701 00:17:46.868636  9899 net.cpp:98] Creating Layer res5a_branch2a
I0701 00:17:46.868649  9899 net.cpp:439] res5a_branch2a <- pool4
I0701 00:17:46.868660  9899 net.cpp:413] res5a_branch2a -> res5a_branch2a
I0701 00:17:46.895473  9899 net.cpp:148] Setting up res5a_branch2a
I0701 00:17:46.895491  9899 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0701 00:17:46.895494  9899 net.cpp:163] Memory required for data: 724172800
I0701 00:17:46.895499  9899 layer_factory.hpp:77] Creating layer res5a_branch2a/bn
I0701 00:17:46.895508  9899 net.cpp:98] Creating Layer res5a_branch2a/bn
I0701 00:17:46.895510  9899 net.cpp:439] res5a_branch2a/bn <- res5a_branch2a
I0701 00:17:46.895515  9899 net.cpp:413] res5a_branch2a/bn -> res5a_branch2a/bn
I0701 00:17:46.895795  9899 net.cpp:148] Setting up res5a_branch2a/bn
I0701 00:17:46.895802  9899 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0701 00:17:46.895804  9899 net.cpp:163] Memory required for data: 737280000
I0701 00:17:46.895809  9899 layer_factory.hpp:77] Creating layer res5a_branch2a/relu
I0701 00:17:46.895813  9899 net.cpp:98] Creating Layer res5a_branch2a/relu
I0701 00:17:46.895815  9899 net.cpp:439] res5a_branch2a/relu <- res5a_branch2a/bn
I0701 00:17:46.895817  9899 net.cpp:400] res5a_branch2a/relu -> res5a_branch2a/bn (in-place)
I0701 00:17:46.895822  9899 net.cpp:148] Setting up res5a_branch2a/relu
I0701 00:17:46.895823  9899 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0701 00:17:46.895825  9899 net.cpp:163] Memory required for data: 750387200
I0701 00:17:46.895828  9899 layer_factory.hpp:77] Creating layer res5a_branch2b
I0701 00:17:46.895833  9899 net.cpp:98] Creating Layer res5a_branch2b
I0701 00:17:46.895835  9899 net.cpp:439] res5a_branch2b <- res5a_branch2a/bn
I0701 00:17:46.895838  9899 net.cpp:413] res5a_branch2b -> res5a_branch2b
I0701 00:17:46.908035  9899 net.cpp:148] Setting up res5a_branch2b
I0701 00:17:46.908052  9899 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0701 00:17:46.908054  9899 net.cpp:163] Memory required for data: 763494400
I0701 00:17:46.908063  9899 layer_factory.hpp:77] Creating layer res5a_branch2b/bn
I0701 00:17:46.908069  9899 net.cpp:98] Creating Layer res5a_branch2b/bn
I0701 00:17:46.908072  9899 net.cpp:439] res5a_branch2b/bn <- res5a_branch2b
I0701 00:17:46.908077  9899 net.cpp:413] res5a_branch2b/bn -> res5a_branch2b/bn
I0701 00:17:46.908362  9899 net.cpp:148] Setting up res5a_branch2b/bn
I0701 00:17:46.908367  9899 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0701 00:17:46.908370  9899 net.cpp:163] Memory required for data: 776601600
I0701 00:17:46.908375  9899 layer_factory.hpp:77] Creating layer res5a_branch2b/relu
I0701 00:17:46.908388  9899 net.cpp:98] Creating Layer res5a_branch2b/relu
I0701 00:17:46.908390  9899 net.cpp:439] res5a_branch2b/relu <- res5a_branch2b/bn
I0701 00:17:46.908393  9899 net.cpp:400] res5a_branch2b/relu -> res5a_branch2b/bn (in-place)
I0701 00:17:46.908397  9899 net.cpp:148] Setting up res5a_branch2b/relu
I0701 00:17:46.908399  9899 net.cpp:155] Top shape: 4 512 40 40 (3276800)
I0701 00:17:46.908401  9899 net.cpp:163] Memory required for data: 789708800
I0701 00:17:46.908403  9899 layer_factory.hpp:77] Creating layer out5a
I0701 00:17:46.908409  9899 net.cpp:98] Creating Layer out5a
I0701 00:17:46.908411  9899 net.cpp:439] out5a <- res5a_branch2b/bn
I0701 00:17:46.908414  9899 net.cpp:413] out5a -> out5a
I0701 00:17:46.912003  9899 net.cpp:148] Setting up out5a
I0701 00:17:46.912012  9899 net.cpp:155] Top shape: 4 64 40 40 (409600)
I0701 00:17:46.912014  9899 net.cpp:163] Memory required for data: 791347200
I0701 00:17:46.912019  9899 layer_factory.hpp:77] Creating layer out5a/bn
I0701 00:17:46.912024  9899 net.cpp:98] Creating Layer out5a/bn
I0701 00:17:46.912026  9899 net.cpp:439] out5a/bn <- out5a
I0701 00:17:46.912030  9899 net.cpp:413] out5a/bn -> out5a/bn
I0701 00:17:46.912330  9899 net.cpp:148] Setting up out5a/bn
I0701 00:17:46.912335  9899 net.cpp:155] Top shape: 4 64 40 40 (409600)
I0701 00:17:46.912338  9899 net.cpp:163] Memory required for data: 792985600
I0701 00:17:46.912343  9899 layer_factory.hpp:77] Creating layer out5a/relu
I0701 00:17:46.912345  9899 net.cpp:98] Creating Layer out5a/relu
I0701 00:17:46.912348  9899 net.cpp:439] out5a/relu <- out5a/bn
I0701 00:17:46.912350  9899 net.cpp:400] out5a/relu -> out5a/bn (in-place)
I0701 00:17:46.912353  9899 net.cpp:148] Setting up out5a/relu
I0701 00:17:46.912356  9899 net.cpp:155] Top shape: 4 64 40 40 (409600)
I0701 00:17:46.912358  9899 net.cpp:163] Memory required for data: 794624000
I0701 00:17:46.912359  9899 layer_factory.hpp:77] Creating layer out5a_up2
I0701 00:17:46.912367  9899 net.cpp:98] Creating Layer out5a_up2
I0701 00:17:46.912370  9899 net.cpp:439] out5a_up2 <- out5a/bn
I0701 00:17:46.912374  9899 net.cpp:413] out5a_up2 -> out5a_up2
I0701 00:17:46.912492  9899 net.cpp:148] Setting up out5a_up2
I0701 00:17:46.912497  9899 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0701 00:17:46.912498  9899 net.cpp:163] Memory required for data: 801177600
I0701 00:17:46.912502  9899 layer_factory.hpp:77] Creating layer out3a
I0701 00:17:46.912505  9899 net.cpp:98] Creating Layer out3a
I0701 00:17:46.912508  9899 net.cpp:439] out3a <- res3a_branch2b/bn_res3a_branch2b/relu_0_split_1
I0701 00:17:46.912511  9899 net.cpp:413] out3a -> out3a
I0701 00:17:46.913384  9899 net.cpp:148] Setting up out3a
I0701 00:17:46.913390  9899 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0701 00:17:46.913393  9899 net.cpp:163] Memory required for data: 807731200
I0701 00:17:46.913395  9899 layer_factory.hpp:77] Creating layer out3a/bn
I0701 00:17:46.913401  9899 net.cpp:98] Creating Layer out3a/bn
I0701 00:17:46.913404  9899 net.cpp:439] out3a/bn <- out3a
I0701 00:17:46.913408  9899 net.cpp:413] out3a/bn -> out3a/bn
I0701 00:17:46.913709  9899 net.cpp:148] Setting up out3a/bn
I0701 00:17:46.913714  9899 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0701 00:17:46.913717  9899 net.cpp:163] Memory required for data: 814284800
I0701 00:17:46.913722  9899 layer_factory.hpp:77] Creating layer out3a/relu
I0701 00:17:46.913727  9899 net.cpp:98] Creating Layer out3a/relu
I0701 00:17:46.913729  9899 net.cpp:439] out3a/relu <- out3a/bn
I0701 00:17:46.913731  9899 net.cpp:400] out3a/relu -> out3a/bn (in-place)
I0701 00:17:46.913735  9899 net.cpp:148] Setting up out3a/relu
I0701 00:17:46.913738  9899 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0701 00:17:46.913740  9899 net.cpp:163] Memory required for data: 820838400
I0701 00:17:46.913743  9899 layer_factory.hpp:77] Creating layer out3_out5_combined
I0701 00:17:46.913748  9899 net.cpp:98] Creating Layer out3_out5_combined
I0701 00:17:46.913750  9899 net.cpp:439] out3_out5_combined <- out5a_up2
I0701 00:17:46.913763  9899 net.cpp:439] out3_out5_combined <- out3a/bn
I0701 00:17:46.913767  9899 net.cpp:413] out3_out5_combined -> out3_out5_combined
I0701 00:17:46.913784  9899 net.cpp:148] Setting up out3_out5_combined
I0701 00:17:46.913789  9899 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0701 00:17:46.913791  9899 net.cpp:163] Memory required for data: 827392000
I0701 00:17:46.913794  9899 layer_factory.hpp:77] Creating layer ctx_conv1
I0701 00:17:46.913800  9899 net.cpp:98] Creating Layer ctx_conv1
I0701 00:17:46.913802  9899 net.cpp:439] ctx_conv1 <- out3_out5_combined
I0701 00:17:46.913806  9899 net.cpp:413] ctx_conv1 -> ctx_conv1
I0701 00:17:46.914702  9899 net.cpp:148] Setting up ctx_conv1
I0701 00:17:46.914710  9899 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0701 00:17:46.914712  9899 net.cpp:163] Memory required for data: 833945600
I0701 00:17:46.914716  9899 layer_factory.hpp:77] Creating layer ctx_conv1/bn
I0701 00:17:46.914719  9899 net.cpp:98] Creating Layer ctx_conv1/bn
I0701 00:17:46.914721  9899 net.cpp:439] ctx_conv1/bn <- ctx_conv1
I0701 00:17:46.914726  9899 net.cpp:413] ctx_conv1/bn -> ctx_conv1/bn
I0701 00:17:46.915021  9899 net.cpp:148] Setting up ctx_conv1/bn
I0701 00:17:46.915026  9899 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0701 00:17:46.915029  9899 net.cpp:163] Memory required for data: 840499200
I0701 00:17:46.915033  9899 layer_factory.hpp:77] Creating layer ctx_conv1/relu
I0701 00:17:46.915036  9899 net.cpp:98] Creating Layer ctx_conv1/relu
I0701 00:17:46.915038  9899 net.cpp:439] ctx_conv1/relu <- ctx_conv1/bn
I0701 00:17:46.915041  9899 net.cpp:400] ctx_conv1/relu -> ctx_conv1/bn (in-place)
I0701 00:17:46.915045  9899 net.cpp:148] Setting up ctx_conv1/relu
I0701 00:17:46.915046  9899 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0701 00:17:46.915048  9899 net.cpp:163] Memory required for data: 847052800
I0701 00:17:46.915050  9899 layer_factory.hpp:77] Creating layer ctx_conv2
I0701 00:17:46.915053  9899 net.cpp:98] Creating Layer ctx_conv2
I0701 00:17:46.915055  9899 net.cpp:439] ctx_conv2 <- ctx_conv1/bn
I0701 00:17:46.915057  9899 net.cpp:413] ctx_conv2 -> ctx_conv2
I0701 00:17:46.915908  9899 net.cpp:148] Setting up ctx_conv2
I0701 00:17:46.915913  9899 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0701 00:17:46.915915  9899 net.cpp:163] Memory required for data: 853606400
I0701 00:17:46.915918  9899 layer_factory.hpp:77] Creating layer ctx_conv2/bn
I0701 00:17:46.915921  9899 net.cpp:98] Creating Layer ctx_conv2/bn
I0701 00:17:46.915923  9899 net.cpp:439] ctx_conv2/bn <- ctx_conv2
I0701 00:17:46.915926  9899 net.cpp:413] ctx_conv2/bn -> ctx_conv2/bn
I0701 00:17:46.916218  9899 net.cpp:148] Setting up ctx_conv2/bn
I0701 00:17:46.916224  9899 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0701 00:17:46.916225  9899 net.cpp:163] Memory required for data: 860160000
I0701 00:17:46.916230  9899 layer_factory.hpp:77] Creating layer ctx_conv2/relu
I0701 00:17:46.916232  9899 net.cpp:98] Creating Layer ctx_conv2/relu
I0701 00:17:46.916234  9899 net.cpp:439] ctx_conv2/relu <- ctx_conv2/bn
I0701 00:17:46.916236  9899 net.cpp:400] ctx_conv2/relu -> ctx_conv2/bn (in-place)
I0701 00:17:46.916239  9899 net.cpp:148] Setting up ctx_conv2/relu
I0701 00:17:46.916242  9899 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0701 00:17:46.916244  9899 net.cpp:163] Memory required for data: 866713600
I0701 00:17:46.916245  9899 layer_factory.hpp:77] Creating layer ctx_conv3
I0701 00:17:46.916250  9899 net.cpp:98] Creating Layer ctx_conv3
I0701 00:17:46.916252  9899 net.cpp:439] ctx_conv3 <- ctx_conv2/bn
I0701 00:17:46.916255  9899 net.cpp:413] ctx_conv3 -> ctx_conv3
I0701 00:17:46.917116  9899 net.cpp:148] Setting up ctx_conv3
I0701 00:17:46.917121  9899 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0701 00:17:46.917124  9899 net.cpp:163] Memory required for data: 873267200
I0701 00:17:46.917126  9899 layer_factory.hpp:77] Creating layer ctx_conv3/bn
I0701 00:17:46.917130  9899 net.cpp:98] Creating Layer ctx_conv3/bn
I0701 00:17:46.917132  9899 net.cpp:439] ctx_conv3/bn <- ctx_conv3
I0701 00:17:46.917143  9899 net.cpp:413] ctx_conv3/bn -> ctx_conv3/bn
I0701 00:17:46.917438  9899 net.cpp:148] Setting up ctx_conv3/bn
I0701 00:17:46.917443  9899 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0701 00:17:46.917444  9899 net.cpp:163] Memory required for data: 879820800
I0701 00:17:46.917449  9899 layer_factory.hpp:77] Creating layer ctx_conv3/relu
I0701 00:17:46.917454  9899 net.cpp:98] Creating Layer ctx_conv3/relu
I0701 00:17:46.917457  9899 net.cpp:439] ctx_conv3/relu <- ctx_conv3/bn
I0701 00:17:46.917461  9899 net.cpp:400] ctx_conv3/relu -> ctx_conv3/bn (in-place)
I0701 00:17:46.917476  9899 net.cpp:148] Setting up ctx_conv3/relu
I0701 00:17:46.917482  9899 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0701 00:17:46.917485  9899 net.cpp:163] Memory required for data: 886374400
I0701 00:17:46.917488  9899 layer_factory.hpp:77] Creating layer ctx_conv4
I0701 00:17:46.917496  9899 net.cpp:98] Creating Layer ctx_conv4
I0701 00:17:46.917500  9899 net.cpp:439] ctx_conv4 <- ctx_conv3/bn
I0701 00:17:46.917505  9899 net.cpp:413] ctx_conv4 -> ctx_conv4
I0701 00:17:46.918362  9899 net.cpp:148] Setting up ctx_conv4
I0701 00:17:46.918368  9899 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0701 00:17:46.918371  9899 net.cpp:163] Memory required for data: 892928000
I0701 00:17:46.918376  9899 layer_factory.hpp:77] Creating layer ctx_conv4/bn
I0701 00:17:46.918382  9899 net.cpp:98] Creating Layer ctx_conv4/bn
I0701 00:17:46.918391  9899 net.cpp:439] ctx_conv4/bn <- ctx_conv4
I0701 00:17:46.918396  9899 net.cpp:413] ctx_conv4/bn -> ctx_conv4/bn
I0701 00:17:46.918701  9899 net.cpp:148] Setting up ctx_conv4/bn
I0701 00:17:46.918707  9899 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0701 00:17:46.918712  9899 net.cpp:163] Memory required for data: 899481600
I0701 00:17:46.918720  9899 layer_factory.hpp:77] Creating layer ctx_conv4/relu
I0701 00:17:46.918725  9899 net.cpp:98] Creating Layer ctx_conv4/relu
I0701 00:17:46.918728  9899 net.cpp:439] ctx_conv4/relu <- ctx_conv4/bn
I0701 00:17:46.918735  9899 net.cpp:400] ctx_conv4/relu -> ctx_conv4/bn (in-place)
I0701 00:17:46.918740  9899 net.cpp:148] Setting up ctx_conv4/relu
I0701 00:17:46.918745  9899 net.cpp:155] Top shape: 4 64 80 80 (1638400)
I0701 00:17:46.918747  9899 net.cpp:163] Memory required for data: 906035200
I0701 00:17:46.918751  9899 layer_factory.hpp:77] Creating layer ctx_final
I0701 00:17:46.918758  9899 net.cpp:98] Creating Layer ctx_final
I0701 00:17:46.918761  9899 net.cpp:439] ctx_final <- ctx_conv4/bn
I0701 00:17:46.918768  9899 net.cpp:413] ctx_final -> ctx_final
I0701 00:17:46.919133  9899 net.cpp:148] Setting up ctx_final
I0701 00:17:46.919139  9899 net.cpp:155] Top shape: 4 20 80 80 (512000)
I0701 00:17:46.919143  9899 net.cpp:163] Memory required for data: 908083200
I0701 00:17:46.919149  9899 layer_factory.hpp:77] Creating layer ctx_final/relu
I0701 00:17:46.919154  9899 net.cpp:98] Creating Layer ctx_final/relu
I0701 00:17:46.919158  9899 net.cpp:439] ctx_final/relu <- ctx_final
I0701 00:17:46.919162  9899 net.cpp:400] ctx_final/relu -> ctx_final (in-place)
I0701 00:17:46.919168  9899 net.cpp:148] Setting up ctx_final/relu
I0701 00:17:46.919173  9899 net.cpp:155] Top shape: 4 20 80 80 (512000)
I0701 00:17:46.919176  9899 net.cpp:163] Memory required for data: 910131200
I0701 00:17:46.919179  9899 layer_factory.hpp:77] Creating layer out_deconv_final_up2
I0701 00:17:46.919186  9899 net.cpp:98] Creating Layer out_deconv_final_up2
I0701 00:17:46.919190  9899 net.cpp:439] out_deconv_final_up2 <- ctx_final
I0701 00:17:46.919195  9899 net.cpp:413] out_deconv_final_up2 -> out_deconv_final_up2
I0701 00:17:46.919301  9899 net.cpp:148] Setting up out_deconv_final_up2
I0701 00:17:46.919306  9899 net.cpp:155] Top shape: 4 20 160 160 (2048000)
I0701 00:17:46.919311  9899 net.cpp:163] Memory required for data: 918323200
I0701 00:17:46.919315  9899 layer_factory.hpp:77] Creating layer out_deconv_final_up4
I0701 00:17:46.919322  9899 net.cpp:98] Creating Layer out_deconv_final_up4
I0701 00:17:46.919325  9899 net.cpp:439] out_deconv_final_up4 <- out_deconv_final_up2
I0701 00:17:46.919337  9899 net.cpp:413] out_deconv_final_up4 -> out_deconv_final_up4
I0701 00:17:46.919448  9899 net.cpp:148] Setting up out_deconv_final_up4
I0701 00:17:46.919454  9899 net.cpp:155] Top shape: 4 20 320 320 (8192000)
I0701 00:17:46.919458  9899 net.cpp:163] Memory required for data: 951091200
I0701 00:17:46.919463  9899 layer_factory.hpp:77] Creating layer out_deconv_final_up8
I0701 00:17:46.919469  9899 net.cpp:98] Creating Layer out_deconv_final_up8
I0701 00:17:46.919473  9899 net.cpp:439] out_deconv_final_up8 <- out_deconv_final_up4
I0701 00:17:46.919478  9899 net.cpp:413] out_deconv_final_up8 -> out_deconv_final_up8
I0701 00:17:46.919584  9899 net.cpp:148] Setting up out_deconv_final_up8
I0701 00:17:46.919589  9899 net.cpp:155] Top shape: 4 20 640 640 (32768000)
I0701 00:17:46.919594  9899 net.cpp:163] Memory required for data: 1082163200
I0701 00:17:46.919598  9899 layer_factory.hpp:77] Creating layer out_deconv_final_up8_out_deconv_final_up8_0_split
I0701 00:17:46.919605  9899 net.cpp:98] Creating Layer out_deconv_final_up8_out_deconv_final_up8_0_split
I0701 00:17:46.919608  9899 net.cpp:439] out_deconv_final_up8_out_deconv_final_up8_0_split <- out_deconv_final_up8
I0701 00:17:46.919612  9899 net.cpp:413] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0701 00:17:46.919618  9899 net.cpp:413] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0701 00:17:46.919625  9899 net.cpp:413] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0701 00:17:46.919652  9899 net.cpp:148] Setting up out_deconv_final_up8_out_deconv_final_up8_0_split
I0701 00:17:46.919656  9899 net.cpp:155] Top shape: 4 20 640 640 (32768000)
I0701 00:17:46.919661  9899 net.cpp:155] Top shape: 4 20 640 640 (32768000)
I0701 00:17:46.919665  9899 net.cpp:155] Top shape: 4 20 640 640 (32768000)
I0701 00:17:46.919669  9899 net.cpp:163] Memory required for data: 1475379200
I0701 00:17:46.919673  9899 layer_factory.hpp:77] Creating layer loss
I0701 00:17:46.919687  9899 net.cpp:98] Creating Layer loss
I0701 00:17:46.919689  9899 net.cpp:439] loss <- out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0701 00:17:46.919694  9899 net.cpp:439] loss <- label_data_1_split_0
I0701 00:17:46.919699  9899 net.cpp:413] loss -> loss
I0701 00:17:46.919711  9899 layer_factory.hpp:77] Creating layer loss
I0701 00:17:46.957788  9899 net.cpp:148] Setting up loss
I0701 00:17:46.957809  9899 net.cpp:155] Top shape: (1)
I0701 00:17:46.957813  9899 net.cpp:158]     with loss weight 1
I0701 00:17:46.957828  9899 net.cpp:163] Memory required for data: 1475379204
I0701 00:17:46.957834  9899 layer_factory.hpp:77] Creating layer accuracy/top1
I0701 00:17:46.957844  9899 net.cpp:98] Creating Layer accuracy/top1
I0701 00:17:46.957849  9899 net.cpp:439] accuracy/top1 <- out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0701 00:17:46.957855  9899 net.cpp:439] accuracy/top1 <- label_data_1_split_1
I0701 00:17:46.957861  9899 net.cpp:413] accuracy/top1 -> accuracy/top1
I0701 00:17:46.957877  9899 net.cpp:148] Setting up accuracy/top1
I0701 00:17:46.957881  9899 net.cpp:155] Top shape: (1)
I0701 00:17:46.957885  9899 net.cpp:163] Memory required for data: 1475379208
I0701 00:17:46.957888  9899 layer_factory.hpp:77] Creating layer accuracy/top5
I0701 00:17:46.957893  9899 net.cpp:98] Creating Layer accuracy/top5
I0701 00:17:46.957897  9899 net.cpp:439] accuracy/top5 <- out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0701 00:17:46.957902  9899 net.cpp:439] accuracy/top5 <- label_data_1_split_2
I0701 00:17:46.957907  9899 net.cpp:413] accuracy/top5 -> accuracy/top5
I0701 00:17:46.957913  9899 net.cpp:148] Setting up accuracy/top5
I0701 00:17:46.957917  9899 net.cpp:155] Top shape: (1)
I0701 00:17:46.957921  9899 net.cpp:163] Memory required for data: 1475379212
I0701 00:17:46.957924  9899 net.cpp:226] accuracy/top5 does not need backward computation.
I0701 00:17:46.957929  9899 net.cpp:226] accuracy/top1 does not need backward computation.
I0701 00:17:46.957942  9899 net.cpp:224] loss needs backward computation.
I0701 00:17:46.957947  9899 net.cpp:224] out_deconv_final_up8_out_deconv_final_up8_0_split needs backward computation.
I0701 00:17:46.957950  9899 net.cpp:224] out_deconv_final_up8 needs backward computation.
I0701 00:17:46.957954  9899 net.cpp:224] out_deconv_final_up4 needs backward computation.
I0701 00:17:46.957958  9899 net.cpp:224] out_deconv_final_up2 needs backward computation.
I0701 00:17:46.957962  9899 net.cpp:224] ctx_final/relu needs backward computation.
I0701 00:17:46.957967  9899 net.cpp:224] ctx_final needs backward computation.
I0701 00:17:46.957970  9899 net.cpp:224] ctx_conv4/relu needs backward computation.
I0701 00:17:46.957973  9899 net.cpp:224] ctx_conv4/bn needs backward computation.
I0701 00:17:46.957978  9899 net.cpp:224] ctx_conv4 needs backward computation.
I0701 00:17:46.957981  9899 net.cpp:224] ctx_conv3/relu needs backward computation.
I0701 00:17:46.957985  9899 net.cpp:224] ctx_conv3/bn needs backward computation.
I0701 00:17:46.957989  9899 net.cpp:224] ctx_conv3 needs backward computation.
I0701 00:17:46.957993  9899 net.cpp:224] ctx_conv2/relu needs backward computation.
I0701 00:17:46.957998  9899 net.cpp:224] ctx_conv2/bn needs backward computation.
I0701 00:17:46.958001  9899 net.cpp:224] ctx_conv2 needs backward computation.
I0701 00:17:46.958004  9899 net.cpp:224] ctx_conv1/relu needs backward computation.
I0701 00:17:46.958009  9899 net.cpp:224] ctx_conv1/bn needs backward computation.
I0701 00:17:46.958012  9899 net.cpp:224] ctx_conv1 needs backward computation.
I0701 00:17:46.958016  9899 net.cpp:224] out3_out5_combined needs backward computation.
I0701 00:17:46.958021  9899 net.cpp:224] out3a/relu needs backward computation.
I0701 00:17:46.958025  9899 net.cpp:224] out3a/bn needs backward computation.
I0701 00:17:46.958029  9899 net.cpp:224] out3a needs backward computation.
I0701 00:17:46.958034  9899 net.cpp:224] out5a_up2 needs backward computation.
I0701 00:17:46.958037  9899 net.cpp:224] out5a/relu needs backward computation.
I0701 00:17:46.958041  9899 net.cpp:224] out5a/bn needs backward computation.
I0701 00:17:46.958045  9899 net.cpp:224] out5a needs backward computation.
I0701 00:17:46.958050  9899 net.cpp:224] res5a_branch2b/relu needs backward computation.
I0701 00:17:46.958053  9899 net.cpp:224] res5a_branch2b/bn needs backward computation.
I0701 00:17:46.958057  9899 net.cpp:224] res5a_branch2b needs backward computation.
I0701 00:17:46.958061  9899 net.cpp:224] res5a_branch2a/relu needs backward computation.
I0701 00:17:46.958065  9899 net.cpp:224] res5a_branch2a/bn needs backward computation.
I0701 00:17:46.958068  9899 net.cpp:224] res5a_branch2a needs backward computation.
I0701 00:17:46.958072  9899 net.cpp:224] pool4 needs backward computation.
I0701 00:17:46.958077  9899 net.cpp:224] res4a_branch2b/relu needs backward computation.
I0701 00:17:46.958081  9899 net.cpp:224] res4a_branch2b/bn needs backward computation.
I0701 00:17:46.958086  9899 net.cpp:224] res4a_branch2b needs backward computation.
I0701 00:17:46.958089  9899 net.cpp:224] res4a_branch2a/relu needs backward computation.
I0701 00:17:46.958092  9899 net.cpp:224] res4a_branch2a/bn needs backward computation.
I0701 00:17:46.958096  9899 net.cpp:224] res4a_branch2a needs backward computation.
I0701 00:17:46.958101  9899 net.cpp:224] pool3 needs backward computation.
I0701 00:17:46.958106  9899 net.cpp:224] res3a_branch2b/bn_res3a_branch2b/relu_0_split needs backward computation.
I0701 00:17:46.958109  9899 net.cpp:224] res3a_branch2b/relu needs backward computation.
I0701 00:17:46.958112  9899 net.cpp:224] res3a_branch2b/bn needs backward computation.
I0701 00:17:46.958117  9899 net.cpp:224] res3a_branch2b needs backward computation.
I0701 00:17:46.958120  9899 net.cpp:224] res3a_branch2a/relu needs backward computation.
I0701 00:17:46.958124  9899 net.cpp:224] res3a_branch2a/bn needs backward computation.
I0701 00:17:46.958128  9899 net.cpp:224] res3a_branch2a needs backward computation.
I0701 00:17:46.958135  9899 net.cpp:224] pool2 needs backward computation.
I0701 00:17:46.958139  9899 net.cpp:224] res2a_branch2b/relu needs backward computation.
I0701 00:17:46.958144  9899 net.cpp:224] res2a_branch2b/bn needs backward computation.
I0701 00:17:46.958148  9899 net.cpp:224] res2a_branch2b needs backward computation.
I0701 00:17:46.958151  9899 net.cpp:224] res2a_branch2a/relu needs backward computation.
I0701 00:17:46.958155  9899 net.cpp:224] res2a_branch2a/bn needs backward computation.
I0701 00:17:46.958159  9899 net.cpp:224] res2a_branch2a needs backward computation.
I0701 00:17:46.958163  9899 net.cpp:224] pool1 needs backward computation.
I0701 00:17:46.958168  9899 net.cpp:224] conv1b/relu needs backward computation.
I0701 00:17:46.958171  9899 net.cpp:224] conv1b/bn needs backward computation.
I0701 00:17:46.958175  9899 net.cpp:224] conv1b needs backward computation.
I0701 00:17:46.958179  9899 net.cpp:224] conv1a/relu needs backward computation.
I0701 00:17:46.958184  9899 net.cpp:224] conv1a/bn needs backward computation.
I0701 00:17:46.958186  9899 net.cpp:224] conv1a needs backward computation.
I0701 00:17:46.958191  9899 net.cpp:226] data/bias does not need backward computation.
I0701 00:17:46.958196  9899 net.cpp:226] label_data_1_split does not need backward computation.
I0701 00:17:46.958200  9899 net.cpp:226] data does not need backward computation.
I0701 00:17:46.958204  9899 net.cpp:268] This network produces output accuracy/top1
I0701 00:17:46.958207  9899 net.cpp:268] This network produces output accuracy/top5
I0701 00:17:46.958211  9899 net.cpp:268] This network produces output loss
I0701 00:17:46.958242  9899 net.cpp:288] Network initialization done.
I0701 00:17:46.969144  9899 caffe.cpp:289] Running for 50 iterations.
I0701 00:17:47.782443  9899 caffe.cpp:312] Batch 0, accuracy/top1 = 0.918487
I0701 00:17:47.782471  9899 caffe.cpp:312] Batch 0, accuracy/top5 = 0.991621
I0701 00:17:47.782476  9899 caffe.cpp:312] Batch 0, loss = 0.161182
I0701 00:17:48.541229  9899 caffe.cpp:312] Batch 1, accuracy/top1 = 0.8981
I0701 00:17:48.541256  9899 caffe.cpp:312] Batch 1, accuracy/top5 = 0.990504
I0701 00:17:48.541260  9899 caffe.cpp:312] Batch 1, loss = 0.271098
I0701 00:17:49.321869  9899 caffe.cpp:312] Batch 2, accuracy/top1 = 0.883237
I0701 00:17:49.321897  9899 caffe.cpp:312] Batch 2, accuracy/top5 = 0.992708
I0701 00:17:49.321902  9899 caffe.cpp:312] Batch 2, loss = 0.203529
I0701 00:17:50.061295  9899 caffe.cpp:312] Batch 3, accuracy/top1 = 0.916812
I0701 00:17:50.061321  9899 caffe.cpp:312] Batch 3, accuracy/top5 = 0.992509
I0701 00:17:50.061326  9899 caffe.cpp:312] Batch 3, loss = 0.159322
I0701 00:17:50.811486  9899 caffe.cpp:312] Batch 4, accuracy/top1 = 0.911028
I0701 00:17:50.811513  9899 caffe.cpp:312] Batch 4, accuracy/top5 = 0.99175
I0701 00:17:50.811519  9899 caffe.cpp:312] Batch 4, loss = 0.220506
I0701 00:17:51.396680  9899 caffe.cpp:312] Batch 5, accuracy/top1 = 0.923971
I0701 00:17:51.396706  9899 caffe.cpp:312] Batch 5, accuracy/top5 = 0.99707
I0701 00:17:51.396710  9899 caffe.cpp:312] Batch 5, loss = 0.193563
I0701 00:17:52.165575  9899 caffe.cpp:312] Batch 6, accuracy/top1 = 0.926876
I0701 00:17:52.165596  9899 caffe.cpp:312] Batch 6, accuracy/top5 = 0.996671
I0701 00:17:52.165599  9899 caffe.cpp:312] Batch 6, loss = 0.130481
I0701 00:17:52.929172  9899 caffe.cpp:312] Batch 7, accuracy/top1 = 0.931263
I0701 00:17:52.929198  9899 caffe.cpp:312] Batch 7, accuracy/top5 = 0.997341
I0701 00:17:52.929201  9899 caffe.cpp:312] Batch 7, loss = 0.145955
I0701 00:17:53.694983  9899 caffe.cpp:312] Batch 8, accuracy/top1 = 0.943895
I0701 00:17:53.695008  9899 caffe.cpp:312] Batch 8, accuracy/top5 = 0.996064
I0701 00:17:53.695011  9899 caffe.cpp:312] Batch 8, loss = 0.101973
I0701 00:17:54.457870  9899 caffe.cpp:312] Batch 9, accuracy/top1 = 0.95246
I0701 00:17:54.457897  9899 caffe.cpp:312] Batch 9, accuracy/top5 = 0.997475
I0701 00:17:54.457902  9899 caffe.cpp:312] Batch 9, loss = 0.0891257
I0701 00:17:55.234784  9899 caffe.cpp:312] Batch 10, accuracy/top1 = 0.945588
I0701 00:17:55.234827  9899 caffe.cpp:312] Batch 10, accuracy/top5 = 0.994037
I0701 00:17:55.234832  9899 caffe.cpp:312] Batch 10, loss = 0.110514
I0701 00:17:56.005313  9899 caffe.cpp:312] Batch 11, accuracy/top1 = 0.941774
I0701 00:17:56.005338  9899 caffe.cpp:312] Batch 11, accuracy/top5 = 0.993535
I0701 00:17:56.005342  9899 caffe.cpp:312] Batch 11, loss = 0.118555
I0701 00:17:56.754138  9899 caffe.cpp:312] Batch 12, accuracy/top1 = 0.919249
I0701 00:17:56.754164  9899 caffe.cpp:312] Batch 12, accuracy/top5 = 0.991955
I0701 00:17:56.754168  9899 caffe.cpp:312] Batch 12, loss = 0.168413
I0701 00:17:57.514235  9899 caffe.cpp:312] Batch 13, accuracy/top1 = 0.945576
I0701 00:17:57.514256  9899 caffe.cpp:312] Batch 13, accuracy/top5 = 0.998279
I0701 00:17:57.514259  9899 caffe.cpp:312] Batch 13, loss = 0.092224
I0701 00:17:58.287880  9899 caffe.cpp:312] Batch 14, accuracy/top1 = 0.953765
I0701 00:17:58.287902  9899 caffe.cpp:312] Batch 14, accuracy/top5 = 0.997621
I0701 00:17:58.287906  9899 caffe.cpp:312] Batch 14, loss = 0.077303
I0701 00:17:59.049310  9899 caffe.cpp:312] Batch 15, accuracy/top1 = 0.912928
I0701 00:17:59.049334  9899 caffe.cpp:312] Batch 15, accuracy/top5 = 0.990188
I0701 00:17:59.049337  9899 caffe.cpp:312] Batch 15, loss = 0.185553
I0701 00:17:59.732883  9899 caffe.cpp:312] Batch 16, accuracy/top1 = 0.937204
I0701 00:17:59.732905  9899 caffe.cpp:312] Batch 16, accuracy/top5 = 0.994914
I0701 00:17:59.732909  9899 caffe.cpp:312] Batch 16, loss = 0.131999
I0701 00:18:00.376860  9899 caffe.cpp:312] Batch 17, accuracy/top1 = 0.943746
I0701 00:18:00.376885  9899 caffe.cpp:312] Batch 17, accuracy/top5 = 0.990367
I0701 00:18:00.376889  9899 caffe.cpp:312] Batch 17, loss = 0.125548
I0701 00:18:01.151095  9899 caffe.cpp:312] Batch 18, accuracy/top1 = 0.933723
I0701 00:18:01.151119  9899 caffe.cpp:312] Batch 18, accuracy/top5 = 0.996805
I0701 00:18:01.151124  9899 caffe.cpp:312] Batch 18, loss = 0.122954
I0701 00:18:01.921828  9899 caffe.cpp:312] Batch 19, accuracy/top1 = 0.955824
I0701 00:18:01.921854  9899 caffe.cpp:312] Batch 19, accuracy/top5 = 0.999114
I0701 00:18:01.921859  9899 caffe.cpp:312] Batch 19, loss = 0.0698131
I0701 00:18:02.697042  9899 caffe.cpp:312] Batch 20, accuracy/top1 = 0.913164
I0701 00:18:02.697067  9899 caffe.cpp:312] Batch 20, accuracy/top5 = 0.991645
I0701 00:18:02.697072  9899 caffe.cpp:312] Batch 20, loss = 0.172564
I0701 00:18:03.348562  9899 caffe.cpp:312] Batch 21, accuracy/top1 = 0.952254
I0701 00:18:03.348588  9899 caffe.cpp:312] Batch 21, accuracy/top5 = 0.992047
I0701 00:18:03.348593  9899 caffe.cpp:312] Batch 21, loss = 0.110627
I0701 00:18:04.123811  9899 caffe.cpp:312] Batch 22, accuracy/top1 = 0.942945
I0701 00:18:04.123834  9899 caffe.cpp:312] Batch 22, accuracy/top5 = 0.995784
I0701 00:18:04.123838  9899 caffe.cpp:312] Batch 22, loss = 0.113229
I0701 00:18:04.883800  9899 caffe.cpp:312] Batch 23, accuracy/top1 = 0.933434
I0701 00:18:04.883826  9899 caffe.cpp:312] Batch 23, accuracy/top5 = 0.995057
I0701 00:18:04.883831  9899 caffe.cpp:312] Batch 23, loss = 0.13276
I0701 00:18:05.639920  9899 caffe.cpp:312] Batch 24, accuracy/top1 = 0.919657
I0701 00:18:05.639943  9899 caffe.cpp:312] Batch 24, accuracy/top5 = 0.992948
I0701 00:18:05.639947  9899 caffe.cpp:312] Batch 24, loss = 0.166093
I0701 00:18:06.402531  9899 caffe.cpp:312] Batch 25, accuracy/top1 = 0.949731
I0701 00:18:06.402552  9899 caffe.cpp:312] Batch 25, accuracy/top5 = 0.994343
I0701 00:18:06.402556  9899 caffe.cpp:312] Batch 25, loss = 0.111027
I0701 00:18:07.155968  9899 caffe.cpp:312] Batch 26, accuracy/top1 = 0.896999
I0701 00:18:07.155993  9899 caffe.cpp:312] Batch 26, accuracy/top5 = 0.996817
I0701 00:18:07.155998  9899 caffe.cpp:312] Batch 26, loss = 0.199358
I0701 00:18:07.909714  9899 caffe.cpp:312] Batch 27, accuracy/top1 = 0.917776
I0701 00:18:07.909739  9899 caffe.cpp:312] Batch 27, accuracy/top5 = 0.987426
I0701 00:18:07.909744  9899 caffe.cpp:312] Batch 27, loss = 0.181024
I0701 00:18:08.679848  9899 caffe.cpp:312] Batch 28, accuracy/top1 = 0.897638
I0701 00:18:08.679872  9899 caffe.cpp:312] Batch 28, accuracy/top5 = 0.992359
I0701 00:18:08.679893  9899 caffe.cpp:312] Batch 28, loss = 0.232927
I0701 00:18:09.444214  9899 caffe.cpp:312] Batch 29, accuracy/top1 = 0.922988
I0701 00:18:09.444242  9899 caffe.cpp:312] Batch 29, accuracy/top5 = 0.985846
I0701 00:18:09.444245  9899 caffe.cpp:312] Batch 29, loss = 0.209912
I0701 00:18:10.130787  9899 caffe.cpp:312] Batch 30, accuracy/top1 = 0.93561
I0701 00:18:10.130812  9899 caffe.cpp:312] Batch 30, accuracy/top5 = 0.996984
I0701 00:18:10.130816  9899 caffe.cpp:312] Batch 30, loss = 0.118794
I0701 00:18:10.872257  9899 caffe.cpp:312] Batch 31, accuracy/top1 = 0.927028
I0701 00:18:10.872283  9899 caffe.cpp:312] Batch 31, accuracy/top5 = 0.991582
I0701 00:18:10.872288  9899 caffe.cpp:312] Batch 31, loss = 0.16961
I0701 00:18:11.628871  9899 caffe.cpp:312] Batch 32, accuracy/top1 = 0.926818
I0701 00:18:11.628893  9899 caffe.cpp:312] Batch 32, accuracy/top5 = 0.995769
I0701 00:18:11.628897  9899 caffe.cpp:312] Batch 32, loss = 0.129326
I0701 00:18:12.395488  9899 caffe.cpp:312] Batch 33, accuracy/top1 = 0.940952
I0701 00:18:12.395509  9899 caffe.cpp:312] Batch 33, accuracy/top5 = 0.995814
I0701 00:18:12.395512  9899 caffe.cpp:312] Batch 33, loss = 0.111134
I0701 00:18:13.169623  9899 caffe.cpp:312] Batch 34, accuracy/top1 = 0.939445
I0701 00:18:13.169649  9899 caffe.cpp:312] Batch 34, accuracy/top5 = 0.993273
I0701 00:18:13.169653  9899 caffe.cpp:312] Batch 34, loss = 0.144633
I0701 00:18:13.942783  9899 caffe.cpp:312] Batch 35, accuracy/top1 = 0.930186
I0701 00:18:13.942811  9899 caffe.cpp:312] Batch 35, accuracy/top5 = 0.996247
I0701 00:18:13.942814  9899 caffe.cpp:312] Batch 35, loss = 0.130125
I0701 00:18:14.679515  9899 caffe.cpp:312] Batch 36, accuracy/top1 = 0.904185
I0701 00:18:14.679540  9899 caffe.cpp:312] Batch 36, accuracy/top5 = 0.98706
I0701 00:18:14.679544  9899 caffe.cpp:312] Batch 36, loss = 0.258469
I0701 00:18:15.456668  9899 caffe.cpp:312] Batch 37, accuracy/top1 = 0.904443
I0701 00:18:15.456727  9899 caffe.cpp:312] Batch 37, accuracy/top5 = 0.993036
I0701 00:18:15.456732  9899 caffe.cpp:312] Batch 37, loss = 0.188421
I0701 00:18:16.219326  9899 caffe.cpp:312] Batch 38, accuracy/top1 = 0.918621
I0701 00:18:16.219352  9899 caffe.cpp:312] Batch 38, accuracy/top5 = 0.990972
I0701 00:18:16.219355  9899 caffe.cpp:312] Batch 38, loss = 0.198106
I0701 00:18:16.965270  9899 caffe.cpp:312] Batch 39, accuracy/top1 = 0.907998
I0701 00:18:16.965296  9899 caffe.cpp:312] Batch 39, accuracy/top5 = 0.991862
I0701 00:18:16.965299  9899 caffe.cpp:312] Batch 39, loss = 0.180723
I0701 00:18:17.728529  9899 caffe.cpp:312] Batch 40, accuracy/top1 = 0.927043
I0701 00:18:17.728555  9899 caffe.cpp:312] Batch 40, accuracy/top5 = 0.992216
I0701 00:18:17.728559  9899 caffe.cpp:312] Batch 40, loss = 0.152103
I0701 00:18:18.490648  9899 caffe.cpp:312] Batch 41, accuracy/top1 = 0.921441
I0701 00:18:18.490674  9899 caffe.cpp:312] Batch 41, accuracy/top5 = 0.995943
I0701 00:18:18.490679  9899 caffe.cpp:312] Batch 41, loss = 0.128036
I0701 00:18:19.272161  9899 caffe.cpp:312] Batch 42, accuracy/top1 = 0.93291
I0701 00:18:19.272188  9899 caffe.cpp:312] Batch 42, accuracy/top5 = 0.996992
I0701 00:18:19.272192  9899 caffe.cpp:312] Batch 42, loss = 0.119998
I0701 00:18:20.036847  9899 caffe.cpp:312] Batch 43, accuracy/top1 = 0.950054
I0701 00:18:20.036870  9899 caffe.cpp:312] Batch 43, accuracy/top5 = 0.997901
I0701 00:18:20.036875  9899 caffe.cpp:312] Batch 43, loss = 0.0774627
I0701 00:18:20.800648  9899 caffe.cpp:312] Batch 44, accuracy/top1 = 0.920683
I0701 00:18:20.800674  9899 caffe.cpp:312] Batch 44, accuracy/top5 = 0.986313
I0701 00:18:20.800678  9899 caffe.cpp:312] Batch 44, loss = 0.194521
I0701 00:18:21.554644  9899 caffe.cpp:312] Batch 45, accuracy/top1 = 0.935109
I0701 00:18:21.554669  9899 caffe.cpp:312] Batch 45, accuracy/top5 = 0.990977
I0701 00:18:21.554673  9899 caffe.cpp:312] Batch 45, loss = 0.149178
I0701 00:18:22.325197  9899 caffe.cpp:312] Batch 46, accuracy/top1 = 0.928223
I0701 00:18:22.325222  9899 caffe.cpp:312] Batch 46, accuracy/top5 = 0.996321
I0701 00:18:22.325227  9899 caffe.cpp:312] Batch 46, loss = 0.135202
I0701 00:18:23.094313  9899 caffe.cpp:312] Batch 47, accuracy/top1 = 0.947517
I0701 00:18:23.094336  9899 caffe.cpp:312] Batch 47, accuracy/top5 = 0.995586
I0701 00:18:23.094341  9899 caffe.cpp:312] Batch 47, loss = 0.104496
I0701 00:18:23.769681  9899 caffe.cpp:312] Batch 48, accuracy/top1 = 0.892747
I0701 00:18:23.769709  9899 caffe.cpp:312] Batch 48, accuracy/top5 = 0.997442
I0701 00:18:23.769713  9899 caffe.cpp:312] Batch 48, loss = 0.217535
I0701 00:18:24.540637  9899 caffe.cpp:312] Batch 49, accuracy/top1 = 0.88619
I0701 00:18:24.540663  9899 caffe.cpp:312] Batch 49, accuracy/top5 = 0.986018
I0701 00:18:24.540668  9899 caffe.cpp:312] Batch 49, loss = 0.255068
I0701 00:18:24.540670  9899 caffe.cpp:317] Loss: 0.153441
I0701 00:18:24.540679  9899 caffe.cpp:329] accuracy/top1 = 0.926386
I0701 00:18:24.540686  9899 caffe.cpp:329] accuracy/top5 = 0.993662
I0701 00:18:24.540694  9899 caffe.cpp:329] loss = 0.153441 (* 1 = 0.153441 loss)
