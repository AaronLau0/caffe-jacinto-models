Logging output to training/cityscapes20_jsegnet21v2_2017-06-29_16-47-23/train-log_2017-06-29_16-47-23.txt
Using pretrained model training/imagenet_jacintonet11_v2_bn_iter_160000.caffemodel
training/cityscapes20_jsegnet21v2_2017-06-29_16-47-23/initial
I0629 16:47:24.622488  8020 caffe.cpp:608] This is NVCaffe 0.16.2 started at Thu Jun 29 16:47:24 2017
I0629 16:47:24.622596  8020 caffe.cpp:611] CuDNN version: 6.0.21
I0629 16:47:24.622598  8020 caffe.cpp:612] CuBLAS version: 8000
I0629 16:47:24.622601  8020 caffe.cpp:613] CUDA version: 8000
I0629 16:47:24.622602  8020 caffe.cpp:614] CUDA driver version: 8000
I0629 16:47:24.892845  8020 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0629 16:47:24.893414  8020 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I0629 16:47:24.893932  8020 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[1]: total=8508145664 free=8379236352
I0629 16:47:24.894445  8020 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[2]: total=8508145664 free=8379236352
I0629 16:47:24.894454  8020 caffe.cpp:208] Using GPUs 0, 1, 2
I0629 16:47:24.894776  8020 caffe.cpp:213] GPU 0: GeForce GTX 1080
I0629 16:47:24.895102  8020 caffe.cpp:213] GPU 1: GeForce GTX 1080
I0629 16:47:24.895424  8020 caffe.cpp:213] GPU 2: GeForce GTX 1080
I0629 16:47:24.895462  8020 solver.cpp:42] Solver data type: FLOAT
I0629 16:47:24.895491  8020 solver.cpp:45] Initializing solver from parameters: 
train_net: "training/cityscapes20_jsegnet21v2_2017-06-29_16-47-23/initial/train.prototxt"
test_net: "training/cityscapes20_jsegnet21v2_2017-06-29_16-47-23/initial/test.prototxt"
test_iter: 125
test_interval: 2000
base_lr: 0.0001
display: 100
max_iter: 32000
lr_policy: "multistep"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 0.0001
snapshot: 10000
snapshot_prefix: "training/cityscapes20_jsegnet21v2_2017-06-29_16-47-23/initial/cityscapes20_jsegnet21v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
test_initialization: false
stepvalue: 24000
iter_size: 1
type: "Adam"
I0629 16:47:24.901581  8020 solver.cpp:77] Creating training net from train_net file: training/cityscapes20_jsegnet21v2_2017-06-29_16-47-23/initial/train.prototxt
I0629 16:47:24.902158  8020 net.cpp:442] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0629 16:47:24.902165  8020 net.cpp:442] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
W0629 16:47:24.902197  8020 parallel.cpp:271] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0629 16:47:24.902434  8020 net.cpp:77] Initializing net from parameters: 
name: "jsegnet21v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/train-image-lmdb"
    label_list_path: "data/train-label-lmdb"
    batch_size: 6
    threads: 1
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 20
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 20
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 20
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
I0629 16:47:24.902571  8020 net.cpp:108] Using FLOAT as default forward math type
I0629 16:47:24.902577  8020 net.cpp:114] Using FLOAT as default backward math type
I0629 16:47:24.902581  8020 layer_factory.hpp:136] Creating layer 'data' of type 'ImageLabelData'
I0629 16:47:24.902585  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:24.902600  8020 net.cpp:183] Created Layer data (0)
I0629 16:47:24.902604  8020 net.cpp:529] data -> data
I0629 16:47:24.902622  8020 net.cpp:529] data -> label
I0629 16:47:24.902689  8020 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0629 16:47:24.902703  8020 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0629 16:47:24.903913  8067 db_lmdb.cpp:35] Opened lmdb data/train-image-lmdb
I0629 16:47:24.915844  8020 data_layer.cpp:188] ReshapePrefetch 6, 3, 640, 640
I0629 16:47:24.915892  8020 data_layer.cpp:206] Output data size: 6, 3, 640, 640
I0629 16:47:24.915899  8020 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0629 16:47:24.915935  8020 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0629 16:47:24.915943  8020 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0629 16:47:24.916630  8068 data_layer.cpp:188] ReshapePrefetch 6, 3, 640, 640
I0629 16:47:24.916640  8068 data_layer.cpp:206] Output data size: 6, 3, 640, 640
I0629 16:47:24.921768  8069 db_lmdb.cpp:35] Opened lmdb data/train-label-lmdb
I0629 16:47:24.922646  8020 data_layer.cpp:188] ReshapePrefetch 6, 1, 640, 640
I0629 16:47:24.922709  8020 data_layer.cpp:206] Output data size: 6, 1, 640, 640
I0629 16:47:24.922718  8020 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0629 16:47:24.922794  8020 net.cpp:244] Setting up data
I0629 16:47:24.922822  8020 net.cpp:251] TRAIN Top shape for layer 0 'data' 6 3 640 640 (7372800)
I0629 16:47:24.922871  8020 net.cpp:251] TRAIN Top shape for layer 0 'data' 6 1 640 640 (2457600)
I0629 16:47:24.922885  8020 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0629 16:47:24.922894  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:24.922920  8020 net.cpp:183] Created Layer data/bias (1)
I0629 16:47:24.922929  8020 net.cpp:560] data/bias <- data
I0629 16:47:24.922943  8020 net.cpp:529] data/bias -> data/bias
I0629 16:47:24.924352  8070 data_layer.cpp:188] ReshapePrefetch 6, 1, 640, 640
I0629 16:47:24.924393  8070 data_layer.cpp:206] Output data size: 6, 1, 640, 640
I0629 16:47:24.930927  8020 net.cpp:244] Setting up data/bias
I0629 16:47:24.930948  8020 net.cpp:251] TRAIN Top shape for layer 1 'data/bias' 6 3 640 640 (7372800)
I0629 16:47:24.930958  8020 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0629 16:47:24.930963  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:24.930991  8020 net.cpp:183] Created Layer conv1a (2)
I0629 16:47:24.930995  8020 net.cpp:560] conv1a <- data/bias
I0629 16:47:24.930999  8020 net.cpp:529] conv1a -> conv1a
I0629 16:47:24.934393  8068 data_layer.cpp:110] [0] Parser threads: 1
I0629 16:47:24.934403  8068 data_layer.cpp:112] [0] Transformer threads: 1
I0629 16:47:24.935308  8070 data_layer.cpp:110] [0] Parser threads: 1
I0629 16:47:24.935317  8070 data_layer.cpp:112] [0] Transformer threads: 1
I0629 16:47:25.274374  8020 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 7.9G, req 0G)
I0629 16:47:25.274412  8020 net.cpp:244] Setting up conv1a
I0629 16:47:25.274422  8020 net.cpp:251] TRAIN Top shape for layer 2 'conv1a' 6 32 320 320 (19660800)
I0629 16:47:25.274435  8020 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0629 16:47:25.274438  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.274453  8020 net.cpp:183] Created Layer conv1a/bn (3)
I0629 16:47:25.274458  8020 net.cpp:560] conv1a/bn <- conv1a
I0629 16:47:25.274462  8020 net.cpp:512] conv1a/bn -> conv1a (in-place)
I0629 16:47:25.275434  8020 net.cpp:244] Setting up conv1a/bn
I0629 16:47:25.275446  8020 net.cpp:251] TRAIN Top shape for layer 3 'conv1a/bn' 6 32 320 320 (19660800)
I0629 16:47:25.275456  8020 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0629 16:47:25.275460  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.275466  8020 net.cpp:183] Created Layer conv1a/relu (4)
I0629 16:47:25.275470  8020 net.cpp:560] conv1a/relu <- conv1a
I0629 16:47:25.275472  8020 net.cpp:512] conv1a/relu -> conv1a (in-place)
I0629 16:47:25.275485  8020 net.cpp:244] Setting up conv1a/relu
I0629 16:47:25.275490  8020 net.cpp:251] TRAIN Top shape for layer 4 'conv1a/relu' 6 32 320 320 (19660800)
I0629 16:47:25.275493  8020 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0629 16:47:25.275496  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.275508  8020 net.cpp:183] Created Layer conv1b (5)
I0629 16:47:25.275512  8020 net.cpp:560] conv1b <- conv1a
I0629 16:47:25.275516  8020 net.cpp:529] conv1b -> conv1b
I0629 16:47:25.320623  8020 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.73G, req 0G)
I0629 16:47:25.320652  8020 net.cpp:244] Setting up conv1b
I0629 16:47:25.320660  8020 net.cpp:251] TRAIN Top shape for layer 5 'conv1b' 6 32 320 320 (19660800)
I0629 16:47:25.320670  8020 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0629 16:47:25.320674  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.320683  8020 net.cpp:183] Created Layer conv1b/bn (6)
I0629 16:47:25.320686  8020 net.cpp:560] conv1b/bn <- conv1b
I0629 16:47:25.320691  8020 net.cpp:512] conv1b/bn -> conv1b (in-place)
I0629 16:47:25.321472  8020 net.cpp:244] Setting up conv1b/bn
I0629 16:47:25.321481  8020 net.cpp:251] TRAIN Top shape for layer 6 'conv1b/bn' 6 32 320 320 (19660800)
I0629 16:47:25.321490  8020 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0629 16:47:25.321492  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.321496  8020 net.cpp:183] Created Layer conv1b/relu (7)
I0629 16:47:25.321498  8020 net.cpp:560] conv1b/relu <- conv1b
I0629 16:47:25.321501  8020 net.cpp:512] conv1b/relu -> conv1b (in-place)
I0629 16:47:25.321506  8020 net.cpp:244] Setting up conv1b/relu
I0629 16:47:25.321508  8020 net.cpp:251] TRAIN Top shape for layer 7 'conv1b/relu' 6 32 320 320 (19660800)
I0629 16:47:25.321511  8020 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0629 16:47:25.321512  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.321521  8020 net.cpp:183] Created Layer pool1 (8)
I0629 16:47:25.321533  8020 net.cpp:560] pool1 <- conv1b
I0629 16:47:25.321537  8020 net.cpp:529] pool1 -> pool1
I0629 16:47:25.321638  8020 net.cpp:244] Setting up pool1
I0629 16:47:25.321646  8020 net.cpp:251] TRAIN Top shape for layer 8 'pool1' 6 32 160 160 (4915200)
I0629 16:47:25.321651  8020 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0629 16:47:25.321655  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.321666  8020 net.cpp:183] Created Layer res2a_branch2a (9)
I0629 16:47:25.321671  8020 net.cpp:560] res2a_branch2a <- pool1
I0629 16:47:25.321674  8020 net.cpp:529] res2a_branch2a -> res2a_branch2a
I0629 16:47:25.360770  8020 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.61G, req 0G)
I0629 16:47:25.360808  8020 net.cpp:244] Setting up res2a_branch2a
I0629 16:47:25.360819  8020 net.cpp:251] TRAIN Top shape for layer 9 'res2a_branch2a' 6 64 160 160 (9830400)
I0629 16:47:25.360833  8020 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0629 16:47:25.360838  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.360849  8020 net.cpp:183] Created Layer res2a_branch2a/bn (10)
I0629 16:47:25.360855  8020 net.cpp:560] res2a_branch2a/bn <- res2a_branch2a
I0629 16:47:25.360860  8020 net.cpp:512] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0629 16:47:25.362782  8020 net.cpp:244] Setting up res2a_branch2a/bn
I0629 16:47:25.362799  8020 net.cpp:251] TRAIN Top shape for layer 10 'res2a_branch2a/bn' 6 64 160 160 (9830400)
I0629 16:47:25.362812  8020 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0629 16:47:25.362818  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.362824  8020 net.cpp:183] Created Layer res2a_branch2a/relu (11)
I0629 16:47:25.362828  8020 net.cpp:560] res2a_branch2a/relu <- res2a_branch2a
I0629 16:47:25.362856  8020 net.cpp:512] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0629 16:47:25.362867  8020 net.cpp:244] Setting up res2a_branch2a/relu
I0629 16:47:25.362874  8020 net.cpp:251] TRAIN Top shape for layer 11 'res2a_branch2a/relu' 6 64 160 160 (9830400)
I0629 16:47:25.362881  8020 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0629 16:47:25.362888  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.362907  8020 net.cpp:183] Created Layer res2a_branch2b (12)
I0629 16:47:25.362916  8020 net.cpp:560] res2a_branch2b <- res2a_branch2a
I0629 16:47:25.362921  8020 net.cpp:529] res2a_branch2b -> res2a_branch2b
I0629 16:47:25.386049  8020 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.52G, req 0G)
I0629 16:47:25.386088  8020 net.cpp:244] Setting up res2a_branch2b
I0629 16:47:25.386101  8020 net.cpp:251] TRAIN Top shape for layer 12 'res2a_branch2b' 6 64 160 160 (9830400)
I0629 16:47:25.386111  8020 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0629 16:47:25.386117  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.386128  8020 net.cpp:183] Created Layer res2a_branch2b/bn (13)
I0629 16:47:25.386135  8020 net.cpp:560] res2a_branch2b/bn <- res2a_branch2b
I0629 16:47:25.386140  8020 net.cpp:512] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0629 16:47:25.387387  8020 net.cpp:244] Setting up res2a_branch2b/bn
I0629 16:47:25.387403  8020 net.cpp:251] TRAIN Top shape for layer 13 'res2a_branch2b/bn' 6 64 160 160 (9830400)
I0629 16:47:25.387414  8020 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0629 16:47:25.387419  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.387425  8020 net.cpp:183] Created Layer res2a_branch2b/relu (14)
I0629 16:47:25.387430  8020 net.cpp:560] res2a_branch2b/relu <- res2a_branch2b
I0629 16:47:25.387452  8020 net.cpp:512] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0629 16:47:25.387462  8020 net.cpp:244] Setting up res2a_branch2b/relu
I0629 16:47:25.387471  8020 net.cpp:251] TRAIN Top shape for layer 14 'res2a_branch2b/relu' 6 64 160 160 (9830400)
I0629 16:47:25.387478  8020 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0629 16:47:25.387485  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.387496  8020 net.cpp:183] Created Layer pool2 (15)
I0629 16:47:25.387504  8020 net.cpp:560] pool2 <- res2a_branch2b
I0629 16:47:25.387509  8020 net.cpp:529] pool2 -> pool2
I0629 16:47:25.387624  8020 net.cpp:244] Setting up pool2
I0629 16:47:25.387634  8020 net.cpp:251] TRAIN Top shape for layer 15 'pool2' 6 64 80 80 (2457600)
I0629 16:47:25.387640  8020 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0629 16:47:25.387648  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.387665  8020 net.cpp:183] Created Layer res3a_branch2a (16)
I0629 16:47:25.387675  8020 net.cpp:560] res3a_branch2a <- pool2
I0629 16:47:25.387679  8020 net.cpp:529] res3a_branch2a -> res3a_branch2a
I0629 16:47:25.411062  8020 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 7.46G, req 0G)
I0629 16:47:25.411103  8020 net.cpp:244] Setting up res3a_branch2a
I0629 16:47:25.411115  8020 net.cpp:251] TRAIN Top shape for layer 16 'res3a_branch2a' 6 128 80 80 (4915200)
I0629 16:47:25.411128  8020 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0629 16:47:25.411134  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.411145  8020 net.cpp:183] Created Layer res3a_branch2a/bn (17)
I0629 16:47:25.411152  8020 net.cpp:560] res3a_branch2a/bn <- res3a_branch2a
I0629 16:47:25.411159  8020 net.cpp:512] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0629 16:47:25.412415  8020 net.cpp:244] Setting up res3a_branch2a/bn
I0629 16:47:25.412431  8020 net.cpp:251] TRAIN Top shape for layer 17 'res3a_branch2a/bn' 6 128 80 80 (4915200)
I0629 16:47:25.412446  8020 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0629 16:47:25.412451  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.412458  8020 net.cpp:183] Created Layer res3a_branch2a/relu (18)
I0629 16:47:25.412462  8020 net.cpp:560] res3a_branch2a/relu <- res3a_branch2a
I0629 16:47:25.412467  8020 net.cpp:512] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0629 16:47:25.412475  8020 net.cpp:244] Setting up res3a_branch2a/relu
I0629 16:47:25.412482  8020 net.cpp:251] TRAIN Top shape for layer 18 'res3a_branch2a/relu' 6 128 80 80 (4915200)
I0629 16:47:25.412485  8020 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0629 16:47:25.412490  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.412504  8020 net.cpp:183] Created Layer res3a_branch2b (19)
I0629 16:47:25.412513  8020 net.cpp:560] res3a_branch2b <- res3a_branch2a
I0629 16:47:25.412519  8020 net.cpp:529] res3a_branch2b -> res3a_branch2b
I0629 16:47:25.427157  8020 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.42G, req 0G)
I0629 16:47:25.427196  8020 net.cpp:244] Setting up res3a_branch2b
I0629 16:47:25.427207  8020 net.cpp:251] TRAIN Top shape for layer 19 'res3a_branch2b' 6 128 80 80 (4915200)
I0629 16:47:25.427218  8020 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0629 16:47:25.427224  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.427235  8020 net.cpp:183] Created Layer res3a_branch2b/bn (20)
I0629 16:47:25.427242  8020 net.cpp:560] res3a_branch2b/bn <- res3a_branch2b
I0629 16:47:25.427248  8020 net.cpp:512] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0629 16:47:25.428465  8020 net.cpp:244] Setting up res3a_branch2b/bn
I0629 16:47:25.428481  8020 net.cpp:251] TRAIN Top shape for layer 20 'res3a_branch2b/bn' 6 128 80 80 (4915200)
I0629 16:47:25.428493  8020 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0629 16:47:25.428498  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.428503  8020 net.cpp:183] Created Layer res3a_branch2b/relu (21)
I0629 16:47:25.428508  8020 net.cpp:560] res3a_branch2b/relu <- res3a_branch2b
I0629 16:47:25.428513  8020 net.cpp:512] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0629 16:47:25.428519  8020 net.cpp:244] Setting up res3a_branch2b/relu
I0629 16:47:25.428525  8020 net.cpp:251] TRAIN Top shape for layer 21 'res3a_branch2b/relu' 6 128 80 80 (4915200)
I0629 16:47:25.428529  8020 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0629 16:47:25.428535  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.428544  8020 net.cpp:183] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (22)
I0629 16:47:25.428552  8020 net.cpp:560] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0629 16:47:25.428560  8020 net.cpp:529] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0629 16:47:25.428581  8020 net.cpp:529] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0629 16:47:25.428668  8020 net.cpp:244] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0629 16:47:25.428678  8020 net.cpp:251] TRAIN Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 6 128 80 80 (4915200)
I0629 16:47:25.428683  8020 net.cpp:251] TRAIN Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 6 128 80 80 (4915200)
I0629 16:47:25.428688  8020 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0629 16:47:25.428692  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.428701  8020 net.cpp:183] Created Layer pool3 (23)
I0629 16:47:25.428707  8020 net.cpp:560] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0629 16:47:25.428712  8020 net.cpp:529] pool3 -> pool3
I0629 16:47:25.428834  8020 net.cpp:244] Setting up pool3
I0629 16:47:25.428844  8020 net.cpp:251] TRAIN Top shape for layer 23 'pool3' 6 128 40 40 (1228800)
I0629 16:47:25.428850  8020 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0629 16:47:25.428855  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.428869  8020 net.cpp:183] Created Layer res4a_branch2a (24)
I0629 16:47:25.428874  8020 net.cpp:560] res4a_branch2a <- pool3
I0629 16:47:25.428880  8020 net.cpp:529] res4a_branch2a -> res4a_branch2a
I0629 16:47:25.462076  8020 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.38G, req 0G)
I0629 16:47:25.462110  8020 net.cpp:244] Setting up res4a_branch2a
I0629 16:47:25.462121  8020 net.cpp:251] TRAIN Top shape for layer 24 'res4a_branch2a' 6 256 40 40 (2457600)
I0629 16:47:25.462129  8020 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0629 16:47:25.462134  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.462146  8020 net.cpp:183] Created Layer res4a_branch2a/bn (25)
I0629 16:47:25.462152  8020 net.cpp:560] res4a_branch2a/bn <- res4a_branch2a
I0629 16:47:25.462157  8020 net.cpp:512] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0629 16:47:25.463217  8020 net.cpp:244] Setting up res4a_branch2a/bn
I0629 16:47:25.463228  8020 net.cpp:251] TRAIN Top shape for layer 25 'res4a_branch2a/bn' 6 256 40 40 (2457600)
I0629 16:47:25.463238  8020 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0629 16:47:25.463240  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.463255  8020 net.cpp:183] Created Layer res4a_branch2a/relu (26)
I0629 16:47:25.463259  8020 net.cpp:560] res4a_branch2a/relu <- res4a_branch2a
I0629 16:47:25.463263  8020 net.cpp:512] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0629 16:47:25.463268  8020 net.cpp:244] Setting up res4a_branch2a/relu
I0629 16:47:25.463274  8020 net.cpp:251] TRAIN Top shape for layer 26 'res4a_branch2a/relu' 6 256 40 40 (2457600)
I0629 16:47:25.463277  8020 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0629 16:47:25.463282  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.463291  8020 net.cpp:183] Created Layer res4a_branch2b (27)
I0629 16:47:25.463297  8020 net.cpp:560] res4a_branch2b <- res4a_branch2a
I0629 16:47:25.463302  8020 net.cpp:529] res4a_branch2b -> res4a_branch2b
I0629 16:47:25.475230  8020 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.36G, req 0G)
I0629 16:47:25.475265  8020 net.cpp:244] Setting up res4a_branch2b
I0629 16:47:25.475275  8020 net.cpp:251] TRAIN Top shape for layer 27 'res4a_branch2b' 6 256 40 40 (2457600)
I0629 16:47:25.475288  8020 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0629 16:47:25.475294  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.475308  8020 net.cpp:183] Created Layer res4a_branch2b/bn (28)
I0629 16:47:25.475313  8020 net.cpp:560] res4a_branch2b/bn <- res4a_branch2b
I0629 16:47:25.475319  8020 net.cpp:512] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0629 16:47:25.476258  8020 net.cpp:244] Setting up res4a_branch2b/bn
I0629 16:47:25.476279  8020 net.cpp:251] TRAIN Top shape for layer 28 'res4a_branch2b/bn' 6 256 40 40 (2457600)
I0629 16:47:25.476289  8020 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0629 16:47:25.476292  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.476300  8020 net.cpp:183] Created Layer res4a_branch2b/relu (29)
I0629 16:47:25.476303  8020 net.cpp:560] res4a_branch2b/relu <- res4a_branch2b
I0629 16:47:25.476307  8020 net.cpp:512] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0629 16:47:25.476312  8020 net.cpp:244] Setting up res4a_branch2b/relu
I0629 16:47:25.476315  8020 net.cpp:251] TRAIN Top shape for layer 29 'res4a_branch2b/relu' 6 256 40 40 (2457600)
I0629 16:47:25.476318  8020 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0629 16:47:25.476320  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.476327  8020 net.cpp:183] Created Layer pool4 (30)
I0629 16:47:25.476332  8020 net.cpp:560] pool4 <- res4a_branch2b
I0629 16:47:25.476338  8020 net.cpp:529] pool4 -> pool4
I0629 16:47:25.476439  8020 net.cpp:244] Setting up pool4
I0629 16:47:25.476454  8020 net.cpp:251] TRAIN Top shape for layer 30 'pool4' 6 256 40 40 (2457600)
I0629 16:47:25.476459  8020 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0629 16:47:25.476464  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.476485  8020 net.cpp:183] Created Layer res5a_branch2a (31)
I0629 16:47:25.476490  8020 net.cpp:560] res5a_branch2a <- pool4
I0629 16:47:25.476495  8020 net.cpp:529] res5a_branch2a -> res5a_branch2a
I0629 16:47:25.504148  8020 net.cpp:244] Setting up res5a_branch2a
I0629 16:47:25.504168  8020 net.cpp:251] TRAIN Top shape for layer 31 'res5a_branch2a' 6 512 40 40 (4915200)
I0629 16:47:25.504174  8020 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0629 16:47:25.504179  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.504186  8020 net.cpp:183] Created Layer res5a_branch2a/bn (32)
I0629 16:47:25.504189  8020 net.cpp:560] res5a_branch2a/bn <- res5a_branch2a
I0629 16:47:25.504204  8020 net.cpp:512] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0629 16:47:25.505358  8020 net.cpp:244] Setting up res5a_branch2a/bn
I0629 16:47:25.505368  8020 net.cpp:251] TRAIN Top shape for layer 32 'res5a_branch2a/bn' 6 512 40 40 (4915200)
I0629 16:47:25.505374  8020 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0629 16:47:25.505378  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.505381  8020 net.cpp:183] Created Layer res5a_branch2a/relu (33)
I0629 16:47:25.505383  8020 net.cpp:560] res5a_branch2a/relu <- res5a_branch2a
I0629 16:47:25.505385  8020 net.cpp:512] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0629 16:47:25.505390  8020 net.cpp:244] Setting up res5a_branch2a/relu
I0629 16:47:25.505393  8020 net.cpp:251] TRAIN Top shape for layer 33 'res5a_branch2a/relu' 6 512 40 40 (4915200)
I0629 16:47:25.505394  8020 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0629 16:47:25.505396  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.505403  8020 net.cpp:183] Created Layer res5a_branch2b (34)
I0629 16:47:25.505405  8020 net.cpp:560] res5a_branch2b <- res5a_branch2a
I0629 16:47:25.505408  8020 net.cpp:529] res5a_branch2b -> res5a_branch2b
I0629 16:47:25.518271  8020 net.cpp:244] Setting up res5a_branch2b
I0629 16:47:25.518295  8020 net.cpp:251] TRAIN Top shape for layer 34 'res5a_branch2b' 6 512 40 40 (4915200)
I0629 16:47:25.518307  8020 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0629 16:47:25.518311  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.518317  8020 net.cpp:183] Created Layer res5a_branch2b/bn (35)
I0629 16:47:25.518321  8020 net.cpp:560] res5a_branch2b/bn <- res5a_branch2b
I0629 16:47:25.518323  8020 net.cpp:512] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0629 16:47:25.519554  8020 net.cpp:244] Setting up res5a_branch2b/bn
I0629 16:47:25.519563  8020 net.cpp:251] TRAIN Top shape for layer 35 'res5a_branch2b/bn' 6 512 40 40 (4915200)
I0629 16:47:25.519569  8020 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0629 16:47:25.519572  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.519575  8020 net.cpp:183] Created Layer res5a_branch2b/relu (36)
I0629 16:47:25.519577  8020 net.cpp:560] res5a_branch2b/relu <- res5a_branch2b
I0629 16:47:25.519580  8020 net.cpp:512] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0629 16:47:25.519584  8020 net.cpp:244] Setting up res5a_branch2b/relu
I0629 16:47:25.519587  8020 net.cpp:251] TRAIN Top shape for layer 36 'res5a_branch2b/relu' 6 512 40 40 (4915200)
I0629 16:47:25.519589  8020 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I0629 16:47:25.519592  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.519603  8020 net.cpp:183] Created Layer out5a (37)
I0629 16:47:25.519605  8020 net.cpp:560] out5a <- res5a_branch2b
I0629 16:47:25.519608  8020 net.cpp:529] out5a -> out5a
I0629 16:47:25.523785  8020 net.cpp:244] Setting up out5a
I0629 16:47:25.523800  8020 net.cpp:251] TRAIN Top shape for layer 37 'out5a' 6 64 40 40 (614400)
I0629 16:47:25.523805  8020 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I0629 16:47:25.523808  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.523823  8020 net.cpp:183] Created Layer out5a/bn (38)
I0629 16:47:25.523825  8020 net.cpp:560] out5a/bn <- out5a
I0629 16:47:25.523828  8020 net.cpp:512] out5a/bn -> out5a (in-place)
I0629 16:47:25.524950  8020 net.cpp:244] Setting up out5a/bn
I0629 16:47:25.524958  8020 net.cpp:251] TRAIN Top shape for layer 38 'out5a/bn' 6 64 40 40 (614400)
I0629 16:47:25.524965  8020 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I0629 16:47:25.524974  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.524978  8020 net.cpp:183] Created Layer out5a/relu (39)
I0629 16:47:25.524981  8020 net.cpp:560] out5a/relu <- out5a
I0629 16:47:25.524983  8020 net.cpp:512] out5a/relu -> out5a (in-place)
I0629 16:47:25.524987  8020 net.cpp:244] Setting up out5a/relu
I0629 16:47:25.524991  8020 net.cpp:251] TRAIN Top shape for layer 39 'out5a/relu' 6 64 40 40 (614400)
I0629 16:47:25.524992  8020 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I0629 16:47:25.524996  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.525009  8020 net.cpp:183] Created Layer out5a_up2 (40)
I0629 16:47:25.525012  8020 net.cpp:560] out5a_up2 <- out5a
I0629 16:47:25.525014  8020 net.cpp:529] out5a_up2 -> out5a_up2
I0629 16:47:25.525339  8020 net.cpp:244] Setting up out5a_up2
I0629 16:47:25.525346  8020 net.cpp:251] TRAIN Top shape for layer 40 'out5a_up2' 6 64 80 80 (2457600)
I0629 16:47:25.525348  8020 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I0629 16:47:25.525351  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.525360  8020 net.cpp:183] Created Layer out3a (41)
I0629 16:47:25.525363  8020 net.cpp:560] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0629 16:47:25.525367  8020 net.cpp:529] out3a -> out3a
I0629 16:47:25.538969  8020 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 7.25G, req 0G)
I0629 16:47:25.538987  8020 net.cpp:244] Setting up out3a
I0629 16:47:25.538992  8020 net.cpp:251] TRAIN Top shape for layer 41 'out3a' 6 64 80 80 (2457600)
I0629 16:47:25.538996  8020 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I0629 16:47:25.539000  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.539003  8020 net.cpp:183] Created Layer out3a/bn (42)
I0629 16:47:25.539006  8020 net.cpp:560] out3a/bn <- out3a
I0629 16:47:25.539008  8020 net.cpp:512] out3a/bn -> out3a (in-place)
I0629 16:47:25.539691  8020 net.cpp:244] Setting up out3a/bn
I0629 16:47:25.539698  8020 net.cpp:251] TRAIN Top shape for layer 42 'out3a/bn' 6 64 80 80 (2457600)
I0629 16:47:25.539703  8020 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I0629 16:47:25.539706  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.539710  8020 net.cpp:183] Created Layer out3a/relu (43)
I0629 16:47:25.539711  8020 net.cpp:560] out3a/relu <- out3a
I0629 16:47:25.539713  8020 net.cpp:512] out3a/relu -> out3a (in-place)
I0629 16:47:25.539716  8020 net.cpp:244] Setting up out3a/relu
I0629 16:47:25.539719  8020 net.cpp:251] TRAIN Top shape for layer 43 'out3a/relu' 6 64 80 80 (2457600)
I0629 16:47:25.539721  8020 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I0629 16:47:25.539723  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.539732  8020 net.cpp:183] Created Layer out3_out5_combined (44)
I0629 16:47:25.539736  8020 net.cpp:560] out3_out5_combined <- out5a_up2
I0629 16:47:25.539737  8020 net.cpp:560] out3_out5_combined <- out3a
I0629 16:47:25.539739  8020 net.cpp:529] out3_out5_combined -> out3_out5_combined
I0629 16:47:25.540701  8020 net.cpp:244] Setting up out3_out5_combined
I0629 16:47:25.540709  8020 net.cpp:251] TRAIN Top shape for layer 44 'out3_out5_combined' 6 64 80 80 (2457600)
I0629 16:47:25.540712  8020 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I0629 16:47:25.540714  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.540721  8020 net.cpp:183] Created Layer ctx_conv1 (45)
I0629 16:47:25.540724  8020 net.cpp:560] ctx_conv1 <- out3_out5_combined
I0629 16:47:25.540726  8020 net.cpp:529] ctx_conv1 -> ctx_conv1
I0629 16:47:25.554333  8020 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 7.2G, req 0G)
I0629 16:47:25.554361  8020 net.cpp:244] Setting up ctx_conv1
I0629 16:47:25.554368  8020 net.cpp:251] TRAIN Top shape for layer 45 'ctx_conv1' 6 64 80 80 (2457600)
I0629 16:47:25.554376  8020 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I0629 16:47:25.554379  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.554388  8020 net.cpp:183] Created Layer ctx_conv1/bn (46)
I0629 16:47:25.554391  8020 net.cpp:560] ctx_conv1/bn <- ctx_conv1
I0629 16:47:25.554394  8020 net.cpp:512] ctx_conv1/bn -> ctx_conv1 (in-place)
I0629 16:47:25.555162  8020 net.cpp:244] Setting up ctx_conv1/bn
I0629 16:47:25.555171  8020 net.cpp:251] TRAIN Top shape for layer 46 'ctx_conv1/bn' 6 64 80 80 (2457600)
I0629 16:47:25.555177  8020 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I0629 16:47:25.555181  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.555183  8020 net.cpp:183] Created Layer ctx_conv1/relu (47)
I0629 16:47:25.555186  8020 net.cpp:560] ctx_conv1/relu <- ctx_conv1
I0629 16:47:25.555188  8020 net.cpp:512] ctx_conv1/relu -> ctx_conv1 (in-place)
I0629 16:47:25.555192  8020 net.cpp:244] Setting up ctx_conv1/relu
I0629 16:47:25.555194  8020 net.cpp:251] TRAIN Top shape for layer 47 'ctx_conv1/relu' 6 64 80 80 (2457600)
I0629 16:47:25.555197  8020 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I0629 16:47:25.555199  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.555207  8020 net.cpp:183] Created Layer ctx_conv2 (48)
I0629 16:47:25.555210  8020 net.cpp:560] ctx_conv2 <- ctx_conv1
I0629 16:47:25.555212  8020 net.cpp:529] ctx_conv2 -> ctx_conv2
I0629 16:47:25.556320  8020 net.cpp:244] Setting up ctx_conv2
I0629 16:47:25.556327  8020 net.cpp:251] TRAIN Top shape for layer 48 'ctx_conv2' 6 64 80 80 (2457600)
I0629 16:47:25.556331  8020 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I0629 16:47:25.556334  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.556337  8020 net.cpp:183] Created Layer ctx_conv2/bn (49)
I0629 16:47:25.556339  8020 net.cpp:560] ctx_conv2/bn <- ctx_conv2
I0629 16:47:25.556342  8020 net.cpp:512] ctx_conv2/bn -> ctx_conv2 (in-place)
I0629 16:47:25.557489  8020 net.cpp:244] Setting up ctx_conv2/bn
I0629 16:47:25.557498  8020 net.cpp:251] TRAIN Top shape for layer 49 'ctx_conv2/bn' 6 64 80 80 (2457600)
I0629 16:47:25.557504  8020 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I0629 16:47:25.557507  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.557510  8020 net.cpp:183] Created Layer ctx_conv2/relu (50)
I0629 16:47:25.557512  8020 net.cpp:560] ctx_conv2/relu <- ctx_conv2
I0629 16:47:25.557514  8020 net.cpp:512] ctx_conv2/relu -> ctx_conv2 (in-place)
I0629 16:47:25.557518  8020 net.cpp:244] Setting up ctx_conv2/relu
I0629 16:47:25.557520  8020 net.cpp:251] TRAIN Top shape for layer 50 'ctx_conv2/relu' 6 64 80 80 (2457600)
I0629 16:47:25.557523  8020 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I0629 16:47:25.557524  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.557529  8020 net.cpp:183] Created Layer ctx_conv3 (51)
I0629 16:47:25.557533  8020 net.cpp:560] ctx_conv3 <- ctx_conv2
I0629 16:47:25.557534  8020 net.cpp:529] ctx_conv3 -> ctx_conv3
I0629 16:47:25.558626  8020 net.cpp:244] Setting up ctx_conv3
I0629 16:47:25.558634  8020 net.cpp:251] TRAIN Top shape for layer 51 'ctx_conv3' 6 64 80 80 (2457600)
I0629 16:47:25.558637  8020 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I0629 16:47:25.558640  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.558651  8020 net.cpp:183] Created Layer ctx_conv3/bn (52)
I0629 16:47:25.558655  8020 net.cpp:560] ctx_conv3/bn <- ctx_conv3
I0629 16:47:25.558656  8020 net.cpp:512] ctx_conv3/bn -> ctx_conv3 (in-place)
I0629 16:47:25.559792  8020 net.cpp:244] Setting up ctx_conv3/bn
I0629 16:47:25.559800  8020 net.cpp:251] TRAIN Top shape for layer 52 'ctx_conv3/bn' 6 64 80 80 (2457600)
I0629 16:47:25.559805  8020 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I0629 16:47:25.559808  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.559811  8020 net.cpp:183] Created Layer ctx_conv3/relu (53)
I0629 16:47:25.559813  8020 net.cpp:560] ctx_conv3/relu <- ctx_conv3
I0629 16:47:25.559816  8020 net.cpp:512] ctx_conv3/relu -> ctx_conv3 (in-place)
I0629 16:47:25.559819  8020 net.cpp:244] Setting up ctx_conv3/relu
I0629 16:47:25.559823  8020 net.cpp:251] TRAIN Top shape for layer 53 'ctx_conv3/relu' 6 64 80 80 (2457600)
I0629 16:47:25.559823  8020 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I0629 16:47:25.559826  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.559831  8020 net.cpp:183] Created Layer ctx_conv4 (54)
I0629 16:47:25.559834  8020 net.cpp:560] ctx_conv4 <- ctx_conv3
I0629 16:47:25.559837  8020 net.cpp:529] ctx_conv4 -> ctx_conv4
I0629 16:47:25.560919  8020 net.cpp:244] Setting up ctx_conv4
I0629 16:47:25.560925  8020 net.cpp:251] TRAIN Top shape for layer 54 'ctx_conv4' 6 64 80 80 (2457600)
I0629 16:47:25.560930  8020 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I0629 16:47:25.560931  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.560935  8020 net.cpp:183] Created Layer ctx_conv4/bn (55)
I0629 16:47:25.560937  8020 net.cpp:560] ctx_conv4/bn <- ctx_conv4
I0629 16:47:25.560940  8020 net.cpp:512] ctx_conv4/bn -> ctx_conv4 (in-place)
I0629 16:47:25.562114  8020 net.cpp:244] Setting up ctx_conv4/bn
I0629 16:47:25.562122  8020 net.cpp:251] TRAIN Top shape for layer 55 'ctx_conv4/bn' 6 64 80 80 (2457600)
I0629 16:47:25.562129  8020 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I0629 16:47:25.562130  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.562134  8020 net.cpp:183] Created Layer ctx_conv4/relu (56)
I0629 16:47:25.562136  8020 net.cpp:560] ctx_conv4/relu <- ctx_conv4
I0629 16:47:25.562139  8020 net.cpp:512] ctx_conv4/relu -> ctx_conv4 (in-place)
I0629 16:47:25.562142  8020 net.cpp:244] Setting up ctx_conv4/relu
I0629 16:47:25.562144  8020 net.cpp:251] TRAIN Top shape for layer 56 'ctx_conv4/relu' 6 64 80 80 (2457600)
I0629 16:47:25.562146  8020 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I0629 16:47:25.562149  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.562153  8020 net.cpp:183] Created Layer ctx_final (57)
I0629 16:47:25.562155  8020 net.cpp:560] ctx_final <- ctx_conv4
I0629 16:47:25.562160  8020 net.cpp:529] ctx_final -> ctx_final
I0629 16:47:25.576900  8020 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 7.15G, req 0G)
I0629 16:47:25.576920  8020 net.cpp:244] Setting up ctx_final
I0629 16:47:25.576926  8020 net.cpp:251] TRAIN Top shape for layer 57 'ctx_final' 6 20 80 80 (768000)
I0629 16:47:25.576933  8020 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I0629 16:47:25.576944  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.576951  8020 net.cpp:183] Created Layer ctx_final/relu (58)
I0629 16:47:25.576956  8020 net.cpp:560] ctx_final/relu <- ctx_final
I0629 16:47:25.576961  8020 net.cpp:512] ctx_final/relu -> ctx_final (in-place)
I0629 16:47:25.576967  8020 net.cpp:244] Setting up ctx_final/relu
I0629 16:47:25.576972  8020 net.cpp:251] TRAIN Top shape for layer 58 'ctx_final/relu' 6 20 80 80 (768000)
I0629 16:47:25.576982  8020 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I0629 16:47:25.576985  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.576993  8020 net.cpp:183] Created Layer out_deconv_final_up2 (59)
I0629 16:47:25.576997  8020 net.cpp:560] out_deconv_final_up2 <- ctx_final
I0629 16:47:25.577002  8020 net.cpp:529] out_deconv_final_up2 -> out_deconv_final_up2
I0629 16:47:25.577324  8020 net.cpp:244] Setting up out_deconv_final_up2
I0629 16:47:25.577332  8020 net.cpp:251] TRAIN Top shape for layer 59 'out_deconv_final_up2' 6 20 160 160 (3072000)
I0629 16:47:25.577338  8020 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I0629 16:47:25.577342  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.577349  8020 net.cpp:183] Created Layer out_deconv_final_up4 (60)
I0629 16:47:25.577353  8020 net.cpp:560] out_deconv_final_up4 <- out_deconv_final_up2
I0629 16:47:25.577356  8020 net.cpp:529] out_deconv_final_up4 -> out_deconv_final_up4
I0629 16:47:25.577646  8020 net.cpp:244] Setting up out_deconv_final_up4
I0629 16:47:25.577652  8020 net.cpp:251] TRAIN Top shape for layer 60 'out_deconv_final_up4' 6 20 320 320 (12288000)
I0629 16:47:25.577658  8020 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I0629 16:47:25.577661  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.577671  8020 net.cpp:183] Created Layer out_deconv_final_up8 (61)
I0629 16:47:25.577674  8020 net.cpp:560] out_deconv_final_up8 <- out_deconv_final_up4
I0629 16:47:25.577678  8020 net.cpp:529] out_deconv_final_up8 -> out_deconv_final_up8
I0629 16:47:25.577967  8020 net.cpp:244] Setting up out_deconv_final_up8
I0629 16:47:25.577973  8020 net.cpp:251] TRAIN Top shape for layer 61 'out_deconv_final_up8' 6 20 640 640 (49152000)
I0629 16:47:25.577980  8020 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0629 16:47:25.577983  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.578001  8020 net.cpp:183] Created Layer loss (62)
I0629 16:47:25.578004  8020 net.cpp:560] loss <- out_deconv_final_up8
I0629 16:47:25.578008  8020 net.cpp:560] loss <- label
I0629 16:47:25.578016  8020 net.cpp:529] loss -> loss
I0629 16:47:25.580363  8020 net.cpp:244] Setting up loss
I0629 16:47:25.580371  8020 net.cpp:251] TRAIN Top shape for layer 62 'loss' (1)
I0629 16:47:25.580375  8020 net.cpp:255]     with loss weight 1
I0629 16:47:25.580382  8020 net.cpp:322] loss needs backward computation.
I0629 16:47:25.580387  8020 net.cpp:322] out_deconv_final_up8 needs backward computation.
I0629 16:47:25.580391  8020 net.cpp:322] out_deconv_final_up4 needs backward computation.
I0629 16:47:25.580395  8020 net.cpp:322] out_deconv_final_up2 needs backward computation.
I0629 16:47:25.580399  8020 net.cpp:322] ctx_final/relu needs backward computation.
I0629 16:47:25.580402  8020 net.cpp:322] ctx_final needs backward computation.
I0629 16:47:25.580406  8020 net.cpp:322] ctx_conv4/relu needs backward computation.
I0629 16:47:25.580410  8020 net.cpp:322] ctx_conv4/bn needs backward computation.
I0629 16:47:25.580415  8020 net.cpp:322] ctx_conv4 needs backward computation.
I0629 16:47:25.580418  8020 net.cpp:322] ctx_conv3/relu needs backward computation.
I0629 16:47:25.580421  8020 net.cpp:322] ctx_conv3/bn needs backward computation.
I0629 16:47:25.580425  8020 net.cpp:322] ctx_conv3 needs backward computation.
I0629 16:47:25.580428  8020 net.cpp:322] ctx_conv2/relu needs backward computation.
I0629 16:47:25.580432  8020 net.cpp:322] ctx_conv2/bn needs backward computation.
I0629 16:47:25.580436  8020 net.cpp:322] ctx_conv2 needs backward computation.
I0629 16:47:25.580440  8020 net.cpp:322] ctx_conv1/relu needs backward computation.
I0629 16:47:25.580443  8020 net.cpp:322] ctx_conv1/bn needs backward computation.
I0629 16:47:25.580454  8020 net.cpp:322] ctx_conv1 needs backward computation.
I0629 16:47:25.580458  8020 net.cpp:322] out3_out5_combined needs backward computation.
I0629 16:47:25.580462  8020 net.cpp:322] out3a/relu needs backward computation.
I0629 16:47:25.580466  8020 net.cpp:322] out3a/bn needs backward computation.
I0629 16:47:25.580471  8020 net.cpp:322] out3a needs backward computation.
I0629 16:47:25.580474  8020 net.cpp:322] out5a_up2 needs backward computation.
I0629 16:47:25.580478  8020 net.cpp:322] out5a/relu needs backward computation.
I0629 16:47:25.580482  8020 net.cpp:322] out5a/bn needs backward computation.
I0629 16:47:25.580485  8020 net.cpp:322] out5a needs backward computation.
I0629 16:47:25.580489  8020 net.cpp:322] res5a_branch2b/relu needs backward computation.
I0629 16:47:25.580493  8020 net.cpp:322] res5a_branch2b/bn needs backward computation.
I0629 16:47:25.580497  8020 net.cpp:322] res5a_branch2b needs backward computation.
I0629 16:47:25.580500  8020 net.cpp:322] res5a_branch2a/relu needs backward computation.
I0629 16:47:25.580504  8020 net.cpp:322] res5a_branch2a/bn needs backward computation.
I0629 16:47:25.580508  8020 net.cpp:322] res5a_branch2a needs backward computation.
I0629 16:47:25.580513  8020 net.cpp:322] pool4 needs backward computation.
I0629 16:47:25.580516  8020 net.cpp:322] res4a_branch2b/relu needs backward computation.
I0629 16:47:25.580519  8020 net.cpp:322] res4a_branch2b/bn needs backward computation.
I0629 16:47:25.580523  8020 net.cpp:322] res4a_branch2b needs backward computation.
I0629 16:47:25.580528  8020 net.cpp:322] res4a_branch2a/relu needs backward computation.
I0629 16:47:25.580530  8020 net.cpp:322] res4a_branch2a/bn needs backward computation.
I0629 16:47:25.580534  8020 net.cpp:322] res4a_branch2a needs backward computation.
I0629 16:47:25.580538  8020 net.cpp:322] pool3 needs backward computation.
I0629 16:47:25.580543  8020 net.cpp:322] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I0629 16:47:25.580546  8020 net.cpp:322] res3a_branch2b/relu needs backward computation.
I0629 16:47:25.580550  8020 net.cpp:322] res3a_branch2b/bn needs backward computation.
I0629 16:47:25.580554  8020 net.cpp:322] res3a_branch2b needs backward computation.
I0629 16:47:25.580559  8020 net.cpp:322] res3a_branch2a/relu needs backward computation.
I0629 16:47:25.580562  8020 net.cpp:322] res3a_branch2a/bn needs backward computation.
I0629 16:47:25.580565  8020 net.cpp:322] res3a_branch2a needs backward computation.
I0629 16:47:25.580569  8020 net.cpp:322] pool2 needs backward computation.
I0629 16:47:25.580574  8020 net.cpp:322] res2a_branch2b/relu needs backward computation.
I0629 16:47:25.580577  8020 net.cpp:322] res2a_branch2b/bn needs backward computation.
I0629 16:47:25.580580  8020 net.cpp:322] res2a_branch2b needs backward computation.
I0629 16:47:25.580585  8020 net.cpp:322] res2a_branch2a/relu needs backward computation.
I0629 16:47:25.580587  8020 net.cpp:322] res2a_branch2a/bn needs backward computation.
I0629 16:47:25.580591  8020 net.cpp:322] res2a_branch2a needs backward computation.
I0629 16:47:25.580595  8020 net.cpp:322] pool1 needs backward computation.
I0629 16:47:25.580600  8020 net.cpp:322] conv1b/relu needs backward computation.
I0629 16:47:25.580603  8020 net.cpp:322] conv1b/bn needs backward computation.
I0629 16:47:25.580606  8020 net.cpp:322] conv1b needs backward computation.
I0629 16:47:25.580610  8020 net.cpp:322] conv1a/relu needs backward computation.
I0629 16:47:25.580615  8020 net.cpp:322] conv1a/bn needs backward computation.
I0629 16:47:25.580617  8020 net.cpp:322] conv1a needs backward computation.
I0629 16:47:25.580622  8020 net.cpp:324] data/bias does not need backward computation.
I0629 16:47:25.580626  8020 net.cpp:324] data does not need backward computation.
I0629 16:47:25.580631  8020 net.cpp:366] This network produces output loss
I0629 16:47:25.580682  8020 net.cpp:388] Top memory (TRAIN) required for data: 959692800 diff: 1092403208
I0629 16:47:25.580685  8020 net.cpp:391] Bottom memory (TRAIN) required for data: 959692800 diff: 959692800
I0629 16:47:25.580693  8020 net.cpp:394] Shared (in-place) memory (TRAIN) by data: 632217600 diff: 632217600
I0629 16:47:25.580698  8020 net.cpp:397] Parameters memory (TRAIN) required for data: 2720256 diff: 2720256
I0629 16:47:25.580701  8020 net.cpp:400] Parameters shared memory (TRAIN) by data: 0 diff: 0
I0629 16:47:25.580704  8020 net.cpp:406] Network initialization done.
I0629 16:47:25.581234  8020 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cityscapes20_jsegnet21v2_2017-06-29_16-47-23/initial/test.prototxt
W0629 16:47:25.581301  8020 parallel.cpp:271] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0629 16:47:25.581481  8020 net.cpp:77] Initializing net from parameters: 
name: "jsegnet21v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/val-image-lmdb"
    label_list_path: "data/val-label-lmdb"
    batch_size: 2
    threads: 1
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 20
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 20
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 20
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
  accuracy_param {
    ignore_label: 255
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
    ignore_label: 255
  }
}
I0629 16:47:25.581621  8020 net.cpp:108] Using FLOAT as default forward math type
I0629 16:47:25.581630  8020 net.cpp:114] Using FLOAT as default backward math type
I0629 16:47:25.581634  8020 layer_factory.hpp:136] Creating layer 'data' of type 'ImageLabelData'
I0629 16:47:25.581638  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.581645  8020 net.cpp:183] Created Layer data (0)
I0629 16:47:25.581648  8020 net.cpp:529] data -> data
I0629 16:47:25.581653  8020 net.cpp:529] data -> label
I0629 16:47:25.581673  8020 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0629 16:47:25.581681  8020 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0629 16:47:25.582437  8086 db_lmdb.cpp:35] Opened lmdb data/val-image-lmdb
I0629 16:47:25.583916  8020 data_layer.cpp:188] ReshapePrefetch 2, 3, 640, 640
I0629 16:47:25.583991  8020 data_layer.cpp:206] Output data size: 2, 3, 640, 640
I0629 16:47:25.584000  8020 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0629 16:47:25.584161  8020 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0629 16:47:25.584172  8020 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0629 16:47:25.585165  8087 data_layer.cpp:188] ReshapePrefetch 2, 3, 640, 640
I0629 16:47:25.585182  8087 data_layer.cpp:206] Output data size: 2, 3, 640, 640
I0629 16:47:25.589689  8087 data_layer.cpp:110] [0] Parser threads: 1
I0629 16:47:25.589705  8087 data_layer.cpp:112] [0] Transformer threads: 1
I0629 16:47:25.591572  8088 db_lmdb.cpp:35] Opened lmdb data/val-label-lmdb
I0629 16:47:25.592296  8020 data_layer.cpp:188] ReshapePrefetch 2, 1, 640, 640
I0629 16:47:25.592350  8020 data_layer.cpp:206] Output data size: 2, 1, 640, 640
I0629 16:47:25.592356  8020 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0629 16:47:25.592396  8020 net.cpp:244] Setting up data
I0629 16:47:25.592404  8020 net.cpp:251] TEST Top shape for layer 0 'data' 2 3 640 640 (2457600)
I0629 16:47:25.592416  8020 net.cpp:251] TEST Top shape for layer 0 'data' 2 1 640 640 (819200)
I0629 16:47:25.592423  8020 layer_factory.hpp:136] Creating layer 'label_data_1_split' of type 'Split'
I0629 16:47:25.592429  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.592439  8020 net.cpp:183] Created Layer label_data_1_split (1)
I0629 16:47:25.592442  8020 net.cpp:560] label_data_1_split <- label
I0629 16:47:25.592447  8020 net.cpp:529] label_data_1_split -> label_data_1_split_0
I0629 16:47:25.592453  8020 net.cpp:529] label_data_1_split -> label_data_1_split_1
I0629 16:47:25.592458  8020 net.cpp:529] label_data_1_split -> label_data_1_split_2
I0629 16:47:25.592536  8020 net.cpp:244] Setting up label_data_1_split
I0629 16:47:25.592542  8020 net.cpp:251] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0629 16:47:25.592547  8020 net.cpp:251] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0629 16:47:25.592552  8020 net.cpp:251] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0629 16:47:25.592557  8020 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0629 16:47:25.592561  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.592571  8020 net.cpp:183] Created Layer data/bias (2)
I0629 16:47:25.592573  8020 net.cpp:560] data/bias <- data
I0629 16:47:25.592577  8020 net.cpp:529] data/bias -> data/bias
I0629 16:47:25.593659  8089 data_layer.cpp:188] ReshapePrefetch 2, 1, 640, 640
I0629 16:47:25.593680  8089 data_layer.cpp:206] Output data size: 2, 1, 640, 640
I0629 16:47:25.595610  8020 net.cpp:244] Setting up data/bias
I0629 16:47:25.595625  8020 net.cpp:251] TEST Top shape for layer 2 'data/bias' 2 3 640 640 (2457600)
I0629 16:47:25.595638  8020 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0629 16:47:25.595643  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.595659  8020 net.cpp:183] Created Layer conv1a (3)
I0629 16:47:25.595670  8020 net.cpp:560] conv1a <- data/bias
I0629 16:47:25.595676  8020 net.cpp:529] conv1a -> conv1a
I0629 16:47:25.596688  8089 data_layer.cpp:110] [0] Parser threads: 1
I0629 16:47:25.596694  8089 data_layer.cpp:112] [0] Transformer threads: 1
I0629 16:47:25.598630  8020 net.cpp:244] Setting up conv1a
I0629 16:47:25.598639  8020 net.cpp:251] TEST Top shape for layer 3 'conv1a' 2 32 320 320 (6553600)
I0629 16:47:25.598646  8020 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0629 16:47:25.598649  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.598655  8020 net.cpp:183] Created Layer conv1a/bn (4)
I0629 16:47:25.598659  8020 net.cpp:560] conv1a/bn <- conv1a
I0629 16:47:25.598661  8020 net.cpp:512] conv1a/bn -> conv1a (in-place)
I0629 16:47:25.600317  8020 net.cpp:244] Setting up conv1a/bn
I0629 16:47:25.600327  8020 net.cpp:251] TEST Top shape for layer 4 'conv1a/bn' 2 32 320 320 (6553600)
I0629 16:47:25.600334  8020 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0629 16:47:25.600337  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.600342  8020 net.cpp:183] Created Layer conv1a/relu (5)
I0629 16:47:25.600343  8020 net.cpp:560] conv1a/relu <- conv1a
I0629 16:47:25.600345  8020 net.cpp:512] conv1a/relu -> conv1a (in-place)
I0629 16:47:25.600350  8020 net.cpp:244] Setting up conv1a/relu
I0629 16:47:25.600353  8020 net.cpp:251] TEST Top shape for layer 5 'conv1a/relu' 2 32 320 320 (6553600)
I0629 16:47:25.600355  8020 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0629 16:47:25.600358  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.600363  8020 net.cpp:183] Created Layer conv1b (6)
I0629 16:47:25.600365  8020 net.cpp:560] conv1b <- conv1a
I0629 16:47:25.600368  8020 net.cpp:529] conv1b -> conv1b
I0629 16:47:25.612630  8020 net.cpp:244] Setting up conv1b
I0629 16:47:25.612651  8020 net.cpp:251] TEST Top shape for layer 6 'conv1b' 2 32 320 320 (6553600)
I0629 16:47:25.612663  8020 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0629 16:47:25.612666  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.612675  8020 net.cpp:183] Created Layer conv1b/bn (7)
I0629 16:47:25.612679  8020 net.cpp:560] conv1b/bn <- conv1b
I0629 16:47:25.612681  8020 net.cpp:512] conv1b/bn -> conv1b (in-place)
I0629 16:47:25.614112  8020 net.cpp:244] Setting up conv1b/bn
I0629 16:47:25.614121  8020 net.cpp:251] TEST Top shape for layer 7 'conv1b/bn' 2 32 320 320 (6553600)
I0629 16:47:25.614130  8020 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0629 16:47:25.614133  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.614142  8020 net.cpp:183] Created Layer conv1b/relu (8)
I0629 16:47:25.614145  8020 net.cpp:560] conv1b/relu <- conv1b
I0629 16:47:25.614148  8020 net.cpp:512] conv1b/relu -> conv1b (in-place)
I0629 16:47:25.614153  8020 net.cpp:244] Setting up conv1b/relu
I0629 16:47:25.614156  8020 net.cpp:251] TEST Top shape for layer 8 'conv1b/relu' 2 32 320 320 (6553600)
I0629 16:47:25.614158  8020 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0629 16:47:25.614161  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.614166  8020 net.cpp:183] Created Layer pool1 (9)
I0629 16:47:25.614168  8020 net.cpp:560] pool1 <- conv1b
I0629 16:47:25.614171  8020 net.cpp:529] pool1 -> pool1
I0629 16:47:25.614251  8020 net.cpp:244] Setting up pool1
I0629 16:47:25.614258  8020 net.cpp:251] TEST Top shape for layer 9 'pool1' 2 32 160 160 (1638400)
I0629 16:47:25.614260  8020 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0629 16:47:25.614264  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.614279  8020 net.cpp:183] Created Layer res2a_branch2a (10)
I0629 16:47:25.614284  8020 net.cpp:560] res2a_branch2a <- pool1
I0629 16:47:25.614285  8020 net.cpp:529] res2a_branch2a -> res2a_branch2a
I0629 16:47:25.622470  8020 net.cpp:244] Setting up res2a_branch2a
I0629 16:47:25.622483  8020 net.cpp:251] TEST Top shape for layer 10 'res2a_branch2a' 2 64 160 160 (3276800)
I0629 16:47:25.622488  8020 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0629 16:47:25.622491  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.622501  8020 net.cpp:183] Created Layer res2a_branch2a/bn (11)
I0629 16:47:25.622504  8020 net.cpp:560] res2a_branch2a/bn <- res2a_branch2a
I0629 16:47:25.622508  8020 net.cpp:512] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0629 16:47:25.623728  8020 net.cpp:244] Setting up res2a_branch2a/bn
I0629 16:47:25.623738  8020 net.cpp:251] TEST Top shape for layer 11 'res2a_branch2a/bn' 2 64 160 160 (3276800)
I0629 16:47:25.623744  8020 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0629 16:47:25.623746  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.623749  8020 net.cpp:183] Created Layer res2a_branch2a/relu (12)
I0629 16:47:25.623751  8020 net.cpp:560] res2a_branch2a/relu <- res2a_branch2a
I0629 16:47:25.623754  8020 net.cpp:512] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0629 16:47:25.623759  8020 net.cpp:244] Setting up res2a_branch2a/relu
I0629 16:47:25.623760  8020 net.cpp:251] TEST Top shape for layer 12 'res2a_branch2a/relu' 2 64 160 160 (3276800)
I0629 16:47:25.623762  8020 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0629 16:47:25.623764  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.623771  8020 net.cpp:183] Created Layer res2a_branch2b (13)
I0629 16:47:25.623775  8020 net.cpp:560] res2a_branch2b <- res2a_branch2a
I0629 16:47:25.623777  8020 net.cpp:529] res2a_branch2b -> res2a_branch2b
I0629 16:47:25.630286  8020 net.cpp:244] Setting up res2a_branch2b
I0629 16:47:25.630297  8020 net.cpp:251] TEST Top shape for layer 13 'res2a_branch2b' 2 64 160 160 (3276800)
I0629 16:47:25.630302  8020 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0629 16:47:25.630306  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.630309  8020 net.cpp:183] Created Layer res2a_branch2b/bn (14)
I0629 16:47:25.630311  8020 net.cpp:560] res2a_branch2b/bn <- res2a_branch2b
I0629 16:47:25.630314  8020 net.cpp:512] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0629 16:47:25.631517  8020 net.cpp:244] Setting up res2a_branch2b/bn
I0629 16:47:25.631526  8020 net.cpp:251] TEST Top shape for layer 14 'res2a_branch2b/bn' 2 64 160 160 (3276800)
I0629 16:47:25.631532  8020 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0629 16:47:25.631536  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.631538  8020 net.cpp:183] Created Layer res2a_branch2b/relu (15)
I0629 16:47:25.631541  8020 net.cpp:560] res2a_branch2b/relu <- res2a_branch2b
I0629 16:47:25.631543  8020 net.cpp:512] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0629 16:47:25.631546  8020 net.cpp:244] Setting up res2a_branch2b/relu
I0629 16:47:25.631549  8020 net.cpp:251] TEST Top shape for layer 15 'res2a_branch2b/relu' 2 64 160 160 (3276800)
I0629 16:47:25.631551  8020 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0629 16:47:25.631553  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.631557  8020 net.cpp:183] Created Layer pool2 (16)
I0629 16:47:25.631559  8020 net.cpp:560] pool2 <- res2a_branch2b
I0629 16:47:25.631562  8020 net.cpp:529] pool2 -> pool2
I0629 16:47:25.631633  8020 net.cpp:244] Setting up pool2
I0629 16:47:25.631645  8020 net.cpp:251] TEST Top shape for layer 16 'pool2' 2 64 80 80 (819200)
I0629 16:47:25.631649  8020 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0629 16:47:25.631650  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.631660  8020 net.cpp:183] Created Layer res3a_branch2a (17)
I0629 16:47:25.631662  8020 net.cpp:560] res3a_branch2a <- pool2
I0629 16:47:25.631665  8020 net.cpp:529] res3a_branch2a -> res3a_branch2a
I0629 16:47:25.636353  8020 net.cpp:244] Setting up res3a_branch2a
I0629 16:47:25.636363  8020 net.cpp:251] TEST Top shape for layer 17 'res3a_branch2a' 2 128 80 80 (1638400)
I0629 16:47:25.636368  8020 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0629 16:47:25.636369  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.636374  8020 net.cpp:183] Created Layer res3a_branch2a/bn (18)
I0629 16:47:25.636378  8020 net.cpp:560] res3a_branch2a/bn <- res3a_branch2a
I0629 16:47:25.636381  8020 net.cpp:512] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0629 16:47:25.637768  8020 net.cpp:244] Setting up res3a_branch2a/bn
I0629 16:47:25.637778  8020 net.cpp:251] TEST Top shape for layer 18 'res3a_branch2a/bn' 2 128 80 80 (1638400)
I0629 16:47:25.637790  8020 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0629 16:47:25.637794  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.637799  8020 net.cpp:183] Created Layer res3a_branch2a/relu (19)
I0629 16:47:25.637802  8020 net.cpp:560] res3a_branch2a/relu <- res3a_branch2a
I0629 16:47:25.637806  8020 net.cpp:512] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0629 16:47:25.637811  8020 net.cpp:244] Setting up res3a_branch2a/relu
I0629 16:47:25.637817  8020 net.cpp:251] TEST Top shape for layer 19 'res3a_branch2a/relu' 2 128 80 80 (1638400)
I0629 16:47:25.637820  8020 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0629 16:47:25.637825  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.637833  8020 net.cpp:183] Created Layer res3a_branch2b (20)
I0629 16:47:25.637836  8020 net.cpp:560] res3a_branch2b <- res3a_branch2a
I0629 16:47:25.637840  8020 net.cpp:529] res3a_branch2b -> res3a_branch2b
I0629 16:47:25.641950  8020 net.cpp:244] Setting up res3a_branch2b
I0629 16:47:25.641964  8020 net.cpp:251] TEST Top shape for layer 20 'res3a_branch2b' 2 128 80 80 (1638400)
I0629 16:47:25.641970  8020 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0629 16:47:25.641973  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.641980  8020 net.cpp:183] Created Layer res3a_branch2b/bn (21)
I0629 16:47:25.641983  8020 net.cpp:560] res3a_branch2b/bn <- res3a_branch2b
I0629 16:47:25.641985  8020 net.cpp:512] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0629 16:47:25.643214  8020 net.cpp:244] Setting up res3a_branch2b/bn
I0629 16:47:25.643225  8020 net.cpp:251] TEST Top shape for layer 21 'res3a_branch2b/bn' 2 128 80 80 (1638400)
I0629 16:47:25.643232  8020 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0629 16:47:25.643235  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.643239  8020 net.cpp:183] Created Layer res3a_branch2b/relu (22)
I0629 16:47:25.643244  8020 net.cpp:560] res3a_branch2b/relu <- res3a_branch2b
I0629 16:47:25.643246  8020 net.cpp:512] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0629 16:47:25.643251  8020 net.cpp:244] Setting up res3a_branch2b/relu
I0629 16:47:25.643254  8020 net.cpp:251] TEST Top shape for layer 22 'res3a_branch2b/relu' 2 128 80 80 (1638400)
I0629 16:47:25.643257  8020 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0629 16:47:25.643260  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.643271  8020 net.cpp:183] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (23)
I0629 16:47:25.643275  8020 net.cpp:560] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0629 16:47:25.643276  8020 net.cpp:529] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0629 16:47:25.643280  8020 net.cpp:529] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0629 16:47:25.643334  8020 net.cpp:244] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0629 16:47:25.643338  8020 net.cpp:251] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 2 128 80 80 (1638400)
I0629 16:47:25.643342  8020 net.cpp:251] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 2 128 80 80 (1638400)
I0629 16:47:25.643344  8020 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0629 16:47:25.643348  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.643352  8020 net.cpp:183] Created Layer pool3 (24)
I0629 16:47:25.643355  8020 net.cpp:560] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0629 16:47:25.643357  8020 net.cpp:529] pool3 -> pool3
I0629 16:47:25.643429  8020 net.cpp:244] Setting up pool3
I0629 16:47:25.643435  8020 net.cpp:251] TEST Top shape for layer 24 'pool3' 2 128 40 40 (409600)
I0629 16:47:25.643437  8020 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0629 16:47:25.643440  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.643446  8020 net.cpp:183] Created Layer res4a_branch2a (25)
I0629 16:47:25.643450  8020 net.cpp:560] res4a_branch2a <- pool3
I0629 16:47:25.643452  8020 net.cpp:529] res4a_branch2a -> res4a_branch2a
I0629 16:47:25.654320  8020 net.cpp:244] Setting up res4a_branch2a
I0629 16:47:25.654330  8020 net.cpp:251] TEST Top shape for layer 25 'res4a_branch2a' 2 256 40 40 (819200)
I0629 16:47:25.654335  8020 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0629 16:47:25.654338  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.654348  8020 net.cpp:183] Created Layer res4a_branch2a/bn (26)
I0629 16:47:25.654351  8020 net.cpp:560] res4a_branch2a/bn <- res4a_branch2a
I0629 16:47:25.654355  8020 net.cpp:512] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0629 16:47:25.655515  8020 net.cpp:244] Setting up res4a_branch2a/bn
I0629 16:47:25.655524  8020 net.cpp:251] TEST Top shape for layer 26 'res4a_branch2a/bn' 2 256 40 40 (819200)
I0629 16:47:25.655531  8020 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0629 16:47:25.655534  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.655537  8020 net.cpp:183] Created Layer res4a_branch2a/relu (27)
I0629 16:47:25.655539  8020 net.cpp:560] res4a_branch2a/relu <- res4a_branch2a
I0629 16:47:25.655541  8020 net.cpp:512] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0629 16:47:25.655550  8020 net.cpp:244] Setting up res4a_branch2a/relu
I0629 16:47:25.655553  8020 net.cpp:251] TEST Top shape for layer 27 'res4a_branch2a/relu' 2 256 40 40 (819200)
I0629 16:47:25.655555  8020 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0629 16:47:25.655558  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.655565  8020 net.cpp:183] Created Layer res4a_branch2b (28)
I0629 16:47:25.655567  8020 net.cpp:560] res4a_branch2b <- res4a_branch2a
I0629 16:47:25.655570  8020 net.cpp:529] res4a_branch2b -> res4a_branch2b
I0629 16:47:25.661154  8020 net.cpp:244] Setting up res4a_branch2b
I0629 16:47:25.661164  8020 net.cpp:251] TEST Top shape for layer 28 'res4a_branch2b' 2 256 40 40 (819200)
I0629 16:47:25.661168  8020 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0629 16:47:25.661180  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.661186  8020 net.cpp:183] Created Layer res4a_branch2b/bn (29)
I0629 16:47:25.661192  8020 net.cpp:560] res4a_branch2b/bn <- res4a_branch2b
I0629 16:47:25.661196  8020 net.cpp:512] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0629 16:47:25.662365  8020 net.cpp:244] Setting up res4a_branch2b/bn
I0629 16:47:25.662374  8020 net.cpp:251] TEST Top shape for layer 29 'res4a_branch2b/bn' 2 256 40 40 (819200)
I0629 16:47:25.662380  8020 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0629 16:47:25.662384  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.662386  8020 net.cpp:183] Created Layer res4a_branch2b/relu (30)
I0629 16:47:25.662389  8020 net.cpp:560] res4a_branch2b/relu <- res4a_branch2b
I0629 16:47:25.662390  8020 net.cpp:512] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0629 16:47:25.662395  8020 net.cpp:244] Setting up res4a_branch2b/relu
I0629 16:47:25.662397  8020 net.cpp:251] TEST Top shape for layer 30 'res4a_branch2b/relu' 2 256 40 40 (819200)
I0629 16:47:25.662400  8020 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0629 16:47:25.662401  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.662405  8020 net.cpp:183] Created Layer pool4 (31)
I0629 16:47:25.662408  8020 net.cpp:560] pool4 <- res4a_branch2b
I0629 16:47:25.662411  8020 net.cpp:529] pool4 -> pool4
I0629 16:47:25.662492  8020 net.cpp:244] Setting up pool4
I0629 16:47:25.662497  8020 net.cpp:251] TEST Top shape for layer 31 'pool4' 2 256 40 40 (819200)
I0629 16:47:25.662498  8020 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0629 16:47:25.662502  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.662511  8020 net.cpp:183] Created Layer res5a_branch2a (32)
I0629 16:47:25.662514  8020 net.cpp:560] res5a_branch2a <- pool4
I0629 16:47:25.662516  8020 net.cpp:529] res5a_branch2a -> res5a_branch2a
I0629 16:47:25.687732  8020 net.cpp:244] Setting up res5a_branch2a
I0629 16:47:25.687757  8020 net.cpp:251] TEST Top shape for layer 32 'res5a_branch2a' 2 512 40 40 (1638400)
I0629 16:47:25.687763  8020 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0629 16:47:25.687767  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.687773  8020 net.cpp:183] Created Layer res5a_branch2a/bn (33)
I0629 16:47:25.687777  8020 net.cpp:560] res5a_branch2a/bn <- res5a_branch2a
I0629 16:47:25.687780  8020 net.cpp:512] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0629 16:47:25.689045  8020 net.cpp:244] Setting up res5a_branch2a/bn
I0629 16:47:25.689055  8020 net.cpp:251] TEST Top shape for layer 33 'res5a_branch2a/bn' 2 512 40 40 (1638400)
I0629 16:47:25.689061  8020 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0629 16:47:25.689064  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.689067  8020 net.cpp:183] Created Layer res5a_branch2a/relu (34)
I0629 16:47:25.689070  8020 net.cpp:560] res5a_branch2a/relu <- res5a_branch2a
I0629 16:47:25.689072  8020 net.cpp:512] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0629 16:47:25.689077  8020 net.cpp:244] Setting up res5a_branch2a/relu
I0629 16:47:25.689079  8020 net.cpp:251] TEST Top shape for layer 34 'res5a_branch2a/relu' 2 512 40 40 (1638400)
I0629 16:47:25.689081  8020 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0629 16:47:25.689083  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.689097  8020 net.cpp:183] Created Layer res5a_branch2b (35)
I0629 16:47:25.689100  8020 net.cpp:560] res5a_branch2b <- res5a_branch2a
I0629 16:47:25.689103  8020 net.cpp:529] res5a_branch2b -> res5a_branch2b
I0629 16:47:25.702345  8020 net.cpp:244] Setting up res5a_branch2b
I0629 16:47:25.702369  8020 net.cpp:251] TEST Top shape for layer 35 'res5a_branch2b' 2 512 40 40 (1638400)
I0629 16:47:25.702380  8020 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0629 16:47:25.702384  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.702392  8020 net.cpp:183] Created Layer res5a_branch2b/bn (36)
I0629 16:47:25.702396  8020 net.cpp:560] res5a_branch2b/bn <- res5a_branch2b
I0629 16:47:25.702399  8020 net.cpp:512] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0629 16:47:25.703645  8020 net.cpp:244] Setting up res5a_branch2b/bn
I0629 16:47:25.703656  8020 net.cpp:251] TEST Top shape for layer 36 'res5a_branch2b/bn' 2 512 40 40 (1638400)
I0629 16:47:25.703663  8020 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0629 16:47:25.703666  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.703671  8020 net.cpp:183] Created Layer res5a_branch2b/relu (37)
I0629 16:47:25.703675  8020 net.cpp:560] res5a_branch2b/relu <- res5a_branch2b
I0629 16:47:25.703677  8020 net.cpp:512] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0629 16:47:25.703682  8020 net.cpp:244] Setting up res5a_branch2b/relu
I0629 16:47:25.703686  8020 net.cpp:251] TEST Top shape for layer 37 'res5a_branch2b/relu' 2 512 40 40 (1638400)
I0629 16:47:25.703688  8020 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I0629 16:47:25.703691  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.703781  8020 net.cpp:183] Created Layer out5a (38)
I0629 16:47:25.703786  8020 net.cpp:560] out5a <- res5a_branch2b
I0629 16:47:25.703789  8020 net.cpp:529] out5a -> out5a
I0629 16:47:25.707132  8020 net.cpp:244] Setting up out5a
I0629 16:47:25.707141  8020 net.cpp:251] TEST Top shape for layer 38 'out5a' 2 64 40 40 (204800)
I0629 16:47:25.707146  8020 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I0629 16:47:25.707149  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.707154  8020 net.cpp:183] Created Layer out5a/bn (39)
I0629 16:47:25.707157  8020 net.cpp:560] out5a/bn <- out5a
I0629 16:47:25.707160  8020 net.cpp:512] out5a/bn -> out5a (in-place)
I0629 16:47:25.707909  8020 net.cpp:244] Setting up out5a/bn
I0629 16:47:25.707916  8020 net.cpp:251] TEST Top shape for layer 39 'out5a/bn' 2 64 40 40 (204800)
I0629 16:47:25.707923  8020 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I0629 16:47:25.707926  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.707931  8020 net.cpp:183] Created Layer out5a/relu (40)
I0629 16:47:25.707932  8020 net.cpp:560] out5a/relu <- out5a
I0629 16:47:25.707937  8020 net.cpp:512] out5a/relu -> out5a (in-place)
I0629 16:47:25.707940  8020 net.cpp:244] Setting up out5a/relu
I0629 16:47:25.707943  8020 net.cpp:251] TEST Top shape for layer 40 'out5a/relu' 2 64 40 40 (204800)
I0629 16:47:25.707945  8020 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I0629 16:47:25.707948  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.707953  8020 net.cpp:183] Created Layer out5a_up2 (41)
I0629 16:47:25.707957  8020 net.cpp:560] out5a_up2 <- out5a
I0629 16:47:25.707958  8020 net.cpp:529] out5a_up2 -> out5a_up2
I0629 16:47:25.708266  8020 net.cpp:244] Setting up out5a_up2
I0629 16:47:25.708272  8020 net.cpp:251] TEST Top shape for layer 41 'out5a_up2' 2 64 80 80 (819200)
I0629 16:47:25.708276  8020 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I0629 16:47:25.708279  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.708286  8020 net.cpp:183] Created Layer out3a (42)
I0629 16:47:25.708297  8020 net.cpp:560] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0629 16:47:25.708299  8020 net.cpp:529] out3a -> out3a
I0629 16:47:25.711606  8020 net.cpp:244] Setting up out3a
I0629 16:47:25.711617  8020 net.cpp:251] TEST Top shape for layer 42 'out3a' 2 64 80 80 (819200)
I0629 16:47:25.711622  8020 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I0629 16:47:25.711625  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.711632  8020 net.cpp:183] Created Layer out3a/bn (43)
I0629 16:47:25.711634  8020 net.cpp:560] out3a/bn <- out3a
I0629 16:47:25.711637  8020 net.cpp:512] out3a/bn -> out3a (in-place)
I0629 16:47:25.712853  8020 net.cpp:244] Setting up out3a/bn
I0629 16:47:25.712862  8020 net.cpp:251] TEST Top shape for layer 43 'out3a/bn' 2 64 80 80 (819200)
I0629 16:47:25.712868  8020 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I0629 16:47:25.712872  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.712874  8020 net.cpp:183] Created Layer out3a/relu (44)
I0629 16:47:25.712877  8020 net.cpp:560] out3a/relu <- out3a
I0629 16:47:25.712879  8020 net.cpp:512] out3a/relu -> out3a (in-place)
I0629 16:47:25.712882  8020 net.cpp:244] Setting up out3a/relu
I0629 16:47:25.712885  8020 net.cpp:251] TEST Top shape for layer 44 'out3a/relu' 2 64 80 80 (819200)
I0629 16:47:25.712888  8020 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I0629 16:47:25.712889  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.712893  8020 net.cpp:183] Created Layer out3_out5_combined (45)
I0629 16:47:25.712895  8020 net.cpp:560] out3_out5_combined <- out5a_up2
I0629 16:47:25.712898  8020 net.cpp:560] out3_out5_combined <- out3a
I0629 16:47:25.712901  8020 net.cpp:529] out3_out5_combined -> out3_out5_combined
I0629 16:47:25.713824  8020 net.cpp:244] Setting up out3_out5_combined
I0629 16:47:25.713834  8020 net.cpp:251] TEST Top shape for layer 45 'out3_out5_combined' 2 64 80 80 (819200)
I0629 16:47:25.713836  8020 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I0629 16:47:25.713838  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.713845  8020 net.cpp:183] Created Layer ctx_conv1 (46)
I0629 16:47:25.713847  8020 net.cpp:560] ctx_conv1 <- out3_out5_combined
I0629 16:47:25.713850  8020 net.cpp:529] ctx_conv1 -> ctx_conv1
I0629 16:47:25.716855  8020 net.cpp:244] Setting up ctx_conv1
I0629 16:47:25.716862  8020 net.cpp:251] TEST Top shape for layer 46 'ctx_conv1' 2 64 80 80 (819200)
I0629 16:47:25.716867  8020 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I0629 16:47:25.716871  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.716878  8020 net.cpp:183] Created Layer ctx_conv1/bn (47)
I0629 16:47:25.716881  8020 net.cpp:560] ctx_conv1/bn <- ctx_conv1
I0629 16:47:25.716882  8020 net.cpp:512] ctx_conv1/bn -> ctx_conv1 (in-place)
I0629 16:47:25.718076  8020 net.cpp:244] Setting up ctx_conv1/bn
I0629 16:47:25.718086  8020 net.cpp:251] TEST Top shape for layer 47 'ctx_conv1/bn' 2 64 80 80 (819200)
I0629 16:47:25.718091  8020 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I0629 16:47:25.718093  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.718096  8020 net.cpp:183] Created Layer ctx_conv1/relu (48)
I0629 16:47:25.718098  8020 net.cpp:560] ctx_conv1/relu <- ctx_conv1
I0629 16:47:25.718101  8020 net.cpp:512] ctx_conv1/relu -> ctx_conv1 (in-place)
I0629 16:47:25.718106  8020 net.cpp:244] Setting up ctx_conv1/relu
I0629 16:47:25.718108  8020 net.cpp:251] TEST Top shape for layer 48 'ctx_conv1/relu' 2 64 80 80 (819200)
I0629 16:47:25.718109  8020 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I0629 16:47:25.718111  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.718125  8020 net.cpp:183] Created Layer ctx_conv2 (49)
I0629 16:47:25.718128  8020 net.cpp:560] ctx_conv2 <- ctx_conv1
I0629 16:47:25.718130  8020 net.cpp:529] ctx_conv2 -> ctx_conv2
I0629 16:47:25.719267  8020 net.cpp:244] Setting up ctx_conv2
I0629 16:47:25.719275  8020 net.cpp:251] TEST Top shape for layer 49 'ctx_conv2' 2 64 80 80 (819200)
I0629 16:47:25.719280  8020 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I0629 16:47:25.719282  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.719285  8020 net.cpp:183] Created Layer ctx_conv2/bn (50)
I0629 16:47:25.719287  8020 net.cpp:560] ctx_conv2/bn <- ctx_conv2
I0629 16:47:25.719290  8020 net.cpp:512] ctx_conv2/bn -> ctx_conv2 (in-place)
I0629 16:47:25.720435  8020 net.cpp:244] Setting up ctx_conv2/bn
I0629 16:47:25.720443  8020 net.cpp:251] TEST Top shape for layer 50 'ctx_conv2/bn' 2 64 80 80 (819200)
I0629 16:47:25.720449  8020 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I0629 16:47:25.720451  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.720454  8020 net.cpp:183] Created Layer ctx_conv2/relu (51)
I0629 16:47:25.720456  8020 net.cpp:560] ctx_conv2/relu <- ctx_conv2
I0629 16:47:25.720459  8020 net.cpp:512] ctx_conv2/relu -> ctx_conv2 (in-place)
I0629 16:47:25.720463  8020 net.cpp:244] Setting up ctx_conv2/relu
I0629 16:47:25.720465  8020 net.cpp:251] TEST Top shape for layer 51 'ctx_conv2/relu' 2 64 80 80 (819200)
I0629 16:47:25.720468  8020 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I0629 16:47:25.720469  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.720474  8020 net.cpp:183] Created Layer ctx_conv3 (52)
I0629 16:47:25.720476  8020 net.cpp:560] ctx_conv3 <- ctx_conv2
I0629 16:47:25.720479  8020 net.cpp:529] ctx_conv3 -> ctx_conv3
I0629 16:47:25.721606  8020 net.cpp:244] Setting up ctx_conv3
I0629 16:47:25.721613  8020 net.cpp:251] TEST Top shape for layer 52 'ctx_conv3' 2 64 80 80 (819200)
I0629 16:47:25.721617  8020 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I0629 16:47:25.721621  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.721623  8020 net.cpp:183] Created Layer ctx_conv3/bn (53)
I0629 16:47:25.721626  8020 net.cpp:560] ctx_conv3/bn <- ctx_conv3
I0629 16:47:25.721628  8020 net.cpp:512] ctx_conv3/bn -> ctx_conv3 (in-place)
I0629 16:47:25.722838  8020 net.cpp:244] Setting up ctx_conv3/bn
I0629 16:47:25.722846  8020 net.cpp:251] TEST Top shape for layer 53 'ctx_conv3/bn' 2 64 80 80 (819200)
I0629 16:47:25.722852  8020 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I0629 16:47:25.722854  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.722857  8020 net.cpp:183] Created Layer ctx_conv3/relu (54)
I0629 16:47:25.722860  8020 net.cpp:560] ctx_conv3/relu <- ctx_conv3
I0629 16:47:25.722862  8020 net.cpp:512] ctx_conv3/relu -> ctx_conv3 (in-place)
I0629 16:47:25.722867  8020 net.cpp:244] Setting up ctx_conv3/relu
I0629 16:47:25.722868  8020 net.cpp:251] TEST Top shape for layer 54 'ctx_conv3/relu' 2 64 80 80 (819200)
I0629 16:47:25.722870  8020 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I0629 16:47:25.722872  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.722877  8020 net.cpp:183] Created Layer ctx_conv4 (55)
I0629 16:47:25.722879  8020 net.cpp:560] ctx_conv4 <- ctx_conv3
I0629 16:47:25.722882  8020 net.cpp:529] ctx_conv4 -> ctx_conv4
I0629 16:47:25.724004  8020 net.cpp:244] Setting up ctx_conv4
I0629 16:47:25.724011  8020 net.cpp:251] TEST Top shape for layer 55 'ctx_conv4' 2 64 80 80 (819200)
I0629 16:47:25.724015  8020 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I0629 16:47:25.724025  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.724028  8020 net.cpp:183] Created Layer ctx_conv4/bn (56)
I0629 16:47:25.724031  8020 net.cpp:560] ctx_conv4/bn <- ctx_conv4
I0629 16:47:25.724033  8020 net.cpp:512] ctx_conv4/bn -> ctx_conv4 (in-place)
I0629 16:47:25.725183  8020 net.cpp:244] Setting up ctx_conv4/bn
I0629 16:47:25.725190  8020 net.cpp:251] TEST Top shape for layer 56 'ctx_conv4/bn' 2 64 80 80 (819200)
I0629 16:47:25.725195  8020 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I0629 16:47:25.725198  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.725201  8020 net.cpp:183] Created Layer ctx_conv4/relu (57)
I0629 16:47:25.725203  8020 net.cpp:560] ctx_conv4/relu <- ctx_conv4
I0629 16:47:25.725205  8020 net.cpp:512] ctx_conv4/relu -> ctx_conv4 (in-place)
I0629 16:47:25.725209  8020 net.cpp:244] Setting up ctx_conv4/relu
I0629 16:47:25.725213  8020 net.cpp:251] TEST Top shape for layer 57 'ctx_conv4/relu' 2 64 80 80 (819200)
I0629 16:47:25.725214  8020 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I0629 16:47:25.725216  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.725221  8020 net.cpp:183] Created Layer ctx_final (58)
I0629 16:47:25.725224  8020 net.cpp:560] ctx_final <- ctx_conv4
I0629 16:47:25.725226  8020 net.cpp:529] ctx_final -> ctx_final
I0629 16:47:25.729092  8020 net.cpp:244] Setting up ctx_final
I0629 16:47:25.729104  8020 net.cpp:251] TEST Top shape for layer 58 'ctx_final' 2 20 80 80 (256000)
I0629 16:47:25.729110  8020 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I0629 16:47:25.729116  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.729122  8020 net.cpp:183] Created Layer ctx_final/relu (59)
I0629 16:47:25.729127  8020 net.cpp:560] ctx_final/relu <- ctx_final
I0629 16:47:25.729131  8020 net.cpp:512] ctx_final/relu -> ctx_final (in-place)
I0629 16:47:25.729138  8020 net.cpp:244] Setting up ctx_final/relu
I0629 16:47:25.729143  8020 net.cpp:251] TEST Top shape for layer 59 'ctx_final/relu' 2 20 80 80 (256000)
I0629 16:47:25.729156  8020 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I0629 16:47:25.729161  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.729174  8020 net.cpp:183] Created Layer out_deconv_final_up2 (60)
I0629 16:47:25.729178  8020 net.cpp:560] out_deconv_final_up2 <- ctx_final
I0629 16:47:25.729182  8020 net.cpp:529] out_deconv_final_up2 -> out_deconv_final_up2
I0629 16:47:25.729557  8020 net.cpp:244] Setting up out_deconv_final_up2
I0629 16:47:25.729565  8020 net.cpp:251] TEST Top shape for layer 60 'out_deconv_final_up2' 2 20 160 160 (1024000)
I0629 16:47:25.729570  8020 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I0629 16:47:25.729574  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.729583  8020 net.cpp:183] Created Layer out_deconv_final_up4 (61)
I0629 16:47:25.729588  8020 net.cpp:560] out_deconv_final_up4 <- out_deconv_final_up2
I0629 16:47:25.729593  8020 net.cpp:529] out_deconv_final_up4 -> out_deconv_final_up4
I0629 16:47:25.729919  8020 net.cpp:244] Setting up out_deconv_final_up4
I0629 16:47:25.729926  8020 net.cpp:251] TEST Top shape for layer 61 'out_deconv_final_up4' 2 20 320 320 (4096000)
I0629 16:47:25.729933  8020 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I0629 16:47:25.729938  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.729953  8020 net.cpp:183] Created Layer out_deconv_final_up8 (62)
I0629 16:47:25.729956  8020 net.cpp:560] out_deconv_final_up8 <- out_deconv_final_up4
I0629 16:47:25.729959  8020 net.cpp:529] out_deconv_final_up8 -> out_deconv_final_up8
I0629 16:47:25.730296  8020 net.cpp:244] Setting up out_deconv_final_up8
I0629 16:47:25.730304  8020 net.cpp:251] TEST Top shape for layer 62 'out_deconv_final_up8' 2 20 640 640 (16384000)
I0629 16:47:25.730309  8020 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8_out_deconv_final_up8_0_split' of type 'Split'
I0629 16:47:25.730314  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.730319  8020 net.cpp:183] Created Layer out_deconv_final_up8_out_deconv_final_up8_0_split (63)
I0629 16:47:25.730324  8020 net.cpp:560] out_deconv_final_up8_out_deconv_final_up8_0_split <- out_deconv_final_up8
I0629 16:47:25.730329  8020 net.cpp:529] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0629 16:47:25.730336  8020 net.cpp:529] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0629 16:47:25.730342  8020 net.cpp:529] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0629 16:47:25.730412  8020 net.cpp:244] Setting up out_deconv_final_up8_out_deconv_final_up8_0_split
I0629 16:47:25.730417  8020 net.cpp:251] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 20 640 640 (16384000)
I0629 16:47:25.730422  8020 net.cpp:251] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 20 640 640 (16384000)
I0629 16:47:25.730427  8020 net.cpp:251] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 20 640 640 (16384000)
I0629 16:47:25.730432  8020 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0629 16:47:25.730437  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.730444  8020 net.cpp:183] Created Layer loss (64)
I0629 16:47:25.730448  8020 net.cpp:560] loss <- out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0629 16:47:25.730453  8020 net.cpp:560] loss <- label_data_1_split_0
I0629 16:47:25.730458  8020 net.cpp:529] loss -> loss
I0629 16:47:25.731719  8020 net.cpp:244] Setting up loss
I0629 16:47:25.731729  8020 net.cpp:251] TEST Top shape for layer 64 'loss' (1)
I0629 16:47:25.731734  8020 net.cpp:255]     with loss weight 1
I0629 16:47:25.731742  8020 layer_factory.hpp:136] Creating layer 'accuracy/top1' of type 'Accuracy'
I0629 16:47:25.731747  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.731758  8020 net.cpp:183] Created Layer accuracy/top1 (65)
I0629 16:47:25.731761  8020 net.cpp:560] accuracy/top1 <- out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0629 16:47:25.731766  8020 net.cpp:560] accuracy/top1 <- label_data_1_split_1
I0629 16:47:25.731772  8020 net.cpp:529] accuracy/top1 -> accuracy/top1
I0629 16:47:25.731781  8020 net.cpp:244] Setting up accuracy/top1
I0629 16:47:25.731784  8020 net.cpp:251] TEST Top shape for layer 65 'accuracy/top1' (1)
I0629 16:47:25.731789  8020 layer_factory.hpp:136] Creating layer 'accuracy/top5' of type 'Accuracy'
I0629 16:47:25.731793  8020 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 16:47:25.731798  8020 net.cpp:183] Created Layer accuracy/top5 (66)
I0629 16:47:25.731803  8020 net.cpp:560] accuracy/top5 <- out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0629 16:47:25.731807  8020 net.cpp:560] accuracy/top5 <- label_data_1_split_2
I0629 16:47:25.731812  8020 net.cpp:529] accuracy/top5 -> accuracy/top5
I0629 16:47:25.731818  8020 net.cpp:244] Setting up accuracy/top5
I0629 16:47:25.731823  8020 net.cpp:251] TEST Top shape for layer 66 'accuracy/top5' (1)
I0629 16:47:25.731828  8020 net.cpp:324] accuracy/top5 does not need backward computation.
I0629 16:47:25.731832  8020 net.cpp:324] accuracy/top1 does not need backward computation.
I0629 16:47:25.731837  8020 net.cpp:322] loss needs backward computation.
I0629 16:47:25.731840  8020 net.cpp:322] out_deconv_final_up8_out_deconv_final_up8_0_split needs backward computation.
I0629 16:47:25.731850  8020 net.cpp:322] out_deconv_final_up8 needs backward computation.
I0629 16:47:25.731854  8020 net.cpp:322] out_deconv_final_up4 needs backward computation.
I0629 16:47:25.731859  8020 net.cpp:322] out_deconv_final_up2 needs backward computation.
I0629 16:47:25.731863  8020 net.cpp:322] ctx_final/relu needs backward computation.
I0629 16:47:25.731868  8020 net.cpp:322] ctx_final needs backward computation.
I0629 16:47:25.731871  8020 net.cpp:322] ctx_conv4/relu needs backward computation.
I0629 16:47:25.731875  8020 net.cpp:322] ctx_conv4/bn needs backward computation.
I0629 16:47:25.731879  8020 net.cpp:322] ctx_conv4 needs backward computation.
I0629 16:47:25.731884  8020 net.cpp:322] ctx_conv3/relu needs backward computation.
I0629 16:47:25.731887  8020 net.cpp:322] ctx_conv3/bn needs backward computation.
I0629 16:47:25.731891  8020 net.cpp:322] ctx_conv3 needs backward computation.
I0629 16:47:25.731895  8020 net.cpp:322] ctx_conv2/relu needs backward computation.
I0629 16:47:25.731899  8020 net.cpp:322] ctx_conv2/bn needs backward computation.
I0629 16:47:25.731904  8020 net.cpp:322] ctx_conv2 needs backward computation.
I0629 16:47:25.731907  8020 net.cpp:322] ctx_conv1/relu needs backward computation.
I0629 16:47:25.731911  8020 net.cpp:322] ctx_conv1/bn needs backward computation.
I0629 16:47:25.731915  8020 net.cpp:322] ctx_conv1 needs backward computation.
I0629 16:47:25.731920  8020 net.cpp:322] out3_out5_combined needs backward computation.
I0629 16:47:25.731925  8020 net.cpp:322] out3a/relu needs backward computation.
I0629 16:47:25.731928  8020 net.cpp:322] out3a/bn needs backward computation.
I0629 16:47:25.731931  8020 net.cpp:322] out3a needs backward computation.
I0629 16:47:25.731936  8020 net.cpp:322] out5a_up2 needs backward computation.
I0629 16:47:25.731940  8020 net.cpp:322] out5a/relu needs backward computation.
I0629 16:47:25.731945  8020 net.cpp:322] out5a/bn needs backward computation.
I0629 16:47:25.731948  8020 net.cpp:322] out5a needs backward computation.
I0629 16:47:25.731952  8020 net.cpp:322] res5a_branch2b/relu needs backward computation.
I0629 16:47:25.731956  8020 net.cpp:322] res5a_branch2b/bn needs backward computation.
I0629 16:47:25.731961  8020 net.cpp:322] res5a_branch2b needs backward computation.
I0629 16:47:25.731966  8020 net.cpp:322] res5a_branch2a/relu needs backward computation.
I0629 16:47:25.731968  8020 net.cpp:322] res5a_branch2a/bn needs backward computation.
I0629 16:47:25.731972  8020 net.cpp:322] res5a_branch2a needs backward computation.
I0629 16:47:25.731976  8020 net.cpp:322] pool4 needs backward computation.
I0629 16:47:25.731981  8020 net.cpp:322] res4a_branch2b/relu needs backward computation.
I0629 16:47:25.731986  8020 net.cpp:322] res4a_branch2b/bn needs backward computation.
I0629 16:47:25.731989  8020 net.cpp:322] res4a_branch2b needs backward computation.
I0629 16:47:25.731993  8020 net.cpp:322] res4a_branch2a/relu needs backward computation.
I0629 16:47:25.731997  8020 net.cpp:322] res4a_branch2a/bn needs backward computation.
I0629 16:47:25.732002  8020 net.cpp:322] res4a_branch2a needs backward computation.
I0629 16:47:25.732005  8020 net.cpp:322] pool3 needs backward computation.
I0629 16:47:25.732010  8020 net.cpp:322] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I0629 16:47:25.732014  8020 net.cpp:322] res3a_branch2b/relu needs backward computation.
I0629 16:47:25.732018  8020 net.cpp:322] res3a_branch2b/bn needs backward computation.
I0629 16:47:25.732023  8020 net.cpp:322] res3a_branch2b needs backward computation.
I0629 16:47:25.732028  8020 net.cpp:322] res3a_branch2a/relu needs backward computation.
I0629 16:47:25.732031  8020 net.cpp:322] res3a_branch2a/bn needs backward computation.
I0629 16:47:25.732034  8020 net.cpp:322] res3a_branch2a needs backward computation.
I0629 16:47:25.732039  8020 net.cpp:322] pool2 needs backward computation.
I0629 16:47:25.732043  8020 net.cpp:322] res2a_branch2b/relu needs backward computation.
I0629 16:47:25.732050  8020 net.cpp:322] res2a_branch2b/bn needs backward computation.
I0629 16:47:25.732054  8020 net.cpp:322] res2a_branch2b needs backward computation.
I0629 16:47:25.732059  8020 net.cpp:322] res2a_branch2a/relu needs backward computation.
I0629 16:47:25.732064  8020 net.cpp:322] res2a_branch2a/bn needs backward computation.
I0629 16:47:25.732067  8020 net.cpp:322] res2a_branch2a needs backward computation.
I0629 16:47:25.732071  8020 net.cpp:322] pool1 needs backward computation.
I0629 16:47:25.732076  8020 net.cpp:322] conv1b/relu needs backward computation.
I0629 16:47:25.732080  8020 net.cpp:322] conv1b/bn needs backward computation.
I0629 16:47:25.732084  8020 net.cpp:322] conv1b needs backward computation.
I0629 16:47:25.732089  8020 net.cpp:322] conv1a/relu needs backward computation.
I0629 16:47:25.732092  8020 net.cpp:322] conv1a/bn needs backward computation.
I0629 16:47:25.732095  8020 net.cpp:322] conv1a needs backward computation.
I0629 16:47:25.732100  8020 net.cpp:324] data/bias does not need backward computation.
I0629 16:47:25.732105  8020 net.cpp:324] label_data_1_split does not need backward computation.
I0629 16:47:25.732110  8020 net.cpp:324] data does not need backward computation.
I0629 16:47:25.732115  8020 net.cpp:366] This network produces output accuracy/top1
I0629 16:47:25.732117  8020 net.cpp:366] This network produces output accuracy/top5
I0629 16:47:25.732121  8020 net.cpp:366] This network produces output loss
I0629 16:47:25.732172  8020 net.cpp:388] Top memory (TEST) required for data: 319897600 diff: 257228808
I0629 16:47:25.732175  8020 net.cpp:391] Bottom memory (TEST) required for data: 319897600 diff: 319897600
I0629 16:47:25.732178  8020 net.cpp:394] Shared (in-place) memory (TEST) by data: 210739200 diff: 210739200
I0629 16:47:25.732182  8020 net.cpp:397] Parameters memory (TEST) required for data: 2720256 diff: 2720256
I0629 16:47:25.732187  8020 net.cpp:400] Parameters shared memory (TEST) by data: 0 diff: 0
I0629 16:47:25.732189  8020 net.cpp:406] Network initialization done.
I0629 16:47:25.732273  8020 solver.cpp:56] Solver scaffolding done.
I0629 16:47:25.741778  8020 caffe.cpp:137] Finetuning from training/imagenet_jacintonet11_v2_bn_iter_160000.caffemodel
I0629 16:47:25.752005  8020 net.cpp:1071] Ignoring source layer input
I0629 16:47:25.752023  8020 net.cpp:1087] Copying source layer data/bias Type:Bias #blobs=1
I0629 16:47:25.752032  8020 net.cpp:1087] Copying source layer conv1a Type:Convolution #blobs=2
I0629 16:47:25.752074  8020 net.cpp:1087] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0629 16:47:25.752611  8020 net.cpp:1100] BN legacy DIGITS format detected ... 
I0629 16:47:25.752619  8020 net.cpp:1106] BN Transforming to new format completed.
I0629 16:47:25.752622  8020 net.cpp:1087] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0629 16:47:25.752624  8020 net.cpp:1087] Copying source layer conv1b Type:Convolution #blobs=2
I0629 16:47:25.752662  8020 net.cpp:1087] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0629 16:47:25.753041  8020 net.cpp:1100] BN legacy DIGITS format detected ... 
I0629 16:47:25.753046  8020 net.cpp:1106] BN Transforming to new format completed.
I0629 16:47:25.753047  8020 net.cpp:1087] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0629 16:47:25.753049  8020 net.cpp:1087] Copying source layer pool1 Type:Pooling #blobs=0
I0629 16:47:25.753051  8020 net.cpp:1087] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0629 16:47:25.753300  8020 net.cpp:1087] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0629 16:47:25.753685  8020 net.cpp:1100] BN legacy DIGITS format detected ... 
I0629 16:47:25.753690  8020 net.cpp:1106] BN Transforming to new format completed.
I0629 16:47:25.753692  8020 net.cpp:1087] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0629 16:47:25.753695  8020 net.cpp:1087] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0629 16:47:25.753824  8020 net.cpp:1087] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0629 16:47:25.754204  8020 net.cpp:1100] BN legacy DIGITS format detected ... 
I0629 16:47:25.754209  8020 net.cpp:1106] BN Transforming to new format completed.
I0629 16:47:25.754211  8020 net.cpp:1087] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0629 16:47:25.754215  8020 net.cpp:1087] Copying source layer pool2 Type:Pooling #blobs=0
I0629 16:47:25.754216  8020 net.cpp:1087] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0629 16:47:25.755198  8020 net.cpp:1087] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0629 16:47:25.755553  8020 net.cpp:1100] BN legacy DIGITS format detected ... 
I0629 16:47:25.755559  8020 net.cpp:1106] BN Transforming to new format completed.
I0629 16:47:25.755561  8020 net.cpp:1087] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0629 16:47:25.755563  8020 net.cpp:1087] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0629 16:47:25.756057  8020 net.cpp:1087] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0629 16:47:25.756403  8020 net.cpp:1100] BN legacy DIGITS format detected ... 
I0629 16:47:25.756408  8020 net.cpp:1106] BN Transforming to new format completed.
I0629 16:47:25.756410  8020 net.cpp:1087] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0629 16:47:25.756412  8020 net.cpp:1087] Copying source layer pool3 Type:Pooling #blobs=0
I0629 16:47:25.756415  8020 net.cpp:1087] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0629 16:47:25.760326  8020 net.cpp:1087] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0629 16:47:25.760730  8020 net.cpp:1100] BN legacy DIGITS format detected ... 
I0629 16:47:25.760735  8020 net.cpp:1106] BN Transforming to new format completed.
I0629 16:47:25.760738  8020 net.cpp:1087] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0629 16:47:25.760740  8020 net.cpp:1087] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0629 16:47:25.762683  8020 net.cpp:1087] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0629 16:47:25.763062  8020 net.cpp:1100] BN legacy DIGITS format detected ... 
I0629 16:47:25.763068  8020 net.cpp:1106] BN Transforming to new format completed.
I0629 16:47:25.763072  8020 net.cpp:1087] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0629 16:47:25.763073  8020 net.cpp:1087] Copying source layer pool4 Type:Pooling #blobs=0
I0629 16:47:25.763075  8020 net.cpp:1087] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0629 16:47:25.778672  8020 net.cpp:1087] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0629 16:47:25.779088  8020 net.cpp:1100] BN legacy DIGITS format detected ... 
I0629 16:47:25.779095  8020 net.cpp:1106] BN Transforming to new format completed.
I0629 16:47:25.779098  8020 net.cpp:1087] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0629 16:47:25.779100  8020 net.cpp:1087] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0629 16:47:25.786831  8020 net.cpp:1087] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0629 16:47:25.787200  8020 net.cpp:1100] BN legacy DIGITS format detected ... 
I0629 16:47:25.787205  8020 net.cpp:1106] BN Transforming to new format completed.
I0629 16:47:25.787209  8020 net.cpp:1087] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0629 16:47:25.787210  8020 net.cpp:1071] Ignoring source layer pool5
I0629 16:47:25.787212  8020 net.cpp:1071] Ignoring source layer fc1000
I0629 16:47:25.787214  8020 net.cpp:1071] Ignoring source layer fc1000_fc1000_0_split
I0629 16:47:25.787216  8020 net.cpp:1071] Ignoring source layer prob
I0629 16:47:25.787219  8020 net.cpp:1071] Ignoring source layer argMaxOut
I0629 16:47:25.796597  8020 net.cpp:1071] Ignoring source layer input
I0629 16:47:25.796618  8020 net.cpp:1087] Copying source layer data/bias Type:Bias #blobs=1
I0629 16:47:25.796623  8020 net.cpp:1087] Copying source layer conv1a Type:Convolution #blobs=2
I0629 16:47:25.796666  8020 net.cpp:1087] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0629 16:47:25.797209  8020 net.cpp:1100] BN legacy DIGITS format detected ... 
I0629 16:47:25.797215  8020 net.cpp:1106] BN Transforming to new format completed.
I0629 16:47:25.797219  8020 net.cpp:1087] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0629 16:47:25.797222  8020 net.cpp:1087] Copying source layer conv1b Type:Convolution #blobs=2
I0629 16:47:25.797261  8020 net.cpp:1087] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0629 16:47:25.797639  8020 net.cpp:1100] BN legacy DIGITS format detected ... 
I0629 16:47:25.797644  8020 net.cpp:1106] BN Transforming to new format completed.
I0629 16:47:25.797647  8020 net.cpp:1087] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0629 16:47:25.797649  8020 net.cpp:1087] Copying source layer pool1 Type:Pooling #blobs=0
I0629 16:47:25.797652  8020 net.cpp:1087] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0629 16:47:25.797909  8020 net.cpp:1087] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0629 16:47:25.798306  8020 net.cpp:1100] BN legacy DIGITS format detected ... 
I0629 16:47:25.798311  8020 net.cpp:1106] BN Transforming to new format completed.
I0629 16:47:25.798315  8020 net.cpp:1087] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0629 16:47:25.798316  8020 net.cpp:1087] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0629 16:47:25.798446  8020 net.cpp:1087] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0629 16:47:25.798861  8020 net.cpp:1100] BN legacy DIGITS format detected ... 
I0629 16:47:25.798867  8020 net.cpp:1106] BN Transforming to new format completed.
I0629 16:47:25.798869  8020 net.cpp:1087] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0629 16:47:25.798871  8020 net.cpp:1087] Copying source layer pool2 Type:Pooling #blobs=0
I0629 16:47:25.798874  8020 net.cpp:1087] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0629 16:47:25.799849  8020 net.cpp:1087] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0629 16:47:25.800206  8020 net.cpp:1100] BN legacy DIGITS format detected ... 
I0629 16:47:25.800211  8020 net.cpp:1106] BN Transforming to new format completed.
I0629 16:47:25.800213  8020 net.cpp:1087] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0629 16:47:25.800216  8020 net.cpp:1087] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0629 16:47:25.800709  8020 net.cpp:1087] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0629 16:47:25.801050  8020 net.cpp:1100] BN legacy DIGITS format detected ... 
I0629 16:47:25.801055  8020 net.cpp:1106] BN Transforming to new format completed.
I0629 16:47:25.801057  8020 net.cpp:1087] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0629 16:47:25.801059  8020 net.cpp:1087] Copying source layer pool3 Type:Pooling #blobs=0
I0629 16:47:25.801062  8020 net.cpp:1087] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0629 16:47:25.804939  8020 net.cpp:1087] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0629 16:47:25.805300  8020 net.cpp:1100] BN legacy DIGITS format detected ... 
I0629 16:47:25.805305  8020 net.cpp:1106] BN Transforming to new format completed.
I0629 16:47:25.805308  8020 net.cpp:1087] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0629 16:47:25.805310  8020 net.cpp:1087] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0629 16:47:25.807257  8020 net.cpp:1087] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0629 16:47:25.807616  8020 net.cpp:1100] BN legacy DIGITS format detected ... 
I0629 16:47:25.807621  8020 net.cpp:1106] BN Transforming to new format completed.
I0629 16:47:25.807623  8020 net.cpp:1087] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0629 16:47:25.807626  8020 net.cpp:1087] Copying source layer pool4 Type:Pooling #blobs=0
I0629 16:47:25.807627  8020 net.cpp:1087] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0629 16:47:25.823151  8020 net.cpp:1087] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0629 16:47:25.823614  8020 net.cpp:1100] BN legacy DIGITS format detected ... 
I0629 16:47:25.823621  8020 net.cpp:1106] BN Transforming to new format completed.
I0629 16:47:25.823622  8020 net.cpp:1087] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0629 16:47:25.823626  8020 net.cpp:1087] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0629 16:47:25.831365  8020 net.cpp:1087] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0629 16:47:25.831743  8020 net.cpp:1100] BN legacy DIGITS format detected ... 
I0629 16:47:25.831748  8020 net.cpp:1106] BN Transforming to new format completed.
I0629 16:47:25.831750  8020 net.cpp:1087] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0629 16:47:25.831753  8020 net.cpp:1071] Ignoring source layer pool5
I0629 16:47:25.831754  8020 net.cpp:1071] Ignoring source layer fc1000
I0629 16:47:25.831756  8020 net.cpp:1071] Ignoring source layer fc1000_fc1000_0_split
I0629 16:47:25.831758  8020 net.cpp:1071] Ignoring source layer prob
I0629 16:47:25.831760  8020 net.cpp:1071] Ignoring source layer argMaxOut
I0629 16:47:25.831847  8020 parallel.cpp:106] [0 - 0] P2pSync adding callback
I0629 16:47:25.831852  8020 parallel.cpp:106] [1 - 1] P2pSync adding callback
I0629 16:47:25.831854  8020 parallel.cpp:106] [2 - 2] P2pSync adding callback
I0629 16:47:25.831856  8020 parallel.cpp:59] Starting Optimization
I0629 16:47:25.831859  8020 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0629 16:47:25.831888  8020 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0629 16:47:25.831903  8020 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0629 16:47:25.832587  8091 device_alternate.hpp:116] NVML initialized on thread 136458831099648
I0629 16:47:25.845044  8091 common.cpp:563] NVML succeeded to set CPU affinity on device 1
I0629 16:47:25.845099  8090 device_alternate.hpp:116] NVML initialized on thread 136458839492352
I0629 16:47:25.845860  8090 common.cpp:563] NVML succeeded to set CPU affinity on device 0
I0629 16:47:25.845916  8092 device_alternate.hpp:116] NVML initialized on thread 136458822706944
I0629 16:47:25.846297  8092 common.cpp:563] NVML succeeded to set CPU affinity on device 2
I0629 16:47:25.849421  8091 solver.cpp:42] Solver data type: FLOAT
W0629 16:47:25.849956  8091 parallel.cpp:271] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0629 16:47:25.850061  8091 net.cpp:108] Using FLOAT as default forward math type
I0629 16:47:25.850067  8091 net.cpp:114] Using FLOAT as default backward math type
I0629 16:47:25.850097  8091 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0629 16:47:25.850105  8091 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0629 16:47:25.854519  8092 solver.cpp:42] Solver data type: FLOAT
I0629 16:47:25.855319  8093 db_lmdb.cpp:35] Opened lmdb data/train-image-lmdb
W0629 16:47:25.855320  8092 parallel.cpp:271] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0629 16:47:25.855558  8092 net.cpp:108] Using FLOAT as default forward math type
I0629 16:47:25.855567  8092 net.cpp:114] Using FLOAT as default backward math type
I0629 16:47:25.855630  8092 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0629 16:47:25.855645  8092 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0629 16:47:25.856516  8094 db_lmdb.cpp:35] Opened lmdb data/train-image-lmdb
I0629 16:47:25.858440  8091 data_layer.cpp:188] ReshapePrefetch 6, 3, 640, 640
I0629 16:47:25.858608  8091 data_layer.cpp:206] Output data size: 6, 3, 640, 640
I0629 16:47:25.858623  8091 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0629 16:47:25.858710  8091 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0629 16:47:25.858724  8091 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0629 16:47:25.859606  8095 data_layer.cpp:188] ReshapePrefetch 6, 3, 640, 640
I0629 16:47:25.859634  8095 data_layer.cpp:206] Output data size: 6, 3, 640, 640
I0629 16:47:25.865247  8096 db_lmdb.cpp:35] Opened lmdb data/train-label-lmdb
I0629 16:47:25.865394  8092 data_layer.cpp:188] ReshapePrefetch 6, 3, 640, 640
I0629 16:47:25.866303  8092 data_layer.cpp:206] Output data size: 6, 3, 640, 640
I0629 16:47:25.866319  8092 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0629 16:47:25.866371  8091 data_layer.cpp:188] ReshapePrefetch 6, 1, 640, 640
I0629 16:47:25.866400  8092 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0629 16:47:25.866413  8092 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0629 16:47:25.866425  8091 data_layer.cpp:206] Output data size: 6, 1, 640, 640
I0629 16:47:25.866430  8091 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0629 16:47:25.867431  8097 data_layer.cpp:188] ReshapePrefetch 6, 3, 640, 640
I0629 16:47:25.867455  8097 data_layer.cpp:206] Output data size: 6, 3, 640, 640
I0629 16:47:25.873353  8099 data_layer.cpp:188] ReshapePrefetch 6, 1, 640, 640
I0629 16:47:25.873395  8099 data_layer.cpp:206] Output data size: 6, 1, 640, 640
I0629 16:47:25.875720  8098 db_lmdb.cpp:35] Opened lmdb data/train-label-lmdb
I0629 16:47:25.878120  8092 data_layer.cpp:188] ReshapePrefetch 6, 1, 640, 640
I0629 16:47:25.879282  8092 data_layer.cpp:206] Output data size: 6, 1, 640, 640
I0629 16:47:25.879333  8092 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0629 16:47:25.884394  8095 data_layer.cpp:110] [1] Parser threads: 1
I0629 16:47:25.884460  8095 data_layer.cpp:112] [1] Transformer threads: 1
I0629 16:47:25.885844  8100 data_layer.cpp:188] ReshapePrefetch 6, 1, 640, 640
I0629 16:47:25.885876  8100 data_layer.cpp:206] Output data size: 6, 1, 640, 640
I0629 16:47:25.889075  8099 data_layer.cpp:110] [1] Parser threads: 1
I0629 16:47:25.889101  8099 data_layer.cpp:112] [1] Transformer threads: 1
I0629 16:47:25.900619  8097 data_layer.cpp:110] [2] Parser threads: 1
I0629 16:47:25.900674  8097 data_layer.cpp:112] [2] Transformer threads: 1
I0629 16:47:25.910711  8100 data_layer.cpp:110] [2] Parser threads: 1
I0629 16:47:25.910742  8100 data_layer.cpp:112] [2] Transformer threads: 1
I0629 16:47:26.394271  8091 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 7.99G, req 0G)
I0629 16:47:26.421128  8092 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 7.99G, req 0G)
I0629 16:47:26.442138  8091 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 1  (limit 7.83G, req 0G)
I0629 16:47:26.474159  8092 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.83G, req 0G)
I0629 16:47:26.483563  8091 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.7G, req 0G)
I0629 16:47:26.509222  8091 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.62G, req 0G)
I0629 16:47:26.524776  8092 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.7G, req 0G)
I0629 16:47:26.536881  8091 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 7.56G, req 0G)
I0629 16:47:26.550351  8092 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.62G, req 0G)
I0629 16:47:26.552405  8091 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 1 3  (limit 7.52G, req 0G)
I0629 16:47:26.576556  8092 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 7.56G, req 0G)
I0629 16:47:26.585729  8091 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.48G, req 0G)
I0629 16:47:26.591467  8092 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.52G, req 0G)
I0629 16:47:26.599738  8091 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.46G, req 0G)
I0629 16:47:26.625020  8092 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.48G, req 0G)
I0629 16:47:26.635555  8092 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.46G, req 0G)
I0629 16:47:26.672271  8091 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 7.35G, req 0G)
I0629 16:47:26.689415  8091 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 7.3G, req 0G)
I0629 16:47:26.707314  8092 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 7.35G, req 0G)
I0629 16:47:26.715009  8091 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 7.25G, req 0G)
I0629 16:47:26.719960  8091 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cityscapes20_jsegnet21v2_2017-06-29_16-47-23/initial/test.prototxt
W0629 16:47:26.720026  8091 parallel.cpp:271] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0629 16:47:26.720149  8091 net.cpp:108] Using FLOAT as default forward math type
I0629 16:47:26.720154  8091 net.cpp:114] Using FLOAT as default backward math type
I0629 16:47:26.720187  8091 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0629 16:47:26.720193  8091 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0629 16:47:26.720948  8117 db_lmdb.cpp:35] Opened lmdb data/val-image-lmdb
I0629 16:47:26.722328  8091 data_layer.cpp:188] ReshapePrefetch 2, 3, 640, 640
I0629 16:47:26.722486  8091 data_layer.cpp:206] Output data size: 2, 3, 640, 640
I0629 16:47:26.722499  8091 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0629 16:47:26.722633  8091 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0629 16:47:26.722647  8091 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0629 16:47:26.723551  8118 data_layer.cpp:188] ReshapePrefetch 2, 3, 640, 640
I0629 16:47:26.723572  8118 data_layer.cpp:206] Output data size: 2, 3, 640, 640
I0629 16:47:26.724522  8092 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 7.3G, req 0G)
I0629 16:47:26.728890  8119 db_lmdb.cpp:35] Opened lmdb data/val-label-lmdb
I0629 16:47:26.731997  8091 data_layer.cpp:188] ReshapePrefetch 2, 1, 640, 640
I0629 16:47:26.732332  8091 data_layer.cpp:206] Output data size: 2, 1, 640, 640
I0629 16:47:26.732347  8091 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0629 16:47:26.734189  8120 data_layer.cpp:188] ReshapePrefetch 2, 1, 640, 640
I0629 16:47:26.734200  8120 data_layer.cpp:206] Output data size: 2, 1, 640, 640
I0629 16:47:26.735033  8118 data_layer.cpp:110] [1] Parser threads: 1
I0629 16:47:26.735049  8118 data_layer.cpp:112] [1] Transformer threads: 1
I0629 16:47:26.738052  8120 data_layer.cpp:110] [1] Parser threads: 1
I0629 16:47:26.738067  8120 data_layer.cpp:112] [1] Transformer threads: 1
I0629 16:47:26.763397  8092 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 7.25G, req 0G)
I0629 16:47:26.768632  8092 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cityscapes20_jsegnet21v2_2017-06-29_16-47-23/initial/test.prototxt
W0629 16:47:26.768700  8092 parallel.cpp:271] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0629 16:47:26.768827  8092 net.cpp:108] Using FLOAT as default forward math type
I0629 16:47:26.768832  8092 net.cpp:114] Using FLOAT as default backward math type
I0629 16:47:26.768854  8092 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0629 16:47:26.768863  8092 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0629 16:47:26.769702  8126 db_lmdb.cpp:35] Opened lmdb data/val-image-lmdb
I0629 16:47:26.771759  8092 data_layer.cpp:188] ReshapePrefetch 2, 3, 640, 640
I0629 16:47:26.771977  8092 data_layer.cpp:206] Output data size: 2, 3, 640, 640
I0629 16:47:26.771996  8092 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0629 16:47:26.772069  8092 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0629 16:47:26.772081  8092 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0629 16:47:26.775709  8128 db_lmdb.cpp:35] Opened lmdb data/val-label-lmdb
I0629 16:47:26.776268  8092 data_layer.cpp:188] ReshapePrefetch 2, 1, 640, 640
I0629 16:47:26.776370  8092 data_layer.cpp:206] Output data size: 2, 1, 640, 640
I0629 16:47:26.776381  8092 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0629 16:47:26.776553  8127 data_layer.cpp:188] ReshapePrefetch 2, 3, 640, 640
I0629 16:47:26.776564  8127 data_layer.cpp:206] Output data size: 2, 3, 640, 640
I0629 16:47:26.778877  8129 data_layer.cpp:188] ReshapePrefetch 2, 1, 640, 640
I0629 16:47:26.778898  8129 data_layer.cpp:206] Output data size: 2, 1, 640, 640
I0629 16:47:26.782861  8127 data_layer.cpp:110] [2] Parser threads: 1
I0629 16:47:26.782881  8127 data_layer.cpp:112] [2] Transformer threads: 1
I0629 16:47:26.783547  8129 data_layer.cpp:110] [2] Parser threads: 1
I0629 16:47:26.783556  8129 data_layer.cpp:112] [2] Transformer threads: 1
I0629 16:47:26.897428  8091 solver.cpp:56] Solver scaffolding done.
I0629 16:47:26.940171  8092 solver.cpp:56] Solver scaffolding done.
I0629 16:47:27.008541  8092 parallel.cpp:161] [2 - 2] P2pSync adding callback
I0629 16:47:27.008566  8091 parallel.cpp:161] [1 - 1] P2pSync adding callback
I0629 16:47:27.008605  8090 parallel.cpp:161] [0 - 0] P2pSync adding callback
I0629 16:47:27.189728  8091 solver.cpp:474] Solving jsegnet21v2_train
I0629 16:47:27.189744  8091 solver.cpp:475] Learning Rate Policy: multistep
I0629 16:47:27.189755  8092 solver.cpp:474] Solving jsegnet21v2_train
I0629 16:47:27.189755  8090 solver.cpp:474] Solving jsegnet21v2_train
I0629 16:47:27.189769  8092 solver.cpp:475] Learning Rate Policy: multistep
I0629 16:47:27.189772  8090 solver.cpp:475] Learning Rate Policy: multistep
I0629 16:47:27.203423  8091 solver.cpp:268] Starting Optimization on GPU 1
I0629 16:47:27.203423  8092 solver.cpp:268] Starting Optimization on GPU 2
I0629 16:47:27.203423  8090 solver.cpp:268] Starting Optimization on GPU 0
I0629 16:47:27.203660  8139 device_alternate.hpp:116] NVML initialized on thread 128458671232768
I0629 16:47:27.203660  8090 solver.cpp:545] Iteration 0, Testing net (#0)
I0629 16:47:27.203686  8139 common.cpp:563] NVML succeeded to set CPU affinity on device 1
I0629 16:47:27.203696  8141 device_alternate.hpp:116] NVML initialized on thread 128458654447360
I0629 16:47:27.203704  8141 common.cpp:563] NVML succeeded to set CPU affinity on device 2
I0629 16:47:27.203711  8140 device_alternate.hpp:116] NVML initialized on thread 128458662840064
I0629 16:47:27.203722  8140 common.cpp:563] NVML succeeded to set CPU affinity on device 0
I0629 16:47:27.547494  8090 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.0242897
I0629 16:47:27.547531  8090 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.157497
I0629 16:47:27.547538  8090 solver.cpp:630]     Test net output #2: loss = 85.2152 (* 1 = 85.2152 loss)
I0629 16:47:27.547543  8090 solver.cpp:295] [MultiGPU] Initial Test completed
I0629 16:47:27.640426  8090 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 5.58G, req 0G)
I0629 16:47:27.642079  8091 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 5.67G, req 0G)
I0629 16:47:27.643919  8092 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 5.67G, req 0G)
I0629 16:47:27.703313  8090 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 1 3  (limit 5.42G, req 0G)
I0629 16:47:27.711657  8091 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 5.51G, req 0G)
I0629 16:47:27.715308  8092 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 5.51G, req 0G)
I0629 16:47:27.764371  8090 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 1 4 3  (limit 5.24G, req 0G)
I0629 16:47:27.776013  8091 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 5.33G, req 0G)
I0629 16:47:27.776849  8092 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 5.33G, req 0G)
I0629 16:47:27.790024  8090 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 5.16G, req 0G)
I0629 16:47:27.804584  8092 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 5.25G, req 0G)
I0629 16:47:27.805011  8091 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 5.25G, req 0G)
I0629 16:47:27.817013  8090 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 5.07G, req 0G)
I0629 16:47:27.830562  8090 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 5.03G, req 0G)
I0629 16:47:27.831921  8092 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 5.16G, req 0G)
I0629 16:47:27.833487  8091 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 5.16G, req 0G)
I0629 16:47:27.847682  8092 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 5.11G, req 0G)
I0629 16:47:27.850648  8091 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 5.11G, req 0G)
I0629 16:47:27.856031  8090 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 5G, req 0G)
I0629 16:47:27.865283  8090 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 4.98G, req 0G)
I0629 16:47:27.874423  8092 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 3  (limit 5.09G, req 0G)
I0629 16:47:27.874871  8091 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 5.09G, req 0G)
I0629 16:47:27.884836  8092 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 5.07G, req 0G)
I0629 16:47:27.885444  8091 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 5.07G, req 0G)
I0629 16:47:27.915050  8090 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 4.71G, req 0G)
I0629 16:47:27.928640  8090 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 4.69G, req 0G)
I0629 16:47:27.935472  8091 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 4.8G, req 0G)
I0629 16:47:27.936295  8092 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 4.8G, req 0G)
I0629 16:47:27.950310  8091 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 4.78G, req 0G)
I0629 16:47:27.951390  8092 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 4.78G, req 0G)
I0629 16:47:27.957466  8090 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 4.54G, req 0G)
I0629 16:47:27.982817  8091 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 4.62G, req 0G)
I0629 16:47:27.983197  8092 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 4.62G, req 0G)
I0629 16:47:28.177747  8090 solver.cpp:354] Iteration 0 (0.630164 s), loss = 3.27205
I0629 16:47:28.177780  8090 solver.cpp:371]     Train net output #0: loss = 3.27205 (* 1 = 3.27205 loss)
I0629 16:47:28.177788  8090 sgd_solver.cpp:137] Iteration 0, lr = 0.0001, m = 0.9
I0629 16:47:28.247634  8092 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 3.28G, req 0G)
I0629 16:47:28.250772  8090 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 3.19G, req 0G)
I0629 16:47:28.251456  8091 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 3.28G, req 0G)
I0629 16:47:28.296313  8092 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 3.28G, req 0G)
I0629 16:47:28.302817  8091 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 3.28G, req 0G)
I0629 16:47:28.304327  8090 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 3.19G, req 0G)
I0629 16:47:28.339265  8092 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 3.28G, req 0G)
I0629 16:47:28.345510  8091 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 3.28G, req 0G)
I0629 16:47:28.346536  8090 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 3.19G, req 0G)
I0629 16:47:28.360013  8092 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 3.28G, req 0G)
I0629 16:47:28.366204  8091 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 3.28G, req 0G)
I0629 16:47:28.366897  8090 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 3.19G, req 0G)
I0629 16:47:28.378290  8092 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 3.28G, req 0G)
I0629 16:47:28.384642  8090 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 3.19G, req 0G)
I0629 16:47:28.385072  8091 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 3.28G, req 0G)
I0629 16:47:28.388029  8092 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 3.28G, req 0G)
I0629 16:47:28.393656  8090 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 3.19G, req 0G)
I0629 16:47:28.394922  8091 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 3.28G, req 0G)
I0629 16:47:28.403277  8092 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 3.28G, req 0G)
I0629 16:47:28.408746  8090 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 3.19G, req 0G)
I0629 16:47:28.408989  8092 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 3.28G, req 0G)
I0629 16:47:28.410168  8091 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 3.28G, req 0G)
I0629 16:47:28.417556  8090 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 3.19G, req 0G)
I0629 16:47:28.418344  8091 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 3.28G, req 0G)
I0629 16:47:28.434793  8092 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 3.28G, req 0G)
I0629 16:47:28.442656  8090 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 3.19G, req 0G)
I0629 16:47:28.444242  8091 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 3.28G, req 0G)
I0629 16:47:28.446518  8092 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 3.28G, req 0G)
I0629 16:47:28.453943  8090 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 3.19G, req 0G)
I0629 16:47:28.455865  8091 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 3.28G, req 0G)
I0629 16:47:28.462375  8092 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 3.28G, req 0G)
I0629 16:47:28.469446  8090 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 3.19G, req 0G)
I0629 16:47:28.472651  8091 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 3.28G, req 0G)
I0629 16:47:28.623433  8090 solver.cpp:354] Iteration 1 (0.445616 s), loss = 3.28857
I0629 16:47:28.623457  8090 solver.cpp:371]     Train net output #0: loss = 3.28857 (* 1 = 3.28857 loss)
I0629 16:47:28.628134  8070 blocking_queue.cpp:40] Waiting for datum
I0629 16:47:28.685492  8090 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'conv1a' with space 1.29G/1 1 0 3  (limit 1.92G, req 0G)
I0629 16:47:28.688278  8091 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'conv1a' with space 1.29G/1 1 0 3  (limit 2.01G, req 0G)
I0629 16:47:28.691076  8092 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'conv1a' with space 1.29G/1 1 0 3  (limit 2.01G, req 0G)
W0629 16:47:28.692512  8090 cudnn_conv_layer.cpp:235] [0] Current workspace (1.29G) Estimated requirement (2.57G)
W0629 16:47:28.697109  8091 cudnn_conv_layer.cpp:235] [1] Current workspace (1.29G) Estimated requirement (2.57G)
W0629 16:47:28.703948  8092 cudnn_conv_layer.cpp:235] [2] Current workspace (1.29G) Estimated requirement (2.57G)
I0629 16:47:28.760324  8090 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'conv1b' with space 1.91G/2 6 4 1  (limit 1.3G, req 0G)
I0629 16:47:28.774356  8092 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'conv1b' with space 1.99G/2 6 4 3  (limit 1.3G, req 0G)
I0629 16:47:28.774714  8091 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'conv1b' with space 1.99G/2 6 4 3  (limit 1.3G, req 0G)
I0629 16:47:28.889535  8090 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.91G/1 1 4 3  (limit 1.3G, req 0G)
W0629 16:47:28.893857  8090 cudnn_conv_layer.cpp:235] [0] Current workspace (1.91G) Estimated requirement (2.57G)
I0629 16:47:28.903451  8092 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.99G/1 6 4 3  (limit 1.3G, req 0G)
I0629 16:47:28.904933  8091 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.99G/1 6 4 3  (limit 1.3G, req 0G)
W0629 16:47:28.907119  8092 cudnn_conv_layer.cpp:235] [2] Current workspace (1.99G) Estimated requirement (2.57G)
W0629 16:47:28.908625  8091 cudnn_conv_layer.cpp:235] [1] Current workspace (1.99G) Estimated requirement (2.57G)
I0629 16:47:28.929769  8090 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.91G/2 6 4 3  (limit 1.3G, req 0G)
I0629 16:47:28.942517  8092 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.99G/2 6 4 3  (limit 1.3G, req 0G)
I0629 16:47:28.944325  8091 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.99G/2 6 4 3  (limit 1.3G, req 0G)
I0629 16:47:29.021929  8090 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.91G/1 6 4 5  (limit 1.3G, req 0.07G)
W0629 16:47:29.024040  8090 cudnn_conv_layer.cpp:235] [0] Current workspace (1.91G) Estimated requirement (2.57G)
I0629 16:47:29.036664  8092 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.99G/1 6 4 5  (limit 1.3G, req 0.07G)
I0629 16:47:29.038766  8091 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.99G/1 6 4 5  (limit 1.3G, req 0.07G)
W0629 16:47:29.040068  8092 cudnn_conv_layer.cpp:235] [2] Current workspace (1.99G) Estimated requirement (2.57G)
W0629 16:47:29.042086  8091 cudnn_conv_layer.cpp:235] [1] Current workspace (1.99G) Estimated requirement (2.57G)
I0629 16:47:29.047031  8090 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.91G/2 1 4 3  (limit 1.3G, req 0.07G)
I0629 16:47:29.064213  8092 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.99G/2 1 4 5  (limit 1.3G, req 0.07G)
I0629 16:47:29.065459  8091 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.99G/2 6 4 3  (limit 1.3G, req 0.07G)
I0629 16:47:29.113117  8090 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.91G/1 6 4 5  (limit 1.3G, req 0.07G)
W0629 16:47:29.115823  8090 cudnn_conv_layer.cpp:235] [0] Current workspace (1.91G) Estimated requirement (2.57G)
I0629 16:47:29.126821  8090 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.91G/2 6 4 3  (limit 1.3G, req 0.07G)
I0629 16:47:29.137023  8092 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.99G/1 6 4 5  (limit 1.3G, req 0.07G)
I0629 16:47:29.138171  8091 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.99G/1 6 4 5  (limit 1.3G, req 0.07G)
W0629 16:47:29.138648  8092 cudnn_conv_layer.cpp:235] [2] Current workspace (1.99G) Estimated requirement (2.57G)
W0629 16:47:29.139789  8091 cudnn_conv_layer.cpp:235] [1] Current workspace (1.99G) Estimated requirement (2.57G)
W0629 16:47:29.144888  8090 cudnn_conv_layer.cpp:235] [0] Current workspace (1.91G) Estimated requirement (2.57G)
I0629 16:47:29.149116  8092 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.99G/2 6 4 3  (limit 1.3G, req 0.07G)
I0629 16:47:29.150319  8091 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.99G/2 6 4 3  (limit 1.3G, req 0.07G)
W0629 16:47:29.167343  8092 cudnn_conv_layer.cpp:235] [2] Current workspace (1.99G) Estimated requirement (2.57G)
W0629 16:47:29.168900  8091 cudnn_conv_layer.cpp:235] [1] Current workspace (1.99G) Estimated requirement (2.57G)
I0629 16:47:29.176007  8090 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'out3a' with space 1.91G/2 6 4 3  (limit 1.3G, req 0.07G)
I0629 16:47:29.200520  8092 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'out3a' with space 1.99G/2 6 1 3  (limit 1.3G, req 0.07G)
I0629 16:47:29.202004  8091 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'out3a' with space 1.99G/2 6 1 3  (limit 1.3G, req 0.07G)
I0629 16:47:29.230260  8090 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 1.91G/1 6 4 3  (limit 1.3G, req 0.07G)
I0629 16:47:29.255420  8092 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 1.99G/1 6 4 3  (limit 1.3G, req 0.07G)
I0629 16:47:29.257962  8091 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 1.99G/1 6 4 3  (limit 1.3G, req 0.07G)
I0629 16:47:29.263659  8090 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 1.91G/1 6 1 3  (limit 1.3G, req 0.07G)
I0629 16:47:29.287593  8092 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 1.99G/1 6 1 3  (limit 1.3G, req 0.07G)
I0629 16:47:29.289876  8091 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 1.99G/1 6 1 3  (limit 1.3G, req 0.07G)
I0629 16:47:29.441313  8090 solver.cpp:349] Iteration 2 (1.22277 iter/s, 0.817819s/100 iter), loss = 3.21476
I0629 16:47:29.441339  8090 solver.cpp:371]     Train net output #0: loss = 3.21476 (* 1 = 3.21476 loss)
I0629 16:47:29.641118  8090 blocking_queue.cpp:40] Data layer prefetch queue empty
I0629 16:47:32.552891  8090 cudnn_conv_layer.cpp:283] [0] Layer 'conv1a' reallocating workspace: 1.91G -> 0.14G
I0629 16:47:32.553613  8091 cudnn_conv_layer.cpp:283] [1] Layer 'conv1a' reallocating workspace: 1.99G -> 0.14G
I0629 16:47:32.553911  8092 cudnn_conv_layer.cpp:283] [2] Layer 'conv1a' reallocating workspace: 1.99G -> 0.14G
I0629 16:49:42.267107  8090 solver.cpp:349] Iteration 100 (0.737829 iter/s, 132.822s/100 iter), loss = 0.926947
I0629 16:49:42.267182  8090 solver.cpp:371]     Train net output #0: loss = 0.926947 (* 1 = 0.926947 loss)
I0629 16:49:42.267189  8090 sgd_solver.cpp:137] Iteration 100, lr = 0.0001, m = 0.9
I0629 16:50:39.737401  8096 data_reader.cpp:262] Starting prefetch of epoch 1
I0629 16:50:39.737401  8098 data_reader.cpp:262] Starting prefetch of epoch 1
I0629 16:50:39.737401  8069 data_reader.cpp:262] Starting prefetch of epoch 1
I0629 16:50:40.299865  8067 data_reader.cpp:262] Starting prefetch of epoch 1
I0629 16:50:40.299865  8093 data_reader.cpp:262] Starting prefetch of epoch 1
I0629 16:50:40.299865  8094 data_reader.cpp:262] Starting prefetch of epoch 1
I0629 16:51:09.615070  8090 solver.cpp:349] Iteration 200 (1.14488 iter/s, 87.3455s/100 iter), loss = 1.04347
I0629 16:51:09.615097  8090 solver.cpp:371]     Train net output #0: loss = 1.04347 (* 1 = 1.04347 loss)
I0629 16:51:09.615100  8090 sgd_solver.cpp:137] Iteration 200, lr = 0.0001, m = 0.9
I0629 16:52:56.856948  8090 solver.cpp:349] Iteration 300 (0.932497 iter/s, 107.239s/100 iter), loss = 0.535334
I0629 16:52:56.857026  8090 solver.cpp:371]     Train net output #0: loss = 0.535334 (* 1 = 0.535334 loss)
I0629 16:52:56.857033  8090 sgd_solver.cpp:137] Iteration 300, lr = 0.0001, m = 0.9
I0629 16:53:17.771600  8069 data_reader.cpp:262] Starting prefetch of epoch 2
I0629 16:53:17.771600  8096 data_reader.cpp:262] Starting prefetch of epoch 2
I0629 16:53:17.771600  8098 data_reader.cpp:262] Starting prefetch of epoch 2
I0629 16:53:18.788959  8093 data_reader.cpp:262] Starting prefetch of epoch 2
I0629 16:53:18.788990  8094 data_reader.cpp:262] Starting prefetch of epoch 2
I0629 16:53:18.788959  8067 data_reader.cpp:262] Starting prefetch of epoch 2
I0629 16:54:32.774715  8090 solver.cpp:349] Iteration 400 (1.04259 iter/s, 95.9151s/100 iter), loss = 0.378937
I0629 16:54:32.774792  8090 solver.cpp:371]     Train net output #0: loss = 0.378937 (* 1 = 0.378937 loss)
I0629 16:54:32.774798  8090 sgd_solver.cpp:137] Iteration 400, lr = 0.0001, m = 0.9
I0629 16:55:56.793841  8069 data_reader.cpp:262] Starting prefetch of epoch 3
I0629 16:55:56.793843  8098 data_reader.cpp:262] Starting prefetch of epoch 3
I0629 16:55:56.793843  8096 data_reader.cpp:262] Starting prefetch of epoch 3
I0629 16:55:57.163686  8093 data_reader.cpp:262] Starting prefetch of epoch 3
I0629 16:55:57.163686  8094 data_reader.cpp:262] Starting prefetch of epoch 3
I0629 16:55:57.163686  8067 data_reader.cpp:262] Starting prefetch of epoch 3
I0629 16:55:58.464172  8090 solver.cpp:349] Iteration 500 (1.16704 iter/s, 85.6871s/100 iter), loss = 0.469649
I0629 16:55:58.464200  8090 solver.cpp:371]     Train net output #0: loss = 0.469649 (* 1 = 0.469649 loss)
I0629 16:55:58.464208  8090 sgd_solver.cpp:137] Iteration 500, lr = 0.0001, m = 0.9
I0629 16:56:27.451545  8090 solver.cpp:349] Iteration 600 (3.44988 iter/s, 28.9865s/100 iter), loss = 0.447693
I0629 16:56:27.460211  8090 solver.cpp:371]     Train net output #0: loss = 0.447693 (* 1 = 0.447693 loss)
I0629 16:56:27.460255  8090 sgd_solver.cpp:137] Iteration 600, lr = 0.0001, m = 0.9
I0629 16:56:39.868842  8069 data_reader.cpp:262] Starting prefetch of epoch 4
I0629 16:56:39.873375  8096 data_reader.cpp:262] Starting prefetch of epoch 4
I0629 16:56:39.873914  8093 data_reader.cpp:262] Starting prefetch of epoch 4
I0629 16:56:39.876608  8098 data_reader.cpp:262] Starting prefetch of epoch 4
I0629 16:56:39.887636  8067 data_reader.cpp:262] Starting prefetch of epoch 4
I0629 16:56:39.889470  8094 data_reader.cpp:262] Starting prefetch of epoch 4
I0629 16:56:48.914860  8090 solver.cpp:349] Iteration 700 (4.66114 iter/s, 21.454s/100 iter), loss = 0.450904
I0629 16:56:48.914883  8090 solver.cpp:371]     Train net output #0: loss = 0.450903 (* 1 = 0.450903 loss)
I0629 16:56:48.914890  8090 sgd_solver.cpp:137] Iteration 700, lr = 0.0001, m = 0.9
I0629 16:57:10.270903  8090 solver.cpp:349] Iteration 800 (4.68265 iter/s, 21.3554s/100 iter), loss = 0.64027
I0629 16:57:10.270965  8090 solver.cpp:371]     Train net output #0: loss = 0.64027 (* 1 = 0.64027 loss)
I0629 16:57:10.270972  8090 sgd_solver.cpp:137] Iteration 800, lr = 0.0001, m = 0.9
I0629 16:57:15.195883  8069 data_reader.cpp:262] Starting prefetch of epoch 5
I0629 16:57:15.200739  8096 data_reader.cpp:262] Starting prefetch of epoch 5
I0629 16:57:15.209358  8098 data_reader.cpp:262] Starting prefetch of epoch 5
I0629 16:57:15.219148  8094 data_reader.cpp:262] Starting prefetch of epoch 5
I0629 16:57:15.222306  8093 data_reader.cpp:262] Starting prefetch of epoch 5
I0629 16:57:15.226335  8067 data_reader.cpp:262] Starting prefetch of epoch 5
I0629 16:57:31.561923  8090 solver.cpp:349] Iteration 900 (4.69696 iter/s, 21.2904s/100 iter), loss = 0.757587
I0629 16:57:31.561947  8090 solver.cpp:371]     Train net output #0: loss = 0.757587 (* 1 = 0.757587 loss)
I0629 16:57:31.561951  8090 sgd_solver.cpp:137] Iteration 900, lr = 0.0001, m = 0.9
I0629 16:57:50.347280  8096 data_reader.cpp:262] Starting prefetch of epoch 6
I0629 16:57:50.355373  8069 data_reader.cpp:262] Starting prefetch of epoch 6
I0629 16:57:50.355373  8098 data_reader.cpp:262] Starting prefetch of epoch 6
I0629 16:57:50.375527  8067 data_reader.cpp:262] Starting prefetch of epoch 6
I0629 16:57:50.379441  8094 data_reader.cpp:262] Starting prefetch of epoch 6
I0629 16:57:50.380244  8093 data_reader.cpp:262] Starting prefetch of epoch 6
I0629 16:57:52.896790  8090 solver.cpp:349] Iteration 1000 (4.6873 iter/s, 21.3343s/100 iter), loss = 0.568325
I0629 16:57:52.896816  8090 solver.cpp:371]     Train net output #0: loss = 0.568325 (* 1 = 0.568325 loss)
I0629 16:57:52.896819  8090 sgd_solver.cpp:137] Iteration 1000, lr = 0.0001, m = 0.9
I0629 16:58:14.322230  8090 solver.cpp:349] Iteration 1100 (4.66748 iter/s, 21.4248s/100 iter), loss = 0.880507
I0629 16:58:14.322253  8090 solver.cpp:371]     Train net output #0: loss = 0.880507 (* 1 = 0.880507 loss)
I0629 16:58:14.322260  8090 sgd_solver.cpp:137] Iteration 1100, lr = 0.0001, m = 0.9
I0629 16:58:25.721297  8098 data_reader.cpp:262] Starting prefetch of epoch 7
I0629 16:58:25.724416  8096 data_reader.cpp:262] Starting prefetch of epoch 7
I0629 16:58:25.730168  8069 data_reader.cpp:262] Starting prefetch of epoch 7
I0629 16:58:25.736588  8093 data_reader.cpp:262] Starting prefetch of epoch 7
I0629 16:58:25.750713  8094 data_reader.cpp:262] Starting prefetch of epoch 7
I0629 16:58:25.750952  8067 data_reader.cpp:262] Starting prefetch of epoch 7
I0629 16:58:35.763700  8090 solver.cpp:349] Iteration 1200 (4.66399 iter/s, 21.4409s/100 iter), loss = 0.609531
I0629 16:58:35.763722  8090 solver.cpp:371]     Train net output #0: loss = 0.609531 (* 1 = 0.609531 loss)
I0629 16:58:35.763726  8090 sgd_solver.cpp:137] Iteration 1200, lr = 0.0001, m = 0.9
I0629 16:58:57.186292  8090 solver.cpp:349] Iteration 1300 (4.6681 iter/s, 21.422s/100 iter), loss = 0.550012
I0629 16:58:57.186338  8090 solver.cpp:371]     Train net output #0: loss = 0.550012 (* 1 = 0.550012 loss)
I0629 16:58:57.186343  8090 sgd_solver.cpp:137] Iteration 1300, lr = 0.0001, m = 0.9
I0629 16:59:01.240134  8096 data_reader.cpp:262] Starting prefetch of epoch 8
I0629 16:59:01.240134  8098 data_reader.cpp:262] Starting prefetch of epoch 8
I0629 16:59:01.245314  8069 data_reader.cpp:262] Starting prefetch of epoch 8
I0629 16:59:01.256311  8067 data_reader.cpp:262] Starting prefetch of epoch 8
I0629 16:59:01.261881  8094 data_reader.cpp:262] Starting prefetch of epoch 8
I0629 16:59:01.271353  8093 data_reader.cpp:262] Starting prefetch of epoch 8
I0629 16:59:18.407176  8090 solver.cpp:349] Iteration 1400 (4.71248 iter/s, 21.2203s/100 iter), loss = 0.483983
I0629 16:59:18.407196  8090 solver.cpp:371]     Train net output #0: loss = 0.483983 (* 1 = 0.483983 loss)
I0629 16:59:18.407200  8090 sgd_solver.cpp:137] Iteration 1400, lr = 0.0001, m = 0.9
I0629 16:59:36.408054  8098 data_reader.cpp:262] Starting prefetch of epoch 9
I0629 16:59:36.414523  8096 data_reader.cpp:262] Starting prefetch of epoch 9
I0629 16:59:36.416838  8069 data_reader.cpp:262] Starting prefetch of epoch 9
I0629 16:59:36.433928  8067 data_reader.cpp:262] Starting prefetch of epoch 9
I0629 16:59:36.442334  8093 data_reader.cpp:262] Starting prefetch of epoch 9
I0629 16:59:36.444176  8094 data_reader.cpp:262] Starting prefetch of epoch 9
I0629 16:59:39.787209  8090 solver.cpp:349] Iteration 1500 (4.67739 iter/s, 21.3794s/100 iter), loss = 0.559704
I0629 16:59:39.787235  8090 solver.cpp:371]     Train net output #0: loss = 0.559704 (* 1 = 0.559704 loss)
I0629 16:59:39.787238  8090 sgd_solver.cpp:137] Iteration 1500, lr = 0.0001, m = 0.9
I0629 17:00:01.048653  8090 solver.cpp:349] Iteration 1600 (4.70348 iter/s, 21.2608s/100 iter), loss = 0.714091
I0629 17:00:01.048676  8090 solver.cpp:371]     Train net output #0: loss = 0.714091 (* 1 = 0.714091 loss)
I0629 17:00:01.048679  8090 sgd_solver.cpp:137] Iteration 1600, lr = 0.0001, m = 0.9
I0629 17:00:11.504384  8069 data_reader.cpp:262] Starting prefetch of epoch 10
I0629 17:00:11.506476  8096 data_reader.cpp:262] Starting prefetch of epoch 10
I0629 17:00:11.515730  8098 data_reader.cpp:262] Starting prefetch of epoch 10
I0629 17:00:11.530995  8067 data_reader.cpp:262] Starting prefetch of epoch 10
I0629 17:00:11.531924  8093 data_reader.cpp:262] Starting prefetch of epoch 10
I0629 17:00:11.533413  8094 data_reader.cpp:262] Starting prefetch of epoch 10
I0629 17:00:22.578891  8090 solver.cpp:349] Iteration 1700 (4.64476 iter/s, 21.5296s/100 iter), loss = 0.55159
I0629 17:00:22.578913  8090 solver.cpp:371]     Train net output #0: loss = 0.551589 (* 1 = 0.551589 loss)
I0629 17:00:22.578917  8090 sgd_solver.cpp:137] Iteration 1700, lr = 0.0001, m = 0.9
I0629 17:00:44.097899  8090 solver.cpp:349] Iteration 1800 (4.64715 iter/s, 21.5185s/100 iter), loss = 0.545083
I0629 17:00:44.097949  8090 solver.cpp:371]     Train net output #0: loss = 0.545083 (* 1 = 0.545083 loss)
I0629 17:00:44.097955  8090 sgd_solver.cpp:137] Iteration 1800, lr = 0.0001, m = 0.9
I0629 17:00:47.279240  8098 data_reader.cpp:262] Starting prefetch of epoch 11
I0629 17:00:47.283558  8096 data_reader.cpp:262] Starting prefetch of epoch 11
I0629 17:00:47.283558  8069 data_reader.cpp:262] Starting prefetch of epoch 11
I0629 17:00:47.292572  8067 data_reader.cpp:262] Starting prefetch of epoch 11
I0629 17:00:47.294641  8093 data_reader.cpp:262] Starting prefetch of epoch 11
I0629 17:00:47.299127  8094 data_reader.cpp:262] Starting prefetch of epoch 11
I0629 17:01:05.544208  8090 solver.cpp:349] Iteration 1900 (4.6628 iter/s, 21.4463s/100 iter), loss = 0.444851
I0629 17:01:05.544229  8090 solver.cpp:371]     Train net output #0: loss = 0.444851 (* 1 = 0.444851 loss)
I0629 17:01:05.544232  8090 sgd_solver.cpp:137] Iteration 1900, lr = 0.0001, m = 0.9
I0629 17:01:22.588433  8098 data_reader.cpp:262] Starting prefetch of epoch 12
I0629 17:01:22.595216  8069 data_reader.cpp:262] Starting prefetch of epoch 12
I0629 17:01:22.588433  8096 data_reader.cpp:262] Starting prefetch of epoch 12
I0629 17:01:22.606344  8067 data_reader.cpp:262] Starting prefetch of epoch 12
I0629 17:01:22.609897  8093 data_reader.cpp:262] Starting prefetch of epoch 12
I0629 17:01:22.614493  8094 data_reader.cpp:262] Starting prefetch of epoch 12
I0629 17:01:26.638669  8090 solver.cpp:545] Iteration 2000, Testing net (#0)
I0629 17:01:47.171591  8119 data_reader.cpp:262] Starting prefetch of epoch 1
I0629 17:01:47.171592  8128 data_reader.cpp:262] Starting prefetch of epoch 1
I0629 17:01:47.171591  8088 data_reader.cpp:262] Starting prefetch of epoch 1
I0629 17:01:47.346146  8126 data_reader.cpp:262] Starting prefetch of epoch 1
I0629 17:01:47.346146  8117 data_reader.cpp:262] Starting prefetch of epoch 1
I0629 17:01:47.346146  8086 data_reader.cpp:262] Starting prefetch of epoch 1
I0629 17:01:54.780357  8090 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.88546
I0629 17:01:54.780481  8090 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.976856
I0629 17:01:54.780490  8090 solver.cpp:630]     Test net output #2: loss = 0.407363 (* 1 = 0.407363 loss)
I0629 17:01:54.780516  8090 solver.cpp:305] [MultiGPU] Tests completed in 28.142s
I0629 17:01:54.992851  8090 solver.cpp:349] Iteration 2000 (2.02229 iter/s, 49.4488s/100 iter), loss = 0.795607
I0629 17:01:54.992877  8090 solver.cpp:371]     Train net output #0: loss = 0.795607 (* 1 = 0.795607 loss)
I0629 17:01:54.992882  8090 sgd_solver.cpp:137] Iteration 2000, lr = 0.0001, m = 0.9
I0629 17:02:16.285061  8090 solver.cpp:349] Iteration 2100 (4.69654 iter/s, 21.2923s/100 iter), loss = 0.341702
I0629 17:02:16.285085  8090 solver.cpp:371]     Train net output #0: loss = 0.341702 (* 1 = 0.341702 loss)
I0629 17:02:16.285089  8090 sgd_solver.cpp:137] Iteration 2100, lr = 0.0001, m = 0.9
I0629 17:02:25.959764  8096 data_reader.cpp:262] Starting prefetch of epoch 13
I0629 17:02:25.975661  8069 data_reader.cpp:262] Starting prefetch of epoch 13
I0629 17:02:25.976775  8098 data_reader.cpp:262] Starting prefetch of epoch 13
I0629 17:02:25.993975  8093 data_reader.cpp:262] Starting prefetch of epoch 13
I0629 17:02:25.994428  8094 data_reader.cpp:262] Starting prefetch of epoch 13
I0629 17:02:25.996116  8067 data_reader.cpp:262] Starting prefetch of epoch 13
I0629 17:02:37.653952  8090 solver.cpp:349] Iteration 2200 (4.67969 iter/s, 21.3689s/100 iter), loss = 0.625563
I0629 17:02:37.653973  8090 solver.cpp:371]     Train net output #0: loss = 0.625563 (* 1 = 0.625563 loss)
I0629 17:02:37.653977  8090 sgd_solver.cpp:137] Iteration 2200, lr = 0.0001, m = 0.9
I0629 17:02:59.175396  8090 solver.cpp:349] Iteration 2300 (4.64652 iter/s, 21.5215s/100 iter), loss = 0.302121
I0629 17:02:59.175482  8090 solver.cpp:371]     Train net output #0: loss = 0.302121 (* 1 = 0.302121 loss)
I0629 17:02:59.175490  8090 sgd_solver.cpp:137] Iteration 2300, lr = 0.0001, m = 0.9
I0629 17:03:01.358960  8098 data_reader.cpp:262] Starting prefetch of epoch 14
I0629 17:03:01.361166  8069 data_reader.cpp:262] Starting prefetch of epoch 14
I0629 17:03:01.363385  8096 data_reader.cpp:262] Starting prefetch of epoch 14
I0629 17:03:01.376708  8094 data_reader.cpp:262] Starting prefetch of epoch 14
I0629 17:03:01.380213  8093 data_reader.cpp:262] Starting prefetch of epoch 14
I0629 17:03:01.384121  8067 data_reader.cpp:262] Starting prefetch of epoch 14
I0629 17:03:20.583809  8090 solver.cpp:349] Iteration 2400 (4.67106 iter/s, 21.4084s/100 iter), loss = 0.377676
I0629 17:03:20.583834  8090 solver.cpp:371]     Train net output #0: loss = 0.377676 (* 1 = 0.377676 loss)
I0629 17:03:20.583840  8090 sgd_solver.cpp:137] Iteration 2400, lr = 0.0001, m = 0.9
I0629 17:03:36.753571  8096 data_reader.cpp:262] Starting prefetch of epoch 15
I0629 17:03:36.756359  8098 data_reader.cpp:262] Starting prefetch of epoch 15
I0629 17:03:36.756359  8069 data_reader.cpp:262] Starting prefetch of epoch 15
I0629 17:03:36.770723  8067 data_reader.cpp:262] Starting prefetch of epoch 15
I0629 17:03:36.772233  8094 data_reader.cpp:262] Starting prefetch of epoch 15
I0629 17:03:36.779088  8093 data_reader.cpp:262] Starting prefetch of epoch 15
I0629 17:03:41.891047  8090 solver.cpp:349] Iteration 2500 (4.69323 iter/s, 21.3073s/100 iter), loss = 0.441076
I0629 17:03:41.891068  8090 solver.cpp:371]     Train net output #0: loss = 0.441076 (* 1 = 0.441076 loss)
I0629 17:03:41.891072  8090 sgd_solver.cpp:137] Iteration 2500, lr = 0.0001, m = 0.9
I0629 17:04:03.396205  8090 solver.cpp:349] Iteration 2600 (4.65003 iter/s, 21.5052s/100 iter), loss = 0.307627
I0629 17:04:03.396229  8090 solver.cpp:371]     Train net output #0: loss = 0.307626 (* 1 = 0.307626 loss)
I0629 17:04:03.396234  8090 sgd_solver.cpp:137] Iteration 2600, lr = 0.0001, m = 0.9
I0629 17:04:12.089462  8098 data_reader.cpp:262] Starting prefetch of epoch 16
I0629 17:04:12.090234  8069 data_reader.cpp:262] Starting prefetch of epoch 16
I0629 17:04:12.092098  8096 data_reader.cpp:262] Starting prefetch of epoch 16
I0629 17:04:12.113636  8067 data_reader.cpp:262] Starting prefetch of epoch 16
I0629 17:04:12.115149  8094 data_reader.cpp:262] Starting prefetch of epoch 16
I0629 17:04:12.121791  8093 data_reader.cpp:262] Starting prefetch of epoch 16
I0629 17:04:24.607753  8090 solver.cpp:349] Iteration 2700 (4.7144 iter/s, 21.2116s/100 iter), loss = 0.358646
I0629 17:04:24.607782  8090 solver.cpp:371]     Train net output #0: loss = 0.358646 (* 1 = 0.358646 loss)
I0629 17:04:24.607790  8090 sgd_solver.cpp:137] Iteration 2700, lr = 0.0001, m = 0.9
I0629 17:04:46.100888  8090 solver.cpp:349] Iteration 2800 (4.65264 iter/s, 21.4932s/100 iter), loss = 0.364985
I0629 17:04:46.100999  8090 solver.cpp:371]     Train net output #0: loss = 0.364985 (* 1 = 0.364985 loss)
I0629 17:04:46.101007  8090 sgd_solver.cpp:137] Iteration 2800, lr = 0.0001, m = 0.9
I0629 17:04:47.427412  8098 data_reader.cpp:262] Starting prefetch of epoch 17
I0629 17:04:47.428220  8096 data_reader.cpp:262] Starting prefetch of epoch 17
I0629 17:04:47.439743  8069 data_reader.cpp:262] Starting prefetch of epoch 17
I0629 17:04:47.450343  8094 data_reader.cpp:262] Starting prefetch of epoch 17
I0629 17:04:47.453438  8093 data_reader.cpp:262] Starting prefetch of epoch 17
I0629 17:04:47.454272  8067 data_reader.cpp:262] Starting prefetch of epoch 17
I0629 17:05:07.534293  8090 solver.cpp:349] Iteration 2900 (4.66562 iter/s, 21.4334s/100 iter), loss = 0.408395
I0629 17:05:07.534317  8090 solver.cpp:371]     Train net output #0: loss = 0.408395 (* 1 = 0.408395 loss)
I0629 17:05:07.534322  8090 sgd_solver.cpp:137] Iteration 2900, lr = 0.0001, m = 0.9
I0629 17:05:22.735896  8069 data_reader.cpp:262] Starting prefetch of epoch 18
I0629 17:05:22.737241  8098 data_reader.cpp:262] Starting prefetch of epoch 18
I0629 17:05:22.741883  8096 data_reader.cpp:262] Starting prefetch of epoch 18
I0629 17:05:22.753824  8094 data_reader.cpp:262] Starting prefetch of epoch 18
I0629 17:05:22.766096  8067 data_reader.cpp:262] Starting prefetch of epoch 18
I0629 17:05:22.768427  8093 data_reader.cpp:262] Starting prefetch of epoch 18
I0629 17:05:28.925400  8090 solver.cpp:349] Iteration 3000 (4.67489 iter/s, 21.3909s/100 iter), loss = 0.350086
I0629 17:05:28.925423  8090 solver.cpp:371]     Train net output #0: loss = 0.350085 (* 1 = 0.350085 loss)
I0629 17:05:28.925427  8090 sgd_solver.cpp:137] Iteration 3000, lr = 0.0001, m = 0.9
I0629 17:05:50.286135  8090 solver.cpp:349] Iteration 3100 (4.68153 iter/s, 21.3605s/100 iter), loss = 0.325465
I0629 17:05:50.286159  8090 solver.cpp:371]     Train net output #0: loss = 0.325465 (* 1 = 0.325465 loss)
I0629 17:05:50.286162  8090 sgd_solver.cpp:137] Iteration 3100, lr = 0.0001, m = 0.9
I0629 17:05:58.151732  8069 data_reader.cpp:262] Starting prefetch of epoch 19
I0629 17:05:58.158036  8098 data_reader.cpp:262] Starting prefetch of epoch 19
I0629 17:05:58.158810  8096 data_reader.cpp:262] Starting prefetch of epoch 19
I0629 17:05:58.168215  8093 data_reader.cpp:262] Starting prefetch of epoch 19
I0629 17:05:58.175482  8067 data_reader.cpp:262] Starting prefetch of epoch 19
I0629 17:05:58.179363  8094 data_reader.cpp:262] Starting prefetch of epoch 19
I0629 17:06:11.572450  8090 solver.cpp:349] Iteration 3200 (4.6979 iter/s, 21.2861s/100 iter), loss = 0.458303
I0629 17:06:11.572474  8090 solver.cpp:371]     Train net output #0: loss = 0.458303 (* 1 = 0.458303 loss)
I0629 17:06:11.572477  8090 sgd_solver.cpp:137] Iteration 3200, lr = 0.0001, m = 0.9
I0629 17:06:32.878262  8090 solver.cpp:349] Iteration 3300 (4.69362 iter/s, 21.3055s/100 iter), loss = 0.397602
I0629 17:06:32.878310  8090 solver.cpp:371]     Train net output #0: loss = 0.397602 (* 1 = 0.397602 loss)
I0629 17:06:32.878317  8090 sgd_solver.cpp:137] Iteration 3300, lr = 0.0001, m = 0.9
I0629 17:06:33.312947  8069 data_reader.cpp:262] Starting prefetch of epoch 20
I0629 17:06:33.320641  8098 data_reader.cpp:262] Starting prefetch of epoch 20
I0629 17:06:33.323632  8096 data_reader.cpp:262] Starting prefetch of epoch 20
I0629 17:06:33.336199  8067 data_reader.cpp:262] Starting prefetch of epoch 20
I0629 17:06:33.345158  8094 data_reader.cpp:262] Starting prefetch of epoch 20
I0629 17:06:33.345554  8093 data_reader.cpp:262] Starting prefetch of epoch 20
I0629 17:06:54.355859  8090 solver.cpp:349] Iteration 3400 (4.65613 iter/s, 21.4771s/100 iter), loss = 0.277623
I0629 17:06:54.355881  8090 solver.cpp:371]     Train net output #0: loss = 0.277623 (* 1 = 0.277623 loss)
I0629 17:06:54.355886  8090 sgd_solver.cpp:137] Iteration 3400, lr = 0.0001, m = 0.9
I0629 17:07:08.687886  8098 data_reader.cpp:262] Starting prefetch of epoch 21
I0629 17:07:08.700909  8069 data_reader.cpp:262] Starting prefetch of epoch 21
I0629 17:07:08.701159  8096 data_reader.cpp:262] Starting prefetch of epoch 21
I0629 17:07:08.704751  8094 data_reader.cpp:262] Starting prefetch of epoch 21
I0629 17:07:08.714455  8067 data_reader.cpp:262] Starting prefetch of epoch 21
I0629 17:07:08.718665  8093 data_reader.cpp:262] Starting prefetch of epoch 21
I0629 17:07:15.721856  8090 solver.cpp:349] Iteration 3500 (4.68043 iter/s, 21.3655s/100 iter), loss = 0.500329
I0629 17:07:15.721884  8090 solver.cpp:371]     Train net output #0: loss = 0.500329 (* 1 = 0.500329 loss)
I0629 17:07:15.721889  8090 sgd_solver.cpp:137] Iteration 3500, lr = 0.0001, m = 0.9
I0629 17:07:36.971341  8090 solver.cpp:349] Iteration 3600 (4.70609 iter/s, 21.2491s/100 iter), loss = 0.2805
I0629 17:07:36.971380  8090 solver.cpp:371]     Train net output #0: loss = 0.2805 (* 1 = 0.2805 loss)
I0629 17:07:36.971388  8090 sgd_solver.cpp:137] Iteration 3600, lr = 0.0001, m = 0.9
I0629 17:07:44.145514  8069 data_reader.cpp:262] Starting prefetch of epoch 22
I0629 17:07:44.149400  8098 data_reader.cpp:262] Starting prefetch of epoch 22
I0629 17:07:44.153245  8096 data_reader.cpp:262] Starting prefetch of epoch 22
I0629 17:07:44.153897  8067 data_reader.cpp:262] Starting prefetch of epoch 22
I0629 17:07:44.165640  8094 data_reader.cpp:262] Starting prefetch of epoch 22
I0629 17:07:44.167891  8093 data_reader.cpp:262] Starting prefetch of epoch 22
I0629 17:07:58.455557  8090 solver.cpp:349] Iteration 3700 (4.65467 iter/s, 21.4838s/100 iter), loss = 0.319518
I0629 17:07:58.455585  8090 solver.cpp:371]     Train net output #0: loss = 0.319518 (* 1 = 0.319518 loss)
I0629 17:07:58.455591  8090 sgd_solver.cpp:137] Iteration 3700, lr = 0.0001, m = 0.9
I0629 17:08:19.446071  8069 data_reader.cpp:262] Starting prefetch of epoch 23
I0629 17:08:19.454810  8096 data_reader.cpp:262] Starting prefetch of epoch 23
I0629 17:08:19.462537  8098 data_reader.cpp:262] Starting prefetch of epoch 23
I0629 17:08:19.469514  8093 data_reader.cpp:262] Starting prefetch of epoch 23
I0629 17:08:19.481068  8094 data_reader.cpp:262] Starting prefetch of epoch 23
I0629 17:08:19.482141  8067 data_reader.cpp:262] Starting prefetch of epoch 23
I0629 17:08:19.862587  8090 solver.cpp:349] Iteration 3800 (4.67144 iter/s, 21.4067s/100 iter), loss = 0.426813
I0629 17:08:19.862617  8090 solver.cpp:371]     Train net output #0: loss = 0.426813 (* 1 = 0.426813 loss)
I0629 17:08:19.862623  8090 sgd_solver.cpp:137] Iteration 3800, lr = 0.0001, m = 0.9
I0629 17:08:41.190232  8090 solver.cpp:349] Iteration 3900 (4.68882 iter/s, 21.3273s/100 iter), loss = 0.380519
I0629 17:08:41.190254  8090 solver.cpp:371]     Train net output #0: loss = 0.380519 (* 1 = 0.380519 loss)
I0629 17:08:41.190258  8090 sgd_solver.cpp:137] Iteration 3900, lr = 0.0001, m = 0.9
I0629 17:08:54.749598  8098 data_reader.cpp:262] Starting prefetch of epoch 24
I0629 17:08:54.754940  8069 data_reader.cpp:262] Starting prefetch of epoch 24
I0629 17:08:54.758621  8096 data_reader.cpp:262] Starting prefetch of epoch 24
I0629 17:08:54.769389  8093 data_reader.cpp:262] Starting prefetch of epoch 24
I0629 17:08:54.774965  8094 data_reader.cpp:262] Starting prefetch of epoch 24
I0629 17:08:54.780241  8067 data_reader.cpp:262] Starting prefetch of epoch 24
I0629 17:09:02.521195  8090 solver.cpp:545] Iteration 4000, Testing net (#0)
I0629 17:09:09.760792  8119 data_reader.cpp:262] Starting prefetch of epoch 2
I0629 17:09:09.760792  8128 data_reader.cpp:262] Starting prefetch of epoch 2
I0629 17:09:09.760805  8088 data_reader.cpp:262] Starting prefetch of epoch 2
I0629 17:09:09.888967  8117 data_reader.cpp:262] Starting prefetch of epoch 2
I0629 17:09:09.888967  8126 data_reader.cpp:262] Starting prefetch of epoch 2
I0629 17:09:09.889998  8086 data_reader.cpp:262] Starting prefetch of epoch 2
I0629 17:09:23.186178  8088 data_reader.cpp:262] Starting prefetch of epoch 3
I0629 17:09:23.310540  8086 data_reader.cpp:262] Starting prefetch of epoch 3
I0629 17:09:23.600281  8119 data_reader.cpp:262] Starting prefetch of epoch 3
I0629 17:09:23.604647  8117 data_reader.cpp:262] Starting prefetch of epoch 3
I0629 17:09:23.637933  8128 data_reader.cpp:262] Starting prefetch of epoch 3
I0629 17:09:23.642455  8126 data_reader.cpp:262] Starting prefetch of epoch 3
I0629 17:09:24.300849  8090 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.894169
I0629 17:09:24.300879  8090 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.986165
I0629 17:09:24.300884  8090 solver.cpp:630]     Test net output #2: loss = 0.34284 (* 1 = 0.34284 loss)
I0629 17:09:24.300909  8090 solver.cpp:305] [MultiGPU] Tests completed in 21.7795s
I0629 17:09:24.511699  8090 solver.cpp:349] Iteration 4000 (2.30835 iter/s, 43.321s/100 iter), loss = 0.312166
I0629 17:09:24.511723  8090 solver.cpp:371]     Train net output #0: loss = 0.312166 (* 1 = 0.312166 loss)
I0629 17:09:24.511729  8090 sgd_solver.cpp:137] Iteration 4000, lr = 0.0001, m = 0.9
I0629 17:09:45.686336  8090 solver.cpp:349] Iteration 4100 (4.72268 iter/s, 21.1744s/100 iter), loss = 0.300508
I0629 17:09:45.686401  8090 solver.cpp:371]     Train net output #0: loss = 0.300508 (* 1 = 0.300508 loss)
I0629 17:09:45.686406  8090 sgd_solver.cpp:137] Iteration 4100, lr = 0.0001, m = 0.9
I0629 17:09:51.751102  8069 data_reader.cpp:262] Starting prefetch of epoch 25
I0629 17:09:51.751760  8098 data_reader.cpp:262] Starting prefetch of epoch 25
I0629 17:09:51.759040  8096 data_reader.cpp:262] Starting prefetch of epoch 25
I0629 17:09:51.776732  8094 data_reader.cpp:262] Starting prefetch of epoch 25
I0629 17:09:51.782169  8067 data_reader.cpp:262] Starting prefetch of epoch 25
I0629 17:09:51.786471  8093 data_reader.cpp:262] Starting prefetch of epoch 25
I0629 17:10:07.148231  8090 solver.cpp:349] Iteration 4200 (4.65948 iter/s, 21.4616s/100 iter), loss = 0.294427
I0629 17:10:07.148255  8090 solver.cpp:371]     Train net output #0: loss = 0.294427 (* 1 = 0.294427 loss)
I0629 17:10:07.148259  8090 sgd_solver.cpp:137] Iteration 4200, lr = 0.0001, m = 0.9
I0629 17:10:27.144557  8098 data_reader.cpp:262] Starting prefetch of epoch 26
I0629 17:10:27.147074  8069 data_reader.cpp:262] Starting prefetch of epoch 26
I0629 17:10:27.150512  8096 data_reader.cpp:262] Starting prefetch of epoch 26
I0629 17:10:27.164638  8067 data_reader.cpp:262] Starting prefetch of epoch 26
I0629 17:10:27.169924  8093 data_reader.cpp:262] Starting prefetch of epoch 26
I0629 17:10:27.171670  8094 data_reader.cpp:262] Starting prefetch of epoch 26
I0629 17:10:31.828353  8090 solver.cpp:349] Iteration 4300 (4.05188 iter/s, 24.6799s/100 iter), loss = 0.360325
I0629 17:10:31.828380  8090 solver.cpp:371]     Train net output #0: loss = 0.360325 (* 1 = 0.360325 loss)
I0629 17:10:31.828384  8090 sgd_solver.cpp:137] Iteration 4300, lr = 0.0001, m = 0.9
I0629 17:10:53.644371  8090 solver.cpp:349] Iteration 4400 (4.58383 iter/s, 21.8158s/100 iter), loss = 0.267552
I0629 17:10:53.644397  8090 solver.cpp:371]     Train net output #0: loss = 0.267552 (* 1 = 0.267552 loss)
I0629 17:10:53.644402  8090 sgd_solver.cpp:137] Iteration 4400, lr = 0.0001, m = 0.9
I0629 17:11:06.380195  8098 data_reader.cpp:262] Starting prefetch of epoch 27
I0629 17:11:06.380784  8069 data_reader.cpp:262] Starting prefetch of epoch 27
I0629 17:11:06.381590  8096 data_reader.cpp:262] Starting prefetch of epoch 27
I0629 17:11:06.405025  8094 data_reader.cpp:262] Starting prefetch of epoch 27
I0629 17:11:06.408073  8067 data_reader.cpp:262] Starting prefetch of epoch 27
I0629 17:11:06.409339  8093 data_reader.cpp:262] Starting prefetch of epoch 27
I0629 17:11:26.008738  8090 solver.cpp:349] Iteration 4500 (3.08984 iter/s, 32.3641s/100 iter), loss = 0.263615
I0629 17:11:26.008764  8090 solver.cpp:371]     Train net output #0: loss = 0.263615 (* 1 = 0.263615 loss)
I0629 17:11:26.008769  8090 sgd_solver.cpp:137] Iteration 4500, lr = 0.0001, m = 0.9
I0629 17:11:47.288245  8090 solver.cpp:349] Iteration 4600 (4.69939 iter/s, 21.2794s/100 iter), loss = 0.36455
I0629 17:11:47.288310  8090 solver.cpp:371]     Train net output #0: loss = 0.36455 (* 1 = 0.36455 loss)
I0629 17:11:47.288316  8090 sgd_solver.cpp:137] Iteration 4600, lr = 0.0001, m = 0.9
I0629 17:11:52.442956  8069 data_reader.cpp:262] Starting prefetch of epoch 28
I0629 17:11:52.449242  8098 data_reader.cpp:262] Starting prefetch of epoch 28
I0629 17:11:52.449242  8096 data_reader.cpp:262] Starting prefetch of epoch 28
I0629 17:11:52.472553  8067 data_reader.cpp:262] Starting prefetch of epoch 28
I0629 17:11:52.472553  8093 data_reader.cpp:262] Starting prefetch of epoch 28
I0629 17:11:52.482545  8094 data_reader.cpp:262] Starting prefetch of epoch 28
I0629 17:12:09.819027  8090 solver.cpp:349] Iteration 4700 (4.43841 iter/s, 22.5306s/100 iter), loss = 0.454798
I0629 17:12:09.819062  8090 solver.cpp:371]     Train net output #0: loss = 0.454798 (* 1 = 0.454798 loss)
I0629 17:12:09.819067  8090 sgd_solver.cpp:137] Iteration 4700, lr = 0.0001, m = 0.9
I0629 17:12:29.104929  8069 data_reader.cpp:262] Starting prefetch of epoch 29
I0629 17:12:29.110919  8093 data_reader.cpp:262] Starting prefetch of epoch 29
I0629 17:12:29.119360  8067 data_reader.cpp:262] Starting prefetch of epoch 29
I0629 17:12:29.120324  8096 data_reader.cpp:262] Starting prefetch of epoch 29
I0629 17:12:29.116147  8098 data_reader.cpp:262] Starting prefetch of epoch 29
I0629 17:12:29.112949  8094 data_reader.cpp:262] Starting prefetch of epoch 29
I0629 17:12:31.227363  8090 solver.cpp:349] Iteration 4800 (4.6711 iter/s, 21.4082s/100 iter), loss = 0.446331
I0629 17:12:31.227391  8090 solver.cpp:371]     Train net output #0: loss = 0.446331 (* 1 = 0.446331 loss)
I0629 17:12:31.227396  8090 sgd_solver.cpp:137] Iteration 4800, lr = 0.0001, m = 0.9
I0629 17:12:52.680688  8090 solver.cpp:349] Iteration 4900 (4.6613 iter/s, 21.4532s/100 iter), loss = 0.241485
I0629 17:12:52.680712  8090 solver.cpp:371]     Train net output #0: loss = 0.241485 (* 1 = 0.241485 loss)
I0629 17:12:52.680716  8090 sgd_solver.cpp:137] Iteration 4900, lr = 0.0001, m = 0.9
I0629 17:13:04.507159  8096 data_reader.cpp:262] Starting prefetch of epoch 30
I0629 17:13:04.510771  8098 data_reader.cpp:262] Starting prefetch of epoch 30
I0629 17:13:04.514219  8069 data_reader.cpp:262] Starting prefetch of epoch 30
I0629 17:13:04.525427  8093 data_reader.cpp:262] Starting prefetch of epoch 30
I0629 17:13:04.529669  8094 data_reader.cpp:262] Starting prefetch of epoch 30
I0629 17:13:04.533462  8067 data_reader.cpp:262] Starting prefetch of epoch 30
I0629 17:13:14.158092  8090 solver.cpp:349] Iteration 5000 (4.65608 iter/s, 21.4773s/100 iter), loss = 0.371622
I0629 17:13:14.158118  8090 solver.cpp:371]     Train net output #0: loss = 0.371622 (* 1 = 0.371622 loss)
I0629 17:13:14.158123  8090 sgd_solver.cpp:137] Iteration 5000, lr = 0.0001, m = 0.9
I0629 17:13:35.602609  8090 solver.cpp:349] Iteration 5100 (4.66321 iter/s, 21.4444s/100 iter), loss = 0.40812
I0629 17:13:35.602686  8090 solver.cpp:371]     Train net output #0: loss = 0.40812 (* 1 = 0.40812 loss)
I0629 17:13:35.602691  8090 sgd_solver.cpp:137] Iteration 5100, lr = 0.0001, m = 0.9
I0629 17:13:40.187943  8096 data_reader.cpp:262] Starting prefetch of epoch 31
I0629 17:13:40.188535  8098 data_reader.cpp:262] Starting prefetch of epoch 31
I0629 17:13:40.187943  8069 data_reader.cpp:262] Starting prefetch of epoch 31
I0629 17:13:40.349947  8067 data_reader.cpp:262] Starting prefetch of epoch 31
I0629 17:13:40.350793  8093 data_reader.cpp:262] Starting prefetch of epoch 31
I0629 17:13:40.350793  8094 data_reader.cpp:262] Starting prefetch of epoch 31
I0629 17:14:11.415657  8090 solver.cpp:349] Iteration 5200 (2.79229 iter/s, 35.8129s/100 iter), loss = 0.438007
I0629 17:14:11.418864  8090 solver.cpp:371]     Train net output #0: loss = 0.438007 (* 1 = 0.438007 loss)
I0629 17:14:11.418879  8090 sgd_solver.cpp:137] Iteration 5200, lr = 0.0001, m = 0.9
I0629 17:14:34.808871  8096 data_reader.cpp:262] Starting prefetch of epoch 32
I0629 17:14:34.809870  8069 data_reader.cpp:262] Starting prefetch of epoch 32
I0629 17:14:34.810493  8098 data_reader.cpp:262] Starting prefetch of epoch 32
I0629 17:14:34.902699  8067 data_reader.cpp:262] Starting prefetch of epoch 32
I0629 17:14:34.905351  8093 data_reader.cpp:262] Starting prefetch of epoch 32
I0629 17:14:34.910650  8094 data_reader.cpp:262] Starting prefetch of epoch 32
I0629 17:14:37.830400  8090 solver.cpp:349] Iteration 5300 (3.78623 iter/s, 26.4115s/100 iter), loss = 0.197436
I0629 17:14:37.830425  8090 solver.cpp:371]     Train net output #0: loss = 0.197436 (* 1 = 0.197436 loss)
I0629 17:14:37.830430  8090 sgd_solver.cpp:137] Iteration 5300, lr = 0.0001, m = 0.9
I0629 17:14:59.152854  8090 solver.cpp:349] Iteration 5400 (4.68995 iter/s, 21.3222s/100 iter), loss = 0.266897
I0629 17:14:59.152930  8090 solver.cpp:371]     Train net output #0: loss = 0.266897 (* 1 = 0.266897 loss)
I0629 17:14:59.152935  8090 sgd_solver.cpp:137] Iteration 5400, lr = 0.0001, m = 0.9
I0629 17:15:10.061275  8069 data_reader.cpp:262] Starting prefetch of epoch 33
I0629 17:15:10.061362  8098 data_reader.cpp:262] Starting prefetch of epoch 33
I0629 17:15:10.065248  8096 data_reader.cpp:262] Starting prefetch of epoch 33
I0629 17:15:10.082058  8094 data_reader.cpp:262] Starting prefetch of epoch 33
I0629 17:15:10.083043  8093 data_reader.cpp:262] Starting prefetch of epoch 33
I0629 17:15:10.091164  8067 data_reader.cpp:262] Starting prefetch of epoch 33
I0629 17:15:20.545701  8090 solver.cpp:349] Iteration 5500 (4.6746 iter/s, 21.3922s/100 iter), loss = 0.225055
I0629 17:15:20.545766  8090 solver.cpp:371]     Train net output #0: loss = 0.225055 (* 1 = 0.225055 loss)
I0629 17:15:20.545786  8090 sgd_solver.cpp:137] Iteration 5500, lr = 0.0001, m = 0.9
I0629 17:15:41.880339  8090 solver.cpp:349] Iteration 5600 (4.68735 iter/s, 21.334s/100 iter), loss = 0.259806
I0629 17:15:41.880388  8090 solver.cpp:371]     Train net output #0: loss = 0.259806 (* 1 = 0.259806 loss)
I0629 17:15:41.880394  8090 sgd_solver.cpp:137] Iteration 5600, lr = 0.0001, m = 0.9
I0629 17:15:45.323420  8069 data_reader.cpp:262] Starting prefetch of epoch 34
I0629 17:15:45.324477  8096 data_reader.cpp:262] Starting prefetch of epoch 34
I0629 17:15:45.327337  8098 data_reader.cpp:262] Starting prefetch of epoch 34
I0629 17:15:45.337607  8067 data_reader.cpp:262] Starting prefetch of epoch 34
I0629 17:15:45.341740  8094 data_reader.cpp:262] Starting prefetch of epoch 34
I0629 17:15:45.354581  8093 data_reader.cpp:262] Starting prefetch of epoch 34
I0629 17:16:03.343538  8090 solver.cpp:349] Iteration 5700 (4.65929 iter/s, 21.4625s/100 iter), loss = 0.194126
I0629 17:16:03.343564  8090 solver.cpp:371]     Train net output #0: loss = 0.194126 (* 1 = 0.194126 loss)
I0629 17:16:03.343567  8090 sgd_solver.cpp:137] Iteration 5700, lr = 0.0001, m = 0.9
I0629 17:16:20.613240  8096 data_reader.cpp:262] Starting prefetch of epoch 35
I0629 17:16:20.618151  8098 data_reader.cpp:262] Starting prefetch of epoch 35
I0629 17:16:20.627475  8069 data_reader.cpp:262] Starting prefetch of epoch 35
I0629 17:16:20.637948  8094 data_reader.cpp:262] Starting prefetch of epoch 35
I0629 17:16:20.652899  8093 data_reader.cpp:262] Starting prefetch of epoch 35
I0629 17:16:20.653606  8067 data_reader.cpp:262] Starting prefetch of epoch 35
I0629 17:16:24.618173  8090 solver.cpp:349] Iteration 5800 (4.70065 iter/s, 21.2737s/100 iter), loss = 0.281127
I0629 17:16:24.618203  8090 solver.cpp:371]     Train net output #0: loss = 0.281127 (* 1 = 0.281127 loss)
I0629 17:16:24.618209  8090 sgd_solver.cpp:137] Iteration 5800, lr = 0.0001, m = 0.9
I0629 17:16:45.954908  8090 solver.cpp:349] Iteration 5900 (4.68695 iter/s, 21.3358s/100 iter), loss = 0.309301
I0629 17:16:45.954931  8090 solver.cpp:371]     Train net output #0: loss = 0.309301 (* 1 = 0.309301 loss)
I0629 17:16:45.954936  8090 sgd_solver.cpp:137] Iteration 5900, lr = 0.0001, m = 0.9
I0629 17:16:55.765306  8098 data_reader.cpp:262] Starting prefetch of epoch 36
I0629 17:16:55.773645  8096 data_reader.cpp:262] Starting prefetch of epoch 36
I0629 17:16:55.793797  8069 data_reader.cpp:262] Starting prefetch of epoch 36
I0629 17:16:55.795717  8067 data_reader.cpp:262] Starting prefetch of epoch 36
I0629 17:16:55.797384  8093 data_reader.cpp:262] Starting prefetch of epoch 36
I0629 17:16:55.799037  8094 data_reader.cpp:262] Starting prefetch of epoch 36
I0629 17:17:07.103466  8090 solver.cpp:545] Iteration 6000, Testing net (#0)
I0629 17:17:25.993172  8128 data_reader.cpp:262] Starting prefetch of epoch 4
I0629 17:17:25.993198  8119 data_reader.cpp:262] Starting prefetch of epoch 4
I0629 17:17:25.993172  8088 data_reader.cpp:262] Starting prefetch of epoch 4
I0629 17:17:26.197950  8117 data_reader.cpp:262] Starting prefetch of epoch 4
I0629 17:17:26.197950  8126 data_reader.cpp:262] Starting prefetch of epoch 4
I0629 17:17:26.197950  8086 data_reader.cpp:262] Starting prefetch of epoch 4
I0629 17:17:35.318642  8090 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.907196
I0629 17:17:35.318665  8090 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.98971
I0629 17:17:35.318670  8090 solver.cpp:630]     Test net output #2: loss = 0.300958 (* 1 = 0.300958 loss)
I0629 17:17:35.318724  8090 solver.cpp:305] [MultiGPU] Tests completed in 28.2138s
I0629 17:17:35.537156  8090 solver.cpp:349] Iteration 6000 (2.01695 iter/s, 49.5798s/100 iter), loss = 0.560826
I0629 17:17:35.537181  8090 solver.cpp:371]     Train net output #0: loss = 0.560826 (* 1 = 0.560826 loss)
I0629 17:17:35.537186  8090 sgd_solver.cpp:137] Iteration 6000, lr = 0.0001, m = 0.9
I0629 17:17:56.826633  8090 solver.cpp:349] Iteration 6100 (4.69739 iter/s, 21.2884s/100 iter), loss = 0.290444
I0629 17:17:56.826680  8090 solver.cpp:371]     Train net output #0: loss = 0.290444 (* 1 = 0.290444 loss)
I0629 17:17:56.826686  8090 sgd_solver.cpp:137] Iteration 6100, lr = 0.0001, m = 0.9
I0629 17:17:59.433126  8098 data_reader.cpp:262] Starting prefetch of epoch 37
I0629 17:17:59.443953  8096 data_reader.cpp:262] Starting prefetch of epoch 37
I0629 17:17:59.449275  8069 data_reader.cpp:262] Starting prefetch of epoch 37
I0629 17:17:59.457613  8094 data_reader.cpp:262] Starting prefetch of epoch 37
I0629 17:17:59.464990  8093 data_reader.cpp:262] Starting prefetch of epoch 37
I0629 17:17:59.466015  8067 data_reader.cpp:262] Starting prefetch of epoch 37
I0629 17:18:18.295112  8090 solver.cpp:349] Iteration 6200 (4.65821 iter/s, 21.4675s/100 iter), loss = 0.224009
I0629 17:18:18.295137  8090 solver.cpp:371]     Train net output #0: loss = 0.224009 (* 1 = 0.224009 loss)
I0629 17:18:18.295140  8090 sgd_solver.cpp:137] Iteration 6200, lr = 0.0001, m = 0.9
I0629 17:18:34.740936  8098 data_reader.cpp:262] Starting prefetch of epoch 38
I0629 17:18:34.747794  8096 data_reader.cpp:262] Starting prefetch of epoch 38
I0629 17:18:34.759356  8069 data_reader.cpp:262] Starting prefetch of epoch 38
I0629 17:18:34.767572  8093 data_reader.cpp:262] Starting prefetch of epoch 38
I0629 17:18:34.769532  8067 data_reader.cpp:262] Starting prefetch of epoch 38
I0629 17:18:34.770979  8094 data_reader.cpp:262] Starting prefetch of epoch 38
I0629 17:18:39.580821  8090 solver.cpp:349] Iteration 6300 (4.69819 iter/s, 21.2848s/100 iter), loss = 0.290426
I0629 17:18:39.580842  8090 solver.cpp:371]     Train net output #0: loss = 0.290427 (* 1 = 0.290427 loss)
I0629 17:18:39.580847  8090 sgd_solver.cpp:137] Iteration 6300, lr = 0.0001, m = 0.9
I0629 17:19:00.993544  8090 solver.cpp:349] Iteration 6400 (4.6703 iter/s, 21.4119s/100 iter), loss = 0.32207
I0629 17:19:00.993566  8090 solver.cpp:371]     Train net output #0: loss = 0.32207 (* 1 = 0.32207 loss)
I0629 17:19:00.993571  8090 sgd_solver.cpp:137] Iteration 6400, lr = 0.0001, m = 0.9
I0629 17:19:09.978984  8069 data_reader.cpp:262] Starting prefetch of epoch 39
I0629 17:19:09.983670  8098 data_reader.cpp:262] Starting prefetch of epoch 39
I0629 17:19:09.983670  8096 data_reader.cpp:262] Starting prefetch of epoch 39
I0629 17:19:10.000586  8067 data_reader.cpp:262] Starting prefetch of epoch 39
I0629 17:19:10.008191  8094 data_reader.cpp:262] Starting prefetch of epoch 39
I0629 17:19:10.009318  8093 data_reader.cpp:262] Starting prefetch of epoch 39
I0629 17:19:22.354178  8090 solver.cpp:349] Iteration 6500 (4.68168 iter/s, 21.3598s/100 iter), loss = 0.321067
I0629 17:19:22.354207  8090 solver.cpp:371]     Train net output #0: loss = 0.321067 (* 1 = 0.321067 loss)
I0629 17:19:22.354210  8090 sgd_solver.cpp:137] Iteration 6500, lr = 0.0001, m = 0.9
I0629 17:19:43.669605  8090 solver.cpp:349] Iteration 6600 (4.6916 iter/s, 21.3147s/100 iter), loss = 0.400282
I0629 17:19:43.669667  8090 solver.cpp:371]     Train net output #0: loss = 0.400282 (* 1 = 0.400282 loss)
I0629 17:19:43.669672  8090 sgd_solver.cpp:137] Iteration 6600, lr = 0.0001, m = 0.9
I0629 17:19:45.359334  8069 data_reader.cpp:262] Starting prefetch of epoch 40
I0629 17:19:45.359928  8098 data_reader.cpp:262] Starting prefetch of epoch 40
I0629 17:19:45.364225  8093 data_reader.cpp:262] Starting prefetch of epoch 40
I0629 17:19:45.366914  8096 data_reader.cpp:262] Starting prefetch of epoch 40
I0629 17:19:45.375396  8094 data_reader.cpp:262] Starting prefetch of epoch 40
I0629 17:19:45.384697  8067 data_reader.cpp:262] Starting prefetch of epoch 40
I0629 17:20:05.015054  8090 solver.cpp:349] Iteration 6700 (4.685 iter/s, 21.3447s/100 iter), loss = 0.333078
I0629 17:20:05.015077  8090 solver.cpp:371]     Train net output #0: loss = 0.333078 (* 1 = 0.333078 loss)
I0629 17:20:05.015081  8090 sgd_solver.cpp:137] Iteration 6700, lr = 0.0001, m = 0.9
I0629 17:20:20.594473  8069 data_reader.cpp:262] Starting prefetch of epoch 41
I0629 17:20:20.599766  8096 data_reader.cpp:262] Starting prefetch of epoch 41
I0629 17:20:20.608145  8098 data_reader.cpp:262] Starting prefetch of epoch 41
I0629 17:20:20.623276  8094 data_reader.cpp:262] Starting prefetch of epoch 41
I0629 17:20:20.627311  8093 data_reader.cpp:262] Starting prefetch of epoch 41
I0629 17:20:20.630764  8067 data_reader.cpp:262] Starting prefetch of epoch 41
I0629 17:20:26.341454  8090 solver.cpp:349] Iteration 6800 (4.68916 iter/s, 21.3258s/100 iter), loss = 0.15871
I0629 17:20:26.341477  8090 solver.cpp:371]     Train net output #0: loss = 0.15871 (* 1 = 0.15871 loss)
I0629 17:20:26.341482  8090 sgd_solver.cpp:137] Iteration 6800, lr = 0.0001, m = 0.9
I0629 17:20:47.652298  8090 solver.cpp:349] Iteration 6900 (4.69257 iter/s, 21.3103s/100 iter), loss = 0.256527
I0629 17:20:47.652321  8090 solver.cpp:371]     Train net output #0: loss = 0.256527 (* 1 = 0.256527 loss)
I0629 17:20:47.652326  8090 sgd_solver.cpp:137] Iteration 6900, lr = 0.0001, m = 0.9
I0629 17:20:55.801841  8098 data_reader.cpp:262] Starting prefetch of epoch 42
I0629 17:20:55.807569  8069 data_reader.cpp:262] Starting prefetch of epoch 42
I0629 17:20:55.814389  8096 data_reader.cpp:262] Starting prefetch of epoch 42
I0629 17:20:55.818603  8094 data_reader.cpp:262] Starting prefetch of epoch 42
I0629 17:20:55.837281  8067 data_reader.cpp:262] Starting prefetch of epoch 42
I0629 17:20:55.837924  8093 data_reader.cpp:262] Starting prefetch of epoch 42
I0629 17:21:09.024147  8090 solver.cpp:349] Iteration 7000 (4.67917 iter/s, 21.3713s/100 iter), loss = 0.367459
I0629 17:21:09.024170  8090 solver.cpp:371]     Train net output #0: loss = 0.367459 (* 1 = 0.367459 loss)
I0629 17:21:09.024174  8090 sgd_solver.cpp:137] Iteration 7000, lr = 0.0001, m = 0.9
I0629 17:21:30.405879  8090 solver.cpp:349] Iteration 7100 (4.677 iter/s, 21.3812s/100 iter), loss = 0.235045
I0629 17:21:30.405928  8090 solver.cpp:371]     Train net output #0: loss = 0.235045 (* 1 = 0.235045 loss)
I0629 17:21:30.405933  8090 sgd_solver.cpp:137] Iteration 7100, lr = 0.0001, m = 0.9
I0629 17:21:31.050730  8096 data_reader.cpp:262] Starting prefetch of epoch 43
I0629 17:21:31.051031  8069 data_reader.cpp:262] Starting prefetch of epoch 43
I0629 17:21:31.057799  8098 data_reader.cpp:262] Starting prefetch of epoch 43
I0629 17:21:31.076174  8093 data_reader.cpp:262] Starting prefetch of epoch 43
I0629 17:21:31.080557  8094 data_reader.cpp:262] Starting prefetch of epoch 43
I0629 17:21:31.084899  8067 data_reader.cpp:262] Starting prefetch of epoch 43
I0629 17:21:51.683641  8090 solver.cpp:349] Iteration 7200 (4.69985 iter/s, 21.2773s/100 iter), loss = 0.360312
I0629 17:21:51.683666  8090 solver.cpp:371]     Train net output #0: loss = 0.360312 (* 1 = 0.360312 loss)
I0629 17:21:51.683671  8090 sgd_solver.cpp:137] Iteration 7200, lr = 0.0001, m = 0.9
I0629 17:22:06.392562  8096 data_reader.cpp:262] Starting prefetch of epoch 44
I0629 17:22:06.397733  8098 data_reader.cpp:262] Starting prefetch of epoch 44
I0629 17:22:06.397733  8069 data_reader.cpp:262] Starting prefetch of epoch 44
I0629 17:22:06.416960  8067 data_reader.cpp:262] Starting prefetch of epoch 44
I0629 17:22:06.421579  8093 data_reader.cpp:262] Starting prefetch of epoch 44
I0629 17:22:06.419684  8094 data_reader.cpp:262] Starting prefetch of epoch 44
I0629 17:22:13.006721  8090 solver.cpp:349] Iteration 7300 (4.68985 iter/s, 21.3226s/100 iter), loss = 0.24965
I0629 17:22:13.006745  8090 solver.cpp:371]     Train net output #0: loss = 0.24965 (* 1 = 0.24965 loss)
I0629 17:22:13.006750  8090 sgd_solver.cpp:137] Iteration 7300, lr = 0.0001, m = 0.9
I0629 17:22:34.438406  8090 solver.cpp:349] Iteration 7400 (4.6661 iter/s, 21.4312s/100 iter), loss = 0.40221
I0629 17:22:34.438428  8090 solver.cpp:371]     Train net output #0: loss = 0.40221 (* 1 = 0.40221 loss)
I0629 17:22:34.438432  8090 sgd_solver.cpp:137] Iteration 7400, lr = 0.0001, m = 0.9
I0629 17:22:41.723805  8069 data_reader.cpp:262] Starting prefetch of epoch 45
I0629 17:22:41.730156  8096 data_reader.cpp:262] Starting prefetch of epoch 45
I0629 17:22:41.736353  8098 data_reader.cpp:262] Starting prefetch of epoch 45
I0629 17:22:41.749475  8067 data_reader.cpp:262] Starting prefetch of epoch 45
I0629 17:22:41.751164  8093 data_reader.cpp:262] Starting prefetch of epoch 45
I0629 17:22:41.762899  8094 data_reader.cpp:262] Starting prefetch of epoch 45
I0629 17:22:55.863502  8090 solver.cpp:349] Iteration 7500 (4.66759 iter/s, 21.4243s/100 iter), loss = 0.312454
I0629 17:22:55.863531  8090 solver.cpp:371]     Train net output #0: loss = 0.312454 (* 1 = 0.312454 loss)
I0629 17:22:55.863538  8090 sgd_solver.cpp:137] Iteration 7500, lr = 0.0001, m = 0.9
I0629 17:23:17.096149  8069 data_reader.cpp:262] Starting prefetch of epoch 46
I0629 17:23:17.099189  8096 data_reader.cpp:262] Starting prefetch of epoch 46
I0629 17:23:17.101421  8098 data_reader.cpp:262] Starting prefetch of epoch 46
I0629 17:23:17.114292  8093 data_reader.cpp:262] Starting prefetch of epoch 46
I0629 17:23:17.122068  8094 data_reader.cpp:262] Starting prefetch of epoch 46
I0629 17:23:17.128756  8067 data_reader.cpp:262] Starting prefetch of epoch 46
I0629 17:23:17.293314  8090 solver.cpp:349] Iteration 7600 (4.66656 iter/s, 21.4291s/100 iter), loss = 0.227678
I0629 17:23:17.293334  8090 solver.cpp:371]     Train net output #0: loss = 0.227678 (* 1 = 0.227678 loss)
I0629 17:23:17.293339  8090 sgd_solver.cpp:137] Iteration 7600, lr = 0.0001, m = 0.9
I0629 17:23:38.775286  8090 solver.cpp:349] Iteration 7700 (4.65522 iter/s, 21.4813s/100 iter), loss = 0.204266
I0629 17:23:38.775310  8090 solver.cpp:371]     Train net output #0: loss = 0.204266 (* 1 = 0.204266 loss)
I0629 17:23:38.775315  8090 sgd_solver.cpp:137] Iteration 7700, lr = 0.0001, m = 0.9
I0629 17:23:52.685029  8069 data_reader.cpp:262] Starting prefetch of epoch 47
I0629 17:23:52.692013  8098 data_reader.cpp:262] Starting prefetch of epoch 47
I0629 17:23:52.693277  8094 data_reader.cpp:262] Starting prefetch of epoch 47
I0629 17:23:52.696965  8093 data_reader.cpp:262] Starting prefetch of epoch 47
I0629 17:23:52.703465  8067 data_reader.cpp:262] Starting prefetch of epoch 47
I0629 17:23:52.703523  8096 data_reader.cpp:262] Starting prefetch of epoch 47
I0629 17:24:00.117002  8090 solver.cpp:349] Iteration 7800 (4.68581 iter/s, 21.341s/100 iter), loss = 0.214627
I0629 17:24:00.117023  8090 solver.cpp:371]     Train net output #0: loss = 0.214627 (* 1 = 0.214627 loss)
I0629 17:24:00.117027  8090 sgd_solver.cpp:137] Iteration 7800, lr = 0.0001, m = 0.9
I0629 17:24:21.374181  8090 solver.cpp:349] Iteration 7900 (4.70444 iter/s, 21.2565s/100 iter), loss = 0.23711
I0629 17:24:21.374202  8090 solver.cpp:371]     Train net output #0: loss = 0.23711 (* 1 = 0.23711 loss)
I0629 17:24:21.374207  8090 sgd_solver.cpp:137] Iteration 7900, lr = 0.0001, m = 0.9
I0629 17:24:27.747844  8098 data_reader.cpp:262] Starting prefetch of epoch 48
I0629 17:24:27.753315  8096 data_reader.cpp:262] Starting prefetch of epoch 48
I0629 17:24:27.754673  8069 data_reader.cpp:262] Starting prefetch of epoch 48
I0629 17:24:27.772425  8093 data_reader.cpp:262] Starting prefetch of epoch 48
I0629 17:24:27.772425  8094 data_reader.cpp:262] Starting prefetch of epoch 48
I0629 17:24:27.772783  8067 data_reader.cpp:262] Starting prefetch of epoch 48
I0629 17:24:42.507472  8090 solver.cpp:545] Iteration 8000, Testing net (#0)
I0629 17:24:49.134276  8088 data_reader.cpp:262] Starting prefetch of epoch 5
I0629 17:24:49.138770  8086 data_reader.cpp:262] Starting prefetch of epoch 5
I0629 17:24:49.174247  8119 data_reader.cpp:262] Starting prefetch of epoch 5
I0629 17:24:49.179039  8117 data_reader.cpp:262] Starting prefetch of epoch 5
I0629 17:24:49.183606  8128 data_reader.cpp:262] Starting prefetch of epoch 5
I0629 17:24:49.188103  8126 data_reader.cpp:262] Starting prefetch of epoch 5
I0629 17:25:02.258074  8086 data_reader.cpp:262] Starting prefetch of epoch 6
I0629 17:25:02.266134  8088 data_reader.cpp:262] Starting prefetch of epoch 6
I0629 17:25:02.793733  8128 data_reader.cpp:262] Starting prefetch of epoch 6
I0629 17:25:02.798174  8126 data_reader.cpp:262] Starting prefetch of epoch 6
I0629 17:25:02.889366  8119 data_reader.cpp:262] Starting prefetch of epoch 6
I0629 17:25:02.893649  8117 data_reader.cpp:262] Starting prefetch of epoch 6
I0629 17:25:03.553910  8090 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.899868
I0629 17:25:03.553939  8090 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.989119
I0629 17:25:03.553947  8090 solver.cpp:630]     Test net output #2: loss = 0.319278 (* 1 = 0.319278 loss)
I0629 17:25:03.553977  8090 solver.cpp:305] [MultiGPU] Tests completed in 21.0459s
I0629 17:25:03.776247  8090 solver.cpp:349] Iteration 8000 (2.35844 iter/s, 42.4008s/100 iter), loss = 0.205412
I0629 17:25:03.776274  8090 solver.cpp:371]     Train net output #0: loss = 0.205412 (* 1 = 0.205412 loss)
I0629 17:25:03.776280  8090 sgd_solver.cpp:137] Iteration 8000, lr = 0.0001, m = 0.9
I0629 17:25:13.913705  8068 blocking_queue.cpp:40] Waiting for datum
I0629 17:25:35.939653  8069 data_reader.cpp:262] Starting prefetch of epoch 49
I0629 17:25:35.947368  8096 data_reader.cpp:262] Starting prefetch of epoch 49
I0629 17:25:35.949090  8098 data_reader.cpp:262] Starting prefetch of epoch 49
I0629 17:25:35.969336  8093 data_reader.cpp:262] Starting prefetch of epoch 49
I0629 17:25:35.972277  8094 data_reader.cpp:262] Starting prefetch of epoch 49
I0629 17:25:35.973795  8067 data_reader.cpp:262] Starting prefetch of epoch 49
I0629 17:25:36.983461  8090 solver.cpp:349] Iteration 8100 (3.01148 iter/s, 33.2063s/100 iter), loss = 0.653348
I0629 17:25:36.983486  8090 solver.cpp:371]     Train net output #0: loss = 0.653348 (* 1 = 0.653348 loss)
I0629 17:25:36.983491  8090 sgd_solver.cpp:137] Iteration 8100, lr = 0.0001, m = 0.9
I0629 17:27:15.672849  8090 solver.cpp:349] Iteration 8200 (1.0133 iter/s, 98.6871s/100 iter), loss = 0.251196
I0629 17:27:15.672924  8090 solver.cpp:371]     Train net output #0: loss = 0.251196 (* 1 = 0.251196 loss)
I0629 17:27:15.672930  8090 sgd_solver.cpp:137] Iteration 8200, lr = 0.0001, m = 0.9
I0629 17:27:50.887152  8069 data_reader.cpp:262] Starting prefetch of epoch 50
I0629 17:27:50.887152  8096 data_reader.cpp:262] Starting prefetch of epoch 50
I0629 17:27:50.888619  8098 data_reader.cpp:262] Starting prefetch of epoch 50
I0629 17:27:51.192334  8093 data_reader.cpp:262] Starting prefetch of epoch 50
I0629 17:27:51.192734  8067 data_reader.cpp:262] Starting prefetch of epoch 50
I0629 17:27:51.197777  8094 data_reader.cpp:262] Starting prefetch of epoch 50
I0629 17:27:59.785015  8090 solver.cpp:349] Iteration 8300 (2.267 iter/s, 44.1112s/100 iter), loss = 0.293508
I0629 17:27:59.785039  8090 solver.cpp:371]     Train net output #0: loss = 0.293508 (* 1 = 0.293508 loss)
I0629 17:27:59.785043  8090 sgd_solver.cpp:137] Iteration 8300, lr = 0.0001, m = 0.9
I0629 17:28:20.898366  8090 solver.cpp:349] Iteration 8400 (4.73644 iter/s, 21.1129s/100 iter), loss = 0.344916
I0629 17:28:20.898486  8090 solver.cpp:371]     Train net output #0: loss = 0.344916 (* 1 = 0.344916 loss)
I0629 17:28:20.898494  8090 sgd_solver.cpp:137] Iteration 8400, lr = 0.0001, m = 0.9
I0629 17:28:26.473774  8096 data_reader.cpp:262] Starting prefetch of epoch 51
I0629 17:28:26.479631  8098 data_reader.cpp:262] Starting prefetch of epoch 51
I0629 17:28:26.479823  8069 data_reader.cpp:262] Starting prefetch of epoch 51
I0629 17:28:26.491633  8093 data_reader.cpp:262] Starting prefetch of epoch 51
I0629 17:28:26.496238  8067 data_reader.cpp:262] Starting prefetch of epoch 51
I0629 17:28:26.508472  8094 data_reader.cpp:262] Starting prefetch of epoch 51
I0629 17:28:42.252904  8090 solver.cpp:349] Iteration 8500 (4.68296 iter/s, 21.354s/100 iter), loss = 0.224521
I0629 17:28:42.252929  8090 solver.cpp:371]     Train net output #0: loss = 0.224521 (* 1 = 0.224521 loss)
I0629 17:28:42.252934  8090 sgd_solver.cpp:137] Iteration 8500, lr = 0.0001, m = 0.9
I0629 17:29:01.744256  8098 data_reader.cpp:262] Starting prefetch of epoch 52
I0629 17:29:01.759225  8096 data_reader.cpp:262] Starting prefetch of epoch 52
I0629 17:29:01.760463  8069 data_reader.cpp:262] Starting prefetch of epoch 52
I0629 17:29:01.767161  8093 data_reader.cpp:262] Starting prefetch of epoch 52
I0629 17:29:01.772414  8067 data_reader.cpp:262] Starting prefetch of epoch 52
I0629 17:29:01.773335  8094 data_reader.cpp:262] Starting prefetch of epoch 52
I0629 17:29:03.649057  8090 solver.cpp:349] Iteration 8600 (4.67383 iter/s, 21.3957s/100 iter), loss = 0.33158
I0629 17:29:03.649077  8090 solver.cpp:371]     Train net output #0: loss = 0.33158 (* 1 = 0.33158 loss)
I0629 17:29:03.649082  8090 sgd_solver.cpp:137] Iteration 8600, lr = 0.0001, m = 0.9
I0629 17:29:24.947248  8090 solver.cpp:349] Iteration 8700 (4.69532 iter/s, 21.2978s/100 iter), loss = 0.320764
I0629 17:29:24.947273  8090 solver.cpp:371]     Train net output #0: loss = 0.320764 (* 1 = 0.320764 loss)
I0629 17:29:24.947276  8090 sgd_solver.cpp:137] Iteration 8700, lr = 0.0001, m = 0.9
I0629 17:29:36.870466  8098 data_reader.cpp:262] Starting prefetch of epoch 53
I0629 17:29:36.874358  8096 data_reader.cpp:262] Starting prefetch of epoch 53
I0629 17:29:36.888401  8067 data_reader.cpp:262] Starting prefetch of epoch 53
I0629 17:29:36.889317  8069 data_reader.cpp:262] Starting prefetch of epoch 53
I0629 17:29:36.898794  8094 data_reader.cpp:262] Starting prefetch of epoch 53
I0629 17:29:36.899348  8093 data_reader.cpp:262] Starting prefetch of epoch 53
I0629 17:29:46.244668  8090 solver.cpp:349] Iteration 8800 (4.69549 iter/s, 21.297s/100 iter), loss = 0.227627
I0629 17:29:46.244693  8090 solver.cpp:371]     Train net output #0: loss = 0.227627 (* 1 = 0.227627 loss)
I0629 17:29:46.244696  8090 sgd_solver.cpp:137] Iteration 8800, lr = 0.0001, m = 0.9
I0629 17:30:07.495241  8090 solver.cpp:349] Iteration 8900 (4.70584 iter/s, 21.2502s/100 iter), loss = 0.221742
I0629 17:30:07.495286  8090 solver.cpp:371]     Train net output #0: loss = 0.221742 (* 1 = 0.221742 loss)
I0629 17:30:07.495292  8090 sgd_solver.cpp:137] Iteration 8900, lr = 0.0001, m = 0.9
I0629 17:30:12.070982  8069 data_reader.cpp:262] Starting prefetch of epoch 54
I0629 17:30:12.070982  8098 data_reader.cpp:262] Starting prefetch of epoch 54
I0629 17:30:12.072407  8096 data_reader.cpp:262] Starting prefetch of epoch 54
I0629 17:30:12.099504  8094 data_reader.cpp:262] Starting prefetch of epoch 54
I0629 17:30:12.101198  8093 data_reader.cpp:262] Starting prefetch of epoch 54
I0629 17:30:12.102058  8067 data_reader.cpp:262] Starting prefetch of epoch 54
I0629 17:30:28.875577  8090 solver.cpp:349] Iteration 9000 (4.67728 iter/s, 21.3799s/100 iter), loss = 0.184251
I0629 17:30:28.875605  8090 solver.cpp:371]     Train net output #0: loss = 0.184252 (* 1 = 0.184252 loss)
I0629 17:30:28.875612  8090 sgd_solver.cpp:137] Iteration 9000, lr = 0.0001, m = 0.9
I0629 17:30:47.406339  8069 data_reader.cpp:262] Starting prefetch of epoch 55
I0629 17:30:47.409495  8098 data_reader.cpp:262] Starting prefetch of epoch 55
I0629 17:30:47.415329  8067 data_reader.cpp:262] Starting prefetch of epoch 55
I0629 17:30:47.424629  8096 data_reader.cpp:262] Starting prefetch of epoch 55
I0629 17:30:47.425806  8094 data_reader.cpp:262] Starting prefetch of epoch 55
I0629 17:30:47.431020  8093 data_reader.cpp:262] Starting prefetch of epoch 55
I0629 17:30:50.166637  8090 solver.cpp:349] Iteration 9100 (4.69689 iter/s, 21.2907s/100 iter), loss = 0.329157
I0629 17:30:50.166661  8090 solver.cpp:371]     Train net output #0: loss = 0.329157 (* 1 = 0.329157 loss)
I0629 17:30:50.166666  8090 sgd_solver.cpp:137] Iteration 9100, lr = 0.0001, m = 0.9
I0629 17:31:11.377637  8090 solver.cpp:349] Iteration 9200 (4.71461 iter/s, 21.2106s/100 iter), loss = 0.239751
I0629 17:31:11.377662  8090 solver.cpp:371]     Train net output #0: loss = 0.239751 (* 1 = 0.239751 loss)
I0629 17:31:11.377666  8090 sgd_solver.cpp:137] Iteration 9200, lr = 0.0001, m = 0.9
I0629 17:31:22.557355  8096 data_reader.cpp:262] Starting prefetch of epoch 56
I0629 17:31:22.565668  8069 data_reader.cpp:262] Starting prefetch of epoch 56
I0629 17:31:22.566561  8098 data_reader.cpp:262] Starting prefetch of epoch 56
I0629 17:31:22.589139  8067 data_reader.cpp:262] Starting prefetch of epoch 56
I0629 17:31:22.589505  8093 data_reader.cpp:262] Starting prefetch of epoch 56
I0629 17:31:22.591316  8094 data_reader.cpp:262] Starting prefetch of epoch 56
I0629 17:31:32.736608  8090 solver.cpp:349] Iteration 9300 (4.68195 iter/s, 21.3586s/100 iter), loss = 0.307757
I0629 17:31:32.736629  8090 solver.cpp:371]     Train net output #0: loss = 0.307757 (* 1 = 0.307757 loss)
I0629 17:31:32.736634  8090 sgd_solver.cpp:137] Iteration 9300, lr = 0.0001, m = 0.9
I0629 17:31:54.166887  8090 solver.cpp:349] Iteration 9400 (4.66637 iter/s, 21.4299s/100 iter), loss = 0.284792
I0629 17:31:54.166939  8090 solver.cpp:371]     Train net output #0: loss = 0.284793 (* 1 = 0.284793 loss)
I0629 17:31:54.166944  8090 sgd_solver.cpp:137] Iteration 9400, lr = 0.0001, m = 0.9
I0629 17:31:57.773057  8098 data_reader.cpp:262] Starting prefetch of epoch 57
I0629 17:31:57.774942  8069 data_reader.cpp:262] Starting prefetch of epoch 57
I0629 17:31:57.789890  8067 data_reader.cpp:262] Starting prefetch of epoch 57
I0629 17:31:57.792244  8096 data_reader.cpp:262] Starting prefetch of epoch 57
I0629 17:31:57.796488  8093 data_reader.cpp:262] Starting prefetch of epoch 57
I0629 17:31:57.803761  8094 data_reader.cpp:262] Starting prefetch of epoch 57
I0629 17:32:15.403261  8090 solver.cpp:349] Iteration 9500 (4.70898 iter/s, 21.236s/100 iter), loss = 0.263381
I0629 17:32:15.403287  8090 solver.cpp:371]     Train net output #0: loss = 0.263381 (* 1 = 0.263381 loss)
I0629 17:32:15.403293  8090 sgd_solver.cpp:137] Iteration 9500, lr = 0.0001, m = 0.9
I0629 17:32:33.108489  8096 data_reader.cpp:262] Starting prefetch of epoch 58
I0629 17:32:33.110777  8098 data_reader.cpp:262] Starting prefetch of epoch 58
I0629 17:32:33.121304  8069 data_reader.cpp:262] Starting prefetch of epoch 58
I0629 17:32:33.124161  8094 data_reader.cpp:262] Starting prefetch of epoch 58
I0629 17:32:33.125342  8093 data_reader.cpp:262] Starting prefetch of epoch 58
I0629 17:32:33.142724  8067 data_reader.cpp:262] Starting prefetch of epoch 58
I0629 17:32:36.680661  8090 solver.cpp:349] Iteration 9600 (4.69989 iter/s, 21.2771s/100 iter), loss = 0.194423
I0629 17:32:36.680685  8090 solver.cpp:371]     Train net output #0: loss = 0.194423 (* 1 = 0.194423 loss)
I0629 17:32:36.680690  8090 sgd_solver.cpp:137] Iteration 9600, lr = 0.0001, m = 0.9
I0629 17:32:58.193162  8090 solver.cpp:349] Iteration 9700 (4.64853 iter/s, 21.5122s/100 iter), loss = 0.249706
I0629 17:32:58.193184  8090 solver.cpp:371]     Train net output #0: loss = 0.249706 (* 1 = 0.249706 loss)
I0629 17:32:58.193188  8090 sgd_solver.cpp:137] Iteration 9700, lr = 0.0001, m = 0.9
I0629 17:33:08.469373  8098 data_reader.cpp:262] Starting prefetch of epoch 59
I0629 17:33:08.471591  8069 data_reader.cpp:262] Starting prefetch of epoch 59
I0629 17:33:08.472254  8096 data_reader.cpp:262] Starting prefetch of epoch 59
I0629 17:33:08.494035  8093 data_reader.cpp:262] Starting prefetch of epoch 59
I0629 17:33:08.497628  8094 data_reader.cpp:262] Starting prefetch of epoch 59
I0629 17:33:08.505024  8067 data_reader.cpp:262] Starting prefetch of epoch 59
I0629 17:33:19.490639  8090 solver.cpp:349] Iteration 9800 (4.69546 iter/s, 21.2972s/100 iter), loss = 0.19051
I0629 17:33:19.490665  8090 solver.cpp:371]     Train net output #0: loss = 0.190511 (* 1 = 0.190511 loss)
I0629 17:33:19.490669  8090 sgd_solver.cpp:137] Iteration 9800, lr = 0.0001, m = 0.9
I0629 17:33:40.767948  8090 solver.cpp:349] Iteration 9900 (4.69991 iter/s, 21.277s/100 iter), loss = 0.428527
I0629 17:33:40.768000  8090 solver.cpp:371]     Train net output #0: loss = 0.428527 (* 1 = 0.428527 loss)
I0629 17:33:40.768005  8090 sgd_solver.cpp:137] Iteration 9900, lr = 0.0001, m = 0.9
I0629 17:33:43.588356  8098 data_reader.cpp:262] Starting prefetch of epoch 60
I0629 17:33:43.590688  8096 data_reader.cpp:262] Starting prefetch of epoch 60
I0629 17:33:43.591233  8069 data_reader.cpp:262] Starting prefetch of epoch 60
I0629 17:33:43.607630  8067 data_reader.cpp:262] Starting prefetch of epoch 60
I0629 17:33:43.611366  8093 data_reader.cpp:262] Starting prefetch of epoch 60
I0629 17:33:43.615412  8094 data_reader.cpp:262] Starting prefetch of epoch 60
I0629 17:34:02.073922  8090 solver.cpp:675] Snapshotting to binary proto file training/cityscapes20_jsegnet21v2_2017-06-29_16-47-23/initial/cityscapes20_jsegnet21v2_iter_10000.caffemodel
I0629 17:34:02.611894  8090 sgd_solver.cpp:288] Snapshotting solver state to binary proto file training/cityscapes20_jsegnet21v2_2017-06-29_16-47-23/initial/cityscapes20_jsegnet21v2_iter_10000.solverstate
I0629 17:34:02.620964  8090 solver.cpp:545] Iteration 10000, Testing net (#0)
I0629 17:34:24.314818  8128 data_reader.cpp:262] Starting prefetch of epoch 7
I0629 17:34:24.314818  8088 data_reader.cpp:262] Starting prefetch of epoch 7
I0629 17:34:24.314860  8119 data_reader.cpp:262] Starting prefetch of epoch 7
I0629 17:34:24.619832  8086 data_reader.cpp:262] Starting prefetch of epoch 7
I0629 17:34:24.619832  8117 data_reader.cpp:262] Starting prefetch of epoch 7
I0629 17:34:24.619832  8126 data_reader.cpp:262] Starting prefetch of epoch 7
I0629 17:34:33.053117  8090 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.917934
I0629 17:34:33.053138  8090 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.992018
I0629 17:34:33.053144  8090 solver.cpp:630]     Test net output #2: loss = 0.264508 (* 1 = 0.264508 loss)
I0629 17:34:33.053169  8090 solver.cpp:305] [MultiGPU] Tests completed in 30.4319s
I0629 17:34:33.270647  8090 solver.cpp:349] Iteration 10000 (1.90469 iter/s, 52.502s/100 iter), loss = 0.224498
I0629 17:34:33.270674  8090 solver.cpp:371]     Train net output #0: loss = 0.224498 (* 1 = 0.224498 loss)
I0629 17:34:33.270681  8090 sgd_solver.cpp:137] Iteration 10000, lr = 0.0001, m = 0.9
I0629 17:34:49.752457  8098 data_reader.cpp:262] Starting prefetch of epoch 61
I0629 17:34:49.759228  8069 data_reader.cpp:262] Starting prefetch of epoch 61
I0629 17:34:49.761009  8096 data_reader.cpp:262] Starting prefetch of epoch 61
I0629 17:34:49.781123  8067 data_reader.cpp:262] Starting prefetch of epoch 61
I0629 17:34:49.784653  8093 data_reader.cpp:262] Starting prefetch of epoch 61
I0629 17:34:49.786142  8094 data_reader.cpp:262] Starting prefetch of epoch 61
I0629 17:34:54.425585  8090 solver.cpp:349] Iteration 10100 (4.72709 iter/s, 21.1547s/100 iter), loss = 0.230953
I0629 17:34:54.425632  8090 solver.cpp:371]     Train net output #0: loss = 0.230954 (* 1 = 0.230954 loss)
I0629 17:34:54.425639  8090 sgd_solver.cpp:137] Iteration 10100, lr = 0.0001, m = 0.9
I0629 17:35:15.795713  8090 solver.cpp:349] Iteration 10200 (4.67949 iter/s, 21.3698s/100 iter), loss = 0.673546
I0629 17:35:15.795737  8090 solver.cpp:371]     Train net output #0: loss = 0.673546 (* 1 = 0.673546 loss)
I0629 17:35:15.795742  8090 sgd_solver.cpp:137] Iteration 10200, lr = 0.0001, m = 0.9
I0629 17:35:25.192966  8069 data_reader.cpp:262] Starting prefetch of epoch 62
I0629 17:35:25.207518  8096 data_reader.cpp:262] Starting prefetch of epoch 62
I0629 17:35:25.210160  8094 data_reader.cpp:262] Starting prefetch of epoch 62
I0629 17:35:25.210913  8098 data_reader.cpp:262] Starting prefetch of epoch 62
I0629 17:35:25.218796  8067 data_reader.cpp:262] Starting prefetch of epoch 62
I0629 17:35:25.221575  8093 data_reader.cpp:262] Starting prefetch of epoch 62
I0629 17:35:37.272534  8090 solver.cpp:349] Iteration 10300 (4.65624 iter/s, 21.4766s/100 iter), loss = 0.214322
I0629 17:35:37.272562  8090 solver.cpp:371]     Train net output #0: loss = 0.214322 (* 1 = 0.214322 loss)
I0629 17:35:37.272565  8090 sgd_solver.cpp:137] Iteration 10300, lr = 0.0001, m = 0.9
I0629 17:35:58.679886  8090 solver.cpp:349] Iteration 10400 (4.67135 iter/s, 21.4071s/100 iter), loss = 0.172192
I0629 17:35:58.679981  8090 solver.cpp:371]     Train net output #0: loss = 0.172192 (* 1 = 0.172192 loss)
I0629 17:35:58.679991  8090 sgd_solver.cpp:137] Iteration 10400, lr = 0.0001, m = 0.9
I0629 17:36:00.660233  8098 data_reader.cpp:262] Starting prefetch of epoch 63
I0629 17:36:00.666653  8096 data_reader.cpp:262] Starting prefetch of epoch 63
I0629 17:36:00.676070  8069 data_reader.cpp:262] Starting prefetch of epoch 63
I0629 17:36:00.683861  8093 data_reader.cpp:262] Starting prefetch of epoch 63
I0629 17:36:00.693536  8094 data_reader.cpp:262] Starting prefetch of epoch 63
I0629 17:36:00.697281  8067 data_reader.cpp:262] Starting prefetch of epoch 63
I0629 17:36:20.071190  8090 solver.cpp:349] Iteration 10500 (4.67487 iter/s, 21.391s/100 iter), loss = 0.19032
I0629 17:36:20.071216  8090 solver.cpp:371]     Train net output #0: loss = 0.19032 (* 1 = 0.19032 loss)
I0629 17:36:20.071220  8090 sgd_solver.cpp:137] Iteration 10500, lr = 0.0001, m = 0.9
I0629 17:36:35.945842  8096 data_reader.cpp:262] Starting prefetch of epoch 64
I0629 17:36:35.951081  8094 data_reader.cpp:262] Starting prefetch of epoch 64
I0629 17:36:35.955487  8098 data_reader.cpp:262] Starting prefetch of epoch 64
I0629 17:36:35.960472  8069 data_reader.cpp:262] Starting prefetch of epoch 64
I0629 17:36:35.963729  8067 data_reader.cpp:262] Starting prefetch of epoch 64
I0629 17:36:35.966588  8093 data_reader.cpp:262] Starting prefetch of epoch 64
I0629 17:36:41.590502  8090 solver.cpp:349] Iteration 10600 (4.64704 iter/s, 21.5191s/100 iter), loss = 0.317703
I0629 17:36:41.590533  8090 solver.cpp:371]     Train net output #0: loss = 0.317703 (* 1 = 0.317703 loss)
I0629 17:36:41.590536  8090 sgd_solver.cpp:137] Iteration 10600, lr = 0.0001, m = 0.9
I0629 17:37:03.257424  8090 solver.cpp:349] Iteration 10700 (4.61538 iter/s, 21.6667s/100 iter), loss = 0.162104
I0629 17:37:03.257454  8090 solver.cpp:371]     Train net output #0: loss = 0.162104 (* 1 = 0.162104 loss)
I0629 17:37:03.257458  8090 sgd_solver.cpp:137] Iteration 10700, lr = 0.0001, m = 0.9
I0629 17:37:11.808943  8098 data_reader.cpp:262] Starting prefetch of epoch 65
I0629 17:37:11.810233  8096 data_reader.cpp:262] Starting prefetch of epoch 65
I0629 17:37:11.810233  8069 data_reader.cpp:262] Starting prefetch of epoch 65
I0629 17:37:11.821753  8093 data_reader.cpp:262] Starting prefetch of epoch 65
I0629 17:37:11.823156  8067 data_reader.cpp:262] Starting prefetch of epoch 65
I0629 17:37:11.831511  8094 data_reader.cpp:262] Starting prefetch of epoch 65
I0629 17:37:24.538377  8090 solver.cpp:349] Iteration 10800 (4.69909 iter/s, 21.2807s/100 iter), loss = 0.27865
I0629 17:37:24.538403  8090 solver.cpp:371]     Train net output #0: loss = 0.27865 (* 1 = 0.27865 loss)
I0629 17:37:24.538406  8090 sgd_solver.cpp:137] Iteration 10800, lr = 0.0001, m = 0.9
I0629 17:37:45.901617  8090 solver.cpp:349] Iteration 10900 (4.68099 iter/s, 21.363s/100 iter), loss = 0.146289
I0629 17:37:45.901695  8090 solver.cpp:371]     Train net output #0: loss = 0.146289 (* 1 = 0.146289 loss)
I0629 17:37:45.901702  8090 sgd_solver.cpp:137] Iteration 10900, lr = 0.0001, m = 0.9
I0629 17:37:46.970571  8098 data_reader.cpp:262] Starting prefetch of epoch 66
I0629 17:37:46.974673  8069 data_reader.cpp:262] Starting prefetch of epoch 66
I0629 17:37:46.977926  8096 data_reader.cpp:262] Starting prefetch of epoch 66
I0629 17:37:46.994330  8067 data_reader.cpp:262] Starting prefetch of epoch 66
I0629 17:37:46.996716  8094 data_reader.cpp:262] Starting prefetch of epoch 66
I0629 17:37:46.999389  8093 data_reader.cpp:262] Starting prefetch of epoch 66
I0629 17:38:07.530156  8090 solver.cpp:349] Iteration 11000 (4.62368 iter/s, 21.6278s/100 iter), loss = 0.299466
I0629 17:38:07.530180  8090 solver.cpp:371]     Train net output #0: loss = 0.299466 (* 1 = 0.299466 loss)
I0629 17:38:07.530184  8090 sgd_solver.cpp:137] Iteration 11000, lr = 0.0001, m = 0.9
I0629 17:38:22.667798  8096 data_reader.cpp:262] Starting prefetch of epoch 67
I0629 17:38:22.674015  8098 data_reader.cpp:262] Starting prefetch of epoch 67
I0629 17:38:22.678097  8069 data_reader.cpp:262] Starting prefetch of epoch 67
I0629 17:38:22.697257  8067 data_reader.cpp:262] Starting prefetch of epoch 67
I0629 17:38:22.697703  8094 data_reader.cpp:262] Starting prefetch of epoch 67
I0629 17:38:22.697814  8093 data_reader.cpp:262] Starting prefetch of epoch 67
I0629 17:38:29.044839  8090 solver.cpp:349] Iteration 11100 (4.64813 iter/s, 21.514s/100 iter), loss = 0.273481
I0629 17:38:29.044862  8090 solver.cpp:371]     Train net output #0: loss = 0.273481 (* 1 = 0.273481 loss)
I0629 17:38:29.044867  8090 sgd_solver.cpp:137] Iteration 11100, lr = 0.0001, m = 0.9
I0629 17:38:50.347460  8090 solver.cpp:349] Iteration 11200 (4.6944 iter/s, 21.302s/100 iter), loss = 0.334936
I0629 17:38:50.347481  8090 solver.cpp:371]     Train net output #0: loss = 0.334936 (* 1 = 0.334936 loss)
I0629 17:38:50.347486  8090 sgd_solver.cpp:137] Iteration 11200, lr = 0.0001, m = 0.9
I0629 17:38:57.883122  8069 data_reader.cpp:262] Starting prefetch of epoch 68
I0629 17:38:57.896505  8098 data_reader.cpp:262] Starting prefetch of epoch 68
I0629 17:38:57.897197  8096 data_reader.cpp:262] Starting prefetch of epoch 68
I0629 17:38:57.900610  8067 data_reader.cpp:262] Starting prefetch of epoch 68
I0629 17:38:57.911316  8093 data_reader.cpp:262] Starting prefetch of epoch 68
I0629 17:38:57.911422  8094 data_reader.cpp:262] Starting prefetch of epoch 68
I0629 17:39:11.728814  8090 solver.cpp:349] Iteration 11300 (4.67711 iter/s, 21.3807s/100 iter), loss = 0.209509
I0629 17:39:11.728839  8090 solver.cpp:371]     Train net output #0: loss = 0.209509 (* 1 = 0.209509 loss)
I0629 17:39:11.728844  8090 sgd_solver.cpp:137] Iteration 11300, lr = 0.0001, m = 0.9
I0629 17:39:32.912170  8090 solver.cpp:349] Iteration 11400 (4.72082 iter/s, 21.1828s/100 iter), loss = 0.199454
I0629 17:39:32.912219  8090 solver.cpp:371]     Train net output #0: loss = 0.199454 (* 1 = 0.199454 loss)
I0629 17:39:32.912225  8090 sgd_solver.cpp:137] Iteration 11400, lr = 0.0001, m = 0.9
I0629 17:39:33.131620  8069 data_reader.cpp:262] Starting prefetch of epoch 69
I0629 17:39:33.134302  8098 data_reader.cpp:262] Starting prefetch of epoch 69
I0629 17:39:33.143342  8096 data_reader.cpp:262] Starting prefetch of epoch 69
I0629 17:39:33.148818  8093 data_reader.cpp:262] Starting prefetch of epoch 69
I0629 17:39:33.151378  8094 data_reader.cpp:262] Starting prefetch of epoch 69
I0629 17:39:33.156760  8067 data_reader.cpp:262] Starting prefetch of epoch 69
I0629 17:39:54.427505  8090 solver.cpp:349] Iteration 11500 (4.64798 iter/s, 21.5147s/100 iter), loss = 0.29185
I0629 17:39:54.427525  8090 solver.cpp:371]     Train net output #0: loss = 0.29185 (* 1 = 0.29185 loss)
I0629 17:39:54.427528  8090 sgd_solver.cpp:137] Iteration 11500, lr = 0.0001, m = 0.9
I0629 17:40:08.484755  8098 data_reader.cpp:262] Starting prefetch of epoch 70
I0629 17:40:08.486464  8096 data_reader.cpp:262] Starting prefetch of epoch 70
I0629 17:40:08.486464  8069 data_reader.cpp:262] Starting prefetch of epoch 70
I0629 17:40:08.511167  8093 data_reader.cpp:262] Starting prefetch of epoch 70
I0629 17:40:08.511960  8094 data_reader.cpp:262] Starting prefetch of epoch 70
I0629 17:40:08.513240  8067 data_reader.cpp:262] Starting prefetch of epoch 70
I0629 17:40:15.723748  8090 solver.cpp:349] Iteration 11600 (4.69579 iter/s, 21.2957s/100 iter), loss = 0.248507
I0629 17:40:15.723772  8090 solver.cpp:371]     Train net output #0: loss = 0.248507 (* 1 = 0.248507 loss)
I0629 17:40:15.723775  8090 sgd_solver.cpp:137] Iteration 11600, lr = 0.0001, m = 0.9
I0629 17:40:37.083772  8090 solver.cpp:349] Iteration 11700 (4.68183 iter/s, 21.3592s/100 iter), loss = 0.178948
I0629 17:40:37.083802  8090 solver.cpp:371]     Train net output #0: loss = 0.178948 (* 1 = 0.178948 loss)
I0629 17:40:37.083811  8090 sgd_solver.cpp:137] Iteration 11700, lr = 0.0001, m = 0.9
I0629 17:40:43.666798  8098 data_reader.cpp:262] Starting prefetch of epoch 71
I0629 17:40:43.672057  8069 data_reader.cpp:262] Starting prefetch of epoch 71
I0629 17:40:43.679925  8096 data_reader.cpp:262] Starting prefetch of epoch 71
I0629 17:40:43.694447  8094 data_reader.cpp:262] Starting prefetch of epoch 71
I0629 17:40:43.696862  8067 data_reader.cpp:262] Starting prefetch of epoch 71
I0629 17:40:43.700199  8093 data_reader.cpp:262] Starting prefetch of epoch 71
I0629 17:40:58.346261  8090 solver.cpp:349] Iteration 11800 (4.70336 iter/s, 21.2614s/100 iter), loss = 0.215031
I0629 17:40:58.346285  8090 solver.cpp:371]     Train net output #0: loss = 0.215031 (* 1 = 0.215031 loss)
I0629 17:40:58.346289  8090 sgd_solver.cpp:137] Iteration 11800, lr = 0.0001, m = 0.9
I0629 17:41:18.875428  8096 data_reader.cpp:262] Starting prefetch of epoch 72
I0629 17:41:18.880798  8069 data_reader.cpp:262] Starting prefetch of epoch 72
I0629 17:41:18.882686  8098 data_reader.cpp:262] Starting prefetch of epoch 72
I0629 17:41:18.900806  8067 data_reader.cpp:262] Starting prefetch of epoch 72
I0629 17:41:18.907932  8094 data_reader.cpp:262] Starting prefetch of epoch 72
I0629 17:41:18.908285  8093 data_reader.cpp:262] Starting prefetch of epoch 72
I0629 17:41:19.731412  8090 solver.cpp:349] Iteration 11900 (4.67637 iter/s, 21.3841s/100 iter), loss = 0.435409
I0629 17:41:19.731437  8090 solver.cpp:371]     Train net output #0: loss = 0.435409 (* 1 = 0.435409 loss)
I0629 17:41:19.731441  8090 sgd_solver.cpp:137] Iteration 11900, lr = 0.0001, m = 0.9
I0629 17:41:40.826159  8090 solver.cpp:545] Iteration 12000, Testing net (#0)
I0629 17:41:48.222151  8088 data_reader.cpp:262] Starting prefetch of epoch 8
I0629 17:41:48.226608  8086 data_reader.cpp:262] Starting prefetch of epoch 8
I0629 17:41:48.256299  8119 data_reader.cpp:262] Starting prefetch of epoch 8
I0629 17:41:48.260578  8117 data_reader.cpp:262] Starting prefetch of epoch 8
I0629 17:41:48.286342  8128 data_reader.cpp:262] Starting prefetch of epoch 8
I0629 17:41:48.290366  8126 data_reader.cpp:262] Starting prefetch of epoch 8
I0629 17:42:06.935305  8128 data_reader.cpp:262] Starting prefetch of epoch 9
I0629 17:42:06.935305  8119 data_reader.cpp:262] Starting prefetch of epoch 9
I0629 17:42:06.935305  8088 data_reader.cpp:262] Starting prefetch of epoch 9
I0629 17:42:07.280499  8117 data_reader.cpp:262] Starting prefetch of epoch 9
I0629 17:42:07.280514  8126 data_reader.cpp:262] Starting prefetch of epoch 9
I0629 17:42:07.280530  8086 data_reader.cpp:262] Starting prefetch of epoch 9
I0629 17:42:07.862282  8090 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.919526
I0629 17:42:07.862305  8090 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.992759
I0629 17:42:07.862310  8090 solver.cpp:630]     Test net output #2: loss = 0.252958 (* 1 = 0.252958 loss)
I0629 17:42:07.862336  8090 solver.cpp:305] [MultiGPU] Tests completed in 27.035s
I0629 17:42:08.074015  8090 solver.cpp:349] Iteration 12000 (2.06866 iter/s, 48.3404s/100 iter), loss = 0.197893
I0629 17:42:08.074038  8090 solver.cpp:371]     Train net output #0: loss = 0.197893 (* 1 = 0.197893 loss)
I0629 17:42:08.074041  8090 sgd_solver.cpp:137] Iteration 12000, lr = 0.0001, m = 0.9
I0629 17:42:21.379710  8096 data_reader.cpp:262] Starting prefetch of epoch 73
I0629 17:42:21.384389  8069 data_reader.cpp:262] Starting prefetch of epoch 73
I0629 17:42:21.386436  8098 data_reader.cpp:262] Starting prefetch of epoch 73
I0629 17:42:21.400647  8094 data_reader.cpp:262] Starting prefetch of epoch 73
I0629 17:42:21.403918  8093 data_reader.cpp:262] Starting prefetch of epoch 73
I0629 17:42:21.413381  8067 data_reader.cpp:262] Starting prefetch of epoch 73
I0629 17:42:29.564491  8090 solver.cpp:349] Iteration 12100 (4.65343 iter/s, 21.4895s/100 iter), loss = 0.298381
I0629 17:42:29.564512  8090 solver.cpp:371]     Train net output #0: loss = 0.298382 (* 1 = 0.298382 loss)
I0629 17:42:29.564515  8090 sgd_solver.cpp:137] Iteration 12100, lr = 0.0001, m = 0.9
I0629 17:42:51.512871  8090 solver.cpp:349] Iteration 12200 (4.55634 iter/s, 21.9475s/100 iter), loss = 0.293254
I0629 17:42:51.512959  8090 solver.cpp:371]     Train net output #0: loss = 0.293254 (* 1 = 0.293254 loss)
I0629 17:42:51.512966  8090 sgd_solver.cpp:137] Iteration 12200, lr = 0.0001, m = 0.9
I0629 17:43:09.748170  8098 data_reader.cpp:262] Starting prefetch of epoch 74
I0629 17:43:09.748170  8069 data_reader.cpp:262] Starting prefetch of epoch 74
I0629 17:43:09.748170  8096 data_reader.cpp:262] Starting prefetch of epoch 74
I0629 17:43:10.426697  8067 data_reader.cpp:262] Starting prefetch of epoch 74
I0629 17:43:10.426697  8094 data_reader.cpp:262] Starting prefetch of epoch 74
I0629 17:43:10.426697  8093 data_reader.cpp:262] Starting prefetch of epoch 74
I0629 17:44:30.446431  8090 solver.cpp:349] Iteration 12300 (1.01082 iter/s, 98.9298s/100 iter), loss = 0.193144
I0629 17:44:30.446502  8090 solver.cpp:371]     Train net output #0: loss = 0.193144 (* 1 = 0.193144 loss)
I0629 17:44:30.446507  8090 sgd_solver.cpp:137] Iteration 12300, lr = 0.0001, m = 0.9
I0629 17:45:55.420660  8096 data_reader.cpp:262] Starting prefetch of epoch 75
I0629 17:45:55.420660  8098 data_reader.cpp:262] Starting prefetch of epoch 75
I0629 17:45:55.420660  8069 data_reader.cpp:262] Starting prefetch of epoch 75
I0629 17:45:55.729681  8067 data_reader.cpp:262] Starting prefetch of epoch 75
I0629 17:45:55.729681  8094 data_reader.cpp:262] Starting prefetch of epoch 75
I0629 17:45:55.729681  8093 data_reader.cpp:262] Starting prefetch of epoch 75
I0629 17:45:57.158090  8090 solver.cpp:349] Iteration 12400 (1.15329 iter/s, 86.7088s/100 iter), loss = 0.293046
I0629 17:45:57.158113  8090 solver.cpp:371]     Train net output #0: loss = 0.293046 (* 1 = 0.293046 loss)
I0629 17:45:57.158118  8090 sgd_solver.cpp:137] Iteration 12400, lr = 0.0001, m = 0.9
I0629 17:46:34.409905  8090 solver.cpp:349] Iteration 12500 (2.68451 iter/s, 37.2507s/100 iter), loss = 0.254161
I0629 17:46:34.410012  8090 solver.cpp:371]     Train net output #0: loss = 0.254161 (* 1 = 0.254161 loss)
I0629 17:46:34.410017  8090 sgd_solver.cpp:137] Iteration 12500, lr = 0.0001, m = 0.9
I0629 17:46:46.693645  8098 data_reader.cpp:262] Starting prefetch of epoch 76
I0629 17:46:46.701545  8096 data_reader.cpp:262] Starting prefetch of epoch 76
I0629 17:46:46.707479  8069 data_reader.cpp:262] Starting prefetch of epoch 76
I0629 17:46:46.714164  8067 data_reader.cpp:262] Starting prefetch of epoch 76
I0629 17:46:46.715536  8093 data_reader.cpp:262] Starting prefetch of epoch 76
I0629 17:46:46.726092  8094 data_reader.cpp:262] Starting prefetch of epoch 76
I0629 17:46:55.773753  8090 solver.cpp:349] Iteration 12600 (4.6811 iter/s, 21.3625s/100 iter), loss = 0.120426
I0629 17:46:55.773777  8090 solver.cpp:371]     Train net output #0: loss = 0.120426 (* 1 = 0.120426 loss)
I0629 17:46:55.773782  8090 sgd_solver.cpp:137] Iteration 12600, lr = 0.0001, m = 0.9
I0629 17:47:17.129261  8090 solver.cpp:349] Iteration 12700 (4.68289 iter/s, 21.3543s/100 iter), loss = 0.351218
I0629 17:47:17.129353  8090 solver.cpp:371]     Train net output #0: loss = 0.351218 (* 1 = 0.351218 loss)
I0629 17:47:17.129357  8090 sgd_solver.cpp:137] Iteration 12700, lr = 0.0001, m = 0.9
I0629 17:47:22.030866  8069 data_reader.cpp:262] Starting prefetch of epoch 77
I0629 17:47:22.035650  8098 data_reader.cpp:262] Starting prefetch of epoch 77
I0629 17:47:22.040757  8096 data_reader.cpp:262] Starting prefetch of epoch 77
I0629 17:47:22.056597  8067 data_reader.cpp:262] Starting prefetch of epoch 77
I0629 17:47:22.061089  8093 data_reader.cpp:262] Starting prefetch of epoch 77
I0629 17:47:22.063081  8094 data_reader.cpp:262] Starting prefetch of epoch 77
I0629 17:47:38.463413  8090 solver.cpp:349] Iteration 12800 (4.68758 iter/s, 21.333s/100 iter), loss = 0.473047
I0629 17:47:38.463443  8090 solver.cpp:371]     Train net output #0: loss = 0.473048 (* 1 = 0.473048 loss)
I0629 17:47:38.463450  8090 sgd_solver.cpp:137] Iteration 12800, lr = 0.0001, m = 0.9
I0629 17:47:57.308827  8098 data_reader.cpp:262] Starting prefetch of epoch 78
I0629 17:47:57.309134  8069 data_reader.cpp:262] Starting prefetch of epoch 78
I0629 17:47:57.322032  8096 data_reader.cpp:262] Starting prefetch of epoch 78
I0629 17:47:57.327865  8094 data_reader.cpp:262] Starting prefetch of epoch 78
I0629 17:47:57.331197  8093 data_reader.cpp:262] Starting prefetch of epoch 78
I0629 17:47:57.334414  8067 data_reader.cpp:262] Starting prefetch of epoch 78
I0629 17:47:59.826213  8090 solver.cpp:349] Iteration 12900 (4.68127 iter/s, 21.3617s/100 iter), loss = 0.243708
I0629 17:47:59.826236  8090 solver.cpp:371]     Train net output #0: loss = 0.243708 (* 1 = 0.243708 loss)
I0629 17:47:59.826239  8090 sgd_solver.cpp:137] Iteration 12900, lr = 0.0001, m = 0.9
I0629 17:48:21.136533  8090 solver.cpp:349] Iteration 13000 (4.69279 iter/s, 21.3093s/100 iter), loss = 0.428738
I0629 17:48:21.136556  8090 solver.cpp:371]     Train net output #0: loss = 0.428738 (* 1 = 0.428738 loss)
I0629 17:48:21.136561  8090 sgd_solver.cpp:137] Iteration 13000, lr = 0.0001, m = 0.9
I0629 17:48:32.432049  8098 data_reader.cpp:262] Starting prefetch of epoch 79
I0629 17:48:32.438370  8096 data_reader.cpp:262] Starting prefetch of epoch 79
I0629 17:48:32.448520  8069 data_reader.cpp:262] Starting prefetch of epoch 79
I0629 17:48:32.455701  8094 data_reader.cpp:262] Starting prefetch of epoch 79
I0629 17:48:32.463142  8093 data_reader.cpp:262] Starting prefetch of epoch 79
I0629 17:48:32.463496  8067 data_reader.cpp:262] Starting prefetch of epoch 79
I0629 17:48:42.471313  8090 solver.cpp:349] Iteration 13100 (4.6874 iter/s, 21.3338s/100 iter), loss = 0.303354
I0629 17:48:42.471338  8090 solver.cpp:371]     Train net output #0: loss = 0.303354 (* 1 = 0.303354 loss)
I0629 17:48:42.471343  8090 sgd_solver.cpp:137] Iteration 13100, lr = 0.0001, m = 0.9
I0629 17:49:03.812389  8090 solver.cpp:349] Iteration 13200 (4.68597 iter/s, 21.3403s/100 iter), loss = 0.253745
I0629 17:49:03.812492  8090 solver.cpp:371]     Train net output #0: loss = 0.253745 (* 1 = 0.253745 loss)
I0629 17:49:03.812500  8090 sgd_solver.cpp:137] Iteration 13200, lr = 0.0001, m = 0.9
I0629 17:49:07.855074  8069 data_reader.cpp:262] Starting prefetch of epoch 80
I0629 17:49:07.860787  8098 data_reader.cpp:262] Starting prefetch of epoch 80
I0629 17:49:07.862110  8096 data_reader.cpp:262] Starting prefetch of epoch 80
I0629 17:49:07.869277  8094 data_reader.cpp:262] Starting prefetch of epoch 80
I0629 17:49:07.880270  8067 data_reader.cpp:262] Starting prefetch of epoch 80
I0629 17:49:07.884457  8093 data_reader.cpp:262] Starting prefetch of epoch 80
I0629 17:49:25.192481  8090 solver.cpp:349] Iteration 13300 (4.67741 iter/s, 21.3794s/100 iter), loss = 0.208967
I0629 17:49:25.192502  8090 solver.cpp:371]     Train net output #0: loss = 0.208967 (* 1 = 0.208967 loss)
I0629 17:49:25.192507  8090 sgd_solver.cpp:137] Iteration 13300, lr = 0.0001, m = 0.9
I0629 17:49:42.979879  8069 data_reader.cpp:262] Starting prefetch of epoch 81
I0629 17:49:42.994051  8096 data_reader.cpp:262] Starting prefetch of epoch 81
I0629 17:49:42.990233  8098 data_reader.cpp:262] Starting prefetch of epoch 81
I0629 17:49:43.009136  8094 data_reader.cpp:262] Starting prefetch of epoch 81
I0629 17:49:43.011759  8067 data_reader.cpp:262] Starting prefetch of epoch 81
I0629 17:49:43.011813  8093 data_reader.cpp:262] Starting prefetch of epoch 81
I0629 17:49:46.455905  8090 solver.cpp:349] Iteration 13400 (4.70305 iter/s, 21.2628s/100 iter), loss = 0.196952
I0629 17:49:46.455935  8090 solver.cpp:371]     Train net output #0: loss = 0.196952 (* 1 = 0.196952 loss)
I0629 17:49:46.455941  8090 sgd_solver.cpp:137] Iteration 13400, lr = 0.0001, m = 0.9
I0629 17:50:07.851131  8090 solver.cpp:349] Iteration 13500 (4.67408 iter/s, 21.3946s/100 iter), loss = 0.244394
I0629 17:50:07.851155  8090 solver.cpp:371]     Train net output #0: loss = 0.244394 (* 1 = 0.244394 loss)
I0629 17:50:07.851158  8090 sgd_solver.cpp:137] Iteration 13500, lr = 0.0001, m = 0.9
I0629 17:50:18.293964  8098 data_reader.cpp:262] Starting prefetch of epoch 82
I0629 17:50:18.297497  8069 data_reader.cpp:262] Starting prefetch of epoch 82
I0629 17:50:18.305622  8096 data_reader.cpp:262] Starting prefetch of epoch 82
I0629 17:50:18.325383  8067 data_reader.cpp:262] Starting prefetch of epoch 82
I0629 17:50:18.327411  8094 data_reader.cpp:262] Starting prefetch of epoch 82
I0629 17:50:18.330235  8093 data_reader.cpp:262] Starting prefetch of epoch 82
I0629 17:50:29.212205  8090 solver.cpp:349] Iteration 13600 (4.68155 iter/s, 21.3605s/100 iter), loss = 0.218802
I0629 17:50:29.212227  8090 solver.cpp:371]     Train net output #0: loss = 0.218802 (* 1 = 0.218802 loss)
I0629 17:50:29.212231  8090 sgd_solver.cpp:137] Iteration 13600, lr = 0.0001, m = 0.9
I0629 17:50:50.517310  8090 solver.cpp:349] Iteration 13700 (4.69385 iter/s, 21.3045s/100 iter), loss = 0.271576
I0629 17:50:50.517359  8090 solver.cpp:371]     Train net output #0: loss = 0.271576 (* 1 = 0.271576 loss)
I0629 17:50:50.517365  8090 sgd_solver.cpp:137] Iteration 13700, lr = 0.0001, m = 0.9
I0629 17:50:53.731264  8093 data_reader.cpp:262] Starting prefetch of epoch 83
I0629 17:50:53.732703  8096 data_reader.cpp:262] Starting prefetch of epoch 83
I0629 17:50:53.732890  8098 data_reader.cpp:262] Starting prefetch of epoch 83
I0629 17:50:53.740502  8069 data_reader.cpp:262] Starting prefetch of epoch 83
I0629 17:50:53.746810  8067 data_reader.cpp:262] Starting prefetch of epoch 83
I0629 17:50:53.748199  8094 data_reader.cpp:262] Starting prefetch of epoch 83
I0629 17:51:12.106573  8090 solver.cpp:349] Iteration 13800 (4.63207 iter/s, 21.5886s/100 iter), loss = 0.302687
I0629 17:51:12.106595  8090 solver.cpp:371]     Train net output #0: loss = 0.302687 (* 1 = 0.302687 loss)
I0629 17:51:12.106598  8090 sgd_solver.cpp:137] Iteration 13800, lr = 0.0001, m = 0.9
I0629 17:51:29.277745  8096 data_reader.cpp:262] Starting prefetch of epoch 84
I0629 17:51:29.288396  8069 data_reader.cpp:262] Starting prefetch of epoch 84
I0629 17:51:29.289942  8098 data_reader.cpp:262] Starting prefetch of epoch 84
I0629 17:51:29.300557  8067 data_reader.cpp:262] Starting prefetch of epoch 84
I0629 17:51:29.303845  8093 data_reader.cpp:262] Starting prefetch of epoch 84
I0629 17:51:29.303845  8094 data_reader.cpp:262] Starting prefetch of epoch 84
I0629 17:51:33.573074  8090 solver.cpp:349] Iteration 13900 (4.65855 iter/s, 21.4659s/100 iter), loss = 0.28451
I0629 17:51:33.573097  8090 solver.cpp:371]     Train net output #0: loss = 0.28451 (* 1 = 0.28451 loss)
I0629 17:51:33.573102  8090 sgd_solver.cpp:137] Iteration 13900, lr = 0.0001, m = 0.9
I0629 17:51:54.649562  8090 solver.cpp:545] Iteration 14000, Testing net (#0)
I0629 17:52:16.193619  8128 data_reader.cpp:262] Starting prefetch of epoch 10
I0629 17:52:16.193661  8119 data_reader.cpp:262] Starting prefetch of epoch 10
I0629 17:52:16.193661  8088 data_reader.cpp:262] Starting prefetch of epoch 10
I0629 17:52:16.458428  8117 data_reader.cpp:262] Starting prefetch of epoch 10
I0629 17:52:16.458428  8126 data_reader.cpp:262] Starting prefetch of epoch 10
I0629 17:52:16.458428  8086 data_reader.cpp:262] Starting prefetch of epoch 10
I0629 17:52:26.344736  8090 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.921578
I0629 17:52:26.344766  8090 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.993222
I0629 17:52:26.344771  8090 solver.cpp:630]     Test net output #2: loss = 0.248183 (* 1 = 0.248183 loss)
I0629 17:52:26.344799  8090 solver.cpp:305] [MultiGPU] Tests completed in 31.6944s
I0629 17:52:26.585857  8090 solver.cpp:349] Iteration 14000 (1.88639 iter/s, 53.0114s/100 iter), loss = 0.302319
I0629 17:52:26.585880  8090 solver.cpp:371]     Train net output #0: loss = 0.302319 (* 1 = 0.302319 loss)
I0629 17:52:26.585885  8090 sgd_solver.cpp:137] Iteration 14000, lr = 0.0001, m = 0.9
I0629 17:52:36.244328  8096 data_reader.cpp:262] Starting prefetch of epoch 85
I0629 17:52:36.248788  8069 data_reader.cpp:262] Starting prefetch of epoch 85
I0629 17:52:36.254873  8098 data_reader.cpp:262] Starting prefetch of epoch 85
I0629 17:52:36.275336  8094 data_reader.cpp:262] Starting prefetch of epoch 85
I0629 17:52:36.285981  8067 data_reader.cpp:262] Starting prefetch of epoch 85
I0629 17:52:36.288704  8093 data_reader.cpp:262] Starting prefetch of epoch 85
I0629 17:52:47.863521  8090 solver.cpp:349] Iteration 14100 (4.69989 iter/s, 21.2771s/100 iter), loss = 0.344442
I0629 17:52:47.863595  8090 solver.cpp:371]     Train net output #0: loss = 0.344442 (* 1 = 0.344442 loss)
I0629 17:52:47.863601  8090 sgd_solver.cpp:137] Iteration 14100, lr = 0.0001, m = 0.9
I0629 17:53:09.031419  8090 solver.cpp:349] Iteration 14200 (4.72427 iter/s, 21.1673s/100 iter), loss = 0.186484
I0629 17:53:09.031438  8090 solver.cpp:371]     Train net output #0: loss = 0.186484 (* 1 = 0.186484 loss)
I0629 17:53:09.031443  8090 sgd_solver.cpp:137] Iteration 14200, lr = 0.0001, m = 0.9
I0629 17:53:11.165865  8098 data_reader.cpp:262] Starting prefetch of epoch 86
I0629 17:53:11.170923  8096 data_reader.cpp:262] Starting prefetch of epoch 86
I0629 17:53:11.178349  8069 data_reader.cpp:262] Starting prefetch of epoch 86
I0629 17:53:11.189116  8094 data_reader.cpp:262] Starting prefetch of epoch 86
I0629 17:53:11.198204  8093 data_reader.cpp:262] Starting prefetch of epoch 86
I0629 17:53:11.198531  8067 data_reader.cpp:262] Starting prefetch of epoch 86
I0629 17:53:30.394630  8090 solver.cpp:349] Iteration 14300 (4.68107 iter/s, 21.3627s/100 iter), loss = 0.343636
I0629 17:53:30.394682  8090 solver.cpp:371]     Train net output #0: loss = 0.343636 (* 1 = 0.343636 loss)
I0629 17:53:30.394687  8090 sgd_solver.cpp:137] Iteration 14300, lr = 0.0001, m = 0.9
I0629 17:53:46.564419  8096 data_reader.cpp:262] Starting prefetch of epoch 87
I0629 17:53:46.571833  8067 data_reader.cpp:262] Starting prefetch of epoch 87
I0629 17:53:46.572105  8069 data_reader.cpp:262] Starting prefetch of epoch 87
I0629 17:53:46.575486  8098 data_reader.cpp:262] Starting prefetch of epoch 87
I0629 17:53:46.586539  8093 data_reader.cpp:262] Starting prefetch of epoch 87
I0629 17:53:46.592432  8094 data_reader.cpp:262] Starting prefetch of epoch 87
I0629 17:53:51.685566  8090 solver.cpp:349] Iteration 14400 (4.69696 iter/s, 21.2904s/100 iter), loss = 0.247865
I0629 17:53:51.685588  8090 solver.cpp:371]     Train net output #0: loss = 0.247865 (* 1 = 0.247865 loss)
I0629 17:53:51.685592  8090 sgd_solver.cpp:137] Iteration 14400, lr = 0.0001, m = 0.9
I0629 17:54:13.030055  8090 solver.cpp:349] Iteration 14500 (4.68517 iter/s, 21.3439s/100 iter), loss = 0.22215
I0629 17:54:13.030097  8090 solver.cpp:371]     Train net output #0: loss = 0.22215 (* 1 = 0.22215 loss)
I0629 17:54:13.030102  8090 sgd_solver.cpp:137] Iteration 14500, lr = 0.0001, m = 0.9
I0629 17:54:21.716466  8069 data_reader.cpp:262] Starting prefetch of epoch 88
I0629 17:54:21.718595  8098 data_reader.cpp:262] Starting prefetch of epoch 88
I0629 17:54:21.718600  8096 data_reader.cpp:262] Starting prefetch of epoch 88
I0629 17:54:21.735424  8067 data_reader.cpp:262] Starting prefetch of epoch 88
I0629 17:54:21.736459  8094 data_reader.cpp:262] Starting prefetch of epoch 88
I0629 17:54:21.739157  8093 data_reader.cpp:262] Starting prefetch of epoch 88
I0629 17:54:34.329994  8090 solver.cpp:349] Iteration 14600 (4.69497 iter/s, 21.2994s/100 iter), loss = 0.16072
I0629 17:54:34.330019  8090 solver.cpp:371]     Train net output #0: loss = 0.16072 (* 1 = 0.16072 loss)
I0629 17:54:34.330024  8090 sgd_solver.cpp:137] Iteration 14600, lr = 0.0001, m = 0.9
I0629 17:54:55.629786  8090 solver.cpp:349] Iteration 14700 (4.695 iter/s, 21.2993s/100 iter), loss = 0.235611
I0629 17:54:55.629853  8090 solver.cpp:371]     Train net output #0: loss = 0.235611 (* 1 = 0.235611 loss)
I0629 17:54:55.629858  8090 sgd_solver.cpp:137] Iteration 14700, lr = 0.0001, m = 0.9
I0629 17:54:56.900601  8098 data_reader.cpp:262] Starting prefetch of epoch 89
I0629 17:54:56.908603  8096 data_reader.cpp:262] Starting prefetch of epoch 89
I0629 17:54:56.921048  8069 data_reader.cpp:262] Starting prefetch of epoch 89
I0629 17:54:56.922155  8093 data_reader.cpp:262] Starting prefetch of epoch 89
I0629 17:54:56.926630  8094 data_reader.cpp:262] Starting prefetch of epoch 89
I0629 17:54:56.939764  8067 data_reader.cpp:262] Starting prefetch of epoch 89
I0629 17:55:16.907702  8090 solver.cpp:349] Iteration 14800 (4.69983 iter/s, 21.2773s/100 iter), loss = 0.227515
I0629 17:55:16.907727  8090 solver.cpp:371]     Train net output #0: loss = 0.227515 (* 1 = 0.227515 loss)
I0629 17:55:16.907732  8090 sgd_solver.cpp:137] Iteration 14800, lr = 0.0001, m = 0.9
I0629 17:55:32.072892  8069 data_reader.cpp:262] Starting prefetch of epoch 90
I0629 17:55:32.077540  8098 data_reader.cpp:262] Starting prefetch of epoch 90
I0629 17:55:32.079753  8096 data_reader.cpp:262] Starting prefetch of epoch 90
I0629 17:55:32.104953  8094 data_reader.cpp:262] Starting prefetch of epoch 90
I0629 17:55:32.108681  8067 data_reader.cpp:262] Starting prefetch of epoch 90
I0629 17:55:32.108788  8093 data_reader.cpp:262] Starting prefetch of epoch 90
I0629 17:55:38.228906  8090 solver.cpp:349] Iteration 14900 (4.69028 iter/s, 21.3207s/100 iter), loss = 0.274334
I0629 17:55:38.228929  8090 solver.cpp:371]     Train net output #0: loss = 0.274335 (* 1 = 0.274335 loss)
I0629 17:55:38.228932  8090 sgd_solver.cpp:137] Iteration 14900, lr = 0.0001, m = 0.9
I0629 17:55:59.652045  8090 solver.cpp:349] Iteration 15000 (4.66796 iter/s, 21.4226s/100 iter), loss = 0.167878
I0629 17:55:59.652065  8090 solver.cpp:371]     Train net output #0: loss = 0.167878 (* 1 = 0.167878 loss)
I0629 17:55:59.652070  8090 sgd_solver.cpp:137] Iteration 15000, lr = 0.0001, m = 0.9
I0629 17:56:07.573544  8096 data_reader.cpp:262] Starting prefetch of epoch 91
I0629 17:56:07.579983  8069 data_reader.cpp:262] Starting prefetch of epoch 91
I0629 17:56:07.579789  8098 data_reader.cpp:262] Starting prefetch of epoch 91
I0629 17:56:07.594676  8094 data_reader.cpp:262] Starting prefetch of epoch 91
I0629 17:56:07.603806  8093 data_reader.cpp:262] Starting prefetch of epoch 91
I0629 17:56:07.604508  8067 data_reader.cpp:262] Starting prefetch of epoch 91
I0629 17:56:21.185652  8090 solver.cpp:349] Iteration 15100 (4.64401 iter/s, 21.5331s/100 iter), loss = 0.234113
I0629 17:56:21.185676  8090 solver.cpp:371]     Train net output #0: loss = 0.234113 (* 1 = 0.234113 loss)
I0629 17:56:21.185681  8090 sgd_solver.cpp:137] Iteration 15100, lr = 0.0001, m = 0.9
I0629 17:56:42.457522  8090 solver.cpp:349] Iteration 15200 (4.70116 iter/s, 21.2714s/100 iter), loss = 0.269275
I0629 17:56:42.457629  8090 solver.cpp:371]     Train net output #0: loss = 0.269275 (* 1 = 0.269275 loss)
I0629 17:56:42.457635  8090 sgd_solver.cpp:137] Iteration 15200, lr = 0.0001, m = 0.9
I0629 17:56:42.889734  8098 data_reader.cpp:262] Starting prefetch of epoch 92
I0629 17:56:42.895522  8069 data_reader.cpp:262] Starting prefetch of epoch 92
I0629 17:56:42.901530  8096 data_reader.cpp:262] Starting prefetch of epoch 92
I0629 17:56:42.918081  8093 data_reader.cpp:262] Starting prefetch of epoch 92
I0629 17:56:42.921902  8067 data_reader.cpp:262] Starting prefetch of epoch 92
I0629 17:56:42.922215  8094 data_reader.cpp:262] Starting prefetch of epoch 92
I0629 17:57:03.694804  8090 solver.cpp:349] Iteration 15300 (4.70883 iter/s, 21.2367s/100 iter), loss = 0.197319
I0629 17:57:03.694828  8090 solver.cpp:371]     Train net output #0: loss = 0.197319 (* 1 = 0.197319 loss)
I0629 17:57:03.694839  8090 sgd_solver.cpp:137] Iteration 15300, lr = 0.0001, m = 0.9
I0629 17:57:17.966373  8098 data_reader.cpp:262] Starting prefetch of epoch 93
I0629 17:57:17.981768  8094 data_reader.cpp:262] Starting prefetch of epoch 93
I0629 17:57:17.982627  8096 data_reader.cpp:262] Starting prefetch of epoch 93
I0629 17:57:17.992677  8069 data_reader.cpp:262] Starting prefetch of epoch 93
I0629 17:57:18.001260  8067 data_reader.cpp:262] Starting prefetch of epoch 93
I0629 17:57:18.002511  8093 data_reader.cpp:262] Starting prefetch of epoch 93
I0629 17:57:24.948587  8090 solver.cpp:349] Iteration 15400 (4.70515 iter/s, 21.2533s/100 iter), loss = 0.227588
I0629 17:57:24.948609  8090 solver.cpp:371]     Train net output #0: loss = 0.227588 (* 1 = 0.227588 loss)
I0629 17:57:24.948613  8090 sgd_solver.cpp:137] Iteration 15400, lr = 0.0001, m = 0.9
I0629 17:57:46.217836  8090 solver.cpp:349] Iteration 15500 (4.70173 iter/s, 21.2688s/100 iter), loss = 0.19266
I0629 17:57:46.217859  8090 solver.cpp:371]     Train net output #0: loss = 0.19266 (* 1 = 0.19266 loss)
I0629 17:57:46.217864  8090 sgd_solver.cpp:137] Iteration 15500, lr = 0.0001, m = 0.9
I0629 17:57:53.252609  8096 data_reader.cpp:262] Starting prefetch of epoch 94
I0629 17:57:53.254441  8098 data_reader.cpp:262] Starting prefetch of epoch 94
I0629 17:57:53.254441  8069 data_reader.cpp:262] Starting prefetch of epoch 94
I0629 17:57:53.265236  8093 data_reader.cpp:262] Starting prefetch of epoch 94
I0629 17:57:53.266149  8094 data_reader.cpp:262] Starting prefetch of epoch 94
I0629 17:57:53.274803  8067 data_reader.cpp:262] Starting prefetch of epoch 94
I0629 17:58:07.538221  8090 solver.cpp:349] Iteration 15600 (4.69045 iter/s, 21.3199s/100 iter), loss = 0.287095
I0629 17:58:07.538244  8090 solver.cpp:371]     Train net output #0: loss = 0.287095 (* 1 = 0.287095 loss)
I0629 17:58:07.538249  8090 sgd_solver.cpp:137] Iteration 15600, lr = 0.0001, m = 0.9
I0629 17:58:28.409483  8069 data_reader.cpp:262] Starting prefetch of epoch 95
I0629 17:58:28.410375  8096 data_reader.cpp:262] Starting prefetch of epoch 95
I0629 17:58:28.410375  8098 data_reader.cpp:262] Starting prefetch of epoch 95
I0629 17:58:28.431793  8093 data_reader.cpp:262] Starting prefetch of epoch 95
I0629 17:58:28.431793  8094 data_reader.cpp:262] Starting prefetch of epoch 95
I0629 17:58:28.438824  8067 data_reader.cpp:262] Starting prefetch of epoch 95
I0629 17:58:28.830798  8090 solver.cpp:349] Iteration 15700 (4.69658 iter/s, 21.2921s/100 iter), loss = 0.312899
I0629 17:58:28.830819  8090 solver.cpp:371]     Train net output #0: loss = 0.312899 (* 1 = 0.312899 loss)
I0629 17:58:28.830823  8090 sgd_solver.cpp:137] Iteration 15700, lr = 0.0001, m = 0.9
I0629 17:58:50.205049  8090 solver.cpp:349] Iteration 15800 (4.67863 iter/s, 21.3738s/100 iter), loss = 0.270971
I0629 17:58:50.205072  8090 solver.cpp:371]     Train net output #0: loss = 0.270971 (* 1 = 0.270971 loss)
I0629 17:58:50.205077  8090 sgd_solver.cpp:137] Iteration 15800, lr = 0.0001, m = 0.9
I0629 17:59:03.713624  8069 data_reader.cpp:262] Starting prefetch of epoch 96
I0629 17:59:03.718392  8096 data_reader.cpp:262] Starting prefetch of epoch 96
I0629 17:59:03.731640  8098 data_reader.cpp:262] Starting prefetch of epoch 96
I0629 17:59:03.748658  8067 data_reader.cpp:262] Starting prefetch of epoch 96
I0629 17:59:03.750008  8094 data_reader.cpp:262] Starting prefetch of epoch 96
I0629 17:59:03.753589  8093 data_reader.cpp:262] Starting prefetch of epoch 96
I0629 17:59:11.582566  8090 solver.cpp:349] Iteration 15900 (4.67791 iter/s, 21.3771s/100 iter), loss = 0.156487
I0629 17:59:11.582588  8090 solver.cpp:371]     Train net output #0: loss = 0.156487 (* 1 = 0.156487 loss)
I0629 17:59:11.582593  8090 sgd_solver.cpp:137] Iteration 15900, lr = 0.0001, m = 0.9
I0629 17:59:32.777328  8090 solver.cpp:545] Iteration 16000, Testing net (#0)
I0629 17:59:41.039219  8088 data_reader.cpp:262] Starting prefetch of epoch 11
I0629 17:59:41.044090  8086 data_reader.cpp:262] Starting prefetch of epoch 11
I0629 17:59:41.155872  8128 data_reader.cpp:262] Starting prefetch of epoch 11
I0629 17:59:41.160372  8126 data_reader.cpp:262] Starting prefetch of epoch 11
I0629 17:59:41.249599  8119 data_reader.cpp:262] Starting prefetch of epoch 11
I0629 17:59:41.254294  8117 data_reader.cpp:262] Starting prefetch of epoch 11
I0629 17:59:54.685977  8088 data_reader.cpp:262] Starting prefetch of epoch 12
I0629 17:59:54.690654  8086 data_reader.cpp:262] Starting prefetch of epoch 12
I0629 17:59:54.955418  8119 data_reader.cpp:262] Starting prefetch of epoch 12
I0629 17:59:54.959882  8117 data_reader.cpp:262] Starting prefetch of epoch 12
I0629 17:59:55.063324  8128 data_reader.cpp:262] Starting prefetch of epoch 12
I0629 17:59:55.067737  8126 data_reader.cpp:262] Starting prefetch of epoch 12
I0629 17:59:55.690784  8090 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.920365
I0629 17:59:55.690810  8090 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.993468
I0629 17:59:55.690816  8090 solver.cpp:630]     Test net output #2: loss = 0.24918 (* 1 = 0.24918 loss)
I0629 17:59:55.690855  8090 solver.cpp:305] [MultiGPU] Tests completed in 22.9131s
I0629 17:59:55.918020  8090 solver.cpp:349] Iteration 16000 (2.25558 iter/s, 44.3345s/100 iter), loss = 0.220613
I0629 17:59:55.918046  8090 solver.cpp:371]     Train net output #0: loss = 0.220613 (* 1 = 0.220613 loss)
I0629 17:59:55.918051  8090 sgd_solver.cpp:137] Iteration 16000, lr = 0.0001, m = 0.9
I0629 18:00:02.001857  8098 data_reader.cpp:262] Starting prefetch of epoch 97
I0629 18:00:02.011211  8069 data_reader.cpp:262] Starting prefetch of epoch 97
I0629 18:00:02.017026  8096 data_reader.cpp:262] Starting prefetch of epoch 97
I0629 18:00:02.027981  8067 data_reader.cpp:262] Starting prefetch of epoch 97
I0629 18:00:02.028992  8094 data_reader.cpp:262] Starting prefetch of epoch 97
I0629 18:00:02.031275  8093 data_reader.cpp:262] Starting prefetch of epoch 97
I0629 18:00:17.296908  8090 solver.cpp:349] Iteration 16100 (4.67761 iter/s, 21.3784s/100 iter), loss = 0.178933
I0629 18:00:17.296958  8090 solver.cpp:371]     Train net output #0: loss = 0.178933 (* 1 = 0.178933 loss)
I0629 18:00:17.296963  8090 sgd_solver.cpp:137] Iteration 16100, lr = 0.0001, m = 0.9
I0629 18:00:37.342967  8069 data_reader.cpp:262] Starting prefetch of epoch 98
I0629 18:00:37.352373  8096 data_reader.cpp:262] Starting prefetch of epoch 98
I0629 18:00:37.359259  8098 data_reader.cpp:262] Starting prefetch of epoch 98
I0629 18:00:37.362745  8067 data_reader.cpp:262] Starting prefetch of epoch 98
I0629 18:00:37.363253  8094 data_reader.cpp:262] Starting prefetch of epoch 98
I0629 18:00:37.373111  8093 data_reader.cpp:262] Starting prefetch of epoch 98
I0629 18:00:38.596089  8090 solver.cpp:349] Iteration 16200 (4.69512 iter/s, 21.2987s/100 iter), loss = 0.291741
I0629 18:00:38.596114  8090 solver.cpp:371]     Train net output #0: loss = 0.291741 (* 1 = 0.291741 loss)
I0629 18:00:38.596118  8090 sgd_solver.cpp:137] Iteration 16200, lr = 0.0001, m = 0.9
I0629 18:00:59.813097  8090 solver.cpp:349] Iteration 16300 (4.7133 iter/s, 21.2166s/100 iter), loss = 0.191995
I0629 18:00:59.820884  8090 solver.cpp:371]     Train net output #0: loss = 0.191995 (* 1 = 0.191995 loss)
I0629 18:00:59.820937  8090 sgd_solver.cpp:137] Iteration 16300, lr = 0.0001, m = 0.9
I0629 18:01:12.405226  8069 data_reader.cpp:262] Starting prefetch of epoch 99
I0629 18:01:12.408987  8098 data_reader.cpp:262] Starting prefetch of epoch 99
I0629 18:01:12.414139  8096 data_reader.cpp:262] Starting prefetch of epoch 99
I0629 18:01:12.432445  8067 data_reader.cpp:262] Starting prefetch of epoch 99
I0629 18:01:12.432445  8094 data_reader.cpp:262] Starting prefetch of epoch 99
I0629 18:01:12.434217  8093 data_reader.cpp:262] Starting prefetch of epoch 99
I0629 18:01:21.304311  8090 solver.cpp:349] Iteration 16400 (4.65486 iter/s, 21.4829s/100 iter), loss = 0.204953
I0629 18:01:21.304335  8090 solver.cpp:371]     Train net output #0: loss = 0.204953 (* 1 = 0.204953 loss)
I0629 18:01:21.304339  8090 sgd_solver.cpp:137] Iteration 16400, lr = 0.0001, m = 0.9
I0629 18:01:42.550374  8090 solver.cpp:349] Iteration 16500 (4.70685 iter/s, 21.2456s/100 iter), loss = 0.18926
I0629 18:01:42.550437  8090 solver.cpp:371]     Train net output #0: loss = 0.189261 (* 1 = 0.189261 loss)
I0629 18:01:42.550442  8090 sgd_solver.cpp:137] Iteration 16500, lr = 0.0001, m = 0.9
I0629 18:01:47.677722  8096 data_reader.cpp:262] Starting prefetch of epoch 100
I0629 18:01:47.692590  8098 data_reader.cpp:262] Starting prefetch of epoch 100
I0629 18:01:47.696516  8067 data_reader.cpp:262] Starting prefetch of epoch 100
I0629 18:01:47.696559  8069 data_reader.cpp:262] Starting prefetch of epoch 100
I0629 18:01:47.707799  8094 data_reader.cpp:262] Starting prefetch of epoch 100
I0629 18:01:47.709770  8093 data_reader.cpp:262] Starting prefetch of epoch 100
I0629 18:02:03.831578  8090 solver.cpp:349] Iteration 16600 (4.69909 iter/s, 21.2807s/100 iter), loss = 0.279924
I0629 18:02:03.831600  8090 solver.cpp:371]     Train net output #0: loss = 0.279924 (* 1 = 0.279924 loss)
I0629 18:02:03.831605  8090 sgd_solver.cpp:137] Iteration 16600, lr = 0.0001, m = 0.9
I0629 18:02:22.981912  8094 data_reader.cpp:262] Starting prefetch of epoch 101
I0629 18:02:22.984268  8096 data_reader.cpp:262] Starting prefetch of epoch 101
I0629 18:02:22.987103  8093 data_reader.cpp:262] Starting prefetch of epoch 101
I0629 18:02:22.988198  8098 data_reader.cpp:262] Starting prefetch of epoch 101
I0629 18:02:22.988577  8069 data_reader.cpp:262] Starting prefetch of epoch 101
I0629 18:02:22.988972  8067 data_reader.cpp:262] Starting prefetch of epoch 101
I0629 18:02:25.083431  8090 solver.cpp:349] Iteration 16700 (4.70557 iter/s, 21.2514s/100 iter), loss = 0.399746
I0629 18:02:25.083454  8090 solver.cpp:371]     Train net output #0: loss = 0.399746 (* 1 = 0.399746 loss)
I0629 18:02:25.083459  8090 sgd_solver.cpp:137] Iteration 16700, lr = 0.0001, m = 0.9
I0629 18:02:46.296993  8090 solver.cpp:349] Iteration 16800 (4.71406 iter/s, 21.2131s/100 iter), loss = 0.26568
I0629 18:02:46.297018  8090 solver.cpp:371]     Train net output #0: loss = 0.26568 (* 1 = 0.26568 loss)
I0629 18:02:46.297021  8090 sgd_solver.cpp:137] Iteration 16800, lr = 0.0001, m = 0.9
I0629 18:02:57.974566  8096 data_reader.cpp:262] Starting prefetch of epoch 102
I0629 18:02:57.976778  8069 data_reader.cpp:262] Starting prefetch of epoch 102
I0629 18:02:57.984989  8098 data_reader.cpp:262] Starting prefetch of epoch 102
I0629 18:02:58.003355  8067 data_reader.cpp:262] Starting prefetch of epoch 102
I0629 18:02:58.010978  8093 data_reader.cpp:262] Starting prefetch of epoch 102
I0629 18:02:58.012043  8094 data_reader.cpp:262] Starting prefetch of epoch 102
I0629 18:03:07.557940  8090 solver.cpp:349] Iteration 16900 (4.70355 iter/s, 21.2605s/100 iter), loss = 0.354659
I0629 18:03:07.557965  8090 solver.cpp:371]     Train net output #0: loss = 0.354659 (* 1 = 0.354659 loss)
I0629 18:03:07.557970  8090 sgd_solver.cpp:137] Iteration 16900, lr = 0.0001, m = 0.9
I0629 18:03:29.870476  8090 solver.cpp:349] Iteration 17000 (4.48187 iter/s, 22.3121s/100 iter), loss = 0.260993
I0629 18:03:29.870563  8090 solver.cpp:371]     Train net output #0: loss = 0.260993 (* 1 = 0.260993 loss)
I0629 18:03:29.870568  8090 sgd_solver.cpp:137] Iteration 17000, lr = 0.0001, m = 0.9
I0629 18:03:34.245378  8098 data_reader.cpp:262] Starting prefetch of epoch 103
I0629 18:03:34.264448  8096 data_reader.cpp:262] Starting prefetch of epoch 103
I0629 18:03:34.264853  8069 data_reader.cpp:262] Starting prefetch of epoch 103
I0629 18:03:34.274983  8094 data_reader.cpp:262] Starting prefetch of epoch 103
I0629 18:03:34.275568  8093 data_reader.cpp:262] Starting prefetch of epoch 103
I0629 18:03:34.277814  8067 data_reader.cpp:262] Starting prefetch of epoch 103
I0629 18:03:51.962563  8090 solver.cpp:349] Iteration 17100 (4.52661 iter/s, 22.0916s/100 iter), loss = 0.332708
I0629 18:03:51.962585  8090 solver.cpp:371]     Train net output #0: loss = 0.332708 (* 1 = 0.332708 loss)
I0629 18:03:51.962589  8090 sgd_solver.cpp:137] Iteration 17100, lr = 0.0001, m = 0.9
I0629 18:04:10.086326  8096 data_reader.cpp:262] Starting prefetch of epoch 104
I0629 18:04:10.096118  8069 data_reader.cpp:262] Starting prefetch of epoch 104
I0629 18:04:10.096284  8098 data_reader.cpp:262] Starting prefetch of epoch 104
I0629 18:04:10.112721  8093 data_reader.cpp:262] Starting prefetch of epoch 104
I0629 18:04:10.118271  8067 data_reader.cpp:262] Starting prefetch of epoch 104
I0629 18:04:10.119092  8094 data_reader.cpp:262] Starting prefetch of epoch 104
I0629 18:04:13.264179  8090 solver.cpp:349] Iteration 17200 (4.69457 iter/s, 21.3012s/100 iter), loss = 0.227466
I0629 18:04:13.264201  8090 solver.cpp:371]     Train net output #0: loss = 0.227466 (* 1 = 0.227466 loss)
I0629 18:04:13.264205  8090 sgd_solver.cpp:137] Iteration 17200, lr = 0.0001, m = 0.9
I0629 18:04:34.669759  8090 solver.cpp:349] Iteration 17300 (4.67177 iter/s, 21.4052s/100 iter), loss = 0.27017
I0629 18:04:34.669782  8090 solver.cpp:371]     Train net output #0: loss = 0.27017 (* 1 = 0.27017 loss)
I0629 18:04:34.669786  8090 sgd_solver.cpp:137] Iteration 17300, lr = 0.0001, m = 0.9
I0629 18:04:45.702103  8098 data_reader.cpp:262] Starting prefetch of epoch 105
I0629 18:04:45.708423  8096 data_reader.cpp:262] Starting prefetch of epoch 105
I0629 18:04:45.712191  8069 data_reader.cpp:262] Starting prefetch of epoch 105
I0629 18:04:45.722721  8093 data_reader.cpp:262] Starting prefetch of epoch 105
I0629 18:04:45.727463  8094 data_reader.cpp:262] Starting prefetch of epoch 105
I0629 18:04:45.722721  8067 data_reader.cpp:262] Starting prefetch of epoch 105
I0629 18:04:56.437259  8090 solver.cpp:349] Iteration 17400 (4.59409 iter/s, 21.7671s/100 iter), loss = 0.180821
I0629 18:04:56.437283  8090 solver.cpp:371]     Train net output #0: loss = 0.180821 (* 1 = 0.180821 loss)
I0629 18:04:56.437288  8090 sgd_solver.cpp:137] Iteration 17400, lr = 0.0001, m = 0.9
I0629 18:05:17.954144  8090 solver.cpp:349] Iteration 17500 (4.6476 iter/s, 21.5165s/100 iter), loss = 0.195496
I0629 18:05:17.954195  8090 solver.cpp:371]     Train net output #0: loss = 0.195496 (* 1 = 0.195496 loss)
I0629 18:05:17.954200  8090 sgd_solver.cpp:137] Iteration 17500, lr = 0.0001, m = 0.9
I0629 18:05:21.406822  8098 data_reader.cpp:262] Starting prefetch of epoch 106
I0629 18:05:21.412559  8096 data_reader.cpp:262] Starting prefetch of epoch 106
I0629 18:05:21.419174  8069 data_reader.cpp:262] Starting prefetch of epoch 106
I0629 18:05:21.436081  8093 data_reader.cpp:262] Starting prefetch of epoch 106
I0629 18:05:21.439420  8067 data_reader.cpp:262] Starting prefetch of epoch 106
I0629 18:05:21.442046  8094 data_reader.cpp:262] Starting prefetch of epoch 106
I0629 18:05:39.444697  8090 solver.cpp:349] Iteration 17600 (4.6533 iter/s, 21.4901s/100 iter), loss = 0.204922
I0629 18:05:39.444723  8090 solver.cpp:371]     Train net output #0: loss = 0.204922 (* 1 = 0.204922 loss)
I0629 18:05:39.444727  8090 sgd_solver.cpp:137] Iteration 17600, lr = 0.0001, m = 0.9
I0629 18:05:56.870237  8069 data_reader.cpp:262] Starting prefetch of epoch 107
I0629 18:05:56.870410  8098 data_reader.cpp:262] Starting prefetch of epoch 107
I0629 18:05:56.877459  8096 data_reader.cpp:262] Starting prefetch of epoch 107
I0629 18:05:56.898319  8067 data_reader.cpp:262] Starting prefetch of epoch 107
I0629 18:05:56.898730  8093 data_reader.cpp:262] Starting prefetch of epoch 107
I0629 18:05:56.899075  8094 data_reader.cpp:262] Starting prefetch of epoch 107
I0629 18:06:00.880408  8090 solver.cpp:349] Iteration 17700 (4.6652 iter/s, 21.4353s/100 iter), loss = 0.193481
I0629 18:06:00.880431  8090 solver.cpp:371]     Train net output #0: loss = 0.193481 (* 1 = 0.193481 loss)
I0629 18:06:00.880435  8090 sgd_solver.cpp:137] Iteration 17700, lr = 0.0001, m = 0.9
I0629 18:06:22.618871  8090 solver.cpp:349] Iteration 17800 (4.60022 iter/s, 21.7381s/100 iter), loss = 0.21392
I0629 18:06:22.618896  8090 solver.cpp:371]     Train net output #0: loss = 0.21392 (* 1 = 0.21392 loss)
I0629 18:06:22.618901  8090 sgd_solver.cpp:137] Iteration 17800, lr = 0.0001, m = 0.9
I0629 18:06:32.452033  8096 data_reader.cpp:262] Starting prefetch of epoch 108
I0629 18:06:32.457813  8069 data_reader.cpp:262] Starting prefetch of epoch 108
I0629 18:06:32.464051  8098 data_reader.cpp:262] Starting prefetch of epoch 108
I0629 18:06:32.468891  8094 data_reader.cpp:262] Starting prefetch of epoch 108
I0629 18:06:32.481976  8093 data_reader.cpp:262] Starting prefetch of epoch 108
I0629 18:06:32.482985  8067 data_reader.cpp:262] Starting prefetch of epoch 108
I0629 18:06:44.044719  8090 solver.cpp:349] Iteration 17900 (4.66734 iter/s, 21.4255s/100 iter), loss = 0.26816
I0629 18:06:44.044740  8090 solver.cpp:371]     Train net output #0: loss = 0.268161 (* 1 = 0.268161 loss)
I0629 18:06:44.044744  8090 sgd_solver.cpp:137] Iteration 17900, lr = 0.0001, m = 0.9
I0629 18:07:05.161087  8090 solver.cpp:545] Iteration 18000, Testing net (#0)
I0629 18:07:26.992007  8128 data_reader.cpp:262] Starting prefetch of epoch 13
I0629 18:07:26.992043  8119 data_reader.cpp:262] Starting prefetch of epoch 13
I0629 18:07:26.992007  8088 data_reader.cpp:262] Starting prefetch of epoch 13
I0629 18:07:27.156695  8086 data_reader.cpp:262] Starting prefetch of epoch 13
I0629 18:07:27.156695  8117 data_reader.cpp:262] Starting prefetch of epoch 13
I0629 18:07:27.156695  8126 data_reader.cpp:262] Starting prefetch of epoch 13
I0629 18:07:37.670603  8090 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.923958
I0629 18:07:37.670696  8090 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.992992
I0629 18:07:37.670704  8090 solver.cpp:630]     Test net output #2: loss = 0.243252 (* 1 = 0.243252 loss)
I0629 18:07:37.670730  8090 solver.cpp:305] [MultiGPU] Tests completed in 32.5091s
I0629 18:07:37.889252  8090 solver.cpp:349] Iteration 18000 (1.85723 iter/s, 53.8437s/100 iter), loss = 0.240831
I0629 18:07:37.889281  8090 solver.cpp:371]     Train net output #0: loss = 0.240832 (* 1 = 0.240832 loss)
I0629 18:07:37.889287  8090 sgd_solver.cpp:137] Iteration 18000, lr = 0.0001, m = 0.9
I0629 18:07:40.454203  8069 data_reader.cpp:262] Starting prefetch of epoch 109
I0629 18:07:40.461659  8098 data_reader.cpp:262] Starting prefetch of epoch 109
I0629 18:07:40.469100  8096 data_reader.cpp:262] Starting prefetch of epoch 109
I0629 18:07:40.471704  8067 data_reader.cpp:262] Starting prefetch of epoch 109
I0629 18:07:40.476101  8094 data_reader.cpp:262] Starting prefetch of epoch 109
I0629 18:07:40.480947  8093 data_reader.cpp:262] Starting prefetch of epoch 109
I0629 18:07:59.119868  8090 solver.cpp:349] Iteration 18100 (4.71026 iter/s, 21.2302s/100 iter), loss = 0.165382
I0629 18:07:59.119889  8090 solver.cpp:371]     Train net output #0: loss = 0.165382 (* 1 = 0.165382 loss)
I0629 18:07:59.119892  8090 sgd_solver.cpp:137] Iteration 18100, lr = 0.0001, m = 0.9
I0629 18:08:15.713886  8096 data_reader.cpp:262] Starting prefetch of epoch 110
I0629 18:08:15.713886  8098 data_reader.cpp:262] Starting prefetch of epoch 110
I0629 18:08:15.718055  8069 data_reader.cpp:262] Starting prefetch of epoch 110
I0629 18:08:15.733119  8067 data_reader.cpp:262] Starting prefetch of epoch 110
I0629 18:08:15.738661  8093 data_reader.cpp:262] Starting prefetch of epoch 110
I0629 18:08:15.739738  8094 data_reader.cpp:262] Starting prefetch of epoch 110
I0629 18:08:20.684734  8090 solver.cpp:349] Iteration 18200 (4.63725 iter/s, 21.5645s/100 iter), loss = 0.193379
I0629 18:08:20.684759  8090 solver.cpp:371]     Train net output #0: loss = 0.193379 (* 1 = 0.193379 loss)
I0629 18:08:20.684763  8090 sgd_solver.cpp:137] Iteration 18200, lr = 0.0001, m = 0.9
I0629 18:08:42.075868  8090 solver.cpp:349] Iteration 18300 (4.67491 iter/s, 21.3908s/100 iter), loss = 0.253861
I0629 18:08:42.075891  8090 solver.cpp:371]     Train net output #0: loss = 0.253861 (* 1 = 0.253861 loss)
I0629 18:08:42.075896  8090 sgd_solver.cpp:137] Iteration 18300, lr = 0.0001, m = 0.9
I0629 18:08:50.983578  8098 data_reader.cpp:262] Starting prefetch of epoch 111
I0629 18:08:50.994957  8069 data_reader.cpp:262] Starting prefetch of epoch 111
I0629 18:08:50.996767  8096 data_reader.cpp:262] Starting prefetch of epoch 111
I0629 18:08:51.012087  8094 data_reader.cpp:262] Starting prefetch of epoch 111
I0629 18:08:51.018956  8093 data_reader.cpp:262] Starting prefetch of epoch 111
I0629 18:08:51.019269  8067 data_reader.cpp:262] Starting prefetch of epoch 111
I0629 18:09:03.364995  8090 solver.cpp:349] Iteration 18400 (4.69731 iter/s, 21.2888s/100 iter), loss = 0.205716
I0629 18:09:03.365018  8090 solver.cpp:371]     Train net output #0: loss = 0.205716 (* 1 = 0.205716 loss)
I0629 18:09:03.365025  8090 sgd_solver.cpp:137] Iteration 18400, lr = 0.0001, m = 0.9
I0629 18:09:24.781354  8090 solver.cpp:349] Iteration 18500 (4.66941 iter/s, 21.416s/100 iter), loss = 0.327009
I0629 18:09:24.781435  8090 solver.cpp:371]     Train net output #0: loss = 0.327009 (* 1 = 0.327009 loss)
I0629 18:09:24.781440  8090 sgd_solver.cpp:137] Iteration 18500, lr = 0.0001, m = 0.9
I0629 18:09:26.484573  8096 data_reader.cpp:262] Starting prefetch of epoch 112
I0629 18:09:26.488567  8098 data_reader.cpp:262] Starting prefetch of epoch 112
I0629 18:09:26.494122  8093 data_reader.cpp:262] Starting prefetch of epoch 112
I0629 18:09:26.499395  8069 data_reader.cpp:262] Starting prefetch of epoch 112
I0629 18:09:26.500417  8094 data_reader.cpp:262] Starting prefetch of epoch 112
I0629 18:09:26.510660  8067 data_reader.cpp:262] Starting prefetch of epoch 112
I0629 18:09:46.104393  8090 solver.cpp:349] Iteration 18600 (4.68985 iter/s, 21.3226s/100 iter), loss = 0.17457
I0629 18:09:46.104414  8090 solver.cpp:371]     Train net output #0: loss = 0.17457 (* 1 = 0.17457 loss)
I0629 18:09:46.104418  8090 sgd_solver.cpp:137] Iteration 18600, lr = 0.0001, m = 0.9
I0629 18:10:01.588122  8069 data_reader.cpp:262] Starting prefetch of epoch 113
I0629 18:10:01.588122  8098 data_reader.cpp:262] Starting prefetch of epoch 113
I0629 18:10:01.591430  8096 data_reader.cpp:262] Starting prefetch of epoch 113
I0629 18:10:01.604315  8094 data_reader.cpp:262] Starting prefetch of epoch 113
I0629 18:10:01.606182  8067 data_reader.cpp:262] Starting prefetch of epoch 113
I0629 18:10:01.606859  8093 data_reader.cpp:262] Starting prefetch of epoch 113
I0629 18:10:07.382151  8090 solver.cpp:349] Iteration 18700 (4.69982 iter/s, 21.2774s/100 iter), loss = 0.145077
I0629 18:10:07.382174  8090 solver.cpp:371]     Train net output #0: loss = 0.145077 (* 1 = 0.145077 loss)
I0629 18:10:07.382179  8090 sgd_solver.cpp:137] Iteration 18700, lr = 0.0001, m = 0.9
I0629 18:10:28.651510  8090 solver.cpp:349] Iteration 18800 (4.70168 iter/s, 21.269s/100 iter), loss = 0.235719
I0629 18:10:28.651532  8090 solver.cpp:371]     Train net output #0: loss = 0.235719 (* 1 = 0.235719 loss)
I0629 18:10:28.651537  8090 sgd_solver.cpp:137] Iteration 18800, lr = 0.0001, m = 0.9
I0629 18:10:36.729768  8096 data_reader.cpp:262] Starting prefetch of epoch 114
I0629 18:10:36.729768  8069 data_reader.cpp:262] Starting prefetch of epoch 114
I0629 18:10:36.741168  8098 data_reader.cpp:262] Starting prefetch of epoch 114
I0629 18:10:36.763170  8094 data_reader.cpp:262] Starting prefetch of epoch 114
I0629 18:10:36.765858  8093 data_reader.cpp:262] Starting prefetch of epoch 114
I0629 18:10:36.781069  8067 data_reader.cpp:262] Starting prefetch of epoch 114
I0629 18:10:50.034178  8090 solver.cpp:349] Iteration 18900 (4.67676 iter/s, 21.3823s/100 iter), loss = 0.276002
I0629 18:10:50.034198  8090 solver.cpp:371]     Train net output #0: loss = 0.276002 (* 1 = 0.276002 loss)
I0629 18:10:50.034202  8090 sgd_solver.cpp:137] Iteration 18900, lr = 0.0001, m = 0.9
I0629 18:11:11.633235  8090 solver.cpp:349] Iteration 19000 (4.62991 iter/s, 21.5987s/100 iter), loss = 0.587491
I0629 18:11:11.633292  8090 solver.cpp:371]     Train net output #0: loss = 0.587491 (* 1 = 0.587491 loss)
I0629 18:11:11.633298  8090 sgd_solver.cpp:137] Iteration 19000, lr = 0.0001, m = 0.9
I0629 18:11:12.298218  8098 data_reader.cpp:262] Starting prefetch of epoch 115
I0629 18:11:12.304570  8096 data_reader.cpp:262] Starting prefetch of epoch 115
I0629 18:11:12.321581  8069 data_reader.cpp:262] Starting prefetch of epoch 115
I0629 18:11:12.326014  8094 data_reader.cpp:262] Starting prefetch of epoch 115
I0629 18:11:12.327132  8093 data_reader.cpp:262] Starting prefetch of epoch 115
I0629 18:11:12.330422  8067 data_reader.cpp:262] Starting prefetch of epoch 115
I0629 18:11:33.183799  8090 solver.cpp:349] Iteration 19100 (4.64033 iter/s, 21.5502s/100 iter), loss = 0.158273
I0629 18:11:33.183823  8090 solver.cpp:371]     Train net output #0: loss = 0.158273 (* 1 = 0.158273 loss)
I0629 18:11:33.183828  8090 sgd_solver.cpp:137] Iteration 19100, lr = 0.0001, m = 0.9
I0629 18:11:48.024310  8096 data_reader.cpp:262] Starting prefetch of epoch 116
I0629 18:11:48.026306  8069 data_reader.cpp:262] Starting prefetch of epoch 116
I0629 18:11:48.036025  8098 data_reader.cpp:262] Starting prefetch of epoch 116
I0629 18:11:48.041579  8067 data_reader.cpp:262] Starting prefetch of epoch 116
I0629 18:11:48.046205  8093 data_reader.cpp:262] Starting prefetch of epoch 116
I0629 18:11:48.056000  8094 data_reader.cpp:262] Starting prefetch of epoch 116
I0629 18:11:54.661726  8090 solver.cpp:349] Iteration 19200 (4.65602 iter/s, 21.4776s/100 iter), loss = 0.243604
I0629 18:11:54.661751  8090 solver.cpp:371]     Train net output #0: loss = 0.243604 (* 1 = 0.243604 loss)
I0629 18:11:54.661756  8090 sgd_solver.cpp:137] Iteration 19200, lr = 0.0001, m = 0.9
I0629 18:12:16.001519  8090 solver.cpp:349] Iteration 19300 (4.68616 iter/s, 21.3395s/100 iter), loss = 0.2486
I0629 18:12:16.001541  8090 solver.cpp:371]     Train net output #0: loss = 0.2486 (* 1 = 0.2486 loss)
I0629 18:12:16.001545  8090 sgd_solver.cpp:137] Iteration 19300, lr = 0.0001, m = 0.9
I0629 18:12:23.224963  8096 data_reader.cpp:262] Starting prefetch of epoch 117
I0629 18:12:23.227717  8069 data_reader.cpp:262] Starting prefetch of epoch 117
I0629 18:12:23.227717  8098 data_reader.cpp:262] Starting prefetch of epoch 117
I0629 18:12:23.244446  8094 data_reader.cpp:262] Starting prefetch of epoch 117
I0629 18:12:23.245133  8067 data_reader.cpp:262] Starting prefetch of epoch 117
I0629 18:12:23.251425  8093 data_reader.cpp:262] Starting prefetch of epoch 117
I0629 18:12:37.244575  8090 solver.cpp:349] Iteration 19400 (4.70749 iter/s, 21.2427s/100 iter), loss = 0.232359
I0629 18:12:37.244596  8090 solver.cpp:371]     Train net output #0: loss = 0.232359 (* 1 = 0.232359 loss)
I0629 18:12:37.244601  8090 sgd_solver.cpp:137] Iteration 19400, lr = 0.0001, m = 0.9
I0629 18:12:58.272016  8069 data_reader.cpp:262] Starting prefetch of epoch 118
I0629 18:12:58.272516  8096 data_reader.cpp:262] Starting prefetch of epoch 118
I0629 18:12:58.281951  8098 data_reader.cpp:262] Starting prefetch of epoch 118
I0629 18:12:58.291649  8094 data_reader.cpp:262] Starting prefetch of epoch 118
I0629 18:12:58.293596  8067 data_reader.cpp:262] Starting prefetch of epoch 118
I0629 18:12:58.297997  8093 data_reader.cpp:262] Starting prefetch of epoch 118
I0629 18:12:58.478549  8090 solver.cpp:349] Iteration 19500 (4.70951 iter/s, 21.2336s/100 iter), loss = 0.212259
I0629 18:12:58.478572  8090 solver.cpp:371]     Train net output #0: loss = 0.212258 (* 1 = 0.212258 loss)
I0629 18:12:58.478577  8090 sgd_solver.cpp:137] Iteration 19500, lr = 0.0001, m = 0.9
I0629 18:13:19.713981  8090 solver.cpp:349] Iteration 19600 (4.70918 iter/s, 21.2351s/100 iter), loss = 0.164148
I0629 18:13:19.714006  8090 solver.cpp:371]     Train net output #0: loss = 0.164148 (* 1 = 0.164148 loss)
I0629 18:13:19.714010  8090 sgd_solver.cpp:137] Iteration 19600, lr = 0.0001, m = 0.9
I0629 18:13:33.651763  8096 data_reader.cpp:262] Starting prefetch of epoch 119
I0629 18:13:33.652873  8069 data_reader.cpp:262] Starting prefetch of epoch 119
I0629 18:13:33.657196  8098 data_reader.cpp:262] Starting prefetch of epoch 119
I0629 18:13:33.666930  8094 data_reader.cpp:262] Starting prefetch of epoch 119
I0629 18:13:33.667738  8067 data_reader.cpp:262] Starting prefetch of epoch 119
I0629 18:13:33.676836  8093 data_reader.cpp:262] Starting prefetch of epoch 119
I0629 18:13:41.097746  8090 solver.cpp:349] Iteration 19700 (4.67658 iter/s, 21.3831s/100 iter), loss = 0.415527
I0629 18:13:41.097769  8090 solver.cpp:371]     Train net output #0: loss = 0.415527 (* 1 = 0.415527 loss)
I0629 18:13:41.097772  8090 sgd_solver.cpp:137] Iteration 19700, lr = 0.0001, m = 0.9
I0629 18:14:02.334349  8090 solver.cpp:349] Iteration 19800 (4.709 iter/s, 21.2359s/100 iter), loss = 0.193474
I0629 18:14:02.334372  8090 solver.cpp:371]     Train net output #0: loss = 0.193474 (* 1 = 0.193474 loss)
I0629 18:14:02.334377  8090 sgd_solver.cpp:137] Iteration 19800, lr = 0.0001, m = 0.9
I0629 18:14:08.768239  8098 data_reader.cpp:262] Starting prefetch of epoch 120
I0629 18:14:08.772397  8096 data_reader.cpp:262] Starting prefetch of epoch 120
I0629 18:14:08.773471  8069 data_reader.cpp:262] Starting prefetch of epoch 120
I0629 18:14:08.800684  8067 data_reader.cpp:262] Starting prefetch of epoch 120
I0629 18:14:08.804008  8094 data_reader.cpp:262] Starting prefetch of epoch 120
I0629 18:14:08.804785  8093 data_reader.cpp:262] Starting prefetch of epoch 120
I0629 18:14:23.712105  8090 solver.cpp:349] Iteration 19900 (4.6779 iter/s, 21.3771s/100 iter), loss = 0.147455
I0629 18:14:23.712131  8090 solver.cpp:371]     Train net output #0: loss = 0.147455 (* 1 = 0.147455 loss)
I0629 18:14:23.712134  8090 sgd_solver.cpp:137] Iteration 19900, lr = 0.0001, m = 0.9
I0629 18:14:44.077606  8096 data_reader.cpp:262] Starting prefetch of epoch 121
I0629 18:14:44.086108  8098 data_reader.cpp:262] Starting prefetch of epoch 121
I0629 18:14:44.088786  8069 data_reader.cpp:262] Starting prefetch of epoch 121
I0629 18:14:44.094632  8093 data_reader.cpp:262] Starting prefetch of epoch 121
I0629 18:14:44.100554  8067 data_reader.cpp:262] Starting prefetch of epoch 121
I0629 18:14:44.105767  8094 data_reader.cpp:262] Starting prefetch of epoch 121
I0629 18:14:44.907851  8090 solver.cpp:675] Snapshotting to binary proto file training/cityscapes20_jsegnet21v2_2017-06-29_16-47-23/initial/cityscapes20_jsegnet21v2_iter_20000.caffemodel
I0629 18:14:45.061719  8090 sgd_solver.cpp:288] Snapshotting solver state to binary proto file training/cityscapes20_jsegnet21v2_2017-06-29_16-47-23/initial/cityscapes20_jsegnet21v2_iter_20000.solverstate
I0629 18:14:45.069720  8090 solver.cpp:545] Iteration 20000, Testing net (#0)
I0629 18:14:53.296751  8088 data_reader.cpp:262] Starting prefetch of epoch 14
I0629 18:14:53.296771  8119 data_reader.cpp:262] Starting prefetch of epoch 14
I0629 18:14:53.296751  8128 data_reader.cpp:262] Starting prefetch of epoch 14
I0629 18:14:53.525199  8117 data_reader.cpp:262] Starting prefetch of epoch 14
I0629 18:14:53.525199  8126 data_reader.cpp:262] Starting prefetch of epoch 14
I0629 18:14:53.525199  8086 data_reader.cpp:262] Starting prefetch of epoch 14
I0629 18:15:13.289386  8088 data_reader.cpp:262] Starting prefetch of epoch 15
I0629 18:15:13.289391  8128 data_reader.cpp:262] Starting prefetch of epoch 15
I0629 18:15:13.289396  8119 data_reader.cpp:262] Starting prefetch of epoch 15
I0629 18:15:13.664429  8117 data_reader.cpp:262] Starting prefetch of epoch 15
I0629 18:15:13.664443  8126 data_reader.cpp:262] Starting prefetch of epoch 15
I0629 18:15:13.664429  8086 data_reader.cpp:262] Starting prefetch of epoch 15
I0629 18:15:14.253288  8090 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.924095
I0629 18:15:14.253381  8090 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.993629
I0629 18:15:14.253391  8090 solver.cpp:630]     Test net output #2: loss = 0.234347 (* 1 = 0.234347 loss)
I0629 18:15:14.253422  8090 solver.cpp:305] [MultiGPU] Tests completed in 29.1829s
I0629 18:15:14.473798  8090 solver.cpp:349] Iteration 20000 (1.97005 iter/s, 50.7602s/100 iter), loss = 0.191972
I0629 18:15:14.473820  8090 solver.cpp:371]     Train net output #0: loss = 0.191972 (* 1 = 0.191972 loss)
I0629 18:15:14.473824  8090 sgd_solver.cpp:137] Iteration 20000, lr = 0.0001, m = 0.9
I0629 18:15:44.203644  8090 solver.cpp:349] Iteration 20100 (3.36372 iter/s, 29.729s/100 iter), loss = 0.231392
I0629 18:15:44.203666  8090 solver.cpp:371]     Train net output #0: loss = 0.231392 (* 1 = 0.231392 loss)
I0629 18:15:44.203670  8090 sgd_solver.cpp:137] Iteration 20100, lr = 0.0001, m = 0.9
I0629 18:15:57.394351  8069 data_reader.cpp:262] Starting prefetch of epoch 122
I0629 18:15:57.394990  8096 data_reader.cpp:262] Starting prefetch of epoch 122
I0629 18:15:57.395483  8098 data_reader.cpp:262] Starting prefetch of epoch 122
I0629 18:15:57.433133  8093 data_reader.cpp:262] Starting prefetch of epoch 122
I0629 18:15:57.434159  8067 data_reader.cpp:262] Starting prefetch of epoch 122
I0629 18:15:57.434458  8094 data_reader.cpp:262] Starting prefetch of epoch 122
I0629 18:16:05.900732  8090 solver.cpp:349] Iteration 20200 (4.60905 iter/s, 21.6965s/100 iter), loss = 0.203294
I0629 18:16:05.900758  8090 solver.cpp:371]     Train net output #0: loss = 0.203294 (* 1 = 0.203294 loss)
I0629 18:16:05.900761  8090 sgd_solver.cpp:137] Iteration 20200, lr = 0.0001, m = 0.9
I0629 18:16:27.173846  8090 solver.cpp:349] Iteration 20300 (4.70091 iter/s, 21.2725s/100 iter), loss = 0.258771
I0629 18:16:27.173872  8090 solver.cpp:371]     Train net output #0: loss = 0.258771 (* 1 = 0.258771 loss)
I0629 18:16:27.173878  8090 sgd_solver.cpp:137] Iteration 20300, lr = 0.0001, m = 0.9
I0629 18:16:32.713258  8069 data_reader.cpp:262] Starting prefetch of epoch 123
I0629 18:16:32.718647  8098 data_reader.cpp:262] Starting prefetch of epoch 123
I0629 18:16:32.724825  8093 data_reader.cpp:262] Starting prefetch of epoch 123
I0629 18:16:32.730357  8096 data_reader.cpp:262] Starting prefetch of epoch 123
I0629 18:16:32.735275  8094 data_reader.cpp:262] Starting prefetch of epoch 123
I0629 18:16:32.741181  8067 data_reader.cpp:262] Starting prefetch of epoch 123
I0629 18:16:48.505606  8090 solver.cpp:349] Iteration 20400 (4.68798 iter/s, 21.3311s/100 iter), loss = 0.257167
I0629 18:16:48.505632  8090 solver.cpp:371]     Train net output #0: loss = 0.257167 (* 1 = 0.257167 loss)
I0629 18:16:48.505636  8090 sgd_solver.cpp:137] Iteration 20400, lr = 0.0001, m = 0.9
I0629 18:17:07.888456  8098 data_reader.cpp:262] Starting prefetch of epoch 124
I0629 18:17:07.892861  8096 data_reader.cpp:262] Starting prefetch of epoch 124
I0629 18:17:07.897177  8069 data_reader.cpp:262] Starting prefetch of epoch 124
I0629 18:17:07.922911  8094 data_reader.cpp:262] Starting prefetch of epoch 124
I0629 18:17:07.924013  8093 data_reader.cpp:262] Starting prefetch of epoch 124
I0629 18:17:07.924749  8067 data_reader.cpp:262] Starting prefetch of epoch 124
I0629 18:17:09.771809  8090 solver.cpp:349] Iteration 20500 (4.70243 iter/s, 21.2656s/100 iter), loss = 0.166289
I0629 18:17:09.771834  8090 solver.cpp:371]     Train net output #0: loss = 0.166289 (* 1 = 0.166289 loss)
I0629 18:17:09.771838  8090 sgd_solver.cpp:137] Iteration 20500, lr = 0.0001, m = 0.9
I0629 18:17:31.184783  8090 solver.cpp:349] Iteration 20600 (4.67021 iter/s, 21.4123s/100 iter), loss = 0.150519
I0629 18:17:31.184805  8090 solver.cpp:371]     Train net output #0: loss = 0.150519 (* 1 = 0.150519 loss)
I0629 18:17:31.184809  8090 sgd_solver.cpp:137] Iteration 20600, lr = 0.0001, m = 0.9
I0629 18:17:43.085384  8069 data_reader.cpp:262] Starting prefetch of epoch 125
I0629 18:17:43.088292  8098 data_reader.cpp:262] Starting prefetch of epoch 125
I0629 18:17:43.090813  8096 data_reader.cpp:262] Starting prefetch of epoch 125
I0629 18:17:43.111981  8093 data_reader.cpp:262] Starting prefetch of epoch 125
I0629 18:17:43.115638  8094 data_reader.cpp:262] Starting prefetch of epoch 125
I0629 18:17:43.116293  8067 data_reader.cpp:262] Starting prefetch of epoch 125
I0629 18:17:52.350611  8090 solver.cpp:349] Iteration 20700 (4.72475 iter/s, 21.1651s/100 iter), loss = 0.134961
I0629 18:17:52.350633  8090 solver.cpp:371]     Train net output #0: loss = 0.134961 (* 1 = 0.134961 loss)
I0629 18:17:52.350637  8090 sgd_solver.cpp:137] Iteration 20700, lr = 0.0001, m = 0.9
I0629 18:18:13.667424  8090 solver.cpp:349] Iteration 20800 (4.69128 iter/s, 21.3161s/100 iter), loss = 0.189964
I0629 18:18:13.667471  8090 solver.cpp:371]     Train net output #0: loss = 0.189964 (* 1 = 0.189964 loss)
I0629 18:18:13.667476  8090 sgd_solver.cpp:137] Iteration 20800, lr = 0.0001, m = 0.9
I0629 18:18:18.184034  8069 data_reader.cpp:262] Starting prefetch of epoch 126
I0629 18:18:18.189740  8096 data_reader.cpp:262] Starting prefetch of epoch 126
I0629 18:18:18.191524  8098 data_reader.cpp:262] Starting prefetch of epoch 126
I0629 18:18:18.207263  8094 data_reader.cpp:262] Starting prefetch of epoch 126
I0629 18:18:18.213543  8093 data_reader.cpp:262] Starting prefetch of epoch 126
I0629 18:18:18.219848  8067 data_reader.cpp:262] Starting prefetch of epoch 126
I0629 18:18:35.001220  8090 solver.cpp:349] Iteration 20900 (4.68755 iter/s, 21.3331s/100 iter), loss = 0.133329
I0629 18:18:35.001241  8090 solver.cpp:371]     Train net output #0: loss = 0.133329 (* 1 = 0.133329 loss)
I0629 18:18:35.001245  8090 sgd_solver.cpp:137] Iteration 20900, lr = 0.0001, m = 0.9
I0629 18:18:53.602041  8069 data_reader.cpp:262] Starting prefetch of epoch 127
I0629 18:18:53.602277  8098 data_reader.cpp:262] Starting prefetch of epoch 127
I0629 18:18:53.606122  8096 data_reader.cpp:262] Starting prefetch of epoch 127
I0629 18:18:53.621881  8094 data_reader.cpp:262] Starting prefetch of epoch 127
I0629 18:18:53.623484  8093 data_reader.cpp:262] Starting prefetch of epoch 127
I0629 18:18:53.627744  8067 data_reader.cpp:262] Starting prefetch of epoch 127
I0629 18:18:56.389181  8090 solver.cpp:349] Iteration 21000 (4.67567 iter/s, 21.3873s/100 iter), loss = 0.262831
I0629 18:18:56.389209  8090 solver.cpp:371]     Train net output #0: loss = 0.262832 (* 1 = 0.262832 loss)
I0629 18:18:56.389214  8090 sgd_solver.cpp:137] Iteration 21000, lr = 0.0001, m = 0.9
I0629 18:19:17.736451  8090 solver.cpp:349] Iteration 21100 (4.68459 iter/s, 21.3466s/100 iter), loss = 0.195597
I0629 18:19:17.736474  8090 solver.cpp:371]     Train net output #0: loss = 0.195597 (* 1 = 0.195597 loss)
I0629 18:19:17.736477  8090 sgd_solver.cpp:137] Iteration 21100, lr = 0.0001, m = 0.9
I0629 18:19:28.839766  8069 data_reader.cpp:262] Starting prefetch of epoch 128
I0629 18:19:28.839766  8098 data_reader.cpp:262] Starting prefetch of epoch 128
I0629 18:19:28.839766  8096 data_reader.cpp:262] Starting prefetch of epoch 128
I0629 18:19:28.862625  8067 data_reader.cpp:262] Starting prefetch of epoch 128
I0629 18:19:28.864684  8093 data_reader.cpp:262] Starting prefetch of epoch 128
I0629 18:19:28.871099  8094 data_reader.cpp:262] Starting prefetch of epoch 128
I0629 18:19:39.192675  8090 solver.cpp:349] Iteration 21200 (4.66079 iter/s, 21.4556s/100 iter), loss = 0.362565
I0629 18:19:39.192701  8090 solver.cpp:371]     Train net output #0: loss = 0.362565 (* 1 = 0.362565 loss)
I0629 18:19:39.192706  8090 sgd_solver.cpp:137] Iteration 21200, lr = 0.0001, m = 0.9
I0629 18:20:00.439162  8090 solver.cpp:349] Iteration 21300 (4.7068 iter/s, 21.2458s/100 iter), loss = 0.144651
I0629 18:20:00.439211  8090 solver.cpp:371]     Train net output #0: loss = 0.144651 (* 1 = 0.144651 loss)
I0629 18:20:00.439218  8090 sgd_solver.cpp:137] Iteration 21300, lr = 0.0001, m = 0.9
I0629 18:20:04.100790  8069 data_reader.cpp:262] Starting prefetch of epoch 129
I0629 18:20:04.108717  8096 data_reader.cpp:262] Starting prefetch of epoch 129
I0629 18:20:04.108717  8098 data_reader.cpp:262] Starting prefetch of epoch 129
I0629 18:20:04.125768  8067 data_reader.cpp:262] Starting prefetch of epoch 129
I0629 18:20:04.135510  8093 data_reader.cpp:262] Starting prefetch of epoch 129
I0629 18:20:04.135622  8094 data_reader.cpp:262] Starting prefetch of epoch 129
I0629 18:20:21.758934  8090 solver.cpp:349] Iteration 21400 (4.69063 iter/s, 21.3191s/100 iter), loss = 0.200796
I0629 18:20:21.758960  8090 solver.cpp:371]     Train net output #0: loss = 0.200796 (* 1 = 0.200796 loss)
I0629 18:20:21.758965  8090 sgd_solver.cpp:137] Iteration 21400, lr = 0.0001, m = 0.9
I0629 18:20:39.463275  8098 data_reader.cpp:262] Starting prefetch of epoch 130
I0629 18:20:39.463747  8069 data_reader.cpp:262] Starting prefetch of epoch 130
I0629 18:20:39.464385  8096 data_reader.cpp:262] Starting prefetch of epoch 130
I0629 18:20:39.467921  8094 data_reader.cpp:262] Starting prefetch of epoch 130
I0629 18:20:39.483768  8093 data_reader.cpp:262] Starting prefetch of epoch 130
I0629 18:20:39.489043  8067 data_reader.cpp:262] Starting prefetch of epoch 130
I0629 18:20:43.046914  8090 solver.cpp:349] Iteration 21500 (4.69763 iter/s, 21.2873s/100 iter), loss = 0.177709
I0629 18:20:43.046936  8090 solver.cpp:371]     Train net output #0: loss = 0.177709 (* 1 = 0.177709 loss)
I0629 18:20:43.046941  8090 sgd_solver.cpp:137] Iteration 21500, lr = 0.0001, m = 0.9
I0629 18:21:04.349512  8090 solver.cpp:349] Iteration 21600 (4.6944 iter/s, 21.302s/100 iter), loss = 0.232417
I0629 18:21:04.349534  8090 solver.cpp:371]     Train net output #0: loss = 0.232417 (* 1 = 0.232417 loss)
I0629 18:21:04.349537  8090 sgd_solver.cpp:137] Iteration 21600, lr = 0.0001, m = 0.9
I0629 18:21:14.659564  8096 data_reader.cpp:262] Starting prefetch of epoch 131
I0629 18:21:14.678280  8098 data_reader.cpp:262] Starting prefetch of epoch 131
I0629 18:21:14.680567  8069 data_reader.cpp:262] Starting prefetch of epoch 131
I0629 18:21:14.694504  8094 data_reader.cpp:262] Starting prefetch of epoch 131
I0629 18:21:14.696007  8093 data_reader.cpp:262] Starting prefetch of epoch 131
I0629 18:21:14.697458  8067 data_reader.cpp:262] Starting prefetch of epoch 131
I0629 18:21:25.730381  8090 solver.cpp:349] Iteration 21700 (4.67722 iter/s, 21.3802s/100 iter), loss = 0.238455
I0629 18:21:25.730406  8090 solver.cpp:371]     Train net output #0: loss = 0.238455 (* 1 = 0.238455 loss)
I0629 18:21:25.730410  8090 sgd_solver.cpp:137] Iteration 21700, lr = 0.0001, m = 0.9
I0629 18:21:47.225890  8090 solver.cpp:349] Iteration 21800 (4.65227 iter/s, 21.4949s/100 iter), loss = 0.179912
I0629 18:21:47.225940  8090 solver.cpp:371]     Train net output #0: loss = 0.179912 (* 1 = 0.179912 loss)
I0629 18:21:47.225946  8090 sgd_solver.cpp:137] Iteration 21800, lr = 0.0001, m = 0.9
I0629 18:21:50.067795  8069 data_reader.cpp:262] Starting prefetch of epoch 132
I0629 18:21:50.073897  8096 data_reader.cpp:262] Starting prefetch of epoch 132
I0629 18:21:50.074354  8098 data_reader.cpp:262] Starting prefetch of epoch 132
I0629 18:21:50.099719  8093 data_reader.cpp:262] Starting prefetch of epoch 132
I0629 18:21:50.100772  8094 data_reader.cpp:262] Starting prefetch of epoch 132
I0629 18:21:50.103632  8067 data_reader.cpp:262] Starting prefetch of epoch 132
I0629 18:22:08.536367  8090 solver.cpp:349] Iteration 21900 (4.69267 iter/s, 21.3098s/100 iter), loss = 0.188333
I0629 18:22:08.536388  8090 solver.cpp:371]     Train net output #0: loss = 0.188333 (* 1 = 0.188333 loss)
I0629 18:22:08.536393  8090 sgd_solver.cpp:137] Iteration 21900, lr = 0.0001, m = 0.9
I0629 18:22:25.101294  8098 data_reader.cpp:262] Starting prefetch of epoch 133
I0629 18:22:25.122484  8093 data_reader.cpp:262] Starting prefetch of epoch 133
I0629 18:22:25.121528  8069 data_reader.cpp:262] Starting prefetch of epoch 133
I0629 18:22:25.124457  8096 data_reader.cpp:262] Starting prefetch of epoch 133
I0629 18:22:25.128415  8094 data_reader.cpp:262] Starting prefetch of epoch 133
I0629 18:22:25.138133  8067 data_reader.cpp:262] Starting prefetch of epoch 133
I0629 18:22:29.541703  8090 solver.cpp:545] Iteration 22000, Testing net (#0)
I0629 18:22:51.469100  8088 data_reader.cpp:262] Starting prefetch of epoch 16
I0629 18:22:51.469100  8119 data_reader.cpp:262] Starting prefetch of epoch 16
I0629 18:22:51.469100  8128 data_reader.cpp:262] Starting prefetch of epoch 16
I0629 18:22:51.794208  8086 data_reader.cpp:262] Starting prefetch of epoch 16
I0629 18:22:51.794208  8126 data_reader.cpp:262] Starting prefetch of epoch 16
I0629 18:22:51.794206  8117 data_reader.cpp:262] Starting prefetch of epoch 16
I0629 18:23:02.736060  8090 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.926009
I0629 18:23:02.736135  8090 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.993681
I0629 18:23:02.736142  8090 solver.cpp:630]     Test net output #2: loss = 0.235844 (* 1 = 0.235844 loss)
I0629 18:23:02.736167  8090 solver.cpp:305] [MultiGPU] Tests completed in 33.1936s
I0629 18:23:02.970206  8090 solver.cpp:349] Iteration 22000 (1.83714 iter/s, 54.4324s/100 iter), loss = 0.35483
I0629 18:23:02.970232  8090 solver.cpp:371]     Train net output #0: loss = 0.35483 (* 1 = 0.35483 loss)
I0629 18:23:02.970237  8090 sgd_solver.cpp:137] Iteration 22000, lr = 0.0001, m = 0.9
I0629 18:23:24.172144  8090 solver.cpp:349] Iteration 22100 (4.71668 iter/s, 21.2013s/100 iter), loss = 0.340491
I0629 18:23:24.172168  8090 solver.cpp:371]     Train net output #0: loss = 0.340491 (* 1 = 0.340491 loss)
I0629 18:23:24.172173  8090 sgd_solver.cpp:137] Iteration 22100, lr = 0.0001, m = 0.9
I0629 18:23:33.563644  8069 data_reader.cpp:262] Starting prefetch of epoch 134
I0629 18:23:33.571651  8098 data_reader.cpp:262] Starting prefetch of epoch 134
I0629 18:23:33.577260  8096 data_reader.cpp:262] Starting prefetch of epoch 134
I0629 18:23:33.577934  8067 data_reader.cpp:262] Starting prefetch of epoch 134
I0629 18:23:33.589859  8093 data_reader.cpp:262] Starting prefetch of epoch 134
I0629 18:23:33.592233  8094 data_reader.cpp:262] Starting prefetch of epoch 134
I0629 18:23:45.627554  8090 solver.cpp:349] Iteration 22200 (4.66096 iter/s, 21.4548s/100 iter), loss = 0.204408
I0629 18:23:45.627576  8090 solver.cpp:371]     Train net output #0: loss = 0.204408 (* 1 = 0.204408 loss)
I0629 18:23:45.627580  8090 sgd_solver.cpp:137] Iteration 22200, lr = 0.0001, m = 0.9
I0629 18:24:06.879609  8090 solver.cpp:349] Iteration 22300 (4.70555 iter/s, 21.2515s/100 iter), loss = 0.138621
I0629 18:24:06.879663  8090 solver.cpp:371]     Train net output #0: loss = 0.138621 (* 1 = 0.138621 loss)
I0629 18:24:06.879667  8090 sgd_solver.cpp:137] Iteration 22300, lr = 0.0001, m = 0.9
I0629 18:24:08.789577  8096 data_reader.cpp:262] Starting prefetch of epoch 135
I0629 18:24:08.796797  8069 data_reader.cpp:262] Starting prefetch of epoch 135
I0629 18:24:08.805261  8098 data_reader.cpp:262] Starting prefetch of epoch 135
I0629 18:24:08.823454  8094 data_reader.cpp:262] Starting prefetch of epoch 135
I0629 18:24:08.824208  8067 data_reader.cpp:262] Starting prefetch of epoch 135
I0629 18:24:08.826709  8093 data_reader.cpp:262] Starting prefetch of epoch 135
I0629 18:24:28.268056  8090 solver.cpp:349] Iteration 22400 (4.67555 iter/s, 21.3878s/100 iter), loss = 0.182794
I0629 18:24:28.268079  8090 solver.cpp:371]     Train net output #0: loss = 0.182794 (* 1 = 0.182794 loss)
I0629 18:24:28.268084  8090 sgd_solver.cpp:137] Iteration 22400, lr = 0.0001, m = 0.9
I0629 18:24:44.072791  8069 data_reader.cpp:262] Starting prefetch of epoch 136
I0629 18:24:44.081241  8098 data_reader.cpp:262] Starting prefetch of epoch 136
I0629 18:24:44.081626  8096 data_reader.cpp:262] Starting prefetch of epoch 136
I0629 18:24:44.102593  8093 data_reader.cpp:262] Starting prefetch of epoch 136
I0629 18:24:44.105468  8094 data_reader.cpp:262] Starting prefetch of epoch 136
I0629 18:24:44.106765  8067 data_reader.cpp:262] Starting prefetch of epoch 136
I0629 18:24:49.601052  8090 solver.cpp:349] Iteration 22500 (4.6877 iter/s, 21.3324s/100 iter), loss = 0.250648
I0629 18:24:49.601071  8090 solver.cpp:371]     Train net output #0: loss = 0.250648 (* 1 = 0.250648 loss)
I0629 18:24:49.601075  8090 sgd_solver.cpp:137] Iteration 22500, lr = 0.0001, m = 0.9
I0629 18:25:10.884780  8090 solver.cpp:349] Iteration 22600 (4.69855 iter/s, 21.2832s/100 iter), loss = 0.117816
I0629 18:25:10.884804  8090 solver.cpp:371]     Train net output #0: loss = 0.117817 (* 1 = 0.117817 loss)
I0629 18:25:10.884807  8090 sgd_solver.cpp:137] Iteration 22600, lr = 0.0001, m = 0.9
I0629 18:25:19.483711  8096 data_reader.cpp:262] Starting prefetch of epoch 137
I0629 18:25:19.484637  8069 data_reader.cpp:262] Starting prefetch of epoch 137
I0629 18:25:19.486416  8098 data_reader.cpp:262] Starting prefetch of epoch 137
I0629 18:25:19.499080  8094 data_reader.cpp:262] Starting prefetch of epoch 137
I0629 18:25:19.499308  8067 data_reader.cpp:262] Starting prefetch of epoch 137
I0629 18:25:19.501757  8093 data_reader.cpp:262] Starting prefetch of epoch 137
I0629 18:25:32.272725  8090 solver.cpp:349] Iteration 22700 (4.67565 iter/s, 21.3874s/100 iter), loss = 0.209797
I0629 18:25:32.272747  8090 solver.cpp:371]     Train net output #0: loss = 0.209797 (* 1 = 0.209797 loss)
I0629 18:25:32.272752  8090 sgd_solver.cpp:137] Iteration 22700, lr = 0.0001, m = 0.9
I0629 18:25:53.541242  8090 solver.cpp:349] Iteration 22800 (4.70191 iter/s, 21.268s/100 iter), loss = 0.164213
I0629 18:25:53.541311  8090 solver.cpp:371]     Train net output #0: loss = 0.164213 (* 1 = 0.164213 loss)
I0629 18:25:53.541316  8090 sgd_solver.cpp:137] Iteration 22800, lr = 0.0001, m = 0.9
I0629 18:25:54.610689  8096 data_reader.cpp:262] Starting prefetch of epoch 138
I0629 18:25:54.611843  8098 data_reader.cpp:262] Starting prefetch of epoch 138
I0629 18:25:54.628396  8069 data_reader.cpp:262] Starting prefetch of epoch 138
I0629 18:25:54.634171  8067 data_reader.cpp:262] Starting prefetch of epoch 138
I0629 18:25:54.642045  8093 data_reader.cpp:262] Starting prefetch of epoch 138
I0629 18:25:54.642326  8094 data_reader.cpp:262] Starting prefetch of epoch 138
I0629 18:26:15.040356  8090 solver.cpp:349] Iteration 22900 (4.65148 iter/s, 21.4985s/100 iter), loss = 0.220085
I0629 18:26:15.040377  8090 solver.cpp:371]     Train net output #0: loss = 0.220085 (* 1 = 0.220085 loss)
I0629 18:26:15.040381  8090 sgd_solver.cpp:137] Iteration 22900, lr = 0.0001, m = 0.9
I0629 18:26:29.917971  8098 data_reader.cpp:262] Starting prefetch of epoch 139
I0629 18:26:29.919006  8096 data_reader.cpp:262] Starting prefetch of epoch 139
I0629 18:26:29.922159  8069 data_reader.cpp:262] Starting prefetch of epoch 139
I0629 18:26:29.938498  8094 data_reader.cpp:262] Starting prefetch of epoch 139
I0629 18:26:29.945863  8093 data_reader.cpp:262] Starting prefetch of epoch 139
I0629 18:26:29.947306  8067 data_reader.cpp:262] Starting prefetch of epoch 139
I0629 18:26:36.261533  8090 solver.cpp:349] Iteration 23000 (4.71239 iter/s, 21.2206s/100 iter), loss = 0.182833
I0629 18:26:36.261556  8090 solver.cpp:371]     Train net output #0: loss = 0.182834 (* 1 = 0.182834 loss)
I0629 18:26:36.261562  8090 sgd_solver.cpp:137] Iteration 23000, lr = 0.0001, m = 0.9
I0629 18:26:57.529839  8090 solver.cpp:349] Iteration 23100 (4.70195 iter/s, 21.2678s/100 iter), loss = 0.187394
I0629 18:26:57.529861  8090 solver.cpp:371]     Train net output #0: loss = 0.187394 (* 1 = 0.187394 loss)
I0629 18:26:57.529866  8090 sgd_solver.cpp:137] Iteration 23100, lr = 0.0001, m = 0.9
I0629 18:27:04.969830  8098 data_reader.cpp:262] Starting prefetch of epoch 140
I0629 18:27:04.971372  8096 data_reader.cpp:262] Starting prefetch of epoch 140
I0629 18:27:04.979187  8069 data_reader.cpp:262] Starting prefetch of epoch 140
I0629 18:27:04.988646  8093 data_reader.cpp:262] Starting prefetch of epoch 140
I0629 18:27:04.992583  8067 data_reader.cpp:262] Starting prefetch of epoch 140
I0629 18:27:04.999933  8094 data_reader.cpp:262] Starting prefetch of epoch 140
I0629 18:27:18.780191  8090 solver.cpp:349] Iteration 23200 (4.70592 iter/s, 21.2498s/100 iter), loss = 0.185157
I0629 18:27:18.780211  8090 solver.cpp:371]     Train net output #0: loss = 0.185157 (* 1 = 0.185157 loss)
I0629 18:27:18.780215  8090 sgd_solver.cpp:137] Iteration 23200, lr = 0.0001, m = 0.9
I0629 18:27:40.085605  8090 solver.cpp:349] Iteration 23300 (4.69376 iter/s, 21.3049s/100 iter), loss = 0.141757
I0629 18:27:40.085652  8090 solver.cpp:371]     Train net output #0: loss = 0.141757 (* 1 = 0.141757 loss)
I0629 18:27:40.085656  8090 sgd_solver.cpp:137] Iteration 23300, lr = 0.0001, m = 0.9
I0629 18:27:40.303356  8096 data_reader.cpp:262] Starting prefetch of epoch 141
I0629 18:27:40.307380  8098 data_reader.cpp:262] Starting prefetch of epoch 141
I0629 18:27:40.307380  8069 data_reader.cpp:262] Starting prefetch of epoch 141
I0629 18:27:40.323449  8067 data_reader.cpp:262] Starting prefetch of epoch 141
I0629 18:27:40.323545  8094 data_reader.cpp:262] Starting prefetch of epoch 141
I0629 18:27:40.330569  8093 data_reader.cpp:262] Starting prefetch of epoch 141
I0629 18:28:01.290582  8090 solver.cpp:349] Iteration 23400 (4.71599 iter/s, 21.2044s/100 iter), loss = 0.164428
I0629 18:28:01.290602  8090 solver.cpp:371]     Train net output #0: loss = 0.164428 (* 1 = 0.164428 loss)
I0629 18:28:01.290607  8090 sgd_solver.cpp:137] Iteration 23400, lr = 0.0001, m = 0.9
I0629 18:28:15.357594  8098 data_reader.cpp:262] Starting prefetch of epoch 142
I0629 18:28:15.357594  8069 data_reader.cpp:262] Starting prefetch of epoch 142
I0629 18:28:15.364827  8096 data_reader.cpp:262] Starting prefetch of epoch 142
I0629 18:28:15.373479  8094 data_reader.cpp:262] Starting prefetch of epoch 142
I0629 18:28:15.381687  8067 data_reader.cpp:262] Starting prefetch of epoch 142
I0629 18:28:15.387643  8093 data_reader.cpp:262] Starting prefetch of epoch 142
I0629 18:28:22.615720  8090 solver.cpp:349] Iteration 23500 (4.68941 iter/s, 21.3246s/100 iter), loss = 0.447248
I0629 18:28:22.615744  8090 solver.cpp:371]     Train net output #0: loss = 0.447248 (* 1 = 0.447248 loss)
I0629 18:28:22.615748  8090 sgd_solver.cpp:137] Iteration 23500, lr = 0.0001, m = 0.9
I0629 18:28:43.989842  8090 solver.cpp:349] Iteration 23600 (4.67867 iter/s, 21.3736s/100 iter), loss = 0.220365
I0629 18:28:43.989864  8090 solver.cpp:371]     Train net output #0: loss = 0.220365 (* 1 = 0.220365 loss)
I0629 18:28:43.989868  8090 sgd_solver.cpp:137] Iteration 23600, lr = 0.0001, m = 0.9
I0629 18:28:50.537289  8098 data_reader.cpp:262] Starting prefetch of epoch 143
I0629 18:28:50.538169  8096 data_reader.cpp:262] Starting prefetch of epoch 143
I0629 18:28:50.543303  8069 data_reader.cpp:262] Starting prefetch of epoch 143
I0629 18:28:50.561252  8094 data_reader.cpp:262] Starting prefetch of epoch 143
I0629 18:28:50.562815  8067 data_reader.cpp:262] Starting prefetch of epoch 143
I0629 18:28:50.568105  8093 data_reader.cpp:262] Starting prefetch of epoch 143
I0629 18:29:05.164947  8090 solver.cpp:349] Iteration 23700 (4.72264 iter/s, 21.1746s/100 iter), loss = 0.211267
I0629 18:29:05.164969  8090 solver.cpp:371]     Train net output #0: loss = 0.211267 (* 1 = 0.211267 loss)
I0629 18:29:05.164974  8090 sgd_solver.cpp:137] Iteration 23700, lr = 0.0001, m = 0.9
I0629 18:29:25.745167  8098 data_reader.cpp:262] Starting prefetch of epoch 144
I0629 18:29:25.747422  8096 data_reader.cpp:262] Starting prefetch of epoch 144
I0629 18:29:25.753718  8069 data_reader.cpp:262] Starting prefetch of epoch 144
I0629 18:29:25.763839  8093 data_reader.cpp:262] Starting prefetch of epoch 144
I0629 18:29:25.775214  8067 data_reader.cpp:262] Starting prefetch of epoch 144
I0629 18:29:25.776275  8094 data_reader.cpp:262] Starting prefetch of epoch 144
I0629 18:29:26.581532  8090 solver.cpp:349] Iteration 23800 (4.66939 iter/s, 21.4161s/100 iter), loss = 0.280077
I0629 18:29:26.581554  8090 solver.cpp:371]     Train net output #0: loss = 0.280077 (* 1 = 0.280077 loss)
I0629 18:29:26.581558  8090 sgd_solver.cpp:137] Iteration 23800, lr = 0.0001, m = 0.9
I0629 18:29:47.863744  8090 solver.cpp:349] Iteration 23900 (4.69887 iter/s, 21.2817s/100 iter), loss = 0.176224
I0629 18:29:47.863766  8090 solver.cpp:371]     Train net output #0: loss = 0.176224 (* 1 = 0.176224 loss)
I0629 18:29:47.863771  8090 sgd_solver.cpp:137] Iteration 23900, lr = 0.0001, m = 0.9
I0629 18:30:01.073652  8069 data_reader.cpp:262] Starting prefetch of epoch 145
I0629 18:30:01.080992  8098 data_reader.cpp:262] Starting prefetch of epoch 145
I0629 18:30:01.082375  8096 data_reader.cpp:262] Starting prefetch of epoch 145
I0629 18:30:01.089562  8094 data_reader.cpp:262] Starting prefetch of epoch 145
I0629 18:30:01.106266  8067 data_reader.cpp:262] Starting prefetch of epoch 145
I0629 18:30:01.109776  8093 data_reader.cpp:262] Starting prefetch of epoch 145
I0629 18:30:08.942075  8090 solver.cpp:545] Iteration 24000, Testing net (#0)
I0629 18:30:17.365327  8088 data_reader.cpp:262] Starting prefetch of epoch 17
I0629 18:30:17.365345  8119 data_reader.cpp:262] Starting prefetch of epoch 17
I0629 18:30:17.365384  8128 data_reader.cpp:262] Starting prefetch of epoch 17
I0629 18:30:17.453408  8126 data_reader.cpp:262] Starting prefetch of epoch 17
I0629 18:30:17.453419  8117 data_reader.cpp:262] Starting prefetch of epoch 17
I0629 18:30:17.453408  8086 data_reader.cpp:262] Starting prefetch of epoch 17
I0629 18:30:36.926666  8088 data_reader.cpp:262] Starting prefetch of epoch 18
I0629 18:30:36.926676  8128 data_reader.cpp:262] Starting prefetch of epoch 18
I0629 18:30:36.926677  8119 data_reader.cpp:262] Starting prefetch of epoch 18
I0629 18:30:37.231539  8086 data_reader.cpp:262] Starting prefetch of epoch 18
I0629 18:30:37.231539  8126 data_reader.cpp:262] Starting prefetch of epoch 18
I0629 18:30:37.231604  8117 data_reader.cpp:262] Starting prefetch of epoch 18
I0629 18:30:38.009729  8090 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.923082
I0629 18:30:38.009752  8090 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.993485
I0629 18:30:38.009757  8090 solver.cpp:630]     Test net output #2: loss = 0.242933 (* 1 = 0.242933 loss)
I0629 18:30:38.009784  8090 solver.cpp:305] [MultiGPU] Tests completed in 29.0671s
I0629 18:30:38.135612  8141 sgd_solver.cpp:48] MultiStep Status: Iteration 24000, step = 1
I0629 18:30:38.135612  8139 sgd_solver.cpp:48] MultiStep Status: Iteration 24000, step = 1
I0629 18:30:38.135612  8140 sgd_solver.cpp:48] MultiStep Status: Iteration 24000, step = 1
I0629 18:30:38.244681  8090 solver.cpp:349] Iteration 24000 (1.98492 iter/s, 50.3799s/100 iter), loss = 0.215641
I0629 18:30:38.244706  8090 solver.cpp:371]     Train net output #0: loss = 0.215641 (* 1 = 0.215641 loss)
I0629 18:30:38.244710  8090 sgd_solver.cpp:137] Iteration 24000, lr = 1e-05, m = 0.9
I0629 18:30:59.523108  8090 solver.cpp:349] Iteration 24100 (4.6997 iter/s, 21.2779s/100 iter), loss = 0.230355
I0629 18:30:59.523136  8090 solver.cpp:371]     Train net output #0: loss = 0.230355 (* 1 = 0.230355 loss)
I0629 18:30:59.523144  8090 sgd_solver.cpp:137] Iteration 24100, lr = 1e-05, m = 0.9
I0629 18:31:05.319682  8098 data_reader.cpp:262] Starting prefetch of epoch 146
I0629 18:31:05.320008  8069 data_reader.cpp:262] Starting prefetch of epoch 146
I0629 18:31:05.325528  8096 data_reader.cpp:262] Starting prefetch of epoch 146
I0629 18:31:05.334174  8094 data_reader.cpp:262] Starting prefetch of epoch 146
I0629 18:31:05.341886  8067 data_reader.cpp:262] Starting prefetch of epoch 146
I0629 18:31:05.349128  8093 data_reader.cpp:262] Starting prefetch of epoch 146
I0629 18:31:20.851845  8090 solver.cpp:349] Iteration 24200 (4.68862 iter/s, 21.3283s/100 iter), loss = 0.13289
I0629 18:31:20.851898  8090 solver.cpp:371]     Train net output #0: loss = 0.13289 (* 1 = 0.13289 loss)
I0629 18:31:20.851903  8090 sgd_solver.cpp:137] Iteration 24200, lr = 1e-05, m = 0.9
I0629 18:31:40.354666  8096 data_reader.cpp:262] Starting prefetch of epoch 147
I0629 18:31:40.354666  8098 data_reader.cpp:262] Starting prefetch of epoch 147
I0629 18:31:40.362221  8069 data_reader.cpp:262] Starting prefetch of epoch 147
I0629 18:31:40.380523  8067 data_reader.cpp:262] Starting prefetch of epoch 147
I0629 18:31:40.380523  8093 data_reader.cpp:262] Starting prefetch of epoch 147
I0629 18:31:40.384186  8094 data_reader.cpp:262] Starting prefetch of epoch 147
I0629 18:31:42.046000  8090 solver.cpp:349] Iteration 24300 (4.71839 iter/s, 21.1937s/100 iter), loss = 0.17218
I0629 18:31:42.046022  8090 solver.cpp:371]     Train net output #0: loss = 0.17218 (* 1 = 0.17218 loss)
I0629 18:31:42.046027  8090 sgd_solver.cpp:137] Iteration 24300, lr = 1e-05, m = 0.9
I0629 18:32:03.430119  8090 solver.cpp:349] Iteration 24400 (4.67647 iter/s, 21.3837s/100 iter), loss = 0.198027
I0629 18:32:03.430163  8090 solver.cpp:371]     Train net output #0: loss = 0.198027 (* 1 = 0.198027 loss)
I0629 18:32:03.430167  8090 sgd_solver.cpp:137] Iteration 24400, lr = 1e-05, m = 0.9
I0629 18:32:15.748486  8069 data_reader.cpp:262] Starting prefetch of epoch 148
I0629 18:32:15.748486  8098 data_reader.cpp:262] Starting prefetch of epoch 148
I0629 18:32:15.748486  8096 data_reader.cpp:262] Starting prefetch of epoch 148
I0629 18:32:15.758260  8093 data_reader.cpp:262] Starting prefetch of epoch 148
I0629 18:32:15.765614  8067 data_reader.cpp:262] Starting prefetch of epoch 148
I0629 18:32:15.765614  8094 data_reader.cpp:262] Starting prefetch of epoch 148
I0629 18:32:24.680315  8090 solver.cpp:349] Iteration 24500 (4.70595 iter/s, 21.2497s/100 iter), loss = 0.162874
I0629 18:32:24.680338  8090 solver.cpp:371]     Train net output #0: loss = 0.162874 (* 1 = 0.162874 loss)
I0629 18:32:24.680342  8090 sgd_solver.cpp:137] Iteration 24500, lr = 1e-05, m = 0.9
I0629 18:32:46.027396  8090 solver.cpp:349] Iteration 24600 (4.68458 iter/s, 21.3466s/100 iter), loss = 0.231846
I0629 18:32:46.027462  8090 solver.cpp:371]     Train net output #0: loss = 0.231847 (* 1 = 0.231847 loss)
I0629 18:32:46.027467  8090 sgd_solver.cpp:137] Iteration 24600, lr = 1e-05, m = 0.9
I0629 18:32:50.926555  8098 data_reader.cpp:262] Starting prefetch of epoch 149
I0629 18:32:50.930580  8096 data_reader.cpp:262] Starting prefetch of epoch 149
I0629 18:32:50.932234  8069 data_reader.cpp:262] Starting prefetch of epoch 149
I0629 18:32:50.953654  8067 data_reader.cpp:262] Starting prefetch of epoch 149
I0629 18:32:50.954282  8093 data_reader.cpp:262] Starting prefetch of epoch 149
I0629 18:32:50.955377  8094 data_reader.cpp:262] Starting prefetch of epoch 149
I0629 18:33:07.307452  8090 solver.cpp:349] Iteration 24700 (4.69935 iter/s, 21.2796s/100 iter), loss = 0.157501
I0629 18:33:07.307476  8090 solver.cpp:371]     Train net output #0: loss = 0.157501 (* 1 = 0.157501 loss)
I0629 18:33:07.307479  8090 sgd_solver.cpp:137] Iteration 24700, lr = 1e-05, m = 0.9
I0629 18:33:26.045496  8098 data_reader.cpp:262] Starting prefetch of epoch 150
I0629 18:33:26.045754  8096 data_reader.cpp:262] Starting prefetch of epoch 150
I0629 18:33:26.049219  8069 data_reader.cpp:262] Starting prefetch of epoch 150
I0629 18:33:26.058439  8094 data_reader.cpp:262] Starting prefetch of epoch 150
I0629 18:33:26.064623  8093 data_reader.cpp:262] Starting prefetch of epoch 150
I0629 18:33:26.065623  8067 data_reader.cpp:262] Starting prefetch of epoch 150
I0629 18:33:28.592308  8090 solver.cpp:349] Iteration 24800 (4.69827 iter/s, 21.2844s/100 iter), loss = 0.20209
I0629 18:33:28.592330  8090 solver.cpp:371]     Train net output #0: loss = 0.20209 (* 1 = 0.20209 loss)
I0629 18:33:28.592334  8090 sgd_solver.cpp:137] Iteration 24800, lr = 1e-05, m = 0.9
I0629 18:33:49.868053  8090 solver.cpp:349] Iteration 24900 (4.70029 iter/s, 21.2753s/100 iter), loss = 0.259423
I0629 18:33:49.868077  8090 solver.cpp:371]     Train net output #0: loss = 0.259423 (* 1 = 0.259423 loss)
I0629 18:33:49.868080  8090 sgd_solver.cpp:137] Iteration 24900, lr = 1e-05, m = 0.9
I0629 18:34:01.200887  8096 data_reader.cpp:262] Starting prefetch of epoch 151
I0629 18:34:01.210768  8098 data_reader.cpp:262] Starting prefetch of epoch 151
I0629 18:34:01.206331  8069 data_reader.cpp:262] Starting prefetch of epoch 151
I0629 18:34:01.227586  8094 data_reader.cpp:262] Starting prefetch of epoch 151
I0629 18:34:01.228461  8093 data_reader.cpp:262] Starting prefetch of epoch 151
I0629 18:34:01.229167  8067 data_reader.cpp:262] Starting prefetch of epoch 151
I0629 18:34:11.296829  8090 solver.cpp:349] Iteration 25000 (4.66672 iter/s, 21.4283s/100 iter), loss = 0.215863
I0629 18:34:11.296850  8090 solver.cpp:371]     Train net output #0: loss = 0.215863 (* 1 = 0.215863 loss)
I0629 18:34:11.296854  8090 sgd_solver.cpp:137] Iteration 25000, lr = 1e-05, m = 0.9
I0629 18:34:32.436276  8090 solver.cpp:349] Iteration 25100 (4.73059 iter/s, 21.139s/100 iter), loss = 0.196681
I0629 18:34:32.436322  8090 solver.cpp:371]     Train net output #0: loss = 0.196681 (* 1 = 0.196681 loss)
I0629 18:34:32.436327  8090 sgd_solver.cpp:137] Iteration 25100, lr = 1e-05, m = 0.9
I0629 18:34:36.515391  8096 data_reader.cpp:262] Starting prefetch of epoch 152
I0629 18:34:36.520014  8098 data_reader.cpp:262] Starting prefetch of epoch 152
I0629 18:34:36.528507  8069 data_reader.cpp:262] Starting prefetch of epoch 152
I0629 18:34:36.535112  8067 data_reader.cpp:262] Starting prefetch of epoch 152
I0629 18:34:36.537783  8094 data_reader.cpp:262] Starting prefetch of epoch 152
I0629 18:34:36.541914  8093 data_reader.cpp:262] Starting prefetch of epoch 152
I0629 18:34:53.768208  8090 solver.cpp:349] Iteration 25200 (4.68791 iter/s, 21.3315s/100 iter), loss = 0.229071
I0629 18:34:53.768230  8090 solver.cpp:371]     Train net output #0: loss = 0.229071 (* 1 = 0.229071 loss)
I0629 18:34:53.768234  8090 sgd_solver.cpp:137] Iteration 25200, lr = 1e-05, m = 0.9
I0629 18:35:11.698382  8069 data_reader.cpp:262] Starting prefetch of epoch 153
I0629 18:35:11.699311  8098 data_reader.cpp:262] Starting prefetch of epoch 153
I0629 18:35:11.699592  8096 data_reader.cpp:262] Starting prefetch of epoch 153
I0629 18:35:11.719337  8094 data_reader.cpp:262] Starting prefetch of epoch 153
I0629 18:35:11.724378  8093 data_reader.cpp:262] Starting prefetch of epoch 153
I0629 18:35:11.726629  8067 data_reader.cpp:262] Starting prefetch of epoch 153
I0629 18:35:15.071947  8090 solver.cpp:349] Iteration 25300 (4.69414 iter/s, 21.3032s/100 iter), loss = 0.149316
I0629 18:35:15.071969  8090 solver.cpp:371]     Train net output #0: loss = 0.149317 (* 1 = 0.149317 loss)
I0629 18:35:15.071974  8090 sgd_solver.cpp:137] Iteration 25300, lr = 1e-05, m = 0.9
I0629 18:35:36.565500  8090 solver.cpp:349] Iteration 25400 (4.65269 iter/s, 21.4929s/100 iter), loss = 0.294837
I0629 18:35:36.565526  8090 solver.cpp:371]     Train net output #0: loss = 0.294837 (* 1 = 0.294837 loss)
I0629 18:35:36.565532  8090 sgd_solver.cpp:137] Iteration 25400, lr = 1e-05, m = 0.9
I0629 18:35:47.020750  8069 data_reader.cpp:262] Starting prefetch of epoch 154
I0629 18:35:47.033238  8098 data_reader.cpp:262] Starting prefetch of epoch 154
I0629 18:35:47.034946  8096 data_reader.cpp:262] Starting prefetch of epoch 154
I0629 18:35:47.046444  8093 data_reader.cpp:262] Starting prefetch of epoch 154
I0629 18:35:47.050902  8094 data_reader.cpp:262] Starting prefetch of epoch 154
I0629 18:35:47.052285  8067 data_reader.cpp:262] Starting prefetch of epoch 154
I0629 18:35:57.822628  8090 solver.cpp:349] Iteration 25500 (4.70444 iter/s, 21.2565s/100 iter), loss = 0.150762
I0629 18:35:57.822651  8090 solver.cpp:371]     Train net output #0: loss = 0.150762 (* 1 = 0.150762 loss)
I0629 18:35:57.822655  8090 sgd_solver.cpp:137] Iteration 25500, lr = 1e-05, m = 0.9
I0629 18:36:19.049175  8090 solver.cpp:349] Iteration 25600 (4.71122 iter/s, 21.2259s/100 iter), loss = 0.185567
I0629 18:36:19.049278  8090 solver.cpp:371]     Train net output #0: loss = 0.185567 (* 1 = 0.185567 loss)
I0629 18:36:19.049285  8090 sgd_solver.cpp:137] Iteration 25600, lr = 1e-05, m = 0.9
I0629 18:36:22.300055  8067 data_reader.cpp:262] Starting prefetch of epoch 155
I0629 18:36:22.303462  8098 data_reader.cpp:262] Starting prefetch of epoch 155
I0629 18:36:22.303732  8096 data_reader.cpp:262] Starting prefetch of epoch 155
I0629 18:36:22.304293  8094 data_reader.cpp:262] Starting prefetch of epoch 155
I0629 18:36:22.313555  8069 data_reader.cpp:262] Starting prefetch of epoch 155
I0629 18:36:22.324682  8093 data_reader.cpp:262] Starting prefetch of epoch 155
I0629 18:36:40.433516  8090 solver.cpp:349] Iteration 25700 (4.67647 iter/s, 21.3837s/100 iter), loss = 0.145
I0629 18:36:40.433537  8090 solver.cpp:371]     Train net output #0: loss = 0.145 (* 1 = 0.145 loss)
I0629 18:36:40.433540  8090 sgd_solver.cpp:137] Iteration 25700, lr = 1e-05, m = 0.9
I0629 18:36:57.532058  8096 data_reader.cpp:262] Starting prefetch of epoch 156
I0629 18:36:57.532058  8069 data_reader.cpp:262] Starting prefetch of epoch 156
I0629 18:36:57.540007  8098 data_reader.cpp:262] Starting prefetch of epoch 156
I0629 18:36:57.558571  8093 data_reader.cpp:262] Starting prefetch of epoch 156
I0629 18:36:57.563665  8094 data_reader.cpp:262] Starting prefetch of epoch 156
I0629 18:36:57.563805  8067 data_reader.cpp:262] Starting prefetch of epoch 156
I0629 18:37:01.786336  8090 solver.cpp:349] Iteration 25800 (4.68335 iter/s, 21.3522s/100 iter), loss = 0.176162
I0629 18:37:01.786360  8090 solver.cpp:371]     Train net output #0: loss = 0.176162 (* 1 = 0.176162 loss)
I0629 18:37:01.786363  8090 sgd_solver.cpp:137] Iteration 25800, lr = 1e-05, m = 0.9
I0629 18:37:23.125447  8090 solver.cpp:349] Iteration 25900 (4.68636 iter/s, 21.3385s/100 iter), loss = 0.131313
I0629 18:37:23.125468  8090 solver.cpp:371]     Train net output #0: loss = 0.131313 (* 1 = 0.131313 loss)
I0629 18:37:23.125473  8090 sgd_solver.cpp:137] Iteration 25900, lr = 1e-05, m = 0.9
I0629 18:37:32.861953  8098 data_reader.cpp:262] Starting prefetch of epoch 157
I0629 18:37:32.868968  8096 data_reader.cpp:262] Starting prefetch of epoch 157
I0629 18:37:32.877540  8069 data_reader.cpp:262] Starting prefetch of epoch 157
I0629 18:37:32.893054  8067 data_reader.cpp:262] Starting prefetch of epoch 157
I0629 18:37:32.895781  8094 data_reader.cpp:262] Starting prefetch of epoch 157
I0629 18:37:32.897611  8093 data_reader.cpp:262] Starting prefetch of epoch 157
I0629 18:37:44.336926  8090 solver.cpp:545] Iteration 26000, Testing net (#0)
I0629 18:38:09.729786  8088 data_reader.cpp:262] Starting prefetch of epoch 19
I0629 18:38:09.729786  8119 data_reader.cpp:262] Starting prefetch of epoch 19
I0629 18:38:09.729786  8128 data_reader.cpp:262] Starting prefetch of epoch 19
I0629 18:38:10.165159  8117 data_reader.cpp:262] Starting prefetch of epoch 19
I0629 18:38:10.165158  8086 data_reader.cpp:262] Starting prefetch of epoch 19
I0629 18:38:10.165179  8126 data_reader.cpp:262] Starting prefetch of epoch 19
I0629 18:38:17.917100  8090 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.931504
I0629 18:38:17.917130  8090 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.99462
I0629 18:38:17.917136  8090 solver.cpp:630]     Test net output #2: loss = 0.216084 (* 1 = 0.216084 loss)
I0629 18:38:17.917161  8090 solver.cpp:305] [MultiGPU] Tests completed in 33.5794s
I0629 18:38:18.140311  8090 solver.cpp:349] Iteration 26000 (1.81774 iter/s, 55.0134s/100 iter), loss = 0.212809
I0629 18:38:18.140332  8090 solver.cpp:371]     Train net output #0: loss = 0.212809 (* 1 = 0.212809 loss)
I0629 18:38:18.140336  8090 sgd_solver.cpp:137] Iteration 26000, lr = 1e-05, m = 0.9
I0629 18:38:39.489985  8090 solver.cpp:349] Iteration 26100 (4.68404 iter/s, 21.3491s/100 iter), loss = 0.128938
I0629 18:38:39.490007  8090 solver.cpp:371]     Train net output #0: loss = 0.128939 (* 1 = 0.128939 loss)
I0629 18:38:39.490011  8090 sgd_solver.cpp:137] Iteration 26100, lr = 1e-05, m = 0.9
I0629 18:38:41.616467  8069 data_reader.cpp:262] Starting prefetch of epoch 158
I0629 18:38:41.618261  8096 data_reader.cpp:262] Starting prefetch of epoch 158
I0629 18:38:41.625144  8098 data_reader.cpp:262] Starting prefetch of epoch 158
I0629 18:38:41.640261  8067 data_reader.cpp:262] Starting prefetch of epoch 158
I0629 18:38:41.640260  8094 data_reader.cpp:262] Starting prefetch of epoch 158
I0629 18:38:41.641172  8093 data_reader.cpp:262] Starting prefetch of epoch 158
I0629 18:39:00.741055  8090 solver.cpp:349] Iteration 26200 (4.70577 iter/s, 21.2505s/100 iter), loss = 0.326607
I0629 18:39:00.741077  8090 solver.cpp:371]     Train net output #0: loss = 0.326607 (* 1 = 0.326607 loss)
I0629 18:39:00.741082  8090 sgd_solver.cpp:137] Iteration 26200, lr = 1e-05, m = 0.9
I0629 18:39:17.063194  8096 data_reader.cpp:262] Starting prefetch of epoch 159
I0629 18:39:17.068964  8069 data_reader.cpp:262] Starting prefetch of epoch 159
I0629 18:39:17.067662  8098 data_reader.cpp:262] Starting prefetch of epoch 159
I0629 18:39:17.077337  8093 data_reader.cpp:262] Starting prefetch of epoch 159
I0629 18:39:17.087230  8067 data_reader.cpp:262] Starting prefetch of epoch 159
I0629 18:39:17.092631  8094 data_reader.cpp:262] Starting prefetch of epoch 159
I0629 18:39:22.216879  8090 solver.cpp:349] Iteration 26300 (4.65652 iter/s, 21.4752s/100 iter), loss = 0.23673
I0629 18:39:22.216904  8090 solver.cpp:371]     Train net output #0: loss = 0.23673 (* 1 = 0.23673 loss)
I0629 18:39:22.216910  8090 sgd_solver.cpp:137] Iteration 26300, lr = 1e-05, m = 0.9
I0629 18:39:43.615334  8090 solver.cpp:349] Iteration 26400 (4.67336 iter/s, 21.3979s/100 iter), loss = 0.181276
I0629 18:39:43.615356  8090 solver.cpp:371]     Train net output #0: loss = 0.181277 (* 1 = 0.181277 loss)
I0629 18:39:43.615360  8090 sgd_solver.cpp:137] Iteration 26400, lr = 1e-05, m = 0.9
I0629 18:39:52.318687  8098 data_reader.cpp:262] Starting prefetch of epoch 160
I0629 18:39:52.318687  8069 data_reader.cpp:262] Starting prefetch of epoch 160
I0629 18:39:52.318687  8096 data_reader.cpp:262] Starting prefetch of epoch 160
I0629 18:39:52.341609  8094 data_reader.cpp:262] Starting prefetch of epoch 160
I0629 18:39:52.343415  8093 data_reader.cpp:262] Starting prefetch of epoch 160
I0629 18:39:52.345803  8067 data_reader.cpp:262] Starting prefetch of epoch 160
I0629 18:40:04.828090  8090 solver.cpp:349] Iteration 26500 (4.71427 iter/s, 21.2122s/100 iter), loss = 0.135945
I0629 18:40:04.828114  8090 solver.cpp:371]     Train net output #0: loss = 0.135945 (* 1 = 0.135945 loss)
I0629 18:40:04.828117  8090 sgd_solver.cpp:137] Iteration 26500, lr = 1e-05, m = 0.9
I0629 18:40:26.390282  8090 solver.cpp:349] Iteration 26600 (4.63787 iter/s, 21.5616s/100 iter), loss = 0.199294
I0629 18:40:26.390375  8090 solver.cpp:371]     Train net output #0: loss = 0.199294 (* 1 = 0.199294 loss)
I0629 18:40:26.390383  8090 sgd_solver.cpp:137] Iteration 26600, lr = 1e-05, m = 0.9
I0629 18:40:27.674196  8096 data_reader.cpp:262] Starting prefetch of epoch 161
I0629 18:40:27.674475  8069 data_reader.cpp:262] Starting prefetch of epoch 161
I0629 18:40:27.679556  8094 data_reader.cpp:262] Starting prefetch of epoch 161
I0629 18:40:27.681135  8098 data_reader.cpp:262] Starting prefetch of epoch 161
I0629 18:40:27.698381  8093 data_reader.cpp:262] Starting prefetch of epoch 161
I0629 18:40:27.708678  8067 data_reader.cpp:262] Starting prefetch of epoch 161
I0629 18:40:47.621475  8090 solver.cpp:349] Iteration 26700 (4.71019 iter/s, 21.2306s/100 iter), loss = 0.161662
I0629 18:40:47.621500  8090 solver.cpp:371]     Train net output #0: loss = 0.161662 (* 1 = 0.161662 loss)
I0629 18:40:47.621503  8090 sgd_solver.cpp:137] Iteration 26700, lr = 1e-05, m = 0.9
I0629 18:41:02.742892  8098 data_reader.cpp:262] Starting prefetch of epoch 162
I0629 18:41:02.753006  8069 data_reader.cpp:262] Starting prefetch of epoch 162
I0629 18:41:02.755475  8096 data_reader.cpp:262] Starting prefetch of epoch 162
I0629 18:41:02.766391  8093 data_reader.cpp:262] Starting prefetch of epoch 162
I0629 18:41:02.774559  8094 data_reader.cpp:262] Starting prefetch of epoch 162
I0629 18:41:02.776758  8067 data_reader.cpp:262] Starting prefetch of epoch 162
I0629 18:41:08.919930  8090 solver.cpp:349] Iteration 26800 (4.6953 iter/s, 21.2979s/100 iter), loss = 0.265246
I0629 18:41:08.919952  8090 solver.cpp:371]     Train net output #0: loss = 0.265246 (* 1 = 0.265246 loss)
I0629 18:41:08.919956  8090 sgd_solver.cpp:137] Iteration 26800, lr = 1e-05, m = 0.9
I0629 18:41:30.084939  8090 solver.cpp:349] Iteration 26900 (4.7249 iter/s, 21.1645s/100 iter), loss = 0.134175
I0629 18:41:30.084960  8090 solver.cpp:371]     Train net output #0: loss = 0.134175 (* 1 = 0.134175 loss)
I0629 18:41:30.084965  8090 sgd_solver.cpp:137] Iteration 26900, lr = 1e-05, m = 0.9
I0629 18:41:37.914547  8096 data_reader.cpp:262] Starting prefetch of epoch 163
I0629 18:41:37.914547  8098 data_reader.cpp:262] Starting prefetch of epoch 163
I0629 18:41:37.931844  8069 data_reader.cpp:262] Starting prefetch of epoch 163
I0629 18:41:37.933842  8093 data_reader.cpp:262] Starting prefetch of epoch 163
I0629 18:41:37.939301  8094 data_reader.cpp:262] Starting prefetch of epoch 163
I0629 18:41:37.942513  8067 data_reader.cpp:262] Starting prefetch of epoch 163
I0629 18:41:51.414934  8090 solver.cpp:349] Iteration 27000 (4.68835 iter/s, 21.3294s/100 iter), loss = 0.244082
I0629 18:41:51.414957  8090 solver.cpp:371]     Train net output #0: loss = 0.244082 (* 1 = 0.244082 loss)
I0629 18:41:51.414960  8090 sgd_solver.cpp:137] Iteration 27000, lr = 1e-05, m = 0.9
I0629 18:42:12.614444  8090 solver.cpp:349] Iteration 27100 (4.71721 iter/s, 21.199s/100 iter), loss = 0.280936
I0629 18:42:12.614493  8090 solver.cpp:371]     Train net output #0: loss = 0.280937 (* 1 = 0.280937 loss)
I0629 18:42:12.614498  8090 sgd_solver.cpp:137] Iteration 27100, lr = 1e-05, m = 0.9
I0629 18:42:13.052399  8069 data_reader.cpp:262] Starting prefetch of epoch 164
I0629 18:42:13.060842  8098 data_reader.cpp:262] Starting prefetch of epoch 164
I0629 18:42:13.062427  8096 data_reader.cpp:262] Starting prefetch of epoch 164
I0629 18:42:13.078282  8067 data_reader.cpp:262] Starting prefetch of epoch 164
I0629 18:42:13.081946  8093 data_reader.cpp:262] Starting prefetch of epoch 164
I0629 18:42:13.083660  8094 data_reader.cpp:262] Starting prefetch of epoch 164
I0629 18:42:33.854885  8090 solver.cpp:349] Iteration 27200 (4.70813 iter/s, 21.2399s/100 iter), loss = 0.149976
I0629 18:42:33.854909  8090 solver.cpp:371]     Train net output #0: loss = 0.149976 (* 1 = 0.149976 loss)
I0629 18:42:33.854917  8090 sgd_solver.cpp:137] Iteration 27200, lr = 1e-05, m = 0.9
I0629 18:42:48.234565  8098 data_reader.cpp:262] Starting prefetch of epoch 165
I0629 18:42:48.237545  8096 data_reader.cpp:262] Starting prefetch of epoch 165
I0629 18:42:48.252712  8069 data_reader.cpp:262] Starting prefetch of epoch 165
I0629 18:42:48.256223  8093 data_reader.cpp:262] Starting prefetch of epoch 165
I0629 18:42:48.262346  8067 data_reader.cpp:262] Starting prefetch of epoch 165
I0629 18:42:48.265018  8094 data_reader.cpp:262] Starting prefetch of epoch 165
I0629 18:42:55.307443  8090 solver.cpp:349] Iteration 27300 (4.66157 iter/s, 21.452s/100 iter), loss = 0.162482
I0629 18:42:55.307471  8090 solver.cpp:371]     Train net output #0: loss = 0.162482 (* 1 = 0.162482 loss)
I0629 18:42:55.307478  8090 sgd_solver.cpp:137] Iteration 27300, lr = 1e-05, m = 0.9
I0629 18:43:16.555647  8090 solver.cpp:349] Iteration 27400 (4.7064 iter/s, 21.2477s/100 iter), loss = 0.225936
I0629 18:43:16.555675  8090 solver.cpp:371]     Train net output #0: loss = 0.225936 (* 1 = 0.225936 loss)
I0629 18:43:16.555680  8090 sgd_solver.cpp:137] Iteration 27400, lr = 1e-05, m = 0.9
I0629 18:43:23.591209  8069 data_reader.cpp:262] Starting prefetch of epoch 166
I0629 18:43:23.593868  8096 data_reader.cpp:262] Starting prefetch of epoch 166
I0629 18:43:23.593868  8098 data_reader.cpp:262] Starting prefetch of epoch 166
I0629 18:43:23.605145  8094 data_reader.cpp:262] Starting prefetch of epoch 166
I0629 18:43:23.605145  8067 data_reader.cpp:262] Starting prefetch of epoch 166
I0629 18:43:23.609334  8093 data_reader.cpp:262] Starting prefetch of epoch 166
I0629 18:43:37.777343  8090 solver.cpp:349] Iteration 27500 (4.71228 iter/s, 21.2212s/100 iter), loss = 0.182749
I0629 18:43:37.777366  8090 solver.cpp:371]     Train net output #0: loss = 0.182749 (* 1 = 0.182749 loss)
I0629 18:43:37.777370  8090 sgd_solver.cpp:137] Iteration 27500, lr = 1e-05, m = 0.9
I0629 18:43:58.614768  8096 data_reader.cpp:262] Starting prefetch of epoch 167
I0629 18:43:58.617491  8098 data_reader.cpp:262] Starting prefetch of epoch 167
I0629 18:43:58.622531  8069 data_reader.cpp:262] Starting prefetch of epoch 167
I0629 18:43:58.626329  8093 data_reader.cpp:262] Starting prefetch of epoch 167
I0629 18:43:58.637794  8067 data_reader.cpp:262] Starting prefetch of epoch 167
I0629 18:43:58.638680  8094 data_reader.cpp:262] Starting prefetch of epoch 167
I0629 18:43:59.015738  8090 solver.cpp:349] Iteration 27600 (4.70857 iter/s, 21.2379s/100 iter), loss = 0.247407
I0629 18:43:59.015761  8090 solver.cpp:371]     Train net output #0: loss = 0.247407 (* 1 = 0.247407 loss)
I0629 18:43:59.015768  8090 sgd_solver.cpp:137] Iteration 27600, lr = 1e-05, m = 0.9
I0629 18:44:20.243389  8090 solver.cpp:349] Iteration 27700 (4.71096 iter/s, 21.2271s/100 iter), loss = 0.181099
I0629 18:44:20.243412  8090 solver.cpp:371]     Train net output #0: loss = 0.181099 (* 1 = 0.181099 loss)
I0629 18:44:20.243417  8090 sgd_solver.cpp:137] Iteration 27700, lr = 1e-05, m = 0.9
I0629 18:44:33.571092  8069 data_reader.cpp:262] Starting prefetch of epoch 168
I0629 18:44:33.571092  8098 data_reader.cpp:262] Starting prefetch of epoch 168
I0629 18:44:33.578259  8096 data_reader.cpp:262] Starting prefetch of epoch 168
I0629 18:44:33.597815  8093 data_reader.cpp:262] Starting prefetch of epoch 168
I0629 18:44:33.602403  8094 data_reader.cpp:262] Starting prefetch of epoch 168
I0629 18:44:33.606077  8067 data_reader.cpp:262] Starting prefetch of epoch 168
I0629 18:44:41.476725  8090 solver.cpp:349] Iteration 27800 (4.70969 iter/s, 21.2328s/100 iter), loss = 0.155251
I0629 18:44:41.476748  8090 solver.cpp:371]     Train net output #0: loss = 0.155251 (* 1 = 0.155251 loss)
I0629 18:44:41.476752  8090 sgd_solver.cpp:137] Iteration 27800, lr = 1e-05, m = 0.9
I0629 18:45:02.654531  8090 solver.cpp:349] Iteration 27900 (4.72204 iter/s, 21.1773s/100 iter), loss = 0.179575
I0629 18:45:02.654554  8090 solver.cpp:371]     Train net output #0: loss = 0.179576 (* 1 = 0.179576 loss)
I0629 18:45:02.654558  8090 sgd_solver.cpp:137] Iteration 27900, lr = 1e-05, m = 0.9
I0629 18:45:08.668946  8096 data_reader.cpp:262] Starting prefetch of epoch 169
I0629 18:45:08.676208  8098 data_reader.cpp:262] Starting prefetch of epoch 169
I0629 18:45:08.679921  8069 data_reader.cpp:262] Starting prefetch of epoch 169
I0629 18:45:08.695305  8094 data_reader.cpp:262] Starting prefetch of epoch 169
I0629 18:45:08.698241  8067 data_reader.cpp:262] Starting prefetch of epoch 169
I0629 18:45:08.699445  8093 data_reader.cpp:262] Starting prefetch of epoch 169
I0629 18:45:23.771399  8090 solver.cpp:545] Iteration 28000, Testing net (#0)
I0629 18:45:33.420620  8088 data_reader.cpp:262] Starting prefetch of epoch 20
I0629 18:45:33.420642  8119 data_reader.cpp:262] Starting prefetch of epoch 20
I0629 18:45:33.420620  8128 data_reader.cpp:262] Starting prefetch of epoch 20
I0629 18:45:33.529006  8086 data_reader.cpp:262] Starting prefetch of epoch 20
I0629 18:45:33.529029  8126 data_reader.cpp:262] Starting prefetch of epoch 20
I0629 18:45:33.529006  8117 data_reader.cpp:262] Starting prefetch of epoch 20
I0629 18:45:50.768801  8088 data_reader.cpp:262] Starting prefetch of epoch 21
I0629 18:45:50.773787  8086 data_reader.cpp:262] Starting prefetch of epoch 21
I0629 18:45:51.037904  8119 data_reader.cpp:262] Starting prefetch of epoch 21
I0629 18:45:51.041800  8117 data_reader.cpp:262] Starting prefetch of epoch 21
I0629 18:45:51.092339  8128 data_reader.cpp:262] Starting prefetch of epoch 21
I0629 18:45:51.097304  8126 data_reader.cpp:262] Starting prefetch of epoch 21
I0629 18:45:51.722120  8090 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.927925
I0629 18:45:51.722146  8090 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.99508
I0629 18:45:51.722152  8090 solver.cpp:630]     Test net output #2: loss = 0.218508 (* 1 = 0.218508 loss)
I0629 18:45:51.722235  8090 solver.cpp:305] [MultiGPU] Tests completed in 27.9502s
I0629 18:45:51.939646  8090 solver.cpp:349] Iteration 28000 (2.02906 iter/s, 49.284s/100 iter), loss = 0.1407
I0629 18:45:51.939667  8090 solver.cpp:371]     Train net output #0: loss = 0.1407 (* 1 = 0.1407 loss)
I0629 18:45:51.939671  8090 sgd_solver.cpp:137] Iteration 28000, lr = 1e-05, m = 0.9
I0629 18:46:11.857585  8096 data_reader.cpp:262] Starting prefetch of epoch 170
I0629 18:46:11.857585  8098 data_reader.cpp:262] Starting prefetch of epoch 170
I0629 18:46:11.857585  8069 data_reader.cpp:262] Starting prefetch of epoch 170
I0629 18:46:11.875332  8093 data_reader.cpp:262] Starting prefetch of epoch 170
I0629 18:46:11.877447  8067 data_reader.cpp:262] Starting prefetch of epoch 170
I0629 18:46:11.882967  8094 data_reader.cpp:262] Starting prefetch of epoch 170
I0629 18:46:13.110121  8090 solver.cpp:349] Iteration 28100 (4.72368 iter/s, 21.17s/100 iter), loss = 0.160328
I0629 18:46:13.110143  8090 solver.cpp:371]     Train net output #0: loss = 0.160328 (* 1 = 0.160328 loss)
I0629 18:46:13.110147  8090 sgd_solver.cpp:137] Iteration 28100, lr = 1e-05, m = 0.9
I0629 18:46:34.564360  8090 solver.cpp:349] Iteration 28200 (4.6612 iter/s, 21.4537s/100 iter), loss = 0.167938
I0629 18:46:34.564414  8090 solver.cpp:371]     Train net output #0: loss = 0.167938 (* 1 = 0.167938 loss)
I0629 18:46:34.564419  8090 sgd_solver.cpp:137] Iteration 28200, lr = 1e-05, m = 0.9
I0629 18:46:47.185840  8098 data_reader.cpp:262] Starting prefetch of epoch 171
I0629 18:46:47.200258  8069 data_reader.cpp:262] Starting prefetch of epoch 171
I0629 18:46:47.202962  8096 data_reader.cpp:262] Starting prefetch of epoch 171
I0629 18:46:47.205245  8093 data_reader.cpp:262] Starting prefetch of epoch 171
I0629 18:46:47.211154  8094 data_reader.cpp:262] Starting prefetch of epoch 171
I0629 18:46:47.212427  8067 data_reader.cpp:262] Starting prefetch of epoch 171
I0629 18:46:55.950844  8090 solver.cpp:349] Iteration 28300 (4.67597 iter/s, 21.3859s/100 iter), loss = 0.163384
I0629 18:46:55.950866  8090 solver.cpp:371]     Train net output #0: loss = 0.163384 (* 1 = 0.163384 loss)
I0629 18:46:55.950870  8090 sgd_solver.cpp:137] Iteration 28300, lr = 1e-05, m = 0.9
I0629 18:47:17.201350  8090 solver.cpp:349] Iteration 28400 (4.70588 iter/s, 21.25s/100 iter), loss = 0.213522
I0629 18:47:17.201480  8090 solver.cpp:371]     Train net output #0: loss = 0.213522 (* 1 = 0.213522 loss)
I0629 18:47:17.201488  8090 sgd_solver.cpp:137] Iteration 28400, lr = 1e-05, m = 0.9
I0629 18:47:22.339360  8096 data_reader.cpp:262] Starting prefetch of epoch 172
I0629 18:47:22.339360  8069 data_reader.cpp:262] Starting prefetch of epoch 172
I0629 18:47:22.349197  8098 data_reader.cpp:262] Starting prefetch of epoch 172
I0629 18:47:22.361155  8094 data_reader.cpp:262] Starting prefetch of epoch 172
I0629 18:47:22.369819  8093 data_reader.cpp:262] Starting prefetch of epoch 172
I0629 18:47:22.373919  8067 data_reader.cpp:262] Starting prefetch of epoch 172
I0629 18:47:38.489300  8090 solver.cpp:349] Iteration 28500 (4.69763 iter/s, 21.2873s/100 iter), loss = 0.240835
I0629 18:47:38.489325  8090 solver.cpp:371]     Train net output #0: loss = 0.240835 (* 1 = 0.240835 loss)
I0629 18:47:38.489328  8090 sgd_solver.cpp:137] Iteration 28500, lr = 1e-05, m = 0.9
I0629 18:47:57.568759  8098 data_reader.cpp:262] Starting prefetch of epoch 173
I0629 18:47:57.573415  8096 data_reader.cpp:262] Starting prefetch of epoch 173
I0629 18:47:57.582675  8069 data_reader.cpp:262] Starting prefetch of epoch 173
I0629 18:47:57.582854  8093 data_reader.cpp:262] Starting prefetch of epoch 173
I0629 18:47:57.585196  8067 data_reader.cpp:262] Starting prefetch of epoch 173
I0629 18:47:57.599541  8094 data_reader.cpp:262] Starting prefetch of epoch 173
I0629 18:47:59.680336  8090 solver.cpp:349] Iteration 28600 (4.71909 iter/s, 21.1905s/100 iter), loss = 0.344427
I0629 18:47:59.680361  8090 solver.cpp:371]     Train net output #0: loss = 0.344427 (* 1 = 0.344427 loss)
I0629 18:47:59.680364  8090 sgd_solver.cpp:137] Iteration 28600, lr = 1e-05, m = 0.9
I0629 18:48:20.940011  8090 solver.cpp:349] Iteration 28700 (4.70385 iter/s, 21.2592s/100 iter), loss = 0.183065
I0629 18:48:20.940034  8090 solver.cpp:371]     Train net output #0: loss = 0.183065 (* 1 = 0.183065 loss)
I0629 18:48:20.940038  8090 sgd_solver.cpp:137] Iteration 28700, lr = 1e-05, m = 0.9
I0629 18:48:32.613759  8096 data_reader.cpp:262] Starting prefetch of epoch 174
I0629 18:48:32.616559  8069 data_reader.cpp:262] Starting prefetch of epoch 174
I0629 18:48:32.617648  8098 data_reader.cpp:262] Starting prefetch of epoch 174
I0629 18:48:32.637007  8094 data_reader.cpp:262] Starting prefetch of epoch 174
I0629 18:48:32.640908  8067 data_reader.cpp:262] Starting prefetch of epoch 174
I0629 18:48:32.641472  8093 data_reader.cpp:262] Starting prefetch of epoch 174
I0629 18:48:42.180516  8090 solver.cpp:349] Iteration 28800 (4.7081 iter/s, 21.24s/100 iter), loss = 0.185058
I0629 18:48:42.180537  8090 solver.cpp:371]     Train net output #0: loss = 0.185058 (* 1 = 0.185058 loss)
I0629 18:48:42.180541  8090 sgd_solver.cpp:137] Iteration 28800, lr = 1e-05, m = 0.9
I0629 18:49:03.443567  8090 solver.cpp:349] Iteration 28900 (4.7031 iter/s, 21.2626s/100 iter), loss = 0.242793
I0629 18:49:03.443614  8090 solver.cpp:371]     Train net output #0: loss = 0.242793 (* 1 = 0.242793 loss)
I0629 18:49:03.443619  8090 sgd_solver.cpp:137] Iteration 28900, lr = 1e-05, m = 0.9
I0629 18:49:07.674644  8096 data_reader.cpp:262] Starting prefetch of epoch 175
I0629 18:49:07.681445  8069 data_reader.cpp:262] Starting prefetch of epoch 175
I0629 18:49:07.682570  8098 data_reader.cpp:262] Starting prefetch of epoch 175
I0629 18:49:07.702291  8094 data_reader.cpp:262] Starting prefetch of epoch 175
I0629 18:49:07.703584  8093 data_reader.cpp:262] Starting prefetch of epoch 175
I0629 18:49:07.705533  8067 data_reader.cpp:262] Starting prefetch of epoch 175
I0629 18:49:24.684298  8090 solver.cpp:349] Iteration 29000 (4.70805 iter/s, 21.2402s/100 iter), loss = 0.267264
I0629 18:49:24.684320  8090 solver.cpp:371]     Train net output #0: loss = 0.267264 (* 1 = 0.267264 loss)
I0629 18:49:24.684324  8090 sgd_solver.cpp:137] Iteration 29000, lr = 1e-05, m = 0.9
I0629 18:49:42.786779  8098 data_reader.cpp:262] Starting prefetch of epoch 176
I0629 18:49:42.786784  8069 data_reader.cpp:262] Starting prefetch of epoch 176
I0629 18:49:42.786784  8096 data_reader.cpp:262] Starting prefetch of epoch 176
I0629 18:49:42.807310  8094 data_reader.cpp:262] Starting prefetch of epoch 176
I0629 18:49:42.807310  8093 data_reader.cpp:262] Starting prefetch of epoch 176
I0629 18:49:42.817306  8067 data_reader.cpp:262] Starting prefetch of epoch 176
I0629 18:49:46.011593  8090 solver.cpp:349] Iteration 29100 (4.68894 iter/s, 21.3268s/100 iter), loss = 0.176312
I0629 18:49:46.011616  8090 solver.cpp:371]     Train net output #0: loss = 0.176312 (* 1 = 0.176312 loss)
I0629 18:49:46.011621  8090 sgd_solver.cpp:137] Iteration 29100, lr = 1e-05, m = 0.9
I0629 18:50:07.476447  8090 solver.cpp:349] Iteration 29200 (4.65889 iter/s, 21.4644s/100 iter), loss = 0.156339
I0629 18:50:07.476475  8090 solver.cpp:371]     Train net output #0: loss = 0.156339 (* 1 = 0.156339 loss)
I0629 18:50:07.476481  8090 sgd_solver.cpp:137] Iteration 29200, lr = 1e-05, m = 0.9
I0629 18:50:18.379711  8096 data_reader.cpp:262] Starting prefetch of epoch 177
I0629 18:50:18.380157  8069 data_reader.cpp:262] Starting prefetch of epoch 177
I0629 18:50:18.385298  8067 data_reader.cpp:262] Starting prefetch of epoch 177
I0629 18:50:18.398378  8098 data_reader.cpp:262] Starting prefetch of epoch 177
I0629 18:50:18.401376  8093 data_reader.cpp:262] Starting prefetch of epoch 177
I0629 18:50:18.403877  8094 data_reader.cpp:262] Starting prefetch of epoch 177
I0629 18:50:28.904886  8090 solver.cpp:349] Iteration 29300 (4.66681 iter/s, 21.4279s/100 iter), loss = 0.113294
I0629 18:50:28.904909  8090 solver.cpp:371]     Train net output #0: loss = 0.113295 (* 1 = 0.113295 loss)
I0629 18:50:28.904913  8090 sgd_solver.cpp:137] Iteration 29300, lr = 1e-05, m = 0.9
I0629 18:50:50.260936  8090 solver.cpp:349] Iteration 29400 (4.68262 iter/s, 21.3556s/100 iter), loss = 0.211707
I0629 18:50:50.261018  8090 solver.cpp:371]     Train net output #0: loss = 0.211707 (* 1 = 0.211707 loss)
I0629 18:50:50.261024  8090 sgd_solver.cpp:137] Iteration 29400, lr = 1e-05, m = 0.9
I0629 18:50:53.687860  8069 data_reader.cpp:262] Starting prefetch of epoch 178
I0629 18:50:53.689640  8096 data_reader.cpp:262] Starting prefetch of epoch 178
I0629 18:50:53.696538  8098 data_reader.cpp:262] Starting prefetch of epoch 178
I0629 18:50:53.709790  8094 data_reader.cpp:262] Starting prefetch of epoch 178
I0629 18:50:53.711457  8067 data_reader.cpp:262] Starting prefetch of epoch 178
I0629 18:50:53.715255  8093 data_reader.cpp:262] Starting prefetch of epoch 178
I0629 18:51:11.674726  8090 solver.cpp:349] Iteration 29500 (4.67001 iter/s, 21.4132s/100 iter), loss = 0.139945
I0629 18:51:11.674756  8090 solver.cpp:371]     Train net output #0: loss = 0.139945 (* 1 = 0.139945 loss)
I0629 18:51:11.674762  8090 sgd_solver.cpp:137] Iteration 29500, lr = 1e-05, m = 0.9
I0629 18:51:29.034173  8096 data_reader.cpp:262] Starting prefetch of epoch 179
I0629 18:51:29.037786  8069 data_reader.cpp:262] Starting prefetch of epoch 179
I0629 18:51:29.041771  8098 data_reader.cpp:262] Starting prefetch of epoch 179
I0629 18:51:29.054581  8094 data_reader.cpp:262] Starting prefetch of epoch 179
I0629 18:51:29.058132  8067 data_reader.cpp:262] Starting prefetch of epoch 179
I0629 18:51:29.064200  8093 data_reader.cpp:262] Starting prefetch of epoch 179
I0629 18:51:33.061451  8090 solver.cpp:349] Iteration 29600 (4.67591 iter/s, 21.3862s/100 iter), loss = 0.173125
I0629 18:51:33.061472  8090 solver.cpp:371]     Train net output #0: loss = 0.173125 (* 1 = 0.173125 loss)
I0629 18:51:33.061477  8090 sgd_solver.cpp:137] Iteration 29600, lr = 1e-05, m = 0.9
I0629 18:51:54.391485  8090 solver.cpp:349] Iteration 29700 (4.68833 iter/s, 21.3296s/100 iter), loss = 0.16082
I0629 18:51:54.391508  8090 solver.cpp:371]     Train net output #0: loss = 0.16082 (* 1 = 0.16082 loss)
I0629 18:51:54.391512  8090 sgd_solver.cpp:137] Iteration 29700, lr = 1e-05, m = 0.9
I0629 18:52:04.165457  8069 data_reader.cpp:262] Starting prefetch of epoch 180
I0629 18:52:04.171856  8096 data_reader.cpp:262] Starting prefetch of epoch 180
I0629 18:52:04.184228  8098 data_reader.cpp:262] Starting prefetch of epoch 180
I0629 18:52:04.195893  8094 data_reader.cpp:262] Starting prefetch of epoch 180
I0629 18:52:04.196873  8093 data_reader.cpp:262] Starting prefetch of epoch 180
I0629 18:52:04.200182  8067 data_reader.cpp:262] Starting prefetch of epoch 180
I0629 18:52:15.761106  8090 solver.cpp:349] Iteration 29800 (4.67965 iter/s, 21.3691s/100 iter), loss = 0.325363
I0629 18:52:15.761129  8090 solver.cpp:371]     Train net output #0: loss = 0.325363 (* 1 = 0.325363 loss)
I0629 18:52:15.761133  8090 sgd_solver.cpp:137] Iteration 29800, lr = 1e-05, m = 0.9
I0629 18:52:37.019775  8090 solver.cpp:349] Iteration 29900 (4.70407 iter/s, 21.2582s/100 iter), loss = 0.189401
I0629 18:52:37.019829  8090 solver.cpp:371]     Train net output #0: loss = 0.189401 (* 1 = 0.189401 loss)
I0629 18:52:37.019834  8090 sgd_solver.cpp:137] Iteration 29900, lr = 1e-05, m = 0.9
I0629 18:52:39.580471  8094 data_reader.cpp:262] Starting prefetch of epoch 181
I0629 18:52:39.586099  8096 data_reader.cpp:262] Starting prefetch of epoch 181
I0629 18:52:39.586099  8098 data_reader.cpp:262] Starting prefetch of epoch 181
I0629 18:52:39.589186  8067 data_reader.cpp:262] Starting prefetch of epoch 181
I0629 18:52:39.592267  8069 data_reader.cpp:262] Starting prefetch of epoch 181
I0629 18:52:39.600033  8093 data_reader.cpp:262] Starting prefetch of epoch 181
I0629 18:52:58.245430  8090 solver.cpp:675] Snapshotting to binary proto file training/cityscapes20_jsegnet21v2_2017-06-29_16-47-23/initial/cityscapes20_jsegnet21v2_iter_30000.caffemodel
I0629 18:52:58.376278  8090 sgd_solver.cpp:288] Snapshotting solver state to binary proto file training/cityscapes20_jsegnet21v2_2017-06-29_16-47-23/initial/cityscapes20_jsegnet21v2_iter_30000.solverstate
I0629 18:52:58.384346  8090 solver.cpp:545] Iteration 30000, Testing net (#0)
I0629 18:53:10.417876  8088 data_reader.cpp:262] Starting prefetch of epoch 22
I0629 18:53:10.423136  8086 data_reader.cpp:262] Starting prefetch of epoch 22
I0629 18:53:10.962219  8119 data_reader.cpp:262] Starting prefetch of epoch 22
I0629 18:53:10.966645  8117 data_reader.cpp:262] Starting prefetch of epoch 22
I0629 18:53:11.026413  8128 data_reader.cpp:262] Starting prefetch of epoch 22
I0629 18:53:11.033700  8126 data_reader.cpp:262] Starting prefetch of epoch 22
I0629 18:53:18.177794  8090 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.93236
I0629 18:53:18.177819  8090 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.994753
I0629 18:53:18.177824  8090 solver.cpp:630]     Test net output #2: loss = 0.213155 (* 1 = 0.213155 loss)
I0629 18:53:18.177927  8090 solver.cpp:305] [MultiGPU] Tests completed in 19.7932s
I0629 18:53:18.396337  8090 solver.cpp:349] Iteration 30000 (2.41688 iter/s, 41.3756s/100 iter), loss = 0.162387
I0629 18:53:18.396363  8090 solver.cpp:371]     Train net output #0: loss = 0.162387 (* 1 = 0.162387 loss)
I0629 18:53:18.396368  8090 sgd_solver.cpp:137] Iteration 30000, lr = 1e-05, m = 0.9
I0629 18:53:34.835904  8098 data_reader.cpp:262] Starting prefetch of epoch 182
I0629 18:53:34.840199  8069 data_reader.cpp:262] Starting prefetch of epoch 182
I0629 18:53:34.853492  8096 data_reader.cpp:262] Starting prefetch of epoch 182
I0629 18:53:34.860257  8093 data_reader.cpp:262] Starting prefetch of epoch 182
I0629 18:53:34.865304  8067 data_reader.cpp:262] Starting prefetch of epoch 182
I0629 18:53:34.867699  8094 data_reader.cpp:262] Starting prefetch of epoch 182
I0629 18:53:39.720759  8090 solver.cpp:349] Iteration 30100 (4.68956 iter/s, 21.3239s/100 iter), loss = 0.206174
I0629 18:53:39.720779  8090 solver.cpp:371]     Train net output #0: loss = 0.206174 (* 1 = 0.206174 loss)
I0629 18:53:39.720783  8090 sgd_solver.cpp:137] Iteration 30100, lr = 1e-05, m = 0.9
I0629 18:54:00.936297  8090 solver.cpp:349] Iteration 30200 (4.71363 iter/s, 21.2151s/100 iter), loss = 0.273191
I0629 18:54:00.936368  8090 solver.cpp:371]     Train net output #0: loss = 0.273191 (* 1 = 0.273191 loss)
I0629 18:54:00.936373  8090 sgd_solver.cpp:137] Iteration 30200, lr = 1e-05, m = 0.9
I0629 18:54:10.013360  8096 data_reader.cpp:262] Starting prefetch of epoch 183
I0629 18:54:10.027014  8069 data_reader.cpp:262] Starting prefetch of epoch 183
I0629 18:54:10.035567  8098 data_reader.cpp:262] Starting prefetch of epoch 183
I0629 18:54:10.037531  8094 data_reader.cpp:262] Starting prefetch of epoch 183
I0629 18:54:10.039504  8093 data_reader.cpp:262] Starting prefetch of epoch 183
I0629 18:54:10.043838  8067 data_reader.cpp:262] Starting prefetch of epoch 183
I0629 18:54:22.313992  8090 solver.cpp:349] Iteration 30300 (4.67789 iter/s, 21.3772s/100 iter), loss = 0.183821
I0629 18:54:22.314016  8090 solver.cpp:371]     Train net output #0: loss = 0.183821 (* 1 = 0.183821 loss)
I0629 18:54:22.314020  8090 sgd_solver.cpp:137] Iteration 30300, lr = 1e-05, m = 0.9
I0629 18:54:43.815536  8090 solver.cpp:349] Iteration 30400 (4.65093 iter/s, 21.5011s/100 iter), loss = 0.200501
I0629 18:54:43.815587  8090 solver.cpp:371]     Train net output #0: loss = 0.200502 (* 1 = 0.200502 loss)
I0629 18:54:43.815593  8090 sgd_solver.cpp:137] Iteration 30400, lr = 1e-05, m = 0.9
I0629 18:54:45.517276  8069 data_reader.cpp:262] Starting prefetch of epoch 184
I0629 18:54:45.518087  8096 data_reader.cpp:262] Starting prefetch of epoch 184
I0629 18:54:45.522517  8098 data_reader.cpp:262] Starting prefetch of epoch 184
I0629 18:54:45.529755  8067 data_reader.cpp:262] Starting prefetch of epoch 184
I0629 18:54:45.535356  8093 data_reader.cpp:262] Starting prefetch of epoch 184
I0629 18:54:45.542461  8094 data_reader.cpp:262] Starting prefetch of epoch 184
I0629 18:55:05.110402  8090 solver.cpp:349] Iteration 30500 (4.69608 iter/s, 21.2944s/100 iter), loss = 0.239857
I0629 18:55:05.110425  8090 solver.cpp:371]     Train net output #0: loss = 0.239857 (* 1 = 0.239857 loss)
I0629 18:55:05.110431  8090 sgd_solver.cpp:137] Iteration 30500, lr = 1e-05, m = 0.9
I0629 18:55:20.581531  8069 data_reader.cpp:262] Starting prefetch of epoch 185
I0629 18:55:20.586387  8098 data_reader.cpp:262] Starting prefetch of epoch 185
I0629 18:55:20.599190  8096 data_reader.cpp:262] Starting prefetch of epoch 185
I0629 18:55:20.607007  8067 data_reader.cpp:262] Starting prefetch of epoch 185
I0629 18:55:20.614244  8094 data_reader.cpp:262] Starting prefetch of epoch 185
I0629 18:55:20.616792  8093 data_reader.cpp:262] Starting prefetch of epoch 185
I0629 18:55:26.373291  8090 solver.cpp:349] Iteration 30600 (4.70313 iter/s, 21.2624s/100 iter), loss = 0.128062
I0629 18:55:26.373313  8090 solver.cpp:371]     Train net output #0: loss = 0.128063 (* 1 = 0.128063 loss)
I0629 18:55:26.373317  8090 sgd_solver.cpp:137] Iteration 30600, lr = 1e-05, m = 0.9
I0629 18:55:47.706517  8090 solver.cpp:349] Iteration 30700 (4.68762 iter/s, 21.3328s/100 iter), loss = 0.152167
I0629 18:55:47.706588  8090 solver.cpp:371]     Train net output #0: loss = 0.152167 (* 1 = 0.152167 loss)
I0629 18:55:47.706607  8090 sgd_solver.cpp:137] Iteration 30700, lr = 1e-05, m = 0.9
I0629 18:55:55.909628  8098 data_reader.cpp:262] Starting prefetch of epoch 186
I0629 18:55:55.911988  8096 data_reader.cpp:262] Starting prefetch of epoch 186
I0629 18:55:55.912912  8093 data_reader.cpp:262] Starting prefetch of epoch 186
I0629 18:55:55.915923  8069 data_reader.cpp:262] Starting prefetch of epoch 186
I0629 18:55:55.928850  8067 data_reader.cpp:262] Starting prefetch of epoch 186
I0629 18:55:55.929828  8094 data_reader.cpp:262] Starting prefetch of epoch 186
I0629 18:56:09.142769  8090 solver.cpp:349] Iteration 30800 (4.66511 iter/s, 21.4357s/100 iter), loss = 0.122691
I0629 18:56:09.142791  8090 solver.cpp:371]     Train net output #0: loss = 0.122691 (* 1 = 0.122691 loss)
I0629 18:56:09.142796  8090 sgd_solver.cpp:137] Iteration 30800, lr = 1e-05, m = 0.9
I0629 18:56:30.471262  8090 solver.cpp:349] Iteration 30900 (4.68866 iter/s, 21.328s/100 iter), loss = 0.484038
I0629 18:56:30.471333  8090 solver.cpp:371]     Train net output #0: loss = 0.484038 (* 1 = 0.484038 loss)
I0629 18:56:30.471338  8090 sgd_solver.cpp:137] Iteration 30900, lr = 1e-05, m = 0.9
I0629 18:56:31.117369  8096 data_reader.cpp:262] Starting prefetch of epoch 187
I0629 18:56:31.118657  8069 data_reader.cpp:262] Starting prefetch of epoch 187
I0629 18:56:31.118934  8098 data_reader.cpp:262] Starting prefetch of epoch 187
I0629 18:56:31.140847  8067 data_reader.cpp:262] Starting prefetch of epoch 187
I0629 18:56:31.142935  8093 data_reader.cpp:262] Starting prefetch of epoch 187
I0629 18:56:31.144656  8094 data_reader.cpp:262] Starting prefetch of epoch 187
I0629 18:56:51.709173  8090 solver.cpp:349] Iteration 31000 (4.70867 iter/s, 21.2374s/100 iter), loss = 0.168163
I0629 18:56:51.709197  8090 solver.cpp:371]     Train net output #0: loss = 0.168163 (* 1 = 0.168163 loss)
I0629 18:56:51.709202  8090 sgd_solver.cpp:137] Iteration 31000, lr = 1e-05, m = 0.9
I0629 18:57:06.445133  8096 data_reader.cpp:262] Starting prefetch of epoch 188
I0629 18:57:06.446959  8098 data_reader.cpp:262] Starting prefetch of epoch 188
I0629 18:57:06.447649  8069 data_reader.cpp:262] Starting prefetch of epoch 188
I0629 18:57:06.464596  8067 data_reader.cpp:262] Starting prefetch of epoch 188
I0629 18:57:06.465507  8093 data_reader.cpp:262] Starting prefetch of epoch 188
I0629 18:57:06.471282  8094 data_reader.cpp:262] Starting prefetch of epoch 188
I0629 18:57:13.064553  8090 solver.cpp:349] Iteration 31100 (4.68276 iter/s, 21.3549s/100 iter), loss = 0.135811
I0629 18:57:13.064576  8090 solver.cpp:371]     Train net output #0: loss = 0.135812 (* 1 = 0.135812 loss)
I0629 18:57:13.064580  8090 sgd_solver.cpp:137] Iteration 31100, lr = 1e-05, m = 0.9
I0629 18:57:34.282690  8090 solver.cpp:349] Iteration 31200 (4.71305 iter/s, 21.2177s/100 iter), loss = 0.286691
I0629 18:57:34.282713  8090 solver.cpp:371]     Train net output #0: loss = 0.286692 (* 1 = 0.286692 loss)
I0629 18:57:34.282717  8090 sgd_solver.cpp:137] Iteration 31200, lr = 1e-05, m = 0.9
I0629 18:57:41.514843  8098 data_reader.cpp:262] Starting prefetch of epoch 189
I0629 18:57:41.515856  8096 data_reader.cpp:262] Starting prefetch of epoch 189
I0629 18:57:41.518214  8069 data_reader.cpp:262] Starting prefetch of epoch 189
I0629 18:57:41.532330  8067 data_reader.cpp:262] Starting prefetch of epoch 189
I0629 18:57:41.537343  8093 data_reader.cpp:262] Starting prefetch of epoch 189
I0629 18:57:41.543285  8094 data_reader.cpp:262] Starting prefetch of epoch 189
I0629 18:57:55.468466  8090 solver.cpp:349] Iteration 31300 (4.72025 iter/s, 21.1853s/100 iter), loss = 0.196883
I0629 18:57:55.468490  8090 solver.cpp:371]     Train net output #0: loss = 0.196883 (* 1 = 0.196883 loss)
I0629 18:57:55.468494  8090 sgd_solver.cpp:137] Iteration 31300, lr = 1e-05, m = 0.9
I0629 18:58:16.598907  8096 data_reader.cpp:262] Starting prefetch of epoch 190
I0629 18:58:16.604740  8069 data_reader.cpp:262] Starting prefetch of epoch 190
I0629 18:58:16.613872  8098 data_reader.cpp:262] Starting prefetch of epoch 190
I0629 18:58:16.623930  8093 data_reader.cpp:262] Starting prefetch of epoch 190
I0629 18:58:16.627720  8067 data_reader.cpp:262] Starting prefetch of epoch 190
I0629 18:58:16.633507  8094 data_reader.cpp:262] Starting prefetch of epoch 190
I0629 18:58:16.800622  8090 solver.cpp:349] Iteration 31400 (4.68786 iter/s, 21.3317s/100 iter), loss = 0.146572
I0629 18:58:16.800645  8090 solver.cpp:371]     Train net output #0: loss = 0.146572 (* 1 = 0.146572 loss)
I0629 18:58:16.800649  8090 sgd_solver.cpp:137] Iteration 31400, lr = 1e-05, m = 0.9
I0629 18:58:38.105414  8090 solver.cpp:349] Iteration 31500 (4.69388 iter/s, 21.3043s/100 iter), loss = 0.146949
I0629 18:58:38.105440  8090 solver.cpp:371]     Train net output #0: loss = 0.146949 (* 1 = 0.146949 loss)
I0629 18:58:38.105448  8090 sgd_solver.cpp:137] Iteration 31500, lr = 1e-05, m = 0.9
I0629 18:58:52.060783  8098 data_reader.cpp:262] Starting prefetch of epoch 191
I0629 18:58:52.064100  8096 data_reader.cpp:262] Starting prefetch of epoch 191
I0629 18:58:52.067476  8069 data_reader.cpp:262] Starting prefetch of epoch 191
I0629 18:58:52.071980  8093 data_reader.cpp:262] Starting prefetch of epoch 191
I0629 18:58:52.073567  8067 data_reader.cpp:262] Starting prefetch of epoch 191
I0629 18:58:52.088157  8094 data_reader.cpp:262] Starting prefetch of epoch 191
I0629 18:58:59.498661  8090 solver.cpp:349] Iteration 31600 (4.67447 iter/s, 21.3928s/100 iter), loss = 0.196242
I0629 18:58:59.498687  8090 solver.cpp:371]     Train net output #0: loss = 0.196242 (* 1 = 0.196242 loss)
I0629 18:58:59.498692  8090 sgd_solver.cpp:137] Iteration 31600, lr = 1e-05, m = 0.9
I0629 18:59:20.865248  8090 solver.cpp:349] Iteration 31700 (4.6803 iter/s, 21.3661s/100 iter), loss = 0.0977103
I0629 18:59:20.865272  8090 solver.cpp:371]     Train net output #0: loss = 0.0977105 (* 1 = 0.0977105 loss)
I0629 18:59:20.865275  8090 sgd_solver.cpp:137] Iteration 31700, lr = 1e-05, m = 0.9
I0629 18:59:27.251430  8069 data_reader.cpp:262] Starting prefetch of epoch 192
I0629 18:59:27.258147  8098 data_reader.cpp:262] Starting prefetch of epoch 192
I0629 18:59:27.264976  8096 data_reader.cpp:262] Starting prefetch of epoch 192
I0629 18:59:27.275763  8094 data_reader.cpp:262] Starting prefetch of epoch 192
I0629 18:59:27.282271  8093 data_reader.cpp:262] Starting prefetch of epoch 192
I0629 18:59:27.286121  8067 data_reader.cpp:262] Starting prefetch of epoch 192
I0629 18:59:42.177304  8090 solver.cpp:349] Iteration 31800 (4.69228 iter/s, 21.3116s/100 iter), loss = 0.200245
I0629 18:59:42.177326  8090 solver.cpp:371]     Train net output #0: loss = 0.200246 (* 1 = 0.200246 loss)
I0629 18:59:42.177330  8090 sgd_solver.cpp:137] Iteration 31800, lr = 1e-05, m = 0.9
I0629 19:00:02.505681  8098 data_reader.cpp:262] Starting prefetch of epoch 193
I0629 19:00:02.506603  8069 data_reader.cpp:262] Starting prefetch of epoch 193
I0629 19:00:02.510133  8096 data_reader.cpp:262] Starting prefetch of epoch 193
I0629 19:00:02.524265  8093 data_reader.cpp:262] Starting prefetch of epoch 193
I0629 19:00:02.529510  8067 data_reader.cpp:262] Starting prefetch of epoch 193
I0629 19:00:02.532750  8094 data_reader.cpp:262] Starting prefetch of epoch 193
I0629 19:00:03.546506  8090 solver.cpp:349] Iteration 31900 (4.67975 iter/s, 21.3687s/100 iter), loss = 0.167548
I0629 19:00:03.546530  8090 solver.cpp:371]     Train net output #0: loss = 0.167548 (* 1 = 0.167548 loss)
I0629 19:00:03.546535  8090 sgd_solver.cpp:137] Iteration 31900, lr = 1e-05, m = 0.9
I0629 19:00:24.633518  8090 solver.cpp:349] Iteration 31999 (4.69498 iter/s, 21.0864s/100 iter), loss = 0.146131
I0629 19:00:24.633540  8090 solver.cpp:371]     Train net output #0: loss = 0.146131 (* 1 = 0.146131 loss)
I0629 19:00:24.709707  8090 solver.cpp:675] Snapshotting to binary proto file training/cityscapes20_jsegnet21v2_2017-06-29_16-47-23/initial/cityscapes20_jsegnet21v2_iter_32000.caffemodel
I0629 19:00:24.787062  8090 sgd_solver.cpp:288] Snapshotting solver state to binary proto file training/cityscapes20_jsegnet21v2_2017-06-29_16-47-23/initial/cityscapes20_jsegnet21v2_iter_32000.solverstate
I0629 19:00:24.908118  8090 solver.cpp:522] Iteration 32000, loss = 0.167183
I0629 19:00:24.908154  8090 solver.cpp:545] Iteration 32000, Testing net (#0)
I0629 19:00:30.816359  8128 data_reader.cpp:262] Starting prefetch of epoch 23
I0629 19:00:30.820714  8126 data_reader.cpp:262] Starting prefetch of epoch 23
I0629 19:00:30.851672  8088 data_reader.cpp:262] Starting prefetch of epoch 23
I0629 19:00:30.856456  8086 data_reader.cpp:262] Starting prefetch of epoch 23
I0629 19:00:31.025085  8119 data_reader.cpp:262] Starting prefetch of epoch 23
I0629 19:00:31.028127  8117 data_reader.cpp:262] Starting prefetch of epoch 23
I0629 19:00:43.465312  8088 data_reader.cpp:262] Starting prefetch of epoch 24
I0629 19:00:43.469804  8086 data_reader.cpp:262] Starting prefetch of epoch 24
I0629 19:00:44.040253  8128 data_reader.cpp:262] Starting prefetch of epoch 24
I0629 19:00:44.044481  8126 data_reader.cpp:262] Starting prefetch of epoch 24
I0629 19:00:44.355182  8119 data_reader.cpp:262] Starting prefetch of epoch 24
I0629 19:00:44.360430  8117 data_reader.cpp:262] Starting prefetch of epoch 24
I0629 19:00:44.996147  8090 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.928664
I0629 19:00:44.996170  8090 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.995112
I0629 19:00:44.996177  8090 solver.cpp:630]     Test net output #2: loss = 0.216214 (* 1 = 0.216214 loss)
I0629 19:00:45.057749  8020 parallel.cpp:71] Root Solver performance on device 0: 4.012 * 6 = 24.07 img/sec
I0629 19:00:45.057771  8020 parallel.cpp:76]      Solver performance on device 1: 4.012 * 6 = 24.07 img/sec
I0629 19:00:45.057778  8020 parallel.cpp:76]      Solver performance on device 2: 4.012 * 6 = 24.07 img/sec
I0629 19:00:45.057781  8020 parallel.cpp:79] Overall multi-GPU performance: 72.2128 img/sec
I0629 19:00:46.065050  8020 caffe.cpp:247] Optimization Done in 2h 13m 22s
training/cityscapes20_jsegnet21v2_2017-06-29_16-47-23/sparse
I0629 19:01:05.037750 16736 caffe.cpp:608] This is NVCaffe 0.16.2 started at Thu Jun 29 19:01:04 2017
I0629 19:01:05.037991 16736 caffe.cpp:611] CuDNN version: 6.0.21
I0629 19:01:05.037995 16736 caffe.cpp:612] CuBLAS version: 8000
I0629 19:01:05.037997 16736 caffe.cpp:613] CUDA version: 8000
I0629 19:01:05.037998 16736 caffe.cpp:614] CUDA driver version: 8000
I0629 19:01:05.346684 16736 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0629 19:01:05.347254 16736 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I0629 19:01:05.347772 16736 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[1]: total=8508145664 free=8379236352
I0629 19:01:05.348284 16736 gpu_memory.cpp:161] Total memory: 8508145664, Free: 8278441984, dev_info[2]: total=8508145664 free=8379236352
I0629 19:01:05.348294 16736 caffe.cpp:208] Using GPUs 0, 1, 2
I0629 19:01:05.348614 16736 caffe.cpp:213] GPU 0: GeForce GTX 1080
I0629 19:01:05.348937 16736 caffe.cpp:213] GPU 1: GeForce GTX 1080
I0629 19:01:05.349257 16736 caffe.cpp:213] GPU 2: GeForce GTX 1080
I0629 19:01:05.352123 16736 solver.cpp:42] Solver data type: FLOAT
I0629 19:01:05.352169 16736 solver.cpp:45] Initializing solver from parameters: 
train_net: "training/cityscapes20_jsegnet21v2_2017-06-29_16-47-23/sparse/train.prototxt"
test_net: "training/cityscapes20_jsegnet21v2_2017-06-29_16-47-23/sparse/test.prototxt"
test_iter: 125
test_interval: 2000
base_lr: 1e-05
display: 100
max_iter: 32000
lr_policy: "multistep"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 0.0001
snapshot: 10000
snapshot_prefix: "training/cityscapes20_jsegnet21v2_2017-06-29_16-47-23/sparse/cityscapes20_jsegnet21v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
snapshot_after_train: true
test_initialization: false
stepvalue: 24000
iter_size: 1
type: "Adam"
display_sparsity: 1000
sparse_mode: SPARSE_UPDATE
sparsity_target: 0.8
sparsity_step_factor: 0.05
sparsity_step_iter: 1000
sparsity_start_iter: 0
sparsity_start_factor: 0
I0629 19:01:05.374461 16736 solver.cpp:77] Creating training net from train_net file: training/cityscapes20_jsegnet21v2_2017-06-29_16-47-23/sparse/train.prototxt
I0629 19:01:05.378082 16736 net.cpp:442] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I0629 19:01:05.378123 16736 net.cpp:442] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
W0629 19:01:05.378481 16736 parallel.cpp:271] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0629 19:01:05.379655 16736 net.cpp:77] Initializing net from parameters: 
name: "jsegnet21v2_train"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/train-image-lmdb"
    label_list_path: "data/train-label-lmdb"
    batch_size: 6
    threads: 1
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 20
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 20
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 20
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
I0629 19:01:05.380476 16736 net.cpp:108] Using FLOAT as default forward math type
I0629 19:01:05.380512 16736 net.cpp:114] Using FLOAT as default backward math type
I0629 19:01:05.380532 16736 layer_factory.hpp:136] Creating layer 'data' of type 'ImageLabelData'
I0629 19:01:05.380553 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:05.385843 16736 net.cpp:183] Created Layer data (0)
I0629 19:01:05.385884 16736 net.cpp:529] data -> data
I0629 19:01:05.385938 16736 net.cpp:529] data -> label
I0629 19:01:05.388730 16736 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0629 19:01:05.388797 16736 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0629 19:01:05.410303 16796 db_lmdb.cpp:35] Opened lmdb data/train-image-lmdb
I0629 19:01:05.432142 16736 data_layer.cpp:188] ReshapePrefetch 6, 3, 640, 640
I0629 19:01:05.432200 16736 data_layer.cpp:206] Output data size: 6, 3, 640, 640
I0629 19:01:05.432207 16736 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0629 19:01:05.432251 16736 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0629 19:01:05.432261 16736 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0629 19:01:05.433092 16797 data_layer.cpp:188] ReshapePrefetch 6, 3, 640, 640
I0629 19:01:05.433104 16797 data_layer.cpp:206] Output data size: 6, 3, 640, 640
I0629 19:01:05.440701 16798 db_lmdb.cpp:35] Opened lmdb data/train-label-lmdb
I0629 19:01:05.442013 16736 data_layer.cpp:188] ReshapePrefetch 6, 1, 640, 640
I0629 19:01:05.442060 16736 data_layer.cpp:206] Output data size: 6, 1, 640, 640
I0629 19:01:05.442066 16736 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0629 19:01:05.442217 16736 net.cpp:244] Setting up data
I0629 19:01:05.442236 16736 net.cpp:251] TRAIN Top shape for layer 0 'data' 6 3 640 640 (7372800)
I0629 19:01:05.442243 16736 net.cpp:251] TRAIN Top shape for layer 0 'data' 6 1 640 640 (2457600)
I0629 19:01:05.442250 16736 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0629 19:01:05.442255 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:05.442589 16736 net.cpp:183] Created Layer data/bias (1)
I0629 19:01:05.442597 16736 net.cpp:560] data/bias <- data
I0629 19:01:05.442605 16736 net.cpp:529] data/bias -> data/bias
I0629 19:01:05.443321 16799 data_layer.cpp:188] ReshapePrefetch 6, 1, 640, 640
I0629 19:01:05.443336 16799 data_layer.cpp:206] Output data size: 6, 1, 640, 640
I0629 19:01:05.448269 16797 data_layer.cpp:110] [0] Parser threads: 1
I0629 19:01:05.448288 16797 data_layer.cpp:112] [0] Transformer threads: 1
I0629 19:01:05.450146 16799 data_layer.cpp:110] [0] Parser threads: 1
I0629 19:01:05.450165 16799 data_layer.cpp:112] [0] Transformer threads: 1
I0629 19:01:05.450388 16736 net.cpp:244] Setting up data/bias
I0629 19:01:05.450402 16736 net.cpp:251] TRAIN Top shape for layer 1 'data/bias' 6 3 640 640 (7372800)
I0629 19:01:05.450414 16736 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0629 19:01:05.450419 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:05.450438 16736 net.cpp:183] Created Layer conv1a (2)
I0629 19:01:05.450441 16736 net.cpp:560] conv1a <- data/bias
I0629 19:01:05.450445 16736 net.cpp:529] conv1a -> conv1a
I0629 19:01:06.161960 16736 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 7.9G, req 0G)
I0629 19:01:06.161998 16736 net.cpp:244] Setting up conv1a
I0629 19:01:06.162006 16736 net.cpp:251] TRAIN Top shape for layer 2 'conv1a' 6 32 320 320 (19660800)
I0629 19:01:06.162017 16736 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0629 19:01:06.162021 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.162041 16736 net.cpp:183] Created Layer conv1a/bn (3)
I0629 19:01:06.162047 16736 net.cpp:560] conv1a/bn <- conv1a
I0629 19:01:06.162053 16736 net.cpp:512] conv1a/bn -> conv1a (in-place)
I0629 19:01:06.163410 16736 net.cpp:244] Setting up conv1a/bn
I0629 19:01:06.163424 16736 net.cpp:251] TRAIN Top shape for layer 3 'conv1a/bn' 6 32 320 320 (19660800)
I0629 19:01:06.163439 16736 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0629 19:01:06.163445 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.163456 16736 net.cpp:183] Created Layer conv1a/relu (4)
I0629 19:01:06.163462 16736 net.cpp:560] conv1a/relu <- conv1a
I0629 19:01:06.163466 16736 net.cpp:512] conv1a/relu -> conv1a (in-place)
I0629 19:01:06.163836 16736 net.cpp:244] Setting up conv1a/relu
I0629 19:01:06.163844 16736 net.cpp:251] TRAIN Top shape for layer 4 'conv1a/relu' 6 32 320 320 (19660800)
I0629 19:01:06.163847 16736 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0629 19:01:06.163851 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.163863 16736 net.cpp:183] Created Layer conv1b (5)
I0629 19:01:06.163866 16736 net.cpp:560] conv1b <- conv1a
I0629 19:01:06.163869 16736 net.cpp:529] conv1b -> conv1b
I0629 19:01:06.212306 16736 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.73G, req 0G)
I0629 19:01:06.212334 16736 net.cpp:244] Setting up conv1b
I0629 19:01:06.212343 16736 net.cpp:251] TRAIN Top shape for layer 5 'conv1b' 6 32 320 320 (19660800)
I0629 19:01:06.212353 16736 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0629 19:01:06.212358 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.212368 16736 net.cpp:183] Created Layer conv1b/bn (6)
I0629 19:01:06.212371 16736 net.cpp:560] conv1b/bn <- conv1b
I0629 19:01:06.212376 16736 net.cpp:512] conv1b/bn -> conv1b (in-place)
I0629 19:01:06.213099 16736 net.cpp:244] Setting up conv1b/bn
I0629 19:01:06.213106 16736 net.cpp:251] TRAIN Top shape for layer 6 'conv1b/bn' 6 32 320 320 (19660800)
I0629 19:01:06.213114 16736 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0629 19:01:06.213116 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.213120 16736 net.cpp:183] Created Layer conv1b/relu (7)
I0629 19:01:06.213122 16736 net.cpp:560] conv1b/relu <- conv1b
I0629 19:01:06.213124 16736 net.cpp:512] conv1b/relu -> conv1b (in-place)
I0629 19:01:06.213129 16736 net.cpp:244] Setting up conv1b/relu
I0629 19:01:06.213131 16736 net.cpp:251] TRAIN Top shape for layer 7 'conv1b/relu' 6 32 320 320 (19660800)
I0629 19:01:06.213134 16736 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0629 19:01:06.213136 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.213153 16736 net.cpp:183] Created Layer pool1 (8)
I0629 19:01:06.213157 16736 net.cpp:560] pool1 <- conv1b
I0629 19:01:06.213160 16736 net.cpp:529] pool1 -> pool1
I0629 19:01:06.213259 16736 net.cpp:244] Setting up pool1
I0629 19:01:06.213266 16736 net.cpp:251] TRAIN Top shape for layer 8 'pool1' 6 32 160 160 (4915200)
I0629 19:01:06.213269 16736 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0629 19:01:06.213274 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.213282 16736 net.cpp:183] Created Layer res2a_branch2a (9)
I0629 19:01:06.213286 16736 net.cpp:560] res2a_branch2a <- pool1
I0629 19:01:06.213289 16736 net.cpp:529] res2a_branch2a -> res2a_branch2a
I0629 19:01:06.256810 16736 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.61G, req 0G)
I0629 19:01:06.256856 16736 net.cpp:244] Setting up res2a_branch2a
I0629 19:01:06.256870 16736 net.cpp:251] TRAIN Top shape for layer 9 'res2a_branch2a' 6 64 160 160 (9830400)
I0629 19:01:06.256886 16736 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0629 19:01:06.256893 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.256906 16736 net.cpp:183] Created Layer res2a_branch2a/bn (10)
I0629 19:01:06.256913 16736 net.cpp:560] res2a_branch2a/bn <- res2a_branch2a
I0629 19:01:06.256920 16736 net.cpp:512] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0629 19:01:06.259313 16736 net.cpp:244] Setting up res2a_branch2a/bn
I0629 19:01:06.259341 16736 net.cpp:251] TRAIN Top shape for layer 10 'res2a_branch2a/bn' 6 64 160 160 (9830400)
I0629 19:01:06.259361 16736 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0629 19:01:06.259372 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.259384 16736 net.cpp:183] Created Layer res2a_branch2a/relu (11)
I0629 19:01:06.259395 16736 net.cpp:560] res2a_branch2a/relu <- res2a_branch2a
I0629 19:01:06.259416 16736 net.cpp:512] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0629 19:01:06.259431 16736 net.cpp:244] Setting up res2a_branch2a/relu
I0629 19:01:06.259445 16736 net.cpp:251] TRAIN Top shape for layer 11 'res2a_branch2a/relu' 6 64 160 160 (9830400)
I0629 19:01:06.259455 16736 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0629 19:01:06.259466 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.259487 16736 net.cpp:183] Created Layer res2a_branch2b (12)
I0629 19:01:06.259498 16736 net.cpp:560] res2a_branch2b <- res2a_branch2a
I0629 19:01:06.259508 16736 net.cpp:529] res2a_branch2b -> res2a_branch2b
I0629 19:01:06.284965 16736 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.52G, req 0G)
I0629 19:01:06.284993 16736 net.cpp:244] Setting up res2a_branch2b
I0629 19:01:06.285001 16736 net.cpp:251] TRAIN Top shape for layer 12 'res2a_branch2b' 6 64 160 160 (9830400)
I0629 19:01:06.285009 16736 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0629 19:01:06.285012 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.285022 16736 net.cpp:183] Created Layer res2a_branch2b/bn (13)
I0629 19:01:06.285024 16736 net.cpp:560] res2a_branch2b/bn <- res2a_branch2b
I0629 19:01:06.285028 16736 net.cpp:512] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0629 19:01:06.285818 16736 net.cpp:244] Setting up res2a_branch2b/bn
I0629 19:01:06.285828 16736 net.cpp:251] TRAIN Top shape for layer 13 'res2a_branch2b/bn' 6 64 160 160 (9830400)
I0629 19:01:06.285835 16736 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0629 19:01:06.285837 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.285852 16736 net.cpp:183] Created Layer res2a_branch2b/relu (14)
I0629 19:01:06.285856 16736 net.cpp:560] res2a_branch2b/relu <- res2a_branch2b
I0629 19:01:06.285857 16736 net.cpp:512] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0629 19:01:06.285861 16736 net.cpp:244] Setting up res2a_branch2b/relu
I0629 19:01:06.285866 16736 net.cpp:251] TRAIN Top shape for layer 14 'res2a_branch2b/relu' 6 64 160 160 (9830400)
I0629 19:01:06.285869 16736 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0629 19:01:06.285872 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.285876 16736 net.cpp:183] Created Layer pool2 (15)
I0629 19:01:06.285879 16736 net.cpp:560] pool2 <- res2a_branch2b
I0629 19:01:06.285881 16736 net.cpp:529] pool2 -> pool2
I0629 19:01:06.285960 16736 net.cpp:244] Setting up pool2
I0629 19:01:06.285969 16736 net.cpp:251] TRAIN Top shape for layer 15 'pool2' 6 64 80 80 (2457600)
I0629 19:01:06.285972 16736 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0629 19:01:06.285976 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.285986 16736 net.cpp:183] Created Layer res3a_branch2a (16)
I0629 19:01:06.285990 16736 net.cpp:560] res3a_branch2a <- pool2
I0629 19:01:06.285993 16736 net.cpp:529] res3a_branch2a -> res3a_branch2a
I0629 19:01:06.308323 16736 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 7.46G, req 0G)
I0629 19:01:06.308353 16736 net.cpp:244] Setting up res3a_branch2a
I0629 19:01:06.308362 16736 net.cpp:251] TRAIN Top shape for layer 16 'res3a_branch2a' 6 128 80 80 (4915200)
I0629 19:01:06.308368 16736 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0629 19:01:06.308372 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.308382 16736 net.cpp:183] Created Layer res3a_branch2a/bn (17)
I0629 19:01:06.308384 16736 net.cpp:560] res3a_branch2a/bn <- res3a_branch2a
I0629 19:01:06.308388 16736 net.cpp:512] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0629 19:01:06.309173 16736 net.cpp:244] Setting up res3a_branch2a/bn
I0629 19:01:06.309182 16736 net.cpp:251] TRAIN Top shape for layer 17 'res3a_branch2a/bn' 6 128 80 80 (4915200)
I0629 19:01:06.309191 16736 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0629 19:01:06.309195 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.309198 16736 net.cpp:183] Created Layer res3a_branch2a/relu (18)
I0629 19:01:06.309201 16736 net.cpp:560] res3a_branch2a/relu <- res3a_branch2a
I0629 19:01:06.309202 16736 net.cpp:512] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0629 19:01:06.309206 16736 net.cpp:244] Setting up res3a_branch2a/relu
I0629 19:01:06.309209 16736 net.cpp:251] TRAIN Top shape for layer 18 'res3a_branch2a/relu' 6 128 80 80 (4915200)
I0629 19:01:06.309211 16736 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0629 19:01:06.309213 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.309222 16736 net.cpp:183] Created Layer res3a_branch2b (19)
I0629 19:01:06.309226 16736 net.cpp:560] res3a_branch2b <- res3a_branch2a
I0629 19:01:06.309227 16736 net.cpp:529] res3a_branch2b -> res3a_branch2b
I0629 19:01:06.325409 16736 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.42G, req 0G)
I0629 19:01:06.325426 16736 net.cpp:244] Setting up res3a_branch2b
I0629 19:01:06.325431 16736 net.cpp:251] TRAIN Top shape for layer 19 'res3a_branch2b' 6 128 80 80 (4915200)
I0629 19:01:06.325436 16736 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0629 19:01:06.325439 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.325444 16736 net.cpp:183] Created Layer res3a_branch2b/bn (20)
I0629 19:01:06.325456 16736 net.cpp:560] res3a_branch2b/bn <- res3a_branch2b
I0629 19:01:06.325459 16736 net.cpp:512] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0629 19:01:06.326148 16736 net.cpp:244] Setting up res3a_branch2b/bn
I0629 19:01:06.326156 16736 net.cpp:251] TRAIN Top shape for layer 20 'res3a_branch2b/bn' 6 128 80 80 (4915200)
I0629 19:01:06.326162 16736 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0629 19:01:06.326165 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.326169 16736 net.cpp:183] Created Layer res3a_branch2b/relu (21)
I0629 19:01:06.326171 16736 net.cpp:560] res3a_branch2b/relu <- res3a_branch2b
I0629 19:01:06.326174 16736 net.cpp:512] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0629 19:01:06.326177 16736 net.cpp:244] Setting up res3a_branch2b/relu
I0629 19:01:06.326179 16736 net.cpp:251] TRAIN Top shape for layer 21 'res3a_branch2b/relu' 6 128 80 80 (4915200)
I0629 19:01:06.326181 16736 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0629 19:01:06.326184 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.326189 16736 net.cpp:183] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (22)
I0629 19:01:06.326190 16736 net.cpp:560] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0629 19:01:06.326194 16736 net.cpp:529] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0629 19:01:06.326200 16736 net.cpp:529] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0629 19:01:06.326239 16736 net.cpp:244] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0629 19:01:06.326243 16736 net.cpp:251] TRAIN Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 6 128 80 80 (4915200)
I0629 19:01:06.326246 16736 net.cpp:251] TRAIN Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 6 128 80 80 (4915200)
I0629 19:01:06.326248 16736 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0629 19:01:06.326251 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.326256 16736 net.cpp:183] Created Layer pool3 (23)
I0629 19:01:06.326259 16736 net.cpp:560] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0629 19:01:06.326263 16736 net.cpp:529] pool3 -> pool3
I0629 19:01:06.326334 16736 net.cpp:244] Setting up pool3
I0629 19:01:06.326340 16736 net.cpp:251] TRAIN Top shape for layer 23 'pool3' 6 128 40 40 (1228800)
I0629 19:01:06.326344 16736 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0629 19:01:06.326345 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.326351 16736 net.cpp:183] Created Layer res4a_branch2a (24)
I0629 19:01:06.326354 16736 net.cpp:560] res4a_branch2a <- pool3
I0629 19:01:06.326356 16736 net.cpp:529] res4a_branch2a -> res4a_branch2a
I0629 19:01:06.352810 16736 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.38G, req 0G)
I0629 19:01:06.352836 16736 net.cpp:244] Setting up res4a_branch2a
I0629 19:01:06.352844 16736 net.cpp:251] TRAIN Top shape for layer 24 'res4a_branch2a' 6 256 40 40 (2457600)
I0629 19:01:06.352852 16736 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0629 19:01:06.352855 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.352866 16736 net.cpp:183] Created Layer res4a_branch2a/bn (25)
I0629 19:01:06.352869 16736 net.cpp:560] res4a_branch2a/bn <- res4a_branch2a
I0629 19:01:06.352872 16736 net.cpp:512] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0629 19:01:06.353610 16736 net.cpp:244] Setting up res4a_branch2a/bn
I0629 19:01:06.353618 16736 net.cpp:251] TRAIN Top shape for layer 25 'res4a_branch2a/bn' 6 256 40 40 (2457600)
I0629 19:01:06.353634 16736 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0629 19:01:06.353637 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.353641 16736 net.cpp:183] Created Layer res4a_branch2a/relu (26)
I0629 19:01:06.353643 16736 net.cpp:560] res4a_branch2a/relu <- res4a_branch2a
I0629 19:01:06.353646 16736 net.cpp:512] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0629 19:01:06.353651 16736 net.cpp:244] Setting up res4a_branch2a/relu
I0629 19:01:06.353653 16736 net.cpp:251] TRAIN Top shape for layer 26 'res4a_branch2a/relu' 6 256 40 40 (2457600)
I0629 19:01:06.353657 16736 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0629 19:01:06.353659 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.353667 16736 net.cpp:183] Created Layer res4a_branch2b (27)
I0629 19:01:06.353670 16736 net.cpp:560] res4a_branch2b <- res4a_branch2a
I0629 19:01:06.353672 16736 net.cpp:529] res4a_branch2b -> res4a_branch2b
I0629 19:01:06.364488 16736 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.36G, req 0G)
I0629 19:01:06.364511 16736 net.cpp:244] Setting up res4a_branch2b
I0629 19:01:06.364517 16736 net.cpp:251] TRAIN Top shape for layer 27 'res4a_branch2b' 6 256 40 40 (2457600)
I0629 19:01:06.364523 16736 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0629 19:01:06.364527 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.364536 16736 net.cpp:183] Created Layer res4a_branch2b/bn (28)
I0629 19:01:06.364538 16736 net.cpp:560] res4a_branch2b/bn <- res4a_branch2b
I0629 19:01:06.364542 16736 net.cpp:512] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0629 19:01:06.365206 16736 net.cpp:244] Setting up res4a_branch2b/bn
I0629 19:01:06.365214 16736 net.cpp:251] TRAIN Top shape for layer 28 'res4a_branch2b/bn' 6 256 40 40 (2457600)
I0629 19:01:06.365221 16736 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0629 19:01:06.365223 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.365227 16736 net.cpp:183] Created Layer res4a_branch2b/relu (29)
I0629 19:01:06.365231 16736 net.cpp:560] res4a_branch2b/relu <- res4a_branch2b
I0629 19:01:06.365233 16736 net.cpp:512] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0629 19:01:06.365238 16736 net.cpp:244] Setting up res4a_branch2b/relu
I0629 19:01:06.365242 16736 net.cpp:251] TRAIN Top shape for layer 29 'res4a_branch2b/relu' 6 256 40 40 (2457600)
I0629 19:01:06.365243 16736 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0629 19:01:06.365247 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.365252 16736 net.cpp:183] Created Layer pool4 (30)
I0629 19:01:06.365253 16736 net.cpp:560] pool4 <- res4a_branch2b
I0629 19:01:06.365257 16736 net.cpp:529] pool4 -> pool4
I0629 19:01:06.365316 16736 net.cpp:244] Setting up pool4
I0629 19:01:06.365324 16736 net.cpp:251] TRAIN Top shape for layer 30 'pool4' 6 256 40 40 (2457600)
I0629 19:01:06.365327 16736 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0629 19:01:06.365329 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.365345 16736 net.cpp:183] Created Layer res5a_branch2a (31)
I0629 19:01:06.365350 16736 net.cpp:560] res5a_branch2a <- pool4
I0629 19:01:06.365351 16736 net.cpp:529] res5a_branch2a -> res5a_branch2a
I0629 19:01:06.391057 16736 net.cpp:244] Setting up res5a_branch2a
I0629 19:01:06.391079 16736 net.cpp:251] TRAIN Top shape for layer 31 'res5a_branch2a' 6 512 40 40 (4915200)
I0629 19:01:06.391086 16736 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0629 19:01:06.391091 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.391111 16736 net.cpp:183] Created Layer res5a_branch2a/bn (32)
I0629 19:01:06.391115 16736 net.cpp:560] res5a_branch2a/bn <- res5a_branch2a
I0629 19:01:06.391119 16736 net.cpp:512] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0629 19:01:06.392288 16736 net.cpp:244] Setting up res5a_branch2a/bn
I0629 19:01:06.392297 16736 net.cpp:251] TRAIN Top shape for layer 32 'res5a_branch2a/bn' 6 512 40 40 (4915200)
I0629 19:01:06.392303 16736 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0629 19:01:06.392307 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.392312 16736 net.cpp:183] Created Layer res5a_branch2a/relu (33)
I0629 19:01:06.392314 16736 net.cpp:560] res5a_branch2a/relu <- res5a_branch2a
I0629 19:01:06.392318 16736 net.cpp:512] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0629 19:01:06.392323 16736 net.cpp:244] Setting up res5a_branch2a/relu
I0629 19:01:06.392326 16736 net.cpp:251] TRAIN Top shape for layer 33 'res5a_branch2a/relu' 6 512 40 40 (4915200)
I0629 19:01:06.392328 16736 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0629 19:01:06.392331 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.392338 16736 net.cpp:183] Created Layer res5a_branch2b (34)
I0629 19:01:06.392340 16736 net.cpp:560] res5a_branch2b <- res5a_branch2a
I0629 19:01:06.392343 16736 net.cpp:529] res5a_branch2b -> res5a_branch2b
I0629 19:01:06.405268 16736 net.cpp:244] Setting up res5a_branch2b
I0629 19:01:06.405292 16736 net.cpp:251] TRAIN Top shape for layer 34 'res5a_branch2b' 6 512 40 40 (4915200)
I0629 19:01:06.405303 16736 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0629 19:01:06.405308 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.405315 16736 net.cpp:183] Created Layer res5a_branch2b/bn (35)
I0629 19:01:06.405319 16736 net.cpp:560] res5a_branch2b/bn <- res5a_branch2b
I0629 19:01:06.405323 16736 net.cpp:512] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0629 19:01:06.406553 16736 net.cpp:244] Setting up res5a_branch2b/bn
I0629 19:01:06.406561 16736 net.cpp:251] TRAIN Top shape for layer 35 'res5a_branch2b/bn' 6 512 40 40 (4915200)
I0629 19:01:06.406569 16736 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0629 19:01:06.406571 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.406576 16736 net.cpp:183] Created Layer res5a_branch2b/relu (36)
I0629 19:01:06.406579 16736 net.cpp:560] res5a_branch2b/relu <- res5a_branch2b
I0629 19:01:06.406582 16736 net.cpp:512] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0629 19:01:06.406587 16736 net.cpp:244] Setting up res5a_branch2b/relu
I0629 19:01:06.406590 16736 net.cpp:251] TRAIN Top shape for layer 36 'res5a_branch2b/relu' 6 512 40 40 (4915200)
I0629 19:01:06.406594 16736 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I0629 19:01:06.406596 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.406608 16736 net.cpp:183] Created Layer out5a (37)
I0629 19:01:06.406611 16736 net.cpp:560] out5a <- res5a_branch2b
I0629 19:01:06.406615 16736 net.cpp:529] out5a -> out5a
I0629 19:01:06.410737 16736 net.cpp:244] Setting up out5a
I0629 19:01:06.410747 16736 net.cpp:251] TRAIN Top shape for layer 37 'out5a' 6 64 40 40 (614400)
I0629 19:01:06.410751 16736 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I0629 19:01:06.410754 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.410766 16736 net.cpp:183] Created Layer out5a/bn (38)
I0629 19:01:06.410769 16736 net.cpp:560] out5a/bn <- out5a
I0629 19:01:06.410773 16736 net.cpp:512] out5a/bn -> out5a (in-place)
I0629 19:01:06.411860 16736 net.cpp:244] Setting up out5a/bn
I0629 19:01:06.411870 16736 net.cpp:251] TRAIN Top shape for layer 38 'out5a/bn' 6 64 40 40 (614400)
I0629 19:01:06.411883 16736 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I0629 19:01:06.411887 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.411890 16736 net.cpp:183] Created Layer out5a/relu (39)
I0629 19:01:06.411893 16736 net.cpp:560] out5a/relu <- out5a
I0629 19:01:06.411897 16736 net.cpp:512] out5a/relu -> out5a (in-place)
I0629 19:01:06.411902 16736 net.cpp:244] Setting up out5a/relu
I0629 19:01:06.411906 16736 net.cpp:251] TRAIN Top shape for layer 39 'out5a/relu' 6 64 40 40 (614400)
I0629 19:01:06.411908 16736 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I0629 19:01:06.411911 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.421535 16736 net.cpp:183] Created Layer out5a_up2 (40)
I0629 19:01:06.421545 16736 net.cpp:560] out5a_up2 <- out5a
I0629 19:01:06.421550 16736 net.cpp:529] out5a_up2 -> out5a_up2
I0629 19:01:06.421855 16736 net.cpp:244] Setting up out5a_up2
I0629 19:01:06.421862 16736 net.cpp:251] TRAIN Top shape for layer 40 'out5a_up2' 6 64 80 80 (2457600)
I0629 19:01:06.421866 16736 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I0629 19:01:06.421870 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.421880 16736 net.cpp:183] Created Layer out3a (41)
I0629 19:01:06.421883 16736 net.cpp:560] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0629 19:01:06.421886 16736 net.cpp:529] out3a -> out3a
I0629 19:01:06.434345 16736 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 7.25G, req 0G)
I0629 19:01:06.434371 16736 net.cpp:244] Setting up out3a
I0629 19:01:06.434378 16736 net.cpp:251] TRAIN Top shape for layer 41 'out3a' 6 64 80 80 (2457600)
I0629 19:01:06.434386 16736 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I0629 19:01:06.434391 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.434398 16736 net.cpp:183] Created Layer out3a/bn (42)
I0629 19:01:06.434401 16736 net.cpp:560] out3a/bn <- out3a
I0629 19:01:06.434406 16736 net.cpp:512] out3a/bn -> out3a (in-place)
I0629 19:01:06.435170 16736 net.cpp:244] Setting up out3a/bn
I0629 19:01:06.435179 16736 net.cpp:251] TRAIN Top shape for layer 42 'out3a/bn' 6 64 80 80 (2457600)
I0629 19:01:06.435185 16736 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I0629 19:01:06.435189 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.435191 16736 net.cpp:183] Created Layer out3a/relu (43)
I0629 19:01:06.435194 16736 net.cpp:560] out3a/relu <- out3a
I0629 19:01:06.435196 16736 net.cpp:512] out3a/relu -> out3a (in-place)
I0629 19:01:06.435200 16736 net.cpp:244] Setting up out3a/relu
I0629 19:01:06.435204 16736 net.cpp:251] TRAIN Top shape for layer 43 'out3a/relu' 6 64 80 80 (2457600)
I0629 19:01:06.435205 16736 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I0629 19:01:06.435207 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.435722 16736 net.cpp:183] Created Layer out3_out5_combined (44)
I0629 19:01:06.435727 16736 net.cpp:560] out3_out5_combined <- out5a_up2
I0629 19:01:06.435730 16736 net.cpp:560] out3_out5_combined <- out3a
I0629 19:01:06.435734 16736 net.cpp:529] out3_out5_combined -> out3_out5_combined
I0629 19:01:06.436728 16736 net.cpp:244] Setting up out3_out5_combined
I0629 19:01:06.436738 16736 net.cpp:251] TRAIN Top shape for layer 44 'out3_out5_combined' 6 64 80 80 (2457600)
I0629 19:01:06.436740 16736 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I0629 19:01:06.436743 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.436750 16736 net.cpp:183] Created Layer ctx_conv1 (45)
I0629 19:01:06.436761 16736 net.cpp:560] ctx_conv1 <- out3_out5_combined
I0629 19:01:06.436764 16736 net.cpp:529] ctx_conv1 -> ctx_conv1
I0629 19:01:06.453028 16736 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 7.2G, req 0G)
I0629 19:01:06.453044 16736 net.cpp:244] Setting up ctx_conv1
I0629 19:01:06.453049 16736 net.cpp:251] TRAIN Top shape for layer 45 'ctx_conv1' 6 64 80 80 (2457600)
I0629 19:01:06.453053 16736 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I0629 19:01:06.453057 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.453061 16736 net.cpp:183] Created Layer ctx_conv1/bn (46)
I0629 19:01:06.453063 16736 net.cpp:560] ctx_conv1/bn <- ctx_conv1
I0629 19:01:06.453065 16736 net.cpp:512] ctx_conv1/bn -> ctx_conv1 (in-place)
I0629 19:01:06.453744 16736 net.cpp:244] Setting up ctx_conv1/bn
I0629 19:01:06.453752 16736 net.cpp:251] TRAIN Top shape for layer 46 'ctx_conv1/bn' 6 64 80 80 (2457600)
I0629 19:01:06.453758 16736 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I0629 19:01:06.453760 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.453763 16736 net.cpp:183] Created Layer ctx_conv1/relu (47)
I0629 19:01:06.453765 16736 net.cpp:560] ctx_conv1/relu <- ctx_conv1
I0629 19:01:06.453768 16736 net.cpp:512] ctx_conv1/relu -> ctx_conv1 (in-place)
I0629 19:01:06.453771 16736 net.cpp:244] Setting up ctx_conv1/relu
I0629 19:01:06.453774 16736 net.cpp:251] TRAIN Top shape for layer 47 'ctx_conv1/relu' 6 64 80 80 (2457600)
I0629 19:01:06.453775 16736 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I0629 19:01:06.453778 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.453784 16736 net.cpp:183] Created Layer ctx_conv2 (48)
I0629 19:01:06.453786 16736 net.cpp:560] ctx_conv2 <- ctx_conv1
I0629 19:01:06.453789 16736 net.cpp:529] ctx_conv2 -> ctx_conv2
I0629 19:01:06.455003 16736 net.cpp:244] Setting up ctx_conv2
I0629 19:01:06.455014 16736 net.cpp:251] TRAIN Top shape for layer 48 'ctx_conv2' 6 64 80 80 (2457600)
I0629 19:01:06.455019 16736 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I0629 19:01:06.455023 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.455027 16736 net.cpp:183] Created Layer ctx_conv2/bn (49)
I0629 19:01:06.455029 16736 net.cpp:560] ctx_conv2/bn <- ctx_conv2
I0629 19:01:06.455032 16736 net.cpp:512] ctx_conv2/bn -> ctx_conv2 (in-place)
I0629 19:01:06.456225 16736 net.cpp:244] Setting up ctx_conv2/bn
I0629 19:01:06.456234 16736 net.cpp:251] TRAIN Top shape for layer 49 'ctx_conv2/bn' 6 64 80 80 (2457600)
I0629 19:01:06.456240 16736 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I0629 19:01:06.456243 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.456248 16736 net.cpp:183] Created Layer ctx_conv2/relu (50)
I0629 19:01:06.456249 16736 net.cpp:560] ctx_conv2/relu <- ctx_conv2
I0629 19:01:06.456251 16736 net.cpp:512] ctx_conv2/relu -> ctx_conv2 (in-place)
I0629 19:01:06.456255 16736 net.cpp:244] Setting up ctx_conv2/relu
I0629 19:01:06.456259 16736 net.cpp:251] TRAIN Top shape for layer 50 'ctx_conv2/relu' 6 64 80 80 (2457600)
I0629 19:01:06.456260 16736 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I0629 19:01:06.456262 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.456267 16736 net.cpp:183] Created Layer ctx_conv3 (51)
I0629 19:01:06.456269 16736 net.cpp:560] ctx_conv3 <- ctx_conv2
I0629 19:01:06.456272 16736 net.cpp:529] ctx_conv3 -> ctx_conv3
I0629 19:01:06.457381 16736 net.cpp:244] Setting up ctx_conv3
I0629 19:01:06.457389 16736 net.cpp:251] TRAIN Top shape for layer 51 'ctx_conv3' 6 64 80 80 (2457600)
I0629 19:01:06.457393 16736 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I0629 19:01:06.457404 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.457408 16736 net.cpp:183] Created Layer ctx_conv3/bn (52)
I0629 19:01:06.457412 16736 net.cpp:560] ctx_conv3/bn <- ctx_conv3
I0629 19:01:06.457413 16736 net.cpp:512] ctx_conv3/bn -> ctx_conv3 (in-place)
I0629 19:01:06.458569 16736 net.cpp:244] Setting up ctx_conv3/bn
I0629 19:01:06.458577 16736 net.cpp:251] TRAIN Top shape for layer 52 'ctx_conv3/bn' 6 64 80 80 (2457600)
I0629 19:01:06.458583 16736 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I0629 19:01:06.458585 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.458590 16736 net.cpp:183] Created Layer ctx_conv3/relu (53)
I0629 19:01:06.458591 16736 net.cpp:560] ctx_conv3/relu <- ctx_conv3
I0629 19:01:06.458593 16736 net.cpp:512] ctx_conv3/relu -> ctx_conv3 (in-place)
I0629 19:01:06.458597 16736 net.cpp:244] Setting up ctx_conv3/relu
I0629 19:01:06.458600 16736 net.cpp:251] TRAIN Top shape for layer 53 'ctx_conv3/relu' 6 64 80 80 (2457600)
I0629 19:01:06.458602 16736 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I0629 19:01:06.458605 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.458611 16736 net.cpp:183] Created Layer ctx_conv4 (54)
I0629 19:01:06.458613 16736 net.cpp:560] ctx_conv4 <- ctx_conv3
I0629 19:01:06.458616 16736 net.cpp:529] ctx_conv4 -> ctx_conv4
I0629 19:01:06.459718 16736 net.cpp:244] Setting up ctx_conv4
I0629 19:01:06.459727 16736 net.cpp:251] TRAIN Top shape for layer 54 'ctx_conv4' 6 64 80 80 (2457600)
I0629 19:01:06.459730 16736 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I0629 19:01:06.459733 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.459738 16736 net.cpp:183] Created Layer ctx_conv4/bn (55)
I0629 19:01:06.459739 16736 net.cpp:560] ctx_conv4/bn <- ctx_conv4
I0629 19:01:06.459741 16736 net.cpp:512] ctx_conv4/bn -> ctx_conv4 (in-place)
I0629 19:01:06.460875 16736 net.cpp:244] Setting up ctx_conv4/bn
I0629 19:01:06.460885 16736 net.cpp:251] TRAIN Top shape for layer 55 'ctx_conv4/bn' 6 64 80 80 (2457600)
I0629 19:01:06.460891 16736 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I0629 19:01:06.460892 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.460896 16736 net.cpp:183] Created Layer ctx_conv4/relu (56)
I0629 19:01:06.460898 16736 net.cpp:560] ctx_conv4/relu <- ctx_conv4
I0629 19:01:06.460901 16736 net.cpp:512] ctx_conv4/relu -> ctx_conv4 (in-place)
I0629 19:01:06.460904 16736 net.cpp:244] Setting up ctx_conv4/relu
I0629 19:01:06.460906 16736 net.cpp:251] TRAIN Top shape for layer 56 'ctx_conv4/relu' 6 64 80 80 (2457600)
I0629 19:01:06.460908 16736 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I0629 19:01:06.460911 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.460916 16736 net.cpp:183] Created Layer ctx_final (57)
I0629 19:01:06.460919 16736 net.cpp:560] ctx_final <- ctx_conv4
I0629 19:01:06.460922 16736 net.cpp:529] ctx_final -> ctx_final
I0629 19:01:06.477030 16736 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 7.15G, req 0G)
I0629 19:01:06.477048 16736 net.cpp:244] Setting up ctx_final
I0629 19:01:06.477053 16736 net.cpp:251] TRAIN Top shape for layer 57 'ctx_final' 6 20 80 80 (768000)
I0629 19:01:06.477057 16736 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I0629 19:01:06.477061 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.477066 16736 net.cpp:183] Created Layer ctx_final/relu (58)
I0629 19:01:06.477067 16736 net.cpp:560] ctx_final/relu <- ctx_final
I0629 19:01:06.477071 16736 net.cpp:512] ctx_final/relu -> ctx_final (in-place)
I0629 19:01:06.477083 16736 net.cpp:244] Setting up ctx_final/relu
I0629 19:01:06.477087 16736 net.cpp:251] TRAIN Top shape for layer 58 'ctx_final/relu' 6 20 80 80 (768000)
I0629 19:01:06.477088 16736 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I0629 19:01:06.477092 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.477097 16736 net.cpp:183] Created Layer out_deconv_final_up2 (59)
I0629 19:01:06.477099 16736 net.cpp:560] out_deconv_final_up2 <- ctx_final
I0629 19:01:06.477102 16736 net.cpp:529] out_deconv_final_up2 -> out_deconv_final_up2
I0629 19:01:06.477401 16736 net.cpp:244] Setting up out_deconv_final_up2
I0629 19:01:06.477406 16736 net.cpp:251] TRAIN Top shape for layer 59 'out_deconv_final_up2' 6 20 160 160 (3072000)
I0629 19:01:06.477411 16736 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I0629 19:01:06.477412 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.477416 16736 net.cpp:183] Created Layer out_deconv_final_up4 (60)
I0629 19:01:06.477419 16736 net.cpp:560] out_deconv_final_up4 <- out_deconv_final_up2
I0629 19:01:06.477421 16736 net.cpp:529] out_deconv_final_up4 -> out_deconv_final_up4
I0629 19:01:06.477692 16736 net.cpp:244] Setting up out_deconv_final_up4
I0629 19:01:06.477697 16736 net.cpp:251] TRAIN Top shape for layer 60 'out_deconv_final_up4' 6 20 320 320 (12288000)
I0629 19:01:06.477700 16736 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I0629 19:01:06.477704 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.477708 16736 net.cpp:183] Created Layer out_deconv_final_up8 (61)
I0629 19:01:06.477711 16736 net.cpp:560] out_deconv_final_up8 <- out_deconv_final_up4
I0629 19:01:06.477713 16736 net.cpp:529] out_deconv_final_up8 -> out_deconv_final_up8
I0629 19:01:06.477988 16736 net.cpp:244] Setting up out_deconv_final_up8
I0629 19:01:06.477994 16736 net.cpp:251] TRAIN Top shape for layer 61 'out_deconv_final_up8' 6 20 640 640 (49152000)
I0629 19:01:06.477998 16736 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0629 19:01:06.478000 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.478221 16736 net.cpp:183] Created Layer loss (62)
I0629 19:01:06.478226 16736 net.cpp:560] loss <- out_deconv_final_up8
I0629 19:01:06.478229 16736 net.cpp:560] loss <- label
I0629 19:01:06.478233 16736 net.cpp:529] loss -> loss
I0629 19:01:06.480543 16736 net.cpp:244] Setting up loss
I0629 19:01:06.480552 16736 net.cpp:251] TRAIN Top shape for layer 62 'loss' (1)
I0629 19:01:06.480556 16736 net.cpp:255]     with loss weight 1
I0629 19:01:06.480559 16736 net.cpp:322] loss needs backward computation.
I0629 19:01:06.480561 16736 net.cpp:322] out_deconv_final_up8 needs backward computation.
I0629 19:01:06.480564 16736 net.cpp:322] out_deconv_final_up4 needs backward computation.
I0629 19:01:06.480566 16736 net.cpp:322] out_deconv_final_up2 needs backward computation.
I0629 19:01:06.480567 16736 net.cpp:322] ctx_final/relu needs backward computation.
I0629 19:01:06.480571 16736 net.cpp:322] ctx_final needs backward computation.
I0629 19:01:06.480572 16736 net.cpp:322] ctx_conv4/relu needs backward computation.
I0629 19:01:06.480574 16736 net.cpp:322] ctx_conv4/bn needs backward computation.
I0629 19:01:06.480576 16736 net.cpp:322] ctx_conv4 needs backward computation.
I0629 19:01:06.480578 16736 net.cpp:322] ctx_conv3/relu needs backward computation.
I0629 19:01:06.480581 16736 net.cpp:322] ctx_conv3/bn needs backward computation.
I0629 19:01:06.480582 16736 net.cpp:322] ctx_conv3 needs backward computation.
I0629 19:01:06.480583 16736 net.cpp:322] ctx_conv2/relu needs backward computation.
I0629 19:01:06.480587 16736 net.cpp:322] ctx_conv2/bn needs backward computation.
I0629 19:01:06.480588 16736 net.cpp:322] ctx_conv2 needs backward computation.
I0629 19:01:06.480597 16736 net.cpp:322] ctx_conv1/relu needs backward computation.
I0629 19:01:06.480599 16736 net.cpp:322] ctx_conv1/bn needs backward computation.
I0629 19:01:06.480602 16736 net.cpp:322] ctx_conv1 needs backward computation.
I0629 19:01:06.480603 16736 net.cpp:322] out3_out5_combined needs backward computation.
I0629 19:01:06.480607 16736 net.cpp:322] out3a/relu needs backward computation.
I0629 19:01:06.480608 16736 net.cpp:322] out3a/bn needs backward computation.
I0629 19:01:06.480610 16736 net.cpp:322] out3a needs backward computation.
I0629 19:01:06.480612 16736 net.cpp:322] out5a_up2 needs backward computation.
I0629 19:01:06.480615 16736 net.cpp:322] out5a/relu needs backward computation.
I0629 19:01:06.480618 16736 net.cpp:322] out5a/bn needs backward computation.
I0629 19:01:06.480619 16736 net.cpp:322] out5a needs backward computation.
I0629 19:01:06.480623 16736 net.cpp:322] res5a_branch2b/relu needs backward computation.
I0629 19:01:06.480623 16736 net.cpp:322] res5a_branch2b/bn needs backward computation.
I0629 19:01:06.480626 16736 net.cpp:322] res5a_branch2b needs backward computation.
I0629 19:01:06.480628 16736 net.cpp:322] res5a_branch2a/relu needs backward computation.
I0629 19:01:06.480630 16736 net.cpp:322] res5a_branch2a/bn needs backward computation.
I0629 19:01:06.480633 16736 net.cpp:322] res5a_branch2a needs backward computation.
I0629 19:01:06.480634 16736 net.cpp:322] pool4 needs backward computation.
I0629 19:01:06.480638 16736 net.cpp:322] res4a_branch2b/relu needs backward computation.
I0629 19:01:06.480639 16736 net.cpp:322] res4a_branch2b/bn needs backward computation.
I0629 19:01:06.480641 16736 net.cpp:322] res4a_branch2b needs backward computation.
I0629 19:01:06.480643 16736 net.cpp:322] res4a_branch2a/relu needs backward computation.
I0629 19:01:06.480645 16736 net.cpp:322] res4a_branch2a/bn needs backward computation.
I0629 19:01:06.480648 16736 net.cpp:322] res4a_branch2a needs backward computation.
I0629 19:01:06.480649 16736 net.cpp:322] pool3 needs backward computation.
I0629 19:01:06.480654 16736 net.cpp:322] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I0629 19:01:06.480655 16736 net.cpp:322] res3a_branch2b/relu needs backward computation.
I0629 19:01:06.480657 16736 net.cpp:322] res3a_branch2b/bn needs backward computation.
I0629 19:01:06.480659 16736 net.cpp:322] res3a_branch2b needs backward computation.
I0629 19:01:06.480662 16736 net.cpp:322] res3a_branch2a/relu needs backward computation.
I0629 19:01:06.480664 16736 net.cpp:322] res3a_branch2a/bn needs backward computation.
I0629 19:01:06.480667 16736 net.cpp:322] res3a_branch2a needs backward computation.
I0629 19:01:06.480669 16736 net.cpp:322] pool2 needs backward computation.
I0629 19:01:06.480672 16736 net.cpp:322] res2a_branch2b/relu needs backward computation.
I0629 19:01:06.480674 16736 net.cpp:322] res2a_branch2b/bn needs backward computation.
I0629 19:01:06.480676 16736 net.cpp:322] res2a_branch2b needs backward computation.
I0629 19:01:06.480679 16736 net.cpp:322] res2a_branch2a/relu needs backward computation.
I0629 19:01:06.480680 16736 net.cpp:322] res2a_branch2a/bn needs backward computation.
I0629 19:01:06.480684 16736 net.cpp:322] res2a_branch2a needs backward computation.
I0629 19:01:06.480685 16736 net.cpp:322] pool1 needs backward computation.
I0629 19:01:06.480689 16736 net.cpp:322] conv1b/relu needs backward computation.
I0629 19:01:06.480690 16736 net.cpp:322] conv1b/bn needs backward computation.
I0629 19:01:06.480692 16736 net.cpp:322] conv1b needs backward computation.
I0629 19:01:06.480695 16736 net.cpp:322] conv1a/relu needs backward computation.
I0629 19:01:06.480696 16736 net.cpp:322] conv1a/bn needs backward computation.
I0629 19:01:06.480698 16736 net.cpp:322] conv1a needs backward computation.
I0629 19:01:06.480701 16736 net.cpp:324] data/bias does not need backward computation.
I0629 19:01:06.480703 16736 net.cpp:324] data does not need backward computation.
I0629 19:01:06.480706 16736 net.cpp:366] This network produces output loss
I0629 19:01:06.480795 16736 net.cpp:388] Top memory (TRAIN) required for data: 959692800 diff: 1092403208
I0629 19:01:06.480799 16736 net.cpp:391] Bottom memory (TRAIN) required for data: 959692800 diff: 959692800
I0629 19:01:06.480801 16736 net.cpp:394] Shared (in-place) memory (TRAIN) by data: 632217600 diff: 632217600
I0629 19:01:06.480804 16736 net.cpp:397] Parameters memory (TRAIN) required for data: 2720256 diff: 2720256
I0629 19:01:06.480806 16736 net.cpp:400] Parameters shared memory (TRAIN) by data: 0 diff: 0
I0629 19:01:06.480808 16736 net.cpp:406] Network initialization done.
I0629 19:01:06.481323 16736 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cityscapes20_jsegnet21v2_2017-06-29_16-47-23/sparse/test.prototxt
W0629 19:01:06.481387 16736 parallel.cpp:271] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0629 19:01:06.481566 16736 net.cpp:77] Initializing net from parameters: 
name: "jsegnet21v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/val-image-lmdb"
    label_list_path: "data/val-label-lmdb"
    batch_size: 2
    threads: 1
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 20
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 20
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 20
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
  accuracy_param {
    ignore_label: 255
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
    ignore_label: 255
  }
}
I0629 19:01:06.481703 16736 net.cpp:108] Using FLOAT as default forward math type
I0629 19:01:06.481706 16736 net.cpp:114] Using FLOAT as default backward math type
I0629 19:01:06.481709 16736 layer_factory.hpp:136] Creating layer 'data' of type 'ImageLabelData'
I0629 19:01:06.481711 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.481716 16736 net.cpp:183] Created Layer data (0)
I0629 19:01:06.481719 16736 net.cpp:529] data -> data
I0629 19:01:06.481722 16736 net.cpp:529] data -> label
I0629 19:01:06.481744 16736 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0629 19:01:06.481750 16736 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0629 19:01:06.482717 16815 db_lmdb.cpp:35] Opened lmdb data/val-image-lmdb
I0629 19:01:06.483947 16736 data_layer.cpp:188] ReshapePrefetch 2, 3, 640, 640
I0629 19:01:06.484074 16736 data_layer.cpp:206] Output data size: 2, 3, 640, 640
I0629 19:01:06.484081 16736 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0629 19:01:06.484133 16736 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0629 19:01:06.484141 16736 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0629 19:01:06.484926 16816 data_layer.cpp:188] ReshapePrefetch 2, 3, 640, 640
I0629 19:01:06.484936 16816 data_layer.cpp:206] Output data size: 2, 3, 640, 640
I0629 19:01:06.487320 16817 db_lmdb.cpp:35] Opened lmdb data/val-label-lmdb
I0629 19:01:06.489123 16736 data_layer.cpp:188] ReshapePrefetch 2, 1, 640, 640
I0629 19:01:06.489269 16736 data_layer.cpp:206] Output data size: 2, 1, 640, 640
I0629 19:01:06.489279 16736 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0629 19:01:06.489342 16736 net.cpp:244] Setting up data
I0629 19:01:06.489372 16736 net.cpp:251] TEST Top shape for layer 0 'data' 2 3 640 640 (2457600)
I0629 19:01:06.489387 16736 net.cpp:251] TEST Top shape for layer 0 'data' 2 1 640 640 (819200)
I0629 19:01:06.489400 16736 layer_factory.hpp:136] Creating layer 'label_data_1_split' of type 'Split'
I0629 19:01:06.489413 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.489428 16736 net.cpp:183] Created Layer label_data_1_split (1)
I0629 19:01:06.489439 16736 net.cpp:560] label_data_1_split <- label
I0629 19:01:06.489449 16736 net.cpp:529] label_data_1_split -> label_data_1_split_0
I0629 19:01:06.489460 16736 net.cpp:529] label_data_1_split -> label_data_1_split_1
I0629 19:01:06.489471 16736 net.cpp:529] label_data_1_split -> label_data_1_split_2
I0629 19:01:06.490167 16736 net.cpp:244] Setting up label_data_1_split
I0629 19:01:06.490190 16736 net.cpp:251] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0629 19:01:06.490203 16736 net.cpp:251] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0629 19:01:06.490214 16736 net.cpp:251] TEST Top shape for layer 1 'label_data_1_split' 2 1 640 640 (819200)
I0629 19:01:06.490226 16736 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0629 19:01:06.490238 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.490257 16736 net.cpp:183] Created Layer data/bias (2)
I0629 19:01:06.490269 16736 net.cpp:560] data/bias <- data
I0629 19:01:06.490280 16736 net.cpp:529] data/bias -> data/bias
I0629 19:01:06.490411 16818 data_layer.cpp:188] ReshapePrefetch 2, 1, 640, 640
I0629 19:01:06.490420 16818 data_layer.cpp:206] Output data size: 2, 1, 640, 640
I0629 19:01:06.490878 16816 data_layer.cpp:110] [0] Parser threads: 1
I0629 19:01:06.490887 16816 data_layer.cpp:112] [0] Transformer threads: 1
I0629 19:01:06.493800 16736 net.cpp:244] Setting up data/bias
I0629 19:01:06.493825 16736 net.cpp:251] TEST Top shape for layer 2 'data/bias' 2 3 640 640 (2457600)
I0629 19:01:06.493841 16736 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0629 19:01:06.493863 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.493885 16736 net.cpp:183] Created Layer conv1a (3)
I0629 19:01:06.493891 16736 net.cpp:560] conv1a <- data/bias
I0629 19:01:06.493896 16736 net.cpp:529] conv1a -> conv1a
I0629 19:01:06.494029 16818 data_layer.cpp:110] [0] Parser threads: 1
I0629 19:01:06.494040 16818 data_layer.cpp:112] [0] Transformer threads: 1
I0629 19:01:06.497570 16736 net.cpp:244] Setting up conv1a
I0629 19:01:06.497608 16736 net.cpp:251] TEST Top shape for layer 3 'conv1a' 2 32 320 320 (6553600)
I0629 19:01:06.497627 16736 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0629 19:01:06.497637 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.497660 16736 net.cpp:183] Created Layer conv1a/bn (4)
I0629 19:01:06.497666 16736 net.cpp:560] conv1a/bn <- conv1a
I0629 19:01:06.497674 16736 net.cpp:512] conv1a/bn -> conv1a (in-place)
I0629 19:01:06.499362 16736 net.cpp:244] Setting up conv1a/bn
I0629 19:01:06.499375 16736 net.cpp:251] TEST Top shape for layer 4 'conv1a/bn' 2 32 320 320 (6553600)
I0629 19:01:06.499384 16736 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0629 19:01:06.499389 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.499399 16736 net.cpp:183] Created Layer conv1a/relu (5)
I0629 19:01:06.499402 16736 net.cpp:560] conv1a/relu <- conv1a
I0629 19:01:06.499405 16736 net.cpp:512] conv1a/relu -> conv1a (in-place)
I0629 19:01:06.499413 16736 net.cpp:244] Setting up conv1a/relu
I0629 19:01:06.499418 16736 net.cpp:251] TEST Top shape for layer 5 'conv1a/relu' 2 32 320 320 (6553600)
I0629 19:01:06.499420 16736 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0629 19:01:06.499424 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.499438 16736 net.cpp:183] Created Layer conv1b (6)
I0629 19:01:06.499441 16736 net.cpp:560] conv1b <- conv1a
I0629 19:01:06.499444 16736 net.cpp:529] conv1b -> conv1b
I0629 19:01:06.513859 16736 net.cpp:244] Setting up conv1b
I0629 19:01:06.513870 16736 net.cpp:251] TEST Top shape for layer 6 'conv1b' 2 32 320 320 (6553600)
I0629 19:01:06.513877 16736 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0629 19:01:06.513880 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.513885 16736 net.cpp:183] Created Layer conv1b/bn (7)
I0629 19:01:06.513888 16736 net.cpp:560] conv1b/bn <- conv1b
I0629 19:01:06.513890 16736 net.cpp:512] conv1b/bn -> conv1b (in-place)
I0629 19:01:06.515180 16736 net.cpp:244] Setting up conv1b/bn
I0629 19:01:06.515190 16736 net.cpp:251] TEST Top shape for layer 7 'conv1b/bn' 2 32 320 320 (6553600)
I0629 19:01:06.515197 16736 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0629 19:01:06.515200 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.515204 16736 net.cpp:183] Created Layer conv1b/relu (8)
I0629 19:01:06.515208 16736 net.cpp:560] conv1b/relu <- conv1b
I0629 19:01:06.515209 16736 net.cpp:512] conv1b/relu -> conv1b (in-place)
I0629 19:01:06.515213 16736 net.cpp:244] Setting up conv1b/relu
I0629 19:01:06.515216 16736 net.cpp:251] TEST Top shape for layer 8 'conv1b/relu' 2 32 320 320 (6553600)
I0629 19:01:06.515218 16736 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0629 19:01:06.515220 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.515224 16736 net.cpp:183] Created Layer pool1 (9)
I0629 19:01:06.515233 16736 net.cpp:560] pool1 <- conv1b
I0629 19:01:06.515238 16736 net.cpp:529] pool1 -> pool1
I0629 19:01:06.515316 16736 net.cpp:244] Setting up pool1
I0629 19:01:06.515321 16736 net.cpp:251] TEST Top shape for layer 9 'pool1' 2 32 160 160 (1638400)
I0629 19:01:06.515324 16736 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0629 19:01:06.515336 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.515343 16736 net.cpp:183] Created Layer res2a_branch2a (10)
I0629 19:01:06.515347 16736 net.cpp:560] res2a_branch2a <- pool1
I0629 19:01:06.515349 16736 net.cpp:529] res2a_branch2a -> res2a_branch2a
I0629 19:01:06.523241 16736 net.cpp:244] Setting up res2a_branch2a
I0629 19:01:06.523262 16736 net.cpp:251] TEST Top shape for layer 10 'res2a_branch2a' 2 64 160 160 (3276800)
I0629 19:01:06.523272 16736 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0629 19:01:06.523275 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.523289 16736 net.cpp:183] Created Layer res2a_branch2a/bn (11)
I0629 19:01:06.523293 16736 net.cpp:560] res2a_branch2a/bn <- res2a_branch2a
I0629 19:01:06.523296 16736 net.cpp:512] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0629 19:01:06.524631 16736 net.cpp:244] Setting up res2a_branch2a/bn
I0629 19:01:06.524641 16736 net.cpp:251] TEST Top shape for layer 11 'res2a_branch2a/bn' 2 64 160 160 (3276800)
I0629 19:01:06.524647 16736 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0629 19:01:06.524651 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.524653 16736 net.cpp:183] Created Layer res2a_branch2a/relu (12)
I0629 19:01:06.524655 16736 net.cpp:560] res2a_branch2a/relu <- res2a_branch2a
I0629 19:01:06.524658 16736 net.cpp:512] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0629 19:01:06.524662 16736 net.cpp:244] Setting up res2a_branch2a/relu
I0629 19:01:06.524665 16736 net.cpp:251] TEST Top shape for layer 12 'res2a_branch2a/relu' 2 64 160 160 (3276800)
I0629 19:01:06.524667 16736 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0629 19:01:06.524669 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.524675 16736 net.cpp:183] Created Layer res2a_branch2b (13)
I0629 19:01:06.524678 16736 net.cpp:560] res2a_branch2b <- res2a_branch2a
I0629 19:01:06.524682 16736 net.cpp:529] res2a_branch2b -> res2a_branch2b
I0629 19:01:06.531006 16736 net.cpp:244] Setting up res2a_branch2b
I0629 19:01:06.531061 16736 net.cpp:251] TEST Top shape for layer 13 'res2a_branch2b' 2 64 160 160 (3276800)
I0629 19:01:06.531088 16736 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0629 19:01:06.531106 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.531134 16736 net.cpp:183] Created Layer res2a_branch2b/bn (14)
I0629 19:01:06.531150 16736 net.cpp:560] res2a_branch2b/bn <- res2a_branch2b
I0629 19:01:06.531164 16736 net.cpp:512] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0629 19:01:06.536842 16736 net.cpp:244] Setting up res2a_branch2b/bn
I0629 19:01:06.536890 16736 net.cpp:251] TEST Top shape for layer 14 'res2a_branch2b/bn' 2 64 160 160 (3276800)
I0629 19:01:06.536926 16736 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0629 19:01:06.536944 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.536962 16736 net.cpp:183] Created Layer res2a_branch2b/relu (15)
I0629 19:01:06.536983 16736 net.cpp:560] res2a_branch2b/relu <- res2a_branch2b
I0629 19:01:06.537009 16736 net.cpp:512] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0629 19:01:06.537044 16736 net.cpp:244] Setting up res2a_branch2b/relu
I0629 19:01:06.537071 16736 net.cpp:251] TEST Top shape for layer 15 'res2a_branch2b/relu' 2 64 160 160 (3276800)
I0629 19:01:06.537093 16736 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0629 19:01:06.537117 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.537153 16736 net.cpp:183] Created Layer pool2 (16)
I0629 19:01:06.537217 16736 net.cpp:560] pool2 <- res2a_branch2b
I0629 19:01:06.537243 16736 net.cpp:529] pool2 -> pool2
I0629 19:01:06.537797 16736 net.cpp:244] Setting up pool2
I0629 19:01:06.537838 16736 net.cpp:251] TEST Top shape for layer 16 'pool2' 2 64 80 80 (819200)
I0629 19:01:06.537864 16736 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0629 19:01:06.537889 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.537962 16736 net.cpp:183] Created Layer res3a_branch2a (17)
I0629 19:01:06.537987 16736 net.cpp:560] res3a_branch2a <- pool2
I0629 19:01:06.538012 16736 net.cpp:529] res3a_branch2a -> res3a_branch2a
I0629 19:01:06.553298 16736 net.cpp:244] Setting up res3a_branch2a
I0629 19:01:06.553349 16736 net.cpp:251] TEST Top shape for layer 17 'res3a_branch2a' 2 128 80 80 (1638400)
I0629 19:01:06.553362 16736 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0629 19:01:06.553370 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.553383 16736 net.cpp:183] Created Layer res3a_branch2a/bn (18)
I0629 19:01:06.553390 16736 net.cpp:560] res3a_branch2a/bn <- res3a_branch2a
I0629 19:01:06.553400 16736 net.cpp:512] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0629 19:01:06.556375 16736 net.cpp:244] Setting up res3a_branch2a/bn
I0629 19:01:06.556398 16736 net.cpp:251] TEST Top shape for layer 18 'res3a_branch2a/bn' 2 128 80 80 (1638400)
I0629 19:01:06.556418 16736 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0629 19:01:06.556427 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.556434 16736 net.cpp:183] Created Layer res3a_branch2a/relu (19)
I0629 19:01:06.556442 16736 net.cpp:560] res3a_branch2a/relu <- res3a_branch2a
I0629 19:01:06.556452 16736 net.cpp:512] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0629 19:01:06.556463 16736 net.cpp:244] Setting up res3a_branch2a/relu
I0629 19:01:06.556471 16736 net.cpp:251] TEST Top shape for layer 19 'res3a_branch2a/relu' 2 128 80 80 (1638400)
I0629 19:01:06.556480 16736 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0629 19:01:06.556489 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.556510 16736 net.cpp:183] Created Layer res3a_branch2b (20)
I0629 19:01:06.556524 16736 net.cpp:560] res3a_branch2b <- res3a_branch2a
I0629 19:01:06.556533 16736 net.cpp:529] res3a_branch2b -> res3a_branch2b
I0629 19:01:06.563361 16736 net.cpp:244] Setting up res3a_branch2b
I0629 19:01:06.563380 16736 net.cpp:251] TEST Top shape for layer 20 'res3a_branch2b' 2 128 80 80 (1638400)
I0629 19:01:06.563390 16736 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0629 19:01:06.563396 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.563407 16736 net.cpp:183] Created Layer res3a_branch2b/bn (21)
I0629 19:01:06.563415 16736 net.cpp:560] res3a_branch2b/bn <- res3a_branch2b
I0629 19:01:06.563423 16736 net.cpp:512] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0629 19:01:06.565425 16736 net.cpp:244] Setting up res3a_branch2b/bn
I0629 19:01:06.565443 16736 net.cpp:251] TEST Top shape for layer 21 'res3a_branch2b/bn' 2 128 80 80 (1638400)
I0629 19:01:06.565455 16736 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0629 19:01:06.565464 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.565474 16736 net.cpp:183] Created Layer res3a_branch2b/relu (22)
I0629 19:01:06.565480 16736 net.cpp:560] res3a_branch2b/relu <- res3a_branch2b
I0629 19:01:06.565487 16736 net.cpp:512] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0629 19:01:06.565498 16736 net.cpp:244] Setting up res3a_branch2b/relu
I0629 19:01:06.565507 16736 net.cpp:251] TEST Top shape for layer 22 'res3a_branch2b/relu' 2 128 80 80 (1638400)
I0629 19:01:06.565531 16736 layer_factory.hpp:136] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I0629 19:01:06.565539 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.565548 16736 net.cpp:183] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (23)
I0629 19:01:06.565556 16736 net.cpp:560] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I0629 19:01:06.565562 16736 net.cpp:529] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I0629 19:01:06.565573 16736 net.cpp:529] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I0629 19:01:06.565698 16736 net.cpp:244] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I0629 19:01:06.565709 16736 net.cpp:251] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 2 128 80 80 (1638400)
I0629 19:01:06.565717 16736 net.cpp:251] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 2 128 80 80 (1638400)
I0629 19:01:06.565724 16736 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0629 19:01:06.565731 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.565740 16736 net.cpp:183] Created Layer pool3 (24)
I0629 19:01:06.565748 16736 net.cpp:560] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I0629 19:01:06.565757 16736 net.cpp:529] pool3 -> pool3
I0629 19:01:06.565922 16736 net.cpp:244] Setting up pool3
I0629 19:01:06.565935 16736 net.cpp:251] TEST Top shape for layer 24 'pool3' 2 128 40 40 (409600)
I0629 19:01:06.565943 16736 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0629 19:01:06.565948 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.565963 16736 net.cpp:183] Created Layer res4a_branch2a (25)
I0629 19:01:06.565970 16736 net.cpp:560] res4a_branch2a <- pool3
I0629 19:01:06.565979 16736 net.cpp:529] res4a_branch2a -> res4a_branch2a
I0629 19:01:06.580008 16736 net.cpp:244] Setting up res4a_branch2a
I0629 19:01:06.580027 16736 net.cpp:251] TEST Top shape for layer 25 'res4a_branch2a' 2 256 40 40 (819200)
I0629 19:01:06.580034 16736 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0629 19:01:06.580039 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.580054 16736 net.cpp:183] Created Layer res4a_branch2a/bn (26)
I0629 19:01:06.580058 16736 net.cpp:560] res4a_branch2a/bn <- res4a_branch2a
I0629 19:01:06.580061 16736 net.cpp:512] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0629 19:01:06.581603 16736 net.cpp:244] Setting up res4a_branch2a/bn
I0629 19:01:06.581614 16736 net.cpp:251] TEST Top shape for layer 26 'res4a_branch2a/bn' 2 256 40 40 (819200)
I0629 19:01:06.581621 16736 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0629 19:01:06.581624 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.581629 16736 net.cpp:183] Created Layer res4a_branch2a/relu (27)
I0629 19:01:06.581631 16736 net.cpp:560] res4a_branch2a/relu <- res4a_branch2a
I0629 19:01:06.581634 16736 net.cpp:512] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0629 19:01:06.581645 16736 net.cpp:244] Setting up res4a_branch2a/relu
I0629 19:01:06.581650 16736 net.cpp:251] TEST Top shape for layer 27 'res4a_branch2a/relu' 2 256 40 40 (819200)
I0629 19:01:06.581652 16736 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0629 19:01:06.581656 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.581665 16736 net.cpp:183] Created Layer res4a_branch2b (28)
I0629 19:01:06.581668 16736 net.cpp:560] res4a_branch2b <- res4a_branch2a
I0629 19:01:06.581671 16736 net.cpp:529] res4a_branch2b -> res4a_branch2b
I0629 19:01:06.588737 16736 net.cpp:244] Setting up res4a_branch2b
I0629 19:01:06.588762 16736 net.cpp:251] TEST Top shape for layer 28 'res4a_branch2b' 2 256 40 40 (819200)
I0629 19:01:06.588768 16736 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0629 19:01:06.588771 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.588778 16736 net.cpp:183] Created Layer res4a_branch2b/bn (29)
I0629 19:01:06.588780 16736 net.cpp:560] res4a_branch2b/bn <- res4a_branch2b
I0629 19:01:06.588783 16736 net.cpp:512] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0629 19:01:06.590011 16736 net.cpp:244] Setting up res4a_branch2b/bn
I0629 19:01:06.590020 16736 net.cpp:251] TEST Top shape for layer 29 'res4a_branch2b/bn' 2 256 40 40 (819200)
I0629 19:01:06.590026 16736 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0629 19:01:06.590029 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.590034 16736 net.cpp:183] Created Layer res4a_branch2b/relu (30)
I0629 19:01:06.590035 16736 net.cpp:560] res4a_branch2b/relu <- res4a_branch2b
I0629 19:01:06.590037 16736 net.cpp:512] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0629 19:01:06.590042 16736 net.cpp:244] Setting up res4a_branch2b/relu
I0629 19:01:06.590044 16736 net.cpp:251] TEST Top shape for layer 30 'res4a_branch2b/relu' 2 256 40 40 (819200)
I0629 19:01:06.590047 16736 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0629 19:01:06.590049 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.590054 16736 net.cpp:183] Created Layer pool4 (31)
I0629 19:01:06.590056 16736 net.cpp:560] pool4 <- res4a_branch2b
I0629 19:01:06.590059 16736 net.cpp:529] pool4 -> pool4
I0629 19:01:06.590148 16736 net.cpp:244] Setting up pool4
I0629 19:01:06.590157 16736 net.cpp:251] TEST Top shape for layer 31 'pool4' 2 256 40 40 (819200)
I0629 19:01:06.590160 16736 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0629 19:01:06.590164 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.590179 16736 net.cpp:183] Created Layer res5a_branch2a (32)
I0629 19:01:06.590183 16736 net.cpp:560] res5a_branch2a <- pool4
I0629 19:01:06.590188 16736 net.cpp:529] res5a_branch2a -> res5a_branch2a
I0629 19:01:06.615330 16736 net.cpp:244] Setting up res5a_branch2a
I0629 19:01:06.615355 16736 net.cpp:251] TEST Top shape for layer 32 'res5a_branch2a' 2 512 40 40 (1638400)
I0629 19:01:06.615362 16736 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0629 19:01:06.615366 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.615373 16736 net.cpp:183] Created Layer res5a_branch2a/bn (33)
I0629 19:01:06.615376 16736 net.cpp:560] res5a_branch2a/bn <- res5a_branch2a
I0629 19:01:06.615381 16736 net.cpp:512] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0629 19:01:06.616677 16736 net.cpp:244] Setting up res5a_branch2a/bn
I0629 19:01:06.616686 16736 net.cpp:251] TEST Top shape for layer 33 'res5a_branch2a/bn' 2 512 40 40 (1638400)
I0629 19:01:06.616693 16736 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0629 19:01:06.616695 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.616699 16736 net.cpp:183] Created Layer res5a_branch2a/relu (34)
I0629 19:01:06.616701 16736 net.cpp:560] res5a_branch2a/relu <- res5a_branch2a
I0629 19:01:06.616704 16736 net.cpp:512] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0629 19:01:06.616708 16736 net.cpp:244] Setting up res5a_branch2a/relu
I0629 19:01:06.616711 16736 net.cpp:251] TEST Top shape for layer 34 'res5a_branch2a/relu' 2 512 40 40 (1638400)
I0629 19:01:06.616714 16736 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0629 19:01:06.616715 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.616730 16736 net.cpp:183] Created Layer res5a_branch2b (35)
I0629 19:01:06.616741 16736 net.cpp:560] res5a_branch2b <- res5a_branch2a
I0629 19:01:06.616744 16736 net.cpp:529] res5a_branch2b -> res5a_branch2b
I0629 19:01:06.629807 16736 net.cpp:244] Setting up res5a_branch2b
I0629 19:01:06.629817 16736 net.cpp:251] TEST Top shape for layer 35 'res5a_branch2b' 2 512 40 40 (1638400)
I0629 19:01:06.629824 16736 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0629 19:01:06.629827 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.629832 16736 net.cpp:183] Created Layer res5a_branch2b/bn (36)
I0629 19:01:06.629834 16736 net.cpp:560] res5a_branch2b/bn <- res5a_branch2b
I0629 19:01:06.629837 16736 net.cpp:512] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0629 19:01:06.631059 16736 net.cpp:244] Setting up res5a_branch2b/bn
I0629 19:01:06.631068 16736 net.cpp:251] TEST Top shape for layer 36 'res5a_branch2b/bn' 2 512 40 40 (1638400)
I0629 19:01:06.631074 16736 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0629 19:01:06.631078 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.631080 16736 net.cpp:183] Created Layer res5a_branch2b/relu (37)
I0629 19:01:06.631083 16736 net.cpp:560] res5a_branch2b/relu <- res5a_branch2b
I0629 19:01:06.631085 16736 net.cpp:512] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0629 19:01:06.631089 16736 net.cpp:244] Setting up res5a_branch2b/relu
I0629 19:01:06.631093 16736 net.cpp:251] TEST Top shape for layer 37 'res5a_branch2b/relu' 2 512 40 40 (1638400)
I0629 19:01:06.631094 16736 layer_factory.hpp:136] Creating layer 'out5a' of type 'Convolution'
I0629 19:01:06.631096 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.631106 16736 net.cpp:183] Created Layer out5a (38)
I0629 19:01:06.631109 16736 net.cpp:560] out5a <- res5a_branch2b
I0629 19:01:06.631111 16736 net.cpp:529] out5a -> out5a
I0629 19:01:06.634519 16736 net.cpp:244] Setting up out5a
I0629 19:01:06.634527 16736 net.cpp:251] TEST Top shape for layer 38 'out5a' 2 64 40 40 (204800)
I0629 19:01:06.634532 16736 layer_factory.hpp:136] Creating layer 'out5a/bn' of type 'BatchNorm'
I0629 19:01:06.634534 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.634538 16736 net.cpp:183] Created Layer out5a/bn (39)
I0629 19:01:06.634541 16736 net.cpp:560] out5a/bn <- out5a
I0629 19:01:06.634543 16736 net.cpp:512] out5a/bn -> out5a (in-place)
I0629 19:01:06.635323 16736 net.cpp:244] Setting up out5a/bn
I0629 19:01:06.635331 16736 net.cpp:251] TEST Top shape for layer 39 'out5a/bn' 2 64 40 40 (204800)
I0629 19:01:06.635337 16736 layer_factory.hpp:136] Creating layer 'out5a/relu' of type 'ReLU'
I0629 19:01:06.635339 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.635344 16736 net.cpp:183] Created Layer out5a/relu (40)
I0629 19:01:06.635345 16736 net.cpp:560] out5a/relu <- out5a
I0629 19:01:06.635347 16736 net.cpp:512] out5a/relu -> out5a (in-place)
I0629 19:01:06.635350 16736 net.cpp:244] Setting up out5a/relu
I0629 19:01:06.635354 16736 net.cpp:251] TEST Top shape for layer 40 'out5a/relu' 2 64 40 40 (204800)
I0629 19:01:06.635355 16736 layer_factory.hpp:136] Creating layer 'out5a_up2' of type 'Deconvolution'
I0629 19:01:06.635357 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.635362 16736 net.cpp:183] Created Layer out5a_up2 (41)
I0629 19:01:06.635365 16736 net.cpp:560] out5a_up2 <- out5a
I0629 19:01:06.635366 16736 net.cpp:529] out5a_up2 -> out5a_up2
I0629 19:01:06.635781 16736 net.cpp:244] Setting up out5a_up2
I0629 19:01:06.635788 16736 net.cpp:251] TEST Top shape for layer 41 'out5a_up2' 2 64 80 80 (819200)
I0629 19:01:06.635794 16736 layer_factory.hpp:136] Creating layer 'out3a' of type 'Convolution'
I0629 19:01:06.635797 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.635810 16736 net.cpp:183] Created Layer out3a (42)
I0629 19:01:06.635813 16736 net.cpp:560] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I0629 19:01:06.635817 16736 net.cpp:529] out3a -> out3a
I0629 19:01:06.639339 16736 net.cpp:244] Setting up out3a
I0629 19:01:06.639353 16736 net.cpp:251] TEST Top shape for layer 42 'out3a' 2 64 80 80 (819200)
I0629 19:01:06.639360 16736 layer_factory.hpp:136] Creating layer 'out3a/bn' of type 'BatchNorm'
I0629 19:01:06.639364 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.639370 16736 net.cpp:183] Created Layer out3a/bn (43)
I0629 19:01:06.639374 16736 net.cpp:560] out3a/bn <- out3a
I0629 19:01:06.639375 16736 net.cpp:512] out3a/bn -> out3a (in-place)
I0629 19:01:06.640648 16736 net.cpp:244] Setting up out3a/bn
I0629 19:01:06.640657 16736 net.cpp:251] TEST Top shape for layer 43 'out3a/bn' 2 64 80 80 (819200)
I0629 19:01:06.640664 16736 layer_factory.hpp:136] Creating layer 'out3a/relu' of type 'ReLU'
I0629 19:01:06.640666 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.640671 16736 net.cpp:183] Created Layer out3a/relu (44)
I0629 19:01:06.640672 16736 net.cpp:560] out3a/relu <- out3a
I0629 19:01:06.640676 16736 net.cpp:512] out3a/relu -> out3a (in-place)
I0629 19:01:06.640679 16736 net.cpp:244] Setting up out3a/relu
I0629 19:01:06.640682 16736 net.cpp:251] TEST Top shape for layer 44 'out3a/relu' 2 64 80 80 (819200)
I0629 19:01:06.640684 16736 layer_factory.hpp:136] Creating layer 'out3_out5_combined' of type 'Eltwise'
I0629 19:01:06.640686 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.640691 16736 net.cpp:183] Created Layer out3_out5_combined (45)
I0629 19:01:06.640693 16736 net.cpp:560] out3_out5_combined <- out5a_up2
I0629 19:01:06.640696 16736 net.cpp:560] out3_out5_combined <- out3a
I0629 19:01:06.640698 16736 net.cpp:529] out3_out5_combined -> out3_out5_combined
I0629 19:01:06.641765 16736 net.cpp:244] Setting up out3_out5_combined
I0629 19:01:06.641774 16736 net.cpp:251] TEST Top shape for layer 45 'out3_out5_combined' 2 64 80 80 (819200)
I0629 19:01:06.641777 16736 layer_factory.hpp:136] Creating layer 'ctx_conv1' of type 'Convolution'
I0629 19:01:06.641780 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.641788 16736 net.cpp:183] Created Layer ctx_conv1 (46)
I0629 19:01:06.641790 16736 net.cpp:560] ctx_conv1 <- out3_out5_combined
I0629 19:01:06.641793 16736 net.cpp:529] ctx_conv1 -> ctx_conv1
I0629 19:01:06.645006 16736 net.cpp:244] Setting up ctx_conv1
I0629 19:01:06.645017 16736 net.cpp:251] TEST Top shape for layer 46 'ctx_conv1' 2 64 80 80 (819200)
I0629 19:01:06.645023 16736 layer_factory.hpp:136] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I0629 19:01:06.645026 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.645036 16736 net.cpp:183] Created Layer ctx_conv1/bn (47)
I0629 19:01:06.645040 16736 net.cpp:560] ctx_conv1/bn <- ctx_conv1
I0629 19:01:06.645041 16736 net.cpp:512] ctx_conv1/bn -> ctx_conv1 (in-place)
I0629 19:01:06.646294 16736 net.cpp:244] Setting up ctx_conv1/bn
I0629 19:01:06.646303 16736 net.cpp:251] TEST Top shape for layer 47 'ctx_conv1/bn' 2 64 80 80 (819200)
I0629 19:01:06.646311 16736 layer_factory.hpp:136] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I0629 19:01:06.646312 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.646317 16736 net.cpp:183] Created Layer ctx_conv1/relu (48)
I0629 19:01:06.646318 16736 net.cpp:560] ctx_conv1/relu <- ctx_conv1
I0629 19:01:06.646322 16736 net.cpp:512] ctx_conv1/relu -> ctx_conv1 (in-place)
I0629 19:01:06.646325 16736 net.cpp:244] Setting up ctx_conv1/relu
I0629 19:01:06.646327 16736 net.cpp:251] TEST Top shape for layer 48 'ctx_conv1/relu' 2 64 80 80 (819200)
I0629 19:01:06.646338 16736 layer_factory.hpp:136] Creating layer 'ctx_conv2' of type 'Convolution'
I0629 19:01:06.646342 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.646349 16736 net.cpp:183] Created Layer ctx_conv2 (49)
I0629 19:01:06.646353 16736 net.cpp:560] ctx_conv2 <- ctx_conv1
I0629 19:01:06.646355 16736 net.cpp:529] ctx_conv2 -> ctx_conv2
I0629 19:01:06.647616 16736 net.cpp:244] Setting up ctx_conv2
I0629 19:01:06.647625 16736 net.cpp:251] TEST Top shape for layer 49 'ctx_conv2' 2 64 80 80 (819200)
I0629 19:01:06.647629 16736 layer_factory.hpp:136] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I0629 19:01:06.647632 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.647636 16736 net.cpp:183] Created Layer ctx_conv2/bn (50)
I0629 19:01:06.647639 16736 net.cpp:560] ctx_conv2/bn <- ctx_conv2
I0629 19:01:06.647642 16736 net.cpp:512] ctx_conv2/bn -> ctx_conv2 (in-place)
I0629 19:01:06.648856 16736 net.cpp:244] Setting up ctx_conv2/bn
I0629 19:01:06.648865 16736 net.cpp:251] TEST Top shape for layer 50 'ctx_conv2/bn' 2 64 80 80 (819200)
I0629 19:01:06.648871 16736 layer_factory.hpp:136] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I0629 19:01:06.648874 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.648876 16736 net.cpp:183] Created Layer ctx_conv2/relu (51)
I0629 19:01:06.648880 16736 net.cpp:560] ctx_conv2/relu <- ctx_conv2
I0629 19:01:06.648881 16736 net.cpp:512] ctx_conv2/relu -> ctx_conv2 (in-place)
I0629 19:01:06.648885 16736 net.cpp:244] Setting up ctx_conv2/relu
I0629 19:01:06.648887 16736 net.cpp:251] TEST Top shape for layer 51 'ctx_conv2/relu' 2 64 80 80 (819200)
I0629 19:01:06.648890 16736 layer_factory.hpp:136] Creating layer 'ctx_conv3' of type 'Convolution'
I0629 19:01:06.648891 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.648896 16736 net.cpp:183] Created Layer ctx_conv3 (52)
I0629 19:01:06.648900 16736 net.cpp:560] ctx_conv3 <- ctx_conv2
I0629 19:01:06.648901 16736 net.cpp:529] ctx_conv3 -> ctx_conv3
I0629 19:01:06.650133 16736 net.cpp:244] Setting up ctx_conv3
I0629 19:01:06.650141 16736 net.cpp:251] TEST Top shape for layer 52 'ctx_conv3' 2 64 80 80 (819200)
I0629 19:01:06.650146 16736 layer_factory.hpp:136] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I0629 19:01:06.650148 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.650152 16736 net.cpp:183] Created Layer ctx_conv3/bn (53)
I0629 19:01:06.650154 16736 net.cpp:560] ctx_conv3/bn <- ctx_conv3
I0629 19:01:06.650157 16736 net.cpp:512] ctx_conv3/bn -> ctx_conv3 (in-place)
I0629 19:01:06.651378 16736 net.cpp:244] Setting up ctx_conv3/bn
I0629 19:01:06.651387 16736 net.cpp:251] TEST Top shape for layer 53 'ctx_conv3/bn' 2 64 80 80 (819200)
I0629 19:01:06.651393 16736 layer_factory.hpp:136] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I0629 19:01:06.651396 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.651399 16736 net.cpp:183] Created Layer ctx_conv3/relu (54)
I0629 19:01:06.651401 16736 net.cpp:560] ctx_conv3/relu <- ctx_conv3
I0629 19:01:06.651403 16736 net.cpp:512] ctx_conv3/relu -> ctx_conv3 (in-place)
I0629 19:01:06.651407 16736 net.cpp:244] Setting up ctx_conv3/relu
I0629 19:01:06.651410 16736 net.cpp:251] TEST Top shape for layer 54 'ctx_conv3/relu' 2 64 80 80 (819200)
I0629 19:01:06.651412 16736 layer_factory.hpp:136] Creating layer 'ctx_conv4' of type 'Convolution'
I0629 19:01:06.651414 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.651419 16736 net.cpp:183] Created Layer ctx_conv4 (55)
I0629 19:01:06.651422 16736 net.cpp:560] ctx_conv4 <- ctx_conv3
I0629 19:01:06.651424 16736 net.cpp:529] ctx_conv4 -> ctx_conv4
I0629 19:01:06.652652 16736 net.cpp:244] Setting up ctx_conv4
I0629 19:01:06.652667 16736 net.cpp:251] TEST Top shape for layer 55 'ctx_conv4' 2 64 80 80 (819200)
I0629 19:01:06.652671 16736 layer_factory.hpp:136] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I0629 19:01:06.652674 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.652678 16736 net.cpp:183] Created Layer ctx_conv4/bn (56)
I0629 19:01:06.652680 16736 net.cpp:560] ctx_conv4/bn <- ctx_conv4
I0629 19:01:06.652683 16736 net.cpp:512] ctx_conv4/bn -> ctx_conv4 (in-place)
I0629 19:01:06.653897 16736 net.cpp:244] Setting up ctx_conv4/bn
I0629 19:01:06.653905 16736 net.cpp:251] TEST Top shape for layer 56 'ctx_conv4/bn' 2 64 80 80 (819200)
I0629 19:01:06.653911 16736 layer_factory.hpp:136] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I0629 19:01:06.653913 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.653916 16736 net.cpp:183] Created Layer ctx_conv4/relu (57)
I0629 19:01:06.653919 16736 net.cpp:560] ctx_conv4/relu <- ctx_conv4
I0629 19:01:06.653921 16736 net.cpp:512] ctx_conv4/relu -> ctx_conv4 (in-place)
I0629 19:01:06.653925 16736 net.cpp:244] Setting up ctx_conv4/relu
I0629 19:01:06.653928 16736 net.cpp:251] TEST Top shape for layer 57 'ctx_conv4/relu' 2 64 80 80 (819200)
I0629 19:01:06.653929 16736 layer_factory.hpp:136] Creating layer 'ctx_final' of type 'Convolution'
I0629 19:01:06.653931 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.653937 16736 net.cpp:183] Created Layer ctx_final (58)
I0629 19:01:06.653939 16736 net.cpp:560] ctx_final <- ctx_conv4
I0629 19:01:06.653942 16736 net.cpp:529] ctx_final -> ctx_final
I0629 19:01:06.658758 16736 net.cpp:244] Setting up ctx_final
I0629 19:01:06.658768 16736 net.cpp:251] TEST Top shape for layer 58 'ctx_final' 2 20 80 80 (256000)
I0629 19:01:06.658773 16736 layer_factory.hpp:136] Creating layer 'ctx_final/relu' of type 'ReLU'
I0629 19:01:06.658776 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.658779 16736 net.cpp:183] Created Layer ctx_final/relu (59)
I0629 19:01:06.658782 16736 net.cpp:560] ctx_final/relu <- ctx_final
I0629 19:01:06.658784 16736 net.cpp:512] ctx_final/relu -> ctx_final (in-place)
I0629 19:01:06.658788 16736 net.cpp:244] Setting up ctx_final/relu
I0629 19:01:06.658790 16736 net.cpp:251] TEST Top shape for layer 59 'ctx_final/relu' 2 20 80 80 (256000)
I0629 19:01:06.658792 16736 layer_factory.hpp:136] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I0629 19:01:06.658794 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.658804 16736 net.cpp:183] Created Layer out_deconv_final_up2 (60)
I0629 19:01:06.658807 16736 net.cpp:560] out_deconv_final_up2 <- ctx_final
I0629 19:01:06.658809 16736 net.cpp:529] out_deconv_final_up2 -> out_deconv_final_up2
I0629 19:01:06.659250 16736 net.cpp:244] Setting up out_deconv_final_up2
I0629 19:01:06.659258 16736 net.cpp:251] TEST Top shape for layer 60 'out_deconv_final_up2' 2 20 160 160 (1024000)
I0629 19:01:06.659262 16736 layer_factory.hpp:136] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I0629 19:01:06.659265 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.659270 16736 net.cpp:183] Created Layer out_deconv_final_up4 (61)
I0629 19:01:06.659271 16736 net.cpp:560] out_deconv_final_up4 <- out_deconv_final_up2
I0629 19:01:06.659274 16736 net.cpp:529] out_deconv_final_up4 -> out_deconv_final_up4
I0629 19:01:06.659603 16736 net.cpp:244] Setting up out_deconv_final_up4
I0629 19:01:06.659610 16736 net.cpp:251] TEST Top shape for layer 61 'out_deconv_final_up4' 2 20 320 320 (4096000)
I0629 19:01:06.659613 16736 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I0629 19:01:06.659616 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.659624 16736 net.cpp:183] Created Layer out_deconv_final_up8 (62)
I0629 19:01:06.659633 16736 net.cpp:560] out_deconv_final_up8 <- out_deconv_final_up4
I0629 19:01:06.659636 16736 net.cpp:529] out_deconv_final_up8 -> out_deconv_final_up8
I0629 19:01:06.659976 16736 net.cpp:244] Setting up out_deconv_final_up8
I0629 19:01:06.659983 16736 net.cpp:251] TEST Top shape for layer 62 'out_deconv_final_up8' 2 20 640 640 (16384000)
I0629 19:01:06.659986 16736 layer_factory.hpp:136] Creating layer 'out_deconv_final_up8_out_deconv_final_up8_0_split' of type 'Split'
I0629 19:01:06.659989 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.659992 16736 net.cpp:183] Created Layer out_deconv_final_up8_out_deconv_final_up8_0_split (63)
I0629 19:01:06.659994 16736 net.cpp:560] out_deconv_final_up8_out_deconv_final_up8_0_split <- out_deconv_final_up8
I0629 19:01:06.659996 16736 net.cpp:529] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0629 19:01:06.660001 16736 net.cpp:529] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0629 19:01:06.660003 16736 net.cpp:529] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0629 19:01:06.660068 16736 net.cpp:244] Setting up out_deconv_final_up8_out_deconv_final_up8_0_split
I0629 19:01:06.660071 16736 net.cpp:251] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 20 640 640 (16384000)
I0629 19:01:06.660074 16736 net.cpp:251] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 20 640 640 (16384000)
I0629 19:01:06.660078 16736 net.cpp:251] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 2 20 640 640 (16384000)
I0629 19:01:06.660080 16736 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0629 19:01:06.660084 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.660091 16736 net.cpp:183] Created Layer loss (64)
I0629 19:01:06.660094 16736 net.cpp:560] loss <- out_deconv_final_up8_out_deconv_final_up8_0_split_0
I0629 19:01:06.660099 16736 net.cpp:560] loss <- label_data_1_split_0
I0629 19:01:06.660102 16736 net.cpp:529] loss -> loss
I0629 19:01:06.661422 16736 net.cpp:244] Setting up loss
I0629 19:01:06.661433 16736 net.cpp:251] TEST Top shape for layer 64 'loss' (1)
I0629 19:01:06.661437 16736 net.cpp:255]     with loss weight 1
I0629 19:01:06.661445 16736 layer_factory.hpp:136] Creating layer 'accuracy/top1' of type 'Accuracy'
I0629 19:01:06.661448 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.661458 16736 net.cpp:183] Created Layer accuracy/top1 (65)
I0629 19:01:06.661463 16736 net.cpp:560] accuracy/top1 <- out_deconv_final_up8_out_deconv_final_up8_0_split_1
I0629 19:01:06.661466 16736 net.cpp:560] accuracy/top1 <- label_data_1_split_1
I0629 19:01:06.661471 16736 net.cpp:529] accuracy/top1 -> accuracy/top1
I0629 19:01:06.661478 16736 net.cpp:244] Setting up accuracy/top1
I0629 19:01:06.661483 16736 net.cpp:251] TEST Top shape for layer 65 'accuracy/top1' (1)
I0629 19:01:06.661486 16736 layer_factory.hpp:136] Creating layer 'accuracy/top5' of type 'Accuracy'
I0629 19:01:06.661489 16736 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0629 19:01:06.661494 16736 net.cpp:183] Created Layer accuracy/top5 (66)
I0629 19:01:06.661497 16736 net.cpp:560] accuracy/top5 <- out_deconv_final_up8_out_deconv_final_up8_0_split_2
I0629 19:01:06.661500 16736 net.cpp:560] accuracy/top5 <- label_data_1_split_2
I0629 19:01:06.661504 16736 net.cpp:529] accuracy/top5 -> accuracy/top5
I0629 19:01:06.661509 16736 net.cpp:244] Setting up accuracy/top5
I0629 19:01:06.661514 16736 net.cpp:251] TEST Top shape for layer 66 'accuracy/top5' (1)
I0629 19:01:06.661516 16736 net.cpp:324] accuracy/top5 does not need backward computation.
I0629 19:01:06.661520 16736 net.cpp:324] accuracy/top1 does not need backward computation.
I0629 19:01:06.661530 16736 net.cpp:322] loss needs backward computation.
I0629 19:01:06.661535 16736 net.cpp:322] out_deconv_final_up8_out_deconv_final_up8_0_split needs backward computation.
I0629 19:01:06.661538 16736 net.cpp:322] out_deconv_final_up8 needs backward computation.
I0629 19:01:06.661542 16736 net.cpp:322] out_deconv_final_up4 needs backward computation.
I0629 19:01:06.661546 16736 net.cpp:322] out_deconv_final_up2 needs backward computation.
I0629 19:01:06.661550 16736 net.cpp:322] ctx_final/relu needs backward computation.
I0629 19:01:06.661553 16736 net.cpp:322] ctx_final needs backward computation.
I0629 19:01:06.661556 16736 net.cpp:322] ctx_conv4/relu needs backward computation.
I0629 19:01:06.661559 16736 net.cpp:322] ctx_conv4/bn needs backward computation.
I0629 19:01:06.661563 16736 net.cpp:322] ctx_conv4 needs backward computation.
I0629 19:01:06.661566 16736 net.cpp:322] ctx_conv3/relu needs backward computation.
I0629 19:01:06.661569 16736 net.cpp:322] ctx_conv3/bn needs backward computation.
I0629 19:01:06.661573 16736 net.cpp:322] ctx_conv3 needs backward computation.
I0629 19:01:06.661577 16736 net.cpp:322] ctx_conv2/relu needs backward computation.
I0629 19:01:06.661581 16736 net.cpp:322] ctx_conv2/bn needs backward computation.
I0629 19:01:06.661584 16736 net.cpp:322] ctx_conv2 needs backward computation.
I0629 19:01:06.661588 16736 net.cpp:322] ctx_conv1/relu needs backward computation.
I0629 19:01:06.661592 16736 net.cpp:322] ctx_conv1/bn needs backward computation.
I0629 19:01:06.661595 16736 net.cpp:322] ctx_conv1 needs backward computation.
I0629 19:01:06.661599 16736 net.cpp:322] out3_out5_combined needs backward computation.
I0629 19:01:06.661604 16736 net.cpp:322] out3a/relu needs backward computation.
I0629 19:01:06.661608 16736 net.cpp:322] out3a/bn needs backward computation.
I0629 19:01:06.661612 16736 net.cpp:322] out3a needs backward computation.
I0629 19:01:06.661617 16736 net.cpp:322] out5a_up2 needs backward computation.
I0629 19:01:06.661620 16736 net.cpp:322] out5a/relu needs backward computation.
I0629 19:01:06.661624 16736 net.cpp:322] out5a/bn needs backward computation.
I0629 19:01:06.661628 16736 net.cpp:322] out5a needs backward computation.
I0629 19:01:06.661633 16736 net.cpp:322] res5a_branch2b/relu needs backward computation.
I0629 19:01:06.661636 16736 net.cpp:322] res5a_branch2b/bn needs backward computation.
I0629 19:01:06.661640 16736 net.cpp:322] res5a_branch2b needs backward computation.
I0629 19:01:06.661643 16736 net.cpp:322] res5a_branch2a/relu needs backward computation.
I0629 19:01:06.661648 16736 net.cpp:322] res5a_branch2a/bn needs backward computation.
I0629 19:01:06.661651 16736 net.cpp:322] res5a_branch2a needs backward computation.
I0629 19:01:06.661655 16736 net.cpp:322] pool4 needs backward computation.
I0629 19:01:06.661659 16736 net.cpp:322] res4a_branch2b/relu needs backward computation.
I0629 19:01:06.661664 16736 net.cpp:322] res4a_branch2b/bn needs backward computation.
I0629 19:01:06.661667 16736 net.cpp:322] res4a_branch2b needs backward computation.
I0629 19:01:06.661671 16736 net.cpp:322] res4a_branch2a/relu needs backward computation.
I0629 19:01:06.661675 16736 net.cpp:322] res4a_branch2a/bn needs backward computation.
I0629 19:01:06.661679 16736 net.cpp:322] res4a_branch2a needs backward computation.
I0629 19:01:06.661684 16736 net.cpp:322] pool3 needs backward computation.
I0629 19:01:06.661687 16736 net.cpp:322] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I0629 19:01:06.661691 16736 net.cpp:322] res3a_branch2b/relu needs backward computation.
I0629 19:01:06.661695 16736 net.cpp:322] res3a_branch2b/bn needs backward computation.
I0629 19:01:06.661700 16736 net.cpp:322] res3a_branch2b needs backward computation.
I0629 19:01:06.661703 16736 net.cpp:322] res3a_branch2a/relu needs backward computation.
I0629 19:01:06.661707 16736 net.cpp:322] res3a_branch2a/bn needs backward computation.
I0629 19:01:06.661711 16736 net.cpp:322] res3a_branch2a needs backward computation.
I0629 19:01:06.661720 16736 net.cpp:322] pool2 needs backward computation.
I0629 19:01:06.661723 16736 net.cpp:322] res2a_branch2b/relu needs backward computation.
I0629 19:01:06.661727 16736 net.cpp:322] res2a_branch2b/bn needs backward computation.
I0629 19:01:06.661731 16736 net.cpp:322] res2a_branch2b needs backward computation.
I0629 19:01:06.661736 16736 net.cpp:322] res2a_branch2a/relu needs backward computation.
I0629 19:01:06.661739 16736 net.cpp:322] res2a_branch2a/bn needs backward computation.
I0629 19:01:06.661743 16736 net.cpp:322] res2a_branch2a needs backward computation.
I0629 19:01:06.661747 16736 net.cpp:322] pool1 needs backward computation.
I0629 19:01:06.661751 16736 net.cpp:322] conv1b/relu needs backward computation.
I0629 19:01:06.661756 16736 net.cpp:322] conv1b/bn needs backward computation.
I0629 19:01:06.661759 16736 net.cpp:322] conv1b needs backward computation.
I0629 19:01:06.661763 16736 net.cpp:322] conv1a/relu needs backward computation.
I0629 19:01:06.661767 16736 net.cpp:322] conv1a/bn needs backward computation.
I0629 19:01:06.661770 16736 net.cpp:322] conv1a needs backward computation.
I0629 19:01:06.661775 16736 net.cpp:324] data/bias does not need backward computation.
I0629 19:01:06.661780 16736 net.cpp:324] label_data_1_split does not need backward computation.
I0629 19:01:06.661785 16736 net.cpp:324] data does not need backward computation.
I0629 19:01:06.661789 16736 net.cpp:366] This network produces output accuracy/top1
I0629 19:01:06.661793 16736 net.cpp:366] This network produces output accuracy/top5
I0629 19:01:06.661798 16736 net.cpp:366] This network produces output loss
I0629 19:01:06.661856 16736 net.cpp:388] Top memory (TEST) required for data: 319897600 diff: 257228808
I0629 19:01:06.661861 16736 net.cpp:391] Bottom memory (TEST) required for data: 319897600 diff: 319897600
I0629 19:01:06.661865 16736 net.cpp:394] Shared (in-place) memory (TEST) by data: 210739200 diff: 210739200
I0629 19:01:06.661870 16736 net.cpp:397] Parameters memory (TEST) required for data: 2720256 diff: 2720256
I0629 19:01:06.661873 16736 net.cpp:400] Parameters shared memory (TEST) by data: 0 diff: 0
I0629 19:01:06.661877 16736 net.cpp:406] Network initialization done.
I0629 19:01:06.661985 16736 solver.cpp:56] Solver scaffolding done.
I0629 19:01:06.672312 16736 caffe.cpp:137] Finetuning from training/cityscapes20_jsegnet21v2_2017-06-29_16-47-23/initial/cityscapes20_jsegnet21v2_iter_32000.caffemodel
I0629 19:01:06.677297 16736 net.cpp:1087] Copying source layer data Type:ImageLabelData #blobs=0
I0629 19:01:06.677317 16736 net.cpp:1087] Copying source layer data/bias Type:Bias #blobs=1
I0629 19:01:06.677348 16736 net.cpp:1087] Copying source layer conv1a Type:Convolution #blobs=2
I0629 19:01:06.677361 16736 net.cpp:1087] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0629 19:01:06.677920 16736 net.cpp:1087] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0629 19:01:06.677928 16736 net.cpp:1087] Copying source layer conv1b Type:Convolution #blobs=2
I0629 19:01:06.677937 16736 net.cpp:1087] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0629 19:01:06.678351 16736 net.cpp:1087] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0629 19:01:06.678359 16736 net.cpp:1087] Copying source layer pool1 Type:Pooling #blobs=0
I0629 19:01:06.678360 16736 net.cpp:1087] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0629 19:01:06.678375 16736 net.cpp:1087] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0629 19:01:06.678813 16736 net.cpp:1087] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0629 19:01:06.678820 16736 net.cpp:1087] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0629 19:01:06.678838 16736 net.cpp:1087] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0629 19:01:06.679252 16736 net.cpp:1087] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0629 19:01:06.679260 16736 net.cpp:1087] Copying source layer pool2 Type:Pooling #blobs=0
I0629 19:01:06.679261 16736 net.cpp:1087] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0629 19:01:06.679311 16736 net.cpp:1087] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0629 19:01:06.679715 16736 net.cpp:1087] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0629 19:01:06.679723 16736 net.cpp:1087] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0629 19:01:06.679745 16736 net.cpp:1087] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0629 19:01:06.680126 16736 net.cpp:1087] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0629 19:01:06.680132 16736 net.cpp:1087] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I0629 19:01:06.680135 16736 net.cpp:1087] Copying source layer pool3 Type:Pooling #blobs=0
I0629 19:01:06.680137 16736 net.cpp:1087] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0629 19:01:06.680255 16736 net.cpp:1087] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0629 19:01:06.680649 16736 net.cpp:1087] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0629 19:01:06.680656 16736 net.cpp:1087] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0629 19:01:06.680713 16736 net.cpp:1087] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0629 19:01:06.681094 16736 net.cpp:1087] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0629 19:01:06.681102 16736 net.cpp:1087] Copying source layer pool4 Type:Pooling #blobs=0
I0629 19:01:06.681103 16736 net.cpp:1087] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0629 19:01:06.681474 16736 net.cpp:1087] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0629 19:01:06.681843 16736 net.cpp:1087] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0629 19:01:06.681849 16736 net.cpp:1087] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0629 19:01:06.682006 16736 net.cpp:1087] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0629 19:01:06.682366 16736 net.cpp:1087] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0629 19:01:06.682373 16736 net.cpp:1087] Copying source layer out5a Type:Convolution #blobs=2
I0629 19:01:06.682420 16736 net.cpp:1087] Copying source layer out5a/bn Type:BatchNorm #blobs=5
I0629 19:01:06.682602 16736 net.cpp:1087] Copying source layer out5a/relu Type:ReLU #blobs=0
I0629 19:01:06.682608 16736 net.cpp:1087] Copying source layer out5a_up2 Type:Deconvolution #blobs=1
I0629 19:01:06.682613 16736 net.cpp:1087] Copying source layer out3a Type:Convolution #blobs=2
I0629 19:01:06.682631 16736 net.cpp:1087] Copying source layer out3a/bn Type:BatchNorm #blobs=5
I0629 19:01:06.682829 16736 net.cpp:1087] Copying source layer out3a/relu Type:ReLU #blobs=0
I0629 19:01:06.682838 16736 net.cpp:1087] Copying source layer out3_out5_combined Type:Eltwise #blobs=0
I0629 19:01:06.682842 16736 net.cpp:1087] Copying source layer ctx_conv1 Type:Convolution #blobs=2
I0629 19:01:06.682862 16736 net.cpp:1087] Copying source layer ctx_conv1/bn Type:BatchNorm #blobs=5
I0629 19:01:06.683027 16736 net.cpp:1087] Copying source layer ctx_conv1/relu Type:ReLU #blobs=0
I0629 19:01:06.683032 16736 net.cpp:1087] Copying source layer ctx_conv2 Type:Convolution #blobs=2
I0629 19:01:06.683050 16736 net.cpp:1087] Copying source layer ctx_conv2/bn Type:BatchNorm #blobs=5
I0629 19:01:06.683238 16736 net.cpp:1087] Copying source layer ctx_conv2/relu Type:ReLU #blobs=0
I0629 19:01:06.683244 16736 net.cpp:1087] Copying source layer ctx_conv3 Type:Convolution #blobs=2
I0629 19:01:06.683269 16736 net.cpp:1087] Copying source layer ctx_conv3/bn Type:BatchNorm #blobs=5
I0629 19:01:06.683470 16736 net.cpp:1087] Copying source layer ctx_conv3/relu Type:ReLU #blobs=0
I0629 19:01:06.683476 16736 net.cpp:1087] Copying source layer ctx_conv4 Type:Convolution #blobs=2
I0629 19:01:06.683496 16736 net.cpp:1087] Copying source layer ctx_conv4/bn Type:BatchNorm #blobs=5
I0629 19:01:06.683701 16736 net.cpp:1087] Copying source layer ctx_conv4/relu Type:ReLU #blobs=0
I0629 19:01:06.683717 16736 net.cpp:1087] Copying source layer ctx_final Type:Convolution #blobs=2
I0629 19:01:06.683732 16736 net.cpp:1087] Copying source layer ctx_final/relu Type:ReLU #blobs=0
I0629 19:01:06.683735 16736 net.cpp:1087] Copying source layer out_deconv_final_up2 Type:Deconvolution #blobs=1
I0629 19:01:06.683743 16736 net.cpp:1087] Copying source layer out_deconv_final_up4 Type:Deconvolution #blobs=1
I0629 19:01:06.683748 16736 net.cpp:1087] Copying source layer out_deconv_final_up8 Type:Deconvolution #blobs=1
I0629 19:01:06.683754 16736 net.cpp:1087] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0629 19:01:06.686868 16736 net.cpp:1087] Copying source layer data Type:ImageLabelData #blobs=0
I0629 19:01:06.686887 16736 net.cpp:1087] Copying source layer data/bias Type:Bias #blobs=1
I0629 19:01:06.686913 16736 net.cpp:1087] Copying source layer conv1a Type:Convolution #blobs=2
I0629 19:01:06.686923 16736 net.cpp:1087] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0629 19:01:06.687486 16736 net.cpp:1087] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0629 19:01:06.687494 16736 net.cpp:1087] Copying source layer conv1b Type:Convolution #blobs=2
I0629 19:01:06.687502 16736 net.cpp:1087] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0629 19:01:06.687922 16736 net.cpp:1087] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0629 19:01:06.687928 16736 net.cpp:1087] Copying source layer pool1 Type:Pooling #blobs=0
I0629 19:01:06.687930 16736 net.cpp:1087] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0629 19:01:06.687947 16736 net.cpp:1087] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0629 19:01:06.688374 16736 net.cpp:1087] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0629 19:01:06.688380 16736 net.cpp:1087] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0629 19:01:06.688392 16736 net.cpp:1087] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0629 19:01:06.688812 16736 net.cpp:1087] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0629 19:01:06.688817 16736 net.cpp:1087] Copying source layer pool2 Type:Pooling #blobs=0
I0629 19:01:06.688820 16736 net.cpp:1087] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0629 19:01:06.688858 16736 net.cpp:1087] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0629 19:01:06.689266 16736 net.cpp:1087] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0629 19:01:06.689273 16736 net.cpp:1087] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0629 19:01:06.689296 16736 net.cpp:1087] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0629 19:01:06.689687 16736 net.cpp:1087] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0629 19:01:06.689694 16736 net.cpp:1087] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I0629 19:01:06.689697 16736 net.cpp:1087] Copying source layer pool3 Type:Pooling #blobs=0
I0629 19:01:06.689698 16736 net.cpp:1087] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0629 19:01:06.689812 16736 net.cpp:1087] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0629 19:01:06.690218 16736 net.cpp:1087] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0629 19:01:06.690224 16736 net.cpp:1087] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0629 19:01:06.690284 16736 net.cpp:1087] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0629 19:01:06.690677 16736 net.cpp:1087] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0629 19:01:06.690683 16736 net.cpp:1087] Copying source layer pool4 Type:Pooling #blobs=0
I0629 19:01:06.690686 16736 net.cpp:1087] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0629 19:01:06.691061 16736 net.cpp:1087] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0629 19:01:06.691444 16736 net.cpp:1087] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0629 19:01:06.691450 16736 net.cpp:1087] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0629 19:01:06.691639 16736 net.cpp:1087] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0629 19:01:06.692018 16736 net.cpp:1087] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0629 19:01:06.692023 16736 net.cpp:1087] Copying source layer out5a Type:Convolution #blobs=2
I0629 19:01:06.692067 16736 net.cpp:1087] Copying source layer out5a/bn Type:BatchNorm #blobs=5
I0629 19:01:06.692265 16736 net.cpp:1087] Copying source layer out5a/relu Type:ReLU #blobs=0
I0629 19:01:06.692271 16736 net.cpp:1087] Copying source layer out5a_up2 Type:Deconvolution #blobs=1
I0629 19:01:06.692276 16736 net.cpp:1087] Copying source layer out3a Type:Convolution #blobs=2
I0629 19:01:06.692293 16736 net.cpp:1087] Copying source layer out3a/bn Type:BatchNorm #blobs=5
I0629 19:01:06.692481 16736 net.cpp:1087] Copying source layer out3a/relu Type:ReLU #blobs=0
I0629 19:01:06.692487 16736 net.cpp:1087] Copying source layer out3_out5_combined Type:Eltwise #blobs=0
I0629 19:01:06.692490 16736 net.cpp:1087] Copying source layer ctx_conv1 Type:Convolution #blobs=2
I0629 19:01:06.692510 16736 net.cpp:1087] Copying source layer ctx_conv1/bn Type:BatchNorm #blobs=5
I0629 19:01:06.692699 16736 net.cpp:1087] Copying source layer ctx_conv1/relu Type:ReLU #blobs=0
I0629 19:01:06.692705 16736 net.cpp:1087] Copying source layer ctx_conv2 Type:Convolution #blobs=2
I0629 19:01:06.692723 16736 net.cpp:1087] Copying source layer ctx_conv2/bn Type:BatchNorm #blobs=5
I0629 19:01:06.692904 16736 net.cpp:1087] Copying source layer ctx_conv2/relu Type:ReLU #blobs=0
I0629 19:01:06.692909 16736 net.cpp:1087] Copying source layer ctx_conv3 Type:Convolution #blobs=2
I0629 19:01:06.692929 16736 net.cpp:1087] Copying source layer ctx_conv3/bn Type:BatchNorm #blobs=5
I0629 19:01:06.693110 16736 net.cpp:1087] Copying source layer ctx_conv3/relu Type:ReLU #blobs=0
I0629 19:01:06.693116 16736 net.cpp:1087] Copying source layer ctx_conv4 Type:Convolution #blobs=2
I0629 19:01:06.693136 16736 net.cpp:1087] Copying source layer ctx_conv4/bn Type:BatchNorm #blobs=5
I0629 19:01:06.693315 16736 net.cpp:1087] Copying source layer ctx_conv4/relu Type:ReLU #blobs=0
I0629 19:01:06.693321 16736 net.cpp:1087] Copying source layer ctx_final Type:Convolution #blobs=2
I0629 19:01:06.693332 16736 net.cpp:1087] Copying source layer ctx_final/relu Type:ReLU #blobs=0
I0629 19:01:06.693334 16736 net.cpp:1087] Copying source layer out_deconv_final_up2 Type:Deconvolution #blobs=1
I0629 19:01:06.693339 16736 net.cpp:1087] Copying source layer out_deconv_final_up4 Type:Deconvolution #blobs=1
I0629 19:01:06.693344 16736 net.cpp:1087] Copying source layer out_deconv_final_up8 Type:Deconvolution #blobs=1
I0629 19:01:06.693348 16736 net.cpp:1087] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0629 19:01:06.693436 16736 parallel.cpp:106] [0 - 0] P2pSync adding callback
I0629 19:01:06.693444 16736 parallel.cpp:106] [1 - 1] P2pSync adding callback
I0629 19:01:06.693447 16736 parallel.cpp:106] [2 - 2] P2pSync adding callback
I0629 19:01:06.693450 16736 parallel.cpp:59] Starting Optimization
I0629 19:01:06.693454 16736 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0629 19:01:06.693486 16736 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0629 19:01:06.693500 16736 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0629 19:01:06.694152 16819 device_alternate.hpp:116] NVML initialized on thread 135942519715584
I0629 19:01:06.711192 16819 common.cpp:563] NVML succeeded to set CPU affinity on device 0
I0629 19:01:06.711216 16820 device_alternate.hpp:116] NVML initialized on thread 135942511322880
I0629 19:01:06.712362 16820 common.cpp:563] NVML succeeded to set CPU affinity on device 1
I0629 19:01:06.712410 16821 device_alternate.hpp:116] NVML initialized on thread 135942502930176
I0629 19:01:06.713201 16821 common.cpp:563] NVML succeeded to set CPU affinity on device 2
I0629 19:01:06.716886 16820 solver.cpp:42] Solver data type: FLOAT
W0629 19:01:06.717437 16820 parallel.cpp:271] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0629 19:01:06.717543 16820 net.cpp:108] Using FLOAT as default forward math type
I0629 19:01:06.717548 16820 net.cpp:114] Using FLOAT as default backward math type
I0629 19:01:06.717579 16820 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0629 19:01:06.717586 16820 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0629 19:01:06.720751 16821 solver.cpp:42] Solver data type: FLOAT
W0629 19:01:06.721264 16821 parallel.cpp:271] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 16 to 18
I0629 19:01:06.721372 16821 net.cpp:108] Using FLOAT as default forward math type
I0629 19:01:06.721377 16821 net.cpp:114] Using FLOAT as default backward math type
I0629 19:01:06.721400 16821 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0629 19:01:06.721405 16821 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0629 19:01:06.721470 16822 db_lmdb.cpp:35] Opened lmdb data/train-image-lmdb
I0629 19:01:06.724050 16820 data_layer.cpp:188] ReshapePrefetch 6, 3, 640, 640
I0629 19:01:06.724076 16823 db_lmdb.cpp:35] Opened lmdb data/train-image-lmdb
I0629 19:01:06.724143 16820 data_layer.cpp:206] Output data size: 6, 3, 640, 640
I0629 19:01:06.724151 16820 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0629 19:01:06.724206 16820 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0629 19:01:06.724218 16820 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0629 19:01:06.725096 16824 data_layer.cpp:188] ReshapePrefetch 6, 3, 640, 640
I0629 19:01:06.725127 16824 data_layer.cpp:206] Output data size: 6, 3, 640, 640
I0629 19:01:06.730517 16825 db_lmdb.cpp:35] Opened lmdb data/train-label-lmdb
I0629 19:01:06.733896 16820 data_layer.cpp:188] ReshapePrefetch 6, 1, 640, 640
I0629 19:01:06.734326 16820 data_layer.cpp:206] Output data size: 6, 1, 640, 640
I0629 19:01:06.734462 16820 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0629 19:01:06.737576 16826 data_layer.cpp:188] ReshapePrefetch 6, 1, 640, 640
I0629 19:01:06.737622 16826 data_layer.cpp:206] Output data size: 6, 1, 640, 640
I0629 19:01:06.737679 16821 data_layer.cpp:188] ReshapePrefetch 6, 3, 640, 640
I0629 19:01:06.740487 16821 data_layer.cpp:206] Output data size: 6, 3, 640, 640
I0629 19:01:06.740561 16821 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0629 19:01:06.740768 16821 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 6
I0629 19:01:06.740787 16821 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0629 19:01:06.743602 16827 data_layer.cpp:188] ReshapePrefetch 6, 3, 640, 640
I0629 19:01:06.743634 16827 data_layer.cpp:206] Output data size: 6, 3, 640, 640
I0629 19:01:06.748741 16824 data_layer.cpp:110] [1] Parser threads: 1
I0629 19:01:06.748769 16824 data_layer.cpp:112] [1] Transformer threads: 1
I0629 19:01:06.750145 16828 db_lmdb.cpp:35] Opened lmdb data/train-label-lmdb
I0629 19:01:06.751807 16826 data_layer.cpp:110] [1] Parser threads: 1
I0629 19:01:06.751883 16826 data_layer.cpp:112] [1] Transformer threads: 1
I0629 19:01:06.753064 16821 data_layer.cpp:188] ReshapePrefetch 6, 1, 640, 640
I0629 19:01:06.753520 16821 data_layer.cpp:206] Output data size: 6, 1, 640, 640
I0629 19:01:06.753559 16821 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0629 19:01:06.760239 16829 data_layer.cpp:188] ReshapePrefetch 6, 1, 640, 640
I0629 19:01:06.760267 16829 data_layer.cpp:206] Output data size: 6, 1, 640, 640
I0629 19:01:06.777055 16827 data_layer.cpp:110] [2] Parser threads: 1
I0629 19:01:06.777125 16827 data_layer.cpp:112] [2] Transformer threads: 1
I0629 19:01:06.777655 16829 data_layer.cpp:110] [2] Parser threads: 1
I0629 19:01:06.777674 16829 data_layer.cpp:112] [2] Transformer threads: 1
I0629 19:01:07.296823 16821 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 7.99G, req 0G)
I0629 19:01:07.297103 16820 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.01G/1 1 0 3  (limit 7.99G, req 0G)
I0629 19:01:07.356384 16820 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.83G, req 0G)
I0629 19:01:07.356869 16821 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 7.83G, req 0G)
I0629 19:01:07.412284 16820 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.7G, req 0G)
I0629 19:01:07.414466 16821 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 7.7G, req 0G)
I0629 19:01:07.439457 16820 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.62G, req 0G)
I0629 19:01:07.444238 16821 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 7.62G, req 0G)
I0629 19:01:07.471143 16820 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 7.56G, req 0G)
I0629 19:01:07.474781 16821 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 7.56G, req 0G)
I0629 19:01:07.488262 16820 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.52G, req 0G)
I0629 19:01:07.494072 16821 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 7.52G, req 0G)
I0629 19:01:07.521294 16820 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.48G, req 0G)
I0629 19:01:07.527796 16821 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 7.48G, req 0G)
I0629 19:01:07.534812 16820 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.46G, req 0G)
I0629 19:01:07.542985 16821 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 7.46G, req 0G)
I0629 19:01:07.604769 16820 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 7.35G, req 0G)
I0629 19:01:07.610167 16821 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 7.35G, req 0G)
I0629 19:01:07.623502 16820 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 7.3G, req 0G)
I0629 19:01:07.630678 16821 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 7.3G, req 0G)
I0629 19:01:07.648718 16820 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 7.25G, req 0G)
I0629 19:01:07.652885 16820 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cityscapes20_jsegnet21v2_2017-06-29_16-47-23/sparse/test.prototxt
W0629 19:01:07.652958 16820 parallel.cpp:271] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0629 19:01:07.653086 16820 net.cpp:108] Using FLOAT as default forward math type
I0629 19:01:07.653091 16820 net.cpp:114] Using FLOAT as default backward math type
I0629 19:01:07.653115 16820 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0629 19:01:07.653120 16820 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0629 19:01:07.653910 16863 db_lmdb.cpp:35] Opened lmdb data/val-image-lmdb
I0629 19:01:07.655333 16821 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 7.25G, req 0G)
I0629 19:01:07.655433 16820 data_layer.cpp:188] ReshapePrefetch 2, 3, 640, 640
I0629 19:01:07.655593 16820 data_layer.cpp:206] Output data size: 2, 3, 640, 640
I0629 19:01:07.655603 16820 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0629 19:01:07.655656 16820 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0629 19:01:07.655674 16820 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0629 19:01:07.657367 16864 data_layer.cpp:188] ReshapePrefetch 2, 3, 640, 640
I0629 19:01:07.657388 16864 data_layer.cpp:206] Output data size: 2, 3, 640, 640
I0629 19:01:07.659328 16865 db_lmdb.cpp:35] Opened lmdb data/val-label-lmdb
I0629 19:01:07.660197 16820 data_layer.cpp:188] ReshapePrefetch 2, 1, 640, 640
I0629 19:01:07.660305 16820 data_layer.cpp:206] Output data size: 2, 1, 640, 640
I0629 19:01:07.660311 16820 internal_thread.cpp:19] Starting 1 internal thread(s) on device 1
I0629 19:01:07.660670 16821 solver.cpp:176] Creating test net (#0) specified by test_net file: training/cityscapes20_jsegnet21v2_2017-06-29_16-47-23/sparse/test.prototxt
W0629 19:01:07.661029 16821 parallel.cpp:271] Batch size must be divisible by the number of solvers (GPUs): it's been adjusted from 4 to 6
I0629 19:01:07.661182 16821 net.cpp:108] Using FLOAT as default forward math type
I0629 19:01:07.661187 16821 net.cpp:114] Using FLOAT as default backward math type
I0629 19:01:07.661228 16821 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0629 19:01:07.661237 16821 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0629 19:01:07.662467 16866 data_layer.cpp:188] ReshapePrefetch 2, 1, 640, 640
I0629 19:01:07.662489 16866 data_layer.cpp:206] Output data size: 2, 1, 640, 640
I0629 19:01:07.663477 16867 db_lmdb.cpp:35] Opened lmdb data/val-image-lmdb
I0629 19:01:07.664739 16864 data_layer.cpp:110] [1] Parser threads: 1
I0629 19:01:07.664764 16864 data_layer.cpp:112] [1] Transformer threads: 1
I0629 19:01:07.665478 16821 data_layer.cpp:188] ReshapePrefetch 2, 3, 640, 640
I0629 19:01:07.665735 16821 data_layer.cpp:206] Output data size: 2, 3, 640, 640
I0629 19:01:07.665763 16821 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0629 19:01:07.665935 16821 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 2
I0629 19:01:07.665958 16821 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0629 19:01:07.667666 16866 data_layer.cpp:110] [1] Parser threads: 1
I0629 19:01:07.667693 16866 data_layer.cpp:112] [1] Transformer threads: 1
I0629 19:01:07.667712 16868 data_layer.cpp:188] ReshapePrefetch 2, 3, 640, 640
I0629 19:01:07.667723 16868 data_layer.cpp:206] Output data size: 2, 3, 640, 640
I0629 19:01:07.670763 16869 db_lmdb.cpp:35] Opened lmdb data/val-label-lmdb
I0629 19:01:07.672595 16821 data_layer.cpp:188] ReshapePrefetch 2, 1, 640, 640
I0629 19:01:07.672818 16821 data_layer.cpp:206] Output data size: 2, 1, 640, 640
I0629 19:01:07.672850 16821 internal_thread.cpp:19] Starting 1 internal thread(s) on device 2
I0629 19:01:07.674818 16870 data_layer.cpp:188] ReshapePrefetch 2, 1, 640, 640
I0629 19:01:07.674882 16870 data_layer.cpp:206] Output data size: 2, 1, 640, 640
I0629 19:01:07.676755 16868 data_layer.cpp:110] [2] Parser threads: 1
I0629 19:01:07.676825 16868 data_layer.cpp:112] [2] Transformer threads: 1
I0629 19:01:07.679055 16870 data_layer.cpp:110] [2] Parser threads: 1
I0629 19:01:07.679065 16870 data_layer.cpp:112] [2] Transformer threads: 1
I0629 19:01:07.827862 16820 solver.cpp:56] Solver scaffolding done.
I0629 19:01:07.832792 16821 solver.cpp:56] Solver scaffolding done.
I0629 19:01:07.914340 16820 parallel.cpp:161] [1 - 1] P2pSync adding callback
I0629 19:01:07.914340 16821 parallel.cpp:161] [2 - 2] P2pSync adding callback
I0629 19:01:07.914340 16819 parallel.cpp:161] [0 - 0] P2pSync adding callback
I0629 19:01:08.096086 16820 net.cpp:2158] All zero weights of convolution layers are frozen
I0629 19:01:08.096086 16819 net.cpp:2158] All zero weights of convolution layers are frozen
I0629 19:01:08.096086 16821 net.cpp:2158] All zero weights of convolution layers are frozen
I0629 19:01:08.113713 16820 solver.cpp:474] Solving jsegnet21v2_train
I0629 19:01:08.113729 16820 solver.cpp:475] Learning Rate Policy: multistep
I0629 19:01:08.116196 16821 solver.cpp:474] Solving jsegnet21v2_train
I0629 19:01:08.116209 16821 solver.cpp:475] Learning Rate Policy: multistep
I0629 19:01:08.117617 16819 solver.cpp:474] Solving jsegnet21v2_train
I0629 19:01:08.117625 16819 solver.cpp:475] Learning Rate Policy: multistep
I0629 19:01:08.132349 16820 solver.cpp:268] Starting Optimization on GPU 1
I0629 19:01:08.132349 16821 solver.cpp:268] Starting Optimization on GPU 2
I0629 19:01:08.132349 16819 solver.cpp:268] Starting Optimization on GPU 0
I0629 19:01:08.132547 16819 solver.cpp:545] Iteration 0, Testing net (#0)
I0629 19:01:08.132597 16871 device_alternate.hpp:116] NVML initialized on thread 127942334670592
I0629 19:01:08.132624 16871 common.cpp:563] NVML succeeded to set CPU affinity on device 0
I0629 19:01:08.132645 16873 device_alternate.hpp:116] NVML initialized on thread 127942351456000
I0629 19:01:08.132659 16873 common.cpp:563] NVML succeeded to set CPU affinity on device 1
I0629 19:01:08.133337 16872 device_alternate.hpp:116] NVML initialized on thread 127942343063296
I0629 19:01:08.133347 16872 common.cpp:563] NVML succeeded to set CPU affinity on device 2
I0629 19:01:08.499389 16819 solver.cpp:630]     Test net output #0: accuracy/top1 = 0.831187
I0629 19:01:08.499428 16819 solver.cpp:630]     Test net output #1: accuracy/top5 = 0.998852
I0629 19:01:08.499433 16819 solver.cpp:630]     Test net output #2: loss = 0.435893 (* 1 = 0.435893 loss)
I0629 19:01:08.499438 16819 solver.cpp:295] [MultiGPU] Initial Test completed
I0629 19:01:08.591475 16819 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 5.58G, req 0G)
I0629 19:01:08.593502 16821 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 5.66G, req 0G)
I0629 19:01:08.595187 16820 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 5.66G, req 0G)
I0629 19:01:08.641801 16819 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 5.42G, req 0G)
I0629 19:01:08.646800 16821 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 5.5G, req 0G)
I0629 19:01:08.656101 16820 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 5.5G, req 0G)
I0629 19:01:08.689667 16819 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 1 4 3  (limit 5.24G, req 0G)
I0629 19:01:08.694859 16821 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 5.32G, req 0G)
I0629 19:01:08.704723 16820 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 5.32G, req 0G)
I0629 19:01:08.714877 16819 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 5.16G, req 0G)
I0629 19:01:08.718519 16821 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 5.24G, req 0G)
I0629 19:01:08.731272 16820 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 5.24G, req 0G)
I0629 19:01:08.739572 16819 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 5.07G, req 0G)
I0629 19:01:08.742271 16821 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 5.15G, req 0G)
I0629 19:01:08.752482 16819 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 5.03G, req 0G)
I0629 19:01:08.755699 16820 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 5.15G, req 0G)
I0629 19:01:08.755987 16821 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 5.11G, req 0G)
I0629 19:01:08.771222 16820 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 5.11G, req 0G)
I0629 19:01:08.775012 16819 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 5G, req 0G)
I0629 19:01:08.778996 16821 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 5.08G, req 0G)
I0629 19:01:08.784541 16819 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 4.98G, req 0G)
I0629 19:01:08.787825 16821 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 5.06G, req 0G)
I0629 19:01:08.798415 16820 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 5.08G, req 0G)
I0629 19:01:08.809837 16820 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 5.06G, req 0G)
I0629 19:01:08.834046 16819 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 4.71G, req 0G)
I0629 19:01:08.841105 16821 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 4.79G, req 0G)
I0629 19:01:08.848345 16819 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 4.69G, req 0G)
I0629 19:01:08.856032 16821 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 4.77G, req 0G)
I0629 19:01:08.857978 16820 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 1 3  (limit 4.79G, req 0G)
I0629 19:01:08.873980 16820 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 4.77G, req 0G)
I0629 19:01:08.877501 16819 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 4.54G, req 0G)
I0629 19:01:08.885885 16821 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 4.62G, req 0G)
I0629 19:01:08.904217 16820 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 4.62G, req 0G)
I0629 19:01:09.107578 16871 solver.cpp:446] Finding and applying thresholds. Target sparsity = 0.05
I0629 19:01:09.180136 16820 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 3.28G, req 0G)
I0629 19:01:09.180595 16821 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 3.28G, req 0G)
I0629 19:01:09.227737 16821 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 3.28G, req 0G)
I0629 19:01:09.230523 16820 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 3.28G, req 0G)
I0629 19:01:09.276324 16821 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 3.28G, req 0G)
I0629 19:01:09.280580 16820 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 3.28G, req 0G)
I0629 19:01:09.297387 16821 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 3.28G, req 0G)
I0629 19:01:09.302511 16820 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 3.28G, req 0G)
I0629 19:01:09.315893 16821 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 3.28G, req 0G)
I0629 19:01:09.323004 16820 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 3.28G, req 0G)
I0629 19:01:09.326150 16821 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 1 3  (limit 3.28G, req 0G)
I0629 19:01:09.333974 16820 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 3.28G, req 0G)
I0629 19:01:09.342384 16821 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 3.28G, req 0G)
I0629 19:01:09.348160 16821 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 3.28G, req 0G)
I0629 19:01:09.350947 16820 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 1  (limit 3.28G, req 0G)
I0629 19:01:09.358443 16820 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 3.28G, req 0G)
I0629 19:01:09.374308 16821 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 3.28G, req 0G)
I0629 19:01:09.387352 16821 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 3.28G, req 0G)
I0629 19:01:09.387634 16820 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 3.28G, req 0G)
I0629 19:01:09.400851 16820 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 3.28G, req 0G)
I0629 19:01:09.404739 16821 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 3.28G, req 0G)
I0629 19:01:09.418649 16820 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 3.28G, req 0G)
I0629 19:01:09.794910 16871 net.cpp:2158] All zero weights of convolution layers are frozen
I0629 19:01:09.800038 16819 solver.cpp:354] Iteration 0 (1.30054 s), loss = 0.230933
I0629 19:01:09.800057 16819 solver.cpp:371]     Train net output #0: loss = 0.230933 (* 1 = 0.230933 loss)
I0629 19:01:09.800065 16819 sgd_solver.cpp:137] Iteration 0, lr = 1e-05, m = 0.9
I0629 19:01:09.856065 16819 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'conv1a' with space 0.02G/1 1 0 3  (limit 3.16G, req 0G)
I0629 19:01:09.901785 16819 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'conv1b' with space 0.02G/2 6 4 3  (limit 3.16G, req 0G)
I0629 19:01:09.942101 16819 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 0.02G/1 6 4 3  (limit 3.16G, req 0G)
I0629 19:01:09.962658 16819 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 0.02G/2 6 4 3  (limit 3.16G, req 0G)
I0629 19:01:09.981875 16819 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.02G/1 6 4 3  (limit 3.16G, req 0G)
I0629 19:01:09.992511 16819 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.02G/2 6 4 3  (limit 3.16G, req 0G)
I0629 19:01:10.009469 16819 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.02G/1 6 4 3  (limit 3.16G, req 0G)
I0629 19:01:10.015317 16819 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.02G/2 6 4 3  (limit 3.16G, req 0G)
I0629 19:01:10.041465 16819 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'out3a' with space 0.02G/2 6 4 3  (limit 3.16G, req 0G)
I0629 19:01:10.054425 16819 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.02G/1 6 4 3  (limit 3.16G, req 0G)
I0629 19:01:10.071859 16819 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 0.02G/1 6 1 3  (limit 3.16G, req 0G)
I0629 19:01:10.218608 16819 solver.cpp:354] Iteration 1 (0.418519 s), loss = 0.128303
I0629 19:01:10.218628 16819 solver.cpp:371]     Train net output #0: loss = 0.128303 (* 1 = 0.128303 loss)
I0629 19:01:10.319862 16819 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'conv1a' with space 1.29G/1 1 0 3  (limit 1.89G, req 0G)
I0629 19:01:10.322621 16821 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'conv1a' with space 1.29G/1 1 0 3  (limit 2.01G, req 0G)
I0629 19:01:10.324348 16820 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'conv1a' with space 1.29G/1 1 0 3  (limit 2.01G, req 0G)
W0629 19:01:10.326515 16819 cudnn_conv_layer.cpp:235] [0] Current workspace (1.29G) Estimated requirement (2.57G)
W0629 19:01:10.337316 16821 cudnn_conv_layer.cpp:235] [2] Current workspace (1.29G) Estimated requirement (2.57G)
W0629 19:01:10.339164 16820 cudnn_conv_layer.cpp:235] [1] Current workspace (1.29G) Estimated requirement (2.57G)
I0629 19:01:10.390276 16819 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'conv1b' with space 1.88G/2 6 4 3  (limit 1.3G, req 0G)
I0629 19:01:10.402874 16821 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'conv1b' with space 1.99G/2 6 4 3  (limit 1.3G, req 0G)
I0629 19:01:10.412118 16820 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'conv1b' with space 1.99G/2 6 4 3  (limit 1.3G, req 0G)
I0629 19:01:10.519121 16819 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.88G/1 6 4 3  (limit 1.3G, req 0G)
W0629 19:01:10.523582 16819 cudnn_conv_layer.cpp:235] [0] Current workspace (1.88G) Estimated requirement (2.57G)
I0629 19:01:10.538017 16821 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.99G/1 6 4 3  (limit 1.3G, req 0G)
W0629 19:01:10.542870 16821 cudnn_conv_layer.cpp:235] [2] Current workspace (1.99G) Estimated requirement (2.57G)
I0629 19:01:10.551270 16820 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.99G/1 6 4 3  (limit 1.3G, req 0G)
W0629 19:01:10.558105 16820 cudnn_conv_layer.cpp:235] [1] Current workspace (1.99G) Estimated requirement (2.57G)
I0629 19:01:10.566757 16819 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.88G/2 1 4 3  (limit 1.3G, req 0G)
I0629 19:01:10.585578 16821 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.99G/2 6 4 3  (limit 1.3G, req 0G)
I0629 19:01:10.602058 16820 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.99G/2 6 4 3  (limit 1.3G, req 0G)
I0629 19:01:10.667794 16819 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.88G/1 6 4 5  (limit 1.3G, req 0.07G)
W0629 19:01:10.671056 16819 cudnn_conv_layer.cpp:235] [0] Current workspace (1.88G) Estimated requirement (2.57G)
I0629 19:01:10.684072 16821 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.99G/1 6 4 5  (limit 1.3G, req 0.07G)
W0629 19:01:10.687311 16821 cudnn_conv_layer.cpp:235] [2] Current workspace (1.99G) Estimated requirement (2.57G)
I0629 19:01:10.700351 16819 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.88G/2 6 4 3  (limit 1.3G, req 0.07G)
I0629 19:01:10.706598 16820 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 1.99G/1 6 4 5  (limit 1.3G, req 0.07G)
W0629 19:01:10.710147 16820 cudnn_conv_layer.cpp:235] [1] Current workspace (1.99G) Estimated requirement (2.57G)
I0629 19:01:10.718924 16821 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.99G/2 6 4 5  (limit 1.3G, req 0.07G)
I0629 19:01:10.736209 16820 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 1.99G/2 6 4 3  (limit 1.3G, req 0.07G)
I0629 19:01:10.766907 16819 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.88G/1 6 4 5  (limit 1.3G, req 0.07G)
W0629 19:01:10.768518 16819 cudnn_conv_layer.cpp:235] [0] Current workspace (1.88G) Estimated requirement (2.57G)
I0629 19:01:10.780740 16819 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.88G/2 6 4 3  (limit 1.3G, req 0.07G)
I0629 19:01:10.788179 16821 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.99G/1 6 4 5  (limit 1.3G, req 0.07G)
W0629 19:01:10.789770 16821 cudnn_conv_layer.cpp:235] [2] Current workspace (1.99G) Estimated requirement (2.57G)
W0629 19:01:10.799571 16819 cudnn_conv_layer.cpp:235] [0] Current workspace (1.88G) Estimated requirement (2.57G)
I0629 19:01:10.801265 16821 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.99G/2 6 4 3  (limit 1.3G, req 0.07G)
I0629 19:01:10.806720 16820 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 1.99G/1 6 4 5  (limit 1.3G, req 0.07G)
W0629 19:01:10.808327 16820 cudnn_conv_layer.cpp:235] [1] Current workspace (1.99G) Estimated requirement (2.57G)
W0629 19:01:10.819892 16821 cudnn_conv_layer.cpp:235] [2] Current workspace (1.99G) Estimated requirement (2.57G)
I0629 19:01:10.820793 16820 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 1.99G/2 6 4 3  (limit 1.3G, req 0.07G)
I0629 19:01:10.831924 16819 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'out3a' with space 1.88G/2 6 4 3  (limit 1.3G, req 0.07G)
W0629 19:01:10.840430 16820 cudnn_conv_layer.cpp:235] [1] Current workspace (1.99G) Estimated requirement (2.57G)
I0629 19:01:10.853883 16821 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'out3a' with space 1.99G/2 6 4 3  (limit 1.3G, req 0.07G)
I0629 19:01:10.877993 16820 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'out3a' with space 1.99G/2 6 4 3  (limit 1.3G, req 0.07G)
I0629 19:01:10.886219 16819 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 1.88G/1 6 4 3  (limit 1.3G, req 0.07G)
I0629 19:01:10.909891 16821 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'ctx_conv1' with space 1.99G/1 6 4 3  (limit 1.3G, req 0.07G)
I0629 19:01:10.920676 16819 cudnn_conv_layer.cpp:833] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 1.88G/1 6 1 3  (limit 1.3G, req 0.07G)
I0629 19:01:10.936614 16820 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'ctx_conv1' with space 1.99G/1 6 4 3  (limit 1.3G, req 0.07G)
I0629 19:01:10.943897 16821 cudnn_conv_layer.cpp:833] [2] Conv Algos (F,BD,BF): 'ctx_final' with space 1.99G/1 6 1 3  (limit 1.3G, req 0.07G)
I0629 19:01:10.971604 16820 cudnn_conv_layer.cpp:833] [1] Conv Algos (F,BD,BF): 'ctx_final' with space 1.99G/1 6 1 3  (limit 1.3G, req 0.07G)
I0629 19:01:11.128862 16819 solver.cpp:349] Iteration 2 (1.09867 iter/s, 0.910194s/100 iter), loss = 0.151443
I0629 19:01:11.128885 16819 solver.cpp:371]     Train net output #0: loss = 0.151443 (* 1 = 0.151443 loss)
I0629 19:01:11.564550 16821 cudnn_conv_layer.cpp:283] [2] Layer 'conv1a' reallocating workspace: 1.99G -> 0.14G
I0629 19:01:11.564571 16820 cudnn_conv_layer.cpp:283] [1] Layer 'conv1a' reallocating workspace: 1.99G -> 0.14G
I0629 19:01:11.564684 16819 cudnn_conv_layer.cpp:283] [0] Layer 'conv1a' reallocating workspace: 1.88G -> 0.14G
