I0816 10:41:32.350684  4780 caffe.cpp:608] This is NVCaffe 0.16.3 started at Wed Aug 16 10:41:32 2017
I0816 10:41:32.350801  4780 caffe.cpp:611] CuDNN version: 6021
I0816 10:41:32.350805  4780 caffe.cpp:612] CuBLAS version: 8000
I0816 10:41:32.350807  4780 caffe.cpp:613] CUDA version: 8000
I0816 10:41:32.350810  4780 caffe.cpp:614] CUDA driver version: 8000
I0816 10:41:32.350816  4780 caffe.cpp:263] Not using GPU #2 for single-GPU function
I0816 10:41:32.350817  4780 caffe.cpp:263] Not using GPU #1 for single-GPU function
I0816 10:41:32.351400  4780 gpu_memory.cpp:159] GPUMemory::Manager initialized with Caching (CUB) GPU Allocator
I0816 10:41:32.351977  4780 gpu_memory.cpp:161] Total memory: 8506769408, Free: 8278441984, dev_info[0]: total=8506769408 free=8278441984
I0816 10:41:32.351982  4780 caffe.cpp:275] Use GPU with device ID 0
I0816 10:41:32.352344  4780 caffe.cpp:279] GPU device name: GeForce GTX 1080
I0816 10:41:32.353601  4780 net.cpp:72] Initializing net from parameters: 
name: "jacintonet11v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 224
    mean_value: 0
    mean_value: 0
    mean_value: 0
  }
  data_param {
    source: "./data/ilsvrc12_val_lmdb"
    batch_size: 50
    backend: LMDB
    threads: 1
    parser_threads: 1
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "res5a_branch2b"
  top: "pool5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc1000"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc1000"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc1000"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "fc1000"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "fc1000"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
quantize: true
I0816 10:41:32.353705  4780 net.cpp:104] Using FLOAT as default forward math type
I0816 10:41:32.353709  4780 net.cpp:110] Using FLOAT as default backward math type
I0816 10:41:32.353713  4780 layer_factory.hpp:136] Creating layer 'data' of type 'Data'
I0816 10:41:32.353715  4780 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 10:41:32.353760  4780 net.cpp:184] Created Layer data (0)
I0816 10:41:32.353763  4780 net.cpp:530] data -> data
I0816 10:41:32.353771  4780 net.cpp:530] data -> label
I0816 10:41:32.353787  4780 data_reader.cpp:52] Data Reader threads: 1, out queues: 1, depth: 50
I0816 10:41:32.354074  4780 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0816 10:41:32.379492  4799 db_lmdb.cpp:24] Opened lmdb ./data/ilsvrc12_val_lmdb
I0816 10:41:32.384337  4780 data_layer.cpp:185] (0) ReshapePrefetch 50, 3, 224, 224
I0816 10:41:32.384384  4780 data_layer.cpp:209] (0) Output data size: 50, 3, 224, 224
I0816 10:41:32.384390  4780 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I0816 10:41:32.384433  4780 net.cpp:245] Setting up data
I0816 10:41:32.384445  4780 net.cpp:252] TEST Top shape for layer 0 'data' 50 3 224 224 (7526400)
I0816 10:41:32.384452  4780 net.cpp:252] TEST Top shape for layer 0 'data' 50 (50)
I0816 10:41:32.384459  4780 layer_factory.hpp:136] Creating layer 'label_data_1_split' of type 'Split'
I0816 10:41:32.384465  4780 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 10:41:32.384479  4780 net.cpp:184] Created Layer label_data_1_split (1)
I0816 10:41:32.384485  4780 net.cpp:561] label_data_1_split <- label
I0816 10:41:32.384493  4780 net.cpp:530] label_data_1_split -> label_data_1_split_0
I0816 10:41:32.384498  4780 net.cpp:530] label_data_1_split -> label_data_1_split_1
I0816 10:41:32.384515  4780 net.cpp:530] label_data_1_split -> label_data_1_split_2
I0816 10:41:32.384551  4780 net.cpp:245] Setting up label_data_1_split
I0816 10:41:32.384557  4780 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 50 (50)
I0816 10:41:32.384562  4780 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 50 (50)
I0816 10:41:32.384564  4780 net.cpp:252] TEST Top shape for layer 1 'label_data_1_split' 50 (50)
I0816 10:41:32.384568  4780 layer_factory.hpp:136] Creating layer 'data/bias' of type 'Bias'
I0816 10:41:32.384572  4780 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 10:41:32.384583  4780 net.cpp:184] Created Layer data/bias (2)
I0816 10:41:32.384588  4780 net.cpp:561] data/bias <- data
I0816 10:41:32.384591  4780 net.cpp:530] data/bias -> data/bias
I0816 10:41:32.385908  4800 data_layer.cpp:97] (0) Parser threads: 1
I0816 10:41:32.385917  4800 data_layer.cpp:99] (0) Transformer threads: 1
I0816 10:41:32.391008  4780 net.cpp:245] Setting up data/bias
I0816 10:41:32.391047  4780 net.cpp:252] TEST Top shape for layer 2 'data/bias' 50 3 224 224 (7526400)
I0816 10:41:32.391067  4780 layer_factory.hpp:136] Creating layer 'conv1a' of type 'Convolution'
I0816 10:41:32.391074  4780 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 10:41:32.391101  4780 net.cpp:184] Created Layer conv1a (3)
I0816 10:41:32.391108  4780 net.cpp:561] conv1a <- data/bias
I0816 10:41:32.391115  4780 net.cpp:530] conv1a -> conv1a
I0816 10:41:32.696094  4780 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.01G/1 1  (limit 8.02G, req 0G)
I0816 10:41:32.696113  4780 net.cpp:245] Setting up conv1a
I0816 10:41:32.696118  4780 net.cpp:252] TEST Top shape for layer 3 'conv1a' 50 32 112 112 (20070400)
I0816 10:41:32.696128  4780 layer_factory.hpp:136] Creating layer 'conv1a/bn' of type 'BatchNorm'
I0816 10:41:32.696141  4780 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 10:41:32.696153  4780 net.cpp:184] Created Layer conv1a/bn (4)
I0816 10:41:32.696157  4780 net.cpp:561] conv1a/bn <- conv1a
I0816 10:41:32.696161  4780 net.cpp:513] conv1a/bn -> conv1a (in-place)
I0816 10:41:32.696604  4780 net.cpp:245] Setting up conv1a/bn
I0816 10:41:32.696611  4780 net.cpp:252] TEST Top shape for layer 4 'conv1a/bn' 50 32 112 112 (20070400)
I0816 10:41:32.696619  4780 layer_factory.hpp:136] Creating layer 'conv1a/relu' of type 'ReLU'
I0816 10:41:32.696621  4780 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 10:41:32.696625  4780 net.cpp:184] Created Layer conv1a/relu (5)
I0816 10:41:32.696627  4780 net.cpp:561] conv1a/relu <- conv1a
I0816 10:41:32.696630  4780 net.cpp:513] conv1a/relu -> conv1a (in-place)
I0816 10:41:32.696640  4780 net.cpp:245] Setting up conv1a/relu
I0816 10:41:32.696642  4780 net.cpp:252] TEST Top shape for layer 5 'conv1a/relu' 50 32 112 112 (20070400)
I0816 10:41:32.696645  4780 layer_factory.hpp:136] Creating layer 'conv1b' of type 'Convolution'
I0816 10:41:32.696648  4780 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 10:41:32.696660  4780 net.cpp:184] Created Layer conv1b (6)
I0816 10:41:32.696662  4780 net.cpp:561] conv1b <- conv1a
I0816 10:41:32.696666  4780 net.cpp:530] conv1b -> conv1b
I0816 10:41:32.705416  4780 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.93G, req 0G)
I0816 10:41:32.705426  4780 net.cpp:245] Setting up conv1b
I0816 10:41:32.705430  4780 net.cpp:252] TEST Top shape for layer 6 'conv1b' 50 32 112 112 (20070400)
I0816 10:41:32.705436  4780 layer_factory.hpp:136] Creating layer 'conv1b/bn' of type 'BatchNorm'
I0816 10:41:32.705438  4780 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 10:41:32.705443  4780 net.cpp:184] Created Layer conv1b/bn (7)
I0816 10:41:32.705446  4780 net.cpp:561] conv1b/bn <- conv1b
I0816 10:41:32.705449  4780 net.cpp:513] conv1b/bn -> conv1b (in-place)
I0816 10:41:32.705857  4780 net.cpp:245] Setting up conv1b/bn
I0816 10:41:32.705864  4780 net.cpp:252] TEST Top shape for layer 7 'conv1b/bn' 50 32 112 112 (20070400)
I0816 10:41:32.705870  4780 layer_factory.hpp:136] Creating layer 'conv1b/relu' of type 'ReLU'
I0816 10:41:32.705873  4780 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 10:41:32.705876  4780 net.cpp:184] Created Layer conv1b/relu (8)
I0816 10:41:32.705878  4780 net.cpp:561] conv1b/relu <- conv1b
I0816 10:41:32.705881  4780 net.cpp:513] conv1b/relu -> conv1b (in-place)
I0816 10:41:32.705884  4780 net.cpp:245] Setting up conv1b/relu
I0816 10:41:32.705888  4780 net.cpp:252] TEST Top shape for layer 8 'conv1b/relu' 50 32 112 112 (20070400)
I0816 10:41:32.705889  4780 layer_factory.hpp:136] Creating layer 'pool1' of type 'Pooling'
I0816 10:41:32.705891  4780 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 10:41:32.705898  4780 net.cpp:184] Created Layer pool1 (9)
I0816 10:41:32.705899  4780 net.cpp:561] pool1 <- conv1b
I0816 10:41:32.705902  4780 net.cpp:530] pool1 -> pool1
I0816 10:41:32.705943  4780 net.cpp:245] Setting up pool1
I0816 10:41:32.705948  4780 net.cpp:252] TEST Top shape for layer 9 'pool1' 50 32 56 56 (5017600)
I0816 10:41:32.705951  4780 layer_factory.hpp:136] Creating layer 'res2a_branch2a' of type 'Convolution'
I0816 10:41:32.705953  4780 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 10:41:32.705958  4780 net.cpp:184] Created Layer res2a_branch2a (10)
I0816 10:41:32.705961  4780 net.cpp:561] res2a_branch2a <- pool1
I0816 10:41:32.705963  4780 net.cpp:530] res2a_branch2a -> res2a_branch2a
I0816 10:41:32.714335  4780 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 6  (limit 7.86G, req 0G)
I0816 10:41:32.714346  4780 net.cpp:245] Setting up res2a_branch2a
I0816 10:41:32.714350  4780 net.cpp:252] TEST Top shape for layer 10 'res2a_branch2a' 50 64 56 56 (10035200)
I0816 10:41:32.714356  4780 layer_factory.hpp:136] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I0816 10:41:32.714359  4780 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 10:41:32.714363  4780 net.cpp:184] Created Layer res2a_branch2a/bn (11)
I0816 10:41:32.714365  4780 net.cpp:561] res2a_branch2a/bn <- res2a_branch2a
I0816 10:41:32.714368  4780 net.cpp:513] res2a_branch2a/bn -> res2a_branch2a (in-place)
I0816 10:41:32.714762  4780 net.cpp:245] Setting up res2a_branch2a/bn
I0816 10:41:32.714769  4780 net.cpp:252] TEST Top shape for layer 11 'res2a_branch2a/bn' 50 64 56 56 (10035200)
I0816 10:41:32.714776  4780 layer_factory.hpp:136] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I0816 10:41:32.714777  4780 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 10:41:32.714781  4780 net.cpp:184] Created Layer res2a_branch2a/relu (12)
I0816 10:41:32.714783  4780 net.cpp:561] res2a_branch2a/relu <- res2a_branch2a
I0816 10:41:32.714787  4780 net.cpp:513] res2a_branch2a/relu -> res2a_branch2a (in-place)
I0816 10:41:32.714789  4780 net.cpp:245] Setting up res2a_branch2a/relu
I0816 10:41:32.714792  4780 net.cpp:252] TEST Top shape for layer 12 'res2a_branch2a/relu' 50 64 56 56 (10035200)
I0816 10:41:32.714794  4780 layer_factory.hpp:136] Creating layer 'res2a_branch2b' of type 'Convolution'
I0816 10:41:32.714797  4780 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 10:41:32.714804  4780 net.cpp:184] Created Layer res2a_branch2b (13)
I0816 10:41:32.714807  4780 net.cpp:561] res2a_branch2b <- res2a_branch2a
I0816 10:41:32.714809  4780 net.cpp:530] res2a_branch2b -> res2a_branch2b
I0816 10:41:32.719374  4780 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.82G, req 0G)
I0816 10:41:32.719384  4780 net.cpp:245] Setting up res2a_branch2b
I0816 10:41:32.719389  4780 net.cpp:252] TEST Top shape for layer 13 'res2a_branch2b' 50 64 56 56 (10035200)
I0816 10:41:32.719401  4780 layer_factory.hpp:136] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I0816 10:41:32.719404  4780 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 10:41:32.719410  4780 net.cpp:184] Created Layer res2a_branch2b/bn (14)
I0816 10:41:32.719413  4780 net.cpp:561] res2a_branch2b/bn <- res2a_branch2b
I0816 10:41:32.719415  4780 net.cpp:513] res2a_branch2b/bn -> res2a_branch2b (in-place)
I0816 10:41:32.719810  4780 net.cpp:245] Setting up res2a_branch2b/bn
I0816 10:41:32.719817  4780 net.cpp:252] TEST Top shape for layer 14 'res2a_branch2b/bn' 50 64 56 56 (10035200)
I0816 10:41:32.719825  4780 layer_factory.hpp:136] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I0816 10:41:32.719826  4780 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 10:41:32.719830  4780 net.cpp:184] Created Layer res2a_branch2b/relu (15)
I0816 10:41:32.719831  4780 net.cpp:561] res2a_branch2b/relu <- res2a_branch2b
I0816 10:41:32.719835  4780 net.cpp:513] res2a_branch2b/relu -> res2a_branch2b (in-place)
I0816 10:41:32.719837  4780 net.cpp:245] Setting up res2a_branch2b/relu
I0816 10:41:32.719841  4780 net.cpp:252] TEST Top shape for layer 15 'res2a_branch2b/relu' 50 64 56 56 (10035200)
I0816 10:41:32.719842  4780 layer_factory.hpp:136] Creating layer 'pool2' of type 'Pooling'
I0816 10:41:32.719844  4780 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 10:41:32.719848  4780 net.cpp:184] Created Layer pool2 (16)
I0816 10:41:32.719851  4780 net.cpp:561] pool2 <- res2a_branch2b
I0816 10:41:32.719853  4780 net.cpp:530] pool2 -> pool2
I0816 10:41:32.719880  4780 net.cpp:245] Setting up pool2
I0816 10:41:32.719884  4780 net.cpp:252] TEST Top shape for layer 16 'pool2' 50 64 28 28 (2508800)
I0816 10:41:32.719887  4780 layer_factory.hpp:136] Creating layer 'res3a_branch2a' of type 'Convolution'
I0816 10:41:32.719889  4780 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 10:41:32.719894  4780 net.cpp:184] Created Layer res3a_branch2a (17)
I0816 10:41:32.719897  4780 net.cpp:561] res3a_branch2a <- pool2
I0816 10:41:32.719899  4780 net.cpp:530] res3a_branch2a -> res3a_branch2a
I0816 10:41:32.728495  4780 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.79G, req 0G)
I0816 10:41:32.728513  4780 net.cpp:245] Setting up res3a_branch2a
I0816 10:41:32.728518  4780 net.cpp:252] TEST Top shape for layer 17 'res3a_branch2a' 50 128 28 28 (5017600)
I0816 10:41:32.728526  4780 layer_factory.hpp:136] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I0816 10:41:32.728529  4780 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 10:41:32.728538  4780 net.cpp:184] Created Layer res3a_branch2a/bn (18)
I0816 10:41:32.728540  4780 net.cpp:561] res3a_branch2a/bn <- res3a_branch2a
I0816 10:41:32.728543  4780 net.cpp:513] res3a_branch2a/bn -> res3a_branch2a (in-place)
I0816 10:41:32.728986  4780 net.cpp:245] Setting up res3a_branch2a/bn
I0816 10:41:32.728994  4780 net.cpp:252] TEST Top shape for layer 18 'res3a_branch2a/bn' 50 128 28 28 (5017600)
I0816 10:41:32.729002  4780 layer_factory.hpp:136] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I0816 10:41:32.729005  4780 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 10:41:32.729008  4780 net.cpp:184] Created Layer res3a_branch2a/relu (19)
I0816 10:41:32.729012  4780 net.cpp:561] res3a_branch2a/relu <- res3a_branch2a
I0816 10:41:32.729013  4780 net.cpp:513] res3a_branch2a/relu -> res3a_branch2a (in-place)
I0816 10:41:32.729017  4780 net.cpp:245] Setting up res3a_branch2a/relu
I0816 10:41:32.729020  4780 net.cpp:252] TEST Top shape for layer 19 'res3a_branch2a/relu' 50 128 28 28 (5017600)
I0816 10:41:32.729022  4780 layer_factory.hpp:136] Creating layer 'res3a_branch2b' of type 'Convolution'
I0816 10:41:32.729024  4780 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 10:41:32.729046  4780 net.cpp:184] Created Layer res3a_branch2b (20)
I0816 10:41:32.729049  4780 net.cpp:561] res3a_branch2b <- res3a_branch2a
I0816 10:41:32.729051  4780 net.cpp:530] res3a_branch2b -> res3a_branch2b
I0816 10:41:32.732678  4780 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.77G, req 0G)
I0816 10:41:32.732691  4780 net.cpp:245] Setting up res3a_branch2b
I0816 10:41:32.732694  4780 net.cpp:252] TEST Top shape for layer 20 'res3a_branch2b' 50 128 28 28 (5017600)
I0816 10:41:32.732699  4780 layer_factory.hpp:136] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I0816 10:41:32.732702  4780 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 10:41:32.732707  4780 net.cpp:184] Created Layer res3a_branch2b/bn (21)
I0816 10:41:32.732710  4780 net.cpp:561] res3a_branch2b/bn <- res3a_branch2b
I0816 10:41:32.732712  4780 net.cpp:513] res3a_branch2b/bn -> res3a_branch2b (in-place)
I0816 10:41:32.733129  4780 net.cpp:245] Setting up res3a_branch2b/bn
I0816 10:41:32.733136  4780 net.cpp:252] TEST Top shape for layer 21 'res3a_branch2b/bn' 50 128 28 28 (5017600)
I0816 10:41:32.733142  4780 layer_factory.hpp:136] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I0816 10:41:32.733145  4780 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 10:41:32.733150  4780 net.cpp:184] Created Layer res3a_branch2b/relu (22)
I0816 10:41:32.733153  4780 net.cpp:561] res3a_branch2b/relu <- res3a_branch2b
I0816 10:41:32.733156  4780 net.cpp:513] res3a_branch2b/relu -> res3a_branch2b (in-place)
I0816 10:41:32.733161  4780 net.cpp:245] Setting up res3a_branch2b/relu
I0816 10:41:32.733165  4780 net.cpp:252] TEST Top shape for layer 22 'res3a_branch2b/relu' 50 128 28 28 (5017600)
I0816 10:41:32.733167  4780 layer_factory.hpp:136] Creating layer 'pool3' of type 'Pooling'
I0816 10:41:32.733170  4780 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 10:41:32.733173  4780 net.cpp:184] Created Layer pool3 (23)
I0816 10:41:32.733176  4780 net.cpp:561] pool3 <- res3a_branch2b
I0816 10:41:32.733180  4780 net.cpp:530] pool3 -> pool3
I0816 10:41:32.733216  4780 net.cpp:245] Setting up pool3
I0816 10:41:32.733220  4780 net.cpp:252] TEST Top shape for layer 23 'pool3' 50 128 14 14 (1254400)
I0816 10:41:32.733222  4780 layer_factory.hpp:136] Creating layer 'res4a_branch2a' of type 'Convolution'
I0816 10:41:32.733225  4780 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 10:41:32.733233  4780 net.cpp:184] Created Layer res4a_branch2a (24)
I0816 10:41:32.733235  4780 net.cpp:561] res4a_branch2a <- pool3
I0816 10:41:32.733237  4780 net.cpp:530] res4a_branch2a -> res4a_branch2a
I0816 10:41:32.745738  4780 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.75G, req 0G)
I0816 10:41:32.745748  4780 net.cpp:245] Setting up res4a_branch2a
I0816 10:41:32.745751  4780 net.cpp:252] TEST Top shape for layer 24 'res4a_branch2a' 50 256 14 14 (2508800)
I0816 10:41:32.745755  4780 layer_factory.hpp:136] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I0816 10:41:32.745759  4780 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 10:41:32.745762  4780 net.cpp:184] Created Layer res4a_branch2a/bn (25)
I0816 10:41:32.745765  4780 net.cpp:561] res4a_branch2a/bn <- res4a_branch2a
I0816 10:41:32.745767  4780 net.cpp:513] res4a_branch2a/bn -> res4a_branch2a (in-place)
I0816 10:41:32.746168  4780 net.cpp:245] Setting up res4a_branch2a/bn
I0816 10:41:32.746176  4780 net.cpp:252] TEST Top shape for layer 25 'res4a_branch2a/bn' 50 256 14 14 (2508800)
I0816 10:41:32.746181  4780 layer_factory.hpp:136] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I0816 10:41:32.746182  4780 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 10:41:32.746194  4780 net.cpp:184] Created Layer res4a_branch2a/relu (26)
I0816 10:41:32.746196  4780 net.cpp:561] res4a_branch2a/relu <- res4a_branch2a
I0816 10:41:32.746199  4780 net.cpp:513] res4a_branch2a/relu -> res4a_branch2a (in-place)
I0816 10:41:32.746202  4780 net.cpp:245] Setting up res4a_branch2a/relu
I0816 10:41:32.746206  4780 net.cpp:252] TEST Top shape for layer 26 'res4a_branch2a/relu' 50 256 14 14 (2508800)
I0816 10:41:32.746207  4780 layer_factory.hpp:136] Creating layer 'res4a_branch2b' of type 'Convolution'
I0816 10:41:32.746210  4780 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 10:41:32.746215  4780 net.cpp:184] Created Layer res4a_branch2b (27)
I0816 10:41:32.746218  4780 net.cpp:561] res4a_branch2b <- res4a_branch2a
I0816 10:41:32.746222  4780 net.cpp:530] res4a_branch2b -> res4a_branch2b
I0816 10:41:32.751868  4780 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.74G, req 0G)
I0816 10:41:32.751876  4780 net.cpp:245] Setting up res4a_branch2b
I0816 10:41:32.751881  4780 net.cpp:252] TEST Top shape for layer 27 'res4a_branch2b' 50 256 14 14 (2508800)
I0816 10:41:32.751885  4780 layer_factory.hpp:136] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I0816 10:41:32.751888  4780 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 10:41:32.751893  4780 net.cpp:184] Created Layer res4a_branch2b/bn (28)
I0816 10:41:32.751895  4780 net.cpp:561] res4a_branch2b/bn <- res4a_branch2b
I0816 10:41:32.751898  4780 net.cpp:513] res4a_branch2b/bn -> res4a_branch2b (in-place)
I0816 10:41:32.752295  4780 net.cpp:245] Setting up res4a_branch2b/bn
I0816 10:41:32.752302  4780 net.cpp:252] TEST Top shape for layer 28 'res4a_branch2b/bn' 50 256 14 14 (2508800)
I0816 10:41:32.752308  4780 layer_factory.hpp:136] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I0816 10:41:32.752310  4780 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 10:41:32.752313  4780 net.cpp:184] Created Layer res4a_branch2b/relu (29)
I0816 10:41:32.752316  4780 net.cpp:561] res4a_branch2b/relu <- res4a_branch2b
I0816 10:41:32.752318  4780 net.cpp:513] res4a_branch2b/relu -> res4a_branch2b (in-place)
I0816 10:41:32.752321  4780 net.cpp:245] Setting up res4a_branch2b/relu
I0816 10:41:32.752324  4780 net.cpp:252] TEST Top shape for layer 29 'res4a_branch2b/relu' 50 256 14 14 (2508800)
I0816 10:41:32.752326  4780 layer_factory.hpp:136] Creating layer 'pool4' of type 'Pooling'
I0816 10:41:32.752328  4780 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 10:41:32.752332  4780 net.cpp:184] Created Layer pool4 (30)
I0816 10:41:32.752334  4780 net.cpp:561] pool4 <- res4a_branch2b
I0816 10:41:32.752336  4780 net.cpp:530] pool4 -> pool4
I0816 10:41:32.752367  4780 net.cpp:245] Setting up pool4
I0816 10:41:32.752373  4780 net.cpp:252] TEST Top shape for layer 30 'pool4' 50 256 7 7 (627200)
I0816 10:41:32.752375  4780 layer_factory.hpp:136] Creating layer 'res5a_branch2a' of type 'Convolution'
I0816 10:41:32.752378  4780 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 10:41:32.752383  4780 net.cpp:184] Created Layer res5a_branch2a (31)
I0816 10:41:32.752385  4780 net.cpp:561] res5a_branch2a <- pool4
I0816 10:41:32.752388  4780 net.cpp:530] res5a_branch2a -> res5a_branch2a
I0816 10:41:32.784339  4780 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.72G, req 0G)
I0816 10:41:32.784355  4780 net.cpp:245] Setting up res5a_branch2a
I0816 10:41:32.784360  4780 net.cpp:252] TEST Top shape for layer 31 'res5a_branch2a' 50 512 7 7 (1254400)
I0816 10:41:32.784368  4780 layer_factory.hpp:136] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I0816 10:41:32.784370  4780 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 10:41:32.784379  4780 net.cpp:184] Created Layer res5a_branch2a/bn (32)
I0816 10:41:32.784382  4780 net.cpp:561] res5a_branch2a/bn <- res5a_branch2a
I0816 10:41:32.784399  4780 net.cpp:513] res5a_branch2a/bn -> res5a_branch2a (in-place)
I0816 10:41:32.784821  4780 net.cpp:245] Setting up res5a_branch2a/bn
I0816 10:41:32.784828  4780 net.cpp:252] TEST Top shape for layer 32 'res5a_branch2a/bn' 50 512 7 7 (1254400)
I0816 10:41:32.784834  4780 layer_factory.hpp:136] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I0816 10:41:32.784837  4780 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 10:41:32.784840  4780 net.cpp:184] Created Layer res5a_branch2a/relu (33)
I0816 10:41:32.784843  4780 net.cpp:561] res5a_branch2a/relu <- res5a_branch2a
I0816 10:41:32.784845  4780 net.cpp:513] res5a_branch2a/relu -> res5a_branch2a (in-place)
I0816 10:41:32.784849  4780 net.cpp:245] Setting up res5a_branch2a/relu
I0816 10:41:32.784852  4780 net.cpp:252] TEST Top shape for layer 33 'res5a_branch2a/relu' 50 512 7 7 (1254400)
I0816 10:41:32.784854  4780 layer_factory.hpp:136] Creating layer 'res5a_branch2b' of type 'Convolution'
I0816 10:41:32.784857  4780 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 10:41:32.784862  4780 net.cpp:184] Created Layer res5a_branch2b (34)
I0816 10:41:32.784864  4780 net.cpp:561] res5a_branch2b <- res5a_branch2a
I0816 10:41:32.784868  4780 net.cpp:530] res5a_branch2b -> res5a_branch2b
I0816 10:41:32.801529  4780 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 1  (limit 7.71G, req 0G)
I0816 10:41:32.801549  4780 net.cpp:245] Setting up res5a_branch2b
I0816 10:41:32.801556  4780 net.cpp:252] TEST Top shape for layer 34 'res5a_branch2b' 50 512 7 7 (1254400)
I0816 10:41:32.801569  4780 layer_factory.hpp:136] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I0816 10:41:32.801574  4780 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 10:41:32.801584  4780 net.cpp:184] Created Layer res5a_branch2b/bn (35)
I0816 10:41:32.801589  4780 net.cpp:561] res5a_branch2b/bn <- res5a_branch2b
I0816 10:41:32.801592  4780 net.cpp:513] res5a_branch2b/bn -> res5a_branch2b (in-place)
I0816 10:41:32.802073  4780 net.cpp:245] Setting up res5a_branch2b/bn
I0816 10:41:32.802079  4780 net.cpp:252] TEST Top shape for layer 35 'res5a_branch2b/bn' 50 512 7 7 (1254400)
I0816 10:41:32.802086  4780 layer_factory.hpp:136] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I0816 10:41:32.802089  4780 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 10:41:32.802093  4780 net.cpp:184] Created Layer res5a_branch2b/relu (36)
I0816 10:41:32.802096  4780 net.cpp:561] res5a_branch2b/relu <- res5a_branch2b
I0816 10:41:32.802098  4780 net.cpp:513] res5a_branch2b/relu -> res5a_branch2b (in-place)
I0816 10:41:32.802104  4780 net.cpp:245] Setting up res5a_branch2b/relu
I0816 10:41:32.802108  4780 net.cpp:252] TEST Top shape for layer 36 'res5a_branch2b/relu' 50 512 7 7 (1254400)
I0816 10:41:32.802110  4780 layer_factory.hpp:136] Creating layer 'pool5' of type 'Pooling'
I0816 10:41:32.802114  4780 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 10:41:32.802117  4780 net.cpp:184] Created Layer pool5 (37)
I0816 10:41:32.802120  4780 net.cpp:561] pool5 <- res5a_branch2b
I0816 10:41:32.802124  4780 net.cpp:530] pool5 -> pool5
I0816 10:41:32.802140  4780 net.cpp:245] Setting up pool5
I0816 10:41:32.802145  4780 net.cpp:252] TEST Top shape for layer 37 'pool5' 50 512 1 1 (25600)
I0816 10:41:32.802147  4780 layer_factory.hpp:136] Creating layer 'fc1000' of type 'InnerProduct'
I0816 10:41:32.802150  4780 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 10:41:32.802156  4780 net.cpp:184] Created Layer fc1000 (38)
I0816 10:41:32.802160  4780 net.cpp:561] fc1000 <- pool5
I0816 10:41:32.802162  4780 net.cpp:530] fc1000 -> fc1000
I0816 10:41:32.812963  4780 net.cpp:245] Setting up fc1000
I0816 10:41:32.812980  4780 net.cpp:252] TEST Top shape for layer 38 'fc1000' 50 1000 (50000)
I0816 10:41:32.812999  4780 layer_factory.hpp:136] Creating layer 'fc1000_fc1000_0_split' of type 'Split'
I0816 10:41:32.813004  4780 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 10:41:32.813010  4780 net.cpp:184] Created Layer fc1000_fc1000_0_split (39)
I0816 10:41:32.813014  4780 net.cpp:561] fc1000_fc1000_0_split <- fc1000
I0816 10:41:32.813017  4780 net.cpp:530] fc1000_fc1000_0_split -> fc1000_fc1000_0_split_0
I0816 10:41:32.813021  4780 net.cpp:530] fc1000_fc1000_0_split -> fc1000_fc1000_0_split_1
I0816 10:41:32.813024  4780 net.cpp:530] fc1000_fc1000_0_split -> fc1000_fc1000_0_split_2
I0816 10:41:32.813060  4780 net.cpp:245] Setting up fc1000_fc1000_0_split
I0816 10:41:32.813063  4780 net.cpp:252] TEST Top shape for layer 39 'fc1000_fc1000_0_split' 50 1000 (50000)
I0816 10:41:32.813066  4780 net.cpp:252] TEST Top shape for layer 39 'fc1000_fc1000_0_split' 50 1000 (50000)
I0816 10:41:32.813069  4780 net.cpp:252] TEST Top shape for layer 39 'fc1000_fc1000_0_split' 50 1000 (50000)
I0816 10:41:32.813071  4780 layer_factory.hpp:136] Creating layer 'loss' of type 'SoftmaxWithLoss'
I0816 10:41:32.813076  4780 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 10:41:32.813086  4780 net.cpp:184] Created Layer loss (40)
I0816 10:41:32.813088  4780 net.cpp:561] loss <- fc1000_fc1000_0_split_0
I0816 10:41:32.813091  4780 net.cpp:561] loss <- label_data_1_split_0
I0816 10:41:32.813094  4780 net.cpp:530] loss -> loss
I0816 10:41:32.813220  4780 net.cpp:245] Setting up loss
I0816 10:41:32.813226  4780 net.cpp:252] TEST Top shape for layer 40 'loss' (1)
I0816 10:41:32.813230  4780 net.cpp:256]     with loss weight 1
I0816 10:41:32.813235  4780 layer_factory.hpp:136] Creating layer 'accuracy/top1' of type 'Accuracy'
I0816 10:41:32.813237  4780 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 10:41:32.813246  4780 net.cpp:184] Created Layer accuracy/top1 (41)
I0816 10:41:32.813249  4780 net.cpp:561] accuracy/top1 <- fc1000_fc1000_0_split_1
I0816 10:41:32.813252  4780 net.cpp:561] accuracy/top1 <- label_data_1_split_1
I0816 10:41:32.813256  4780 net.cpp:530] accuracy/top1 -> accuracy/top1
I0816 10:41:32.813261  4780 net.cpp:245] Setting up accuracy/top1
I0816 10:41:32.813266  4780 net.cpp:252] TEST Top shape for layer 41 'accuracy/top1' (1)
I0816 10:41:32.813268  4780 layer_factory.hpp:136] Creating layer 'accuracy/top5' of type 'Accuracy'
I0816 10:41:32.813271  4780 layer_factory.hpp:148] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I0816 10:41:32.813274  4780 net.cpp:184] Created Layer accuracy/top5 (42)
I0816 10:41:32.813277  4780 net.cpp:561] accuracy/top5 <- fc1000_fc1000_0_split_2
I0816 10:41:32.813279  4780 net.cpp:561] accuracy/top5 <- label_data_1_split_2
I0816 10:41:32.813282  4780 net.cpp:530] accuracy/top5 -> accuracy/top5
I0816 10:41:32.813285  4780 net.cpp:245] Setting up accuracy/top5
I0816 10:41:32.813288  4780 net.cpp:252] TEST Top shape for layer 42 'accuracy/top5' (1)
I0816 10:41:32.813292  4780 net.cpp:325] accuracy/top5 does not need backward computation.
I0816 10:41:32.813293  4780 net.cpp:325] accuracy/top1 does not need backward computation.
I0816 10:41:32.813297  4780 net.cpp:323] loss needs backward computation.
I0816 10:41:32.813298  4780 net.cpp:323] fc1000_fc1000_0_split needs backward computation.
I0816 10:41:32.813300  4780 net.cpp:323] fc1000 needs backward computation.
I0816 10:41:32.813302  4780 net.cpp:323] pool5 needs backward computation.
I0816 10:41:32.813304  4780 net.cpp:323] res5a_branch2b/relu needs backward computation.
I0816 10:41:32.813307  4780 net.cpp:323] res5a_branch2b/bn needs backward computation.
I0816 10:41:32.813308  4780 net.cpp:323] res5a_branch2b needs backward computation.
I0816 10:41:32.813310  4780 net.cpp:323] res5a_branch2a/relu needs backward computation.
I0816 10:41:32.813313  4780 net.cpp:323] res5a_branch2a/bn needs backward computation.
I0816 10:41:32.813314  4780 net.cpp:323] res5a_branch2a needs backward computation.
I0816 10:41:32.813323  4780 net.cpp:323] pool4 needs backward computation.
I0816 10:41:32.813325  4780 net.cpp:323] res4a_branch2b/relu needs backward computation.
I0816 10:41:32.813328  4780 net.cpp:323] res4a_branch2b/bn needs backward computation.
I0816 10:41:32.813329  4780 net.cpp:323] res4a_branch2b needs backward computation.
I0816 10:41:32.813333  4780 net.cpp:323] res4a_branch2a/relu needs backward computation.
I0816 10:41:32.813333  4780 net.cpp:323] res4a_branch2a/bn needs backward computation.
I0816 10:41:32.813335  4780 net.cpp:323] res4a_branch2a needs backward computation.
I0816 10:41:32.813338  4780 net.cpp:323] pool3 needs backward computation.
I0816 10:41:32.813340  4780 net.cpp:323] res3a_branch2b/relu needs backward computation.
I0816 10:41:32.813343  4780 net.cpp:323] res3a_branch2b/bn needs backward computation.
I0816 10:41:32.813344  4780 net.cpp:323] res3a_branch2b needs backward computation.
I0816 10:41:32.813346  4780 net.cpp:323] res3a_branch2a/relu needs backward computation.
I0816 10:41:32.813347  4780 net.cpp:323] res3a_branch2a/bn needs backward computation.
I0816 10:41:32.813349  4780 net.cpp:323] res3a_branch2a needs backward computation.
I0816 10:41:32.813351  4780 net.cpp:323] pool2 needs backward computation.
I0816 10:41:32.813354  4780 net.cpp:323] res2a_branch2b/relu needs backward computation.
I0816 10:41:32.813355  4780 net.cpp:323] res2a_branch2b/bn needs backward computation.
I0816 10:41:32.813357  4780 net.cpp:323] res2a_branch2b needs backward computation.
I0816 10:41:32.813359  4780 net.cpp:323] res2a_branch2a/relu needs backward computation.
I0816 10:41:32.813361  4780 net.cpp:323] res2a_branch2a/bn needs backward computation.
I0816 10:41:32.813364  4780 net.cpp:323] res2a_branch2a needs backward computation.
I0816 10:41:32.813365  4780 net.cpp:323] pool1 needs backward computation.
I0816 10:41:32.813369  4780 net.cpp:323] conv1b/relu needs backward computation.
I0816 10:41:32.813370  4780 net.cpp:323] conv1b/bn needs backward computation.
I0816 10:41:32.813372  4780 net.cpp:323] conv1b needs backward computation.
I0816 10:41:32.813374  4780 net.cpp:323] conv1a/relu needs backward computation.
I0816 10:41:32.813376  4780 net.cpp:323] conv1a/bn needs backward computation.
I0816 10:41:32.813377  4780 net.cpp:323] conv1a needs backward computation.
I0816 10:41:32.813380  4780 net.cpp:325] data/bias does not need backward computation.
I0816 10:41:32.813382  4780 net.cpp:325] label_data_1_split does not need backward computation.
I0816 10:41:32.813386  4780 net.cpp:325] data does not need backward computation.
I0816 10:41:32.813388  4780 net.cpp:367] This network produces output accuracy/top1
I0816 10:41:32.813390  4780 net.cpp:367] This network produces output accuracy/top5
I0816 10:41:32.813392  4780 net.cpp:367] This network produces output loss
I0816 10:41:32.813421  4780 net.cpp:389] Top memory (TEST) required for data: 933273600 diff: 8
I0816 10:41:32.813424  4780 net.cpp:392] Bottom memory (TEST) required for data: 933273600 diff: 933273600
I0816 10:41:32.813427  4780 net.cpp:395] Shared (in-place) memory (TEST) by data: 622182400 diff: 622182400
I0816 10:41:32.813428  4780 net.cpp:398] Parameters memory (TEST) required for data: 9450960 diff: 9450960
I0816 10:41:32.813431  4780 net.cpp:401] Parameters shared memory (TEST) by data: 0 diff: 0
I0816 10:41:32.813432  4780 net.cpp:407] Network initialization done.
I0816 10:41:32.887840  4780 net.cpp:1095] Copying source layer data Type:Data #blobs=0
I0816 10:41:32.887861  4780 net.cpp:1095] Copying source layer data/bias Type:Bias #blobs=1
I0816 10:41:32.887897  4780 net.cpp:1095] Copying source layer conv1a Type:Convolution #blobs=2
I0816 10:41:32.887910  4780 net.cpp:1095] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I0816 10:41:32.888084  4780 net.cpp:1095] Copying source layer conv1a/relu Type:ReLU #blobs=0
I0816 10:41:32.888090  4780 net.cpp:1095] Copying source layer conv1b Type:Convolution #blobs=2
I0816 10:41:32.888103  4780 net.cpp:1095] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I0816 10:41:32.888236  4780 net.cpp:1095] Copying source layer conv1b/relu Type:ReLU #blobs=0
I0816 10:41:32.888243  4780 net.cpp:1095] Copying source layer pool1 Type:Pooling #blobs=0
I0816 10:41:32.888247  4780 net.cpp:1095] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I0816 10:41:32.888264  4780 net.cpp:1095] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I0816 10:41:32.888376  4780 net.cpp:1095] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I0816 10:41:32.888381  4780 net.cpp:1095] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I0816 10:41:32.888396  4780 net.cpp:1095] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I0816 10:41:32.888509  4780 net.cpp:1095] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I0816 10:41:32.888515  4780 net.cpp:1095] Copying source layer pool2 Type:Pooling #blobs=0
I0816 10:41:32.888520  4780 net.cpp:1095] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I0816 10:41:32.888558  4780 net.cpp:1095] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I0816 10:41:32.888664  4780 net.cpp:1095] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I0816 10:41:32.888669  4780 net.cpp:1095] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I0816 10:41:32.888694  4780 net.cpp:1095] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I0816 10:41:32.888788  4780 net.cpp:1095] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I0816 10:41:32.888794  4780 net.cpp:1095] Copying source layer pool3 Type:Pooling #blobs=0
I0816 10:41:32.888798  4780 net.cpp:1095] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I0816 10:41:32.888911  4780 net.cpp:1095] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I0816 10:41:32.889011  4780 net.cpp:1095] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I0816 10:41:32.889016  4780 net.cpp:1095] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I0816 10:41:32.889080  4780 net.cpp:1095] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I0816 10:41:32.889180  4780 net.cpp:1095] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I0816 10:41:32.889186  4780 net.cpp:1095] Copying source layer pool4 Type:Pooling #blobs=0
I0816 10:41:32.889190  4780 net.cpp:1095] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I0816 10:41:32.889565  4780 net.cpp:1095] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I0816 10:41:32.889675  4780 net.cpp:1095] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I0816 10:41:32.889681  4780 net.cpp:1095] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I0816 10:41:32.889865  4780 net.cpp:1095] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I0816 10:41:32.889961  4780 net.cpp:1095] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I0816 10:41:32.889967  4780 net.cpp:1095] Copying source layer pool5 Type:Pooling #blobs=0
I0816 10:41:32.889971  4780 net.cpp:1095] Copying source layer fc1000 Type:InnerProduct #blobs=2
I0816 10:41:32.890100  4780 net.cpp:1095] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I0816 10:41:32.890161  4780 caffe.cpp:290] Running for 1000 iterations.
I0816 10:41:32.896109  4780 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.02G/1 1  (limit 7.68G, req 0G)
I0816 10:41:32.909580  4780 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 0.02G/2 6  (limit 7.52G, req 0G)
I0816 10:41:32.925426  4780 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 0.02G/1 1  (limit 7.33G, req 0G)
I0816 10:41:32.932736  4780 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 0.02G/2 6  (limit 7.25G, req 0G)
I0816 10:41:32.944617  4780 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 0.02G/1 6  (limit 7.15G, req 0G)
I0816 10:41:32.949653  4780 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 0.02G/2 6  (limit 7.11G, req 0G)
I0816 10:41:32.958955  4780 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 0.02G/1 6  (limit 7.06G, req 0G)
I0816 10:41:32.963837  4780 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 0.02G/2 6  (limit 7.04G, req 0G)
I0816 10:41:32.972712  4780 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2a' with space 0.02G/1 1  (limit 7.02G, req 0G)
I0816 10:41:32.977346  4780 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2b' with space 0.02G/2 6  (limit 7G, req 0G)
I0816 10:41:33.012751  4780 caffe.cpp:313] Batch 0, accuracy/top1 = 0.56
I0816 10:41:33.012778  4780 caffe.cpp:313] Batch 0, accuracy/top5 = 0.8
I0816 10:41:33.012781  4780 caffe.cpp:313] Batch 0, loss = 1.76059
I0816 10:41:33.012784  4780 net.cpp:1620] Adding quantization params at infer/iter index: 1
I0816 10:41:33.020959  4780 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1a' with space 0.74G/1 1  (limit 6.25G, req 0G)
I0816 10:41:33.040329  4780 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'conv1b' with space 1.48G/2 6  (limit 5.51G, req 0G)
I0816 10:41:33.064442  4780 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2a' with space 1.48G/1 1  (limit 5.51G, req 0G)
I0816 10:41:33.074771  4780 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res2a_branch2b' with space 1.48G/2 6  (limit 5.51G, req 0G)
I0816 10:41:33.089224  4780 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2a' with space 1.48G/1 6  (limit 5.51G, req 0G)
I0816 10:41:33.095278  4780 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res3a_branch2b' with space 1.48G/2 6  (limit 5.51G, req 0G)
I0816 10:41:33.108670  4780 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2a' with space 1.48G/1 6  (limit 5.51G, req 0G)
I0816 10:41:33.114042  4780 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res4a_branch2b' with space 1.48G/2 6  (limit 5.51G, req 0G)
I0816 10:41:33.133347  4780 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2a' with space 1.48G/1 7  (limit 5.51G, req 0.05G)
I0816 10:41:33.138979  4780 cudnn_conv_layer.cpp:1010] (0) Conv Algo (F): 'res5a_branch2b' with space 1.48G/2 6  (limit 5.51G, req 0.05G)
I0816 10:41:33.170783  4780 caffe.cpp:313] Batch 1, accuracy/top1 = 0.04
I0816 10:41:33.170806  4780 caffe.cpp:313] Batch 1, accuracy/top5 = 0.08
I0816 10:41:33.170809  4780 caffe.cpp:313] Batch 1, loss = 6.39309
I0816 10:41:33.233657  4780 caffe.cpp:313] Batch 2, accuracy/top1 = 0.02
I0816 10:41:33.233677  4780 caffe.cpp:313] Batch 2, accuracy/top5 = 0.04
I0816 10:41:33.233680  4780 caffe.cpp:313] Batch 2, loss = 6.85953
I0816 10:41:33.296779  4780 caffe.cpp:313] Batch 3, accuracy/top1 = 0.02
I0816 10:41:33.296800  4780 caffe.cpp:313] Batch 3, accuracy/top5 = 0.06
I0816 10:41:33.296803  4780 caffe.cpp:313] Batch 3, loss = 6.57253
I0816 10:41:33.359609  4780 caffe.cpp:313] Batch 4, accuracy/top1 = 0.02
I0816 10:41:33.359630  4780 caffe.cpp:313] Batch 4, accuracy/top5 = 0.04
I0816 10:41:33.359634  4780 caffe.cpp:313] Batch 4, loss = 6.7196
I0816 10:41:33.422576  4780 caffe.cpp:313] Batch 5, accuracy/top1 = 0.04
I0816 10:41:33.422598  4780 caffe.cpp:313] Batch 5, accuracy/top5 = 0.08
I0816 10:41:33.422601  4780 caffe.cpp:313] Batch 5, loss = 6.38219
I0816 10:41:33.485453  4780 caffe.cpp:313] Batch 6, accuracy/top1 = 0
I0816 10:41:33.485476  4780 caffe.cpp:313] Batch 6, accuracy/top5 = 0.02
I0816 10:41:33.485479  4780 caffe.cpp:313] Batch 6, loss = 6.78995
I0816 10:41:33.548310  4780 caffe.cpp:313] Batch 7, accuracy/top1 = 0
I0816 10:41:33.548332  4780 caffe.cpp:313] Batch 7, accuracy/top5 = 0.02
I0816 10:41:33.548336  4780 caffe.cpp:313] Batch 7, loss = 6.76213
I0816 10:41:33.611245  4780 caffe.cpp:313] Batch 8, accuracy/top1 = 0
I0816 10:41:33.611268  4780 caffe.cpp:313] Batch 8, accuracy/top5 = 0
I0816 10:41:33.611270  4780 caffe.cpp:313] Batch 8, loss = 6.9369
I0816 10:41:33.674190  4780 caffe.cpp:313] Batch 9, accuracy/top1 = 0
I0816 10:41:33.674211  4780 caffe.cpp:313] Batch 9, accuracy/top5 = 0.04
I0816 10:41:33.674214  4780 caffe.cpp:313] Batch 9, loss = 6.83217
I0816 10:41:33.737052  4780 caffe.cpp:313] Batch 10, accuracy/top1 = 0.02
I0816 10:41:33.737073  4780 caffe.cpp:313] Batch 10, accuracy/top5 = 0.02
I0816 10:41:33.737076  4780 caffe.cpp:313] Batch 10, loss = 7.03034
I0816 10:41:33.799996  4780 caffe.cpp:313] Batch 11, accuracy/top1 = 0.04
I0816 10:41:33.800017  4780 caffe.cpp:313] Batch 11, accuracy/top5 = 0.04
I0816 10:41:33.800020  4780 caffe.cpp:313] Batch 11, loss = 6.52805
I0816 10:41:33.862934  4780 caffe.cpp:313] Batch 12, accuracy/top1 = 0
I0816 10:41:33.862956  4780 caffe.cpp:313] Batch 12, accuracy/top5 = 0.04
I0816 10:41:33.862959  4780 caffe.cpp:313] Batch 12, loss = 6.52774
I0816 10:41:33.925833  4780 caffe.cpp:313] Batch 13, accuracy/top1 = 0.02
I0816 10:41:33.925854  4780 caffe.cpp:313] Batch 13, accuracy/top5 = 0.06
I0816 10:41:33.925858  4780 caffe.cpp:313] Batch 13, loss = 6.38895
I0816 10:41:33.988886  4780 caffe.cpp:313] Batch 14, accuracy/top1 = 0
I0816 10:41:33.988901  4780 caffe.cpp:313] Batch 14, accuracy/top5 = 0.02
I0816 10:41:33.988905  4780 caffe.cpp:313] Batch 14, loss = 7.13626
I0816 10:41:34.051890  4780 caffe.cpp:313] Batch 15, accuracy/top1 = 0.04
I0816 10:41:34.051909  4780 caffe.cpp:313] Batch 15, accuracy/top5 = 0.04
I0816 10:41:34.051913  4780 caffe.cpp:313] Batch 15, loss = 6.64636
I0816 10:41:34.114761  4780 caffe.cpp:313] Batch 16, accuracy/top1 = 0
I0816 10:41:34.114783  4780 caffe.cpp:313] Batch 16, accuracy/top5 = 0.06
I0816 10:41:34.114786  4780 caffe.cpp:313] Batch 16, loss = 6.88673
I0816 10:41:34.177546  4780 caffe.cpp:313] Batch 17, accuracy/top1 = 0
I0816 10:41:34.177567  4780 caffe.cpp:313] Batch 17, accuracy/top5 = 0.02
I0816 10:41:34.177572  4780 caffe.cpp:313] Batch 17, loss = 6.87315
I0816 10:41:34.240449  4780 caffe.cpp:313] Batch 18, accuracy/top1 = 0.02
I0816 10:41:34.240470  4780 caffe.cpp:313] Batch 18, accuracy/top5 = 0.04
I0816 10:41:34.240473  4780 caffe.cpp:313] Batch 18, loss = 6.9762
I0816 10:41:34.303267  4780 caffe.cpp:313] Batch 19, accuracy/top1 = 0
I0816 10:41:34.303287  4780 caffe.cpp:313] Batch 19, accuracy/top5 = 0.02
I0816 10:41:34.303292  4780 caffe.cpp:313] Batch 19, loss = 6.78835
I0816 10:41:34.366278  4780 caffe.cpp:313] Batch 20, accuracy/top1 = 0
I0816 10:41:34.366300  4780 caffe.cpp:313] Batch 20, accuracy/top5 = 0.02
I0816 10:41:34.366304  4780 caffe.cpp:313] Batch 20, loss = 6.90261
I0816 10:41:34.429091  4780 caffe.cpp:313] Batch 21, accuracy/top1 = 0.04
I0816 10:41:34.429111  4780 caffe.cpp:313] Batch 21, accuracy/top5 = 0.04
I0816 10:41:34.429114  4780 caffe.cpp:313] Batch 21, loss = 6.53161
I0816 10:41:34.491917  4780 caffe.cpp:313] Batch 22, accuracy/top1 = 0.04
I0816 10:41:34.491938  4780 caffe.cpp:313] Batch 22, accuracy/top5 = 0.06
I0816 10:41:34.491941  4780 caffe.cpp:313] Batch 22, loss = 6.67881
I0816 10:41:34.554858  4780 caffe.cpp:313] Batch 23, accuracy/top1 = 0
I0816 10:41:34.554878  4780 caffe.cpp:313] Batch 23, accuracy/top5 = 0.02
I0816 10:41:34.554883  4780 caffe.cpp:313] Batch 23, loss = 6.91076
I0816 10:41:34.617708  4780 caffe.cpp:313] Batch 24, accuracy/top1 = 0.02
I0816 10:41:34.617730  4780 caffe.cpp:313] Batch 24, accuracy/top5 = 0.04
I0816 10:41:34.617733  4780 caffe.cpp:313] Batch 24, loss = 6.50534
I0816 10:41:34.680625  4780 caffe.cpp:313] Batch 25, accuracy/top1 = 0.02
I0816 10:41:34.680647  4780 caffe.cpp:313] Batch 25, accuracy/top5 = 0.04
I0816 10:41:34.680650  4780 caffe.cpp:313] Batch 25, loss = 6.68518
I0816 10:41:34.743351  4780 caffe.cpp:313] Batch 26, accuracy/top1 = 0.04
I0816 10:41:34.743373  4780 caffe.cpp:313] Batch 26, accuracy/top5 = 0.06
I0816 10:41:34.743377  4780 caffe.cpp:313] Batch 26, loss = 6.63346
I0816 10:41:34.806165  4780 caffe.cpp:313] Batch 27, accuracy/top1 = 0.02
I0816 10:41:34.806186  4780 caffe.cpp:313] Batch 27, accuracy/top5 = 0.06
I0816 10:41:34.806190  4780 caffe.cpp:313] Batch 27, loss = 6.66639
I0816 10:41:34.869109  4780 caffe.cpp:313] Batch 28, accuracy/top1 = 0
I0816 10:41:34.869127  4780 caffe.cpp:313] Batch 28, accuracy/top5 = 0.02
I0816 10:41:34.869130  4780 caffe.cpp:313] Batch 28, loss = 7.13374
I0816 10:41:34.931964  4780 caffe.cpp:313] Batch 29, accuracy/top1 = 0
I0816 10:41:34.931985  4780 caffe.cpp:313] Batch 29, accuracy/top5 = 0.02
I0816 10:41:34.931988  4780 caffe.cpp:313] Batch 29, loss = 6.69644
I0816 10:41:34.995213  4780 caffe.cpp:313] Batch 30, accuracy/top1 = 0
I0816 10:41:34.995230  4780 caffe.cpp:313] Batch 30, accuracy/top5 = 0.02
I0816 10:41:34.995234  4780 caffe.cpp:313] Batch 30, loss = 6.90523
I0816 10:41:35.058148  4780 caffe.cpp:313] Batch 31, accuracy/top1 = 0.02
I0816 10:41:35.058167  4780 caffe.cpp:313] Batch 31, accuracy/top5 = 0.04
I0816 10:41:35.058171  4780 caffe.cpp:313] Batch 31, loss = 6.78393
I0816 10:41:35.120980  4780 caffe.cpp:313] Batch 32, accuracy/top1 = 0.04
I0816 10:41:35.120999  4780 caffe.cpp:313] Batch 32, accuracy/top5 = 0.04
I0816 10:41:35.121002  4780 caffe.cpp:313] Batch 32, loss = 6.71631
I0816 10:41:35.183887  4780 caffe.cpp:313] Batch 33, accuracy/top1 = 0
I0816 10:41:35.183907  4780 caffe.cpp:313] Batch 33, accuracy/top5 = 0.02
I0816 10:41:35.183912  4780 caffe.cpp:313] Batch 33, loss = 6.9588
I0816 10:41:35.246732  4780 caffe.cpp:313] Batch 34, accuracy/top1 = 0.02
I0816 10:41:35.246754  4780 caffe.cpp:313] Batch 34, accuracy/top5 = 0.04
I0816 10:41:35.246757  4780 caffe.cpp:313] Batch 34, loss = 6.72803
I0816 10:41:35.309518  4780 caffe.cpp:313] Batch 35, accuracy/top1 = 0.02
I0816 10:41:35.309538  4780 caffe.cpp:313] Batch 35, accuracy/top5 = 0.06
I0816 10:41:35.309541  4780 caffe.cpp:313] Batch 35, loss = 6.5651
I0816 10:41:35.372401  4780 caffe.cpp:313] Batch 36, accuracy/top1 = 0.02
I0816 10:41:35.372421  4780 caffe.cpp:313] Batch 36, accuracy/top5 = 0.06
I0816 10:41:35.372423  4780 caffe.cpp:313] Batch 36, loss = 6.63975
I0816 10:41:35.435575  4780 caffe.cpp:313] Batch 37, accuracy/top1 = 0
I0816 10:41:35.435592  4780 caffe.cpp:313] Batch 37, accuracy/top5 = 0.02
I0816 10:41:35.435596  4780 caffe.cpp:313] Batch 37, loss = 6.90154
I0816 10:41:35.498560  4780 caffe.cpp:313] Batch 38, accuracy/top1 = 0
I0816 10:41:35.498580  4780 caffe.cpp:313] Batch 38, accuracy/top5 = 0
I0816 10:41:35.498584  4780 caffe.cpp:313] Batch 38, loss = 6.79664
I0816 10:41:35.561314  4780 caffe.cpp:313] Batch 39, accuracy/top1 = 0
I0816 10:41:35.561336  4780 caffe.cpp:313] Batch 39, accuracy/top5 = 0.02
I0816 10:41:35.561339  4780 caffe.cpp:313] Batch 39, loss = 6.75582
I0816 10:41:35.624256  4780 caffe.cpp:313] Batch 40, accuracy/top1 = 0
I0816 10:41:35.624279  4780 caffe.cpp:313] Batch 40, accuracy/top5 = 0.02
I0816 10:41:35.624281  4780 caffe.cpp:313] Batch 40, loss = 6.96982
I0816 10:41:35.687235  4780 caffe.cpp:313] Batch 41, accuracy/top1 = 0.02
I0816 10:41:35.687258  4780 caffe.cpp:313] Batch 41, accuracy/top5 = 0.04
I0816 10:41:35.687260  4780 caffe.cpp:313] Batch 41, loss = 6.88476
I0816 10:41:35.750064  4780 caffe.cpp:313] Batch 42, accuracy/top1 = 0.02
I0816 10:41:35.750085  4780 caffe.cpp:313] Batch 42, accuracy/top5 = 0.02
I0816 10:41:35.750088  4780 caffe.cpp:313] Batch 42, loss = 6.73154
I0816 10:41:35.812770  4780 caffe.cpp:313] Batch 43, accuracy/top1 = 0.04
I0816 10:41:35.812791  4780 caffe.cpp:313] Batch 43, accuracy/top5 = 0.1
I0816 10:41:35.812795  4780 caffe.cpp:313] Batch 43, loss = 6.52047
I0816 10:41:35.875540  4780 caffe.cpp:313] Batch 44, accuracy/top1 = 0.02
I0816 10:41:35.875562  4780 caffe.cpp:313] Batch 44, accuracy/top5 = 0.02
I0816 10:41:35.875566  4780 caffe.cpp:313] Batch 44, loss = 6.76825
I0816 10:41:35.938455  4780 caffe.cpp:313] Batch 45, accuracy/top1 = 0
I0816 10:41:35.938475  4780 caffe.cpp:313] Batch 45, accuracy/top5 = 0
I0816 10:41:35.938479  4780 caffe.cpp:313] Batch 45, loss = 6.63504
I0816 10:41:36.001446  4780 caffe.cpp:313] Batch 46, accuracy/top1 = 0.02
I0816 10:41:36.001463  4780 caffe.cpp:313] Batch 46, accuracy/top5 = 0.06
I0816 10:41:36.001466  4780 caffe.cpp:313] Batch 46, loss = 6.51433
I0816 10:41:36.064363  4780 caffe.cpp:313] Batch 47, accuracy/top1 = 0
I0816 10:41:36.064383  4780 caffe.cpp:313] Batch 47, accuracy/top5 = 0
I0816 10:41:36.064385  4780 caffe.cpp:313] Batch 47, loss = 6.67375
I0816 10:41:36.127126  4780 caffe.cpp:313] Batch 48, accuracy/top1 = 0
I0816 10:41:36.127161  4780 caffe.cpp:313] Batch 48, accuracy/top5 = 0.02
I0816 10:41:36.127164  4780 caffe.cpp:313] Batch 48, loss = 6.51088
I0816 10:41:36.189954  4780 caffe.cpp:313] Batch 49, accuracy/top1 = 0
I0816 10:41:36.189975  4780 caffe.cpp:313] Batch 49, accuracy/top5 = 0.02
I0816 10:41:36.189978  4780 caffe.cpp:313] Batch 49, loss = 6.72732
I0816 10:41:36.252799  4780 caffe.cpp:313] Batch 50, accuracy/top1 = 0.04
I0816 10:41:36.252820  4780 caffe.cpp:313] Batch 50, accuracy/top5 = 0.08
I0816 10:41:36.252822  4780 caffe.cpp:313] Batch 50, loss = 6.70924
I0816 10:41:36.315693  4780 caffe.cpp:313] Batch 51, accuracy/top1 = 0
I0816 10:41:36.315714  4780 caffe.cpp:313] Batch 51, accuracy/top5 = 0.06
I0816 10:41:36.315717  4780 caffe.cpp:313] Batch 51, loss = 6.92068
I0816 10:41:36.378489  4780 caffe.cpp:313] Batch 52, accuracy/top1 = 0
I0816 10:41:36.378509  4780 caffe.cpp:313] Batch 52, accuracy/top5 = 0.04
I0816 10:41:36.378513  4780 caffe.cpp:313] Batch 52, loss = 6.99774
I0816 10:41:36.441365  4780 caffe.cpp:313] Batch 53, accuracy/top1 = 0.02
I0816 10:41:36.441386  4780 caffe.cpp:313] Batch 53, accuracy/top5 = 0.02
I0816 10:41:36.441390  4780 caffe.cpp:313] Batch 53, loss = 6.66647
I0816 10:41:36.504300  4780 caffe.cpp:313] Batch 54, accuracy/top1 = 0.02
I0816 10:41:36.504321  4780 caffe.cpp:313] Batch 54, accuracy/top5 = 0.02
I0816 10:41:36.504324  4780 caffe.cpp:313] Batch 54, loss = 6.66666
I0816 10:41:36.567155  4780 caffe.cpp:313] Batch 55, accuracy/top1 = 0
I0816 10:41:36.567176  4780 caffe.cpp:313] Batch 55, accuracy/top5 = 0.02
I0816 10:41:36.567180  4780 caffe.cpp:313] Batch 55, loss = 6.82552
I0816 10:41:36.630028  4780 caffe.cpp:313] Batch 56, accuracy/top1 = 0.02
I0816 10:41:36.630049  4780 caffe.cpp:313] Batch 56, accuracy/top5 = 0.04
I0816 10:41:36.630053  4780 caffe.cpp:313] Batch 56, loss = 6.60413
I0816 10:41:36.692814  4780 caffe.cpp:313] Batch 57, accuracy/top1 = 0
I0816 10:41:36.692834  4780 caffe.cpp:313] Batch 57, accuracy/top5 = 0
I0816 10:41:36.692837  4780 caffe.cpp:313] Batch 57, loss = 6.72393
I0816 10:41:36.755662  4780 caffe.cpp:313] Batch 58, accuracy/top1 = 0
I0816 10:41:36.755683  4780 caffe.cpp:313] Batch 58, accuracy/top5 = 0
I0816 10:41:36.755686  4780 caffe.cpp:313] Batch 58, loss = 6.75915
I0816 10:41:36.818593  4780 caffe.cpp:313] Batch 59, accuracy/top1 = 0.02
I0816 10:41:36.818614  4780 caffe.cpp:313] Batch 59, accuracy/top5 = 0.06
I0816 10:41:36.818617  4780 caffe.cpp:313] Batch 59, loss = 6.52408
I0816 10:41:36.881436  4780 caffe.cpp:313] Batch 60, accuracy/top1 = 0.02
I0816 10:41:36.881453  4780 caffe.cpp:313] Batch 60, accuracy/top5 = 0.04
I0816 10:41:36.881455  4780 caffe.cpp:313] Batch 60, loss = 6.87797
I0816 10:41:36.944206  4780 caffe.cpp:313] Batch 61, accuracy/top1 = 0
I0816 10:41:36.944227  4780 caffe.cpp:313] Batch 61, accuracy/top5 = 0.02
I0816 10:41:36.944231  4780 caffe.cpp:313] Batch 61, loss = 6.74458
I0816 10:41:37.007191  4780 caffe.cpp:313] Batch 62, accuracy/top1 = 0.02
I0816 10:41:37.007212  4780 caffe.cpp:313] Batch 62, accuracy/top5 = 0.04
I0816 10:41:37.007215  4780 caffe.cpp:313] Batch 62, loss = 6.61693
I0816 10:41:37.070214  4780 caffe.cpp:313] Batch 63, accuracy/top1 = 0
I0816 10:41:37.070233  4780 caffe.cpp:313] Batch 63, accuracy/top5 = 0.06
I0816 10:41:37.070237  4780 caffe.cpp:313] Batch 63, loss = 6.77911
I0816 10:41:37.133127  4780 caffe.cpp:313] Batch 64, accuracy/top1 = 0
I0816 10:41:37.133147  4780 caffe.cpp:313] Batch 64, accuracy/top5 = 0
I0816 10:41:37.133150  4780 caffe.cpp:313] Batch 64, loss = 6.85652
I0816 10:41:37.196032  4780 caffe.cpp:313] Batch 65, accuracy/top1 = 0
I0816 10:41:37.196053  4780 caffe.cpp:313] Batch 65, accuracy/top5 = 0.02
I0816 10:41:37.196056  4780 caffe.cpp:313] Batch 65, loss = 7.08748
I0816 10:41:37.258956  4780 caffe.cpp:313] Batch 66, accuracy/top1 = 0
I0816 10:41:37.258977  4780 caffe.cpp:313] Batch 66, accuracy/top5 = 0.06
I0816 10:41:37.258981  4780 caffe.cpp:313] Batch 66, loss = 6.5883
I0816 10:41:37.321799  4780 caffe.cpp:313] Batch 67, accuracy/top1 = 0.02
I0816 10:41:37.321832  4780 caffe.cpp:313] Batch 67, accuracy/top5 = 0.04
I0816 10:41:37.321836  4780 caffe.cpp:313] Batch 67, loss = 6.55684
I0816 10:41:37.384590  4780 caffe.cpp:313] Batch 68, accuracy/top1 = 0
I0816 10:41:37.384611  4780 caffe.cpp:313] Batch 68, accuracy/top5 = 0.02
I0816 10:41:37.384615  4780 caffe.cpp:313] Batch 68, loss = 6.92724
I0816 10:41:37.447510  4780 caffe.cpp:313] Batch 69, accuracy/top1 = 0
I0816 10:41:37.447530  4780 caffe.cpp:313] Batch 69, accuracy/top5 = 0
I0816 10:41:37.447533  4780 caffe.cpp:313] Batch 69, loss = 6.70257
I0816 10:41:37.510298  4780 caffe.cpp:313] Batch 70, accuracy/top1 = 0.02
I0816 10:41:37.510318  4780 caffe.cpp:313] Batch 70, accuracy/top5 = 0.04
I0816 10:41:37.510320  4780 caffe.cpp:313] Batch 70, loss = 6.74436
I0816 10:41:37.573369  4780 caffe.cpp:313] Batch 71, accuracy/top1 = 0.02
I0816 10:41:37.573386  4780 caffe.cpp:313] Batch 71, accuracy/top5 = 0.02
I0816 10:41:37.573390  4780 caffe.cpp:313] Batch 71, loss = 6.80307
I0816 10:41:37.636339  4780 caffe.cpp:313] Batch 72, accuracy/top1 = 0
I0816 10:41:37.636359  4780 caffe.cpp:313] Batch 72, accuracy/top5 = 0.04
I0816 10:41:37.636363  4780 caffe.cpp:313] Batch 72, loss = 6.75294
I0816 10:41:37.699239  4780 caffe.cpp:313] Batch 73, accuracy/top1 = 0.02
I0816 10:41:37.699259  4780 caffe.cpp:313] Batch 73, accuracy/top5 = 0.02
I0816 10:41:37.699262  4780 caffe.cpp:313] Batch 73, loss = 6.82386
I0816 10:41:37.761986  4780 caffe.cpp:313] Batch 74, accuracy/top1 = 0
I0816 10:41:37.762006  4780 caffe.cpp:313] Batch 74, accuracy/top5 = 0.04
I0816 10:41:37.762011  4780 caffe.cpp:313] Batch 74, loss = 6.87908
I0816 10:41:37.824887  4780 caffe.cpp:313] Batch 75, accuracy/top1 = 0.02
I0816 10:41:37.824908  4780 caffe.cpp:313] Batch 75, accuracy/top5 = 0.04
I0816 10:41:37.824910  4780 caffe.cpp:313] Batch 75, loss = 6.82397
I0816 10:41:37.887830  4780 caffe.cpp:313] Batch 76, accuracy/top1 = 0
I0816 10:41:37.887850  4780 caffe.cpp:313] Batch 76, accuracy/top5 = 0.04
I0816 10:41:37.887853  4780 caffe.cpp:313] Batch 76, loss = 6.60735
I0816 10:41:37.950652  4780 caffe.cpp:313] Batch 77, accuracy/top1 = 0.02
I0816 10:41:37.950672  4780 caffe.cpp:313] Batch 77, accuracy/top5 = 0.02
I0816 10:41:37.950675  4780 caffe.cpp:313] Batch 77, loss = 6.46479
I0816 10:41:38.013648  4780 caffe.cpp:313] Batch 78, accuracy/top1 = 0
I0816 10:41:38.013669  4780 caffe.cpp:313] Batch 78, accuracy/top5 = 0.06
I0816 10:41:38.013674  4780 caffe.cpp:313] Batch 78, loss = 6.95148
I0816 10:41:38.076576  4780 caffe.cpp:313] Batch 79, accuracy/top1 = 0.04
I0816 10:41:38.076597  4780 caffe.cpp:313] Batch 79, accuracy/top5 = 0.06
I0816 10:41:38.076601  4780 caffe.cpp:313] Batch 79, loss = 6.49811
I0816 10:41:38.139375  4780 caffe.cpp:313] Batch 80, accuracy/top1 = 0
I0816 10:41:38.139396  4780 caffe.cpp:313] Batch 80, accuracy/top5 = 0.02
I0816 10:41:38.139400  4780 caffe.cpp:313] Batch 80, loss = 6.65541
I0816 10:41:38.202210  4780 caffe.cpp:313] Batch 81, accuracy/top1 = 0.02
I0816 10:41:38.202231  4780 caffe.cpp:313] Batch 81, accuracy/top5 = 0.04
I0816 10:41:38.202234  4780 caffe.cpp:313] Batch 81, loss = 6.67308
I0816 10:41:38.265017  4780 caffe.cpp:313] Batch 82, accuracy/top1 = 0.02
I0816 10:41:38.265038  4780 caffe.cpp:313] Batch 82, accuracy/top5 = 0.04
I0816 10:41:38.265041  4780 caffe.cpp:313] Batch 82, loss = 6.7849
I0816 10:41:38.327883  4780 caffe.cpp:313] Batch 83, accuracy/top1 = 0.02
I0816 10:41:38.327904  4780 caffe.cpp:313] Batch 83, accuracy/top5 = 0.02
I0816 10:41:38.327908  4780 caffe.cpp:313] Batch 83, loss = 6.76723
I0816 10:41:38.390707  4780 caffe.cpp:313] Batch 84, accuracy/top1 = 0
I0816 10:41:38.390727  4780 caffe.cpp:313] Batch 84, accuracy/top5 = 0.02
I0816 10:41:38.390730  4780 caffe.cpp:313] Batch 84, loss = 6.79074
I0816 10:41:38.453692  4780 caffe.cpp:313] Batch 85, accuracy/top1 = 0
I0816 10:41:38.453713  4780 caffe.cpp:313] Batch 85, accuracy/top5 = 0.02
I0816 10:41:38.453717  4780 caffe.cpp:313] Batch 85, loss = 6.93379
I0816 10:41:38.516398  4780 caffe.cpp:313] Batch 86, accuracy/top1 = 0.02
I0816 10:41:38.516419  4780 caffe.cpp:313] Batch 86, accuracy/top5 = 0.04
I0816 10:41:38.516435  4780 caffe.cpp:313] Batch 86, loss = 6.89344
I0816 10:41:38.579205  4780 caffe.cpp:313] Batch 87, accuracy/top1 = 0
I0816 10:41:38.579226  4780 caffe.cpp:313] Batch 87, accuracy/top5 = 0.04
I0816 10:41:38.579229  4780 caffe.cpp:313] Batch 87, loss = 6.76605
I0816 10:41:38.642024  4780 caffe.cpp:313] Batch 88, accuracy/top1 = 0.02
I0816 10:41:38.642045  4780 caffe.cpp:313] Batch 88, accuracy/top5 = 0.04
I0816 10:41:38.642047  4780 caffe.cpp:313] Batch 88, loss = 6.7903
I0816 10:41:38.704844  4780 caffe.cpp:313] Batch 89, accuracy/top1 = 0.02
I0816 10:41:38.704865  4780 caffe.cpp:313] Batch 89, accuracy/top5 = 0.02
I0816 10:41:38.704869  4780 caffe.cpp:313] Batch 89, loss = 6.89452
I0816 10:41:38.767575  4780 caffe.cpp:313] Batch 90, accuracy/top1 = 0.02
I0816 10:41:38.767596  4780 caffe.cpp:313] Batch 90, accuracy/top5 = 0.04
I0816 10:41:38.767599  4780 caffe.cpp:313] Batch 90, loss = 6.86053
I0816 10:41:38.830307  4780 caffe.cpp:313] Batch 91, accuracy/top1 = 0.02
I0816 10:41:38.830328  4780 caffe.cpp:313] Batch 91, accuracy/top5 = 0.06
I0816 10:41:38.830332  4780 caffe.cpp:313] Batch 91, loss = 6.70264
I0816 10:41:38.893059  4780 caffe.cpp:313] Batch 92, accuracy/top1 = 0
I0816 10:41:38.893074  4780 caffe.cpp:313] Batch 92, accuracy/top5 = 0
I0816 10:41:38.893077  4780 caffe.cpp:313] Batch 92, loss = 6.9859
I0816 10:41:38.956034  4780 caffe.cpp:313] Batch 93, accuracy/top1 = 0
I0816 10:41:38.956059  4780 caffe.cpp:313] Batch 93, accuracy/top5 = 0.02
I0816 10:41:38.956064  4780 caffe.cpp:313] Batch 93, loss = 6.88291
I0816 10:41:39.019062  4780 caffe.cpp:313] Batch 94, accuracy/top1 = 0
I0816 10:41:39.019085  4780 caffe.cpp:313] Batch 94, accuracy/top5 = 0
I0816 10:41:39.019089  4780 caffe.cpp:313] Batch 94, loss = 6.96657
I0816 10:41:39.081833  4780 caffe.cpp:313] Batch 95, accuracy/top1 = 0.02
I0816 10:41:39.081854  4780 caffe.cpp:313] Batch 95, accuracy/top5 = 0.08
I0816 10:41:39.081858  4780 caffe.cpp:313] Batch 95, loss = 6.68132
I0816 10:41:39.144551  4780 caffe.cpp:313] Batch 96, accuracy/top1 = 0
I0816 10:41:39.144573  4780 caffe.cpp:313] Batch 96, accuracy/top5 = 0.04
I0816 10:41:39.144578  4780 caffe.cpp:313] Batch 96, loss = 6.59279
I0816 10:41:39.207495  4780 caffe.cpp:313] Batch 97, accuracy/top1 = 0
I0816 10:41:39.207516  4780 caffe.cpp:313] Batch 97, accuracy/top5 = 0
I0816 10:41:39.207520  4780 caffe.cpp:313] Batch 97, loss = 7.00901
I0816 10:41:39.270412  4780 caffe.cpp:313] Batch 98, accuracy/top1 = 0
I0816 10:41:39.270434  4780 caffe.cpp:313] Batch 98, accuracy/top5 = 0
I0816 10:41:39.270438  4780 caffe.cpp:313] Batch 98, loss = 6.92265
I0816 10:41:39.333233  4780 caffe.cpp:313] Batch 99, accuracy/top1 = 0.02
I0816 10:41:39.333256  4780 caffe.cpp:313] Batch 99, accuracy/top5 = 0.02
I0816 10:41:39.333259  4780 caffe.cpp:313] Batch 99, loss = 6.90217
I0816 10:41:39.395994  4780 caffe.cpp:313] Batch 100, accuracy/top1 = 0
I0816 10:41:39.396015  4780 caffe.cpp:313] Batch 100, accuracy/top5 = 0
I0816 10:41:39.396019  4780 caffe.cpp:313] Batch 100, loss = 7.11321
I0816 10:41:39.458742  4780 caffe.cpp:313] Batch 101, accuracy/top1 = 0
I0816 10:41:39.458763  4780 caffe.cpp:313] Batch 101, accuracy/top5 = 0.04
I0816 10:41:39.458768  4780 caffe.cpp:313] Batch 101, loss = 6.55405
I0816 10:41:39.521680  4780 caffe.cpp:313] Batch 102, accuracy/top1 = 0
I0816 10:41:39.521703  4780 caffe.cpp:313] Batch 102, accuracy/top5 = 0
I0816 10:41:39.521706  4780 caffe.cpp:313] Batch 102, loss = 6.88537
I0816 10:41:39.584518  4780 caffe.cpp:313] Batch 103, accuracy/top1 = 0
I0816 10:41:39.584540  4780 caffe.cpp:313] Batch 103, accuracy/top5 = 0.04
I0816 10:41:39.584545  4780 caffe.cpp:313] Batch 103, loss = 6.72657
I0816 10:41:39.647557  4780 caffe.cpp:313] Batch 104, accuracy/top1 = 0
I0816 10:41:39.647577  4780 caffe.cpp:313] Batch 104, accuracy/top5 = 0.02
I0816 10:41:39.647580  4780 caffe.cpp:313] Batch 104, loss = 6.84332
I0816 10:41:39.710433  4780 caffe.cpp:313] Batch 105, accuracy/top1 = 0
I0816 10:41:39.710451  4780 caffe.cpp:313] Batch 105, accuracy/top5 = 0.02
I0816 10:41:39.710472  4780 caffe.cpp:313] Batch 105, loss = 6.84204
I0816 10:41:39.773516  4780 caffe.cpp:313] Batch 106, accuracy/top1 = 0
I0816 10:41:39.773537  4780 caffe.cpp:313] Batch 106, accuracy/top5 = 0.04
I0816 10:41:39.773541  4780 caffe.cpp:313] Batch 106, loss = 6.61483
I0816 10:41:39.836419  4780 caffe.cpp:313] Batch 107, accuracy/top1 = 0
I0816 10:41:39.836441  4780 caffe.cpp:313] Batch 107, accuracy/top5 = 0.02
I0816 10:41:39.836446  4780 caffe.cpp:313] Batch 107, loss = 6.93704
I0816 10:41:39.899355  4780 caffe.cpp:313] Batch 108, accuracy/top1 = 0
I0816 10:41:39.899376  4780 caffe.cpp:313] Batch 108, accuracy/top5 = 0.02
I0816 10:41:39.899380  4780 caffe.cpp:313] Batch 108, loss = 6.83323
I0816 10:41:39.962291  4780 caffe.cpp:313] Batch 109, accuracy/top1 = 0
I0816 10:41:39.962313  4780 caffe.cpp:313] Batch 109, accuracy/top5 = 0.02
I0816 10:41:39.962318  4780 caffe.cpp:313] Batch 109, loss = 6.7658
I0816 10:41:40.025182  4780 caffe.cpp:313] Batch 110, accuracy/top1 = 0
I0816 10:41:40.025200  4780 caffe.cpp:313] Batch 110, accuracy/top5 = 0.02
I0816 10:41:40.025205  4780 caffe.cpp:313] Batch 110, loss = 6.76207
I0816 10:41:40.087993  4780 caffe.cpp:313] Batch 111, accuracy/top1 = 0
I0816 10:41:40.088013  4780 caffe.cpp:313] Batch 111, accuracy/top5 = 0
I0816 10:41:40.088017  4780 caffe.cpp:313] Batch 111, loss = 6.74421
I0816 10:41:40.150856  4780 caffe.cpp:313] Batch 112, accuracy/top1 = 0
I0816 10:41:40.150877  4780 caffe.cpp:313] Batch 112, accuracy/top5 = 0.02
I0816 10:41:40.150882  4780 caffe.cpp:313] Batch 112, loss = 6.76349
I0816 10:41:40.213764  4780 caffe.cpp:313] Batch 113, accuracy/top1 = 0.04
I0816 10:41:40.213785  4780 caffe.cpp:313] Batch 113, accuracy/top5 = 0.06
I0816 10:41:40.213789  4780 caffe.cpp:313] Batch 113, loss = 6.6759
I0816 10:41:40.276582  4780 caffe.cpp:313] Batch 114, accuracy/top1 = 0
I0816 10:41:40.276602  4780 caffe.cpp:313] Batch 114, accuracy/top5 = 0.02
I0816 10:41:40.276607  4780 caffe.cpp:313] Batch 114, loss = 6.81273
I0816 10:41:40.339437  4780 caffe.cpp:313] Batch 115, accuracy/top1 = 0
I0816 10:41:40.339458  4780 caffe.cpp:313] Batch 115, accuracy/top5 = 0
I0816 10:41:40.339462  4780 caffe.cpp:313] Batch 115, loss = 6.71699
I0816 10:41:40.402402  4780 caffe.cpp:313] Batch 116, accuracy/top1 = 0
I0816 10:41:40.402423  4780 caffe.cpp:313] Batch 116, accuracy/top5 = 0.02
I0816 10:41:40.402427  4780 caffe.cpp:313] Batch 116, loss = 7.12657
I0816 10:41:40.465325  4780 caffe.cpp:313] Batch 117, accuracy/top1 = 0.02
I0816 10:41:40.465346  4780 caffe.cpp:313] Batch 117, accuracy/top5 = 0.06
I0816 10:41:40.465350  4780 caffe.cpp:313] Batch 117, loss = 6.7452
I0816 10:41:40.528210  4780 caffe.cpp:313] Batch 118, accuracy/top1 = 0.02
I0816 10:41:40.528232  4780 caffe.cpp:313] Batch 118, accuracy/top5 = 0.04
I0816 10:41:40.528236  4780 caffe.cpp:313] Batch 118, loss = 6.74263
I0816 10:41:40.591022  4780 caffe.cpp:313] Batch 119, accuracy/top1 = 0.02
I0816 10:41:40.591045  4780 caffe.cpp:313] Batch 119, accuracy/top5 = 0.02
I0816 10:41:40.591049  4780 caffe.cpp:313] Batch 119, loss = 7.02369
I0816 10:41:40.653880  4780 caffe.cpp:313] Batch 120, accuracy/top1 = 0.04
I0816 10:41:40.653903  4780 caffe.cpp:313] Batch 120, accuracy/top5 = 0.06
I0816 10:41:40.653908  4780 caffe.cpp:313] Batch 120, loss = 6.56707
I0816 10:41:40.716660  4780 caffe.cpp:313] Batch 121, accuracy/top1 = 0.02
I0816 10:41:40.716682  4780 caffe.cpp:313] Batch 121, accuracy/top5 = 0.06
I0816 10:41:40.716686  4780 caffe.cpp:313] Batch 121, loss = 6.59297
I0816 10:41:40.779541  4780 caffe.cpp:313] Batch 122, accuracy/top1 = 0
I0816 10:41:40.779562  4780 caffe.cpp:313] Batch 122, accuracy/top5 = 0.04
I0816 10:41:40.779567  4780 caffe.cpp:313] Batch 122, loss = 6.78956
I0816 10:41:40.842314  4780 caffe.cpp:313] Batch 123, accuracy/top1 = 0
I0816 10:41:40.842335  4780 caffe.cpp:313] Batch 123, accuracy/top5 = 0.02
I0816 10:41:40.842339  4780 caffe.cpp:313] Batch 123, loss = 6.48505
I0816 10:41:40.905308  4780 caffe.cpp:313] Batch 124, accuracy/top1 = 0.02
I0816 10:41:40.905330  4780 caffe.cpp:313] Batch 124, accuracy/top5 = 0.08
I0816 10:41:40.905349  4780 caffe.cpp:313] Batch 124, loss = 6.35382
I0816 10:41:40.968624  4780 caffe.cpp:313] Batch 125, accuracy/top1 = 0.02
I0816 10:41:40.968646  4780 caffe.cpp:313] Batch 125, accuracy/top5 = 0.04
I0816 10:41:40.968649  4780 caffe.cpp:313] Batch 125, loss = 6.74861
I0816 10:41:41.031780  4780 caffe.cpp:313] Batch 126, accuracy/top1 = 0.02
I0816 10:41:41.031797  4780 caffe.cpp:313] Batch 126, accuracy/top5 = 0.04
I0816 10:41:41.031802  4780 caffe.cpp:313] Batch 126, loss = 6.71368
I0816 10:41:41.094585  4780 caffe.cpp:313] Batch 127, accuracy/top1 = 0
I0816 10:41:41.094606  4780 caffe.cpp:313] Batch 127, accuracy/top5 = 0.02
I0816 10:41:41.094611  4780 caffe.cpp:313] Batch 127, loss = 6.65639
I0816 10:41:41.157490  4780 caffe.cpp:313] Batch 128, accuracy/top1 = 0
I0816 10:41:41.157510  4780 caffe.cpp:313] Batch 128, accuracy/top5 = 0.02
I0816 10:41:41.157516  4780 caffe.cpp:313] Batch 128, loss = 6.79107
I0816 10:41:41.220535  4780 caffe.cpp:313] Batch 129, accuracy/top1 = 0.02
I0816 10:41:41.220557  4780 caffe.cpp:313] Batch 129, accuracy/top5 = 0.02
I0816 10:41:41.220561  4780 caffe.cpp:313] Batch 129, loss = 7.01645
I0816 10:41:41.283514  4780 caffe.cpp:313] Batch 130, accuracy/top1 = 0
I0816 10:41:41.283535  4780 caffe.cpp:313] Batch 130, accuracy/top5 = 0.02
I0816 10:41:41.283540  4780 caffe.cpp:313] Batch 130, loss = 6.59367
I0816 10:41:41.346510  4780 caffe.cpp:313] Batch 131, accuracy/top1 = 0
I0816 10:41:41.346531  4780 caffe.cpp:313] Batch 131, accuracy/top5 = 0.04
I0816 10:41:41.346536  4780 caffe.cpp:313] Batch 131, loss = 6.48379
I0816 10:41:41.409317  4780 caffe.cpp:313] Batch 132, accuracy/top1 = 0
I0816 10:41:41.409339  4780 caffe.cpp:313] Batch 132, accuracy/top5 = 0.02
I0816 10:41:41.409343  4780 caffe.cpp:313] Batch 132, loss = 6.43501
I0816 10:41:41.472144  4780 caffe.cpp:313] Batch 133, accuracy/top1 = 0
I0816 10:41:41.472165  4780 caffe.cpp:313] Batch 133, accuracy/top5 = 0
I0816 10:41:41.472169  4780 caffe.cpp:313] Batch 133, loss = 6.67041
I0816 10:41:41.534972  4780 caffe.cpp:313] Batch 134, accuracy/top1 = 0
I0816 10:41:41.534994  4780 caffe.cpp:313] Batch 134, accuracy/top5 = 0.04
I0816 10:41:41.534999  4780 caffe.cpp:313] Batch 134, loss = 6.63328
I0816 10:41:41.597934  4780 caffe.cpp:313] Batch 135, accuracy/top1 = 0.02
I0816 10:41:41.597956  4780 caffe.cpp:313] Batch 135, accuracy/top5 = 0.02
I0816 10:41:41.597960  4780 caffe.cpp:313] Batch 135, loss = 6.68429
I0816 10:41:41.660820  4780 caffe.cpp:313] Batch 136, accuracy/top1 = 0
I0816 10:41:41.660841  4780 caffe.cpp:313] Batch 136, accuracy/top5 = 0.04
I0816 10:41:41.660845  4780 caffe.cpp:313] Batch 136, loss = 6.804
I0816 10:41:41.723832  4780 caffe.cpp:313] Batch 137, accuracy/top1 = 0
I0816 10:41:41.723853  4780 caffe.cpp:313] Batch 137, accuracy/top5 = 0.04
I0816 10:41:41.723858  4780 caffe.cpp:313] Batch 137, loss = 6.83238
I0816 10:41:41.786860  4780 caffe.cpp:313] Batch 138, accuracy/top1 = 0.02
I0816 10:41:41.786880  4780 caffe.cpp:313] Batch 138, accuracy/top5 = 0.02
I0816 10:41:41.786885  4780 caffe.cpp:313] Batch 138, loss = 6.9469
I0816 10:41:41.849879  4780 caffe.cpp:313] Batch 139, accuracy/top1 = 0.04
I0816 10:41:41.849898  4780 caffe.cpp:313] Batch 139, accuracy/top5 = 0.06
I0816 10:41:41.849902  4780 caffe.cpp:313] Batch 139, loss = 6.86703
I0816 10:41:41.913240  4780 caffe.cpp:313] Batch 140, accuracy/top1 = 0
I0816 10:41:41.913260  4780 caffe.cpp:313] Batch 140, accuracy/top5 = 0.02
I0816 10:41:41.913265  4780 caffe.cpp:313] Batch 140, loss = 6.48605
I0816 10:41:41.976586  4780 caffe.cpp:313] Batch 141, accuracy/top1 = 0.02
I0816 10:41:41.976603  4780 caffe.cpp:313] Batch 141, accuracy/top5 = 0.04
I0816 10:41:41.976608  4780 caffe.cpp:313] Batch 141, loss = 6.78464
I0816 10:41:42.039510  4780 caffe.cpp:313] Batch 142, accuracy/top1 = 0
I0816 10:41:42.039528  4780 caffe.cpp:313] Batch 142, accuracy/top5 = 0.02
I0816 10:41:42.039533  4780 caffe.cpp:313] Batch 142, loss = 6.5605
I0816 10:41:42.102421  4780 caffe.cpp:313] Batch 143, accuracy/top1 = 0.04
I0816 10:41:42.102454  4780 caffe.cpp:313] Batch 143, accuracy/top5 = 0.1
I0816 10:41:42.102459  4780 caffe.cpp:313] Batch 143, loss = 6.70384
I0816 10:41:42.165349  4780 caffe.cpp:313] Batch 144, accuracy/top1 = 0
I0816 10:41:42.165370  4780 caffe.cpp:313] Batch 144, accuracy/top5 = 0.04
I0816 10:41:42.165375  4780 caffe.cpp:313] Batch 144, loss = 6.5565
I0816 10:41:42.228265  4780 caffe.cpp:313] Batch 145, accuracy/top1 = 0.02
I0816 10:41:42.228286  4780 caffe.cpp:313] Batch 145, accuracy/top5 = 0.02
I0816 10:41:42.228291  4780 caffe.cpp:313] Batch 145, loss = 6.5637
I0816 10:41:42.291168  4780 caffe.cpp:313] Batch 146, accuracy/top1 = 0.02
I0816 10:41:42.291191  4780 caffe.cpp:313] Batch 146, accuracy/top5 = 0.02
I0816 10:41:42.291195  4780 caffe.cpp:313] Batch 146, loss = 6.79822
I0816 10:41:42.354050  4780 caffe.cpp:313] Batch 147, accuracy/top1 = 0.02
I0816 10:41:42.354073  4780 caffe.cpp:313] Batch 147, accuracy/top5 = 0.04
I0816 10:41:42.354076  4780 caffe.cpp:313] Batch 147, loss = 6.51814
I0816 10:41:42.416865  4780 caffe.cpp:313] Batch 148, accuracy/top1 = 0.02
I0816 10:41:42.416888  4780 caffe.cpp:313] Batch 148, accuracy/top5 = 0.04
I0816 10:41:42.416893  4780 caffe.cpp:313] Batch 148, loss = 6.95773
I0816 10:41:42.479738  4780 caffe.cpp:313] Batch 149, accuracy/top1 = 0.02
I0816 10:41:42.479760  4780 caffe.cpp:313] Batch 149, accuracy/top5 = 0.02
I0816 10:41:42.479764  4780 caffe.cpp:313] Batch 149, loss = 6.54134
I0816 10:41:42.542722  4780 caffe.cpp:313] Batch 150, accuracy/top1 = 0.02
I0816 10:41:42.542744  4780 caffe.cpp:313] Batch 150, accuracy/top5 = 0.02
I0816 10:41:42.542748  4780 caffe.cpp:313] Batch 150, loss = 6.64117
I0816 10:41:42.605643  4780 caffe.cpp:313] Batch 151, accuracy/top1 = 0.04
I0816 10:41:42.605664  4780 caffe.cpp:313] Batch 151, accuracy/top5 = 0.06
I0816 10:41:42.605669  4780 caffe.cpp:313] Batch 151, loss = 6.61265
I0816 10:41:42.668661  4780 caffe.cpp:313] Batch 152, accuracy/top1 = 0
I0816 10:41:42.668684  4780 caffe.cpp:313] Batch 152, accuracy/top5 = 0.04
I0816 10:41:42.668687  4780 caffe.cpp:313] Batch 152, loss = 6.93935
I0816 10:41:42.731639  4780 caffe.cpp:313] Batch 153, accuracy/top1 = 0.04
I0816 10:41:42.731662  4780 caffe.cpp:313] Batch 153, accuracy/top5 = 0.04
I0816 10:41:42.731665  4780 caffe.cpp:313] Batch 153, loss = 7.05941
I0816 10:41:42.794417  4780 caffe.cpp:313] Batch 154, accuracy/top1 = 0.02
I0816 10:41:42.794438  4780 caffe.cpp:313] Batch 154, accuracy/top5 = 0.06
I0816 10:41:42.794442  4780 caffe.cpp:313] Batch 154, loss = 6.70386
I0816 10:41:42.857229  4780 caffe.cpp:313] Batch 155, accuracy/top1 = 0
I0816 10:41:42.857251  4780 caffe.cpp:313] Batch 155, accuracy/top5 = 0.02
I0816 10:41:42.857256  4780 caffe.cpp:313] Batch 155, loss = 6.73855
I0816 10:41:42.920060  4780 caffe.cpp:313] Batch 156, accuracy/top1 = 0
I0816 10:41:42.920081  4780 caffe.cpp:313] Batch 156, accuracy/top5 = 0.02
I0816 10:41:42.920086  4780 caffe.cpp:313] Batch 156, loss = 6.52785
I0816 10:41:42.983934  4780 caffe.cpp:313] Batch 157, accuracy/top1 = 0
I0816 10:41:42.983952  4780 caffe.cpp:313] Batch 157, accuracy/top5 = 0
I0816 10:41:42.983956  4780 caffe.cpp:313] Batch 157, loss = 6.61266
I0816 10:41:43.046859  4780 caffe.cpp:313] Batch 158, accuracy/top1 = 0
I0816 10:41:43.046878  4780 caffe.cpp:313] Batch 158, accuracy/top5 = 0.04
I0816 10:41:43.046882  4780 caffe.cpp:313] Batch 158, loss = 6.7749
I0816 10:41:43.109735  4780 caffe.cpp:313] Batch 159, accuracy/top1 = 0
I0816 10:41:43.109756  4780 caffe.cpp:313] Batch 159, accuracy/top5 = 0.04
I0816 10:41:43.109760  4780 caffe.cpp:313] Batch 159, loss = 6.78035
I0816 10:41:43.172705  4780 caffe.cpp:313] Batch 160, accuracy/top1 = 0.02
I0816 10:41:43.172727  4780 caffe.cpp:313] Batch 160, accuracy/top5 = 0.04
I0816 10:41:43.172732  4780 caffe.cpp:313] Batch 160, loss = 6.4614
I0816 10:41:43.235633  4780 caffe.cpp:313] Batch 161, accuracy/top1 = 0
I0816 10:41:43.235656  4780 caffe.cpp:313] Batch 161, accuracy/top5 = 0.04
I0816 10:41:43.235661  4780 caffe.cpp:313] Batch 161, loss = 6.59131
I0816 10:41:43.298575  4780 caffe.cpp:313] Batch 162, accuracy/top1 = 0.04
I0816 10:41:43.298611  4780 caffe.cpp:313] Batch 162, accuracy/top5 = 0.12
I0816 10:41:43.298615  4780 caffe.cpp:313] Batch 162, loss = 6.40958
I0816 10:41:43.361366  4780 caffe.cpp:313] Batch 163, accuracy/top1 = 0
I0816 10:41:43.361388  4780 caffe.cpp:313] Batch 163, accuracy/top5 = 0
I0816 10:41:43.361392  4780 caffe.cpp:313] Batch 163, loss = 7.12328
I0816 10:41:43.424345  4780 caffe.cpp:313] Batch 164, accuracy/top1 = 0
I0816 10:41:43.424366  4780 caffe.cpp:313] Batch 164, accuracy/top5 = 0
I0816 10:41:43.424371  4780 caffe.cpp:313] Batch 164, loss = 6.97739
I0816 10:41:43.487300  4780 caffe.cpp:313] Batch 165, accuracy/top1 = 0
I0816 10:41:43.487320  4780 caffe.cpp:313] Batch 165, accuracy/top5 = 0.02
I0816 10:41:43.487325  4780 caffe.cpp:313] Batch 165, loss = 6.92026
I0816 10:41:43.550246  4780 caffe.cpp:313] Batch 166, accuracy/top1 = 0
I0816 10:41:43.550266  4780 caffe.cpp:313] Batch 166, accuracy/top5 = 0.02
I0816 10:41:43.550271  4780 caffe.cpp:313] Batch 166, loss = 6.89585
I0816 10:41:43.612996  4780 caffe.cpp:313] Batch 167, accuracy/top1 = 0
I0816 10:41:43.613018  4780 caffe.cpp:313] Batch 167, accuracy/top5 = 0.04
I0816 10:41:43.613021  4780 caffe.cpp:313] Batch 167, loss = 6.47307
I0816 10:41:43.675853  4780 caffe.cpp:313] Batch 168, accuracy/top1 = 0
I0816 10:41:43.675873  4780 caffe.cpp:313] Batch 168, accuracy/top5 = 0.06
I0816 10:41:43.675879  4780 caffe.cpp:313] Batch 168, loss = 6.45188
I0816 10:41:43.738672  4780 caffe.cpp:313] Batch 169, accuracy/top1 = 0
I0816 10:41:43.738692  4780 caffe.cpp:313] Batch 169, accuracy/top5 = 0.02
I0816 10:41:43.738698  4780 caffe.cpp:313] Batch 169, loss = 6.50469
I0816 10:41:43.801574  4780 caffe.cpp:313] Batch 170, accuracy/top1 = 0
I0816 10:41:43.801595  4780 caffe.cpp:313] Batch 170, accuracy/top5 = 0
I0816 10:41:43.801599  4780 caffe.cpp:313] Batch 170, loss = 6.83735
I0816 10:41:43.864416  4780 caffe.cpp:313] Batch 171, accuracy/top1 = 0.04
I0816 10:41:43.864439  4780 caffe.cpp:313] Batch 171, accuracy/top5 = 0.06
I0816 10:41:43.864442  4780 caffe.cpp:313] Batch 171, loss = 6.78932
I0816 10:41:43.927382  4780 caffe.cpp:313] Batch 172, accuracy/top1 = 0
I0816 10:41:43.927402  4780 caffe.cpp:313] Batch 172, accuracy/top5 = 0
I0816 10:41:43.927405  4780 caffe.cpp:313] Batch 172, loss = 7.01655
I0816 10:41:43.990797  4780 caffe.cpp:313] Batch 173, accuracy/top1 = 0
I0816 10:41:43.990814  4780 caffe.cpp:313] Batch 173, accuracy/top5 = 0.02
I0816 10:41:43.990818  4780 caffe.cpp:313] Batch 173, loss = 6.93093
I0816 10:41:44.053901  4780 caffe.cpp:313] Batch 174, accuracy/top1 = 0
I0816 10:41:44.053921  4780 caffe.cpp:313] Batch 174, accuracy/top5 = 0
I0816 10:41:44.053925  4780 caffe.cpp:313] Batch 174, loss = 6.96771
I0816 10:41:44.116698  4780 caffe.cpp:313] Batch 175, accuracy/top1 = 0
I0816 10:41:44.116720  4780 caffe.cpp:313] Batch 175, accuracy/top5 = 0.08
I0816 10:41:44.116724  4780 caffe.cpp:313] Batch 175, loss = 6.38634
I0816 10:41:44.179538  4780 caffe.cpp:313] Batch 176, accuracy/top1 = 0
I0816 10:41:44.179558  4780 caffe.cpp:313] Batch 176, accuracy/top5 = 0
I0816 10:41:44.179563  4780 caffe.cpp:313] Batch 176, loss = 7.01348
I0816 10:41:44.242465  4780 caffe.cpp:313] Batch 177, accuracy/top1 = 0.04
I0816 10:41:44.242486  4780 caffe.cpp:313] Batch 177, accuracy/top5 = 0.1
I0816 10:41:44.242489  4780 caffe.cpp:313] Batch 177, loss = 6.58451
I0816 10:41:44.305392  4780 caffe.cpp:313] Batch 178, accuracy/top1 = 0
I0816 10:41:44.305413  4780 caffe.cpp:313] Batch 178, accuracy/top5 = 0.02
I0816 10:41:44.305418  4780 caffe.cpp:313] Batch 178, loss = 6.87208
I0816 10:41:44.368150  4780 caffe.cpp:313] Batch 179, accuracy/top1 = 0.02
I0816 10:41:44.368172  4780 caffe.cpp:313] Batch 179, accuracy/top5 = 0.04
I0816 10:41:44.368176  4780 caffe.cpp:313] Batch 179, loss = 6.63214
I0816 10:41:44.431005  4780 caffe.cpp:313] Batch 180, accuracy/top1 = 0
I0816 10:41:44.431026  4780 caffe.cpp:313] Batch 180, accuracy/top5 = 0.06
I0816 10:41:44.431031  4780 caffe.cpp:313] Batch 180, loss = 6.64598
I0816 10:41:44.493857  4780 caffe.cpp:313] Batch 181, accuracy/top1 = 0
I0816 10:41:44.493892  4780 caffe.cpp:313] Batch 181, accuracy/top5 = 0.02
I0816 10:41:44.493897  4780 caffe.cpp:313] Batch 181, loss = 6.89922
I0816 10:41:44.556809  4780 caffe.cpp:313] Batch 182, accuracy/top1 = 0
I0816 10:41:44.556831  4780 caffe.cpp:313] Batch 182, accuracy/top5 = 0.02
I0816 10:41:44.556836  4780 caffe.cpp:313] Batch 182, loss = 6.76418
I0816 10:41:44.619683  4780 caffe.cpp:313] Batch 183, accuracy/top1 = 0
I0816 10:41:44.619704  4780 caffe.cpp:313] Batch 183, accuracy/top5 = 0.04
I0816 10:41:44.619709  4780 caffe.cpp:313] Batch 183, loss = 6.65508
I0816 10:41:44.682481  4780 caffe.cpp:313] Batch 184, accuracy/top1 = 0.02
I0816 10:41:44.682502  4780 caffe.cpp:313] Batch 184, accuracy/top5 = 0.04
I0816 10:41:44.682507  4780 caffe.cpp:313] Batch 184, loss = 6.72709
I0816 10:41:44.745339  4780 caffe.cpp:313] Batch 185, accuracy/top1 = 0.02
I0816 10:41:44.745362  4780 caffe.cpp:313] Batch 185, accuracy/top5 = 0.02
I0816 10:41:44.745365  4780 caffe.cpp:313] Batch 185, loss = 6.84304
I0816 10:41:44.808080  4780 caffe.cpp:313] Batch 186, accuracy/top1 = 0.04
I0816 10:41:44.808101  4780 caffe.cpp:313] Batch 186, accuracy/top5 = 0.1
I0816 10:41:44.808106  4780 caffe.cpp:313] Batch 186, loss = 6.70026
I0816 10:41:44.871035  4780 caffe.cpp:313] Batch 187, accuracy/top1 = 0.02
I0816 10:41:44.871057  4780 caffe.cpp:313] Batch 187, accuracy/top5 = 0.02
I0816 10:41:44.871060  4780 caffe.cpp:313] Batch 187, loss = 6.61994
I0816 10:41:44.933957  4780 caffe.cpp:313] Batch 188, accuracy/top1 = 0.02
I0816 10:41:44.933979  4780 caffe.cpp:313] Batch 188, accuracy/top5 = 0.02
I0816 10:41:44.933982  4780 caffe.cpp:313] Batch 188, loss = 6.65992
I0816 10:41:44.997140  4780 caffe.cpp:313] Batch 189, accuracy/top1 = 0
I0816 10:41:44.997156  4780 caffe.cpp:313] Batch 189, accuracy/top5 = 0
I0816 10:41:44.997160  4780 caffe.cpp:313] Batch 189, loss = 7.0128
I0816 10:41:45.060057  4780 caffe.cpp:313] Batch 190, accuracy/top1 = 0
I0816 10:41:45.060078  4780 caffe.cpp:313] Batch 190, accuracy/top5 = 0.02
I0816 10:41:45.060083  4780 caffe.cpp:313] Batch 190, loss = 6.81235
I0816 10:41:45.122884  4780 caffe.cpp:313] Batch 191, accuracy/top1 = 0
I0816 10:41:45.122905  4780 caffe.cpp:313] Batch 191, accuracy/top5 = 0.02
I0816 10:41:45.122910  4780 caffe.cpp:313] Batch 191, loss = 6.73836
I0816 10:41:45.185897  4780 caffe.cpp:313] Batch 192, accuracy/top1 = 0.04
I0816 10:41:45.185920  4780 caffe.cpp:313] Batch 192, accuracy/top5 = 0.06
I0816 10:41:45.185925  4780 caffe.cpp:313] Batch 192, loss = 6.81391
I0816 10:41:45.248790  4780 caffe.cpp:313] Batch 193, accuracy/top1 = 0
I0816 10:41:45.248811  4780 caffe.cpp:313] Batch 193, accuracy/top5 = 0
I0816 10:41:45.248816  4780 caffe.cpp:313] Batch 193, loss = 6.91667
I0816 10:41:45.311568  4780 caffe.cpp:313] Batch 194, accuracy/top1 = 0
I0816 10:41:45.311589  4780 caffe.cpp:313] Batch 194, accuracy/top5 = 0
I0816 10:41:45.311594  4780 caffe.cpp:313] Batch 194, loss = 7.13187
I0816 10:41:45.374521  4780 caffe.cpp:313] Batch 195, accuracy/top1 = 0
I0816 10:41:45.374542  4780 caffe.cpp:313] Batch 195, accuracy/top5 = 0.02
I0816 10:41:45.374547  4780 caffe.cpp:313] Batch 195, loss = 6.89093
I0816 10:41:45.437490  4780 caffe.cpp:313] Batch 196, accuracy/top1 = 0.02
I0816 10:41:45.437512  4780 caffe.cpp:313] Batch 196, accuracy/top5 = 0.06
I0816 10:41:45.437516  4780 caffe.cpp:313] Batch 196, loss = 6.53685
I0816 10:41:45.500274  4780 caffe.cpp:313] Batch 197, accuracy/top1 = 0
I0816 10:41:45.500294  4780 caffe.cpp:313] Batch 197, accuracy/top5 = 0
I0816 10:41:45.500298  4780 caffe.cpp:313] Batch 197, loss = 6.81742
I0816 10:41:45.563046  4780 caffe.cpp:313] Batch 198, accuracy/top1 = 0
I0816 10:41:45.563067  4780 caffe.cpp:313] Batch 198, accuracy/top5 = 0.02
I0816 10:41:45.563071  4780 caffe.cpp:313] Batch 198, loss = 6.719
I0816 10:41:45.625938  4780 caffe.cpp:313] Batch 199, accuracy/top1 = 0.02
I0816 10:41:45.625959  4780 caffe.cpp:313] Batch 199, accuracy/top5 = 0.06
I0816 10:41:45.625963  4780 caffe.cpp:313] Batch 199, loss = 6.6451
I0816 10:41:45.688921  4780 caffe.cpp:313] Batch 200, accuracy/top1 = 0
I0816 10:41:45.688956  4780 caffe.cpp:313] Batch 200, accuracy/top5 = 0.02
I0816 10:41:45.688961  4780 caffe.cpp:313] Batch 200, loss = 6.883
I0816 10:41:45.751687  4780 caffe.cpp:313] Batch 201, accuracy/top1 = 0.02
I0816 10:41:45.751708  4780 caffe.cpp:313] Batch 201, accuracy/top5 = 0.04
I0816 10:41:45.751711  4780 caffe.cpp:313] Batch 201, loss = 6.82249
I0816 10:41:45.814621  4780 caffe.cpp:313] Batch 202, accuracy/top1 = 0
I0816 10:41:45.814642  4780 caffe.cpp:313] Batch 202, accuracy/top5 = 0.04
I0816 10:41:45.814647  4780 caffe.cpp:313] Batch 202, loss = 6.83308
I0816 10:41:45.877530  4780 caffe.cpp:313] Batch 203, accuracy/top1 = 0.02
I0816 10:41:45.877552  4780 caffe.cpp:313] Batch 203, accuracy/top5 = 0.06
I0816 10:41:45.877557  4780 caffe.cpp:313] Batch 203, loss = 6.47675
I0816 10:41:45.940515  4780 caffe.cpp:313] Batch 204, accuracy/top1 = 0
I0816 10:41:45.940536  4780 caffe.cpp:313] Batch 204, accuracy/top5 = 0.04
I0816 10:41:45.940541  4780 caffe.cpp:313] Batch 204, loss = 6.81947
I0816 10:41:46.003866  4780 caffe.cpp:313] Batch 205, accuracy/top1 = 0
I0816 10:41:46.003885  4780 caffe.cpp:313] Batch 205, accuracy/top5 = 0.04
I0816 10:41:46.003890  4780 caffe.cpp:313] Batch 205, loss = 6.97322
I0816 10:41:46.066756  4780 caffe.cpp:313] Batch 206, accuracy/top1 = 0.02
I0816 10:41:46.066776  4780 caffe.cpp:313] Batch 206, accuracy/top5 = 0.02
I0816 10:41:46.066781  4780 caffe.cpp:313] Batch 206, loss = 6.63958
I0816 10:41:46.129743  4780 caffe.cpp:313] Batch 207, accuracy/top1 = 0
I0816 10:41:46.129763  4780 caffe.cpp:313] Batch 207, accuracy/top5 = 0.06
I0816 10:41:46.129768  4780 caffe.cpp:313] Batch 207, loss = 6.57864
I0816 10:41:46.192620  4780 caffe.cpp:313] Batch 208, accuracy/top1 = 0
I0816 10:41:46.192642  4780 caffe.cpp:313] Batch 208, accuracy/top5 = 0.06
I0816 10:41:46.192646  4780 caffe.cpp:313] Batch 208, loss = 6.57706
I0816 10:41:46.255496  4780 caffe.cpp:313] Batch 209, accuracy/top1 = 0
I0816 10:41:46.255517  4780 caffe.cpp:313] Batch 209, accuracy/top5 = 0.06
I0816 10:41:46.255520  4780 caffe.cpp:313] Batch 209, loss = 6.60662
I0816 10:41:46.318361  4780 caffe.cpp:313] Batch 210, accuracy/top1 = 0.02
I0816 10:41:46.318382  4780 caffe.cpp:313] Batch 210, accuracy/top5 = 0.06
I0816 10:41:46.318385  4780 caffe.cpp:313] Batch 210, loss = 6.77642
I0816 10:41:46.381212  4780 caffe.cpp:313] Batch 211, accuracy/top1 = 0
I0816 10:41:46.381233  4780 caffe.cpp:313] Batch 211, accuracy/top5 = 0.12
I0816 10:41:46.381237  4780 caffe.cpp:313] Batch 211, loss = 6.23663
I0816 10:41:46.443967  4780 caffe.cpp:313] Batch 212, accuracy/top1 = 0
I0816 10:41:46.443989  4780 caffe.cpp:313] Batch 212, accuracy/top5 = 0.02
I0816 10:41:46.443994  4780 caffe.cpp:313] Batch 212, loss = 6.65486
I0816 10:41:46.506865  4780 caffe.cpp:313] Batch 213, accuracy/top1 = 0.02
I0816 10:41:46.506886  4780 caffe.cpp:313] Batch 213, accuracy/top5 = 0.06
I0816 10:41:46.506889  4780 caffe.cpp:313] Batch 213, loss = 6.76637
I0816 10:41:46.569741  4780 caffe.cpp:313] Batch 214, accuracy/top1 = 0.02
I0816 10:41:46.569763  4780 caffe.cpp:313] Batch 214, accuracy/top5 = 0.04
I0816 10:41:46.569767  4780 caffe.cpp:313] Batch 214, loss = 6.62644
I0816 10:41:46.632550  4780 caffe.cpp:313] Batch 215, accuracy/top1 = 0
I0816 10:41:46.632572  4780 caffe.cpp:313] Batch 215, accuracy/top5 = 0.02
I0816 10:41:46.632577  4780 caffe.cpp:313] Batch 215, loss = 7.04235
I0816 10:41:46.695399  4780 caffe.cpp:313] Batch 216, accuracy/top1 = 0.02
I0816 10:41:46.695421  4780 caffe.cpp:313] Batch 216, accuracy/top5 = 0.04
I0816 10:41:46.695425  4780 caffe.cpp:313] Batch 216, loss = 6.44955
I0816 10:41:46.758224  4780 caffe.cpp:313] Batch 217, accuracy/top1 = 0
I0816 10:41:46.758246  4780 caffe.cpp:313] Batch 217, accuracy/top5 = 0.02
I0816 10:41:46.758251  4780 caffe.cpp:313] Batch 217, loss = 6.75391
I0816 10:41:46.821039  4780 caffe.cpp:313] Batch 218, accuracy/top1 = 0.02
I0816 10:41:46.821060  4780 caffe.cpp:313] Batch 218, accuracy/top5 = 0.02
I0816 10:41:46.821064  4780 caffe.cpp:313] Batch 218, loss = 6.85504
I0816 10:41:46.883951  4780 caffe.cpp:313] Batch 219, accuracy/top1 = 0
I0816 10:41:46.883973  4780 caffe.cpp:313] Batch 219, accuracy/top5 = 0.04
I0816 10:41:46.883977  4780 caffe.cpp:313] Batch 219, loss = 6.66016
I0816 10:41:46.946858  4780 caffe.cpp:313] Batch 220, accuracy/top1 = 0
I0816 10:41:46.946880  4780 caffe.cpp:313] Batch 220, accuracy/top5 = 0.02
I0816 10:41:46.946884  4780 caffe.cpp:313] Batch 220, loss = 6.87689
I0816 10:41:47.010041  4780 caffe.cpp:313] Batch 221, accuracy/top1 = 0
I0816 10:41:47.010057  4780 caffe.cpp:313] Batch 221, accuracy/top5 = 0
I0816 10:41:47.010062  4780 caffe.cpp:313] Batch 221, loss = 6.77549
I0816 10:41:47.073107  4780 caffe.cpp:313] Batch 222, accuracy/top1 = 0
I0816 10:41:47.073127  4780 caffe.cpp:313] Batch 222, accuracy/top5 = 0
I0816 10:41:47.073132  4780 caffe.cpp:313] Batch 222, loss = 6.90723
I0816 10:41:47.135994  4780 caffe.cpp:313] Batch 223, accuracy/top1 = 0
I0816 10:41:47.136015  4780 caffe.cpp:313] Batch 223, accuracy/top5 = 0
I0816 10:41:47.136019  4780 caffe.cpp:313] Batch 223, loss = 6.99936
I0816 10:41:47.198938  4780 caffe.cpp:313] Batch 224, accuracy/top1 = 0
I0816 10:41:47.198959  4780 caffe.cpp:313] Batch 224, accuracy/top5 = 0.02
I0816 10:41:47.198964  4780 caffe.cpp:313] Batch 224, loss = 6.6654
I0816 10:41:47.261788  4780 caffe.cpp:313] Batch 225, accuracy/top1 = 0
I0816 10:41:47.261811  4780 caffe.cpp:313] Batch 225, accuracy/top5 = 0
I0816 10:41:47.261814  4780 caffe.cpp:313] Batch 225, loss = 6.72114
I0816 10:41:47.324766  4780 caffe.cpp:313] Batch 226, accuracy/top1 = 0.02
I0816 10:41:47.324787  4780 caffe.cpp:313] Batch 226, accuracy/top5 = 0.08
I0816 10:41:47.324791  4780 caffe.cpp:313] Batch 226, loss = 6.5778
I0816 10:41:47.387778  4780 caffe.cpp:313] Batch 227, accuracy/top1 = 0
I0816 10:41:47.387799  4780 caffe.cpp:313] Batch 227, accuracy/top5 = 0.02
I0816 10:41:47.387804  4780 caffe.cpp:313] Batch 227, loss = 6.69502
I0816 10:41:47.450625  4780 caffe.cpp:313] Batch 228, accuracy/top1 = 0
I0816 10:41:47.450646  4780 caffe.cpp:313] Batch 228, accuracy/top5 = 0
I0816 10:41:47.450650  4780 caffe.cpp:313] Batch 228, loss = 6.60119
I0816 10:41:47.513491  4780 caffe.cpp:313] Batch 229, accuracy/top1 = 0.02
I0816 10:41:47.513514  4780 caffe.cpp:313] Batch 229, accuracy/top5 = 0.04
I0816 10:41:47.513517  4780 caffe.cpp:313] Batch 229, loss = 6.69526
I0816 10:41:47.576237  4780 caffe.cpp:313] Batch 230, accuracy/top1 = 0.02
I0816 10:41:47.576258  4780 caffe.cpp:313] Batch 230, accuracy/top5 = 0.04
I0816 10:41:47.576262  4780 caffe.cpp:313] Batch 230, loss = 6.71713
I0816 10:41:47.639240  4780 caffe.cpp:313] Batch 231, accuracy/top1 = 0
I0816 10:41:47.639262  4780 caffe.cpp:313] Batch 231, accuracy/top5 = 0.04
I0816 10:41:47.639266  4780 caffe.cpp:313] Batch 231, loss = 6.60307
I0816 10:41:47.702111  4780 caffe.cpp:313] Batch 232, accuracy/top1 = 0
I0816 10:41:47.702132  4780 caffe.cpp:313] Batch 232, accuracy/top5 = 0.06
I0816 10:41:47.702136  4780 caffe.cpp:313] Batch 232, loss = 6.53735
I0816 10:41:47.764945  4780 caffe.cpp:313] Batch 233, accuracy/top1 = 0
I0816 10:41:47.764967  4780 caffe.cpp:313] Batch 233, accuracy/top5 = 0.04
I0816 10:41:47.764972  4780 caffe.cpp:313] Batch 233, loss = 6.47176
I0816 10:41:47.827873  4780 caffe.cpp:313] Batch 234, accuracy/top1 = 0
I0816 10:41:47.827894  4780 caffe.cpp:313] Batch 234, accuracy/top5 = 0.06
I0816 10:41:47.827899  4780 caffe.cpp:313] Batch 234, loss = 6.53746
I0816 10:41:47.890664  4780 caffe.cpp:313] Batch 235, accuracy/top1 = 0.02
I0816 10:41:47.890686  4780 caffe.cpp:313] Batch 235, accuracy/top5 = 0.04
I0816 10:41:47.890689  4780 caffe.cpp:313] Batch 235, loss = 6.78632
I0816 10:41:47.953475  4780 caffe.cpp:313] Batch 236, accuracy/top1 = 0
I0816 10:41:47.953497  4780 caffe.cpp:313] Batch 236, accuracy/top5 = 0.06
I0816 10:41:47.953501  4780 caffe.cpp:313] Batch 236, loss = 6.76176
I0816 10:41:48.016468  4780 caffe.cpp:313] Batch 237, accuracy/top1 = 0.02
I0816 10:41:48.016490  4780 caffe.cpp:313] Batch 237, accuracy/top5 = 0.06
I0816 10:41:48.016494  4780 caffe.cpp:313] Batch 237, loss = 6.3931
I0816 10:41:48.079294  4780 caffe.cpp:313] Batch 238, accuracy/top1 = 0.04
I0816 10:41:48.079314  4780 caffe.cpp:313] Batch 238, accuracy/top5 = 0.06
I0816 10:41:48.079319  4780 caffe.cpp:313] Batch 238, loss = 6.60494
I0816 10:41:48.142107  4780 caffe.cpp:313] Batch 239, accuracy/top1 = 0.02
I0816 10:41:48.142128  4780 caffe.cpp:313] Batch 239, accuracy/top5 = 0.02
I0816 10:41:48.142133  4780 caffe.cpp:313] Batch 239, loss = 6.85642
I0816 10:41:48.205127  4780 caffe.cpp:313] Batch 240, accuracy/top1 = 0.02
I0816 10:41:48.205147  4780 caffe.cpp:313] Batch 240, accuracy/top5 = 0.02
I0816 10:41:48.205150  4780 caffe.cpp:313] Batch 240, loss = 6.78659
I0816 10:41:48.268255  4780 caffe.cpp:313] Batch 241, accuracy/top1 = 0
I0816 10:41:48.268275  4780 caffe.cpp:313] Batch 241, accuracy/top5 = 0
I0816 10:41:48.268280  4780 caffe.cpp:313] Batch 241, loss = 6.73182
I0816 10:41:48.331266  4780 caffe.cpp:313] Batch 242, accuracy/top1 = 0
I0816 10:41:48.331287  4780 caffe.cpp:313] Batch 242, accuracy/top5 = 0.02
I0816 10:41:48.331292  4780 caffe.cpp:313] Batch 242, loss = 7.03257
I0816 10:41:48.394177  4780 caffe.cpp:313] Batch 243, accuracy/top1 = 0
I0816 10:41:48.394198  4780 caffe.cpp:313] Batch 243, accuracy/top5 = 0.06
I0816 10:41:48.394202  4780 caffe.cpp:313] Batch 243, loss = 6.38003
I0816 10:41:48.457052  4780 caffe.cpp:313] Batch 244, accuracy/top1 = 0
I0816 10:41:48.457073  4780 caffe.cpp:313] Batch 244, accuracy/top5 = 0.02
I0816 10:41:48.457078  4780 caffe.cpp:313] Batch 244, loss = 6.78203
I0816 10:41:48.519929  4780 caffe.cpp:313] Batch 245, accuracy/top1 = 0.02
I0816 10:41:48.519950  4780 caffe.cpp:313] Batch 245, accuracy/top5 = 0.04
I0816 10:41:48.519954  4780 caffe.cpp:313] Batch 245, loss = 6.96408
I0816 10:41:48.582854  4780 caffe.cpp:313] Batch 246, accuracy/top1 = 0
I0816 10:41:48.582876  4780 caffe.cpp:313] Batch 246, accuracy/top5 = 0.06
I0816 10:41:48.582880  4780 caffe.cpp:313] Batch 246, loss = 6.69493
I0816 10:41:48.645556  4780 caffe.cpp:313] Batch 247, accuracy/top1 = 0
I0816 10:41:48.645577  4780 caffe.cpp:313] Batch 247, accuracy/top5 = 0
I0816 10:41:48.645581  4780 caffe.cpp:313] Batch 247, loss = 6.70666
I0816 10:41:48.708348  4780 caffe.cpp:313] Batch 248, accuracy/top1 = 0
I0816 10:41:48.708369  4780 caffe.cpp:313] Batch 248, accuracy/top5 = 0.02
I0816 10:41:48.708374  4780 caffe.cpp:313] Batch 248, loss = 6.70937
I0816 10:41:48.771334  4780 caffe.cpp:313] Batch 249, accuracy/top1 = 0.04
I0816 10:41:48.771355  4780 caffe.cpp:313] Batch 249, accuracy/top5 = 0.06
I0816 10:41:48.771359  4780 caffe.cpp:313] Batch 249, loss = 6.59584
I0816 10:41:48.834223  4780 caffe.cpp:313] Batch 250, accuracy/top1 = 0.08
I0816 10:41:48.834244  4780 caffe.cpp:313] Batch 250, accuracy/top5 = 0.1
I0816 10:41:48.834249  4780 caffe.cpp:313] Batch 250, loss = 6.58456
I0816 10:41:48.897258  4780 caffe.cpp:313] Batch 251, accuracy/top1 = 0
I0816 10:41:48.897279  4780 caffe.cpp:313] Batch 251, accuracy/top5 = 0.02
I0816 10:41:48.897282  4780 caffe.cpp:313] Batch 251, loss = 6.84937
I0816 10:41:48.960083  4780 caffe.cpp:313] Batch 252, accuracy/top1 = 0
I0816 10:41:48.960103  4780 caffe.cpp:313] Batch 252, accuracy/top5 = 0
I0816 10:41:48.960106  4780 caffe.cpp:313] Batch 252, loss = 6.99916
I0816 10:41:49.023150  4780 caffe.cpp:313] Batch 253, accuracy/top1 = 0.02
I0816 10:41:49.023169  4780 caffe.cpp:313] Batch 253, accuracy/top5 = 0.04
I0816 10:41:49.023174  4780 caffe.cpp:313] Batch 253, loss = 6.93556
I0816 10:41:49.086220  4780 caffe.cpp:313] Batch 254, accuracy/top1 = 0
I0816 10:41:49.086241  4780 caffe.cpp:313] Batch 254, accuracy/top5 = 0.06
I0816 10:41:49.086244  4780 caffe.cpp:313] Batch 254, loss = 6.78755
I0816 10:41:49.149181  4780 caffe.cpp:313] Batch 255, accuracy/top1 = 0
I0816 10:41:49.149204  4780 caffe.cpp:313] Batch 255, accuracy/top5 = 0.02
I0816 10:41:49.149209  4780 caffe.cpp:313] Batch 255, loss = 6.65278
I0816 10:41:49.212121  4780 caffe.cpp:313] Batch 256, accuracy/top1 = 0
I0816 10:41:49.212144  4780 caffe.cpp:313] Batch 256, accuracy/top5 = 0.02
I0816 10:41:49.212149  4780 caffe.cpp:313] Batch 256, loss = 7.1135
I0816 10:41:49.274897  4780 caffe.cpp:313] Batch 257, accuracy/top1 = 0
I0816 10:41:49.274917  4780 caffe.cpp:313] Batch 257, accuracy/top5 = 0.04
I0816 10:41:49.274922  4780 caffe.cpp:313] Batch 257, loss = 6.75834
I0816 10:41:49.337785  4780 caffe.cpp:313] Batch 258, accuracy/top1 = 0
I0816 10:41:49.337807  4780 caffe.cpp:313] Batch 258, accuracy/top5 = 0.1
I0816 10:41:49.337812  4780 caffe.cpp:313] Batch 258, loss = 6.47004
I0816 10:41:49.400689  4780 caffe.cpp:313] Batch 259, accuracy/top1 = 0
I0816 10:41:49.400712  4780 caffe.cpp:313] Batch 259, accuracy/top5 = 0
I0816 10:41:49.400715  4780 caffe.cpp:313] Batch 259, loss = 6.81382
I0816 10:41:49.463599  4780 caffe.cpp:313] Batch 260, accuracy/top1 = 0
I0816 10:41:49.463620  4780 caffe.cpp:313] Batch 260, accuracy/top5 = 0.02
I0816 10:41:49.463624  4780 caffe.cpp:313] Batch 260, loss = 6.64723
I0816 10:41:49.526510  4780 caffe.cpp:313] Batch 261, accuracy/top1 = 0.04
I0816 10:41:49.526533  4780 caffe.cpp:313] Batch 261, accuracy/top5 = 0.08
I0816 10:41:49.526537  4780 caffe.cpp:313] Batch 261, loss = 6.59737
I0816 10:41:49.589303  4780 caffe.cpp:313] Batch 262, accuracy/top1 = 0
I0816 10:41:49.589324  4780 caffe.cpp:313] Batch 262, accuracy/top5 = 0
I0816 10:41:49.589329  4780 caffe.cpp:313] Batch 262, loss = 6.71622
I0816 10:41:49.652235  4780 caffe.cpp:313] Batch 263, accuracy/top1 = 0.06
I0816 10:41:49.652256  4780 caffe.cpp:313] Batch 263, accuracy/top5 = 0.1
I0816 10:41:49.652259  4780 caffe.cpp:313] Batch 263, loss = 6.38868
I0816 10:41:49.715088  4780 caffe.cpp:313] Batch 264, accuracy/top1 = 0
I0816 10:41:49.715109  4780 caffe.cpp:313] Batch 264, accuracy/top5 = 0.02
I0816 10:41:49.715113  4780 caffe.cpp:313] Batch 264, loss = 6.87215
I0816 10:41:49.777884  4780 caffe.cpp:313] Batch 265, accuracy/top1 = 0.02
I0816 10:41:49.777906  4780 caffe.cpp:313] Batch 265, accuracy/top5 = 0.06
I0816 10:41:49.777910  4780 caffe.cpp:313] Batch 265, loss = 6.59849
I0816 10:41:49.840780  4780 caffe.cpp:313] Batch 266, accuracy/top1 = 0.04
I0816 10:41:49.840802  4780 caffe.cpp:313] Batch 266, accuracy/top5 = 0.08
I0816 10:41:49.840806  4780 caffe.cpp:313] Batch 266, loss = 6.55866
I0816 10:41:49.903839  4780 caffe.cpp:313] Batch 267, accuracy/top1 = 0
I0816 10:41:49.903861  4780 caffe.cpp:313] Batch 267, accuracy/top5 = 0.02
I0816 10:41:49.903864  4780 caffe.cpp:313] Batch 267, loss = 6.89263
I0816 10:41:49.966945  4780 caffe.cpp:313] Batch 268, accuracy/top1 = 0
I0816 10:41:49.966967  4780 caffe.cpp:313] Batch 268, accuracy/top5 = 0.02
I0816 10:41:49.966972  4780 caffe.cpp:313] Batch 268, loss = 6.80078
I0816 10:41:50.029994  4780 caffe.cpp:313] Batch 269, accuracy/top1 = 0
I0816 10:41:50.030014  4780 caffe.cpp:313] Batch 269, accuracy/top5 = 0
I0816 10:41:50.030017  4780 caffe.cpp:313] Batch 269, loss = 6.73038
I0816 10:41:50.092736  4780 caffe.cpp:313] Batch 270, accuracy/top1 = 0
I0816 10:41:50.092756  4780 caffe.cpp:313] Batch 270, accuracy/top5 = 0.02
I0816 10:41:50.092761  4780 caffe.cpp:313] Batch 270, loss = 6.76014
I0816 10:41:50.155668  4780 caffe.cpp:313] Batch 271, accuracy/top1 = 0.02
I0816 10:41:50.155690  4780 caffe.cpp:313] Batch 271, accuracy/top5 = 0.08
I0816 10:41:50.155694  4780 caffe.cpp:313] Batch 271, loss = 6.49757
I0816 10:41:50.218652  4780 caffe.cpp:313] Batch 272, accuracy/top1 = 0.04
I0816 10:41:50.218672  4780 caffe.cpp:313] Batch 272, accuracy/top5 = 0.06
I0816 10:41:50.218677  4780 caffe.cpp:313] Batch 272, loss = 6.47462
I0816 10:41:50.281641  4780 caffe.cpp:313] Batch 273, accuracy/top1 = 0.08
I0816 10:41:50.281663  4780 caffe.cpp:313] Batch 273, accuracy/top5 = 0.12
I0816 10:41:50.281667  4780 caffe.cpp:313] Batch 273, loss = 6.72129
I0816 10:41:50.344646  4780 caffe.cpp:313] Batch 274, accuracy/top1 = 0.02
I0816 10:41:50.344666  4780 caffe.cpp:313] Batch 274, accuracy/top5 = 0.04
I0816 10:41:50.344669  4780 caffe.cpp:313] Batch 274, loss = 6.5418
I0816 10:41:50.407665  4780 caffe.cpp:313] Batch 275, accuracy/top1 = 0.02
I0816 10:41:50.407685  4780 caffe.cpp:313] Batch 275, accuracy/top5 = 0.04
I0816 10:41:50.407688  4780 caffe.cpp:313] Batch 275, loss = 6.7214
I0816 10:41:50.470592  4780 caffe.cpp:313] Batch 276, accuracy/top1 = 0
I0816 10:41:50.470613  4780 caffe.cpp:313] Batch 276, accuracy/top5 = 0.02
I0816 10:41:50.470616  4780 caffe.cpp:313] Batch 276, loss = 6.85083
I0816 10:41:50.533318  4780 caffe.cpp:313] Batch 277, accuracy/top1 = 0
I0816 10:41:50.533339  4780 caffe.cpp:313] Batch 277, accuracy/top5 = 0.02
I0816 10:41:50.533342  4780 caffe.cpp:313] Batch 277, loss = 6.84031
I0816 10:41:50.596114  4780 caffe.cpp:313] Batch 278, accuracy/top1 = 0.02
I0816 10:41:50.596138  4780 caffe.cpp:313] Batch 278, accuracy/top5 = 0.04
I0816 10:41:50.596143  4780 caffe.cpp:313] Batch 278, loss = 6.78275
I0816 10:41:50.658965  4780 caffe.cpp:313] Batch 279, accuracy/top1 = 0
I0816 10:41:50.658987  4780 caffe.cpp:313] Batch 279, accuracy/top5 = 0.04
I0816 10:41:50.658990  4780 caffe.cpp:313] Batch 279, loss = 6.91963
I0816 10:41:50.721694  4780 caffe.cpp:313] Batch 280, accuracy/top1 = 0.02
I0816 10:41:50.721715  4780 caffe.cpp:313] Batch 280, accuracy/top5 = 0.06
I0816 10:41:50.721719  4780 caffe.cpp:313] Batch 280, loss = 6.96264
I0816 10:41:50.784482  4780 caffe.cpp:313] Batch 281, accuracy/top1 = 0.04
I0816 10:41:50.784503  4780 caffe.cpp:313] Batch 281, accuracy/top5 = 0.04
I0816 10:41:50.784507  4780 caffe.cpp:313] Batch 281, loss = 6.69977
I0816 10:41:50.847399  4780 caffe.cpp:313] Batch 282, accuracy/top1 = 0
I0816 10:41:50.847420  4780 caffe.cpp:313] Batch 282, accuracy/top5 = 0.02
I0816 10:41:50.847424  4780 caffe.cpp:313] Batch 282, loss = 6.90849
I0816 10:41:50.910182  4780 caffe.cpp:313] Batch 283, accuracy/top1 = 0
I0816 10:41:50.910202  4780 caffe.cpp:313] Batch 283, accuracy/top5 = 0.02
I0816 10:41:50.910207  4780 caffe.cpp:313] Batch 283, loss = 6.77065
I0816 10:41:50.973302  4780 caffe.cpp:313] Batch 284, accuracy/top1 = 0
I0816 10:41:50.973326  4780 caffe.cpp:313] Batch 284, accuracy/top5 = 0.04
I0816 10:41:50.973330  4780 caffe.cpp:313] Batch 284, loss = 6.62049
I0816 10:41:51.036375  4780 caffe.cpp:313] Batch 285, accuracy/top1 = 0.02
I0816 10:41:51.036394  4780 caffe.cpp:313] Batch 285, accuracy/top5 = 0.04
I0816 10:41:51.036397  4780 caffe.cpp:313] Batch 285, loss = 6.34797
I0816 10:41:51.099514  4780 caffe.cpp:313] Batch 286, accuracy/top1 = 0
I0816 10:41:51.099530  4780 caffe.cpp:313] Batch 286, accuracy/top5 = 0.02
I0816 10:41:51.099535  4780 caffe.cpp:313] Batch 286, loss = 6.65748
I0816 10:41:51.162353  4780 caffe.cpp:313] Batch 287, accuracy/top1 = 0
I0816 10:41:51.162374  4780 caffe.cpp:313] Batch 287, accuracy/top5 = 0
I0816 10:41:51.162379  4780 caffe.cpp:313] Batch 287, loss = 6.60385
I0816 10:41:51.225093  4780 caffe.cpp:313] Batch 288, accuracy/top1 = 0.02
I0816 10:41:51.225116  4780 caffe.cpp:313] Batch 288, accuracy/top5 = 0.04
I0816 10:41:51.225119  4780 caffe.cpp:313] Batch 288, loss = 6.7376
I0816 10:41:51.287963  4780 caffe.cpp:313] Batch 289, accuracy/top1 = 0.02
I0816 10:41:51.287984  4780 caffe.cpp:313] Batch 289, accuracy/top5 = 0.04
I0816 10:41:51.287989  4780 caffe.cpp:313] Batch 289, loss = 6.61707
I0816 10:41:51.350867  4780 caffe.cpp:313] Batch 290, accuracy/top1 = 0.04
I0816 10:41:51.350888  4780 caffe.cpp:313] Batch 290, accuracy/top5 = 0.06
I0816 10:41:51.350891  4780 caffe.cpp:313] Batch 290, loss = 6.7489
I0816 10:41:51.413877  4780 caffe.cpp:313] Batch 291, accuracy/top1 = 0.02
I0816 10:41:51.413897  4780 caffe.cpp:313] Batch 291, accuracy/top5 = 0.04
I0816 10:41:51.413902  4780 caffe.cpp:313] Batch 291, loss = 6.59333
I0816 10:41:51.476593  4780 caffe.cpp:313] Batch 292, accuracy/top1 = 0
I0816 10:41:51.476614  4780 caffe.cpp:313] Batch 292, accuracy/top5 = 0.02
I0816 10:41:51.476619  4780 caffe.cpp:313] Batch 292, loss = 6.68273
I0816 10:41:51.539513  4780 caffe.cpp:313] Batch 293, accuracy/top1 = 0
I0816 10:41:51.539535  4780 caffe.cpp:313] Batch 293, accuracy/top5 = 0.04
I0816 10:41:51.539539  4780 caffe.cpp:313] Batch 293, loss = 6.90589
I0816 10:41:51.602434  4780 caffe.cpp:313] Batch 294, accuracy/top1 = 0
I0816 10:41:51.602455  4780 caffe.cpp:313] Batch 294, accuracy/top5 = 0.02
I0816 10:41:51.602474  4780 caffe.cpp:313] Batch 294, loss = 6.73906
I0816 10:41:51.665432  4780 caffe.cpp:313] Batch 295, accuracy/top1 = 0
I0816 10:41:51.665453  4780 caffe.cpp:313] Batch 295, accuracy/top5 = 0
I0816 10:41:51.665457  4780 caffe.cpp:313] Batch 295, loss = 7.07828
I0816 10:41:51.728204  4780 caffe.cpp:313] Batch 296, accuracy/top1 = 0.02
I0816 10:41:51.728226  4780 caffe.cpp:313] Batch 296, accuracy/top5 = 0.02
I0816 10:41:51.728230  4780 caffe.cpp:313] Batch 296, loss = 6.74291
I0816 10:41:51.791118  4780 caffe.cpp:313] Batch 297, accuracy/top1 = 0
I0816 10:41:51.791139  4780 caffe.cpp:313] Batch 297, accuracy/top5 = 0.02
I0816 10:41:51.791144  4780 caffe.cpp:313] Batch 297, loss = 6.67059
I0816 10:41:51.854050  4780 caffe.cpp:313] Batch 298, accuracy/top1 = 0
I0816 10:41:51.854073  4780 caffe.cpp:313] Batch 298, accuracy/top5 = 0.02
I0816 10:41:51.854077  4780 caffe.cpp:313] Batch 298, loss = 6.77077
I0816 10:41:51.916991  4780 caffe.cpp:313] Batch 299, accuracy/top1 = 0
I0816 10:41:51.917013  4780 caffe.cpp:313] Batch 299, accuracy/top5 = 0
I0816 10:41:51.917017  4780 caffe.cpp:313] Batch 299, loss = 6.97543
I0816 10:41:51.980199  4780 caffe.cpp:313] Batch 300, accuracy/top1 = 0.04
I0816 10:41:51.980217  4780 caffe.cpp:313] Batch 300, accuracy/top5 = 0.06
I0816 10:41:51.980221  4780 caffe.cpp:313] Batch 300, loss = 6.64023
I0816 10:41:52.043105  4780 caffe.cpp:313] Batch 301, accuracy/top1 = 0
I0816 10:41:52.043123  4780 caffe.cpp:313] Batch 301, accuracy/top5 = 0.02
I0816 10:41:52.043128  4780 caffe.cpp:313] Batch 301, loss = 6.78635
I0816 10:41:52.105983  4780 caffe.cpp:313] Batch 302, accuracy/top1 = 0.02
I0816 10:41:52.106003  4780 caffe.cpp:313] Batch 302, accuracy/top5 = 0.02
I0816 10:41:52.106007  4780 caffe.cpp:313] Batch 302, loss = 6.53906
I0816 10:41:52.168787  4780 caffe.cpp:313] Batch 303, accuracy/top1 = 0
I0816 10:41:52.168807  4780 caffe.cpp:313] Batch 303, accuracy/top5 = 0
I0816 10:41:52.168810  4780 caffe.cpp:313] Batch 303, loss = 6.69793
I0816 10:41:52.231597  4780 caffe.cpp:313] Batch 304, accuracy/top1 = 0
I0816 10:41:52.231618  4780 caffe.cpp:313] Batch 304, accuracy/top5 = 0.02
I0816 10:41:52.231622  4780 caffe.cpp:313] Batch 304, loss = 6.69393
I0816 10:41:52.294394  4780 caffe.cpp:313] Batch 305, accuracy/top1 = 0
I0816 10:41:52.294415  4780 caffe.cpp:313] Batch 305, accuracy/top5 = 0
I0816 10:41:52.294420  4780 caffe.cpp:313] Batch 305, loss = 6.68991
I0816 10:41:52.357336  4780 caffe.cpp:313] Batch 306, accuracy/top1 = 0
I0816 10:41:52.357357  4780 caffe.cpp:313] Batch 306, accuracy/top5 = 0.02
I0816 10:41:52.357360  4780 caffe.cpp:313] Batch 306, loss = 6.62658
I0816 10:41:52.420327  4780 caffe.cpp:313] Batch 307, accuracy/top1 = 0.02
I0816 10:41:52.420347  4780 caffe.cpp:313] Batch 307, accuracy/top5 = 0.04
I0816 10:41:52.420351  4780 caffe.cpp:313] Batch 307, loss = 6.75183
I0816 10:41:52.483359  4780 caffe.cpp:313] Batch 308, accuracy/top1 = 0.04
I0816 10:41:52.483378  4780 caffe.cpp:313] Batch 308, accuracy/top5 = 0.1
I0816 10:41:52.483382  4780 caffe.cpp:313] Batch 308, loss = 6.52508
I0816 10:41:52.546454  4780 caffe.cpp:313] Batch 309, accuracy/top1 = 0
I0816 10:41:52.546473  4780 caffe.cpp:313] Batch 309, accuracy/top5 = 0.02
I0816 10:41:52.546478  4780 caffe.cpp:313] Batch 309, loss = 6.99954
I0816 10:41:52.609261  4780 caffe.cpp:313] Batch 310, accuracy/top1 = 0.04
I0816 10:41:52.609282  4780 caffe.cpp:313] Batch 310, accuracy/top5 = 0.1
I0816 10:41:52.609287  4780 caffe.cpp:313] Batch 310, loss = 6.44062
I0816 10:41:52.672091  4780 caffe.cpp:313] Batch 311, accuracy/top1 = 0
I0816 10:41:52.672112  4780 caffe.cpp:313] Batch 311, accuracy/top5 = 0
I0816 10:41:52.672116  4780 caffe.cpp:313] Batch 311, loss = 7.09568
I0816 10:41:52.734807  4780 caffe.cpp:313] Batch 312, accuracy/top1 = 0
I0816 10:41:52.734827  4780 caffe.cpp:313] Batch 312, accuracy/top5 = 0.02
I0816 10:41:52.734832  4780 caffe.cpp:313] Batch 312, loss = 6.68518
I0816 10:41:52.797608  4780 caffe.cpp:313] Batch 313, accuracy/top1 = 0.04
I0816 10:41:52.797629  4780 caffe.cpp:313] Batch 313, accuracy/top5 = 0.06
I0816 10:41:52.797649  4780 caffe.cpp:313] Batch 313, loss = 6.68248
I0816 10:41:52.860513  4780 caffe.cpp:313] Batch 314, accuracy/top1 = 0
I0816 10:41:52.860536  4780 caffe.cpp:313] Batch 314, accuracy/top5 = 0.02
I0816 10:41:52.860540  4780 caffe.cpp:313] Batch 314, loss = 6.69199
I0816 10:41:52.923432  4780 caffe.cpp:313] Batch 315, accuracy/top1 = 0
I0816 10:41:52.923454  4780 caffe.cpp:313] Batch 315, accuracy/top5 = 0.04
I0816 10:41:52.923458  4780 caffe.cpp:313] Batch 315, loss = 6.59825
I0816 10:41:52.986683  4780 caffe.cpp:313] Batch 316, accuracy/top1 = 0
I0816 10:41:52.986701  4780 caffe.cpp:313] Batch 316, accuracy/top5 = 0
I0816 10:41:52.986704  4780 caffe.cpp:313] Batch 316, loss = 6.84045
I0816 10:41:53.049571  4780 caffe.cpp:313] Batch 317, accuracy/top1 = 0
I0816 10:41:53.049590  4780 caffe.cpp:313] Batch 317, accuracy/top5 = 0.04
I0816 10:41:53.049594  4780 caffe.cpp:313] Batch 317, loss = 6.77027
I0816 10:41:53.112586  4780 caffe.cpp:313] Batch 318, accuracy/top1 = 0
I0816 10:41:53.112602  4780 caffe.cpp:313] Batch 318, accuracy/top5 = 0.06
I0816 10:41:53.112607  4780 caffe.cpp:313] Batch 318, loss = 6.71941
I0816 10:41:53.175560  4780 caffe.cpp:313] Batch 319, accuracy/top1 = 0
I0816 10:41:53.175582  4780 caffe.cpp:313] Batch 319, accuracy/top5 = 0
I0816 10:41:53.175586  4780 caffe.cpp:313] Batch 319, loss = 7.09354
I0816 10:41:53.238541  4780 caffe.cpp:313] Batch 320, accuracy/top1 = 0
I0816 10:41:53.238564  4780 caffe.cpp:313] Batch 320, accuracy/top5 = 0
I0816 10:41:53.238566  4780 caffe.cpp:313] Batch 320, loss = 6.8381
I0816 10:41:53.301296  4780 caffe.cpp:313] Batch 321, accuracy/top1 = 0.02
I0816 10:41:53.301318  4780 caffe.cpp:313] Batch 321, accuracy/top5 = 0.04
I0816 10:41:53.301321  4780 caffe.cpp:313] Batch 321, loss = 6.71833
I0816 10:41:53.364245  4780 caffe.cpp:313] Batch 322, accuracy/top1 = 0
I0816 10:41:53.364267  4780 caffe.cpp:313] Batch 322, accuracy/top5 = 0
I0816 10:41:53.364271  4780 caffe.cpp:313] Batch 322, loss = 6.98324
I0816 10:41:53.427036  4780 caffe.cpp:313] Batch 323, accuracy/top1 = 0
I0816 10:41:53.427057  4780 caffe.cpp:313] Batch 323, accuracy/top5 = 0.04
I0816 10:41:53.427060  4780 caffe.cpp:313] Batch 323, loss = 6.73208
I0816 10:41:53.490103  4780 caffe.cpp:313] Batch 324, accuracy/top1 = 0
I0816 10:41:53.490124  4780 caffe.cpp:313] Batch 324, accuracy/top5 = 0
I0816 10:41:53.490128  4780 caffe.cpp:313] Batch 324, loss = 6.84329
I0816 10:41:53.552999  4780 caffe.cpp:313] Batch 325, accuracy/top1 = 0.02
I0816 10:41:53.553021  4780 caffe.cpp:313] Batch 325, accuracy/top5 = 0.04
I0816 10:41:53.553025  4780 caffe.cpp:313] Batch 325, loss = 6.71501
I0816 10:41:53.615887  4780 caffe.cpp:313] Batch 326, accuracy/top1 = 0
I0816 10:41:53.615908  4780 caffe.cpp:313] Batch 326, accuracy/top5 = 0.02
I0816 10:41:53.615912  4780 caffe.cpp:313] Batch 326, loss = 6.83041
I0816 10:41:53.678647  4780 caffe.cpp:313] Batch 327, accuracy/top1 = 0.04
I0816 10:41:53.678668  4780 caffe.cpp:313] Batch 327, accuracy/top5 = 0.08
I0816 10:41:53.678673  4780 caffe.cpp:313] Batch 327, loss = 6.50126
I0816 10:41:53.741493  4780 caffe.cpp:313] Batch 328, accuracy/top1 = 0
I0816 10:41:53.741515  4780 caffe.cpp:313] Batch 328, accuracy/top5 = 0.02
I0816 10:41:53.741520  4780 caffe.cpp:313] Batch 328, loss = 6.80659
I0816 10:41:53.804308  4780 caffe.cpp:313] Batch 329, accuracy/top1 = 0
I0816 10:41:53.804329  4780 caffe.cpp:313] Batch 329, accuracy/top5 = 0
I0816 10:41:53.804333  4780 caffe.cpp:313] Batch 329, loss = 6.86037
I0816 10:41:53.867167  4780 caffe.cpp:313] Batch 330, accuracy/top1 = 0.02
I0816 10:41:53.867189  4780 caffe.cpp:313] Batch 330, accuracy/top5 = 0.06
I0816 10:41:53.867193  4780 caffe.cpp:313] Batch 330, loss = 6.73044
I0816 10:41:53.930100  4780 caffe.cpp:313] Batch 331, accuracy/top1 = 0.02
I0816 10:41:53.930121  4780 caffe.cpp:313] Batch 331, accuracy/top5 = 0.04
I0816 10:41:53.930126  4780 caffe.cpp:313] Batch 331, loss = 6.56599
I0816 10:41:53.993284  4780 caffe.cpp:313] Batch 332, accuracy/top1 = 0
I0816 10:41:53.993300  4780 caffe.cpp:313] Batch 332, accuracy/top5 = 0
I0816 10:41:53.993320  4780 caffe.cpp:313] Batch 332, loss = 7.02645
I0816 10:41:54.056246  4780 caffe.cpp:313] Batch 333, accuracy/top1 = 0.02
I0816 10:41:54.056265  4780 caffe.cpp:313] Batch 333, accuracy/top5 = 0.02
I0816 10:41:54.056269  4780 caffe.cpp:313] Batch 333, loss = 6.80658
I0816 10:41:54.119020  4780 caffe.cpp:313] Batch 334, accuracy/top1 = 0
I0816 10:41:54.119041  4780 caffe.cpp:313] Batch 334, accuracy/top5 = 0
I0816 10:41:54.119045  4780 caffe.cpp:313] Batch 334, loss = 6.70889
I0816 10:41:54.181921  4780 caffe.cpp:313] Batch 335, accuracy/top1 = 0
I0816 10:41:54.181942  4780 caffe.cpp:313] Batch 335, accuracy/top5 = 0.04
I0816 10:41:54.181947  4780 caffe.cpp:313] Batch 335, loss = 6.83672
I0816 10:41:54.244710  4780 caffe.cpp:313] Batch 336, accuracy/top1 = 0.02
I0816 10:41:54.244731  4780 caffe.cpp:313] Batch 336, accuracy/top5 = 0.02
I0816 10:41:54.244735  4780 caffe.cpp:313] Batch 336, loss = 6.88687
I0816 10:41:54.307634  4780 caffe.cpp:313] Batch 337, accuracy/top1 = 0.02
I0816 10:41:54.307656  4780 caffe.cpp:313] Batch 337, accuracy/top5 = 0.06
I0816 10:41:54.307659  4780 caffe.cpp:313] Batch 337, loss = 6.4144
I0816 10:41:54.370574  4780 caffe.cpp:313] Batch 338, accuracy/top1 = 0
I0816 10:41:54.370596  4780 caffe.cpp:313] Batch 338, accuracy/top5 = 0.02
I0816 10:41:54.370600  4780 caffe.cpp:313] Batch 338, loss = 6.96346
I0816 10:41:54.433480  4780 caffe.cpp:313] Batch 339, accuracy/top1 = 0
I0816 10:41:54.433501  4780 caffe.cpp:313] Batch 339, accuracy/top5 = 0.02
I0816 10:41:54.433506  4780 caffe.cpp:313] Batch 339, loss = 6.95704
I0816 10:41:54.496320  4780 caffe.cpp:313] Batch 340, accuracy/top1 = 0
I0816 10:41:54.496341  4780 caffe.cpp:313] Batch 340, accuracy/top5 = 0
I0816 10:41:54.496345  4780 caffe.cpp:313] Batch 340, loss = 6.71867
I0816 10:41:54.559245  4780 caffe.cpp:313] Batch 341, accuracy/top1 = 0.02
I0816 10:41:54.559265  4780 caffe.cpp:313] Batch 341, accuracy/top5 = 0.04
I0816 10:41:54.559269  4780 caffe.cpp:313] Batch 341, loss = 6.87828
I0816 10:41:54.622361  4780 caffe.cpp:313] Batch 342, accuracy/top1 = 0
I0816 10:41:54.622380  4780 caffe.cpp:313] Batch 342, accuracy/top5 = 0.02
I0816 10:41:54.622385  4780 caffe.cpp:313] Batch 342, loss = 6.6356
I0816 10:41:54.685350  4780 caffe.cpp:313] Batch 343, accuracy/top1 = 0
I0816 10:41:54.685370  4780 caffe.cpp:313] Batch 343, accuracy/top5 = 0.02
I0816 10:41:54.685374  4780 caffe.cpp:313] Batch 343, loss = 6.78595
I0816 10:41:54.748162  4780 caffe.cpp:313] Batch 344, accuracy/top1 = 0
I0816 10:41:54.748184  4780 caffe.cpp:313] Batch 344, accuracy/top5 = 0.06
I0816 10:41:54.748188  4780 caffe.cpp:313] Batch 344, loss = 6.92524
I0816 10:41:54.810899  4780 caffe.cpp:313] Batch 345, accuracy/top1 = 0.02
I0816 10:41:54.810920  4780 caffe.cpp:313] Batch 345, accuracy/top5 = 0.04
I0816 10:41:54.810923  4780 caffe.cpp:313] Batch 345, loss = 6.54972
I0816 10:41:54.873765  4780 caffe.cpp:313] Batch 346, accuracy/top1 = 0
I0816 10:41:54.873786  4780 caffe.cpp:313] Batch 346, accuracy/top5 = 0
I0816 10:41:54.873790  4780 caffe.cpp:313] Batch 346, loss = 6.81323
I0816 10:41:54.936553  4780 caffe.cpp:313] Batch 347, accuracy/top1 = 0
I0816 10:41:54.936574  4780 caffe.cpp:313] Batch 347, accuracy/top5 = 0.02
I0816 10:41:54.936579  4780 caffe.cpp:313] Batch 347, loss = 6.73782
I0816 10:41:54.999894  4780 caffe.cpp:313] Batch 348, accuracy/top1 = 0.02
I0816 10:41:54.999912  4780 caffe.cpp:313] Batch 348, accuracy/top5 = 0.02
I0816 10:41:54.999917  4780 caffe.cpp:313] Batch 348, loss = 6.79314
I0816 10:41:55.062676  4780 caffe.cpp:313] Batch 349, accuracy/top1 = 0
I0816 10:41:55.062696  4780 caffe.cpp:313] Batch 349, accuracy/top5 = 0
I0816 10:41:55.062700  4780 caffe.cpp:313] Batch 349, loss = 6.83353
I0816 10:41:55.125566  4780 caffe.cpp:313] Batch 350, accuracy/top1 = 0.02
I0816 10:41:55.125581  4780 caffe.cpp:313] Batch 350, accuracy/top5 = 0.1
I0816 10:41:55.125586  4780 caffe.cpp:313] Batch 350, loss = 6.33319
I0816 10:41:55.188724  4780 caffe.cpp:313] Batch 351, accuracy/top1 = 0
I0816 10:41:55.188745  4780 caffe.cpp:313] Batch 351, accuracy/top5 = 0
I0816 10:41:55.188765  4780 caffe.cpp:313] Batch 351, loss = 6.87009
I0816 10:41:55.251629  4780 caffe.cpp:313] Batch 352, accuracy/top1 = 0
I0816 10:41:55.251652  4780 caffe.cpp:313] Batch 352, accuracy/top5 = 0
I0816 10:41:55.251655  4780 caffe.cpp:313] Batch 352, loss = 6.72369
I0816 10:41:55.314563  4780 caffe.cpp:313] Batch 353, accuracy/top1 = 0.04
I0816 10:41:55.314584  4780 caffe.cpp:313] Batch 353, accuracy/top5 = 0.06
I0816 10:41:55.314589  4780 caffe.cpp:313] Batch 353, loss = 6.59774
I0816 10:41:55.377290  4780 caffe.cpp:313] Batch 354, accuracy/top1 = 0.02
I0816 10:41:55.377310  4780 caffe.cpp:313] Batch 354, accuracy/top5 = 0.04
I0816 10:41:55.377315  4780 caffe.cpp:313] Batch 354, loss = 6.65535
I0816 10:41:55.440255  4780 caffe.cpp:313] Batch 355, accuracy/top1 = 0.04
I0816 10:41:55.440277  4780 caffe.cpp:313] Batch 355, accuracy/top5 = 0.06
I0816 10:41:55.440281  4780 caffe.cpp:313] Batch 355, loss = 6.63964
I0816 10:41:55.503193  4780 caffe.cpp:313] Batch 356, accuracy/top1 = 0.02
I0816 10:41:55.503216  4780 caffe.cpp:313] Batch 356, accuracy/top5 = 0.06
I0816 10:41:55.503219  4780 caffe.cpp:313] Batch 356, loss = 6.69493
I0816 10:41:55.566090  4780 caffe.cpp:313] Batch 357, accuracy/top1 = 0
I0816 10:41:55.566112  4780 caffe.cpp:313] Batch 357, accuracy/top5 = 0
I0816 10:41:55.566117  4780 caffe.cpp:313] Batch 357, loss = 7.16927
I0816 10:41:55.629052  4780 caffe.cpp:313] Batch 358, accuracy/top1 = 0
I0816 10:41:55.629076  4780 caffe.cpp:313] Batch 358, accuracy/top5 = 0
I0816 10:41:55.629079  4780 caffe.cpp:313] Batch 358, loss = 6.64183
I0816 10:41:55.691928  4780 caffe.cpp:313] Batch 359, accuracy/top1 = 0
I0816 10:41:55.691949  4780 caffe.cpp:313] Batch 359, accuracy/top5 = 0
I0816 10:41:55.691953  4780 caffe.cpp:313] Batch 359, loss = 6.8133
I0816 10:41:55.754716  4780 caffe.cpp:313] Batch 360, accuracy/top1 = 0
I0816 10:41:55.754737  4780 caffe.cpp:313] Batch 360, accuracy/top5 = 0
I0816 10:41:55.754741  4780 caffe.cpp:313] Batch 360, loss = 6.62449
I0816 10:41:55.817651  4780 caffe.cpp:313] Batch 361, accuracy/top1 = 0
I0816 10:41:55.817672  4780 caffe.cpp:313] Batch 361, accuracy/top5 = 0
I0816 10:41:55.817677  4780 caffe.cpp:313] Batch 361, loss = 6.853
I0816 10:41:55.880434  4780 caffe.cpp:313] Batch 362, accuracy/top1 = 0.02
I0816 10:41:55.880456  4780 caffe.cpp:313] Batch 362, accuracy/top5 = 0.06
I0816 10:41:55.880461  4780 caffe.cpp:313] Batch 362, loss = 6.86798
I0816 10:41:55.943331  4780 caffe.cpp:313] Batch 363, accuracy/top1 = 0
I0816 10:41:55.943352  4780 caffe.cpp:313] Batch 363, accuracy/top5 = 0.02
I0816 10:41:55.943356  4780 caffe.cpp:313] Batch 363, loss = 6.39292
I0816 10:41:56.006566  4780 caffe.cpp:313] Batch 364, accuracy/top1 = 0.02
I0816 10:41:56.006583  4780 caffe.cpp:313] Batch 364, accuracy/top5 = 0.04
I0816 10:41:56.006588  4780 caffe.cpp:313] Batch 364, loss = 6.87146
I0816 10:41:56.069582  4780 caffe.cpp:313] Batch 365, accuracy/top1 = 0
I0816 10:41:56.069603  4780 caffe.cpp:313] Batch 365, accuracy/top5 = 0.02
I0816 10:41:56.069607  4780 caffe.cpp:313] Batch 365, loss = 6.88054
I0816 10:41:56.132542  4780 caffe.cpp:313] Batch 366, accuracy/top1 = 0.04
I0816 10:41:56.132563  4780 caffe.cpp:313] Batch 366, accuracy/top5 = 0.06
I0816 10:41:56.132567  4780 caffe.cpp:313] Batch 366, loss = 6.65207
I0816 10:41:56.195384  4780 caffe.cpp:313] Batch 367, accuracy/top1 = 0.02
I0816 10:41:56.195405  4780 caffe.cpp:313] Batch 367, accuracy/top5 = 0.08
I0816 10:41:56.195410  4780 caffe.cpp:313] Batch 367, loss = 6.79684
I0816 10:41:56.258380  4780 caffe.cpp:313] Batch 368, accuracy/top1 = 0.04
I0816 10:41:56.258404  4780 caffe.cpp:313] Batch 368, accuracy/top5 = 0.04
I0816 10:41:56.258407  4780 caffe.cpp:313] Batch 368, loss = 6.68359
I0816 10:41:56.321279  4780 caffe.cpp:313] Batch 369, accuracy/top1 = 0.02
I0816 10:41:56.321300  4780 caffe.cpp:313] Batch 369, accuracy/top5 = 0.06
I0816 10:41:56.321303  4780 caffe.cpp:313] Batch 369, loss = 6.38065
I0816 10:41:56.384263  4780 caffe.cpp:313] Batch 370, accuracy/top1 = 0.04
I0816 10:41:56.384284  4780 caffe.cpp:313] Batch 370, accuracy/top5 = 0.06
I0816 10:41:56.384304  4780 caffe.cpp:313] Batch 370, loss = 6.51939
I0816 10:41:56.447222  4780 caffe.cpp:313] Batch 371, accuracy/top1 = 0
I0816 10:41:56.447244  4780 caffe.cpp:313] Batch 371, accuracy/top5 = 0.04
I0816 10:41:56.447248  4780 caffe.cpp:313] Batch 371, loss = 6.68402
I0816 10:41:56.510115  4780 caffe.cpp:313] Batch 372, accuracy/top1 = 0
I0816 10:41:56.510136  4780 caffe.cpp:313] Batch 372, accuracy/top5 = 0.08
I0816 10:41:56.510141  4780 caffe.cpp:313] Batch 372, loss = 6.47664
I0816 10:41:56.573010  4780 caffe.cpp:313] Batch 373, accuracy/top1 = 0
I0816 10:41:56.573032  4780 caffe.cpp:313] Batch 373, accuracy/top5 = 0
I0816 10:41:56.573035  4780 caffe.cpp:313] Batch 373, loss = 6.80768
I0816 10:41:56.636010  4780 caffe.cpp:313] Batch 374, accuracy/top1 = 0.02
I0816 10:41:56.636032  4780 caffe.cpp:313] Batch 374, accuracy/top5 = 0.02
I0816 10:41:56.636036  4780 caffe.cpp:313] Batch 374, loss = 6.67528
I0816 10:41:56.698905  4780 caffe.cpp:313] Batch 375, accuracy/top1 = 0
I0816 10:41:56.698923  4780 caffe.cpp:313] Batch 375, accuracy/top5 = 0.04
I0816 10:41:56.698928  4780 caffe.cpp:313] Batch 375, loss = 6.71987
I0816 10:41:56.762076  4780 caffe.cpp:313] Batch 376, accuracy/top1 = 0.02
I0816 10:41:56.762094  4780 caffe.cpp:313] Batch 376, accuracy/top5 = 0.02
I0816 10:41:56.762099  4780 caffe.cpp:313] Batch 376, loss = 6.75508
I0816 10:41:56.825105  4780 caffe.cpp:313] Batch 377, accuracy/top1 = 0
I0816 10:41:56.825126  4780 caffe.cpp:313] Batch 377, accuracy/top5 = 0
I0816 10:41:56.825130  4780 caffe.cpp:313] Batch 377, loss = 6.88996
I0816 10:41:56.888015  4780 caffe.cpp:313] Batch 378, accuracy/top1 = 0
I0816 10:41:56.888036  4780 caffe.cpp:313] Batch 378, accuracy/top5 = 0.04
I0816 10:41:56.888041  4780 caffe.cpp:313] Batch 378, loss = 6.62851
I0816 10:41:56.951009  4780 caffe.cpp:313] Batch 379, accuracy/top1 = 0.02
I0816 10:41:56.951031  4780 caffe.cpp:313] Batch 379, accuracy/top5 = 0.04
I0816 10:41:56.951035  4780 caffe.cpp:313] Batch 379, loss = 6.67571
I0816 10:41:57.014286  4780 caffe.cpp:313] Batch 380, accuracy/top1 = 0
I0816 10:41:57.014303  4780 caffe.cpp:313] Batch 380, accuracy/top5 = 0.02
I0816 10:41:57.014307  4780 caffe.cpp:313] Batch 380, loss = 6.8488
I0816 10:41:57.077180  4780 caffe.cpp:313] Batch 381, accuracy/top1 = 0.02
I0816 10:41:57.077201  4780 caffe.cpp:313] Batch 381, accuracy/top5 = 0.04
I0816 10:41:57.077205  4780 caffe.cpp:313] Batch 381, loss = 6.83065
I0816 10:41:57.140085  4780 caffe.cpp:313] Batch 382, accuracy/top1 = 0.02
I0816 10:41:57.140108  4780 caffe.cpp:313] Batch 382, accuracy/top5 = 0.12
I0816 10:41:57.140112  4780 caffe.cpp:313] Batch 382, loss = 6.62505
I0816 10:41:57.203265  4780 caffe.cpp:313] Batch 383, accuracy/top1 = 0
I0816 10:41:57.203284  4780 caffe.cpp:313] Batch 383, accuracy/top5 = 0
I0816 10:41:57.203289  4780 caffe.cpp:313] Batch 383, loss = 6.84379
I0816 10:41:57.266243  4780 caffe.cpp:313] Batch 384, accuracy/top1 = 0.02
I0816 10:41:57.266265  4780 caffe.cpp:313] Batch 384, accuracy/top5 = 0.04
I0816 10:41:57.266269  4780 caffe.cpp:313] Batch 384, loss = 6.89904
I0816 10:41:57.329272  4780 caffe.cpp:313] Batch 385, accuracy/top1 = 0.02
I0816 10:41:57.329294  4780 caffe.cpp:313] Batch 385, accuracy/top5 = 0.04
I0816 10:41:57.329298  4780 caffe.cpp:313] Batch 385, loss = 6.55637
I0816 10:41:57.392196  4780 caffe.cpp:313] Batch 386, accuracy/top1 = 0.02
I0816 10:41:57.392218  4780 caffe.cpp:313] Batch 386, accuracy/top5 = 0.04
I0816 10:41:57.392221  4780 caffe.cpp:313] Batch 386, loss = 6.56479
I0816 10:41:57.455111  4780 caffe.cpp:313] Batch 387, accuracy/top1 = 0.02
I0816 10:41:57.455132  4780 caffe.cpp:313] Batch 387, accuracy/top5 = 0.06
I0816 10:41:57.455137  4780 caffe.cpp:313] Batch 387, loss = 6.76634
I0816 10:41:57.518043  4780 caffe.cpp:313] Batch 388, accuracy/top1 = 0.04
I0816 10:41:57.518064  4780 caffe.cpp:313] Batch 388, accuracy/top5 = 0.04
I0816 10:41:57.518067  4780 caffe.cpp:313] Batch 388, loss = 6.81766
I0816 10:41:57.580999  4780 caffe.cpp:313] Batch 389, accuracy/top1 = 0.02
I0816 10:41:57.581032  4780 caffe.cpp:313] Batch 389, accuracy/top5 = 0.04
I0816 10:41:57.581037  4780 caffe.cpp:313] Batch 389, loss = 6.96643
I0816 10:41:57.643867  4780 caffe.cpp:313] Batch 390, accuracy/top1 = 0
I0816 10:41:57.643889  4780 caffe.cpp:313] Batch 390, accuracy/top5 = 0.02
I0816 10:41:57.643893  4780 caffe.cpp:313] Batch 390, loss = 6.9482
I0816 10:41:57.706871  4780 caffe.cpp:313] Batch 391, accuracy/top1 = 0
I0816 10:41:57.706892  4780 caffe.cpp:313] Batch 391, accuracy/top5 = 0
I0816 10:41:57.706895  4780 caffe.cpp:313] Batch 391, loss = 6.79959
I0816 10:41:57.769795  4780 caffe.cpp:313] Batch 392, accuracy/top1 = 0.02
I0816 10:41:57.769816  4780 caffe.cpp:313] Batch 392, accuracy/top5 = 0.02
I0816 10:41:57.769820  4780 caffe.cpp:313] Batch 392, loss = 6.69552
I0816 10:41:57.832751  4780 caffe.cpp:313] Batch 393, accuracy/top1 = 0
I0816 10:41:57.832773  4780 caffe.cpp:313] Batch 393, accuracy/top5 = 0.04
I0816 10:41:57.832777  4780 caffe.cpp:313] Batch 393, loss = 6.9429
I0816 10:41:57.895715  4780 caffe.cpp:313] Batch 394, accuracy/top1 = 0
I0816 10:41:57.895737  4780 caffe.cpp:313] Batch 394, accuracy/top5 = 0.02
I0816 10:41:57.895741  4780 caffe.cpp:313] Batch 394, loss = 6.78381
I0816 10:41:57.958731  4780 caffe.cpp:313] Batch 395, accuracy/top1 = 0
I0816 10:41:57.958753  4780 caffe.cpp:313] Batch 395, accuracy/top5 = 0.02
I0816 10:41:57.958758  4780 caffe.cpp:313] Batch 395, loss = 7.08432
I0816 10:41:58.022018  4780 caffe.cpp:313] Batch 396, accuracy/top1 = 0
I0816 10:41:58.022038  4780 caffe.cpp:313] Batch 396, accuracy/top5 = 0.08
I0816 10:41:58.022042  4780 caffe.cpp:313] Batch 396, loss = 6.60961
I0816 10:41:58.084980  4780 caffe.cpp:313] Batch 397, accuracy/top1 = 0
I0816 10:41:58.085000  4780 caffe.cpp:313] Batch 397, accuracy/top5 = 0.02
I0816 10:41:58.085005  4780 caffe.cpp:313] Batch 397, loss = 7.0703
I0816 10:41:58.148012  4780 caffe.cpp:313] Batch 398, accuracy/top1 = 0
I0816 10:41:58.148032  4780 caffe.cpp:313] Batch 398, accuracy/top5 = 0.02
I0816 10:41:58.148036  4780 caffe.cpp:313] Batch 398, loss = 6.63724
I0816 10:41:58.210875  4780 caffe.cpp:313] Batch 399, accuracy/top1 = 0
I0816 10:41:58.210896  4780 caffe.cpp:313] Batch 399, accuracy/top5 = 0.02
I0816 10:41:58.210901  4780 caffe.cpp:313] Batch 399, loss = 6.8105
I0816 10:41:58.273782  4780 caffe.cpp:313] Batch 400, accuracy/top1 = 0
I0816 10:41:58.273803  4780 caffe.cpp:313] Batch 400, accuracy/top5 = 0.02
I0816 10:41:58.273808  4780 caffe.cpp:313] Batch 400, loss = 6.6608
I0816 10:41:58.336802  4780 caffe.cpp:313] Batch 401, accuracy/top1 = 0.02
I0816 10:41:58.336823  4780 caffe.cpp:313] Batch 401, accuracy/top5 = 0.06
I0816 10:41:58.336827  4780 caffe.cpp:313] Batch 401, loss = 6.69599
I0816 10:41:58.399693  4780 caffe.cpp:313] Batch 402, accuracy/top1 = 0
I0816 10:41:58.399714  4780 caffe.cpp:313] Batch 402, accuracy/top5 = 0
I0816 10:41:58.399718  4780 caffe.cpp:313] Batch 402, loss = 6.69042
I0816 10:41:58.462560  4780 caffe.cpp:313] Batch 403, accuracy/top1 = 0.02
I0816 10:41:58.462582  4780 caffe.cpp:313] Batch 403, accuracy/top5 = 0.1
I0816 10:41:58.462587  4780 caffe.cpp:313] Batch 403, loss = 6.40949
I0816 10:41:58.525321  4780 caffe.cpp:313] Batch 404, accuracy/top1 = 0
I0816 10:41:58.525343  4780 caffe.cpp:313] Batch 404, accuracy/top5 = 0.04
I0816 10:41:58.525347  4780 caffe.cpp:313] Batch 404, loss = 6.78652
I0816 10:41:58.588380  4780 caffe.cpp:313] Batch 405, accuracy/top1 = 0
I0816 10:41:58.588402  4780 caffe.cpp:313] Batch 405, accuracy/top5 = 0.02
I0816 10:41:58.588407  4780 caffe.cpp:313] Batch 405, loss = 6.5035
I0816 10:41:58.651289  4780 caffe.cpp:313] Batch 406, accuracy/top1 = 0
I0816 10:41:58.651310  4780 caffe.cpp:313] Batch 406, accuracy/top5 = 0.04
I0816 10:41:58.651315  4780 caffe.cpp:313] Batch 406, loss = 6.53255
I0816 10:41:58.714309  4780 caffe.cpp:313] Batch 407, accuracy/top1 = 0
I0816 10:41:58.714330  4780 caffe.cpp:313] Batch 407, accuracy/top5 = 0.02
I0816 10:41:58.714334  4780 caffe.cpp:313] Batch 407, loss = 6.51354
I0816 10:41:58.777204  4780 caffe.cpp:313] Batch 408, accuracy/top1 = 0.02
I0816 10:41:58.777235  4780 caffe.cpp:313] Batch 408, accuracy/top5 = 0.02
I0816 10:41:58.777240  4780 caffe.cpp:313] Batch 408, loss = 6.63556
I0816 10:41:58.840271  4780 caffe.cpp:313] Batch 409, accuracy/top1 = 0
I0816 10:41:58.840291  4780 caffe.cpp:313] Batch 409, accuracy/top5 = 0.02
I0816 10:41:58.840294  4780 caffe.cpp:313] Batch 409, loss = 6.67952
I0816 10:41:58.903472  4780 caffe.cpp:313] Batch 410, accuracy/top1 = 0.02
I0816 10:41:58.903492  4780 caffe.cpp:313] Batch 410, accuracy/top5 = 0.04
I0816 10:41:58.903496  4780 caffe.cpp:313] Batch 410, loss = 6.68256
I0816 10:41:58.966456  4780 caffe.cpp:313] Batch 411, accuracy/top1 = 0.02
I0816 10:41:58.966475  4780 caffe.cpp:313] Batch 411, accuracy/top5 = 0.04
I0816 10:41:58.966480  4780 caffe.cpp:313] Batch 411, loss = 6.85914
I0816 10:41:59.029561  4780 caffe.cpp:313] Batch 412, accuracy/top1 = 0
I0816 10:41:59.029580  4780 caffe.cpp:313] Batch 412, accuracy/top5 = 0
I0816 10:41:59.029584  4780 caffe.cpp:313] Batch 412, loss = 6.71135
I0816 10:41:59.092393  4780 caffe.cpp:313] Batch 413, accuracy/top1 = 0
I0816 10:41:59.092416  4780 caffe.cpp:313] Batch 413, accuracy/top5 = 0.04
I0816 10:41:59.092420  4780 caffe.cpp:313] Batch 413, loss = 6.8406
I0816 10:41:59.155349  4780 caffe.cpp:313] Batch 414, accuracy/top1 = 0.04
I0816 10:41:59.155372  4780 caffe.cpp:313] Batch 414, accuracy/top5 = 0.04
I0816 10:41:59.155376  4780 caffe.cpp:313] Batch 414, loss = 6.71673
I0816 10:41:59.218508  4780 caffe.cpp:313] Batch 415, accuracy/top1 = 0.02
I0816 10:41:59.218526  4780 caffe.cpp:313] Batch 415, accuracy/top5 = 0.04
I0816 10:41:59.218530  4780 caffe.cpp:313] Batch 415, loss = 6.54219
I0816 10:41:59.281426  4780 caffe.cpp:313] Batch 416, accuracy/top1 = 0
I0816 10:41:59.281448  4780 caffe.cpp:313] Batch 416, accuracy/top5 = 0.06
I0816 10:41:59.281453  4780 caffe.cpp:313] Batch 416, loss = 6.63389
I0816 10:41:59.344290  4780 caffe.cpp:313] Batch 417, accuracy/top1 = 0
I0816 10:41:59.344310  4780 caffe.cpp:313] Batch 417, accuracy/top5 = 0.06
I0816 10:41:59.344314  4780 caffe.cpp:313] Batch 417, loss = 6.54414
I0816 10:41:59.407156  4780 caffe.cpp:313] Batch 418, accuracy/top1 = 0.02
I0816 10:41:59.407178  4780 caffe.cpp:313] Batch 418, accuracy/top5 = 0.06
I0816 10:41:59.407182  4780 caffe.cpp:313] Batch 418, loss = 6.48082
I0816 10:41:59.470039  4780 caffe.cpp:313] Batch 419, accuracy/top1 = 0
I0816 10:41:59.470062  4780 caffe.cpp:313] Batch 419, accuracy/top5 = 0.04
I0816 10:41:59.470067  4780 caffe.cpp:313] Batch 419, loss = 6.89282
I0816 10:41:59.533025  4780 caffe.cpp:313] Batch 420, accuracy/top1 = 0
I0816 10:41:59.533046  4780 caffe.cpp:313] Batch 420, accuracy/top5 = 0.04
I0816 10:41:59.533051  4780 caffe.cpp:313] Batch 420, loss = 6.70602
I0816 10:41:59.595995  4780 caffe.cpp:313] Batch 421, accuracy/top1 = 0.04
I0816 10:41:59.596016  4780 caffe.cpp:313] Batch 421, accuracy/top5 = 0.06
I0816 10:41:59.596020  4780 caffe.cpp:313] Batch 421, loss = 6.7025
I0816 10:41:59.658948  4780 caffe.cpp:313] Batch 422, accuracy/top1 = 0.02
I0816 10:41:59.658970  4780 caffe.cpp:313] Batch 422, accuracy/top5 = 0.08
I0816 10:41:59.658973  4780 caffe.cpp:313] Batch 422, loss = 6.60275
I0816 10:41:59.722056  4780 caffe.cpp:313] Batch 423, accuracy/top1 = 0
I0816 10:41:59.722077  4780 caffe.cpp:313] Batch 423, accuracy/top5 = 0.04
I0816 10:41:59.722082  4780 caffe.cpp:313] Batch 423, loss = 6.66298
I0816 10:41:59.785048  4780 caffe.cpp:313] Batch 424, accuracy/top1 = 0.02
I0816 10:41:59.785070  4780 caffe.cpp:313] Batch 424, accuracy/top5 = 0.08
I0816 10:41:59.785074  4780 caffe.cpp:313] Batch 424, loss = 6.59472
I0816 10:41:59.847975  4780 caffe.cpp:313] Batch 425, accuracy/top1 = 0.02
I0816 10:41:59.847995  4780 caffe.cpp:313] Batch 425, accuracy/top5 = 0.04
I0816 10:41:59.848000  4780 caffe.cpp:313] Batch 425, loss = 6.62453
I0816 10:41:59.910919  4780 caffe.cpp:313] Batch 426, accuracy/top1 = 0.02
I0816 10:41:59.910941  4780 caffe.cpp:313] Batch 426, accuracy/top5 = 0.04
I0816 10:41:59.910945  4780 caffe.cpp:313] Batch 426, loss = 6.75072
I0816 10:41:59.974043  4780 caffe.cpp:313] Batch 427, accuracy/top1 = 0
I0816 10:41:59.974086  4780 caffe.cpp:313] Batch 427, accuracy/top5 = 0.02
I0816 10:41:59.974090  4780 caffe.cpp:313] Batch 427, loss = 6.71629
I0816 10:42:00.037134  4780 caffe.cpp:313] Batch 428, accuracy/top1 = 0.02
I0816 10:42:00.037153  4780 caffe.cpp:313] Batch 428, accuracy/top5 = 0.1
I0816 10:42:00.037158  4780 caffe.cpp:313] Batch 428, loss = 6.51779
I0816 10:42:00.100036  4780 caffe.cpp:313] Batch 429, accuracy/top1 = 0.02
I0816 10:42:00.100054  4780 caffe.cpp:313] Batch 429, accuracy/top5 = 0.04
I0816 10:42:00.100059  4780 caffe.cpp:313] Batch 429, loss = 6.77312
I0816 10:42:00.163244  4780 caffe.cpp:313] Batch 430, accuracy/top1 = 0
I0816 10:42:00.163264  4780 caffe.cpp:313] Batch 430, accuracy/top5 = 0.04
I0816 10:42:00.163269  4780 caffe.cpp:313] Batch 430, loss = 6.71312
I0816 10:42:00.226150  4780 caffe.cpp:313] Batch 431, accuracy/top1 = 0.02
I0816 10:42:00.226172  4780 caffe.cpp:313] Batch 431, accuracy/top5 = 0.08
I0816 10:42:00.226176  4780 caffe.cpp:313] Batch 431, loss = 6.75977
I0816 10:42:00.289125  4780 caffe.cpp:313] Batch 432, accuracy/top1 = 0.02
I0816 10:42:00.289147  4780 caffe.cpp:313] Batch 432, accuracy/top5 = 0.04
I0816 10:42:00.289151  4780 caffe.cpp:313] Batch 432, loss = 6.86505
I0816 10:42:00.352095  4780 caffe.cpp:313] Batch 433, accuracy/top1 = 0
I0816 10:42:00.352116  4780 caffe.cpp:313] Batch 433, accuracy/top5 = 0.02
I0816 10:42:00.352121  4780 caffe.cpp:313] Batch 433, loss = 6.68479
I0816 10:42:00.414938  4780 caffe.cpp:313] Batch 434, accuracy/top1 = 0
I0816 10:42:00.414960  4780 caffe.cpp:313] Batch 434, accuracy/top5 = 0
I0816 10:42:00.414964  4780 caffe.cpp:313] Batch 434, loss = 7.07212
I0816 10:42:00.477766  4780 caffe.cpp:313] Batch 435, accuracy/top1 = 0.06
I0816 10:42:00.477787  4780 caffe.cpp:313] Batch 435, accuracy/top5 = 0.12
I0816 10:42:00.477792  4780 caffe.cpp:313] Batch 435, loss = 6.5107
I0816 10:42:00.540789  4780 caffe.cpp:313] Batch 436, accuracy/top1 = 0
I0816 10:42:00.540812  4780 caffe.cpp:313] Batch 436, accuracy/top5 = 0.04
I0816 10:42:00.540817  4780 caffe.cpp:313] Batch 436, loss = 6.65327
I0816 10:42:00.603845  4780 caffe.cpp:313] Batch 437, accuracy/top1 = 0
I0816 10:42:00.603868  4780 caffe.cpp:313] Batch 437, accuracy/top5 = 0.02
I0816 10:42:00.603873  4780 caffe.cpp:313] Batch 437, loss = 6.63071
I0816 10:42:00.666798  4780 caffe.cpp:313] Batch 438, accuracy/top1 = 0
I0816 10:42:00.666820  4780 caffe.cpp:313] Batch 438, accuracy/top5 = 0.02
I0816 10:42:00.666824  4780 caffe.cpp:313] Batch 438, loss = 6.62349
I0816 10:42:00.729635  4780 caffe.cpp:313] Batch 439, accuracy/top1 = 0.02
I0816 10:42:00.729657  4780 caffe.cpp:313] Batch 439, accuracy/top5 = 0.08
I0816 10:42:00.729661  4780 caffe.cpp:313] Batch 439, loss = 6.6632
I0816 10:42:00.792644  4780 caffe.cpp:313] Batch 440, accuracy/top1 = 0.04
I0816 10:42:00.792665  4780 caffe.cpp:313] Batch 440, accuracy/top5 = 0.06
I0816 10:42:00.792670  4780 caffe.cpp:313] Batch 440, loss = 6.31894
I0816 10:42:00.855533  4780 caffe.cpp:313] Batch 441, accuracy/top1 = 0.02
I0816 10:42:00.855556  4780 caffe.cpp:313] Batch 441, accuracy/top5 = 0.04
I0816 10:42:00.855559  4780 caffe.cpp:313] Batch 441, loss = 6.82436
I0816 10:42:00.918372  4780 caffe.cpp:313] Batch 442, accuracy/top1 = 0.04
I0816 10:42:00.918395  4780 caffe.cpp:313] Batch 442, accuracy/top5 = 0.08
I0816 10:42:00.918398  4780 caffe.cpp:313] Batch 442, loss = 6.82695
I0816 10:42:00.981529  4780 caffe.cpp:313] Batch 443, accuracy/top1 = 0
I0816 10:42:00.981546  4780 caffe.cpp:313] Batch 443, accuracy/top5 = 0.04
I0816 10:42:00.981550  4780 caffe.cpp:313] Batch 443, loss = 6.73529
I0816 10:42:01.044528  4780 caffe.cpp:313] Batch 444, accuracy/top1 = 0.02
I0816 10:42:01.044546  4780 caffe.cpp:313] Batch 444, accuracy/top5 = 0.06
I0816 10:42:01.044551  4780 caffe.cpp:313] Batch 444, loss = 6.68043
I0816 10:42:01.107491  4780 caffe.cpp:313] Batch 445, accuracy/top1 = 0.02
I0816 10:42:01.107512  4780 caffe.cpp:313] Batch 445, accuracy/top5 = 0.04
I0816 10:42:01.107517  4780 caffe.cpp:313] Batch 445, loss = 6.80436
I0816 10:42:01.170389  4780 caffe.cpp:313] Batch 446, accuracy/top1 = 0
I0816 10:42:01.170410  4780 caffe.cpp:313] Batch 446, accuracy/top5 = 0.04
I0816 10:42:01.170415  4780 caffe.cpp:313] Batch 446, loss = 6.82122
I0816 10:42:01.233278  4780 caffe.cpp:313] Batch 447, accuracy/top1 = 0.02
I0816 10:42:01.233294  4780 caffe.cpp:313] Batch 447, accuracy/top5 = 0.02
I0816 10:42:01.233299  4780 caffe.cpp:313] Batch 447, loss = 6.83673
I0816 10:42:01.296066  4780 caffe.cpp:313] Batch 448, accuracy/top1 = 0
I0816 10:42:01.296084  4780 caffe.cpp:313] Batch 448, accuracy/top5 = 0.04
I0816 10:42:01.296089  4780 caffe.cpp:313] Batch 448, loss = 6.90309
I0816 10:42:01.359035  4780 caffe.cpp:313] Batch 449, accuracy/top1 = 0
I0816 10:42:01.359056  4780 caffe.cpp:313] Batch 449, accuracy/top5 = 0.02
I0816 10:42:01.359061  4780 caffe.cpp:313] Batch 449, loss = 6.78877
I0816 10:42:01.421958  4780 caffe.cpp:313] Batch 450, accuracy/top1 = 0
I0816 10:42:01.421979  4780 caffe.cpp:313] Batch 450, accuracy/top5 = 0.06
I0816 10:42:01.421983  4780 caffe.cpp:313] Batch 450, loss = 6.77065
I0816 10:42:01.484899  4780 caffe.cpp:313] Batch 451, accuracy/top1 = 0.04
I0816 10:42:01.484921  4780 caffe.cpp:313] Batch 451, accuracy/top5 = 0.04
I0816 10:42:01.484925  4780 caffe.cpp:313] Batch 451, loss = 6.58338
I0816 10:42:01.547730  4780 caffe.cpp:313] Batch 452, accuracy/top1 = 0.02
I0816 10:42:01.547752  4780 caffe.cpp:313] Batch 452, accuracy/top5 = 0.02
I0816 10:42:01.547756  4780 caffe.cpp:313] Batch 452, loss = 6.87967
I0816 10:42:01.610536  4780 caffe.cpp:313] Batch 453, accuracy/top1 = 0.02
I0816 10:42:01.610558  4780 caffe.cpp:313] Batch 453, accuracy/top5 = 0.02
I0816 10:42:01.610563  4780 caffe.cpp:313] Batch 453, loss = 6.68807
I0816 10:42:01.673440  4780 caffe.cpp:313] Batch 454, accuracy/top1 = 0.02
I0816 10:42:01.673462  4780 caffe.cpp:313] Batch 454, accuracy/top5 = 0.02
I0816 10:42:01.673466  4780 caffe.cpp:313] Batch 454, loss = 6.95332
I0816 10:42:01.736521  4780 caffe.cpp:313] Batch 455, accuracy/top1 = 0.04
I0816 10:42:01.736542  4780 caffe.cpp:313] Batch 455, accuracy/top5 = 0.14
I0816 10:42:01.736547  4780 caffe.cpp:313] Batch 455, loss = 6.46457
I0816 10:42:01.799391  4780 caffe.cpp:313] Batch 456, accuracy/top1 = 0.04
I0816 10:42:01.799413  4780 caffe.cpp:313] Batch 456, accuracy/top5 = 0.06
I0816 10:42:01.799417  4780 caffe.cpp:313] Batch 456, loss = 6.43602
I0816 10:42:01.862376  4780 caffe.cpp:313] Batch 457, accuracy/top1 = 0
I0816 10:42:01.862398  4780 caffe.cpp:313] Batch 457, accuracy/top5 = 0.02
I0816 10:42:01.862402  4780 caffe.cpp:313] Batch 457, loss = 6.78731
I0816 10:42:01.925354  4780 caffe.cpp:313] Batch 458, accuracy/top1 = 0
I0816 10:42:01.925374  4780 caffe.cpp:313] Batch 458, accuracy/top5 = 0
I0816 10:42:01.925379  4780 caffe.cpp:313] Batch 458, loss = 7.09862
I0816 10:42:01.988631  4780 caffe.cpp:313] Batch 459, accuracy/top1 = 0
I0816 10:42:01.988648  4780 caffe.cpp:313] Batch 459, accuracy/top5 = 0.06
I0816 10:42:01.988653  4780 caffe.cpp:313] Batch 459, loss = 6.75139
I0816 10:42:02.051702  4780 caffe.cpp:313] Batch 460, accuracy/top1 = 0
I0816 10:42:02.051722  4780 caffe.cpp:313] Batch 460, accuracy/top5 = 0.04
I0816 10:42:02.051725  4780 caffe.cpp:313] Batch 460, loss = 6.54006
I0816 10:42:02.114745  4780 caffe.cpp:313] Batch 461, accuracy/top1 = 0
I0816 10:42:02.114766  4780 caffe.cpp:313] Batch 461, accuracy/top5 = 0
I0816 10:42:02.114770  4780 caffe.cpp:313] Batch 461, loss = 6.89768
I0816 10:42:02.177616  4780 caffe.cpp:313] Batch 462, accuracy/top1 = 0
I0816 10:42:02.177639  4780 caffe.cpp:313] Batch 462, accuracy/top5 = 0.04
I0816 10:42:02.177642  4780 caffe.cpp:313] Batch 462, loss = 6.64687
I0816 10:42:02.240494  4780 caffe.cpp:313] Batch 463, accuracy/top1 = 0.02
I0816 10:42:02.240516  4780 caffe.cpp:313] Batch 463, accuracy/top5 = 0.02
I0816 10:42:02.240520  4780 caffe.cpp:313] Batch 463, loss = 6.95229
I0816 10:42:02.303421  4780 caffe.cpp:313] Batch 464, accuracy/top1 = 0.04
I0816 10:42:02.303443  4780 caffe.cpp:313] Batch 464, accuracy/top5 = 0.04
I0816 10:42:02.303447  4780 caffe.cpp:313] Batch 464, loss = 6.87861
I0816 10:42:02.366298  4780 caffe.cpp:313] Batch 465, accuracy/top1 = 0.02
I0816 10:42:02.366384  4780 caffe.cpp:313] Batch 465, accuracy/top5 = 0.06
I0816 10:42:02.366389  4780 caffe.cpp:313] Batch 465, loss = 6.53616
I0816 10:42:02.429162  4780 caffe.cpp:313] Batch 466, accuracy/top1 = 0.02
I0816 10:42:02.429184  4780 caffe.cpp:313] Batch 466, accuracy/top5 = 0.04
I0816 10:42:02.429188  4780 caffe.cpp:313] Batch 466, loss = 6.90594
I0816 10:42:02.492094  4780 caffe.cpp:313] Batch 467, accuracy/top1 = 0
I0816 10:42:02.492117  4780 caffe.cpp:313] Batch 467, accuracy/top5 = 0.08
I0816 10:42:02.492121  4780 caffe.cpp:313] Batch 467, loss = 6.58684
I0816 10:42:02.554991  4780 caffe.cpp:313] Batch 468, accuracy/top1 = 0
I0816 10:42:02.555011  4780 caffe.cpp:313] Batch 468, accuracy/top5 = 0.02
I0816 10:42:02.555016  4780 caffe.cpp:313] Batch 468, loss = 6.85238
I0816 10:42:02.617954  4780 caffe.cpp:313] Batch 469, accuracy/top1 = 0
I0816 10:42:02.617974  4780 caffe.cpp:313] Batch 469, accuracy/top5 = 0.02
I0816 10:42:02.617980  4780 caffe.cpp:313] Batch 469, loss = 6.81018
I0816 10:42:02.680876  4780 caffe.cpp:313] Batch 470, accuracy/top1 = 0
I0816 10:42:02.680896  4780 caffe.cpp:313] Batch 470, accuracy/top5 = 0.02
I0816 10:42:02.680902  4780 caffe.cpp:313] Batch 470, loss = 6.92594
I0816 10:42:02.743746  4780 caffe.cpp:313] Batch 471, accuracy/top1 = 0.04
I0816 10:42:02.743768  4780 caffe.cpp:313] Batch 471, accuracy/top5 = 0.06
I0816 10:42:02.743772  4780 caffe.cpp:313] Batch 471, loss = 6.6202
I0816 10:42:02.806725  4780 caffe.cpp:313] Batch 472, accuracy/top1 = 0
I0816 10:42:02.806747  4780 caffe.cpp:313] Batch 472, accuracy/top5 = 0
I0816 10:42:02.806751  4780 caffe.cpp:313] Batch 472, loss = 6.77429
I0816 10:42:02.869730  4780 caffe.cpp:313] Batch 473, accuracy/top1 = 0
I0816 10:42:02.869752  4780 caffe.cpp:313] Batch 473, accuracy/top5 = 0.1
I0816 10:42:02.869757  4780 caffe.cpp:313] Batch 473, loss = 6.45051
I0816 10:42:02.932701  4780 caffe.cpp:313] Batch 474, accuracy/top1 = 0.02
I0816 10:42:02.932723  4780 caffe.cpp:313] Batch 474, accuracy/top5 = 0.08
I0816 10:42:02.932726  4780 caffe.cpp:313] Batch 474, loss = 6.38153
I0816 10:42:02.995903  4780 caffe.cpp:313] Batch 475, accuracy/top1 = 0
I0816 10:42:02.995920  4780 caffe.cpp:313] Batch 475, accuracy/top5 = 0.04
I0816 10:42:02.995925  4780 caffe.cpp:313] Batch 475, loss = 6.59164
I0816 10:42:03.058770  4780 caffe.cpp:313] Batch 476, accuracy/top1 = 0
I0816 10:42:03.058789  4780 caffe.cpp:313] Batch 476, accuracy/top5 = 0.02
I0816 10:42:03.058794  4780 caffe.cpp:313] Batch 476, loss = 6.5579
I0816 10:42:03.121862  4780 caffe.cpp:313] Batch 477, accuracy/top1 = 0
I0816 10:42:03.121881  4780 caffe.cpp:313] Batch 477, accuracy/top5 = 0.04
I0816 10:42:03.121886  4780 caffe.cpp:313] Batch 477, loss = 6.67203
I0816 10:42:03.184949  4780 caffe.cpp:313] Batch 478, accuracy/top1 = 0.02
I0816 10:42:03.184968  4780 caffe.cpp:313] Batch 478, accuracy/top5 = 0.06
I0816 10:42:03.184973  4780 caffe.cpp:313] Batch 478, loss = 6.8931
I0816 10:42:03.248162  4780 caffe.cpp:313] Batch 479, accuracy/top1 = 0.04
I0816 10:42:03.248179  4780 caffe.cpp:313] Batch 479, accuracy/top5 = 0.08
I0816 10:42:03.248183  4780 caffe.cpp:313] Batch 479, loss = 6.66656
I0816 10:42:03.311020  4780 caffe.cpp:313] Batch 480, accuracy/top1 = 0
I0816 10:42:03.311043  4780 caffe.cpp:313] Batch 480, accuracy/top5 = 0
I0816 10:42:03.311046  4780 caffe.cpp:313] Batch 480, loss = 6.87658
I0816 10:42:03.373913  4780 caffe.cpp:313] Batch 481, accuracy/top1 = 0
I0816 10:42:03.373934  4780 caffe.cpp:313] Batch 481, accuracy/top5 = 0
I0816 10:42:03.373937  4780 caffe.cpp:313] Batch 481, loss = 6.76478
I0816 10:42:03.436892  4780 caffe.cpp:313] Batch 482, accuracy/top1 = 0
I0816 10:42:03.436914  4780 caffe.cpp:313] Batch 482, accuracy/top5 = 0.04
I0816 10:42:03.436918  4780 caffe.cpp:313] Batch 482, loss = 6.94601
I0816 10:42:03.499796  4780 caffe.cpp:313] Batch 483, accuracy/top1 = 0
I0816 10:42:03.499817  4780 caffe.cpp:313] Batch 483, accuracy/top5 = 0.02
I0816 10:42:03.499821  4780 caffe.cpp:313] Batch 483, loss = 7.07482
I0816 10:42:03.562628  4780 caffe.cpp:313] Batch 484, accuracy/top1 = 0
I0816 10:42:03.562660  4780 caffe.cpp:313] Batch 484, accuracy/top5 = 0.04
I0816 10:42:03.562665  4780 caffe.cpp:313] Batch 484, loss = 6.88372
I0816 10:42:03.625612  4780 caffe.cpp:313] Batch 485, accuracy/top1 = 0
I0816 10:42:03.625632  4780 caffe.cpp:313] Batch 485, accuracy/top5 = 0
I0816 10:42:03.625635  4780 caffe.cpp:313] Batch 485, loss = 6.91614
I0816 10:42:03.688575  4780 caffe.cpp:313] Batch 486, accuracy/top1 = 0.02
I0816 10:42:03.688596  4780 caffe.cpp:313] Batch 486, accuracy/top5 = 0.04
I0816 10:42:03.688601  4780 caffe.cpp:313] Batch 486, loss = 6.74465
I0816 10:42:03.751473  4780 caffe.cpp:313] Batch 487, accuracy/top1 = 0.02
I0816 10:42:03.751494  4780 caffe.cpp:313] Batch 487, accuracy/top5 = 0.06
I0816 10:42:03.751498  4780 caffe.cpp:313] Batch 487, loss = 6.63581
I0816 10:42:03.814441  4780 caffe.cpp:313] Batch 488, accuracy/top1 = 0
I0816 10:42:03.814462  4780 caffe.cpp:313] Batch 488, accuracy/top5 = 0.04
I0816 10:42:03.814466  4780 caffe.cpp:313] Batch 488, loss = 6.80598
I0816 10:42:03.877465  4780 caffe.cpp:313] Batch 489, accuracy/top1 = 0
I0816 10:42:03.877486  4780 caffe.cpp:313] Batch 489, accuracy/top5 = 0.06
I0816 10:42:03.877490  4780 caffe.cpp:313] Batch 489, loss = 6.70324
I0816 10:42:03.940474  4780 caffe.cpp:313] Batch 490, accuracy/top1 = 0
I0816 10:42:03.940493  4780 caffe.cpp:313] Batch 490, accuracy/top5 = 0.04
I0816 10:42:03.940498  4780 caffe.cpp:313] Batch 490, loss = 6.76914
I0816 10:42:04.003684  4780 caffe.cpp:313] Batch 491, accuracy/top1 = 0
I0816 10:42:04.003700  4780 caffe.cpp:313] Batch 491, accuracy/top5 = 0.04
I0816 10:42:04.003703  4780 caffe.cpp:313] Batch 491, loss = 6.81462
I0816 10:42:04.066777  4780 caffe.cpp:313] Batch 492, accuracy/top1 = 0
I0816 10:42:04.066797  4780 caffe.cpp:313] Batch 492, accuracy/top5 = 0.04
I0816 10:42:04.066802  4780 caffe.cpp:313] Batch 492, loss = 6.60371
I0816 10:42:04.129685  4780 caffe.cpp:313] Batch 493, accuracy/top1 = 0
I0816 10:42:04.129709  4780 caffe.cpp:313] Batch 493, accuracy/top5 = 0
I0816 10:42:04.129711  4780 caffe.cpp:313] Batch 493, loss = 6.87969
I0816 10:42:04.192466  4780 caffe.cpp:313] Batch 494, accuracy/top1 = 0
I0816 10:42:04.192487  4780 caffe.cpp:313] Batch 494, accuracy/top5 = 0.04
I0816 10:42:04.192492  4780 caffe.cpp:313] Batch 494, loss = 6.66916
I0816 10:42:04.255499  4780 caffe.cpp:313] Batch 495, accuracy/top1 = 0
I0816 10:42:04.255522  4780 caffe.cpp:313] Batch 495, accuracy/top5 = 0
I0816 10:42:04.255525  4780 caffe.cpp:313] Batch 495, loss = 6.90167
I0816 10:42:04.318440  4780 caffe.cpp:313] Batch 496, accuracy/top1 = 0
I0816 10:42:04.318462  4780 caffe.cpp:313] Batch 496, accuracy/top5 = 0.06
I0816 10:42:04.318466  4780 caffe.cpp:313] Batch 496, loss = 6.85959
I0816 10:42:04.381309  4780 caffe.cpp:313] Batch 497, accuracy/top1 = 0
I0816 10:42:04.381331  4780 caffe.cpp:313] Batch 497, accuracy/top5 = 0.06
I0816 10:42:04.381336  4780 caffe.cpp:313] Batch 497, loss = 6.67201
I0816 10:42:04.444049  4780 caffe.cpp:313] Batch 498, accuracy/top1 = 0
I0816 10:42:04.444072  4780 caffe.cpp:313] Batch 498, accuracy/top5 = 0.02
I0816 10:42:04.444075  4780 caffe.cpp:313] Batch 498, loss = 6.82252
I0816 10:42:04.507076  4780 caffe.cpp:313] Batch 499, accuracy/top1 = 0.02
I0816 10:42:04.507098  4780 caffe.cpp:313] Batch 499, accuracy/top5 = 0.02
I0816 10:42:04.507102  4780 caffe.cpp:313] Batch 499, loss = 7.01955
I0816 10:42:04.569962  4780 caffe.cpp:313] Batch 500, accuracy/top1 = 0.02
I0816 10:42:04.569983  4780 caffe.cpp:313] Batch 500, accuracy/top5 = 0.04
I0816 10:42:04.569988  4780 caffe.cpp:313] Batch 500, loss = 6.59338
I0816 10:42:04.632892  4780 caffe.cpp:313] Batch 501, accuracy/top1 = 0.02
I0816 10:42:04.632915  4780 caffe.cpp:313] Batch 501, accuracy/top5 = 0.04
I0816 10:42:04.632918  4780 caffe.cpp:313] Batch 501, loss = 6.73946
I0816 10:42:04.695771  4780 caffe.cpp:313] Batch 502, accuracy/top1 = 0
I0816 10:42:04.695792  4780 caffe.cpp:313] Batch 502, accuracy/top5 = 0.02
I0816 10:42:04.695797  4780 caffe.cpp:313] Batch 502, loss = 7.05547
I0816 10:42:04.758831  4780 caffe.cpp:313] Batch 503, accuracy/top1 = 0
I0816 10:42:04.758864  4780 caffe.cpp:313] Batch 503, accuracy/top5 = 0.02
I0816 10:42:04.758869  4780 caffe.cpp:313] Batch 503, loss = 6.58787
I0816 10:42:04.821779  4780 caffe.cpp:313] Batch 504, accuracy/top1 = 0
I0816 10:42:04.821799  4780 caffe.cpp:313] Batch 504, accuracy/top5 = 0.04
I0816 10:42:04.821804  4780 caffe.cpp:313] Batch 504, loss = 6.69828
I0816 10:42:04.884809  4780 caffe.cpp:313] Batch 505, accuracy/top1 = 0
I0816 10:42:04.884831  4780 caffe.cpp:313] Batch 505, accuracy/top5 = 0.02
I0816 10:42:04.884835  4780 caffe.cpp:313] Batch 505, loss = 6.64796
I0816 10:42:04.947710  4780 caffe.cpp:313] Batch 506, accuracy/top1 = 0.02
I0816 10:42:04.947732  4780 caffe.cpp:313] Batch 506, accuracy/top5 = 0.04
I0816 10:42:04.947736  4780 caffe.cpp:313] Batch 506, loss = 6.86503
I0816 10:42:05.010840  4780 caffe.cpp:313] Batch 507, accuracy/top1 = 0.02
I0816 10:42:05.010856  4780 caffe.cpp:313] Batch 507, accuracy/top5 = 0.06
I0816 10:42:05.010860  4780 caffe.cpp:313] Batch 507, loss = 6.88284
I0816 10:42:05.073803  4780 caffe.cpp:313] Batch 508, accuracy/top1 = 0
I0816 10:42:05.073825  4780 caffe.cpp:313] Batch 508, accuracy/top5 = 0.02
I0816 10:42:05.073830  4780 caffe.cpp:313] Batch 508, loss = 6.67121
I0816 10:42:05.136715  4780 caffe.cpp:313] Batch 509, accuracy/top1 = 0
I0816 10:42:05.136737  4780 caffe.cpp:313] Batch 509, accuracy/top5 = 0.02
I0816 10:42:05.136741  4780 caffe.cpp:313] Batch 509, loss = 6.89403
I0816 10:42:05.199654  4780 caffe.cpp:313] Batch 510, accuracy/top1 = 0
I0816 10:42:05.199676  4780 caffe.cpp:313] Batch 510, accuracy/top5 = 0.06
I0816 10:42:05.199681  4780 caffe.cpp:313] Batch 510, loss = 6.97373
I0816 10:42:05.262773  4780 caffe.cpp:313] Batch 511, accuracy/top1 = 0
I0816 10:42:05.262794  4780 caffe.cpp:313] Batch 511, accuracy/top5 = 0
I0816 10:42:05.262797  4780 caffe.cpp:313] Batch 511, loss = 6.68633
I0816 10:42:05.325917  4780 caffe.cpp:313] Batch 512, accuracy/top1 = 0
I0816 10:42:05.325935  4780 caffe.cpp:313] Batch 512, accuracy/top5 = 0.04
I0816 10:42:05.325940  4780 caffe.cpp:313] Batch 512, loss = 6.73653
I0816 10:42:05.388993  4780 caffe.cpp:313] Batch 513, accuracy/top1 = 0
I0816 10:42:05.389015  4780 caffe.cpp:313] Batch 513, accuracy/top5 = 0.04
I0816 10:42:05.389019  4780 caffe.cpp:313] Batch 513, loss = 6.9341
I0816 10:42:05.451959  4780 caffe.cpp:313] Batch 514, accuracy/top1 = 0.02
I0816 10:42:05.451982  4780 caffe.cpp:313] Batch 514, accuracy/top5 = 0.04
I0816 10:42:05.451985  4780 caffe.cpp:313] Batch 514, loss = 6.49772
I0816 10:42:05.514825  4780 caffe.cpp:313] Batch 515, accuracy/top1 = 0.02
I0816 10:42:05.514847  4780 caffe.cpp:313] Batch 515, accuracy/top5 = 0.06
I0816 10:42:05.514850  4780 caffe.cpp:313] Batch 515, loss = 6.72594
I0816 10:42:05.577554  4780 caffe.cpp:313] Batch 516, accuracy/top1 = 0
I0816 10:42:05.577575  4780 caffe.cpp:313] Batch 516, accuracy/top5 = 0
I0816 10:42:05.577579  4780 caffe.cpp:313] Batch 516, loss = 6.82591
I0816 10:42:05.640549  4780 caffe.cpp:313] Batch 517, accuracy/top1 = 0.02
I0816 10:42:05.640570  4780 caffe.cpp:313] Batch 517, accuracy/top5 = 0.02
I0816 10:42:05.640574  4780 caffe.cpp:313] Batch 517, loss = 6.74259
I0816 10:42:05.703400  4780 caffe.cpp:313] Batch 518, accuracy/top1 = 0
I0816 10:42:05.703423  4780 caffe.cpp:313] Batch 518, accuracy/top5 = 0.02
I0816 10:42:05.703426  4780 caffe.cpp:313] Batch 518, loss = 7.05686
I0816 10:42:05.766376  4780 caffe.cpp:313] Batch 519, accuracy/top1 = 0.02
I0816 10:42:05.766398  4780 caffe.cpp:313] Batch 519, accuracy/top5 = 0.04
I0816 10:42:05.766402  4780 caffe.cpp:313] Batch 519, loss = 7.00414
I0816 10:42:05.829262  4780 caffe.cpp:313] Batch 520, accuracy/top1 = 0.02
I0816 10:42:05.829285  4780 caffe.cpp:313] Batch 520, accuracy/top5 = 0.02
I0816 10:42:05.829289  4780 caffe.cpp:313] Batch 520, loss = 6.83355
I0816 10:42:05.892087  4780 caffe.cpp:313] Batch 521, accuracy/top1 = 0.02
I0816 10:42:05.892107  4780 caffe.cpp:313] Batch 521, accuracy/top5 = 0.06
I0816 10:42:05.892112  4780 caffe.cpp:313] Batch 521, loss = 6.6813
I0816 10:42:05.954996  4780 caffe.cpp:313] Batch 522, accuracy/top1 = 0.04
I0816 10:42:05.955029  4780 caffe.cpp:313] Batch 522, accuracy/top5 = 0.08
I0816 10:42:05.955034  4780 caffe.cpp:313] Batch 522, loss = 6.48399
I0816 10:42:06.018283  4780 caffe.cpp:313] Batch 523, accuracy/top1 = 0
I0816 10:42:06.018301  4780 caffe.cpp:313] Batch 523, accuracy/top5 = 0
I0816 10:42:06.018306  4780 caffe.cpp:313] Batch 523, loss = 6.98704
I0816 10:42:06.081303  4780 caffe.cpp:313] Batch 524, accuracy/top1 = 0
I0816 10:42:06.081324  4780 caffe.cpp:313] Batch 524, accuracy/top5 = 0
I0816 10:42:06.081328  4780 caffe.cpp:313] Batch 524, loss = 6.69636
I0816 10:42:06.144104  4780 caffe.cpp:313] Batch 525, accuracy/top1 = 0
I0816 10:42:06.144125  4780 caffe.cpp:313] Batch 525, accuracy/top5 = 0.02
I0816 10:42:06.144131  4780 caffe.cpp:313] Batch 525, loss = 6.72022
I0816 10:42:06.207041  4780 caffe.cpp:313] Batch 526, accuracy/top1 = 0
I0816 10:42:06.207062  4780 caffe.cpp:313] Batch 526, accuracy/top5 = 0.02
I0816 10:42:06.207067  4780 caffe.cpp:313] Batch 526, loss = 7.08768
I0816 10:42:06.269935  4780 caffe.cpp:313] Batch 527, accuracy/top1 = 0.02
I0816 10:42:06.269956  4780 caffe.cpp:313] Batch 527, accuracy/top5 = 0.02
I0816 10:42:06.269960  4780 caffe.cpp:313] Batch 527, loss = 6.88702
I0816 10:42:06.332949  4780 caffe.cpp:313] Batch 528, accuracy/top1 = 0.02
I0816 10:42:06.332972  4780 caffe.cpp:313] Batch 528, accuracy/top5 = 0.04
I0816 10:42:06.332975  4780 caffe.cpp:313] Batch 528, loss = 6.7504
I0816 10:42:06.395823  4780 caffe.cpp:313] Batch 529, accuracy/top1 = 0
I0816 10:42:06.395843  4780 caffe.cpp:313] Batch 529, accuracy/top5 = 0.02
I0816 10:42:06.395848  4780 caffe.cpp:313] Batch 529, loss = 6.78847
I0816 10:42:06.458745  4780 caffe.cpp:313] Batch 530, accuracy/top1 = 0.02
I0816 10:42:06.458765  4780 caffe.cpp:313] Batch 530, accuracy/top5 = 0.02
I0816 10:42:06.458770  4780 caffe.cpp:313] Batch 530, loss = 6.78084
I0816 10:42:06.521634  4780 caffe.cpp:313] Batch 531, accuracy/top1 = 0.02
I0816 10:42:06.521656  4780 caffe.cpp:313] Batch 531, accuracy/top5 = 0.04
I0816 10:42:06.521659  4780 caffe.cpp:313] Batch 531, loss = 6.65675
I0816 10:42:06.584553  4780 caffe.cpp:313] Batch 532, accuracy/top1 = 0
I0816 10:42:06.584575  4780 caffe.cpp:313] Batch 532, accuracy/top5 = 0.02
I0816 10:42:06.584579  4780 caffe.cpp:313] Batch 532, loss = 6.77546
I0816 10:42:06.647441  4780 caffe.cpp:313] Batch 533, accuracy/top1 = 0
I0816 10:42:06.647462  4780 caffe.cpp:313] Batch 533, accuracy/top5 = 0.06
I0816 10:42:06.647467  4780 caffe.cpp:313] Batch 533, loss = 6.72666
I0816 10:42:06.710367  4780 caffe.cpp:313] Batch 534, accuracy/top1 = 0
I0816 10:42:06.710388  4780 caffe.cpp:313] Batch 534, accuracy/top5 = 0
I0816 10:42:06.710392  4780 caffe.cpp:313] Batch 534, loss = 6.61542
I0816 10:42:06.773267  4780 caffe.cpp:313] Batch 535, accuracy/top1 = 0.02
I0816 10:42:06.773289  4780 caffe.cpp:313] Batch 535, accuracy/top5 = 0.04
I0816 10:42:06.773293  4780 caffe.cpp:313] Batch 535, loss = 6.82862
I0816 10:42:06.836310  4780 caffe.cpp:313] Batch 536, accuracy/top1 = 0
I0816 10:42:06.836331  4780 caffe.cpp:313] Batch 536, accuracy/top5 = 0.02
I0816 10:42:06.836336  4780 caffe.cpp:313] Batch 536, loss = 6.78259
I0816 10:42:06.899320  4780 caffe.cpp:313] Batch 537, accuracy/top1 = 0
I0816 10:42:06.899341  4780 caffe.cpp:313] Batch 537, accuracy/top5 = 0
I0816 10:42:06.899345  4780 caffe.cpp:313] Batch 537, loss = 6.87866
I0816 10:42:06.962188  4780 caffe.cpp:313] Batch 538, accuracy/top1 = 0
I0816 10:42:06.962208  4780 caffe.cpp:313] Batch 538, accuracy/top5 = 0
I0816 10:42:06.962213  4780 caffe.cpp:313] Batch 538, loss = 6.98301
I0816 10:42:07.025290  4780 caffe.cpp:313] Batch 539, accuracy/top1 = 0
I0816 10:42:07.025310  4780 caffe.cpp:313] Batch 539, accuracy/top5 = 0.04
I0816 10:42:07.025313  4780 caffe.cpp:313] Batch 539, loss = 6.73278
I0816 10:42:07.088325  4780 caffe.cpp:313] Batch 540, accuracy/top1 = 0
I0816 10:42:07.088346  4780 caffe.cpp:313] Batch 540, accuracy/top5 = 0.04
I0816 10:42:07.088349  4780 caffe.cpp:313] Batch 540, loss = 6.52223
I0816 10:42:07.151152  4780 caffe.cpp:313] Batch 541, accuracy/top1 = 0
I0816 10:42:07.151186  4780 caffe.cpp:313] Batch 541, accuracy/top5 = 0.04
I0816 10:42:07.151191  4780 caffe.cpp:313] Batch 541, loss = 6.65045
I0816 10:42:07.213989  4780 caffe.cpp:313] Batch 542, accuracy/top1 = 0
I0816 10:42:07.214010  4780 caffe.cpp:313] Batch 542, accuracy/top5 = 0
I0816 10:42:07.214015  4780 caffe.cpp:313] Batch 542, loss = 7.02873
I0816 10:42:07.276808  4780 caffe.cpp:313] Batch 543, accuracy/top1 = 0.02
I0816 10:42:07.276829  4780 caffe.cpp:313] Batch 543, accuracy/top5 = 0.08
I0816 10:42:07.276834  4780 caffe.cpp:313] Batch 543, loss = 6.62216
I0816 10:42:07.339876  4780 caffe.cpp:313] Batch 544, accuracy/top1 = 0.04
I0816 10:42:07.339895  4780 caffe.cpp:313] Batch 544, accuracy/top5 = 0.04
I0816 10:42:07.339898  4780 caffe.cpp:313] Batch 544, loss = 6.94375
I0816 10:42:07.403151  4780 caffe.cpp:313] Batch 545, accuracy/top1 = 0
I0816 10:42:07.403168  4780 caffe.cpp:313] Batch 545, accuracy/top5 = 0.02
I0816 10:42:07.403173  4780 caffe.cpp:313] Batch 545, loss = 6.92199
I0816 10:42:07.466394  4780 caffe.cpp:313] Batch 546, accuracy/top1 = 0.02
I0816 10:42:07.466414  4780 caffe.cpp:313] Batch 546, accuracy/top5 = 0.02
I0816 10:42:07.466418  4780 caffe.cpp:313] Batch 546, loss = 6.70733
I0816 10:42:07.529201  4780 caffe.cpp:313] Batch 547, accuracy/top1 = 0
I0816 10:42:07.529222  4780 caffe.cpp:313] Batch 547, accuracy/top5 = 0.02
I0816 10:42:07.529227  4780 caffe.cpp:313] Batch 547, loss = 6.8003
I0816 10:42:07.591979  4780 caffe.cpp:313] Batch 548, accuracy/top1 = 0.02
I0816 10:42:07.592000  4780 caffe.cpp:313] Batch 548, accuracy/top5 = 0.04
I0816 10:42:07.592005  4780 caffe.cpp:313] Batch 548, loss = 6.60415
I0816 10:42:07.654899  4780 caffe.cpp:313] Batch 549, accuracy/top1 = 0
I0816 10:42:07.654922  4780 caffe.cpp:313] Batch 549, accuracy/top5 = 0.04
I0816 10:42:07.654925  4780 caffe.cpp:313] Batch 549, loss = 6.71053
I0816 10:42:07.717829  4780 caffe.cpp:313] Batch 550, accuracy/top1 = 0.02
I0816 10:42:07.717850  4780 caffe.cpp:313] Batch 550, accuracy/top5 = 0.04
I0816 10:42:07.717854  4780 caffe.cpp:313] Batch 550, loss = 6.72329
I0816 10:42:07.780810  4780 caffe.cpp:313] Batch 551, accuracy/top1 = 0
I0816 10:42:07.780831  4780 caffe.cpp:313] Batch 551, accuracy/top5 = 0.02
I0816 10:42:07.780835  4780 caffe.cpp:313] Batch 551, loss = 7.0253
I0816 10:42:07.843581  4780 caffe.cpp:313] Batch 552, accuracy/top1 = 0
I0816 10:42:07.843602  4780 caffe.cpp:313] Batch 552, accuracy/top5 = 0.08
I0816 10:42:07.843607  4780 caffe.cpp:313] Batch 552, loss = 6.70135
I0816 10:42:07.906481  4780 caffe.cpp:313] Batch 553, accuracy/top1 = 0.02
I0816 10:42:07.906502  4780 caffe.cpp:313] Batch 553, accuracy/top5 = 0.06
I0816 10:42:07.906505  4780 caffe.cpp:313] Batch 553, loss = 6.5647
I0816 10:42:07.969491  4780 caffe.cpp:313] Batch 554, accuracy/top1 = 0.04
I0816 10:42:07.969513  4780 caffe.cpp:313] Batch 554, accuracy/top5 = 0.04
I0816 10:42:07.969517  4780 caffe.cpp:313] Batch 554, loss = 6.67162
I0816 10:42:08.032620  4780 caffe.cpp:313] Batch 555, accuracy/top1 = 0.02
I0816 10:42:08.032639  4780 caffe.cpp:313] Batch 555, accuracy/top5 = 0.02
I0816 10:42:08.032644  4780 caffe.cpp:313] Batch 555, loss = 6.83053
I0816 10:42:08.095571  4780 caffe.cpp:313] Batch 556, accuracy/top1 = 0.02
I0816 10:42:08.095593  4780 caffe.cpp:313] Batch 556, accuracy/top5 = 0.04
I0816 10:42:08.095597  4780 caffe.cpp:313] Batch 556, loss = 6.77857
I0816 10:42:08.158453  4780 caffe.cpp:313] Batch 557, accuracy/top1 = 0.02
I0816 10:42:08.158475  4780 caffe.cpp:313] Batch 557, accuracy/top5 = 0.02
I0816 10:42:08.158479  4780 caffe.cpp:313] Batch 557, loss = 6.74549
I0816 10:42:08.221504  4780 caffe.cpp:313] Batch 558, accuracy/top1 = 0
I0816 10:42:08.221525  4780 caffe.cpp:313] Batch 558, accuracy/top5 = 0.02
I0816 10:42:08.221530  4780 caffe.cpp:313] Batch 558, loss = 6.58273
I0816 10:42:08.284355  4780 caffe.cpp:313] Batch 559, accuracy/top1 = 0
I0816 10:42:08.284376  4780 caffe.cpp:313] Batch 559, accuracy/top5 = 0
I0816 10:42:08.284379  4780 caffe.cpp:313] Batch 559, loss = 6.90969
I0816 10:42:08.347407  4780 caffe.cpp:313] Batch 560, accuracy/top1 = 0.06
I0816 10:42:08.347429  4780 caffe.cpp:313] Batch 560, accuracy/top5 = 0.08
I0816 10:42:08.347434  4780 caffe.cpp:313] Batch 560, loss = 6.72129
I0816 10:42:08.410320  4780 caffe.cpp:313] Batch 561, accuracy/top1 = 0.02
I0816 10:42:08.410341  4780 caffe.cpp:313] Batch 561, accuracy/top5 = 0.04
I0816 10:42:08.410346  4780 caffe.cpp:313] Batch 561, loss = 6.38202
I0816 10:42:08.473228  4780 caffe.cpp:313] Batch 562, accuracy/top1 = 0
I0816 10:42:08.473249  4780 caffe.cpp:313] Batch 562, accuracy/top5 = 0.08
I0816 10:42:08.473254  4780 caffe.cpp:313] Batch 562, loss = 6.66956
I0816 10:42:08.536290  4780 caffe.cpp:313] Batch 563, accuracy/top1 = 0
I0816 10:42:08.536312  4780 caffe.cpp:313] Batch 563, accuracy/top5 = 0.06
I0816 10:42:08.536316  4780 caffe.cpp:313] Batch 563, loss = 6.74116
I0816 10:42:08.599124  4780 caffe.cpp:313] Batch 564, accuracy/top1 = 0
I0816 10:42:08.599146  4780 caffe.cpp:313] Batch 564, accuracy/top5 = 0.04
I0816 10:42:08.599150  4780 caffe.cpp:313] Batch 564, loss = 6.60367
I0816 10:42:08.662077  4780 caffe.cpp:313] Batch 565, accuracy/top1 = 0.04
I0816 10:42:08.662099  4780 caffe.cpp:313] Batch 565, accuracy/top5 = 0.06
I0816 10:42:08.662103  4780 caffe.cpp:313] Batch 565, loss = 6.52517
I0816 10:42:08.724938  4780 caffe.cpp:313] Batch 566, accuracy/top1 = 0.02
I0816 10:42:08.724961  4780 caffe.cpp:313] Batch 566, accuracy/top5 = 0.08
I0816 10:42:08.724964  4780 caffe.cpp:313] Batch 566, loss = 6.58019
I0816 10:42:08.787866  4780 caffe.cpp:313] Batch 567, accuracy/top1 = 0.04
I0816 10:42:08.787889  4780 caffe.cpp:313] Batch 567, accuracy/top5 = 0.08
I0816 10:42:08.787892  4780 caffe.cpp:313] Batch 567, loss = 6.6042
I0816 10:42:08.851083  4780 caffe.cpp:313] Batch 568, accuracy/top1 = 0
I0816 10:42:08.851105  4780 caffe.cpp:313] Batch 568, accuracy/top5 = 0.04
I0816 10:42:08.851109  4780 caffe.cpp:313] Batch 568, loss = 6.91499
I0816 10:42:08.914147  4780 caffe.cpp:313] Batch 569, accuracy/top1 = 0
I0816 10:42:08.914170  4780 caffe.cpp:313] Batch 569, accuracy/top5 = 0.04
I0816 10:42:08.914175  4780 caffe.cpp:313] Batch 569, loss = 6.48161
I0816 10:42:08.977345  4780 caffe.cpp:313] Batch 570, accuracy/top1 = 0
I0816 10:42:08.977368  4780 caffe.cpp:313] Batch 570, accuracy/top5 = 0
I0816 10:42:08.977371  4780 caffe.cpp:313] Batch 570, loss = 6.72176
I0816 10:42:09.040329  4780 caffe.cpp:313] Batch 571, accuracy/top1 = 0.02
I0816 10:42:09.040350  4780 caffe.cpp:313] Batch 571, accuracy/top5 = 0.04
I0816 10:42:09.040354  4780 caffe.cpp:313] Batch 571, loss = 6.92028
I0816 10:42:09.103477  4780 caffe.cpp:313] Batch 572, accuracy/top1 = 0
I0816 10:42:09.103500  4780 caffe.cpp:313] Batch 572, accuracy/top5 = 0.04
I0816 10:42:09.103505  4780 caffe.cpp:313] Batch 572, loss = 6.50181
I0816 10:42:09.166653  4780 caffe.cpp:313] Batch 573, accuracy/top1 = 0.02
I0816 10:42:09.166678  4780 caffe.cpp:313] Batch 573, accuracy/top5 = 0.04
I0816 10:42:09.166682  4780 caffe.cpp:313] Batch 573, loss = 6.67573
I0816 10:42:09.229804  4780 caffe.cpp:313] Batch 574, accuracy/top1 = 0
I0816 10:42:09.229830  4780 caffe.cpp:313] Batch 574, accuracy/top5 = 0
I0816 10:42:09.229833  4780 caffe.cpp:313] Batch 574, loss = 6.94221
I0816 10:42:09.292923  4780 caffe.cpp:313] Batch 575, accuracy/top1 = 0.02
I0816 10:42:09.292948  4780 caffe.cpp:313] Batch 575, accuracy/top5 = 0.04
I0816 10:42:09.292951  4780 caffe.cpp:313] Batch 575, loss = 6.54037
I0816 10:42:09.356158  4780 caffe.cpp:313] Batch 576, accuracy/top1 = 0
I0816 10:42:09.356178  4780 caffe.cpp:313] Batch 576, accuracy/top5 = 0.04
I0816 10:42:09.356182  4780 caffe.cpp:313] Batch 576, loss = 6.8021
I0816 10:42:09.419211  4780 caffe.cpp:313] Batch 577, accuracy/top1 = 0.02
I0816 10:42:09.419236  4780 caffe.cpp:313] Batch 577, accuracy/top5 = 0.08
I0816 10:42:09.419240  4780 caffe.cpp:313] Batch 577, loss = 6.88114
I0816 10:42:09.482318  4780 caffe.cpp:313] Batch 578, accuracy/top1 = 0.04
I0816 10:42:09.482341  4780 caffe.cpp:313] Batch 578, accuracy/top5 = 0.04
I0816 10:42:09.482344  4780 caffe.cpp:313] Batch 578, loss = 6.87852
I0816 10:42:09.545580  4780 caffe.cpp:313] Batch 579, accuracy/top1 = 0
I0816 10:42:09.545603  4780 caffe.cpp:313] Batch 579, accuracy/top5 = 0.04
I0816 10:42:09.545608  4780 caffe.cpp:313] Batch 579, loss = 6.69115
I0816 10:42:09.606717  4780 caffe.cpp:313] Batch 580, accuracy/top1 = 0
I0816 10:42:09.606740  4780 caffe.cpp:313] Batch 580, accuracy/top5 = 0.02
I0816 10:42:09.606745  4780 caffe.cpp:313] Batch 580, loss = 6.86278
I0816 10:42:09.606765  4780 blocking_queue.cpp:40] Data layer prefetch queue empty
I0816 10:42:09.674309  4780 caffe.cpp:313] Batch 581, accuracy/top1 = 0
I0816 10:42:09.674336  4780 caffe.cpp:313] Batch 581, accuracy/top5 = 0.06
I0816 10:42:09.674340  4780 caffe.cpp:313] Batch 581, loss = 6.65921
I0816 10:42:09.737527  4780 caffe.cpp:313] Batch 582, accuracy/top1 = 0
I0816 10:42:09.737553  4780 caffe.cpp:313] Batch 582, accuracy/top5 = 0
I0816 10:42:09.737557  4780 caffe.cpp:313] Batch 582, loss = 6.93653
I0816 10:42:09.800635  4780 caffe.cpp:313] Batch 583, accuracy/top1 = 0
I0816 10:42:09.800662  4780 caffe.cpp:313] Batch 583, accuracy/top5 = 0
I0816 10:42:09.800665  4780 caffe.cpp:313] Batch 583, loss = 6.50745
I0816 10:42:09.863896  4780 caffe.cpp:313] Batch 584, accuracy/top1 = 0.02
I0816 10:42:09.863922  4780 caffe.cpp:313] Batch 584, accuracy/top5 = 0.04
I0816 10:42:09.863925  4780 caffe.cpp:313] Batch 584, loss = 6.74607
I0816 10:42:09.927184  4780 caffe.cpp:313] Batch 585, accuracy/top1 = 0.02
I0816 10:42:09.927208  4780 caffe.cpp:313] Batch 585, accuracy/top5 = 0.04
I0816 10:42:09.927212  4780 caffe.cpp:313] Batch 585, loss = 6.61774
I0816 10:42:09.991056  4780 caffe.cpp:313] Batch 586, accuracy/top1 = 0
I0816 10:42:09.991081  4780 caffe.cpp:313] Batch 586, accuracy/top5 = 0.02
I0816 10:42:09.991086  4780 caffe.cpp:313] Batch 586, loss = 6.74313
I0816 10:42:10.054417  4780 caffe.cpp:313] Batch 587, accuracy/top1 = 0.02
I0816 10:42:10.054445  4780 caffe.cpp:313] Batch 587, accuracy/top5 = 0.04
I0816 10:42:10.054447  4780 caffe.cpp:313] Batch 587, loss = 6.73604
I0816 10:42:10.117794  4780 caffe.cpp:313] Batch 588, accuracy/top1 = 0
I0816 10:42:10.117820  4780 caffe.cpp:313] Batch 588, accuracy/top5 = 0
I0816 10:42:10.117823  4780 caffe.cpp:313] Batch 588, loss = 6.67317
I0816 10:42:10.181383  4780 caffe.cpp:313] Batch 589, accuracy/top1 = 0
I0816 10:42:10.181407  4780 caffe.cpp:313] Batch 589, accuracy/top5 = 0.04
I0816 10:42:10.181411  4780 caffe.cpp:313] Batch 589, loss = 6.74206
I0816 10:42:10.244874  4780 caffe.cpp:313] Batch 590, accuracy/top1 = 0
I0816 10:42:10.244899  4780 caffe.cpp:313] Batch 590, accuracy/top5 = 0.06
I0816 10:42:10.244904  4780 caffe.cpp:313] Batch 590, loss = 6.77975
I0816 10:42:10.308135  4780 caffe.cpp:313] Batch 591, accuracy/top1 = 0
I0816 10:42:10.308158  4780 caffe.cpp:313] Batch 591, accuracy/top5 = 0.02
I0816 10:42:10.308162  4780 caffe.cpp:313] Batch 591, loss = 6.68314
I0816 10:42:10.371907  4780 caffe.cpp:313] Batch 592, accuracy/top1 = 0
I0816 10:42:10.371924  4780 caffe.cpp:313] Batch 592, accuracy/top5 = 0.04
I0816 10:42:10.371927  4780 caffe.cpp:313] Batch 592, loss = 6.98298
I0816 10:42:10.435004  4780 caffe.cpp:313] Batch 593, accuracy/top1 = 0
I0816 10:42:10.435025  4780 caffe.cpp:313] Batch 593, accuracy/top5 = 0.06
I0816 10:42:10.435029  4780 caffe.cpp:313] Batch 593, loss = 6.89936
I0816 10:42:10.498003  4780 caffe.cpp:313] Batch 594, accuracy/top1 = 0
I0816 10:42:10.498023  4780 caffe.cpp:313] Batch 594, accuracy/top5 = 0.04
I0816 10:42:10.498028  4780 caffe.cpp:313] Batch 594, loss = 6.65896
I0816 10:42:10.560824  4780 caffe.cpp:313] Batch 595, accuracy/top1 = 0.02
I0816 10:42:10.560845  4780 caffe.cpp:313] Batch 595, accuracy/top5 = 0.04
I0816 10:42:10.560848  4780 caffe.cpp:313] Batch 595, loss = 6.60643
I0816 10:42:10.623735  4780 caffe.cpp:313] Batch 596, accuracy/top1 = 0
I0816 10:42:10.623756  4780 caffe.cpp:313] Batch 596, accuracy/top5 = 0
I0816 10:42:10.623759  4780 caffe.cpp:313] Batch 596, loss = 6.71518
I0816 10:42:10.686729  4780 caffe.cpp:313] Batch 597, accuracy/top1 = 0.04
I0816 10:42:10.686750  4780 caffe.cpp:313] Batch 597, accuracy/top5 = 0.08
I0816 10:42:10.686775  4780 caffe.cpp:313] Batch 597, loss = 6.65801
I0816 10:42:10.749667  4780 caffe.cpp:313] Batch 598, accuracy/top1 = 0
I0816 10:42:10.749688  4780 caffe.cpp:313] Batch 598, accuracy/top5 = 0
I0816 10:42:10.749691  4780 caffe.cpp:313] Batch 598, loss = 7.2276
I0816 10:42:10.812633  4780 caffe.cpp:313] Batch 599, accuracy/top1 = 0.02
I0816 10:42:10.812654  4780 caffe.cpp:313] Batch 599, accuracy/top5 = 0.08
I0816 10:42:10.812656  4780 caffe.cpp:313] Batch 599, loss = 6.85948
I0816 10:42:10.875574  4780 caffe.cpp:313] Batch 600, accuracy/top1 = 0
I0816 10:42:10.875596  4780 caffe.cpp:313] Batch 600, accuracy/top5 = 0.04
I0816 10:42:10.875598  4780 caffe.cpp:313] Batch 600, loss = 6.81739
I0816 10:42:10.938679  4780 caffe.cpp:313] Batch 601, accuracy/top1 = 0.02
I0816 10:42:10.938701  4780 caffe.cpp:313] Batch 601, accuracy/top5 = 0.02
I0816 10:42:10.938704  4780 caffe.cpp:313] Batch 601, loss = 6.75851
I0816 10:42:11.001484  4780 caffe.cpp:313] Batch 602, accuracy/top1 = 0
I0816 10:42:11.001502  4780 caffe.cpp:313] Batch 602, accuracy/top5 = 0.06
I0816 10:42:11.001507  4780 caffe.cpp:313] Batch 602, loss = 6.76846
I0816 10:42:11.064477  4780 caffe.cpp:313] Batch 603, accuracy/top1 = 0.02
I0816 10:42:11.064498  4780 caffe.cpp:313] Batch 603, accuracy/top5 = 0.02
I0816 10:42:11.064502  4780 caffe.cpp:313] Batch 603, loss = 6.73846
I0816 10:42:11.127275  4780 caffe.cpp:313] Batch 604, accuracy/top1 = 0.02
I0816 10:42:11.127296  4780 caffe.cpp:313] Batch 604, accuracy/top5 = 0.02
I0816 10:42:11.127300  4780 caffe.cpp:313] Batch 604, loss = 6.78964
I0816 10:42:11.190255  4780 caffe.cpp:313] Batch 605, accuracy/top1 = 0
I0816 10:42:11.190275  4780 caffe.cpp:313] Batch 605, accuracy/top5 = 0.02
I0816 10:42:11.190279  4780 caffe.cpp:313] Batch 605, loss = 7.02361
I0816 10:42:11.253348  4780 caffe.cpp:313] Batch 606, accuracy/top1 = 0
I0816 10:42:11.253371  4780 caffe.cpp:313] Batch 606, accuracy/top5 = 0.04
I0816 10:42:11.253374  4780 caffe.cpp:313] Batch 606, loss = 6.66637
I0816 10:42:11.316314  4780 caffe.cpp:313] Batch 607, accuracy/top1 = 0
I0816 10:42:11.316335  4780 caffe.cpp:313] Batch 607, accuracy/top5 = 0.02
I0816 10:42:11.316339  4780 caffe.cpp:313] Batch 607, loss = 6.71062
I0816 10:42:11.379282  4780 caffe.cpp:313] Batch 608, accuracy/top1 = 0
I0816 10:42:11.379299  4780 caffe.cpp:313] Batch 608, accuracy/top5 = 0
I0816 10:42:11.379302  4780 caffe.cpp:313] Batch 608, loss = 6.54567
I0816 10:42:11.442219  4780 caffe.cpp:313] Batch 609, accuracy/top1 = 0
I0816 10:42:11.442240  4780 caffe.cpp:313] Batch 609, accuracy/top5 = 0.04
I0816 10:42:11.442243  4780 caffe.cpp:313] Batch 609, loss = 6.99878
I0816 10:42:11.505309  4780 caffe.cpp:313] Batch 610, accuracy/top1 = 0.02
I0816 10:42:11.505329  4780 caffe.cpp:313] Batch 610, accuracy/top5 = 0.02
I0816 10:42:11.505332  4780 caffe.cpp:313] Batch 610, loss = 6.81663
I0816 10:42:11.568209  4780 caffe.cpp:313] Batch 611, accuracy/top1 = 0.02
I0816 10:42:11.568228  4780 caffe.cpp:313] Batch 611, accuracy/top5 = 0.02
I0816 10:42:11.568231  4780 caffe.cpp:313] Batch 611, loss = 6.60832
I0816 10:42:11.631325  4780 caffe.cpp:313] Batch 612, accuracy/top1 = 0.02
I0816 10:42:11.631345  4780 caffe.cpp:313] Batch 612, accuracy/top5 = 0.04
I0816 10:42:11.631348  4780 caffe.cpp:313] Batch 612, loss = 6.82273
I0816 10:42:11.694135  4780 caffe.cpp:313] Batch 613, accuracy/top1 = 0.02
I0816 10:42:11.694154  4780 caffe.cpp:313] Batch 613, accuracy/top5 = 0.08
I0816 10:42:11.694157  4780 caffe.cpp:313] Batch 613, loss = 6.49056
I0816 10:42:11.757412  4780 caffe.cpp:313] Batch 614, accuracy/top1 = 0.04
I0816 10:42:11.757432  4780 caffe.cpp:313] Batch 614, accuracy/top5 = 0.04
I0816 10:42:11.757436  4780 caffe.cpp:313] Batch 614, loss = 6.70252
I0816 10:42:11.820509  4780 caffe.cpp:313] Batch 615, accuracy/top1 = 0
I0816 10:42:11.820530  4780 caffe.cpp:313] Batch 615, accuracy/top5 = 0
I0816 10:42:11.820533  4780 caffe.cpp:313] Batch 615, loss = 6.594
I0816 10:42:11.883431  4780 caffe.cpp:313] Batch 616, accuracy/top1 = 0.02
I0816 10:42:11.883466  4780 caffe.cpp:313] Batch 616, accuracy/top5 = 0.02
I0816 10:42:11.883468  4780 caffe.cpp:313] Batch 616, loss = 6.68122
I0816 10:42:11.946439  4780 caffe.cpp:313] Batch 617, accuracy/top1 = 0
I0816 10:42:11.946460  4780 caffe.cpp:313] Batch 617, accuracy/top5 = 0.02
I0816 10:42:11.946463  4780 caffe.cpp:313] Batch 617, loss = 6.68502
I0816 10:42:12.009627  4780 caffe.cpp:313] Batch 618, accuracy/top1 = 0
I0816 10:42:12.009644  4780 caffe.cpp:313] Batch 618, accuracy/top5 = 0
I0816 10:42:12.009647  4780 caffe.cpp:313] Batch 618, loss = 6.82617
I0816 10:42:12.072662  4780 caffe.cpp:313] Batch 619, accuracy/top1 = 0.04
I0816 10:42:12.072685  4780 caffe.cpp:313] Batch 619, accuracy/top5 = 0.06
I0816 10:42:12.072687  4780 caffe.cpp:313] Batch 619, loss = 6.65185
I0816 10:42:12.135696  4780 caffe.cpp:313] Batch 620, accuracy/top1 = 0
I0816 10:42:12.135717  4780 caffe.cpp:313] Batch 620, accuracy/top5 = 0
I0816 10:42:12.135720  4780 caffe.cpp:313] Batch 620, loss = 6.77124
I0816 10:42:12.198581  4780 caffe.cpp:313] Batch 621, accuracy/top1 = 0.02
I0816 10:42:12.198602  4780 caffe.cpp:313] Batch 621, accuracy/top5 = 0.04
I0816 10:42:12.198606  4780 caffe.cpp:313] Batch 621, loss = 6.75767
I0816 10:42:12.261490  4780 caffe.cpp:313] Batch 622, accuracy/top1 = 0.04
I0816 10:42:12.261513  4780 caffe.cpp:313] Batch 622, accuracy/top5 = 0.06
I0816 10:42:12.261517  4780 caffe.cpp:313] Batch 622, loss = 6.43619
I0816 10:42:12.324426  4780 caffe.cpp:313] Batch 623, accuracy/top1 = 0.04
I0816 10:42:12.324447  4780 caffe.cpp:313] Batch 623, accuracy/top5 = 0.08
I0816 10:42:12.324450  4780 caffe.cpp:313] Batch 623, loss = 6.51526
I0816 10:42:12.387367  4780 caffe.cpp:313] Batch 624, accuracy/top1 = 0
I0816 10:42:12.387389  4780 caffe.cpp:313] Batch 624, accuracy/top5 = 0
I0816 10:42:12.387392  4780 caffe.cpp:313] Batch 624, loss = 6.79499
I0816 10:42:12.450270  4780 caffe.cpp:313] Batch 625, accuracy/top1 = 0.02
I0816 10:42:12.450291  4780 caffe.cpp:313] Batch 625, accuracy/top5 = 0.06
I0816 10:42:12.450294  4780 caffe.cpp:313] Batch 625, loss = 6.75921
I0816 10:42:12.513119  4780 caffe.cpp:313] Batch 626, accuracy/top1 = 0.04
I0816 10:42:12.513140  4780 caffe.cpp:313] Batch 626, accuracy/top5 = 0.06
I0816 10:42:12.513144  4780 caffe.cpp:313] Batch 626, loss = 6.583
I0816 10:42:12.575985  4780 caffe.cpp:313] Batch 627, accuracy/top1 = 0.04
I0816 10:42:12.576005  4780 caffe.cpp:313] Batch 627, accuracy/top5 = 0.06
I0816 10:42:12.576009  4780 caffe.cpp:313] Batch 627, loss = 6.3658
I0816 10:42:12.638928  4780 caffe.cpp:313] Batch 628, accuracy/top1 = 0
I0816 10:42:12.638949  4780 caffe.cpp:313] Batch 628, accuracy/top5 = 0
I0816 10:42:12.638952  4780 caffe.cpp:313] Batch 628, loss = 6.8575
I0816 10:42:12.701905  4780 caffe.cpp:313] Batch 629, accuracy/top1 = 0.04
I0816 10:42:12.701925  4780 caffe.cpp:313] Batch 629, accuracy/top5 = 0.04
I0816 10:42:12.701930  4780 caffe.cpp:313] Batch 629, loss = 6.56881
I0816 10:42:12.764695  4780 caffe.cpp:313] Batch 630, accuracy/top1 = 0
I0816 10:42:12.764717  4780 caffe.cpp:313] Batch 630, accuracy/top5 = 0.02
I0816 10:42:12.764721  4780 caffe.cpp:313] Batch 630, loss = 6.74306
I0816 10:42:12.827556  4780 caffe.cpp:313] Batch 631, accuracy/top1 = 0
I0816 10:42:12.827577  4780 caffe.cpp:313] Batch 631, accuracy/top5 = 0.04
I0816 10:42:12.827580  4780 caffe.cpp:313] Batch 631, loss = 6.64685
I0816 10:42:12.890595  4780 caffe.cpp:313] Batch 632, accuracy/top1 = 0
I0816 10:42:12.890616  4780 caffe.cpp:313] Batch 632, accuracy/top5 = 0.02
I0816 10:42:12.890619  4780 caffe.cpp:313] Batch 632, loss = 6.82884
I0816 10:42:12.953599  4780 caffe.cpp:313] Batch 633, accuracy/top1 = 0
I0816 10:42:12.953621  4780 caffe.cpp:313] Batch 633, accuracy/top5 = 0.02
I0816 10:42:12.953624  4780 caffe.cpp:313] Batch 633, loss = 6.8718
I0816 10:42:13.016779  4780 caffe.cpp:313] Batch 634, accuracy/top1 = 0
I0816 10:42:13.016796  4780 caffe.cpp:313] Batch 634, accuracy/top5 = 0.04
I0816 10:42:13.016799  4780 caffe.cpp:313] Batch 634, loss = 6.731
I0816 10:42:13.079830  4780 caffe.cpp:313] Batch 635, accuracy/top1 = 0
I0816 10:42:13.079864  4780 caffe.cpp:313] Batch 635, accuracy/top5 = 0.02
I0816 10:42:13.079867  4780 caffe.cpp:313] Batch 635, loss = 6.71349
I0816 10:42:13.142771  4780 caffe.cpp:313] Batch 636, accuracy/top1 = 0
I0816 10:42:13.142792  4780 caffe.cpp:313] Batch 636, accuracy/top5 = 0
I0816 10:42:13.142796  4780 caffe.cpp:313] Batch 636, loss = 7.01072
I0816 10:42:13.205679  4780 caffe.cpp:313] Batch 637, accuracy/top1 = 0
I0816 10:42:13.205699  4780 caffe.cpp:313] Batch 637, accuracy/top5 = 0.02
I0816 10:42:13.205703  4780 caffe.cpp:313] Batch 637, loss = 6.68731
I0816 10:42:13.268596  4780 caffe.cpp:313] Batch 638, accuracy/top1 = 0.02
I0816 10:42:13.268617  4780 caffe.cpp:313] Batch 638, accuracy/top5 = 0.02
I0816 10:42:13.268620  4780 caffe.cpp:313] Batch 638, loss = 6.77979
I0816 10:42:13.331674  4780 caffe.cpp:313] Batch 639, accuracy/top1 = 0.02
I0816 10:42:13.331693  4780 caffe.cpp:313] Batch 639, accuracy/top5 = 0.04
I0816 10:42:13.331696  4780 caffe.cpp:313] Batch 639, loss = 6.70881
I0816 10:42:13.394405  4780 caffe.cpp:313] Batch 640, accuracy/top1 = 0.02
I0816 10:42:13.394426  4780 caffe.cpp:313] Batch 640, accuracy/top5 = 0.06
I0816 10:42:13.394429  4780 caffe.cpp:313] Batch 640, loss = 6.5415
I0816 10:42:13.457456  4780 caffe.cpp:313] Batch 641, accuracy/top1 = 0.02
I0816 10:42:13.457476  4780 caffe.cpp:313] Batch 641, accuracy/top5 = 0.04
I0816 10:42:13.457479  4780 caffe.cpp:313] Batch 641, loss = 6.6372
I0816 10:42:13.520421  4780 caffe.cpp:313] Batch 642, accuracy/top1 = 0.02
I0816 10:42:13.520442  4780 caffe.cpp:313] Batch 642, accuracy/top5 = 0.04
I0816 10:42:13.520444  4780 caffe.cpp:313] Batch 642, loss = 6.75722
I0816 10:42:13.583380  4780 caffe.cpp:313] Batch 643, accuracy/top1 = 0.02
I0816 10:42:13.583402  4780 caffe.cpp:313] Batch 643, accuracy/top5 = 0.04
I0816 10:42:13.583405  4780 caffe.cpp:313] Batch 643, loss = 6.69143
I0816 10:42:13.646301  4780 caffe.cpp:313] Batch 644, accuracy/top1 = 0.02
I0816 10:42:13.646322  4780 caffe.cpp:313] Batch 644, accuracy/top5 = 0.04
I0816 10:42:13.646325  4780 caffe.cpp:313] Batch 644, loss = 6.77146
I0816 10:42:13.709254  4780 caffe.cpp:313] Batch 645, accuracy/top1 = 0.02
I0816 10:42:13.709275  4780 caffe.cpp:313] Batch 645, accuracy/top5 = 0.06
I0816 10:42:13.709277  4780 caffe.cpp:313] Batch 645, loss = 6.66723
I0816 10:42:13.772214  4780 caffe.cpp:313] Batch 646, accuracy/top1 = 0
I0816 10:42:13.772235  4780 caffe.cpp:313] Batch 646, accuracy/top5 = 0.06
I0816 10:42:13.772238  4780 caffe.cpp:313] Batch 646, loss = 7.03462
I0816 10:42:13.835170  4780 caffe.cpp:313] Batch 647, accuracy/top1 = 0
I0816 10:42:13.835188  4780 caffe.cpp:313] Batch 647, accuracy/top5 = 0.02
I0816 10:42:13.835192  4780 caffe.cpp:313] Batch 647, loss = 6.91001
I0816 10:42:13.898269  4780 caffe.cpp:313] Batch 648, accuracy/top1 = 0
I0816 10:42:13.898288  4780 caffe.cpp:313] Batch 648, accuracy/top5 = 0.02
I0816 10:42:13.898293  4780 caffe.cpp:313] Batch 648, loss = 6.71799
I0816 10:42:13.961200  4780 caffe.cpp:313] Batch 649, accuracy/top1 = 0
I0816 10:42:13.961220  4780 caffe.cpp:313] Batch 649, accuracy/top5 = 0.02
I0816 10:42:13.961225  4780 caffe.cpp:313] Batch 649, loss = 7.09347
I0816 10:42:14.024191  4780 caffe.cpp:313] Batch 650, accuracy/top1 = 0
I0816 10:42:14.024209  4780 caffe.cpp:313] Batch 650, accuracy/top5 = 0.02
I0816 10:42:14.024214  4780 caffe.cpp:313] Batch 650, loss = 6.54761
I0816 10:42:14.087169  4780 caffe.cpp:313] Batch 651, accuracy/top1 = 0.02
I0816 10:42:14.087190  4780 caffe.cpp:313] Batch 651, accuracy/top5 = 0.02
I0816 10:42:14.087194  4780 caffe.cpp:313] Batch 651, loss = 6.87104
I0816 10:42:14.150050  4780 caffe.cpp:313] Batch 652, accuracy/top1 = 0
I0816 10:42:14.150071  4780 caffe.cpp:313] Batch 652, accuracy/top5 = 0
I0816 10:42:14.150074  4780 caffe.cpp:313] Batch 652, loss = 7.02792
I0816 10:42:14.212829  4780 caffe.cpp:313] Batch 653, accuracy/top1 = 0.02
I0816 10:42:14.212852  4780 caffe.cpp:313] Batch 653, accuracy/top5 = 0.04
I0816 10:42:14.212854  4780 caffe.cpp:313] Batch 653, loss = 6.53459
I0816 10:42:14.275784  4780 caffe.cpp:313] Batch 654, accuracy/top1 = 0
I0816 10:42:14.275817  4780 caffe.cpp:313] Batch 654, accuracy/top5 = 0.02
I0816 10:42:14.275821  4780 caffe.cpp:313] Batch 654, loss = 6.57887
I0816 10:42:14.338892  4780 caffe.cpp:313] Batch 655, accuracy/top1 = 0
I0816 10:42:14.338913  4780 caffe.cpp:313] Batch 655, accuracy/top5 = 0.02
I0816 10:42:14.338917  4780 caffe.cpp:313] Batch 655, loss = 6.91574
I0816 10:42:14.401707  4780 caffe.cpp:313] Batch 656, accuracy/top1 = 0.04
I0816 10:42:14.401729  4780 caffe.cpp:313] Batch 656, accuracy/top5 = 0.1
I0816 10:42:14.401732  4780 caffe.cpp:313] Batch 656, loss = 6.54683
I0816 10:42:14.464635  4780 caffe.cpp:313] Batch 657, accuracy/top1 = 0.04
I0816 10:42:14.464656  4780 caffe.cpp:313] Batch 657, accuracy/top5 = 0.08
I0816 10:42:14.464659  4780 caffe.cpp:313] Batch 657, loss = 6.57467
I0816 10:42:14.527467  4780 caffe.cpp:313] Batch 658, accuracy/top1 = 0
I0816 10:42:14.527487  4780 caffe.cpp:313] Batch 658, accuracy/top5 = 0
I0816 10:42:14.527490  4780 caffe.cpp:313] Batch 658, loss = 6.69812
I0816 10:42:14.590378  4780 caffe.cpp:313] Batch 659, accuracy/top1 = 0.02
I0816 10:42:14.590399  4780 caffe.cpp:313] Batch 659, accuracy/top5 = 0.06
I0816 10:42:14.590402  4780 caffe.cpp:313] Batch 659, loss = 6.89728
I0816 10:42:14.653228  4780 caffe.cpp:313] Batch 660, accuracy/top1 = 0.02
I0816 10:42:14.653249  4780 caffe.cpp:313] Batch 660, accuracy/top5 = 0.02
I0816 10:42:14.653251  4780 caffe.cpp:313] Batch 660, loss = 6.77429
I0816 10:42:14.716176  4780 caffe.cpp:313] Batch 661, accuracy/top1 = 0.06
I0816 10:42:14.716198  4780 caffe.cpp:313] Batch 661, accuracy/top5 = 0.08
I0816 10:42:14.716202  4780 caffe.cpp:313] Batch 661, loss = 6.45356
I0816 10:42:14.779048  4780 caffe.cpp:313] Batch 662, accuracy/top1 = 0
I0816 10:42:14.779068  4780 caffe.cpp:313] Batch 662, accuracy/top5 = 0.02
I0816 10:42:14.779072  4780 caffe.cpp:313] Batch 662, loss = 6.98369
I0816 10:42:14.841879  4780 caffe.cpp:313] Batch 663, accuracy/top1 = 0.06
I0816 10:42:14.841900  4780 caffe.cpp:313] Batch 663, accuracy/top5 = 0.08
I0816 10:42:14.841903  4780 caffe.cpp:313] Batch 663, loss = 6.4312
I0816 10:42:14.904783  4780 caffe.cpp:313] Batch 664, accuracy/top1 = 0
I0816 10:42:14.904804  4780 caffe.cpp:313] Batch 664, accuracy/top5 = 0
I0816 10:42:14.904808  4780 caffe.cpp:313] Batch 664, loss = 7.00906
I0816 10:42:14.967676  4780 caffe.cpp:313] Batch 665, accuracy/top1 = 0
I0816 10:42:14.967697  4780 caffe.cpp:313] Batch 665, accuracy/top5 = 0
I0816 10:42:14.967700  4780 caffe.cpp:313] Batch 665, loss = 6.91237
I0816 10:42:15.030894  4780 caffe.cpp:313] Batch 666, accuracy/top1 = 0
I0816 10:42:15.030911  4780 caffe.cpp:313] Batch 666, accuracy/top5 = 0
I0816 10:42:15.030915  4780 caffe.cpp:313] Batch 666, loss = 6.93799
I0816 10:42:15.093675  4780 caffe.cpp:313] Batch 667, accuracy/top1 = 0
I0816 10:42:15.093696  4780 caffe.cpp:313] Batch 667, accuracy/top5 = 0.04
I0816 10:42:15.093700  4780 caffe.cpp:313] Batch 667, loss = 6.89168
I0816 10:42:15.156536  4780 caffe.cpp:313] Batch 668, accuracy/top1 = 0.02
I0816 10:42:15.156556  4780 caffe.cpp:313] Batch 668, accuracy/top5 = 0.02
I0816 10:42:15.156559  4780 caffe.cpp:313] Batch 668, loss = 6.7217
I0816 10:42:15.219565  4780 caffe.cpp:313] Batch 669, accuracy/top1 = 0
I0816 10:42:15.219585  4780 caffe.cpp:313] Batch 669, accuracy/top5 = 0.04
I0816 10:42:15.219588  4780 caffe.cpp:313] Batch 669, loss = 6.8401
I0816 10:42:15.282529  4780 caffe.cpp:313] Batch 670, accuracy/top1 = 0
I0816 10:42:15.282552  4780 caffe.cpp:313] Batch 670, accuracy/top5 = 0.06
I0816 10:42:15.282554  4780 caffe.cpp:313] Batch 670, loss = 6.72844
I0816 10:42:15.345422  4780 caffe.cpp:313] Batch 671, accuracy/top1 = 0.04
I0816 10:42:15.345443  4780 caffe.cpp:313] Batch 671, accuracy/top5 = 0.08
I0816 10:42:15.345445  4780 caffe.cpp:313] Batch 671, loss = 6.35188
I0816 10:42:15.408392  4780 caffe.cpp:313] Batch 672, accuracy/top1 = 0.02
I0816 10:42:15.408412  4780 caffe.cpp:313] Batch 672, accuracy/top5 = 0.04
I0816 10:42:15.408416  4780 caffe.cpp:313] Batch 672, loss = 6.7516
I0816 10:42:15.471819  4780 caffe.cpp:313] Batch 673, accuracy/top1 = 0
I0816 10:42:15.471837  4780 caffe.cpp:313] Batch 673, accuracy/top5 = 0.04
I0816 10:42:15.471839  4780 caffe.cpp:313] Batch 673, loss = 6.80094
I0816 10:42:15.534801  4780 caffe.cpp:313] Batch 674, accuracy/top1 = 0.02
I0816 10:42:15.534821  4780 caffe.cpp:313] Batch 674, accuracy/top5 = 0.04
I0816 10:42:15.534824  4780 caffe.cpp:313] Batch 674, loss = 6.77411
I0816 10:42:15.597828  4780 caffe.cpp:313] Batch 675, accuracy/top1 = 0.02
I0816 10:42:15.597849  4780 caffe.cpp:313] Batch 675, accuracy/top5 = 0.06
I0816 10:42:15.597852  4780 caffe.cpp:313] Batch 675, loss = 6.71253
I0816 10:42:15.660722  4780 caffe.cpp:313] Batch 676, accuracy/top1 = 0
I0816 10:42:15.660742  4780 caffe.cpp:313] Batch 676, accuracy/top5 = 0
I0816 10:42:15.660745  4780 caffe.cpp:313] Batch 676, loss = 6.7157
I0816 10:42:15.723798  4780 caffe.cpp:313] Batch 677, accuracy/top1 = 0
I0816 10:42:15.723819  4780 caffe.cpp:313] Batch 677, accuracy/top5 = 0.04
I0816 10:42:15.723822  4780 caffe.cpp:313] Batch 677, loss = 6.69016
I0816 10:42:15.786854  4780 caffe.cpp:313] Batch 678, accuracy/top1 = 0
I0816 10:42:15.786875  4780 caffe.cpp:313] Batch 678, accuracy/top5 = 0.04
I0816 10:42:15.786878  4780 caffe.cpp:313] Batch 678, loss = 6.66847
I0816 10:42:15.849956  4780 caffe.cpp:313] Batch 679, accuracy/top1 = 0.02
I0816 10:42:15.849977  4780 caffe.cpp:313] Batch 679, accuracy/top5 = 0.06
I0816 10:42:15.849980  4780 caffe.cpp:313] Batch 679, loss = 6.8869
I0816 10:42:15.913015  4780 caffe.cpp:313] Batch 680, accuracy/top1 = 0.02
I0816 10:42:15.913036  4780 caffe.cpp:313] Batch 680, accuracy/top5 = 0.02
I0816 10:42:15.913039  4780 caffe.cpp:313] Batch 680, loss = 6.88391
I0816 10:42:15.976056  4780 caffe.cpp:313] Batch 681, accuracy/top1 = 0.02
I0816 10:42:15.976078  4780 caffe.cpp:313] Batch 681, accuracy/top5 = 0.06
I0816 10:42:15.976080  4780 caffe.cpp:313] Batch 681, loss = 6.48058
I0816 10:42:16.039335  4780 caffe.cpp:313] Batch 682, accuracy/top1 = 0.02
I0816 10:42:16.039355  4780 caffe.cpp:313] Batch 682, accuracy/top5 = 0.02
I0816 10:42:16.039358  4780 caffe.cpp:313] Batch 682, loss = 6.79423
I0816 10:42:16.102427  4780 caffe.cpp:313] Batch 683, accuracy/top1 = 0
I0816 10:42:16.102447  4780 caffe.cpp:313] Batch 683, accuracy/top5 = 0.04
I0816 10:42:16.102452  4780 caffe.cpp:313] Batch 683, loss = 6.7682
I0816 10:42:16.165400  4780 caffe.cpp:313] Batch 684, accuracy/top1 = 0
I0816 10:42:16.165422  4780 caffe.cpp:313] Batch 684, accuracy/top5 = 0.02
I0816 10:42:16.165426  4780 caffe.cpp:313] Batch 684, loss = 6.71152
I0816 10:42:16.228339  4780 caffe.cpp:313] Batch 685, accuracy/top1 = 0.02
I0816 10:42:16.228361  4780 caffe.cpp:313] Batch 685, accuracy/top5 = 0.02
I0816 10:42:16.228364  4780 caffe.cpp:313] Batch 685, loss = 6.69285
I0816 10:42:16.291277  4780 caffe.cpp:313] Batch 686, accuracy/top1 = 0.02
I0816 10:42:16.291298  4780 caffe.cpp:313] Batch 686, accuracy/top5 = 0.04
I0816 10:42:16.291301  4780 caffe.cpp:313] Batch 686, loss = 6.87552
I0816 10:42:16.354193  4780 caffe.cpp:313] Batch 687, accuracy/top1 = 0.04
I0816 10:42:16.354214  4780 caffe.cpp:313] Batch 687, accuracy/top5 = 0.08
I0816 10:42:16.354218  4780 caffe.cpp:313] Batch 687, loss = 6.57398
I0816 10:42:16.417168  4780 caffe.cpp:313] Batch 688, accuracy/top1 = 0
I0816 10:42:16.417188  4780 caffe.cpp:313] Batch 688, accuracy/top5 = 0.04
I0816 10:42:16.417192  4780 caffe.cpp:313] Batch 688, loss = 6.77761
I0816 10:42:16.480126  4780 caffe.cpp:313] Batch 689, accuracy/top1 = 0
I0816 10:42:16.480149  4780 caffe.cpp:313] Batch 689, accuracy/top5 = 0.08
I0816 10:42:16.480152  4780 caffe.cpp:313] Batch 689, loss = 6.6375
I0816 10:42:16.543097  4780 caffe.cpp:313] Batch 690, accuracy/top1 = 0
I0816 10:42:16.543118  4780 caffe.cpp:313] Batch 690, accuracy/top5 = 0.06
I0816 10:42:16.543121  4780 caffe.cpp:313] Batch 690, loss = 6.67165
I0816 10:42:16.606055  4780 caffe.cpp:313] Batch 691, accuracy/top1 = 0.06
I0816 10:42:16.606076  4780 caffe.cpp:313] Batch 691, accuracy/top5 = 0.06
I0816 10:42:16.606079  4780 caffe.cpp:313] Batch 691, loss = 6.5871
I0816 10:42:16.669051  4780 caffe.cpp:313] Batch 692, accuracy/top1 = 0.02
I0816 10:42:16.669071  4780 caffe.cpp:313] Batch 692, accuracy/top5 = 0.02
I0816 10:42:16.669075  4780 caffe.cpp:313] Batch 692, loss = 6.51943
I0816 10:42:16.731947  4780 caffe.cpp:313] Batch 693, accuracy/top1 = 0
I0816 10:42:16.731968  4780 caffe.cpp:313] Batch 693, accuracy/top5 = 0.04
I0816 10:42:16.731972  4780 caffe.cpp:313] Batch 693, loss = 6.95169
I0816 10:42:16.794842  4780 caffe.cpp:313] Batch 694, accuracy/top1 = 0
I0816 10:42:16.794862  4780 caffe.cpp:313] Batch 694, accuracy/top5 = 0.04
I0816 10:42:16.794864  4780 caffe.cpp:313] Batch 694, loss = 6.98439
I0816 10:42:16.857789  4780 caffe.cpp:313] Batch 695, accuracy/top1 = 0
I0816 10:42:16.857808  4780 caffe.cpp:313] Batch 695, accuracy/top5 = 0
I0816 10:42:16.857811  4780 caffe.cpp:313] Batch 695, loss = 6.92065
I0816 10:42:16.920779  4780 caffe.cpp:313] Batch 696, accuracy/top1 = 0.02
I0816 10:42:16.920800  4780 caffe.cpp:313] Batch 696, accuracy/top5 = 0.02
I0816 10:42:16.920804  4780 caffe.cpp:313] Batch 696, loss = 6.91234
I0816 10:42:16.983799  4780 caffe.cpp:313] Batch 697, accuracy/top1 = 0.06
I0816 10:42:16.983821  4780 caffe.cpp:313] Batch 697, accuracy/top5 = 0.06
I0816 10:42:16.983826  4780 caffe.cpp:313] Batch 697, loss = 6.62305
I0816 10:42:17.046900  4780 caffe.cpp:313] Batch 698, accuracy/top1 = 0.02
I0816 10:42:17.046919  4780 caffe.cpp:313] Batch 698, accuracy/top5 = 0.02
I0816 10:42:17.046923  4780 caffe.cpp:313] Batch 698, loss = 6.60814
I0816 10:42:17.109771  4780 caffe.cpp:313] Batch 699, accuracy/top1 = 0.02
I0816 10:42:17.109794  4780 caffe.cpp:313] Batch 699, accuracy/top5 = 0.02
I0816 10:42:17.109797  4780 caffe.cpp:313] Batch 699, loss = 6.82592
I0816 10:42:17.172685  4780 caffe.cpp:313] Batch 700, accuracy/top1 = 0.02
I0816 10:42:17.172706  4780 caffe.cpp:313] Batch 700, accuracy/top5 = 0.04
I0816 10:42:17.172710  4780 caffe.cpp:313] Batch 700, loss = 6.57929
I0816 10:42:17.235539  4780 caffe.cpp:313] Batch 701, accuracy/top1 = 0.02
I0816 10:42:17.235560  4780 caffe.cpp:313] Batch 701, accuracy/top5 = 0.04
I0816 10:42:17.235564  4780 caffe.cpp:313] Batch 701, loss = 6.63995
I0816 10:42:17.298437  4780 caffe.cpp:313] Batch 702, accuracy/top1 = 0
I0816 10:42:17.298458  4780 caffe.cpp:313] Batch 702, accuracy/top5 = 0.08
I0816 10:42:17.298463  4780 caffe.cpp:313] Batch 702, loss = 6.75124
I0816 10:42:17.361332  4780 caffe.cpp:313] Batch 703, accuracy/top1 = 0.02
I0816 10:42:17.361353  4780 caffe.cpp:313] Batch 703, accuracy/top5 = 0.02
I0816 10:42:17.361357  4780 caffe.cpp:313] Batch 703, loss = 6.92383
I0816 10:42:17.424371  4780 caffe.cpp:313] Batch 704, accuracy/top1 = 0
I0816 10:42:17.424393  4780 caffe.cpp:313] Batch 704, accuracy/top5 = 0.04
I0816 10:42:17.424397  4780 caffe.cpp:313] Batch 704, loss = 6.63869
I0816 10:42:17.487496  4780 caffe.cpp:313] Batch 705, accuracy/top1 = 0
I0816 10:42:17.487514  4780 caffe.cpp:313] Batch 705, accuracy/top5 = 0.02
I0816 10:42:17.487517  4780 caffe.cpp:313] Batch 705, loss = 6.82766
I0816 10:42:17.550406  4780 caffe.cpp:313] Batch 706, accuracy/top1 = 0
I0816 10:42:17.550428  4780 caffe.cpp:313] Batch 706, accuracy/top5 = 0.02
I0816 10:42:17.550433  4780 caffe.cpp:313] Batch 706, loss = 6.59105
I0816 10:42:17.613353  4780 caffe.cpp:313] Batch 707, accuracy/top1 = 0
I0816 10:42:17.613374  4780 caffe.cpp:313] Batch 707, accuracy/top5 = 0
I0816 10:42:17.613379  4780 caffe.cpp:313] Batch 707, loss = 6.82205
I0816 10:42:17.676213  4780 caffe.cpp:313] Batch 708, accuracy/top1 = 0
I0816 10:42:17.676235  4780 caffe.cpp:313] Batch 708, accuracy/top5 = 0.04
I0816 10:42:17.676239  4780 caffe.cpp:313] Batch 708, loss = 6.86341
I0816 10:42:17.739297  4780 caffe.cpp:313] Batch 709, accuracy/top1 = 0.02
I0816 10:42:17.739320  4780 caffe.cpp:313] Batch 709, accuracy/top5 = 0.04
I0816 10:42:17.739323  4780 caffe.cpp:313] Batch 709, loss = 6.54688
I0816 10:42:17.802193  4780 caffe.cpp:313] Batch 710, accuracy/top1 = 0
I0816 10:42:17.802214  4780 caffe.cpp:313] Batch 710, accuracy/top5 = 0.02
I0816 10:42:17.802219  4780 caffe.cpp:313] Batch 710, loss = 6.84322
I0816 10:42:17.865177  4780 caffe.cpp:313] Batch 711, accuracy/top1 = 0
I0816 10:42:17.865198  4780 caffe.cpp:313] Batch 711, accuracy/top5 = 0
I0816 10:42:17.865202  4780 caffe.cpp:313] Batch 711, loss = 6.74282
I0816 10:42:17.928262  4780 caffe.cpp:313] Batch 712, accuracy/top1 = 0
I0816 10:42:17.928284  4780 caffe.cpp:313] Batch 712, accuracy/top5 = 0
I0816 10:42:17.928288  4780 caffe.cpp:313] Batch 712, loss = 7.00361
I0816 10:42:17.991576  4780 caffe.cpp:313] Batch 713, accuracy/top1 = 0
I0816 10:42:17.991595  4780 caffe.cpp:313] Batch 713, accuracy/top5 = 0
I0816 10:42:17.991598  4780 caffe.cpp:313] Batch 713, loss = 7.0592
I0816 10:42:18.054605  4780 caffe.cpp:313] Batch 714, accuracy/top1 = 0.02
I0816 10:42:18.054625  4780 caffe.cpp:313] Batch 714, accuracy/top5 = 0.06
I0816 10:42:18.054628  4780 caffe.cpp:313] Batch 714, loss = 6.64813
I0816 10:42:18.117648  4780 caffe.cpp:313] Batch 715, accuracy/top1 = 0
I0816 10:42:18.117668  4780 caffe.cpp:313] Batch 715, accuracy/top5 = 0.02
I0816 10:42:18.117673  4780 caffe.cpp:313] Batch 715, loss = 6.95856
I0816 10:42:18.180801  4780 caffe.cpp:313] Batch 716, accuracy/top1 = 0
I0816 10:42:18.180821  4780 caffe.cpp:313] Batch 716, accuracy/top5 = 0.02
I0816 10:42:18.180826  4780 caffe.cpp:313] Batch 716, loss = 6.78113
I0816 10:42:18.243832  4780 caffe.cpp:313] Batch 717, accuracy/top1 = 0
I0816 10:42:18.243854  4780 caffe.cpp:313] Batch 717, accuracy/top5 = 0.02
I0816 10:42:18.243858  4780 caffe.cpp:313] Batch 717, loss = 6.86954
I0816 10:42:18.306686  4780 caffe.cpp:313] Batch 718, accuracy/top1 = 0
I0816 10:42:18.306707  4780 caffe.cpp:313] Batch 718, accuracy/top5 = 0.04
I0816 10:42:18.306711  4780 caffe.cpp:313] Batch 718, loss = 6.97284
I0816 10:42:18.369635  4780 caffe.cpp:313] Batch 719, accuracy/top1 = 0.02
I0816 10:42:18.369657  4780 caffe.cpp:313] Batch 719, accuracy/top5 = 0.06
I0816 10:42:18.369662  4780 caffe.cpp:313] Batch 719, loss = 6.98998
I0816 10:42:18.432443  4780 caffe.cpp:313] Batch 720, accuracy/top1 = 0
I0816 10:42:18.432464  4780 caffe.cpp:313] Batch 720, accuracy/top5 = 0
I0816 10:42:18.432468  4780 caffe.cpp:313] Batch 720, loss = 6.83329
I0816 10:42:18.495347  4780 caffe.cpp:313] Batch 721, accuracy/top1 = 0
I0816 10:42:18.495369  4780 caffe.cpp:313] Batch 721, accuracy/top5 = 0.06
I0816 10:42:18.495373  4780 caffe.cpp:313] Batch 721, loss = 6.72878
I0816 10:42:18.558370  4780 caffe.cpp:313] Batch 722, accuracy/top1 = 0
I0816 10:42:18.558392  4780 caffe.cpp:313] Batch 722, accuracy/top5 = 0.02
I0816 10:42:18.558395  4780 caffe.cpp:313] Batch 722, loss = 6.6695
I0816 10:42:18.621366  4780 caffe.cpp:313] Batch 723, accuracy/top1 = 0
I0816 10:42:18.621387  4780 caffe.cpp:313] Batch 723, accuracy/top5 = 0.02
I0816 10:42:18.621392  4780 caffe.cpp:313] Batch 723, loss = 6.71222
I0816 10:42:18.684422  4780 caffe.cpp:313] Batch 724, accuracy/top1 = 0.02
I0816 10:42:18.684444  4780 caffe.cpp:313] Batch 724, accuracy/top5 = 0.02
I0816 10:42:18.684448  4780 caffe.cpp:313] Batch 724, loss = 6.67552
I0816 10:42:18.747323  4780 caffe.cpp:313] Batch 725, accuracy/top1 = 0.02
I0816 10:42:18.747344  4780 caffe.cpp:313] Batch 725, accuracy/top5 = 0.02
I0816 10:42:18.747349  4780 caffe.cpp:313] Batch 725, loss = 6.89736
I0816 10:42:18.810145  4780 caffe.cpp:313] Batch 726, accuracy/top1 = 0
I0816 10:42:18.810166  4780 caffe.cpp:313] Batch 726, accuracy/top5 = 0.04
I0816 10:42:18.810170  4780 caffe.cpp:313] Batch 726, loss = 6.58267
I0816 10:42:18.873155  4780 caffe.cpp:313] Batch 727, accuracy/top1 = 0
I0816 10:42:18.873178  4780 caffe.cpp:313] Batch 727, accuracy/top5 = 0
I0816 10:42:18.873181  4780 caffe.cpp:313] Batch 727, loss = 6.86534
I0816 10:42:18.936194  4780 caffe.cpp:313] Batch 728, accuracy/top1 = 0
I0816 10:42:18.936216  4780 caffe.cpp:313] Batch 728, accuracy/top5 = 0.02
I0816 10:42:18.936221  4780 caffe.cpp:313] Batch 728, loss = 6.61993
I0816 10:42:18.999740  4780 caffe.cpp:313] Batch 729, accuracy/top1 = 0
I0816 10:42:18.999758  4780 caffe.cpp:313] Batch 729, accuracy/top5 = 0.04
I0816 10:42:18.999778  4780 caffe.cpp:313] Batch 729, loss = 6.56829
I0816 10:42:19.062686  4780 caffe.cpp:313] Batch 730, accuracy/top1 = 0.04
I0816 10:42:19.062707  4780 caffe.cpp:313] Batch 730, accuracy/top5 = 0.04
I0816 10:42:19.062711  4780 caffe.cpp:313] Batch 730, loss = 6.56138
I0816 10:42:19.125557  4780 caffe.cpp:313] Batch 731, accuracy/top1 = 0
I0816 10:42:19.125579  4780 caffe.cpp:313] Batch 731, accuracy/top5 = 0.04
I0816 10:42:19.125584  4780 caffe.cpp:313] Batch 731, loss = 6.87549
I0816 10:42:19.188467  4780 caffe.cpp:313] Batch 732, accuracy/top1 = 0.04
I0816 10:42:19.188488  4780 caffe.cpp:313] Batch 732, accuracy/top5 = 0.06
I0816 10:42:19.188493  4780 caffe.cpp:313] Batch 732, loss = 6.85604
I0816 10:42:19.251438  4780 caffe.cpp:313] Batch 733, accuracy/top1 = 0
I0816 10:42:19.251461  4780 caffe.cpp:313] Batch 733, accuracy/top5 = 0.02
I0816 10:42:19.251466  4780 caffe.cpp:313] Batch 733, loss = 6.83324
I0816 10:42:19.314414  4780 caffe.cpp:313] Batch 734, accuracy/top1 = 0
I0816 10:42:19.314435  4780 caffe.cpp:313] Batch 734, accuracy/top5 = 0.02
I0816 10:42:19.314440  4780 caffe.cpp:313] Batch 734, loss = 6.89447
I0816 10:42:19.377246  4780 caffe.cpp:313] Batch 735, accuracy/top1 = 0.02
I0816 10:42:19.377269  4780 caffe.cpp:313] Batch 735, accuracy/top5 = 0.06
I0816 10:42:19.377272  4780 caffe.cpp:313] Batch 735, loss = 6.75379
I0816 10:42:19.440213  4780 caffe.cpp:313] Batch 736, accuracy/top1 = 0.04
I0816 10:42:19.440237  4780 caffe.cpp:313] Batch 736, accuracy/top5 = 0.04
I0816 10:42:19.440240  4780 caffe.cpp:313] Batch 736, loss = 6.52072
I0816 10:42:19.503264  4780 caffe.cpp:313] Batch 737, accuracy/top1 = 0.02
I0816 10:42:19.503283  4780 caffe.cpp:313] Batch 737, accuracy/top5 = 0.02
I0816 10:42:19.503288  4780 caffe.cpp:313] Batch 737, loss = 6.48302
I0816 10:42:19.566321  4780 caffe.cpp:313] Batch 738, accuracy/top1 = 0.04
I0816 10:42:19.566344  4780 caffe.cpp:313] Batch 738, accuracy/top5 = 0.04
I0816 10:42:19.566347  4780 caffe.cpp:313] Batch 738, loss = 6.66081
I0816 10:42:19.629178  4780 caffe.cpp:313] Batch 739, accuracy/top1 = 0
I0816 10:42:19.629199  4780 caffe.cpp:313] Batch 739, accuracy/top5 = 0.02
I0816 10:42:19.629204  4780 caffe.cpp:313] Batch 739, loss = 6.96581
I0816 10:42:19.692210  4780 caffe.cpp:313] Batch 740, accuracy/top1 = 0.02
I0816 10:42:19.692232  4780 caffe.cpp:313] Batch 740, accuracy/top5 = 0.04
I0816 10:42:19.692237  4780 caffe.cpp:313] Batch 740, loss = 6.66167
I0816 10:42:19.755170  4780 caffe.cpp:313] Batch 741, accuracy/top1 = 0
I0816 10:42:19.755192  4780 caffe.cpp:313] Batch 741, accuracy/top5 = 0.04
I0816 10:42:19.755197  4780 caffe.cpp:313] Batch 741, loss = 6.85886
I0816 10:42:19.818150  4780 caffe.cpp:313] Batch 742, accuracy/top1 = 0.04
I0816 10:42:19.818172  4780 caffe.cpp:313] Batch 742, accuracy/top5 = 0.06
I0816 10:42:19.818176  4780 caffe.cpp:313] Batch 742, loss = 6.67649
I0816 10:42:19.881216  4780 caffe.cpp:313] Batch 743, accuracy/top1 = 0.06
I0816 10:42:19.881237  4780 caffe.cpp:313] Batch 743, accuracy/top5 = 0.1
I0816 10:42:19.881242  4780 caffe.cpp:313] Batch 743, loss = 6.54921
I0816 10:42:19.944180  4780 caffe.cpp:313] Batch 744, accuracy/top1 = 0
I0816 10:42:19.944202  4780 caffe.cpp:313] Batch 744, accuracy/top5 = 0
I0816 10:42:19.944206  4780 caffe.cpp:313] Batch 744, loss = 7.07248
I0816 10:42:20.007324  4780 caffe.cpp:313] Batch 745, accuracy/top1 = 0
I0816 10:42:20.007341  4780 caffe.cpp:313] Batch 745, accuracy/top5 = 0.02
I0816 10:42:20.007345  4780 caffe.cpp:313] Batch 745, loss = 6.79803
I0816 10:42:20.070334  4780 caffe.cpp:313] Batch 746, accuracy/top1 = 0.04
I0816 10:42:20.070354  4780 caffe.cpp:313] Batch 746, accuracy/top5 = 0.04
I0816 10:42:20.070358  4780 caffe.cpp:313] Batch 746, loss = 6.60374
I0816 10:42:20.133359  4780 caffe.cpp:313] Batch 747, accuracy/top1 = 0.02
I0816 10:42:20.133379  4780 caffe.cpp:313] Batch 747, accuracy/top5 = 0.08
I0816 10:42:20.133383  4780 caffe.cpp:313] Batch 747, loss = 6.77134
I0816 10:42:20.196327  4780 caffe.cpp:313] Batch 748, accuracy/top1 = 0.02
I0816 10:42:20.196348  4780 caffe.cpp:313] Batch 748, accuracy/top5 = 0.04
I0816 10:42:20.196368  4780 caffe.cpp:313] Batch 748, loss = 6.75928
I0816 10:42:20.259359  4780 caffe.cpp:313] Batch 749, accuracy/top1 = 0
I0816 10:42:20.259378  4780 caffe.cpp:313] Batch 749, accuracy/top5 = 0.02
I0816 10:42:20.259382  4780 caffe.cpp:313] Batch 749, loss = 6.54952
I0816 10:42:20.322618  4780 caffe.cpp:313] Batch 750, accuracy/top1 = 0.02
I0816 10:42:20.322638  4780 caffe.cpp:313] Batch 750, accuracy/top5 = 0.04
I0816 10:42:20.322643  4780 caffe.cpp:313] Batch 750, loss = 6.67753
I0816 10:42:20.385593  4780 caffe.cpp:313] Batch 751, accuracy/top1 = 0.04
I0816 10:42:20.385614  4780 caffe.cpp:313] Batch 751, accuracy/top5 = 0.06
I0816 10:42:20.385618  4780 caffe.cpp:313] Batch 751, loss = 6.94812
I0816 10:42:20.448536  4780 caffe.cpp:313] Batch 752, accuracy/top1 = 0.02
I0816 10:42:20.448559  4780 caffe.cpp:313] Batch 752, accuracy/top5 = 0.02
I0816 10:42:20.448562  4780 caffe.cpp:313] Batch 752, loss = 7.04474
I0816 10:42:20.511373  4780 caffe.cpp:313] Batch 753, accuracy/top1 = 0.02
I0816 10:42:20.511395  4780 caffe.cpp:313] Batch 753, accuracy/top5 = 0.02
I0816 10:42:20.511399  4780 caffe.cpp:313] Batch 753, loss = 6.79959
I0816 10:42:20.574214  4780 caffe.cpp:313] Batch 754, accuracy/top1 = 0
I0816 10:42:20.574235  4780 caffe.cpp:313] Batch 754, accuracy/top5 = 0.06
I0816 10:42:20.574240  4780 caffe.cpp:313] Batch 754, loss = 6.79287
I0816 10:42:20.637307  4780 caffe.cpp:313] Batch 755, accuracy/top1 = 0
I0816 10:42:20.637329  4780 caffe.cpp:313] Batch 755, accuracy/top5 = 0.02
I0816 10:42:20.637333  4780 caffe.cpp:313] Batch 755, loss = 6.64155
I0816 10:42:20.700145  4780 caffe.cpp:313] Batch 756, accuracy/top1 = 0
I0816 10:42:20.700165  4780 caffe.cpp:313] Batch 756, accuracy/top5 = 0.02
I0816 10:42:20.700170  4780 caffe.cpp:313] Batch 756, loss = 6.70852
I0816 10:42:20.763110  4780 caffe.cpp:313] Batch 757, accuracy/top1 = 0.02
I0816 10:42:20.763133  4780 caffe.cpp:313] Batch 757, accuracy/top5 = 0.1
I0816 10:42:20.763136  4780 caffe.cpp:313] Batch 757, loss = 6.55744
I0816 10:42:20.825979  4780 caffe.cpp:313] Batch 758, accuracy/top1 = 0.04
I0816 10:42:20.826001  4780 caffe.cpp:313] Batch 758, accuracy/top5 = 0.04
I0816 10:42:20.826005  4780 caffe.cpp:313] Batch 758, loss = 6.81079
I0816 10:42:20.889065  4780 caffe.cpp:313] Batch 759, accuracy/top1 = 0
I0816 10:42:20.889088  4780 caffe.cpp:313] Batch 759, accuracy/top5 = 0
I0816 10:42:20.889092  4780 caffe.cpp:313] Batch 759, loss = 7.07874
I0816 10:42:20.952162  4780 caffe.cpp:313] Batch 760, accuracy/top1 = 0
I0816 10:42:20.952183  4780 caffe.cpp:313] Batch 760, accuracy/top5 = 0.04
I0816 10:42:20.952188  4780 caffe.cpp:313] Batch 760, loss = 6.57536
I0816 10:42:21.015348  4780 caffe.cpp:313] Batch 761, accuracy/top1 = 0
I0816 10:42:21.015364  4780 caffe.cpp:313] Batch 761, accuracy/top5 = 0
I0816 10:42:21.015368  4780 caffe.cpp:313] Batch 761, loss = 6.64042
I0816 10:42:21.078297  4780 caffe.cpp:313] Batch 762, accuracy/top1 = 0
I0816 10:42:21.078318  4780 caffe.cpp:313] Batch 762, accuracy/top5 = 0
I0816 10:42:21.078322  4780 caffe.cpp:313] Batch 762, loss = 7.09949
I0816 10:42:21.141213  4780 caffe.cpp:313] Batch 763, accuracy/top1 = 0
I0816 10:42:21.141234  4780 caffe.cpp:313] Batch 763, accuracy/top5 = 0.04
I0816 10:42:21.141239  4780 caffe.cpp:313] Batch 763, loss = 6.6952
I0816 10:42:21.204011  4780 caffe.cpp:313] Batch 764, accuracy/top1 = 0.02
I0816 10:42:21.204033  4780 caffe.cpp:313] Batch 764, accuracy/top5 = 0.06
I0816 10:42:21.204037  4780 caffe.cpp:313] Batch 764, loss = 6.59384
I0816 10:42:21.266826  4780 caffe.cpp:313] Batch 765, accuracy/top1 = 0
I0816 10:42:21.266849  4780 caffe.cpp:313] Batch 765, accuracy/top5 = 0
I0816 10:42:21.266852  4780 caffe.cpp:313] Batch 765, loss = 7.01931
I0816 10:42:21.329715  4780 caffe.cpp:313] Batch 766, accuracy/top1 = 0
I0816 10:42:21.329736  4780 caffe.cpp:313] Batch 766, accuracy/top5 = 0
I0816 10:42:21.329741  4780 caffe.cpp:313] Batch 766, loss = 6.91747
I0816 10:42:21.392664  4780 caffe.cpp:313] Batch 767, accuracy/top1 = 0
I0816 10:42:21.392685  4780 caffe.cpp:313] Batch 767, accuracy/top5 = 0.02
I0816 10:42:21.392709  4780 caffe.cpp:313] Batch 767, loss = 6.82929
I0816 10:42:21.455613  4780 caffe.cpp:313] Batch 768, accuracy/top1 = 0.04
I0816 10:42:21.455636  4780 caffe.cpp:313] Batch 768, accuracy/top5 = 0.06
I0816 10:42:21.455639  4780 caffe.cpp:313] Batch 768, loss = 6.64129
I0816 10:42:21.518456  4780 caffe.cpp:313] Batch 769, accuracy/top1 = 0.04
I0816 10:42:21.518477  4780 caffe.cpp:313] Batch 769, accuracy/top5 = 0.12
I0816 10:42:21.518481  4780 caffe.cpp:313] Batch 769, loss = 6.63224
I0816 10:42:21.581712  4780 caffe.cpp:313] Batch 770, accuracy/top1 = 0
I0816 10:42:21.581733  4780 caffe.cpp:313] Batch 770, accuracy/top5 = 0.1
I0816 10:42:21.581737  4780 caffe.cpp:313] Batch 770, loss = 6.67442
I0816 10:42:21.644670  4780 caffe.cpp:313] Batch 771, accuracy/top1 = 0
I0816 10:42:21.644692  4780 caffe.cpp:313] Batch 771, accuracy/top5 = 0.02
I0816 10:42:21.644696  4780 caffe.cpp:313] Batch 771, loss = 6.75357
I0816 10:42:21.707584  4780 caffe.cpp:313] Batch 772, accuracy/top1 = 0.02
I0816 10:42:21.707605  4780 caffe.cpp:313] Batch 772, accuracy/top5 = 0.06
I0816 10:42:21.707609  4780 caffe.cpp:313] Batch 772, loss = 6.58616
I0816 10:42:21.770447  4780 caffe.cpp:313] Batch 773, accuracy/top1 = 0.04
I0816 10:42:21.770468  4780 caffe.cpp:313] Batch 773, accuracy/top5 = 0.06
I0816 10:42:21.770473  4780 caffe.cpp:313] Batch 773, loss = 6.68555
I0816 10:42:21.833501  4780 caffe.cpp:313] Batch 774, accuracy/top1 = 0.02
I0816 10:42:21.833523  4780 caffe.cpp:313] Batch 774, accuracy/top5 = 0.02
I0816 10:42:21.833526  4780 caffe.cpp:313] Batch 774, loss = 6.61063
I0816 10:42:21.896522  4780 caffe.cpp:313] Batch 775, accuracy/top1 = 0
I0816 10:42:21.896543  4780 caffe.cpp:313] Batch 775, accuracy/top5 = 0
I0816 10:42:21.896548  4780 caffe.cpp:313] Batch 775, loss = 7.05172
I0816 10:42:21.959481  4780 caffe.cpp:313] Batch 776, accuracy/top1 = 0
I0816 10:42:21.959501  4780 caffe.cpp:313] Batch 776, accuracy/top5 = 0
I0816 10:42:21.959506  4780 caffe.cpp:313] Batch 776, loss = 6.95642
I0816 10:42:22.022706  4780 caffe.cpp:313] Batch 777, accuracy/top1 = 0.02
I0816 10:42:22.022723  4780 caffe.cpp:313] Batch 777, accuracy/top5 = 0.08
I0816 10:42:22.022727  4780 caffe.cpp:313] Batch 777, loss = 6.72619
I0816 10:42:22.085692  4780 caffe.cpp:313] Batch 778, accuracy/top1 = 0.02
I0816 10:42:22.085713  4780 caffe.cpp:313] Batch 778, accuracy/top5 = 0.04
I0816 10:42:22.085717  4780 caffe.cpp:313] Batch 778, loss = 6.97248
I0816 10:42:22.148653  4780 caffe.cpp:313] Batch 779, accuracy/top1 = 0.02
I0816 10:42:22.148674  4780 caffe.cpp:313] Batch 779, accuracy/top5 = 0.02
I0816 10:42:22.148679  4780 caffe.cpp:313] Batch 779, loss = 6.65337
I0816 10:42:22.211519  4780 caffe.cpp:313] Batch 780, accuracy/top1 = 0.02
I0816 10:42:22.211540  4780 caffe.cpp:313] Batch 780, accuracy/top5 = 0.04
I0816 10:42:22.211544  4780 caffe.cpp:313] Batch 780, loss = 6.67796
I0816 10:42:22.274416  4780 caffe.cpp:313] Batch 781, accuracy/top1 = 0.04
I0816 10:42:22.274437  4780 caffe.cpp:313] Batch 781, accuracy/top5 = 0.08
I0816 10:42:22.274441  4780 caffe.cpp:313] Batch 781, loss = 6.65152
I0816 10:42:22.337469  4780 caffe.cpp:313] Batch 782, accuracy/top1 = 0
I0816 10:42:22.337491  4780 caffe.cpp:313] Batch 782, accuracy/top5 = 0.06
I0816 10:42:22.337496  4780 caffe.cpp:313] Batch 782, loss = 6.56019
I0816 10:42:22.400532  4780 caffe.cpp:313] Batch 783, accuracy/top1 = 0
I0816 10:42:22.400552  4780 caffe.cpp:313] Batch 783, accuracy/top5 = 0.04
I0816 10:42:22.400555  4780 caffe.cpp:313] Batch 783, loss = 6.76032
I0816 10:42:22.463567  4780 caffe.cpp:313] Batch 784, accuracy/top1 = 0
I0816 10:42:22.463587  4780 caffe.cpp:313] Batch 784, accuracy/top5 = 0.02
I0816 10:42:22.463591  4780 caffe.cpp:313] Batch 784, loss = 6.77156
I0816 10:42:22.526551  4780 caffe.cpp:313] Batch 785, accuracy/top1 = 0
I0816 10:42:22.526571  4780 caffe.cpp:313] Batch 785, accuracy/top5 = 0.02
I0816 10:42:22.526576  4780 caffe.cpp:313] Batch 785, loss = 6.81845
I0816 10:42:22.589717  4780 caffe.cpp:313] Batch 786, accuracy/top1 = 0.04
I0816 10:42:22.589752  4780 caffe.cpp:313] Batch 786, accuracy/top5 = 0.12
I0816 10:42:22.589756  4780 caffe.cpp:313] Batch 786, loss = 6.45202
I0816 10:42:22.652739  4780 caffe.cpp:313] Batch 787, accuracy/top1 = 0
I0816 10:42:22.652760  4780 caffe.cpp:313] Batch 787, accuracy/top5 = 0.06
I0816 10:42:22.652765  4780 caffe.cpp:313] Batch 787, loss = 6.58697
I0816 10:42:22.715754  4780 caffe.cpp:313] Batch 788, accuracy/top1 = 0
I0816 10:42:22.715775  4780 caffe.cpp:313] Batch 788, accuracy/top5 = 0.02
I0816 10:42:22.715778  4780 caffe.cpp:313] Batch 788, loss = 6.8556
I0816 10:42:22.778726  4780 caffe.cpp:313] Batch 789, accuracy/top1 = 0.04
I0816 10:42:22.778749  4780 caffe.cpp:313] Batch 789, accuracy/top5 = 0.04
I0816 10:42:22.778753  4780 caffe.cpp:313] Batch 789, loss = 6.80304
I0816 10:42:22.841639  4780 caffe.cpp:313] Batch 790, accuracy/top1 = 0.04
I0816 10:42:22.841661  4780 caffe.cpp:313] Batch 790, accuracy/top5 = 0.1
I0816 10:42:22.841665  4780 caffe.cpp:313] Batch 790, loss = 6.71333
I0816 10:42:22.904574  4780 caffe.cpp:313] Batch 791, accuracy/top1 = 0
I0816 10:42:22.904594  4780 caffe.cpp:313] Batch 791, accuracy/top5 = 0
I0816 10:42:22.904598  4780 caffe.cpp:313] Batch 791, loss = 6.83575
I0816 10:42:22.967485  4780 caffe.cpp:313] Batch 792, accuracy/top1 = 0
I0816 10:42:22.967506  4780 caffe.cpp:313] Batch 792, accuracy/top5 = 0
I0816 10:42:22.967510  4780 caffe.cpp:313] Batch 792, loss = 6.72532
I0816 10:42:23.030699  4780 caffe.cpp:313] Batch 793, accuracy/top1 = 0
I0816 10:42:23.030716  4780 caffe.cpp:313] Batch 793, accuracy/top5 = 0.04
I0816 10:42:23.030720  4780 caffe.cpp:313] Batch 793, loss = 6.63729
I0816 10:42:23.093540  4780 caffe.cpp:313] Batch 794, accuracy/top1 = 0.02
I0816 10:42:23.093561  4780 caffe.cpp:313] Batch 794, accuracy/top5 = 0.04
I0816 10:42:23.093565  4780 caffe.cpp:313] Batch 794, loss = 6.56656
I0816 10:42:23.156420  4780 caffe.cpp:313] Batch 795, accuracy/top1 = 0.02
I0816 10:42:23.156442  4780 caffe.cpp:313] Batch 795, accuracy/top5 = 0.04
I0816 10:42:23.156446  4780 caffe.cpp:313] Batch 795, loss = 6.65747
I0816 10:42:23.220398  4780 caffe.cpp:313] Batch 796, accuracy/top1 = 0
I0816 10:42:23.220424  4780 caffe.cpp:313] Batch 796, accuracy/top5 = 0
I0816 10:42:23.220429  4780 caffe.cpp:313] Batch 796, loss = 6.85313
I0816 10:42:23.283344  4780 caffe.cpp:313] Batch 797, accuracy/top1 = 0
I0816 10:42:23.283365  4780 caffe.cpp:313] Batch 797, accuracy/top5 = 0.04
I0816 10:42:23.283368  4780 caffe.cpp:313] Batch 797, loss = 6.52623
I0816 10:42:23.346407  4780 caffe.cpp:313] Batch 798, accuracy/top1 = 0
I0816 10:42:23.346428  4780 caffe.cpp:313] Batch 798, accuracy/top5 = 0.04
I0816 10:42:23.346431  4780 caffe.cpp:313] Batch 798, loss = 6.88593
I0816 10:42:23.409272  4780 caffe.cpp:313] Batch 799, accuracy/top1 = 0
I0816 10:42:23.409293  4780 caffe.cpp:313] Batch 799, accuracy/top5 = 0.02
I0816 10:42:23.409296  4780 caffe.cpp:313] Batch 799, loss = 6.82543
I0816 10:42:23.472309  4780 caffe.cpp:313] Batch 800, accuracy/top1 = 0.02
I0816 10:42:23.472332  4780 caffe.cpp:313] Batch 800, accuracy/top5 = 0.02
I0816 10:42:23.472334  4780 caffe.cpp:313] Batch 800, loss = 6.8977
I0816 10:42:23.535151  4780 caffe.cpp:313] Batch 801, accuracy/top1 = 0
I0816 10:42:23.535171  4780 caffe.cpp:313] Batch 801, accuracy/top5 = 0
I0816 10:42:23.535173  4780 caffe.cpp:313] Batch 801, loss = 6.81941
I0816 10:42:23.598428  4780 caffe.cpp:313] Batch 802, accuracy/top1 = 0.02
I0816 10:42:23.598446  4780 caffe.cpp:313] Batch 802, accuracy/top5 = 0.04
I0816 10:42:23.598449  4780 caffe.cpp:313] Batch 802, loss = 6.81188
I0816 10:42:23.661290  4780 caffe.cpp:313] Batch 803, accuracy/top1 = 0.02
I0816 10:42:23.661311  4780 caffe.cpp:313] Batch 803, accuracy/top5 = 0.04
I0816 10:42:23.661314  4780 caffe.cpp:313] Batch 803, loss = 6.74244
I0816 10:42:23.724232  4780 caffe.cpp:313] Batch 804, accuracy/top1 = 0
I0816 10:42:23.724254  4780 caffe.cpp:313] Batch 804, accuracy/top5 = 0.02
I0816 10:42:23.724258  4780 caffe.cpp:313] Batch 804, loss = 6.87173
I0816 10:42:23.787070  4780 caffe.cpp:313] Batch 805, accuracy/top1 = 0
I0816 10:42:23.787102  4780 caffe.cpp:313] Batch 805, accuracy/top5 = 0.04
I0816 10:42:23.787106  4780 caffe.cpp:313] Batch 805, loss = 6.77714
I0816 10:42:23.849946  4780 caffe.cpp:313] Batch 806, accuracy/top1 = 0
I0816 10:42:23.849967  4780 caffe.cpp:313] Batch 806, accuracy/top5 = 0
I0816 10:42:23.849970  4780 caffe.cpp:313] Batch 806, loss = 7.02738
I0816 10:42:23.912973  4780 caffe.cpp:313] Batch 807, accuracy/top1 = 0
I0816 10:42:23.912994  4780 caffe.cpp:313] Batch 807, accuracy/top5 = 0
I0816 10:42:23.912997  4780 caffe.cpp:313] Batch 807, loss = 6.88253
I0816 10:42:23.975976  4780 caffe.cpp:313] Batch 808, accuracy/top1 = 0
I0816 10:42:23.975997  4780 caffe.cpp:313] Batch 808, accuracy/top5 = 0
I0816 10:42:23.976001  4780 caffe.cpp:313] Batch 808, loss = 6.90523
I0816 10:42:24.038926  4780 caffe.cpp:313] Batch 809, accuracy/top1 = 0.02
I0816 10:42:24.038944  4780 caffe.cpp:313] Batch 809, accuracy/top5 = 0.06
I0816 10:42:24.038947  4780 caffe.cpp:313] Batch 809, loss = 6.90405
I0816 10:42:24.101903  4780 caffe.cpp:313] Batch 810, accuracy/top1 = 0.02
I0816 10:42:24.101924  4780 caffe.cpp:313] Batch 810, accuracy/top5 = 0.02
I0816 10:42:24.101928  4780 caffe.cpp:313] Batch 810, loss = 6.7728
I0816 10:42:24.164798  4780 caffe.cpp:313] Batch 811, accuracy/top1 = 0
I0816 10:42:24.164819  4780 caffe.cpp:313] Batch 811, accuracy/top5 = 0
I0816 10:42:24.164822  4780 caffe.cpp:313] Batch 811, loss = 6.84297
I0816 10:42:24.227710  4780 caffe.cpp:313] Batch 812, accuracy/top1 = 0
I0816 10:42:24.227732  4780 caffe.cpp:313] Batch 812, accuracy/top5 = 0
I0816 10:42:24.227735  4780 caffe.cpp:313] Batch 812, loss = 6.83528
I0816 10:42:24.290675  4780 caffe.cpp:313] Batch 813, accuracy/top1 = 0
I0816 10:42:24.290696  4780 caffe.cpp:313] Batch 813, accuracy/top5 = 0.02
I0816 10:42:24.290700  4780 caffe.cpp:313] Batch 813, loss = 6.85066
I0816 10:42:24.353618  4780 caffe.cpp:313] Batch 814, accuracy/top1 = 0
I0816 10:42:24.353641  4780 caffe.cpp:313] Batch 814, accuracy/top5 = 0.02
I0816 10:42:24.353644  4780 caffe.cpp:313] Batch 814, loss = 6.81777
I0816 10:42:24.416558  4780 caffe.cpp:313] Batch 815, accuracy/top1 = 0.04
I0816 10:42:24.416579  4780 caffe.cpp:313] Batch 815, accuracy/top5 = 0.06
I0816 10:42:24.416581  4780 caffe.cpp:313] Batch 815, loss = 6.60046
I0816 10:42:24.479427  4780 caffe.cpp:313] Batch 816, accuracy/top1 = 0
I0816 10:42:24.479446  4780 caffe.cpp:313] Batch 816, accuracy/top5 = 0.06
I0816 10:42:24.479450  4780 caffe.cpp:313] Batch 816, loss = 6.62951
I0816 10:42:24.542539  4780 caffe.cpp:313] Batch 817, accuracy/top1 = 0
I0816 10:42:24.542558  4780 caffe.cpp:313] Batch 817, accuracy/top5 = 0
I0816 10:42:24.542562  4780 caffe.cpp:313] Batch 817, loss = 6.94833
I0816 10:42:24.605715  4780 caffe.cpp:313] Batch 818, accuracy/top1 = 0
I0816 10:42:24.605734  4780 caffe.cpp:313] Batch 818, accuracy/top5 = 0.04
I0816 10:42:24.605738  4780 caffe.cpp:313] Batch 818, loss = 6.83626
I0816 10:42:24.668725  4780 caffe.cpp:313] Batch 819, accuracy/top1 = 0
I0816 10:42:24.668747  4780 caffe.cpp:313] Batch 819, accuracy/top5 = 0.02
I0816 10:42:24.668751  4780 caffe.cpp:313] Batch 819, loss = 6.9168
I0816 10:42:24.731766  4780 caffe.cpp:313] Batch 820, accuracy/top1 = 0
I0816 10:42:24.731788  4780 caffe.cpp:313] Batch 820, accuracy/top5 = 0
I0816 10:42:24.731792  4780 caffe.cpp:313] Batch 820, loss = 6.83218
I0816 10:42:24.794633  4780 caffe.cpp:313] Batch 821, accuracy/top1 = 0.02
I0816 10:42:24.794654  4780 caffe.cpp:313] Batch 821, accuracy/top5 = 0.06
I0816 10:42:24.794658  4780 caffe.cpp:313] Batch 821, loss = 6.73381
I0816 10:42:24.857633  4780 caffe.cpp:313] Batch 822, accuracy/top1 = 0
I0816 10:42:24.857653  4780 caffe.cpp:313] Batch 822, accuracy/top5 = 0.06
I0816 10:42:24.857657  4780 caffe.cpp:313] Batch 822, loss = 6.69377
I0816 10:42:24.920666  4780 caffe.cpp:313] Batch 823, accuracy/top1 = 0.04
I0816 10:42:24.920687  4780 caffe.cpp:313] Batch 823, accuracy/top5 = 0.08
I0816 10:42:24.920691  4780 caffe.cpp:313] Batch 823, loss = 6.75275
I0816 10:42:24.983489  4780 caffe.cpp:313] Batch 824, accuracy/top1 = 0
I0816 10:42:24.983516  4780 caffe.cpp:313] Batch 824, accuracy/top5 = 0
I0816 10:42:24.983520  4780 caffe.cpp:313] Batch 824, loss = 7.02938
I0816 10:42:25.046636  4780 caffe.cpp:313] Batch 825, accuracy/top1 = 0
I0816 10:42:25.046653  4780 caffe.cpp:313] Batch 825, accuracy/top5 = 0.06
I0816 10:42:25.046656  4780 caffe.cpp:313] Batch 825, loss = 6.82749
I0816 10:42:25.109580  4780 caffe.cpp:313] Batch 826, accuracy/top1 = 0.02
I0816 10:42:25.109601  4780 caffe.cpp:313] Batch 826, accuracy/top5 = 0.02
I0816 10:42:25.109604  4780 caffe.cpp:313] Batch 826, loss = 7.00685
I0816 10:42:25.172487  4780 caffe.cpp:313] Batch 827, accuracy/top1 = 0
I0816 10:42:25.172508  4780 caffe.cpp:313] Batch 827, accuracy/top5 = 0.02
I0816 10:42:25.172510  4780 caffe.cpp:313] Batch 827, loss = 6.92516
I0816 10:42:25.235394  4780 caffe.cpp:313] Batch 828, accuracy/top1 = 0
I0816 10:42:25.235415  4780 caffe.cpp:313] Batch 828, accuracy/top5 = 0
I0816 10:42:25.235419  4780 caffe.cpp:313] Batch 828, loss = 6.80029
I0816 10:42:25.298367  4780 caffe.cpp:313] Batch 829, accuracy/top1 = 0
I0816 10:42:25.298388  4780 caffe.cpp:313] Batch 829, accuracy/top5 = 0.08
I0816 10:42:25.298391  4780 caffe.cpp:313] Batch 829, loss = 6.64363
I0816 10:42:25.361155  4780 caffe.cpp:313] Batch 830, accuracy/top1 = 0
I0816 10:42:25.361176  4780 caffe.cpp:313] Batch 830, accuracy/top5 = 0.04
I0816 10:42:25.361179  4780 caffe.cpp:313] Batch 830, loss = 6.71215
I0816 10:42:25.424147  4780 caffe.cpp:313] Batch 831, accuracy/top1 = 0.02
I0816 10:42:25.424168  4780 caffe.cpp:313] Batch 831, accuracy/top5 = 0.04
I0816 10:42:25.424171  4780 caffe.cpp:313] Batch 831, loss = 6.58685
I0816 10:42:25.487195  4780 caffe.cpp:313] Batch 832, accuracy/top1 = 0
I0816 10:42:25.487215  4780 caffe.cpp:313] Batch 832, accuracy/top5 = 0
I0816 10:42:25.487218  4780 caffe.cpp:313] Batch 832, loss = 6.96297
I0816 10:42:25.550150  4780 caffe.cpp:313] Batch 833, accuracy/top1 = 0
I0816 10:42:25.550171  4780 caffe.cpp:313] Batch 833, accuracy/top5 = 0
I0816 10:42:25.550174  4780 caffe.cpp:313] Batch 833, loss = 6.76939
I0816 10:42:25.613370  4780 caffe.cpp:313] Batch 834, accuracy/top1 = 0.02
I0816 10:42:25.613387  4780 caffe.cpp:313] Batch 834, accuracy/top5 = 0.18
I0816 10:42:25.613390  4780 caffe.cpp:313] Batch 834, loss = 6.34124
I0816 10:42:25.676246  4780 caffe.cpp:313] Batch 835, accuracy/top1 = 0.02
I0816 10:42:25.676268  4780 caffe.cpp:313] Batch 835, accuracy/top5 = 0.04
I0816 10:42:25.676272  4780 caffe.cpp:313] Batch 835, loss = 6.62782
I0816 10:42:25.739298  4780 caffe.cpp:313] Batch 836, accuracy/top1 = 0
I0816 10:42:25.739320  4780 caffe.cpp:313] Batch 836, accuracy/top5 = 0.04
I0816 10:42:25.739325  4780 caffe.cpp:313] Batch 836, loss = 6.84232
I0816 10:42:25.802125  4780 caffe.cpp:313] Batch 837, accuracy/top1 = 0.04
I0816 10:42:25.802146  4780 caffe.cpp:313] Batch 837, accuracy/top5 = 0.04
I0816 10:42:25.802150  4780 caffe.cpp:313] Batch 837, loss = 6.68059
I0816 10:42:25.865052  4780 caffe.cpp:313] Batch 838, accuracy/top1 = 0.04
I0816 10:42:25.865072  4780 caffe.cpp:313] Batch 838, accuracy/top5 = 0.06
I0816 10:42:25.865077  4780 caffe.cpp:313] Batch 838, loss = 6.71123
I0816 10:42:25.927878  4780 caffe.cpp:313] Batch 839, accuracy/top1 = 0
I0816 10:42:25.927899  4780 caffe.cpp:313] Batch 839, accuracy/top5 = 0
I0816 10:42:25.927902  4780 caffe.cpp:313] Batch 839, loss = 7.0263
I0816 10:42:25.991255  4780 caffe.cpp:313] Batch 840, accuracy/top1 = 0
I0816 10:42:25.991279  4780 caffe.cpp:313] Batch 840, accuracy/top5 = 0.02
I0816 10:42:25.991282  4780 caffe.cpp:313] Batch 840, loss = 6.87271
I0816 10:42:26.054383  4780 caffe.cpp:313] Batch 841, accuracy/top1 = 0
I0816 10:42:26.054401  4780 caffe.cpp:313] Batch 841, accuracy/top5 = 0
I0816 10:42:26.054404  4780 caffe.cpp:313] Batch 841, loss = 6.94401
I0816 10:42:26.117256  4780 caffe.cpp:313] Batch 842, accuracy/top1 = 0.04
I0816 10:42:26.117277  4780 caffe.cpp:313] Batch 842, accuracy/top5 = 0.1
I0816 10:42:26.117280  4780 caffe.cpp:313] Batch 842, loss = 6.54023
I0816 10:42:26.180156  4780 caffe.cpp:313] Batch 843, accuracy/top1 = 0.02
I0816 10:42:26.180191  4780 caffe.cpp:313] Batch 843, accuracy/top5 = 0.04
I0816 10:42:26.180194  4780 caffe.cpp:313] Batch 843, loss = 6.83176
I0816 10:42:26.242977  4780 caffe.cpp:313] Batch 844, accuracy/top1 = 0
I0816 10:42:26.242998  4780 caffe.cpp:313] Batch 844, accuracy/top5 = 0.02
I0816 10:42:26.243002  4780 caffe.cpp:313] Batch 844, loss = 6.7773
I0816 10:42:26.305811  4780 caffe.cpp:313] Batch 845, accuracy/top1 = 0.06
I0816 10:42:26.305832  4780 caffe.cpp:313] Batch 845, accuracy/top5 = 0.06
I0816 10:42:26.305835  4780 caffe.cpp:313] Batch 845, loss = 6.22439
I0816 10:42:26.368628  4780 caffe.cpp:313] Batch 846, accuracy/top1 = 0
I0816 10:42:26.368649  4780 caffe.cpp:313] Batch 846, accuracy/top5 = 0.04
I0816 10:42:26.368652  4780 caffe.cpp:313] Batch 846, loss = 6.79015
I0816 10:42:26.431655  4780 caffe.cpp:313] Batch 847, accuracy/top1 = 0
I0816 10:42:26.431676  4780 caffe.cpp:313] Batch 847, accuracy/top5 = 0.06
I0816 10:42:26.431679  4780 caffe.cpp:313] Batch 847, loss = 6.48812
I0816 10:42:26.494585  4780 caffe.cpp:313] Batch 848, accuracy/top1 = 0.02
I0816 10:42:26.494604  4780 caffe.cpp:313] Batch 848, accuracy/top5 = 0.06
I0816 10:42:26.494607  4780 caffe.cpp:313] Batch 848, loss = 6.77217
I0816 10:42:26.557430  4780 caffe.cpp:313] Batch 849, accuracy/top1 = 0.04
I0816 10:42:26.557448  4780 caffe.cpp:313] Batch 849, accuracy/top5 = 0.04
I0816 10:42:26.557451  4780 caffe.cpp:313] Batch 849, loss = 6.83813
I0816 10:42:26.620530  4780 caffe.cpp:313] Batch 850, accuracy/top1 = 0
I0816 10:42:26.620549  4780 caffe.cpp:313] Batch 850, accuracy/top5 = 0.06
I0816 10:42:26.620553  4780 caffe.cpp:313] Batch 850, loss = 6.87646
I0816 10:42:26.683702  4780 caffe.cpp:313] Batch 851, accuracy/top1 = 0
I0816 10:42:26.683722  4780 caffe.cpp:313] Batch 851, accuracy/top5 = 0
I0816 10:42:26.683724  4780 caffe.cpp:313] Batch 851, loss = 6.74998
I0816 10:42:26.746815  4780 caffe.cpp:313] Batch 852, accuracy/top1 = 0
I0816 10:42:26.746835  4780 caffe.cpp:313] Batch 852, accuracy/top5 = 0.02
I0816 10:42:26.746840  4780 caffe.cpp:313] Batch 852, loss = 6.78105
I0816 10:42:26.809691  4780 caffe.cpp:313] Batch 853, accuracy/top1 = 0
I0816 10:42:26.809713  4780 caffe.cpp:313] Batch 853, accuracy/top5 = 0.02
I0816 10:42:26.809716  4780 caffe.cpp:313] Batch 853, loss = 6.89128
I0816 10:42:26.872812  4780 caffe.cpp:313] Batch 854, accuracy/top1 = 0.02
I0816 10:42:26.872833  4780 caffe.cpp:313] Batch 854, accuracy/top5 = 0.08
I0816 10:42:26.872836  4780 caffe.cpp:313] Batch 854, loss = 6.29266
I0816 10:42:26.935685  4780 caffe.cpp:313] Batch 855, accuracy/top1 = 0.04
I0816 10:42:26.935708  4780 caffe.cpp:313] Batch 855, accuracy/top5 = 0.06
I0816 10:42:26.935710  4780 caffe.cpp:313] Batch 855, loss = 6.66851
I0816 10:42:26.998688  4780 caffe.cpp:313] Batch 856, accuracy/top1 = 0.02
I0816 10:42:26.998709  4780 caffe.cpp:313] Batch 856, accuracy/top5 = 0.04
I0816 10:42:26.998713  4780 caffe.cpp:313] Batch 856, loss = 6.45687
I0816 10:42:27.061561  4780 caffe.cpp:313] Batch 857, accuracy/top1 = 0.02
I0816 10:42:27.061583  4780 caffe.cpp:313] Batch 857, accuracy/top5 = 0.08
I0816 10:42:27.061585  4780 caffe.cpp:313] Batch 857, loss = 6.5188
I0816 10:42:27.124374  4780 caffe.cpp:313] Batch 858, accuracy/top1 = 0.02
I0816 10:42:27.124395  4780 caffe.cpp:313] Batch 858, accuracy/top5 = 0.06
I0816 10:42:27.124399  4780 caffe.cpp:313] Batch 858, loss = 6.67326
I0816 10:42:27.187337  4780 caffe.cpp:313] Batch 859, accuracy/top1 = 0
I0816 10:42:27.187358  4780 caffe.cpp:313] Batch 859, accuracy/top5 = 0.04
I0816 10:42:27.187361  4780 caffe.cpp:313] Batch 859, loss = 6.91425
I0816 10:42:27.250226  4780 caffe.cpp:313] Batch 860, accuracy/top1 = 0.02
I0816 10:42:27.250247  4780 caffe.cpp:313] Batch 860, accuracy/top5 = 0.04
I0816 10:42:27.250250  4780 caffe.cpp:313] Batch 860, loss = 6.66113
I0816 10:42:27.313072  4780 caffe.cpp:313] Batch 861, accuracy/top1 = 0.02
I0816 10:42:27.313093  4780 caffe.cpp:313] Batch 861, accuracy/top5 = 0.12
I0816 10:42:27.313097  4780 caffe.cpp:313] Batch 861, loss = 6.45314
I0816 10:42:27.375933  4780 caffe.cpp:313] Batch 862, accuracy/top1 = 0
I0816 10:42:27.375953  4780 caffe.cpp:313] Batch 862, accuracy/top5 = 0
I0816 10:42:27.375957  4780 caffe.cpp:313] Batch 862, loss = 6.8842
I0816 10:42:27.438930  4780 caffe.cpp:313] Batch 863, accuracy/top1 = 0
I0816 10:42:27.438951  4780 caffe.cpp:313] Batch 863, accuracy/top5 = 0.04
I0816 10:42:27.438953  4780 caffe.cpp:313] Batch 863, loss = 6.48555
I0816 10:42:27.501739  4780 caffe.cpp:313] Batch 864, accuracy/top1 = 0
I0816 10:42:27.501761  4780 caffe.cpp:313] Batch 864, accuracy/top5 = 0.02
I0816 10:42:27.501765  4780 caffe.cpp:313] Batch 864, loss = 6.53293
I0816 10:42:27.564705  4780 caffe.cpp:313] Batch 865, accuracy/top1 = 0
I0816 10:42:27.564726  4780 caffe.cpp:313] Batch 865, accuracy/top5 = 0
I0816 10:42:27.564729  4780 caffe.cpp:313] Batch 865, loss = 6.94082
I0816 10:42:27.627746  4780 caffe.cpp:313] Batch 866, accuracy/top1 = 0.02
I0816 10:42:27.627763  4780 caffe.cpp:313] Batch 866, accuracy/top5 = 0.02
I0816 10:42:27.627768  4780 caffe.cpp:313] Batch 866, loss = 6.96305
I0816 10:42:27.690663  4780 caffe.cpp:313] Batch 867, accuracy/top1 = 0
I0816 10:42:27.690685  4780 caffe.cpp:313] Batch 867, accuracy/top5 = 0
I0816 10:42:27.690688  4780 caffe.cpp:313] Batch 867, loss = 6.88446
I0816 10:42:27.753690  4780 caffe.cpp:313] Batch 868, accuracy/top1 = 0
I0816 10:42:27.753711  4780 caffe.cpp:313] Batch 868, accuracy/top5 = 0.06
I0816 10:42:27.753715  4780 caffe.cpp:313] Batch 868, loss = 6.51662
I0816 10:42:27.816540  4780 caffe.cpp:313] Batch 869, accuracy/top1 = 0.02
I0816 10:42:27.816561  4780 caffe.cpp:313] Batch 869, accuracy/top5 = 0.02
I0816 10:42:27.816565  4780 caffe.cpp:313] Batch 869, loss = 6.59509
I0816 10:42:27.879331  4780 caffe.cpp:313] Batch 870, accuracy/top1 = 0
I0816 10:42:27.879353  4780 caffe.cpp:313] Batch 870, accuracy/top5 = 0.02
I0816 10:42:27.879355  4780 caffe.cpp:313] Batch 870, loss = 7.00453
I0816 10:42:27.942302  4780 caffe.cpp:313] Batch 871, accuracy/top1 = 0
I0816 10:42:27.942323  4780 caffe.cpp:313] Batch 871, accuracy/top5 = 0.04
I0816 10:42:27.942327  4780 caffe.cpp:313] Batch 871, loss = 6.70952
I0816 10:42:28.005483  4780 caffe.cpp:313] Batch 872, accuracy/top1 = 0.02
I0816 10:42:28.005501  4780 caffe.cpp:313] Batch 872, accuracy/top5 = 0.04
I0816 10:42:28.005503  4780 caffe.cpp:313] Batch 872, loss = 6.71516
I0816 10:42:28.068517  4780 caffe.cpp:313] Batch 873, accuracy/top1 = 0.02
I0816 10:42:28.068538  4780 caffe.cpp:313] Batch 873, accuracy/top5 = 0.02
I0816 10:42:28.068542  4780 caffe.cpp:313] Batch 873, loss = 7.08485
I0816 10:42:28.131340  4780 caffe.cpp:313] Batch 874, accuracy/top1 = 0
I0816 10:42:28.131359  4780 caffe.cpp:313] Batch 874, accuracy/top5 = 0
I0816 10:42:28.131362  4780 caffe.cpp:313] Batch 874, loss = 6.81517
I0816 10:42:28.194296  4780 caffe.cpp:313] Batch 875, accuracy/top1 = 0
I0816 10:42:28.194317  4780 caffe.cpp:313] Batch 875, accuracy/top5 = 0.02
I0816 10:42:28.194321  4780 caffe.cpp:313] Batch 875, loss = 6.89531
I0816 10:42:28.257182  4780 caffe.cpp:313] Batch 876, accuracy/top1 = 0
I0816 10:42:28.257203  4780 caffe.cpp:313] Batch 876, accuracy/top5 = 0.02
I0816 10:42:28.257207  4780 caffe.cpp:313] Batch 876, loss = 6.88042
I0816 10:42:28.320163  4780 caffe.cpp:313] Batch 877, accuracy/top1 = 0.04
I0816 10:42:28.320183  4780 caffe.cpp:313] Batch 877, accuracy/top5 = 0.04
I0816 10:42:28.320186  4780 caffe.cpp:313] Batch 877, loss = 6.79281
I0816 10:42:28.383049  4780 caffe.cpp:313] Batch 878, accuracy/top1 = 0.02
I0816 10:42:28.383071  4780 caffe.cpp:313] Batch 878, accuracy/top5 = 0.04
I0816 10:42:28.383074  4780 caffe.cpp:313] Batch 878, loss = 6.62643
I0816 10:42:28.445940  4780 caffe.cpp:313] Batch 879, accuracy/top1 = 0.04
I0816 10:42:28.445961  4780 caffe.cpp:313] Batch 879, accuracy/top5 = 0.06
I0816 10:42:28.445966  4780 caffe.cpp:313] Batch 879, loss = 6.72904
I0816 10:42:28.508936  4780 caffe.cpp:313] Batch 880, accuracy/top1 = 0.04
I0816 10:42:28.508957  4780 caffe.cpp:313] Batch 880, accuracy/top5 = 0.08
I0816 10:42:28.508960  4780 caffe.cpp:313] Batch 880, loss = 6.56981
I0816 10:42:28.571825  4780 caffe.cpp:313] Batch 881, accuracy/top1 = 0.06
I0816 10:42:28.571846  4780 caffe.cpp:313] Batch 881, accuracy/top5 = 0.06
I0816 10:42:28.571849  4780 caffe.cpp:313] Batch 881, loss = 6.41708
I0816 10:42:28.634708  4780 caffe.cpp:313] Batch 882, accuracy/top1 = 0
I0816 10:42:28.634730  4780 caffe.cpp:313] Batch 882, accuracy/top5 = 0.04
I0816 10:42:28.634733  4780 caffe.cpp:313] Batch 882, loss = 6.64185
I0816 10:42:28.697686  4780 caffe.cpp:313] Batch 883, accuracy/top1 = 0
I0816 10:42:28.697707  4780 caffe.cpp:313] Batch 883, accuracy/top5 = 0.02
I0816 10:42:28.697711  4780 caffe.cpp:313] Batch 883, loss = 7.04833
I0816 10:42:28.760680  4780 caffe.cpp:313] Batch 884, accuracy/top1 = 0
I0816 10:42:28.760699  4780 caffe.cpp:313] Batch 884, accuracy/top5 = 0
I0816 10:42:28.760702  4780 caffe.cpp:313] Batch 884, loss = 6.6703
I0816 10:42:28.823849  4780 caffe.cpp:313] Batch 885, accuracy/top1 = 0.02
I0816 10:42:28.823868  4780 caffe.cpp:313] Batch 885, accuracy/top5 = 0.02
I0816 10:42:28.823870  4780 caffe.cpp:313] Batch 885, loss = 6.93325
I0816 10:42:28.886945  4780 caffe.cpp:313] Batch 886, accuracy/top1 = 0.08
I0816 10:42:28.886965  4780 caffe.cpp:313] Batch 886, accuracy/top5 = 0.16
I0816 10:42:28.886968  4780 caffe.cpp:313] Batch 886, loss = 6.19512
I0816 10:42:28.949900  4780 caffe.cpp:313] Batch 887, accuracy/top1 = 0
I0816 10:42:28.949920  4780 caffe.cpp:313] Batch 887, accuracy/top5 = 0.04
I0816 10:42:28.949924  4780 caffe.cpp:313] Batch 887, loss = 6.75307
I0816 10:42:29.012715  4780 caffe.cpp:313] Batch 888, accuracy/top1 = 0
I0816 10:42:29.012737  4780 caffe.cpp:313] Batch 888, accuracy/top5 = 0
I0816 10:42:29.012739  4780 caffe.cpp:313] Batch 888, loss = 6.81502
I0816 10:42:29.075590  4780 caffe.cpp:313] Batch 889, accuracy/top1 = 0.04
I0816 10:42:29.075611  4780 caffe.cpp:313] Batch 889, accuracy/top5 = 0.06
I0816 10:42:29.075614  4780 caffe.cpp:313] Batch 889, loss = 6.54071
I0816 10:42:29.138494  4780 caffe.cpp:313] Batch 890, accuracy/top1 = 0
I0816 10:42:29.138515  4780 caffe.cpp:313] Batch 890, accuracy/top5 = 0.04
I0816 10:42:29.138519  4780 caffe.cpp:313] Batch 890, loss = 7.01914
I0816 10:42:29.201436  4780 caffe.cpp:313] Batch 891, accuracy/top1 = 0
I0816 10:42:29.201457  4780 caffe.cpp:313] Batch 891, accuracy/top5 = 0.02
I0816 10:42:29.201459  4780 caffe.cpp:313] Batch 891, loss = 6.80313
I0816 10:42:29.264312  4780 caffe.cpp:313] Batch 892, accuracy/top1 = 0
I0816 10:42:29.264333  4780 caffe.cpp:313] Batch 892, accuracy/top5 = 0.04
I0816 10:42:29.264336  4780 caffe.cpp:313] Batch 892, loss = 6.78325
I0816 10:42:29.327193  4780 caffe.cpp:313] Batch 893, accuracy/top1 = 0.02
I0816 10:42:29.327214  4780 caffe.cpp:313] Batch 893, accuracy/top5 = 0.02
I0816 10:42:29.327217  4780 caffe.cpp:313] Batch 893, loss = 6.59266
I0816 10:42:29.390192  4780 caffe.cpp:313] Batch 894, accuracy/top1 = 0
I0816 10:42:29.390213  4780 caffe.cpp:313] Batch 894, accuracy/top5 = 0
I0816 10:42:29.390216  4780 caffe.cpp:313] Batch 894, loss = 6.75126
I0816 10:42:29.453158  4780 caffe.cpp:313] Batch 895, accuracy/top1 = 0.02
I0816 10:42:29.453179  4780 caffe.cpp:313] Batch 895, accuracy/top5 = 0.04
I0816 10:42:29.453183  4780 caffe.cpp:313] Batch 895, loss = 6.67026
I0816 10:42:29.516019  4780 caffe.cpp:313] Batch 896, accuracy/top1 = 0.02
I0816 10:42:29.516041  4780 caffe.cpp:313] Batch 896, accuracy/top5 = 0.02
I0816 10:42:29.516043  4780 caffe.cpp:313] Batch 896, loss = 6.92885
I0816 10:42:29.578944  4780 caffe.cpp:313] Batch 897, accuracy/top1 = 0.04
I0816 10:42:29.578966  4780 caffe.cpp:313] Batch 897, accuracy/top5 = 0.08
I0816 10:42:29.578969  4780 caffe.cpp:313] Batch 897, loss = 6.57505
I0816 10:42:29.641965  4780 caffe.cpp:313] Batch 898, accuracy/top1 = 0
I0816 10:42:29.641988  4780 caffe.cpp:313] Batch 898, accuracy/top5 = 0.04
I0816 10:42:29.641991  4780 caffe.cpp:313] Batch 898, loss = 6.78271
I0816 10:42:29.705044  4780 caffe.cpp:313] Batch 899, accuracy/top1 = 0.02
I0816 10:42:29.705063  4780 caffe.cpp:313] Batch 899, accuracy/top5 = 0.04
I0816 10:42:29.705066  4780 caffe.cpp:313] Batch 899, loss = 6.91307
I0816 10:42:29.768044  4780 caffe.cpp:313] Batch 900, accuracy/top1 = 0.02
I0816 10:42:29.768065  4780 caffe.cpp:313] Batch 900, accuracy/top5 = 0.04
I0816 10:42:29.768069  4780 caffe.cpp:313] Batch 900, loss = 6.63393
I0816 10:42:29.830942  4780 caffe.cpp:313] Batch 901, accuracy/top1 = 0.02
I0816 10:42:29.830963  4780 caffe.cpp:313] Batch 901, accuracy/top5 = 0.06
I0816 10:42:29.830967  4780 caffe.cpp:313] Batch 901, loss = 6.49529
I0816 10:42:29.893920  4780 caffe.cpp:313] Batch 902, accuracy/top1 = 0.06
I0816 10:42:29.893942  4780 caffe.cpp:313] Batch 902, accuracy/top5 = 0.08
I0816 10:42:29.893946  4780 caffe.cpp:313] Batch 902, loss = 6.46957
I0816 10:42:29.956951  4780 caffe.cpp:313] Batch 903, accuracy/top1 = 0.04
I0816 10:42:29.956974  4780 caffe.cpp:313] Batch 903, accuracy/top5 = 0.08
I0816 10:42:29.956976  4780 caffe.cpp:313] Batch 903, loss = 6.52926
I0816 10:42:30.020102  4780 caffe.cpp:313] Batch 904, accuracy/top1 = 0
I0816 10:42:30.020117  4780 caffe.cpp:313] Batch 904, accuracy/top5 = 0.06
I0816 10:42:30.020121  4780 caffe.cpp:313] Batch 904, loss = 6.58988
I0816 10:42:30.083003  4780 caffe.cpp:313] Batch 905, accuracy/top1 = 0.04
I0816 10:42:30.083024  4780 caffe.cpp:313] Batch 905, accuracy/top5 = 0.08
I0816 10:42:30.083026  4780 caffe.cpp:313] Batch 905, loss = 6.53825
I0816 10:42:30.145805  4780 caffe.cpp:313] Batch 906, accuracy/top1 = 0.02
I0816 10:42:30.145828  4780 caffe.cpp:313] Batch 906, accuracy/top5 = 0.04
I0816 10:42:30.145830  4780 caffe.cpp:313] Batch 906, loss = 6.87774
I0816 10:42:30.208714  4780 caffe.cpp:313] Batch 907, accuracy/top1 = 0
I0816 10:42:30.208735  4780 caffe.cpp:313] Batch 907, accuracy/top5 = 0.02
I0816 10:42:30.208739  4780 caffe.cpp:313] Batch 907, loss = 6.78643
I0816 10:42:30.271610  4780 caffe.cpp:313] Batch 908, accuracy/top1 = 0
I0816 10:42:30.271631  4780 caffe.cpp:313] Batch 908, accuracy/top5 = 0.02
I0816 10:42:30.271634  4780 caffe.cpp:313] Batch 908, loss = 6.52174
I0816 10:42:30.334576  4780 caffe.cpp:313] Batch 909, accuracy/top1 = 0.02
I0816 10:42:30.334597  4780 caffe.cpp:313] Batch 909, accuracy/top5 = 0.02
I0816 10:42:30.334600  4780 caffe.cpp:313] Batch 909, loss = 6.70915
I0816 10:42:30.397449  4780 caffe.cpp:313] Batch 910, accuracy/top1 = 0
I0816 10:42:30.397469  4780 caffe.cpp:313] Batch 910, accuracy/top5 = 0.02
I0816 10:42:30.397474  4780 caffe.cpp:313] Batch 910, loss = 6.48009
I0816 10:42:30.460361  4780 caffe.cpp:313] Batch 911, accuracy/top1 = 0
I0816 10:42:30.460381  4780 caffe.cpp:313] Batch 911, accuracy/top5 = 0
I0816 10:42:30.460384  4780 caffe.cpp:313] Batch 911, loss = 6.81485
I0816 10:42:30.523353  4780 caffe.cpp:313] Batch 912, accuracy/top1 = 0.02
I0816 10:42:30.523375  4780 caffe.cpp:313] Batch 912, accuracy/top5 = 0.08
I0816 10:42:30.523377  4780 caffe.cpp:313] Batch 912, loss = 6.37481
I0816 10:42:30.586112  4780 caffe.cpp:313] Batch 913, accuracy/top1 = 0.02
I0816 10:42:30.586133  4780 caffe.cpp:313] Batch 913, accuracy/top5 = 0.06
I0816 10:42:30.586138  4780 caffe.cpp:313] Batch 913, loss = 6.77251
I0816 10:42:30.649096  4780 caffe.cpp:313] Batch 914, accuracy/top1 = 0.02
I0816 10:42:30.649117  4780 caffe.cpp:313] Batch 914, accuracy/top5 = 0.02
I0816 10:42:30.649121  4780 caffe.cpp:313] Batch 914, loss = 6.71611
I0816 10:42:30.711951  4780 caffe.cpp:313] Batch 915, accuracy/top1 = 0
I0816 10:42:30.711971  4780 caffe.cpp:313] Batch 915, accuracy/top5 = 0
I0816 10:42:30.711974  4780 caffe.cpp:313] Batch 915, loss = 6.65095
I0816 10:42:30.774845  4780 caffe.cpp:313] Batch 916, accuracy/top1 = 0.04
I0816 10:42:30.774866  4780 caffe.cpp:313] Batch 916, accuracy/top5 = 0.08
I0816 10:42:30.774869  4780 caffe.cpp:313] Batch 916, loss = 6.4107
I0816 10:42:30.837594  4780 caffe.cpp:313] Batch 917, accuracy/top1 = 0
I0816 10:42:30.837615  4780 caffe.cpp:313] Batch 917, accuracy/top5 = 0.06
I0816 10:42:30.837618  4780 caffe.cpp:313] Batch 917, loss = 6.8017
I0816 10:42:30.900621  4780 caffe.cpp:313] Batch 918, accuracy/top1 = 0
I0816 10:42:30.900640  4780 caffe.cpp:313] Batch 918, accuracy/top5 = 0.02
I0816 10:42:30.900655  4780 caffe.cpp:313] Batch 918, loss = 6.99165
I0816 10:42:30.963598  4780 caffe.cpp:313] Batch 919, accuracy/top1 = 0.06
I0816 10:42:30.963616  4780 caffe.cpp:313] Batch 919, accuracy/top5 = 0.08
I0816 10:42:30.963620  4780 caffe.cpp:313] Batch 919, loss = 6.28064
I0816 10:42:31.026603  4780 caffe.cpp:313] Batch 920, accuracy/top1 = 0
I0816 10:42:31.026620  4780 caffe.cpp:313] Batch 920, accuracy/top5 = 0.04
I0816 10:42:31.026624  4780 caffe.cpp:313] Batch 920, loss = 6.83557
I0816 10:42:31.089536  4780 caffe.cpp:313] Batch 921, accuracy/top1 = 0.02
I0816 10:42:31.089557  4780 caffe.cpp:313] Batch 921, accuracy/top5 = 0.06
I0816 10:42:31.089560  4780 caffe.cpp:313] Batch 921, loss = 6.67749
I0816 10:42:31.152398  4780 caffe.cpp:313] Batch 922, accuracy/top1 = 0
I0816 10:42:31.152418  4780 caffe.cpp:313] Batch 922, accuracy/top5 = 0
I0816 10:42:31.152422  4780 caffe.cpp:313] Batch 922, loss = 6.91467
I0816 10:42:31.215395  4780 caffe.cpp:313] Batch 923, accuracy/top1 = 0
I0816 10:42:31.215415  4780 caffe.cpp:313] Batch 923, accuracy/top5 = 0
I0816 10:42:31.215418  4780 caffe.cpp:313] Batch 923, loss = 6.74619
I0816 10:42:31.278097  4780 caffe.cpp:313] Batch 924, accuracy/top1 = 0.02
I0816 10:42:31.278118  4780 caffe.cpp:313] Batch 924, accuracy/top5 = 0.08
I0816 10:42:31.278121  4780 caffe.cpp:313] Batch 924, loss = 6.81697
I0816 10:42:31.341084  4780 caffe.cpp:313] Batch 925, accuracy/top1 = 0.04
I0816 10:42:31.341106  4780 caffe.cpp:313] Batch 925, accuracy/top5 = 0.06
I0816 10:42:31.341109  4780 caffe.cpp:313] Batch 925, loss = 6.89004
I0816 10:42:31.404060  4780 caffe.cpp:313] Batch 926, accuracy/top1 = 0
I0816 10:42:31.404080  4780 caffe.cpp:313] Batch 926, accuracy/top5 = 0
I0816 10:42:31.404083  4780 caffe.cpp:313] Batch 926, loss = 6.75945
I0816 10:42:31.466868  4780 caffe.cpp:313] Batch 927, accuracy/top1 = 0
I0816 10:42:31.466889  4780 caffe.cpp:313] Batch 927, accuracy/top5 = 0.02
I0816 10:42:31.466893  4780 caffe.cpp:313] Batch 927, loss = 6.95287
I0816 10:42:31.529793  4780 caffe.cpp:313] Batch 928, accuracy/top1 = 0.06
I0816 10:42:31.529814  4780 caffe.cpp:313] Batch 928, accuracy/top5 = 0.08
I0816 10:42:31.529817  4780 caffe.cpp:313] Batch 928, loss = 6.31633
I0816 10:42:31.592680  4780 caffe.cpp:313] Batch 929, accuracy/top1 = 0.02
I0816 10:42:31.592701  4780 caffe.cpp:313] Batch 929, accuracy/top5 = 0.06
I0816 10:42:31.592705  4780 caffe.cpp:313] Batch 929, loss = 6.54554
I0816 10:42:31.655525  4780 caffe.cpp:313] Batch 930, accuracy/top1 = 0.02
I0816 10:42:31.655547  4780 caffe.cpp:313] Batch 930, accuracy/top5 = 0.06
I0816 10:42:31.655550  4780 caffe.cpp:313] Batch 930, loss = 6.75912
I0816 10:42:31.718634  4780 caffe.cpp:313] Batch 931, accuracy/top1 = 0
I0816 10:42:31.718652  4780 caffe.cpp:313] Batch 931, accuracy/top5 = 0.02
I0816 10:42:31.718657  4780 caffe.cpp:313] Batch 931, loss = 6.86415
I0816 10:42:31.781519  4780 caffe.cpp:313] Batch 932, accuracy/top1 = 0
I0816 10:42:31.781540  4780 caffe.cpp:313] Batch 932, accuracy/top5 = 0.04
I0816 10:42:31.781543  4780 caffe.cpp:313] Batch 932, loss = 6.7094
I0816 10:42:31.844419  4780 caffe.cpp:313] Batch 933, accuracy/top1 = 0
I0816 10:42:31.844439  4780 caffe.cpp:313] Batch 933, accuracy/top5 = 0.04
I0816 10:42:31.844444  4780 caffe.cpp:313] Batch 933, loss = 6.98489
I0816 10:42:31.907443  4780 caffe.cpp:313] Batch 934, accuracy/top1 = 0
I0816 10:42:31.907462  4780 caffe.cpp:313] Batch 934, accuracy/top5 = 0.06
I0816 10:42:31.907466  4780 caffe.cpp:313] Batch 934, loss = 6.7582
I0816 10:42:31.970270  4780 caffe.cpp:313] Batch 935, accuracy/top1 = 0
I0816 10:42:31.970290  4780 caffe.cpp:313] Batch 935, accuracy/top5 = 0
I0816 10:42:31.970293  4780 caffe.cpp:313] Batch 935, loss = 7.14136
I0816 10:42:32.033380  4780 caffe.cpp:313] Batch 936, accuracy/top1 = 0
I0816 10:42:32.033397  4780 caffe.cpp:313] Batch 936, accuracy/top5 = 0.06
I0816 10:42:32.033401  4780 caffe.cpp:313] Batch 936, loss = 6.68803
I0816 10:42:32.096261  4780 caffe.cpp:313] Batch 937, accuracy/top1 = 0
I0816 10:42:32.096282  4780 caffe.cpp:313] Batch 937, accuracy/top5 = 0.02
I0816 10:42:32.096299  4780 caffe.cpp:313] Batch 937, loss = 6.75986
I0816 10:42:32.159255  4780 caffe.cpp:313] Batch 938, accuracy/top1 = 0
I0816 10:42:32.159274  4780 caffe.cpp:313] Batch 938, accuracy/top5 = 0.02
I0816 10:42:32.159278  4780 caffe.cpp:313] Batch 938, loss = 6.6119
I0816 10:42:32.222236  4780 caffe.cpp:313] Batch 939, accuracy/top1 = 0
I0816 10:42:32.222257  4780 caffe.cpp:313] Batch 939, accuracy/top5 = 0.1
I0816 10:42:32.222261  4780 caffe.cpp:313] Batch 939, loss = 6.70183
I0816 10:42:32.285094  4780 caffe.cpp:313] Batch 940, accuracy/top1 = 0
I0816 10:42:32.285115  4780 caffe.cpp:313] Batch 940, accuracy/top5 = 0.04
I0816 10:42:32.285118  4780 caffe.cpp:313] Batch 940, loss = 7.03089
I0816 10:42:32.347980  4780 caffe.cpp:313] Batch 941, accuracy/top1 = 0.02
I0816 10:42:32.348001  4780 caffe.cpp:313] Batch 941, accuracy/top5 = 0.02
I0816 10:42:32.348004  4780 caffe.cpp:313] Batch 941, loss = 6.97436
I0816 10:42:32.410799  4780 caffe.cpp:313] Batch 942, accuracy/top1 = 0
I0816 10:42:32.410912  4780 caffe.cpp:313] Batch 942, accuracy/top5 = 0.02
I0816 10:42:32.410917  4780 caffe.cpp:313] Batch 942, loss = 6.77303
I0816 10:42:32.473757  4780 caffe.cpp:313] Batch 943, accuracy/top1 = 0
I0816 10:42:32.473778  4780 caffe.cpp:313] Batch 943, accuracy/top5 = 0.02
I0816 10:42:32.473783  4780 caffe.cpp:313] Batch 943, loss = 6.76047
I0816 10:42:32.536763  4780 caffe.cpp:313] Batch 944, accuracy/top1 = 0
I0816 10:42:32.536783  4780 caffe.cpp:313] Batch 944, accuracy/top5 = 0.02
I0816 10:42:32.536787  4780 caffe.cpp:313] Batch 944, loss = 6.82866
I0816 10:42:32.599697  4780 caffe.cpp:313] Batch 945, accuracy/top1 = 0
I0816 10:42:32.599720  4780 caffe.cpp:313] Batch 945, accuracy/top5 = 0.04
I0816 10:42:32.599723  4780 caffe.cpp:313] Batch 945, loss = 6.82558
I0816 10:42:32.662536  4780 caffe.cpp:313] Batch 946, accuracy/top1 = 0.02
I0816 10:42:32.662559  4780 caffe.cpp:313] Batch 946, accuracy/top5 = 0.04
I0816 10:42:32.662561  4780 caffe.cpp:313] Batch 946, loss = 6.70276
I0816 10:42:32.725380  4780 caffe.cpp:313] Batch 947, accuracy/top1 = 0
I0816 10:42:32.725400  4780 caffe.cpp:313] Batch 947, accuracy/top5 = 0.04
I0816 10:42:32.725404  4780 caffe.cpp:313] Batch 947, loss = 7.12549
I0816 10:42:32.788332  4780 caffe.cpp:313] Batch 948, accuracy/top1 = 0
I0816 10:42:32.788353  4780 caffe.cpp:313] Batch 948, accuracy/top5 = 0.02
I0816 10:42:32.788357  4780 caffe.cpp:313] Batch 948, loss = 6.83708
I0816 10:42:32.851164  4780 caffe.cpp:313] Batch 949, accuracy/top1 = 0.02
I0816 10:42:32.851186  4780 caffe.cpp:313] Batch 949, accuracy/top5 = 0.06
I0816 10:42:32.851189  4780 caffe.cpp:313] Batch 949, loss = 6.61975
I0816 10:42:32.914316  4780 caffe.cpp:313] Batch 950, accuracy/top1 = 0
I0816 10:42:32.914338  4780 caffe.cpp:313] Batch 950, accuracy/top5 = 0.04
I0816 10:42:32.914341  4780 caffe.cpp:313] Batch 950, loss = 6.46194
I0816 10:42:32.977296  4780 caffe.cpp:313] Batch 951, accuracy/top1 = 0
I0816 10:42:32.977318  4780 caffe.cpp:313] Batch 951, accuracy/top5 = 0
I0816 10:42:32.977320  4780 caffe.cpp:313] Batch 951, loss = 6.75645
I0816 10:42:33.040340  4780 caffe.cpp:313] Batch 952, accuracy/top1 = 0.02
I0816 10:42:33.040359  4780 caffe.cpp:313] Batch 952, accuracy/top5 = 0.02
I0816 10:42:33.040362  4780 caffe.cpp:313] Batch 952, loss = 6.98994
I0816 10:42:33.103446  4780 caffe.cpp:313] Batch 953, accuracy/top1 = 0.02
I0816 10:42:33.103466  4780 caffe.cpp:313] Batch 953, accuracy/top5 = 0.02
I0816 10:42:33.103469  4780 caffe.cpp:313] Batch 953, loss = 6.76402
I0816 10:42:33.166537  4780 caffe.cpp:313] Batch 954, accuracy/top1 = 0.02
I0816 10:42:33.166556  4780 caffe.cpp:313] Batch 954, accuracy/top5 = 0.02
I0816 10:42:33.166560  4780 caffe.cpp:313] Batch 954, loss = 6.60633
I0816 10:42:33.229435  4780 caffe.cpp:313] Batch 955, accuracy/top1 = 0.02
I0816 10:42:33.229456  4780 caffe.cpp:313] Batch 955, accuracy/top5 = 0.04
I0816 10:42:33.229460  4780 caffe.cpp:313] Batch 955, loss = 6.85679
I0816 10:42:33.292471  4780 caffe.cpp:313] Batch 956, accuracy/top1 = 0.02
I0816 10:42:33.292493  4780 caffe.cpp:313] Batch 956, accuracy/top5 = 0.02
I0816 10:42:33.292496  4780 caffe.cpp:313] Batch 956, loss = 6.90865
I0816 10:42:33.355556  4780 caffe.cpp:313] Batch 957, accuracy/top1 = 0
I0816 10:42:33.355577  4780 caffe.cpp:313] Batch 957, accuracy/top5 = 0.02
I0816 10:42:33.355581  4780 caffe.cpp:313] Batch 957, loss = 6.76895
I0816 10:42:33.418555  4780 caffe.cpp:313] Batch 958, accuracy/top1 = 0
I0816 10:42:33.418576  4780 caffe.cpp:313] Batch 958, accuracy/top5 = 0.02
I0816 10:42:33.418579  4780 caffe.cpp:313] Batch 958, loss = 6.74618
I0816 10:42:33.481489  4780 caffe.cpp:313] Batch 959, accuracy/top1 = 0
I0816 10:42:33.481509  4780 caffe.cpp:313] Batch 959, accuracy/top5 = 0
I0816 10:42:33.481513  4780 caffe.cpp:313] Batch 959, loss = 6.5118
I0816 10:42:33.544371  4780 caffe.cpp:313] Batch 960, accuracy/top1 = 0
I0816 10:42:33.544392  4780 caffe.cpp:313] Batch 960, accuracy/top5 = 0.02
I0816 10:42:33.544395  4780 caffe.cpp:313] Batch 960, loss = 6.78954
I0816 10:42:33.607491  4780 caffe.cpp:313] Batch 961, accuracy/top1 = 0
I0816 10:42:33.607523  4780 caffe.cpp:313] Batch 961, accuracy/top5 = 0.04
I0816 10:42:33.607527  4780 caffe.cpp:313] Batch 961, loss = 6.67924
I0816 10:42:33.670493  4780 caffe.cpp:313] Batch 962, accuracy/top1 = 0.02
I0816 10:42:33.670514  4780 caffe.cpp:313] Batch 962, accuracy/top5 = 0.04
I0816 10:42:33.670517  4780 caffe.cpp:313] Batch 962, loss = 6.78653
I0816 10:42:33.733698  4780 caffe.cpp:313] Batch 963, accuracy/top1 = 0.04
I0816 10:42:33.733716  4780 caffe.cpp:313] Batch 963, accuracy/top5 = 0.08
I0816 10:42:33.733718  4780 caffe.cpp:313] Batch 963, loss = 6.554
I0816 10:42:33.796684  4780 caffe.cpp:313] Batch 964, accuracy/top1 = 0.04
I0816 10:42:33.796705  4780 caffe.cpp:313] Batch 964, accuracy/top5 = 0.04
I0816 10:42:33.796710  4780 caffe.cpp:313] Batch 964, loss = 6.61776
I0816 10:42:33.859735  4780 caffe.cpp:313] Batch 965, accuracy/top1 = 0
I0816 10:42:33.859755  4780 caffe.cpp:313] Batch 965, accuracy/top5 = 0.06
I0816 10:42:33.859758  4780 caffe.cpp:313] Batch 965, loss = 6.81437
I0816 10:42:33.922865  4780 caffe.cpp:313] Batch 966, accuracy/top1 = 0.06
I0816 10:42:33.922886  4780 caffe.cpp:313] Batch 966, accuracy/top5 = 0.14
I0816 10:42:33.922889  4780 caffe.cpp:313] Batch 966, loss = 6.35012
I0816 10:42:33.985728  4780 caffe.cpp:313] Batch 967, accuracy/top1 = 0
I0816 10:42:33.985749  4780 caffe.cpp:313] Batch 967, accuracy/top5 = 0
I0816 10:42:33.985752  4780 caffe.cpp:313] Batch 967, loss = 6.89137
I0816 10:42:34.049106  4780 caffe.cpp:313] Batch 968, accuracy/top1 = 0
I0816 10:42:34.049124  4780 caffe.cpp:313] Batch 968, accuracy/top5 = 0.02
I0816 10:42:34.049129  4780 caffe.cpp:313] Batch 968, loss = 6.64851
I0816 10:42:34.111896  4780 caffe.cpp:313] Batch 969, accuracy/top1 = 0.02
I0816 10:42:34.111918  4780 caffe.cpp:313] Batch 969, accuracy/top5 = 0.02
I0816 10:42:34.111922  4780 caffe.cpp:313] Batch 969, loss = 6.91761
I0816 10:42:34.174857  4780 caffe.cpp:313] Batch 970, accuracy/top1 = 0
I0816 10:42:34.174880  4780 caffe.cpp:313] Batch 970, accuracy/top5 = 0.02
I0816 10:42:34.174882  4780 caffe.cpp:313] Batch 970, loss = 6.86173
I0816 10:42:34.237813  4780 caffe.cpp:313] Batch 971, accuracy/top1 = 0
I0816 10:42:34.237834  4780 caffe.cpp:313] Batch 971, accuracy/top5 = 0.04
I0816 10:42:34.237838  4780 caffe.cpp:313] Batch 971, loss = 6.89392
I0816 10:42:34.300648  4780 caffe.cpp:313] Batch 972, accuracy/top1 = 0
I0816 10:42:34.300668  4780 caffe.cpp:313] Batch 972, accuracy/top5 = 0.02
I0816 10:42:34.300673  4780 caffe.cpp:313] Batch 972, loss = 6.84175
I0816 10:42:34.363590  4780 caffe.cpp:313] Batch 973, accuracy/top1 = 0
I0816 10:42:34.363611  4780 caffe.cpp:313] Batch 973, accuracy/top5 = 0.02
I0816 10:42:34.363615  4780 caffe.cpp:313] Batch 973, loss = 6.82309
I0816 10:42:34.426349  4780 caffe.cpp:313] Batch 974, accuracy/top1 = 0.02
I0816 10:42:34.426370  4780 caffe.cpp:313] Batch 974, accuracy/top5 = 0.04
I0816 10:42:34.426373  4780 caffe.cpp:313] Batch 974, loss = 6.72232
I0816 10:42:34.489286  4780 caffe.cpp:313] Batch 975, accuracy/top1 = 0.02
I0816 10:42:34.489307  4780 caffe.cpp:313] Batch 975, accuracy/top5 = 0.04
I0816 10:42:34.489310  4780 caffe.cpp:313] Batch 975, loss = 6.69343
I0816 10:42:34.552181  4780 caffe.cpp:313] Batch 976, accuracy/top1 = 0.04
I0816 10:42:34.552201  4780 caffe.cpp:313] Batch 976, accuracy/top5 = 0.08
I0816 10:42:34.552204  4780 caffe.cpp:313] Batch 976, loss = 6.47932
I0816 10:42:34.615180  4780 caffe.cpp:313] Batch 977, accuracy/top1 = 0
I0816 10:42:34.615200  4780 caffe.cpp:313] Batch 977, accuracy/top5 = 0
I0816 10:42:34.615203  4780 caffe.cpp:313] Batch 977, loss = 6.78922
I0816 10:42:34.678025  4780 caffe.cpp:313] Batch 978, accuracy/top1 = 0.06
I0816 10:42:34.678045  4780 caffe.cpp:313] Batch 978, accuracy/top5 = 0.06
I0816 10:42:34.678050  4780 caffe.cpp:313] Batch 978, loss = 6.54779
I0816 10:42:34.741065  4780 caffe.cpp:313] Batch 979, accuracy/top1 = 0.02
I0816 10:42:34.741086  4780 caffe.cpp:313] Batch 979, accuracy/top5 = 0.04
I0816 10:42:34.741089  4780 caffe.cpp:313] Batch 979, loss = 6.80365
I0816 10:42:34.804121  4780 caffe.cpp:313] Batch 980, accuracy/top1 = 0.06
I0816 10:42:34.804155  4780 caffe.cpp:313] Batch 980, accuracy/top5 = 0.06
I0816 10:42:34.804159  4780 caffe.cpp:313] Batch 980, loss = 6.76599
I0816 10:42:34.866878  4780 caffe.cpp:313] Batch 981, accuracy/top1 = 0
I0816 10:42:34.866899  4780 caffe.cpp:313] Batch 981, accuracy/top5 = 0.04
I0816 10:42:34.866902  4780 caffe.cpp:313] Batch 981, loss = 6.58985
I0816 10:42:34.929764  4780 caffe.cpp:313] Batch 982, accuracy/top1 = 0
I0816 10:42:34.929785  4780 caffe.cpp:313] Batch 982, accuracy/top5 = 0.04
I0816 10:42:34.929788  4780 caffe.cpp:313] Batch 982, loss = 6.67314
I0816 10:42:34.992727  4780 caffe.cpp:313] Batch 983, accuracy/top1 = 0
I0816 10:42:34.992748  4780 caffe.cpp:313] Batch 983, accuracy/top5 = 0
I0816 10:42:34.992750  4780 caffe.cpp:313] Batch 983, loss = 6.80786
I0816 10:42:35.055629  4780 caffe.cpp:313] Batch 984, accuracy/top1 = 0.02
I0816 10:42:35.055649  4780 caffe.cpp:313] Batch 984, accuracy/top5 = 0.02
I0816 10:42:35.055651  4780 caffe.cpp:313] Batch 984, loss = 6.82565
I0816 10:42:35.118569  4780 caffe.cpp:313] Batch 985, accuracy/top1 = 0
I0816 10:42:35.118592  4780 caffe.cpp:313] Batch 985, accuracy/top5 = 0.02
I0816 10:42:35.118595  4780 caffe.cpp:313] Batch 985, loss = 6.83389
I0816 10:42:35.181735  4780 caffe.cpp:313] Batch 986, accuracy/top1 = 0
I0816 10:42:35.181754  4780 caffe.cpp:313] Batch 986, accuracy/top5 = 0.02
I0816 10:42:35.181757  4780 caffe.cpp:313] Batch 986, loss = 6.55647
I0816 10:42:35.244784  4780 caffe.cpp:313] Batch 987, accuracy/top1 = 0.02
I0816 10:42:35.244803  4780 caffe.cpp:313] Batch 987, accuracy/top5 = 0.02
I0816 10:42:35.244807  4780 caffe.cpp:313] Batch 987, loss = 6.81546
I0816 10:42:35.307829  4780 caffe.cpp:313] Batch 988, accuracy/top1 = 0
I0816 10:42:35.307849  4780 caffe.cpp:313] Batch 988, accuracy/top5 = 0.04
I0816 10:42:35.307853  4780 caffe.cpp:313] Batch 988, loss = 6.88663
I0816 10:42:35.370775  4780 caffe.cpp:313] Batch 989, accuracy/top1 = 0
I0816 10:42:35.370797  4780 caffe.cpp:313] Batch 989, accuracy/top5 = 0.04
I0816 10:42:35.370800  4780 caffe.cpp:313] Batch 989, loss = 6.94921
I0816 10:42:35.433739  4780 caffe.cpp:313] Batch 990, accuracy/top1 = 0
I0816 10:42:35.433760  4780 caffe.cpp:313] Batch 990, accuracy/top5 = 0
I0816 10:42:35.433764  4780 caffe.cpp:313] Batch 990, loss = 6.76345
I0816 10:42:35.496709  4780 caffe.cpp:313] Batch 991, accuracy/top1 = 0
I0816 10:42:35.496729  4780 caffe.cpp:313] Batch 991, accuracy/top5 = 0.04
I0816 10:42:35.496733  4780 caffe.cpp:313] Batch 991, loss = 6.7323
I0816 10:42:35.559633  4780 caffe.cpp:313] Batch 992, accuracy/top1 = 0
I0816 10:42:35.559654  4780 caffe.cpp:313] Batch 992, accuracy/top5 = 0.02
I0816 10:42:35.559658  4780 caffe.cpp:313] Batch 992, loss = 7.10579
I0816 10:42:35.622550  4780 caffe.cpp:313] Batch 993, accuracy/top1 = 0
I0816 10:42:35.622570  4780 caffe.cpp:313] Batch 993, accuracy/top5 = 0.06
I0816 10:42:35.622575  4780 caffe.cpp:313] Batch 993, loss = 6.83126
I0816 10:42:35.685602  4780 caffe.cpp:313] Batch 994, accuracy/top1 = 0.04
I0816 10:42:35.685623  4780 caffe.cpp:313] Batch 994, accuracy/top5 = 0.06
I0816 10:42:35.685626  4780 caffe.cpp:313] Batch 994, loss = 6.69874
I0816 10:42:35.748687  4780 caffe.cpp:313] Batch 995, accuracy/top1 = 0
I0816 10:42:35.748704  4780 caffe.cpp:313] Batch 995, accuracy/top5 = 0.02
I0816 10:42:35.748708  4780 caffe.cpp:313] Batch 995, loss = 6.80175
I0816 10:42:35.811559  4780 caffe.cpp:313] Batch 996, accuracy/top1 = 0
I0816 10:42:35.811580  4780 caffe.cpp:313] Batch 996, accuracy/top5 = 0.04
I0816 10:42:35.811584  4780 caffe.cpp:313] Batch 996, loss = 7.03019
I0816 10:42:35.857236  4799 data_reader.cpp:288] Starting prefetch of epoch 1
I0816 10:42:35.874706  4780 caffe.cpp:313] Batch 997, accuracy/top1 = 0.02
I0816 10:42:35.874732  4780 caffe.cpp:313] Batch 997, accuracy/top5 = 0.02
I0816 10:42:35.874737  4780 caffe.cpp:313] Batch 997, loss = 6.71294
I0816 10:42:35.937736  4780 caffe.cpp:313] Batch 998, accuracy/top1 = 0.02
I0816 10:42:35.937757  4780 caffe.cpp:313] Batch 998, accuracy/top5 = 0.02
I0816 10:42:35.937773  4780 caffe.cpp:313] Batch 998, loss = 6.96561
I0816 10:42:36.001528  4780 caffe.cpp:313] Batch 999, accuracy/top1 = 0
I0816 10:42:36.001545  4780 caffe.cpp:313] Batch 999, accuracy/top5 = 0.06
I0816 10:42:36.001549  4780 caffe.cpp:313] Batch 999, loss = 6.44659
I0816 10:42:36.001552  4780 caffe.cpp:318] Loss: 6.7386
I0816 10:42:36.001559  4780 caffe.cpp:330] accuracy/top1 = 0.012
I0816 10:42:36.001564  4780 caffe.cpp:330] accuracy/top5 = 0.0360803
I0816 10:42:36.001567  4780 caffe.cpp:330] loss = 6.7386 (* 1 = 6.7386 loss)
